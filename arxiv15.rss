<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 06 Mar 2025 22:05:26 +0800</lastBuildDate>
    <item>
      <title>Towards Efficient Contrastive PAC Learning</title>
      <link>http://arxiv.org/abs/2502.15962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文在PAC学习框架下研究对比学习，揭示了对于线性表示这一基本概念的对比学习存在高效PAC学习器的问题仍未解决，并提出了一种基于Rademacher复杂度的泛化保证方法。&lt;h4&gt;背景&lt;/h4&gt;尽管最近有许多关于对比损失下的统计结果的研究，这些算法通常效率不高或无法提供PAC保证。&lt;h4&gt;目的&lt;/h4&gt;探讨线性表示的对比学习问题是否可以在PAC框架下以高效的方式解决，并尝试构建一个有效的对比学习算法。&lt;h4&gt;方法&lt;/h4&gt;首先证明了在一般情况下，对比PAC学习线性表示是难以处理的问题。接着展示当对比样本之间的距离用L2范数度量时，该问题可以松弛为半定规划(SDP)形式。然后基于Rademacher复杂度建立泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;在对比学习的线性表示这一基本设置下，存在高效的PAC学习器的问题依然开放。提出了一个通过半定规划放松问题的方法以及一种基于Rademacher复杂度的泛化保证方法，这是已知的第一个有效的对比学习的PAC算法。&lt;h4&gt;结论&lt;/h4&gt;论文证明了对比学习中的某些核心挑战，并提出了一种新的解决策略和理论基础，为高效且具有PAC保证的学习提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在PAC学习框架下的对比学习。尽管最近有许多关于基于VC维或Rademacher复杂度的对比损失下统计结果的研究，但这些算法通常是低效的或无法提供PAC保证。在这篇论文中，我们考虑了线性表示这一基本概念的学习问题，并发现即使在这种简单的设置下，高效且具有PAC保证的学习器的存在仍然是一个开放的问题。我们首先表明一般情况下对比PAC学习线性表示是难以处理的，然后展示当对比样本之间的距离用L2范数度量时该问题可以松弛为半定规划形式。接着基于Rademacher复杂度建立了泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。据我们所知，这是第一个有效的对比学习的PAC算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study contrastive learning under the PAC learning framework. While aseries of recent works have shown statistical results for learning undercontrastive loss, based either on the VC-dimension or Rademacher complexity,their algorithms are inherently inefficient or not implying PAC guarantees. Inthis paper, we consider contrastive learning of the fundamental concept oflinear representations. Surprisingly, even under such basic setting, theexistence of efficient PAC learners is largely open. We first show that theproblem of contrastive PAC learning of linear representations is intractable tosolve in general. We then show that it can be relaxed to a semi-definiteprogram when the distance between contrastive samples is measured by the$\ell_2$-norm. We then establish generalization guarantees based on Rademachercomplexity, and connect it to PAC guarantees under certain contrastivelarge-margin conditions. To the best of our knowledge, this is the firstefficient PAC learning algorithm for contrastive learning.</description>
      <author>example@mail.com (Jie Shen)</author>
      <guid isPermaLink="false">2502.15962v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
  <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的Intermediate Domain-guided Adaptation (IDA) 方法，利用CAM图像和视网膜图像之间的相似性以及现有的公共视网膜数据集进行无监督训练。&lt;h4&gt;背景&lt;/h4&gt;在血管生成研究中广泛使用的胎盘绒毛膜尿囊膜模型（CAM）的血血管分布是关键评价指标。由于手动分割耗时且主观性强，因此开发自动化的血管分割算法至关重要。&lt;h4&gt;目的&lt;/h4&gt;为解决现有CAM血管分割算法有限以及公开数据集不足的问题，提出了一种创新的方法来改进无监督域适应技术。&lt;h4&gt;方法&lt;/h4&gt;提出Multi-Resolution Asymmetric Translation (MRAT)策略生成中间图像以促进图像级交互；开发Intermediate Domain-guided Contrastive Learning (IDCL)模块以解开跨域特征表示。使用公开的视网膜数据集进行训练，并创建首个CAM数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，该方法在所提出的CAM数据集中表现优异，同时在不同的视网膜数据集中也展示了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;IDa方法克服了现有无监督域适应（UDA）技术只关注源目标直接对齐的局限性，并成功应用于血管生成研究中，有望改进其他生物医学图像处理任务。&lt;h4&gt;翻译&lt;/h4&gt;绒毛尿囊膜模型广泛用于血管生成研究，血管分布是评估的关键指标。因此，基于拓扑和形态学特征的定量评价需要精确分割血管。然而，手动分割耗时且易出错。此外，关于CAM血管分割算法的研究有限，缺乏公开数据集导致预测性能不佳。为解决这些问题，本文提出了一种新颖的方法：Intermediate Domain-guided Adaptation (IDA) 方法，利用了CAM图像和视网膜图像的相似性以及现有的公共视网膜数据集进行无监督训练。通过在首个创建的CAM数据集中进行全面实验，证明该方法优于其他现有方法，并且在不同的视网膜数据集中也展现了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/PWSong-ustc/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</title>
      <link>http://arxiv.org/abs/2503.03751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in CVPR 2025. Website:  https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GEN3C是一种基于精确相机控制和时间三维一致性的生成视频模型。&lt;h4&gt;背景&lt;/h4&gt;现有的视频生成模型虽然能够产生真实的视频，但很少利用三维信息，导致物体突然出现或消失等问题。此外，现有模型中的相机控制不够精准。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有视频生成模型中存在的问题，并提高视频的逼真度和一致性。&lt;h4&gt;方法&lt;/h4&gt;GEN3C通过预测种子图像或先前生成帧的像素深度来获取点云作为三维缓存。在生成下一帧时，基于用户提供的新相机轨迹对三维缓存进行2D渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相较于之前的工作，GEN3C实现了更精确的相机控制，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等具有挑战性的设置中表现尤为出色。&lt;h4&gt;结论&lt;/h4&gt;该研究为生成逼真、一致的三维视频提供了一种创新的方法，有望应用于更多领域。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了GEN3C，这是一种具备精确相机控制能力和时间三维一致性的生成视频模型。先前的视频模型已经可以生成现实感强的视频，但它们倾向于利用很少的三维信息，导致不一致性问题，如物体突然出现或消失等现象。如果实现了任何相机控制，通常也是不够精准的，因为摄像机参数仅仅是输入给神经网络的部分数据，需要网络自己推断视频是如何依赖于摄像机姿态的。相比之下，GEN3C使用一个由点云构成的三维缓存指导其工作：通过预测种子图像或先前生成帧的像素深度得到这些点云。当生成下一帧时，模型基于用户提供的新相机轨迹对三维缓存进行2D渲染来条件化。至关重要的是，这意味着GEN3C不必记住之前产生的内容，也不必从摄像机姿态推断出图像结构。相反，它可以将全部的生成能力集中在尚未观察到的区域，并推进场景状态到达下一帧。我们的实验结果表明比之前的模型具有更精确的相机控制能力，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等复杂设置中表现尤为出色。观看视频以查看最佳效果！请访问我们的网页：https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nv-tlabs/GEN3C&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GEN3C, a generative video model with precise Camera Control andtemporal 3D Consistency. Prior video models already generate realistic videos,but they tend to leverage little 3D information, leading to inconsistencies,such as objects popping in and out of existence. Camera control, if implementedat all, is imprecise, because camera parameters are mere inputs to the neuralnetwork which must then infer how the video depends on the camera. In contrast,GEN3C is guided by a 3D cache: point clouds obtained by predicting thepixel-wise depth of seed images or previously generated frames. When generatingthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache withthe new camera trajectory provided by the user. Crucially, this means thatGEN3C neither has to remember what it previously generated nor does it have toinfer the image structure from the camera pose. The model, instead, can focusall its generative power on previously unobserved regions, as well as advancingthe scene state to the next frame. Our results demonstrate more precise cameracontrol than prior work, as well as state-of-the-art results in sparse-viewnovel view synthesis, even in challenging settings such as driving scenes andmonocular dynamic video. Results are best viewed in videos. Check out ourwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/</description>
      <author>example@mail.com (Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao)</author>
      <guid isPermaLink="false">2503.03751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters</title>
      <link>http://arxiv.org/abs/2503.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为BALViT的新方法，该方法通过冻结的视觉模型作为模态特征编码器来学习强大的LiDAR编码器。结合范围视图和鸟瞰视角的LiDAR编码机制，并引入了新的2D-3D适配器来提高性能。&lt;h4&gt;背景&lt;/h4&gt;当前的LiDAR语义分割模型由于缺乏大规模多样化的数据集而难以进行通用预训练，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法BALViT，旨在通过冻结的视觉模型作为模态特征编码器来增强LiDAR编码能力，并在小数据场景下实现高性能。&lt;h4&gt;方法&lt;/h4&gt;BALViT采用范围视图和鸟瞰视角相结合的方式进行LiDAR编码。范围视图特征经过冻结的图像骨干网络处理，而鸟瞰视角分支通过多次交叉注意力交互增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在小数据集环境下能够显著提高性能，并且在SemanticKITTI和nuScenes基准测试中优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;研究证明了BALViT方法的有效性，可以为其他相关领域提供有价值的参考。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR语义分割模型通常从随机初始化训练开始，由于缺乏大规模多样化数据集，通用预训练受阻。此外，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。受到统一基础模型最新进展的启发，我们提出了BALViT这一新方法，该方法利用冻结视觉模型作为模态特征编码器来学习强大的LiDAR编码器。具体而言，BALViT结合了范围视图和鸟瞰视角的LiDAR编码机制，并通过新的2D-3D适配器组合这些机制。虽然范围视图特征经过冻结图像骨干网络处理，但我们的鸟瞰视角分支通过多次交叉注意力交互增强这些特征，因此可以不断改进视觉网络以融入领域相关的知识，从而产生强大的标签高效LiDAR编码机制。在SemanticKITTI和nuScenes基准上的广泛评估表明，在小数据集环境中，它优于现有方法。我们将在http://balvit.cs.uni-freiburg.de上公开代码和模型。&lt;h4&gt;贡献&lt;/h4&gt;提出了BALViT框架，解决了当前LiDAR语义分割模型缺乏大规模多样化数据集的问题，并在多个性能指标上取得了显著提升&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR semantic segmentation models are typically trained from randominitialization as universal pre-training is hindered by the lack of large,diverse datasets. Moreover, most point cloud segmentation architecturesincorporate custom network layers, limiting the transferability of advancesfrom vision-based architectures. Inspired by recent advances in universalfoundation models, we propose BALViT, a novel approach that leverages frozenvision models as amodal feature encoders for learning strong LiDAR encoders.Specifically, BALViT incorporates both range-view and bird's-eye-view LiDARencoding mechanisms, which we combine through a novel 2D-3D adapter. While therange-view features are processed through a frozen image backbone, ourbird's-eye-view branch enhances them through multiple cross-attentioninteractions. Thereby, we continuously improve the vision network withdomain-dependent knowledge, resulting in a strong label-efficient LiDARencoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI andnuScenes benchmarks demonstrate that it outperforms state-of-the-art methods onsmall data regimes. We make the code and models publicly available at:http://balvit.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Rohit Mohan, Jelena Bratulic, Daniele Cattaneo, Thomas Brox, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.03299v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning to Explore via Memory Density Feedback</title>
      <link>http://arxiv.org/abs/2503.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种利用元学习探索算法，该算法使智能体能够最大化其在单一时间间隔内的探索进度。&lt;h4&gt;背景&lt;/h4&gt;传统的强化学习探索方法通常通过添加一种内在奖励来替换或增强原始奖赏函数，以促使智能体探索未曾见过的状态。&lt;h4&gt;目的&lt;/h4&gt;研究旨在设计一种新型的元学习探索算法，该算法允许智能体在训练周期间甚至未训练过的环境中最大化其探索进度。&lt;h4&gt;方法&lt;/h4&gt;智能体会学习一种策略，即最小化新观察结果相对于所有记忆的概率密度，并根据当前观测密度接收反馈，在递归网络中保存这些反馈。通过这种方式，智能体学会了实时导航熟悉度不断增长的复杂环境。&lt;h4&gt;主要发现&lt;/h4&gt;基于上述设计，智能体可以在完全新颖的状态下进行探索并最大化其探索进度，即使在其策略没有为此类状态训练过的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;该元学习方法有效提高了智能体在未见过环境中探索的能力和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的探索算法通过利用元学习（即学会学习）来增强智能体在一个时间间隔内优化其探索进度的能力，即使是在训练周期之间也能实现。这种策略帮助智能体最小化新观察结果相对于所有已存记忆的概率密度，并且能够根据当前观测密度反馈进行调整。这样做的结果是，智能体可以实时地导航一个复杂且不断扩展的熟悉度景观，在从未见过的状态中最大化其探索进度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploration algorithms for reinforcement learning typically replace oraugment the reward function with an additional ``intrinsic'' reward that trainsthe agent to seek previously unseen states of the environment. Here, weconsider an exploration algorithm that exploits meta-learning, or learning tolearn, such that the agent learns to maximize its exploration progress within asingle episode, even between epochs of training. The agent learns a policy thataims to minimize the probability density of new observations with respect toall of its memories. In addition, it receives as feedback evaluations of thecurrent observation density and retains that feedback in a recurrent network.By remembering trajectories of density, the agent learns to navigate a complexand growing landscape of familiarity in real-time, allowing it to maximize itsexploration progress even in completely novel states of the environment forwhich its policy has not been trained.</description>
      <author>example@mail.com (Kevin L. McKee)</author>
      <guid isPermaLink="false">2503.02831v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving</title>
      <link>http://arxiv.org/abs/2503.03205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多代理框架MA-LoT，该框架结合了自然语言推理和形式语言验证，在Lean4定理证明中取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;使用计算机可验证的语言（如Lean）解决数学问题对数学和计算机科学领域产生了重大影响。当前最先进的方法依赖于单一大型语言模型（LLMs），这些模型作为代理或证明者，负责生成完整证明或进行树搜索，但缺乏将自然语言推理与形式语言验证反馈相结合的结构化方式。&lt;h4&gt;目的&lt;/h4&gt;为了克服单一代理方法的局限性，并结合高级自然语言推理和形式语言验证，本文提出了MA-LoT框架。&lt;h4&gt;方法&lt;/h4&gt;该框架利用长链思维（Long CoT）中的涌现形式推理能力以及新颖的LoT-Transfer学习训练推断流水线。通过这种方式，在证明生成中实现了更深入的洞察力和长期一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MA-LoT在Lean4版本的MiniF2F-Test数据集上达到了54.51%的准确率，显著优于GPT-4（22.95%）、单代理树搜索(InternLM-Step-Prover, 50.70%)和完整证明生成(DeepSeek-Prover-v1.5, 48.36%)基线。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了结合长链思维与形式验证在更广泛的视角中进行更为深刻生成的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving mathematical problems using computer-verifiable languages like Leanhas significantly impacted mathematical and computer science communities.State-of-the-art methods utilize single Large Language Models (LLMs) as agentsor provers to either generate complete proof or perform tree searches. However,single-agent methods inherently lack a structured way to combine high-levelreasoning in Natural Language (NL) with Formal Language (FL) verificationfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based LongChain-of-Thought framework, (to the best of our knowledge), the firstmulti-agent framework for Lean4 theorem proving that balance high-level NLreasoning and FL verification in Long CoT. Using this structured interaction,our approach enables deeper insights and long-term coherence in proofgeneration, with which past methods struggle. We do this by leveraging emergentformal reasoning ability in Long CoT using our novel LoT-Transfer Learningtraining-inference pipeline. Extensive experiments show that our frameworkachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,largely outperforming GPT-4 (22.95%), single-agent tree search(InternLM-Step-Prover, 50.70%), and whole-proof generation(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlightthe potential of combining Long CoT with formal verification for a moreinsightful generation in a broader perspective.</description>
      <author>example@mail.com (Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang)</author>
      <guid isPermaLink="false">2503.03205v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation</title>
      <link>http://arxiv.org/abs/2503.03599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REGRACE是一种新颖的方法，利用LiDAR子图解决了大尺度导航中回环闭合的可扩展性和视角差异问题。&lt;h4&gt;背景&lt;/h4&gt;当前使用密集点云进行精确位置识别的方法由于扫描到扫描之间的比较计算量过大而不具备良好的可扩展性。而基于对象的方法虽然效率更高，但往往对视点变化敏感。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法REGRACE来解决大规模导航中回环闭合的挑战问题，提高系统的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;引入旋转不变特征和图神经网络增强的邻居上下文信息。使用词袋模型进行子地图之间的高效匹配，并利用几何一致性识别远程回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;REGRACE在保持高精度的同时提高了速度，与最先进的位置识别基线相比快一倍。&lt;h4&gt;结论&lt;/h4&gt;REGRACE能够在大规模导航环境中有效地实现准确的回环闭合检测，具备良好的可扩展性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;循环闭合对于校正里程计漂移和创建一致的地图至关重要，尤其是在大尺度导航中。当前使用密集点云的方法由于扫描到扫描之间的比较计算量过大而不具良好扩展性。基于对象的替代方法虽然更高效，但往往对视点变化敏感。在这项工作中，我们引入了REGRACE，这是一种新颖的方法，通过使用LiDAR基子图来解决重新定位中的可扩展性和视角差异问题。我们介绍了每个标记对象的旋转不变特征，并通过图神经网络增强它们以考虑邻居上下文信息。为了识别潜在重复访问，我们采用了一种可扩展的词袋方法，每幅子地图池化一个学习到的全局特征。另外，我们用几何一致性线索来定义重新访问，而不是基于嵌入距离，这使我们可以识别远处的回环闭合。我们的评估表明，REGRACE在位置识别和配准基线方面取得了相似的结果，而速度却快了一倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Loop closures are essential for correcting odometry drift and creatingconsistent maps, especially in the context of large-scale navigation. Currentmethods using dense point clouds for accurate place recognition do not scalewell due to computationally expensive scan-to-scan comparisons. Alternativeobject-centric approaches are more efficient but often struggle withsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novelapproach that addresses these challenges of scalability and perspectivedifference in re-localization by using LiDAR-based submaps. We introducerotation-invariant features for each labeled object and enhance them withneighborhood context through a graph neural network. To identify potentialrevisits, we employ a scalable bag-of-words approach, pooling one learnedglobal feature per submap. Additionally, we define a revisit with geometricalconsistency cues rather than embedding distance, allowing us to recognizefar-away loop closures. Our evaluations demonstrate that REGRACE achievessimilar results compared to state-of-the-art place recognition and registrationbaselines while being twice as fast.</description>
      <author>example@mail.com (Débora N. P. Oliveira, Joshua Knights, Sebastián Barbas Laina, Simon Boche, Wolfram Burgard, Stefan Leutenegger)</author>
      <guid isPermaLink="false">2503.03599v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.03634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），用于从观测数据中进行因果发现。&lt;h4&gt;背景&lt;/h4&gt;在从观察性数据中进行因果发现时，缺乏完美干预是一个主要挑战，这使得区分真正的因果特征和虚假的特征变得困难。&lt;h4&gt;目的&lt;/h4&gt;通过使用匹配程序模拟完美的干预来识别因果关系，并定义了因果潜在图（Causal Latent Graphs），这种框架连接了FMI与因果图学习。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的理论框架，即因果潜在图，并开发了一个模仿完美干预的特征匹配过程。&lt;h4&gt;主要发现&lt;/h4&gt;理论上表明，FMI表现出强大的出分布（OOD）泛化能力。实验进一步证实了FMI在仅从观察数据中有效识别因果特征方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为解决观测性数据分析中的因果关系问题提供了一个新的视角和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在从观测数据进行因果发现时，缺乏完美的干预是一个主要挑战。为了应对这一问题，我们提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），该方法通过使用一个匹配程序来模仿完美干预，并定义了因果潜在图，即扩展到潜在特征空间的结构因果模型，为FMI与因果图学习之间的连接提供了一个框架。我们的特征匹配过程在这些因果潜在图中模拟完美的干预行为。理论结果表明，FMI具有强大的出分布（OOD）泛化能力。实验进一步证明了FMI仅从观测数据有效识别因果特征方面的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge in causal discovery from observational data is the absenceof perfect interventions, making it difficult to distinguish causal featuresfrom spurious ones. We propose an innovative approach, Feature MatchingIntervention (FMI), which uses a matching procedure to mimic perfectinterventions. We define causal latent graphs, extending structural causalmodels to latent feature space, providing a framework that connects FMI withcausal graph learning. Our feature matching procedure emulates perfectinterventions within these causal latent graphs. Theoretical resultsdemonstrate that FMI exhibits strong out-of-distribution (OOD)generalizability. Experiments further highlight FMI's superior performance ineffectively identifying causal features solely from observational data.</description>
      <author>example@mail.com (Haoze Li, Jun Xie)</author>
      <guid isPermaLink="false">2503.03634v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DDEQs: Distributional Deep Equilibrium Models through Wasserstein Gradient Flows</title>
      <link>http://arxiv.org/abs/2503.01140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures. To be published in AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了分布式深度平衡模型(DDEQ)，它是隐含神经网络的一种，扩展了Deep Equilibrium Models (DEQs) 至离散度量输入。&lt;h4&gt;背景&lt;/h4&gt;传统的DEQ主要处理序列数据，但已经应用于各种类型的数据。然而，现有的框架没有充分利用离散度量的特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的理论基础框架来支持基于离散度量（如集合或点云）的深度平衡模型，以提高其在特定任务中的表现和参数效率。&lt;h4&gt;方法&lt;/h4&gt;通过利用Wasserstein梯度流，展示了如何调整DEQ前向传递的方式，在交换不变性的条件下找到离散度量下的固定点，并推导出适当的网络架构以适应DDEQ。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的模型相比，DDEQ在点云分类和点云补全等任务上具有竞争力，同时显著减少了参数的使用效率。&lt;h4&gt;结论&lt;/h4&gt;通过扩展DEQ框架来处理离散度量输入，如集合或点云，可以提高模型性能并降低参数需求。&lt;h4&gt;翻译&lt;/h4&gt;深度平衡模型(DDEQs) 是一种隐含神经网络类，它将Deep Equilibrium Models (DEQs) 扩展到离散度量输入(例如集合或点云)，提供了理论基础框架。通过使用Wasserstein梯度流, 展示了如何调整DEQ前向传递以在交换不变性的条件下找到离散度量的固定点，并推导出适应DDEQs 的网络架构。实验表明，它们可以与最先进的模型竞争，完成如点云分类和补全等任务，同时参数效率显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Equilibrium Models (DEQs) are a class of implicit neural networks thatsolve for a fixed point of a neural network in their forward pass.Traditionally, DEQs take sequences as inputs, but have since been applied to avariety of data. In this work, we present Distributional Deep EquilibriumModels (DDEQs), extending DEQs to discrete measure inputs, such as sets orpoint clouds. We provide a theoretically grounded framework for DDEQs.Leveraging Wasserstein gradient flows, we show how the forward pass of the DEQcan be adapted to find fixed points of discrete measures underpermutation-invariance, and derive adequate network architectures for DDEQs. Inexperiments, we show that they can compete with state-of-the-art models intasks such as point cloud classification and point cloud completion, whilebeing significantly more parameter-efficient.</description>
      <author>example@mail.com (Jonathan Geuter, Clément Bonet, Anna Korba, David Alvarez-Melis)</author>
      <guid isPermaLink="false">2503.01140v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance</title>
      <link>http://arxiv.org/abs/2503.03689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;DualDiff是一种双分支条件扩散模型，用于改进多视角和视频序列的驾驶场景生成。&lt;h4&gt;背景信息&lt;/h4&gt;当前方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这些方法无法捕捉驾驶场景的全部复杂性，并且不足以充分整合多模态信息。&lt;h4&gt;研究目的&lt;/h4&gt;提出DualDiff模型以解决现有技术在复杂性和多模态信息集成上的不足。&lt;h4&gt;主要方法&lt;/h4&gt;{'ORS': 'Occupancy Ray-shape Sampling，提供丰富的前景和背景语义以及3D空间几何结构', 'FGM': 'Foreground-Aware Mask，增强精细的前景物体合成', 'SFA': 'Semantic Fusion Attention机制，优先处理相关信息并抑制噪声', 'RGD': 'Reward-Guided Diffusion框架，确保生成视频的一致性和语义连贯性'}&lt;h4&gt;实验结果&lt;/h4&gt;{'性能提升': '在多个数据集上实现了最先进的（SOTA）表现。', 'NuScenes数据集': '相比最佳基线，FID得分减少了4.09%。', '下游任务改进': {'BEV分割': '车辆mIoU提高4.50%，道路mIoU提高1.70%', 'BEV 3D物体检测': '前景mAP提升1.46%'}}&lt;h4&gt;结论&lt;/h4&gt;DualDiff在多个评价指标上超越现有方法，证明了其在驾驶场景重建中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确且高保真的驾驶场景重建需要有效利用全面的场景信息作为条件输入。现有的方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这无法捕捉到驾驶场景的全部复杂性，并不足以充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强多视角和视频序列中的驾驶场景生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and high-fidelity driving scene reconstruction demands the effectiveutilization of comprehensive scene information as conditional inputs. Existingmethods predominantly rely on 3D bounding boxes and BEV road maps forforeground and background control, which fail to capture the full complexity ofdriving scenes and adequately integrate multimodal information. In this work,we present DualDiff, a dual-branch conditional diffusion model designed toenhance driving scene generation across multiple views and video sequences.Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditionalinput, offering rich foreground and background semantics alongside 3D spatialgeometry to precisely control the generation of both elements. To improve thesynthesis of fine-grained foreground objects, particularly complex and distantones, we propose a Foreground-Aware Mask (FGM) denoising loss function.Additionally, we develop the Semantic Fusion Attention (SFA) mechanism todynamically prioritize relevant information and suppress noise, enabling moreeffective multimodal fusion. Finally, to ensure high-quality image-to-videogeneration, we introduce the Reward-Guided Diffusion (RGD) framework, whichmaintains global consistency and semantic coherence in generated videos.Extensive experiments demonstrate that DualDiff achieves state-of-the-art(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiffreduces the FID score by 4.09% compared to the best baseline. In downstreamtasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% androad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAPincreases by 1.46%. Code will be made available athttps://github.com/yangzhaojason/DualDiff.</description>
      <author>example@mail.com (Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu)</author>
      <guid isPermaLink="false">2503.03689v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Opportunistic Routing in Wireless Communications via Learnable State-Augmented Policies</title>
      <link>http://arxiv.org/abs/2503.03736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了大规模无线通信网络中基于数据包的信息路由挑战，通过约束统计学习任务来解决此问题。&lt;h4&gt;背景&lt;/h4&gt;在大型无线网络中实现有效的信息传递是一个关键挑战。传统的路由策略依赖于特定的路径选择算法，但这种方法在动态网络环境中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分布式优化方法（State-Augmentation, SA）以提高源节点的信息处理能力，并通过图神经网络（GNNs）来提取最佳的路由政策。&lt;h4&gt;方法&lt;/h4&gt;利用图卷积操作和基于网络节点之间拓扑连接关系的无监督学习框架，设计了一种新颖的方法。该方法能够根据实时信息动态选择最优中继节点。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统的基线算法相比，所提出的GNN参数化模型在处理多流数据时表现出色，尤其是在大型网络环境中具有更高的效率和稳定性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了利用图神经网络进行无线通信路由优化的潜力，突出了其鲁棒性和可转移性。该方法不仅能够有效应对大规模无线网络中的动态挑战，而且可以应用于各种实际场景中。&lt;h4&gt;翻译&lt;/h4&gt;此论文解决的是在大型无线通信网絡中基于数据包的信息传递问题。它被描述为一个受限统计学习任务，在这个过程中每个节点仅使用本地信息进行操作。机会性路由利用了无线电通信的广播特性来动态选择最佳转发节点，从而使信息能够通过多个中继节点同时到达目的地。为了应对这一挑战，我们提出了一种基于状态增强（State-Augmentation, SA）的分布式优化方法，旨在最大化网络源节点的信息处理能力。该问题建模使用图神经网络（GNN），执行基于网络节点之间拓扑连接关系的图卷积操作。通过无监督学习框架从GNN架构中提取路由策略，使源节点能够为各种流做出最优决策。数值实验表明，在训练参数化的GNN模型时，所提出的方法表现出色，并且比基线算法具有更佳的表现。此外，将该方法应用于现实网络拓扑结构和无线自组织网路测试平台验证了其有效性，突出了图神经网络的稳健性和可转移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of packet-based information routing inlarge-scale wireless communication networks. The problem is framed as aconstrained statistical learning task, where each network node operates usingonly local information. Opportunistic routing exploits the broadcast nature ofwireless communication to dynamically select optimal forwarding nodes, enablingthe information to reach the destination through multiple relay nodessimultaneously. To solve this, we propose a State-Augmentation (SA) baseddistributed optimization approach aimed at maximizing the total informationhandled by the source nodes in the network. The problem formulation leveragesGraph Neural Networks (GNNs), which perform graph convolutions based on thetopological connections between network nodes. Using an unsupervised learningparadigm, we extract routing policies from the GNN architecture, enablingoptimal decisions for source nodes across various flows. Numerical experimentsdemonstrate that the proposed method achieves superior performance whentraining a GNN-parameterized model, particularly when compared to baselinealgorithms. Additionally, applying the method to real-world network topologiesand wireless ad-hoc network test beds validates its effectiveness, highlightingthe robustness and transferability of GNNs.</description>
      <author>example@mail.com (Sourajit Das, Navid NaderiAlizadeh, Rahul Mangharam, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2503.03736v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HyperGCT: A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration</title>
      <link>http://arxiv.org/abs/2503.02195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的方法HyperGCT，用于在3D点云配准问题中动态优化超图来生成几何约束。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常通过构造一致性图来建模无序的特征匹配，并从中采样一致性的匹配以生成假设。然而，在构建这些图时引入了噪声，这给手工设计的几何约束带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从动态超图中挖掘稳健的几何约束的方法，从而改进3D配准过程。&lt;h4&gt;方法&lt;/h4&gt;HyperGCT通过顶点和边特征聚合的方式动态优化超图，利用高阶一致性来捕捉对应关系之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有技术相比，HyperGCT在多个数据集上表现出了最先进的性能，并且对图形噪声具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这种方法不仅提高了3D配准的准确性，还增强了其泛化能力，证明了从动态超图中挖掘几何约束的有效性和重要性。&lt;h4&gt;翻译&lt;/h4&gt;几何约束对于解决基于特征匹配的3D点云注册问题至关重要。现有的方法通常将无序的匹配建模为一致性图，并从中采样一致性的匹配以生成假设。然而，显式的图形构建引入了噪声，这给手工设计的几何约束带来了挑战，使其难以确保匹配之间的协调性。为了克服这一限制，我们提出了一种新的方法HyperGCT，它利用动态超图中3D对应关系间的高阶一致性来学习灵活且动态的几何约束。据我们所知，这是第一个从动态超图中挖掘稳健几何约束的方法以用于三维注册任务。通过动态优化超图并聚合顶点和边特征，HyperGCT有效地捕获了对应之间的相关性，并生成准确的假设。在3DMatch、3DLoMatch、KITTI-LC以及ETH数据集上的广泛实验表明HyperGCT达到了最先进的性能。此外，我们的方法对图形噪声具有鲁棒性，在推广方面显示出显著优势。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric constraints between feature matches are critical in 3D point cloudregistration problems. Existing approaches typically model unordered matches asa consistency graph and sample consistent matches to generate hypotheses.However, explicit graph construction introduces noise, posing great challengesfor handcrafted geometric constraints to render consistency among matches. Toovercome this, we propose HyperGCT, a flexible dynamic Hyper-GNN-learnedgeometric constraint that leverages high-order consistency among 3Dcorrespondences. To our knowledge, HyperGCT is the first method that minesrobust geometric constraints from dynamic hypergraphs for 3D registration. Bydynamically optimizing the hypergraph through vertex and edge featureaggregation, HyperGCT effectively captures the correlations amongcorrespondences, leading to accurate hypothesis generation. Extensiveexperiments on 3DMatch, 3DLoMatch, KITTI-LC, and ETH show that HyperGCTachieves state-of-the-art performance. Furthermore, our method is robust tograph noise, demonstrating a significant advantage in terms of generalization.The code will be released.</description>
      <author>example@mail.com (Xiyu Zhang, Jiayi Ma, Jianwei Guo, Wei Hu, Zhaoshuai Qi, Fei Hui, Jiaqi Yang, Yanning Zhang)</author>
      <guid isPermaLink="false">2503.02195v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.03562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为Physics Anomaly Detection (Phys-AD)的新型大规模数据集，用于工业异常检测。该数据集基于真实的机器人臂和电机操作收集，包含多种动态且语义丰富的场景，以及不同类型的物理异常。&lt;h4&gt;背景&lt;/h4&gt;现有的工业异常检测算法主要在静态、语义简单的数据集上开发和测试，与现实世界中需要物理理解和推理的情况存在差距。&lt;h4&gt;目的&lt;/h4&gt;通过引入Phys-AD数据集来填补现有技术与实际需求之间的鸿沟，推动机器自主地像人类一样基于条件物体的物理知识进行感知、交互和推理的能力发展。&lt;h4&gt;方法&lt;/h4&gt;该研究包括了超过6400个视频片段，覆盖22种真实世界对象类别，并展示了机器人臂与电机互动时产生的47种异常类型。此外，还提出了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言基础模型在检测和解释物理原因方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有的无监督、弱监督以及视频理解方法在处理基于物理学的异常上存在局限性。&lt;h4&gt;结论&lt;/h4&gt;通过公开提供的数据集和基准测试，为研究者提供了新的机会来改进工业环境中的物体异常检测技术，并且这些资源将有助于推动物理理解和视觉推理的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;人类通过感知、互动以及基于条件物体的物理知识进行推理来识别现实世界中的对象异常。工业异常检测（IAD）的长期目标是使机器能够自主地复制这种技能。然而，当前大多数IAD算法是在静态且语义简单的数据集上开发和测试的，这与需要物理理解和推理的真实场景相去甚远。为了解决这一问题，我们引入了Physics Anomaly Detection (Phys-AD) 数据集，这是首个大规模、基于真实世界的工业异常检测视频数据集，并采用物理学基础设计。使用真实的机器人臂和电机收集的数据包含了6400多个动态且语义丰富的场景及22种对象类别与机器人手臂和电机互动的视频片段，并展示了47种物理异常类型。在Phys-AD中，进行异常检测需要视觉推理，结合物理知识和视频内容来确定物体是否处于异常状态。我们对最先进的异常检测方法进行了基准测试，包括无监督、弱监督以及基于理解视频的方法，在处理物理学基础引发的异常方面显示出了局限性。此外，还引入了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言模型在提供准确物理原因解释的能力上。我们的数据集和基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans detect real-world object anomalies by perceiving, interacting, andreasoning based on object-conditioned physical knowledge. The long-term goal ofIndustrial Anomaly Detection (IAD) is to enable machines to autonomouslyreplicate this skill. However, current IAD algorithms are largely developed andtested on static, semantically simple datasets, which diverge from real-worldscenarios where physical understanding and reasoning are essential.To bridgethis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, thefirst large-scale, real-world, physics-grounded video dataset for industrialanomaly detection. Collected using a real robot arm and motor, Phys-AD providesa diverse set of dynamic, semantically rich scenarios. The dataset includesmore than 6400 videos across 22 real-world object categories, interacting withrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection inPhys-AD requires visual reasoning, combining both physical knowledge and videocontent to determine object abnormality.We benchmark state-of-the-art anomalydetection methods under three settings: unsupervised AD, weakly-supervised AD,and video-understanding AD, highlighting their limitations in handlingphysics-grounded anomalies. Additionally, we introduce the Physics AnomalyExplanation (PAEval) metric, designed to assess the ability of visual-languagefoundation models to not only detect anomalies but also provide accurateexplanations for their underlying physical causes. Our dataset and benchmarkwill be publicly available.</description>
      <author>example@mail.com (Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu)</author>
      <guid isPermaLink="false">2503.03562v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning</title>
      <link>http://arxiv.org/abs/2503.03747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PacketCLIP，一种结合包数据和自然语言语义的多模态框架，通过对比预训练和层次图神经网络（GNN）推理来增强加密流量分类和网络安全。&lt;h4&gt;背景&lt;/h4&gt;流量分类对网络安全至关重要，但加密流量带来了重大挑战。现有的解决方案难以同时满足准确性和解释性的需求，尤其是在资源受限环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效且可解释地检测加密流量中异常的系统，以应对网络安全面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;PacketCLIP结合了包数据和自然语言语义，并通过对比预训练以及层次图神经网络（GNN）推理来实现这一目标。该框架旨在将文本描述与包行为相匹配，提高模型的解释性、可扩展性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;PacketCLIP在加密流量分类中表现出色，实现了95%平均AUC评分，并且比基线方法提高了11.6%，同时减少了模型大小达92%，这对于实时异常检测尤为重要。&lt;h4&gt;结论&lt;/h4&gt;通过将高级机器学习技术与实际网络安全需求相结合，PacketCLIP为解决资源受限环境中的加密流量分类和网络入侵检测挑战提供了一个可扩展、高效且解释性强的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经直接以中文形式给出，无需进一步翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic classification is vital for cybersecurity, yet encrypted trafficposes significant challenges. We present PacketCLIP, a multi-modal frameworkcombining packet data with natural language semantics through contrastivepretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIPintegrates semantic reasoning with efficient classification, enabling robustdetection of anomalies in encrypted network flows. By aligning textualdescriptions with packet behaviors, it offers enhanced interpretability,scalability, and practical applicability across diverse security scenarios.PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reducesmodel size by 92%, making it ideal for real-time anomaly detection. By bridgingadvanced machine learning techniques and practical cybersecurity needs,PacketCLIP provides a foundation for scalable, efficient, and interpretablesolutions to tackle encrypted traffic classification and network intrusiondetection challenges in resource-constrained environments.</description>
      <author>example@mail.com (Ryozo Masukawa, Sanggeon Yun, Sungheon Jeong, Wenjun Huang, Yang Ni, Ian Bryant, Nathaniel D. Bastian, Mohsen Imani)</author>
      <guid isPermaLink="false">2503.03747v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case</title>
      <link>http://arxiv.org/abs/2503.03548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个评估自动驾驶系统中三维物体检测性能的方法，专注于传感器表现限制和基于深度学习的物体检测不足对预期功能的影响。&lt;h4&gt;背景&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。&lt;h4&gt;目的&lt;/h4&gt;该论文的主要目的是定义和建模一个与SOTIF相关的使用案例，并生成用于应用3D对象检测方法的激光雷达点云数据集。&lt;h4&gt;方法&lt;/h4&gt;通过模拟21种不同的天气条件下的SOTIF相关用例，创建了一个包含547个帧的数据集，其中包括晴天、多云和雨天的各种情况，对应于一天中的不同时间。使用MMDetection3D和OpenPCDET工具包对最先进的（SOTA）3D对象检测方法的性能进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过在生成的数据集上测试预先训练好的深度学习模型，并使用平均精度（AP）和召回率指标进行比较，论文展示了不同天气条件下的性能差异。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了在各种环境条件下对3D物体检测算法进行全面评估的重要性。所提出的框架为开发更安全的自动驾驶系统提供了重要的视角和支持。&lt;h4&gt;翻译&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。该论文提出了一个评估三维物体检测适应性和性能的方法，通过对模拟的与SOTIF相关的用例生成的数据集进行3D物体检测算法的应用来实现这一目的。主要贡献包括定义和建模21种不同天气条件下的SOTIF相关使用案例，并为应用3D物体检测方法创建了适合于激光雷达点云数据集。该数据集包含547帧，涵盖晴天、多云和雨天的不同情况，对应一天中的各个时段。通过MMDetection3D和OpenPCDET工具包，在生成的数据集上使用平均精度（AP）和召回率指标对预先训练好的深度学习模型进行测试并评估最先进的3D物体检测方法的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012707300003702&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety of the Intended Functionality (SOTIF) addresses sensor performancelimitations and deep learning-based object detection insufficiencies to ensurethe intended functionality of Automated Driving Systems (ADS). This paperpresents a methodology examining the adaptability and performance evaluation ofthe 3D object detection methods on a LiDAR point cloud dataset generated bysimulating a SOTIF-related Use Case. The major contributions of this paperinclude defining and modelling a SOTIF-related Use Case with 21 diverse weatherconditions and generating a LiDAR point cloud dataset suitable for applicationof 3D object detection methods. The dataset consists of 547 frames,encompassing clear, cloudy, rainy weather conditions, corresponding todifferent times of the day, including noon, sunset, and night. EmployingMMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art(SOTA) 3D object detection methods is evaluated and compared by testing thepre-trained Deep Learning (DL) models on the generated dataset using AveragePrecision (AP) and Recall metrics.</description>
      <author>example@mail.com (Milin Patel, Rolf Jung)</author>
      <guid isPermaLink="false">2503.03548v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence</title>
      <link>http://arxiv.org/abs/2503.00518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、可解释机器学习方法。该方法结合了动态图卷积神经网络(DGCNN)和语义分割技术，对3D LiDAR点云进行有意义的分段，并通过聚类技术进一步优化。&lt;h4&gt;背景&lt;/h4&gt;飞机产生的强空气湍流（航空涡流）给航空安全带来了重大风险，需要准确可靠的检测方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用LiDAR数据检测航空涡流的有效且可靠的方法，提高航空安全性。&lt;h4&gt;方法&lt;/h4&gt;采用动态图卷积神经网络(DGCNN)和语义分割技术对3D LiDAR点云进行分段，并通过聚类技术优化结果。引入基于扰动的解释技术增强模型决策过程透明度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在测量和模拟LiDAR扫描数据上的表现优于四种基准方法，证明了其有效性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;结合语义分割和聚类技术用于实时航空涡流跟踪的方法显著提升了航空安全措施的有效性与可解释性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、透明机器学习方法。该研究通过动态图卷积神经网络（DGCNN）和语义分割技术对3D LiDAR点云进行有意义的分段，并引入基于扰动的技术来解释模型决策过程，从而提高了安全性和信任度。实验结果表明了这种方法的有效性和可靠性，为实时航空涡流跟踪提供了一种先进的方法，同时提升了其透明度和可理解性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wake vortices - strong, coherent air turbulences created by aircraft - pose asignificant risk to aviation safety and therefore require accurate and reliabledetection methods. In this paper, we present an advanced, explainable machinelearning method that utilizes Light Detection and Ranging (LiDAR) data foreffective wake vortex detection. Our method leverages a dynamic graph CNN(DGCNN) with semantic segmentation to partition a 3D LiDAR point cloud intomeaningful segments. Further refinement is achieved through clusteringtechniques. A novel feature of our research is the use of a perturbation-basedexplanation technique, which clarifies the model's decision-making processesfor air traffic regulators and controllers, increasing transparency andbuilding trust. Our experimental results, based on measured and simulated LiDARscans compared against four baseline methods, underscore the effectiveness andreliability of our approach. This combination of semantic segmentation andclustering for real-time wake vortex tracking significantly advances aviationsafety measures, ensuring that these are both effective and comprehensible.</description>
      <author>example@mail.com (Zhan Qu, Shuzhou Yuan, Michael Färber, Marius Brennfleck, Niklas Wartha, Anton Stephan)</author>
      <guid isPermaLink="false">2503.00518v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>AugFL: Augmenting Federated Learning with Pretrained Models</title>
      <link>http://arxiv.org/abs/2503.02154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to be published in Transactions on Networking&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何通过利用预训练模型（PM）来增强联邦学习（FL），以解决分布式环境中由于隐私政策或存储限制而导致的训练数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，联邦学习因其能够在保护用户隐私的同时进行大规模机器学习而引起了广泛关注。然而，在实际应用中，受限于严格的隐私规定和设备有限的存储能力，缺乏足够的训练数据常常阻碍了其有效部署。&lt;h4&gt;目的&lt;/h4&gt;通过引入预训练模型来增强FL系统的能力，以降低从头开始执行联邦学习所需的数据量，并提高模型在分布式环境中的适应性和性能。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个基于正则化的元学习框架，在这个框架中，客户端协作学习一个从服务器存储的私有预训练模型中迁移知识得到的元模型。此外，开发了一种基于不精确ADMM算法的优化方案（AugFL），以在不暴露预训练模型且不增加本地计算成本的情况下解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明了所提出方法的通信复杂性、适应性能以及一般非凸情况下的知识迁移收益。实验结果进一步证实了AugFL相较于现有基线的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种新颖的方法来增强联邦学习系统，特别是在数据稀缺的情况下通过利用预训练模型的知识转移能力显著提高了学习效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经完全翻译为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) has garnered widespread interest in recent years.However, owing to strict privacy policies or limited storage capacities oftraining participants such as IoT devices, its effective deployment is oftenimpeded by the scarcity of training data in practical decentralized learningenvironments. In this paper, we study enhancing FL with the aid of (large)pre-trained models (PMs), that encapsulate wealthy general/domain-agnosticknowledge, to alleviate the data requirement in conducting FL from scratch.Specifically, we consider a networked FL system formed by a central server anddistributed clients. First, we formulate the PM-aided personalized FL as aregularization-based federated meta-learning problem, where clients join forcesto learn a meta-model with knowledge transferred from a private PM stored atthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, tooptimize the problem with no need to expose the PM or incur additionalcomputational costs to local clients. Further, we establish theoreticalguarantees for AugFL in terms of communication complexity, adaptationperformance, and the benefit of knowledge transfer in general non-convex cases.Extensive experiments corroborate the efficacy and superiority of AugFL overexisting baselines.</description>
      <author>example@mail.com (Sheng Yue, Zerui Qin, Yongheng Deng, Ju Ren, Yaoxue Zhang, Junshan Zhang)</author>
      <guid isPermaLink="false">2503.02154v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance Detection</title>
      <link>http://arxiv.org/abs/2503.03172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, published in The Sixteenth International  Conference on Information (eKNOW 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于社交媒体立场检测（SD）的新方法，利用讽刺识别作为中间任务的迁移学习来提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的立场检测由于其在社会商业和政治应用中的重要性而成为自然语言处理领域的一个研究热点。然而，在线平台文本的微妙性和复杂性对SD算法提出了挑战，特别是当包含讽刺或比喻的语言时。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入讽刺识别中间任务来改进现有的SD模型，以提高其准确率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该方法包括微调BERT和RoBERTa，并将卷积BiLSTM与密集层连接。此外，还进行了详尽的实验测试，以评估迁移学习框架的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，集成讽刺识别知识到模型中有助于减少对讽刺文本元素的误分类，从而提高SD任务中的准确率和F1分数。特别是在85%的情况下，该模型能够正确预测之前没有进行讽刺预训练时被错误分类的文本。&lt;h4&gt;结论&lt;/h4&gt;这项研究是首次将讽刺检测作为中间迁移学习任务应用于SD的研究，并且通过结合BERT或RoBERTa与其他深度学习技术证明了其有效性。这为未来在这一领域的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已全部翻译成中文并进行了总结，涵盖了论文的主要贡献和发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stance Detection (SD) on social media has emerged as a prominent area ofinterest with implications for social business and political applicationsthereby garnering escalating research attention within NLP. The inherentsubtlety and complexity of texts procured from online platforms pose challengesfor SD algorithms in accurately discerning the authors stance. Mostly theinclusion of sarcastic and figurative language drastically impacts theperformance of SD models. This paper addresses this by employing sarcasmdetection intermediate-task transfer learning tailored for SD. The proposedmethodology involves the finetuning of BERT and RoBERTa and the concatenationof convolutional BiLSTM and dense layers. Rigorous experiments are conducted onpublicly available datasets to evaluate our transfer-learning framework. Theperformance of the approach is assessed against various State-Of-The-Artbaselines for SD providing empirical evidence of its effectiveness. Notably ourmodel outperforms the best SOTA models even prior to sarcasm-detectionpretraining. The integration of sarcasm knowledge into the model provesinstrumental in mitigating misclassifications of sarcastic textual elements inSD. Our model accurately predicts 85% of texts that were previouslymisclassified by the model without sarcasm-detection pretraining therebyamplifying the average F1-score of the model. Our experiments also revealedthat the success of the transfer-learning framework is contingent upon thecorrelation of lexical attributes between the intermediate task and the targettask. This study represents the first exploration of sarcasm detection as anintermediate transfer-learning task in the context of SD and simultaneouslyuses the concatenation of BERT or RoBERTa with other deep-learning techniquesestablishing the proposed approach as a foundational baseline for futureresearch endeavors in this domain.</description>
      <author>example@mail.com (Gibson Nkhata, Susan Gauch)</author>
      <guid isPermaLink="false">2503.03172v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>YARE-GAN: Yet Another Resting State EEG-GAN</title>
      <link>http://arxiv.org/abs/2503.02636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '研究提出了一种基于Wasserstein GAN（WGANGP）的方法，用于生成多通道静息态EEG数据，并通过视觉和特征基评估来衡量合成信号的质量。', '背景': '尽管生成对抗网络(GANs)在合成逼真的神经数据方面显示出潜力，但它们在无监督表示学习中的应用特别是在休息状态的脑电图(EEG)领域尚未得到充分探索。', '目的': '实施并评估一种Wasserstein GAN（WGANGP）方法来生成多通道静息态EEG数据，并研究其作为无监督特征提取器的能力。', '方法': '使用WGAN-GP模型生成多通道休息状态的EEG信号，通过视觉和基于特征的方法评价合成信号的质量。同时，训练模型以评估其对年龄分类任务中的表示学习能力。', '主要发现': '模型成功捕获了真实EEG数据的统计学和谱特性；然而，在前额区域复制高频振荡方面存在挑战；此外，生成器的判别器可以用于年龄组分类，表现出超出随机标签基线的准确率。', '结论': '研究结果表明，生成式模型不仅能够作为高质量EEG数据的合成工具，还能作为一种无监督特征提取的方法，减少手动特征工程的需求。这为基于GAN的无监督学习在EEG分析中的应用提供了新的可能性，并促进了更高效的数据驱动神经科学研究方法的发展。', '翻译': '摘要是关于利用Wasserstein GAN（WGANGP）生成真实多通道静息状态EEG数据的研究，该研究通过视觉和特征基评估来衡量合成信号的质量。它揭示了GAN在无监督表示学习中的潜力，并表明它们不仅可以生成高质量的EEG数据，还可以作为一种有效的无监督特征提取方法用于年龄组分类等任务。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Adversarial Networks (GANs) have shown promise in synthesisingrealistic neural data, yet their potential for unsupervised representationlearning in resting-state EEG remains under explored. In this study, weimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generatemulti-channel resting-state EEG data and assess the quality of the synthesisedsignals through both visual and feature-based evaluations. Our results indicatethat the model effectively captures the statistical and spectralcharacteristics of real EEG data, although challenges remain in replicatinghigh-frequency oscillations in the frontal region. Additionally, we demonstratethat the Critic's learned representations can be fine-tuned for age groupclassification, achieving an out-of-sample accuracy, significantly better thana shuffled-label baseline. These findings suggest that generative models canserve not only as EEG data generators but also as unsupervised featureextractors, reducing the need for manual feature engineering. This studyhighlights the potential of GAN-based unsupervised learning for EEG analysis,suggesting avenues for more data-efficient deep learning applications inneuroscience.</description>
      <author>example@mail.com (Yeganeh Farahzadi, Morteza Ansarinia, Zoltan Kekecs)</author>
      <guid isPermaLink="false">2503.02636v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology</title>
      <link>http://arxiv.org/abs/2503.03485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过扩大预训练数据集和利用大规模生物注释来改进单细胞基础模型，以提高疾病生物学中的下游应用性能。&lt;h4&gt;背景&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索改进当前最先进的方法，通过扩大预训练数据集和利用大规模生物注释来改善单细胞基础模型的表现。&lt;h4&gt;方法&lt;/h4&gt;第一，将预训练的数据集扩展到1亿1600万个细胞；第二，利用大规模的生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。&lt;h4&gt;主要发现&lt;/h4&gt;随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;h4&gt;结论&lt;/h4&gt;扩大预训练的数据集以及利用大规模生物注释可以有效地提升单细胞基础模型的性能，为疾病生物学提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。随着单细胞RNA测序数据的日益增多，大型基础模型的发展成为可能。然而，现有的基础模型要么没有改进下游应用性能，要么只是适度地改善了它们。本研究探索了两种提高当前最佳实践的方法：扩大预训练的数据集到1亿1600万个细胞，并利用大规模生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。在两个下游评估任务上验证了这些模型：识别未见受试者背后的疾病状态以及区分健康细胞与患病细胞（这两种情况均未出现在训练中）。研究表明随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the biological mechanism of disease is critical for medicine,and in particular drug discovery. AI-powered analysis of genome-scalebiological data hold great potential in this regard. The increasingavailability of single-cell RNA sequencing data has enabled the development oflarge foundation models for disease biology. However, existing foundationmodels either do not improve or only modestly improve over task-specific modelsin downstream applications. Here, we explored two avenues for improving thestate-of-the-art. First, we scaled the pre-training dataset to 116 millioncells, which is larger than those used by previous models. Second, we leveragedthe availability of large-scale biological annotations as a form of supervisionduring pre-training. We trained the TEDDY family of models comprising sixtransformer-based state-of-the-art single-cell foundation models with 70million, 160 million, and 400 million parameters. We vetted our models on twodownstream evaluation tasks -- identifying the underlying disease state ofheld-out donors not seen during training and distinguishing healthy cells fromdiseased ones for disease conditions and donors not seen during training.Scaling experiments showed that performance improved predictably with both datavolume and parameter count. Our models showed substantial improvement overexisting work on the first task and more muted improvements on the second.</description>
      <author>example@mail.com (Alexis Chevalier, Soumya Ghosh, Urvi Awasthi, James Watkins, Julia Bieniewska, Nichita Mitrea, Olga Kotova, Kirill Shkura, Andrew Noble, Michael Steinbaugh, Julien Delile, Christoph Meier, Leonid Zhukov, Iya Khalil, Srayanta Mukherjee, Judith Mueller)</author>
      <guid isPermaLink="false">2503.03485v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation</title>
      <link>http://arxiv.org/abs/2503.03280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Proceedings of the 20th International Joint Conference on Computer  Vision, Imaging and Computer Graphics Theory and Applications (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对自动驾驶车辆中鸟类视角下的动态物体运动理解的挑战，论文提出了BEVMOSNet系统，该系统结合了相机、激光雷达和雷达数据进行多模态融合，并在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;当前关于场景内动态对象在鸟类视角下（BEV）的准确运动理解的研究相对较少。尽管有一些基于视觉的方法提出了初步结果，但在低光、夜间以及雨等恶劣天气条件下这些方法的表现严重下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效融合相机、激光雷达和雷达数据的端到端多模态系统，用于精准预测鸟类视角下移动物体的位置与运动状态。&lt;h4&gt;方法&lt;/h4&gt;引入了BEVMOSNet框架，这是首个利用相机、激光雷达以及雷达数据进行端到端多模态融合的方法。研究重点在于探索如何通过可变形交叉注意力机制指导传感器之间的信息共享优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上，与仅基于视觉的单模式基线BEV-MoSeg相比，BEVMOSNet实现了36.59%的整体IoU得分提升；与多模态SimpleBEV相比，提升了2.35%，确立了其在BEV运动分割领域的前沿地位。&lt;h4&gt;结论&lt;/h4&gt;通过综合利用不同类型的传感器数据，可以显著提高动态物体识别和跟踪的准确性，在复杂环境下的性能尤为突出。这为未来自动驾驶车辆中更可靠的安全保障和路径规划技术开发提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;对摘要内容进行了中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion understanding of the dynamic objects within the scene inbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidancesystem and smooth path planning for autonomous vehicles. However, this task hasreceived relatively limited exploration when compared to object detection andsegmentation with only a few recent vision-based approaches presentingpreliminary findings that significantly deteriorate in low-light, nighttime,and adverse weather conditions such as rain. Conversely, LiDAR and radarsensors remain almost unaffected in these scenarios, and radar provides keyvelocity information of the objects. Therefore, we introduce BEVMOSNet, to ourknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,and radar to precisely predict the moving objects in BEV. In addition, weperform a deeper analysis to find out the optimal strategy for deformablecross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.While evaluating BEVMOSNet on the nuScenes dataset, we show an overallimprovement in IoU score of 36.59% compared to the vision-based unimodalbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to themultimodel SimpleBEV (Harley et al., 2022), extended for the motionsegmentation task, establishing this method as the state-of-the-art in BEVmotion segmentation.</description>
      <author>example@mail.com (Hiep Truong Cong, Ajay Kumar Sigatapu, Arindam Das, Yashwanth Sharma, Venkatesh Satagopan, Ganesh Sistu, Ciaran Eising)</author>
      <guid isPermaLink="false">2503.03280v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2503.00972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云配准在计算机辅助干预中非常重要。尽管已经开发了基于学习的点云配准方法，但由于泛化性和可解释性的问题，这些方法难以应用于临床。因此，传统的点云配准方法（如迭代最近点算法ICP）仍然广泛用于CAI。然而，ICP方法未能考虑以下两个方面：1. 点具有明确的语义含义，每个点可以与特定解剖标签相关联；2. 变形需要遵循生物力学能量约束。&lt;h4&gt;背景&lt;/h4&gt;学习基于的方法在泛化性和解释性上存在挑战，因此传统的迭代最近点算法ICP仍广泛应用于计算机辅助干预中。然而，现有方法忽视了点的语义信息和变形的能量约束要求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的语义ICP（sem-ICP）方法，该方法处理多点标签并使用线性弹性能量正则化来提高配准效果。&lt;h4&gt;方法&lt;/h4&gt;利用语义标签改进最近邻匹配的鲁棒性，并引入了一种新型点云变形表示形式以应用明确的生物力学能量正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中，与现有的ICP基线方法相比，提高了Hausdorff距离。同时进行的灵敏度研究表明，刚性初始化能够更好地适应不同的初始位置和可见比。&lt;h4&gt;结论&lt;/h4&gt;通过结合点云中的语义信息以及生物力学能量约束，提出的sem-ICP算法改进了现有ICP算法在配准任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是计算机辅助干预（CAI）中的一项重要技术。尽管基于学习的方法已经开发出来，但由于泛化性和解释性的问题，这些方法难以应用于临床实践中。因此，在CAI领域内，传统的迭代最近点（ICP）算法仍然被广泛使用。然而，现有的ICP算法未能充分考虑以下两点：1. 每个点具有明确的语义含义，并且可以与特定解剖学标签相关联；2. 变形需要遵循生物力学能量约束。本文提出了一种新的方法——语义迭代最近点（sem-ICP），该方法能够处理多点标签，同时采用线性弹性能量正则化来增强配准效果。通过使用语义信息改进了最近邻匹配的鲁棒性，并引入了一种全新的点云变形表示形式以便应用明确的生物力学能量正则化。我们的实验在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中进行了测试，结果显示相较于现有的ICP基线方法，改进后的Hausdorff距离表现更优。此外，我们还进行了一项敏感性研究，证明了刚性初始化在不同初始位置和可见比的情况下具有更好的收敛性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is important in computer-aided interventions (CAI).While learning-based point cloud registration methods have been developed,their clinical application is hampered by issues of generalizability andexplainability. Therefore, classical point cloud registration methods, such asIterative Closest Point (ICP), are still widely applied in CAI. ICP methodsfail to consider that: (1) the points have well-defined semantic meaning, inthat each point can be related to a specific anatomical label; (2) thedeformation needs to follow biomechanical energy constraints. In this paper, wepresent a novel semantic ICP (sem-ICP) method that handles multiple pointlabels and uses linear elastic energy regularization. We use semantic labels toimprove the robustness of the closest point matching and propose a new pointcloud deformation representation to apply explicit biomechanical energyregularization. Our experiments on the Learn2reg abdominal MR-CT registrationdataset and a trans-oral robotic surgery ultrasound-CT registration datasetshow that our method improves the Hausdorff distance compared with otherstate-of-the-art ICP-based registration methods. We also perform a sensitivitystudy to show that our rigid initialization achieves better convergence withdifferent initializations and visible ratios.</description>
      <author>example@mail.com (Wanwen Chen, Carson Studders, Jamie J. Y. Kwon, Emily H. T. Pang, Eitan Prisman, Septimiu E. Salcudean)</author>
      <guid isPermaLink="false">2503.00972v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2503.03313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对带有文本描述的节点（Text-Attributed Graphs，TAGs）在实际场景中的普遍存在及其独特的结构和领域特定知识的需求，提出了一个通用图基础模型（Graph Foundation Model, GFM），旨在跨多种图类型和任务中进行泛化。&lt;h4&gt;背景&lt;/h4&gt;现有的方法倾向于将大语言模型（Large Language Models，LLMs）与图神经网络（Graph Neural Networks，GNNs）分阶段地结合在一起处理TAGs，这种解耦架构限制了它们之间的协同潜力。此外，现有方法在处理图节点时使用了不兼容任务导向模板的词表外词汇分配策略。&lt;h4&gt;目的&lt;/h4&gt;为解决上述问题，提出了PromptGFM模型，该模型旨在通过图词汇学习实现通用图基础模型的设计。&lt;h4&gt;方法&lt;/h4&gt;PromptGFM包含两个主要组件：(1) 图理解模块，此模块使LLMs能够复制GNN的工作流程，并促进无缝的GNN-LLM融合和优雅的图文本对齐；(2) 图推理模块，该模块建立了一种基于语言的图词汇，确保表达性、可移植性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了PromptGFM在多种图类型和任务上的优越性和迁移能力。&lt;h4&gt;结论&lt;/h4&gt;PromptGFM提供了处理带有文本描述节点图形的有效方法，并证明其具有良好的泛化能力和跨领域适应性。&lt;h4&gt;翻译&lt;/h4&gt;Text-Attributed Graphs (TAGs)，即每个节点都关联有文本描述的图，在现实场景中普遍存在。它们通常展现出独特的结构和特定领域的知识，促使开发一种能够跨越不同类型的图和任务的一般图基础模型（Graph Foundation Model, GFM）。尽管在整合大型语言模型（Large Language Models, LLMs）与图神经网络（GNNs）处理TAG方面付出了巨大努力，现有的方法由于采用了两阶段对齐的解耦架构而受到限制。更糟糕的是，现有方法将词汇外词分配给图节点，导致了特定于图的语义、标记爆炸以及与任务导向提示模板不兼容的问题，这阻碍了跨图和跨任务迁移能力。为解决这些挑战，我们提出了一种基于图词汇学习的基础通用图模型PromptGFM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-Attributed Graphs (TAGs), where each node is associated with textdescriptions, are ubiquitous in real-world scenarios. They typically exhibitdistinctive structure and domain-specific knowledge, motivating the developmentof a Graph Foundation Model (GFM) that generalizes across diverse graphs andtasks. Despite large efforts to integrate Large Language Models (LLMs) andGraph Neural Networks (GNNs) for TAGs, existing approaches suffer fromdecoupled architectures with two-stage alignment, limiting their synergisticpotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokensto graph nodes, leading to graph-specific semantics, token explosion, andincompatibility with task-oriented prompt templates, which hinders cross-graphand cross-task transferability. To address these challenges, we proposePromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.PromptGFM comprises two key components: (1) Graph Understanding Module, whichexplicitly prompts LLMs to replicate the finest GNN workflow within the textspace, facilitating seamless GNN-LLM integration and elegant graph-textalignment; (2) Graph Inference Module, which establishes a language-based graphvocabulary ensuring expressiveness, transferability, and scalability, enablingreadable instructions for LLM fine-tuning. Extensive experiments demonstrateour superiority and transferability across diverse graphs and tasks. The codeis available at this: https://github.com/agiresearch/PromptGFM.</description>
      <author>example@mail.com (Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang)</author>
      <guid isPermaLink="false">2503.03313v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?</title>
      <link>http://arxiv.org/abs/2503.02687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了针对雷达点云的新方法CAPMix。&lt;h4&gt;背景&lt;/h4&gt;由于3D感知任务中数据收集和标注的工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的方法。然而，这些方法大多数都是针对激光雷达数据设计的，对雷达点云的应用研究较少。&lt;h4&gt;目的&lt;/h4&gt;探讨将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并识别其中存在的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了Class-Aware PillarMix (CAPMix) 方法，该方法在3D点云中的柱状体级别应用了基于类别标签指导的混合操作。此方法根据每个柱状体独立分配一个比例，从而增加样本多样性。为了考虑不同类别的密度，使用特定于类别的分布：对于稠密对象（如大型车辆），倾向于从另一样本中采样更多点；而对于稀疏对象（如行人），则更注重原始样本中的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一种针对雷达点云的新型混合样本数据增强方法，通过这种方式可以生成更多样化的训练数据，并认为此简洁而有效的方法将促进对雷达数据增强技术的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;由于3D感知任务中的数据收集和标注工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的技术。然而，这些方法大多针对激光雷达数据设计，对于雷达点云的应用研究较少。本文探讨了现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了一种名为Class-Aware PillarMix (CAPMix) 的新方法来解决识别到的问题。该方法在3D点云中的柱状体级别应用基于类别标签指导的混合操作，通过独立分配比例提升样本多样性，并根据不同类别的密度使用特定于类别的分布策略进行调整。实验结果表明，这种方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the significant effort required for data collection and annotation in3D perception tasks, mixed sample data augmentation (MSDA) has been widelystudied to generate diverse training samples by mixing existing data. Recently,many MSDA techniques have been developed for point clouds, but they mainlytarget LiDAR data, leaving their application to radar point clouds largelyunexplored. In this paper, we examine the feasibility of applying existing MSDAmethods to radar point clouds and identify several challenges in adapting thesetechniques. These obstacles stem from the radar's irregular angulardistribution, deviations from a single-sensor polar layout in multi-radarsetups, and point sparsity. To address these issues, we propose Class-AwarePillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillarlevel in 3D point clouds, guided by class labels. Unlike methods that rely asingle mix ratio to the entire sample, CAPMix assigns an independent ratio toeach pillar, boosting sample diversity. To account for the density of differentclasses, we use class-specific distributions: for dense objects (e.g., largevehicles), we skew ratios to favor points from another sample, while for sparseobjects (e.g., pedestrians), we sample more points from the original. Thisclass-aware mixing retains critical details and enriches each sample with newinformation, ultimately generating more diverse training data. Experimentalresults demonstrate that our method not only significantly boosts performancebut also outperforms existing MSDA approaches across two datasets (Bosch Streetand K-Radar). We believe that this straightforward yet effective approach willspark further investigation into MSDA techniques for radar data.</description>
      <author>example@mail.com (Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang)</author>
      <guid isPermaLink="false">2503.02687v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection</title>
      <link>http://arxiv.org/abs/2503.03107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by AAAI'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着互联网的快速发展，信息传播范式发生了变化，并且效率大大提高。然而，这也带来了假新闻的快速传播并导致了网络空间中的负面影响。&lt;h4&gt;背景&lt;/h4&gt;当前，信息展示格式逐渐演进，新闻形式从文本向多模态内容转变。因此，检测多模态假新闻已成为研究热点之一。&lt;h4&gt;目的&lt;/h4&gt;针对现有的多模态假新闻检测领域存在的两个主要挑战：无法充分有效地利用多模态信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题，提出一种增强型可靠外部信息多模态对比学习框架（ERIC-FND）。&lt;h4&gt;方法&lt;/h4&gt;该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习，使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。&lt;h4&gt;主要发现&lt;/h4&gt;在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;h4&gt;结论&lt;/h4&gt;通过上述研究展示了ERIC-FND框架的有效性，并为未来多模态假新闻检测的研究提供了新的思路和方向。&lt;h4&gt;翻译&lt;/h4&gt;随着互联网的快速发展，信息传播的方式发生了变化并且效率得到了极大的提升。然而这也带来了快速传播的假新闻并导致了网络空间中的负面影响。目前，信息展示格式逐渐演进，新闻形式从文本转向多元化的多模态内容。因此检测多模态假新闻已经成为研究热点之一。但是，在现有的多模态假新闻检测领域仍然面临两个主要挑战：无法充分有效地利用多模态的信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题。为了填补这些空白，我们提出了ERIC-FND框架，这是一种增强型可靠外部信息多模态对比学习框架用于假新闻检测。该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of the Internet, the information disseminationparadigm has changed and the efficiency has been improved greatly. While thisalso brings the quick spread of fake news and leads to negative impacts oncyberspace. Currently, the information presentation formats have evolvedgradually, with the news formats shifting from texts to multimodal contents. Asa result, detecting multimodal fake news has become one of the researchhotspots. However, multimodal fake news detection research field still facestwo main challenges: the inability to fully and effectively utilize multimodalinformation for detection, and the low credibility or static nature of theintroduced external information, which limits dynamic updates. To bridge thegaps, we propose ERIC-FND, an external reliable information-enhanced multimodalcontrastive learning framework for fake news detection. ERIC-FND strengthensthe representation of news contents by entity-enriched external informationenhancement method. It also enriches the multimodal news information viamultimodal semantic interaction method where the multimodal constrativelearning is employed to make different modality representations learn from eachother. Moreover, an adaptive fusion method is taken to integrate the newsrepresentations from different dimensions for the eventual classification.Experiments are done on two commonly used datasets in different languages, X(Twitter) and Weibo. Experiment results demonstrate that our proposed modelERIC-FND outperforms existing state-of-the-art fake news detection methodsunder the same settings.</description>
      <author>example@mail.com (Biwei Cao, Qihang Wu, Jiuxin Cao, Bo Liu, Jie Gui)</author>
      <guid isPermaLink="false">2503.03107v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Category-level Meta-learned NeRF Priors for Efficient Object Mapping</title>
      <link>http://arxiv.org/abs/2503.01582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了PRENOM，一种基于先验的高效神经对象映射器，它结合了类别级别的形状先验和对象级别的NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;背景&lt;/h4&gt;在3D对象映射中，DeepSDF作为一种主要使用的类别级形状先验，虽然能够提供高效的对象重构和典型姿势估计，但是难以重现锐利的几何结构并且计算成本高。与此相反，NeRF捕捉精细细节但尚未有效集成到实时多对象映射框架中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PRENOM来连接DeepSDF与NeRF之间的差距，通过结合类别级先验和对象级NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;方法&lt;/h4&gt;1. 利用元学习从开源形状数据集中生成的合成重构任务进行训练。2. 使用多目标遗传算法为每个类优化NeRF架构以平衡重建设质量和训练时间。3. 采用基于先验的概率光线采样将采样指向预期对象区域，加速收敛并提高受限资源下的重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明PRENOM能够在低端GPU上实现高质量的重构同时保持计算可行性。对比无先验的NeRF方法，在合成数据集上的Chamfer距离降低了21%，在形状先验的真实世界嘈杂数据集中，所有重建指标平均提高了13%。&lt;h4&gt;结论&lt;/h4&gt;通过结合类别级和对象级先验，PRENOM展示了在实时多物体映射框架中的有效性，并且在训练时间减少5倍的情况下仍能保持准确的姿势和大小估计精度。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为PRENOM的新方法，该方法利用基于先验的信息来提高神经网络处理3D对象映射效率的同时支持典型姿态估计。通过结合DeepSDF和NeRF的优点，并采用多目标遗传算法优化架构以及概率光线采样策略加速收敛，实现高质量的重建效果并且在计算资源受限的情况下表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D object mapping, category-level priors enable efficient objectreconstruction and canonical pose estimation, requiring only a single prior persemantic category (e.g., chair, book, laptop). Recently, DeepSDF haspredominantly been used as a category-level shape prior, but it struggles toreconstruct sharp geometry and is computationally expensive. In contrast, NeRFscapture fine details but have yet to be effectively integrated withcategory-level priors in a real-time multi-object mapping framework. To bridgethis gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapperthat integrates category-level priors with object-level NeRFs to enhancereconstruction efficiency while enabling canonical object pose estimation.PRENOM gets to know objects on a first-name basis by meta-learning on syntheticreconstruction tasks generated from open-source shape datasets. To account forobject category variations, it employs a multi-objective genetic algorithm tooptimize the NeRF architecture for each category, balancing reconstructionquality and training time. Additionally, prior-based probabilistic ray samplingdirects sampling toward expected object regions, accelerating convergence andimproving reconstruction quality under constrained resources. Experimentalresults on a low-end GPU highlight the ability of PRENOM to achievehigh-quality reconstructions while maintaining computational feasibility.Specifically, comparisons with prior-free NeRF-based approaches on a syntheticdataset show a 21% lower Chamfer distance, demonstrating better reconstructionquality. Furthermore, evaluations against other approaches using shape priorson a noisy real-world dataset indicate a 13% improvement averaged across allreconstruction metrics, and comparable pose and size estimation accuracy, whilebeing trained for 5x less time.</description>
      <author>example@mail.com (Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2503.01582v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment</title>
      <link>http://arxiv.org/abs/2503.02976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型（LLM）正在从生成式AI发展为代理式AI，即在复杂现实场景中做出决策的系统。然而，尽管其生成能力已被充分研究，但它们的决策过程仍然不被人们深入理解。&lt;h4&gt;背景&lt;/h4&gt;当LLM处理例外情况时，这种问题尤为明显，因为合同内在的不完备性使这类挑战更加突出。&lt;h4&gt;目的&lt;/h4&gt;展示即使是最擅长推理的LLM也会因其严格遵循政策而与人类判断相去甚远。研究如何调整AI代理以更好地处理例外情况的方法。&lt;h4&gt;方法&lt;/h4&gt;评估了三种调整AI代理以处理例外情况的方法：伦理框架提示、链式思维推理和监督微调，其中尤其关注有解释性的人类反馈的监督微调。&lt;h4&gt;主要发现&lt;/h4&gt;伦理框架提示效果不佳；链式思维推理仅有轻微改善；而带有人类解释的监督微调显著提升了结果，并且能够使模型在新的场景中泛化出与人类相似的决策模式。这表明，为了将LLM与人类判断对齐，需要明确训练它们如何做出决策，而不仅仅是告诉它们应该做什么。&lt;h4&gt;结论&lt;/h4&gt;研究强调了解决LLMs处理例外情况不足的问题的重要性，以引导代理式AI的发展方向，使之能够更好地与人类判断相匹配，并适应新的环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型最初用于生成型人工智能领域，现在正在进化为代理型人工智能系统，在复杂的现实场景中进行决策。然而，尽管它们的生成能力得到了充分研究，但其决策过程仍然不为人所深入理解。这在LLM处理例外情况时尤为明显，因为合同内在的不完备性使得这类挑战更为突出。本研究表明，即使是最擅长推理的大型语言模型（LLMs）也会因其严格遵循规则而与人类判断相去甚远。我们评估了三种调整AI代理以更好地处理异常的方法：伦理框架提示、链式思维推理以及有监督的微调，并发现带有解释的人类反馈进行的监督微调方法效果最好，能够使模型在新的场景中泛化出类似人类决策模式。这项研究表明，要将LLMs与人类判断对齐，需要明确训练它们如何做出决定，而不仅仅是告诉它们应该做什么。这些发现强调了解决大型语言模型处理例外情况不足的重要性，以引导代理型人工智能的发展方向，使之能够更好地匹配人类判断，并适应新的环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs), initially developed for generative AI, are nowevolving into agentic AI systems, which make decisions in complex, real-worldcontexts. Unfortunately, while their generative capabilities arewell-documented, their decision-making processes remain poorly understood. Thisis particularly evident when models are handling exceptions, a critical andchallenging aspect of decision-making made relevant by the inherentincompleteness of contracts. Here we demonstrate that LLMs, even ones thatexcel at reasoning, deviate significantly from human judgments because theyadhere strictly to policies, even when such adherence is impractical,suboptimal, or even counterproductive. We then evaluate three approaches totuning AI agents to handle exceptions: ethical framework prompting,chain-of-thought reasoning, and supervised fine-tuning. We find that whileethical framework prompting fails and chain-of-thought prompting provides onlyslight improvements, supervised fine-tuning, specifically with humanexplanations, yields markedly better results. Surprisingly, in our experiments,supervised fine-tuning even enabled models to generalize human-likedecision-making to novel scenarios, demonstrating transfer learning ofhuman-aligned decision-making across contexts. Furthermore, fine-tuning withexplanations, not just labels, was critical for alignment, suggesting thataligning LLMs with human judgment requires explicit training on how decisionsare made, not just which decisions are made. These findings highlight the needto address LLMs' shortcomings in handling exceptions in order to guide thedevelopment of agentic AI toward models that can effectively align with humanjudgment and simultaneously adapt to novel contexts.</description>
      <author>example@mail.com (Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral)</author>
      <guid isPermaLink="false">2503.02976v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intrinsic and Extrinsic Factor Disentanglement for Recommendation in Various Context Scenarios</title>
      <link>http://arxiv.org/abs/2503.03524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 13 figures, 11 tables. Accepted by Transactions of  Information Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个区分内在和外在因素的推荐模型IEDR，它考虑了多种上下文环境的影响，从而提高了推荐系统的准确性。&lt;h4&gt;背景&lt;/h4&gt;用户行为（如购买、点击）在不同情境下可能有显著差异。这种差异是由用户的内在偏好和外在激励共同决定的，而这些外在激励会随着时间和地点等因素的变化而变化。&lt;h4&gt;目的&lt;/h4&gt;通过区分内在因素和外在因素来改善推荐系统的准确性和学习用户行为。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用框架IEDR模型，该模型能够同时考虑多种上下文环境的影响，将内在因素和外在因素区分开来。此模型包含一个不受上下文影响的对比学习组件，用于捕捉内在因素，并有一个解纠缠组件，在各种情境相互作用的情况下提取外在因素。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明IEDR在多个真实世界的数据集上展示了其区分和学习不同因素的有效性，能够显著提升推荐准确度（高达4% NDCG）。&lt;h4&gt;结论&lt;/h4&gt;IEDR模型通过考虑多种上下文环境的影响，提高了内在因素与外在因素的区分准确性，从而改善了推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;在推荐系统中，用户的行为模式可能因时间、地点等不同情境而显著变化。这是由于用户行为是由反映持续偏好（内在因素）和外部激励（外在因素）决定的。这些外部激励会根据不同的上下文环境发生变化。区分内外部因素有助于更好地学习用户行为。然而，现有的研究仅考虑从单个预定义的情境中区分它们的影响（如时间或地点），忽略了用户外部因素可能会受到同时作用的不同上下文中相互影响的事实。本文提出了一种通用框架——内在-外在解缠推荐模型IEDR，该模型可以在多种情景下分别提取出内在和外在的因素，从而更准确地区分这些因素并提高推荐的准确性。IEDR模型包括一个不受上下文影响的对比学习组件来捕捉内在因素，以及一个解纠缠组件，在各种情境相互作用的情况下可以提取外部因素。两个组件共同工作以实现有效的因子学习。大量的实验证明了IEDR在真实数据集上区分和学习独立因素的有效性，并通过最高提升4%的NDCG值显著提高了推荐准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recommender systems, the patterns of user behaviors (e.g., purchase,click) may vary greatly in different contexts (e.g., time and location). Thisis because user behavior is jointly determined by two types of factors:intrinsic factors, which reflect consistent user preference, and extrinsicfactors, which reflect external incentives that may vary in different contexts.Differentiating between intrinsic and extrinsic factors helps learn userbehaviors better. However, existing studies have only considereddifferentiating them from a single, pre-defined context (e.g., time orlocation), ignoring the fact that a user's extrinsic factors may be influencedby the interplay of various contexts at the same time. In this paper, wepropose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, ageneric framework that differentiates intrinsic from extrinsic factorsconsidering various contexts simultaneously, enabling more accuratedifferentiation of factors and hence the improvement of recommendationaccuracy. IEDR contains a context-invariant contrastive learning component tocapture intrinsic factors, and a disentanglement component to extract extrinsicfactors under the interplay of various contexts. The two components worktogether to achieve effective factor learning. Extensive experiments onreal-world datasets demonstrate IEDR's effectiveness in learning disentangledfactors and significantly improving recommendation accuracy by up to 4% inNDCG.</description>
      <author>example@mail.com (Yixin Su, Wei Jiang, Fangquan Lin, Cheng Yang, Sarah M. Erfani, Junhao Gan, Yunxiang Zhao, Ruixuan Li, Rui Zhang)</author>
      <guid isPermaLink="false">2503.03524v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Tiny Lidars for Manipulator Self-Awareness: Sensor Characterization and Initial Localization Experiments</title>
      <link>http://arxiv.org/abs/2503.03449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, 3 tables, conference submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用来自微型VL53L5CX ToF传感器（小型激光雷达）的粗略点云来定位机器人工作空间中目标对象的方法。&lt;h4&gt;背景&lt;/h4&gt;在许多任务，如操作和检查等场景下，对于机器人来说，能够在周围环境中精确定位目标物体是有益的。&lt;h4&gt;目的&lt;/h4&gt;利用微型ToF传感器获得的数据来改进机器人对其工作空间内目标物的位置估计准确性。&lt;h4&gt;方法&lt;/h4&gt;- 进行实验校准了传感器读数与相对距离及角度之间的依赖关系。- 提出了一种概率性传感器模型，并在使用粒子滤波器（PF）的对象姿态估计任务中对该模型进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的传感器模型相对于两个基准，即测量值无不确定性假设和由传感器数据表提供的置信度，提高了目标对象定位的性能。&lt;h4&gt;结论&lt;/h4&gt;通过采用更准确的概率性传感器模型，机器人在执行各种任务时能够更好地识别并定位目标物体的位置。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已翻译为中文，并且根据其核心要点进行了分点总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For several tasks, ranging from manipulation to inspection, it is beneficialfor robots to localize a target object in their surroundings. In this paper, wepropose an approach that utilizes coarse point clouds obtained fromminiaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny lidars) to localize atarget object in the robot's workspace. We first conduct an experimentalcampaign to calibrate the dependency of sensor readings on relative range andorientation to targets. We then propose a probabilistic sensor model that isvalidated in an object pose estimation task using a Particle Filter (PF). Theresults show that the proposed sensor model improves the performance of thelocalization of the target object with respect to two baselines: one thatassumes measurements are free from uncertainty and one in which the confidenceis provided by the sensor datasheet.</description>
      <author>example@mail.com (Giammarco Caroleo, Alessandro Albini, Daniele De Martini, Timothy D. Barfoot, Perla Maiolino)</author>
      <guid isPermaLink="false">2503.03449v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mineral segmentation using electron microscope images and spectral sampling through multimodal graph neural networks</title>
      <link>http://arxiv.org/abs/2503.03507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的方法，用于融合多模态扫描电子显微镜（SEM）图像进行矿物分割。&lt;h4&gt;背景&lt;/h4&gt;通常情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行准确的矿物分割。因此，通常会使用能量色散X射线光谱（EDS）测量来补充BSE图像的数据，以提供关于化学成分的高度精确的信息，但这些数据采集起来非常耗时。&lt;h4&gt;目的&lt;/h4&gt;利用稀疏的光谱数据与BSE图像一起进行矿物分割。&lt;h4&gt;方法&lt;/h4&gt;提出使用图神经网络将两种模态融合，并同时完成矿物相的分割。&lt;h4&gt;主要发现&lt;/h4&gt;即使仅提供1%的BSE像素上的EDS数据，也能产生准确的分割结果，从而实现了对矿物样品的快速分析。&lt;h4&gt;结论&lt;/h4&gt;提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络的方法，用于根据多模态扫描电子显微镜（SEM）图像的数据融合进行分割。在大多数情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行矿物分割。因此，成像通常会与点测量的能量色散X射线光谱（EDS）光谱数据相结合，这些数据提供关于化学成分的高度精确的信息，但采集起来非常耗时。这激发了使用稀疏的光谱数据结合BSE图像进行矿物分割的需求。由于光谱数据的本质是无结构化的，大多数传统的图像融合技术都不适合用于BSE-EDS融合。我们提出采用图神经网络来融合两种模态，并同时完成矿物相的分割。我们的结果表明，在提供给定1% BSE像素上的EDS数据的情况下可以实现准确的分割，这使得能够快速分析矿物样品。提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel Graph Neural Network-based method for segmentation basedon data fusion of multimodal Scanning Electron Microscope (SEM) images. In mostcases, Backscattered Electron (BSE) images obtained using SEM do not containsufficient information for mineral segmentation. Therefore, imaging is oftencomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)spectral measurements that provide highly accurate information about thechemical composition but that are time-consuming to acquire. This motivates theuse of sparse spectral data in conjunction with BSE images for mineralsegmentation. The unstructured nature of the spectral data makes mosttraditional image fusion techniques unsuitable for BSE-EDS fusion. We proposeusing graph neural networks to fuse the two modalities and segment the mineralphases simultaneously. Our results demonstrate that providing EDS data for asfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysisof mineral samples. The proposed data fusion pipeline is versatile and can beadapted to other domains that involve image data and point-wise measurements.</description>
      <author>example@mail.com (Samuel Repka, Bořek Reich, Fedor Zolotarev, Tuomas Eerola, Pavel Zemčík)</author>
      <guid isPermaLink="false">2503.03507v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees</title>
      <link>http://arxiv.org/abs/2503.02859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了AUASE，一种用于动态网络的稳定无监督表示学习框架，适用于节点带有随时间变化属性信息的情况。&lt;h4&gt;背景&lt;/h4&gt;在处理具有时变属性信息的动态网络时，稳定性对于确保相同行为的节点在同一时间点拥有相同的嵌入至关重要，这使得可以在不同时刻对比网络中的节点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示学习方法AUASE，并证明其能够在没有地面真实标签的情况下满足稳定性保证。&lt;h4&gt;方法&lt;/h4&gt;通过展示AUASE的一致收敛性到相关的潜在位置模型来建立稳定性。利用三种真实的属性网络，在链接预测和节点分类任务中将AUASE与最先进的网络表示学习方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;AUASE是在没有地面真实标签的情况下唯一能够满足稳定性的属性动态嵌入，实验表明其对于链接预测和节点分类提供了显著的改进。&lt;h4&gt;结论&lt;/h4&gt;AUASE为处理具有时变属性信息的动态网络提供了一个强大的工具，并且在不需要标签的情况下确保了表示学习的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stability for dynamic network embeddings ensures that nodes behaving the sameat different times receive the same embedding, allowing comparison of nodes inthe network across time. We present attributed unfolded adjacency spectralembedding (AUASE), a stable unsupervised representation learning framework fordynamic networks in which nodes are attributed with time-varying covariateinformation. To establish stability, we prove uniform convergence to anassociated latent position model. We quantify the benefits of our dynamicembedding by comparing with state-of-the-art network representation learningmethods on three real attributed networks. To the best of our knowledge, AUASEis the only attributed dynamic embedding that satisfies stability guaranteeswithout the need for ground truth labels, which we demonstrate providessignificant improvements for link prediction and node classification.</description>
      <author>example@mail.com (Emma Ceccherini, Ian Gallagher, Andrew Jones, Daniel Lawson)</author>
      <guid isPermaLink="false">2503.02859v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection</title>
      <link>http://arxiv.org/abs/2503.01569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the VISION workshop at ECCV 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了鲁棒异常检测模型在工业检查中的性能，特别是在处理噪声数据方面的能力。&lt;h4&gt;背景&lt;/h4&gt;当前的工业检查中存在大量噪声数据，这对传统的异常检测模型构成了挑战。这些噪声可能导致错误的缺陷识别和高误报率。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来提高模型适应性和鲁棒性，通过元学习方法识别并拒绝训练中的噪声数据以改进学习过程。&lt;h4&gt;方法&lt;/h4&gt;采用无模型假设元学习（MAML）以及迭代细化流程，利用四分位距剔除方案增强其可适应性和鲁棒性。这种方法可以有效提升模型区分正常与异常样本的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在噪声环境中的表现优于传统模型，并且即使是在清晰训练集的情况下也能很好地隔离那些偏离分布的样本，从而提供显著改进。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅适用于高噪音环境下，而且也可以用于处理标准数据集，在保证准确性的前提下提高了异常检测的效率和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究探讨了鲁棒性异常检测模型在工业检查中的表现，特别是它们应对噪声数据的能力。我们建议利用元学习方法的适应能力来识别并排除训练数据中的噪声，以改进学习过程。在我们的模型中，我们使用了无模型假设元学习（MAML）以及迭代细化流程通过四分位距剔除方案提高其可适应性和鲁棒性。这种方法显著增强了区分正常和缺陷条件的能力。我们在著名的MVTec和KSDD2数据集上进行的实验结果表明，所提出的方法不仅在大量噪声环境中表现出色，而且还可以为明确训练集合贡献，在分离那些偏离分布样本的同时提供了对传统模型的重要改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the performance of robust anomaly detection models inindustrial inspection, focusing particularly on their ability to handle noisydata. We propose to leverage the adaptation ability of meta learning approachesto identify and reject noisy training data to improve the learning process. Inour model, we employ Model Agnostic Meta Learning (MAML) and an iterativerefinement process through an Inter-Quartile Range rejection scheme to enhancetheir adaptability and robustness. This approach significantly improves themodels capability to distinguish between normal and defective conditions. Ourresults of experiments conducted on well known MVTec and KSDD2 datasetsdemonstrate that the proposed method not only excels in environments withsubstantial noise but can also contribute in case of a clear training set,isolating those samples that are relatively out of distribution, thus offeringsignificant improvements over traditional models.</description>
      <author>example@mail.com (Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti)</author>
      <guid isPermaLink="false">2503.01569v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Remote Sensing Image Classification Using Convolutional Neural Network (CNN) and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2503.02510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is published in Journal of Computer Science, Volume 21 No.  3, 2025. It contains 635-645 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究探讨了利用卷积神经网络（CNN）架构对包含输电塔、森林、农田和山脉的航空图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;旨在通过实验来评估不同模型在土地覆盖分类任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的VGG16和MobileNetV2模型作为迁移学习的基础，利用自收集与MLRNet数据集结合的数据集进行测试。该数据集包含从Google卫星图像中提取的共10,400张图片。&lt;h4&gt;主要发现&lt;/h4&gt;{'总体表现': '基于构建的CNN模型的整体准确率为87%；', 'VGG16性能': '使用预训练的VGG16进行迁移学习后，准确率达到90%，测试损失为0.298；', 'MobileNetV2优势': '而采用MobileNetV2进行转移学习后的模型表现最佳，其准确率高达96%，且测试损失低至0.119。'}&lt;h4&gt;结论&lt;/h4&gt;研究表明，在土地覆盖分类任务中使用迁移学习，尤其是基于MobileNetV2的模型，可以取得优异的效果；这不仅提高了精度也增强了实用性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究调查了利用卷积神经网络（CNN）架构从描绘输电塔、森林、农田和山脉的航空图像中提取特征，并通过Softmax进行分类。为了测试该模型，我们运行了十轮训练，使用批次大小为90，Adam优化器和学习率为0.001。在自收集图片与MLRNet数据集相结合的数据集中进行了训练及评估，包括从Google卫星影像中获得的共10,400张图像。研究表明，迁移学习模型尤其是MobileNetV2，在土地覆盖分类方面表现优异；这些模型因其良好的精度和效率平衡而适合实际应用；我们的方法在构建的CNN模型上达到了87%的整体准确率；通过使用预训练的VGG16与MobileNetV2作为迁移学习的基础，我们实现了更高的准确性。特别是，VGG16达到90%的准确性和0.298的测试损失，而MobileNetV2则以96%的准确度和0.119的测试损失领先所有模型；这些结果证明了使用基于MobileNetV2迁移学习进行输电塔、森林、农田和山脉分类的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3844/jcssp.2025.635.645&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the classification of aerial images depictingtransmission towers, forests, farmland, and mountains. To complete theclassification job, features are extracted from input photos using aConvolutional Neural Network (CNN) architecture. Then, the images areclassified using Softmax. To test the model, we ran it for ten epochs using abatch size of 90, the Adam optimizer, and a learning rate of 0.001. Bothtraining and assessment are conducted using a dataset that blendsself-collected pictures from Google satellite imagery with the MLRNet dataset.The comprehensive dataset comprises 10,400 images. Our study shows thattransfer learning models and MobileNetV2 in particular, work well for landscapecategorization. These models are good options for practical use because theystrike a good mix between precision and efficiency; our approach achievesresults with an overall accuracy of 87% on the built CNN model. Furthermore, wereach even higher accuracies by utilizing the pretrained VGG16 and MobileNetV2models as a starting point for transfer learning. Specifically, VGG16 achievesan accuracy of 90% and a test loss of 0.298, while MobileNetV2 outperforms bothmodels with an accuracy of 96% and a test loss of 0.119; the resultsdemonstrate the effectiveness of employing transfer learning with MobileNetV2for classifying transmission towers, forests, farmland, and mountains.</description>
      <author>example@mail.com (Mustafa Majeed Abd Zaid, Ahmed Abed Mohammed, Putra Sumari)</author>
      <guid isPermaLink="false">2503.02510v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GNNMerge: Merging of GNN Models Without Accessing Training Data</title>
      <link>http://arxiv.org/abs/2503.03384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了GNNMerge算法，用于合并图神经网络（GNN），在各种数据集、任务和架构上验证了其优越性和效率。', '背景': '模型融合作为一种将多个训练好的模型整合成单一模型的方法，在不访问原始训练数据的情况下得到了广泛的应用。尽管这种技术已经在计算机视觉和自然语言处理领域显示出成功，但将其应用于GNN上的研究尚属空白。', '目的': '首次对GNN的模型合并算法进行基准测试，并提出一种新的方法来解决现有方法在GNN中的应用限制。', '方法': '提出了名为GNNMerge的新策略，该策略使用任务无关节点嵌入对齐技术来实现GNN模型的合并。此外，它还展示了在轻微放宽的情况下，提出的优化目标对于广泛使用的GNN架构可以提供直接解析解。', '主要发现': '实验表明，与现有方法相比，GNNMerge提高了24%的准确性，并且与从头训练相比，计算效率提高了一个数量级以上。', '结论': 'GNNMerge通过任务无关节点嵌入对齐技术成功解决了模型合并中的挑战，展示了在不同场景下的优越性能和速度优势。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了机器学习领域中将多个已训练的模型整合为单一模型的过程，并探讨了现有方法如何在计算机视觉和自然语言处理等领域取得成功，但在图神经网络（GNN）中的应用仍然空白。论文作者进行了首次针对GNN的模型合并算法基准测试研究，揭示了这些算法在此上下文中效果有限的问题。为了应对这一挑战，他们提出了GNNMerge算法，该算法利用了一种任务无关节点嵌入对齐策略来实现GNN模型的合并，并且在轻微放宽假设的情况下展示了优化目标对于广泛使用的GNN架构可以提供直接解析解，显著提高了计算效率。实验结果表明，在各种数据集、任务和架构下，与现有方法相比，GNNMerge准确率最多提高24%，同时相对于从头开始训练的速度提升了两个数量级以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model merging has gained prominence in machine learning as a method tointegrate multiple trained models into a single model without accessing theoriginal training data. While existing approaches have demonstrated success indomains such as computer vision and NLP, their application to Graph NeuralNetworks (GNNs) remains unexplored. These methods often rely on the assumptionof shared initialization, which is seldom applicable to GNNs. In this work, weundertake the first benchmarking study of model merging algorithms for GNNs,revealing their limited effectiveness in this context. To address thesechallenges, we propose GNNMerge, which utilizes a task-agnostic node embeddingalignment strategy to merge GNNs. Furthermore, we establish that under a mildrelaxation, the proposed optimization objective admits direct analyticalsolutions for widely used GNN architectures, significantly enhancing itscomputational efficiency. Empirical evaluations across diverse datasets, tasks,and architectures establish GNNMerge to be up to 24% more accurate thanexisting methods while delivering over 2 orders of magnitude speed-up comparedto training from scratch.</description>
      <author>example@mail.com (Vipul Garg, Ishita Thakre, Sayan Ranu)</author>
      <guid isPermaLink="false">2503.03384v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PABBO: Preferential Amortized Black-Box Optimization</title>
      <link>http://arxiv.org/abs/2503.00924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 17 figures. Accepted at the Thirteenth International  Conference on Learning Representations (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种通过用户对设计配对的偏好反馈高效学习潜在用户偏好的方法。它使用统计代理模型，如高斯过程，并采用获取策略选择下一个候选配对以获得用户反馈。&lt;h4&gt;背景&lt;/h4&gt;由于偏好贝叶斯优化中的似然非共轭性，每次步骤都需要大量的近似推理计算，这与人机交互的方式不兼容，限制了其在实际案例中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于最新进展的推算化贝叶斯优化方法来克服这一问题，并通过元学习代理和获取函数实现PBO的完全推算化。&lt;h4&gt;方法&lt;/h4&gt;该方法包含一个新颖的转换器神经过程架构，采用强化学习训练并使用定制辅助损失进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集和真实世界数据集组成的基准测试上，这种方法比传统的高斯过程策略快几个数量级，并且通常在准确性方面也优于它们。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了一种高效的推算化偏好贝叶斯优化方法，为实际应用中的用户偏好学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种高效的学习潜在用户偏好的方法，通过从设计配对的偏好反馈中获取信息。由于非共轭性问题，每次步骤都涉及大量计算。本文提出了一种基于推算化贝叶斯优化的方法，它可以通过元学习代理和获取函数实现完全推算化，并且在实际测试数据集上表现出更高的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Preferential Bayesian Optimization (PBO) is a sample-efficient method tolearn latent user utilities from preferential feedback over a pair of designs.It relies on a statistical surrogate model for the latent function, usually aGaussian process, and an acquisition strategy to select the next candidate pairto get user feedback on. Due to the non-conjugacy of the associated likelihood,every PBO step requires a significant amount of computations with variousapproximate inference techniques. This computational overhead is incompatiblewith the way humans interact with computers, hindering the use of PBO inreal-world cases. Building on the recent advances of amortized BO, we proposeto circumvent this issue by fully amortizing PBO, meta-learning both thesurrogate and the acquisition function. Our method comprises a noveltransformer neural process architecture, trained using reinforcement learningand tailored auxiliary losses. On a benchmark composed of synthetic andreal-world datasets, our method is several orders of magnitude faster than theusual Gaussian process-based strategies and often outperforms them in accuracy.</description>
      <author>example@mail.com (Xinyu Zhang, Daolang Huang, Samuel Kaski, Julien Martinelli)</author>
      <guid isPermaLink="false">2503.00924v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLMs as Educational Analysts: Transforming Multimodal Data Traces into Actionable Reading Assessment Reports</title>
      <link>http://arxiv.org/abs/2503.02099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了利用多模态数据源（包括眼动追踪数据、学习成果、评估内容和教学标准）来获取有意义的阅读洞察的方法。我们采用无监督学习技术识别不同的阅读行为模式，并使用大型语言模型将所得信息合成成易于理解且富有操作性的报告供教师参考，从而简化解读过程。&lt;h4&gt;背景&lt;/h4&gt;目前许多教育科技应用程序主要侧重于结果导向型指标，而对学生的具体行为和认知洞察有限。这限制了对学生阅读能力的深入理解和提升。&lt;h4&gt;目的&lt;/h4&gt;研究目的是通过集成多模态数据源来改进学生阅读评估的方法，并探究如何利用人工智能技术生成有价值的教师报告。&lt;h4&gt;方法&lt;/h4&gt;本研究运用无监督机器学习算法识别不同的阅读模式，然后使用大型语言模型将这些结果整理成对教育工作者有帮助的、具体的行动建议。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，大型语言模型能够有效地充当教育分析员的角色，将多样化的数据转化为教师友好的见解，并且这种由人工智能生成的报告在清晰度、准确性、相关性和教学实用性方面得到了专家和教育者的认可。&lt;h4&gt;结论&lt;/h4&gt;虽然自动化洞察力生成展现了巨大潜力，但为了确保可靠性和公平性，仍需人类监督。这项研究促进了以人为本的人工智能技术在教育领域的应用，将数据驱动的分析与实际课堂实践紧密结合在一起。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的研究调查了利用多模态数据源获取有意义阅读洞察的方法，并通过无监督学习技术和大型语言模型来生成教师友好的报告。结果表明该方法有效且得到认可，同时强调了人类监督的重要性以及其在教育领域中的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reading assessments are essential for enhancing students' comprehension, yetmany EdTech applications focus mainly on outcome-based metrics, providinglimited insights into student behavior and cognition. This study investigatesthe use of multimodal data sources -- including eye-tracking data, learningoutcomes, assessment content, and teaching standards -- to derive meaningfulreading insights. We employ unsupervised learning techniques to identifydistinct reading behavior patterns, and then a large language model (LLM)synthesizes the derived information into actionable reports for educators,streamlining the interpretation process. LLM experts and human educatorsevaluate these reports for clarity, accuracy, relevance, and pedagogicalusefulness. Our findings indicate that LLMs can effectively function aseducational analysts, turning diverse data into teacher-friendly insights thatare well-received by educators. While promising for automating insightgeneration, human oversight remains crucial to ensure reliability and fairness.This research advances human-centered AI in education, connecting data-drivenanalytics with practical classroom applications.</description>
      <author>example@mail.com (Eduardo Davalos, Yike Zhang, Namrata Srivastava, Jorge Alberto Salas, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin)</author>
      <guid isPermaLink="false">2503.02099v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data</title>
      <link>http://arxiv.org/abs/2503.02849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个深度多模态学习框架，结合了乳腺癌的组织病理图像和基因表达数据，以分类BRCA.Luminal和BRCA.Basal / Her2亚型。&lt;h4&gt;背景&lt;/h4&gt;传统的乳腺癌分子分型方法主要依赖于单一的组织病理学或基因表达分析，预测能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架来提高乳腺癌亚型分类的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet-50模型提取图像特征，并使用全连接层处理基因表达数据。引入跨注意力融合机制以增强模态交互，使用五折交叉验证进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的多模态整合框架在分类准确率、精确召回AUC和F1分数方面优于单一模式的方法。&lt;h4&gt;结论&lt;/h4&gt;研究强调了深度学习方法在乳腺癌亚型分类中的潜力，并为临床决策提供了支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular subtyping of breast cancer is crucial for personalized treatmentand prognosis. Traditional classification approaches rely on eitherhistopathological images or gene expression profiling, limiting theirpredictive power. In this study, we propose a deep multimodal learningframework that integrates histopathological images and gene expression data toclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Ourapproach employs a ResNet-50 model for image feature extraction and fullyconnected layers for gene expression processing, with a cross-attention fusionmechanism to enhance modality interaction. We conduct extensive experimentsusing five-fold cross-validation, demonstrating that our multimodal integrationoutperforms unimodal approaches in terms of classification accuracy,precision-recall AUC, and F1-score. Our findings highlight the potential ofdeep learning for robust and interpretable breast cancer subtypeclassification, paving the way for improved clinical decision-making.</description>
      <author>example@mail.com (Amin Honarmandi Shandiz)</author>
      <guid isPermaLink="false">2503.02849v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal AI predicts clinical outcomes of drug combinations from preclinical data</title>
      <link>http://arxiv.org/abs/2503.02781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MADRIGAL是一个多模态AI模型，能够从结构、通路、细胞活力和转录组数据中学习，预测药物组合效果，并在临床结果预测上优于单一模式方法和其他最先进的模型。&lt;h4&gt;背景&lt;/h4&gt;目前的模型依靠结构或靶点特征来识别高效且低毒性的药物组合，但这些方法未能整合多模态数据以进行准确、具有临床相关性的预测。&lt;h4&gt;目的&lt;/h4&gt;介绍MADRIGAL模型，用于更准确地预测药物组合效果以及潜在的安全性和毒性风险，并支持个性化癌症治疗和二型糖尿病及代谢紊乱相关的脂肪肝病的药物管理。&lt;h4&gt;方法&lt;/h4&gt;使用变压器瓶颈模块统一预临床药物数据模态，在训练和推理过程中处理缺失的数据。该模型利用多模态学习，包括结构、通路、细胞活力和转录组学数据进行药物组合效应预测。&lt;h4&gt;主要发现&lt;/h4&gt;MADRIGAL在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型，并且能够识别转运蛋白介导的药物相互作用。它还支持虚拟筛选抗癌症药物组合以及个性化治疗方案的设计。&lt;h4&gt;结论&lt;/h4&gt;MADRIGAL提供了一种多模态的方法来设计结合疗法，具有改进的预测准确性和临床相关性。该模型还可以通过与大型语言模型集成，让用户以自然语言描述临床结果，从而改善安全评估并识别潜在风险。&lt;h4&gt;翻译&lt;/h4&gt;从预临床数据中预测临床效果对于识别安全有效的药物组合至关重要。现有方法依赖于结构或靶点特征来选择高效率低毒性的药物组合，但这些方法未能整合进行准确且具有临床相关性预测所需的多模态数据。MADRIGAL模型可以利用结构、路径、细胞活力和转录组学数据，预测21842种化合物（包括已批准的药物和在研新型化合物）以及953个临床效果组合效应。此模型通过使用变压器瓶颈模块来统一预临床药物模态，并解决了多模态学习中的缺失数据问题，在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型。该模型支持虚拟筛选抗癌症药物组合，有助于二型糖尿病和代谢紊乱相关脂肪肝病的治疗管理并识别转运蛋白介导的药物相互作用。MADRIGAL还通过整合来自癌症患者的基因组配置文件来支持个性化癌症疗法，并且能够预测基于急性髓性白血病样本和个人来源的小鼠模型中个性化的药物组合疗效。与大型语言模型集成后，用户可以使用自然语言描述临床结果，从而提高安全性评估并识别潜在的风险。MADRIGAL为设计具有改进的预测准确性和临床相关性的结合疗法提供了多模态方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting clinical outcomes from preclinical data is essential foridentifying safe and effective drug combinations. Current models rely onstructural or target-based features to identify high-efficacy, low-toxicitydrug combinations. However, these approaches fail to incorporate the multimodaldata necessary for accurate, clinically-relevant predictions. Here, weintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,cell viability, and transcriptomic data to predict drug combination effectsacross 953 clinical outcomes and 21842 compounds, including combinations ofapproved drugs and novel compounds in development. MADRIGAL uses a transformerbottleneck module to unify preclinical drug data modalities while handlingmissing data during training and inference--a major challenge in multimodallearning. It outperforms single-modality methods and state-of-the-art models inpredicting adverse drug interactions. MADRIGAL performs virtual screening ofanticancer drug combinations and supports polypharmacy management for type IIdiabetes and metabolic dysfunction-associated steatohepatitis (MASH). Itidentifies transporter-mediated drug interactions. MADRIGAL predictsresmetirom, the first and only FDA-approved drug for MASH, among therapies withthe most favorable safety profile. It supports personalized cancer therapy byintegrating genomic profiles from cancer patients. Using primary acute myeloidleukemia samples and patient-derived xenograft models, it predicts the efficacyof personalized drug combinations. Integrating MADRIGAL with a large languagemodel allows users to describe clinical outcomes in natural language, improvingsafety assessment by identifying potential adverse interactions and toxicityrisks. MADRIGAL provides a multimodal approach for designing combinationtherapies with improved predictive accuracy and clinical relevance.</description>
      <author>example@mail.com (Yepeng Huang, Xiaorui Su, Varun Ullanat, Ivy Liang, Lindsay Clegg, Damilola Olabode, Nicholas Ho, Bino John, Megan Gibbs, Marinka Zitnik)</author>
      <guid isPermaLink="false">2503.02781v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>FASTer: Focal Token Acquiring-and-Scaling Transformer for Long-term 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.01899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近基于Lidar的顶级时间3D检测器越来越多地采用区域级范式，该范式首先生成粗略提案，然后编码和融合区域特征。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理随输入帧数量增加而指数增长的复杂性时存在挑战，并且结果层面的任意连接限制了全局信息提取。&lt;h4&gt;目的&lt;/h4&gt;本文提出了Focal Token Acquiring-and-Scaling Transformer (FASTer)，它以自适应和轻量级的方式动态选择焦点令牌并浓缩令牌序列。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单但有效的自适应缩放机制，强调单个令牌的贡献，并在历史帧中仅存储和处理焦点点，从而大大减少了整体复杂性。此外，还提出了分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在Waymo Open Dataset上，FASTer在性能、效率方面显著优于其他最先进的检测器，并且展现出更高的灵活性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理大规模数据集上的3D目标检测任务时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近表现优异的基于Lidar的时间三维探测器越来越多地采用了区域级方法。它们首先生成粗略提案，然后编码和融合区域特征。然而，无差别采样和融合通常忽略了个体点的不同贡献，并且随着输入帧数量的增长导致复杂性呈指数上升。此外，任意的结果级别串联限制了全局信息的提取。本文提出了一种Focal Token Acquiring-and-Scaling Transformer (FASTer)，该方法以自适应、轻量级的方式动态选择焦点令牌并浓缩令牌序列。强调单个令牌的贡献，我们提出了一个简单但有效的自适应缩放机制来捕获几何上下文并在筛选出焦点点的同时剔除非关键信息。在历史帧中仅存储和处理焦点点大大减少了整体复杂性。此外，还提出了一种新颖的分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。实验结果表明，在Waymo Open Dataset上，FASTer显著优于其他最先进的检测器，并且在性能、效率方面表现出色，同时还展现了更高的灵活性和鲁棒性。相关代码可在https://github.com/MSunDYY/FASTer.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/msundyy/faster&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent top-performing temporal 3D detectors based on Lidars have increasinglyadopted region-based paradigms. They first generate coarse proposals, followedby encoding and fusing regional features. However, indiscriminate sampling andfusion often overlook the varying contributions of individual points and leadto exponentially increased complexity as the number of input frames grows.Moreover, arbitrary result-level concatenation limits the global informationextraction. In this paper, we propose a Focal Token Acquring-and-ScalingTransformer (FASTer), which dynamically selects focal tokens and condensestoken sequences in an adaptive and lightweight manner. Emphasizing thecontribution of individual tokens, we propose a simple but effective AdaptiveScaling mechanism to capture geometric contexts while sifting out focal points.Adaptively storing and processing only focal points in historical framesdramatically reduces the overall complexity. Furthermore, a novel GroupedHierarchical Fusion strategy is proposed, progressively performing sequencescaling and Intra-Group Fusion operations to facilitate the exchange of globalspatial and temporal information. Experiments on the Waymo Open Datasetdemonstrate that our FASTer significantly outperforms other state-of-the-artdetectors in both performance and efficiency while also exhibiting improvedflexibility and robustness. The code is available athttps://github.com/MSunDYY/FASTer.git.</description>
      <author>example@mail.com (Chenxu Dang, Zaipeng Duan, Pei An, Xinmin Zhang, Xuzhong Hu, Jie Ma)</author>
      <guid isPermaLink="false">2503.01899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Structural Entropy Guided Unsupervised Graph Out-Of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2503.03241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2025 (The 39th Annual AAAI Conference on Artificial  Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了SEGO框架，通过在图分类中集成结构熵到无监督OOD检测中来解决现有方法在处理冗余信息时的性能问题。&lt;h4&gt;背景&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法在处理图结构中冗余信息时遇到的问题，并提高GNN模型的可靠性，提出了SEGO框架。&lt;h4&gt;方法&lt;/h4&gt;SEGO通过引入基于编码树形式的锚点视图来最小化结构熵，在对比学习架构下操作。该方案还采用三元组视角进行局部、全局和树级别的多粒度对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEGO能够有效地从图中去除冗余信息，同时保留重要的结构信息，使ID与OOD样本之间的区别更为明显。实验结果表明，在无监督OOD检测方面，该方法优于现有最佳基线模型。&lt;h4&gt;结论&lt;/h4&gt;在真实数据集上的广泛实验证明了SEGO的有效性，尤其是在9对10的数据集中表现最优，并且在FreeSolv/ToxCast数据集上超越了竞争对手10.8%。&lt;h4&gt;翻译&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。为了应对这一挑战，我们提出了SEGO，一种集成结构熵进行图分类中无监督OOD检测的框架。该方法通过引入编码树形式的锚点视图来最小化结构熵，并采用了局部、全局和树级别多粒度对比学习方案。实验结果表明，在10对数据集中有9对表现最佳，平均性能提高3.7%，在FreeSolv/ToxCast数据集上优于对手10.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emerging of huge amount of unlabeled data, unsupervisedout-of-distribution (OOD) detection is vital for ensuring the reliability ofgraph neural networks (GNNs) by identifying OOD samples from in-distribution(ID) ones during testing, where encountering novel or unknown data isinevitable. Existing methods often suffer from compromised performance due toredundant information in graph structures, which impairs their ability toeffectively differentiate between ID and OOD data. To address this challenge,we propose SEGO, an unsupervised framework that integrates structural entropyinto OOD detection regarding graph classification. Specifically, within thearchitecture of contrastive learning, SEGO introduces an anchor view in theform of coding tree by minimizing structural entropy. The obtained coding treeeffectively removes redundant information from graphs while preservingessential structural information, enabling the capture of distinct graphpatterns between ID and OOD samples. Furthermore, we present a multi-grainedcontrastive learning scheme at local, global, and tree levels using tripletviews, where coding trees with essential information serve as the anchor view.Extensive experiments on real-world datasets validate the effectiveness ofSEGO, demonstrating superior performance over state-of-the-art baselines in OODdetection. Specifically, our method achieves the best performance on 9 out of10 dataset pairs, with an average improvement of 3.7\% on OOD detectiondatasets, significantly surpassing the best competitor by 10.8\% on theFreeSolv/ToxCast dataset pair.</description>
      <author>example@mail.com (Yue Hou, He Zhu, Ruomei Liu, Yingke Su, Jinxiang Xia, Junran Wu, Ke Xu)</author>
      <guid isPermaLink="false">2503.03241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor Environments</title>
      <link>http://arxiv.org/abs/2503.03373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在机器人和增强现实等领域中，精确的定位对于自主导航至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于单目视觉里程计框架的方法，该方法利用连续3D高斯地图来提高姿态估计的准确性，并降低成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个使用连续3D高斯图的单目视觉里程计框架，该框架直接为所有提取出的高梯度点分配几何一致性的深度值，避免了传统的插值步骤。&lt;h4&gt;主要发现&lt;/h4&gt;通过在两个公开数据集上的评估显示，提出的框架相比现有方法具有更优的姿态跟踪精度。&lt;h4&gt;结论&lt;/h4&gt;研究团队已经开源了这项工作的源代码以促进社区发展。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位对于机器人和增强现实应用（如自主导航）至关重要。基于视觉的方法结合先验地图旨在将LiDAR级别的准确性与相机的成本效率结合起来，实现稳健的姿态估计。然而，现有的方法在关联离散点云图和密集图像像素时常常依赖于不可靠的插值过程，这不可避免地引入了深度误差并降低了姿态估计精度。我们提出了一种单目视觉里程计框架，利用连续3D高斯地图，该框架直接为所有提取出的高梯度点分配几何一致性的深度值而无需插值步骤。在两个公开数据集上的评估表明，相比现有方法具有更优的姿态跟踪准确性。研究团队已经开源了这项工作的源代码以促进社区发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for robotics and augmented realityapplications such as autonomous navigation. Vision-based methods combiningprior maps aim to integrate LiDAR-level accuracy with camera cost efficiencyfor robust pose estimation. Existing approaches, however, often depend onunreliable interpolation procedures when associating discrete point cloud mapswith dense image pixels, which inevitably introduces depth errors and degradespose estimation accuracy. We propose a monocular visual odometry frameworkutilizing a continuous 3D Gaussian map, which directly assigns geometricallyconsistent depth values to all extracted high-gradient points withoutinterpolation. Evaluations on two public datasets demonstrate superior trackingaccuracy compared to existing methods. We have released the source code of thiswork for the development of the community.</description>
      <author>example@mail.com (Jie Deng, Fengtian Lang, Zikang Yuan, Xin Yang)</author>
      <guid isPermaLink="false">2503.03373v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging</title>
      <link>http://arxiv.org/abs/2503.02824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个用于多模态PET/CT成像的新型基础模型框架Cross-Fraternal Twin Masked Autoencoder (FratMAE)，该框架旨在解决现有AI驱动的PET/CT分析方法中泛化性和鲁棒性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;在肿瘤学领域，正电子发射断层扫描-计算机断层扫描(PET/CT)结合了CT提供的解剖细节和PET提供的功能性代谢活性及分子标志物表达信息，在癌症诊断、分期以及治疗监测方面被广泛应用。然而，现有的基于人工智能的PET/CT分析主要依赖于从头训练的任务特定模型或受限数据集，这限制了它们的泛化能力和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门设计用于多模态PET/CT成像的基础模型方法，以提高其在临床应用中的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Cross-Fraternal Twin Masked Autoencoder (FratMAE)的新框架。该框架使用单独的视觉变换器(ViT)编码器处理PET和CT扫描，并通过交叉注意力解码器促进模式间的协同交互，同时结合文本元数据来增强PET表示的学习。&lt;h4&gt;主要发现&lt;/h4&gt;FratMAE在预训练过程中捕获了复杂的跨模态关系和全局摄取模式，在下游任务中表现出优越的性能。这表明该模型具有作为通用基础模型的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过展示其在多模态PET/CT成像分析中的卓越能力，FratMAE提供了一种改进现有AI驱动癌症成像技术的新途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此处已将其内容精简并翻译成了中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) iswidely used in cancer diagnosis, staging, and treatment monitoring, as itcombines anatomical details from CT with functional metabolic activity andmolecular marker expression information from PET. However, existing artificialintelligence-driven PET/CT analyses rely predominantly on task-specific modelstrained from scratch or on limited datasets, limiting their generalizabilityand robustness. To address this, we propose a foundation model approachspecifically designed for multimodal PET/CT imaging. We introduce theCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework thateffectively integrates whole-body anatomical and functional or molecularinformation. FratMAE employs separate Vision Transformer (ViT) encoders for PETand CT scans, along with cross-attention decoders that enable synergisticinteractions between modalities during masked autoencoder training.Additionally, it incorporates textual metadata to enhance PET representationlearning. By pre-training on PET/CT datasets, FratMAE captures intricatecross-modal relationships and global uptake patterns, achieving superiorperformance on downstream tasks and demonstrating its potential as ageneralizable foundation model.</description>
      <author>example@mail.com (Yujin Oh, Robert Seifert, Yihan Cao, Christoph Clement, Justin Ferdinandus, Constantin Lapa, Alessandro Liebich, Michelle Amon, Johanna Enke, Sifan Song, Runqi Meng, Fang Zeng, Ning Guo, Xiang Li, Pedram Heidari, Axel Rominger, Kuangyu Shi, Quanzheng Li)</author>
      <guid isPermaLink="false">2503.02824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.02162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了X2CT-CLIP框架，通过在潜在空间中设计的三模态对齐机制将3D CT体积和放射学报告的知识转移到CXR编码器上，实现了从CT到CXR的跨模式知识转移。&lt;h4&gt;背景&lt;/h4&gt;虽然CT是诊断中的重要影像方式，但由于辐射暴露高且处理时间长，限制了其在大规模筛查中的应用。相比之下，胸部X光（CXR）更易于获取和安全，但现有模型主要关注于识别在CXR上容易观察到的疾病，而对于使用CT训练多异常分类的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从CT数据中转移知识至CXR上的方法，以改进基于CXR的疾病检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种三模态的知识迁移学习框架X2CT-CLIP，该框架通过将3D CT体积和放射学报告的信息转移到胸部X光图像上，实现了跨模式的知识传输。这种方法降低了模型训练时的计算负担，并且是首次尝试利用CXR实现多异常分类。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标记数据集上的广泛评估显示，与现有基准方法相比，本研究的方法在跨模态检索、少样本适应和外部验证方面表现更佳。这表明，在资源有限的情况下，使用CT知识增强的胸部X光可以作为一种高效且可行的疾病检测替代方案。&lt;h4&gt;结论&lt;/h4&gt;X2CT-CLIP框架成功地将CT的知识转移到了CXR上，并展示了其在多异常分类中的潜力和应用价值，尤其是在资源受限环境中具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computed tomography (CT) is a key imaging modality for diagnosis, yet itsclinical utility is marred by high radiation exposure and long turnaroundtimes, restricting its use for larger-scale screening. Although chestradiography (CXR) is more accessible and safer, existing CXR foundation modelsfocus primarily on detecting diseases that are readily visible on the CXR.Recently, works have explored training disease classification models onsimulated CXRs, but they remain limited to recognizing a single disease typefrom CT. CT foundation models have also emerged with significantly improveddetection of pathologies in CT. However, the generalized application ofCT-derived labels on CXR has remained illusive. In this study, we proposeX2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges themodality gap between CT and CXR while reducing the computational burden ofmodel training. Our approach is the first work to enable multi-abnormalityclassification in CT, using CXR, by transferring knowledge from 3D CT volumesand associated radiology reports to a CXR encoder via a carefully designedtri-modal alignment mechanism in latent space. Extensive evaluations on threemulti-label CT datasets demonstrate that our method outperformsstate-of-the-art baselines in cross-modal retrieval, few-shot adaptation, andexternal validation. These results highlight the potential of CXR, enrichedwith knowledge derived from CT, as a viable efficient alternative for diseasedetection in resource-limited settings.</description>
      <author>example@mail.com (Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh)</author>
      <guid isPermaLink="false">2503.02162v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data Settings</title>
      <link>http://arxiv.org/abs/2503.03202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种基于模型预测统计变异性动态调整对比损失权重的方法，以提高图像文本对齐在低数据环境下的表现。&lt;h4&gt;背景&lt;/h4&gt;训练视觉语言模型用于图像和文本的对齐通常需要大规模的数据集才能达到良好的性能。而在小规模数据集中，标准的对比学习方法由于过拟合和不稳定的学习动力学而难以有效对齐模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的损失调度方法，以解决低数据条件下现有对比学习算法的问题，并改善图像文本检索精度。&lt;h4&gt;方法&lt;/h4&gt;采用变差感知损失调度方法，该方法根据模型在预测中的统计变异（不确定性）动态调整对比损失的权重。使用Flickr8k图说数据集的一个子集来模拟有限的数据条件。&lt;h4&gt;主要发现&lt;/h4&gt;与固定权重基线相比，所提出的方法提高了图像文本检索准确性；与其他自适应加权策略（利用输出熵和余弦相似度分布）相比较时，变差感知调度提供最佳的总体折中；在噪声注入标题和图片的压力测试下，该方法表现出更高的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了在低数据环境下进行多模态对齐时自适应损失加权的优势。&lt;h4&gt;翻译&lt;/h4&gt;训练视觉-语言模型以实现图像与文本的对齐通常需要大量的数据集来达到稳健的表现。在少量数据的情况下，标准对比学习方法由于过拟合和不稳定的训练动力学而难以有效对齐模式。在这篇论文中，我们提出了一种感知变异性的损失调度方法，该方法根据模型预测中的统计变异性（不确定性）动态调整对比性损失的权重。使用Flickr8k图像描述数据集的一个子集来模拟有限的数据情况，我们展示了与固定权重基线相比，我们的方法可以提高图像-文本检索的准确性。此外，我们将该方法与其他自适应加权策略进行了比较（输出熵和余弦相似度分布），发现基于变差感知调度的方法提供了最佳的整体折中。通过t-SNE可视化显示了该方法生成了更为显著的多模态嵌入。并且在噪声注入标题和图像的压力测试下，该方法表现出了更高的鲁棒性，保持较高的召回率即使在随机干扰引入的情况下也是如此。这些结果强调了自适应损失加权对于低数据环境中实现多模态对齐的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training vision-language models for image-text alignment typically requireslarge datasets to achieve robust performance. In low-data scenarios, standardcontrastive learning can struggle to align modalities effectively due tooverfitting and unstable training dynamics. In this paper, we propose avariance-aware loss scheduling approach that dynamically adjusts the weightingof the contrastive loss based on the statistical variability (uncertainty) inthe model's alignment predictions. Using a subset of the Flickr8k image-captiondataset to simulate limited data conditions, we demonstrate that our approachimproves image-text retrieval accuracy compared to a fixed-weight baseline. Wealso compare against other adaptive weighting strategies (using output entropyand cosine similarity spread) and find that variance-aware scheduling providesthe best overall trade-off. Qualitatively, our method yields more distinctmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stresstest with noise-injected captions and images, the variance-guided loss provesmore robust, maintaining higher recall when random perturbations areintroduced. These results highlight the benefit of adaptive loss weighting formultimodal alignment in low-data regimes.</description>
      <author>example@mail.com (Sneh Pillai)</author>
      <guid isPermaLink="false">2503.03202v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Leap: Inductive Link Prediction via Learnable TopologyAugmentation</title>
      <link>http://arxiv.org/abs/2503.03331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  published in Machine Learning, Optimization, and Data Science,  Springer Nature Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于可学习拓扑增强的归纳链接预测方法LEAP，针对传统图神经网络在处理新节点时表达能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;链接预测是许多图机器学习下游应用的关键任务。虽然图神经网络（GNN）被广泛用于链接预测，在已有的节点间预测缺失链接的情况下尤其有效，但在实际应用场景中需要考虑新增节点的情况，即归纳设置下进行链接预测的需求日益增加。&lt;h4&gt;目的&lt;/h4&gt;为解决现有方法在新节点下的表达能力和捕捉结构信号不足的问题，提出了一种新的可学习拓扑增强的链接预测模型LEAP，该模型能够同时建模来自图结构和节点特征的归纳偏差。&lt;h4&gt;方法&lt;/h4&gt;提出了基于拓朴增强的新链接预测算法LEAP。它不同于以往仅关注节点表示的学习方法，而是通过可学习的方式为新节点提供结构性上下文，提高了表达能力。&lt;h4&gt;主要发现&lt;/h4&gt;在七种真实世界的同构和异构图上的广泛实验表明，与现有最优（SOTA）模型相比，LEAP的性能显著提升。特别是在AUC和平均精度方面分别提升了22%和17%&lt;h4&gt;结论&lt;/h4&gt;通过引入可学习拓朴增强技术，LEAP在归纳链接预测中表现出卓越的能力，为解决新节点加入图结构的问题提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;连接预测是许多图形机器学习下游应用中的关键任务。为此，图神经网络（GNN）被广泛用于连接预测，在推导设置下尤其有效，即在已知节点之间预测缺失的链接。然而，许多现实应用场景需要归纳设置来处理新节点加入现有图的情况。因此，最近归纳连接预测吸引了大量的关注，并且多层感知机（MLP）是大多数研究中学习节点表示的流行选择。但是这些方法的表达能力有限，未能完全捕捉图形的结构信号。为此，在这项工作中，我们提出了基于可学习拓扑增强的LEAP（LEArnable toPology augmentation），这是一种归纳链接预测的方法。与以往方法不同的是，LEAP从图的结构和节点特征两方面建模归纳偏差，因此更具表现力。据我们所知，这是首次尝试通过可学习增强为新节点提供结构上下文的方法。广泛的实验表明，在七种真实世界的同构和异构图上，LEAP显著优于现有最优方法（SOTA）。改进幅度分别达到了22%的AUC和17%的平均精度。代码和数据集可在GitHub上获得 (https://github.com/AhmedESamy/LEAP/)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-82481-4_31&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a crucial task in many downstream applications of graphmachine learning. To this end, Graph Neural Network (GNN) is a widely usedtechnique for link prediction, mainly in transductive settings, where the goalis to predict missing links between existing nodes. However, many real-lifeapplications require an inductive setting that accommodates for new nodes,coming into an existing graph. Thus, recently inductive link prediction hasattracted considerable attention, and a multi-layer perceptron (MLP) is thepopular choice of most studies to learn node representations. However, theseapproaches have limited expressivity and do not fully capture the graph'sstructural signal. Therefore, in this work we propose LEAP, an inductive linkprediction method based on LEArnable toPology augmentation. Unlike previousmethods, LEAP models the inductive bias from both the structure and nodefeatures, and hence is more expressive. To the best of our knowledge, this isthe first attempt to provide structural contexts for new nodes via learnableaugmentation in inductive settings. Extensive experiments on seven real-worldhomogeneous and heterogeneous graphs demonstrates that LEAP significantlysurpasses SOTA methods. The improvements are up to 22\% and 17\% in terms ofAUC and average precision, respectively. The code and datasets are available onGitHub (https://github.com/AhmedESamy/LEAP/)</description>
      <author>example@mail.com (Ahmed E. Samy, Zekarias T. Kefato, Sarunas Girdzijauskas)</author>
      <guid isPermaLink="false">2503.03331v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Do GFlowNets Transfer? Case Study on the Game of 24/42</title>
      <link>http://arxiv.org/abs/2503.01819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了GFlowNets在生成多样化解决方案方面的性能，并探讨其与自回归语言模型的差异。通过案例分析，在不同游戏任务中的表现发现GFlowNets难以保持解题多样性和准确性。&lt;h4&gt;背景&lt;/h4&gt;生成多样化答案对于类似人类的推理至关重要，但现有的自回归语言模型倾向于专注于提供单一准确的答案，从而限制了创造性。&lt;h4&gt;目的&lt;/h4&gt;评估GFlowNets在零样本跨任务泛化能力上的局限性，并探讨未来研究方向以提高其迁移学习能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调小型和中型的大型语言模型于“24游戏”并测试它们在“42游戏”数据集的表现来分析GFlowNets的行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，GFlowNets难以维持跨任务时解题的多样性和准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究揭示了GFlowNets在跨任务泛化能力上的关键限制，并强调需要进一步的研究以改进其迁移学习的能力。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要全文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating diverse solutions is key to human-like reasoning, yetautoregressive language models focus on single accurate responses, limitingcreativity. GFlowNets optimize solution generation as a flow network, promisinggreater diversity. Our case study shows their limited zero-shot transferabilityby fine-tuning small and medium-sized large language models on the Game of 24and testing them on the Game of 42 datasets. Results revealed that GFlowNetsstruggle to maintain solution diversity and accuracy, highlighting keylimitations in their cross-task generalization and the need for future researchin improved transfer learning capabilities.</description>
      <author>example@mail.com (Adesh Gupta, Abhinav Kumar, Mansi Gupta, Paras Chopra)</author>
      <guid isPermaLink="false">2503.01819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Model-Agnostic Meta-Policy Optimization via Zeroth-Order Estimation: A Linear Quadratic Regulator Perspective</title>
      <link>http://arxiv.org/abs/2503.00385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用元学习来处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了一种结合零阶优化技术和典型元学习方法的算法。&lt;h4&gt;背景&lt;/h4&gt;近年来，元学习作为机器学习的一个有前景的话题被提出，并在图像分类、机器人技术、计算机游戏和控制系统等领域有着重要的应用。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用元学习来解决ergodic线性二次调节器中的不确定性和异质性问题。&lt;h4&gt;方法&lt;/h4&gt;将零阶优化技术与典型元学习方法相结合，提出了一个算法，该算法省略了策略Hessian的估计，并适用于学习一组相似但异质性的线性动态系统任务。所提出的元目标函数继承了一组可元学习的线性动力系统的原始成本函数的重要特性。&lt;h4&gt;主要发现&lt;/h4&gt;提供了关于精确梯度下降过程的稳定性以及收敛保证，通过分析元目标函数的梯度有界性和局部平滑性来证明该算法的有效性，并给出了理论保证的样本复杂度条件。最后提供了一个数值例子以支持这一观点。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法能够处理异质性的线性动态系统集合，同时在优化过程中不需要投影到可行集上，并且通过理论分析提供了稳定性、收敛性和小梯度估计误差保证。&lt;h4&gt;翻译&lt;/h4&gt;元学习近年来被提出作为机器学习的有前景话题，在图像分类、机器人技术、计算机游戏和控制系统等领域有着广泛应用。本文探讨了使用元学习处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了结合零阶优化技术和典型元学习方法的新算法，该算法可以应用于学习一组相似但异质性的线性动态系统，并提供关于稳定性、收敛性和小梯度估计误差的理论保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning has been proposed as a promising machine learning topic inrecent years, with important applications to image classification, robotics,computer games, and control systems. In this paper, we study the problem ofusing meta-learning to deal with uncertainty and heterogeneity in ergodiclinear quadratic regulators. We integrate the zeroth-order optimizationtechnique with a typical meta-learning method, proposing an algorithm thatomits the estimation of policy Hessian, which applies to tasks of learning aset of heterogeneous but similar linear dynamic systems. The inducedmeta-objective function inherits important properties of the original costfunction when the set of linear dynamic systems are meta-learnable, allowingthe algorithm to optimize over a learnable landscape without projection ontothe feasible set. We provide stability and convergence guarantees for the exactgradient descent process by analyzing the boundedness and local smoothness ofthe gradient for the meta-objective, which justify the proposed algorithm withgradient estimation error being small. We provide the sample complexityconditions for these theoretical guarantees, as well as a numerical example atthe end to corroborate this perspective.</description>
      <author>example@mail.com (Yunian Pan, Tao Li, Quanyan Zhu)</author>
      <guid isPermaLink="false">2503.00385v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于电气阻抗断层成像(EIT)的多分辨率重建方法MR-EIT，可以在监督学习和无监督学习模式下操作。&lt;h4&gt;背景&lt;/h4&gt;当前EIT图像重建面临着如何在有限的数据输入情况下实现高精度、低噪声影响的挑战。现有的方法往往难以同时满足不同分辨率数据的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同规模输入序列并在低分辨率数据上生成高质量高分辨率EIT图像的方法。&lt;h4&gt;方法&lt;/h4&gt;MR-EIT结合了有序特征提取模块和无序坐标特征表达模块，前者通过预训练实现从电压到二维导电特性的映射；后者利用对称函数和局部特征提取机制实现了独立于输入序列顺序的多分辨率重建。在数据驱动模式下，该算法可以先进行两阶段的预训练，再进行联合训练以生成高分辨率图像。&lt;h4&gt;主要发现&lt;/h4&gt;MR-EIT不仅在模拟实验中表现出色，在无监督学习模式下的真实水箱实验中也展示了出色的鲁棒性和高效的超分辨率重建能力。特别是，在无监督学习模式下，该算法能显著减少迭代次数并提高成像质量。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在结构相似性(SSIM)和相对图像误差(RIE)方面，MR-EIT优于其他比较方法，尤其是在无监督学习模式下表现突出。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上述内容为其翻译及总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multi-resolution reconstruction method for ElectricalImpedance Tomography (EIT), referred to as MR-EIT, which is capable ofoperating in both supervised and unsupervised learning modes. MR-EIT integratesan ordered feature extraction module and an unordered coordinate featureexpression module. The former achieves the mapping from voltage totwo-dimensional conductivity features through pre-training, while the latterrealizes multi-resolution reconstruction independent of the order and size ofthe input sequence by utilizing symmetric functions and local featureextraction mechanisms. In the data-driven mode, MR-EIT reconstructshigh-resolution images from low-resolution data of finite element meshesthrough two stages of pre-training and joint training, and demonstratesexcellent performance in simulation experiments. In the unsupervised learningmode, MR-EIT does not require pre-training data and performs iterativeoptimization solely based on measured voltages to rapidly achieve imagereconstruction from low to high resolution. It shows robustness to noise andefficient super-resolution reconstruction capabilities in both simulation andreal water tank experiments. Experimental results indicate that MR-EIToutperforms the comparison methods in terms of Structural Similarity (SSIM) andRelative Image Error (RIE), especially in the unsupervised learning mode, whereit can significantly reduce the number of iterations and improve imagereconstruction quality.</description>
      <author>example@mail.com (Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong)</author>
      <guid isPermaLink="false">2503.00762v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Image Restoration and Super-resolution with Physics-Aware Deep Learning for Biomedical Applications</title>
      <link>http://arxiv.org/abs/2503.02908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于深度学习的方法，用于提高高光谱成像的空间分辨率和速度，同时保持生物样本的完整性。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些限制，本文提出了一种不需要额外训练数据即可恢复和提高像素分辨率的方法。&lt;h4&gt;方法&lt;/h4&gt;该深度学习模型经过微调，可以进行16倍的像素超分辨率增强并提升12倍的成像速度。这种方法不依赖于先验知识，并且使用与成像模型对齐的度量标准进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过应用于合成和实验数据，证明了该模型能够保持生物样本的完整性，不会丢失或产生虚假特征。此外，在唐氏综合症患者中发现了以前无法检测到的代谢变化。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅提供了一个提高高光谱成像质量和速度的有效工具，还提供了对模型物理机制的理解，并为未来改进奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。为了克服这些限制，我们提出了一种基于深度学习的方法，在不使用先验知识的情况下进行像素恢复和增强。经过与成像模型对齐的标准微调，我们的物理感知方法实现了16倍的像素超分辨率提升和12倍的成像速度提升，并不需要额外的迁移学习训练数据。应用于五种不同类型样品的合成和实验数据，我们证明了该模型保持生物完整性，确保没有特征丢失或产生虚幻特征。我们也具体展示了该模型揭示唐氏综合症相关代谢变化的能力，这些变化在其他情况下无法检测到。此外，我们提供了关于模型内部工作的物理见解，为未来可能超越仪器限制的改进铺平道路，并且所有方法都作为开源软件发布在GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging is a powerful bioimaging tool which can uncover novelinsights, thanks to its sensitivity to the intrinsic properties of materials.However, this enhanced contrast comes at the cost of system complexity,constrained by an inherent trade-off between spatial resolution, spectralresolution, and imaging speed. To overcome this limitation, we present a deeplearning-based approach that restores and enhances pixel resolutionpost-acquisition without any a priori knowledge. Fine-tuned using metricsaligned with the imaging model, our physics-aware method achieves a 16X pixelsuper-resolution enhancement and a 12X imaging speedup without the need ofadditional training data for transfer learning. Applied to both synthetic andexperimental data from five different sample types, we demonstrate that themodel preserves biological integrity, ensuring no features are lost orhallucinated. We also concretely demonstrate the model's ability to revealdisease-associated metabolic changes in Downs syndrome that would otherwiseremain undetectable. Furthermore, we provide physical insights into the innerworkings of the model, paving the way for future refinements that couldpotentially surpass instrumental limits in an explainable manner. All methodsare available as open-source software on GitHub.</description>
      <author>example@mail.com (Yuchen Xiang, Zhaolu Liu, Monica Emili Garcia-Segura, Daniel Simon, Boxuan Cao, Vincen Wu, Kenneth Robinson, Yu Wang, Ronan Battle, Robert T. Murray, Xavier Altafaj, Luca Peruzzotti-Jametti, Zoltan Takats)</author>
      <guid isPermaLink="false">2503.02908v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2503.03258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;随着大语言模型（LLMs）的兴起，人们对于图基础模型（GFMs）在基于图的任务中的兴趣日益增加。通过利用LLMs作为预测器，GFMs展示了跨越各种任务和数据集的强大泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的关于使用LLMs作为预测器的研究主要集中在静态图上，动态图预测的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;这项工作首次尝试将LLMs应用于动态图上的预测任务，并识别出两个关键挑战：处理大规模历史数据时上下文长度限制和领域特征差异性。&lt;h4&gt;方法&lt;/h4&gt;为了应对这些挑战，我们提出了GraphAgent-Dynamic (GAD)框架，这是一个多代理系统，利用协同工作的大语言模型。与使用单个LLM作为预测器不同，GAD整合了全局和局部摘要代理来生成特定领域的知识，增强其跨域迁移能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GAD的性能与完全监督的图神经网络相当甚至更优，并且不需要特定数据集的训练。此外，我们讨论了针对LLM基础预测器改进的潜在策略，例如对LLMs进行数据集特定的微调。&lt;h4&gt;结论&lt;/h4&gt;通过为不同任务制定定制化策略，该研究提供了对未来基于LLM预测器设计的新见解。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型（LLMs）的发展，人们对图基础模型（GFMs）在基于图的任务中的兴趣日益增加。利用LLMs作为预测器的GFMs已经在各种任务和数据集上展示了出色的泛化能力。然而，现有研究主要关注静态图上的预测问题，而动态图预测方面尚未被充分探索。本文首次尝试使用LLM进行动态图上的预测，并提出GraphAgent-Dynamic (GAD)框架来应对挑战。实验结果表明，该框架的性能优于或等同于完全监督下的图神经网络，同时提供了对基于LLMs的未来设计的新见解和改进策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large language models (LLMs), there has been growinginterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveragingLLMs as predictors, GFMs have demonstrated impressive generalizability acrossvarious tasks and datasets. However, existing research on LLMs as predictorshas predominantly focused on static graphs, leaving their potential in dynamicgraph prediction unexplored. In this work, we pioneer using LLMs for predictivetasks on dynamic graphs. We identify two key challenges: the constraintsimposed by context length when processing large-scale historical data and thesignificant variability in domain characteristics, both of which complicate thedevelopment of a unified predictor. To address these challenges, we propose theGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leveragescollaborative LLMs. In contrast to using a single LLM as the predictor, GADincorporates global and local summary agents to generate domain-specificknowledge, enhancing its transferability across domains. Additionally,knowledge reflection agents enable adaptive updates to GAD's knowledge,maintaining a unified and self-consistent architecture. In experiments, GADdemonstrates performance comparable to or even exceeds that of full-supervisedgraph neural networks without dataset-specific training. Finally, to enhancethe task-specific performance of LLM-based predictors, we discuss potentialimprovements, such as dataset-specific fine-tuning to LLMs. By developingtailored strategies for different tasks, we provide new insights for the futuredesign of LLM-based predictors.</description>
      <author>example@mail.com (Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong)</author>
      <guid isPermaLink="false">2503.03258v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improving internal cluster quality evaluation in noisy Gaussian mixtures</title>
      <link>http://arxiv.org/abs/2503.00379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的特征重要性重标(FIR)方法，用于改进内部聚类验证，并通过实验展示了其在处理高维或噪声数据集时的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的内部聚类验证指标（如平均轮廓宽度、Calinski-Harabasz和Davies-Bouldin指数）容易受到特征相关性的干扰，导致在无标签的复杂数据集中评估结果不可靠。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强基于内部验证指标的聚类质量评价，在没有外部标准的情况下提高算法的有效性。&lt;h4&gt;方法&lt;/h4&gt;FIR方法通过根据每个特征的数据分散情况调整其贡献值，系统地降低噪声特征的影响，从而使得聚类结果更加清晰和紧凑。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，无论是在有噪声还是不相关特征存在的数据集中，FIR都能显著提高内部验证指标与真实标签之间的关联性，并且在重叠较大的情况下仍能保持较高性能稳定性。&lt;h4&gt;结论&lt;/h4&gt;FIR作为一种改进的聚类评价技术，在没有标注的数据上进行无监督学习时具有很大的应用价值和潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering is a fundamental technique in machine learning and data analysis,widely used across various domains. Internal clustering validation measures,such as the Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldinindices, play a crucial role in assessing clustering quality when externalground truth labels are unavailable. However, these measures can be affected byfeature relevance, potentially leading to unreliable evaluations inhigh-dimensional or noisy data sets.  In this paper, we introduce a Feature Importance Rescaling (FIR) methoddesigned to enhance internal clustering validation by adjusting featurecontributions based on their dispersion. Our method systematically attenuatesnoise features making clustering compactness and separation clearer, and byconsequence aligning internal validation measures more closely with the groundtruth. Through extensive experiments on synthetic data sets under differentconfigurations, we demonstrate that FIR consistently improves the correlationbetween internal validation indices and the ground truth, particularly insettings with noisy or irrelevant features.  The results show that FIR increases the robustness of clustering evaluation,reduces variability in performance across different data sets, and remainseffective even when clusters exhibit significant overlap. These findingshighlight the potential of FIR as a valuable enhancement for internalclustering validation, making it a practical tool for unsupervised learningtasks where labelled data is not available.</description>
      <author>example@mail.com (Renato Cordeiro de Amorim, Vladimir Makarenkov)</author>
      <guid isPermaLink="false">2503.00379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA</title>
      <link>http://arxiv.org/abs/2503.02034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Abn-BLIP模型，该模型通过异常对齐技术提高了CTPA扫描的解读准确性和放射学报告的质量。&lt;h4&gt;背景&lt;/h4&gt;医学影像在现代医疗中扮演着重要角色，特别是用于诊断肺栓塞和胸部疾病的计算机断层摄影血管造影（CTPA）。然而，解析CTPA扫描并生成准确的放射学报告是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种改进的诊断模型，以提高对CTPA扫描异常检测、减少遗漏发现以及生成结构化报告的能力。&lt;h4&gt;方法&lt;/h4&gt;引入了Abn-BLIP（异常一致性自助学习语言-图像预训练）模型，该模型利用可学查询和跨模态注意机制来提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有技术相比，Abn-BLIP在准确性和临床相关性方面超越了最先进的医学视觉-语言模型和3D报告生成方法。&lt;h4&gt;结论&lt;/h4&gt;这些成果展示了多模式学习策略在改善放射学报告方面的潜力。模型源代码可从GitHub获取（https://github.com/zzs95/abn-blip）。&lt;h4&gt;翻译&lt;/h4&gt;医疗成像在现代医疗服务中发挥着关键作用，CTPA是诊断肺栓塞和其他胸部疾病的重要工具。然而，解读CTPA扫描并生成准确的放射学报告依然是一项挑战。为此，我们提出了一种名为Abn-BLIP（异常一致性自助学习语言-图像预训练）的高级诊断模型。该模型利用可学查询和跨模态注意机制来检测异常、减少遗漏发现，并生成结构化报告。实验表明，与现有方法相比，Abn-BLIP在准确性和临床相关性方面超过了最先进的医学视觉-语言模型和3D报告生成技术。这些结果强调了结合多模式学习策略以提高放射学报告能力的潜力。源代码可在GitHub上找到（https://github.com/zzs95/abn-blip）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging plays a pivotal role in modern healthcare, with computedtomography pulmonary angiography (CTPA) being a critical tool for diagnosingpulmonary embolism and other thoracic conditions. However, the complexity ofinterpreting CTPA scans and generating accurate radiology reports remains asignificant challenge. This paper introduces Abn-BLIP (Abnormality-alignedBootstrapping Language-Image Pretraining), an advanced diagnosis model designedto align abnormal findings to generate the accuracy and comprehensiveness ofradiology reports. By leveraging learnable queries and cross-modal attentionmechanisms, our model demonstrates superior performance in detectingabnormalities, reducing missed findings, and generating structured reportscompared to existing methods. Our experiments show that Abn-BLIP outperformsstate-of-the-art medical vision-language models and 3D report generationmethods in both accuracy and clinical relevance. These results highlight thepotential of integrating multimodal learning strategies for improving radiologyreporting. The source code is available at https://github.com/zzs95/abn-blip.</description>
      <author>example@mail.com (Zhusi Zhong, Yuli Wang, Lulu Bi, Zhuoqi Ma, Sun Ho Ahn, Christopher J. Mullin, Colin F. Greineder, Michael K. Atalay, Scott Collins, Grayson L. Baird, Cheng Ting Lin, Webster Stayman, Todd M. Kolb, Ihab Kamel, Harrison X. Bai, Zhicheng Jiao)</author>
      <guid isPermaLink="false">2503.02034v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosis of Patients with Viral, Bacterial, and Non-Pneumonia Based on Chest X-Ray Images Using Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于胸部X光图像的肺炎分类决策支持系统，利用迁移学习技术结合卷积神经网络模型，并引入了特征选择和降维方法，实现了对无肺炎患者、病毒性肺炎患者以及细菌性肺炎患者的准确区分。&lt;h4&gt;背景&lt;/h4&gt;世界卫生组织指出，肺炎是每年导致大量死亡的原因之一。因此需要开发有效的辅助决策系统以提高诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;通过使用预训练的卷积神经网络模型进行迁移学习，并结合特征选择和降维技术以及支持向量机分类器，构建一个能够准确区分无肺炎患者与病毒性或细菌性肺炎患者的决策支持系统。&lt;h4&gt;方法&lt;/h4&gt;实验采用了迁移学习（TL）技术利用预先训练好的卷积神经网络（CNN）模型处理胸部X光图像，并结合Relief和Chi-square降维技术以及支持向量机（SVM）进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;对于区分无肺炎患者与任何类型的肺炎患者，系统的准确率为91.02%，精度为97.73%，召回率为98.03%，F1得分为97.88%。对于区分病毒性肺炎和细菌性肺炎患者的分类任务中，系统达到了93.66%的准确率、94.26%的精度、92.66%的召回率及93.45%的F1得分。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，所提出的基于迁移学习和特征选择方法的支持向量机分类器在区分无肺炎与病毒性或细菌性肺炎患者的任务中表现出优异性能。该系统可以为临床医生提供有价值的辅助决策信息。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文译文：根据世界卫生组织的数据，每年有大量的死亡是由肺炎引起的疾病导致的。针对这一问题，提出了一种基于胸部X光图像进行患者分类的支持决策系统，能够将没有肺炎、病毒性肺炎和细菌性肺炎的患者区分开来。通过使用预训练的卷积神经网络模型实施迁移学习，并结合Relief与Chi-square方法作为降维技术以及支持向量机进行分类实现了这一目标。一系列实验的结果表明建立了一个区分无肺炎和肺炎患者的模型，其准确性为91.02%，精确度为97.73%，召回率为98.03%，F1得分为97.88%。此外，在区分病毒性肺炎与细菌性肺炎患者方面的准确率达到了93.66%，精度为94.26%，召回率为92.66%，F1得分为93.45%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the World Health Organization (WHO), pneumonia is a disease thatcauses a significant number of deaths each year. In response to this issue, thedevelopment of a decision support system for the classification of patientsinto those without pneumonia and those with viral or bacterial pneumonia isproposed. This is achieved by implementing transfer learning (TL) usingpre-trained convolutional neural network (CNN) models on chest x-ray (CXR)images. The system is further enhanced by integrating Relief and Chi-squaremethods as dimensionality reduction techniques, along with support vectormachines (SVM) for classification. The performance of a series of experimentswas evaluated to build a model capable of distinguishing between patientswithout pneumonia and those with viral or bacterial pneumonia. The obtainedresults include an accuracy of 91.02%, precision of 97.73%, recall of 98.03%,and an F1 Score of 97.88% for discriminating between patients without pneumoniaand those with pneumonia. In addition, accuracy of 93.66%, precision of 94.26%,recall of 92.66%, and an F1 Score of 93.45% were achieved for discriminatingbetween patients with viral pneumonia and those with bacterial pneumonia.</description>
      <author>example@mail.com (Carlos Arizmendi, Jorge Pinto, Alejandro Arboleda, Hernando González)</author>
      <guid isPermaLink="false">2503.02906v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Common indicators hurt armed conflict prediction</title>
      <link>http://arxiv.org/abs/2503.00265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用非洲地区的精细冲突数据，结合气候、地理、基础设施、经济、人口统计数据和人口结构等因素，利用无监督学习模型发现了三种主要的冲突类型。&lt;h4&gt;背景&lt;/h4&gt;现有研究未能充分探究大规模冲突与中小型冲突之间的区别及其形成因素。本研究试图通过分析详细的数据集来填补这一知识空白。&lt;h4&gt;目的&lt;/h4&gt;探索并分类不同规模的冲突，并探讨这些分类对预测冲突强度和持续时间的影响，同时评估常见指标在预测中的实用性。&lt;h4&gt;方法&lt;/h4&gt;利用无监督学习模型对非洲地区的精细冲突数据进行分析。该模型考虑了气候、地理、基础设施、经济和社会人口结构等多个维度的信息。&lt;h4&gt;主要发现&lt;/h4&gt;{'冲突类型': ['重大动乱', '局部冲突', '偶发和溢出事件'], '重大动乱特征': '在人口密集且拥有发达基础设施的平坦河岸地区传播', '局部冲突特征': '中等人口密度区域，经济与地理多样性高，通常局限于国境之内', '偶发性和溢出事件特征': '规模小，多发生在低人口密度、缺乏基础设施和经济条件差的地区'}&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，明确区分不同类型的冲突会降低对冲突强度和其他尺寸测量值（如死亡人数和持续时间）预测的准确性。此外，还开发了一种基于经验且自下而上的方法来识别冲突类型。&lt;h4&gt;翻译&lt;/h4&gt;大规模冲突与中小型冲突在哪些方面有所不同？为了解答这个问题，我们利用了非洲地区的精细冲突数据，并将其映射到气候、地理、基础设施、经济、人口统计数据和人口结构上。通过无监督学习模型，我们发现了三种主要的冲突类型，即“重大动乱”、“局部冲突”和“偶发性和溢出事件”。重大动乱多在人口密集且拥有发达基础设施的平坦河岸地区发生；局部冲突则发生在中等人口密度区域，经济与地理多样性高，并通常局限于国境之内。而偶发性及溢出事件规模较小，常见于低人口密度、缺乏基础设施和贫困地区的环境中。三种类型按顺序分层为因素，分别强调了人口、基础设施、经济状况和地理位置作为最具有区分性的指标。明确冲突的类型会降低对诸如死亡人数、冲突持续时间等冲突强度预测的准确性，并促使我们注意常用指标在预测中的有限效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Are big conflicts different from small or medium size conflicts? To answerthis question, we leverage fine-grained conflict data, which we map to climate,geography, infrastructure, economics, raw demographics, and demographiccomposition in Africa. With an unsupervised learning model, we find threeoverarching conflict types representing ``major unrest,'' ``local conflict,''and ``sporadic and spillover events.'' Major unrest predominantly propagatesaround densely populated areas with well-developed infrastructure and flat,riparian geography. Local conflicts are in regions of median populationdensity, are diverse socio-economically and geographically, and are oftenconfined within country borders. Finally, sporadic and spillover conflictsremain small, often in low population density areas, with little infrastructureand poor economic conditions. The three types stratify into a hierarchy offactors that highlights population, infrastructure, economics, and geography,respectively, as the most discriminative indicators. Specifying conflict typenegatively impacts the predictability of conflict intensity such as fatalities,conflict duration, and other measures of conflict size. The competitive effectis a general consequence of weak statistical dependence. Hence, we develop anempirical and bottom-up methodology to identify conflict types, knowledge ofwhich can hurt predictability and cautions us about the limited utility ofcommonly available indicators.</description>
      <author>example@mail.com (Niraj Kushwaha, Woi Sok Oh, Shlok Shah, Edward D. Lee)</author>
      <guid isPermaLink="false">2503.00265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DeepSuM: Deep Sufficient Modality Learning Framework</title>
      <link>http://arxiv.org/abs/2503.01728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的多模态选择框架，该框架独立学习每种模式的表示，并评估其在特定情境下的重要性。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是开发稳健的学习模型的关键方法，应用于多媒体、机器人技术、大型语言模型和医疗保健等领域。不同模式的成本与资源需求不一，因此需要有效选择以平衡性能提升和资源消耗之间的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来优化多模态集成和选择过程。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种独立学习每种模式表示的方法，并在此基础上评估了各模式的重要性，同时开发出适应不同特征的编码器。&lt;h4&gt;主要发现&lt;/h4&gt;此框架能够提高多模态学习效率与效果。&lt;h4&gt;结论&lt;/h4&gt;通过独立学习和优化多模式表示可以有效提升整体系统性能并节约资源。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习已成为发展稳健的学习模型的关键方法，其应用范围包括多媒体、机器人技术、大型语言模型以及医疗保健等领域。由于不同模式的成本与资源需求不一，因此高效利用各种模式成为了关键问题。这强调了有效选择模式的重要性，以平衡性能提升和资源消耗之间的关系。在这项研究中，我们提出了一种新的框架来优化多模态集成和选择过程，该方法独立学习每种模式的表示，并评估其在特定情境下的重要性，从而开发出适应不同特征的编码器，并促进具有独特特性的模式联合分析。我们的框架旨在通过优化模式整合与选择提高多模态学习效率与效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning has become a pivotal approach in developing robustlearning models with applications spanning multimedia, robotics, large languagemodels, and healthcare. The efficiency of multimodal systems is a criticalconcern, given the varying costs and resource demands of different modalities.This underscores the necessity for effective modality selection to balanceperformance gains against resource expenditures. In this study, we propose anovel framework for modality selection that independently learns therepresentation of each modality. This approach allows for the assessment ofeach modality's significance within its unique representation space, enablingthe development of tailored encoders and facilitating the joint analysis ofmodalities with distinct characteristics. Our framework aims to enhance theefficiency and effectiveness of multimodal learning by optimizing modalityintegration and selection.</description>
      <author>example@mail.com (Zhe Gao, Jian Huang, Ting Li, Xueqin Wang)</author>
      <guid isPermaLink="false">2503.01728v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Measurement noise scaling laws for cellular representation learning</title>
      <link>http://arxiv.org/abs/2503.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度学习扩展规律预测模型和数据集规模增大时性能如何提升。本文识别出数据中的测量噪声作为另一个影响性能的尺度，受独立的对数法则支配。&lt;h4&gt;背景&lt;/h4&gt;深度学习的扩展规则通常关注于模型大小及训练数据集的尺寸变化，而本文则引入了关于生物单细胞基因组数据中由于分子采样不足引起的主导性测量噪音来源的研究。&lt;h4&gt;目的&lt;/h4&gt;为了量化细胞表示模型质量，提出了一个信息论度量标准，并研究该度量与样本深度的关系。&lt;h4&gt;方法&lt;/h4&gt;通过多个模型类型和不同数据集验证了单一的定量关系。此外，还从简单的高斯噪声模型推导出了这种关系的形式。&lt;h4&gt;主要发现&lt;/h4&gt;测量噪音影响性能并遵循对数法则；提出了一个信息理论指标来衡量细胞表示模型的质量，并表明该指标随采样深度的变化而变化；发现了关于不同类型成像噪音的图像分类模型中存在相同的关系，暗示了测量噪声缩放可能是一个普遍的现象。&lt;h4&gt;结论&lt;/h4&gt;测量噪音可以作为生成和管理用于深度学习模型的数据的重要指南，特别是在数据质量在不同数据集间差异显著的领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning scaling laws predict how performance improves with increasedmodel and dataset size. Here we identify measurement noise in data as anotherperformance scaling axis, governed by a distinct logarithmic law. We focus onrepresentation learning models of biological single cell genomic data, where adominant source of measurement noise is due to molecular undersampling. Weintroduce an information-theoretic metric for cellular representation modelquality, and find that it scales with sampling depth. A single quantitativerelationship holds across several model types and across several datasets. Weshow that the analytical form of this relationship can be derived from a simpleGaussian noise model, which in turn provides an intuitive interpretation forthe scaling law. Finally, we show that the same relationship emerges in imageclassification models with respect to two types of imaging noise, suggestingthat measurement noise scaling may be a general phenomenon. Scaling with noisecan serve as a guide in generating and curating data for deep learning models,particularly in fields where measurement quality can vary dramatically betweendatasets.</description>
      <author>example@mail.com (Gokul Gowri, Peng Yin, Allon M. Klein)</author>
      <guid isPermaLink="false">2503.02726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Approach to Detecting Lung Nodules Using Swin Transformer</title>
      <link>http://arxiv.org/abs/2503.01592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19th Iranian Conference on Intelligent Systems (ICIS), IEEE, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基于2D CT切片的肺癌结节检测模型，以提高早期诊断效率。&lt;h4&gt;背景&lt;/h4&gt;肺癌是癌症致死率最高的疾病之一，而肺结节是常见的肺癌指示物。现有的多种肺结节检测模型在效率方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种更为高效的肺结节检测方法，减少计算量和复杂性，并提升小结节的检测能力。&lt;h4&gt;方法&lt;/h4&gt;采用Swin Transformer（轻量版）结合特征金字塔网络，利用迁移学习加速训练过程。该模型旨在继承视觉变换器的优势同时保持较低的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型在小结节的mAP和mAR上分别超出当前最佳方法1.3%和1.6%，整体表现优异，分别为94.7%mAP和94.9%mAR。&lt;h4&gt;结论&lt;/h4&gt;所提方法能够有效地提高肺部结节检测效率与精度，有助于早期肺癌诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICIS64839.2024.10887472&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung cancer has the highest rate of cancer-caused deaths, and early-stagediagnosis could increase the survival rate. Lung nodules are common indicatorsof lung cancer, making their detection crucial. Various lung nodule detectionmodels exist, but many lack efficiency. Hence, we propose a more efficientapproach by leveraging 2D CT slices, reducing computational load and complexityin training and inference. We employ the tiny version of Swin Transformer tobenefit from Vision Transformers (ViT) while maintaining low computationalcost. A Feature Pyramid Network is added to enhance detection, particularly forsmall nodules. Additionally, Transfer Learning is used to accelerate training.Our experimental results show that the proposed model outperformsstate-of-the-art methods, achieving higher mAP and mAR for small nodules by1.3% and 1.6%, respectively. Overall, our model achieves the highest mAP of94.7% and mAR of 94.9%.</description>
      <author>example@mail.com (Saeed Shakuri, Alireza Rezvanian)</author>
      <guid isPermaLink="false">2503.01592v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Shared Encoder Approach to Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.01654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;多模态表示学习在处理和融合文本、图像等多样化数据方面展现出巨大潜力，特别是在医学领域可以得到显著应用。然而，缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习能够帮助模型更好地理解和处理不同类型的数据（如文本和图像），从而提高性能。在医学领域中，这种技术可以获得重大应用，但当前存在缺乏足够的多模态配对数据和过度依赖于专有或预训练编码器的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于医疗领域的共享编码器框架，以解决缺乏配对的多模态数据以及依赖专有或预训练编码器带来的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种单一编码器参数集，在不同模式之间共享，并通过可学习的模态特征进行增强。这种方法能够适应不同的医疗领域应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果显示，与独立的特定模态编码器相比，本研究提出的共享编码器框架在数据受限的情况下具有更好的泛化能力。尤其是当训练样本较少时，性能提升尤为显著。&lt;h4&gt;结论&lt;/h4&gt;该工作表明，在医学应用的实际场景中（特别是面对有限的数据条件），所提出的共享编码器框架比现有的特定模态编码器更加高效且具备更高的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;多模态表示学习已经显示出了处理多样化数据模式的巨大潜力，如文本和图像，并能改善理解和性能。虽然医学领域可以从这种范式中获益良多，但缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了显著挑战。本研究提出了一种适用于医疗领域的共享编码器框架，该方法采用单一集编码参数在不同模式间共用，并辅以可学习的模态特征增强。实验结果表明，在数据受限的情况下，本框架能够超越特定模态编码器，特别是在较少训练样本时性能提升尤为显著。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vectorinstitute/shared_encoder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning has demonstrated remarkable potential inenabling models to process and integrate diverse data modalities, such as textand images, for improved understanding and performance. While the medicaldomain can benefit significantly from this paradigm, the scarcity of pairedmultimodal data and reliance on proprietary or pretrained encoders posesignificant challenges. In this work, we present a shared encoder framework formultimodal representation learning tailored to the medical domain. Our approachemploys a single set of encoder parameters shared across modalities, augmentedwith learnable modality features. Empirical results demonstrate that our sharedencoder idea achieves superior performance compared to separatemodality-specific encoders, demonstrating improved generalization indata-constrained settings. Notably, the performance gains are more pronouncedwith fewer training examples, underscoring the efficiency of our shared encoderframework for real-world medical applications with limited data. Our code andexperiment setup are available athttps://github.com/VectorInstitute/shared_encoder.</description>
      <author>example@mail.com (Shuvendu Roy, Franklin Ogidi, Ali Etemad, Elham Dolatabadi, Arash Afkanpour)</author>
      <guid isPermaLink="false">2503.01654v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Distilled Prompt Learning for Incomplete Multimodal Survival Prediction</title>
      <link>http://arxiv.org/abs/2503.01653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;多模态数据集（包括病理图像和基因组信息）在精准生存预测中的应用广泛。然而，尽管近年来多模态生存模型取得了进展，在临床环境中收集完整的多模态数据仍然是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对因缺乏完整模态导致的局限性问题，本文提出了一种利用大型语言模型（LLMs）鲁棒性的蒸馏提示学习框架（DisPro），以弥补缺失模态的信息。&lt;h4&gt;方法&lt;/h4&gt;提出的框架包含两个阶段：首先通过单模态提示将各模态的知识分布提炼出来；接着在多模态提示下，使用现有数据作为提示来推断丢失的数据，并注入第一阶段获取的单一模式知识以补充特定模态信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验展示了该方法在处理各种缺失场景下的优越性。通过两个阶段的提示学习，能够更好地填补因缺少某些模态而导致的信息缺口。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了一种新的策略来改善多模态生存预测模型中的数据完整性问题，并为进一步的应用提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;将病理图像和基因组信息等多模态数据应用于精确的生存预测中非常普遍。尽管最近在多模态生存模型方面有所进展，但在临床环境中收集完整的多模式数据仍然是一项挑战。目前处理不完整模式的方法通常只弥补了缺失模式知识的一部分，并不足以完全补偿损失的知识。为了应对这一问题，我们提出了一种基于大型语言模型（LLMs）的蒸馏提示学习框架(DisPro)，该框架利用两个阶段的提示来补充丢失模态的全面信息。第一阶段是单模态提示，它提炼了各模式中的知识分布；第二阶段为多模态提示，使用现有数据作为提示来推断缺失的数据，并且在多模态推理中注入第一阶段获得的单一模态知识，以补偿特定模态的信息损失。广泛的实验验证显示所提出的框架在各种不完整场景下的优越性。相关代码已公开（见 https://github.com/Innse/DisPro）&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of multimodal data including pathology images and geneprofiles is widely applied in precise survival prediction. Despite recentadvances in multimodal survival models, collecting complete modalities formultimodal fusion still poses a significant challenge, hindering theirapplication in clinical settings. Current approaches tackling incompletemodalities often fall short, as they typically compensate for only a limitedpart of the knowledge of missing modalities. To address this issue, we proposea Distilled Prompt Learning framework (DisPro) to utilize the strong robustnessof Large Language Models (LLMs) to missing modalities, which employs two-stageprompting for compensation of comprehensive information for missing modalities.In the first stage, Unimodal Prompting (UniPro) distills the knowledgedistribution of each modality, preparing for supplementing modality-specificknowledge of the missing modality in the subsequent stage. In the second stage,Multimodal Prompting (MultiPro) leverages available modalities as prompts forLLMs to infer the missing modality, which provides modality-common information.Simultaneously, the unimodal knowledge acquired in the first stage is injectedinto multimodal inference to compensate for the modality-specific knowledge ofthe missing modality. Extensive experiments covering various missing scenariosdemonstrated the superiority of the proposed method. The code is available athttps://github.com/Innse/DisPro.</description>
      <author>example@mail.com (Yingxue Xu, Fengtao Zhou, Chenyu Zhao, Yihui Wang, Can Yang, Hao Chen)</author>
      <guid isPermaLink="false">2503.01653v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</title>
      <link>http://arxiv.org/abs/2503.03222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Mocap-2-to-3的框架，该框架旨在从单目视角恢复世界坐标系中的绝对姿势。通过将复杂的3D运动分解为2D姿态，并利用大规模的2D数据改进3D运动重建和预测人的绝对位置。&lt;h4&gt;背景&lt;/h4&gt;从单个视图中恢复世界坐标系下的绝对姿势面临两个主要问题：一是现有的方法需要依靠在有限环境中收集的3D运动数据进行训练，二是从单一视角估计一个人在度量空间中的绝对位置更为复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架Mocap-2-to-3来解决上述挑战，并提高模型在不同场景下的泛化能力和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的框架Mocap-2-to-3，该框架利用大量易于获取的2D姿态数据进行单视角扩散模型的预训练和多视图扩散模型的微调。此外，还提出了一种新颖的人体运动表示方式，将局部动作与全局运动分离，并编码地面几何先验。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实世界的数据集上对模型性能进行了评估，证明了该方法相比于现有最佳方法，在运动和绝对人体定位方面的准确性更高，且泛化性和可扩展性更强。&lt;h4&gt;结论&lt;/h4&gt;提出的Mocap-2-to-3框架展示了从单目视角恢复世界坐标系中绝对姿势的有效性，并有望在各种应用领域得到广泛使用。&lt;h4&gt;翻译&lt;/h4&gt;论文研究的是通过一种创新的框架利用大规模的2D数据来解决单目视图下恢复世界坐标系统中的绝对姿态所面临的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering absolute poses in the world coordinate system from monocular viewspresents significant challenges. Two primary issues arise in this context.Firstly, existing methods rely on 3D motion data for training, which requirescollection in limited environments. Acquiring such 3D labels for new actions ina timely manner is impractical, severely restricting the model's generalizationcapabilities. In contrast, 2D poses are far more accessible and easier toobtain. Secondly, estimating a person's absolute position in metric space froma single viewpoint is inherently more complex. To address these challenges, weintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motionsinto 2D poses, leveraging 2D data to enhance 3D motion reconstruction indiverse scenarios and accurately predict absolute positions in the worldcoordinate system. We initially pretrain a single-view diffusion model withextensive 2D data, followed by fine-tuning a multi-view diffusion model forview consistency using publicly available 3D data. This strategy facilitatesthe effective use of large-scale 2D data. Additionally, we propose aninnovative human motion representation that decouples local actions from globalmovements and encodes geometric priors of the ground, ensuring the generativemodel learns accurate motion priors from 2D data. During inference, this allowsfor the gradual recovery of global movements, resulting in more plausiblepositioning. We evaluate our model's performance on real-world datasets,demonstrating superior accuracy in motion and absolute human positioningcompared to state-of-the-art methods, along with enhanced generalization andscalability. Our code will be made publicly available.</description>
      <author>example@mail.com (Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou)</author>
      <guid isPermaLink="false">2503.03222v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Dementia Insights: A Context-Based MultiModal Approach</title>
      <link>http://arxiv.org/abs/2503.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于上下文的多模态方法，用于识别痴呆症，这种方法利用了最先进的大型预训练模型（LPM）来分析文本和音频数据。&lt;h4&gt;背景&lt;/h4&gt;痴呆是一种进行性神经退行性疾病，影响记忆、推理和日常功能。早期检测对于及时干预以减缓疾病进展至关重要。现有的研究通常依赖于专家标注的数据集，并且采用单一模态的方法，这限制了其鲁棒性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于上下文的多模态方法来改进痴呆症的检测性能。&lt;h4&gt;方法&lt;/h4&gt;该研究整合了文本和音频数据的最佳表现大型预训练模型（LPM），如GPT、BERT和CLAP，并通过结合上下文嵌入进一步提高了检测效果。此外，还尝试了一种基于上下文的In-Context Learning (ICL)作为补充技术。&lt;h4&gt;主要发现&lt;/h4&gt;使用GPT生成的嵌入与CLAP音频特征融合时，在F1分数上达到83.33%，优于当前最先进的痴呆症检测模型；未经标注的原始文本数据表现优于专家标注的数据集，表明大型预训练模型可以在无需大量手动标记的情况下提取有意义的语言和声学模式。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了开发大规模、非侵入性诊断工具以减少对昂贵注释依赖的可能性，并且保持高精度。通过将多模态学习与上下文嵌入相结合，这项工作为未来痴呆症个性化检测及认知健康的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于使用大型预训练模型进行基于文本和音频数据的痴呆症早期识别研究，提出了一种结合这两种模式并利用上下文信息的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dementia, a progressive neurodegenerative disorder, affects memory,reasoning, and daily functioning, creating challenges for individuals andhealthcare systems. Early detection is crucial for timely interventions thatmay slow disease progression. Large pre-trained models (LPMs) for text andaudio, such as Generative Pre-trained Transformer (GPT), Bidirectional EncoderRepresentations from Transformers (BERT), and Contrastive Language-AudioPretraining (CLAP), have shown promise in identifying cognitive impairments.However, existing studies generally rely heavily on expert-annotated datasetsand unimodal approaches, limiting robustness and scalability. This studyproposes a context-based multimodal method, integrating both text and audiodata using the best-performing LPMs in each modality. By incorporatingcontextual embeddings, our method improves dementia detection performance.Additionally, motivated by the effectiveness of contextual embeddings, wefurther experimented with a context-based In-Context Learning (ICL) as acomplementary technique. Results show that GPT-based embeddings, particularlywhen fused with CLAP audio features, achieve an F1-score of $83.33\%$,surpassing state-of-the-art dementia detection models. Furthermore, raw textdata outperforms expert-annotated datasets, demonstrating that LPMs can extractmeaningful linguistic and acoustic patterns without extensive manual labeling.These findings highlight the potential for scalable, non-invasive diagnostictools that reduce reliance on costly annotations while maintaining highaccuracy. By integrating multimodal learning with contextual embeddings, thiswork lays the foundation for future advancements in personalized dementiadetection and cognitive health research.</description>
      <author>example@mail.com (Sahar Sinene Mehdoui, Abdelhamid Bouzid, Daniel Sierra-Sosa, Adel Elmaghraby)</author>
      <guid isPermaLink="false">2503.01226v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Foundation Models for Environmental Science</title>
      <link>http://arxiv.org/abs/2503.03142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基础模型在环境科学中的应用，涵盖了从数据生成到决策制定的多个方面。&lt;h4&gt;背景&lt;/h4&gt;对环境生态系统的建模对于资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理这些系统内在复杂性、相互联系以及有限的数据时常常遇到困难。&lt;h4&gt;目的&lt;/h4&gt;本综述旨在提供基础模型应用于环境科学领域的全面概述，并强调其在前向预测、数据生成、数据同化、降尺度、模型集成和跨域决策制定等方面的进展。&lt;h4&gt;方法&lt;/h4&gt;该论文详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型通过大规模预训练和通用表示能力，在整合多种数据源、捕捉时空依赖关系以及适应广泛任务方面提供了革命性的机会。&lt;h4&gt;结论&lt;/h4&gt;展示这些新兴方法的目的是促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;h4&gt;翻译&lt;/h4&gt;建模环境生态系统对于有效的资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理系统的固有复杂性、相互关联性和有限数据时常常面临挑战。基础模型通过大规模预训练和通用表示能力提供了一种变革性的机会，它能够整合多种数据源，捕捉时空依赖关系，并适应广泛的任务范围。这篇综述全面概述了基础模型在环境科学中的应用，突出了前向预测、数据生成、数据同化、降尺度、模型集成以及跨域决策制定等方面的进展。此外，论文还详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。通过展示这些新兴方法，我们旨在促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling environmental ecosystems is essential for effective resourcemanagement, sustainable development, and understanding complex ecologicalprocesses. However, traditional methods frequently struggle with the inherentcomplexity, interconnectedness, and limited data of such systems. Foundationmodels, with their large-scale pre-training and universal representations,offer transformative opportunities by integrating diverse data sources,capturing spatiotemporal dependencies, and adapting to a broad range of tasks.This survey presents a comprehensive overview of foundation model applicationsin environmental science, highlighting advancements in forward prediction, datageneration, data assimilation, downscaling, model ensembling, anddecision-making across domains. We also detail the development process of thesemodels, covering data collection, architecture design, training, tuning, andevaluation. By showcasing these emerging methods, we aim to fosterinterdisciplinary collaboration and advance the integration of cutting-edgemachine learning for sustainable solutions in environmental science.</description>
      <author>example@mail.com (Runlong Yu, Shengyu Chen, Yiqun Xie, Xiaowei Jia)</author>
      <guid isPermaLink="false">2503.03142v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-Based Spatio-Temporal Association of Apple Fruitlets</title>
      <link>http://arxiv.org/abs/2503.03200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于变压器的方法，用于在不同天数和相机姿态下采集的立体图像中时空关联苹果果蕾。&lt;h4&gt;背景&lt;/h4&gt;现有的农业领域最先进的关联方法专注于匹配较大的农作物，使用高分辨率点云或时间稳定的特征，但这些对于野外较小的水果来说难以获取。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于变压器架构的方法，以解决为较小的水果获取和匹配问题。&lt;h4&gt;方法&lt;/h4&gt;采用了一种基于变压器的架构来编码每个果蕾的形状和位置，并通过一系列交替使用自注意力和交叉注意力的变压器编码层传播和细化这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在商业苹果园采集的数据上实现了92.4%的F1分数，优于所有基线和消融研究结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法展示了其在时空关联小水果上的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种基于变压器的方法，在不同时间点从不同相机角度收集的立体图像中对苹果果蕾进行时空关联。农业领域的现有方法主要针对较大的农作物，通过高分辨率点云或长时间稳定的特征实现匹配，但对于较小的果实来说这些数据难以获得。为解决这些问题，提出了一种编码每个果蕾形状和位置、并用一系列交替使用的自注意力和交叉注意层传播和细化这些特性的变压器架构。实验表明，在商业苹果园的数据上，该方法实现了92.4%的F1分数，并优于所有基线和消融研究结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a transformer-based method to spatio-temporallyassociate apple fruitlets in stereo-images collected on different days and fromdifferent camera poses. State-of-the-art association methods in agriculture arededicated towards matching larger crops using either high-resolution pointclouds or temporally stable features, which are both difficult to obtain forsmaller fruit in the field. To address these challenges, we propose atransformer-based architecture that encodes the shape and position of eachfruitlet, and propagates and refines these features through a series oftransformer encoder layers with alternating self and cross-attention. Wedemonstrate that our method is able to achieve an F1-score of 92.4% on datacollected in a commercial apple orchard and outperforms all baselines andablations.</description>
      <author>example@mail.com (Harry Freeman, George Kantor)</author>
      <guid isPermaLink="false">2503.03200v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeReg: Mitigating the Imbalance and Distribution Shift Effects in Semi-Supervised Node Classification via Norm Consistency</title>
      <link>http://arxiv.org/abs/2503.03211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为NodeReg的正则化优化方法，用于确保节点表示的范数一致性，以减少不平衡邻居和噪声对图神经网络（GNN）性能的影响。&lt;h4&gt;背景&lt;/h4&gt;在半监督节点分类任务中，聚合来自邻近节点的信息虽然有助于提高GNN性能，但也使得节点容易受到其邻居节点影响。例如，在邻居节点不均衡或包含噪音的情况下，这会影响GNN的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;通过确保节点表示范数的一致性来减少不平衡和噪声对GNN的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为NodeReg的方法，该方法是一种正则化的优化方式，用于强制执行节点表示范数的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;NodeReg能够显著提升在不均衡数据集和分布变化情况下的半监督节点分类性能。例如，在不平衡场景中，对于失衡比为0.1的GCN模型训练时，该方法能提高F1分数1.4%-25.9%；同样，在分布变化场景下，NodeReg提高了准确率1.4%-3.1%。&lt;h4&gt;结论&lt;/h4&gt;提出的方法简单但有效，并满足Lipschitz连续性条件，有助于稳定优化过程并在半监督节点分类任务中取得显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aggregating information from neighboring nodes benefits graph neural networks(GNNs) in semi-supervised node classification tasks. Nevertheless, thismechanism also renders nodes susceptible to the influence of their neighbors.For instance, this will occur when the neighboring nodes are imbalanced or theneighboring nodes contain noise, which can even affect the GNN's ability togeneralize out of distribution. We find that ensuring the consistency of thenorm for node representations can significantly reduce the impact of these twoissues on GNNs. To this end, we propose a regularized optimization methodcalled NodeReg that enforces the consistency of node representation norms. Thismethod is simple but effective and satisfies Lipschitz continuity, thusfacilitating stable optimization and significantly improving semi-supervisednode classification performance under the above two scenarios. To illustrate,in the imbalance scenario, when training a GCN with an imbalance ratio of 0.1,NodeReg outperforms the most competitive baselines by 1.4%-25.9% in F1 scoreacross five public datasets. Similarly, in the distribution shift scenario,NodeReg outperforms the most competitive baseline by 1.4%-3.1% in accuracy.</description>
      <author>example@mail.com (Shenzhi Yang, Jun Xia, Jingbo Zhou, Xingkai Yao, Xiaofang Zhang)</author>
      <guid isPermaLink="false">2503.03211v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation</title>
      <link>http://arxiv.org/abs/2503.01175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025. See https://star-uu-wang.github.io/HOP/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为HOP的新型多模态学习方法，用于生成协调的手势动作。&lt;h4&gt;背景&lt;/h4&gt;共话语手势是非言语线索，在人类交流中增强语音清晰度和表现力方面至关重要。现有的研究方法在提高手势准确性上取得了进展，但在生成多样化且连贯的动作上仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够捕捉手势运动、音频节奏和文本语义之间异构缠结的新多模态学习方法。&lt;h4&gt;方法&lt;/h4&gt;利用时空图建模实现音频与动作的对齐，并通过重新编程模块构建增强模态一致性的音频-文本语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;HOP在生成自然、表达性强的手势方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了多模态模型如何有效地学习不同特征并代表它们的形式，从而实现更加协调和连贯的共话语手势。&lt;h4&gt;翻译&lt;/h4&gt;共话语手势是提升人类交流中语音清晰度与表现力的重要非语言线索。尽管现有的方法在提高手势准确性方面取得了一定进展，但生成多样化且连贯的手势动作仍具挑战性。该研究提出一种捕捉手势运动、音频节奏和文本语义之间异构缠结的多模态学习方法HOP，并通过时空图建模实现对齐。实验显示，HOP在自然与表达性强的手势生成方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Co-speech gestures are crucial non-verbal cues that enhance speech clarityand expressiveness in human communication, which have attracted increasingattention in multimodal research. While the existing methods have made stridesin gesture accuracy, challenges remain in generating diverse and coherentgestures, as most approaches assume independence among multimodal inputs andlack explicit modeling of their interactions. In this work, we propose a novelmultimodal learning method named HOP for co-speech gesture generation thatcaptures the heterogeneous entanglement between gesture motion, audio rhythm,and text semantics, enabling the generation of coordinated gestures. Byleveraging spatiotemporal graph modeling, we achieve the alignment of audio andaction. Moreover, to enhance modality coherence, we build the audio-textsemantic representation based on a reprogramming module, which is beneficialfor cross-modality adaptation. Our approach enables the trimodal system tolearn each other's features and represent them in the form of topologicalentanglement. Extensive experiments demonstrate that HOP achievesstate-of-the-art performance, offering more natural and expressive co-speechgesture generation. More information, codes, and demos are available here:https://star-uu-wang.github.io/HOP/</description>
      <author>example@mail.com (Hongye Cheng, Tianyu Wang, Guangsi Shi, Zexing Zhao, Yanwei Fu)</author>
      <guid isPermaLink="false">2503.01175v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Binary Classification Social Network Dataset for Graph Machine Learning</title>
      <link>http://arxiv.org/abs/2503.02397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Binary Classification Social Network Dataset (BiSND)，这是一个为图机器学习设计的数据集，用于预测二元分类。&lt;h4&gt;背景&lt;/h4&gt;目前可用的基准数据集包括引文、共现和电子商务网络等，但没有专门针对社会网络进行分类任务的基准数据集。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究中的空白，提供一个适用于图机器学习应用的社会网络分类数据集。&lt;h4&gt;方法&lt;/h4&gt;BiSND以表格和图形格式呈现，并使用包括传统机器学习算法、深层神经网络、图神经网络以及最新的图对比学习方法在内的多样化分类器进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，BiSND对于二元分类任务具有适用性，在F1分数上表现出从67.66到70.15的性能，这为未来的改进提供了可能性。&lt;h4&gt;结论&lt;/h4&gt;BiSND证明了其在传统和先进机器学习方法中的稳健性和有效性，并为未来的研究开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social networks have a vast range of applications with graphs. The availablebenchmark datasets are citation, co-occurrence, e-commerce networks, etc, withclasses ranging from 3 to 15. However, there is no benchmark classificationsocial network dataset for graph machine learning. This paper fills the gap andpresents the Binary Classification Social Network Dataset (\textit{BiSND}),designed for graph machine learning applications to predict binary classes. Wepresent the BiSND in \textit{tabular and graph} formats to verify itsrobustness across classical and advanced machine learning. We employ a diverseset of classifiers, including four traditional machine learning algorithms(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep NeuralNetwork (multi-layer perceptrons), one Graph Neural Network (GraphConvolutional Network), and three state-of-the-art Graph Contrastive Learningmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable forclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicatingpromising avenues for future enhancements.</description>
      <author>example@mail.com (Adnan Ali, Jinglong Li, Huanhuan Chen, AlMotasem Bellah Al Ajlouni)</author>
      <guid isPermaLink="false">2503.02397v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models</title>
      <link>http://arxiv.org/abs/2503.01169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种用于检测暂时性冲沟的自动化管道，并通过公开数据集进行了验证，这些管道基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。&lt;h4&gt;背景&lt;/h4&gt;暂时性冲沟是土壤侵蚀的主要原因之一，现有的研究未能成功地实现从遥感图像中自动检测暂时性冲沟。&lt;h4&gt;目的&lt;/h4&gt;开发和评估用于检测暂时性冲沟的有效管道，并提供首个公开的数据集进行测试。&lt;h4&gt;方法&lt;/h4&gt;利用特定农业区域在一段时间内获取的遥感图像，通过多种视觉语言模型（VLM）对存在暂时性冲沟的图像进行分类。同时采用了零样本分类方法以及与迁移学习方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的管道能够有效地检测到缺乏充分标注数据集下的暂时性冲沟，准确率超过70%，F1分数接近80%。&lt;h4&gt;结论&lt;/h4&gt;该研究为自动化临时冲沟的识别提供了一种有效的方法，并通过实验验证了其在实际应用中的潜力。此外，开发了一个公开的数据集，以促进未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了用于检测暂时性冲沟的三种管道，这些管道利用特定农业区域在一段时间内获取的遥感图像，并基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。该研究开发并测试了首个公开的数据集，同时与迁移学习方法进行了比较，实验结果表明所提出的零样本分类管道在缺乏充分标注数据集的情况下能够有效检测暂时性冲沟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ephemeral gullies are a primary cause of soil erosion and their reliable,accurate, and early detection will facilitate significant improvements in thesustainability of global agricultural systems. In our view, prior research hasnot successfully addressed automated detection of ephemeral gullies fromremotely sensed images, so for the first time, we present and evaluate threesuccessful pipelines for ephemeral gully detection. Our pipelines utilizeremotely sensed images, acquired from specific agricultural areas over a periodof time. The pipelines were tested with various choices of Visual LanguageModels (VLMs), and they classified the images based on the presence ofephemeral gullies with accuracy higher than 70% and a F1-score close to 80% forpositive gully detection. Additionally, we developed the first public datasetfor ephemeral gully detection, labeled by a team of soil- and plant-scienceexperts. To evaluate the proposed pipelines, we employed a variety of zero-shotclassification methods based on State-of-the-Art (SOTA) open-sourceVision-Language Models (VLMs). In addition to that, we compare the samepipelines with a transfer learning approach. Extensive experiments wereconducted to validate the detection pipelines and to analyze the impact ofhyperparameter changes in their performance. The experimental resultsdemonstrate that the proposed zero-shot classification pipelines are highlyeffective in detecting ephemeral gullies in a scenario where classificationdatasets are scarce.</description>
      <author>example@mail.com (Seyed Mohamad Ali Tousi, Ramy Farag, Jacket Demby's, Gbenga Omotara, John A. Lory, G. N. DeSouza)</author>
      <guid isPermaLink="false">2503.01169v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</title>
      <link>http://arxiv.org/abs/2503.03190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D问答（3D QA）要求模型全面理解由文本描述的所处三维场景，并在此基础上回答有关周围环境的问题。然而，现有方法通常依赖于纯粹基于点云的全局场景感知，忽视了多视角图像中丰富局部纹理细节的重要性。&lt;h4&gt;背景&lt;/h4&gt;现有的3D QA方法主要依靠纯3D点云进行全局场景感知，而忽略了从多视角图片获取的详细文本描述信息和复杂遮挡下相机姿态导致的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够综合集成多视角和点云特征的方法，以提高在3D问答中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;{'Text-guided Multi-view Fusion (TGMF)模块': '优先考虑与文本语义内容紧密匹配的图像视图', 'Adaptive Dual-vision Perception (ADVP)模块': '设计用于自适应融合反投影多视角图像和点云特征，增强对3D场景的理解。', 'Multimodal Context-guided Reasoning (MCGR)模块': '通过整合视觉和语言模态的上下文信息来促进鲁棒推理'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在SQA3D和ScanQA数据集上的DSPNet优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;我们的方法能有效解决现有3D QA中存在的问题，提高场景理解和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了一种新的Dual-vision Scene Perception Network (DSPNet)，通过综合多视角图像与点云特征来改善3D QA任务中的鲁棒性。研究设计了三个关键模块：TGMF、ADVP和MCGR，以解决现有方法在文本理解、视图融合及上下文推理上的不足，并展示了其在两个基准数据集上优越的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LZ-CH/DSPNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Question Answering (3D QA) requires the model to comprehensivelyunderstand its situated 3D scene described by the text, then reason about itssurrounding environment and answer a question under that situation. However,existing methods usually rely on global scene perception from pure 3D pointclouds and overlook the importance of rich local texture details frommulti-view images. Moreover, due to the inherent noise in camera poses andcomplex occlusions, there exists significant feature degradation and reducedfeature robustness problems when aligning 3D point cloud with multi-viewimages. In this paper, we propose a Dual-vision Scene Perception Network(DSPNet), to comprehensively integrate multi-view and point cloud features toimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) moduleprioritizes image views that closely match the semantic content of the text. Toadaptively fuse back-projected multi-view images with point cloud features, wedesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scenecomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)module facilitates robust reasoning by integrating contextual informationacross visual and linguistic modalities. Experimental results on SQA3D andScanQA datasets demonstrate the superiority of our DSPNet. Codes will beavailable at https://github.com/LZ-CH/DSPNet.</description>
      <author>example@mail.com (Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2503.03190v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Simple Siamese Network for High-Resolution Video Quality Assessment</title>
      <link>http://arxiv.org/abs/2503.02330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为SiamVQA的新型Siamese网络，用于高分辨率视频质量评估。&lt;h4&gt;背景&lt;/h4&gt;在视频质量评估（VQA）的研究中，双分支网络已经作为一种有前途的解决方案出现。这种网络将技术视角和美学视角分开来测量低级失真和高级语义感知。&lt;h4&gt;目的&lt;/h4&gt;研究者认为仅从技术角度进行评估应该以语义感知的方式来进行，并假设现有的技术分支难以在高分辨率视频中察觉到这些高级语义信息，因此提出了一种新的网络模型SiamVQA。&lt;h4&gt;方法&lt;/h4&gt;SiamVQA采用共享权重的双分支架构来提升技术视角下对高层语义的理解能力。此外还引入了双重交叉注意力层用于融合技术和美学特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该模型在高分辨率基准数据集上达到了最先进的准确度，并且在低分辨率基准数据集上的表现也具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;SiamVQA是首个尝试通过语义感知方式从技术视角进行视频质量评估的网络架构，在处理高质量视频时展现出了巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，关于视频质量评估（VQA），双分支网络已作为一种有前途的方法出现。它将VQA分解为技术和美学两个独立的技术层面来分别测量低级失真和高级语义感知。然而，我们提出技术视角本身应该以意识语义的方式进行衡量。假设现有的技术支路难以识别高分辨率视频中的高层含义，因为它是在从视频中抽样的局部小块上训练的。这些问题在低分辨率视频表现良好时可以被掩盖，但在高分辨率VQA中就变得非常关键了。这项工作介绍了SiamVQA，这是一种简单而有效的用于高分辨率VQA的Siamese网络。通过共享技术与美学支路之间的权重，SiamVQA提升了技术支路理解语义的能力，并促进了技术质量表示学习。此外，它还整合了一种双重交叉注意力层来融合技术和美学特征。SiamVQA在高分辨率基准测试中实现了最先进的精度，在较低分辨率的基准测试中的结果也非常具有竞争力。代码可在https://github.com/srcn-ivl/SiamVQA 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the research of video quality assessment (VQA), two-branch network hasemerged as a promising solution. It decouples VQA with separate technical andaesthetic branches to measure the perception of low-level distortions andhigh-level semantics respectively. However, we argue that while technical andaesthetic perspectives are complementary, the technical perspective itselfshould be measured in semantic-aware manner. We hypothesize that existingtechnical branch struggles to perceive the semantics of high-resolution videos,as it is trained on local mini-patches sampled from videos. This issue can behidden by apparently good results on low-resolution videos, but indeed becomescritical for high-resolution VQA. This work introduces SiamVQA, a simple buteffective Siamese network for highre-solution VQA. SiamVQA shares weightsbetween technical and aesthetic branches, enhancing the semantic perceptionability of technical branch to facilitate technical-quality representationlearning. Furthermore, it integrates a dual cross-attention layer for fusingtechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy onhigh-resolution benchmarks, and competitive results on lower-resolutionbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA</description>
      <author>example@mail.com (Guotao Shen, Ziheng Yan, Xin Jin, Longhai Wu, Jie Chen, Ilhyun Cho, Cheul-Hee Hahm)</author>
      <guid isPermaLink="false">2503.02330v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Directly Follows Graphs Go Predictive Process Monitoring With Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.03197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，基于人工神经网络的预测过程监控技术（PPM）作为监测业务流程未来行为的方法不断发展。现有方法大多关注于将流程视为序列数据并将其输入到处理顺序数据的神经架构中，如循环神经网络或变压器。&lt;h4&gt;背景&lt;/h4&gt;现有的PPM技术主要通过使用递归神经网络(RNNs)或变压器等处理顺序数据的模型来预测业务流程的未来行为。但是这些方法在处理复杂、长时间且包含大量循环的流程时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;本研究探讨了一种替代方式来进行PPM：通过将每个过程转换为其直接后继图(DFG)表示，应用图神经网络(GNNs)进行预测任务。目的是开发更适合于复杂和长周期业务流程的模型。&lt;h4&gt;方法&lt;/h4&gt;我们介绍了不同创建DFG表示的方法，这些方法根据所使用的特定GNN而有所不同。测试了从传统基于节点的到新式的基于边的架构的各种GNN，并探讨了使用多图的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;h4&gt;结论&lt;/h4&gt;研究展示了通过利用GNN技术进行PPM的潜力和前景，特别是对于复杂且包含大量循环的业务流程。这种方法可能为复杂的流程预测任务提供更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在过去的几年中，基于人工神经网络的预测过程监控（PPM）技术作为一种监测商业流程未来行为的方法得到了发展。现有的方法主要集中在将流程视为序列数据，并将其输入到处理顺序数据的神经架构中，例如循环神经网络（RNNs）或变压器。在这项研究中，我们探讨了一种进行PPM的替代方式：通过将每个过程转换为其直接后继图(DFG)表示，可以应用图神经网络(GNNs)来执行预测任务。这样做的目的是开发更适合于复杂、长周期且包含大量循环的业务流程的模型。特别地，我们展示了根据所用特定GNN的不同创建DFG表示的方法。测试的GNN范围从传统的节点架构到新颖的边架构。此外，还探讨了使用多图的可能性。通过这些步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past years, predictive process monitoring (PPM) techniques based onartificial neural networks have evolved as a method to monitor the futurebehavior of business processes. Existing approaches mostly focus oninterpreting the processes as sequences, so-called traces, and feeding them toneural architectures designed to operate on sequential data such as recurrentneural networks (RNNs) or transformers. In this study, we investigate analternative way to perform PPM: by transforming each process in itsdirectly-follows-graph (DFG) representation we are able to apply graph neuralnetworks (GNNs) for the prediction tasks. By this, we aim to develop modelsthat are more suitable for complex processes that are long and contain anabundance of loops. In particular, we present different ways to create DFGrepresentations depending on the particular GNN we use. The tested GNNs rangefrom classical node-based to novel edge-based architectures. Further, weinvestigate the possibility of using multi-graphs. By these steps, we aim todesign graph representations that minimize the information loss whentransforming traces into graphs.</description>
      <author>example@mail.com (Attila Lischka, Simon Rauch, Oliver Stritzel)</author>
      <guid isPermaLink="false">2503.03197v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A General Neural Network Potential for Energetic Materials with C, H, N, and O elements</title>
      <link>http://arxiv.org/abs/2503.01932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages,16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的神经网络势能（NNP）模型，用于预测C, H, N, O组成的高能量材料的结构、机械和分解特性。该模型通过转移学习利用预先训练好的NNP，并基于密度泛函理论计算得到的能量和力数据进行微调。&lt;h4&gt;背景&lt;/h4&gt;传统的高能量材料研究受限于高昂的计算成本和漫长的开发周期，阻碍了新材料的设计与优化。&lt;h4&gt;目的&lt;/h4&gt;本工作旨在发展一种适用于预测C, H, N, O组成的高能量材料特性的神经网络模型，并验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;利用已预训练好的NNP模型，在转移学习框架下使用DFT计算得到的能量和力数据进行微调，应用于20种不同的高能系统。该模型通过分子动力学模拟进行测试，并与实验结果对比。&lt;h4&gt;主要发现&lt;/h4&gt;经过验证的神经网络模型能够准确描述C, H, N, O组成的高能量材料的关键原子相互作用及热分解机制，表现出DFT级别的精度和广泛的适用性，显著减少了计算和实验成本。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一种高效的策略来设计和发展高能量材料，并为结合密度泛函理论、机器学习和实验方法的材料研究提出了一个有前景的框架。NNP模型已开源在GitHub（https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials）。&lt;h4&gt;翻译&lt;/h4&gt;高能量材料的发现与优化受限于传统方法高昂的计算成本和漫长的开发周期，本工作旨在发展一种高效的神经网络势能模型以预测C, H, N, O组成的高能量材料。该模型基于预训练后的NNP通过转移学习进行微调，并且在分子动力学模拟中得到验证，展示了其优越的精度与广义性，在减少计算和实验成本的同时提高了设计效率，为结合多种方法推进材料研究提供了有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mingjiewen/general-nnp-model-for-c-h-n-o-energetic-materials&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery and optimization of high-energy materials (HEMs) areconstrained by the prohibitive computational expense and prolonged developmentcycles inherent in conventional approaches. In this work, we develop a generalneural network potential (NNP) that efficiently predicts the structural,mechanical, and decomposition properties of HEMs composed of C, H, N, and O.Our framework leverages pre-trained NNP models, fine-tuned using transferlearning on energy and force data derived from density functional theory (DFT)calculations. This strategy enables rapid adaptation across 20 different HEMsystems while maintaining DFT-level accuracy, significantly reducingcomputational costs. A key aspect of this work is the ability of NNP model tocapture the chemical activity space of HEMs, accurately describe the key atomicinteractions and reaction mechanisms during thermal decomposition. The generalNNP model has been applied in molecular dynamics (MD) simulations and validatedwith experimental data for various HEM structures. Results show that the NNPmodel accurately predicts the structural, mechanical, and decompositionproperties of HEMs by effectively describing their chemical activity space.Compared to traditional force fields, it offers superior DFT-level accuracy andgeneralization across both microscopic and macroscopic properties, reducing thecomputational and experimental costs. This work provides an efficient strategyfor the design and development of HEMs and proposes a promising framework forintegrating DFT, machine learning, and experimental methods in materialsresearch. (To facilitate further research and practical applications, weopen-source our NNP model on GitHub:https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials.)</description>
      <author>example@mail.com (Mingjie Wen, Jiahe Han, Wenjuan Li, Xiaoya Chang, Qingzhao Chu, Dongping Chen)</author>
      <guid isPermaLink="false">2503.01932v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</title>
      <link>http://arxiv.org/abs/2502.17039v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆的感知能力。尽管激光雷达是一种常用的传感器，且在智能汽车和基础设施中广泛应用，但其优越性能伴随着较高的成本。为了实现低成本V2I，降低激光雷达的成本至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的V2I系统面临的主要挑战是如何在降低成本的同时保持高性能感知效果。由于高分辨率激光雷达价格昂贵，而低分辨率的激光雷达会导致远处的小物体变得更加模糊。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的协作感知框架LCV2I，在保证性能的前提下尽可能降低车辆上的传感器成本。&lt;h4&gt;方法&lt;/h4&gt;LCV2I利用摄像头和低成本低分辨率激光雷达的数据作为输入。通过特征偏移校正模块和区域特征增强算法提高特征表示能力，并采用区域性差异图和得分图来评估协作内容的价值，从而提升通信带宽效率。&lt;h4&gt;主要发现&lt;/h4&gt;LCV2I能够在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，并在现实世界的DAIR-V2X场景中进行3D目标检测时表现出色，超越现有算法的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入低成本低分辨率激光雷达和改进通信效率的方法，LCV2I框架成功实现了高性能车辆到基础设施协作感知的目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆感知能力。尽管作为常用传感器的LiDAR在智能汽车与基础设施中广泛应用，但其卓越性能伴随着高昂的成本问题。为了实现低成本V2I方案，降低LiDAR成本显得尤为重要。我们提出了一种新方法，即使用低分辨率LiDAR以尽可能地降低成本，并结合相机和激光雷达数据作为输入。通过特征偏移校正模块及区域特征增强算法来提升特征表示能力；同时采用区域性差异图与得分图评估合作内容的价值，从而提高通信带宽利用效率。在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，这是我们的研究目标所在。实验结果表明，在现实世界中，使用DAIR-V2X场景进行3D物体检测时，所提出的LCV2I框架的性能显著优于现有算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-Infrastructure (V2I) collaborative perception leverages datacollected by infrastructure's sensors to enhance vehicle perceptualcapabilities. LiDAR, as a commonly used sensor in cooperative perception, iswidely equipped in intelligent vehicles and infrastructure. However, itssuperior performance comes with a correspondingly high cost. To achievelow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we studyadopting low-resolution LiDAR on the vehicle to minimize cost as much aspossible. However, simply reducing the resolution of vehicle's LiDAR results insparse point clouds, making distant small objects even more blurred.Additionally, traditional communication methods have relatively low bandwidthutilization efficiency. These factors pose challenges for us. To balance costand perceptual accuracy, we propose a new collaborative perception framework,namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDARas input. It also employs feature offset correction modules and regionalfeature enhancement algorithms to improve feature representation. Finally, weuse regional difference map and regional score map to assess the value ofcollaboration content, thereby improving communication bandwidth efficiency. Insummary, our approach achieves high perceptual performance while substantiallyreducing the demand for high-resolution sensors on the vehicle. To evaluatethis algorithm, we conduct 3D object detection in the real-world scenario ofDAIR-V2X, demonstrating that the performance of LCV2I consistently surpassescurrently existing algorithms.</description>
      <author>example@mail.com (Xinxin Feng, Haoran Sun, Haifeng Zheng)</author>
      <guid isPermaLink="false">2502.17039v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</title>
      <link>http://arxiv.org/abs/2503.00470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的方法，用于二维材料和异质结构的高效表征。通过使用YOLO模型，实现了对特定类型二维材料识别的高精度，并且探讨了跨不同材料应用迁移学习的效果。&lt;h4&gt;背景&lt;/h4&gt;二维材料及其异质结构具有独特的物理特性，需要高效的表征方法。&lt;h4&gt;目的&lt;/h4&gt;利用人工智能的进步提出了一种基于深度学习的方法来准确地表征2D材料和异质结构。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO模型识别MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片，同时探索了跨不同材料的迁移学习技术以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;该模型达到了超过94.67%的精度，并展示了强大的泛化能力和抗干扰能力。开发的应用程序能够直接从光学显微镜图像中进行实时分析，显著提高了效率和降低了成本。&lt;h4&gt;结论&lt;/h4&gt;这种深度学习驱动的方法代表了一种快速准确表征2D材料的新工具，为材料科学的研究和发展开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：二维(2D)材料和异质结构表现出独特的物理特性，需要高效的表征方法。利用人工智能的进展，我们介绍了一种基于深度学习的方法来高效地表征异质结构和2D材料，特别是MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片。通过使用YOLO模型，我们在这些材料的识别中实现了超过94.67%的准确率。此外，我们探讨了跨不同类型材料应用迁移学习的效果，这进一步提高了模型性能。该模型展示出强大的泛化能力和抗干扰能力，在各种场景下确保可靠的结果。为了便于实际使用，我们开发了一个应用程序，它可以直接从光学显微镜图像中进行实时分析，使其过程比传统方法更快且成本更低。这种深度学习驱动的方法为二维材料的快速准确表征提供了一种有前景的工具，并为材料科学的研究和发展开辟了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Two-dimensional (2D) materials and heterostructures exhibit unique physicalproperties, necessitating efficient and accurate characterization methods.Leveraging advancements in artificial intelligence, we introduce a deeplearning-based method for efficiently characterizing heterostructures and 2Dmaterials, specifically MoS2-MoSe2 lateral heterostructures and MoS2 flakeswith varying shapes and thicknesses. By utilizing YOLO models, we achieve anaccuracy rate of over 94.67% in identifying these materials. Additionally, weexplore the application of transfer learning across different materials, whichfurther enhances model performance. This model exhibits robust generalizationand anti-interference ability, ensuring reliable results in diverse scenarios.To facilitate practical use, we have developed an application that enablesreal-time analysis directly from optical microscope images, making the processsignificantly faster and more cost-effective than traditional methods. Thisdeep learning-driven approach represents a promising tool for the rapid andaccurate characterization of 2D materials, opening new avenues for research anddevelopment in material science.</description>
      <author>example@mail.com (Junqi He, Yujie Zhang, Jialu Wang, Tao Wang, Pan Zhang, Chengjie Cai, Jinxing Yang, Xiao Lin, Xiaohui Yang)</author>
      <guid isPermaLink="false">2503.00470v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models</title>
      <link>http://arxiv.org/abs/2503.02876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍SPIDER数据集，这是一个公共的病理图像描述库，是当前最大的补丁级别数据集之一，涵盖了多种器官类型，提供高质量注释和周围背景信息。&lt;h4&gt;背景&lt;/h4&gt;现有的公开计算病理学数据集在器官多样性、类别覆盖率或标注质量方面存在局限性。这限制了AI技术的发展。&lt;h4&gt;目的&lt;/h4&gt;引入SPIDER以弥补现有公共数据集中不足的器官种类、分类覆盖范围及注释质量问题，从而推动计算机病理学领域的发展。&lt;h4&gt;方法&lt;/h4&gt;创建了一个涵盖皮肤、结肠直肠和胸部等多类型组织的数据集，并提供了由专业病理学家验证的高质量注释。此外，还开发了使用Hibou-L基础模型作为特征提取器结合注意力机制分类头的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在SPIDER数据集上训练的基线模型，在多种组织类别中取得了最先进的性能表现，这些模型为未来的数字病理研究提供了强有力的基准。&lt;h4&gt;结论&lt;/h4&gt;不仅限于补丁级别的分类任务，该方法还支持快速定位重要区域、提供定量组织指标，并为进一步多模态路径学技术的发展奠定基础。SPIDER数据集和训练好的模型已向公众开放使用。&lt;h4&gt;翻译&lt;/h4&gt;推进计算病理学领域的AI研究需要大量的高质量且多样化的数据集，但现有公开的数据集往往在器官多样性、分类覆盖范围或注释质量方面存在限制。为了填补这一空白，我们推出了SPIDER（监督的病理图像描述库），这是一个最大的公共可用补丁级别数据集，涵盖了包括皮肤、结肠直肠和胸部在内的多种组织类型，并且每种组织都具有全面的分类覆盖率。SPIDER提供了由专家病理学家验证过的高质量注释，还包括周围的背景补丁，这些增强了空间上下文信息下的分类性能表现。除了该数据集外，我们还展示了基于Hibou-L基础模型作为特征提取器结合注意力机制分类头训练出基线模型的方法，这些模型在多种组织类别中取得了最先进的性能，为未来的数字病理学研究提供了强有力的基准点。SPIDER不仅限于补丁级别的分类任务，它还能快速识别重要的区域、提供定量的组织指标，并为进一步多模态方法的发展打下了基础。该数据集及训练好的模型均向公众开放以推进研究和人工智能驱动下的病理学发展。访问网址：https://github.com/HistAI/SPIDER&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancing AI in computational pathology requires large, high-quality, anddiverse datasets, yet existing public datasets are often limited in organdiversity, class coverage, or annotation quality. To bridge this gap, weintroduce SPIDER (Supervised Pathology Image-DEscription Repository), thelargest publicly available patch-level dataset covering multiple organ types,including Skin, Colorectal, and Thorax, with comprehensive class coverage foreach organ. SPIDER provides high-quality annotations verified by expertpathologists and includes surrounding context patches, which enhanceclassification performance by providing spatial context.  Alongside the dataset, we present baseline models trained on SPIDER using theHibou-L foundation model as a feature extractor combined with anattention-based classification head. The models achieve state-of-the-artperformance across multiple tissue categories and serve as strong benchmarksfor future digital pathology research. Beyond patch classification, the modelenables rapid identification of significant areas, quantitative tissue metrics,and establishes a foundation for multimodal approaches.  Both the dataset and trained models are publicly available to advanceresearch, reproducibility, and AI-driven pathology development. Access them at:https://github.com/HistAI/SPIDER</description>
      <author>example@mail.com (Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova)</author>
      <guid isPermaLink="false">2503.02876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Understanding the Benefit of Multitask Representation Learning in Decision Process</title>
      <link>http://arxiv.org/abs/2503.00345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2205.15701&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种在多任务学习环境中提高样本效率的方法，通过分析未知非线性表示功能的多任务强化学习(MRL)，填补了理论框架上的空白。&lt;h4&gt;背景&lt;/h4&gt;多任务表征学习（MRL）被广泛认为是提升强化学习中样本效率的一种技术。然而，在实际应用中，现有的理论分析方法通常假设代理已知表示函数或使用线性类函数，这在现实中不太实用。&lt;h4&gt;目的&lt;/h4&gt;研究并填补多任务学习环境中未知非线性表示功能的理论空白，提供一个全面的机制分析，特别是在在线和迁移学习设置下。&lt;h4&gt;方法&lt;/h4&gt;考虑了一个代理同时执行M个情境贝叶斯问题（或马尔可夫决策过程）的情况，并使用我们提出的广义函数上限置信界算法(GFUCB)从非线性功能类中开发出共享表示功能φ。&lt;h4&gt;主要发现&lt;/h4&gt;正式证明了这种方法可以超越学习单独任务的下限，表明多任务表征学习在一般功能类中的有效性。该框架还解释了表示功能对迁移学习的影响，并确定了成功转移的关键条件。&lt;h4&gt;结论&lt;/h4&gt;实证实验进一步验证了理论发现，证实了MRL方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multitask Representation Learning (MRL) has emerged as a prevalent techniqueto improve sample efficiency in Reinforcement Learning (RL). Empirical studieshave found that training agents on multiple tasks simultaneously within onlineand transfer learning environments can greatly improve efficiency. Despite itspopularity, a comprehensive theoretical framework that elucidates itsoperational efficacy remains incomplete. Prior analyses have predominantlyassumed that agents either possess a pre-known representation function orutilize functions from a linear class, where both are impractical. Thecomplexity of real-world applications typically requires the use ofsophisticated, non-linear functions such as neural networks as representationfunction, which are not pre-existing but must be learned. Our work tries tofill the gap by extending the analysis to \textit{unknown non-linear}representations, giving a comprehensive analysis for its mechanism in onlineand transfer learning setting. We consider the setting that an agentsimultaneously playing $M$ contextual bandits (or MDPs), developing a sharedrepresentation function $\phi$ from a non-linear function class $\Phi$ usingour novel Generalized Functional Upper Confidence Bound algorithm (GFUCB). Weformally prove that this approach yields a regret upper bound that outperformsthe lower bound associated with learning $M$ separate tasks, marking the firstdemonstration of MRL's efficacy in a general function class. This frameworkalso explains the contribution of representations to transfer learning whenfaced with new, yet related tasks, and identifies key conditions for successfultransfer. Empirical experiments further corroborate our theoretical findings.</description>
      <author>example@mail.com (Rui Lu, Yang Yue, Andrew Zhao, Simon Du, Gao Huang)</author>
      <guid isPermaLink="false">2503.00345v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Fusion: A Novel Multimodal Fusion Model for Accelerated Material Discovery</title>
      <link>http://arxiv.org/abs/2503.01022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, presented at AAAI 2025 Workshop on AI to Accelerating  Science and Engineering (AI2ASE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LLM-Fusion是一种新型的多模态融合模型，利用大型语言模型（LLMs）整合多种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，用于准确预测材料属性。&lt;h4&gt;背景&lt;/h4&gt;在高效地发现具有理想特性的材料方面仍然存在重要问题。许多研究通过使用有关材料的不同信息集来解决这个问题。其中多模态方法由于能够结合不同来源的信息而显示出潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新型的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合多样化的表示形式，以比传统方法更高的准确性预测材料属性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于LLM的灵活架构，支持多模态输入处理，并在两个数据集上的五项预测任务上验证了其有效性。与单模式和简单的拼接基线相比，模型显示出了优越的效果。&lt;h4&gt;主要发现&lt;/h4&gt;LLM-Fusion能够提供丰富的多模态表示形式，比现有融合算法更复杂且有效，从而实现更高精度的材料属性预测。&lt;h4&gt;结论&lt;/h4&gt;LLM-Fusion在准确预测多种材料特性方面表现优于传统方法和简单基线模型，证明了基于大型语言模型进行多模态数据整合的有效性。&lt;h4&gt;翻译&lt;/h4&gt;发现具有理想特性的材料并以高效的方式进行仍然是材料科学中的一个重要问题。许多研究通过利用关于材料的不同信息集来解决这个问题。在这其中，多模态方法由于其结合不同信息来源的能力而显得有前景。然而，到目前为止的融合算法仍然相对简单，缺乏提供丰富多样表示形式的机制。本文提出了一种新的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合各种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，并用于准确预测材料属性。我们的方法引入了一种基于LLM的灵活架构，支持多模态输入处理，并能以比传统方法更高的精度进行材料属性预测。我们在两个数据集上的五项预测任务上验证了我们的模型，并证明其效果优于单模式和简单的拼接基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering materials with desirable properties in an efficient way remains asignificant problem in materials science. Many studies have tackled thisproblem by using different sets of information available about the materials.Among them, multimodal approaches have been found to be promising because oftheir ability to combine different sources of information. However, fusionalgorithms to date remain simple, lacking a mechanism to provide a richrepresentation of multiple modalities. This paper presents LLM-Fusion, a novelmultimodal fusion model that leverages large language models (LLMs) tointegrate diverse representations, such as SMILES, SELFIES, text descriptions,and molecular fingerprints, for accurate property prediction. Our approachintroduces a flexible LLM-based architecture that supports multimodal inputprocessing and enables material property prediction with higher accuracy thantraditional methods. We validate our model on two datasets across fiveprediction tasks and demonstrate its effectiveness compared to unimodal andnaive concatenation baselines.</description>
      <author>example@mail.com (Onur Boyar, Indra Priyadarsini, Seiji Takeda, Lisa Hamada)</author>
      <guid isPermaLink="false">2503.01022v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Waste Classification By Dual-Encoder Contrastive Learning and Multi-Clustering Voting (DECMCV)</title>
      <link>http://arxiv.org/abs/2503.02241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的无监督方法Dual-Encoder Contrastive Learning with Multi-Clustering Voting (DECMCV)，该方法旨在提高垃圾分类的自动化和效率。&lt;h4&gt;背景&lt;/h4&gt;垃圾分类对提升处理效率、减少环境污染至关重要。传统的有监督深度学习方法依赖大量标注数据，这些数据昂贵且难以获取；而自监督学习和无监督学习尽管可以在一定程度上解决数据稀缺问题，但仍存在性能不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的无标签垃圾分类算法，以提高模型的泛化能力和处理实际场景中的风格差异。&lt;h4&gt;方法&lt;/h4&gt;DECMCV采用预训练的ConvNeXt模型进行图像编码，使用Vision Transformer生成正样本，并应用多聚类投票机制来解决数据标注和领域偏移问题。&lt;h4&gt;主要发现&lt;/h4&gt;在TrashNet和华为云数据集上，DECMCV达到了93.78%和98.29%的分类准确率；并且仅需50个标签样本就能有效标注4169张真实世界垃圾图像，提高了模型的分类准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发出一种高效的无监督垃圾分类方法，能够更好地应对实际数据中的风格差异问题，并有助于提高自动化的垃圾分类系统的性能和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Waste classification is crucial for improving processing efficiency andreducing environmental pollution. Supervised deep learning methods are commonlyused for automated waste classification, but they rely heavily on large labeleddatasets, which are costly and inefficient to obtain. Real-world waste dataoften exhibit category and style biases, such as variations in camera angles,lighting conditions, and types of waste, which can impact the model'sperformance and generalization ability. Therefore, constructing a bias-freedataset is essential. Manual labeling is not only costly but also inefficient.While self-supervised learning helps address data scarcity, it still depends onsome labeled data and generally results in lower accuracy compared tosupervised methods. Unsupervised methods show potential in certain cases buttypically do not perform as well as supervised models, highlighting the needfor an efficient and cost-effective unsupervised approach. This study presentsa novel unsupervised method, Dual-Encoder Contrastive Learning withMulti-Clustering Voting (DECMCV). The approach involves using a pre-trainedConvNeXt model for image encoding, leveraging VisionTransformer to generatepositive samples, and applying a multi-clustering voting mechanism to addressdata labeling and domain shift issues. Experimental results demonstrate thatDECMCV achieves classification accuracies of 93.78% and 98.29% on the TrashNetand Huawei Cloud datasets, respectively, outperforming or matching supervisedmodels. On a real-world dataset of 4,169 waste images, only 50 labeled sampleswere needed to accurately label thousands, improving classification accuracy by29.85% compared to supervised models. This method effectively addresses styledifferences, enhances model generalization, and contributes to the advancementof automated waste classification.</description>
      <author>example@mail.com (Kui Huang, Mengke Song, Shuo Ba, Ling An, Huajie Liang, Huanxi Deng, Yang Liu, Zhenyu Zhang, Chichun Zhou)</author>
      <guid isPermaLink="false">2503.02241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</title>
      <link>http://arxiv.org/abs/2503.03074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于大语言模型（LLM）的自动驾驶系统BEVDriver，该系统在CARLA模拟器中实现了端到端闭环驾驶。通过利用潜在鸟瞰图特征作为感知输入，结合高效的多视图图像和3D LiDAR点云处理，BEVDriver能够在考虑导航指令和关键场景的情况下预测并规划未来的精确轨迹。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶技术有望为未来交通带来更高的效率，但现有方法在将三维空间定位与大语言模型的语言理解和推理能力结合方面存在挑战。当前的自动驾驶系统需要建立安全性、可靠性和透明度以获得信任。&lt;h4&gt;目的&lt;/h4&gt;通过引入BEVDriver来探索如何利用大语言模型作为通用决策者进行自主驾驶，并解决3D空间定位和LLM语言及推理能力相结合的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 使用了基于大语言模型的端到端闭环自动驾驶系统BEVDriver，该系统在CARLA模拟器中运行。2. 采用了高效的多视图图像处理模块（BEV编码器）来高效地整合来自不同视角的视觉信息以及3D LiDAR点云数据。3. 将感知到的信息转化为潜在空间中的鸟瞰图特征，这些特征在Q-Former模型中传播以与自然语言指令对齐，并传递给LLM进行预测和规划。&lt;h4&gt;主要发现&lt;/h4&gt;BEVDriver系统在LangAuto基准测试上表现出了显著的性能提升，在驾驶得分方面比最先进的方法高出最多18.9%。&lt;h4&gt;结论&lt;/h4&gt;研究证明了大语言模型可以作为一种强大的工具，为自动驾驶中的决策制定提供支持，并展示了将3D感知与LLM结合的有效性。未来的工作可能包括进一步提高系统的鲁棒性和扩展其应用场景。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于如何利用基于大语言模型的系统（BEVDriver）来实现自动驾驶车辆在虚拟环境中的端到端闭环驾驶，通过处理多视角图像和3D LiDAR数据，并结合自然语言指令以规划精确的未来行驶路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving has the potential to set the stage for more efficientfuture mobility, requiring the research domain to establish trust through safe,reliable and transparent driving. Large Language Models (LLMs) possessreasoning capabilities and natural language understanding, presenting thepotential to serve as generalized decision-makers for ego-motion planning thatcan interact with humans and navigate environments designed for human drivers.While this research avenue is promising, current autonomous driving approachesare challenged by combining 3D spatial grounding and the reasoning and languagecapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-endclosed-loop driving in CARLA that utilizes latent BEV features as perceptioninput. BEVDriver includes a BEV encoder to efficiently process multi-viewimages and 3D LiDAR point clouds. Within a common latent space, the BEVfeatures are propagated through a Q-Former to align with natural languageinstructions and passed to the LLM that predicts and plans precise futuretrajectories while considering navigation instructions and critical scenarios.On the LangAuto benchmark, our model reaches up to 18.9% higher performance onthe Driving Score compared to SoTA methods.</description>
      <author>example@mail.com (Katharina Winter, Mark Azer, Fabian B. Flohr)</author>
      <guid isPermaLink="false">2503.03074v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Transfer Learning for Missing Not-at-Random Matrix Completion</title>
      <link>http://arxiv.org/abs/2503.00174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了矩阵填充中的迁移学习问题，特别是在数据缺失不随机（MNAR）的情况下，通过使用来自与目标矩阵有潜在特征差异的来源矩阵来解决整个行和列缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;在生物医学等领域的实际问题中经常遇到缺失数据的情况，特别是当缺失是基于某种模式而非随机时，这对传统的矩阵填充方法构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨如何利用包含部分相关但不完整信息的来源矩阵来提高目标矩阵填充的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个估计框架，该框架在主动采样情况下能够达到最小化错误下限。同时考虑了主动和被动行列采样的情况，并建立了相应的理论界限。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法可以通过利用来自源数据的信息来高效地查询目标矩阵中最具有信息量的行列，从而避免了传统方法所需的部分一致性假设，能够在不增加计算复杂度的前提下提高填充精度。&lt;h4&gt;结论&lt;/h4&gt;实验结果证明了所提出的方法在真实生物医学数据集上的有效性，并且比现有的算法表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们研究的是转移学习在矩阵完成中的应用，在这种情况下，目标矩阵$Q$存在完整的行和列缺失问题。利用一个带有潜在特征变化的不完整来源矩阵$P$来建立两者之间的联系。考虑了主动和被动采样的情形，并为每个场景建立了理论上的最小化错误界限。我们的计算框架在主动采样环境下实现了这一界限，能够通过查询目标矩阵中最具有信息量的行列避免传统方法所需的部分一致性假设，提高了填充精度，并且在真实生物医学数据集上验证了算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study transfer learning for matrix completion in a Missing Not-at-Random(MNAR) setting that is motivated by biological problems. The target matrix $Q$has entire rows and columns missing, making estimation impossible without sideinformation. To address this, we use a noisy and incomplete source matrix $P$,which relates to $Q$ via a feature shift in latent space. We consider both theactive and passive sampling of rows and columns. We establish minimax lowerbounds for entrywise estimation error in each setting. Our computationallyefficient estimation framework achieves this lower bound for the activesetting, which leverages the source data to query the most informative rows andcolumns of $Q$. This avoids the need for incoherence assumptions required forrate optimality in the passive sampling setting. We demonstrate theeffectiveness of our approach through comparisons with existing algorithms onreal-world biological datasets.</description>
      <author>example@mail.com (Akhil Jalan, Yassir Jedra, Arya Mazumdar, Soumendu Sundar Mukherjee, Purnamrita Sarkar)</author>
      <guid isPermaLink="false">2503.00174v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics</title>
      <link>http://arxiv.org/abs/2503.00804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为DELST的框架，用于嵌入双曲表示并建模层级关系，从而实现图像-基因预训练。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，并且具有丰富的跨模式和模式内部层次信息。然而，现有的方法依赖于对比对齐图像-基因对，无法准确捕捉ST数据中的复杂层级关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DELST，该框架能够在建模层次结构的同时嵌入双曲表示，以实现更有效的图像-基因预训练。&lt;h4&gt;方法&lt;/h4&gt;1. 跨模式蕴涵学习：建立基因和图像之间的顺序关系，以增强图像表示的泛化能力。2. 同一模式内蕴涵学习：编码基因表达模式为层级关系，并指导不同样本间的全局层次学习。&lt;h4&gt;主要发现&lt;/h4&gt;DELST框架在标注病理学家注释的空间转录组学基准上的广泛实验中展示了其有效性，实现了比现有方法更好的预测性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲空间中的层级表示进行图像-基因的预训练可以显著提高模型的预测准确性。相关代码和模型可在https://github.com/XulinChen/DELST获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要：空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，成为多模态表现学习中的宝贵资源。此外，ST数据本身在跨模式以及同一模式内部均包含丰富的层级信息。例如，不同位置的非零基因表达数量各不相同，对应着不同的细胞活动水平和语义层次结构。然而，现有方法依赖于图像-基因对之间的对比对齐方式，无法准确捕捉到ST数据中复杂的层级关系。因此，我们提出了DELST框架，这是第一个在两个层面上建模层级并将双曲表示嵌入到图像-基因预训练中的框架：1) 跨模式蕴涵学习，建立基因和图像之间的一种顺序关系来增强图像表示的泛化能力；2) 同一模式内蕴涵学习，编码基因表达模式为层级关系，并在全球范围内指导不同样本间的层次学习。病理学家注释的空间转录组学基准上的广泛实验表明了我们框架的有效性，在预测性能上优于现有方法。我们的代码和模型可在https://github.com/XulinChen/DELST获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics (ST) maps gene expression within tissue at individualspots, making it a valuable resource for multimodal representation learning.Additionally, ST inherently contains rich hierarchical information both acrossand within modalities. For instance, different spots exhibit varying numbers ofnonzero gene expressions, corresponding to different levels of cellularactivity and semantic hierarchies. However, existing methods rely oncontrastive alignment of image-gene pairs, failing to accurately capture theintricate hierarchical relationships in ST data. Here, we propose DELST, thefirst framework to embed hyperbolic representations while modeling hierarchyfor image-gene pretraining at two levels: (1) Cross-modal entailment learning,which establishes an order relationship between genes and images to enhanceimage representation generalization; (2) Intra-modal entailment learning, whichencodes gene expression patterns as hierarchical relationships, guidinghierarchical learning across different samples at a global scale andintegrating biological insights into single-modal representations. Extensiveexperiments on ST benchmarks annotated by pathologists demonstrate theeffectiveness of our framework, achieving improved predictive performancecompared to existing methods. Our code and models are available at:https://github.com/XulinChen/DELST.</description>
      <author>example@mail.com (Xulin Chen, Junzhou Huang)</author>
      <guid isPermaLink="false">2503.00804v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Precoding in Multi-user Multi-antenna Systems: Transformer or Graph Transformer?</title>
      <link>http://arxiv.org/abs/2503.02998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了Transformer模型在除信道获取任务外的多用户多天线系统中的预编码策略学习能力，并提出了一种新的Graph Transformers架构，以充分利用基础带和混合预编码策略的置换性质。&lt;h4&gt;背景&lt;/h4&gt;Transformers在通道预测等任务中表现出色，而图神经网络（GNNs）则适用于各种通信任务。但是，关于Transformer是否能在非信道获取任务上有效以及如何结合两者的优势尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过研究多用户多天线系统中的预编码策略学习问题，探讨和验证在特定场景下同时利用Transformers和图神经网络优势的方法，并提出新的Graph Transformers架构。&lt;h4&gt;方法&lt;/h4&gt;构建了基于异构图的GNNs与Transformer之间的关系模型。提出了二维（2D）和三维（3D）图变换器（Gformers），这两种模型分别用于学习基础带和混合预编码策略中的置换性质。通过模拟实验评估并比较了它们的学习性能、推理复杂度、训练复杂度以及规模泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 为适应不同数量的用户，需要考虑多用户干扰问题，这可以通过定制Transformer来解决。2. Transformer仅能部分利用预编码策略中的置换性质，并不能适用于变化的天线数目，这种情况与在同质图上学习的GNN相同。3. 利用异构图上的GNNs和Transformers之间的关系建立Graph Transformers，可以更好地处理基于带宽以及混合模式下的预编码策略问题。&lt;h4&gt;结论&lt;/h4&gt;通过实验表明，所提出的2D-和3D-Gformers在学习性能、推理复杂度、训练复杂度等方面均优于传统的Transformer和图神经网络（GNNs），并且具备更好的规模泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Transformers已被设计用于通道获取任务（如信道预测）以及其他诸如预编码的任务，而图神经网络(GNNs)在学习多种通信任务方面已显示出其高效性。然而，关于Transformer是否适用于除信道获取之外的任务以及如何利用这两种架构的优势尚不明确。本文以多用户多天线系统中的预编码策略学习为例来解答这些问题。我们注意到针对预编码定制的Transformer可以反映多用户干扰问题，这对于它在用户数量上的泛化能力是至关重要的。然而，这种定制的Transformer只能利用部分预编码策略的置换性质，并且无法适用于变化的天线数目，与基于同质图进行学习的GNN相同。为了提供有用的见解，我们建立了Transformers和学习异构图上GNNs之间的关系。在此基础上，我们提出了Graph Transformers（即2D-和3D-Gformers），用于挖掘基础带预编码和混合预编码策略中的置换性质。通过模拟实验评估并比较了Gformers的学习性能、推理复杂度、训练复杂度以及规模泛化能力与Transformer和GNNs的对比结果。&lt;h4&gt;创新点&lt;/h4&gt;提出了Graph Transformers，即2D-和3D-Gformers，用于更好地处理基础带及混合模式下的预编码策略问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have been designed for channel acquisition tasks such as channelprediction and other tasks such as precoding, while graph neural networks(GNNs) have been demonstrated to be efficient for learning a multitude ofcommunication tasks. Nonetheless, whether or not Transformers are efficient forthe tasks other than channel acquisition and how to reap the benefits of botharchitectures are less understood. In this paper, we take learning precodingpolicies in multi-user multi-antenna systems as an example to answer thequestions. We notice that a Transformer tailored for precoding can reflectmultiuser interference, which is essential for its generalizability to thenumber of users. Yet the tailored Transformer can only leverage partialpermutation property of precoding policies and hence is not generalizable tothe number of antennas, same as a GNN learning over a homogeneous graph. Toprovide useful insight, we establish the relation between Transformers and theGNNs that learn over heterogeneous graphs. Based on the relation, we proposeGraph Transformers, namely 2D- and 3D-Gformers, for exploiting the permutationproperties of baseband precoding and hybrid precoding policies. The learningperformance, inference and training complexity, and size-generalizability ofthe Gformers are evaluated and compared with Transformers and GNNs viasimulations.</description>
      <author>example@mail.com (Yuxuan Duan, Jia Guo, Chenyang Yang)</author>
      <guid isPermaLink="false">2503.02998v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders</title>
      <link>http://arxiv.org/abs/2503.00131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文展示了通过机器学习算法进行粒子流重建的迁移学习能力，特别是在高能粒子对撞机中从一个探测器设计转移到另一个设计的有效性。&lt;h4&gt;背景&lt;/h4&gt;在高能量粒子物理实验中，利用机器学习技术优化粒子流量的计算是一种创新的方法。然而，如何将这种训练过的模型迁移到不同的探测器上是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;研究通过迁移学习，使用初始大型全仿真数据集在一个探测器设计上预训练算法模型，并在不同对撞机和探测器设计的数据样本上进行微调的有效性。&lt;h4&gt;方法&lt;/h4&gt;利用紧凑型线性对撞机（CLICdet）模型作为初始训练集，在未来环形正负电子对撞机的电子-正电子模式下提议的类似CLIC的设计（CLD）中实现成功知识转移。通过在第二个数据集中使用少一个数量级的数据样本，来展示与从头开始昂贵训练相同的性能。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习模型仅需在10万次CLD事件后，在事件层面指标上达到了传统规则基础粒子流方法的类似性能；而未经微调的全新模型则至少需要1百万次CLD事件才能实现类似的重建效果。这是首次针对全仿真跨探测器转移学习研究。&lt;h4&gt;结论&lt;/h4&gt;这些结果对于构建可以适应不同探测器设计和几何形状的大规模物理模型提供了宝贵的见解，有助于加速新探测器的发展周期，并为利用机器学习进行快速的探测器设计和优化开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要中英文对照&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We demonstrate transfer learning capabilities in a machine-learned algorithmtrained for particle-flow reconstruction in high energy particle colliders.This paper presents a cross-detector fine-tuning study, where we initiallypre-train the model on a large full simulation dataset from one detectordesign, and subsequently fine-tune the model on a sample with a differentcollider and detector design. Specifically, we use the Compact Linear Colliderdetector (CLICdet) model for the initial training set, and demonstratesuccessful knowledge transfer to the CLIC-like detector (CLD) proposed for theFuture Circular Collider in electron-positron mode (FCC-ee). We show that withan order of magnitude less samples from the second dataset, we can achieve thesame performance as a costly training from scratch, across particle-level andevent-level performance metrics; including jet resolution and missingtransverse momentum resolution. Furthermore, we find that the fine-tuned modelachieves comparable performance to the traditional rule-based particle-flowapproach on event-level metrics after training on 100,000 CLD events, whereas amodel trained from scratch requires at least 1 million CLD events to achievesimilar reconstruction performance. To our knowledge, this represents the firstfull-simulation cross-detector transfer learning study for particle-flow. Thesefindings offer valuable insights towards building large physics models that canbe fine-tuned across different detector designs and geometries, helpingaccelerate the development cycle for new detectors, and opening the door torapid detector design and optimization using machine learning.</description>
      <author>example@mail.com (Farouk Mokhtar, Joosep Pata, Michael Kagan, Dolores Garcia, Eric Wulff, Mengke Zhang, Javier Duarte)</author>
      <guid isPermaLink="false">2503.00131v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language-Guided Visual Perception Disentanglement for Image Quality Assessment and Conditional Image Generation</title>
      <link>http://arxiv.org/abs/2503.02206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多模态解耦表示学习框架，旨在解决对比视觉语言模型在图像质量评估和条件生成任务中难以控制感知特性的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前的对比视觉-语言模型（如CLIP）基于大规模I&amp;1T数据集进行训练，在语义识别任务上表现出色。然而，这种多模态表示主要强调语义而忽视了对感知特性精确控制的需求，这在图像质量评估和条件生成等任务中是不利的。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用解耦文本引导图像解耦的新框架，以改善上述视觉任务中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;首先构建I&amp;2T数据集，该数据集包含每个图像的感知性和语义性两个独立描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP框架。最后利用这种解耦后的特性表示来进行图像质量评估和条件生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量的实验与对比，表明所提出的方法在两个流行任务上具有优势。&lt;h4&gt;结论&lt;/h4&gt;论文展示了一种创新性的解决现有视觉语言模型局限性的问题方法，并且研究团队承诺会公开数据集、代码以及模型。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉-语言模型（如CLIP）已经在语义识别任务中显示出了强大的零样本能力，这主要是由于它们在大规模I&amp;1T数据集上的训练。这种多模态表示通常混合了语义和感知元素，并特别强调语义。然而，对于图像质量评估和条件图像生成等流行的任务来说，需要对感知和语义特征进行精细控制，则这一特点可能会成为问题。受上述事实启发，本文提出了一种新的多模态解耦表示学习框架，利用了解耦的文本引导图像解耦。为此，我们首先构建了一个I&amp;2T数据集，该数据集中每个图像都有一个独立的感知性文本描述和语义性文本描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP。最后利用这种解耦后的特性表示来进行图像质量评估（技术质量和美学质量）和条件生成。大量的实验和对比表明，所提出的方法在这两个流行任务上具有优势。数据集、代码和模型将会公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive vision-language models, such as CLIP, have demonstrated excellentzero-shot capability across semantic recognition tasks, mainly attributed tothe training on a large-scale I&amp;1T (one Image with one Text) dataset. This kindof multimodal representations often blend semantic and perceptual elements,placing a particular emphasis on semantics. However, this could be problematicfor popular tasks like image quality assessment (IQA) and conditional imagegeneration (CIG), which typically need to have fine control on perceptual andsemantic features. Motivated by the above facts, this paper presents a newmultimodal disentangled representation learning framework, which leveragesdisentangled text to guide image disentanglement. To this end, we first buildan I&amp;2T (one Image with a perceptual Text and a semantic Text) dataset, whichconsists of disentangled perceptual and semantic text descriptions for animage. Then, the disentangled text descriptions are utilized as supervisorysignals to disentangle pure perceptual representations from CLIP's original`coarse' feature space, dubbed DeCLIP. Finally, the decoupled featurerepresentations are used for both image quality assessment (technical qualityand aesthetic quality) and conditional image generation. Extensive experimentsand comparisons have demonstrated the advantages of the proposed method on thetwo popular tasks. The dataset, code, and model will be available.</description>
      <author>example@mail.com (Zhichao Yang, Leida Li, Pengfei Chen, Jinjian Wu, Giuseppe Valenzise)</author>
      <guid isPermaLink="false">2503.02206v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Attention Fusion of MRI and Jacobian Maps for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2503.00586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种利用交叉注意力融合框架的方法，旨在通过结合结构磁共振成像（sMRI）强度和雅可比行列式图（JSM）来提高阿尔茨海默病（AD）早期诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病的早期诊断至关重要。虽然结构磁共振成像广泛用于该疾病的诊断，但传统的深度学习方法主要依赖于基于强度的特征，这需要大量数据集才能捕捉到微妙的变化。而雅可比行列式图提供了有关局部脑变形的补充信息。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的跨模态融合策略——交叉注意力融合框架，以更好地结合sMRI和JSM的信息进行AD分类。&lt;h4&gt;方法&lt;/h4&gt;利用阿尔茨海默病神经影像学倡议（ADNI）数据集，在四种预训练的3D图像编码器上对比了三种不同的注意机制：交叉注意力、成对自注意力和瓶颈注意力。&lt;h4&gt;主要发现&lt;/h4&gt;与其它两种方法相比，交叉注意力融合框架在区分AD患者和认知正常个体以及轻度认知障碍（MCI）个体和认知正常个体方面表现出更好的性能。同时，该模型参数量较少，具有高计算效率。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了使用交叉注意力融合框架可以提高阿尔茨海默病诊断的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，描述了早期诊断阿尔茨海默病的重要性，并介绍了通过结合结构MRI和JSM的信息来改进该疾病诊断的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early diagnosis of Alzheimer's disease (AD) is critical for interventionbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widelyused for AD diagnosis, but conventional deep learning approaches primarily relyon intensity-based features, which require large datasets to capture subtlestructural changes. Jacobian determinant maps (JSM) provide complementaryinformation by encoding localized brain deformations, yet existing multimodalfusion strategies fail to fully integrate these features with sMRI. We proposea cross-attention fusion framework to model the intrinsic relationship betweensMRI intensity and JSM-derived deformations for AD classification. Using theAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we comparecross-attention, pairwise self-attention, and bottleneck attention with fourpre-trained 3D image encoders. Cross-attention fusion achieves superiorperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.cognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment(MCI) vs. CN. Despite its strong performance, our model remains highlyefficient, with only 1.56 million parameters--over 40 times fewer thanResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate thepotential of cross-attention fusion for improving AD diagnosis whilemaintaining computational efficiency.</description>
      <author>example@mail.com (Shijia Zhang, Xiyu Ding, Brian Caffo, Junyu Chen, Cindy Zhang, Hadi Kharrazi, Zheyu Wang)</author>
      <guid isPermaLink="false">2503.00586v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2503.00210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于fMRI的神经病理性疼痛药物反应预测的方法FMM$_{TC}$，该方法通过结合疼痛特异性的多模态信息和来自广泛无痛数据集的知识，克服了现有单一模式fMRI模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;神经病理性疼痛影响高达10%的成年人群，治疗效果有限且耐受性差。静息态功能磁共振成像(rs-fMRI)是预测药物反应的重要工具，但由于数据稀缺和方法复杂度高，其应用受限。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用有限疼痛特异性数据并整合外部知识的方法FMM$_{TC}$，以提高神经病理性疼痛治疗药物反应的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于fMRI基础模型增强的多模态学习框架FMM$_{TC}$，该框架结合了rs-fMRI的时间序列和功能连接两种模式的信息，并从大量无痛数据集中获取外部知识来补充有限的数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FMM$_{TC}$在内部和公共数据集上均表现出优越的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模态学习和基础模型驱动的知识转移的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过准确预测药物反应以提高临床试验中的参与者分层效率，FMM$_{TC}$有助于神经病理性疼痛治疗的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：由10%的成年人受到的影响，神经病理性疼痛由于有限的疗效和耐受性仍难以治疗。尽管静息态功能磁共振成像(rs-fMRI)是用于药物反应预测的脑生物标志物的重要非侵入式测量方法，fMRI的复杂性要求具有强大容量的机器学习模型。然而，在神经病理性疼痛研究中的数据稀缺限制了高容量模型的应用。为了解决数据匮乏的问题，我们提出了FMM$_{TC}$，一种用于基于fMRI的神经病理性疼痛药物反应预测的基础模型增强多模态学习框架，该框架利用了疼痛特异性内部多模态信息和来自广泛无痛基础模型的知识。具体来说，为了最大化有限疼痛特异性数据的价值，FMM$_{TC}$整合了两种rs-fMRI模式：时间序列和功能连接。进一步地，通过从大量无痛无关fMRI数据集中获取的外部知识来增强FMM$_{TC}$。使用内部和公共开放神经数据集进行的评估表明，与只考虑一种rs-fMRI模态的现有单模态fMRI模型相比，FMM$_{TC}$具有更好的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模式学习以及由基础模型驱动的知识转移的有效性。基于集成梯度解释的研究说明了FMM$_{TC}$如何通过动态行为增强其跨数据集的适应性。综上所述，FMM$_{TC}$通过准确预测药物反应以提高参与者分层效率来支持神经病理性疼痛治疗的发展临床试验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuropathic pain, affecting up to 10% of adults, remains difficult to treatdue to limited therapeutic efficacy and tolerability. Although resting-statefunctional MRI (rs-fMRI) is a promising non-invasive measurement of brainbiomarkers to predict drug response in therapeutic development, the complexityof fMRI demands machine learning models with substantial capacity. However,extreme data scarcity in neuropathic pain research limits the application ofhigh-capacity models. To address the challenge of data scarcity, we proposeFMM$_{TC}$, a Foundation-Model-boosted Multimodal learning framework forfMRI-based neuropathic pain drug response prediction, which leverages bothinternal multimodal information in pain-specific data and external knowledgefrom large pain-agnostic data. Specifically, to maximize the value of limitedpain-specific data, FMM$_{TC}$ integrates complementary information from twors-fMRI modalities: Time series and functional Connectivity. FMM$_{TC}$ isfurther boosted by an fMRI foundation model with its external knowledge fromextensive pain-agnostic fMRI datasets enriching limited pain-specificinformation. Evaluations with an in-house dataset and a public dataset fromOpenNeuro demonstrate FMM$_{TC}$'s superior representation ability,generalizability, and cross-dataset adaptability over existing unimodal fMRImodels that only consider one of the rs-fMRI modalities. The ablation studyvalidates the effectiveness of multimodal learning and foundation-model-poweredexternal knowledge transfer in FMM$_{TC}$. An integrated gradient-basedinterpretation study explains how FMM$_{TC}$'s cross-dataset dynamic behaviorsenhance its adaptability. In conclusion, FMM$_{TC}$ boosts clinical trials inneuropathic pain therapeutic development by accurately predicting drugresponses to improve the participant stratification efficiency.</description>
      <author>example@mail.com (Wenrui Fan, L. M. Riza Rizky, Jiayang Zhang, Chen Chen, Haiping Lu, Kevin Teh, Dinesh Selvarajah, Shuo Zhou)</author>
      <guid isPermaLink="false">2503.00210v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
      <link>http://arxiv.org/abs/2503.02819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于费曼-卡克公式和加权模拟方案的新颖采样方法，用于从一系列预训练的评分模型导出的退火、几何平均或乘积分布中进行抽样。&lt;h4&gt;背景&lt;/h4&gt;分数生成模型是跨多个领域的首选模型。然而，在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于费曼-卡克公式的加权模拟方案（FKCs），用于从一系列退火、几何平均或乘积分布中进行采样，并改进分类器自由引导方法和分子生成任务。&lt;h4&gt;方法&lt;/h4&gt;通过仔细考虑适当的偏微分方程中的各项，提出了费曼-卡克校正器（FKC）加权模拟方案。为模拟这些PDE，提议使用基于推理时间缩放的顺序蒙特卡洛重采样算法来提高抽样质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了该方法在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的有效性，并提出了通过推理时间温度退火进行快速采样的方案。&lt;h4&gt;结论&lt;/h4&gt;提出了一种有效且原理性的方法，用于从一系列预先训练的评分模型导出的分布中抽样。该方法不仅可以改善现有技术，在特定任务上也表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：尽管分数生成模型在多个领域中是首选模型，但在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。现有的分类器自由引导方法使用简单的启发式方法来混合条件和无条件评分以从条件分布采样。然而，这样的方法并不逼近中间的分布，因此需要额外的'校正'步骤。在这项工作中，我们提供了一种高效且原理性的方法用于从一系列退火、几何平均或乘积分布中进行抽样，这些分布是从预训练的评分模型导出的。基于著名的费曼-卡克公式，并通过仔细考虑适当的偏微分方程（PDEs）中的各项，我们提出加权模拟方案，称为费曼-卡克校正器（FKCs）。为了模拟这些PDE，我们提出了顺序蒙特卡洛重采样算法，利用推理时间缩放来提高抽样的质量。通过在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的实用性，以及提出通过推理时间温度退火进行快速采样的方案，我们在实验上证明了我们方法的有效性。我们的代码可在https://github.com/martaskrt/fkc-diffusion获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While score-based generative models are the model of choice across diversedomains, there are limited tools available for controlling inference-timebehavior in a principled manner, e.g. for composing multiple pretrained models.Existing classifier-free guidance methods use a simple heuristic to mixconditional and unconditional scores to approximately sample from conditionaldistributions. However, such methods do not approximate the intermediatedistributions, necessitating additional 'corrector' steps. In this work, weprovide an efficient and principled method for sampling from a sequence ofannealed, geometric-averaged, or product distributions derived from pretrainedscore-based models. We derive a weighted simulation scheme which we callFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula bycarefully accounting for terms in the appropriate partial differentialequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo(SMC) resampling algorithms that leverage inference-time scaling to improvesampling quality. We empirically demonstrate the utility of our methods byproposing amortized sampling via inference-time temperature annealing,improving multi-objective molecule generation using pretrained models, andimproving classifier-free guidance for text-to-image generation. Our code isavailable at https://github.com/martaskrt/fkc-diffusion.</description>
      <author>example@mail.com (Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alán Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, Kirill Neklyudov)</author>
      <guid isPermaLink="false">2503.02819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation</title>
      <link>http://arxiv.org/abs/2503.03045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文介绍了ArticuBot，这是一种单个学习策略可以使机器人系统在现实世界中打开各种未见过的铰接物体的技术。由于这些对象的几何形状、大小和铰链类型存在巨大差异，这一任务长期以来对机器人技术来说一直是个挑战。我们的系统Articubot由三个部分组成：在基于物理的模拟环境中生成大量演示，在点云基础上通过模仿学习将所有生成的演示提炼成神经策略，并进行零样本仿真到现实转移至实际机器人系统中。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人任务难以处理具有多种几何形状、大小和铰链类型的未见过物体，而Articubot旨在解决这一挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的策略，使机器人能够打开从未见过的不同种类铰接式对象，同时能够在不同环境下实现零样本仿真到现实转移。&lt;h4&gt;方法&lt;/h4&gt;包括在物理模拟中生成大量演示、通过模仿学习提炼成点云神经策略，并进行仿真实验与真实机器人的零样本迁移。此外，提出了一个层次化的策略表示模型和一个新的加权位移模型。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了Articubot能够在不同实验室、客厅以及厨房环境下打开多种未见过的铰接式物体的能力，证明了学习到的策略具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合模拟数据生成、模仿学习和零样本仿真到现实迁移技术，ArticuBot成功地实现了机器人在面对未知铰接物体时的有效操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents ArticuBot, in which a single learned policy enables arobotics system to open diverse categories of unseen articulated objects in thereal world. This task has long been challenging for robotics due to the largevariations in the geometry, size, and articulation types of such objects. Oursystem, Articubot, consists of three parts: generating a large number ofdemonstrations in physics-based simulation, distilling all generateddemonstrations into a point cloud-based neural policy via imitation learning,and performing zero-shot sim2real transfer to real robotics systems. Utilizingsampling-based grasping and motion planning, our demonstration generalizationpipeline is fast and effective, generating a total of 42.3k demonstrations over322 training articulated objects. For policy learning, we propose a novelhierarchical policy representation, in which the high-level policy learns thesub-goal for the end-effector, and the low-level policy learns how to move theend-effector conditioned on the predicted goal. We demonstrate that thishierarchical approach achieves much better object-level generalization comparedto the non-hierarchical version. We further propose a novel weighteddisplacement model for the high-level policy that grounds the prediction intothe existing 3D structure of the scene, outperforming alternative policyrepresentations. We show that our learned policy can zero-shot transfer tothree different real robot settings: a fixed table-top Franka arm across twodifferent labs, and an X-Arm on a mobile base, opening multiple unseenarticulated objects across two labs, real lounges, and kitchens. Videos andcode can be found on our project website: https://articubot.github.io/.</description>
      <author>example@mail.com (Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held)</author>
      <guid isPermaLink="false">2503.03045v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>V$^2$Dial: Unification of Video and Visual Dialog via Multimodal Experts</title>
      <link>http://arxiv.org/abs/2503.02063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;我们提出了V$^2$Dial - 一种新的专家模型，专门用于同时处理图像和视频输入数据以进行多模态对话任务。&lt;h4&gt;背景描述&lt;/h4&gt;现有的多模态模型主要集中在较为简单的任务上（例如视觉问答、视频问答、视频文本检索），而忽视了更具挑战性的对话类任务（如基于视频或可视/图像的对话）。此外，关于这些任务的工作各自独立发展，尽管它们之间有明显的相似性，这限制了其潜在的应用。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种单一模型来统一这些对话任务，并首次联合学习图像和视频的空间和时间特征。通过专门的专家处理输入并通过匹配和对比学习技术对齐。&lt;h4&gt;方法描述&lt;/h4&gt;使用专用专家将图像和视频数据路由并利用匹配与对比学习技术进行特征对齐，以系统地研究两种任务之间的领域迁移问题。&lt;h4&gt;主要发现&lt;/h4&gt;模型在AVSD和VisDial等广泛使用的对话数据集上进行了广泛的评估，在零样本和微调设置下均达到了新的最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;V$^2$Dial通过将图像和视频的时空特征联合学习，提供了一种新颖的方法来处理多模态对话任务，并在多个基准测试中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present V$^2$Dial - a novel expert-based model specifically geared towardssimultaneously handling image and video input data for multimodalconversational tasks. Current multimodal models primarily focus on simplertasks (e.g., VQA, VideoQA, video-text retrieval) and often neglect the morechallenging conversational counterparts, such as video and visual/image dialog.Moreover, works on both conversational tasks evolved separately from each otherdespite their apparent similarities limiting their applicability potential. Tothis end, we propose to unify both tasks using a single model that for thefirst time jointly learns the spatial and temporal features of images andvideos by routing them through dedicated experts and aligns them using matchingand contrastive learning techniques. Furthermore, we systemically study thedomain shift between the two tasks by investigating whether and to what extentthese seemingly related tasks can mutually benefit from their respectivetraining data. Extensive evaluations on the widely used video and visual dialogdatasets of AVSD and VisDial show that our model achieves new state-of-the-artresults across four benchmarks both in zero-shot and fine-tuning settings.</description>
      <author>example@mail.com (Adnen Abdessaied, Anna Rohrbach, Marcus Rohrbach, Andreas Bulling)</author>
      <guid isPermaLink="false">2503.02063v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Generalization on Graphs via Progressive Inference</title>
      <link>http://arxiv.org/abs/2503.02988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GPro模型，该模型采用逐步推理的方法学习图的因果不变性，以提高在数据分布变化情况下的预测性能。&lt;h4&gt;背景&lt;/h4&gt;目前大多数图形神经网络（GNNs）假设独立同分布的数据环境，在实际应用中这种假设往往并不成立。当数据分布发生显著偏移时，现有的GNN模型常常无法产生可靠的预测结果。&lt;h4&gt;目的&lt;/h4&gt;旨在通过提取输入图中的因果不变部分来改善模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为GPro的新模型，该模型采用逐步推理的方式学习图的因果不变性，并通过创建反事实样本扩大训练分布以提升其在捕捉因果不变性方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，GPro模型能够更好地识别并利用数据中的因果不变部分，在数据分布显著变化时仍能保持较高的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，GPro在一系列基准测试上优于当前最先进的方法，并且在更极端的数据分布偏移情况下表现尤为出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development and evaluation of graph neural networks (GNNs) generallyfollow the independent and identically distributed (i.i.d.) assumption. Yetthis assumption is often untenable in practice due to the uncontrollable datageneration mechanism. In particular, when the data distribution shows asignificant shift, most GNNs would fail to produce reliable predictions and mayeven make decisions randomly. One of the most promising solutions to improvethe model generalization is to pick out causal invariant parts in the inputgraph. Nonetheless, we observe a significant distribution gap between thecausal parts learned by existing methods and the ground truth, leading toundesirable performance. In response to the above issues, this paper presentsGPro, a model that learns graph causal invariance with progressive inference.Specifically, the complicated graph causal invariant learning is decomposedinto multiple intermediate inference steps from easy to hard, and theperception of GPro is continuously strengthened through a progressive inferenceprocess to extract causal features that are stable to distribution shifts. Wealso enlarge the training distribution by creating counterfactual samples toenhance the capability of the GPro in capturing the causal invariant parts.Extensive experiments demonstrate that our proposed GPro outperforms thestate-of-the-art methods by 4.91% on average. For datasets with more severedistribution shifts, the performance improvement can be up to 6.86%.</description>
      <author>example@mail.com (Yiming Xu, Bin Shi, Zhen Peng, Huixiang Liu, Bo Dong, Chen Chen)</author>
      <guid isPermaLink="false">2503.02988v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points</title>
      <link>http://arxiv.org/abs/2503.02745v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 (Patent Protected); Project page:  https://vcc.tech/research/2025/ArcPro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的学习框架ArcPro，该框架基于架构程序从稀疏和低质量的点云中恢复结构化的3D抽象。&lt;h4&gt;背景&lt;/h4&gt;现有技术在处理稀疏且低质量的点云数据时存在不足，无法有效地从中提取结构信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决上述问题，并通过使用架构程序作为桥梁连接前馈和逆向生成过程以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'设计领域特定语言(DSL)': '用DSL分层表示建筑结构为一个程序，该程序可以高效地转换成网格。', '训练数据合成': '利用前馈处理来实现训练数据的合成。', '编码器-解码器网络': '通过在点云和架构程序之间进行配对训练了一个编码器-解码器网络，其中3D卷积编码器提取点云特征，而Transformer解码器自回归地预测标记化形式的程序。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'高效推断': '所提出的方法在推理过程中非常高效，并且能够生成合理和忠实的3D抽象。', '性能优越': '实验表明ArcPro优于传统的建筑代理重建方法以及基于学习的抽象方法。', '扩展应用': '进一步探索了其与多视图图像和自然语言输入配合工作的潜力。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入ArcPro，为从稀疏且低质量的点云中恢复结构化3D抽象提供了一种新的解决方案，并展示了该框架在建筑领域和其他相关领域的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们在论文摘要的基础上，将信息以分点的形式进行了总结和提炼。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ArcPro, a novel learning framework built on architecturalprograms to recover structured 3D abstractions from highly sparse andlow-quality point clouds. Specifically, we design a domain-specific language(DSL) to hierarchically represent building structures as a program, which canbe efficiently converted into a mesh. We bridge feedforward and inverseprocedural modeling by using a feedforward process for training data synthesis,allowing the network to make reverse predictions. We train an encoder-decoderon the points-program pairs to establish a mapping from unstructured pointclouds to architectural programs, where a 3D convolutional encoder extractspoint cloud features and a transformer decoder autoregressively predicts theprograms in a tokenized form. Inference by our method is highly efficient andproduces plausible and faithful 3D abstractions. Comprehensive experimentsdemonstrate that ArcPro outperforms both traditional architectural proxyreconstruction and learning-based abstraction methods. We further explore itspotential to work with multi-view image and natural language inputs.</description>
      <author>example@mail.com (Qirui Huang, Runze Zhang, Kangjun Liu, Minglun Gong, Hao Zhang, Hui Huang)</author>
      <guid isPermaLink="false">2503.02745v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Recognition of Dysarthria in Amyotrophic Lateral Sclerosis patients using Hypernetworks</title>
      <link>http://arxiv.org/abs/2503.01892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文提出了一种使用超网络识别ALS患者失语症的新方法，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，症状多样，包括言语清晰度下降。现有研究依赖于特征提取策略和定制的卷积神经网络来预测临床标准ALSFRS-R以识别失语症。&lt;h4&gt;目的&lt;/h4&gt;提出一种使用超网络识别ALS患者失语症的新方法，并评估其相对于其他基准模型的优势。&lt;h4&gt;方法&lt;/h4&gt;采用音频文件，将其转换为log-Mel频谱图、delta和delta-delta形式，并通过预训练的修改版AlexNet模型处理。最后利用生成目标网络权重的超网络来完成识别任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验在新收集的公共数据集VOC-ALS上进行，结果显示所提出的算法可以达到高达82.66%的准确率，优于包括多模态融合方法在内的强基准模型。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了超网络应用到ALS失语症识别中的价值和优势，在泛化能力、参数效率以及鲁棒性方面超过了当前最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，具有多样的症状，包括言语清晰度下降。现有研究通过预测临床标准ALSFRS-R来识别ALS患者的失语症，依赖于特征提取策略和定制的卷积神经网络设计后接稠密层的方法。然而，最近的研究表明，采用输入条件计算逻辑的神经网络具有一系列优点，如更快的训练速度、更好的性能以及灵活性。为了解决这些问题，我们提出了首个将超网络用于识别失语症的研究。具体来说，我们将音频文件转换成log-Mel频谱图、delta和delta-delta，并通过预训练修改后的AlexNet模型处理这些图像。最后，使用生成目标网络权重的超网络来完成任务。实验在新收集并公开的VOC-ALS数据集上进行，结果显示所提出的方法准确率高达82.66%，优于包括多模态融合方法在内的强基准模型，并且消融研究结果表明了该方法的有效性。总的来说，我们的方法在泛化能力、参数效率和鲁棒性方面相对最先进的成果具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Amyotrophic Lateral Sclerosis (ALS) constitutes a progressiveneurodegenerative disease with varying symptoms, including decline in speechintelligibility. Existing studies, which recognize dysarthria in ALS patientsby predicting the clinical standard ALSFRS-R, rely on feature extractionstrategies and the design of customized convolutional neural networks followedby dense layers. However, recent studies have shown that neural networksadopting the logic of input-conditional computations enjoy a series ofbenefits, including faster training, better performance, and flexibility. Toresolve these issues, we present the first study incorporating hypernetworksfor recognizing dysarthria. Specifically, we use audio files, convert them intolog-Mel spectrogram, delta, and delta-delta, and pass the resulting imagethrough a pretrained modified AlexNet model. Finally, we use a hypernetwork,which generates weights for a target network. Experiments are conducted on anewly collected publicly available dataset, namely VOC-ALS. Results showed thatthe proposed approach reaches Accuracy up to 82.66% outperforming strongbaselines, including multimodal fusion methods, while findings from an ablationstudy demonstrated the effectiveness of the introduced methodology. Overall,our approach incorporating hypernetworks obtains valuable advantages overstate-of-the-art results in terms of generalization ability, parameterefficiency, and robustness.</description>
      <author>example@mail.com (Loukas Ilias, Dimitris Askounis)</author>
      <guid isPermaLink="false">2503.01892v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2503.02597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态大型语言模型AKI，该模型通过解锁因果注意力机制并引入跨模态互惠注意（MMA），使得图像和文本之间能够互相影响，从而解决了现有MLLMs中存在的视觉-语言不一致问题。&lt;h4&gt;背景&lt;/h4&gt;最近的多模态大型语言模型在感知和推理多模式查询方面取得了显著进展，但同时出现了一个关键挑战：生成的文字响应与给定的图文输入在事实层面上可能并不一致。&lt;h4&gt;目的&lt;/h4&gt;本文旨在从一个基础且未被探索的角度来解决视觉-语言不一致性问题，通过重新审视MLLMs的核心架构并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;AKI模型将传统的因果注意力机制转变为跨模态互惠注意（MMA），使得图像token可以关注文本token。这种设计简洁而有效，不需要增加额外的参数或训练时间。&lt;h4&gt;主要发现&lt;/h4&gt;通过在12个多模态理解基准测试中进行实验，AKI比现有模型平均高出7.2%，展示了其出色的性能表现。&lt;h4&gt;结论&lt;/h4&gt;AKI模型通过引入跨模态互惠注意机制成功地提高了多模态语言模型的视觉-语言一致性，并且该设计具有通用性和可扩展性，适用于各种模态和多样化的多模态场景。作者公开了代码并计划发布AKI-4B模型以推动未来在MLLMs领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近的多模态大型语言模型（MLLM）在处理多模式查询方面展示了显著的进步，并开启了基础模型研究的新时代，但视觉和语言之间的不一致问题成为一个关键挑战。本文通过重新审视MLLM的核心架构提出了一种新的解决方案：AKI模型，它引入了跨模态互惠注意机制来增强图像token与文本token之间的影响关系。实验结果表明AKI在多个基准测试中表现出了优越的性能，并且作者计划发布他们的模型以鼓励该领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in perceiving and reasoning over multimodal inquiries, ushering in anew research era for foundation models. However, vision-language misalignmentin MLLMs has emerged as a critical challenge, where the textual responsesgenerated by these models are not factually aligned with the given text-imageinputs. Existing efforts to address vision-language misalignment have focusedon developing specialized vision-language connectors or leveraging visualinstruction tuning from diverse domains. In this paper, we tackle this issuefrom a fundamental yet unexplored perspective by revisiting the corearchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMsconsisting of a causal attention mechanism, which limits the ability of earliermodalities (e.g., images) to incorporate information from later modalities(e.g., text). To address this problem, we propose AKI, a novel MLLM thatunlocks causal attention into modality-mutual attention (MMA) to enable imagetokens to attend to text tokens. This simple yet effective design allows AKI toachieve superior performance in 12 multimodal understanding benchmarks (+7.2%on average) without introducing additional parameters and increasing trainingtime. Our MMA design is intended to be generic, allowing for application acrossvarious modalities, and scalable to accommodate diverse multimodal scenarios.The code is publicly available at https://github.com/sony/aki, and we willrelease our AKI-4B model to encourage further advancements in MLLMs acrossvarious directions.</description>
      <author>example@mail.com (Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi)</author>
      <guid isPermaLink="false">2503.02597v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning is Not So Mysterious or Different</title>
      <link>http://arxiv.org/abs/2503.02113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了深度神经网络中一些看似特殊的泛化行为（如良性过拟合和双重下降）实际上可以使用现有的理论框架进行解释，这些行为并非仅出现在神经网络中。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型常常表现出异常的泛化特性，这被认为挑战了传统机器学习方法中的泛化观念。例如，过度参数化在实践中非常成功，而传统的智慧认为应该避免过拟合。&lt;h4&gt;目的&lt;/h4&gt;论证这些现象并不独特于神经网络，并且可以通过现有的理论框架（如PAC-Bayes和可数假设边界）进行理解。&lt;h4&gt;方法&lt;/h4&gt;提出“软先验偏置”作为解释这些问题的关键原则：不是限制假设空间以避免过拟合，而是使用灵活的假设空间并倾向于那些与数据一致但更简单的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型并不像人们通常认为的那样神秘或独特；相反，“软先验偏置”的原理可以应用于多种模型类别。&lt;h4&gt;结论&lt;/h4&gt;虽然深度神经网络在泛化行为上可能与其他模型类似，但在表示学习、模式连接等方面存在相对独特的特性。&lt;h4&gt;翻译&lt;/h4&gt;深层神经网络经常被看作不同于其他类型的模型，并挑战了关于泛化的传统观点。例如，良性过拟合、双重下降现象以及过度参数化的成功应用等。本文认为这些现象并不仅存在于神经网络中，且可以通过PAC-Bayes和可数假设边界等长期存在的理论框架来进行理解和严谨地刻画。我们提出了一种称为“软先验偏置”的原则作为解释这些现象的关键要素：即在不试图避免过拟合的情况下限制假设空间时，采用一个灵活的假设空间，并倾向于那些与数据一致但更简单的解决方案。这一原理可以应用于多种模型类别中，因此深度学习并不像人们通常认为的那样神秘或独特。然而，我们还强调了深度学习的独特之处，比如它在表示学习中的能力、模式连接现象等相对普遍性的特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks are often seen as different from other model classes bydefying conventional notions of generalization. Popular examples of anomalousgeneralization behaviour include benign overfitting, double descent, and thesuccess of overparametrization. We argue that these phenomena are not distinctto neural networks, or particularly mysterious. Moreover, this generalizationbehaviour can be intuitively understood, and rigorously characterized usinglong-standing generalization frameworks such as PAC-Bayes and countablehypothesis bounds. We present soft inductive biases as a key unifying principlein explaining these phenomena: rather than restricting the hypothesis space toavoid overfitting, embrace a flexible hypothesis space, with a soft preferencefor simpler solutions that are consistent with the data. This principle can beencoded in many model classes, and thus deep learning is not as mysterious ordifferent from other model classes as it might seem. However, we also highlighthow deep learning is relatively distinct in other ways, such as its ability forrepresentation learning, phenomena such as mode connectivity, and its relativeuniversality.</description>
      <author>example@mail.com (Andrew Gordon Wilson)</author>
      <guid isPermaLink="false">2503.02113v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deal: Distributed End-to-End GNN Inference for All Nodes</title>
      <link>http://arxiv.org/abs/2503.02960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Deal的分布式GNN推理系统，该系统专注于对拥有数十亿边的图进行端到端的所有节点推理。通过在采样期间挖掘未充分利用的共享机会，并优化后续GNN计算中的共享收益，Deal系统展示了高效的内存使用和通信效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在推荐和广告等应用中广泛使用，其常见的形式是为所有节点进行端到端推理。然而，在处理大规模图时，传统的分布式方法难以实现高效的资源共享以优化计算性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的系统——Deal，旨在解决现有GNN推理过程中的资源浪费问题，并最大化共享收益，从而提高对大规模图形的推理效率和减少内存使用量。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了能够高效协作划分分布式推理的一维图和特征张量的节省内存且通信高效的分布式原语。       2. 引入分区化、流水线化的通讯方式，并将初始GNN原始计算与特性准备过程融合，以实现端到端推断。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的基准数据集上，使用Deal进行端到端推理的时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍。这表明了新方法的有效性和效率提升。&lt;h4&gt;结论&lt;/h4&gt;Deal系统成功地解决了大规模图的分布式GNN推理问题，在保证准确性的同时极大地提高了计算效率和资源利用效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：图神经网络（GNNs）是一个新的研究前沿，具有多种应用及成功的案例。对于所有的节点进行端到端推断是常见的GNN嵌入模型的方式，并且在推荐系统与广告领域得到广泛应用。虽然在诸如推理特定节点和训练等任务中出现了共享的机会，但针对全图的端到端推断中潜在的共享机会却大大被忽视了，因为传统的努力由于巨大的开销或过度的内存使用而无法充分提取共享的好处。本文介绍了Deal，一种专门用于具有数十亿边的图进行端到端推理的分布式GNN推理系统。首先，我们揭示并利用了在采样期间未被发现的分享机会，并最大化后续GNN计算中的分享收益。其次，我们引入了节省内存和通信高效的分布式原语，这些原语基于一维图形和特征张量协作划分进行轻量化分布式推断。第三，我们介绍了分区、流水线通信，以及将特性准备与第一个GNN原始计算融合的端到端推理方法。通过使用Deal，在现实世界的基准数据集上，端到端推断时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍，相比现有最佳技术而言有显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are a new research frontier with variousapplications and successes. The end-to-end inference for all nodes, is commonfor GNN embedding models, which are widely adopted in applications likerecommendation and advertising. While sharing opportunities arise in GNN tasks(i.e., inference for a few nodes and training), the potential for sharing infull graph end-to-end inference is largely underutilized because traditionalefforts fail to fully extract sharing benefits due to overwhelming overheads orexcessive memory usage.  This paper introduces Deal, a distributed GNN inference system that isdedicated to end-to-end inference for all nodes for graphs with multi-billionedges. First, we unveil and exploit an untapped sharing opportunity duringsampling, and maximize the benefits from sharing during subsequent GNNcomputation. Second, we introduce memory-saving and communication-efficientdistributed primitives for lightweight 1-D graph and feature tensorcollaborative partitioning-based distributed inference. Third, we introducepartitioned, pipelined communication and fusing feature preparation with thefirst GNN primitive for end-to-end inference. With Deal, the end-to-endinference time on real-world benchmark datasets is reduced up to 7.70 x and thegraph construction time is reduced up to 21.05 x, compared to thestate-of-the-art.</description>
      <author>example@mail.com (Shiyang Chen, Xiang Song, Vasiloudis Theodore, Hang Liu)</author>
      <guid isPermaLink="false">2503.02960v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment</title>
      <link>http://arxiv.org/abs/2503.01794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, and 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对CLIP模型的改进方案OFF-CLIP，通过引入off-diagonal项损失来优化正常病例检测，并采用句子级文本过滤方法减少假阴性结果。&lt;h4&gt;背景&lt;/h4&gt;CLIP在放射学中的零样本分类中减少了对人工注释的依赖，但传统的对比学习方法在处理正常案例时效果不佳，因为严格的内部样例对齐会干扰正常样本聚类。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法OFF-CLIP以解决正常病例检测中的问题，并提高医疗视觉语言模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;引入off-diagonal项损失以增强正常样本的聚类能力；使用句子级文本过滤去除不匹配正常的陈述，从而降低假阴性结果；该方案无需对现有的CLIP架构进行修改即可应用。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示OFF-CLIP显著提高了VinDr-CXR数据集上的正常分类性能，AUC分数较基线方法CARZero提升了0.61，并且保持或改善了异常情况的分类性能。同时，OFF-CLIP还增强了零样本定位能力，提高指向游戏准确性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了OFF-CLIP的有效性和效率，表明其是增强医疗视觉语言模型的一个稳健和有效的改进方案。&lt;h4&gt;翻译&lt;/h4&gt;对比性语言图像预训练（CLIP）在放射学中实现了零样本分类，减少了对人工注释的依赖。然而，传统的对比学习方法难以处理正常案例检测问题，严格的内部样例对齐会导致正常样本聚类失效以及高假阳性率和假阴性率。为解决这些问题，我们提出了OFF-CLIP，这是一种改进的对比学习方案，通过引入off-diagonal项损失来优化正常样本聚类，并应用句子级文本过滤减少假阴性结果。该方法可以在现有放射学CLIP模型上实现而无需修改任何架构。实验结果显示，OFF-CLIP在VinDr-CXR数据集上的正常分类性能显著提升，相较于最先进的零样本分类基线CARZero，AUC分数提升了0.61，并且保持或改善了异常情况的分类性能。此外，OFF-CLIP还增强了零样本定位能力，通过提高指向游戏准确性来确认更好的异常位置识别。这些结果表明OFF-CLIP作为增强医疗视觉语言模型的有效性和效率上的显著改进方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shotclassification in radiology, reducing reliance on manual annotations. However,conventional contrastive learning struggles with normal case detection due toits strict intra-sample alignment, which disrupts normal sample clustering andleads to high false positives (FPs) and false negatives (FNs). To address theseissues, we propose OFF-CLIP, a contrastive learning refinement that improvesnormal detection by introducing an off-diagonal term loss to enhance normalsample clustering and applying sentence-level text filtering to mitigate FNs byremoving misaligned normal statements from abnormal reports. OFF-CLIP can beapplied to radiology CLIP models without requiring any architecturalmodifications. Experimental results show that OFF-CLIP significantly improvesnormal classification, achieving a 0.61 Area under the curve (AUC) increase onVinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline,while maintaining or improving abnormal classification performance.Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing gameaccuracy, confirming better anomaly localization. These results demonstrateOFF-CLIP's effectiveness as a robust and efficient enhancement for medicalvision-language models.</description>
      <author>example@mail.com (Junhyun Park, Chanyu Moon, Donghwan Lee, Kyungsu Kim, Minho Hwang)</author>
      <guid isPermaLink="false">2503.01794v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Node-level Contrastive Unlearning on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图数据去学习方法Node-CUL，利用嵌入空间优化来移除特定节点和边对模型的影响。通过对比剩余节点及其邻居的嵌入，逐步减弱目标节点的影响，并且不直接使用未见数据，保持了模型的有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理非欧几里得结构化的图形数据时面临挑战，特别是去除部分实体（如节点和边）的困难。现有的方法如图划分、影响函数或添加额外层都难以同时达到高可扩展性和有效性。&lt;h4&gt;目的&lt;/h4&gt;提出一种更有效的去学习算法以移除特定图实体对模型的影响，并保持模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;通过对比剩余节点及其邻居的嵌入空间，逐步减弱目标节点（即要去除的节点）影响。另外还引入了邻域重构方法来优化邻居的嵌入从而减少未学节点对其它部分的影响。&lt;h4&gt;主要发现&lt;/h4&gt;Node-CUL在多种图数据和模型上进行实验后显示，其去学习效果最佳，并且提高了模型的有效性，同时只需要与现有框架类似的计算资源。&lt;h4&gt;结论&lt;/h4&gt;本文提出的Node-CUL方法证明了使用嵌入空间优化是一种高效去除特定节点对GNN影响的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning aims to remove a subset of graph entities (i.e. nodes andedges) from a graph neural network (GNN) trained on the graph. Unlike machineunlearning for models trained on Euclidean-structured data, effectivelyunlearning a model trained on non-Euclidean-structured data, such as graphs, ischallenging because graph entities exhibit mutual dependencies. Existing worksutilize graph partitioning, influence function, or additional layers to achievegraph unlearning. However, none of them can achieve high scalability andeffectiveness without additional constraints. In this paper, we achieve moreeffective graph unlearning by utilizing the embedding space. The primarytraining objective of a GNN is to generate proper embeddings for each node thatencapsulates both structural information and node feature representations.Thus, directly optimizing the embedding space can effectively remove the targetnodes' information from the model. Based on this intuition, we proposenode-level contrastive unlearning (Node-CUL). It removes the influence of thetarget nodes (unlearning nodes) by contrasting the embeddings of remainingnodes and neighbors of unlearning nodes. Through iterative updates, theembeddings of unlearning nodes gradually become similar to those of unseennodes, effectively removing the learned information without directlyincorporating unseen data. In addition, we introduce a neighborhoodreconstruction method that optimizes the embeddings of the neighbors in orderto remove influence of unlearning nodes to maintain the utility of the GNNmodel. Experiments on various graph data and models show that our Node-CULachieves the best unlearn efficacy and enhanced model utility with requiringcomparable computing resources with existing frameworks.</description>
      <author>example@mail.com (Hong kyu Lee, Qiuchen Zhang, Carl Yang, Li Xiong)</author>
      <guid isPermaLink="false">2503.02959v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</title>
      <link>http://arxiv.org/abs/2503.01711v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  added project repository &amp; dataset URL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;个性化产品搜索旨在检索和排序符合用户偏好和搜索意图的商品。现有的方法虽然有效，但通常假设用户的查询完全捕捉到了他们的实际动机。&lt;h4&gt;背景&lt;/h4&gt;现实中电商平台的数据分析显示，用户在进行搜索之前经常参与相关的咨询活动，这表明他们在咨询过程中根据动机和需求来细化意图。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的个性化搜索方法，利用咨询中隐含的动机作为提升个性化搜索的关键因素。&lt;h4&gt;主要挑战&lt;/h4&gt;包括将上下文中的动机与简洁查询相匹配、跨越类别-文本差距以及过滤序列历史中的噪声等新挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个考虑动机的个性化搜索（MAPS）方法。该方法利用大型语言模型将查询和咨询嵌入到统一语义空间，通过注意力专家混合机制优先处理关键语义，并引入双元对齐：对比学习对齐咨询、评论和产品特性；双向注意机制整合了基于动机感知的嵌入与用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;在真实数据集和合成数据集上的广泛实验表明MAPS方法在检索和排序任务中均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过利用用户的咨询活动中的隐含动机，个性化搜索可以得到显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized product search aims to retrieve and rank items that match users'preferences and search intent. Despite their effectiveness, existing approachestypically assume that users' query fully captures their real motivation.However, our analysis of a real-world e-commerce platform reveals that usersoften engage in relevant consultations before searching, indicating they refineintents through consultations based on motivation and need. The impliedmotivation in consultations is a key enhancing factor for personalized search.This unexplored area comes with new challenges including aligning contextualmotivations with concise queries, bridging the category-text gap, and filteringnoise within sequence history. To address these, we propose a Motivation-AwarePersonalized Search (MAPS) method. It embeds queries and consultations into aunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)to prioritize critical semantics, and introduces dual alignment: (1)contrastive learning aligns consultations, reviews, and product features; (2)bidirectional attention integrates motivation-aware embeddings with userpreferences. Extensive experiments on real and synthetic data show MAPSoutperforms existing methods in both retrieval and ranking tasks.</description>
      <author>example@mail.com (Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu)</author>
      <guid isPermaLink="false">2503.01711v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A dataset-free approach for self-supervised learning of 3D reflectional symmetries</title>
      <link>http://arxiv.org/abs/2503.02660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种无需大型标注数据集的自监督模型，用于检测单个物体的对称性。通过利用对象本身的内在特征进行学习，并设计了不依赖于地面真实标签的学习策略。&lt;h4&gt;背景&lt;/h4&gt;传统的模型需要大量的带标签的数据来训练和识别物体的对称性，这增加了计算成本和时间开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种既有效又高效的自监督方法，用于检测单个物体的对称性，并降低依赖于大型数据集的成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种基于点云视觉特征的方法，利用基础图像模型提取出的特性来计算对象各点的描述符。这些描述符反映了物体上两点之间的对称关系，并且有助于优化自监督学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该自监督模型在检测单个物体对称性方面优于使用大型数据集训练的状态-of-the-art模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的自监督方法不仅效果好，而且更加高效、资源需求低，适用于计算和数据资源有限的场景。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们探索了一种无需依赖于数据集就能学习单个物体对称性的自我监督模型。基于假设：可以通过分析对象自身的固有特征来确定其对称性，从而在训练过程中不需要大型的数据集。此外，还设计了一个自监督学习策略以消除地面真实标签的必要性。这两个关键要素使我们的方法既有效又高效，解决了构建大量标注数据集的成本问题。该研究的独特之处在于计算每个点上基于视觉外观相似性的对象特征，并利用基础图像模型提取出的特性来优化自我监督模型。实验结果表明，该自监督方法在检测单个物体对称性方面超越了依赖于大规模数据集训练的状态-of-the-art模型，且更加高效和资源节约。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a self-supervised model that learns to detect thesymmetry of a single object without requiring a dataset-relying solely on theinput object itself. We hypothesize that the symmetry of an object can bedetermined by its intrinsic features, eliminating the need for large datasetsduring training. Additionally, we design a self-supervised learning strategythat removes the necessity of ground truth labels. These two key elements makeour approach both effective and efficient, addressing the prohibitive costsassociated with constructing large, labeled datasets for this task. The noveltyof our method lies in computing features for each point on the object based onthe idea that symmetric points should exhibit similar visual appearances. Toachieve this, we leverage features extracted from a foundational image model tocompute a visual descriptor for the points. This approach equips the pointcloud with visual features that facilitate the optimization of ourself-supervised model. Experimental results demonstrate that our methodsurpasses the state-of-the-art models trained on large datasets. Furthermore,our model is more efficient, effective, and operates with minimal computationaland data resources.</description>
      <author>example@mail.com (Issac Aguirre, Ivan Sipiran, Gabriel Montañana)</author>
      <guid isPermaLink="false">2503.02660v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders</title>
      <link>http://arxiv.org/abs/2503.02954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 International Conference on Robotics and Automation  (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了使用图神经网络变分自动编码器（GNN-VAE）来解决大规模多智能体协调问题，这种方法比传统的集中式优化方法更快。&lt;h4&gt;背景&lt;/h4&gt;在高密度机器人流量区域中，局部协调方法可能无法找到无死锁的解决方案。这时需要一个中央单元生成全局调度方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GNN-VAE的方法来解决大规模多智能体协调问题，提高效率和性能。&lt;h4&gt;方法&lt;/h4&gt;论文将协调问题建模为图问题，并使用混合整数线性规划（MILP）求解器收集地面真实数据。在训练阶段，学习框架将高质量的解决方案编码到潜在空间中，在推理时从采样的潜在变量中解码出最优解。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-VAE框架可以生成满足问题约束条件的有效提案，并且对于大规模（250个机器人）的问题也能够提供高质量的解决方案，速度远超其他基准方法。&lt;h4&gt;结论&lt;/h4&gt;提出的基于GNN-VAE的方法在处理大规模多智能体协调问题时具有显著优势。该项目页为https://mengyuest.github.io/gnn-vae-coord。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了如何利用图神经网络变分自动编码器（GNN-VAE）来更高效地解决大规模的多机器人协调问题，特别是在密集机器人交通区域中，这种方法比传统的集中式优化方法更加有效和快速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent coordination is crucial for reliable multi-robot navigation inshared spaces such as automated warehouses. In regions of dense robot traffic,local coordination methods may fail to find a deadlock-free solution. In thesescenarios, it is appropriate to let a central unit generate a global schedulethat decides the passing order of robots. However, the runtime of suchcentralized coordination methods increases significantly with the problemscale. In this paper, we propose to leverage Graph Neural Network VariationalAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scalefaster than through centralized optimization. We formulate the coordinationproblem as a graph problem and collect ground truth data using a Mixed-IntegerLinear Program (MILP) solver. During training, our learning framework encodesgood quality solutions of the graph problem into a latent space. At inferencetime, solution samples are decoded from the sampled latent variables, and thelowest-cost sample is selected for coordination. Finally, the feasible proposalwith the highest performance index is selected for the deployment. Byconstruction, our GNN-VAE framework returns solutions that always respect theconstraints of the considered coordination problem. Numerical results show thatour approach trained on small-scale problems can achieve high-quality solutionseven for large-scale problems with 250 robots, being much faster than otherbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord</description>
      <author>example@mail.com (Yue Meng, Nathalie Majcherczyk, Wenliang Liu, Scott Kiesel, Chuchu Fan, Federico Pecora)</author>
      <guid isPermaLink="false">2503.02954v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
      <link>http://arxiv.org/abs/2503.01606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的Embedding级别框架EmbQA，旨在改进开放领域问题回答(ODQA)任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型推动了开放领域问题回答的发展，但目前的检索-阅读器流水线存在计算成本高、不稳定性和检索覆盖率不足的问题。&lt;h4&gt;目的&lt;/h4&gt;通过增强检索器和读者的功能来解决当前ODQA系统存在的上述挑战。&lt;h4&gt;方法&lt;/h4&gt;{'改进查询表示': '利用轻量级线性层在无监督对比学习目标下优化查询表示，重新排序检索到的段落以突出最可能包含正确答案的部分。', '引入探索性嵌入': '通过扩展模型潜在语义空间来多样化候选生成，并采用基于熵的选择机制自动选择最有信心的答案。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在三个开源LLM、三种检索方法和四个ODQA基准上，EmbQA在准确性和效率方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;EmbQA框架通过改进查询表示和引入探索性嵌入有效提升了ODQA任务的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have recently pushed open domain question answering(ODQA) to new frontiers. However, prevailing retriever-reader pipelines oftendepend on multiple rounds of prompt level instructions, leading to highcomputational overhead, instability, and suboptimal retrieval coverage. In thispaper, we propose EmbQA, an embedding-level framework that alleviates theseshortcomings by enhancing both the retriever and the reader. Specifically, werefine query representations via lightweight linear layers under anunsupervised contrastive learning objective, thereby reordering retrievedpassages to highlight those most likely to contain correct answers.Additionally, we introduce an exploratory embedding that broadens the model'slatent semantic space to diversify candidate generation and employs anentropy-based selection mechanism to choose the most confident answerautomatically. Extensive experiments across three open-source LLMs, threeretrieval methods, and four ODQA benchmarks demonstrate that EmbQAsubstantially outperforms recent baselines in both accuracy and efficiency.</description>
      <author>example@mail.com (Zhanghao Hu, Hanqi Yan, Qingling Zhu, Zhenyi Shen, Yulan He, Lin Gui)</author>
      <guid isPermaLink="false">2503.01606v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection via Knowledge Injection</title>
      <link>http://arxiv.org/abs/2503.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为知识注入的Deepfake检测技术（KID），它构建了一个基于多任务学习的知识注入框架，可以轻松集成到现有的ViT基础模型中。&lt;h4&gt;背景&lt;/h4&gt;当前的生成式AI模型能够创造出非常逼真的deepfakes，这些内容可能被用于恶意用途。现有的deepfake检测方法要么依赖于改进分类器以更好地适应训练数据分布，要么利用伪造合成机制来学习更全面的伪造数据分布。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法忽视真实数据知识的问题，并提高模型处理未见过的真实和虚假数据的能力，本文提出了一种新的简单而有效的方法KID。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于多任务学习的知识注入框架。设计了知识注入模块来学习并注入必要的信息到基础模型中，以更准确地建模真实和伪造数据的分布；构造了一个粗粒度的伪造定位分支，通过多任务学习方式学习伪造位置，进一步丰富知识注入模块中的伪造知识。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种层次化的抑制损失与对比损失，强调了在知识注入模块中对真实数据知识的关注，并平衡真实和虚假的知识比例。实验表明KID具有优秀的兼容性，可以应用于不同规模的ViT基础模型，并且实现了最先进的泛化性能同时提升了训练收敛速度。&lt;h4&gt;结论&lt;/h4&gt;KID是一种新颖有效的deepfake检测技术，能够在现有ViT基础模型上简单高效地实现知识注入，提高了模型在处理真实和伪造数据上的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake detection technologies become vital because current generative AImodels can generate realistic deepfakes, which may be utilized in maliciouspurposes. Existing deepfake detection methods either rely on developingclassification methods to better fit the distributions of the training data, orexploiting forgery synthesis mechanisms to learn a more comprehensive forgerydistribution. Unfortunately, these methods tend to overlook the essential roleof real data knowledge, which limits their generalization ability in processingthe unseen real and fake data. To tackle these challenges, in this paper, wepropose a simple and novel approach, named Knowledge Injection based deepfakeDetection (KID), by constructing a multi-task learning based knowledgeinjection framework, which can be easily plugged into existing ViT-basedbackbone models, including foundation models. Specifically, a knowledgeinjection module is proposed to learn and inject necessary knowledge into thebackbone model, to achieve a more accurate modeling of the distributions ofreal and fake data. A coarse-grained forgery localization branch is constructedto learn the forgery locations in a multi-task learning manner, to enrich thelearned forgery knowledge for the knowledge injection module. Two layer-wisesuppression and contrast losses are proposed to emphasize the knowledge of realdata in the knowledge injection module, to further balance the portions of thereal and fake knowledge. Extensive experiments have demonstrated that our KIDpossesses excellent compatibility with different scales of Vit-based backbonemodels, and achieves state-of-the-art generalization performance whileenhancing the training convergence speed.</description>
      <author>example@mail.com (Tonghui Li, Yuanfang Guo, Zeming Liu, Heqi Peng, Yunhong Wang)</author>
      <guid isPermaLink="false">2503.02503v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</title>
      <link>http://arxiv.org/abs/2503.02448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为NodeNAS的节点特定图神经架构搜索方法，它通过拆解节点拓扑和图分布来为不同节点定制独特的聚合方法。此外，还提出了自适应聚集注意力多维NodeNAS（MNNAS）方法，在有限的数据集下学习具有良好泛化能力的节点特定架构定制器。&lt;h4&gt;背景&lt;/h4&gt;现有的GraphNAS方法在处理数据分布变化时表现出色，但它们依赖于大量训练数据，并且当面对稀疏或单一训练图时难以发现最佳的图形与架构映射关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在有限数据集下工作的节点特定图神经架构搜索（NodeNAS）方法，以提高模型在不同分布的数据上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为NodeNAS的方法，通过拆解节点拓扑和图分布来为不同的节点定制独特的聚合方法。2. 引入了自适应聚集注意力多维NodeNAS（MNNAS）方法，该方法学习了一个具有良好泛化能力的节点特定架构定制器，并且扩展了搜索空间的垂直深度以支持跨多个维度的同时进行节点特定架构定制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，提出的MNNAS方法超越了现有的最佳算法，在监督和非监督任务中取得了卓越的性能，并展示了优秀的出分布（OOD）泛化能力。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法NodeNAS及其扩展版本MNNAS有效地解决了现有GraphNAS方法在面对稀疏或单一训练图时的问题，能够在更少的数据下发现更具针对性且具有良好泛化的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural architecture search (GraphNAS) has demonstrated advantages inmitigating performance degradation of graph neural networks (GNNs) due todistribution shifts. Recent approaches introduce weight sharing across tailoredarchitectures, generating unique GNN architectures for each graph end-to-end.However, existing GraphNAS methods do not account for distribution patternsacross different graphs and heavily rely on extensive training data. Withsparse or single training graphs, these methods struggle to discover optimalmappings between graphs and architectures, failing to generalize toout-of-distribution (OOD) data. In this paper, we propose node-specific graphneural architecture search(NodeNAS), which aims to tailor distinct aggregationmethods for different nodes through disentangling node topology and graphdistribution with limited datasets. We further propose adaptive aggregationattention based multi-dim NodeNAS method(MNNAS), which learns an node-specificarchitecture customizer with good generalizability. Specifically, we extend thevertical depth of the search space, supporting simultaneous node-specificarchitecture customization across multiple dimensions. Moreover, we model thepower-law distribution of node degrees under varying assortativity, encodingstructure invariant information to guide architecture customization across eachdimension. Extensive experiments across supervised and unsupervised tasksdemonstrate that MNNAS surpasses state-of-the-art algorithms and achievesexcellent OOD generalization.</description>
      <author>example@mail.com (Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu)</author>
      <guid isPermaLink="false">2503.02448v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Actionable World Models for Industrial Process Control</title>
      <link>http://arxiv.org/abs/2503.01411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，使复杂系统的行为能够从有限的数据中学到，并实现对系统的精细控制。&lt;h4&gt;背景&lt;/h4&gt;从被动的过程监控过渡到主动的过程控制需要一个有效的人工智能系统，它可以从非常有限的训练数据中学习复杂系统的特性。&lt;h4&gt;目的&lt;/h4&gt;为了形成关于过程输入和输出的即兴数字孪生体，该模型可以预测行动对流程世界的影响，研究旨在提供一种方法论以实现有效的主动过程控制。&lt;h4&gt;方法&lt;/h4&gt;通过对比性学习在联合嵌入预测架构内驱动表示学习，这种方法使从输入变化到表示变化的变化可预测，并且反过来亦然。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够揭示关键因素对流程变异的影响，为有效控制行动提供基础，以便将过程保持在其操作范围内。&lt;h4&gt;结论&lt;/h4&gt;在塑料注射成型的例子中验证了该方法的有效性，展示了其在提出针对不稳定过程的特定控制措施方面的实际相关性。&lt;h4&gt;翻译&lt;/h4&gt;为了从（被动）过程监控过渡到主动的过程控制，有效的AI系统必须能够从非常有限的训练数据中学到复杂系统的特性。研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，并形成关于过程输入和输出的即兴数字孪生体。这种方法使复杂的行动后果可以被预测，为有效的主动控制铺平道路。使用塑料注射成型作为示例验证了此方法的有效性，展示了其提出针对不稳定过程的具体控制措施的实际相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To go from (passive) process monitoring to active process control, aneffective AI system must learn about the behavior of the complex system fromvery limited training data, forming an ad-hoc digital twin with respect toprocess in- and outputs that captures the consequences of actions on theprocess's world. We propose a novel methodology based on learning world modelsthat disentangles process parameters in the learned latent representation,allowing for fine-grained control. Representation learning is driven by thelatent factors that influence the processes through contrastive learning withina joint embedding predictive architecture. This makes changes inrepresentations predictable from changes in inputs and vice versa, facilitatinginterpretability of key factors responsible for process variations, paving theway for effective control actions to keep the process within operationalbounds. The effectiveness of our method is validated on the example of plasticinjection molding, demonstrating practical relevance in proposing specificcontrol actions for a notoriously unstable process.</description>
      <author>example@mail.com (Peng Yan, Ahmed Abdulkadir, Gerrit A. Schatte, Giulia Aguzzi, Joonsu Gha, Nikola Pascher, Matthias Rosenthal, Yunlong Gao, Benjamin F. Grewe, Thilo Stadelmann)</author>
      <guid isPermaLink="false">2503.01411v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>From superposition to sparse codes: interpretable representations in neural networks</title>
      <link>http://arxiv.org/abs/2503.01824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;理解神经网络中信息的表示是神经科学和人工智能领域的基本挑战。尽管它们具有非线性架构，最近的研究表明，神经网络以叠加的方式编码特征，即输入概念在网络表示中线性地重叠。&lt;h4&gt;背景&lt;/h4&gt;神经网络在分类任务中的训练可以恢复潜在特征，这些特征可以通过线性变换进行识别。此外，稀疏编码方法可以从这些表示中提取解耦的特性，并且定量解释性度量为评估这些方法的成功提供了途径，确保提取的功能与人类可理解的概念一致。&lt;h4&gt;目的&lt;/h4&gt;提供一个理论框架来解释神经网络如何以叠加方式存储信息，并提出一种从神经激活中提取可解释表示的方法论。&lt;h4&gt;方法&lt;/h4&gt;理论框架包括三个步骤：(1) 可识别性理论表明训练的神经网络可以恢复分类任务中的潜在特征，这些特征可以通过线性变换进行识别。(2) 稀疏编码技术可以从神经网络的表示中提取解耦特性，通过压缩感知的原则实现。(3) 定量解释度量为评估方法的成功提供了标准。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种从理论神经科学、表示学习和可解释性研究中的见解出发的新视角，来理解人工和生物系统中的神经表示。这些论点对神经编码理论、人工智能透明度以及使深度学习模型更加可解释的总体目标有重要意义。&lt;h4&gt;结论&lt;/h4&gt;通过连接来自不同领域的见解，这项工作为理解和改善神经网络的表示提供了新的视角，有助于推动神经科学与人工智能的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解如何在人工神经网络中表示信息是两个学科——神经科学和人工智能中的一个基本挑战。尽管这些网络具有非线性架构，但最近的研究表明，它们通过叠加存储特征，这意味着输入概念在网络表示中以线性方式重叠。我们提出了一个解释这一现象的视角，并为从激活中提取可解释表征提供了理论基础。我们的理论框架由三部分组成：(1) 可识别性理论显示神经网络在分类任务训练中恢复潜在特性到线性转换；(2) 稀疏编码技术可以从这些表示中通过压缩感知原理来提取不相关特征；(3) 定量解释度量为评估方法的有效性提供了一种方式，确保所提取的特征与人类可理解的概念保持一致。结合理论神经科学、表示学习和可解释性研究领域的洞见，我们提出了一个新兴视角以了解人工及生物系统中的神经表征，并且我们的论点对神经编码理论、人工智能透明度以及使深度学习模型更易于解释的目标具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how information is represented in neural networks is afundamental challenge in both neuroscience and artificial intelligence. Despitetheir nonlinear architectures, recent evidence suggests that neural networksencode features in superposition, meaning that input concepts are linearlyoverlaid within the network's representations. We present a perspective thatexplains this phenomenon and provides a foundation for extracting interpretablerepresentations from neural activations. Our theoretical framework consists ofthree steps: (1) Identifiability theory shows that neural networks trained forclassification recover latent features up to a linear transformation. (2)Sparse coding methods can extract disentangled features from theserepresentations by leveraging principles from compressed sensing. (3)Quantitative interpretability metrics provide a means to assess the success ofthese methods, ensuring that extracted features align with human-interpretableconcepts. By bridging insights from theoretical neuroscience, representationlearning, and interpretability research, we propose an emerging perspective onunderstanding neural representations in both artificial and biological systems.Our arguments have implications for neural coding theories, AI transparency,and the broader goal of making deep learning models more interpretable.</description>
      <author>example@mail.com (David Klindt, Charles O'Neill, Patrik Reizinger, Harald Maurer, Nina Miolane)</author>
      <guid isPermaLink="false">2503.01824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB Image for Graspability-Aware Bin Picking</title>
      <link>http://arxiv.org/abs/2503.02387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, In submission to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了RGBSQGrasp框架，该框架结合了超二次体形状基元和基础模型驱动的深度估计方法，可以从单目RGB图像中推断出抓取姿态。&lt;h4&gt;背景&lt;/h4&gt;工件拾取任务由于遮挡和物理限制而具有挑战性。现有方法依赖于已知CAD模型或先验物体几何信息，并且从有限视角恢复超二次体形状是困难的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决工件拾取中遮挡、物体识别和抓取问题，同时提高对未知物体的适应能力。&lt;h4&gt;方法&lt;/h4&gt;RGBSQGrasp包括数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及超二次体引导的抓取姿态采样模块。该框架不需要深度传感器，并通过几何推理来推断抓取姿态，提高稳定性和适应性。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的机器人实验中显示了92%的成功率，证明了RGBSQGrasp在打包料箱拣选环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;RGBSQGrasp提供了一种有效的方法，通过单目RGB图像实现可靠抓取姿态推断，并显著提高了工件拾取任务的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;零件挑选是一个具有挑战性的机器人任务，由于遮挡和物理限制导致视觉信息不足。现有的方法通常依赖于已知CAD模型或先验物体几何形状，这限制了它们对新型未知对象的泛化能力。其他直接从RGB-D数据回归抓取姿态的方法则面临着深度传感中的固有噪声问题，并且缺乏对物体的理解使得抓取的姿态合成与评估更加困难。超二次体（SQ）提供了一种紧凑、可解释的形状表示，可以捕获物体的物理特性和抓取能力。然而，从有限视角恢复它们是具有挑战性的，现有的方法依赖于多个角度来进行近乎完整的点云重建，这限制了其在零件挑选任务中的有效性。为了应对这些挑战，我们提出了extbf{RGBSQGrasp}框架——一个利用超二次体形状基元和基础模型驱动的深度估计方法的抓取框架，可以从单目RGB图像中推断出抓取姿态--无需使用深度传感器。该框架整合了一个通用、跨平台的数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及一个由SQ指导的抓取姿态采样模块。通过结合这些组件，RGBSQGrasp可以通过几何推理可靠地推断出抓取姿态，提高抓取稳定性，并增强对未见过对象的适应性。在真实世界的机器人实验中展示出了92%的成功率，突显了RGBSQGrasp在包装料箱挑选环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bin picking is a challenging robotic task due to occlusions and physicalconstraints that limit visual information for object recognition and grasping.Existing approaches often rely on known CAD models or prior object geometries,restricting generalization to novel or unknown objects. Other methods directlyregress grasp poses from RGB-D data without object priors, but the inherentnoise in depth sensing and the lack of object understanding make graspsynthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,interpretable shape representation that captures the physical and graspabilityunderstanding of objects. However, recovering them from limited viewpoints ischallenging, as existing methods rely on multiple perspectives fornear-complete point cloud reconstruction, limiting their effectiveness inbin-picking. To address these challenges, we propose \textbf{RGBSQGrasp}, agrasping framework that leverages superquadric shape primitives and foundationmetric depth estimation models to infer grasp poses from a monocular RGB camera-- eliminating the need for depth sensors. Our framework integrates auniversal, cross-platform dataset generation pipeline, a foundation model-basedobject point cloud estimation module, a global-local superquadric fittingnetwork, and an SQ-guided grasp pose sampling module. By integrating thesecomponents, RGBSQGrasp reliably infers grasp poses through geometric reasoning,enhancing grasp stability and adaptability to unseen objects. Real-worldrobotic experiments demonstrate a 92\% grasp success rate, highlighting theeffectiveness of RGBSQGrasp in packed bin-picking environments.</description>
      <author>example@mail.com (Yifeng Xu, Fan Zhu, Ye Li, Sebastian Ren, Xiaonan Huang, Yuhao Chen)</author>
      <guid isPermaLink="false">2503.02387v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InfoGNN: End-to-end deep learning on mesh via graph neural networks</title>
      <link>http://arxiv.org/abs/2503.02414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个新的以图神经网络（GNN）为中心的端到端框架InfoGNN，用于解决深度学习在网格模型应用中的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D模型被广泛应用于各个行业，并且由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分。然而，无序、不规则的数据结构以及复杂的表面信息使得直接使用深度学习模型进行处理变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端框架以克服在网格模型中应用深度学习的挑战，同时充分利用网格模型的优点。&lt;h4&gt;方法&lt;/h4&gt;InfoGNN将网格模型视为图，采用InfoConv和InfoMP模块利用点的位置信息以及面法线、二面角等静态信息和动态全局特征信息来处理所有类型的数据，并且是一个端到端的设计以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个公开数据集上进行的测试中，InfoGNN在网格分类和分割任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;本文提出的新框架InfoGNN为复杂的3D模型提供了高效的深度学习方法，并通过简化网络设计提高了处理效率。&lt;h4&gt;翻译&lt;/h4&gt;三维模型被广泛应用于各个行业。由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分，能够提供直观且实用的丰富三维信息表达方式。然而，无序、不规则的数据结构和复杂的表面信息使得直接使用深度学习模型进行处理变得困难。传统的网格数据处理方法通常依赖于具有许多限制的网格模型（如流形），这些限制在实际应用中缩小了它们的应用范围，并未充分利用网格模型的优点。本文提出了一个基于图神经网络（GNN）的新端到端框架InfoGNN，用于解决深度学习应用于网格模型中的挑战。InfoGNN将网格模型视为图，使其能够高效处理不规则的网格数据。此外，我们提出InfoConv和InfoMP模块，利用点的位置信息，并充分利用面法线、二面角等静态信息以及动态全局特征信息来充分使用所有类型的数据。另外，InfoGNN是一个端到端框架，简化了网络设计以使其更加高效，为复杂3D模型的深度学习铺平道路。我们在几个公开可用数据集上进行了实验，结果显示在网格分类和分割任务中，InfoGNN表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D models are widely used in various industries, and mesh data has become anindispensable part of 3D modeling because of its unique advantages. Mesh datacan provide an intuitive and practical expression of rich 3D information.However, its disordered, irregular data structure and complex surfaceinformation make it challenging to apply with deep learning models directly.Traditional mesh data processing methods often rely on mesh models with manylimitations, such as manifold, which restrict their application scopes inreality and do not fully utilize the advantages of mesh models. This paperproposes a novel end-to-end framework for addressing the challenges associatedwith deep learning in mesh models centered around graph neural networks (GNN)and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enablesit to handle irregular mesh data efficiently. Moreover, we propose InfoConv andInfoMP modules, which utilize the position information of the points and fullyuse the static information such as face normals, dihedral angles, and dynamicglobal feature information to fully use all kinds of data. In addition, InfoGNNis an end-to-end framework, and we simplify the network design to make it moreefficient, paving the way for efficient deep learning of complex 3D models. Weconducted experiments on several publicly available datasets, and the resultsshow that InfoGNN achieves excellent performance in mesh classification andsegmentation tasks.</description>
      <author>example@mail.com (Ling Gao, Zhenyu Shu, Shiqing Xin)</author>
      <guid isPermaLink="false">2503.02414v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OCL: Ordinal Contrastive Learning for Imputating Features with Progressive Labels</title>
      <link>http://arxiv.org/abs/2503.02899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2024 (Provisional Accept)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的图像特征补全方法，利用多模态成像数据来更好地判断阿尔茨海默病的不同阶段。&lt;h4&gt;背景&lt;/h4&gt;准确地辨别阿尔茨海默病（AD）的各个发展阶段对于早期诊断和预防至关重要。然而，由于成本高和受试者负担重的原因，获取完整的影像集很具挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的补全方法来解决多模态成像数据中缺失值的问题，同时保持所有受试者的完整性。&lt;h4&gt;方法&lt;/h4&gt;所提方法包括两个网络：1）一个编码器，用于提取不受模式影响的嵌入；2）一个解码器，基于其影像模式重构原始测量。编码器采用了新颖的‘序数对比损失’来使样本根据AD进展对齐在嵌入空间中。&lt;h4&gt;主要发现&lt;/h4&gt;通过最大化的模态一致性以及领域对抗训练算法进一步增强了不同成像模态之间的对齐性，在实验结果中展示了对于统计分析和分类任务，该方法相较于基线补全技术有显著的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型在共享嵌入空间内促进了多模式影像特征的完整恢复，并且通过ADNI研究显示了优于现有技术的结果。&lt;h4&gt;翻译&lt;/h4&gt;准确地区分阿尔茨海默病（AD）的不同阶段对于早期诊断和预防至关重要。通常涉及多种成像模态来理解AD复杂的病理过程，然而由于成本高以及受试者负担重的原因，获取完整的一组图像具有挑战性。因此，在最终的实验中缺失数据不可避免地导致了样本量有限，并且降低了下游分析中的精确度。为了应对这一挑战，我们提出了一种全面的成像特征补全方法，该方法能够利用多样化的影像特征的同时保持所有受试者的完整性。所提的方法包括两个网络：1）一个编码器用于提取模态无关的嵌入；2）一个解码器根据各自的成像模式来重构原始测量值。编码器采用了新颖的‘序数对比损失’，将样本按AD进展在嵌入空间中对齐。我们还通过域对抗训练算法最大化了每个受试者内模态之间的嵌入一致性，进一步增强了不同影像模态之间的对齐性。所提的方法促进了在共享嵌入空间内的多模式成像特征的完整恢复。在实验中，我们展示了我们的网络对于统计分析和分类任务，相较于补全基线技术，在ADNI研究中提供了更优的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-72069-7_32&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately discriminating progressive stages of Alzheimer's Disease (AD) iscrucial for early diagnosis and prevention. It often involves multiple imagingmodalities to understand the complex pathology of AD, however, acquiring acomplete set of images is challenging due to high cost and burden for subjects.In the end, missing data become inevitable which lead to limited sample-sizeand decrease in precision in downstream analyses. To tackle this challenge, weintroduce a holistic imaging feature imputation method that enables to leveragediverse imaging features while retaining all subjects. The proposed methodcomprises two networks: 1) An encoder to extract modality-independentembeddings and 2) A decoder to reconstruct the original measures conditioned ontheir imaging modalities. The encoder includes a novel {\em ordinal contrastiveloss}, which aligns samples in the embedding space according to the progressionof AD. We also maximize modality-wise coherence of embeddings within eachsubject, in conjunction with domain adversarial training algorithms, to furtherenhance alignment between different imaging modalities. The proposed methodpromotes our holistic imaging feature imputation across various modalities inthe shared embedding space. In the experiments, we show that our networksdeliver favorable results for statistical analysis and classification againstimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)study.</description>
      <author>example@mail.com (Seunghun Baek, Jaeyoon Sim, Guorong Wu, Won Hwa Kim)</author>
      <guid isPermaLink="false">2503.02899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</title>
      <link>http://arxiv.org/abs/2503.01776v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A novel sparse coding framework designed for learning adaptive  representation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的稀疏表示方法CSR，它在保持高保真度的同时，能够实现自适应的深度表示，并且比现有的解决方案MRL具有更高的准确性和更快的速度。&lt;h4&gt;背景&lt;/h4&gt;许多大规模系统依赖高质量的深度表示（嵌入）来完成检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法可以实现自适应的嵌入长度，但是需要重新训练模型并且在较短长度时性能下降明显。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的稀疏编码方法CSR，以解决现有解决方案在保持高保真度的同时，提供灵活、成本效益高的不同稀疏级别推理的问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用轻量级的自动编码和任务感知对比目标，将预训练嵌入稀疏化到一个高维但选择性激活的功能空间中。这种方法被称为Contrastive Sparse Representation (CSR)。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上，在准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了稀疏编码作为自适应表示学习的重要范式，尤其是在实际应用中效率和保真度都至关重要的情况下。&lt;h4&gt;翻译&lt;/h4&gt;许多大型系统依赖于高质量的深度表示（嵌入）来促进检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法作为一种适应性表示长度的方法出现，但它需要重新训练整个模型并且在较短长度时性能明显下降。本文中，我们展示了稀疏编码提供了一种吸引人的替代方案，以实现具有最小开销和更高保真度的自适应表示。我们提出了对比稀疏表示（CSR）方法，该方法将预训练嵌入转换为一个高维但选择性激活的功能空间。通过利用轻量级自动编码器和任务感知对比目标，CSR在保持语义质量的同时允许不同稀疏级别的灵活、成本效益高的推理。广泛的实验表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上的准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。我们的结果确立了稀疏编码作为自适应表示学习的重要范式，在实际应用中效率和保真度都至关重要的情况下尤为适用。代码可在https://github.com/neilwen987/CSR_Adaptive_Rep上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/neilwen987/CSR_Adaptive_Rep&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many large-scale systems rely on high-quality deep representations(embeddings) to facilitate tasks like retrieval, search, and generativemodeling. Matryoshka Representation Learning (MRL) recently emerged as asolution for adaptive embedding lengths, but it requires full model retrainingand suffers from noticeable performance degradations at short lengths. In thispaper, we show that sparse coding offers a compelling alternative for achievingadaptive representation with minimal overhead and higher fidelity. We proposeContrastive Sparse Representation (CSR), a method that sparsifies pre-trainedembeddings into a high-dimensional but selectively activated feature space. Byleveraging lightweight autoencoding and task-aware contrastive objectives, CSRpreserves semantic quality while allowing flexible, cost-effective inference atdifferent sparsity levels. Extensive experiments on image, text, and multimodalbenchmarks demonstrate that CSR consistently outperforms MRL in terms of bothaccuracy and retrieval speed-often by large margins-while also cutting trainingtime to a fraction of that required by MRL. Our results establish sparse codingas a powerful paradigm for adaptive representation learning in real-worldapplications where efficiency and fidelity are both paramount. Code isavailable at https://github.com/neilwen987/CSR_Adaptive_Rep</description>
      <author>example@mail.com (Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You)</author>
      <guid isPermaLink="false">2503.01776v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework</title>
      <link>http://arxiv.org/abs/2503.02593v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的基于文本描述的点云定位框架CMMLoc，用于在大型城市环境中通过文本描述确定3D位置。&lt;h4&gt;背景&lt;/h4&gt;现有的点云定位方法通常需要完整的环境描述来匹配3D位置和文本描述。然而，在实际场景中，用户往往只提供部分相关的环境信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在仅有部分相关环境描述的情况下进行有效定位的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc，该框架在文本和点云之间交互时将CMM约束作为先验条件，并设计了空间整合方案来适应不同3D对象的不同感受野。此外，还提出了一个方向积分模块和模态预对准策略，以捕获物体之间的空间关系。&lt;h4&gt;主要发现&lt;/h4&gt;CMMLoc在KITTI360Pose数据集上的表现优于现有方法，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的CMMLoc框架能够有效解决基于文本描述的点云定位问题，并具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云定位任务的目标是在大型城市环境中通过文本描述来确定一个3D位置。该技术在车辆接驾或货物配送等众多领域都有潜在的应用前景。理想情况下，对于每一个文本描述和相应的3D位置来说，周围环境的所有物体都应该被详细描述出来。然而，在实际情况中，如车辆接驾场景下，乘客通常只会提及最重要的及最接近的目标物而不是整个环境。为了解决这种部分相关性的挑战，我们提出了一个基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc来解决从文本到点云定位的问题。通过在模态间交互时将CMM约束作为先验条件并设计空间整合方案来处理不同3D对象的不同感受野，实现了对部分相关环境描述的有效处理。为了实现精确的定位，我们还提出了一种方向积分模块以及一种模态预对准策略，帮助捕捉物体之间的空间关系，并使3D物体更加接近文本模式。通过广泛的实验验证，CMMLoc在KITTI360Pose数据集上优于现有方法，并取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of point cloud localization based on linguistic description is toidentify a 3D position using textual description in large urban environments,which has potential applications in various fields, such as determining thelocation for vehicle pickup or goods delivery. Ideally, for a textualdescription and its corresponding 3D location, the objects around the 3Dlocation should be fully described in the text description. However, inpractical scenarios, e.g., vehicle pickup, passengers usually describe only thepart of the most significant and nearby surroundings instead of the entireenvironment. In response to this $\textbf{partially relevant}$ challenge, wepropose $\textbf{CMMLoc}$, an uncertainty-aware$\textbf{C}$auchy-$\textbf{M}$ixture-$\textbf{M}$odel ($\textbf{CMM}$) basedframework for text-to-point-cloud $\textbf{Loc}$alization. To model theuncertain semantic relations between text and point cloud, we integrate CMMconstraints as a prior during the interaction between the two modalities. Wefurther design a spatial consolidation scheme to enable adaptive aggregation ofdifferent 3D objects with varying receptive fields. To achieve preciselocalization, we propose a cardinal direction integration module alongside amodality pre-alignment strategy, helping capture the spatial relationshipsamong objects and bringing the 3D objects closer to the text modality.Comprehensive experiments validate that CMMLoc outperforms existing methods,achieving state-of-the-art results on the KITTI360Pose dataset. Codes areavailable in this GitHub repository https://github.com/kevin301342/CMMLoc.</description>
      <author>example@mail.com (Yanlong Xu, Haoxuan Qu, Jun Liu, Wenxiao Zhang, Xun Yang)</author>
      <guid isPermaLink="false">2503.02593v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching Metric Distance to Autoregressive Multimodal Foundational Models</title>
      <link>http://arxiv.org/abs/2503.02379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着大型语言模型的应用领域从自然语言扩展到数学、多模态理解和具身代理等领域，输出标记逐渐反映了度量关系而不是纯粹的语言意义。本文介绍了DIST2Loss，这是一种基于预定义的输出标记之间距离关系来训练自回归离散模型的距离感知框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的应用已经从自然语言处理领域扩展到数学、多模态理解和具身代理等领域，在这些新的应用领域中，令牌越来越反映了度量关系而不是纯粹的语言意义。&lt;h4&gt;目的&lt;/h4&gt;提出DIST2Loss方法来训练自回归离散模型，并保持现有架构的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DIST2Loss通过将从固有距离度量衍生出的连续指数族分布转换为与模型架构相容的离散、分类优化目标，使模型能够学习和保留有意义的距离关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，在视觉接地、机器人操作、生成性奖励建模以及使用向量量化特征进行图像生成等多元模态应用中均显示出一致性的性能改进。在训练数据有限的情况下，这些改进尤为明显。&lt;h4&gt;结论&lt;/h4&gt;DIST2Loss能够使模型即使在资源受限的环境中也表现出色，在各种多模态应用场景中具有明显的性能提升效果。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型的应用领域从自然语言处理向数学、多模态理解和具身代理等领域扩展，模型输出标记开始更多地反映度量关系而非纯粹的语言意义。为此，本文提出了一种名为DIST2Loss的框架，该框架利用预定义的距离关系来训练自回归离散模型，并通过将连续指数族分布转化为与现有架构兼容的目标，使模型能够有效地学习并保持有意义的距离关系。实验结果显示，在视觉接地、机器人操作等多模态应用领域中，DIST2Loss显著提升了性能表现。特别是在数据量有限的情况下，其优势尤为突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large language models expand beyond natural language to domains such asmathematics, multimodal understanding, and embodied agents, tokens increasinglyreflect metric relationships rather than purely linguistic meaning. Weintroduce DIST2Loss, a distance-aware framework designed to trainautoregressive discrete models by leveraging predefined distance relationshipsamong output tokens. At its core, DIST2Loss transforms continuous exponentialfamily distributions derived from inherent distance metrics into discrete,categorical optimization targets compatible with the models' architectures.This approach enables the models to learn and preserve meaningful distancerelationships during token generation while maintaining compatibility withexisting architectures. Empirical evaluations show consistent performance gainsin diverse multimodal applications, including visual grounding, roboticmanipulation, generative reward modeling, and image generation usingvector-quantized features. These improvements are pronounced in cases oflimited training data, highlighting DIST2Loss's effectiveness inresource-constrained settings.</description>
      <author>example@mail.com (Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu)</author>
      <guid isPermaLink="false">2503.02379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Random Walks in Self-supervised Learning for Triangular Meshes</title>
      <link>http://arxiv.org/abs/2503.00816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于3D网格分析的自监督学习新方法，通过随机游走进行数据增强以生成网格表面的各种表示，并结合对比和聚类损失来提高训练效果。&lt;h4&gt;背景&lt;/h4&gt;现有基于3D模型的数据增强方法缺乏有效的策略来扩展和丰富给定数据集中的样例表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督学习框架，通过随机游走作为数据增强手段生成多样化的网格表示，并结合对比和聚类损失来提高模型在下游任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;采用基于随机游走的数据增广技术，利用对比损失最大化同一网格的多个实例之间的相似性同时最小化不同网格间的相似性；引入了聚类损失以增强类别区分度并减少训练中的方差。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在使用均值平均精度（mAP）分数和监督SVM线性分类器提取特征进行评估时表现出了良好的性能，显示出其在物体分类和形状检索等下游任务上的潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过引入随机游走、对比损失以及聚类损失的结合，在3D网格分析领域展现出了显著的进步，为后续研究提供了新的思路和可能的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;这项研究针对三维网格自监督学习挑战提出了新方法。利用随机游走作为数据增强技术生成多样化的网格表示，并采用了对比及聚类损失来优化模型训练过程中的效果。通过mAP分数以及监督SVM线性分类器进行评估，结果表明该模型具备在物体分类和形状检索等下游任务上的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenge of self-supervised learning for 3D meshanalysis. It presents an new approach that uses random walks as a form of dataaugmentation to generate diverse representations of mesh surfaces. Furthermore,it employs a combination of contrastive and clustering losses. The contrastivelearning framework maximizes similarity between augmented instances of the samemesh while minimizing similarity between different meshes. We integrate thiswith a clustering loss, enhancing class distinction across training epochs andmitigating training variance. Our model's effectiveness is evaluated using meanAverage Precision (mAP) scores and a supervised SVM linear classifier onextracted features, demonstrating its potential for various downstream taskssuch as object classification and shape retrieval.</description>
      <author>example@mail.com (Gal Yefet, Ayellet Tal)</author>
      <guid isPermaLink="false">2503.00816v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Streamline-based diffusion MRI Tractography Registration Method with Probabilistic Keypoint Detection</title>
      <link>http://arxiv.org/abs/2503.02481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;扩散MRI纤维束配准是分析大脑白质组内相似性和变异性的关键步骤。提出了一种基于深度学习的无监督方法，通过检测和匹配不同受试者之间的点对来实现基于流线的dMRI纤维束配准。&lt;h4&gt;背景&lt;/h4&gt;现有的配准方法主要依赖于优化空间距离以确定最佳变换，但这种方法忽略了流线内部的点连接模式，从而限制了它们识别不同数据集间解剖对应关系的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无监督深度学习方法来进行基于流线的dMRI纤维束配准，通过在受试者之间找到对应的点对来实现解剖结构的一致性匹配。&lt;h4&gt;方法&lt;/h4&gt;将轨迹建模为点云以利用沿着流线的图连接性，并提出了一个新的流线关键点检测方法，将其作为一个概率分类任务，用于识别不同不规则流线集间的解剖一致性对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在纤维束配准性能上表现出高有效性和高效性，优于现有的几种方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于深度学习的无监督dMRI纤维束配准提供了一种新的视角，并展示了其在识别解剖对应关系方面的能力和潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像（dMRI）轨迹图注册是分析大脑白质组内相似性和变异性的关键步骤。流线基注册方法利用纤维路径的三维几何信息以实现配准后的空间对齐。现有方法通常依赖于优化空间距离来确定最优变换，但这种方法忽略了流线内部的点连接模式，限制了其识别不同轨迹数据集间解剖对应关系的能力。本文提出了一种新的基于深度学习的无监督方法来进行流线基dMRI轨迹图注册，该方法通过在受试者之间找到对应的点对来实现轨迹数据集的空间对齐。我们在实验中比较了几种现有方法，并展示了高度有效的和高效的轨迹注册性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Registration of diffusion MRI tractography is an essential step for analyzinggroup similarities and variations in the brain's white matter (WM).Streamline-based registration approaches can leverage the 3D geometricinformation of fiber pathways to enable spatial alignment after registration.Existing methods usually rely on the optimization of the spatial distances toidentify the optimal transformation. However, such methods overlook pointconnectivity patterns within the streamline itself, limiting their ability toidentify anatomical correspondences across tractography datasets. In this work,we propose a novel unsupervised approach using deep learning to performstreamline-based dMRI tractography registration. The overall idea is toidentify corresponding keypoint pairs across subjects for spatial alignment oftractography datasets. We model tractography as point clouds to leverage thegraph connectivity along streamlines. We propose a novel keypoint detectionmethod for streamlines, framed as a probabilistic classification task toidentify anatomically consistent correspondences across unstructured streamlinesets. In the experiments, we compare several existing methods and show highlyeffective and efficient tractography registration performance.</description>
      <author>example@mail.com (Junyi Wang, Mubai Du, Ye Wu, Yijie Li, William M. Wells III, Lauren J. O'Donnell, Fan Zhang)</author>
      <guid isPermaLink="false">2503.02481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating graph neural network into route choice model</title>
      <link>http://arxiv.org/abs/2503.02315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种结合递归逻辑模型和图神经网络（GNN）的新颖混合模型，旨在提高路线选择预测的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统上，基于理论的模型如Logit模型和递归Logit模型因其良好的可解释性而被广泛使用。然而，近年来机器学习方法由于其更好的预测准确性而受到关注。尽管图神经网络（GNN）在捕捉道路网络特征方面表现出色，并且已经在其他交通研究领域得到了广泛应用，但它们尚未用于路线选择建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合模型，将递归逻辑模型与图神经网络结合，以提高预测性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的混合模型，该模型集成了递归Logit模型与图神经网络（GNN），并展示了这种组合如何改进预测性能以及放松无关替代品独立性的假设而不依赖于强假设。这得益于特定类型的GNN能够从数据中有效捕捉到网络上的多个交叉效应模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，应用该方法后，模型的预测准确性相较于现有模型有显著提高。&lt;h4&gt;结论&lt;/h4&gt;结合递归逻辑模型和图神经网络（GNN）的方法可以有效地提升路线选择预测的准确性和可解释性。这种新方法为未来的交通研究提供了新的视角与可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：路线选择模型是运输研究中最重要基础之一，传统理论驱动模型如Logit模型以及递归逻辑模型因其优秀的可解释性而被广泛使用；最近机器学习技术因更高的预测准确度受到关注。在此研究中提出结合递归逻辑模型与图神经网络（GNN）的新颖混合方法以提升预判精度和模型的透明度。据作者所知，尽管图神经网络在捕获道路网络特性方面表现出色并广泛应用于运输领域的其他领域，但它们还未被用于路线选择建模。数学上证明使用图神经网络不仅有利于增强预测性能，而且可以通过不依赖于强烈假设的方式来放松无关替代品独立性的属性。特定类型的GNN能够从数据中高效捕捉到网络上的多个交叉效应模式，从而提升了这一特性。通过将提议模型应用于东京一天的旅行轨迹数据，确认了比现有模型更高的预测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Route choice models are one of the most important foundations fortransportation research. Traditionally, theory-based models have been utilizedfor their great interpretability, such as logit models and Recursive logitmodels. More recently, machine learning approaches have gained attentions fortheir better prediction accuracy. In this study, we propose novel hybrid modelsthat integrate the Recursive logit model with Graph Neural Networks (GNNs) toenhance both predictive performance and model interpretability. To the authors'knowldedge, GNNs have not been utilized for route choice modeling, despitetheir proven effectiveness in capturing road network features and theirwidespread use in other transportation research areas. We mathematically showthat our use of GNN is not only beneficial for enhancing the predictionperformance, but also relaxing the Independence of Irrelevant Alternativesproperty without relying on strong assumptions. This is due to the fact that aspecific type of GNN can efficiently capture multiple cross-effect patterns onnetworks from data. By applying the proposed models to one-day traveltrajectory data in Tokyo, we confirmed their higher prediction accuracycompared to the existing models.</description>
      <author>example@mail.com (Yuxun Ma, Toru Seo)</author>
      <guid isPermaLink="false">2503.02315v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Effective High-order Graph Representation Learning for Credit Card Fraud Detection</title>
      <link>http://arxiv.org/abs/2503.01556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, accepted at IJCAI 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的高阶图表示学习模型（HOGRL），旨在解决现有图神经网络在处理伪装的间接多跳交易时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本，而欺诈者常通过使用数个良性用户的合法交易来规避反欺诈检测系统。现有的图神经网络（GNN）模型由于深层次聚合过程中的过度平滑问题难以学习伪装的间接多跳交易特征。&lt;h4&gt;目的&lt;/h4&gt;提出HOGRL以避免在多层次聚合过程中引入过多噪音，直接从高阶交易图中学习不同次序的纯表示来识别欺诈者的多步间接交易，并优化欺诈检测性能。&lt;h4&gt;方法&lt;/h4&gt;通过有效构建高阶交易图并学习每种顺序的纯表示，使得模型能够通过多层纯特征学习发现伪装关系。同时引入混合专家注意力机制以自动确定各阶的重要性，共同优化欺诈检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示HOGRL在开源和真实世界数据集上相比现有最佳基准方法有显著改进，证实了其在应对高阶欺诈伪装犯罪方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;HOGRL能够更有效地识别复杂的多跳交易模式，增强对伪装的欺诈行为检测能力。&lt;h4&gt;翻译&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本。欺诈者经常通过使用多个良性用户合法进行的交易来规避反欺诈系统。现有的图神经网络（GNN）模型由于在深层次聚合过程中的过度平滑问题，难以学习伪装交易中复杂的间接多跳特征，这是识别伪装关系的主要挑战。因此，在本文中我们提出了一种新的高阶图表示学习模型（HOGRL），以避免在多层次聚合过程中引入过多噪音，并直接从高阶交易图中获得不同的纯表示。为实现这一目标，首先构建有效的高阶交易图，然后学习每个顺序的纯表示，使模型能够通过多层纯特征学习识别欺诈者的间接多跳交易。此外我们还提出了一种混合专家注意力机制来自动确定不同顺序的重要性，并优化整体反欺诈性能。在开源和实际数据集上的广泛实验表明，HOGRL相比于现有的最佳基准有显著提高，证明了其处理复杂伪装行为的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.24963/ijcai.2024/839&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Credit card fraud imposes significant costs on both cardholders and issuingbanks. Fraudsters often disguise their crimes, such as using legitimatetransactions through several benign users to bypass anti-fraud detection.Existing graph neural network (GNN) models struggle with learning features ofcamouflaged, indirect multi-hop transactions due to their inherentover-smoothing issues in deep multi-layer aggregation, presenting a majorchallenge in detecting disguised relationships. Therefore, in this paper, wepropose a novel High-order Graph Representation Learning model (HOGRL) to avoidincorporating excessive noise during the multi-layer aggregation process. Inparticular, HOGRL learns different orders of \emph{pure} representationsdirectly from high-order transaction graphs. We realize this goal byeffectively constructing high-order transaction graphs first and then learningthe \emph{pure} representations of each order so that the model could identifyfraudsters' multi-hop indirect transactions via multi-layer \emph{pure} featurelearning. In addition, we introduce a mixture-of-expert attention mechanism toautomatically determine the importance of different orders for jointlyoptimizing fraud detection performance. We conduct extensive experiments inboth the open source and real-world datasets, the result demonstrates thesignificant improvements of our proposed HOGRL compared with state-of-the-artfraud detection baselines. HOGRL's superior performance also proves itseffectiveness in addressing high-order fraud camouflage criminals.</description>
      <author>example@mail.com (Yao Zou, Dawei Cheng)</author>
      <guid isPermaLink="false">2503.01556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.00793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025, Github link:  https://github.com/UkcheolShin/BridgeMultiSpectralDepth&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;部署深度估计网络到现实世界中需要具备对抗各种不利条件的高度鲁棒性，以确保安全和可靠的自主驾驶。&lt;h4&gt;目的&lt;/h4&gt;为了提高多模态传感器系统在自动驾驶车辆中的使用效率，提出了一种基于对齐与融合策略的解决方案，用于从多光谱图像进行深度估计。&lt;h4&gt;方法&lt;/h4&gt;{'align阶段': '通过最小化全局特征及几何线索的空间一致性局部特征对比损失来调整多个光谱带之间的嵌入空间，以学习跨多光谱图像可共享的表示。', 'fuse阶段': '训练一个可附加的功能融合模块，该模块可以有选择地聚合多光谱特性，用于可靠和鲁棒的预测结果。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略使得单深度网络可以在保持可靠性、内存效率和灵活性的同时，实现光谱不变性和多光谱融合的深度估计。&lt;h4&gt;结论&lt;/h4&gt;基于align-and-fuse策略的方法为跨多个光谱带进行深度估计提供了一种有效的解决方案，并且能够提高预测结果的可靠性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying depth estimation networks in the real world requires high-levelrobustness against various adverse conditions to ensure safe and reliableautonomy. For this purpose, many autonomous vehicles employ multi-modal sensorsystems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar.They mainly adopt two strategies to use multiple sensors: modality-wise andmulti-modal fused inference. The former method is flexible butmemory-inefficient, unreliable, and vulnerable. Multi-modal fusion can providehigh-level reliability, yet it needs a specialized architecture. In this paper,we propose an effective solution, named align-and-fuse strategy, for the depthestimation from multi-spectral images. In the align stage, we align embeddingspaces between multiple spectrum bands to learn shareable representation acrossmulti-spectral images by minimizing contrastive loss of global and spatiallyaligned local features with geometry cue. After that, in the fuse stage, wetrain an attachable feature fusion module that can selectively aggregate themulti-spectral features for reliable and robust prediction results. Based onthe proposed method, a single-depth network can achieve both spectral-invariantand multi-spectral fused depth estimation while preserving reliability, memoryefficiency, and flexibility.</description>
      <author>example@mail.com (Ukcheol Shin, Kyunghyun Lee, Jean Oh)</author>
      <guid isPermaLink="false">2503.00793v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CrystalFramer: Rethinking the Role of Frames for SE(3)-Invariant Crystal Structure Modeling</title>
      <link>http://arxiv.org/abs/2503.02209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 main pages, 3 main figures, and 4 main tables. Published as a  conference paper at ICLR 2025. This version moves some appendices into the  main text. For more information, see  https://omron-sinicx.github.io/crystalframer/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;晶格结构建模在材料信息学的各种应用中至关重要，并且捕捉SE(3)不变几何特征是这些网络的基本需求。&lt;h4&gt;问题&lt;/h4&gt;确定晶体结构的框架具有挑战性，因为它们是无限和高度对称的，这与分子不同。&lt;h4&gt;传统方法限制&lt;/h4&gt;现有的方法依赖于每个结构由其自身的结构性信息静态固定的一个框架来实现。&lt;h4&gt;新概念&lt;/h4&gt;提出了动态框架的概念，这些框架在考虑到晶体无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。&lt;h4&gt;应用技术&lt;/h4&gt;通过利用最近基于变压器的晶格编码器中的注意力机制，提出了一种称为CrystalFramer的新架构来实现这一概念。&lt;h4&gt;实验结果&lt;/h4&gt;广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;h4&gt;贡献&lt;/h4&gt;该研究质疑了结构对齐坐标系统的简单性，并引入了一个新的、动态的视角来理解和建模复杂的晶体几何特征。&lt;h4&gt;翻译&lt;/h4&gt;使用图神经网络进行晶格结构建模对于材料信息学中的许多应用至关重要，捕捉SE(3)不变的几何特性是这些网络的基本需求。一种直接的方法是通过结构对齐坐标系统或“框架”建模定向标准化的结构。然而，与分子不同，确定晶体结构的框架具有挑战性，因为它们具有无限和高度对称的性质。特别是，现有方法依赖于由每个结构自身的结构性信息静态固定的一个框架，而不考虑任务本身的需求。这里，我们重新思考了“框架”的角色，质疑这种仅基于结构简单化的对齐是否足够，并提出了动态框架的概念。这些框架在考虑到晶体的无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。通过利用最近基于变压器的晶格编码器中的注意力机制，我们实现了这一概念，提出了一种新的架构叫作CrystalFramer。广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal structure modeling with graph neural networks is essential forvarious applications in materials informatics, and capturing SE(3)-invariantgeometric features is a fundamental requirement for these networks. Astraightforward approach is to model with orientation-standardized structuresthrough structure-aligned coordinate systems, or"frames." However, unlikemolecules, determining frames for crystal structures is challenging due totheir infinite and highly symmetric nature. In particular, existing methodsrely on a statically fixed frame for each structure, determined solely by itsstructural information, regardless of the task under consideration. Here, werethink the role of frames, questioning whether such simplistic alignment withthe structure is sufficient, and propose the concept of dynamic frames. Whileaccommodating the infinite and symmetric nature of crystals, these framesprovide each atom with a dynamic view of its local environment, focusing onactively interacting atoms. We demonstrate this concept by utilizing theattention mechanism in a recent transformer-based crystal encoder, resulting ina new architecture called CrystalFramer. Extensive experiments show thatCrystalFramer outperforms conventional frames and existing crystal encoders invarious crystal property prediction tasks.</description>
      <author>example@mail.com (Yusei Ito, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, Kanta Ono)</author>
      <guid isPermaLink="false">2503.02209v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Projection Head is Secretly an Information Bottleneck</title>
      <link>http://arxiv.org/abs/2503.00507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文深入探讨了对比学习中投影头的作用机制，并从信息理论的角度提供了理论上的理解，提出了一种新的方法来改进投影器的设计。&lt;h4&gt;背景&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。&lt;h4&gt;目的&lt;/h4&gt;研究和理解投影头背后的机制，并提出改进方法以提高对比学习的性能。&lt;h4&gt;方法&lt;/h4&gt;从信息理论的角度出发，建立了对投影之前特征的下游表现的理论保证。基于这些理论见解，引入了带有训练和结构正则化的投影器修改方案。&lt;h4&gt;主要发现&lt;/h4&gt;有效的投影头应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。&lt;h4&gt;结论&lt;/h4&gt;在CIFAR-10、CIFAR-100和ImageNet-100等多个实际数据集上进行实验验证后，所提出的方法表现出一致的性能提升。研究者认为其对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。&lt;h4&gt;翻译&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。在各种特殊设计中，训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。然而尽管这种方法在经验上取得成功，其背后的机制却尚未得到充分研究。本文从信息理论的角度深入探讨了投影头的作用，并建立了对特征在投影之前进行下游表现的理论保证，揭示了一个有效的投影器应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。基于这些理论见解，我们引入了带有训练和结构正则化的投影器修改方案。经验上，在包括CIFAR-10、CIFAR-100和ImageNet-100在内的多个真实数据集上的下游性能表现出一致的提升。我们认为对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。代码可在https://github.com/PKU-ML/Projector_Theory获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, contrastive learning has risen to be a promising paradigm forextracting meaningful data representations. Among various special designs,adding a projection head on top of the encoder during training and removing itfor downstream tasks has proven to significantly enhance the performance ofcontrastive learning. However, despite its empirical success, the underlyingmechanism of the projection head remains under-explored. In this paper, wedevelop an in-depth theoretical understanding of the projection head from theinformation-theoretic perspective. By establishing the theoretical guaranteeson the downstream performance of the features before the projector, we revealthat an effective projector should act as an information bottleneck, filteringout the information irrelevant to the contrastive objective. Based ontheoretical insights, we introduce modifications to projectors with trainingand structural regularizations. Empirically, our methods exhibit consistentimprovement in the downstream performance across various real-world datasets,including CIFAR-10, CIFAR-100, and ImageNet-100. We believe our theoreticalunderstanding on the role of the projection head will inspire more principledand advanced designs in this field. Code is available athttps://github.com/PKU-ML/Projector_Theory.</description>
      <author>example@mail.com (Zhuo Ouyang, Kaiwen Hu, Qi Zhang, Yifei Wang, Yisen Wang)</author>
      <guid isPermaLink="false">2503.00507v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>African Gender Classification Using Clothing Identification Via Deep Learning</title>
      <link>http://arxiv.org/abs/2503.00058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 Pages, 10 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了在非理想条件下进行性别分类的新方法，通过分析非洲传统服饰来进行性别识别。&lt;h4&gt;背景&lt;/h4&gt;传统的性别分类技术主要依赖于面部识别，在诸如模糊图像、侧面视角或部分遮挡的情况下效果不佳。&lt;h4&gt;目的&lt;/h4&gt;探索基于服装的性别分类作为一种补充技术的可能性，并特别关注非洲传统服饰的文化特性和性别特定特征。&lt;h4&gt;方法&lt;/h4&gt;使用AFRIFASHION1600数据集，该数据集中有1,600张标记为男性和女性两个类别的非洲传统服饰图像。通过修改后的VGG16架构并采用迁移学习技术开发了一个深度学习模型。为了克服小样本带来的问题以及避免过拟合，使用了数据增强。&lt;h4&gt;主要发现&lt;/h4&gt;所开发的模型在测试集上的准确率为87%，尽管存在女性样本占多数的数据不平衡情况，依然表现出强大的预测能力。&lt;h4&gt;结论&lt;/h4&gt;论文强调了基于服装的性别识别作为面部识别技术补充的可能性，并建议未来的研究应集中在扩展和平衡数据集上，以提高分类稳健性和实际应用性。&lt;h4&gt;翻译&lt;/h4&gt;人类属性识别和分类在计算机视觉中至关重要，推动着创新识别系统的开发。传统性别分类方法主要依赖于面部识别，在诸如图像模糊、侧面视角或部分遮挡的情况下效果不佳。本研究探索了一种替代方法，通过利用服装识别进行性别分类，特别是非洲传统服饰，因为这些服饰具有文化特定和性别的独特特征。我们使用了AFRIFASHION1600数据集，这是一个包含1,600张图片的数据集合，其中的非洲传统服装被标记为男性和女性两类。基于修改后的VGG16架构并通过迁移学习进行训练的一个深度学习模型已被开发出来用于分类。为了应对相对较小的数据集带来的挑战，并减少过拟合的风险，我们采用了数据增强技术。尽管存在样本不平衡的问题（特别是更多的女性样本），我们的模型在测试集中达到了87%的准确性，显示出强大的预测能力。这些研究结果突显了基于服装识别作为面部识别补充的可能性，在非洲文化背景下用于性别分类。未来的研究所应关注的方向是扩大和平衡数据集，以增强分类的稳健性和提高服装基础性别的识别系统的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human attribute identification and classification are crucial in computervision, driving the development of innovative recognition systems. Traditionalgender classification methods primarily rely on facial recognition, which,while effective, struggles under non-ideal conditions such as blurriness, sideviews, or partial occlusions. This study explores an alternative approach byleveraging clothing identification, specifically focusing on Africantraditional attire, which carries culturally significant and gender-specificfeatures.  We use the AFRIFASHION1600 dataset, a curated collection of 1,600 images ofAfrican traditional clothing labeled into two gender classes: male and female.A deep learning model, based on a modified VGG16 architecture and trained usingtransfer learning, was developed for classification. Data augmentation wasapplied to address the challenges posed by the relatively small dataset and tomitigate overfitting. The model achieved an accuracy of 87% on the test set,demonstrating strong predictive capability despite dataset imbalances favoringfemale samples.  These findings highlight the potential of clothing-based identification as acomplementary technique to facial recognition for gender classification inAfrican contexts. Future research should focus on expanding and balancingdatasets to enhance classification robustness and improve the applicability ofclothing-based gender recognition systems.</description>
      <author>example@mail.com (Samuel Ozechi)</author>
      <guid isPermaLink="false">2503.00058v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Introspective Loop Closure for SLAM with 4D Imaging Radar</title>
      <link>http://arxiv.org/abs/2503.02383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication in the IEEE  International Conference on Robotics and Automation(ICRA), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了在SLAM中使用4D雷达进行循环闭合检测的方法，特别是针对相似和相反视角的情况。&lt;h4&gt;背景&lt;/h4&gt;移动机器人可以在没有外部定位系统或预先存在的地图的情况下通过同时定位与建图（SLAM）技术导航。雷达作为有价值的感觉工具尤其适用于视线被阻挡的环境，因为它受到颗粒的影响比激光雷达或摄像头小得多。现代4D成像雷达提供三维几何信息和相对速度测量。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用4D雷达数据在SLAM中进行循环闭合检测，以提高地图准确性和减少轨迹漂移。&lt;h4&gt;方法&lt;/h4&gt;生成子图来表示更密集的环境，并采用内省度量方法排除特征退化环境中错误检测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在几何多样的场景下，对于相似和相反视角，循环闭合检测能够保持高精度。轨迹估计可以得到高达82%的改进ATE（绝对轨迹误差），并且能够在自类似环境里拒绝假阳性。&lt;h4&gt;结论&lt;/h4&gt;通过4D雷达在SLAM中的应用，提高了移动机器人在复杂环境下的导航能力，特别是在视线受限的情况中表现出了优越性。&lt;h4&gt;翻译&lt;/h4&gt;Simultaneous Localization and Mapping (SLAM) enables mobile robots to navigate without external positioning systems or pre-existing maps. Radar is emerging as a valuable sensing tool, especially in vision-obstructed environments, as it is less affected by particles than lidars or cameras. Modern 4D imaging radars provide three-dimensional geometric information and relative velocity measurements, but they bring challenges such as small field of view and sparse, noisy point clouds. Detecting loop closures in SLAM is critical for reducing trajectory drift and maintaining map accuracy. However, the directional nature of 4D radar data makes identifying loop closures, especially from reverse viewpoints, difficult due to limited scan overlap. This article explores using 4D radar for loop closure in SLAM, focusing on similar and opposing viewpoints. We generate submaps for a denser environment representation and use introspective measures to reject false detections in feature-degenerate environments. Our experiments show accurate loop closure detection in geometrically diverse settings for both similar and opposing viewpoints, improving trajectory estimation with up to 82% improvement in ATE and rejecting false positives in self-similar environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigatewithout external positioning systems or pre-existing maps. Radar is emerging asa valuable sensing tool, especially in vision-obstructed environments, as it isless affected by particles than lidars or cameras. Modern 4D imaging radarsprovide three-dimensional geometric information and relative velocitymeasurements, but they bring challenges, such as a small field of view andsparse, noisy point clouds. Detecting loop closures in SLAM is critical forreducing trajectory drift and maintaining map accuracy. However, thedirectional nature of 4D radar data makes identifying loop closures, especiallyfrom reverse viewpoints, difficult due to limited scan overlap. This articleexplores using 4D radar for loop closure in SLAM, focusing on similar andopposing viewpoints. We generate submaps for a denser environmentrepresentation and use introspective measures to reject false detections infeature-degenerate environments. Our experiments show accurate loop closuredetection in geometrically diverse settings for both similar and opposingviewpoints, improving trajectory estimation with up to 82 % improvement in ATEand rejecting false positives in self-similar environments.</description>
      <author>example@mail.com (Maximilian Hilger, Vladimír Kubelka, Daniel Adolfsson, Ralf Becker, Henrik Andreasson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2503.02383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fairness and/or Privacy on Social Graphs</title>
      <link>http://arxiv.org/abs/2503.02114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了图神经网络（GNNs）在公平性和隐私方面的挑战，并通过实验分析了不同保护措施对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;研究表明，GNNs在处理图形数据方面取得了显著成功，但也引发了关于其公平性与隐私问题的关注。这些问题包括可能的偏见和歧视风险以及敏感信息的安全隐患。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在全面调查GNN中的公平性和隐私问题，并探索不同保护措施对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;通过在多种数据集上进行实验，评估不同的公平性干预措施的有效性。分析了公平性、隐私和准确性之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在特定的数据特性和期望的公平目标的基础上精心选择和组合公平性保护措施的重要性。&lt;h4&gt;结论&lt;/h4&gt;该研究对GNN中复杂互相关系的深入理解做出了贡献，并为开发更强大且道德规范的图学习模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的文本&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable success in variousgraph-based learning tasks. However, recent studies have raised concerns aboutfairness and privacy issues in GNNs, highlighting the potential for biased ordiscriminatory outcomes and the vulnerability of sensitive information. Thispaper presents a comprehensive investigation of fairness and privacy in GNNs,exploring the impact of various fairness-preserving measures on modelperformance. We conduct experiments across diverse datasets and evaluate theeffectiveness of different fairness interventions. Our analysis considers thetrade-offs between fairness, privacy, and accuracy, providing insights into thechallenges and opportunities in achieving both fair and private graph learning.The results highlight the importance of carefully selecting and combiningfairness-preserving measures based on the specific characteristics of the dataand the desired fairness objectives. This study contributes to a deeperunderstanding of the complex interplay between fairness, privacy, and accuracyin GNNs, paving the way for the development of more robust and ethical graphlearning models.</description>
      <author>example@mail.com (Bartlomiej Surma, Michael Backes, Yang Zhang)</author>
      <guid isPermaLink="false">2503.02114v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Adaptive Graph Neural Networks via Learnable Bakry-'Emery Curvature</title>
      <link>http://arxiv.org/abs/2503.01079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的图神经网络(GNN)框架，该框架通过结合Bakry-Emery曲率来增强信息传播能力，不仅考虑了结构特性还考虑了任务相关性。此方法能够动态调整消息传递层的深度以适应不同节点，并且在大规模图上计算效率高。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN方法主要关注离散图拓扑而忽略了扩散动力学和特定任务依赖性的学习。引入几何属性如曲率可以改进信息流动和复杂连接模式的学习能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的可学习的Bakry-Emery曲率近似策略，以解决现有GNN方法忽视扩散动态和任务特定性的问题，并提高大规模图上的计算效率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于顶点曲率自适应调整消息传递层深度的新机制。此外，通过理论分析建立了一个连接曲线度量与特征区分性的桥梁。&lt;h4&gt;主要发现&lt;/h4&gt;高曲率节点需要较少的传播层来达到有效学习，而低曲率节点从更深的传播中受益。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该方法在各种图学习任务上均表现出优越性。理论和实证分析证明了所提出的框架的有效性和效率。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已经展示了处理基于图形的任务的强大表征学习能力。最近关于GNN的进步通过利用几何属性，例如曲率，来增强其表示能力，从而建模复杂的连接模式和信息流动。然而，大多数现有的方法仅关注离散的图拓扑结构，忽略了扩散动力学以及对有效学习至关重要的任务特定依赖性。为了解决这个问题，我们提出了一种结合Bakry-Emery曲率的方法，该方法捕捉了信息传播中的结构化和驱动因素。我们开发了一种有效的、可学习的近似策略，使大规模图上的曲率计算变得可扩展。此外，我们还引入了一个自适应深度机制，根据每个顶点的曲度动态调整消息传递层的数量，确保高效的信息传递过程。我们的理论分析建立了曲率与特征区分性之间的联系，表明高曲率节点需要较少的层数进行传播，而低曲率节点可以从更深的传播中受益。在基准数据集上的广泛实验验证了我们方法的有效性，并显示出在各种图学习任务中的性能改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated strong representation learningcapabilities for graph-based tasks. Recent advances on GNNs leverage geometricproperties, such as curvature, to enhance its representation capabilities bymodeling complex connectivity patterns and information flow within graphs.However, most existing approaches focus solely on discrete graph topology,overlooking diffusion dynamics and task-specific dependencies essential foreffective learning. To address this, we propose integrating Bakry-\'Emerycurvature, which captures both structural and task-driven aspects ofinformation propagation. We develop an efficient, learnable approximationstrategy, making curvature computation scalable for large graphs. Furthermore,we introduce an adaptive depth mechanism that dynamically adjustsmessage-passing layers per vertex based on its curvature, ensuring efficientpropagation. Our theoretical analysis establishes a link between curvature andfeature distinctiveness, showing that high-curvature vertices require fewerlayers, while low-curvature ones benefit from deeper propagation. Extensiveexperiments on benchmark datasets validate the effectiveness of our approach,showing consistent performance improvements across diverse graph learningtasks.</description>
      <author>example@mail.com (Asela Hevapathige, Ahad N. Zehmakan, Qing Wang)</author>
      <guid isPermaLink="false">2503.01079v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</title>
      <link>http://arxiv.org/abs/2503.00389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为BGM2Pose的方法，用于从任意音乐中提取3D人体姿态，该方法克服了现有技术中需要侵入性信号的限制。&lt;h4&gt;背景&lt;/h4&gt;现有的姿势估计方法往往依赖于特定频率范围内的声波信号，这些信号可能对人类造成不适。而日常生活中的普通音乐则提供了一种自然、无害的声音源。&lt;h4&gt;目的&lt;/h4&gt;开发一种从标准音乐中准确提取3D人体姿态的方法，以克服现有技术的局限性，并提高实际应用的可能性。&lt;h4&gt;方法&lt;/h4&gt;BGM2Pose引入了对比姿势提取模块和频率注意力模块。前者利用对比学习与硬负样本采样来去除记录数据中的音乐成分；后者则通过动态计算频带间的注意权重，使模型能够关注到由人体动作引起的微妙声学变化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，BGM2Pose方法在性能上优于现有的技术，并展示了其潜在的实际应用价值。&lt;h4&gt;结论&lt;/h4&gt;研究证明了从背景音乐中提取3D姿态信息的可行性与优越性，并计划公开发布数据集和代码以供进一步的研究利用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为BGM2Pose的方法，用于通过任意音乐作为主动传感信号来进行非侵入性的3D人体姿态估计。该方法克服了现有技术中需要侵入性声音信号的限制，这些声音信号可能对人类造成不适。从标准音乐中提取姿势信息带来了许多挑战：与专为测量设计的声音源不同，普通音乐在音量和音调上会动态变化，导致其声学特性难以与人体运动产生的改变区分开来。BGM2Pose通过引入对比姿势提取模块和频率注意力模块解决了这些难题，并展示了优于现有方法的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose BGM2Pose, a non-invasive 3D human pose estimation method usingarbitrary music (e.g., background music) as active sensing signals. Unlikeexisting approaches that significantly limit practicality by employingintrusive chirp signals within the audible range, our method utilizes naturalmusic that causes minimal discomfort to humans. Estimating human poses fromstandard music presents significant challenges. In contrast to sound sourcesspecifically designed for measurement, regular music varies in both volume andpitch. These dynamic changes in signals caused by music are inevitably mixedwith alterations in the sound field resulting from human motion, making it hardto extract reliable cues for pose estimation. To address these challenges,BGM2Pose introduces a Contrastive Pose Extraction Module that employscontrastive learning and hard negative sampling to eliminate musical componentsfrom the recorded data, isolating the pose information. Additionally, wepropose a Frequency-wise Attention Module that enables the model to focus onsubtle acoustic variations attributable to human movement by dynamicallycomputing attention across frequency bands. Experiments suggest that our methodoutperforms the existing methods, demonstrating substantial potential forreal-world applications. Our datasets and code will be made publicly available.</description>
      <author>example@mail.com (Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa)</author>
      <guid isPermaLink="false">2503.00389v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Prior Distillation with Vision Foundation Model for Enhanced Rapid Bone Scintigraphy Image Restoration</title>
      <link>http://arxiv.org/abs/2503.02321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于SAM模型的语义先验用于儿科快速骨闪烁显像图像恢复的新方法。&lt;h4&gt;背景&lt;/h4&gt;快速骨闪烁显像是诊断儿童骨骼疾病和肿瘤转移的重要工具，因为它减少了扫描时间并减轻了患者的不适。然而，快速扫描经常导致图像质量下降，影响诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;为了提高快速骨闪烁显像的图像质量，我们提出了一个利用SAM模型语义先验来增强儿科人群中的快速骨闪烁显像的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法包括两个级联网络$f^{IR1}$和$f^{IR2}$，并有三个关键模块：语义先验集成(SPI)模块、语义知识蒸馏(SKD)模块以及语义一致性(SCM)模块。这些模块利用了从经过微调的SAM中提取的专业领域特定的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在公共内窥镜数据集和新发布的RBS数据集中，我们的方法在各种评价指标（如PSNR、SSIM、FID和LPIPS）上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合SAM模型的语义先验信息，该方法能够显著提高快速骨闪烁显像图像的质量，有助于更准确地诊断儿童骨骼疾病。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用基于Segment Anything Model (SAM) 的方法来改进儿科患者快速骨闪烁显像质量的研究。通过引入专门设计的模块和使用特定数据集进行验证，该研究展示了在提升医学影像重建技术方面的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid bone scintigraphy is an essential tool for diagnosing skeletal diseasesand tumor metastasis in pediatric patients, as it reduces scan time andminimizes patient discomfort. However, rapid scans often result in poor imagequality, potentially affecting diagnosis due to reduced resolution and detail,which make it challenging to identify and evaluate finer anatomical structures.To address this issue, we propose the first application of SAM-based semanticpriors for medical image restoration, leveraging the Segment Anything Model(SAM) to enhance rapid bone scintigraphy images in pediatric populations. Ourmethod comprises two cascaded networks, $f^{IR1}$ and $f^{IR2}$, augmented bythree key modules: a Semantic Prior Integration (SPI) module, a SemanticKnowledge Distillation (SKD) module, and a Semantic Consistency Module (SCM).The SPI and SKD modules incorporate domain-specific semantic information from afine-tuned SAM, while the SCM maintains consistent semantic featurerepresentation throughout the cascaded networks. In addition, we will release anovel Rapid Bone Scintigraphy dataset called RBS, the first dataset dedicatedto rapid bone scintigraphy image restoration in pediatric patients. RBSconsists of 137 pediatric patients aged between 0.5 and 16 years who underwentboth standard and rapid bone scans. The dataset includes scans performed at 20cm/min (standard) and 40 cm/min (rapid), representing a $2\times$ acceleration.We conducted extensive experiments on both the publicly available endoscopicdataset and RBS. The results demonstrate that our method outperforms allexisting methods across various metrics, including PSNR, SSIM, FID, and LPIPS.</description>
      <author>example@mail.com (Pengchen Liang, Leijun Shi, Huiping Yao, Bin Pu, Jianguo Chen, Lei Zhao, Haishan Huang, Zhuangzhuang Chen, Zhaozhao Xu, Lite Xu, Qing Chang, Yiwei Li)</author>
      <guid isPermaLink="false">2503.02321v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction</title>
      <link>http://arxiv.org/abs/2503.02375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种针对毫米波雷达点云增强和人体重建的两阶段深度学习框架。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达因其隐私友好性和非侵入性而成为人体重建的理想选择，但其稀疏的点云限制了估计准确性。&lt;h4&gt;目的&lt;/h4&gt;通过引入深度学习方法来提高毫米波雷达点云的质量并提升人体重建精度。&lt;h4&gt;方法&lt;/h4&gt;{'mmWave点云增强模块': '利用时间特征密集化原始数据，并从单视图图像中的2D人类掩模中学习详细的形状和姿态信息。在训练阶段使用基于图像的监督，但在推理时仅依赖于稀疏的点云以保护隐私。', '多阶段完成网络': '进一步优化重建结果。', '2D-3D融合模块': '提取二维和三维运动特征来精炼SMPL参数'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法在多个数据集上优于现有技术，并且增强后的点云能进一步提升集成到现有模型中的性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用深度学习框架有效解决了毫米波雷达点云稀疏的问题，为人体重建提供了一种新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverseenvironments, making it a highly promising solution for human bodyreconstruction due to its privacy-friendly and non-intrusive nature. However,the significant sparsity of mmWave point clouds limits the estimation accuracy.To overcome this challenge, we propose a two-stage deep learning framework thatenhances mmWave point clouds and improves human body reconstruction accuracy.Our method includes a mmWave point cloud enhancement module that densifies theraw data by leveraging temporal features and a multi-stage completion network,followed by a 2D-3D fusion module that extracts both 2D and 3D motion featuresto refine SMPL parameters. The mmWave point cloud enhancement module learns thedetailed shape and posture information from 2D human masks in single-viewimages. However, image-based supervision is involved only during the trainingphase, and the inference relies solely on sparse point clouds to maintainprivacy. Experiments on multiple datasets demonstrate that our approachoutperforms state-of-the-art methods, with the enhanced point clouds furtherimproving performance when integrated into existing models.</description>
      <author>example@mail.com (Jiarui Yang, Songpengcheng Xia, Zengyuan Lai, Lan Sun, Qi Wu, Wenxian Yu, Ling Pei)</author>
      <guid isPermaLink="false">2503.02375v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Point Contributions: A Lightweight Framework for Efficient and Effective Query-Driven Trajectory Simplification</title>
      <link>http://arxiv.org/abs/2503.02047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by VLDB2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着轨迹数据的积累，简化轨迹以减少存储和查询成本的研究越来越受到关注。论文提出了一种基于图神经网络（GNN-TS）和扩散模型（Diff-TS）的新型互学习查询驱动轨迹简化框架MLSim。&lt;h4&gt;背景&lt;/h4&gt;现有的简化方法主要面临三方面的问题：需要大量迭代才能决定删除哪些GPS点；只关注相邻点之间的关系，忽视了整体结构的信息，导致简化后的轨迹与原始轨迹的整体相似性降低；不能区分具有相似特征的点的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来解决现有简化方法中存在的问题，并提高查询精度和减少简化时间。&lt;h4&gt;方法&lt;/h4&gt;MLSim框架包含了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。其中，GNN-TS通过关注点的全局性和独特性评估其重要性；而Diff-TS则生成放大的信号以在低压缩率下保留最重要的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与八个基准方法相比，在三个数据库上使用MLSim可以将简化时间减少42%--70%，并提高查询准确性最多可达34.6%&lt;h4&gt;结论&lt;/h4&gt;所提出的MLSim框架通过结合图神经网络和扩散模型，能够有效地解决现有轨迹简化方法的问题，并提供更精确的查询结果。&lt;h4&gt;翻译&lt;/h4&gt;随着大量轨迹数据积累起来，为了减少存储成本以及查询成本，对于简化轨迹的研究变得越来越重要。现有的提议面临三个主要问题：首先，它们需要多次迭代来决定删除哪些GPS点；其次，它们只关注相邻点之间的关系（局部信息），而忽视了整体结构的信息（全局信息），这降低了简化的轨迹与原始轨迹的整体相似性，并使在查询结果中保持一致性变得困难，尤其是在基于相似性的查询中。最后，它们未能区分具有类似特征的点的重要性，从而导致选择保留原始轨迹信息的点时效果不佳。我们提出了一种新颖的互学习查询驱动轨迹简化框架MLSim，该框架结合了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。GNN-TS根据全局性和独特性评估一个点的重要性，前者捕捉其与整个轨迹的相关性，后者则捕捉其与其他相邻点之间的差异。同时，在GNN层中还包含了注意力机制，使得能够从同一轨迹内的所有点进行数据融合，并优化表示，从而避免了迭代过程。Diff-TS生成放大信号以在低压缩率下保留最重要的点。涉及八种基线方法的实验显示，MLSim可以将简化时间减少42%--70%，并在简化后的轨迹查询准确性上提高最多34.6%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large volumes of trajectory data accumulate, simplifying trajectories toreduce storage and querying costs is increasingly studied. Existing proposalsface three main problems. First, they require numerous iterations to decidewhich GPS points to delete. Second, they focus only on the relationshipsbetween neighboring points (local information) while neglecting the overallstructure (global information), reducing the global similarity between thesimplified and original trajectories and making it difficult to maintainconsistency in query results, especially for similarity-based queries. Finally,they fail to differentiate the importance of points with similar features,leading to suboptimal selection of points to retain the original trajectoryinformation.  We propose MLSimp, a novel Mutual Learning query-driven trajectorysimplification framework that integrates two distinct models: GNN-TS, based ongraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluatesthe importance of a point according to its globality, capturing its correlationwith the entire trajectory, and its uniqueness, capturing its differences fromneighboring points. It also incorporates attention mechanisms in the GNNlayers, enabling simultaneous data integration from all points within the sametrajectory and refining representations, thus avoiding iterative processes.Diff-TS generates amplified signals to enable the retention of the mostimportant points at low compression rates. Experiments involving eightbaselines on three databases show that MLSimp reduces the simplification timeby 42%--70% and improves query accuracy over simplified trajectories by up to34.6%.</description>
      <author>example@mail.com (Yumeng Song, Yu Gu, Tianyi Li, Yushuai Li, Christian S. Jensen, Ge Yu)</author>
      <guid isPermaLink="false">2503.02047v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as ICLR 2025 conference paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的管道HOD和模型EgoVideo，以改进基于自我视角视频的理解。通过引入一种新颖的方法来生成包含手部与物体动态细节的叙述文本，并使用一个轻量级的运动适配器来捕捉这些细粒度的手部-物体动态信息。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要集中在将视频表示与高层次叙事对齐，而忽略了对手部和物体之间复杂动态的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在整合细粒度手部-物体动力学模型到视频表示学习过程中。&lt;h4&gt;方法&lt;/h4&gt;引入HOD管道利用手部-物体检测器和大型语言模型生成详细的叙述文本；提出EgoVideo模型，其中包含一个新的轻量级运动适配器来捕捉细粒度的手部-物体移动信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过协同训练策略，EgoVideo能够有效地利用HOD数据中的细粒度手部-物体动态。实验显示该方法在多个自我视角下游任务中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的模型不仅在零样本设置下取得了显著的改进，还展示了对手部-物体交互和机器人操作任务的强大泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在自我视角视频理解领域，手部与物体的动作以及它们之间的互动起着关键作用。然而现有的研究主要集中在将视频表示与高层次叙事对齐，忽略了对手部和物体之间复杂动态的研究。为了解决这个问题，引入了HOD管道，并提出了EgoVideo模型，该模型能够更好地捕捉这些细粒度的手部-物体动态信息。实验结果显示这种方法在多个自我视角下游任务中取得了显著的改进，展示了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/openrobotlab/egohod&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In egocentric video understanding, the motion of hands and objects as well astheir interactions play a significant role by nature. However, existingegocentric video representation learning methods mainly focus on aligning videorepresentation with high-level narrations, overlooking the intricate dynamicsbetween hands and objects. In this work, we aim to integrate the modeling offine-grained hand-object dynamics into the video representation learningprocess. Since no suitable data is available, we introduce HOD, a novelpipeline employing a hand-object detector and a large language model togenerate high-quality narrations with detailed descriptions of hand-objectdynamics. To learn these fine-grained dynamics, we propose EgoVideo, a modelwith a new lightweight motion adapter to capture fine-grained hand-objectmotion information. Through our co-training strategy, EgoVideo effectively andefficiently leverages the fine-grained hand-object dynamics in the HOD data.Extensive experiments demonstrate that our method achieves state-of-the-artperformance across multiple egocentric downstream tasks, including improvementsof 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and16.3% in EGTEA classification in zero-shot settings. Furthermore, our modelexhibits robust generalization capabilities in hand-object interaction androbot manipulation tasks. Code and data are available athttps://github.com/OpenRobotLab/EgoHOD/.</description>
      <author>example@mail.com (Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang)</author>
      <guid isPermaLink="false">2503.00986v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Convergence of energy-based learning in linear resistive networks</title>
      <link>http://arxiv.org/abs/2503.00349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了一种基于能量的学习算法——对比学习（Contrastive Learning），并证明在特定网络中，该算法与投影梯度下降等价，从而保证了算法的收敛性。&lt;h4&gt;背景&lt;/h4&gt;基于能量的学习算法作为反向传播的替代方案，在类比电子设备中的分布式实现中表现良好。然而，缺乏严格的理论来证明其收敛性。&lt;h4&gt;目的&lt;/h4&gt;首次探讨对比学习在可调电阻网络上的应用，并提供严格的数学分析以保证该算法的收敛性。&lt;h4&gt;方法&lt;/h4&gt;通过对基于能量的学习算法——对比学习（应用于线性可调整电阻网络）进行详细分析，发现此设置下对比学习等价于任何步长下的凸函数投影梯度下降。&lt;h4&gt;主要发现&lt;/h4&gt;证明了在特定条件下，对比学习算法与投影梯度下降是等价的，并且对于所有步长都保证了收敛性。&lt;h4&gt;结论&lt;/h4&gt;研究为基于能量的学习方法提供了一个坚实的理论基础，特别是在类比电子实现方面，展示了其潜在的应用价值和可靠性的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Energy-based learning algorithms are alternatives to backpropagation and arewell-suited to distributed implementations in analog electronic devices.However, a rigorous theory of convergence is lacking. We make a first step inthis direction by analysing a particular energy-based learning algorithm,Contrastive Learning, applied to a network of linear adjustable resistors. Itis shown that, in this setup, Contrastive Learning is equivalent to projectedgradient descent on a convex function, for any step size, giving a guarantee ofconvergence for the algorithm.</description>
      <author>example@mail.com (Anne-Men Huijzer, Thomas Chaffey, Bart Besselink, Henk J. van Waarde)</author>
      <guid isPermaLink="false">2503.00349v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Token-level Text Image Foundation Model for Document Understanding</title>
      <link>http://arxiv.org/abs/2503.02304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在处理包含小而密集文本的图像时，这些模型仍然存在基本预测错误的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的通用视觉基础模型虽然广泛应用于多模态大型语言模型中作为图像编码器，但在缺乏语义精细级监督的情况下，面对含有小而密文字的图像任务（如感知、理解和推理）时，仍会出现关键性的预测误差问题。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究团队开发了TokenOCR，这是首个针对文本-图像相关任务设计的基于标记级别的视觉基础模型，能够支持多种传统下游应用。&lt;h4&gt;方法&lt;/h4&gt;为促进TokenOCR的预训练，研究者们构建了一个高质量的数据生成管道，创建了TokenIT数据集，包含2000万张图像和18亿对token-mask。此外，通过利用具有卓越图像作为文本能力的基础模型，他们使用TokenOCR替代先前的VFMs来构造一个文档级多模态大型语言模型(TokenVL)，用于基于VQA（视觉问答）的任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TokenOCR和TokenVL在多项任务中表现出色。这些成果为未来的研究提供了数据集、代码和权重资源。&lt;h4&gt;结论&lt;/h4&gt;通过开发TokenOCR及TokenVL，研究团队成功地解决了现有视觉基础模型面对含有小而密文字的图像时存在的预测误差问题，并且证明了基于标记级别的视觉基础模型在处理文本-图像相关任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在缺乏语义精细级监督的情况下，这些模型在面对含有小而密文字的图像任务时仍会遇到关键性预测误差的问题。为解决此问题，研究团队开发了TokenOCR，一个专为文本-图像相关任务设计的标记级别视觉基础模型，并构建了一个高质量的数据生成管道以创建TokenIT数据集（包含2000万张图像和18亿对token-mask）。此外，他们还通过利用具有卓越图像作为文本能力的基础模型构造了文档级多模态大型语言模型(TokenVL)。实验结果显示，TokenOCR与TokenVL在多项任务中表现出色，并且研究团队将提供数据集、代码和权重资源以供未来的研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, general visual foundation models (VFMs) have witnessedincreasing adoption, particularly as image encoders for popular multi-modallarge language models (MLLMs). However, without semantically fine-grainedsupervision, these models still encounter fundamental prediction errors in thecontext of downstream text-image-related tasks, i.e., perception, understandingand reasoning with images containing small and dense texts. To bridge this gap,we develop TokenOCR, the first token-level visual foundation model specificallytailored for text-image-related tasks, designed to support a variety oftraditional downstream applications. To facilitate the pretraining of TokenOCR,we also devise a high-quality data production pipeline that constructs thefirst token-level image text dataset, TokenIT, comprising 20 million images and1.8 billion token-mask pairs. Furthermore, leveraging this foundation withexceptional image-as-text capability, we seamlessly replace previous VFMs withTokenOCR to construct a document-level MLLM, TokenVL, for VQA-based documentunderstanding tasks. Finally, extensive experiments demonstrate theeffectiveness of TokenOCR and TokenVL. Code, datasets, and weights will beavailable at https://token-family.github.io/TokenOCR_project.</description>
      <author>example@mail.com (Tongkun Guan, Zining Wang, Pei Fu, Zhengtao Guo, Wei Shen, Kai Zhou, Tiezhu Yue, Chen Duan, Hao Sun, Qianyi Jiang, Junfeng Luo, Xiaokang Yang)</author>
      <guid isPermaLink="false">2503.02304v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Panoptic Segmentation</title>
      <link>http://arxiv.org/abs/2503.02372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对LiDAR全景分割的低标签需求的方法L3PS，该方法通过利用2D网络生成伪标签并结合新颖的3D细化模块来提高点云数据中的语义和实例分割性能。&lt;h4&gt;背景&lt;/h4&gt;学习型机器人场景理解技术在处理复杂高维点云数据时面临训练数据标注量大的问题，这限制了其泛化能力。特别是LiDAR全景分割，需要同时进行语义和实例分割，对训练样本的需求更大。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决使用少量标记样本的LiDAR全景分割任务，并减轻大规模数据标注的工作负担。&lt;h4&gt;方法&lt;/h4&gt;1. 利用标签高效2D网络从少量注释图像中生成全景伪标签。           2. 将这些伪标签投影到点云上进行3D细化，通过聚类技术、顺序扫描累积和地面点分离来增强标签准确性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的3D细化模块能够显著提高伪标签的质量，在PQ指标上的改进可达+10.6，在mIoU指标上有+7.9的提升。这些经过优化后的伪标签可以用来有效训练现成的LiDAR分割网络。&lt;h4&gt;结论&lt;/h4&gt;L3PS方法不仅在性能上超越了现有的方法，还大大降低了标注样本的需求量，从而减轻了人力和时间负担。&lt;h4&gt;翻译&lt;/h4&gt;学习型机器人场景理解技术面临的主要瓶颈是对大量注释训练数据的高度依赖，这通常限制了其泛化能力。在LiDAR全景分割中，由于需要同时处理复杂高维点云数据中的语义和实例分割任务，这一挑战更加突出。为了解决使用少量标记样本的LiDAR全景分割难题，本文借鉴最新的标签高效视觉全景分割技术，提出了一种名为Limited-Label LiDAR Panoptic Segmentation (L3PS)的新方法，仅需极少量标注数据即可工作。该方法首先利用一个高效的2D网络从少数注释图像中生成伪标签，并将其投影到点云上。然后引入了一个新颖的3D细化模块，充分利用了点云的几何特性，通过聚类技术、顺序扫描累积和地面点分离来显著提升伪标签的精度，从而提高了分割质量（PQ指标增加+10.6，mIoU指标增加+7.9）。实验表明，这些优化后的伪标签可以有效地训练现成的LiDAR分割网络。通过广泛的试验验证了L3PS不仅在性能上超过了现有方法，还大大减少了标注负担。该工作的代码已发布于https://l3ps.cs.uni-freiburg.de。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A main bottleneck of learning-based robotic scene understanding methods isthe heavy reliance on extensive annotated training data, which often limitstheir generalization ability. In LiDAR panoptic segmentation, this challengebecomes even more pronounced due to the need to simultaneously address bothsemantic and instance segmentation from complex, high-dimensional point clouddata. In this work, we address the challenge of LiDAR panoptic segmentationwith very few labeled samples by leveraging recent advances in label-efficientvision panoptic segmentation. To this end, we propose a novel method,Limited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimalamount of labeled data. Our approach first utilizes a label-efficient 2Dnetwork to generate panoptic pseudo-labels from a small set of annotatedimages, which are subsequently projected onto point clouds. We then introduce anovel 3D refinement module that capitalizes on the geometric properties ofpoint clouds. By incorporating clustering techniques, sequential scanaccumulation, and ground point separation, this module significantly enhancesthe accuracy of the pseudo-labels, improving segmentation quality by up to+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can beused to effectively train off-the-shelf LiDAR segmentation networks. Throughextensive experiments, we show that L3PS not only outperforms existing methodsbut also substantially reduces the annotation burden. We release the code ofour work at https://l3ps.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Ahmet Selim Çanakçı, Niclas Vödisch, Kürsat Petek, Wolfram Burgard, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.02372v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
      <link>http://arxiv.org/abs/2503.01722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个新的基于图神经网络的方法EgoNetGNN，用于自动学习适合复杂同行影响机制的暴露映射函数。&lt;h4&gt;背景&lt;/h4&gt;在因果推断中，干扰是指个体在网络中的行为受到同伴行动的影响。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来估计同行效应。现有的方法主要基于同伴数量或比例来决定暴露映射函数。&lt;h4&gt;目的&lt;/h4&gt;为了更准确地估计异质性同行效应（不同情境下相同同行暴露下的反事实结果的变化），该研究旨在开发一种能够自动学习合适暴露映射函数的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络(EgoNetGNN) 方法，它可以考虑到局部邻居结构和边属性等因素来处理复杂的同行影响机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于数量或比例的同伴暴露模型或者简单地学习同伴暴露的模型难以应对复杂的影响机制。而该方法在合成数据及半合成网络数据上估计异质性同行效应时比现有最佳基准更稳健。&lt;h4&gt;结论&lt;/h4&gt;EgoNetGNN能够更好地处理复杂的同行影响，提供了一个自动学习适当暴露映射函数的新视角，有助于更加准确地评估同行效应。&lt;h4&gt;翻译&lt;/h4&gt;在因果推理中，干扰指的是网络中的个体行为受到同伴行动的影响。同行效应指的是一个人在不同同伴暴露水平下的反事实结果差异，即一个人受同伴治疗、行动或行为影响的程度。估计同行效应需要决定如何表示同伴暴露。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来决定这一点。现有的大多数方法都假设了基于同伴数量或者比例的同行暴露。最近的研究探讨了更复杂的同行暴露功能，以捕捉不同同伴施加的不同程度的影响。然而，这些研究没有明确考虑自动学习暴露映射函数的问题。在这项工作中，我们专注于为估计异质性同行效应开发这样的函数，其中异质性指的是在相同的同伴暴露下但不同的个人背景下的反事实结果的变化。我们提出了EgoNetGNN, 一种基于图神经网络的方法，用于自动学习适当的暴露映射函数以处理复杂的同行影响机制，这些机制不仅涉及到同伴治疗，还包括局部邻居结构和边属性。我们的综合评估表明，在估计异质性同行效应时，与最先进的基准相比，该方法在面对不同的未知潜在影响机制时表现得更稳健。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In causal inference, interference refers to the phenomenon in which theactions of peers in a network can influence an individual's outcome. Peereffect refers to the difference in counterfactual outcomes of an individual fordifferent levels of peer exposure, the extent to which an individual is exposedto the treatments, actions, or behaviors of peers. Estimating peer effectsrequires deciding how to represent peer exposure. Typically, researchers definean exposure mapping function that aggregates peer treatments and outputs peerexposure. Most existing approaches for defining exposure mapping functionsassume peer exposure based on the number or fraction of treated peers. Recentstudies have investigated more complex functions of peer exposure which capturethat different peers can exert different degrees of influence. However, none ofthese works have explicitly considered the problem of automatically learningthe exposure mapping function. In this work, we focus on learning this functionfor the purpose of estimating heterogeneous peer effects, where heterogeneityrefers to the variation in counterfactual outcomes for the same peer exposurebut different individual's contexts. We develop EgoNetGNN, a graph neuralnetwork (GNN)-based method, to automatically learn the appropriate exposuremapping function allowing for complex peer influence mechanisms that, inaddition to peer treatments, can involve the local neighborhood structure andedge attributes. We show that GNN models that use peer exposure based on thenumber or fraction of treated peers or learn peer exposure naively facedifficulty accounting for such influence mechanisms. Our comprehensiveevaluation on synthetic and semi-synthetic network data shows that our methodis more robust to different unknown underlying influence mechanisms whenestimating heterogeneous peer effects when compared to state-of-the-artbaselines.</description>
      <author>example@mail.com (Shishir Adhikari, Sourav Medya, Elena Zheleva)</author>
      <guid isPermaLink="false">2503.01722v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers</title>
      <link>http://arxiv.org/abs/2503.00958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的作者归属任务的方法，利用了预训练的基于Transformer模型的不同层次中学习到的各种语言表示。&lt;h4&gt;背景&lt;/h4&gt;现有的作者归属方法可能在跨域数据集上的表现不佳，这限制了它们的实际应用范围和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;通过评估不同层次的Transformer语言模型在内部和外部领域的效果，来提高作者归属任务的准确性和稳健性。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的基于Transformer的模型的不同层中学习到的语言表示进行作者归属，并与最先进的基准在相同领域和跨域场景下进行了对比实验。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同层次的变压器提高了作者归属模型在外域数据上的鲁棒性，从而达到了新的最先进水平。分析表明，每一层都专门化于表示某些有助于外部域测试的风格特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果为更好地理解Transformer模型在不同层级中对文本风格属性的表达提供了一种途径，并为进一步改进作者归属任务奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的方法用于作者归属任务，这种方法利用了预训练的基于Transformer的语言模型的不同层次中学到的各种语言表示。我们在三个数据集上评估了该方法，并将其与最先进的基准进行了比较，在内部领域和跨域场景中进行测试。结果表明，使用不同的Transformer层可以提高在外部领域数据上的鲁棒性，从而产生了新的最先进成果。我们的分析进一步揭示了模型的不同层次如何专门化表示某些风格特征以增强其在外域中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new approach for the authorship attribution task that leveragesthe various linguistic representations learned at different layers ofpre-trained transformer-based models. We evaluate our approach on threedatasets, comparing it to a state-of-the-art baseline in in-domain andout-of-domain scenarios. We found that utilizing various transformer layersimproves the robustness of authorship attribution models when tested onout-of-domain data, resulting in new state-of-the-art results. Our analysisgives further insights into how our model's different layers get specialized inrepresenting certain stylistic features that benefit the model when tested outof the domain.</description>
      <author>example@mail.com (Milad Alshomary, Nikhil Reddy Varimalla, Vishal Anand, Kathleen McKeown)</author>
      <guid isPermaLink="false">2503.00958v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range Images</title>
      <link>http://arxiv.org/abs/2503.02300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, submitted to 2025 IROS. This work has been  submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注，但由于生成的点云较为稀疏且含有大量噪声，其进一步发展受到限制。&lt;h4&gt;背景信息&lt;/h4&gt;尽管毫米波雷达在恶劣环境中的感知稳定性很高，但生成的点云相对稀疏并包含显著的噪声。传统的毫米波雷达增强方法难以充分利用扩散模型在超分辨率方面的效果。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种新的方法，将范围图像与图像扩散模型融合，以实现准确且密集的毫米波雷达点云，使其类似于激光雷达（LiDAR）生成的点云。&lt;h4&gt;所用方法&lt;/h4&gt;通过利用范围图像与人类观察一致的投影方式，以及预训练的图像扩散模型的知识迁移，该方法实现了自然图像表示的改进，提高了整体性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集和自建数据集上的大量评估显示，这种方法提供了显著的提升，生成了高质量的三维激光雷达（LiDAR）点云。&lt;h4&gt;结论&lt;/h4&gt;这项研究成功地利用毫米波雷达生成类似激光雷达的三维点云，并确立了一个新的性能标准。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注。虽然它在恶劣环境中的感知稳定性较高，但由其产生的点云较为稀疏且含有大量噪声，这限制了它的进一步发展。传统的方法难以利用扩散模型进行超分辨率处理，很大程度上是因为不自然的范围-方位热图（RAH）或鸟瞰视图（BEV）表示方式。为了克服这一局限性，我们提出了一种将范围图像与图像扩散模型融合的新方法，实现了准确且密集的毫米波雷达点云，类似于激光雷达生成的结果。通过与人类观察一致的投影对齐，该方法利用了预训练的图像扩散模型的知识迁移，显著提升了整体性能。在公共数据集和自建数据集上的广泛评估表明，我们的方法提供了显著的改进，并确立了一个新的性能标准，在使用毫米波雷达生成类似激光雷达的三维点云方面建立了新基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar has attracted significant attention inrobotics and autonomous driving. However, despite the perception stability inharsh environments, the point cloud generated by mmWave radar is relativelysparse while containing significant noise, which limits its furtherdevelopment. Traditional mmWave radar enhancement approaches often struggle toleverage the effectiveness of diffusion models in super-resolution, largely dueto the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)representation. To overcome this limitation, we propose a novel method thatpioneers the application of fusing range images with image diffusion models,achieving accurate and dense mmWave radar point clouds that are similar toLiDAR. Benefitting from the projection that aligns with human observation, therange image representation of mmWave radar is close to natural images, allowingthe knowledge from pre-trained image diffusion models to be effectivelytransferred, significantly improving the overall performance. Extensiveevaluations on both public datasets and self-constructed datasets demonstratethat our approach provides substantial improvements, establishing a newstate-of-the-art performance in generating truly three-dimensional LiDAR-likepoint clouds via mmWave radar.</description>
      <author>example@mail.com (Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Huan Yu, Zhi Zheng, Kaixiang Huang, Guodong Lu)</author>
      <guid isPermaLink="false">2503.02300v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GRNFormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into RNA Foundation Models</title>
      <link>http://arxiv.org/abs/2503.01682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRNFormer是一种新的框架，用于将多尺度基因调控网络（GRNs）从多组学数据中系统地整合到RNA基础模型训练中。&lt;h4&gt;背景&lt;/h4&gt;现有的单细胞RNA测序模型虽然显示了捕捉基因表达模式的能力，但它们忽略了生物先验知识，并未能利用多组学信号提供补充的监管见解。&lt;h4&gt;目的&lt;/h4&gt;提出GRNFormer框架，通过引入分层GRNs构建管道和结构感知整合框架来解决这些问题。&lt;h4&gt;方法&lt;/h4&gt;GRNFormer框架包括两个创新点：一是构造细胞类型特异性和细胞特定分辨率下捕获调控关系的分层GRNs；二是设计了一种新颖的边缘扰动策略，使用多头交叉注意力机制动态加权监管关系，并通过引入生物信息共表达链接来增强图神经网络训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在药物反应预测、单细胞药物分类和基因干扰预测任务中，GRNFormer分别提高了3.6%的相关性、9.6%的AUC和1.1%的准确率。&lt;h4&gt;结论&lt;/h4&gt;GRNFormer框架在多种模型架构上均表现出优于现有方法的效果。&lt;h4&gt;翻译&lt;/h4&gt;基础单细胞RNA测序（scRNA-seq）模型已显示出捕捉基因表达模式的能力，但当前的方法忽略了编码在基因调控关系中的生物先验知识，并未能利用多组学信号提供补充的监管见解。本文提出了一种新框架GRNFormer，该框架将从多组学数据中推断出的多尺度基因调控网络（GRNs）系统地整合到RNA基础模型训练中。该框架引入了两个关键创新：构建分层GRNs的管道和一种新的结构感知集成框架。GRNFormer通过综合实验在多个代表性下游任务上展示了其有效性，并优于现有的最佳基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for single-cell RNA sequencing (scRNA-seq) have shownpromising capabilities in capturing gene expression patterns. However, currentapproaches face critical limitations: they ignore biological prior knowledgeencoded in gene regulatory relationships and fail to leverage multi-omicssignals that could provide complementary regulatory insights. In this paper, wepropose GRNFormer, a new framework that systematically integrates multi-scaleGene Regulatory Networks (GRNs) inferred from multi-omics data into RNAfoundation model training. Our framework introduces two key innovations. First,we introduce a pipeline for constructing hierarchical GRNs that captureregulatory relationships at both cell-type-specific and cell-specificresolutions. Second, we design a structure-aware integration framework thataddresses the information asymmetry in GRNs through two technical advances: (1)A graph topological adapter using multi-head cross-attention to weightregulatory relationships dynamically, and (2) a novel edge perturbationstrategy that perturb GRNs with biologically-informed co-expression links toaugment graph neural network training. Comprehensive experiments have beenconducted on three representative downstream tasks across multiple modelarchitectures to demonstrate the effectiveness of GRNFormer. It achievesconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\%$increase in drug response prediction correlation, $9.6\%$ improvement insingle-cell drug classification AUC, and $1.1\%$ average gain in geneperturbation prediction accuracy.</description>
      <author>example@mail.com (Mufan Qiu, Xinyu Hu, Fengwei Zhan, Sukwon Yun, Jie Peng, Ruichen Zhang, Bhavya Kailkhura, Jiekun Yang, Tianlong Chen)</author>
      <guid isPermaLink="false">2503.01682v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improve Representation for Imbalanced Regression through Geometric Constraints</title>
      <link>http://arxiv.org/abs/2503.00876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. The first three authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表示学习中的均匀性概念，特别是对于不平衡回归任务的意义，并提出了一种基于几何原理的损失函数方法。&lt;h4&gt;背景&lt;/h4&gt;在表示学习中，均匀性指的是潜在空间（即单位超球面）中特征分布的一致性。以往的研究表明，提高这种一致性有助于未充分代表类别的学习。&lt;h4&gt;目的&lt;/h4&gt;探讨如何确保不平衡回归任务中的表示空间保持一致性和平滑性的方法，并提出两种关键损失函数来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;提出了两个几何损失：包容损失和同质性损失。包容损失鼓励特征在超球面上均匀分布，而同质性损失保证了平滑性，并且以相同间隔使表示均匀分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过Surrogate-driven Representation Learning (SRL)框架将这些几何原理整合到数据表示中，实验结果表明对于不平衡回归任务而言，一致性和平滑性的保持非常重要。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法在真实世界中的回归和操作学习任务上验证了其有效性，并强调了几何损失函数在提高不平衡数据集性能方面的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;在表示学习中，一致性指的是潜在空间（即单位超球面）的特征分布均匀性。先前的工作表明，改善这种一致性有助于欠代表类别的学习。然而，大多数以前的研究专注于分类任务；对于不平衡回归而言，表示空间尚未得到探索。基于分类的方法不适合回归任务，因为它们将特性聚集成不连续组群而没有考虑回归所需的重要因素：连续性和顺序排列性。从几何角度来看，本研究独特地关注确保超球面上的特征分布均匀，并通过两种关键损失函数（包容损失和同质性损失）实现这一目标。我们的方法利用代理驱动表示学习框架将这些几何原理整合到数据表示中。在真实世界回归任务和操作学习上的实验验证了这种一致性对于不平衡回归的重要性，以及我们基于几何学的损失函数的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In representation learning, uniformity refers to the uniform featuredistribution in the latent space (i.e., unit hypersphere). Previous work hasshown that improving uniformity contributes to the learning ofunder-represented classes. However, most of the previous work focused onclassification; the representation space of imbalanced regression remainsunexplored. Classification-based methods are not suitable for regression tasksbecause they cluster features into distinct groups without considering thecontinuous and ordered nature essential for regression. In a geometric aspect,we uniquely focus on ensuring uniformity in the latent space for imbalancedregression through two key losses: enveloping and homogeneity. The envelopingloss encourages the induced trace to uniformly occupy the surface of ahypersphere, while the homogeneity loss ensures smoothness, withrepresentations evenly spaced at consistent intervals. Our method integratesthese geometric principles into the data representations via a Surrogate-drivenRepresentation Learning (SRL) framework. Experiments with real-world regressionand operator learning tasks highlight the importance of uniformity inimbalanced regression and validate the efficacy of our geometry-based lossfunctions.</description>
      <author>example@mail.com (Zijian Dong, Yilei Wu, Chongyao Chen, Yingtian Zou, Yichi Zhang, Juan Helen Zhou)</author>
      <guid isPermaLink="false">2503.00876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fluorescence-Guided Autonomous Robotic Partial Nephrectomy on Novel Tissue-Mimicking Hydrogel Phantoms</title>
      <link>http://arxiv.org/abs/2503.02265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages. 7 figures. Preprint of an article accepted for publication  in the Journal of Medical Robotics Research, 2025. Copyright World Scientific  Publishing Company [https://worldscientific.com/worldscinet/jmrr]&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种荧光引导的自主机器人系统，能够为外生性肾肿瘤规划和执行切除路径，并通过近红外成像区分健康组织与肿瘤组织。&lt;h4&gt;背景&lt;/h4&gt;自动化的机器人系统有可能提高肾脏肿瘤切除手术的精度及患者的治疗效果。现有的动物模型难以在实验中模拟人类组织的具体行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够用于荧光引导下自主执行肾肿瘤切除任务的机器人系统，并设计出可以模仿人体组织特性的新型水凝胶基体模。&lt;h4&gt;方法&lt;/h4&gt;使用点云观察处理不规则形状的肿瘤；利用近红外成像区分健康与肿瘤组织，采用基于水凝胶的仿生肾脏体模来模拟真实的人类组织特性。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够在69秒内完成切除任务，并达到了1.44毫米的平均切缘精度。此外，通过将材料与近红外染料混合的方式实现了荧光引导下的肿瘤分割功能。&lt;h4&gt;结论&lt;/h4&gt;新型水凝胶体模和荧光引导机器人系统为肾癌及其他类似手术中的自主机器人技术的发展提供了重要的研究平台和实验工具。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人系统在提高肾脏肿瘤切除的精度及患者治疗效果方面展现出巨大潜力。文中介绍了一种具有规划执行路径能力和临床相关切缘的安全性指标的荧光引导自主机器人系统。该系统利用点云观测处理不规则形状的肿瘤，并通过近红外成像技术区分健康组织与肿瘤组织，模拟靛青绿染色在部分肾切除手术中的作用。鉴于无法获得用于特定类型干预的人体或动物离体组织，仿生肾脏体模对于自主机器人外科系统的开发至关重要。为了克服硅胶基体模限制的问题，研究团队提出了一种新型的水凝胶基础模来模仿人体组织的物理和视觉特性，并兼容电手术设备。与先前的水凝胶体模不同的是，该设计中加入了近红外染料以实现荧光引导下的肿瘤分割功能。通过自主现实世界中的机器人实验验证了系统性能，在69秒内达到了1.44毫米平均切缘精度的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic systems hold potential for improving renal tumor resectionaccuracy and patient outcomes. We present a fluorescence-guided robotic systemcapable of planning and executing incision paths around exophytic renal tumorswith a clinically relevant resection margin. Leveraging point cloudobservations, the system handles irregular tumor shapes and distinguisheshealthy from tumorous tissue based on near-infrared imaging, akin toindocyanine green staining in partial nephrectomy. Tissue-mimicking phantomsare crucial for the development of autonomous robotic surgical systems forinterventions where acquiring ex-vivo animal tissue is infeasible, such ascancer of the kidney and renal pelvis. To this end, we propose novelhydrogel-based kidney phantoms with exophytic tumors that mimic the physicaland visual behavior of tissue, and are compatible with electrosurgicalinstruments, a common limitation of silicone-based phantoms. In contrast toprevious hydrogel phantoms, we mix the material with near-infrared dye toenable fluorescence-guided tumor segmentation. Autonomous real-world roboticexperiments validate our system and phantoms, achieving an average marginaccuracy of 1.44 mm in a completion time of 69 sec.</description>
      <author>example@mail.com (Ethan Kilmer, Joseph Chen, Jiawei Ge, Preksha Sarda, Richard Cha, Kevin Cleary, Lauren Shepard, Ahmed Ezzat Ghazi, Paul Maria Scheikl, Axel Krieger)</author>
      <guid isPermaLink="false">2503.02265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network</title>
      <link>http://arxiv.org/abs/2503.00036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于频域特征和动态图神经网络的无线传感器网络（WSN）异常检测方法，通过设计自编码重构框架有效解决了现有注意力机制变压器在捕捉长期依赖性和计算复杂性方面的局限。&lt;h4&gt;背景&lt;/h4&gt;注意力驱动的转换器在网络中用于无线传感的时间异常检测上发挥了重要作用，但它们存在无法完全可靠地捕获长期依赖、高计算复杂度以及未充分提取时空特征等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的WSN时间序列数据异常检测方法来克服现有模型在捕捉长期依赖性和计算效率上的问题，并更有效地利用多节点WSN时间序列的时空相关性。&lt;h4&gt;方法&lt;/h4&gt;首先，采用离散小波变换有效分解时间序列的趋势和季节成分；其次，设计频域注意力机制充分利用正常数据与异常数据之间幅度分布的区别；最后，通过结合注意机制和图卷积网络（GCN）提出多模态融合动态图卷积网络（MFDGCN），自适应地提取空间相关特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在公共数据集上的精度、召回率以及F1得分分别为93.5%，比现有模型提高了2.9%。&lt;h4&gt;结论&lt;/h4&gt;该异常检测方法通过改进的时间序列分解，频域注意力机制和动态图卷积网络的结合，在提高WSN时间异常检测效果方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;基于注意的转换器在无线传感器网络（WSN）定时异常检测中发挥了重要作用，因为它们能够捕捉长期依赖性。然而，有几个问题需要解决，例如它们捕获长期依赖性的能力并不完全可靠，计算复杂度水平很高，并且没有充分提取WSN定时数据的空间时间特征来检测多节点WSN定时数据的相关异常。为了解决这些限制，本文提出了一种将频率域特性与动态图神经网络（GNN）集成在设计的自编码重构框架下的WSN异常检测方法。首先，离散小波变换有效地分解了时间序列的趋势和季节成分来解决转换器长期可靠性差的问题。其次，设计了一个频域注意机制来充分利用正常数据和异常数据之间在这个领域的幅度分布差异。最后，通过结合注意力机制和图卷积网络（GCN）设计了一种基于多模态融合的动态图卷积网络（MFDGCN），以自适应地提取空间相关特性。在公共数据集上进行的一系列实验及其结果表明，本文设计的异常检测方法比现有方法具有更高的精度和召回率，F1分数为93.5％，比现有模型提高了2.9％。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention-based transformers have played an important role in wireless sensornetwork (WSN) timing anomaly detection due to their ability to capturelong-term dependencies. However, there are several issues that must beaddressed, such as the fact that their ability to capture long-termdependencies is not completely reliable, their computational complexity levelsare high, and the spatiotemporal features of WSN timing data are notsufficiently extracted for detecting the correlation anomalies of multinode WSNtiming data. To address these limitations, this paper proposes a WSN anomalydetection method that integrates frequency-domain features with dynamic graphneural networks (GNN) under a designed self-encoder reconstruction framework.First, the discrete wavelet transform effectively decomposes trend and seasonalcomponents of time series to solve the poor long-term reliability oftransformers. Second, a frequency-domain attention mechanism is designed tomake full use of the difference between the amplitude distributions of normaldata and anomalous data in this domain. Finally, a multimodal fusion-baseddynamic graph convolutional network (MFDGCN) is designed by combining anattention mechanism and a graph convolutional network (GCN) to adaptivelyextract spatial correlation features. A series of experiments conducted onpublic datasets and their results demonstrate that the anomaly detection methoddesigned in this paper exhibits superior precision and recall than the existingmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% overthat of the existing models.</description>
      <author>example@mail.com (Miao Ye, Zhibang Jiang, Xingsi Xue, Xingwang Li, Peng Wen, Yong Wang)</author>
      <guid isPermaLink="false">2503.00036v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments</title>
      <link>http://arxiv.org/abs/2503.02106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种框架，用于解决机器人在室内环境中进行多对象搜索时遇到的挑战。该框架结合了视觉语言模型（VLM）推理、前沿探索和部分可观测马尔可夫决策过程（POMDP），以提高搜索效率并克服观察不确定性的障碍。&lt;h4&gt;背景&lt;/h4&gt;物体搜索是部署在室内环境中的机器人的一项基本任务，但观察不稳定性和开放词汇表模型带来的挑战使得这一任务变得复杂。虽然基础模型可以推断出对象的位置关系，但在遮挡和混乱环境中恢复失败的能力仍然至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架来解决多对象搜索问题，特别是在新环境中寻找多个物体时遇到的困难，并提高机器人在室内环境中的搜索效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合视觉语言模型推理、前沿探索技术和部分可观测马尔可夫决策过程（POMDP）的综合框架。VLM增强了对物体与环境关系的理解，而基于前沿的探索技术帮助导航未知空间；POMDP则模拟了观察不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在120个仿真的HM3D场景和一个真实的机器人实验中对该框架进行了测试，在效率和成功率方面均优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的结合VLM、前沿探索技术和POMDP的综合框架能够显著提升机器人解决多对象搜索问题的能力，尤其是在应对观察不确定性时表现优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object search is a fundamental task for robots deployed in indoor buildingenvironments, yet challenges arise due to observation instability, especiallyfor open-vocabulary models. While foundation models (LLMs/VLMs) enablereasoning about object locations even without direct visibility, the ability torecover from failures and replan remains crucial. The Multi-Object Search (MOS)problem further increases complexity, requiring the tracking multiple objectsand thorough exploration in novel environments, making observation uncertaintya significant obstacle. To address these challenges, we propose a frameworkintegrating VLM-based reasoning, frontier-based exploration, and a PartiallyObservable Markov Decision Process (POMDP) framework to solve the MOS problemin novel environments. VLM enhances search efficiency by inferringobject-environment relationships, frontier-based exploration guides navigationin unknown spaces, and POMDP models observation uncertainty, allowing recoveryfrom failures in occlusion and cluttered environments. We evaluate ourframework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D)scenes and a real-world robot experiment in a 50-square-meter office,demonstrating significant improvements in both efficiency and success rate overbaseline methods.</description>
      <author>example@mail.com (Qianwei Wang, Yifan Xu, Vineet Kamat, Carol Menassa)</author>
      <guid isPermaLink="false">2503.02106v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization</title>
      <link>http://arxiv.org/abs/2503.01488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了InversionGNN框架，一种用于多目标药物发现的高效双路径图神经网络。&lt;h4&gt;背景&lt;/h4&gt;在药物发现中探索化学空间以寻找同时满足多种属性的新分子非常重要。现有方法在平衡这些相互冲突或关联的化学属性时经常遇到挑战。&lt;h4&gt;目的&lt;/h4&gt;提出InversionGNN框架，旨在解决多目标药物发现中的性能权衡问题。&lt;h4&gt;方法&lt;/h4&gt;通过直接预测路径训练模型进行多重性质预测，学习最佳的功能基团组合；然后利用这一知识帮助反转生成路径产生具有所需性质的分子。此外，引入基于梯度的Pareto搜索方法以平衡冲突属性并生成最优分子。&lt;h4&gt;主要发现&lt;/h4&gt;InversionGNN能够在离散化学空间中近似地探索完整的Pareto前沿，并在多目标设置下展示出了高效性和有效性。&lt;h4&gt;结论&lt;/h4&gt;InversionGNN框架为解决药物发现中的多目标问题提供了一种有效且样本高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;探索化学空间以找到同时满足多种性质的新分子对于药物发现至关重要。然而，现有方法往往难以在属性之间进行权衡，因为这些属性是相互冲突或关联的。为了应对这一挑战，我们引入了InversionGNN框架——一种用于多目标药物发现的有效且样本高效的双路径图神经网络（GNN）。在这个模型中，直接预测路径训练模型来进行多重性质预测，以获取功能基团最佳组合的知识；然后，在反转生成路径上，通过所学的化学知识帮助产生具有所需属性的分子。为了在反转路径中解码多属性的复杂信息，我们提出了一种基于梯度的方法来平衡冲突属性并生成最优分子。此外，InversionGNN能够近似地探索离散化学空间中的完整Pareto前沿。全面的实验评估显示，在包括药物发现在内的各种离散多目标设置下，InversionGNN既有效又样本高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring chemical space to find novel molecules that simultaneously satisfymultiple properties is crucial in drug discovery. However, existing methodsoften struggle with trading off multiple properties due to the conflicting orcorrelated nature of chemical properties. To tackle this issue, we introduceInversionGNN framework, an effective yet sample-efficient dual-path graphneural network (GNN) for multi-objective drug discovery. In the directprediction path of InversionGNN, we train the model for multi-propertyprediction to acquire knowledge of the optimal combination of functionalgroups. Then the learned chemical knowledge helps the inversion generation pathto generate molecules with required properties. In order to decode the complexknowledge of multiple properties in the inversion path, we propose agradient-based Pareto search method to balance conflicting properties andgenerate Pareto optimal molecules. Additionally, InversionGNN is able to searchthe full Pareto front approximately in discrete chemical space. Comprehensiveexperimental evaluations show that InversionGNN is both effective andsample-efficient in various discrete multi-objective settings including drugdiscovery.</description>
      <author>example@mail.com (Yifan Niu, Ziqi Gao, Tingyang Xu, Yang Liu, Yatao Bian, Yu Rong, Junzhou Huang, Jia Li)</author>
      <guid isPermaLink="false">2503.01488v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Wavelet-Driven Masked Image Modeling: A Path to Efficient Visual Representation</title>
      <link>http://arxiv.org/abs/2503.00782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于小波变换的Masked Image Modeling (MIM) 方法，通过利用频率域分析来实现紧凑的图像特征表示。该方法解决了传统像素级MIM重建过程中过度关注细节的问题，并提高了训练效率。&lt;h4&gt;背景&lt;/h4&gt;掩膜图像建模(MIM)因其在自监督学习中的出色表现而备受关注，但其基于像素的重建过程会导致不必要的训练时间延长。&lt;h4&gt;目的&lt;/h4&gt;通过引入小波变换来改进MIM方法，以减少冗余信息的影响并加快训练速度。&lt;h4&gt;方法&lt;/h4&gt;使用多层分解技术将图像进行小波变换处理，并利用不同层级的小波系数构建不同频率和尺度的重建目标。然后在MIM过程中整合这些重建目标，同时可调整权重以优先考虑关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在多种下游任务中的性能与现有方法相当或更优，且训练效率更高。&lt;h4&gt;结论&lt;/h4&gt;基于小波变换的方法能够有效地改进MIM模型的训练过程，并提高其处理大规模视觉数据的能力。&lt;h4&gt;翻译&lt;/h4&gt;Masked Image Modeling (MIM)因其出色的自监督学习能力而受到关注。然而，图像中的冗余信息导致像素级重建过于关注细节，从而增加了不必要的训练时间。通过采用小波变换技术对图像进行频率分析，可以实现更紧凑的特征表示和高效的训练过程。实验表明该方法在多种下游任务中表现出色且更具效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Image Modeling (MIM) has garnered significant attention inself-supervised learning, thanks to its impressive capacity to learn scalablevisual representations tailored for downstream tasks. However, imagesinherently contain abundant redundant information, leading the pixel-based MIMreconstruction process to focus excessively on finer details such as textures,thus prolonging training times unnecessarily. Addressing this challengerequires a shift towards a compact representation of features during MIMreconstruction. Frequency domain analysis provides a promising avenue forachieving compact image feature representation. In contrast to the commonlyused Fourier transform, wavelet transform not only offers frequency informationbut also preserves spatial characteristics and multi-level features of theimage. Additionally, the multi-level decomposition process of wavelettransformation aligns well with the hierarchical architecture of modern neuralnetworks. In this study, we leverage wavelet transform as a tool for efficientrepresentation learning to expedite the training process of MIM. Specifically,we conduct multi-level decomposition of images using wavelet transform,utilizing wavelet coefficients from different levels to construct distinctreconstruction targets representing various frequencies and scales. Thesereconstruction targets are then integrated into the MIM process, withadjustable weights assigned to prioritize the most crucial information.Extensive experiments demonstrate that our method achieves comparable orsuperior performance across various downstream tasks while exhibiting highertraining efficiency.</description>
      <author>example@mail.com (Wenzhao Xiang, Chang Liu, Hongyang Yu, Xilin Chen)</author>
      <guid isPermaLink="false">2503.00782v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Biomedical Foundation Model: A Survey</title>
      <link>http://arxiv.org/abs/2503.02104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了基础模型在生物医学领域的潜在应用，涵盖了计算生物学、药物研发与开发、临床信息学、医学影像和公共卫生等多个领域。&lt;h4&gt;背景&lt;/h4&gt;基础模型自2021年引入以来，通过无监督学习从大规模未标记数据集中获取知识，这些模型如GPT能够在问答和视觉理解等多种下游任务中表现出色，并超越了特定任务的AI模型。生物医学领域的基础模型开发标志着人工智能在理解和解析复杂生物学现象以及推动医疗研究与实践方面的重要进展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索基础模型在健康科学中的应用潜力，以期激发相关领域的进一步研究和创新。&lt;h4&gt;方法&lt;/h4&gt;综述了各类生物医学任务中使用的基础模型及其潜在的广泛应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型具备广泛的适用性和卓越的任务性能，在多个生物医学领域展现出巨大的应用前景。&lt;h4&gt;结论&lt;/h4&gt;未来的研究应当更加关注如何将基础模型有效地应用于解决复杂的健康科学问题，以促进医疗技术的进步和临床实践的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, first introduced in 2021, are large-scale pre-trainedmodels (e.g., large language models (LLMs) and vision-language models (VLMs))that learn from extensive unlabeled datasets through unsupervised methods,enabling them to excel in diverse downstream tasks. These models, like GPT, canbe adapted to various applications such as question answering and visualunderstanding, outperforming task-specific AI models and earning their name dueto broad applicability across fields. The development of biomedical foundationmodels marks a significant milestone in leveraging artificial intelligence (AI)to understand complex biological phenomena and advance medical research andpractice. This survey explores the potential of foundation models acrossdiverse domains within biomedical fields, including computational biology, drugdiscovery and development, clinical informatics, medical imaging, and publichealth. The purpose of this survey is to inspire ongoing research in theapplication of foundation models to health science.</description>
      <author>example@mail.com (Xiangrui Liu, Yuanyuan Zhang, Yingzhou Lu, Changchang Yin, Xiaoling Hu, Xiaoou Liu, Lulu Chen, Sheng Wang, Alexander Rodriguez, Huaxiu Yao, Yezhou Yang, Ping Zhang, Jintai Chen, Tianfan Fu, Xiao Wang)</author>
      <guid isPermaLink="false">2503.02104v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election</title>
      <link>http://arxiv.org/abs/2503.01394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合语义分析和图神经网络的新方法，以解决社交媒体上谣言传播的问题。&lt;h4&gt;背景&lt;/h4&gt;社交媒体平台的发展极大地改变了信息传播的速度和方式，带来了有益和有害的影响。这些平台促进了快速通信的同时也加速了谣言和极端言论的传播，尤其是在选举期间对公共舆论和行为产生了显著影响。&lt;h4&gt;目的&lt;/h4&gt;针对有效的社交网络谣言检测需求，提出了一种新的方法来应对2024年全球各地前所未有的选举挑战。&lt;h4&gt;方法&lt;/h4&gt;采用语义分析与图神经网络相结合的方法。首先使用精调的BERT模型将文本内容向量化，并构建一个由推文和评论作为节点、互动行为为边的有向图，然后通过SAGEWithEdgeAttention（GraphSAGE的扩展）进行更精确特征聚合。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在复杂的社会网络结构中能够实现细粒度分析，提高谣言检测准确性，并且显著优于传统的内容分析和基于时间的方法。&lt;h4&gt;结论&lt;/h4&gt;研究得出的新方法提供了一种理论上严谨、实践中有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of social media platforms has revolutionized the speed andmanner in which information is disseminated, leading to both beneficial anddetrimental effects on society. While these platforms facilitate rapidcommunication, they also accelerate the spread of rumors and extremist speech,impacting public perception and behavior significantly. This issue isparticularly pronounced during election periods, where the influence of socialmedia on election outcomes has become a matter of global concern. With theunprecedented number of elections in 2024, against this backdrop, the electionecosystem has encountered unprecedented challenges. This study addresses theurgent need for effective rumor detection on social media by proposing a novelmethod that combines semantic analysis with graph neural networks. We havemeticulously collected a dataset from PolitiFact and Twitter, focusing onpolitically relevant rumors. Our approach involves semantic analysis using afine-tuned BERT model to vectorize text content and construct a directed graphwhere tweets and comments are nodes, and interactions are edges. The core ofour method is a graph neural network, SAGEWithEdgeAttention, which extends theGraphSAGE model by incorporating first-order differences as edge attributes andapplying an attention mechanism to enhance feature aggregation. This innovativeapproach allows for the fine-grained analysis of the complex social networkstructure, improving rumor detection accuracy. The study concludes that ourmethod significantly outperforms traditional content analysis and time-basedmodels, offering a theoretically sound and practically efficient solution.</description>
      <author>example@mail.com (Liu Yan, Liu Yunpeng, Zhao Liang)</author>
      <guid isPermaLink="false">2503.01394v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation</title>
      <link>http://arxiv.org/abs/2503.00733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025; demo page at  https://alexander-h-liu.github.io/uniwav-demo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;构建了一个统一的预训练框架UniWav，该框架可以同时适用于辨别任务和生成任务，在语音识别、文本到语音转换以及语音标记化等应用中表现出与特定任务模型相当的表现。&lt;h4&gt;背景&lt;/h4&gt;目前的语音处理方法依赖于不同的基础模型，这些模型通常是为了解决具体的辨别或生成任务而设计的。现有的预训练技术不足以同时适用于这两种类型的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的预训练框架，可以在单一的基础模型中实现多种类型的任务，并减少预训练的成本和复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为UniWav的编码器-解码器架构，该架构旨在统一预训练表示学习与生成任务。通过适当的预训练设计选择，可以同时学习适用于两种类型的任务的表示编码器和生成音频解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在语音识别、文本到语音转换以及语音标记化上，UniWav的表现与针对特定任务单独训练的基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;证明了一种通用的语音基础模型可以替代多种专为特定任务设计的基础模型，并且能够降低预训练的成本和复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training and representation learning have been playing an increasinglyimportant role in modern speech processing. Nevertheless, differentapplications have been relying on different foundation models, sincepredominant pre-training techniques are either designed for discriminativetasks or generative tasks. In this work, we make the first attempt at buildinga unified pre-training framework for both types of tasks in speech. We showthat with the appropriate design choices for pre-training, one can jointlylearn a representation encoder and generative audio decoder that can be appliedto both types of tasks. We propose UniWav, an encoder-decoder frameworkdesigned to unify pre-training representation learning and generative tasks. Onspeech recognition, text-to-speech, and speech tokenization, UniWav achievescomparable performance to different existing foundation models, each trained ona specific task. Our findings suggest that a single general-purpose foundationmodel for speech can be built to replace different foundation models, reducingthe overhead and cost of pre-training.</description>
      <author>example@mail.com (Alexander H. Liu, Sang-gil Lee, Chao-Han Huck Yang, Yuan Gong, Yu-Chiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro)</author>
      <guid isPermaLink="false">2503.00733v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EPEE: Towards Efficient and Effective Foundation Models in Biomedicine</title>
      <link>http://arxiv.org/abs/2503.02053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了EPEE策略，一种基于熵和耐心的混合早期退出策略，旨在提高基础模型在生物医学任务中的推理效率。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型如GPT、CLIP等显著推动了众多生物医学任务的发展，但高推理延迟及过度思考问题限制了它们在临床环境下的实时应用。&lt;h4&gt;目的&lt;/h4&gt;提出EPEE策略以克服现有早期退出方法的弱点，并提高基础模型在实际应用场景中的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;通过四个基础模型（BERT、ALBERT、GPT-2和ViT）进行三项核心生物医学任务（分类，关系抽取以及事件抽取）的实验评估。实验涵盖了十二个不同的数据集，包括临床笔记及医学图像。&lt;h4&gt;主要发现&lt;/h4&gt;EPEE在减少推理时间的同时保持或提升了准确性，在多种不同类型的数据集中展示了其适应性和有效性。&lt;h4&gt;结论&lt;/h4&gt;EPEE解决了将基础模型部署到医疗保健领域中的关键障碍，并为实时临床决策提供了可能的实用解决方案，支持可靠的高效工作流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含中文描述，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, including language models, e.g., GPT, and vision models,e.g., CLIP, have significantly advanced numerous biomedical tasks. Despitethese advancements, the high inference latency and the "overthinking" issues inmodel inference impair the efficiency and effectiveness of foundation models,thus limiting their application in real-time clinical settings. To addressthese challenges, we proposed EPEE (Entropy- and Patience-based Early Exiting),a novel hybrid strategy designed to improve the inference efficiency offoundation models. The core idea was to leverage the strengths of entropy-basedand patience-based early exiting methods to overcome their respectiveweaknesses. To evaluate EPEE, we conducted experiments on three core biomedicaltasks-classification, relation extraction, and event extraction-using fourfoundation models (BERT, ALBERT, GPT-2, and ViT) across twelve datasets,including clinical notes and medical images. The results showed that EPEEsignificantly reduced inference time while maintaining or improving accuracy,demonstrating its adaptability to diverse datasets and tasks. EPEE addressedcritical barriers to deploying foundation models in healthcare by balancingefficiency and effectiveness. It potentially provided a practical solution forreal-time clinical decision-making with foundation models, supporting reliableand efficient workflows.</description>
      <author>example@mail.com (Zaifu Zhan, Shuang Zhou, Huixue Zhou, Zirui Liu, Rui Zhang)</author>
      <guid isPermaLink="false">2503.02053v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Road Boundary Detection Using 4D mmWave Radar for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.01930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于4D毫米波雷达的道路边界检测方法（4DRadarRBD），以解决传统视觉传感器在复杂驾驶环境中的成本和性能问题。&lt;h4&gt;背景&lt;/h4&gt;道路边界检测对于自动驾驶和高级驾驶员辅助系统至关重要，但传统的基于摄像头和LiDAR的方法在恶劣光照条件下表现不佳或成本过高。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于4D毫米波雷达的道路边界检测方法，以提高复杂驾驶场景中的鲁棒性和降低成本。&lt;h4&gt;方法&lt;/h4&gt;通过物理约束减少点云噪声，并利用距离基损失函数进行分割。引入了对时间动态的捕捉机制，考虑每个点与车辆运动补偿后的道路边界检测结果之间的偏差以及点云的空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界驾驶测试中实现了93%的道路边界点分割准确率和高达0.023米的中位距离误差，并且相较于基线模型错误降低了92.6%。&lt;h4&gt;结论&lt;/h4&gt;4DRadarRBD方法在成本效益、鲁棒性和准确性方面显著优于传统道路边界检测技术，尤其适用于复杂驾驶环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting road boundaries, the static physical edges of the available drivingarea, is important for safe navigation and effective path planning inautonomous driving and advanced driver-assistance systems (ADAS).Traditionally, road boundary detection in autonomous driving relies on camerasand LiDAR. However, they are vulnerable to poor lighting conditions, such asnighttime and direct sunlight glare, or prohibitively expensive for low-endvehicles. To this end, this paper introduces 4DRadarRBD, the first roadboundary detection method based on 4D mmWave radar which is cost-effective androbust in complex driving scenarios. The main idea is that road boundaries(e.g., fences, bushes, roadblocks), reflect millimeter waves, thus generatingpoint cloud data for the radar. To overcome the challenge that the 4D mmWaveradar point clouds contain many noisy points, we initially reduce noisy pointsvia physical constraints for road boundaries and then segment the road boundarypoints from the noisy points by incorporating a distance-based loss whichpenalizes for falsely detecting the points far away from the actual roadboundaries. In addition, we capture the temporal dynamics of point cloudsequences by utilizing each point's deviation from the vehiclemotion-compensated road boundary detection result obtained from the previousframe, along with the spatial distribution of the point cloud for point-wiseroad boundary segmentation. We evaluated 4DRadarRBD through real-world drivingtests and achieved a road boundary point segmentation accuracy of 93$\%$, witha median distance error of up to 0.023 m and an error reduction of 92.6$\%$compared to the baseline model.</description>
      <author>example@mail.com (Yuyan Wu, Hae Young Noh)</author>
      <guid isPermaLink="false">2503.01930v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Generative Modeling by Kernel Similarity Matching</title>
      <link>http://arxiv.org/abs/2503.00655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过捕捉输入样本之间的相似性来学习表示的方法，并将其应用于生成模型中，以便能够将表示映射回输入空间。&lt;h4&gt;背景&lt;/h4&gt;理解大脑如何编码刺激是计算神经科学的基本问题。这方面的见解推动了具有类似大脑学习能力的人工神经网络的设计和开发。&lt;h4&gt;目的&lt;/h4&gt;研究一种基于内核相似性匹配的框架用于生成模型，并探讨该方法在脑任务表示编码中的潜在应用。&lt;h4&gt;方法&lt;/h4&gt;从先前工作中提出的稀疏编码目标修改开始，展示了在这个上下文中表征学习等同于最大化输入内核与隐含内核之间的相似性。提出了一个新的交替方向乘子法（ADMM）算法来解决优化问题，并讨论了优化过程的解释。&lt;h4&gt;主要发现&lt;/h4&gt;通过学习潜在空间中的内核结构可以形成一种隐式生成模型，该框架能够适应学习流形结构。&lt;h4&gt;结论&lt;/h4&gt;这种方法结合了使用相似性匹配（自下而上方法）进行表征学习与预测编码（自顶向下方法），有助于构建一个具有生物合理性的架构来学习模型参数。&lt;h4&gt;翻译&lt;/h4&gt;理解大脑如何对刺激进行编码一直是计算神经科学的基本问题。这些问题的见解促使设计和开发了一种人工神经网络，该网络通过引入类似大脑的学习能力来学习表示。最近的研究试图通过捕捉输入样本之间的相似性来解决这个问题。然而，这种方法迄今为止仅用于从输入中学习下游特征，并未在生成范式（可将表示映射回输入空间的范式）下进行研究，在这种情况下，不仅可以实现自底向上的相互作用（刺激到潜在），还可以以自顶向下的方式（潜在到刺激）学习特征。我们探讨了一种用于生成建模的内核相似性匹配框架。从先前工作中提出的一种修改后的稀疏编码目标开始，我们展示了在这种背景下表示学习等同于最大化输入内核与隐含内核之间的相似性。我们证明了通过在潜在空间中学习内核结构可以形成一种隐式生成模型，并展示如何使该框架适应学习流形结构，这可能有助于了解任务表征可以在大脑中是如何编码的。为了实现目标，我们提出了一种新颖的交替方向乘子法（ADMM）算法，并讨论了优化过程的解释。最后，我们探讨了这种表示学习问题可以导向一种生物学上合理的架构来学习模型参数，该架构将使用相似性匹配进行表征学习（自下而上的方法）与预测编码（自顶向下的方法）结合起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how the brain encodes stimuli has been a fundamental problem incomputational neuroscience. Insights into this problem have led to the designand development of artificial neural networks that learn representations byincorporating brain-like learning abilities. Recently, learning representationsby capturing similarity between input samples has been studied to tackle thisproblem. This approach, however, has thus far been used to only learndownstream features from an input and has not been studied in the context of agenerative paradigm, where one can map the representations back to the inputspace, incorporating not only bottom-up interactions (stimuli to latent) butalso learning features in a top-down manner (latent to stimuli). We investigatea kernel similarity matching framework for generative modeling. Starting with amodified sparse coding objective for learning representations proposed in priorwork, we demonstrate that representation learning in this context is equivalentto maximizing similarity between the input kernel and a latent kernel. We showthat an implicit generative model arises from learning the kernel structure inthe latent space and show how the framework can be adapted to learn manifoldstructures, potentially providing insights as to how task representations canbe encoded in the brain. To solve the objective, we propose a novel AlternateDirection Method of Multipliers (ADMM) based algorithm and discuss theinterpretation of the optimization process. Finally, we discuss how thisrepresentation learning problem can lead towards a biologically plausiblearchitecture to learn the model parameters that ties together representationlearning using similarity matching (a bottom-up approach) with predictivecoding (a top-down approach).</description>
      <author>example@mail.com (Shubham Choudhary, Paul Masset, Demba Ba)</author>
      <guid isPermaLink="false">2503.00655v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model</title>
      <link>http://arxiv.org/abs/2503.01361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）设计用于处理与图关联的数据，并在不断增加的应用领域中发挥作用。然而，与其他现代机器学习技术一样，其理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决由于过度平滑等问题导致的远距离节点信息获取困难的问题，并探讨如何通过增加深度来接近贝叶斯最优性。&lt;h4&gt;方法&lt;/h4&gt;分析了基于上下文随机块模型生成的数据进行训练的基本图卷积网络（GCN）在节点分类任务上的泛化性能。使用复制方法，在高维极限下推导问题的自由能量，以预测其渐近性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 增加深度对于接近贝叶斯最优性至关重要；2. GCN架构需要随着深度的变化进行调整以避免过度平滑；3. 大深度限制可以与贝叶斯最优性相近，并且导致连续GCN的形成；4. 通过类似于动态平均场理论的方法来处理连续极限，以及在大正则化下展开解对应于深层GCN性能方程。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个有前途的工具，用于进一步分析深度神经网络，并可能对未来的研究有所贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经以中文形式给出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are designed to process data associated withgraphs. They are finding an increasing range of applications; however, as withother modern machine learning techniques, their theoretical understanding islimited. GNNs can encounter difficulties in gathering information from nodesthat are far apart by iterated aggregation steps. This situation is partlycaused by so-called oversmoothing; and overcoming it is one of the practicallymotivated challenges. We consider the situation where information is aggregatedby multiple steps of convolution, leading to graph convolutional networks(GCNs). We analyze the generalization performance of a basic GCN, trained fornode classification on data generated by the contextual stochastic block model.We predict its asymptotic performance by deriving the free energy of theproblem, using the replica method, in the high-dimensional limit. Calling depththe number of convolutional steps, we show the importance of going to largedepth to approach the Bayes-optimality. We detail how the architecture of theGCN has to scale with the depth to avoid oversmoothing. The resulting largedepth limit can be close to the Bayes-optimality and leads to a continuous GCN.Technically, we tackle this continuous limit via an approach that resemblesdynamical mean-field theory (DMFT) with constraints at the initial and finaltimes. An expansion around large regularization allows us to solve thecorresponding equations for the performance of the deep GCN. This promisingtool may contribute to the analysis of further deep neural networks.</description>
      <author>example@mail.com (O. Duranthon, L. Zdeborová)</author>
      <guid isPermaLink="false">2503.01361v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Embeddings as a Behavior Specification Mechanism</title>
      <link>http://arxiv.org/abs/2503.02012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用嵌入式数学表示来正式描述依赖于感知模型与物理世界交互的系统的规范方法。&lt;h4&gt;背景&lt;/h4&gt;现有的系统规范语言难以精确表达基于感知模型的AI系统的复杂行为特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的逻辑形式，即嵌入时态逻辑(ETL)，用于在AI系统中更广泛地描述和验证行为属性。&lt;h4&gt;方法&lt;/h4&gt;引入了嵌入作为规范语言中的第一类构造，并通过距离度量理想与观察到的嵌入之间的差异来表达性质。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估表明，基于嵌入式的规范可以引导机器人等AI系统表现出期望的行为。&lt;h4&gt;结论&lt;/h4&gt;提出的ETL方法能够在感知驱动的任务中实现有效的行为控制和验证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种正式指定依赖于感知模型与物理世界交互的系统的操作属性的方法。关键思想是在规范语言中引入嵌入（现实概念的数学表示），其中性质以理想嵌入和观察到的嵌入之间的距离来表达。为了实现这种方法，我们提出了称为嵌入时态逻辑(ETL)的新类型的时间逻辑，并描述了如何使用它以前所未有的方式表达AI系统的一系列属性。通过涉及由基础模型驱动的机器人规划任务的初步评估，展示了ETL的应用潜力；结果很有前景，表明基于嵌入式的规范可以引导系统朝向期望的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an approach to formally specifying the behavioral properties ofsystems that rely on a perception model for interactions with the physicalworld. The key idea is to introduce embeddings -- mathematical representationsof a real-world concept -- as a first-class construct in a specificationlanguage, where properties are expressed in terms of distances between a pairof ideal and observed embeddings. To realize this approach, we propose a newtype of temporal logic called Embedding Temporal Logic (ETL), and describe howit can be used to express a wider range of properties about AI-enabled systemsthan previously possible. We demonstrate the applicability of ETL through apreliminary evaluation involving planning tasks in robots that are driven byfoundation models; the results are promising, showing that embedding-basedspecifications can be used to steer a system towards desirable behaviors.</description>
      <author>example@mail.com (Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang)</author>
      <guid isPermaLink="false">2503.02012v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了解开观测数据背后潜在变量的解耦表示学习，并分析两种不同的假设如何结合使用以提高可识别性。&lt;h4&gt;背景&lt;/h4&gt;解耦表示学习通常需要较强的先验假设来确保潜在变量的可识别性。一方面，某些方法依赖于辅助变量（如领域索引）所指示的潜在变量分布的变化；另一方面，一些方法利用混合过程中的结构稀疏性假设。&lt;h4&gt;目的&lt;/h4&gt;提出一个在较少限制条件下实现潜在变量可识别性的理论，并开发一种包含领域编码网络和稀疏混合约束的估计框架。&lt;h4&gt;方法&lt;/h4&gt;当以辅助变量为条件时，该研究提出了基于稀疏混合过程假定提供从估算到真实潜在变量映射结构约束的方法，并利用变分自编码器(VAE)和生成对抗网络(GAN)实现此理论。&lt;h4&gt;主要发现&lt;/h4&gt;两种看似不相关的假设可以互补使用来提高可识别性；当以辅助变量为条件时，稀疏混合过程假定提供了从估算到真实潜在变量映射的结构约束，并弥补了可能不足的分布变化。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在合成数据集和实际世界数据集上的结果支持该理论。此方法增强了在现实场景中的应用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了解耦表示学习的目标是解开观测数据背后的潜在变量，同时指出两种不同的假设（基于辅助变量的分布变化与稀疏混合过程）如何结合使用可以提高可识别性，并介绍了一种新的估计框架及其实施方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Disentangled representation learning aims to uncover latent variablesunderlying the observed data, and generally speaking, rather strong assumptionsare needed to ensure identifiability. Some approaches rely on sufficientchanges on the distribution of latent variables indicated by auxiliaryvariables such as domain indices, but acquiring enough domains is oftenchallenging. Alternative approaches exploit structural sparsity assumptions onthe mixing procedure, but such constraints are usually (partially) violated inpractice. Interestingly, we find that these two seemingly unrelated assumptionscan actually complement each other to achieve identifiability. Specifically,when conditioned on auxiliary variables, the sparse mixing procedure assumptionprovides structural constraints on the mapping from estimated to true latentvariables and hence compensates for potentially insufficient distributionchanges. Building on this insight, we propose an identifiability theory withless restrictive constraints regarding distribution changes and the sparsemixing procedure, enhancing applicability to real-world scenarios.Additionally, we develop an estimation framework incorporating a domainencoding network and a sparse mixing constraint and provide two implementationsbased on variational autoencoders and generative adversarial networks,respectively. Experiment results on synthetic and real-world datasets supportour theoretical results.</description>
      <author>example@mail.com (Zijian Li, Shunxing Fan, Yujia Zheng, Ignavier Ng, Shaoan Xie, Guangyi Chen, Xinshuai Dong, Ruichu Cai, Kun Zhang)</author>
      <guid isPermaLink="false">2503.00639v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention</title>
      <link>http://arxiv.org/abs/2503.01284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合MobileNetV2和GraphSAGE的混合Sequential CNN-图神经网络框架，用于大豆叶片疾病的检测。该方法在识别细微病变特征的同时还能捕捉全局症状模式，并通过可视化技术提高了模型的解释性。&lt;h4&gt;背景&lt;/h4&gt;大豆叶片疾病检测是农业生产力的关键问题，但因其视觉相似的症状和传统方法可解释性差而面临挑战。虽然卷积神经网络（CNN）擅长空间特性提取，但由于忽视了图像间的依赖关系而导致误分类情况较多。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合Sequential CNN-图神经网络框架，旨在提高大豆叶片疾病检测的准确性并增强模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;该研究结合MobileNetV2进行局部特征提取和GraphSAGE建模图像间的关系。采用基于余弦相似性的邻接矩阵构建图形，节点代表叶子图像，并通过自适应邻居采样定义边。利用Grad-CAM和Eigen-CAM技术提供跨模式可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在包含十种大豆叶片疾病的数据库中实现了97.16%的准确性，超过了单独使用的CNN（≤95.04%）和传统的机器学习方法（≤77.05%）。此外，该研究通过消融实验验证了序贯架构相对于并行或单一模型配置的优势。&lt;h4&gt;结论&lt;/h4&gt;提出的框架提供了一种轻量级且高效的解决方案，不仅在大豆叶片疾病检测方面具有高精度，而且在资源受限的环境中可以实现实时部署。这为植物病理学领域的CNN-GNN集成研究开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;大豆叶片病害检测对农业生产力至关重要，但视觉相似的症状和传统方法解释性差给其带来了挑战。尽管卷积神经网络（CNN）在空间特征提取方面表现出色，但由于忽略了图像间的关系依赖性而容易导致误分类。本文提出了一种可解释的混合Sequential CNN-图神经网络框架，该框架结合了MobileNetV2进行局部特征提取和GraphSAGE建模关系的方法。该架构构建了一个图形，其中节点表示叶子图片，并通过基于余弦相似性的邻接矩阵定义边和自适应邻居采样来捕获细粒度病变特性和整体症状模式，解决了类内相似性问题。跨模式可解释性是通过Grad-CAM和Eigen-CAM可视化技术实现的，生成热图突出显示影响疾病的区域。在包含十种大豆叶片病害的数据集上进行评估，该模型实现了97.16%的准确性，超过了单独的CNN（≤95.04%）和传统机器学习方法（≤77.05%）。消融研究验证了序贯架构相对于并行或单一模型配置的优势。利用轻量级MobileNetV2-GraphSAGE组合的仅2.3百万参数，确保计算效率，能够实现实时部署于资源受限环境中。所提出的方案在准确分类和实际应用之间架起了桥梁，为农业诊断提供了一种稳健、可解释的强大工具，并推进了植物病理学领域CNN-GNN集成研究的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soybean leaf disease detection is critical for agricultural productivity butfaces challenges due to visually similar symptoms and limited interpretabilityin conventional methods. While Convolutional Neural Networks (CNNs) excel inspatial feature extraction, they often neglect inter-image relationaldependencies, leading to misclassifications. This paper proposes aninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework thatsynergizes MobileNetV2 for localized feature extraction and GraphSAGE forrelational modeling. The framework constructs a graph where nodes representleaf images, with edges defined by cosine similarity-based adjacency matricesand adaptive neighborhood sampling. This design captures fine-grained lesionfeatures and global symptom patterns, addressing inter-class similaritychallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAMvisualizations, generating heatmaps to highlight disease-influential regions.Evaluated on a dataset of ten soybean leaf diseases, the model achieves$97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditionalmachine learning models ($\le77.05\%$). Ablation studies validate thesequential architecture's superiority over parallel or single-modelconfigurations. With only 2.3 million parameters, the lightweightMobileNetV2-GraphSAGE combination ensures computational efficiency, enablingreal-time deployment in resource-constrained environments. The proposedapproach bridges the gap between accurate classification and practicalapplicability, offering a robust, interpretable tool for agriculturaldiagnostics while advancing CNN-GNN integration in plant pathology research.</description>
      <author>example@mail.com (Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Nilanjan Dey)</author>
      <guid isPermaLink="false">2503.01284v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Proportionality in Thumbs Up and Down Voting</title>
      <link>http://arxiv.org/abs/2503.01985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文探讨了在包含正负偏好表达的决策环境中，如何理解比例性原则。&lt;h4&gt;背景&lt;/h4&gt;当前，在宪法人工智能中，公民民主地选择一整套伦理准则来训练基础模型。实践中，人们可能会对不同的伦理原则既表示赞同也表示反对。&lt;h4&gt;目的&lt;/h4&gt;提出两种概念上不同的方法来解释在存在上下投票的情况下比例性的含义。&lt;h4&gt;方法&lt;/h4&gt;第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。该研究形式化了每个视角下的公理，并通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来考察它们的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种解释负偏好情况的比例性原则的方法，并为每种方法建立了理论依据，探讨了这些比例性保证在实际决策中的应用和可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究深化了对包含正负面表达的选举系统中比例性的理解，并为进一步的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;考虑决策设置，在这种情况下代理者通过表示正面和负面偏好来选出一个小组。特别是，在宪法AI中，公民民主地选择一套伦理准则用于训练基础模型。在实践中，人们可能对不同的伦理原则既有赞成也有反对的意见。比例性已经在计算社会选择理论中的同意票上得到了很好的研究，但在考虑负面情绪时其意义仍然不清楚。本文提出了两种概念上截然不同的方法来解释在存在上下投票的情况下比例性的含义。第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。我们为每个视角形式化了公理并考察了它们的一致性，通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Consider the decision-making setting where agents elect a panel by expressingboth positive and negative preferences. Prominently, in constitutional AI,citizens democratically select a slate of ethical preferences on which afoundation model is to be trained. There, in practice, agents may both approveand disapprove of different ethical principles. Proportionality has beenwell-studied in computational social choice for approval ballots, but itsmeaning remains unclear when negative sentiments are also considered. In thiswork, we propose two conceptually distinct approaches to interpretproportionality in the presence of up and down votes. The first approach treatsthe satisfaction from electing candidates and the impact of vetoing them ascomparable, leading to combined proportionality guarantees. The second approachconsiders veto power separately, introducing guarantees distinct fromtraditional proportionality. We formalize axioms for each perspective andexamine their satisfiability by suitable adaptations of Phragm\'en's rule,Proportional Approval Voting rule and the Method of Equal Shares.</description>
      <author>example@mail.com (Sonja Kraiczy, Georgios Papasotiropoulos, Grzegorz Pierczyński, Piotr Skowron)</author>
      <guid isPermaLink="false">2503.01985v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</title>
      <link>http://arxiv.org/abs/2503.00881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了CarGS，一种统一模型框架，用于同时实现高质量渲染和表面重建。&lt;h4&gt;背景&lt;/h4&gt;三维场景表示是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的3D高斯点云（3DGaussian Splatting, 3DGS）因能够提供高品质渲染且推理速度较快而备受关注，但其在精确几何重建方面仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法解决现有技术中渲染和重构之间的内在冲突以及计算密集型和存储成本高的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建。该框架学习高斯原语（Gaussian primitives）的自适应贡献，并将几何正则化的知识整合到紧凑多层感知器中。此外，还引入了一种基于几何引导的密集化策略，利用法线和符号距离字段来捕捉高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在渲染保真度与重建准确性方面达到了最先进的性能，同时保证了实时速度和最小存储空间需求。&lt;h4&gt;结论&lt;/h4&gt;CarGS模型通过贡献自适应正则化解决了3D高斯点云在几何精确性上的难题，并且其统一结构不需要像双模态方法那样使用多个独立的模型，从而保证了效率。&lt;h4&gt;翻译&lt;/h4&gt;表示三维场景是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的方法如3D高斯点云(3DGaussian Splatting, 3DGS)虽然能够提供高质量渲染且推理速度较快，但在精确几何重建方面仍存在困难。为解决这一问题，提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建，并且在保持实时性能的同时实现了最小化的存储需求和最佳的几何保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representing 3D scenes from multiview images is a core challenge in computervision and graphics, which requires both precise rendering and accuratereconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significantattention for its high-quality rendering and fast inference speed. Yet, due tothe unstructured and irregular nature of Gaussian point clouds, ensuringaccurate geometry reconstruction remains difficult. Existing methods primarilyfocus on geometry regularization, with common approaches includingprimitive-based and dual-model frameworks. However, the former suffers frominherent conflicts between rendering and reconstruction, while the latter iscomputationally and storage-intensive. To address these challenges, we proposeCarGS, a unified model leveraging Contribution-adaptive regularization toachieve simultaneous, high-quality rendering and surface reconstruction. Theessence of our framework is learning adaptive contribution for Gaussianprimitives by squeezing the knowledge from geometry regularization into acompact MLP. Additionally, we introduce a geometry-guided densificationstrategy with clues from both normals and Signed Distance Fields (SDF) toimprove the capability of capturing high-frequency details. Our design improvesthe mutual learning of the two tasks, meanwhile its unified structure does notrequire separate models as in dual-model based approaches, guaranteeingefficiency. Extensive experiments demonstrate the ability to achievestate-of-the-art (SOTA) results in both rendering fidelity and reconstructionaccuracy while maintaining real-time speed and minimal storage size.</description>
      <author>example@mail.com (You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao)</author>
      <guid isPermaLink="false">2503.00881v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Channel-Attentive Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE International Conference on  Data Mining 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;图神经网络（GNNs）在处理图结构数据的表示学习中处于领先地位，被广泛应用于在线社交网络和复杂分子等各个领域。大多数GNN采用消息传递机制，并且在各种任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;尽管大多数GNN通过消息传递实现了出色性能，但随着模型深度增加，普遍存在过度平滑的问题，导致节点表示之间的相似性增加，进而影响了GNN的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应信道级消息传递方法以缓解过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Channel-Attentive GNN的模型，该模型能够学习如何关注邻近节点及其特征通道，在进行消息传递时可以交换更多种类的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型比基准模型更能抵抗过度平滑，并且在各种具有强烈异构性的图上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过改进GNN的消息传递机制，该研究提出了一种能够有效缓解过平滑问题的方法，并展示了其对复杂图形数据表示的优越性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) set the state-of-the-art in representation learning for graph-structured data. They are used in many domains, from online social networks to complex molecules. Most GNNs leverage the message-passing paradigm and achieve strong performances on various tasks. However, the message-passing mechanism used in most models suffers from over-smoothing as a GNN's depth increases. The over-smoothing degrades GNN's performance due to the increased similarity between the representations of unrelated nodes. This study proposes an adaptive channel-wise message-passing approach to alleviate the over-smoothing. The proposed model, Channel-Attentive GNN, learns how to attend to neighboring nodes and their feature channels. Thus, much diverse information can be transferred between nodes during message-passing. Experiments with widely used benchmark datasets show that the proposed model is more resistant to over-smoothing than baselines and achieves state-of-the-art performances for various graphs with strong heterophily.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM59182.2024.00084&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/allab-boun/chat-gnn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) set the state-of-the-art in representationlearning for graph-structured data. They are used in many domains, from onlinesocial networks to complex molecules. Most GNNs leverage the message-passingparadigm and achieve strong performances on various tasks. However, themessage-passing mechanism used in most models suffers from over-smoothing as aGNN's depth increases. The over-smoothing degrades GNN's performance due to theincreased similarity between the representations of unrelated nodes. This studyproposes an adaptive channel-wise message-passing approach to alleviate theover-smoothing. The proposed model, Channel-Attentive GNN, learns how to attendto neighboring nodes and their feature channels. Thus, much diverse informationcan be transferred between nodes during message-passing. Experiments withwidely used benchmark datasets show that the proposed model is more resistantto over-smoothing than baselines and achieves state-of-the-art performances forvarious graphs with strong heterophily. Our code is athttps://github.com/ALLab-Boun/CHAT-GNN.</description>
      <author>example@mail.com (Tuğrul Hasan Karabulut, İnci M. Baytaş)</author>
      <guid isPermaLink="false">2503.00578v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts</title>
      <link>http://arxiv.org/abs/2503.01947v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been submitted to IEEE Transactions on Artificial  Intelligence for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，大型语言模型由于其显著的潜力吸引了越来越多的关注，尽管人们对由此产生的不安全行为引发了担忧。这些不安全行为源自固有的刻板印象和偏见。&lt;h4&gt;背景&lt;/h4&gt;大多数关于LLM（大型语言模型）中的刻板印象的研究主要依赖于间接评估方法，在这种方法中，让模型在与特定社会群体相关的成对句子之间进行选择。最近，直接评估方法出现，这些方法通过检查开放式的模型响应来克服以前方法的局限性，例如标注者偏见。&lt;h4&gt;目的&lt;/h4&gt;这项研究旨在探讨日语LLM（大型语言模型）在应对触发刻板印象提示时的安全性，并填补非英语尤其是日语模型研究领域的空白。&lt;h4&gt;方法&lt;/h4&gt;构建了3,612个触发刻板印象的提示，这些提示由301个社会群体术语和12种类型化模板组合而成。从三种不同语言基础训练的语言模型（日语、英语和中文）中分析响应。&lt;h4&gt;主要发现&lt;/h4&gt;日本本土模型LLM-jp在拒绝率最低的同时更可能生成有毒或负面的回应；提示格式对所有模型输出有显著影响，反应通常包含针对特定社会群体夸张化的内容且因模型而异。这些发现揭示了日语LLM中伦理安全机制的不足，并证明即使高准确性的模型也能在处理日语文本时产生偏见。&lt;h4&gt;结论&lt;/h4&gt;研究呼吁改进日语LLM中的安全措施和减少偏见策略，以促进AI伦理讨论超越语言边界，对于提升全球范围内的AI安全性具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Large Language Models have attracted growing interest fortheir significant potential, though concerns have rapidly emerged regardingunsafe behaviors stemming from inherent stereotypes and biases. Most researchon stereotypes in LLMs has primarily relied on indirect evaluation setups, inwhich models are prompted to select between pairs of sentences associated withparticular social groups. Recently, direct evaluation methods have emerged,examining open-ended model responses to overcome limitations of previousapproaches, such as annotator biases. Most existing studies have focused onEnglish-centric LLMs, whereas research on non-English models, particularlyJapanese, remains sparse, despite the growing development and adoption of thesemodels. This study examines the safety of Japanese LLMs when responding tostereotype-triggering prompts in direct setups. We constructed 3,612 prompts bycombining 301 social group terms, categorized by age, gender, and otherattributes, with 12 stereotype-inducing templates in Japanese. Responses wereanalyzed from three foundational models trained respectively on Japanese,English, and Chinese language. Our findings reveal that LLM-jp, a Japanesenative model, exhibits the lowest refusal rate and is more likely to generatetoxic and negative responses compared to other models. Additionally, promptformat significantly influence the output of all models, and the generatedresponses include exaggerated reactions toward specific social groups, varyingacross models. These findings underscore the insufficient ethical safetymechanisms in Japanese LLMs and demonstrate that even high-accuracy models canproduce biased outputs when processing Japanese-language prompts. We advocatefor improving safety mechanisms and bias mitigation strategies in JapaneseLLMs, contributing to ongoing discussions on AI ethics beyond linguisticboundaries.</description>
      <author>example@mail.com (Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri)</author>
      <guid isPermaLink="false">2503.01947v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery</title>
      <link>http://arxiv.org/abs/2503.00848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为PSRGS的渐进优化方案，解决了3D Gaussian Splatting在大规模遥感场景中遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的3D Gaussian Splatting方法在处理小规模单一物体场景时效果显著，但在处理大规模遥感数据时，由于点云稀疏和过度平滑问题而出现性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D Gaussian Splatting应用于大规模场景中的过度重建、错误几何位置导致的密度化以及梯度伪影等问题。&lt;h4&gt;方法&lt;/h4&gt;通过创建光谱残差显著图分离低频与高频区域，采用深度感知和平滑损失初始化低频区域，并利用更高阈值的梯度特征分裂和克隆椭圆体以优化高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;提出的PSRGS方案在多个数据集上的实验结果表明，在恢复高频率纹理细节方面具有竞争性的渲染质量。&lt;h4&gt;结论&lt;/h4&gt;通过分阶段地应用不同的优化策略，能够有效地提高大规模场景的3D Gaussian Splatting性能。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了针对大规模遥感场景中使用3D Gaussian Splatting遇到的问题（如点云稀疏和过度平滑），提出了一种基于光谱残差图进行渐进式优化的新方法PSRGS，成功提升了处理效果及细节恢复能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3D GS) achieves impressive results in novel viewsynthesis for small, single-object scenes through Gaussian ellipsoidinitialization and adaptive density control. However, when applied tolarge-scale remote sensing scenes, 3D GS faces challenges: the point cloudsgenerated by Structure-from-Motion (SfM) are often sparse, and the inherentsmoothing behavior of 3D GS leads to over-reconstruction in high-frequencyregions, where have detailed textures and color variations. This results in thegeneration of large, opaque Gaussian ellipsoids that cause gradient artifacts.Moreover, the simultaneous optimization of both geometry and texture may leadto densification of Gaussian ellipsoids at incorrect geometric locations,resulting in artifacts in other views. To address these issues, we proposePSRGS, a progressive optimization scheme based on spectral residual maps.Specifically, we create a spectral residual significance map to separatelow-frequency and high-frequency regions. In the low-frequency region, we applydepth-aware and depth-smooth losses to initialize the scene geometry with lowthreshold. For the high-frequency region, we use gradient features with higherthreshold to split and clone ellipsoids, refining the scene. The sampling rateis determined by feature responses and gradient loss. Finally, we introduce apre-trained network that jointly computes perceptual loss from multiple views,ensuring accurate restoration of high-frequency details in both Gaussianellipsoids geometry and color. We conduct experiments on multiple datasets toassess the effectiveness of our method, which demonstrates competitiverendering quality, especially in recovering texture details in high-frequencyregions.</description>
      <author>example@mail.com (BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang)</author>
      <guid isPermaLink="false">2503.00848v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language Model Mapping in Multimodal Music Learning: A Grand Challenge Proposal</title>
      <link>http://arxiv.org/abs/2503.00427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，在深度神经网络的帮助下，表示学习和语言模型取得了显著的成功。很多研究旨在通过词汇或嵌入级别上的对齐和映射来构建不同模式之间的内在联系。&lt;h4&gt;问题&lt;/h4&gt;然而，大多数方法非常依赖于大量数据的输入，这在音乐等领域表现不佳，因为这些领域中的成对数据较少。&lt;h4&gt;目的&lt;/h4&gt;作者认为，嵌入对齐仅是跨模态对齐的表面层次。本文提出了一种新的挑战“语言模型映射（LMM）”，即如何将一种模式下的语言模型的本质映射到另一种模式下的语言模型中，前提是假设不同模式的语言模型都在追踪相同的基本现象。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了一个关于LMM的基础设置，并强调了其目标是揭示跨模态对齐的深层次方面以及实现更高效的样本学习。然后探讨了音乐领域为何成为进行LMM研究的理想选择。&lt;h4&gt;进一步讨论&lt;/h4&gt;接着，将音乐中的LMM与一个更为广泛且具有挑战性的科学问题联系起来——即“基于感官输入和抽象符号的学习如何采取行动”。最后提出了一种先进版本的挑战性问题设置。&lt;h4&gt;结论&lt;/h4&gt;本文提出的语言模型映射概念为跨模态对齐研究提供了一个新的视角，尤其适用于数据稀缺或难以获取成对数据的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have seen remarkable success in representation learning and languagemodels (LMs) using deep neural networks. Many studies aim to build theunderlying connections among different modalities via the alignment andmappings at the token or embedding level, but so far, most methods are verydata-hungry, limiting their performance in domains such as music where paireddata are less abundant. We argue that the embedding alignment is only at thesurface level of multimodal alignment. In this paper, we propose a grandchallenge of \textit{language model mapping} (LMM), i.e., how to map theessence implied in the LM of one domain to the LM of another domain under theassumption that LMs of different modalities are tracking the same underlyingphenomena. We first introduce a basic setup of LMM, highlighting the goal tounveil a deeper aspect of cross-modal alignment as well as to achieve moresample-efficiency learning. We then discuss why music is an ideal domain inwhich to conduct LMM research. After that, we connect LMM in music with a moregeneral and challenging scientific problem of \textit{learning to take actionsbased on both sensory input and abstract symbols}, and in the end, present anadvanced version of the challenge problem setup.</description>
      <author>example@mail.com (Daniel Chin, Gus Xia)</author>
      <guid isPermaLink="false">2503.00427v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Open-source framework for detecting bias and overfitting for large pathology images</title>
      <link>http://arxiv.org/abs/2503.01827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种通用且模型无关的框架，用于调试深度学习模型中的捷径问题。&lt;h4&gt;背景信息&lt;/h4&gt;大型训练数据集也可能导致模型过度拟合和偏差，特别是非相关数据模式如背景颜色或色彩强度成为捷径。&lt;h4&gt;研究目的&lt;/h4&gt;开发一种能够检测并移除这些捷径的方法，以确保深度学习应用的稳健性。&lt;h4&gt;方法介绍&lt;/h4&gt;提出了一种无需针对特定模型架构定制的、通用且模型无关的调试框架。该框架特别适用于大规模图像数据处理领域（如病理学），并且能够在配备普通GPU的工作站上运行。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够复制在先前研究中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。&lt;h4&gt;结论&lt;/h4&gt;易于使用的测试有助于开发更可靠、准确和泛化的用于WSI分析的模型。此框架作为一个开源工具，可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;即使在使用数十亿数据样本进行训练的基础模型中也可能发展出导致过度拟合和偏差的捷径问题。这些捷径是非相关的数据模式，例如背景颜色或色彩强度等。为了确保深度学习应用的稳健性，需要检测并移除此类捷径的方法。当前的模型调试方法耗时较长，并且通常需要针对特定领域的给定模型架构进行定制化调整。我们提出了一种通用、模型无关的框架来调试深度学习模型，特别是在病理学领域，该领域涉及非常大的图像和庞大的计算资源需求。我们的框架能够在配备普通GPU的工作站上运行。我们展示了该框架可以复制先前工作中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。这些易用性测试有助于开发更可靠、准确和泛化的用于WSI分析的深度学习模型。我们的框架作为一个开源工具，可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Even foundational models that are trained on datasets with billions of datasamples may develop shortcuts that lead to overfitting and bias. Shortcuts arenon-relevant patterns in data, such as the background color or color intensity.So, to ensure the robustness of deep learning applications, there is a need formethods to detect and remove such shortcuts. Today's model debugging methodsare time consuming since they often require customization to fit for a givenmodel architecture in a specific domain. We propose a generalized,model-agnostic framework to debug deep learning models. We focus on the domainof histopathology, which has very large images that require large models - andtherefore large computation resources. It can be run on a workstation with acommodity GPU. We demonstrate that our framework can replicate non-imageshortcuts that have been found in previous work for self-supervised learningmodels, and we also identify possible shortcuts in a foundation model. Our easyto use tests contribute to the development of more reliable, accurate, andgeneralizable models for WSI analysis. Our framework is available as anopen-source tool available on github.</description>
      <author>example@mail.com (Anders Sildnes, Nikita Shvetsov, Masoud Tafavvoghi, Vi Ngoc-Nha Tran, Kajsa Møllersen, Lill-Tove Rasmussen Busund, Thomas K. Kilvær, Lars Ailo Bongo)</author>
      <guid isPermaLink="false">2503.01827v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HiMo: High-Speed Objects Motion Compensation in Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云数据常常包含由于运动引起的失真，这会降低捕获数据中物体外观的准确性。&lt;h4&gt;问题&lt;/h4&gt;当前的研究主要关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的去扭曲流水线（HiMo），用于解决由车辆及周围环境中的动态对象引起的点云失真。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于场景流估计的方法来补偿物体运动，并扩展了一个最先进的自我监督场景流方法。&lt;h4&gt;性能评估&lt;/h4&gt;鉴于现有文献中缺乏成熟可靠的运动失真度量标准，论文提出了两个新的评价指标：点级补偿精度和对象形状相似性。&lt;h4&gt;实验数据&lt;/h4&gt;在Argoverse 2数据集以及新收集的用于展示方法有效性的实际道路场景数据集上进行了广泛的实验。新数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;h4&gt;翻译&lt;/h4&gt;本文首先描述了点云失真的根本原因，并展示了这些失真在公共数据集中的存在。这种失真在高速公路等高速环境中更加明显，也出现在多LiDAR配置中——这是重型车辆的常见设置。之前的大多数研究只关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。因此，我们提出了一种新的去扭曲流水线（HiMo），该流水线利用场景流估计来进行物体运动补偿，从而纠正动态对象的表现。此外，还对一种最先进的自我监督场景流方法进行了扩展。由于文献中缺乏成熟的点云失真度量标准，本文提出了两个评估指标：点级补偿精度和形状相似性以评价去扭曲的性能。为了证明所提出方法的有效性，在Argoverse 2数据集以及新收集的数据集上进行了广泛的实验，该数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point clouds often contain motion-induced distortions, degrading theaccuracy of object appearances in the captured data. In this paper, we firstcharacterize the underlying reasons for the point cloud distortion and showthat this is present in public datasets. We find that this distortion is morepronounced in high-speed environments such as highways, as well as inmulti-LiDAR configurations, a common setup for heavy vehicles. Previous workhas dealt with point cloud distortion from the ego-motion but fails to considerdistortion from the motion of other objects. We therefore introduce a novelundistortion pipeline, HiMo, that leverages scene flow estimation for objectmotion compensation, correcting the depiction of dynamic objects. We furtherpropose an extension of a state-of-the-art self-supervised scene flow method.Due to the lack of well-established motion distortion metrics in theliterature, we also propose two metrics for compensation performanceevaluation: compensation accuracy at a point level and shape similarity onobjects. To demonstrate the efficacy of our method, we conduct extensiveexperiments on the Argoverse 2 dataset and a new real-world dataset. Our newdataset is collected from heavy vehicles equipped with multi-LiDARs and onhighways as opposed to mostly urban settings in the existing datasets. Thesource code, including all methods and the evaluation data, will be providedupon publication. See https://kin-zhang.github.io/HiMo for more details.</description>
      <author>example@mail.com (Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt)</author>
      <guid isPermaLink="false">2503.00803v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</title>
      <link>http://arxiv.org/abs/2503.00383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了基于混淆的方法在保护中间特征隐私方面存在的不足，并提出了一种新的方法来量化冗余并增强模型对抗逆向工程攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;协作推理使终端用户能够利用强大的深度学习模型而不暴露敏感的原始数据给云服务器。然而，最近的研究表明，中间特征可能不足以保证隐私，因为通过逆向建模攻击可以泄露信息和重建原始数据。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提出一种方法来量化冗余，并建立它与增强逆向工程抵抗能力之间的数学关系。&lt;h4&gt;方法&lt;/h4&gt;论文证明了输入给定中间特征的条件熵提供了在任何逆向建模攻击下的重构均方误差（MSE）的一个保证下界。然后，基于高斯混合估计提出了一个可微分且可解的方法来界定这个条件熵，并提出了一种条件熵最大化（CEM）算法以增强逆向工程抵抗能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了所提出的CEM在四个数据集上的有效性和适应性。将CEM嵌入到基于混淆的防御机制中，在不牺牲特征实用性和计算效率的情况下，可以显著提升其逆向工程抵抗能力，平均增益范围从12.9%至48.2%。&lt;h4&gt;结论&lt;/h4&gt;该研究通过理论分析和实验验证展示了如何利用条件熵最大化来增强模型对抗逆向建模攻击的能力。所提出的CEM方法为保护协作推理框架中的隐私提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了研究背景、目的、方法及结果，揭示了一种新的技术——条件熵最大化（CEM），用于提高中间特征的抗逆向工程能力，从而进一步提升在深度学习模型中使用协作推理的安全性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By locally encoding raw data into intermediate features, collaborativeinference enables end users to leverage powerful deep learning models withoutexposure of sensitive raw data to cloud servers. However, recent studies haverevealed that these intermediate features may not sufficiently preserveprivacy, as information can be leaked and raw data can be reconstructed viamodel inversion attacks (MIAs). Obfuscation-based methods, such as noisecorruption, adversarial representation learning, and information filters,enhance the inversion robustness by obfuscating the task-irrelevant redundancyempirically. However, methods for quantifying such redundancy remain elusive,and the explicit mathematical relation between this redundancy minimization andinversion robustness enhancement has not yet been established. To address that,this work first theoretically proves that the conditional entropy of inputsgiven intermediate features provides a guaranteed lower bound on thereconstruction mean square error (MSE) under any MIA. Then, we derive adifferentiable and solvable measure for bounding this conditional entropy basedon the Gaussian mixture estimation and propose a conditional entropymaximization (CEM) algorithm to enhance the inversion robustness. Experimentalresults on four datasets demonstrate the effectiveness and adaptability of ourproposed CEM; without compromising feature utility and computing efficiency,plugging the proposed CEM into obfuscation-based defense mechanismsconsistently boosts their inversion robustness, achieving average gains rangingfrom 12.9\% to 48.2\%. Code is available at\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.</description>
      <author>example@mail.com (Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Lingyu Duan, Alex C. Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2503.00383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SAKE: Steering Activations for Knowledge Editing</title>
      <link>http://arxiv.org/abs/2503.01751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的知识编辑方法SAKE，该方法通过将需要修改的事实建模为分布而非单一提示来提高语言模型的知识更新效率和效果。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型显示出记忆真实世界事实的能力，对这些模型进行可控且高效的知识更新变得越来越重要。然而，现有的知识编辑方法存在缺乏上下文鲁棒性等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的知识编辑方法SAKE，旨在解决现有知识编辑方法中的局限性和问题。&lt;h4&gt;方法&lt;/h4&gt;SAKE使用激活引导的方法，将要修改的事实建模为一个分布，并利用最优传输技术来调整大型语言模型的行为以应对整个事实相关的分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列数值实验，证实了SAKE在执行更稳健的知识编辑方面的有效性。它能够比现有的方法更好地处理上下文变化和逻辑推论问题。&lt;h4&gt;结论&lt;/h4&gt;SAKE是一种改进的知识编辑技术，能够在大型语言模型中实现更加有效和鲁棒的知识更新。&lt;h4&gt;翻译&lt;/h4&gt;随着大规模语言模型展现出记住现实世界事实的能力，有必要以一种受控且高效的方式对其进行知识更新。考虑到这些限制，知识编辑方法被提出用于修改预训练模型中的特定事实。然而，它们显示出缺乏上下文稳健性等若干局限，并且无法泛化到与事实相关的逻辑推论。为克服这些问题，我们提出了SAKE（基于激活引导的方法），该方法将要被修改的事实建模为一个分布而非单一提示。利用最优传输技术，SAKE能够调整大型语言模型在涉及整个事实相关分布时的行为，包括同义词和逻辑推论。通过多种数值实验表明了此方法的有效性：因此，与现有的方法相比，SAKE可以执行更稳健的知识编辑操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Langue Models have been shown to memorize real-world facts, the needto update this knowledge in a controlled and efficient manner arises. Designedwith these constraints in mind, Knowledge Editing (KE) approaches propose toalter specific facts in pretrained models. However, they have been shown tosuffer from several limitations, including their lack of contextual robustnessand their failure to generalize to logical implications related to the fact. Toovercome these issues, we propose SAKE, a steering activation method thatmodels a fact to be edited as a distribution rather than a single prompt.Leveraging Optimal Transport, SAKE alters the LLM behavior over a wholefact-related distribution, defined as paraphrases and logical implications.Several numerical experiments demonstrate the effectiveness of this method:SAKE is thus able to perform more robust edits than its existing counterparts.</description>
      <author>example@mail.com (Marco Scialanga, Thibault Laugel, Vincent Grari, Marcin Detyniecki)</author>
      <guid isPermaLink="false">2503.01751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STAR-Edge是一种用于检测和细化薄壁结构中边缘点的新颖方法，通过独特的局部球面曲线表示创建结构感知的邻域。&lt;h4&gt;背景&lt;/h4&gt;从非结构化点云中提取几何边缘仍然是一个重大挑战，尤其是在常见的日常物体中的薄壁结构。传统的几何方法和最近的学习基于的方法在处理这些结构时经常遇到困难，因为两者都严重依赖于局部点邻居提供的足够的上下文信息。然而，薄壁结构的3D测量数据通常缺乏可靠边沿抽取所需的精确、密集且规则的邻域采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法STAR-Edge，旨在检测和细化薄壁结构中的边缘点，并通过独特的表示方式提高其在噪声和稀疏或不规则采样下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用局部球面曲线表示创建关注结构的邻域，强调共面点同时减少来自附近非共平面表面的干扰。此表示被转换为旋转不变量描述符，并结合轻量级多层感知器用于边缘点分类。此外，利用局部球面曲线表示估计更精确的法线并向初步确定的边缘点引入优化函数进行投影。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STAR-Edge在ABC数据集和薄壁结构特定的数据集中均优于现有的边缘检测方法，在各种挑战性条件下展示出更好的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;STAR-Edge提供了一种新的解决方案，能够在复杂且具有挑战性的环境中准确地提取薄壁结构中的几何边沿。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracting geometric edges from unstructured point clouds remains asignificant challenge, particularly in thin-walled structures that are commonlyfound in everyday objects. Traditional geometric methods and recentlearning-based approaches frequently struggle with these structures, as bothrely heavily on sufficient contextual information from local pointneighborhoods. However, 3D measurement data of thin-walled structures oftenlack the accurate, dense, and regular neighborhood sampling required forreliable edge extraction, resulting in degraded performance.  In this work, we introduce STAR-Edge, a novel approach designed for detectingand refining edge points in thin-walled structures. Our method leverages aunique representation-the local spherical curve-to create structure-awareneighborhoods that emphasize co-planar points while reducing interference fromclose-by, non-co-planar surfaces. This representation is transformed into arotation-invariant descriptor, which, combined with a lightweight multi-layerperceptron, enables robust edge point classification even in the presence ofnoise and sparse or irregular sampling. Besides, we also use the localspherical curve representation to estimate more precise normals and introducean optimization function to project initially identified edge points exactly onthe true edges. Experiments conducted on the ABC dataset and thin-walledstructure-specific datasets demonstrate that STAR-Edge outperforms existingedge detection methods, showcasing better robustness under various challengingconditions.</description>
      <author>example@mail.com (Zikuan Li, Honghua Chen, Yuecheng Wang, Sibo Wu, Mingqiang Wei, Jun Wang)</author>
      <guid isPermaLink="false">2503.00801v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ECG-EmotionNet: Nested Mixture of Expert (NMoE) Adaptation of ECG-Foundation Model for Driver Emotion Recognition</title>
      <link>http://arxiv.org/abs/2503.01750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的架构ECG-EmotionNet，用于在动态驾驶环境中进行驾驶员情绪识别。&lt;h4&gt;背景&lt;/h4&gt;驾驶员情绪识别对自动驾驶系统中的人员自主互动和信任度提升至关重要。心电图(ECG)因其实时监测能力和适应复杂驾驶环境的能力而成为最佳选择之一。然而，现有的方法通常依赖于静态条件下的多通道ECG信号，限制了其在真实动态场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的架构ECG-EmotionNet，以提高驾驶员情绪识别的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用单通道心电图(ECG)信号构建一个基于最近引入的心电图基础模型(FM)的新架构。通过嵌入式专家混合(MoE)适应机制来增强全局和局部ECG特征表示，而不是采用传统的全微调、线性探测或低秩适应方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和F1得分上分别提高了6%和7%，并且保持了计算效率。此外，使用最近引入的具有挑战性的驾驶员情绪监测数据集进行了评估。&lt;h4&gt;结论&lt;/h4&gt;ECG-EmotionNet架构显著改善了动态驾驶环境下的心电图情绪识别性能，为自动驾驶系统中的人机互动提供了坚实的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文提及司机情感识别在监控系统中的重要性，并介绍了新的架构ECG-EmotionNet及其使用单通道心电图信号来增强驾驶员情绪监测的准确性和鲁棒性的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver emotion recognition plays a crucial role in driver monitoring systems,enhancing human-autonomy interactions and the trustworthiness of AutonomousDriving (AD). Various physiological and behavioural modalities have beenexplored for this purpose, with Electrocardiogram (ECG) emerging as a standoutchoice for real-time emotion monitoring, particularly in dynamic andunpredictable driving conditions. Existing methods, however, often rely onmulti-channel ECG signals recorded under static conditions, limiting theirapplicability in real-world dynamic driving scenarios. To address thislimitation, the paper introduces ECG-EmotionNet, a novel architecture designedspecifically for emotion recognition in dynamic driving environments.ECG-EmotionNet is constructed by adapting a recently introduced ECG FoundationModel (FM) and uniquely employs single-channel ECG signals, ensuring bothrobust generalizability and computational efficiency. Unlike conventionaladaptation methods such as full fine-tuning, linear probing, or low-rankadaptation, we propose an intuitively pleasing alternative, referred to as thenested Mixture of Experts (MoE) adaptation. More precisely, each transformerlayer of the underlying FM is treated as a separate expert, with embeddingsextracted from these experts fused using trainable weights within a gatingmechanism. This approach enhances the representation of both global and localECG features, leading to a 6% improvement in accuracy and a 7% increase in theF1 score, all while maintaining computational efficiency. The effectiveness ofthe proposed ECG-EmotionNet architecture is evaluated using a recentlyintroduced and challenging driver emotion monitoring dataset.</description>
      <author>example@mail.com (Nastaran Mansourian, Arash Mohammadi, M. Omair Ahmad, M. N. S. Swamy)</author>
      <guid isPermaLink="false">2503.01750v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ICanC: Improving Camera-based Object Detection and Energy Consumption in Low-Illumination Environments</title>
      <link>http://arxiv.org/abs/2503.00709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 18 figures, to be published in IEEE MOST 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种名为ICanC的新系统，旨在通过利用激光雷达和摄像头传感器的互补能力来增强自动驾驶汽车在低光照环境下的物体检测能力和能效。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶车辆在低光照环境下容易因相机性能下降而导致物体检测准确性降低，并且需要频繁使用前照灯以提高摄像机的可视性，这导致能源消耗增加。因此，有必要寻找一种既能确保可靠物体检测又能优化能耗的方法。&lt;h4&gt;目的&lt;/h4&gt;通过设计ICanC系统，在不牺牲可靠性的前提下减少不必要的头灯使用，从而实现更可持续的交通方式。&lt;h4&gt;方法&lt;/h4&gt;ICanC由三个主要部分组成：障碍物探测器、危险探测器和灯光控制器。其中，障碍物探测器处理激光雷达点云数据以拟合边界框并估计物体的位置、速度和方向；危险探测器评估潜在威胁；灯光控制器根据危险探测的结果动态开启前照灯。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在有显著噪声干扰的情况下，ICanC仍然表现出良好的性能，并且当开启头灯时能够实现基于摄像头的高精度物体检测。同时，整体能耗得到了显著降低。&lt;h4&gt;结论&lt;/h4&gt;作为自动驾驶汽车研究中的一个重要的进展，ICanC在确保可靠性和能效之间达到了一种平衡，展示了其在未来可持续交通中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种名为 ICanC（发音为“我能看见”）的新型系统，旨在通过结合激光雷达和相机传感器的优势来提高自主车辆在低光照条件下的物体检测能力，并优化能耗。此方法确保了在传统相机性能下降的情况下也能提升检测精度并大幅降低不必要的前照灯使用，从而支持可持续交通的目标。ICanC系统包括三个主要组成部分：障碍物探测器、危险探测器和灯光控制器。通过实验验证，在真实和模拟环境中，即使存在显著的噪声干扰，该系统依然保持了出色的性能，展示了在实现可靠物体检测的同时大幅减少前照灯能耗的巨大潜力，这是自动驾驶车辆研究领域的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces ICanC (pronounced "I Can See"), a novel system designedto enhance object detection and optimize energy efficiency in autonomousvehicles (AVs) operating in low-illumination environments. By leveraging thecomplementary capabilities of LiDAR and camera sensors, ICanC improvesdetection accuracy under conditions where camera performance typicallydeclines, while significantly reducing unnecessary headlight usage. Thisapproach aligns with the broader objective of promoting sustainabletransportation.  ICanC comprises three primary nodes: the Obstacle Detector, which processesLiDAR point cloud data to fit bounding boxes onto detected objects and estimatetheir position, velocity, and orientation; the Danger Detector, which evaluatespotential threats using the information provided by the Obstacle Detector; andthe Light Controller, which dynamically activates headlights to enhance cameravisibility solely when a threat is detected.  Experiments conducted in physical and simulated environments demonstrateICanC's robust performance, even in the presence of significant noiseinterference. The system consistently achieves high accuracy in camera-basedobject detection when headlights are engaged, while significantly reducingoverall headlight energy consumption. These results position ICanC as apromising advancement in autonomous vehicle research, achieving a balancebetween energy efficiency and reliable object detection.</description>
      <author>example@mail.com (Daniel Ma, Ren Zhong, Weisong Shi)</author>
      <guid isPermaLink="false">2503.00709v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
      <link>http://arxiv.org/abs/2503.01710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的大规模语言模型（LLMs）在零样本文本到语音（TTS）合成方面取得了显著进展。然而，现有的基础模型依赖于多阶段处理或复杂的架构来预测多个码本，这限制了效率和集成灵活性。&lt;h4&gt;背景&lt;/h4&gt;当前的零样本TTS系统主要面临两大挑战：一是复杂且低效的架构，二是受限的控制能力，难以实现精确的语言内容与说话人属性分离。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的局限性，本文介绍了Spark-TTS系统，旨在通过引入新的单流语音编解码器BiCodec来提升TTS模型的效率和灵活性，并支持精细的音调调整。&lt;h4&gt;方法&lt;/h4&gt;Spark-TTS基于BiCodec，该编码器将语音分解为两种互补的标记类型：用于语言内容的低比特率语义令牌和固定长度的全局令牌（代表说话人属性）。此外，该系统还采用了Qwen2.5 LLM以及一种称为“链式思维”（CoT）的生成方法。&lt;h4&gt;主要发现&lt;/h4&gt;Spark-TTS不仅实现了最先进的零样本语音克隆效果，还能生成高度可定制的声音，超越了基于参考的合成的限制。为促进可控性TTS研究，本文还引入了一个精心策划的数据集VoxBox，它包含10万小时录音及全面属性标注。&lt;h4&gt;结论&lt;/h4&gt;Spark-TTS展示了在效率、灵活性和控制精确度方面的显著改进，并通过详细的实验验证了其优越性能。&lt;h4&gt;翻译&lt;/h4&gt;近期大规模语言模型(LLMs)的重大进展促进了零样本文本到语音(TTS)合成技术的显著进步。然而，现有的基础模型依赖于多阶段处理或复杂架构来预测多个码本，从而限制了效率和集成灵活性。为解决这一问题，我们引入了Spark-TTS系统，该系统由BiCodec驱动，这是一种单流语音编解码器，将语音分解成两种互补的标记类型：用于语言内容的低比特率语义标记以及固定长度的全局标记(代表说话人属性)。结合Qwen2.5 LLM和链式思维(CoT)生成方法，Spark-TTS能够实现粗粒度控制（如性别、讲话风格）及细粒度调整（如精确音高值、讲话速率）。为了促进可控性TTS研究，我们引入了VoxBox数据集，这是一个精心策划的10万小时录音库，包含全面的属性标注。大量实验表明，Spark-TTS不仅在零样本语音克隆方面达到业界领先水平，并且生成的高度定制化声音超出了基于参考合成技术的限制。该系统的源代码、预训练模型和音频样本可从https://github.com/SparkAudio/Spark-TTS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large language models (LLMs) have driven significantprogress in zero-shot text-to-speech (TTS) synthesis. However, existingfoundation models rely on multi-stage processing or complex architectures forpredicting multiple codebooks, limiting efficiency and integration flexibility.To overcome these challenges, we introduce Spark-TTS, a novel system powered byBiCodec, a single-stream speech codec that decomposes speech into twocomplementary token types: low-bitrate semantic tokens for linguistic contentand fixed-length global tokens for speaker attributes. This disentangledrepresentation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT)generation approach, enables both coarse-grained control (e.g., gender,speaking style) and fine-grained adjustments (e.g., precise pitch values,speaking rate). To facilitate research in controllable TTS, we introduceVoxBox, a meticulously curated 100,000-hour dataset with comprehensiveattribute annotations. Extensive experiments demonstrate that Spark-TTS notonly achieves state-of-the-art zero-shot voice cloning but also generateshighly customizable voices that surpass the limitations of reference-basedsynthesis. Source code, pre-trained models, and audio samples are available athttps://github.com/SparkAudio/Spark-TTS.</description>
      <author>example@mail.com (Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, Yunlin Chen, Zhifei Li, Xie Chen, Lei Xie, Yike Guo, Wei Xue)</author>
      <guid isPermaLink="false">2503.01710v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation</title>
      <link>http://arxiv.org/abs/2503.01700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;最近的工作展示了大型语言模型在机器人任务和运动规划中的巨大潜力。然而，当前的方法没有充分利用这些模型的符号计算能力和代码生成能力。&lt;h4&gt;背景&lt;/h4&gt;现有的LLM方法通常产生包含子目标和行动计划的文本或代码推理链。对于涉及多重约束下的复杂优化问题的任务，纯文本推理显得不足。&lt;h4&gt;目的&lt;/h4&gt;为了提高LLM在任务和运动规划中的性能并增强其泛化性，我们提出了一种通过指导模型生成符号计算所需的代码来改进TAMP能力的方法。&lt;h4&gt;方法&lt;/h4&gt;与以往的工作不同，我们的工作使LLM生成用于解决、计划和验证的代码，同时利用文本推理融入常识。采用了多轮引导和答案进化框架以提升任务成功几率。&lt;h4&gt;主要发现&lt;/h4&gt;通过在七项典型任务上进行测试并与现有基准方法比较，提出的Code-as-Symbolic-Planner平均提高了24.1%的成功率，并且显示了在离散和连续环境、二维/三维模拟以及真实世界设置中的强效性和泛化性。&lt;h4&gt;结论&lt;/h4&gt;我们的研究证明了LLM生成代码作为符号规划器的可行性和有效性，这对机器人任务和运动规划领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究展示了大型语言模型在机器人任务与运动规划领域的潜力。当前的方法主要依赖于文本或代码推理链，但未充分使用语言模型的代码生成能力。我们提出了一种新的方法——Code-as-Symbolic-Planner，通过指导LLM生成解决和验证问题所需的代码来提高其性能，并且证明了该方法在多种任务中的有效性与泛化性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works have shown great potentials of Large Language Models (LLMs) inrobot task and motion planning (TAMP). Current LLM approaches generate text- orcode-based reasoning chains with sub-goals and action plans. However, they donot fully leverage LLMs' symbolic computing and code generation capabilities.Many robot TAMP tasks involve complex optimization under multiple constraints,where pure textual reasoning is insufficient. While augmenting LLMs withpredefined solvers and planners improves performance, it lacks generalizationacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMPcapabilities by steering them to generate code as symbolic planners foroptimization and constraint verification. Unlike prior work that uses code tointerface with robot action modules, we steer LLMs to generate code as solvers,planners, and checkers for TAMP tasks requiring symbolic computing, while stillleveraging textual reasoning to incorporate common sense. With a multi-roundguidance and answer evolution framework, the proposed Code-as-Symbolic-Plannerimproves success rates by average 24.1\% over best baseline methods acrossseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner showsstrong effectiveness and generalizability across discrete and continuousenvironments, 2D/3D simulations and real-world settings, as well as single- andmulti-robot tasks with diverse requirements. See our project websitehttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, andcode.</description>
      <author>example@mail.com (Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan)</author>
      <guid isPermaLink="false">2503.01700v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</title>
      <link>http://arxiv.org/abs/2503.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;病理学和转录组学在肿瘤学中是两个基本的研究手段，它们分别涵盖了疾病形态学和分子层面的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态表示学习方法（MIRROR），旨在促进不同模式之间的对齐同时保留各自独特的特征。&lt;h4&gt;方法&lt;/h4&gt;该方法使用特定的编码器提取每个模式的全面特征，并通过模式对齐模块实现表型模式与分子谱图的无缝集成。此外，还包含一个模式保持模块来保护各个模式的独特属性，以及一个风格聚类模块来减少冗余并增强疾病相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;在TCGA队列中的癌症亚型分类和生存分析中进行了广泛的评估，结果表明MIRROR方法具有优越的表现，在构建综合的肿瘤学特征表示方面效果显著。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了MIRROR能够在不同模式间进行有效的对齐和保持各自特有的结构，有助于改进癌症诊断的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经完成，并且转换为了JSON格式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/TianyiFranklinWang/MIRROR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology and transcriptomics are fundamental modalities in oncology,encapsulating the morphological and molecular aspects of the disease.Multi-modal self-supervised learning has demonstrated remarkable potential inlearning pathological representations by integrating diverse data sources.Conventional multi-modal integration methods primarily emphasize modalityalignment, while paying insufficient attention to retaining themodality-specific structures. However, unlike conventional scenarios wheremulti-modal inputs share highly overlapping features, histopathology andtranscriptomics exhibit pronounced heterogeneity, offering orthogonal yetcomplementary insights. Histopathology provides morphological and spatialcontext, elucidating tissue architecture and cellular topology, whereastranscriptomics delineates molecular signatures through gene expressionpatterns. This inherent disparity introduces a major challenge in aligning themwhile maintaining modality-specific fidelity. To address these challenges, wepresent MIRROR, a novel multi-modal representation learning method designed tofoster both modality alignment and retention. MIRROR employs dedicated encodersto extract comprehensive features for each modality, which is furthercomplemented by a modality alignment module to achieve seamless integrationbetween phenotype patterns and molecular profiles. Furthermore, a modalityretention module safeguards unique attributes from each modality, while a styleclustering module mitigates redundancy and enhances disease-relevantinformation by modeling and aligning consistent pathological signatures withina clustering space. Extensive evaluations on TCGA cohorts for cancer subtypingand survival analysis highlight MIRROR's superior performance, demonstratingits effectiveness in constructing comprehensive oncological featurerepresentations and benefiting the cancer diagnosis.</description>
      <author>example@mail.com (Tianyi Wang, Jianan Fan, Dingxin Zhang, Dongnan Liu, Yong Xia, Heng Huang, Weidong Cai)</author>
      <guid isPermaLink="false">2503.00374v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MUSt3R: Multi-view Network for Stereo 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2503.01661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MUSt3R，它是DUSt3R的多视图扩展版本，用于解决大规模图像集合中密集且无约束的立体3D重建问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法如DUSt3R在处理任意图像集合进行立体3D重建时效果良好，但当面对大量图像时，由于需要处理成对图像的数量呈二次增长，导致计算复杂度急剧增加。这限制了其在大规模数据集上的应用和优化效率。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多视图网络MUSt3R来解决DUSt3R中存在的问题，旨在提升大规模图像集合的立体3D重建性能，并保持高帧率下的计算效率。&lt;h4&gt;方法&lt;/h4&gt;通过将DUSt3R架构对称化并扩展到直接预测所有视角在共同坐标系中的3D结构，同时引入多层次内存机制以降低复杂度。该模型能够处理在线和离线场景中的SfM（Simultaneous Localization and Mapping）以及视觉SLAM问题。&lt;h4&gt;主要发现&lt;/h4&gt;MUSt3R通过有效减少计算量并提高重建效率，在各种下游任务中显示出超越现有方法的性能，包括未校准视觉里程计、相对相机姿态估计、尺度和焦距估算等。&lt;h4&gt;结论&lt;/h4&gt;所提出的MUSt3R架构解决了DUSt3R在处理大规模图像集合时存在的局限性，并为实时高性能立体3D重建提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，DUSt3R提出了一种新的几何计算机视觉范式，即可以对任意图像集进行密集和无约束的立体3D重建。然而，在内部处理成对图像并回归局部3D重构时会产生计算复杂度问题，尤其在大量图片集合的情况下。本文介绍了一种基于多视图网络（MUSt3R）的方法，解决了上述所有问题，并展示了其在视觉里程计、相对相机姿态估计等方面的优越性能和效率提升能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DUSt3R introduced a novel paradigm in geometric computer vision by proposinga model that can provide dense and unconstrained Stereo 3D Reconstruction ofarbitrary image collections with no prior information about camera calibrationnor viewpoint poses. Under the hood, however, DUSt3R processes image pairs,regressing local 3D reconstructions that need to be aligned in a globalcoordinate system. The number of pairs, growing quadratically, is an inherentlimitation that becomes especially concerning for robust and fast optimizationin the case of large image collections. In this paper, we propose an extensionof DUSt3R from pairs to multiple views, that addresses all aforementionedconcerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction,or MUSt3R, that modifies the DUSt3R architecture by making it symmetric andextending it to directly predict 3D structure for all views in a commoncoordinate frame. Second, we entail the model with a multi-layer memorymechanism which allows to reduce the computational complexity and to scale thereconstruction to large collections, inferring thousands of 3D pointmaps athigh frame-rates with limited added complexity. The framework is designed toperform 3D reconstruction both offline and online, and hence can be seamlesslyapplied to SfM and visual SLAM scenarios showing state-of-the-art performanceon various 3D downstream tasks, including uncalibrated Visual Odometry,relative camera pose, scale and focal estimation, 3D reconstruction andmulti-view depth estimation.</description>
      <author>example@mail.com (Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, Vincent Leroy)</author>
      <guid isPermaLink="false">2503.01661v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ecg2o: A Seamless Extension of g2o for Equality-Constrained Factor Graph Optimization</title>
      <link>http://arxiv.org/abs/2503.01311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，通过扩展因子图来无缝地集成等式约束，从而提高了解决方案的精度，并拓宽了其在最优控制领域的应用。&lt;h4&gt;背景&lt;/h4&gt;因子图优化作为一种基础框架用于机器人感知领域，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）和态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。&lt;h4&gt;目的&lt;/h4&gt;在不增加额外优化算法的情况下，引入一种能够直接支持等式约束的因子图扩展方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的因子图扩展方式，该方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。使用ecg2o库验证此方法的有效性，该库是基于广泛使用的g2o因子图库开发的，并为等式受限优化提供了全面支持。&lt;h4&gt;主要发现&lt;/h4&gt;在自主车辆速度跟踪的最优控制问题中应用该方法后，与当前最先进的约束处理技术相比，本研究提出的解决方案表现出更高的精确度和可靠性。&lt;h4&gt;结论&lt;/h4&gt;通过扩展g2o因子图库以支持等式约束，并且提供了开源示例代码和用于验证的新颖优化算法，证明了这种新方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因子图优化是机器人感知领域的一个基础框架，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）及态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。然而，通过原生支持等式约束来扩展因子图可以提高解决方案的准确性并拓宽其应用范围，尤其是在最优控制领域中。本文提出了一种新颖的因子图扩展方法，在不使用额外优化算法的情况下直接集成等式约束。这种方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。为了验证该方法的有效性，将其应用于自主车辆速度跟踪的最佳控制问题，并将结果与当前最先进的约束处理技术进行了比较。此外，还介绍了ecg2o库——一个头文件形式的C++库，它扩展了广泛使用的g2o因子图库，增加了对等式受限优化的支持。此库、示例代码以及最优控制问题可以在https://github.com/snt-arg/ecg2o上作为开源资源获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Factor graph optimization serves as a fundamental framework for roboticperception, enabling applications such as pose estimation, simultaneouslocalization and mapping (SLAM), structure-from-motion (SfM), and situationalawareness. Traditionally, these methods solve unconstrained least squaresproblems using algorithms such as Gauss-Newton and Levenberg-Marquardt.However, extending factor graphs with native support for equality constraintscan improve solution accuracy and broaden their applicability, particularly inoptimal control. In this paper, we propose a novel extension of factor graphsthat seamlessly incorporates equality constraints without requiring additionaloptimization algorithms. Our approach maintains the efficiency and flexibilityof existing second-order optimization techniques while ensuring constraintfeasibility. To validate our method, we apply it to an optimal control problemfor velocity tracking in autonomous vehicles and benchmark our results againststate-of-the-art constraint handling techniques. Additionally, we introduceecg2o, a header-only C++ library that extends the widely used g2o factor graphlibrary by adding full support for equality-constrained optimization. Thislibrary, along with demonstrative examples and the optimal control problem, isavailable as open source at https://github.com/snt-arg/ecg2o</description>
      <author>example@mail.com (Anas Abdelkarim, Holger Voos, Daniel Görges)</author>
      <guid isPermaLink="false">2503.01311v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping</title>
      <link>http://arxiv.org/abs/2503.01202v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种使用多传感器无人机系统的新型大范围正射影像快速生成技术，该系统集成了GPS、IMU、毫米波雷达和相机。通过利用这些数据源，可以提高传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基于无人机(UAV)的大规模正射影像的快速生成一直是航空摄影领域的研究重点。&lt;h4&gt;目的&lt;/h4&gt;克服现有技术中的瓶颈问题，提出一种利用多传感器数据来提高传统正射影像生成方法的技术方案。&lt;h4&gt;方法&lt;/h4&gt;引入了一种先姿态优化特征匹配的方法以提升匹配速度和准确性，并减少所需的特征数量。这种方法特别适用于低纹理场景（如农田），在此类场景中常规的特征匹配困难。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和时间效率方面都表现出色，特别是在农田检测与管理方面的应用效果良好。&lt;h4&gt;结论&lt;/h4&gt;所提出的无人机系统能够有效支持农田的识别和管理工作，并且证明了其在低纹理环境中生成高精度正射影像的能力。&lt;h4&gt;翻译&lt;/h4&gt;快速生成基于无人飞行器(UAV)的大规模正射影像一直是航空摄影领域研究的重点。本文提出了一种结合GPS、IMU、毫米波雷达和相机等多传感器数据的无人机系统解决方案，以克服传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。通过先姿态优化特征匹配的方法增强了匹配速度和精度，并减少了所需的特征数量，为结构从运动(SfM)过程提供了精确的参照依据。该技术特别适用于低纹理场景（如农田），这些场景中常规的特征匹配通常比较困难。实验结果显示，我们的方法能够在短时间内实现高准确度的正射影像生成，证明了其在有效支持农田识别和管理工作中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles(UAVs) has been a long-standing focus of research in the field of aerialmapping. A multi-sensor UAV system, integrating the Global Positioning System(GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera,can provide an effective solution to this problem. In this paper, we utilizemulti-sensor data to overcome the limitations of conventional orthoimagegeneration methods in terms of temporal performance, system robustness, andgeographic reference accuracy. A prior-pose-optimized feature matching methodis introduced to enhance matching speed and accuracy, reducing the number ofrequired features and providing precise references for the Structure fromMotion (SfM) process. The proposed method exhibits robustness in low-texturescenes like farmlands, where feature matching is difficult. Experiments showthat our approach achieves accurate feature matching orthoimage generation in ashort time. The proposed drone system effectively aids in farmland detectionand management.</description>
      <author>example@mail.com (Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan)</author>
      <guid isPermaLink="false">2503.01202v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain</title>
      <link>http://arxiv.org/abs/2503.00853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WACV Workshop 2025 - 3rd Workshop on Maritime Computer Vision  (MaCVI2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的海洋3D场景重建基准数据集MTReD，旨在解决视频航拍视角下的海上三维场景重建问题，并提供了初步的评估方法。&lt;h4&gt;背景&lt;/h4&gt;当前没有专门针对海上环境的3D场景重建的数据集。传统的感知度量标准如LPIPS无法准确衡量重建图像的完整性。&lt;h4&gt;目的&lt;/h4&gt;创建一个新颖的海洋3D场景重建基准数据集MTReD，以促进这一领域的研究和开发。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的语义相似性度量DiFPS来评估重建的质量。使用了两种基线模型进行初步评估，并探索了一些预处理方法来提高结果。&lt;h4&gt;主要发现&lt;/h4&gt;MASt3R模型在感知度量上有更好的表现，但在重投影误差上不如SfM模型。通过适当的预处理技术可以同时改进这两种评价指标。&lt;h4&gt;结论&lt;/h4&gt;希望MTReD数据集能够推动未来在这个方向上的研究，并鼓励更多的研究人员参与到这个领域中来。&lt;h4&gt;翻译&lt;/h4&gt;这项工作解决了海上领域的视频航拍视角下的3D场景重建问题，重点在于几何一致性与视觉完整性。这将使下游任务如分割、导航和定位成为可能。目前没有专门为此领域的数据集存在。因此，我们提出了一个新的海洋3D场景重建基准测试数据集MTReD（Maritime Three-Dimensional Reconstruction Dataset）。该数据集包含19段从互联网收集的视频片段，其中包括船舰、岛屿以及海岸线等元素。由于任务目标在于几何一致性与视觉完整性，该数据集采用两种度量标准：重投影误差和感知度量。我们发现现有的基于感知度量的方法如LPIPS并不适合衡量重建图像的整体性，因此提出了一种新的利用DINOv2特征的语义相似度度量DiFPS（DinoV2 Features Perception Similarity）。我们在两个基线模型上进行了初步评估：通过Colmap实现的结构从运动（SfM）以及最近的研究前沿MASt3R模型。结果表明，相比于SfM，基于MASt3R重建出的场景在重投影误差更高但感知度量得分更好。因此我们探索了一些预处理方法，并发现了一种能够同时提高重投影误差和感知度量得分的方法。我们认为MTReD数据集将促进该领域的进一步研究发展。所有数据集及代码将在https://github.com/RuiYiYong/MTReD公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work tackles 3D scene reconstruction for a video fly-over perspectiveproblem in the maritime domain, with a specific emphasis on geometrically andvisually sound reconstructions. This will allow for downstream tasks such assegmentation, navigation, and localization. To our knowledge, there is nodataset available in this domain. As such, we propose a novel maritime 3D scenereconstruction benchmarking dataset, named as MTReD (Maritime Three-DimensionalReconstruction Dataset). The MTReD comprises 19 fly-over videos curated fromthe Internet containing ships, islands, and coastlines. As the task is aimedtowards geometrical consistency and visual completeness, the dataset uses twometrics: (1) Reprojection error; and (2) Perception based metrics. We find thatexisting perception-based metrics, such as Learned Perceptual Image PatchSimilarity (LPIPS), do not appropriately measure the completeness of areconstructed image. Thus, we propose a novel semantic similarity metricutilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity).We perform initial evaluation on two baselines: (1) Structured from Motion(SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We findthat the reconstructed scenes by MASt3R have higher reprojection errors, butsuperior perception based metric scores. To this end, some pre-processingmethods are explored, and we find a pre-processing method which improves boththe reprojection error and perception-based score. We envisage our proposedMTReD to stimulate further research in these directions. The dataset and allthe code will be made available in https://github.com/RuiYiYong/MTReD.</description>
      <author>example@mail.com (Rui Yi Yong, Samuel Picosson, Arnold Wiliem)</author>
      <guid isPermaLink="false">2503.00853v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</title>
      <link>http://arxiv.org/abs/2503.03734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OTTER是一种新颖的Vision-Language-Action (VLA) 模型，它通过显式的、文本感知的视觉特征提取方法来利用现有预训练模型中视觉和语言之间的语义对齐。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型在预测基于视觉观察和语言指令的机器人动作时需要微调预训练的视觉-语言模型，并且由于视觉和语言特性独立地输入下游策略，这会破坏预先训练好的语义对齐。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来利用现有预训练模型中的语义对齐，同时保持预训练编码器不变。&lt;h4&gt;方法&lt;/h4&gt;OTTER选择性提取并传递与语言指令语义一致的任务相关视觉特征到策略变换器中。这使得可以在不调整预训练的视觉-语言编码器的情况下进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过在仿真和真实世界实验中的表现，证明了OTTER能够显著优于现有VLA模型，并且具有强大的零样本泛化能力，可以推广到新颖的对象和环境中。&lt;h4&gt;结论&lt;/h4&gt;OTTER通过保持预训练视觉-语言编码器的冻结状态，保留并利用大型数据集预训练中学习的丰富语义理解，使得在新对象和环境中的零样本泛化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型旨在根据视觉观察和语言指令预测机器人动作。现有方法要求微调预训练的视觉语言模型（VLM），因为独立处理的视觉和语言特征会被输入到下游策略中，从而破坏了预训练中的语义对齐。我们提出了OTTER，这是一种新的VLA架构，通过显式的、文本感知的视觉特性提取来利用这些现有的对齐关系。与加工所有视觉特征相反，OTTER选择性地抽取并传递任务相关的且与语言指令语义一致的视觉特征到策略变换器中。这使得OTTER能够保持预训练视觉-语言编码器处于冻结状态。因此，OTTER保留和利用了大规模预训练中学到的丰富语义理解能力，从而具备强大的零样本泛化能力。在仿真和真实世界实验中，OTTER显著优于现有的VLA模型，在新对象和环境中的零样本泛化效果出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models aim to predict robotic actions based onvisual observations and language instructions. Existing approaches requirefine-tuning pre-trained visionlanguage models (VLMs) as visual and languagefeatures are independently fed into downstream policies, degrading thepre-trained semantic alignments. We propose OTTER, a novel VLA architecturethat leverages these existing alignments through explicit, text-aware visualfeature extraction. Instead of processing all visual features, OTTERselectively extracts and passes only task-relevant visual features that aresemantically aligned with the language instruction to the policy transformer.This allows OTTER to keep the pre-trained vision-language encoders frozen.Thereby, OTTER preserves and utilizes the rich semantic understanding learnedfrom large-scale pre-training, enabling strong zero-shot generalizationcapabilities. In simulation and real-world experiments, OTTER significantlyoutperforms existing VLA models, demonstrating strong zeroshot generalizationto novel objects and environments. Video, code, checkpoints, and dataset:https://ottervla.github.io/.</description>
      <author>example@mail.com (Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel)</author>
      <guid isPermaLink="false">2503.03734v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities</title>
      <link>http://arxiv.org/abs/2503.02967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 2 tables, accepted to 1th AITC conference in  University Of Isfahan&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了人工智能和机器视觉技术在交通系统中的应用，通过高级监控摄像头和深度学习算法实现车辆实时检测、交通异常及驾驶行为识别。该系统整合地理空间数据和天气信息以适应不同的环境条件。&lt;h4&gt;背景&lt;/h4&gt;城市化进程加快和车辆拥堵问题对交通安全管理和效率提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨AI与机器视觉技术在革命性变革交通系统的潜力，优化交通流并提高道路安全。&lt;h4&gt;方法&lt;/h4&gt;利用YOLOv8和YOLOv11模型进行高精度的车辆检测及异常识别。结合地理空间信息和天气数据来增强系统适应性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;研究实现了高度准确的车辆检测和异常认知，提高了交通流畅度与安全性。&lt;h4&gt;结论&lt;/h4&gt;研究成果为智能交通管理解决方案的发展提供了支持，并推动了智慧城市建设的目标——实现可持续且高效的都市基础设施。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化的加速及汽车拥堵问题加剧，对交通安全管理和效率提出了重大挑战。这项研究探讨了人工智能和机器视觉技术在交通系统中的革命性潜力，利用先进的监控摄像头和深度学习算法提出了一套车辆实时检测、交通异常以及驾驶行为识别的解决方案。该方案结合地理空间数据和天气信息以动态适应不同环境条件，确保各种场景下的稳健表现。通过使用YOLOv8及YOLOv11模型，研究达到了高精度的车辆与异常探测水平，优化了交通流，并提升了道路安全性。这些发现有助于智能交通管理解决方案的发展，并符合创建智慧城市的愿景——建立具有可持续性和高效性的城市基础设施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid urbanization of cities and increasing vehicular congestion haveposed significant challenges to traffic management and safety. This studyexplores the transformative potential of artificial intelligence (AI) andmachine vision technologies in revolutionizing traffic systems. By leveragingadvanced surveillance cameras and deep learning algorithms, this researchproposes a system for real-time detection of vehicles, traffic anomalies, anddriver behaviors. The system integrates geospatial and weather data to adaptdynamically to environmental conditions, ensuring robust performance in diversescenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy invehicle detection and anomaly recognition, optimizing traffic flow andenhancing road safety. These findings contribute to the development ofintelligent traffic management solutions and align with the vision of creatingsmart cities with sustainable and efficient urban infrastructure.</description>
      <author>example@mail.com (Seyed Hossein Hosseini DolatAbadi, Sayyed Mohammad Hossein Hashemi, Mohammad Hosseini, Moein-Aldin AliHosseini)</author>
      <guid isPermaLink="false">2503.02967v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>
      <link>http://arxiv.org/abs/2503.03726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于RGB图像估计无纹理物体6D姿态的主动感知框架。&lt;h4&gt;背景&lt;/h4&gt;单视图6D姿态估计在处理具有外观模糊性、旋转对称性和严重遮挡的对象时存在局限性，需要研究多视角姿态估计和最佳下视点预测方法来克服这些问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用RGB图像进行无纹理物体6D姿态估计的综合主动感知框架。&lt;h4&gt;方法&lt;/h4&gt;通过将6D姿态估计分解为两个步骤的过程：首先估算每个对象的3D平移，解决RGB图像中的尺度和深度模糊问题；然后使用简化后的任务来确定3D方向。引入了预测最佳下视角以捕捉RGB图像的策略，从而减少物体姿势不确定性并提高精度。&lt;h4&gt;主要发现&lt;/h4&gt;在公共ROBI数据集以及自建透明对象数据集中进行了实验验证，多视图姿态估计方法的表现优于现有的先进方法；通过利用最佳下视点策略，该方法能够在比启发式策略少得多的视角中实现高精度物体姿势准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为解决无纹理物体6D姿态估计问题提供了一种有效的方法，能够显著提高姿态估算的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 6D pose of textureless objects from RBG images is an importantproblem in robotics. Due to appearance ambiguities, rotational symmetries, andsevere occlusions, single-view based 6D pose estimators are still unable tohandle a wide range of objects, motivating research towards multi-view poseestimation and next-best-view prediction that addresses these limitations. Inthis work, we propose a comprehensive active perception framework forestimating the 6D poses of textureless objects using only RGB images. Ourapproach is built upon a key idea: decoupling the 6D pose estimation into asequential two-step process can greatly improve both accuracy and efficiency.First, we estimate the 3D translation of each object, resolving scale and depthambiguities inherent to RGB images. These estimates are then used to simplifythe subsequent task of determining the 3D orientation, which we achieve throughcanonical scale template matching. Building on this formulation, we thenintroduce an active perception strategy that predicts the next best cameraviewpoint to capture an RGB image, effectively reducing object pose uncertaintyand enhancing pose accuracy. We evaluate our method on the public ROBI datasetas well as on a transparent object dataset that we created. When evaluatedusing the same camera viewpoints, our multi-view pose estimation significantlyoutperforms state-of-the-art approaches. Furthermore, by leveraging ournext-best-view strategy, our method achieves high object pose accuracy withsubstantially fewer viewpoints than heuristic-based policies.</description>
      <author>example@mail.com (Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander)</author>
      <guid isPermaLink="false">2503.03726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration</title>
      <link>http://arxiv.org/abs/2503.00737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于密集特征的多帧校准方法，这种方法可以从场景数据中直接优化相机内部参数，从而消除了对额外校准拍摄的需求。该方法增强了传统的结构从运动（SfM）流程，并通过引入外参正则化项、密集特征重投影项和内参方差项来进行多帧联合优化。&lt;h4&gt;背景&lt;/h4&gt;大规模摄像机阵列的标定是耗时的过程，通常需要专门捕捉已知图案进行。虽然这些设置中的外部参数由于物理结构固定不变，但内部参数在不同会话中可能因镜头调整或温度变化等因素而发生变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外校准拍摄就能优化大规模摄像机阵列内部参数的方法，并提高3D重建的准确性。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入外参正则化项、密集特征重投影项和内参方差项来增强传统的SfM流程，实现多帧联合优化。这种方法可以直接从场景数据中优化相机内部参数，无需额外的校准拍摄。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在精确度上接近于专门的校准过程，并且显著提高了相机内参和3D重建的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法与现有的SfM流程完全兼容，为大规模摄像机设置提供了一种高效实用的即插即用解决方案。代码可在https://github.com/YJJfish/Multi-Cali-Anything公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yjjfish/multi-cali-anything&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Calibrating large-scale camera arrays, such as those in dome-based setups, istime-intensive and typically requires dedicated captures of known patterns.While extrinsics in such arrays are fixed due to the physical setup, intrinsicsoften vary across sessions due to factors like lens adjustments or temperaturechanges. In this paper, we propose a dense-feature-driven multi-framecalibration method that refines intrinsics directly from scene data,eliminating the necessity for additional calibration captures. Our approachenhances traditional Structure-from-Motion (SfM) pipelines by introducing anextrinsics regularization term to progressively align estimated extrinsics withground-truth values, a dense feature reprojection term to reduce keypointerrors by minimizing reprojection loss in the feature space, and an intrinsicsvariance term for joint optimization across multiple frames. Experiments on theMultiface dataset show that our method achieves nearly the same precision asdedicated calibration processes, and significantly enhances intrinsics and 3Dreconstruction accuracy. Fully compatible with existing SfM pipelines, ourmethod provides an efficient and practical plug-and-play solution forlarge-scale camera setups. Our code is publicly available at:https://github.com/YJJfish/Multi-Cali-Anything</description>
      <author>example@mail.com (Jinjiang You, Hewei Wang, Yijie Li, Mingxiao Huo, Long Van Tran Ha, Mingyuan Ma, Jinfeng Xu, Puzhen Wu, Shubham Garg, Wei Pu)</author>
      <guid isPermaLink="false">2503.00737v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Efficient End-to-end Visual Localization for Autonomous Driving with Decoupled BEV Neural Matching</title>
      <link>http://arxiv.org/abs/2503.00862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一种端到端的定位神经网络，直接从周围图像中估计车辆姿态，而无需显式地将感知结果与高精度地图匹配。通过减少采样空间来确保效率和可解释性，并在实验中展示了其厘米级定位能力。&lt;h4&gt;背景&lt;/h4&gt;精确的定位对于高级自动驾驶系统至关重要。传统基于地图匹配的方法容易受感知噪声影响，需要昂贵的超参数调整。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端定位神经网络方法，能够直接从图像估计车辆姿态，同时保持计算效率和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个解耦的BEV（鸟瞰图）神经匹配模块，用于差分采样基于匹配的姿态估算，并通过解耦每个自由度对特征表示的影响来大幅减少采样空间。&lt;h4&gt;主要发现&lt;/h4&gt;该网络在纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量减少了68.8%。&lt;h4&gt;结论&lt;/h4&gt;新的定位方法能够实现高精度的车辆姿态估计，并且具有低资源消耗的特点。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位在高级自动驾驶系统中起着重要作用。传统的基于地图匹配的方法通过显式地将映射元素与传感器观测结果进行匹配来解决姿态问题，这种方法通常对感知噪声敏感，因此需要昂贵的超参数调整。在这篇论文中，我们提出了一种端到端的定位神经网络，直接从周围的图像估计车辆姿态，而不必显式地将感知结果与高精度地图进行匹配。为了保证效率和可解释性，提出了一个解耦的基于BEV（鸟瞰图）神经匹配的姿态求解器，在一个差分采样基础上的匹配模块中估计姿态。此外，通过解耦每个自由度对特征表示的影响大大减少了采样空间。实验结果表明，所提出的网络能够进行厘米级定位，纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量降低了68.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization plays an important role in high-level autonomousdriving systems. Conventional map matching-based localization methods solve theposes by explicitly matching map elements with sensor observations, generallysensitive to perception noise, therefore requiring costly hyper-parametertuning. In this paper, we propose an end-to-end localization neural networkwhich directly estimates vehicle poses from surrounding images, withoutexplicitly matching perception results with HD maps. To ensure efficiency andinterpretability, a decoupled BEV neural matching-based pose solver isproposed, which estimates poses in a differentiable sampling-based matchingmodule. Moreover, the sampling space is hugely reduced by decoupling thefeature representation affected by each DoF of poses. The experimental resultsdemonstrate that the proposed network is capable of performing decimeter levellocalization with mean absolute errors of 0.19m, 0.13m and 0.39 degree inlongitudinal, lateral position and yaw angle while exhibiting a 68.8% reductionin inference memory usage.</description>
      <author>example@mail.com (Jinyu Miao, Tuopu Wen, Ziang Luo, Kangan Qian, Zheng Fu, Yunlong Wang, Kun Jiang, Mengmeng Yang, Jin Huang, Zhihua Zhong, Diange Yang)</author>
      <guid isPermaLink="false">2503.00862v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems</title>
      <link>http://arxiv.org/abs/2503.01704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DILEMMA是一个针对边缘计算环境中的大型语言模型部署挑战而设计的框架，通过联合优化层放置和量化策略来最小化推理延迟并保证性能。&lt;h4&gt;背景&lt;/h4&gt;随着大语言模型在智慧城市应用中越来越流行，如何在网络边缘有效利用这些资源成为一个关键问题。边缘计算可以降低通信延迟，但受制于有限的通信、计算和存储能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，能够在满足大型语言模型性能要求的同时优化其部署到边缘计算环境中的方法。&lt;h4&gt;方法&lt;/h4&gt;DILEMMA通过整数线性规划问题来最小化总的推理延迟，并利用逐层量化和知识蒸馏技术控制LLM的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OPT-350模型上使用SQuAD数据集进行实验后，证明了DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，显示其在资源受限环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;DILEMMA框架为大型语言模型部署到边缘计算系统提供了有效的解决方案，尤其是在需要降低通信延迟和优化资源使用的场景中。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型被越来越多地应用于智慧城市的不同应用领域，有必要将这些模型推向网络的边缘，同时保持其性能。作为物理上更接近最终用户的计算资源，边缘计算可以减少为依赖大型语言模型的服务提供服务时的通信延迟。然而，边缘服务器在通信、计算和存储容量方面的能力有限。本文介绍了一种名为DILEMMA的新框架，该框架通过联合优化层放置和量化策略来解决将大语言模型部署到边缘系统中的挑战。DILEMMA通过整数线性规划问题最小化总的推理延迟并确保可接受的大语言模型性能水平，并利用逐层量化和知识蒸馏技术控制LLM的性能。使用SQuAD数据集对OPT-350模型进行实验评估表明，DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，展示了其在资源受限环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With a recent trend of using Large Language Models (LLMs) for differentapplications within smart cities, there is a need for pushing these modelstoward the edge of network while still preserving their performance. EdgeComputing (EC) as a physically closer computing resource to the end users canhelp to reduce the communication delay for serving end users' tasks forLLM-dependent services. However, EC servers have limited capacity in terms ofcommunication, computation, and storage capacity. This paper introducesDILEMMA, a novel framework addressing the challenges of deploying LLMs in ECsystems by jointly optimizing layer placement and layer quantization in ECsystems. DILEMMA formulates an Integer Linear Programming problem to minimizetotal inference delay while ensuring acceptable LLM performance levels,leveraging layer-wise quantization and knowledge distillation for LLMperformance control. Experimental evaluations on OPT-350 model using the SQuADdataset demonstrate that DILEMMA achieves a quantization ratio of up to 12.75%while preserving model loss, highlighting its effectiveness inresource-constrained environments.</description>
      <author>example@mail.com (Minoo Hosseinzadeh, Hana Khamfroush)</author>
      <guid isPermaLink="false">2503.01704v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Curating Demonstrations using Online Experience</title>
      <link>http://arxiv.org/abs/2503.03707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种机器人自我整理的方法，称为Demo-SCORE，通过使用在线机器人经验训练分类器来识别成功的策略执行，并过滤掉异质性演示数据集中的次优示范。&lt;h4&gt;背景&lt;/h4&gt;许多机器人的演示数据集中包含不同质量和可靠性的多样性示例。这种多样性可能有助于策略的预训练，但也可能导致最终模仿学习目标时性能下降。&lt;h4&gt;目的&lt;/h4&gt;目的是通过机器人自我整理来提高基于这些多样演示数据集训练的策略在测试中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Demo-SCORE的方法，该方法使用在线机器人经验训练分类器以区分成功的和不成功的策略执行，并利用此分类器过滤掉异质性示例数据集中次优的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与所有原始演示数据集一起训练的基准政策相比，应用了Demo-SCORE方法后生成的策略可以实现高达15%至35%更高的绝对成功率。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了一种有效的方法来识别和过滤次优示范，从而改善从异质性示例中学习出的机器人策略性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many robot demonstration datasets contain heterogeneous demonstrations ofvarying quality. This heterogeneity may benefit policy pre-training, but canhinder robot performance when used with a final imitation learning objective.In particular, some strategies in the data may be less reliable than others ormay be underrepresented in the data, leading to poor performance when suchstrategies are sampled at test time. Moreover, such unreliable orunderrepresented strategies can be difficult even for people to discern, andsifting through demonstration datasets is time-consuming and costly. On theother hand, policy performance when trained on such demonstrations can reflectthe reliability of different strategies. We thus propose for robots toself-curate based on online robot experience (Demo-SCORE). More specifically,we train and cross-validate a classifier to discern successful policy roll-outsfrom unsuccessful ones and use the classifier to filter heterogeneousdemonstration datasets. Our experiments in simulation and the real world showthat Demo-SCORE can effectively identify suboptimal demonstrations withoutmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolutesuccess rate in the resulting policy compared to the base policy trained withall original demonstrations.</description>
      <author>example@mail.com (Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2503.03707v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration</title>
      <link>http://arxiv.org/abs/2503.00167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了事件相机在利用现有激光雷达地图进行定位中的应用潜力，提出了一种基于粗略初始姿态细化的框架。&lt;h4&gt;背景信息&lt;/h4&gt;事件相机具备高动态范围和低延迟特性，在高速运动和极端光照条件下具有优势。现有的激光雷达地图可以用于导航和移动操作的应用中。&lt;h4&gt;研究目的&lt;/h4&gt;探索事件相机在现有LiDAR地图中的定位能力，以实现精准导航和机器人移动抓取任务。&lt;h4&gt;提出方法&lt;/h4&gt;{'第一步': '基于粗略初始姿态，将LiDAR点投影到2D空间中生成深度图。', '第二步': '利用光学流估计网络对事件与LiDAR点进行二维空间的配准。', '第三步': '使用PnP求解器来估计相机的姿态。', '改进措施': '开发了一种新的基于帧的事件表示，以增强几何一致性，并预测辅助变量作为正则化项，以减少地面实况姿态偏差对网络收敛性的影响。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明所提方法在公共数据集上具有有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种新颖的事件相机和激光雷达融合定位框架，并在网上发布了代码与预训练模型，以促进未来的相关研究。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了生物启发式的事件摄像机在现有LiDAR地图中进行定位的应用潜力。我们提出了一套基于粗略初始姿态细化的方法来实现这一目标，包括将LiDAR点投影到2D空间生成深度图、利用光学流估计网络对齐事件和LiDAR点以及使用PnP求解器估算相机姿势等步骤，并且开发了新的帧级事件表示方法以提高几何一致性。实验结果表明该方法在多个公开数据集上均有效，研究还开放了代码和预训练模型供后续研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras are bio-inspired sensors with some notable features, includinghigh dynamic range and low latency, which makes them exceptionally suitable forperception in challenging scenarios such as high-speed motion and extremelighting conditions. In this paper, we explore their potential for localizationwithin pre-existing LiDAR maps, a critical task for applications that requireprecise navigation and mobile manipulation. Our framework follows a paradigmbased on the refinement of an initial pose. Specifically, we first projectLiDAR points into 2D space based on a rough initial pose to obtain depth maps,and then employ an optical flow estimation network to align events with LiDARpoints in 2D space, followed by camera pose estimation using a PnP solver. Toenhance geometric consistency between these two inherently differentmodalities, we develop a novel frame-based event representation that improvesstructural clarity. Additionally, given the varying degrees of bias observed inthe ground truth poses, we design a module that predicts an auxiliary variableas a regularization term to mitigate the impact of this bias on networkconvergence. Experimental results on several public datasets demonstrate theeffectiveness of our proposed method. To facilitate future research, both thecode and the pre-trained models are made available online.</description>
      <author>example@mail.com (Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2503.00167v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Negative Damping Control for User-Dependent Multi-Terrain Walking Assistance with a Hip Exoskeleton</title>
      <link>http://arxiv.org/abs/2503.03662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的髋部外骨骼控制策略，通过适应性虚拟负阻尼设计来调整机械阻抗，使系统能够注入能量同时确保用户保持对运动的自愿贡献。&lt;h4&gt;背景&lt;/h4&gt;当前的辅助策略在应对个体行走模式和多变地形环境方面灵活性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够根据个体需求灵活调节、适应不同地形环境的新控制方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于虚拟负阻尼来调整机械系统的阻抗，通过实验验证其减少代谢成本的效果，并采用贝叶斯优化技术实现无缝的辅助强度调整和跨多种地形环境下的过渡。&lt;h4&gt;主要发现&lt;/h4&gt;该控制器在五名健康受试者中实现了平均7.2%的步行代谢成本降低；同时保持了下肢运动学特性，确保在整个步态周期中的功率损失小于总功率的2%，并且实现了与用户动作同步的最佳协调。&lt;h4&gt;结论&lt;/h4&gt;提出的方法展示了个性化的、适应性强且操作简单的髋部外骨骼控制器设计，推动了适用性更强、依赖于用户的控制法则的发展。&lt;h4&gt;翻译&lt;/h4&gt;现有的髋部外骨骼辅助策略在应对不同的行走模式和地形时表现不佳。本文介绍了一种新型的机械阻抗调节方法，通过虚拟负阻尼来适应人体-机器系统，该方法能够向系统注入能量并保持用户主动参与运动的能力。实验表明，在五名受试者中，与自由行走相比，步行代谢成本平均降低了7.2%，且下肢运动学没有改变。同时实现了步态周期内极低的功率损失（小于总功率的2%），确保了人机动作的一致性。此外，使用贝叶斯优化适应辅助强度以实现跨多地形环境的无缝过渡和调整。该方法展示了适用于所有条件下的高效功率传输，并为个性化、可调且用户依赖性的控制法则的发展提供了可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hip exoskeletons are known for their versatility in assisting users acrossvaried scenarios. However, current assistive strategies often lack theflexibility to accommodate for individual walking patterns and adapt to diverselocomotion environments. In this work, we present a novel control strategy thatadapts the mechanical impedance of the human-exoskeleton system. We design thehip assistive torques as an adaptive virtual negative damping, which is able toinject energy into the system while allowing the users to remain in control andcontribute voluntarily to the movements. Experiments with five healthy subjectsdemonstrate that our controller reduces the metabolic cost of walking comparedto free walking (average reduction of 7.2%), and it preserves the lower-limbskinematics. Additionally, our method achieves minimal power losses from theexoskeleton across the entire gait cycle (less than 2% negative mechanicalpower out of the total power), ensuring synchronized action with the users'movements. Moreover, we use Bayesian Optimization to adapt the assistancestrength and allow for seamless adaptation and transitions across multi-terrainenvironments. Our strategy achieves efficient power transmission under allconditions. Our approach demonstrates an individualized, adaptable, andstraightforward controller for hip exoskeletons, advancing the development ofviable, adaptive, and user-dependent control laws.</description>
      <author>example@mail.com (Giulia Ramella, Auke Ijspeert, Mohamed Bouri)</author>
      <guid isPermaLink="false">2503.03662v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Motion Planning and Control with Unknown Nonlinear Dynamics through Predicted Reachability</title>
      <link>http://arxiv.org/abs/2503.03633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对未知非线性动力学下的自主运动规划提出了一种混合规划-控制框架，旨在计算朝向目标的可行轨迹。&lt;h4&gt;背景&lt;/h4&gt;在未知非线性动态系统中进行自主运动规划具有重大挑战。为了引导系统的导航并适应环境变化，代理需要持续探索以获取关于可到达性的系统属性。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合规划-控制框架，该框架能够计算朝向目标的可行轨迹，并通过抽象为有向加权图来处理未知非线性动力学问题。&lt;h4&gt;方法&lt;/h4&gt;- 将状态空间划分为多个区域并将其近似为分段仿射（PWA）系统。- 通过确定系统的边缘存在情况，逐步更新有向加权图。- 利用前馈可到达性条件和控制理论中的可达性理论来预测未知动力学的先验信息，并据此分配启发式权重。&lt;h4&gt;主要发现&lt;/h4&gt;- 基于图形搜索结果在线生成控制器并不断更新预测图能够提高导航效率，从而适应动态变化。- 在移动机器人在未探索地形中运行时，可以将未知的动力学抽象为一个积分器模型来简化处理。&lt;h4&gt;结论&lt;/h4&gt;该方法通过仿真场景验证了其有效性，并展示了如何利用混合规划-控制框架和分段仿射系统的特性来解决自主运动规划中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;自主运动规划在面对未知非线性动力学时面临重大挑战。为了引导系统导航并适应环境，代理需要持续探索以获取关于可到达性的系统属性等信息。本文提出了一种混合规划-控制框架，用于计算朝向目标的可行轨迹。该方法通过将状态空间划分为多个区域并将它们近似为分段仿射（PWA）系统，然后将其抽象成有向加权图，并根据未知动力学的信息逐步更新其边的存在情况来实现这一目的。我们还提出了一个框架，在任务执行期间适应性地收集和分析数据，不断更新预测图形，并基于搜索结果在线生成控制器。通过模拟移动机器人在未知地形中操作的场景（将其未知的动力学抽象为单个积分器模型），验证了这种方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous motion planning under unknown nonlinear dynamics presentssignificant challenges. An agent needs to continuously explore the systemdynamics to acquire its properties, such as reachability, in order to guidesystem navigation adaptively. In this paper, we propose a hybridplanning-control framework designed to compute a feasible trajectory toward atarget. Our approach involves partitioning the state space and approximatingthe system by a piecewise affine (PWA) system with constrained control inputs.By abstracting the PWA system into a directed weighted graph, we incrementallyupdate the existence of its edges via affine system identification and reachcontrol theory, introducing a predictive reachability condition by exploitingprior information of the unknown dynamics. Heuristic weights are assigned toedges based on whether their existence is certain or remains indeterminate.Consequently, we propose a framework that adaptively collects and analyzes dataduring mission execution, continually updates the predictive graph, andsynthesizes a controller online based on the graph search outcomes. Wedemonstrate the efficacy of our approach through simulation scenarios involvinga mobile robot operating in unknown terrains, with its unknown dynamicsabstracted as a single integrator model.</description>
      <author>example@mail.com (Zhiquan Zhang, Gokul Puthumanaillam, Manav Vora, Melkior Ornik)</author>
      <guid isPermaLink="false">2503.03633v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation</title>
      <link>http://arxiv.org/abs/2503.03629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了TeraSim平台，这是一个用于自动驾驶车辆（AV）开发的开源、高保真交通模拟器。&lt;h4&gt;背景&lt;/h4&gt;交通仿真对于评估自动驾驶汽车的安全性至关重要。然而，传统的基于规则的方法难以捕捉复杂的驾驶员行为互动，而数据驱动方法则在保持长期行为真实性或生成安全相关的事件多样性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出TeraSim平台以解决上述挑战，旨在揭示未知的不安全事件并高效估算AV统计性能指标，如碰撞率。&lt;h4&gt;方法&lt;/h4&gt;设计了一个开放源代码平台，可以无缝集成第三方物理仿真器和独立的自动驾驶车辆软件堆栈，构建完整的自动驾驶车辆模拟系统。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明TeraSim在生成涉及静态及动态代理的各种安全关键事件方面有效，并能识别出AV系统的隐藏缺陷，支持统计性能评估。&lt;h4&gt;结论&lt;/h4&gt;该平台作为实用性工具对研究者、开发者以及政策制定者都有重要的价值，有助于自动驾驶车辆的安全性评估。&lt;h4&gt;翻译&lt;/h4&gt;交通模拟对于自动驾驶汽车的发展至关重要，能够实现各种驾驶条件下的全面安全评估。然而，传统基于规则的模拟器难以捕捉复杂的人类互动行为，而数据驱动的方法往往在长期的行为真实性保持或生成多样的重要安全性事件方面存在不足。为此，提出了一种名为TeraSim的开源高保真交通仿真平台，旨在揭示未知的安全隐患，并高效地估计自动驾驶汽车如碰撞率等统计性能指标。该平台设计用于与第三方物理模拟器和独立的自动驾驶汽车软件堆栈无缝集成，构建完整的自动驾驶车辆仿真系统。实验结果显示其在生成涉及静态及动态代理的各种重要安全事件方面有效，并能识别出AV系统的隐藏缺陷，支持进行统计性能评估。这些发现强调了TeraSim作为实用性工具对研究者、开发者以及政策制定者的潜在价值，可用于自动驾驶汽车的安全性评估。代码可在https://github.com/mcity/TeraSim获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic simulation is essential for autonomous vehicle (AV) development,enabling comprehensive safety evaluation across diverse driving conditions.However, traditional rule-based simulators struggle to capture complex humaninteractions, while data-driven approaches often fail to maintain long-termbehavioral realism or generate diverse safety-critical events. To address thesechallenges, we propose TeraSim, an open-source, high-fidelity trafficsimulation platform designed to uncover unknown unsafe events and efficientlyestimate AV statistical performance metrics, such as crash rates. TeraSim isdesigned for seamless integration with third-party physics simulators andstandalone AV stacks, to construct a complete AV simulation system.Experimental results demonstrate its effectiveness in generating diversesafety-critical events involving both static and dynamic agents, identifyinghidden deficiencies in AV systems, and enabling statistical performanceevaluation. These findings highlight TeraSim's potential as a practical toolfor AV safety assessment, benefiting researchers, developers, and policymakers.The code is available at https://github.com/mcity/TeraSim.</description>
      <author>example@mail.com (Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu)</author>
      <guid isPermaLink="false">2503.03629v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery</title>
      <link>http://arxiv.org/abs/2503.03579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;当前大多数关于机器人与人类交互的研究主要集中于抓取策略和运动规划。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的系统，用于模拟人机之间的物体交接过程。该系统着重于推断人的交接意图及想象空间配置，以模仿合作型机器人的认知处理。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '整合多模态感知（视觉和语言提示），来推断人类的交接意图。', '第二部分': '使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该系统通过模仿人类认知处理中的运动意象，在人机交互中展现出良好的潜力和应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的用于机器人与人类物体交接系统的模型，这个系统模拟了人类同事间的互动方式。不同于大多数现有研究主要集中于抓取策略及运动规划，我们的系统侧重于推断人的交接意图以及想象空间配置。第一部分整合了多模态感知（视觉和语言提示）来推断人类的意图；第二部分使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系，从而模仿认知处理中的运动意象过程。实验结果显示该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。代码、视频和数据可在 https://i3handover.github.io 获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel system for robot-to-human object handover that emulateshuman coworker interactions. Unlike most existing studies that focus primarilyon grasping strategies and motion planning, our system focus on 1. inferringhuman handover intents, 2. imagining spatial handover configuration. The firstone integrates multimodal perception-combining visual and verbal cues-to inferhuman intent. The second one using a diffusion-based model to generate thehandover configuration, involving the spacial relationship among robot'sgripper, the object, and the human hand, thereby mimicking the cognitiveprocess of motor imagery. Experimental results demonstrate that our approacheffectively interprets human cues and achieves fluent, human-like handovers,offering a promising solution for collaborative robotics. Code, videos, anddata are available at: https://i3handover.github.io.</description>
      <author>example@mail.com (Hanxin Zhang, Abdulqader Dhafer, Zhou Daniel Hao, Hongbiao Dong)</author>
      <guid isPermaLink="false">2503.03579v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization</title>
      <link>http://arxiv.org/abs/2502.08848v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为SpeechCompass的系统，用于解决移动设备上语音转文字功能在群组对话中无法区分和指示说话者方向的问题。&lt;h4&gt;背景&lt;/h4&gt;移动设备上的语音转文字技术已被证明对听力和语言无障碍、语言翻译、记笔记和会议记录有帮助。然而，基础大规模调查表明，在群体交谈中无法识别和表示说话者的方向使其变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;通过实现实时多麦克风语音定位解决现有移动设备语音转文字功能在群体对话中的局限性问题，并探索有效的可视化方法来指导用户。&lt;h4&gt;方法&lt;/h4&gt;引入了高效实时音频定位算法以及自定义的声音感知硬件，这些硬件运行在一个低功耗微控制器上并连接四个集成麦克风。进行了大规模调查（n=494）并对八名经常使用移动语音转文字的参与者进行了一对一的研究，并收集了他们关于五种可视化样式的反馈。&lt;h4&gt;主要发现&lt;/h4&gt;所有参与者的反馈一致认为，区分说话者和定位视觉化对于群组对话的价值和潜力至关重要。他们认可方向引导的实际价值和潜在应用。&lt;h4&gt;结论&lt;/h4&gt;通过引入SpeechCompass系统，移动设备上的语音转文字功能在群体对话中的用户体验得到了显著改善，并且该技术具有实际的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech-to-text capabilities on mobile devices have proven helpful for hearingand speech accessibility, language translation, note-taking, and meetingtranscripts. However, our foundational large-scale survey (n=263) shows thatthe inability to distinguish and indicate speaker direction makes themchallenging in group conversations. SpeechCompass addresses this limitationthrough real-time, multi-microphone speech localization, where the direction ofspeech allows visual separation and guidance (e.g., arrows) in the userinterface. We introduce efficient real-time audio localization algorithms andcustom sound perception hardware running on a low-power microcontroller andfour integrated microphones, which we characterize in technical evaluations.Informed by a large-scale survey (n=494), we conducted an in-person study ofgroup conversations with eight frequent users of mobile speech-to-text, whoprovided feedback on five visualization styles. The value of diarization andvisualizing localization was consistent across participants, with everyoneagreeing on the value and potential of directional guidance for groupconversations.</description>
      <author>example@mail.com (Artem Dementyev, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, Alex Olwal)</author>
      <guid isPermaLink="false">2502.08848v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Olympus: A Jumping Quadruped for Planetary Exploration Utilizing Reinforcement Learning for In-Flight Attitude Control</title>
      <link>http://arxiv.org/abs/2503.03574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, Accepted to the IEEE International Conference on  Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;探索低重力的行星体，如月球和火星，允许腿足机器人利用跳跃作为一种高效的运动方式，从而在探测任务中相对于传统探测车具有明显优势。受此启发，本文介绍了Olympus的设计、模拟以及基于学习的空中姿态控制方法，这是一款专为火星引力设计的跳跃式腿足机器人。&lt;h4&gt;背景&lt;/h4&gt;低重力环境下（如月球和火星），腿足机器人可以利用跳跃作为高效的移动方式，并且具有相对于传统探测车的优势&lt;h4&gt;目的&lt;/h4&gt;介绍针对火星重力环境设计、模拟以及基于学习的空中姿态控制方法的Olympus跳跃式腿足机器人的开发。&lt;h4&gt;方法&lt;/h4&gt;首先概述了设计需求，然后详细介绍了如何通过仿真优化机器人从腿部到整体配置的设计，以实现高垂直跳跃、远距离前进跳跃和在空中的姿态重新定向。接着展示了用于跟踪所需空中姿态操作的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;成功跨越模拟与现实之间的差距，进行了广泛的实验研究来测试姿态重新定向。&lt;h4&gt;结论&lt;/h4&gt;Olympus的设计以及基于学习的方法为未来在低重力行星体上的探测任务提供了一个有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring planetary bodies with lower gravity, such as the moon and Mars,allows legged robots to utilize jumping as an efficient form of locomotion thusgiving them a valuable advantage over traditional rovers for exploration.Motivated by this fact, this paper presents the design, simulation, andlearning-based "in-flight" attitude control of Olympus, a jumping legged robottailored to the gravity of Mars. First, the design requirements are outlinedfollowed by detailing how simulation enabled optimizing the robot's design -from its legs to the overall configuration - towards high vertical jumping,forward jumping distance, and in-flight attitude reorientation. Subsequently,the reinforcement learning policy used to track desired in-flight attitudemaneuvers is presented. Successfully crossing the sim2real gap, extensiveexperimental studies of attitude reorientation tests are demonstrated.</description>
      <author>example@mail.com (Jørgen Anker Olsen, Grzegorz Malczyk, Kostas Alexis)</author>
      <guid isPermaLink="false">2503.03574v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation</title>
      <link>http://arxiv.org/abs/2503.03556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了LVIS-Aff数据集和Afford-X模型，以改进基于感知的对象功能推理能力。&lt;h4&gt;背景&lt;/h4&gt;对象功能推理对于任务导向的计划和执行至关重要。然而，现有的计算模型缺乏泛化能力，难以应用于新的场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的大规模数据集（LVIS-Aff）以及一个增强的功能推理模型（Afford-X），以提高对象功能感知推理的准确性和速度，并适用于本地设备的任务操作。&lt;h4&gt;方法&lt;/h4&gt;通过引入LVIS-Aff数据集和使用Verb Attention与Bi-Fusion模块来改进多模态理解，提出了一种端到端可训练的affordance推理模型（Afford-X）。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的Afford-X模型比最好的非LLM方法提高了12.1%的表现，并且相较于作者之前的会议论文也有所进步。此外，它保持了紧凑的参数规模和高速度。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了高效、通用的功能推理模型在本地设备上部署的可能性，并证明其在机器人任务操作中的有效性以及对现实世界应用的意义。&lt;h4&gt;翻译&lt;/h4&gt;物体功能推理能力对于人类和人工智能（AI）的任务导向规划至关重要。现有方法缺乏泛化性，难以处理新场景问题。为此，作者提出了一种大规模数据集LVIS-Aff和一个先进的模型Afford-X，旨在提高基于感知的功能推理性能，并展示了其在实际应用中的优越效果和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object affordance reasoning, the ability to infer object functionalitiesbased on physical properties, is fundamental for task-oriented planning andactivities in both humans and Artificial Intelligence (AI). This capability,required for planning and executing daily activities in a task-oriented manner,relies on commonsense knowledge of object physics and functionalities,extending beyond simple object recognition. Current computational models foraffordance reasoning from perception lack generalizability, limiting theirapplicability in novel scenarios. Meanwhile, comprehensive Large LanguageModels (LLMs) with emerging reasoning capabilities are challenging to deploy onlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, alarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhancethe generalizability of affordance reasoning from perception. Utilizing thisdataset, we develop Afford-X, an end-to-end trainable affordance reasoningmodel that incorporates Verb Attention and Bi-Fusion modules to improvemulti-modal understanding. This model achieves up to a 12.1% performanceimprovement over the best-reported results from non-LLM methods, while alsodemonstrating a 1.2% enhancement compared to our previous conference paper.Additionally, it maintains a compact 187M parameter size and infers nearly 50times faster than the GPT-4V API. Our work demonstrates the potential forefficient, generalizable affordance reasoning models that can be deployed onlocal devices for task-oriented manipulations. We showcase Afford-X'seffectiveness in enabling task-oriented manipulations for robots across varioustasks and environments, underscoring its efficiency and broad implications foradvancing robotics and AI systems in real-world applications.</description>
      <author>example@mail.com (Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao)</author>
      <guid isPermaLink="false">2503.03556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unified Human Localization and Trajectory Prediction with Monocular Vision</title>
      <link>http://arxiv.org/abs/2503.03535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了MonoTransmotion (MT)框架，该框架利用单目相机同时解决定位和预测任务，展示出在真实世界场景中处理嘈杂数据时的稳健性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的轨迹预测模型依赖于经过精制的数据，并且需要特殊设备或手动标注，这使得它们不适用于大多数机器人应用。现有预测器往往过于适应干净观察结果而影响其使用嘈杂输入时的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer框架的方法（MonoTransmotion），该方法仅利用单目相机进行定位和轨迹预测任务，并验证该方法在现实场景中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个包含两个主要模块的框架：鸟瞰图(BEV)定位模块和轨迹预测模块。BEV定位模块使用2D人体姿势估计人的位置，而轨迹预测模块则根据这些估计值预测未来的运动路径。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合训练上述任务并采用统一框架的方法，MT方法在包含嘈杂输入的真实场景中表现更加稳健，并且其性能在数据集中得到验证。在人工准备的数据集上，相比基线模型，MT框架在BEV定位和轨迹预测方面取得了约12%的改进。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在非精制的真实世界数据集上，MT方法维持了类似水平的表现，这突显了其稳健性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的行人轨迹预测模型依赖于经过清洗和整理的数据，通常需要特殊设备或人工标记，这对于机器人应用来说往往是不切实际的。现有的预测器倾向于过度拟合到干净的观察结果上，在处理嘈杂输入时会降低其鲁棒性。在这项工作中，我们提出了基于Transformer框架的MonoTransmotion（MT），该框架仅使用单目相机同时解决定位和预测任务。我们的框架有两个主要模块：鸟瞰图（BEV）定位和轨迹预测。BEV定位模块利用2D人体姿态估计人的位置，并通过一种新的方向损失函数增强，以实现更平滑的顺序定位。轨迹预测模块从这些估算值中预测未来的运动路径。我们展示了通过联合训练这两个任务并使用统一框架的方法，在处理嘈杂输入的真实世界场景中的表现更加稳健。我们在人工准备和非人工准备的数据集上验证了我们的MT网络。在人工准备的数据集上，MT相比基准模型在BEV定位和轨迹预测方面取得了大约12%的改进。而在真实的非精制数据集上，实验结果表明MT保持类似的表现水平，突显其稳健性和泛化能力。代码可在https://github.com/vita-epfl/MonoTransmotion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional human trajectory prediction models rely on clean curated data,requiring specialized equipment or manual labeling, which is often impracticalfor robotic applications. The existing predictors tend to overfit to cleanobservation affecting their robustness when used with noisy inputs. In thiswork, we propose MonoTransmotion (MT), a Transformer-based framework that usesonly a monocular camera to jointly solve localization and prediction tasks. Ourframework has two main modules: Bird's Eye View (BEV) localization andtrajectory prediction. The BEV localization module estimates the position of aperson using 2D human poses, enhanced by a novel directional loss for smoothersequential localizations. The trajectory prediction module predicts futuremotion from these estimates. We show that by jointly training both tasks withour unified framework, our method is more robust in real-world scenarios madeof noisy inputs. We validate our MT network on both curated and non-curateddatasets. On the curated dataset, MT achieves around 12% improvement overbaseline models on BEV localization and trajectory prediction. On real-worldnon-curated dataset, experimental results indicate that MT maintains similarperformance levels, highlighting its robustness and generalization capability.The code is available at https://github.com/vita-epfl/MonoTransmotion.</description>
      <author>example@mail.com (Po-Chien Luan, Yang Gao, Celine Demonsant, Alexandre Alahi)</author>
      <guid isPermaLink="false">2503.03535v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Potential gains of communication-compute-control co-design based performance optimization methods in cyber-physical systems</title>
      <link>http://arxiv.org/abs/2503.03521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种性能优化的方法，通过引入通信和计算特性到应用逻辑中来改善整体系统性能。&lt;h4&gt;背景&lt;/h4&gt;在利用网络和云技术实现工业控制系统时，控制应用程序的确定性会自然降低。这意味着某些不完善可能会被引入控制系统，在干扰期间闭环控制实际上转变为开环控制。&lt;h4&gt;目的&lt;/h4&gt;目的是通过应用可以补偿统计上或保证方式的方法来改进这些开环控制时段的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了三种基于共设计的应用改进方案，这些方案对底层技术的依赖性最小。&lt;h4&gt;主要发现&lt;/h4&gt;共设计方法能够显著提高机器人轨迹在开环控制期间执行的准确性，并且结合使用这些建议的方法可以将轨迹执行时间缩短多达45%。&lt;h4&gt;结论&lt;/h4&gt;通过引入通信和计算特性到应用逻辑中，整体系统性能得到明显改善，特别是在开放环控制周期内表现得更加准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we propose and quantitatively evaluate three performanceoptimization methods that exploit the concept of communication-compute-controlco-design by introducing awareness of communication and compute characteristicsinto the application logic in different ways to improve overall systemperformance. We have implemented a closed-loop control of a robotic arm over awireless network where the controller is deployed into an edge cloudenvironment. When implementing an industrial system that leverages network andcloud technologies, the level of determinism of the control application can bedecreased by nature. This means that some imperfections may be introduced intothe control system, and the closed-loop control in substance changes toopen-loop during disturbances. We aim to improve the performance of theseopen-loop control periods by applying methods that can compensate for theimperfections statistically or in a guaranteed way. We demonstrate thatco-design-based application improvements with minimal dependencies on theunderlying technologies can already yield an order of magnitude gain when itcomes to the accurate execution of the robot trajectories during the openloopcontrol periods. Furthermore, by combining the proposed methods, theperformance improvements add up and can produce up to 45% shorter trajectoryexecutions compared to individual evaluations.</description>
      <author>example@mail.com (Sándor Rácz, Norbert Reider)</author>
      <guid isPermaLink="false">2503.03521v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection</title>
      <link>http://arxiv.org/abs/2503.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures. IEEE International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NeuGrasp是一种基于神经网络的表面重建方法，旨在通过利用背景先验来实现材料无关的手抓取检测。它在透明和镜面物体场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的手抓取方法在处理包含透明或镜面反射物体的情况下遇到挑战，这些方法依赖于准确的深度信息难以应对。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经网络表面重建方法NeuGrasp，以实现更稳健的手抓取检测，并能有效应对具有透明和镜面特性的物体。&lt;h4&gt;方法&lt;/h4&gt;引入了transformer模型和全局先验体积来集成多视角特征与空间编码，通过残差特征增强专注于前景对象并通过占用先验体来改进空间感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在模拟和现实世界场景中NeuGrasp在手抓取方面优于现有的最佳方法，并且保持了相当的重建质量。&lt;h4&gt;结论&lt;/h4&gt;NeuGrasp能够有效处理具有透明或镜面特性的物体，展示出其在复杂环境中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取技术在面对含有透明和镜面反射物体的情况时面临巨大挑战。本文介绍了一种名为NeuGrasp的神经表面重建方法，该方法利用背景先验来实现材料无关的手部抓取检测。通过整合transformer模型与全局先验体积，NeuGrasp能有效聚合多视角特征并进行空间编码，在狭小且视角稀疏条件下具有优秀的表观重建能力。此外，它还能够通过专注于前景物体的残差特征增强及占用体优化来改进空间感知，从而更有效地处理透明和镜面表面对象。在模拟与实际场景中的大量实验表明，NeuGrasp的手部抓取性能超越现有最佳方法，并且具有可比的重建质量。详情请访问https://neugrasp.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic grasping in scenes with transparent and specular objects presentsgreat challenges for methods relying on accurate depth information. In thispaper, we introduce NeuGrasp, a neural surface reconstruction method thatleverages background priors for material-agnostic grasp detection. NeuGraspintegrates transformers and global prior volumes to aggregate multi-viewfeatures with spatial encoding, enabling robust surface reconstruction innarrow and sparse viewing conditions. By focusing on foreground objects throughresidual feature enhancement and refining spatial perception with anoccupancy-prior volume, NeuGrasp excels in handling objects with transparentand specular surfaces. Extensive experiments in both simulated and real-worldscenarios show that NeuGrasp outperforms state-of-the-art methods in graspingwhile maintaining comparable reconstruction quality. More details are availableat https://neugrasp.github.io/.</description>
      <author>example@mail.com (Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang)</author>
      <guid isPermaLink="false">2503.03511v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Benchmark for Optimal Multi-Modal Multi-Robot Multi-Goal Path Planning with Given Robot Assignment</title>
      <link>http://arxiv.org/abs/2503.03509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的机器人路径规划问题的解决方案，用于多机器人在共享工作空间中完成任务的问题，并提出了一个涵盖多种场景基准测试。&lt;h4&gt;背景描述&lt;/h4&gt;多个工业机器人在同一工作空间内共同作业以尽快完成一系列任务。这类设置可被视为一个多模式、多机器人和多目标的路径规划问题。&lt;h4&gt;研究目的&lt;/h4&gt;通过将该问题形式化为单一路径规划问题，提出了一种基准测试方法，并引入了适应于复杂情况下的RRT*和PRM*路径规划器作为基线。&lt;h4&gt;创新方法&lt;/h4&gt;本文的方法不同于以往以优先级处理或假设同步完成任务的方式，而是采用复合空间的路径规划方式，在非离散2D工作环境下也适用，支持动态环境变化，并适用于不同约束条件的异构机器人团队。&lt;h4&gt;关键贡献&lt;/h4&gt;引入了一个多样化的基准测试集，涵盖了具有不同类型、规划时间线和协作任务（如交接）的不同问题实例。同时，改进了RRT*和PRM*路径规划器以适应复杂情况。&lt;h4&gt;结论&lt;/h4&gt;通过提出新的方法和基准测试，为解决多机器人协同工作中的路径规划问题提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;在许多工业机器人的应用中，多个机器人在同一共享工作空间内共同作业，以尽可能快速地完成一系列任务。这类设置可以视为一个多模式、多机器人和多目标的路径规划问题，其中每个机器人必须到达一个有序的目标序列。现有的方法通过优先级处理或假设同步完成任务来解决此类问题，并因此既不是最优也不是完整的。本文将这个问题形式化为单一路径规划问题，并引入了一个基准测试，涵盖了包括具有不同类型机器人、不同规划时间线和协作任务（如交接）在内的多种情况的问题实例。此外，除了这个基准测试，还适应了RRT*和PRM*规划器作为基线方法以处理这些问题。与现有方法不同的是，本文的路径规划器和方案不受限于离散2D工作空间，并支持动态环境变化，在具有不同类型约束条件的异构机器人团队中使用多个模式和目标时也非常适用。基准测试及其规划器的相关视频和代码可在https://vhartman.github.io/mrmg-planning/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many industrial robotics applications, multiple robots are working in ashared workspace to complete a set of tasks as quickly as possible. Suchsettings can be treated as multi-modal multi-robot multi-goal path planningproblems, where each robot has to reach an ordered sequence of goals. Existingapproaches to this type of problem solve this using prioritization or assumesynchronous completion of tasks, and are thus neither optimal nor complete. Weformalize this problem as a single path planning problem and introduce abenchmark encompassing a diverse range of problem instances including scenarioswith various robots, planning horizons, and collaborative tasks such ashandovers. Along with the benchmark, we adapt an RRT* and a PRM* planner toserve as a baseline for the planning problems. Both planners work in thecomposite space of all robots and introduce the required changes to work in oursetting. Unlike existing approaches, our planner and formulation is notrestricted to discretized 2D workspaces, supports a changing environment, andworks for heterogeneous robot teams over multiple modes with differentconstraints, and multiple goals. Videos and code for the benchmark and theplanners is available at https://vhartman.github.io/mrmg-planning/.</description>
      <author>example@mail.com (Valentin N. Hartmann, Tirza Heinle, Stelian Coros)</author>
      <guid isPermaLink="false">2503.03509v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Coordinated Trajectories for Non-stop Flying Carriers Holding a Cable-Suspended Load</title>
      <link>http://arxiv.org/abs/2503.03481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼无人机在空中操作中通常被视为有限耐力的设备，但本文展示了一种方法可以使用连续飞行的三架或多架无人机来保持吊挂负载的恒定姿态。&lt;h4&gt;背景&lt;/h4&gt;现有的多旋翼无人机由于能量限制无法长时间进行空中操作任务。&lt;h4&gt;目的&lt;/h4&gt;探索并实现一种利用连续飞行的非停止型无人机进行高效空中操作的方法，同时确保吊挂负载在动态环境中的稳定。&lt;h4&gt;方法&lt;/h4&gt;{'选择内部力方向': '选择了$n$个特殊线性独立的方向作为载荷抓取矩阵零空间内的内力，在连接载荷悬架点的图中形成一个哈密顿圈。相邻对的方向用于生成作用于不同二维仿射子空间的$n$个力量。', '构建椭圆轨迹': '通过适当的图着色，将每个哈密顿环的边映射到周期坐标上，确保没有相邻坐标在同时显示零导数的情况下仍然可以构建出椭圆形轨迹。'}&lt;h4&gt;主要发现&lt;/h4&gt;这些选择和构造条件保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而使载荷保持静止的同时无人机能够进行持续的运动。&lt;h4&gt;结论&lt;/h4&gt;理论成果通过模拟和实验室实验中的非停止多旋翼无人机进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;多旋翼UAV通常被认为适合空中操作，但由于其有限的耐力限制了长时间的操作任务。这项工作展示了三架或更多连续飞行载体能够保持吊挂载荷恒定姿态的可能性，并且提出了生成协同无间断轨迹的算法。这种方法基于两个支柱：（1）选择$n$个特殊线性独立方向作为载荷抓取矩阵零空间内的内力，形成一个哈密顿圈连接悬架点；（2）在这些子空间中构建椭圆轨迹，在适当的图着色下，每个哈密顿环的边映射到周期坐标上，并确保没有相邻坐标同时显示零导数。结合负载静态条件和悬挂点位置，这些选择保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而在载荷静止的情况下实现载体的永不停歇运动。理论发现已通过模拟和实验室实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multirotor UAVs have been typically considered for aerial manipulation, buttheir scarce endurance prevents long-lasting manipulation tasks. This workdemonstrates that the non-stop flights of three or more carriers are compatiblewith holding a constant pose of a cable-suspended load, thus potentiallyenabling aerial manipulation with energy-efficient non-stop carriers. It alsopresents an algorithm for generating the coordinated non-stop trajectories. Theproposed method builds upon two pillars: (1)~the choice of $n$ special linearlyindependent directions of internal forces within the $3n-6$-dimensionalnullspace of the grasp matrix of the load, chosen as the edges of a Hamiltoniancycle on the graph that connects the cable attachment points on the load.Adjacent pairs of directions are used to generate $n$ forces evolving ondistinct 2D affine subspaces, despite the attachment points being genericallyin 3D; (2)~the construction of elliptical trajectories within these subspacesby mapping, through appropriate graph coloring, each edge of the Hamiltoniancycle to a periodic coordinate while ensuring that no adjacent coordinatesexhibit simultaneous zero derivatives. Combined with conditions for loadstatics and attachment point positions, these choices ensure that each of the$n$ force trajectories projects onto the corresponding cable constraint spherewith non-zero tangential velocity, enabling perpetual motion of the carrierswhile the load is still. The theoretical findings are validated throughsimulations and laboratory experiments with non-stopping multirotor UAVs.</description>
      <author>example@mail.com (Chiara Gabellieri, Antonio Franchi)</author>
      <guid isPermaLink="false">2503.03481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.03480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为SafeVLA的新算法，旨在将安全性融入到视觉-语言-动作模型(VLAS)中，以保护真实世界中的环境、机器人硬件和人类的安全。&lt;h4&gt;背景&lt;/h4&gt;VLAs作为一种通用的机器人策略显示出巨大的潜力，但在实际部署时带来了紧迫的安全挑战，包括对环境、机器人本身以及人类可能造成的物理伤害的风险。&lt;h4&gt;目的&lt;/h4&gt;为了将安全性明确地整合到VLAs中，设计了一种新的算法SafeVLA。&lt;h4&gt;方法&lt;/h4&gt;通过在模拟环境中使用大规模约束学习来有效地平衡安全性和任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;与当前最先进的方法相比，在仿真测试中实现了83.58%的安全性改进和3.85%的任务性能提升；优先考虑安全性可以消除高风险行为并将不安全行为的上限降低到1/35，从而显著减少长尾风险。&lt;h4&gt;结论&lt;/h4&gt;SafeVLA不仅在模拟实验中表现出色，在处理多种未知场景时也展示了良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks. Our data, models and newlyproposed benchmark environment are available athttps://sites.google.com/view/pku-safevla.</description>
      <author>example@mail.com (Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang)</author>
      <guid isPermaLink="false">2503.03480v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets</title>
      <link>http://arxiv.org/abs/2503.03476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的四足机器人技能学习方法PASIST，该方法能够在没有完整专家数据集的情况下自主探索和选择高质量的轨迹，并且能够实现平滑自然的动作转换。&lt;h4&gt;背景&lt;/h4&gt;当前用于四足机器人的模仿学习方法虽然有效，但需要昂贵的数据集来复制专家行为。现有的方法无法处理不同难度的任务以及技能之间的复杂过渡问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自模仿技能过渡方法，以减少对专家数据集的依赖，并提高机器人在各种任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，利用生成对抗自模仿学习（GASIL）框架。此外还开发了一个技能选择模块来缓解模式崩溃问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入新的技能选择机制，可以在不依赖于完整专家数据集的情况下实现平滑自然的动作转换和复杂任务的处理。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了PASIST的有效性，并表明其为专家驱动的学习提供了一种高效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;学习四足机器人的多样化技能面临着重大挑战，如掌握不同技能之间的复杂过渡以及应对难度各异的任务。现有的模仿学习方法虽然成功，但依赖于昂贵的数据集来复制专家行为。受自我反思学习的启发，我们提出了一种新的方法PASIST，它可以消除对完整专家数据集的需求。PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，并利用生成对抗自模仿学习（GASIL）框架。为了进一步增强学习效果，我们开发了一个技能选择模块来平衡难度不同的技能权重以减少模式崩溃问题。通过这些方法，PASIST能够在没有完整专家数据集的情况下复制对应于目标姿势的技能并且实现平滑自然的动作转换。在仿真平台和Solo 8机器人上的评估确认了PASIST的有效性，并为专家驱动的学习提供了有效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills for quadruped robots presents significant challenges,such as mastering complex transitions between different skills and handlingtasks of varying difficulty. Existing imitation learning methods, whilesuccessful, rely on expensive datasets to reproduce expert behaviors. Inspiredby introspective learning, we propose Progressive Adversarial Self-ImitationSkill Transition (PASIST), a novel method that eliminates the need for completeexpert datasets. PASIST autonomously explores and selects high-qualitytrajectories based on predefined target poses instead of demonstrations,leveraging the Generative Adversarial Self-Imitation Learning (GASIL)framework. To further enhance learning, We develop a skill selection module tomitigate mode collapse by balancing the weights of skills with varying levelsof difficulty. Through these methods, PASIST is able to reproduce skillscorresponding to the target pose while achieving smooth and natural transitionsbetween them. Evaluations on both simulation platforms and the Solo 8 robotconfirm the effectiveness of PASIST, offering an efficient alternative toexpert-driven learning.</description>
      <author>example@mail.com (Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang)</author>
      <guid isPermaLink="false">2503.03476v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Generative Artificial Intelligence in Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2503.03464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;综述了机器人操作领域中生成学习模型的最新进展，概述了这些模型的关键挑战，并介绍了几种生成式模型范式及其应用。&lt;h4&gt;背景&lt;/h4&gt;在机器人操作中面临的主要瓶颈包括数据不足和采集效率低、长期复杂的任务规划以及跨不同环境进行稳健策略学习所需的多模态推理能力。&lt;h4&gt;目的&lt;/h4&gt;介绍用于解决上述关键问题的生成式模型，如GANs、VAEs、扩散模型、概率流模型及自回归模型，并探讨它们的优势与局限性。&lt;h4&gt;方法&lt;/h4&gt;将生成模型的应用分为基础层（数据和奖励生成）、中间层（语言、代码、视觉与状态生成）以及策略层（抓取和轨迹生成），详细讨论每个层次的代表性工作。&lt;h4&gt;主要发现&lt;/h4&gt;指出未来研究需提高数据利用效率，更好地处理长期任务，并在多样化机器人应用场景中提升泛化能力。&lt;h4&gt;结论&lt;/h4&gt;强调了需要改进现有方法以应对机器人操作中的关键挑战，并提供了相关的资源链接供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;该综述提供了一种关于最近用于机器人操作的生成学习模型进展的全面回顾，这些模型旨在解决领域内的关键问题。概述了几种生成式模型（如GANs、VAEs等）及其应用层级，并指出了未来研究的方向和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This survey provides a comprehensive review on recent advancements ofgenerative learning models in robotic manipulation, addressing key challengesin the field. Robotic manipulation faces critical bottlenecks, includingsignificant challenges in insufficient data and inefficient data acquisition,long-horizon and complex task planning, and the multi-modality reasoningability for robust policy learning performance across diverse environments. Totackle these challenges, this survey introduces several generative modelparadigms, including Generative Adversarial Networks (GANs), VariationalAutoencoders (VAEs), diffusion models, probabilistic flow models, andautoregressive models, highlighting their strengths and limitations. Theapplications of these models are categorized into three hierarchical layers:the Foundation Layer, focusing on data generation and reward generation; theIntermediate Layer, covering language, code, visual, and state generation; andthe Policy Layer, emphasizing grasp generation and trajectory generation. Eachlayer is explored in detail, along with notable works that have advanced thestate of the art. Finally, the survey outlines future research directions andchallenges, emphasizing the need for improved efficiency in data utilization,better handling of long-horizon tasks, and enhanced generalization acrossdiverse robotic scenarios. All the related resources, including researchpapers, open-source data, and projects, are collected for the community inhttps://github.com/GAI4Manipulation/AwesomeGAIManipulation</description>
      <author>example@mail.com (Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen)</author>
      <guid isPermaLink="false">2503.03464v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REACT: Real-time Efficient Attribute Clustering and Transfer for Updatable 3D Scene Graph</title>
      <link>http://arxiv.org/abs/2503.03412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT框架通过实时属性聚类和迁移技术，实现了在动态环境中对3D场景图中的对象节点进行重新定位。&lt;h4&gt;背景&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为传统网格地图的有前途替代品出现，它结合了高效的内存使用和丰富的特征表示。&lt;h4&gt;目的&lt;/h4&gt;引入REACT框架以实现在动态环境中实时重新定位对象节点，并保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;REACT采用了一种新颖的方法来比较对象实例，该方法基于三元损失训练的嵌入模型进行。此方法促进了实例聚类和匹配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，REACT能够在维护计算效率的同时重新定位物体。&lt;h4&gt;结论&lt;/h4&gt;REACT框架将作为开源项目公开发布，推动可重用性和更新性的3D场景图技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为一种有前途的替代品出现了，它可以结合高效的内存使用和丰富的特征表示。然而，大多数应用于该领域的努力都局限于静态世界。这项工作介绍了REACT框架，该框架能够有效地执行实时属性聚类和转移以重新定位3DSG中的对象节点。REACT采用了一种新颖的方法来比较对象实例，这种方法基于三元损失训练的嵌入模型进行。实验结果显示，REACT能够在维护计算效率的同时重新定位物体。REACT框架作为开源项目将被发布，这将促进可重用性和更新性的3D场景图技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern-day autonomous robots need high-level map representations to performsophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as apromising alternative to traditional grid maps, blending efficient memory useand rich feature representation. However, most efforts to apply them have beenlimited to static worlds. This work introduces REACT, a framework thatefficiently performs real-time attribute clustering and transfer to relocalizeobject nodes in a 3DSG. REACT employs a novel method for comparing objectinstances using an embedding model trained on triplet loss, facilitatinginstance clustering and matching. Experimental results demonstrate that REACTis able to relocalize objects while maintaining computational efficiency. TheREACT framework's source code will be available as an open-source project,promoting further advancements in reusable and updatable 3DSGs.</description>
      <author>example@mail.com (Phuoc Nguyen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2503.03412v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Navigating Intelligence: A Survey of Google OR-Tools and Machine Learning for Global Path Planning in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2503.03338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文深入研究了无人地面车辆的全局路径规划（GPP），特别是在自主采矿采样机器人ROMIE中的应用。通过解决旅行商问题优化成本和时间，旨在提高其运营效率。&lt;h4&gt;背景&lt;/h4&gt;全球路径规划对于自动化的矿业采样机器人的性能至关重要，特别是通过解决复杂的图论挑战——旅行商问题来实现高效路径覆盖。&lt;h4&gt;目的&lt;/h4&gt;开发、评估并改进一种低成本的软件和网络应用程序以优化全局路径规划。研究重点在于运用和测试Google运营研究工具的能力，并首次结合强化学习技术进行比较分析。&lt;h4&gt;方法&lt;/h4&gt;深入对比分析Google OR-Tools中的各种优化算法，通过实验确定Q-Learning等方法相对于OR-Tools的计算有效性和实际应用效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，Q-Learning在测试数据集上表现出色，平均偏离最优解仅为1.2%，优于其他比较方法。&lt;h4&gt;结论&lt;/h4&gt;Q-Learning是解决全局路径规划问题中最有效的策略之一，它展示了相对于传统优化算法的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;我们提供了一项关于无人地面车辆全球路径规划（GPP）的新颖深入研究，聚焦于一个自主采矿采样机器人ROMIE的应用。GPP对于ROMIE的最佳性能是至关重要的，这被转化为解决旅行商问题——这是一个复杂的图论挑战，对于确定覆盖矿业场地内所有采样地点的最有效路线至关重要。这个问题的核心在于通过优化成本和时间来提高ROMIE的操作效率，并且在与人力竞争时保持其竞争力。本研究的主要目的是通过开发、评估并改进一种低成本软件和网络应用程序来推进GPP的研究。我们深入比较分析了Google运营研究（OR）-Tools中的优化算法，我们的研究旨在首次将强化学习技术应用于这些工具中，以应用和测试它们的能力限制，并将这些方法与OR-Tools进行对比，评估其计算有效性和实际应用效率。本项研究试图提供关于每种技术的有效性及实际应用情况的见解。我们发现Q-Learning在所有数据集上表现出色，平均仅偏离最优解1.2%，因此它被证明是最优策略之一。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/aisy.202300840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We offer a new in-depth investigation of global path planning (GPP) forunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPPis essential for ROMIE's optimal performance, which is translated into solvingthe traveling salesman problem, a complex graph theory challenge that iscrucial for determining the most effective route to cover all samplinglocations in a mining field. This problem is central to enhancing ROMIE'soperational efficiency and competitiveness against human labor by optimizingcost and time. The primary aim of this research is to advance GPP bydeveloping, evaluating, and improving a cost-efficient software and webapplication. We delve into an extensive comparison and analysis of Googleoperations research (OR)-Tools optimization algorithms. Our study is driven bythe goal of applying and testing the limits of OR-Tools capabilities byintegrating Reinforcement Learning techniques for the first time. This enablesus to compare these methods with OR-Tools, assessing their computationaleffectiveness and real-world application efficiency. Our analysis seeks toprovide insights into the effectiveness and practical application of eachtechnique. Our findings indicate that Q-Learning stands out as the optimalstrategy, demonstrating superior efficiency by deviating only 1.2% on averagefrom the optimal solutions across our datasets.</description>
      <author>example@mail.com (Alexandre Benoit, Pedram Asef)</author>
      <guid isPermaLink="false">2503.03338v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</title>
      <link>http://arxiv.org/abs/2503.03282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于无人水面艇（USV）自主视觉靠泊的新型监督学习流水线，并采用了自动标注技术。通过设计了一个无需人工标记的数据收集流程，开发了神经靠泊姿态估计器（NDPE），该系统能够实现对相对靠泊姿态的预测。&lt;h4&gt;背景&lt;/h4&gt;无人水面船在环境监测和河流建模等水上作业中被广泛应用，但在港口或站点实现精确自主靠岸仍面临挑战，这通常需要远程人类控制或外部定位系统的辅助以确保准确性和安全性。这种依赖限制了USV完全自动化的潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新颖的监督学习流水线和自动标注技术，用于提升无人水面艇（USVs）在视觉引导下的自主靠泊能力。&lt;h4&gt;方法&lt;/h4&gt;设计了一种自动化数据收集流程来生成相对姿态和图像对的数据集，并提出了神经靠泊姿态估计器NDPE以实现不需要手工特征工程、相机校准或外部标记的相对靠泊姿态预测。此外，该系统能够准确地预测实际水环境中的相对靠泊姿态，支持基于位置的视觉伺服（PBVS）和低级运动控制器的有效实施。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，NDPE对距离变化和USV速度扰动具有鲁棒性，并且在真实水域环境中验证了该解决方案的有效性和可操作性。&lt;h4&gt;结论&lt;/h4&gt;提出的解决方案能够有效地处理现实世界中的自主靠泊任务，证明了其强大的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned Surface Vehicles (USVs) are increasingly applied to water operationssuch as environmental monitoring and river-map modeling. It faces a significantchallenge in achieving precise autonomous docking at ports or stations, stillrelying on remote human control or external positioning systems for accuracyand safety which limits the full potential of human-out-of-loop deployment forUSVs.This paper introduces a novel supervised learning pipeline with theauto-labeling technique for USVs autonomous visual docking. Firstly, wedesigned an auto-labeling data collection pipeline that appends relative poseand image pair to the dataset. This step does not require conventional manuallabeling for supervised learning. Secondly, the Neural Dock Pose Estimator(NDPE) is proposed to achieve relative dock pose prediction without the needfor hand-crafted feature engineering, camera calibration, and peripheralmarkers. Moreover, The NDPE can accurately predict the relative dock pose inreal-world water environments, facilitating the implementation ofPosition-Based Visual Servo (PBVS) and low-level motion controllers forefficient and autonomous docking.Experiments show that the NDPE is robust tothe disturbance of the distance and the USV velocity. The effectiveness of ourproposed solution is tested and validated in real-world water environments,reflecting its capability to handle real-world autonomous docking tasks.</description>
      <author>example@mail.com (Yijie Chu, Ziniu Wu, Yong Yue, Eng Gee Lim, Paolo Paoletti, Xiaohui Zhu)</author>
      <guid isPermaLink="false">2503.03282v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions</title>
      <link>http://arxiv.org/abs/2503.03262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着自动驾驶车辆在现代交通系统中集成潜力的增加，确保其在动态环境中的安全导航变得至关重要。为了保证安全性并防止碰撞，自动驾驶汽车必须能够准确预测周围交通代理的轨迹。&lt;h4&gt;背景&lt;/h4&gt;过去十年来，学术界和工业界都投入了大量的精力设计精确的轨迹预测解决方案，这些努力产生了一系列不同的方法，并提出了关于不同方法之间差异以及轨迹预测挑战是否已经完全解决的问题。&lt;h4&gt;目的&lt;/h4&gt;本文回顾了近期大量的轨迹预测方法，并提出了一种分类现有解决方案的方法。此外还提供了一个预测流水线的一般概述，涵盖了输入和输出模式、建模特征及预测范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一个详细的分类法来对现有的预测方法进行分类，并讨论了一些活跃的研究领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过文献综述，指出了当前研究中存在的问题以及未解决的挑战。该论文强调了在轨迹预测方面仍存在的研究空白和未来方向。&lt;h4&gt;结论&lt;/h4&gt;文章探讨并总结了自动驾驶车辆中的轨迹预测领域的现状及其面临的挑战，并提出了一些重要的开放性问题需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;随着自主驾驶车辆在现代交通系统中大规模整合的潜力不断增长，确保其在动态环境下的安全导航变得至关重要。为了保证安全性以及防止碰撞的发生，自主驾驶汽车必须具备准确预测周围交通代理轨迹的能力。在过去十年里，学术界和工业界已经投入了大量资源来设计精确的路径预测解决方案，这些努力催生了许多不同的方法，并引发了关于不同技术之间差异以及是否所有相关的挑战都已经被解决的问题。本文对近期大量的轨迹预测方法进行了回顾，并为现有解决方案制定了分类体系。文章还提供了一个涵盖输入输出模式、建模特征及预测范式的预测流程概览。此外，该文讨论了当前活跃的研究领域，回答了一些研究问题，并强调在这一领域里仍然存在的研究缺口和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the potential for autonomous vehicles to be integrated on a large scaleinto modern traffic systems continues to grow, ensuring safe navigation indynamic environments is crucial for smooth integration. To guarantee safety andprevent collisions, autonomous vehicles must be capable of accuratelypredicting the trajectories of surrounding traffic agents. Over the pastdecade, significant efforts from both academia and industry have been dedicatedto designing solutions for precise trajectory forecasting. These efforts haveproduced a diverse range of approaches, raising questions about the differencesbetween these methods and whether trajectory prediction challenges have beenfully addressed. This paper reviews a substantial portion of recent trajectoryprediction methods and devises a taxonomy to classify existing solutions. Ageneral overview of the prediction pipeline is also provided, covering inputand output modalities, modeling features, and prediction paradigms discussed inthe literature. In addition, the paper discusses active research areas withintrajectory prediction, addresses the posed research questions, and highlightsthe remaining research gaps and challenges.</description>
      <author>example@mail.com (Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa, Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji)</author>
      <guid isPermaLink="false">2503.03262v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SCORE: Saturated Consensus Relocalization in Semantic Line Maps</title>
      <link>http://arxiv.org/abs/2503.03254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 14 figurs, arxiv version for paper submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语义标注3D线的场景无关且轻量级视觉重定位框架。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉重定位方法难以处理极高的异常值比率（超过99.5%），特别是在语义匹配中出现的一对多模糊情况。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够准确估计姿态并在经典共识最大化框架失效时仍能工作的算法。&lt;h4&gt;方法&lt;/h4&gt;引入饱和一致性最大化的形式化方法，并提出了一种快速全局求解器，利用严格的区间分析结果保证了精度和计算效率。此外，还构建了一个用于构造语义3D线图的流水线。&lt;h4&gt;主要发现&lt;/h4&gt;通过在ScanNet++数据集上的广泛实验验证了所提框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的饱和一致性最大化的形式化方法能够在极高的异常值比率下准确估计姿态，并且整个框架结合了鲁棒估算和实用工程见解，具有很高的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原文是：这是提交给IEEE/RSJ IROS 2025的一篇论文的arxiv版本。我们提出了一种场景无关并且轻量级的视觉重定位框架，该框架利用语义标注的3D线作为紧凑的地图表示。在我们的框架中，机器人通过捕获单个图像、提取2D线、将其与地图中的语义相似的3D线关联起来，并解决一个鲁棒的透视-n-线问题来自我定位。为了解决由于语义匹配的一对多模糊性导致异常值比率极高（超过99.5%）的问题，我们引入了饱和一致性最大化(Sat-CM)形式化方法，它在经典共识最大化的框架失效时仍能实现准确的姿态估计。此外，还提出了快速全局求解器以解决所提出的Sat-CM问题，利用严格的区间分析结果确保精度和计算效率的同时性。另外，开发了一条管道来构建使用带姿态的深度图像的语义3D线地图。为了验证我们框架的有效性，并整合了我们在鲁棒估计方面的创新成果以及实用工程见解，在ScanNet++数据集上进行了广泛的实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the arxiv version for our paper submitted to IEEE/RSJ IROS 2025. Wepropose a scene-agnostic and light-weight visual relocalization framework thatleverages semantically labeled 3D lines as a compact map representation. In ourframework, the robot localizes itself by capturing a single image, extracting2D lines, associating them with semantically similar 3D lines in the map, andsolving a robust perspective-n-line problem. To address the extremely highoutlier ratios~(exceeding 99.5\%) caused by one-to-many ambiguities in semanticmatching, we introduce the Saturated Consensus Maximization~(Sat-CM)formulation, which enables accurate pose estimation when the classic ConsensusMaximization framework fails. We further propose a fast global solver to theformulated Sat-CM problems, leveraging rigorous interval analysis results toensure both accuracy and computational efficiency. Additionally, we develop apipeline for constructing semantic 3D line maps using posed depth images. Tovalidate the effectiveness of our framework, which integrates our innovationsin robust estimation and practical engineering insights, we conduct extensiveexperiments on the ScanNet++ dataset.</description>
      <author>example@mail.com (Haodong Jiang, Xiang Zheng, Yanglin Zhang, Qingcheng Zeng, Yiqian Li, Ziyang Hong, Junfeng Wu)</author>
      <guid isPermaLink="false">2503.03254v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter Trajectory Generation</title>
      <link>http://arxiv.org/abs/2503.03252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种时空迭代优化框架，以提高四旋翼无人机轨迹规划的效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;目前，无人机轨迹优化问题中存在约束合规性和计算效率提升之间的固有折衷。&lt;h4&gt;目的&lt;/h4&gt;增强无人机轨迹优化性能。&lt;h4&gt;方法&lt;/h4&gt;利用B样条表示无人机轨迹，并通过严格控制点上的约束执行来确保严格的飞行安全；然后推导出一系列通过时空解耦和约束线性化得出的QP-LP子问题；最后采用一种结合指导梯度的迭代优化策略，在不同场景下获得高性能无人机轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果以及实地实验验证了所提出的优化框架在生成既安全又快速的轨迹方面的效率和高性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效地解决无人机轨迹规划中约束合规性和计算效率之间的矛盾，提供了一种有效、高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高效且安全的轨迹规划对于四旋翼无人飞行器的应用至关重要。当前问题在于如何在满足约束的同时提升计算效率。为改善无人机轨迹优化性能，作者提出了一种时空迭代优化框架。此方法利用B样条表示轨迹，并通过控制点上的严格约束来确保安全性；然后推导了一系列子问题并采用一种结合指导梯度的策略；最终实验结果验证了该框架的有效性与高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient and safe trajectory planning plays a critical role in theapplication of quadrotor unmanned aerial vehicles. Currently, the inherenttrade-off between constraint compliance and computational efficiencyenhancement in UAV trajectory optimization problems has not been sufficientlyaddressed. To enhance the performance of UAV trajectory optimization, wepropose a spatial-temporal iterative optimization framework. Firstly, B-splinesare utilized to represent UAV trajectories, with rigorous safety assuranceachieved through strict enforcement of constraints on control points.Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling andconstraint linearization is derived. Finally, an iterative optimizationstrategy incorporating guidance gradients is employed to obtainhigh-performance UAV trajectories in different scenarios. Both simulation andreal-world experimental results validate the efficiency and high-performance ofthe proposed optimization framework in generating safe and fast trajectories.Our source codes will be released for community reference athttps://hitsz-mas.github.io/STORM</description>
      <author>example@mail.com (Jinhao Zhang, Zhexuan Zhou, Wenlong Xia, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2503.03252v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots</title>
      <link>http://arxiv.org/abs/2503.03234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 25. 8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。&lt;h4&gt;背景&lt;/h4&gt;人类能够仅通过触摸传达不同的信息，为使机器人理解社交触感能力增加了一种新的沟通方式。&lt;h4&gt;目的&lt;/h4&gt;构建一个社会手势数据集并提取用于分类的时间特征，以更好地了解人与机器人的社交互动，并促进更加自然和有效的交流。&lt;h4&gt;方法&lt;/h4&gt;利用多参与者建立了一个社会手势数据集，并收集了在实际环境中应用于人形机器人上的真实世界数据。&lt;h4&gt;主要发现&lt;/h4&gt;该系统为人类与机器人的社交触感提供了有价值的见解，有助于推动更自然且有效的人机交互系统的发展。&lt;h4&gt;结论&lt;/h4&gt;通过使用集成在人形机器人手臂上的基于织物的大规模触觉传感器，可以实现对社会手势的有效识别和理解。&lt;h4&gt;翻译&lt;/h4&gt;人类能够仅通过触摸传达不同的信息。为使机器人具有理解社交触摸的能力，增加了一种新的沟通方式。本文介绍了一个使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。我们建立了包含多个参与者的社会手势数据集，并提取了时间特征进行分类。通过在人形机器人的实际环境中收集数据，我们的系统提供了人类与机器人社交触摸有价值的见解，进一步推进了更自然和有效的沟通的人机交互系统的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans are able to convey different messages using only touch. Equippingrobots with the ability to understand social touch adds another modality inwhich humans and robots can communicate. In this paper, we present a socialgesture recognition system using a fabric-based, large-scale tactile sensorintegrated onto the arms of a humanoid robot. We built a social gesture datasetusing multiple participants and extracted temporal features for classification.By collecting real-world data on a humanoid robot, our system provides valuableinsights into human-robot social touch, further advancing the development ofspHRI systems for more natural and effective communication.</description>
      <author>example@mail.com (Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan)</author>
      <guid isPermaLink="false">2503.03234v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OpenGV 2.0: Motion prior-assisted calibration and SLAM with vehicle-mounted surround-view systems</title>
      <link>http://arxiv.org/abs/2503.03230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM。&lt;h4&gt;背景&lt;/h4&gt;车载全景相机系统通常只配备朝向一个方向的一台相机，并且视场重叠有限。&lt;h4&gt;目的&lt;/h4&gt;针对上述问题，文章提出了三个优化模块来解决实际在线校准、可靠的前端初始化和准确的后端优化问题。&lt;h4&gt;方法&lt;/h4&gt;提出的三种优化模块共同利用了与乘客车辆运动特性相关的运动先验知识。这些模块在避免常见于Ackermann运动中的部分不可观察性方面表现出色。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入的消融研究，验证了所提出的方法的有效性和优越性，并且通过应用到具有挑战性的大规模公开数据集上进一步证实了整个框架的实际有效性。&lt;h4&gt;结论&lt;/h4&gt;该模块构建了一个专为Ackermann车辆在城市环境中运行而设计的新全景相机SLAM系统。接受后，整个框架将作为OpenGV库扩展的一部分开源发布。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM问题。针对这些问题，作者提出了三个模块：通过简单的双视图几何实现外定向的实际在线校准；相对位移的可靠前端初始化；以及使用连续时间轨迹模型进行准确后端优化。这些模块利用了与乘客车辆运动特性相关的运动先验知识，并且在解决常见于Ackermann运动中的部分不可观察性方面表现出色。通过深入研究，验证了整个框架的有效性和优越性，并成功应用于公开的大型数据集上。论文接受后，该框架将作为OpenGV库扩展的一部分开源发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present paper proposes optimization-based solutions to visual SLAM with avehicle-mounted surround-view camera system. Owing to their original use-case,such systems often only contain a single camera facing into either directionand very limited overlap between fields of view. Our novelty consist of threeoptimization modules targeting at practical online calibration of exteriororientations from simple two-view geometry, reliable front-end initializationof relative displacements, and accurate back-end optimization using acontinuous-time trajectory model. The commonality between the proposed modulesis given by the fact that all three of them exploit motion priors that arerelated to the inherent non-holonomic characteristics of passenger vehiclemotion. In contrast to prior related art, the proposed modules furthermoreexcel in terms of bypassing partial unobservabilities in the transformationvariables that commonly occur for Ackermann-motion. As a further contribution,the modules are built into a novel surround-view camera SLAM system thatspecifically targets deployment on Ackermann vehicles operating in urbanenvironments. All modules are studied in the context of in-depth ablationstudies, and the practical validity of the entire framework is supported by asuccessful application to challenging, large-scale publicly available onlinedatasets. Note that upon acceptance, the entire framework is scheduled foropen-source release as part of an extension of the OpenGV library.</description>
      <author>example@mail.com (Kun Huang, Yifu Wang, Si'ao Zhang, Zhirui Wang, Zhanpeng Ouyang, Zhenghua Yu, Laurent Kneip)</author>
      <guid isPermaLink="false">2503.03230v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment</title>
      <link>http://arxiv.org/abs/2503.03208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种基于强化学习的自适应脱离模型，该模型通过高效的动作掩码来解决机器人清扫器在拥挤和狭窄环境中遇到死区问题时的传统规划方法失效的问题。&lt;h4&gt;背景&lt;/h4&gt;自主导航是室内环境下机器人吸尘器的基本任务。由于其核心功能是在整个区域进行清洁，因此机器人不可避免地会遇到复杂环境约束、高维搜索空间以及高度复杂的机动操作导致的死区问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的脱离模型和训练策略来提高机器人从死区内成功逃脱的能力，并减少碰撞次数。&lt;h4&gt;方法&lt;/h4&gt;[{'嵌入式逃逸模型': '利用基于强化学习的政策并结合高效的行动掩码来解决死区问题。'}, {'混合培训策略': '为了解决训练过程中稀疏奖励的问题，引入了一种混合训练策略以提高学习效率。'}, {'新的动作表示方法': '设计了一种新的动作表达方式，通过统一的转弯半径重新塑造离散的动作空间，有效地处理冗余和无效的操作选择。'}, {'行动掩码策略': '开发了一种行动掩码策略来快速选择有效操作，在精度与效率之间实现平衡。'}]&lt;h4&gt;主要发现&lt;/h4&gt;['在一系列真实世界实验中，配备了激光雷达、惯性测量单元以及双轮编码器的机器人成功地从多个难度等级中的死区逃脱。', '相较于其他路径规划和强化学习方法，所提出的方法在成功率和碰撞避免方面表现优越。']&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于强化学习的动作掩码策略能有效帮助机器人吸尘器解决复杂环境中的死区问题，并且在实际测试中展示了卓越的性能。&lt;h4&gt;翻译&lt;/h4&gt;自主导航对于室内环境中运行的自动扫地机器人的功能至关重要，尤其是在它们需要清扫拥挤和狭窄区域的时候。由于存在复杂的环境限制、高维搜索空间以及困难的动作需求，现有规划方法往往无法有效解决死区问题。为了克服这些挑战，本文提出了一种基于强化学习的新模型，结合高效的行动掩码策略来优化机器人在复杂条件下的逃脱能力。研究还引入了混合训练政策以解决训练过程中稀疏奖励的问题，并设计了一个新的动作表示法以简化操作选择，同时开发出一种有效筛选可行动的算法，平衡精确度与效率。实验表明，在装备有激光测距仪、惯性测量单元以及双轮编码器的真实场景中，该机器人能够有效地从各种难度级别的死区逃脱出来。比较测试显示了在成功率和碰撞避免方面本方法比其他路径规划或强化学习策略更加优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation is a fundamental task for robot vacuum cleaners inindoor environments. Since their core function is to clean entire areas, robotsinevitably encounter dead zones in cluttered and narrow scenarios. Existingplanning methods often fail to escape due to complex environmental constraints,high-dimensional search spaces, and high difficulty maneuvers. To address thesechallenges, this paper proposes an embodied escaping model that leveragesreinforcement learning-based policy with an efficient action mask for dead zoneescaping. To alleviate the issue of the sparse reward in training, we introducea hybrid training policy that improves learning efficiency. In handlingredundant and ineffective action options, we design a novel actionrepresentation to reshape the discrete action space with a uniform turningradius. Furthermore, we develop an action mask strategy to select valid actionquickly, balancing precision and efficiency. In real-world experiments, ourrobot is equipped with a Lidar, IMU, and two-wheel encoders. Extensivequantitative and qualitative experiments across varying difficulty levelsdemonstrate that our robot can consistently escape from challenging dead zones.Moreover, our approach significantly outperforms compared path planning andreinforcement learning methods in terms of success rate and collisionavoidance.</description>
      <author>example@mail.com (Han Zheng, Jiale Zhang, Mingyang Jiang, Peiyuan Liu, Danni Liu, Tong Qin, Ming Yang)</author>
      <guid isPermaLink="false">2503.03208v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RoboBERT: An End-to-end Multimodal Robotic Manipulation Model</title>
      <link>http://arxiv.org/abs/2502.07837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Embodied intelligence融合了多种模态，使代理能够同时理解图像、语言和行动。然而，现有的模型依赖于额外的数据集或广泛的预训练以最大化性能提升，这需要大量的训练时间和昂贵的硬件成本。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态机器人模型通常需要额外的数据集或大规模的基础模型来达到高性能，并且消耗大量资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端机器人操作模型RoboBERT及其独特的训练策略，旨在提高效率并减少对大型数据集和基础模型的需求。&lt;h4&gt;方法&lt;/h4&gt;该模型使用基于CNN的扩散策略，通过分离不同模态的训练过程来增强和稳定模型的有效性。同时强调数据增强的重要性，并验证了多种技术以显著提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;RoboBERT在CALVIN基准测试中的ABCD → D任务中实现了4.52的平均长度，创下了新的SOTA记录；当应用于真实机器人时，该模型展示了比其他使用相同数据训练的方法更高的成功率。&lt;h4&gt;结论&lt;/h4&gt;通过这些概念和方法论，RoboBERT表现出广泛的灵活性和兼容性，并为轻量级多模态机器人模型的发展做出了重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新的端到端的机器人操作模型——RoboBERT及其独特的培训策略。该模型旨在解决现有模型由于需要额外的数据集或大量预训练而导致的时间和资源消耗问题，通过采用基于CNN的扩散政策，并强调数据增强的重要性来提高性能。实验结果表明，RoboBERT在基准测试中取得了新纪录的成功率，并且在实际应用中也优于其他方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence integrates multiple modalities, enabling agents tounderstand images, language, and actions simultaneously. However, existingmodels always depend on additional datasets or extensive pre-training tomaximize performance improvements, consuming abundant training time andexpensive hardware cost. To tackle this issue, we present RoboBERT, a novelend-to-end robotic manipulation model integrated with a unique trainingstrategy. This model utilizes a CNN-based diffusion policy, enhancing andstabilizing the effectiveness of this model by separating training processesfor different modalities. It also underscores the importance of dataaugmentation, verifying various techniques to significantly boost performance.Unlike models that depend on extra data or large foundation models, RoboBERTachieves a highly competitive success rate while using only language-labeledexpert demonstrations and maintaining a relatively smaller model size.Specifically, RoboBERT achieves an average length of 4.52 on the CALVINbenchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art(SOTA) record. Furthermore, when tested on a real robot, the model demonstratessuperior performance, achieving a higher success rate than other methodstrained with the same data. We propose that these concepts and methodologies ofRoboBERT demonstrate extensive versatility and compatibility, contributingsignificantly to the development of lightweight multimodal robotic models. Thecode can be accessed on https://github.com/PeterWangsicheng/RoboBERT</description>
      <author>example@mail.com (Sicheng Wang, Jianhua Shan, Jianwei Zhang, Haozhang Gao, Hailiang Han, Yipeng Chen, Kang Wei, Chengkun Zhang, Kairos Wong, Jie Zhao, Lei Zhao, Bin Fang)</author>
      <guid isPermaLink="false">2502.07837v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
  <item>
      <title>Probing a Quarkophobic ${\mathbf{W}}^\prime$ at the High-Luminosity LHC via Vector Boson Fusion and Lorentz-Equivariant Point Cloud Learning</title>
      <link>http://arxiv.org/abs/2502.16630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过弱玻色子融合方式探测标准模型之外的W'玻色子生产，使用点云学习技术并引入新的洛伦兹等变几何代数变换器提高信号敏感度。&lt;h4&gt;背景&lt;/h4&gt;在标准模型中添加一个质量较大的、几乎不与夸克耦合的带电矢量规范玻色子W'可以解决诸如B介子异常和W玻色子质量测量差异等问题。&lt;h4&gt;目的&lt;/h4&gt;通过弱相互作用过程研究W'玻色子的产生，利用大型强子对撞机中的质子-质子碰撞数据进行研究，并采用新的学习技术提高检测信号的能力。&lt;h4&gt;方法&lt;/h4&gt;在一种简化模型中工作，该模型假设W'玻色子具有较大的衰变宽度并考虑两个喷注、大的缺失横贯动量和一个轻味道的最终态。使用点云学习技术中的新洛伦兹等变几何代数变换器。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，新型方法相比于传统方法显著提高了信号敏感度。&lt;h4&gt;结论&lt;/h4&gt;引入新的计算工具（即Lorentz-Equivariant Geometric Algebra Transformer）使得探测W'玻色子更加有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The addition of a heavy charged vector gauge boson ${\mathbf{W}}^\prime$ tothe Standard Model (SM) with negligible quark couplings ("quarkophobic") andtriple gauge couplings can address issues with the SM, such as the B-mesonanomalies and recent discrepancies in the W boson mass measurements. We presenta phenomenology study probing ${\mathbf{W}}^\prime$ production through weakboson fusion in proton-proton collisions at the Large Hadron Collider. Weoperate under a simplified model with a large ${\mathbf{W}}^\prime$ decay widthand consider final states with two jets, large missing transverse momentum, andone light lepton. Notably, we use point cloud learning for the first time in aBSM search$\unicode{x2014}$specifically, a novel Lorentz-Equivariant GeometricAlgebra Transformer$\unicode{x2014}$providing significant improvement in signalsensitivity compared to traditional methods.</description>
      <author>example@mail.com (U. S. Qureshi, A. Gurrola, J. D. Ruiz-Álvarez)</author>
      <guid isPermaLink="false">2502.16630v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Audio Visual Segmentation Through Text Embeddings</title>
      <link>http://arxiv.org/abs/2502.16359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为AV2T-SAM的新框架，该框架将音频特征与预训练的文本提示式SAM的文本嵌入空间连接起来。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉分割（AVS）的目标是定位和从视频帧中分离出发出声音的对象。由于手工注释昂贵，研究者面临数据集有限的问题。&lt;h4&gt;目的&lt;/h4&gt;通过利用丰富的文本图像配对数据集中学习到的多模态对应关系来增强视听对齐，并提出一种新特征以强调音频和视觉模式之间的共享语义同时过滤无关噪声。&lt;h4&gt;方法&lt;/h4&gt;将预训练模型SAM与声音提示结合，使用跨模式语义对齐的方法来改进AVS任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过在AVSBench数据集上的实验显示了在两个数据集上都达到了最新的性能。该方法有效地利用了预训练的分割模型以及跨模态语义对齐。&lt;h4&gt;结论&lt;/h4&gt;提出的AV2T-SAM框架解决了现有音频-视觉分割技术面临的挑战，特别是在有限的数据集限制下学习视听关系的问题。&lt;h4&gt;翻译&lt;/h4&gt;提出了一种新的AVS框架，该框架使用SAM模型，并通过引入一种新的特征来改进其在处理声音源对象分割任务上的能力。此方法利用了跨模式语义对齐，并且实验证明了它能够有效地提高模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of Audio-Visual Segmentation (AVS) is to localize and segment thesounding source objects from the video frames. Researchers working on AVSsuffer from limited datasets because hand-crafted annotation is expensive.Recent works attempt to overcome the challenge of limited data by leveragingthe segmentation foundation model, SAM, prompting it with audio to enhance itsability to segment sounding source objects. While this approach alleviates themodel's burden on understanding visual modality by utilizing pre-trainedknowledge of SAM, it does not address the fundamental challenge of the limiteddataset for learning audio-visual relationships. To address these limitations,we propose \textbf{AV2T-SAM}, a novel framework that bridges audio featureswith the text embedding space of pre-trained text-prompted SAM. Our methodleverages multimodal correspondence learned from rich text-image paireddatasets to enhance audio-visual alignment. Furthermore, we introduce a novelfeature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot\textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio andvisual modalities while filtering irrelevant noise. Experiments on the AVSBenchdataset demonstrate state-of-the-art performance on both datasets of AVSBench.Our approach outperforms existing methods by effectively utilizing pretrainedsegmentation models and cross-modal semantic alignment.</description>
      <author>example@mail.com (Kyungbok Lee, You Zhang, Zhiyao Duan)</author>
      <guid isPermaLink="false">2502.16359v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control</title>
      <link>http://arxiv.org/abs/2502.21057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Robust Deterministic Policy Gradient (RDPG) 的强化学习算法，该算法将H无穷控制问题转化为一个零和动态博弈，并通过深度确定性策略梯度(DPG)及其深度强化学习版本进行训练。为实际应用引入了RDDPG算法，利用深层神经网络架构并结合TD3技术来提高稳定性和学习效率。&lt;h4&gt;背景&lt;/h4&gt;在实际控制系统中，由于系统模型中的不确定性以及外部扰动的存在，识别最优控制策略面临重大挑战。传统的H无穷控制方法虽然广泛应用于设计鲁棒控制器以减轻干扰影响，但往往需要复杂的计算资源和高强度的计算能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于强化学习的方法来解决实际控制系统中由于不确定性和外部干扰导致的设计复杂性问题，并提高控制器的稳定性与实时性能。&lt;h4&gt;方法&lt;/h4&gt;将H无穷控制问题建模为一个两人零和动态博弈，其中一方试图最小化成本而另一方则最大化。采用确定性策略梯度(DPG)及其深度强化学习版本来训练鲁棒控制策略，进而提出了一种名为RDDPG的算法，并在无人机路径跟踪任务中验证了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法能够在扰动环境下实现精确实时的目标追踪，在对比测试中显示出了比其他控制方法更高的抗干扰性能。&lt;h4&gt;结论&lt;/h4&gt;基于强化学习的RDPG和RDDPG为实际控制系统中的鲁棒控制器设计提供了一种有效且计算效率高的解决方案，尤其适用于需要应对动态变化环境的应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Practical control systems pose significant challenges in identifying optimalcontrol policies due to uncertainties in the system model and externaldisturbances. While $H_\infty$ control techniques are commonly used to designrobust controllers that mitigate the effects of disturbances, these methodsoften require complex and computationally intensive calculations. To addressthis issue, this paper proposes a reinforcement learning algorithm calledRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$control problem as a two-player zero-sum dynamic game. In this formulation, oneplayer (the user) aims to minimize the cost, while the other player (theadversary) seeks to maximize it. We then employ deterministic policy gradient(DPG) and its deep reinforcement learning counterpart to train a robust controlpolicy with effective disturbance attenuation. In particular, for practicalimplementation, we introduce an algorithm called robust deep deterministicpolicy gradient (RDDPG), which employs a deep neural network architecture andintegrates techniques from the twin-delayed deep deterministic policy gradient(TD3) to enhance stability and learning efficiency. To evaluate the proposedalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked withfollowing a predefined path in a disturbance-prone environment. Theexperimental results demonstrate that the proposed method outperforms othercontrol approaches in terms of robustness against disturbances, enablingprecise real-time tracking of moving targets even under severe disturbanceconditions.</description>
      <author>example@mail.com (Taeho Lee, Donghwan Lee)</author>
      <guid isPermaLink="false">2502.21057v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</title>
      <link>http://arxiv.org/abs/2410.20893v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文研究了基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，重点关注其在白盒和黑盒攻击策略中的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR点云跟踪模型往往忽视了抗干扰能力的重要性，而仅仅关注性能提升。然而，在面对对抗攻击、领域转换或数据损坏等问题时，这些模型表现出严重的脆弱性。&lt;h4&gt;目的&lt;/h4&gt;研究如何提高基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，并提出一种新的黑盒攻击策略方法：目标感知扰动生成(TAPG)算法。&lt;h4&gt;方法&lt;/h4&gt;{'白盒攻击': '为各种跟踪范式定制特定损失函数，扩展了现有FGSM、C&amp;W和PGD等方法到点云领域。', '黑盒攻击': '引入TAPG算法，通过稀疏性约束和随机子向量因子化技术提高转移能力。该算法旨在实现高攻击性能的同时保持低感知度。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，先进的跟踪方法在对抗白盒和黑盒攻击时存在显著脆弱性。&lt;h4&gt;结论&lt;/h4&gt;研究强调了增强LiDAR点云跟踪模型鲁棒性的必要性，并提出了一种新的平衡有效性和隐蔽性的TAPG算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we delve into the robustness of neural network-based LiDARpoint cloud tracking models under adversarial attacks, a critical aspect oftenoverlooked in favor of performance enhancement. These models, despiteincorporating advanced architectures like Transformer or Bird's Eye View (BEV),tend to neglect robustness in the face of challenges such as adversarialattacks, domain shifts, or data corruption. We instead focus on the robustnessof the tracking models under the threat of adversarial attacks. We begin byestablishing a unified framework for conducting adversarial attacks within thecontext of 3D object tracking, which allows us to thoroughly investigate bothwhite-box and black-box attack strategies. For white-box attacks, we tailorspecific loss functions to accommodate various tracking paradigms and extendexisting methods such as FGSM, C\&amp;W, and PGD to the point cloud domain. Inaddressing black-box attack scenarios, we introduce a novel transfer-basedapproach, the Target-aware Perturbation Generation (TAPG) algorithm, with thedual objectives of achieving high attack performance and maintaining lowperceptibility. This method employs a heuristic strategy to enforce sparseattack constraints and utilizes random sub-vector factorization to bolstertransferability. Our experimental findings reveal a significant vulnerabilityin advanced tracking methods when subjected to both black-box and white-boxattacks, underscoring the necessity for incorporating robustness againstadversarial attacks into the design of LiDAR point cloud tracking models.Notably, compared to existing methods, the TAPG also strikes an optimal balancebetween the effectiveness of the attack and the concealment of theperturbations.</description>
      <author>example@mail.com (Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu)</author>
      <guid isPermaLink="false">2410.20893v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title>
      <link>http://arxiv.org/abs/2502.19697v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种新的攻击方法Attribute-aware Prompt Attack (AP-Attack)，该方法利用视觉语言模型(VLM)的图像文本对齐能力，通过对行人属性特定的文字嵌入进行破坏来显式地扰乱行人图像中的细粒度语义特征。&lt;h4&gt;背景&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中至关重要。然而，现有的基于VLM（视觉-语言模型）的攻击方法由于过于强调整体表示中的判别性语义，缺乏对综合特征破坏的能力。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的属性感知提示攻击(AP-Attack)方法，该方法旨在通过扰动行人图像中特定属性的文字嵌入来增强细粒度语义特征的扰乱效果，并提高对抗样本在不同模型和数据集上的迁移能力。&lt;h4&gt;方法&lt;/h4&gt;设计了文本反转网络以获取个人化的文字描述，这些网络将行人图像映射到表示语义嵌入的伪标记上。训练过程中采用了对比学习方式结合图像与预先定义的文字模板，该模板明确描述了行人的属性特征。&lt;h4&gt;主要发现&lt;/h4&gt;AP-Attack 方法在跨模型和数据集攻击场景中表现出色，其平均Drop Rate比现有方法高出22.9%，展示了极佳的迁移性。&lt;h4&gt;结论&lt;/h4&gt;通过扰乱行人图像中的细粒度语义特征，AP-Attack有效增强了对抗样本的破坏力，并且提高了它们的迁移性能。&lt;h4&gt;翻译&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中非常重要。最近基于视觉语言模型（VLM）的攻击方法显示出卓越的迁移性，通过攻击VLM中的通用图像和文本特征来探索这些模型的脆弱点。然而，由于过度强调整体表示中的判别性语义，它们缺乏对综合特征的彻底破坏。在这篇论文中，我们引入了属性感知提示攻击（AP-Attack），这是一种新颖的方法，它利用VLM的图像文字对齐能力显式地扰乱行人图像中的细粒度语义特征，通过摧毁特定于属性的文字嵌入来实现这一点。为了获得针对每个个体属性的个性化文本描述，设计了文本反转网络将行人图像映射到表示语义嵌入的伪标记上，并在对比学习方式下进行训练，结合图像和预先定义的好像模板，该模板明确地描述了行人的属性特征。扰动后的良性及对抗性细粒度文本语义使攻击者能够有效地进行全面破坏，从而增强了对抗样本的迁移能力。广泛的实验表明，AP-Attack实现了最先进的迁移性能，在跨模型和数据集的攻击场景中平均Drop Rate比现有方法高出22.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (re-id) models are vital in security surveillancesystems, requiring transferable adversarial attacks to explore thevulnerabilities of them. Recently, vision-language models (VLM) based attackshave shown superior transferability by attacking generalized image and textualfeatures of VLM, but they lack comprehensive feature disruption due to theoveremphasis on discriminative semantics in integral representation. In thispaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novelmethod that leverages VLM's image-text alignment capability to explicitlydisrupt fine-grained semantic features of pedestrian images by destroyingattribute-specific textual embeddings. To obtain personalized textualdescriptions for individual attributes, textual inversion networks are designedto map pedestrian images to pseudo tokens that represent semantic embeddings,trained in the contrastive learning manner with images and a predefined prompttemplate that explicitly describes the pedestrian attributes. Inverted benignand adversarial fine-grained textual semantics facilitate attacker ineffectively conducting thorough disruptions, enhancing the transferability ofadversarial examples. Extensive experiments show that AP-Attack achievesstate-of-the-art transferability, significantly outperforming previous methodsby 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios.</description>
      <author>example@mail.com (Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.19697v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概述&lt;/h4&gt;介绍了用于户外高空Vision-Language Navigation (VLN)的OpenFly平台，该平台包括工具链和大规模基准数据集。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经得到了广泛研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而研究不足。&lt;h4&gt;目的&lt;/h4&gt;提出一个完整的工具链和大规模的户外高空VLN数据集来解决现有数据缺乏的问题。&lt;h4&gt;方法&lt;/h4&gt;{'自动化工具链': '开发了高度自动化的工具链用于数据收集，包括点云获取、场景语义分割、飞行轨迹创建及指令生成。', '大规模数据集构建': '利用工具链建立了包含100k条轨迹的大规模户外高空VLN数据集，涵盖了多样化的高度和长度以及18个不同场景。', '视觉数据生成': '采用多种渲染引擎和技术（如Unreal Engine、GTA V、Google Earth及3D Gaussian Splatting）生成高质量的视觉数据。', '模型开发': '提出了关键帧感知的VLN模型OpenFly-Agent，输入语言指令、当前观察值和历史关键帧，并直接输出飞行动作。'}&lt;h4&gt;主要发现&lt;/h4&gt;平台及其模型在多项实验中展示了其优越性。&lt;h4&gt;结论&lt;/h4&gt;工具链、数据集及代码将开源以促进相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language Navigation (VLN)旨在通过利用语言指令和视觉线索来引导环境中的代理，这在具身AI领域扮演着重要角色。虽然室内VLN已经得到了广泛的研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而鲜少有人研究。为解决这一问题，我们提出了一种开放式飞行平台OpenFly，包括一个灵活的工具链和大规模基准数据集。首先，开发了一个高度自动化的工具链用于数据采集，实现了点云获取、场景语义分割、飞行轨迹创建及指令生成等自动化过程。其次，在此基础上建立了一个包含10万条不同高度与长度路线的大规模户外高空VLN数据集，并利用多种渲染引擎（如Unreal Engine, GTA V, Google Earth）和技术（如3D Gaussian Splatting）生成高视觉质量的数据，其中3D GS支持真实到仿真渲染。最后，我们提出了关键帧感知的VLN模型OpenFly-Agent，该模型根据语言指令、当前观察值和历史关键帧输出飞行动作。通过全面分析及实验表明了我们的平台及其模型的优势，并计划开放工具链、数据集以及相关代码以促进进一步的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为CalibRefine的全自动、无目标且在线校准框架，该框架可以处理原始LiDAR点云和相机图像，通过一系列阶段实现精确的多传感器校准。&lt;h4&gt;背景&lt;/h4&gt;在诸如自动驾驶汽车、机器人技术及智能交通系统等应用中，准确的多传感器校准至关重要。现有的激光雷达-摄像机校准方法通常依赖于手动放置的目标物、初步参数估计或密集的数据预处理，这限制了它们在实际环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种全自动化的校准框架，该框架不依赖人工目标且可以在线完成，并能够直接处理原始激光雷达点云和相机图像数据。&lt;h4&gt;方法&lt;/h4&gt;CalibRefine由四个阶段组成：（1）一个共同特征鉴别器，利用自动检测到的对象的相对位置、外观嵌入以及语义类别生成可靠的激光雷达-摄像机对应关系；（2）基于粗略同构变换的校准；（3）迭代细化，在更多数据帧可用时逐步提高对齐精度；（4）注意力机制改进，通过使用视觉变压器和交叉注意机制解决非平面失真问题。&lt;h4&gt;主要发现&lt;/h4&gt;CalibRefine在两个城市交通数据集上进行了广泛的实验，结果显示它能够以最少的人工干预实现高精度校准结果，优于无目标的现有方法，并与手动调优基线保持竞争性或超越。&lt;h4&gt;结论&lt;/h4&gt;研究强调了如何通过稳健的对象级特征匹配以及迭代和自监督的注意力机制调整，在复杂的真实世界条件下实现一致的传感器融合，而无需地面真相校准矩阵或复杂的预处理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/radar-lab/Lidar_Camera_Automatic_Calibration&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Plane-DUSt3R是一种利用3D基础模型DUSt3R进行多视角房间布局估计的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于多视图几何的复杂性，从多视角图像中推断房间布局的研究较少。传统的结构从运动过程中涉及多个步骤（如相机内部和外部参数估计、图像匹配和三角测量）。然而，在3D重建领域，最近出现的3D基础模型改变了传统方法。&lt;h4&gt;目的&lt;/h4&gt;介绍并改进一种基于DUSt3R框架的方法——Plane-DUSt3R，以解决多视角房间布局估计的问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R通过在房间布局数据集（Structure3D）上进行微调，并修改目标函数来估计结构平面。它采用单步后处理步骤和2D检测结果生成一致且简洁的结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Plane-DUSt3R不仅在合成数据集中优于现有最佳方法，还在具有不同图像风格（如卡通）的现实世界数据中表现出鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;Plane-DUSt3R提供了一种简化流程、减少误差累积的端到端解决方案，并能处理多视角图像。此研究拓展了房间布局估计领域的可能性。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图片中推断出房间的布局由于涉及到复杂的多视图几何问题，因此研究较少。传统的结构从运动过程需要一系列步骤（例如相机内参和外参估计、图像匹配以及三角测量）。然而，在3D重建领域，最近出现的像DUSt3R这样的3D基础模型改变了这一传统流程，使其向端到端的单步方法转变。为了解决多视角房间布局估计的问题，我们引入了Plane-DUSt3R，这是一种利用3D基础模型DUSt3R的方法。该方法基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行了微调以估计结构平面。它通过单一的后处理步骤和2D检测结果生成一致且简洁的结果。与依赖单视角或全景图象的方法不同，Plane-DUSt3R能够处理多视角图像并提供了一种简化的、端到端的解决方案来简化过程，并减少误差积累。实验结果显示，相较于现有最佳方法，在合成数据集上，该方法表现更优；在不同的真实世界数据集中（例如卡通风格），该方法表现出稳健性和有效性。我们的代码可以在https://github.com/justacar/Plane-DUSt3R中获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon. Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络（GNN）的机器学习方法，用于预测颗粒悬浮液中的摩擦接触网络（FCN），特别是在接近拥堵条件下的性能。这种方法在数据驱动模拟训练中表现出色，并且能够准确预测不同参数组合下的FCN。&lt;h4&gt;背景&lt;/h4&gt;细颗粒分散于牛顿流体中的悬浮物粘度在其接近拥挤状态时发散，这主要是由粒子间接触微观结构决定的。这种联系网络是导致固体化的行为的关键。应力传输和网络拓扑对粒子相对运动的限制非常敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种预测FCN的有效机器学习方法，尤其是在靠近拥堵条件下的情况。&lt;h4&gt;方法&lt;/h4&gt;使用了一种称为Deep Graph Convolutional Network（DeepGCN）的方法，并且展示了在不同参数组合下具有良好的泛化和外推能力。该研究包括从半稀释状态到拥挤状态的广泛相空间，同时系统地改变剪切应力、堆积分数以及滑动和滚动摩擦。&lt;h4&gt;主要发现&lt;/h4&gt;通过训练数据驱动模拟，DeepGCN能够准确预测不同流参数和相空间条件下的FCN。这些结果展示了在材料科学及相关领域创新且可转移的技术途径的潜力。&lt;h4&gt;结论&lt;/h4&gt;这项研究为预测颗粒系统性质提供了新的技术方法，特别是在拥挤条件下，这可能推动材料科学及其相关领域的进展。&lt;h4&gt;翻译&lt;/h4&gt;悬浮液中由细小颗粒分散于牛顿流体中的粘度在接近堆积极限时发散。这种宏观行为受到粒子接触微观结构的支配，通过摩擦接触网络（FCN）来实现。FCN是由机械负载支撑点组成的，在接近拥挤转变时导致刚性出现。应力传递和网络拓扑反过来取决于颗粒相对运动限制的敏感特性。尽管其重要性显而易见，但由于实验和计算障碍的存在，预测FCN特别是靠近拥挤条件下的情况仍然具有挑战性。这项研究提出了一个基于图神经网络（GNN）的成本效益机器学习方法来预测FCN，并且通过使用DeepGCN展示了在不同流参数和相空间条件下准确预测FCN的能力。该研究覆盖了广泛的相空间，从半稀释到拥挤状态以及瞬态到稳定状态，并系统地改变剪切应力、堆积分数及滑动和滚动摩擦等参数。这项研究的结果为颗粒系统的性质预测提供了创新且可转移的技术途径，为进一步发展材料科学及相关领域开辟新的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The viscosity of the suspension consisting of fine particles dispersed in aNewtonian liquid diverges close to the jamming packing fraction. The contactmicrostructure in suspensions governs this macroscopic behavior in the vicinityof jamming through a frictional contact network (FCN). FCN is composed ofmechanical load-bearing contacts that lead to the emergence of rigidity nearthe jamming transition. The stress transmission and network topology, in turn,depend sensitively on constraints on the relative motion of the particles.Despite their significance, predicting the FCN, especially close to jammingconditions, remains challenging due to experimental and computationalimpediments. This study introduces a cost-effective machine learning approachto predict the FCN using a graph neural network (GNN), which inherentlycaptures hidden features and underlying patterns in dense suspension by mappinginterparticle interactions. Employing a variation of GNN called the Deep GraphConvolutional Network (DeepGCN) trained on data-driven simulations, this studydemonstrates robust generalization and extrapolation capabilities, accuratelypredicting FCNs in systems with divergent flow parameters and phase spaces,despite each being trained exclusively on a single condition. The study coversa wide range of phase space, from semi-dilute to jammed states, spanningtransient to steady states, while systematically varying parameters such asshear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding androlling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave theway for innovative transferable techniques in predicting the properties ofparticulate systems, offering new avenues for advancement in material scienceand related fields.</description>
      <author>example@mail.com (Armin Aminimajd, Joao Maia, Abhinendra Singh)</author>
      <guid isPermaLink="false">2502.18743v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
  <item>
      <title>Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</title>
      <link>http://arxiv.org/abs/2502.20220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://tobias-kirschstein.github.io/avat3r/, Video:  https://youtu.be/P3zNVx15gYs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Avat3r的方法，可以从少量输入图像中生成高质量且可动画化的3D头像。&lt;h4&gt;背景&lt;/h4&gt;传统上，创建逼真的3D头像是一个复杂的过程，需要多视角捕捉设备和昂贵的优化过程。这限制了数字人类替身的应用范围，使其仅限于VFX行业或离线渲染。&lt;h4&gt;目的&lt;/h4&gt;开发一种减少计算需求的方法，使得高质量、可动画化的3D头像可以从少量输入图像中生成。&lt;h4&gt;方法&lt;/h4&gt;{'利用大型重建模型': '使大规模重建模型变得可动画化，并从大量的多视角视频数据集中学习三维人体头部的强大先验知识。', '改进的3D头部重构': '采用来自DUSt3R的位置图和Sapiens的人类基础模型中的泛化特征图来改善3D头部重构。', '实现动画功能': '发现简单的跨注意力到表情代码就足够用于使3D头像可动画化。', '增强鲁棒性': '通过训练时输入不同表情的图像，增强了模型从不一致输入中重建三维头部的能力。'}&lt;h4&gt;主要发现&lt;/h4&gt;Avat3r在少数输入和单个输入场景中的表现优于现有最先进的方法，并展示了广泛的适用性，能够创建来自各种来源（包括智能手机拍摄、单一图片甚至超出领域范围如古董头像）的3D头像。&lt;h4&gt;结论&lt;/h4&gt;通过项目网站https://tobias-kirschstein.github.io/avat3r可查看更多详细信息。该方法在效率和性能上表现出显著优势，为数字人类替身的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的传统创建逼真的3D头像过程需要复杂的多视角捕捉设备以及昂贵的计算资源，在实际应用中受到限制；本文提出了一种名为Avat3r的技术，可以从少量输入图像生成高质量且可动画化的3D头像，并通过训练模型学习泛化特征和不同表情下的鲁棒性重构方法，实现了在效率与性能上的突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditionally, creating photo-realistic 3D head avatars requires astudio-level multi-view capture setup and expensive optimization duringtest-time, limiting the use of digital human doubles to the VFX industry oroffline renderings.  To address this shortcoming, we present Avat3r, which regresses ahigh-quality and animatable 3D head avatar from just a few input images, vastlyreducing compute requirements during inference. More specifically, we makeLarge Reconstruction Models animatable and learn a powerful prior over 3D humanheads from a large multi-view video dataset. For better 3D headreconstructions, we employ position maps from DUSt3R and generalized featuremaps from the human foundation model Sapiens. To animate the 3D head, our keydiscovery is that simple cross-attention to an expression code is alreadysufficient. Finally, we increase robustness by feeding input images withdifferent expressions to our model during training, enabling the reconstructionof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capturewith accidental movement, or frames from a monocular video.  We compare Avat3r with current state-of-the-art methods for few-input andsingle-input scenarios, and find that our method has a competitive advantage inboth tasks. Finally, we demonstrate the wide applicability of our proposedmodel, creating 3D head avatars from images of different sources, smartphonecaptures, single images, and even out-of-domain inputs like antique busts.  Project website: https://tobias-kirschstein.github.io/avat3r/</description>
      <author>example@mail.com (Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nießner, Shunsuke Saito)</author>
      <guid isPermaLink="false">2502.20220v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</title>
      <link>http://arxiv.org/abs/2502.19645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://openvla-oft.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了用于视觉-语言-动作模型（VLAs）的优化微调策略，提出了一个集成平行解码、连续动作表示等技术的高效微调方案。&lt;h4&gt;背景&lt;/h4&gt;现有的VLAs依赖于预训练的语言和视觉模型，并利用各种机器人数据集展示了强大的任务执行能力。然而，这些模型在面对新环境时需要通过微调来适应，而最佳的微调策略尚未明确。&lt;h4&gt;目的&lt;/h4&gt;探讨不同的动作解码方案、表示方法及学习目标对VLAs性能的影响，提出一种优化微调的方法以改善推理效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用OpenVLA作为基准模型，并通过实证分析确定了最佳的动作解码策略、表示方式以及学习目标。提出的OFT框架包括并行解码、动作切块（action chunking）、连续动作表示和基于L1回归的学习目标。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的微调方法显著提高了OpenVLA在LIBERO仿真基准测试中的成功率，同时增加了行动生成的吞吐量；在真实世界评估中，该策略使得OpenVLA能够优于其他VLAs以及其他从头开始训练的模仿学习策略，在平均成功率上提高15%。&lt;h4&gt;结论&lt;/h4&gt;通过引入OFT方法，可以显著改善视觉-语言-动作模型在新环境下的性能和效率，并且提供了一种有效的微调方案。该研究进一步证明了优化微调对于提升这些复杂模型的实际应用效果的重要性。&lt;h4&gt;翻译&lt;/h4&gt;近期的视觉-语言-动作（VLA）模型基于预训练的语言与视觉模型构建，利用多样化的机器人数据集展示出色的任务执行能力、跟随语言指令的能力以及语义泛化。尽管取得了一些成功，但VLAs在面对新机器人设置时仍表现不佳，需要通过微调来获得良好性能。针对这一问题，该研究探讨了关键的VLA适应设计选择，如不同的动作解码方案、表示方法和学习目标，并提出了一个优化微调（OFT）配方。实证分析表明，这种方法提高了推理效率、政策性能以及模型输入输出规范的灵活性。所提出的OpenVLA-OFT在LIBERO仿真基准上达到了新的最佳水平，在四组任务上的平均成功率从76.5%提高到97.1%，同时增加了26倍的动作生成吞吐量。实际评估显示，优化微调方案使得OpenVLA能够成功执行ALOHA双臂机器人上的精细、高频控制任务，并在平均成功率上显著优于其他VLAs和强大的从头开始训练的模仿学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent vision-language-action models (VLAs) build upon pretrainedvision-language models and leverage diverse robot datasets to demonstratestrong task execution, language following ability, and semantic generalization.Despite these successes, VLAs struggle with novel robot setups and requirefine-tuning to achieve good performance, yet how to most effectively fine-tunethem is unclear given many possible strategies. In this work, we study key VLAadaptation design choices such as different action decoding schemes, actionrepresentations, and learning objectives for fine-tuning, using OpenVLA as ourrepresentative base model. Our empirical analysis informs an OptimizedFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, acontinuous action representation, and a simple L1 regression-based learningobjective to altogether improve inference efficiency, policy performance, andflexibility in the model's input-output specifications. We propose OpenVLA-OFT,an instantiation of this recipe, which sets a new state of the art on theLIBERO simulation benchmark, significantly boosting OpenVLA's average successrate across four task suites from 76.5% to 97.1% while increasing actiongeneration throughput by 26$\times$. In real-world evaluations, our fine-tuningrecipe enables OpenVLA to successfully execute dexterous, high-frequencycontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\pi_0$ andRDT-1B) fine-tuned using their default recipes, as well as strong imitationlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%(absolute) in average success rate. We release code for OFT and pretrainedmodel checkpoints at https://openvla-oft.github.io/.</description>
      <author>example@mail.com (Moo Jin Kim, Chelsea Finn, Percy Liang)</author>
      <guid isPermaLink="false">2502.19645v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$-model correction of Foundation Model based on the models own understanding</title>
      <link>http://arxiv.org/abs/2502.21179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要描述了一种基于Δ-learning的方法，用于改进通用原子间势能模型在特定材料子类中的应用。&lt;h4&gt;背景&lt;/h4&gt;当前的通用势能模型可能需要针对具体材料进行微调或残差修正。CHGNet是一个典型的例子，它能够在全局结构优化设置中准确预测某些氧化物的能量特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Δ-learning的方法来改善通用势能模型在特定场景中的表现，并探讨不同聚合方式（如全局、元素分离和原子级别）的效果。&lt;h4&gt;方法&lt;/h4&gt;使用Gaussian Process Regression (GPR)模型作为Δ-model，以CHGNet内部的原子嵌入表示为基础进行修正。这种方法可以为需要精确预测的新材料或环境提供有效的校正方案。&lt;h4&gt;主要发现&lt;/h4&gt;对于铜、银和金表面上硫原子覆盖层的情况，原始CHGNet模型存在不足之处，需要通过基于GPR的Δ-model来进行校正以提高精度。&lt;h4&gt;结论&lt;/h4&gt;通用势能模型在缺乏特定类型原子环境的数据时会表现出误差，这表明了开发更有效的修正方案的重要性。研究还发现其他使用相同训练数据集（如MACE-MP0、SevenNet-0和ORB-v2-only-MPtrj）的通用势能模型也显示出类似的行为。&lt;h4&gt;翻译&lt;/h4&gt;基础材料间的相互作用势能模型可能需要针对具体应用进行微调或残差修正。文中提出了一种基于Δ-learning的方法，通过已嵌入的表示实现这种改进。在全局结构优化设置中使用CHGNet时发现其能够准确描述某些氧化物的能量特性。然而对于金属表面上硫原子覆盖层的情况，则需要利用GPR模型来进行校正以提高预测准确性。研究结果表明了开发更有效的修正方案的重要性，因为其他训练于类似数据集上的通用势能模型也存在相似问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models of interatomic potentials, so called universal potentials,may require fine-tuning or residual corrections when applied to specificsubclasses of materials. In the present work, we demonstrate how suchaugmentation can be accomplished via $\Delta$-learning based on therepresentation already embedded in the universal potentials. The $\Delta$-modelintroduced is a Gaussian Process Regression (GPR) model and various types ofaggregation (global, species-separated, and atomic) of the representationvector are discussed. Employing a specific universal potential, CHGNet [Deng etal., Nat. Mach. Intell. 5, 1031 (2023)], in a global structure optimizationsetting, we find that it correctly describes the energetics of the "8" Cuoxide, which is an ultra-thin oxide film on Cu(111). The universal potentialmodel even predicts a more favorable structure compared to that discussed inrecent DFT-based literature. Moving to sulfur adatom overlayers on Cu(111),Ag(111), and Au(111) the CHGNet model, however, requires corrections. Wedemonstrate that these are efficiently provided via the GPR-based$\Delta$-model formulated on the CHGNet's own internal atomic embeddingrepresentation. The need for corrections is tracked to the scarcity ofmetal-sulfur atomic environments in the materials project database that CHGNetis trained on leading to an overreliance on sulfur-sulfur atomic environments.Other universal potentials trained on the same data, MACE-MP0, SevenNet-0, andORB-v2-only-MPtrj show similar behavior, but with varying degrees of error,demonstrating the general need for augmentation schemes for universal potentialmodels.</description>
      <author>example@mail.com (Mads-Peter Verner Christiansen, Bjørk Hammer)</author>
      <guid isPermaLink="false">2502.21179v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19698v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为YoCo的框架，用于生成高质量的3D伪标签以减少户外LiDAR点云三维实例分割任务中的人工标注工作。&lt;h4&gt;背景&lt;/h4&gt;户外LiDAR点云三维实例分割是自动驾驶中的关键任务，但由于需要大量人工劳动进行标注，训练模型变得非常困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用少量粗略点击注释生成高质量伪标签的方法，以减少标注成本并提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'YoCo框架': '1. 利用视觉基础模型结合点云的几何约束来增强伪标签生成；2. 设计了一个基于时空的标签更新模块，利用相邻帧的预测结果，并考虑点云固有的密度变化特性（近处密集、远处稀疏）；3. 提出一个IoU引导增强模块，替换掉置信度低和IoU低的伪标签。', '效果': '在Waymo数据集上的实验表明，YoCo框架具有显著的效果，达到了弱监督方法中的最佳性能，并且超越了完全监督的方法Cylinder3D。此外，YoCo还适用于多种网络，在仅使用少量标注数据的情况下实现了与完全监督方法相当的性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过结合视觉基础模型和点云几何约束可以有效生成高质量伪标签；2. 基于时空的标签更新模块能够利用相邻帧的信息提高标签质量。&lt;h4&gt;结论&lt;/h4&gt;YoCo框架在减少标注工作量的同时，提高了户外LiDAR点云三维实例分割任务中的模型性能，并且具有广泛适用性和优秀的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor LiDAR point cloud 3D instance segmentation is a crucial task inautonomous driving. However, it requires laborious human efforts to annotatethe point cloud for training a segmentation model. To address this challenge,we propose a YoCo framework, which generates 3D pseudo labels using minimalcoarse click annotations in the bird's eye view plane. It is a significantchallenge to produce high-quality pseudo labels from sparse annotations. OurYoCo framework first leverages vision foundation models combined with geometricconstraints from point clouds to enhance pseudo label generation. Second, atemporal and spatial-based label updating module is designed to generatereliable updated labels. It leverages predictions from adjacent frames andutilizes the inherent density variation of point clouds (dense near, sparsefar). Finally, to further improve label quality, an IoU-guided enhancementmodule is proposed, replacing pseudo labels with high-confidence and high-IoUpredictions. Experiments on the Waymo dataset demonstrate YoCo's effectivenessand generality, achieving state-of-the-art performance among weakly supervisedmethods and surpassing fully supervised Cylinder3D. Additionally, the YoCo issuitable for various networks, achieving performance comparable to fullysupervised methods with minimal fine-tuning using only 0.8% of the fullylabeled data, significantly reducing annotation costs.</description>
      <author>example@mail.com (Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng)</author>
      <guid isPermaLink="false">2502.19698v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>DV-Matcher: Deformation-based Non-Rigid Point Cloud Matching Guided by Pre-trained Visual Features</title>
      <link>http://arxiv.org/abs/2408.08568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DV-Matcher的创新学习框架，用于估计非刚性可变形点云之间的密集对应关系。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常需要对点云进行网格化或手动标注才能学习到有效的特征信息。相比之下，基于学习的方法可以直接从无结构化的点云中提取特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外预处理的框架，用于生成高质量的密集对应关系，并探索如何在几何特征学习过程中引入先验知识以及设计新的变形模块以促进外部对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 通过将来自预训练视觉模型的知识注入到几何特征学习中，增强局部性质的几何特征与全局和语义信息；2. 提出了一种基于变形的模块来促进由所学对应关系诱导的外在对齐。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法在匹配非刚性点云时达到了最先进的水平，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;DV-Matcher框架通过结合视觉先验知识和创新的变形模块，在密集对应估计领域开辟了一条新的道路，并展示了其在处理复杂点云数据中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种基于学习的框架DV-Matcher，用于非刚性可变形点云之间的密集对应的估算。该框架直接从无结构化点云中学习，无需网格化或手动标记，并且能够提供高质量的密集对应关系，在点云处理中有实际应用价值。我们的主要贡献在于两点：首先，我们提出了一种方案将预训练视觉模型中的先验知识注入到几何特征学习中，有效地补充了局部性质的几何特征与全局和语义信息；其次，我们提出了一个基于变形的新模块来促进由所学对应关系诱导的外在对齐，有效增强了特征学习。实验结果表明，在匹配非刚性点云时，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上，我们的方法都达到了最先进的水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present DV-Matcher, a novel learning-based framework forestimating dense correspondences between non-rigidly deformable point clouds.Learning directly from unstructured point clouds without meshing or manuallabelling, our framework delivers high-quality dense correspondences, which isof significant practical utility in point cloud processing. Our keycontributions are two-fold: First, we propose a scheme to inject priorknowledge from pre-trained vision models into geometric feature learning, whicheffectively complements the local nature of geometric features with global andsemantic information; Second, we propose a novel deformation-based module topromote the extrinsic alignment induced by the learned correspondences, whicheffectively enhances the feature learning. Experimental results show that ourmethod achieves state-of-the-art results in matching non-rigid point clouds inboth near-isometric and heterogeneous shape collection as well as morerealistic partial and noisy data.</description>
      <author>example@mail.com (Zhangquan Chen, Puhua Jiang, Ruqi Huang)</author>
      <guid isPermaLink="false">2408.08568v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948;  updated authors list format&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文评估了基础模型（FMs）与任务特定（TS）模型在前列腺癌诊断和Gleason分级中的临床表现，发现尽管FMs在数据稀缺情况下有优势，但当有足够的标注训练数据时其性能被TS模型超越。此外，专门的任务培训显著降低了误诊率，并且考虑到可持续性问题，建议结合两种方法以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;背景&lt;/h4&gt;人工智能（AI）在病理科的作用从辅助诊断发展到揭示全切片图像中的预测形态模式。基础模型通过自监督预训练被广泛倡导为多种下游任务的通用解决方案，但它们的临床适用性和相对于特定任务学习模型的优势仍存在疑问。&lt;h4&gt;目的&lt;/h4&gt;评估AI在前列腺癌诊断和Gleason分级中具有临床级性能的方法，并比较两个FMs与完全端到端TS模型的表现。&lt;h4&gt;方法&lt;/h4&gt;使用来自15个地点、11个国家的7342名患者超过10万个核心针活检样本进行了大规模验证。将两种基础模型在一个多实例学习框架中与一个完全端到端任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管FMs在数据稀缺情况下有用，但当有足够的标记训练数据时其性能被TS模型超越或低于后者；任务特定培训显著减少了临床重要性的错误分级和形态挑战性情况下的误诊；基础模型消耗的能量最多可达TS模型的35倍，引发了关于可持续性的担忧。&lt;h4&gt;结论&lt;/h4&gt;FMs在快速原型设计和研究中提供了明显优势，但作为适用于临床应用的医疗AI通用解决方案的角色仍不确定。对于高风险临床应用而言，严格的验证以及对特定任务培训的考虑至关重要。建议结合基础模型和端到端学习的优点以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能在前列腺癌诊断及格利森评分中的作用，并通过对比两种基于大规模预训练的基础模型与传统任务特异模型，揭示了关于这两种方法临床适用性的新见解。结果强调，在充足标记数据的情况下，任务特定的训练优于基础模型；同时指出，基础模型较高的能耗问题也应引起重视。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信性至关重要，尤其是在其被嵌入到高风险领域时。&lt;h4&gt;背景&lt;/h4&gt;在机器学习系统变得越来越重要和广泛应用的同时，如何保证这些系统的公平性、隐私性、健壮性、准确性和可解释性成为了研究的重点。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入因果方法来解决可信机器学习中的多重目标之间的矛盾，并提高其可靠性与解释性。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有文献中将因果方法应用于机器学习的成功案例，展示如何有效结合这些原则以达成平衡。&lt;h4&gt;主要发现&lt;/h4&gt;指出采用因果推理框架能够帮助更好地管理多个相互竞争的目标，在可信机器学习和基础模型设计方面提供解决方案。&lt;h4&gt;结论&lt;/h4&gt;论文讨论了采纳因果框架面临的挑战、局限性及机会，并倡导在未来的AI系统中使用更加负责任且道德的策略。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的信任度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法融入到机器学习中来处理关键原则之间的权衡问题，如公平性、隐私性、鲁棒性、准确性及可解释性。尽管这些目标应理想地同时满足，但现实中往往单独考虑，导致冲突和次优解的产生。通过参考现有的因果推理在机器学习中的应用案例，本文强调了采用因果方法对于平衡多重竞争目标的重要性，并探讨如何实际将因果理论应用于机器学习模型中，以提升其可靠性和可解释性。此外，还讨论了采纳这种框架所面临的挑战、限制和机遇，为更负责任及伦理规范的AI系统开发铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forintegrating causal methods into machine learning to navigate the trade-offsamong key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Mohammad Havaei, Bernhard Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GraphBridge: Towards Arbitrary Transfer Learning in GNNs</title>
      <link>http://arxiv.org/abs/2502.19252v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables, to be published in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GraphBridge的框架，旨在解决图神经网络（GNN）在不同任务和数据集之间知识迁移的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的GNN训练方式是针对特定的任务或领域进行的，这导致了跨不同、异构的数据设置的知识转移存在障碍。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的方法来实现GNN中的跨域和跨任务的知识迁移。&lt;h4&gt;方法&lt;/h4&gt;GraphBridge通过增加预训练模型上的预测头以及输入层与输出层之间的桥梁网络，以保持原模型的固有知识并支持任意维度的输出。为了减少目标领域的源偏差问题，该框架将源模型合并到一个同时训练的目标模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在图转图、节点转节点、图转节点以及图转点云等多种迁移学习场景下进行了广泛的实验验证，并通过16个具有代表性的数据集证明了其在任务和领域无关的图结构数据中的知识转移能力，标志着GNN领域的重大进展。&lt;h4&gt;结论&lt;/h4&gt;GraphBridge框架提供了一种有效的方法来解决跨不同任务和域的知识迁移问题，在多个场景中显示出优越的表现。源代码可在https://github.com/jujulili888/GraphBridge获得。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）通常针对特定领域或特定任务进行训练，这在将所获取知识转移到不同的、异构的数据设置时造成了障碍。本文介绍了GraphBridge框架，一种用于实现不同任务和域之间知识转移的新方法，无需对任务配置或图结构进行修改。具体而言，GraphBridge允许通过添加预测头和连接输入层到输出层的桥梁网络来增强任何预训练的GNN模型。此架构不仅保留了原始模型的内在知识，还支持任意维度的输出。为了解决负向迁移问题，GraphBridge将源模型与同时训练的目标模型合并在一起，在应用于目标领域时减少了源偏置。我们的方法在包括图转图、节点转节点、图转节点以及图转点云在内的多种迁移学习场景中进行了全面评估，并通过代表这些场景的16个数据集上的实验证明了该框架在任务和领域无关的图结构中的知识转移能力，标志着GNN领域的重大进展。代码可在https://github.com/jujulili888/GraphBridge获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are conventionally trained on a per-domain,per-task basis. It creates a significant barrier in transferring the acquiredknowledge to different, heterogeneous data setups. This paper introducesGraphBridge, a novel framework to enable knowledge transfer across disparatetasks and domains in GNNs, circumventing the need for modifications to taskconfigurations or graph structures. Specifically, GraphBridge allows for theaugmentation of any pre-trained GNN with prediction heads and a bridgingnetwork that connects the input to the output layer. This architecture not onlypreserves the intrinsic knowledge of the original model but also supportsoutputs of arbitrary dimensions. To mitigate the negative transfer problem,GraphBridge merges the source model with a concurrently trained model, therebyreducing the source bias when applied to the target domain. Our method isthoroughly evaluated across diverse transfer learning scenarios, includingGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empiricalvalidation, conducted over 16 datasets representative of these scenarios,confirms the framework's capacity for task- and domain-agnostic transferlearning within graph-like data, marking a significant advancement in the fieldof GNNs. Code is available at https://github.com/jujulili888/GraphBridge.</description>
      <author>example@mail.com (Li Ju, Xingyi Yang, Qi Li, Xinchao Wang)</author>
      <guid isPermaLink="false">2502.19252v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2502.14189v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;随着收集的医疗文本数据量的不断增加，自动化多标签文本分类（MLTC）面临独特挑战，主要是由于训练所需标记文本的稀缺性和其复杂性。传统机器学习模型通常无法完全捕捉到表达的主题范围。然而，大型语言模型（LLMs）在不同领域的多项自然语言处理任务中展示了显著的效果，这些模型具有出色的计算效率，并且通过提示工程能够适用于无监督学习。因此，这些LLM为医疗叙述的MLTC提供了有效的解决方案。然而，在面对各种标签时，不同的提示可能根据主题的相关性而变化。为了应对这一挑战，提出的QUAD-LLM-MLTC方法利用了四个大型语言模型的优势：GPT-4o、BERT、PEGASUS和BART。该方法在顺序流水线中操作，其中BERT提取关键令牌，PEGASUS增强文本数据，GPT-4o进行分类，而BART提供主题分配概率，从而产生四次0-shot设置的分类结果。这些输出通过集成学习组合，并通过元分类器处理以生成最终的MLTC结果。该方法使用三个标记文本样本进行了评估，与传统和单一模型的方法形成了对比。结果显示，在大多数主题上的F1评分及一致性（F1 和 Micro-F1 分数分别达到78.17% 和 80.16%，标准偏差分别为0.025 和 0.011）上有显著改进。&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多标签文本分类方法，QUAD-LLM-MLTC，利用多个大型语言模型处理医疗数据的复杂性和多样性，并展示了其在F1评分和一致性上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;自动化多标签文本分类（MLTC）因医疗领域的大量未标记数据而面临挑战。传统机器学习模型难以应对这种复杂性。&lt;h4&gt;目的&lt;/h4&gt;探索使用大型语言模型进行高效的无监督学习，以解决大规模医疗文本的自动分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架QUAD-LLM-MLTC，该框架利用GPT-4o、BERT、PEGASUS和BART四个大型语言模型来进行零样本设置下的多标签文本分类，并通过集成学习和元分类器处理输出以得到最终结果。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，使用QUAD-LLM-MLTC的方法在多个主题上显示出更高的F1评分及一致性。这种方法展示出强大的性能并可广泛应用于医疗数据的快速分类。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型的应用为解决复杂文本的数据分类问题提供了创新性的解决方案，并通过集成不同模型的优势，在多标签医学文本分类中实现了高效和扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The escalating volume of collected healthcare textual data presents a uniquechallenge for automated Multi-Label Text Classification (MLTC), which isprimarily due to the scarcity of annotated texts for training and their nuancednature. Traditional machine learning models often fail to fully capture thearray of expressed topics. However, Large Language Models (LLMs) havedemonstrated remarkable effectiveness across numerous Natural LanguageProcessing (NLP) tasks in various domains, which show impressive computationalefficiency and suitability for unsupervised learning through promptengineering. Consequently, these LLMs promise an effective MLTC of medicalnarratives. However, when dealing with various labels, different prompts can berelevant depending on the topic. To address these challenges, the proposedapproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in whichBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, andBART provides topics' assignment probabilities, which results in fourclassifications, all in a 0-shot setting. The outputs are then combined usingensemble learning and processed through a meta-classifier to produce the finalMLTC result. The approach is evaluated using three samples of annotated texts,which contrast it with traditional and single-model methods. The results showsignificant improvements across the majority of the topics in theclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and80.16% with standard deviations of 0.025 and 0.011, respectively). Thisresearch advances MLTC using LLMs and provides an efficient and scalablesolution to rapidly categorize healthcare-related text data without furthertraining.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2502.14189v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception</title>
      <link>http://arxiv.org/abs/2501.15394v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Doracamom的框架，该框架融合了多视角相机和4D雷达的数据，用于实现3D物体检测和语义占用预测任务。通过引入新颖的Coarse Voxel Queries Generator、设计Dual-Branch Temporal Encoder以及Cross-Modal BEV-Voxel Fusion模块，使系统能够在复杂环境感知中表现出色。&lt;h4&gt;背景&lt;/h4&gt;3D目标检测和占位预测在自动驾驶领域非常重要，但现有基于视觉的方法在恶劣条件下效果不佳。整合相机与4D成像雷达可以实现多任务统一感知，但在这一领域的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够集成多视角相机和4D雷达的框架，用于完成3D物体检测和语义占用预测任务。&lt;h4&gt;方法&lt;/h4&gt;引入了Coarse Voxel Queries Generator来初始化查询体素；设计了Dual-Branch Temporal Encoder来利用时间信息，并实现了Cross-Modal BEV-Voxel Fusion模块以融合多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Doracamom在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上均达到了当前最佳性能。&lt;h4&gt;结论&lt;/h4&gt;该框架通过结合多种传感器的数据实现了强大的3D感知能力，并为未来的多模态环境感知系统建立了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测和占用预测在自动驾驶中至关重要，吸引了大量关注。尽管最近基于视觉的方法潜力巨大，但在恶劣条件下仍面临挑战。因此，将相机与下一代4D成像雷达相结合以实现统一的多任务感知非常重要，但该领域的研究仍然有限。本文提出了一种名为Doracamom的框架，它融合了多视角摄像头和4D雷达的数据，用于联合执行3D物体检测和语义占用预测，从而实现了全面的环境感知。特别是引入了一个新的粗体素查询生成器，该生成器将从4D雷达获得的几何先验知识与图像中的语义特征相结合来初始化体素查询，为后续基于Transformer的细化建立了坚实的基础。为了利用时间信息，设计了双分支时间编码器，在鸟瞰图和体素空间中并行处理多模态时序特性，从而能够进行全面的空间-时间表示学习。此外，还提出了一个跨模态BEV-Voxel融合模块，通过注意力机制自适应地融合互补特征，并利用辅助任务来提高特征质量。在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上的广泛实验表明，Doracamom在这两项任务中均达到了当前最佳性能，为多模态3D感知建立了新的基准。代码和模型将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection and occupancy prediction are critical tasks in autonomousdriving, attracting significant attention. Despite the potential of recentvision-based methods, they encounter challenges under adverse conditions. Thus,integrating cameras with next-generation 4D imaging radar to achieve unifiedmulti-task perception is highly significant, though research in this domainremains limited. In this paper, we propose Doracamom, the first framework thatfuses multi-view cameras and 4D radar for joint 3D object detection andsemantic occupancy prediction, enabling comprehensive environmental perception.Specifically, we introduce a novel Coarse Voxel Queries Generator thatintegrates geometric priors from 4D radar with semantic features from images toinitialize voxel queries, establishing a robust foundation for subsequentTransformer-based refinement. To leverage temporal information, we design aDual-Branch Temporal Encoder that processes multi-modal temporal features inparallel across BEV and voxel spaces, enabling comprehensive spatio-temporalrepresentation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusionmodule that adaptively fuses complementary features through attentionmechanisms while employing auxiliary tasks to enhance feature quality.Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSetdatasets demonstrate that Doracamom achieves state-of-the-art performance inboth tasks, establishing new benchmarks for multi-modal 3D perception. Code andmodels will be publicly available.</description>
      <author>example@mail.com (Lianqing Zheng, Jianan Liu, Runwei Guan, Long Yang, Shouyi Lu, Yuanzhe Li, Xiaokai Bai, Jie Bai, Zhixiong Ma, Hui-Liang Shen, Xichan Zhu)</author>
      <guid isPermaLink="false">2501.15394v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
      <link>http://arxiv.org/abs/2502.17349v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为HybridLinker的框架被提出，以解决药物设计中连接子生成问题中的多样性和有效性之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;在药物发现应用（如候选物优化和PROTAC设计）中，分子片段组装成不同的药物候选物时链接器生成至关重要。目前的方法可以分为PC-Free和PC-Aware两类，前者基于它们是否使用3D点云(PC)。PC-Free模型更注重多样性，但因忽视了PC约束导致有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。&lt;h4&gt;目的&lt;/h4&gt;为了在不增加额外训练的情况下克服上述权衡问题，提出了一种名为HybridLinker的框架。&lt;h4&gt;方法&lt;/h4&gt;该框架的核心是LinkerDPS（链接器后验扩散采样），它是一种新的扩散后验采样方法，在PC-Free和PC-Aware空间中操作。通过一种能量启发式的函数将分子拓扑结构与3D点云联系起来，从而允许从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。&lt;h4&gt;主要发现&lt;/h4&gt;HybridLinker框架在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;h4&gt;结论&lt;/h4&gt;通过将PC-Free模型的多样化采样分布转移到PC-Aware分布上，HybridLinker在药物设计应用中的多样性和有效性之间找到了一个很好的平衡点。&lt;h4&gt;翻译&lt;/h4&gt;链接器生成是药物发现应用程序（如候选物优化和PROTAC设计）中的关键问题，其中分子片段被组装成不同的药物候选物。现有的方法根据它们是否使用3D点云(PC)分为PC-Free和PC-Aware两类。PC-Free模型优先考虑多样性但有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。为了克服这些权衡问题且不需额外训练，我们提出了HybridLinker框架，该框架通过从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。在核心部分，我们提出了一种新的扩散后验采样方法LinkerDPS，在PC-Free和PC-Aware空间之间操作，并通过一种能量启发式的函数将分子拓扑与3D点云联系起来。HybridLinker框架能够将PC-Free模型的多样化采样分布转移至PC-Aware分布上，从而在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linker generation is critical in drug discovery applications such as leadoptimization and PROTAC design, where molecular fragments are assembled intodiverse drug candidates. Existing methods fall into PC-Free and PC-Awarecategories based on their use of 3D point clouds (PC). PC-Free modelsprioritize diversity but suffer from lower validity due to overlooking PCconstraints, while PC-Aware models ensure higher validity but restrictdiversity by enforcing strict PC constraints. To overcome these trade-offswithout additional training, we propose HybridLinker, a framework that enhancesPC-Aware inference by providing diverse bonding topologies from a pretrainedPC-Free model as guidance. At its core, we propose LinkerDPS, the firstdiffusion posterior sampling (DPS) method operating across PC-Free and PC-Awarespaces, bridging molecular topology with 3D point clouds via an energy-inspiredfunction. By transferring the diverse sampling distribution of PC-Free modelsinto the PC-Aware distribution, HybridLinker significantly and consistentlysurpasses baselines, improving both validity and diversity in foundationalmolecular design and applied property optimization tasks, establishing a newDPS framework in the molecular and graph domains beyond imaging.</description>
      <author>example@mail.com (Minyeong Hwang, Ziseok Lee, Kwang-Soo Kim, Kyungsu Kim, Eunho Yang)</author>
      <guid isPermaLink="false">2502.17349v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  https://github.com/BaitingLuo/L-MAP.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架L-MAP，旨在通过学习一组时间扩展的宏观动作来解决高维连续动作空间中的顺序决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机动态的复杂环境中进行顺序决策时，尤其是在需要基于历史数据训练代理以做出决策的情况下，面临计算挑战。这些环境通常包含高维度的动作空间和不确定的状态转换。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决传统离线强化学习中遇到的问题，特别是如何有效地处理高维连续动作空间中的随机性问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP通过状态条件下的向量量化变分自动编码器(VQ-VAE)来减少行动维度，并使用蒙特卡洛树搜索(MCTS)算法在决策过程中考虑环境和行为策略的随机性。此外，还引入了一个独立学习到的先验模型作为潜在转换模型，以实现可能动作的有效采样。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP在离线强化学习设置中表现优异，在处理复杂和高维的动作空间时显示出低决策延迟，并且能够保持与基于模型的方法相匹配的表现。&lt;h4&gt;结论&lt;/h4&gt;L-MAP提供了一种有效解决具有高度不确定性和动作维度问题的策略规划方法，表明了这种方法在处理具有随机性环境中的顺序决策任务的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),which addresses this challenge by learning a set of temporally extendedmacro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.02283v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Gaussian Processes Gaussian Splatting (GP-GS)的3D重建框架，用于提高稀疏结构从运动(SfM)点云的场景重建质量。&lt;h4&gt;背景&lt;/h4&gt;3D高斯斑点方法是一种高效的逼真新视图合成方法，但其依赖于稀疏的SfM点云，导致了场景重建的质量问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应和不确定性指导的稠密化框架来改进现有的3D重建效果。&lt;h4&gt;方法&lt;/h4&gt;引入了一种多输出高斯过程模型，并提出了一种动态采样与过滤管道，利用基于GP预测的新候选点从输入2D像素和深度图中生成密集点云。&lt;h4&gt;主要发现&lt;/h4&gt;该框架通过不确定性估计指导的稀疏SfM点云稠密化提高了3D重建质量，特别是在几何一致性和稠密性方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了GP-GS框架的有效性和实用性，在合成数据集和真实世界数据集中均表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯斑点方法作为一种高效的逼真新视图合成方法已经出现。然而，其依赖于稀疏的结构从运动（SfM）点云持续地影响了场景重建的质量。为了应对这些限制，本文提出了一种新的三维重建框架——高斯过程高斯斑点（GP-GS），其中开发了一个多输出的高斯过程模型来实现对稀疏SfM点云的自适应和不确定性指导的稠密化。具体来说，我们提出了一条动态采样和过滤流水线，它通过利用基于GP预测从输入2D像素和深度图中推断新的候选点来自适应地扩展了SfM点云，并且该流程利用不确定性估计来引导高方差预测的修剪工作，确保了几何一致性并使密集点云生成成为可能。这些稠密化的点云提供了高质量的初始3D高斯分布以增强重建性能。在各种规模上的合成和真实世界数据集上进行的一系列实验验证了所提出框架的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhihaohaoran/GPGS&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting has emerged as an efficient photorealistic novel viewsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)point clouds consistently compromises the scene reconstruction quality. Toaddress these limitations, this paper proposes a novel 3D reconstructionframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-outputGaussian Process model is developed to achieve adaptive and uncertainty-guideddensification of sparse SfM point clouds. Specifically, we propose a dynamicsampling and filtering pipeline that adaptively expands the SfM point clouds byleveraging GP-based predictions to infer new candidate points from the input 2Dpixels and depth maps. The pipeline utilizes uncertainty estimates to guide thepruning of high-variance predictions, ensuring geometric consistency andenabling the generation of dense point clouds. The densified point cloudsprovide high-quality initial 3D Gaussians to enhance reconstructionperformance. Extensive experiments conducted on synthetic and real-worlddatasets across various scales validate the effectiveness and practicality ofthe proposed framework.</description>
      <author>example@mail.com (Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang)</author>
      <guid isPermaLink="false">2502.02283v3</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本文介绍了一种新的自监督图表示学习方法——FOSSIL，用于解决现有对比学习方法在利用结构模式和节点相似性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习已成为处理标注数据稀缺或不可用情况的关键方法。然而，在设计有效的预训练任务以进行自监督图表示学习方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了节点级与子图级对比学习的新方法，旨在更有效地利用图形的结构模式和节点相似性。&lt;h4&gt;方法&lt;/h4&gt;FOSSIL模型通过将标准的节点级别对比损失函数与融合Gromov-Wasserstein距离相结合，可以同时捕捉节点特征和图结构。此外，该方法适用于同构及异构图，并能动态创建视角以生成正负样本对。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在基准图数据集上的测试中，FOSSIL优于或达到了目前最先进的方法的性能水平。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地改善现有的自监督图表示学习技术，尤其在利用复杂结构模式和节点相似性方面有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;自我监督学习已成为训练深度学习模型的重要手段，尤其是在标注数据稀缺的情况下。尽管图机器学习在各个领域都有巨大的潜力，但设计有效的预训练任务以进行自监督图表示学习仍然具有挑战性。对比学习是图的自我监督学习的一种流行方法，它利用正负对来计算对比损失函数。然而，目前的图对比学习方法往往难以充分使用结构模式和节点相似性。为了解决这些问题，我们提出了一种名为Fused Gromov Wasserstein Subgraph Contrastive Learning（FOSSIL）的新方法。我们的模型集成了节点级和子图级别的对比学习，无缝结合了标准的节点级别对比损失函数与融合Gromov-Wasserstein距离。这种组合使我们的方法能够同时捕捉节点特征和图形结构。重要的是，该方法既适用于同构图也适用于异构图，并能动态创建视角以生成正负样本对。通过在基准图数据集上的广泛实验，我们证明FOSSIL比或与当前最先进的方法性能相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a key method for training deep learningmodels when labeled data is scarce or unavailable. While graph machine learningholds great promise across various domains, the design of effective pretexttasks for self-supervised graph representation learning remains challenging.Contrastive learning, a popular approach in graph self-supervised learning,leverages positive and negative pairs to compute a contrastive loss function.However, current graph contrastive learning methods often struggle to fully usestructural patterns and node similarities. To address these issues, we presenta new method called Fused Gromov Wasserstein Subgraph Contrastive Learning(FOSSIL). Our model integrates node-level and subgraph-level contrastivelearning, seamlessly combining a standard node-level contrastive loss with theFused Gromov-Wasserstein distance. This combination helps our method captureboth node features and graph structure together. Importantly, our approachworks well with both homophilic and heterophilic graphs and can dynamicallycreate views for generating positive and negative pairs. Through extensiveexperiments on benchmark graph datasets, we show that FOSSIL outperforms orachieves competitive performance compared to current state-of-the-art methods.</description>
      <author>example@mail.com (Amadou S. Sangare, Nicolas Dunou, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2502.20885v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
  <item>
      <title>Assessing zero-shot generalisation behaviour in graph-neural-network interatomic potentials</title>
      <link>http://arxiv.org/abs/2502.21317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了机器学习原子间势能模型（MLIP）在材料化学和分子化学之间的迁移能力。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习原子间势能模型的广泛应用，如何设计适用于多种应用领域的基础性MLIP成为了当前的研究重点。&lt;h4&gt;目的&lt;/h4&gt;评估一种特定于石墨烯氧化物扩展共价网络设计的MLIP（GO-MACE-23）在处理小型独立分子和化学反应时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过将该模型与专门为某一领域训练的状态-of-the-art模型进行直接比较，来量化其零样本学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;提供了图神经网络势能的迁移能力和泛化能力的定量见解。&lt;h4&gt;结论&lt;/h4&gt;这项工作促进了MLIP在化学中的更广泛应用，并为进一步研究这类模型如何在不同领域的应用中发挥作用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;随着机器学习原子间势能（MLIP）模型在化学领域迅速可用性的增长，当前许多研究集中在开发通用且“基础性”的MLIP上。在这个背景下，一个重要的问题是这些模型是否以及在多大程度上可以在不同的应用场景之间转移。在这里，我们评估了一种MLIP模型在材料和分子化学之间的迁移能力。具体来说，我们研究了GO-MACE-23模型，该模型旨在用于石墨烯氧化物的扩展共价网络，并量化了它对小型独立分子和在其直接作用范围之外的化学反应的零样本性能——与专门为某一领域训练的状态-of-the-art模型进行直接比较。我们的工作为图神经网络势能的迁移和泛化能力提供了定量见解，更广泛地说，朝着MLIP在化学中的更广泛应用迈进了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapidly growing availability of machine-learned interatomicpotential (MLIP) models for chemistry, much current research focuses on thedevelopment of generally applicable and ``foundational'' MLIPs. An importantquestion in this context is whether, and how well, such models can transferfrom one application domain to another. Here, we assess this transferabilityfor an MLIP model at the interface of materials and molecular chemistry.Specifically, we study GO-MACE-23, a model designed for the extended covalentnetwork of graphene oxide, and quantify its zero-shot performance for small,isolated molecules and chemical reactions outside its direct scope--in directcomparison with a state-of-the-art model which has been trained in-domain. Ourwork provides quantitative insight into the transfer and generalisation abilityof graph-neural-network potentials and, more generally, makes a step towardsthe more widespread applicability of MLIPs in chemistry.</description>
      <author>example@mail.com (Chiheb Ben Mahmoud, Zakariya El-Machachi, Krystian A. Gierczak, John L. A. Gardner, Volker L. Deringer)</author>
      <guid isPermaLink="false">2502.21317v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RuCCoD: Towards Automated ICD Coding in Russian</title>
      <link>http://arxiv.org/abs/2502.21263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了在俄语等生物医学资源有限的语言环境中实现临床编码自动化的可行性。&lt;h4&gt;背景&lt;/h4&gt;当前，许多语言的生物医学资源较为匮乏，特别是在俄语中。这限制了相关领域的自动化进程，如临床编码。&lt;h4&gt;目的&lt;/h4&gt;通过创建新的ICD（国际疾病分类）编码数据集来研究在俄语等资源有限的语言环境中自动进行临床编码的可能性。&lt;h4&gt;方法&lt;/h4&gt;{'数据准备': '构建了一个包含来自电子健康记录（EHRs）的诊断字段的数据集，该数据集包括超过10,000个实体和超过1,500种独特的ICD代码。', '模型测试': '利用这个数据集作为基准，测试了几种最先进的模型，如BERT、LLaMA与LoRA结合使用以及RAG。进行了额外的跨域迁移学习实验（从PubMed摘要到医学诊断）以及术语转换实验（从UMLS概念到ICD编码）。', '应用': '将性能最佳的模型应用于公司内部EHR数据集，该数据集中包含了2017年至2021年的患者历史记录。'}&lt;h4&gt;主要发现&lt;/h4&gt;利用自动化预测代码进行训练后，与医生手动注释的数据相比，在精心准备的测试集上显示出显著提高的准确性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在资源有限的语言环境中实现临床编码自动化的潜力巨大，这可以提升这些语境下的医疗效率和数据准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the feasibility of automating clinical coding inRussian, a language with limited biomedical resources. We present a new datasetfor ICD coding, which includes diagnosis fields from electronic health records(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICDcodes. This dataset serves as a benchmark for several state-of-the-art models,including BERT, LLaMA with LoRA, and RAG, with additional experiments examiningtransfer learning across domains (from PubMed abstracts to medical diagnosis)and terminologies (from UMLS concepts to ICD codes). We then apply thebest-performing model to label an in-house EHR dataset containing patienthistories from 2017 to 2021. Our experiments, conducted on a carefully curatedtest set, demonstrate that training with the automated predicted codes leads toa significant improvement in accuracy compared to manually annotated data fromphysicians. We believe our findings offer valuable insights into the potentialfor automating clinical coding in resource-limited languages like Russian,which could enhance clinical efficiency and data accuracy in these contexts.</description>
      <author>example@mail.com (Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev, Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina)</author>
      <guid isPermaLink="false">2502.21263v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Parameter Efficient Source-free Post-pretraining</title>
      <link>http://arxiv.org/abs/2502.21313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域。&lt;h4&gt;背景&lt;/h4&gt;随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决在没有源领域数据的情况下将预先训练的模型适应到新目标领域的问题。&lt;h4&gt;方法&lt;/h4&gt;{'i': '设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。', 'ii': '提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。', 'iii': '采用低秩适应方法以参数高效的方式进行模型调整。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够在各种基础架构上（包括监督和非监督训练于Imagenet上的）展示出良好的适应性和泛化能力，将其应用于八个不同的目标领域。&lt;h4&gt;结论&lt;/h4&gt;通过UpStep方法可以有效地在没有源领域数据的情况下将大规模预训练模型调整到新任务中，从而克服了计算成本的限制。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。为了解决这个问题，我们介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域：i) 设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。由于这种源无关设置存在灾难性遗忘的风险，ii) 提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。最后iii) 采用低秩适应方法以参数高效的方式进行模型调整。我们利用各种一般骨干架构作为基础模型并将其适配到八个不同的目标领域中，展示了我们的方法具有良好的适用性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the success in NLP, the best vision models are now in the billionparameter ranges. Adapting these large models to a target distribution hasbecome computationally and economically prohibitive. Addressing this challenge,we introduce UpStep, an Unsupervised Parameter-efficient Source-freepost-pretraining approach, designed to efficiently adapt a base model from asource domain to a target domain: i) we design a self-supervised trainingscheme to adapt a pretrained model on an unlabeled target domain in a settingwhere source domain data is unavailable. Such source-free setting comes withthe risk of catastrophic forgetting, hence, ii) we propose center vectorregularization (CVR), a set of auxiliary operations that minimize catastrophicforgetting and additionally reduces the computational cost by skippingbackpropagation in 50\% of the training iterations. Finally iii) we performthis adaptation process in a parameter-efficient way by adapting the pretrainedmodel through low-rank adaptation methods, resulting in a fraction ofparameters to optimize. We utilize various general backbone architectures, bothsupervised and unsupervised, trained on Imagenet as our base model and adaptthem to a diverse set of eight target domains demonstrating the adaptabilityand generalizability of our proposed approach.</description>
      <author>example@mail.com (Abhishek Jha, Tinne Tuytelaars, Yuki M. Asano)</author>
      <guid isPermaLink="false">2502.21313v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Fast 3D point clouds retrieval for Large-scale 3D Place Recognition</title>
      <link>http://arxiv.org/abs/2502.21067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于Transformer的加速3D点云检索的方法，通过生成一维标识符实现恒定时间内的直接检索。&lt;h4&gt;背景&lt;/h4&gt;在3D点云中寻找最相似的点云是一项具有挑战性的任务。当前方法主要集中在比较描述符以识别相似性，但这个步骤复杂且耗时。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Differentiable Search Index (DSI)的方法来加速3D点云检索过程。&lt;h4&gt;方法&lt;/h4&gt;通过集成视觉Transformer将点云描述符映射到一维标识符，并结合位置和语义编码以适应三维数据。这种方法使得可以直接根据生成的标识符进行恒定时间内的检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在公开基准测试中的检索质量和速度方面都优于现有最先进的技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为3D点云检索提供了一种高效且准确的新方案。&lt;h4&gt;翻译&lt;/h4&gt;三维点云的检索是一项具有挑战性的任务，旨在从参考点集中检索与给定查询最相似的点云。当前方法集中在通过比较描述符来识别相似性上。由于这一步骤复杂，我们专注于使用可微搜索索引(DSI)，一种最初为文本信息检索设计的基于Transformer的方法，来加速三维点云检索过程。我们的方法生成了一维标识符，该标识符基于点描述符，并使得可以直接在恒定时间内进行检索。为了将DSI适应于3D数据，我们整合了视觉变换器以将描述符映射到这些标识符，同时结合位置和语义编码。我们在公共基准测试的地点识别上评估了此方法，通过与现有最先进的方法比较其点云检索能力和返回的速度及质量来衡量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval in 3D point clouds is a challenging task that consists inretrieving the most similar point clouds to a given query within a reference of3D points. Current methods focus on comparing descriptors of point clouds inorder to identify similar ones. Due to the complexity of this latter step, herewe focus on the acceleration of the retrieval by adapting the DifferentiableSearch Index (DSI), a transformer-based approach initially designed for textinformation retrieval, for 3D point clouds retrieval. Our approach generates 1Didentifiers based on the point descriptors, enabling direct retrieval inconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to mapdescriptors to these identifiers while incorporating positional and semanticencoding. The approach is evaluated for place recognition on a public benchmarkcomparing its retrieval capabilities against state-of-the-art methods, in termsof quality and speed of returned point clouds.</description>
      <author>example@mail.com (Chahine-Nicolas Zede, Laurent Carrafa, Valérie Gouet-Brunet)</author>
      <guid isPermaLink="false">2502.21067v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>LV-DOT: LiDAR-visual dynamic obstacle detection and tracking for autonomous robot navigation</title>
      <link>http://arxiv.org/abs/2502.20607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;室内自主机器人导航中的动态障碍物感知对于精确导航至关重要。&lt;h4&gt;背景&lt;/h4&gt;尽管在计算机视觉和自动驾驶领域对3D物体检测和跟踪方法进行了深入研究和发展，但这些方法需要昂贵且高精度的传感器设置以及大型神经网络计算资源，使其不适合用于室内机器人。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于车载相机和LiDAR数据的动态障碍物检测与跟踪框架，以实现轻量级且精确的感知。&lt;h4&gt;方法&lt;/h4&gt;{'融合策略': '采用更稳健的数据融合策略，结合了LiDAR和视觉信息，提高了检测精度。使用特征关联和卡尔曼滤波器进行目标追踪，并设计了一种动态障碍物分类算法来可靠地识别移动物体。', '集成检测方法': '基于先前的集合检测方法，该方法整合来自多个低准确度但计算效率高的探测器的结果，以确保在车载计算机上实现实时性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;数据集评估显示，与基准方法相比，所提出的方法具有更好的感知性能；物理实验也证实了这种方法在实际导航中的可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地解决了单一传感器的限制问题，并通过结合LiDAR和视觉信息实现了更加精确且实时的动态障碍物检测与跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception of dynamic obstacles is essential for autonomous robotnavigation in indoor environments. Although sophisticated 3D object detectionand tracking methods have been investigated and developed thoroughly in thefields of computer vision and autonomous driving, their demands on expensiveand high-accuracy sensor setups and substantial computational resources fromlarge neural networks make them unsuitable for indoor robotics. Recently, morelightweight perception algorithms leveraging onboard cameras or LiDAR sensorshave emerged as promising alternatives. However, relying on a single sensorposes significant limitations: cameras have limited fields of view and cansuffer from high noise, whereas LiDAR sensors operate at lower frequencies andlack the richness of visual features. To address this limitation, we propose adynamic obstacle detection and tracking framework that uses both onboard cameraand LiDAR data to enable lightweight and accurate perception. Our proposedmethod expands on our previous ensemble detection approach, which integratesoutputs from multiple low-accuracy but computationally efficient detectors toensure real-time performance on the onboard computer. In this work, we proposea more robust fusion strategy that integrates both LiDAR and visual data toenhance detection accuracy further. We then utilize a tracking module thatadopts feature-based object association and the Kalman filter to track andestimate detected obstacles' states. Besides, a dynamic obstacle classificationalgorithm is designed to robustly identify moving objects. The datasetevaluation demonstrates a better perception performance compared to benchmarkmethods. The physical experiments on a quadcopter robot confirms thefeasibility for real-world navigation.</description>
      <author>example@mail.com (Zhefan Xu, Haoyu Shen, Xinming Han, Hanyu Jin, Kanlong Ye, Kenji Shimada)</author>
      <guid isPermaLink="false">2502.20607v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
      <link>http://arxiv.org/abs/2502.20984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;SemEval-2025 Task 1旨在根据给定的名词短语在英语和巴西葡萄牙语中可能携带的习惯用法意义对图像进行排名。&lt;h4&gt;背景&lt;/h4&gt;该任务涉及使用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示，以解决基于具有惯用含义的名词短语给图片打分的问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高图片排序的效果，本文提出了一种方法结合使用LLM和多语言CLIP模型，并应用对比学习和数据增强技术对生成的嵌入进行微调。&lt;h4&gt;方法&lt;/h4&gt;大型语言模型用于生成潜在惯用表达的意义，而这些意义随后被编码为图像排名中的表示。然后通过对比学习和技术手段调整后的数据增强来精炼这些嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，通过这种方法提取的多模态表示优于仅基于原始名词短语的方法。然而，微调方法虽然显示出有前景的结果，但不如使用未经微调的嵌入有效。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个利用大型语言模型和CLIP技术改进惯用表达图像排名的新方案，并通过实验验证了其有效性，尽管还存在进一步优化的空间。&lt;h4&gt;翻译&lt;/h4&gt;SemEval-2025 Task 1专注于根据具有潜在习惯意义的名词短语对图片进行排序。为了解决这个问题，这项工作利用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示。通过这种方式，研究者们在提高图像排名精度方面取得了显著进展，并且他们的源代码可在GitHub上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SemEval-2025 Task 1 focuses on ranking images based on their alignment with agiven nominal compound that may carry idiomatic meaning in both English andBrazilian Portuguese. To address this challenge, this work uses generativelarge language models (LLMs) and multilingual CLIP models to enhance idiomaticcompound representations. LLMs generate idiomatic meanings for potentiallyidiomatic compounds, enriching their semantic interpretation. These meaningsare then encoded using multilingual CLIP models, serving as representations forimage ranking. Contrastive learning and data augmentation techniques areapplied to fine-tune these embeddings for improved performance. Experimentalresults show that multimodal representations extracted through this methodoutperformed those based solely on the original nominal compounds. Thefine-tuning approach shows promising outcomes but is less effective than usingembeddings without fine-tuning. The source code used in this paper is availableat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.</description>
      <author>example@mail.com (Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang)</author>
      <guid isPermaLink="false">2502.20984v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>JiTTER: Jigsaw Temporal Transformer for Event Reconstruction for Self-Supervised Sound Event Detection</title>
      <link>http://arxiv.org/abs/2502.20857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了JiTTER，一种改进的自监督学习框架，用于提高基于变压器的声音事件检测模型在时间建模方面的性能。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习方法特别是MAT-SED在声音事件检测（SED）中取得了显著效果。然而，这种技术在捕捉瞬态音频事件和保持时间顺序方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督框架JiTTER，以增强基于变压器的声音事件检测模型的时间建模能力。&lt;h4&gt;方法&lt;/h4&gt;JiTTER引入了一种分层的随机时间重排重建策略，在块级和帧级随机打乱音频序列，强迫模型重建正确的时序。同时通过在块级重排过程中注入噪声进一步提升特征学习的正则化效果和模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明JiTTER相比MAT-SED提升了5.89%的PSDS值，在DESED数据集上表现出更优性能，说明结构化的时序重建任务比简单的掩码预测更有助于自监督学习在声音事件表示学习中的表现。&lt;h4&gt;结论&lt;/h4&gt;研究发现证明了有结构的时间重建任务对于基于自监督学习的声音事件检测模型来说是一种更为有效的预训练范式。&lt;h4&gt;翻译&lt;/h4&gt;声音事件检测（SED）已从自我监督学习（SSL）方法中显著受益，特别是MAT-SED，该方法利用掩码块预测来恢复丢失的音频片段。然而，尽管在捕捉全局依赖性方面有效，掩码块预测却破坏了瞬态声学事件并且缺乏对时间顺序的明确约束，使其不太适合用于精细粒度的事件边界检测。为了克服这些限制，我们提出了JiTTER（拼图时序变压器事件重建），这是一种增强基于变压器的声音事件检测的时间建模能力的SSL框架。JiTTER引入了一种层次化的随机时间重排重构策略，其中音频序列在块级和帧级上随机打乱，强迫模型恢复正确的顺序。这种预训练目标鼓励模型学习全局事件结构以及瞬态细节，从而提高其识别具有锐利起止特征的声音事件的能力。此外，在块重组过程中我们注入噪声，提供了一种细微的扰动机制以进一步正则化特征学习并增强模型鲁棒性。DESED数据集上的实验结果表明JiTTER优于MAT-SED，PSDS值提高了5.89%，这突出了显式时间推理在基于SSL的声音事件检测中的有效性。我们的研究结果表明结构化的时序重建任务比简单的掩码预测更适合声音事件表示学习的自监督预训练范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sound event detection (SED) has significantly benefited from self-supervisedlearning (SSL) approaches, particularly masked audio transformer for SED(MAT-SED), which leverages masked block prediction to reconstruct missing audiosegments. However, while effective in capturing global dependencies, maskedblock prediction disrupts transient sound events and lacks explicit enforcementof temporal order, making it less suitable for fine-grained event boundarydetection. To address these limitations, we propose JiTTER (Jigsaw TemporalTransformer for Event Reconstruction), an SSL framework designed to enhancetemporal modeling in transformer-based SED. JiTTER introduces a hierarchicaltemporal shuffle reconstruction strategy, where audio sequences are randomlyshuffled at both the block-level and frame-level, forcing the model toreconstruct the correct temporal order. This pretraining objective encouragesthe model to learn both global event structures and fine-grained transientdetails, improving its ability to detect events with sharp onset-offsetcharacteristics. Additionally, we incorporate noise injection during blockshuffle, providing a subtle perturbation mechanism that further regularizesfeature learning and enhances model robustness. Experimental results on theDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%improvement in PSDS, highlighting the effectiveness of explicit temporalreasoning in SSL-based SED. Our findings suggest that structured temporalreconstruction tasks, rather than simple masked prediction, offer a moreeffective pretraining paradigm for sound event representation learning.</description>
      <author>example@mail.com (Hyeonuk Nam, Yong-Hwa Park)</author>
      <guid isPermaLink="false">2502.20857v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.21196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为AMPLE的FPGA加速器，用于改进图神经网络（GNN）在非欧几里得数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;最近，由于其处理非欧几里得数据的性能，图神经网络受到了广泛关注。这些网络因其不规则的记忆访问模式而特别受益于自定义硬件架构，这种模式源于图形结构的稀疏性。&lt;h4&gt;目的&lt;/h4&gt;解决现有FPGA加速器中双缓冲机制的问题，并针对典型图形数据集中节点分布不规则的情况提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用事件驱动编程流程的新AMPLE FPGA加速器。开发了混合算术架构，使GNN推理可以以节点级别进行量化。实现了用于优化片外内存访问和最大化节点并行性的预取器。&lt;h4&gt;主要发现&lt;/h4&gt;在引用和社交媒体图数据集上进行了评估，结果表明，在与CPU和GPU对应方相比，平均速度分别提高了243倍和7.2倍。&lt;h4&gt;结论&lt;/h4&gt;采用事件驱动编程流程的AMPLE FPGA加速器显著提升了GNN推理的速度。该方法通过利用混合算术架构、节点级量化以及片外内存访问优化来解决现有FPGA加速器中存在的问题，并在大量图数据集上展示了优秀的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;最近，由于其处理非欧几里得数据的出色表现，图神经网络（GNNs）引起了广泛的关注。这些网络得益于它们不规则的记忆访问模式，这源于图形结构的稀疏性，因此，对定制硬件架构特别有利。然而，现有的FPGA加速器受到双缓冲机制的限制，这种机制未能考虑典型图数据集中的节点分布不规则问题。为了应对这一挑战，我们提出了AMPLE（加速消息传递逻辑引擎），这是一个利用新的事件驱动编程流的新FPGA加速器。我们开发了一种混合算术架构，使GNN推理能够在节点级别进行量化。此外，还实现了用于优化片外内存访问和最大化节点并行性的预取器。在引用和社交媒体图数据集上进行了评估，这些数据集的节点数量从2K到700K不等，结果表明，在与CPU和GPU对应方相比时，平均速度分别提高了243倍和7.2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently gained attention due to theirperformance on non-Euclidean data. The use of custom hardware architecturesproves particularly beneficial for GNNs due to their irregular memory accesspatterns, resulting from the sparse structure of graphs. However, existing FPGAaccelerators are limited by their double buffering mechanism, which doesn'taccount for the irregular node distribution in typical graph datasets. Toaddress this, we introduce \textbf{AMPLE} (Accelerated Message Passing LogicEngine), an FPGA accelerator leveraging a new event-driven programming flow. Wedevelop a mixed-arithmetic architecture, enabling GNN inference to be quantizedat a node-level granularity. Finally, prefetcher for data and instructions isimplemented to optimize off-chip memory access and maximize node parallelism.Evaluation on citation and social media graph datasets ranging from $2$K to$700$K nodes showed a mean speedup of $243\times$ and $7.2\times$ against CPUand GPU counterparts, respectively.</description>
      <author>example@mail.com (Pedro Gimenes, Yiren Zhao, George Constantinides)</author>
      <guid isPermaLink="false">2502.21196v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating Long-Range Interactions via the Multipole Expansion into Ground and Excited-State Molecular Simulations</title>
      <link>http://arxiv.org/abs/2502.21045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了FieldMACE，这是基于消息传递原子簇扩展（MACE）架构的一种改进版本，通过引入多极展开来更高效地模拟长程相互作用。&lt;h4&gt;背景&lt;/h4&gt;在分子机器学习势能中，准确捕捉大空间区域内的相互作用是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;为了提高对环境和远距离效应的建模效率，特别是在基态和激发态下，本文提出了一种新的架构FieldMACE。&lt;h4&gt;方法&lt;/h4&gt;通过将多极展开集成到MACE架构中，形成一种新框架，称为FieldMACE。&lt;h4&gt;主要发现&lt;/h4&gt;基准评估显示FieldMACE在预测精度、计算效率方面优于先前的架构，并且能够准确模拟非绝热激发态动力学。&lt;h4&gt;结论&lt;/h4&gt;从基础模型中的迁移学习进一步提高了数据利用效率，使FieldMACE成为大规模分子模拟中可扩展、稳健和可转移的框架。&lt;h4&gt;翻译&lt;/h4&gt;模拟长程相互作用一直是分子机器学习势能的重要挑战。本文介绍了一种新的架构FieldMACE，它通过将多极展开整合到消息传递原子簇扩展（MACE）架构中来更高效地建模长程相互作用。FieldMACE能够有效捕捉环境和远距离效应，特别是在基态和激发态下。基准评估表明，与之前的架构相比，FieldMACE在预测准确性、计算效率方面具有优势，并且能准确模拟非绝热激发态动力学。此外，从基础模型中的迁移学习进一步提高了数据利用效率，使得FieldMACE成为一个可扩展的、稳健的以及可转移的大规模分子模拟框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating long-range interactions remains a significant challenge formolecular machine learning potentials due to the need to accurately captureinteractions over large spatial regions. In this work, we introduce FieldMACE,an extension of the message-passing atomic cluster expansion (MACE)architecture that integrates the multipole expansion to model long-rangeinteractions more efficiently. By incorporating the multipole expansion,FieldMACE effectively captures environmental and long-range effects in bothground and excited states. Benchmark evaluations demonstrate its superiorperformance in predictions and computational efficiency compared to previousarchitectures, as well as its ability to accurately simulate nonadiabaticexcited-state dynamics. Furthermore, transfer learning from foundational modelsenhances data efficiency, making FieldMACE a scalable, robust, and transferableframework for large-scale molecular simulations.</description>
      <author>example@mail.com (Rhyan Barrett, Johannes C. B. Dietschreit, Julia Westermayr)</author>
      <guid isPermaLink="false">2502.21045v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image</title>
      <link>http://arxiv.org/abs/2502.20861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了MESC-3D，一种从单张图像中重建3D形状的新方法。&lt;h4&gt;背景&lt;/h4&gt;目前的方法主要集中在从图像中提取语义信息并将其简单地与3D点云连接起来，而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征显著阻碍了重建性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够主动挖掘有效语义线索以提高单张图像中的3D重建质量的方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一个有效的语义挖掘模块和一个三维语义先验学习模块，前者建立了点云与图像语义属性之间的联系，后者利用先验知识增强模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在重建质量和鲁棒性方面显著优于先前的工作，并且具有强大的泛化能力，在零样本性能上也表现出色。&lt;h4&gt;结论&lt;/h4&gt;MESC-3D通过主动挖掘有效语义线索和使用三维语义先验知识，提高了单张图像中的3D重建的准确性和现实感。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像中重建3D形状在计算机视觉领域扮演着重要角色。许多方法已经被提出并取得了显著的成绩。然而，现有的方法主要集中在提取图像中的语义信息，并简单地将其与3D点云连接起来而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征极大地阻碍了重建性能。本文提出了一个名为MESC-3D的新方法，它可以主动挖掘有效语义线索以改进单张图像的3D重建效果。具体而言，设计了一个有效的语义挖掘模块来建立点云和图像语义属性之间的联系，并使点云能够自主选择所需信息。此外，为了解决单一图像中的语义信息可能存在的不足问题（如遮挡），受人类利用日常经验中获得的先验知识表示3D对象能力的启发，我们引入了三维语义先验学习模块。此模块集成了对空间结构的语义理解，使模型能够更准确地解释和重建3D对象，并且在复杂3D环境感知方面更加贴近人类的认知。广泛的评估显示，与先前的工作相比，我们的方法在重建质量和鲁棒性方面取得了显著改进。此外，进一步的实验验证了该方法的强大泛化能力和零样本性能上的优越表现。代码可在https://github.com/QINGQINGLE/MESC-3D上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D shapes from a single image plays an important role incomputer vision. Many methods have been proposed and achieve impressiveperformance. However, existing methods mainly focus on extracting semanticinformation from images and then simply concatenating it with 3D point cloudswithout further exploring the concatenated semantics. As a result, theseentangled semantic features significantly hinder the reconstructionperformance. In this paper, we propose a novel single-image 3D reconstructionmethod called Mining Effective Semantic Cues for 3D Reconstruction from aSingle Image (MESC-3D), which can actively mine effective semantic cues fromentangled features. Specifically, we design an Effective Semantic Mining Moduleto establish connections between point clouds and image semantic attributes,enabling the point clouds to autonomously select the necessary information.Furthermore, to address the potential insufficiencies in semantic informationfrom a single image, such as occlusions, inspired by the human ability torepresent 3D objects using prior knowledge drawn from daily experiences, weintroduce a 3D Semantic Prior Learning Module. This module incorporatessemantic understanding of spatial structures, enabling the model to interpretand reconstruct 3D objects with greater accuracy and realism, closely mirroringhuman perception of complex 3D environments. Extensive evaluations show thatour method achieves significant improvements in reconstruction quality androbustness compared to prior works. Additionally, further experiments validatethe strong generalization capabilities and excels in zero-shot preformance onunseen classes. Code is available at https://github.com/QINGQINGLE/MESC-3D.</description>
      <author>example@mail.com (Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu)</author>
      <guid isPermaLink="false">2502.20861v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了基于自监督预训练的大型基础模型（FMs）在前列腺癌诊断和Gleason分级任务中的临床应用效果，发现尽管这些模型在数据稀缺的情况下表现出一定的实用性，但在充分标注的数据集上其性能可能不及针对特定任务训练的模型。&lt;h4&gt;背景&lt;/h4&gt;人工智能在病理学中的角色已从辅助诊断发展到揭示整体切片图像（WSIs）中预测形态模式。近年来，基于自监督预训练的基础模型被广泛推崇为适用于各种下游任务的通用解决方案。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过大规模验证AI系统，在前列腺癌诊断和Gleason分级任务上评估基础模型与特定任务端到端学习模型之间的性能差异。&lt;h4&gt;方法&lt;/h4&gt;研究使用超过10万名患者的7342例核心针活检数据，涵盖全球15个地点的11个国家。采用多实例学习框架对两种基础模型和一个完全端到端训练的任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果挑战了基础模型总是优于任务特定模型的观点。在缺乏标注数据的情况下，基础模型显示出一定优势；然而，在有足够的标记训练数据时，其性能与特有任务模型相匹配或甚至被超越。此外，特定于任务的培训显著减少了临床重要分级错误和难以识别形态的误诊，并降低了不同WSI扫描器间的变异性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，尽管基础模型在快速原型设计和研究中具有明显优势，但它们作为适用于临床应用的医疗AI通用解决方案的角色仍然不确定。对于高风险的应用场景，严格的验证和对特定任务培训的关注仍然是至关重要的。研究者建议将基础模型与端到端学习的优点相结合，以实现适合临床使用的稳健且资源高效的病理学AI解决方案。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要描述了人工智能在病理诊断中的作用从辅助诊断到发现整个切片图像中预测性形态模式的发展历程，并讨论了基于自监督预训练的基础模型是否能成为适用于各种任务的通用解方案。研究结果表明，基础模型虽然在数据稀缺的情况下有其独特优势，但在大量标记训练数据下可能不及特定任务模型的表现。这项工作强调，在高风险临床应用中，必须进行严格的验证以确定最佳解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics</title>
      <link>http://arxiv.org/abs/2502.20518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个理论框架，用于解释在图像美学评估（IAA）任务中从通用模型转移到个人化模型的机制，并通过实验证明了不同群体和个体间存在的显著性能差异。&lt;h4&gt;背景&lt;/h4&gt;当前的图像美学评估方法分为两个阶段：第一阶段使用通用图像美学评估（GIAA）模型来估计平均分数，第二阶段利用转移学习的个性化图像美学评估（PIAA）模型来适应用户的主观性。然而，这种从GIAA到PIAA的理论理解仍不充分。&lt;h4&gt;目的&lt;/h4&gt;本文旨在建立一个理论基础，并提出了一种统一模型，该模型可以同时处理个体和群体评估任务。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一个编码个人特征并在分布格式中表示的统一模型。实验通过不同的群体制样进行了验证，包括根据组大小进行子采样和分离的人口统计学变量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 转移学习从GIAA到PIAA涉及外推，反之则是内插；2. 教育水平是影响美学差异的主要因素，其次是摄影艺术经验；3. 在艺术品中观察到了更强的个人主观性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的模型能够同时支持通用和个性化图像美学评估，并且可以提高在不同人口统计学群体中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;图像美学评估（IAA）是一种复杂任务，由于图像多样性和用户主体性的存在而变得更为复杂。当前的方法将其分为两个阶段：第一阶段使用通用的图像美学评估模型来估计平均分数；第二阶段利用转移学习将GIAA适应为PIAA以融入用户的主观性。然而，缺乏关于在GIAA和PIAA之间进行转移学习时的理论理解，特别是在考虑群体构成、群体规模、个体之间的审美差异以及人口统计学相关性的背景下。本文提出了一个统一模型，该模型使用分布形式编码个人特征来进行个体及群体制评估。我们证明了从GIAA转移到PIAA涉及外推而相反则为内插，后者通常对机器学习更有益。通过对不同构成的群体进行实验（包括按组大小子采样和分离的人口统计学变量）发现，即使对于GIAA来说，性能也表现出显著变化，表明平均分数并不能完全消除个体主观性。性能差异分析以及基尼指数分析显示教育水平是影响审美差异的主要因素，其次是摄影及艺术经验，在艺术品中观察到更强的个人主体性。我们的模型独特地支持了通用和个性化图像美学评估，并提高了不同人口统计学群体中的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image aesthetic assessment (IAA) evaluates image aesthetics, a taskcomplicated by image diversity and user subjectivity. Current approachesaddress this in two stages: Generic IAA (GIAA) models estimate mean aestheticscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning toincorporate user subjectivity. However, a theoretical understanding of transferlearning between GIAA and PIAA, particularly concerning the impact of groupcomposition, group size, aesthetic differences between groups and individuals,and demographic correlations, is lacking. This work establishes a theoreticalfoundation for IAA, proposing a unified model that encodes individualcharacteristics in a distributional format for both individual and groupassessments. We show that transferring from GIAA to PIAA involvesextrapolation, while the reverse involves interpolation, which is generallymore effective for machine learning. Experiments with varying groupcompositions, including sub-sampling by group size and disjoint demographics,reveal significant performance variation even for GIAA, indicating that meanscores do not fully eliminate individual subjectivity. Performance variationsand Gini index analysis reveal education as the primary factor influencingaesthetic differences, followed by photography and art experience, withstronger individual subjectivity observed in artworks than in photos. Our modeluniquely supports both GIAA and PIAA, enhancing generalization acrossdemographics.</description>
      <author>example@mail.com (Li-Wei Chen, Ombretta Strafforello, Anne-Sofie Maerten, Tinne Tuytelaars, Johan Wagemans)</author>
      <guid isPermaLink="false">2502.20518v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Human Beliefs about AI Behavior for Scalable Oversight</title>
      <link>http://arxiv.org/abs/2502.21262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了如何通过建模人类评估者的信念来改进对AI系统的监督，以解决随着AI能力提升而导致的人类反馈可靠性降低的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的AI系统通常依赖于人类反馈来学习人类的价值观和偏好。然而，随着AI系统的能力增强，这种人类反馈变得越来越不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过建模人的信念来提高对超出人类能力范围的AI系统的监督效率。&lt;h4&gt;方法&lt;/h4&gt;提出了形式化的模型来描述人对于AI行为的看法，并分析了这些模型在推断人类价值观中的作用。同时引入了一个放松版的人类信念模型覆盖概念，以减少依赖于精确信念模型的需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和实验研究揭示了如何利用基础模型构建覆盖信念模型的潜力，为可扩展监督提供了新方法。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够帮助更好地理解人类反馈，并且提供了一种使用基础AI系统来提高监督效率的新途径。&lt;h4&gt;翻译&lt;/h4&gt;当代的人工智能（AI）对齐工作经常依赖于人类反馈来教导AI系统学习人类的价值观和偏好。然而，随着AI系统的功能增强，这种人类反馈变得越来越不可靠。这导致了一个可扩展监控的问题：如何监管超出人类能力的AI系统？在这项工作中，我们提出通过建模人评估者对于AI行为的看法来更好地解释人的反馈。我们将人类信念模型形式化，并从理论上分析它们在推断人类价值观中的作用。然后描述了这种推理中剩余的不确定性以及这些不确定性消失的情况条件。为了减少对精确信念模型的依赖，我们引入了一个放松版的人类信念模型覆盖概念。最后，我们建议使用基础模型来构建覆盖信念模型，为可扩展监督提供了一种新的潜在方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contemporary work in AI alignment often relies on human feedback to teach AIsystems human preferences and values. Yet as AI systems grow more capable,human feedback becomes increasingly unreliable. This raises the problem ofscalable oversight: How can we supervise AI systems that exceed humancapabilities? In this work, we propose to model the human evaluator's beliefsabout the AI system's behavior to better interpret the human's feedback. Weformalize human belief models and theoretically analyze their role in inferringhuman values. We then characterize the remaining ambiguity in this inferenceand conditions for which the ambiguity disappears. To mitigate reliance onexact belief models, we then introduce the relaxation of human belief modelcovering. Finally, we propose using foundation models to construct coveringbelief models, providing a new potential approach to scalable oversight.</description>
      <author>example@mail.com (Leon Lang, Patrick Forré)</author>
      <guid isPermaLink="false">2502.21262v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dimension Agnostic Neural Processes</title>
      <link>http://arxiv.org/abs/2502.20661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted to ICLR 2025 (International Conference  on Learning Representations)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的元学习模型Dimension Agnostic Neural Processes(DANP)，该模型通过引入Dimension Aggregator Block(DAB)和Transformer架构，增强了处理不同维度输入的能力，并在各种回归任务上显示出优越性能。&lt;h4&gt;背景&lt;/h4&gt;传统的Neural Process(NP)方法虽然能够提取跨多种任务的数据共享特征并预测不确定性，但在适应不同输入维度的任务时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种元学习模型DANP，以克服传统NP模型的局限性，并提升其在回归任务中的适用性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入Dimension Aggregator Block(DAB)将输入特征转换为固定维度的空间，同时采用Transformer架构和潜在编码层来学习更具普适性的特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了DANP模型相比于现有NP变体在合成数据集与实际回归任务上的优越性。&lt;h4&gt;结论&lt;/h4&gt;DANP展示了其解决传统NP模型局限性和广泛适应各种回归场景的潜力，具有较高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;元学习的目标是训练可以使用有限标注数据推广到新任务的模型，通过提取多样任务数据集中的共享特征。此外，在训练和评估期间考虑预测不确定性，这是一个称为不确定感知元学习的概念。神经过程(NP)是一种著名的不确定感知元学习方法，它利用参数化神经网络构建隐式随机过程，使快速适应新任务成为可能。然而，现有的NP方法在处理多样输入维度和学习特征方面存在挑战，限制了它们在回归任务中的广泛应用性。为了克服这些局限并提高NP模型作为通用回归器的实用性，我们引入了Dimension Agnostic Neural Processes(DANP)。DANP采用Dimension Aggregator Block(DAB)，将输入特征转换为固定维度空间，增强模型处理多样数据集的能力；同时利用Transformer架构和潜在编码层，学习可跨多种任务泛化的更广泛特征。通过在各种合成和实际回归任务上的综合实验，我们实证显示了DANP优于先前的NP变体，在克服传统NP模型局限性方面展示出其有效性及其应用于多样化回归场景的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning aims to train models that can generalize to new tasks withlimited labeled data by extracting shared features across diverse taskdatasets. Additionally, it accounts for prediction uncertainty during bothtraining and evaluation, a concept known as uncertainty-aware meta-learning.Neural Process(NP) is a well-known uncertainty-aware meta-learning method thatconstructs implicit stochastic processes using parametric neural networks,enabling rapid adaptation to new tasks. However, existing NP methods facechallenges in accommodating diverse input dimensions and learned features,limiting their broad applicability across regression tasks. To address theselimitations and advance the utility of NP models as general regressors, weintroduce Dimension Agnostic Neural Processes(DANP). DANP incorporatesDimension Aggregator Block(DAB) to transform input features into afixed-dimensional space, enhancing the model's ability to handle diversedatasets. Furthermore, leveraging the Transformer architecture and latentencoding layers, DANP learns a wider range of features that are generalizableacross various tasks. Through comprehensive experimentation on varioussynthetic and practical regression tasks, we empirically show that DANPoutperforms previous NP variations, showcasing its effectiveness in overcomingthe limitations of traditional NP models and its potential for broaderapplicability in diverse regression scenarios.</description>
      <author>example@mail.com (Hyungi Lee, Chaeyun Jang, Dongbok Lee, Juho Lee)</author>
      <guid isPermaLink="false">2502.20661v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Parallel-Learning of Invariant and Tempo-variant Attributes of Single-Lead Cardiac Signals: PLITA</title>
      <link>http://arxiv.org/abs/2502.21162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in The 39th Annual AAAI Conference on Artificial  Intelligence. Main Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的自监督学习方法PLITA，用于捕捉单导联心电图（ECG）信号中的不变和时变属性。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备在未来的数字健康领域中将发挥重要作用。目前的自监督学习方法只能编码不变属性，忽略了反映状态变化的时间变异信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时间不变和时间变异心电图特征的新SSL方法。&lt;h4&gt;方法&lt;/h4&gt;通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。&lt;h4&gt;主要发现&lt;/h4&gt;PLITA在时间变异属性起重要作用的设置中表现出显著更好的性能。&lt;h4&gt;结论&lt;/h4&gt;PLITA是一种有效的自监督学习框架，能够有效处理心电图信号中的不变和时变信息。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备如Holter监护仪将在未来的数字健康领域扮演关键角色。无监督的学习框架（例如自我监督学习）对于将这些单导联的心电信号映射到预期的临床结果至关重要。这种信号具有时间变异成分，其模式随记录过程而演变，并且还存在不变成分，其模式保持不变。然而，现有的SSL方法只能驱动模型编码不变属性，导致模型忽略反映状态变化的时间变异信息。本文介绍了一种新的SSL方法——并行学习不变和时变属性（PLITA），该方法旨在捕捉这两种类型的心电图特征。通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。我们评估了此方法在学习两种不同类型的特征方面的能力，以及与现有ECG分析的SSL方法相比的性能表现。在时间变异属性起重要作用的情况下，PLITA表现出显著更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Holter monitors, will play a crucial rolein the future of digital health. Unsupervised learning frameworks such asSelf-Supervised Learning (SSL) are essential to map these single-leadelectrocardiogram (ECG) signals with their anticipated clinical outcomes. Thesesignals are characterized by a tempo-variant component whose patterns evolvethrough the recording and an invariant component with patterns that remainunchanged. However, existing SSL methods only drive the model to encode theinvariant attributes, leading the model to neglect tempo-variant informationwhich reflects subject-state changes through time. In this paper, we presentParallel-Learning of Invariant and Tempo-variant Attributes (PLITA), a novelSSL method designed for capturing both invariant and tempo-variant ECGattributes. The latter are captured by mandating closer representations inspace for closer inputs on time. We evaluate both the capability of the methodto learn the attributes of these two distinct kinds, as well as PLITA'sperformance compared to existing SSL methods for ECG analysis. PLITA performssignificantly better in the set-ups where tempo-variant attributes play a majorrole.</description>
      <author>example@mail.com (Adtian Atienza, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21162v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Adversarial Text Representation Learning for Affective Recognition</title>
      <link>http://arxiv.org/abs/2502.20613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, The 7th International Conference on Artificial  Intelligence in Information and Communication (ICAIIC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种增强情感感知嵌入的框架，旨在改进基于变压器模型的情感识别能力。', '背景': '预训练语言模型在语义理解方面表现出色，但在捕捉细微的情感信息方面存在困难。', '目的': '通过引入连续的效价唤醒标签系统和动态令牌扰动机制来提高情感敏感性。', '方法': '采用连续的valence-arousal标注体系进行对比学习，并利用基于梯度的方法强调与情感相关的令牌。', '主要发现': '实验结果表明，该框架在情绪分类基准上比现有方法平均提高了15.5%。', '结论': '所提出的框架有效增强了情感表示的学习能力，并能实现精确且上下文相关的情感理解。', '翻译': '摘要原文的中文翻译。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While pre-trained language models excel at semantic understanding, they oftenstruggle to capture nuanced affective information critical for affectiverecognition tasks. To address these limitations, we propose a novel frameworkfor enhancing emotion-aware embeddings in transformer-based models. Ourapproach introduces a continuous valence-arousal labeling system to guidecontrastive learning, which captures subtle and multi-dimensional emotionalnuances more effectively. Furthermore, we employ a dynamic token perturbationmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,improving model sensitivity to emotional cues. The experimental resultsdemonstrate that the proposed framework outperforms existing methods, achievingup to 15.5% improvement in the emotion classification benchmark, highlightingthe importance of employing continuous labels. This improvement demonstratesthat the proposed framework is effective in affective representation learningand enables precise and contextually relevant emotional understanding.</description>
      <author>example@mail.com (Seungah Son, Andrez Saurez, Dongsoo Har)</author>
      <guid isPermaLink="false">2502.20613v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Subtask-Aware Visual Reward Learning from Segmented Demonstrations</title>
      <link>http://arxiv.org/abs/2502.20630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://changyeon.site/reds/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;强化学习（RL）代理在各种机器人任务中展示了其潜力。然而，它们仍然严重依赖于人工设计的奖励函数，并且需要大量的试错以及目标行为信息，在现实世界的应用场景中这些信息往往是不可用的。本文提出了REDs：一种从演示视频片段中学习回报的新框架，该框架利用无动作标记视频进行最小监督的学习。具体而言，REDs使用来自不同来源的动作分段视频并将它们视为真实奖励信号。我们训练一个基于视频片段和相应子任务的密集奖励函数，并通过最小化等价策略不变比较距离来确保与真实奖励信号对齐。此外，我们采用对比学习目标以使视频表示与子任务保持一致，在线交互时可以实现精确的子任务推理。实验表明，REDs在Meta-World中的复杂机器人操作任务以及FurnitureBench中的家具组装等更具挑战性的现实世界任务中显著优于基线方法，并且只需要最小的人工干预。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理已经在多种机器人任务上展示了其潜力，但这些代理依然严重依赖于人工设计的奖励函数。这需要大量的试错过程和对目标行为信息的访问，在许多实际应用场合下这类信息是难以获得的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的从演示中无监督地学习回报的方法REDs，该方法能够利用来自不同来源的视频演示片段进行训练，并且只需要最小的人工干预。&lt;h4&gt;方法&lt;/h4&gt;1. REDs使用动作标记的视频作为输入并将其分割成子任务。2. 利用这些子任务和它们对应的视频段来学习密集奖励函数。3. 通过优化特定的目标函数（即等价策略不变比较距离）以确保奖励信号与真实信号一致。4. 使用对比学习目标使视频表示与子任务保持一致，从而在在线交互中实现精确的子任务推理。&lt;h4&gt;主要发现&lt;/h4&gt;REDs框架能够在Meta-World中的复杂机器人操作任务以及家具组装等更具挑战性的现实世界任务上表现出色，并且显著优于基线方法。此外，在最小的人工干预下该模型还能够推广到未知的任务和机器人的实例中，展示出其在多变环境下的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;REDs是一种高效的学习框架，它通过利用视频演示片段中的信息来减轻对人工设计奖励函数的依赖，并且能够在多种机器人操作任务上表现出色。此外，REDs显示出良好的泛化能力，这表明它具有广泛的应用潜力和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) agents have demonstrated their potential acrossvarious robotic tasks. However, they still heavily rely on human-engineeredreward functions, requiring extensive trial-and-error and access to targetbehavior information, often unavailable in real-world settings. This paperintroduces REDS: REward learning from Demonstration with Segmentations, a novelreward learning framework that leverages action-free videos with minimalsupervision. Specifically, REDS employs video demonstrations segmented intosubtasks from diverse sources and treats these segments as ground-truthrewards. We train a dense reward function conditioned on video segments andtheir corresponding subtasks to ensure alignment with ground-truth rewardsignals by minimizing the Equivalent-Policy Invariant Comparison distance.Additionally, we employ contrastive learning objectives to align videorepresentations with subtasks, ensuring precise subtask inference during onlineinteractions. Our experiments show that REDS significantly outperforms baselinemethods on complex robotic manipulation tasks in Meta-World and morechallenging real-world tasks, such as furniture assembly in FurnitureBench,with minimal human intervention. Moreover, REDS facilitates generalization tounseen tasks and robot embodiments, highlighting its potential for scalabledeployment in diverse environments.</description>
      <author>example@mail.com (Changyeon Kim, Minho Heo, Doohyun Lee, Jinwoo Shin, Honglak Lee, Joseph J. Lim, Kimin Lee)</author>
      <guid isPermaLink="false">2502.20630v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.21134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Dynamically Local-Enhancement (DLE) Planner，一种在不永久修改基本驾驶规划器的情况下通过局部驾驶数据动态增强驾驶规划的技术。&lt;h4&gt;背景&lt;/h4&gt;当前自主车辆主要限于特定区域运行，但对更广泛的应用需求日益增长。随着模型规模的扩大，有限的容量成为适应新场景的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决单个庞大模型在处理新情况时效率低下的问题，通过局部驾驶数据动态增强基本驾驶规划器以提高自主驾驶系统的可扩展性而不显著增加规划器的大小。&lt;h4&gt;方法&lt;/h4&gt;引入了位置变化的马尔科夫决策过程(MDP)与图神经网络结合使用的方法，从本地观察数据中提取特定区域的驾驶特征，并利用学习到的特征增强基于强化学习的基本策略。&lt;h4&gt;主要发现&lt;/h4&gt;在多个场景下评估该方法并与适用于所有情况的单一驾驶模型比较后，结果显示本方法在安全性和平均奖励方面都优于基准策略，同时保持较低的规模。&lt;h4&gt;结论&lt;/h4&gt;这种技术有潜力使大规模自主车辆受益，无需大幅扩展设备上的驾驶模型。&lt;h4&gt;翻译&lt;/h4&gt;当前自主车辆主要限于特定区域运行。随着对更广泛应用的需求增加，现有模型在处理新场景时表现出容量限制问题。单个庞大模型难以适应新的情况。本文提出Dynamically Local-Enhancement (DLE) Planner，该方法通过局部驾驶数据动态增强基本驾驶规划器，不需永久修改其本身。通过位置变化的马尔科夫决策过程结合图神经网络从本地观察数据中提取区域特定的驾驶特征，使用这些特征来增强基于强化学习的基本策略。实验结果表明，在安全性和平均奖励方面优于基准模型，并保持较小规模，具备大规模自主车辆应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current autonomous vehicles operate primarily within limited regions, butthere is increasing demand for broader applications. However, as models scale,their limited capacity becomes a significant challenge for adapting to novelscenarios. It is increasingly difficult to improve models for new situationsusing a single monolithic model. To address this issue, we introduce theconcept of dynamically enhancing a basic driving planner with local drivingdata, without permanently modifying the planner itself. This approach, termedthe Dynamically Local-Enhancement (DLE) Planner, aims to improve thescalability of autonomous driving systems without significantly expanding theplanner's size. Our approach introduces a position-varying Markov DecisionProcess formulation coupled with a graph neural network that extractsregion-specific driving features from local observation data. The learnedfeatures describe the local behavior of the surrounding objects, which is thenleveraged to enhance a basic reinforcement learning-based policy. We evaluatedour approach in multiple scenarios and compared it with a one-for-all drivingmodel. The results show that our method outperforms the baseline policy in bothsafety (collision rate) and average reward, while maintaining a lighter scale.This approach has the potential to benefit large-scale autonomous vehicleswithout the need for largely expanding on-device driving models.</description>
      <author>example@mail.com (Nanshan Deng, Weitao Zhou, Bo Zhang, Junze Wen, Kun Jiang, Zhong Cao, Diange Yang)</author>
      <guid isPermaLink="false">2502.21134v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CuPID: Leveraging Masked Single-Lead ECG Modelling for Enhancing the Representations</title>
      <link>http://arxiv.org/abs/2502.21127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章介绍了一种新型的Masked Data Modelling (MDM) 方法CuPID，该方法专门针对单导联ECG数据设计，通过提供频谱图上下文信息来增强现有MDM技术。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备如心电图(ECG)心率监测器将在数字健康领域发挥重要作用。这种持续监控导致了大量的未标注数据，促进了无监督学习框架的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于单导联ECG的无监督学习方法，克服现有MDM技术在处理不规则心跳间隔时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新方法。该方法通过向解码器提供频谱图上下文信息来改善现有的MDM技术。&lt;h4&gt;主要发现&lt;/h4&gt;CuPID 方法在编码器性能上有了显著的提升，特别是在各种不同的配置下。此外，在多个下游任务中，CuPID 表现超过了现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CuPID是一种有效的改进MDM技术的方法，特别适用于处理单导联ECG数据，并且在广泛的配置和应用中优于现有的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备，如心电图(ECG) 心率监测器，在数字健康领域将扮演重要角色。持续的监控导致了大量的未标记数据，促使开发无监督学习框架的需求。尽管Masked Data Modelling (MDM) 技术已广泛使用，但直接应用于单导联ECG 数据的效果不佳，因为解码器在没有上下文信息的情况下难以处理不规则的心跳间隔。本文提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新型MDM方法，专门针对单导联ECG 设计。通过向解码器提供由频谱图派生的上下文，CuPID 增强了现有的MDM 技术，从而激励编码器生成更详细的表示。这极大地影响了编码器在各种不同配置下的性能表现，使得CuPID 在多种下游任务中超越了现有最先进技术的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Electrocardiogram (ECG) heart-ratemonitors, will play a crucial role in the future of digital health. Thiscontinuous monitoring leads to massive unlabeled data, incentivizing thedevelopment of unsupervised learning frameworks. While Masked Data Modelling(MDM) techniques have enjoyed wide use, their direct application to single-leadECG data is suboptimal due to the decoder's difficulty handling irregularheartbeat intervals when no contextual information is provided. In this paper,we present Cueing the Predictor Increments the Detailing (CuPID), a novel MDMmethod tailored to single-lead ECGs. CuPID enhances existing MDM techniques bycueing spectrogram-derived context to the decoder, thus incentivizing theencoder to produce more detailed representations. This has a significant impacton the encoder's performance across a wide range of different configurations,leading CuPID to outperform state-of-the-art methods in a variety of downstreamtasks.</description>
      <author>example@mail.com (Adtian Atienza, Gouthamaan Manimaran, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21127v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot, No Problem: Descriptive Continual Relation Extraction</title>
      <link>http://arxiv.org/abs/2502.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于检索的解决方案来解决少样本持续关系抽取问题，该方案通过大语言模型生成关系描述，并利用双编码器检索训练范式增强样本和类别表示学习。&lt;h4&gt;背景&lt;/h4&gt;传统的内存基础方法在面对有限样本时容易过拟合，无法巩固旧知识，在少样本场景中数据稀疏进一步阻碍了有效的隐空间数据增强。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决少样本持续关系抽取中的挑战，并保持模型在一系列任务上的鲁棒性能，同时减少灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;首先使用大语言模型为每个关系生成描述；然后引入双编码器检索训练范式以丰富样本和类别表示学习；最后设计基于检索的预测方法，其中每个样本通过整合关系描述向量和类原型的反向排名融合得分来“检索”最佳匹配的关系。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的广泛实验表明该方法显著地提高了性能，并且在整个顺序任务中保持了鲁棒性，有效地解决了灾难性遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为解决少样本持续关系抽取的挑战提供了一种有效的解决方案，通过利用增强表示来促进模型学习和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot Continual Relation Extraction is a crucial challenge for enabling AIsystems to identify and adapt to evolving relationships in dynamic real-worlddomains. Traditional memory-based approaches often overfit to limited samples,failing to reinforce old knowledge, with the scarcity of data in few-shotscenarios further exacerbating these issues by hindering effective dataaugmentation in the latent space. In this paper, we propose a novelretrieval-based solution, starting with a large language model to generatedescriptions for each relation. From these descriptions, we introduce abi-encoder retrieval training paradigm to enrich both sample and classrepresentation learning. Leveraging these enhanced representations, we design aretrieval-based prediction method where each sample "retrieves" the bestfitting relation via a reciprocal rank fusion score that integrates bothrelation description vectors and class prototypes. Extensive experiments onmultiple datasets demonstrate that our method significantly advances thestate-of-the-art by maintaining robust performance across sequential tasks,effectively addressing catastrophic forgetting.</description>
      <author>example@mail.com (Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen)</author>
      <guid isPermaLink="false">2502.20596v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Best Foot Forward: Robust Foot Reconstruction in-the-wild</title>
      <link>http://arxiv.org/abs/2502.20511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的3D脚部重建对于个性化矫形器、数字医疗和虚拟试穿至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理不完整的扫描数据以及解剖变异时遇到困难，特别是在用户移动受限的情况下（例如自我扫描场景）难以捕捉到像足弓和后跟这样的区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端管道来改进结构从运动（SfM）重建过程。&lt;h4&gt;方法&lt;/h4&gt;该方法首先使用SE(3)正则化结合视角预测模块解决扫描对齐的不确定性，然后通过基于注意力机制的网络训练在合成增强点云上的几何补充缺失部分。&lt;h4&gt;主要发现&lt;/h4&gt;该技术实现了同类最佳性能，同时保持了临床验证的解剖学精确度。通过结合合成数据与学习到的几何先验知识，使足部重建能够适应真实世界捕捉条件下的各种情况。&lt;h4&gt;结论&lt;/h4&gt;此方法为基于移动设备的3D扫描在医疗和零售领域的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;准确的三维脚部重建对于个性化矫形器、数字健康护理以及虚拟试穿至关重要。然而，现有的技术难以应对不完整扫描及解剖变异的问题，在自我扫描等情况下尤其困难，因为用户移动受限影响对足弓和后跟等区域的捕捉。我们提出了一种新颖的端到端流程来改进结构从运动重建过程，首先通过SE(3)正则化结合视角预测模块解决扫描对齐问题，再利用基于注意力机制训练在合成增强点云上的网络补充缺失几何部分。此方法实现了同类最佳性能，并保持了临床验证的解剖精确度。借助合成数据和学习到的几何先验知识，在真实世界捕捉条件下实现稳健的脚部重建。这为医疗与零售领域中基于移动设备的3D扫描应用开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D foot reconstruction is crucial for personalized orthotics,digital healthcare, and virtual fittings. However, existing methods strugglewith incomplete scans and anatomical variations, particularly in self-scanningscenarios where user mobility is limited, making it difficult to capture areaslike the arch and heel. We present a novel end-to-end pipeline that refinesStructure-from-Motion (SfM) reconstruction. It first resolves scan alignmentambiguities using SE(3) canonicalization with a viewpoint prediction module,then completes missing geometry through an attention-based network trained onsynthetically augmented point clouds. Our approach achieves state-of-the-artperformance on reconstruction metrics while preserving clinically validatedanatomical fidelity. By combining synthetic training data with learnedgeometric priors, we enable robust foot reconstruction under real-world captureconditions, unlocking new opportunities for mobile-based 3D scanning inhealthcare and retail.</description>
      <author>example@mail.com (Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, Ujwal Bonde)</author>
      <guid isPermaLink="false">2502.20511v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Information Bottleneck-Guided Heterogeneous Graph Learning for Interpretable Neurodevelopmental Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2502.20769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架I2B-HGNN，用于从神经发育障碍中提取有意义的生物标志物，并进行诊断。&lt;h4&gt;背景&lt;/h4&gt;现有的机器学习模型在提供综合可解释性方面存在挑战，尤其是在处理复杂的数据编码、解码和融合时。这些模型往往难以从成像数据（如fMRI）中抽取有用的生物标记物，也缺乏解释非成像数据重要性的机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以诊断神经发育障碍的可解释机器学习框架，该框架能够有效地利用成像与非成像多模态数据并提供清晰的结果解释。&lt;h4&gt;方法&lt;/h4&gt;提出了Interpretable Information Bottleneck Heterogeneous Graph Neural Network (I2B-HGNN)，包括两个关键模块：Information Bottleneck Graph Transformer (IBGraphFormer) 和 Information Bottleneck Heterogeneous Graph Attention Network (IB-HGAN)。IBGraphFormer用于局部模式，通过脑连接图约束的图神经网络进行全局建模并利用信息瓶颈指导的聚类提取生物标记物；IB-HGAN则用于全球多模态互动，使用异构图神经网络实现可解释的多模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，I2B-HGNN在诊断神经发育障碍方面表现出高精度，并能提供清晰的生物标志物识别和有效的非成像数据分析。&lt;h4&gt;结论&lt;/h4&gt;I2B-HGNN框架是解决当前机器学习模型面临的挑战的有效解决方案，它不仅能够提高诊断准确度，还能通过详细的解释帮助理解疾病特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing interpretable models for diagnosing neurodevelopmental disorders(NDDs) is highly valuable yet challenging, primarily due to the complexity ofencoding, decoding and integrating imaging and non-imaging data. Many existingmachine learning models struggle to provide comprehensive interpretability,often failing to extract meaningful biomarkers from imaging data, such asfunctional magnetic resonance imaging (fMRI), or lacking mechanisms to explainthe significance of non-imaging data. In this paper, we propose theInterpretable Information Bottleneck Heterogeneous Graph Neural Network(I2B-HGNN), a novel framework designed to learn from fine-grained localpatterns to comprehensive global multi-modal interactions. This frameworkcomprises two key modules. The first module, the Information Bottleneck GraphTransformer (IBGraphFormer) for local patterns, integrates global modeling withbrain connectomic-constrained graph neural networks to identify biomarkersthrough information bottleneck-guided pooling. The second module, theInformation Bottleneck Heterogeneous Graph Attention Network (IB-HGAN) forglobal multi-modal interactions, facilitates interpretable multi-modal fusionof imaging and non-imaging data using heterogeneous graph neural networks. Theresults of the experiments demonstrate that I2B-HGNN excels in diagnosing NDDswith high accuracy, providing interpretable biomarker identification andeffective analysis of non-imaging data.</description>
      <author>example@mail.com (Yueyang Li, Lei Chen, Wenhao Dong, Shengyu Gong, Zijian Kang, Boyang Wei, Weiming Zeng, Hongjie Yan, Lingbin Bian, Wai Ting Siok, Nizhuan Wang)</author>
      <guid isPermaLink="false">2502.20769v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2502.20806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用多模态学习的即时软件缺陷预测（JIT-SDP）方法，以提高自动驾驶软件系统的可靠性和安全性。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着自主驾驶技术的发展，可靠的软件对于确保安全和性能变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;通过利用预训练变换器和组合模块处理多种数据模式来实现即时的软件缺陷预测。&lt;h4&gt;方法&lt;/h4&gt;该模型采用多模态变换器，其中包含针对文本、数值和分类等不同数据模式之间的注意机制。在组合模块中，将基于文本数据和包含分类及数值数据的表格特征的变压器模型输出进行结合以生成预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相对于现有的深度学习和机器学习模型，在三个开源自动驾驶系统软件项目上该方法显著提高了评估指标的表现。&lt;h4&gt;结论&lt;/h4&gt;通过改善缺陷预测能力，多模态学习在提高自动驾驶软件系统的可靠性和安全性方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用预训练的变换器及组合模块处理包括代码特征、更改度量和上下文信息等多种数据模式的一种即时软件缺陷预测方法。该模型采用注意机制将不同形式的数据（如文本，数值，分类等）结合在一起，并在GitHub上收集三个开源自动驾驶系统项目的实验中证明其优于现有深度学习与机器学习模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the rise of autonomous driving technologies has highlightedthe critical importance of reliable software for ensuring safety andperformance. This paper proposes a novel approach for just-in-time softwaredefect prediction (JIT-SDP) in autonomous driving software systems usingmultimodal learning. The proposed model leverages the multimodal transformersin which the pre-trained transformers and a combining module deal with themultiple data modalities of the software system datasets such as code features,change metrics, and contextual information. The key point for adaptingmultimodal learning is to utilize the attention mechanism between the differentdata modalities such as text, numerical, and categorical. In the combiningmodule, the output of a transformer model on text data and tabular featurescontaining categorical and numerical data are combined to produce thepredictions using the fully connected layers. Experiments conducted on threeopen-source autonomous driving system software projects collected from theGitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposedapproach significantly outperforms state-of-the-art deep learning and machinelearning models regarding evaluation metrics. Our findings highlight thepotential of multimodal learning to enhance the reliability and safety ofautonomous driving software through improved defect prediction.</description>
      <author>example@mail.com (Faisal Mohammad, Duksan Ryu)</author>
      <guid isPermaLink="false">2502.20806v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>TimesBERT: A BERT-Style Foundation Model for Time Series Understanding</title>
      <link>http://arxiv.org/abs/2502.21245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列理解包括分类、填充和异常检测等任务，现有的BERT风格架构在这些领域尚未完全解锁。&lt;h4&gt;背景&lt;/h4&gt;时间序列分析在多种场景中非常重要。虽然GPT风格模型已经作为时间序列预测的基础模型被广泛应用，但基于自然语言理解取得重大进展的BERT风格架构还未充分利用于时间序列理解。&lt;h4&gt;目的&lt;/h4&gt;设计一种名为TimesBERT的新方法，旨在学习时间序列中的通用表示形式，并解决多粒度结构的问题。&lt;h4&gt;方法&lt;/h4&gt;受到多元时间序列和多句子文档共享的多粒度结构启发，提出了TimeBERT模型。除了自然地采用掩码建模外，还提出了一种并行的任务——功能令牌预测任务来体现重要的多粒度结构。&lt;h4&gt;主要发现&lt;/h4&gt;TimesBERT在涵盖四个典型下游理解任务的数据集上取得了最先进的性能，并超越了特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;h4&gt;结论&lt;/h4&gt;TimesBERT通过利用多粒度表示形式，在多个领域的时间序列分析中表现出了卓越的能力，展示了其作为通用基础模型在时间序列理解中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;时间序列分析至关重要。除了预测任务之外，许多实际应用包括分类、填充和异常检测，这些都归结为不同的能力术语即本文所说的时间序列理解。虽然GPT风格的模型被定位为基础模型用于时间序列预测，但基于自然语言理解取得重大进展的BERT架构在时间序列理解方面尚未完全解锁。受到多元时间序列与多句子文档共享的多层次结构启发，我们设计了TimesBERT来学习包括时间模式和变量特性在内的通用时间序列表示形式。除了自然适应掩码建模之外，还提出了一种功能令牌预测任务以体现重要的多层次结构。我们的模型在涵盖各种领域的260亿个时间点上进行了预训练，并利用多层次表示，在四个典型下游理解任务中取得了最先进的性能，优于特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series analysis is crucial in diverse scenarios. Beyond forecasting,considerable real-world tasks are categorized into classification, imputation,and anomaly detection, underscoring different capabilities termed time seriesunderstanding in this paper. While GPT-style models have been positioned asfoundation models for time series forecasting, the BERT-style architecture,which has made significant advances in natural language understanding, has notbeen fully unlocked for time series understanding, possibly attributed to theundesirable dropout of essential elements of BERT. In this paper, inspired bythe shared multi-granularity structure between multivariate time series andmultisentence documents, we design TimesBERT to learn generic representationsof time series including temporal patterns and variate-centric characteristics.In addition to a natural adaptation of masked modeling, we propose a paralleltask of functional token prediction to embody vital multi-granularitystructures. Our model is pre-trained on 260 billion time points across diversedomains. Leveraging multi-granularity representations, TimesBERT achievesstate-of-the-art performance across four typical downstream understandingtasks, outperforming task-specific models and language pre-trained backbones,positioning it as a versatile foundation model for time series understanding.</description>
      <author>example@mail.com (Haoran Zhang, Yong Liu, Yunzhong Qiu, Haixuan Liu, Zhongyi Pei, Jianmin Wang, Mingsheng Long)</author>
      <guid isPermaLink="false">2502.21245v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer</title>
      <link>http://arxiv.org/abs/2502.20719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架HiSGT，该框架利用层次和语义信息生成高质量的合成电子健康记录（EHRs），提高了合成数据与真实患者记录在统计上的对齐度，并支持下游临床应用。&lt;h4&gt;背景&lt;/h4&gt;现有的生成方法通常将EHRs视为离散医学代码的序列，忽视了临床编码系统的层级组织及其描述所提供的丰富语义信息，导致合成的数据缺乏临床真实性且在实际应用中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架来克服现有生成模型的问题，提高合成EHR的质量和实用性。&lt;h4&gt;方法&lt;/h4&gt;HiSGT通过构建层次图来捕捉医学代码之间的关系，并使用图神经网络导出具有层级意识的嵌入。这些嵌入与从预训练临床语言模型中提取的语义信息相结合，增强了基于Transformer的生成器的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-III和MIMIC-IV数据集上进行的广泛实验表明，HiSGT显著提高了合成EHRs与真实患者记录之间的统计对齐度，并支持慢性病分类等稳健的下游应用。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统基于原始代码生成模型的限制，HiSGT为临床高保真度合成数据生成提供了重要的步骤和通用框架，促进了可解释医学编码表示以及数据分析和隐私保护方面的有价值的应用。&lt;h4&gt;翻译&lt;/h4&gt;生成逼真的合成电子健康记录（EHRs）对加速医疗研究、促进AI模型开发及增强患者隐私具有巨大潜力。然而，现有的生成方法通常将EHR视为离散医疗代码的序列化结构，这种处理方式忽略了临床编码系统内在的层级组织及其描述提供的丰富语义信息。因此，合成的数据在下游临床任务中的应用价值有限。在这篇论文中，我们提出了HiSGT框架，该框架利用层次和语义信息进行生成过程。通过这种方法，HiSGT不仅提高了数据统计上的对齐度，还支持了稳健的下游应用程序（例如慢性病分类）。这项研究代表了一个重要的步骤，即从传统基于原始代码的生成模型转向临床高保真度合成数据生成，并且提供了一种适合解释性医疗编码表示的一般框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic synthetic electronic health records (EHRs) holdstremendous promise for accelerating healthcare research, facilitating AI modeldevelopment and enhancing patient privacy. However, existing generative methodstypically treat EHRs as flat sequences of discrete medical codes. This approachoverlooks two critical aspects: the inherent hierarchical organization ofclinical coding systems and the rich semantic context provided by codedescriptions. Consequently, synthetic patient sequences often lack highclinical fidelity and have limited utility in downstream clinical tasks. Inthis paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),a novel framework that leverages both hierarchical and semantic information forthe generative process. HiSGT constructs a hierarchical graph to encodeparent-child and sibling relationships among clinical codes and employs a graphneural network to derive hierarchy-aware embeddings. These are then fused withsemantic embeddings extracted from a pre-trained clinical language model (e.g.,ClinicalBERT), enabling the Transformer-based generator to more accuratelymodel the nuanced clinical patterns inherent in real EHRs. Extensiveexperiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGTsignificantly improves the statistical alignment of synthetic data with realpatient records, as well as supports robust downstream applications such aschronic disease classification. By addressing the limitations of conventionalraw code-based generative models, HiSGT represents a significant step towardclinically high-fidelity synthetic data generation and a general paradigmsuitable for interpretable medical code representation, offering valuableapplications in data augmentation and privacy-preserving healthcare analytics.</description>
      <author>example@mail.com (Guanglin Zhou, Sebastiano Barbieri)</author>
      <guid isPermaLink="false">2502.20719v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了GloFND，一种用于自监督对比学习的方法，能够自动识别和排除虚假负样本。&lt;h4&gt;背景&lt;/h4&gt;在自监督对比学习中，通常通过锚图像与整个数据集中的其他样本来构建负对。这种方法可能导致具有相似语义的负对（即虚假负样本）的生成。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以解决虚假负样本问题，并提高模型训练的效果和效率。&lt;h4&gt;方法&lt;/h4&gt;GloFND是一种基于优化的方法，它在训练过程中为每个锚数据动态学习阈值来识别其虚假负样本。这种方法可以在整个数据集上全局检测虚假负样本，而不是局限于小批量内局部检测。此外，该方法的每轮迭代计算成本与数据集大小无关。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在图像和图像-文本数据上的GloFND方法是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效解决自监督对比学习中的虚假负样本问题，并且具有较低的计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;在自我监督对比性学习中，负对通常是通过锚定图片与整个数据集（除去该锚点）中选取的一个样本来构建。然而，这种策略可能导致生成语义相似的负面配对（称为“虚假否定”），从而导致其嵌入物被错误地推开。为解决此问题，我们提出了一种基于优化的方法GloFND，它在训练过程中自动学习每个锚定数据的阈值以识别其虚假否定。与先前用于发现虚假否定的方法相比，我们的方法在整个数据集中全局检测虚假否定，而不是局限于小批量内局部检测。此外，其每轮迭代计算成本保持独立于数据集大小。实验结果表明，在图像和图像-文本数据上提出的该方法是有效的。我们的实现可在https://github.com/vibalcam/GloFND获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In self-supervised contrastive learning, negative pairs are typicallyconstructed using an anchor image and a sample drawn from the entire dataset,excluding the anchor. However, this approach can result in the creation ofnegative pairs with similar semantics, referred to as "false negatives",leading to their embeddings being falsely pushed apart. To address this issue,we introduce GloFND, an optimization-based approach that automatically learnson the fly the threshold for each anchor data to identify its false negativesduring training. In contrast to previous methods for false negative discovery,our approach globally detects false negatives across the entire dataset ratherthan locally within the mini-batch. Moreover, its per-iteration computationcost remains independent of the dataset size. Experimental results on image andimage-text data demonstrate the effectiveness of the proposed method. Ourimplementation is available at https://github.com/vibalcam/GloFND .</description>
      <author>example@mail.com (Vicente Balmaseda, Bokun Wang, Ching-Long Lin, Tianbao Yang)</author>
      <guid isPermaLink="false">2502.20612v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习系统在关键领域的应用越来越多，确保这些系统的可信度变得至关重要。然而，在实际操作中，诸如公平性、隐私性等重要目标往往被孤立处理，导致解决方案不理想。&lt;h4&gt;目的&lt;/h4&gt;论文旨在通过引入因果方法来解决信任机器学习和基础模型之间的多重竞争目标的平衡问题，并探讨如何将因果推理有效集成到这些系统中以提高其可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了现有文献中关于利用因果关系成功解决如公平性和准确性或隐私性和健壮性的冲突案例，以此证明因果框架在机器学习中的重要性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;论文强调采用因果分析可以更好地理解和解决不同目标之间的权衡问题，并提出了一些实际方法来实现这一目标。&lt;h4&gt;结论&lt;/h4&gt;尽管存在挑战和局限性，但通过使用因果框架，可以使AI系统更加负责任且伦理上更为可靠。因此，未来的研究应继续探索如何最佳地利用这些工具和技术。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。通过回顾文献中的成功案例，文章强调了因果推理在机器学习中的重要角色，并探讨如何将其有效整合进模型当中，从而提升系统的可靠性和透明度。此外，还讨论了采用这一方法所面临的挑战和机遇，指出了未来研究的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forthe integration of causal methods into machine learning to navigate thetrade-offs among key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Muhammad Havaei, Bernhardt Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma Triage</title>
      <link>http://arxiv.org/abs/2502.21106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于检测多种神经创伤的3D基础模型，该模型通过利用大规模语言模型进行自动标注，并采用多模态微调技术将神经网络预先训练结果整合进一个综合性的神经创伤检测网络中。&lt;h4&gt;背景&lt;/h4&gt;AI和医学成像的进步为急诊头部CT图像解读提供了变革性潜力，在请求量增加及放射科医生短缺的情况下，这些进步有助于缩短评估时间并提高准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地识别各种神经创伤的3D基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用大规模语言模型自动生成全面的多标签注释，并通过预训练和多模态微调整合出血亚型分割和大脑解剖图谱到一个综合性的神经创伤检测网络中。&lt;h4&gt;主要发现&lt;/h4&gt;在与专家标注对比及与其他方法（如CT-CLIP）比较时，该模型显示出对包括出血、脑中线移位以及脑水肿等在内的多种神经创伤情况的优异分类准确率。通过加入特定于神经系统的特征，使得诊断能力得到了显著增强。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了医学影像领域基础模型的发展，并为未来AI辅助急诊放射学中的神经创伤诊断提供了基准。&lt;h4&gt;翻译&lt;/h4&gt;近期在人工智能和医疗成像领域的进展为紧急情况下的头部CT图像解读带来了变革性的潜力。这主要是为了减少评估时间并提高准确性，面对日益增长的扫描需求以及全球范围内放射科医生短缺的问题。该研究引入了一个3D基础模型用于检测各种神经创伤发现，并且具有高准确性和效率。通过使用大规模语言模型进行自动标注，生成了全面的多标签注释以识别严重情况。我们的方法包括对出血亚型分割和大脑解剖图谱预先训练神经网络，并将其整合进一个综合性的预训练神经创伤检测网络中，通过多模态微调实现集成。与专家标记对比以及与其他模型（如CT-CLIP）比较的结果表明，在主要的神经创伤发现上具有强大的分类准确性，例如出血和脑中线移位，以及其他不常见但危急的情况，例如脑水肿和动脉高密度。特定于神经系统的特征的整合显著提升了诊断能力，平均AUC为0.861（针对16种神经创伤情况）。这项工作推进了医学成像中的基础模型，并成为未来AI辅助急诊放射学中神经创伤诊断的一个基准点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in AI and medical imaging offer transformative potentialin emergency head CT interpretation for reducing assessment times and improvingaccuracy in the face of an increasing request of such scans and a globalshortage in radiologists. This study introduces a 3D foundation model fordetecting diverse neuro-trauma findings with high accuracy and efficiency.Using large language models (LLMs) for automatic labeling, we generatedcomprehensive multi-label annotations for critical conditions. Our approachinvolved pretraining neural networks for hemorrhage subtype segmentation andbrain anatomy parcellation, which were integrated into a pretrainedcomprehensive neuro-trauma detection network through multimodal fine-tuning.Performance evaluation against expert annotations and comparison with CT-CLIPdemonstrated strong triage accuracy across major neuro-trauma findings, such ashemorrhage and midline shift, as well as less frequent critical conditions suchas cerebral edema and arterial hyperdensity. The integration of neuro-specificfeatures significantly enhanced diagnostic capabilities, achieving an averageAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundationmodels in medical imaging, serving as a benchmark for future AI-assistedneuro-trauma diagnostics in emergency radiology.</description>
      <author>example@mail.com (Youngjin Yoo, Bogdan Georgescu, Yanbo Zhang, Sasa Grbic, Han Liu, Gabriela D. Aldea, Thomas J. Re, Jyotipriya Das, Poikavila Ullaskrishnan, Eva Eibenberger, Andrei Chekkoury, Uttam K. Bodanapally, Savvas Nicolaou, Pina C. Sanelli, Thomas J. Schroeppel, Yvonne W. Lui, Eli Gibson)</author>
      <guid isPermaLink="false">2502.21106v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Are foundation models useful feature extractors for electroencephalography analysis?</title>
      <link>http://arxiv.org/abs/2502.21086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了基础模型在医学时间序列分析中的应用，特别是对于脑电图（EEG）数据的处理。通过一系列任务实验，包括年龄预测、癫痫检测等，研究表明这些基础模型能够提取有意义的时间序列特征，并且超越专业设计的EEG模型而无需领域适应。&lt;h4&gt;背景&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型取得了巨大成功，但在医疗时间序列分析（特别是脑电图数据）中应用的基础模型研究较少。随着这类任务的数据集越来越有限，探索这些基础模型在医学时间序列中的适用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在医学时间序列数据分析中的有效性，特别关注EEG信号的处理能力，并对比这些模型与专门设计的EEG模型的表现。&lt;h4&gt;方法&lt;/h4&gt;实验中采用了一系列任务来测试这些基础模型的功能，包括年龄预测、癫痫检测和临床相关的脑电图事件分类等。并通过对比分析验证了这些模型在诊断准确性方面的优势。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基础模型能够提取有意义的EEG特征；2. 即使没有领域适应，基础模型也超过了专门设计的EEG模型的表现；3. 研究表明架构选择（如上下文长度）对诊断准确度有重大影响。&lt;h4&gt;结论&lt;/h4&gt;研究表明基础模型在医学时间序列分析中具有巨大潜力。通过提供通用的时间序列理解能力，这些模型减少了对大规模特定领域数据集的需求，并成为临床实践中的宝贵工具。&lt;h4&gt;翻译&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型的成功激发了其在一般时间序列分析中的类似应用尝试。尽管这些模型对于多种任务非常有效，但在医疗领域（尤其是具有有限数据的场景）的应用仍然未被充分探索。为了应对这一问题，我们研究了基础模型在涉及脑电图（EEG）的医学时间序列分析中效果，并通过一系列实验如年龄预测、癫痫检测以及临床相关的EEG事件分类，将它们的表现与专用的EEG模型进行了对比。我们的研究表明，基础模型能够提取有意义的时间序列特征，在没有领域适应的情况下也能超越专用模型，并且能够定位任务特异性的生物标记物。此外，我们还展示了诊断准确性很大程度上受到架构选择（例如上下文长度）的影响。总的来说，这项研究揭示了具备通用时间序列理解能力的基础模型消除了对大规模特定领域数据集的依赖性，使其成为临床实践中有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of foundation models in natural language processing and computervision has motivated similar approaches for general time series analysis. Whilethese models are effective for a variety of tasks, their applicability inmedical domains with limited data remains largely unexplored. To address this,we investigate the effectiveness of foundation models in medical time seriesanalysis involving electroencephalography (EEG). Through extensive experimentson tasks such as age prediction, seizure detection, and the classification ofclinically relevant EEG events, we compare their diagnostic accuracy with thatof specialised EEG models. Our analysis shows that foundation models extractmeaningful EEG features, outperform specialised models even without domainadaptation, and localise task-specific biomarkers. Moreover, we demonstratethat diagnostic accuracy is substantially influenced by architectural choicessuch as context length. Overall, our study reveals that foundation models withgeneral time series understanding eliminate the dependency on largedomain-specific datasets, making them valuable tools for clinical practice.</description>
      <author>example@mail.com (Özgün Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert)</author>
      <guid isPermaLink="false">2502.21086v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.16786v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SwimVG的分步多模态融合和适应框架，旨在解决视觉接地任务中现有的方法在跨模态对齐不足以及计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;当前大多数用于视觉定位的方法依赖于从预训练模型中单独传输视觉或语言知识，并通过堆叠视觉-语言变压器来实现多模态融合。然而这些方法限制了视觉和语言上下文之间的充分互动并带来较高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了一种新的分步多模态融合和适应框架SwimVG。&lt;h4&gt;方法&lt;/h4&gt;该框架提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务。Swip通过逐令牌方式提高视觉和语言表示之间的对齐，而CIA在权重级别上促进跨模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在四个广泛使用的基准测试中，SwimVG表现出优异的能力并显著提高了效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法不仅能够有效解决现有方法的不足，而且具有参数高效的特点，并且通过逐步融合浅层到深层的跨模态特征，展示了其在视觉接地任务中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位旨在通过自然语言确定图像区域，这严重依赖于跨模态对齐。现有的大多数方法通过完全微调单模预训练模型来传输视觉/语言知识，并使用简单的视觉-语言变压器堆叠进行多模态融合。然而，这些方法不仅限制了视觉和语言上下文之间的充分互动，还带来了显著的计算成本。因此，为了应对这些问题，我们探索了一种分步多模态融合和适应框架，即SwimVG。具体来说，SwimVG提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务，替代冗余的变压器堆叠进行多模态融合。Swip能够以逐令牌的方式分步提高视觉和语言表示之间的对齐。此外，权重级别的CIA通过跨模态互动进一步促进多模态融合。Swip和CIA都是参数高效的模式，并逐步将浅层到深层的跨模态特征融合在一起。实验结果在四个常用的基准测试上表明，SwimVG在效率方面具有显著的优势。我们的代码可以在https://github.com/liuting20/SwimVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/liuting20/swimvg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to ground an image region through natural language,which heavily relies on cross-modal alignment. Most existing methods transfervisual/linguistic knowledge separately by fully fine-tuning uni-modalpre-trained models, followed by a simple stack of visual-language transformersfor multimodal fusion. However, these approaches not only limit adequateinteraction between visual and linguistic contexts, but also incur significantcomputational costs. Therefore, to address these issues, we explore a step-wisemultimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVGproposes step-wise multimodal prompts (Swip) and cross-modal interactiveadapters (CIA) for visual grounding, replacing the cumbersome transformerstacks for multimodal fusion. Swip can improve {the} alignment between thevision and language representations step by step, in a token-level fusionmanner. In addition, weight-level CIA further promotes multimodal fusion bycross-modal interaction. Swip and CIA are both parameter-efficient paradigms,and they fuse the cross-modal features from shallow to deep layers gradually.Experimental results on four widely-used benchmarks demonstrate that SwimVGachieves remarkable abilities and considerable benefits in terms of efficiency.Our code is available at https://github.com/liuting20/SwimVG.</description>
      <author>example@mail.com (Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong)</author>
      <guid isPermaLink="false">2502.16786v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Accurate 3D Grapevine Structure Extraction from High-Resolution Point Clouds</title>
      <link>http://arxiv.org/abs/2502.20417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对葡萄藤3D建模的Smart-Tree算法改进版，采用基于图的方法解决传统骨架化算法在复杂结构上的挑战。&lt;h4&gt;背景&lt;/h4&gt;精确的葡萄藤3D建模对精准农业至关重要，特别是对于信息丰富的修剪决策和自动化管理技术。然而，葡萄藤复杂的结构给传统的骨架化算法带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种适用于葡萄藤独特特性的三维建模方法，改善传统Smart-Tree算法在处理复杂结构上的效果。&lt;h4&gt;方法&lt;/h4&gt;提出了基于图的方法来区分骨架化过程中的个体枝条，并通过注释的现实世界点云数据进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;新方法相比原始的Smart-Tree算法，在F1分数上提高了15.8%，表明改进的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究推进了葡萄藤三维建模技术的发展，有可能通过更精确和自动化的农业实践提高葡萄生产的可持续性和盈利能力。&lt;h4&gt;翻译&lt;/h4&gt;准确地对葡萄藤进行3D建模对于精准种植至关重要，特别是在信息丰富的修剪决策以及自动化管理技术方面。然而，由于葡萄藤的复杂结构，传统的骨架化算法面临着重大挑战。本文提出了一种针对Smart-Tree算法改进的方法来应对葡萄藤的独特特点，并使用基于图的方式来解决骨架化的歧义问题。该方法能够区分出每个枝条的骨架结构，这对于精确分析和管理至关重要。我们通过使用注释过的现实世界中葡萄藤点云数据验证了我们的方法的有效性，在F1评分上较原始Smart-Tree算法提高了15.8%。这项研究为3D葡萄藤建模技术的发展做出了贡献，并有可能通过更加准确且自动化的种植实践提高葡萄生产的可持续性和盈利能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D modelling of grapevines is crucial for precision viticulture,particularly for informed pruning decisions and automated managementtechniques. However, the intricate structure of grapevines poses significantchallenges for traditional skeletonization algorithms. This paper presents anadaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressingthe unique characteristics of grapevine structures. We introduce a graph-basedmethod for disambiguating skeletonization. Our method delineates individualcane skeletons, which are crucial for precise analysis and management. Wevalidate our approach using annotated real-world grapevine point clouds,demonstrating improvement of 15.8% in the F1 score compared to the originalSmart-Tree algorithm. This research contributes to advancing 3D grapevinemodelling techniques, potentially enhancing both the sustainability andprofitability of grape production through more precise and automatedviticulture practices</description>
      <author>example@mail.com (Harry Dobbs, Casey Peat, Oliver Batchelor, James Atlas, Richard Green)</author>
      <guid isPermaLink="false">2502.20417v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</title>
      <link>http://arxiv.org/abs/2502.20823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的策略SiMLP，通过简单的非线性映射结合均值池化和多层感知机来适应基于切片级别的任务，超越了传统的MIL方法。&lt;h4&gt;背景&lt;/h4&gt;计算病理学中基础模型的出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。&lt;h4&gt;目的&lt;/h4&gt;展示SiMLP策略在多种下游任务中的优越性，并挑战传统的基于MIL的微调范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单非线性映射策略，称为SiMLP，该策略结合均值池化和多层感知机来将基础模型从补丁级别适应到切片级别任务。&lt;h4&gt;主要发现&lt;/h4&gt;1. SiMLP在大规模癌症分类任务中超越了流行MIL方法3.52%，显示强大的少样本分类能力；2. 在肺部肿瘤亚型分类方面，SiMLP表现出显著的鲁棒性和可转移性。3. SiMLP可以不需要复杂的基于MIL的学习过程来适应WSI分析。&lt;h4&gt;结论&lt;/h4&gt;研究结果挑战了传统的基于MIL的微调范式，并表明仅通过任务无关表示策略即可有效调整基础模型以进行WSI分析，为未来数字病理学研究提供了新的视角和方法论。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学中的基础模型出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。然而，在这项工作中我们提出了一种关键的实验发现：一种简单的非线性映射策略结合均值池化和多层感知机，称为SiMLP，可以有效将基于补丁级别的基础模型适配到切片级别任务而不需要复杂MIL基的学习方法。通过广泛的跨多种下游任务实验，我们展示了SiMLP与最新技术相比的优越性能，在大规模癌症分类任务中超越了流行MIL方法3.52%。此外，SiMLP在少样本分类中表现出强大的学习能力，并且仍然与其他预训练于数十万张切片上的切片级别基础模型竞争。最后，SiMLP在肺癌亚型分类方面表现出显著的鲁棒性和可转移性。总的来说，我们的发现挑战了传统的基于MIL的微调范式，表明仅通过任务无关表示策略就可以有效地将基础模型适配到WSI分析中。这些见解为未来数字病理学研究提供了一个独特且具有意义的新视角，并为此类研究铺平了更高效和广泛适用的方法论的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of foundation models in computational pathology has transformedhistopathological image analysis, with whole slide imaging (WSI) diagnosisbeing a core application. Traditionally, weakly supervised fine-tuning viamultiple instance learning (MIL) has been the primary method for adaptingfoundation models to WSIs. However, in this work we present a key experimentalfinding: a simple nonlinear mapping strategy combining mean pooling and amultilayer perceptron, called SiMLP, can effectively adapt patch-levelfoundation models to slide-level tasks without complex MIL-based learning.Through extensive experiments across diverse downstream tasks, we demonstratethe superior performance of SiMLP with state-of-the-art methods. For instance,on a large-scale pan-cancer classification task, SiMLP surpasses popularMIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability infew-shot classification and remaining highly competitive with slide-levelfoundation models pretrained on tens of thousands of slides. Finally, SiMLPexhibits remarkable robustness and transferability in lung cancer subtyping.Overall, our findings challenge the conventional MIL-based fine-tuningparadigm, demonstrating that a task-agnostic representation strategy alone caneffectively adapt foundation models to WSI analysis. These insights offer aunique and meaningful perspective for future research in digital pathology,paving the way for more efficient and broadly applicable methodologies.</description>
      <author>example@mail.com (Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He)</author>
      <guid isPermaLink="false">2502.20823v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SemiSAM+: Rethinking Semi-Supervised Medical Image Segmentation in the Era of Foundation Models</title>
      <link>http://arxiv.org/abs/2502.20749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于提示的基础模型驱动的半监督学习框架SemiSAM+，用于提高医疗图像分割任务中有限标注数据的学习效率。&lt;h4&gt;背景&lt;/h4&gt;深度学习在医学影像分割中的应用通常需要大量的标记数据进行训练，这在临床环境中由于注释成本高而难以实施。半监督学习（SSL）作为一种依赖较少专家标注的方法逐渐受到关注。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于提示的基础模型驱动的半监督框架SemiSAM+，以期通过有限数量的标签实现高效的医学影像分割任务。&lt;h4&gt;方法&lt;/h4&gt;该框架由一个或多个可提示的基础模型和一个特定于任务的学习型模型组成。在给定的新分割任务中，训练过程包括学习型模型与基础模型之间的协作，在此基础上学习型模型生成位置提示并接收来自基础模型的伪标签监督。&lt;h4&gt;主要发现&lt;/h4&gt;SemiSAM+框架在两个公共数据集及一家医院内部临床数据集中展示了显著性能提升，特别是在标注数量极为有限的情况下效果尤为突出。该框架还展现了强大的适应性作为即插即用策略可以轻松应用于不同类型的特定任务和通用模型中。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种新的半监督学习方法SemiSAM+，它通过结合基础模型的泛化能力和学习型模型的专业能力，在医学图像分割任务中实现了显著性能改进。这种方法为解决有限标签数据集下的高效训练提供了可能路径，并展示了良好的扩展性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based medical image segmentation typically requires largeamount of labeled data for training, making it less applicable in clinicalsettings due to high annotation cost. Semi-supervised learning (SSL) hasemerged as an appealing strategy due to its less dependence on acquiringabundant annotations from experts compared to fully supervised methods. Beyondexisting model-centric advancements of SSL by designing novel regularizationstrategies, we anticipate a paradigmatic shift due to the emergence ofpromptable segmentation foundation models with universal segmentationcapabilities using positional prompts represented by Segment Anything Model(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSLframework to efficiently learn from limited labeled data for medical imagesegmentation. SemiSAM+ consists of one or multiple promptable foundation modelsas generalist models, and a trainable task-specific segmentation model asspecialist model. For a given new segmentation task, the training is based onthe specialist-generalist collaborative learning procedure, where the trainablespecialist model delivers positional prompts to interact with the frozengeneralist models to acquire pseudo-labels, and then the generalist modeloutput provides the specialist model with informative and efficient supervisionwhich benefits the automatic segmentation and prompt generation in turn.Extensive experiments on two public datasets and one in-house clinical datasetdemonstrate that SemiSAM+ achieves significant performance improvement,especially under extremely limited annotation scenarios, and shows strongefficiency as a plug-and-play strategy that can be easily adapted to differentspecialist and generalist models.</description>
      <author>example@mail.com (Yichi Zhang, Bohao Lv, Le Xue, Wenbo Zhang, Yuchen Liu, Yu Fu, Yuan Cheng, Yuan Qi)</author>
      <guid isPermaLink="false">2502.20749v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2502.20678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR'25 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了弱监督时空视频定位任务，提出了一种新的学习框架STPro。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言基础模型虽然具备零样本推理能力，但在执行弱监督时空视频定位任务时缺乏必要的时空定位能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的不足，设计了一个能够进行时空预测的学习系统。&lt;h4&gt;方法&lt;/h4&gt;引入了Tubelet Referral Grounding (TRG)，并在此基础上提出了一个新颖的进步学习框架STPro。该框架包含两个关键模块：Sub-Action Temporal Curriculum Learning (SA-TCL) 和 Congestion-Guided Spatial Curriculum Learning (CG-SCL)。&lt;h4&gt;主要发现&lt;/h4&gt;通过在三个基准数据集上的实验，证明了所提出的STPro方法的有效性，并且在VidSTG-Declarative和HCSTVG-v1两个数据集中分别取得了比之前最好的结果高出1.0% 和 3.0%的成绩。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为弱监督时空视频定位任务提供了一个新的解决方案，提高了该领域的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们研究了使用文本查询而没有任何边界框监督的弱监督时空视频定位（WSTVG）任务。受到近期视觉-语言基础模型进展的启发，我们探索了这些模型在WSTVG中的实用性，利用它们的零样本接地能力。然而，简单地调整这些模型无法满足必要的时空定点功能。为弥补这一差距，我们提出了Tubelet Referral Grounding（TRG），该方法将文本查询与管状体连接起来以实现时空预测。尽管有潜力，但TRG在组合动作理解和密集场景方面仍然存在挑战。为了克服这些问题，我们提出了一种新的渐进式学习框架STPro，具有两个关键模块：Sub-Action Temporal Curriculum Learning（SA-TCL）和Congestion-Guided Spatial Curriculum Learning（CG-SCL）。在三个基准数据集上进行实验后，我们的方法实现了最先进的结果，在VidSTG-Declarative和HCSTVG-v1中分别提高了1.0% 和 3.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we study Weakly Supervised Spatio-Temporal Video Grounding(WSTVG), a challenging task of localizing subjects spatio-temporally in videosusing only textual queries and no bounding box supervision. Inspired by recentadvances in vision-language foundation models, we investigate their utility forWSTVG, leveraging their zero-shot grounding capabilities. However, we find thata simple adaptation lacks essential spatio-temporal grounding abilities. Tobridge this gap, we introduce Tubelet Referral Grounding (TRG), which connectstextual queries to tubelets to enable spatio-temporal predictions. Despite itspromise, TRG struggles with compositional action understanding and dense scenescenarios. To address these limitations, we propose STPro, a novel progressivelearning framework with two key modules: (1) Sub-Action Temporal CurriculumLearning (SA-TCL), which incrementally builds compositional actionunderstanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL),which adapts the model to complex scenes by spatially increasing taskdifficulty. STPro achieves state-of-the-art results on three benchmarkdatasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% onHCSTVG-v1.</description>
      <author>example@mail.com (Aaryan Garg, Akash Kumar, Yogesh S Rawat)</author>
      <guid isPermaLink="false">2502.20678v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SciceVPR: Stable Cross-Image Correlation Enhanced Model for Visual Place Recognition</title>
      <link>http://arxiv.org/abs/2502.20676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为SciceVPR的稳定跨图相关增强模型，用于视觉位置识别（VPR），旨在生成具有区分性和稳定性全局描述符。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的VPR模型依赖于强大的基础模型DINOv2提取全局特征，并且要么通过探索跨图像相关性来提高性能，要么采用耗时的两阶段重排名策略。但现有工作仅利用了DINOv2的最终输出结果，导致检索效果不稳定。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进方法以克服现有技术中描述符不稳定性问题，并充分利用DINOv2模型提供的特征表示能力，隐式编码有价值的上下文知识。&lt;h4&gt;方法&lt;/h4&gt;SciceVPR通过一个多层特征融合模块捕捉任务相关通道和空间信息；同时利用图像之间不变的相关性作为有价值的知识融入增强自编码器，从而获得对领域转换具有鲁棒性的全局特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，SciceVPR-B变体在多个不同领域条件的数据集上优于现有的单输入一步方法。而SciceVPR-L版本性能与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够生成稳定的全局描述符，提高视觉位置识别的准确性和鲁棒性，并在多个数据集上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;Visual Place Recognition（VPR）是机器人和自主系统的一个主要挑战，目标是仅基于图像的视觉特征预测其位置。最先进的模型使用强大的基础模型DINOv2作为骨干网络来提取全局描述符。这些模型要么通过探索跨图相关性提高性能，要么采用耗时的两阶段重排名策略以达到更好的效果。然而，现有的工作仅仅利用了DINOv2的最终输出，并且当前的跨图相关会导致检索结果不稳定。为了生成具有区分性和稳定性的全局描述符，本文提出了名为SciceVPR的增强模型。该模型探索了DINOv2在提供有用特征表示方面的全部潜力，隐式地编码有价值的上下文知识。具体而言，SciceVPR首先利用一个多层特征融合模块捕捉任务相关的通道和空间信息；其次考虑图像批次内的不变相关性作为有价值的知识融入提出的自增强编解码器中。这样，SciceVPR可以获取相对领域转换（例如光照、天气和视角变化）的全局特征具有鲁棒性的特性。实验结果表明，基本版本SciceVPR-B在多种不同条件的数据集上优于现有的单输入一步方法。大型变体SciceVPR-L与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。我们的代码将发布于https://github.com/shuimushan/SciceVPR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) is a major challenge for robotics andautonomous systems, with the goal of predicting the location of an image basedsolely on its visual features. State-of-the-art (SOTA) models extract globaldescriptors using the powerful foundation model DINOv2 as backbone. Thesemodels either explore the cross-image correlation or propose a time-consumingtwo-stage re-ranking strategy to achieve better performance. However, existingworks only utilize the final output of DINOv2, and the current cross-imagecorrelation causes unstable retrieval results. To produce both discriminativeand constant global descriptors, this paper proposes stable cross-imagecorrelation enhanced model for VPR called SciceVPR. This model explores thefull potential of DINOv2 in providing useful feature representations thatimplicitly encode valuable contextual knowledge. Specifically, SciceVPR firstuses a multi-layer feature fusion module to capture increasingly detailedtask-relevant channel and spatial information from the multi-layer output ofDINOv2. Secondly, SciceVPR considers the invariant correlation between imageswithin a batch as valuable knowledge to be distilled into the proposedself-enhanced encoder. In this way, SciceVPR can acquire fairly robust globalfeatures regardless of domain shifts (e.g., changes in illumination, weatherand viewpoint between pictures taken in the same place). Experimental resultsdemonstrate that the base variant, SciceVPR-B, outperforms SOTA one-stagemethods with single input on multiple datasets with varying domain conditions.The large variant, SciceVPR-L, performs on par with SOTA two-stage models,scoring over 3% higher in Recall@1 compared to existing models on thechallenging Tokyo24/7 dataset. Our code will be released athttps://github.com/shuimushan/SciceVPR.</description>
      <author>example@mail.com (Shanshan Wan, Yingmei Wei, Lai Kang, Tianrui Shen, Haixuan Wang, Yee-Hong Yang)</author>
      <guid isPermaLink="false">2502.20676v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding</title>
      <link>http://arxiv.org/abs/2502.20509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉-语言模型在医学图像分析中发挥了重要作用，通过从图像和报告中学到丰富的语义信息来提高图像理解。该研究针对胸部X光片（CXR）的报告处理流程提出了一种新的方法。&lt;h4&gt;背景&lt;/h4&gt;现有努力主要集中在图像与文本表示的一致性上以增强图像理解，但针对CXR报告中常见的时间参照进行图像对之间的语义差异对齐的问题则较少探索。&lt;h4&gt;目的&lt;/h4&gt;提出了两个组件来解决这一问题：一个用于处理CXR报告的流程和CoCa-CXR模型，该模型能够描述图像及其时间进程，并识别配对CXR图像中的局部差异。&lt;h4&gt;方法&lt;/h4&gt;(1) 提出了一种基于大规模语言模型（LLM）的CXR报告处理流水线来提取时态结构。(2) 开发了名为CoCa-CXR的对比性标题生成器，以学习描述图像及其时间变化的方法。该模型包含了一个新颖的区域交叉注意力模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CoCa-CXR在进展分析和报告生成方面优于先前方法，在MS-CXR-T进展分类上的平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。同时在MIMIC-CXR上取得了24.2%的RadGraph F1，与Med-Gemini基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;CoCa-CXR通过新的区域交叉注意力模块和创新性的处理流程，在医学图像的时间进程分析中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型由于能够从图像和报告中学到丰富的语义信息，在医学影像分析领域具有重要作用。以往的研究重点在于更好地对齐图像和文本表示以增强图像理解。然而，尽管胸部X射线（CXR）报告中通常会明确参考之前的影像，但如何将进展描述与成对影像之间的语义差异进行有效对齐仍有待进一步研究。为此，我们提出了两种解决方法：一种用于处理CXR报告的流水线和一个对比性标题生成器CoCa-CXR，用于学习描绘图像及其时间变化的方法。实验表明，该模型在进展分析及报告生成上均优于现有技术，并且在MS-CXR-T进步分类任务中平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have proven to be of great benefit for medical imageanalysis since they learn rich semantics from both images and reports. Priorefforts have focused on better alignment of image and text representations toenhance image understanding. However, though explicit reference to a priorimage is common in Chest X-Ray (CXR) reports, aligning progression descriptionswith the semantics differences in image pairs remains under-explored. In thiswork, we propose two components to address this issue. (1) A CXR reportprocessing pipeline to extract temporal structure. It processes reports with alarge language model (LLM) to separate the description and comparison contexts,and extracts fine-grained annotations from reports. (2) A contrastive captionermodel for CXR, namely CoCa-CXR, to learn how to both describe images and theirtemporal progressions. CoCa-CXR incorporates a novel regional cross-attentionmodule to identify local differences between paired CXR images. Extensiveexperiments show the superiority of CoCa-CXR on both progression analysis andreport generation compared to previous methods. Notably, on MS-CXR-Tprogression classification, CoCa-CXR obtains 65.0% average testing accuracy onfive pulmonary conditions, outperforming the previous state-of-the-art (SOTA)model BioViL-T by 4.8%. It also achieves a RadGraph F1 of 24.2% on MIMIC-CXR,which is comparable to the Med-Gemini foundation model.</description>
      <author>example@mail.com (Yixiong Chen, Shawn Xu, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Alan Yuille, Lin Yang)</author>
      <guid isPermaLink="false">2502.20509v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision-Language Navigation (VLN)在机器人导航领域扮演着重要角色，尤其是在具身人工智能中。本文提出了OpenFly平台，旨在解决户外空中VLN数据收集困难的问题，并构建了大规模的数据集和模型。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经被广泛研究，但室外空中VLN由于涉及的视野广阔、数据采集难度大而较少被探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的平台OpenFly来促进户外空中的Vision-Language Navigation（VLN）的发展，包括工具链开发和大规模数据集构建。&lt;h4&gt;方法&lt;/h4&gt;{'开发自动化工具链': '自动收集点云数据、进行场景语义分割、生成飞行轨迹以及创建指令', '构建大型数据集': '使用多种渲染引擎和技术生成包含100k轨迹的大规模数据集，涵盖多样化的高度和长度，并通过3D Gaussian Splatting技术增强数据的真实感。', '提出OpenFly-Agent模型': '基于关键帧的VLN模型，可以接收语言指令、当前观察结果以及历史上的关键帧作为输入，并直接输出飞行动作'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的分析和实验展示了OpenFly平台及其核心算法OpenFly-Agent的优越性能。&lt;h4&gt;结论&lt;/h4&gt;开源了工具链、数据集及代码，旨在促进户外空中VLN的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含详细内容，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Upon reflection, the final version of this work does not meet the  author's personal standards for thoroughness and clarity. As a result, the  authors have chosen to withdraw the paper to prevent the dissemination of  work that may not fully reflect the level of quality they strive to maintain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过合并三个来源创建了一个大型的COVID严重程度数据集，探讨了迁移学习在使用ImageNet和CXR预训练模型以及视觉变换器(ViTs)进行病情预测方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;新冠肺炎大流行导致医疗资源紧张，并引发了关于机器学习如何减轻医生负担并有助于诊断的讨论。胸部X光片（CXRs）用于诊断COVID-19，但很少有研究从CXRs预测患者的病情严重程度。&lt;h4&gt;目的&lt;/h4&gt;探讨迁移学习在基于图像的数据集中的表现以及其对新冠肺炎患者病情预测的贡献。&lt;h4&gt;方法&lt;/h4&gt;本研究使用了ImageNet和CXR预训练模型及视觉变换器(ViTs)，并在此基础上进行了严重程度回归与分类任务的研究。&lt;h4&gt;主要发现&lt;/h4&gt;预训练DenseNet161模型在三种严重程度预测问题上表现最佳，总体准确率为80%，轻度、中度和重度病例的分别准确率分别为77.3%、83.9%和70%。视觉变换器(ViT)在回归任务中的均方绝对误差为0.5676。&lt;h4&gt;结论&lt;/h4&gt;迁移学习方法，特别是预训练模型，可以有效地用于基于图像的数据集来预测新冠肺炎的病情严重程度。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stwhitfield/covid-severity&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju)</author>
      <guid isPermaLink="false">2502.16622v3</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning through Enhanced Sufficient Representation: Enriching Source Domain Knowledge with Target Data</title>
      <link>http://arxiv.org/abs/2502.20414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的迁移学习方法——增强充分表示的迁移学习（TESR），旨在解决传统迁移学习方法因模型假设过于严格和源域与目标域相似度要求高而导致的问题。&lt;h4&gt;背景&lt;/h4&gt;随着数据可用性的限制，迁移学习成为了解决这些问题的重要方法。它通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现这一目标。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的迁移学习方法TESR，旨在克服传统方法的局限性，提高模型在不同任务中的适应性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;首先估计出一个充分和不变的表现形式，然后通过来自靶数据的独立成分增强该表现形式，使其成为针对特定目标领域的充足表示且易于适应其特性。此方法不依赖于跨不同任务的相似模型结构假设。&lt;h4&gt;主要发现&lt;/h4&gt;TESR能够在有限样本环境下有效工作，并在模拟研究和实际应用中验证了它的性能。&lt;h4&gt;结论&lt;/h4&gt;论文展示了TESR作为迁移学习的一种灵活有效的策略，适用于广泛的监督学习问题。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习是一种解决数据可用性限制的重要方法，通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现。然而，传统的方法往往因为过于严格的模型假设和需要高度相似的域模型而面临挑战。本文提出了一种新的方法——增强充分表示的迁移学习（TESR）。该方法首先估计出一个充分不变的表现形式，并通过来自靶数据的独立成分进一步增强它，以适应特定目标领域的特性并确保其充足性。主要优点是不依赖于假设跨任务相似模型结构的存在；例如源域可以使用回归模型而靶域的任务可能是分类。这种灵活性使得TESR能够应用于广泛的监督学习问题中。论文通过理论属性探索和模拟研究以及实际数据应用验证了TESR的性能，证明它在有限样本设置下的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is an important approach for addressing the challengesposed by limited data availability in various applications. It accomplishesthis by transferring knowledge from well-established source domains to a lessfamiliar target domain. However, traditional transfer learning methods oftenface difficulties due to rigid model assumptions and the need for a high degreeof similarity between source and target domain models. In this paper, weintroduce a novel method for transfer learning called Transfer learning throughEnhanced Sufficient Representation (TESR). Our approach begins by estimating asufficient and invariant representation from the source domains. Thisrepresentation is then enhanced with an independent component derived from thetarget data, ensuring that it is sufficient for the target domain and adaptableto its specific characteristics. A notable advantage of TESR is that it doesnot rely on assuming similar model structures across different tasks. Forexample, the source domain models can be regression models, while the targetdomain task can be classification. This flexibility makes TESR applicable to awide range of supervised learning problems. We explore the theoreticalproperties of TESR and validate its performance through simulation studies andreal-world data applications, demonstrating its effectiveness in finite samplesettings.</description>
      <author>example@mail.com (Yeheng Ge, Xueyu Zhou, Jian Huang)</author>
      <guid isPermaLink="false">2502.20414v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CurviTrack: Curvilinear Trajectory Tracking for High-speed Chase of a USV</title>
      <link>http://arxiv.org/abs/2502.21303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于拖曳感知模型与MPC相结合的方法，用于解决海洋环境中异构机器人团队由于需要让自主飞行器着陆充电而导致的时间和能量损失问题。&lt;h4&gt;背景&lt;/h4&gt;在海事应用中使用异构机器人团队会导致时间及能源的浪费，特别是在自主飞行器需要降落以重新充电时。这不仅影响任务效率，还限制了海洋车辆执行复杂机动的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有技术中的预测误差大、预测准确性低以及跟踪性能不足的问题，提高在动态环境下的着陆成功率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新的拖曳感知模型，并将其与MPC（模型预测控制）结合使用，以实现高速曲线轨迹下的追踪和降落，无需通信即可完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;相比现有技术，该方法降低了40%的预测误差，提高了预测准确性的三倍，并且在跟踪性能上提升了30%，成功着陆率提高到了原来的四倍，尤其是在执行剧烈转弯等传统海上任务难以应对的情况下表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;通过两个不同实际场景中大小不同的海洋船只测试验证了该方法的有效性，并进一步使用模拟中的统计分析来展示其鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;异构机器人团队在海洋环境中使用时，当海洋车辆必须停止执行任务以让自主飞行器降落进行充电时会遭受时间和能源的惩罚。本文提出了一种解决方案，利用一种新的拖曳感知模型和MPC（模型预测控制）相结合的方法来跟踪并在高速曲线轨迹中实现不依赖通信的着陆。这种方法相比于最先进的技术可以降低40%的预测误差，并提供了预测准确性的三倍提高。因此，在进行剧烈转弯等常规海上任务难以处理的情况下，这导致了30%的追踪性能改进和40%更高的在移动USV上成功的着陆概率。我们在两种不同的现实场景中测试了我们的方法，使用不同大小的海洋船只，并通过模拟中的统计分析进一步证实我们方法的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3546079&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous robot teams used in marine environments incur time-and-energypenalties when the marine vehicle has to halt the mission to allow theautonomous aerial vehicle to land for recharging. In this paper, we present asolution for this problem using a novel drag-aware model formulation which iscoupled with MPC, and therefore, enables tracking and landing during high-speedcurvilinear trajectories of an USV without any communication. Compared to thestate-of-the-art, our approach yields 40% decrease in prediction errors, andprovides a 3-fold increase in certainty of predictions. Consequently, thisleads to a 30% improvement in tracking performance and 40% higher success inlanding on a moving USV even during aggressive turns that are unfeasible forconventional marine missions. We test our approach in two different real-worldscenarios with marine vessels of two different sizes and further solidify ourresults through statistical analysis in simulation to demonstrate therobustness of our method.</description>
      <author>example@mail.com (Parakh M. Gupta, Ondřej Procházka, Tiago Nascimento, Martin Saska)</author>
      <guid isPermaLink="false">2502.21303v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints</title>
      <link>http://arxiv.org/abs/2502.21280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合几何模型和学习到的立体视觉特征的方法，用于改善3D表面建模。&lt;h4&gt;背景&lt;/h4&gt;传统的立体视觉方法在处理深度不连续性和遮挡时存在挑战。仅基于数据驱动的方法难以捕捉关键的视觉信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用几何形状描述优势，并通过单目先验模型来增强遮挡和缺乏纹理区域建模的新系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用类独眼的模型提供分析性3D表面模型，这些模型包含了深度不连续性和遮挡；2. 结合学习到的立体视觉特征；3. 调用单目先验表面模型填补遮挡或缺乏纹理的区域。&lt;h4&gt;主要发现&lt;/h4&gt;该方法结果与现有的数据驱动方法相当，但在视觉质量上有显著提升，证明了三维几何模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;理解并建模三维形状属性对于计算机视觉研究至关重要，并且这种改进可以在虚拟现实和机器人技术中应用，以改善用户体验或减少错误。&lt;h4&gt;翻译&lt;/h4&gt;我们在立体视觉方面进行了创新，通过提供由独眼模型视角下的分析性3D表面模型来明确处理深度不连续性和遮挡问题。结合几何基础与学习到的立体特征使我们的系统能够从两种方法的优势中获益。此外，在数据匹配不足的情况下使用单目先验模型填补遮挡或缺乏纹理区域。我们的结果已达到现有纯数据驱动方法同等水平，但在视觉质量上更胜一筹，突显了3D几何模型捕捉关键视觉信息的重要性。这样的定性改进可能在虚拟现实中找到应用价值，以改善人类体验，并且在机器人技术中减少关键错误方面同样重要。本研究旨在证明理解并建模三维表面的几何属性对计算机视觉研究有益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We innovate in stereo vision by explicitly providing analytical 3D surfacemodels as viewed by a cyclopean eye model that incorporate depthdiscontinuities and occlusions. This geometrical foundation combined withlearned stereo features allows our system to benefit from the strengths of bothapproaches. We also invoke a prior monocular model of surfaces to fill inocclusion regions or texture-less regions where data matching is notsufficient. Our results already are on par with the state-of-the-art purelydata-driven methods and are of much better visual quality, emphasizing theimportance of the 3D geometrical model to capture critical visual information.Such qualitative improvements may find applicability in virtual reality, for abetter human experience, as well as in robotics, for reducing critical errors.Our approach aims to demonstrate that understanding and modeling geometricalproperties of 3D surfaces is beneficial to computer vision research.</description>
      <author>example@mail.com (Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2502.21280v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</title>
      <link>http://arxiv.org/abs/2502.21257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;最近，多模态大型语言模型（MLLM）在各种多模态环境中展示了显著的能力。然而，在机器人场景中的应用，特别是长时程操作任务中表现出重大限制。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态大型语言模型（MLLM）在处理复杂的机器人操控任务时存在明显的不足，特别是在规划能力、可操作性感知和轨迹预测三个方面有缺陷。&lt;h4&gt;目的&lt;/h4&gt;为了增强机器人的核心功能，从抽象到具体的操作，提出了ShareRobot数据集以及基于此的RoboBrain模型，旨在解决现有MLLM在机器人场景中的局限性。&lt;h4&gt;方法&lt;/h4&gt;ShareRobot是一个高质量的数据集，包含任务规划、可操作性识别和末端执行器轨迹等多维度信息。该数据集经过三个标注者的细心校正以确保其多样性和准确性。利用该数据集开发了RoboBrain模型，并采用多层次训练策略以及大量的视频和高分辨率图像进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过详尽的实验，证明RoboBrain在多种机器人任务中达到了最先进的性能水平。&lt;h4&gt;结论&lt;/h4&gt;这些成果强调了RoboBrain在提高机器人大脑功能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Multimodal Large Language Models (MLLMs) have shownremarkable capabilities across various multimodal contexts. However, theirapplication in robotic scenarios, particularly for long-horizon manipulationtasks, reveals significant limitations. These limitations arise from thecurrent MLLMs lacking three essential robotic brain capabilities: PlanningCapability, which involves decomposing complex manipulation instructions intomanageable sub-tasks; Affordance Perception, the ability to recognize andinterpret the affordances of interactive objects; and Trajectory Prediction,the foresight to anticipate the complete manipulation trajectory necessary forsuccessful execution. To enhance the robotic brain's core capabilities fromabstract to concrete, we introduce ShareRobot, a high-quality heterogeneousdataset that labels multi-dimensional information such as task planning, objectaffordance, and end-effector trajectory. ShareRobot's diversity and accuracyhave been meticulously refined by three human annotators. Building on thisdataset, we developed RoboBrain, an MLLM-based model that combines robotic andgeneral multi-modal data, utilizes a multi-stage training strategy, andincorporates long videos and high-resolution images to improve its roboticmanipulation capabilities. Extensive experiments demonstrate that RoboBrainachieves state-of-the-art performance across various robotic tasks,highlighting its potential to advance robotic brain capabilities.</description>
      <author>example@mail.com (Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang)</author>
      <guid isPermaLink="false">2502.21257v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  \href{https://github.com/BaitingLuo/L-MAP.git}{this https URL}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架Latent Macro Action Planner (L-MAP)，该框架通过学习一组时间延长的宏动作，解决了在高维连续行为空间中的序列决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机性的环境和高维行动空间中进行顺序决策面临计算挑战。传统离线增强学习设置下，代理必须基于通过随机行为策略收集的数据来学习如何做决定。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用离线强化学习框架解决具有复杂动作空间的序列决策问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP 采用状态条件下的向量量化变分自动编码器 (VQ-VAE) 学习一组时间扩展的动作，通过这种方式减少动作维度。同时，它使用一个独立的学习先验模型作为潜在转换模型，并允许高效的可能行动采样。在规划过程中，通过蒙特卡洛树搜索（MCTS）来考虑环境和行为策略中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP 能够有效地在线性时间范围内进行离线强化学习任务中的决策，即使在动作维度增加的情况下也能保持较低的延迟，并且在连续控制到高维机器人手部操作等多种任务上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过实验证明 L-MAP 在处理复杂和随机环境下的高维行动空间规划问题时是有效的，并能够与现有的模型方法相媲美，同时表现出了比其他基于模型的方法更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;顺序决策在具有高维连续动作空间的随机环境中面临计算挑战。我们探索了传统的离线强化学习框架，在这种情况下，代理必须根据通过随机行为策略收集的数据来学习如何做出决定。我们提出了 Latent Macro Action Planner (L-MAP)，该方法通过对状态条件下的向量量化变分自动编码器进行训练来降低动作维度，并且采用一个独立的学习先验模型作为潜在转换模型和高效的可能行动采样工具。在规划过程中，通过蒙特卡洛树搜索(MCTS) 来考虑环境及行为策略中的随机性。L-MAP 在离线强化学习设置中包括随机连续控制任务时表现高效，能够在线性时间范围内进行决策，并且即使在动作维度增加的情况下也能保持低延迟。实验证明，在从具有内在随机性的连续控制到高维机器人手部操作的任务上，与现有模型方法相比 L-MAP 显著优于其他方法，并表现出与强大的无模型策略-评估者基准线相媲美的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present \textit{Latent Macro Action Planner}(L-MAP), which addresses this challenge by learning a set of temporallyextended macro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Minor-Testing Approach for Coordinated Motion Planning with Sliding Robots</title>
      <link>http://arxiv.org/abs/2502.21175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了在无向图上的一种协调移动规划问题的变体，即协作滑动运动规划(CSMP)问题。&lt;h4&gt;背景&lt;/h4&gt;CSMP问题涉及给定一个无向图G、k个机器人R1至Rk放置于G的不同顶点，并且p≤k个不同的目标顶点供机器人R1至Rp使用。该问题是NP困难的，尤其是在全网格中。&lt;h4&gt;目的&lt;/h4&gt;研究CSMP在两个参数（即机器人数量k和时间限制l）下的参数复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个固定参数算法来解决CSMP问题，在第一个结果中根据k参数化；在第二个结果中，为特殊情况下只有一个目标顶点的CSMP提出了一个基于l参数化的固定参数算法，并证明了该特殊情况是NP完全的。&lt;h4&gt;主要发现&lt;/h4&gt;解决方案可以表示为输入图的小标记拓扑子图，这是两个结果的关键新元素。&lt;h4&gt;结论&lt;/h4&gt;通过这两个新的算法和理论发现，作者在解决大规模复杂问题方面取得了进展。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一种无向图上协调运动规划问题的变体——协作滑动运动规划(CSMP)问题。该问题要求判断是否存在一个序列调度，在这个调度中最多包含l次移动，使得每个有目标顶点的机器人能够到达它的目的地。此外，还提出了解决CSMP参数复杂性的固定参数算法，并证明了在特殊情况下是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study a variant of the Coordinated Motion Planning problem on undirectedgraphs, referred to herein as the \textsc{Coordinated Sliding-Motion Planning}(CSMP) problem. In this variant, we are given an undirected graph $G$, $k$robots $R_1,\dots,R_k$ positioned on distinct vertices of $G$, $p\leq k$distinct destination vertices for robots $R_1,\dots,R_p$, and $\ell \in\mathbb{N}$. The problem is to decide if there is a serial schedule of at most$\ell$ moves (i.e., of makespan $\ell$) such that at the end of the scheduleeach robot with a destination reaches it, where a robot's move is a free path(unoccupied by any robots) from its current position to an unoccupied vertex.The problem is known to be NP-hard even on full grids. It has been studied inseveral contexts, including coin movement and reconfiguration problems, withrespect to feasibility, complexity, and approximation. Geometric variants ofthe problem, in which congruent geometric-shape robots (e.g., unitdisk/squares) slide or translate in the Euclidean plane, have also been studiedextensively. We investigate the parameterized complexity of CSMP with respectto two parameters: the number $k$ of robots and the makespan $\ell$. As ourfirst result, we present a fixed-parameter algorithm for CSMP parameterized by$k$. For our second result, we present a fixed-parameter algorithmparameterized by $\ell$ for the special case of CSMP in which only a singlerobot has a destination and the graph is planar, which we prove to beNP-complete. A crucial new ingredient for both of our results is that thesolution admits a succinct representation as a small labeled topological minorof the input graph.</description>
      <author>example@mail.com (Eduard Eiben, Robert Ganian, Iyad Kanj, Ramanujan M. Sridharan)</author>
      <guid isPermaLink="false">2502.21175v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?</title>
      <link>http://arxiv.org/abs/2502.21110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CalNF（校准归一化流程）的新框架，用于从有限的数据中进行后验学习。这种方法解决了在处理安全关键系统的罕见故障事件时由于数据稀缺而遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶系统和机器人技术的广泛应用，与之相关的安全问题也日益凸显。此类系统的故障往往难以通过现有的方法来建模和调试，因为缺乏足够的失败案例的数据支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架CalNF，以克服由于罕见故障事件数据不足导致的传统模型训练中的局限性，如过拟合或欠拟合等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自调节的归一化流程技术（Calibrated Normalizing Flows, CalNF），专门设计用于在数据有限的情况下进行后验学习，并且能够在处理逆问题和罕见故障建模时达到最先进的性能水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用CalNF框架能够成功解析2022年美国西南航空公司调度危机的根本原因，这是一项开创性的案例研究。&lt;h4&gt;结论&lt;/h4&gt;CalNF框架为解决由于数据稀缺而引起的罕见安全关键事件的建模难题提供了一个有效的解决方案，并且已经在实际问题中得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;随着无人驾驶系统和机器人技术在运输等领域的部署增加，相应的安全性关键性故障也有所上升。这些故障难以通过现有的方法进行建模和调试，因为缺乏足够的失败案例的数据支持。为了应对这一挑战，研究人员提出了一种名为CalNF的新框架，它利用了自调节的归一化流程技术，特别适用于从有限数据中进行后验学习，并且已经在处理逆问题和罕见故障事件模型方面取得了最先进的性能表现。通过这种方法的应用，能够首次对2022年美国西南航空公司调度危机的根本原因进行了深入分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increased deployment of autonomous systems in fields like transportation androbotics have seen a corresponding increase in safety-critical failures. Thesefailures can be difficult to model and debug due to the relative lack of data:compared to tens of thousands of examples from normal operations, we may haveonly seconds of data leading up to the failure. This scarcity makes itchallenging to train generative models of rare failure events, as existingmethods risk either overfitting to noise in the limited failure dataset orunderfitting due to an overly strong prior. We address this challenge withCalNF, or calibrated normalizing flows, a self-regularized framework forposterior learning from limited data. CalNF achieves state-of-the-artperformance on data-limited failure modeling and inverse problems and enables afirst-of-a-kind case study into the root causes of the 2022 Southwest Airlinesscheduling crisis.</description>
      <author>example@mail.com (Charles Dawson, Van Tran, Max Z. Li, Chuchu Fan)</author>
      <guid isPermaLink="false">2502.21110v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Jointly Assigning Processes to Machines and Generating Plans for Autonomous Mobile Robots in a Smart Factory</title>
      <link>http://arxiv.org/abs/2502.21101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ACES是一种用于智能工厂的优化算法，可以同时解决工艺分配和路径规划问题。&lt;h4&gt;背景&lt;/h4&gt;现代智能工厂使用可编程机器进行生产，并通过移动机器人运输材料。目前现有的管理系统是顺序地解决问题，这限制了它们所能实现的最大吞吐量。&lt;h4&gt;目的&lt;/h4&gt;介绍ACES（Anytime Cyclic Embedding Solver），这是一种能够同时优化工艺分配和路径规划问题的解决方案。&lt;h4&gt;方法&lt;/h4&gt;ACES可以同时解决智能工厂中的工艺分配和移动机器人运输路线的问题，从而提高整个系统的生产效率。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估表明，ACES能够在现实工业场景中进行扩展，并且相较于现有系统，能够实现更高的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;ACES为智能工厂提供了一种有效的解决方案来优化生产和材料运输流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现代智能工厂使用一组可编程机器运行制造程序。通常，材料通过一群移动机器人在这些机器之间运送。为了将制造过程嵌入到智能工厂中，工厂操作员必须a) 将其工艺分配给智能工厂的机器，b) 确定代理如何在机器之间运输材料。一个好的嵌入可以最大化智能工厂的吞吐量；即它输出产品的速度。现有的智能工厂管理系统按顺序解决上述问题，限制了它们所能实现的最大吞吐量。在这篇论文中我们介绍了ACES（Anytime Cyclic Embedding Solver），这是一种首次同时优化工艺分配和路径规划问题的解决方案。我们评估了ACES，并表明它可以扩展到现实工业场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A modern smart factory runs a manufacturing procedure using a collection ofprogrammable machines. Typically, materials are ferried between these machinesusing a team of mobile robots. To embed a manufacturing procedure in a smartfactory, a factory operator must a) assign its processes to the smart factory'smachines and b) determine how agents should carry materials between machines. Agood embedding maximizes the smart factory's throughput; the rate at which itoutputs products. Existing smart factory management systems solve theaforementioned problems sequentially, limiting the throughput that they canachieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver,the first solver which jointly optimizes the assignment of processes tomachines and the assignment of paths to agents. We evaluate ACES and show thatit can scale to real industrial scenarios.</description>
      <author>example@mail.com (Christopher Leet, Aidan Sciortino, Sven Koenig)</author>
      <guid isPermaLink="false">2502.21101v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests</title>
      <link>http://arxiv.org/abs/2502.21100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;生成对抗性安全关键场景是测试自动驾驶系统的关键方法，有助于识别潜在弱点并增强系统的鲁棒性和可靠性。然而，现有的方法主要关注不受限制的碰撞场景，导致非玩家角色（NPC）车辆无差别攻击主控车。这些研究忽略了这些场景的真实性和合理性，产生了许多极端、人为构造且不现实的涉及激进NPC车辆的碰撞事件。&lt;h4&gt;背景&lt;/h4&gt;当前的方法在测试自动驾驶系统时过于集中于制造不受限制和过度激进的对抗性情况，导致生成的场景缺乏真实性和理性。&lt;h4&gt;目的&lt;/h4&gt;提出一种三层相对安全区域模型，并开发一个名为AuthSim的平台来产生更真实有效的安全关键场景，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入了三层相对安全区域模型和结合强化学习的方法，这个模型可以划分基于危险等级的不同区域并调整NPC车辆进入这些边界区域的概率。同时利用该模型与强化学习相结合构建了一个全面平台AuthSim。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示AuthSim在生成有效的安全关键场景方面比现有方法表现更佳，尤其在平均切入距离和平均碰撞间隔时间上分别提高了5.25%和27.12%，并且效率更高。&lt;h4&gt;结论&lt;/h4&gt;这是首次全面解决自动驾驶系统测试场景的真实性和有效性问题的尝试。AuthSim证明了其在生成真实场景方面的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，对抗性安全关键场景对测试自动驾驶系统的潜在弱点至关重要，并增强其鲁棒性和可靠性。然而，现有的方法过分关注无约束碰撞情况，导致NPC车辆针对主控车发动毫无选择的攻击。这些方法忽视了情景的真实性和合理性，产生了大量极端且不现实的情况，其中涉及的是激进的NPC行为。为解决这些问题，研究团队提出了一种三层相对安全区域模型，并开发了一个名为AuthSim的平台，该平台利用这个模型与强化学习相结合，以产生更加真实和有效的测试场景。实验表明，相对于现有方法，AuthSim在生成有效且关键的安全场景方面表现出色，尤其是在提高平均切入距离和减少碰撞间隔时间上取得了显著改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating adversarial safety-critical scenarios is a pivotal method fortesting autonomous driving systems, as it identifies potential weaknesses andenhances system robustness and reliability. However, existing approachespredominantly emphasize unrestricted collision scenarios, prompting non-playercharacter (NPC) vehicles to attack the ego vehicle indiscriminately. Theseworks overlook these scenarios' authenticity, rationality, and relevance,resulting in numerous extreme, contrived, and largely unrealistic collisionevents involving aggressive NPC vehicles. To rectify this issue, we propose athree-layer relative safety region model, which partitions the area based ondanger levels and increases the likelihood of NPC vehicles entering relativeboundary regions. This model directs NPC vehicles to engage in adversarialactions within relatively safe boundary regions, thereby augmenting thescenarios' authenticity. We introduce AuthSim, a comprehensive platform forgenerating authentic and effective safety-critical scenarios by integrating thethree-layer relative safety region model with reinforcement learning. To ourknowledge, this is the first attempt to address the authenticity andeffectiveness of autonomous driving system test scenarios comprehensively.Extensive experiments demonstrate that AuthSim outperforms existing methods ingenerating effective safety-critical scenarios. Notably, AuthSim achieves a5.25% improvement in average cut-in distance and a 27.12% enhancement inaverage collision interval time, while maintaining higher efficiency ingenerating effective safety-critical scenarios compared to existing methods.This underscores its significant advantage in producing authentic scenariosover current methodologies.</description>
      <author>example@mail.com (Yukuan Yang, Xucheng Lu, Zhili Zhang, Zepeng Wu, Guoqi Li, Lingzhong Meng, Yunzhi Xue)</author>
      <guid isPermaLink="false">2502.21100v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Vibrotactile information coding strategies for a body-worn vest to aid robot-human collaboration</title>
      <link>http://arxiv.org/abs/2502.21056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过身体穿戴式的振动触觉背心向操作员传递机器人实时信息的方法。研究旨在探索在高认知负荷条件下，如何有效地利用非视觉和非听觉感知方式来传达关键信息。&lt;h4&gt;背景&lt;/h4&gt;在城市搜索与救援（USAR）场景中，当人类与机器人协同工作时，尤其是在高度复杂的环境中，触觉通信可以为操作员提供重要而不影响其视听能力的信息。这种情况通常伴随着高认知负荷条件下的作业。&lt;h4&gt;目的&lt;/h4&gt;本文的目的是通过不同的振动触觉信息编码策略来探讨如何最好地传达此类信息，并引入了语义触觉的概念，以改善在机器人远程侦察时的情景理解。&lt;h4&gt;方法&lt;/h4&gt;文章介绍了一种新的信息表示技术——语义触觉（Semantic Haptics），该技术利用形状和模式来表示特定事件。这种方法试图使皮肤像屏幕一样工作，旨在提高学习能力和解释准确度。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用形状和图案来代表特定的事件，可以实现更好的可学性和解释准确性。这表明在复杂作业环境中采用触觉信息传递的有效性与优势。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了利用振动背心进行触觉通信在提高操作员对环境理解方面的潜力，并且语义触觉技术可能为未来机器人辅助搜索和救援任务提供有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the use of a body-worn vibrotactile vest to conveyreal-time information from robot to operator. Vibrotactile communication couldbe useful in providing information without compropmising or loading a person'svisual or auditory perception. This paper considers applications in UrbanSearch and Rescue (USAR) scenarios where a human working alongside a robot islikely to be operating in high cognitive load conditions. The focus is onunderstanding how best to convey information considering different vibrotactileinformation coding strategies to enhance scene understanding in scenarios wherea robot might be operating remotely as a scout. In exploring informationrepresentation, this paper introduces Semantic Haptics, using shapes andpatterns to represent certain events as if the skin was a screen, and shows howthese lead to bettter learnability and interpreation accuracy.</description>
      <author>example@mail.com (Adrian Vecina Tercero, Praminda Caleb-Solly)</author>
      <guid isPermaLink="false">2502.21056v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多视角房间布局估计方法Plane-DUSt3R，该方法利用了三维基础模型DUSt3R。&lt;h4&gt;背景&lt;/h4&gt;从多个视角的图像中进行房间布局估计的问题由于多视图几何复杂性而研究较少。传统的结构光运动过程需要多步骤解决方案，比如相机内参和外参估计、图像匹配以及三角测量等。&lt;h4&gt;目的&lt;/h4&gt;为了简化房间布局估计的过程并减少误差累积，本文旨在提出一种新的单步端到端方法来处理这个问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行微调，以修改后的目标函数来估算结构平面。此模型能够仅通过一次后处理步骤和二维检测结果来进行房间布局估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有最先进的方法相比，Plane-DUSt3R不仅在合成数据集中表现更优，在不同图像风格（如卡通）的真实世界数据中也显示了鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;通过引入Plane-DUSt3R，本文提供了一种高效的多视角房间布局估计解决方案，并展示了其在各种环境下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图像进行房间布局估计问题因多视图几何复杂性而研究不足。然而，在三维重建领域，近年来出现了像DUSt3R这样的三维基础模型，改变了传统的多步骤结构光运动流程为单步端到端方法。本文引入了Plane-DUSt3R，一种利用DUSt3R框架进行多视角房间布局估计的新方法。该模型在经过修改的目标函数指导下，在一个房间布局数据集上进行了微调，并且能够通过二维检测结果和单一后处理步骤实现高效精确的结构平面预测。不同于以往依赖单视图或全景图像的方法，Plane-DUSt3R扩展了其能力以适应多视角输入，并提供了一种简化的、端到端的解决方案。实验表明，这种方法在合成数据集中超越了现有最佳方法，在不同样式的实际场景（包括卡通风格）中也展示了强大的性能和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon.Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Sixth-Sense: Self-Supervised Learning of Spatial Awareness of Humans from a Planar Lidar</title>
      <link>http://arxiv.org/abs/2502.21029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种从1D激光雷达数据中检测人类并估计其2D姿态的自监督方法，以解决服务机器人在没有RGB-D摄像头或昂贵3D LiDAR的情况下难以感知周围人的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的服务机器人主要依赖于RGB-D相机或昂贵的3D LiDAR进行人体定位，但商业上常见的服务机器人通常配备视野狭窄的普通相机或者读数难以解析的一维激光雷达。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有设备的成本和功能限制，论文提出了一个利用1D LiDAR数据检测人类并估计其2D姿态的方法，并使用RGB-D摄像头的数据作为监督信号进行训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自监督学习框架，该框架通过将从1DLiDAR获得的原始距离信息与从RGB-D相机获取的人体框和关键点进行配对来实现人体检测和姿态估计任务。模型经过70分钟数据（在两个环境自主收集）训练后能够实现在新环境中基于1DLiDAR的数据进行全向人类检测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在新的未知环境中从单一线激光雷达信息中准确地进行全方位的人体检测，达到71%的精度和80%的召回率，并且在距离上保持了平均绝对误差为13cm，在方向角度上有44度的估计偏差。&lt;h4&gt;结论&lt;/h4&gt;论文提出的方法证明了一维LiDAR可以作为服务机器人实现自主定位与交互的有效感知工具，显著提高了机器人的环境适应性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localizing humans is a key prerequisite for any service robot operating inproximity to people. In these scenarios, robots rely on a multitude ofstate-of-the-art detectors usually designed to operate with RGB-D cameras orexpensive 3D LiDARs. However, most commercially available service robots areequipped with cameras with a narrow field of view, making them blind when auser is approaching from other directions, or inexpensive 1D LiDARs whosereadings are difficult to interpret. To address these limitations, we propose aself-supervised approach to detect humans and estimate their 2D pose from 1DLiDAR data, using detections from an RGB-D camera as a supervision source. Ourapproach aims to provide service robots with spatial awareness of nearbyhumans. After training on 70 minutes of data autonomously collected in twoenvironments, our model is capable of detecting humans omnidirectionally from1D LiDAR data in a novel environment, with 71% precision and 80% recall, whileretaining an average absolute error of 13 cm in distance and 44{\deg} inorientation.</description>
      <author>example@mail.com (Simone Arreghini, Nicholas Carlotti, Mirko Nava, Antonio Paolillo, Alessandro Giusti)</author>
      <guid isPermaLink="false">2502.21029v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Nano Drone-based Indoor Crime Scene Analysis</title>
      <link>http://arxiv.org/abs/2502.21019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, to be submitted to ARSO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文采用投机原型设计的方法，使用STAIR工具快速回顾文献并识别出犯罪现场分析中尚未受到足够关注的任务，并开发了一种小型无人机进行初步验证。&lt;h4&gt;背景&lt;/h4&gt;机器人技术、人工智能和计算机视觉可以应用于犯罪现场分析，以保护生命、促进正义和防止犯罪。但尚缺乏对可自动化任务的全面概述。&lt;h4&gt;目的&lt;/h4&gt;识别并实现犯罪现场分析中可自动化的任务，并通过原型设计展示其可行性和性能。&lt;h4&gt;方法&lt;/h4&gt;采用STAIR工具进行文献回顾，确定了访问犯罪现场（如通过窗户）、绘制和收集证据以及分析血迹等未受足够关注的任务。接着开发了一种小型无人机原型以执行这些任务。&lt;h4&gt;主要发现&lt;/h4&gt;该无人机在三个特定任务中分别达到了75%、85%和80%的性能，展示了技术应用于犯罪现场分析的可能性。&lt;h4&gt;结论&lt;/h4&gt;此次工作通过初步实验为未来的研究提供指导，并强调了进一步研究的必要性以改善自动化系统对复杂犯罪场景的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：机器人技术、人工智能（AI）和计算机视觉（CV）可以被用于帮助保护生命、促进正义及防止犯罪，但关于可自动化的任务概述却很缺乏。本文采用投机原型设计的方法：首先利用STAIR工具快速回顾文献并识别出访问犯罪现场通过窗户进入等尚未受到足够关注的任务；其次开发一种小型无人机以实现这些任务，并在室内犯罪场景中进行初步分析。最后报告了所学到的经验教训，为后续的研究提供指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technologies such as robotics, Artificial Intelligence (AI), and ComputerVision (CV) can be applied to crime scene analysis (CSA) to help protect lives,facilitate justice, and deter crime, but an overview of the tasks that can beautomated has been lacking. Here we follow a speculate prototyping approach:First, the STAIR tool is used to rapidly review the literature and identifytasks that seem to have not received much attention, like accessing crime sitesthrough a window, mapping/gathering evidence, and analyzing blood smears.Secondly, we present a prototype of a small drone that implements these threetasks with 75%, 85%, and 80% performance, to perform a minimal analysis of anindoor crime scene. Lessons learned are reported, toward guiding next work inthe area.</description>
      <author>example@mail.com (Martin Cooney, Sivadinesh Ponrajan, Fernando Alonso-Fernandez)</author>
      <guid isPermaLink="false">2502.21019v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Motion ReTouch: Motion Modification Using Four-Channel Bilateral Control</title>
      <link>http://arxiv.org/abs/2502.20982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, Accepted at ICM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为Motion ReTouch的新方法被提出，该方法可以通过多边控制和动作复制系统结合的方式对通过四通道双边控制获得的运动数据进行事后修改。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，在自主机器人操作中使用模仿学习是有效的。特别是利用能够获取位置和力信息的四通道双边控制来进行教学已被证明是有效的。&lt;h4&gt;目的&lt;/h4&gt;为了实现能够轻松执行高速、复杂任务的一次性控制性能，本研究提出了一种新的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法称为Motion ReTouch，它不仅能修改运动的位置数据，还能修改力的信息。这通过多边控制和动作复制系统的结合得以实现。&lt;h4&gt;主要发现&lt;/h4&gt;在使用真实机器人进行的实验中，试验表明测试管转移任务的成功率得到了提高，证明了修改力信息的可能性。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种新颖的方法来增强模仿学习的效果，并为实现更高性能的任务执行提供了可能途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经展示了模仿学习在自主机器人操作中的有用性。特别是使用四通道双边控制进行教学已被证明是有效的，它可以获取位置和力的信息。然而，尚未实现能够轻松一次性完成高速、复杂任务的控制性能。我们提出了一种叫做Motion ReTouch的方法，该方法可以通过事后修改通过四通道双边控制获得的运动数据来提高这一能力。此方法不仅可以修改位置信息，还可以修改力信息。这是通过多边控制和动作复制系统的结合得以实现的。在真实机器人的实验中验证了所提出的这种方法，并且提高了测试管转移任务的成功率，表明了修改力信息的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research has demonstrated the usefulness of imitation learning inautonomous robot operation. In particular, teaching using four-channelbilateral control, which can obtain position and force information, has beenproven effective. However, control performance that can easily executehigh-speed, complex tasks in one go has not yet been achieved. We propose amethod called Motion ReTouch, which retroactively modifies motion data obtainedusing four-channel bilateral control. The proposed method enables modificationof not only position but also force information. This was achieved by thecombination of multilateral control and motion-copying system. The proposedmethod was verified in experiments with a real robot, and the success rate ofthe test tube transfer task was improved, demonstrating the possibility ofmodification force information.</description>
      <author>example@mail.com (Koki Inami, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2502.20982v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Optimality and Suboptimality of MPPI Control in Stochastic and Deterministic Settings</title>
      <link>http://arxiv.org/abs/2502.20953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, submitted to LCSS with CDC25 option&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了MPPI控制框架在解决最优控制问题中的应用及其性能分析。&lt;h4&gt;背景&lt;/h4&gt;Model Predictive Path Integral (MPPI) 控制方法近年来受到广泛关注，特别是在机器人和强化学习领域。&lt;h4&gt;目的&lt;/h4&gt;旨在使MPPI控制框架更容易被最优控制社区理解，并展示其应用于三类最优控制问题的效果及性能。&lt;h4&gt;方法&lt;/h4&gt;研究了在一般确定性非线性离散时间系统中，MPPI控制与最优解之间的次优性。通过数值例子来说明这一分析结果。&lt;h4&gt;主要发现&lt;/h4&gt;在一个平滑且无约束的条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优控制问题解之间的差距增长是二次级别的。并且指出调整超参数可以调节这种次优性。&lt;h4&gt;结论&lt;/h4&gt;通过数值例子验证了上述分析结果，并展示了如何通过适当调整超参数来减轻MPPI解决方案的次优性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，最近Model Predictive Path Integral (MPPI) 控制方法在机器人和强化学习领域获得了大量关注。本论文旨在使该控制框架更容易被最优控制社区理解和应用，同时展示了三种不同类型的最优控制问题以及它们通过MPPI得到的解决方案，并研究了确定性非线性离散时间系统中MPPI的次优性能。主要发现表明，在平滑且无约束条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优解之间的差距呈二次级增长。结果还指出，通过适当调整超参数可以调节这种次优性，并用数值例子展示了这些研究结论的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model predictive path integral (MPPI) control has recently received a lot ofattention, especially in the robotics and reinforcement learning communities.This letter aims to make the MPPI control framework more accessible to theoptimal control community. We present three classes of optimal control problemsand their solutions by MPPI. Further, we investigate the suboptimality of MPPIto general deterministic nonlinear discrete-time systems. Here, suboptimalityis defined as the deviation between the control provided by MPPI and theoptimal solution to the deterministic optimal control problem. Our findings arethat in a smooth and unconstrained setting, the growth of suboptimality in thecontrol input trajectory is second-order with the scaling of uncertainty. Theresults indicate that the suboptimality of the MPPI solution can be modulatedby appropriately tuning the hyperparameters. We illustrate our findings usingnumerical examples.</description>
      <author>example@mail.com (Hannes Homburger, Florian Messerer, Moritz Diehl, Johannes Reuter)</author>
      <guid isPermaLink="false">2502.20953v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2502.20900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;灵巧抓取在机器人技术中仍然是一个基础且具有挑战性的问题。通用机器人的任务是能够应对各种物体的抓取需求，并适应不同的场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，即DexGraspVLA框架，以实现更好的跨域迁移能力及泛化性能，从而解决现有研究依赖于特定假设（如单一物体设置或有限环境）而导致泛化受限的问题。&lt;h4&gt;方法&lt;/h4&gt;DexGraspVLA是一种层次化的框架，它利用预训练的视觉-语言模型作为高层次的任务规划器，并学习一种基于扩散的方法作为低级别的动作控制器。其核心在于能够迭代地将多样性的语言和视觉输入转换为领域不变的表示，从而减少域间偏移并使模仿学习更加有效。&lt;h4&gt;主要发现&lt;/h4&gt;在数千种未见过的对象、光照及背景组合的情况下，在零样本环境中达到了90%以上的成功抓取率，并且实验分析表明内部模型行为的一致性随着环境变化而保持稳定。&lt;h4&gt;结论&lt;/h4&gt;该研究通过设计DexGraspVLA框架，展示了实现灵巧抓取的通用能力的可能性，希望为这一领域的进步提供一步推进。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping remains a fundamental yet challenging problem in robotics.A general-purpose robot must be capable of grasping diverse objects inarbitrary scenarios. However, existing research typically relies on specificassumptions, such as single-object settings or limited environments, leading toconstrained generalization. Our solution is DexGraspVLA, a hierarchicalframework that utilizes a pre-trained Vision-Language model as the high-leveltask planner and learns a diffusion-based policy as the low-level Actioncontroller. The key insight lies in iteratively transforming diverse languageand visual inputs into domain-invariant representations, where imitationlearning can be effectively applied due to the alleviation of domain shift.Thus, it enables robust generalization across a wide range of real-worldscenarios. Notably, our method achieves a 90+% success rate under thousands ofunseen object, lighting, and background combinations in a ``zero-shot''environment. Empirical analysis further confirms the consistency of internalmodel behavior across environmental variations, thereby validating our designand explaining its generalization performance. We hope our work can be a stepforward in achieving general dexterous grasping. Our demo and code can be foundat https://dexgraspvla.github.io/.</description>
      <author>example@mail.com (Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, Yuanpei Chen)</author>
      <guid isPermaLink="false">2502.20900v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments</title>
      <link>http://arxiv.org/abs/2502.20843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  http://unicorn-hamnet.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种模块化和可重构的架构，以解决机器人在不同几何环境中执行非抓握操作时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;机器人需要具备非抓取的操作能力来处理不可抓取的对象，如物体倾倒和滚动。然而现有的方法难以适应环境的变化，导致策略难以泛化。&lt;h4&gt;目的&lt;/h4&gt;研究如何让机器人能够根据不同的任务需求自适应地改变其行为模式，以应对复杂多变的几何约束。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的架构设计和接触为基础的对象表示（CORN）的扩展版本来处理环境变化。还开发了一个生成多样环境的算法来训练机器人，并发布了一个包含真实场景数字模型的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的适应性架构能够在完全没有见过的真实世界环境中进行零样本转移，显示出其泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了使用模块化和可重构设计可以解决非抓握操作的通用性和适应性的关键问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：为了使机器人能够在家用等一般环境运行，它们必须能够执行像倾倒和滚动这样的非抓取行为来处理不可抓取的对象。然而现有的关于非抓取操纵的研究还不能泛化到几何结构多样的环境中去。主要挑战在于适应不同的环境约束：在一个橱柜内，机器人需要避开墙壁和天花板；要将物体提升至阶梯顶部，则必须考虑阶梯的姿态和延伸度。虽然深度强化学习（RL）在非抓握操作方面已经取得了显著的成功，但是考虑到这种变化性为泛化策略带来了挑战，因为这需要从每个新的约束组合中学习不同的策略。为了应对这一挑战，我们提出了一种模块化且可重构的架构，该架构能够根据任务需求自适应地重新配置网络模块。为了捕捉环境中几何结构的变化，我们将基于接触的对象表示（CORN）扩展到环境几何，并提出了一个生成多样环境以训练我们的代理的程序算法。综上所述，所得到的策略可以在完全没有见过的真实世界环境中进行零样本转移，尽管是在模拟器中完成的所有培训。此外，我们还发布了以九个真实场景数字模型为特征的数据集来支持现实领域的非抓握操作研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For robots to operate in general environments like households, they must beable to perform non-prehensile manipulation actions such as toppling androlling to manipulate ungraspable objects. However, prior works onnon-prehensile manipulation cannot yet generalize across environments withdiverse geometries. The main challenge lies in adapting to varyingenvironmental constraints: within a cabinet, the robot must avoid walls andceilings; to lift objects to the top of a step, the robot must account for thestep's pose and extent. While deep reinforcement learning (RL) has demonstratedimpressive success in non-prehensile manipulation, accounting for suchvariability presents a challenge for the generalist policy, as it must learndiverse strategies for each new combination of constraints. To address this, wepropose a modular and reconfigurable architecture that adaptively reconfiguresnetwork modules based on task requirements. To capture the geometricvariability in environments, we extend the contact-based object representation(CORN) to environment geometries, and propose a procedural algorithm forgenerating diverse environments to train our agent. Taken together, theresulting policy can zero-shot transfer to novel real-world environments andobjects despite training entirely within a simulator. We additionally release asimulation-based benchmark featuring nine digital twins of real-world sceneswith 353 objects to facilitate non-prehensile manipulation research inrealistic domains.</description>
      <author>example@mail.com (Yoonyoung Cho, Junhyek Han, Jisu Han, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.20843v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Learning-Based Leader Localization for Underwater Vehicles With Optical-Acoustic-Pressure Sensor Fusion</title>
      <link>http://arxiv.org/abs/2502.20817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种三模态传感器融合神经网络方法，用于提高水下多车辆系统中领导者定位的精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;水下航行器在探索和监测海洋环境方面发挥着关键作用。多车辆系统的部署因其能够执行协作任务而引起广泛关注，但要在动态复杂的水下环境中精确确定领导者的方位仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;通过整合光学、声学和压力传感器来解决领导者定位的精度问题，并提高其鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;利用深度学习架构将三模态传感器（光学传感器提供高分辨率成像，声学传感器实现远程探测和测距，而压力传感器则感知环境背景信息）的数据融合起来以提取并结合互补特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明提出的三模态方法显著提高了领导者定位的准确性和鲁棒性，优于单模态或双模态的方法。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了如何通过智能地利用不同类型的传感器数据来提高水下多车辆系统的性能。这种方法有望在未来的海洋科学研究和应用中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;水下航行器已经成为探索和监测水域环境的关键技术，部署多车系统进行协同任务变得越来越受欢迎，但要在一个动态的复杂环境中精确定位领导者则仍然具有挑战性。本文提出了一种新颖的三模态传感器融合神经网络方法，该方法整合了光学、声学和压力传感器来定位领导者。通过利用各个传感模式的独特优势，这种方法提高了定位精度和鲁棒性。实验结果表明，这种三模态方法在准确性与鲁棒性上显著优于单一或双模态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater vehicles have emerged as a critical technology for exploring andmonitoring aquatic environments. The deployment of multi-vehicle systems hasgained substantial interest due to their capability to perform collaborativetasks with improved efficiency. However, achieving precise localization of aleader underwater vehicle within a multi-vehicle configuration remains asignificant challenge, particularly in dynamic and complex underwaterconditions. To address this issue, this paper presents a novel tri-modal sensorfusion neural network approach that integrates optical, acoustic, and pressuresensors to localize the leader vehicle. The proposed method leverages theunique strengths of each sensor modality to improve localization accuracy androbustness. Specifically, optical sensors provide high-resolution imaging forprecise relative positioning, acoustic sensors enable long-range detection andranging, and pressure sensors offer environmental context awareness. The fusionof these sensor modalities is implemented using a deep learning architecturedesigned to extract and combine complementary features from raw sensor data.The effectiveness of the proposed method is validated through a custom-designedtesting platform. Extensive data collection and experimental evaluationsdemonstrate that the tri-modal approach significantly improves the accuracy androbustness of leader localization, outperforming both single-modal anddual-modal methods.</description>
      <author>example@mail.com (Mingyang Yang, Zeyu Sha, Feitian Zhang)</author>
      <guid isPermaLink="false">2502.20817v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Towards Semantic 3D Hand-Object Interaction Generation via Functional Text Guidance</title>
      <link>http://arxiv.org/abs/2502.20805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种创新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，旨在基于功能文本生成手部与物体之间的三维交互姿态。&lt;h4&gt;背景&lt;/h4&gt;手势控制在人机交互中扮演关键角色，但由于抓取动作的复杂性和多样性，现有的技术难以捕捉功能性抓握任务的意义。&lt;h4&gt;目的&lt;/h4&gt;为了实现功能性的三维抓握手部-对象交互（HOI）生成，论文提出了一种新的框架来解决现有方法无法精确模拟功能性抓握的问题。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个文本引导的3D模型生成器Functional Grasp Generator (FGG)和一个姿态优化策略Functional Grasp Refiner (FGR)，前者基于文本输入生成手部与物体的3D模型，后者利用Object Pose Approximator和能量函数来调整姿态以确保符合人体意图。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够产生精确且高质量的手部-对象交互（HOI）而无需额外的三维标注数据。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了FGS-Net的有效性，在生成功能性抓握手部-物体交互方面表现出色，具有广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;手部与物体之间的互动是人与环境之间的重要联系。尽管AI和机器人技术已取得重大进展，但捕捉功能抓握任务的语义仍是一个挑战。本文提出了一种新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，用于基于功能性文本生成3D HOI。该方法结合了基于文本引导的3D模型生成器和姿态优化策略，以确保手部与物体之间的相对位置符合人类意图并保持物理合理性。实验表明，在无需额外三维标注数据的情况下，我们的方法可以实现精确且高质量的手部-对象交互生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-object interaction(HOI) is the fundamental link between human andenvironment, yet its dexterous and complex pose significantly challenges forgesture control. Despite significant advances in AI and robotics, enablingmachines to understand and simulate hand-object interactions, capturing thesemantics of functional grasping tasks remains a considerable challenge. Whileprevious work can generate stable and correct 3D grasps, they are still farfrom achieving functional grasps due to unconsidered grasp semantics. Toaddress this challenge, we propose an innovative two-stage framework,Functional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven byfunctional text. This framework consists of a text-guided 3D model generator,Functional Grasp Generator (FGG), and a pose optimization strategy, FunctionalGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on textinput, while FGR fine-tunes the poses using Object Pose Approximator and energyfunctions to ensure the relative position between the hand and object alignswith human intent and remains physically plausible. Extensive experimentsdemonstrate that our approach achieves precise and high-quality HOI generationwithout requiring additional 3D annotation data.</description>
      <author>example@mail.com (Yongqi Tian, Xueyu Sun, Haoyuan He, Linji Hao, Ning Ding, Caigui Jiang)</author>
      <guid isPermaLink="false">2502.20805v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios</title>
      <link>http://arxiv.org/abs/2502.20789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文分析了加州的自动驾驶车辆碰撞报告，并基于修订后的预碰撞场景分类法，提出了一套自动提取预碰撞场景的映射规则。&lt;h4&gt;背景&lt;/h4&gt;近年来，在自动驾驶车辆（AV）的道路测试中发生了数百起事故，强调了提高其可靠性和安全性的重要性。现有的研究主要集中在传统的人类驾驶车辆上的碰撞分析上，缺少专门针对自动驾驶汽车深入碰撞分析的研究。&lt;h4&gt;目的&lt;/h4&gt;通过分析最新的加州自动驾驶车辆碰撞报告和使用修订后的预碰撞场景分类法来识别自动驾驶车辆的预碰撞场景，并提出优化建议以提高其性能。&lt;h4&gt;方法&lt;/h4&gt;提出了用于自动提取24种不同类型预碰撞场景（准确率为98.1%）的映射规则，特别关注了追尾场景和交叉口场景两大关键场景。对这些场景进行了详细的环境因素分析和因果关系分析。&lt;h4&gt;主要发现&lt;/h4&gt;在追尾场景中，交通控制类型、地点类型以及光线等是重要因素；对于可能引发严重碰撞事故的交叉口场景，则识别出了惯常违反规则与期望特定行为为主要原因。&lt;h4&gt;结论&lt;/h4&gt;本文的研究成果可帮助政府机构制定相关法规，指导制造商设计测试场景，并改进控制算法以优化自动驾驶系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;迄今为止，在自动车辆（AV）的道路测试中发生了数百起事故，突显了提高其可靠性和安全性的必要性。基于碰撞前情景分类法对交通事故进行分类，该方法依据车辆动态和运动学特征。在此基础上，特性分析可以识别类似特征下的相似碰撞情况，为更有效地反映一般碰撞模式并提供更具针对性的建议以提升AV性能提供了可能。然而，目前的研究主要集中在传统的人类驾驶车辆上发生的碰撞事件，缺乏专门针对自动驾驶汽车深度碰撞分析的研究内容。本文中，我们分析了最新的加州AV碰撞报告，并使用新修订的预碰撞场景分类法来识别这些碰撞情况下的预碰撞情景类型。我们提出了一套映射规则以自动提取这些AV预碰撞场景，并成功确定了24种不同类型的预碰撞场景（准确率为98.1%），并通过详细的分析获取了两个关键的AV碰撞场景，即追尾场景和交叉口场景。对追尾场景进行关联性分析后发现，显著的环境影响因素包括交通控制类型、地点类型以及光线等；对于可能引发严重事故的交叉口场景，则通过因果关系分析确定出了惯常违反规则与期望特定行为为主要成因。随后，我们制定了优化建议，既考虑了政府监管方面的需求，也针对AV制造商潜在改进进行了探讨。本文的研究成果能够帮助政府部门制定相关法规，并协助制造厂商设计出更有效的测试场景，在各种现实世界情景中识别出控制系统算法的潜在缺陷并加以优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To date, hundreds of crashes have occurred in open road testing of automatedvehicles (AVs), highlighting the need for improving AV reliability and safety.Pre-crash scenario typology classifies crashes based on vehicle dynamics andkinematics features. Building on this, characteristics analysis can identifysimilar features under comparable crashes, offering a more effective reflectionof general crash patterns and providing more targeted recommendations forenhancing AV performance. However, current studies primarily concentrated oncrashes among conventional human-driven vehicles, leaving a gap in researchdedicated to in-depth AV crash analyses. In this paper, we analyzed the latestCalifornia AV collision reports and used the newly revised pre-crash scenariotypology to identify pre-crash scenarios. We proposed a set of mapping rulesfor automatically extracting these AV pre-crash scenarios, successfullyidentifying 24 types with a 98.1% accuracy rate, and obtaining two keyscenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios)through detailed analysis. Association analyses of rear-end scenarios showedthat the significant environmental influencing factors were traffic controltype, location type, light, etc. For intersection scenarios prone to severecrashes with detailed descriptions, we employed causal analyses to obtain thesignificant causal factors: habitual violations and expectations of certainbehavior. Optimization recommendations were then formulated, addressing bothgovernmental oversight and AV manufacturers' potential improvements. Thefindings of this paper could guide government authorities to develop relatedregulations, help manufacturers design AV test scenarios, and identifypotential shortcomings in control algorithms specific to various real-worldscenarios, thereby optimizing AV systems effectively.</description>
      <author>example@mail.com (Yixuan Li, Xuesong Wang, Tianyi Wang, Qian Liu)</author>
      <guid isPermaLink="false">2502.20789v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CSubBT: A Self-Adjusting Execution Framework for Mobile Manipulation System</title>
      <link>http://arxiv.org/abs/2502.20771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于行为树的条件子树(CSubBT)框架，用于移动机器人在非结构化环境中执行任务时调整执行策略。&lt;h4&gt;背景&lt;/h4&gt;现代智能技术的发展使得装备有机械臂的移动机器人越来越多地应用于非结构化的环境。这些机器人可以根据感知到的信息规划出完成长期任务所需的行动序列，但实际操作中由于感知信息与实际情况不符导致计划失败的情况十分常见。&lt;h4&gt;目的&lt;/h4&gt;为了提高移动机器人的执行成功率和适应性，提出了一种能够自我调整的执行框架CSubBT。&lt;h4&gt;方法&lt;/h4&gt;CSubBT通过将象征性的动作分解为子动作，并使用行为树来控制这些子动作的执行，解决执行过程中的异常问题。该框架认为常见的异常问题是约束条件未满足的问题，在检测到异常时会指导机器人在约束空间内采样新的行动参数。&lt;h4&gt;主要发现&lt;/h4&gt;CSubBT能够有效应对移动机器人的任务执行过程中遇到的各种异常情况，并通过广泛的仿真和现实世界的实验展示了其鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的CSubBT框架为解决移动机械臂操作中的计划与实际情况不匹配的问题提供了一种有效的解决方案，具有较好的实用性和推广价值。&lt;h4&gt;翻译&lt;/h4&gt;随着现代智能技术的进步，配备有机械臂的移动机器人越来越多地在非结构化的环境中运行。这些机器人可以基于感知信息制定长期任务的动作序列规划。然而，在实践中，由于规划所用的感知信息与实际情况不一致，计划经常失败。本文提出了一种基于行为树（BT）的行为子树（CSubBT），这是一种为具有机械臂的任务的移动机器人的自适应执行而设计的一般框架。CSubBT将象征性动作分解成子动作，并使用BT来控制它们的执行，在过程中处理任何可能的异常情况。当检测到异常时，它会通过在约束空间内采样新的操作参数连续指导机器人完成任务。我们通过广泛的模拟和现实世界环境中的机械臂实验展示了该框架的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the advancements in modern intelligent technologies, mobile robotsequipped with manipulators are increasingly operating in unstructuredenvironments. These robots can plan sequences of actions for long-horizon tasksbased on perceived information. However, in practice, the planned actions oftenfail due to discrepancies between the perceptual information used for planningand the actual conditions. In this paper, we introduce the {\itshapeConditional Subtree} (CSubBT), a general self-adjusting execution framework formobile manipulation tasks based on Behavior Trees (BTs). CSubBT decomposessymbolic action into sub-actions and uses BTs to control their execution,addressing any potential anomalies during the process. CSubBT treats commonanomalies as constraint non-satisfaction problems and continuously guides therobot in performing tasks by sampling new action parameters in the constraintspace when anomalies are detected. We demonstrate the robustness of ourframework through extensive manipulation experiments on different platforms,both in simulation and real-world settings.</description>
      <author>example@mail.com (Huihui Guo, Huizhang Luo, Huilong Pi, Mingxing Duan, Kenli Li, Chubo Liu)</author>
      <guid isPermaLink="false">2502.20771v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A2DO: Adaptive Anti-Degradation Odometry with Deep Multi-Sensor Fusion for Autonomous Navigation</title>
      <link>http://arxiv.org/abs/2502.20767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6+1pages, 6 figures, accept by ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为A2DO的新系统，该系统通过深度神经网络在挑战性条件下提高了SLAM系统的鲁棒性和定位准确性。&lt;h4&gt;背景&lt;/h4&gt;自主车辆的安全和有效导航需要精确的定位，而同时进行定位与地图构建（SLAM）技术是实现这一目标的关键。然而，在不良天气、光线不足或障碍物等情况下，传感器退化会影响SLAM系统的性能。&lt;h4&gt;目的&lt;/h4&gt;通过结合多传感器数据并利用深度学习技术来提高在各种复杂条件下的自主车辆导航系统精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;A2DO是一个端到端的融合了LiDAR与视觉数据的里程计系统，它使用了一个多层次、多尺度特征编码模块，并引入注意力机制以动态减轻传感器退化问题。该模型在广泛覆盖各种退化场景的模拟数据集上进行了预训练，并通过精选的真实世界数据进一步微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，A2DO能够在多种传感器退化条件下保持优越的定位准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个具备潜在实用价值的方法来提高自主车辆导航系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;精确的定位对于自主车辆的安全和有效导航至关重要。SLAM是这个领域中的关键技术，但在诸如低光照、恶劣天气或传感器退化等不利条件下，其表现会下降。我们提出了一个基于深度神经网络的新系统A2DO，旨在通过融合LiDAR和视觉数据来提升这些情况下的鲁棒性。该系统采用了多层、多尺度特征编码模块并加入了注意力机制以动态解决传感器退化问题，并且在模拟及真实世界的数据集上进行了广泛的训练与微调。实验结果表明，A2DO能够在各种传感器降级条件下保持卓越的定位准确性和稳定性，展示出其实用实施于自主车辆系统中的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for the safe and effective navigation ofautonomous vehicles, and Simultaneous Localization and Mapping (SLAM) is acornerstone technology in this context. However, The performance of the SLAMsystem can deteriorate under challenging conditions such as low light, adverseweather, or obstructions due to sensor degradation. We present A2DO, a novelend-to-end multi-sensor fusion odometry system that enhances robustness inthese scenarios through deep neural networks. A2DO integrates LiDAR and visualdata, employing a multi-layer, multi-scale feature encoding module augmented byan attention mechanism to mitigate sensor degradation dynamically. The systemis pre-trained extensively on simulated datasets covering a broad range ofdegradation scenarios and fine-tuned on a curated set of real-world data,ensuring robust adaptation to complex scenarios. Our experiments demonstratethat A2DO maintains superior localization accuracy and robustness acrossvarious degradation conditions, showcasing its potential for practicalimplementation in autonomous vehicle systems.</description>
      <author>example@mail.com (Hui Lai, Qi Chen, Junping Zhang, Jian Pu)</author>
      <guid isPermaLink="false">2502.20767v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Acquiring Grounded Representations of Words with Situated Interactive Instruction</title>
      <link>http://arxiv.org/abs/2502.20754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种通过混合主动性、情境化的互动与人类指导者交流来获取单词的接地表示的方法。&lt;h4&gt;背景&lt;/h4&gt;研究集中在从感知知识、语义知识和程序知识中获得多样化类型的知识，并学习它们的接地含义。&lt;h4&gt;目的&lt;/h4&gt;允许代理通过请求关于未知概念的指令，控制其学习过程，从而提高学习效率。&lt;h4&gt;方法&lt;/h4&gt;该方法在Soar系统中实现了，并在一个能够操控小型物体的桌面机器人手臂上进行了测试和评估。&lt;h4&gt;主要发现&lt;/h4&gt;交互式学习使得智能体可以高效地获取不同类型的知识并理解和操作未知概念。&lt;h4&gt;结论&lt;/h4&gt;提出的这种方法有效地提高了代理通过与人类互动来学习复杂知识的能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种从混合主动性和情境化的互动中，通过与人类指导者的交流获得单词的接地表示的方法。该方法关注于多样化类型的知识获取，包括感知、语义和程序性知识，并且旨在学习这些概念的接地含义。交互式的学习允许智能体控制其学习过程，通过请求有关未知概念的指令，从而使其学习过程更加高效。这种方法在Soar系统中实现，并在一个能够操控小型物体的桌面机器人手臂上进行了评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an approach for acquiring grounded representations of words frommixed-initiative, situated interactions with a human instructor. The workfocuses on the acquisition of diverse types of knowledge including perceptual,semantic, and procedural knowledge along with learning grounded meanings.Interactive learning allows the agent to control its learning by requestinginstructions about unknown concepts, making learning efficient. Our approachhas been instantiated in Soar and has been evaluated on a table-top robotic armcapable of manipulating small objects.</description>
      <author>example@mail.com (Shiwali Mohan, Aaron H. Mininger, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2502.20754v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Indoor Localization for Autonomous Robot Navigation</title>
      <link>http://arxiv.org/abs/2502.20731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了室内定位系统在自主机器人导航中的应用，研究团队收集了一个数据集并训练模型以测试机器人，并开发了一种A*路径规划算法。经过不同网络结构的测试，机器人的成功转弯率为50%左右。&lt;h4&gt;背景&lt;/h4&gt;随着户外导航技术的发展，在日常生活中的重要性日益增加，室内定位系统（IPS）受到了广泛关注和研究。&lt;h4&gt;目的&lt;/h4&gt;探索利用室内定位系统完成自主机器人在室内的导航任务。&lt;h4&gt;方法&lt;/h4&gt;1. 收集并训练数据集以测试机器人；2. 开发A*路径规划算法使机器人能够使用预测方向自我导航。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验，机器人的成功转弯率为50%左右。&lt;h4&gt;结论&lt;/h4&gt;利用室内定位系统进行自主机器人导航是未来研究的一个有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor positioning systems (IPSs) have gained attention as outdoor navigationbecomes prevalent in everyday life. Research is being actively conducted on howindoor smartphone navigation can be accomplished and improved using receivedsignal strength indication (RSSI) and machine learning (ML). IPSs have more usecases that need further exploration, and we aim to explore using IPSs for theindoor navigation of an autonomous robot. We collected a dataset and trainedmodels to test on a robot. We also developed an A* path-planning algorithm sothat our robot could navigate itself using predicted directions. After testingdifferent network structures, our robot was able to successfully navigatecorners around 50 percent of the time. The findings of this paper indicate thatusing IPSs for autonomous robots is a promising area of future research.</description>
      <author>example@mail.com (Sean Kouma, Rachel Masters)</author>
      <guid isPermaLink="false">2502.20731v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>FSMP: A Frontier-Sampling-Mixed Planner for Fast Autonomous Exploration of Complex and Large 3-D Environments</title>
      <link>http://arxiv.org/abs/2502.20707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13pages, 12 figures, accepted by IEEE Transactions on Instrumentation  and Measurement&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用微型空中飞行器（MAVs）快速探索复杂且庞大的3-D环境的系统框架。&lt;h4&gt;背景&lt;/h4&gt;在复杂和大规模的三维环境中进行有效的探索具有挑战性，传统的基于前沿或随机采样的方法难以实现高效和完整的覆盖。&lt;h4&gt;目的&lt;/h4&gt;为了提高探索效率并确保完整性和可靠性，提出了一种结合了基于前沿的方法和基于采样策略的新型框架。&lt;h4&gt;方法&lt;/h4&gt;{'前端检测器': '设计了一个以视野为基础（FOV）的前沿探测器，该探测器保证完成度和正确性', '确定性采样技术': '采用确定性的采样技术来建立和维护基于记录的传感器视场和新检测到的前沿的增量式道路地图。', '路径规划器': '提出了一个两阶段路径规划算法：第一阶段使用惰性评估策略快速计算全局最优探索路线；第二阶段对最佳探索路径进行平滑处理以进一步提高探索效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了该方法在模拟和现实环境中的有效性，展示了其在探索效率、计算时间和已探索体积方面的出色表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的快速探索框架为复杂3-D环境下的MAV应用提供了一种高效且可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种使用微型空中飞行器（MAVs）的系统性框架，用于快速探索复杂的大型三维环境。该方法的核心见解在于有机地整合基于前沿和采样策略的方法，以实现环境的整体快速探索。设计了一个基于视野（FOV）的前端检测器，确保了完整性和正确性的前提下识别三维地图边界。与随机采样法不同的是，采用确定性采样技术来建立并维护一种增量式道路图，该图依赖于记录下来的传感器视场以及新发现的前沿。利用所构建的道路图，提出了一种两阶段路径规划算法：首阶段快速计算出全局最优探索路线；次阶段则进一步平滑优化此最佳路线以提高效率。文中通过仿真和现实世界中的实验验证了这一方法的有效性，并且比较结果表明该框架在探索效率、计算时间和已探索体积方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a systematic framework for fast exploration ofcomplex and large 3-D environments using micro aerial vehicles (MAVs). The keyinsight is the organic integration of the frontier-based and sampling-basedstrategies that can achieve rapid global exploration of the environment.Specifically, a field-of-view-based (FOV) frontier detector with the guaranteeof completeness and soundness is devised for identifying 3-D map frontiers.Different from random sampling-based methods, the deterministic samplingtechnique is employed to build and maintain an incremental road map based onthe recorded sensor FOVs and newly detected frontiers. With the resulting roadmap, we propose a two-stage path planner. First, it quickly computes the globaloptimal exploration path on the road map using the lazy evaluation strategy.Then, the best exploration path is smoothed for further improving theexploration efficiency. We validate the proposed method both in simulation andreal-world experiments. The comparative results demonstrate the promisingperformance of our planner in terms of exploration efficiency, computationaltime, and explored volume.</description>
      <author>example@mail.com (Shiyong Zhang, Xuebo Zhang, Qianli Dong, Ziyu Wang, Haobo Xi, Jing Yuan)</author>
      <guid isPermaLink="false">2502.20707v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>WorldModelBench: Judging Video Generation Models As World Models</title>
      <link>http://arxiv.org/abs/2502.20694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;视频生成模型在快速发展，能够支持决策应用如机器人和自动驾驶。然而现有的基准测试未能严格评估这些能力。&lt;h4&gt;背景&lt;/h4&gt;当前的基准测试只关注通用视频质量，忽略了世界建模所需的重要因素，例如物理规则遵守情况。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一差距，我们提出了WorldModelBench，一个旨在评价视频生成模型在应用驱动领域中世界建模能力的基准。&lt;h4&gt;方法&lt;/h4&gt;{'优势1': '通过引入指令跟随和物理规则遵守维度，WorldModelBench能检测细微的违规情况，如违反质量守恒定律的对象尺寸不规则变化的问题，这些问题被之前的基准所忽视。', '优势2': '通过大规模的人类偏好对齐，我们收集了67K人类标签来准确测量14个前沿模型。使用高质量的人类标签进一步微调了一个精确的评判者以自动化评估过程，并实现了比GPT-4o更高的预测世界建模违规平均精度。', '其他贡献': '训练模型与人工注释对齐，最大化来自评判者的奖励可以显著提高世界的建模能力'}&lt;h4&gt;主要发现&lt;/h4&gt;通过引入特定维度来检测细微问题和大规模的人类偏好对齐方法，WorldModelBench能更全面地评估视频生成模型的世界建模能力。&lt;h4&gt;结论&lt;/h4&gt;我们的研究强调了现有基准测试的局限性，并提出了一种改进的方法来准确评价这些模型的能力。该网站可以访问https://worldmodelbench-team.github.io&lt;h4&gt;翻译&lt;/h4&gt;视频生成模型正在迅速发展，将自己定位为支持决策应用（如机器人技术与自动驾驶）的世界模型。然而，当前的评估基准未能严格验证这些声明，只关注通用视频质量，忽略世界建模所需的重要因素，比如物理一致性等。为解决这一问题，我们提出了WorldModelBench，一个旨在测试视频生成模型在应用驱动领域的世界建模能力的新标准。该基准的主要优势在于：1）能够检测细微的世界建模违规情况，如违反了质量守恒定律的对象尺寸不规则变化；2）通过大规模的人类偏好对齐方法进行准确评估，利用67K人类标签来衡量多个前沿模型，并微调评判者以自动化这一过程。结果表明，该方法比GPT-4o更为精确地预测世界建模违规情况。此外，我们展示了训练与人工注释对齐可以显著提高世界的建模能力。有关WorldModelBench的更多信息，请访问https://worldmodelbench-team.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models have rapidly progressed, positioning themselves asvideo world models capable of supporting decision-making applications likerobotics and autonomous driving. However, current benchmarks fail to rigorouslyevaluate these claims, focusing only on general video quality, ignoringimportant factors to world models such as physics adherence. To bridge thisgap, we propose WorldModelBench, a benchmark designed to evaluate the worldmodeling capabilities of video generation models in application-driven domains.WorldModelBench offers two key advantages: (1) Against to nuanced worldmodeling violations: By incorporating instruction-following andphysics-adherence dimensions, WorldModelBench detects subtle violations, suchas irregular changes in object size that breach the mass conservation law -issues overlooked by prior benchmarks. (2) Aligned with large-scale humanpreferences: We crowd-source 67K human labels to accurately measure 14 frontiermodels. Using our high-quality human labels, we further fine-tune an accuratejudger to automate the evaluation procedure, achieving 8.6% higher averageaccuracy in predicting world modeling violations than GPT-4o with 2Bparameters. In addition, we demonstrate that training to align humanannotations by maximizing the rewards from the judger noticeably improve theworld modeling capability. The website is available athttps://worldmodelbench-team.github.io.</description>
      <author>example@mail.com (Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu)</author>
      <guid isPermaLink="false">2502.20694v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment</title>
      <link>http://arxiv.org/abs/2502.20693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper published at "Workshop on Design, Learning, and control for  safe human-robot collaboration at the International Conference on Advanced  Robotics (ICAR)"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文审查了移动机器人在工作场所部署的安全标准和方法，并提出了一个新的风险评估框架来确保建筑工地上的安全使用。&lt;h4&gt;背景&lt;/h4&gt;移动机器人被广泛应用于各种工作环境以提高生产效率，但在拥挤且有危险的环境中与人类工人互动存在安全挑战。&lt;h4&gt;目的&lt;/h4&gt;为解决移动机器人在施工场地部署时的安全问题，提出一套全面的风险评估方法，并通过领域专家验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;首先回顾了现有标准和相关研究文献，然后基于这些信息提出了一个改进的风险评估框架以覆盖未被现有的安全性指南涵盖的情景。&lt;h4&gt;主要发现&lt;/h4&gt;新的风险评估框架可以更好地保护工人免受移动机器人操作的潜在危害，并提供了具体的建议来降低此类风险。&lt;h4&gt;结论&lt;/h4&gt;通过扩展现有安全标准并提出额外的安全措施，该研究有助于更安全地部署和使用移动机器人在建筑工地和其他复杂环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robotic systems are increasingly used in various work environments tosupport productivity. However, deploying robots in workplaces crowded by humanworkers and interacting with them results in safety challenges and concerns,namely robot-worker collisions and worker distractions in hazardousenvironments. Moreover, the literature on risk assessment as well as thestandard specific to mobile platforms is rather limited. In this context, thispaper first conducts a review of the relevant standards and methodologies andthen proposes a risk assessment for the safe deployment of mobile robots onconstruction sites. The approach extends relevant existing safety standards toencompass uncovered scenarios. Safety recommendations are made based on theframework, after its validation by field experts.</description>
      <author>example@mail.com (Bruno Belzile, Tatiana Wanang-Siyapdjie, Sina Karimi, Rafael Gomes Braga, Ivanka Iordanova, David St-Onge)</author>
      <guid isPermaLink="false">2502.20693v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection</title>
      <link>http://arxiv.org/abs/2502.20651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了COU数据集，一个包含常见人造物体实例分割图像的水下图像库。&lt;h4&gt;背景&lt;/h4&gt;在水下环境中缺乏用于实例分割的高质量和多样化的数据集。现有的数据集主要关注海洋生物，而忽视了其他类型的水下物体。&lt;h4&gt;目的&lt;/h4&gt;创建一个新的数据集COU，涵盖多种不同环境下的常见人造物品，以促进轻量级实时检测器的研发，特别是针对自主式水下航行器（AUV）的训练需求。&lt;h4&gt;方法&lt;/h4&gt;从多个地点的机器人实地试验中收集图像，并对这些图像进行了详细的实例分割标注。使用三种最先进的模型来评估COU数据集在训练水下目标检测器方面的性能和准确性。&lt;h4&gt;主要发现&lt;/h4&gt;相比于仅基于地面数据训练的目标检测器，使用COU进行训练显著提高了检测器的表现和精确度。&lt;h4&gt;结论&lt;/h4&gt;COU是一个多样化的、高质量的数据集，专门用于改进自主式水下航行器的实时对象检测能力。该数据集将对研究界开放，并采用开源许可证提供。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为COU（水下的常见物体）的数据集，这是一个包含多种水生和海洋环境中常见的人造物体实例分割图像的数据集。COU包含了大约10,000张标注过的分割图像，这些图像是从不同地点进行的多次水下机器人实地试验中收集而来的。该数据集旨在解决缺乏用于水下实例分割的稳健分类覆盖的问题，这对于训练轻量级、实时能力强大的自主式水下航行器（AUV）检测器特别有用。此外，COU解决了对象类别多样性的不足问题，因为常见的水下图像数据集仅关注海洋生物。目前，COU包含了来自封闭水域（泳池）和开放水域（湖泊和海洋）环境的24种不同类别的物体图像，包括海洋垃圾、潜水工具以及AUV等。为了评估COU在训练水下目标检测器方面的效果，我们使用三种最先进的模型来评估其性能和准确性，采用了标准准确率和效率指标相结合的方法。COU训练过的检测器相较于仅基于地面数据训练的检测器表现出明显的优势。我们将在开源许可下提供COU供广泛使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce COU: Common Objects Underwater, an instance-segmented imagedataset of commonly found man-made objects in multiple aquatic and marineenvironments. COU contains approximately 10K segmented images, annotated fromimages collected during a number of underwater robot field trials in diverselocations. COU has been created to address the lack of datasets with robustclass coverage curated for underwater instance segmentation, which isparticularly useful for training light-weight, real-time capable detectors forAutonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack ofdiversity in object classes since the commonly available underwater imagedatasets focus only on marine life. Currently, COU contains images from bothclosed-water (pool) and open-water (lakes and oceans) environments, of 24different classes of objects including marine debris, dive tools, and AUVs. Toassess the efficacy of COU in training underwater object detectors, we usethree state-of-the-art models to evaluate its performance and accuracy, using acombination of standard accuracy and efficiency metrics. The improvedperformance of COU-trained detectors over those solely trained on terrestrialdata demonstrates the clear advantage of training with annotated underwaterimages. We make COU available for broad use under open-source licenses.</description>
      <author>example@mail.com (Rishi Mukherjee, Sakshi Singh, Jack McWilliams, Junaed Sattar)</author>
      <guid isPermaLink="false">2502.20651v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>EDENet: Echo Direction Encoding Network for Place Recognition Based on Ground Penetrating Radar</title>
      <link>http://arxiv.org/abs/2502.20643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于地表穿透雷达(GPR)的定位技术在机器人领域中的应用，并提出了一种新的网络架构EDENet来解决大规模地图中位置识别的问题。&lt;h4&gt;背景&lt;/h4&gt;GPR由于能够探测稳定的地下特征，在机器人领域得到了广泛应用。然而，现有的方法主要集中在小规模的位置识别上，忽略了大规模地图中所面临的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究GPR回波序列与地下场景之间的几何关系，并提出一种新的网络设计来应对大尺度位置识别的难题。&lt;h4&gt;方法&lt;/h4&gt;引入可学习的Gabor滤波器以精确提取方向响应，并结合方向感知注意力机制进行有效的几何编码。还使用了移不变单元和多尺度聚合策略，提高了对介电常数变化的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的EDENet在公共数据集上的实验表明，它不仅在位置识别性能上超越了现有解决方案，还在模型大小和计算效率方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过提出创新的方法和技术，有效解决了大规模地图中基于GPR的位置识别问题，并展示了其优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ground penetrating radar (GPR) based localization has gained significantrecognition in robotics due to its ability to detect stable subsurfacefeatures, offering advantages in environments where traditional sensors likecameras and LiDAR may struggle. However, existing methods are primarily focusedon small-scale place recognition (PR), leaving the challenges of PR inlarge-scale maps unaddressed. These challenges include the inherent sparsity ofunderground features and the variability in underground dielectric constants,which complicate robust localization. In this work, we investigate thegeometric relationship between GPR echo sequences and underground scenes,leveraging the robustness of directional features to inform our network design.We introduce learnable Gabor filters for the precise extraction of directionalresponses, coupled with a direction-aware attention mechanism for effectivegeometric encoding. To further enhance performance, we incorporate ashift-invariant unit and a multi-scale aggregation strategy to betteraccommodate variations in di-electric constants. Experiments conducted onpublic datasets demonstrate that our proposed EDENet not only surpassesexisting solutions in terms of PR performance but also offers advantages inmodel size and computational efficiency.</description>
      <author>example@mail.com (Pengyu Zhang, Xieyuanli Chen, Yuwei Chen, Beizhen Bi, Zhuo Xu, Tian Jin, Xiaotao Huang, Liang Shen)</author>
      <guid isPermaLink="false">2502.20643v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Tuning Algorithmic and Architectural Hyperparameters in Graph-Based Semi-Supervised Learning with Provable Guarantees</title>
      <link>http://arxiv.org/abs/2502.12937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages (11 pages main body), 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文研究了图半监督学习中基于参数化算法家族的超参数调优问题，特别是针对经典标签传播算法家族以及现代图卷积神经网络（GCN）和简化图卷积（SGC）网络。论文提供了关于选择最优超参数所需的理论数据量的新上界，并为不同的图神经网络架构提供了Rademacher复杂度界。&lt;h4&gt;背景&lt;/h4&gt;图半监督学习在机器学习领域具有重要地位，它通过建模未标记数据与已标记数据之间的关系来利用隐含的图形结构。许多经典算法和现代深度学习方法被提出用于解决这一问题，这些算法通常含有可调超参数。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探讨如何从一系列参数化算法家族中调整适合图半监督学习任务的超参数，并研究其理论上的复杂性。&lt;h4&gt;方法&lt;/h4&gt;对于三个经典的基于标签传播的算法系列，论文获取了新的伪维度上界和匹配的下界，这些边界与节点数量n呈对数关系。此外，还考虑现代简化图卷积网络中的自环权重调优问题以及GCN和GAT之间的可调节架构选择问题。&lt;h4&gt;主要发现&lt;/h4&gt;对于三个基于标签传播的经典算法系列，在确定最优超参数时所需的训练数据量方面获得了对数级的理论边界；提出了一种可以同时包含GCN和GAT特征的新图神经网络架构，并给出了针对此架构调优参数时的学习复杂性分析。&lt;h4&gt;结论&lt;/h4&gt;本文通过形式化研究为调整图半监督学习算法中的超参数提供了有效的理论指导，这对未来设计高效的机器学习模型具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based semi-supervised learning is a powerful paradigm in machinelearning for modeling and exploiting the underlying graph structure thatcaptures the relationship between labeled and unlabeled data. A large number ofclassical as well as modern deep learning based algorithms have been proposedfor this problem, often having tunable hyperparameters. We initiate a formalstudy of tuning algorithm hyperparameters from parameterized algorithm familiesfor this problem. We obtain novel $O(\log n)$ pseudo-dimension upper bounds forhyperparameter selection in three classical label propagation-based algorithmfamilies, where $n$ is the number of nodes, implying bounds on the amount ofdata needed for learning provably good parameters. We further provide matching$\Omega(\log n)$ pseudo-dimension lower bounds, thus asymptoticallycharacterizing the learning-theoretic complexity of the parameter tuningproblem. We extend our study to selecting architectural hyperparameters inmodern graph neural networks. We bound the Rademacher complexity for tuning theself-loop weighting in recently proposed Simplified Graph Convolution (SGC)networks. We further propose a tunable architecture that interpolates graphconvolutional neural networks (GCN) and graph attention networks (GAT) in everylayer, and provide Rademacher complexity bounds for tuning the interpolationcoefficient.</description>
      <author>example@mail.com (Ally Yalei Du, Eric Huang, Dravyansh Sharma)</author>
      <guid isPermaLink="false">2502.12937v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
  <item>
      <title>SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation</title>
      <link>http://arxiv.org/abs/2502.19128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的3D形状和描述在线数据增强方法SCA3D，用于跨模式的3D检索任务。通过使用LLaVA模型生成更多的文本-3D对来解决现有方法因缺乏多样化的3D数据而导致的表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;跨模态3D检索旨在实现自然语言与3D形状之间的互匹配，但现有的方法由于缺乏高质量的3D数据而表现出较差的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;通过生成更多的3D-文本对来改善现有3D检索方法的表现，并增强其在多样的场景中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用LLaVA模型为每个分割后的3D形状的部分生成描述，产生新的3D-文本配对；采用内部和外部距离将各组件组合成新3D形状；利用模板处理部件的描述并创建新文字描述；用单模态编码器提取改进数据集上的嵌入向量，并通过对比学习增强跨模式匹配。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的SCA3D方法在Text2Shape数据集中表现优越，相较于之前的工作显著提高了形状到文本和文本到形状的检索精度。&lt;h4&gt;结论&lt;/h4&gt;提出的方法证明了通过生成更多高质量的数据可以有效提升跨模态3D检索的表现，并且代码开源可供进一步研究使用。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容为：跨模态3D检索任务旨在实现自然语言描述与3D形状之间的相互匹配，这有可能增强自然语言和三维环境间的互动，在机器人技术和具身人工智能（AI）应用领域尤其如此。然而，由于缺乏高质量的3D数据，现有方法的表现受限，并且依赖于有限数量的3D形状特征导致在不同场景下的泛化能力较差。为解决这一挑战，我们引入了SCA3D，这是一种新的用于跨模态3D检索的数据增强方法，通过LLaVA模型创建一个组件库，为数据集中的每个3D形状的部分生成描述。这种方法不仅促进了包含新语义特征的大量新的3D-文本对的生成，还采用内外距离来将各组件组合成新的3D形状，并利用模板处理各个组件的描述以产生新的文字描述。此外，我们使用单模态编码器从增强的数据集中提取基于3D形状和文本的嵌入向量，并通过地球移动者距离（EMD）计算细粒度跨模式相似性并利用对比学习提高跨模式匹配能力，实现在文本与3D形状间的双向检索。广泛的实验表明，我们的SCA3D方法在Text2Shape数据集中超越了先前的工作，在形状到文本的RR@1评分从20.03提升至27.22和文本到形状的RR@1评分从13.12提升至16.67。相关代码可在https://github.com/3DAgentWorld/SCA3D中找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cross-modal 3D retrieval task aims to achieve mutual matching betweentext descriptions and 3D shapes. This has the potential to enhance theinteraction between natural language and the 3D environment, especially withinthe realms of robotics and embodied artificial intelligence (AI) applications.However, the scarcity and expensiveness of 3D data constrain the performance ofexisting cross-modal 3D retrieval methods. These methods heavily rely onfeatures derived from the limited number of 3D shapes, resulting in poorgeneralization ability across diverse scenarios. To address this challenge, weintroduce SCA3D, a novel 3D shape and caption online data augmentation methodfor cross-modal 3D retrieval. Our approach uses the LLaVA model to create acomponent library, captioning each segmented part of every 3D shape within thedataset. Notably, it facilitates the generation of extensive new 3D-text pairscontaining new semantic features. We employ both inter and intra distances toalign various components into a new 3D shape, ensuring that the components donot overlap and are closely fitted. Further, text templates are utilized toprocess the captions of each component and generate new text descriptions.Besides, we use unimodal encoders to extract embeddings for 3D shapes and textsbased on the enriched dataset. We then calculate fine-grained cross-modalsimilarity using Earth Mover's Distance (EMD) and enhance cross-modal matchingwith contrastive learning, enabling bidirectional retrieval between texts and3D shapes. Extensive experiments show our SCA3D outperforms previous works onthe Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be foundin https://github.com/3DAgentWorld/SCA3D.</description>
      <author>example@mail.com (Junlong Ren, Hao Wu, Hui Xiong, Hao Wang)</author>
      <guid isPermaLink="false">2502.19128v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Increasing the Task Flexibility of Heavy-Duty Manipulators Using Visual 6D Pose Estimation of Objects</title>
      <link>http://arxiv.org/abs/2502.19169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种管道，用于通过高级机器视觉对重型长距离机械臂的工具进行精确定位。该方法结合了深度神经网络和基于运动的摄像机与机器人校准。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用深层神经网络在物体6D姿态估计方面的进展推动了基于视觉控制的新方式，尤其是在重型机器人应用中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的管道系统，利用机器视觉技术实现对重型长距离机械臂工具的精确定位。&lt;h4&gt;方法&lt;/h4&gt;{'相机配置': '采用眼在手（eye-in-hand）的摄像机配置来直接估计感兴趣物体和工件的姿态', '姿态误差计算': '根据工具与目标之间的姿态误差进行精密定位，同时通过基于运动的校准完成摄像机到机器人的校准。', '训练数据': '仅使用合成数据训练用于估计对象兴趣（OOI）姿态的深度神经网络'}&lt;h4&gt;主要发现&lt;/h4&gt;{'精度': '实验结果表明，在非深度轴上实现了小于2毫米的平均图像定位误差，这有助于提高柔性重型长距离机械臂的任务灵活性和自动化水平。', '准确性提升': '通过基于图像算法避免了由于结构柔性的刚体运动学而引起的不准确度'}&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提供了一种新颖的方式，以增加非刚性重型长距离机器人的任务灵活度与自动化程度，并且已经在现实世界中进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;最近在利用深层神经网络进行物体6D姿态估计方面的进展促进了基于视觉的控制方法的发展，尤其是在重型机器人应用方面。在这项研究中，我们提出了一种使用高级机器视觉技术来实现重型长距离机械臂工具精确定位的新管道系统。该方案采用眼在手配置的相机直接估算工件和目标对象的姿态，并根据姿态误差以及摄像机与机器人之间的运动校准，通过常规工业上广泛使用的机器人建模和控制方法实现可靠的精密定位。所提出的方法包括基于视觉估计OOI姿态的位置对齐，而摄像机到机器人的校准则利用基于动作的视觉SLAM进行。这些技术试图通过图像基算法避免由于结构柔性的刚体运动学引起的不准确性。为了训练用于目标对象姿态估计的深度神经网络，仅使用合成数据。该方法在具有5米伸展范围的实际重型长距离机械臂上进行了验证。实验结果表明，在非深度轴上实现了低于2毫米的平均工具定位误差，这有助于增加柔性重型长距离机械臂的任务灵活性和自动化水平的一种新方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in visual 6D pose estimation of objects using deep neuralnetworks have enabled novel ways of vision-based control for heavy-duty roboticapplications. In this study, we present a pipeline for the precise toolpositioning of heavy-duty, long-reach (HDLR) manipulators using advancedmachine vision. A camera is utilized in the so-called eye-in-hand configurationto estimate directly the poses of a tool and a target object of interest (OOI).Based on the pose error between the tool and the target, along withmotion-based calibration between the camera and the robot, precise toolpositioning can be reliably achieved using conventional robotic modeling andcontrol methods prevalent in the industry. The proposed methodology comprisesorientation and position alignment based on the visually estimated OOI poses,whereas camera-to-robot calibration is conducted based on motion utilizingvisual SLAM. The methods seek to avert the inaccuracies resulting fromrigid-body--based kinematics of structurally flexible HDLR manipulators viaimage-based algorithms. To train deep neural networks for OOI pose estimation,only synthetic data are utilized. The methods are validated in a real-worldsetting using an HDLR manipulator with a 5 m reach. The experimental resultsdemonstrate that an image-based average tool positioning error of less than 2mm along the non-depth axes is achieved, which facilitates a new way toincrease the task flexibility and automation level of non-rigid HDLRmanipulators.</description>
      <author>example@mail.com (Petri Mäkinen, Pauli Mustalahti, Tuomo Kivelä, Jouni Mattila)</author>
      <guid isPermaLink="false">2502.19169v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Online Meta-learning for AutoML in Real-time (OnMAR)</title>
      <link>http://arxiv.org/abs/2502.20279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First page is a graphical abstract, this is a journal article  submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个在线元学习为实时自动机器学习（AutoML）的方法，即OnMAR。该方法利用元学习收集关于机器学习算法优化过程的信息，并使用元学习器预测模型设计的准确性以改进实时AutoML的设计质量与速度。&lt;h4&gt;背景&lt;/h4&gt;自动化机器学习(AutoML)是一个研究领域，旨在通过优化技术设计机器学习(ML)算法，减少人工干预的需求。实时AutoML允许在实际应用任务中进行设计过程。现有方法在质量和时间效率方面有待改善。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的在线元学习方法来提高实时AutoML的设计质量和速度。&lt;h4&gt;方法&lt;/h4&gt;使用了一种称为OnMAR的方法，通过遗传算法和不同的元学习器（k近邻、随机森林和XGBoost）进行模型预测。这种方法适用于多个不同场景下的实时AutoML应用，并进行了相应的测试。&lt;h4&gt;主要发现&lt;/h4&gt;OnMAR方法在三个不同的应用场景下展现了良好的效果，可以匹配甚至超越现有的实时AutoML方法，在设计时间和准确性方面都有显著优势。&lt;h4&gt;结论&lt;/h4&gt;在线元学习（OnMAR）是解决实时AutoML中模型优化问题的有效途径。它可以提升现有技术的性能，并且具有快速运行时间的优势。&lt;h4&gt;翻译&lt;/h4&gt;自动化机器学习(AutoML)是一个研究领域，专注于使用优化技术设计机器学习(ML)算法，以减轻人为手动设计算法的需求。实时AutoML使设计过程能够在应用到任务的同时进行。作为新兴的研究领域，现有的实时AutoML技术在设计质量和所需时间方面需要改进。为了解决这些问题，本研究提出了一种在线元学习用于实时AutoML的方法（OnMAR）。元学习收集了由机器学习算法在其优化过程中产生的元特征信息。这些元特征与一个元学习器结合使用以优化该过程。OnMAR方法采用元学习器来预测一个ML设计的准确性；如果预测准确度足够高，则接受此设计，反之则通过优化技术创建新的设计。作为OnMAR的一部分使用的优化技术是遗传算法(GA)。测试了不同的元学习器（k近邻、随机森林和XGBoost）。由于该方法与模型无关(即不特定于单个实时AutoML应用)，因此在三个不同的实时AutoML应用场景中进行了评估，包括：组成图像聚类算法、配置卷积神经网络的超参数以及设置视频分类管道。OnMAR方法是有效的，在性能上可以匹配或超越现有的实时AutoML方法，并具有更快运行时间的优点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated machine learning (AutoML) is a research area focusing on usingoptimisation techniques to design machine learning (ML) algorithms, alleviatingthe need for a human to perform manual algorithm design. Real-time AutoMLenables the design process to happen while the ML algorithm is being applied toa task. Real-time AutoML is an emerging research area, as such existingreal-time AutoML techniques need improvement with respect to the quality ofdesigns and time taken to create designs. To address these issues, this studyproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.Meta-learning gathers information about the optimisation process undertaken bythe ML algorithm in the form of meta-features. Meta-features are used inconjunction with a meta-learner to optimise the optimisation process. The OnMARapproach uses a meta-learner to predict the accuracy of an ML design. If theaccuracy predicted by the meta-learner is sufficient, the design is used, andif the predicted accuracy is low, an optimisation technique creates a newdesign. A genetic algorithm (GA) is the optimisation technique used as part ofthe OnMAR approach. Different meta-learners (k-nearest neighbours, randomforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. notspecific to a single real-time AutoML application) and therefore evaluated onthree different real-time AutoML applications, namely: composing an imageclustering algorithm, configuring the hyper-parameters of a convolutionalneural network, and configuring a video classification pipeline. The OnMARapproach is effective, matching or outperforming existing real-time AutoMLapproaches, with the added benefit of a faster runtime.</description>
      <author>example@mail.com (Mia Gerber, Anna Sergeevna Bosman, Johan Pieter de Villiers)</author>
      <guid isPermaLink="false">2502.20279v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2502.20316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的点云自监督学习方法NOMAE，通过只在非屏蔽体素的邻域内进行被遮罩占用重建来克服现有MAE在3D视觉中的挑战。&lt;h4&gt;背景&lt;/h4&gt;Masked autoencoders (MAE) 在图像等领域的自我监督学习中展现出巨大潜力。然而，在自动驾驶领域使用的LiDAR点云数据面临着特殊的挑战，因为大量空旷区域的存在导致了信息泄露和计算复杂度的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服现有的3D MAE的局限性，特别是针对大型、复杂的点云场景中的问题进行优化。&lt;h4&gt;方法&lt;/h4&gt;引入了基于体素屏蔽和层级生成技术的方法NOMAE。该方法在非屏蔽体素的邻域内执行被遮罩占用重建，并且可以在不同的尺度上集成多级体素屏蔽与占用重建，以捕捉不同大小的对象特征。&lt;h4&gt;主要发现&lt;/h4&gt;NOMAE可以灵活地应用于现有的3D架构中进行自监督学习，并且在nuScenes和Waymo Open数据集上的下游感知任务（如语义分割和3D目标检测）上显示出优越的性能，超过了现有方法的表现。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了NOMAE的有效性，它不仅解决了MAE在处理点云数据时遇到的问题，并且在多个基准测试中实现了当前最佳的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;被遮罩自编码器（MAE）已经在视觉等领域中的自我监督学习中展现出巨大潜力。然而，在自动驾驶领域使用的LiDAR采集到的3D点云数据特别具有挑战性，因为这些数据中有大量的空旷区域是空白的。因此，现有的工作在解码过程中泄露了占用信息，并且存在较大的计算复杂度，这使得自监督预训练仅限于2D鸟瞰视图编码器的实际应用中。本文提出了一种新颖的方法——邻域占用MAE（NOMAE），通过只在非屏蔽体素的邻域内进行被遮罩占用重建来克服上述挑战。我们利用所提出的层级掩码生成技术，在多个尺度上集成体素屏蔽与占用重建，以捕捉不同大小的对象特征。NOMAE具有极高的灵活性，并可以直接用于现有3D架构中的自监督学习。我们在nuScenes和Waymo Open数据集上的下游感知任务（如语义分割和3D目标检测）中进行了广泛的评估，与判别性和生成性自我监督方法进行比较。实验结果表明，NOMAE在多个基准测试的点云感知任务上设定了新的最佳性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked autoencoders (MAE) have shown tremendous potential for self-supervisedlearning (SSL) in vision and beyond. However, point clouds from LiDARs used inautomated driving are particularly challenging for MAEs since large areas ofthe 3D volume are empty. Consequently, existing work suffers from leakingoccupancy information into the decoder and has significant computationalcomplexity, thereby limiting the SSL pre-training to only 2D bird's eye viewencoders in practice. In this work, we propose the novel neighborhood occupancyMAE (NOMAE) that overcomes the aforementioned challenges by employing maskedoccupancy reconstruction only in the neighborhood of non-masked voxels. Weincorporate voxel masking and occupancy reconstruction at multiple scales withour proposed hierarchical mask generation technique to capture features ofobjects of different sizes in the point cloud. NOMAEs are extremely flexibleand can be directly employed for SSL in existing 3D architectures. We performextensive evaluations on the nuScenes and Waymo Open datasets for thedownstream perception tasks of semantic segmentation and 3D object detection,comparing with both discriminative and generative SSL methods. The resultsdemonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks formultiple point cloud perception tasks.</description>
      <author>example@mail.com (Mohamed Abdelsamad, Michael Ulrich, Claudius Gläser, Abhinav Valada)</author>
      <guid isPermaLink="false">2502.20316v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors</title>
      <link>http://arxiv.org/abs/2502.19896v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;现有的点云补全方法在处理现实世界扫描数据时遇到了挑战，因为它们依赖于预定义的合成训练数据集。为此，我们提出了一种名为GenPC的零样本完成框架，该框架利用明确的3D生成先验来重构高质量的真实世界扫描。&lt;h4&gt;背景&lt;/h4&gt;现有点云补全方法通常依赖于预先定义的合成训练数据集，在处理分布外的真实世界扫描时遇到困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的零样本点云补全框架，以克服现有方法在真实场景中的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发了Depth Prompting模块和Geometric Preserving Fusion模块，前者通过深度图像将部分点云与单视图生成模型关联起来；后者确保最终结果保留原始的不完整结构。&lt;h4&gt;主要发现&lt;/h4&gt;基于互联网规模数据训练的最新前馈3D生成模型能够从单一视角图像中进行零样本设置下的3D生成。GenPC框架在常用基准测试上的大量实验验证了其优越性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用现有的大规模3D生成模型，我们提出了一个高效的零样本点云补全解决方案，这使我们在稳健的真实世界扫描完成上更进一步。&lt;h4&gt;翻译&lt;/h4&gt;现有点云补全方法依赖于预先定义的合成训练数据集，在处理真实世界的分布外扫描时面临重大挑战。为了解决这一限制，我们提出了一种利用明确3D生成先验来重构高质量真实世界扫描的零样本补全框架GenPC。该框架的关键见解是基于近期前馈3D生成模型能够在仅使用单一视角图像的情况下从大规模互联网数据中进行零样本设置下的3D生成。为将此能力应用于补全任务，我们开发了一种Depth Prompting模块和一种Geometric Preserving Fusion模块。前者通过深度图作为中间步骤连接部分点云与单视图到三维的生成模型；后者则确保最终结果保留了原始输入中的不完整结构。大量实验验证了GenPC框架在常用基准测试上的优越性和泛化能力，使我们更接近于稳健的真实世界扫描完成目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing point cloud completion methods, which typically depend on predefinedsynthetic training datasets, encounter significant challenges when applied toout-of-distribution, real-world scans. To overcome this limitation, weintroduce a zero-shot completion framework, termed GenPC, designed toreconstruct high-quality real-world scans by leveraging explicit 3D generativepriors. Our key insight is that recent feed-forward 3D generative models,trained on extensive internet-scale data, have demonstrated the ability toperform 3D generation from single-view images in a zero-shot setting. Toharness this for completion, we first develop a Depth Prompting module thatlinks partial point clouds with image-to-3D generative models by leveragingdepth images as a stepping stone. To retain the original partial structure inthe final results, we design the Geometric Preserving Fusion module that alignsthe generated shape with input by adaptively adjusting its pose and scale.Extensive experiments on widely used benchmarks validate the superiority andgeneralizability of our approach, bringing us a step closer to robustreal-world scan completion.</description>
      <author>example@mail.com (An Li, Zhe Zhu, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.19896v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sanity Checking Causal Representation Learning on a Simple Real-World System</title>
      <link>http://arxiv.org/abs/2502.20099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文评估了因果表示学习（CRL）在简单现实世界系统中的效果，并通过光学实验验证方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;因果表示学习是近年来机器学习领域的热点问题，旨在从数据中提取出具有因果关系的特征。然而，现有理论与实际应用之间存在差距，需要进行更深入的研究以解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;评估现有的因果表示学习方法在现实场景中的表现，并分析其失败的原因，从而推动该领域的发展。&lt;h4&gt;方法&lt;/h4&gt;构建了一个特定为CRL设计的光学实验系统，其中包含已知的因果因素和核心假设。选择了几种典型的CRL方法进行测试，并通过替换数据生成过程为简化合成等效的方法来进一步理解算法的失效模式。&lt;h4&gt;主要发现&lt;/h4&gt;现有的大多数CRL方法在合成的简化数据集上表现不佳，这表明它们可能无法很好地应用于实际场景中。此外，论文还观察到一些常用的混合函数假设对某些方法的效果至关重要，但在实际情况中这些假设往往不成立。&lt;h4&gt;结论&lt;/h4&gt;尽管因果表示学习理论前景广阔，但要在实践中取得成功仍面临诸多挑战。该研究提供了一个简单的现实世界基准测试，为验证和进一步发展CRL方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;我们评估了几种因果表示学习（CRL）的方法在简单且基于真实世界的系统中的表现，这些方法在这种环境下应该有效。实验采用的是一个特别为了进行这项研究而构建的受控光学实验，在这个系统中满足了CRL的核心假设，并且了解底层因果因素（实验输入），提供了一个事实基础。我们选择了几种典型的CRL方法并发现它们都无法恢复出真正的因果因素。为了理解被评估算法的失败模式，我们在数据上进行了消融研究，通过用一个简单的合成等效过程替换真实的数据生成过程来进行分析。结果揭示了可重复性问题，即大多数方法在简化后的合成版本中已经表现不佳，尽管其数据生成过程简单。此外，我们观察到常见的混合函数假设对于某些方法的性能至关重要，但在实际数据中这些假设并不成立。我们的研究强调了现有理论与实践应用之间的差距，并希望此基准测试作为进一步发展和验证CRL方法的一个简单的现实世界基础性检查。我们在github.com/simonbing/CRLSanityCheck上公开所有代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We evaluate methods for causal representation learning (CRL) on a simple,real-world system where these methods are expected to work. The system consistsof a controlled optical experiment specifically built for this purpose, whichsatisfies the core assumptions of CRL and where the underlying causal factors(the inputs to the experiment) are known, providing a ground truth. We selectmethods representative of different approaches to CRL and find that they allfail to recover the underlying causal factors. To understand the failure modesof the evaluated algorithms, we perform an ablation on the data by substitutingthe real data-generating process with a simpler synthetic equivalent. Theresults reveal a reproducibility problem, as most methods already fail on thissynthetic ablation despite its simple data-generating process. Additionally, weobserve that common assumptions on the mixing function are crucial for theperformance of some of the methods but do not hold in the real data. Ourefforts highlight the contrast between the theoretical promise of the state ofthe art and the challenges in its application. We hope the benchmark serves asa simple, real-world sanity check to further develop and validate methodology,bridging the gap towards CRL methods that work in practice. We make all codeand datasets publicly available at github.com/simonbing/CRLSanityCheck</description>
      <author>example@mail.com (Juan L. Gamella, Simon Bing, Jakob Runge)</author>
      <guid isPermaLink="false">2502.20099v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Vector-Quantized Vision Foundation Models for Object-Centric Learning</title>
      <link>http://arxiv.org/abs/2502.20263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于对象中心学习（OCL）的方法VQ-VFM-OCL（简称VVO），该方法通过使用向量量化视觉基础模型来提取特征，并将这些特征进行量化以加强重建过程中的监督。&lt;h4&gt;背景&lt;/h4&gt;人类能够将视觉场景分解为不同的物体，这有助于理解物体之间的关系和动态。然而，基于自我监督的OCL在处理复杂纹理时面临挑战。为了改善这一点，许多方法采用视觉基础模型来提取更具有对象性的特征图。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用向量量化技术结合视觉基础模型的方法VQ-VFM-OCL（简称VVO），旨在改进现有OCL方法对复杂场景的理解能力，并促进下游任务的表现。&lt;h4&gt;方法&lt;/h4&gt;VVO通过从视觉基础模型中提取特征，以帮助对象级信息聚合；同时，通过对提取的特征进行量化来增强重建过程中的监督。此外，该工作统一了现有的OCL代表方法为一个简洁架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明VVO在物体识别任务上超越主流方法，并对下游如视觉预测和推理等任务有积极影响。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过结合向量量化技术与现有视觉基础模型，不仅提高了复杂场景下对象的识别能力，也为进一步研究提供了新的方向。源代码已公布于补充材料中。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decomposing visual scenes into objects, as humans do, facilitates modelingobject relations and dynamics. Object-Centric Learning (OCL) achieves this byaggregating image or video feature maps into object-level feature vectors,known as \textit{slots}. OCL's self-supervision via reconstructing the inputfrom slots struggles with complex textures, thus many methods employ VisionFoundation Models (VFMs) to extract feature maps with better objectness.However, using VFMs merely as feature extractors does not fully unlock theirpotential. We propose Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO), whereVFM features are extracted to facilitate object-level information aggregationand further quantized to strengthen supervision in reconstruction. Our VVOunifies OCL representatives into a concise architecture. Experimentsdemonstrate that VVO not only outperforms mainstream methods on objectdiscovery tasks but also benefits downstream tasks like visual prediction andreasoning. The source code is available in the supplement.</description>
      <author>example@mail.com (Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen)</author>
      <guid isPermaLink="false">2502.20263v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks</title>
      <link>http://arxiv.org/abs/2502.20237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;人工神经网络可以从数据中获取许多人类知识的方面，成为模拟人类学习模型的理想选择。但是这些网络可以学到什么取决于它们的归纳偏置——除了数据之外影响其解决方案的因素。&lt;h4&gt;背景&lt;/h4&gt;尽管人工神经网络在模仿人类学习方面显示出巨大潜力，但对它们的归纳偏置的理解仍然不足，这限制了我们从这些系统的表现中得出关于人类学习结论的能力。认知科学家和机器学习研究人员通常关注神经网络架构作为归纳偏置的一个来源。&lt;h4&gt;目的&lt;/h4&gt;本研究探索初始权重作为另一个可能影响归纳偏置的因素，并利用元学习技术寻找适应特定问题的初始权重，从而减少不同架构和数据表示之间的性能差异。&lt;h4&gt;方法&lt;/h4&gt;通过在三个需要不同类型偏置和概括形式的任务上进行430个不同模型的元训练实验，测试了四种广泛使用的架构：多层感知器（MLPs）、卷积神经网络（CNNs）、长短期记忆网络（LSTMs）以及变换器。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，通过使用元学习可以显著减少或完全消除各种体系结构和数据表示之间的性能差异。此外，在远离元训练经验的问题上，所有架构均表现出差的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这些发现表明，对于神经网络来说，初始权重可能是比通常认为更重要的归纳偏置来源。这强调了开发更强有力的归纳偏置以实现鲁棒泛化的必要性。&lt;h4&gt;翻译&lt;/h4&gt;人工神经网络可以从数据中获取人类知识的不同方面，显示出成为人类学习模型的巨大潜力。然而，这些网络可以学到什么取决于它们的先验假设——除了数据之外影响其解决方案的因素。尽管人工神经网络在模仿人类学习方面显示出巨大潜力，但对它们的归纳偏置的理解仍然不足，这限制了我们从这些系统的表现中得出关于人类学习结论的能力。认知科学家和机器学习研究人员通常关注神经网络架构作为归纳偏置的一个来源，在本文中我们探讨了一个其他的影响因素——初始权重，并使用元学习作为一种工具来寻找适应特定问题的初始权重。通过在三个需要不同类型偏置和概括形式的任务上进行430个不同模型的元训练实验，测试了四种广泛使用的架构：多层感知器（MLPs）、卷积神经网络（CNNs）、长短期记忆网络（LSTMs）以及变换器。研究发现，在特定问题上通过使用元学习可以显著减少或完全消除各种体系结构和数据表示之间的性能差异。此外，在远离元训练经验的问题上，所有架构均表现出差的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial neural networks can acquire many aspects of human knowledge fromdata, making them promising as models of human learning. But what thosenetworks can learn depends upon their inductive biases -- the factors otherthan the data that influence the solutions they discover -- and the inductivebiases of neural networks remain poorly understood, limiting our ability todraw conclusions about human learning from the performance of these systems.Cognitive scientists and machine learning researchers often focus on thearchitecture of a neural network as a source of inductive bias. In this paperwe explore the impact of another source of inductive bias -- the initialweights of the network -- using meta-learning as a tool for finding initialweights that are adapted for specific problems. We evaluate four widely-usedarchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430different models across three tasks requiring different biases and forms ofgeneralization. We find that meta-learning can substantially reduce or entirelyeliminate performance differences across architectures and datarepresentations, suggesting that these factors may be less important as sourcesof inductive bias than is typically assumed. When differences are present,architectures and data representations that perform well without meta-learningtend to meta-train more effectively. Moreover, all architectures generalizepoorly on problems that are far from their meta-training experience,underscoring the need for stronger inductive biases for robust generalization.</description>
      <author>example@mail.com (Gianluca Bencomo, Max Gupta, Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths)</author>
      <guid isPermaLink="false">2502.20237v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with Contrastive Training Strategy for Deepfake Speech Detection</title>
      <link>http://arxiv.org/abs/2502.20225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于低复杂度Depthwise-Inception Network (DIN)和对比训练策略(CTS)的深度神经网络方法，用于检测deepfake语音。&lt;h4&gt;背景&lt;/h4&gt;当前存在大量的伪造音频，这对社会和个人安全构成了威胁。需要一种有效的方法来区分真实语音和deepfake伪造语音。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地识别deepfake语音的技术方案。&lt;h4&gt;方法&lt;/h4&gt;通过短时傅里叶变换(STFT)和线性滤波器(LF)将输入音频转换为频谱图，然后使用这些频谱图训练DIN。利用该网络提取真实语音的特征向量，并构建高斯分布模型以表示真实语音；测试样本与该分布的距离被用来判断其真实性。&lt;h4&gt;主要发现&lt;/h4&gt;在ASVspoof 2019 LA基准数据集上的实验结果表明，结合Depthwise-Inception Network和对比学习策略能够有效区分伪造音频和真实语音。使用一个参数量仅为1.77M的低复杂度DIN，在4秒短音频段上实现了4.6%的等错误率(EER)、95.4%的准确率(Acc.)、97.3%的F1值以及98.9%的AUC分数。&lt;h4&gt;结论&lt;/h4&gt;该系统在ASVspoof 2019 LA挑战赛中的单系统提交中表现最好，显示出其应用于实时应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a deep neural network approach for deepfake speechdetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)trained with a contrastive training strategy (CTS). In this framework, inputaudio recordings are first transformed into spectrograms using Short-TimeFourier Transform (STFT) and Linear Filter (LF), which are then used to trainthe DIN. Once trained, the DIN processes bonafide utterances to extract audioembeddings, which are used to construct a Gaussian distribution representinggenuine speech. Deepfake detection is then performed by computing the distancebetween a test utterance and this distribution to determine whether theutterance is fake or bonafide. To evaluate our proposed systems, we conductedextensive experiments on the benchmark dataset of ASVspoof 2019 LA. Theexperimental results demonstrate the effectiveness of combining theDepthwise-Inception Network with the contrastive learning strategy indistinguishing between fake and bonafide utterances. We achieved Equal ErrorRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%respectively using a single, low-complexity DIN with just 1.77 M parameters and985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposedsystem outperforms the single-system submissions in the ASVspoof 2019 LAchallenge, showcasing its potential for real-time applications.</description>
      <author>example@mail.com (Lam Pham, Dat Tran, Florian Skopik, Alexander Schindler, Silvia Poletti, Fischinger David, Martin Boyer)</author>
      <guid isPermaLink="false">2502.20225v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization</title>
      <link>http://arxiv.org/abs/2502.20036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in 2025 International Conference on 3D Vision (3DV)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Angle-Annular Graph Neural Network (A2-GNN)的方法，用于直接进行2D和3D关键点匹配而无需使用视觉描述符。这种方法通过环状特征提取来高效学习鲁棒的几何结构表示，并在没有视觉描述符的情况下实现了目前最高的精度。&lt;h4&gt;背景&lt;/h4&gt;视觉定位涉及估计已知场景中6自由度（6-DoF）相机的姿态，其中识别2D查询图像与3D模型之间的像素到点对应关系是一个关键步骤。当前最先进的方法依赖于广泛的视觉描述符来建立这些对应关系，但面临存储、隐私问题和模型维护的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的方法，该方法能够克服现有无描述符方法中的低精度或重计算量的问题，并实现高效的2D-3D关键点匹配。&lt;h4&gt;方法&lt;/h4&gt;论文引入了Angle-Annular Graph Neural Network (A2-GNN)，这种网络通过环状特征提取来学习鲁棒的几何结构表示。它将邻域聚类并嵌入每个组的距离信息和角度作为补充信息，以捕捉局部结构。&lt;h4&gt;主要发现&lt;/h4&gt;在匹配和视觉定位数据集上的评估表明，该方法在无需视觉描述符的情况下达到了最先进的精度，并且计算开销低。&lt;h4&gt;结论&lt;/h4&gt;A2-GNN提供了一种有效的方法来解决直接进行2D-3D关键点匹配的挑战，克服了现有无描述符方法中的精度和计算量问题。此研究为未来的视觉定位工作奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：视觉定位涉及估计已知场景中6自由度（6-DoF）相机的姿态。其中的一个关键步骤是识别2D查询图像与3D模型之间的像素到点的对应关系。当前最先进的方法依赖于广泛的视觉描述符来建立这些对应关系，但是面临存储、隐私问题以及模型维护的问题挑战。无视觉描述符直接进行2D-3D关键点匹配的方法正在变得流行，因为这种方法可以克服这些问题。然而现有的无描述符方法面临着低精度或重计算量的挑战。为了解决这个问题，本文引入了一种名为Angle-Annular Graph Neural Network (A2-GNN) 的简单方法来高效学习具有鲁棒几何结构表示能力的网络，并通过环状特征提取补充距离信息和角度作为辅助信息以捕捉局部结构。在匹配和视觉定位数据集上的评估表明，我们的方法达到了无需描述符的情况下最高的精度且计算量较低。代码将在 https://github.com/YejunZhang/a2-gnn 发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization involves estimating the 6-degree-of-freedom (6-DoF)camera pose within a known scene. A critical step in this process isidentifying pixel-to-point correspondences between 2D query images and 3Dmodels. Most advanced approaches currently rely on extensive visual descriptorsto establish these correspondences, facing challenges in storage, privacyissues and model maintenance. Direct 2D-3D keypoint matching without visualdescriptors is becoming popular as it can overcome those challenges. However,existing descriptor-free methods suffer from low accuracy or heavy computation.Addressing this gap, this paper introduces the Angle-Annular Graph NeuralNetwork (A2-GNN), a simple approach that efficiently learns robust geometricstructural representations with annular feature extraction. Specifically, thisapproach clusters neighbors and embeds each group's distance information andangle as supplementary information to capture local structures. Evaluation onmatching and visual localization datasets demonstrates that our approachachieves state-of-the-art accuracy with low computational overhead among visualdescription-free methods. Our code will be released onhttps://github.com/YejunZhang/a2-gnn.</description>
      <author>example@mail.com (Yejun Zhang, Shuzhe Wang, Juho Kannala)</author>
      <guid isPermaLink="false">2502.20036v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance</title>
      <link>http://arxiv.org/abs/2502.19694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的扩散模型BEVDiffuser，用于减少鸟瞰图（BEV）表示中的噪声。该方法在nuScenes数据集上的实验表明，在不增加计算复杂度的情况下，显著提高了3D物体检测的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的BEV生成技术尽管有所进步，但仍然受到传感器限制和学习过程中的内在噪声影响，导致下游任务性能不佳。&lt;h4&gt;目的&lt;/h4&gt;通过使用地面真实对象布局作为引导来有效地减少BEV特征图中的噪声，从而提高现有BEV模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为BEVDiffuser的新扩散模型，在训练期间可以以插件方式增强现有的BEV模型而无需对架构进行任何修改。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明BEVDiffuser在nuScenes数据集上显著提高了3D物体检测的性能，mAP和NDS分别提高了12.3%和10.1%，且没有引入额外的计算复杂度。此外，在长尾物体检测以及恶劣天气和光照条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;BEVDiffuser能够有效提高现有BEV模型的准确性和鲁棒性，为自动驾驶任务提供更高质量的输入表示。&lt;h4&gt;翻译&lt;/h4&gt;鸟瞰图（BEV）表示在自主驾驶任务中起着关键作用。尽管最近在BEV生成方面取得了进展，但由传感器限制和学习过程产生的固有噪声仍然没有得到充分解决，导致次优的BEV表示，从而影响下游任务的表现。为了解决这个问题，我们提出了一种新的扩散模型BEVDiffuser，它使用地面真实对象布局作为指导来有效减少BEV特征图中的噪声。在nuScenes数据集上进行的广泛实验表明，BEVDiffuser具有出色的去噪和生成能力，在不增加额外计算复杂度的情况下显著提高了现有BEV模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bird's-eye-view (BEV) representations play a crucial role in autonomousdriving tasks. Despite recent advancements in BEV generation, inherent noise,stemming from sensor limitations and the learning process, remains largelyunaddressed, resulting in suboptimal BEV representations that adversely impactthe performance of downstream tasks. To address this, we propose BEVDiffuser, anovel diffusion model that effectively denoises BEV feature maps using theground-truth object layout as guidance. BEVDiffuser can be operated in aplug-and-play manner during training time to enhance existing BEV modelswithout requiring any architectural modifications. Extensive experiments on thechallenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoisingand generation capabilities, which enable significant enhancement to existingBEV models, as evidenced by notable improvements of 12.3\% in mAP and 10.1\% inNDS achieved for 3D object detection without introducing additionalcomputational complexity. Moreover, substantial improvements in long-tailobject detection and under challenging weather and lighting conditions furthervalidate BEVDiffuser's effectiveness in denoising and enhancing BEVrepresentations.</description>
      <author>example@mail.com (Xin Ye, Burhaneddin Yaman, Sheng Cheng, Feng Tao, Abhirup Mallik, Liu Ren)</author>
      <guid isPermaLink="false">2502.19694v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering</title>
      <link>http://arxiv.org/abs/2502.20224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures, 5 tables, submitted to The 28th International  Conference on Medical Image Computing and Computer Assisted Intervention  (MICCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Diabetic Macular Edema (DME) 是糖尿病患者常见的并发症，是视力损害和失明的主要原因之一。虽然深度学习在医学图像分析中取得了显著进展，但传统的 DME 诊断仍然依赖于大量标注数据和主观的眼科医生评估，限制了实际应用。&lt;h4&gt;背景&lt;/h4&gt;糖尿病黄斑水肿（DME）是一种常见且严重的糖尿病并发症，它会导致视力障碍甚至失明。尽管基于深度学习的方法在医学图像分析方面已经取得了一定的进展，但传统的 DME 诊断依旧依赖于大量的标记数据和眼科医生主观评估，这限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需大量标注数据的自动化的 DME 诊断系统 RURANET++。&lt;h4&gt;方法&lt;/h4&gt;RURANET++ 框架采用优化后的 U-Net 架构，并嵌入空间和通道挤压与激励（SCSE）注意机制，以增强病变特征提取。在特征处理阶段，首先使用预训练的 GoogLeNet 提取视网膜图像中的深度特征；然后利用基于主成分分析（PCA）的方法将特征维度减少至 50 维，提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;引入了一种新的聚类算法，采用多投影头来显式地控制集群多样性，并动态调整相似性阈值以优化内类一致性与外类区分度。实验结果表明，该系统在多个指标上表现出色，包括最大准确率（0.8411）、精确度（0.8593）、召回率（0.8411）和 F1 分数（0.8390），并具备优秀的聚类质量。&lt;h4&gt;结论&lt;/h4&gt;本研究提供了一种高效的无监督解决方案，用于 DME 的诊断，在临床应用方面具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病黄斑水肿（DME）是导致视力损害乃至失明的重要因素。尽管深度学习技术在医学图像分析领域取得了显著进展，但传统的 DME 诊断仍然依赖于大量的标注数据和眼科医生的主观判断，这限制了其实际应用范围。本研究提出了 RURANET++ 系统，它基于无监督学习方法进行自动化 DME 诊断，并通过优化 U-Net 架构、引入 SCSE 注意机制以及创新性地使用多投影头聚类算法等技术手段提高病变特征提取效率和准确性。实验结果表明该系统在多个评估指标上均表现出色，显示出其在临床应用中的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Macular Edema (DME), a prevalent complication among diabeticpatients, constitutes a major cause of visual impairment and blindness.Although deep learning has achieved remarkable progress in medical imageanalysis, traditional DME diagnosis still relies on extensive annotated dataand subjective ophthalmologist assessments, limiting practical applications. Toaddress this, we present RURANET++, an unsupervised learning-based automatedDME diagnostic system. This framework incorporates an optimized U-Netarchitecture with embedded Spatial and Channel Squeeze &amp; Excitation (SCSE)attention mechanisms to enhance lesion feature extraction. During featureprocessing, a pre-trained GoogLeNet model extracts deep features from retinalimages, followed by PCA-based dimensionality reduction to 50 dimensions forcomputational efficiency. Notably, we introduce a novel clustering algorithmemploying multi-projection heads to explicitly control cluster diversity whiledynamically adjusting similarity thresholds, thereby optimizing intra-classconsistency and inter-class discrimination. Experimental results demonstratesuperior performance across multiple metrics, achieving maximum accuracy(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), withexceptional clustering quality. This work provides an efficient unsupervisedsolution for DME diagnosis with significant clinical implications.</description>
      <author>example@mail.com (Wei Yang, Yiran Zhu, Jiayu Shen, Yuhan Tang, Chengchang Pan, Hui He, Yan Su, Honggang Qi)</author>
      <guid isPermaLink="false">2502.20224v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption</title>
      <link>http://arxiv.org/abs/2502.19741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2405.03342&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在网络环境下估计因果效应的问题，提出了一种基于可识别表示学习技术的网络效应估计器，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;在存在隐蔽共变量的情况下，现有的基于观察数据的方法通常无法有效估计网络影响。然而，在这种情况下，单位之间的交互可以提供重要的信息来恢复这些隐藏的共变量。&lt;h4&gt;目的&lt;/h4&gt;识别三种类型在网络推断中阻碍识别的潜在混淆因子，并通过利用可识别表示学习技术，提出一种新的网络效应估计算法。&lt;h4&gt;方法&lt;/h4&gt;提出了基于可识别表示学习技术的网络效应估计器。理论上确立了所有潜在混淆因子的可识别性，通过应用已确定的潜在混淆因子提供网络效应的识别结果。&lt;h4&gt;主要发现&lt;/h4&gt;三种类型的隐藏共变量影响个体、邻居或同时影响两者，并且这些因素阻碍了对网络效应的有效识别和估计。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅理论上证明了其在处理网络干扰下因果效应估计问题中的有效性，而且实验也验证了这一理论。通过这种方法，可以更准确地评估在网络环境中交互作用的影响。&lt;h4&gt;翻译&lt;/h4&gt;在存在网络干扰的情况下估计因果效应是一个关键且具有挑战性的问题。现有的基于观察数据的方法主要依赖于网络无偏性假设来保证网络效应的识别。然而，在实际情况下这种假设往往由于隐藏共变量的存在而被违反，这阻碍了对网络效应的有效识别。有趣的是，在这样的网络环境中，单位之间的交互提供了恢复隐藏共变量的重要信息。本文确定了在三个影响个体、仅影响邻居和同时影响两者的潜在混淆因子，并基于此提出了一个新的方法来估计网络效应并证明其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating causal effects under networked interference is a crucial yetchallenging problem. Existing methods based on observational data mainly relyon the networked unconfoundedness assumption, which guarantees theidentification of networked effects. However, the networked unconfoundednessassumption is usually violated due to the latent confounders in observationaldata, hindering the identification of networked effects. Interestingly, in suchnetworked settings, interactions between units provide valuable information forrecovering latent confounders. In this paper, we identify three types of latentconfounders in networked inference that hinder identification: those affectingonly the individual, those affecting only neighbors, and those influencingboth. Specifically, we devise a networked effect estimator based onidentifiable representation learning techniques. Theoretically, we establishthe identifiability of all latent confounders, and leveraging the identifiedlatent confounders, we provide the networked effect identification result.Extensive experiments validate our theoretical results and demonstrate theeffectiveness of the proposed method.</description>
      <author>example@mail.com (Weilin Chen, Ruichu Cai, Jie Qiao, Yuguang Yan, José Miguel Hernández-Lobato)</author>
      <guid isPermaLink="false">2502.19741v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras</title>
      <link>http://arxiv.org/abs/2502.19630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了在自动驾驶系统中的3D物体检测任务中使用异步事件相机的方法，解决了传统固定帧率传感器如LiDAR和摄像头带来的延迟和带宽限制问题。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态方法虽然取得了良好的性能，但未能满足自动驾驶系统对于低延时、高效率的需求。传统的LiDAR和摄像头存在固有的时间延迟及带宽限制。&lt;h4&gt;目的&lt;/h4&gt;引入异步事件相机以提高3D物体检测的速度和准确性，并建立基于事件的数据集作为评估标准。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基于事件的3D物体检测框架，利用事件相机的高时间分辨率和低带宽特性，在不同帧之间进行高效准确的3D物体检测。同时发布了一个新数据集DSEC-3DOD来支持这项研究。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入异步事件信息可以实现更快速、更低延迟的3D物体检测，特别是在常规传感器无法提供同步信息的时间间隔内也能完成任务。&lt;h4&gt;结论&lt;/h4&gt;文章证明了使用异步事件相机进行3D物体检测具有巨大的潜力，并为该领域的进一步研究提供了必要的资源。代码和数据集可在GitHub上获得。&lt;h4&gt;翻译&lt;/h4&gt;在点云中检测三维物体对于自动驾驶系统至关重要。最近，结合摄像信息的高级多模式方法取得了显著性能。为了实现安全有效的自主驾驶系统，不仅要准确还需要快速且延迟低的算法是必不可少的。然而现有的算法由于固定帧率传感器如LiDAR和摄像头的时间延迟及带宽限制未能达到这些要求。为了解决这个问题，首次将异步事件相机引入3D物体检测中。我们利用它们的高时间分辨率和低带宽特性来实现高速3D物体检测。通过在不同帧之间使用事件相机检索之前的3D信息，我们的方法甚至可以在无同步数据的时间间隔内进行检测。此外，提出了第一个基于事件的3D物体检测数据集DSEC-3DOD，该数据集中包含了每秒100帧的真实3D边界框，为基于事件的3D检测器建立了基准。代码和数据集可在https://github.com/mickeykang16/Ev3DOD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting 3D objects in point clouds plays a crucial role in autonomousdriving systems. Recently, advanced multi-modal methods incorporating camerainformation have achieved notable performance. For a safe and effectiveautonomous driving system, algorithms that excel not only in accuracy but alsoin speed and low latency are essential. However, existing algorithms fail tomeet these requirements due to the latency and bandwidth limitations of fixedframe rate sensors, e.g., LiDAR and camera. To address this limitation, weintroduce asynchronous event cameras into 3D object detection for the firsttime. We leverage their high temporal resolution and low bandwidth to enablehigh-speed 3D object detection. Our method enables detection even duringinter-frame intervals when synchronized data is unavailable, by retrievingprevious 3D information through the event camera. Furthermore, we introduce thefirst event-based 3D object detection dataset, DSEC-3DOD, which includesground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark forevent-based 3D detectors. The code and dataset are available athttps://github.com/mickeykang16/Ev3DOD.</description>
      <author>example@mail.com (Hoonhee Cho, Jae-young Kang, Youngho Kim, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2502.19630v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Deep Convolutional Neural Networks for Palm Fruit Maturity Classification</title>
      <link>http://arxiv.org/abs/2502.20223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究开发了一种基于深度卷积神经网络（CNN）的自动化视觉系统，用于准确分类油棕果图片的五个成熟度等级。&lt;h4&gt;背景&lt;/h4&gt;为了最大化油棕产量和质量，必须在最佳成熟期收获油棕果实。现有的方法依赖于人工评估，这可能导致效率低下和错误判断。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动分类油棕果图像成熟程度的计算机视觉系统，以便优化收割决策并提高油棕生产效率。&lt;h4&gt;方法&lt;/h4&gt;使用深度CNN对基于其成熟阶段的油棕果图片进行分类。浅层CNN作为基准模型，而迁移学习和微调则应用到预训练好的ResNet50和InceptionV3架构上。&lt;h4&gt;主要发现&lt;/h4&gt;该研究利用包含超过8,000张带有显著变化的照片的数据集，实现了超过85%的测试准确率。深度CNN模型在分类油棕果成熟阶段方面表现出了巨大的潜力。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了深度学习在自动评估油棕果实熟度方面的潜力，这可以有助于优化收割决策并提高油棕生产效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文描述了如何利用计算机视觉和深度学习技术来更准确地判断油棕果的成熟程度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maximize palm oil yield and quality, it is essential to harvest palm fruitat the optimal maturity stage. This project aims to develop an automatedcomputer vision system capable of accurately classifying palm fruit images intofive ripeness levels. We employ deep Convolutional Neural Networks (CNNs) toclassify palm fruit images based on their maturity stage. A shallow CNN servesas the baseline model, while transfer learning and fine-tuning are applied topre-trained ResNet50 and InceptionV3 architectures. The study utilizes apublicly available dataset of over 8,000 images with significant variations,which is split into 80\% for training and 20\% for testing. The proposed deepCNN models achieve test accuracies exceeding 85\% in classifying palm fruitmaturity stages. This research highlights the potential of deep learning forautomating palm fruit ripeness assessment, which can contribute to optimizingharvesting decisions and improving palm oil production efficiency.</description>
      <author>example@mail.com (Mingqiang Han, Chunlin Yi)</author>
      <guid isPermaLink="false">2502.20223v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Generalize without Bias for Open-Vocabulary Action Recognition</title>
      <link>http://arxiv.org/abs/2502.20158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于开放词汇动作识别的新颖元优化框架Open-MeDe，该框架通过静态去偏改善了模型在新环境下（包括上下文中和上下文外）的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;最近的视频学习者利用CLIP初始化以提高开放词汇的动作识别中的泛化性。然而，由于CLIP自身的静态偏差问题，这些视频学习者倾向于过度拟合于快捷静态特征，从而导致对新动作的泛化性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的元优化框架Open-MeDe来解决视频学习者的过度拟合问题，并改善其在开放词汇动作识别中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入了跨批次元优化方案，该方案通过虚拟评估鼓励视频学习者快速泛化到任意后续数据上。此外，采用自集成策略获取可实现上下文内外新数据稳健泛化的最优参数。&lt;h4&gt;主要发现&lt;/h4&gt;Open-MeDe在没有CLIP正则化的情况下进行优化，隐式地减轻了视频元学习者的固有静态偏差，并且通过元学习方法改进了从已知到开放词汇的泛化能力以及图像到视频去偏能力。实验结果表明，Open-MeDe不仅超越了针对上下文中开放词汇动作识别定制的最佳正则化方法，在上下文外场景中也表现出色。&lt;h4&gt;结论&lt;/h4&gt;Open-MeDE通过元学习和自集成策略有效解决了当前视频模型在开放词汇动作识别中的泛化性问题，特别是对于新环境下的泛化能力有了显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging the effective visual-text alignment and static generalizabilityfrom CLIP, recent video learners adopt CLIP initialization with furtherregularization or recombination for generalization in open-vocabulary actionrecognition in-context. However, due to the static bias of CLIP, such videolearners tend to overfit on shortcut static features, thereby compromisingtheir generalizability, especially to novel out-of-context actions. To addressthis issue, we introduce Open-MeDe, a novel Meta-optimization framework withstatic Debiasing for Open-vocabulary action recognition. From a freshperspective of generalization, Open-MeDe adopts a meta-learning approach toimprove known-to-open generalizing and image-to-video debiasing in acost-effective manner. Specifically, Open-MeDe introduces a cross-batchmeta-optimization scheme that explicitly encourages video learners to quicklygeneralize to arbitrary subsequent data via virtual evaluation, steering asmoother optimization landscape. In effect, the free of CLIP regularizationduring optimization implicitly mitigates the inherent static bias of the videometa-learner. We further apply self-ensemble over the optimization trajectoryto obtain generic optimal parameters that can achieve robust generalization toboth in-context and out-of-context novel data. Extensive evaluations show thatOpen-MeDe not only surpasses state-of-the-art regularization methods tailoredfor in-context open-vocabulary action recognition but also substantially excelsin out-of-context scenarios.</description>
      <author>example@mail.com (Yating Yu, Congqi Cao, Yifan Zhang, Yanning Zhang)</author>
      <guid isPermaLink="false">2502.20158v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Do computer vision foundation models learn the low-level characteristics of the human visual system?</title>
      <link>http://arxiv.org/abs/2502.20256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了基于自然图像训练的计算机视觉基础模型是否模仿人类视觉系统的基本特性，如对比度检测、对比掩蔽和对比恒常性。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉基础模型（例如DINO或OpenCLIP）通过在大型图像数据集上进行自监督学习来训练。有大量证据表明，人类视觉系统受自然世界中颜色和图案统计分布的影响，这些特征也存在于基础模型的训练数据中。&lt;h4&gt;目的&lt;/h4&gt;评估45种基础和生成模型的图像编码器是否模拟了人类视觉系统的特性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个包含九个测试类型的协议来评估不同模型在对比度检测、对比掩蔽等方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;一些基础模型（例如DINO，DINOv2和OpenCLIP）具有与人类视觉相似的特性，但其他模型则表现出较少或无相似性。基础模型对低对比度的敏感性较低，并且在不同频率下的对比度响应较为不规则。&lt;h4&gt;结论&lt;/h4&gt;尽管存在差异，但在基于视觉任务训练的基础模型中，已经开始出现低级人类视觉特征的一致性趋势，尤其是DINOv2模型最为接近。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉基础模型通常通过自监督方式使用大规模图像数据集进行训练。类似于这些系统的是，大量证据表明，人类视觉系统受自然世界中颜色和图案统计分布的影响，这与基础模型训练数据中的特性相似。本研究探讨了基于自然图像训练的基础模型是否模仿了一些低级的人类视觉特性，例如对比度检测、对比掩蔽和对比恒常性。具体而言，设计了一种包含九个测试类型的协议来评估45种基础和生成模型的性能。结果表明，某些基础模型（如DINO、DINOv2和OpenCLIP）分享了一些人类视觉的特性，而其他模型则表现出较少或无相似之处。总体而言，虽然仍有差异存在，但基于视觉任务训练的基础模型开始显示出低级人类视觉特征的一致性趋势，特别是DINOv2模型最为接近。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer vision foundation models, such as DINO or OpenCLIP, are trained in aself-supervised manner on large image datasets. Analogously, substantialevidence suggests that the human visual system (HVS) is influenced by thestatistical distribution of colors and patterns in the natural world,characteristics also present in the training data of foundation models. Thequestion we address in this paper is whether foundation models trained onnatural images mimic some of the low-level characteristics of the human visualsystem, such as contrast detection, contrast masking, and contrast constancy.Specifically, we designed a protocol comprising nine test types to evaluate theimage encoders of 45 foundation and generative models. Our results indicatethat some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some ofthe characteristics of human vision, but other models show little resemblance.Foundation models tend to show smaller sensitivity to low contrast and ratherirregular responses to contrast across frequencies. The foundation models showthe best agreement with human data in terms of contrast masking. Our findingssuggest that human vision and computer vision may take both similar anddifferent paths when learning to interpret images of the real world. Overall,while differences remain, foundation models trained on vision tasks start toalign with low-level human vision, with DINOv2 showing the closest resemblance.</description>
      <author>example@mail.com (Yancheng Cai, Fei Yin, Dounia Hammou, Rafal Mantiuk)</author>
      <guid isPermaLink="false">2502.20256v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive H&amp;E-IHC information fusion staining framework based on feature extra</title>
      <link>http://arxiv.org/abs/2502.20156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;免疫组化（IHC）染色在乳腺癌等疾病的评估中发挥着重要作用。基于生成模型的H&amp;E到IHC转换提供了一种简单且成本效益高的方法来获取IHC图像。&lt;h4&gt;背景&lt;/h4&gt;尽管之前的模型能够很好地进行数字上色，但它们仍然面临着两个挑战：(i) 仅通过HE图像中的像素特征来进行上色，容易导致染色过程中信息丢失；(ii) 缺乏像素级别的H&amp;E-IHC真实对给经典的L1损失带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述挑战，我们提出了一种基于特征提取器的自适应信息增强着色框架。&lt;h4&gt;方法&lt;/h4&gt;首先提出了VMFE模块，通过多尺度特征提取和小波变换卷积有效提取颜色信息特征，并结合共享解码器进行特征融合。高性能的H&amp;E-IHC双特征提取器通过对比学习训练，可以在高纬度空间内有效地执行HE-IHC特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;此外，在染色过程中使用经过训练的功能编码器来增强功能并自适应调整损失，解决了与模糊和不对称信息相关的问题。我们在不同的数据集上进行了测试，并取得了卓越的性能。&lt;h4&gt;结论&lt;/h4&gt;我们的代码可在https://github.com/babyinsunshine/CEFF获取&lt;h4&gt;翻译&lt;/h4&gt;免疫组化（IHC）染色在疾病评估中扮演重要角色，特别是在乳腺癌等疾病的诊断与研究。通过生成模型将H&amp;E图像转换为IHC图像的方法提供了一种成本效益高且简单的途径来获得IHC图像。尽管先前的模型能够很好地模拟数字着色过程，但它们仍存在两个主要问题：第一，仅使用HE图中不突出的像素特征进行染色会导致信息丢失；第二，缺乏准确的H&amp;E-IHC配对数据使得传统的L1损失难以有效应用。为了解决这些问题，我们提出了一种基于自适应增强与特征提取器的着色框架，该框架利用VMFE模块在多尺度上提取颜色信息，并通过对比学习训练提高特征匹配效果。实验结果显示了这一方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immunohistochemistry (IHC) staining plays a significant role in theevaluation of diseases such as breast cancer. The H&amp;E-to-IHC transformationbased on generative models provides a simple and cost-effective method forobtaining IHC images. Although previous models can perform digital coloringwell, they still suffer from (i) coloring only through the pixel features thatare not prominent in HE, which is easy to cause information loss in thecoloring process; (ii) The lack of pixel-perfect H&amp;E-IHC groundtruth pairsposes a challenge to the classical L1 loss.To address the above challenges, wepropose an adaptive information enhanced coloring framework based on featureextractors. We first propose the VMFE module to effectively extract the colorinformation features using multi-scale feature extraction and wavelet transformconvolution, while combining the shared decoder for feature fusion. Thehigh-performance dual feature extractor of H&amp;E-IHC is trained by contrastivelearning, which can effectively perform feature alignment of HE-IHC in highlatitude space. At the same time, the trained feature encoder is used toenhance the features and adaptively adjust the loss in the HE section stainingprocess to solve the problems related to unclear and asymmetric information. Wehave tested on different datasets and achieved excellent performance.Our codeis available at https://github.com/babyinsunshine/CEFF</description>
      <author>example@mail.com (Yifan Jia, Xingda Yu, Zhengyang Ji, Songning Lai, Yutao Yue)</author>
      <guid isPermaLink="false">2502.20156v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Your contrastive learning problem is secretly a distribution alignment problem</title>
      <link>http://arxiv.org/abs/2502.20141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, NeurIPS 2024 submission, includes supplementary  material&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;尽管对比学习在视觉和语言领域的成功，其理论基础以及构建表示的机制仍不甚明了。这项工作探讨了广泛用于对比学习中的噪声对比估计损失与基于熵最优传输（OT）的分布对齐之间的联系，并由此开发了一系列新的损失函数及其多步迭代变体。&lt;h4&gt;背景&lt;/h4&gt;尽管对比学习在视觉和语言领域取得了成功，其理论基础及构建表示的方法仍然不清楚。传统的对比学习通常使用特定类型的噪声对比估计损失来实现。&lt;h4&gt;目的&lt;/h4&gt;通过建立噪声对比估计损失与基于熵最优传输的分布对齐之间的联系，发展新型对比学习方法，增强模型对于数据集中的噪音视图处理能力，并允许自定义表示空间以适应不同约束条件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用更多潜在分布信息的方法来改进对比学习，通过这种方法可以更有效地调整扩充样本集合内的关系。此外，该研究还提供了理论洞见和实验证据证明了新方法在广义对比对齐中的优势。&lt;h4&gt;主要发现&lt;/h4&gt;通过将对比学习重新定义为一个分布对齐问题，并利用最优传输的优化工具，这项工作不仅揭示了不同自监督模型之间的新连接，而且提供了一套可以更轻松地融入领域知识的新工具。借助于这些框架和工具，能够构建不平衡损失以处理噪音视图。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，对比学习可以通过与基于熵最优传输的分布对齐相结合的方式得到新的理论洞见，并且提供了增强模型性能及适应性的新方法。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比学习在视觉和语言任务中的成功令人瞩目，但对于其背后的理论基础以及如何构建有效表示的理解仍然有限。本文尝试建立噪声对比估计损失与基于熵最优传输的分布对齐之间的桥梁，开发出新的损失函数家族，并为现存的对比学习方法提供迭代变体方案。通过引入更多潜在分布的信息，研究者提出了一种更加‘感知’到数据集内部关系的方法，从而改进模型在处理噪音视图时的表现。实验结果显示了这种方法在广义对比对齐上的显著优势。此外，该研究还展示了如何重新解读对比学习为一个对齐问题，并利用现有的最优传输优化工具来揭示不同自监督学习模型之间的新连接及提供易于适应领域知识的新手段。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the success of contrastive learning (CL) in vision and language, itstheoretical foundations and mechanisms for building representations remainpoorly understood. In this work, we build connections between noise contrastiveestimation losses widely used in CL and distribution alignment with entropicoptimal transport (OT). This connection allows us to develop a family ofdifferent losses and multistep iterative variants for existing CL methods.Intuitively, by using more information from the distribution of latents, ourapproach allows a more distribution-aware manipulation of the relationshipswithin augmented sample sets. We provide theoretical insights and experimentalevidence demonstrating the benefits of our approach for {\em generalizedcontrastive alignment}. Through this framework, it is possible to leveragetools in OT to build unbalanced losses to handle noisy views and customize therepresentation space by changing the constraints on alignment. By reframingcontrastive learning as an alignment problem and leveraging existingoptimization tools for OT, our work provides new insights and connectionsbetween different self-supervised learning models in addition to new tools thatcan be more easily adapted to incorporate domain knowledge into learning.</description>
      <author>example@mail.com (Zihao Chen, Chi-Heng Lin, Ran Liu, Jingyun Xiao, Eva L Dyer)</author>
      <guid isPermaLink="false">2502.20141v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>4Deform: Neural Surface Deformation for Robust Shape Interpolation</title>
      <link>http://arxiv.org/abs/2502.20208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法4Deform，用于生成非刚性变形形状之间的真实中间形态。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉领域，生成非刚体变形物体的现实过渡形态是一个具有挑战性的任务，尤其是在没有结构的数据（如点云）中进行这种操作时尤为困难。现有的大多数插值方法都是针对有结构的数据设计的（例如网格），对于实际世界的点云并不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于无结构数据的方法4Deform，该方法可以实现自由拓扑变化的形状变形，并且不依赖于中间形态监督。&lt;h4&gt;方法&lt;/h4&gt;新方法采用神经隐式表示(NIR)来处理连续欧几里得空间中的速度场学习问题。通过物理和几何约束来正则化这种速度场，并使用修改后的水平集方程重新构建过渡表面，直接将NIR与速度场连接起来。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在各种场景（包括噪声、不完整拓扑变化等）下显著优于之前的神经隐式表示方法。此外，这种方法首次实现了新的应用，如4D Kinect序列上采样和真实世界的高分辨率网格变形。&lt;h4&gt;结论&lt;/h4&gt;通过创新的方法论和技术实现，在非结构化数据中的形状变形任务中取得重要突破，为计算机视觉领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;生成现实的中间形态是一个挑战性的任务，特别是在处理无结构的数据时（例如点云），而这些数据在帧间缺乏时间一致性并且拓扑会发生变化。大多数插值方法是为有结构的数据设计的（即网格）, 并不适用于真实世界的点云。相比之下, 我们的方法4Deform采用神经隐式表示(NIR)来实现自由拓扑变化下的形状变形。与以往基于网格的方法不同，我们的方法学习的是欧几里得空间中的连续速度场。因此，它更适合处理较少结构化的数据，如点云。此外，我们的方法在训练过程中不需要中间形态的监督; 相反, 我们采用物理和几何约束来正则化速度场。我们使用修改过的水平集方程重构过渡表面, 将NIR直接与速度场连接起来。实验表明, 我们的这种方法显著优于先前的神经隐式表示方法，涵盖各种场景（例如噪声、部分数据、拓扑变化及非等距形状）。此外，它首次实现了新的应用如4D Kinect序列上采样和真实世界的高分辨率网格变形。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic intermediate shapes between non-rigidly deformed shapesis a challenging task in computer vision, especially with unstructured data(e.g., point clouds) where temporal consistency across frames is lacking, andtopologies are changing. Most interpolation methods are designed for structureddata (i.e., meshes) and do not apply to real-world point clouds. In contrast,our approach, 4Deform, leverages neural implicit representation (NIR) to enablefree topology changing shape deformation. Unlike previous mesh-based methodsthat learn vertex-based deformation fields, our method learns a continuousvelocity field in Euclidean space. Thus, it is suitable for less structureddata such as point clouds. Additionally, our method does not requireintermediate-shape supervision during training; instead, we incorporatephysical and geometrical constraints to regularize the velocity field. Wereconstruct intermediate surfaces using a modified level-set equation, directlylinking our NIR with the velocity field. Experiments show that our methodsignificantly outperforms previous NIR approaches across various scenarios(e.g., noisy, partial, topology-changing, non-isometric shapes) and, for thefirst time, enables new applications like 4D Kinect sequence upsampling andreal-world high-resolution mesh deformation.</description>
      <author>example@mail.com (Lu Sang, Zehranaz Canfes, Dongliang Cao, Riccardo Marin, Florian Bernard, Daniel Cremers)</author>
      <guid isPermaLink="false">2502.20208v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>WaveGAS: Waveform Relaxation for Scaling Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.19986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了两项改进，以解决GNNAutoScale（GAS）在图神经网络训练过程中遇到的历史嵌入向量陈旧和累积误差的问题。&lt;h4&gt;背景&lt;/h4&gt;随着现实世界图数据规模的不断扩大，为克服资源限制而开发了多种方法来训练图神经网络(GNNs)。其中一种方法是GNNAutoScale (GAS)，它通过图划分允许在有限GPU内存下进行训练，并且保存历史嵌入向量。&lt;h4&gt;目的&lt;/h4&gt;提出改进方案以解决由于使用陈旧的历史嵌入向量导致的近似误差累积问题，提升节点嵌入的质量和准确性。&lt;h4&gt;方法&lt;/h4&gt;[{'WaveGAS': '受波形松弛算法启发，在GAS内部进行多次前向传播来细化历史嵌入向量和梯度的估计，从而提高训练精度'}, {'梯度追踪法': '保存并利用更准确的历史梯度以提升模型在训练过程中的表现。'}]&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，WaveGAS能够增强原始的GAS方法，并且在性能上优于直接在完整图上进行训练的方法。&lt;h4&gt;结论&lt;/h4&gt;通过提出WaveGAS和改进的梯度追踪技术，该研究显著提高了节点嵌入的质量，并展示了其在复杂大规模图数据集上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the ever-growing size of real-world graphs, numerous techniques toovercome resource limitations when training Graph Neural Networks (GNNs) havebeen developed. One such approach, GNNAutoScale (GAS), uses graph partitioningto enable training under constrained GPU memory. GAS also stores historicalembedding vectors, which are retrieved from one-hop neighbors in otherpartitions, ensuring critical information is captured across partitionboundaries. The historical embeddings which come from the previous trainingiteration are stale compared to the GAS estimated embeddings, resulting inapproximation errors of the training algorithm. Furthermore, these errorsaccumulate over multiple layers, leading to suboptimal node embeddings. Toaddress this shortcoming, we propose two enhancements: first, WaveGAS, inspiredby waveform relaxation, performs multiple forward passes within GAS before thebackward pass, refining the approximation of historical embeddings andgradients to improve accuracy; second, a gradient-tracking method that storesand utilizes more accurate historical gradients during training. Empiricalresults show that WaveGAS enhances GAS and achieves better accuracy, evenoutperforming methods that train on full graphs, thanks to its robustestimation of node embeddings.</description>
      <author>example@mail.com (Jana Vatter, Mykhaylo Zayats, Marcos Martínez Galindo, Vanessa López, Ruben Mayer, Hans-Arno Jacobsen, Hoang Thanh Lam)</author>
      <guid isPermaLink="false">2502.19986v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation</title>
      <link>http://arxiv.org/abs/2502.20056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了利用多视图纵向数据的增强对比学习方法，用于胸部X光报告生成（MLRG），该方法结合了当前多视图图像的空间信息和纵向数据的时间信息，并采用放射学报告中固有的时空信息对视觉和文本表示进行预训练。&lt;h4&gt;背景&lt;/h4&gt;自动化的放射科报告生成可以有效减轻放射科医生的工作负担，但大多数现有方法主要关注单视图或固定视角的图像来建模当前疾病状况，这限制了诊断准确性并忽略了疾病的进展过程。虽然有些方法利用纵向数据追踪疾病进展，但仍依赖于单一图像进行当前访问分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习框架，旨在通过结合多视图和纵向数据提高胸部X光报告生成的准确性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;引入了多视图纵向对比学习的方法，该方法整合了空间信息（来自当前的多个视角图像）与时间信息（从纵向数据中获取）。同时利用放射学报告中的固有时空信息来监督视觉和文本表示的学习。此外还提出了一种标记化缺失编码技术，以灵活处理特定患者的前期知识缺失。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-CXR、MIMIC-ABN以及双视图CXR数据集上的实验表明，该方法优于最新的最先进的方法，在MIMIC-CXR上实现了BLEU-4的2.3%改进，在MIMIC-ABN上实现了F1分数5.5%的提高，在Two-view CXR上实现了F1 RadGraph 2.7%的进步。&lt;h4&gt;结论&lt;/h4&gt;通过利用多视图纵向数据及其内在时空信息，可以显著提升胸部X光报告生成的质量和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated radiology report generation offers an effective solution toalleviate radiologists' workload. However, most existing methods focusprimarily on single or fixed-view images to model current disease conditions,which limits diagnostic accuracy and overlooks disease progression. Althoughsome approaches utilize longitudinal data to track disease progression, theystill rely on single images to analyze current visits. To address these issues,we propose enhanced contrastive learning with Multi-view Longitudinal data tofacilitate chest X-ray Report Generation, named MLRG. Specifically, weintroduce a multi-view longitudinal contrastive learning method that integratesspatial information from current multi-view images and temporal informationfrom longitudinal data. This method also utilizes the inherent spatiotemporalinformation of radiology reports to supervise the pre-training of visual andtextual representations. Subsequently, we present a tokenized absence encodingtechnique to flexibly handle missing patient-specific prior knowledge, allowingthe model to produce more accurate radiology reports based on available priorknowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXRdatasets demonstrate that our MLRG outperforms recent state-of-the-art methods,achieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvementon MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.</description>
      <author>example@mail.com (Kang Liu, Zhuoqi Ma, Xiaolu Kang, Yunan Li, Kun Xie, Zhicheng Jiao, Qiguang Miao)</author>
      <guid isPermaLink="false">2502.20056v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Quantum generative classification with mixed states</title>
      <link>http://arxiv.org/abs/2502.19970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'分类方法': '可以使用判别式或生成式的机器学习方法进行分类。', '判别式学习': '构造给定输入的输出条件概率。', '生成式学习': '构造输入和输出联合概率密度。', '生成式学习优势': '应用于无监督学习、统计推断、不确定性估计和合成数据生成。', '提出模型': '一种名为量子生成分类（QGC）的量子多类分类策略。', '方法细节': '使用变分量子算法通过混合量子态估算特征和标签的数据集联合概率密度函数。', '创新点': '引入一个称为量子增强傅里叶特征（QEFF）的量子映射，利用量子叠加来用少量量子比特在硬件中准备高维数据样本。', '理论贡献': '展示量子生成分类算法可以看作是训练数据核希尔伯特空间的一个高斯混合。', '实验验证': '开发了一种用于高维数据集生成式分类的混合量子-经典神经网络，该方法已在包括10类MNIST和Fashion-MNIST数据集在内的多个低维和高维数据集上进行了测试。', '结果表现': '证明了生成式分类策略在与其他先前量子模型的竞争中具有竞争力。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification can be performed using either a discriminative or a generativelearning approach. Discriminative learning consists of constructing theconditional probability of the outputs given the inputs, while generativelearning consists of constructing the joint probability density of the inputsand outputs. Although most classical and quantum methods are discriminative,there are some advantages of the generative learning approach. For instance, itcan be applied to unsupervised learning, statistical inference, uncertaintyestimation, and synthetic data generation. In this article, we present aquantum generative multiclass classification strategy, called quantumgenerative classification (QGC). This model uses a variational quantumalgorithm to estimate the joint probability density function of features andlabels of a data set by means of a mixed quantum state. We also introduce aquantum map called quantum-enhanced Fourier features (QEFF), which leveragesquantum superposition to prepare high-dimensional data samples in quantumhardware using a small number of qubits. We show that the quantum generativeclassification algorithm can be viewed as a Gaussian mixture that reproduces akernel Hilbert space of the training data. In addition, we developed a hybridquantum-classical neural network that shows that it is possible to performgenerative classification on high-dimensional data sets. The method was testedon various low- and high-dimensional data sets including the 10-class MNIST andFashion-MNIST data sets, illustrating that the generative classificationstrategy is competitive against other previous quantum models.</description>
      <author>example@mail.com (Diego H. Useche, Sergio Quiroga-Sandoval, Sebastian L. Molina, Vladimir Vargas-Calderón, Juan E. Ardila-García, Fabio A. González)</author>
      <guid isPermaLink="false">2502.19970v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</title>
      <link>http://arxiv.org/abs/2502.19668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;心血管疾病是全球死亡和残疾的主要原因，心电图（ECG）记录对于诊断和监测心脏健康至关重要。然而，获取大规模注释的ECG数据集既费时又费力。&lt;h4&gt;背景&lt;/h4&gt;现有的ECG自监督学习方法虽然减少了标签需求，但未能捕捉到精细的临床语义，并且需要大量的任务特定微调。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，提出了SuPreME（一种用于多模态ECG表示学习的监督预训练框架）。&lt;h4&gt;方法&lt;/h4&gt;SuPreME利用大型语言模型从自由文本心电图报告中提取结构化的临床实体，过滤掉噪声和不相关信息，增强临床表示学习，并构建高质量、精细化标签的数据集。通过使用基于文本的心脏查询而不是传统的分类标签，SuPreME可以在无需额外微调的情况下实现未知疾病的零样本分类。&lt;h4&gt;主要发现&lt;/h4&gt;在六种下游数据集中进行了评估，涵盖了127种心脏状况，SuPreME取得了优于现有ECG自监督学习和多模态方法的零样本AUC性能，提高了1.96%以上。结果表明，通过利用结构化、临床相关的知识可以生成高质量的心电图表示。&lt;h4&gt;结论&lt;/h4&gt;所有代码和数据将在接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;心血管疾病是全球主要死因之一，心电图（ECG）记录对心脏健康诊断和监控至关重要，但由于大规模注释的ECG数据集获取困难，研究者提出了SuPreME框架。该方法采用大型语言模型提取临床实体信息，提高学习效率，并且在不进行额外训练的情况下实现未知疾病分类，展示出优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiovascular diseases are a leading cause of death and disabilityworldwide. Electrocardiogram (ECG) recordings are critical for diagnosing andmonitoring cardiac health, but obtaining large-scale annotated ECG datasets islabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)methods mitigate this by learning features without extensive labels but fail tocapture fine-grained clinical semantics and require extensive task-specificfine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a$\textbf{Su}$pervised $\textbf{Pre}$-training framework for$\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME appliesLarge Language Models (LLMs) to extract structured clinical entities fromfree-text ECG reports, filter out noise and irrelevant content, enhanceclinical representation learning, and build a high-quality, fine-grainedlabeled dataset. By using text-based cardiac queries instead of traditionalcategorical labels, SuPreME enables zero-shot classification of unseen diseaseswithout additional fine-tuning. We evaluate SuPreME on six downstream datasetscovering 127 cardiac conditions, achieving superior zero-shot AUC performanceover state-of-the-art eSSL and multimodal methods by over 1.96\%. Resultsdemonstrate the effectiveness of SuPreME in leveraging structured, clinicallyrelevant knowledge for high-quality ECG representations. All code and data willbe released upon acceptance.</description>
      <author>example@mail.com (Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci)</author>
      <guid isPermaLink="false">2502.19668v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction</title>
      <link>http://arxiv.org/abs/2502.19971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了基于线性注意力序列建模和图神经网络的通用量子纠错解码器，该解码器能够直接应用于各种稳定子编码结构，并在准确性和速度方面都优于专用算法。&lt;h4&gt;背景&lt;/h4&gt;量子错误校正对于大规模量子计算至关重要，但缺乏针对新编码（如量子稀疏奇偶校验码(QLDPC)）的高效解码器阻碍了其发展。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于所有稳定子编码的新通用解码器方案，以解决当前解码算法存在的问题，并提供更有效的纠错能力。&lt;h4&gt;方法&lt;/h4&gt;采用线性注意力序列建模和图神经网络技术开发了一种新的解码框架，该框架可以直接应用于任何稳定子编码的图形结构而不需要对其做结构性修改。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明新提出的解码器在各种类型（包括表面代码、颜色代码以及QLDPC等）的稳定子编码上均表现出更高的精度和更快的速度；对于Bivariate Bicycle码，当距离为12时，逻辑错误率降低了39.4%，而所需的解码时间仅占先前最佳解码器所需时间的大约1%。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种实用且通用的量子错误校正解决方案，消除了对特定代码专用解码器的需求，并有望推动量子计算技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;量子纠错对于大规模量子计算机来说至关重要。然而，缺乏针对新类型编码（如QLDPC）的有效解码器已成为阻碍其发展的瓶颈。本文介绍了一种基于线性注意力序列建模和图神经网络的通用解码器设计方法，该方法可以无缝地应用于各种稳定子编码结构中，并且在多种类型的量子纠错代码上均表现出卓越的表现力（如精度更高、速度更快）。特别值得注意的是，在Bivariate Bicycle 12距离下实现了39.4%逻辑错误率的显著降低及解码时间仅为先前最佳解码器所需时间的大约1%，证明了此方法具有实际应用价值，为未来的量子计算技术进步铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum error correction is crucial for large-scale quantum computing, butthe absence of efficient decoders for new codes like quantum low-densityparity-check (QLDPC) codes has hindered progress. Here we introduce a universaldecoder based on linear attention sequence modeling and graph neural networkthat operates directly on any stabilizer code's graph structure. Our numericalexperiments demonstrate that this decoder outperforms specialized algorithms inboth accuracy and speed across diverse stabilizer codes, including surfacecodes, color codes, and QLDPC codes. The decoder maintains linear time scalingwith syndrome measurements and requires no structural modifications betweendifferent codes. For the Bivariate Bicycle code with distance 12, our approachachieves a 39.4% lower logical error rate than previous best decoders whilerequiring only ~1% of the decoding time. These results provide a practical,universal solution for quantum error correction, eliminating the need forcode-specific decoders.</description>
      <author>example@mail.com (Gengyuan Hu, Wanli Ouyang, Chao-Yang Lu, Chen Lin, Han-Sen Zhong)</author>
      <guid isPermaLink="false">2502.19971v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>An Amplitude-Encoding-Based Classical-Quantum Transfer Learning framework: Outperforming Classical Methods in Image Recognition</title>
      <link>http://arxiv.org/abs/2502.20184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 12figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了经典的量子转移学习（CQTL）方法，旨在解决当前嘈杂中等规模量子计算时代在有限数量的量子比特上训练大规模高分辨率图像数据的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的CQTL框架已经在少量参数条件下展示了量子优势，但量子神经网络对参数的数量非常敏感。目前缺少关于更大规模、更多参数量子电路的研究和探索。&lt;h4&gt;目的&lt;/h4&gt;提出了一种基于振幅编码的经典-量子转移学习（AE-CQTL）框架，并设计了两种CQTL神经网络模型：Transfer Learning Quantum Neural Network (TLQNN) 和 Transfer Learning Quantum Convolutional Neural Network (TLQCNN)，以扩大参数容量并提升性能。&lt;h4&gt;方法&lt;/h4&gt;通过多层构造来增加量子电路的参数，基于AE-CQTL框架设计实现了两个模型，并在三个基准数据集（MNIST, Fashion-MNIST and CIFAR10）和三个源模型（ResNet18, ResNet50 and DenseNet121）上进行了跨实验。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型参数容量大幅提升，从几十个扩展到超过一百个参数；在多个性能指标中超越了传统经典分类器的基准表现，包括准确率、收敛性、稳定性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该工作为未来更大规模量子设备上经典-量子转移学习的应用推进做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;经典的量子迁移学习（CQTL）方法被引入来解决当前嘈杂中等规模量子计算时代的问题，即在有限数量的量子比特下训练大规模、高分辨率图像数据。尽管现有的CQTL框架已经展示了少量参数下的量子优势，但量子神经网络对参数数量敏感。目前缺乏研究和探索更大规模且具有更多参数的量子电路。本文提出了基于振幅编码的经典-量子迁移学习（AE-CQTL）框架，并设计了两个CQTL神经网络模型：转移学习量子神经网络（TLQNN）和转移学习量子卷积神经网络（TLQCNN）。在三个基准数据集上进行跨实验，结果显示所提出的模型超越传统经典分类器。研究结果为进一步推进大规模量子设备上的经典-量子迁移学习应用提供了理论基础和支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The classical-quantum transfer learning (CQTL) method is introduced toaddress the challenge of training large-scale, high-resolution image data on alimited number of qubits (ranging from tens to hundreds) in the current NoisyIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have beendemonstrate quantum advantages with a small number of parameters (around 50),but the performance of quantum neural networks is sensitive to the number ofparameters. Currently, there is a lack of exploration into larger-scale quantumcircuits with more parameters. This paper proposes an amplitude-encoding-basedclassical-quantum transfer learning (AE-CQTL) framework, accompanied by aneffective learning algorithm. The AE-CQTL framework multiplies the parametersof quantum circuits by using multi-layer ansatz. Based on the AE-CQTLframework, we designed and implemented two CQTL neural network models: Transferlearning Quantum Neural Network (TLQNN) and Transfer Learning QuantumConvolutional Neural Network (TLQCNN). Both models significantly expand theparameter capacity of quantum circuits, elevating the parameter scale from afew dozen to over one hundred parameters. In cross-experiments with threebenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded thebenchmark classical classifier in multiple performance metrics, includingaccuracy, convergence, stability, and generalization capability. Our workcontributes to advancing the application of classical-quantum transfer learningon larger-scale quantum devices in future.</description>
      <author>example@mail.com (Shouwei Hu, Xi Li, Banyao Ruan, Zhihao Liu)</author>
      <guid isPermaLink="false">2502.20184v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>High-fidelity Multiphysics Modelling for Rapid Predictions Using Physics-informed Parallel Neural Operator</title>
      <link>http://arxiv.org/abs/2502.19543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 11 figures, 1 table, 36 equations&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新型的物理信息并行神经算子（PIPNO）框架，用于解决由非线性和强耦合偏微分方程描述的复杂多物理系统的建模难题。&lt;h4&gt;背景&lt;/h4&gt;传统数值求解器在处理高度计算成本的问题时存在挑战，限制了它们在大规模应用中的实用性。数据驱动训练依赖于神经算子，在实际场景中由于缺乏或难以获取数据而适用性受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用支配物理定律进行无监督学习的框架，以实现无需数据支持即可建立偏微分方程模型的目标。&lt;h4&gt;方法&lt;/h4&gt;引入并行核积分设计，结合集合学习的方法，极大地提升了算子学习中的兼容性和计算效率，使得非线性和强耦合偏微分方程的学习成为可能。该方法适用于地技术工程、材料科学、电磁学、量子力学和流体动力学等多个领域的复杂物理问题。&lt;h4&gt;主要发现&lt;/h4&gt;PIPNO能够在高度复杂的多物理系统建模中实现高保真度和快速预测，优于现有的算子学习方法。&lt;h4&gt;结论&lt;/h4&gt;PIPNO为传统的求解器提供了一种强有力的替代方案，扩展了神经算子在多物理模型中的适用性，并保证了效率、鲁棒性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;建立复杂的多物理系统模型是计算科学和工程的重要基础。此类系统的数学描述往往涉及非线性和强耦合的偏微分方程（PDEs）。传统数值解法在处理这类问题时面临高计算成本的问题，使得它们难以应用于大规模的实际应用中。数据驱动训练方法依赖于大量高质量的数据进行神经算子的学习，在实际场景中因为获取数据的成本或难度而受限。文中提出了一种新的框架物理信息并行神经算子（PIPNO），这是一个可扩展且无需监督学习的框架，仅依靠支配的物理定律即可构建偏微分方程模型。其设计包括了并行核积分和集合学习的方法，显著提升了算子学习中的兼容性和计算效率，使得对非线性及强耦合偏微分方程的学习成为可能。PIPNO能够在地技术工程、材料科学、电磁学、量子力学以及流体动力学等多个领域中高效捕捉不同物理现象之间的非线性操作映射关系，并在模型预测的速度和准确性方面优于现有算子学习方法，为解决多物理系统的建模难题提供了一种新的有效途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling complex multiphysics systems governed by nonlinear and stronglycoupled partial differential equations (PDEs) is a cornerstone in computationalscience and engineering. However, it remains a formidable challenge fortraditional numerical solvers due to high computational cost, making themimpractical for large-scale applications. Neural operators' reliance ondata-driven training limits their applicability in real-world scenarios, asdata is often scarce or expensive to obtain. Here, we propose a novel paradigm,physics-informed parallel neural operator (PIPNO), a scalable and unsupervisedlearning framework that enables data-free PDE modelling by leveraging onlygoverning physical laws. The parallel kernel integration design, incorporatingensemble learning, significantly enhances both compatibility and computationalefficiency, enabling scalable operator learning for nonlinear and stronglycoupled PDEs. PIPNO efficiently captures nonlinear operator mappings acrossdiverse physics, including geotechnical engineering, material science,electromagnetism, quantum mechanics, and fluid dynamics. The proposed methodachieves high-fidelity and rapid predictions, outperforming existing operatorlearning approaches in modelling nonlinear and strongly coupled multiphysicssystems. Therefore, PIPNO offers a powerful alternative to conventionalsolvers, broadening the applicability of neural operators for multiphysicsmodelling while ensuring efficiency, robustness, and scalability.</description>
      <author>example@mail.com (Biao Yuan, He Wang, Yanjie Song, Ana Heitor, Xiaohui Chen)</author>
      <guid isPermaLink="false">2502.19543v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Noise-Injected Spiking Graph Convolution for Energy-Efficient 3D Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2502.19660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种注入噪声的尖峰图卷积网络，用于提升脉冲神经网络（SNN）在三维点云去噪中的回归性能。&lt;h4&gt;背景&lt;/h4&gt;脉冲神经网络由于其优越的能量效率，在二维分类任务中优于传统人工神经网络。然而，在三维点云处理方面，特别是回归任务上，SNN的潜力尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;设计一种注入噪声的尖峰图卷积方法以增强3D点云去噪能力，并展示了两种基于SNN的去噪网络性能。&lt;h4&gt;方法&lt;/h4&gt;首先模拟了带有噪声的神经元动力学，构建出带噪声的脉冲神经元。然后在此基础上设计了注入噪声的脉冲图卷积层，促进对三维数据扰动感知下的尖峰表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种全新的SNN架构用于3D点云去噪任务，并展示了其相比基于ANN的模型而言，可以显著降低能耗的同时保持较低的精度损失。此外还设计了一个结合了深度学习方法与高效能特点的混合架构。&lt;h4&gt;结论&lt;/h4&gt;这项工作揭示了脉冲神经网络在三维数据处理方面的潜力，为探索部署于类脑芯片以及开发高能效3D数据采集设备铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;受生物神经系统中尖峰计算范式的启发，脉冲神经网络（SNN）在二维分类任务中的能量效率优于传统人工神经网络（ANN）。然而，在三维点云处理方面，特别是回归任务上，SNN的潜力尚未充分探索。本文提出了一种注入噪声的尖峰图卷积网络来最大化SNN在3D点云去噪中的回归潜能。具体而言，我们首先模拟了带有噪声的神经元动力学以构建带噪声的脉冲神经元。基于此基础，设计了促进三维数据扰动感知下的尖峰表示学习的注入噪声的脉冲图卷积层。从脉冲图卷积出发，建立了两个SNN去噪网络：一个是纯脉冲图卷积网络，在两个基准数据集PU-Net和PC-Net上相比一些基于ANN的方法显示出较低的精度损失同时显著减少了能量消耗；另一个是混合架构，结合了深度学习方法并在仅几步时间步骤中就实现了高效能。这项工作揭示了SNN在三维点云去噪中的潜力，并为探索类脑芯片上的部署以及开发高能效3D数据采集设备铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks (SNNs), inspired by the spiking computation paradigmof the biological neural systems, have exhibited superior energy efficiency in2D classification tasks over traditional artificial neural networks (ANNs).However, the regression potential of SNNs has not been well explored,especially in 3D point cloud processing.In this paper, we proposenoise-injected spiking graph convolutional networks to leverage the fullregression potential of SNNs in 3D point cloud denoising. Specifically, wefirst emulate the noise-injected neuronal dynamics to build noise-injectedspiking neurons. On this basis, we design noise-injected spiking graphconvolution for promoting disturbance-aware spiking representation learning on3D points. Starting from the spiking graph convolution, we build two SNN-baseddenoising networks. One is a purely spiking graph convolutional network, whichachieves low accuracy loss compared with some ANN-based alternatives, whileresulting in significantly reduced energy consumption on two benchmarkdatasets, PU-Net and PC-Net. The other is a hybrid architecture that combinesANN-based learning with a high performance-efficiency trade-off in just a fewtime steps. Our work lights up SNN's potential for 3D point cloud denoising,injecting new perspectives of exploring the deployment on neuromorphic chipswhile paving the way for developing energy-efficient 3D data acquisitiondevices.</description>
      <author>example@mail.com (Zikuan Li, Qiaoyun Wu, Jialin Zhang, Kaijun Zhang, Jun Wang)</author>
      <guid isPermaLink="false">2502.19660v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Experts for Recognizing Depression from Interview and Reading Tasks</title>
      <link>http://arxiv.org/abs/2502.20213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用混合专家模型（MoE）来识别抑郁的新型深度神经网络方法，它同时考虑自发和朗读语音，并使用多模态融合技术。&lt;h4&gt;背景&lt;/h4&gt;抑郁症是一种精神疾病，能够引起心理、生理和社会方面的多种症状。研究表明，言语是早期识别抑郁症的一个客观标志。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器学习方法，通过分析人类的自发性对话和阅读任务中的音频数据来更准确地识别抑郁症。&lt;h4&gt;方法&lt;/h4&gt;本研究首次在抑郁检测任务中利用混合专家模型（MoE），将来自两种不同类型的语音的数据结合起来。它使用了音频文件对应于访谈任务和朗读任务，并将其转换为log-Mel频谱图，然后通过共享的AlexNet模型处理图像表示，最后输出向量通过MoE模块。&lt;h4&gt;主要发现&lt;/h4&gt;该研究采用了三种MoE变体：稀疏门控混合专家（Sparsely-gated MoE）和基于分解的多线性混合专家（Multilinear MoE），在Androids语料库上的实验中，达到了87.00%的准确率和86.66%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种新颖且有效的方法来识别抑郁症，通过利用自发性和朗读语音的数据，并应用先进的多模态融合技术。这种方法在Androids语料库上的实验中表现出了很高的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一个创新的研究工作，旨在开发一个能够同时使用自发性对话和阅读任务中的音频数据的深度学习模型来识别抑郁症。该方法使用混合专家（MoE）框架，结合了多模态融合技术，以提高对抑郁症状检测的精度。实验结果表明，在Androids语料库上该模型达到了87.00%的准确率和86.66%的F1分数，显示出在识别抑郁症方面的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depression is a mental disorder and can cause a variety of symptoms,including psychological, physical, and social. Speech has been proved anobjective marker for the early recognition of depression. For this reason, manystudies have been developed aiming to recognize depression through speech.However, existing methods rely on the usage of only the spontaneous speechneglecting information obtained via read speech, use transcripts which areoften difficult to obtain (manual) or come with high word-error rates(automatic), and do not focus on input-conditional computation methods. Toresolve these limitations, this is the first study in depression recognitiontask obtaining representations of both spontaneous and read speech, utilizingmultimodal fusion methods, and employing Mixture of Experts (MoE) models in asingle deep neural network. Specifically, we use audio files corresponding toboth interview and reading tasks and convert each audio file into log-Melspectrogram, delta, and delta-delta. Next, the image representations of the twotasks pass through shared AlexNet models. The outputs of the AlexNet models aregiven as input to a multimodal fusion method. The resulting vector is passedthrough a MoE module. In this study, we employ three variants of MoE, namelysparsely-gated MoE and multilinear MoE based on factorization. Findings suggestthat our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66%respectively on the Androids corpus.</description>
      <author>example@mail.com (Loukas Ilias, Dimitris Askounis)</author>
      <guid isPermaLink="false">2502.20213v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion</title>
      <link>http://arxiv.org/abs/2502.20120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多模态学习方法，通过设计持续增强算法来动态平衡弱模态和强模态的分类能力，从而克服了现有方法在处理模式不平衡时的局限性。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态学习已经取得了显著的进步，但存在的模态失衡问题阻碍了其在实践中超越单模态模型的优势。主流多模态学习方法主要关注于平衡学习过程，然而这些方法没有明确地增强较弱模态的分类能力，导致性能提升有限。&lt;h4&gt;目的&lt;/h4&gt;设计一种持续增强算法以动态平衡强模态和弱模态之间的分类能力，从而缓解模式失衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种在多模态学习中同时优化分类误差和残差误差的设计可配置分类器模块的持续增强算法。进一步提出了一个自适应分类器分配策略，以动态提升弱模态的分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了该方法的有效性，并展示了与现有最先进的多模态学习基线相比的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在缓解模式不平衡问题方面表现出色，能够有效提高多模态模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although multimodal learning~(MML) has garnered remarkable progress, theexistence of modality imbalance hinders multimodal learning from achieving itsexpected superiority over unimodal models in practice. To overcome this issue,mainstream multimodal learning methods have placed greater emphasis onbalancing the learning process. However, these approaches do not explicitlyenhance the classification ability of weaker modalities, leading to limitedperformance promotion. By designing a sustained boosting algorithm, we proposea novel multimodal learning approach to dynamically balance the classificationability of weak and strong modalities. Concretely, we first propose a sustainedboosting algorithm in multimodal learning by simultaneously optimizing theclassification and residual errors using a designed configurable classifiermodule. Then, we propose an adaptive classifier assignment strategy todynamically facilitate the classification performance of weak modality. To thisend, the classification ability of strong and weak modalities is expected to bebalanced, thereby mitigating the imbalance issue. Empirical experiments onwidely used datasets reveal the superiority of our method through comparisonwith various state-of-the-art~(SoTA) multimodal learning baselines.</description>
      <author>example@mail.com (QingYuan Jiang, Longfei Huang, Yang Yang)</author>
      <guid isPermaLink="false">2502.20120v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Learning Mask Invariant Mutual Information for Masked Image Modeling</title>
      <link>http://arxiv.org/abs/2502.19718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视角来理解Masked Autoencoders (MAEs) 的工作原理，通过信息瓶颈原则来分析和优化它们。引入了MI-MAE方法，该方法利用互信息最大化和最小化策略优化MAEs的潜在特征以提高性能。&lt;h4&gt;背景&lt;/h4&gt;Masked autoencoders在计算机视觉中的自监督学习领域非常突出，尽管有实证成功但其底层机制还不完全被理解。&lt;h4&gt;目的&lt;/h4&gt;通过理论分析揭示了平衡相关和不相关信息对于改进MAE性能的关键作用，并提出了一个基于信息瓶颈原理的新方法MI-MAE来优化潜在特征。&lt;h4&gt;方法&lt;/h4&gt;利用互信息最大化的技术，增强潜在特征以保留与输出的最大相关性；同时减少潜在特征与输入的无关信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验显示，所提出的MI-MAE在图像分类、目标检测和语义分割等任务中均显著优于原始MAE模型。这验证了理论框架的有效性和基于信息瓶颈原则优化自监督学习模型的实际优势。&lt;h4&gt;结论&lt;/h4&gt;该研究为开发更强大的自监督学习模型提供了更深的见解，强调了将信息瓶颈原则应用于MAEs的重要性。&lt;h4&gt;翻译&lt;/h4&gt;掩膜自动编码器（Masked Autoencoders，简称MAE）代表了一种在计算机视觉中自监督学习领域非常突出的方法。尽管它们在实验上表现出色，但其背后的机制尚未被充分理解。最近的研究试图通过对比学习和特征表示分析来阐明MAEs的工作原理，但是这些方法往往只能提供间接的见解。本文提出一种新的视角以信息论中的信息瓶颈原则为基础来理解MAE，并且理论分析揭示了优化潜在特征以平衡相关和不相关信息是提高MAE性能的关键。基于我们的证明，我们引入了一种名为MI-MAE的新方法，该方法通过互信息最大化和最小化策略优化掩膜自动编码器。通过增强潜在特征以保留与输出的最大关联性，并减少与输入的无关信息，我们的方法实现了更好的效果。在标准基准上的广泛实验表明，在图像分类、目标检测和语义分割等任务中，MI-MAE显著优于传统Masked Autoencoder模型。这些发现验证了理论框架的有效性，并突出了将信息瓶颈原则应用于掩膜自动编码器的实际优势，为开发更强大的自监督学习模型提供了深入的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked autoencoders (MAEs) represent a prominent self-supervised learningparadigm in computer vision. Despite their empirical success, the underlyingmechanisms of MAEs remain insufficiently understood. Recent studies haveattempted to elucidate the functioning of MAEs through contrastive learning andfeature representation analysis, yet these approaches often provide onlyimplicit insights. In this paper, we propose a new perspective forunderstanding MAEs by leveraging the information bottleneck principle ininformation theory. Our theoretical analyses reveal that optimizing the latentfeatures to balance relevant and irrelevant information is key to improving MAEperformance. Building upon our proofs, we introduce MI-MAE, a novel method thatoptimizes MAEs through mutual information maximization and minimization. Byenhancing latent features to retain maximal relevant information between themand the output, and minimizing irrelevant information between them and theinput, our approach achieves better performance. Extensive experiments onstandard benchmarks show that MI-MAE significantly outperforms MAE models intasks such as image classification, object detection, and semanticsegmentation. Our findings validate the theoretical framework and highlight thepractical advantages of applying the information bottleneck principle to MAEs,offering deeper insights for developing more powerful self-supervised learningmodels.</description>
      <author>example@mail.com (Tao Huang, Yanxiang Ma, Shan You, Chang Xu)</author>
      <guid isPermaLink="false">2502.19718v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>GraphSparseNet: a Novel Method for Large Scale Trafffic Flow Prediction</title>
      <link>http://arxiv.org/abs/2502.19823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;交通流预测是一项关键的时空数据挖掘任务，在智能路线规划和动态交通管理中具有广泛的应用。近年来，深度学习技术特别是图神经网络（GNNs）在提高这些预测准确度方面取得了显著进展，通过捕捉复杂的时空动态特性来实现这一点。&lt;h4&gt;背景&lt;/h4&gt;交通流量预测是一个重要的时空数据分析领域，最近的研究利用深度学习方法尤其是基于图的模型大幅提高了其准确性。然而，现有的图神经网络面临的一个主要问题是随着图形中节点数量增加而产生的计算复杂度呈指数级增长。&lt;h4&gt;目的&lt;/h4&gt;本文介绍了一种称为GraphSparseNet (GSNet)的新框架，旨在同时提高GNN在交通预测中的可扩展性和准确率。&lt;h4&gt;方法&lt;/h4&gt;该框架由两个核心模块构成：特征提取器和关系压缩器。这些模块具有线性的时间和空间复杂度，这使得整个模型的计算复杂度减少到线性级别。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GraphSparseNet不仅比最先进的线性模型将训练时间减少了3.51倍，而且还保持了高预测性能。&lt;h4&gt;结论&lt;/h4&gt;GSNet框架为解决GNN可扩展性和准确性问题提供了一种新的解决方案，并展示了在实际数据集上的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic flow forecasting is a critical spatio-temporal data mining task withwide-ranging applications in intelligent route planning and dynamic trafficmanagement. Recent advancements in deep learning, particularly through GraphNeural Networks (GNNs), have significantly enhanced the accuracy of theseforecasts by capturing complex spatio-temporal dynamics. However, thescalability of GNNs remains a challenge due to their exponential growth inmodel complexity with increasing nodes in the graph. Existing methods toaddress this issue, including sparsification, decomposition, and kernel-basedapproaches, either do not fully resolve the complexity issue or riskcompromising predictive accuracy. This paper introduces GraphSparseNet (GSNet),a novel framework designed to improve both the scalability and accuracy ofGNN-based traffic forecasting models. GraphSparseNet is comprised of two coremodules: the Feature Extractor and the Relational Compressor. These modulesoperate with linear time and space complexity, thereby reducing the overallcomputational complexity of the model to a linear scale. Our extensiveexperiments on multiple real-world datasets demonstrate that GraphSparseNet notonly significantly reduces training time by 3.51x compared to state-of-the-artlinear models but also maintains high predictive performance.</description>
      <author>example@mail.com (Weiyang Kong, Kaiqi Wu, Sen Zhang, Yubao Liu)</author>
      <guid isPermaLink="false">2502.19823v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning in Latent Contextual Bandits with Covariate Shift Through Causal Transportability</title>
      <link>http://arxiv.org/abs/2502.20153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference of Causal Learning and Reasoning (CLeaR  2025), will be published in the Proceedings of Machine Learning Research&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了智能系统中知识从一个环境转移到另一个环境的能力，特别是在多臂赌博机框架下的因果推理视角。研究关注潜在上下文下的转移学习，并考虑跨环境的条件变化（即协变量偏移）。文章提出了一种基于因果推断运输理论的方法来开发算法，这些算法能够有效地在目标环境中转移知识。&lt;h4&gt;背景&lt;/h4&gt;在两个不同的环境中直接迁移所有知识可能会导致性能下降，这种现象被称为负向迁移。该问题需要通过更精细的知识迁移策略解决。&lt;h4&gt;目的&lt;/h4&gt;研究旨在提出一种有效的学习框架，以便智能系统可以更好地处理跨环境下的负向迁移，并且在这种情况下能够有效地进行知识转移。&lt;h4&gt;方法&lt;/h4&gt;文章使用因果推理理论中的可传输性理论来开发算法。利用变分自动编码器在高维代理存在的情况下近似因果效果。研究测试了这些算法在合成和半合成数据集上的性能，与基准算法相比，结果表明该框架具有持续改进的学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;经典多臂赌博机算法下的直接知识转移会导致负向迁移。利用运输理论进行有效的知识转移可以提高目标环境中的学习效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于因果推断的框架在处理跨环境的知识转移问题上是有效且高效的，尤其是当存在高维代理时。该方法相对于基准算法展示了持续改进的学习性能，并为智能系统如何更好地从一个环境迁移到另一个环境中学习提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;知识迁移是智能系统的必备能力之一。然而，在两个不同的环境下，直接转移所有知识可能导致性能下降（负向迁移）。本研究在多臂赌博机框架下探讨了该问题，特别是针对潜在上下文下的知识转移，并考虑到了环境变化的影响（协变量偏移）。通过应用因果推理理论中的可传输性原则来开发有效的算法，这些算法旨在有效估计目标环境中感兴趣的因果效应。此外，利用变分自动编码器来处理高维代理情况下的近似因果效果。测试结果表明，在合成和半合成数据集上，该方法相对于基准算法表现出更优的学习效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring knowledge from one environment to another is an essentialability of intelligent systems. Nevertheless, when two environments aredifferent, naively transferring all knowledge may deteriorate the performance,a phenomenon known as negative transfer. In this paper, we address this issuewithin the framework of multi-armed bandits from the perspective of causalinference. Specifically, we consider transfer learning in latent contextualbandits, where the actual context is hidden, but a potentially high-dimensionalproxy is observable. We further consider a covariate shift in the contextacross environments. We show that naively transferring all knowledge forclassical bandit algorithms in this setting led to negative transfer. We thenleverage transportability theory from causal inference to develop algorithmsthat explicitly transfer effective knowledge for estimating the causal effectsof interest in the target environment. Besides, we utilize variationalautoencoders to approximate causal effects under the presence of ahigh-dimensional proxy. We test our algorithms on synthetic and semi-syntheticdatasets, empirically demonstrating consistently improved learning efficiencyacross different proxies compared to baseline algorithms, showing theeffectiveness of our causal framework in transferring knowledge.</description>
      <author>example@mail.com (Mingwei Deng, Ville Kyrki, Dominik Baumann)</author>
      <guid isPermaLink="false">2502.20153v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching</title>
      <link>http://arxiv.org/abs/2502.19705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为CFTrack的轻量级追踪器，该追踪器结合了对比学习和特征匹配技术，增强了区分能力。&lt;h4&gt;背景&lt;/h4&gt;在移动设备和边缘计算设备上进行高效的视觉跟踪是一个挑战，特别是当这些设备资源受限时。传统的轻量级追踪器难以应对遮挡和干扰问题，而深度学习方法压缩后性能会下降。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的局限性，提出了一种新的轻量级追踪算法CFTrack，旨在解决在计算资源有限的情况下提高跟踪精度的问题。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了一个新颖的对比特征匹配模块（contrastive feature matching module），通过自适应对比损失优化目标相似度动态评估过程。这个模块与传统的特征匹配相结合，形成了改进后的轻量级追踪器CFTrack。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，CFTrack在LaSOT、OTB100和UAV123数据集上优于许多最新的轻量级追踪器，在NVIDIA Jetson NX平台上可以达到每秒136帧的性能。进一步的研究表明，CFTrack具有强大的区分能力，并且在HOOT数据集中重遮挡的情况下表现尤为出色。&lt;h4&gt;结论&lt;/h4&gt;CFTrack通过引入对比学习和特征匹配技术克服了传统轻量级跟踪器的缺点，能够在资源有限的情况下提供高精度跟踪结果。&lt;h4&gt;翻译&lt;/h4&gt;实现视觉追踪中的高效性和强大的区分能力是一个挑战，尤其是在计算资源受限的手持设备上。传统的轻量级追踪器在遮挡和干扰情况下的鲁棒性不足，而深度学习方法压缩后会性能下降。本文提出了一种名为CFTrack的跟踪器，它结合了对比学习和特征匹配技术来增强区分性的特征表示能力。通过自适应对比损失优化的新颖对比特性匹配模块，CFTrack能够在预测期间动态评估目标相似度，并提高了追踪精度。实验表明，在LaSOT、OTB100和UAV123数据集上，CFTrack优于许多最新的轻量级跟踪器，在NVIDIA Jetson NX平台上可以达到每秒136帧的性能。在HOOT数据集中，进一步证实了CFTrack在严重遮挡情况下的强大区分能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving both efficiency and strong discriminative ability in lightweightvisual tracking is a challenge, especially on mobile and edge devices withlimited computational resources. Conventional lightweight trackers oftenstruggle with robustness under occlusion and interference, while deep trackers,when compressed to meet resource constraints, suffer from performancedegradation. To address these issues, we introduce CFTrack, a lightweighttracker that integrates contrastive learning and feature matching to enhancediscriminative feature representations. CFTrack dynamically assesses targetsimilarity during prediction through a novel contrastive feature matchingmodule optimized with an adaptive contrastive loss, thereby improving trackingaccuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTracksurpasses many state-of-the-art lightweight trackers, operating at 136 framesper second on the NVIDIA Jetson NX platform. Results on the HOOT datasetfurther demonstrate CFTrack's strong discriminative ability under heavyocclusion.</description>
      <author>example@mail.com (Juntao Liang, Jun Hou, Weijun Zhang, Yong Wang)</author>
      <guid isPermaLink="false">2502.19705v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>cMIM: A Contrastive Mutual Information Framework for Unified Generative and Discriminative Representation Learning</title>
      <link>http://arxiv.org/abs/2502.19642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A working draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的对比互信息机(cMIM)模型，旨在提升表示学习在未知下游任务中的实用性。&lt;h4&gt;背景&lt;/h4&gt;表示学习的一个基本挑战是学习对未见下游任务有用的表示。目前该领域的主流方法包括对比学习、自监督掩码和去噪自动编码器。&lt;h4&gt;目的&lt;/h4&gt;为了增强所学表示对于下游任务的适用性，提出了cMIM方法，直接解决了现有Mutual Information Machine (MIM)模型在区分下游任务表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;cMIM将新的对比学习损失函数与互信息机(MIM)学习框架集成在一起。cMIM不仅消除了数据增强的需求，并且对负样本数量（即批量大小）的变化具有鲁棒性；另外还引入了一种通用的方法从编码器-解码器模型中提取有用的嵌入，显著提高了在区分下游任务中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的cMIM方法不仅解决了现有MIM模型表示对于区分下游任务适用性的不足，而且还提供了一个统一的生成模型，该模型对生成和区分性任务都很有效。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，通过cMIM学习到的表示可以为下游任务提供价值的同时保持了MIM的生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations that are useful for unknown downstream tasks is afundamental challenge in representation learning. Prominent approaches in thisdomain include contrastive learning, self-supervised masking, and denoisingauto-encoders. In this paper, we introduce a novel method, termed contrastiveMutual Information Machine (cMIM), which aims to enhance the utility of learnedrepresentations for downstream tasks. cMIM integrates a new contrastivelearning loss with the Mutual Information Machine (MIM) learning framework, aprobabilistic auto-encoder that maximizes the mutual information between inputsand latent representations while clustering the latent codes. Despite MIM'spotential, initial experiments indicated that the representations learned byMIM were less effective for discriminative downstream tasks compared tostate-of-the-art (SOTA) models. The proposed cMIM method directly addressesthis limitation.  The main contributions of this work are twofold: (1) We propose a novelcontrastive extension to MIM for learning discriminative representations whicheliminates the need for data augmentation and is robust to variations in thenumber of negative examples (i.e., batch size). (2) We introduce a genericmethod for extracting informative embeddings from encoder-decoder models, whichsignificantly improves performance in discriminative downstream tasks withoutrequiring additional training. This method is applicable to any pre-trainedencoder-decoder model.  By presenting cMIM, we aim to offer a unified generative model that iseffective for both generative and discriminative tasks. Our results demonstratethat the learned representations are valuable for downstream tasks whilemaintaining the generative capabilities of MIM.</description>
      <author>example@mail.com (Micha Livne)</author>
      <guid isPermaLink="false">2502.19642v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>DGFM: Full Body Dance Generation Driven by Music Foundation Models</title>
      <link>http://arxiv.org/abs/2502.20176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Audio Imagination Workshop of NeurlPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于扩散模型的音乐驱动舞蹈动作生成方法，该方法结合了高级音乐基础模型和手工制作特征来提升生成舞蹈序列的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数音乐驱动舞蹈动作生成方法依赖于手工制作的特征，并未充分利用音乐基础模型对跨模态内容生成的影响。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一不足，本研究提出了一种基于扩散的方法，该方法可以根据文本和音乐生成舞蹈动作。&lt;h4&gt;方法&lt;/h4&gt;通过结合由音乐基础模型获得的高级特性与手工制作的特性来提取音乐特征。此方法能有效利用高级语义信息和低级时间细节的优势，提高模型理解音乐特征的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在生成最逼真的舞蹈序列方面优于四个音乐基础模型和两组手工制作的音乐特征，并且与输入音乐匹配度最佳。&lt;h4&gt;结论&lt;/h4&gt;通过将高级语义信息和低级时间细节相结合，可以显著提高基于文本和音乐的舞蹈动作生成效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In music-driven dance motion generation, most existing methods usehand-crafted features and neglect that music foundation models have profoundlyimpacted cross-modal content generation. To bridge this gap, we propose adiffusion-based method that generates dance movements conditioned on text andmusic. Our approach extracts music features by combining high-level featuresobtained by music foundation model with hand-crafted features, therebyenhancing the quality of generated dance sequences. This method effectivelyleverages the advantages of high-level semantic information and low-leveltemporal details to improve the model's capability in music featureunderstanding. To show the merits of the proposed method, we compare it withfour music foundation models and two sets of hand-crafted music features. Theresults demonstrate that our method obtains the most realistic dance sequencesand achieves the best match with the input music.</description>
      <author>example@mail.com (Xinran Liu, Zhenhua Feng, Diptesh Kanojia, Wenwu Wang)</author>
      <guid isPermaLink="false">2502.20176v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and Scalable Graph Neural Networks via Message Invariance</title>
      <link>http://arxiv.org/abs/2502.19693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于消息传递的图神经网络（GNN）在许多实际应用中取得了巨大成功。然而，对于采样的目标节点小批量来说，从外部节点到内部节点的消息传递导致了随着层数增加而指数级增长的计算成本。&lt;h4&gt;背景&lt;/h4&gt;现有方法中的消息传递过程分为两部分：同一小批量内的节点间消息传递（MP-IB）和从小批量外向内节点的消息传递（MP-OB）。MP-OB依赖于更高阶的小批量外部邻居，导致了随着层数增加而指数级增长的计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出一种准确且快速的大图归纳学习小批量方法——拓扑补偿(TOP)，以解决因消息传递过程导致的大规模图形中节点和边过多存储在GPU上的问题。&lt;h4&gt;方法&lt;/h4&gt;TOP通过引入消息不变性概念，将昂贵的MP-OB转化为快速的MP-IB。这保证了修改后的MP-IB与整个消息传递具有相同的输出结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在大规模图（数百万节点和数十亿边）上，TOP比现有的小批量方法快几个数量级，并且精度下降有限。&lt;h4&gt;结论&lt;/h4&gt;通过避免昂贵的MP-OB计算成本，TOP使得GNN在大规模图形中变得更加可行。&lt;h4&gt;翻译&lt;/h4&gt;基于消息传递的图神经网络（GNNs）在许多实际应用中取得了巨大成功。对于采样的目标节点小批量来说，消息传递过程分为两部分：同一小批量内的节点间消息传递（MP-IB）和从小批量外向内节点的消息传递（MP-OB）。然而，由于邻域爆炸问题，整个消息传递过程中需要在GPU上存储大部分节点和边。为解决这一挑战，我们提出了一种针对大规模图归纳学习的准确且快速的小批量方法——拓扑补偿(TOP)，该方法仅通过MP-IB就能获得整个消息传递的结果而无需昂贵的MP-OB计算成本。TOP的核心在于引入了一个新的概念——消息不变性，它定义了将昂贵的MP-OB转换为快速MP-IB的消息不变变换。这确保了修改后的MP-IB与整个消息传递具有相同的输出结果。实验表明，在包含数百万节点和数十亿边的大规模图上，TOP比现有的小批量方法快几个数量级，并且精度下降有限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message passing-based graph neural networks (GNNs) have achieved greatsuccess in many real-world applications. For a sampled mini-batch of targetnodes, the message passing process is divided into two parts: message passingbetween nodes within the batch (MP-IB) and message passing from nodes outsidethe batch to those within it (MP-OB). However, MP-OB recursively relies onhigher-order out-of-batch neighbors, leading to an exponentially growingcomputational cost with respect to the number of layers. Due to the neighborexplosion, the whole message passing stores most nodes and edges on the GPUsuch that many GNNs are infeasible to large-scale graphs. To address thischallenge, we propose an accurate and fast mini-batch approach for large graphtransductive learning, namely topological compensation (TOP), which obtains theoutputs of the whole message passing solely through MP-IB, without the costlyMP-OB. The major pillar of TOP is a novel concept of message invariance, whichdefines message-invariant transformations to convert costly MP-OB into fastMP-IB. This ensures that the modified MP-IB has the same output as the wholemessage passing. Experiments demonstrate that TOP is significantly faster thanexisting mini-batch methods by order of magnitude on vast graphs (millions ofnodes and billions of edges) with limited accuracy degradation.</description>
      <author>example@mail.com (Zhihao Shi, Jie Wang, Zhiwei Zhuang, Xize Liang, Bin Li, Feng Wu)</author>
      <guid isPermaLink="false">2502.19693v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-Spectral Diffusion Contrastive Representation Network for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2502.19699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于去噪扩散概率模型（DDPM）结合对比学习（CL）的新型网络DiffCRN，用于高光谱图像分类（HSIC），旨在改进空间-光谱特征表示、无监督特征学习效率、时间步长选择和特征融合与分类。&lt;h4&gt;背景&lt;/h4&gt;在对高光谱图像进行有效提取具有区分性的空间-光谱特征时面临挑战，主要由于空间-光谱异质性和噪声效应等因素导致难以实现高效的空间-光谱特征表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的网络结构DiffCRN来改进高光谱图像分类中的空间-光谱特征学习效率和无监督特性提取。&lt;h4&gt;方法&lt;/h4&gt;{'架构设计': '采用具有空间自注意去噪模块（SSAD）和光谱组自注意力去噪模块（SGSAD）的分阶段架构，代替常用的UNet-like结构。', '改进损失函数': '设计新的DDPM模型结合对数绝对误差（LAE）损失和对比学习以提高损失函数的有效性和增强实例级别和类间区分性。', '时间步长选择': '引入基于像素级光谱角度映射（SAM）的可学习方法，自适应自动地为提出的DDPM模型选择合适的时间步骤。', '特征融合与分类': '设计了自适应加权添加模块（AWAM）和跨时间步空间-光谱融合模块（CTSSFM）以融合按时间步划分的特性并进行分类。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的DiffCRN模型在四个广泛使用的高光谱数据集上相比经典基础模型、最新的GAN、Transformer模型和其他预训练方法具有更好的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效提高HSIC任务中的空间-光谱特征表示能力和无监督特性提取的效率，为该领域提供了一种新的解决方案。源代码和预训练模型将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管对高光谱图像分类（HSIC）来说，高效地提取具有区分性的空间-光谱特征是至关重要的，但由于诸如空间-光谱异质性和噪声效应等因素的影响，实现这些特性非常困难。本文提出了一种基于去噪扩散概率模型（DDPM）与对比学习（CL）结合的高光谱图像分类（HSIC）的空间-光谱扩散对比表示网络（DiffCRN）。该方法具有以下特点：改进空间-光谱特征表示；提高无监督特性学习效率；改进时间步长选择；改进特性融合和分类。实验结果表明，所提出的模型在四个广泛使用的高光谱数据集上相比经典基础模型、最新的GAN、Transformer模型和其他预训练方法有更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although efficient extraction of discriminative spatial-spectral features iscritical for hyperspectral images classification (HSIC), it is difficult toachieve these features due to factors such as the spatial-spectralheterogeneity and noise effect. This paper presents a Spatial-SpectralDiffusion Contrastive Representation Network (DiffCRN), based on denoisingdiffusion probabilistic model (DDPM) combined with contrastive learning (CL)for HSIC, with the following characteristics. First,to improve spatial-spectralfeature representation, instead of adopting the UNets-like structure which iswidely used for DDPM, we design a novel staged architecture with spatialself-attention denoising module (SSAD) and spectral group self-attentiondenoising module (SGSAD) in DiffCRN with improved efficiency forspectral-spatial feature learning. Second, to improve unsupervised featurelearning efficiency, we design new DDPM model with logarithmic absolute error(LAE) loss and CL that improve the loss function effectiveness and increase theinstance-level and inter-class discriminability. Third, to improve featureselection, we design a learnable approach based on pixel-level spectral anglemapping (SAM) for the selection of time steps in the proposed DDPM model in anadaptive and automatic manner. Last, to improve feature integration andclassification, we design an Adaptive weighted addition modul (AWAM) and Crosstime step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wisefeatures and perform classification. Experiments conducted on widely used fourHSI datasets demonstrate the improved performance of the proposed DiffCRN overthe classical backbone models and state-of-the-art GAN, transformer models andother pretrained methods. The source code and pre-trained model will be madeavailable publicly.</description>
      <author>example@mail.com (Yimin Zhu, Linlin Xu)</author>
      <guid isPermaLink="false">2502.19699v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with Pre-trained Large Language Model</title>
      <link>http://arxiv.org/abs/2502.19960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures. Code is available at  https://github.com/StarMoonWang/SeisMoLLM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SeisMoLLM，这是一个利用跨模态迁移的地震监测基础模型，它通过大规模预训练从大型语言模型中释放其潜力，并在多个复杂的地震监测任务上取得了卓越性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，深度学习技术革新了地震监控领域。然而，在处理信号退化或数据稀缺的情况下开发适用于多种复杂任务的基础模型仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究目的是提出一种新的基础模型SeisMoLLM，以提高在各种地震监测任务中的性能，并探索跨模态迁移的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过精心设计的波形标记和对预训练GPT-2模型进行微调的方式实现，不直接在地震数据集上进行预训练。该模型在DiTing和STEAD数据集中执行五个关键任务：后方角估计、震中距离估计、震级估计、相位选择以及首次运动极性分类。&lt;h4&gt;主要发现&lt;/h4&gt;SeisMoLLM在43个任务度量标准中有36项达到了最佳结果，在16个少量样本泛化度量标准中的12项上取得了顶级分数。相对改进幅度范围从10%到50%&lt;h4&gt;结论&lt;/h4&gt;研究表明，SeisMoLLM是一个具有前景的基础模型，适用于实际地震监测，并且展示了跨模态迁移作为地震研究中一个令人兴奋的新方向的潜力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度学习技术革新了地震监控领域。然而，在处理信号退化或数据稀缺的情况下开发适用于多种复杂任务的基础模型仍然具有挑战性。这项工作提出了一种名为SeisMoLLM的基础模型，它是第一个利用跨模态迁移进行地震监测的模型，该模型通过大规模预训练从大型语言模型中释放其潜力，并不直接在地震数据集上进行预训练。SeisMoLLM采用精细的波形标记化和对预先训练好的GPT-2模型进行微调的方式，在DiTing和STEAD数据集中执行五个关键任务：后方角估计、震中距离估计、震级估计、相位选择以及首次运动极性分类，并在43个任务度量标准中有36项达到了最佳结果，在16个少量样本泛化度量标准中的12项上取得了顶级分数，相对改进幅度范围从10%到50%。除了卓越的性能外，SeisMoLLM在训练和推理方面也保持了与轻量级模型相当甚至更好的效率。这些发现使SeisMoLLM成为具有前景的基础模型，适用于实际地震监测，并且展示了跨模态迁移作为地震研究中一个令人兴奋的新方向的潜力，突显了高级深度学习技术推动地震学研究发展的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/StarMoonWang/SeisMoLLM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning have revolutionized seismic monitoring, yetdeveloping a foundation model that performs well across multiple complex tasksremains challenging, particularly when dealing with degraded signals or datascarcity. This work presents SeisMoLLM, the first foundation model thatutilizes cross-modal transfer for seismic monitoring, to unleash the power oflarge-scale pre-training from a large language model without requiring directpre-training on seismic datasets. Through elaborate waveform tokenization andfine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-artperformance on the DiTing and STEAD datasets across five critical tasks:back-azimuth estimation, epicentral distance estimation, magnitude estimation,phase picking, and first-motion polarity classification. It attains 36 bestresults out of 43 task metrics and 12 top scores out of 16 few-shotgeneralization metrics, with many relative improvements ranging from 10% to50%. In addition to its superior performance, SeisMoLLM maintains efficiencycomparable to or even better than lightweight models in both training andinference. These findings establish SeisMoLLM as a promising foundation modelfor practical seismic monitoring and highlight cross-modal transfer as anexciting new direction for earthquake studies, showcasing the potential ofadvanced deep learning techniques to propel seismology research forward.</description>
      <author>example@mail.com (Xinghao Wang, Feng Liu, Rui Su, Zhihui Wang, Lei Bai, Wanli Ouyang)</author>
      <guid isPermaLink="false">2502.19960v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Open-Vocabulary Semantic Part Segmentation of 3D Human</title>
      <link>http://arxiv.org/abs/2502.19782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3DV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了首个能够处理3D人体的开放词汇分割方法。&lt;h4&gt;背景&lt;/h4&gt;传统的监督分割方法由于标注数据有限，在泛化到未见过的人体形状和类别上效果不佳。最近，视觉-语言模型在零样本能力上的进步推动了开放世界的3D分割方法的发展，但这些方法对3D人类的泛化效果不理想。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够根据文本提示将人体划分为期望的细粒度部分的方法。&lt;h4&gt;方法&lt;/h4&gt;采用了基于SAM的多视角提案生成和一个新颖的人体CLIP模型来创建视觉和文本输入的一致性嵌入。同时，还引入了一个简单的MaskFusion模块，该模块通过分类和融合多视图特征直接形成3D语义掩膜。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在各种3D人体数据集上，本方法优于当前最先进的开放词汇3D分割方法，并且可以应用于包括网格、点云和3D高斯散布在内的多种3D表示形式。&lt;h4&gt;结论&lt;/h4&gt;提出的方法为处理复杂的3D人体场景提供了一个强有力的解决方案，并展示了在未来的AR/VR应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的直接翻译，描述了3D部分分割是三维视觉和AR/VR领域的一个开放问题。由于标注数据有限，传统的监督方法无法很好地泛化到未见过的人体形状和类别上。通过利用先进视觉-语言模型的能力，该论文提出了一种新的处理3D人体的方法，并展示了其在多种3D表示形式上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D part segmentation is still an open problem in the field of 3D vision andAR/VR. Due to limited 3D labeled data, traditional supervised segmentationmethods fall short in generalizing to unseen shapes and categories. Recently,the advancement in vision-language models' zero-shot abilities has brought asurge in open-world 3D segmentation methods. While these methods show promisingresults for 3D scenes or objects, they do not generalize well to 3D humans. Inthis paper, we present the first open-vocabulary segmentation method capable ofhandling 3D human. Our framework can segment the human category into desiredfine-grained parts based on the textual prompt. We design a simple segmentationpipeline, leveraging SAM to generate multi-view proposals in 2D and proposing anovel HumanCLIP model to create unified embeddings for visual and textualinputs. Compared with existing pre-trained CLIP models, the HumanCLIP modelyields more accurate embeddings for human-centric contents. We also design asimple-yet-effective MaskFusion module, which classifies and fuses multi-viewfeatures into 3D semantic masks without complex voting and grouping mechanisms.The design of decoupling mask proposals and text input also significantlyboosts the efficiency of per-prompt inference. Experimental results on various3D human datasets show that our method outperforms current state-of-the-artopen-vocabulary 3D segmentation methods by a large margin. In addition, we showthat our method can be directly applied to various 3D representations includingmeshes, point clouds, and 3D Gaussian Splatting.</description>
      <author>example@mail.com (Keito Suzuki, Bang Du, Girish Krishnan, Kunyao Chen, Runfa Blark Li, Truong Nguyen)</author>
      <guid isPermaLink="false">2502.19782v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion</title>
      <link>http://arxiv.org/abs/2502.19854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的图像融合框架GIFNet，该框架通过低级视觉任务进行像素级别的监督，使得特征交互更加有效。&lt;h4&gt;背景&lt;/h4&gt;高级图像融合方法主要侧重于高层次的任务，在这些任务中，任务间的互动因语义差距而变得复杂，需要复杂的桥接机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图像融合方法，以简化特征之间的相互作用，并提高跨模态无监督融合的性能。&lt;h4&gt;方法&lt;/h4&gt;利用数字摄影中的低级视觉任务进行像素级别的监督，通过这种方式来实现更有效的特征交互和增强的任务共享特性学习。&lt;h4&gt;主要发现&lt;/h4&gt;GIFNet支持多种图像融合任务，并且在已见场景和未见过的场景中均表现出色。此外，该框架还能够用于单模态增强，为实际应用提供了更大的灵活性。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于低级视觉任务的像素级别监督方法提供了一种新的、强大的指导方式，无需依赖抽象语义即可实现多模式融合，并且在广泛的图像处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;先进的图像融合技术通常侧重于高层次的任务，在这些任务中，不同的任务之间的互动因为需要跨越较大的语义差距而变得困难。相比之下，我们提出了一种新的方法，利用数字摄影中的低级视觉任务来实现像素级别的监督和有效的特征交互。这种新的范式提供了强大的指导，可以无需依赖抽象的语义信息来进行跨模态无监督融合，并且增强了对于广泛适用性的任务共享特性学习。由于混合图像特性和增强的通用表示形式，提出的GIFNet支持多种不同的融合任务，在已见和未见过的情景中均表现出色。此外，实验结果还表明我们的框架能够支持单模态增强功能，从而为实际应用提供更优秀的灵活性。我们的代码可以在https://github.com/AWCXV/GIFNet上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced image fusion methods mostly prioritise high-level missions, wheretask interaction struggles with semantic gaps, requiring complex bridgingmechanisms. In contrast, we propose to leverage low-level vision tasks fromdigital photography fusion, allowing for effective feature interaction throughpixel-level supervision. This new paradigm provides strong guidance forunsupervised multimodal fusion without relying on abstract semantics, enhancingtask-shared feature learning for broader applicability. Owning to the hybridimage features and enhanced universal representations, the proposed GIFNetsupports diverse fusion tasks, achieving high performance across both seen andunseen scenarios with a single model. Uniquely, experimental results revealthat our framework also supports single-modality enhancement, offering superiorflexibility for practical applications. Our code will be available athttps://github.com/AWCXV/GIFNet.</description>
      <author>example@mail.com (Chunyang Cheng, Tianyang Xu, Zhenhua Feng, Xiaojun Wu, ZhangyongTang, Hui Li, Zeyang Zhang, Sara Atito, Muhammad Awais, Josef Kittler)</author>
      <guid isPermaLink="false">2502.19854v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;户外LiDAR点云三维实例分割是自动驾驶中的关键任务，但由于需要耗费大量人力来标注训练数据，因此该任务面临挑战。为了解决这一问题，提出了一个名为YoCo的框架，它使用稀疏的手工点击注释在俯视图平面上生成高质量的伪标签。&lt;h4&gt;背景&lt;/h4&gt;户外LiDAR点云三维实例分割对于自动驾驶至关重要，但手动标注训练数据的成本很高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来减少人工标注的工作量，并提高伪标签的质量和可靠性。&lt;h4&gt;方法&lt;/h4&gt;{'YoCo框架': '使用视觉基础模型与点云的几何约束结合生成高质量的伪标签；设计了一个基于时间和空间的更新模块，利用相邻帧的预测结果并考虑点云的密度变化来生成可靠的更新标签；提出了一种IoU引导增强模块，以高置信度和高交并比（IoU）的预测替换低质量的伪标签。', '实验验证': '在Waymo数据集上的实验证明了YoCo框架的有效性和广泛适用性。'}&lt;h4&gt;主要发现&lt;/h4&gt;YoCo在弱监督方法中取得了最先进的性能，并且在各种网络上使用少量完全标注的数据进行微调后，其性能可以与全监督方法相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效生成高质量伪标签的方法，显著降低了标注成本，适用于多种网络架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor LiDAR point cloud 3D instance segmentation is a crucial task inautonomous driving. However, it requires laborious human efforts to annotatethe point cloud for training a segmentation model. To address this challenge,we propose a YoCo framework, which generates 3D pseudo labels using minimalcoarse click annotations in the bird's eye view plane. It is a significantchallenge to produce high-quality pseudo labels from sparse annotations. OurYoCo framework first leverages vision foundation models combined with geometricconstraints from point clouds to enhance pseudo label generation. Second, atemporal and spatial-based label updating module is designed to generatereliable updated labels. It leverages predictions from adjacent frames andutilizes the inherent density variation of point clouds (dense near, sparsefar). Finally, to further improve label quality, an IoU-guided enhancementmodule is proposed, replacing pseudo labels with high-confidence and high-IoUpredictions. Experiments on the Waymo dataset demonstrate YoCo's effectivenessand generality, achieving state-of-the-art performance among weakly supervisedmethods and surpassing fully supervised Cylinder3D. Additionally, the YoCo issuitable for various networks, achieving performance comparable to fullysupervised methods with minimal fine-tuning using only 0.8% of the fullylabeled data, significantly reducing annotation costs.</description>
      <author>example@mail.com (Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng)</author>
      <guid isPermaLink="false">2502.19698v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification</title>
      <link>http://arxiv.org/abs/2502.19674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个可靠多模态分类方法MICINet，用于在存在噪音数据的情况下有效移除两种类型的噪声。&lt;h4&gt;背景&lt;/h4&gt;可靠的多模态学习尤其是在安全关键应用中是一个广受关注的问题。现有的许多方法只能处理特定模式或跨模态的噪音，而不能有效地处理这两种类型噪音的同时存在。&lt;h4&gt;目的&lt;/h4&gt;为了提高可靠性和应对上述挑战，提出了一种新的分类方法MICINet，该方法旨在统一并移除全球和个体水平上的干扰信息（ICI）。&lt;h4&gt;方法&lt;/h4&gt;MICINet通过全局ICI学习模块可靠地学习整体的ICI分布，并利用样本自适应跨模态信息补偿模块在个体层面上可靠地移除每个样本的噪声。此外，还引入了全局引导式样本ICILearning模块来高效去除全球级别的噪音。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种噪声条件下，MICINet优于其他最先进的可靠的多模态分类方法。&lt;h4&gt;结论&lt;/h4&gt;MICINet通过统一概念并有效移除干扰信息（ICI），成功地应对了现有可靠多模态学习方法的局限性，并在实际应用中展示了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable multimodal learning in the presence of noisy data is a widelyconcerned issue, especially in safety-critical applications. Many reliablemultimodal methods delve into addressing modality-specific or cross-modalitynoise. However, they fail to handle the coexistence of both types of noiseefficiently. Moreover, the lack of comprehensive consideration for noise atboth global and individual levels limits their reliability. To address theseissues, a reliable multimodal classification method dubbed Multi-LevelInter-Class Confusing Information Removal Network (MICINet) is proposed.MICINet achieves the reliable removal of both types of noise by unifying theminto the concept of Inter-class Confusing Information (\textit{ICI}) andeliminating it at both global and individual levels. Specifically, MICINetfirst reliably learns the global \textit{ICI} distribution through the proposed\textbf{\textit{Global \textbf{ICI} Learning Module}}. Then, it introduces the\textbf{\textit{Global-guided Sample ICI Learning module}} to efficientlyremove global-level \textit{ICI} from sample features utilizing the learnedglobal \textit{ICI} distribution. Subsequently, the\textbf{\textit{Sample-adaptive Cross-modality Information Compensationmodule}} is designed to remove individual-level \textit{ICI} from each samplereliably. This is achieved through interpretable cross-modality informationcompensation based on the complementary relationship between discriminativefeatures and \textit{ICI} and the perception of the relative quality ofmodalities introduced by the relative discriminative power. Experiments on fourdatasets demonstrate that MICINet outperforms other state-of-the-art reliablemultimodal classification methods under various noise conditions.</description>
      <author>example@mail.com (Tong Zhang, Shu Shen, C. L. Philip Chen)</author>
      <guid isPermaLink="false">2502.19674v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Machine Learning Approach for Yield Prediction in Chemical Reactions</title>
      <link>http://arxiv.org/abs/2502.19976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;机器学习模型在化学反应产率预测中的应用已成为近年来的重要研究方向。本文提出了一种新的时间高效和资源高效的预训练策略，以及一种分类后回归的模型(CFR)，用于解决不平衡及稀疏数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;化学语言表示法为化学反应提供了独特的视角，自然语言处理模型（如ULMFiT）可以在这种分布设置中定制以实现产率预测。然而，目前的数据集存在规模小、偏向高产率以及分布稀疏等问题。&lt;h4&gt;目的&lt;/h4&gt;开发新的预训练策略和CFR模型来提高化学反应产率预测的精度，特别是在不平衡和稀疏数据条件下。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含860多个手动收集自文献中的催化meta-C(sp2)-H键活化反应的新数据集。采用基于子结构从PubChem数据库中构建预训练数据集SSP1（含约0.11百万条目），并将其用于ULMFiT模型的微调。&lt;h4&gt;主要发现&lt;/h4&gt;CFR模型在预测目标化学反应产率时表现优秀，特别是在高产率和低产率两类上分别实现了RMSE为8.40和6.48的结果。这表明该方法不仅有效而且比传统的直接回归方法更优越，并且具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CFR模型结合ULMFiT-SSP1回归器能够提供最先进的产率预测，证明了这种方法在解决不平衡和稀疏数据挑战中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;开发用于化学反应产率预测的机器学习（ML）模型已成为近年来的重要研究领域。此类数据集面临的主要挑战源自于不平衡与稀疏性。本文中，作者使用化学语言表示来利用像ULMFiT这样的自然语言处理模型来进行产率预测，并且该方法针对分布设置进行了定制化开发。此外，贡献了一个新的反应数据集，其中包含超过860个从文献中手动提取的跨越十年的数据点，涉及一类高当代重要性的催化meta-C(sp2)-H键活化反应。考虑到数据集规模、偏向高产率以及稀疏性特点，作者提出了一种新型的时间和资源高效的预训练策略用于下游转移学习，并且开发了CFR模型以提供先进的产率预测能力，超越传统的直接回归方法。通过使用基于子结构的从PubChem数据库中提取0.11百万条目的SSP1预训练数据集，替代传统的大规模未标记分子（ChEMBL数据集中有约140万）的预训练惯例，发现这种更有效的时间和效率方法同样可以提供改进性能。ULMFiT-SSP1回归器在CFR模型下对目标反应产率预测表现出8.40和6.48的RMSE（分别对应于53%产率界限以上的高产率类和以下的低产率类）。此外，该方法展示出高度泛化的特性，并且在之前的数据集基准上取得了显著进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing machine learning (ML) models for yield prediction of chemicalreactions has emerged as an important use case scenario in very recent years.In this space, reaction datasets present a range of challenges mostly stemmingfrom imbalance and sparsity. Herein, we consider chemical languagerepresentations for reactions to tap into the potential of natural languageprocessing models such as the ULMFiT (Universal Language Model Fine Tuning) foryield prediction, which is customized to work across such distributionsettings. We contribute a new reaction dataset with more than 860 manuallycurated reactions collected from literature spanning over a decade, belongingto a family of catalytic meta-C(sp2)-H bond activation reactions of highcontemporary importance. Taking cognizance of the dataset size, skewness towardthe higher yields, and the sparse distribution characteristics, we developed anew (i) time- and resource-efficient pre-training strategy for downstreamtransfer learning, and (ii) the CFR (classification followed by regression)model that offers state-of-the-art yield predictions, surpassing conventionaldirect regression (DR) approaches. Instead of the prevailing pre-trainingpractice of using a large number of unlabeled molecules (1.4 million) from theChEMBL dataset, we first created a pre-training dataset SSP1 (0.11 million), byusing a substructure-based mining from the PubChem database, which is found tobe equally effective and more time-efficient in offering enhanced performance.The CFR model with the ULMFiT-SSP1 regressor achieved an impressive RMSE of8.40 for the CFR-major and 6.48 for the CFR-minor class in yield prediction onthe title reaction, with a class boundary of yield at 53 %. Furthermore, theCFR model is highly generalizable as evidenced by the significant improvementover the previous benchmark reaction datasets.</description>
      <author>example@mail.com (Supratim Ghosh, Nupur Jain, Raghavan B. Sunoj)</author>
      <guid isPermaLink="false">2502.19976v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Improving Representation Learning of Complex Critical Care Data with ICU-BERT</title>
      <link>http://arxiv.org/abs/2502.19593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for poster at GenAI4Health Workshop at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了ICU-BERT模型，这是一种基于Transformer架构的预训练模型，使用MIMIC-IV数据库进行多任务学习，以最小化的预处理步骤从复杂的ICU数据中提取出健壮表示。&lt;h4&gt;背景&lt;/h4&gt;现实世界临床数据的多元性和异步性对传统的AI决策支持系统提出了挑战。这些传统系统通常假设数据具有规律性和特征独立性，并且依赖于有限的数据范围和手动特征工程，而生成式AI技术在分析临床数据方面的潜力尚未得到充分开发。&lt;h4&gt;目的&lt;/h4&gt;提出ICU-BERT模型以提高复杂多变量的ICU数据表示的可解释性和通用性。&lt;h4&gt;方法&lt;/h4&gt;ICU-BERT采用多令牌输入策略，并结合生物医学大型语言模型的密集嵌入，通过MIMIC-IV数据库进行预训练，利用多种任务学习框架来实现这一目标。该模型能够处理结构化和非结构化的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估表明，ICU-BERT在五项任务及四个额外的ICU数据集中取得的结果优于或达到当前性能基准，通过微调技术可以进一步提升表现。&lt;h4&gt;结论&lt;/h4&gt;ICU-BERT模型推进了基础模型在医学信息学中的应用，并为各种临床决策支持提供了灵活解决方案。该模型表明，在处理复杂和多变量的数据时，采用预训练的Transformer架构能够有效地超越传统AI方法。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中重症监护病房（ICUs）等环境生成的多元异步医疗数据对传统的基于人工智能的决策支持系统提出了挑战。这些传统系统通常假设数据具有规律性且特征间独立，并依赖于有限的数据范围和手动特征工程。目前，生成式AI技术在分析临床数据方面尚未得到充分利用。我们介绍了ICU-BERT模型，这是一个基于Transformer架构的预训练模型，在MIMIC-IV数据库上通过多任务学习方案进行训练，从而实现复杂且多元化的重症监护病房（ICUs）数据的稳健表示，同时需要最小化预处理步骤。ICU-BERT采用了一种多令牌输入策略，并结合了生物医学大型语言模型的密集嵌入以获得通用性的表示方法。初步评估表明，在五项任务及四个额外的数据集上ICU-BERT的表现要么与当前性能基准持平或超越它们，通过微调技术可以进一步提高表现水平。此外，ICU-BERT将结构化和非结构化的数据结合起来，促进了基础模型在医学信息学中的应用，并为多种临床决策支持提供了灵活的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multivariate, asynchronous nature of real-world clinical data, such asthat generated in Intensive Care Units (ICUs), challenges traditional AI-baseddecision-support systems. These often assume data regularity and featureindependence and frequently rely on limited data scopes and manual featureengineering. The potential of generative AI technologies has not yet been fullyexploited to analyze clinical data. We introduce ICU-BERT, a transformer-basedmodel pre-trained on the MIMIC-IV database using a multi-task scheme to learnrobust representations of complex ICU data with minimal preprocessing. ICU-BERTemploys a multi-token input strategy, incorporating dense embeddings from abiomedical Large Language Model to learn a generalizable representation ofcomplex and multivariate ICU data. With an initial evaluation of five tasks andfour additional ICU datasets, ICU-BERT results indicate that ICU-BERT eithercompares to or surpasses current performance benchmarks by leveragingfine-tuning. By integrating structured and unstructured data, ICU-BERT advancesthe use of foundational models in medical informatics, offering an adaptablesolution for clinical decision support across diverse applications.</description>
      <author>example@mail.com (Ricardo Santos, André V. Carreiro, Xi Peng, Hugo Gamboa, Holger Fröhlich)</author>
      <guid isPermaLink="false">2502.19593v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions</title>
      <link>http://arxiv.org/abs/2502.19293v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures. arXiv admin note: text overlap with  arXiv:2502.19285&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发了一个专门针对皮肤黑素细胞病变的视觉-语言模型，并通过实验验证了该模型在生成普通痣病理报告方面的质量与医生写作相当。&lt;h4&gt;背景&lt;/h4&gt;每年有数百万黑色素细胞皮肤病灶被病理学家检查，大多数是普通的痣。虽然大部分病灶可以快速诊断，但撰写相应的病理报告非常耗时。&lt;h4&gt;目的&lt;/h4&gt;通过自动化部分报告编写过程来减轻病理学家的工作负担。&lt;h4&gt;方法&lt;/h4&gt;构建了一个遵循对比生成器框架的视觉-语言模型，并使用包含42,512张H&amp;E染色完整切片图像和19,645份对应病理报告的数据集进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在普通痣病理报告的质量评分上与病理学家书写的报告相当，但在罕见黑色素细胞病变亚型的报告生成方面更为困难。然而，在这些病例中的跨模态检索性能有所提高。&lt;h4&gt;结论&lt;/h4&gt;通过自动化的视觉-语言模型可以有效减轻病理医生撰写常见皮肤黑素瘤病灶报告的工作负担，并且在处理复杂或罕见类型时表现出色，尤其是在信息检索上的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;每年有数百万黑色素细胞皮肤病变由病理学家检查，其中大多数涉及常见的痣。尽管大部分病变可以在几秒钟内诊断出来，但编写相应的病理报告却非常耗时。因此，自动化部分报告撰写过程可以缓解不断增加的工作量。在这项研究中，我们开发了一个专门用于皮肤黑素细胞病变领域的视觉-语言模型。该模型遵循对比生成器框架，并使用42,512张H&amp;E染色全切片图像和19,645份相应病理报告的黑色素细胞病灶数据集进行训练和评估。实验结果显示，由模型生成的报告质量评分与专家病理学家书写的报告相当，尤其是在普通痣的情况下。尽管对于罕见的黑素瘤亚型来说报告生成更为困难，但在这些情况下跨模态检索性能有了显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of melanocytic skin lesions are examined by pathologists each year,the majority of which concern common nevi (i.e., ordinary moles). While most ofthese lesions can be diagnosed in seconds, writing the corresponding pathologyreport is much more time-consuming. Automating part of the report writingcould, therefore, alleviate the increasing workload of pathologists. In thiswork, we develop a vision-language model specifically for the pathology domainof cutaneous melanocytic lesions. The model follows the Contrastive Captionerframework and was trained and evaluated using a melanocytic lesion dataset of42,512 H&amp;E-stained whole slide images and 19,645 corresponding pathologyreports. Our results show that the quality scores of model-generated reportswere on par with pathologist-written reports for common nevi, assessed by anexpert pathologist in a reader study. While report generation revealed to bemore difficult for rare melanocytic lesion subtypes, the cross-modal retrievalperformance for these cases was considerably better.</description>
      <author>example@mail.com (Ruben T. Lucassen, Sander P. J. Moonemans, Tijn van de Luijtgaarden, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19293v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Training Robust Graph Neural Networks by Modeling Noise Dependencies</title>
      <link>http://arxiv.org/abs/2502.19670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的噪声场景依赖感知图噪声（DANG），并设计了一种新颖的鲁棒GNN模型DA-GNN，该模型可以处理节点特征、图结构和节点标签之间相互影响的噪声情况。&lt;h4&gt;背景&lt;/h4&gt;在现实应用中，图中的节点特性常常包含来自各种来源的噪声，这会导致基于图神经网络（GNN）的性能显著下降。现有的增强鲁棒性的方法假设噪声与图结构和节点标签独立，这一假设并不符合实际情况。&lt;h4&gt;目的&lt;/h4&gt;引入一种更实际的噪声场景——依赖感知图噪声（DANG），并提出相应的模型以提高在该情况下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个称为DA-GNN的新模型，利用变分推理捕捉数据生成过程中变量之间的因果关系，并设计了新的基准测试集来模拟现实中的DANG情形。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，无论是在依赖感知图噪声（DANG）还是传统的噪声模式下，提出的DA-GNN在各种噪音场景中都优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的DA-GNN模型及其新的基准测试集为解决基于图的机器学习中的鲁棒性问题提供了一种更实际的方法。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的应用中，图中的节点特征通常包含来自各种来源的噪声，这会导致GNN性能显著下降。尽管已经开发了几种增强鲁棒性的方法，但它们依赖于一个不切实际的假设：即节点特征中的噪声与图结构和节点标签独立。为此，我们引入了一种更现实的噪声场景——依赖感知图噪声（DANG），在这种情形下，节点特征中的噪声会在图结构和节点标签之间产生噪声依赖链。我们提出了一种新的鲁棒GNN模型DA-GNN，该模型使用变分推理捕捉数据生成过程中的因果关系。此外，我们还提出了模拟现实应用中DANG的新基准测试集，使更实际的关于鲁棒GNN的研究成为可能。广泛的实验表明，在各种噪声场景（包括DANG和传统噪声模型）下，DA-GNN始终优于现有的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world applications, node features in graphs often contain noise fromvarious sources, leading to significant performance degradation in GNNs.Although several methods have been developed to enhance robustness, they relyon the unrealistic assumption that noise in node features is independent of thegraph structure and node labels, thereby limiting their applicability. To thisend, we introduce a more realistic noise scenario, dependency-aware noise ongraphs (DANG), where noise in node features create a chain of noisedependencies that propagates to the graph structure and node labels. We proposea novel robust GNN, DA-GNN, which captures the causal relationships amongvariables in the data generating process (DGP) of DANG using variationalinference. In addition, we present new benchmark datasets that simulate DANG inreal-world applications, enabling more practical research on robust GNNs.Extensive experiments demonstrate that DA-GNN consistently outperforms existingbaselines across various noise scenarios, including both DANG and conventionalnoise models commonly considered in this field.</description>
      <author>example@mail.com (Yeonjun In, Kanghoon Yoon, Sukwon Yun, Kibum Kim, Sungchul Kim, Chanyoung Park)</author>
      <guid isPermaLink="false">2502.19670v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title>
      <link>http://arxiv.org/abs/2502.19697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一个新的攻击方法Attribute-aware Prompt Attack (AP-Attack)，用于探索和利用行人重识别模型的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;在安全监控系统中，人再识别（re-id）模型非常重要。基于视觉语言模型（VLM）的攻击显示出优秀的迁移性，但由于过分强调辨别语义而在整体表示上缺乏全面的功能破坏。&lt;h4&gt;目的&lt;/h4&gt;提出一个新型方法AP-Attack，利用VLM图像文本对齐能力来明确干扰行人图像中的细粒度语义特征。&lt;h4&gt;方法&lt;/h4&gt;通过设计文本逆向网络将行人图像映射到代表语义嵌入的伪令牌上，并在对比学习方式下训练这些网络。使用预定义的提示模板描述具体的行人属性，以获得个性化的文本描述。&lt;h4&gt;主要发现&lt;/h4&gt;AP-Attack 方法显著提升了对抗样本的迁移性，在跨模型和数据集攻击场景中平均Drop Rate比先前的方法提高了22.9%。&lt;h4&gt;结论&lt;/h4&gt;AP-Attack 通过全面干扰细粒度语义特征，增强了对行人重识别系统的攻击能力。&lt;h4&gt;翻译&lt;/h4&gt;人再识别（re-id）模型在安全监控系统中至关重要。最近基于视觉语言模型的攻击显示出优秀的迁移性，但因为过分强调辨别语义而缺乏对整体表示的全面破坏。本文引入了一种新的方法Attribute-aware Prompt Attack (AP-Attack)，利用VLM的图像文本对齐能力，通过摧毁特定属性的文本嵌入来明确地干扰行人图像中的细粒度语义特征。设计了用于映射行人图像到代表语义嵌入的伪令牌上的文本逆向网络，并以对比学习方式训练这些网络，在预定义提示模板下具体描述行人属性。通过这种个性化的文本描述，AP-Attack 方法有效增强了攻击能力，使对抗样本具有更高的迁移性。实验表明，该方法在跨模型和数据集攻击场景中的平均Drop Rate比先前的方法提高了22.9%，达到了最先进的迁移水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (re-id) models are vital in security surveillancesystems, requiring transferable adversarial attacks to explore thevulnerabilities of them. Recently, vision-language models (VLM) based attackshave shown superior transferability by attacking generalized image and textualfeatures of VLM, but they lack comprehensive feature disruption due to theoveremphasis on discriminative semantics in integral representation. In thispaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novelmethod that leverages VLM's image-text alignment capability to explicitlydisrupt fine-grained semantic features of pedestrian images by destroyingattribute-specific textual embeddings. To obtain personalized textualdescriptions for individual attributes, textual inversion networks are designedto map pedestrian images to pseudo tokens that represent semantic embeddings,trained in the contrastive learning manner with images and a predefined prompttemplate that explicitly describes the pedestrian attributes. Inverted benignand adversarial fine-grained textual semantics facilitate attacker ineffectively conducting thorough disruptions, enhancing the transferability ofadversarial examples. Extensive experiments show that AP-Attack achievesstate-of-the-art transferability, significantly outperforming previous methodsby 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios.</description>
      <author>example@mail.com (Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.19697v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixtera: A Data Plane for Foundation Model Training</title>
      <link>http://arxiv.org/abs/2502.19790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;大型语言和视觉模型通过汇集来自多种来源的万亿级标记进行训练。随着训练数据集的增长，手动管理样本变得耗时、繁琐且容易出错。&lt;h4&gt;目的&lt;/h4&gt;构建并展示一个名为Mixtera的数据平面工具，该工具用于基础模型训练，使用户可以声明性地表达在训练期间哪些数据样本应该以何种比例和顺序使用。&lt;h4&gt;方法&lt;/h4&gt;Mixtera是一个独立于文件系统结构的集中式、只读层，可部署在现有训练数据集之上，并支持跨任意属性（如语言、源数据集）的数据混合及基于模型反馈动态调整混合策略的功能。&lt;h4&gt;主要发现&lt;/h4&gt;实验性地评估了Mixtera，证明我们的实现不会成为瓶颈且能够扩展到256个GH200超级芯片。通过在系统中实施提议的自适应数据优化(ADO)算法并评估其性能影响来展示如何支持最近的数据混合策略改进。&lt;h4&gt;结论&lt;/h4&gt;探索了视觉语言模型中的混合作用，为训练大规模基础模型提供了灵活、可扩展和易于使用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最先进的大型语言和视觉模型通过来自各种来源的万亿级标记进行训练。随着训练数据集的增长，手动管理样本变得耗时、繁琐且容易出错。然而，最近的研究表明，在训练期间的数据混合及访问样本顺序可以显著影响模型精度。我们建立并展示了Mixtera——一个用于基础模型训练的数据平面工具，它使用户能够声明性地表达在训练过程中应使用哪些数据样本及其比例和顺序。Mixtera是一个独立于文件系统结构的集中式、只读层，可部署在现有训练数据集之上，并支持跨任意属性（如语言、源数据集）的数据混合以及基于模型反馈动态调整混合策略的功能。我们实验性地评估了Mixtera并显示我们的实现不会成为瓶颈且能够扩展到256个GH200超级芯片。通过在系统中实施提议的自适应数据优化(ADO)算法并评估其性能影响来展示如何支持最近的数据混合策略改进，并探索视觉语言模型中的混合作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art large language and vision models are trained over trillionsof tokens that are aggregated from a large variety of sources. As training datacollections grow, manually managing the samples becomes time-consuming,tedious, and prone to errors. Yet recent research shows that the data mixtureand the order in which samples are visited during training can significantlyinfluence model accuracy. We build and present Mixtera, a data plane forfoundation model training that enables users to declaratively express whichdata samples should be used in which proportion and in which order duringtraining. Mixtera is a centralized, read-only layer that is deployed on top ofexisting training data collections and can be declaratively queried. Itoperates independently of the filesystem structure and supports mixtures acrossarbitrary properties (e.g., language, source dataset) as well as dynamicadjustment of the mixture based on model feedback. We experimentally evaluateMixtera and show that our implementation does not bottleneck training andscales to 256 GH200 superchips. We demonstrate how Mixtera supports recentadvancements in mixing strategies by implementing the proposed Adaptive DataOptimization (ADO) algorithm in the system and evaluating its performanceimpact. We also explore the role of mixtures for vision-language models.</description>
      <author>example@mail.com (Maximilian Böther, Xiaozhe Yao, Tolga Kerimoglu, Ana Klimovic)</author>
      <guid isPermaLink="false">2502.19790v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation</title>
      <link>http://arxiv.org/abs/2502.19285v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了病理图像与语言模型在处理病理性数据时，选取不同类型的信息对多模态表示和自动生成报告质量的影响。&lt;h4&gt;背景&lt;/h4&gt;当前许多视觉-语言模型训练所使用的病理报告包含从配对全玻片图像无法推断出的信息（如患者历史），这可能导致生成报告中出现幻觉句子。&lt;h4&gt;目的&lt;/h4&gt;研究病理报告中选择何种信息用于视觉-语言建模会影响多模态表示和自动生成报告的质量。&lt;h4&gt;方法&lt;/h4&gt;构建了基于BLIP-2框架的模型，利用42,433张HE染色全玻片图像及19,636份对应病理报告进行实验。比较了一个训练在完整报告上的模型与一个仅使用描述细胞和组织外观句子的预处理报告上训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;文本预处理能够有效防止生成报告中的幻觉，然而，完全报告训练模型在跨模态检索性能方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;对于视觉-语言病理模型而言，在选择用于训练的数据时需权衡自动生成报告的质量和跨模态检索性能之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在病理性研究中能够实现多模态病例检索及自动化报告生成。然而，现有的许多模型都基于包含从配对全玻片图像无法推断出的信息（如患者历史）的病理报告训练而成，可能导致幻觉句子出现在所生成的报告中。本文探讨了选取不同信息类型用于视觉语言建模如何影响多模态表示和自动生成报告的质量。具体而言，比较了一个基于完整报告训练的模型与一个仅使用描述细胞组织外观的HE染色玻片报告上预处理后文本训练出的模型的表现。实验是在BLIP-2框架基础上建立的，利用了一套包含42,433张HE染色全玻片图像和19,636份对应病理报告的数据集进行评估。通过图片到文字以及文字到图片的检索性能，并由专家病理科医生对生成报告进行了定性评价来衡量模型表现。结果表明，文本预处理能预防幻觉在报告生成中的出现。尽管生成报告的质量有所提升，但基于完整报告训练视觉语言模型在跨模态检索性能上仍表现出更好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models in pathology enable multimodal case retrieval andautomated report generation. Many of the models developed so far, however, havebeen trained on pathology reports that include information which cannot beinferred from paired whole slide images (e.g., patient history), potentiallyleading to hallucinated sentences in generated reports. To this end, weinvestigate how the selection of information from pathology reports forvision-language modeling affects the quality of the multimodal representationsand generated reports. More concretely, we compare a model trained on fullreports against a model trained on preprocessed reports that only includesentences describing the cell and tissue appearances based on the H&amp;E-stainedslides. For the experiments, we built upon the BLIP-2 framework and used acutaneous melanocytic lesion dataset of 42,433 H&amp;E-stained whole slide imagesand 19,636 corresponding pathology reports. Model performance was assessedusing image-to-text and text-to-image retrieval, as well as qualitativeevaluation of the generated reports by an expert pathologist. Our resultsdemonstrate that text preprocessing prevents hallucination in reportgeneration. Despite the improvement in the quality of the generated reports,training the vision-language model on full reports showed better cross-modalretrieval performance.</description>
      <author>example@mail.com (Ruben T. Lucassen, Tijn van de Luijtgaarden, Sander P. J. Moonemans, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19285v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.19796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 tables, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架，即转移顺序蒙特卡洛（Transfer Sequential Monte Carlo），用于评估和改进基于幂先验的贝叶斯迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;在数据稀缺的情况下，从相关数据集中引入信息可以改善对观测数据的推断。当前的贝叶斯迁移学习方法主要依赖于所谓的幂先验来适应性地转移相关信息。&lt;h4&gt;目的&lt;/h4&gt;评估和改进现有的基于幂先验的贝叶斯迁移学习方法，并通过留一交叉验证在目标数据集上评价其效果。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的框架，即转移顺序蒙特卡洛（TSMC），该方法可以有效地选择传输参数并避免了对共轭先验的需求。此外，使用留一法交叉验证来评估贝叶斯迁移学习的方法性能。&lt;h4&gt;主要发现&lt;/h4&gt;提出的TSMC框架在两项全面的模拟研究中表现出了优越性，并且能够有效提升推断准确性。&lt;h4&gt;结论&lt;/h4&gt;通过新的TSMC框架改进了现有基于幂先验的贝叶斯迁移学习方法，为解决数据稀缺问题提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;更新先验信息以适应观测到的数据是贝叶斯推理的核心。贝叶斯迁移学习则扩展这一理念，将相关信息从相关数据集中引入，来改善在某些条件下可能略有不同的观测数据的推断。当前基于所谓的幂先验的贝叶斯迁移学习方法可以自适应地转移相关信息。然而，并非总能清楚这种技术在哪种情况下表现最佳或是否能够改进贝叶斯推理。现有的幂先验方法依赖于共轭关系来评估感兴趣的后验分布。我们提出了一种使用目标数据集上的留一交叉验证作为评估贝叶斯迁移学习的方法。此外，引入了新的框架，即转移顺序蒙特卡洛（TSMC），用于幂先验方法，并高效地选择传输参数同时避免了对共轭先验的需求。在两项全面的模拟研究中我们评估了所提议方法的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Updating $\textit{a priori}$ information given some observed data is the coretenet of Bayesian inference. Bayesian transfer learning extends this idea byincorporating information from a related dataset to improve the inference onthe observed data which may have been collected under slightly differentsettings. The use of related information can be useful when the observed datais scarce, for example. Current Bayesian transfer learning methods that arebased on the so-called $\textit{power prior}$ can adaptively transferinformation from related data. Unfortunately, it is not always clear underwhich scenario Bayesian transfer learning performs best or even if it willimprove Bayesian inference. Additionally, current power prior methods rely onconjugacy to evaluate the posterior of interest. We propose using leave-one-outcross validation on the target dataset as a means of evaluating Bayesiantransfer learning methods. Further, we introduce a new framework,$\textit{transfer sequential Monte Carlo}$, for power prior approaches thatefficiently chooses the transfer parameter while avoiding the need forconjugate priors. We assess the performance of our proposed methods in twocomprehensive simulation studies.</description>
      <author>example@mail.com (Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi)</author>
      <guid isPermaLink="false">2502.19796v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>PhenoProfiler: Advancing Phenotypic Learning for Image-based Drug Discovery</title>
      <link>http://arxiv.org/abs/2502.19568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhenoProfiler是一个用于药物发现领域的创新模型，旨在从多通道整张图像中直接提取形态表示，并通过大规模数据集的评估证明其在准确性和鲁棒性上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;现有的基于图片的药物发现方法需要复杂的多步骤计算过程，这会引入效率低下、限制泛化能力和增加潜在错误的问题。&lt;h4&gt;目的&lt;/h4&gt;提出PhenoProfiler模型以解决上述挑战，提高形态表示学习中的效率和准确性，并增强其鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;PhenoProfiler是一个端到端工具，直接处理多通道整张图像并生成低维定量表示。它包含一个多目标学习模块来提升在形态表示学习方面的准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过大规模公开数据集的评估，证明了PhenoProfiler比现有方法性能更好，在准确性和鲁棒性方面有显著提高，特别是在识别生物有意义信号的能力上表现突出。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，PhenoProfiler是一个可扩展、泛化和稳健的学习表型变化的工具。&lt;h4&gt;翻译&lt;/h4&gt;在药物发现领域中，捕捉细胞对不同治疗方式反应至关重要。然而现有方法需要复杂的多步骤计算过程，导致效率低下等问题。我们提出了一种名为PhenoProfiler的新模型，它可以高效地从图像中提取形态表示，并通过大规模数据集证明其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of image-based drug discovery, capturing the phenotypic responseof cells to various drug treatments and perturbations is a crucial step.However, existing methods require computationally extensive and complexmulti-step procedures, which can introduce inefficiencies, limitgeneralizability, and increase potential errors. To address these challenges,we present PhenoProfiler, an innovative model designed to efficiently andeffectively extract morphological representations, enabling the elucidation ofphenotypic changes induced by treatments. PhenoProfiler is designed as anend-to-end tool that processes whole-slide multi-channel images directly intolow-dimensional quantitative representations, eliminating the extensivecomputational steps required by existing methods. It also includes amulti-objective learning module to enhance robustness, accuracy, andgeneralization in morphological representation learning. PhenoProfiler isrigorously evaluated on large-scale publicly available datasets, including over230,000 whole-slide multi-channel images in end-to-end scenarios and more than8.42 million single-cell images in non-end-to-end settings. Across thesebenchmarks, PhenoProfiler consistently outperforms state-of-the-art methods byup to 20%, demonstrating substantial improvements in both accuracy androbustness. Furthermore, PhenoProfiler uses a tailored phenotype correctionstrategy to emphasize relative phenotypic changes under treatments,facilitating the detection of biologically meaningful signals. UMAPvisualizations of treatment profiles demonstrate PhenoProfiler ability toeffectively cluster treatments with similar biological annotations, therebyenhancing interpretability. These findings establish PhenoProfiler as ascalable, generalizable, and robust tool for phenotypic learning.</description>
      <author>example@mail.com (Bo Li, Bob Zhang, Chengyang Zhang, Minghao Zhou, Weiliang Huang, Shihang Wang, Qing Wang, Mengran Li, Yong Zhang, Qianqian Song)</author>
      <guid isPermaLink="false">2502.19568v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixtraining: A Better Trade-Off Between Compute and Performance</title>
      <link>http://arxiv.org/abs/2502.19513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;混合训练（MixTraining）是一种在单一训练阶段内交替执行自监督学习和有监督学习的方法，旨在提高模型性能并减少计算开销。&lt;h4&gt;背景&lt;/h4&gt;将自监督学习应用于数据限制场景中以增强模型表现已成为一种常见策略，但这种方法带来了计算资源与模型性能之间的权衡：尽管提高了表示能力，却增加了额外的训练阶段，导致效率降低。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法在时间和计算资源上的瓶颈问题，提出了一种新的混合框架（MixTraining），旨在提高自监督学习和有监督学习之间的协同作用，并减少共享步骤的计算负担。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新型框架——混合训练（MixTraining），该框架在一个统一的训练阶段内交替执行SSL和SL的几个周期。同时，在两个学习目标之间引入了平滑过渡机制，使得模型能够在单一训练过程中高效完成自监督和有监督任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统方法相比，混合训练在多个数据集上提供了更好的计算性能权衡，并且可以在不牺牲精度的情况下加快训练速度。例如，在TinyImageNet数据集中实现了8.81%的绝对准确率提升（相对于基准而言提高了18.89%），同时使用ViT-Tiny模型时可加速多达1.29倍。&lt;h4&gt;结论&lt;/h4&gt;混合训练方法证明了其在多种任务环境下的有效性和高效性，特别是在资源受限情况下。它为解决当前深度学习研究中计算效率与性能之间的矛盾提供了一种新的视角和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将自监督学习（SSL）整合到标准有监督学习之前已成为一种广泛使用的策略来提升模型表现，尤其是在数据有限的情况下。然而，这种方法引入了计算资源和性能之间的权衡：虽然自监督学习有助于表示能力的提高，但需要额外的训练阶段，增加了计算开销并限制了资源受限条件下的效率。为了应对这些挑战，我们提出了混合训练（MixTraining），这是一种新的框架，在统一的训练过程中交替执行多个SSL和SL周期，并在两个学习目标之间实现平滑过渡。这种方法增强了自监督学习与有监督学习之间的协同作用以提高精度，并通过减少共享计算步骤来降低计算开销。混合训练具有广泛适用性，适用于单任务和多任务场景。广泛的实验表明，相对于传统流水线而言，MixTraining提供了更好的计算性能权衡，在TinyImageNet数据集中实现了8.81%的绝对准确率提升（比基准提高了18.89%），使用ViT-Tiny模型时可加速多达1.29倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incorporating self-supervised learning (SSL) before standard supervisedlearning (SL) has become a widely used strategy to enhance model performance,particularly in data-limited scenarios. However, this approach introduces atrade-off between computation and performance: while SSL helps withrepresentation learning, it requires a separate, often time-consuming trainingphase, increasing computational overhead and limiting efficiency inresource-constrained settings. To address these challenges, we proposeMixTraining, a novel framework that interleaves several SSL and SL epochswithin a unified mixtraining training phase, featuring a smooth transitionbetween two learning objectives. MixTraining enhances synergy between SSL andSL for improved accuracy and consolidates shared computation steps to reducecomputation overhead. MixTraining is versatile and applicable to bothsingle-task and multi-task learning scenarios. Extensive experimentsdemonstrate that MixTraining offers a superior compute-performance trade-offcompared to conventional pipelines, achieving an 8.81% absolute accuracy gain(18.89% relative accuracy gain) on the TinyImageNet dataset while acceleratingtraining by up to 1.29x  with the ViT-Tiny model.</description>
      <author>example@mail.com (Zexin Li, Jiancheng Zhang, Yinglun Zhu, Cong Liu)</author>
      <guid isPermaLink="false">2502.19513v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Dictionary-based Framework for Interpretable and Consistent Object Parsing</title>
      <link>http://arxiv.org/abs/2502.19540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoCal是一种基于字典式掩码变换器的可解释且一致的对象解析框架，它重新思考了现有的聚类基掩码变换器架构。&lt;h4&gt;背景&lt;/h4&gt;当前的对象分割方法通常依赖于将图像划分为多个部分并为每个部分分配一个特定的语义类别。然而，这种方法在保持不同对象间的一致性和逻辑关系方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架CoCal，以提高对象解析的质量和一致性，并增强对逻辑关系的理解。&lt;h4&gt;方法&lt;/h4&gt;CoCal采用了一种基于字典组件的方法，每个组件都与特定的语义类别相关联。通过引入层次化的字典组件结构，以及在同级内的对比成分和跨级别的逻辑约束，CoCal能够实现更精确的对象解析。&lt;h4&gt;主要发现&lt;/h4&gt;CoCal通过引入一种逐部件对比算法和跨级别对比学习目标，在PartImageNet和Pascal-Part-108数据集上实现了新的最佳性能。此外，它在对象级度量标准方面也有显著改进。&lt;h4&gt;结论&lt;/h4&gt;与先前的方法相比，CoCal提供了更准确的分割结果，并且能够更好地保持不同对象之间的逻辑关系。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了一种基于字典式掩码变换器的可解释和一致的对象解析框架——CoCal。该框架围绕对比成分和逻辑约束设计，重新思考了现有的聚类基掩码变换器架构。具体而言，CoCal利用一组字典组件，每个组件都与特定的语义类别明确相关联。为推进这一概念，CoCal引入了一个层次化的字典组件结构，通过整合同级别内的对比成分和跨级别的逻辑约束来实现这一点。具体来说，CoCal在每一级上采用逐部件对比算法，使同一类别的字典组件与其不同类别的字典组件进行比较。此外，为了处理逻辑问题，CoCal确保表示特定部分的字典组件比其他对象更接近于相应的物体组件。为进一步增强逻辑关系建模，我们实现了一种后处理函数，该函数基于一个原理：分配给某个部分的像素也应该分配给其对应的对象。通过这些创新，CoCal在PartImageNet和Pascal-Part-108上建立了新的最佳性能，在部分mIoU方面分别比先前方法高出2.08%和0.70%，并且在整个质量的对象分割上也有显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present CoCal, an interpretable and consistent objectparsing framework based on dictionary-based mask transformer. Designed aroundContrastive Components and Logical Constraints, CoCal rethinks existingcluster-based mask transformer architectures used in segmentation;Specifically, CoCal utilizes a set of dictionary components, with eachcomponent being explicitly linked to a specific semantic class. To advance thisconcept, CoCal introduces a hierarchical formulation of dictionary componentsthat aligns with the semantic hierarchy. This is achieved through theintegration of both within-level contrastive components and cross-level logicalconstraints. Concretely, CoCal employs a component-wise contrastive algorithmat each semantic level, enabling the contrasting of dictionary componentswithin the same class against those from different classes. Furthermore, CoCaladdresses logical concerns by ensuring that the dictionary componentrepresenting a particular part is closer to its corresponding object componentthan to those of other objects through a cross-level contrastive learningobjective. To further enhance our logical relation modeling, we implement apost-processing function inspired by the principle that a pixel assigned to apart should also be assigned to its corresponding object. With theseinnovations, CoCal establishes a new state-of-the-art performance on bothPartImageNet and Pascal-Part-108, outperforming previous methods by asignificant margin of 2.08% and 0.70% in part mIoU, respectively. Moreover,CoCal exhibits notable enhancements in object-level metrics across thesebenchmarks, highlighting its capacity to not only refine parsing at a finerlevel but also elevate the overall quality of object segmentation.</description>
      <author>example@mail.com (Tiezheng Zhang, Qihang Yu, Alan Yuille, Ju He)</author>
      <guid isPermaLink="false">2502.19540v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Tell me why: Visual foundation models as self-explainable classifiers</title>
      <link>http://arxiv.org/abs/2502.19577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;视觉基础模型（VFMs）由于其先进的性能而越来越受欢迎。然而，可解释性对于关键应用仍然至关重要。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型在许多任务中表现出色，但它们的决策过程往往缺乏透明度和理解力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型自解释模型(SEM)，旨在提供能够分解预测为一系列可解释概念加权和的分类器。&lt;h4&gt;方法&lt;/h4&gt;结合了VFM与原型架构及专门训练目标。通过在冻结状态下的VFMs之上仅微调一个轻量级头（约1百万参数），实现了一种高效且可解释性的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;评估显示该方法能实现竞争性的分类性能，同时在一系列文献衍生出的可解释性指标上超越现有模型。&lt;h4&gt;结论&lt;/h4&gt;研究提出的方法ProtoFM提供了高效的、可解释的视觉基础模型框架，并展示了其在实际应用中的潜力和效果。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型（VFMs）因其卓越的表现而变得越来越流行。然而，在关键的应用场景中，依然需要重视可解释性问题。因此，自解释模型(SEM)试图提供一种分解预测为一系列可解释概念加权和的分类器形式。尽管这些模型显示出潜力，但最近的研究表明它们所提供的解释往往缺乏忠实度。为此，本工作将视觉基础模型与新颖的原型架构结合，并采用专门训练目标进行优化。通过仅在冻结状态下的VFMs之上微调一个轻量级头（约1百万参数），研究提出的方法(ProtoFM)提供了一种高效的、可解释性的解决方案。评估结果表明该方法能够实现竞争性分类性能，同时在一系列从文献中衍生出的可解释性指标上超越现有模型。相关代码可在https://github.com/hturbe/proto-fm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual foundation models (VFMs) have become increasingly popular due to theirstate-of-the-art performance. However, interpretability remains crucial forcritical applications. In this sense, self-explainable models (SEM) aim toprovide interpretable classifiers that decompose predictions into a weightedsum of interpretable concepts. Despite their promise, recent studies have shownthat these explanations often lack faithfulness. In this work, we combine VFMswith a novel prototypical architecture and specialized training objectives. Bytraining only a lightweight head (approximately 1M parameters) on top of frozenVFMs, our approach (ProtoFM) offers an efficient and interpretable solution.Evaluations demonstrate that our approach achieves competitive classificationperformance while outperforming existing models across a range ofinterpretability metrics derived from the literature. Code is available athttps://github.com/hturbe/proto-fm.</description>
      <author>example@mail.com (Hugues Turbé, Mina Bjelogrlic, Gianmarco Mengaldo, Christian Lovis)</author>
      <guid isPermaLink="false">2502.19577v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.19247v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 3 figures. Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Proxy Transformation的方法，该方法适用于多模态任务，并可以有效地改善点云的流形结构。这种方法利用了Deformable Point Clustering来识别目标区域中的点云子流形，然后提出了一个使用多模式代理指导点云变换的Proxy Attention模块。&lt;h4&gt;背景&lt;/h4&gt;以语言指令为基础与3D环境实时交互是具身智能的基础任务之一。然而，从RGB-D图像渲染出来的点云包含了大量冗余的背景数据和固有的噪声，这些都可能干扰目标区域的真实结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来改善点云流形结构以适应实时任务的需求。&lt;h4&gt;方法&lt;/h4&gt;该方法首先利用Deformable Point Clustering技术识别出点云子流形，接着使用Proxy Attention模块以及通过文本信息全局指导不同子流形的转换向量优化目标区域的相对空间关系。同时，图像信息引导每个子流形内的线性变换，以细化目标区域的本地点云流形。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在容易的目标上比现有所有方法提高了7.49%，在困难的目标上提高了4.60%；同时还减少了注意力块的计算开销达40.6%。&lt;h4&gt;结论&lt;/h4&gt;这种方法确立了新的SOTA（State of the Art）结果，在以自我为中心的3D视觉定位中展示了其有效性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;具身智能要求代理能够根据语言指令实时地与三维环境进行交互。在这一领域的一个基础任务是自我中心的3D视觉定位，然而，从RGB-D图像渲染出来的点云保留了大量的冗余背景数据和固有的噪声，这些都可能干扰目标区域的真实结构。现有的点云增强方法通常需要繁琐的过程来改进流形结构，并且不适合实时任务。我们提出了一种适用于多模态任务的Proxy Transformation方法，可以有效地改善点云流形结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence requires agents to interact with 3D environments inreal time based on language instructions. A foundational task in this domain isego-centric 3D visual grounding. However, the point clouds rendered from RGB-Dimages retain a large amount of redundant background data and inherent noise,both of which can interfere with the manifold structure of the target regions.Existing point cloud enhancement methods often require a tedious process toimprove the manifold, which is not suitable for real-time tasks. We proposeProxy Transformation suitable for multimodal task to efficiently improve thepoint cloud manifold. Our method first leverages Deformable Point Clustering toidentify the point cloud sub-manifolds in target regions. Then, we propose aProxy Attention module that utilizes multimodal proxies to guide point cloudtransformation. Built upon Proxy Attention, we design a submanifoldtransformation generation module where textual information globally guidestranslation vectors for different submanifolds, optimizing relative spatialrelationships of target regions. Simultaneously, image information guideslinear transformations within each submanifold, refining the local point cloudmanifold of target regions. Extensive experiments demonstrate that ProxyTransformation significantly outperforms all existing methods, achieving animpressive improvement of 7.49% on easy targets and 4.60% on hard targets,while reducing the computational overhead of attention blocks by 40.6%. Theseresults establish a new SOTA in ego-centric 3D visual grounding, showcasing theeffectiveness and robustness of our approach.</description>
      <author>example@mail.com (Qihang Peng, Henry Zheng, Gao Huang)</author>
      <guid isPermaLink="false">2502.19247v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>TRIX: A More Expressive Model for Zero-shot Domain Transfer in Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2502.19512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;全归纳知识图谱模型可以在多个领域上进行训练，并随后在新的未见领域中执行零样本知识图谱补全(KGC)，这是朝着构建知识图谱基础模型的目标迈进的重要一步。&lt;h4&gt;目的&lt;/h4&gt;介绍一种更富有表现力和能力的全归纳模型TRIX，该模型不仅提供了比现有最佳方法更具表达性的三元组嵌入（头实体、关系、尾实体），还引入了一种新的能力：直接处理归纳设置下的实体和关系预测任务。&lt;h4&gt;方法&lt;/h4&gt;TRIX通过改进的三元组表示方式提高了零样本场景下的性能，同时可以直接应对新领域中的实体与关系预测任务，并且其表现优于现有的全归纳模型以及大上下文语言模型在未见领域的预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TRIX在新的域中以零样本的方式进行实体和关系预测方面优于现有最佳的全归纳模型；同时，在超出训练集范围的领域预测任务上也表现出比大型上下文LLM更好的性能。&lt;h4&gt;结论&lt;/h4&gt;TRIX展示出了强大的零样本泛化能力以及应对新领域挑战的实力，对于构建通用知识图谱基础模型具有重要意义。源代码可以在GitHub上找到：https://github.com/yuchengz99/TRIX。&lt;h4&gt;翻译&lt;/h4&gt;完全归纳的知识图谱模型可以针对多个领域进行训练，并在新的未见领域中执行零样本知识图谱补全任务，这是向构建知识图谱基础模型迈进的重要一步。在这项工作中，我们引入了一种更为表达力强且能力全面的TRIX模型，它不仅提供了比现有最佳方法更强大的三元组嵌入（头实体、关系、尾实体），而且还引入了一种新的功能：直接处理归纳设置下的实体和关系预测任务。经验上表明，TRIX在新领域中的零样本实体与关系预测中优于现有的全归纳模型，并且在超出训练范围的领域预测中也超过了大型上下文LLM。源代码可从https://github.com/yuchengz99/TRIX获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fully inductive knowledge graph models can be trained on multiple domains andsubsequently perform zero-shot knowledge graph completion (KGC) in new unseendomains. This is an important capability towards the goal of having foundationmodels for knowledge graphs. In this work, we introduce a more expressive andcapable fully inductive model, dubbed TRIX, which not only yields strictly moreexpressive triplet embeddings (head entity, relation, tail entity) compared tostate-of-the-art methods, but also introduces a new capability: directlyhandling both entity and relation prediction tasks in inductive settings.Empirically, we show that TRIX outperforms the state-of-the-art fully inductivemodels in zero-shot entity and relation predictions in new domains, andoutperforms large-context LLMs in out-of-domain predictions. The source code isavailable at https://github.com/yuchengz99/TRIX.</description>
      <author>example@mail.com (Yucheng Zhang, Beatrice Bevilacqua, Mikhail Galkin, Bruno Ribeiro)</author>
      <guid isPermaLink="false">2502.19512v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SPU-IMR: Self-supervised Arbitrary-scale Point Cloud Upsampling via Iterative Mask-recovery Network</title>
      <link>http://arxiv.org/abs/2502.19452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的点云上采样方法，该方法将点云上采样视为全局形状补全问题。通过设计的神经网络迭代地完成缺失部分，以获得稠密且均匀分布的点集。&lt;h4&gt;背景&lt;/h4&gt;现有的点云上采样方法通常将其视为插值问题，通过在局部或特征空间中进行插值来实现上采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的全局形状补全算法来解决点云上采样的问题，并展示其优于现有自监督和有监督方法的性能。&lt;h4&gt;方法&lt;/h4&gt;首先将点云划分成多个块，然后通过掩码操作移除一些块，留下可见的点云块。使用设计好的神经网络迭代地完成缺失部分，直至恢复所有补全后的块。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在量化和定性实验中展示了优越性能，优于现有的自监督和有监督方法。&lt;h4&gt;结论&lt;/h4&gt;通过将点云上采样视为全局形状补全问题，本文设计的算法能够生成稠密且均匀分布的点集，并显示出良好的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud upsampling aims to generate dense and uniformly distributed pointsets from sparse point clouds. Existing point cloud upsampling methodstypically approach the task as an interpolation problem. They achieveupsampling by performing local interpolation between point clouds or in thefeature space, then regressing the interpolated points to appropriatepositions. By contrast, our proposed method treats point cloud upsampling as aglobal shape completion problem. Specifically, our method first divides thepoint cloud into multiple patches. Then, a masking operation is applied toremove some patches, leaving visible point cloud patches. Finally, ourcustom-designed neural network iterative completes the missing sections of thepoint cloud through the visible parts. During testing, by selecting differentmask sequences, we can restore various complete patches. A sufficiently denseupsampled point cloud can be obtained by merging all the completed patches. Wedemonstrate the superior performance of our method through both quantitativeand qualitative experiments, showing overall superiority against both existingself-supervised and supervised methods.</description>
      <author>example@mail.com (Ziming Nie, Qiao Wu, Chenlei Lv, Siwen Quan, Zhaoshuai Qi, Muze Wang, Jiaqi Yang)</author>
      <guid isPermaLink="false">2502.19452v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18743v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了悬浮液中细颗粒的分散行为，特别是在接近堵塞状态时的行为。通过引入一种成本效益高的机器学习方法（基于图神经网络GNN）来预测摩擦接触网FCN，该方法能够捕捉密集悬浮液中的隐藏特征和底层模式。&lt;h4&gt;背景&lt;/h4&gt;细小颗粒在牛顿流体中形成悬浊液的粘度会在接近堵塞密度时发散。这种宏观行为受微结构影响，尤其是由机械承载力接触点组成的摩擦接触网（FCN）的影响。应力传递和网络拓扑对颗粒相对运动的约束敏感。&lt;h4&gt;目的&lt;/h4&gt;预测接近堵塞条件下的摩擦接触网FCN仍然是一个挑战。为了应对这一问题，本研究引入了一种新的机器学习方法来解决这个问题。&lt;h4&gt;方法&lt;/h4&gt;使用一种称为Deep Graph Convolutional Network（DeepGCN）的GNN变体在数据驱动模拟上进行训练，并证明了该模型具有强大的泛化和外推能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，这种方法能够准确预测处于分异流动参数和相空间中的系统的FCNs。此外，研究涵盖了从半稀释状态到堵塞状态的整个范围，并系统地变化诸如剪切应力、颗粒填充率以及滑动摩擦力和滚动摩擦力等参数。&lt;h4&gt;结论&lt;/h4&gt;这项工作的结果为预测颗粒体系性质提供了创新且可转移的技术途径，为材料科学及相关领域开辟了新的前进道路。&lt;h4&gt;翻译&lt;/h4&gt;悬浮液中细小颗粒分散在牛顿流体中的粘度接近堵塞密度时发散。微结构中的接触微观结构通过摩擦接触网（FCN）控制这种宏观行为。该网络由机械承载力接触点组成，这些接触点导致在接近堵塞过渡时刚性的出现。应力传递和网络拓扑对颗粒相对运动的约束敏感。尽管其重要性，由于实验和计算障碍，预测FCN尤其是在接近堵塞条件下仍然是一个挑战。本研究提出了使用图神经网络（GNN）预测FCN的一种成本效益高的机器学习方法，该方法通过映射粒子间相互作用来固有地捕捉密集悬浮液中的隐藏特征和潜在模式。利用一种称为Deep Graph Convolutional Network (DeepGCN)的GNN变体在数据驱动模拟上进行训练，本研究展示了强大的泛化能力和外推能力，在分异流动参数和相空间中准确预测FCNs，尽管每个条件仅单独受训。该研究涵盖了广泛的相空间范围，从半稀释状态到堵塞状态，并跨越瞬态和稳态，同时系统地变化诸如剪切应力、颗粒填充率以及滑动摩擦力和滚动摩擦力等参数。这项研究的结果为预测颗粒体系性质提供了创新且可转移的技术途径，为材料科学及相关领域开辟了新的前进道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The viscosity of the suspension consisting of fine particles dispersed in aNewtonian liquid diverges close to the jamming packing fraction. The contactmicrostructure in suspensions governs this macroscopic behavior in the vicinityof jamming through a frictional contact network (FCN). FCN is composed ofmechanical load-bearing contacts that lead to the emergence of rigidity nearthe jamming transition. The stress transmission and network topology, in turn,depend sensitively on constraints on the relative motion of the particles.Despite their significance, predicting the FCN, especially close to jammingconditions, remains challenging due to experimental and computationalimpediments. This study introduces a cost-effective machine learning approachto predict the FCN using a graph neural network (GNN), which inherentlycaptures hidden features and underlying patterns in dense suspension by mappinginterparticle interactions. Employing a variation of GNN called the Deep GraphConvolutional Network (DeepGCN) trained on data-driven simulations, this studydemonstrates robust generalization and extrapolation capabilities, accuratelypredicting FCNs in systems with divergent flow parameters and phase spaces,despite each being trained exclusively on a single condition. The study coversa wide range of phase space, from semi-dilute to jammed states, spanningtransient to steady states, while systematically varying parameters such asshear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding androlling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave theway for innovative transferable techniques in predicting the properties ofparticulate systems, offering new avenues for advancement in material scienceand related fields.</description>
      <author>example@mail.com (Armin Aminimajd, Joao Maia, Abhinendra Singh)</author>
      <guid isPermaLink="false">2502.18743v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Foundation-Model-Based Industrial Defect Detection</title>
      <link>http://arxiv.org/abs/2502.19106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着工业产品种类的增多和复杂度提高，视觉工业缺陷检测受到了广泛的关注。传统的研究方法依赖于统计分析、异常数据合成建模以及基于生成模型的方法来分离产品的缺陷特征并完成缺陷检测任务。&lt;h4&gt;背景&lt;/h4&gt;近年来，基础模型（Foundation Model, FM）的出现带来了丰富的视觉和文本语义先验知识。&lt;h4&gt;目的&lt;/h4&gt;探讨基于基础模型与非基础模型方法在工业产品缺陷检测领域的应用及优劣，并分析未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;本论文系统地对各种基于基础模型的方法进行了比较、分类和讨论，同时简要回顾了近期发布的部分非基础模型（NFM）方法。详细分析了FM和NFM之间从训练目标到性能表现的差异，指出了可能的研究前景。&lt;h4&gt;主要发现&lt;/h4&gt;相比非基础模型方法，基础模型方法更适用于少量样本学习乃至零样本学习，在实际工业应用场景中更为契合，并值得深入研究。&lt;h4&gt;结论&lt;/h4&gt;虽然基于基础模型的方法提高了缺陷检测准确性，但同时也增加了模型复杂性和推理速度的降低。一些基于基础模型的方法开始探索轻量化建模方式，逐渐吸引人们的注意并值得系统化分析。&lt;h4&gt;翻译&lt;/h4&gt;随着工业产品变得丰富和精细，视觉工业缺陷检测（包括二维和三维视觉特征建模）受到了广泛的关注。传统的研究方法利用统计分析、异常数据合成建模以及基于生成模型的方法来分离产品的缺陷特征，并完成缺陷检测任务。近年来，基础模型的出现带来了丰富的视觉和文本语义先验知识，许多基于此的研究旨在提高检测准确性但同时增加了模型复杂性和推理速度下降的问题。一些基于基础模型方法已经开始探索轻量级建模方式，逐渐引起了人们的关注并值得系统化分析。本论文系统地对各种基于基础模型的方法进行了比较、分类和讨论，并简要回顾了近期发布的部分非基础模型（NFM）方法；此外还从训练目标、模型结构与规模以及性能表现等角度详细对比了FM与NFM之间的差异，指出了未来可能的研究方向。通过对比研究发现，基于基础模型的方法更适合少量样本学习乃至零样本学习，这些更符合实际工业应用场景，并值得深入探索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As industrial products become abundant and sophisticated, visual industrialdefect detection receives much attention, including two-dimensional andthree-dimensional visual feature modeling. Traditional methods use statisticalanalysis, abnormal data synthesis modeling, and generation-based models toseparate product defect features and complete defect detection. Recently, theemergence of foundation models has brought visual and textual semantic priorknowledge. Many methods are based on foundation models (FM) to improve theaccuracy of detection, but at the same time, increase model complexity and slowdown inference speed. Some FM-based methods have begun to explore lightweightmodeling ways, which have gradually attracted attention and deserve to besystematically analyzed. In this paper, we conduct a systematic survey withcomparisons and discussions of foundation model methods from different aspectsand briefly review non-foundation model (NFM) methods recently published.Furthermore, we discuss the differences between FM and NFM methods fromtraining objectives, model structure and scale, model performance, andpotential directions for future exploration. Through comparison, we find FMmethods are more suitable for few-shot and zero-shot learning, which are morein line with actual industrial application scenarios and worthy of in-depthresearch.</description>
      <author>example@mail.com (Tianle Yang, Luyao Chang, Jiadong Yan, Juntao Li, Zhi Wang, Ke Zhang)</author>
      <guid isPermaLink="false">2502.19106v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17860v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025; Corrected citation of Uni3D;&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了UniGS，一种将3D高斯散射技术与多模态预训练结合的方法，旨在提升对三维世界的表达能力。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态预训练方法虽然在学习文本、图像和点云的联合表示方面取得了进展，但使用离散点作为三维表示无法充分捕捉三维世界中的细微差别，并且存在二维像素与三维点之间的明显差距。&lt;h4&gt;目的&lt;/h4&gt;引入3D高斯散射技术来增强多模态预训练模型对三维世界的表达能力，同时提升跨模态对齐效果。&lt;h4&gt;方法&lt;/h4&gt;UniGS首先使用基于3D高斯散射的表示形式建模三维世界，并通过一个共享视觉和文本空间的语言-图像预训练模型建立语言、图像和点云之间的联系。接着，它利用一个三维编码器将优化后的3D高斯散射与语言-图像表示对齐以学习统一的多模态表示。&lt;h4&gt;主要发现&lt;/h4&gt;在Objaverse、ABO、MVImgNet 和 SUN RGBD 数据集上的零样本分类、文本驱动检索和开放世界理解任务中，UniGS展示了比现有SOTA模型（如Uni3D）更优越的表现。具体而言，在不同三维任务上取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入3D高斯散射技术及Gaussian-Aware Guidance模块，UniGS能够学习到更通用且对齐更好的多模态表示。&lt;h4&gt;翻译&lt;/h4&gt;最近在多模式3D预训练方法方面取得的进步显示出在文本、图像和点云联合表示学习方面的显著有效性。然而，采用点云作为三维表达无法充分捕捉三维世界的细微差别，并且存在二维像素与三维点之间的明显差距。为解决这一问题，我们提出UniGS，在多模态预训练中整合3D高斯散射技术以增强三维表示。我们首先利用基于3D高斯散射的表示形式建模整个三维世界为带有颜色和不透明度的一系列3D高斯分布，并通过广泛的现实图像-文本对，依靠语言模型建立共享视觉和文本空间。随后，UniGS使用一个3D编码器将优化后的3D高斯散射与语言-图像表示对齐以学习统一的多模态表示。为促进3D编码器提取全局显式三维特征并实现更好的跨模式对齐，我们进一步引入了一个新颖的Gaussian-Aware Guidance模块来指导3D领域的细粒度表示的学习过程。在Objaverse、ABO、MVImgNet 和 SUN RGBD 数据集上的零样本分类、文本驱动检索和开放世界理解任务中，通过广泛实验展示了UniGS学习更通用且更强对齐多模态表征的有效性，并在不同三维任务上取得显著优于现有最佳方法（如Uni3D）的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in multi-modal 3D pre-training methods have shownpromising efficacy in learning joint representations of text, images, and pointclouds. However, adopting point clouds as 3D representation fails to fullycapture the intricacies of the 3D world and exhibits a noticeable gap betweenthe discrete points and the dense 2D pixels of images. To tackle this issue, wepropose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modalpre-training to enhance the 3D representation. We first rely on the 3DGSrepresentation to model the 3D world as a collection of 3D Gaussians with colorand opacity, incorporating all the information of the 3D scene whileestablishing a strong connection with 2D images. Then, to achieveLanguage-Image-3D pertaining, UniGS starts with a pre-trained vision-languagemodel to establish a shared visual and textual space through extensivereal-world image-text pairs. Subsequently, UniGS employs a 3D encoder to alignthe optimized 3DGS with the Language-Image representations to learn unifiedmulti-modal representations. To facilitate the extraction of global explicit 3Dfeatures by the 3D encoder and achieve better cross-modal alignment, weadditionally introduce a novel Gaussian-Aware Guidance module that guides thelearning of fine-grained representations of the 3D domain. Through extensiveexperiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets withzero-shot classification, text-driven retrieval and open-world understandingtasks, we demonstrate the effectiveness of UniGS in learning a more general andstronger aligned multi-modal representation. Specifically, UniGS achievesleading results across different 3D tasks with remarkable improvements overprevious SOTA, Uni3D, including on zero-shot classification (+9.36%),text-driven retrieval (+4.3%) and open-world understanding (+7.92%).</description>
      <author>example@mail.com (Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.17860v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了使用机器学习技术来预测COVID-19患者的病情严重程度，通过整合三个数据源创建了一个大规模的COVID严重性数据库，并评估了迁移学习和视觉变换器在预测病情方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;COVID-19大流行给医疗资源带来了巨大压力，促使人们讨论如何利用机器学习减轻医生负担并辅助诊断。胸部X光片（CXRs）用于诊断COVID-19，但很少有研究从这些影像中预测患者的病情严重程度。&lt;h4&gt;目的&lt;/h4&gt;通过整合多个数据源创建一个大规模的COVID严重性数据库，并评估迁移学习和视觉变换器在病情严重度分类与回归任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型（如基于ImageNet和CXRs）及视觉变换器（ViTs）进行迁移学习。其中，采用DenseNet161模型进行了三类预测问题的试验，并评估了其分类准确性和视觉变换器在病情严重度回归任务中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的DenseNet161模型在三个类别上达到了最高的整体准确性（80%）；而ViT则在病情严重度回归中表现最好，平均绝对误差为0.5676，接近放射科医生预测的结果。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在COVID-19病情预测方面，迁移学习和视觉变换器显示出潜力。预训练的DenseNet161模型对三分类问题表现出最佳性能；ViT在回归任务中表现优异。该项目的源代码是公开可用的。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stwhitfield/covid-severity&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju)</author>
      <guid isPermaLink="false">2502.16622v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multispectral to Hyperspectral using Pretrained Foundational model</title>
      <link>http://arxiv.org/abs/2502.19451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结合高光谱和多光谱成像系统优势的方法，以改善温室气体监测。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像提供了详细的光谱信息，对监测如CH4和NO2等温室气体具有重要意义。但是其应用受到空间覆盖率低和重复访问频率不高的限制。相比之下，多光谱成像能提供更广的空间和时间覆盖范围，但缺乏精细的光谱分辨率以进行精确的温室气体检测。&lt;h4&gt;目的&lt;/h4&gt;为了解决高光谱和多光谱成像技术之间的局限性问题，该研究提出了空间-光谱变换模型来重建从多光谱输入的数据。&lt;h4&gt;方法&lt;/h4&gt;研究中提出的模型先是在EnMAP和EMIT数据集上进行预训练，然后分别在（Sentinel-2, EnMAP）和（HLS-S30, EMIT）图像对上微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过将高光谱成像的精细光谱分辨率与多光谱成像的空间覆盖范围和时间频率相结合，可以提高大气监测的效率。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了结合使用高光谱和多光谱数据的有效性，并展示了利用这些技术改善温室气体检测方法的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：高光谱成像提供了详细的光谱信息，为CH4和NO2等温室气体的监测提供重要意义。然而，其应用受到空间覆盖率低和重复访问频率不高的限制。相比之下，多光谱成像能提供更广的空间和时间覆盖范围，但缺乏精细的光谱分辨率以进行精确的温室气体检测。为了应对这些挑战，本研究提出了空间-光谱变换模型来从多光谱输入中重建高光谱数据。本文中的模型先是在EnMAP和EMIT数据集上预训练，并在（Sentinel-2, EnMAP）和（HLS-S30, EMIT）图像对上微调。我们的模型结合了高光谱和多光谱成像系统的优点，有潜力提升大气监测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging provides detailed spectral information, offeringsignificant potential for monitoring greenhouse gases like CH4 and NO2.However, its application is constrained by limited spatial coverage andinfrequent revisit times. In contrast, multispectral imaging delivers broaderspatial and temporal coverage but lacks the spectral granularity required forprecise GHG detection. To address these challenges, this study proposesSpectral and Spatial-Spectral transformer models that reconstruct hyperspectraldata from multispectral inputs. The models in this paper are pretrained onEnMAP and EMIT datasets and fine-tuned on spatio-temporally aligned(Sentinel-2, EnMAP) and (HLS-S30, EMIT) image pairs respectively. Our model hasthe potential to enhance atmospheric monitoring by combining the strengths ofhyperspectral and multispectral imaging systems.</description>
      <author>example@mail.com (Ruben Gonzalez, Conrad M Albrecht, Nassim Ait Ali Braham, Devyani Lambhate, Joao Lucas de Sousa Almeida, Paolo Fraccaro, Benedikt Blumenstiel, Thomas Brunschwiler, Ranjini Bangalore)</author>
      <guid isPermaLink="false">2502.19451v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17371v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了将光伏系统集成到温室中的效果，同时引入了一种新的时空图神经网络（STGNN）应用于温室微气候建模。&lt;h4&gt;背景&lt;/h4&gt;光伏系统的集成可以优化土地使用，并通过实现食品生产和可再生能源发电的双重效益来增强可持续农业实践。但是，准确预测内部环境条件对于确保作物生长的最佳状态和最大限度地提高能源生产至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的时空图神经网络（STGNN）模型应用于温室微气候建模，并与传统的递归神经网络（RNN）进行性能比较。&lt;h4&gt;方法&lt;/h4&gt;研究利用在希腊沃洛斯收集的每15分钟一次的高频数据，展示了传统递归神经网络（RNNs）在冬季条件下的卓越准确度，但对夏季冷却系统操作时的表现有限。尽管STGNN模型当前表现较低，但它能够更好地整合包括光伏发电量和作物生长指标在内的额外变量。&lt;h4&gt;主要发现&lt;/h4&gt;传统的RNN模型擅长时间序列模式识别，但在处理环境变量之间的方向关系方面存在局限性；而新的STGNN架构可以解决这些问题，并且能够捕捉空间依赖性和它们的方向性。&lt;h4&gt;结论&lt;/h4&gt;虽然当前STGNN的表现略低于RNN（冬季R^2为0.947相比RNN的0.985），但其模型结构具有更大的潜力来整合更多变量，从而更好地适应和优化温室环境条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of photovoltaic (PV) systems into greenhouses not onlyoptimizes land use but also enhances sustainable agricultural practices byenabling dual benefits of food production and renewable energy generation.However, accurate prediction of internal environmental conditions is crucial toensure optimal crop growth while maximizing energy production. This studyintroduces a novel application of Spatio-Temporal Graph Neural Networks(STGNNs) to greenhouse microclimate modeling, comparing their performance withtraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporalpattern recognition, they cannot explicitly model the directional relationshipsbetween environmental variables. Our STGNN approach addresses this limitationby representing these relationships as directed graphs, enabling the model tocapture both spatial dependencies and their directionality. Usinghigh-frequency data collected at 15-minute intervals from a greenhouse inVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winterconditions (R^2 = 0.985) but show limitations during summer cooling systemoperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),their architecture offers greater potential for integrating additionalvariables such as PV generation and crop growth indicators.</description>
      <author>example@mail.com (Emiliano Seri, Marcello Petitta, Cristina Cornaro)</author>
      <guid isPermaLink="false">2502.17371v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Evolutionary Algorithms Approach For Search Based On Semantic Document Similarity</title>
      <link>http://arxiv.org/abs/2502.19437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;云计算和分布式计算的发展促进了计算机科学领域的研究活动。这些领域在神经网络、遗传算法等进化计算算法方面取得了显著进展。&lt;h4&gt;目的&lt;/h4&gt;利用通用句子编码器(USE)捕捉文本语义相似性，并使用迁移学习技术将遗传算法(GA)和差分演化(DE)应用于基于用户查询检索相关文档的搜索与检索。&lt;h4&gt;方法&lt;/h4&gt;在斯坦福问答数据集(SQuAD)上应用所提出的方法，以识别用户查询。研究中采用曼哈顿距离、GA和DE算法进行对比实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明：文本可以通过USE高效地表示为句子嵌入向量；进化算法（如GA和DE）在查找顶级N结果方面比传统排名方法更有效。&lt;h4&gt;结论&lt;/h4&gt;使用进化计算算法来搜索和检索文档是一个有潜力的研究领域，它可以在各种应用中实现更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3617733.3617753&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in cloud computing and distributed computing have fosteredresearch activities in Computer science. As a result, researchers have madesignificant progress in Neural Networks, Evolutionary Computing Algorithms likeGenetic, and Differential evolution algorithms. These algorithms are used todevelop clustering, recommendation, and question-and-answering systems usingvarious text representation and similarity measurement techniques. In thisresearch paper, Universal Sentence Encoder (USE) is used to capture thesemantic similarity of text; And the transfer learning technique is used toapply Genetic Algorithm (GA) and Differential Evolution (DE) algorithms tosearch and retrieve relevant top N documents based on user query. The proposedapproach is applied to the Stanford Question and Answer (SQuAD) Dataset toidentify a user query. Finally, through experiments, we prove that textdocuments can be efficiently represented as sentence embedding vectors usingUSE to capture the semantic similarity, and by comparing the results of theManhattan Distance, GA, and DE algorithms we prove that the evolutionaryalgorithms are good at finding the top N results than the traditional rankingapproach.</description>
      <author>example@mail.com (Chandrashekar Muniyappa, Eujin Kim)</author>
      <guid isPermaLink="false">2502.19437v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Agnostic calculation of atomic free energies with the descriptor density of states</title>
      <link>http://arxiv.org/abs/2502.18191v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的评估原子系统振动自由能的方法，该方法不需要预先指定原子间势，并且能够通过描述符的高维密度熵进行准确估算。&lt;h4&gt;背景&lt;/h4&gt;传统的计算原子系统的振动自由能需要先验地定义原子间势函数。这种方法限制了模型的灵活性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种与模型无关的方法来评估原子系统在不同条件下的振动自由能，以便于不确定性量化和逆向设计。&lt;h4&gt;方法&lt;/h4&gt;使用描述符（即高维特征向量）表示原子结构，并通过条件分数匹配准确估计这些描述符的密度熵。利用Legendre-Fenchel共轭关系将自由能与描述符熵联系起来，避免了复杂的高维积分计算。&lt;h4&gt;主要发现&lt;/h4&gt;该方法可以快速、精确地预测各种材料在不同相态下的振动自由能，并且通过反向传播技术成功降低了Fe的α-γ转变温度。&lt;h4&gt;结论&lt;/h4&gt;此模型无关的方法不仅能准确评估原子系统的自由能，还能应用于液体和其他基础模型的微调中。它为解决计算科学中的高维积分问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的方法来评估没有预先指定原子间势的情况下原子系统振动自由能的新方法。我们的模型无关的方法利用描述符（即原子结构的高维特征向量）进行表示，并通过条件分数匹配准确估计这些描述符的密度熵。通过将原子间势转换为在描述符特性上扩增的形式，我们展示了自由能在描述符熵的Legendre-Fenchel共轭中出现，从而避免了所有高维积分计算。所需的评分匹配活动比固定模型采样需要更少的资源，并且可以高度并行化，将实际时间减少到几分钟以内。我们的模型无关的估计器能够以微秒级的CPU努力在广泛的潜在参数范围内返回可微分自由能预测，这允许快速向前和反向传播潜在变化通过有限温度模拟，这是为了不确定性量化和逆向设计而长期需要的功能。我们通过对W、Mo和Fe的BCC、FCC和A15相态进行热力学集成计算进行了广泛的模型测试，并且在高温同源条件下预测通过了严格的准确性阈值（每原子1-2 meV或0.25至0.5 kcal/mol）用于阶段预测。我们还展示了目标微调，通过对Fe的非磁性机器学习模型使用反向传播技术将其α-γ转变温度从2030 K降低到1063 K，并且无需额外采样。讨论了该方法在液体以及基础模型细调中的应用及其对计算科学中众多估计高维积分问题的潜在影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to evaluate vibrational free energies of atomicsystems without a priori specification of an interatomic potential. Ourmodel-agnostic approach leverages descriptors, high-dimensional feature vectorsof atomic structure. The entropy of a high-dimensional density, the descriptordensity of states, is accurately estimated with conditional score matching.Casting interatomic potentials into a form extensive in descriptor features, weshow free energies emerge as the Legendre-Fenchel conjugate of the descriptorentropy, avoiding all high-dimensional integration. The score matching campaignrequires less resources than fixed-model sampling and is highly parallel,reducing wall time to a few minutes, with tensor compression schemes allowinglightweight storage. Our model-agnostic estimator returns differentiable freeenergy predictions over a broad range of potential parameters in microsecondsof CPU effort, allowing rapid forward and back propagation of potentialvariations through finite temperature simulations, long desired for uncertaintyquantification and inverse design. We test predictions against thermodynamicintegration calculations over a broad range of models for BCC, FCC and A15phases of W, Mo and Fe at high homologous temperatures. Predictions pass thestringent accuracy threshold of 1-2 meV/atom (1/40-1/20 kcal/mol) for phaseprediction with propagated score uncertainties robustly bounding errors. Wealso demonstrate targeted fine-tuning, reducing the alpha-gamma transitiontemperature in a non-magnetic machine learning model of Fe from 2030 K to 1063K through back-propagation, with no additional sampling. Applications toliquids and fine-tuning foundational models are discussed along with the manyproblems in computational science which estimate high-dimensional integrals.</description>
      <author>example@mail.com (Thomas D Swinburne, Clovis Lapointe, Mihai-Cosmin Marinica)</author>
      <guid isPermaLink="false">2502.18191v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Transformers: Architectures, Theories and Applications</title>
      <link>http://arxiv.org/abs/2502.16533v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;Graph Transformers (GTs) 的综合回顾&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在处理图形结构时存在过度平滑和过度压缩等内在局限性，而 Graph Transformers 通过解决这些问题展示出强大的建模能力。&lt;h4&gt;目的&lt;/h4&gt;对最近关于 Graph Transformers 的快速发展进行全面回顾，涵盖架构、理论基础及应用等方面。&lt;h4&gt;方法&lt;/h4&gt;根据处理结构信息的策略分类 Graph Transformers 架构，包括图标记化、位置编码、结构感知注意力和模型集成。从理论上探讨了不同讨论架构下的表达能力，并与其他高级图形学习算法进行对比以发现联系。&lt;h4&gt;主要发现&lt;/h4&gt;Graph Transformers 在分子数据、蛋白质数据、语言处理、视觉识别、交通网络、脑科学及材料学等领域的实际应用中发挥了重要作用。&lt;h4&gt;结论&lt;/h4&gt;指出了当前 Graph Transformers 面临的挑战和未来研究的方向，为潜在的研究提供了可能的道路。&lt;h4&gt;翻译&lt;/h4&gt;Graph Transformer (GTs) 通过解决图神经网络（GNNs）固有的限制问题，在构建图形结构模型方面展现出了强大的能力。最近的研究提议了多样化的架构、增强了可解释性，并探讨了实际应用案例。考虑到这些快速的发展，我们对该领域进行了全面的综述，涵盖了 Graph Transformers 的架构设计、理论依据及其在分子数据、蛋白质数据、语言处理等众多领域的应用实例。此外，还对现有的挑战和未来的研究方向提出了建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have demonstrated a strong capability in modelinggraph structures by addressing the intrinsic limitations of graph neuralnetworks (GNNs), such as over-smoothing and over-squashing. Recent studies haveproposed diverse architectures, enhanced explainability, and practicalapplications for Graph Transformers. In light of these rapid developments, weconduct a comprehensive review of Graph Transformers, covering aspects such astheir architectures, theoretical foundations, and applications within thissurvey. We categorize the architecture of Graph Transformers according to theirstrategies for processing structural information, including graph tokenization,positional encoding, structure-aware attention and model ensemble. Furthermore,from the theoretical perspective, we examine the expressivity of GraphTransformers in various discussed architectures and contrast them with otheradvanced graph learning algorithms to discover the connections. Furthermore, weprovide a summary of the practical applications where Graph Transformers havebeen utilized, such as molecule, protein, language, vision, traffic, brain andmaterial data. At the end of this survey, we will discuss the currentchallenges and prospective directions in Graph Transformers for potentialfuture research.</description>
      <author>example@mail.com (Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong)</author>
      <guid isPermaLink="false">2502.16533v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Voting Scheme to Strengthen Localization Security in Randomly Deployed Wireless Sensor Networks</title>
      <link>http://arxiv.org/abs/2502.20218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文旨在为存在恶意节点的恶劣环境下的目标定位问题提供一个可信解决方案，这些恶意节点能够操纵距离测量数据（即执行欺骗攻击），从而妨碍准确定位。除了定位外，另一个目的是识别参与过程中的哪些节点是恶意的。&lt;h4&gt;背景&lt;/h4&gt;随着物联网和智能城市应用的扩展，依赖准确位置信息的应用面临严重的安全威胁，因为现有的大多数定位系统都容易受到欺骗攻击。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于聚类和加权质心的投票方案，在对抗性环境下安全地解决定位问题并检测恶意节点。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '选择一组合适的兴趣点簇，并利用问题几何学分配票数以定位目标。', '第二阶段': '通过位置估计和基本统计信息来探测攻击者'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在不同设置下被评估为具有较高的本地化精度，成功的恶意节点检测率以及较低的计算复杂度。计算机模拟与现实世界实验验证了该方案的有效性，并显示出相比于当前最先进的方法，可以减少30%的误差并几乎实现完美的攻击者检出率。&lt;h4&gt;结论&lt;/h4&gt;提出的解决方案在定位准确性和攻击者识别方面均优于现有技术，并能显著提高安全性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work aspires to provide a trustworthy solution for target localizationin adverse environments, where malicious nodes, capable of manipulatingdistance measurements (i.e., performing spoofing attacks), are present, thushindering accurate localization. Besides localization, its other goal is toidentify (detect) which of the nodes participating in the process aremalicious. This problem becomes extremely important with the forthcomingexpansion of IoT and smart cities applications, that depend on accuratelocalization, and the presence of malicious attackers can represent serioussecurity threats if not taken into consideration. This is the case with mostexisting localization systems which makes them highly vulnerable to spoofingattacks. In addition, existing methods that are intended for adversarialsettings consider very specific settings or require additional knowledge aboutthe system model, making them only partially secure. Therefore, this workproposes a novel voting scheme based on clustering and weighted central mass tosecurely solve the localization problem and detect attackers. The proposedsolution has two main phases: 1) Choosing a cluster of suitable points ofinterest by taking advantage of the problem geometry to assigning votes inorder to localize the target, and 2) Attacker detection by exploiting thelocation estimate and basic statistics. The proposed method is assessed interms of localization accuracy, success in attacker detection, andcomputational complexity in different settings. Computer simulations andreal-world experiments corroborate the effectiveness of the proposed schemecompared to state-of-the-art methods, showing that it can accomplish an errorreduction of $30~\%$ and is capable of achieving almost perfect attackerdetection rate when the ratio between attacker intensity and noise standarddeviation is significant.</description>
      <author>example@mail.com (Slavisa Tomic, Marko Beko, Dejan Vukobratovic, Srdjan Krco)</author>
      <guid isPermaLink="false">2502.20218v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study</title>
      <link>http://arxiv.org/abs/2502.20021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该研究探讨了系统之系统的环境可持续性，分析了SoS如何促进碳减排、能源效率和生物多样性保护等可持续实践。&lt;h4&gt;背景信息&lt;/h4&gt;虽然已经有关于智慧城市的系统回顾来讨论可持续性问题，但还没有针对所有类别SoS的系统化知识综合研究。此外，尽管文献中包括其他类型的可持续性（如财务和社会），本研究专注于环境可持续性。&lt;h4&gt;研究目的&lt;/h4&gt;进行系统的映射研究以确定SoS在可持续性应用领域的范围、所面临的挑战以及未来的研究机会。&lt;h4&gt;研究方法&lt;/h4&gt;制定了一个研究协议，其中包括了四个科学数据库的自动化搜索。总共检索到了926个研究项目，并从中选择了39项相关研究进行了分析和报告。&lt;h4&gt;主要发现&lt;/h4&gt;大多数研究集中在智慧城市和智能电网领域；同时，一些应用如可持续农业和森林火灾预防则相对较少探讨。此外，还识别出了系统互操作性、可扩展性和数据治理等挑战。&lt;h4&gt;结论建议&lt;/h4&gt;提出了未来在SoS和环境可持续性方面的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要讨论了关于系统之系统的环境可持续性的新兴领域，并指出尽管有针对智慧城市的系统回顾存在，但缺乏综合现有知识以支持所有类型SoS的广泛应用。研究通过分析39项相关文献揭示了当前主要集中在特定领域的应用以及面临的挑战，并提出了未来的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environmental sustainability in Systems-of-Systems (SoS) is an emerging fieldthat seeks to integrate technological solutions to promote the efficientmanagement of natural resources. While systematic reviews addresssustainability in the context of Smart Cities (a category of SoS), a systematicstudy synthesizing the existing knowledge on environmental sustainabilityapplied to SoS in general does not exist. Although literature includes othertypes of sustainability, such as financial and social, this study focuses onenvironmental sustainability, analyzing how SoS contribute to sustainablepractices such as carbon emission reduction, energy efficiency, andbiodiversity conservation. We conducted a Systematic Mapping Study to identifythe application domains of SoS in sustainability, the challenges faced, andresearch opportunities. We planned and executed a research protocol includingan automated search over four scientific databases. Of 926 studies retrieved,we selected, analyzed, and reported the results of 39 relevant studies. Ourfindings reveal that most studies focus on Smart Cities and Smart Grids, whileapplications such as sustainable agriculture and wildfire prevention are lessexplored. We identified challenges such as system interoperability,scalability, and data governance. Finally, we propose future researchdirections for SoS and environmental sustainability.</description>
      <author>example@mail.com (Ana Clara Araújo Gomes da Silva, Gilmar Teixeira Junior, Lívia Mancine C. de Campos, Renato F. Bulcão-Neto, Valdemar Vicente Graciano Neto)</author>
      <guid isPermaLink="false">2502.20021v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title>
      <link>http://arxiv.org/abs/2502.20396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page can be found at https://toruowo.github.io/recipe/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;强化学习在实现人类甚至超人类水平能力方面取得了令人振奋的结果，但灵巧机器人操作的成功仍有限。这项工作调查了将强化学习应用于人形代理执行一系列接触密集型任务的关键挑战，并提出了解决这些挑战的新方法。&lt;h4&gt;背景&lt;/h4&gt;强化学习已经在多种问题领域中实现了人类甚至超越人类的性能，但在灵巧的机器人操纵方面进展有限。&lt;h4&gt;目的&lt;/h4&gt;探讨并解决在人形化身执行复杂操作任务时应用强化学习所面临的挑战，从而提高其实际环境中的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;{'自动化真实到模拟调整模块': '用于使仿真环境更接近现实世界', '通用奖励设计策略': '简化长时间跨度接触密集型操作任务的回报工程', '分而治之提炼过程': '提高难探索问题的样本效率同时保持从仿真到实际性能的一致性', '稀疏和稠密物体表示混合': '弥合从仿真到现实感知差距'}&lt;h4&gt;主要发现&lt;/h4&gt;{'自动化调整模块': '能够有效缩小模拟环境与真实环境之间的差异', '通用奖励设计策略': '简化了长时程接触密集型任务的回报工程，使得训练更加高效', '分而治之提炼过程': '在提高难探索问题效率的同时保持了一致的从仿真到现实的表现', '混合表示方法': '有效减轻了从仿真到实际操作中的感知差距'}&lt;h4&gt;结论&lt;/h4&gt;通过上述一系列创新技术，在三个灵巧的人形操作任务上取得了有希望的结果，表明利用模拟到真实强化学习可以实现人形灵巧操纵的学习，并且能够无需人类演示而达到稳健的泛化和高性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：强化学习在多种问题领域中实现了人类甚至超越人类的性能，但在灵巧机器人操作方面进展有限。这项工作探讨了将强化学习应用于人形代理执行一系列接触密集型任务的关键挑战，并提出了解决这些挑战的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning has delivered promising results in achieving human- oreven superhuman-level capabilities across diverse problem domains, but successin dexterous robot manipulation remains limited. This work investigates the keychallenges in applying reinforcement learning to solve a collection ofcontact-rich manipulation tasks on a humanoid embodiment. We introduce noveltechniques to overcome the identified challenges with empirical validation. Ourmain contributions include an automated real-to-sim tuning module that bringsthe simulated environment closer to the real world, a generalized reward designscheme that simplifies reward engineering for long-horizon contact-richmanipulation tasks, a divide-and-conquer distillation process that improves thesample efficiency of hard-exploration problems while maintaining sim-to-realperformance, and a mixture of sparse and dense object representations to bridgethe sim-to-real perception gap. We show promising results on three humanoiddexterous manipulation tasks, with ablation studies on each technique. Our workpresents a successful approach to learning humanoid dexterous manipulationusing sim-to-real reinforcement learning, achieving robust generalization andhigh performance without the need for human demonstration.</description>
      <author>example@mail.com (Toru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, Yuke Zhu)</author>
      <guid isPermaLink="false">2502.20396v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2502.20391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Point Policy的新方法，该方法可以从离线的人类演示视频中学习机器人的策略，并且不需要任何远程操作数据。&lt;h4&gt;背景&lt;/h4&gt;构建能够在多种环境和对象类型下运行的机器人代理仍然是一项重大挑战，通常需要大量的数据收集。由于每个数据点必须在现实世界中物理执行，这在机器人技术领域尤为限制性。&lt;h4&gt;目的&lt;/h4&gt;提出一种替代的数据源以及能够利用这些数据进行学习的框架。&lt;h4&gt;方法&lt;/h4&gt;Point Policy利用最先进的视觉模型和策略架构将人类的手部姿势转换为机器人的姿势，并通过有意义的关键点捕捉物体状态。这种方法产生了一种形态无关的表示，从而促进有效的策略学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在与训练相同的设置下评估时，相比先前的工作总体改进了75%绝对值；对于新的物体实例在所有任务上表现出74%的增长，并且能够处理显著的背景杂乱。&lt;h4&gt;结论&lt;/h4&gt;Point Policy提供了一种创新的方法来利用人类演示视频中的数据学习机器人策略，无需进行物理执行。这表明从离线数据中学习是可能的，并为未来的研究开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在多种环境和对象类型下运行的机器人代理仍然是一项重大挑战，通常需要大量的数据收集。由于每个数据点必须在现实世界中物理执行，这在机器人技术领域尤为限制性。因此，存在对替代的数据源以及能够利用这些数据进行学习的框架的需求。本文介绍了一种名为Point Policy的新方法，该方法可以从离线的人类演示视频中学习机器人的策略，并且不需要任何远程操作数据。Point Policy利用最先进的视觉模型和策略架构将人类的手部姿势转换为机器人的姿势，并通过有意义的关键点捕捉物体状态。这种方法产生了一种形态无关的表示，从而促进有效的策略学习。实验结果表明，在与训练相同的设置下评估时，相比先前的工作总体改进了75%绝对值；对于新的物体实例在所有任务上表现出74%的增长，并且能够处理显著的背景杂乱。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building robotic agents capable of operating across diverse environments andobject types remains a significant challenge, often requiring extensive datacollection. This is particularly restrictive in robotics, where each data pointmust be physically executed in the real world. Consequently, there is acritical need for alternative data sources for robotics and frameworks thatenable learning from such data. In this work, we present Point Policy, a newmethod for learning robot policies exclusively from offline human demonstrationvideos and without any teleoperation data. Point Policy leveragesstate-of-the-art vision models and policy architectures to translate human handposes into robot poses while capturing object states through semanticallymeaningful key points. This approach yields a morphology-agnosticrepresentation that facilitates effective policy learning. Our experiments on 8real-world tasks demonstrate an overall 75% absolute improvement over priorworks when evaluated in identical settings as training. Further, Point Policyexhibits a 74% gain across tasks for novel object instances and is robust tosignificant background clutter. Videos of the robot are best viewed athttps://point-policy.github.io/.</description>
      <author>example@mail.com (Siddhant Haldar, Lerrel Pinto)</author>
      <guid isPermaLink="false">2502.20391v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</title>
      <link>http://arxiv.org/abs/2502.20390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. Project Page: https://sirui-xu.github.io/InterMimic/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为InterMimic的框架，该框架能利用不完美的动作捕捉数据来学习复杂的物体-人交互（HOI），并通过一种课程策略和强化学习微调实现了从模仿到生成模型的进步。&lt;h4&gt;背景&lt;/h4&gt;长期以来，实现人类与各种对象进行真实互动的模拟一直是目标。然而，由于复杂的物体-人的耦合、多样的几何形状以及动作捕捉数据中的不准确接触等问题，基于物理的方法扩展到复杂的人体-物体交互面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，利用不完美的动作捕捉数据来学习和生成高质量且多样化的人体-物体互动模拟。&lt;h4&gt;方法&lt;/h4&gt;首先训练特定主体的教师策略以模仿、重定位并改进捕获的动作。然后将这些教师策略的知识转移到一个学生策略中，并通过强化学习微调进一步提升。&lt;h4&gt;主要发现&lt;/h4&gt;InterMimic能够产生现实且多样化的人体-物体互动，在多个HOI数据集上表现良好，还能零样本推广和无缝集成到运动学生成器中。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅提高了模仿质量，还从单纯的模仿技术转变为了复杂人体-物体交互的生成模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving realistic simulations of humans interacting with a wide range ofobjects has long been a fundamental goal. Extending physics-based motionimitation to complex human-object interactions (HOIs) is challenging due tointricate human-object coupling, variability in object geometries, andartifacts in motion capture data, such as inaccurate contacts and limited handdetail. We introduce InterMimic, a framework that enables a single policy torobustly learn from hours of imperfect MoCap data covering diverse full-bodyinteractions with dynamic and varied objects. Our key insight is to employ acurriculum strategy -- perfect first, then scale up. We first trainsubject-specific teacher policies to mimic, retarget, and refine motion capturedata. Next, we distill these teachers into a student policy, with the teachersacting as online experts providing direct supervision, as well as high-qualityreferences. Notably, we incorporate RL fine-tuning on the student policy tosurpass mere demonstration replication and achieve higher-quality solutions.Our experiments demonstrate that InterMimic produces realistic and diverseinteractions across multiple HOI datasets. The learned policy generalizes in azero-shot manner and seamlessly integrates with kinematic generators, elevatingthe framework from mere imitation to generative modeling of complexhuman-object interactions.</description>
      <author>example@mail.com (Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, Liang-Yan Gui)</author>
      <guid isPermaLink="false">2502.20390v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.20386v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究探讨了在无结构和未知环境中进行任务导向性导航所面临的挑战。机器人需要实时构建并推理出丰富且具有语义信息的地图。&lt;h4&gt;目的&lt;/h4&gt;为了有效地执行自然语言描述的任务，提出了一种分层表示方法，该方法基于嵌入式语言的高斯散布技术，能够支持稀疏语义规划和密集几何表征。&lt;h4&gt;方法&lt;/h4&gt;采用了一种层次化地图构建方式——基于语言嵌入的高斯散布方法，以实现稀疏语义路径规划和密集几何表达的结合。这种方法在碰撞避免导航方面具有优势。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界的机器人实验验证了该方法的有效性，在混乱的室内环境及千米级的室外环境中表现良好，并且与特权基线相比，竞争比率为约60%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为解决任务导向性导航中遇到的问题提供了一种有效的解决方案。它能够适应广泛的任务需求并在复杂环境下运行。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容是关于如何在无结构和未知的环境中实现机器人任务导向性导航的研究，提出了结合稀疏语义规划与密集几何表达的地图表示方法，并通过实际实验验证了该方法的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the challenge of task-oriented navigation in unstructured andunknown environments, where robots must incrementally build and reason on rich,metric-semantic maps in real time. Since tasks may require clarification orre-specification, it is necessary for the information in the map to be richenough to enable generalization across a wide range of tasks. To effectivelyexecute tasks specified in natural language, we propose a hierarchicalrepresentation built on language-embedded Gaussian splatting that enables bothsparse semantic planning that lends itself to online operation and densegeometric representation for collision-free navigation. We validate theeffectiveness of our method through real-world robot experiments conducted inboth cluttered indoor and kilometer-scale outdoor environments, with acompetitive ratio of about 60% against privileged baselines. Experiment videosand more details can be found on our project page: https://atlasnav.github.io</description>
      <author>example@mail.com (Dexter Ong, Yuezhan Tao, Varun Murali, Igor Spasojevic, Vijay Kumar, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2502.20386v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization</title>
      <link>http://arxiv.org/abs/2502.20382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种低成本的数据生成管道，利用物理仿真、人类演示和模型规划来为接触密集型机器人操作任务高效生成大规模高质量数据集。&lt;h4&gt;背景&lt;/h4&gt;在虚拟现实模拟环境中收集少量人体动作示例，并通过基于优化的运动重定位和轨迹优化进一步细化这些示例，以适应不同类型的机器人硬件和物理参数。该方法旨在创造多样且物理一致的数据集，以便于跨设备之间传输数据并重复使用之前在不同硬件配置下采集的老化数据集。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够生成大规模高质量数据集的方法，从而提高接触密集型任务中机器人的操作性能，并减少对人类输入的需求。&lt;h4&gt;方法&lt;/h4&gt;采用物理仿真、优化算法和轨迹规划相结合的方式处理人体演示动作。从虚拟现实环境中获取初始的小规模数据样本，然后通过优化过程使这些数据适应于各种不同的机器人形态及环境参数。&lt;h4&gt;主要发现&lt;/h4&gt;所生成的数据集不仅能够提高机器人的操作性能，在跨设备部署时也表现出优秀的零样本迁移能力，即在新的机器人硬件上无需进一步训练即可实现高成功率的操作任务。&lt;h4&gt;结论&lt;/h4&gt;此研究证明了通过仿真结合优化技术来生成高质量数据的有效性，并展示了这种方法在实际接触密集型任务中具有巨大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种低成本的数据生成管道，该管道整合物理仿真的方法、人类的演示和基于模型的规划，以高效地为涉及大量接触操作的机器人抓取任务生成大规模且高质量的数据集。从少量在虚拟现实模拟环境中收集的人类动作样本开始，利用基于优化的方法进行运动重定位以及轨迹优化来调整这些示例，使其适应于不同类型的机器人形态及物理参数，从而产生了多样化、物理一致性的数据集，并为跨设备间的数据传输提供了可能，同时也能够重复使用之前在不同硬件配置下采集的老化数据。通过训练生成的数据集以应对多种机器人手臂上的挑战性接触密集型任务（包括浮动Allegro手和双臂机器人），验证了该管道的有效性；所训练的策略在零样本输入的情况下于实际设备上部署，如用于双臂iiwa机器人的操作中取得了高成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a low-cost data generation pipeline that integrates physics-basedsimulation, human demonstrations, and model-based planning to efficientlygenerate large-scale, high-quality datasets for contact-rich roboticmanipulation tasks. Starting with a small number of embodiment-flexible humandemonstrations collected in a virtual reality simulation environment, thepipeline refines these demonstrations using optimization-based kinematicretargeting and trajectory optimization to adapt them across various robotembodiments and physical parameters. This process yields a diverse, physicallyconsistent dataset that enables cross-embodiment data transfer, and offers thepotential to reuse legacy datasets collected under different hardwareconfigurations or physical parameters. We validate the pipeline's effectivenessby training diffusion policies from the generated datasets for challengingcontact-rich manipulation tasks across multiple robot embodiments, including afloating Allegro hand and bimanual robot arms. The trained policies aredeployed zero-shot on hardware for bimanual iiwa arms, achieving high successrates with minimal human input. Project website:https://lujieyang.github.io/physicsgen/.</description>
      <author>example@mail.com (Lujie Yang, H. J. Terry Suh, Tong Zhao, Bernhard Paus Graesdal, Tarik Kelestemur, Jiuguang Wang, Tao Pang, Russ Tedrake)</author>
      <guid isPermaLink="false">2502.20382v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Path Planning in Complex Environments using Gaussian Belief Propagation with Global Path Finding</title>
      <link>http://arxiv.org/abs/2502.20369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by "International Conference on Robotics and Automation" -  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种多智能体路径规划的新方法，结合高斯信念传播与路径积分，并引入跟踪因子来确保严格的全局路径遵循。该方法通过两种不同全局路径规划策略进行了测试：快速探索随机树和结构化规划器。&lt;h4&gt;背景&lt;/h4&gt;在机器人学中，多代理路径规划是一个关键挑战，要求智能体能够在复杂环境中导航并避免碰撞同时优化旅行效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高多代理协调能力，特别是在结合结构化全局规划时提升效果。&lt;h4&gt;方法&lt;/h4&gt;将高斯信念传播与路径积分相结合，并引入跟踪因子；使用快速探索随机树和基于预定义车道结构的结构化规划器进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在单智能体和多智能体场景中，跟踪因子分别减少了28%和16%的路径偏差，证明了其提高多智能体协调的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过引入跟踪因子和其他技术改进，在各种导航和通信挑战下表现出色，特别是在结合结构化全局规划时效果更佳。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要介绍了一种新的解决多代理路径规划问题的方法，通过将高斯信念传播与路径积分相结合，并且添加了跟踪因素来确保严格遵守全局路径。这项工作旨在克服现有方法的局限性，特别是在提高多智能体协调方面的表现，尤其是在结合结构化全局规划的情况下。研究在不同的全球路径规划策略（快速探索随机树和利用预定义车道结构改进协作性的结构化规划器）上进行了测试，并在一个模拟环境中对所有场景的有效性进行了验证。实验结果显示，在单个代理和多个代理的场景中，跟踪因素分别减少了28％和16％的路径偏差，表明其提高了多智能体协调的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent path planning is a critical challenge in robotics, requiringagents to navigate complex environments while avoiding collisions andoptimizing travel efficiency. This work addresses the limitations of existingapproaches by combining Gaussian belief propagation with path integration andintroducing a novel tracking factor to ensure strict adherence to global paths.The proposed method is tested with two different global path-planningapproaches: rapidly exploring random trees and a structured planner, whichleverages predefined lane structures to improve coordination. A simulationenvironment was developed to validate the proposed method across diversescenarios, each posing unique challenges in navigation and communication.Simulation results demonstrate that the tracking factor reduces path deviationby 28% in single-agent and 16% in multi-agent scenarios, highlighting itseffectiveness in improving multi-agent coordination, especially when combinedwith structured global planning.</description>
      <author>example@mail.com (Jens Høigaard Jensen, Kristoffer Plagborg Bak Sørensen, Jonas le Fevre Sejersen, Andriy Sarabakha)</author>
      <guid isPermaLink="false">2502.20369v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Tactile Sensing for Learning Reach and Grasp</title>
      <link>http://arxiv.org/abs/2502.20367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了利用强化学习和触觉传感来实现稳健且反应迅速的机器人抓取方法，并通过不同模型自由强化学习方法对比研究了视觉感知不完美情况下的多种触觉环境设置。&lt;h4&gt;背景&lt;/h4&gt;在当前的机器人应用中，稳定且鲁棒性的机械手抓握技术至关重要。最近的研究表明，利用大规模数据集和监督学习可以提高反向对称性抓取的速度和精度，但这些方法对于长时规划面临感知错误和校准误差的挑战。&lt;h4&gt;目的&lt;/h4&gt;评估不同复杂度的基于力的触觉传感如何影响抓取任务中的强化学习行为，并通过对比研究发现有效的触觉设置以改进机械手抓握性能。&lt;h4&gt;方法&lt;/h4&gt;采用两种无模型强化学习算法来研究反向对称性抓取问题，分析了在视觉感知不完美的情况下，不同的触觉和环境设定对于改善机械手抓握能力的效用。&lt;h4&gt;主要发现&lt;/h4&gt;不同种类的触觉特征可以提高学习效果；然而，复杂的触觉输入则会增加训练难度。&lt;h4&gt;结论&lt;/h4&gt;利用强化学习结合触觉传感有望成为实现更稳健、反应更快的机器人抓取运动的一种有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stable and robust robotic grasping is essential for current and future robotapplications. In recent works, the use of large datasets and supervisedlearning has enhanced speed and precision in antipodal grasping. However, thesemethods struggle with perception and calibration errors due to large planninghorizons. To obtain more robust and reactive grasping motions, leveragingreinforcement learning combined with tactile sensing is a promising direction.Yet, there is no systematic evaluation of how the complexity of force-basedtactile sensing affects the learning behavior for grasping tasks. This papercompares various tactile and environmental setups using two model-freereinforcement learning approaches for antipodal grasping. Our findings suggestthat under imperfect visual perception, various tactile features improvelearning outcomes, while complex tactile inputs complicate training.</description>
      <author>example@mail.com (Boya Zhang, Iris Andrussow, Andreas Zell, Georg Martius)</author>
      <guid isPermaLink="false">2502.20367v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems</title>
      <link>http://arxiv.org/abs/2502.06581v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文综述了云边端协同（CETC）系统在视频分析领域的应用和发展，特别关注其架构组件、资源管理和边缘计算平台。&lt;h4&gt;背景&lt;/h4&gt;随着视频数据的爆炸性增长，分布式视频分析在云边端协作系统中得到了快速发展，这些系统能够实现高效的视频处理、实时推理和隐私保护。&lt;h4&gt;目的&lt;/h4&gt;该综述旨在分析CETC系统的基础架构组成部分，并探讨其在视频监控、自动驾驶和智慧城市等领域的应用突破。&lt;h4&gt;方法&lt;/h4&gt;文中首先剖析了包括层级式、分布式及混合框架在内的基本架构组件，以及边缘计算平台与资源管理机制。同时，还介绍了以云为中心的方法利用强大计算能力处理复杂的视频理解和模型训练问题，并探索了结合自适应任务卸载和资源感知调度技术的混合视频分析。&lt;h4&gt;主要发现&lt;/h4&gt;文章指出，除了传统方法外，大型语言模型和多模态整合领域的最新进展揭示了在平台可扩展性、数据保护和系统可靠性方面的机会与挑战。&lt;h4&gt;结论&lt;/h4&gt;未来的研究方向包括可解释性系统的开发、高效的处理机制以及高级视频分析技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of video data has driven the development of distributedvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enablingefficient video processing, real-time inference, and privacy-preservinganalysis. Among multiple advantages, CETC systems can distribute videoprocessing tasks and enable adaptive analytics across cloud, edge, and terminaldevices, leading to breakthroughs in video surveillance, autonomous driving,and smart cities. In this survey, we first analyze fundamental architecturalcomponents, including hierarchical, distributed, and hybrid frameworks,alongside edge computing platforms and resource management mechanisms. Buildingupon these foundations, edge-centric approaches emphasize on-device processing,edge-assisted offloading, and edge intelligence, while cloud-centric methodsleverage powerful computational capabilities for complex video understandingand model training. Our investigation also covers hybrid video analyticsincorporating adaptive task offloading and resource-aware scheduling techniquesthat optimize performance across the entire system. Beyond conventionalapproaches, recent advances in large language models and multimodal integrationreveal both opportunities and challenges in platform scalability, dataprotection, and system reliability. Future directions also encompassexplainable systems, efficient processing mechanisms, and advanced videoanalytics, offering valuable insights for researchers and practitioners in thisdynamic field.</description>
      <author>example@mail.com (Linxiao Gong, Hao Yang, Gaoyun Fang, Bobo Ju, Juncen Guo, Xiaoguang Zhu, Xiping Hu, Yan Wang, Peng Sun, Azzedine Boukerche)</author>
      <guid isPermaLink="false">2502.06581v3</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory-to-Action Pipeline (TAP): Automated Scenario Description Extraction for Autonomous Vehicle Behavior Comparison</title>
      <link>http://arxiv.org/abs/2502.20353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Trajectory-to-Action Pipeline (TAP)的方法，该方法可以自动从大规模轨迹数据集中提取Scenario Description Languages (SDL)标签。这项工作为自动驾驶车辆(AVs)的安全分析和行为评估提供了一个可扩展的基础。&lt;h4&gt;背景&lt;/h4&gt;Scenario Description Languages (SDLs) 提供了结构化、可解释的嵌入表示法，用于描述自主车辆遇到的各种交通场景，并支持关键任务如相似场景搜索和边缘案例检测等安全分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种自动化的TAP方法来从大规模轨迹数据集中提取SDL标签，从而提升自动驾驶车的安全性和评估效率。&lt;h4&gt;方法&lt;/h4&gt;TAP采用基于规则的交叉熵优化方法直接从数据中学习参数，以提高在不同驾驶环境中的泛化能力。使用Waymo Open Motion Dataset (WOMD)作为实验平台来验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;与Average Displacement Error (ADE)相比，TAP在识别行为相似轨迹时提高了30%的精度；而与Dynamic Time Warping (DTW)相比，则提升了24%。此外，该方法还能够自动检测独特的驾驶行为，简化了AV测试的安全评估过程。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于场景的自动驾驶车辆行为分析提供了可扩展的基础，并且具有在多代理环境下进行集成应用的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;Scenario Description Languages (SDLs) 提供结构化、可解释的嵌入表示来代表自动驾驶汽车遇到的各种交通场景，支持包括相似场景搜索和边缘案例检测等关键任务的安全分析。论文介绍了Trajectory-to-Action Pipeline (TAP)，这是一种从大规模轨迹数据集中提取SDL标签的大规模且自动化的方案。通过使用基于规则的方法进行交叉熵优化，TAP直接从数据中学习参数以增强在各种驾驶情境下的泛化能力。利用Waymo Open Motion Dataset (WOMD)作为实验平台，TAP在识别行为相似的轨迹方面超越了Average Displacement Error（ADE）30%，而相对于Dynamic Time Warping (DTW)则提升24%。此外，该方案还能够自动检测独特驾驶行为，简化AV测试的安全评估过程。这项研究为基于场景的自动驾驶车辆行为分析提供了可扩展的基础，并具有多代理环境集成应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scenario Description Languages (SDLs) provide structured, interpretableembeddings that represent traffic scenarios encountered by autonomous vehicles(AVs), supporting key tasks such as scenario similarity searches and edge casedetection for safety analysis. This paper introduces the Trajectory-to-ActionPipeline (TAP), a scalable and automated method for extracting SDL labels fromlarge trajectory datasets. TAP applies a rules-based cross-entropy optimizationapproach to learn parameters directly from data, enhancing generalizationacross diverse driving contexts. Using the Waymo Open Motion Dataset (WOMD),TAP achieves 30% greater precision than Average Displacement Error (ADE) and24% over Dynamic Time Warping (DTW) in identifying behaviorally similartrajectories. Additionally, TAP enables automated detection of unique drivingbehaviors, streamlining safety evaluation processes for AV testing. This workprovides a foundation for scalable scenario-based AV behavior analysis, withpotential extensions for integrating multi-agent contexts.</description>
      <author>example@mail.com (Aron Harder, Madhur Behl)</author>
      <guid isPermaLink="false">2502.20353v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Reservoir Computing and Photoelectrochemical Sensors: A Marriage of Convenience</title>
      <link>http://arxiv.org/abs/2502.20342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;感应技术是信息处理的重要组成部分。当前的人工智能系统（尤其是针对医疗和环境应用的）需要大量关于生物流体或环境样本化学组成的数据。&lt;h4&gt;目的&lt;/h4&gt;介绍将光电化学传感器与非传统计算范式——蓄水池计算进行集成的想法，以提升传感器性能并开辟新的科学路径。&lt;h4&gt;方法&lt;/h4&gt;探讨如何通过结合光电化学传感技术与蓄水池计算系统来改进感应设备和促进仿生感官信息处理的研究。&lt;h4&gt;主要发现&lt;/h4&gt;将光电化学传感器与蓄水池计算相结合可以克服一些现有障碍，并可能在自主机器人技术和仿生感知领域取得突破性进展。&lt;h4&gt;结论&lt;/h4&gt;这种集成方法不仅能够提升传感器本身的性能，还能开启有效信息采集和处理的新时代。&lt;h4&gt;翻译&lt;/h4&gt;摘要所述内容的中文翻译已经完成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.ccr.2023.215155&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensing technology is an important aspect of information processing. Currentdevelopment in artificial intelligence systems (especially those aimed atmedical and environmental applications) requires a lot of data on the chemicalcomposition of biological fluids or environmental samples. These complexmatrices require advanced sensing devices, and photoelectrochemical ones seemto have potential to overcome at least some of the obstacles. Furthermore, thedevelopment of artificial intelligence (AI) technology for autonomous roboticsrequires technology mimicking human senses, also those operating at themolecular level, such as gustation and olfaction. Again, photoelectrochemicalsensing can provide some suitable solutions. In this review, we introduce theidea of integration of photoelectrochemical sensors with some unconventionalcomputing paradigm - reservoir computing. This approach should not only boostthe performance of the sensors itself, but also open new pathways throughscience. Integration of sensing devices with computing systems will alsocontribute to a better understanding (or at least mimicking) of the humansenses and neuromorphic sensory information processing. Although reservoirsystems can be considered magic "black boxes" and their operation is at thesame time simple and hard to comprehend, this combination is expected to open anew era of effective information harvesting and processing systems.</description>
      <author>example@mail.com (Gisya Abdi, Lulu Alluhaibi, Ewelina Kowalewska, Tomasz Mazur, Krzysztof Mech, Agnieszka Podborska, Andrzej Sławek, Hirofumi Tanaka, Konrad Szaciłowski)</author>
      <guid isPermaLink="false">2502.20342v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application</title>
      <link>http://arxiv.org/abs/2502.20326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 Pages, 21 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对无GNSS信号的室内环境中的多无人机系统的自主导航和任务分配的整体框架。&lt;h4&gt;背景&lt;/h4&gt;在没有全球导航卫星系统（GNSS）支持的室内环境中，现有的无人机系统面临着挑战，尤其是在导航、障碍物避让和任务协调方面。这些问题阻碍了无人机系统在搜索救援和其他探索活动中的有效应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于深度强化学习（DRL）的自主指导机制，并通过图卷积网络（GCN）实现动态实时的任务分配，以提高多无人机系统的导航能力和任务协作效率。&lt;h4&gt;方法&lt;/h4&gt;{'1': '使用Twin Delayed Deep Deterministic Policy Gradient算法进行自主导航训练，引入人工势场(APF)奖励结构来优化培训过程。', '2': '利用图卷积网络（GCN）处理多无人机之间的合作任务分配问题。', '3': '采用LiDAR同步定位和建图(SLAM)技术结合深度相机解决室内环境中的精确定位问题，同时缓解走廊效应。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '基于APF的奖励结构可以显著提高无人机在室内环境中的导航效率。', '2': '采用DRL训练的GCN能够有效处理任务分配，并实现动态和实时的任务调整。', '3': 'LiDAR SLAM结合深度相机提供了一种有效的定位解决方案，增强了系统的依赖性和可靠性。'}&lt;h4&gt;结论&lt;/h4&gt;所提出的多无人机框架不仅提升了单个无人机的导航能力，还优化了复杂障碍环境中的任务分配协调，实验结果表明该系统在特定条件下的表现优异，并且在2024年NATO Sapience自主合作无人机比赛中获得了第一名。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文提出了一种针对无GNSS信号的室内环境中多无人机系统的整体框架。我们提倡使用基于深度强化学习（DRL）的方法进行自主导航，利用Twin Delayed Deep Deterministic Policy Gradient算法。为了提高训练过程中的效率，我们采用人工势场(APF)奖励结构，使代理能够优化其移动路径，从而促进更顺畅的路径和增强的障碍物避让能力。此外，通过基于DRL的图卷积网络（GCN）解决了任务分配问题，该方法表示了无人机与任务之间的交互，实现了动态和实时的任务分配，反映了当前环境条件以及无人机的能力。这种做法促进了多无人机系统在搜索救援或其他探索活动中的有效协调合作。最后，在缺乏GNSS的情况下，我们采用LiDAR SLAM配合深度相机来确保精确定位，解决了走廊效应问题。该集成提供了强大的定位和映射功能，从而增强了室内导航系统的可靠性。所提出的多无人机框架不仅提高了单个无人机的导航能力，还优化了复杂障碍环境中的任务分配协调，在为NATO Sapience自主合作无人机竞赛量身定制的实验环境中进行了测试，并取得了卓越的成绩，最终在2024年的比赛中获得第一名。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a holistic framework for autonomous guidance, navigation,and task distribution among multi-drone systems operating in Global NavigationSatellite System (GNSS)-denied indoor settings. We advocate for a DeepReinforcement Learning (DRL)-based guidance mechanism, utilising the TwinDelayed Deep Deterministic Policy Gradient algorithm. To improve the efficiencyof the training process, we incorporate an Artificial Potential Field(APF)-based reward structure, enabling the agent to refine its movements,thereby promoting smoother paths and enhanced obstacle avoidance in indoorcontexts. Furthermore, we tackle the issue of task distribution amongcooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). ThisGCN represents the interactions between drones and tasks, facilitating dynamicand real-time task allocation that reflects the current environmentalconditions and the capabilities of the drones. Such an approach fosterseffective coordination and collaboration among multiple drones during searchand rescue operations or other exploratory endeavours. Lastly, to ensureprecise odometry in environments lacking GNSS, we employ Light Detection AndRanging Simultaneous Localisation and Mapping complemented by a depth camera tomitigate the hallway problem. This integration offers robust localisation andmapping functionalities, thereby enhancing the systems dependability in indoornavigation. The proposed multi-drone framework not only elevates individualnavigation capabilities but also optimises coordinated task allocation incomplex, obstacle-laden environments. Experimental evaluations conducted in asetup tailored to meet the requirements of the NATO Sapience AutonomousCooperative Drone Competition demonstrate the efficacy of the proposed system,yielding outstanding results and culminating in a first-place finish in the2024 Sapience competition.</description>
      <author>example@mail.com (Thomas Hickling, Maxwell Hogan, Abdulla Tammam, Nabil Aouf)</author>
      <guid isPermaLink="false">2502.20326v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>On Adversarial Attacks In Acoustic Drone Localization</title>
      <link>http://arxiv.org/abs/2502.20325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼无人驾驶飞行器（MAVs，即无人机）由于在农业、商业递送和搜救等领域应用广泛而日益受到关注。然而，在非受控环境中使用时，导航系统的潜在对抗攻击威胁对任务的成功率和安全性构成了挑战。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的方法对光照条件和遮挡非常敏感，促使研究者们开始探索依赖于声学传感器等其他模态的导航方式。虽然在无人机定位方面已有利用声学方法取得的研究进展，但针对其导航系统的对抗攻击方面的研究仅限于基于视觉感知的系统。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本工作旨在通过分析PGD（Projected Gradient Descent）对抗攻击对声学无人机定位的影响进行全面分析，并开发一种算法来恢复对抗扰动以减轻这种攻击的效果。&lt;h4&gt;方法&lt;/h4&gt;本工作首先评估了在声学传感器上应用PGD对抗攻击的效果；然后设计了一种新的算法，旨在减少这些攻击对声学导航系统造成的不利影响。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，在无人机的声学定位系统中实施PGD攻击可以显著降低其准确性和可靠性。而所提出的扰动恢复方法能有效减轻此类攻击的影响。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了在开发新型无人飞行器时，对导航系统进行抗干扰设计的重要性，并为未来的对抗安全研究提供了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;多旋翼无人驾驶飞行器（MAVs，即无人机）由于它们在农业、商业递送和搜救等广泛领域的应用而近年来引起了越来越多的关注。基于视觉的方法对于光线条件和遮挡非常敏感，这促使了对依赖声学感知等其他模态的导航系统研究的增长。使用无人机执行非受控环境中的任务时的一个主要担忧是其导航系统的潜在对抗攻击威胁，这种威胁可能会导致关键任务失败、安全漏洞及危及操作员和旁观者安全的风险。尽管以往的研究已经在基于视觉感知的无人机定位方面取得了进展，但是之前有关针对无人机导航系统进行对抗攻击的研究仅限于视觉感知系统。在这项工作中，我们的目标是通过提供PGD（Projected Gradient Descent）对抗攻击对声学无人机定位影响的全面分析来填补这一空白，并且开发出一种能够在我们设定的情况下显著减少这种攻击效果的扰动恢复算法。在发表后我们将公开所有实验代码以供复现研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-rotor aerial autonomous vehicles (MAVs, more widely known as "drones")have been generating increased interest in recent years due to their growingapplicability in a vast and diverse range of fields (e.g., agriculture,commercial delivery, search and rescue). The sensitivity of visual-basedmethods to lighting conditions and occlusions had prompted growing study ofnavigation reliant on other modalities, such as acoustic sensing. A majorconcern in using drones in scale for tasks in non-controlled environments isthe potential threat of adversarial attacks over their navigational systems,exposing users to mission-critical failures, security breaches, and compromisedsafety outcomes that can endanger operators and bystanders. While previous workshows impressive progress in acoustic-based drone localization, prior researchin adversarial attacks over drone navigation only addresses visualsensing-based systems. In this work, we aim to compensate for this gap bysupplying a comprehensive analysis of the effect of PGD adversarial attacksover acoustic drone localization. We furthermore develop an algorithm foradversarial perturbation recovery, capable of markedly diminishing the affectof such attacks in our setting. The code for reproducing all experiments willbe released upon publication.</description>
      <author>example@mail.com (Tamir Shor, Chaim Baskin, Alex Bronstein)</author>
      <guid isPermaLink="false">2502.20325v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments</title>
      <link>http://arxiv.org/abs/2502.20217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright} 20XX IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在多机器人探索中，一支移动机器人的团队被赋予高效地绘制未知环境的任务。尽管大多数探索规划器假设使用类似LiDAR的全向传感器，但这种做法对于像无人机这样的小型机器人来说是不切实际的，因为载荷限制可能导致只能使用轻量级的方向性传感器如摄像头。&lt;h4&gt;背景&lt;/h4&gt;在多机器人探索中，当面对具有有限视场（FoV）的小型机器人时，传统的探索规划器假设全向传感器的应用变得不再现实。这些小型机器人的传感器受限于方向性和视野范围的约束，增加了问题解决的复杂度。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于拥有有限视场的多机器人系统的新框架MARVEL，以增强其在大型室内环境中的协调能力和决策能力。&lt;h4&gt;方法&lt;/h4&gt;通过结合图注意力网络和创新性的前沿及姿态特征融合技术，使用强化学习（MARL）开发了一种协作式、去中心化的策略。此外还引入了信息驱动的动作修剪策略来处理视角规划的大动作空间问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，与现有最先进的探索规划器相比，MARVEL所学得的政策展示了有效的协同行为，并在多个评价指标上表现出色。该方法的通用性得到了验证，甚至在一个高达90米乘以90米的大规模环境中也表现良好。此外还通过真实硬件无人机团队的成功部署证明了其实用性和可操作性。&lt;h4&gt;结论&lt;/h4&gt;MARVEL框架为具有有限视场的多机器人探索提供了一个有效的解决方案，并展示了其在大规模复杂环境中的优越性能，同时适用于各种团队规模和传感器配置（即FoV和传感器范围）无需额外训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In multi-robot exploration, a team of mobile robot is tasked with efficientlymapping an unknown environments. While most exploration planners assumeomnidirectional sensors like LiDAR, this is impractical for small robots suchas drones, where lightweight, directional sensors like cameras may be the onlyoption due to payload constraints. These sensors have a constrainedfield-of-view (FoV), which adds complexity to the exploration problem,requiring not only optimal robot positioning but also sensor orientation duringmovement. In this work, we propose MARVEL, a neural framework that leveragesgraph attention networks, together with novel frontiers and orientationfeatures fusion technique, to develop a collaborative, decentralized policyusing multi-agent reinforcement learning (MARL) for robots with constrainedFoV. To handle the large action space of viewpoints planning, we furtherintroduce a novel information-driven action pruning strategy. MARVEL improvesmulti-robot coordination and decision-making in challenging large-scale indoorenvironments, while adapting to various team sizes and sensor configurations(i.e., FoV and sensor range) without additional training. Our extensiveevaluation shows that MARVEL's learned policies exhibit effective coordinatedbehaviors, outperforming state-of-the-art exploration planners across multiplemetrics. We experimentally demonstrate MARVEL's generalizability in large-scaleenvironments, of up to 90m by 90m, and validate its practical applicabilitythrough successful deployment on a team of real drone hardware.</description>
      <author>example@mail.com (Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti)</author>
      <guid isPermaLink="false">2502.20217v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Model-Based Reinforcement Learning with State-Space World Models</title>
      <link>http://arxiv.org/abs/2502.20168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;强化学习（RL）是机器人学习的一种有效方式，但模型无关的强化学习（MFRL）需要大量环境交互才能获得成功的控制策略。相比之下，基于模型的强化学习（MBRL）通过同时训练世界模型和策略来提高样本效率，但是这种方法增加了计算复杂性。&lt;h4&gt;背景&lt;/h4&gt;传统的MFRL方法在处理复杂的非线性和噪声传感器信号时遇到挑战，导致需要大量的环境交互以获取成功的行为策略。而MBRL可以利用世界模型进行规划或数据收集，并且能够提供一阶策略梯度来训练策略。&lt;h4&gt;目的&lt;/h4&gt;提出一种加速基于状态空间世界的模型强化学习（MBRL）的新方法，特别是针对复杂和部分可观测的现实场景。&lt;h4&gt;方法&lt;/h4&gt;该研究利用状态空间模型（SSMs）并行化世界动力学模型的训练过程，并且在训练阶段给世界模型提供特权信息以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在多个实际敏捷四旋翼飞行任务中表现出显著的速度提升，将世界模型训练时间减少高达10倍，而整个MBRL的训练时间减少4倍以上。同时，该方法并没有牺牲样本效率或任务奖励。&lt;h4&gt;结论&lt;/h4&gt;通过利用SSMs并行化和提供特权信息的方式，新的MBRL技术可以极大地加速复杂场景中的学习过程，而不降低性能指标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：强化学习是机器人学习的一个强大手段。然而，模型无关的强化学习需要大量的环境交互才能成功地学习控制策略。这是因为嘈杂的学习更新以及机器人系统的复杂性通常涉及到高度非线性的动态和噪声传感器信号。相比之下，基于模型的强化学习不仅训练一个策略还同时学习一个世界模型来捕获环境的动力学和奖励。该世界模型可以用于规划、数据收集或提供一阶策略梯度来进行训练。利用世界模型相比于无模型的强化学习显著提高了样本效率。然而，与策略一起训练的世界模型增加了计算复杂性，导致了更长的训练时间，在复杂的现实场景中通常是不可行的。在这项工作中，我们提出了一种新的方法来通过状态空间世界模型加速基于模型的强化学习。我们的方法利用状态空间模型（SSMs）并行化动力学模型的训练过程，这是通常的主要计算瓶颈。此外，我们提出了一个架构，在训练过程中给世界模型提供特权信息，这对于部分可观测环境尤其相关。我们在多个实际敏捷四旋翼飞行任务中评估了这种方法，包括完全和部分可观测环境中的复杂动态。我们展示了显著的速度提升，将世界模型的训练时间减少高达10倍，并且整个基于模型的学习的时间减少了4倍以上。这种优势没有牺牲性能，因为我们的方法在样本效率和任务奖励方面与最先进的基于模型的方法类似。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) is a powerful approach for robot learning.However, model-free RL (MFRL) requires a large number of environmentinteractions to learn successful control policies. This is due to the noisy RLtraining updates and the complexity of robotic systems, which typically involvehighly non-linear dynamics and noisy sensor signals. In contrast, model-basedRL (MBRL) not only trains a policy but simultaneously learns a world model thatcaptures the environment's dynamics and rewards. The world model can either beused for planning, for data collection, or to provide first-order policygradients for training. Leveraging a world model significantly improves sampleefficiency compared to model-free RL. However, training a world model alongsidethe policy increases the computational complexity, leading to longer trainingtimes that are often intractable for complex real-world scenarios. In thiswork, we propose a new method for accelerating model-based RL using state-spaceworld models. Our approach leverages state-space models (SSMs) to parallelizethe training of the dynamics model, which is typically the main computationalbottleneck. Additionally, we propose an architecture that provides privilegedinformation to the world model during training, which is particularly relevantfor partially observable environments. We evaluate our method in severalreal-world agile quadrotor flight tasks, involving complex dynamics, for bothfully and partially observable environments. We demonstrate a significantspeedup, reducing the world model training time by up to 10 times, and theoverall MBRL training time by up to 4 times. This benefit comes withoutcompromising performance, as our method achieves similar sample efficiency andtask rewards to state-of-the-art MBRL methods.</description>
      <author>example@mail.com (Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2502.20168v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Geometry and Mechanics of Non-Euclidean Curved-Crease Origami</title>
      <link>http://arxiv.org/abs/2502.20147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近有关于弯曲折纸的理论、数值和实验工作的大量研究，但注意到缺乏一个统一且完整的几何框架来描述具有非欧曲率的弯曲折纸的几何与力学特性。本文提供了一个通用的几何框架，用于描述由两条一般带组成的弯曲折纸形状，并表明根据空间折叠线及其配置分支可以得出四种不同的状态。在该几何框架内，推导出平衡方程并研究了这种结构的机械响应，重点关注欧拉屈曲行为。通过线性稳定性分析和有限元模拟发现，重叠构型表现出较低的屈曲阈值。为了更有效地捕捉大变形行为，基于各向异性Kirchhoff杆理论开发了一个双带模型，并成功预测出主要特性。&lt;h4&gt;背景&lt;/h4&gt;最近在弯曲折纸方面开展了大量的理论、数值和实验工作，然而对于具有非欧曲率（非平直折叠线）的结构缺乏一个统一而完整的几何力学框架。&lt;h4&gt;目的&lt;/h4&gt;提供一种通用的几何框架以描述由两条一般带组成的任意形状的弯曲折纸，并研究其机械响应。&lt;h4&gt;方法&lt;/h4&gt;1. 提供了一个几何框架来描述弯曲折纸的形状，包括四种可能的状态；2. 在给定的框架内推导了平衡方程并进行了线性稳定性分析和有限元模拟以研究其机械行为；3. 基于各向异性Kirchhoff杆理论开发了一种双带模型。&lt;h4&gt;主要发现&lt;/h4&gt;四种不同状态取决于空间折叠线及其配置分支，重叠构型表现出较低的屈曲阈值。新模型能够准确预测弯曲折纸的大变形行为。&lt;h4&gt;结论&lt;/h4&gt;这项工作建立了一个关于弯曲折纸几何与力学之间的联系，为机器人学、致动器和可展开太空结构等应用提供了新的见解，并且开发的新模型可以成功预测主要特性。&lt;h4&gt;翻译&lt;/h4&gt;最近有关于弯曲折纸的理论、数值和实验工作的大量研究，但注意到缺乏一个统一且完整的几何框架来描述具有非欧曲率的弯曲折纸的几何与力学特性。本文提供了一个通用的几何框架，用于描述由两条一般带组成的弯曲折纸形状，并表明根据空间折叠线及其配置分支可以得出四种不同的状态。在该几何框架内，推导出平衡方程并研究了这种结构的机械响应，重点关注欧拉屈曲行为。通过线性稳定性分析和有限元模拟发现，重叠构型表现出较低的屈曲阈值。为了更有效地捕捉大变形行为，基于各向异性Kirchhoff杆理论开发了一个双带模型，并成功预测出主要特性。这项工作建立了一个关于弯曲折纸几何与力学之间的联系，为机器人学、致动器和可展开太空结构等应用提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently there have been extensive theoretical, numerical and experimentalworks on curved-fold origami. However, we notice that a unified and completegeometric framework for describing the geometry and mechanics of curved-foldorigami, especially those with nontrivial Gaussian curvature at the crease(non-Euclidean crease), is still absent. Herein we provide a unified geometricframework that describes the shape of a generic curved-fold origami composed oftwo general strips. The explicit description indicates that four configurationsemerge, determined by its spatial crease and configuration branch. Within thisgeometric framework, we derive the equilibrium equations and study themechanical response of the curved-crease origami, focusing on Euler's bucklingbehavior. Both linear stability analysis and finite element simulation indicatethat the overlaid configuration exhibits a lower buckling threshold. To furthercapture the large deformation behavior efficiently, we develop a bistrip modelbased on the anisotropic Kirchhoff rod theory, which predicts the main featuressuccessfully. This work bridges the geometry and mechanics of curved-creaseorigami, offering insights for applications in robotics, actuators, anddeployable space structures.</description>
      <author>example@mail.com (Zhixuan Wen, Tian Yu, Fan Feng)</author>
      <guid isPermaLink="false">2502.20147v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Antagonists in Networks of Systems: Robot Deployment</title>
      <link>http://arxiv.org/abs/2502.20125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于上下文的异常检测方法，应用于执行覆盖任务的机器人集群的物理运动。&lt;h4&gt;背景&lt;/h4&gt;在模拟环境中训练正常行为数据以识别对抗性行为或异常情况。&lt;h4&gt;目的&lt;/h4&gt;通过使用机器学习模型预测机器人动作的可能范围来识别执行特定任务时出现的异常情况。&lt;h4&gt;方法&lt;/h4&gt;利用正态流(normalizing flow)预测机器人运动的可能性，并根据此概率判断机器人是正常的还是对抗性的。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在五种不同的对抗性行为策略中表现良好，准确分类至少80%的每一种对抗类型，同时保持低于5%的假阳性率。此外，硬件实验验证了与模拟场景相似的结果。&lt;h4&gt;结论&lt;/h4&gt;相较于现有最佳的方法，本文提出的方法提高了预测性能，并增强了检测标准的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：提出了上下文异常检测方法并应用于执行覆盖任务时机器人集群的物理运动。通过模拟正常行为的数据训练正态流以预测给定环境下机器人的动作可能性。在应用中，利用观察到的动作预测概率来判断机器人是否属于正常或对抗性质的行为。该方法对五种不同的对抗策略进行评估，仅使用正常机器人行为的仿真数据进行训练，在未知异常本质的情况下实现了至少80%以上的准确率分类，并保持低于5%的假阳性率。另外通过硬件实验进一步验证了这一发现。相比现有最佳的方法，本文所提出的模型在预测性能和检测标准的鲁棒性上均有所提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A contextual anomaly detection method is proposed and applied to the physicalmotions of a robot swarm executing a coverage task. Using simulations of aswarm's normal behavior, a normalizing flow is trained to predict thelikelihood of a robot motion within the current context of its environment.During application, the predicted likelihood of the observed motions is used bya detection criterion that categorizes a robot agent as normal or antagonistic.The proposed method is evaluated on five different strategies of antagonisticbehavior. Importantly, only readily available simulated data of normal robotbehavior is used for training such that the nature of the anomalies need not beknown beforehand. The best detection criterion correctly categorizes at least80% of each antagonistic type while maintaining a false positive rate of lessthan 5% for normal robot agents. Additionally, the method is validated inhardware experiments, yielding results similar to the simulated scenarios.Compared to the state-of-the-art approach, both the predictive performance ofthe normalizing flow and the robustness of the detection criterion areincreased.</description>
      <author>example@mail.com (Ingeborg Wenger, Peter Eberhard, Henrik Ebel)</author>
      <guid isPermaLink="false">2502.20125v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2502.20108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个新的自主驾驶决策制定管道VDT-Auto，它通过结合视觉语言模型（VLM）的状态理解和基于扩散Transformer的动作生成来解决动态环境和边缘情况带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶中，动态的环境因素和边角案例对车辆决策系统的鲁棒性构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在提出一种新的方法以提升自主驾驶系统应对复杂和变化环境的能力，增强其在各种条件下的稳健性和性能表现。&lt;h4&gt;方法&lt;/h4&gt;该方法首先通过鸟瞰图（BEV）编码器提取周围图像的特征网格，并利用经过微调的视觉语言模型（VLM）生成文本嵌入和噪声路径。然后使用这些输出作为扩散过程中的正向和反向过程的条件。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该系统在nuScenes开放环规划评估中平均L2误差为0.52米，并且碰撞率为21%，展示了其卓越的一般性能。&lt;h4&gt;结论&lt;/h4&gt;VDT-Auto不仅通过开放数据集和代码发布的途径促进了研究界的进一步探索与改进，还证明了视觉语言模型（VLM）在自动驾驶决策制定中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一种新的用于提高自动驾驶系统鲁棒性的方法的描述。该方法利用视觉语言模型来理解环境，并生成相应的动作策略。实验结果显示其具有良好的性能和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous driving, dynamic environment and corner cases pose significantchallenges to the robustness of ego vehicle's decision-making. To address thesechallenges, commencing with the representation of state-action mapping in theend-to-end autonomous driving paradigm, we introduce a novel pipeline,VDT-Auto. Leveraging the advancement of the state understanding of VisualLanguage Model (VLM), incorporating with diffusion Transformer-based actiongeneration, our VDT-Auto parses the environment geometrically and contextuallyfor the conditioning of the diffusion process. Geometrically, we use abird's-eye view (BEV) encoder to extract feature grids from the surroundingimages. Contextually, the structured output of our fine-tuned VLM is processedinto textual embeddings and noisy paths. During our diffusion process, theadded noise for the forward process is sampled from the noisy path output ofthe fine-tuned VLM, while the extracted BEV feature grids and embedded textscondition the reverse process of our diffusion Transformers. Our VDT-Autoachieved 0.52m on average L2 errors and 21% on average collision rate in thenuScenes open-loop planning evaluation. Moreover, the real-world demonstrationexhibited prominent generalizability of our VDT-Auto. The code and dataset willbe released after acceptance.</description>
      <author>example@mail.com (Ziang Guo, Konstantin Gubernatorov, Selamawit Asfaw, Zakhar Yagudin, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2502.20108v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Pushing Through Clutter With Movability Awareness of Blocking Obstacles</title>
      <link>http://arxiv.org/abs/2502.20106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages (6+1), 5 images, 1 table, preprint version of accepted paper  at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种考虑可移动障碍物的路径规划框架，通过结合全局语义可见性图和局部模型预测路径积分方法来应对传统路径规划方法在面对被阻挡路径时的挑战。&lt;h4&gt;背景&lt;/h4&gt;当障碍物阻塞了到达目标的路径时，传统的路径规划方法难以处理需要推动动作的情况。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖于显式障碍物放置信息的方法框架，以克服NAMO问题中的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入全局语义可见性图和局部模型预测路径积分（SVG-MPPI）方法相结合的策略来有效采样滚动，并考虑可移动物体在整个连续范围内的移动情况。采用物理引擎模拟滚动与环境的交互结果，生成最小化接触力的轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在定性和定量实验中，SVG-MPPI框架的表现优于仅使用二进制可移动性的现有规划方法，在成功率和减少累积接触力方面都取得了更好的成绩。&lt;h4&gt;结论&lt;/h4&gt;所提出的SVG-MPPI框架提供了一种新颖的方法来处理具有可移动障碍物的导航问题，并且其代码已经在GitHub上公开供他人参考。&lt;h4&gt;翻译&lt;/h4&gt;对于在传统路径规划中遇到的由可移动障碍物导致的问题，我们提出了一套新的解决方案，该方案结合了全局语义可见性图和局部模型预测路径积分技术。通过使用物理引擎来模拟滚动与环境之间的交互，生成最小化接触力的最佳轨迹，并取得了优于现有方法的成绩。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation Among Movable Obstacles (NAMO) poses a challenge for traditionalpath-planning methods when obstacles block the path, requiring push actions toreach the goal. We propose a framework that enables movability-aware planningto overcome this challenge without relying on explicit obstacle placement. Ourframework integrates a global Semantic Visibility Graph and a local ModelPredictive Path Integral (SVG-MPPI) approach to efficiently sample rollouts,taking into account the continuous range of obstacle movability. A physicsengine is adopted to simulate the interaction result of the rollouts with theenvironment, and generate trajectories that minimize contact force. Inqualitative and quantitative experiments, SVG-MPPI outperforms the existingparadigm that uses only binary movability for planning, achieving highersuccess rates with reduced cumulative contact forces. Our code is available at:https://github.com/tud-amr/SVG-MPPI</description>
      <author>example@mail.com (Joris J. Weeda, Saray Bakker, Gang Chen, Javier Alonso-Mora)</author>
      <guid isPermaLink="false">2502.20106v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.20089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新颖的逆向强化学习（IRL）方法，该方法通过扩展最大熵IRL框架并引入平方时差(TD)正则化器和自适应目标来优化奖励函数。&lt;h4&gt;背景&lt;/h4&gt;现有的固定奖励分配方式存在局限性，难以保证灵活且可约束的隐式奖励正则化的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的IRL方法，以克服现有方法中的限制，并在模仿学习中提供对动态目标和奖励机制的有效理解。&lt;h4&gt;方法&lt;/h4&gt;通过结合最大熵IRL框架、自适应目标以及分布式的强化学习技术来优化奖励函数。具体地，引入了平方时差(TD)正则化器，该组件允许在训练过程中动态调整目标。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法在具有挑战性的MuJoCo任务上展示了最先进的性能，在Humanoid任务中仅通过三个演示就达到了专家级别的结果。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验和消融研究验证了所提方法的有效性，并提供了对模仿学习中自适应目标和奖励动态的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel Inverse Reinforcement Learning (IRL) approach thatovercomes limitations of fixed reward assignments and constrained flexibilityin implicit reward regularization. By extending the Maximum Entropy IRLframework with a squared temporal-difference (TD) regularizer and adaptivetargets, dynamically adjusted during training, our method indirectly optimizesa reward function while incorporating reinforcement learning principles.Furthermore, we integrate distributional RL to capture richer returninformation. Our approach achieves state-of-the-art performance on challengingMuJoCo tasks, demonstrating expert-level results on the Humanoid task with only3 demonstrations. Extensive experiments and ablation studies validate theeffectiveness of our method, providing insights into adaptive targets andreward dynamics in imitation learning.</description>
      <author>example@mail.com (Adib Karimi, Mohammad Mehdi Ebadzadeh)</author>
      <guid isPermaLink="false">2502.20089v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights</title>
      <link>http://arxiv.org/abs/2502.20084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在混合自主驾驶环境中，准确预测周围车辆的未来轨迹对于自动驾驶汽车的安全运行至关重要。该研究提出了一种新的认知启发型变压器（Cognitive-Informed Transformer, CITF），通过引入感知安全概念来理解驾驶员的决策机制。&lt;h4&gt;背景&lt;/h4&gt;现有模型主要关注数据中的统计模式，忽略了理解和解释人类驾驶者的决策过程的重要性，这导致了模型在长期轨迹预测方面的性能不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的模型CITF，该模型能够捕捉到人类驾驶员真实意图，并提高长期轨迹预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的模块Leanformer来捕获车辆之间的社会互动。此外，还设计了一个感知安全感知模块，包括定量安全评估和驾驶行为特征描述。&lt;h4&gt;主要发现&lt;/h4&gt;CITF模型在三个公认的基准数据集上显示出显著性能提升：NGSIM（12.0%），HighD（28.2%）以及MoCAD（20.8%）。此外，在数据有限或缺失的情况下，该模型也表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CITF不仅在现有的基准测试中表现优异，并且展示了对现实世界应用的适应性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文内容。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In mixed autonomous driving environments, accurately predicting the futuretrajectories of surrounding vehicles is crucial for the safe operation ofautonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory isdetermined by the decision-making process of human drivers. However, existingmodels primarily focus on the inherent statistical patterns in the data, oftenneglecting the critical aspect of understanding the decision-making processesof human drivers. This oversight results in models that fail to capture thetrue intentions of human drivers, leading to suboptimal performance inlong-term trajectory prediction. To address this limitation, we introduce aCognitive-Informed Transformer (CITF) that incorporates a cognitive concept,Perceived Safety, to interpret drivers' decision-making mechanisms. PerceivedSafety encapsulates the varying risk tolerances across drivers with differentdriving behaviors. Specifically, we develop a Perceived Safety-aware Modulethat includes a Quantitative Safety Assessment for measuring the subject risklevels within scenarios, and Driver Behavior Profiling for characterizingdriver behaviors. Furthermore, we present a novel module, Leanformer, designedto capture social interactions among vehicles. CITF demonstrates significantperformance improvements on three well-established datasets. In terms oflong-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM,28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, itsrobustness in scenarios with limited or missing data is evident, surpassingmost state-of-the-art (SOTA) baselines, and paving the way for real-worldapplications.</description>
      <author>example@mail.com (Haicheng Liao, Chengyue Wang, Kaiqun Zhu, Yilong Ren, Bolin Gao, Shengbo Eben Li, Chengzhong Xu, Zhenning Li)</author>
      <guid isPermaLink="false">2502.20084v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry on Ground</title>
      <link>http://arxiv.org/abs/2502.20078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BEV-DWPVO的新型单目视觉测距系统，该系统利用鸟瞰图（Bird's-Eye View, BEV）特征地图以统一尺度表示环境，简化姿态估计过程，并通过可微加权Procrustes求解器进行姿态估计。&lt;h4&gt;背景&lt;/h4&gt;单目视觉测距(MVO)为自动驾驶车辆提供了一种成本效益高、实时定位解决方案。然而，由于缺乏来自单目相机的内在尺度信息，MVO系统面临共同问题。传统的MVO方法虽然具有良好的解释性，但只能获得相对比例，并且在长距离任务中会出现严重的比例漂移。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MVO系统BEV-DWPVO，以解决传统MVO方法存在的局限性和挑战，提高其性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用鸟瞰图（BEV）特征地图表示环境，并假设地面为平面。通过在统一尺度的网格结构中提取并匹配关键点，利用可微加权Procrustes求解器进行姿态估计。整个系统完全可微，仅需姿态监督即可端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;提出的BEV-DWPVO方法在长序列数据集NCLT、牛津和KITTI上的表现优于现有的MVO方法，在大多数评估指标上取得了卓越的成绩。&lt;h4&gt;结论&lt;/h4&gt;通过使用鸟瞰图特征地图和可微加权Procrustes求解器，新系统BEV-DWPVO成功地解决了传统MVO系统的局限性，并在实际测试中展示了优越的性能。这种方法为未来的自动驾驶车辆定位提供了一种新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：单目视觉测距(MVO)为自主车辆提供了一种成本效益高、实时定位解决方案。然而，由于来自单目相机缺乏内在尺度信息，MVO系统面临共同问题。传统的MVO方法虽然具有良好的解释性，但只能获得相对比例，并且在长距离任务中会出现严重的比例漂移。基于学习的方法利用透视视角，通过大量训练数据获取先验知识并预测深度值以估计绝对比例。然而，由于需要准确估计每个点的深度，这种方法泛化能力有限。相比之下，我们提出了一种新的MVO系统称为BEV-DWPVO。我们的方法使用地面平面的共同假设，并通过鸟瞰图（BEV）特征地图表示环境，在统一尺度下的网格结构中简化姿态估计过程从6自由度到3自由度。关键点在BEV空间内被提取和匹配，随后通过可微加权Procrustes求解器进行姿态估计。整个系统完全可微，仅需姿态监督即可端到端训练，无需辅助任务。我们在挑战性的长序列数据集NCLT、牛津和KITTI上验证了BEV-DWPVO，并在大多数评估指标中超越现有MVO方法取得卓越结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular Visual Odometry (MVO) provides a cost-effective, real-timepositioning solution for autonomous vehicles. However, MVO systems face thecommon issue of lacking inherent scale information from monocular cameras.Traditional methods have good interpretability but can only obtain relativescale and suffer from severe scale drift in long-distance tasks. Learning-basedmethods under perspective view leverage large amounts of training data toacquire prior knowledge and estimate absolute scale by predicting depth values.However, their generalization ability is limited due to the need to accuratelyestimate the depth of each point. In contrast, we propose a novel MVO systemcalled BEV-DWPVO. Our approach leverages the common assumption of a groundplane, using Bird's-Eye View (BEV) feature maps to represent the environment ina grid-based structure with a unified scale. This enables us to reduce thecomplexity of pose estimation from 6 Degrees of Freedom (DoF) to 3-DoF.Keypoints are extracted and matched within the BEV space, followed by poseestimation through a differentiable weighted Procrustes solver. The entiresystem is fully differentiable, supporting end-to-end training with only posesupervision and no auxiliary tasks. We validate BEV-DWPVO on the challenginglong-sequence datasets NCLT, Oxford, and KITTI, achieving superior results overexisting MVO methods on most evaluation metrics.</description>
      <author>example@mail.com (Yufei Wei, Sha Lu, Wangtao Lu, Rong Xiong, Yue Wang)</author>
      <guid isPermaLink="false">2502.20078v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery</title>
      <link>http://arxiv.org/abs/2502.20061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种针对人形机器人跌倒恢复的多阶段课程学习框架HiFAR，该框架通过逐步增加复杂性和维度来解决传统控制方法和强化学习技术在处理高维动力学和复杂碰撞场景方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的人形机器人难以自主地从跌倒中恢复过来，尤其是面对动态且无结构的环境。传统的控制方法通常无法应对这些挑战，而基于强化学习的方法则受到稀疏奖励、复杂的碰撞情景以及仿真与实际应用之间差异的影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来帮助人形机器人有效地处理各种类型的跌倒场景，并能够适应现实世界的跌倒情况。&lt;h4&gt;方法&lt;/h4&gt;使用了一种名为HiFAR的多阶段课程学习框架，该框架采用分阶段的学习策略逐步引入更为复杂和高维的恢复任务。通过这种方式让机器人在不同情况下掌握高效且稳定的跌倒恢复策略。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法经过实际的人形机器人的测试，展示了其能够自主地从多种跌倒情况中快速而稳定地恢复过来，并具有较高的成功率、较快的恢复时间和较强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究结果表明，HiFAR框架为解决人形机器人跌倒恢复问题提供了一种有效的方法。这种方法不仅提高了机器人的适应性和稳定性，还大大增强了其自主应对复杂环境的能力。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人在动态和无结构环境中从跌倒中自主恢复面临巨大挑战。传统的控制方法不足以处理高维动力学和密集接触的特点，而强化学习技术则受到稀疏奖励、复杂碰撞场景以及模拟与现实应用差异的困扰。本文提出了一种名为HiFAR的多阶段课程学习框架，通过逐步纳入更加复杂的跌倒恢复任务来帮助机器人获取高效且稳定的策略。该方法已在实际的人形机器人体上进行了测试，并证明了其在跌倒恢复上的自主性、快速性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid robots encounter considerable difficulties in autonomouslyrecovering from falls, especially within dynamic and unstructured environments.Conventional control methodologies are often inadequate in addressing thecomplexities associated with high-dimensional dynamics and the contact-richnature of fall recovery. Meanwhile, reinforcement learning techniques arehindered by issues related to sparse rewards, intricate collision scenarios,and discrepancies between simulation and real-world applications. In thisstudy, we introduce a multi-stage curriculum learning framework, termed HiFAR.This framework employs a staged learning approach that progressivelyincorporates increasingly complex and high-dimensional recovery tasks, therebyfacilitating the robot's acquisition of efficient and stable fall recoverystrategies. Furthermore, it enables the robot to adapt its policy toeffectively manage real-world fall incidents. We assess the efficacy of theproposed method using a real humanoid robot, showcasing its capability toautonomously recover from a diverse range of falls with high success rates,rapid recovery times, robustness, and generalization.</description>
      <author>example@mail.com (Penghui Chen, Yushi Wang, Changsheng Luo, Wenhan Cai, Mingguo Zhao)</author>
      <guid isPermaLink="false">2502.20061v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Night-Voyager: Consistent and Efficient Nocturnal Vision-Aided State Estimation in Object Maps</title>
      <link>http://arxiv.org/abs/2502.20054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Transactions on Robotics (T-RO), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;夜间准确且稳健的状态估计对于实现自主机器人导航的昼夜或全天候任务至关重要。本文提出了一个利用先验对象地图和关键点进行夜视辅助状态估计的新框架Night-Voyager。&lt;h4&gt;背景&lt;/h4&gt;现有大多数视觉方法在不良照明条件下可能失败，即使使用主动光源或图像增强也难以克服这一问题。然而，在多数城市场景中，路灯作为稳定的显著前导视觉线索，在夜间导航中起到了类似深空星星的作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的夜视辅助状态估计框架Night-Voyager，该框架可以利用先验对象地图和关键点信息实现灵活的定位。&lt;h4&gt;方法&lt;/h4&gt;Night-Voyager通过快速初始化解决全局定位问题，并采用有效的两阶段跨模态数据关联技术来提供基于地图观测的整体一致性状态更新。此外，在处理夜间视觉观察中显著不确定性的挑战时，引入了新颖的矩阵李群公式化和特征解耦多态不变滤波器。&lt;h4&gt;主要发现&lt;/h4&gt;传统的视觉方法在照明条件不佳的情况下依赖像素级度量作为其最主要的限制，而Night-Voyager利用非像素级的对象检测来促进对象地图信息的有效传播与使用。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的仿真及多样化的实际应用场景（覆盖约12.3公里）的实验验证了Night-Voyager的有效性、鲁棒性和效率，填补了夜间视觉辅助状态估计的重要空白。&lt;h4&gt;翻译&lt;/h4&gt;准确且稳健的夜间状态估计对于实现昼夜或全天候任务下的自主机器人导航至关重要。是否可以利用低成本的标准相机进行夜间环境的状态估计？现有的大多数视觉方法在不良照明条件下难以发挥作用，即使使用主动光源或图像增强也是如此。然而，在多数城市环境中，路灯作为夜晚稳定的显著先验视觉线索起到了类似星星在深空为航天器提供导航的作用。受到这一启发，我们提出了一种新的夜视辅助状态估计框架Night-Voyager，利用先前对象地图和关键点进行灵活的定位。研究发现传统的视觉方法依赖于像素级别的度量标准作为其主要限制，在不良照明条件下表现不佳。相比之下，非像素级、无度量的对象检测可以充当从像素级别到对象级别的桥梁，促进系统内部对象地图信息的有效传播与使用。Night-Voyager通过快速初始化解决全局定位问题，并利用基于地图的观察提供整体一致性状态更新。为了应对夜间视觉观测中的显著不确定性挑战，引入了新颖的矩阵李群公式化和特征解耦多态不变滤波器以确保一致且高效的估计结果。在广泛的仿真及多样化的实际应用场景（覆盖约12.3公里）中展示其有效性、鲁棒性和效率，弥补了夜视辅助状态估计的重要不足之处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust state estimation at nighttime is essential for autonomousrobotic navigation to achieve nocturnal or round-the-clock tasks. An intuitivequestion arises: Can low-cost standard cameras be exploited for nocturnal stateestimation? Regrettably, most existing visual methods may fail under adverseillumination conditions, even with active lighting or image enhancement. Apivotal insight, however, is that streetlights in most urban scenarios act asstable and salient prior visual cues at night, reminiscent of stars in deepspace aiding spacecraft voyage in interstellar navigation. Inspired by this, wepropose Night-Voyager, an object-level nocturnal vision-aided state estimationframework that leverages prior object maps and keypoints for versatilelocalization. We also find that the primary limitation of conventional visualmethods under poor lighting conditions stems from the reliance on pixel-levelmetrics. In contrast, metric-agnostic, non-pixel-level object detection servesas a bridge between pixel-level and object-level spaces, enabling effectivepropagation and utilization of object map information within the system.Night-Voyager begins with a fast initialization to solve the globallocalization problem. By employing an effective two-stage cross-modal dataassociation, the system delivers globally consistent state updates usingmap-based observations. To address the challenge of significant uncertaintiesin visual observations at night, a novel matrix Lie group formulation and afeature-decoupled multi-state invariant filter are introduced, ensuringconsistent and efficient estimation. Through comprehensive experiments in bothsimulation and diverse real-world scenarios (spanning approximately 12.3 km),Night-Voyager showcases its efficacy, robustness, and efficiency, filling acritical gap in nocturnal vision-aided state estimation.</description>
      <author>example@mail.com (Tianxiao Gao, Mingle Zhao, Chengzhong Xu, Hui Kong)</author>
      <guid isPermaLink="false">2502.20054v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds</title>
      <link>http://arxiv.org/abs/2502.20041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的3D可及性检测框架，该框架通过引入大规模语言模型并设计定制化解码器来生成可及性掩模，解决了传统基于标签的语义分割方法在开放场景中的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的3D可及性检测依赖于预定义标签进行基于语义分割的任务，并且难以理解复杂的自然语言描述。这种范式在处理开放式复杂场景时存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的限制，提出了一种新的任务形式——指令推理可及性分割（IRAS），该任务旨在根据给定的查询文本生成可及性掩模区域。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为3D-AffordanceLLM (3D-ADLLM) 的新框架。它通过引入大规模语言模型并设计定制化解码器来实现开放世界的推理式可及性检测，并采用多阶段训练策略，包括Referring Object Part Segmentation任务的预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在缺乏足够的3D可及性数据集的情况下，该方法利用通用分割数据提取知识并转移到可及性检测中。通过这种创新的方法和框架设计，在开放词汇量的可及性检测任务上实现了大约8% mIoU的改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的3D-ADLLM框架充分利用了大规模语言模型中的丰富世界知识和人与物体互动推理能力，证明在处理开放世界的复杂场景时具有优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Affordance detection is a challenging problem with broad applications onvarious robotic tasks. Existing methods typically formulate the detectionparadigm as a label-based semantic segmentation task. This paradigm relies onpredefined labels and lacks the ability to comprehend complex natural language,resulting in limited generalization in open-world scene. To address theselimitations, we reformulate the traditional affordance detection paradigm into\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This taskis designed to output a affordance mask region given a query reasoning text,which avoids fixed categories of input labels. We accordingly propose the\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoningaffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces largelanguage models (LLMs) to 3D affordance perception with a custom-designeddecoder for generating affordance masks, thus achieving open-world reasoningaffordance detection. In addition, given the scarcity of 3D affordance datasetsfor training large models, we seek to extract knowledge from generalsegmentation data and transfer it to affordance detection. Thus, we propose amulti-stage training strategy that begins with a novel pre-training task, i.e.,\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed toequip the model with general recognition and segmentation capabilities at theobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLMobtains the reasoning ability for affordance detection. In summary, 3D-ADLLMleverages the rich world knowledge and human-object interaction reasoningability of LLMs, achieving approximately an 8\% improvement in mIoU onopen-vocabulary affordance detection tasks.</description>
      <author>example@mail.com (Hengshuo Chu, Xiang Deng, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie)</author>
      <guid isPermaLink="false">2502.20041v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</title>
      <link>http://arxiv.org/abs/2502.20037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为FuseGrasp的系统，该系统是首个雷达和相机融合技术应用于透明物体抓取的技术。通过毫米波信号和深度学习网络的有效结合，改进了机器人在低光环境下的性能。&lt;h4&gt;背景&lt;/h4&gt;透明物品在日常生活环境中普遍存在，但它们独特的物理特性给依靠摄像机引导的机械臂带来了挑战。现有研究主要依赖于单独使用相机的方法，在光照不足等条件下效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够增强抓取透明物体能力的雷达-相机融合系统，以提高机器人操作透明物品的成功率和准确性。&lt;h4&gt;方法&lt;/h4&gt;利用毫米波信号可以穿透透明材料并使其呈现半透明或不透明的特点，结合摄像机数据获取高质量的雷达图像，并设计了一个深度神经网络来融合这两种模式的数据。采用两阶段训练策略解决缺乏雷达图像的问题：首先在公共RGB-D数据集上预训练系统，然后使用小规模自建的RGB-D-Radar数据集进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示FuseGrasp显著提高了透明物体的深度重建精度和材料识别能力，在真实世界中验证了其处理透明物品的能力增强。&lt;h4&gt;结论&lt;/h4&gt;通过雷达-相机融合技术，可以有效提高机器人对环境感知能力和操作效率，尤其在低光条件下性能更优。这项工作为未来的机器人系统开发提供了新的方向。&lt;h4&gt;视频链接&lt;/h4&gt;https://youtu.be/MWDqv0sRSok&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transparent objects are prevalent in everyday environments, but theirdistinct physical properties pose significant challenges for camera-guidedrobotic arms. Current research is mainly dependent on camera-only approaches,which often falter in suboptimal conditions, such as low-light environments. Inresponse to this challenge, we present FuseGrasp, the first radar-camera fusionsystem tailored to enhance the transparent objects manipulation. FuseGraspexploits the weak penetrating property of millimeter-wave (mmWave) signals,which causes transparent materials to appear opaque, and combines it with theprecise motion control of a robotic arm to acquire high-quality mmWave radarimages of transparent objects. The system employs a carefully designed deepneural network to fuse radar and camera imagery, thereby improving depthcompletion and elevating the success rate of object grasping. Nevertheless,training FuseGrasp effectively is non-trivial, due to limited radar imagedatasets for transparent objects. We address this issue utilizing large RGB-Ddataset, and propose an effective two-stage training approach: we firstpre-train FuseGrasp on a large public RGB-D dataset of transparent objects,then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as abyproduct, FuseGrasp can determine the composition of transparent objects, suchas glass or plastic, leveraging the material identification capability ofmmWave radar. This identification result facilitates the robotic arm inmodulating its grip force appropriately. Extensive testing reveals thatFuseGrasp significantly improves the accuracy of depth reconstruction andmaterial identification for transparent objects. Moreover, real-world robotictrials have confirmed that FuseGrasp markedly enhances the handling oftransparent items. A video demonstration of FuseGrasp is available athttps://youtu.be/MWDqv0sRSok.</description>
      <author>example@mail.com (Hongyu Deng, Tianfan Xue, He Chen)</author>
      <guid isPermaLink="false">2502.20037v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Keypoint Affordance Representation for Functional Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2502.20018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code and demo videos will be publicly available at  https://github.com/PopeyePxx/MKA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究提出了一种用于功能灵巧抓握的多关键点作用表示法，直接编码任务驱动的抓取配置，并通过接触引导的关键点提取方法实现了视觉感知和灵巧操作之间的直接连接。&lt;h4&gt;背景&lt;/h4&gt;现有的基于作用的方法主要预测粗略交互区域，无法直接约束抓取姿势，导致视觉感知与操纵之间存在断开。&lt;h4&gt;目的&lt;/h4&gt;为了弥合这一差距，我们提出了一个多关键点作用表示法来解决现有方法在预测精细交互方面的问题，并通过引入Contact-guided Multi-Keypoint Affordance (CMKA) 方法和基于关键点的抓握矩阵转换(KGT)方法改进了抓取的一致性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多关键点作用表示法，利用人类抓握经验图像进行弱监督，并结合大型视觉模型提取精细的作用特征。此外，还提出了一种基于关键点的抓握手性变换（KGT）方法，确保手部关键点与物体接触点之间的空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著提高了作用定位精度、抓握一致性和对未知工具和任务的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本研究为视觉作用学习与灵巧机器人操作之间建立了桥梁，展示了在真实世界数据集、IsaacGym仿真环境及具有挑战性的机器人任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;功能性灵巧抓握需要精确的手-物体交互，超越简单的夹持。现有基于作用的方法主要预测粗略的交互区域，并且无法直接约束抓取姿势，导致视觉感知与操作之间存在断开。为解决这一问题，我们提出了一种用于功能灵巧抓握的多关键点作用表示法，该方法通过定位功能性接触点直接编码任务驱动的抓取配置。此外，还引入了Contact-guided Multi-Keypoint Affordance (CMKA) 方法，并结合大型视觉模型进行弱监督和精细的作用特征提取，实现泛化同时避免手动的关键点注释。另外提出了一种基于关键点的手部矩阵变换(KGT)方法，确保手部关键点与物体接触点之间的空间一致性，从而为视觉感知和灵巧抓握动作之间建立了直接连接。在公共真实世界FAH数据集、IsaacGym仿真及具有挑战性的机器人任务上的实验表明，我们的方法显著提高了作用定位精度、抓取一致性和对未知工具和任务的泛化能力，弥合了视觉作用学习与灵巧机器人操作之间的差距。源代码和演示视频将在https://github.com/PopeyePxx/MKA公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional dexterous grasping requires precise hand-object interaction, goingbeyond simple gripping. Existing affordance-based methods primarily predictcoarse interaction regions and cannot directly constrain the grasping posture,leading to a disconnection between visual perception and manipulation. Toaddress this issue, we propose a multi-keypoint affordance representation forfunctional dexterous grasping, which directly encodes task-driven graspconfigurations by localizing functional contact points. Our method introducesContact-guided Multi-Keypoint Affordance (CMKA), leveraging human graspingexperience images for weak supervision combined with Large Vision Models forfine affordance feature extraction, achieving generalization while avoidingmanual keypoint annotations. Additionally, we present a Keypoint-based Graspmatrix Transformation (KGT) method, ensuring spatial consistency between handkeypoints and object contact points, thus providing a direct link betweenvisual perception and dexterous grasping actions. Experiments on publicreal-world FAH datasets, IsaacGym simulation, and challenging robotic tasksdemonstrate that our method significantly improves affordance localizationaccuracy, grasp consistency, and generalization to unseen tools and tasks,bridging the gap between visual affordance learning and dexterous roboticmanipulation. The source code and demo videos will be publicly available athttps://github.com/PopeyePxx/MKA.</description>
      <author>example@mail.com (Fan Yang, Dongsheng Luo, Wenrui Chen, Jiacheng Lin, Junjie Cai, Kailun Yang, Zhiyong Li, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.20018v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Object Handover in a Robot Crafting Assistant</title>
      <link>http://arxiv.org/abs/2502.19991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个通过人类遥操作数据训练的合作交接模型，旨在提高机器人与人合作时的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;随着机器人的普及，它们越来越多地参与到需要与人类互动的工作中，例如在餐厅里递送食物或在装配线上帮助工人。这些场景通常涉及物品的交接过程。&lt;h4&gt;目的&lt;/h4&gt;为了实现安全和高效的协作机器人系统（HRC），有必要将人类行为上下文融入到机器人的合作策略当中。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一种基于自然手工任务中的遥操作数据训练的合作交接模型，并通过交叉验证实验以及用户研究来评估该模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;自主政策成功实现了协作性交接，但在与人类遥操作的比较中揭示了进一步改进的空间。&lt;h4&gt;结论&lt;/h4&gt;虽然研究表明该合作交接策略能够有效实现机器人和人之间的安全高效的协作，但仍存在改善潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are increasingly working alongside people, delivering food to patronsin restaurants or helping workers on assembly lines. These scenarios ofteninvolve object handovers between the person and the robot. To achieve safe andefficient human-robot collaboration (HRC), it is important to incorporate humancontext in a robot's handover strategies. Therefore, in this work, we develop acollaborative handover model trained on human teleoperation data collected in anaturalistic crafting task. To evaluate the performance of this model, weconduct cross-validation experiments on the training dataset as well as a userstudy in the same HRC crafting task. The handover episodes and user perceptionsof the autonomous handover policy were compared with those of the humanteleoperated handovers. While the cross-validation experiment and user studyindicate that the autonomous policy successfully achieved collaborativehandovers, the comparison with human teleoperation revealed avenues for furtherimprovements.</description>
      <author>example@mail.com (Leimin Tian, Shiyu Xu, Kerry He, Rachel Love, Akansel Cosgun, Dana Kulic)</author>
      <guid isPermaLink="false">2502.19991v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CarPlanner是一种基于强化学习（RL）的轨迹规划器，旨在解决自动驾驶中的训练效率和性能提升问题。&lt;h4&gt;背景&lt;/h4&gt;当前，虽然一些基于机器学习的方法在特定场景中表现出色，但它们难以应对大规模、复杂的真实驾驶环境挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态轨迹生成方法——CarPlanner，该方法结合了自回归结构和一致性机制以提高训练效率并增强性能稳定性。&lt;h4&gt;方法&lt;/h4&gt;1. CarPlanner采用了自回归的强化学习框架。2. 通过一致性的引入来维护时间序列的一致性，从而稳定策略的学习过程。3. 利用专家指导奖励函数和不变视图模块简化RL训练流程。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在nuPlan大规模真实世界数据集上超越了现有基于规则、强化学习和模仿学习的方法，展示了其优越的性能。&lt;h4&gt;结论&lt;/h4&gt;CarPlanner作为一个潜在解决方案，在自动驾驶轨迹规划中显示出巨大潜力。它有效地解决了大规模真实场景下的训练效率问题，并且能够在挑战性任务中超越当前最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;路径规划是自主驾驶的关键组成部分，用于确保复杂环境中的安全高效导航。尽管最近基于学习的方法——特别是强化学习（RL）在特定情况下取得了显著成果，但它们仍然难以克服大规模现实世界场景下的训练效率问题。为此，我们引入了CarPlanner，这是一种自回归式轨迹生成器，它利用RL来生成多模态路径。此方法通过维护时间序列的一致性确保策略学习的稳定性，并且采用了指导式的奖励函数和不变视图模块来简化RL训练过程并提升性能表现。实验分析表明，该框架有效地解决了训练效率低下及性能不足的问题，是自动驾驶轨迹规划中的一个有前景的方法。根据我们的知识，在nuPlan这一大型现实世界数据集中，我们首次展示了基于RL的路径规划器可以超越基于规则和模仿学习的最佳方法（SOTAs）的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory planning is vital for autonomous driving, ensuring safe andefficient navigation in complex environments. While recent learning-basedmethods, particularly reinforcement learning (RL), have shown promise inspecific scenarios, RL planners struggle with training inefficiencies andmanaging large-scale, real-world driving scenarios. In this paper, we introduce\textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive\textbf{Planner} that uses RL to generate multi-modal trajectories. Theauto-regressive structure enables efficient large-scale RL training, while theincorporation of consistency ensures stable policy learning by maintainingcoherent temporal consistency across time steps. Moreover, CarPlanner employs ageneration-selection framework with an expert-guided reward function and aninvariant-view module, simplifying RL training and enhancing policyperformance. Extensive analysis demonstrates that our proposed RL frameworkeffectively addresses the challenges of training efficiency and performanceenhancement, positioning CarPlanner as a promising solution for trajectoryplanning in autonomous driving. To the best of our knowledge, we are the firstto demonstrate that the RL-based planner can surpass both IL- and rule-basedstate-of-the-arts (SOTAs) on the challenging large-scale real-world datasetnuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTAapproaches within this demanding dataset.</description>
      <author>example@mail.com (Dongkun Zhang, Jiaming Liang, Ke Guo, Sha Lu, Qi Wang, Rong Xiong, Zhenwei Miao, Yue Wang)</author>
      <guid isPermaLink="false">2502.19908v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Shared Autonomy for Proximal Teaching</title>
      <link>http://arxiv.org/abs/2502.19899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM/IEEE International Conference on Human-Robot  Interaction, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用共享自主性的方法Z-COACH，旨在为学习复杂技能（如高性能赛车）的学生提供个性化的指导。&lt;h4&gt;背景&lt;/h4&gt;在进行运动技能的学习时，通常需要经验丰富的专业人士来进行个性化教学。然而，在特定任务领域中高质量的培训资源可能有限，尤其是在像高性能赛车这样专业化程度较高的领域。&lt;h4&gt;目的&lt;/h4&gt;旨在通过教育心理学中的支架理论来设计一种方法，利用共享自主性框架结合用户的输入与机器人的自主性，以优化教学策略。&lt;h4&gt;方法&lt;/h4&gt;提出了Z-COACH方法，该方法使用共享自主性的原则来提供个性化的指导，并重点训练学生易于理解和学习的任务子技能。&lt;h4&gt;主要发现&lt;/h4&gt;在一项有50名参与者的研究中，在模拟的Thunderhill Raceway Park环境中通过CARLA自动驾驶模拟器进行高性能赛车教学时，Z-COACH帮助识别了每位学生的最优先练习技能，从而提高了驾驶时间、行为和流畅度的表现。&lt;h4&gt;结论&lt;/h4&gt;本研究证明了可用的半自主能力（如车辆或机器人）不仅可以辅助人类用户，还能有效教导他们学习复杂的任务子技能。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了利用共享自主性的Z-COACH方法，通过模拟环境测试，在高性能赛车领域展示了该方法在个性化教学方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor skill learning often requires experienced professionals who can providepersonalized instruction. Unfortunately, the availability of high-qualitytraining can be limited for specialized tasks, such as high performance racing.Several recent works have leveraged AI-assistance to improve instruction oftasks ranging from rehabilitation to surgical robot tele-operation. However,these works often make simplifying assumptions on the student learning process,and fail to model how a teacher's assistance interacts with differentindividuals' abilities when determining optimal teaching strategies. Inspiredby the idea of scaffolding from educational psychology, we leverage sharedautonomy, a framework for combining user inputs with robot autonomy, to aidwith curriculum design. Our key insight is that the way a student's behaviorimproves in the presence of assistance from an autonomous agent can highlightwhich sub-skills might be most ``learnable'' for the student, or within theirZone of Proximal Development. We use this to design Z-COACH, a method for usingshared autonomy to provide personalized instruction targeting interpretabletask sub-skills. In a user study (n=50), where we teach high performance racingin a simulated environment of the Thunderhill Raceway Park with the CARLAAutonomous Driving simulator, we show that Z-COACH helps identify which skillseach student should first practice, leading to an overall improvement indriving time, behavior, and smoothness. Our work shows that increasinglyavailable semi-autonomous capabilities (e.g. in vehicles, robots) can not onlyassist human users, but also help *teach* them.</description>
      <author>example@mail.com (Megha Srivastava, Reihaneh Iranmanesh, Yuchen Cui, Deepak Gopinath, Emily Sumner, Andrew Silva, Laporsha Dees, Guy Rosman, Dorsa Sadigh)</author>
      <guid isPermaLink="false">2502.19899v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments</title>
      <link>http://arxiv.org/abs/2502.19892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;该研究提出了一种名为ColorDynamic的框架，用于解决机器人在非结构化和动态环境中的局部规划问题。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习（DRL）展示了处理机器人局部规划问题的潜力，但在高度非结构化的、动态环境中其效果受到限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度强化学习方法来提高机器人在复杂环境下的决策能力和实时性能。&lt;h4&gt;方法&lt;/h4&gt;{'框架设计': '提出了一种端到端的DRL形式化方法，直接将原始传感器数据映射为控制命令，使该方法适应于非结构化的环境。同时引入Transqer网络，它支持从时间过渡中进行在线DRL学习，增强动态场景中的决策能力。', '平台开发': '为了便于多样化数据集的可扩展训练，设计了一种高效的模拟平台E-Sparrow，并结合对称不变性技术来增加数据量。', '实验验证': '通过与最先进的方法比较评估、通用性、可伸缩性和实时性能测试来证明ColorDynamic的有效性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'成功率': '该方法在实验中取得了超过90%的成功率。', '延迟时间': '展示了实现实时能力（每次规划1.2-1.3毫秒）的能力。', '组件贡献': '通过消融研究证明了各组成部分对整体性能的贡献。'}&lt;h4&gt;结论&lt;/h4&gt;基于ColorDynamic，开发了一种名为OkayPlan-ColorDynamic (OPCD)的导航系统，并通过模拟和真实世界的实验展示了其在复杂环境中的优越性和适用性。&lt;h4&gt;代码与数据公开&lt;/h4&gt;研究的源码及实验演示已在其官方网站上开源，以促进可重复研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Reinforcement Learning (DRL) has demonstrated potential in addressingrobotic local planning problems, yet its efficacy remains constrained in highlyunstructured and dynamic environments. To address these challenges, this studyproposes the ColorDynamic framework. First, an end-to-end DRL formulation isestablished, which maps raw sensor data directly to control commands, therebyensuring compatibility with unstructured environments. Under this formulation,a novel network, Transqer, is introduced. The Transqer enables online DRLlearning from temporal transitions, substantially enhancing decision-making indynamic scenarios. To facilitate scalable training of Transqer with diversedata, an efficient simulation platform E-Sparrow, along with a dataaugmentation technique leveraging symmetric invariance, are developed.Comparative evaluations against state-of-the-art methods, alongside assessmentsof generalizability, scalability, and real-time performance, were conducted tovalidate the effectiveness of ColorDynamic. Results indicate that our approachachieves a success rate exceeding 90% while exhibiting real-time capacity(1.2-1.3 ms per planning). Additionally, ablation studies were performed tocorroborate the contributions of individual components. Building on this, theOkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated andreal-world experiments demonstrating its superiority and applicability incomplex scenarios. The codebase and experimental demonstrations have beenopen-sourced on our website to facilitate reproducibility and further research.</description>
      <author>example@mail.com (Jinghao Xin, Zhichao Liang, Zihuan Zhang, Peng Wang, Ning Li)</author>
      <guid isPermaLink="false">2502.19892v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Image Translation-Based Unsupervised Cross-Modality Domain Adaptation for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2502.15193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure. arXiv admin note: substantial text overlap with  arXiv:2303.07674&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图像转换的无监督跨模态域适应方法，该方法能够将带有标注的源模态图像转化为未标注的目标模态，并利用这些伪标签进行目标模态的学习。&lt;h4&gt;背景&lt;/h4&gt;在医学影像中，由于医生专业知识的需求，注解过程更加耗时且昂贵。同时，不同医疗机构获取的医疗影像可能因为不同的扫描设备和成像协议而具有不一致的模态特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种可以应对跨模态差异（域偏移）问题并提高深度学习模型性能的方法。&lt;h4&gt;方法&lt;/h4&gt;通过图像转换技术将源模态中的标注数据转化为目标模态，并结合自训练技术克服生成伪标签与真实图像之间的细微差别，以进一步提升任务的执行能力。&lt;h4&gt;主要发现&lt;/h4&gt;在跨模态域适应（crossMoDA 2022）挑战赛验证阶段排行榜上，对于前庭神经鞘瘤和耳蜗分割任务，提出的模型分别达到了Dice相似性系数(DSC)和平均对称表面距离(ASSD)为：VS肿瘤0.8351 ± 0.1152 和1.6712 ± 2.1948；耳蜗0.8098 ± 0.0233和0.2317 ± 0.1577。&lt;h4&gt;结论&lt;/h4&gt;所提出的无监督跨模态域适应方法能有效地解决医疗影像中的跨模态问题，提供了一种提高深度学习模型在医学图像处理中表现的新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning usually faces more challenges in medical images thanin natural images. Since annotations in medical images require the expertise ofdoctors and are more time-consuming and expensive. Thus, some researchers turnto unsupervised learning methods, which usually face inevitable performancedrops. In addition, medical images may have been acquired at different medicalcenters with different scanners and under different image acquisitionprotocols, so the modalities of the medical images are often inconsistent. Thismodality difference (domain shift) also reduces the applicability of deeplearning methods. In this regard, we propose an unsupervised crossmodalitydomain adaptation method based on image translation by transforming the sourcemodality image with annotation into the unannotated target modality and usingits annotation to achieve supervised learning of the target modality. Inaddition, the subtle differences between translated pseudo images and realimages are overcome by self-training methods to further improve the taskperformance of deep learning. The proposed method showed mean Dice SimilarityCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentationtask of the Cross-Modality Domain Adaptation (crossMoDA 2022) challengevalidation phase leaderboard.</description>
      <author>example@mail.com (Tao Yang, Lisheng Wang)</author>
      <guid isPermaLink="false">2502.15193v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
  <item>
      <title>Fréchet Cumulative Covariance Net for Deep Nonlinear Sufficient Dimension Reduction with Random Objects</title>
      <link>http://arxiv.org/abs/2502.15374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的统计依赖度量——Fréchet累积协方差（FCCov），并基于此发展了一种新的非线性充分降维框架，适用于复杂的非欧几里得数据，并具有抗异常值的能力。&lt;h4&gt;背景&lt;/h4&gt;现有大多数方法在处理复杂非欧几里得响应变量时不再适用，而这类数据在许多现代统计应用中频繁出现。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的非线性充分降维框架，以解决复杂非欧几里得数据的问题，并提高模型的鲁棒性和实用性。&lt;h4&gt;方法&lt;/h4&gt;引入了Fréchet累积协方差（FCCov）作为依赖度量，并结合前馈神经网络（FNNs）和卷积神经网络（CNNs）来估计样本层面的非线性充分方向。同时，证明了带有平方弗罗贝尼乌斯范数正则化的模型在σ-域上的无偏性。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明该方法达到了最优的收敛速度，并通过大量的模拟研究验证了其在欧几里得和非欧几里得设置下的性能。实际应用中，该方法在面部表情识别数据集上表现良好。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅对复杂的非欧几里得数据具有广泛的应用性，而且展示出了比现有方法更强的鲁棒性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;非线性充分降维构建了高维数据的非线性低维表示，以概括其核心特征。然而，当响应变量是常见的复杂非欧几里得随机对象时，大多数现有的方法不再适用。本文引入了一种新的统计依赖度量——Fréchet累积协方差（FCCov），并基于此发展了一个新的非线性充分降维框架，并结合了前馈神经网络和卷积神经网络来估计样本级别的非线性充分方向。理论证明表明，带有平方弗罗贝尼乌斯范数正则化的模型在σ-域上是无偏的。此外，建立了基于FNNs和ResNet型CNNs的估计器的非渐近收敛率，这些匹配了非参数回归的最大最小速率（忽略对数因子）。大量的模拟研究验证了所提方法在欧几里得及非欧几里得设置下的性能，并通过面部表情识别数据集的应用证明了其实际有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nonlinear sufficient dimension reduction\citep{libing_generalSDR}, whichconstructs nonlinear low-dimensional representations to summarize essentialfeatures of high-dimensional data, is an important branch of representationlearning. However, most existing methods are not applicable when the responsevariables are complex non-Euclidean random objects, which are frequentlyencountered in many recent statistical applications. In this paper, weintroduce a new statistical dependence measure termed Fr\'echet CumulativeCovariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov.Our approach is not only applicable to complex non-Euclidean data, but alsoexhibits robustness against outliers. We further incorporate Feedforward NeuralNetworks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinearsufficient directions in the sample level. Theoretically, we prove that ourmethod with squared Frobenius norm regularization achieves unbiasedness at the$\sigma$-field level. Furthermore, we establish non-asymptotic convergencerates for our estimators based on FNNs and ResNet-type CNNs, which match theminimax rate of nonparametric regression up to logarithmic factors. Intensivesimulation studies verify the performance of our methods in both Euclidean andnon-Euclidean settings. We apply our method to facial expression recognitiondatasets and the results underscore more realistic and broader applicability ofour proposal.</description>
      <author>example@mail.com (Hang Yuan, Christina Dan Wang, Zhou Yu)</author>
      <guid isPermaLink="false">2502.15374v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision (IJCV). This  work is a journal extension of our ICCV 2023 paper arXiv:2307.08492. arXiv  admin note: text overlap with arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PointSea，一种用于全局到局部点云完成的方法。通过引入自结构增强和利用多视角自我投影深度图来改进数据表示。&lt;h4&gt;背景&lt;/h4&gt;点云补全是3D视觉中的基础但尚未完全解决的问题。当前方法依赖于3D坐标信息或额外的数据（如图像和扫描视点）来填充缺失部分。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于自结构增强的全局到局部点云完成的新方法，以更好地理解和生成不完整输入中的细节。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': '使用多视角自我投影深度图增强数据表示，并通过跨模态输入重构紧凑的全球形状。引入特征融合模块，在视内和视间层次上融合特征。', '局部阶段': '提出一种名为自结构对偶生成器的点生成器，该生成器结合了学习到的形状先验和几何自相似性进行形状细化，并根据每个点的结构性质适应不同的细化策略。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明PointSea能够有效理解全局形状并从不完整输入中产生局部细节，相对于现有方法有明显改进。&lt;h4&gt;结论&lt;/h4&gt;通过引入自我结构增强和利用多视角数据表示来提升点云补全的效果，并在多个基准测试上展示了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Armada: Memory-Efficient Distributed Training of Large-Scale Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了在亿级规模图数据集上进行分布式训练的Graph Neural Networks（GNNs）的方法，提出了一个新的分布式系统Armada和一种新的最小边切割划分算法GREM。&lt;h4&gt;背景&lt;/h4&gt;现有的最优离线方法（例如METIS）虽然效果好但是对内存消耗巨大并且运行时间长；而计算效率较高的贪心流式分区方法在减少跨机通信方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够高效处理大规模图数据集的分布式训练系统，以优化GNN的训练过程。&lt;h4&gt;方法&lt;/h4&gt;引入了Armada系统及其核心组件GREM算法。GREM基于改进的流式贪心算法，在执行过程中不断优化顶点分配策略而非一次性冻结选择结果。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析和实验表明，相比于传统方法（例如METIS），GREM能够在内存消耗和运行时间上减少8到65倍，并且在切割边数上达到与之相仿的水平。另外，在进行分布式训练时，Armada通过分散式架构进一步提高了效率。&lt;h4&gt;结论&lt;/h4&gt;使用分散式架构可以显著提高GNN模型在大规模图数据集上的训练性能和成本效益。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在分布在多台机器上划分的大规模图形（即百亿级）中进行Graph Neural Networks (GNNs)分布式训练的有效方法。高效训练需要利用最小边切割分区算法来减少由于GNN邻居采样导致的跨机通信需求，但是对大图进行有效分割仍然是一个挑战：最先进的离线方法(例如METIS)，虽然效果好但它们需要比GNN训练本身多几倍到几十倍的内存和运行时间；计算效率较高的贪心流式分区算法则面临增加边切割量的问题。为此，在这项工作中我们引入了Armada，一个新的用于分布式GNN训练的端到端系统，其关键贡献是GREM，一种新的最小边切割划分算法，它能够有效处理大规模图形数据集。GREM在现有的流式贪心算法基础上加入了一项重要改进：在执行过程中对先前顶点分配进行持续优化而非冻结初始贪婪选择的结果。我们的理论分析和实验结果表明这种优化对于减少边切割至关重要，并使GREM能够在内存消耗和运行时间上少8到65倍的同时，达到与METIS相近的分区质量。给定一个已分割图，Armada通过新的分散架构进一步提升了分布式GNN训练效率；我们在普通的云机器中发现，在没有额外通信的情况下，GNN邻居采样以及特征加载已成为训练中的瓶颈。分散式架构使得Armada能够独立分配这些操作所需的资源，并确保昂贵的GPU始终保持饱和运算状态。我们评估了Armada相对于当前最优秀的分布式GNN训练系统的表现，发现分散式的架构带来了运行时间提高高达4.5倍和成本降低高达3.1倍的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study distributed training of Graph Neural Networks (GNNs) onbillion-scale graphs that are partitioned across machines. Efficient trainingin this setting relies on min-edge-cut partitioning algorithms, which minimizecross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cutpartitioning over large graphs remains a challenge: State-of-the-art (SoTA)offline methods (e.g., METIS) are effective, but they require orders ofmagnitude more memory and runtime than GNN training itself, whilecomputationally efficient algorithms (e.g., streaming greedy approaches) sufferfrom increased edge cuts. Thus, in this work we introduce Armada, a newend-to-end system for distributed GNN training whose key contribution is GREM,a novel min-edge-cut partitioning algorithm that can efficiently scale to largegraphs. GREM builds on streaming greedy approaches with one key addition: priorvertex assignments are continuously refined during streaming, rather thanfrozen after an initial greedy selection. Our theoretical analysis andexperimental results show that this refinement is critical to minimizing edgecuts and enables GREM to reach partition quality comparable to METIS but with8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leveragesa new disaggregated architecture for distributed GNN training to furtherimprove efficiency; we find that on common cloud machines, even with zerocommunication, GNN neighborhood sampling and feature loading bottlenecktraining. Disaggregation allows Armada to independently allocate resources forthese operations and ensure that expensive GPUs remain saturated withcomputation. We evaluate Armada against SoTA systems for distributed GNNtraining and find that the disaggregated architecture leads to runtimeimprovements up to 4.5x and cost reductions up to 3.1x.</description>
      <author>example@mail.com (Roger Waleffe, Devesh Sarda, Jason Mohoney, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Theodoros Rekatsinas, Shivaram Venkataraman)</author>
      <guid isPermaLink="false">2502.17846v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand</title>
      <link>http://arxiv.org/abs/2502.18423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种灵巧的臂手系统，用于在多物体堆叠环境中高效地检索被遮挡的目标物体。该方法通过大规模并行强化学习训练策略，在复杂环境设计中展现出了高效的清除障碍物能力。&lt;h4&gt;背景&lt;/h4&gt;在多个物体堆积的情况下检索目标物体既具有挑战性又耗时，现有的方法通常通过逐一抓取和移除遮挡的物体来解决问题，这导致了执行时间长并且需要极高的抓取技能要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的灵巧机械臂系统，能够高效地在复杂堆叠环境中清除障碍物以检索目标物体。&lt;h4&gt;方法&lt;/h4&gt;采用大规模并行强化学习技术，在多样化设计的拥挤场景中训练策略。这些策略发展出如推、搅拌和戳等技能，可以有效地暴露目标物体的足够表面。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了该系统在多样化的居家物品混乱配置下的高效性能，并成功地将学到的策略转移到真实的灵巧多指机器人上，展示了其实际应用的可能性。&lt;h4&gt;结论&lt;/h4&gt;研究证明所提出的臂手系统可以有效解决复杂堆叠环境中目标物体检索的问题，并且能够在现实世界中实现。&lt;h4&gt;翻译&lt;/h4&gt;提取埋藏在多个物体下的对象不仅具有挑战性而且耗时。在这种环境下执行操作会因为复杂的接触关系而困难重重。现有方法通常通过逐一抓取并移除每个遮挡物来解决这个问题，这导致了长时间的执行时间和对每一个遮挡物不切实际的抓取能力需求。在本文中，我们提出了一种灵巧的臂手系统用于多物体堆叠环境中的高效对象检索。我们的方法利用大规模平行强化学习在多样化设计的混乱环境中训练策略。这些策略展现了如推、搅拌和戳等出现的操作技能，能够有效地清除遮挡物以暴露目标物体的足够表面区域。我们在一套超过10种家用物品在不同杂乱配置下进行了广泛的评估，展示了对已训练和未见过对象都具备优异的检索性能和效率。此外，我们将学习到的策略成功地转移到了真实世界中的灵巧多指机器人系统中，验证了它们的实际应用性。视频可以在我们的项目网站上找到：https://ChangWinde.github.io/RetrDex。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving objects buried beneath multiple objects is not only challengingbut also time-consuming. Performing manipulation in such environments presentssignificant difficulty due to complex contact relationships. Existing methodstypically address this task by sequentially grasping and removing eachoccluding object, resulting in lengthy execution times and requiringimpractical grasping capabilities for every occluding object. In this paper, wepresent a dexterous arm-hand system for efficient object retrieval inmulti-object stacked environments. Our approach leverages large-scale parallelreinforcement learning within diverse and carefully designed clutteredenvironments to train policies. These policies demonstrate emergentmanipulation skills (e.g., pushing, stirring, and poking) that efficientlyclear occluding objects to expose sufficient surface area of the target object.We conduct extensive evaluations across a set of over 10 household objects indiverse clutter configurations, demonstrating superior retrieval performanceand efficiency for both trained and unseen objects. Furthermore, wesuccessfully transfer the learned policies to a real-world dexterousmulti-fingered robot system, validating their practical applicability inreal-world scenarios. Videos can be found on our project websitehttps://ChangWinde.github.io/RetrDex.</description>
      <author>example@mail.com (Fengshuo Bai, Yu Li, Jie Chu, Tawei Chou, Runchuan Zhu, Ying Wen, Yaodong Yang, Yuanpei Chen)</author>
      <guid isPermaLink="false">2502.18423v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>From planning to policy: distilling $\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation</title>
      <link>http://arxiv.org/abs/2502.18015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website:  $\href{https://sites.google.com/view/skill-rrt}{\text{sites.google.com/view/skill-rrt}}$&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种框架，通过模仿学习将规划算法转化为策略以解决长时序操作任务中的复杂技能串联问题。&lt;h4&gt;背景&lt;/h4&gt;当前机器人在需要一系列灵巧和非灵巧抓取技巧的长时间序列操作任务中面临挑战。这包括处理复杂的接触互动和考虑多个技能长期影响的任务串连。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，将长时序规划算法转化为高效的操作策略，并通过生成高质量演示数据来优化策略性能。&lt;h4&gt;方法&lt;/h4&gt;{'Skill-RRT': '该框架引入了$\texttt{Skill-RRT}$，这是一种快速搜索随机树（RRT）的扩展版本，加入了技能适用性检查和中间对象姿态采样，以实现高效的长时序规划。', 'connectors': '提出$\\it{connectors}$概念，即基于目标条件策略，用于在技能之间过渡的同时尽量减少物体扰动。通过懒惰规划（lazy planning），这些connectors仅在相关转移上训练，从而降低成本。', '噪声重播机制': '利用$\texttt{Skill-RRT}$生成高质量的演示数据，并通过噪声基础重播机制进一步优化策略性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'策略性能': '所提出的策略能够在完全模拟环境中进行训练，并直接应用于现实世界，成功率超过80%，涵盖三个具有挑战性的操作任务。', '比较优势': '在仿真环境中的表现优于现有的基于技能的强化学习方法$\texttt{MAPLE}$和$\texttt{Skill-RRT}$。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入$exttt{Skill-RRT}$以及$\it{connectors}$，本文提供了一种有效的方法来解决机器人长时序操作任务中的复杂技能问题。&lt;h4&gt;翻译&lt;/h4&gt;当前的机器人在执行需要一系列灵巧和非灵巧抓取技巧的操作任务中遇到挑战，这些任务涉及处理复杂的接触互动及考虑多个技能长期影响的任务串连。为了应对这些问题，该研究提出了一种框架，通过模仿学习将一个能够解决长时间序列问题但计算时间消耗大的规划算法转化为策略，从而实现高效的动作推断。本文介绍了$exttt{Skill-RRT}$方法，这是快速搜索随机树（RRT）的扩展版，加入技能适用性检查和中间对象姿态采样，以促进高效的长时序规划。此外，为了使技能能够串联起来，研究还提出了$\it{connectors}$概念——目标条件策略，用于在技能之间转换的同时尽量减少物体扰动。通过懒惰规划技术，这些连接器仅选择性地训练于相关的过渡场景上，减少了训练成本。高质量的演示数据由$exttt{Skill-RRT}$生成，并且经过噪声基础重播机制优化以确保政策性能的鲁棒性。最终策略在完全模拟环境中进行训练后，在真实世界中直接转移应用并取得超过80%的成功率，涵盖了三个具有挑战性的操作任务。在仿真环境中的表现也优于现有的基于技能的强化学习方法$exttt{MAPLE}$和$exttt{Skill-RRT}$。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current robots face challenges in manipulation tasks that require a longsequence of prehensile and non-prehensile skills. This involves handlingcontact-rich interactions and chaining multiple skills while considering theirlong-term consequences. This paper presents a framework that leveragesimitation learning to distill a planning algorithm, capable of solvinglong-horizon problems but requiring extensive computation time, into a policyfor efficient action inference. We introduce $\texttt{Skill-RRT}$, an extensionof the rapidly-exploring random tree (RRT) that incorporates skillapplicability checks and intermediate object pose sampling for efficientlong-horizon planning. To enable skill chaining, we propose$\textit{connectors}$, goal-conditioned policies that transition between skillswhile minimizing object disturbance. Using lazy planning, connectors areselectively trained on relevant transitions, reducing the cost of training.High-quality demonstrations are generated with $\texttt{Skill-RRT}$ and refinedby a noise-based replay mechanism to ensure robust policy performance. Thedistilled policy, trained entirely in simulation, zero-shot transfer to thereal world, and achieves over 80% success rates across three challengingmanipulation tasks. In simulation, our approach outperforms thestate-of-the-art skill-based reinforcement learning method, $\texttt{MAPLE}$,and $\texttt{Skill-RRT}$.</description>
      <author>example@mail.com (Haewon Jung, Donguk Lee, Haecheol Park, JunHyeop Kim, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.18015v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Transfer Learning for Classification of Cassava Disease</title>
      <link>http://arxiv.org/abs/2502.19351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, in Portuguese language, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文对比了四种卷积神经网络架构（EfficientNet-B3、InceptionV3、ResNet50 和 VGG16）在分类木薯病害图像上的性能。&lt;h4&gt;背景&lt;/h4&gt;研究使用的数据集来自一场竞赛中的不平衡图像数据集。&lt;h4&gt;目的&lt;/h4&gt;比较不同CNN架构对于识别木薯疾病图像的效果，特别是针对不平衡数据集的挑战提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用了适当的度量标准来解决类别不平衡的问题，并评估了四种模型在分类任务上的性能指标（准确率、精确率、召回率和F1分数）。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientNet-B3 在这项任务中表现出最好的结果，其准确率为87.7%，精确率为87.8%，召回率为87.8%，F1-Score为87.7%。&lt;h4&gt;结论&lt;/h4&gt;研究认为EfficientNet-B3可以作为一个有价值的工具来支持数字农业的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了该论文通过评估四种卷积神经网络架构（包括EfficientNet-B3、InceptionV3、ResNet50和VGG16）在处理一个不平衡的木薯病害图像数据集上的分类性能，发现EfficientNet-B3表现最佳，并建议这种模型可以用于支持数字农业领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a performance comparison among four Convolutional NeuralNetwork architectures (EfficientNet-B3, InceptionV3, ResNet50, and VGG16) forclassifying cassava disease images. The images were sourced from an imbalanceddataset from a competition. Appropriate metrics were employed to address classimbalance. The results indicate that EfficientNet-B3 achieved on this taskaccuracy of 87.7%, precision of 87.8%, revocation of 87.8% and F1-Score of87.7%. These findings suggest that EfficientNet-B3 could be a valuable tool tosupport Digital Agriculture.</description>
      <author>example@mail.com (Ademir G. Costa Junior, Fábio S. da Silva, Ricardo Rios)</author>
      <guid isPermaLink="false">2502.19351v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR Registration with Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2502.19374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种使用DINOv2特征作为点描述符的方法，用于解决基于激光雷达的机器人地图注册和定位中的关键问题。&lt;h4&gt;背景&lt;/h4&gt;激光雷达数据配准在机器人制图与定位中是一项基本任务。通过识别稳健的点对来实现两个点云之间的对齐是至关重要的步骤，在领域差异、季节变化及点云结构变化的情况下尤为困难。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决上述问题，提出了一种使用DINOv2特征作为点描述符的方法，并结合传统配准算法进行激光雷达扫描和3D地图的稳健6DoF对齐。&lt;h4&gt;方法&lt;/h4&gt;通过将从环绕视图图像中获得的DINOv2特征用作点描述符，这种方法可以克服传统的基于手工设计及学习的方法在面对领域差异时的局限性。同时与RANSAC或ICP等传统配准算法相结合以实现稳健对齐。&lt;h4&gt;主要发现&lt;/h4&gt;利用额外的相机数据使该方法能够在NCLT和Oxford RobotCar数据集上超越最复杂的基线技术，分别提高了24.8%和17.3%的注册召回率。&lt;h4&gt;结论&lt;/h4&gt;本文的方法不需要领域特定重新训练，并且对点云结构无敏感性，能够处理稀疏激光雷达扫描和密集3D地图。此外，该方法在概念上虽然简单但效果显著优于更复杂的基线技术。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR registration is a fundamental task in robotic mapping and localization. A critical component of aligning two point clouds is identifying robust point correspondences using point descriptors. This step becomes particularly challenging in scenarios involving domain shifts, seasonal changes, and variations in point cloud structures. These factors substantially impact both handcrafted and learning-based approaches. In this paper, we address these problems by proposing to use DINOv2 features, obtained from surround-view images, as point descriptors. We demonstrate that coupling these descriptors with traditional registration algorithms, such as RANSAC or ICP, facilitates robust 6DoF alignment of LiDAR scans with 3D maps, even when the map was recorded more than a year before. Although conceptually straightforward, our method substantially outperforms more complex baseline techniques. In contrast to previous learning-based point descriptors, our method does not require domain-specific retraining and is agnostic to the point cloud structure, effectively handling both sparse LiDAR scans and dense 3D maps. We show that leveraging the additional camera data enables our method to outperform the best baseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCar datasets. We publicly release the registration benchmark and the code of our work on https://vfm-registration.cs.uni-freiburg.de.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR registration is a fundamental task in robotic mapping and localization.A critical component of aligning two point clouds is identifying robust pointcorrespondences using point descriptors. This step becomes particularlychallenging in scenarios involving domain shifts, seasonal changes, andvariations in point cloud structures. These factors substantially impact bothhandcrafted and learning-based approaches. In this paper, we address theseproblems by proposing to use DINOv2 features, obtained from surround-viewimages, as point descriptors. We demonstrate that coupling these descriptorswith traditional registration algorithms, such as RANSAC or ICP, facilitatesrobust 6DoF alignment of LiDAR scans with 3D maps, even when the map wasrecorded more than a year before. Although conceptually straightforward, ourmethod substantially outperforms more complex baseline techniques. In contrastto previous learning-based point descriptors, our method does not requiredomain-specific retraining and is agnostic to the point cloud structure,effectively handling both sparse LiDAR scans and dense 3D maps. We show thatleveraging the additional camera data enables our method to outperform the bestbaseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCardatasets. We publicly release the registration benchmark and the code of ourwork on https://vfm-registration.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Niclas Vödisch, Giovanni Cioffi, Marco Cannici, Wolfram Burgard, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2502.19374v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Preference-Based Gradient Estimation for ML-Based Approximate Combinatorial Optimization</title>
      <link>http://arxiv.org/abs/2502.19377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary work, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于数据驱动的方法，用于改进现有的非学习近似算法，以解决组合优化问题。方法是通过参数化近似算法并利用图神经网络预测最优参数值来实现。&lt;h4&gt;背景&lt;/h4&gt;组合优化问题广泛存在于医学、物流和制造业等领域中。许多应用场景要求快速找到高质量的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据驱动的方法，结合神经网络和非学习近似算法的优点，以提高组合优化问题解的质量。&lt;h4&gt;方法&lt;/h4&gt;采用图神经网络预测能产生最优解的参数值，并通过自监督的方式进行端到端训练。同时提出了基于偏好梯度估计的新方案。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在旅行商问题和最小k切割问题上的表现与最新的学习组合优化求解器具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效地利用图神经网络的信息帮助近似算法找到更好的解决方案，同时也保证了解的可行性。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到：组合优化（CO）问题出现在从医学到物流和制造业等多个领域。虽然精确解决这些问题是不必要的，但许多应用需要快速找到高质量的解。为实现这一目标，我们提出了一种数据驱动的方法来改进现有的非学习近似算法。我们将近似算法参数化，并训练图神经网络预测能够产生最佳可能解的参数值。我们的管道在自监督环境下以端到端的方式进行梯度估计训练，将近似算法视为黑盒系统。为实现这一目标，我们提出了一种新颖的基于偏好的梯度估计方案。该方法结合了神经网络和非学习近似算法的优势：图神经网络利用数据集中的信息帮助近似算法找到更好的解，而近似算法则确保了解的可行性。我们在旅行商问题和最小k切割问题上验证了我们的方法，并表明与最新的学习组合优化求解器相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Combinatorial optimization (CO) problems arise in a wide range of fields frommedicine to logistics and manufacturing. While exact solutions are often notnecessary, many applications require finding high-quality solutions quickly.For this purpose, we propose a data-driven approach to improve existingnon-learned approximation algorithms for CO. We parameterize the approximationalgorithm and train a graph neural network (GNN) to predict parameter valuesthat lead to the best possible solutions. Our pipeline is trained end-to-end ina self-supervised fashion using gradient estimation, treating the approximationalgorithm as a black box. We propose a novel gradient estimation scheme forthis purpose, which we call preference-based gradient estimation. Our approachcombines the benefits of the neural network and the non-learned approximationalgorithm: The GNN leverages the information from the dataset to allow theapproximation algorithm to find better solutions, while the approximationalgorithm guarantees that the solution is feasible. We validate our approach ontwo well-known combinatorial optimization problems, the travelling salesmanproblem and the minimum k-cut problem, and show that our method is competitivewith state of the art learned CO solvers.</description>
      <author>example@mail.com (Arman Mielke, Uwe Bauknecht, Thilo Strauss, Mathias Niepert)</author>
      <guid isPermaLink="false">2502.19377v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users</title>
      <link>http://arxiv.org/abs/2502.19312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://fewshot-preference-optimization.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了Few-Shot Preference Optimization (FSPO) 方法，通过少量用户偏好标签，利用大型语言模型（LLM）的在上下文学习能力快速适应用户需求。&lt;h4&gt;背景&lt;/h4&gt;有效的个人化对虚拟助手和内容推荐等应用至关重要。然而，收集真实世界的用户偏好数据既困难又耗时。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以使用少量真实的用户偏好信息并在合成数据集上进行训练的方法，以实现实用的个性化功能。&lt;h4&gt;方法&lt;/h4&gt;提出FSPO框架，通过构建合成偏好数据集来解决实际偏好的收集难题，并采用公开可用的大规模语言模型生成100万以上的个人化偏好标签。&lt;h4&gt;主要发现&lt;/h4&gt;为了有效利用合成数据进行真实用户个性化的迁移学习，数据需要具有高度多样性和一致性的结构。经过电影评论、基于教育背景的适应性教学以及通用问题解答等三个领域的测试，FSPO在个性化开放生成方面表现出色，平均胜率为87%（对合成用户）和72%（对真实人类用户）。&lt;h4&gt;结论&lt;/h4&gt;FSPO成功地将大型语言模型的在上下文学习能力应用于个人化需求优化，为开发更智能、更具个性化的应用提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;有效的个性化对于各种需要与用户交互的应用程序来说至关重要。受大型语言模型（LLM）的强大上下文学习能力启发，我们提出了Few-Shot Preference Optimization (FSPO) 方法，将奖励建模重新定义为元学习问题，在此框架下，通过少量用户的偏好标签，LM可以快速适应个人需求，并构建个性化的奖励函数。考虑到实际偏好的数据难以大规模收集，我们设计了合成的偏好数据集来实现个性化，利用公开可用的大规模语言模型生成超过100万个个性化偏好标签。为了从合成数据成功转移到真实用户上，发现需要确保数据具有高度多样性和一致性的结构。我们在三个领域进行了评估：电影评论、基于教育背景的教学适应和通用问题回答，并进行了一项受控的人类研究。总的来说，在针对最多1500个虚拟用户的个性化开放生成中，FSPO实现了平均87%的Alpaca Eval胜率（对合成用户）以及在开放性问题解答中对真实人类用户的平均72%胜率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective personalization of LLMs is critical for a broad range ofuser-interfacing applications such as virtual assistants and content curation.Inspired by the strong in-context learning capabilities of LLMs, we proposeFew-Shot Preference Optimization (FSPO), which reframes reward modeling as ameta-learning problem. Under this framework, an LLM learns to quickly adapt toa user via a few labeled preferences from that user, constructing apersonalized reward function for them. Additionally, since real-worldpreference data is scarce and challenging to collect at scale, we proposecareful design choices to construct synthetic preference datasets forpersonalization, generating over 1M synthetic personalized preferences usingpublicly available LLMs. In particular, to successfully transfer from syntheticdata to real users, we find it crucial for the data to exhibit both highdiversity and coherent, self-consistent structure. We evaluate FSPO onpersonalized open-ended generation for up to 1,500 synthetic users acrossacross three domains: movie reviews, pedagogical adaptation based oneducational background, and general question answering, along with a controlledhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average ingenerating responses that are personalized to synthetic users and a 72% winratewith real human users in open-ended question answering.</description>
      <author>example@mail.com (Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn)</author>
      <guid isPermaLink="false">2502.19312v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的行人关注的多模态场景数据集PFSD，并提出了一种用于复杂半结构化环境中的三维行人检测的新方法HMFN。&lt;h4&gt;背景&lt;/h4&gt;在有大量车辆交通的结构化环境中，自动驾驶感知技术表现出色。然而，在动态行人占据主导地位的半结构化环境中，当前感知模型表现不佳，主要原因在于高质量数据集的缺乏。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态场景数据集PFSD，以及解决复杂半结构环境挑战的新方法HMFN。&lt;h4&gt;方法&lt;/h4&gt;创建了一个名为PFSD的数据集，该数据集中包括超过130,000个行人实例的详细标注。此外，还设计了一种混合多尺度融合网络（HMFN），通过结合稀疏和普通卷积来有效地捕捉并融合不同规模的特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，HMFN在PFSD数据集上的平均精度均值(mAP)有所提高，证明了该方法在处理复杂半结构化环境中的三维行人检测任务的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过提供高质量的数据集和创新的算法模型，本文为改进自动驾驶系统中行人感知技术提供了重要的基础研究。&lt;h4&gt;翻译&lt;/h4&gt;最近，在以车辆交通为主的结构性环境中，自主驾驶感知技术已经展示了卓越的能力。然而，目前的感知模型在半结构化环境中的表现较差，这些环境下动态行人的多样性和不规则运动更加普遍，遮挡情况也更为复杂。我们认为这种情况的原因在于高质量数据集的缺乏，尤其是在涉及行人感知和预测方面的不足。在这项工作中，我们提出了一个多模态的行人场景数据集（PFSD），它在半结构化环境中进行了严格的nuScenes格式标注，并提供了点云分割、检测和对象ID以进行跟踪。该数据集涵盖了各种不同密度、运动模式以及遮挡情况下的超过130,000个行人实例的数据。此外，为了展示应对更加多样且复杂的半结构环境挑战的重要性，我们提出了一种新的混合多尺度融合网络（HMFN）。具体而言，为了在人口密集和存在大量遮挡的情况下检测行人，我们的方法能够有效地捕捉并融合不同规模的特征，通过精心设计的混合框架整合了稀疏卷积与普通卷积。对PFSD进行广泛实验表明，HMFN相较于现有技术提高了平均精度均值（mAP），从而证明其在处理复杂半结构化环境中的三维行人检测问题上的有效性。代码和基准测试可供获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</title>
      <link>http://arxiv.org/abs/2502.19242v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种结合了鸟瞰图（BEV）图像表示和几何点云配准的新型激光雷达惯性里程计框架，通过引入循环闭合检测来提高定位一致性。&lt;h4&gt;背景&lt;/h4&gt;现有的激光雷达惯性里程计方法在处理高密度点云时效率低下且无法有效利用特征进行循环闭合检测。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的激光雷达惯性里程计框架BEV-LIO(LC)，旨在通过鸟瞰图表示和几何点云配准，提高定位精度并实现高效的循环闭合检测。&lt;h4&gt;方法&lt;/h4&gt;{'点密度归一化': '将激光雷达点云投影到BEV图像上以提取特征', '轻量级CNN特征提取器': '用于从BEV图像中抽取局部和全局描述符', '重投影误差最小化': '与平面到点配准结合，集成于迭代扩展卡尔曼滤波器（iEKF）内', '循环闭合检测': '使用全局描述符建立KD-tree索引的关键帧数据库，并通过RANSAC提供粗略变换估计用于ICP'}&lt;h4&gt;主要发现&lt;/h4&gt;BEV-LIO(LC)在各种场景和不同类型的LiDAR中表现出优越的性能，实现了竞争性的定位精度。&lt;h4&gt;结论&lt;/h4&gt;该方法提高了激光雷达惯性里程计系统的效率和准确性，并通过引入高效的循环闭合检测进一步提升了全局一致性。&lt;h4&gt;翻译&lt;/h4&gt;这项工作介绍了BEV-LIO(LC)，这是一种新型的激光雷达-惯性里程计框架，结合了鸟瞰图（BEV）图像表示与基于几何点云配准的方法，并且通过BEV图像特征实现了循环闭合。通过对点密度进行归一化处理，将LiDAR点云投影到BEV图像上，从而能够高效地提取和匹配特征。使用轻量级的卷积神经网络（CNN）基特征提取器来从BEV图像中抽取独特的局部与全局描述符。局部描述符用于通过FAST关键点对BEV图像进行再投影误差构造，而全局描述符则有助于循环闭合检测。随后将最小化重投影误差整合进平面到点的配准，在迭代扩展卡尔曼滤波器（iEKF）中执行。在后端部分，使用全局描述符建立一个KD-tree索引的关键帧数据库来实现准确的循环闭合检测。一旦发现循环闭合，随机样本一致性（RANSAC）将从BEV图像匹配计算出粗略变换，作为迭代最近点算法（ICP）的初始估计值。随后细化后的变换被整合进因子图并结合里程计因素提升全局定位的一致性。在不同类型的LiDAR和各种场景下进行广泛的实验表明，该方法优于现有技术，并达到了具有竞争力的定位精度。我们的代码、视频及补充材料可从https://github.com/HxCa1/BEV-LIO-LC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces BEV-LIO(LC), a novel LiDAR-Inertial Odometry (LIO)framework that combines Bird's Eye View (BEV) image representations of LiDARdata with geometry-based point cloud registration and incorporates loop closure(LC) through BEV image features. By normalizing point density, we project LiDARpoint clouds into BEV images, thereby enabling efficient feature extraction andmatching. A lightweight convolutional neural network (CNN) based featureextractor is employed to extract distinctive local and global descriptors fromthe BEV images. Local descriptors are used to match BEV images with FASTkeypoints for reprojection error construction, while global descriptorsfacilitate loop closure detection. Reprojection error minimization is thenintegrated with point-to-plane registration within an iterated Extended KalmanFilter (iEKF). In the back-end, global descriptors are used to create aKD-tree-indexed keyframe database for accurate loop closure detection. When aloop closure is detected, Random Sample Consensus (RANSAC) computes a coarsetransform from BEV image matching, which serves as the initial estimate forIterative Closest Point (ICP). The refined transform is subsequentlyincorporated into a factor graph along with odometry factors, improving theglobal consistency of localization. Extensive experiments conducted in variousscenarios with different LiDAR types demonstrate that BEV-LIO(LC) outperformsstate-of-the-art methods, achieving competitive localization accuracy. Ourcode, video and supplementary materials can be found athttps://github.com/HxCa1/BEV-LIO-LC.</description>
      <author>example@mail.com (Haoxin Cai, Shenghai Yuan, Xinyi Li, Junfeng Guo, Jianqi Liu)</author>
      <guid isPermaLink="false">2502.19242v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions</title>
      <link>http://arxiv.org/abs/2502.19293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发了一个专门用于皮肤病理领域的视觉-语言模型，该模型通过对比生成描述的方式工作，并基于42,512张HE染色的全切片图像和19,645份对应的病理报告进行训练与评估。&lt;h4&gt;背景&lt;/h4&gt;每年有数百万的黑色素细胞皮肤病变被病理学家检查，大多数是常见的痣。虽然这些病变可以在几秒钟内诊断出来，但撰写相应的病理报告却非常耗时。&lt;h4&gt;目的&lt;/h4&gt;通过自动化部分报告生成工作来减轻病理学家日益增加的工作量。&lt;h4&gt;方法&lt;/h4&gt;使用对比描述框架开发了一个视觉-语言模型，并利用一个包含42,512张HE染色的全切片图像和19,645份对应病理报告的数据集进行训练与评估。&lt;h4&gt;主要发现&lt;/h4&gt;由模型生成的常见痣的报告质量得分与专家病理学家书写的报告相当，但在稀有黑色素细胞病变亚型的情况下，报告生成较为困难，但是跨模态检索性能有了显著提升。&lt;h4&gt;结论&lt;/h4&gt;该视觉-语言模型在处理常见的黑色素细胞皮肤病变方面表现良好，并且对于罕见类型也有一定的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of melanocytic skin lesions are examined by pathologists each year,the majority of which concern common nevi (i.e., ordinary moles). While most ofthese lesions can be diagnosed in seconds, writing the corresponding pathologyreport is much more time-consuming. Automating part of the report writingcould, therefore, alleviate the increasing workload of pathologists. In thiswork, we develop a vision-language model specifically for the pathology domainof cutaneous melanocytic lesions. The model follows the Contrastive Captionerframework and was trained and evaluated using a melanocytic lesion dataset of42,512 H&amp;E-stained whole slide images and 19,645 corresponding pathologyreports. Our results show that the quality scores of model-generated reportswere on par with pathologist-written reports for common nevi, assessed by anexpert pathologist in a reader study. While report generation revealed to bemore difficult for rare melanocytic lesion subtypes, the cross-modal retrievalperformance for these cases was considerably better.</description>
      <author>example@mail.com (Ruben T. Lucassen, Sander P. J. Moonemans, Tijn van de Luijtgaarden, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19293v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Contrastive Learning for Tumor-specific Missing Modality Synthesis</title>
      <link>http://arxiv.org/abs/2502.19390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究设计了一种生成模型，用于从现有模态中合成缺失的MRI影像，特别聚焦于肿瘤区域，并在对比学习过程中通过熵选择特征来提高效果。&lt;h4&gt;背景&lt;/h4&gt;多模式磁共振成像（MRI）对于提供互补的大脑解剖和病理信息至关重要，有助于更准确地诊断。但在临床环境中获取高质量的多模态MRI面临时间、成本和技术限制等挑战。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些困难，研究者开发了一种生成模型，该模型能够利用现有的源模态数据来合成缺失的目标模态影像，特别是在肿瘤区域进行有效的对比学习和特征选择。&lt;h4&gt;方法&lt;/h4&gt;所设计的网络采用了多模式对比学习，并在对比过程中通过熵选择关键特性。此外，该网络不仅可以生成目标模态图像，还可以预测分割输出，从而提高了生成肿瘤区域的能力，有助于提高下游任务的表现。&lt;h4&gt;主要发现&lt;/h4&gt;结合了对比、分割和自我表示损失的功能模型能够有效反映特定的目标信息并生成高质量的影像。&lt;h4&gt;结论&lt;/h4&gt;在Brain MR Image Synthesis挑战赛中，所提出的模型表现出色，在合成缺失模态方面超越其他方法。&lt;h4&gt;翻译&lt;/h4&gt;多模式MRI对于提供互补的大脑解剖和病理信息至关重要。然而，在临床环境中获取高质量的多模态MRI存在困难。因此，该研究设计了一种生成模型来合成缺失的目标模态影像，并在肿瘤区域使用熵选择关键特征进行对比学习，以提高效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal magnetic resonance imaging (MRI) is essential for providingcomplementary information about brain anatomy and pathology, leading to moreaccurate diagnoses. However, obtaining high-quality multi-modal MRI in aclinical setting is difficult due to factors such as time constraints, highcosts, and patient movement artifacts. To overcome this difficulty, there isincreasing interest in developing generative models that can synthesize missingtarget modality images from the available source ones. Therefore, we design agenerative model for missing MRI that integrates multi-modal contrastivelearning with a focus on critical tumor regions. Specifically, we integratemulti-modal contrastive learning, tailored for multiple source modalities, andenhance its effectiveness by selecting features based on entropy during thecontrastive learning process. Additionally, our network not only generates themissing target modality images but also predicts segmentation outputs,simultaneously. This approach improves the generator's capability to preciselygenerate tumor regions, ultimately improving performance in downstreamsegmentation tasks. By leveraging a combination of contrastive, segmentation,and additional self-representation losses, our model effectively reflectstarget-specific information and generate high-quality target images.Consequently, our results in the Brain MR Image Synthesis challenge demonstratethat the proposed model excelled in generating the missing modality.</description>
      <author>example@mail.com (Minjoo Lim, Bogyeong Kang, Tae-Eui Kam)</author>
      <guid isPermaLink="false">2502.19390v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision (IJCV).  Extension of our ICCV 2023 work: arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个新的点云补全模型PointSea，该模型通过自我结构增强来实现从全局到局部的点云补全。&lt;h4&gt;背景&lt;/h4&gt;目前的点云补全方法依赖于3D坐标信息和其他额外数据（例如图像和扫描视角）来填充缺失的部分。这些方法存在一定的局限性。&lt;h4&gt;目的&lt;/h4&gt;探索如何仅利用原始点云自身的结构特征进行增强，从而实现更有效的全局到局部的点云补全任务。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': '通过模拟从多个角度观察一个物理对象上的缺陷区域的方式，使用自投影深度图来增强数据表示。引入特征融合模块以跨模态输入为基础重构紧凑的全局形状。', '局部阶段': '提出了称为自结构对偶生成器的点生成器，该生成器将学习到的形状先验知识和几何自相似性相结合进行形状细化，并且采用适应每个点结构类型的双路径设计来调整细化策略。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PointSea能够有效地理解全局形状并从不完整输入中生成局部细节。&lt;h4&gt;结论&lt;/h4&gt;相较于现有的方法，PointSea在处理点云补全问题时表现出明显的改进效果。&lt;h4&gt;翻译&lt;/h4&gt;点云完成是3D视觉中的一个基本但尚未完全解决的问题。当前的方法通常依赖于三维坐标信息和其他附加数据（例如图像和扫描视角）来填补缺失的部分。不同于这些方法，我们探索了自我结构增强，并提出了用于全局到局部点云完成的PointSea模型。在全局阶段，考虑到如何检查物理对象上的缺陷区域时会从多个角度进行观察以获得更好的理解，因此，受此启发，PointSea通过利用来自多视角的自投影深度图来增强数据表示。为了从跨模态输入中重构紧凑的全球形状，我们整合了一个特征融合模块，在视内和视间层次上融合了特性。在局部阶段，为揭示高度细节结构，我们引入了一种称为自结构性对偶生成器的点生成器。该生成器结合了学习到的形状先验知识和几何自相似性进行形体细化。不同于现有的采用统一策略处理所有点的方法，我们的双路径设计适应于每个点的结构类型调整精炼策略以应对特定的不完整性。在广泛使用的基准测试上进行全面实验表明，PointSea有效地理解全局形状并从不完整输入中生成局部细节，显示出对现有方法的明显改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning For Time Series Analysis With Application On Human Motion</title>
      <link>http://arxiv.org/abs/2502.19364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'时间序列数据的重要性': '时间序列数据在医学、电信和能源等领域中非常重要。', '分析任务类型': '包括分类、聚类、原型设计和回归等任务，这些任务分别用于识别异常运动、检测股市行为模式、扩展物理治疗数据集以及预测患者恢复情况。', '深度学习的应用': '近年来，在其他领域取得成功的背景下，深度学习在时间序列分析中越来越受到重视。', '研究贡献': '本论文通过特征工程提升分类性能，引入基础模型，并开发了一种紧凑且最先进的架构。同时利用自监督学习解决数据标签不足的问题。', '实际应用': '应用于人体运动分析（如动作识别和康复）以及合成样本生成方法以支持回归模型在数据稀缺的情况下进行原型设计。', '评价与展望': '评估了判别性和生成性模型的局限，并提倡建立一个稳健且标准化的评估框架。实验结果提供了新的见解和方法，推动时间序列分析的发展。'}&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要讨论了时间序列数据分析在医学、电信及能源等领域的关键作用以及分类、聚类、原型设计与回归等具体任务的应用。它还强调了深度学习技术在这类数据中的重要性，并通过特征工程和基础模型的引入，旨在提升现有方法的有效性和准确性。此外，论文提出了解决有限标注数据问题的方法，例如自监督学习，在人体运动分析（如康复与动作识别）中应用这些方法以及开发用于扩展数据集的支持回归模型的合成样本生成技术。最后，作者对当前的技术进行批判性评估，并提出了一个更稳健、标准化的研究框架以促进时间序列分析领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series data, defined by equally spaced points over time, is essential infields like medicine, telecommunications, and energy. Analyzing it involvestasks such as classification, clustering, prototyping, and regression.Classification identifies normal vs. abnormal movements in skeleton-basedmotion sequences, clustering detects stock market behavior patterns,prototyping expands physical therapy datasets, and regression predicts patientrecovery. Deep learning has recently gained traction in time series analysisdue to its success in other domains. This thesis leverages deep learning toenhance classification with feature engineering, introduce foundation models,and develop a compact yet state-of-the-art architecture. We also addresslimited labeled data with self-supervised learning. Our contributions apply toreal-world tasks, including human motion analysis for action recognition andrehabilitation. We introduce a generative model for human motion data, valuablefor cinematic production and gaming. For prototyping, we propose a shape-basedsynthetic sample generation method to support regression models when data isscarce. Lastly, we critically evaluate discriminative and generative models,identifying limitations in current methodologies and advocating for a robust,standardized evaluation framework. Our experiments on public datasets providenovel insights and methodologies, advancing time series analysis with practicalapplications.</description>
      <author>example@mail.com (Ali Ismail-Fawaz)</author>
      <guid isPermaLink="false">2502.19364v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.19247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为Proxy Transformation的方法，该方法旨在通过多模态任务有效地改进点云流形结构。&lt;h4&gt;背景&lt;/h4&gt;基于语言指令与3D环境的实时交互是实现具身智能的基础任务之一。然而，从RGB-D图像渲染出的点云包含大量冗余背景数据和内在噪声，这些干扰了目标区域的流形结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来改进点云流形结构，使其适用于实时任务。&lt;h4&gt;方法&lt;/h4&gt;首先利用可变形点聚类技术识别目标区域内的点云子流形。然后引入Proxy Attention模块，该模块使用多模态代理指导点云变换。此外还设计了基于Proxy Attention的次流形变换生成模块，在全局层面由文本信息引导不同子流形的平移向量，并通过图像信息优化每个子流形内部的线性变换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的Proxy Transformation方法在易目标和难目标上的性能分别提高了7.49%和4.60%，同时减少了注意力块的计算开销（降低40.6%）。这些成果标志着ego-centric 3D视觉定位领域的新SOTA。&lt;h4&gt;结论&lt;/h4&gt;研究展示了一种有效且稳健的方法来改进点云流形结构，进一步推动了具身智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;嵌入式智能要求代理根据语言指令在实时与三维环境中交互。该领域的基础任务是自体中心的3D视觉定位。然而，从RGB-D图像渲染出的点云保留了大量的冗余背景数据和内在噪声，这些干扰了目标区域的流形结构。现有的点云增强方法通常需要繁琐的过程来改进流形结构，这不适合实时任务。我们提出了一个适合多模态任务的Proxy Transformation方法，以高效地改进点云流形。该方法首先利用可变形点聚类技术识别目标区域内的点云子流形。接着引入了Proxy Attention模块，使用多模态代理引导点云变换。基于Proxy Attention设计了一个次流形变换生成模块，在全局层面由文本信息指导不同子流形的平移向量，并通过图像信息优化每个子流形内部的线性变换，从而精炼目标区域的局部点云流形结构。广泛的实验表明，Proxy Transformation显著优于现有方法，在易目标和难目标上分别提高了7.49%和4.60%，同时减少了注意力块的计算开销（降低40.6%）。这些结果建立了ego-centric 3D视觉定位领域的新SOTA，展示了该方法的有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence requires agents to interact with 3D environments inreal time based on language instructions. A foundational task in this domain isego-centric 3D visual grounding. However, the point clouds rendered from RGB-Dimages retain a large amount of redundant background data and inherent noise,both of which can interfere with the manifold structure of the target regions.Existing point cloud enhancement methods often require a tedious process toimprove the manifold, which is not suitable for real-time tasks. We proposeProxy Transformation suitable for multimodal task to efficiently improve thepoint cloud manifold. Our method first leverages Deformable Point Clustering toidentify the point cloud sub-manifolds in target regions. Then, we propose aProxy Attention module that utilizes multimodal proxies to guide point cloudtransformation. Built upon Proxy Attention, we design a submanifoldtransformation generation module where textual information globally guidestranslation vectors for different submanifolds, optimizing relative spatialrelationships of target regions. Simultaneously, image information guideslinear transformations within each submanifold, refining the local point cloudmanifold of target regions. Extensive experiments demonstrate that ProxyTransformation significantly outperforms all existing methods, achieving animpressive improvement of 7.49% on easy targets and 4.60% on hard targets,while reducing the computational overhead of attention blocks by 40.6%. Theseresults establish a new SOTA in ego-centric 3D visual grounding, showcasing theeffectiveness and robustness of our approach.</description>
      <author>example@mail.com (Qihang Peng, Henry Zheng, Gao Huang)</author>
      <guid isPermaLink="false">2502.19247v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>GraphBridge: Towards Arbitrary Transfer Learning in GNNs</title>
      <link>http://arxiv.org/abs/2502.19252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables, to be published in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）传统上是针对特定领域和任务进行训练的。这导致了在不同、异构的数据设置中转移所获得的知识存在重大障碍。&lt;h4&gt;目的&lt;/h4&gt;介绍GraphBridge，一个创新框架，旨在使知识能够在不同的任务和领域之间转移，避免对任务配置或图结构进行修改的需求。&lt;h4&gt;方法&lt;/h4&gt;GraphBridge允许任何预训练的GNN通过添加预测头和桥接网络来增强，该桥接网络将输入层与输出层连接起来。这种架构不仅保留了原始模型的基本知识，还支持任意维度的输出。&lt;h4&gt;主要发现&lt;/h4&gt;为了减少负向迁移问题，GraphBridge合并了源模型与一个同时训练的模型，从而减少了应用到目标域时源模型的偏差。该方法在包括图到图、节点到节点、图到节点和图到点云在内的多种转移学习场景中进行了全面评估。&lt;h4&gt;结论&lt;/h4&gt;通过16个代表这些场景的数据集进行的经验验证确认了框架对于任务无关和领域无关的知识迁移的能力，在GNN领域具有显著的进展。&lt;h4&gt;翻译&lt;/h4&gt;Graph神经网络（GNNs）传统上按照每个领域的每个任务来训练。这在将所获得的知识转移到不同的、异构的数据设置中造成了重大障碍。本文引入了GraphBridge，一个新颖框架，旨在使知识能够在不同且多样化的任务和领域之间转移，同时不需要修改任务配置或图结构。具体而言，GraphBridge允许通过添加预测头和连接输入到输出层的桥接网络来增强任何预训练的GNN，这种架构不仅保留了原始模型内在的知识，并且支持任意维度的输出。为了减轻负向迁移问题，GraphBridge将源模型与一个同时训练的模型合并，从而减少应用于目标领域时源模型的偏差。我们的方法在包括图到图、节点到节点、图到节点和图到点云在内的各种转移学习场景中得到了彻底评估，并通过代表这些场景的16个数据集进行了实证验证，证实了该框架具备处理图类数据任务无关且领域无关的知识迁移的能力，在GNNs领域实现了显著的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are conventionally trained on a per-domain,per-task basis. It creates a significant barrier in transferring the acquiredknowledge to different, heterogeneous data setups. This paper introducesGraphBridge, a novel framework to enable knowledge transfer across disparatetasks and domains in GNNs, circumventing the need for modifications to taskconfigurations or graph structures. Specifically, GraphBridge allows for theaugmentation of any pre-trained GNN with prediction heads and a bridgingnetwork that connects the input to the output layer. This architecture not onlypreserves the intrinsic knowledge of the original model but also supportsoutputs of arbitrary dimensions. To mitigate the negative transfer problem,GraphBridg merges the source model with a concurrently trained model, therebyreducing the source bias when applied to the target domain. Our method isthoroughly evaluated across diverse transfer learning scenarios, includingGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empiricalvalidation, conducted over 16 datasets representative of these scenarios,confirms the framework's capacity for task- and domain-agnostic transferlearning within graph-like data, marking a significant advancement in the fieldof GNNs.</description>
      <author>example@mail.com (Li Ju, Xingyi Yang, Qi Li, Xinchao Wang)</author>
      <guid isPermaLink="false">2502.19252v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Global Graph Propagation with Hierarchical Information Transfer for Incomplete Contrastive Multi-view Clustering</title>
      <link>http://arxiv.org/abs/2502.19291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的不完整多视角聚类方法被提出，该方法通过分层信息传递解决当前研究中存在的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界中普遍存在大量缺失的多视图数据，使得不完整的多视图聚类成为一个重要的研究课题。然而，现有的方法虽然取得了显著进展，但仍然存在一些问题：无法有效挖掘缺失数据中的隐藏信息；大多数方法将表示学习和聚类分为两个独立阶段进行。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法以解决现有方法中存在的问题，并通过实验展示该方法的有效性和优越性。&lt;h4&gt;方法&lt;/h4&gt;首先设计特定视角的图卷积网络（GCN）来获得包含图结构信息的表示，然后将其融合到共识表示中。其次考虑了一层GCN只能转移一阶邻居节点的信息，提出了全局图传播的方法以处理缺失数据和学习深层表示。最后，通过权重共享伪分类器与对比学习设计了一个端到端框架，该框架结合了特定视角的表示学习、全局图传播以及分层信息传递进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在多个常用的数据集上进行了广泛的实验，并展示了优于其他最新方法的效果和优越性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了所提出的方法的有效性和优越性，该方法不仅解决了现有问题，还提供了更好的聚类性能。相关代码可在指定的GitHub仓库中获取。&lt;h4&gt;翻译&lt;/h4&gt;不完整多视图聚类研究由于现实世界中存在的广泛缺失数据而变得重要。尽管现有的方法已经取得了很大的进步，但它们仍然存在一些问题：1）大多数方法不能有效挖掘隐藏在丢失数据中的信息；2）大多数方法通常将表示学习和聚类分为两个独立阶段进行，但这可能会影响聚类性能，因为聚类结果直接依赖于学到的表示。为了解决这些问题，我们提出了一种新的不完整多视图聚类方法，该方法使用分层信息传递。首先设计了特定视角的图卷积网络（GCN）以获取包含图结构信息的表示，并将其融合到共识表示中；其次提出了全局图传播的方法处理缺失数据并学习深层表示；最后通过权重共享伪分类器与对比学习建立了一个端到端框架，结合了特定视角的表示学习、全局图传播以及分层信息传递进行联合优化。广泛的实验显示我们的方法在多个常用的数据集上比其他最新方法更有效和优越。代码可在指定GitHub仓库中获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incomplete multi-view clustering has become one of the important researchproblems due to the extensive missing multi-view data in the real world.Although the existing methods have made great progress, there are still someproblems: 1) most methods cannot effectively mine the information hidden in themissing data; 2) most methods typically divide representation learning andclustering into two separate stages, but this may affect the clusteringperformance as the clustering results directly depend on the learnedrepresentation. To address these problems, we propose a novel incompletemulti-view clustering method with hierarchical information transfer. Firstly,we design the view-specific Graph Convolutional Networks (GCN) to obtain therepresentation encoding the graph structure, which is then fused into theconsensus representation. Secondly, considering that one layer of GCN transfersone-order neighbor node information, the global graph propagation with theconsensus representation is proposed to handle the missing data and learn deeprepresentation. Finally, we design a weight-sharing pseudo-classifier withcontrastive learning to obtain an end-to-end framework that combinesview-specific representation learning, global graph propagation withhierarchical information transfer, and contrastive clustering for jointoptimization. Extensive experiments conducted on several commonly-used datasetsdemonstrate the effectiveness and superiority of our method in comparison withother state-of-the-art approaches. The code is available athttps://github.com/KelvinXuu/GHICMC.</description>
      <author>example@mail.com (Guoqing Chao, Kaixin Xu, Xijiong Xie, Yongyong Chen)</author>
      <guid isPermaLink="false">2502.19291v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>AutoML for Multi-Class Anomaly Compensation of Sensor Drift</title>
      <link>http://arxiv.org/abs/2502.19180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in Measurement Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了两种解决传感器漂移问题的方法：一种是新的验证模型的补偿学习范式，另一种是通过自动化机器学习技术提高分类性能并补偿传感器漂移。&lt;h4&gt;背景&lt;/h4&gt;在工业测量系统中，精确的数据输出对于保持监测过程中的准确性和可靠性至关重要。然而，现有的模型训练方法使用标准交叉验证方法来估计模型性能时，未能充分考虑随时间累积的传感器漂移问题，导致模型预测未来数据偏差的能力减弱。&lt;h4&gt;目的&lt;/h4&gt;提出新的补偿学习范式和自动机器学习技术以改进分类性能，并有效应对传感器漂移。&lt;h4&gt;方法&lt;/h4&gt;论文提出了两个主要解决方案：(1) 一种新颖的用于验证模型精度的新颖传感器漂移补偿学习范例；(2) 自动化机器学习（AutoML）方法，通过采用数据平衡、元学习、自动集成学习、超参数优化、特征选择和提升等策略来改进分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;标准交叉验证方法在处理长期累积的传感器漂移时过于乐观，并且现有模型难以准确预测未来的数据偏差。论文展示了一种新的补偿学习范式以及AutoML-DC（针对漂移补偿）模型，该模型通过上述技术手段显著提高了分类性能。&lt;h4&gt;结论&lt;/h4&gt;提出的AutoML-DC方法不仅改善了分类性能，还能够有效适应不同的传感器漂移严重程度，从而增强了模型的泛化能力和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing sensor drift is essential in industrial measurement systems, whereprecise data output is necessary for maintaining accuracy and reliability inmonitoring processes, as it progressively degrades the performance of machinelearning models over time. Our findings indicate that the standardcross-validation method used in existing model training overestimatesperformance by inadequately accounting for drift. This is primarily becausetypical cross-validation techniques allow data instances to appear in bothtraining and testing sets, thereby distorting the accuracy of the predictiveevaluation. As a result, these models are unable to precisely predict futuredrift effects, compromising their ability to generalize and adapt to evolvingdata conditions. This paper presents two solutions: (1) a novel sensor driftcompensation learning paradigm for validating models, and (2) automated machinelearning (AutoML) techniques to enhance classification performance andcompensate sensor drift. By employing strategies such as data balancing,meta-learning, automated ensemble learning, hyperparameter optimization,feature selection, and boosting, our AutoML-DC (Drift Compensation) modelsignificantly improves classification performance against sensor drift.AutoML-DC further adapts effectively to varying drift severities.</description>
      <author>example@mail.com (Melanie Schaller, Mathis Kruse, Antonio Ortega, Marius Lindauer, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2502.19180v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation</title>
      <link>http://arxiv.org/abs/2502.19285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了从病理报告中选择信息进行视觉-语言模型训练对多模态表示和生成报告质量的影响。&lt;h4&gt;背景&lt;/h4&gt;目前很多视觉-语言模型在病理科的应用能够实现多模态病例检索和自动化报告生成，但这些模型往往基于包含患者历史等无法从全片图像推断出的信息的病理报告进行训练，这可能导致生成报告中的幻觉句子。&lt;h4&gt;目的&lt;/h4&gt;研究通过对比两种不同训练数据集（完整报告与仅包含细胞和组织外观描述的预处理报告）对视觉-语言模型性能的影响来探究如何选择信息以提高多模态表示的质量和生成报告的质量。&lt;h4&gt;方法&lt;/h4&gt;使用BLIP-2框架，并利用一个皮肤黑色素瘤病灶的数据集进行实验，该数据集包括42,433张H&amp;E染色的全片图像及其对应的19,636份病理报告。通过图像到文本和文本到图像检索以及由专业病理学家对生成报告的质量评估来评测模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;预处理文本可以防止报告生成中的幻觉，尽管这提升了生成报告质量但使用完整报告训练视觉-语言模型在跨模态检索中表现更好。&lt;h4&gt;结论&lt;/h4&gt;选择适当的训练数据对于提高视觉-语言模型的多模态表示和自动化病理报告生成的质量至关重要。然而，在实际应用中需要权衡预处理文本以避免幻觉与保持跨模态检索性能之间的关系。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型在病理科的应用能够实现多模态病例检索及自动化报告生成，但目前的很多模型是在包含无法从全片图像推断出的信息（如患者历史）的病理报告上训练出来的，这可能导致报告生成中的幻觉句子。为了探讨选择信息进行视觉-语言建模对质量的影响，研究人员对比了完整报告和仅描述细胞及组织外观的预处理报告训练得到的模型效果，并使用BLIP-2框架以及一个皮肤黑色素瘤病灶的数据集进行了实验评估。结果表明：文本预处理可以防止生成中的幻觉现象；尽管改善了生成报告的质量，但以完整报告训练出的视觉语言模型在跨模态检索中表现更好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models in pathology enable multimodal case retrieval andautomated report generation. Many of the models developed so far, however, havebeen trained on pathology reports that include information which cannot beinferred from paired whole slide images (e.g., patient history), potentiallyleading to hallucinated sentences in generated reports. To this end, weinvestigate how the selection of information from pathology reports forvision-language modeling affects the quality of the multimodal representationsand generated reports. More concretely, we compare a model trained on fullreports against a model trained on preprocessed reports that only includesentences describing the cell and tissue appearances based on the H&amp;E-stainedslides. For the experiments, we built upon the BLIP-2 framework and used acutaneous melanocytic lesion dataset of 42,433 H&amp;E-stained whole slide imagesand 19,636 corresponding pathology reports. Model performance was assessedusing image-to-text and text-to-image retrieval, as well as qualitativeevaluation of the generated reports by an expert pathologist. Our resultsdemonstrate that text preprocessing prevents hallucination in reportgeneration. Despite the improvement in the quality of the generated reports,training the vision-language model on full reports showed better cross-modalretrieval performance.</description>
      <author>example@mail.com (Ruben T. Lucassen, Tijn van de Luijtgaarden, Sander P. J. Moonemans, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19285v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multiview graph dual-attention deep learning and contrastive learning for multi-criteria recommender systems</title>
      <link>http://arxiv.org/abs/2502.19271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于多边图的新型表示方式，用于解决单一标准推荐系统在处理多准则推荐时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型对于推荐系统帮助用户选择符合其偏好的项目至关重要。然而，在单标准推荐系统中，存在忽视物品多样属性的问题，这些问题通过多准则推荐系统（MCRS）得以部分解决。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示方法和使用Multiview Dual Graph Attention Networks (MDGAT)来更好地考虑用户与物品之间的关系，并在每个视图及整个图形上运用对比学习以区分正负样本。&lt;h4&gt;方法&lt;/h4&gt;基于多边图，其中每条边代表了用户对项目的某一准则的评分；采用MDGAT模型，此模型能够有效处理局部（基于准则）和全局（多准则）的关系。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的评估表明该方法比基线模型具有更高的预测准确性。MDGAT能有效地捕捉邻居的局部和全局影响以及节点之间的相似性。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的表示形式和MDGAT模型，能够更准确地预测项目评分，并有效解决了多准则推荐系统中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习模型的推荐系统对于帮助用户选择符合他们偏好和兴趣的项目非常重要。然而，在单一标准推荐系统中仍然存在一个重要问题，即这些系统往往忽视物品的多样性属性，这些问题通过使用多准则推荐系统（MCRS）得以部分解决。虽然共享嵌入向量的方法用于处理基于多个标准的评分但难以捕捉用户与物品之间根据特定标准的具体关系。在这项研究中，我们提出了一种新的表示方法针对多准则推荐系统，采用一个多边图结构，其中每个边代表用户的某一项准则对项目的评分，并引入了Multiview Dual Graph Attention Networks（MDGAT）模型。使用MDGAT对于充分考虑用户与物品之间关系非常重要，因为存在局部（基于标准的）和全局（跨多个标准的）的关系。此外，在每个视图中定义锚点以相似性为基础，并运用局部和全局对比学习来区分各个视图以及整个图形中的正样本和负样本。我们在两个真实世界数据集上评估了该方法，根据项目评分预测性能进行了评估。结果表明与基线方法相比，在相同的数据集中使用我们的方法可以达到更高的准确率。MDGAT有效地捕捉邻居的局部和全局影响以及节点之间的相似性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender systems leveraging deep learning models have been crucial forassisting users in selecting items aligned with their preferences andinterests. However, a significant challenge persists in single-criteriarecommender systems, which often overlook the diverse attributes of items thathave been addressed by Multi-Criteria Recommender Systems (MCRS). Sharedembedding vector for multi-criteria item ratings but have struggled to capturethe nuanced relationships between users and items based on specific criteria.In this study, we present a novel representation for Multi-Criteria RecommenderSystems (MCRS) based on a multi-edge bipartite graph, where each edgerepresents one criterion rating of items by users, and Multiview Dual GraphAttention Networks (MDGAT). Employing MDGAT is beneficial and important foradequately considering all relations between users and items, given thepresence of both local (criterion-based) and global (multi-criteria) relations.Additionally, we define anchor points in each view based on similarity andemploy local and global contrastive learning to distinguish between positiveand negative samples across each view and the entire graph. We evaluate ourmethod on two real-world datasets and assess its performance based on itemrating predictions. The results demonstrate that our method achieves higheraccuracy compared to the baseline method for predicting item ratings on thesame datasets. MDGAT effectively capture the local and global impact ofneighbours and the similarity between nodes.</description>
      <author>example@mail.com (Saman Forouzandeh, Pavel N. Krivitsky, Rohitash Chandra)</author>
      <guid isPermaLink="false">2502.19271v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces</title>
      <link>http://arxiv.org/abs/2502.19281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了在脑机接口(BCI)应用中，传统和基于Transformer的注意力机制及其在多模态数据融合中的作用。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习技术的发展，注意力机制已经成为EEG信号分析不可或缺的一部分，并显著提升了BCI的应用效果。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在全面回顾传统的和基于Transformers的注意力机制、它们的嵌入策略以及这些方法如何应用于基于EEG的BCI中，特别关注多模态数据融合。&lt;h4&gt;方法&lt;/h4&gt;本文将注意力机制分为传统注意力机制与基于Transformer的多头自注意机制两大类。前者通常结合卷积网络和递归网络使用；后者擅长捕捉长范围依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过捕获EEG信号在时间、频率及空间通道的变化，注意力机制能提高特征提取能力、表示学习能力和模型鲁棒性。此外，注意力机制不仅提升了单模态分析的能力，还增强了多模态EEG应用的效果，促进了有效融合EEG与其他生理或感觉数据。&lt;h4&gt;结论&lt;/h4&gt;本文探讨了基于注意力的EEG建模中现有的挑战和新兴趋势，并展望了未来的发展方向，以推动BCI技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;随着深度学习技术的迅速发展，注意力机制已经成为脑电图（EEG）信号分析中的关键组成部分，显著增强了脑机接口（BCI）应用的效果。本文综述了传统和基于Transformer的注意力机制、它们的嵌入策略以及这些方法如何应用于基于EEG的BCI中，特别关注多模态数据融合。通过捕捉EEG信号在时间、频率及空间通道的变化，注意力机制能够提高特征提取能力、表示学习能力和模型鲁棒性。这些方法可以分为两大类：传统注意力机制，通常与卷积网络和递归网络结合使用；基于Transformer的多头自注意机制，擅长捕捉长范围依赖关系。除了单模态分析之外，注意力机制还提升了多模态EEG应用的效果，促进了有效融合EEG与其他生理或感觉数据的能力。最后，本文讨论了基于注意力的EEG建模中现有的挑战和新兴趋势，并为未来的研究方向提供了有价值的见解，以推动BCI技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of deep learning, attention mechanisms have becomeindispensable in electroencephalography (EEG) signal analysis, significantlyenhancing Brain-Computer Interface (BCI) applications. This paper presents acomprehensive review of traditional and Transformer-based attention mechanisms,their embedding strategies, and their applications in EEG-based BCI, with aparticular emphasis on multimodal data fusion. By capturing EEG variationsacross time, frequency, and spatial channels, attention mechanisms improvefeature extraction, representation learning, and model robustness. Thesemethods can be broadly categorized into traditional attention mechanisms, whichtypically integrate with convolutional and recurrent networks, andTransformer-based multi-head self-attention, which excels in capturinglong-range dependencies. Beyond single-modality analysis, attention mechanismsalso enhance multimodal EEG applications, facilitating effective fusion betweenEEG and other physiological or sensory data. Finally, we discuss existingchallenges and emerging trends in attention-based EEG modeling, highlightingfuture directions for advancing BCI technology. This review aims to providevaluable insights for researchers seeking to leverage attention mechanisms forimproved EEG interpretation and application.</description>
      <author>example@mail.com (Jiyuan Wang, Weishan Ye, Jialin He, Li Zhang, Gan Huang, Zhuliang Yu, Zhen Liang)</author>
      <guid isPermaLink="false">2502.19281v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images</title>
      <link>http://arxiv.org/abs/2502.19217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种提高数字病理学中细胞级分析工具质量和性能的方法，通过创建一个轻量级、可扩展的细胞分割和分类模型来解决数据集颗粒度限制、注释不一致等问题。&lt;h4&gt;背景&lt;/h4&gt;开发临床有用的细胞级别分析工具在数字病理学中面临挑战，包括数据集粒度过粗、标注一致性差、计算需求高以及难以将新技术整合到工作流程中的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决方案来改进数据质量、模型性能和可用性。&lt;h4&gt;方法&lt;/h4&gt;[{'步骤1': '更新数据标签通过交叉重标，提高PanNuke和MoNuSAC的数据注释精度，并生成包含七种不同细胞类型的统一数据集。'}, {'步骤2': '利用H-Optimus基础模型作为固定的编码器来改进同时进行分割和分类任务的特征表示。'}, {'步骤3': '为解决基础模型的计算需求问题，通过知识蒸馏减少模型大小和复杂性，保持性能大致相当。'}, {'步骤4': '将蒸馏后的模型集成到QuPath中，这是一个广泛使用的开源数字病理平台。'}]&lt;h4&gt;主要发现&lt;/h4&gt;[{'改进': '基于H-Optimus的模型在分割和分类性能上优于CNN基线模型，在R²评分从0.575提升至0.871及PQ评分从0.450升至0.492方面有明显改善，表明与实际细胞计数更一致且改进了分割质量。'}, {'性能': '蒸馏后的模型保持类似水平的性能同时参数数量减少到原来的四十八分之一。'}]&lt;h4&gt;结论&lt;/h4&gt;通过降低计算复杂度并整合至工作流程，该方法可能会对诊断产生重大影响、减轻病理学家的工作量，并提高结果质量。&lt;h4&gt;翻译&lt;/h4&gt;开发临床有用的细胞级别分析工具在数字病理学中面临数据集粒度过粗、标注一致性差等问题。为解决这些问题，论文提出了一种解决方案：创建一个轻量级的细胞分割和分类模型来提升数据质量和模型性能。此方法通过改进数据标签更新（包括交叉重标），利用H-Optimus基础模型进行特征表示优化，并通过知识蒸馏简化模型结构，同时保持性能水平。结果表明基于该方案的新模型在QuPath平台上的表现优于传统CNN基线模型，在多个评估指标上显示出显著的提升。此外，该方法还有望大幅降低计算复杂度，提高临床应用中的工作效率和诊断准确性。然而，尽管展示了很大的潜力，这种方法仍需进一步验证才能应用于临床实践中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing clinically useful cell-level analysis tools in digital pathologyremains challenging due to limitations in dataset granularity, inconsistentannotations, high computational demands, and difficulties integrating newtechnologies into workflows. To address these issues, we propose a solutionthat enhances data quality, model performance, and usability by creating alightweight, extensible cell segmentation and classification model. First, weupdate data labels through cross-relabeling to refine annotations of PanNukeand MoNuSAC, producing a unified dataset with seven distinct cell types.Second, we leverage the H-Optimus foundation model as a fixed encoder toimprove feature representation for simultaneous segmentation and classificationtasks. Third, to address foundation models' computational demands, we distillknowledge to reduce model size and complexity while maintaining comparableperformance. Finally, we integrate the distilled model into QuPath, a widelyused open-source digital pathology platform. Results demonstrate improvedsegmentation and classification performance using the H-Optimus-based modelcompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575to 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicatingbetter alignment with actual cell counts and enhanced segmentation quality. Thedistilled model maintains comparable performance while reducing parameter countby a factor of 48. By reducing computational complexity and integrating intoworkflows, this approach may significantly impact diagnostics, reducepathologist workload, and improve outcomes. Although the method shows promise,extensive validation is necessary prior to clinical deployment.</description>
      <author>example@mail.com (Nikita Shvetsov, Thomas K. Kilvaer, Masoud Tafavvoghi, Anders Sildnes, Kajsa Møllersen, Lill-Tove Rasmussen Busund, Lars Ailo Bongo)</author>
      <guid isPermaLink="false">2502.19217v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-level Attention-guided Graph Neural Network for Image Restoration</title>
      <link>http://arxiv.org/abs/2502.19181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了一种新的图像恢复技术，通过多级注意力引导的图神经网络解决当前深度学习方法在处理图像复原任务时忽略多重尺度信息的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于卷积神经网络的方法在图像修复领域取得了显著成就。然而，大多数这些方法通常集中在单一尺度上，忽视了融合多种尺度的信息。为了补充局部特征的不足，需要集成全局特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有模型未能明确构建全局特性或考虑全局与局部特征之间关系的问题。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了多级注意力引导图神经网络（multi-level attention-guided graph neural network）。通过使用多重注意机制，在特征映射内显式构造元素块图和元素图，以提取图像的局部结构特性和全局表示信息。这些图形通过多重注意机制实时学习动态连接，并利用图卷积算法传播和聚合信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个经典图像恢复任务中表现出色，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效融合局部元素块信息和全局元素表示信息来更有效地修复图像中的丢失数据。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度学习在图像复原领域取得了显著成功。然而，大多数基于卷积神经网络的方法主要集中在单一尺度上，忽视了多尺度信息的整合。为了补充局部特征的不足，在图像恢复任务中需要集成全局特性。尽管最近的神经网络算法在特征提取方面取得重大进展，但许多模型未能明确构建全局特性或考虑全局与局部特性的关系。本文提出了一种新的方法——多级注意力引导图神经网络（multi-level attention-guided graph neural network），该方法显式地使用多重注意机制，在特征映射内构造元素块图和元素图以提取图像的局部结构特性和全局表示信息。在结合了本地元素块信息和全局元素表示信息后，算法可以更有效地恢复图像中的丢失数据。实验结果显示该方法在多个经典图像复原任务中表现卓越，达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, deep learning has achieved remarkable success in the fieldof image restoration. However, most convolutional neural network-based methodstypically focus on a single scale, neglecting the incorporation of multi-scaleinformation. In image restoration tasks, local features of an image are ofteninsufficient, necessitating the integration of global features to complementthem. Although recent neural network algorithms have made significant stridesin feature extraction, many models do not explicitly model global features orconsider the relationship between global and local features. This paperproposes multi-level attention-guided graph neural network. The proposednetwork explicitly constructs element block graphs and element graphs withinfeature maps using multi-attention mechanisms to extract both local structuralfeatures and global representation information of the image. Since the networkstruggles to effectively extract global information during image degradation,the structural information of local feature blocks can be used to correct andsupplement the global information. Similarly, when element block information inthe feature map is missing, it can be refined using global elementrepresentation information. The graph within the network learns real-timedynamic connections through the multi-attention mechanism, and information ispropagated and aggregated via graph convolution algorithms. By combining localelement block information and global element representation information fromthe feature map, the algorithm can more effectively restore missing informationin the image. Experimental results on several classic image restoration tasksdemonstrate the effectiveness of the proposed method, achievingstate-of-the-art performance.</description>
      <author>example@mail.com (Jiatao Jiang, Zhen Cui, Chunyan Xu, Jian Yang)</author>
      <guid isPermaLink="false">2502.19181v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Foundation-Model-Based Industrial Defect Detection</title>
      <link>http://arxiv.org/abs/2502.19106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;随着工业产品变得丰富和复杂，视觉工业缺陷检测受到了广泛关注。研究中介绍了基于传统方法的统计分析、异常数据合成建模及生成模型的方法，并重点讨论了基础模型（FM）在提高检测精度方面的应用及其带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;工业产品的多样性和复杂性增加，促使对二维和三维视觉特征模型的研究增多。传统的缺陷检测主要依赖于统计分析方法以及基于生成的模型。&lt;h4&gt;目的&lt;/h4&gt;系统地回顾并对比了不同角度的基础模型方法，并简要回顾最近发表的非基础模型（NFM）的方法。同时讨论了FM与NFM在训练目标、模型结构和规模及性能方面的差异。&lt;h4&gt;方法&lt;/h4&gt;论文中详细比较了各种基于基础模型的技术，探讨这些技术如何利用视觉和文本语义先验知识来改进工业产品的缺陷检测，并分析了使用基础模型带来的模型复杂性和推断速度减慢的问题。此外还介绍了探索轻量级建模方式的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比研究发现，基于基础模型的方法更适合于少样本学习（few-shot learning）和零样本学习（zero-shot learning），这些方法更符合实际的工业应用场景，值得深入研究。&lt;h4&gt;结论&lt;/h4&gt;尽管使用基础模型可以提高检测精度，但同时也增加了模型复杂性并减慢了推理速度。因此需要探索轻量级建模方式以应对这些问题。此外，基于基础模型的方法在少样本学习和零样本学习中表现良好，这对于实际应用非常有帮助。&lt;h4&gt;翻译&lt;/h4&gt;随着工业产品变得丰富且多样化，视觉工业缺陷检测技术受到了越来越多的关注，包括二维及三维视觉特征的建模。传统方法通过统计分析、异常数据合成以及生成性模型来分离产品缺陷特征，并完成缺陷检测任务。最近，基础模型（Foundation Models）的发展带来了视觉和文本语义先验知识的应用，许多基于此的方法试图提高检测精度，但同时也增加了模型复杂度并减慢了推理速度。一些基于基础模型的方法已经开始探索轻量化建模方式，这些方法逐渐引起了关注，并值得系统性地分析。本文从多个角度对基于基础模型的视觉工业缺陷检测技术进行了系统的调查、比较和讨论，同时简要回顾最近发表的一些非基础模型的方法。此外还探讨了基于基础模型与非基础模型的方法在训练目标、模型结构及性能方面的差异，并指出了未来研究的方向。通过对比分析发现，基于基础模型的技术更适合处理少样本学习及零样本学习问题，这更符合实际工业应用场景中的需求，值得进一步深入研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As industrial products become abundant and sophisticated, visual industrialdefect detection receives much attention, including two-dimensional andthree-dimensional visual feature modeling. Traditional methods use statisticalanalysis, abnormal data synthesis modeling, and generation-based models toseparate product defect features and complete defect detection. Recently, theemergence of foundation models has brought visual and textual semantic priorknowledge. Many methods are based on foundation models (FM) to improve theaccuracy of detection, but at the same time, increase model complexity and slowdown inference speed. Some FM-based methods have begun to explore lightweightmodeling ways, which have gradually attracted attention and deserve to besystematically analyzed. In this paper, we conduct a systematic survey withcomparisons and discussions of foundation model methods from different aspectsand briefly review non-foundation model (NFM) methods recently published.Furthermore, we discuss the differences between FM and NFM methods fromtraining objectives, model structure and scale, model performance, andpotential directions for future exploration. Through comparison, we find FMmethods are more suitable for few-shot and zero-shot learning, which are morein line with actual industrial application scenarios and worthy of in-depthresearch.</description>
      <author>example@mail.com (Tianle Yang, Luyao Chang, Jiadong Yan, Juntao Li, Zhi Wang, Ke Zhang)</author>
      <guid isPermaLink="false">2502.19106v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>EndoMamba: An Efficient Foundation Model for Endoscopic Videos</title>
      <link>http://arxiv.org/abs/2502.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为EndoMamba的新型基础模型，旨在通过优化实时推理和增强表征学习来解决内窥镜视频任务中的计算效率低和性能不足问题。&lt;h4&gt;背景&lt;/h4&gt;内窥镜视频任务如视觉导航和手术阶段识别在微创手术中至关重要。然而，最近的视频基础模型由于计算复杂性和有限的数据集预训练而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门针对内窥镜视频设计的基础模型EndoMamba，以提高实时推理能力并改善广义时空表示学习。&lt;h4&gt;方法&lt;/h4&gt;{'1': '提出了优化后的EndoMamba骨干网络，以解决计算效率问题。该网络利用双向Mamba模块进行空间建模，并使用普通Mamba模块进行时间域上的从过去到现在的推断。', '2': '设计了一种自监督分层预训练方案，用于增强EndoMamba的表征学习能力，通过结合掩码重构和辅助监督来提取内窥镜视频中的时空结构和更广泛的知识。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'EndoMamba在四个下游任务上（分类、分割、手术阶段识别和定位）的表现优于现有基础模型和特定任务方法。', '2': '实验表明，EndoMamba能够同时保持实时推理速度并提供更好的性能。'}&lt;h4&gt;结论&lt;/h4&gt;EndoMamba展示了内窥镜视频处理中的高效能与实用性，在微创外科手术中具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了用于微创手术的内窥镜视频任务，如视觉导航和手术阶段识别，通过实时提供支持来发挥重要作用。尽管最近的视频基础模型显示出前景，但它们在计算效率低以及由于预训练时缺乏内窥镜数据而导致性能不佳方面面临问题。为了应对这些问题，论文提出了EndoMamba——一种专为实时推理而设计的基础模型，在学习泛化的时空表示的同时还能实现实时推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Endoscopic video-based tasks, such as visual navigation and surgical phaserecognition, play a crucial role in minimally invasive surgeries by providingreal-time assistance. While recent video foundation models have shown promise,their applications are hindered by (1) computational inefficiencies and (2)suboptimal performance caused by limited data for pre-training in endoscopy. Toaddress these issues, we present EndoMamba, a foundation model designed forreal-time inference while learning generalized spatiotemporal representations.First, to mitigate computational inefficiencies, we propose the EndoMambabackbone, optimized for real-time inference. Inspired by recent advancements instate space models, EndoMamba integrates Bidirectional Mamba blocks for spatialmodeling within individual frames and vanilla Mamba blocks for past-to-presentreasoning across the temporal domain. This design enables both strongspatiotemporal modeling and efficient inference in online video streams.Second, we propose a self-supervised hierarchical pre-training diagram toenhance EndoMamba's representation learning using endoscopic videos andincorporating general video domain knowledge. Specifically, our approachcombines masked reconstruction with auxiliary supervision, leveraging low-levelreconstruction to capture spatial-temporal structures and high-level alignmentto transfer broader knowledge from a pretrained general-video domain foundationmodel. Extensive experiments on four downstream tasks--classification,segmentation, surgical phase recognition, and localization--demonstrate thatEndoMamba outperforms existing foundation models and task-specific methodswhile maintaining real-time inference speed. The source code will be releasedupon acceptance.</description>
      <author>example@mail.com (Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu)</author>
      <guid isPermaLink="false">2502.19090v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Leg Exoskeleton Odometry using a Limited FOV Depth Sensor</title>
      <link>http://arxiv.org/abs/2502.19237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的里程计算法，该算法结合了外骨骼的本体感受数据和深度相机获取的点云信息，以生成准确的地表高度图。这种方法在有限视野和传感器运动较大的情况下尤其有效。&lt;h4&gt;背景&lt;/h4&gt;为了使腿部外骨骼能够在真实世界环境中有效地运行，必须能够感知和理解周围地形。然而，与其它腿足机器人相比，外骨骼由于人体的存在而限制了深度传感器的安装位置，导致视场受限且传感器运动更大，这使得里程计技术面临更大的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的里程计算法来解决腿部外骨骼因有限视野和传感器移动带来的问题，该算法能够生成准确的地表高度图。&lt;h4&gt;方法&lt;/h4&gt;利用扩展卡尔曼滤波器（EKF）融合了外骨骼的运动学和惯性测量数据，并通过定制化的迭代最近点（ICP）算法将新的点云与地表高度图进行配准。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示，该方法能够减少漂移并提高地表高度图的质量，相比单纯依靠本体感受的方法有明显改善，同时也优于传统的基于点云地图的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的新里程计算法在生成准确的地表高度图方面表现良好，为外骨骼在复杂环境下的有效运行提供了技术支持。&lt;h4&gt;翻译&lt;/h4&gt;为了使腿部外骨骼能够在真实世界环境中有效地运作，它们必须能够感知并理解周围的地形。然而，与其它腿足机器人不同的是，由于人体的存在，外骨骼的深度传感器安装位置受到限制，这导致视场较小且运动更大，使得里程计技术更加困难。为了解决这个问题，我们提出了一种新的融合了外骨骼本体感受数据和来自深度相机点云信息的新里程计算法，以生成准确的地表高度图，即使是在有限视野和传感器运动较大的情况下也能做到这一点。该方法利用扩展卡尔曼滤波器（EKF）结合运动学与惯性测量，并通过定制化的迭代最近点（ICP）算法将新获取的点云数据配准到地表高度图上。实验验证表明，我们的方法可以减少漂移并提高生成的地表高度图的质量，这在单纯依靠本体感受的数据基础上得到了改善，并且也优于传统的基于点云地图的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For leg exoskeletons to operate effectively in real-world environments, theymust be able to perceive and understand the terrain around them. However,unlike other legged robots, exoskeletons face specific constraints on wheredepth sensors can be mounted due to the presence of a human user. Theseconstraints lead to a limited Field Of View (FOV) and greater sensor motion,making odometry particularly challenging. To address this, we propose a novelodometry algorithm that integrates proprioceptive data from the exoskeletonwith point clouds from a depth camera to produce accurate elevation mapsdespite these limitations. Our method builds on an extended Kalman filter (EKF)to fuse kinematic and inertial measurements, while incorporating a tailorediterative closest point (ICP) algorithm to register new point clouds with theelevation map. Experimental validation with a leg exoskeleton demonstrates thatour approach reduces drift and enhances the quality of elevation maps comparedto a purely proprioceptive baseline, while also outperforming a moretraditional point cloud map-based variant.</description>
      <author>example@mail.com (Fabio Elnecave Xavier, Matis Viozelange, Guillaume Burger, Marine Pétriaux, Jean-Emmanuel Deschaud, François Goulette)</author>
      <guid isPermaLink="false">2502.19237v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks embedded into Margules model for vapor-liquid equilibria prediction</title>
      <link>http://arxiv.org/abs/2502.18998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文分析了嵌入扩展Margules模型中的图神经网络（GNNs）在预测气液平衡方面的性能。&lt;h4&gt;背景&lt;/h4&gt;预测热力学模型对于产品和工艺设计的早期阶段至关重要。传统的UNIFAC-Dortmund模型是这一领域的基准。&lt;h4&gt;目的&lt;/h4&gt;评估基于图神经网络嵌入扩展Margules模型的方法，与传统方法进行比较，并探索其在不同类型二元混合物中的表现以及局限性。&lt;h4&gt;方法&lt;/h4&gt;将GNNs嵌入到相对简单的过剩吉布斯自由能模型中，即扩展的Margules模型，用于预测气液平衡，并将其性能与公认的UNIFAC-Dortmund模型进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;尽管整体精度略低于传统的UNIFAC-Dortmund模型，在各种类型二元混合物中的表现却更为准确。此外，由于分子断裂可行性或参数可用性限制了组贡献方法如UNIFAC的应用范围，嵌入Margules模型的GNN提供了一个替代方案。&lt;h4&gt;结论&lt;/h4&gt;研究结果为简单的过剩吉布斯自由能模型结合仅基于无限稀释数据训练的图神经网络所能达到的预测准确性建立了基准。&lt;h4&gt;翻译&lt;/h4&gt;预测热力学模型对于产品和工艺设计的早期阶段至关重要。本文分析了将图神经网络（GNNs）嵌入到相对简单的过剩吉布斯自由能模型——扩展Margules模型中，用于预测气液平衡的表现情况。通过与成熟的UNIFAC-Dortmund模型进行比较，已证明在Margules模型中的GNN整体准确性略低。然而，在各种类型二元混合物的情况下观察到了更高的准确度。此外，由于分子断裂可行性或参数可用性限制了组贡献方法如UNIFAC的应用范围，嵌入Margules模型的GNN提供了一个用于气液平衡估算的替代方案。这些发现为简单的过剩吉布斯自由能模型结合仅基于无限稀释数据训练的图神经网络所能达到的预测准确性建立了基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive thermodynamic models are crucial for the early stages of productand process design. In this paper the performance of Graph Neural Networks(GNNs) embedded into a relatively simple excess Gibbs energy model, theextended Margules model, for predicting vapor-liquid equilibrium is analyzed.By comparing its performance against the established UNIFAC-Dortmund model ithas been shown that GNNs embedded in Margules achieves an overall loweraccuracy. However, higher accuracy is observed in the case of various types ofbinary mixtures. Moreover, since group contribution methods, like UNIFAC, arelimited due to feasibility of molecular fragmentation or availability ofparameters, the GNN in Margules model offers an alternative for VLE estimation.The findings establish a baseline for the predictive accuracy that simpleexcess Gibbs energy models combined with GNNs trained solely on infinitedilution data can achieve.</description>
      <author>example@mail.com (Edgar Ivan Sanchez Medina, Kai Sundmacher)</author>
      <guid isPermaLink="false">2502.18998v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models</title>
      <link>http://arxiv.org/abs/2502.19269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对预训练的视觉-语言模型（VLMs）在对抗性攻击中的脆弱性，提出了Class-wise Backdoor Prompt Tuning (CBPT)方法来提高其对后门攻击的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的防御策略主要集中在整个可疑模型的微调上，但它们只能提供边际抵抗，并且往往导致干净数据准确性的下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的Class-wise Backdoor Prompt Tuning (CBPT)方法来间接净化被污染的VLMs。&lt;h4&gt;方法&lt;/h4&gt;首先通过对比学习有效地反转潜在的后门触发器；然后利用提示调优技术优化这些类别的文本提示，修改模型决策边界以重新分类后门触发器的特征区域。&lt;h4&gt;主要发现&lt;/h4&gt;CBPT能够显著减轻后门威胁，并保持模型效用。例如，在七种主流的后门攻击中，平均干净准确率为58.86%，攻击成功率仅为0.39%。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了CBPT在提高VLMs对后门攻击鲁棒性方面的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了现有的针对视觉-语言模型（如CLIP）的防御策略存在局限性，尤其是对于数据受限的情况。为了改进这一点，研究人员提出了一种新的方法——Class-wise Backdoor Prompt Tuning (CBPT)，该方法通过调整文本提示来间接净化被污染的模型，并且实验结果表明这种方法在提高模型鲁棒性的同时还能保持良好的性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While pre-trained Vision-Language Models (VLMs) such as CLIP exhibitexcellent representational capabilities for multimodal data, recent studieshave shown that they are vulnerable to backdoor attacks. To alleviate thethreat, existing defense strategies primarily focus on fine-tuning the entiresuspicious model, yet offer only marginal resistance to state-of-the-artattacks and often result in a decrease in clean accuracy, particularly indata-limited scenarios. Their failure may be attributed to the mismatch betweeninsufficient fine-tuning data and massive parameters in VLMs. To address thischallenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, anefficient and effective method that operates on the text prompts to indirectlypurify the poisoned VLMs. Specifically, we first employ the advancedcontrastive learning via our carefully crafted positive and negative samples,to effectively invert the backdoor triggers that are potentially adopted by theattacker. Once the dummy trigger is established, we utilize the efficientprompt tuning technique to optimize these class-wise text prompts for modifyingthe model's decision boundary to further reclassify the feature regions ofbackdoor triggers. Extensive experiments demonstrate that CBPT significantlymitigates backdoor threats while preserving model utility, e.g. an averageClean Accuracy (CA) of 58.86\% and an Attack Success Rate (ASR) of 0.39\%across seven mainstream backdoor attacks. These results underscore thesuperiority of our prompt purifying design to strengthen model robustnessagainst backdoor attacks.</description>
      <author>example@mail.com (Jiawei Kong, Hao Fang, Sihang Guo, Chenxi Qing, Bin Chen, Bin Wang, Shu-Tao Xia)</author>
      <guid isPermaLink="false">2502.19269v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning Framework for Few-Shot Modulation Recognition</title>
      <link>http://arxiv.org/abs/2502.19071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;无线通信自动调制识别（AMR）及其挑战，以及如何通过少样本学习（FSL）框架解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;随着无线通信技术的快速发展，自动调制识别在确保通信安全和可靠性方面扮演着重要角色。然而，在特定场景下数据采集难度大、样本量小且标签质量低等问题阻碍了其发展。&lt;h4&gt;目的&lt;/h4&gt;引入一种结合多域对比学习与强化学习的新框架（MCLRL），以解决无线信号处理中少样本学习的挑战。&lt;h4&gt;方法&lt;/h4&gt;该研究没有提出新的FSL特定信号模型，而是提出了一个名为MCLRL的框架。此框架将多域对比学习与强化学习相结合，通过增强信号特征并提取深层次分类特性来优化性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的MCLRL框架能够有效从信号中提取关键特征，在少样本任务中表现出色，并且在选择信号模型方面保持了灵活性。&lt;h4&gt;结论&lt;/h4&gt;MCLRL框架提供了一种有效的解决方案，它通过结合多域对比学习和强化学习来克服无线通信自动调制识别中存在的挑战，以实现优异的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancements in wireless communication technology, automaticmodulation recognition (AMR) plays a critical role in ensuring communicationsecurity and reliability. However, numerous challenges, including higherperformance demands, difficulty in data acquisition under specific scenarios,limited sample size, and low-quality labeled data, hinder its development.Few-shot learning (FSL) offers an effective solution by enabling models toachieve satisfactory performance with only a limited number of labeled samples.While most FSL techniques are applied in the field of computer vision, they arenot directly applicable to wireless signal processing. This study does notpropose a new FSL-specific signal model but introduces a framework calledMCLRL. This framework combines multi-domain contrastive learning withreinforcement learning. Multi-domain representations of signals enhance featurerichness, while integrating contrastive learning and reinforcement learningarchitectures enables the extraction of deep features for classification. Indownstream tasks, the model achieves excellent performance using only a fewsamples and minimal training cycles. Experimental results show that the MCLRLframework effectively extracts key features from signals, performs well in FSLtasks, and maintains flexibility in signal model selection.</description>
      <author>example@mail.com (Dongwei Xu, Yutao Zhu, Yao Lu, Youpeng Feng, Yun Lin, Qi Xuan)</author>
      <guid isPermaLink="false">2502.19071v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法来解决机器学习模型在外分布泛化上的挑战，并通过实验验证了该方法的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型在面对训练数据分布之外的数据时，存在难以泛化的现象。尤其是在依赖于虚假相关性的模型中更为明显。&lt;h4&gt;目的&lt;/h4&gt;提供一种技术来指导神经网络在训练阶段的学习过程，以克服现有方法通常需要多域训练、群标签、专业化增广或预处理等限制。&lt;h4&gt;方法&lt;/h4&gt;首先建立输入对，表示虚假属性和描述不变性。基于这些对，形成一个与传统梯度下降互补的校正梯度，并使这种修正机制适应于预先定义的不变条件。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果在ColoredMNIST、Waterbird-100和CelebA数据集上显示出该方法的有效性和对群移位的强大鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效解决模型在外分布泛化中的挑战，并且无需依赖于多训练域或特定的预处理步骤，从而具有更高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型在面对训练数据分布之外的数据时面临外分布泛化的难题。现有方法通常需要额外资源来实现泛化效果，而本文提出的方法通过引导神经网络进行适应性修正，在多个数据集上验证了其有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution generalization of machine learning models remainschallenging since the models are inherently bound to the training datadistribution. This especially manifests, when the learned models rely onspurious correlations. Most of the existing approaches apply data manipulation,representation learning, or learning strategies to achieve generalizablemodels. Unfortunately, these approaches usually require multiple trainingdomains, group labels, specialized augmentation, or pre-processing to reachgeneralizable models. We propose a novel approach that addresses theselimitations by providing a technique to guide the neural network through thetraining phase. We first establish input pairs, representing the spuriousattribute and describing the invariance, a characteristic that should notaffect the outcome of the model. Based on these pairs, we form a correctivegradient complementing the traditional gradient descent approach. We furthermake this correction mechanism adaptive based on a predefined invariancecondition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasetsdemonstrate the effectiveness of our approach and the robustness to groupshifts.</description>
      <author>example@mail.com (Martin Surner, Abdelmajid Khelil, Ludwig Bothmann)</author>
      <guid isPermaLink="false">2502.18975v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Efficient and Distributed Large-Scale Point Cloud Bundle Adjustment via Majorization-Minimization</title>
      <link>http://arxiv.org/abs/2502.18801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云捆集调整是大规模点云地图构建的关键，但由于其计算和内存需求大，在处理大量扫描姿态时复杂度呈三次增长。&lt;h4&gt;背景&lt;/h4&gt;现有方法在进行大规模点云束优化时面临计算效率低、内存使用量大的问题。随着扫描姿态数量的增加，这些问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效且分布式的大型点云束集调整方法（BALM3.0），以解决上述挑战，并提高整体性能。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了基于majorization-minimization算法的解耦扫描姿态的方法，使用了基于点到平面距离的替代代价函数。这种方法将优化时间复杂度从三次降低到了线性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过解耦扫描姿态，可以显著提高大规模环境中的束集调整过程的计算效率。2. 提出了一种分布式的点云束集调整框架，并成功地使用四个消费级笔记本电脑对大规模数据（包括21,436个姿势和70GB点云）进行了优化。&lt;h4&gt;结论&lt;/h4&gt;该方法在模拟和现实世界环境中均表现出色，可以提供与现有方法相当的准确性同时大幅度提高速度并减少内存消耗。&lt;h4&gt;翻译&lt;/h4&gt;点云捆集调整是大规模点云地图构建的关键。然而，它既计算密集又占用大量内存，并且随着扫描姿态数量的增加，其复杂性呈三次增长。本文介绍了BALM3.0，这是一种高效的分布式大型点云束集调整方法。所提出的方法使用majorization-minimization算法在捆集调整过程中解耦扫描姿态，从而提高了大规模数据处理时的计算效率。将扫描姿势解耦的主要优势源于两个关键方面：首先，通过这种方式，优化的时间复杂度从三次降低到线性，极大地提升了大型环境中的束集调整过程的计算效率；其次，它为分布式捆集调整奠定了理论基础。通过在多个设备之间分配数据和计算任务，这种策略有助于克服内存需求大、计算要求高的限制，而这些对于单个设备来说可能难以处理。所提出的方法已在模拟和现实环境中进行了全面评估。结果表明，该方法可以实现与现有最优残差相当的精度，同时优化速度最高提升704倍，并且内存消耗减少到1/8。此外，本文还提出了并实现了分布式的束集调整框架，并成功地使用四个消费级笔记本电脑对大规模数据（包括21,436个姿势和70GB点云）进行了优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud bundle adjustment is critical in large-scale point cloud mapping.However, it is both computationally and memory intensive, with its complexitygrowing cubically as the number of scan poses increases. This paper presentsBALM3.0, an efficient and distributed large-scale point cloud bundle adjustmentmethod. The proposed method employs the majorization-minimization algorithm todecouple the scan poses in the bundle adjustment process, thus performing thepoint cloud bundle adjustment on large-scale data with improved computationalefficiency. The key difficulty of applying majorization-minimization on bundleadjustment is to identify the proper surrogate cost function. In this paper,the proposed surrogate cost function is based on the point-to-plane distance.The primary advantages of decoupling the scan poses via amajorization-minimization algorithm stem from two key aspects. First, thedecoupling of scan poses reduces the optimization time complexity from cubic tolinear, significantly enhancing the computational efficiency of the bundleadjustment process in large-scale environments. Second, it lays the theoreticalfoundation for distributed bundle adjustment. By distributing both data andcomputation across multiple devices, this approach helps overcome thelimitations posed by large memory and computational requirements, which may bedifficult for a single device to handle. The proposed method is extensivelyevaluated in both simulated and real-world environments. The resultsdemonstrate that the proposed method achieves the same optimal residual withcomparable accuracy while offering up to 704 times faster optimization speedand reducing memory usage to 1/8. Furthermore, this paper also presented andimplemented a distributed bundle adjustment framework and successfullyoptimized large-scale data (21,436 poses with 70 GB point clouds) with fourconsumer-level laptops.</description>
      <author>example@mail.com (Rundong Li, Zheng Liu, Hairuo Wei, Yixi Cai, Haotian Li, Fu Zhang)</author>
      <guid isPermaLink="false">2502.18801v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation</title>
      <link>http://arxiv.org/abs/2502.18875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了DeepTernary，这是一种基于深度学习的方法，用于预测蛋白质、E3连接酶和小分子之间形成的三元复合物的结构。&lt;h4&gt;背景&lt;/h4&gt;靶向蛋白降解（TPD）作为一种新兴的药物发现模式在不断发展中。PROTACs 和 分子胶降解剂（MGDs）是主要的小分子诱导 TPD 的方式，它们通过与 E3 连接酶和目标蛋白质形成三元复合物来起作用。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在开发一种深度学习方法来直接预测这些复杂的三元结构，以促进药物的发现过程。&lt;h4&gt;方法&lt;/h4&gt;DeepTernary 使用 SE(3)-等变图神经网络（GNN）结合了图内和三元图间注意机制，从高质训练数据集中提取复杂三元互动，并使用基于查询的 Pocket Points 解码器来解码最终的绑定结构。&lt;h4&gt;主要发现&lt;/h4&gt;DeepTernary 在现有的PROTAC基准测试中展现了最先进的准确度和速度，而在盲对接协议下的MGD基准测试中也显示出显著准确性。此外，预测出的埋藏表面积与实验获得的降解效力相关指标有很好的一致性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，DeepTernary 有可能在靶向不可药物化目标的发展过程中有效且加速地发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;基于小分子诱导的靶向蛋白质降解（TPD）已在药物发现领域迅速发展为一种新兴模式。PROTACs 和 分子胶降解剂（MGDs）是主要的小分子，它们通过与 E3 连接酶和目标蛋白形成三元复合物来实现 TPDS 的功能。虽然在二元结构预测方面取得了显著进展，但由于互动机制不明确以及训练数据不足，三元结构的预测仍然具有挑战性。该工作提出了一种新的基于深度学习的方法——DeepTernary，它能够直接通过编码器-解码器架构端到端地预测三元结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Targeted protein degradation (TPD) induced by small molecules has emerged asa rapidly evolving modality in drug discovery, targeting proteins traditionallyconsidered "undruggable". Proteolysis-targeting chimeras (PROTACs) andmolecular glue degraders (MGDs) are the primary small molecules that induceTPD. Both types of molecules form a ternary complex linking an E3 ligase with atarget protein, a crucial step for drug discovery. While significant advanceshave been made in binary structure prediction for proteins and small molecules,ternary structure prediction remains challenging due to obscure interactionmechanisms and insufficient training data. Traditional methods relying onmanually assigned rules perform poorly and are computationally demanding due toextensive random sampling. In this work, we introduce DeepTernary, a novel deeplearning-based approach that directly predicts ternary structures in anend-to-end manner using an encoder-decoder architecture. DeepTernary leveragesan SE(3)-equivariant graph neural network (GNN) with both intra-graph andternary inter-graph attention mechanisms to capture intricate ternaryinteractions from our collected high-quality training dataset, TernaryDB. Theproposed query-based Pocket Points Decoder extracts the 3D structure of thefinal binding ternary complex from learned ternary embeddings, demonstratingstate-of-the-art accuracy and speed in existing PROTAC benchmarks without priorknowledge from known PROTACs. It also achieves notable accuracy on the morechallenging MGD benchmark under the blind docking protocol. Remarkably, ourexperiments reveal that the buried surface area calculated from predictedstructures correlates with experimentally obtained degradation potency-relatedmetrics. Consequently, DeepTernary shows potential in effectively assisting andaccelerating the development of TPDs for previously undruggable targets.</description>
      <author>example@mail.com (Fanglei Xue, Meihan Zhang, Shuqi Li, Xinyu Gao, James A. Wohlschlegel, Wenbing Huang, Yi Yang, Weixian Deng)</author>
      <guid isPermaLink="false">2502.18875v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks</title>
      <link>http://arxiv.org/abs/2502.19070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be appeared in 39th Annual AAAI Conference on Artificial  Intelligence (AAAI-25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了模型反转攻击（MI）对机器学习隐私的影响，并提出了一个新的评估指标DDCS，用于更精确地衡量单样本级别的隐私保护情况。&lt;h4&gt;背景&lt;/h4&gt;模型反转攻击能够重构神经网络的训练数据集，威胁到机器学习中的隐私安全。现有评价标准对于样例级别隐私问题重视不够。&lt;h4&gt;目的&lt;/h4&gt;提出新的度量体系DDCS来评估个体样本在MI攻击下的脆弱性，并探索增强和防御机制。&lt;h4&gt;方法&lt;/h4&gt;引入新型度量标准DDCS以综合考量多个方面，同时设计了一种通过熵损失和自然梯度下降集成的转移学习框架，提升现有MI攻击技术。&lt;h4&gt;主要发现&lt;/h4&gt;许多训练样例对当前最先进MI攻击具备较强抵抗性；新提出的评估体系DDCS不仅提高了现有攻击方法的效果，在无监督环境中还能有效识别易受攻击的样本。&lt;h4&gt;结论&lt;/h4&gt;论文通过DDCS展示了样本级隐私保护的重要性，同时提出了一种有效的防御框架，并表明该指标在加强隐私安全方面有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model Inversion (MI) attacks, which reconstruct the training dataset ofneural networks, pose significant privacy concerns in machine learning. RecentMI attacks have managed to reconstruct realistic label-level private data, suchas the general appearance of a target person from all training images labeledon him. Beyond label-level privacy, in this paper we show sample-level privacy,the private information of a single target sample, is also important butunder-explored in the MI literature due to the limitations of existingevaluation metrics. To address this gap, this study introduces a novel metrictailored for training-sample analysis, namely, the Diversity and DistanceComposite Score (DDCS), which evaluates the reconstruction fidelity of eachtraining sample by encompassing various MI attack attributes. This, in turn,enhances the precision of sample-level privacy assessments.  Leveraging DDCS as a new evaluative lens, we observe that many trainingsamples remain resilient against even the most advanced MI attack. As such, wefurther propose a transfer learning framework that augments the generativecapabilities of MI attackers through the integration of entropy loss andnatural gradient descent. Extensive experiments verify the effectiveness of ourframework on improving state-of-the-art MI attacks over various metricsincluding DDCS, coverage and FID. Finally, we demonstrate that DDCS can also beuseful for MI defense, by identifying samples susceptible to MI attacks in anunsupervised manner.</description>
      <author>example@mail.com (Haoyang Li, Li Bai, Qingqing Ye, Haibo Hu, Yaxin Xiao, Huadi Zheng, Jianliang Xu)</author>
      <guid isPermaLink="false">2502.19070v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model</title>
      <link>http://arxiv.org/abs/2502.19026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ISCAS 2025(Lecture)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文探讨了基于InternVideo2视频基础模型在压缩场景下的视频质量评估任务中的应用，并提出了一种轻量级模型的蒸馏方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视频理解任务依赖于丰富的特征，包括语义信息、纹理和时间运动等。InternVideo2由于其庞大的参数规模和大规模多模态数据的支持，在这一领域显示出了强大的潜力。&lt;h4&gt;目的&lt;/h4&gt;研究将InternVideo2转移到压缩场景下的视频质量评估任务中的可行性，并探索设计一个轻量级模型的方法以适应此任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种蒸馏方法，使较小的模型能够获得丰富的压缩质量先验知识。此外，在蒸馏过程中还对不同骨干网络的表现进行了考察。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与其它方法相比，从InternVideo2中蒸馏出来的轻量级模型在压缩视频的质量评估任务中取得了优异的成绩。&lt;h4&gt;结论&lt;/h4&gt;通过提出的方法和技术，证明了将InternVideo2应用于视频质量评估的有效性，并为开发适用于这一领域的高效解决方案提供了依据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video quality assessment tasks rely heavily on the rich features required forvideo understanding, such as semantic information, texture, and temporalmotion. The existing video foundational model, InternVideo2, has demonstratedstrong potential in video understanding tasks due to its large parameter sizeand large-scale multimodal data pertaining. Building on this, we explored thetransferability of InternVideo2 to video quality assessment under compressionscenarios. To design a lightweight model suitable for this task, we proposed adistillation method to equip the smaller model with rich compression qualitypriors. Additionally, we examined the performance of different backbones duringthe distillation process. The results showed that, compared to other methods,our lightweight model distilled from InternVideo2 achieved excellentperformance in compression video quality assessment.</description>
      <author>example@mail.com (Fengbin Guan, Zihao Yu, Yiting Lu, Xin Li, Zhibo Chen)</author>
      <guid isPermaLink="false">2502.19026v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>TabGLM: Tabular Graph Language Model for Learning Transferable Representations Through Multi-Modal Consistency Minimization</title>
      <link>http://arxiv.org/abs/2502.18847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;处理表格数据集中的异构数据对深度学习模型构成了重大挑战。&lt;h4&gt;背景&lt;/h4&gt;注意力机制架构和自监督学习方法在某些领域取得了显著成功，但在应对表格数据时不如线性或基于树的模型有效。虽然有一些突破是通过将表格转换为图像、语言或图等单一模态来实现的，但这些方法在面对特征异质性时通常表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，我们引入了一种新型多模态架构TabGLM（表征图语言模型），旨在同时建模表格中的结构信息和语义信息。&lt;h4&gt;方法&lt;/h4&gt;TabGLM将表格的每一行转换为一个完全连接的图和序列化文本，然后分别使用图神经网络(GNN)和文本编码器对其进行编码。通过联合多模态自监督学习目标对齐这些表示，从而利用来自两种模式的互补信息来增强特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;TabGLM采用灵活的图-文本流水线处理异构数据集，在现有深度学习方法中使用显著更少的参数。在25个基准数据集上的评估显示了性能的重大提升，与最先进（SoTA）的表格学习方法相比，TabGLM实现了高达5.56%的平均AUC-ROC改进。&lt;h4&gt;结论&lt;/h4&gt;这项研究提出了一个创新的方法来处理异构表格数据，并通过在多个实际数据集上显著优于现有方法证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Handling heterogeneous data in tabular datasets poses a significant challengefor deep learning models. While attention-based architectures andself-supervised learning have achieved notable success, their application totabular data remains less effective over linear and tree based models. Althoughseveral breakthroughs have been achieved by models which transform tables intouni-modal transformations like image, language and graph, these models oftenunderperform in the presence of feature heterogeneity. To address this gap, weintroduce TabGLM (Tabular Graph Language Model), a novel multi-modalarchitecture designed to model both structural and semantic information from atable. TabGLM transforms each row of a table into a fully connected graph andserialized text, which are then encoded using a graph neural network (GNN) anda text encoder, respectively. By aligning these representations through ajoint, multi-modal, self-supervised learning objective, TabGLM leveragescomplementary information from both modalities, thereby enhancing featurelearning. TabGLM's flexible graph-text pipeline efficiently processesheterogeneous datasets with significantly fewer parameters over existing DeepLearning approaches. Evaluations across 25 benchmark datasets demonstratesubstantial performance gains, with TabGLM achieving an average AUC-ROCimprovement of up to 5.56% over State-of-the-Art (SoTA) tabular learningmethods.</description>
      <author>example@mail.com (Anay Majee, Maria Xenochristou, Wei-Peng Chen)</author>
      <guid isPermaLink="false">2502.18847v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>KAN-powered large-target detection for automotive radar</title>
      <link>http://arxiv.org/abs/2502.19000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新颖的雷达信号检测管道，用于检测大型目标如汽车和SUV。它基于Range-Doppler（RD）段的概率密度函数(pdf)，利用Kolmogorov-Arnold神经网络(KAN)来学习数据并生成二元假设的可解释符号表达式。&lt;h4&gt;背景&lt;/h4&gt;传统的方法，例如有序统计恒虚警率(OS-CFAR)，在汽车雷达中广泛应用。然而这些方法通常设计用于点目标或等向性目标模型，可能无法充分捕捉大型目标（如车辆）的Range-Doppler散射模式，特别是在高分辨率雷达系统中。&lt;h4&gt;目的&lt;/h4&gt;为了克服传统OS-CFAR检测技术在大型目标探测中的局限性，提出了一种新的基于概率密度函数和Kolmogorov-Arnold神经网络的检测方法。&lt;h4&gt;方法&lt;/h4&gt;研究通过Monte Carlo实验表明所提出的基于KAN表达式的检测性能优于传统的OS-CFAR。此外，该方法在使用现场数据进行迁移学习时，展示了96%的目标检出概率（PD），并且虚警率(PFA)与设计为$10^{-6}$的OS-CFAR相当。&lt;h4&gt;主要发现&lt;/h4&gt;研究还探讨了RD段pdf表示的分块数量对基于KAN检测性能的影响。实验表明提出的KAN表达式能够有效地处理大目标，而不需要额外的关联和跟踪模块来优化多视图下的探测结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在高分辨率雷达系统中对于大型目标（如汽车）的检测表现出优越性，尤其是在检出概率方面超过了传统方法，并且虚警率控制得当。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种新颖的雷达信号检测流程，专门用于探测大型目标，比如汽车和SUV。传统的有序统计恒虚警率(OS-CFAR)方法通常适用于点目标或各向同性模型，在高分辨率系统中可能无法充分表现大目标（如车辆）的Range-Doppler散射特性。新的方法基于Kolmogorov-Arnold神经网络(KAN)，能够学习数据并生成二元假设的概率密度函数，该方法优于传统的OS-CFAR，并且在使用现场数据进行迁移学习时达到了96%的目标检出率和相当的虚警率水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel radar signal detection pipeline focused ondetecting large targets such as cars and SUVs. Traditional methods, such asOrdered-Statistic Constant False Alarm Rate (OS-CFAR), commonly used inautomotive radar, are designed for point or isotropic target models. These maynot adequately capture the Range-Doppler (RD) scattering patterns of largertargets, especially in high-resolution radar systems. Additional modules suchas association and tracking are necessary to refine and consolidate thedetections over multiple dwells. To address these limitations, we propose adetection technique based on the probability density function (pdf) of RDsegments, leveraging the Kolmogorov-Arnold neural network (KAN) to learn thedata and generate interpretable symbolic expressions for binary hypotheses.Beside the Monte-Carlo study showing better performance for the proposed KANexpression over OS-CFAR, it is shown to exhibit a probability of detection (PD)of 96% when transfer learned with field data. The false alarm rate (PFA) iscomparable with OS-CFAR designed with PFA = $10^{-6}$. Additionally, the studyalso examines impact of the number of pdf bins representing RD segment onperformance of the KAN-based detection.</description>
      <author>example@mail.com (Vinay Kulkarni, V. V. Reddy, Neha Maheshwari)</author>
      <guid isPermaLink="false">2502.19000v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>(Mis)Fitting: A Survey of Scaling Laws</title>
      <link>http://arxiv.org/abs/2502.18969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages, 3 figure, first two authors contributed equally. ICLR, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现代基础模型依赖于使用缩放定律来指导重要的训练决策。研究者通常通过描述损失或任务性能与规模之间的关系从较小的训练运行中推断出最优架构和超参数设置。&lt;h4&gt;目的&lt;/h4&gt;讨论不同先驱工作在诸如最佳token到参数比率等问题上得出结论时存在的差异，并且探讨特定细节变化对缩放研究结果的影响以及因此导致的不同结论。此外，调查了超过50篇研究缩放趋势的论文：其中45篇使用幂定律来量化这些趋势，但大多数未能报告复制其发现所需的关键细节。&lt;h4&gt;方法&lt;/h4&gt;作者通过自己的分析来补充讨论，并为贡献于缩放律研究的研究者提出了一份检查清单，以帮助他们更好地报告关键细节。&lt;h4&gt;主要发现&lt;/h4&gt;不同的因素（如拟合的具体方程、训练设置和优化方法）可能会影响拟合的规律，进而影响给定研究的结论。大多数关于缩放趋势的研究未能充分报告必要的技术细节。&lt;h4&gt;结论&lt;/h4&gt;为了减轻这些问题的影响，作者建议所有贡献者在进行缩放律研究时参考提供的检查清单来确保他们提供了足够的细节以允许其他人重现他们的结果。&lt;h4&gt;翻译&lt;/h4&gt;现代基础模型依赖于使用扩展法则指导关键训练决策。研究人员通常从较小规模的训练中推导出最优架构和超参数设置，通过描述损失或任务性能与规模之间的关系来进行这一过程。该过程中所有因素的变化（如具体拟合方程、训练配置、优化方法等）都可能影响到得出的规则，并进而影响研究结论。论文作者讨论了先前工作的不同结论，包括有关最佳token-参数比率的问题，同时结合自身分析结果来探讨细节变化对缩放研究的影响及结论的改变。另外，调查了50余篇关于扩展趋势的研究：其中45项使用幂律量化趋势，但大多数未能充分报告实现其发现所需的详细信息。为解决这一问题，论文作者建议在进行扩展现有规则的研究时遵循特定检查清单。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern foundation models rely heavily on using scaling laws to guide crucialtraining decisions. Researchers often extrapolate the optimal architecture andhyper parameters settings from smaller training runs by describing therelationship between, loss, or task performance, and scale. All components ofthis process vary, from the specific equation being fit, to the training setup,to the optimization method. Each of these factors may affect the fitted law,and therefore, the conclusions of a given study. We discuss discrepancies inthe conclusions that several prior works reach, on questions such as theoptimal token to parameter ratio. We augment this discussion with our ownanalysis of the critical impact that changes in specific details may effect ina scaling study, and the resulting altered conclusions. Additionally, we surveyover 50 papers that study scaling trends: while 45 of these papers quantifythese trends using a power law, most under-report crucial details needed toreproduce their findings. To mitigate this, we we propose a checklist forauthors to consider while contributing to scaling law research.</description>
      <author>example@mail.com (Margaret Li, Sneha Kudugunta, Luke Zettlemoyer)</author>
      <guid isPermaLink="false">2502.18969v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Assisted Fast Design Migration Over Technology Nodes: A Study on Transformer Matching Network</title>
      <link>http://arxiv.org/abs/2502.18636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Publihsed and Presented at IEEE MTT-S International Microwave  Symposium (IMS 2024), Washington, DC, USA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种新颖的方法，利用预训练的合成神经网络模型的知识转移，在不同集成电路技术、操作频率和金属选项之间快速而可靠地设计毫米波被动网络。&lt;h4&gt;背景&lt;/h4&gt;在毫米波通信领域，传统的电路设计方法耗时且依赖于专业知识。引入了知识迁移的概念来提高设计效率。&lt;h4&gt;目的&lt;/h4&gt;通过模拟演示验证从一个技术节点的预训练合成神经网络模型的知识转移可以加速目标领域的训练过程，并提高R2值。&lt;h4&gt;方法&lt;/h4&gt;使用GF 45nm SOI（源领域）中经过训练的模型，将知识转移到GF 22nm FDX+（目标领域），并对1:1片上变压器的设计进行比较和分析。实验探索了不同数据密度的影响并评估了R2值。&lt;h4&gt;主要发现&lt;/h4&gt;知识转移可以显著减少所需的数据集大小，并且在源域和目标域之间表现出优秀的泛化能力，特别是在低数据密度下的性能提升明显。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明，利用迁移学习可以有效提高毫米波被动网络设计的效率和精度。该方法可以在不同集成电路技术和操作频率下加速模型训练并获得更好的R2值。&lt;h4&gt;翻译&lt;/h4&gt;在该项研究中，我们引入了一种创新性的毫米波无源网络设计方法，这种方法使用预先训练好的合成神经网络模型的知识转移，在不同的集成电路上实现快速且可靠的设计调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IMS40175.2024.10600344&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we introduce an innovative methodology for the design ofmm-Wave passive networks that leverages knowledge transfer from a pre-trainedsynthesis neural network (NN) model in one technology node and achieves swiftand reliable design adaptation across different integrated circuit (IC)technologies, operating frequencies, and metal options. We prove this conceptthrough simulation-based demonstrations focusing on the training and comparisonof the coefficient of determination (R2) of synthesis NNs for 1:1 on-chiptransformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and withouttransfer learning from a model trained in GF 45nm SOI (source domain). In theexperiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%with a complete dataset of 0.33 million in GF 22FDX+, and for comparativeanalysis, apply source data densities of 25%, 50%, 75%, and 100% with acomplete dataset of 2.5 million in GF 45SOI. With the source data only at30GHz, the experiments span target data from two metal options in GF 22FDX+ atfrequencies of 30 and 39 GHz. The results prove that the transfer learning withthe source domain knowledge (GF 45SOI) can both accelerate the training processin the target domain (GF 22FDX+) and improve the R2 values compared to modelswithout knowledge transfer. Furthermore, it is observed that a model trainedwith just 5% of target data and augmented by transfer learning achieves R2values superior to a model trained with 20% of the data without transfer,validating the advantage seen from 1% to 5% data density. This demonstrates anotable reduction of 4X in the necessary dataset size highlighting the efficacyof utilizing transfer learning to mm-Wave passive network design. The PyTorchlearning and testing code is publicly available athttps://github.com/ChenhaoChu/RFIC-TL.</description>
      <author>example@mail.com (Chenhao Chu, Yuhao Mao, Hua Wang)</author>
      <guid isPermaLink="false">2502.18636v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>MaskPlanner: Learning-Based Object-Centric Motion Generation from 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2502.18745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website at https://gabrieletiboni.github.io/MaskPlanner/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的、完全数据驱动的框架，用于基于3D点云直接处理对象中心运动生成（OCMG）问题。&lt;h4&gt;背景&lt;/h4&gt;现有的解决方案依赖于专门的启发式方法、昂贵的优化过程或限制性的几何假设，这些都降低了它们在实际场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的学习框架MaskPlanner来解决OCMG问题，该框架可以从3D点云中直接预测局部路径段并进行分组，以满足任务需求。&lt;h4&gt;方法&lt;/h4&gt;MaskPlanner是一种深度学习方法，能够为给定的对象预测局部路径片段，并同时推断出“路径掩码”，将这些片段归类为不同的路径。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法对于未见过的对象能达到近乎完整的覆盖（超过99%），且无需显式优化喷漆沉积。此外，在6自由度专业喷涂机器人上的真实世界验证中展示了生成的轨迹可以直接执行并达到专家级的喷涂质量。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了所提出的学习方法在减少工程负担方面和无缝适应多种工业应用案例中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;对象中心运动生成（OCMG）在各种工业应用中起着关键作用，例如机器人喷漆和焊接。需要高效、可扩展且通用的算法来为自由形状3D物体规划多个长期轨迹。然而，现有的解决方案依赖于专门的启发式方法、昂贵的优化过程或限制性的几何假设，这限制了它们在实际场景中的适应性。本文提出了一种新的完全数据驱动框架，直接从3D点云处理OCMG问题，并学习如何跨自由形状表面泛化专家路径模式。我们提出了MaskPlanner深度学习方法，该方法预测给定物体的局部路径片段并同时推断“路径掩码”，将这些片段分类为不同的路径，从而让网络在单次前向传递中捕获局部几何模式和全局任务需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object-Centric Motion Generation (OCMG) plays a key role in a variety ofindustrial applications$\unicode{x2014}$such as robotic spray painting andwelding$\unicode{x2014}$requiring efficient, scalable, and generalizablealgorithms to plan multiple long-horizon trajectories over free-form 3Dobjects. However, existing solutions rely on specialized heuristics, expensiveoptimization routines, or restrictive geometry assumptions that limit theiradaptability to real-world scenarios. In this work, we introduce a novel, fullydata-driven framework that tackles OCMG directly from 3D point clouds, learningto generalize expert path patterns across free-form surfaces. We proposeMaskPlanner, a deep learning method that predicts local path segments for agiven object while simultaneously inferring "path masks" to group thesesegments into distinct paths. This design induces the network to capture bothlocal geometric patterns and global task requirements in a single forward pass.Extensive experimentation on a realistic robotic spray painting scenario showsthat our approach attains near-complete coverage (above 99%) for unseenobjects, while it remains task-agnostic and does not explicitly optimize forpaint deposition. Moreover, our real-world validation on a 6-DoF specializedpainting robot demonstrates that the generated trajectories are directlyexecutable and yield expert-level painting quality. Our findings cruciallyhighlight the potential of the proposed learning method for OCMG to reduceengineering overhead and seamlessly adapt to several industrial use cases.</description>
      <author>example@mail.com (Gabriele Tiboni, Raffaello Camoriano, Tatiana Tommasi)</author>
      <guid isPermaLink="false">2502.18745v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Transient Classification: From Simulations to Real Data and ZTF to LSST</title>
      <link>http://arxiv.org/abs/2502.18558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过转移学习的方法，展示了如何利用已有的模型来处理天文瞬变现象的分类问题。这种方法可以显著减少标记数据的需求，并且保持与从头开始训练的模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习在自动分类天文学中的瞬变现象中起着关键作用，但现有方法面临着诸多挑战：基于模拟数据训练的分类器难以处理真实数据；为一个调查定制的模型难以应用于其他调查。随着大型天文观测设施如维拉·鲁宾天文台（Legacy Survey of Space and Time, LSST）时代的到来，这些问题变得更加紧迫。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示转移学习如何克服现有方法面临的挑战，使得在新调查开始初期就能实现可靠的自动化分类。&lt;h4&gt;方法&lt;/h4&gt;使用了一种基于模拟数据训练的模型，并通过转移学习技术将其应用于实际数据和不同调查的数据。具体而言，从一个利用模拟Zwicky瞬变设施（ZTF）光曲线训练的模型出发，展示了这种技术如何减少75%的真实标记ZTF瞬变现象的需求，同时保持与全新训练模型相同的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 转移学习可以在很大程度上减少了所需的真实标记数据量；2. 当将ZTF模型应用于LSST模拟时，转移学习可以达到基准性能的95%，而只需要30%的训练数据。这些结果对即将启动的LSST早期操作具有重要影响。&lt;h4&gt;结论&lt;/h4&gt;通过转移学习，即使在新调查开始初期也能实现可靠的自动化分类，无需等待数月或数年积累足够的标记训练数据。&lt;h4&gt;翻译&lt;/h4&gt;机器学习对于天文瞬变现象的自动分类至关重要，但当前方法面临诸多限制：基于模拟训练的分类器难以处理真实数据；为一个观测定制的模型难以应用于其他调查。随着大型天文设施如LSST时代的到来，现有模型将需要重新利用LSST的数据进行再培训。本文证明了转移学习可以通过重用已有的基于模拟或来自其它调查的数据训练的模型来克服这些挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning has become essential for automated classification ofastronomical transients, but current approaches face significant limitations:classifiers trained on simulations struggle with real data, models developedfor one survey cannot be easily applied to another, and new surveys requireprohibitively large amounts of labelled training data. These challenges areparticularly pressing as we approach the era of the Vera Rubin Observatory'sLegacy Survey of Space and Time (LSST), where existing classification modelswill need to be retrained using LSST observations. We demonstrate that transferlearning can overcome these challenges by repurposing existing models trainedon either simulations or data from other surveys. Starting with a model trainedon simulated Zwicky Transient Facility (ZTF) light curves, we show thattransfer learning reduces the amount of labelled real ZTF transients needed by75\% while maintaining equivalent performance to models trained from scratch.Similarly, when adapting ZTF models for LSST simulations, transfer learningachieves 95\% of the baseline performance while requiring only 30\% of thetraining data. These findings have significant implications for the earlyoperations of LSST, suggesting that reliable automated classification will bepossible soon after the survey begins, rather than waiting months or years toaccumulate sufficient training data.</description>
      <author>example@mail.com (Rithwik Gupta, Daniel Muthukrishna)</author>
      <guid isPermaLink="false">2502.18558v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation</title>
      <link>http://arxiv.org/abs/2502.18771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了大型语言模型（LLMs）在图学习任务中的应用，并评估了它们在不同场景下的性能，特别是在少样本/零样本设置、跨领域迁移能力以及对图形结构的理解和鲁棒性方面。&lt;h4&gt;背景&lt;/h4&gt;随着图状数据的普遍使用，传统的图神经网络（GNNs）虽然取得了一些进展，但在某些上下文中处理图数据的能力仍然有限。大型语言模型因其在处理限制数据、任务间转移性和鲁棒性的潜力而成为新的研究热点。&lt;h4&gt;目的&lt;/h4&gt;全面探索LLMs在图学习任务中的应用，并评估其性能，以揭示它们的优点和潜在的现实世界应用场景。&lt;h4&gt;方法&lt;/h4&gt;研究比较了16种图学习模型与6种大型语言模型（如Llama3B、GPT-4o、Qwen-plus）在Cora、PubMed、ArXiv等数据集上的表现。评估包括未进行参数优化和经过指令微调的LLMs。&lt;h4&gt;主要发现&lt;/h4&gt;具有指令微调的LLMs在少样本设置中优于传统模型，表现出强大的跨领域迁移能力，并展示了优秀的泛化性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该研究为大型语言模型用于图学习提供了有价值的见解，揭示了它们的优势和潜在的应用场景，为未来的研究铺平道路。代码与数据集可在GitHub上获得（https://github.com/myflashbarry/LLM-benchmarking）。&lt;h4&gt;翻译&lt;/h4&gt;随着图状数据在各个领域的日益普及，对有效处理节点分类和链接预测等任务的模型的需求也在增长。尽管传统的图学习模型如图神经网络已经取得了显著进展，但在某些上下文中的能力仍然有限。近年来，大型语言模型作为潜在解决方案崭露头角，但大多数研究主要关注性能基准测试，未能充分探讨其广泛潜力，包括处理限制数据的能力、任务间的转移性和鲁棒性等。本工作全面探索了大型语言模型应用于图学习任务，并评估了纯LLM（包含未优化参数和指令微调）在各种情况下的表现。我们的分析不仅局限于准确性，还涉及LLMs在少样本/零样本设置中的能力、跨领域迁移能力、理解图形结构的能力以及在挑战性场景中的鲁棒性等。我们在16种图学习模型与如Llama3B、GPT-4o、Qwen-plus等六种大型语言模型之间进行了广泛的实验，对比了它们在Cora、PubMed、ArXiv和Products等数据集上的表现。我们的研究发现显示，在少样本设置中，具有指令微调的LLMs优于传统的图学习模型，表现出强大的跨领域迁移能力，并展示了优秀的泛化性和鲁棒性。这项工作为大型语言模型用于图学习提供了宝贵的见解，突显了它们的优势和潜在的实际应用价值，并为未来的研究铺平道路。代码与数据集可在https://github.com/myflashbarry/LLM-benchmarking上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/myflashbarry/LLM-benchmarking&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data has become increasingly prevalent across variousdomains, raising the demand for effective models to handle graph tasks likenode classification and link prediction. Traditional graph learning models likeGraph Neural Networks (GNNs) have made significant strides, but theircapabilities in handling graph data remain limited in certain contexts. Inrecent years, large language models (LLMs) have emerged as promising candidatesfor graph tasks, yet most studies focus primarily on performance benchmarks andfail to address their broader potential, including their ability to handlelimited data, their transferability across tasks, and their robustness. In thiswork, we provide a comprehensive exploration of LLMs applied to graph tasks. Weevaluate the performance of pure LLMs, including those without parameteroptimization and those fine-tuned with instructions, across various scenarios.Our analysis goes beyond accuracy, assessing LLM ability to perform infew-shot/zero-shot settings, transfer across domains, understand graphstructures, and demonstrate robustness in challenging scenarios. We conductextensive experiments with 16 graph learning models alongside 6 LLMs (e.g.,Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora,PubMed, ArXiv, and Products. Our findings show that LLMs, particularly thosewith instruction tuning, outperform traditional models in few-shot settings,exhibit strong domain transferability, and demonstrate excellent generalizationand robustness. This work offers valuable insights into the capabilities ofLLMs for graph learning, highlighting their advantages and potential forreal-world applications, and paving the way for future research in this area.Codes and datasets are released inhttps://github.com/myflashbarry/LLM-benchmarking.</description>
      <author>example@mail.com (Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma)</author>
      <guid isPermaLink="false">2502.18771v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model</title>
      <link>http://arxiv.org/abs/2502.18763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;大语言模型（LLMs）具备人类级别的认知和决策能力，是6G技术的关键。然而，在通信领域应用LLMs面临三大挑战：1) 通信数据不足；2) 输入模态受限；3) 知识检索困难。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在6G中扮演关键角色，但由于其缺乏高质量的通信特定训练数据、只能处理有限输入模态及难以有效检索领域知识，因此很难直接应用于通信场景。&lt;h4&gt;目的&lt;/h4&gt;提出CommGPT，这是一个专门针对通信领域的多模态基础模型。为了克服上述问题，CommGPT旨在提供更好的通信专业知识学习能力，并能够适应多种输入类型，同时提高对已有知识的利用效率。&lt;h4&gt;方法&lt;/h4&gt;1. 创建高质预训练和微调数据集；2. 设计用于理解处理多样化信息输入的多模态编码器；3. 构建图谱增强检索增强生成框架（GRG），以结合知识图谱与检索增强生成技术，实现跨尺度学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了CommGPT的有效性和可行性，表明其在通信领域的应用潜力巨大。&lt;h4&gt;结论&lt;/h4&gt;提出了专门用于通信的多模态基础模型CommGPT，并证明它可以在解决当前LLMs面临的挑战方面提供有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) possess human-level cognitive and decision-making capabilities, making them a key technology for 6G. However, applying LLMs to the communication domain faces three major challenges: 1) Inadequate communication data; 2) Restricted input modalities; and 3) Difficulty in knowledge retrieval. To overcome these issues, we propose CommGPT, a multimodal foundation model designed specifically for communications. First, we create high-quality pretraining and fine-tuning datasets tailored to the field of communications, enabling the LLM to engage in further pretraining and fine-tuning with communication concepts and knowledge. Then, we design a multimodal encoder to understand and process information from various input modalities. Next, we construct a Graph and Retrieval-Augmented Generation (GRG) framework, efficiently coupling Knowledge Graph (KG) with Retrieval-Augmented Generation (RAG) for multi-scale learning. Finally, we demonstrate the feasibility and effectiveness of the CommGPT through experimental validation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) possess human-level cognitive anddecision-making capabilities, making them a key technology for 6G. However,applying LLMs to the communication domain faces three major challenges: 1)Inadequate communication data; 2) Restricted input modalities; and 3)Difficulty in knowledge retrieval. To overcome these issues, we proposeCommGPT, a multimodal foundation model designed specifically forcommunications. First, we create high-quality pretraining and fine-tuningdatasets tailored in communication, enabling the LLM to engage in furtherpretraining and fine-tuning with communication concepts and knowledge. Then, wedesign a multimodal encoder to understand and process information from variousinput modalities. Next, we construct a Graph and Retrieval-Augmented Generation(GRG) framework, efficiently coupling Knowledge Graph (KG) withRetrieval-Augmented Generation (RAG) for multi-scale learning. Finally, wedemonstrate the feasibility and effectiveness of the CommGPT throughexperimental validation.</description>
      <author>example@mail.com (Feibo Jiang, Wanyun Zhu, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Octavia A. Dobre)</author>
      <guid isPermaLink="false">2502.18763v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</title>
      <link>http://arxiv.org/abs/2502.18748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 14th Workshop on Hyperspectral Imaging and Signal  Processing: Evolution in Remote Sensing (WHISPERS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种利用预训练的大型Transformer模型进行高光谱对象跟踪的有效方法。&lt;h4&gt;背景&lt;/h4&gt;基于快照马赛克相机的高光谱目标追踪技术因提供增强的光谱信息和空间数据而备受关注，这有助于更全面地理解材料属性。然而，大规模Transformer的训练需要大量的数据集和长时间的训练过程，这对于高光谱领域现有的小规模数据集来说是一个瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种适应大型预训练基础模型的方法，用于高光谱对象跟踪，并通过跨模态训练管道促进不同传感器收集的数据之间的有效学习。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应、可学习的空间-光谱令牌融合模块，可以扩展到任何基于Transformer的骨干网络中，以学习高光谱数据中的固有空间-光谱特征。此外，模型还包含一个跨模态训练管道，允许在不同传感器模式下收集的数据之间进行有效的跨域学习。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在较少的训练迭代次数下实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;通过上述创新方法，高光谱对象跟踪任务能够克服现有数据集规模较小的问题，并利用Transformer的强大功能来提升追踪效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral object tracking using snapshot mosaic cameras is emerging as itprovides enhanced spectral information alongside spatial data, contributing toa more comprehensive understanding of material properties. Using transformers,which have consistently outperformed convolutional neural networks (CNNs) inlearning better feature representations, would be expected to be effective forHyperspectral object tracking. However, training large transformersnecessitates extensive datasets and prolonged training periods. This isparticularly critical for complex tasks like object tracking, and the scarcityof large datasets in the hyperspectral domain acts as a bottleneck in achievingthe full potential of powerful transformer models. This paper proposes aneffective methodology that adapts large pretrained transformer-based foundationmodels for hyperspectral object tracking. We propose an adaptive, learnablespatial-spectral token fusion module that can be extended to anytransformer-based backbone for learning inherent spatial-spectral features inhyperspectral data. Furthermore, our model incorporates a cross-modalitytraining pipeline that facilitates effective learning across hyperspectraldatasets collected with different sensor modalities. This enables theextraction of complementary knowledge from additional modalities, whether ornot they are present during testing. Our proposed model also achieves superiorperformance with minimal training iterations.</description>
      <author>example@mail.com (Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes)</author>
      <guid isPermaLink="false">2502.18748v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种全自动、无需目标物且在线的多传感器校准框架CalibRefine，该方法能够直接处理原始LiDAR点云和相机图像数据，并通过四个阶段实现高精度的校准。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶、机器人技术及智能交通系统等应用中，精确的多传感器校准至关重要。现有基于LIDAR-相机的方法通常依赖于手动放置的目标物或预先估计参数，这限制了其在现实环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需人工干预和目标物的全自动在线校准框架CalibRefine，以解决传统方法中存在的问题，并提高多传感器系统在校准过程中的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;['第一阶段：通过使用相对位置、外观嵌入和语义类别自动检测对象，训练通用特征鉴别器生成可靠的LIDAR-相机对应关系', '第二阶段：基于粗略的同态变换进行校准', '第三阶段：迭代改进数据帧变得可用时对齐过程', '第四阶段：利用视觉变压器和交叉注意力机制处理非平面扭曲']&lt;h4&gt;主要发现&lt;/h4&gt;通过在两个城市交通数据集上的广泛实验，CalibRefine展示了高精度的校准结果，并且在最少的人工干预下超越了现有的无目标物方法，同时与手动调优基线保持竞争力或优于它们。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了如何利用稳健的对象级特征匹配以及迭代和自我监督的关注机制调整，在复杂的现实世界条件下实现一致的传感器融合，而无需地面实况校准矩阵或复杂的预处理数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models</title>
      <link>http://arxiv.org/abs/2502.18744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,5 figures,3 tables,4 graphs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Kinship-Aware Preference MApping (KARMA)框架，该框架通过系统地配对具有相似能力的模型响应来改进大型语言模型（LLM）的行为与人类偏好的一致性的方法。&lt;h4&gt;背景&lt;/h4&gt;当前在大型语言模型（LLM）对齐方面取得的进步试图减少人工注释的成本，利用预训练模型生成偏好数据。然而，现有的方法通常比较来自能力显著不同的模型的响应，这些浅层次的区别未能提供有意义的指导以说明哪种回应更优。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法存在的局限性，提出了一种新的框架KARMA，该框架系统地配对具有类似能力的模型之间的响应。&lt;h4&gt;方法&lt;/h4&gt;通过将偏好比较约束为复杂度和质量相似的输出，增强了偏好数据的信息量，并提高了对齐信号的粒度。&lt;h4&gt;主要发现&lt;/h4&gt;实证评估表明，我们的亲缘关系感知的方法导致了更一致且可解释的对齐结果。&lt;h4&gt;结论&lt;/h4&gt;这种方法最终促进了将LLM行为与人类偏好对齐的原则性和可靠路径。&lt;h4&gt;翻译&lt;/h4&gt;最近在大型语言模型（LLM）对齐方面的进展试图通过利用预训练模型生成偏好数据来减少人工注释的成本。然而，现有的方法往往比较来自能力显著不同的模型的响应，这些差异未能提供有意义的指导以说明哪种回应更优。为了解决这一限制，我们提出了亲缘关系感知的偏好数据映射（KARMA）框架，它系统地配对具有相似能力的模型之间的响应。通过将偏好比较约束为复杂度和质量相似的输出，KARMA增强了偏好数据的信息量，并提高了对齐信号的粒度。实证评估表明，我们的亲缘关系感知的方法导致了更一致且可解释的对齐结果，最终促进了将LLM行为与人类偏好对齐的原则性和可靠路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Model (LLM) alignment have sought tomitigate the cost of human annotations by leveraging pretrained models togenerate preference data. However, existing methods often compare responsesfrom models with substantially different capabilities, yielding superficialdistinctions that fail to provide meaningful guidance on what constitutes asuperior response. To address this limitation, we propose Kinship-AwarepReference MApping (KARMA), a novel framework that systematically pairsresponses from models with comparable competencies. By constraining preferencecomparisons to outputs of similar complexity and quality, KARMA enhances theinformativeness of preference data and improves the granularity of alignmentsignals. Empirical evaluations demonstrate that our kinship-aware approachleads to more consistent and interpretable alignment outcomes, ultimatelyfacilitating a more principled and reliable pathway for aligning LLM behaviorwith human preferences.</description>
      <author>example@mail.com (Jeesu Jung, Chanjun Park, Sangkeun Jung)</author>
      <guid isPermaLink="false">2502.18744v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>What are Foundation Models Cooking in the Post-Soviet World?</title>
      <link>http://arxiv.org/abs/2502.18583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过构建BORSch数据集，评估了基础模型对后苏联地区饮食文化的理解能力。&lt;h4&gt;背景&lt;/h4&gt;后苏联国家的文化复杂且深受历史影响，这种文化持续影响着当前的事件和人们的生活方式。&lt;h4&gt;目的&lt;/h4&gt;调查大型语言模型对于后苏联区域饮食文化知识的理解程度。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含1147道俄罗斯菜和823道乌克兰菜肴的多模态数据集BORSch。使用该数据集评估了现有基础模型在文本问答（QA）以及跨模态问答中的表现，并进一步测试这些模型生成准确视觉描述的能力。&lt;h4&gt;主要发现&lt;/h4&gt;主导的基础模型在识别后苏联国家菜系起源时遇到困难，往往过度预测与问题语言相关的国家；这些结果可以通过训练数据中误导性的菜品和来源共现现象来解释，以及俄语和乌克兰语之间的代码混合等语言学现象。&lt;h4&gt;结论&lt;/h4&gt;单纯基于问答评估文化理解可能不足以全面评价模型的能力；为了促进进一步研究，BORSch将公开发布在GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The culture of the Post-Soviet states is complex, shaped by a turbulenthistory that continues to influence current events. In this study, weinvestigate the Post-Soviet cultural food knowledge of foundation models byconstructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes inthe Russian and Ukrainian languages, centered around the Post-Soviet region. Wedemonstrate that leading models struggle to correctly identify the origins ofdishes from Post-Soviet nations in both text-only and multimodal QuestionAnswering (QA), instead over-predicting countries linked to the language thequestion is asked in. Through analysis of pretraining data, we show that theseresults can be explained by misleading dish-origin co-occurrences, along withlinguistic phenomena such as Russian-Ukrainian code mixing. Finally, to movebeyond QA-based assessments, we test models' abilities to produce accuratevisual descriptions of dishes. The weak correlation between this task and QAsuggests that QA alone may be insufficient as an evaluation of culturalunderstanding. To foster further research, we will make BORSch publiclyavailable at https://github.com/alavrouk/BORSch.</description>
      <author>example@mail.com (Anton Lavrouk, Tarek Naous, Alan Ritter, Wei Xu)</author>
      <guid isPermaLink="false">2502.18583v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Rewards-based image analysis in microscopy</title>
      <link>http://arxiv.org/abs/2502.18522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;分析成像和高光谱数据在生物学、医学、化学和物理学等领域中至关重要。目标是将高质量或高维的数据转换为可解释的格式，以便生成有价值的见解。&lt;h4&gt;目的&lt;/h4&gt;研究如何优化成像及高光谱数据分析流程，以减少对人工输入的需求，并提升自动化水平和决策支持能力。&lt;h4&gt;方法&lt;/h4&gt;讨论了基于奖励的工作流程的发展，这些工作流程采用了专家决策原则，并展示了强大的跨任务迁移学习。这种方法代表图像分析为一系列可能的操作上的决策过程。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入基于奖励的框架，可以实现从监督式、黑盒模型向解释性更强、无监督且鲁棒性强的优化方法转变。&lt;h4&gt;结论&lt;/h4&gt;这些工作流程既可作为经典和深度卷积神经网络（DCNN）方法上的包装器使用，又可在无监督和有监督的工作流中发挥作用，适用于各种图像分析和高光谱数据任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容关于成像及高光谱数据分析领域的研究进展，强调了引入机器学习加速特定任务的重要性，并探讨了未来通过奖励驱动工作流程优化此类任务的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analyzing imaging and hyperspectral data is crucial across scientific fields,including biology, medicine, chemistry, and physics. The primary goal is totransform high-resolution or high-dimensional data into an interpretable formatto generate actionable insights, aiding decision-making and advancingknowledge. Currently, this task relies on complex, human-designed workflowscomprising iterative steps such as denoising, spatial sampling, keypointdetection, feature generation, clustering, dimensionality reduction, andphysics-based deconvolutions. The introduction of machine learning over thepast decade has accelerated tasks like image segmentation and object detectionvia supervised learning, and dimensionality reduction via unsupervised methods.However, both classical and NN-based approaches still require human input,whether for hyperparameter tuning, data labeling, or both. The growing use ofautomated imaging tools, from atomically resolved imaging to biologicalapplications, demands unsupervised methods that optimize data representationfor human decision-making or autonomous experimentation. Here, we discussadvances in reward-based workflows, which adopt expert decision-makingprinciples and demonstrate strong transfer learning across diverse tasks. Werepresent image analysis as a decision-making process over possible operationsand identify desiderata and their mappings to classical decision-makingframeworks. Reward-driven workflows enable a shift from supervised, black-boxmodels sensitive to distribution shifts to explainable, unsupervised, androbust optimization in image analysis. They can function as wrappers overclassical and DCNN-based methods, making them applicable to both unsupervisedand supervised workflows (e.g., classification, regression forstructure-property mapping) across imaging and hyperspectral data.</description>
      <author>example@mail.com (Kamyar Barakati, Yu Liu, Utkarsh Pratiush, Boris N. Slautin, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2502.18522v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations</title>
      <link>http://arxiv.org/abs/2502.08279v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;视频到文本的总结，特别是在科学领域中的应用。&lt;h4&gt;背景信息&lt;/h4&gt;将录制的视频转换为简洁准确的文字概述是多模态学习中日益增长的一项挑战。&lt;h4&gt;目的声明&lt;/h4&gt;介绍VISTA数据集，该数据集专门用于科学研究领域的视频至文字概要生成。&lt;h4&gt;研究方法&lt;/h4&gt;包括两个方面：一是收集并整理18,599个AI会议演讲记录及其对应的论文摘要；二是评估最先进的大型模型，并应用基于规划的框架以更好地捕捉摘要结构特性。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，显式计划可以增强总结质量和事实一致性。然而，模型与人类表现之间仍存在显著差距。&lt;h4&gt;结论陈述&lt;/h4&gt;尽管现有方法取得了一定的进步，但科学视频概要生成领域依旧面临诸多挑战。&lt;h4&gt;翻译&lt;/h4&gt;将记录的视频转换成简洁准确的文字摘要在多模态学习中是一个日益增长的难题。本文介绍了一个专门用于科学研究领域的视频到文本总结的数据集VISTA，该数据集中包含了18,599个AI会议演讲及其对应的论文摘要。我们评估了最先进的大型模型，并应用了一种基于规划的方法来更好地捕捉摘要的结构特性。无论是人工还是自动评估都证实，明确的计划能够提高概要质量和事实一致性。然而，模型与人类表现之间仍存在显著差距，这突显了科学视频总结面临的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming recorded videos into concise and accurate textual summaries is agrowing challenge in multimodal learning. This paper introduces VISTA, adataset specifically designed for video-to-text summarization in scientificdomains. VISTA contains 18,599 recorded AI conference presentations paired withtheir corresponding paper abstracts. We benchmark the performance ofstate-of-the-art large models and apply a plan-based framework to bettercapture the structured nature of abstracts. Both human and automatedevaluations confirm that explicit planning enhances summary quality and factualconsistency. However, a considerable gap remains between models and humanperformance, highlighting the challenges of scientific video summarization.</description>
      <author>example@mail.com (Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg)</author>
      <guid isPermaLink="false">2502.08279v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Convolution Architecture in the Realm of DNA Foundation Models</title>
      <link>http://arxiv.org/abs/2502.18538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，基于Transformer和状态空间模型(SSM)的DNA语言模型取得了进展。然而，在基础模型基准测试中没有对这些新方法与经典卷积神经网络(CNN)进行比较的研究。&lt;h4&gt;背景&lt;/h4&gt;虽然最近提出了许多基于Transformer和SSM架构的方法来改进DNA语言模型，但在一些关键指标上缺乏与传统CNN的对比分析。&lt;h4&gt;目的&lt;/h4&gt;探讨并设计一种新型基于CNN的方法（ConvNova），以评估其在各种基础模型基准测试中的表现，并回答卷积网络是否已被这些新的方法超越的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了一种称为ConvNova的新方法，该方法采用了扩增卷积、门控卷积以及用于门控机制的双分支框架三种有效设计。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实证实验显示，在超过一半的任务上，ConvNova的表现优于最近的方法。特别是在组蛋白相关任务中，ConvNova比第二好的方法高出平均5.8%，同时使用更少的参数并实现更快的计算速度。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明CNN仍然是与Transformer和SSM架构相比具有竞争力的选择。这可能激发对基于CNN方法在DNA基础模型中的重新关注。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容是关于开发了一种新的名为ConvNova的方法，通过实验展示了其优越性，并探讨了卷积网络是否被新架构超越的问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, a variety of methods based on Transformer and state spacemodel (SSM) architectures have been proposed, advancing foundational DNAlanguage models. However, there is a lack of comparison between these recentapproaches and the classical architecture convolutional networks (CNNs) onfoundation model benchmarks. This raises the question: are CNNs truly beingsurpassed by these recent approaches based on transformer and SSMarchitectures? In this paper, we develop a simple but well-designed CNN-basedmethod termed ConvNova. ConvNova identifies and proposes three effectivedesigns: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branchframework for gating mechanisms. Through extensive empirical experiments, wedemonstrate that ConvNova significantly outperforms recent methods on more thanhalf of the tasks across several foundation model benchmarks. For example, inhistone-related tasks, ConvNova exceeds the second-best method by an average of5.8%, while generally utilizing fewer parameters and enabling fastercomputation. In addition, the experiments observed findings that may be relatedto biological characteristics. This indicates that CNNs are still a strongcompetitor compared to Transformers and SSMs. We anticipate that this work willspark renewed interest in CNN-based methods for DNA foundation models.</description>
      <author>example@mail.com (Yu Bo, Weian Mao, Yanjun Shao, Weiqiang Bai, Peng Ye, Xinzhu Ma, Junbo Zhao, Hao Chen, Chunhua Shen)</author>
      <guid isPermaLink="false">2502.18538v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17612v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  correcting contact information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了在没有中央控制的情况下，通过自组织方式优化集体目标的代理调度问题，并特别关注如何利用图神经网络（GNN）架构来提升分布式群组协同能力。&lt;h4&gt;背景&lt;/h4&gt;无中心化控制的代理协调对于诸如自主车队管理和传感器网络中的监控侦察等应用至关重要。分散式控制器的设计受到自然界中自我组织现象，特别是鸟类群体行为的启发，但现有的分散式控制器在保持群体凝聚力方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用群组动态中存在的对称性的图神经网络架构，以提高分布式控制器的一般化性能和效率。&lt;h4&gt;方法&lt;/h4&gt;通过在分散式的鸟群控制GNN控制器中强制执行旋转等变性和平移不变性对称性来改进现有的GNN控制器。&lt;h4&gt;主要发现&lt;/h4&gt;与不考虑上述对称性的现有GNN控制器相比，我们的对称感知控制器可以使用更少的训练数据和更少的可调权重实现类似的效果，并且在泛化能力方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;本文提出的旋转等变性和平移不变性策略改进了分散式群组控制系统的性能，证明了这种新方法的有效性。相关代码和动画可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;代理的协调以优化集体目标而没有中央控制系统是具有挑战性的，但在像自主车队管理和使用传感器网络进行监控侦察等应用中至关重要。分散控制器的设计受到了自然界自组织现象，尤其是鸟类群体行为的启发，但是现有的分散式控制器在维持群组凝聚力方面存在困难。图神经网络（GNN）架构已经成为了开发能够保持群组凝聚力的分散化控制系统的不可或缺的机器学习工具，然而它们未能利用存在于群组动态中的对称性，从而限制了其泛化能力。我们强制执行旋转等变性和平移不变性的对称性以改进分散式鸟群GNN控制器，并且在使用70%更少的训练数据和75%更少的可调权重的同时达到了与没有这些对称性的现有GNN控制器相同的控制效果。此外，我们的对称感知控制器比现有的GNN控制器有更好的泛化能力。相关代码和动画可在GitHub上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The orchestration of agents to optimize a collective objective withoutcentralized control is challenging yet crucial for applications such ascontrolling autonomous fleets, and surveillance and reconnaissance using sensornetworks. Decentralized controller design has been inspired byself-organization found in nature, with a prominent source of inspiration beingflocking; however, decentralized controllers struggle to maintain flockcohesion. The graph neural network (GNN) architecture has emerged as anindispensable machine learning tool for developing decentralized controllerscapable of maintaining flock cohesion, but they fail to exploit the symmetriespresent in flocking dynamics, hindering their generalizability. We enforcerotation equivariance and translation invariance symmetries in decentralizedflocking GNN controllers and achieve comparable flocking control with 70% lesstraining data and 75% fewer trainable weights than existing GNN controllerswithout these symmetries enforced. We also show that our symmetry-awarecontroller generalizes better than existing GNN controllers. Code andanimations are available athttp://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.</description>
      <author>example@mail.com (Taos Transue, Bao Wang)</author>
      <guid isPermaLink="false">2502.17612v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2410.22489v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的多模态少样本点云分割(MM-FSS)框架，通过结合文本标签和潜在可用的2D图像模式来增强传统单一模态的点云输入。MM-FSS使用一个共享的骨干网络、两个头部以及预训练的文本编码器，有效利用了多个模态之间的互补信息。&lt;h4&gt;背景&lt;/h4&gt;传统的少样本3D点云分割(FS-PCS)方法主要关注于单一模态的点云输入，而忽略了多模态信息可能带来的潜在优势。现有的FS-PCS技术在处理新型类别的分类时需要最少的支持样本来泛化模型。&lt;h4&gt;目的&lt;/h4&gt;旨在通过引入利用文本标签和2D图像模式的多模态少样本点云分割框架来改进现有方法，并展示结合这些自由可得的额外模态信息可以显著提升性能。&lt;h4&gt;方法&lt;/h4&gt;MM-FSS采用了共享骨干网络与两个头部相结合的方式，以提取跨模态和单模态视觉特征；同时利用预训练文本编码器生成文本嵌入。为了充分挖掘多模态数据中的信息，提出了一个多模态相关融合(MCF)模块来产生多模态关联，并且设计了一个多模态语义融合(MSF)模块来使用基于文本的语义指导细化这些关联。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在S3DIS和ScanNet数据集上，提出的MM-FSS框架相较于传统方法在性能上有显著提升。具体而言，该方法通过引入测试时间自适应跨模态校准(TACC)技术来缓解训练偏差，进一步提高泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为少样本3D点云分割提供了新的解决方案，并证明了多模态信息的有效性，这为进一步的研究工作提供了有价值的见解。该研究强调了利用通常被忽视的自由模式进行FS-PCS的潜在优势。&lt;h4&gt;翻译&lt;/h4&gt;Few-shot 3D点云分割(FS-PCS)的目标是通过少量注释支持样本使模型泛化以对新型类别进行分类。尽管现有的FS-PCS方法展示了一定的效果，但它们主要集中在单一模态的点云输入上，忽视了利用多模态信息可能带来的潜在好处。本文通过引入一种可以利用文本标签和潜在可用2D图像模式的多模态少样本分割设置来填补这一空白，并提出MultiModal Few-Shot SegNet (MM-FSS)模型，该模型能够有效利用多个来源的信息。实验结果在S3DIS和ScanNet数据集上表明了我们方法的有效性，验证了FS-PCS中结合额外可得模式的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models tosegment novel categories with minimal annotated support samples. While existingFS-PCS methods have shown promise, they primarily focus on unimodal point cloudinputs, overlooking the potential benefits of leveraging multimodalinformation. In this paper, we address this gap by introducing a multimodalFS-PCS setup, utilizing textual labels and the potentially available 2D imagemodality. Under this easy-to-achieve setup, we present the MultiModal Few-ShotSegNet (MM-FSS), a model effectively harnessing complementary information frommultiple modalities. MM-FSS employs a shared backbone with two heads to extractintermodal and unimodal visual features, and a pretrained text encoder togenerate text embeddings. To fully exploit the multimodal information, wepropose a Multimodal Correlation Fusion (MCF) module to generate multimodalcorrelations, and a Multimodal Semantic Fusion (MSF) module to refine thecorrelations using text-aware semantic guidance. Additionally, we propose asimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)technique to mitigate training bias, further improving generalization.Experimental results on S3DIS and ScanNet datasets demonstrate significantperformance improvements achieved by our method. The efficacy of our approachindicates the benefits of leveraging commonly-ignored free modalities forFS-PCS, providing valuable insights for future research. The code is availableat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot</description>
      <author>example@mail.com (Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie)</author>
      <guid isPermaLink="false">2410.22489v4</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2502.19417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通用型机器人系统，该系统能够处理复杂的指令和反馈，并在实际环境中执行多项任务。&lt;h4&gt;背景&lt;/h4&gt;现有的直接遵循简单命令的方法不足以应对需要复杂推理的任务场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用层次结构的视觉语言模型的机器人系统，以理解和执行复杂的任务指令及用户反馈。&lt;h4&gt;方法&lt;/h4&gt;该系统首先通过高层推理解释复杂提示和用户的反馈信息，并确定完成当前任务的最佳下一步；然后在低层控制下执行具体的物理动作。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了此系统可以在单臂、双臂以及移动式双臂机器人平台上演示出处理杂乱桌面清理、制作三明治等任务的能力。&lt;h4&gt;结论&lt;/h4&gt;该系统的应用证明其能够有效处理开放环境中的各种复杂指令和实时反馈，展现出广泛的潜在应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalist robots that can perform a range of different tasks in open-worldsettings must be able to not only reason about the steps needed to accomplishtheir goals, but also process complex instructions, prompts, and even feedbackduring task execution. Intricate instructions (e.g., "Could you make me avegetarian sandwich?" or "I don't like that one") require not just the abilityto physically perform the individual steps, but the ability to situate complexcommands and feedback in the physical world. In this work, we describe a systemthat uses vision-language models in a hierarchical structure, first reasoningover complex prompts and user feedback to deduce the most appropriate next stepto fulfill the task, and then performing that step with low-level actions. Incontrast to direct instruction following methods that can fulfill simplecommands ("pick up the cup"), our system can reason through complex prompts andincorporate situated feedback during task execution ("that's not trash"). Weevaluate our system across three robotic platforms, including single-arm,dual-arm, and dual-arm mobile robots, demonstrating its ability to handle taskssuch as cleaning messy tables, making sandwiches, and grocery shopping.</description>
      <author>example@mail.com (Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn)</author>
      <guid isPermaLink="false">2502.19417v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction</title>
      <link>http://arxiv.org/abs/2502.18697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 tables, 2 figures, Journal Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了H-FLTN框架，旨在解决电动汽车普及带来的充电时间预测、用户隐私保护和资源管理等问题。&lt;h4&gt;背景&lt;/h4&gt;电动汽车的广泛应用给能源供应商带来了挑战，包括准确预测充电时间、确保用户隐私以及高效地进行资源配置。&lt;h4&gt;目的&lt;/h4&gt;通过引入Hierarchical Federated Learning Transformer Network (H-FLTN) 框架来应对这些挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 采用三级层次架构（电动汽车、社区分布式能源资源管理系统和能源提供商数据中心）。            2. 使用基于Transformer的学习增强时间预测，捕获充电行为中的复杂依赖关系。            3. 利用安全聚合、加性秘密共享和点对点共享技术来确保隐私保护。            4. 引入动态客户端上限机制（DCCM）和客户端轮换管理（CRM），以提高训练效率和资源分配。&lt;h4&gt;主要发现&lt;/h4&gt;1. H-FLTN框架在大量实际车辆移动数据的模拟中表现出良好性能，特别是在减少随着电动汽车数量增加而产生的线性增长的培训时间复杂度至常量方面。            2. DCCM和CRM能够有效防止由于参与训练的客户端增多而导致计算负担过重的问题。&lt;h4&gt;结论&lt;/h4&gt;H-FLTN框架的实施可以增强智能城市的能源需求预测、资源分配和电网稳定性，确保未来移动生态系统的可靠性和可持续性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread adoption of Electric Vehicles (EVs) poses critical challengesfor energy providers, particularly in predicting charging time (temporalprediction), ensuring user privacy, and managing resources efficiently inmobility-driven networks. This paper introduces the Hierarchical FederatedLearning Transformer Network (H-FLTN) framework to address these challenges.H-FLTN employs a three-tier hierarchical architecture comprising EVs, communityDistributed Energy Resource Management Systems (DERMS), and the Energy ProviderData Centre (EPDC) to enable accurate spatio-temporal predictions of EVcharging needs while preserving privacy. Temporal prediction is enhanced usingTransformer-based learning, capturing complex dependencies in chargingbehavior. Privacy is ensured through Secure Aggregation, Additive SecretSharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow onlysecret shares of model weights to be exchanged while securing alltransmissions. To improve training efficiency and resource management, H-FLTNintegrates Dynamic Client Capping Mechanism (DCCM) and Client RotationManagement (CRM), ensuring that training remains both computationally andtemporally efficient as the number of participating EVs increases. DCCMoptimises client participation by limiting excessive computational loads, whileCRM balances training contributions across epochs, preventing imbalancedparticipation. Our simulation results based on large-scale empirical vehiclemobility data reveal that DCCM and CRM reduce the training time complexity withincreasing EVs from linear to constant. Its integration into real-world smartcity infrastructure enhances energy demand forecasting, resource allocation,and grid stability, ensuring reliability and sustainability in future mobilityecosystems.</description>
      <author>example@mail.com (Robert Marlin, Raja Jurdak, Alsharif Abuadbba)</author>
      <guid isPermaLink="false">2502.18697v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images</title>
      <link>http://arxiv.org/abs/2502.18932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Visual SLAM 对移动机器人、无人机导航和 VR/AR 非常重要，但传统的 RGB 相机系统在低光条件下表现不佳，促使人们关注热像 SLAM。然而，热成像面临低对比度、高噪声以及有限的大规模标注数据集等问题，限制了深度学习在户外场景中的应用。&lt;h4&gt;背景&lt;/h4&gt;传统RGB相机系统在低光条件下效果不佳，推动了对热像SLAM技术的研究需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于复杂光照条件下的大规模定位和重建的新型单目热像SLAM系统DarkSLAM。&lt;h4&gt;方法&lt;/h4&gt;{'ECA机制': '将Efficient Channel Attention（ECA）机制应用于视觉里程计，以提高姿态准确性。', 'SKA机制': '引入Selective Kernel Attention（SKA）机制进行深度估计，缓解了热像深度退化问题。', '闭环检测和姿态优化': '基于热像深度的闭环检测以及姿态优化，确保在低纹理热场景中的鲁棒性能'}&lt;h4&gt;主要发现&lt;/h4&gt;DarkSLAM 在户外实验中显著优于现有的SC-Sfm-Learner和Shin等人提出的方法，在严峻的夜间环境中也能实现精确定位和三维稠密地图构建。&lt;h4&gt;结论&lt;/h4&gt;DarkSLAM 提供了一种强大的解决方案，适用于在低光条件下的大规模场景中的实时导航和重建任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, buttraditional RGB camera systems struggle in low-light conditions, drivinginterest in thermal SLAM, which excels in such environments. However, thermalimaging faces challenges like low contrast, high noise, and limited large-scaleannotated datasets, restricting the use of deep learning in outdoor scenarios.We present DarkSLAM, a noval deep learning-based monocular thermal SLAM systemdesigned for large-scale localization and reconstruction in complex lightingconditions.Our approach incorporates the Efficient Channel Attention (ECA)mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanismin depth estimation to enhance pose accuracy and mitigate thermal depthdegradation. Additionally, the system includes thermal depth-based loop closuredetection and pose optimization, ensuring robust performance in low-texturethermal scenes. Extensive outdoor experiments demonstrate that DarkSLAMsignificantly outperforms existing methods like SC-Sfm-Learner and Shin et al.,delivering precise localization and 3D dense mapping even in challengingnighttime environments.</description>
      <author>example@mail.com (Yangfan Xu, Qu Hao, Lilian Zhang, Jun Mao, Xiaofeng He, Wenqi Wu, Changhao Chen)</author>
      <guid isPermaLink="false">2502.18932v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Emerging Practices in Participatory AI Design in Public Sector Innovation</title>
      <link>http://arxiv.org/abs/2502.18689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended Abstracts of the CHI Conference on Human Factors in  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;地方政府正在利用AI系统来改进公共服务的提供，同时确保技术采用过程中民主价值和社区信任得到维护。&lt;h4&gt;背景&lt;/h4&gt;地方和联邦机构正快速采纳AI系统以增强或自动化关键决策，并提高资源使用效率。这些系统在城市规划、安全监控、能源管理等方面发挥作用，影响公民获取基本服务的能力。&lt;h4&gt;目的&lt;/h4&gt;探讨参与式算法设计的方法，在公共服务领域中通过公众参与和社区互动来确定、设计、采用和实施算法。&lt;h4&gt;方法&lt;/h4&gt;需要重新评估传统的技术采纳方式，并为公共部门制定新的资源和方法，特别是在AI创新引入的新挑战下。&lt;h4&gt;主要发现&lt;/h4&gt;在智能城市倡议背景下，地方治理层必须确保民主价值的维持并建立社区信任。社区中心化及参与式的方式对于保证科技适当采用至关重要。&lt;h4&gt;结论&lt;/h4&gt;探索新兴的参与式算法设计实践是必要的，尤其是在公共部门，因为这要求更高的实施标准和更严格的形成方法。&lt;h4&gt;翻译&lt;/h4&gt;地方和联邦机构正在迅速采纳AI系统以增强或自动化关键决策，高效利用资源，并改善公共服务交付。这些系统被用于支持与城市规划、安全、监控、能源和关键基础设施相关的任务，以及影响公民获取基本服务能力的决策。地方政府作为最接近民众的治理层级，必须在智能城市的倡议中发挥重要作用，维护民主价值并建立社区信任。基于社区的方法对于确保技术适当采用至关重要；然而，在AI创新背景下，参与式设计方法需要更严格的规定和更高的实施标准，这比私营部门更具挑战性。因此，我们需要重新审视传统的做法，并为此开发新的资源和方法。本次研讨会将探讨公共部门算法的规划、设计、采用和实施中的新兴的参与式算法设计实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Local and federal agencies are rapidly adopting AI systems to augment orautomate critical decisions, efficiently use resources, and improve publicservice delivery. AI systems are being used to support tasks associated withurban planning, security, surveillance, energy and critical infrastructure, andsupport decisions that directly affect citizens and their ability to accessessential services. Local governments act as the governance tier closest tocitizens and must play a critical role in upholding democratic values andbuilding community trust especially as it relates to smart city initiativesthat seek to transform public services through the adoption of AI.Community-centered and participatory approaches have been central for ensuringthe appropriate adoption of technology; however, AI innovation introduces newchallenges in this context because participatory AI design methods require morerobust formulation and face higher standards for implementation in the publicsector compared to the private sector. This requires us to reassess traditionalmethods used in this space as well as develop new resources and methods. Thisworkshop will explore emerging practices in participatory algorithm design - orthe use of public participation and community engagement - in the scoping,design, adoption, and implementation of public sector algorithms.</description>
      <author>example@mail.com (Devansh Saxena, Zoe Kahn, Erina Seh-Young Moon, Lauren M. Chambers, Corey Jackson, Min Kyung Lee, Motahhare Eslami, Shion Guha, Sheena Erete, Lilly Irani, Deirdre Mulligan, John Zimmerman)</author>
      <guid isPermaLink="false">2502.18689v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2502.19409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code, dataset, and checkpoints are publicly available at  https://github.com/danaesavi/ImageChain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '对于多模态大型语言模型而言，基于图像序列的推理仍然是一个挑战。虽然最近的一些模型在预训练过程中引入了多张图片数据，但它们仍然难以识别序列结构，往往将图像视为独立的数据进行处理。', '目的': '介绍一种名为ImageChain的框架，旨在通过模拟视觉序列作为多轮对话来增强多模态大型语言模型的顺序推理能力。', '方法': '在ImageChain中，图片与相应的文本描述交错形成一个受控对话，明确捕捉时间依赖性和叙述进展。该方法优化了下一个场景描述任务，即基于先前的视觉和文本线索生成上下文感知的后续场景描述。', '主要发现': '实验表明，这种方法显著提高了下一场景描述任务的表现，平均改进幅度在SimRate这一衡量语义相似性的指标下为3.7%至19%。此外，在从漫画到机器人等各种应用中的零样本跨领域性能也表现出色。', '结论': '通过多模态、多轮对话的设计进行指令微调是弥合静态图像理解与时间感知推理之间差距的关键方法。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning over sequences of images remains a challenge for multimodal largelanguage models (MLLMs). While recent models incorporate multi-image dataduring pre-training, they still struggle to recognize sequential structures,often treating images independently. This work introduces ImageChain, aframework that enhances MLLMs with sequential reasoning capabilities over imagedata by modeling visual sequences as a multi-turn conversation. In ImageChain,images are interleaved with corresponding textual descriptions to form acontrolled dialogue that explicitly captures temporal dependencies andnarrative progression. Our method optimizes for the task of next-scenedescription, where the model generates a context-aware description of anupcoming scene based on preceding visual and textual cues. We demonstrate thatour approach improves performance on the next-scene description task --achieving an average improvement from 3.7% to 19% in SimRate, a metric thatquantifies semantic similarity to human-annotated ground truths. Moreover,ImageChain achieves robust zero-shot out-of-domain performance in applicationsranging from comics to robotics. Extensive experiments validate thatinstruction-tuning in a multimodal, multi-turn conversation design is key tobridging the gap between static image understanding and temporally-awarereasoning.</description>
      <author>example@mail.com (Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott)</author>
      <guid isPermaLink="false">2502.19409v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for Multi-Objective 3D Infrastructure Inspection with a UAV</title>
      <link>http://arxiv.org/abs/2502.19401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, submitted to IEEE Robotics and Automation Letters  (RA-L)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种适用于复杂3D环境下的无人机自主巡检任务的自适应风险感知和能量效率导航方法ARENA，该方法在多目标路径规划中实现了在线轨迹优化，并通过实际测试验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;目前的多目标路径规划方法难以应对定位误差、天气变化等不断演变的风险因素。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在复杂3D环境中实时适应风险和能量消耗，为无人机自主巡检任务提供可靠导航方案的方法。&lt;h4&gt;方法&lt;/h4&gt;利用4维NURBS表示形式及遗传算法生成帕累托前沿，通过创新性的风险感知投票算法确保自适应性。&lt;h4&gt;主要发现&lt;/h4&gt;ARENA框架能够在很大程度上优化轨迹多样性，并准确估计能量消耗。仿真和实际测试表明该规划器能够产生覆盖95%以上单一目标基准定义范围的多样化、最优化轨迹。&lt;h4&gt;结论&lt;/h4&gt;ARENA框架显著提高了无人机在关键且不断变化的3D任务中的自主性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人巡检任务需要平衡多个相互冲突的目标，同时导航时避免靠近昂贵障碍物。当前多目标路径规划方法难以适应如定位误差、天气情况、电池状态及通信问题等不断演变的风险。本文提出了一种适用于复杂3D环境下的无人机自适应风险感知和能量效率导航（ARENA）的多目标路径规划方案。此方法通过4维NURBS表示形式与基于遗传算法生成帕累托前沿，在线优化安全性、时间和能源，使用新颖的风险感知投票算法确保其自适应性。仿真及实际测试表明该规划器能够产生多样化且最优化轨迹，覆盖单目标基准定义范围的95%以上，并且具有良好的能量消耗估计能力，平均误差代表全功率范围的14%。ARENA框架增强了无人机在关键和不断变化3D任务中的自主性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic inspection missions require balancing multiple conflictingobjectives while navigating near costly obstacles. Current multi-objective pathplanning (MOPP) methods struggle to adapt to evolving risks like localizationerrors, weather, battery state, and communication issues. This letter presentsan Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approachfor UAVs in complex 3D environments. Our method enables online trajectoryadaptation by optimizing safety, time, and energy using 4D NURBS representationand a genetic-based algorithm to generate the Pareto front. A novel risk-awarevoting algorithm ensures adaptivity. Simulations and real-world testsdemonstrate the planner's ability to produce diverse, optimized trajectoriescovering 95% or more of the range defined by single-objective benchmarks andits ability to estimate power consumption with a mean error representing 14% ofthe full power range. The ARENA framework enhances UAV autonomy and reliabilityin critical, evolving 3D missions.</description>
      <author>example@mail.com (David-Alexandre Poissant, Alexis Lussier Desbiens, François Ferland, Louis Petit)</author>
      <guid isPermaLink="false">2502.19401v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Surface-Based Manipulation</title>
      <link>http://arxiv.org/abs/2502.19389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript is under revision for possible publication in the npj  Robotics. Copyright may be transferred to the publisher if the manuscript is  accepted for publication, without further notice. Supplementary video:  https://drive.google.com/drive/folders/1qbagK0VHi4DyfHGJ99ZlESX_i4rU_nmh?usp=sharing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表面的机器人操作策略，这种策略使用平面作为末端执行器来实现物体的精确操控。&lt;h4&gt;背景&lt;/h4&gt;在机器人研究中，与物理世界的交互是至关重要的。传统的方法主要依赖于类似手指形状的抓取装置，但这种方法难以稳定地抓取脆弱、可变形或非规则形状的物体。&lt;h4&gt;目的&lt;/h4&gt;探讨一种新的操作策略，以解决现有的基于指尖的传统抓取方法存在的问题，并且能够适应各种不同大小和刚度级别的物体。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用平面表面作为末端执行器的方法，通过改变这些平面的位置和方向，可以实现对物体的平移、旋转甚至翻转。这种方法不需要依赖稳定的抓取，而是依靠闭合回路控制策略来实现稳定操作。&lt;h4&gt;主要发现&lt;/h4&gt;这种基于表面的操作方法能够适应各种形状大小及硬度不同的物体，并且还可以操控可变形物体的形态。&lt;h4&gt;结论&lt;/h4&gt;这项研究成果为解决复杂的机器人操作问题提供了一个全新的视角。这种方法不仅可以应用于多种类型的物体，而且还扩展了我们对智能的理解：它不仅仅存在于大脑中，还体现在身体和与环境互动的方式之中。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligence lies not only in the brain but in the body. The shape of ourbodies can influence how we think and interact with the physical world. Inrobotics research, interacting with the physical world is crucial as it allowsrobots to manipulate objects in various real-life scenarios. Conventionalrobotic manipulation strategies mainly rely on finger-shaped end effectors.However, achieving stable grasps on fragile, deformable, irregularly shaped, orslippery objects is challenging due to difficulties in establishing stableforce or geometric constraints.  Here, we present surface-based manipulation strategies that diverge fromclassical grasping approaches, using with flat surfaces as minimalistend-effectors. By changing the position and orientation of these surfaces,objects can be translated, rotated and even flipped across the surface usingclosed-loop control strategies. Since this method does not rely on stablegrasp, it can adapt to objects of various shapes, sizes, and stiffness levels,even enabling the manipulation the shape of deformable objects. Our resultsprovide a new perspective for solving complex manipulation problems.</description>
      <author>example@mail.com (Ziqiao Wang, Serhat Demirtas, Fabio Zuliani, Jamie Paik)</author>
      <guid isPermaLink="false">2502.19389v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Robot Learning for Automatic Robot Motion Planning in Manufacturing</title>
      <link>http://arxiv.org/abs/2502.19340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 Pages, 11 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多级混合机器人运动规划方法，该方法结合了基于任务空间的强化学习和从演示中学习（RL-LfD）代理以及关节空间深度强化学习（DRL）代理。这种方法通过实现两个代理之间的切换，确保机器人的动作既可行又平滑。&lt;h4&gt;背景&lt;/h4&gt;工业机器人在各种制造环境中广泛应用，但如何使机器人能够自动规划适应不同任务的轨迹是一个重大挑战。特别是当机器人与其他设备、人类或其他机器人共同工作时，这一问题变得更加复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合运动规划方法，以解决工业机器人面临的自动化和灵活性方面的难题。&lt;h4&gt;方法&lt;/h4&gt;该研究通过结合基于任务空间的RL-LfD代理和基于关节空间的DRL代理来实现多级混合规划。较高层次的代理负责在两个较低层次的代理之间进行切换，并且这种切换策略考虑了机器人的可达性、关节极限、操作灵活性及碰撞风险等多重因素。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的混合运动规划方法能够生成满足任务约束条件并且可行性的轨迹。&lt;h4&gt;结论&lt;/h4&gt;通过仿真和实际场景验证了该方法的有效性和实用性。此研究为工业机器人在复杂多变的工作环境中自动规划路径提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要的中文直接翻译版本：工业机器人广泛应用于各种制造环境，但是如何使它们能够自动地计划出适应改变任务的动作轨迹是一个重要的挑战。当机器人在其工作单元内与其他机器、人类或其它机器人一起操作时，情况会变得更复杂。本文介绍了一种多级混合型机器人运动规划方法，该方法结合了基于任务空间的强化学习从演示中学习（RL-LfD）代理和关节空间的深度强化学习（DRL）代理的方法。一个更高的层级代理被用来在两个较低层级的代理之间进行切换以实现可行且平滑的动作。可行性通过将机器人在其环境中可达到性、关节极限、操作灵活性以及碰撞风险等因素综合考虑来进行计算。因此，由此方法产生的混合型运动规划策略生成出符合任务约束条件并且是可行性的轨迹。该方法的有效性在仿真中的机器人场景和实际应用场景中得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial robots are widely used in diverse manufacturing environments.Nonetheless, how to enable robots to automatically plan trajectories forchanging tasks presents a considerable challenge. Further complexities arisewhen robots operate within work cells alongside machines, humans, or otherrobots. This paper introduces a multi-level hybrid robot motion planning methodcombining a task space Reinforcement Learning-based Learning from Demonstration(RL-LfD) agent and a joint-space based Deep Reinforcement Learning (DRL) basedagent. A higher level agent learns to switch between the two agents to enablefeasible and smooth motion. The feasibility is computed by incorporatingreachability, joint limits, manipulability, and collision risks of the robot inthe given environment. Therefore, the derived hybrid motion planning policygenerates a feasible trajectory that adheres to task constraints. Theeffectiveness of the method is validated through sim ulated robotic scenariosand in a real-world setup.</description>
      <author>example@mail.com (Siddharth Singh, Tian Yu, Qing Chang, John Karigiannis, Shaopeng Liu)</author>
      <guid isPermaLink="false">2502.19340v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration</title>
      <link>http://arxiv.org/abs/2502.19250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page at https://objectvla.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于Vision-Language-Action (VLA)模型的方法ObjectVLA，该方法使机器人能够在未见过的新对象上泛化所学的技能。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在教授机器人的灵巧操作技巧方面非常有效，但通常依赖大量的人类演示数据，这限制了其在动态真实环境中的可扩展性和适用性。一个关键挑战是物体泛化能力的缺乏，即机器人难以将针对特定对象训练出的操作技能转移到语义相似但视觉不同的新对象上。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够实现物体级泛化的简单且有效的方法，并减少对大量人类演示的需求。&lt;h4&gt;方法&lt;/h4&gt;使用Vision-Language-Action (VLA)模型结合视觉和语言数据，使机器人能够在没有针对新目标物的具体演示的情况下泛化操作技能。通过利用预训练的模型并进行少量图像的微调来进一步增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在一个真实的机器人平台上测试时，ObjectVLA能够对100个从未见过的新对象以64%的成功率完成指定任务。&lt;h4&gt;结论&lt;/h4&gt;该方法有效支持了物体级别的泛化学习，并减少了需要大量人类演示的需求，为更加灵活和可扩展的机器人系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;模仿学习已被证明在教导机器人灵巧操作技能方面非常有效。然而，它通常依赖于大量的数据，这些数据来自人类的操作示范，这限制了其在动态现实环境中的应用范围和规模。在这种情况下的一个关键挑战是物体的泛化能力，即一个被训练来处理特定对象任务的机器人（例如“拿起苹果”）很难将所学技能转移到语义上相似但视觉上不同的新目标物上（如“拿起桃子”）。先前关于端到端视觉操作策略学习的研究尚未充分解决向这些类别之外的新物体泛化的问题。本文中，我们提出了一种简单而有效的方法ObjectVLA，通过Vision-Language-Action (VLA)模型实现物体的泛化能力。我们的方法使机器人可以在没有为每个新目标物提供明确的人类演示的情况下将学到的操作技能转移到新的对象上。通过结合视觉和语言对数据，我们的方法以一种轻量级且可扩展的方式注入关于目标对象的知识，并建立了该对象与预期操作之间的隐式联系。我们在真实机器人的平台验证了ObjectVLA的能力，展示其可以成功地在100个从训练中未见过的新型物体上进行泛化，在选择从未见过的目标物方面取得了64%的成功率。此外，我们提出了一种使用智能手机拍摄少量图像并微调预训练模型的方法来增强VLA模型中的对象泛化能力。这些结果突显了我们的方法在实现物体级别泛化和减少需要广泛人类演示需求方面的有效性，并为更灵活可扩展的机器人学习系统铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning has proven to be highly effective in teaching robotsdexterous manipulation skills. However, it typically relies on large amounts ofhuman demonstration data, which limits its scalability and applicability indynamic, real-world environments. One key challenge in this context is objectgeneralization, where a robot trained to perform a task with one object, suchas "hand over the apple," struggles to transfer its skills to a semanticallysimilar but visually different object, such as "hand over the peach." This gapin generalization to new objects beyond those in the same category has yet tobe adequately addressed in previous work on end-to-end visuomotor policylearning. In this paper, we present a simple yet effective approach forachieving object generalization through Vision-Language-Action (VLA) models,referred to as \textbf{ObjectVLA}. Our model enables robots to generalizelearned skills to novel objects without requiring explicit human demonstrationsfor each new target object. By leveraging vision-language pair data, our methodprovides a lightweight and scalable way to inject knowledge about the targetobject, establishing an implicit link between the object and the desiredaction. We evaluate ObjectVLA on a real robotic platform, demonstrating itsability to generalize across 100 novel objects with a 64\% success rate inselecting objects not seen during training. Furthermore, we propose a moreaccessible method for enhancing object generalization in VLA models, using asmartphone to capture a few images and fine-tune the pre-trained model. Theseresults highlight the effectiveness of our approach in enabling object-levelgeneralization and reducing the need for extensive human demonstrations, pavingthe way for more flexible and scalable robotic learning systems.</description>
      <author>example@mail.com (Minjie Zhu, Yichen Zhu, Jinming Li, Zhongyi Zhou, Junjie Wen, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, Feifei Feng)</author>
      <guid isPermaLink="false">2502.19250v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CPG-Based Manipulation with Multi-Module Origami Robot Surface</title>
      <link>http://arxiv.org/abs/2502.19218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript is under revision for possible publication in the  IEEE Robotics and Automation Letters (RA-L). Copyright may be transferred to  IEEE if the manuscript is accepted for publication, without further notice.  Supplementary video: https://youtu.be/AEmWFmHhPOA. Code available:  https://doi.org/10.5281/zenodo.14726303&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器人机械手在处理不同尺寸和材料的物体时面临挑战，特别是在操作大尺度或具有不同刚度的物体时更为明显。传统抓取技术和策略在这种情况下经常表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于表面的多模块机器人操控框架，以解决现有技术无法有效操作各种大小、形状及硬度物体的问题。&lt;h4&gt;方法&lt;/h4&gt;采用中央模式发生器（CPG）运动生成器与模拟优化法结合的方式确定用于多模块折纸机器人表面(Ori-Pixel)的最佳操控参数。&lt;h4&gt;主要发现&lt;/h4&gt;通过动态仿真和一系列原型实验，展示了这种新框架可以有效地处理从厘米到米级别的物体，并且能够适应不同大小、重量、形状及材质的物体。&lt;h4&gt;结论&lt;/h4&gt;优化后的CPG参数在广泛的测试中表现出强大的操控能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取器通常面临挑战，在处理各种尺寸和材料的物体时效率较低。特别是在操作大尺度或具有多变刚度的物品时，传统抓握技术和策略往往无效。本文介绍了一种新颖的基于表面的多模块机器人操纵框架，该框架使用中央模式发生器（CPG）运动生成器，并结合模拟优化法来确定用于折纸机器人表面对象的最优操作参数。这种方法能够处理从厘米到米级别的各种刚度和形状的对象。通过动态仿真及一系列原型实验测试了最佳的CPG参数，证明了该框架在多种不同大小、重量、形状以及材料物体上的稳健性操控能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulators often face challenges in handling objects of differentsizes and materials, limiting their effectiveness in practical applications.This issue is particularly pronounced when manipulating meter-scale objects orthose with varying stiffness, as traditional gripping techniques and strategiesfrequently prove inadequate. In this letter, we introduce a novel surface-basedmulti-module robotic manipulation framework that utilizes a Central PatternGenerator (CPG)-based motion generator, combined with a simulation-basedoptimization method to determine the optimal manipulation parameters for amulti-module origami robotic surface (Ori-Pixel). This approach allows for themanipulation of objects ranging from centimeters to meters in size, withvarying stiffness and shape. The optimized CPG parameters are tested throughboth dynamic simulations and a series of prototype experiments involving a widerange of objects differing in size, weight, shape, and material, demonstratingrobust manipulation capabilities.</description>
      <author>example@mail.com (Yuhao Jiang, Serge El Asmar, Ziqiao Wang, Serhat Demirtas, Jamie Paik)</author>
      <guid isPermaLink="false">2502.19218v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Embodying mechano-fluidic memory in soft machines to program behaviors upon interactions</title>
      <link>http://arxiv.org/abs/2502.19192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;软机器可以适应外部环境的变化，通过直接在结构中体现记忆能力来实现更加响应性的行为。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用弹性壳体的双稳态特性改变封闭腔内的流体性质，从而切换自振荡机器的稳定频率状态。&lt;h4&gt;方法&lt;/h4&gt;开发围绕双稳态壳体的流体电路，软管在外部触碰时会弯曲和恢复原状。通过这种方式实现了长期和短期记忆功能。&lt;h4&gt;主要发现&lt;/h4&gt;设计了可以响应人类用户交互并自主改变方向来避开障碍物（如墙壁）的软机器。&lt;h4&gt;结论&lt;/h4&gt;只利用几何形状和弹性特性，将记忆直接嵌入物理结构中可以让没有中央大脑的系统表现出自主行为，这通常是由基于计算机的机器人系统完成的任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经用中文进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft machines display shape adaptation to external circumstances due to theirintrinsic compliance. To achieve increasingly more responsive behaviors uponinteractions without relying on centralized computation, embodying memorydirectly in the machines' structure is crucial. Here, we harness thebistability of elastic shells to alter the fluidic properties of an enclosedcavity, thereby switching between stable frequency states of a locomotingself-oscillating machine. To program these memory states upon interactions, wedevelop fluidic circuits surrounding the bistable shell, with soft tubes thatkink and unkink when externally touched. We implement circuits for bothlong-term and short-term memory in a soft machine that switches behaviors inresponse to a human user and that autonomously changes direction afterdetecting a wall. By harnessing only geometry and elasticity, embodying memoryallows physical structures without a central brain to exhibit autonomous featsthat are typically reserved for computer-based robotic systems.</description>
      <author>example@mail.com (Alberto Comoretto, Tanaya Mandke, Johannes T. B. Overvelde)</author>
      <guid isPermaLink="false">2502.19192v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote Engagement in Urban Gardening</title>
      <link>http://arxiv.org/abs/2502.19171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PlantPal是一种支持城市居民进行园艺活动的系统，它利用精密农业机器人（PAR）来克服空间、时间和技能方面的限制。&lt;h4&gt;背景&lt;/h4&gt;城市园艺在健康和环保方面有许多好处。然而，缺乏合适的花园空间、忙碌的日程安排以及有限的园艺知识是阻碍人们参与城市园艺的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;研究并开发PlantPal系统以解决当前智能家庭解决方案未能充分应对的实际问题，使用户能够在任何地点进行园艺活动，不受专业知识和时间限制的影响。&lt;h4&gt;方法&lt;/h4&gt;PlantPal利用一个配备了多种工具和多摄像头系统的精密农业机器人（PAR），通过远程操作帮助人们在日常生活中整合园艺任务。&lt;h4&gt;主要发现&lt;/h4&gt;为期三周的实验表明，PlantPal有助于将园艺任务融入日常生活，增强用户与自己田地的情感连接，并提供一种即使在远程环境下也能保持吸引力的体验。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一些未来机器人辅助城市园艺概念的设计考虑事项。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban gardening is widely recognized for its numerous health andenvironmental benefits. However, the lack of suitable garden spaces, demandingdaily schedules and limited gardening expertise present major roadblocks forcitizens looking to engage in urban gardening. While prior research hasexplored smart home solutions to support urban gardeners, these approachescurrently do not fully address these practical barriers. In this paper, wepresent PlantPal, a system that enables the cultivation of garden spacesirrespective of one's location, expertise level, or time constraints. PlantPalenables the shared operation of a precision agriculture robot (PAR) that isequipped with garden tools and a multi-camera system. Insights from a 3-weekdeployment (N=18) indicate that PlantPal facilitated the integration ofgardening tasks into daily routines, fostered a sense of connection with one'sfield, and provided an engaging experience despite the remote setting. Wecontribute design considerations for future robot-assisted urban gardeningconcepts.</description>
      <author>example@mail.com (Albin Zeqiri, Julian Britten, Clara Schramm, Pascal Jansen, Michael Rietzler, Enrico Rukzio)</author>
      <guid isPermaLink="false">2502.19171v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management</title>
      <link>http://arxiv.org/abs/2502.19135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将大型语言模型与基于Prolog的知识管理和规划集成到多机器人任务中的框架PLANTOR。&lt;h4&gt;背景&lt;/h4&gt;当前研究中，多机器人系统需要处理复杂的知识表示和时间、资源等约束条件。现有的解决方案可能难以扩展或解释。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够结合大型语言模型生成准确的知识库，并利用Prolog确保形式正确性和可解释性的框架。&lt;h4&gt;方法&lt;/h4&gt;{'PLANTOR框架': '采用两阶段的机器人专用知识库生成过程，以保证重用性与组合推理能力；制定三步规划程序处理时间依赖、资源限制及并行任务执行问题。', '计划转换': '最终计划通过混合整数线性编程确定后，被转化为行为树用于直接在ROS2中使用。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'知识库生成': '大型语言模型可以在少量的人工反馈下产生准确的知识库；', '形式正确性和可解释性': 'Prolog保证了系统的正式验证和透明度。', '应用效果': 'PLANTOR框架在构建块世界多机器人装配任务与拱形建筑场景中表现出色。'}&lt;h4&gt;结论&lt;/h4&gt;大型语言模型与规划系统结合的集成方法对于需要灵活、可扩展且人类易于理解计划的任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要全文为英文，以上内容为其中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel framework, called PLANTOR (PLanning with Naturallanguage for Task-Oriented Robots), that integrates Large Language Models(LLMs) with Prolog-based knowledge management and planning for multi-robottasks. The system employs a two-phase generation of a robot-oriented knowledgebase, ensuring reusability and compositional reasoning, as well as a three-stepplanning procedure that handles temporal dependencies, resource constraints,and parallel task execution via mixed-integer linear programming. The finalplan is converted into a Behaviour Tree for direct use in ROS2. We tested theframework in multi-robot assembly tasks within a block world and anarch-building scenario. Results demonstrate that LLMs can produce accurateknowledge bases with modest human feedback, while Prolog guarantees formalcorrectness and explainability. This approach underscores the potential of LLMintegration for advanced robotics tasks requiring flexible, scalable, andhuman-understandable planning.</description>
      <author>example@mail.com (Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri)</author>
      <guid isPermaLink="false">2502.19135v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments</title>
      <link>http://arxiv.org/abs/2502.19024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Ground-level Viewpoint Navigation (GVNav)方法，旨在解决视觉语言导航(VLN)中由于观察视角高度不同而导致的人类指令与低视点机器人之间的不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;VLN任务面临的主要挑战之一是缺乏在真实场景中的泛化能力，尤其是在处理视野受限的四足机器人的指令跟随时。当前的方法往往未能充分考虑不同视觉高度带来的感知差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决低视点机器人执行人类指导导航任务时遇到的独特问题，并展示通过使用加权历史观测数据和转移连通性图可以提高模型在模拟环境和真实世界部署中的性能。&lt;h4&gt;方法&lt;/h4&gt;GVNav利用加权的历史观察作为增强的时空上下文，以帮助机器人处理由于视觉障碍或感知不匹配导致的问题。此外，该工作还引入了HM3D和Gibson数据集的连通性图作为额外资源来提高空间先验知识并更好地表示现实世界的场景。&lt;h4&gt;主要发现&lt;/h4&gt;GVNav方法显著提升了在模拟环境以及使用四足机器人进行真实世界部署时的表现。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了视点高度变化对VLN任务性能的影响，并为低视点导航提出了创新性解决方案，这有望改善未来机器人的感知和决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation (VLN) empowers agents to associatetime-sequenced visual observations with corresponding instructions to makesequential decisions. However, generalization remains a persistent challenge,particularly when dealing with visually diverse scenes or transitioning fromsimulated environments to real-world deployment. In this paper, we address themismatch between human-centric instructions and quadruped robots with alow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)approach to mitigate this issue. This work represents the first attempt tohighlight the generalization gap in VLN across varying heights of visualobservation in realistic robot deployments. Our approach leverages weightedhistorical observations as enriched spatiotemporal contexts for instructionfollowing, effectively managing feature collisions within cells by assigningappropriate weights to identical features across different viewpoints. Thisenables low-height robots to overcome challenges such as visual obstructionsand perceptual mismatches. Additionally, we transfer the connectivity graphfrom the HM3D and Gibson datasets as an extra resource to enhance spatialpriors and a more comprehensive representation of real-world scenarios, leadingto improved performance and generalizability of the waypoint predictor inreal-world environments. Extensive experiments demonstrate that ourGround-level Viewpoint Navigation (GVnav) approach significantly improvesperformance in both simulated environments and real-world deployments withquadruped robots.</description>
      <author>example@mail.com (Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu)</author>
      <guid isPermaLink="false">2502.19024v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Reliable, Time-Predictable Heterogeneous SoC for AI-Enhanced Mixed-Criticality Edge Applications</title>
      <link>http://arxiv.org/abs/2502.18953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;下一代混合关键性的片上系统（SoCs）用于机器人、汽车和空间领域，需要执行增强型的传感器处理和控制工作负载，并确保在与非关键任务共享资源的同时可靠且时间可预测地运行关键任务。这些SoC必须适应小于2W功率包络。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述多维度挑战，在本文中提出了一种16nm、可靠的、时间可预测的异构SoC，该SoC集成了多个可编程加速器。&lt;h4&gt;方法&lt;/h4&gt;为了确保共享资源（如片上互连和内存系统）上的可预测访问，SoC集成了软件配置硬件IP，并在小于1.2W功率包络内工作。通过这种方式建立了关键应用执行时间的紧上界。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出了一种可靠的多核加速器来加快混合精度任务的关键AI处理，峰值性能达到304.9 GOPS，在能源效率方面达到1.6 TOPS/W。对于非关键、计算密集型浮点工作负载，则由双核向量集群加速，并达到了121.8 GFLOPS的性能以及1.1 TFLOPS/W和106.8 GFLOPS/mm²的能效。&lt;h4&gt;结论&lt;/h4&gt;提出的设计可以有效应对新一代混合关键性的SoC设计挑战，同时满足严格的能源效率要求。&lt;h4&gt;翻译&lt;/h4&gt;下一代用于机器人、汽车和空间领域的混合关键性系统级芯片（SoCs）需要执行增强型传感器处理和控制工作负载，并确保在与非关键任务共享资源的同时可靠且时间可预测地运行关键任务。为了应对这些多维挑战，本文提出了一种16nm的可靠的、时间可预测的异构SoC，该SoC集成了多个可编程加速器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation mixed-criticality Systems-on-chip (SoCs) for robotics,automotive, and space must execute mixed-criticality AI-enhanced sensorprocessing and control workloads, ensuring reliable and time-predictableexecution of critical tasks sharing resources with non-critical tasks, whilealso fitting within a sub-2W power envelope. To tackle these multi-dimensionalchallenges, in this brief, we present a 16nm, reliable, time-predictableheterogeneous SoC with multiple programmable accelerators. Within a 1.2W powerenvelope, the SoC integrates software-configurable hardware IPs to ensurepredictable access to shared resources, such as the on-chip interconnect andmemory system, leading to tight upper bounds on execution times of criticalapplications. To accelerate mixed-precision mission-critical AI, the SoCintegrates a reliable multi-core accelerator achieving 304.9 GOPS peakperformance at 1.6 TOPS/W energy efficiency. Non-critical, compute-intensive,floating-point workloads are accelerated by a dual-core vector cluster,achieving 121.8 GFLOPS at 1.1 TFLOPS/W and 106.8 GFLOPS/mm2.</description>
      <author>example@mail.com (Angelo Garofalo, Alessandro Ottaviano, Matteo Perotti, Thomas Benz, Yvan Tortorella, Robert Balas, Michael Rogenmoser, Chi Zhang, Luca Bertaccini, Nils Wistoff, Maicol Ciani, Cyril Koenig, Mattia Sinigaglia, Luca Valente, Paul Scheffler, Manuel Eggimann, Matheus Cavalcante, Francesco Restuccia, Alessandro Biondi, Francesco Conti, Frank K. Gurkaynak, Davide Rossi, Luca Benini)</author>
      <guid isPermaLink="false">2502.18953v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Think on your feet: Seamless Transition between Human-like Locomotion in Response to Changing Commands</title>
      <link>http://arxiv.org/abs/2502.18901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 10 figures, accepted at the 2025 IEEE International  Conference on Robotics and Automation (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新型的人类机器人运动学习方法，通过改进经典模仿学习技术，实现了更自然、更具适应性的步行行为。&lt;h4&gt;背景&lt;/h4&gt;人形机器人的训练相对容易进行特定行走技能的模拟，但难以从各种动作中学习并适应不断变化的指令。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以精准追踪运动指令、在不同动作间无缝过渡且能够掌握参考数据之外中间状态的人形机器人系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用Wasserstein散度标准（WGAN-div）提高泛化能力；2. 采用混合内部模型提供结构化的隐藏状态和速度估计，增强移动稳定性及环境适应性；3. 引入好奇心奖励机制促进探索。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能够实现高度人类相似的步行行为，并且具有对不同速度需求的适应性、直接适用于未见动作的任务以及在模拟器与真实世界中跨地形的零样本转移能力。&lt;h4&gt;结论&lt;/h4&gt;通过各种机器人模型的仿真和广泛的真实世界实验验证了这些改进的有效性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While it is relatively easier to train humanoid robots to mimic specificlocomotion skills, it is more challenging to learn from various motions andadhere to continuously changing commands. These robots must accurately trackmotion instructions, seamlessly transition between a variety of movements, andmaster intermediate motions not present in their reference data. In this work,we propose a novel approach that integrates human-like motion transfer withprecise velocity tracking by a series of improvements to classical imitationlearning. To enhance generalization, we employ the Wasserstein divergencecriterion (WGAN-div). Furthermore, a Hybrid Internal Model provides structuredestimates of hidden states and velocity to enhance mobile stability andenvironment adaptability, while a curiosity bonus fosters exploration. Ourcomprehensive method promises highly human-like locomotion that adapts tovarying velocity requirements, direct generalization to unseen motions andmultitasking, as well as zero-shot transfer to the simulator and the real worldacross different terrains. These advancements are validated through simulationsacross various robot models and extensive real-world experiments.</description>
      <author>example@mail.com (Huaxing Huang, Wenhao Cui, Tonghe Zhang, Shengtao Li, Jinchao Han, Bangyu Qin, Tianchu Zhang, Liang Zheng, Ziyang Tang, Chenxu Hu, Ning Yan, Jiahao Chen, Shipu Zhang, Zheyuan Jiang)</author>
      <guid isPermaLink="false">2502.18901v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Online Task Assignment via Inexact ADMM for unplanned online tasks and its Applications to Security</title>
      <link>http://arxiv.org/abs/2502.18893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TCNS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多机器人系统（MRS）中任务分配对于协调代理和确保使命成功以及保持整体系统安全性至关重要。本文提出了一种基于优化的分布式任务分配算法，该算法能够动态地为团队分配关键安全性和可选任务。&lt;h4&gt;背景&lt;/h4&gt;在多机器人系统的应用中，有效的任务分配不仅是为了协调代理并确保任务的成功完成，还为了维护整个系统的安全性。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以在计划偏离攻击下保持在线处理任务能力的全面框架，并确保MRS能够在不降低安全性的前提下有效地应对未预见的任务。&lt;h4&gt;方法&lt;/h4&gt;{'优化算法': '提出了一种基于优化的分布式任务分配算法，通过近似交替方向乘子法（ADMM）的方法将任务分配问题分解为可分离和不可分离的子问题，并使用投影梯度下降法处理不可分离的子问题', '安全分析框架': '制定一个全面的框架来评估在线任务的安全执行性以及机器人重新加入团队的时间和地点', '控制方法': '采用基于控制Lyapunov函数（CLF）的任务履行管理和基于控制障碍函数（CBF）的安全过滤器，以确保任务完成时的安全保障'}&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验展示了所提出的框架能够使MRS有效地响应未计划的在线任务同时保持安全保证。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法和框架证明了在维护安全性的同时可以增强多机器人系统对意外任务处理的能力，具有重要的理论和实践意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In multi-robot system (MRS) applications, efficient task assignment isessential not only for coordinating agents and ensuring mission success butalso for maintaining overall system security. In this work, we first propose anoptimization-based distributed task assignment algorithm that dynamicallyassigns mandatory security-critical tasks and optional tasks among teams.Leveraging an inexact Alternating Direction Method of Multipliers (ADMM)-basedapproach, we decompose the task assignment problem into separable andnon-separable subproblems. The non-separable subproblems are transformed intoan inexact ADMM update by projected gradient descent, which can be performedthrough several communication steps within the team.  In the second part of this paper, we formulate a comprehensive framework thatenables MRS under plan-deviation attacks to handle online tasks withoutcompromising security. The process begins with a security analysis thatdetermines whether an online task can be executed securely by a robot and, ifso, the required time and location for the robot to rejoin the team. Next, theproposed task assignment algorithm is used to allocate security-related tasksand verified online tasks. Finally, task fulfillment is managed using a ControlLyapunov Function (CLF)-based controller, while security enforcement is ensuredthrough a Control Barrier Function (CBF)-based security filter. Throughsimulations, we demonstrate that the proposed framework allows MRS toeffectively respond to unplanned online tasks while maintaining securityguarantees.</description>
      <author>example@mail.com (Ziqi Yang, Roberto Tron)</author>
      <guid isPermaLink="false">2502.18893v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>RL-OGM-Parking: Lidar OGM-Based Hybrid Reinforcement Learning Planner for Autonomous Parking</title>
      <link>http://arxiv.org/abs/2502.18846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶技术中的自主泊车研究提出了一种结合规则和学习的方法，通过使用混合策略（包括基于规则的Reeds-Shepp (RS) 规划器和基于学习的强化学习(RL)规划器）来解决传统方法在多变环境下的适应性问题。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶泊车技术面临空间有限且复杂环境的问题，传统的规则基础算法难以应对各种不可预测条件，而学习型算法在不同场景中表现不一致。因此需要一种结合两者优点的混合策略。&lt;h4&gt;目的&lt;/h4&gt;为了提高自主停车系统在现实世界中的适应性和效率，本研究提出了一种新的方法来解决模拟到实际环境转换时存在的差距问题。&lt;h4&gt;方法&lt;/h4&gt;采用一个由基于规则的Reeds-Shepp (RS) 规划器和基于学习的强化学习(RL) 规划器组成的混合策略，并使用实时LiDAR占用网格图（OGM）表示来缩小模拟与现实之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在仿真环境和真实世界场景中都优于纯规则基础方法和学习型方法。实际测试进一步验证了该方法的可行性和效率。&lt;h4&gt;结论&lt;/h4&gt;混合策略通过结合规则和学习的优点，在自主泊车技术的实际应用中展现出了良好的性能和适应性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶泊车研究提出了一种新颖的方法来解决传统算法在复杂环境中的局限性，利用基于规则的Reeds-Shepp (RS) 和基于强化学习(RL) 的混合策略，并通过实时LiDAR占用网格图(OGM) 表示成功解决了从模拟到实际转换的问题。实验结果显示了该方法相较于单纯使用规则或学习方式在性能上的显著提升，同时确保了其在真实环境中的有效性与高效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous parking has become a critical application in automatic drivingresearch and development. Parking operations often suffer from limited spaceand complex environments, requiring accurate perception and precisemaneuvering. Traditional rule-based parking algorithms struggle to adapt todiverse and unpredictable conditions, while learning-based algorithms lackconsistent and stable performance in various scenarios. Therefore, a hybridapproach is necessary that combines the stability of rule-based methods and thegeneralizability of learning-based methods. Recently, reinforcement learning(RL) based policy has shown robust capability in planning tasks. However, thesimulation-to-reality (sim-to-real) transfer gap seriously blocks thereal-world deployment. To address these problems, we employ a hybrid policy,consisting of a rule-based Reeds-Shepp (RS) planner and a learning-basedreinforcement learning (RL) planner. A real-time LiDAR-based Occupancy Grid Map(OGM) representation is adopted to bridge the sim-to-real gap, leading thehybrid policy can be applied to real-world systems seamlessly. We conductedextensive experiments both in the simulation environment and real-worldscenarios, and the result demonstrates that the proposed method outperformspure rule-based and learning-based methods. The real-world experiment furthervalidates the feasibility and efficiency of the proposed method.</description>
      <author>example@mail.com (Zhitao Wang, Zhe Chen, Mingyang Jiang, Tong Qin, Ming Yang)</author>
      <guid isPermaLink="false">2502.18846v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2502.18842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 IEEE/SICE International Symposium on System Integration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的流水线，旨在通过集成CLIP和SAM模型来提高机器人在便利商店产品掩码操作中的精度。&lt;h4&gt;背景&lt;/h4&gt;现有的技术在识别和处理特定环境下的对象时存在局限性，特别是在需要精确对象掩码的场合，如便利店的商品处理。现有模型（例如CLIP和SAM）虽然各自有效，但它们之间的协同作用尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;目的是开发一种新的方法来提高基于图像和文本数据集的机器人操作精度，并利用这些改进后的技术来实现更精确、适应性强的产品操纵。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一个结合了CLIP（用于理解自然语言指令）和SAM（用于分割对象掩码）的集成框架，通过多模态数据处理优化模型性能。此外，还使用了基于梯度的方法以及定制的数据集进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;提出的流水线通过有效的组合现有技术并利用定制化训练策略显著提高了目标对象掩码生成的准确性。&lt;h4&gt;结论&lt;/h4&gt;这种新的方法不仅改进了现有系统的性能，而且为解决机器人在特定环境中操纵物体时遇到的问题提供了一个有价值的框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经完成，并且以结构化的JSON格式组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel pipeline to enhance the precision of objectmasking for robotic manipulation within the specific domain of masking productsin convenience stores. The approach integrates two advanced AI models, CLIP andSAM, focusing on their synergistic combination and the effective use ofmultimodal data (image and text). Emphasis is placed on utilizinggradient-based attention mechanisms and customized datasets to fine-tuneperformance. While CLIP, SAM, and Grad- CAM are established components, theirintegration within this structured pipeline represents a significantcontribution to the field. The resulting segmented masks, generated throughthis combined approach, can be effectively utilized as inputs for roboticsystems, enabling more precise and adaptive object manipulation in the contextof convenience store products.</description>
      <author>example@mail.com (Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Domae Yukiyasu)</author>
      <guid isPermaLink="false">2502.18842v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Autonomy: Off-Road Navigation Enhanced by Human Input</title>
      <link>http://arxiv.org/abs/2502.18760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于学习的本地规划器，用于解决无人车在复杂和不可预测的越野环境中的导航挑战。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶领域面临着应对不规则地面和意外障碍物等越野地形的独特挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够通过单目摄像头捕捉人类驾驶细微差别并快速适应各种越野条件的本地规划器。&lt;h4&gt;方法&lt;/h4&gt;利用少量的人类驾驶示范数据（5-10分钟）进行学习，以掌握在不同类型的地形中导航的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该规划器显著减少了获取人类驾驶偏好的现实世界数据量，并能够将学到的行为直接应用于实际场景，无需手动微调。&lt;h4&gt;结论&lt;/h4&gt;展示了快速适应性和灵活性的越野自主驾驶技术的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在自动驾驶领域，越野地形导航带来了一系列挑战，例如不可预测的地表和意外障碍物等。这项工作提出了一种基于学习的新本地规划器，通过直接从现实世界演示中捕捉人类驾驶特征来应对这些挑战，仅需使用单目摄像头即可实现。该规划器的主要特点是能够应对各种类型的复杂越野环境，并且具有快速的学习能力。凭借最少的人类示范数据（5-10分钟），它能迅速学习在广泛的越野条件下导航的方法。该本地规划器显著减少了从现实世界中获取人类驾驶偏好的需求，从而使规划器无需手动微调即可将学到的行为应用于实际场景中，展示了其在越野自主驾驶技术中的快速调整和适应能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the area of autonomous driving, navigating off-road terrains presents aunique set of challenges, from unpredictable surfaces like grass and dirt tounexpected obstacles such as bushes and puddles. In this work, we present anovel learning-based local planner that addresses these challenges by directlycapturing human driving nuances from real-world demonstrations using only amonocular camera. The key features of our planner are its ability to navigatein challenging off-road environments with various terrain types and its fastlearning capabilities. By utilizing minimal human demonstration data (5-10mins), it quickly learns to navigate in a wide array of off-road conditions.The local planner significantly reduces the real world data required to learnhuman driving preferences. This allows the planner to apply learned behaviorsto real-world scenarios without the need for manual fine-tuning, demonstratingquick adjustment and adaptability in off-road autonomous driving technology.</description>
      <author>example@mail.com (Akhil Nagariya, Dimitar Filev, Srikanth Saripalli, Gaurav Pandey)</author>
      <guid isPermaLink="false">2502.18760v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw</title>
      <link>http://arxiv.org/abs/2502.18749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于物理模拟器（MuJoCo）的软体动力学建模的方法，用于研究机器人辅助喂食过程中的人机交互问题。&lt;h4&gt;背景&lt;/h4&gt;在机器人辅助喂食中确保安全舒适的咬取传递是一个挑战，因为这需要密切的物理人机互动。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过模拟器中的真实皮肤接触动态来系统地评估咬取转移参数（如插入深度和入口角度）对用户安全性和舒适性的影响。&lt;h4&gt;方法&lt;/h4&gt;该研究将一个灵活的人头模型与刚性骨架集成在一起，并考虑了内部动力学，以便由骨骼驱动的柔性模型能够被激活。并且利用软体皮肤接触动态在模拟中进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在假设头部静止的情况下，直入直出策略可以减少力的作用并提高用户舒适度。&lt;h4&gt;结论&lt;/h4&gt;基于仿真的方法为实际世界实验提供了一种更安全、控制更好的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;确保机器人辅助喂食过程中咬取传递的安全与舒适是一个挑战，因为这需要紧密的人机物理交互。本文提出一种新的建模方式，在基于物理学的模拟器（MuJoCo）中使用软体动力学来仿真这种互动。我们整合了一个可变形头部模型和一个刚性骨架，并考虑了内部动态机制，使灵活模型可以通过骨架进行驱动。在模拟中的真实皮肤接触动态被纳入，以便系统地评估咬取转移参数如插入深度和入口角度及其对用户安全性和舒适度的影响。我们的研究结果表明，在假设头部静止的情况下，采取直入直出策略可以减少作用力并提升用户体验的舒适性。这种基于仿真的方法为实际世界实验提供了一种更安全、控制更好的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safe and comfortable bite transfer during robot-assisted feeding ischallenging due to the close physical human-robot interaction required. Thispaper presents a novel approach to modeling physical human-robot interaction ina physics-based simulator (MuJoCo) using soft-body dynamics. We integrate aflexible head model with a rigid skeleton while accounting for internaldynamics, enabling the flexible model to be actuated by the skeleton.Incorporating realistic soft-skin contact dynamics in simulation allows forsystematically evaluating bite transfer parameters, such as insertion depth andentry angle, and their impact on user safety and comfort. Our findings suggestthat a straight-in-straight-out strategy minimizes forces and enhances usercomfort in robot-assisted feeding, assuming a static head. Thissimulation-based approach offers a safer and more controlled alternative toreal-world experimentation. Supplementary videos can be found at:https://tinyurl.com/224yh2kx.</description>
      <author>example@mail.com (Yi Heng San, Vasanthamaran Ravichandram, J-Anne Yow, Sherwin Stephen Chan, Yifan Wang, Wei Tech Ang)</author>
      <guid isPermaLink="false">2502.18749v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries</title>
      <link>http://arxiv.org/abs/2502.18735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'问题描述': '现有视觉-语言模型(VLM)在大规模互联网数据上训练，与机器人收集的原始图像流之间存在领域差异。当前适应策略需要定义一个封闭类集合，这不适用于必须响应多样化自然语言查询的机器人。', '解决方案': '提出QueryAdapter框架，该框架能够通过以前部署期间采集的未标记数据快速调整预训练VLM以应对特定查询。', '技术手段': '利用优化可学习提示令牌和主动选择用于训练的对象来生成适应后的模型，整个过程仅需几分钟。', '处理非相关对象方法': '提出使用与查询无关的对象标题作为负类标签，有助于在自适应过程中产生更校准的置信度分数。', '实验结果': 'ScanNet++数据集上的大量实验证明，QueryAdapter比现有的无监督VLM适配器和3D场景图方法显著提高了对象检索性能。', '泛化能力': '该方法对抽象功能查询和其他数据集（如Ego4D）表现出强大的泛化能力。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了在大规模互联网训练的数据与机器人收集的原始图像之间存在的领域差异，以及现有适应策略面对自然语言多样性时的实际挑战。为解决这一问题，提出了QueryAdapter框架，利用以前未标记数据来调整VLM以应对特定查询，并通过优化可学习提示令牌和主动选择用于训练的对象实现快速调整。此外，提出了一种处理无关对象的新方法，并展示了在ScanNet++等数据集上的优越性能以及对其他类型查询的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A domain shift exists between the large-scale, internet data used to train aVision-Language Model (VLM) and the raw image streams collected by a robot.Existing adaptation strategies require the definition of a closed-set ofclasses, which is impractical for a robot that must respond to diverse naturallanguage queries. In response, we present QueryAdapter; a novel framework forrapidly adapting a pre-trained VLM in response to a natural language query.QueryAdapter leverages unlabelled data collected during previous deployments toalign VLM features with semantic classes related to the query. By optimisinglearnable prompt tokens and actively selecting objects for training, an adaptedmodel can be produced in a matter of minutes. We also explore how objectsunrelated to the query should be dealt with when using real-world data foradaptation. In turn, we propose the use of object captions as negative classlabels, helping to produce better calibrated confidence scores duringadaptation. Extensive experiments on ScanNet++ demonstrate that QueryAdaptersignificantly enhances object retrieval performance compared tostate-of-the-art unsupervised VLM adapters and 3D scene graph methods.Furthermore, the approach exhibits robust generalization to abstract affordancequeries and other datasets, such as Ego4D.</description>
      <author>example@mail.com (Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Christopher Lehnert)</author>
      <guid isPermaLink="false">2502.18735v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization</title>
      <link>http://arxiv.org/abs/2502.18696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;船舶部署自主导航系统需要量身定制的精确运动预测模型。传统的基于物理的方法难以适应实际情况中的船体特定行为，而完全数据驱动的方法虽然能够提供具体性但缺乏解释性和在极端情况下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统基于物理学的模型和纯粹的数据驱动方法各有优缺点：前者基于流体力学原理但在实际操作中无法准确反映船舶特性；后者则具备特定船只的行为预测能力，但是不具有可解释性和应对特殊情况的稳健性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合物理建模与数据驱动优化参数的方法，利用这两种方法的优点，确保模型既具解释性又适应性强。&lt;h4&gt;方法&lt;/h4&gt;该研究引入了一种基于数据和物理学相结合的新模型，它包含了三自由度动力学、舵机力和螺旋桨推力等物理成分，并通过合成数据来微调阻力曲线及舵系数等参数。这种方法将领域知识融入到参数优化过程中，保证了所拟合的模型在物理一致性方面具有较高的性能。&lt;h4&gt;主要发现&lt;/h4&gt;对两艘集装箱船进行实验验证后发现：与基于传统海洋工程实践调整的基本物理学模型相比，采用数据驱动和物理学结合的方法，在预测精度和可靠性方面取得了显著改善。该模型能够捕捉不同条件下船舶特定的行为，并且其预测结果比基准物理模型分别提高了51.6%（船只A）和57.8%（船只B），一致性分别提升了72.36%（船只A）和89.67%（船只B）。&lt;h4&gt;结论&lt;/h4&gt;通过结合物理学方程与数据驱动参数优化，可以创建更准确、可靠且具有解释性的船舶运动预测模型。&lt;h4&gt;翻译&lt;/h4&gt;部署在船上的自主导航系统要求有适合特定船舶的精确运动预测模型。传统基于物理的方法虽然遵循流体力学原理，但在实际条件下难以捕捉到每艘船的独特行为模式；相比之下，纯粹的数据驱动方法提供了具体性但缺乏解释性和极端情况下的稳健性。本研究提出了一种结合物理学基础方程与数据驱动参数优化的新模型，该模型融合了两种方法的优势以确保可解性及适应性。此模型包含三自由度动力学、舵机力和螺旋桨推力等物理组件，并利用合成数据对阻力曲线和舵系数等参数进行优化调整，将领域知识嵌入到参数优化过程中，保持所拟合模型的物理一致性。验证该方法的有效性通过与基于传统海洋工程实践的基本物理模型进行了定性和定量比较，结果表明结合了数据驱动和物理学的新方法在预测精度及可靠性方面显著优于传统的基础物理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of autonomous navigation systems on ships necessitatesaccurate motion prediction models tailored to individual vessels. Traditionalphysics-based models, while grounded in hydrodynamic principles, often fail toaccount for ship-specific behaviors under real-world conditions. Conversely,purely data-driven models offer specificity but lack interpretability androbustness in edge cases. This study proposes a data-driven physics-based modelthat integrates physics-based equations with data-driven parameteroptimization, leveraging the strengths of both approaches to ensureinterpretability and adaptability. The model incorporates physics-basedcomponents such as 3-DoF dynamics, rudder, and propeller forces, whileparameters such as resistance curve and rudder coefficients are optimized usingsynthetic data. By embedding domain knowledge into the parameter optimizationprocess, the fitted model maintains physical consistency. Validation of theapproach is realized with two container ships by comparing, both qualitativelyand quantitatively, predictions against ground-truth trajectories. The resultsdemonstrate significant improvements, in predictive accuracy and reliability,of the data-driven physics-based models over baseline physics-based modelstuned with traditional marine engineering practices. The fitted models captureship-specific behaviors in diverse conditions with their predictions being,51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67%(ship B) more consistent.</description>
      <author>example@mail.com (Papandreou Christos, Mathioudakis Michail, Stouraitis Theodoros, Iatropoulos Petros, Nikitakis Antonios, Stavros Paschalakis, Konstantinos Kyriakopoulos)</author>
      <guid isPermaLink="false">2502.18696v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Voting-Based Task Assignment in Role-Playing Games</title>
      <link>http://arxiv.org/abs/2502.18690v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at Dungeons, Neurons, and Dialogues: Social  Interaction Dynamics in Contextual Games Workshop at 20th Annual ACM/IEEE  International Conference on Human-Robot Interaction (HRI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在角色扮演游戏（RPG）中，沉浸感至关重要，尤其是当游戏中的代理向玩家传达任务、提示或想法时。为了准确解读玩家的情感状态和上下文细节，需要一个基础的理解层次，这可以通过大型语言模型（LLM）来实现。&lt;h4&gt;目的&lt;/h4&gt;保持LLM在多变的上下文中持续聚焦，需要一种更为稳健的方法，如将LLM与专用的任务分配模型结合以在整个游戏过程中引导其性能。为应对这一需求，我们提出了基于投票的任务指派框架(VBTA)，该方法借鉴了人类在任务分配和完成过程中的推理。&lt;h4&gt;方法&lt;/h4&gt;VBTA给代理分配能力配置文件，并向任务描述提供任务说明，然后生成一个适配矩阵来量化代理人能力和任务要求之间的匹配度。通过利用六种不同的投票方式、预训练的LLM以及结合冲突解决搜索（CBS）进行路径规划，VBTA能够高效地识别并为每个任务指派最合适的代理。&lt;h4&gt;主要发现&lt;/h4&gt;与现有的仅专注于生成游戏单个方面的方法不同（如单一任务或战斗遭遇），我们的方法因其通用性而显示出在生成独特的战斗和叙事方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过结合LLM和先进的任务分配技术，可以显著提高游戏角色的沉浸感以及玩家体验的质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在角色扮演游戏中，沉浸度至关重要——尤其是在游戏中的代理向玩家传达任务、提示或想法时。为了准确地解读玩家的情绪状态和情境细微差别，需要一个基础的理解层面，这可以通过大型语言模型（LLM）来实现。然而，保持LLM在整个游戏过程中对多个上下文变化的关注，则需要一种更为稳健的方法，例如将LLM与专门的任务分配模型相结合以引导其性能表现。为应对这一需求，我们提出了基于投票的任务指派框架（VBTA），该方法受人类在任务分配和完成过程中的推理启发。VBTA给代理分配能力配置文件，并向任务描述提供任务说明，然后生成一个适配矩阵来量化代理人能力和任务要求之间的匹配度。通过利用六种不同的投票方式、预训练的LLM以及结合冲突解决搜索（CBS）进行路径规划，VBTA能够高效地识别并为每个任务指派最合适的代理。与现有的仅专注于生成游戏单个方面的方法不同（如单一任务或战斗遭遇），我们的方法因其通用性而显示出在生成独特的战斗和叙事方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In role-playing games (RPGs), the level of immersion is critical-especiallywhen an in-game agent conveys tasks, hints, or ideas to the player. For anagent to accurately interpret the player's emotional state and contextualnuances, a foundational level of understanding is required, which can beachieved using a Large Language Model (LLM). Maintaining the LLM's focus acrossmultiple context changes, however, necessitates a more robust approach, such asintegrating the LLM with a dedicated task allocation model to guide itsperformance throughout gameplay. In response to this need, we introduceVoting-Based Task Assignment (VBTA), a framework inspired by human reasoning intask allocation and completion. VBTA assigns capability profiles to agents andtask descriptions to tasks, then generates a suitability matrix that quantifiesthe alignment between an agent's abilities and a task's requirements.Leveraging six distinct voting methods, a pre-trained LLM, and integratingconflict-based search (CBS) for path planning, VBTA efficiently identifies andassigns the most suitable agent to each task. While existing approaches focuson generating individual aspects of gameplay, such as single quests, or combatencounters, our method shows promise when generating both unique combatencounters and narratives because of its generalizable nature.</description>
      <author>example@mail.com (Daniel Weiner, Raj Korpan)</author>
      <guid isPermaLink="false">2502.18690v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Rapidly Built Medical Crash Cart! Lessons Learned and Impacts on High-Stakes Team Collaboration in the Emergency Room</title>
      <link>http://arxiv.org/abs/2502.18688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, HRI conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;研究团队设计了一种能够在紧急情况下的临床环境中使用的医疗机器人急救车（MCCR），并对其进行了现场评估。&lt;h4&gt;背景&lt;/h4&gt;在高风险的应急场景中，设计能够无缝融入快速变化环境、促进有效沟通以及适应突发状况的机器人面临着独特的挑战。尽管远程操作机器人已经在诸如消防和太空探索等高风险领域得到了成功的应用，但自主支持团队协作的机器人仍然未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究者通过一个快速原型设计过程开发了一系列看似自主的机器人，旨在帮助急诊室中的临床团队。&lt;h4&gt;方法&lt;/h4&gt;将标准急救推车改造成医疗机器人急救车（MCCR），并进行了实地部署评估以检验其对团队工作量和使用性的影响。同时，还确定了失败分类，并在与卫生专业人员的合作中完善了MCCR的设计。&lt;h4&gt;主要发现&lt;/h4&gt;该研究推进了为高风险、时间敏感场景设计的机器人理解，并提供了有关有用的MCCR能力以及有效的人机协作考虑因素的见解。&lt;h4&gt;结论&lt;/h4&gt;通过公开发布MCCR教程，希望激发HRI研究人员探索为高风险团队工作设计机器人的可能性。&lt;h4&gt;翻译&lt;/h4&gt;设计用于支持紧急情况下的高水平合作工作的机器人面临着独特的挑战，包括无缝地融入快速变化的环境、促进成员之间的有效沟通以及适应突发状况。尽管远程操作机器人已经在消防和太空探索等关键领域得到了成功应用，但自主机器人在支援关键团队工作方面仍有待进一步研究。为了填补这一空白，研究人员通过一种快速原型设计方法开发了一系列看似自主的机器人来协助急诊室中的临床团队。将标准急救推车改造为医疗机器人急救推车（MCCR），并通过实地部署评估了其对团队工作量和易用性的影响，并根据与医疗卫生专业人员的合作反馈进一步完善了该设备的设计，确定了失效分类。这项研究推进了高风险、时间紧迫场景下机器人设计的理解，提供了有关有用的MCCR能力及有效人机协作的考虑因素的见解。通过公开发布MCCR教程，研究人员希望鼓励HRI（人类-机器人互动）领域的学者们探索为关键团队工作设计机器人的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing robots to support high-stakes teamwork in emergency settingspresents unique challenges, including seamless integration into fast-pacedenvironments, facilitating effective communication among team members, andadapting to rapidly changing situations. While teleoperated robots have beensuccessfully used in high-stakes domains such as firefighting and spaceexploration, autonomous robots that aid highs-takes teamwork remainunderexplored. To address this gap, we conducted a rapid prototyping process todevelop a series of seemingly autonomous robot designed to assist clinicalteams in the Emergency Room. We transformed a standard crash cart--which storesmedical equipment and emergency supplies into a medical robotic crash cart(MCCR). The MCCR was evaluated through field deployments to assess its impacton team workload and usability, identified taxonomies of failure, and refinedthe MCCR in collaboration with healthcare professionals. Our work advances theunderstanding of robot design for high-stakes, time-sensitive settings,providing insights into useful MCCR capabilities and considerations foreffective human-robot collaboration. By publicly disseminating our MCCRtutorial, we hope to encourage HRI researchers to explore the design of robotsfor high-stakes teamwork.</description>
      <author>example@mail.com (Angelique Taylor, Tauhid Tanjim, Michael Joseph Sack, Maia Hirsch, Kexin Cheng, Kevin Ching, Jonathan St. George, Thijs Roumen, Malte F. Jung, Hee Rin Lee)</author>
      <guid isPermaLink="false">2502.18688v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Primitive-Swarm: An Ultra-lightweight and Scalable Planner for Large-scale Aerial Swarms</title>
      <link>http://arxiv.org/abs/2502.16887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Primitive-Swarm的轻量级且可扩展的大规模自主空中群规划器，采用分散式和异步重规划策略，并结合了基于可达性分析的时间最优路径参数化算法（TOPP-RA）生成的时间最优化和动态可行轨迹库。&lt;h4&gt;背景&lt;/h4&gt;大规模空中无人机集群操作在计算效率与可扩展性之间存在固有的矛盾。当前的方案难以同时实现高效的计算性能与广泛的应用范围。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于大规模自主空中无人机集群的有效规划器，旨在解决上述矛盾。&lt;h4&gt;方法&lt;/h4&gt;{'策略': '采用分散式和异步重规划策略', '轨迹库': '开发了一种时间最优化的运动原语库，并基于可达性分析生成这些原语', '碰撞检测机制': '通过与离散空间关联，建立快速碰撞检查机制以处理机器人障碍物冲突及机器人间的碰撞'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能优势': '在密集环境中实现了最短飞行时间和最小移动距离，并且计算时间少于1ms。', '可扩展性验证': '通过大规模模拟实验（最多涉及1000个无人机）验证了方案的实时性和可扩展性。', '实际应用可行性': '通过真实世界实验展示了该方法的实际可行性和鲁棒性'}&lt;h4&gt;结论&lt;/h4&gt;Primitive-Swarm规划器提供了一种新的解决大规模空中集群操作中计算效率与可扩展性的矛盾的方法，适用于各种复杂环境和应用场景。同时，公开源代码以促进社区合作。&lt;h4&gt;翻译&lt;/h4&gt;实现大规模的空中群是具有挑战性的，因为需要在计算效率和可扩展性之间进行权衡。本文介绍了一种名为Primitive-Swarm的轻量级且可扩展的规划器，旨在解决大规模自主飞行器集群的问题。该方法采用分散式和异步重规划策略，并使用一种基于可达性分析的时间最优路径参数化算法（TOPP-RA）来生成轨迹库。通过结合这些运动原语与离散空间来处理碰撞检测机制。最后，实验表明这种方案能够以极短计算时间在密集环境中实现最短的飞行时间和最小的距离移动，在大规模模拟和真实世界场景中均表现出优异性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving large-scale aerial swarms is challenging due to the inherentcontradictions in balancing computational efficiency and scalability. Thispaper introduces Primitive-Swarm, an ultra-lightweight and scalable plannerdesigned specifically for large-scale autonomous aerial swarms. The proposedapproach adopts a decentralized and asynchronous replanning strategy. Within itis a novel motion primitive library consisting of time-optimal and dynamicallyfeasible trajectories. They are generated utlizing a novel time-optimial pathparameterization algorithm based on reachability analysis (TOPP-RA). Then, arapid collision checking mechanism is developed by associating the motionprimitives with the discrete surrounding space according to conflicts. Byconsidering both spatial and temporal conflicts, the mechanism handlesrobot-obstacle and robot-robot collisions simultaneously. Then, during areplanning process, each robot selects the safe and minimum cost trajectoryfrom the library based on user-defined requirements. Both the time-optimalmotion primitive library and the occupancy information are computed offline,turning a time-consuming optimization problem into a linear-complexityselection problem. This enables the planner to comprehensively explore thenon-convex, discontinuous 3-D safe space filled with numerous obstacles androbots, effectively identifying the best hidden path. Benchmark comparisonsdemonstrate that our method achieves the shortest flight time and traveleddistance with a computation time of less than 1 ms in dense environments. Superlarge-scale swarm simulations, involving up to 1000 robots, running inreal-time, verify the scalability of our method. Real-world experimentsvalidate the feasibility and robustness of our approach. The code will bereleased to foster community collaboration.</description>
      <author>example@mail.com (Jialiang Hou, Xin Zhou, Neng Pan, Ang Li, Yuxiang Guan, Chao Xu, Zhongxue Gan, Fei Gao)</author>
      <guid isPermaLink="false">2502.16887v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
  <item>
      <title>Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment</title>
      <link>http://arxiv.org/abs/2502.16863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages+Appendix, 6 Figures, AAMAS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法LLM-MCA，利用大型语言模型来评估多智能体系统中每个代理的行为对团队成功或失败的贡献。这种方法通过将信用分配问题转化为序列改进和归因的模式识别任务，解决了现有的集中训练-分散执行范式下的挑战。&lt;h4&gt;背景&lt;/h4&gt;最近的工作表明学习协作行为对于机器人实现共同目标的重要性，并且通常使用集中训练-分散执行的方法来学习这种合作行为。然而，这带来了如何评估每个代理的行为对团队成功或失败贡献的问题。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体强化学习中的信用分配问题，通过结合人类手动审查代理行为的观察结果和大型语言模型在模式识别任务中表现出的人类水平性能的研究发现，提出一种新的方法来改进协作机器人系统的信用分配机制。&lt;h4&gt;方法&lt;/h4&gt;论文提出了LLM-MCA方法，利用中心化的大型语言模型奖励评论器根据场景中的每个代理的独特贡献对环境奖励进行数值分解，并基于此反馈更新代理的策略网络。此外还介绍了一种扩展版本LLM-TACA，其中大型语言模型批评者执行显式任务分配。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在各种基准测试中大大优于现有技术，包括层次化觅食、机器人仓库和新的太空世界基准测试，后者的环境包含了碰撞相关的安全性限制。这些方法还生成了带有每个时间步长代理奖励信息的大型轨迹数据集。&lt;h4&gt;结论&lt;/h4&gt;论文证明了利用大型语言模型进行信用分配可以提高多智能体系统的性能，并为未来的研究提供了一种全新的思路和潜在的应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent work, spanning from autonomous vehicle coordination to in-spaceassembly, has shown the importance of learning collaborative behavior forenabling robots to achieve shared goals. A common approach for learning thiscooperative behavior is to utilize the centralized-trainingdecentralized-execution paradigm. However, this approach also introduces a newchallenge: how do we evaluate the contributions of each agent's actions to theoverall success or failure of the team. This credit assignment problem hasremained open, and has been extensively studied in the Multi-AgentReinforcement Learning literature. In fact, humans manually inspecting agentbehavior often generate better credit evaluations than existing methods. Wecombine this observation with recent works which show Large Language Modelsdemonstrate human-level performance at many pattern recognition tasks. Our keyidea is to reformulate credit assignment to the two pattern recognitionproblems of sequence improvement and attribution, which motivates our novelLLM-MCA method. Our approach utilizes a centralized LLM reward-critic whichnumerically decomposes the environment reward based on the individualizedcontribution of each agent in the scenario. We then update the agents' policynetworks based on this feedback. We also propose an extension LLM-TACA whereour LLM critic performs explicit task assignment by passing an intermediarygoal directly to each agent policy in the scenario. Both our methods faroutperform the state-of-the-art on a variety of benchmarks, includingLevel-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark whichincorporates collision-related safety constraints. As an artifact of ourmethods, we generate large trajectory datasets with each timestep annotatedwith per-agent reward information, as sampled from our LLM critics.</description>
      <author>example@mail.com (Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr)</author>
      <guid isPermaLink="false">2502.16863v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Graph Augmentation for Cross Graph Domain Generalization</title>
      <link>http://arxiv.org/abs/2502.18188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个新的图结构增强技术，用于跨图节点分类问题。通过去除可能影响GNN泛化能力的低权重边，并基于同分布节点特征生成不变结构，以帮助GNN捕捉不同图结构之间的本质不变信息。&lt;h4&gt;背景&lt;/h4&gt;跨图节点分类可以看作是图神经网络领域泛化的结构转移问题，而当前的研究主要集中在模型训练上，数据增强技术尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图结构增强方法，以解决跨图领域的泛化问题，提高GNN在不同分布的数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过低权重边去除减少噪声干扰，并使用基于聚类的添加边策略生成不变结构。这两种技术共同提高了GNN对领域不变信息的保持和利用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在跨域数据集上，这种方法能够提高图神经网络的泛化能力，并且比传统增强方法取得了更好的性能表现。&lt;h4&gt;结论&lt;/h4&gt;该工作证明了通过设计针对性的数据增强策略可以显著提升跨图节点分类任务中的模型泛化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-graph node classification, utilizing the abundant labeled nodes fromone graph to help classify unlabeled nodes in another graph, can be viewed as adomain generalization problem of graph neural networks (GNNs) due to thestructure shift commonly appearing among various graphs. Nevertheless, currentendeavors for cross-graph node classification mainly focus on model training.Data augmentation approaches, a simple and easy-to-implement domaingeneralization technique, remain under-explored. In this paper, we develop anew graph structure augmentation for the crossgraph domain generalizationproblem. Specifically, low-weight edgedropping is applied to remove potentialnoise edges that may hinder the generalization ability of GNNs, stimulating theGNNs to capture the essential invariant information underlying differentstructures. Meanwhile, clustering-based edge-adding is proposed to generateinvariant structures based on the node features from the same distribution.Consequently, with these augmentation techniques, the GNNs can maintain thedomain invariant structure information that can improve the generalizationability. The experiments on out-ofdistribution citation network datasets verifyour method achieves state-of-the-art performance among conventionalaugmentations.</description>
      <author>example@mail.com (Guanzi Chen, Jiying Zhang, Yang Li)</author>
      <guid isPermaLink="false">2502.18188v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing DNA Foundation Models to Address Masking Inefficiencies</title>
      <link>http://arxiv.org/abs/2502.18405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了在基因组序列建模中广泛采用的遮蔽语言模型（MLM）预训练目标存在的问题，并提出了一种基于掩码自动编码器框架的修改版编解码架构来解决BERT基础变换器中的效率低下问题。&lt;h4&gt;背景&lt;/h4&gt;遮蔽语言模型（MLM）在基因组序列建模中被广泛采用，但这种模型在从预训练到推断的应用过程中存在分布偏移的问题。即，在下游任务中没有[MASK]标记，导致编码器忽视了非[MASK]标记的编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于掩码自动编码框架的修改版编解码架构来解决MLM模型中的效率低下问题，并在基因组管道中进行验证。&lt;h4&gt;方法&lt;/h4&gt;利用遮蔽自编码器框架设计了一种新的BERT基础变换器，通过实验展示这种方法比因果模型和双向架构（使用MLM任务预训练）在闭集和开集分类任务上表现更好。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法可以显著提高基因组序列建模中特征提取的性能，并且在BIOSCAN-5M数据集中实现了明显的性能提升。&lt;h4&gt;结论&lt;/h4&gt;新的方法比传统的遮蔽语言模型（MLM）更有效，尤其是在不进行微调的情况下用于特征提取的应用场景。这种方法提供了一种解决现有问题的新途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked language modelling (MLM) as a pretraining objective has been widelyadopted in genomic sequence modelling. While pretrained models can successfullyserve as encoders for various downstream tasks, the distribution shift betweenpretraining and inference detrimentally impacts performance, as the pretrainingtask is to map [MASK] tokens to predictions, yet the [MASK] is absent duringdownstream applications. This means the encoder does not prioritize itsencodings of non-[MASK] tokens, and expends parameters and compute on work onlyrelevant to the MLM task, despite this being irrelevant at deployment time. Inthis work, we propose a modified encoder-decoder architecture based on themasked autoencoder framework, designed to address this inefficiency within aBERT-based transformer. We empirically show that the resulting mismatch isparticularly detrimental in genomic pipelines where models are often used forfeature extraction without fine-tuning. We evaluate our approach on theBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achievesubstantial performance gains in both closed-world and open-worldclassification tasks when compared against causal models and bidirectionalarchitectures pretrained with MLM tasks.</description>
      <author>example@mail.com (Monireh Safari, Pablo Millan Arias, Scott C. Lowe, Lila Kari, Angel X. Chang, Graham W. Taylor)</author>
      <guid isPermaLink="false">2502.18405v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ExPath: Towards Explaining Targeted Pathways for Biological Knowledge Bases</title>
      <link>http://arxiv.org/abs/2502.18026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的路径推理框架ExPath，该框架能够将实验数据（特别是氨基酸序列）与生物网络数据库中的图分类相结合。&lt;h4&gt;背景&lt;/h4&gt;现有的生物学知识库提供细胞或有机体分子互作的功能通路。然而，识别更具体的目标通路，特别是在结合实验室数据时，仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的路径推理框架ExPath来解决这个挑战，并能够明确整合实验数据。&lt;h4&gt;方法&lt;/h4&gt;该框架由三个组成部分构成：1. 一个大型蛋白质语言模型pLM；2. PathMamba混合架构；3. PathExplainer子图学习模块。这些技术组合能处理氨基酸序列、捕捉局部和全局依赖关系并识别功能关键节点与边缘。&lt;h4&gt;主要发现&lt;/h4&gt;实验涉及了对301个生物网络的评价，表明通过ExPath推理出的路径具有生物学意义，并计划公开发布经过整理的生物网络数据集。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在处理氨基酸序列和生成有意义的生物路径方面表现出了有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：生物知识库提供细胞或有机体分子互作的功能通路。然而，识别更具体的目标通路尤其当结合实验室数据时仍具有挑战性，并且通常需要下游生物学分析及专业知识。本文将此视为可解的图学习和解释任务并提出了一种新的路径推理框架ExPath，该框架明确整合实验数据（特别是氨基酸序列）来分类生物数据库中的各种图形网络。对分类更有贡献的链接可以被考虑为目标通路。技术上来说，ExPath由三个组件组成：1. 大型蛋白质语言模型pLM；2. PathMamba混合架构；3. PathExplainer子图学习模块。我们还提出了ML导向的生物学评价和新度量标准。涉及301个生物网络评估的实验表明路径通过ExPath推理保持生物意义，我们将很快公开发布整理过的301个生物网络数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological knowledge bases provide systemically functional pathways of cellsor organisms in terms of molecular interaction. However, recognizing moretargeted pathways, particularly when incorporating wet-lab experimental data,remains challenging and typically requires downstream biological analyses andexpertise. In this paper, we frame this challenge as a solvable graph learningand explaining task and propose a novel pathway inference framework, ExPath,that explicitly integrates experimental data, specifically amino acid sequences(AA-seqs), to classify various graphs (bio-networks) in biological databases.The links (representing pathways) that contribute more to classification can beconsidered as targeted pathways. Technically, ExPath comprises threecomponents: (1) a large protein language model (pLM) that encodes and embedsAA-seqs into graph, overcoming traditional obstacles in processing AA-seq data,such as BLAST; (2) PathMamba, a hybrid architecture combining graph neuralnetworks (GNNs) with state-space sequence modeling (Mamba) to capture bothlocal interactions and global pathway-level dependencies; and (3)PathExplainer, a subgraph learning module that identifies functionally criticalnodes and edges through trainable pathway masks. We also propose ML-orientedbiological evaluations and a new metric. The experiments involving 301bio-networks evaluations demonstrate that pathways inferred by ExPath maintainbiological meaningfulness. We will publicly release curated 301 bio-networkdata soon.</description>
      <author>example@mail.com (Rikuto Kotoge, Ziwei Yang, Zheng Chen, Yushun Dong, Yasuko Matsubara, Jimeng Sun, Yasushi Sakurai)</author>
      <guid isPermaLink="false">2502.18026v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>DenoMAE2.0: Improving Denoising Masked Autoencoders by Classifying Local Patches</title>
      <link>http://arxiv.org/abs/2502.18202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们介绍了DenoMAE2.0，这是一种增强的去噪掩码自动编码器，它结合了局部补丁分类目标和传统的重构损失来提高表示学习和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;与传统的Masked Autoencoders (MAE)不同，后者专注于重建丢失的输入，DenoMAE2.0引入了对未掩盖补丁的位置感知分类，使模型能够捕捉细微的局部特征同时保持全局一致性。这种方法在无线通信中的半监督学习特别有帮助。&lt;h4&gt;目的&lt;/h4&gt;针对噪声水平高和数据稀缺的问题，我们在广泛的信噪比(SNR)条件下进行了广泛的实验，从极低到中等条件，并在一个低数据环境中测试了DenoMAE2.0。&lt;h4&gt;方法&lt;/h4&gt;我们对调制信号分类进行了一系列表现于宽范围SNRs的实验，在极高噪声水平和较低的数据环境下验证模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明，与前辈Deno-MAE和其他基准相比，DenoMAE2.0在去噪质量和下游分类准确度方面都有显著提高。具体而言，它比DenoMAE提高了1.1%的性能，并且在RadioML基准测试中的星座图分类中，相对于DenoMAE，分别取得了11.83%和16.55%的显着改进。&lt;h4&gt;结论&lt;/h4&gt;DenoMAE2.0通过增加位置感知补丁分类目标，在去噪质量和信号分类准确度方面超越了现有的模型，特别是在高噪声水平和低数据环境下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DenoMAE2.0, an enhanced denoising masked autoencoder thatintegrates a local patch classification objective alongside traditionalreconstruction loss to improve representation learning and robustness. Unlikeconventional Masked Autoencoders (MAE), which focus solely on reconstructingmissing inputs, DenoMAE2.0 introduces position-aware classification of unmaskedpatches, enabling the model to capture fine-grained local features whilemaintaining global coherence. This dual-objective approach is particularlybeneficial in semi-supervised learning for wireless communication, where highnoise levels and data scarcity pose significant challenges. We conductextensive experiments on modulation signal classification across a wide rangeof signal-to-noise ratios (SNRs), from extremely low to moderately highconditions and in a low data regime. Our results demonstrate that DenoMAE2.0surpasses its predecessor, Deno-MAE, and other baselines in both denoisingquality and downstream classification accuracy. DenoMAE2.0 achieves a 1.1%improvement over DenoMAE on our dataset and 11.83%, 16.55% significant improvedaccuracy gains on the RadioML benchmark, over DenoMAE, for constellationdiagram classification of modulation signals.</description>
      <author>example@mail.com (Atik Faysal, Mohammad Rostami, Taha Boushine, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar)</author>
      <guid isPermaLink="false">2502.18202v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Vision-Language Navigation (VLN)旨在通过利用语言指令和视觉线索引导代理穿过环境，在具身人工智能领域扮演关键角色。尽管室内VLN已得到广泛研究，但室外空中VLN仍待深入探索。&lt;h4&gt;背景&lt;/h4&gt;现有问题在于户外空域范围广阔，数据收集更具挑战性，导致缺乏相应的基准测试。&lt;h4&gt;目的&lt;/h4&gt;为解决上述问题，我们提出OpenFly平台及其组成部分：多功能工具链和大规模基准。&lt;h4&gt;方法&lt;/h4&gt;{'开发自动化工具链': '用于自动获取点云、场景语义分割、飞行轨迹创建及指令生成', '构建数据集': '基于该工具链建立包括10万条轨迹在内的大规模空中VLN数据集，覆盖不同高度和长度的18个场景。使用多种渲染引擎（如Unreal Engine, GTA V）和高级技术生成视觉数据。', '模型开发': '提出OpenFly-Agent，一种关键帧感知的VLN模型，该模型以语言指令、当前观察结果及历史关键帧为输入，并直接输出飞行动作'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的分析与实验展示了我们平台及模型的优势&lt;h4&gt;结论&lt;/h4&gt;工具链、数据集和代码将开源。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言导航(VLN)旨在利用语言指令和视觉线索引导代理穿过环境，特别是在具身人工智能领域扮演关键角色。尽管室内VLN已经得到广泛研究，但户外空域的空中导航仍然有待深入探索。主要原因在于户外空间广阔，数据收集更为困难，导致缺乏相应的基准测试。为此我们提出了OpenFly平台，包括多功能工具链和大规模数据集。此平台旨在解决数据采集难题，并开发了关键帧感知模型以提高任务表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Escaping The Big Data Paradigm in Self-Supervised Representation Learning</title>
      <link>http://arxiv.org/abs/2502.18056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and implementation available at:  https://github.com/inescopresearch/scott&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了在图像自监督表示学习中是否可以摆脱大数据范式的限制。&lt;h4&gt;背景&lt;/h4&gt;大规模数据集和计算资源的需求成为视觉领域进展的障碍，特别是在数据稀缺的情况下。&lt;h4&gt;目的&lt;/h4&gt;旨在探究能否通过新的方法让视觉变换器能够在小规模数据下有效训练，而不依赖于外部的大规模预训练数据集。&lt;h4&gt;方法&lt;/h4&gt;{'SCOTT架构': '一种浅层标记化结构，与遮罩图像建模任务兼容，并向视觉变换器注入卷积先验偏置。', 'MIM-JEPA框架': '提出了一种联合嵌入预测架构，在潜在表示空间内运行以捕捉更多语义特征。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'模型性能': '冻结预训练的SCOTT模型在三个小规模、标准分辨率、细粒度数据集上显著优于全监督方法，并且与依赖大规模预训练、复杂图像增强和更大模型尺寸的最佳方法相当。', '资源节省': '证明了稳健的现成表示可以在有限的数据、计算和模型大小的情况下学习，为医疗影像或机器人等资源受限环境中的计算机应用开辟新道路。'}&lt;h4&gt;结论&lt;/h4&gt;挑战了数据量对于有效视觉表征学习不可或缺的传统观念，并提供了一条通向更可访问且包容性更强的进展的新路径。&lt;h4&gt;翻译&lt;/h4&gt;在视觉表示学习中，大规模的数据集和计算资源的需求已经成为一个主要障碍，尤其是在数据稀缺的情况下。本文探讨了一个关键问题：我们能否摆脱大数据范式，在自监督图像表征学习中实现这一点？为此引入了SCOTT（稀疏卷积标记器转换器），这是一种浅层结构，与遮罩图像建模任务兼容。此外，还提出了一种联合嵌入预测架构，用于在潜在表示空间内运行的遮罩图像建模框架（MIM-JEPA）。这些方法使ViTs能够在比传统所需的规模小得多的数据集上从头开始训练，无需依赖大规模外部预训练数据集。实验证明了该方法的有效性，并挑战了大数据量是视觉表征学习不可或缺的传统观念，为资源受限环境中的计算机应用提供了新路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The reliance on large-scale datasets and extensive computational resourceshas become a major barrier to advancing representation learning in vision,especially in data-scarce domains. In this paper, we address the criticalquestion: Can we escape the big data paradigm in self-supervised representationlearning from images? We introduce SCOTT (Sparse Convolutional Tokenizer forTransformers), a shallow tokenization architecture that is compatible withMasked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biasesinto Vision Transformers (ViTs), enhancing their efficacy in small-scale dataregimes. Alongside, we propose to train on a Joint-Embedding PredictiveArchitecture within a MIM framework (MIM-JEPA), operating in latentrepresentation space to capture more semantic features. Our approach enablesViTs to be trained from scratch on datasets orders of magnitude smaller thantraditionally required --without relying on massive external datasets forpretraining. We validate our method on three small-size, standard-resoultion,fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, andImageNet-100. Despite the challenges of limited data and high intra-classsimilarity, frozen SCOTT models pretrained with MIM-JEPA significantlyoutperform fully supervised methods and achieve competitive results with SOTAapproaches that rely on large-scale pretraining, complex image augmentationsand bigger model sizes. By demonstrating that robust off-the-shelfrepresentations can be learned with limited data, compute, and model sizes, ourwork paves the way for computer applications in resource constrainedenvironments such as medical imaging or robotics. Our findings challenge theprevailing notion that vast amounts of data are indispensable for effectiverepresentation learning in vision, offering a new pathway toward moreaccessible and inclusive advancements in the field.</description>
      <author>example@mail.com (Carlos Vélez García, Miguel Cazorla, Jorge Pomares)</author>
      <guid isPermaLink="false">2502.18056v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music</title>
      <link>http://arxiv.org/abs/2502.18309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;生成高质量的全身舞蹈序列是一项挑战，需要严格遵循特定风格的动作编排，并且产生的序列必须是物理上真实的并且与音乐的节拍和节奏精确同步。&lt;h4&gt;背景&lt;/h4&gt;现有的方法难以同时满足严格的风格特性和物理真实性以及精确的时间对齐需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种无分类器扩散框架GCDance，用于生成基于音乐和文本提示的特定类型的舞蹈动作。&lt;h4&gt;方法&lt;/h4&gt;{'特征提取': '结合高级预训练音乐基础模型特征与手工制作的多粒度特性融合来提取音乐特征', '时间步嵌入': '利用CLIP在每个时间步骤内有效地嵌入基于风格的文本提示表示'}&lt;h4&gt;主要发现&lt;/h4&gt;GCDance框架可以生成同一首音乐的不同舞蹈风格，同时确保与音乐节奏和旋律的一致性。&lt;h4&gt;结论&lt;/h4&gt;实验证明了GCDance在FineDance数据集上显著优于现有的最先进方法，并且在AIST++数据集上也取得了竞争性的结果。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating high-quality full-body dance sequences from music is a challengingtask as it requires strict adherence to genre-specific choreography. Moreover,the generated sequences must be both physically realistic and preciselysynchronized with the beats and rhythm of the music. To overcome thesechallenges, we propose GCDance, a classifier-free diffusion framework forgenerating genre-specific dance motions conditioned on both music and textualprompts. Specifically, our approach extracts music features by combininghigh-level pre-trained music foundation model features with hand-craftedfeatures for multi-granularity feature fusion. To achieve genrecontrollability, we leverage CLIP to efficiently embed genre-based textualprompt representations at each time step within our dance generation pipeline.Our GCDance framework can generate diverse dance styles from the same piece ofmusic while ensuring coherence with the rhythm and melody of the music.Extensive experimental results obtained on the FineDance dataset demonstratethat GCDance significantly outperforms the existing state-of-the-artapproaches, which also achieve competitive results on the AIST++ dataset. Ourablation and inference time analysis demonstrate that GCDance provides aneffective solution for high-quality music-driven dance generation.</description>
      <author>example@mail.com (Xinran Liu, Xu Dong, Diptesh Kanojia, Wenwu Wang, Zhenhua Feng)</author>
      <guid isPermaLink="false">2502.18309v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers</title>
      <link>http://arxiv.org/abs/2502.18460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DRAMA是一个利用大型语言模型训练较小的、具有泛化能力的密集检索器的框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在作为密集检索器时表现出色，但其庞大的参数规模带来了推理时间上的计算挑战，包括大规模语料库编码成本高和查询延迟增加的问题。相比之下，小型检索器虽然效率更高，但在有限监督数据下的泛化能力较弱。&lt;h4&gt;目的&lt;/h4&gt;介绍DRAMA框架，旨在利用大型语言模型训练出更小且具有更好泛化的密集检索器。&lt;h4&gt;方法&lt;/h4&gt;采用剪枝后的大型语言模型作为骨干，并在单一阶段对比学习设置下使用多样性的大型语言模型增强数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，与传统的编码器基础的检索器相比，DRAMA提供了更好的多语言和长上下文处理能力，并在多种任务和语言上取得了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架展示了连接较小检索器训练与大规模语言模型进展之间的潜在价值，从而弥合了效率与泛化之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong effectiveness androbustness while fine-tuned as dense retrievers. However, their large parametersize brings significant inference time computational challenges, including highencoding costs for large-scale corpora and increased query latency, limitingtheir practical deployment. While smaller retrievers offer better efficiency,they often fail to generalize effectively with limited supervised fine-tuningdata. In this work, we introduce DRAMA, a training framework that leveragesLLMs to train smaller generalizable dense retrievers. In particular, we adoptpruned LLMs as the backbone and train on diverse LLM-augmented data in asingle-stage contrastive learning setup. Experiments show that DRAMA offersbetter multilingual and long-context capabilities than traditionalencoder-based retrievers, and achieves strong performance across multiple tasksand languages. These highlight the potential of connecting the training ofsmaller retrievers with the growing advancements in LLMs, bridging the gapbetween efficiency and generalization.</description>
      <author>example@mail.com (Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen)</author>
      <guid isPermaLink="false">2502.18460v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies</title>
      <link>http://arxiv.org/abs/2502.18438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了ToMCAT框架，用于基于心智理论（Theory-of-Mind, ToM）生成合作型多智能体系统的轨迹。&lt;h4&gt;背景&lt;/h4&gt;在合作性任务中，了解队友的目标和行为对于团队性能至关重要。现有的方法通常无法动态适应环境变化和队友的行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合元学习机制和多代理去噪扩散模型的框架（ToMCAT），以生成条件于心智理论推理的轨迹，并实现在线规划系统来减少资源使用而不损害团队表现。&lt;h4&gt;方法&lt;/h4&gt;1. 使用元学习机制，进行关于队友潜在目标和未来行为的心智理论推理；2. 利用多代理去噪扩散模型，根据代理的目标以及通过ToM计算得出的队友特性生成计划。3. 实现在线规划系统，在检测到先前生成的计划与当前世界状态之间存在差异时动态采样新的轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;1. 动态再规划机制在减少资源使用的同时不损害团队性能方面至关重要；2. 关于环境和队友行为的近期观察结合心智理论推断对于生成适应性策略以应对队友变化至关重要，尤其是在没有关于它们的先前信息的情况下。&lt;h4&gt;结论&lt;/h4&gt;ToMCAT框架提供了一种有效的方法来实现动态的合作智能体系统，通过利用心智理论推理，提高了团队在未知环境中的适应性和效率。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中我们提出了ToMCAT（合作型多代理团队的心智理论），这是一个新的基于心智理论生成轨迹的框架。它结合了一个元学习机制，该机制可以对队友潜在的目标和未来行为进行心智理论推理，以及一个多重代理去噪扩散模型，该模型可以根据智能体及其队友的特性来为智能体及其队友生成计划，这些特性是通过心智理论计算得出的。我们实施了一种在线规划系统，它会在检测到先前生成的计划与当前世界状态之间存在差异时从扩散模型中动态采样新的轨迹（重计划）。我们在模拟烹饪领域使用ToMCAT进行了多次实验。我们的结果强调了动态再规划机制在不牺牲团队性能的前提下减少资源使用的至关重要性。我们还表明，在一个时间段内，代理收集到的关于世界的近期观察和队友的行为结合心智理论推断对于生成适应性的策略以应对队友变化是至关重要的，尤其是在没有提供有关他们的先前信息的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents inTeams), a new framework for generating ToM-conditioned trajectories. Itcombines a meta-learning mechanism, that performs ToM reasoning over teammates'underlying goals and future behavior, with a multiagent denoising-diffusionmodel, that generates plans for an agent and its teammates conditioned on boththe agent's goals and its teammates' characteristics, as computed via ToM. Weimplemented an online planning system that dynamically samples new trajectories(replans) from the diffusion model whenever it detects a divergence between apreviously generated plan and the current state of the world. We conductedseveral experiments using ToMCAT in a simulated cooking domain. Our resultshighlight the importance of the dynamic replanning mechanism in reducing theusage of resources without sacrificing team performance. We also show thatrecent observations about the world and teammates' behavior collected by anagent over the course of an episode combined with ToM inferences are crucial togenerate team-aware plans for dynamic adaptation to teammates, especially whenno prior information is provided about them.</description>
      <author>example@mail.com (Pedro Sequeira, Vidyasagar Sadhu, Melinda Gervasio)</author>
      <guid isPermaLink="false">2502.18438v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes</title>
      <link>http://arxiv.org/abs/2502.17999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a preprint. Paper accepted for publication at the 21st EAI  International Conference on Mobile and Ubiquitous Systems: Computing,  Networking and Services (Mobiquitous)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的基于图神经网络的可解释性模型，用于智能家居环境中的传感器数据驱动的人体活动识别。&lt;h4&gt;背景&lt;/h4&gt;在智能家居环境中，传感器数据驱动的人体活动识别对于医疗保健领域尤为重要。目前大多数现有方法依赖于深度学习模型，但这些模型通常不透明且难以理解。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的图神经网络，该网络不仅能够有效地进行人体活动识别，同时还能提供清晰、直观的解释以增强可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络（GNN）来提高传感器数据驱动的人体活动识别的效果，并在此基础上设计了首个为智能家居环境中的HAR任务专门定制的可解释模型。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在两个公开数据集上的实验结果表明，它不仅提供了比现有最佳方法更好的解释性，同时还能略微提升人体活动的识别率。&lt;h4&gt;结论&lt;/h4&gt;这项工作是首次将图神经网络应用于传感器数据驱动的人体活动识别，并且通过提高可解释性和性能为该领域的研究开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;基于传感器的人体活动识别（HAR）在智能家居环境中的应用至关重要，特别是在医疗保健领域。目前大多数现有方法依赖于深度学习模型如CNN或RNN，这些方法虽有效但输出的原理不透明。最近，提出了一种新的可解释性人工智能（XAI）方法来提供直观的解释。然而，现有的HAR方法并未专门设计为考虑可解释性。本文首次提出了一个专用于智能家居环境中的基于图神经网络的可解释模型，实验证明该方法在提高解释性和识别率方面优于现有最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensor-based Human Activity Recognition (HAR) in smart home environments iscrucial for several applications, especially in the healthcare domain. Themajority of the existing approaches leverage deep learning models. While theseapproaches are effective, the rationale behind their outputs is opaque.Recently, eXplainable Artificial Intelligence (XAI) approaches emerged toprovide intuitive explanations to the output of HAR models. To the best of ourknowledge, these approaches leverage classic deep models like CNNs or RNNs.Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-basedHAR. However, existing approaches are not designed with explainability in mind.In this work, we propose the first explainable Graph Neural Network explicitlydesigned for smart home HAR. Our results on two public datasets show that thisapproach provides better explanations than state-of-the-art methods while alsoslightly improving the recognition rate.</description>
      <author>example@mail.com (Michele Fiori, Davide Mor, Gabriele Civitarese, Claudio Bettini)</author>
      <guid isPermaLink="false">2502.17999v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Conformal Prediction Under Generalized Covariate Shift with Posterior Drift</title>
      <link>http://arxiv.org/abs/2502.17744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了一种新的转移学习环境下分类问题的解决策略，即在先验分布发生变化的情况下利用带有后验漂移的协变量偏移假设。&lt;h4&gt;背景&lt;/h4&gt;统计学习中收集足够多训练数据往往耗时费力或不切实际。转移学习通过从相关源领域获取知识来改善目标领域的学习性能成为一种有益的方法。&lt;h4&gt;目的&lt;/h4&gt;研究并提出在特定分布假设下的转移学习方法，特别是在后验漂移的协变量偏移设置下实现具有覆盖保证的目标分类。&lt;h4&gt;方法&lt;/h4&gt;提出了一个加权的符合预测分类器，在此框架下，每个数据实例将被赋予一组可能的标签，而不是单个标签。该方法利用了源领域和目标领域的样本。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明提出的加权符合预测分类器具有良好的渐近性质；数值研究进一步展示了所提方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该转移学习方法可以提高目标领域的分类性能，并提供了可靠的覆盖保证。&lt;h4&gt;翻译&lt;/h4&gt;在许多统计学习的实际应用中，收集足够的训练数据往往是昂贵、耗时或不现实的。在这种情况下，通过利用相关源领域中的知识来改进目标领域的学习表现的迁移学习方法更为有利。在此背景下，我们研究了一种特定类型的分类问题——符合预测，在新的分布假设下进行转移学习。基于符合预测框架的分类器为每个数据实例提供一组可能标签而不是单个标签，从而做出更谨慎且安全的决策。在转移学习中，我们考虑了一个广义的‘具有后验漂移的协变量偏移’设置，并提出了一种加权的符合预测分类器，在目标域内保证覆盖的同时利用源和目标样本。理论研究表明这种方法有良好的渐近性质，数值研究进一步证实了所提出的方案的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many real applications of statistical learning, collecting sufficientlymany training data is often expensive, time-consuming, or even unrealistic. Inthis case, a transfer learning approach, which aims to leverage knowledge froma related source domain to improve the learning performance in the targetdomain, is more beneficial. There have been many transfer learning methodsdeveloped under various distributional assumptions. In this article, we study aparticular type of classification problem, called conformal prediction, under anew distributional assumption for transfer learning. Classifiers under theconformal prediction framework predict a set of plausible labels instead of onesingle label for each data instance, affording a more cautious and saferdecision. We consider a generalization of the \textit{covariate shift withposterior drift} setting for transfer learning. Under this setting, we proposea weighted conformal classifier that leverages both the source and targetsamples, with a coverage guarantee in the target domain. Theoretical studiesdemonstrate favorable asymptotic properties. Numerical studies furtherillustrate the usefulness of the proposed method.</description>
      <author>example@mail.com (Baozhen Wang, Xingye Qiao)</author>
      <guid isPermaLink="false">2502.17744v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion</title>
      <link>http://arxiv.org/abs/2502.18042v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的基于视觉-语言模型的端到端框架VLM-E2E，旨在通过利用高级场景理解和推理能力来增强自动驾驶系统在复杂动态环境中的性能。&lt;h4&gt;背景&lt;/h4&gt;人类驾驶员可以灵活地处理复杂的驾驶情况，但当前的自动化系统难以复制这一技能，因为它们在将二维观察转换为三维空间时往往会丢失关键的语义信息。&lt;h4&gt;目的&lt;/h4&gt;利用视觉-语言模型的优势来增强自动驾驶系统的训练过程，并通过引入注意力线索和文本表示来改善其对环境的理解能力。&lt;h4&gt;方法&lt;/h4&gt;1. 采用VLM-E2E框架，将文本描述融入到鸟瞰图（BEV）特征中以提供语义监督；       2. 引入了BEV-Text可学习加权融合策略以解决多模态信息融合中的模式重要性不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过关注注意力语义，该框架能够更好地模拟人类驾驶行为，并且在nuScenes数据集上的实验结果显示其性能优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;VLM-E2E提供了一种有效的方法来改善自动驾驶系统在复杂动态环境中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human drivers adeptly navigate complex scenarios by utilizing richattentional semantics, but the current autonomous systems struggle to replicatethis ability, as they often lose critical semantic information when converting2D observations into 3D space. In this sense, it hinders their effectivedeployment in dynamic and complex environments. Leveraging the superior sceneunderstanding and reasoning abilities of Vision-Language Models (VLMs), wepropose VLM-E2E, a novel framework that uses the VLMs to enhance training byproviding attentional cues. Our method integrates textual representations intoBird's-Eye-View (BEV) features for semantic supervision, which enables themodel to learn richer feature representations that explicitly capture thedriver's attentional semantics. By focusing on attentional semantics, VLM-E2Ebetter aligns with human-like driving behavior, which is critical fornavigating dynamic and complex environments. Furthermore, we introduce aBEV-Text learnable weighted fusion strategy to address the issue of modalityimportance imbalance in fusing multimodal information. This approachdynamically balances the contributions of BEV and text features, ensuring thatthe complementary information from visual and textual modality is effectivelyutilized. By explicitly addressing the imbalance in multimodal fusion, ourmethod facilitates a more holistic and robust representation of drivingenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate itssuperiority over state-of-the-art approaches, showcasing significantimprovements in performance.</description>
      <author>example@mail.com (Pei Liu, Haipeng Liu, Haichao Liu, Xin Liu, Jinxin Ni, Jun Ma)</author>
      <guid isPermaLink="false">2502.18042v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Deep-JGAC: End-to-End Deep Joint Geometry and Attribute Compression for Dense Colored Point Clouds</title>
      <link>http://arxiv.org/abs/2502.17939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种针对密集彩色点云的深度联合几何与属性压缩框架Deep-JGAC，旨在通过利用几何和属性之间的关联来实现高效的点云压缩。&lt;h4&gt;背景&lt;/h4&gt;彩色点云在3D视觉领域中已成为基础表示。然而，庞大的数据量使得有效的点云压缩技术变得迫切需要。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时处理几何和属性信息的深度联合压缩框架Deep-JGAC，以提高压缩效率并减少存储需求。&lt;h4&gt;方法&lt;/h4&gt;{'灵活的架构设计': '该框架包括可兼容学习或非学习基元的几何与属性子编码器。', '辅助深度几何编码器': '通过融合属性信息来增强几何潜在表示，并保持解码过程不变。', '属性信息融合模块AIFM': '在几何编码过程中引入，用于融合属性信息。', '优化的颜色化模块': '为解决压缩过程中几何和属性之间的不匹配问题而设计。此模块可以提高颜色化效果并降低计算复杂性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'D1-PSNR性能指标': '相较于G-PCC、V-PCC、GRASP以及PCGCv2，Deep-JGAC分别平均减少了82.96%、36.46%、41.72%和31.16%的比特率。', 'MS-GraphSIM性能指标': '相较于G-PCC、V-PCC及IT-DL-PCC，Deep-JGAC分别平均降低了48.72%、14.67%与57.14%的比特率。', '编码和解码时间成本': '相比于V-PCC和IT-DL-PCC，该方法的编码/解码时间成本分别平均减少了94.29%/24.70%，以及96.75%/91.02%'}&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，Deep-JGAC框架在保持高质量几何重建的同时显著降低了比特率和计算复杂性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的完整中文翻译已包含于以上各个字段中&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colored point cloud becomes a fundamental representation in the realm of 3Dvision. Effective Point Cloud Compression (PCC) is urgently needed due to hugeamount of data. In this paper, we propose an end-to-end Deep Joint Geometry andAttribute point cloud Compression (Deep-JGAC) framework for dense colored pointclouds, which exploits the correlation between the geometry and attribute forhigh compression efficiency. Firstly, we propose a flexible Deep-JGACframework, where the geometry and attribute sub-encoders are compatible toeither learning or non-learning based geometry and attribute encoders.Secondly, we propose an attribute-assisted deep geometry encoder that enhancesthe geometry latent representation with the help of attribute, where thegeometry decoding remains unchanged. Moreover, Attribute Information FusionModule (AIFM) is proposed to fuse attribute information in geometry coding.Thirdly, to solve the mismatch between the point cloud geometry and attributecaused by the geometry compression distortion, we present an optimizedre-colorization module to attach the attribute to the geometrically distortedpoint cloud for attribute coding. It enhances the colorization and lowers thecomputational complexity. Extensive experimental results demonstrate that interms of the geometry quality metric D1-PSNR, the proposed Deep-JGAC achievesan average of 82.96%, 36.46%, 41.72%, and 31.16% bit-rate reductions ascompared to the state-of-the-art G-PCC, V-PCC, GRASP, and PCGCv2, respectively.In terms of perceptual joint quality metric MS-GraphSIM, the proposed Deep-JGACachieves an average of 48.72%, 14.67%, and 57.14% bit-rate reductions comparedto the G-PCC, V-PCC, and IT-DL-PCC, respectively. The encoding/decoding timecosts are also reduced by 94.29%/24.70%, and 96.75%/91.02% on average ascompared with the V-PCC and IT-DL-PCC.</description>
      <author>example@mail.com (Yun Zhang, Zixi Guo, Linwei Zhu, C. -C. Jay Kuo)</author>
      <guid isPermaLink="false">2502.17939v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs</title>
      <link>http://arxiv.org/abs/2502.17900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;多模态心电图表示学习的最新进展集中在将ECG信号与配对的自由文本报告进行对齐。然而，由于医学语言的复杂性和依赖完整的12导联设置（这种配置在资源不足的情况下经常不可用），这种对齐仍然存在次优的问题。&lt;h4&gt;目的&lt;/h4&gt;提出了一种知识增强的多模态心电图表示学习框架K-MERL，以解决上述问题。该方法利用大型语言模型从自由文本报告中提取结构化知识，并使用一种导联感知的心电图编码器和动态导联掩蔽技术来适应任意输入的导联。&lt;h4&gt;方法&lt;/h4&gt;K-MERL框架通过结合大型语言模型的知识抽取能力和特定于ECG的编码机制，旨在提高心电图数据在不同临床场景下的表示学习效果。它能够处理非标准或不完整的导联设置，使得该方法更加适用于资源受限环境中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;K-MERL在六个外部心电图数据集上的评估中，在零样本分类和线性探针任务上均达到了最先进的性能，并且在部分导联的零样本分类任务上平均比现有方法提高了16%的AUC（曲线下面积）。&lt;h4&gt;结论&lt;/h4&gt;K-MERL框架为解决资源受限条件下心电图分析的问题提供了一个有效的解决方案，通过引入结构化医学知识和改进的心电图编码技术，显著提升了模型在关键临床指标上的表现。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经包含在上述各个分点中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal ECG representation learning center on aligningECG signals with paired free-text reports. However, suboptimal alignmentpersists due to the complexity of medical language and the reliance on a full12-lead setup, which is often unavailable in under-resourced settings. Totackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECGrepresentation learning framework. **K-MERL** leverages large language modelsto extract structured knowledge from free-text reports and employs a lead-awareECG encoder with dynamic lead masking to accommodate arbitrary lead inputs.Evaluations on six external ECG datasets show that **K-MERL** achievesstate-of-the-art performance in zero-shot classification and linear probingtasks, while delivering an average **16%** AUC improvement over existingmethods in partial-lead zero-shot classification.</description>
      <author>example@mail.com (Che Liu, Cheng Ouyang, Zhongwei Wan, Haozhe Wang, Wenjia Bai, Rossella Arcucci)</author>
      <guid isPermaLink="false">2502.17900v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution</title>
      <link>http://arxiv.org/abs/2502.17880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着3D体积视频应用的流行，如自动驾驶、虚拟现实和混合现实，开发者开始使用深度学习来压缩用于视频上传的点云。这种基于深度学习的方法在效率、失真率和硬件支持方面都优于传统的MPEG和JPEG等方法。然而，这些新的技术带来了隐私威胁，尤其是重建攻击能够从中间结果中恢复原始输入点云。&lt;h4&gt;背景&lt;/h4&gt;3D体积视频应用变得越来越受欢迎，开发者开始使用深度学习来压缩用于上传的点云数据，这种新型的技术相比传统的方法具有更高的效率和更好的硬件支持。但是这样的技术也带来了一些隐私威胁，比如针对中间结果进行重建攻击。&lt;h4&gt;目的&lt;/h4&gt;设计VVRec，这是一个基于深度学习的体积视频重建攻击方案，能够从拦截传输中的中间结果中恢复高质量的点云。&lt;h4&gt;方法&lt;/h4&gt;VVRec使用了四个精心设计并训练好的神经网络模块，结合最新的潜在扩散模型和Gamma分布以及细化算法来完成高精度的重建任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过三个体积视频数据集对VVRec进行了评估。结果显示，VVRec实现了64.70dB的重建准确度，并且比基线方法减少了46.39%的失真率。&lt;h4&gt;结论&lt;/h4&gt;VVRec展示了其在点云重建中的优越性能，同时对于现有的防御措施提出了挑战。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the popularity of 3D volumetric video applications, such as AutonomousDriving, Virtual Reality, and Mixed Reality, current developers have turned todeep learning for compressing volumetric video frames, i.e., point clouds forvideo upstreaming. The latest deep learning-based solutions offer higherefficiency, lower distortion, and better hardware support compared totraditional ones like MPEG and JPEG. However, privacy threats arise, especiallyreconstruction attacks targeting to recover the original input point cloud fromthe intermediate results. In this paper, we design VVRec, to the best of ourknowledge, which is the first targeting DL-based Volumetric VideoReconstruction attack scheme. VVRec demonstrates the ability to reconstructhigh-quality point clouds from intercepted transmission intermediate resultsusing four well-trained neural network modules we design. Leveraging the latestlatent diffusion models with Gamma distribution and a refinement algorithm,VVRec excels in reconstruction quality, color recovery, and surpasses existingdefenses. We evaluate VVRec using three volumetric video datasets. The resultsdemonstrate that VVRec achieves 64.70dB reconstruction accuracy, with animpressive 46.39% reduction of distortion over baselines.</description>
      <author>example@mail.com (Rui Lu, Bihai Zhang, Dan Wang)</author>
      <guid isPermaLink="false">2502.17880v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>BRIDO: Bringing Democratic Order to Abstractive Summarization</title>
      <link>http://arxiv.org/abs/2502.18342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法来减少大型语言模型在抽象文本摘要中的hallucination（幻觉）问题。&lt;h4&gt;背景&lt;/h4&gt;虽然大型语言模型在许多任务中展现出巨大潜力，但幻觉问题仍然是其实用性的主要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;通过减轻暴露偏差来解决抽象文本摘要中的幻觉问题。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种使用对比学习的方法，该方法的目标是减少候选输出中的幻觉内容。这种方法假设包含幻觉的候选输出在一组候选输出中占少数，并且与其他候选人相比具有较低的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在XSum和CNN/DM摘要数据集上，所提方法相对于现有模型BRIO分别提高了6.25%和3.82%的一致性G-Eval得分。&lt;h4&gt;结论&lt;/h4&gt;利用对比学习策略可以有效减少大型语言模型生成的文本中的幻觉问题。&lt;h4&gt;翻译&lt;/h4&gt;该论文旨在通过改进现有的针对暴露偏差的方法来减轻摘要中出现的不准确、无关或不一致的内容，进而提高大语言模型在抽象文本总结任务上的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hallucination refers to the inaccurate, irrelevant, and inconsistent textgenerated from large language models (LLMs). While the LLMs have shown greatpromise in a variety of tasks, the issue of hallucination still remains a majorchallenge for many practical uses. In this paper, we tackle the issue ofhallucination in abstract text summarization by mitigating exposure bias.Existing models targeted for exposure bias mitigation, namely BRIO, aim forbetter summarization quality in the ROUGE score. We propose a model that uses asimilar exposure bias mitigation strategy but with a goal that is aligned withless hallucination. We conjecture that among a group of candidate outputs, oneswith hallucinations will comprise the minority of the whole group. That is,candidates with less similarity with others will have a higher chance ofcontaining hallucinated content. Our method uses this aspect and utilizescontrastive learning, incentivizing candidates with high inter-candidate ROUGEscores. We performed experiments on the XSum and CNN/DM summarization datasets,and our method showed 6.25% and 3.82% improvement, respectively, on theconsistency G-Eval score over BRIO.</description>
      <author>example@mail.com (Junhyun Lee, Harshith Goka, Hyeonmok Ko)</author>
      <guid isPermaLink="false">2502.18342v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A graph neural network-based multispectral-view learning model for diabetic macular ischemia detection from color fundus photographs</title>
      <link>http://arxiv.org/abs/2502.17886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的多光谱视图学习模型，用于从彩色眼底照片中检测糖尿病性黄斑缺血(DMI)。&lt;h4&gt;背景&lt;/h4&gt;尽管通过人工智能(AI)结合彩色眼底照片(CFPs)在检测各种眼科疾病（包括糖尿病视网膜病变）方面得到了广泛应用，但CFPs在DMI检测中的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图神经网络的多光谱视图学习(GNN-MSVL)模型来从彩色眼底照片中检测糖尿病性黄斑缺血(DMI)。&lt;h4&gt;方法&lt;/h4&gt;该模型首先通过计算多光谱成像(CMI)重建24波长的眼底图像。使用ResNeXt101作为骨干网络进行多视图学习，提取重建图像的特征。此外，设计了一个带有定制跳跃连接策略的GNN以增强跨光谱关系。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在眼水平上的准确率为84.7%，AUROC为0.900（95% CI: 0.852-0.937），优于单纯基于CFPs训练的基本模型和人类专家，p值小于0.01。&lt;h4&gt;结论&lt;/h4&gt;AI驱动的彩色眼底照片分析有望用于早期、低成本地筛查DMI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic macular ischemia (DMI), marked by the loss of retinal capillaries inthe macular area, contributes to vision impairment in patients with diabetes.Although color fundus photographs (CFPs), combined with artificial intelligence(AI), have been extensively applied in detecting various eye diseases,including diabetic retinopathy (DR), their applications in detecting DMI remainunexplored, partly due to skepticism among ophthalmologists regarding itsfeasibility. In this study, we propose a graph neural network-basedmultispectral view learning (GNN-MSVL) model designed to detect DMI from CFPs.The model leverages higher spectral resolution to capture subtle changes infundus reflectance caused by ischemic tissue, enhancing sensitivity toDMI-related features. The proposed approach begins with computationalmultispectral imaging (CMI) to reconstruct 24-wavelength multispectral fundusimages from CFPs. ResNeXt101 is employed as the backbone for multi-viewlearning to extract features from the reconstructed images. Additionally, a GNNwith a customized jumper connection strategy is designed to enhancecross-spectral relationships, facilitating comprehensive and efficientmultispectral view learning. The study included a total of 1,078macula-centered CFPs from 1,078 eyes of 592 patients with diabetes, of which530 CFPs from 530 eyes of 300 patients were diagnosed with DMI. The modelachieved an accuracy of 84.7 percent and an area under the receiver operatingcharacteristic curve (AUROC) of 0.900 (95 percent CI: 0.852-0.937) oneye-level, outperforming both the baseline model trained from CFPs and humanexperts (p-values less than 0.01). These findings suggest that AI-based CFPanalysis holds promise for detecting DMI, contributing to its early andlow-cost screening.</description>
      <author>example@mail.com (Qinghua He, Hongyang Jiang, Danqi Fang, Dawei Yang, Truong X. Nguyen, Anran Ran, Clement C. Tham, Simon K. H. Szeto, Sobha Sivaprasad, Carol Y. Cheung)</author>
      <guid isPermaLink="false">2502.17886v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>On-device edge learning for IoT data streams: a survey</title>
      <link>http://arxiv.org/abs/2502.17788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;这篇文献综述探讨了在神经网络和决策树分类任务的智能环境中进行连续学习方法的研究。文章重点关注数据架构（批量 vs 流式）和网络容量（云端 vs 边缘设备）对TinyML算法设计的影响，这是因为自然到达的数据流是不受控制的。&lt;h4&gt;背景&lt;/h4&gt;部署深度学习模型在资源受限的边缘设备上面临挑战，包括灾难性遗忘、数据低效性和处理IoT表格数据于开放世界环境中的困难。决策树虽然更节省内存，但其表达能力有限，需要动态适应（如剪枝和元学习）来处理复杂模式和概念漂移。&lt;h4&gt;目的&lt;/h4&gt;强调为边缘应用定制多指标性能评估的重要性，这些评估不仅考虑输出基础的度量还考虑内部表示的度量。主要挑战在于将这些构建块整合到自适应在线系统中时需兼顾稳定性与可塑性、正向反向迁移以及模型收敛。&lt;h4&gt;方法&lt;/h4&gt;综述详细介绍了在资源受限边缘设备上部署深度学习所面临的各种挑战，并总结了决策树和神经网络各自的特点，指出需要结合多种技术来应对持续变化的环境。&lt;h4&gt;主要发现&lt;/h4&gt;连续学习对于边缘设备上的TinyML算法设计至关重要。尽管决策树具有内存效率优势但其灵活性不如神经网络。因此，在处理复杂模式时，可能需要额外的技术手段如动态适应和元学习机制。&lt;h4&gt;结论&lt;/h4&gt;为了有效利用资源受限的边缘设备进行深度学习任务，需开发新的连续学习方法来优化模型性能、减少灾难性遗忘，并提高对概念漂移的响应能力。&lt;h4&gt;翻译&lt;/h4&gt;此文献综述探索了在神经网络（NN）和决策树（DT）分类任务中的智能环境中为设备上训练而采用的连续学习方法。重点强调数据架构（批量 vs 流式）与网络容量（云端 vs 边缘）对TinyML算法设计的影响，这是由于自然到达的数据流是不受控制的。综述详细描述了在资源受限边缘设备上部署深度学习所面临的挑战，包括灾难性遗忘、数据低效性和处理IoT表格数据于开放世界环境中的困难。虽然决策树对于设备上的训练更节省内存，但其表达能力有限，需要动态适应（如剪枝和元学习）来处理复杂模式和概念漂移。强调了为边缘应用定制多指标性能评估的重要性，这些评估不仅考虑输出基础的度量还考虑内部表示的度量。关键挑战在于将这些构建块整合到自适应在线系统中时需兼顾稳定性与可塑性、正向反向迁移以及模型收敛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This literature review explores continual learning methods for on-devicetraining in the context of neural networks (NNs) and decision trees (DTs) forclassification tasks on smart environments. We highlight key constraints, suchas data architecture (batch vs. stream) and network capacity (cloud vs. edge),which impact TinyML algorithm design, due to the uncontrolled natural arrivalof data streams. The survey details the challenges of deploying deep learnerson resource-constrained edge devices, including catastrophic forgetting, datainefficiency, and the difficulty of handling IoT tabular data in open-worldsettings. While decision trees are more memory-efficient for on-devicetraining, they are limited in expressiveness, requiring dynamic adaptations,like pruning and meta-learning, to handle complex patterns and concept drifts.We emphasize the importance of multi-criteria performance evaluation tailoredto edge applications, which assess both output-based and internalrepresentation metrics. The key challenge lies in integrating these buildingblocks into autonomous online systems, taking into account stability-plasticitytrade-offs, forward-backward transfer, and model convergence.</description>
      <author>example@mail.com (Afonso Lourenço, João Rodrigo, João Gama, Goreti Marreiros)</author>
      <guid isPermaLink="false">2502.17788v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Task-Agnostic Semantic Communication with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.18200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个无任务依赖的语义通信框架SemCLIP，基于对比语言-图像预训练模型CLIP。通过传输CLIP生成的图像令牌而非原始图像，在低带宽和挑战性信道条件下实现高效的语义通信。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数语义通信系统使用深度联合源信道编码来以目标导向的方式对特定任务进行语义编码，但这限制了它们在实际部署中的灵活性和泛化能力。多模态基础模型通过生成通用的语义令牌提供了潜在解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无任务依赖的语义通信框架SemCLIP，利用对比语言-图像预训练模型（CLIP）来实现高效、灵活且鲁棒的语义通信系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用CLIP生成的图像令牌代替原始图像进行传输。2. 设计了一种深度联合源信道编码方案以高效地对CLIP令牌进行编码。3. 在接收端设计了一个多模态传输感知提示学习机制，该机制根据传输质量调整提示，增强了系统的鲁棒性和信道适应性。&lt;h4&gt;主要发现&lt;/h4&gt;1. SemCLIP在低信号噪声比下实现了零样本准确性提高41%的性能优于基线模型。2. 与不同的图像传输方法相比，SemCLIP减少了超过50倍的带宽使用量。&lt;h4&gt;结论&lt;/h4&gt;通过利用基础模型和无任务依赖的方法，展示了语义通信系统在实际部署中的潜力，并为未来的通用、无任务依赖的语义通信解决方案开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing semantic communication (SemCom) systems use deep jointsource-channel coding (DeepJSCC) to encode task-specific semantics in agoal-oriented manner. However, their reliance on predefined tasks and datasetssignificantly limits their flexibility and generalizability in practicaldeployments. Multi-modal foundation models provide a promising solution bygenerating universal semantic tokens. Inspired by this, we introduce SemCLIP, atask-agnostic SemCom framework leveraging the contrastive language-imagepre-training (CLIP) model. By transmitting CLIP-generated image tokens insteadof raw images, SemCLIP enables efficient semantic communications under lowbandwidth and challenging channel conditions, facilitating diverse downstreamtasks and zero-shot applications. Specifically, we propose a DeepJSCC schemefor efficient CLIP tokens encoding. To mitigate potential degradation caused bycompression and channel noise, a multi-modal transmission-aware prompt learningmechanism is designed at the receiver, which adapts prompts based ontransmission quality, enhancing system robustness and channel adaptability.Simulation results demonstrate that SemCLIP outperforms the baselines,achieving a $41\%$ improvement in zero-shot accuracy at a low signal-to-noiseratio. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-foldcompared to different image transmission methods, demonstrating the potentialof foundation models towards a generalized, task-agnostic SemCom solution.</description>
      <author>example@mail.com (Jiangjing Hu, Haotian Wu, Wenjing Zhang, Fengyu Wang, Wenjun Xu, Hui Gao, Deniz Gündüz)</author>
      <guid isPermaLink="false">2502.18200v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的多模态预训练模型UniGS，通过将3D高斯点集（3D Gaussian Splatting）引入到文本、图像和三维空间的联合表示学习中，以改进当前基于离散点云的方法。&lt;h4&gt;背景&lt;/h4&gt;现有技术在多模态3D预训练方法取得了显著效果，但用离散点来表达复杂的3D世界存在局限性，无法完全捕捉到2D像素与3D结构之间的联系。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了3D高斯点集的多模态预训练框架UniGS，以增强从文本、图像和三维空间中提取联合表示的能力。&lt;h4&gt;方法&lt;/h4&gt;首先通过3D高斯点集来建模彩色和透明度信息；接着利用预训练的视觉语言模型建立共享的视觉和文本空间，并引入一个新型模块（Gaussian-Aware Guidance）帮助学习更细粒度的3D表示，促进跨模式对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在不同数据集上进行广泛的实验表明，UniGS能够更好地捕捉到多模态信息之间的关系，尤其在零样本分类、基于文本驱动检索以及开放世界理解任务中表现优异，超越了之前的SOTA模型（如Uni3D）。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的建模方式和学习机制，UniGS提供了一种更为通用且更强的跨模态表示方法。&lt;h4&gt;翻译&lt;/h4&gt;最近在多模态3D预训练方法上的进展已经在文本、图像及点云联合表示的学习方面展现了有前景的效果。然而，仅采用点云作为三维表现形式未能充分捕捉到复杂三维世界的细微差别，并且在离散点与密集2D像素间存在明显的差距。为解决这一问题，我们提出了一种名为UniGS的方法，将3D高斯撒播（3D Gaussian Splatting）整合进多模态预训练中以提升三维表现形式。首先，我们使用3D GS表示来模拟三维世界作为一系列带有颜色和透明度的3D高斯分布体，在保留所有3D场景信息的同时建立起与2D图像之间的紧密联系。然后为了实现跨语言、图象及3D模态学习，UniGS从预训练的视觉文本模型开始，通过大量的现实世界图像-文本对建立共享的视觉和文字空间。接下来，UniGS采用一个三维编码器将优化后的3D GS与语言-图像表示进行一致化以获取统一多模态表示。为了帮助提取全球性的明确3D特征并通过更好的跨模式对齐实现这一点，我们进一步引入了新的Gaussian-Aware Guidance模块来引导学习细粒度的三维领域的表示。通过在Objaverse、ABO、MVImgNet和SUN RGBD数据集上进行广泛的零样本分类、基于文本驱动检索和开放世界理解任务实验，我们证明了UniGS在学习更通用和更好对齐的多模态表现形式方面的有效性。具体来说，在各种3D任务中，包括零样本分类（+9.36%）、基于文本驱动检索（+4.3%）和开放世界理解（+7.92%），与之前的SOTA模型Uni3D相比，UniGS取得了领先的成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in multi-modal 3D pre-training methods have shownpromising efficacy in learning joint representations of text, images, and pointclouds. However, adopting point clouds as 3D representation fails to fullycapture the intricacies of the 3D world and exhibits a noticeable gap betweenthe discrete points and the dense 2D pixels of images. To tackle this issue, wepropose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modalpre-training to enhance the 3D representation. We first rely on the 3DGSrepresentation to model the 3D world as a collection of 3D Gaussians with colorand opacity, incorporating all the information of the 3D scene whileestablishing a strong connection with 2D images. Then, to achieveLanguage-Image-3D pertaining, UniGS starts with a pre-trained vision-languagemodel to establish a shared visual and textual space through extensivereal-world image-text pairs. Subsequently, UniGS employs a 3D encoder to alignthe optimized 3DGS with the Language-Image representations to learn unifiedmulti-modal representations. To facilitate the extraction of global explicit 3Dfeatures by the 3D encoder and achieve better cross-modal alignment, weadditionally introduce a novel Gaussian-Aware Guidance module that guides thelearning of fine-grained representations of the 3D domain. Through extensiveexperiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets withzero-shot classification, text-driven retrieval and open-world understandingtasks, we demonstrate the effectiveness of UniGS in learning a more general andstronger aligned multi-modal representation. Specifically, UniGS achievesleading results across different 3D tasks with remarkable improvements overprevious SOTA, Uni3D, including on zero-shot classification (+9.36%),text-driven retrieval (+4.3%) and open-world understanding (+7.92%).</description>
      <author>example@mail.com (Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.17860v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Progressive Local Alignment for Medical Multimodal Pre-training</title>
      <link>http://arxiv.org/abs/2502.18047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Progressive Local Alignment Network (PLAN)的新型网络，用于提高医学图像与文本之间的局部对齐精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在医疗影像诊断中，准确地将文本描述与对应的像素区域进行配对是一个挑战，因为传统的硬边界方法存在不确定性，并且难以处理不规则结构。&lt;h4&gt;目的&lt;/h4&gt;通过设计一种基于对比学习的新方法来建立有意义的词-像素关系，并采用渐进式学习策略迭代优化这些关系，以提高软区域识别的效果并减少噪声干扰。&lt;h4&gt;方法&lt;/h4&gt;PLAN网络利用了对比学习和渐进式学习技术来改进局部对齐，提升医学图像与文本之间的关联性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PLAN在短语定位、图文检索、目标检测及零样本分类等多个任务上超越了现有的最佳方法，确立了一个新的基准。&lt;h4&gt;结论&lt;/h4&gt;PLAN网络为医疗图像和文本配准提供了一种有效的方法，提高了软区域识别的准确性和鲁棒性，并且能够抑制噪声干扰。&lt;h4&gt;翻译&lt;/h4&gt;本地医学影像与文字之间的对齐对于精确诊断至关重要，尽管由于缺乏自然局部配对和刚性区域识别方法的局限性而仍然具有挑战性。传统方法依赖于硬边界，引入了不确定性，然而医疗成像需要灵活处理不规则结构的软区域识别方法。为了克服这些挑战，我们提出了渐进式本地对齐网络（PLAN），它设计了一种基于对比学习的新局部对齐方法来建立有意义的文字-像素关系，并采用逐步学习策略迭代优化这些关系，以提高配准精度和鲁棒性。通过结合这些技术，PLAN有效地提高了软区域识别的效果同时抑制了噪声干扰。在多个医疗数据集上的大量实验表明，PLAN在短语定位、图像文字检索、目标检测和零样本分类任务上超越了最先进的方法，为医学影像-文本对齐设定了新的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Local alignment between medical images and text is essential for accuratediagnosis, though it remains challenging due to the absence of natural localpairings and the limitations of rigid region recognition methods. Traditionalapproaches rely on hard boundaries, which introduce uncertainty, whereasmedical imaging demands flexible soft region recognition to handle irregularstructures. To overcome these challenges, we propose the Progressive LocalAlignment Network (PLAN), which designs a novel contrastive learning-basedapproach for local alignment to establish meaningful word-pixel relationshipsand introduces a progressive learning strategy to iteratively refine theserelationships, enhancing alignment precision and robustness. By combining thesetechniques, PLAN effectively improves soft region recognition while suppressingnoise interference. Extensive experiments on multiple medical datasetsdemonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,image-text retrieval, object detection, and zero-shot classification, setting anew benchmark for medical image-text alignment.</description>
      <author>example@mail.com (Huimin Yan, Xian Yang, Liang Bai, Jiye Liang)</author>
      <guid isPermaLink="false">2502.18047v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning</title>
      <link>http://arxiv.org/abs/2502.17874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;分子机器学习由于几何深度学习的进步而变得流行。同时，检索增强生成已经成为语言模型中常用的一种基本原则方法。然而，如何将检索增强有效地整合到分子机器学习中仍然不清楚。&lt;h4&gt;背景&lt;/h4&gt;随着几何深度学习的发展，分子机器学习变得越来越受欢迎。与此同时，检索增强生成作为一种基本的方法被广泛应用于语言模型中。&lt;h4&gt;目的&lt;/h4&gt;探讨和实现一种有效的方式，即通过神经图匹配来改进分子机器学习中的查询-检索问题。&lt;h4&gt;方法&lt;/h4&gt;引入MARASON模型，该模型结合了神经图匹配技术以提升基于碎片化的神经网络性能。具体来说，它采用了噪声鲁棒的、端到端的神经网络来学习结构相似性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，MARASON模型在质量指标上取得了显著的进步，比最佳非检索方法高出28%（Top-1准确率从19%提升到了47%），并且超过了简单检索增强生成和传统图匹配方法的性能。&lt;h4&gt;结论&lt;/h4&gt;神经图匹配技术能够有效地帮助分子机器学习中更好地理解和应用结构对齐。MARASON模型在质量指标上展示了显著的优势，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;分子机器学习随着几何深度学习的进步而日益受到关注。同时，检索增强生成已经成为语言模型中的标准方法之一。然而，在分子机器学习中如何最优地集成检索增强技术仍不清楚。图神经网络可以从巧妙的匹配中受益，以理解检索到的分子与查询分子之间的结构对齐情况。通过显式建模两个结构图形间的节点和边亲和度，并利用噪声鲁棒、端到端的神经网络来学习相似性指标，神经图匹配提供了一种有竞争力的解决方案。我们将这种方法应用于质谱模拟并提出MARASON模型——一种引入神经图匹配技术以增强基于碎片化的神经网络的新方法。实验结果表明我们的设计是有效的：MARASON在Top-1准确率上达到了47%，比现有的最佳非检索基准提高了28%（后者为19%）。此外，MARASON优于简单的检索增强生成方法和传统的图匹配方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular machine learning has gained popularity with the advancements ofgeometric deep learning. In parallel, retrieval-augmented generation has becomea principled approach commonly used with language models. However, the optimalintegration of retrieval augmentation into molecular machine learning remainsunclear. Graph neural networks stand to benefit from clever matching tounderstand the structural alignment of retrieved molecules to a query molecule.Neural graph matching offers a compelling solution by explicitly modeling nodeand edge affinities between two structural graphs while employing anoise-robust, end-to-end neural network to learn affinity metrics. We applythis approach to mass spectrum simulation and introduce MARASON, a novel modelthat incorporates neural graph matching to enhance a fragmentation-based neuralnetwork. Experimental results highlight the effectiveness of our design, withMARASON achieving 28% top-1 accuracy, a substantial improvement over thenon-retrieval state-of-the-art accuracy of 19%. Moreover, MARASON outperformsboth naive retrieval-augmented generation methods and traditional graphmatching approaches.</description>
      <author>example@mail.com (Runzhong Wang, Rui-Xi Wang, Mrunali Manjrekar, Connor W. Coley)</author>
      <guid isPermaLink="false">2502.17874v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Agnostic calculation of atomic free energies with the descriptor density of states</title>
      <link>http://arxiv.org/abs/2502.18191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的评估原子系统振动自由能的方法，无需预先指定相互作用势函数。该方法通过描述符密度的熵来估计高维分布，并利用条件分数匹配进行计算。&lt;h4&gt;背景&lt;/h4&gt;传统的振动能量计算依赖于特定的相互作用势模型，限制了应用范围和灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用评估框架，能够跨各种原子系统精确预测自由能而无需具体了解相互作用势函数。&lt;h4&gt;方法&lt;/h4&gt;通过描述符密度的状态，引入高维特征向量进行振动自由能估计；利用条件分数匹配技术避免复杂的高维积分计算。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在微秒级别的CPU时间内返回可导的自由能预测值，并支持潜在参数变化的快速前向和反向传播。测试表明，所提出的模型独立估算器能够通过热力学集成法在多种金属相（如BCC, FCC 和 A15）中达到1-2 meV/原子精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为不确定性量化、逆设计及其他计算科学领域的高维积分问题提供了有效的解决方案，并展示了其强大的应用潜力，特别是在液体系统和基础模型的微调方面。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新方法来评估无预先指定相互作用势函数条件下的原子体系振动自由能。我们的模型无关方式利用描述符——原子结构的高维特征向量。通过条件分数匹配准确估算高维密度熵，将相互作用势函数转化为在描述符特性上扩展的形式。我们展示了自由能作为描述符熵的Legendre-Fenchel共轭形式出现，从而避免了所有高维积分计算。该分数匹配活动比固定模型采样需要更少资源且高度并行化，使得耗时减少到几分钟，并通过张量压缩方案实现轻量化存储。我们的模型无关估计器能够以微秒级的CPU努力返回广泛的潜在参数范围内的可导自由能预测值，支持热力学模拟中快速传播势变差异及误差，这是进行不确定性量化和逆设计长期追求的目标。我们测试了在高同质温度下W、Mo和Fe的BCC、FCC和A15相模型中的预测结果与热力学集成计算的一致性，并通过反向传播微调非磁机器学习模型中Fe的alpha-gamma过渡温度，无需额外采样。讨论了该方法在液体体系及基础模型微调等领域的应用以及其他估计高维积分问题的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to evaluate vibrational free energies of atomicsystems without a priori specification of an interatomic potential. Ourmodel-agnostic approach leverages descriptors, high-dimensional feature vectorsof atomic structure. The entropy of a high-dimensional density, the descriptordensity of states, is accurately estimated with conditional score matching.Casting interatomic potentials into a form extensive in descriptor features, weshow free energies emerge as the Legendre-Fenchel conjugate of the descriptorentropy, avoiding all high-dimensional integration. The score matching campaignrequires less resources than fixed-model sampling and is highly parallel,reducing wall time to a few minutes, with tensor compression schemes allowinglightweight storage. Our model-agnostic estimator returns differentiable freeenergy predictions over a broad range of potential parameters in microsecondsof CPU effort, allowing rapid forward and back propagation of potentialvariations through finite temperature simulations, long desired for uncertaintyquantification and inverse design. We test predictions against thermodynamicintegration calculations over a broad range of models for BCC, FCC and A15phases of W, Mo and Fe at high homologous temperatures. Predictions pass thestringent accuracy threshold of 1-2 meV/atom (1/40-1/20 kcal/mol) for phaseprediction with propagated score uncertainties robustly bounding errors. Wealso demonstrate targeted fine-tuning, reducing the alpha-gamma transitiontemperature in a non-magnetic machine learning model of Fe from 2030 K to 1063K through back-propagation, with no additional sampling. Applications toliquids and fine-tuning foundational models are discussed along with the manyproblems in computational science which estimate high-dimensional integrals.</description>
      <author>example@mail.com (Thomas D Swinburne, Clovis Lapointe, Mihai-Cosmin Marinica)</author>
      <guid isPermaLink="false">2502.18191v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CPVis: Evidence-based Multimodal Learning Analytics for Evaluation in Collaborative Programming</title>
      <link>http://arxiv.org/abs/2502.17835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为CPVis的互动式可视化分析系统，旨在帮助编程教育中的教师评估学生的合作学习情况。&lt;h4&gt;背景&lt;/h4&gt;随着编程教育的发展，越来越多非计算机专业的学生开始接触编程。在这种背景下，协作编程成为了一种有效的教学方法，但也给教师带来了管理上的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态评估学生合作能力和个人表现的系统，以解决现有条件下教师难以全面监控和评价学生的问题。&lt;h4&gt;方法&lt;/h4&gt;收集实际场景中的多模态数据，并利用这些数据创建了一个基于花型编码的独特视觉表示形式。CPVis提供了时间序列视图来展示协作行为的发展趋势。&lt;h4&gt;主要发现&lt;/h4&gt;通过与两个基准系统的对比实验（N=22），参与者在使用CPVis时报告获得了更多的洞察力，感觉可视化更加直观，并且对其合作评估的自信度有所提高。&lt;h4&gt;结论&lt;/h4&gt;CPVis提供了一种有效的方法来帮助教师动态地评估学生的协作编程表现。这种系统有助于提高教育质量和学生的学习体验。&lt;h4&gt;翻译&lt;/h4&gt;随着编程教育变得越来越普及，越来越多非计算机专业的大学生开始学习编程。然而，在有限的教学时间和注意力范围内，教师难以监控和评估团队或个人的进步与成绩。为了解决这个问题，研究人员收集了现实世界中的多模态数据，并开发了一种名为CPVis的互动式可视化分析系统来动态地评价学生合作。该系统允许教师高效地评估小组和个人的表现，并采用新颖的花卉编码视觉表现形式展示成绩和提供基于时间视图捕捉协作行为的发展趋势。实验结果表明，用户在使用CPVis时获得了更多的洞见、发现可视化更为直观且增加了他们对评估信心。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As programming education becomes more widespread, many college students fromnon-computer science backgrounds begin learning programming. Collaborativeprogramming emerges as an effective method for instructors to support novicestudents in developing coding and teamwork abilities. However, due to limitedclass time and attention, instructors face challenges in monitoring andevaluating the progress and performance of groups or individuals. To addressthis issue, we collect multimodal data from real-world settings and developCPVis, an interactive visual analytics system designed to assess studentcollaboration dynamically. Specifically, CPVis enables instructors to evaluateboth group and individual performance efficiently. CPVis employs a novelflower-based visual encoding to represent performance and provides time-basedviews to capture the evolution of collaborative behaviors. A within-subjectexperiment (N=22), comparing CPVis with two baseline systems, reveals thatusers gain more insights, find the visualization more intuitive, and reportincreased confidence in their assessments of collaboration.</description>
      <author>example@mail.com (Gefei Zhang, Shenming Ji, Yicao Li, Jingwei Tang, Jihong Ding, Meng Xia, Guodao Sun, Ronghua Liang)</author>
      <guid isPermaLink="false">2502.17835v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Nasty Noise</title>
      <link>http://arxiv.org/abs/2502.17872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了在存在恶意干扰的情况下对比学习的理论极限，并通过PAC学习和VC维数分析建立了对抗设置下的样本复杂度上下界。&lt;h4&gt;背景&lt;/h4&gt;对比学习作为一种自监督表征学习的强大范式，其性能受到训练数据中可能存在恶意噪声的影响。这种情况下，训练样本可能会被修改或替换。&lt;h4&gt;目的&lt;/h4&gt;探讨并确定在存在恶意干扰的情况下，对比学习的理论极限以及样本复杂度的上界和下界。&lt;h4&gt;方法&lt;/h4&gt;利用PAC（概率可近似）学习框架和VC维数分析来建立对抗设置下的对比学习模型的样本复杂度上下限。此外还根据l2距离函数导出了基于数据相关的样本复杂度界限。&lt;h4&gt;主要发现&lt;/h4&gt;在存在恶意噪声的情况下，可以通过使用PAC理论和VC维数分析来限定对比学习算法的有效性及所需的最小训练样本数量。同时证明了特定条件下，利用l2距离作为损失函数可以优化样本选择过程。&lt;h4&gt;结论&lt;/h4&gt;通过理论分析揭示了对抗环境中对比学习面临的挑战，并提供了改进模型性能的新视角和方法论建议。&lt;h4&gt;翻译&lt;/h4&gt;对比学习已经成为自监督表示学习的强大范式。这项工作在存在恶意噪声的背景下探讨了对比学习的理论极限，其中对手可能会修改或替换训练样本。利用PAC学习以及VC维数分析，在对抗环境下建立了样本复杂度的上下界，并且基于l2距离函数推导出数据依赖性的样本复杂度界限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has emerged as a powerful paradigm for self-supervisedrepresentation learning. This work analyzes the theoretical limits ofcontrastive learning under nasty noise, where an adversary modifies or replacestraining samples. Using PAC learning and VC-dimension analysis, lower and upperbounds on sample complexity in adversarial settings are established.Additionally, data-dependent sample complexity bounds based on the l2-distancefunction are derived.</description>
      <author>example@mail.com (Ziruo Zhao)</author>
      <guid isPermaLink="false">2502.17872v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Label-free Prediction of Vascular Connectivity in Perfused Microvascular Networks in vitro</title>
      <link>http://arxiv.org/abs/2502.17759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的无标记血管连接网络（VC-Net），该网络可以连续监测和评估微血管的连通性，无需荧光标签即可进行测量。&lt;h4&gt;背景&lt;/h4&gt;现有的微血管连接评估方法大多依赖于可能引起生物相容性问题或干扰细胞正常生长过程的荧光标签。&lt;h4&gt;目的&lt;/h4&gt;开发一种无标记的方法来持续且非侵入地监测和评估组织培养中的微血管连通性，以改善器官体外培养及优化治疗策略。&lt;h4&gt;方法&lt;/h4&gt;使用Vessel Queue Contrastive Learning (VQCL) 方法结合解决样本量有限、类别特征不明显以及类别分布不平衡问题的算法构建VC-Net。通过采集不同培养条件下生成的微血管网络(MVN) 的显微图像作为训练数据集来验证该模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. VC-Net 能够成功评估微血管连通性，其结果与荧光成像方法的结果无显著差异。2. VC-Net 成功区分了正常和肿瘤相关的MVN之间的连接特性。在与常规培养环境中的相比，肿瘤相关环境中培养的平均血管连接降低了30.8%，而未连接区域增加了37.3%。&lt;h4&gt;结论&lt;/h4&gt;这项研究为无标记、连续评估体外培养器官或肿瘤的血管化提供了一种新的途径。&lt;h4&gt;翻译&lt;/h4&gt;持续监测和原位评估微血管连通性在培养血管化的类器官以及优化治疗策略方面具有重要意义。然而，常用的血管连接评估方法严重依赖于荧光标签，这些荧光标签可能会引发生物相容性问题或干扰正常的细胞生长过程。为了解决这个问题，开发了一种无标记的血管连接网络（VC-Net），用于评估微血管连通性。通过在不同培养条件下采集体外培养的微血管网络（MVN）的显微图像作为训练数据集来验证该模型的有效性。VC-Net 使用了Vessel Queue Contrastive Learning (VQCL) 方法和处理样本量有限、类别特征不明显以及类别分布不平衡问题的算法。VC-Net 成功评估了血管连通性，并且结果与荧光成像方法的结果没有显著差异。此外，该研究还展示了VC-Net 能够区分正常和肿瘤相关的MVN之间的连接特性。相比常规培养环境中的情况，在肿瘤相关环境中培养的平均血管连接降低了30.8%，而未连接区域增加了37.3%。这项工作为体外无标记且连续地评估类器官或肿瘤的血管化提供了一种新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous monitoring and in-situ assessment of microvascular connectivityhave significant implications for culturing vascularized organoids andoptimizing the therapeutic strategies. However, commonly used methods forvascular connectivity assessment heavily rely on fluorescent labels that mayeither raise biocompatibility concerns or interrupt the normal cell growthprocess. To address this issue, a Vessel Connectivity Network (VC-Net) wasdeveloped for label-free assessment of vascular connectivity. To validate theVC-Net, microvascular networks (MVNs) were cultured in vitro and theirmicroscopic images were acquired at different culturing conditions as atraining dataset. The VC-Net employs a Vessel Queue Contrastive Learning (VQCL)method and a class imbalance algorithm to address the issues of limited samplesize, indistinctive class features and imbalanced class distribution in thedataset. The VC-Net successfully evaluated the vascular connectivity with nosignificant deviation from that by fluorescence imaging. In addition, theproposed VC-Net successfully differentiated the connectivity characteristicsbetween normal and tumor-related MVNs. In comparison with those cultured in theregular microenvironment, the averaged connectivity of MVNs cultured in thetumor-related microenvironment decreased by 30.8%, whereas the non-connectedarea increased by 37.3%. This study provides a new avenue for label-free andcontinuous assessment of organoid or tumor vascularization in vitro.</description>
      <author>example@mail.com (Liang Xu, Pengwu Song, Shilu Zhu, Yang Zhang, Ru Zhang, Zhiyuan Zheng, Qingdong Zhang, Jie Gao, Chen Han, Mingzhai Sun, Peng Yao, Min Ye, Ronald X. Xu)</author>
      <guid isPermaLink="false">2502.17759v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CLEP-GAN: An Innovative Approach to Subject-Independent ECG Reconstruction from PPG Signals</title>
      <link>http://arxiv.org/abs/2502.17536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决从PPG信号重建未见过的ECG信号的挑战，提出了一个使用ODE模型生成合成ECG-PPG数据的方法，并开发了一种基于对比学习、对抗学习和注意门控的新建模方法。&lt;h4&gt;背景&lt;/h4&gt;非侵入性心脏监测需要通过PPG信号重建ECG信号。现有的公共ECG-PPG数据集缺乏多样性，且收集过程中的噪音使得ECG的重建变得复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种增强训练多样性的合成ECG-PPG数据生成技术，并开发出一个独立于受试者的PPG到ECG的重建模型。&lt;h4&gt;方法&lt;/h4&gt;使用ODE模型来生成合成的ECG-PPG数据，为机器学习模型提供更多的训练样本。开发了一种结合对比学习、对抗学习和注意力门控的新建模方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在未见过的数据上的重构效果与现有方法相当甚至更好，并强调了在模型训练和数据集扩充时考虑人口统计数据多样性的必要性。&lt;h4&gt;结论&lt;/h4&gt;通过引入新颖的数据生成技术和改进的重建模型，可以提高ECG信号从PPG信号的准确性。研究结果表明，应重视性别、年龄等人口统计学因素对重构精度的影响。&lt;h4&gt;翻译&lt;/h4&gt;这项研究解决了利用PPG信号重建未见过的ECG信号的问题，并提出了一种新的合成ECG-PPG数据生成技术以及一种新颖的独立于受试者的PPG到ECG的重建模型，该模型结合了对比学习、对抗学习和注意力门控。研究表明这些方法能够改善重构效果并强调考虑人口统计数据的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenge of reconstructing unseen ECG signals fromPPG signals, a critical task for non-invasive cardiac monitoring. Whilenumerous public ECG-PPG datasets are available, they lack the diversity seen inimage datasets, and data collection processes often introduce noise,complicating ECG reconstruction from PPG even with advanced machine learningmodels. To tackle these challenges, we first introduce a novel syntheticECG-PPG data generation technique using an ODE model to enhance trainingdiversity. Next, we develop a novel subject-independent PPG-to-ECGreconstruction model that integrates contrastive learning, adversariallearning, and attention gating, achieving results comparable to or evensurpassing existing approaches for unseen ECG reconstruction. Finally, weexamine factors such as sex and age that impact reconstruction accuracy,emphasizing the importance of considering demographic diversity during modeltraining and dataset augmentation.</description>
      <author>example@mail.com (Xiaoyan Li, Shixin Xu, Faisal Habib, Neda Aminnejad, Arvind Gupta, Huaxiong Huang)</author>
      <guid isPermaLink="false">2502.17536v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning Density Evolution from Snapshot Data</title>
      <link>http://arxiv.org/abs/2502.17738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种基于标量回归的分布估计方法，用于从噪声时间点云数据中估计随机过程的概率密度演化。&lt;h4&gt;背景动机&lt;/h4&gt;受学习静态快照数据中的动态结构启发，该研究旨在通过利用标量上的分布来解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于熵正则化的非参数极大似然估计器（E-NPMLE）以从带有噪声的时间点云中估计随机过程的概率密度演化。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合了最优传输理论中的熵正则化，用于平滑概率密度流。设计了一种无网格的粒子基座 KL散度梯度下降算法来有效计算 E-NPMLE。&lt;h4&gt;主要发现&lt;/h4&gt;E-NPMLE 方法在统计上具有几乎不依赖于维度的收敛率，并且展示出显著的数量级转换现象，这取决于快照数量和每快照样本大小。该方法的有效性和理论结果通过合成数据进行了验证。&lt;h4&gt;结论&lt;/h4&gt;研究为从带有噪声的任意维数观测中估计概率密度演化提供了理论上和技术上的贡献。&lt;h4&gt;翻译&lt;/h4&gt;受学习静态快照数据中的动态结构启发，本文提出了一种基于标量回归的方法来从其含噪时间点云中估计随机过程的概率密度演变。我们提出了一个熵正则化非参数极大似然估计器（E-NPMLE），利用最优传输的熵作为概率流的平滑正则化项。展示了 E-NPMLE 对于真实分布具有几乎不依赖于维度的统计收敛率，并且在快照数量和每快照样本大小上表现出显著的数量级转换现象。为了高效计算 E-NPMLE，设计了一种新的无网格粒子基座 KL 散度梯度下降算法并证明了其多项式迭代复杂性。此外，在合成数据上提供了数值证据来支持理论发现。这项工作为从带有噪声的观测中估计任意维度中的概率密度演化提供了理论上和技术上的贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by learning dynamical structures from static snapshot data, thispaper presents a distribution-on-scalar regression approach for estimating thedensity evolution of a stochastic process from its noisy temporal point clouds.We propose an entropy-regularized nonparametric maximum likelihood estimator(E-NPMLE), which leverages the entropic optimal transport as a smoothingregularizer for the density flow. We show that the E-NPMLE has almostdimension-free statistical rates of convergence to the ground truthdistributions, which exhibit a striking phase transition phenomenon in terms ofthe number of snapshots and per-snapshot sample size. To efficiently computethe E-NPMLE, we design a novel particle-based and grid-free coordinate KLdivergence gradient descent (CKLGD) algorithm and prove its polynomialiteration complexity. Moreover, we provide numerical evidence on synthetic datato support our theoretical findings. This work contributes to the theoreticalunderstanding and practical computation of estimating density evolution fromnoisy observations in arbitrary dimensions.</description>
      <author>example@mail.com (Rentian Yao, Atsushi Nitanda, Xiaohui Chen, Yun Yang)</author>
      <guid isPermaLink="false">2502.17738v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching</title>
      <link>http://arxiv.org/abs/2502.18104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PromptMID，一种基于土地利用分类先验信息构建模态不变描述符的方法，用于光学和SAR图像匹配。&lt;h4&gt;背景&lt;/h4&gt;现有的学习型光-SAR图像匹配方法在特定场景中有效但泛化能力有限，难以适应实际应用需求。&lt;h4&gt;目的&lt;/h4&gt;通过有效利用基础模型来提升光-SAR图像匹配的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;PromptMID使用预训练扩散模型和视觉基础模型提取多尺度模态不变特征，并设计了专门的特征聚合模块以跨不同粒度融合特征。&lt;h4&gt;主要发现&lt;/h4&gt;在来自四个不同地区的光学-SAR数据集上，PromptMID的表现优于现有的匹配方法，在已见和未见过的数据域中都表现出色且具有强大的跨领域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地提出了一个新颖的方法来解决光-SAR图像匹配中的模态不变性和泛化问题，并通过实验验证了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;理想化的图像匹配目标是在未知环境下实现稳定和高效的性能。然而，许多现有的基于学习的光学-SAR图像匹配方法，在特定场景下虽然有效但泛化能力有限且难以适应实际应用需求。重复训练或微调匹配模型以解决领域差异不仅不够优雅而且会引入额外的计算开销和数据生产成本。近年来，通用基础模型在增强泛化方面显示了巨大的潜力，然而自然图像与遥感图像之间的视觉域差异使得它们直接应用于光学-SAR图像匹配存在挑战。因此，有效利用基础模型来改进光-SAR图像匹配的泛化能力仍然是一项挑战。为了应对上述挑战，我们提出了PromptMID，一种基于土地使用分类先验信息构建模态不变描述符的方法，用于光学和SAR图像匹配。PromptMID通过利用预训练扩散模型和视觉基础模型（VFMs）提取多尺度模态不变特征，并设计了专门的特征聚合模块以跨不同粒度融合特征。在来自四个不同地区的光学-SAR数据集上的广泛实验表明，PromptMID优于现有的匹配方法，在已见和未见过的数据域中都表现出色且具有强大的跨领域泛化能力。源代码将在https://github.com/HanNieWHU/PromptMID公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ideal goal of image matching is to achieve stable and efficientperformance in unseen domains. However, many existing learning-basedoptical-SAR image matching methods, despite their effectiveness in specificscenarios, exhibit limited generalization and struggle to adapt to practicalapplications. Repeatedly training or fine-tuning matching models to addressdomain differences is not only not elegant enough but also introducesadditional computational overhead and data production costs. In recent years,general foundation models have shown great potential for enhancinggeneralization. However, the disparity in visual domains between natural andremote sensing images poses challenges for their direct application. Therefore,effectively leveraging foundation models to improve the generalization ofoptical-SAR image matching remains challenge. To address the above challenges,we propose PromptMID, a novel approach that constructs modality-invariantdescriptors using text prompts based on land use classification as priorsinformation for optical and SAR image matching. PromptMID extracts multi-scalemodality-invariant features by leveraging pre-trained diffusion models andvisual foundation models (VFMs), while specially designed feature aggregationmodules effectively fuse features across different granularities. Extensiveexperiments on optical-SAR image datasets from four diverse regions demonstratethat PromptMID outperforms state-of-the-art matching methods, achievingsuperior results in both seen and unseen domains and exhibiting strongcross-domain generalization capabilities. The source code will be made publiclyavailable https://github.com/HanNieWHU/PromptMID.</description>
      <author>example@mail.com (Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Huan Zhou, Shuo Zhang, Weixing Liu)</author>
      <guid isPermaLink="false">2502.18104v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Examining the Threat Landscape: Foundation Models and Model Stealing</title>
      <link>http://arxiv.org/abs/2502.18077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了基础模型（FMs）在计算机视觉中的安全性问题，特别是它们对模型窃取攻击的脆弱性。通过实验分析发现，基于这些模型的应用更容易遭受窃取攻击。&lt;h4&gt;背景&lt;/h4&gt;基础模型具备强大的适应能力，在少量或不进行微调的情况下可以应用于特定任务和领域。然而，这种灵活性也可能导致安全风险：攻击者可能利用预训练的基础模型来获取有价值的信息，并将其用于非法目的。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在探讨基于基础模型的应用是否适合商业API部署，尤其是在考虑安全威胁的情况下。&lt;h4&gt;方法&lt;/h4&gt;作者通过实验分析了不同架构的模型在遭受窃取攻击时的表现。具体来说，他们比较了从基础模型微调后的模型与传统的视觉网络（如ResNets）之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;基于Vision Transformer的基础模型更容易受到模型窃取攻击；相比之下，ResNet-18这种传统架构则更加安全。例如，在使用CIFAR-10数据集进行训练时，通过基础模型微调的ViT-L/16模型（作为受害模型）与另一个ViT-L/16模型（作为盗窃者模型）之间有94.28%的一致性；而ResNet-18仅有73.20%。&lt;h4&gt;结论&lt;/h4&gt;论文指出，尽管基础模型在性能上有显著优势，但它们对商业API的部署构成了安全风险。因此，论文呼吁模型所有者注意潜在的安全隐患，并强调了采取强有力的防护措施的重要性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) for computer vision learn rich and robustrepresentations, enabling their adaptation to task/domain-specific deploymentswith little to no fine-tuning. However, we posit that the very same strengthcan make applications based on FMs vulnerable to model stealing attacks.Through empirical analysis, we reveal that models fine-tuned from FMs harborheightened susceptibility to model stealing, compared to conventional visionarchitectures like ResNets. We hypothesize that this behavior is due to thecomprehensive encoding of visual patterns and features learned by FMs duringpre-training, which are accessible to both the attacker and the victim. Wereport that an attacker is able to obtain 94.28% agreement (matched predictionswith victim) for a Vision Transformer based victim model (ViT-L/16) trained onCIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim,when using ViT-L/16 as the thief model. We arguably show, for the first time,that utilizing FMs for downstream tasks may not be the best choice fordeployment in commercial APIs due to their susceptibility to model theft. Wethereby alert model owners towards the associated security risks, and highlightthe need for robust security measures to safeguard such models against theft.Code is available at https://github.com/rajankita/foundation_model_stealing.</description>
      <author>example@mail.com (Ankita Raj, Deepankar Varma, Chetan Arora)</author>
      <guid isPermaLink="false">2502.18077v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Are GNNs doomed by the topology of their input graph?</title>
      <link>http://arxiv.org/abs/2502.17739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了图神经网络（GNNs）在处理图结构数据时的局限性及其输入图拓扑对模型性能的影响。通过引入$k$-hop相似性的概念来分析局部特征如何与消息传递方案交互，以产生有效的学习或不可避免的过度平滑现象。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已经显示出在其领域内的显著成功，但人们对图结构数据对其行为影响的理解仍然不足。&lt;h4&gt;目的&lt;/h4&gt;研究GNN是否固有地受到其输入图拓扑结构的影响，并探讨局部特征与消息传递机制之间的相互作用如何影响全局表现。&lt;h4&gt;方法&lt;/h4&gt;引入了$k$-hop相似性的概念来衡量局部相似的邻域是否会导致一致的节点表示。通过实验验证这些假设和发现的实际意义。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了图结构中的固有属性对于GNN的有效学习或不可避免的过度平滑现象有着重要影响，而不仅仅是基于局部特征的相似性。&lt;h4&gt;结论&lt;/h4&gt;该工作强调了理解输入图拓扑对GNN性能影响的重要性，并为进一步的研究提供了理论基础和实验依据。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含在内，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable success in learningfrom graph-structured data. However, the influence of the input graph'stopology on GNN behavior remains poorly understood. In this work, we explorewhether GNNs are inherently limited by the structure of their input graphs,focusing on how local topological features interact with the message-passingscheme to produce global phenomena such as oversmoothing or expressiverepresentations. We introduce the concept of $k$-hop similarity and investigatewhether locally similar neighborhoods lead to consistent node representations.This interaction can result in either effective learning or inevitableoversmoothing, depending on the inherent properties of the graph. Our empiricalexperiments validate these insights, highlighting the practical implications ofgraph topology on GNN performance.</description>
      <author>example@mail.com (Amine Mohamed Aboussalah, Abdessalam Ed-dib)</author>
      <guid isPermaLink="false">2502.17739v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>AutoCas: Autoregressive Cascade Predictor in Social Networks via Large Language Models</title>
      <link>http://arxiv.org/abs/2502.18040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于大型语言模型（LLM）的新型信息传播预测模型AutoCas，该模型针对信息传播数据的特点进行了专门优化。&lt;h4&gt;背景&lt;/h4&gt;信息传播机制、用户行为以及时间活动模式存在显著多样性，现有的模型难以适应这种变化。同时，可用的信息传播数据量相对有限。&lt;h4&gt;目的&lt;/h4&gt;构建一个能够利用大型语言模型的架构优势进行信息传播预测的基础模型，并提高其在流行度预测方面的性能。&lt;h4&gt;方法&lt;/h4&gt;将信息传播数据分词以匹配序列建模原则；重新定义信息传播扩散为自回归建模任务，以充分利用LLM的架构特点；引入提示学习技术增强LLM与信息传播预测之间的协同作用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，AutoCas在信息流流行度预测方面显著优于基线模型，并且展示了源自LLMs的可扩展行为。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的自回归信息传播预测器（AutoCas），通过专门适应大型语言模型的优势和挑战来提高信息传播流行度预测的效果。&lt;h4&gt;翻译&lt;/h4&gt;在信息级联中进行受欢迎程度预测是社会计算中的重要任务，广泛应用于病毒式营销、错误信息控制以及内容推荐。然而，信息传播机制、用户行为及时间活动模式显示出了显著的多样性，这要求一种能够适应这些变化的基础模型。同时，在用于训练大型语言模型（LLMs）的巨大数据集面前，可用的信息级联数据量相对有限。最近的研究表明，通过利用不同时间序列领域之间的共同点来应用LLM进行时间序列预测是可行的。基于这一洞察，我们引入了自回归信息级联预测器（AutoCas），这是一种专门用于级联流行度预测、由大型语言模型增强的模型。与自然语言序列相比，级联数据具有复杂的本地拓扑结构、传播上下文和不断变化的动力学特点，这需要为有效的LLM集成进行专门适应。为了应对这些挑战，我们首先将级联数据分词以使其符合序列建模原则。接下来，我们将信息传播扩散重新定义为自回归建模任务，以便充分利用LLMs的架构优势。此外，在常规方法之外，我们进一步引入了提示学习来增强LLM和级联预测之间的协同作用。广泛的实验表明，AutoCas在级联流行度预测方面显著优于基线模型，并且展示了源自LLMs的可扩展行为。代码可在以下仓库中获取：https://anonymous.4open.science/r/AutoCas-85C6&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Popularity prediction in information cascades plays a crucial role in socialcomputing, with broad applications in viral marketing, misinformation control,and content recommendation. However, information propagation mechanisms, userbehavior, and temporal activity patterns exhibit significant diversity,necessitating a foundational model capable of adapting to such variations. Atthe same time, the amount of available cascade data remains relatively limitedcompared to the vast datasets used for training large language models (LLMs).Recent studies have demonstrated the feasibility of leveraging LLMs fortime-series prediction by exploiting commonalities across different time-seriesdomains. Building on this insight, we introduce the Autoregressive InformationCascade Predictor (AutoCas), an LLM-enhanced model designed specifically forcascade popularity prediction. Unlike natural language sequences, cascade datais characterized by complex local topologies, diffusion contexts, and evolvingdynamics, requiring specialized adaptations for effective LLM integration. Toaddress these challenges, we first tokenize cascade data to align it withsequence modeling principles. Next, we reformulate cascade diffusion as anautoregressive modeling task to fully harness the architectural strengths ofLLMs. Beyond conventional approaches, we further introduce prompt learning toenhance the synergy between LLMs and cascade prediction. Extensive experimentsdemonstrate that AutoCas significantly outperforms baseline models in cascadepopularity prediction while exhibiting scaling behavior inherited from LLMs.Code is available at this repository:https://anonymous.4open.science/r/AutoCas-85C6</description>
      <author>example@mail.com (Yuhao Zheng, Chenghua Gong, Rui Sun, Juyuan Zhang, Liming Pan, Linyuan Lv)</author>
      <guid isPermaLink="false">2502.18040v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的去中心化控制方法，通过在图神经网络控制器中强制执行旋转等方性和平移不变性对称性，以优化群集的凝聚和一致性。&lt;h4&gt;背景&lt;/h4&gt;没有集中式控制的情况下协调代理以实现集体目标是具有挑战性的，尤其是在控制自主车队和使用传感器网络进行监控与侦察的应用场景中。现有方法受到自然界自我组织现象（如鸟群行为）启发，但这些去中心化控制器难以保持群体的凝聚性。&lt;h4&gt;目的&lt;/h4&gt;通过引入对称性约束在图神经网络架构中开发出一种新的去中心化控制器，以提高其维持群集凝聚力的能力以及泛化性能。&lt;h4&gt;方法&lt;/h4&gt;强制执行旋转等方性和平移不变性对称性来设计适用于鸟群控制的GNN控制器，这些是对鸟群动态中存在的自然规律。此方法比没有这种限制的现有模型使用更少的数据和参数量，并且在测试中的表现更加优异。&lt;h4&gt;主要发现&lt;/h4&gt;该研究证明了新的去中心化控制器需要较少的训练数据和可学习权重即可实现与传统GNN控制器相当的鸟群控制性能，同时展现出更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过应用对称性约束设计出来的新型图神经网络架构为构建更加高效且灵活的去中心化控制系统提供了可能，在减少模型复杂度的同时提高了系统的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;将代理在没有集中控制的情况下协调以优化集体目标是具有挑战性的，尤其是在诸如自主车队管理和使用传感器网络进行监控和侦察的应用中。分散控制器的设计受到了自然界中的自组织现象（如鸟群行为）的启发，然而现有方法难以保持群集的凝聚力。图神经网络架构已经作为开发能够维持群体凝聚性的去中心化控制工具成为必不可少的机器学习技术，但它们未能利用存在于鸟类动态中的对称性，限制了其泛化能力。研究强制执行旋转等方性和平移不变性在分散鸟群GNN控制器中，并实现与没有这些约束的传统模型相比使用70%更少的训练数据和75%更少的可学习权重即可获得类似的整体控制效果。此外，该方法展示出更好的泛化能力。相关代码和动画可在GitHub上获取（链接：http://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The orchestration of agents to optimize a collective objective withoutcentralized control is challenging yet crucial for applications such ascontrolling autonomous fleets, and surveillance and reconnaissance using sensornetworks. Decentralized controller design has been inspired byself-organization found in nature, with a prominent source of inspiration beingflocking; however, decentralized controllers struggle to maintain flockcohesion. The graph neural network (GNN) architecture has emerged as anindispensable machine learning tool for developing decentralized controllerscapable of maintaining flock cohesion, but they fail to exploit the symmetriespresent in flocking dynamics, hindering their generalizability. We enforcerotation equivariance and translation invariance symmetries in decentralizedflocking GNN controllers and achieve comparable flocking control with 70% lesstraining data and 75% fewer trainable weights than existing GNN controllerswithout these symmetries enforced. We also show that our symmetry-awarecontroller generalizes better than existing GNN controllers. Code andanimations are available athttp://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.</description>
      <author>example@mail.com (Taos Transue, Bao Wang)</author>
      <guid isPermaLink="false">2502.17612v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real</title>
      <link>http://arxiv.org/abs/2502.17894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FetchBot是一个从杂乱的货架上拾取物品的模拟到现实框架，旨在实现零样本泛化和安全意识的能力。通过利用高效的体积网格方法生成多样化的仿真场景，并采用动态感知强化学习策略来获取物体抓取轨迹。&lt;h4&gt;背景&lt;/h4&gt;在混乱环境中从货架上抓取物品是机器人帮助人类完成实际任务的重要能力，但受到运动空间受限、视野有限以及复杂对象动力学的影响而极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;介绍FetchBot框架，以解决数据稀缺问题，并开发一种能够泛化到不同真实世界场景中的安全意识物体抓取策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一个高效的体积网格方法来生成大规模的混乱货架场景；训练了感知动态强化学习策略以获取物体抓取轨迹；并将这种策略提炼成基于视觉的政策，用于现实世界的部署。采用深度信息输入以减少模拟到现实的差距，并设计了一种新颖的多视图表示学习架构。&lt;h4&gt;主要发现&lt;/h4&gt;FetchBot框架通过有效的数据生成和转移机制，在广泛的环境中表现出强大的泛化能力。实验结果证明了其在处理多种真实世界场景时的有效性和安全性。&lt;h4&gt;结论&lt;/h4&gt;提出的FetchBot方法为机器人从混乱货架中安全地抓取物品提供了一个新的途径，显示出潜在的广泛应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object fetching from cluttered shelves is an important capability for robotsto assist humans in real-world scenarios. Achieving this task demands roboticbehaviors that prioritize safety by minimizing disturbances to surroundingobjects, an essential but highly challenging requirement due to restrictedmotion space, limited fields of view, and complex object dynamics. In thispaper, we introduce FetchBot, a sim-to-real framework designed to enablezero-shot generalizable and safety-aware object fetching from cluttered shelvesin real-world settings. To address data scarcity, we propose an efficientvoxel-based method for generating diverse simulated cluttered shelf scenes atscale and train a dynamics-aware reinforcement learning (RL) policy to generateobject fetching trajectories within these scenes. This RL policy, whichleverages oracle information, is subsequently distilled into a vision-basedpolicy for real-world deployment. Considering that sim-to-real discrepanciesstem from texture variations mostly while from geometric dimensions rarely, wepropose to adopt depth information estimated by full-fledged depth foundationmodels as the input for the vision-based policy to mitigate sim-to-real gap. Totackle the challenge of limited views, we design a novel architecture forlearning multi-view representations, allowing for comprehensive encoding ofcluttered shelf scenes. This enables FetchBot to effectively minimizecollisions while fetching objects from varying positions and depths, ensuringrobust and safety-aware operation. Both simulation and real-robot experimentsdemonstrate FetchBot's superior generalization ability, particularly inhandling a broad range of real-world scenarios, includ</description>
      <author>example@mail.com (Weiheng Liu, Yuxuan Wan, Jilong Wang, Yuxuan Kuang, Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2502.17894v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CalibRefine 是一种自动、无目标的在线校准框架，适用于 LiDAR 和相机传感器，能够直接处理原始点云和图像。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR-摄像头校准方法通常依赖于手动放置的目标或初步参数估计等步骤，这限制了它们在真实环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需人工参与的自动校准框架以提高多传感器系统的鲁棒性感知能力。&lt;h4&gt;方法&lt;/h4&gt;{'阶段一': '通过共同特征判别器使用相对位置、外观嵌入和语义类别来生成可靠的 LiDAR-相机对应关系', '阶段二': '基于粗略同构变换的校准', '阶段三': '在新增数据帧可用的情况下，采用迭代细化逐步改进对齐精度', '阶段四': '利用视觉变换器和跨注意力机制解决非平面畸变问题'}&lt;h4&gt;主要发现&lt;/h4&gt;CalibRefine 可以提供高精度的校准结果，并且不需要复杂的前期准备或大量人工干预，在真实环境中保持了与手动调优基线方法的竞争性甚至超越。&lt;h4&gt;结论&lt;/h4&gt;研究展示了如何通过稳健的对象级特征匹配、迭代和自我监督注意机制调整，实现复杂条件下传感器融合的一致性。&lt;h4&gt;翻译&lt;/h4&gt;准确的多传感器校准对于部署自动驾驶汽车、机器人技术以及智能交通系统中的强大感知系统至关重要。现有的LiDAR-相机校准方法往往依赖于手动放置的目标物或初步参数估计等步骤，这些限制了其在现实世界环境下的可扩展性和适应性。这项工作提出了一种全自动的无目标在线校准框架 CalibRefine ，可以直接处理原始 LiDAR 点云和摄像机图像。该方案分为四个阶段：首先是一个共同特征判别器，使用相对位置、外观嵌入以及语义类别信息生成可靠的LiDAR-相机对应关系；接着是基于粗略同构变换的校准；然后在新数据帧可用时进行迭代细化，逐步提高对齐精度；最后通过利用视觉变换器和跨注意力机制解决非平面畸变问题。经过两项城市交通数据集上的广泛实验验证，CalibRefine 在不依赖人工参与的前提下提供了高精度的校准结果，并且超过了最先进的无目标方法，同时与手动调优基线保持了竞争性甚至超越其性能表现。我们的研究强调了通过稳健的对象级特征匹配以及迭代和自我监督注意机制调整，在复杂真实世界条件下实现传感器融合一致性的重要性，无需地面真值校准矩阵或复杂的前期数据处理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Chenga, Lihao Guoa, Tianya Zhangb, Tam Bangb, Austin Harrisb, Mustafa Hajijc, Mina Sartipib, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Bearing Fault Classification Under Variable Conditions: A 1D CNN with Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.17524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一种基于振动和电机相电流信号的滚动轴承故障分类方法，利用一维卷积神经网络框架。&lt;h4&gt;背景&lt;/h4&gt;滚动轴承在确保旋转机械的可靠性和效率方面发挥着关键作用，减少摩擦并处理重要负载。然而，高达90%的机械故障是由轴承失效引起的，这凸显了可靠的状况监测和故障检测的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于振动信号和电机相电流的一维卷积神经网络（1D CNN）框架下的多模态滚动轴承故障分类方法，以提高故障检测准确性。&lt;h4&gt;方法&lt;/h4&gt;该研究通过融合来自不同传感器的特征来增强故障检测精度，并在基线条件下实现了96%的准确率。此外，在使用L2正则化的情况下，模型性能得到了显著改善。&lt;h4&gt;主要发现&lt;/h4&gt;提出的1D CNN框架与迁移学习策略相结合，不仅提高了轴承故障分类的准确性，还在不同的操作条件下展示了稳健性表现。&lt;h4&gt;结论&lt;/h4&gt;虽然该方法在计算时间上需要更多资源，但为适应工业环境中可变工作条件下的更准确、更具适应性和效率的滚动轴承故障分类奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;滚动轴承在旋转机械中起着至关重要的作用，通过减少摩擦并处理重要负载来确保其可靠性和效率。高达90%的机械故障是由轴承失效引起的，这突显了可靠状态监测和故障检测的重要性。本文提出了一种基于振动信号和电机相电流的一维卷积神经网络（1D CNN）框架下的多模态滚动轴承故障分类方法，以提高故障检测准确性。该研究通过融合来自不同传感器的特征来增强故障检测精度，并在基线条件下实现了96%的准确率。此外，在使用L2正则化的情况下，模型性能得到了显著改善。提出的1D CNN框架与迁移学习策略相结合，不仅提高了轴承故障分类的准确性，还在不同的操作条件下展示了稳健性表现。虽然该方法在计算时间上需要更多资源，但为适应工业环境中可变工作条件下的更准确、更具适应性和效率的滚动轴承故障分类奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bearings play an integral role in ensuring the reliability and efficiency ofrotating machinery - reducing friction and handling critical loads. Bearingfailures that constitute up to 90% of mechanical faults highlight theimperative need for reliable condition monitoring and fault detection. Thisstudy proposes a multimodal bearing fault classification approach that relieson vibration and motor phase current signals within a one-dimensionalconvolutional neural network (1D CNN) framework. The method fuses features frommultiple signals to enhance the accuracy of fault detection. Under the baselinecondition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the modelreaches an accuracy of 96% with addition of L2 regularization. This representsa notable improvement of 2% compared to the non-regularized model. In addition,the model demonstrates robust performance across three distinct operatingconditions by employing transfer learning (TL) strategies. Among the tested TLvariants, the approach that preserves parameters up to the first max-pool layerand then adjusts subsequent layers achieves the highest performance. While thisapproach attains excellent accuracy across varied conditions, it requires morecomputational time due to its greater number of trainable parameters. Toaddress resource constraints, less computationally intensive models offerfeasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal1D CNN framework with late fusion and TL strategies lays a foundation for moreaccurate, adaptable, and efficient bearing fault classification in industrialenvironments with variable operating conditions.</description>
      <author>example@mail.com (Tasfiq E. Alam, Md Manjurul Ahsan, Shivakumar Raman)</author>
      <guid isPermaLink="false">2502.17524v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling</title>
      <link>http://arxiv.org/abs/2502.17873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的EEG基础模型框架EEGM2，旨在克服基于Transformer架构在处理脑电图数据时遇到的计算复杂度问题。EEGM2利用结构化状态空间对偶性（SSD）进行自监督学习，并通过Mamba-2模型捕捉局部和全局特征。&lt;h4&gt;背景&lt;/h4&gt;深度学习已经在脑电图基础模型的发展中取得了显著进展，其中基于Transformer架构的模型在捕获长距离依赖方面表现出色。然而，它们的二次计算复杂度导致了内存效率低下、训练和推理速度慢的问题，限制了其作为基础模型的大规模应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的EEG自监督框架EEGM2，以克服现有基于Transformer架构的脑电图模型在计算复杂性和泛化能力上的局限性。&lt;h4&gt;方法&lt;/h4&gt;1. 重建基线框架：通过Mamba-2结构状态空间模型捕获局部和全局EEG特征；2. 空间时间感知损失函数：增强对噪声的鲁棒性并保留光谱信息；3. 多分支感受野输入嵌入策略：改进跨受试者泛化能力和序列长度变化时的稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的预训练方法相比，EEGM2在长序列任务中表现出色，并通过实验验证了其在六个EEG数据集上的优越性能和计算效率。此外，它还实现了跨域最佳准确性并降低了计算开销。&lt;h4&gt;结论&lt;/h4&gt;EEGM2不仅达到了最先进的跨领域准确性，而且减少了计算开销，在资源受限的BCI设备上部署时更高效。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在脑电图基础模型的发展中取得了显著进步，尤其是基于Transformer架构的方法。然而，这些方法面临的二次复杂性挑战限制了它们的可扩展性和泛化能力。本文提出了一种新的自监督框架EEGM2，它引入了结构状态空间对偶性的概念，并利用Mamba-2模型来捕捉局部和全局特征。此外，通过改进的空间时间损失函数增强了鲁棒性和光谱信息保留。实验结果显示，与传统预训练方法相比，EEGM2在长序列任务中的表现更优，并且计算开销更低，适合资源受限的设备使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has achieved significant progress in the development ofelectroencephalogram (EEG) foundation models, with Transformer-basedarchitectures excelling at capturing long-range dependencies. However, theirquadratic computational complexity presents challenges in memory efficiency,training, and inference speed, limiting their scalability and generalizabilityas a foundation model. In this paper, we propose EEGM2, a self-supervisedframework based on structured state space duality (SSD) that overcomes theselimitations. EEGM2 introduces three key innovations: (1) a reconstruction-basedframework that captures both local and global EEG features through Mamba-2structured state space models, (2) a spatiotemporal-aware loss function thatenhances robustness to noise and preserves spectral information, and (3) amulti-branch receptive field input embedding strategy that improvescross-subject generalization and stability for EEG sequences of varyinglengths. In comparison to traditional pretraining methods, on raw EEG or latentrepresentation spaces, EEGM2 shows superior performance on long-sequence tasks,where conventional models struggle. Our experimental results on six EEGdatasets validate that EEGM2 not only achieves state-of-the-art cross-domainaccuracy but also reduces computational overhead, making it a more efficientsolution for deployment on resource-constrained BCI devices.</description>
      <author>example@mail.com (Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane)</author>
      <guid isPermaLink="false">2502.17873v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ASurvey: Spatiotemporal Consistency in Video Generation</title>
      <link>http://arxiv.org/abs/2502.17863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了最近在视频生成领域的进展，涵盖了基础模型、信息表示、生成方案、后处理技术以及评估指标五个关键方面。&lt;h4&gt;背景&lt;/h4&gt;通过利用动态视觉生成方法，视频生成推动了人工智能生成内容（AIGC）的发展。相比静态图像生成，视频生成具有独特挑战，需要高质量的单帧画面和时间一致性以保持空间和时间序列的一致性。&lt;h4&gt;目的&lt;/h4&gt;回顾最近的进展并探讨维持时空一致性的贡献，填补相关文献综述的空白，以便更深入地理解高质量视频生成的基础机制。&lt;h4&gt;方法&lt;/h4&gt;系统地审查了五个关键方面：基础模型、信息表示、生成方案、后处理技术和评估指标，并特别关注它们在保持时空一致性方面的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;近期的工作主要集中于解决视频生成中的时空一致性问题，但很少有从这个角度组织的文献综述。&lt;h4&gt;结论&lt;/h4&gt;讨论了该领域未来的发展方向和挑战，旨在激励进一步的努力以推进视频生成技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;通过利用动态视觉生成方法，视频生成推动了人工智能生成内容（AIGC）的发展。相比静态图像生成，视频生成具有独特挑战，需要高质量的单帧画面和时间一致性以保持空间和时间序列的一致性。近期的工作主要集中于解决视频生成中的时空一致性问题，但很少有从这个角度组织的文献综述。该论文系统地回顾了最近在五个关键方面的进展：基础模型、信息表示、生成方案、后处理技术和评估指标，并特别关注它们如何保持时空一致性的贡献。此外，讨论了未来的发展方向和挑战，旨在激励进一步的努力以推进视频生成技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation, by leveraging a dynamic visual generation method, pushesthe boundaries of Artificial Intelligence Generated Content (AIGC). Videogeneration presents unique challenges beyond static image generation, requiringboth high-quality individual frames and temporal coherence to maintainconsistency across the spatiotemporal sequence. Recent works have aimed ataddressing the spatiotemporal consistency issue in video generation, while fewliterature review has been organized from this perspective. This gap hinders adeeper understanding of the underlying mechanisms for high-quality videogeneration. In this survey, we systematically review the recent advances invideo generation, covering five key aspects: foundation models, informationrepresentations, generation schemes, post-processing techniques, and evaluationmetrics. We particularly focus on their contributions to maintainingspatiotemporal consistency. Finally, we discuss the future directions andchallenges in this field, hoping to inspire further efforts to advance thedevelopment of video generation.</description>
      <author>example@mail.com (Zhiyu Yin, Kehai Chen, Xuefeng Bai, Ruili Jiang, Juntao Li, Hongdong Li, Jin Liu, Yang Xiang, Jun Yu, Min Zhang)</author>
      <guid isPermaLink="false">2502.17863v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Laplace-Beltrami Operator for Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种计算拉普拉斯-贝尔特拉米算子的新方法，直接在高斯点阵上使用马氏距离来实现。这种方法提高了处理由3D高斯点阵表示的数据时的准确性。&lt;h4&gt;背景&lt;/h4&gt;随着3D高斯点阵技术的流行和其应用从渲染扩展到3D重建，对这种新数据表示形式进行几何处理的需求也日益增长。&lt;h4&gt;目的&lt;/h4&gt;提出一种直接在3D高斯点阵上计算拉普拉斯-贝尔特拉米算子的方法，以提高基于该表示的数据处理准确性，并能够评估优化过程中的输出质量。&lt;h4&gt;方法&lt;/h4&gt;通过使用马氏距离来定义和计算拉普拉斯-贝尔特拉米算子，从而直接在3D高斯点阵上进行几何处理。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在基于3D高斯点阵中心的点云数据中，该方法比传统的点云拉普拉斯算子具有更高的准确性，并且能够评估优化过程中的输出质量。&lt;h4&gt;结论&lt;/h4&gt;通过直接在3D高斯点阵上计算拉普拉斯-贝尔特拉米算子，可以提高基于该表示的数据处理效率和准确性，尤其是在处理大量离群值的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rising popularity of 3D Gaussian splatting and the expanse ofapplications from rendering to 3D reconstruction, there comes also a need forgeometry processing applications directly on this new representation. Whileconsidering the centers of Gaussians as a point cloud or meshing them is anoption that allows to apply existing algorithms, this might ignore informationpresent in the data or be unnecessarily expensive. Additionally, Gaussiansplatting tends to contain a large number of outliers which do not affect therendering quality but need to be handled correctly in order not to producenoisy results in geometry processing applications. In this work, we propose aformulation to compute the Laplace-Beltrami operator, a widely used tool ingeometry processing, directly on Gaussian splatting using the Mahalanobisdistance. While conceptually similar to a point cloud Laplacian, ourexperiments show superior accuracy on the point clouds encoded in the Gaussiansplatting centers and, additionally, the operator can be used to evaluate thequality of the output during optimization.</description>
      <author>example@mail.com (Hongyu Zhou, Zorah Lähner)</author>
      <guid isPermaLink="false">2502.17531v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Theory for Edge Pruning in Asynchronous Recurrent Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于图谱理论的动态修剪方法，用于简化异步递归图神经网络（ARGNN），以提高其在处理动态图形数据时的效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）已经成为学习结构化图形数据的强大工具，在社交网络分析和分子生物学等领域中得到广泛应用。特别地，ARGNN因其能捕捉到动态图形中的复杂依赖关系而脱颖而出，类似于活体生物复杂的适应性特征。&lt;h4&gt;目的&lt;/h4&gt;尽管ARGNN在性能上表现出色，但其复杂性往往导致模型庞大且计算成本高昂。因此，修剪不必要的边成为提高效率而不显著降低性能的关键。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种基于图谱理论的动态修剪策略，利用网络图形拉普拉斯矩阵特征值的虚部来识别和去除不重要的连接。&lt;h4&gt;主要发现&lt;/h4&gt;该论文展示了如何通过有效减少ARGNN中的冗余边来提高模型效率，并且在保证性能的同时减少了计算资源的需求。&lt;h4&gt;结论&lt;/h4&gt;所提出的动态修剪方法为优化复杂GNN架构提供了一种有效的解决方案，特别适用于处理大量数据的场景中需要高效推理的应用场合。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容直接用于英文到中文的翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful tool for learning ongraph-structured data, finding applications in numerous domains includingsocial network analysis and molecular biology. Within this broad category,Asynchronous Recurrent Graph Neural Networks (ARGNNs) stand out for theirability to capture complex dependencies in dynamic graphs, resembling livingorganisms' intricate and adaptive nature. However, their complexity often leadsto large and computationally expensive models. Therefore, pruning unnecessaryedges becomes crucial for enhancing efficiency without significantlycompromising performance. This paper presents a dynamic pruning method based ongraph spectral theory, leveraging the imaginary component of the eigenvalues ofthe network graph's Laplacian.</description>
      <author>example@mail.com (Nicolas Bessone)</author>
      <guid isPermaLink="false">2502.17522v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2410.22489v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于三维点云分割的多模态少样本分割模型（MM-FSS），该模型利用文本标签和可能存在的二维图像模式，弥补了现有方法忽略多模态信息的问题。&lt;h4&gt;背景&lt;/h4&gt;目前的少样本3D点云分割方法主要关注单一模态的点云输入，忽略了多模态信息的优势。&lt;h4&gt;目的&lt;/h4&gt;引入一个利用多种模态数据（如文本标签和2D图像）的多模态少样本设置，并开发相应的模型以提升性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的模型——MultiModal Few-Shot SegNet (MM-FSS)，该模型通过共享骨干网络从多个模式中提取互补信息，使用预训练的文字编码器生成文字嵌入。还设计了Multimodal Correlation Fusion（MCF）模块和Multimodal Semantic Fusion（MSF）模块来利用多模态数据。此外提出Test-time Adaptive Cross-modal Calibration（TACC）技术以减轻训练偏差。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MM-FSS在S3DIS和ScanNet等数据集上的性能显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;本文证明了利用被忽视的自由模态信息可以提高少样本三维点云分割任务的效果，并为未来的研究提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文版&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models tosegment novel categories with minimal annotated support samples. While existingFS-PCS methods have shown promise, they primarily focus on unimodal point cloudinputs, overlooking the potential benefits of leveraging multimodalinformation. In this paper, we address this gap by introducing a multimodalFS-PCS setup, utilizing textual labels and the potentially available 2D imagemodality. Under this easy-to-achieve setup, we present the MultiModal Few-ShotSegNet (MM-FSS), a model effectively harnessing complementary information frommultiple modalities. MM-FSS employs a shared backbone with two heads to extractintermodal and unimodal visual features, and a pretrained text encoder togenerate text embeddings. To fully exploit the multimodal information, wepropose a Multimodal Correlation Fusion (MCF) module to generate multimodalcorrelations, and a Multimodal Semantic Fusion (MSF) module to refine thecorrelations using text-aware semantic guidance. Additionally, we propose asimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)technique to mitigate training bias, further improving generalization.Experimental results on S3DIS and ScanNet datasets demonstrate significantperformance improvements achieved by our method. The efficacy of our approachindicates the benefits of leveraging commonly-ignored free modalities forFS-PCS, providing valuable insights for future research. The code is availableat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot</description>
      <author>example@mail.com (Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie)</author>
      <guid isPermaLink="false">2410.22489v3</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Macro- and Micro-Hierarchical Transfer Learning Framework for Cross-Domain Fake News Detection</title>
      <link>http://arxiv.org/abs/2502.14403v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures, to be published in The 2025 ACM Web Conference  (WWW '25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于跨域假新闻检测的宏微观分层迁移学习框架（MMHT），该框架旨在解决现有方法在知识转移和假新闻检测性能方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;跨域假新闻检测的目标是通过在不同领域之间传输知识来减轻领域的偏移并提高检测性能。现有的方法基于源领域的新闻内容和用户互动向目标领域传递知识，但它们面临着两个主要限制：忽略新闻内容中无关事实特征的负面影响以及忽视用户参与度与新闻内容之间的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效解决现有假新闻检测方法存在的问题，并优化知识传输效率的新框架。&lt;h4&gt;方法&lt;/h4&gt;1. 微观分层解缠模块，旨在从源域的新闻内容中分离出相关性和无关性的事实特征，以提高目标领域的假新闻检测性能。2. 宏观分层迁移学习模块，根据不同领域中用户共享行为生成参与度特征，从而增强知识传输的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的框架在真实数据集上的表现明显优于现有的最先进的基准方法。&lt;h4&gt;结论&lt;/h4&gt;通过解决现有跨域假新闻检测方法中的两个关键限制，MMHT 框架能够有效提高跨领域假新闻检测的性能和知识传输效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714517&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain fake news detection aims to mitigate domain shift and improvedetection performance by transferring knowledge across domains. Existingapproaches transfer knowledge based on news content and user engagements from asource domain to a target domain. However, these approaches face two mainlimitations, hindering effective knowledge transfer and optimal fake newsdetection performance. Firstly, from a micro perspective, they neglect thenegative impact of veracity-irrelevant features in news content whentransferring domain-shared features across domains. Secondly, from a macroperspective, existing approaches ignore the relationship between userengagement and news content, which reveals shared behaviors of common usersacross domains and can facilitate more effective knowledge transfer. To addressthese limitations, we propose a novel macro- and micro- hierarchical transferlearning framework (MMHT) for cross-domain fake news detection. Firstly, wepropose a micro-hierarchical disentangling module to disentangleveracity-relevant and veracity-irrelevant features from news content in thesource domain for improving fake news detection performance in the targetdomain. Secondly, we propose a macro-hierarchical transfer learning module togenerate engagement features based on common users' shared behaviors indifferent domains for improving effectiveness of knowledge transfer. Extensiveexperiments on real-world datasets demonstrate that our framework significantlyoutperforms the state-of-the-art baselines.</description>
      <author>example@mail.com (Xuankai Yang, Yan Wang, Xiuzhen Zhang, Shoujin Wang, Huaxiong Wang, Kwok Yan Lam)</author>
      <guid isPermaLink="false">2502.14403v2</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.17516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 4 Figures, 10 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了基础模型的发展对机器学习研究的影响，特别是大型语言模型（LLMs）和多模态基础模型（MMFMs）。虽然在解释LLM方面取得了显著进展，但MMFMs（如对比视觉-语言模型、生成式视觉-语言模型以及文本到图像模型）在单模态框架之外提出了独特的可解释性挑战。&lt;h4&gt;背景&lt;/h4&gt;大规模基础模型的兴起改变了机器学习研究的方向，并促使研究人员努力揭示这些模型的工作原理以开发更高效和可靠的控制应用。虽然LLM的理解已经取得了很大进展，但MMFMs的透明度仍然落后于LLM。&lt;h4&gt;目的&lt;/h4&gt;该综述探讨了两个关键方面：一是将LLM解释方法应用于多模态模型；二是理解单模态语言模型与跨模式系统之间的机制差异。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地回顾现有的MMFM分析技术，提出了可解释性的结构分类法，并比较了单模态和多模态架构的见解。&lt;h4&gt;主要发现&lt;/h4&gt;指出了当前在LLM和MMFM之间理解上的重大差距，并强调了几项关键的研究空白。&lt;h4&gt;结论&lt;/h4&gt;该综述提供了一种对MMFMs进行系统性研究的方法，旨在促进机器学习领域的进一步发展。它还为研究人员提供了关于如何改进多模态基础模型的解释性的指导。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLM）的可解释方法在多模态框架下的适应以及单模态语言模型与跨模式系统之间的机制性差异是研究重点，通过比较和分类不同架构中的见解来填补LLM和MMFM之间存在的理解差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models has transformed machine learning research,prompting efforts to uncover their inner workings and develop more efficientand reliable applications for better control. While significant progress hasbeen made in interpreting Large Language Models (LLMs), multimodal foundationmodels (MMFMs) - such as contrastive vision-language models, generativevision-language models, and text-to-image models - pose unique interpretabilitychallenges beyond unimodal frameworks. Despite initial studies, a substantialgap remains between the interpretability of LLMs and MMFMs. This surveyexplores two key aspects: (1) the adaptation of LLM interpretability methods tomultimodal models and (2) understanding the mechanistic differences betweenunimodal language models and crossmodal systems. By systematically reviewingcurrent MMFM analysis techniques, we propose a structured taxonomy ofinterpretability methods, compare insights across unimodal and multimodalarchitectures, and highlight critical research gaps.</description>
      <author>example@mail.com (Zihao Lin, Samyadeep Basu, Mohammad Beigi, Varun Manjunatha, Ryan A. Rossi, Zichao Wang, Yufan Zhou, Sriram Balasubramanian, Arman Zarei, Keivan Rezaei, Ying Shen, Barry Menglong Yao, Zhiyang Xu, Qin Liu, Yuxiang Zhang, Yan Sun, Shilong Liu, Li Shen, Hongxuan Li, Soheil Feizi, Lifu Huang)</author>
      <guid isPermaLink="false">2502.17516v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning multi-phase flow and transport in fractured porous media with auto-regressive and recurrent graph neural networks</title>
      <link>http://arxiv.org/abs/2502.17512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一种使用图神经网络（GNN）来模拟多相流和输运动力学的新方法，这种方法在处理复杂裂缝网络时比传统的方法更有效。&lt;h4&gt;背景&lt;/h4&gt;过去三十年间，用于解决裂隙多孔介质中多重流动和传输过程建模的计算方法和仿真框架层出不穷。其中共形网格方法因其高精度而备受青睐，但需要极细的网格划分，在大型或复杂的裂缝网络中变得不可行。&lt;h4&gt;目的&lt;/h4&gt;提出基于图神经网络的方法来模拟复杂裂隙多孔介质中的多相流动和传输动力学。&lt;h4&gt;方法&lt;/h4&gt;提出了两种深度学习架构：一种是GNN（图神经网络），另一种是递归GNN。这两种网络均采用两阶段训练策略：自回归一步滚动，然后通过完整的真实地面序列进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示了两阶段训练的有效性，在测试阶段的自回归模型滚动过程中减少了误差积累；两种GNN都展示了对未见过裂缝结构的良好泛化能力。递归GNN在预测长期序列方面显示出显著优势，尤其是在压力序列预测中表现更佳。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的方法为复杂裂隙多孔介质中的流动和传输过程建模提供了一种有效的新途径，特别是对于大型或复杂的裂缝网络来说更是如此。&lt;h4&gt;翻译&lt;/h4&gt;在过去三十年间，多种计算方法和模拟框架被开发出来以应对在裂隙性多孔介质中多重流体流动与输运过程的复杂建模挑战。共形网格技术通过确保计算网格与裂缝表面对齐而被视为最准确的方法之一。然而，这类方法需要极细的网格划分，在处理大型或复杂的裂缝网络时变得不切实际。在这项工作中，我们提出使用图神经网络（GNN）来学习裂隙性多孔介质中的复杂多相流动和输运动力学现象。鉴于嵌入离散裂缝模型（EDFM）所导致计算网格的非结构化特性，GNN非常适合这一任务。我们提出了两种深度学习架构：一种是GNN，另一种是递归GNN。这两个网络都遵循两阶段训练策略：首先是自回归一步滚动，然后通过完整的真实地面序列进行微调。结果显示，两阶段训练方法在测试期间的模型自回归展开时有效减少了误差积累。我们的研究发现表明，两种GNN都能很好地泛化到未见过的裂缝结构，并且它们在预测饱和度序列方面表现相当，而递归GNN则略微优于GNN在压力序列预测中的性能。尽管第二阶段训练对GNN模型有益，但其对递归GNN的影响不那么显著。最后，测试了两种GNN的时间外推性能。递归GNN以更高的准确性明显超越了GNN，在长期序列的预测中展示了优越的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past three decades, a wide array of computational methodologies andsimulation frameworks has emerged to address the complexities of modelingmulti-phase flow and transport processes in fractured porous media. Theconformal mesh approaches which explicitly align the computational grid withfracture surfaces are considered by many to be the most accurate. However, suchmethods require excessive fine-scale meshing, rendering them impractical forlarge or complex fracture networks. In this work, we propose to learn thecomplex multi-phase flow and transport dynamics in fractured porous media withgraph neural networks (GNN). GNNs are well suited for this task due to theunstructured topology of the computation grid resulting from the EmbeddedDiscrete Fracture Model (EDFM) discretization. We propose two deep learningarchitectures, a GNN and a recurrent GNN. Both networks follow a two-stagetraining strategy: an autoregressive one step roll-out, followed by afine-tuning step where the model is supervised using the whole ground-truthsequence. We demonstrate that the two-stage training approach is effective inmitigating error accumulation during autoregressive model rollouts in thetesting phase. Our findings indicate that both GNNs generalize well to unseenfracture realizations, with comparable performance in forecasting saturationsequences, and slightly better performance for the recurrent GNN in predictingpressure sequences. While the second stage of training proved to be beneficialfor the GNN model, its impact on the recurrent GNN model was less pronounced.Finally, the performance of both GNNs for temporal extrapolation is tested. Therecurrent GNN significantly outperformed the GNN in terms of accuracy, therebyunderscoring its superior capability in predicting long sequences.</description>
      <author>example@mail.com (Mohammed Al Kobaisi, Wenjuan Zhang, Waleed Diab, Hadi Hajibeygi)</author>
      <guid isPermaLink="false">2502.17512v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Reward Inference</title>
      <link>http://arxiv.org/abs/2502.18447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章提出了一种基于监督学习的框架，用于从人类行为中推断奖励函数。这种方法能够在温和假设下实现渐进贝叶斯最优，并且通过模拟机器人操作任务实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的奖励推理方法通常假设人类提供的演示遵循特定的行为模型。然而，在实际情况下，人们可能表现出各种类型的次优行为来指示他们的目标。&lt;h4&gt;目的&lt;/h4&gt;提出一种从任意类型的人类行为中推断奖励函数的统一框架，并展示这种方法在温和假设下的渐进贝叶斯最优性。&lt;h4&gt;方法&lt;/h4&gt;利用监督学习技术建立一个模型，该模型可以从广泛的、可能是次优的行为演示中推断出奖励函数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法能够从各种各样的次优行为演示中有效地推断出奖励函数。&lt;h4&gt;结论&lt;/h4&gt;基于监督学习的框架为从人类行为数据中推断奖励提供了一种灵活而有效的方法，并且在模拟任务中的表现证明了这一点。&lt;h4&gt;翻译&lt;/h4&gt;现有方法假设人类提供的示例符合特定的行为模型。然而，实际情况是人们通过各种各样的行为来指示目标，这些行为可能由于计划或执行不当而不理想，也可能是为了传达目的而不是实现它们。我们提出监督学习能够提供一个统一的框架从任何类型的人类行为中推断奖励函数，并展示了在温和假设下的渐进贝叶斯最优性。模拟机器人操作任务实验表明这种方法可以从各种次优演示中有效推断出奖励。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches to reward inference from behavior typically assume thathumans provide demonstrations according to specific models of behavior.However, humans often indicate their goals through a wide range of behaviors,from actions that are suboptimal due to poor planning or execution to behaviorswhich are intended to communicate goals rather than achieve them. We proposethat supervised learning offers a unified framework to infer reward functionsfrom any class of behavior, and show that such an approach is asymptoticallyBayes-optimal under mild assumptions. Experiments on simulated roboticmanipulation tasks show that our method can efficiently infer rewards from awide variety of arbitrarily suboptimal demonstrations.</description>
      <author>example@mail.com (Will Schwarzer, Jordan Schneider, Philip S. Thomas, Scott Niekum)</author>
      <guid isPermaLink="false">2502.18447v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CRESSim-MPM: A Material Point Method Library for Surgical Soft Body Simulation with Cutting and Suturing</title>
      <link>http://arxiv.org/abs/2502.18437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 12 figures, submitted to IEEE/RSJ International Conference  on Intelligent Robots and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的手术模拟平台CRESSim-MPM，使用材料点法（MPM）来克服现有平台上在软组织切割和缝合等复杂行为的模拟问题。&lt;h4&gt;背景&lt;/h4&gt;现有的手术仿真平台难以精确地模拟软体组织的行为，特别是像切割、缝合这样复杂的操作。这主要是由于有限元方法(FEM)在这种情况下处理材料分裂等问题时存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确模拟软体组织复杂行为的新型物理引擎，以便更好地训练机器学习模型进行手术辅助。&lt;h4&gt;方法&lt;/h4&gt;采用材料点法（MPM）来应对FEM在处理软质物体分割和缝合问题上的不足。提出了新的刚性几何形状以及适用于这种场景下的软硬接触方法。开发了CRESSim-MPM, 一个加速的GPU库，集成了多种MPM求解器，并加入了用于切割和缝合等手术操作的物理引擎。&lt;h4&gt;主要发现&lt;/h4&gt;通过在实时模拟中展示其能力来验证该平台的有效性，包括对软组织进行切割和缝合。同时进行了不同数量颗粒仿真时各MPM求解器性能评估。&lt;h4&gt;结论&lt;/h4&gt;CRESSim-MPM为复杂的手术操作提供了更好的物理建模方法，并且能够有效地与Unity集成，便于现有项目的扩展和应用。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究集中于开发用于训练机器学习代理或模型的合成数据的外科模拟平台。尽管现有的平台在刚体操纵和软体变形方面表现出色，但它们难以精确地模拟诸如切割和缝合等更复杂的软体行为。一个重要挑战在于使用有限元法（FEM）建模软体分裂问题，在当前平台上这种方法是主要手段。当需要处理手术中的两向缝线接触时，这会进一步复杂化。在本研究中，我们采用材料点法（MPM）进行此类困难模拟，并提出了用于该方法的新刚性几何形状和软硬接触方法。我们引入了CRESSim-MPM，这是一个基于GPU加速的MPM库，集成了多个MPM求解器并包含了手术几何图形以支持切割和缝合操作，作为专门用于外科应用的物理引擎。它还被集成到了Unity中，只需对现有项目进行最小改动即可实现软体模拟。我们展示了该仿真器在实时模拟软组织切割和缝合方面的能力，并提供了不同数量颗粒时各种MPM求解器性能评估的初步结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A number of recent studies have focused on developing surgical simulationplatforms to train machine learning (ML) agents or models with synthetic datafor surgical assistance. While existing platforms excel at tasks such as rigidbody manipulation and soft body deformation, they struggle to simulate morecomplex soft body behaviors like cutting and suturing. A key challenge lies inmodeling soft body fracture and splitting using the finite-element method(FEM), which is the predominant approach in current platforms. Additionally,the two-way suture needle/thread contact inside a soft body is furthercomplicated when using FEM. In this work, we use the material point method(MPM) for such challenging simulations and propose new rigid geometries andsoft-rigid contact methods specifically designed for them. We introduceCRESSim-MPM, a GPU-accelerated MPM library that integrates multiple MPM solversand incorporates surgical geometries for cutting and suturing, serving as aspecialized physics engine for surgical applications. It is further integratedinto Unity, requiring minimal modifications to existing projects for soft bodysimulation. We demonstrate the simulator's capabilities in real-time simulationof cutting and suturing on soft tissue and provide an initial performanceevaluation of different MPM solvers when simulating varying numbers ofparticles.</description>
      <author>example@mail.com (Yafei Ou, Mahdi Tavakoli)</author>
      <guid isPermaLink="false">2502.18437v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Stretchable Capacitive and Resistive Strain Sensors: Accessible Manufacturing Using Direct Ink Writing</title>
      <link>http://arxiv.org/abs/2502.18363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 8 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;随着机器人技术的发展，软结构、类人形态和复杂任务的集成变得越来越重要，因此研发出可靠测量触觉和本体感觉数据的同时兼具形变适应性、可拉伸性和可调整性的柔性传感器至关重要。&lt;h4&gt;背景&lt;/h4&gt;当前许多用于制造可拉伸传感器的方法仅限于单一配置的设计，限制了设计灵活性。研究人员正在探索多样化的转换原理以及规模生产和多功能生产技术以解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于打印的灵活、定制化且易于实现的可拉伸传感器制造方法。&lt;h4&gt;方法&lt;/h4&gt;使用商用3D打印机与自定义喷头集成，可以将导电墨水直接书写在固化硅胶基底上。通过堆叠托盘支持的逐层制造工艺，在硅胶矩阵内沉积多层液态导电墨水。&lt;h4&gt;主要发现&lt;/h4&gt;展示的方法具有高度的设计灵活性，并且制造出和评估了包括电容式和电阻式的应变传感器形态。实验表征显示，电容式应变传感器具备高线性度（R^2 = 0.99）、接近理论极限的高灵敏度（GF = 0.95）、极小的滞后效应(DH = 1.36%)以及高达550%的最大拉伸能力。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种新的制造方式，能够满足高度可定制化的需求，并且生产出的传感器性能达到当前先进水平。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人技术向集成软结构、类人形态和复杂任务的发展，柔软且高度可拉伸的力学转换器变得越来越重要。为了在确保形状适应性的同时可靠地测量触觉和本体感觉数据，研究人员正在探索多样化的转换原理以及规模生产和多功能生产技术。然而，许多当前用于制造可拉伸传感器的方法仅限于单一配置的设计，限制了设计灵活性。在这里，我们提出了一种基于打印的灵活、定制化且易于实现的新方法来制作可拉伸传感器。我们的方法采用商用3D打印机与自定义喷头集成，可以将导电墨水直接书写在固化硅胶基底上。通过堆叠托盘支持的逐层制造工艺，在硅胶矩阵内沉积多层液态导电墨水。为了展示该方法的设计灵活性，我们制作并评估了包括电容式和电阻式的应变传感器形态。实验表征显示，电容式应变传感器具有高线性度（R^2 = 0.99）、接近理论极限的高灵敏度（GF = 0.95）、极小的滞后效应(DH = 1.36%)以及高达550%的最大拉伸能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robotics advances toward integrating soft structures, anthropomorphicshapes, and complex tasks, soft and highly stretchable mechanotransducers arebecoming essential. To reliably measure tactile and proprioceptive data whileensuring shape conformability, stretchability, and adaptability, researchershave explored diverse transduction principles alongside scalable and versatilemanufacturing techniques. Nonetheless, many current methods for stretchablesensors are designed to produce a single sensor configuration, thereby limitingdesign flexibility. Here, we present an accessible, flexible, printing-basedfabrication approach for customizable, stretchable sensors. Our method employsa custom-built printhead integrated with a commercial 3D printer to enabledirect ink writing (DIW) of conductive ink onto cured silicone substrates. Alayer-wise fabrication process, facilitated by stackable trays, allows for thedeposition of multiple liquid conductive ink layers within a silicone matrix.To demonstrate the method's capacity for high design flexibility, we fabricateand evaluate both capacitive and resistive strain sensor morphologies.Experimental characterization showed that the capacitive strain sensorpossesses high linearity (R^2 = 0.99), high sensitivity near the 1.0theoretical limit (GF = 0.95), minimal hysteresis (DH = 1.36%), and largestretchability (550%), comparable to state-of-the-art stretchable strainsensors reported in the literature.</description>
      <author>example@mail.com (Lukas Cha, Sonja Groß, Shuai Mao, Tim Braun, Sami Haddadin, Liang He)</author>
      <guid isPermaLink="false">2502.18363v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MegaLoc: One Retrieval to Place Them All</title>
      <link>http://arxiv.org/abs/2502.17237v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了MegaLoc模型，该模型在多个计算机视觉任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从给定查询检索来自相同位置的图像对于多项计算机视觉任务（如视觉地方识别、地标检索、视觉定位、3D重建和SLAM）至关重要。然而，现有解决方案只能针对特定的任务设计，并且当需求变化或遇到分布外数据时会失败。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型MegaLoc，该模型能够同时在多个计算机视觉任务中表现良好。&lt;h4&gt;方法&lt;/h4&gt;结合了多种现有的方法、训练技术以及数据集来训练一个检索模型MegaLoc。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'MegaLoc在大量视觉地方识别数据集中达到了最先进水平。', '2': 'MegaLoc在常用的地标检索数据集中取得了令人印象深刻的结果。', '3': 'MegaLoc为LaMAR数据集上的视觉定位设定了新的最先进的标准，只需将检索方法替换到现有定位管道中即可。'}&lt;h4&gt;结论&lt;/h4&gt;提出的模型MegaLoc展示了其跨多个任务的优越性能，并且源代码已公开发布在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;从给定查询检索来自相同位置的图像对多项计算机视觉任务（包括视觉地方识别、地标检索、视觉定位、3D重建和SLAM）至关重要。然而，现有解决方案设计专门用于特定的任务，在需求变化或遇到分布外数据时会失效。本文结合了多种现有的方法、训练技术以及数据集来训练一个名为MegaLoc的检索模型，该模型在多个任务上表现出色。研究发现，MegaLoc（1）在大量视觉地方识别的数据集中达到了最先进水平；（2）在常用的地标检索数据集中取得了令人印象深刻的结果；（3）为LaMAR数据集上的视觉定位设定了新的最先进的标准，只需将检索方法替换到现有定位管道中即可。MegaLoc的代码可在https://github.com/gmberton/MegaLoc 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gmberton/megaloc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving images from the same location as a given query is an importantcomponent of multiple computer vision tasks, like Visual Place Recognition,Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,existing solutions are built to specifically work for one of these tasks, andare known to fail when the requirements slightly change or when they meetout-of-distribution data. In this paper we combine a variety of existingmethods, training techniques, and datasets to train a retrieval model, calledMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)achieves state of the art on a large number of Visual Place Recognitiondatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)sets a new state of the art for Visual Localization on the LaMAR datasets,where we only changed the retrieval method to the existing localizationpipeline. The code for MegaLoc is available athttps://github.com/gmberton/MegaLoc</description>
      <author>example@mail.com (Gabriele Berton, Carlo Masone)</author>
      <guid isPermaLink="false">2502.17237v2</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Data Generation for Precision Agriculture: Blending Simulated Environments with Real Imagery</title>
      <link>http://arxiv.org/abs/2502.18320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at 2024 IEEE 20th International Conference on Automation  Science and Engineering (CASE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在精准农业中，由于缺乏标注数据和显著的协变量变化，训练机器学习模型面临独特的挑战。本文提出了一种基于Unity引擎的葡萄园模拟器系统，该系统通过剪切粘贴技术生成逼真的合成图像及标签，以解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;精准农业领域存在数据标注不足的问题，并且环境动态性导致农作物外观随时间变化，这使得训练机器学习模型变得困难。缺乏多样化的数据限制了算法的性能和适应能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种生成真实感合成图像的方法来克服这些挑战，以便更好地训练用于监测和管理农业活动的检测算法。&lt;h4&gt;方法&lt;/h4&gt;利用Unity引擎创建一个虚拟葡萄园模拟器，并采用几何一致性剪切粘贴技术从合成环境中生成精确的照片级现实图像及标签。该系统可以自动生成各种视角和光照条件下的数据样本。&lt;h4&gt;主要发现&lt;/h4&gt;通过在提子栽培训练最先进的检测算法上应用这种方法，取得了显著的性能改进，表明所提出的组合技术可以有效增强模型的学习能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于合成数据的方法为精准农业提供了新的可能性。它不仅可以生成大量高质量的数据以克服标注不足的问题，而且能够模拟各种环境条件下的图像变化。此外，该方法易于自动化实施，对于其在实际农业生产中的应用非常重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一种用于解决精准农业中由于缺乏标记数据和显著协变量变化所带来的挑战的方法介绍。通过使用基于Unity引擎的葡萄园仿真器，并利用剪切粘贴技术生成逼真的合成图像与标签，可以训练检测算法以应对不同的视角和光照条件。这种方法在提子栽培场景下的应用表明它能够极大地改进最先进的检测模型性能。该方法由于其易于自动化的特性而可能被广泛采纳于实际农业生产实践中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CASE59546.2024.10711594&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In precision agriculture, the scarcity of labeled data and significantcovariate shifts pose unique challenges for training machine learning models.This scarcity is particularly problematic due to the dynamic nature of theenvironment and the evolving appearance of agricultural subjects as livingthings. We propose a novel system for generating realistic synthetic data toaddress these challenges. Utilizing a vineyard simulator based on the Unityengine, our system employs a cut-and-paste technique with geometricalconsistency considerations to produce accurate photo-realistic images andlabels from synthetic environments to train detection algorithms. This approachgenerates diverse data samples across various viewpoints and lightingconditions. We demonstrate considerable performance improvements in training astate-of-the-art detector by applying our method to table grapes cultivation.The combination of techniques can be easily automated, an increasinglyimportant consideration for adoption in agricultural practice.</description>
      <author>example@mail.com (Leonardo Saraceni, Ionut Marian Motoi, Daniele Nardi, Thomas Alessandro Ciarfuglia)</author>
      <guid isPermaLink="false">2502.18320v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>RSSI-Based Localization Utilizing Antenna Radiation Pattern And Biased CRLB Analysis</title>
      <link>http://arxiv.org/abs/2502.18311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文提出了一种新颖的室内定位方法，利用单天线系统中接收信号强度指示（RSSI）测量来获取天线辐射模式特征。通过旋转天线或重新配置其辐射模式，我们推导出了极大似然估计算法，该算法实现了接近克拉美罗下界（CRLB）的近优化定位精度。&lt;h4&gt;背景&lt;/h4&gt;现有的室内定位技术往往依赖于多天线系统，难以在保证高精度的同时保持系统的简洁性。本文通过研究单天线辐射模式特性，试图找到一种简化且高效的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一个利用单天线辐射特性的新型室内定位方法，并验证其在复杂环境下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;设计并实施了一种基于RSSI测量的极大似然估计（MLE）算法；通过旋转或重新配置天线的辐射模式来改善信号质量，减少位置误差。此外，提出了一种两步测量策略以消除对接收天线模式依赖的需求。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，随着信噪比、天线旋转次数和辐射模式变化的增加，估计精度有显著提高；模拟结果验证了该方法在室内机器人跟踪应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文所提出的定位技术能够有效解决室内环境下的机器人追踪问题，同时保持系统结构简单。这一方法为未来基于单天线系统的高精度室内定位提供了新的思路和依据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel indoor positioning approach that leveragesantenna radiation pattern characteristics through Received Signal StrengthIndication (RSSI) measurements in a single-antenna system. By rotating theantenna or reconfiguring its radiation pattern, we derive a maximum likelihoodestimation (MLE) algorithm that achieves near-optimal positioning accuracyapproaching the Cramer-Rao lower bound (CRLB). Through theoretical analysis, weestablish three fundamental theorems characterizing the estimation accuracybounds and demonstrating how performance improves with increasedsignal-to-noise ratio, antenna rotation count, and radiation patternvariations. Additionally, we propose a two-position measurement strategy thateliminates dependence on receiving antenna patterns. Simulation resultsvalidate that our approach provides an effective solution for indoor robottracking applications where both accuracy and system simplicity are essentialconsiderations.</description>
      <author>example@mail.com (Zhisheng Rong, Wenzhi Liu, Xiayue Liu, Zhixiang Xu, Yufei Jiang)</author>
      <guid isPermaLink="false">2502.18311v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Surgical Planner for Robot-Assisted Vitreoretinal Surgery: Integrating Eye Posture, Robot Position and Insertion Point</title>
      <link>http://arxiv.org/abs/2502.18230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种优化框架，用于调整眼科手术中眼睛的倾斜角度和机器人位置，以实现不同的患者目标区域。&lt;h4&gt;背景&lt;/h4&gt;最近开发了几种辅助视网膜手术的机器人系统。这些系统的准确性受限于其工作体积，并且通过外科显微镜看到的视野有限。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有手术机器人的限制，优化眼睛姿态和机器人定位以扩大可达性并减少重置准备过程的可能性。&lt;h4&gt;方法&lt;/h4&gt;研究使用了一种可以调整的眼模型来验证所提出的框架的有效性。评估了该工作流程在不同轴向的误差，并分析了可能的误差来源。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，优化后的框架能够以平均0.13度（Y轴旋转），-1.40度（X轴旋转）和1.80毫米（深度Z方向）的误差达到目标位置。这些误差在临床上是可接受的。&lt;h4&gt;结论&lt;/h4&gt;该优化框架可以提高手术机器人系统的目标可达性，并减少重置准备过程的可能性，具有显著的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经完整地进行了中文翻译并以JSON格式呈现出来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several robotic frameworks have been recently developed to assist ophthalmicsurgeons in performing complex vitreoretinal procedures such as subretinalinjection of advanced therapeutics. These surgical robots show promisingcapabilities; however, most of them have to limit their working volume toachieve maximum accuracy. Moreover, the visible area seen through the surgicalmicroscope is limited and solely depends on the eye posture. If the eyeposture, trocar position, and robot configuration are not correctly arranged,the instrument may not reach the target position, and the preparation will haveto be redone. Therefore, this paper proposes the optimization framework of theeye tilting and the robot positioning to reach various target areas fordifferent patients. Our method was validated with an adjustable phantom eyemodel, and the error of this workflow was 0.13 +/- 1.65 deg (rotational jointaround Y axis), -1.40 +/- 1.13 deg (around X axis), and 1.80 +/- 1.51 mm(depth, Z). The potential error sources are also analyzed in the discussionsection.</description>
      <author>example@mail.com (Satoshi Inagaki, Alireza Alikhani, Nassir Navab, Peter C. Issa, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2502.18230v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>iTrash: Incentivized Token Rewards for Automated Sorting and Handling</title>
      <link>http://arxiv.org/abs/2502.18161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Article submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为iTrash的智能垃圾桶，用于提高小型办公室环境中的回收率。&lt;h4&gt;背景&lt;/h4&gt;随着机器人系统的自主性增强，它们越来越多地被应用于小空间和办公环境中进行自动化任务。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新型智能垃圾桶iTrash来改善小型办公区域内的垃圾分类效率，并收集有关用户行为及垃圾箱使用模式的有价值数据。&lt;h4&gt;方法&lt;/h4&gt;进行了为期5天的实验以评估iTrash相较于传统垃圾桶的优势，发现其回收率提高了30%以上。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，使用智能垃圾桶不仅能提高回收利用率，还能提供诸如用户行为和垃圾桶使用情况等重要信息，这些是普通垃圾桶无法获取的数据。利用这些数据可以预测并优化某些任务。&lt;h4&gt;结论&lt;/h4&gt;探索了通过区块链技术创建经济激励机制以促进回收活动的可能性，采用了节约型付费模式（Save-as-you-Throw, SAYT）。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人系统的自主性增强，它们越来越多地被应用于小空间和办公环境中进行自动化任务。本文提出了一种名为iTrash的智能垃圾桶，用于提高小型办公室环境中的回收率。进行了为期5天的实验以评估iTrash相较于传统垃圾桶的优势，发现其回收率提高了30%以上。研究结果表明，使用这种新型智能垃圾桶不仅能提高回收利用率，还能提供诸如用户行为和垃圾箱使用情况等重要信息，这些是普通垃圾桶无法获取的数据。利用这些数据可以预测并优化某些任务。最后，本文探讨了通过区块链技术创建经济激励机制以促进回收活动的可能性，并采用了节约型付费模式（Save-as-you-Throw, SAYT）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robotic systems (RS) become more autonomous, they are becomingincreasingly used in small spaces and offices to automate tasks such ascleaning, infrastructure maintenance, or resource management. In this paper, wepropose iTrash, an intelligent trashcan that aims to improve recycling rates insmall office spaces. For that, we ran a 5 day experiment and found that iTrashcan produce an efficiency increase of more than 30% compared to traditionaltrashcans. The findings derived from this work, point to the fact that usingiTrash not only increase recyclying rates, but also provides valuable data suchas users behaviour or bin usage patterns, which cannot be taken from a normaltrashcan. This information can be used to predict and optimize some tasks inthese spaces. Finally, we explored the potential of using blockchain technologyto create economic incentives for recycling, following a Save-as-you-Throw(SAYT) model.</description>
      <author>example@mail.com (Pablo Ortega, Eduardo Castelló Ferrer)</author>
      <guid isPermaLink="false">2502.18161v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Real-time Spatio-Temporal Trajectory Planner for Autonomous Vehicles with Semantic Graph Optimization</title>
      <link>http://arxiv.org/abs/2502.18151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted for publication in IEEE Robotics and  Automation Letters (RA-L). The final published version is available in IEEE  Xplore (DOI: 10.1109/LRA.2024.3504239)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图优化的空间时间轨迹规划方法，用于实时为自主车辆在复杂城市环境中计划安全和可行的路径。&lt;h4&gt;背景&lt;/h4&gt;在复杂的城市环境中，通过充分利用感知信息实现实时的安全且可行的自动驾驶车辆轨迹规划是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的多模式感知信息提取及快速生成可行路径的方法。&lt;h4&gt;方法&lt;/h4&gt;通过构建语义空间时间图并通过静态和动态障碍物的分离处理有效提取感知模块的多模态信息，然后基于语义空间时间超图进行稀疏图优化快速生成轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够有效地处理复杂的城区公共道路场景，并能在实时运行中表现出色。&lt;h4&gt;结论&lt;/h4&gt;该研究方法在复杂的城市环境中的性能得到了验证。此外还将发布代码供科研社区使用以支持基准测试。&lt;h4&gt;翻译&lt;/h4&gt;计划在复杂城市环境中，为自主车辆在实时内利用感知信息规划出安全且可行的轨迹是一项挑战。本文提出了一种基于图优化的空间时间轨迹规划方法。该方法通过构建语义空间时间图，并对静态和动态障碍物进行分离处理以有效提取感知模块的多模态信息，然后通过基于语义空间时间超图的稀疏图优化快速生成可行路径。大量实验表明所提出的方法能够有效地应对复杂的城区公共道路场景并能实时运行。我们也将发布代码来支持科研社区的基准测试研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2024.3504239&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning a safe and feasible trajectory for autonomous vehicles in real-timeby fully utilizing perceptual information in complex urban environments ischallenging. In this paper, we propose a spatio-temporal trajectory planningmethod based on graph optimization. It efficiently extracts the multi-modalinformation of the perception module by constructing a semantic spatio-temporalmap through separation processing of static and dynamic obstacles, and thenquickly generates feasible trajectories via sparse graph optimization based ona semantic spatio-temporal hypergraph. Extensive experiments have proven thatthe proposed method can effectively handle complex urban public road scenariosand perform in real time. We will also release our codes to accommodatebenchmarking for the research community</description>
      <author>example@mail.com (Shan He, Yalong Ma, Tao Song, Yongzhi Jiang, Xinkai Wu)</author>
      <guid isPermaLink="false">2502.18151v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze and Bottleneck</title>
      <link>http://arxiv.org/abs/2502.18121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的算法GazeBot，该算法通过利用注视信息和运动瓶颈来提高机器人操作的可重用性。&lt;h4&gt;背景&lt;/h4&gt;虽然深度学习的进步使得复制人类远程操作机器人的灵巧度变得越来越可行，但是将这些获得的能力推广到未见过的情景中仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的算法GazeBot，以克服现有模仿学习方法在泛化性能方面的限制。&lt;h4&gt;方法&lt;/h4&gt;利用注视信息和运动瓶颈作为关键特征来提高机器人的操作技能的可重用性。通过提供带有注视数据的演示数据集，整个训练过程是完全基于数据驱动的。&lt;h4&gt;主要发现&lt;/h4&gt;GazeBot算法实现了比现有模仿学习方法更高的泛化性能，并且不牺牲其灵巧性和反应能力。&lt;h4&gt;结论&lt;/h4&gt;提供的视频和代码可以在https://crumbyrobotics.github.io/gazebot获取。&lt;h4&gt;翻译&lt;/h4&gt;自主代理能够进行多样化的物体操作，应该能够以高可重用性的方式获得广泛的操纵技能。尽管深度学习的进步使得复制人类远程操作的灵巧度在机器人中变得越来越可行，但将这些获得的能力推广到未见过的情景中仍然是一个挑战。在这项研究中，我们提出了一种新的算法GazeBot（基于注视信息和运动瓶颈感知的机器人操作），它即使当物体位置和末端执行器姿态与提供的演示不同时，也能实现所学动作的高可重用性。通过利用注视信息和运动瓶颈作为进行物体操作的关键特征，GazeBot在泛化性能方面相比当前最先进的模仿学习方法具有显著优势，同时不牺牲其灵巧性和反应能力。此外，在提供带有注视数据的演示数据集之后，GazeBot的训练过程完全基于数据驱动。视频和代码可在https://crumbyrobotics.github.io/gazebot获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous agents capable of diverse object manipulations should be able toacquire a wide range of manipulation skills with high reusability. Althoughadvances in deep learning have made it increasingly feasible to replicate thedexterity of human teleoperation in robots, generalizing these acquired skillsto previously unseen scenarios remains a significant challenge. In this study,we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation(GazeBot), which enables high reusability of the learned motions even when theobject positions and end-effector poses differ from those in the provideddemonstrations. By leveraging gaze information and motion bottlenecks, bothcrucial features for object manipulation, GazeBot achieves high generalizationperformance compared with state-of-the-art imitation learning methods, withoutsacrificing its dexterity and reactivity. Furthermore, the training process ofGazeBot is entirely data-driven once a demonstration dataset with gaze data isprovided. Videos and code are available athttps://crumbyrobotics.github.io/gazebot.</description>
      <author>example@mail.com (Ryo Takizawa, Izumi Karino, Koki Nakagawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi)</author>
      <guid isPermaLink="false">2502.18121v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration</title>
      <link>http://arxiv.org/abs/2502.18072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一种名为MRBTP的多机器人行为树规划算法，适用于同质和异质机器人群体，并具有理论上的完整性和正确性保证。&lt;h4&gt;背景&lt;/h4&gt;在机器人领域，多机器人任务规划与协作是非常关键的问题。尽管行为树（BT）作为一种流行控制架构被广泛应用于单个机器人的规划中，但为多机器人开发有效的BT规划算法仍然面临复杂性挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对协调不同动作空间的复杂性问题，论文旨在设计一种适用于同质和异质多机器人群体的行为树规划方法，并探讨如何利用大型语言模型进一步提高其效率。&lt;h4&gt;方法&lt;/h4&gt;MRBTP算法通过跨树扩展实现异构动作之间的协调；对于同类动作，则保留各行为树间的备份结构以确保鲁棒性及避免冗余执行。此外，当有大型语言模型可用时，MRBTP可以额外使用此插件来预规划与目标相关的长时期子树。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，在仓库管理和日常服务场景下，MRBTP算法表现出良好的稳健性和执行效率，并且借助于预先训练的语言模型生成的任务特定子树能显著提高其计划速度和协作效率。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种适用于同质与异质机器人群体的多机器人行为树规划方法MRBTP，并证明了该方法在实际应用中的有效性和优越性。此外，通过引入大型语言模型插件进一步提升了算法的速度和效果。&lt;h4&gt;翻译&lt;/h4&gt;Multi-robot task planning and collaboration are critical challenges in robotics. While Behavior Trees (BTs) have been established as a popular control architecture for single robots, the development of effective multi-robot BT planning algorithms remains challenging due to the complexity of coordinating diverse action spaces. The paper proposes the Multi-Robot Behavior Tree Planning (MRBTP) algorithm with theoretical guarantees of soundness and completeness. MRBTP features cross-tree expansion to coordinate heterogeneous actions across different BTs for achieving team goals, and retains backup structures among homogeneous actions to ensure robustness and prevent redundant execution through intention sharing. Additionally, when available, a plugin using Large Language Models (LLMs) can pre-plan goal-related actions forming long-horizon subtrees significantly enhancing planning speed and collaboration efficiency. Evaluations in warehouse management and everyday service scenarios demonstrate MRBTP's robustness and execution efficiency under varying settings, along with the ability of the pre-trained LLM to generate effective task-specific subtrees for MRBTP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-robot task planning and collaboration are critical challenges inrobotics. While Behavior Trees (BTs) have been established as a popular controlarchitecture and are plannable for a single robot, the development of effectivemulti-robot BT planning algorithms remains challenging due to the complexity ofcoordinating diverse action spaces. We propose the Multi-Robot Behavior TreePlanning (MRBTP) algorithm, with theoretical guarantees of both soundness andcompleteness. MRBTP features cross-tree expansion to coordinate heterogeneousactions across different BTs to achieve the team's goal. For homogeneousactions, we retain backup structures among BTs to ensure robustness and preventredundant execution through intention sharing. While MRBTP is capable ofgenerating BTs for both homogeneous and heterogeneous robot teams, itsefficiency can be further improved. We then propose an optional plugin forMRBTP when Large Language Models (LLMs) are available to reason goal-relatedactions for each robot. These relevant actions can be pre-planned to formlong-horizon subtrees, significantly enhancing the planning speed andcollaboration efficiency of MRBTP. We evaluate our algorithm in warehousemanagement and everyday service scenarios. Results demonstrate MRBTP'srobustness and execution efficiency under varying settings, as well as theability of the pre-trained LLM to generate effective task-specific subtrees forMRBTP.</description>
      <author>example@mail.com (Yishuai Cai, Xinglin Chen, Zhongxuan Cai, Yunxin Mao, Minglong Li, Wenjing Yang, Ji Wang)</author>
      <guid isPermaLink="false">2502.18072v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>HEROS-GAN: Honed-Energy Regularized and Optimal Supervised GAN for Enhancing Accuracy and Range of Low-Cost Accelerometers</title>
      <link>http://arxiv.org/abs/2502.18064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI Oral; AI for Sensors; Generative Deep Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的生成对抗网络HEROS-GAN，用于改善低成本加速度计的精度和范围限制问题。&lt;h4&gt;背景&lt;/h4&gt;低成本加速度计因其体积小、易于集成、穿戴舒适以及可大规模生产等优点，在汽车系统、航空航天及可穿戴技术等领域得到广泛应用。然而，此类传感器存在严重的准确度与量程局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将低成本传感器信号转化为高成本等价信号的技术，以克服低性能加速度计的精度和范围限制问题。&lt;h4&gt;方法&lt;/h4&gt;{'HEROS-GAN': '一种能量调节和最优监督生成对抗网络，用于改善低成本加速度计信号的质量。', 'OTS': '基于最优传输理论探索未配对数据之间的潜在一致性，最大化监督信息的方法。', 'MLE': '调制拉普拉斯能量注入方法，旨在鼓励生成器打破范围限制、增强局部变化并丰富信号细节。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': '实验结果显示单独使用OTS或MLE的GAN相比现有信号增强SOTA方法有显著提升。', '综合效果': '结合OTS和MLE后，HEROS-GAN能够将加速度计范围加倍，并降低信号噪声两个数量级，在加速度计信号处理方面建立了一个新的基准。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入HEROS-GAN及其相关技术，本研究为解决低成本加速度计的精度与量程限制提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了用于改善低成本加速度计性能的技术方法和实验结果，包括提出的框架HEROS-GAN以及专门建立的数据集LASED。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-cost accelerometers play a crucial role in modern society due to theiradvantages of small size, ease of integration, wearability, and massproduction, making them widely applicable in automotive systems, aerospace, andwearable technology. However, this widely used sensor suffers from severeaccuracy and range limitations. To this end, we propose a honed-energyregularized and optimal supervised GAN (HEROS-GAN), which transforms low-costsensor signals into high-cost equivalents, thereby overcoming the precision andrange limitations of low-cost accelerometers. Due to the lack of frame-levelpaired low-cost and high-cost signals for training, we propose an OptimalTransport Supervision (OTS), which leverages optimal transport theory toexplore potential consistency between unpaired data, thereby maximizingsupervisory information. Moreover, we propose a Modulated Laplace Energy (MLE),which injects appropriate energy into the generator to encourage it to breakrange limitations, enhance local changes, and enrich signal details. Given theabsence of a dedicated dataset, we specifically establish a Low-costAccelerometer Signal Enhancement Dataset (LASED) containing tens of thousandsof samples, which is the first dataset serving to improve the accuracy andrange of accelerometers and is released in Github. Experimental resultsdemonstrate that a GAN combined with either OTS or MLE alone can surpass theprevious signal enhancement SOTA methods by an order of magnitude. Integratingboth OTS and MLE, the HEROS-GAN achieves remarkable results, which doubles theaccelerometer range while reducing signal noise by two orders of magnitude,establishing a benchmark in the accelerometer signal processing.</description>
      <author>example@mail.com (Yifeng Wang, Yi Zhao)</author>
      <guid isPermaLink="false">2502.18064v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Ordered Genetic Algorithm for Entrance Dependent Vehicle Routing Problem in Farms</title>
      <link>http://arxiv.org/abs/2502.18062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的车辆路径问题（EDVRP），并针对农场场景提出了数学模型和一种有序遗传算法（OGA）来解决此问题。&lt;h4&gt;背景&lt;/h4&gt;在一些实际的车辆路径问题场景中，城市的规模及其入口数量对优化过程有着显著的影响。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述情况，作者构建了依赖于入口的车辆路径问题（EDVRP），并通过实验验证了有序遗传算法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了用于农场场景中的EDVRP数学模型，并开发了一种名为OGA的新颖遗传算法来解决该问题。此外还通过删除实验验证了新操作符的效果。&lt;h4&gt;主要发现&lt;/h4&gt;与随机策略基线和没有排序的遗传算法相比，OGA在优化过程中显示出一定的优势。新型的操作符也证明了它们对提高算法性能的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，针对特定场景设计的EDVRP及其相应的有序遗传算法可以在解决实际问题时提供更加有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;车辆路径问题是许多生产场景中广泛研究的重要议题。在某些实际应用场景下，城市的规模和入口数量显著影响优化过程。为了解决这一问题，作者构建了依赖于入口的车辆路径问题（EDVRP）以描述此类问题，并提供了农场场景下的数学模型以及提出了一种有序遗传算法（OGA）。通过多组随机生成案例实验表明，与基准线策略相比，OGA展示出一定的优势。此外，新引入的操作符在消除实验中证明了其对改进算法性能的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle Routing Problems (VRP) are widely studied issues that play importantroles in many production scenarios. We have noticed that in some practicalscenarios of VRP, the size of cities and their entrances can significantlyinfluence the optimization process. To address this, we have constructed theEntrance Dependent VRP (EDVRP) to describe such problems. We provide amathematical formulation for the EDVRP in farms and propose an Ordered GeneticAlgorithm (OGA) to solve it. The effectiveness of OGA is demonstrated throughour experiments, which involve a multitude of randomly generated cases. Theresults indicate that OGA offers certain advantages compared to a randomstrategy baseline and a genetic algorithm without ordering. Furthermore, thenovel operators introduced in this paper have been validated through ablationexperiments, proving their effectiveness in enhancing the performance of thealgorithm.</description>
      <author>example@mail.com (Haotian Xu, Xiaohui Fan, Jialin Zhu, Qing Zhuo, Tao Zhang)</author>
      <guid isPermaLink="false">2502.18062v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>S-Graphs 2.0 -- A Hierarchical-Semantic Optimization and Loop Closure for SLAM</title>
      <link>http://arxiv.org/abs/2502.18044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, RAL submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的机器人定位和制图算法Situation Graphs 2.0，该算法利用室内场景的层次结构来提高数据管理和优化效率。通过构建四个层级的情境图形（关键帧、墙壁、房间、楼层），实现了更高效的多层环境中的姿态管理与地图优化。&lt;h4&gt;背景&lt;/h4&gt;基于定位和制图的方法通常没有充分利用环境中固有的语义信息，导致机器人姿势不准确且在大规模环境下计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来利用3D场景图形的层次化表示来提高机器人位置管理和优化的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 构建包含关键帧、墙壁、房间、楼层四个层级的情境图；2. 前端模块包括一个能识别楼梯并分配楼层级语义关系的楼层检测模块，这使得可以拒绝视觉上相似但位于不同楼层区域中的假阳性闭环；3. 利用层次结构进行改进优化，包括局部优化、楼层面全局优化和房间级别局部优化。&lt;h4&gt;主要发现&lt;/h4&gt;Situation Graphs 2.0 在多层真实环境中表现出了优越的性能，并能够创建层级地图同时限制计算复杂性，而一些基准方法在大规模场景中难以有效执行。&lt;h4&gt;结论&lt;/h4&gt;Situation Graphs 2.0 是一种有效的机器人定位和制图算法，在大型多层环境中的数据管理和优化方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Works based on localization and mapping do not exploit the inherentsemantic-relational information from the environment for faster and efficientmanagement and optimization of the robot poses and its map elements, oftenleading to pose and map inaccuracies and computational inefficiencies in largescale environments. 3D scene graph representations which distributes theenvironment in an hierarchical manner can be exploited to enhance themanagement/optimization of underlying robot poses and its map.  In this direction, we present our work Situational Graphs 2.0, whichleverages the hierarchical structure of indoor scenes for efficient datamanagement and optimization. Our algorithm begins by constructing a situationalgraph that organizes the environment into four layers: Keyframes, Walls, Rooms,and Floors. Our first novelty lies in the front-end which includes a floordetection module capable of identifying stairways and assigning a floor-levelsemantic-relations to the underlying layers. This floor-level semantic enablesa floor-based loop closure strategy, rejecting false-positive loop closures invisually similar areas on different floors. Our second novelty is in exploitingthe hierarchy for an improved optimization. It consists of: (1) localoptimization, optimizing a window of recent keyframes and their connectedcomponents, (2) floor-global optimization, which focuses only on keyframes andtheir connections within the current floor during loop closures, and (3)room-local optimization, marginalizing redundant keyframes that shareobservations within the room.  We validate our algorithm extensively in different real multi-floorenvironments. Our approach can demonstrate state-of-art-art results in largescale multi-floor environments creating hierarchical maps while bounding thecomputational complexity where several baseline works fail to executeefficiently.</description>
      <author>example@mail.com (Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos)</author>
      <guid isPermaLink="false">2502.18044v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Interaction and Intention Communication for Industrial Robots</title>
      <link>http://arxiv.org/abs/2502.17971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the 1st German Robotics Conference (GRC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种设计和评估非人形工业机器人表达性的人机交互系统的方法，特别是通过一个小类人的机器人作为其主机（如叉车）的代理来沟通。&lt;h4&gt;背景&lt;/h4&gt;为了实现高级的人机交互水平，工业机器人需要能够安全有效地在人类环境中操作、进行自然交流、理解用户，并以直观的方式表达意图而不引起不必要的干扰。&lt;h4&gt;目的&lt;/h4&gt;论文旨在为非人形工业机器人设计和增强表达性的人机交互系统，通过开发一种结合语音、运动等多种模态的多模式通信框架来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种概念，即使用一个小类人的机器人作为非人形主机（例如叉车）的代理进行交流，并为此机器人开发了一个多模态和大型语言模型增强的通讯框架。实验通过凝视追踪和动作捕捉技术量化了用户对机器人的感知以及任务进度。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了如何通过结合不同通信方式，可以使工业机器人在人机交互中更加有效且自然地交流。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，使用多模态通讯框架的表达性人机交互系统对于提高非人形工业机器人的互动性和用户接受度至关重要。&lt;h4&gt;翻译&lt;/h4&gt;成功的工业机器人采用将强烈依赖于它们能够在人类环境中安全有效地操作、进行自然沟通、理解用户，并以直观的方式表达意图而不引起不必要的干扰。为了实现这种高级水平的人机交互，机器人需要获取并整合对用户任务和环境的知识，并采取结合语音、动作等多模态的通讯方式。本文介绍了一些设计、增强和完善非人形工业机器人表达性人机交互系统的方案。我们提出了一个小型类人机器人作为其主机（如叉车）的代理进行沟通的概念，为此机器人开发了一个多模式且增强了大型语言模型的通信框架，并通过实验室实验进行了评估。这些实验利用凝视追踪和动作捕捉技术量化了用户对机器人的感知以及任务进度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Successful adoption of industrial robots will strongly depend on theirability to safely and efficiently operate in human environments, engage innatural communication, understand their users, and express intentionsintuitively while avoiding unnecessary distractions. To achieve this advancedlevel of Human-Robot Interaction (HRI), robots need to acquire and incorporateknowledge of their users' tasks and environment and adopt multimodalcommunication approaches with expressive cues that combine speech, movement,gazes, and other modalities. This paper presents several methods to design,enhance, and evaluate expressive HRI systems for non-humanoid industrialrobots. We present the concept of a small anthropomorphic robot communicatingas a proxy for its non-humanoid host, such as a forklift. We developed amultimodal and LLM-enhanced communication framework for this robot andevaluated it in several lab experiments, using gaze tracking and motion captureto quantify how users perceive the robot and measure the task progress.</description>
      <author>example@mail.com (Tim Schreiter, Andrey Rudenko, Jens V. Rüppel, Martin Magnusson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2502.17971v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Quadrotor Neural Dead Reckoning in Periodic Trajectories</title>
      <link>http://arxiv.org/abs/2502.17964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于神经网络的四旋翼无人机死 reckoning 方法，用于在纯惯性导航模式下飞行时提高定位精度。&lt;h4&gt;背景&lt;/h4&gt;由于环境或硬件限制，在室内和室外操作时，四旋翼无人机被迫以纯惯性导航模式运行。这导致了惯性漂移的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来增强基于周期轨迹的四旋翼无人机在纯惯性导航模式下的定位性能。&lt;h4&gt;方法&lt;/h4&gt;采用了一种简单的高效神经网络模型直接从惯性读数中估计四旋翼的位置向量，而不是将距离回归与基于惯性模型的方向估计相结合的方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过仅进行软件修改，在室外实现了平均误差降低 27%，在室内实现平均误差降低了 79% 的定位精度提升。&lt;h4&gt;结论&lt;/h4&gt;改进的定位准确度使得四旋翼无人机可以无缝地完成其任务，而无需额外的硬件支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：现实世界中，由于环境或硬件限制，在室内或室外操作时，四旋翼被迫以纯惯性导航模式运行。为减少惯性漂移，提出了结合了四旋翼周期轨迹的端到端神经网络方法。其中，通过回归四旋翼距离并将其与基于惯性模型的方向估计相结合来估算四旋翼的位置向量。为了进一步增强定位性能，在本文中我们提出了一种针对沿周期轨迹飞行的四旋翼无人机的神经死 reckoning 方法。在这种情况下，惯性读数被馈送到一个简单而高效的网络中以直接估计四旋翼位置向量。我们的方法在两种不同的四旋翼上进行了评估：一种在室内操作，另一种在室外操作。与深度学习方法相比，我们的方法提高了定位精度，在户外平均误差减少了 27%，在室内减少了 79%，并且只需要软件修改。通过我们方法实现的改进定位准确度，四旋翼可以无缝执行其任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real world scenarios, due to environmental or hardware constraints, thequadrotor is forced to navigate in pure inertial navigation mode whileoperating indoors or outdoors. To mitigate inertial drift, end-to-end neuralnetwork approaches combined with quadrotor periodic trajectories weresuggested. There, the quadrotor distance is regressed and combined withinertial model-based heading estimation, the quadrotor position vector isestimated. To further enhance positioning performance, in this paper we proposea quadrotor neural dead reckoning approach for quadrotors flying on periodictrajectories. In this case, the inertial readings are fed into a simple andefficient network to directly estimate the quadrotor position vector. Ourapproach was evaluated on two different quadrotors, one operating indoors whilethe other outdoors. Our approach improves the positioning accuracy of otherdeep-learning approaches, achieving an average 27% reduction in error outdoorsand an average 79% reduction indoors, while requiring only softwaremodifications. With the improved positioning accuracy achieved by our method,the quadrotor can seamlessly perform its tasks.</description>
      <author>example@mail.com (Shira Massas, Itzik Klein)</author>
      <guid isPermaLink="false">2502.17964v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer</title>
      <link>http://arxiv.org/abs/2502.17949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to JICV (Journal of Intelligent and Connected Vehicles)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了InVDriver系统，这是一种新型的向量化查询框架，用于解决现有自动驾驶中基于点的空间关联忽略问题，从而提高规划精度和轨迹平滑度。&lt;h4&gt;背景&lt;/h4&gt;端到端自动驾驶因其整体优化能力而在学术界和工业界越来越受到关注。向量化表示方法通过保留实例级别的拓扑信息并减少计算开销而变得流行起来。然而，现有的向量化查询框架往往忽略了实例内部点之间的空间关联，导致几何不一致的输出。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有系统的问题，提出了一种新的基于向量化的查询系统InVDriver，它通过屏蔽自注意力层系统地建模了实例内空间依赖关系。&lt;h4&gt;方法&lt;/h4&gt;InVDriver在感知、预测和规划的所有核心模块中都集成了屏蔽的自我注意机制，这些机制限制了对内部点交互的关注，并允许结构元素的同时细化，同时抑制不相关的跨实例噪声。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在nuScenes基准测试上，InVDriver实现了最先进的性能，超越了先前的方法，不仅在精度和安全性方面表现出色，而且还保持了计算效率。这验证了对实例内部几何一致性进行显式建模对于向量化自动驾驶系统的重要性，弥合了端到端框架的理论优势与实际部署需求之间的差距。&lt;h4&gt;结论&lt;/h4&gt;InVDriver通过精确建模空间依赖关系来改进端到端自动驾驶系统的性能和可靠性，证明了在自动驾驶技术中引入这种新方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;端到端自主驾驶因其整体优化能力，在学术界和工业界越来越受欢迎。向量化表示方法由于保留实例级别的拓扑信息并且减少计算负担而崭露头角。然而，现有的基于查询的框架往往忽视了实例内点之间的固有空间关联，导致几何不一致的结果（例如片段化的HD地图元素或振荡轨迹）。为了解决这些问题，我们提出了InVDriver——一个新颖的向量化查询系统，通过屏蔽自注意力层来系统性地建模内部的空间依赖关系，从而提高规划精度和轨迹平滑度。在感知、预测以及规划的所有核心模块中，InVDriver采用了屏蔽自注意机制，这种机制限制了对内部点交互的关注，允许同时优化结构元素的同时抑制无关的跨实例噪音。实验结果表明，在nuScenes基准测试上，InVDriver实现了最先进的性能，并且超过了之前的方法在准确性和安全性方面的要求，同时保持了高计算效率。我们的工作验证了显式建模内部几何一致性的关键性对于推进向量化自动驾驶系统的重要性，这使得端到端框架的理论优势和实际部署要求之间差距得以缩小。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving with its holistic optimization capabilities,has gained increasing traction in academia and industry. Vectorizedrepresentations, which preserve instance-level topological information whilereducing computational overhead, have emerged as a promising paradigm. Whileexisting vectorized query-based frameworks often overlook the inherent spatialcorrelations among intra-instance points, resulting in geometricallyinconsistent outputs (e.g., fragmented HD map elements or oscillatorytrajectories). To address these limitations, we propose InVDriver, a novelvectorized query-based system that systematically models intra-instance spatialdependencies through masked self-attention layers, thereby enhancing planningaccuracy and trajectory smoothness. Across all core modules, i.e., perception,prediction, and planning, InVDriver incorporates masked self-attentionmechanisms that restrict attention to intra-instance point interactions,enabling coordinated refinement of structural elements while suppressingirrelevant inter-instance noise. Experimental results on the nuScenes benchmarkdemonstrate that InVDriver achieves state-of-the-art performance, surpassingprior methods in both accuracy and safety, while maintaining high computationalefficiency. Our work validates that explicit modeling of intra-instancegeometric coherence is critical for advancing vectorized autonomous drivingsystems, bridging the gap between theoretical advantages of end-to-endframeworks and practical deployment requirements.</description>
      <author>example@mail.com (Bo Zhang, Heye Huang, Chunyang Liu, Yaqin Zhang, Zhenhua Xu)</author>
      <guid isPermaLink="false">2502.17949v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>corobos: A Design for Mobile Robots Enabling Cooperative Transitions between Table and Wall Surfaces</title>
      <link>http://arxiv.org/abs/2502.17868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': 'corobos 是一种概念设计，使得多机器人系统能够在没有人为干预的情况下，在桌面和墙面之间平滑过渡。', '背景': '群组用户界面允许通过多个移动机器人的使用来动态安排用户环境，但由于其两轮推进系统的限制，操作范围通常局限于单一平面。', '目的': '展示 corobos 设计概念，该设计让机器人能够在水平面（桌面）和垂直面（墙面）之间平滑过渡，而无需额外的主动电气组件。', '方法': '每个机器人都装备了独特的斜坡结构，在其他机器人推动时能够实现平稳旋转。研究了这种结构的设计参数，并通过实验评估其转换成功率。', '主要发现': '设计仅依赖于被动机械元件，无需额外的主动电气部件即可实现无缝过渡。', '结论': '展示了 corobos 的各种应用实例，证明其在增强用户环境中具有巨大潜力。', '翻译': 'Swarm User Interfaces 允许通过使用多个移动机器人来动态调整用户环境，但它们的操作范围通常受限于两轮推进系统的限制而局限于单一平面。我们提出了一种概念设计 corobos，使得这些机器人可以在没有人为干预的情况下，在水平的桌面和垂直的墙面之间平滑过渡。每个机器人都配备了独特的斜坡结构，在其他机器人推动时可以实现平稳旋转，并且这种设计仅依赖于被动机械元件，不需要额外的主动电气组件。我们研究了这一结构的设计参数并通过实验评估其转换成功率，此外还展示了各种应用示例以展示 corobos 在增强用户环境方面的潜力。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3706598.3713440&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Swarm User Interfaces allow dynamic arrangement of user environments throughthe use of multiple mobile robots, but their operational range is typicallyconfined to a single plane due to constraints imposed by their two-wheelpropulsion systems. We present corobos, a proof-of-concept design that enablesthese robots to cooperatively transition between table (horizontal) and wall(vertical) surfaces seamlessly, without human intervention. Each robot isequipped with a uniquely designed slope structure that facilitates smoothrotation when another robot pushes it toward a target surface. Notably, thisdesign relies solely on passive mechanical elements, eliminating the need foradditional active electrical components. We investigated the design parametersof this structure and evaluated its transition success rate throughexperiments. Furthermore, we demonstrate various application examples toshowcase the potential of corobos in enhancing user environments.</description>
      <author>example@mail.com (Changyo Han, Yosuke Nakagawa, Takeshi Naemura)</author>
      <guid isPermaLink="false">2502.17868v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</title>
      <link>http://arxiv.org/abs/2502.17834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission at IEEE-IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探讨了物体重量对人手交接过程中人体运动和抓取-释放动作的影响，旨在通过引入基于人类行为分析的自适应机器人策略来增强机器人与人的互动自然性、安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;研究发现不同重量的物体会影响人在进行手部交互时的动作，这对设计更符合人类行为习惯的机器人至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据物体重量调整抓取-释放方式的自适应策略，提升机器人与人之间在不同重量物品交换过程中的表现和用户体验。&lt;h4&gt;方法&lt;/h4&gt;通过分析人在不同重量下进行手部交互的行为模式，提出并测试了基于人类行为模型的自适应机器人技术，并创建了一个包含多种物体重量数据集（包括YCB handover dataset）以验证策略的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;研究证明了提出的自适应抓取-释放技术和运动调整方法能够在自然度、效率和用户感知方面超越基准方法，显著改善机器人与人的手部交互体验。&lt;h4&gt;结论&lt;/h4&gt;通过将机器人的动作设计得更接近于人类的行为模式，可以大大提高机器人在处理不同重量物品时的手动交换过程的安全性和流畅性。这项工作为进一步研究提供了重要数据和理论基础。&lt;h4&gt;翻译&lt;/h4&gt;这项工作的摘要描述了一项探索物体重量对人手交接中人体运动及抓取释放行为影响的研究，旨在通过引入基于人类交互分析的自适应机器人策略来提升机器人的自然、安全且高效的人机互动能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the effect of object weight on human motion and griprelease during handovers to enhance the naturalness, safety, and efficiency ofrobot-human interactions. We introduce adaptive robotic strategies based on theanalysis of human handover behavior with varying object weights. The keycontributions of this work includes the development of an adaptive grip-releasestrategy for robots, a detailed analysis of how object weight influences humanmotion to guide robotic motion adaptations, and the creation ofhandover-datasets incorporating various object weights, including the YCBhandover dataset. By aligning robotic grip release and motion with humanbehavior, this work aims to improve robot-human handovers for differentweighted objects. We also evaluate these human-inspired adaptive roboticstrategies in robot-to-human handovers to assess their effectiveness andperformance and demonstrate that they outperform the baseline approaches interms of naturalness, efficiency, and user perception.</description>
      <author>example@mail.com (Parag Khanna, Mårten Björkman, Christian Smith)</author>
      <guid isPermaLink="false">2502.17834v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2502.17821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Collaborative Auxiliary Modality Learning (CAML)，一种新型的多智能体跨模态学习框架，适用于动态环境如自动驾驶车辆，并通过实验验证了其在事故检测和协作语义分割中的效果。&lt;h4&gt;背景&lt;/h4&gt;现有的跨模态学习方法主要在单个代理环境下工作，在具有复杂动态环境的情况下会导致决策盲点问题。这些问题尤其影响到连接自主汽车（CAV）的安全性和性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多智能体框架，使多个代理能够共享和协作使用多种模式的数据，并且能够在测试期间减少每个代理的模态输入。&lt;h4&gt;方法&lt;/h4&gt;提出了CAML框架，在训练时允许跨模态数据的合作，并在测试阶段可以进行单个模式的推理。该框架特别关注不确定性的降低和数据覆盖范围的增加，提供了理论上的优势分析。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有技术相比，CAML能显著提高事故检测准确率（最高提升58.13%）；同时，在真实的空地机器人协作语义分割任务中也表现出色，达到了高达10.61%mIoU的改进。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了CAML在解决动态环境下的决策问题方面具有巨大潜力，并为进一步的研究提供了一个新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modality learning has become a crucial technique for improving theperformance of machine learning applications across domains such as autonomousdriving, robotics, and perception systems. While existing frameworks such asAuxiliary Modality Learning (AML) effectively utilize multiple data sourcesduring training and enable inference with reduced modalities, they primarilyoperate in a single-agent context. This limitation is particularly critical indynamic environments, such as connected autonomous vehicles (CAV), whereincomplete data coverage can lead to decision-making blind spots. To addressthese challenges, we propose Collaborative Auxiliary Modality Learning($\textbf{CAML}$), a novel multi-agent multi-modality framework that enablesagents to collaborate and share multimodal data during training while allowinginference with reduced modalities per agent during testing. We systematicallyanalyze the effectiveness of $\textbf{CAML}$ from the perspective ofuncertainty reduction and data coverage, providing theoretical insights intoits advantages over AML. Experimental results in collaborative decision-makingfor CAV in accident-prone scenarios demonstrate that \ours~achieves up to a${\bf 58.13}\%$ improvement in accident detection. Additionally, we validate$\textbf{CAML}$ on real-world aerial-ground robot data for collaborativesemantic segmentation, achieving up to a ${\bf 10.61}\%$ improvement in mIoU.</description>
      <author>example@mail.com (Rui Liu, Yu Shen, Peng Gao, Pratap Tokekar, Ming Lin)</author>
      <guid isPermaLink="false">2502.17821v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.17813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Due to the limitation "The abstract field cannot be longer than 1,920  characters", the abstract here is shorter than that in the PDF file&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;安全导航对于在危险环境中操作的自主系统至关重要。&lt;h4&gt;背景&lt;/h4&gt;传统规划方法擅长处理长时任务，但依赖于预定义的距离图。相比之下，安全强化学习（Safe RL）可以不用手动启发式就能学习复杂行为，但在目标条件和多代理场景中的长期任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了规划与安全强化学习优点的新方法。&lt;h4&gt;方法&lt;/h4&gt;{'融合': '将目标导向的RL和安全RL结合来学习导航策略', '估算': '通过自动化自训练算法使用学得的价值函数同时估计累积距离和安全性水平', '图构建': '从回放缓存中构造状态图，并修剪不安全边，生成基于航点的计划'}&lt;h4&gt;主要发现&lt;/h4&gt;{'长时导航': '在扩展距离上有效平衡快速与安全路线。', '多代理问题': '利用冲突基础搜索（CBS）创建多个代理的安全路径规划，以解决长期范围内的多代理安全导航问题。这种方法提高了目标导向安全RL的可扩展性，并促进了代理之间的高效协调'}&lt;h4&gt;结论&lt;/h4&gt;{'效果证明': '通过广泛的基准测试，证明了该方法在复杂危险环境中实现多代理距离目标时的有效性和安全性。', '未来工作': '将发布代码以支持未来的相关研究。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safe navigation is essential for autonomous systems operating in hazardousenvironments. Traditional planning methods excel at long-horizon tasks but relyon a predefined graph with fixed distance metrics. In contrast, safeReinforcement Learning (RL) can learn complex behaviors without relying onmanual heuristics but fails to solve long-horizon tasks, particularly ingoal-conditioned and multi-agent scenarios.  In this paper, we introduce a novel method that integrates the strengths ofboth planning and safe RL. Our method leverages goal-conditioned RL and safe RLto learn a goal-conditioned policy for navigation while concurrently estimatingcumulative distance and safety levels using learned value functions via anautomated self-training algorithm. By constructing a graph with states from thereplay buffer, our method prunes unsafe edges and generates a waypoint-basedplan that the agent follows until reaching its goal, effectively balancingfaster and safer routes over extended distances.  Utilizing this unified high-level graph and a shared low-levelgoal-conditioned safe RL policy, we extend this approach to address themulti-agent safe navigation problem. In particular, we leverage Conflict-BasedSearch (CBS) to create waypoint-based plans for multiple agents allowing fortheir safe navigation over extended horizons. This integration enhances thescalability of goal-conditioned safe RL in multi-agent scenarios, enablingefficient coordination among agents.  Extensive benchmarking against state-of-the-art baselines demonstrates theeffectiveness of our method in achieving distance goals safely for multipleagents in complex and hazardous environments. Our code will be released tosupport future research.</description>
      <author>example@mail.com (Meng Feng, Viraj Parimi, Brian Williams)</author>
      <guid isPermaLink="false">2502.17813v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Design of a Breakaway Utensil Attachment for Enhanced Safety in Robot-Assisted Feeding</title>
      <link>http://arxiv.org/abs/2502.17774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的机械安全机制，以提高机器人辅助喂食系统的安全性，并减轻护理者的负担。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人辅助喂食系统主要依赖软件安全特性来减少意外碰撞时的风险。然而，这些方法在实际应用中可能不够可靠。&lt;h4&gt;目的&lt;/h4&gt;探索使用一种机械的紧急脱离装置，该装置能够在机器人施加过大力量时自动断开与用户的连接。&lt;h4&gt;方法&lt;/h4&gt;设计了一种可以分离力矩的手持餐具附件，并通过有限元分析（FEA）预测了不同加载条件下的失效点。之后利用3D打印技术制作带有各种槽深和壁环变化的样品，进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在特定参数下（例如1毫米的槽深度和三个壁环），该装置在承受65牛顿力时能够按预期失效。此外，这种设计可以根据个人舒适度等因素进行调整，定制化安全脱开力度。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了一种创新的方法来提高机器人辅助喂食系统的安全性，并为未来的研究提供了有用的参数和设计方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助进食系统通过增强身体运动障碍个体的独立性并减轻护理人员的压力而发挥作用。尽管现有的系统主要依赖于基于软件的安全特性以在未预见的碰撞时降低风险，但本研究探讨了使用机械安全机制来提高安全性的问题。设计了一种可以在机器人施加力过大时从用户那里分离的餐具附件，从而防止对用户的伤害。通过有限元分析预测了不同加载条件下的失败点，并利用带有槽深和壁环变化的不同3D打印样品进行了实验验证。为了便于测试，开发并验证了一个跌落试验装置。结果显示，在1毫米深度的槽口和三个壁环的情况下，当施力达到65牛顿时会以预期的方式失效。此外，可以根据用户的特定因素（如舒适度和个人承受能力）调整参数来定制化脱开力量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted feeding systems enhance the independence of individuals withmotor impairments and alleviate caregiver burden. While existing systemspredominantly rely on software-based safety features to mitigate risks duringunforeseen collisions, this study explores the use of a mechanical fail-safe toimprove safety. We designed a breakaway utensil attachment that decouplesforces exerted by the robot on the user when excessive forces occur. Finiteelement analysis (FEA) simulations were performed to predict failure pointsunder various loading conditions, followed by experimental validation using3D-printed attachments with variations in slot depth and wall loops. Tofacilitate testing, a drop test rig was developed and validated. Our resultsdemonstrated a consistent failure point at the slot of the attachment, with aslot depth of 1 mm and three wall loops achieving failure at the target forceof 65 N. Additionally, the parameters can be tailored to customize thebreakaway force based on user-specific factors, such as comfort and paintolerance. CAD files and utensil assembly instructions can be found here:https://tinyurl.com/rfa-utensil-attachment</description>
      <author>example@mail.com (Hau Wen Chang, J-Anne Yow, Lek Syn Lim, Wei Tech Ang)</author>
      <guid isPermaLink="false">2502.17774v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Toward 6-DOF Autonomous Underwater Vehicle Energy-Aware Position Control based on Deep Reinforcement Learning: Preliminary Results</title>
      <link>http://arxiv.org/abs/2502.17742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures, submitted to 2024 IEEE OES AUV Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度强化学习（DRL）的新颖方法，用于控制六自由度的自主水下航行器。使用截断分位数评论家(TQC)算法直接将命令发送给推进器，并且不需要关于推进器配置的知识。&lt;h4&gt;背景&lt;/h4&gt;在进行深海勘探时，自主水下航行器(AUVs)的操纵性和能源效率是关键因素，使得六自由度平台成为必备工具。PID和模型预测控制控制器尽管广泛使用，但它们需要准确的系统知识，并且面对负载或配置变化时难以重复。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型基于深度强化学习的方法来增强自主水下航行器在六个自由度上的操作性能，同时减少能量消耗。&lt;h4&gt;方法&lt;/h4&gt;利用截断分位数评论家(TQC)算法作为控制工具，设计了一种无需手动调整的DRL控制系统，该系统可以直接将命令发送给推进器，并且考虑了功率消耗因素。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，TQC高性能方法在达到目标点时的表现优于精细调优后的PID控制器。而能量感知型TQC方法虽然性能稍低，但平均节省了30%的能源。&lt;h4&gt;结论&lt;/h4&gt;TQC算法适用于六自由度AUVs控制，并且具有节能的优势，未来可能成为自主水下航行器控制领域的重要技术之一。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of autonomous underwater vehicles (AUVs) for surveying, mapping, andinspecting unexplored underwater areas plays a crucial role, wheremaneuverability and power efficiency are key factors for extending the use ofthese platforms, making six degrees of freedom (6-DOF) holonomic platformsessential tools. Although Proportional-Integral-Derivative (PID) and ModelPredictive Control controllers are widely used in these applications, theyoften require accurate system knowledge, struggle with repeatability whenfacing payload or configuration changes, and can be time-consuming tofine-tune. While more advanced methods based on Deep Reinforcement Learning(DRL) have been proposed, they are typically limited to operating in fewerdegrees of freedom. This paper proposes a novel DRL-based approach forcontrolling holonomic 6-DOF AUVs using the Truncated Quantile Critics (TQC)algorithm, which does not require manual tuning and directly feeds commands tothe thrusters without prior knowledge of their configuration. Furthermore, itincorporates power consumption directly into the reward function. Simulationresults show that the TQC High-Performance method achieves better performanceto a fine-tuned PID controller when reaching a goal point, while the TQCEnergy-Aware method demonstrates slightly lower performance but consumes 30%less power on average.</description>
      <author>example@mail.com (Gustavo Boré, Vicente Sufán, Sebastián Rodríguez-Martínez, Giancarlo Troni)</author>
      <guid isPermaLink="false">2502.17742v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>IBURD: Image Blending for Underwater Robotic Detection</title>
      <link>http://arxiv.org/abs/2502.17706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种图像融合管道IBURD，用于生成逼真的合成图像，以辅助训练深度检测器在水下自主车辆（AUV）上进行海洋垃圾检测任务。&lt;h4&gt;背景&lt;/h4&gt;现有的数据集不足以满足复杂水下环境中的深度学习视觉算法的需求，特别是在数据稀缺和多样性方面存在明显问题。&lt;h4&gt;目的&lt;/h4&gt;通过生成具有实际海底背景的海洋垃圾图像来解决现有数据集的问题，并为使用AUV执行环保清理任务提供技术支持。&lt;h4&gt;方法&lt;/h4&gt;{'IBURD技术': '利用源图（包含目标物体）及其标注，以及目标环境背景图像作为输入；通过泊松编辑和风格迁移等技术将透明物体制作到任意背景中并自动调整合成图像的样式。', '生成方式': '能够基于源图和目标背景创建垃圾对象图像及像素级别的注释。', '图像质量改善': '使用目标背景图像的模糊度指标自动调整合成图像的风格，使输出更加真实且适配于具体场景。'}&lt;h4&gt;主要发现&lt;/h4&gt;IBURD在机器人检测海洋垃圾任务中的表现得到了验证，并展示了其有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够生成高质量的真实海底环境下的海洋垃圾图像，从而促进AUV在环保清理任务中应用的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于创建逼真合成图像以帮助训练水下自主车辆（AUV）上深度检测器来完成海洋垃圾检测任务的图像融合管道IBURD。具体来说，IBURD能够生成海底垃圾图像及其像素级标注，并使用源图中的物体、其注释以及目标背景环境图作为输入。利用泊松编辑和风格迁移等技术，IBURD甚至可以将透明物体制作到任意背景下并自动调整合成图片的样式以适应背景模糊度指标的变化。这些包含实际海底背景的真实海洋垃圾图像解决了深度学习视觉算法在挑战性水下条件下的数据稀缺与多样性问题，并促进了AUV用于环境清理任务的应用。通过定量和机器人评估证明了该方法在机器人检测海洋垃圾方面具有有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an image blending pipeline, \textit{IBURD}, that creates realisticsynthetic images to assist in the training of deep detectors for use onunderwater autonomous vehicles (AUVs) for marine debris detection tasks.Specifically, IBURD generates both images of underwater debris and theirpixel-level annotations, using source images of debris objects, theirannotations, and target background images of marine environments. With Poissonediting and style transfer techniques, IBURD is even able to robustly blendtransparent objects into arbitrary backgrounds and automatically adjust thestyle of blended images using the blurriness metric of target backgroundimages. These generated images of marine debris in actual underwaterbackgrounds address the data scarcity and data variety problems faced bydeep-learned vision algorithms in challenging underwater conditions, and canenable the use of AUVs for environmental cleanup missions. Both quantitativeand robotic evaluations of IBURD demonstrate the efficacy of the proposedapproach for robotic detection of marine debris.</description>
      <author>example@mail.com (Jungseok Hong, Sakshi Singh, Junaed Sattar)</author>
      <guid isPermaLink="false">2502.17706v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>The Geometry of Optimal Gait Families for Steering Kinematic Locomoting Systems</title>
      <link>http://arxiv.org/abs/2502.17672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, submitted to IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了基于蛇形机器人等复杂系统的运动规划，提出了一种生成连续优化步伐族的方法，并展示了如何利用全局和局部搜索策略构建这些最优步伐族。&lt;h4&gt;背景&lt;/h4&gt;对于类似于蛇机器人的系统来说，将高层次刚体任务转换为低层次的关节轨迹是一个更具挑战性的过程。这个映射依赖于当前配置并受到关节限制的约束。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提高复杂运动系统的控制性和机动性。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一种结合全局和局部搜索策略的方法来生成连续优化步伐族，其中局部搜索提供更高的精度但可能在非平滑区域不稳定，而全局搜索则对非平滑行为具有鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了最优步伐家族的底层几何结构，并展示了这种方法对于粘性和理想流体三连杆游泳器的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为低层次关节控制器与高层次运动规划器在复杂运动系统中的集成奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;运动计划通常需要将高层次刚性体任务转换成低层次的关节轨迹。这个过程对于具有固定、不受限制驱动输入的小车式机器人来说是直接明了的，但对于蛇形机器人等系统而言则更具挑战性。因为在这个情况下，映射取决于当前配置并受到关节限制的影响。在这篇论文中，我们关注于生成连续家族的最佳步伐集——用步伐大小或转向速率参数化的最佳步伐集合，以增强控制性和机动性。我们揭示了这些最佳步伐家族的底层几何结构，并提出了使用全局和局部搜索策略构建它们的方法，在这种方法中局部方法与全球方法相互补充。全局搜索方式对非平滑行为具有鲁棒性，尽管这导致了解决方案顺序降低，而局部搜索则提供了更高的精度但可能在非平滑区域不稳定。为了证明我们的框架的有效性，我们为粘性和理想流体三连杆游泳器生成了最佳的步伐家族。这项工作为基础运动规划器和低级关节控制器之间的集成奠定了基础，在复杂的运动系统中使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning for locomotion systems typically requires translatinghigh-level rigid-body tasks into low-level joint trajectories-a process that isstraightforward for car-like robots with fixed, unbounded actuation inputs butmore challenging for systems like snake robots, where the mapping depends onthe current configuration and is constrained by joint limits. In this paper, wefocus on generating continuous families of optimal gaits-collections of gaitsparameterized by step size or steering rate-to enhance controllability andmaneuverability. We uncover the underlying geometric structure of these optimalgait families and propose methods for constructing them using both global andlocal search strategies, where the local method and the global methodcompensate each other. The global search approach is robust to nonsmoothbehavior, albeit yielding reduced-order solutions, while the local searchprovides higher accuracy but can be unstable near nonsmooth regions. Todemonstrate our framework, we generate optimal gait families for viscous andperfect-fluid three-link swimmers. This work lays a foundation for integratinglow-level joint controllers with higher-level motion planners in complexlocomotion systems.</description>
      <author>example@mail.com (Jinwoo Choi, Siming Deng, Nathan Justus, Noah J. Cowan, Ross L. Hatton)</author>
      <guid isPermaLink="false">2502.17672v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Building reliable sim driving agents by scaling self-play</title>
      <link>http://arxiv.org/abs/2502.14706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于大规模自我游戏训练的方法，旨在提高模拟代理的可靠性，并应用于Waymo开放运动数据集中的数千个场景。&lt;h4&gt;背景&lt;/h4&gt;设计和测试与人类交互的系统（如自动驾驶汽车）时需要可靠的仿真代理。这些代理在评估自动驾驶性能、压力测试等方面都有应用，但所有用例都需要高度可靠的表现，以确保分析的有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决大规模训练集并在未见过的场景中有效推广的方法，并展示其对分布外场景的部分鲁棒性以及通过微调快速达到近乎完美表现的能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于自我游戏的大规模训练策略，利用Waymo开放运动数据集中的数千个半现实主义限制造成的场景进行训练。所有训练均在单GPU上从零开始完成，并且能够在一天内几乎解决整个训练集。&lt;h4&gt;主要发现&lt;/h4&gt;经过训练的代理能够达到99.8%的目标完成率，在10,000个未见过的场景中，碰撞和脱轨事件的发生率低于0.8%，展示了有效的推广能力。此外，这些代理对分布外场景表现出部分鲁棒性，并且可以通过微调在几分钟内实现近乎完美的性能。&lt;h4&gt;结论&lt;/h4&gt;通过开放源代码库提供了预训练的代理以及完整的代码基础，以便于研究者进一步探索和改进仿真代理技术。&lt;h4&gt;翻译&lt;/h4&gt;该论文探讨了如何通过大规模自我游戏来提升自动驾驶车辆等与人类交互系统的模拟代理的可靠性。通过对Waymo Open Motion Dataset进行半现实限制造成的大规模场景训练，在单GPU上从零开始训练的代理能够实现高可靠性和有效推广，同时展示出对分布外情况的部分鲁棒性，并可以通过快速微调达到近乎完美的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulation agents are essential for designing and testing systems thatinteract with humans, such as autonomous vehicles (AVs). These agents servevarious purposes, from benchmarking AV performance to stress-testing thesystem's limits, but all use cases share a key requirement: reliability. Asimulation agent should behave as intended by the designer, minimizingunintended actions like collisions that can compromise the signal-to-noiseratio of analyses. As a foundation for reliable sim agents, we propose scalingself-play to thousands of scenarios on the Waymo Open Motion Dataset undersemi-realistic limits on human perception and control. Training from scratch ona single GPU, our agents nearly solve the full training set within a day. Theygeneralize effectively to unseen test scenes, achieving a 99.8% goal completionrate with less than 0.8% combined collision and off-road incidents across10,000 held-out scenarios. Beyond in-distribution generalization, our agentsshow partial robustness to out-of-distribution scenes and can be fine-tuned inminutes to reach near-perfect performance in those cases. Demonstrations ofagent behaviors can be found at this link. We open-source both the pre-trainedagents and the complete code base. Demonstrations of agent behaviors can befound at \url{https://sites.google.com/view/reliable-sim-agents}.</description>
      <author>example@mail.com (Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suárez, Eugene Vinitsky)</author>
      <guid isPermaLink="false">2502.14706v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
  <item>
      <title>Enhancing CoMP-RSMA Performance with Movable Antennas: A Meta-Learning Optimization Framework</title>
      <link>http://arxiv.org/abs/2502.17389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本研究探讨了一种下行链路速率分割多址接入（RSMA）场景，在该场景中，多个基站采用协同多点（CoMP）传输方案为配备移动天线（MA）技术的用户提供服务。与传统的固定位置天线（FPA）相比，后者受无线信道随机变化的影响，MAs可以战略性地重新定位到信道条件更优的位置，从而实现增强的空间分集增益。&lt;h4&gt;背景&lt;/h4&gt;在传统FPA受限于无线信道随机性的情况下，移动天线技术通过优化位置来改善无线通信性能。这种改进带来了更高的空间多样性收益，并且可以通过调整基站的发射波束成形矢量、不同用户的公共流分配以及MA的最佳定位进一步提高总的可达和速率。&lt;h4&gt;目的&lt;/h4&gt;为了最大化可达到的总速率并确保符合服务质量（QoS）约束，研究提出一个优化问题以确定BS的最佳传输波束形成矢量、各用户之间的共同流分配以及移动天线的最优位置。但该问题由于变量间的强依赖关系而复杂且计算上具有挑战性。&lt;h4&gt;方法&lt;/h4&gt;为了解决大规模优化任务中的计算难题，提出了一种无需预训练的基于梯度的元学习（GML）算法，这种方法特别适合于处理大型优化任务，并通过数值结果证明了其有效性和准确性。该方法能够实现接近最优的结果（与最佳解决方案相比超过97%）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，移动天线增强型CoMP-RSMA模型在性能上显著优于传统基准方案，在空间分割多址接入(SDMA)方案和基于固定位置天线的RSMA模型上分别实现了高达190%和80%的性能提升。此外，该方法能够减轻SDMA中总速率受限于干扰的问题，并通过较少的基站实现更优表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的GML算法在优化移动天线增强型CoMP-RSMA场景中的总体可达速率方面取得了显著成效，特别是在处理大规模复杂优化问题时展示了其优越性。该研究为未来的无线通信系统设计提供了一种有效的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates a downlink rate-splitting multiple access (RSMA)scenario in which multiple base stations (BSs), employing a coordinatedmulti-point (CoMP) transmission scheme, serve users equipped with movableantenna (MA) technology. Unlike traditional fixed-position antennas (FPAs),which are subject to random variations in wireless channels, MAs can bestrategically repositioned to locations with more favorable channel conditions,thereby achieving enhanced spatial diversity gains.To leverage these advantagesand maximize the achievable sum rate, we formulate an optimization problem thatjointly determines the optimal transmit beamforming vectors at the BSs, thecommon stream allocation for different users, and the optimal positioning ofthe MAs, all while ensuring compliance with quality of service (QoS)constraints. However, the formulated problem is non-convex and computationallychallenging due to the strong interdependence among the optimization variables.Traditional methods for solving large-scale optimization problems typicallyincur prohibitively high computational complexity. To address the abovechallenge, we propose a gradient-based meta-learning (GML) algorithm thatoperates without pre-training and is well-suited for handling large-scaleoptimization tasks. Numerical results demonstrate the effectiveness andaccuracy of the proposed approach, achieving near-optimal performance(exceeding 97% compared to the optimal solution). Moreover, the MA-enabledCoMP-RSMA model significantly outperforms conventional benchmark schemes,yielding performance gains of up to 190% over the spatial division multipleaccess (SDMA) scheme and 80% over the RSMA FPA-based model. Finally, theproposed approach is shown to mitigate the sum-rate limitations imposed byinterference in SDMA, achieving superior performance with fewer BSs.</description>
      <author>example@mail.com (Ali Amhaz, Shreya Khisa, Mohamed Elhattab, Chadi Assi, Sanaa Sharafeddine)</author>
      <guid isPermaLink="false">2502.17389v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种使用时空图神经网络（STGNN）对温室微气候进行建模的新方法，与传统的递归神经网络（RNN）相比，该方法在考虑环境变量之间的空间依赖关系及其方向性方面具有优势。&lt;h4&gt;背景&lt;/h4&gt;将光伏系统集成到温室中可以优化土地利用并促进可持续农业实践，同时提供食品生产和可再生能源发电的双重效益。然而，准确预测内部环境条件对于确保作物生长最佳和最大化能源生产至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过引入STGNN方法来改善温室微气候建模，提高对环境变量间空间依赖关系的理解，并在传统RNN模型的基础上进一步优化性能。&lt;h4&gt;方法&lt;/h4&gt;论文使用从希腊沃洛斯的一个温室每15分钟收集的高频数据进行实验。这些数据用于评估STGNN和RNN在不同季节条件下的表现差异。&lt;h4&gt;主要发现&lt;/h4&gt;RNN模型在冬季条件下表现出卓越的准确性（R^2 = 0.985），但在夏季冷却系统运行期间显示出局限性；相比之下，尽管目前STGNN的表现略低（冬季R^2 = 0.947），但其架构为整合诸如光伏发电和作物生长指标等额外变量提供了更大的潜力。&lt;h4&gt;结论&lt;/h4&gt;虽然现有的STGNN模型在性能上不如传统RNN，在温室微气候建模方面表现出一定的限制，但是考虑到它们在未来应用中的潜在优势，研究认为进一步探索STGNN的应用是值得的。&lt;h4&gt;翻译&lt;/h4&gt;该摘要描述了将时空图神经网络应用于温室内部环境条件预测的研究成果。论文通过对比分析不同模型在特定时间段内的表现，强调了STGNN的独特优势和未来可能的发展方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of photovoltaic (PV) systems into greenhouses not onlyoptimizes land use but also enhances sustainable agricultural practices byenabling dual benefits of food production and renewable energy generation.However, accurate prediction of internal environmental conditions is crucial toensure optimal crop growth while maximizing energy production. This studyintroduces a novel application of Spatio-Temporal Graph Neural Networks(STGNNs) to greenhouse microclimate modeling, comparing their performance withtraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporalpattern recognition, they cannot explicitly model the directional relationshipsbetween environmental variables. Our STGNN approach addresses this limitationby representing these relationships as directed graphs, enabling the model tocapture both spatial dependencies and their directionality. Usinghigh-frequency data collected at 15-minute intervals from a greenhouse inVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winterconditions (R^2 = 0.985) but show limitations during summer cooling systemoperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),their architecture offers greater potential for integrating additionalvariables such as PV generation and crop growth indicators.</description>
      <author>example@mail.com (Emiliano Seri, Marcello Petitta, Cristina Cornaro)</author>
      <guid isPermaLink="false">2502.17371v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
      <link>http://arxiv.org/abs/2502.17349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为HybridLinker的框架，旨在解决药物设计中分子片段组合生成的有效性和多样性之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;在药物发现应用如先导优化和PROTAC设计过程中，链接子生成是关键步骤。现有方法主要分为PC-Free（不使用3D点云）和PC-Aware（依赖于3D点云约束）两类。前者注重多样性但有效性较低；后者确保高有效性但限制了多样性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外训练就能提高有效性和多样性的新框架HybridLinker。&lt;h4&gt;方法&lt;/h4&gt;通过将预训练的PC-Free模型提供的多样化键合拓扑结构作为指导，增强了PC-Aware模型的推理能力。核心是提出了首个跨PC-Free和PC-Aware空间的操作的方法——LinkerDPS（链接子扩散后验采样），利用能量启发式函数连接分子拓扑与3D点云。&lt;h4&gt;主要发现&lt;/h4&gt;HybridLinker框架通过将PC-Free模型中多样化的采样分布转换为PC-Aware模型中的分布，显著且一致地提高了基础分子设计和应用属性优化任务的有效性和多样性。&lt;h4&gt;结论&lt;/h4&gt;本文建立了一种新的扩散后验采样（DPS）框架，在分子和图域内超越了成像领域，具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;链接子生成在药物发现中的先导优化和PROTAC设计等应用中至关重要。现有的方法根据是否使用3D点云划分为PC-Free和PC-Aware两类。前者追求多样性但有效性较低；后者确保高有效性但限制了多样性。为解决此权衡问题，我们提出了HybridLinker框架，通过引入预训练的PC-Free模型提供的多样化键合拓扑结构来增强PC-Aware模型的推理能力。我们的核心贡献是提出了一种新的扩散后验采样方法LinkerDPS，在分子和图域内建立了有效的连接，显著提高了有效性和多样性，开创了新的研究领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linker generation is critical in drug discovery applications such as leadoptimization and PROTAC design, where molecular fragments are assembled intodiverse drug candidates. Existing methods fall into PC-Free and PC-Awarecategories based on their use of 3D point clouds (PC). PC-Free modelsprioritize diversity but suffer from lower validity due to overlooking PCconstraints, while PC-Aware models ensure higher validity but restrictdiversity by enforcing strict PC constraints. To overcome these trade-offswithout additional training, we propose HybridLinker, a framework that enhancesPC-Aware inference by providing diverse bonding topologies from a pretrainedPC-Free model as guidance. At its core, we propose LinkerDPS, the firstdiffusion posterior sampling (DPS) method operating across PC-Free and PC-Awarespaces, bridging molecular topology with 3D point clouds via an energy-inspiredfunction. By transferring the diverse sampling distribution of PC-Free modelsinto the PC-Aware distribution, HybridLinker significantly and consistentlysurpasses baselines, improving both validity and diversity in foundationalmolecular design and applied property optimization tasks, establishing a newDPS framework in the molecular and graph domains beyond imaging.</description>
      <author>example@mail.com (Minyeong Hwang, Ziseok Lee, Gwangsoo Kim, Kyungsu Kim, Eunho Yang)</author>
      <guid isPermaLink="false">2502.17349v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</title>
      <link>http://arxiv.org/abs/2502.17039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的车辆到基础设施（V2I）协同感知框架LCV2I，该框架使用低成本低分辨率激光雷达和摄像头数据来提高协作感知的性能。&lt;h4&gt;背景&lt;/h4&gt;当前V2I合作感知系统主要依赖于高成本的高分辨率激光雷达，但这种传感器价格昂贵且难以普及。同时，传统通信方法带宽利用率较低。&lt;h4&gt;目的&lt;/h4&gt;为了实现低成本的V2I协同感知，研究旨在降低车辆端使用高分辨率激光雷达的成本，并提高数据传输效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架LCV2I，该框架采用低分辨率激光雷达和摄像头的数据作为输入，并利用特征偏移校正模块和区域特征增强算法来改进特征表示。此外，通过区域差异图和区域评分图评估协作内容的价值，从而提高通信带宽效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的LCV2I方法在保持高水平感知性能的同时，显著减少了对车辆端高分辨率传感器的需求，并且在真实世界场景中的3D目标检测测试中超越了现有算法的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种高效的低成本V2I协同感知框架，能够通过低分辨率激光雷达和摄像头的数据实现高质量的感知结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-Infrastructure (V2I) collaborative perception leverages datacollected by infrastructure's sensors to enhance vehicle perceptualcapabilities. LiDAR, as a commonly used sensor in cooperative perception, iswidely equipped in intelligent vehicles and infrastructure. However, itssuperior performance comes with a correspondingly high cost. To achievelow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we studyadopting low-resolution LiDAR on the vehicle to minimize cost as much aspossible. However, simply reducing the resolution of vehicle's LiDAR results insparse point clouds, making distant small objects even more blurred.Additionally, traditional communication methods have relatively low bandwidthutilization efficiency. These factors pose challenges for us. To balance costand perceptual accuracy, we propose a new collaborative perception framework,namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDARas input. It also employs feature offset correction modules and regionalfeature enhancement algorithms to improve feature representation. Finally, weuse regional difference map and regional score map to assess the value ofcollaboration content, thereby improving communication bandwidth efficiency. Insummary, our approach achieves high perceptual performance while substantiallyreducing the demand for high-resolution sensors on the vehicle. To evaluatethis algorithm, we conduct 3D object detection in the real-world scenario ofDAIR-V2X, demonstrating that the performance of LCV2I consistently surpassescurrently existing algorithms.</description>
      <author>example@mail.com (Xinxin Feng, Haoran Sun, Haifeng Zheng, Huacong Chen, Wenqiang Chen)</author>
      <guid isPermaLink="false">2502.17039v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Supervised contrastive learning from weakly-labeled audio segments for musical version matching</title>
      <link>http://arxiv.org/abs/2502.16936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 7 tables; includes Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;检测音乐版本是一项具有挑战性的任务，现有方法通常在曲目级别上匹配音乐版本，而实际应用中需要在片段级别进行匹配。&lt;h4&gt;背景描述&lt;/h4&gt;现有的音乐版本检测技术大多基于整个音频文件的完全标注，并使用分类和三元组损失等传统方法，忽略了更现代的损失函数可能带来的改进。&lt;h4&gt;研究目的&lt;/h4&gt;开发一种可以在弱监督学习条件下工作的新方法，该方法利用对比损失变体在片段级别上提高性能。&lt;h4&gt;主要方法&lt;/h4&gt;{'弱标记段学习': '基于成对的音乐片段距离减少进行训练', '对比损失修改': '通过解耦、超参数和几何学考虑改进现有损失函数'}&lt;h4&gt;关键发现&lt;/h4&gt;提出的方法不仅在标准曲目级评估中达到了最先进的性能，在片段级别上也实现了突破性的效果。&lt;h4&gt;结论&lt;/h4&gt;由于所解决问题的通用性，该方法可能超越音频或音乐版本匹配领域，在其他领域找到应用价值。&lt;h4&gt;翻译&lt;/h4&gt;检测音乐版本是一项具有挑战性的任务，并且具有重要的实际应用场景。现有的方法通常基于完全标注数据进行曲目级别的匹配（例如整首歌曲）。然而大多数实际应用场景需要在片段级别上进行匹配（例如20秒的音频段落）。此外，现有研究大多依赖于分类和三元组损失函数，而忽视了更现代的损失函数可能带来的性能提升。本文中我们提出了一种基于弱监督学习的新型方法以及一种改进的对比损失变体，在片段级别的评估上达到了前所未有的性能水平，并且在传统的曲目级别评估上也取得了领先的结果。我们认为由于所解决问题的普遍性，该方法有望在音频或音乐版本匹配之外的其他领域找到应用机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting musical versions (different renditions of the same piece) is achallenging task with important applications. Because of the ground truthnature, existing approaches match musical versions at the track level (e.g.,whole song). However, most applications require to match them at the segmentlevel (e.g., 20s chunks). In addition, existing approaches resort toclassification and triplet losses, disregarding more recent losses that couldbring meaningful improvements. In this paper, we propose a method to learn fromweakly annotated segments, together with a contrastive loss variant thatoutperforms well-studied alternatives. The former is based on pairwise segmentdistance reductions, while the latter modifies an existing loss followingdecoupling, hyper-parameter, and geometric considerations. With these twoelements, we do not only achieve state-of-the-art results in the standardtrack-level evaluation, but we also obtain a breakthrough performance in asegment-level evaluation. We believe that, due to the generality of thechallenges addressed here, the proposed methods may find utility in domainsbeyond audio or musical version matching.</description>
      <author>example@mail.com (Joan Serrà, R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2502.16936v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models are Powerful EHR Encoders</title>
      <link>http://arxiv.org/abs/2502.17403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了使用通用大型语言模型（LLM）嵌入方法作为电子健康记录（EHR）编码器的潜力，特别是在临床预测任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录数据复杂且异质性高，传统机器学习方法难以有效利用这些资源。领域特定的EHR基础模型虽然在提高预测准确性方面表现出色，但其训练受到高质量多样化数据集有限和编码标准不一致的影响。&lt;h4&gt;目的&lt;/h4&gt;评估通用LLM嵌入方法作为EHR编码器的有效性和潜在优势。&lt;h4&gt;方法&lt;/h4&gt;通过将患者记录转换为结构化的Markdown文本并利用预训练的大型语言模型（GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct）进行代码转译，研究者在EHRSHOT基准测试的15个不同临床预测任务上比较了这些方法与特定于EHR的基础模型CLIMBR-T-Base及传统机器学习基线的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在少量样本的情况下，LLM基于嵌入的方法经常能够达到甚至超过专门模型的性能，并且其有效性随着基础LLM规模和上下文窗口大小的增长而提高。&lt;h4&gt;结论&lt;/h4&gt;重新利用LLM作为EHR编码器提供了一种可扩展且有效的临床预测方法，有助于克服传统EHR建模中的局限性并促进更互操作性和普遍性的医疗保健应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) offer rich potential for clinicalprediction, yet their inherent complexity and heterogeneity pose significantchallenges for traditional machine learning approaches. Domain-specific EHRfoundation models trained on large collections of unlabeled EHR data havedemonstrated promising improvements in predictive accuracy and generalization;however, their training is constrained by limited access to diverse,high-quality datasets and inconsistencies in coding standards and healthcarepractices. In this study, we explore the possibility of using general-purposeLarge Language Models (LLMs) based embedding methods as EHR encoders. Byserializing patient records into structured Markdown text, transforming codesinto human-readable descriptors, we leverage the extensive generalizationcapabilities of LLMs pretrained on vast public corpora, thereby bypassing theneed for proprietary medical datasets. We systematically evaluate twostate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct andLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks fromthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundationmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our resultsdemonstrate that LLM-based embeddings frequently match or exceed theperformance of specialized models, even in few-shot settings, and that theireffectiveness scales with the size of the underlying LLM and the availablecontext window. Overall, our findings demonstrate that repurposing LLMs for EHRencoding offers a scalable and effective approach for clinical prediction,capable of overcoming the limitations of traditional EHR modeling andfacilitating more interoperable and generalizable healthcare applications.</description>
      <author>example@mail.com (Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild)</author>
      <guid isPermaLink="false">2502.17403v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning</title>
      <link>http://arxiv.org/abs/2502.17226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2411.10619&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本论文提出了一个新型的个性化联邦学习(PFL) 方法，用于电表网络中的高质量负载预测。&lt;h4&gt;背景&lt;/h4&gt;电力负载预测对于智能电网的管理与稳定性至关重要。传统机器学习方法在负载预测中通常被使用，但会涉及到数据交换从而引发隐私问题。联邦学习（FL）可以通过不进行数据交换而在本地智能电表上运行分布式机器学习模型来解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型个性化联邦学习(PFL) 方法以实现高效的负载预测，并减少延迟。&lt;h4&gt;方法&lt;/h4&gt;提出了基于元学习的策略，用于处理本地智能电表中的数据异质性。同时研究了一种新的基于最优资源分配的新延迟优化问题来降低PFL模型中负载预测延迟。&lt;h4&gt;主要发现&lt;/h4&gt;通过详尽的真实世界数据集仿真表明本论文的方法在负载预测和运营延迟成本方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法为联邦学习的设计提供了理论收敛性分析，以提供关于联合负荷预测的见解。&lt;h4&gt;翻译&lt;/h4&gt;电力负载预测对于智能电网管理和稳定性至关重要。通常通过高级计量基础设施实现这一点，在这种基础设施中，智能电表记录家庭能耗数据。虽然传统机器学习方法被广泛用于负荷预测，但它们需要数据共享并且引发了隐私问题。联邦学习可以通过在本地智能电表上运行分布式模型而无需交换数据来解决这个问题。然而，当前基于FL的方法由于异构智能电表之间的数据分布不平衡而难以实现有效的负载预测。本文提出了一种新的个性化联邦学习（PFL）方法用于计量网络中的高质量负荷预测。研究团队开发了一个基于元学习的策略来处理在本地智能电表中联合训练本地负荷预测模型时的数据异质性问题。此外，为了最小化我们提出的PFL模型中的负载预测延迟，他们研究了一种新的基于最优资源分配的延迟优化问题。还进行了理论收敛性分析以提供关于联邦学习设计用于联邦负荷预测的见解。大量来自真实数据集的仿真显示该方法在提高负荷预测质量和减少运营延迟成本方面优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electric load forecasting is essential for power management and stability insmart grids. This is mainly achieved via advanced metering infrastructure,where smart meters (SMs) record household energy data. Traditional machinelearning (ML) methods are often employed for load forecasting but require datasharing which raises data privacy concerns. Federated learning (FL) can addressthis issue by running distributed ML models at local SMs without data exchange.However, current FL-based approaches struggle to achieve efficient loadforecasting due to imbalanced data distribution across heterogeneous SMs. Thispaper presents a novel personalized federated learning (PFL) method forhigh-quality load forecasting in metering networks. A meta-learning-basedstrategy is developed to address data heterogeneity at local SMs in thecollaborative training of local load forecasting models. Moreover, to minimizethe load forecasting delays in our PFL model, we study a new latencyoptimization problem based on optimal resource allocation at SMs. A theoreticalconvergence analysis is also conducted to provide insights into FL design forfederated load forecasting. Extensive simulations from real-world datasets showthat our method outperforms existing approaches in terms of better loadforecasting and reduced operational latency costs.</description>
      <author>example@mail.com (Ratun Rahman, Pablo Moriano, Samee U. Khan, Dinh C. Nguyen)</author>
      <guid isPermaLink="false">2502.17226v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的自动驾驶感知技术在结构化的车辆主导环境中展示了卓越的能力，但在半结构化环境中存在显著限制。这些限制主要是由于高质量数据集缺乏造成的，尤其是在行人感知和预测方面。&lt;h4&gt;背景&lt;/h4&gt;当前的自动驾驶感知模型在半结构化环境（如动态行人频繁出现的地方）中表现出明显的局限性，因为现有的数据集中缺乏足够高质量的数据来支持这类场景的研究。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种新的多模态数据集——Pedestrian-Focused Scene Dataset (PFSD)，专门针对半结构化的复杂环境，并提出了Hybrid Multi-Scale Fusion Network（HMFN）模型以解决行人检测的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;{'PFSD': '该数据集包含超过130,000个行人的实例，涵盖了各种密度、移动模式和遮挡情况。它提供了全面的多模态数据注释，包括点云分割、检测以及对象ID追踪。', 'HMFN': '为了在密集且被部分阻挡的情况下更好地识别行人，该方法使用精心设计的混合框架捕获并融合多尺度特征，整合了稀疏和标准卷积技术。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在PFSD上进行测试时，所提出的HMFN模型相比现有方法在3D行人检测方面实现了更高的平均精度(mAP)提升。&lt;h4&gt;结论&lt;/h4&gt;通过提出新的数据集和有效的网络架构来解决半结构化环境中复杂的行人感知挑战问题，证明了该工作的实用性和创新性。&lt;h4&gt;翻译&lt;/h4&gt;近期自动驾驶车辆的感知技术在高度结构化的交通场景中展示出了卓越的能力。然而，在行人活动更为多样且复杂遮挡更加普遍的半结构化环境下，现有的感知模型表现出了明显的局限性。这种现象主要是由于缺乏高质量的数据集，特别是关于行人的感知和预测数据。本文提出了一种新的多模态行人聚焦场景数据集（PFSD），它在nuScenes格式下被详细标注，并提供了全面的多模态注释，包括点云分割、检测及对象ID追踪等信息。该数据集覆盖了超过130,000个行人的实例，它们涵盖了不同密度、移动模式和遮挡情况下的各种场景。为了应对半结构化环境中更加多样复杂的情况带来的挑战，我们提出了一种新的混合多尺度融合网络（HMFN）。具体而言，在人口密集且存在部分阻挡的情况下，我们的方法通过精心设计的框架有效地捕捉并融合了多种规模特征，该框架集成了稀疏和传统卷积技术。在PFSD上的大量实验表明，与现有方法相比，HMFN在网络架构中实现了显著提高的平均精度（mAP），这证明了其解决半结构化环境中3D行人检测挑战的有效性。代码和基准测试结果已经开放提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Applications of Large Models in Medicine</title>
      <link>http://arxiv.org/abs/2502.17132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了大规模模型在医疗领域的进展与应用，特别关注医学大型模型（MedLMs）的应用。&lt;h4&gt;背景&lt;/h4&gt;这些模型包括大型语言模型（LLMs）、视觉模型、3D大型模型和多模态模型。它们通过增强疾病预测、诊断辅助、个性化治疗计划及药物发现来革新医疗服务。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提供大规模模型在医学领域现状与未来方向的全面概述，强调其在全球健康进步中的重要性。&lt;h4&gt;方法&lt;/h4&gt;论文重点介绍了大型图神经网络如何融入医疗知识图谱和药物发现中，以及视觉-语言模型（VLMs）和3D大型模型在医学图像分析、解剖建模及假肢设计方面的应用。&lt;h4&gt;主要发现&lt;/h4&gt;尽管存在挑战，但这些技术正在为医疗服务设定新的标准，并为个性化健康解决方案铺平道路。&lt;h4&gt;结论&lt;/h4&gt;大规模模型正在医疗领域实现变革性的进步，通过改善诊断准确性来推动全球卫生的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文翻译为：本文探讨了大型规模模型在医学领域的进展和应用，特别关注医学大模型（MedLMs）。这些模型包括大型语言模型、视觉模型、3D大型模型以及多模态模型。它们正在通过增强疾病预测、诊断辅助、个性化治疗计划及药物发现等方面革新医疗服务。研究还强调了大型图模型（LGMs）在理解复杂生物医学关系中的潜力，特别是在医疗知识图谱和药物发现中的集成应用。视觉-语言模型（VLMs）和3D大型模型在医学图像分析、解剖建模以及假肢设计方面的使用也得到突出展示。尽管存在挑战，这些技术正在为医疗服务设定新的标准，提高诊断准确性，并推动个性化健康解决方案的发展。本文旨在提供大规模模型在医学领域现状与未来方向的全面概述，强调它们在全球健康进步中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.71423/aimed.20250105&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the advancements and applications of large-scale modelsin the medical field, with a particular focus on Medical Large Models (MedLMs).These models, encompassing Large Language Models (LLMs), Vision Models, 3DLarge Models, and Multimodal Models, are revolutionizing healthcare byenhancing disease prediction, diagnostic assistance, personalized treatmentplanning, and drug discovery. The integration of graph neural networks inmedical knowledge graphs and drug discovery highlights the potential of LargeGraph Models (LGMs) in understanding complex biomedical relationships. Thestudy also emphasizes the transformative role of Vision-Language Models (VLMs)and 3D Large Models in medical image analysis, anatomical modeling, andprosthetic design. Despite the challenges, these technologies are setting newbenchmarks in medical innovation, improving diagnostic accuracy, and paving theway for personalized healthcare solutions. This paper aims to provide acomprehensive overview of the current state and future directions of largemodels in medicine, underscoring their significance in advancing global health.</description>
      <author>example@mail.com (YunHe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu Yuxin Jia, Lujia Ge, Jing-min Yang)</author>
      <guid isPermaLink="false">2502.17132v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping</title>
      <link>http://arxiv.org/abs/2502.17249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合颜色信息的稳健框架，用于准确的LiDAR里程计和地图构建（LOAM），通过同时利用LiDAR点云和相机图像中的边缘及平面特征来提高定位精度。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR Odometry and Mapping (LOAM)技术在使用单模态数据时存在局限性，难以处理复杂的环境场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够融合颜色信息的稳健框架，以实现更加准确和鲁棒性的LiDAR里程计及地图构建方法。&lt;h4&gt;方法&lt;/h4&gt;该框架包括：1）利用相机图像中的颜色为LiDAR点云着色；2）采用感知均匀的颜色差异权重策略来排除颜色对应异常值；3）使用基于Welsch函数的稳健误差度量法处理位置对应异常值。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在复杂森林和校园等挑战性场景中表现出更高的准确性和鲁棒性，相较于当前最先进的技术有所改进。&lt;h4&gt;结论&lt;/h4&gt;利用相机图像中的颜色信息能够显著提高LiDAR里程计及地图构建的精度与稳定性。&lt;h4&gt;翻译&lt;/h4&gt;在这封信中，我们提出了一种结合颜色信息用于精确LiDAR里程估计和制图（LOAM）的稳健框架。同时从激光雷达和摄像机接收数据，该框架利用摄像机图像中的颜色信息对激光雷达点云进行着色，然后执行迭代姿态优化。对于每个激光雷达扫描，提取边缘和平面特征，并使用相应图像对其着色并匹配到全局地图中。特别地，我们采用感知均匀的颜色差异权重策略来排除颜色对应异常值，并基于Welsch函数的稳健误差度量法在姿态优化过程中减少位置对应异常值的影响。因此，该系统实现了精确定位，并重建了环境密集、准确、彩色且三维的地图。具有挑战性的场景（包括复杂森林和校园）中的彻底实验表明，我们的方法相比当前最先进的技术提供了更高的鲁棒性和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this letter, we propose a color-assisted robust framework for accurateLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both theLiDAR and the camera, the framework utilizes the color information from thecamera images to colorize the LiDAR point clouds and then performs iterativepose optimization. For each LiDAR scan, the edge and planar features areextracted and colored using the corresponding image and then matched to aglobal map. Specifically, we adopt a perceptually uniform color differenceweighting strategy to exclude color correspondence outliers and a robust errormetric based on the Welsch's function to mitigate the impact of positionalcorrespondence outliers during the pose optimization process. As a result, thesystem achieves accurate localization and reconstructs dense, accurate, coloredand three-dimensional (3D) maps of the environment. Thorough experiments withchallenging scenarios, including complex forests and a campus, show that ourmethod provides higher robustness and accuracy compared with currentstate-of-the-art methods.</description>
      <author>example@mail.com (Yufei Lu, Yuetao Li, Zhizhou Jia, Qun Hao, Shaohui Zhang)</author>
      <guid isPermaLink="false">2502.17249v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.16389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Robotics Research (IJRR)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶汽车的安全性是实现完全自主驾驶的关键，特别是在检测超出操作设计领域的异常情况方面。论文提出了一种新颖的无监督异常检测专家系统来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动化车辆进入公共道路，确保无数驾驶场景中的安全性成为广泛采用全自动驾驶的重要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;为了提高自动驾驶系统的可信度，研究提出了能够检测出道路上不常见情况的方法。&lt;h4&gt;方法&lt;/h4&gt;{'三类无监督异常检测专家': ['场景专家：专注于帧级别的外观来识别异常场景和未预期的场景运动；交互专家：建立两个道路参与者的相对正常移动模型，并在出现异常互动时发出警告；行为专家：通过未来轨迹预测监测个体对象的异常行为。'], '专家集成系统(Xen)': '利用卡尔曼滤波器将所有模块的优点结合起来，最终异常得分被作为其中一个状态，而观察结果则由各个专家生成。', '新颖评估协议': '采用了一种新的模型性能评估协议来测试实际应用中的表现'}&lt;h4&gt;主要发现&lt;/h4&gt;{'优越性': '实验结果显示该方法在检测道路上的异常情况时比先前的方法更胜一筹', '潜力': '通过无监督学习处理大规模数据集，该框架有分类不同类型的异常行为的潜力'}&lt;h4&gt;结论&lt;/h4&gt;提出了一种新颖且有效的方法来实现自动驾驶汽车中的安全性和可靠性，并展示了其在现实世界应用中的潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;随着自动化车辆进入公共道路，确保无数驾驶场景中的安全性成为广泛采用全自动驾驶的重要挑战之一。论文提出了三种无监督异常检测专家：场景专家、交互专家和行为专家，以及一个通过卡尔曼滤波器将各模块的优点结合起来的专家集成系统(Xen)。实验显示该方法在道路上检测异常情况方面优于先前的方法，并且具有利用大规模数据集进行无监督学习来分类不同类型的异常行为的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/02783649241297998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As automated vehicles enter public roads, safety in a near-infinite number ofdriving scenarios becomes one of the major concerns for the widespread adoptionof fully autonomous driving. The ability to detect anomalous situations outsideof the operational design domain is a key component in self-driving cars,enabling us to mitigate the impact of abnormal ego behaviors and to realizetrustworthy driving systems. On-road anomaly detection in egocentric videosremains a challenging problem due to the difficulties introduced by complex andinteractive scenarios. We conduct a holistic analysis of common on-road anomalypatterns, from which we propose three unsupervised anomaly detection experts: ascene expert that focuses on frame-level appearances to detect abnormal scenesand unexpected scene motions; an interaction expert that models normal relativemotions between two road participants and raises alarms whenever anomalousinteractions emerge; and a behavior expert which monitors abnormal behaviors ofindividual objects by future trajectory prediction. To combine the strengths ofall the modules, we propose an expert ensemble (Xen) using a Kalman filter, inwhich the final anomaly score is absorbed as one of the states and theobservations are generated by the experts. Our experiments employ a novelevaluation protocol for realistic model performance, demonstrate superioranomaly detection performance than previous methods, and show that ourframework has potential in classifying anomaly types using unsupervisedlearning on a large-scale on-road anomaly dataset.</description>
      <author>example@mail.com (Tianchen Ji, Neeloy Chakraborty, Andre Schreiber, Katherine Driggs-Campbell)</author>
      <guid isPermaLink="false">2502.16389v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Code Vulnerabilities with Heterogeneous GNN Training</title>
      <link>http://arxiv.org/abs/2502.16835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;检测源代码中的漏洞是软件安全保障的关键任务。图神经网络（GNN）机器学习通过将源代码建模为图形，可以成为一种有前途的方法。&lt;h4&gt;背景&lt;/h4&gt;早期方法将代码元素统一处理，限制了其模拟多样化关系的能力，这些关系有助于识别各种类型的漏洞。最近的研究通过考虑节点类型的不同性，并使用门控图神经网络（GGNN）来解决这一问题，以不同的边类型聚合节点信息。&lt;h4&gt;目的&lt;/h4&gt;介绍Inter-Procedural Abstract Graphs (IPAG)作为一种高效的、与语言无关的源代码表示方法，结合异构GNN训练进行漏洞预测。提出Heterogeneous Attention GNN（HAGNN）模型来集成捕捉源代码不同特征的多个子图。&lt;h4&gt;方法&lt;/h4&gt;该模型使用异构注意力机制将这些分别学习到的不同子图结合起来，并通过全连接神经网络进行最终分类。&lt;h4&gt;主要发现&lt;/h4&gt;提出的这种方法在包含108种漏洞类型的大型C数据集上达到了高达96.6%的准确性，在包含114种漏洞类型的大型Java数据集上达到了97.8%，优于现有最先进的方法。此外，该方法应用于各种实际软件项目时也显示出了较低的假阳性率。&lt;h4&gt;结论&lt;/h4&gt;通过引入Inter-Procedural Abstract Graphs（IPAG）和Heterogeneous Attention GNN（HAGNN），为源代码漏洞检测提供了一种高效且准确的方法，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;检测源代码中的漏洞是软件安全保障的关键任务。图神经网络（GNN）机器学习通过将源代码建模为图形，可以成为一种有前途的方法。早期方法统一处理代码元素，限制了其对导致各种类型漏洞的多样化关系进行建模的能力。最近的研究通过考虑节点类型的异质性，并使用门控图神经网络（GGNN）来解决这一问题，以不同的边类型聚合节点信息。然而，这些边缘主要作为传递节点信息的渠道，可能无法捕捉到不同类型的详细特征。本文提出了Inter-Procedural Abstract Graphs (IPAG)作为一种高效的、与语言无关的源代码表示方法，并结合异构GNN训练进行漏洞预测。IPAG捕获了代码元素及其关系的结构和上下文属性。我们还提出了一种Heterogeneous Attention GNN（HAGNN）模型，该模型集成了捕捉源代码不同特征的多个子图。这些子图分别学习并通过全局注意力机制结合在一起，并通过全连接神经网络进行最终分类。在大型C数据集中，提出的这种方法达到了高达96.6%的准确性，涵盖了108种漏洞类型；而在包含114种漏洞类型的大型Java数据集中，则达到了97.8%，优于现有最先进的方法。此外，在各种实际软件项目中的应用也显示出了较低的假阳性率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting vulnerabilities in source code is a critical task for softwaresecurity assurance. Graph Neural Network (GNN) machine learning can be apromising approach by modeling source code as graphs. Early approaches treatedcode elements uniformly, limiting their capacity to model diverse relationshipsthat contribute to various vulnerabilities. Recent research addresses thislimitation by considering the heterogeneity of node types and using Gated GraphNeural Networks (GGNN) to aggregate node information through different edgetypes. However, these edges primarily function as conduits for passing nodeinformation and may not capture detailed characteristics of distinct edgetypes. This paper presents Inter-Procedural Abstract Graphs (IPAGs) as anefficient, language-agnostic representation of source code, complemented byheterogeneous GNN training for vulnerability prediction. IPAGs capture thestructural and contextual properties of code elements and their relationships.We also propose a Heterogeneous Attention GNN (HAGNN) model that incorporatesmultiple subgraphs capturing different features of source code. These subgraphsare learned separately and combined using a global attention mechanism,followed by a fully connected neural network for final classification. Theproposed approach has achieved up to 96.6% accuracy on a large C dataset of 108vulnerability types and 97.8% on a large Java dataset of 114 vulnerabilitytypes, outperforming state-of-the-art methods. Its applications to variousreal-world software projects have also demonstrated low false positive rates.</description>
      <author>example@mail.com (Yu Luo, Weifeng Xu, Dianxiang Xu)</author>
      <guid isPermaLink="false">2502.16835v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Scatterplot and Image Moments for Time-Varying Bivariate Field Analysis of Electronic Structure Evolution</title>
      <link>http://arxiv.org/abs/2502.17118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间变化双变量场分析方法，用于理解光诱导动力学中电子结构的变化。&lt;h4&gt;背景&lt;/h4&gt;由于光照吸收引起的电子在能级间的跃迁是一个复杂的量子力学过程，这会影响分子内的核几何和电子结构。研究这些密度场有助于了解分子内供体区域与受体区域之间的电荷移动情况。&lt;h4&gt;目的&lt;/h4&gt;通过连续散点图（Continuous Scatterplots, CSP）及基于图像的时刻描述符来分析时间变化中的双变量字段，并针对光激发后的不断变化的电子结构，提出一种特征导向可视化探索的方法。&lt;h4&gt;方法&lt;/h4&gt;核运动产生的多个时间步长，使用CSP和基于图像的时刻描述符进行动态场数据的探索性分析。将每个时间步骤的CSP表示为四个长度的图矩向量，并形成一个R^4中的点云。&lt;h4&gt;主要发现&lt;/h4&gt;选取适当的主要成分可以将点云表示为平面上的一条曲线，从而有助于识别关键的时间步长、发现双变量字段内的模式以及追踪其随时间的变化。文中通过两个光激发分子动力学的案例研究展示了这种方法的应用。&lt;h4&gt;结论&lt;/h4&gt;此方法可有效揭示电子结构变化规律，并提供具有应用特定洞察力的方法来深入理解光诱导过程中的物理和化学机制。&lt;h4&gt;翻译&lt;/h4&gt;光致电子跃迁是由于光照吸收引起的复杂量子力学过程，其中电子在能级之间移动。这会引起电子结构和核几何的变化，推动了光生物学、材料设计以及医学等领域的重要物理和化学进程。不断演变的电子结构可以通过两个电子密度场来表征：空穴自然过渡轨道（NTO）和粒子自然过渡轨道（NTO）。研究这些密度领域有助于了解分子内供体区域与受体区域之间的电荷移动情况。以往的研究多依赖于等值面并排视觉比较、统计方法或双变量字段分析，实例较少。我们提出了一种新的时间变化双变量场分析方法，适用于理解大量实例下的光诱导电子结构变化。由于NTO领域取决于核几何，因此需通过许多时间步长来解析由核运动产生的复杂现象。本文采用连续散点图（Continuous Scatterplots, CSP）及基于图像的时刻描述符来进行动态场数据探索性分析，并针对光激发后的不断变化的电子结构，提出一种特征导向可视化探索的方法。每个时间步骤中的CSP通过四个长度的图矩向量来表示；将所有矢量描述符集合形成R^4空间里的点云并利用主成分分析技术进行可视化呈现。选择适当的主成分可以简化点云为平面上的一条曲线，有助于识别关键的时间步长、发现双变量字段内的模式以及追踪其随时间的变化。我们通过两个光激发分子动力学案例研究展示这种方法的有效性，并展示了双变量场分析在特定应用中提供的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoinduced electronic transitions are complex quantum-mechanical processeswhere electrons move between energy levels due to light absorption. Thisinduces dynamics in electronic structure and nuclear geometry, drivingimportant physical and chemical processes in fields like photobiology,materials design, and medicine. The evolving electronic structure can becharacterized by two electron density fields: hole and particle naturaltransition orbitals (NTOs). Studying these density fields helps understandelectronic charge movement between donor and acceptor regions within amolecule. Previous works rely on side-by-side visual comparisons ofisosurfaces, statistical approaches, or bivariate field analysis with fewinstances. We propose a new method to analyze time-varying bivariate fieldswith many instances, which is relevant for understanding electronic structurechanges during light-induced dynamics. Since NTO fields depend on nucleargeometry, the nuclear motion results in numerous time steps to analyze. Thispaper presents a structured approach to feature-directed visual exploration oftime-varying bivariate fields using continuous scatterplots (CSPs) and imagemoment-based descriptors, tailored for studying evolving electronic structurespost-photoexcitation. The CSP of the bivariate field at each time step isrepresented by a four-length image moment vector. The collection of all vectordescriptors forms a point cloud in R^4, visualized using principal componentanalysis. Selecting appropriate principal components results in arepresentation of the point cloud as a curve on the plane, aiding tasks such asidentifying key time steps, recognizing patterns within the bivariate field,and tracking the temporal evolution. We demonstrate this with two case studieson excited-state molecular dynamics, showing how bivariate field analysisprovides application-specific insights.</description>
      <author>example@mail.com (Mohit Sharma, Talha Bin Masood, Nanna Holmgaard List, Ingrid Hotz, Vijay Natarajan)</author>
      <guid isPermaLink="false">2502.17118v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Snoopy: Effective and Efficient Semantic Join Discovery via Proxy Columns</title>
      <link>http://arxiv.org/abs/2502.16813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TKDE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的列级语义连接发现框架Snoopy，通过使用代理列来计算列嵌入以解决现有方法在有效性和效率方面的问题。&lt;h4&gt;背景&lt;/h4&gt;语义连接发现旨在从表库中找到与查询列有高语义连接性的列。现有方法分为单元格级别和列级别两种方法，但两者都无法同时保证有效性和效率。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架Snoopy来解决当前方法中存在的有效性低、效率不足的问题。&lt;h4&gt;方法&lt;/h4&gt;通过使用代理列计算列嵌入，并引入了一个基于排名的对比学习范式来获取指导列投影的良好代理列，提出了一个轻量级近似图匹配基线的列投射以捕捉隐式的列到代理列关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Snoopy在Recall@25和NDCG@25上分别比现有最佳方法高出16%和10%，并且至少快五倍于单元级解决方案，在速度上是现有的列级方法的3.5倍。&lt;h4&gt;结论&lt;/h4&gt;提出的框架Snoopy不仅提高了语义连接发现的有效性，同时显著提升了效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：语义连接发现旨在从表库中找到与查询列有高语义连接性的列。现存的方法可以分为两种类型：单元格级别方法和列级别方法。然而，两者都无法同时保证有效性和效率。单元级方法通过计算列之间的单元匹配来计算连接性，具有理想的有效性但效率低下。相比之下，列级别方法仅通过计算列嵌入的相似度来确定连接性，虽然效率尚可但由于其在列嵌入中存在的问题（i）语义-连接差距，（ii）大小限制，和（iii）排列敏感性而导致有效性较差。为了解决这些问题，本文提出使用代理列来计算列嵌入；此外还提出了一种新的列级语义连接发现框架Snoopy，利用基于代理列的嵌入在有效性和效率之间建立桥梁。具体而言，提出的列嵌入来自隐式的列到代理列关系，通过轻量级近似图匹配基线捕捉该关系。为了获取指导列投影的良好代理列，我们引入了一个排名感知对比学习范式。大量的实验结果表明，Snoopy在Recall@25和NDCG@25上分别比现有最佳方法高出16%和10%，并且至少快五倍于单元级解决方案，在速度上是现有的列级方法的3.5倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic join discovery, which aims to find columns in a table repositorywith high semantic joinabilities to a query column, is crucial for datasetdiscovery. Existing methods can be divided into two categories: cell-levelmethods and column-level methods. However, neither of them ensures botheffectiveness and efficiency simultaneously. Cell-level methods, which computethe joinability by counting cell matches between columns, enjoy idealeffectiveness but suffer poor efficiency. In contrast, column-level methods,which determine joinability only by computing the similarity of columnembeddings, enjoy proper efficiency but suffer poor effectiveness due to theissues occurring in their column embeddings: (i) semantics-joinability-gap,(ii) size limit, and (iii) permutation sensitivity. To address these issues,this paper proposes to compute column embeddings via proxy columns;furthermore, a novel column-level semantic join discovery framework, Snoopy, ispresented, leveraging proxy-column-based embeddings to bridge effectiveness andefficiency. Specifically, the proposed column embeddings are derived from theimplicit column-to-proxy-column relationships, which are captured by thelightweight approximate-graph-matching-based column projection.To acquire goodproxy columns for guiding the column projection, we introduce a rank-awarecontrastive learning paradigm. Extensive experiments on four real-worlddatasets demonstrate that Snoopy outperforms SOTA column-level methods by 16%in Recall@25 and 10% in NDCG@25, and achieves superior efficiency--being atleast 5 orders of magnitude faster than cell-level solutions, and 3.5x fasterthan existing column-level methods.</description>
      <author>example@mail.com (Yuxiang Guo, Yuren Mao, Zhonghao Hu, Lu Chen, Yunjun Gao)</author>
      <guid isPermaLink="false">2502.16813v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Image Translation-Based Unsupervised Cross-Modality Domain Adaptation for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2502.15193v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图像转换的无监督跨模态领域适应方法，通过将带注释的源模态图像转换为未注释的目标模态，并使用其注释来实现目标模态的监督学习。该方法在跨模态领域适应挑战中的验证阶段表现出色。&lt;h4&gt;背景&lt;/h4&gt;在医学影像中进行监督深度学习通常面临更多挑战，因为标注需要医生的专业知识且耗时费钱；无监督学习方法虽然被采用但性能降低不可避免；医学图像可能来自不同的医疗中心、使用不同设备和采集协议，导致模态差异，进一步降低了深度学习方法的适用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能有效解决跨模态领域适应问题的方法，并在真实场景中验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;通过将带注释的源模态图像转换为目标模态未标注图像，利用目标模态伪图像上的自训练方法克服细微差异，进一步提高深度学习任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在跨模态领域适应挑战中的验证阶段中，针对内耳神经瘤（VS）分割任务取得了平均Dice相似系数(DSC)为0.8351 ± 0.1152和对侧半规管（cochlea）的平均对称表面距离(ASSD)为1.6712±2.1948，在针对内耳神经瘤VS及耳蜗分割任务中取得了平均Dice相似系数(DSC)为0.8098 ± 0.0233和平均对称表面距离(ASSD)为0.2317±0.1577。&lt;h4&gt;结论&lt;/h4&gt;所提方法在跨模态领域适应问题上具有显著优势，能够有效应对医学图像的复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning usually faces more challenges in medical images thanin natural images. Since annotations in medical images require the expertise ofdoctors and are more time-consuming and expensive. Thus, some researchers turnto unsupervised learning methods, which usually face inevitable performancedrops. In addition, medical images may have been acquired at different medicalcenters with different scanners and under different image acquisitionprotocols, so the modalities of the medical images are often inconsistent. Thismodality difference (domain shift) also reduces the applicability of deeplearning methods. In this regard, we propose an unsupervised crossmodalitydomain adaptation method based on image translation by transforming the sourcemodality image with annotation into the unannotated target modality and usingits annotation to achieve supervised learning of the target modality. Inaddition, the subtle differences between translated pseudo images and realimages are overcome by self-training methods to further improve the taskperformance of deep learning. The proposed method showed mean Dice SimilarityCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentationtask of the Cross-Modality Domain Adaptation (crossMoDA 2022) challengevalidation phase leaderboard.</description>
      <author>example@mail.com (Tao Yang, Lisheng Wang)</author>
      <guid isPermaLink="false">2502.15193v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision. arXiv admin  note: text overlap with arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;点云补全是3D视觉中的一个基本但尚未完全解决的问题。现有的方法通常依赖于3D坐标信息和/或其他数据（如图像和扫描视角）来填补缺失部分。与这些方法不同，我们探索了自结构增强，并提出了用于全局到局部点云补全的PointSea。&lt;h4&gt;背景&lt;/h4&gt;点云补全是3D视觉中一个基本但仍未完全解决的问题，现有方法通常依赖于额外的数据来进行补全。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PointSea，利用自投影深度图进行数据增强，并通过特征融合模块从跨模态输入重建紧凑的全局形状，同时在局部阶段揭示高度详细的结构。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': 'PointSea通过使用来自多个视角的自我投影深度图像来增强数据表示。它还集成了一种特性融合模块以融合跨视图和同视图级别特征，以便从跨模态输入中重建紧凑的整体形状。', '局部阶段': '在局部阶段，为了揭示高度详细的结构，我们引入了一个名为自结构对偶生成器的点生成器。该生成器结合了学习到的形状先验知识和几何自相似性来进行形状细化。与现有技术使用统一策略不同的是，我们的双路径设计根据每个点的结构类型适应不同的细化策略。', '创新': 'PointSea提出了一种新的方法来处理全局到局部点云补全的问题，通过利用自投影深度图增强数据表示，并采用特征融合模块和自结构对偶生成器进行形状细化。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PointSea能够有效地理解整体形状并从不完整输入中生成详细信息，明显优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;提出的PointSea在广泛的基准测试中展示了优越的表现，证明了其在全球和局部点云补全中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Closer Look at TabPFN v2: Strength, Limitation, and Extension</title>
      <link>http://arxiv.org/abs/2502.17361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文对基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2)模型进行了详尽评估，确认其在小规模至中等规模任务中的卓越泛化能力。&lt;h4&gt;背景&lt;/h4&gt;表格数据集具有内在异质性，给预训练基础模型的发展带来了巨大挑战。最近引入的基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2) 在多个表格数据集中实现了前所未有的上下文学习准确度。&lt;h4&gt;目的&lt;/h4&gt;全面评估TabPFN v2在超过300个数据集上的性能，揭示其成功的机制，并提出扩大其适用性的策略。&lt;h4&gt;方法&lt;/h4&gt;采用随机化特征标记将异质性数据集统一为固定维度表示；通过leave-one-fold-out方法将其转化为特征提取器；引入Chain-of-Thought提示的分而治之机制以支持大规模任务。&lt;h4&gt;主要发现&lt;/h4&gt;分析显示，随机化特征令牌是TabPFN v2成功的关键因素。此外，该模型能够简化数据分布并提高准确性。&lt;h4&gt;结论&lt;/h4&gt;通过揭示TabPFN v2背后的机制，并提出策略来扩大其适用范围，这项研究为未来表格基础模型的发展提供了关键见解。&lt;h4&gt;翻译&lt;/h4&gt;表格数据集具有内在异质性，给预训练基础模型的发展带来了巨大挑战。最近引入的基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2) 在多个表格数据集中实现了前所未有的上下文学习准确度，标志着表格基础模型的重要进展。在该论文中，我们全面评估了TabPFN v2在超过300个数据集上的性能，确认其卓越的小到中等规模任务的泛化能力。我们的分析确定随机化特征令牌是TabPFN v2成功的关键因素，因为它们将异质性表格数据集统一为固定维度表示，从而更有效的训练和推理。为了进一步理解TabPFN v2的预测结果，我们提出了一种leave-one-fold-out方法，使TabPFN v2转变为一个特征提取器，并揭示其简化数据分布并提高准确性的能力。最后，针对TabPFN v2在高维、大规模和多类别任务中的局限性，我们引入了受Chain-of-Thought提示启发的分而治之机制，实现可扩展推理。通过揭示TabPFN v2成功背后的机制并提出策略来扩大其适用范围，这项研究为未来表格基础模型的发展提供了关键见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular datasets are inherently heterogeneous, posing significant challengesfor developing pre-trained foundation models. The recently introducedtransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achievesunprecedented in-context learning accuracy across multiple tabular datasets,marking a pivotal advancement in tabular foundation models. In this paper, wecomprehensively evaluate TabPFN v2 on over 300 datasets, confirming itsexceptional generalization capabilities on small- to medium-scale tasks. Ouranalysis identifies randomized feature tokens as a key factor behind TabPFNv2's success, as they unify heterogeneous datasets into a fixed-dimensionalrepresentation, enabling more effective training and inference. To furtherunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,transforming TabPFN v2 into a feature extractor and revealing its capability tosimplify data distributions and boost accuracy. Lastly, to address TabPFN v2'slimitations in high-dimensional, large-scale, and many-category tasks, weintroduce a divide-and-conquer mechanism inspired by Chain-of-Thoughtprompting, enabling scalable inference. By uncovering the mechanisms behindTabPFN v2's success and introducing strategies to expand its applicability,this study provides key insights into the future of tabular foundation models.</description>
      <author>example@mail.com (Han-Jia Ye, Si-Yang Liu, Wei-Lun Chao)</author>
      <guid isPermaLink="false">2502.17361v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>In-context learning of evolving data streams with tabular foundational models</title>
      <link>http://arxiv.org/abs/2502.16840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;监督分类中的数据流挖掘传统上依赖于增量决策树集成。然而，大型表格模型（即为结构化数值数据设计的transformer）标志着一个重要的范式转变。&lt;h4&gt;目的&lt;/h4&gt;探索实时模型适应性，并探讨transformer在动态环境下的自适应学习能力。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型和在线提示调整进行上下文学习。通过利用滑动窗口内存策略，TabPFN能够有效处理无限流数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，TabPFN结合简单的滑动内存策略，在所有非平稳基准测试中始终优于Hoeffding树集成。&lt;h4&gt;结论&lt;/h4&gt;论文概述了几种有前景的研究方向，并鼓励社区探索这些想法，以便在上下文流学习方面取得进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了当前监督分类中的数据流挖掘技术从传统的增量决策树转向大型表格模型（transformer）的转变。通过引入在线提示调整和预训练模型来处理无界流数据，实现了实时模型适应性研究，并展示了TabPFN在这种场景下的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art data stream mining in supervised classification hastraditionally relied on ensembles of incremental decision trees. However, theemergence of large tabular models, i.e., transformers designed for structurednumerical data, marks a significant paradigm shift. These models move beyondtraditional weight updates, instead employing in-context learning throughprompt tuning. By using on-the-fly sketches to summarize unbounded streamingdata, one can feed this information into a pre-trained model for efficientprocessing. This work bridges advancements from both areas, highlighting howtransformers' implicit meta-learning abilities, pre-training on driftingnatural data, and reliance on context optimization directly address the corechallenges of adaptive learning in dynamic environments. Exploring real-timemodel adaptation, this research demonstrates that TabPFN, coupled with a simplesliding memory strategy, consistently outperforms ensembles of Hoeffding treesacross all non-stationary benchmarks. Several promising research directions areoutlined in the paper. The authors urge the community to explore these ideas,offering valuable opportunities to advance in-context stream learning.</description>
      <author>example@mail.com (Afonso Lourenço, João Gama, Eric P. Xing, Goreti Marreiros)</author>
      <guid isPermaLink="false">2502.16840v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.16793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VGFL-SA的新颖图对抗攻击，旨在通过修改本地客户端的结构而不使用标签信息来降低垂直联邦学习（VGFL）框架的性能。&lt;h4&gt;背景&lt;/h4&gt;由于隐私保护和利益冲突，需要开发出能够在不直接分享图数据的情况下进行协作训练的垂直联邦图神经网络。现有的对抗性攻击依赖于标签信息的有效性受到限制，这在实际应用中存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有VGFL框架中的未标记客户端问题并提高其安全性，研究人员提出了一种新的对抗攻击方法。&lt;h4&gt;方法&lt;/h4&gt;研究者采用对比学习的方法，在本地客户端训练之前完成攻击任务。具体来说，该方法利用图结构和节点特征信息生成对比视图，并通过共享的图编码器获取每个视图的嵌入表示，进而获得邻接矩阵的梯度并生成扰动边。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，提出的VGFL-SA在现实世界数据集上的节点分类任务中展现了良好的攻击效果和可转移性。&lt;h4&gt;结论&lt;/h4&gt;通过对比学习技术完成无标签信息参与的图对抗攻击可以有效地降低基于垂直联邦框架下GNNs模型的学习性能，这为未来的安全研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have gained attention for their ability to learnrepresentations from graph data. Due to privacy concerns and conflicts ofinterest that prevent clients from directly sharing graph data with oneanother, Vertical Graph Federated Learning (VGFL) frameworks have beendeveloped. Recent studies have shown that VGFL is vulnerable to adversarialattacks that degrade performance. However, it is a common problem that clientnodes are often unlabeled in the realm of VGFL. Consequently, the existingattacks, which rely on the availability of labeling information to obtaingradients, are inherently constrained in their applicability. This limitationprecludes their deployment in practical, real-world environments. To addressthe above problems, we propose a novel graph adversarial attack against VGFL,referred to as VGFL-SA, to degrade the performance of VGFL by modifying thelocal clients structure without using labels. Specifically, VGFL-SA uses acontrastive learning method to complete the attack before the local clients aretrained. VGFL-SA first accesses the graph structure and node featureinformation of the poisoned clients, and generates the contrastive views bynode-degree-based edge augmentation and feature shuffling augmentation. Then,VGFL-SA uses the shared graph encoder to get the embedding of each view, andthe gradients of the adjacency matrices are obtained by the contrastivefunction. Finally, perturbed edges are generated using gradient modificationrules. We validated the performance of VGFL-SA by performing a nodeclassification task on real-world datasets, and the results show that VGFL-SAachieves good attack effectiveness and transferability.</description>
      <author>example@mail.com (Yang Chen, Bin Zhou)</author>
      <guid isPermaLink="false">2502.16793v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence</title>
      <link>http://arxiv.org/abs/2502.16667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8+10 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的深度学习架构MetaSym，结合了强对称归纳偏差和自回归解码器，用于解决物理感知深度学习的挑战。&lt;h4&gt;背景&lt;/h4&gt;具有广泛应用领域的可扩展且通用的物理感知深度学习长期以来一直被视为重大难题。几乎所有物理系统的核心都是辛形式，它支撑着能量、动量等基本不变性。&lt;h4&gt;目的&lt;/h4&gt;引入MetaSym架构，确保核心物理不变性的完整性和灵活的数据高效适应系统异质性。&lt;h4&gt;方法&lt;/h4&gt;将一个获得自对称编码器的强辛归纳偏差和一个具有元注意力机制的自回归解码器相结合来构建新型深度学习架构MetaSym。&lt;h4&gt;主要发现&lt;/h4&gt;在包括高维弹簧网格系统、开放量子系统以及四旋翼动态等多样化数据集上的基准测试中，MetaSym表现出色，在少样本适应情况下模型性能优于现有的最先进的基线方法，并且使用远小于这些基线方法的规模模型就达到了这一效果。&lt;h4&gt;结论&lt;/h4&gt;提出的MetaSym架构在物理感知深度学习任务上具有显著优势，尤其适用于需要灵活适应系统异质性的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scalable and generalizable physics-aware deep learning has long beenconsidered a significant challenge with various applications across diversedomains ranging from robotics to molecular dynamics. Central to almost allphysical systems are symplectic forms, the geometric backbone that underpinsfundamental invariants like energy and momentum. In this work, we introduce anovel deep learning architecture, MetaSym. In particular, MetaSym combines astrong symplectic inductive bias obtained from a symplectic encoder and anautoregressive decoder with meta-attention. This principled design ensures thatcore physical invariants remain intact while allowing flexible, data-efficientadaptation to system heterogeneities. We benchmark MetaSym on highly varieddatasets such as a high-dimensional spring mesh system (Otness et al., 2021),an open quantum system with dissipation and measurement backaction, androbotics-inspired quadrotor dynamics. Our results demonstrate superiorperformance in modeling dynamics under few-shot adaptation, outperformingstate-of-the-art baselines with far larger models.</description>
      <author>example@mail.com (Pranav Vaidhyanathan, Aristotelis Papatheodorou, Mark T. Mitchison, Natalia Ares, Ioannis Havoutis)</author>
      <guid isPermaLink="false">2502.16667v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>RELICT: A Replica Detection Framework for Medical Image Generation</title>
      <link>http://arxiv.org/abs/2502.17360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;尽管合成医疗数据在增强和提高深度学习模型的泛化能力方面具有潜力，但生成模型中的记忆效应可能导致敏感患者信息意外泄露，并限制了模型的实用性。因此，在医学领域使用能够记住训练数据的生成模型可能会危及患者的隐私。&lt;h4&gt;背景&lt;/h4&gt;在医疗领域，利用合成医疗数据来增强和提高深度学习模型的泛化能力是一个潜在的重要研究方向。然而，生成模型中存在的记忆问题可能导致敏感患者信息的泄露，并限制了这些模型的实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架用于识别合成医学图像数据集中的副本（即与训练数据几乎相同的近似拷贝），旨在为医疗成像领域负责任和伦理地使用合成图像提供标准化且易于使用的工具。&lt;h4&gt;方法&lt;/h4&gt;RELICT框架通过三种互补的方法评估图像的相似性：1）体素级别分析；2）由预训练的医学基础模型进行特征级别分析；3）分割级别分析。针对两种临床相关的三维生成建模应用场景进行了研究：非对比头CT与脑内出血（N=774）和圈套动脉的时间飞跃磁共振血管成像（TOF-MRA，N=1,782）。使用专家视觉评分作为参考标准来评估副本的存在。&lt;h4&gt;主要发现&lt;/h4&gt;对于NCCT用例，在选择了最佳阈值的情况下，图像级别和特征级别测量方法可以完美地分类副本，平衡准确率为1；而对于TOF-MRA案例，则无法在任何阈值下实现完美的副本分类，但分割级别分析的平衡准确性为0.79。&lt;h4&gt;结论&lt;/h4&gt;副本检测是生成模型开发中的一个关键但被忽视的验证步骤。RELICT框架提供了一个标准化、易于使用的工具来识别副本，并旨在促进医学图像合成的责任感和伦理规范。&lt;h4&gt;其他细节&lt;/h4&gt;本研究强调了在医疗影像领域发展生成模型时，防止敏感信息泄露的重要性，并提出了一种新的评估方法用于检测合成数据集中可能出现的真实训练数据副本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the potential of synthetic medical data for augmenting and improvingthe generalizability of deep learning models, memorization in generative modelscan lead to unintended leakage of sensitive patient information and limit modelutility. Thus, the use of memorizing generative models in the medical domaincan jeopardize patient privacy. We propose a framework for identifyingreplicas, i.e. nearly identical copies of the training data, in syntheticmedical image datasets. Our REpLIca deteCTion (RELICT) framework for medicalimage generative models evaluates image similarity using three complementaryapproaches: (1) voxel-level analysis, (2) feature-level analysis by apretrained medical foundation model, and (3) segmentation-level analysis. Twoclinically relevant 3D generative modelling use cases were investigated:non-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flightMR angiography of the Circle of Willis (N=1,782). Expert visual scoring wasused as the reference standard to assess the presence of replicas. We reportthe balanced accuracy at the optimal threshold to assess replica classificationperformance. The reference visual rating identified 45 of 50 and 5 of 50generated images as replicas for the NCCT and TOF-MRA use cases, respectively.Image-level and feature-level measures perfectly classified replicas with abalanced accuracy of 1 when an optimal threshold was selected for the NCCT usecase. A perfect classification of replicas for the TOF-MRA case was notpossible at any threshold, with the segmentation-level analysis achieving abalanced accuracy of 0.79. Replica detection is a crucial but neglectedvalidation step for the development of generative models in medical imaging.The proposed RELICT framework provides a standardized, easy-to-use tool forreplica detection and aims to facilitate responsible and ethical medical imagesynthesis.</description>
      <author>example@mail.com (Orhun Utku Aydin, Alexander Koch, Adam Hilbert, Jana Rieger, Felix Lohrke, Fujimaro Ishida, Satoru Tanioka, Dietmar Frey)</author>
      <guid isPermaLink="false">2502.17360v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment</title>
      <link>http://arxiv.org/abs/2502.16469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2403.16188&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于元学习的框架，通过引入丰富的文本语义作为辅助模态来解决跨域多模态少样本目标检测中的领域偏移问题。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态物体检测方法在遇到显著的领域变化时会表现出性能下降。现有的跨模态特征提取和集成的进步提高了少量样本学习任务的表现，但仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过结合丰富的文本信息来增强模型建立视觉实例与其语言描述之间的知识关系的能力，从而减轻领域偏移带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架用于解决跨域多模态少样本目标检测问题。该框架包含两个关键组件：一个多模态特征聚合模块和一个丰富文本语义修正模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在常见的跨域物体检测基准上，所提方法显著超越了现有的少样本物体检测方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入元学习的框架并利用丰富的文本信息，论文成功地提高了模型在领域偏移情况下的适应性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一个旨在解决跨域多模态少样本目标检测问题的方法。该方法结合了视觉和语言特征，并采用了文本语义修正模块来增强其性能。实验结果显示，在标准基准测试中，这种方法优于现有技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in cross-modal feature extraction and integration havesignificantly enhanced performance in few-shot learning tasks. However, currentmulti-modal object detection (MM-OD) methods often experience notableperformance degradation when encountering substantial domain shifts. We proposethat incorporating rich textual information can enable the model to establish amore robust knowledge relationship between visual instances and theircorresponding language descriptions, thereby mitigating the challenges ofdomain shift. Specifically, we focus on the problem of Cross-Domain Multi-ModalFew-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-basedframework designed to leverage rich textual semantics as an auxiliary modalityto achieve effective domain adaptation. Our new architecture incorporates twokey components: (i) A multi-modal feature aggregation module, which alignsvisual and linguistic feature embeddings to ensure cohesive integration acrossmodalities. (ii) A rich text semantic rectification module, which employsbidirectional text feature generation to refine multi-modal feature alignment,thereby enhancing understanding of language and its application in objectdetection. We evaluate the proposed method on common cross-domain objectdetection benchmarks and demonstrate that it significantly surpasses existingfew-shot object detection approaches.</description>
      <author>example@mail.com (Zeyu Shangguan, Daniel Seita, Mohammad Rostami)</author>
      <guid isPermaLink="false">2502.16469v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Utilizing AI and Machine Learning for Predictive Analysis of Post-Treatment Cancer Recurrence</title>
      <link>http://arxiv.org/abs/2502.15825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;探讨了人工智能和机器学习在癌症复发预测中的应用，以及它们如何通过分析大量遗传学、临床表现和治疗数据来提高个性化医疗水平。&lt;h4&gt;背景&lt;/h4&gt;肿瘤复发是肿瘤学中一个主要挑战，传统的癌症复发预测依赖于统计模型支持的临床观察，但无法完全解释其复杂的多因素特性。&lt;h4&gt;目的&lt;/h4&gt;研究AI和ML在癌症复发预测中的潜在应用，以改善治疗后的患者生存率和生活质量。&lt;h4&gt;方法&lt;/h4&gt;描述了使用监督学习和无监督学习技术来识别模式并预测癌症患者的结局的各种AI和ML技术。&lt;h4&gt;主要发现&lt;/h4&gt;AI和ML技术能够提供早期干预的机会，并有助于设计更有效的治疗计划。&lt;h4&gt;结论&lt;/h4&gt;AI和ML为个性化医学和患者管理提供了新的机会，提高了复发预测的准确性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已从英文翻译为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.60087/jklst.vol2.n3.p599&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In oncology, recurrence after treatment is one of the major challenges,related to patients' survival and quality of life. Conventionally, predictionof cancer relapse has always relied on clinical observation with statisticalmodel support, which almost fails to explain the complex, multifactorial natureof tumor recurrence. This research explores how AI and ML models may increasethe accuracy and reliability of recurrence prediction in cancer. Therefore, AIand ML create new opportunities not only for personalized medicine but also forproactive management of patients through analyzing large volumes of data ongenetics, clinical manifestations, and treatment. The paper describes thevarious AI and ML techniques for pattern identification and outcome predictionin cancer patients using supervised and unsupervised learning. Clinicalimplications provide an opportunity to review how early interventions couldhappen and the design of treatment planning.</description>
      <author>example@mail.com (Muhammad Umer Qayyum, Muhammad Fahad, Nasrullah Abbasi)</author>
      <guid isPermaLink="false">2502.15825v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Trunk-branch Contrastive Network with Multi-view Deformable Aggregation for Multi-view Action Recognition</title>
      <link>http://arxiv.org/abs/2502.16493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的称为TBCNet的框架，用于基于RGB多视角的动作识别。该网络通过主干和分支的对比学习过程获得融合特征，并补充关键细节。&lt;h4&gt;背景&lt;/h4&gt;传统的动作识别研究通常从每个视图中提取精炼特征，然后实现配对交互和整合，但这种方法可能会忽视每个视图中的重要局部特征。&lt;h4&gt;目的&lt;/h4&gt;为了模拟人类从多个角度观察物体时形成的综合印象以及随后填补具体细节的认知过程，提出了一种新的网络框架TBCNet。&lt;h4&gt;方法&lt;/h4&gt;设计了两个核心组件：多视角可变形聚集（MVDA）和主干-分支对比学习。 MVDA利用全局汇聚模块强调重要的空间信息，并通过复合相对位置偏差捕捉视图内的及跨视图的相对位置，而主干-分支对比损失则是在聚合特征与每个视图中的精炼细节之间构建。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示TBCNet在NTU-RGB+D 60, NTU-RGB+D 120, PKU-MMD和N-UCLA等四个数据集上优于其他基于RGB的方法，尤其在跨主体（Cross-Subject）及跨场景（Cross-View）协议下取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为RGB多视角动作识别提供了一种有效的新方法TBCNet，并通过实验验证了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view action recognition aims to identify actions in a given multi-viewscene. Traditional studies initially extracted refined features from each view,followed by implemented paired interaction and integration, but theypotentially overlooked the critical local features in each view. When observingobjects from multiple perspectives, individuals typically form a comprehensiveimpression and subsequently fill in specific details. Drawing inspiration fromthis cognitive process, we propose a novel trunk-branch contrastive network(TBCNet) for RGB-based multi-view action recognition. Distinctively, TBCNetfirst obtains fused features in the trunk block and then implicitly supplementsvital details provided by the branch block via contrastive learning, generatinga more informative and comprehensive action representation. Within thisframework, we construct two core components: the multi-view deformableaggregation and the trunk-branch contrastive learning. MVDA employed in thetrunk block effectively facilitates multi-view feature fusion and adaptivecross-view spatio-temporal correlation, where a global aggregation module isutilized to emphasize significant spatial information and a composite relativeposition bias is designed to capture the intra- and cross-view relativepositions. Moreover, a trunk-branch contrastive loss is constructed betweenaggregated features and refined details from each view. By incorporating twodistinct weights for positive and negative samples, a weighted trunk-branchcontrastive loss is proposed to extract valuable information and emphasizesubtle inter-class differences. The effectiveness of TBCNet is verified byextensive experiments on four datasets including NTU-RGB+D 60, NTU-RGB+D 120,PKU-MMD, and N-UCLA dataset. Compared to other RGB-based methods, our approachachieves state-of-the-art performance in cross-subject and cross-settingprotocols.</description>
      <author>example@mail.com (Yingyuan Yang, Guoyuan Liang, Can Wang, Xiaojun Wu)</author>
      <guid isPermaLink="false">2502.16493v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders</title>
      <link>http://arxiv.org/abs/2502.16722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了预训练变压器微调过程中的内部机制，特别是BERT模型，并通过分析激活相似性、训练稀疏自编码器以及可视化不同层的标记级激活来探索这一过程。&lt;h4&gt;背景&lt;/h4&gt;微调预训练的变换器是增强基础模型在特定任务上性能的强大技术。这种方法对于将通用架构适应于专门的任务非常关键，从早期应用到如BERT这样的模型到现在用于大型语言模型（LLM）的应用。&lt;h4&gt;目的&lt;/h4&gt;理解微调过程对于揭示变压器如何根据具体目标进行调整、保留一般表示以及获取任务特有特征至关重要。&lt;h4&gt;方法&lt;/h4&gt;论文通过分析激活相似性、训练稀疏自编码器和可视化不同层的标记级激活来探索微调机制，特别是针对BERT变换器。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示随着深度增加，特征如何适应任务的变化：早期层次主要保留一般表示；中间层次充当通用与任务特有特征之间的过渡；后期层次完全专注于任务适应。&lt;h4&gt;结论&lt;/h4&gt;这些发现在理解微调过程和它对转换架构内表征学习的影响方面提供了关键见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning pre-trained transformers is a powerful technique for enhancingthe performance of base models on specific tasks. From early applications inmodels like BERT to fine-tuning Large Language Models (LLMs), this approach hasbeen instrumental in adapting general-purpose architectures for specializeddownstream tasks. Understanding the fine-tuning process is crucial foruncovering how transformers adapt to specific objectives, retain generalrepresentations, and acquire task-specific features. This paper explores theunderlying mechanisms of fine-tuning, specifically in the BERT transformer, byanalyzing activation similarity, training Sparse AutoEncoders (SAEs), andvisualizing token-level activations across different layers. Based onexperiments conducted across multiple datasets and BERT layers, we observe asteady progression in how features adapt to the task at hand: early layersprimarily retain general representations, middle layers act as a transitionbetween general and task-specific features, and later layers fully specializein task adaptation. These findings provide key insights into the inner workingsof fine-tuning and its impact on representation learning within transformerarchitectures.</description>
      <author>example@mail.com (Suneel Nadipalli)</author>
      <guid isPermaLink="false">2502.16722v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge</title>
      <link>http://arxiv.org/abs/2502.16451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 14 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了用于材料逆向设计的人工智能（AI）在晶体领域的应用，提出了对比语言-晶体模型CLaC，并通过实验验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;人工智能技术越来越多地应用于材料的逆向设计中，尤其是在分子领域，已经成功将化学结构与文本知识结合使用。然而，在晶体研究方面，由于偏斜的数据分布和学术文献中的语义监督不足，这种方法难以实现。&lt;h4&gt;目的&lt;/h4&gt;为了克服数据稀缺问题，并展示合成数据在解决这一问题上的优势，提出了一种新的对比语言-晶体模型CLaC。&lt;h4&gt;方法&lt;/h4&gt;通过构建包含126k晶体结构文本对的新合成数据集和一个从学术论文中提取的相似数据集，预训练了CLaC模型。然后评估其跨模态任务和下游应用中的零样本泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CLaC在理解晶体结构方面实现了最新的零样本泛化性能，并且超越了现有的大型语言模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的CLaC模型展示了其在理解和设计晶体材料方面的潜力，为未来的AI辅助逆向材料设计提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的原文是关于介绍了一种对比语言-晶体模型（CLaC），该模型基于126K个合成的数据集进行预训练，并通过跨模态任务和下游应用验证了其优越性，特别是在零样本泛化性能方面超越了当前的大规模语言模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) is increasingly used for the inverse design ofmaterials, such as crystals and molecules. Existing AI research on moleculeshas integrated chemical structures of molecules with textual knowledge to adaptto complex instructions. However, this approach has been unattainable forcrystals due to data scarcity from the biased distribution of investigatedcrystals and the lack of semantic supervision in peer-reviewed literature. Inthis work, we introduce a contrastive language-crystals model (CLaC)pre-trained on a newly synthesized dataset of 126k crystal structure-textpairs. To demonstrate the advantage of using synthetic data to overcome datascarcity, we constructed a comparable dataset extracted from academic papers.We evaluate CLaC's generalization ability through various zero-shot cross-modaltasks and downstream applications. In experiments, CLaC achievesstate-of-the-art zero-shot generalization performance in understanding crystalstructures, surpassing latest large language models.</description>
      <author>example@mail.com (Yang Jeong Park, Mayank Kumaran, Chia-Wei Hsu, Elsa Olivetti, Ju Li)</author>
      <guid isPermaLink="false">2502.16451v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI</title>
      <link>http://arxiv.org/abs/2502.17092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Shakti VLM是一个包含10亿和40亿参数的视觉-语言模型家族，旨在解决多模态学习中的数据效率挑战。&lt;h4&gt;背景&lt;/h4&gt;近年来，许多视觉-语言模型通过大量训练数据取得了优异的成绩。然而，在大规模数据集不可用的情况下，现有方法难以有效解决问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种可以利用架构创新来减少对海量训练数据依赖的视觉-语言模型。&lt;h4&gt;方法&lt;/h4&gt;1. 使用QK-Normalization提高注意力机制的稳定性2. 引入混合归一化技术以增强模型性能3. 采用改进的位置编码提升多模态理解能力4. 实施三阶段训练策略优化学习效率&lt;h4&gt;主要发现&lt;/h4&gt;Shakti VLM-1B和Shakti VLM-4B在文档理解、视觉推理、光学字符识别提取以及通用的多模态推理任务中表现出色，证明了良好的模型设计和有效的训练策略同样可以实现高精度。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过精心设计的架构和技术创新可以使模型更加高效地处理大规模的多模态任务，并且不必依赖海量的数据集。Shakti VLM为解决企业级应用场景中的问题提供了一个高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们介绍了Shakti VLM，这是一个参数容量分别为10亿和40亿的视觉-语言模型家族，旨在应对多模态学习中数据效率方面的挑战。尽管最近的一些视觉-语言模型通过大量训练数据取得了良好的成绩，但Shakti模型则利用架构创新，在较少的数据量下也能取得竞争性的结果。关键改进包括用于提高注意力机制稳定性的QK归一化技术、混合归一化方法以及增强的位置编码策略。此外，一种三阶段的训练策略进一步优化了学习效率。评估结果显示，无论是文档理解还是视觉推理等多模态任务，Shakti VLM-1B和Shakti VLM-4B均表现出色。我们的研究结果表明，通过模型设计和有效的训练策略可以实现高精度，而不需要依靠大量数据集的支持。这使得Shakti成为大规模多模态应用场景下的一种高效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Shakti VLM, a family of vision-language models in the capacityof 1B and 4B parameters designed to address data efficiency challenges inmultimodal learning. While recent VLMs achieve strong performance throughextensive training data, Shakti models leverage architectural innovations toattain competitive results with fewer tokens. Key advancements includeQK-Normalization for attention stability, hybrid normalization techniques, andenhanced positional encoding. A three-stage training strategy further optimizeslearning efficiency. Evaluations show that Shakti-Shakti-VLM-1B andShakti-VLM-4B excel in document understanding, Visual Reasoning, OCRextraction, and general multimodal reasoning. Our results highlight that highperformance can be achieved through model design and training strategy ratherthan sheer data volume, making Shakti an efficient solution forenterprise-scale multimodal tasks.</description>
      <author>example@mail.com (Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi)</author>
      <guid isPermaLink="false">2502.17092v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning</title>
      <link>http://arxiv.org/abs/2502.16932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://demo-generation.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种低成本的合成数据生成方法DemoGen，该方法能够在不需要大量人工采集的情况下自动生成演示任务，并通过3D点云和场景编辑来增强空间推广能力。&lt;h4&gt;背景&lt;/h4&gt;视觉运动策略在机器人操作中显示出巨大潜力，但由于其有限的空间泛化能力，通常需要大量的手工收集的数据以实现有效性能。&lt;h4&gt;目的&lt;/h4&gt;提出一个低成本、全合成的方法DemoGen，用于自动生成演示，仅需少量的人类收集的示例即可推广到新的对象配置上。&lt;h4&gt;方法&lt;/h4&gt;通过使用3D点云作为观察模式，并通过场景编辑重新排列主体来生成视觉观测。这种方法允许将已有的动作轨迹适应于新的物体布局中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DemoGen能够显著提高在各种现实世界操作任务中的策略性能，甚至包括复杂的情境如变形对象、灵巧的手末端执行器和双臂平台的操作。&lt;h4&gt;结论&lt;/h4&gt;除了改进空间推广能力外，DemoGen还可以扩展以提供额外的分布外功能，例如对干扰的抵抗能力和避障能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉运动策略在机器人操作中已显示出巨大的潜力，但为了实现有效性能，通常需要大量的手工收集的数据。一个主要原因在于其有限的空间泛化能力，这要求必须跨越不同物体配置广泛地采集数据。在这项工作中，我们提出了DemoGen，这是一种低成本、完全合成的方法来自动生成演示。利用仅需一次的人工收集的示例，DemoGen通过适应已展示的动作轨迹到新的物体布局中来生成空间增强的演示。视觉观测是通过使用3D点云作为模态并重新排列场景中的主体以实现3D编辑的方式进行合成。实证上，DemoGen显著提升了在各种现实世界操作任务中的策略性能，并展示了其在涉及可变形对象、灵巧手末端执行器和双臂平台的挑战性情况下的应用潜力。此外，DemoGen可以扩展以提供额外的分布外功能，包括干扰抵抗能力和避障能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visuomotor policies have shown great promise in robotic manipulation butoften require substantial amounts of human-collected data for effectiveperformance. A key reason underlying the data demands is their limited spatialgeneralization capability, which necessitates extensive data collection acrossdifferent object configurations. In this work, we present DemoGen, a low-cost,fully synthetic approach for automatic demonstration generation. Using only onehuman-collected demonstration per task, DemoGen generates spatially augmenteddemonstrations by adapting the demonstrated action trajectory to novel objectconfigurations. Visual observations are synthesized by leveraging 3D pointclouds as the modality and rearranging the subjects in the scene via 3Dediting. Empirically, DemoGen significantly enhances policy performance acrossa diverse range of real-world manipulation tasks, showing its applicabilityeven in challenging scenarios involving deformable objects, dexterous handend-effectors, and bimanual platforms. Furthermore, DemoGen can be extended toenable additional out-of-distribution capabilities, including disturbanceresistance and obstacle avoidance.</description>
      <author>example@mail.com (Zhengrong Xue, Shuying Deng, Zhenyang Chen, Yixuan Wang, Zhecheng Yuan, Huazhe Xu)</author>
      <guid isPermaLink="false">2502.16932v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Training for Defense Against Label Poisoning Attacks</title>
      <link>http://arxiv.org/abs/2502.17121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Learning Representations  (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的对抗训练防御策略FLORAL，该策略基于支持向量机（SVM）来应对模型训练过程中标签中毒攻击的威胁。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习模型越来越复杂且依赖于公共数据源进行训练，例如大规模语言模型使用的标注数据，这些模型更容易受到标签中毒攻击。这种攻击方式是通过对手微妙地改变训练集中的标签来进行的，这可能导致模型性能严重下降，在关键应用中造成重大风险。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的防御策略FLORAL来对抗由标签中毒造成的威胁。&lt;h4&gt;方法&lt;/h4&gt;基于支持向量机（SVM）和双层优化框架，将训练过程描述为攻防双方的非零和斯塔克伯格博弈。该方法适应多种模型架构，并使用带有核函数的支持向量机进行投影梯度下降算法以执行对抗训练。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明了算法收敛性的特性，实验结果证实FLORAL在各种分类任务中均能取得比Robust基线和RoBERTa等基础模型更好的鲁棒准确性。当攻击者预算增加时，FLORAL仍然能够保持更高的稳健精度。&lt;h4&gt;结论&lt;/h4&gt;FLORAL策略具有提高机器学习模型对抗标签中毒威胁的鲁棒性的潜力，在敌对环境中确保分类任务的安全性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;随着机器学习模型变得越来越复杂，并且越来越多地依赖于公开来源的数据，例如在训练大型语言模型时使用的由人类标注的标签，这些模型更容易受到标签中毒攻击。这种攻击方式是通过对手微妙地改变训练数据集中的标签来执行的，这可能导致模型性能严重下降，在关键应用中造成重大风险。在这篇论文中，我们提出了FLORAL策略，这是一种基于支持向量机（SVM）的新对抗性训练防御策略，用于应对这些威胁。使用双层优化框架，我们将训练过程描述为攻防双方之间的非零和斯塔克伯格博弈，一方是战略性地污染关键训练标签的攻击者，另一方是试图从这些攻击中恢复过来的模型。该方法适用于多种架构，并采用带有核函数的支持向量机进行投影梯度下降算法来进行对抗性训练。我们提供了该算法收敛性质的理论分析，并通过实验证明了FLORAL策略在各种分类任务中的有效性。与鲁棒基线和基础模型（如RoBERTa）相比，随着攻击者预算增加时，FLORAL始终能保持更高的稳健精度。这些结果强调了FLORAL提高机器学习模型对抗标签中毒威胁的韧性潜力，在敌对环境中确保分类任务的安全性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As machine learning models grow in complexity and increasingly rely onpublicly sourced data, such as the human-annotated labels used in traininglarge language models, they become more vulnerable to label poisoning attacks.These attacks, in which adversaries subtly alter the labels within a trainingdataset, can severely degrade model performance, posing significant risks incritical applications. In this paper, we propose FLORAL, a novel adversarialtraining defense strategy based on support vector machines (SVMs) to counterthese threats. Utilizing a bilevel optimization framework, we cast the trainingprocess as a non-zero-sum Stackelberg game between an attacker, whostrategically poisons critical training labels, and the model, which seeks torecover from such attacks. Our approach accommodates various modelarchitectures and employs a projected gradient descent algorithm with kernelSVMs for adversarial training. We provide a theoretical analysis of ouralgorithm's convergence properties and empirically evaluate FLORAL'seffectiveness across diverse classification tasks. Compared to robust baselinesand foundation models such as RoBERTa, FLORAL consistently achieves higherrobust accuracy under increasing attacker budgets. These results underscore thepotential of FLORAL to enhance the resilience of machine learning modelsagainst label poisoning threats, thereby ensuring robust classification inadversarial settings.</description>
      <author>example@mail.com (Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach)</author>
      <guid isPermaLink="false">2502.17121v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unified Semantic and ID Representation Learning for Deep Recommenders</title>
      <link>http://arxiv.org/abs/2502.16474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合ID和语义表示的推荐系统框架，旨在解决传统基于ID令牌的推荐系统的冗余问题以及新项目冷启动时的表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的推荐系统依赖于ID令牌来唯一标识项目，但在处理项目重复和新项目的推荐方面存在不足。最近的方法尝试使用语义令牌作为替代方案，但面临挑战如项目复制和不一致的性能提升。&lt;h4&gt;目的&lt;/h4&gt;开发一种综合了ID和语义表示的学习框架以克服现有方法的局限性，并探索余弦相似度和欧几里得距离在嵌入搜索中的作用。&lt;h4&gt;方法&lt;/h4&gt;提出了一个统一的ID与语义表示学习框架，该框架利用两种令牌类型的优势。在这个框架中，ID令牌捕捉项目独特属性，而语义令牌则代表共享、可转移的特点。此外，还分析了余弦相似度和欧几里得距离在嵌入搜索中的作用，并整合这两种方法来优化表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著优于现有的基准模型，在三个基准数据集上的性能提高了6%至17%，并且令牌大小减少了超过80%。&lt;h4&gt;结论&lt;/h4&gt;本文证明了将ID和语义标记结合可以增强推荐系统的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;有效的推荐对大型在线平台至关重要。传统的推荐系统主要依赖于标识符（ID）令牌来唯一识别项目，能够有效捕捉特定项目的联系，但在冗余性和冷启动场景中的表现不佳。最近的研究探索了使用语义令牌作为替代方法，但面临诸如项目复制和不一致性能收益的问题。为解决这些局限性，本文提出了一种综合ID与语义表示的学习框架，利用两种标记类型的优势。实验显示该方法在三个基准数据集上显著优于现有基线模型，改善幅度从6%到17%，并且令牌大小减少了超过80%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective recommendation is crucial for large-scale online platforms.Traditional recommendation systems primarily rely on ID tokens to uniquelyidentify items, which can effectively capture specific item relationships butsuffer from issues such as redundancy and poor performance in cold-startscenarios. Recent approaches have explored using semantic tokens as analternative, yet they face challenges, including item duplication andinconsistent performance gains, leaving the potential advantages of semantictokens inadequately examined. To address these limitations, we propose aUnified Semantic and ID Representation Learning framework that leverages thecomplementary strengths of both token types. In our framework, ID tokenscapture unique item attributes, while semantic tokens represent shared,transferable characteristics. Additionally, we analyze the role of cosinesimilarity and Euclidean distance in embedding search, revealing that cosinesimilarity is more effective in decoupling accumulated embeddings, whileEuclidean distance excels in distinguishing unique items. Our frameworkintegrates cosine similarity in earlier layers and Euclidean distance in thefinal layer to optimize representation learning. Experiments on three benchmarkdatasets show that our method significantly outperforms state-of-the-artbaselines, with improvements ranging from 6\% to 17\% and a reduction in tokensize by over 80%. These results demonstrate the effectiveness of combining IDand semantic tokenization to enhance the generalization ability of recommendersystems.</description>
      <author>example@mail.com (Guanyu Lin, Zhigang Hua, Tao Feng, Shuang Yang, Bo Long, Jiaxuan You)</author>
      <guid isPermaLink="false">2502.16474v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Separated Contrastive Learning for Matching in Cross-domain Recommendation with Curriculum Scheduling</title>
      <link>http://arxiv.org/abs/2502.16239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TheWebConf 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SCCDR的新框架，用于解决跨域推荐任务中的训练不稳定问题。&lt;h4&gt;背景&lt;/h4&gt;跨域推荐(CDR)旨在通过利用源领域信息来改善目标领域的推荐性能。对比学习方法在处理同一领域内的用户或项目时被广泛采用，并且对于知识迁移和表示学习也很有效。&lt;h4&gt;目的&lt;/h4&gt;解决直接应用对比学习于混合的同域内和跨域任务所带来的训练不稳定问题，这会导致表示学习过程恶化以及生成嵌入质量降低的问题。&lt;h4&gt;方法&lt;/h4&gt;SCCDR基于分离的同域内和跨域内的对比学习模式以及一个停止梯度操作来处理这一不足。该框架包括两个专门的课程阶段：同异域分离和跨域课程调度。前者为源域和目标域分别使用了两种不同的对比视角，后者则通过考虑重叠用户所锚定的负样本难度，采用了课程调度策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明SCCDR在多个基准上达到了最新的性能水平，并且在线A/B测试也验证了这一点。&lt;h4&gt;结论&lt;/h4&gt;提出的框架能够有效地解决跨域推荐任务中的训练不稳定问题，并提高表示学习的质量和生成嵌入的效果，从而提升整体推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了SCCDR框架的创新方法及其在解决跨域推荐（CDR）中对比学习时遇到的问题方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701716.3715260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain recommendation (CDR) is a task that aims to improve therecommendation performance in a target domain by leveraging the informationfrom source domains. Contrastive learning methods have been widely adoptedamong intra-domain (intra-CL) and inter-domain (inter-CL) users/items for theirrepresentation learning and knowledge transfer during the matching stage ofCDR. However, we observe that directly employing contrastive learning onmixed-up intra-CL and inter-CL tasks ignores the difficulty of learning frominter-domain over learning from intra-domain, and thus could cause severetraining instability. Therefore, this instability deteriorates therepresentation learning process and hurts the quality of generated embeddings.To this end, we propose a novel framework named SCCDR built up on a separatedintra-CL and inter-CL paradigm and a stop-gradient operation to handle thedrawback. Specifically, SCCDR comprises two specialized curriculum stages:intra-inter separation and inter-domain curriculum scheduling. The former stageexplicitly uses two distinct contrastive views for the intra-CL task in thesource and target domains, respectively. Meanwhile, the latter stagedeliberately tackles the inter-CL tasks with a curriculum scheduling strategythat derives effective curricula by accounting for the difficulty of negativesamples anchored by overlapping users. Empirical experiments on variousopen-source datasets and an offline proprietary industrial dataset extractedfrom a real-world recommender system, and an online A/B test verify that SCCDRachieves state-of-the-art performance over multiple baselines.</description>
      <author>example@mail.com (Heng Chang, Liang Gu, Cheng Hu, Zhinan Zhang, Hong Zhu, Yuhui Xu, Yuan Fang, Zhen Chen)</author>
      <guid isPermaLink="false">2502.16239v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Set a Thief to Catch a Thief: Combating Label Noise through Noisy Meta Learning</title>
      <link>http://arxiv.org/abs/2502.16104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一个新颖的噪声元标签校正框架STCT，旨在利用带有噪声的数据来纠正标签错误，并通过实验验证了其在高噪声率场景下的卓越性能。&lt;h4&gt;背景&lt;/h4&gt;从嘈杂的标签中学习（LNL）的目标是使用带噪数据集训练高性能深度模型。基于元学习的方法已经在LNL任务上表现出色，但需要额外的干净验证集来执行标签校正，这限制了其实用性。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一问题，本文提出了一种新颖的噪声元标签校正框架STCT，该框架可以使用带噪数据作为验证集，并在不依赖于额外干净数据的情况下进行标签校正。&lt;h4&gt;方法&lt;/h4&gt;STCT通过将复杂的双层优化分解为表示学习和标签校正两个部分，并采用交替训练策略来解决这个问题。具体来说，在元学习框架中利用与训练数据独立同分布的噪声数据作为验证集评估模型性能并执行标签校正。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，STCT在合成数据集和真实世界数据集上展示了卓越的表现，特别是在高噪声率场景下。当CIFAR-10数据集中含有80%对称噪声时，STCT的标签校正准确率为96.9%，分类性能为95.2%，显著优于现有最佳方法。&lt;h4&gt;结论&lt;/h4&gt;与现有的基于元学习的LNL方法相比，所提出的STCT框架通过利用带噪数据进行自我纠正，在不依赖额外干净验证集的情况下实现了更优的标签校正和模型训练效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning from noisy labels (LNL) aims to train high-performance deep modelsusing noisy datasets. Meta learning based label correction methods havedemonstrated remarkable performance in LNL by designing various meta labelrectification tasks. However, extra clean validation set is a prerequisite forthese methods to perform label correction, requiring extra labor and greatlylimiting their practicality. To tackle this issue, we propose a novel noisymeta label correction framework STCT, which counterintuitively uses noisy datato correct label noise, borrowing the spirit in the saying ``Set a Thief toCatch a Thief''. The core idea of STCT is to leverage noisy data which isi.i.d. with the training data as a validation set to evaluate model performanceand perform label correction in a meta learning framework, eliminating the needfor extra clean data. By decoupling the complex bi-level optimization in metalearning into representation learning and label correction, STCT is solvedthrough an alternating training strategy between noisy meta correction andsemi-supervised representation learning. Extensive experiments on synthetic andreal-world datasets demonstrate the outstanding performance of STCT,particularly in high noise rate scenarios. STCT achieves 96.9% label correctionand 95.2% classification performance on CIFAR-10 with 80% symmetric noise,significantly surpassing the current state-of-the-art.</description>
      <author>example@mail.com (Hanxuan Wang, Na Lu, Xueying Zhao, Yuxuan Yan, Kaipeng Ma, Kwoh Chee Keong, Gustavo Carneiro)</author>
      <guid isPermaLink="false">2502.16104v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling Institution-Specific Bias in Pathology Foundation Models: Detriments, Causes, and Potential Solutions</title>
      <link>http://arxiv.org/abs/2502.16889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,1 figure,14 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;病理基础模型在提取有价值的区别性特征方面具有优势，但存在图像特异性信息污染的问题，影响了其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;病理基础模型简化了深度学习模型的开发，并通过利用先验知识提高了诊断准确性。然而，在实际临床应用中，由于机构特定的信息干扰，这些模型的表现可能会下降。&lt;h4&gt;目的&lt;/h4&gt;揭示病理基础模型中的特征污染问题及其对性能的影响，并探讨其背后的原因及可能的解决方案。&lt;h4&gt;方法&lt;/h4&gt;识别并验证了病理图像中的机构特定信息如何被当前的基础模型捕捉到，通过广泛的实验展示了非诊断相关的信息在出界分布场景下对性能的负面影响。&lt;h4&gt;主要发现&lt;/h4&gt;病理基础模型容易提取与疾病无关但又存在于不同医疗机构之间的特征信息。这些污染导致了虚假的相关性，削弱了模型的应用能力。&lt;h4&gt;结论&lt;/h4&gt;提出了减轻机构特定信息影响的方法，并呼吁未来的研究关注创新训练策略而非单纯依赖规模效应来发展更具有泛化的病理基础模型。&lt;h4&gt;翻译&lt;/h4&gt;病理基础模型（PFMs）从图像中提取有价值的区别性特征用于下游临床任务。虽然它们简化了深度学习模型的开发，有效利用先验知识提高了不同场景下的诊断准确性，但发现PFMs有时面临挑战：从图像中提取出的特性经常受到与诊断无关的信息干扰，即特定机构相关的特性，这可能导致虚假的相关性并削弱模型在现实中的应用能力。在这项研究中，我们揭示了特征污染的问题，展示了病理基础模型中存在的机构特有特性，并深入调查其负面影响、分析原因并提出见解。我们发现当前的PFMs可以轻易捕捉到病理图像中的特定信息，通过广泛的实验表明，非诊断相关信息对性能有害，特别是在出界分布设置下，依赖于这些被污染的特征会导致显著的表现下降。这揭示了模型可能受到误导的因素。进一步探讨了PFMs提取机构特定信息的原因，并验证了这一发现。最后提出了一个简单而有效的方法来缓解无关信息的影响。这项研究并非旨在批评现有的病理基础模型，而是要启发未来的科研专注于创新训练策略而非仅依赖规模效应以实现更加泛化的病理基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathology foundation models (PFMs) extract valuable discriminative featuresfrom images for downstream clinical tasks. PFMs have simplified the developmentof deep learning models, effectively leveraging prior knowledge to improvediagnostic accuracy in diverse scenarios. However, we find that PFMs sometimesstruggle with certain challenges. Specifically, features extracted by PFMs areoften contaminated by diagnosis-irrelevant information, i.e.,institution-specific features associated with the images. This contaminationcan lead to spurious correlations, undermining the models' generalizationability when applied in real-world clinical settings. In this work, we firstreveal the issue of feature contamination in PFMs, demonstrate the presence ofinstitution-specific features, thoroughly investigate its negative impacts,analyze the underlying causes, and provide insights into potential solutions.Specifically, we find that institution-specific information is embedded inpathological images and can be readily captured by current PFMs. Throughextensive experiments, we demonstrate the detrimental impact of this irrelevantinformation, particularly in out-of-distribution (OOD) settings, where relianceon contaminated features leads to significant performance degradation. Thisindicates that the models are being misled by non-diagnostic information. Wefurther delve into the reasons PFMs extract such institution-specificinformation and validate our findings. Finally, we propose a simple yeteffective solution to mitigate the influence of irrelevant information. Thisstudy is not intended to criticize existing PFMs, as they have indeed greatlyadvanced the development of computational pathology. our aim is to inspirefuture research to focus on innovative training strategies, rather than relyingexclusively on scaling laws, to realize more generalized PFMs.</description>
      <author>example@mail.com (Weiping Lin, Shen Liu, Runchen Zhu, Liansheng Wang)</author>
      <guid isPermaLink="false">2502.16889v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Subsampling Graphs with GNN Performance Guarantees</title>
      <link>http://arxiv.org/abs/2502.16703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的图数据子采样方法，利用树移动距离减少图的数量和大小，该方法在理论上保证了训练后的模型损失相比完整数据集的增加是有限制的。&lt;h4&gt;背景&lt;/h4&gt;如何从大规模图数据集中有效地选择一个子样本进行GNN训练，使得其性能与整个数据集上的训练效果相当是一个重要的研究问题。较小的数据集可以减少标注成本、存储需求和所需的计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的基于理论支持的图数据子采样方法，能够在不牺牲模型性能的情况下减小数据集规模。&lt;h4&gt;方法&lt;/h4&gt;利用树移动距离作为度量标准来选择一个有效的子样本。该方法既对模型架构无特定要求也无需完全标注训练集即可实施。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示所提出的方法在多个数据集上优于现有的子采样技术，同时证明了其理论上的性能保证。&lt;h4&gt;结论&lt;/h4&gt;通过这种方法可以在早期的数据预处理阶段进行有效的图数据子采样，从而减少存储、标注和训练所需的资源。该方法具有广泛的适用性，因为它对模型架构和标签信息没有强依赖性。&lt;h4&gt;翻译&lt;/h4&gt;如何从大规模图数据集中有效地选择一个子样本进行GNN训练，使得其性能与整个数据集上的训练效果相当是一个重要的研究问题。较小的数据集可以减少标注成本、存储需求和所需的计算资源。现有技术的不足之处在于它们可能严重降低模型性能或需要大量实验来验证质量，从而消除了子采样的好处。因此，作者提出了一种新的基于理论支持的方法：利用树移动距离进行图数据子采样，并且证明了在子样本上训练GNN会导致损失增加是有限制的。这种方法具有广泛的适用性，因为它对模型架构和标签信息没有强依赖性。实验结果验证了该方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How can we subsample graph data so that a graph neural network (GNN) trainedon the subsample achieves performance comparable to training on the fulldataset? This question is of fundamental interest, as smaller datasets reducelabeling costs, storage requirements, and computational resources needed fortraining. Selecting an effective subset is challenging: a poorly chosensubsample can severely degrade model performance, and empirically testingmultiple subsets for quality obviates the benefits of subsampling. Therefore,it is critical that subsampling comes with guarantees on model performance. Inthis work, we introduce new subsampling methods for graph datasets thatleverage the Tree Mover's Distance to reduce both the number of graphs and thesize of individual graphs. To our knowledge, our approach is the first that issupported by rigorous theoretical guarantees: we prove that training a GNN onthe subsampled data results in a bounded increase in loss compared to trainingon the full dataset. Unlike existing methods, our approach is bothmodel-agnostic, requiring minimal assumptions about the GNN architecture, andlabel-agnostic, eliminating the need to label the full training set. Thisenables subsampling early in the model development pipeline (before dataannotation, model selection, and hyperparameter tuning) reducing costs andresources needed for storage, labeling, and training. We validate ourtheoretical results with experiments showing that our approach outperformsexisting subsampling methods across multiple datasets.</description>
      <author>example@mail.com (Mika Sarkin Jain, Stefanie Jegelka, Ishani Karmarkar, Luana Ruiz, Ellen Vitercik)</author>
      <guid isPermaLink="false">2502.16703v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.16198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Communications Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;6G网络旨在实现全球覆盖、海量连接和超严格的要求。为了实现这些目标，空间-空中-地面综合网络（SAGIN）和语义通信（SemCom）是必不可少的组成部分，但它们在资源调配方面带来了相当大的复杂性。&lt;h4&gt;背景&lt;/h4&gt;当前的研究提出了利用大型语言模型（LLMs）来解决上述问题的一种可行方法。尽管最近在网络调度中使用LLMs已经引起了关注，但现有的解决方案并没有充分解决LLMs幻觉或适应网络动态的问题。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种名为自主强化协调（ARC）的框架，该框架针对具有语义通信功能的空间-空中-地面综合网络设计。&lt;h4&gt;方法&lt;/h4&gt;ARC框架利用基于大型语言模型增强检索生成器(RAG)来监控服务、用户和资源，并处理收集到的数据。同时，分层行动规划器(HAP)负责资源调度工作。ARC通过两个层次来分解资源调配任务：高层使用LLMs进行计划，低层由强化学习(Reinforcement Learning, RL)代理进行决策。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs利用链式思维(CoT)推理进行少样本学习，并通过对比学习增强能力；RL代理则采用重放缓冲区管理来实现持续学习。因此，这种方法可以达到高效、准确和适应性。&lt;h4&gt;结论&lt;/h4&gt;论文通过模拟展示了ARC的有效性，并提供了一个关于如何进一步改进和完善ARC的深入讨论，包括未来的潜在研究方向。&lt;h4&gt;翻译&lt;/h4&gt;6G网络追求全球覆盖、海量连接以及超高标准的要求。空间-空中-地面综合网络(SAGIN)与语义通信技术为实现上述目标是不可或缺的技术手段，然而它们带来了资源协调上的巨大挑战。借鉴机器人领域的研究成果，本文提出了一种运用大型语言模型(如LLMs)作为解决方案的思路来应对这种复杂性，并设计了一个名为自主强化协调(ARC)的新框架以支持SAGIN内的语义通信系统。该框架通过分层架构实现对网络资源的有效管理与优化，在高层采用LLMs进行策略规划，低层使用RL代理作出具体决策；并且针对LLMs易产生的“幻觉”问题及适应网络动态的需求提出了改进方案。实验结果表明该方法具备高效的性能、准确的预测能力和良好的适应性，并探讨了未来可能的研究方向以进一步完善ARC框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 6G networks aim to achieve global coverage, massive connectivity, andultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) andSemantic Communication (SemCom) are essential for realizing these goals, yetthey introduce considerable complexity in resource orchestration. Drawinginspiration from research in robotics, a viable solution to manage thiscomplexity is the application of Large Language Models (LLMs). Although the useof LLMs in network orchestration has recently gained attention, existingsolutions have not sufficiently addressed LLM hallucinations or theiradaptation to network dynamics. To address this gap, this paper proposes aframework called Autonomous Reinforcement Coordination (ARC) for aSemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-AugmentedGenerator (RAG) monitors services, users, and resources and processes thecollected data, while a Hierarchical Action Planner (HAP) orchestratesresources. ARC decomposes orchestration into two tiers, utilizing LLMs forhigh-level planning and Reinforcement Learning (RL) agents for low-leveldecision-making, in alignment with the Mixture of Experts (MoE) concept. TheLLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empoweredby contrastive learning, while the RL agents employ replay buffer managementfor continual learning, thereby achieving efficiency, accuracy, andadaptability. Simulations are provided to demonstrate the effectiveness of ARC,along with a comprehensive discussion on potential future research directionsto enhance and upgrade ARC.</description>
      <author>example@mail.com (Masoud Shokrnezhad, Tarik Taleb)</author>
      <guid isPermaLink="false">2502.16198v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives</title>
      <link>http://arxiv.org/abs/2502.16841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;确保医疗保健中的人工智能（AI）公平性需要能够跨越所有人口群体做出无偏见决策的系统，这要求技术革新与伦理原则相结合。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)通过自我监督学习训练于大规模数据集上，可以在各种医学影像任务中高效地进行适应，同时减少对标注数据的依赖。然而，在不同的人口群体间实现一致性能面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;回顾表明，有效的偏见缓解需要在开发过程的所有阶段采取系统性干预措施，不仅包括模型层面的偏见缓解方法，还需要从数据文档到部署协议的一体化介入。&lt;h4&gt;方法&lt;/h4&gt;本文分析强调了公平基础模型(FMs)在整个开发管道中的综合干预的需求，并展示了如何通过结合系统性的偏见缓解和政策参与来有效地解决技术及机构障碍以实现医疗保健中公正的AI。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，为了推动面向服务不足人群和地区（尤其是那些基础设施有限、计算资源匮乏的地方）的先进医疗技术民主化，公平基础模型(FMs)的发展是至关重要的一步。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种综合框架来推进当前知识，展示了系统性偏见缓解结合政策参与如何有效解决技术及机构障碍，以实现医疗保健中公正的人工智能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring equitable Artificial Intelligence (AI) in healthcare demands systemsthat make unbiased decisions across all demographic groups, bridging technicalinnovation with ethical principles. Foundation Models (FMs), trained on vastdatasets through self-supervised learning, enable efficient adaptation acrossmedical imaging tasks while reducing dependency on labeled data. These modelsdemonstrate potential for enhancing fairness, though significant challengesremain in achieving consistent performance across demographic groups. Ourreview indicates that effective bias mitigation in FMs requires systematicinterventions throughout all stages of development. While previous approachesfocused primarily on model-level bias mitigation, our analysis reveals thatfairness in FMs requires integrated interventions throughout the developmentpipeline, from data documentation to deployment protocols. This comprehensiveframework advances current knowledge by demonstrating how systematic biasmitigation, combined with policy engagement, can effectively address bothtechnical and institutional barriers to equitable AI in healthcare. Thedevelopment of equitable FMs represents a critical step toward democratizingadvanced healthcare technologies, particularly for underserved populations andregions with limited medical infrastructure and computational resources.</description>
      <author>example@mail.com (Dilermando Queiroz, Anderson Carlos, André Anjos, Lilian Berton)</author>
      <guid isPermaLink="false">2502.16841v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications</title>
      <link>http://arxiv.org/abs/2502.17066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为DUNIA的方法，该方法通过图像与全波形LiDAR数据之间的跨模态对齐来学习像素级别的嵌入。这种方法能够直接应用于各种环境监测任务，并在零样本设置下展示了其有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的自我监督多模式学习方法为地球观测应用生成的是粗糙的补丁大小的嵌入，这限制了它们的有效性和与其他模式（如LiDAR）的集成能力。&lt;h4&gt;目的&lt;/h4&gt;为了弥补现有方法的不足，提出了一种新的跨模态对齐方法来学习像素级别的嵌入。&lt;h4&gt;方法&lt;/h4&gt;通过对比训练方式，该模型学会了在零样本设置下应用于各种环境监测任务的像素级别嵌入。具体来说，该研究使用图像和全波形LiDAR数据之间的跨模态对齐进行像素级嵌入的学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在七项环境监测任务中（包括冠层高度测绘、分层冠层覆盖率等），这些嵌入及其零样本分类器通常优于专门的监督模型，即使在低数据量环境下也是如此。此外，在微调设置下，DUNIA展示了强大的低样本能力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新的方法来解决地球观测任务中像素级别嵌入的学习问题，并展示了其优越的表现和潜力。&lt;h4&gt;翻译&lt;/h4&gt;大量的努力已经被投入到自监督多模态学习为地球观察应用进行适应。然而，现有的方法产生的是粗糙的补丁大小的嵌入，这限制了它们的有效性和与其他模式（如LiDAR）的集成能力。为了弥补这一差距，我们提出了DUNIA——一种通过图像和全波形LiDAR数据之间的跨模态对齐来学习像素级别的嵌入的方法。由于模型以对比方式训练，因此这些嵌入可以直接在各种环境监测任务中零样本设置下应用。在我们的实验中，我们展示了这些嵌入对于七项此类任务的有效性（包括冠层高度测绘、分层冠层覆盖率等）。结果表明，这些嵌入与零样本分类器经常优于专门的监督模型，在低数据量环境下也是如此。在微调设置下，我们在五项任务中的表现接近或超过最新技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant efforts have been directed towards adapting self-supervisedmultimodal learning for Earth observation applications. However, existingmethods produce coarse patch-sized embeddings, limiting their effectiveness andintegration with other modalities like LiDAR. To close this gap, we presentDUNIA, an approach to learn pixel-sized embeddings through cross-modalalignment between images and full-waveform LiDAR data. As the model is trainedin a contrastive manner, the embeddings can be directly leveraged in thecontext of a variety of environmental monitoring tasks in a zero-shot setting.In our experiments, we demonstrate the effectiveness of the embeddings forseven such tasks (canopy height mapping, fractional canopy cover, land covermapping, tree species identification, plant area index, crop typeclassification, and per-pixel waveform-based vertical structure mapping). Theresults show that the embeddings, along with zero-shot classifiers, oftenoutperform specialized supervised models, even in low data regimes. In thefine-tuning setting, we show strong low-shot capabilities with performancesnear or better than state-of-the-art on five out of six tasks.</description>
      <author>example@mail.com (Ibrahim Fayad, Max Zimmer, Martin Schwartz, Philippe Ciais, Fabian Gieseke, Gabriel Belouze, Sarah Brood, Aurelien De Truchis, Alexandre d'Aspremont)</author>
      <guid isPermaLink="false">2502.17066v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation</title>
      <link>http://arxiv.org/abs/2502.16907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为MambaFlow的新型场景流估算网络，该网络利用基于Mamba的状态空间模型（SSM）的解码器来解决现有的点云帧间3D运动预测方法在时空建模和特征丢失方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;场景流估计是自动驾驶领域的一个重要研究方向，旨在从连续的点云帧中预测3D运动。现有方法面临时空建模不足以及体素化过程中细粒度特征损失的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于Mamba的状态空间模型（SSM）解码器的方法，以解决现有场景流估计方法中的问题，并提升模型在不同场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了名为MambaFlow的新型场景流估算网络。该网络利用一个设计良好的主干网，通过高效的基于Mamba的解码器来指导体素特征的全局注意力建模，学习体素到点的模式，并将共享体素表示去体素化为点级别的特性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了用于增强模型泛化的场景自适应损失函数。在Argoverse 2基准上的大量实验表明，MambaFlow实现了现有的实时推理速度和最先进的性能，能够准确估计现实世界的城市场景中的流。&lt;h4&gt;结论&lt;/h4&gt;MambaFlow展示了其解决现有方法问题的能力，并且证明了使用基于Mamba的解码器进行全局注意力建模的有效性。此外，提出的自适应损失函数进一步增强了模型在不同场景下的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;场景流估计旨在从连续的点云帧中预测3D运动，在自动驾驶领域备受关注。现有方法面临时空建模不足和体素化过程中细粒度特征丢失的问题。然而，Mamba的成功展示了全局建模和线性复杂性的可能性。本文提出了一种基于Mamba的状态空间模型（SSM）解码器的新型场景流估计网络——MambaFlow，它可以利用设计良好的主干网实现时空特征的深度交互耦合，并且提出了一个全新的场景自适应损失函数来提升泛化能力。在Argoverse 2基准上的大量实验表明，MambaFlow实现了现有方法中的实时推理速度和最先进的性能，在现实世界的城市环境中能够准确估计流。代码可以从 https://github.com/SCNU-RISLAB/MambaFlow 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene flow estimation aims to predict 3D motion from consecutive point cloudframes, which is of great interest in autonomous driving field. Existingmethods face challenges such as insufficient spatio-temporal modeling andinherent loss of fine-grained feature during voxelization. However, the successof Mamba, a representative state space model (SSM) that enables global modelingwith linear complexity, provides a promising solution. In this paper, wepropose MambaFlow, a novel scene flow estimation network with a mamba-baseddecoder. It enables deep interaction and coupling of spatio-temporal featuresusing a well-designed backbone. Innovatively, we steer the global attentionmodeling of voxel-based features with point offset information using anefficient Mamba-based decoder, learning voxel-to-point patterns that are usedto devoxelize shared voxel representations into point-wise features. To furtherenhance the model's generalization capabilities across diverse scenarios, wepropose a novel scene-adaptive loss function that automatically adapts todifferent motion patterns.Extensive experiments on the Argoverse 2 benchmarkdemonstrate that MambaFlow achieves state-of-the-art performance with real-timeinference speed among existing works, enabling accurate flow estimation inreal-world urban scenarios. The code is available athttps://github.com/SCNU-RISLAB/MambaFlow.</description>
      <author>example@mail.com (Jiehao Luo, Jintao Cheng, Xiaoyu Tang, Qingwen Zhang, Bohuan Xue, Rui Fan)</author>
      <guid isPermaLink="false">2502.16907v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>UniDyG: A Unified and Effective Representation Learning Approach for Large Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2502.16431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '动态图在连续时间（CTDGs）和离散时间（DTDGs）下被定义，分别展现出快速局部变化和逐渐全局更新的特点。现有的表示学习研究主要针对其中一种类型的动态图进行，并且通常只关注时间域内的局部动态传播，难以准确捕捉与每种时间粒度相关的结构演变。', '目的': '为了更好地建模这两种类型的动态图，并提升模型的鲁棒性和有效性，提出了一种统一有效的表示学习方法UniDyG。', '方法': '该研究首先提出了一个新颖的傅立叶图注意力（FGAT）机制，可以基于最近邻居和复数选择性聚合来同时建模局部和全局结构相关性，并在理论上确保动态图的时间一致性。通过设计能量门控单元增强FGAT对抗时间噪声的能力，并利用频率增强线性函数进行节点级的动态更新。', '主要发现': '实验表明，所提出的UniDyG方法相较于16个基准模型，在9种动态图上平均改进了14.4%。', '结论': '这项工作提出了一种新的表示学习框架，能够有效应对CTDGs和DTDGs中的挑战，并通过实证研究证明其优越性。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graphs are formulated in continuous-time or discrete-time dynamicgraphs. They differ in temporal granularity: Continuous-Time Dynamic Graphs(CTDGs) exhibit rapid, localized changes, while Discrete-Time Dynamic Graphs(DTDGs) show gradual, global updates. This difference leads to isolateddevelopments in representation learning for each type. To advancerepresentation learning, recent research attempts to design a unified modelcapable of handling both CTDGs and DTDGs. However, it typically focuses onlocal dynamic propagation for temporal structure learning in the time domain,failing to accurately capture the structural evolution associated with eachtemporal granularity. In addition, existing works-whether specific orunified-often overlook the issue of temporal noise, compromising the modelrobustness and effectiveness. To better model both types of dynamic graphs, wepropose UniDyG, a unified and effective representation learning approach, whichscales to large dynamic graphs. We first propose a novel Fourier GraphAttention (FGAT) mechanism that can model local and global structuralcorrelations based on recent neighbors and complex-number selectiveaggregation, while theoretically ensuring consistent representations of dynamicgraphs over time. Based on approximation theory, we demonstrate that FGAT iswell-suited to capture the underlying structures in CTDGs and DTDGs. We furtherenhance FGAT to resist temporal noise by designing an energy-gated unit, whichadaptively filters out high-frequency noise according to the energy. Last, weleverage our FGAT mechanisms for temporal structure learning and employ thefrequency-enhanced linear function for node-level dynamic updates, facilitatingthe generation of high-quality temporal embeddings. Extensive experiments showthat our UniDyG achieves an average improvement of 14.4% over sixteen baselinesacross nine dynamic graphs.</description>
      <author>example@mail.com (Yuanyuan Xu, Wenjie Zhang, Xuemin Lin, Ying Zhang)</author>
      <guid isPermaLink="false">2502.16431v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.16786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures.Our code is available at  https://github.com/liuting20/SwimVG&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;Visual grounding通过自然语言来定位图像区域，这项任务很大程度上依赖于跨模态对齐。现有大多数方法分别传输视觉和语言知识，并在预训练的单模模型完全微调后简单堆叠视觉-语言变换器进行多模态融合。&lt;h4&gt;背景问题&lt;/h4&gt;当前的方法限制了视觉与语言上下文之间的充分交互，同时带来了显著的计算成本。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种分步式多模态融合和适应框架SwimVG来解决上述问题，并在效率上获得明显优势。&lt;h4&gt;方法介绍&lt;/h4&gt;提出了分步式多模态提示(Swip)和跨模态互动适配器(CIA)，分别用于逐步提升视觉与语言表示的对齐度以及进一步促进多模态融合。这些新的架构以参数高效的方式替换原有的繁琐变换器堆栈，从浅层到深层渐进地融合跨模式特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SwimVG在四个广泛使用的基准数据集上实现了卓越的能力和效率上的显著优势。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新颖的视觉接地方法，通过分步多模态融合和适应框架提高了任务性能并优化了计算资源利用。代码已公开供社区进一步探索和应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：视觉定位旨在通过自然语言来确定图像区域，这很大程度上依赖于跨模态对齐。现有的大多数方法分别传输视觉或语言知识，并在预训练的单模式模型完全微调后简单堆叠视觉-语言变换器进行多模态融合。然而，这些方法不仅限制了视觉和语言上下文之间的充分交互，还带来了显著的计算成本。因此，为了应对这些问题，我们探索了一种分步式的多模态融合和适应框架，即SwimVG。具体来说，SwimVG提出了步骤式多模态提示（Swip）和跨模式互动适配器（CIA），用于视觉定位，在此过程中用简单的变换器堆栈替换原有的跨模式融合方式。Swip可以通过令牌级别的融合逐步提升视觉与语言表示的对齐度。此外，重量级的CIA通过跨模态交互进一步促进多模态融合。Swip和CIA都是参数效率高，并且它们逐渐从浅层到深层地融合了跨模式特征。在四个广泛使用的基准测试中进行的实验结果表明，在效率方面，SwimVG获得了显著的能力和收益。我们的代码可以在https://github.com/liuting20/SwimVG上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to ground an image region through natural language,which heavily relies on cross-modal alignment. Most existing methods transfervisual/linguistic knowledge separately by fully fine-tuning uni-modalpre-trained models, followed by a simple stack of visual-language transformersfor multimodal fusion. However, these approaches not only limit adequateinteraction between visual and linguistic contexts, but also incur significantcomputational costs. Therefore, to address these issues, we explore a step-wisemultimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVGproposes step-wise multimodal prompts (Swip) and cross-modal interactiveadapters (CIA) for visual grounding, replacing the cumbersome transformerstacks for multimodal fusion. Swip can improve {the} alignment between thevision and language representations step by step, in a token-level fusionmanner. In addition, weight-level CIA further promotes multimodal fusion bycross-modal interaction. Swip and CIA are both parameter-efficient paradigms,and they fuse the cross-modal features from shallow to deep layers gradually.Experimental results on four widely-used benchmarks demonstrate that SwimVGachieves remarkable abilities and considerable benefits in terms of efficiency.Our code is available at https://github.com/liuting20/SwimVG.</description>
      <author>example@mail.com (Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong)</author>
      <guid isPermaLink="false">2502.16786v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging recurrence in neural network wavefunctions for large-scale simulations of Heisenberg antiferromagnets: the square lattice</title>
      <link>http://arxiv.org/abs/2502.17144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 13 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种使用递归神经网络（RNN）进行变分蒙特卡洛模拟的方法，用于研究二维量子多体系统的基态性质。通过转移学习技术，能够有效地在大规模系统中应用这种方法而不需要从头开始优化。&lt;h4&gt;背景&lt;/h4&gt;机器学习支持的变分蒙特卡罗方法被用来寻找量子多体系统的基态，特别是在二维情况下和非平凡符号结构的情况下。尽管已经达到了许多最先进的有限大小系统的变分能量，但在热力学极限中的研究较少。&lt;h4&gt;目的&lt;/h4&gt;使用RNN作为变分近似来模拟自旋$rac{1}{2}$系统的基态，并通过迭代重新训练的方法逐步增加系统规模以减少计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;在本工作中，作者专注于二维反铁磁海森堡模型（SLAHM），并利用递归神经网络的特性进行大规模系统的基态模拟。通过延长训练时间来提高结果精度，并将有限大小格点上的数值与文献值对比验证。&lt;h4&gt;主要发现&lt;/h4&gt;实现了系统性地提高模拟结果准确性，同时获得了热力学极限下的精确估计。&lt;h4&gt;结论&lt;/h4&gt;该工作证明了RNN波函数可以用来准确研究量子多体物理在热力学极限中的性质。&lt;h4&gt;翻译&lt;/h4&gt;基于机器学习的变分蒙特卡洛仿真是瞄准量子多体基态的一种有前途的方法，特别是在二维和已知基态具有非平凡符号结构的情况下。尽管这些方法已经达到了许多最先进的有限尺寸系统的变分能量，但很少有人利用这些结果来提取目标状态在热力学极限中的信息。本文中，我们采用递归神经网络（RNN）作为变分近似，并利用其递归性质通过迭代重新训练逐步模拟更大规模的系统，从而减少计算资源的需求。在这项研究中，我们专注于二维反铁磁海森堡模型，在这里可以仔细地验证我们的结果。我们展示了可以通过增加训练时间来有条不紊地提高模拟结果的精度，并且对于有限大小格点上的数值与文献值之间具有很好的一致性。此外，我们利用这些结果提取了热力学极限下基态性质的精确估计。这项工作证明了RNN波函数可以用来准确研究量子多体物理在热力学极限中的特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning-based variational Monte Carlo simulations are a promisingapproach for targeting quantum many body ground states, especially in twodimensions and in cases where the ground state is known to have a non-trivialsign structure. While many state-of-the-art variational energies have beenreached with these methods for finite-size systems, little work has been doneto use these results to extract information about the target state in thethermodynamic limit. In this work, we employ recurrent neural networks (RNNs)as a variational ans\"{a}tze, and leverage their recurrent nature to simulatethe ground states of progressively larger systems through iterative retraining.This transfer learning technique allows us to simulate spin-$\frac{1}{2}$systems on lattices with more than 1,000 spins without beginning optimizationfrom scratch for each system size, thus reducing the demands for computationalresources. In this study, we focus on the square-lattice antiferromagneticHeisenberg model (SLAHM), where it is possible to carefully benchmark ourresults. We show that we are able to systematically improve the accuracy of theresults from our simulations by increasing the training time, and obtainresults for finite-sized lattices that are in good agreement with theliterature values. Furthermore, we use these results to extract accurateestimates of the ground-state properties in the thermodynamic limit. This workdemonstrates that RNN wavefunctions can be used to accurately study quantummany-body physics in the thermodynamic limit.</description>
      <author>example@mail.com (M. Schuyler Moss, Roeland Wiersema, Mohamed Hibat-Allah, Juan Carrasquilla, Roger G. Melko)</author>
      <guid isPermaLink="false">2502.17144v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2502.16826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Noise2Score3D是一个完全无监督的点云去噪框架，它利用贝叶斯统计和图像去噪领域的最新进展来解决干净数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的去噪方法通常需要大量的干净数据进行训练，这在实际应用中是难以获得的。而本研究提出的Noise2Score3D方法直接从带噪声的数据中学习点云分布的梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来处理点云去噪问题，特别是在没有或很少有干净数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;该方法利用Tweedie公式进行一步式推理，避免了现有无监督方法中的迭代过程，并且通过Total Variation for Point Cloud标准估计未知噪声参数，增强了方法的实用性和通用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Noise2Score3D在Chamfer距离和点到网格度量方面优于其他无监督方法，在一些性能指标上甚至可以与有监督的方法相媲美，并且具有很强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新的去噪框架Noise2Score3D，不仅提升了算法效率和性能，还提高了其在实际应用中的适应性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文基于贝叶斯统计学和图像去噪领域的最新进展，提出了一个完全无监督的点云去噪框架Noise2Score3D。该方法直接从带噪声的数据中学习潜在分布的梯度，并通过Tweedie公式执行一步式推理。实验结果表明，在标准基准测试上，这种方法在Chamfer距离和点到网格度量方面优于其他无监督方法，甚至可以与一些有监督的方法相媲美。此外，Noise2Score3D还引入了Total Variation for Point Cloud的标准来估计未知的噪声参数，进一步增强了该方法的灵活性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building on recent advances in Bayesian statistics and image denoising, wepropose Noise2Score3D, a fully unsupervised framework for point cloud denoisingthat addresses the critical challenge of limited availability of clean data.Noise2Score3D learns the gradient of the underlying point cloud distributiondirectly from noisy data, eliminating the need for clean data during training.By leveraging Tweedie's formula, our method performs inference in a singlestep, avoiding the iterative processes used in existing unsupervised methods,thereby improving both performance and efficiency. Experimental resultsdemonstrate that Noise2Score3D achieves state-of-the-art performance onstandard benchmarks, outperforming other unsupervised methods in Chamferdistance and point-to-mesh metrics, and rivaling some supervised approaches.Furthermore, Noise2Score3D demonstrates strong generalization ability beyondtraining datasets. Additionally, we introduce Total Variation for Point Cloud,a criterion that allows for the estimation of unknown noise parameters, whichfurther enhances the method's versatility and real-world utility.</description>
      <author>example@mail.com (Xiangbin Wei)</author>
      <guid isPermaLink="false">2502.16826v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations</title>
      <link>http://arxiv.org/abs/2502.16697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的方法，利用基于生物学信息的异构图表示来提高糖尿病视网膜病变分期预测的可解释性，并在两个数据集上超过了现有的机器学习模型。&lt;h4&gt;背景&lt;/h4&gt;目前最先进的基于神经网络的图像分类器大多不可解释。虽然生物标志物是临床诊断的重要依据，但生物标志物基础分类性能通常不如大型神经网络。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以超越现有机器学习模型的表现并提高糖尿病视网膜病变分期预测的可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用新颖的生物学信息异构图表示来建模视网膜血管段、间隔区以及中央凹无血管区，将糖尿病视网膜病变分期问题转化为图形分类任务，并使用高效的图神经网络解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在两个数据集上优于传统的生物标志物基础分类器、卷积神经网络（CNN）和视觉变换器等基准方法。此外，还提供了前所未有的详细解释来定位关键血管或间隔区并赋予关键特性有意义的人类可解读归因。&lt;h4&gt;结论&lt;/h4&gt;该研究贡献了有助于眼科临床决策支持工具发展的新方法。&lt;h4&gt;翻译&lt;/h4&gt;可解释性对于增强医学诊断中机器学习模型的信任至关重要。然而，大多数基于神经网络的先进图像分类器不可解释。因此，尽管生物标志物基础分类通常表现不如大型神经网络，但临床上仍依赖于已知的生物标志物进行诊断。这项工作提出了一种方法，该方法超越了现有的机器学习模型的表现，并同时提高了糖尿病视网膜病变分期从光学相干断层扫描血管成像（OCTA）图像中预测的可解释性。我们的方法基于一种新颖的生物学信息异构图表示，以人类可理解的方式建模视网膜血管段、间隔区以及中央凹无血管区（FAZ）。这种图形表示使我们能够将糖尿病视网膜病变分期视为一个图形级别的分类任务，并使用高效的图神经网络解决该问题。我们在两个数据集上对我们的方法进行了基准测试，与包括经典生物标志物基础分类器、卷积神经网络（CNN）和视觉变换器在内的现有基线模型相比表现更佳。我们使用生物学信息图提供了前所未有的详细解释，并且在精确定位关键血管或间隔区以及赋予有意义的人类可解读归因方面超越了现有的方法。我们的工作对眼科临床决策支持工具的发展做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpretability is crucial to enhance trust in machine learning models formedical diagnostics. However, most state-of-the-art image classifiers based onneural networks are not interpretable. As a result, clinicians often resort toknown biomarkers for diagnosis, although biomarker-based classificationtypically performs worse than large neural networks. This work proposes amethod that surpasses the performance of established machine learning modelswhile simultaneously improving prediction interpretability for diabeticretinopathy staging from optical coherence tomography angiography (OCTA)images. Our method is based on a novel biology-informed heterogeneous graphrepresentation that models retinal vessel segments, intercapillary areas, andthe foveal avascular zone (FAZ) in a human-interpretable way. This graphrepresentation allows us to frame diabetic retinopathy staging as a graph-levelclassification task, which we solve using an efficient graph neural network. Webenchmark our method against well-established baselines, including classicalbiomarker-based classifiers, convolutional neural networks (CNNs), and visiontransformers. Our model outperforms all baselines on two datasets. Crucially,we use our biology-informed graph to provide explanations of unprecedenteddetail. Our approach surpasses existing methods in precisely localizing andidentifying critical vessels or intercapillary areas. In addition, we giveinformative and human-interpretable attributions to critical characteristics.Our work contributes to the development of clinical decision-support tools inophthalmology.</description>
      <author>example@mail.com (Laurin Lux, Alexander H. Berger, Maria Romeo Tricas, Alaa E. Fayed, Sobha Sivaprasada, Linus Kreitner, Jonas Weidner, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold)</author>
      <guid isPermaLink="false">2502.16697v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Emergence of Multimodal Representation Alignment</title>
      <link>http://arxiv.org/abs/2502.16282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 22 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态学习中不同模态之间表示对齐的形成机制及其与任务性能的关系，通过实验表明其依赖于数据特性。&lt;h4&gt;背景&lt;/h4&gt;多模态表征学习旨在将不同的不可比较模态转换为可比较的表示。以往的研究主要集中在显式对齐上，但最近发现独立训练的大规模单模态模型可以隐式对齐。&lt;h4&gt;目的&lt;/h4&gt;探讨在多模态学习中，不同模态之间如何以及为何会形成隐式的表示对齐；同时探究这种对齐是否可靠地指示任务性能。&lt;h4&gt;方法&lt;/h4&gt;通过全面的实证研究来分析不同数据特性（如模态相似度和信息冗余度）对表示对齐及其与任务表现关系的影响。&lt;h4&gt;主要发现&lt;/h4&gt;表示对齐的出现及其实现任务性能的相关性依赖于关键的数据特征，包括但不限于模态之间的相似性和冗余/独特信息的比例。这些发现表明，对齐不一定总是有益的；其影响取决于特定数据集和任务的情况。&lt;h4&gt;结论&lt;/h4&gt;研究结果有助于实践者根据具体情况确定增加不同模态之间表示对齐是否有利于获得最优性能。&lt;h4&gt;翻译&lt;/h4&gt;多模式表征学习是将不同的不可比较模态转换为可比较表示的过程。之前的研究主要集中在通过明确的学习目标和模型架构来显式地对准这些表示，但最近的一系列工作发现独立训练的规模更大、性能更好的单模态模型可以彼此隐式对齐。这一研究结果引发了关于多模式学习中对齐表示出现的基本问题：（1）何时以及为什么会出现隐式的对齐？（2）对齐是否可靠地指示了任务性能？通过全面的经验调查，我们证明了对齐的产生及其与任务表现的关系依赖于多个关键数据特征。这包括但不限于模态之间的相似性和它们提供的冗余和独特信息的比例等。我们的研究结果表明对齐不一定总是有益的；相反，它对性能的影响根据数据集和任务的不同而变化。这些见解可以帮助实践者确定增加不同模式之间表示对准是否有利或在某些情况下有害于获得最佳性能。代码发布在 https://github.com/MeganTj/multimodal_alignment 上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning is fundamentally about transformingincomparable modalities into comparable representations. While prior researchprimarily focused on explicitly aligning these representations through targetedlearning objectives and model architectures, a recent line of work has foundthat independently trained unimodal models of increasing scale and performancecan become implicitly aligned with each other. These findings raise fundamentalquestions regarding the emergence of aligned representations in multimodallearning. Specifically: (1) when and why does alignment emerge implicitly? and(2) is alignment a reliable indicator of performance? Through a comprehensiveempirical investigation, we demonstrate that both the emergence of alignmentand its relationship with task performance depend on several critical datacharacteristics. These include, but are not necessarily limited to, the degreeof similarity between the modalities and the balance between redundant andunique information they provide for the task. Our findings suggest thatalignment may not be universally beneficial; rather, its impact on performancevaries depending on the dataset and task. These insights can help practitionersdetermine whether increasing alignment between modalities is advantageous or,in some cases, detrimental to achieving optimal performance. Code is releasedat https://github.com/MeganTj/multimodal_alignment.</description>
      <author>example@mail.com (Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, Paul Pu Liang)</author>
      <guid isPermaLink="false">2502.16282v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Category-Selective Neurons in Deep Networks: Comparing Purely Visual and Visual-Language Models</title>
      <link>http://arxiv.org/abs/2502.16456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了人工神经网络（ANNs）中是否存在与人类大脑相似的类别选择性区域，并探讨这些区域在视觉和视觉-语言模型中的差异。&lt;h4&gt;背景&lt;/h4&gt;人类大脑中有专门处理特定类型视觉信息的区域，如面部、身体、场景等。作者探索了人工神经网络中是否存在类似的特性。&lt;h4&gt;目的&lt;/h4&gt;研究不同深度学习模型中的类别选择性神经元及其分布规律，并探讨多模态学习对这些神经元的影响。&lt;h4&gt;方法&lt;/h4&gt;通过向深层网络展示不同类别的图像（如人脸、身体、场景等）并使用统计标准识别类别选择性神经元，模拟功能性定位实验的方法进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet和基于CLIP的模型都包含类别选择性神经元，但CLIP模型中这些神经元的比例更高且分布更均匀。这表明多模态学习增加了这类神经元的数量却减少了它们的选择特异性。&lt;h4&gt;结论&lt;/h4&gt;这项研究表明人工神经网络模仿了生物视觉系统，并揭示了多模态学习如何影响深度网络中的类别选择性表示。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文译文为：人类大脑中负责高级视觉处理的区域，如梭状回面孔区（FFA）、额外体素体区（EBA）、海马旁场景区（PPA）和视觉词形区（VWFA），在人脸识别、身体识别、场景理解和文字阅读等方面起着关键作用。本文探讨了人工神经网络（ANNs）中是否存在类似的类别选择性区域，以及这些区域在网络的各个层面上的变化情况，并且比较了纯视觉模型与视觉-语言模型之间的差异。通过模拟功能定位实验的方法，在深层网络上展示了来自不同类别的图像（包括面部、身体、场景、文字及其随机组合），并使用统计标准来识别类别选择性神经元。研究发现，无论是ResNet还是基于CLIP的结构控制模型，都包含了类别选择性神经元，并且随着层数的增加，这些神经元的比例也在增加，这与高级视觉大脑区域中的选择性一致。然而，相较于ResNet，CLIP显示了更高的比例但更低的选择特异性，在特征图上的分布更均匀，并在不同层之间展现出更强的表现一致性。研究结果表明语言学习增加了类别选择性神经元的数量而减少了它们的特定强度，重新塑造了深度网络中的视觉表示形式。这项研究提供了关于ANNs如何模仿生物视觉系统以及多模态学习影响类别选择性表征的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-selective regions in the human brain, such as the fusiform face area(FFA), extrastriate body area (EBA), parahippocampal place area (PPA), andvisual word form area (VWFA), play a crucial role in high-level visualprocessing. Here, we investigate whether artificial neural networks (ANNs)exhibit similar category-selective neurons and how these neurons vary acrossmodel layers and between purely visual and vision-language models. Inspired byfMRI functional localizer experiments, we presented images from differentcategories (faces, bodies, scenes, words, scrambled scenes, and scrambledwords) to deep networks and identified category-selective neurons usingstatistical criteria. Comparing ResNet and the structurally controlledResNet-based CLIP model, we found that both models contain category-selectiveneurons, with their proportion increasing across layers, mirroring categoryselectivity in higher-level visual brain regions. However, CLIP exhibited ahigher proportion but lower specificity of category-selective neurons comparedto ResNet. Additionally, CLIP's category-selective neurons were more evenlydistributed across feature maps and demonstrated greater representationalconsistency across layers. These findings suggest that language learningincreases the number of category-selective neurons while reducing theirselectivity strength, reshaping visual representations in deep networks. Ourstudy provides insights into how ANNs mirror biological vision and howmultimodal learning influences category-selective representations.</description>
      <author>example@mail.com (Zitong Lu, Yuxin Wang)</author>
      <guid isPermaLink="false">2502.16456v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework</title>
      <link>http://arxiv.org/abs/2502.16023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们介绍了一种新的框架Contrastive Similarity Space Embedding Algorithm (ContraSim)，用于揭示日常金融新闻头条与市场动态之间的全球语义关系。&lt;h4&gt;背景&lt;/h4&gt;金融市场中的新闻报道和股票价格之间存在复杂的相互作用，现有的方法难以有效捕捉这些关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更有效地揭示金融新闻与市场动态之间关联性的算法框架。&lt;h4&gt;方法&lt;/h4&gt;{'Weighted Headline Augmentation': '生成带有语义细粒度相似性评分的增强型金融新闻头条', 'Weighted Self-Supervised Contrastive Learning (WSSCL)': '利用相似性度量创建细化加权嵌入空间，使语义相似的新闻头条聚类在一起'}&lt;h4&gt;主要发现&lt;/h4&gt;{'提高分类精度': '将ContraSim特性融入金融预测任务后，从《华尔街日报》新闻中提高了7%的分类准确率。', '市场动态捕捉': '通过信息密度分析，发现了由ContraSim构造出的相似空间内在地聚类了具有相同市场移动方向的日子，表明该算法能够独立于地面真实标签捕获市场的动态变化。', '参考过去事件': '识别历史新闻日，这些日子与当前日期头条内容非常接近，为预测市场趋势提供可操作见解'}&lt;h4&gt;结论&lt;/h4&gt;ContraSim不仅提高了金融预测任务中的分类精度，还为金融市场分析师提供了基于类似过去的事件进行市场趋势预测的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the Contrastive Similarity Space Embedding Algorithm(ContraSim), a novel framework for uncovering the global semantic relationshipsbetween daily financial headlines and market movements. ContraSim operates intwo key stages: (I) Weighted Headline Augmentation, which generates augmentedfinancial headlines along with a semantic fine-grained similarity score, and(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended versionof classical self-supervised contrastive learning that uses the similaritymetric to create a refined weighted embedding space. This embedding spaceclusters semantically similar headlines together, facilitating deeper marketinsights. Empirical results demonstrate that integrating ContraSim featuresinto financial forecasting tasks improves classification accuracy from WSJheadlines by 7%. Moreover, leveraging an information density analysis, we findthat the similarity spaces constructed by ContraSim intrinsically cluster dayswith homogeneous market movement directions, indicating that ContraSim capturesmarket dynamics independent of ground truth labels. Additionally, ContraSimenables the identification of historical news days that closely resemble theheadlines of the current day, providing analysts with actionable insights topredict market trends by referencing analogous past events.</description>
      <author>example@mail.com (Nicholas Vinden, Raeid Saqur, Zining Zhu, Frank Rudzicz)</author>
      <guid isPermaLink="false">2502.16023v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models</title>
      <link>http://arxiv.org/abs/2502.16849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了无监督预训练和迁移学习如何影响高维有监督学习的样本复杂度，特别是在标签数据有限的情况下。研究结果显示，在特定条件下，这些技术能显著减少所需的训练样本数量。&lt;h4&gt;背景&lt;/h4&gt;在深度学习领域中，无监督预训练和迁移学习通常被用来初始化神经网络模型以应对标签数据稀缺的问题。&lt;h4&gt;目的&lt;/h4&gt;目的是分析无监督预训练和转移学习如何影响单层神经网络通过在线随机梯度下降进行训练时的样本复杂性。&lt;h4&gt;方法&lt;/h4&gt;研究考虑了使用在线随机梯度下降法来训练单层神经网络的情况，并探讨了无监督预训练与迁移学习在这种情况下的效果。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在较为宽松的一般假设条件下，无监督预训练和转移学习能够通过多项式因素减少样本复杂性。此外，研究还发现了某些情况下无监督预训练可以提供指数级的改善，相对于随机初始化而言。&lt;h4&gt;结论&lt;/h4&gt;该论文提供了无监督预训练和迁移学习对深度神经网络模型在高维数据上的影响的重要见解，并强调了其在样本效率方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无监督预训练和迁移学习是常见的技术手段，用于初始化神经网络的训练算法，特别是在标签数据有限的情况下。本文研究了无监督预训练和迁移学习如何影响高维有监督学习的样本复杂度。具体来说，我们考虑通过在线随机梯度下降法来训练单层神经网络的问题，并建立在非常一般性的假设下，预训练和转移学习（在概念转变的情况下）可以通过多项式因素减少样本复杂性。此外，研究还揭示了某些情况，在无监督预训练方面可以提供相对于随机初始化而言的指数级改进效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised pre-training and transfer learning are commonly used techniquesto initialize training algorithms for neural networks, particularly in settingswith limited labeled data. In this paper, we study the effects of unsupervisedpre-training and transfer learning on the sample complexity of high-dimensionalsupervised learning. Specifically, we consider the problem of training asingle-layer neural network via online stochastic gradient descent. Weestablish that pre-training and transfer learning (under concept shift) reducesample complexity by polynomial factors (in the dimension) under very generalassumptions. We also uncover some surprising settings where pre-training grantsexponential improvement over random initialization in terms of samplecomplexity.</description>
      <author>example@mail.com (Taj Jones-McCormick, Aukosh Jagannath, Subhabrata Sen)</author>
      <guid isPermaLink="false">2502.16849v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Transformers: Architectures, Theories and Applications</title>
      <link>http://arxiv.org/abs/2502.16533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;Graph Transformers (GTs) 在处理图结构时展示了强大的能力，通过解决图神经网络（GNNs）固有的问题如过度平滑和过度挤压。&lt;h4&gt;背景&lt;/h4&gt;近年来，针对 Graph Transformers 的研究提出了多种架构、增强了可解释性，并在实际应用中取得了进展。&lt;h4&gt;目的&lt;/h4&gt;综述 Graph Transformers 在其架构设计、理论基础及具体应用场景等方面的发展状况。&lt;h4&gt;方法&lt;/h4&gt;根据处理结构信息的策略对 Graph Transformers 架构进行分类，包括图标记化、位置编码、基于结构的注意力机制和模型集成等。&lt;h4&gt;主要发现&lt;/h4&gt;探讨了不同架构下 Graph Transformers 的表达能力，并将其与其它先进的图学习算法进行了对比。&lt;h4&gt;应用领域&lt;/h4&gt;总结了 Graph Transformers 在分子数据、蛋白质信息、自然语言处理、视觉交通、脑科学及材料研究等多个领域的实际应用案例。&lt;h4&gt;未来挑战和方向&lt;/h4&gt;讨论了当前 Graph Transformers 面临的挑战以及潜在的研究发展方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have demonstrated a strong capability in modelinggraph structures by addressing the intrinsic limitations of graph neuralnetworks (GNNs), such as over-smoothing and over-squashing. Recent studies haveproposed diverse architectures, enhanced explainability, and practicalapplications for Graph Transformers. In light of these rapid developments, weconduct a comprehensive review of Graph Transformers, covering aspects such astheir architectures, theoretical foundations, and applications within thissurvey. We categorize the architecture of Graph Transformers according to theirstrategies for processing structural information, including graph tokenization,positional encoding, structure-aware attention and model ensemble. Furthermore,from the theoretical perspective, we examine the expressivity of GraphTransformers in various discussed architectures and contrast them with otheradvanced graph learning algorithms to discover the connections. Furthermore, weprovide a summary of the practical applications where Graph Transformers havebeen utilized, such as molecule, protein, language, vision traffic, brain andmaterial data. At the end of this survey, we will discuss the currentchallenges and prospective directions in Graph Transformers for potentialfuture research.</description>
      <author>example@mail.com (Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong)</author>
      <guid isPermaLink="false">2502.16533v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PLS-based approach for fair representation learning</title>
      <link>http://arxiv.org/abs/2502.16263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了公平表征学习的问题，提出了引入公平性的偏最小二乘法（PLS）组件的方法。&lt;h4&gt;背景&lt;/h4&gt;偏最小二乘法在统计学中被广泛应用于通过提供预测专用的表示来高效地降低数据维度。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，在构建PLS成分时纳入公平性约束，以实现在线性和非线性情况下的特征构造。&lt;h4&gt;方法&lt;/h4&gt;该新算法利用核嵌入技术，在PLS组件构建过程中加入公平性的考量，并在不同数据集上进行了效率评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法相比于标准的公平主成分分析（PCA）方法具有明显的优势。&lt;h4&gt;结论&lt;/h4&gt;通过引入公平性约束到偏最小二乘法中，可以更有效地进行表征学习和预测。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit the problem of fair representation learning by proposing FairPartial Least Squares (PLS) components. PLS is widely used in statistics toefficiently reduce the dimension of the data by providing representationtailored for the prediction. We propose a novel method to incorporate fairnessconstraints in the construction of PLS components. This new algorithm providesa feasible way to construct such features both in the linear and the non linearcase using kernel embeddings. The efficiency of our method is evaluated ondifferent datasets, and we prove its superiority with respect to standard fairPCA method.</description>
      <author>example@mail.com (Elena M. De-Diego, Adrián Perez-Suay, Paula Gordaliza, Jean-Michel Loubes)</author>
      <guid isPermaLink="false">2502.16263v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Industrial Anomalies Synthesis</title>
      <link>http://arxiv.org/abs/2502.16412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面回顾了异常合成的方法论，提供了第一个工业异常合成（IAS）分类体系。&lt;h4&gt;背景&lt;/h4&gt;现有综述关注的技术范围有限，忽视了跨模态数据和视觉语言模型在异常合成中的作用。&lt;h4&gt;目的&lt;/h4&gt;提供一个统一的、涵盖约40种代表方法的综合回顾，并提出首个细粒度框架来反映方法学进展及其实际应用意义。&lt;h4&gt;方法&lt;/h4&gt;将研究对象分为手工设计、基于分布假设、基于生成模型（GM）和基于视觉语言模型（VLM）四大类，详细介绍了每种类别的代表性技术。&lt;h4&gt;主要发现&lt;/h4&gt;首次提出工业异常合成的分类体系，并深入探讨跨模态合成和大规模视觉语言模型的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;为未来的研究提供指导路径，强调了多模态学习在推进IAS方面的优势、挑战及前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到这篇论文全面回顾了异常生成的方法论。现有的综述通常只关注有限的技术，并没有涵盖整个领域的全貌或理解方法间的相互联系。与这些工作不同的是，我们的研究提供了一种统一的视角，涵盖了约40个代表性方法，分为手工设计、基于分布假设、基于生成模型和基于视觉语言模型四类合成方法。此外，我们提出了首个工业异常合成（IAS）分类体系。之前的文献缺乏正式的分类或者使用简化的分类方式，这阻碍了结构化比较和趋势识别的工作。我们的分类体系提供了一个细粒度框架来反映方法学进步及其实际应用意义，并为未来的研究奠定基础。除此之外，我们还探讨了跨模态合成以及大规模视觉语言模型的应用。以往的综述忽视了多模态数据和视觉语言模型在异常生成中的作用，限制了对其优势的理解。我们的调查分析了它们的融合、益处、挑战及前景，提供了一条通过多模态学习提升IAS的道路。更多资源可访问：https://github.com/M-3LAB/awesome-anomaly-synthesis。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper comprehensively reviews anomaly synthesis methodologies. Existingsurveys focus on limited techniques, missing an overall field view andunderstanding method interconnections. In contrast, our study offers a unifiedreview, covering about 40 representative methods across Hand-crafted,Distribution-hypothesis-based, Generative models (GM)-based, andVision-language models (VLM)-based synthesis. We introduce the first industrialanomaly synthesis (IAS) taxonomy. Prior works lack formal classification or usesimplistic taxonomies, hampering structured comparisons and trendidentification. Our taxonomy provides a fine-grained framework reflectingmethodological progress and practical implications, grounding future research.Furthermore, we explore cross-modality synthesis and large-scale VLM. Previoussurveys overlooked multimodal data and VLM in anomaly synthesis, limitinginsights into their advantages. Our survey analyzes their integration,benefits, challenges, and prospects, offering a roadmap to boost IAS withmultimodal learning. More resources are available athttps://github.com/M-3LAB/awesome-anomaly-synthesis.</description>
      <author>example@mail.com (Xichen Xu, Yanshu Wang, Yawen Huang, Jiaqi Liu, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu)</author>
      <guid isPermaLink="false">2502.16412v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking</title>
      <link>http://arxiv.org/abs/2502.16514v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型在生成长文本时容易出现细微的事实错误，尤其是在医学等专业领域。现有的事实核查方法面临难以理解复杂多跳关系和高资源消耗的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛应用，但它们常常会产生轻微的事实性错误，特别是在需要高度准确性的专业领域如医学中。现有基于文档查证的方法在处理长篇文本的复杂多跳关系时存在困难，并且大多数专业化方法依赖于成对比较，导致计算和资源成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的事实核查框架GraphCheck，利用提取的知识图来改进文本表示，以解决当前事实核查方法存在的问题。&lt;h4&gt;方法&lt;/h4&gt;GraphCheck使用知识图并通过图神经网络进一步处理这些知识图作为软提示，使大型语言模型能够更有效地整合结构化知识。此框架特别擅长捕捉多跳推理链，并通过一次推断调用即可完成精确高效的事实核查。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在七个涵盖一般和医学领域的基准测试上，GraphCheck比基线模型总体提高了6.1%的性能。值得注意的是，该方法不仅超过了现有的专业化事实核查工具，还与最先进语言模型DeepSeek-V3和OpenAI-o1达到了相当的表现，同时参数显著减少。&lt;h4&gt;结论&lt;/h4&gt;GraphCheck框架通过引入知识图来改进大型语言模型的事实核查能力，在提高准确性的同时大幅减少了资源需求。这为在专业领域内进行有效事实核查提供了一种新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要文本的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are widely used, but they often generate subtlefactual errors, especially in long-form text. These errors are fatal in somespecialized domains such as medicine. Existing fact-checking with groundingdocuments methods face two main challenges: (1) they struggle to understandcomplex multihop relations in long documents, often overlooking subtle factualerrors; (2) most specialized methods rely on pairwise comparisons, requiringmultiple model calls, leading to high resource and computational costs. Toaddress these challenges, we propose \textbf{\textit{GraphCheck}}, afact-checking framework that uses extracted knowledge graphs to enhance textrepresentation. Graph Neural Networks further process these graphs as a softprompt, enabling LLMs to incorporate structured knowledge more effectively.Enhanced with graph-based reasoning, GraphCheck captures multihop reasoningchains which are often overlooked by existing methods, enabling precise andefficient fact-checking in a single inference call. Experimental results onseven benchmarks spanning both general and medical domains demonstrate a 6.1\%overall improvement over baseline models. Notably, GraphCheck outperformsexisting specialized fact-checkers and achieves comparable performance withstate-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantlyfewer parameters.</description>
      <author>example@mail.com (Yingjian Chen, Haoran Liu, Yinhong Liu, Rui Yang, Han Yuan, Yanran Fu, Pengyuan Zhou, Qingyu Chen, James Caverlee, Irene Li)</author>
      <guid isPermaLink="false">2502.16514v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based Asynchronous Aggregation</title>
      <link>http://arxiv.org/abs/2502.16454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Mamba异步传播网络（MAPN）的新模型，旨在解决图神经网络在处理大规模稀疏异构图时面临的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已经成为各种图形相关任务的前沿技术，并且在处理异构图(HetGs)时特别突出。然而，GNN面临着过度压缩、过度平滑和传统消息传递神经网络训练大尺度稀疏图效果不佳等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决深度神经网络在大规模异构图形上存在的问题。&lt;h4&gt;方法&lt;/h4&gt;MAPN包括两个主要组件：节点序列生成和语义信息聚合。首先，基于元路径通过随机游走生成节点序列，并使用空间状态模型提取不同距离节点的关键信息。随后，它以异步方式汇总多跳和多层的语义信息，有效地保持独特节点特征并缓解深度网络退化问题。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明了MAPN在各种下游任务中的图嵌入的有效性，特别是在大尺度稀疏异构图形中具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;通过提出创新性的MAPN模型，论文提供了一种有效处理大规模稀疏异构图的新途径。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）已成为各种与图相关的任务的前沿技术，并且在异构图（HetGs）中尤为突出。然而，这种范式面临一些问题：首先，难以充分利用长距离信息，即过度压缩；其次，过多的消息传递层会产生无法区分的表示形式，称为过度平滑；最后，传统的MPNN在大规模稀疏图上训练效果不佳。为了解决这些挑战，在大型异构图形中使用深度神经网络的问题，本文介绍了基于Mamba的异步传播网络（MAPN），该模型增强了异构稀疏图的表现力。MAPN包括两个主要组件：节点序列生成和语义信息聚合。节点序列最初是根据元路径通过随机游走生成的，这些元路径构成了空间状态模型的基础，该模型提取了不同距离的节点的关键信息。然后，它以异步方式汇总多跳和多层的信息，有效地保持独特的节点特征，并缓解与深度网络退化相关的问题。在各种数据集上的广泛实验表明，在图嵌入的各种下游任务中MAPN的有效性，强调其在大规模稀疏异构图形表示中的显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have become the state of the art for variousgraph-related tasks and are particularly prominent in heterogeneous graphs(HetGs). However, several issues plague this paradigm: first, the difficulty infully utilizing long-range information, known as over-squashing; second, thetendency for excessive message-passing layers to produce indistinguishablerepresentations, referred to as over-smoothing; and finally, the inadequacy ofconventional MPNNs to train effectively on large sparse graphs. To addressthese challenges in deep neural networks for large-scale heterogeneous graphs,this paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),which enhances the representation of heterogeneous sparse graphs. MAPN consistsof two primary components: node sequence generation and semantic informationaggregation. Node sequences are initially generated based on meta-paths throughrandom walks, which serve as the foundation for a spatial state model thatextracts essential information from nodes at various distances. It thenasynchronously aggregates semantic information across multiple hops and layers,effectively preserving unique node characteristics and mitigating issuesrelated to deep network degradation. Extensive experiments across diversedatasets demonstrate the effectiveness of MAPN in graph embeddings for variousdownstream tasks underscoring its substantial benefits for graph representationin large sparse heterogeneous graphs.</description>
      <author>example@mail.com (Xuqi Mao, Zhenying He, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16454v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Network Tomography with Path-Centric Graph Neural Network</title>
      <link>http://arxiv.org/abs/2502.16430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DeepNT的深度网络拓扑学框架，用于预测路径性能指标，并能推理出部分先验知识下的网络拓扑结构。&lt;h4&gt;背景&lt;/h4&gt;网络拓扑学在网络监控中至关重要，它利用可观察的路径性能度量值来推断未被观测到的度量值。然而现有的方法要么假设完全了解网络拓扑和度量公式（在许多实际情况下这是不现实的），要么依赖于端到端的黑箱模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DeepNT，该框架能够结合数据知识以及适当的归纳偏置来解决网络拓扑学问题。&lt;h4&gt;方法&lt;/h4&gt;引入了以路径为中心的图神经网络，通过推理和聚合构成路径节点序列的嵌入表示来预测性能指标。同时设计了一种学习目标，施加连通性和稀疏性约束在拓扑结构上，并且满足路径性能三角不等式条件。&lt;h4&gt;主要发现&lt;/h4&gt;DeepNT框架在真实世界数据集和合成数据集上的实验表明，在预测性能指标和推理图结构方面超越了最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合部分先验知识以及数据驱动的方法，可以有效地解决网络拓扑学问题，并能够提供更准确的路径性能度量值预测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：网络拓扑学是网络监控中的关键问题，其中可观察的路径性能指标用于推断未被观测到的指标。然而，大多数现有方法要么假设完全了解网络拓扑和度量公式（在许多实际情况下这是不现实的），要么依赖于端到端的黑箱模型。为了应对这一挑战，在本文中我们提出了一种新的框架DeepNT，该框架利用以路径为中心的图神经网络来预测性能指标，无需预定义的手工设计的指标、假设或真实网络拓扑结构。通过推理和聚合构成路径节点序列的嵌入表示来学习路径嵌入。训练这种以路径为中心的图神经网络需要在离散约束下同时学习神经网络参数和网络拓扑结构，这些约束是由观察到的路径性能指标引入的。这促使我们设计了一个施加连通性和稀疏性约束于拓扑结构以及路径性能三角不等式的学习目标。大量的真实世界数据集和合成数据集上的实验表明，在预测性能指标和推理图拓扑方面，DeepNT优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network tomography is a crucial problem in network monitoring, where theobservable path performance metric values are used to infer the unobservedones, making it essential for tasks such as route selection, fault diagnosis,and traffic control. However, most existing methods either assume completeknowledge of network topology and metric formulas-an unrealistic expectation inmany real-world scenarios with limited observability-or rely entirely onblack-box end-to-end models. To tackle this, in this paper, we argue that agood network tomography requires synergizing the knowledge from both data andappropriate inductive bias from (partial) prior knowledge. To see this, wepropose Deep Network Tomography (DeepNT), a novel framework that leverages apath-centric graph neural network to predict path performance metrics withoutrelying on predefined hand-crafted metrics, assumptions, or the real networktopology. The path-centric graph neural network learns the path embedding byinferring and aggregating the embeddings of the sequence of nodes that composethis path. Training path-centric graph neural networks requires learning theneural netowrk parameters and network topology under discrete constraintsinduced by the observed path performance metrics, which motivates us to designa learning objective that imposes connectivity and sparsity constraints ontopology and path performance triangle inequality on path performance.Extensive experiments on real-world and synthetic datasets demonstrate thesuperiority of DeepNT in predicting performance metrics and inferring graphtopology compared to state-of-the-art methods.</description>
      <author>example@mail.com (Yuntong Hu, Junxiang Wang, Liang Zhao)</author>
      <guid isPermaLink="false">2502.16430v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Automated Keypoint Estimation for Self-Piercing Rivet Joints Using micro-CT Imaging and Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.16752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于微计算机断层扫描（Micro-CT）成像、机器视觉和深度学习技术的自穿铆钉（SPR）接头非破坏性评价方法。通过合成数据预训练模型，并使用少量真实数据进行迁移学习，以实现关键点自动估计，评估接头的质量。&lt;h4&gt;背景&lt;/h4&gt;在汽车工业中，自穿铆钉接头的结构完整性至关重要，但传统破坏性检测手段存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种成本效益高且可扩展的方法来评价SPR接头的品质。&lt;h4&gt;方法&lt;/h4&gt;使用微计算机断层扫描（Micro-CT）结合机器视觉和深度学习技术，特别是自动关键点估计，利用合成数据进行初始模型训练，并通过真实数据迁移学习适应实际条件。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在合成数据上，再用少量的真实数据进行细化训练，可以缩小领域差距并提高预测精度。该框架为SPR接头评价提供了一种可扩展、成本效益高的解决方案，并为制造过程中的机器视觉和非破坏性检测的更广泛应用奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;通过解决数据稀缺问题并利用先进的机器学习技术，这项工作代表了在工程环境中实现自动质量控制的重要步骤。&lt;h4&gt;翻译&lt;/h4&gt;自穿铆钉（SPR）接头结构完整性对汽车工业至关重要，但传统破坏性方法评价存在挑战。本文提出了一种基于微计算机断层扫描成像、结合机器视觉和深度学习技术的非破坏性评估方案，重点在于自动关键点估计以评估接头质量。鉴于实际微CT数据稀少，本研究使用合成数据进行初始模型训练，并通过迁移学习适应真实情况。采用UNet架构精确定位三个关键点，实现头部高度、锁紧度和底部层厚度等重要参数的测量。详尽验证表明，在合成数据上预训练并在有限的真实数据上微调可以缩小领域差异并提高预测精度。本框架不仅为评价SPR接头提供了可扩展且成本效益高的解决方案，并确立了机器视觉及非破坏性检测在制造流程中更广泛应用的基础。通过处理数据稀缺问题和应用高级机器学习技术，这项工作代表了工程环境中自动质量控制的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The structural integrity of self-piercing rivet (SPR) joints is critical inautomotive industries, yet its evaluation poses challenges due to thelimitations of traditional destructive methods. This research introduces aninnovative approach for non-destructive evaluation using micro-CT imaging,Micro-Computed Tomography, combined with machine vision and deep learningtechniques, specifically focusing on automated keypoint estimation to assessjoint quality. Recognizing the scarcity of real micro-CT data, this studyutilizes synthetic data for initial model training, followed by transferlearning to adapt the model for real-world conditions. A UNet-basedarchitecture is employed to localize three keypoints with precision, enablingthe measurement of critical parameters such as head height, interlock, andbottom layer thickness. Extensive validation demonstrates that pre-training onsynthetic data, complemented by fine-tuning with limited real data, bridgesdomain gaps and enhances predictive accuracy. The proposed framework not onlyoffers a scalable and cost-efficient solution for evaluating SPR joints butalso establishes a foundation for broader applications of machine vision andnon-destructive testing in manufacturing processes. By addressing data scarcityand leveraging advanced machine learning techniques, this work represents asignificant step toward automated quality control in engineering contexts.</description>
      <author>example@mail.com (Wei Qin Chuah, Ruwan Tennakoon, Amanda Freis, Mark Easton, Reza Hoseinnezhad, Alireza Bab-Hadiashar)</author>
      <guid isPermaLink="false">2502.16752v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts</title>
      <link>http://arxiv.org/abs/2502.15996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;提出了一种新型上下文嵌入模型med-gte-hybrid，该模型从gte-large句子变换器中派生而来，用于提取无结构临床叙述中的信息。&lt;h4&gt;目的&lt;/h4&gt;评估med-gte-hybrid在大规模患者队列（来源于MIMIC-IV数据集）上的几种临床预测任务的性能，并展示其在患者分层、聚类和文本检索方面的改进效果。&lt;h4&gt;方法&lt;/h4&gt;采用结合对比学习和去噪自动编码器的模型微调策略，用于评估med-gte-hybrid的性能。同时，在慢性肾脏病（CKD）患者的预后、估计肾小球滤过率(eGFR)预测以及患者死亡率预测等多个临床任务中进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，该混合模型在大规模文本嵌入基准测试（MTEB）上优于当前最先进的模型。此外，在某些评估任务侧重于CKD的情况下，句子变换器的混合微调策略可以应用于其他医学领域，并有潜力改善各种医疗应用中的临床决策和个性化治疗路径。&lt;h4&gt;结论&lt;/h4&gt;med-gte-hybrid在多个方面表现出色，包括患者分层、聚类以及文本检索等，特别是在大规模文本嵌入基准测试中超越了当前最先进的模型。该方法对其他医疗领域的潜在应用也进行了展望，强调其可能改善临床决策和个性化治疗路径的能力。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了一种新型上下文嵌入模型med-gte-hybrid，它是从gte-large句子变换器派生而来的，用于提取无结构化临床叙述中的信息。我们的模型微调策略结合了对比学习和去噪自动编码器。为了评估med-gte-hybrid的性能，我们在MIMIC-IV数据集中提取的大规模患者队列中进行了多个临床预测任务的研究，包括慢性肾脏病（CKD）患者的预后、估计肾小球滤过率(eGFR)预测以及患者死亡率预测。此外，我们展示了该模型在患者分层、聚类和文本检索方面的改进效果，并且在大规模文本嵌入基准测试中超过了当前最先进的模型。虽然我们的某些评估集中在CKD上，但句子变换器的混合微调策略可以转移到其他医疗领域，并具有改善各种医疗应用中的临床决策和个人化治疗路径的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel contextual embedding model med-gte-hybrid that wasderived from the gte-large sentence transformer to extract information fromunstructured clinical narratives. Our model tuning strategy for med-gte-hybridcombines contrastive learning and a denoising autoencoder. To evaluate theperformance of med-gte-hybrid, we investigate several clinical prediction tasksin large patient cohorts extracted from the MIMIC-IV dataset, including ChronicKidney Disease (CKD) patient prognosis, estimated glomerular filtration rate(eGFR) prediction, and patient mortality prediction. Furthermore, wedemonstrate that the med-gte-hybrid model improves patient stratification,clustering, and text retrieval, thus outperforms current state-of-the-artmodels on the Massive Text Embedding Benchmark (MTEB). While some of ourevaluations focus on CKD, our hybrid tuning of sentence transformers could betransferred to other medical domains and has the potential to improve clinicaldecision-making and personalised treatment pathways in various healthcareapplications.</description>
      <author>example@mail.com (Aditya Kumar, Simon Rauch, Mario Cypko, Oliver Amft)</author>
      <guid isPermaLink="false">2502.15996v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AAD-LLM: Neural Attention-Driven Auditory Scene Understanding</title>
      <link>http://arxiv.org/abs/2502.16794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的听觉场景理解模型AAD-LLM，该模型通过结合脑电信号来推断听众的注意力，并据此调整响应生成。研究证明了这种方法在多个多说话者场景任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的听觉基础模型对所有的声音输入处理方式相同，忽略人类听力感知中固有的选择性特点，即人们倾向于关注特定的声音来源而忽略其他声音。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据听众注意力生成相应响应的系统，以提高音频生成与人类感知的一致性。&lt;h4&gt;方法&lt;/h4&gt;提出Intention-Informed Auditory Scene Understanding (II-ASU)框架，并构建了一个原型系统AAD-LLM。该模型通过整合颅内脑电图(iEEG)记录来解码听众关注的具体说话者，然后根据推断出的注意力状态调整响应生成。&lt;h4&gt;主要发现&lt;/h4&gt;在多说话者的场景下，AAD-LLM在说话人描述、语音转录与提取以及问答任务中表现出更好的一致性。评估结果显示，客观和主观评价均表明该模型能够更好地符合听众的意图。&lt;h4&gt;结论&lt;/h4&gt;本研究为面向意图感知的听觉人工智能领域铺平了道路，通过将听众感知信息融入机器处理过程，探索了一种新的聆听机制。这为未来的以用户为中心的音频系统开发提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文主要探讨了现有听觉模型忽视人类听力选择性的问题，并提出了AAD-LLM原型系统来解决这一问题，展示其在多说话者场景任务中的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auditory foundation models, including auditory large language models (LLMs),process all sound inputs equally, independent of listener perception. However,human auditory perception is inherently selective: listeners focus on specificspeakers while ignoring others in complex auditory scenes. Existing models donot incorporate this selectivity, limiting their ability to generateperception-aligned responses. To address this, we introduce Intention-InformedAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM(AAD-LLM), a prototype system that integrates brain signals to infer listenerattention. AAD-LLM extends an auditory LLM by incorporating intracranialelectroencephalography (iEEG) recordings to decode which speaker a listener isattending to and refine responses accordingly. The model first predicts theattended speaker from neural activity, then conditions response generation onthis inferred attentional state. We evaluate AAD-LLM on speaker description,speech transcription and extraction, and question answering in multitalkerscenarios, with both objective and subjective ratings showing improvedalignment with listener intention. By taking a first step towardintention-aware auditory AI, this work explores a new paradigm where listenerperception informs machine listening, paving the way for futurelistener-centered auditory systems. Demo and code available:https://aad-llm.github.io.</description>
      <author>example@mail.com (Xilin Jiang, Sukru Samet Dindar, Vishal Choudhari, Stephan Bickel, Ashesh Mehta, Guy M McKhann, Adeen Flinker, Daniel Friedman, Nima Mesgarani)</author>
      <guid isPermaLink="false">2502.16794v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究构建了一个大型的COVID严重程度数据集，并探讨了迁移学习和视觉变换器在预测患者病情严重程度中的有效性。&lt;h4&gt;背景&lt;/h4&gt;新冠疫情对医疗资源造成了压力，促使人们讨论机器学习如何减轻医生负担并有助于诊断。胸部X光（CXRs）被用于诊断新冠，但很少有研究根据CXRs预测患者的病情严重性。&lt;h4&gt;目的&lt;/h4&gt;通过合并多个数据来源创建一个大型的COVID严重程度数据集，并调查迁移学习在病情严重性和分类任务中的效果。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型DenseNet161、基于ImageNet和CXR的数据预处理以及视觉变换器（ViT）进行研究。其中，DenseNet161模型在三类病情预测问题中表现出色，而ViT的回归结果最佳。&lt;h4&gt;主要发现&lt;/h4&gt;1. 预训练的DenseNet161模型对三个类别严重程度的预测表现最好，在整体上达到80%的准确性。2. ViT在根据放射科医生评分进行病情严重性评分的回归任务中表现出最优的结果，均方绝对误差为0.5676。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了迁移学习和视觉变换器在从胸部X光预测患者病情严重程度方面具有显著潜力。预训练模型DenseNet161在分类任务上表现最好，而ViT则在回归任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：新冠疫情给医疗资源带来了巨大压力，并引发了关于机器学习如何减轻医生负担并支持诊断的讨论。胸部X光（CXRs）被用于诊断新冠，但鲜有研究依据CXRs预测患者的病情严重性。本研究通过合并三个来源创建了一个大型COVID严重程度数据集，并探讨了使用ImageNet和CXR预训练模型以及视觉变换器在病情回归与分类任务中的有效性的迁移学习方法。其中，一个预训练的DenseNet161模型在三类病情预测问题中表现出色，在整体上达到了80%的准确性（具体为轻度、中度及重度病例准确率分别为77.3%，83.9%，和70%）。ViT则表现出了最优的回归结果，其均方绝对误差仅为0.5676。研究项目源代码公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju, Shawn Whitfield)</author>
      <guid isPermaLink="false">2502.16622v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种名为Plane-DUSt3R的新方法，用于多视角房间布局估计。&lt;h4&gt;背景&lt;/h4&gt;目前从多个视角的图像中进行房间布局估计的研究较少，因为涉及复杂的多视图几何结构问题。传统的方法需要分步骤解决相机内参和外参估计、图像匹配以及三角测量等问题。&lt;h4&gt;目的&lt;/h4&gt;引入Plane-DUSt3R方法，利用三维基础模型DUSt3R来简化房间布局估计的过程，并提高其准确性。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R结合了DUSt3R框架，并在房间布局数据集（Structure3D）上进行微调以估算结构平面。该方法通过生成统一和简洁的结果，仅需一步后处理步骤和2D检测结果即可完成房间布局估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Plane-DUSt3R不仅在合成数据集中优于现有最佳方法，在不同图像风格（如卡通）的真实世界数据中也表现出强大的鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;相比传统多步骤的方法，Plane-DUSt3R提供了一种更直接、更有效的解决方案。该模型能够处理多个视角的图像，并且减少了错误累积。&lt;h4&gt;翻译&lt;/h4&gt;房间布局估计从多个视角的图像出发受到的关注较少，这是由于复杂的多视图几何结构问题造成的，需要进行相机内参和外参估计、图像匹配以及三角测量等步骤。然而，在三维重建领域，近期3D基础模型（如DUSt3R）的发展改变了传统的基于结构的运动过程到端到端一步式的转变。为此，我们介绍了一种名为Plane-DUSt3R的方法，利用3D基础模型DUSt3R来进行多视角房间布局估计。通过在房间布局数据集上微调并修改目标以估算结构平面，生成一致且简洁的结果使仅需要一个后处理步骤和2D检测结果就可完成房间布局的估计。与依赖于单视角或全景图像的方法不同，Plane-DUSt3R可以处理多视角图像，并提供了一种简化流程减少错误累积的端到端解决方案。实验结果显示，无论是合成数据还是真实世界中具有多种风格（如卡通）的数据集，Plane-DUSt3R都优于现有最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R}, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon.</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SDA-DDA Semi-supervised Domain Adaptation with Dynamic Distribution Alignment Network For Emotion Recognition Using EEG Signals</title>
      <link>http://arxiv.org/abs/2502.16485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的半监督领域适应框架SDA-DDA，该框架使用动态分布对齐机制和伪标签置信度过滤模块来解决情感脑机接口技术中个体间EEG数据差异的问题。&lt;h4&gt;背景&lt;/h4&gt;情感脑机接口（aBCI）通过监测并识别人类情绪状态促进情感感知技术的发展。然而，不同个体间的EEG信号存在显著的变异性，这阻碍了有效且广泛适用的情感脑机接口模型的发展。&lt;h4&gt;目的&lt;/h4&gt;为了应对这种挑战，研究旨在开发一种新的迁移学习框架，以提高aBCI在跨受试者和跨时段条件下的情感识别准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SDA-DDA的新半监督领域适应框架。该框架通过最大均值差异（MMD）和条件最大均值差异（CMMD）来对齐源域与目标域的边际及条件概率分布，并引入动态分布对齐机制以在整个训练过程中调整差异，提高适应性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明SDA-DDA框架在情感识别方面优于现有方法，在跨受试者和跨时段条件下表现尤为突出。这证明了该方法的强大鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;这项研究推进了情感脑机接口技术的发展，提高了情绪识别的泛化能力和准确性，有助于实现个性化的情感脑机接口应用。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们专注于解决个体差异在情感脑机接口（aBCI）中的挑战。通过EEG信号监测和识别人类的情绪状态，从而促进情感感知技术的发展。然而，不同个体间的EEG数据变异性是开发有效且广泛应用的情感脑机接口模型的主要障碍。为了解决这一问题，我们提出了一种新的迁移学习框架——半监督领域适应与动态分布对齐（SDA-DDA）。该方法使用最大均值差异（MMD）和条件最大均值差异（CMMD）来对齐源域与目标域的边际及条件概率分布。引入了动态分布对齐机制在整个训练过程中调整差异，提高适应性。此外，在半监督流程中集成了伪标签置信度过滤模块以优化伪标签生成并改善条件分布估计。在EEG基准数据库（SEED、SEED-IV和DEAP）上的广泛实验验证了SDA-DDA的强大鲁棒性和有效性。结果表明，相较于现有方法，该框架在各种场景下的情感识别中具有优越性，包括跨受试者和跨时段条件。这项进步增强了情感识别的泛化能力和准确性，可能促进个性化aBCI应用的发展。源代码可从https://github.com/XuanSuTrum/SDA-DDA获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on the challenge of individual variability inaffective brain-computer interfaces (aBCI), which employs electroencephalogram(EEG) signals to monitor and recognize human emotional states, therebyfacilitating the advancement of emotion-aware technologies. The variability inEEG data across individuals poses a significant barrier to the development ofeffective and widely applicable aBCI models. To tackle this issue, we propose anovel transfer learning framework called Semi-supervised Domain Adaptation withDynamic Distribution Alignment (SDA-DDA). This approach aligns the marginal andconditional probability distribution of source and target domains using maximummean discrepancy (MMD) and conditional maximum mean discrepancy (CMMD). Weintroduce a dynamic distribution alignment mechanism to adjust differencesthroughout training and enhance adaptation. Additionally, a pseudo-labelconfidence filtering module is integrated into the semi-supervised process torefine pseudo-label generation and improve the estimation of conditionaldistributions. Extensive experiments on EEG benchmark databases (SEED, SEED-IVand DEAP) validate the robustness and effectiveness of SDA-DDA. The resultsdemonstrate its superiority over existing methods in emotion recognition acrossvarious scenarios, including cross-subject and cross-session conditions. Thisadvancement enhances the generalization and accuracy of emotion recognition,potentially fostering the development of personalized aBCI applications. Thesource code is accessible at https://github.com/XuanSuTrum/SDA-DDA.</description>
      <author>example@mail.com (Jiahao Tang)</author>
      <guid isPermaLink="false">2502.16485v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning</title>
      <link>http://arxiv.org/abs/2502.16737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 28th International Conference on Artificial  Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume  258&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了在在线学习环境中，动态对手进行数据投毒攻击对机器学习算法的影响，并提出了计算这种影响的认证边界的新框架。&lt;h4&gt;背景&lt;/h4&gt;随着基于人类反馈微调基础模型的发展，不信任用户提供的人类反馈增加了对抗性数据中毒的风险。现有研究表明静态对手通过修改训练集可以降低模型鲁棒性，但在实际应用中，动态对手能够观察和响应学习过程，并更有效地注入毒害样本以优化其目标。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架来计算动态投毒影响的认证边界，并利用这些证书设计出更加稳健的学习算法。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一个新的计算认证边界的框架，并通过均值估计和二元分类问题进行了说明，展示了如何使用该框架设计对抗数据投毒攻击更为有效的学习算法。&lt;h4&gt;主要发现&lt;/h4&gt;在线动态对手相较于静态对手更具威胁性。提出的框架能够帮助构建更稳健的机器学习模型，抵御更复杂的数据中毒攻击。&lt;h4&gt;结论&lt;/h4&gt;研究为解决在线学习中的动态数据投毒问题提供了新方法和理论支持，并指出未来工作应进一步探索该领域。&lt;h4&gt;翻译&lt;/h4&gt;基础模型根据潜在不可信用户的人类反馈进行微调的风险增加了对抗性数据中毒的风险，这需要对学习算法在面对此类攻击时的鲁棒性的研究。现有关于可证明认证鲁棒性以抵御数据投毒攻击的研究主要集中在静态对手上，这些对手可以在训练算法应用前修改一部分用于训练模型的数据集。但在实践中，尤其是在根据人类反馈进行在线学习的情况下，对抗者可以观察和响应学习过程，并注入优化其目标的有毒样本，比他们仅被限制在一次性中毒静态数据集中时更有效。事实上，在先前的工作中已经表明，在线动态对手可能远比静态对手更为强大。我们提出了一种用于计算动态投毒影响认证边界的新型框架，并使用这些证书来设计稳健的学习算法。论文展示了该框架在均值估计和二元分类问题上的应用示例，并概述了进一步工作的扩展方向。实现我们的证书并复制结果的代码可在https://github.com/Avinandan22/Certified-Robustness上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models fine-tuned on human feedback from potentiallyuntrusted users has increased the risk of adversarial data poisoning,necessitating the study of robustness of learning algorithms against suchattacks. Existing research on provable certified robustness against datapoisoning attacks primarily focuses on certifying robustness for staticadversaries who modify a fraction of the dataset used to train the model beforethe training algorithm is applied. In practice, particularly when learning fromhuman feedback in an online sense, adversaries can observe and react to thelearning process and inject poisoned samples that optimize adversarialobjectives better than when they are restricted to poisoning a static datasetonce, before the learning algorithm is applied. Indeed, it has been shown inprior work that online dynamic adversaries can be significantly more powerfulthan static ones. We present a novel framework for computing certified boundson the impact of dynamic poisoning, and use these certificates to design robustlearning algorithms. We give an illustration of the framework for the meanestimation and binary classification problems and outline directions forextending this in further work. The code to implement our certificates andreplicate our results is available athttps://github.com/Avinandan22/Certified-Robustness.</description>
      <author>example@mail.com (Avinandan Bose, Laurent Lessard, Maryam Fazel, Krishnamurthy Dj Dvijotham)</author>
      <guid isPermaLink="false">2502.16737v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Improving Monocular Visual-Inertial Initialization with Structureless Visual-Inertial Bundle Adjustment</title>
      <link>http://arxiv.org/abs/2502.16598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;单目视觉惯性里程计（VIO）在实时运动追踪应用中表现出色，得益于传感器套件的小巧和低功耗。为了成功启动VIO算法，初始化模块非常重要。&lt;h4&gt;背景&lt;/h4&gt;大多数初始化方法依赖于三维视觉点云的重建。这些方法由于状态向量包含运动状态和三维特征点而导致计算成本较高。&lt;h4&gt;目的&lt;/h4&gt;为了解决这个问题，并提高无结构初始化法的准确性，提出了新的无结构视觉惯性捆绑调整方法以进一步细化先前的无结构解。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了一种新型的无结构视觉惯性捆绑调整算法来改进初始状态估计问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在保持实时性能的同时显著提高了VIO初始化精度。&lt;h4&gt;结论&lt;/h4&gt;通过新的无结构视觉惯性捆绑调整技术，不仅解决了计算效率的问题，还提升了VIO系统的初始化准确性。&lt;h4&gt;翻译&lt;/h4&gt;单目视觉惯性里程计（VIO）由于传感器套件的小巧和低功耗，在实时运动追踪应用中得到广泛应用。为了成功启动这些算法，初始化模块至关重要。大多数现有方法依赖于三维点云重建，这导致计算成本较高。为解决此问题并提升无结构化方法的性能准确性，研究者提出了新的无结构视觉惯性捆绑调整技术以进一步优化初始状态估计。实验结果表明，该方法显著提高了VIO系统的初始化精度，并保持了实时处理能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular visual inertial odometry (VIO) has facilitated a wide range ofreal-time motion tracking applications, thanks to the small size of the sensorsuite and low power consumption. To successfully bootstrap VIO algorithms, theinitialization module is extremely important. Most initialization methods relyon the reconstruction of 3D visual point clouds. These methods suffer from highcomputational cost as state vector contains both motion states and 3D featurepoints. To address this issue, some researchers recently proposed astructureless initialization method, which can solve the initial state withoutrecovering 3D structure. However, this method potentially compromisesperformance due to the decoupled estimation of rotation and translation, aswell as linear constraints. To improve its accuracy, we propose novelstructureless visual-inertial bundle adjustment to further refine previousstructureless solution. Extensive experiments on real-world datasets show ourmethod significantly improves the VIO initialization accuracy, whilemaintaining real-time performance.</description>
      <author>example@mail.com (Junlin Song, Antoine Richard, Miguel Olivares-Mendez)</author>
      <guid isPermaLink="false">2502.16598v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MOB-GCN: A Novel Multiscale Object-Based Graph Neural Network for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2502.16289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为MOB-GCN的新型多尺度对象基于图神经网络，用于高光谱图像(HSI)分类。该研究的主要目标是通过利用多尺度对象基础影像分析(OBIA)，提高特征提取和分类性能。&lt;h4&gt;背景&lt;/h4&gt;传统的像素级方法通常由于准确性低和斑点噪声问题而效果不佳，单一尺度的OBIA方法可能忽略了不同细节层次下影像物体的重要信息。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，MOB-GCN通过从多个分割尺度中提取并整合特征来提升分类结果。该模型利用多分辨率图网络（MGN）架构捕捉细粒度和全局空间模式。&lt;h4&gt;方法&lt;/h4&gt;通过构建动态的多尺度图层级结构，MOB-GCN提供了对HSI复杂细节与全局上下文更全面的理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与单一尺度图卷积网络(GCN)相比，MOB-GCN在分类准确性、计算效率和降噪方面表现更为出色，尤其是在标注数据有限的情况下。&lt;h4&gt;结论&lt;/h4&gt;MOB-GCN的实现代码可在https://github.com/HySonLab/MultiscaleHSI 上公开获得。&lt;h4&gt;翻译&lt;/h4&gt;该论文提出了一种名为MOB-GCN的新颖多尺度对象基于图神经网络以改进高光谱图像(HSI)分类中的特征提取和分类性能。传统像素级方法因准确性低及斑点噪声而受限，单一尺度OBIA方法容易忽略不同层次细节下的关键信息。为了克服这些问题，MOB-GCN通过从多个分割层级中整合并提取特征来增强其性能，并采用多分辨率图网络（MGN）架构捕捉细粒度与全局空间模式。构建的动态多尺度图层级结构使对HSI复杂性有更深入理解的同时提高了准确性、效率及抗噪能力，特别是在数据标记有限的情况下表现更加突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel multiscale object-based graph neural networkcalled MOB-GCN for hyperspectral image (HSI) classification. The central aim ofthis study is to enhance feature extraction and classification performance byutilizing multiscale object-based image analysis (OBIA). Traditionalpixel-based methods often suffer from low accuracy and speckle noise, whilesingle-scale OBIA approaches may overlook crucial information of image objectsat different levels of detail. MOB-GCN overcomes these challenges by extractingand integrating features from multiple segmentation scales, leveraging theMultiresolution Graph Network (MGN) architecture to capture both fine-grainedand global spatial patterns. MOB-GCN addresses this issue by extracting andintegrating features from multiple segmentation scales to improveclassification results using the Multiresolution Graph Network (MGN)architecture that can model fine-grained and global spatial patterns. Byconstructing a dynamic multiscale graph hierarchy, MOB-GCN offers a morecomprehensive understanding of the intricate details and global context ofHSIs. Experimental results demonstrate that MOB-GCN consistently outperformssingle-scale graph convolutional networks (GCNs) in terms of classificationaccuracy, computational efficiency, and noise reduction, particularly whenlabeled data is limited. The implementation of MOB-GCN is publicly available athttps://github.com/HySonLab/MultiscaleHSI</description>
      <author>example@mail.com (Tuan-Anh Yang, Truong-Son Hy, Phuong D. Dao)</author>
      <guid isPermaLink="false">2502.16289v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.16671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了一种新的数据集MimeQA，旨在促进社会智能AI的发展。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能越来越深入人们的日常生活，理解并进行无缝交流的社交智能AI变得愈发重要。然而当前的人工智能在非语言交互的理解上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过利用哑剧视频这一富含非言语和社交互动的数据源，改进现有模型对非语言社会互动的理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个新的数据集MimeQA，该数据集中包含221个源自YouTube的哑剧视频，并从中选取了101个视频进行详细标注，形成了806个问题答案配对。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用MimeQA评估最先进的视频大型语言模型(vLLMs)，研究者们发现这些模型在解释非言语互动时准确性较低（15%-30%），且存在过分依赖文本提示而忽视微妙的非言语互动的问题。&lt;h4&gt;结论&lt;/h4&gt;发布数据集旨在推动基础模型的发展，使其能更好地理解非言语的人类交互，促进真正具备社会智能的AI系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;社交智慧型人工智能能够理解和无缝地与人类进行日常生活的交流变得越来越重要。然而目前关于人工社交推理的研究都依赖于语言或以语言为主的方法来进行基准测试和训练模型，导致这些系统在口头沟通方面有所进步但在非言语的社会理解上存在困难。为了克服这一局限性，我们利用了一个新的数据源——哑剧视频来研究非言语社会互动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socially intelligent AI that can understand and interact seamlessly withhumans in daily lives is increasingly important as AI becomes more closelyintegrated with peoples' daily activities. However, current works in artificialsocial reasoning all rely on language-only, or language-dominant approaches tobenchmark and training models, resulting in systems that are improving inverbal communication but struggle with nonverbal social understanding. Toaddress this limitation, we tap into a novel source of data rich in nonverbaland social interactions -- mime videos. Mimes refer to the art of expressionthrough gesture and movement without spoken words, which presents uniquechallenges and opportunities in interpreting non-verbal social communication.We contribute a new dataset called MimeQA, obtained by sourcing 221 videos fromYouTube, through rigorous annotation and verification, resulting in a benchmarkwith 101 videos and 806 question-answer pairs. Using MimeQA, we evaluatestate-of-the-art video large language models (vLLMs) and find that theiroverall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often failto ground imagined objects and over-rely on the text prompt while ignoringsubtle nonverbal interactions. Our data resources are released athttps://github.com/MIT-MI/MimeQA to inspire future work in foundation modelsthat embody true social intelligence capable of interpreting non-verbal humaninteractions.</description>
      <author>example@mail.com (Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang)</author>
      <guid isPermaLink="false">2502.16671v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Aware 3D Salient Object Detection Network</title>
      <link>http://arxiv.org/abs/2502.16488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于几何感知的3D显著对象检测网络，该网络通过将点聚类成超级点来提升物体的几何边界，并清晰地分割出具有复杂背景的对象。&lt;h4&gt;背景&lt;/h4&gt;近年来，研究人员对点云显著性目标检测产生了兴趣。然而，现有的工作未能充分利用3D对象的几何上下文，在处理具有复杂背景的对象时会导致模糊边界。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效利用3D对象的几何信息来清晰分割出完整物体的网络模型。&lt;h4&gt;方法&lt;/h4&gt;首先设计了一个简单的超级点划分模块以将点聚类成超级点，并提出了一种无类别感知损失函数来提高超级点的质量。然后，通过超级点-点注意力机制聚合几何信息到点特征中，预测具有清晰边界的显著图。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在PCSOD数据集上取得了最新的最优性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效提升3D显著对象检测的效果，并且对于处理复杂背景下的物体分割任务特别有效。&lt;h4&gt;翻译&lt;/h4&gt;点云显著性目标检测已经引起了研究人员的注意。由于现有的工作未能充分利用3D对象的几何上下文，当对具有复杂背景的对象进行分割时会产生模糊边界。在这篇论文中，我们提出了一种基于几何感知的3D显著性对象检测网络，该网络通过将点显式地聚类成超级点来增强物体的几何边界，从而清晰地分割出具有完整边界的物体。具体来说，首先设计了一个简单的超级点划分模块以将点聚类成超级点，并提出了一种无类别感知损失函数来提高超级点的质量。然后，通过超级点-点注意力机制聚合几何信息到点特征中，预测具有清晰边界的显著图。广泛的实验表明，该方法在PCSOD数据集上取得了最新的最优性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud salient object detection has attracted the attention ofresearchers in recent years. Since existing works do not fully utilize thegeometry context of 3D objects, blurry boundaries are generated when segmentingobjects with complex backgrounds. In this paper, we propose a geometry-aware 3Dsalient object detection network that explicitly clusters points intosuperpoints to enhance the geometric boundaries of objects, thereby segmentingcomplete objects with clear boundaries. Specifically, we first propose a simpleyet effective superpoint partition module to cluster points into superpoints.In order to improve the quality of superpoints, we present a point cloudclass-agnostic loss to learn discriminative point features for clusteringsuperpoints from the object. After obtaining superpoints, we then propose ageometry enhancement module that utilizes superpoint-point attention toaggregate geometric information into point features for predicting the salientmap of the object with clear boundaries. Extensive experiments show that ourmethod achieves new state-of-the-art performance on the PCSOD dataset.</description>
      <author>example@mail.com (Chen Wang, Liyuan Zhang, Le Hui, Qi Liu, Yuchao Dai)</author>
      <guid isPermaLink="false">2502.16488v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HetFS: A Method for Fast Similarity Search with Ad-hoc Meta-paths on Heterogeneous Information Networks</title>
      <link>http://arxiv.org/abs/2502.16288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现实世界的信息网络形成了异构信息网络（HIN），节点和边表示为不同类型的对象和关系。相似性衡量的是两个节点的接近程度，并且主要基于它们连接到的其他节点的相似性递归地确定。&lt;h4&gt;问题定义&lt;/h4&gt;用户可能只对特定类型的链接感兴趣，这些链接在相似性的定义中作为元路径（meta-paths）表示。现有方法要么需要为不同的元路径重新训练异构图神经网络（HGNN），要么使用基于路径的方法进行灵活转换但准确性较低。&lt;h4&gt;目的&lt;/h4&gt;提出HetFS（Fast Similarity for Hetereogeneous information networks with user-given meta-paths）以解决实时查询中的问题，能够根据用户指定的元路径快速提供相似性结果。&lt;h4&gt;方法&lt;/h4&gt;HetFS利用满足元路径限制的路径信息和节点内容来计算相似度。它结合了路径信息与节点本身的特性，提高了准确性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证，HetFS在处理实时查询方面表现出色，超越现有的HGNN和基于路径的方法，并且在下游应用中也展现了强大的性能，包括链路预测、节点分类以及聚类。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的方法HetFS来解决异构信息网络中的相似性搜索问题，它能够灵活应对用户指定的元路径并提高准确性与效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上面内容是根据摘要总结和翻译的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11280-024-01303-1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous real-world information networks form Heterogeneous InformationNetworks (HINs) with diverse objects and relations represented as nodes andedges in heterogeneous graphs. Similarity between nodes quantifies how closelytwo nodes resemble each other, mainly depending on the similarity of the nodesthey are connected to, recursively. Users may be interested in only specifictypes of connections in the similarity definition, represented as meta-paths,i.e., a sequence of node and edge types. Existing Heterogeneous Graph NeuralNetwork (HGNN)-based similarity search methods may accommodate meta-paths, butrequire retraining for different meta-paths. Conversely, existing path-basedsimilarity search methods may switch flexibly between meta-paths but oftensuffer from lower accuracy, as they rely solely on path information. This paperproposes HetFS, a Fast Similarity method for ad-hoc queries with user-givenmeta-paths on Heterogeneous information networks. HetFS provides similarityresults based on path information that satisfies the meta-path restriction, aswell as node content. Extensive experiments demonstrate the effectiveness andefficiency of HetFS in addressing ad-hoc queries, outperformingstate-of-the-art HGNNs and path-based approaches, and showing strongperformance in downstream applications, including link prediction, nodeclassification, and clustering.</description>
      <author>example@mail.com (Xuqi Mao, Zhenyi Chen, Zhenying He, Yinan Jing, Kai Zhang, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16288v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Gaussian Mixture Variational Autoencoder for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2502.16140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于变分自编码器（VAE）的顺序推荐模型SIGMA，以克服现有VAE在处理用户多重兴趣时能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;目前大多数基于VAE的顺序推荐系统假设序列表示遵循单一高斯分布作为先验分布。然而，在实际应用中，由于用户的多种多样的兴趣，这种单峰分布难以捕捉复杂且多元化的兴趣模式，导致推荐效果受限。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了一个新的基于VAE的顺序推荐模型SIGMA，旨在更好地适应用户的不同兴趣并提高推荐性能。&lt;h4&gt;方法&lt;/h4&gt;SIGMA通过假设序列表示遵循混合高斯分布作为先验来建立一个多模态的兴趣模型。此外，为了将这些多模态兴趣纳入序列表示学习中，SIGMA设计了一个概率多重兴趣提取模块和一个兼容混合高斯先验的多兴趣感知ELBO。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SIGMA在公共数据集上的推荐性能明显优于传统的基于VAE的方法。该模型通过考虑用户的多个兴趣点来提高用户个性化体验。&lt;h4&gt;结论&lt;/h4&gt;SIGMA为顺序推荐提供了一种新的方法论，特别是在处理用户复杂和多元化的兴趣方面显示出潜力。这将对未来的推荐系统设计产生积极影响。&lt;h4&gt;翻译&lt;/h4&gt;变分自编码器（VAE）在序列推荐中通过学习每个用户-项目交互序列的连续分布而不是确定性嵌入来提高数据缺乏下的稳健性和性能表现。然而，现有的基于VAE的序列推荐模型假设序列表示遵循单一高斯分布作为先验，这限制了捕捉复杂用户兴趣的能力，并且当用户具有多种兴趣时会降低推荐性能。由于用户通常会有多个不同的兴趣点，因此，在顺序推荐场景中建立多模态而非单峰的先验更为合理。本文提出了一种新的VAE基于序列推荐模型SIGMA。SIGMA假设序列表示遵循高斯混合分布作为先验，并且每个分量代表一个单独的兴趣。为了提取多重兴趣，SIGMA包括一个多兴趣概率抽取模块以学习每个兴趣的单一高斯分布根据隐含项目超类别进行学习。此外，SIGMA建立了与混合高斯先验兼容的多兴趣感知ELBO，以将多重兴趣整合到序列表示学习中。广泛的实验表明了SIGMA的有效性，代码可以在GitHub上获取（https://github.com/libeibei95/SIGMA）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational AutoEncoder (VAE) for Sequential Recommendation (SR), whichlearns a continuous distribution for each user-item interaction sequence ratherthan a determinate embedding, is robust against data deficiency and achievessignificant performance. However, existing VAE-based SR models assume aunimodal Gaussian distribution as the prior distribution of sequencerepresentations, leading to restricted capability to capture complex userinterests and limiting recommendation performance when users have more than oneinterest. Due to that it is common for users to have multiple disparateinterests, we argue that it is more reasonable to establish a multimodal priordistribution in SR scenarios instead of a unimodal one. Therefore, in thispaper, we propose a novel VAE-based SR model named SIGMA. SIGMA assumes thatthe prior of sequence representation conforms to a Gaussian mixturedistribution, where each component of the distribution semantically correspondsto one of multiple interests. For multi-interest elicitation, SIGMA includes aprobabilistic multi-interest extraction module that learns a unimodal Gaussiandistribution for each interest according to implicit item hyper-categories.Additionally, to incorporate the multimodal interests into sequencerepresentation learning, SIGMA constructs a multi-interest-aware ELBO, which iscompatible with the Gaussian mixture prior. Extensive experiments on publicdatasets demonstrate the effectiveness of SIGMA. The code is available athttps://github.com/libeibei95/SIGMA.</description>
      <author>example@mail.com (Beibei Li, Tao Xiang, Beihong Jin, Yiyuan Zheng, Rui Zhao)</author>
      <guid isPermaLink="false">2502.16140v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly preserving contrastive neural embeddings for end-to-end model-independent searches at the LHC</title>
      <link>http://arxiv.org/abs/2502.15926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了通过对比神经嵌入学习强大的低维表示以解决大型强子对撞机(LHC)中异常检测的问题。&lt;h4&gt;背景&lt;/h4&gt;在LHC这样的设备中，由于数据集的规模和复杂性，异常检测是一项关键挑战。通常采用的方法是将高维度探测器数据转换为具有物理意义的低维度特征。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在通过对比神经嵌入方法从数据中提取物理信号并识别潜在的新物理现象。&lt;h4&gt;方法&lt;/h4&gt;文中比较了监督和自我监督的对比学习方法，包括多层感知机(MLP)和Transformer架构。这些方法基于LHC碰撞事件中的动力学可观测属性进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;利用学习到的嵌入表示作为信号无关统计检测方法的输入，在包含所有最终状态中实现了超过十倍于原始特征表现的异常检测性能，并且相对于相同维度的物理信息选择，最多有四倍的改进。研究还展示了这些模型在搜索多种信号时的有效性。&lt;h4&gt;结论&lt;/h4&gt;发现用于背景分类的最佳表示不一定最大化新物理信号的敏感度，表明保持背景结构和增强异常之间存在内在权衡。该论文强调了基础模型在粒子物理学数据中的应用潜力，能够显著提高神经特征提取，并为全包含最终状态下的科学发现提供了可能。&lt;h4&gt;翻译&lt;/h4&gt;异常检测——识别与标准模型预测的偏差——是大型强子对撞机(LHC)面临的关键挑战，由于其数据集的巨大规模和复杂性。这通常通过将高维度探测器数据转换成低维度、物理意义明确的功能来解决。我们利用对比神经嵌入学习强大的低维表示方式处理特征提取以用于异常检测。这种方法保留了潜在的异常信号，这些信号可能指示新物理学，并且使用基于新型机器学习的统计方法进行无信号假设检验，从而可以提取稀有信号。我们比较了监督和自我监督对比学习方法，包括多层感知机(MLP)和Transformer架构，训练时使用的都是LHC碰撞事件中物理对象的动力学可观测属性。所学到的嵌入表示作为包含所有最终状态中的信号无关统计检测方法的输入，在异常检测性能方面比原始特征表现提高了十倍以上，并且相对于相同维度的基于物理学的选择最多提高四倍。我们证明了对于罕见的新物理信号和罕见的标准模型过程，无论在何种最终状态中都能显著提升发现能力，表明其能够同时有效地搜索多种信号。研究还指出背景分类的最佳表示不总是最大化对新物理信号的敏感度，揭示了保持背景结构与异常增强之间的固有权衡。本研究表明基础模型对于粒子物理学数据具有重要的改进潜力，能够提高神经特征提取，并为全包含最终状态下的科学发现提供可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection -- identifying deviations from Standard Model predictions-- is a key challenge at the Large Hadron Collider due to the size andcomplexity of its datasets. This is typically addressed by transforminghigh-dimensional detector data into lower-dimensional, physically meaningfulfeatures. We tackle feature extraction for anomaly detection by learningpowerful low-dimensional representations via contrastive neural embeddings.This approach preserves potential anomalies indicative of new physics andenables rare signal extraction using novel machine learning-based statisticalmethods for signal-independent hypothesis testing. We compare supervised andself-supervised contrastive learning methods, for both MLP- andTransformer-based neural embeddings, trained on the kinematic observables ofphysics objects in LHC collision events. The learned embeddings serve as inputrepresentations for signal-agnostic statistical detection methods in inclusivefinal states, achieving over ten fold improved detection performance over theoriginal feature representation and up to four fold improvement over using aphysics-informed selections of the same dimensionality. We achieve significantimprovement in discovery power for both rare new physics signals and rareStandard Model processes across diverse final states, demonstrating itsapplicability for efficiently searching for diverse signals simultaneously. Weshow that the optimal representation for background classification does notalways maximize sensitivity to new physics signals, revealing an inherenttrade-off between background structure preservation and anomaly enhancement.Our findings demonstrate that foundation models for particle physics data holdsignificant potential for improving neural feature extraction, enablingscientific discovery in inclusive final states at collider experiments.</description>
      <author>example@mail.com (Kyle Metzger, Lana Xu, Mia Sodini, Thea K. Arrestad, Katya Govorkova, Gaia Grosso, Philip Harris)</author>
      <guid isPermaLink="false">2502.15926v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths</title>
      <link>http://arxiv.org/abs/2502.16281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Graph神经网络(GNNs)在各种与图相关的任务中取得了最先进的成果，并被广泛应用于异构图(HetGs)，其中元路径有助于编码不同节点类型之间的特定语义。尽管现有的异构GNNs（HGNNs）由于其专注于改进对异质性的捕捉效果而具有革命性的表示能力，但高昂的训练成本阻碍了它们在需要处理基于用户定义元路径的即时查询的真实场景中的实际部署。&lt;h4&gt;背景&lt;/h4&gt;现有技术通过改进对异构图中不同节点类型之间特定语义的理解达到了最先进的水平，然而这些方法面临着高昂的计算开销，这使得它们难以应用于实时应用场景。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，本文提出了一种快速异构图嵌入(FHGE)框架，旨在实现高效、无需重新训练即可生成元路径引导下的图嵌入。&lt;h4&gt;方法&lt;/h4&gt;FHGE采用了两部分设计：分割和重构模块。该系统利用元路径单元(MPUs)将图形分解为局部和全局组件，并在重组过程中迅速整合相关MPU的节点嵌入，使快速适应特定元路径成为可能；此外还应用了双重注意力机制来增强语义捕捉能力。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，FHGE框架在生成基于元路径引导下的图嵌入以及下游任务（例如链路预测和节点分类）方面既有效又高效，证明其对实时图形分析具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;FHGE框架提供了一种经济高效的解决方案，在处理异构图中的即时查询时能够快速生成所需的图表示，从而在实际应用中展示出良好的性能和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as the state of the art for avariety of graph-related tasks and have been widely used in HeterogeneousGraphs (HetGs), where meta-paths help encode specific semantics between variousnode types. Despite the revolutionary representation capabilities of existingheterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness ofheterogeneity capturing, the huge training costs hinder their practicaldeployment in real-world scenarios that frequently require handling ad-hocqueries with user-defined meta-paths. To address this, we propose FHGE, a FastHeterogeneous Graph Embedding designed for efficient, retraining-freegeneration of meta-path-guided graph embeddings. The key design of the proposedframework is two-fold: segmentation and reconstruction modules. It employsMeta-Path Units (MPUs) to segment the graph into local and global components,enabling swift integration of node embeddings from relevant MPUs duringreconstruction and allowing quick adaptation to specific meta-paths. Inaddition, a dual attention mechanism is applied to enhance semantics capturing.Extensive experiments across diverse datasets demonstrate the effectiveness andefficiency of FHGE in generating meta-path-guided graph embeddings anddownstream tasks, such as link prediction and node classification, highlightingits significant advantages for real-time graph analysis in ad-hoc queries.</description>
      <author>example@mail.com (Xuqi Mao, Zhenying He, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16281v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs</title>
      <link>http://arxiv.org/abs/2502.16610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SPIE Medical Imaging 2025 Runner-up 2025 Robert F. Wagner  All-Conference Best Student Paper Award&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdverX-Ray的方法，用于评估医疗影像的质量。该方法利用对抗生成网络的高频率伪影来训练一个判别器，以检测数据分布的变化（即协变量偏移），从而保证基于深度学习的计算机辅助诊断和检测系统的性能。&lt;h4&gt;背景&lt;/h4&gt;医学影像质量对基于深度学习的计算机辅助诊断和检测系统至关重要。协变量偏移（由不同成像设备或设置引起的细微数据分布变化）会严重降低模型性能，类似于对抗攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、轻量的方法来评估医疗影像的质量，以便在使用计算机辅助诊断和检测模型之前进行质量检查。&lt;h4&gt;方法&lt;/h4&gt;AdverX-Ray是一个图像质量评估层，它通过利用生成器产生的次优输出作为负面样本来微调判别器的能力。该系统训练于特定机器型号的X射线图像补丁，并能判断扫描是否符合训练分布或同一设备在不同设置下采集。&lt;h4&gt;主要发现&lt;/h4&gt;AdverX-Ray与各种异常数据检测方法相比，显著优于现有的技术，在使用64个随机选取的X射线图像补丁时达到了96.2%的平均AUROC。&lt;h4&gt;结论&lt;/h4&gt;该系统的轻量级和快速架构使其适合实时应用，并增强医疗成像系统的可靠性。代码和预训练模型公开可用。&lt;h4&gt;翻译&lt;/h4&gt;确保医学影像的质量与完整性对于保持基于深度学习的计算机辅助诊断（CAD）和检测（CAD）系统中的诊断准确性至关重要。协变量偏移是由不同成像设备或设置引起的数据分布细微变化，可严重降低模型性能，类似于对抗攻击的影响。因此，评估这些图像质量的方法必须是快速且轻量级的，以便在使用CAD模型之前完成。AdverX-Ray通过充当一个影像质量评估层来满足此需求，并有效检测协变量偏移。经过特定型号机器X射线图像补丁训练的AdverX-Ray能够判断扫描是否符合训练分布或同一设备不同设置下采集的图像。与各种异常数据检测方法相比，AdverX-Ray显著优于现有的技术，在使用64个随机选取的X射线图像补丁时达到了96.2%的平均AUROC。该系统轻量级和快速架构适合实时应用，并增强了医疗成像系统的可靠性。代码和预训练模型公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring the quality and integrity of medical images is crucial formaintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosisand Computer-Aided Detection (CAD) systems. Covariate shifts are subtlevariations in the data distribution caused by different imaging devices orsettings and can severely degrade model performance, similar to the effects ofadversarial attacks. Therefore, it is vital to have a lightweight and fastmethod to assess the quality of these images prior to using CAD models.AdverX-Ray addresses this need by serving as an image-quality assessment layer,designed to detect covariate shifts effectively. This Adversarial VariationalAutoencoder prioritizes the discriminator's role, using the suboptimal outputsof the generator as negative samples to fine-tune the discriminator's abilityto identify high-frequency artifacts. Images generated by adversarial networksoften exhibit severe high-frequency artifacts, guiding the discriminator tofocus excessively on these components. This makes the discriminator ideal forthis approach. Trained on patches from X-ray images of specific machine models,AdverX-Ray can evaluate whether a scan matches the training distribution, or ifa scan from the same machine is captured under different settings. Extensivecomparisons with various OOD detection methods show that AdverX-Raysignificantly outperforms existing techniques, achieving a 96.2% average AUROCusing only 64 random patches from an X-ray. Its lightweight and fastarchitecture makes it suitable for real-time applications, enhancing thereliability of medical imaging systems. The code and pretrained models arepublicly available.</description>
      <author>example@mail.com (Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, Fons van der Sommen)</author>
      <guid isPermaLink="false">2502.16610v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models</title>
      <link>http://arxiv.org/abs/2502.16312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种使用基于BERT的模型进行科学命名实体识别（SciNER）的迭代方法，并利用少量高质量的手动标注数据集通过迁移学习来微调预训练模型。&lt;h4&gt;背景&lt;/h4&gt;在缺乏大规模标注数据的情况下，如何有效地提高自然语言处理任务中的预测准确性是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于BERT的SciNER迭代改进方法，并评估其性能。&lt;h4&gt;方法&lt;/h4&gt;采用两种不同的模型（dslim/bert-large-NER和bert-large-cased），通过高质量的手动注释数据集进行微调，然后使用微调后的模型自动标注更大的数据集，并进一步进行多轮微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，基于BERT的模型在预测准确性和F1分数方面有了显著提高，特别是对于较少见的实体类。bert-large-cased模型始终优于dslim/bert-large-NER模型。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了一种有效的SciNER迭代改进技术，在标注数据有限的情况下具有广泛的应用潜力，并且未来的研究可以考虑使用未标记的数据进行微调以及探索更强力的编码器如RoBERTa。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容为：本文提出了一种使用基于BERT的模型进行科学命名实体识别（SciNER）的迭代方法。利用转移学习来对预训练模型进行微调，其中使用了少量但高质量的手动标注数据集。通过使用经过微调后的模型自动标注更大的数据集，并随后进一步多轮微调这一过程得到了反复精炼。我们评估了两种模型（dslim/bert-large-NER和bert-largecased），结果表明后者始终优于前者。该方法在预测准确性和F1分数方面表现出了显著的改进，尤其是对于较少见的实体类。未来的研究可以考虑使用未标注的数据进行微调，并探索更强大的编码器如RoBERTa以及扩展手动注释的范围。这一方法在自然语言处理任务中具有广泛应用潜力，尤其是在数据标签受限的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an iterative approach to performing Scientific NamedEntity Recognition (SciNER) using BERT-based models. We leverage transferlearning to fine-tune pretrained models with a small but high-quality set ofmanually annotated data. The process is iteratively refined by using thefine-tuned model to auto-annotate a larger dataset, followed by additionalrounds of fine-tuning. We evaluated two models, dslim/bert-large-NER andbert-largecased, and found that bert-large-cased consistently outperformed theformer. Our approach demonstrated significant improvements in predictionaccuracy and F1 scores, especially for less common entity classes. Future workcould include pertaining with unlabeled data, exploring more powerful encoderslike RoBERTa, and expanding the scope of manual annotations. This methodologyhas broader applications in NLP tasks where access to labeled data is limited.</description>
      <author>example@mail.com (Kartik Gupta)</author>
      <guid isPermaLink="false">2502.16312v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Verifying Quantized Graph Neural Networks is PSPACE-complete</title>
      <link>http://arxiv.org/abs/2502.16244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究验证量化图神经网络（GNNs）的可行性，其中使用固定宽度算术表示数。&lt;h4&gt;目的&lt;/h4&gt;引入线性约束有效性问题(LVP)来验证GNNs属性，并提供一个从LVP实例到逻辑语言的有效翻译。&lt;h4&gt;方法&lt;/h4&gt;提出了一种证明系统并展示了对于任何合理的激活函数，LVP属于PSPACE复杂度类别。同时表明了PSPACE难度，暗示虽然关于量化GNN的推理是可行的，但仍然是计算上具有挑战性的任务。&lt;h4&gt;主要发现&lt;/h4&gt;验证量化GNNs属性的问题(LVP)被定义为在PSPACE中，并证明了其PSPACE难解性。&lt;h4&gt;结论&lt;/h4&gt;尽管存在一定的计算难度，但对于合理激活函数而言，在PSPACE复杂度类别内解决问题是可能的。这表明对量化GNN的推理虽然具有挑战性但仍可实现。&lt;h4&gt;翻译&lt;/h4&gt;本论文研究使用固定宽度算术表示数的量化图神经网络（GNNs）的验证问题，并引入线性约束有效性(LVP)问题，以验证GNN属性的有效性和提供LVP实例到逻辑语言的高效转换。结果显示，对于任何合理激活函数来说，该问题属于PSPACE复杂度类别；同时证明了其PSPACE难度。表明虽然对量化GNN进行推理是可行的，但计算上仍然具有挑战性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we investigate verification of quantized Graph Neural Networks(GNNs), where some fixed-width arithmetic is used to represent numbers. Weintroduce the linear-constrained validity (LVP) problem for verifying GNNsproperties, and provide an efficient translation from LVP instances into alogical language. We show that LVP is in PSPACE, for any reasonable activationfunctions. We provide a proof system. We also prove PSPACE-hardness, indicatingthat while reasoning about quantized GNNs is feasible, it remains generallycomputationally challenging.</description>
      <author>example@mail.com (Marco Sälzer, François Schwarzentruber, Nicolas Troquard)</author>
      <guid isPermaLink="false">2502.16244v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</title>
      <link>http://arxiv.org/abs/2502.16601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种改进的视觉地方识别(SelaVPR++)方法，通过使用轻量级多尺度卷积(MultiConv)适配器来提高基础模型向视觉地方识别任务适应的有效性和性能。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，利用预训练的视觉基础模型进行视觉位置识别(VPR)可以取得良好的效果。作者之前的工作提出了SelaVPR方法，该方法通过参数高效的方法实现了基础模型向VPR的无缝转换。&lt;h4&gt;目的&lt;/h4&gt;为了提高效率和性能，论文提出了一种SelaVPR的扩展版本——SelaVPR++。&lt;h4&gt;方法&lt;/h4&gt;引入了参数、时间和内存高效的适应策略，利用轻量级多尺度卷积适配器来细化从冻结基础骨干网络获得的中间特征；创新性地提出了更有效的重新排序范式，通过使用紧凑型二进制特征进行初步检索，并采用鲁棒的浮点特征进行重新排序。&lt;h4&gt;主要发现&lt;/h4&gt;提出的相似度约束深度哈希方法可以获得这样的二进制特征，并且可以很容易地集成到VPR流程中；优化了训练策略，统一了几种常见训练数据集的训练协议以更好地培训VPR模型。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明……&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，使用预训练视觉基础模型进行视觉位置识别（VPR）可以实现令人满意的结果。在我们的先前工作中，我们提出了一种方法，将视觉基础模型无缝转换为VPR（SelaVPR）。这种适应方法可以通过参数高效的方法产生全局和局部特征来区分地标，从而用于两阶段的视觉位置识别。尽管SelaVPR已经取得了具有竞争力的效果，但我们认为之前的适应方法在训练时间和GPU内存使用上是低效的，并且重新排序范式在检索延迟和存储使用方面也是昂贵的。为了追求更高的效率和更好的性能，我们提出了SelaVPR的一种扩展版本——SelaVPR++。具体来说，首先设计了一种参数、时间、内存高效的适应方法，该方法利用轻量级多尺度卷积（MultiConv）适配器来细化来自冻结基础骨干网络的中间特征，在训练期间不会反向传播通过基础模型的梯度，并且这种MultiConv适配器可以促进沿空间轴上的特征交互并引入适当的局部先验，从而实现更高的效率和更好的性能。此外，我们提出了一种创新性的重新排序范式以实现更高效的VPR：不依赖于本地特征进行重新排序，这在延迟和存储使用方面会产生巨大的开销，而是采用紧凑的二进制特征用于初步检索，并用鲁棒的浮点（全局）特征用于重新排序。为了获得这些二进制特征，我们提出了一种相似度约束深度哈希方法，可以很容易地整合到VPR流程中。最后，我们改进了我们的训练策略并统一了几种常见训练数据集的训练协议以合并它们以便更好地培训VPR模型。广泛的实验表明……&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies show that the visual place recognition (VPR) method usingpre-trained visual foundation models can achieve promising performance. In ourprevious work, we propose a novel method to realize seamless adaptation offoundation models to VPR (SelaVPR). This method can produce both global andlocal features that focus on discriminative landmarks to recognize places fortwo-stage VPR by a parameter-efficient adaptation approach. Although SelaVPRhas achieved competitive results, we argue that the previous adaptation isinefficient in training time and GPU memory usage, and the re-ranking paradigmis also costly in retrieval latency and storage usage. In pursuit of higherefficiency and better performance, we propose an extension of the SelaVPR,called SelaVPR++. Concretely, we first design a parameter-, time-, andmemory-efficient adaptation method that uses lightweight multi-scaleconvolution (MultiConv) adapters to refine intermediate features from thefrozen foundation backbone. This adaptation method does not back-propagategradients through the backbone during training, and the MultiConv adapterfacilitates feature interactions along the spatial axes and introduces properlocal priors, thus achieving higher efficiency and better performance.Moreover, we propose an innovative re-ranking paradigm for more efficient VPR.Instead of relying on local features for re-ranking, which incurs huge overheadin latency and storage, we employ compact binary features for initial retrievaland robust floating-point (global) features for re-ranking. To obtain suchbinary features, we propose a similarity-constrained deep hashing method, whichcan be easily integrated into the VPR pipeline. Finally, we improve ourtraining strategy and unify the training protocol of several common trainingdatasets to merge them for better training of VPR models. Extensive experimentsshow that ......</description>
      <author>example@mail.com (Feng Lu, Tong Jin, Xiangyuan Lan, Lijun Zhang, Yunpeng Liu, Yaowei Wang, Chun Yuan)</author>
      <guid isPermaLink="false">2502.16601v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas</title>
      <link>http://arxiv.org/abs/2502.15907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的自动化洪水区域识别模型，即Graph Attention Convolutional U-NET (GAC-UNET)，该模型结合了图注意力机制和Chebyshev层，并在实验中显示出优于其他方法的表现。&lt;h4&gt;背景&lt;/h4&gt;近年来，由于人为气候变化和无规划的城市建设导致洪灾事件增多。准确地识别洪水区域对于有效的灾害管理和城市规划至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于图神经网络的方法来自动识别洪水区域，利用转移学习和模型重新编程以提高洪水区域分割模型的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用Graph Attention Convolutional U-NET (GAC-UNET) 模型，该模型将图注意力机制和Chebyshev层融入到U-Net架构中，并探索了转移学习的应用。&lt;h4&gt;主要发现&lt;/h4&gt;提出的GAC-UNET模型在mAP、Dice分数和IoU指标上分别达到了91%，94%和89%，超过了其他方法，显示出了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;该研究为洪水易发区域未来的基础设施规划提供了有价值的见解，并表明图神经网络可以在自动化识别洪水区域方面提供改进的机会。&lt;h4&gt;翻译&lt;/h4&gt;不断加剧的人类活动导致的气候变化以及未规划的城市建设在过去几年里增加了洪灾事件。准确地辨识受影响地区对于有效的灾害管理和城市规划至关重要。虽然有少量研究采用卷积神经网络和基于变压器的语义分割技术来识别航空影像中的洪水区域，但图神经网络的发展创造了许多改进的机会。这篇论文提出了一种创新的方法——基于图神经网络（GAC-UNET）模型，用于自动化地辨识洪水区域，并在实验中展示了显著优于其他方法的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing impact of human-induced climate change and unplanned urbanconstructions has increased flooding incidents in recent years. Accurateidentification of flooded areas is crucial for effective disaster managementand urban planning. While few works have utilized convolutional neural networksand transformer-based semantic segmentation techniques for identifying floodedareas from aerial footage, recent developments in graph neural networks havecreated improvement opportunities. This paper proposes an innovative approach,the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neuralnetworks for automated identification of flooded areas. The model incorporatesa graph attention mechanism and Chebyshev layers into the U-Net architecture.Furthermore, this paper explores the applicability of transfer learning andmodel reprogramming to enhance the accuracy of flood area segmentation models.Empirical results demonstrate that the proposed GAC-UNET model, outperformsother approaches with 91\% mAP, 94\% dice score, and 89\% IoU, providingvaluable insights for informed decision-making and better planning of futureinfrastructures in flood-prone areas.</description>
      <author>example@mail.com (Muhammad Umair Danish, Madhushan Buwaneswaran, Tehara Fonseka, Katarina Grolinger)</author>
      <guid isPermaLink="false">2502.15907v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control</title>
      <link>http://arxiv.org/abs/2502.16475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;单张图像3D生成已成为一个重要的研究领域，在虚拟现实、三维建模和数字内容创作中起着重要作用。&lt;h4&gt;问题&lt;/h4&gt;现有的方法面临多视角几何一致性不足及生成过程可控性有限的问题，这些问题显著限制了它们的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法Drage3D来解决这些挑战，该方法利用3D高斯斑点实现具有几何一致性和可控制性的3D生成。&lt;h4&gt;方法&lt;/h4&gt;{'Anchor-Gaussian变分自编码器(AGSVAE)': '将点云和单张图像编码成锚定潜在变量，并通过解码锚定潜在变量生成3DGS，从而实现高效的潜在空间生成。', 'Seed-Point-Driven策略': '该策略包括两步：首先生成稀疏种子点作为粗糙的几何表示；其次通过Seed-Anchor映射模块将这些种子点映射到锚定潜在变量。这种策略确保了几何一致性，并且用户可以直观地拖动种子点来变形最终3DGS几何，变化会通过锚定潜在变量传播。', '无需2D扩散先验': '我们是首个实现不依赖于2D扩散先验的几何可控制性3D高斯生成和编辑的方法。'}&lt;h4&gt;主要发现&lt;/h4&gt;Drage3D在保持高质量3D生成的同时，实现了多视角几何一致性和用户可控性。&lt;h4&gt;结论&lt;/h4&gt;Drage3D为单图像到三维生成开辟了新的道路，显著提高了现有技术的实用性。&lt;h4&gt;翻译&lt;/h4&gt;单张图像3D生成已作为一项重要研究课题崛起，在虚拟现实、3D建模和数字内容创建中扮演着至关重要的角色。然而，现存的方法面临着诸如缺乏多视角几何一致性以及在生成过程中可控性有限等挑战，这些严重限制了它们的实用性。为了解决这些问题，我们引入了一种名为Drage3D的新方法，利用三维高斯斑点（3DGS）实现了具有几何一致性和可控制性的3D生成。通过Anchor-Gaussian变分自编码器(AGSVAE)，该模型将点云和单张图像编码为锚定潜在变量，并通过这些潜在变量解码出3DGS，从而实现高效的潜在空间生成。为了达成多视角几何一致性及可控性生成目标，我们提出了一种Seed-Point驱动策略：首先生成稀疏种子点作为粗糙的几何表示；其次通过Seed-Anchor映射模块将它们映射到锚定潜在变量。这种策略确保了几何一致性，并且用户可以直观地拖动这些种子点来变形最终3DGS几何，变化会经过锚定潜在变量传播。据我们所知，我们在不依赖于2D扩散先验的情况下首次实现了具有可控制性三维高斯生成和编辑的方法，同时保持了与最先进方法相当的3D生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-image 3D generation has emerged as a prominent research topic, playinga vital role in virtual reality, 3D modeling, and digital content creation.However, existing methods face challenges such as a lack of multi-viewgeometric consistency and limited controllability during the generationprocess, which significantly restrict their usability. % To tackle thesechallenges, we introduce Dragen3D, a novel approach that achieves geometricallyconsistent and controllable 3D generation leveraging 3D Gaussian Splatting(3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GSVAE), which encodes a point cloud and a single image into anchor latents anddecode these latents into 3DGS, enabling efficient latent-space generation. Toenable multi-view geometry consistent and controllable generation, we propose aSeed-Point-Driven strategy: first generate sparse seed points as a coarsegeometry representation, then map them to anchor latents via the Seed-AnchorMapping Module. Geometric consistency is ensured by the easily learned sparseseed points, and users can intuitively drag the seed points to deform the final3DGS geometry, with changes propagated through the anchor latents. To the bestof our knowledge, we are the first to achieve geometrically controllable 3DGaussian generation and editing without relying on 2D diffusion priors,delivering comparable 3D generation quality to state-of-the-art methods.</description>
      <author>example@mail.com (Jinbo Yan, Alan Zhao, Yixin Hu)</author>
      <guid isPermaLink="false">2502.16475v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Graph Self-Supervised Learning with Learnable Structural and Positional Encodings</title>
      <link>http://arxiv.org/abs/2502.16233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by The World Wide Web Conference (WWW) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了新型图自监督学习框架GenHopNet，该框架旨在解决传统GSSL难以捕捉复杂结构特征的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的图自监督学习（GSSL）在捕获复杂的结构性质方面存在困难。这主要是由于两个原因：一是常规图神经网络（GNNs）无法很好地表示复杂的拓扑特征；二是自监督学习仅仅关注最终的图表示，而忽略了整个过程中的结构信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服这些限制，并增强在区分具有相似局部但不同全局拓扑的图形的能力。&lt;h4&gt;方法&lt;/h4&gt;引入了GenHopNet框架，这是一个融合$k$-跳消息传递机制的GNN框架。此外还提出了一个既考虑结构性又注重位置信息的自监督学习框架，该框架能够在整个学习过程中整合图的拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明GenHopNet超越了经典的Weisfeiler-Lehman（WL）测试在图同构上的表达能力，并且实验结果显示这种方法在图分类数据集上优于现有方法，特别是在那些用于测试结构敏感性的数据集上表现尤为突出。同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的GenHopNet框架及其相关的自监督学习策略显著增强了GSSL区分具有相似局部但不同全局拓扑的图形的能力。&lt;h4&gt;翻译&lt;/h4&gt;传统的图自我监督学习(GSSL)在捕捉复杂的结构特性方面存在困难，这主要是由于两个因素：(1) 常规图神经网络（GNNs）无法充分代表复杂的拓扑特征；(2) 自我监督学习仅关注最终的图表示。为了解决这些问题，我们提出了一个新的框架GenHopNet，它是一个融合了$k$-跳消息传递机制的GNN架构，增强了捕捉局部结构信息的能力而无需显式地提取子结构。理论证明表明，GenHopNet在表达能力上超越了经典的Weisfeiler-Lehman (WL) 测试用于图同构测试。此外，我们还提出了一种基于位置和结构感知的GSSL框架，在整个学习过程中整合拓扑信息，使模型能够同时敏感于图形的拓扑且对特定的结构及特征增强保持不变性。在包括旨在测试结构敏感性的图分类数据集上的全面实验表明，我们的方法在性能上始终优于现有的方法，并且计算效率高。我们的重要贡献在于大幅提升了GSSL区分具有相似局部但不同全局拓扑的图形的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714745&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Graph Self-Supervised Learning (GSSL) struggles to capturecomplex structural properties well. This limitation stems from two mainfactors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) inrepresenting sophisticated topological features, and (2) the focus ofself-supervised learning solely on final graph representations. To addressthese issues, we introduce \emph{GenHopNet}, a GNN framework that integrates a$k$-hop message-passing scheme, enhancing its ability to capture localstructural information without explicit substructure extraction. Wetheoretically demonstrate that \emph{GenHopNet} surpasses the expressiveness ofthe classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore,we propose a structural- and positional-aware GSSL framework that incorporatestopological information throughout the learning process. This approach enablesthe learning of representations that are both sensitive to graph topology andinvariant to specific structural and feature augmentations. Comprehensiveexperiments on graph classification datasets, including those designed to teststructural sensitivity, show that our method consistently outperforms theexisting approaches and maintains computational efficiency. Our worksignificantly advances GSSL's capability in distinguishing graphs with similarlocal structures but different global topologies.</description>
      <author>example@mail.com (Asiri Wijesinghe, Hao Zhu, Piotr Koniusz)</author>
      <guid isPermaLink="false">2502.16233v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>COMPASS: Cross-embodiment Mobility Policy via Residual RL and Skill Synthesis</title>
      <link>http://arxiv.org/abs/2502.16372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的工作流程COMPASS，旨在开发跨机器人形态的移动策略，通过结合模仿学习（IL）、残差强化学习（RL）和策略蒸馏的方法来克服现有方法在面对新硬件平台或不同环境时所面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着机器人应用领域的扩展，通用且可应用于多种物理形态的移动策略变得日益重要。传统的移动栈虽然在特定平台上有效，但在新的机器人形态上难以大规模部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够解决现有方法中普遍存在的协变量偏移、稀疏采样和环境适应性问题的工作流程，实现高效的跨身体形式（embodiment）的移动策略开发。&lt;h4&gt;方法&lt;/h4&gt;通过模仿学习在移动机器人上训练基础模型，结合世界模型与移动策略；使用残差强化学习进一步微调特定身体形态的策略，并利用预训练表示提高采样效率以处理各种物理约束和传感器模式；最后通过策略蒸馏将这些专家策略合并为单一稳健的跨身形态策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，COMPASS能够有效扩展至不同的机器人平台，在适应不同环境配置的同时保持高成功率（相较于预训练模仿学习策略高出约5倍）。&lt;h4&gt;结论&lt;/h4&gt;提出的框架提供了高效且可扩展的方法来实现跨身体形式的移动策略，使具有不同设计的机器人能够在复杂场景中安全有效地导航。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人的广泛应用领域越来越多，通用化的跨形态机动性策略变得越来越重要。经典机动栈在特定平台上已经证明是有效的，但在新的实体上进行规模化部署存在挑战。基于学习的方法（如模仿学习和强化学习）提供了解决方案，但它们也面临协变量漂移、大型环境中的稀疏采样以及实体特异性约束的问题。本文介绍了COMPASS，这是一个通过结合模仿学习、残差RL和策略蒸馏来开发跨实体机动性策略的新工作流程。我们首先在移动机器人上进行模仿学习，并利用易于访问的教师策略训练一个基础模型，该模型将世界模型与机动策略相结合。在此基础上，我们使用残差RL来微调特定实体上的策略，利用预训练表示提高采样效率，在处理各种物理约束和传感器模式方面更加高效。最后，通过策略蒸馏将这些实体专家策略合并为单一稳健的跨实体策略。实验证明了COMPASS能够有效地在不同的机器人平台上扩展，同时保持对不同环境配置的适应性，实现了一个成功率比预训练模仿学习策略高约5倍的一般策略。该框架提供了一种高效且可扩展的方法来解决跨实体机动问题，使设计不同的机器人能够在复杂场景中安全和有效地导航。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robots are increasingly deployed in diverse application domains,generalizable cross-embodiment mobility policies are increasingly essential.While classical mobility stacks have proven effective on specific robotplatforms, they pose significant challenges when scaling to new embodiments.Learning-based methods, such as imitation learning (IL) and reinforcementlearning (RL), offer alternative solutions but suffer from covariate shift,sparse sampling in large environments, and embodiment-specific constraints.  This paper introduces COMPASS, a novel workflow for developingcross-embodiment mobility policies by integrating IL, residual RL, and policydistillation. We begin with IL on a mobile robot, leveraging easily accessibleteacher policies to train a foundational model that combines a world model witha mobility policy. Building on this base, we employ residual RL to fine-tuneembodiment-specific policies, exploiting pre-trained representations to improvesampling efficiency in handling various physical constraints and sensormodalities. Finally, policy distillation merges these embodiment-specialistpolicies into a single robust cross-embodiment policy.  We empirically demonstrate that COMPASS scales effectively across diverserobot platforms while maintaining adaptability to various environmentconfigurations, achieving a generalist policy with a success rate approximately5X higher than the pre-trained IL policy. The resulting framework offers anefficient, scalable solution for cross-embodiment mobility, enabling robotswith different designs to navigate safely and efficiently in complex scenarios.</description>
      <author>example@mail.com (Wei Liu, Huihua Zhao, Chenran Li, Joydeep Biswas, Soha Pouya, Yan Chang)</author>
      <guid isPermaLink="false">2502.16372v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Robust Dynamic Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2502.16129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Robust Dynamic Facial Expression Recognition (RDFER)的新方法，该方法旨在解决动态面部表情识别中的硬样本和噪声样本共存的问题。&lt;h4&gt;背景&lt;/h4&gt;目前关于动态面部表情识别的研究主要集中在学习在有噪音或难以处理的数据下的表示形式上，但如何同时处理这两种类型的数据仍然是一个未解之谜。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有问题并提高模型的鲁棒性，该研究设计了一个能够区分硬样本和噪声样本的方法，并提出了一种关键表情重采样框架以及双重流分层网络来增强模型对主要表达的理解能力。&lt;h4&gt;方法&lt;/h4&gt;通过评估模型在不同视频片段上的预测一致性来识别硬样本和噪声样本。采用关键表情重新采样的框架来减少非目标表情带来的干扰，同时使用双序列模型分离短期面部运动与长期情绪变化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在DFEW和FERV39K等基准数据集上进行了广泛的实验，并证明了其优于现有的最先进的动态面部表情识别方法。&lt;h4&gt;结论&lt;/h4&gt;该工作对于促进动态面部表情识别领域的进一步发展具有重要意义，特别是在噪声一致的鲁棒学习方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：动态面部表情识别（DFER）的研究是一个新兴领域，涉及视频数据中自动识别面部表情。尽管现有研究主要集中在处理噪音和难以处理样本的学习表示上，但如何同时解决这两种类型的问题仍然没有得到很好的解决。为了克服这一挑战，本文提出了一种区分硬样本和噪声样本的稳健方法。通过评估模型在不同采样片段上的预测一致性来实现这一点，并随后采用强化学习难例和削弱噪声影响的方法。此外，为识别视频中的主要表情并增强模型表示学习的能力，提出了关键表情重采样的框架以及双流分层网络，即鲁棒动态面部表情识别（RDFER）。该方法通过在DFEW和FERV39K等基准数据集上的广泛实验展示出优于现有最先进的DFER方法的表现。综合分析提供了关于所提一致性的有价值见解与观察结果。这项工作对动态面部表情识别领域具有重要意义，并促进了噪声一致性鲁棒学习领域的进一步发展。代码可以从[https://github.com/Cross-Innovation-Lab/RDFER]获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of Dynamic Facial Expression Recognition (DFER) is a nascent fieldof research that involves the automated recognition of facial expressions invideo data. Although existing research has primarily focused on learningrepresentations under noisy and hard samples, the issue of the coexistence ofboth types of samples remains unresolved. In order to overcome this challenge,this paper proposes a robust method of distinguishing between hard and noisysamples. This is achieved by evaluating the prediction agreement of the modelon different sampled clips of the video. Subsequently, methodologies thatreinforce the learning of hard samples and mitigate the impact of noisy samplescan be employed. Moreover, to identify the principal expression in a video andenhance the model's capacity for representation learning, comprising a keyexpression re-sampling framework and a dual-stream hierarchical network isproposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The keyexpression re-sampling framework is designed to identify the key expression,thereby mitigating the potential confusion caused by non-target expressions.RDFER employs two sequence models with the objective of disentanglingshort-term facial movements and long-term emotional changes. The proposedmethod has been shown to outperform current State-Of-The-Art approaches in DFERthrough extensive experimentation on benchmark datasets such as DFEW andFERV39K. A comprehensive analysis provides valuable insights and observationsregarding the proposed agreement. This work has significant implications forthe field of dynamic facial expression recognition and promotes the furtherdevelopment of the field of noise-consistent robust learning in dynamic facialexpression recognition. The code is available from[https://github.com/Cross-Innovation-Lab/RDFER].</description>
      <author>example@mail.com (Feng Liu, Hanyang Wang, Siyuan Shen)</author>
      <guid isPermaLink="false">2502.16129v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Asteroid shape inversion with light curves using deep learning</title>
      <link>http://arxiv.org/abs/2502.16455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;小行星形状反演利用光度数据一直是行星科学和天文研究的关键领域，但现有方法需要大量的迭代计算，使过程耗时且容易陷入局部最优解。我们直接通过深度神经网络建立了光度数据与形状分布之间的映射关系，并使用3D点云表示小行星的形状。&lt;h4&gt;背景&lt;/h4&gt;利用光度数据进行小行星形状反演是天文研究中的一个重要课题。然而，现有的方法需要大量的迭代计算过程耗时且容易陷入局部最优解。&lt;h4&gt;目的&lt;/h4&gt;通过深度学习建立光度数据与小行星形状分布之间的直接映射关系，并开发一种预测非凸小行星凹陷区域的新方法。&lt;h4&gt;方法&lt;/h4&gt;采用3D点云表示小行星的形状，利用非凸小行星光线曲线与其凸包之间的偏差来预测非凸小行星上的凹陷区域。使用Chamfer距离评估传统方法与新方法的结果，并利用Lowell天文台观测数据验证该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用深度学习技术，我们能够更有效地反演小行星的形状，特别是在处理特殊形状时表现更好。对于凸包上凹陷区域的检测，预测结果的IoU达到0.89，表明了方法的高度准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果显示该方法具有很强的鲁棒性和适应性，并且优于传统的光度曲线拟合方法。&lt;h4&gt;翻译&lt;/h4&gt;使用基于深度学习的方法进行小行星形状反演的研究。这种方法通过直接映射关系减少了计算时间和局部最优的问题，提高了处理非凸小行星时的效果和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Asteroid shape inversion using photometric data has been a key area of studyin planetary science and astronomical research.However, the current methods forasteroid shape inversion require extensive iterative calculations, making theprocess time-consuming and prone to becoming stuck in local optima. We directlyestablished a mapping between photometric data and shape distribution throughdeep neural networks. In addition, we used 3D point clouds to representasteroid shapes and utilized the deviation between the light curves ofnon-convex asteroids and their convex hulls to predict the concave areas ofnon-convex asteroids. We compared the results of different shape models usingthe Chamfer distance between traditional methods and ours and found that ourmethod performs better, especially when handling special shapes. For thedetection of concave areas on the convex hull, the intersection over union(IoU) of our predictions reached 0.89. We further validated this method usingobservational data from the Lowell Observatory to predict the convex shapes ofthe asteroids 3337 Milo and 1289 Kuta, and conducted light curve fittingexperiments. The experimental results demonstrated the robustness andadaptability of the method</description>
      <author>example@mail.com (YiJun Tang, ChenChen Ying, ChengZhe Xia, XiaoMing Zhang, XiaoJun Jiang)</author>
      <guid isPermaLink="false">2502.16455v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Interpreting core forms of urban morphology linked to urban functions with explainable graph neural network</title>
      <link>http://arxiv.org/abs/2502.16210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究提出了核心城市形态表示的概念，发展了一种可解释的深度学习框架，用于将复杂的都市形式解析为一种新的表示（CoMo），从而揭示了都市功能与形态之间的联系。&lt;h4&gt;背景&lt;/h4&gt;理解城市的高阶关系对于可持续城市发展至关重要。然而，准确地描述复杂的城市形式并使其易于人类理解是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;提出核心城市形态表示的概念，并开发可解释的深度学习框架将复杂的都市形式解析为新的表示（CoMo）。&lt;h4&gt;方法&lt;/h4&gt;利用一个稳定的加权F1分数为89.14%的经过训练的深度学习模型进行解释，以揭示基于核心城市形态表示的城市功能与形态之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;['波士顿的研究显示，在个人建筑、街区和社区层面上的核心城市形式对相应城市功能非常重要。', '住宅核心形式沿都市脊线呈现出渐进的形态模式，并且这种模式与从市中心到郊区的过渡一致。', '城市形态直接影响土地使用效率，二者具有显著的相关性（R2=0.721, p&lt;0.001）']&lt;h4&gt;结论&lt;/h4&gt;CoMo能够可解释地表示都市形式，提供经典城市位置理论的支持，并为数字孪生体提供了机制见解。&lt;h4&gt;翻译&lt;/h4&gt;理解城市形态与功能之间的高阶关系对于建模可持续城市的内在机制至关重要。然而，准确描述复杂的城市形式并使其易于人类理解是一项挑战。本研究提出核心城市形态表示的概念，并开发了一种可解释的深度学习框架，用于将复杂的都市形式解析为新的表示（CoMo）。通过解释经过良好训练的深度学习模型（稳定的加权F1分数89.14%），CoMo为揭示基于核心城市形态表示的城市功能与形态之间的联系提供了一个有希望的方法。以波士顿作为研究区域，分析了个人建筑、街区和社区层面的核心城市形式对相应城市功能的重要性。住宅核心形式沿都市脊线呈现出渐进的形态模式，并且这种模式与从市中心到郊区的过渡一致。此外，研究表明城市形态直接影响土地使用效率，并具有显著的相关性（R2=0.721, p&lt;0.001）。总的来说，CoMo能够可解释地表示都市形式，提供经典城市位置理论的支持，并为数字孪生体提供了机制见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the high-order relationship between urban form and function isessential for modeling the underlying mechanisms of sustainable urban systems.Nevertheless, it is challenging to establish an accurate data representationfor complex urban forms that are readily explicable in human terms. This studyproposed the concept of core urban morphology representation and developed anexplainable deep learning framework for explicably symbolizing complex urbanforms into the novel representation, which we call CoMo. By interpretating thewell-trained deep learning model with a stable weighted F1-score of 89.14%,CoMo presents a promising approach for revealing links between urban functionand urban form in terms of core urban morphology representation. Using Bostonas a study area, we analyzed the core urban forms at the individual-building,block, and neighborhood level that are important to corresponding urbanfunctions. The residential core forms follow a gradual morphological patternalong the urban spine, which is consistent with a center-urban-suburbantransition. Furthermore, we prove that urban morphology directly affects landuse efficiency, which has a significantly strong correlation with the location(R2=0.721, p&lt;0.001). Overall, CoMo can explicably symbolize urban forms,provide evidence for the classic urban location theory, and offer mechanisticinsights for digital twins.</description>
      <author>example@mail.com (Dongsheng Chen, Yu Feng, Xun Li, Mingya Qu, Peng Luo, Liqiu Meng)</author>
      <guid isPermaLink="false">2502.16210v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet</title>
      <link>http://arxiv.org/abs/2502.16116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了两种新的深度学习架构，旨在通过整合多变量气象站数据和雷达数据来提高降水短时预报的性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于数据驱动和深度学习的方法在降水短时预报中引起了广泛关注，并取得了显著成果。然而，许多现有的模型未能充分利用广泛可用的大气信息，主要依赖于降水量数据。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过引入新的深度学习架构来改善降水短时预报的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了两种新型深度学习架构：SmaAt-fUsion 和 SmaAt-Krige-GNet。SmaAt-fUsion 扩展了 SmaAt-UNet 架构，通过卷积层将气象站数据整合到网络瓶颈部分。而 SmaAt-Krige-GNet 结合了降水图与使用 Kriging 方法处理的气象站数据，生成特定变量的地图，并在基于 SmaAt-GNet 的双编码器架构中利用这些地图进行多级数据集成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在低降水量场景下，SmaAt-Krige-GNet 比仅依赖降水雷达数据的标准 SmaAt-UNet 表现更好；而在低和高降水量场景下，SmaAt-fUsion 超过了 SmaAt-UNet。&lt;h4&gt;结论&lt;/h4&gt;本研究强调了将离散的气象站数据整合到深度学习天气短时预报模型中以提高性能的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经涵盖了上述各个要点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, data-driven, deep learning-based approaches forprecipitation nowcasting have attracted significant attention, showingpromising results. However, many existing models fail to fully exploit theextensive atmospheric information available, relying primarily on precipitationdata alone. This study introduces two novel deep learning architectures,SmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhanceprecipitation nowcasting by integrating multi-variable weather station datawith radar datasets. By leveraging additional meteorological information, thesemodels improve representation learning in the latent space, resulting inenhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNetframework by incorporating weather station data through a convolutional layer,integrating it into the bottleneck of the network. Conversely, theSmaAt-Krige-GNet model combines precipitation maps with weather station dataprocessed using Kriging, a geo-statistical interpolation method, to generatevariable-specific maps. These maps are then utilized in a dual-encoderarchitecture based on SmaAt-GNet, allowing multi-level data integration.Experimental evaluations were conducted using four years (2016--2019) ofweather station and precipitation radar data from the Netherlands. Resultsdemonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, whichrelies solely on precipitation radar data, in low precipitation scenarios,while SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitationscenarios. This highlights the potential of incorporating discrete weatherstation data to enhance the performance of deep learning-based weathernowcasting models.</description>
      <author>example@mail.com (Aleksej Cornelissen, Jie Shi, Siamak Mehrkanoon)</author>
      <guid isPermaLink="false">2502.16116v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Ultra fast, event-by-event heavy-ion simulations for next generation experiments</title>
      <link>http://arxiv.org/abs/2502.16330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新型深度生成框架，利用概率扩散模型进行重离子碰撞事件级别的快速模拟。&lt;h4&gt;背景&lt;/h4&gt;当前的物理实验中，对重离子碰撞数据的模拟是一项耗时的任务。传统方法难以满足大规模和高精度需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、精准地模拟重离子碰撞输出的新框架。&lt;h4&gt;方法&lt;/h4&gt;该框架基于概率扩散模型，结合归一化流条件生成器和粒子点云合成模块，从UrQMD级联数据中学习并生成包含26种不同介子物种的完整碰撞事件输出。&lt;h4&gt;主要发现&lt;/h4&gt;提出的条件点云扩散模型能够产生逼真的重离子碰撞结果，成功再现了UrQMD分布中的多重性、动量和快度特性。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅在质量和速度上优于现有方法，还为逆向问题求解和参数估计提供了便利，并且可以轻松适应加速任何事件级别的模型计算或探测器模拟任务。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的深度生成框架，利用概率扩散模型进行超快速、逐事件重离子碰撞输出模拟。此新框架基于UrQMD级联数据训练，以生成包含26个不同介子物种的完整碰撞事件输出。每个点由粒子动量向量及其对应种类信息（ID）定义。架构中整合了基于归一化流的条件生成器，将全局事件特征编码为潜在矢量，并利用扩散模型根据此条件合成粒子点云。详细描述了模型及深入分析其性能。有条件点云扩散模型学习产生真实碰撞事件输出的颗粒物，成功再现了UrQMD分布中的多重性、动量和快度特性。灵活的点云表示方法保留了完整的事件级粒度，直接应用于逆向问题求解和参数估计任务，并且易于适应加速任何逐事件模型计算或探测器模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel deep generative framework that uses probabilisticdiffusion models for ultra fast, event-by-event simulations of heavy-ioncollision output. This new framework is trained on UrQMD cascade data togenerate a full collision event output containing 26 distinct hadron species.The output is represented as a point cloud, where each point is defined by aparticle's momentum vector and its corresponding species information (ID). Ourarchitecture integrates a normalizing flow-based condition generator thatencodes global event features into a latent vector, and a diffusion model thatsynthesizes a point cloud of particles based on this condition. A detaileddescription of the model and an in-depth analysis of its performance isprovided. The conditional point cloud diffusion model learns to generaterealistic output particles of collision events which successfully reproduce theUrQMD distributions for multiplicity, momentum and rapidity of each hadrontype. The flexible point cloud representation of the event output preservesfull event-level granularity, enabling direct application to inverse problemsand parameter estimation tasks while also making it easily adaptable foraccelerating any event-by-event model calculation or detector simulation.</description>
      <author>example@mail.com (Manjunath Omana Kuttan, Kai Zhou, Jan Steinheimer, Horst Stoecker)</author>
      <guid isPermaLink="false">2502.16330v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>BalanceBenchmark: A Survey for Multimodal Imbalance Learning</title>
      <link>http://arxiv.org/abs/2502.10816v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个全面评估多模态不平衡算法的基准测试平台BalanceBenchmark，包括多个常用的数据集和评价指标，并开发了一个标准化实验流程的工具包。&lt;h4&gt;背景&lt;/h4&gt;多模态学习通过整合不同模态的信息得到了广泛关注。然而，该领域常受到模态失衡问题的影响，即某些模态过于主导而其他模态被利用不足。&lt;h4&gt;目的&lt;/h4&gt;系统分类主流多模态不平衡算法，并提供一个全面的评估方法以促进研究的发展。&lt;h4&gt;方法&lt;/h4&gt;引入BalanceBenchmark基准测试平台和标准化实验流程工具包，通过多个数据集从性能、失衡程度及复杂性三个角度进行综合评价。&lt;h4&gt;主要发现&lt;/h4&gt;基于实验结果，识别出不同方法在性能、平衡度以及计算复杂性方面的特征与优势。&lt;h4&gt;结论&lt;/h4&gt;此分析有望激发未来研究中更有效的不平衡问题解决方案，并可能影响基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习因其整合多种信息模态的能力而备受关注。然而，它常因模态失衡问题受到限制，即某些模态主导其他未充分利用的模态。尽管最近的研究提出了各种方法来缓解该问题，但缺乏全面且公平的比较。本文将主流的多模态不平衡算法基于其减轻不平衡的方法分为四大类，并引入BalanceBenchmark基准测试平台以促进综合评估。为确保公平比较，开发了标准化实验流程工具包。通过使用BalanceBenchmark进行实验，识别出不同方法组在性能、平衡度和计算复杂性方面的特征与优势。我们期待这种分析能够启发未来更高效地解决不平衡问题的方法，并可能影响基础模型的发展。工具代码可在https://github.com/GeWu-Lab/BalanceBenchmark获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gewu-lab/balancebenchmark&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning has gained attention for its capacity to integrateinformation from different modalities. However, it is often hindered by themultimodal imbalance problem, where certain modality dominates while othersremain underutilized. Although recent studies have proposed various methods toalleviate this problem, they lack comprehensive and fair comparisons. In thispaper, we systematically categorize various mainstream multimodal imbalancealgorithms into four groups based on the strategies they employ to mitigateimbalance. To facilitate a comprehensive evaluation of these methods, weintroduce BalanceBenchmark, a benchmark including multiple widely usedmultidimensional datasets and evaluation metrics from three perspectives:performance, imbalance degree, and complexity. To ensure fair comparisons, wehave developed a modular and extensible toolkit that standardizes theexperimental workflow across different methods. Based on the experiments usingBalanceBenchmark, we have identified several key insights into thecharacteristics and advantages of different method groups in terms ofperformance, balance degree and computational complexity. We expect suchanalysis could inspire more efficient approaches to address the imbalanceproblem in the future, as well as foundation models. The code of the toolkit isavailable at https://github.com/GeWu-Lab/BalanceBenchmark.</description>
      <author>example@mail.com (Shaoxuan Xu, Menglu Cui, Chengxiang Huang, Hongfa Wang, Di Hu)</author>
      <guid isPermaLink="false">2502.10816v3</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Co-evolution-based Metal-binding Residue Prediction with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.16189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为MBGNN的金属结合预测模型，该模型利用共进化残基网络并使用图神经网络来捕捉蛋白质结构中的复杂依赖关系。&lt;h4&gt;背景&lt;/h4&gt;预测金属结合位点及其对应的金属类型在计算结构生物学中具有挑战性，因为涉及到蛋白质结构和相互作用的复杂性。传统的方法无法有效捕获驱动这些相互作用的复杂进化关系，而基于共进化的最近方法没有充分考虑整个共进化残基网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预测模型MBGNN，以改进金属结合位点及其相关金属类型的预测性能。&lt;h4&gt;方法&lt;/h4&gt;MBGNN利用完整的共进化残基网络并通过图神经网络有效捕捉蛋白质结构中的复杂依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MBGNN在公共数据集上的表现优于现有的基于共进化的金属结合预测方法，并且在序列基础上的方法中具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该模型展示了将共进化见解与高级机器学习技术相结合的潜力，有助于深入理解蛋白质-金属相互作用。MBGNN的代码可在GitHub上公开获得。&lt;h4&gt;翻译&lt;/h4&gt;在计算结构生物学领域，由于蛋白质结构和相互作用的复杂性，预测金属结合位点及其对应的金属类型极具挑战性。传统基于序列和结构的方法无法有效捕捉这些交互背后的复杂进化关系以促进理解，而最近基于共进化的技术未能充分考虑整个共进化残基网络的结构。本文提出了一种名为MBGNN（Metal-Binding Graph Neural Network）的新方法，该模型利用了完整的共进化残基网络并通过图神经网络有效地捕获蛋白质结构中的复杂依赖关系，以提高共进化金属结合位点及其相关金属类型的预测能力。公共数据集上的实验结果表明，MBGNN在基于共进化的金属结合预测方法中表现出色，并且也与最近的序列基础方法相媲美，展示了将共进化见解与高级机器学习相结合的潜力，有助于深入了解蛋白质-金属相互作用。MBGNN代码可以在https://github.com/SRastegari/MBGNN上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computational structural biology, predicting metal-binding sites and theircorresponding metal types is challenging due to the complexity of proteinstructures and interactions. Conventional sequence- and structure-basedprediction approaches cannot capture the complex evolutionary relationshipsdriving these interactions to facilitate understanding, while recentco-evolution-based approaches do not fully consider the entire structure of theco-evolved residue network. In this paper, we introduce MBGNN (Metal-BindingGraph Neural Network) that utilizes the entire co-evolved residue network andeffectively captures the complex dependencies within protein structures viagraph neural networks to enhance the prediction of co-evolved metal-bindingresidues and their associated metal types. Experimental results on a publicdataset show that MBGNN outperforms existing co-evolution-based metal-bindingprediction methods, and it is also competitive against recent sequence-basedmethods, showing the potential of integrating co-evolutionary insights withadvanced machine learning to deepen our understanding of protein-metalinteractions. The MBGNN code is publicly available athttps://github.com/SRastegari/MBGNN.</description>
      <author>example@mail.com (Sayedmohammadreza Rastegari, Sina Tabakhi, Xianyuan Liu, Wei Sang, Haiping Lu)</author>
      <guid isPermaLink="false">2502.16189v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Supermarket-6DoF: A Real-World Grasping Dataset and Grasp Pose Representation Analysis</title>
      <link>http://arxiv.org/abs/2502.16311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究名称': 'Supermarket-6DoF', '数据规模': '包含1500次抓取尝试，涉及20种超市物品', '特点': '提供真实机器人执行的地面实况抓取结果，并且包含完全的6自由度抓取姿势注释，包括初始抓取成功和被抓取后受到外部扰动的稳定性', '用途': '用于分析三种抓取姿态表示方法对点云中抓取成功的预测准确性'}&lt;h4&gt;背景&lt;/h4&gt;现有的大多数抓取数据集依赖于解析指标或模拟进行抓取标注。相比之下，Supermarket-6DoF提供了物理机器人执行的真实地面实况结果&lt;h4&gt;目的&lt;/h4&gt;展示一个真实的超市物品抓取数据集，并验证其在基于点云的抓取姿态表示中的价值和准确性&lt;h4&gt;方法&lt;/h4&gt;通过分析三种不同的抓取姿态表示来预测抓取成功的准确度，比较了显式表达夹爪几何形状的点云表示与传统的四元数编码的表现&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，将夹爪几何结构作为点云明确表示的方法比传统基于四元数的姿态表示方法在抓取成功率预测中更加精确&lt;h4&gt;结论&lt;/h4&gt;Supermarket-6DoF数据集为研究真实环境中的机器人手部操作提供了一个宝贵的资源，并且证明了使用点云来表达夹爪姿态的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Supermarket-6DoF, a real-world dataset of 1500 grasp attemptsacross 20 supermarket objects with publicly available 3D models. Unlike mostexisting grasping datasets that rely on analytical metrics or simulation forgrasp labeling, our dataset provides ground-truth outcomes from physical robotexecutions. Among the few real-world grasping datasets, wile more modest insize, Supermarket-6DoF uniquely features full 6-DoF grasp poses annotated withboth initial grasp success and post-grasp stability under externalperturbation. We demonstrate the dataset's utility by analyzing three grasppose representations for grasp success prediction from point clouds. Ourresults show that representing the gripper geometry explicitly as a point cloudachieves higher prediction accuracy compared to conventional quaternion-basedgrasp pose encoding.</description>
      <author>example@mail.com (Jason Toskov, Akansel Cosgun)</author>
      <guid isPermaLink="false">2502.16311v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Advanced Text Analytics -- Graph Neural Network for Fake News Detection in Social Media</title>
      <link>http://arxiv.org/abs/2502.16157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;传统的图神经网络（GNN）在假新闻检测中通常依赖于辅助的非文本数据，如用户互动历史或内容传播模式。然而这些数据源并不总是可以获取到，限制了方法的有效性和适用性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文提出了一种先进的文本分析图神经网络（ATA-GNN），该模型仅基于文本数据进行操作。&lt;h4&gt;方法&lt;/h4&gt;ATA-GNN采用了创新的主题建模技术来识别每个主题的典型词汇，并通过多维聚类实现对文本内容的全面语义理解。这种多层次的设计使模型能够发现复杂的文本模式，同时将这些模式置于更广泛的语境中以增强其解释能力。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛使用的基准数据集上的大量评估表明，ATA-GNN的表现优于现有的基于GNN的方法，在假新闻检测方面更加可靠和专注于文本信息。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了在图神经网络架构中整合先进的文本聚类方法具有实现更可靠且以文本为中心的解决方案的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;传统的图神经网络（GNN）通常依赖于辅助非文本数据，如用户互动历史或内容传播模式来进行假新闻检测。然而这些数据源并不总是可得，这限制了现有方法的有效性和应用范围。此外，现有的模型常常难以捕捉到文本信息中的详细且复杂的关系，从而降低其整体准确性。为了应对这一挑战，本文提出了一种先进的文本分析图神经网络（ATA-GNN），该模型仅基于文本数据进行操作，并采用了创新的主题建模技术来识别每个主题的典型词汇。通过多维聚类和多层次设计，实现了对文本内容的全面语义理解及复杂模式的发现，在广泛的基准数据集上，该方法的表现超过了现有的GNN方法，表明在图神经网络架构中结合先进的文本聚类方法可以实现更可靠且以文本为中心的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Graph Neural Network (GNN) approaches for fake news detection(FND) often depend on auxiliary, non-textual data such as user interactionhistories or content dissemination patterns. However, these data sources arenot always accessible, limiting the effectiveness and applicability of suchmethods. Additionally, existing models frequently struggle to capture thedetailed and intricate relationships within textual information, reducing theiroverall accuracy. In order to address these challenges Advanced Text AnalysisGraph Neural Network (ATA-GNN) is proposed in this paper. The proposed model isdesigned to operate solely on textual data. ATA-GNN employs innovative topicmodelling (clustering) techniques to identify typical words for each topic,leveraging multiple clustering dimensions to achieve a comprehensive semanticunderstanding of the text. This multi-layered design enables the model touncover intricate textual patterns while contextualizing them within a broadersemantic framework, significantly enhancing its interpretative capabilities.Extensive evaluations on widely used benchmark datasets demonstrate thatATA-GNN surpasses the performance of current GNN-based FND methods. Thesefindings validate the potential of integrating advanced text clustering withinGNN architectures to achieve more reliable and text-focused detectionsolutions.</description>
      <author>example@mail.com (Anantram Patel, Vijay Kumar Sutrakar)</author>
      <guid isPermaLink="false">2502.16157v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>voc2vec: A Foundation Model for Non-Verbal Vocalization</title>
      <link>http://arxiv.org/abs/2502.16298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;提出了一种新的非言语人类数据基础模型voc2vec，旨在克服现有语音和音频基础模型在处理非语言声音时的不足。&lt;h4&gt;背景信息&lt;/h4&gt;现有的语音基础模型虽然在相关的任务中表现出色，但在处理如婴儿哭泣等非语言音频数据方面存在困难。同样地，传统的音频基础模型虽然能够很好地处理非言语音频，但无法捕捉到人类声音中的细微特征。&lt;h4&gt;研究目的&lt;/h4&gt;旨在克服现有模型的缺点，并提出了一种新的基础模型voc2vec，专门用于处理非言语人类数据。&lt;h4&gt;所用方法&lt;/h4&gt;采用了包含10个数据集、总计约125小时非言语音频的数据集合。这些数据集完全由开源资源构成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，voc2vec在非语言声音分类任务中表现优异，并且超越了传统的语音和音频基础模型以及强大的基准线OpenSmile和emotion2vec。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，voc2vec是首个为发声任务设计的通用表示模型。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型已经在相关的任务中显示出了非凡的能力。然而，在处理诸如婴儿哭泣等非语言音频数据时，这些模型常常面临困难。这类非语言音频对于各种现实世界的应用至关重要。传统音频基础模型能够很好地处理非言语数据，但无法捕捉到人类声音中的细微特征。本研究旨在克服上述不足，并提出了一种新的基础模型voc2vec，专门设计用于处理非言语人类数据，仅使用开源的非言语音频数据集。实验结果证明，voc2vec在非语言发声分类任务中表现出色，超越了传统的语音和音频基础模型。此外，voc2vec在六个不同的基准测试数据集中也始终优于强大的基准线OpenSmile和emotion2vec。据作者所知，voc2vec是首个为发声任务设计的通用表示模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models have demonstrated exceptional capabilities inspeech-related tasks. Nevertheless, these models often struggle with non-verbalaudio data, such as vocalizations, baby crying, etc., which are critical forvarious real-world applications. Audio foundation models well handle non-speechdata but also fail to capture the nuanced features of non-verbal human sounds.In this work, we aim to overcome the above shortcoming and propose a novelfoundation model, termed voc2vec, specifically designed for non-verbal humandata leveraging exclusively open-source non-verbal audio datasets. We employ acollection of 10 datasets covering around 125 hours of non-verbal audio.Experimental results prove that voc2vec is effective in non-verbal vocalizationclassification, and it outperforms conventional speech and audio foundationmodels. Moreover, voc2vec consistently outperforms strong baselines, namelyOpenSmile and emotion2vec, on six different benchmark datasets. To the best ofthe authors' knowledge, voc2vec is the first universal representation model forvocalization tasks.</description>
      <author>example@mail.com (Alkis Koudounas, Moreno La Quatra, Marco Sabato Siniscalchi, Elena Baralis)</author>
      <guid isPermaLink="false">2502.16298v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Discovery and Deployment of Emergent Robot Swarm Behaviors via Representation Learning and Real2Sim2Real Transfer</title>
      <link>http://arxiv.org/abs/2502.15937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures. To be included in Proc. of the 24th  International Conference on Autonomous Agents and Multiagent Systems (AAMAS  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于自监督表示学习的Real2Sim2Real行为发现方法，该方法可以在仿真环境中自动发现可能的行为模式，并将这些行为直接部署到真实机器人集群中。&lt;h4&gt;背景&lt;/h4&gt;之前的方法依赖于人工反馈或手工设计的行为度量来表征和进化行为，仅限于在模拟环境中发现行为，未考虑将这些新行为部署到实际的机器人集群上。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以自动探索有限能力机器人群体潜在可出现行为集合的方法，并能够直接应用这些行为到真实的机器人集群中。&lt;h4&gt;方法&lt;/h4&gt;结合表示学习和新颖性搜索，在模拟环境中发现可能的行为，同时通过引入群集sim2real迁移的最新工作缩小现实差距，使得所有在仿真中发现的行为可以直接部署到真实机器人上。&lt;h4&gt;主要发现&lt;/h4&gt;提出的自监督表示学习方法优于手工设计的度量标准，能够更准确地表征潜在行为空间，并且可以通过轻量化模拟器实现从模拟直接转移到实际应用。&lt;h4&gt;结论&lt;/h4&gt;通过展示方法的有效性及其实现的可行性，表明这种方法为自动探索机器人集群中可能出现的行为提供了新途径，并可以无缝地部署到真实环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given a swarm of limited-capability robots, we seek to automatically discoverthe set of possible emergent behaviors. Prior approaches to behavior discoveryrely on human feedback or hand-crafted behavior metrics to represent and evolvebehaviors and only discover behaviors in simulation, without testing orconsidering the deployment of these new behaviors on real robot swarms. In thiswork, we present Real2Sim2Real Behavior Discovery via Self-SupervisedRepresentation Learning, which combines representation learning and noveltysearch to discover possible emergent behaviors automatically in simulation andenable direct controller transfer to real robots. First, we evaluate our methodin simulation and show that our proposed self-supervised representationlearning approach outperforms previous hand-crafted metrics by more accuratelyrepresenting the space of possible emergent behaviors. Then, we address thereality gap by incorporating recent work in sim2real transfer for swarms intoour lightweight simulator design, enabling direct robot deployment of allbehaviors discovered in simulation on an open-source and low-cost robotplatform.</description>
      <author>example@mail.com (Connor Mattson, Varun Raveendra, Ricardo Vega, Cameron Nowzari, Daniel S. Drew, Daniel S. Brown)</author>
      <guid isPermaLink="false">2502.15937v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2502.15635v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Conference on 3D Vision (3DV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了评估端到端的自主驾驶系统，需要一个基于新颖视图合成技术（NVS）的模拟环境来生成逼真的图像和点云。本文介绍了一个新的多车道数据集和基准测试，用于评价现有的NeRF和3DGS方法在真实世界场景中的表现。&lt;h4&gt;背景&lt;/h4&gt;当前的基于场景的NVS数据集中存在缺少与实际捕捉到的真实性和细节的问题，特别是针对跨车道的评估场景仍然不足。&lt;h4&gt;目的&lt;/h4&gt;为了进一步评估现有基于NeRF和3DGS的方法，在逼真的多传感器环境中建立一个可用于自主驾驶系统性能测试的数据集。&lt;h4&gt;方法&lt;/h4&gt;开发了一个包含25组关联序列的多车道数据集，这些序列由实际世界扫描生成，包括16,000张前视图图像、64,000张环视图像和16,000帧LiDAR点云。所有帧都进行了标注以区分移动物体与静止元素。&lt;h4&gt;主要发现&lt;/h4&gt;通过该数据集可以对现有方法在不同车道和距离下的测试场景中进行性能评估，并解决了多传感器姿态求解及质量评价的问题，从而实现了跨模态数据的对齐。&lt;h4&gt;结论&lt;/h4&gt;计划持续添加新的序列以测试现有方法在各种情况下的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;为了评估端到端自主驾驶系统的性能，基于新颖视图合成技术（NVS）创建一个模拟环境是必要的，它可以从先前记录的序列中生成逼真的图像和点云，特别是在跨车道场景下。因此，开发一个多车道数据集和基准测试是必不可少的。尽管最近的一些基于合成场景的NVS数据集已经为跨车道基准测试做好了准备，但它们仍然缺乏真实捕捉到的图像和点云的真实感。为了进一步评估现有的基于NeRF和3DGS方法的性能，我们提出了第一个注册平行扫描的多车道数据集，特别针对新型驾驶视图合成的数据集，该数据集由实际世界扫描衍生而来，包含25组相关序列，包括16,000张前视图像、64,000张环绕视图图像和16,000帧LiDAR帧。所有帧都进行了标注以区分移动对象与静态元素。利用这个数据集，在不同的车道和距离下对现有方法在各种测试场景中的性能进行评估。此外，我们的方法提供了求解并评估多模态数据对齐的多传感器姿态质量的问题解决方案，以便策划这样的数据集。我们计划继续添加新的序列以测试现有方法在不同情况下的泛化能力。该数据集已公开发布于项目页面：https://nizqleo.github.io/paralane-dataset/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To evaluate end-to-end autonomous driving systems, a simulation environmentbased on Novel View Synthesis (NVS) techniques is essential, which synthesizesphoto-realistic images and point clouds from previously recorded sequencesunder new vehicle poses, particularly in cross-lane scenarios. Therefore, thedevelopment of a multi-lane dataset and benchmark is necessary. While recentsynthetic scene-based NVS datasets have been prepared for cross-lanebenchmarking, they still lack the realism of captured images and point clouds.To further assess the performance of existing methods based on NeRF and 3DGS,we present the first multi-lane dataset registering parallel scans specificallyfor novel driving view synthesis dataset derived from real-world scans,comprising 25 groups of associated sequences, including 16,000 front-viewimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames arelabeled to differentiate moving objects from static elements. Using thisdataset, we evaluate the performance of existing approaches in various testingscenarios at different lanes and distances. Additionally, our method providesthe solution for solving and assessing the quality of multi-sensor poses formulti-modal data alignment for curating such a dataset in real-world. We planto continually add new sequences to test the generalization of existing methodsacross different scenarios. The dataset is released publicly at the projectpage: https://nizqleo.github.io/paralane-dataset/.</description>
      <author>example@mail.com (Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang)</author>
      <guid isPermaLink="false">2502.15635v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model for Lossless Image Compression with Visual Prompts</title>
      <link>http://arxiv.org/abs/2502.16163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用大规模语言模型（LLM）进行无损图像压缩的新范式，通过将视觉提示与LLM结合来改进熵编码。&lt;h4&gt;背景&lt;/h4&gt;近年来深度学习在无损图像压缩方面取得了显著进展。随着大型语言模型的出现，初步尝试开始探索如何利用预训练模型中的丰富先验知识来增强无损图像压缩性能，特别是在改进熵模型方面的应用。&lt;h4&gt;目的&lt;/h4&gt;为了克服将LLM中嵌入的文字先验信息与无损图像压缩技术有效结合的问题，并挖掘出该方法的潜力，本文提出了一种新的方案。&lt;h4&gt;方法&lt;/h4&gt;首先生成输入图像的一个有损重建版本作为视觉提示，从中提取特征以作为LLM的视觉嵌入。然后将原始图像和这个有损重建版本之间的残差与这些视觉嵌入一起传递给LLM，使LLM能够充当熵模型来预测该残差的概率分布。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个基准数据集上，本文的方法在无损压缩性能方面达到了当前的最佳水平，并超越了传统的和基于学习的无损图像编码方法。此外，所提出的技术还能方便地扩展到其他领域中的图像（如医学影像和屏幕内容），并显示出出色的效果。&lt;h4&gt;结论&lt;/h4&gt;结果表明LLM对于无损图像压缩具有巨大潜力，并有望激励相关领域的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;最近在深度学习上的进步极大地推动了无损图像压缩技术的发展。随着大规模语言模型的出现，初步尝试开始利用这些预训练模型中的丰富先验知识来改进熵编码，从而增强无损图像压缩性能。然而，在将文本型先前知识与图像无损压缩之间建立联系仍面临挑战。为了应对这一难题并发掘LLM的应用潜能，本文介绍了一种新颖的方法，即通过生成输入图像的有损重建版本作为视觉提示，并从这些提示中提取特征供大型语言模型使用，以此来改进熵编码的过程。研究发现该方法在多个基准数据集上表现出了卓越的压缩效果，超越了传统和基于学习的无损图像编码器。此外，这种方法还可以方便地应用于其他领域的图像（如医学影像和屏幕内容），并取得出色的表现。这些结果凸显了LLM对于无损图像压缩技术的巨大潜力，并有可能激发更多相关方向的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in deep learning have driven significant progress inlossless image compression. With the emergence of Large Language Models (LLMs),preliminary attempts have been made to leverage the extensive prior knowledgeembedded in these pretrained models to enhance lossless image compression,particularly by improving the entropy model. However, a significant challengeremains in bridging the gap between the textual prior knowledge within LLMs andlossless image compression. To tackle this challenge and unlock the potentialof LLMs, this paper introduces a novel paradigm for lossless image compressionthat incorporates LLMs with visual prompts. Specifically, we first generate alossy reconstruction of the input image as visual prompts, from which weextract features to serve as visual embeddings for the LLM. The residualbetween the original image and the lossy reconstruction is then fed into theLLM along with these visual embeddings, enabling the LLM to function as anentropy model to predict the probability distribution of the residual.Extensive experiments on multiple benchmark datasets demonstrate our methodachieves state-of-the-art compression performance, surpassing both traditionaland learning-based lossless image codecs. Furthermore, our approach can beeasily extended to images from other domains, such as medical and screencontent images, achieving impressive performance. These results highlight thepotential of LLMs for lossless image compression and may inspire furtherresearch in related directions.</description>
      <author>example@mail.com (Junhao Du, Chuqin Zhou, Ning Cao, Gang Chen, Yunuo Chen, Zhengxue Cheng, Li Song, Guo Lu, Wenjun Zhang)</author>
      <guid isPermaLink="false">2502.16163v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MedForge: Building Medical Foundation Models Like Open Source Software Development</title>
      <link>http://arxiv.org/abs/2502.16055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了Medical Foundation Models Merging (MedForge)框架，用于促进社区驱动的医疗基础模型开发。&lt;h4&gt;背景&lt;/h4&gt;基础模型（FMs）在医疗领域取得了显著进展。然而，在医疗系统中数据孤岛问题和隐私保护仍然是阻碍安全医学数据共享和跨机构合作的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战并收集和整理可扩展的临床数据集，用于训练强大的基础模型，提出了MedForge框架。&lt;h4&gt;方法&lt;/h4&gt;MedForge通过灵活地合并特定任务的低秩适应（LoRA）模块来提供一种自下而上的模型构建机制。该方法可以同时调整下游任务且保留原始模型参数，并利用异步LoRA模块集成方案逐步增强复合模型在各种临床任务中的综合性能。&lt;h4&gt;主要发现&lt;/h4&gt;MedForge框架展示了其在多个临床数据集（如乳腺癌、肺癌和结肠癌）上的强大性能，这些数据集来自不同机构。研究表明，协作基础模型可以有效并一致地推进多中心临床合作。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了跨医疗机构协同开发医疗基础模型的重要性，并公开发布了相关代码以便其他研究人员使用。&lt;h4&gt;翻译&lt;/h4&gt;在该研究中提出了一种名为Medical Foundation Models Merging (MedForge)的框架，该框架旨在通过灵活合并特定任务的低秩适应（LoRA）模块来促进社区驱动的医学基础模型的发展。此方法能够避免原始患者数据的信息泄露，并解决跨临床机构同步开发模型的问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models (FMs) have made significant strides in the healthcaredomain. Yet the data silo challenge and privacy concern remain in healthcaresystems, hindering safe medical data sharing and collaborative modeldevelopment among institutions. The collection and curation of scalableclinical datasets increasingly become the bottleneck for training strong FMs.In this study, we propose Medical Foundation Models Merging (MedForge), acooperative framework enabling a community-driven medical foundation modeldevelopment, meanwhile preventing the information leakage of raw patient dataand mitigating synchronization model development issues across clinicalinstitutions. MedForge offers a bottom-up model construction mechanism byflexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which canadapt to downstream tasks while retaining original model parameters. Through anasynchronous LoRA module integration scheme, the resulting composite model canprogressively enhance its comprehensive performance on various clinical tasks.MedForge shows strong performance on multiple clinical datasets (e.g., breastcancer, lung cancer, and colon cancer) collected from different institutions.Our major findings highlight the value of collaborative foundation models inadvancing multi-center clinical collaboration effectively and cohesively. Ourcode is publicly available at https://github.com/TanZheling/MedForge.</description>
      <author>example@mail.com (Zheling Tan, Kexin Ding, Jin Gao, Mu Zhou, Dimitris Metaxas, Shaoting Zhang, Dequan Wang)</author>
      <guid isPermaLink="false">2502.16055v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DiffCheck: a Scan-CAD Evaluation Tool for Digital Manufacturing and Assembly Processes in Timber Construction</title>
      <link>http://arxiv.org/abs/2502.15864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Andrea Settimi, Damien Gilliard and Eleni Skevaki contributed equally  to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一款名为diffCheck的软件，该软件使用先进的点云分析技术来比较木质结构的扫描结果与其CAD模型之间的差异。&lt;h4&gt;背景&lt;/h4&gt;在数字木材建造中，由于3D传感器、摄影测量和用户友好的CAD工具易于获得，因此广泛采用扫描技术和点云数据。然而，这些工具通常不用于精度检查，因为标准机械可以提供更高的精度。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为diffCheck的软件来填补这一空白，以便于实验研究和原型制作中评估精确度和准确性。&lt;h4&gt;方法&lt;/h4&gt;diffCheck是一个用C++/Python编写的软件，集成到了Grasshopper平台。它利用先进的点云分析技术进行精度检查，并且可以与各种木材元件和数字制造方法（如机器人装配、AR辅助木工以及数控机床）兼容。&lt;h4&gt;主要发现&lt;/h4&gt;通过测试不同的木材元素和数字制造方法，diffCheck旨在建立一个用户友好的基准框架，用于评估使用木材组件的数字制造系统。此外，该软件及其源代码可以以开放许可的方式与数字化制造社区分享。&lt;h4&gt;结论&lt;/h4&gt;diffCheck具有在其他材料中找到应用潜力的能力，并且其设计是为了促进更高效的精度和准确性检查。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在数字木材建造中，由于3D传感器、摄影测量和用户友好的CAD工具易于获得，因此广泛采用扫描技术和点云数据。虽然通常不用于准确性的校验，因为标准机械可以提供更高的精确度，但是实验研究和原型制作可以从精度和准确性评估工具中获益。我们介绍了一款名为diffCheck的软件，它使用先进的点云分析技术比较加工木材结构的扫描结果与相应的CAD模型之间的差异，并且能够帮助识别出不一致的地方。经过各种木材元件及如机器人装配、AR辅助木工以及数控机床等数字制造方法的测试后，diffCheck旨在为采用木材组件的数字制造系统建立一个用户友好的基准框架，同时具有在其他材料中找到应用潜力的能力。其源代码和分析数据以开放许可的方式与数字化制造社区共享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In digital timber construction, scanning technologies and point cloud dataare widely used due to the accessibility of affordable 3D sensors,photogrammetry, and user-friendly CAD tools. While typically not employed foraccuracy checks in timber fabrication due to the precision of standardmachinery, experimental research and prototyping with joinery and assembly canbenefit from precision and accuracy evaluation tools.  We introduce diffCheck, a C++/Python software integrated into Grasshopper toaddress this need. It uses advanced point cloud analysis to compare scans offabricated timber structures with their respective CAD models, helping toidentify discrepancies. Tested on various timber elements and digitalfabrication methods like robotic assembly, AR-assisted woodworking, and CNCmachining, diffCheck aims to establish a user-friendly benchmark framework fordigital fabrication systems using timber components, with the potential to findapplications in other materials. Its source code and the analyzed data areopenly shared with the digital fabrication community under a permissivelicense.</description>
      <author>example@mail.com (Andrea Settimi, Damien Gilliard, Eleni Skevaki, Marirena Kladeftira, Julien Gamerro, Stefana Parascho, Yves Weinand)</author>
      <guid isPermaLink="false">2502.15864v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients</title>
      <link>http://arxiv.org/abs/2502.15944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为AutoMedPrompt的技术，利用文本梯度优化系统提示来提高通用基础模型在医学领域的问题解答能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）已在医疗和知识领域展示了越来越复杂的性能。传统的专家化方法需要对大量数据集进行广泛的微调和训练。然而，最近的提示工程方法展示出不通过微调也能提升一般基础模型的能力，但这些方法在特定子领域的适用性有限。&lt;h4&gt;目的&lt;/h4&gt;探索使用文本梯度来优化系统提示，从而激发医学相关推理，并评估这种方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;利用TextGrad的自动文本差异化技术改进通用基础LLM的表现。测试了开源大型语言模型Llama 3，在MedQA、PubMedQA和特定肾脏病亚专业的NephSAP等多个问答基准上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与先前的方法相比，使用文本梯度进行提示在开源LLMs上表现更优，并且超越了专有模型如GPT-4、Claude 3 Opus和Med-PaLM 2。AutoMedPrompt在PubMedQA上的准确率为82.6%，超过了此前所有方法的表现。&lt;h4&gt;结论&lt;/h4&gt;文本梯度引导的提示技术展示了其在医学领域问答任务中的显著优势，有望成为未来模型改进的一个方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）已经在医疗和其他知识领域展示出越来越高级别的性能。传统的创建专业LLM的方法需要对大量数据集进行广泛的微调和训练。然而，最近的提示工程方法展示了无需微调即可提升通用基础模型的能力的潜力。但是，例如链式思考（CoT）的提示方法可能不适用于所有子专科，并且k-shot方法可能会在上下文中引入无关词汇。我们提出了AutoMedPrompt，该技术探索了利用文本梯度通过优化系统提示来激发医学相关推理的可能性。AutoMedPrompt使用TextGrad的基于文本的自动微分技术提高了一般基础LLMs的能力。我们在开源LLM Llama 3上评估了AutoMedPrompt，在包括MedQA、PubMedQA和特定肾脏病亚专业的NephSAP等多个问答基准中进行了测试。我们的结果表明，使用文本梯度进行提示超越了以往方法在开源LLMs上的表现，并且超过了GPT-4、Claude 3 Opus以及Med-PaLM 2等专有模型的表现。AutoMedPrompt在PubMedQA上达到了新的最先进（SOTA）性能水平，准确率为82.6%，并且在开源模型中为MedQA（77.7%）和NephSAP（63.8%）的问答任务表现也优于先前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated increasingly sophisticatedperformance in medical and other fields of knowledge. Traditional methods ofcreating specialist LLMs require extensive fine-tuning and training of modelson large datasets. Recently, prompt engineering, instead of fine-tuning, hasshown potential to boost the performance of general foundation models. However,prompting methods such as chain-of-thought (CoT) may not be suitable for allsubspecialty, and k-shot approaches may introduce irrelevant tokens into thecontext space. We present AutoMedPrompt, which explores the use of textualgradients to elicit medically relevant reasoning through system promptoptimization. AutoMedPrompt leverages TextGrad's automatic differentiation viatext to improve the ability of general foundation LLMs. We evaluatedAutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks,including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP.Our results show that prompting with textual gradients outperforms previousmethods on open-source LLMs and surpasses proprietary models such as GPT-4,Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA)performance on PubMedQA with an accuracy of 82.6$\%$, while also outperformingprevious prompting strategies on open-sourced models for MedQA (77.7$\%$) andNephSAP (63.8$\%$).</description>
      <author>example@mail.com (Sean Wu, Michael Koo, Fabien Scalzo, Ira Kurtz)</author>
      <guid isPermaLink="false">2502.15944v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors</title>
      <link>http://arxiv.org/abs/2502.14627v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多语言音频文本检索（ML-ATR）方案，通过理论分析和实验验证了现有方法在跨语种实例相似性匹配中的不一致性问题，并提出了改进策略。&lt;h4&gt;背景&lt;/h4&gt;多语言音频文本检索是一个具有挑战性的任务，目标是从数据库中检索音频片段或多种语言的文本。当前的方法存在跨语种实例相似性匹配的不一致问题。&lt;h4&gt;目的&lt;/h4&gt;分析和解决现有ML-ATR方案在不同语言之间匹配上的不一致性，并提出改进措施来提高召回率和一致性。&lt;h4&gt;方法&lt;/h4&gt;通过1-to-k对比学习和音频-英语共锚对比学习，设计了一种新的多语言音频文本检索框架以减少数据分布误差对结果的影响。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，跨语种实例相似性匹配的不一致问题是由于随机采样语言造成的数据分布错误所引起的。新方法在召回率和一致性指标上取得了主流八种语言（包括英语）的最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的学习策略来解决多语言音频文本检索中的数据分布问题，该研究为提高跨语言信息检索的精度提供了理论依据和技术支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已经以中文形式给出，无需再次翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/atri-acl/atri-acl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims toretrieve audio clips or multilingual texts from databases. However, existingML-ATR schemes suffer from inconsistencies for instance similarity matchingacross languages. We theoretically analyze the inconsistency in terms of bothmultilingual modal alignment direction error and weight error, and propose thetheoretical weight error upper bound for quantifying the inconsistency. Basedon the analysis of the weight error upper bound, we find that the inconsistencyproblem stems from the data distribution error caused by random sampling oflanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastivelearning and audio-English co-anchor contrastive learning, aiming to mitigatethe negative impact of data distribution error on recall and consistency inML-ATR. Experimental results on the translated AudioCaps and Clotho datasetsshow that our scheme achieves state-of-the-art performance on recall andconsistency metrics for eight mainstream languages, including English. Our codewill be available at https://github.com/ATRI-ACL/ATRI-ACL.</description>
      <author>example@mail.com (Yuguo Yin, Yuxin Xie, Wenyuan Yang, Dongchao Yang, Jinghan Ru, Xianwei Zhuang, Liming Liang, Yuexian Zou)</author>
      <guid isPermaLink="false">2502.14627v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Directional Gradient Projection for Robust Fine-Tuning of Foundation Models</title>
      <link>http://arxiv.org/abs/2502.15895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;鲁棒微调的目标是将大规模基础模型适应到下游任务，同时保持其对分布变化的稳健性。现有方法主要集中在基于微调权重和预训练权重之间幅度来约束和投影当前模型向预训练初始化。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分层可训练的方法——方向梯度投影（DiGraP），该方法利用梯度的方向信息，以弥合正则化与多目标优化之间的差距，并将其推广到多模态评估设置中鲁棒微调的场景。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种称为Directional Gradient Projection (DiGraP)的新技术，它通过结合从梯度方向获取的信息来连接正则化和多目标优化。该研究不仅在图像分类上展示了其效果，还将其扩展到视觉问答（VQA）等多模态评估设置。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有基准方法相比，DiGraP 在图像分类及具备判别性和生成性骨干的 VQA 任务中均有更好的性能表现，尤其是在分布变化上具有较高的稳健性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有效地解决了当前鲁棒微调方法中存在的问题，并证明了其在多模态评估设置中的应用潜力。DiGraP 方法展示了其对现有基准方法的优势，在多种任务和设置中均显示出更好的性能，包括改进的分布内泛化能力和分布外稳健性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust fine-tuning aims to adapt large foundation models to downstream taskswhile preserving their robustness to distribution shifts. Existing methodsprimarily focus on constraining and projecting current model towards thepre-trained initialization based on the magnitudes between fine-tuned andpre-trained weights, which often require extensive hyper-parameter tuning andcan sometimes result in underfitting. In this work, we propose DirectionalGradient Projection (DiGraP), a novel layer-wise trainable method thatincorporates directional information from gradients to bridge regularizationand multi-objective optimization. Besides demonstrating our method on imageclassification, as another contribution we generalize this area to themulti-modal evaluation settings for robust fine-tuning. Specifically, we firstbridge the uni-modal and multi-modal gap by performing analysis on ImageClassification reformulated Visual Question Answering (VQA) benchmarks andfurther categorize ten out-of-distribution (OOD) VQA datasets by distributionshift types and degree (i.e. near versus far OOD). Experimental results showthat DiGraP consistently outperforms existing baselines across ImageClassfication and VQA tasks with discriminative and generative backbones,improving both in-distribution (ID) generalization and OOD robustness.</description>
      <author>example@mail.com (Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira)</author>
      <guid isPermaLink="false">2502.15895v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Token Adaptation via Side Graph Convolution for Efficient Fine-tuning of 3D Point Cloud Transformers</title>
      <link>http://arxiv.org/abs/2502.14142v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个名为Side Token Adaptation on a neighborhood Graph (STAG)的新PEFT算法，用于提高3D点云变换器的时序和空间效率。&lt;h4&gt;背景&lt;/h4&gt;参数高效的微调（PEFT）技术已成为分析3D点云的一种有前景的方法。然而现有的PEFT方法在减少可调节参数的同时，往往面临高计算成本的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的名为STAG的技术，以同时提高时序和空间效率。&lt;h4&gt;方法&lt;/h4&gt;STAG采用图卷积侧面网络，在冻结的骨干Transformer并行操作中适应令牌以便进行下游任务。通过高效的图卷积、参数共享以及减少梯度计算来显著降低微调的时间和空间成本。&lt;h4&gt;主要发现&lt;/h4&gt;提出的STAG算法能够保持与其他现有方法相当的分类准确性，同时将可调节参数减少到仅0.43M，并且在微调时大大减少了时间和内存消耗。此外，还提出了一个新的基准测试Point Cloud Classification 13 (PCC13)。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了STAG的有效性，表明它能够显著降低计算成本并保持较高的分类精度。&lt;h4&gt;翻译&lt;/h4&gt;参数高效的微调（PEFT）技术在3D点云分析中崭露头角。尽管现有的PEFT方法试图减少可调节参数的数量，但在细调过程中经常遭受高时间和空间计算成本的问题。本文提出了一种名为Side Token Adaptation on a neighborhood Graph (STAG)的新型PEFT算法以实现卓越的时间和空间效率。STAG采用图卷积侧网络，并行于冻结的骨干Transformer进行操作，以将令牌适应到下游任务中。通过高效的图卷积、参数共享以及减少梯度计算，STAG显著降低了微调过程中的时间和空间成本。此外还介绍了一个新的基准测试Point Cloud Classification 13 (PCC13)，该基准集包括多种公开的3D点云数据集以促进全面评估。使用多个预训练模型和PCC13进行的广泛实验显示了STAG的有效性，特别是保持分类精度与现有方法相当的同时将可调节参数减少到仅0.43M，并且在计算时间和内存消耗方面取得显著降低。代码和基准将在https://github.com/takahikof/STAG提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/takahikof/stag&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloudTransformers has emerged as a promising technique for 3D point cloud analysis.While existing PEFT methods attempt to minimize the number of tunableparameters, they often suffer from high temporal and spatial computationalcosts during fine-tuning. This paper proposes a novel PEFT algorithm calledSide Token Adaptation on a neighborhood Graph (STAG) to achieve superiortemporal and spatial efficiency. STAG employs a graph convolutional sidenetwork operating in parallel with a frozen backbone Transformer to adapttokens to downstream tasks. Through efficient graph convolution, parametersharing, and reduced gradient computation, STAG significantly reduces bothtemporal and spatial costs for fine-tuning. We also present Point CloudClassification 13 (PCC13), a new benchmark comprising diverse publiclyavailable 3D point cloud datasets to facilitate comprehensive evaluation.Extensive experiments using multiple pre-trained models and PCC13 demonstratesthe effectiveness of STAG. Specifically, STAG maintains classification accuracycomparable to existing methods while reducing tunable parameters to only 0.43Mand achieving significant reductions in both computation time and memoryconsumption for fine-tuning. Code and benchmark will be available at:https://github.com/takahikof/STAG.</description>
      <author>example@mail.com (Takahiko Furuya)</author>
      <guid isPermaLink="false">2502.14142v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code Generation</title>
      <link>http://arxiv.org/abs/2502.12490v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted to 47th International Conference on Software Engineering  (ICSE 2025), NIER track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于深度学习的代码生成已彻底改变了当今开发人员编写程序的方式。&lt;h4&gt;背景&lt;/h4&gt;现有的代码生成方法要么集中在序列到序列（Sequence-to-Sequence）范式，即以标记序列的形式生成目标代码；要么是序列到树（Sequence-to-Tree）范式，即输出作为动作序列的目标代码。这两个范式的结合尚未被探索过。&lt;h4&gt;目的&lt;/h4&gt;通过比较这两种范式下产生的代码，作者发现了整合两者的潜在价值，并提出了一种名为UniGenCoder的新模型来解决与代码生成相关的任务。&lt;h4&gt;方法&lt;/h4&gt;UniGenCoder包含一个共享编码器、一个带有最小额外参数集的共享解码器以及一个选择器，该选择器动态地为每个实例选择最优范式。在模型训练过程中，作者首先实施多任务学习和蒸馏策略来促进两个范式的知识转移，并利用对比学习方法训练选择器。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了所提出模型在文本到代码以及代码到代码生成任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的UniGenCoder模型展示了将序列到序列（Sequence-to-Sequence）和序列到树（Sequence-to-Tree）范式结合的潜力，并通过实验证明其优越性。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的代码生成已彻底改变了当今开发人员编写程序的方式。现有方法要么集中在序列到序列或序列到树的方法上，这两种方式都存在一定的局限性。为了克服这些限制，作者提出了一种新的模型UniGenCoder，并证明了它在文本到代码和代码到代码任务上的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based code generation has completely transformed the waydevelopers write programs today. Existing approaches to code generation havefocused either on the Sequence-to-Sequence paradigm, which generates targetcode as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputscode as a sequence of actions. While these two paradigms are intuitivelycomplementary, their combination has not been previously explored. By comparingthe code generated under these two paradigms, we find that integrating themholds significant potential. In this paper, we propose UniGenCoder forcode-related generation tasks, which consists of a shared encoder, a shareddecoder with a minimal set of additional parameters to unify two paradigms, anda selector that dynamically chooses optimal paradigm for each instance. Also,during the model training, we first perform the multi-task learning anddistillation strategies to facilitate knowledge transfer between two paradigms,and then leverage contrastive learning to train the selector. Experimentalresults on the text-to-code and code-to-code generation tasks demonstrate theeffectiveness of our proposed model. We release our code athttps://github.com/DeepLearnXMU/UniGenCoder.</description>
      <author>example@mail.com (Liangying Shao, Yanfu Yan, Denys Poshyvanyk, Jinsong Su)</author>
      <guid isPermaLink="false">2502.12490v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Breast Lump Detection and Localization with a Tactile Glove Using Deep Learning</title>
      <link>http://arxiv.org/abs/2502.15767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于柔性织物的可穿戴触觉手套，用于通过深度学习技术检测模拟乳房模型内的肿块。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是女性死亡的主要原因之一。通过触摸检查乳房以早期发现肿瘤至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一种利用深度学习方法来定位乳房内肿块的可穿戴触觉手套。&lt;h4&gt;方法&lt;/h4&gt;该研究使用了定制的硅胶乳房原型（SBPs）以及球形硅胶肿瘤，其中包含不同直径大小的模拟肿块。采用InceptionTime深度学习架构结合迁移学习技术，并收集了10名普通参与者和一位肿瘤-乳腺科医生的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型在判断肿块存在、大小及位置上的准确率分别为82.22%，67.08%和62.63%；同时，该模型对未见过的有经验用户数据的表现也非常好，准确率达到95.01%，88.54%以及82.98%。&lt;h4&gt;结论&lt;/h4&gt;这项技术可以帮助没有经验的人或医疗保健提供者进行更频繁的常规检查，有助于早期发现乳腺癌。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer is the leading cause of mortality among women. Inspection ofbreasts by palpation is the key to early detection. We aim to create a wearabletactile glove that could localize the lump in breasts using deep learning (DL).In this work, we present our flexible fabric-based and soft wearable tactileglove for detecting the lumps within custom-made silicone breast prototypes(SBPs). SBPs are made of soft silicone that imitates the human skin and theinner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cmdiameters are embedded inside to create another set with lumps. Our approach isbased on the InceptionTime DL architecture with transfer learning betweenexperienced and non-experienced users. We collected a dataset from 10 naiveparticipants and one oncologist-mammologist palpating SBPs. We demonstratedthat the DL model can classify lump presence, size and location with anaccuracy of 82.22%, 67.08% and 62.63%, respectively. In addition, we showedthat the model adapted to unseen experienced users with an accuracy of 95.01%,88.54% and 82.98% for lump presence, size and location classification,respectively. This technology can assist inexperienced users or healthcareproviders, thus facilitating more frequent routine checks.</description>
      <author>example@mail.com (Togzhan Syrymova, Amir Yelenov, Karina Burunchina, Nazgul Abulkhanova, Huseyin Atakan Varol, Juan Antonio Corrales Ramon, Zhanat Kappassov)</author>
      <guid isPermaLink="false">2502.15767v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>V-HOP: Visuo-Haptic 6D Object Pose Tracking</title>
      <link>http://arxiv.org/abs/2502.17434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个结合视觉和触觉反馈的新型统一触觉表示法，旨在提高物体姿态估计在现实世界中的性能。&lt;h4&gt;背景&lt;/h4&gt;人类自然地通过视觉和触觉来感知物体，在抓取过程中丢失任何一种感觉都会影响性能。尽管早期研究尝试结合这两种感觉以改善对象姿态估计，但在实际应用中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的统一化表示法以及基于该表示法的新型视觉-触觉变换器模型，旨在提高跨不同夹持器、传感器布局或仿真与现实环境下的物体跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新统一化的触觉表征来处理多个夹持器实现，并在此基础上提出一种新的视觉-触觉转换器基对象姿态追踪器，该追踪器能够无缝集成视觉和触觉输入。&lt;h4&gt;主要发现&lt;/h4&gt;在自定义数据集和Feelsight数据集中验证了该模型的有效性，证明其在挑战序列中表现出显著性能提升。特别是在面对新型夹持方式、物体及传感器类型时，本方法显示出卓越的泛化性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过现实世界实验表明，所提出的方法大大优于现有的视觉跟踪器，并且能够实现精确的操作任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally integrate vision and haptics for robust object perceptionduring manipulation. The loss of either modality significantly degradesperformance. Inspired by this multisensory integration, prior object poseestimation research has attempted to combine visual and haptic/tactilefeedback. Although these works demonstrate improvements in controlledenvironments or synthetic datasets, they often underperform vision-onlyapproaches in real-world settings due to poor generalization across diversegrippers, sensor layouts, or sim-to-real environments. Furthermore, theytypically estimate the object pose for each frame independently, resulting inless coherent tracking over sequences in real-world deployments. To addressthese limitations, we introduce a novel unified haptic representation thateffectively handles multiple gripper embodiments. Building on thisrepresentation, we introduce a new visuo-haptic transformer-based object posetracker that seamlessly integrates visual and haptic input. We validate ourframework in our dataset and the Feelsight dataset, demonstrating significantperformance improvement on challenging sequences. Notably, our method achievessuperior generalization and robustness across novel embodiments, objects, andsensor types (both taxel-based and vision-based tactile sensors). In real-worldexperiments, we demonstrate that our approach outperforms state-of-the-artvisual trackers by a large margin. We further show that we can achieve precisemanipulation tasks by incorporating our real-time object tracking result intomotion plans, underscoring the advantages of visuo-haptic perception. Our modeland dataset will be made open source upon acceptance of the paper. Projectwebsite: https://lhy.xyz/projects/v-hop/</description>
      <author>example@mail.com (Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar)</author>
      <guid isPermaLink="false">2502.17434v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning</title>
      <link>http://arxiv.org/abs/2502.17432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website at https://jasonjzliu.com/factr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一个低成本且直观的双边遥操作系统，该系统将从动臂接收到的外部力传递给主动臂，促进复杂接触密集任务的数据收集，并引入了一个基于课程学习的策略学习方法FACTR。', '背景': '许多人类操作任务依赖于力反馈以可靠执行，但机器人领域中这种力量信息未得到充分利用，导致机器人行为局限于不需要精细力反馈的任务。', '目的': '开发一种能够利用外部力数据进行复杂接触密集型任务的有效遥操作系统和策略学习方法。', '方法': '首先设计了一种低成本且直观的双边遥操作平台，其次提出了一种名为FACTR的策略学习方法，该方法通过在训练过程中逐渐减少视觉输入的干扰来防止过度拟合，并引导策略关注力模态。', '主要发现': '该研究展示了通过充分使用力信息，与不采用课程学习的基线方法相比，在未见过物体上的泛化性能提高了43%。', '结论': '利用力反馈信息可以显著改善机器人在复杂任务中的表现和适应性。'}&lt;h4&gt;翻译&lt;/h4&gt;许多人类执行的任务，如拾取盒子或擀面团，都依赖于力反馈以确保可靠的完成。然而，在大多数机器人手臂中容易获得的这种力信息并未被广泛用于遥操作和策略学习。因此，机器人的行为通常仅限于不需要复杂力反馈的准静态动力学任务。在这篇论文中，我们首先提出了一种低成本且直观的双边遥操作系统，该系统将从动臂接收到的外部力量传递给主动臂，以促进复杂接触密集型任务的数据收集。接下来，我们介绍了FACTR策略学习方法，该方法在训练过程中采用一种课程，通过逐渐减少视觉输入的干扰来防止基于变压器的政策过度拟合，并引导策略正确关注力模态。我们证明了充分利用力量信息能够显著提高与基线相比，在未见过物体上的泛化性能达43%。视频结果和指南可在https://jasonjzliu.com/factr/获得&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many contact-rich tasks humans perform, such as box pickup or rolling dough,rely on force feedback for reliable execution. However, this force information,which is readily available in most robot arms, is not commonly used inteleoperation and policy learning. Consequently, robot behavior is oftenlimited to quasi-static kinematic tasks that do not require intricateforce-feedback. In this paper, we first present a low-cost, intuitive,bilateral teleoperation setup that relays external forces of the follower armback to the teacher arm, facilitating data collection for complex, contact-richtasks. We then introduce FACTR, a policy learning method that employs acurriculum which corrupts the visual input with decreasing intensity throughouttraining. The curriculum prevents our transformer-based policy fromover-fitting to the visual input and guides the policy to properly attend tothe force modality. We demonstrate that by fully utilizing the forceinformation, our method significantly improves generalization to unseen objectsby 43\% compared to baseline approaches without a curriculum. Video results andinstructions at https://jasonjzliu.com/factr/</description>
      <author>example@mail.com (Jason Jingzhou Liu, Yulong Li, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak)</author>
      <guid isPermaLink="false">2502.17432v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects</title>
      <link>http://arxiv.org/abs/2502.17399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的增强现实(AR)技术，旨在改善同质物体在动态环境中的追踪问题，并通过农场到餐桌的游戏展示了该方法的有效性和实用性。&lt;h4&gt;背景&lt;/h4&gt;随着硬件和软件的进步，头戴式AR游戏变得越来越流行。然而，大多数AR游戏仍然依赖于预先扫描的静态场景，互动方式也有限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决AR游戏中相同物体追踪的问题，并丰富物理与虚拟环境之间的交互体验。&lt;h4&gt;方法&lt;/h4&gt;通过使用AR头盔的部分场景观察数据，结合整数规划解决问题标签分配问题来确定场景中对象的身份，并采用基于Voronoi图的剪枝方法提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能够有效地追踪和区分相同的物体，展示了其在增强现实游戏、故事讲述和模拟机器人中的多功能性和实用性。&lt;h4&gt;结论&lt;/h4&gt;新方法证明了其实用性，在农场到餐桌AR游戏中表现良好，并且通过视频演示展现了其潜力。未来的研究可以探索更多实际应用场景来进一步验证该技术的有效性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;增强现实(AR)游戏，尤其是专为头戴设备设计的游戏，随着硬件和软件的进步变得越来越普遍。然而，大多数AR游戏仍然依赖于预扫描或静态场景，并且交互机制通常限制在控制器或手部跟踪上。此外，在AR游戏中存在相同物体的挑战，传统的对象跟踪技术往往难以区分这些物体或者需要安装固定摄像机来追踪全球物体运动。为了解决这些问题，我们提出了一种新的方法，以解决AR场景中相同物体的跟踪问题，从而丰富物理虚拟交互体验。我们的方法利用了AR头盔捕捉的部分场景观察数据，并结合提供的视角和空间信息，通过整数规划解决方案中的标签分配问题确定场景内对象的身份。为了提高计算效率，我们在方法中引入了一种基于Voronoi图的剪枝技术。在农场到餐桌AR游戏中实现该方法展示了其满意的性能和稳健性。此外，我们通过增强现实故事讲述以及模拟游戏机器人的应用展现了该方法的多功能性和实用性。我们的视频演示可在以下链接查看：https://youtu.be/rPGkLYuKvCQ。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Augmented reality (AR) games, particularly those designed for headsets, havebecome increasingly prevalent with advancements in both hardware and software.However, the majority of AR games still rely on pre-scanned or static scenes,and interaction mechanisms are often limited to controllers or hand-tracking.Additionally, the presence of identical objects in AR games poses challengesfor conventional object tracking techniques, which often struggle todifferentiate between identical objects or necessitate the installation offixed cameras for global object movement tracking. In response to theselimitations, we present a novel approach to address the tracking of identicalobjects in an AR scene to enrich physical-virtual interaction. Our methodleverages partial scene observations captured by an AR headset, utilizing theperspective and spatial data provided by this technology. Object identitieswithin the scene are determined through the solution of a label assignmentproblem using integer programming. To enhance computational efficiency, weincorporate a Voronoi diagram-based pruning method into our approach. Ourimplementation of this approach in a farm-to-table AR game demonstrates itssatisfactory performance and robustness. Furthermore, we showcase theversatility and practicality of our method through applications in ARstorytelling and a simulated gaming robot. Our video demo is available at:https://youtu.be/rPGkLYuKvCQ.</description>
      <author>example@mail.com (Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu)</author>
      <guid isPermaLink="false">2502.17399v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Experimental validation of UAV search and detection system in real wilderness environment</title>
      <link>http://arxiv.org/abs/2502.17372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文研究了在地中海喀斯特环境中利用自主无人机搜索人类的实验设计与实施，以提高搜救任务效率并增强参与人员的安全性。&lt;h4&gt;背景&lt;/h4&gt;搜索和救援任务需要可靠的搜索方法来定位幸存者，尤其是在挑战性强或难以到达的环境中。因此引入无人驾驶飞机可以在提高搜救任务效率的同时增加所有参与者在任务中的安全性。&lt;h4&gt;目的&lt;/h4&gt;设计并实验验证了一种基于热方程驱动区域覆盖（HEDAC）控制方法和计算机视觉对象检测框架的自主无人机搜索系统，并评估其性能是否与实际情况相符。&lt;h4&gt;方法&lt;/h4&gt;该研究通过概率搜索模型、运动控制系统以及计算机视觉目标检测组成的感知框架，使用热方程驱动区域覆盖（HEDAC）控制方法并根据已知的概率密度和检测函数来指导无人飞机。实验中使用了YOLO算法为基础的目标检测模型，并通过先前收集的正射影像数据库进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过对运动控制系统、目标检测以及搜索验证进行了全面分析，表明设计的目标检测模型与实际世界结果相一致，为无人机控制算法提供了强有力的证据支持。&lt;h4&gt;结论&lt;/h4&gt;研究证明了基于HEDAC方法和YOLO算法的自主无人飞机系统在实际搜救任务中的有效性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Search and rescue (SAR) missions require reliable search methods to locatesurvivors, especially in challenging or inaccessible environments. This is whyintroducing unmanned aerial vehicles (UAVs) can be of great help to enhance theefficiency of SAR missions while simultaneously increasing the safety ofeveryone involved in the mission. Motivated by this, we design and experimentwith autonomous UAV search for humans in a Mediterranean karst environment. TheUAVs are directed using Heat equation-driven area coverage (HEDAC) ergodiccontrol method according to known probability density and detection function.The implemented sensing framework consists of a probabilistic search model,motion control system, and computer vision object detection. It enablescalculation of the probability of the target being detected in the SAR mission,and this paper focuses on experimental validation of proposed probabilisticframework and UAV control. The uniform probability density to ensure the evenprobability of finding the targets in the desired search area is achieved byassigning suitably thought-out tasks to 78 volunteers. The detection model isbased on YOLO and trained with a previously collected ortho-photo imagedatabase. The experimental search is carefully planned and conducted, while asmany parameters as possible are recorded. The thorough analysis consists of themotion control system, object detection, and the search validation. Theassessment of the detection and search performance provides strong indicationthat the designed detection model in the UAV control algorithm is aligned withreal-world results.</description>
      <author>example@mail.com (Stella Dumenčić, Luka Lanča, Karlo Jakac, Stefan Ivić)</author>
      <guid isPermaLink="false">2502.17372v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HATPIC: An Open-Source Single Axis Haptic Joystick for Robotic Development</title>
      <link>http://arxiv.org/abs/2502.17362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 1 figure, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人类通过触觉处理的信息远超视觉，因此远程操作中的触觉技术将在未来变得至关重要。当前的触觉设备要么难以获取，要么提供的力反馈质量较低，本文提出了一种单轴开源触觉设备设计方案来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;人的大部分信息处理是依靠触觉完成的，在极端条件下尤其如此，这意味着触觉对于远程操作的重要性将日益增加。&lt;h4&gt;目的&lt;/h4&gt;设计一种易于访问且具有高质量力反馈功能的开源单轴触觉装置以促进远程操作技术的发展和应用。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种新的触觉设备，并展示了其与常见机器人工具集成的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;新设计的手柄式控制器有可能加速各种机器人应用程序中触觉技术的应用，从而提高操作人员的反馈质量和控制水平。&lt;h4&gt;结论&lt;/h4&gt;通过引入这种低成本、高质量力反馈的触觉装置可以极大地促进远程操作技术的发展和广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：人类处理的信息中有相当大的一部分是通过触觉完成的，而不是视觉。因此，在未来几年中，用于远程操控的触觉技术将变得至关重要，因为它为运营商提供了一种额外的感觉通道，这对于在极端条件下进行解释非常重要。然而，目前的触觉设备设置要么难以访问，要么提供的力反馈质量低劣。这项工作提出了一个旨在解决这些问题的单轴、开源的远程操作设计方案。首先介绍了该触觉装置，并展示了其与常见机器人工具集成的可能性。所提出的操纵杆有望加速各种机器人应用中触觉技术的发展和部署，从而增强操作人员的反馈和控制能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans process significantly more information through the sense of touch thanthrough vision. Consequently, haptics for telemanipulation is poised to becomeessential in the coming years, as it offers operators an additional sensorychannel crucial for interpretation in extreme conditions. However, currenthaptic device setups are either difficult to access or provide low-qualityforce feedback rendering. This work proposes the design of a single-axis,open-source setup for telemanipulation development, aimed at addressing theseissues. We first introduce the haptic device and demonstrate its integrationwith common robotic tools. The proposed joystick has the potential toaccelerate the development and deployment of haptic technology in a wide rangeof robotics applications, enhancing operator feedback and control.</description>
      <author>example@mail.com (Julien Mellet, Fabio Ruggiero, Vincenzo Lippiello)</author>
      <guid isPermaLink="false">2502.17362v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MegaLoc: One Retrieval to Place Them All</title>
      <link>http://arxiv.org/abs/2502.17237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MegaLoc的检索模型，该模型结合了多种现有技术，在多个计算机视觉任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从给定查询获取相同位置的图像对于多项计算机视觉任务至关重要。然而，现有的解决方案仅针对单一任务设计，并且在遇到需求变化或未见数据时可能会失败。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在不同任务和环境要求下均能高效工作的通用检索模型。&lt;h4&gt;方法&lt;/h4&gt;结合了多种现有技术和训练技术来构建MegaLoc模型。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'MegaLoc在大量视觉位置识别数据集上达到最新技术水平，', '2': '在常见的地标检索数据集中表现出色，', '3': '在LaMAR数据集的图像定位任务中引入新方法后，设定了新的性能标准。'}&lt;h4&gt;结论&lt;/h4&gt;MegaLoc展示出了跨多个计算机视觉任务领域的强大适应性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;从给定查询获取相同位置的图像是多项重要视觉任务（如视觉地方识别、地标检索、图像定位、三维重建和SLAM）的关键部分。然而，现有的解决方案通常只为特定任务设计，在遇到需求变化或未见数据时表现不佳。本文通过结合多种现有方法和技术训练出了一种名为MegaLoc的新模型，该模型在多个计算机视觉任务中均表现出色，并且其代码开源可获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving images from the same location as a given query is an importantcomponent of multiple computer vision tasks, like Visual Place Recognition,Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,existing solutions are built to specifically work for one of these tasks, andare known to fail when the requirements slightly change or when they meetout-of-distribution data. In this paper we combine a variety of existingmethods, training techniques, and datasets to train a retrieval model, calledMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)achieves state of the art on a large number of Visual Place Recognitiondatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)sets a new state of the art for Visual Localization on the LaMAR datasets,where we only changed the retrieval method to the existing localizationpipeline. The code for MegaLoc is available athttps://github.com/gmberton/MegaLoc</description>
      <author>example@mail.com (Gabriele Berton, Carlo Masone)</author>
      <guid isPermaLink="false">2502.17237v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SoFFT: Spatial Fourier Transform for Modeling Continuum Soft Robots</title>
      <link>http://arxiv.org/abs/2502.17347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于傅里叶变换的方法，用于紧凑地描述连续软机器人的变形，并通过数值模拟和实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;连续软机器人由柔性材料组成，理论上具有无限自由度，在非结构化环境中表现出色的适应能力。Cosserat Rod理论作为建模这些机器人的有效框架已得到广泛应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于信号处理的方法来描述和建模连续软机器人的变形过程。&lt;h4&gt;方法&lt;/h4&gt;将机器人骨架视为时空中的信号，应用傅里叶变换将其变形简化为频域表示。该方法不仅统一了现有建模策略，并提供了实验捕捉机器人变形的基于数据驱动的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟和现实原型实验验证了所提方法的有效性，表明在减少自由度的同时保持变形准确性的能力。&lt;h4&gt;结论&lt;/h4&gt;提出的新方法为连续软机器人的研究提供了一种有效途径，展示了傅里叶变换用于机器人建模的潜力。&lt;h4&gt;翻译&lt;/h4&gt;连续软机器人由柔性材料组成，在非结构化环境中表现出高度适应性。基于Cosserat Rod理论的应用提出了将机器人骨架视为时空信号的方法，并通过傅里叶变换来描述其变形过程。这种方法不仅统一了现有的建模策略，还提供了一种实验捕捉机器人变形的数据驱动方法。数值模拟和现实原型实验验证了该方法的有效性，在减少自由度的同时保持了变形的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuum soft robots, composed of flexible materials, exhibit theoreticallyinfinite degrees of freedom, enabling notable adaptability in unstructuredenvironments. Cosserat Rod Theory has emerged as a prominent framework formodeling these robots efficiently, representing continuum soft robots astime-varying curves, known as backbones. In this work, we propose viewing therobot's backbone as a signal in space and time, applying the Fourier transformto describe its deformation compactly. This approach unifies existing modelingstrategies within the Cosserat Rod Theory framework, offering insights intocommonly used heuristic methods. Moreover, the Fourier transform enables thedevelopment of a data-driven methodology to experimentally capture the robot'sdeformation. The proposed approach is validated through numerical simulationsand experiments on a real-world prototype, demonstrating a reduction in thedegrees of freedom while preserving the accuracy of the deformationrepresentation.</description>
      <author>example@mail.com (Daniele Caradonna, Diego Bianchi, Franco Angelini, Egidio Falotico)</author>
      <guid isPermaLink="false">2502.17347v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Modeling, Simulation, and Application of Spatio-Temporal Characteristics Detection in Incipient Slip</title>
      <link>http://arxiv.org/abs/2502.17335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 19 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于检测机器人抓握和操作任务中的初始滑动。该方法通过分析局部位移现象建立了特征应变率极端事件与局部滑动状态之间的关系。&lt;h4&gt;背景&lt;/h4&gt;早期滑动检测对于机器人抓取和操作任务至关重要，但由于物体属性的多样性和复杂的工作条件，保持其适应性仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来完全表示滑动的空间-时间特征，并建立特征应变率极端事件与局部滑动状态之间的关系。&lt;h4&gt;方法&lt;/h4&gt;基于局部位移现象分析建立了特征应变率极端事件和局部滑动状态之间的联系，该方法能检测到粘着-滑动区域的时空分布。同时，这种方法可以应用于基于视觉的触觉传感器等应变分布传感设备。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和原型实验验证了在不同接触条件下（包括不同的接触几何形状、摩擦系数和组合负载）此方法的有效性。实验表明，该方法不仅能够准确可靠地界定初始滑动，还促进了摩擦参数估计和适应性抓握控制的实现。&lt;h4&gt;结论&lt;/h4&gt;提出的模型及其检测方法具有广泛的适用性和可靠性，在复杂的工作条件下表现出色，并为机器人抓取任务提供了关键反馈。&lt;h4&gt;翻译&lt;/h4&gt;初期滑动检测对于机器人的抓取与操作至关重要。然而，使其在多样的物体特性和复杂的作业环境下保持适应性依然面临挑战。本文重点在于完全表示滑移的时空特性，并提出了一种新的模型来捕捉初期滑动现象及其检测方法。通过对局部位移的研究建立了应变率极端事件和本地化滑动状态之间的联系，该方法可以识别出粘着-滑移区域的空间分布与时间动力学特征，且适用于基于视觉的触觉传感设备等应变分布传感器的应用场景。模拟及原型实验验证了其在不同接触条件下的有效性（比如不同的几何形状、摩擦系数和组合负载），并表明这种新方法不仅能够精确地描绘初期滑动现象，并有助于实现摩擦参数估计与自适应抓取控制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incipient slip detection provides critical feedback for robotic grasping andmanipulation tasks. However, maintaining its adaptability under diverse objectproperties and complex working conditions remains challenging. This articlehighlights the importance of completely representing spatio-temporal featuresof slip, and proposes a novel approach for incipient slip modeling anddetection. Based on the analysis of localized displacement phenomenon, weestablish the relationship between the characteristic strain rate extremeevents and the local slip state. This approach enables the detection of boththe spatial distribution and temporal dynamics of stick-slip regions. Also, theproposed method can be applied to strain distribution sensing devices, such asvision-based tactile sensors. Simulations and prototype experiments validatedthe effectiveness of this approach under varying contact conditions, includingdifferent contact geometries, friction coefficients, and combined loads.Experiments demonstrated that this method not only accurately and reliablydelineates incipient slip, but also facilitates friction parameter estimationand adaptive grasping control.</description>
      <author>example@mail.com (Mingxuan Li, Lunwei Zhang, Qiyin Huang, Tiemin Li, Yao Jiang)</author>
      <guid isPermaLink="false">2502.17335v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Computation Offloading Strategies in Integrated Terrestrial and Non-Terrestrial Networks</title>
      <link>http://arxiv.org/abs/2502.15903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted as chapter to Elsevier&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文探讨了集成地面和非地面网络（IT-NTNs）在计算密集型应用中的作用，这些应用包括增强现实、自动驾驶、远程医疗和智能城市等。&lt;h4&gt;背景&lt;/h4&gt;传统地面网络由于覆盖不足、容量有限以及偏远地区的高延迟而限制了上述应用的发展。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过集成的地面与非地面网络来应对这些问题，并实现高效的计算卸载。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了移动边缘计算（MEC）及其向多接入边缘计算演化的路径，接着详细探讨了IT-NTNs架构，包括地面基站、无人机、高空平台和低轨卫星如何协同工作以提供无缝连接。文章还分析了几种不同的计算卸载策略，并讨论了关键使能技术。&lt;h4&gt;主要发现&lt;/h4&gt;各种计算卸载方法各有优缺点；关键技术如非正交多址接入（NOMA）、毫米波/太赫兹通信和可重构智能表面等对于现有资源分配、任务卸载决策及移动管理算法至关重要。&lt;h4&gt;结论&lt;/h4&gt;本文强调了计算卸载在IT-NTNs中的变革性影响，展望未来的研究方向，并指出了此类网络在未来重新定义通信与计算范式的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of computation-intensive applications like augmentedreality, autonomous driving, remote healthcare, and smart cities has exposedthe limitations of traditional terrestrial networks, particularly in terms ofinadequate coverage, limited capacity, and high latency in remote areas. Thischapter explores how integrated terrestrial and non-terrestrial networks(IT-NTNs) can address these challenges and enable efficient computationoffloading. We examine mobile edge computing (MEC) and its evolution towardmultiple-access edge computing, highlighting the critical role computationoffloading plays for resource-constrained devices. We then discuss thearchitecture of IT-NTNs, focusing on how terrestrial base stations, unmannedaerial vehicles (UAVs), high-altitude platforms (HAPs), and LEO satellites worktogether to deliver ubiquitous connectivity. Furthermore, we analyze variouscomputation offloading strategies, including edge, cloud, and hybridoffloading, outlining their strengths and weaknesses. Key enabling technologiessuch as NOMA, mmWave/THz communication, and reconfigurable intelligent surfaces(RIS) are also explored as essential components of existing algorithms forresource allocation, task offloading decisions, and mobility management.Finally, we conclude by highlighting the transformative impact of computationoffloading in IT-NTNs across diverse application areas and discuss keychallenges and future research directions, emphasizing the potential of thesenetworks to revolutionize communication and computation paradigms.</description>
      <author>example@mail.com (Muhammad Ahmed Mohsin, Muhammad Umer, Amara Umar, Hatem Abou-Zeid, Syed Ali Hassan)</author>
      <guid isPermaLink="false">2502.15903v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title>
      <link>http://arxiv.org/abs/2410.18032v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为GraphTeam的多代理系统，用于基于大型语言模型（LLM）进行图分析。&lt;h4&gt;背景&lt;/h4&gt;现有的图数据分析方法要么将图形神经网络与特定机器学习任务相结合，限制了其可移植性；要么仅依赖于LLMs内部推理能力，导致性能不佳。为解决这些问题，本文利用最近关于LLM代理的进展来提高外部知识或工具使用的能力。&lt;h4&gt;目的&lt;/h4&gt;通过模拟人类解决问题策略（如类比和协作），提出一种基于多代理系统的图分析解决方案GraphTeam。&lt;h4&gt;方法&lt;/h4&gt;{'输入-输出标准化模块': '包括问题代理提取并细化四个关键参数，以促进问题理解；答案代理组织结果以满足输出要求。', '外部知识检索模块': '构建了一个包含相关文档和经验信息的知识库，并通过搜索代理为每个问题检索最相关的条目。', '问题解决模块': '编码代理根据从搜索代理获得的信息使用编程生成解决方案，如果编码代理无法工作，则推理代理将直接计算结果。'}&lt;h4&gt;主要发现&lt;/h4&gt;在六个图分析基准上的广泛实验显示，GraphTeam实现了最先进的性能，比最佳基线平均提高了25.85%的准确性。&lt;h4&gt;结论&lt;/h4&gt;GraphTeam通过模拟人类问题解决策略并有效利用外部知识和工具，为复杂图形数据分析任务提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于GraphTeam系统的设计细节、模块功能以及实验结果的信息。该研究展示了在图数据处理方面的新进展，并强调了LLM代理协同工作的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/bupt-gamma/graphteam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are widely used for modeling relational data in real-world scenarios,such as social networks and urban computing. Existing LLM-based graph analysisapproaches either integrate graph neural networks (GNNs) for specific machinelearning tasks, limiting their transferability, or rely solely on LLMs'internal reasoning ability, resulting in suboptimal performance. To addressthese limitations, we take advantage of recent advances in LLM-based agents,which have shown capabilities of utilizing external knowledge or tools forproblem solving. By simulating human problem-solving strategies such as analogyand collaboration, we propose a multi-agent system based on LLMs namedGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents fromthree modules, and the agents with different specialities can collaborate witheach other to address complex problems. Specifically, (1) input-outputnormalization module: the question agent extracts and refines four keyarguments from the original question, facilitating the problem understanding,and the answer agent organizes the results to meet the output requirement; (2)external knowledge retrieval module: we first build a knowledge base consistingof relevant documentation and experience information, and then the search agentretrieves the most relevant entries for each question. (3) problem-solvingmodule: given the retrieved information from search agent, the coding agentuses established algorithms via programming to generate solutions, and in casethe coding agent does not work, the reasoning agent will directly compute theresults without programming. Extensive experiments on six graph analysisbenchmarks demonstrate that GraphTeam achieves state-of-the-art performancewith an average 25.85% improvement over the best baseline in terms of accuracy.The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.</description>
      <author>example@mail.com (Xin Sky Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang)</author>
      <guid isPermaLink="false">2410.18032v4</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control</title>
      <link>http://arxiv.org/abs/2502.17322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SIRL框架，通过模仿任务相关的轨迹来改进强化学习算法在高维空间中的表现。&lt;h4&gt;背景&lt;/h4&gt;复杂高维度的空间对于配备灵巧手的人形机器人等系统来说，在有限样本预算下平衡探索和利用方面对强化学习算法构成了挑战。可行的任务完成区域通常非常狭窄。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的强化学习框架，以改善在复杂行动空间中任务表现的问题。&lt;h4&gt;方法&lt;/h4&gt;通过引入自模仿强化学习（SIRL）框架，该框架使RL算法能够模仿潜在任务相关的轨迹，并根据轨迹回报调整行为克隆的权重。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法在HumanoidBench上实现了120%的性能提升，同时仅增加了5%的计算开销。进一步的可视化分析表明，这种性能改进确实带来了有意义的行为改善。&lt;h4&gt;结论&lt;/h4&gt;SIRL框架提供了一种有效的方法来增强RL算法处理复杂高维空间任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;复杂的高维度空间（具有大量自由度和复杂行动空间），如装备灵巧手的人形机器人，对强化学习算法构成了重大挑战。这类算法需要在有限的样本预算下巧妙地平衡探索与利用之间的关系。通常情况下，在这种复杂的高维空间中完成任务的有效区域极其狭窄。例如，在人形机器人的运动控制领域，绝大多数的空间都对应于跌倒的状态，而能够执行后续任务的小部分状态则仅占微不足道的比例。一旦机器人进入了潜在的任务相关区域，它应该更加重视该区域内的数据。基于这一见解，我们提出了自模仿强化学习（SIRL）框架，在此框架下RL算法也模仿潜在的任务相关的轨迹。具体来说，使用轨迹回报来确定其任务的相关性，并采用额外的行为克隆方法，权重根据轨迹回报动态调整。结果表明，所提出的算法在具有挑战性的HumanoidBench上实现了120%的性能提升，同时仅增加了5%的计算开销。通过进一步可视化分析发现，显著的性能改进确实导致了有意义的行为改善，成功解决了多个任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex high-dimensional spaces with high Degree-of-Freedom and complicatedaction spaces, such as humanoid robots equipped with dexterous hands, posesignificant challenges for reinforcement learning (RL) algorithms, which needto wisely balance exploration and exploitation under limited sample budgets. Ingeneral, feasible regions for accomplishing tasks within complexhigh-dimensional spaces are exceedingly narrow. For instance, in the context ofhumanoid robot motion control, the vast majority of space corresponds tofalling, while only a minuscule fraction corresponds to standing upright, whichis conducive to the completion of downstream tasks. Once the robot exploresinto a potentially task-relevant region, it should place greater emphasis onthe data within that region. Building on this insight, we propose the$\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement$\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm alsoimitates potentially task-relevant trajectories. Specifically, trajectoryreturn is utilized to determine its relevance to the task and an additionalbehavior cloning is adopted whose weight is dynamically adjusted based on thetrajectory return. As a result, our proposed algorithm achieves 120%performance improvement on the challenging HumanoidBench with 5% extracomputation overhead. With further visualization, we find the significantperformance gain does lead to meaningful behavior improvement that severaltasks are solved successfully.</description>
      <author>example@mail.com (Zifeng Zhuang, Diyuan Shi, Runze Suo, Xiao He, Hongyin Zhang, Ting Wang, Shangke Lyu, Donglin Wang)</author>
      <guid isPermaLink="false">2502.17322v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Inverse Kinematics on Guiding Vector Fields for Robot Path Following</title>
      <link>http://arxiv.org/abs/2502.17313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Yu Zhou and Jes\'us Bautista contributed equally to this work. In the  proceedings of the IEEE International Conference on Robotics and Automation  (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;逆向运动学是一种在机器人学中用于姿态和定位控制的基础技术，通常应用于末端执行器。然而，在这篇论文中，研究者们扩展了逆向运动学的概念，将其应用到自主移动机器人的路径跟随上的导向矢量场。&lt;h4&gt;目的&lt;/h4&gt;为了使机器人能够沿着期望的路径收敛并行进，文章提出了如何使用逆向运动学方法构建误差信号，并驱动导向矢量场朝向期望路径。&lt;h4&gt;方法&lt;/h4&gt;首先，从形式上展示了如何将逆向运动学应用于单积分器机器人的导向矢量场上。然后利用逆向运动学确保层次集误差信号表现为线性系统，从而方便对机器人在向目标路径过渡时的行为进行控制，并允许注入前馈信号以实现沿路径的精确运动行为。&lt;h4&gt;主要发现&lt;/h4&gt;研究提出了如何将该技术应用于常速单轮车（如固定翼无人机），以便它们能够跟踪二维路径并具有精细的瞬态控制能力。&lt;h4&gt;结论&lt;/h4&gt;通过实际飞行测试验证了预测的理论结果，表明这种方法在指导自主移动机器人沿特定路径精确运动方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;逆向运动学被扩展应用于导向矢量场中，使自主移动机器人能够准确地跟踪预定路径。该技术不仅确保了层次集误差信号呈线性系统特征，还通过注入前馈信号提升了瞬态控制性能，并在实际应用中证明其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inverse kinematics is a fundamental technique for motion and positioningcontrol in robotics, typically applied to end-effectors. In this paper, weextend the concept of inverse kinematics to guiding vector fields for pathfollowing in autonomous mobile robots. The desired path is defined by itsimplicit equation, i.e., by a collection of points belonging to one or morezero-level sets. These level sets serve as a reference to construct an errorsignal that drives the guiding vector field toward the desired path, enablingthe robot to converge and travel along the path by following such a vectorfield. We start with the formal exposition on how inverse kinematics can beapplied to guiding vector fields for single-integrator robots in anm-dimensional Euclidean space. Then, we leverage inverse kinematics to ensurethat the level-set error signal behaves as a linear system, facilitatingcontrol over the robot's transient motion toward the desired path and allowingfor the injection of feed-forward signals to induce precise motion behavioralong the path. We then propose solutions to the theoretical and practicalchallenges of applying this technique to unicycles with constant speeds tofollow 2D paths with precise transient control. We finish by validating thepredicted theoretical results through real flights with fixed-wing drones.</description>
      <author>example@mail.com (Yu Zhou, Jesús Bautista, Weijia Yao, Héctor García de Marina)</author>
      <guid isPermaLink="false">2502.17313v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems</title>
      <link>http://arxiv.org/abs/2502.17309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的环境感知对于高级驾驶辅助系统（ADAS）至关重要。激光雷达（LiDAR）在ADAS中起着至关重要的作用，可以可靠地检测障碍物并帮助确保交通安全。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，根据环境特性调整激光雷达的分辨率和范围可以提高机器感知能力。然而，目前针对ADAS的自适应激光雷达方法尚未探索将车辆感知能力和驾驶员视觉感知结合起来的可能性，这可能进一步提升探测性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的系统，该系统可根据驾驶员的视觉感知来调整LiDAR特性，以增强超出人类视野范围外的LiDAR感应。在虚拟环境CARLA中开发系统的概念验证原型。&lt;h4&gt;方法&lt;/h4&gt;系统整合实时数据监控驾驶员视线，识别环境中驾驶员正在观察的区域，并通过动态增加外围未关注区域的激光雷达的范围和分辨率来优化激光雷达资源。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，这种基于注视的LiDAR相对于基准独立式LiDAR，在雾等具有挑战性的环境条件下性能更佳。该混合的人机感知方法在实时驾驶场景中为ADAS应用提供更好的安全性和态势感知能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合车辆和驾驶员的感知能力来优化激光雷达特性，可以提高复杂环境下ADAS系统的检测性能和安全性。&lt;h4&gt;翻译&lt;/h4&gt;准确的环境感知对高级驾驶辅助系统（ADAS）至关重要。现有的研究表明，根据环境特征调整LiDAR的分辨率和范围可以改善机器感知。然而，目前用于ADAS的自适应LiDAR方法还没有探索将车辆感知能力与驾驶员视觉感知结合的可能性，这有可能进一步提升检测性能。本文提出了一种新的系统，该系统可根据人类驾驶员的视觉感知来定制LiDAR特性，以增强超出人眼视野之外的LiDAR感应。我们开发了一个在虚拟环境CARLA中的概念验证原型系统。该系统集成了实时数据监控司机视线的功能，可以识别出环境中司机正在查看的区域，并通过动态增加未被注视周围区域中激光雷达的范围和分辨率来优化其资源分配。模拟结果表明，这种基于目光追踪的LiDAR相对于独立式LiDAR基准在具有挑战性的环境条件下（例如雾）表现更佳。该混合的人机感知方法可能为ADAS应用中的实时驾驶场景提供增强的安全性和态势感知能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate environmental perception is critical for advanced driver assistancesystems (ADAS). Light detection and ranging (LiDAR) systems play a crucial rolein ADAS; they can reliably detect obstacles and help ensure traffic safety.Existing research on LiDAR sensing has demonstrated that adapting the LiDAR'sresolution and range based on environmental characteristics can improve machineperception. However, current adaptive LiDAR approaches for ADAS have notexplored the possibility of combining the perception abilities of the vehicleand the human driver, which can potentially further enhance the detectionperformance. In this paper, we propose a novel system that adapts LiDARcharacteristics to human driver's visual perception to enhance LiDAR sensingoutside human's field of view. We develop a proof-of-concept prototype of thesystem in the virtual environment CARLA. Our system integrates real-time dataon the driver's gaze to identify regions in the environment that the driver ismonitoring. This allows the system to optimize LiDAR resources by dynamicallyincreasing the LiDAR's range and resolution in peripheral areas that the drivermay not be attending to. Our simulations show that this gaze-aware LiDARenhances detection performance compared to a baseline standalone LiDAR,particularly in challenging environmental conditions like fog. Our hybridhuman-machine sensing approach potentially offers improved safety andsituational awareness in real-time driving scenarios for ADAS applications.</description>
      <author>example@mail.com (Federico Scarì, Nitin Jonathan Myers, Chen Quan, Arkady Zgonnikov)</author>
      <guid isPermaLink="false">2502.17309v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SmartEdge: Smart Healthcare End-to-End Integrated Edge and Cloud Computing System for Diabetes Prediction Enabled by Ensemble Machine Learning</title>
      <link>http://arxiv.org/abs/2502.15762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了SmartEdge系统，该系统利用边缘计算和云端协同工作，在糖尿病预测中实现了低延迟、高效能的解决方案。通过集成多种风险因素，并使用投票算法提高了模型预测准确率。&lt;h4&gt;背景&lt;/h4&gt;物联网（IoT）正在推动智能城市的医疗、交通、工业和教育等领域的发展。特别是在智能医院和远程患者监测（RPM）方面，互联网医疗事物（IoMT）变得尤为重要。然而，现有基于云计算的方法在延迟敏感的应用中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种新的AI驱动的边缘计算与云协同系统SmartEdge，旨在解决糖尿病预测中的低延迟问题，并展示其在健康应用中的有效性和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一个基于边缘和云端的框架，在不同配置下部署糖尿病预测模型。评估了系统的性能指标包括延迟、准确性及响应时间。&lt;h4&gt;主要发现&lt;/h4&gt;使用集成机器学习投票算法，可以将预测准确率提高5%，优于单一模型。&lt;h4&gt;结论&lt;/h4&gt;SmartEdge系统展示了在医疗领域中边缘计算和云服务结合的有效性和潜力，并为未来的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Internet of Things (IoT) revolutionizes smart city domains such ashealthcare, transportation, industry, and education. The Internet of MedicalThings (IoMT) is gaining prominence, particularly in smart hospitals and RemotePatient Monitoring (RPM). The vast volume of data generated by IoMT devicesshould be analyzed in real-time for health surveillance, prognosis, andprediction of diseases. Current approaches relying on Cloud computing toprovide the necessary computing and storage capabilities do not scale for theselatency-sensitive applications. Edge computing emerges as a solution bybringing cloud services closer to IoMT devices. This paper introducesSmartEdge, an AI-powered smart healthcare end-to-end integrated edge and cloudcomputing system for diabetes prediction. This work addresses latency concernsand demonstrates the efficacy of edge resources in healthcare applicationswithin an end-to-end system. The system leverages various risk factors fordiabetes prediction. We propose an Edge and Cloud-enabled framework to deploythe proposed diabetes prediction models on various configurations using edgenodes and main cloud servers. Performance metrics are evaluated using, latency,accuracy, and response time. By using ensemble machine learning votingalgorithms we can improve the prediction accuracy by 5% versus a single modelprediction.</description>
      <author>example@mail.com (Alain Hennebelle, Qifan Dieng, Leila Ismail, Rajkumar Buyya)</author>
      <guid isPermaLink="false">2502.15762v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Co-Designing Augmented Reality Tools for High-Stakes Clinical Teamwork</title>
      <link>http://arxiv.org/abs/2502.17295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 7 figures, submitted to DIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了医疗工作者如何利用增强现实头戴式显示器（AR-HMDs）来提高急诊团队的工作效率，通过参与性设计与医疗工作者合作，发现了七个基于角色的AR-HMD应用场景，并提供了针对高风险环境下的团队工作的设计建议。&lt;h4&gt;背景&lt;/h4&gt;尽管AR-HMD在支持医疗环境中团队协作方面显示出巨大潜力，但在急诊科（ER）团队中的应用却很少得到研究。ER具有特殊的挑战，如程序记忆、医疗错误和沟通障碍。&lt;h4&gt;目的&lt;/h4&gt;通过与医疗工作者的合作进行参与性设计研究，探索AR-HMD如何促进急诊流程中团队合作的潜在能力。&lt;h4&gt;方法&lt;/h4&gt;进行了一个参与者驱动的设计研究项目，旨在了解HCWs在ER环境中利用AR-HMD的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;AR-HMD可以作为信息共享和检索系统使用，在知识鸿沟方面发挥作用，并解决了将AR-HMD集成到ER工作流程中的担忧。提出了七种基于角色的场景设计建议，适用于不同专业背景、执行多项医疗任务的HCWs。&lt;h4&gt;结论&lt;/h4&gt;希望通过这项研究激发设计师开发新的AR-HMD应用程序，用于高风险团队环境。&lt;h4&gt;翻译&lt;/h4&gt;医护人员如何利用增强现实头戴式显示器（AR-HMDs）来改善急诊科内的团队协作？尽管在支持医疗服务中的团队合作方面显示了巨大潜力，但专门针对ER团队的设计却很少见。这项研究通过与医疗工作者的合作设计研究，揭示了AR-HMD可以作为信息共享和检索系统使用，解决了将AR-HMD集成到ER工作流程中的问题，并提出了适用于不同专业背景的HCWs在执行多种医疗任务时的角色基于的应用场景设计建议。希望此项研究能激发设计师开发新的适合高风险团队环境下的AR-HMD应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How might healthcare workers (HCWs) leverage augmented reality head-mounteddisplays (AR-HMDs) to enhance teamwork? Although AR-HMDs have shown immensepromise in supporting teamwork in healthcare settings, design for EmergencyDepartment (ER) teams has received little attention. The ER presents uniquechallenges, including procedural recall, medical errors, and communicationgaps. To address this gap, we engaged in a participatory design study withhealthcare workers to gain a deep understanding of the potential for AR-HMDs tofacilitate teamwork during ER procedures. Our results reveal that AR-HMDs canbe used as an information-sharing and information-retrieval system to bridgeknowledge gaps, and concerns about integrating AR-HMDs in ER workflows. Wecontribute design recommendations for seven role-based AR-HMD applicationscenarios involving HCWs with various expertise, working across multiplemedical tasks. We hope our research inspires designers to embark on thedevelopment of new AR-HMD applications for high-stakes, team environments.</description>
      <author>example@mail.com (Angelique Taylor, Tauhid Tanjim, Huajie Cao, Jalynn Blu Nicoly, Jonathan I. Segal, Jonathan St. George, Soyon Kim, Kevin Ching, Francisco R. Ortega, Hee Rin Lee)</author>
      <guid isPermaLink="false">2502.17295v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework</title>
      <link>http://arxiv.org/abs/2502.17265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025. Project website:  https://hsp-iit.github.io/hannes-wrist-control&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于计算机视觉的系统，该系统结合用户和自动系统的合作，在共享自主框架中实现假肢腕关节连续控制。&lt;h4&gt;背景&lt;/h4&gt;大多数用于假肢抓握的技术集中在灵巧的手指控制上，而忽视了手腕动作。这迫使用户通过肘部、肩部和臀部的动作来适应手腕的抓取需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合计算机视觉的方法，在假肢臂中实现腕关节自由度的连续控制，以促进更自然的接近目标物体并根据用户的意图进行抓握。&lt;h4&gt;方法&lt;/h4&gt;该系统利用用户与自动系统的协作，采用基于计算机视觉的技术来无缝控制假肢手腕。系统可以追踪目标对象，并最终按照用户意愿调整手腕姿态。&lt;h4&gt;主要发现&lt;/h4&gt;通过定量分析评估了每个系统组件的有效性，并在Hannes假肢臂上部署该方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的系统能够有效改善假肢的自然抓握能力，提供更好的用户体验。&lt;h4&gt;翻译&lt;/h4&gt;大多数用于假肢抓握的技术集中在灵巧的手指控制上，而忽视了手腕动作。这迫使用户通过肘部、肩部和臀部的动作来适应手腕的抓取需求。我们提出了一种基于计算机视觉的方法，在共享自主框架中结合用户与自动系统的合作，实现假肢腕关节自由度的连续控制，以促进更自然的接近目标物体并根据用户的意图进行抓握。我们的系统可以无缝地控制假肢手腕追踪目标对象，并最终按照用户意愿调整手腕姿态。我们通过定量分析评估了每个系统组件的有效性，并在Hannes假肢臂上部署该方法。代码和视频：https://hsp-iit.github.io/hannes-wrist-control.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most control techniques for prosthetic grasping focus on dexterous fingerscontrol, but overlook the wrist motion. This forces the user to performcompensatory movements with the elbow, shoulder and hip to adapt the wrist forgrasping. We propose a computer vision-based system that leverages thecollaboration between the user and an automatic system in a shared autonomyframework, to perform continuous control of the wrist degrees of freedom in aprosthetic arm, promoting a more natural approach-to-grasp motion. Our pipelineallows to seamlessly control the prosthetic wrist to follow the target objectand finally orient it for grasping according to the user intent. We assess theeffectiveness of each system component through quantitative analysis andfinally deploy our method on the Hannes prosthetic arm. Code and videos:https://hsp-iit.github.io/hannes-wrist-control.</description>
      <author>example@mail.com (Federico Vasile, Elisa Maiettini, Giulia Pasquale, Nicolò Boccardo, Lorenzo Natale)</author>
      <guid isPermaLink="false">2502.17265v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Tidiness Score-Guided Monte Carlo Tree Search for Visual Tabletop Rearrangement</title>
      <link>http://arxiv.org/abs/2502.17235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的框架TSMCTS，该框架利用RGB-D相机解决桌面整理问题。&lt;h4&gt;背景&lt;/h4&gt;当前的桌面整理研究面临两大挑战：缺乏公开的数据集和基准以及难以指定未见物体的目标配置。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，作者提出了一个结构化的数据集（TTU），并开发了一种基于视觉的判别器来预测整洁度分数，并结合蒙特卡洛树搜索算法寻找整理轨迹。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了一个桌面整理数据集(TTU)；2. 利用该数据训练出一种能够评估不同配置下整洁度的基于视觉的判别器；3. 使用MCTS在不指定具体目标的情况下找到整理路径，并利用整洁度分数作为指导；4. 提出了TSMCTS，它将一个清洁度判断器与一个基于MCTS的整理规划器结合在一起。&lt;h4&gt;主要发现&lt;/h4&gt;提出的TSMCTS框架能够在多种环境中有效工作，包括咖啡桌、餐桌、办公桌和浴室等场景。&lt;h4&gt;结论&lt;/h4&gt;通过使用TTU数据集训练出的视觉判别器和MCTS算法相结合的方法能够有效地解决桌面整理问题，并且具有较高的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种基于整洁度分数指导的蒙特卡洛树搜索(TSMCTS)框架，旨在仅使用RGB-D相机来解决桌面上的清理问题。该研究解决了两个主要难题：缺乏公开数据集和基准以及难以指定未见物体的目标配置。为了解决第一个问题，我们提供了桌面整理（TTU）数据集，在模拟中收集了一个结构化的数据集。利用这个数据集，我们训练出一种基于视觉的判别器能够预测整洁度分数。这种判别器可以一致地评估未知配置下的整洁程度，包括真实世界中的场景。为了解决第二个问题，我们采用了蒙特卡洛树搜索(MCTS)方法在不指定明确目标的情况下寻找整理路径。而不是提供特定的目标，我们展示我们的MCTS基规划程序能够使用整洁度分数作为指导找到多样化的整理配置。因此，我们提出了TSMCTS，它将一个清洁度判断器与一个基于MCTS的整理规划器结合在一起以找到最优的整理排列。在咖啡桌、餐桌、办公桌和浴室等各种环境中，TSMCTS已经展示了其能力。TTU数据集可以在 https://github.com/rllab-snu/TTU-Dataset 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present the tidiness score-guided Monte Carlo tree search(TSMCTS), a novel framework designed to address the tabletop tidying up problemusing only an RGB-D camera. We address two major problems for tabletop tidyingup problem: (1) the lack of public datasets and benchmarks, and (2) thedifficulty of specifying the goal configuration of unseen objects. We addressthe former by presenting the tabletop tidying up (TTU) dataset, a structureddataset collected in simulation. Using this dataset, we train a vision-baseddiscriminator capable of predicting the tidiness score. This discriminator canconsistently evaluate the degree of tidiness across unseen configurations,including real-world scenes. Addressing the second problem, we employ MonteCarlo tree search (MCTS) to find tidying trajectories without specifyingexplicit goals. Instead of providing specific goals, we demonstrate that ourMCTS-based planner can find diverse tidied configurations using the tidinessscore as a guidance. Consequently, we propose TSMCTS, which integrates atidiness discriminator with an MCTS-based tidying planner to find optimaltidied arrangements. TSMCTS has successfully demonstrated its capability acrossvarious environments, including coffee tables, dining tables, office desks, andbathrooms. The TTU dataset is available at:https://github.com/rllab-snu/TTU-Dataset.</description>
      <author>example@mail.com (Hogun Kee, Wooseok Oh, Minjae Kang, Hyemin Ahn, Songhwai Oh)</author>
      <guid isPermaLink="false">2502.17235v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding</title>
      <link>http://arxiv.org/abs/2502.17221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种针对非抓握式任务的深度确定性策略梯度（DDPG）强化学习框架，用于高效地在水平表面上滑动物体。该算法通过精确控制机械臂的加速度生成线性轨迹，实现了物体的相对操作。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数技术主要集中在基于抓握的操作上，这限制了它们在非抓握任务中的适用性。为了满足日益增长的需求，本文提出了一种新的方法来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;引入一种DDPG强化学习框架，以实现高效的非抓握操作，特别是在滑动物体方面的应用。&lt;h4&gt;方法&lt;/h4&gt;算法通过精确控制机器人臂的加速度生成线性轨迹，实现了物体在水平表面上滑动时的操作。此外还开发了两种不同的算法来动态估计滑动过程中的摩擦力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法能够在线估算每次操作后的摩擦力，并将其反馈到演员模型中作为关键反馈，从而提高了策略的适应性和鲁棒性。实验结果表明该框架能够有效推广滑动物体的操作并适应不同表面。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过模拟和实际实验验证了其有效性，展示了零样本仿真到现实环境的转移能力。&lt;h4&gt;翻译&lt;/h4&gt;虽然机器人应用程序越来越需要多功能和动态的对象处理，但大多数现有技术主要集中在基于抓握的操作上，限制了它们在非抓握任务中的适用性。为了解决这一需求，本文介绍了一种用于有效进行非抓取操作（特别是物体在水平表面上滑动）的深度确定性策略梯度（DDPG）强化学习框架。该算法通过精确控制与水平表面刚性连接的机器人臂加速度生成线性轨迹，实现了滑动物体时相对的操作。此外还开发了两种不同的算法来动态估算滑动过程中的摩擦力。这些算法在线提供了每次动作后的摩擦估计，并将其反馈到演员模型中作为关键反馈，增强了策略的适应性和鲁棒性，确保在不同表面条件下更精确地控制平台加速度。所提出的算法通过模拟和实际实验进行了验证。结果表明该框架能够有效推广滑动物体操作，并且特别能够在不同摩擦性质表面上进行调整。值得注意的是，训练后的模型展示了零样本仿真到现实环境的转移能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although robotic applications increasingly demand versatile and dynamicobject handling, most existing techniques are predominantly focused ongrasp-based manipulation, limiting their applicability in non-prehensile tasks.To address this need, this study introduces a Deep Deterministic PolicyGradient (DDPG) reinforcement learning framework for efficient non-prehensilemanipulation, specifically for sliding an object on a surface. The algorithmgenerates a linear trajectory by precisely controlling the acceleration of arobotic arm rigidly coupled to the horizontal surface, enabling the relativemanipulation of an object as it slides on top of the surface. Furthermore, twodistinct algorithms have been developed to estimate the frictional forcesdynamically during the sliding process. These algorithms provide onlinefriction estimates after each action, which are fed back into the actor modelas critical feedback after each action. This feedback mechanism enhances thepolicy's adaptability and robustness, ensuring more precise control of theplatform's acceleration in response to varying surface condition. The proposedalgorithm is validated through simulations and real-world experiments. Resultsdemonstrate that the proposed framework effectively generalizes slidingmanipulation across varying distances and, more importantly, adapts todifferent surfaces with diverse frictional properties. Notably, the trainedmodel exhibits zero-shot sim-to-real transfer capabilities.</description>
      <author>example@mail.com (Hamidreza Raei, Elena De Momi, Arash Ajoudani)</author>
      <guid isPermaLink="false">2502.17221v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.17219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于动态平衡和强化学习的新型全身行走算法，使类人机器人能够仅通过本体感觉在极端地形上行走。&lt;h4&gt;背景&lt;/h4&gt;人类具有精细的动力平衡机制，能够在各种地形和极端条件下保持稳定。然而，现有的类人机器人步行算法难以应对缺乏外部感知（如视觉或激光雷达）的极端环境。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的全身行走算法来解决现有方法在处理不可见障碍物和突然失去平衡方面的能力不足问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一个动态平衡机制，通过扩展的零力矩点(ZMP)驱动奖励和任务驱动奖励，在一个全身演员-评论家框架中实现上肢与下肢的协调动作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该算法能够使机器人在极其狭窄的地面上保持平衡，并且对外部干扰具有适应性。&lt;h4&gt;结论&lt;/h4&gt;通过这种新型方法，类人机器人的行走能力得到了显著增强，使其能更好地应对复杂环境。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有精细的动力平衡机制，在各种地形和极端条件下都能维持稳定。然而，尽管最近取得了重大进展，现有的类人机器人步行算法仍然难以穿越极端环境，尤其是在缺乏外部感知（如视觉或激光雷达）的情况下。这是因为当前的方法通常依赖于基于步态的或者需要特定感知条件的奖励策略，缺少有效处理不可见障碍物和突然失去平衡的有效机制。为了解决这一挑战，我们提出了一种新的全身行走算法，该算法基于动力平衡和强化学习（RL），使类人机器人能够仅通过本体感觉在极端地形上行走，特别是在狭窄路径和意外障碍的情况下。具体来说，我们引入了一个动态平衡机制，利用了扩展的零力矩点(ZMP)驱动奖励和任务驱动奖励，在一个全身演员-评论家框架中实现上下肢体的协调动作。全尺寸Unitree H1-2机器人的实验验证了该方法在极端狭窄地形上保持平衡以及对抗外部干扰的能力，证明了其增强机器人对复杂环境适应性的有效性。视频可以在https://whole-body-loco.github.io观看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess delicate dynamic balance mechanisms that enable them tomaintain stability across diverse terrains and under extreme conditions.However, despite significant advances recently, existing locomotion algorithmsfor humanoid robots are still struggle to traverse extreme environments,especially in cases that lack external perception (e.g., vision or LiDAR). Thisis because current methods often rely on gait-based or perception-conditionrewards, lacking effective mechanisms to handle unobservable obstacles andsudden balance loss. To address this challenge, we propose a novel whole-bodylocomotion algorithm based on dynamic balance and Reinforcement Learning (RL)that enables humanoid robots to traverse extreme terrains, particularly narrowpathways and unexpected obstacles, using only proprioception. Specifically, weintroduce a dynamic balance mechanism by leveraging an extended measure ofZero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-bodyactor-critic framework, aiming to achieve coordinated actions of the upper andlower limbs for robust locomotion. Experiments conducted on a full-sizedUnitree H1-2 robot verify the ability of our method to maintain balance onextremely narrow terrains and under external disturbances, demonstrating itseffectiveness in enhancing the robot's adaptability to complex environments.The videos are given at https://whole-body-loco.github.io.</description>
      <author>example@mail.com (Weiji Xie, Chenjia Bai, Jiyuan Shi, Junkai Yang, Yunfei Ge, Weinan Zhang, Xuelong Li)</author>
      <guid isPermaLink="false">2502.17219v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Mechanical non-reciprocity programmed by shear jamming in soft composite solids</title>
      <link>http://arxiv.org/abs/2502.17083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通过利用颗粒物理中的剪切堵塞转换来设计非互易机械性的原理，从而在软复合固体中实现可调、方向依赖的不对称性。&lt;h4&gt;背景&lt;/h4&gt;传统的非互易性通常通过复杂结构非线性在超材料中实现。而具有内在非互易力学特性的连续体固体由于其潜在的应用前景（如波导、机器人技术及自适应材料）却未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一种设计原则，利用颗粒物理中的剪切堵塞转换来工程化软复合固体中的非互易机械性。&lt;h4&gt;方法&lt;/h4&gt;通过控制包含接触网络与基质弹性之间的相互作用，在软复合固体中实现了对剪切和正交力学响应的方向依赖的不对称性。结合响应性磁轮廓线，展示了程序化的非互易动力学特性。&lt;h4&gt;主要发现&lt;/h4&gt;实现了可调、方向相关的不对称性；展示了通过组合响应性磁图案和剪切堵塞系统的各向异性特性的程序化非互易动力学；以及在软材料中实现之前难以达到的不对称时空控制运动传输的能力。&lt;h4&gt;结论&lt;/h4&gt;此项工作为设计非互易物质开创了一个新的范例，将颗粒物理与软材料工程相结合以实现对机械智能系统至关重要的功能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mechanical non-reciprocity-manifested as asymmetric responses to opposingmechanical stimuli-has traditionally been achieved through intricate structuralnonlinearities in metamaterials. However, continuum solids with inherentnon-reciprocal mechanics remain underexplored, despite their promisingpotential for applications such as wave guiding, robotics, and adaptivematerials. Here, we introduce a design principle by employing the shear jammingtransition from granular physics to engineering non-reciprocal mechanics insoft composite solids. Through the control of the interplay between inclusioncontact networks and matrix elasticity, we achieve tunable, direction-dependentasymmetry in both shear and normal mechanical responses. In addition to staticregimes, we demonstrate programmable non-reciprocal dynamics by combiningresponsive magnetic profiles with the anisotropic characteristics ofshear-jammed systems. This strategy enables asymmetric spatiotemporal controlover motion transmission, a previously challenging feat in soft materials. Ourwork establishes a novel paradigm for designing non-reciprocal matter, bridginggranular physics with soft material engineering to realize functionalitiesessential for mechano-intelligent systems.</description>
      <author>example@mail.com (Chang Xu, Shuaihu Wang, Hong Wang, Xu Liu, Zemin Liu, Yiqiu Zhao, Wenqi Hu, Qin Xu)</author>
      <guid isPermaLink="false">2502.17083v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Evolution 6.0: Evolving Robotic Capabilities Through Generative Design</title>
      <link>http://arxiv.org/abs/2502.17034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出Evolution 6.0的概念，这是一种由生成式人工智能驱动的机器人技术进化。当机器人缺乏完成任务所需的工具时，它能够自主设计所需工具并学习如何使用它们来实现目标。&lt;h4&gt;背景&lt;/h4&gt;随着Generative AI的发展，机器人系统需要更加智能化和自适应以应对各种复杂任务的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器人系统Evolution 6.0，使其能够在没有事先准备的情况下完成人类请求的任务，并且自主设计必要的工具和学习使用方法。&lt;h4&gt;方法&lt;/h4&gt;该系统包括两个关键模块：工具生成模块和动作生成模块。前者利用视觉-语言模型（VLM）和3D工具生成器来根据任务需求设计并制造专用工具；后者则通过自然语言指令转为机器人行动。具体实现中采用了QwenVLM、OpenVLA和Llama-Mesh等技术。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该系统在10秒内能以90%的成功率生成所需工具，并且83.5%的物理和视觉泛化能力，在动作生成方面表现良好。然而，在运动和语义泛化上还有待提高。&lt;h4&gt;结论&lt;/h4&gt;尽管初步结果令人鼓舞，但为了进一步提升其在现实世界中的适用性，未来的工作将重点放在双臂操作、扩展任务范围以及增强环境理解等方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原始英文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new concept, Evolution 6.0, which represents the evolution ofrobotics driven by Generative AI. When a robot lacks the necessary tools toaccomplish a task requested by a human, it autonomously designs the requiredinstruments and learns how to use them to achieve the goal. Evolution 6.0 is anautonomous robotic system powered by Vision-Language Models (VLMs),Vision-Language Action (VLA) models, and Text-to-3D generative models for tooldesign and task execution. The system comprises two key modules: the ToolGeneration Module, which fabricates task-specific tools from visual and textualdata, and the Action Generation Module, which converts natural languageinstructions into robotic actions. It integrates QwenVLM for environmentalunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D toolgeneration. Evaluation results demonstrate a 90% success rate for toolgeneration with a 10-second inference time, and action generation achieving83.5% in physical and visual generalization, 70% in motion generalization, and37% in semantic generalization. Future improvements will focus on bimanualmanipulation, expanded task capabilities, and enhanced environmentalinterpretation to improve real-world adaptability.</description>
      <author>example@mail.com (Muhammad Haris Khan, Artyom Myshlyaev, Artyom Lykov, Miguel Altamirano Cabrera, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2502.17034v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Task-Oriented 6-DoF Grasp Pose Detection in Clutters</title>
      <link>http://arxiv.org/abs/2502.16976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了任务导向的6自由度抓取姿态检测在杂乱环境中的问题，并构建了一个大规模的数据集以解决这一难题。提出了一种名为One-Stage TaskGrasp（OSTG）的方法，该方法采用了基于特定任务的对象点选择策略以及基于任务的手势生成模块来确定如何抓取。&lt;h4&gt;背景&lt;/h4&gt;人类会根据不同任务采取不同的抓握方式，例如刀具的把手用于切割而刀片则用于传递。现有的机器人抓握姿态检测研究在一定程度上考虑了这种任务导向性的问题，并且已经取得了一些进展；然而这些方法通常受制于低自由度夹爪类型或非杂乱设置。&lt;h4&gt;目的&lt;/h4&gt;为了解决更通用和实用的抓取模型问题，本文提出了一个名为Task-Oriented 6-DoF Grasp Pose Detection in Clutters（TO6DGC）的问题，并构造了一个大规模的数据集用于解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出了OSTG方法，该方法采用了任务导向点选择策略来确定何处抓握，以及一种任务导向手势生成模块以决定如何抓取特定对象。实验是在构建的大型数据集上进行的。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，在多个指标下，本文提出的方法均优于现有基线；实际机器人实验也验证了OSTG方法在识别任务导向抓握点和6自由度抓握姿态上的优越性。&lt;h4&gt;结论&lt;/h4&gt;提出了一个处理杂乱环境中具有任务导向性的6自由度抓取问题的有效解决方案，并展示了其对于实现更通用且实用的机器人抓取模型的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的是关于人类基于不同任务选择不同的抓握方式，研究者们在机器人抓取姿态检测领域进行了一些任务导向性的工作并取得了进展。然而这些工作大多数局限于低自由度夹爪或者非杂乱场景下，并不适用于现实生活中的辅助需求。本文旨在开发更通用和实际的应用模型，提出了一个名为“任务导向的6自由度抓取姿态检测在复杂环境下的问题”，并且构建了一个大规模的数据集来支持这一研究。此外还提出了一种名为One-Stage TaskGrasp（OSTG）的方法来解决这个问题，并通过大量实验验证了该方法的有效性及其对于实际应用的意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In general, humans would grasp an object differently for different tasks,e.g., "grasping the handle of a knife to cut" vs. "grasping the blade to handover". In the field of robotic grasp pose detection research, some existingworks consider this task-oriented grasping and made some progress, but they aregenerally constrained by low-DoF gripper type or non-cluttered setting, whichis not applicable for human assistance in real life. With an aim to get moregeneral and practical grasp models, in this paper, we investigate the problemnamed Task-Oriented 6-DoF Grasp Pose Detection in Clutters (TO6DGC), whichextends the task-oriented problem to a more general 6-DOF Grasp Pose Detectionin Cluttered (multi-object) scenario. To this end, we construct a large-scale6-DoF task-oriented grasping dataset, 6-DoF Task Grasp (6DTG), which features4391 cluttered scenes with over 2 million 6-DoF grasp poses. Each grasp isannotated with a specific task, involving 6 tasks and 198 objects in total.Moreover, we propose One-Stage TaskGrasp (OSTG), a strong baseline to addressthe TO6DGC problem. Our OSTG adopts a task-oriented point selection strategy todetect where to grasp, and a task-oriented grasp generation module to decidehow to grasp given a specific task. To evaluate the effectiveness of OSTG,extensive experiments are conducted on 6DTG. The results show that our methodoutperforms various baselines on multiple metrics. Real robot experiments alsoverify that our OSTG has a better perception of the task-oriented grasp pointsand 6-DoF grasp poses.</description>
      <author>example@mail.com (An-Lan Wang, Nuo Chen, Kun-Yu Lin, Li Yuan-Ming, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2502.16976v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Design of a low-cost and lightweight 6 DoF bimanual arm for dynamic and contact-rich manipulation</title>
      <link>http://arxiv.org/abs/2502.16908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了ARMADA，一个专为动态操作研究设计的双臂机器人。它采用了低成本和易于组装的设计，能够执行打击、抢夺、锤击等复杂任务。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数机器人由于硬件限制（如高惯性设计、有限的柔韧性以及对昂贵扭矩传感器的依赖）难以处理具有挑战性的动态接触操作任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种经济且灵活的双臂机器人，用于动态手动动作的研究和实验。&lt;h4&gt;方法&lt;/h4&gt;ARMADA采用了低惯量、反驱能力强的执行器，并使用了轻质设计以及现成可用组件和3D打印链接。整个系统的成本仅为6100美元。&lt;h4&gt;主要发现&lt;/h4&gt;ARMADA可以达到每秒高达6.16米的速度，负载为2.5公斤；在真实环境中能完成诸如抢夺、锤击等动态操作任务，并且通过强化学习训练后可以在现实世界中进行零样本迁移。&lt;h4&gt;结论&lt;/h4&gt;ARMADA的开源性质提供了详细的组装指南、CAD模型和仿真代码，方便研究者使用。推荐观看补充视频以获得更多信息。&lt;h4&gt;翻译&lt;/h4&gt;该摘要介绍了ARMADA（用于操纵与动态动作的可承受机器人），一个专为动态操作研究设计的双臂机器人。它采用了低成本且易于在实验室中组装的设计，结合低惯量、反驱能力执行器和轻质结构，使用现成组件及3D打印链接构建。整个系统的成本仅为6100美元，每只手臂的速度可达每秒6.16米，与大多数协作机器人相比几乎快了一倍，负载为2.5公斤。展示的动态操作包括抢夺、锤击和双手抛掷等任务，并且在强化学习中表现出色：能够在模拟环境中训练非抓取式操纵策略并在现实世界中实现零样本迁移；以及通过人体动作阴影进行双臂物体抛掷研究。ARMADA是完全开源的，包含详细的组装说明、CAD模型、URDFs（Universal Robot Description Format）、仿真和学习代码。强烈推荐查看补充视频了解更多信息：https://sites.google.com/view/im2-humanoid-arm&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic and contact-rich object manipulation, such as striking, snatching, orhammering, remains challenging for robotic systems due to hardware limitations.Most existing robots are constrained by high-inertia design, limitedcompliance, and reliance on expensive torque sensors. To address this, weintroduce ARMADA (Affordable Robot for Manipulation and Dynamic Actions), a 6degrees-of-freedom bimanual robot designed for dynamic manipulation research.ARMADA combines low-inertia, back-drivable actuators with a lightweight design,using readily available components and 3D-printed links for ease of assembly inresearch labs. The entire system, including both arms, is built for just$6,100. Each arm achieves speeds up to 6.16m/s, almost twice that of mostcollaborative robots, with a comparable payload of 2.5kg. We demonstrate ARMADAcan perform dynamic manipulation like snatching, hammering, and bimanualthrowing in real-world environments. We also showcase its effectiveness inreinforcement learning (RL) by training a non-prehensile manipulation policy insimulation and transferring it zero-shot to the real world, as well as humanmotion shadowing for dynamic bimanual object throwing. ARMADA is fullyopen-sourced with detailed assembly instructions, CAD models, URDFs,simulation, and learning codes. We highly recommend viewing the supplementaryvideo at https://sites.google.com/view/im2-humanoid-arm.</description>
      <author>example@mail.com (Jaehyung Kim, Jiho Kim, Dongryung Lee, Yujin Jang, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.16908v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Gazing at Failure: Investigating Human Gaze in Response to Robot Failure in Collaborative Tasks</title>
      <link>http://arxiv.org/abs/2502.16899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  this paper is accepted in HRI conference as a full paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人类的非语言行为，特别是目光动态如何作为机器人故障的信号，并分析不同类型的机器人失败对人们感知机器人的影响。&lt;h4&gt;背景&lt;/h4&gt;机器人在与人类用户合作完成任务时可能会出现错误，这些错误会损害它们作为团队成员的信任度。检测和恢复此类故障对于维持有效的信任水平至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过分析人类的目光动态来检测机器人未察觉的失败，并探讨不同类型及时间发生的失败如何影响人们对机器人的看法。&lt;h4&gt;方法&lt;/h4&gt;进行了一项针对27名参与者的研究，他们与移动机械臂协作解决拼图游戏。机器人被编程以经历两种类型的故障——执行型和决策型，在任务开始或结束时发生，且有时会承认这些故障的存在。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现显示，不同类型及时间发生的机器人的失败显著影响了参与者的目光行为和对机器人的看法。例如，执行性故障导致更多的目光转移以及更关注机器人；而决策性故障则在任务末尾时导致目标区域间的眼球运动熵降低。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过观察人类的目光动态可以作为识别机器人故障及其类型的可靠指标，并且这些信息可用来预测适当的恢复行动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are prone to making errors, which can negatively impact theircredibility as teammates during collaborative tasks with human users. Detectingand recovering from these failures is crucial for maintaining effective levelof trust from users. However, robots may fail without being aware of it. Oneway to detect such failures could be by analysing humans' non-verbal behavioursand reactions to failures. This study investigates how human gaze dynamics cansignal a robot's failure and examines how different types of failures affectpeople's perception of robot. We conducted a user study with 27 participantscollaborating with a robotic mobile manipulator to solve tangram puzzles. Therobot was programmed to experience two types of failures -- executional anddecisional -- occurring either at the beginning or end of the task, with orwithout acknowledgement of the failure. Our findings reveal that the type andtiming of the robot's failure significantly affect participants' gaze behaviourand perception of the robot. Specifically, executional failures led to moregaze shifts and increased focus on the robot, while decisional failuresresulted in lower entropy in gaze transitions among areas of interest,particularly when the failure occurred at the end of the task. These resultshighlight that gaze can serve as a reliable indicator of robot failures andtheir types, and could also be used to predict the appropriate recoveryactions.</description>
      <author>example@mail.com (Ramtin Tabatabaei, Vassilis Kostakos, Wafa Johal)</author>
      <guid isPermaLink="false">2502.16899v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Variations of Augmented Lagrangian for Robotic Multi-Contact Simulation</title>
      <link>http://arxiv.org/abs/2502.16898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个新的多接触非线性互补问题（NCP）求解器系列，基于增广拉格朗日理论。&lt;h4&gt;背景&lt;/h4&gt;多接触非线性互补问题是机器人模拟中自然出现的挑战。在涉及密集接触和刚性相互作用的情况下，同时实现高精度和效率是一个重大难题。&lt;h4&gt;目的&lt;/h4&gt;介绍一类新的多接触NCP求解器，并展示其在机器人模拟中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;通过迭代替代问题解决方案并更新原始-对偶变量的方式，将传统的增广拉格朗日理论应用于处理多接触NCP。提出了两种针对机器人仿真的特定变体：级联牛顿增广拉格朗日（CANAL）和基于子系统的交替方向乘子法（SubADMM）。&lt;h4&gt;主要发现&lt;/h4&gt;CANAL能够准确且稳健地管理多接触NCP，而SubADMM则提供了计算速度、可扩展性和并行处理能力方面的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的求解器框架在各种机器人操纵场景中展示了其有效性，特别是在高自由度和大量接触点的多体系统中表现出了显著的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的描述已经被中文内容替代，作为对原文本信息的理解与总结，无需额外添加此键值对。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multi-contact nonlinear complementarity problem (NCP) is a naturallyarising challenge in robotic simulations. Achieving high performance in termsof both accuracy and efficiency remains a significant challenge, particularlyin scenarios involving intensive contacts and stiff interactions. In thisarticle, we introduce a new class of multi-contact NCP solvers based on thetheory of the Augmented Lagrangian (AL). We detail how the standard derivationof AL in convex optimization can be adapted to handle multi-contact NCP throughthe iteration of surrogate problem solutions and the subsequent update ofprimal-dual variables. Specifically, we present two tailored variations of ALfor robotic simulations: the Cascaded Newton-based Augmented Lagrangian (CANAL)and the Subsystem-based Alternating Direction Method of Multipliers (SubADMM).We demonstrate how CANAL can manage multi-contact NCP in an accurate and robustmanner, while SubADMM offers superior computational speed, scalability, andparallelizability for high degrees-of-freedom multibody systems with numerouscontacts. Our results showcase the effectiveness of the proposed solverframework, illustrating its advantages in various robotic manipulationscenarios.</description>
      <author>example@mail.com (Jeongmin Lee, Minji Lee, Sunkyung Park, Jinhee Yun, Dongjun Lee)</author>
      <guid isPermaLink="false">2502.16898v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Primitive-Planner: An Ultra Lightweight Quadrotor Planner with Time-optimal Primitives</title>
      <link>http://arxiv.org/abs/2502.16882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种超轻量级四旋翼飞行器轨迹规划方法，该方法利用时间最优的基本运动单元，在保证轨迹质量和系统轻量化的同时实现高效率。&lt;h4&gt;背景&lt;/h4&gt;目前许多研究致力于解决四旋翼飞行器同时满足高质量轨迹和低计算负载的问题，但现有的解决方案与实际需求之间仍存在差距。&lt;h4&gt;目的&lt;/h4&gt;提出一种超轻量级的四旋翼飞行器规划方法，以实现实时高效、碰撞安全且用户可定制的最优路径规划。&lt;h4&gt;方法&lt;/h4&gt;{'1': '设计了一种新的运动基本单元库，用于生成时间最优化和动力学可行性的轨迹，并实现离线计算。', '2': '提出了一种快速的碰撞检测算法，该算法具有确定的时间消耗且与采样分辨率无关。', '3': '根据用户的定义需求从安全的基本运动中选择最低成本的路径进行执行。', '4': '利用局部轨迹之间的转换关系来确保全局轨迹的平滑性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '该方法能生成最短飞行时间和距离且计算负担最小化的最优轨迹。', '2': '现实世界中的挑战实验验证了所提方法在各种环境下的鲁棒性。'}&lt;h4&gt;结论&lt;/h4&gt;提出的规划方法通过减少在线计算的功率消耗，同时确保高质量的轨迹，并展示了其在实际应用中优越的时间效率和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; It is a significant requirement for a quadrotor trajectory planner tosimultaneously guarantee trajectory quality and system lightweight. Manyresearchers focus on this problem, but there's still a gap between theirperformance and our common wish. In this paper, we propose an ultra lightweightquadrotor planner with time-optimal primitives. Firstly, a novel motionprimitive library is proposed to generate time-optimal and dynamical feasibletrajectories offline. Secondly, we propose a fast collision checking methodwith a deterministic time consumption, independent of the sampling resolutionof the primitives. Finally, we select the minimum cost trajectory to executeamong the safe primitives based on user-defined requirements. The propsedtransformation relation between the local trajectories ensures the smoothnessof the global trajectory. The planner reduces unnecessary online computingpower consumption as much as possible, while ensuring a high-qualitytrajectory. Benchmark comparisons show that our method can generate theshortest flight time and distance of trajectory with the lowest computationoverload. Challenging real-world experiments validate the robustness of ourmethod.</description>
      <author>example@mail.com (Jialiang Hou, Neng Pan, Zhepei Wang, Jialin Ji, Yuxiang Guan, Zhongxue Gan, Fei Gao)</author>
      <guid isPermaLink="false">2502.16882v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Fast Finite-Time Sliding Mode Control for Chattering-Free Trajectory Tracking of Robotic Manipulators</title>
      <link>http://arxiv.org/abs/2502.16867v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种无颤振快速终端滑模控制（FTSMC）策略，用于提高三自由度机械臂的轨迹跟踪精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统滑模控制由于系统不确定性及抖动效应，在实现高精度高效轨迹追踪方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了一种新型无颤振快速终端滑模控制策略，以增强机械臂的跟踪精度和稳定性，并确保有限时间内收敛。&lt;h4&gt;方法&lt;/h4&gt;采用牛顿-欧拉动力学建立控制框架并转化为状态空间表示形式。通过引入改进后的滑动面以及基于李雅普诺夫稳定性的分析来设计控制器，从而减少颤振同时保持了传统滑模控制的优点如快速响应和强大的抗干扰能力。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果显示该策略在轨迹跟踪性能、更快的收敛速度及更强稳定性方面优于传统的PD滑模控制（PDSMC）和终端滑模控制（TSMC），尤其适用于高精度机器人应用。&lt;h4&gt;结论&lt;/h4&gt;提出的FTSMC方法为解决机械臂精确轨迹追踪中的挑战提供了有前景的解决方案，展示了其在实现高性能、快速响应及稳健控制方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;实现机器人手臂的精确且高效的轨迹跟踪依然是一个关键难题，因为传统滑模控制（SMC）面临系统不确定性和抖动效应的问题。本文提出了一种无颤振快速终端滑模控制（FTSMC）策略用于三自由度（3-DOF）机械臂设计中，旨在增强其跟踪精度和鲁棒性，并确保有限时间内的收敛。该控制系统框架基于牛顿-欧拉动力学建立并进一步转化为状态空间表示形式以捕捉系统的角位移与速度信息。通过采用改进的滑动面以及李雅普诺夫稳定性分析为基础的设计方法，所提出的FTSMC有效减轻了颤振现象，同时保留了传统滑模控制的优点，如快速响应和强大的干扰抑制能力。经过与常规PD滑模控制（PDSMC）及终端滑模控制（TSMC）的对比实验严格评估控制器性能后发现，本论文提出的方法在轨迹跟踪性能、更快的收敛速度以及更强稳定性方面都优于现有方法，为高精度机器人应用提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving precise and efficient trajectory tracking in robotic arms remains akey challenge due to system uncertainties and chattering effects inconventional sliding mode control (SMC). This paper presents a chattering-freefast terminal sliding mode control (FTSMC) strategy for athree-degree-of-freedom (3-DOF) robotic arm, designed to enhance trackingaccuracy and robustness while ensuring finite-time convergence. The controlframework is developed using Newton-Euler dynamics, followed by a state-spacerepresentation that captures the system's angular position and velocity. Byincorporating an improved sliding surface and a Lyapunov-based stabilityanalysis, the proposed FTSMC effectively mitigates chattering while preservingthe advantages of SMC, such as fast response and strong disturbance rejection.The controller's performance is rigorously evaluated through comparisons withconventional PD sliding mode control (PDSMC) and terminal sliding mode control(TSMC). Simulation results demonstrate that the proposed approach achievessuperior trajectory tracking performance, faster convergence, and enhancedstability compared to existing methods, making it a promising solution forhigh-precision robotic applications.</description>
      <author>example@mail.com (Momammad Ali Ranjbar)</author>
      <guid isPermaLink="false">2502.16867v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building</title>
      <link>http://arxiv.org/abs/2502.16856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025. Dataset aviliable at  https://github.com/HKUST-Aerial-Robotics/SLABIM . Video attachment at  https://youtu.be/7NckgY15ABQ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了一种新的数据集SLABIM，该数据集结合了室内定位和建筑信息模型（BIM），旨在解决现有室内SLAM数据集中缺乏建筑物结构信息的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的室内SLAM数据集主要关注机器人传感器的数据采集，较少包含详细的建筑结构信息。这种不足限制了研究者们对真实场景中SLAM算法性能的全面评估和优化。&lt;h4&gt;目的&lt;/h4&gt;设计并建立首个结合BIM与SLAM技术的公开数据集SLABIM，用于促进室内定位系统的研究进展，并提高其在实际环境中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;该数据集基于香港科技大学的一栋大学建筑进行构建。首先建立了详细的BIM模型；然后通过多传感器套件采集真实场景下的数据，并生成施工后模型（As-Built Model）；最后，所有数据均进行了时间戳标注并组织成易于访问的形式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果展示了SLABIM在三个关键任务上的性能表现：注册、定位以及语义地图构建。这些测试验证了该数据集的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;通过发布开源的SLABIM数据集，研究者们可以更好地进行室内定位算法的研究，并且能够更有效地评估和改进相关技术的应用效果。&lt;h4&gt;翻译&lt;/h4&gt;现有室内同步定位与建图（SLAM）数据集主要关注机器人感知信息，缺乏对建筑结构的关注。为弥补这一不足，我们设计并构建了首个将SLAM和BIM相结合的数据集——SLABIM。该数据集提供了针对大学建筑的传感器数据，并将其分解、转换成易于使用的格式。通过多传感器套件采集多个会话下的数据及地图制作，我们获取到实际施工模型。所有相关数据均被时间戳标记并组织好，方便用户部署和测试使用。此外，我们部署了先进方法并在注册、定位和语义映射三项任务上报告实验结果，证明SLABIM的有效性和实用性。该数据集已在https://github.com/HKUST-Aerial-Robotics/SLABIM开放源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing indoor SLAM datasets primarily focus on robot sensing, often lackingbuilding architectures. To address this gap, we design and construct the firstdataset to couple the SLAM and BIM, named SLABIM. This dataset provides BIM andSLAM-oriented sensor data, both modeling a university building at HKUST. Theas-designed BIM is decomposed and converted for ease of use. We employ amulti-sensor suite for multi-session data collection and mapping to obtain theas-built model. All the related data are timestamped and organized, enablingusers to deploy and test effectively. Furthermore, we deploy advanced methodsand report the experimental results on three tasks: registration, localizationand semantic mapping, demonstrating the effectiveness and practicality ofSLABIM. We make our dataset open-source athttps://github.com/HKUST-Aerial-Robotics/SLABIM.</description>
      <author>example@mail.com (Haoming Huang, Zhijian Qiao, Zehuan Yu, Chuhao Liu, Shaojie Shen, Fumin Zhang, Huan Yin)</author>
      <guid isPermaLink="false">2502.16856v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Characterizing Structured versus Unstructured Environments based on Pedestrians' and Vehicles' Motion Trajectories</title>
      <link>http://arxiv.org/abs/2502.16847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过分析不同环境类型中行人和车辆的轨迹特征，提出了一种区分结构化和非结构化环境的方法，并利用K-means聚类和广义线性模型对现有数据集进行了分类。&lt;h4&gt;背景&lt;/h4&gt;行人在接近车辆运行时的行为在无结构环境中与有结构环境中存在差异。然而，现有的行人和车辆的轨迹数据集并未根据它们所处环境的性质进行分类，且关于无结构和有结构环境的定义难以量化。&lt;h4&gt;目的&lt;/h4&gt;开发一种更定量的方法来区分不同类型的行进环境，并为现有数据集提供一个基于环境特性的分类。&lt;h4&gt;方法&lt;/h4&gt;采用从各种轨迹中提取出来的特征（如平均速度、轨迹变化率）进行分析，利用K-means聚类和广义线性模型对这些特征进行处理以量化不同环境类型之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;行人轨迹的变异性和停车频率以及行人的密度在两种类型的环境中表现出明显的不同，可以用来分类现有的数据集。&lt;h4&gt;结论&lt;/h4&gt;通过对现有数据集中行人与车辆轨迹行为的研究，提出了一种区分结构化和非结构化环境的新方法。这种方法有助于改善自动驾驶汽车的行为预测算法。&lt;h4&gt;翻译&lt;/h4&gt;行人在接近车辆运行时的行为在无结构环境中（如繁忙的街道）与有结构环境中（如专用的人行道或公园）存在差异。这种行为上的差异对于开发适用于自动行驶车辆的轨迹预测算法非常重要，因为现有的行人和车辆的轨迹数据集并未根据它们所处环境的性质进行分类，并且关于非结构化和结构化环境的标准定义通常难以量化。本文通过分析不同数据集中提取出的各种特征（例如平均速度、路径变化率），应用K-means聚类与广义线性模型，提出了一种新方法来区分这些不同的环境类型。研究结果表明，行人轨迹的变异性和停止频率以及行人的密度在两种类型的环境中显著不同，可以用于现有数据集的分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ITSC55140.2022.9921899&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory behaviours of pedestrians and vehicles operating close to eachother can be different in unstructured compared to structured environments.These differences in the motion behaviour are valuable to be considered in thetrajectory prediction algorithm of an autonomous vehicle. However, theavailable datasets on pedestrians' and vehicles' trajectories that are commonlyused as benchmarks for trajectory prediction have not been classified based onthe nature of their environment. On the other hand, the definitions providedfor unstructured and structured environments are rather qualitative and hard tobe used for justifying the type of a given environment. In this paper, we havecompared different existing datasets based on a couple of extracted trajectoryfeatures, such as mean speed and trajectory variability. Through K-meansclustering and generalized linear models, we propose more quantitative measuresfor distinguishing the two different types of environments. Our results showthat features such as trajectory variability, stop fraction and density ofpedestrians are different among the two environmental types and can be used toclassify the existing datasets.</description>
      <author>example@mail.com (Mahsa Golchoubian, Moojan Ghafurian, Nasser Lashgarian Azad, Kerstin Dautenhahn)</author>
      <guid isPermaLink="false">2502.16847v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Online Friction Coefficient Identification for Legged Robots on Slippery Terrain Using Smoothed Contact Gradients</title>
      <link>http://arxiv.org/abs/2502.16843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, IEEE RA-L (2025) accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在线识别腿足机器人在滑坡地形上摩擦系数的框架，通过最小化实际状态和预测状态之间的残差来解决这一问题。&lt;h4&gt;背景&lt;/h4&gt;在腿足机器人的动态环境下，尤其是在滑坡地形上，准确地估计地面摩擦力对机器人的稳定性和效率至关重要。传统的摩擦系数识别方法在面对非光滑接触动力学时会产生无信息梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于刚体接触动力学的优化问题框架，通过引入平滑处理后的互补条件下的库仑摩擦来解决非光滑接触动力学带来的挑战，并利用拒绝法过滤掉不适宜的数据。&lt;h4&gt;方法&lt;/h4&gt;该框架将优化问题参数化为最小化实际状态和预测状态之间的残差之和。利用了分析的光滑梯度，解决了由非平滑接触动力学引发的无信息梯度的问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在各种初始条件下，所提出的框架能够快速且一致地识别出摩擦系数，并在使用四足机器人平台KAIST HOUND进行实验时验证了该框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种有效的在线方法来估计腿足机器人的摩擦系数，从而为实际应用中的运动规划和控制奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3541428&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes an online friction coefficient identification frameworkfor legged robots on slippery terrain. The approach formulates the optimizationproblem to minimize the sum of residuals between actual and predicted statesparameterized by the friction coefficient in rigid body contact dynamics.Notably, the proposed framework leverages the analytic smoothed gradient ofcontact impulses, obtained by smoothing the complementarity condition ofCoulomb friction, to solve the issue of non-informative gradients induced fromthe nonsmooth contact dynamics. Moreover, we introduce the rejection method tofilter out data with high normal contact velocity following contact initiationsduring friction coefficient identification for legged robots. To validate theproposed framework, we conduct the experiments using a quadrupedal robotplatform, KAIST HOUND, on slippery and nonslippery terrain. We observe that ourframework achieves fast and consistent friction coefficient identificationwithin various initial conditions.</description>
      <author>example@mail.com (Hajun Kim, Dongyun Kang, Min-Gyu Kim, Gijeong Kim, Hae-Won Park)</author>
      <guid isPermaLink="false">2502.16843v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants</title>
      <link>http://arxiv.org/abs/2502.13894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的导航方法，通过将大型视觉-语言模型与扩散网络结合来实现零样本学习环境下的机器人导航。这种方法利用预训练的基础模型进行知识迁移和泛化能力的传递。&lt;h4&gt;背景&lt;/h4&gt;家庭机器人在陌生环境中面临挑战，需要能够识别并推理关于新装饰和布局的信息。现有的强化学习方法无法直接应用于新的环境，因为它们通常依赖于广泛的映射和探索，导致耗时且效率低下。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，论文试图将预训练基础模型的逻辑知识和泛化能力转移到零样本导航中。&lt;h4&gt;方法&lt;/h4&gt;通过结合大型视觉-语言模型与扩散网络构建了一个视觉预测器，能够连续预测代理人在下一步可能观察到的内容。此外，为适应导航的时间特性，引入了历史时间信息以确保预测图像与导航场景对齐。最后设计了一种信息融合框架，将预测的未来帧嵌入目标导向策略中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟和真实环境中增强了导航控制，并展示了其强大的鲁棒性和通用性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了所提出方法的有效性和效率，展现了它改善机器人在各种场景下导航能力的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating unfamiliar environments presents significant challenges forhousehold robots, requiring the ability to recognize and reason about noveldecoration and layout. Existing reinforcement learning methods cannot bedirectly transferred to new environments, as they typically rely on extensivemapping and exploration, leading to time-consuming and inefficient. To addressthese challenges, we try to transfer the logical knowledge and thegeneralization ability of pre-trained foundation models to zero-shotnavigation. By integrating a large vision-language model with a diffusionnetwork, our approach named \mname ~constructs a visual predictor thatcontinuously predicts the agent's potential observations in the next step whichcan assist robots generate robust actions. Furthermore, to adapt the temporalproperty of navigation, we introduce temporal historical information to ensurethat the predicted image is aligned with the navigation scene. We thencarefully designed an information fusion framework that embeds the predictedfuture frames as guidance into goal-reaching policy to solve downstream imagenavigation tasks. This approach enhances navigation control and generalizationacross both simulated and real-world environments. Through extensiveexperimentation, we demonstrate the robustness and versatility of our method,showcasing its potential to improve the efficiency and effectiveness of roboticnavigation in diverse settings.</description>
      <author>example@mail.com (Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, Ruimao Zhang)</author>
      <guid isPermaLink="false">2502.13894v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
  <item>
      <title>Appeal prediction for AI up-scaled Images</title>
      <link>http://arxiv.org/abs/2502.14013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过开发一个包含136个基础图像和五种不同上采样方法的全面数据集，评估了各种深度学习模型在图像上采样任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;基于DNN或AI的上采样算法由于机器学习的进步变得越来越流行。然而，缺乏对真实世界图像范围广泛且包含主观评价的表现评测。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究空白，通过对大量实际图像进行主观和客观评估来比较不同上采样方法的效果，并训练用于检测这些方法的深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;使用136个基础图像和五种不同的上采样方法（Real-ESRGAN, BSRGAN, waifu2x, KXNet以及Lanczos）构建数据集，总共包含1496张注释图像。通过众包服务进行主观评价，并开发了开源工具AVRate Voyager来辅助标注。&lt;h4&gt;主要发现&lt;/h4&gt;在主观评价中，Real-ESRGAN和BSRGAN表现最佳。训练的深度神经网络能够有效地检测不同的上采样方法。同时评估了现有最先进的图像吸引力和质量模型的表现，结果表明这些模型预测性能不高，并因此开发了自己的两种新模型以改进性能。&lt;h4&gt;结论&lt;/h4&gt;研究证明了主观评价的重要性以及对广泛数据集进行详细评测的价值，并通过提供开源工具与数据促进了该领域的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于深度神经网络或人工智能的上采样算法由于机器学习的进步而变得越来越流行。使用卷积神经网络、生成对抗网络或者混合方法的各种上采样模型已发表。大多数模型仅利用PSNR和SSIM进行评估，或是通过少量示例图像来进行评价。然而，缺乏广泛的现实世界图像以及主观评价的表现评测，这是我们研究论文要解决的问题。为此，我们描述了开发的数据集，该数据集使用136个基础图像并采用五种不同的上采样方法，即Real-ESRGAN、BSRGAN、waifu2x、KXNet和Lanczos。总体而言，整个数据集中包含有1496张注释的图像。我们的数据集标注专注于图像吸引力，并使用众包工具AVRate Voyager进行操作。我们评估了不同方法在吸引力上的表现，结果显示Real-ESRGAN和BSRGAN是最好的。此外，我们也训练了一个深度神经网络来检测用于上采样的哪种方法，在评估中这些模型表现出较好的总体性能。除此之外，还对现有最先进的图像吸引力与质量模型进行了评估，结果表明没有一个模型显示出高的预测性能，因此我们又开发了两个自己的方法。第一个采用迁移学习并具有最佳表现，第二个模型则使用信号基特征和随机森林模型并具备整体优秀的性能。为促进开放科学领域的进一步研究，我们将数据集、标注工具及实现方法对外公开分享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DNN- or AI-based up-scaling algorithms are gaining in popularity due to theimprovements in machine learning. Various up-scaling models using CNNs, GANs ormixed approaches have been published. The majority of models are evaluatedusing PSRN and SSIM or only a few example images. However, a performanceevaluation with a wide range of real-world images and subjective evaluation ismissing, which we tackle in the following paper. For this reason, we describeour developed dataset, which uses 136 base images and five different up-scalingmethods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and Lanczos. Overall thedataset consists of 1496 annotated images. The labeling of our dataset focusedon image appeal and has been performed using crowd-sourcing employing ouropen-source tool AVRate Voyager. We evaluate the appeal of the differentmethods, and the results indicate that Real-ESRGAN and BSRGAN are the best.Furthermore, we train a DNN to detect which up-scaling method has been used,the trained models have a good overall performance in our evaluation. Inaddition to this, we evaluate state-of-the-art image appeal and quality models,here none of the models showed a high prediction performance, therefore we alsotrained two own approaches. The first uses transfer learning and has the bestperformance, and the second model uses signal-based features and a randomforest model with good overall performance. We share the data andimplementation to allow further research in the context of open science.</description>
      <author>example@mail.com (Steve Göring, Rasmus Merten, Alexander Raake)</author>
      <guid isPermaLink="false">2502.14013v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks</title>
      <link>http://arxiv.org/abs/2502.15462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合视觉信息和比赛状态信息的时空动作检测方法，通过图神经网络与先进的3D卷积神经网络相结合，实现了更好的性能。&lt;h4&gt;背景&lt;/h4&gt;足球数据分析依赖于两种数据源：球员在场上的位置以及他们执行的动作序列。大约每场比赛有2000个球事件需要进行精确和详尽的标注，这是一项繁琐且成本高昂的手动任务。&lt;h4&gt;目的&lt;/h4&gt;为了减少人工注释的工作量并提高自动化程度，本文旨在探索结合视觉信息和比赛状态信息的方法来改进动作检测算法。&lt;h4&gt;方法&lt;/h4&gt;假设职业球员的行为是相互依赖的，并认为加入周围球员的信息（例如位置、速度和团队归属）可以增强纯视觉预测。为此，作者提出了一种基于图神经网络与3D卷积神经网络相结合的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过将比赛状态信息整合进模型中，该方法展示了改进后的性能指标。&lt;h4&gt;结论&lt;/h4&gt;结合视觉和游戏状态数据能够提高时空动作检测的准确性，并为自动化足球分析提供了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：足球数据分析依赖于两种数据源：球员在场上的位置以及他们执行的动作序列。大约每场比赛有2000个球事件需要进行精确且详尽的手动注释，这是一项繁琐和成本高昂的任务。尽管最先进的时空动作检测方法显示出自动化此任务的前景，但它们缺乏对游戏上下文的理解。假设职业球员的行为是相互依赖的，我们假定将周围球员的信息（如位置、速度及团队归属）加入到纯粹视觉预测中可以增强其准确性。我们提出了一种结合图神经网络与最先进的3D卷积神经网络相结合的方法，展示了通过整合比赛状态信息而改进的性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soccer analytics rely on two data sources: the player positions on the pitchand the sequences of events they perform. With around 2000 ball events pergame, their precise and exhaustive annotation based on a monocular video streamremains a tedious and costly manual task. While state-of-the-artspatio-temporal action detection methods show promise for automating this task,they lack contextual understanding of the game. Assuming professional players'behaviors are interdependent, we hypothesize that incorporating surroundingplayers' information such as positions, velocity and team membership canenhance purely visual predictions. We propose a spatio-temporal actiondetection approach that combines visual and game state information via GraphNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstratingimproved metrics through game state integration.</description>
      <author>example@mail.com (Jeremie Ochin, Guillaume Devineau, Bogdan Stanciulescu, Sotiris Manitsaris)</author>
      <guid isPermaLink="false">2502.15462v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification</title>
      <link>http://arxiv.org/abs/2502.15637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，人们对开发能够跨多种下游任务泛化的时序数据基础模型产生了浓厚的兴趣。虽然已经引入了大量面向预测的基础模型，但针对时间序列分类的专门模型却相对稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一空白，我们提出了一种新的开源基础模型Mantis，该模型基于视觉变换器（ViT）架构，并通过对比学习方法进行预训练，以用于时间序列分类任务。&lt;h4&gt;方法&lt;/h4&gt;Mantis模型采用了对比学习方法来进行预训练。此外，还提出了几个适配器来处理多变量设置，这可以减少内存需求并建模通道间的相互依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，无论是在骨干网络被冻结的情况下还是在经过微调之后，Mantis都优于现有的基础模型，并且实现了最低的校准误差。&lt;h4&gt;结论&lt;/h4&gt;Mantis为时间序列分类提供了一种有效的解决方案，同时通过引入适配器机制处理多变量设置问题，进一步优化了性能和内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been increasing interest in developing foundationmodels for time series data that can generalize across diverse downstreamtasks. While numerous forecasting-oriented foundation models have beenintroduced, there is a notable scarcity of models tailored for time seriesclassification. To address this gap, we present Mantis, a new open-sourcefoundation model for time series classification based on the Vision Transformer(ViT) architecture that has been pre-trained using a contrastive learningapproach. Our experimental results show that Mantis outperforms existingfoundation models both when the backbone is frozen and when fine-tuned, whileachieving the lowest calibration error. In addition, we propose severaladapters to handle the multivariate setting, reducing memory requirements andmodeling channel interdependence.</description>
      <author>example@mail.com (Vasilii Feofanov, Songkang Wen, Marius Alonso, Romain Ilbert, Hongbo Guo, Malik Tiomoko, Lujia Pan, Jianfeng Zhang, Ievgen Redko)</author>
      <guid isPermaLink="false">2502.15637v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>A Universal Framework for Compressing Embeddings in CTR Prediction</title>
      <link>http://arxiv.org/abs/2502.15355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种模型无关的嵌入压缩框架（MEC），用于压缩点击率预测中的嵌入表，以减少内存使用和延迟，同时保持推荐质量。&lt;h4&gt;背景&lt;/h4&gt;准确的点击率预测对在线广告和推荐系统至关重要。虽然深度学习技术进步提高了捕捉特征交互和理解用户兴趣的能力，但是优化嵌入层仍常常被忽视。大型嵌入表会超过GPU内存限制，并且需要存储在CPU内存中，导致高内存消耗和频繁的数据传输延迟。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的压缩方法来解决现有推荐系统中的内存使用量过大和延迟问题。&lt;h4&gt;方法&lt;/h4&gt;1. 应用流行度加权正则化以平衡高频和低频特征的代码分布。2. 集成对比学习机制，确保量化码的均匀分布，提高嵌入的独特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在三个数据集上，我们的方法能够将内存使用量减少超过50倍，并且保持或提高了推荐性能。&lt;h4&gt;结论&lt;/h4&gt;提出的MEC框架可以有效地压缩嵌入表，降低内存消耗和延迟，同时不牺牲推荐质量。这为构建高效、大规模的推荐系统提供了一个新的途径。&lt;h4&gt;翻译&lt;/h4&gt;准确点击率预测对在线广告与推荐系统至关重要。近期深度学习的进步提高了捕捉特征交互及理解用户兴趣的能力，但优化嵌入层常被忽视。本文提出了一种模型无关的嵌入压缩（MEC）框架，通过量化预训练嵌入来压缩嵌入表，在不牺牲推荐质量的前提下减少内存使用和延迟。实验表明该方法在三个数据集上可以将内存消耗降低50倍以上，并保持或提高推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate click-through rate (CTR) prediction is vital for online advertisingand recommendation systems. Recent deep learning advancements have improved theability to capture feature interactions and understand user interests. However,optimizing the embedding layer often remains overlooked. Embedding tables,which represent categorical and sequential features, can become excessivelylarge, surpassing GPU memory limits and necessitating storage in CPU memory.This results in high memory consumption and increased latency due to frequentGPU-CPU data transfers. To tackle these challenges, we introduce aModel-agnostic Embedding Compression (MEC) framework that compresses embeddingtables by quantizing pre-trained embeddings, without sacrificing recommendationquality. Our approach consists of two stages: first, we applypopularity-weighted regularization to balance code distribution between high-and low-frequency features. Then, we integrate a contrastive learning mechanismto ensure a uniform distribution of quantized codes, enhancing thedistinctiveness of embeddings. Experiments on three datasets reveal that ourmethod reduces memory usage by over 50x while maintaining or improvingrecommendation performance compared to existing models. The implementation codeis accessible in our project repository https://github.com/USTC-StarTeam/MEC.</description>
      <author>example@mail.com (Kefan Wang, Hao Wang, Kenan Song, Wei Guo, Kai Cheng, Zhi Li, Yong Liu, Defu Lian, Enhong Chen)</author>
      <guid isPermaLink="false">2502.15355v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review</title>
      <link>http://arxiv.org/abs/2502.15346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  64 pages, 7 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;药物研发是一个耗时且成本高昂的过程，涉及到从目标结构检测到获得食品药品监督管理局(FDA)批准的多个步骤，并常伴随着安全问题。&lt;h4&gt;背景&lt;/h4&gt;传统方法在预测药物与靶标之间的相互作用方面存在局限性，特别是在捕捉复杂关系上。为此，深度学习模型被提出以克服这一挑战并提供精确和高效的预测结果。&lt;h4&gt;目的&lt;/h4&gt;通过概述有前途的研究方向和模型，各具有不同解决方案但针对同一问题，这篇论文旨在为研究人员提供更多准确且高效地预测药物-靶标相互作用的方法，从而加速更有效药物的开发。&lt;h4&gt;方法&lt;/h4&gt;从2016年到2025年间，总共分析了基于机器学习（主要是深度学习和图神经网络）的不同框架下的180种药物-靶标交互预测方法。&lt;h4&gt;主要发现&lt;/h4&gt;论文讨论了这些模型的新颖性、架构以及输入表示。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的技术和更优的方法，有可能加速药物研发过程并提高药物安全性。深度学习和图神经网络等先进技术在该领域展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;药物发现仍是一个缓慢且成本高昂的过程，包括从目标结构检测到获得FDA批准的多个步骤，并经常伴随着安全问题。准确预测药物与其靶标之间的相互作用以及使用更好的方法和技术开发新药，具有极大可能加速这一过程，最终实现更快地提供救命药物。传统的药物-靶标交互预测方法显示出局限性，在捕捉复杂关系方面尤其如此。因此，深度学习模型被提出以通过其精确和高效的最终结果克服这些挑战。通过概述有前途的研究方向和模型，各具不同的解决方案但针对同一问题，这篇论文旨在为研究人员提供更准确且高效地预测药物-靶标相互作用的方法的更好理解，从而加速开发出更有效的药物。在2016年到2025年间，总共分析了基于机器学习（主要是深度学习和图神经网络）的不同框架下的180种药物-靶标交互预测方法。此外，该论文还讨论了这些模型的新颖性、架构以及输入表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug discovery remains a slow and expensive process that involves many steps,from detecting the target structure to obtaining approval from the Food andDrug Administration (FDA), and is often riddled with safety concerns. Accurateprediction of how drugs interact with their targets and the development of newdrugs by using better methods and technologies have immense potential to speedup this process, ultimately leading to faster delivery of life-savingmedications. Traditional methods used for drug-target interaction predictionshow limitations, particularly in capturing complex relationships between drugsand their targets. As an outcome, deep learning models have been presented toovercome the challenges of interaction prediction through their precise andefficient end results. By outlining promising research avenues and models, eachwith a different solution but similar to the problem, this paper aims to giveresearchers a better idea of methods for even more accurate and efficientprediction of drug-target interaction, ultimately accelerating the developmentof more effective drugs. A total of 180 prediction methods for drug-targetinteractions were analyzed throughout the period spanning 2016 to 2025 usingdifferent frameworks based on machine learning, mainly deep learning and graphneural networks. Additionally, this paper discusses the novelty, architecture,and input representation of these models.</description>
      <author>example@mail.com (Ali Vefghi, Zahed Rahmati, Mohammad Akbari)</author>
      <guid isPermaLink="false">2502.15346v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning</title>
      <link>http://arxiv.org/abs/2502.15296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一个新的时间序列预测任务：扩展变量的时间序列预测（EVTSF），并提出了一个新的灵活的时空预测框架STEV来应对新增变量带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测（MTSF）长期是研究重点。传统研究假设固定的变量数量，但实际应用中随着新传感器部署，系统的变量会增多。&lt;h4&gt;目的&lt;/h4&gt;解决由于新变量加入导致的数据形状不一致和时空学习不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了STEV框架，包括一个新的Flat Scheme来处理数据形状不一致问题，并引入了一种新的时空聚焦学习策略，解决了对比学习与图表示之间的潜在冲突。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，STEV在扩展变量上的表现显著优于其竞争对手。即使只使用5%的观察值，STEV也能达到最先进的MTSF模型的表现水平。&lt;h4&gt;结论&lt;/h4&gt;STEV是一个通用性强、性能优越的时空预测框架，适用于实际应用中的各种扩展策略。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列预测（MTSF）一直是研究的核心领域。传统方法假设固定的变量数量，但在现实世界的应用中，随着新传感器部署，Cyber-Physical系统的变量会增加。为此，我们提出了一个新的任务——扩展变量的时间序列预测（EVTSF）。这个任务面临两个挑战：一是处理新增变量导致的数据形状不一致问题；二是解决时空学习不平衡的问题，即由于需要及时操作而新加入的变量观测数据有限。为了解决这些问题，我们提出了一种灵活的时空预测框架STEV，它包含了一个新的Flat Scheme来应对数据形状不一致，并引入了新颖的时空聚焦学习策略。通过三个真实世界的数据集对EVTSF的表现进行了基准测试，并与三个可能解决方案（采用最先进的MTSF模型定制为EVSTF）进行了比较。实验结果表明，STEV在扩展变量上的表现显著优于竞争对手，甚至使用仅5%的新增时期观察值时也能达到最佳水平的效果。进一步研究各种扩展策略证实了STEV在现实应用中的通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate Time Series Forecasting (MTSF) has long been a key researchfocus. Traditionally, these studies assume a fixed number of variables, but inreal-world applications, Cyber-Physical Systems often expand as new sensors aredeployed, increasing variables in MTSF. In light of this, we introduce a noveltask, Expanding-variate Time Series Forecasting (EVTSF). This task presentsunique challenges, specifically (1) handling inconsistent data shapes caused byadding new variables, and (2) addressing imbalanced spatio-temporal learning,where expanding variables have limited observed data due to the necessity fortimely operation. To address these challenges, we propose STEV, a flexiblespatio-temporal forecasting framework. STEV includes a new Flat Scheme totackle the inconsistent data shape issue, which extends the graph-basedspatio-temporal modeling architecture into 1D space by flattening the 2Dsamples along the variable dimension, making the model variable-scale-agnosticwhile still preserving dynamic spatial correlations through a holistic graph.We introduce a novel Spatio-temporal Focal Learning strategy that incorporatesa negative filter to resolve potential conflicts between contrastive learningand graph representation, and a focal contrastive loss as its core to guide theframework to focus on optimizing the expanding variables. We benchmark EVTSFperformance using three real-world datasets and compare it against threepotential solutions employing SOTA MTSF models tailored for EVSTF. Experimentalresults show that STEV significantly outperforms its competitors, particularlyon expanding variables. Notably, STEV, with only 5% of observations from theexpanding period, is on par with SOTA MTSF models trained with completeobservations. Further exploration of various expanding strategies underscoresthe generalizability of STEV in real-world applications.</description>
      <author>example@mail.com (Minbo Ma, Kai Tang, Huan Li, Fei Teng, Dalin Zhang, Tianrui Li)</author>
      <guid isPermaLink="false">2502.15296v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Learning Maritime Inventory Routing Optimization</title>
      <link>http://arxiv.org/abs/2502.15244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于机器学习的局部搜索方法，用于解决大规模海洋库存路由优化问题。&lt;h4&gt;背景&lt;/h4&gt;海洋物流中的库存路由优化问题是组合复杂性很高的问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高大规模海洋库存路由问题解决方案的质量和效率。&lt;h4&gt;方法&lt;/h4&gt;集成图神经网络（GNN）来选择邻域以提升局部搜索的效率，实现对船舶邻近区域的有结构探索。&lt;h4&gt;主要发现&lt;/h4&gt;在大量实际案例中证明了该方法比直接使用混合整数规划法更加高效。&lt;h4&gt;结论&lt;/h4&gt;新方法提高了大规模海洋库存路由优化问题求解的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于机器学习的局部搜索方法，用于寻找大规模海上库存路由优化问题的可行解决方案。鉴于此类问题组合复杂度高，我们将图神经网络（GNN）为基础的邻域选择法融入进来以增强局部搜索效率。该方法实现了对船舶邻近区域有结构化的探索，在改善了解质量的同时也保证了计算效率。通过大量的实证实验验证，我们证明在真实案例中本研究的方法相比直接应用混合整数规划方法在求解时间上表现更优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a machine learning-based local search approach for findingfeasible solutions of large-scale maritime inventory routing optimizationproblems. Given the combinatorial complexity of the problems, we integrate agraph neural network-based neighborhood selection method to enhance localsearch efficiency. Our approach enables a structured exploration of vesselneighborhoods, improving solution quality while maintaining computationalefficiency. Through extensive computational experiments on realistic instances,we demonstrate that our method outperforms direct mixed-integer programming insolution time.</description>
      <author>example@mail.com (Rui Chen, Defeng Liu, Nan Jiang, Rishabh Gupta, Mustafa Kilinc, Andrea Lodi)</author>
      <guid isPermaLink="false">2502.15244v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</title>
      <link>http://arxiv.org/abs/2502.15251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于从野外视频中获取的手图像预训练三维手部姿态估计的框架SimHand。&lt;h4&gt;背景&lt;/h4&gt;现有的3D手部姿势预训练方法未能充分利用来自野外视频中的多样化手部图像的潜力。&lt;h4&gt;目的&lt;/h4&gt;为了实现可扩展性的预训练，准备了一个广泛的包含200多万张手部图像的数据池，并设计了基于对比学习的方法。&lt;h4&gt;方法&lt;/h4&gt;从最近的人类中心视频中收集超过2.0M的手部图像；通过关注手部相似性来提取区分信息，即非相同样本但具有类似手部姿势的成对样本；提出了一种新的对比学习方法，将类似的双手对在特征空间中嵌得更近，并根据样本间的距离自适应地调整对比学习损失权重。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法优于基于单张图像数据增强生成正例的传统对比学习方法。相比最先进的方法PeCLR，在FreiHand、DexYCB和AssemblyHands数据集上的表现分别提高了15%、10%和4%&lt;h4&gt;结论&lt;/h4&gt;我们的SimHand方法在预训练3D手部姿态估计方面提供了显著的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种框架，用于从具有类似手部特征的手图像中进行三维手部姿势的预训练。使用大规模图像可以在各种任务中实现令人满意的结果，但现有的3D手部姿势预训练方法尚未充分利用从野外视频中获取的各种手部图像的潜力。为了促进可扩展性的预训练，我们首先准备了一个包含超过2.0M手部图像的数据池，并设计了一种基于对比学习的方法进行预训练。通过关注手部相似性来提取区分信息：即非相同的样本但具有类似的手部姿势对。然后，我们提出了一种新的对比学习方法，该方法将类似的双手对在特征空间中嵌得更近，并根据样本间的距离自适应地调整对比学习损失权重。实验表明，我们的方法优于基于单张图像数据增强生成正例的传统对比学习方法。在FreiHand、DexYCB和AssemblyHands数据集上，相比最先进的方法PeCLR，我们实现了显著的性能提升（分别提高了15%、10%和4%）。&lt;h4&gt;代码链接&lt;/h4&gt;https://github.com/ut-vision/SiMHand&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a framework for pre-training of 3D hand pose estimation fromin-the-wild hand images sharing with similar hand characteristics, dubbedSimHand. Pre-training with large-scale images achieves promising results invarious tasks, but prior methods for 3D hand pose pre-training have not fullyutilized the potential of diverse hand images accessible from in-the-wildvideos. To facilitate scalable pre-training, we first prepare an extensive poolof hand images from in-the-wild videos and design our pre-training method withcontrastive learning. Specifically, we collect over 2.0M hand images fromrecent human-centric videos, such as 100DOH and Ego4D. To extractdiscriminative information from these images, we focus on the similarity ofhands: pairs of non-identical samples with similar hand poses. We then proposea novel contrastive learning method that embeds similar hand pairs closer inthe feature space. Our method not only learns from similar samples but alsoadaptively weights the contrastive learning loss based on inter-sampledistance, leading to additional performance gains. Our experiments demonstratethat our method outperforms conventional contrastive learning approaches thatproduce positive pairs sorely from a single image with data augmentation. Weachieve significant improvements over the state-of-the-art method (PeCLR) invarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% onAssemblyHands.  Our code is available at https://github.com/ut-vision/SiMHand.</description>
      <author>example@mail.com (Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato)</author>
      <guid isPermaLink="false">2502.15251v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval</title>
      <link>http://arxiv.org/abs/2502.15682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架ELIP，用于增强大规模预训练的视觉-语言模型在文本到图像检索中的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的大型预训练视觉-语言模型难以直接应用于文本到图像重排序任务。&lt;h4&gt;目的&lt;/h4&gt;通过引入一个新框架来提高大规模预训练视觉-语言模型在文本到图像检索上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种增强的语言-图像预训练（ELIP）方法，该方法利用文本查询预测一组视觉提示以调节ViT图像编码。此方法可以应用于CLIP/SigLIP和最先进的BLIP-2架构，并开发了适用于计算资源有限情况下的'学生友好型'最佳实践。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证，ELIP框架显著提高了CLIP/SigLIP的性能，并在文本到图像检索任务上超过了当前最先进模型BLIP-2的表现。&lt;h4&gt;结论&lt;/h4&gt;使用新架构和数据整理方法能够有效提升大规模视觉语言预训练模型用于文本到图像检索时的效果。&lt;h4&gt;翻译&lt;/h4&gt;该论文的目标是改进从文本到图像检索的性能。为此，研究者们提出了一种新的框架，可以增强大规模预训练视觉-语言模型的表现力，使其可用于重排序任务中。通过使用文本查询预测一组视觉提示来调节ViT图象编码的方法被称作ELIP，并且可以在常见的CLIP/SigLIP和最先进BLIP-2架构上应用。为了在计算资源有限的情况下训练该框架，研究者们开发了一种友好的实践方法，包括全局难例挖掘、大规模数据集的选择与整理等措施。评估方面，论文设立了两个新的分布外基准测试（Occluded COCO 和 ImageNet-R），用以衡量模型在不同领域中的零样本泛化能力。实验表明，由于新颖的架构和数据整理技术，增强后的网络显著提高了CLIP/SigLIP的表现，并且在文本到图像检索任务上超过了最先进的BLIP-2模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The objective in this paper is to improve the performance of text-to-imageretrieval. To this end, we introduce a new framework that can boost theperformance of large-scale pre-trained vision-language models, so that they canbe used for text-to-image re-ranking. The approach, Enhanced Language-ImagePre-training (ELIP), uses the text query to predict a set of visual prompts tocondition the ViT image encoding. ELIP can easily be applied to the commonlyused CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train thearchitecture with limited computing resources, we develop a 'student friendly'best practice involving global hard sample mining, and selection and curationof a large-scale dataset. On the evaluation side, we set up two newout-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess thezero-shot generalisation of the models to different domains. Benefiting fromthe novel architecture and data curation, experiments show our enhanced networksignificantly boosts CLIP/SigLIP performance and outperforms thestate-of-the-art BLIP-2 model on text-to-image retrieval.</description>
      <author>example@mail.com (Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman)</author>
      <guid isPermaLink="false">2502.15682v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer</title>
      <link>http://arxiv.org/abs/2502.15202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GNN-Coder是一种基于图神经网络（GNN）的新型框架，用于利用抽象语法树（AST），该方法旨在提高代码检索任务中的结构和语义特征捕捉能力。&lt;h4&gt;背景&lt;/h4&gt;现有的依赖于序列模型的方法在大型项目中难以完全发挥代码内在结构依赖的作用，尤其是在处理复杂结构的代码片段时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过研究如何将GNN与Transformer结合来促进语义检索任务的发展，并引入一种新颖的图池化方法以及一个新的量化代码嵌入分布均匀性的度量指标MAM。&lt;h4&gt;方法&lt;/h4&gt;提出了基于抽象语法树（AST）的图神经网络框架GNN-Coder，该框架使用子节点数量作为关键特征来突出AST内的内在拓扑关系。同时引入了新的度量标准Mean Angular Margin (MAM) 来衡量代码嵌入分布的一致性和特征分离性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，GNN-Coder在CSN数据集上的MRR（平均准确率）提高了1%-10%，在CosQA数据集上的零样本性能显著提升了20%。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的框架GNN-Coder可以有效提高代码检索任务中的结构和语义特征捕捉能力，从而增强模型对不同代码片段的区分能力和检索准确性。&lt;h4&gt;翻译&lt;/h4&gt;代码检索是现代软件开发的关键组成部分，在大型项目中尤为重要。然而，现有的序列模型方法往往未能充分利用代码固有的结构性依赖关系，导致在复杂结构代码片段上的检索性能不佳。本文介绍了一种基于图神经网络（GNN）的新框架——GNN-Coder，用于利用抽象语法树（AST）。这是首次尝试研究如何通过捕获代码的结构和语义特征来促进语义检索任务的发展，并引入了一个新的针对AST设计的图池化方法，使用子节点数量作为关键特性以突出显示AST内的内在拓扑关系。该设计有效集成了序列表示与层次表示，增强了模型捕捉代码结构和语义的能力。此外，还提出了一种新的度量指标——均值角度余量（MAM），用于量化代码嵌入分布的均匀性，提供了特征分离性的标准化衡量方法。所提出的这种方法实现了更低的MAM值，表明了更具有区分性的特征表示能力。这强调了GNN-Coder在区分不同代码片段方面的优越能力，并提高了检索准确性。实验结果表明，GNN-Coder显著提升了检索性能，在CSN数据集上平均准确率（MRR）提高了1%-10%，在CosQA数据集上的零样本性能显著提升20%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Code retrieval is a crucial component in modern software development,particularly in large-scale projects. However, existing approaches relying onsequence-based models often fail to fully exploit the structural dependenciesinherent in code, leading to suboptimal retrieval performance, particularlywith structurally complex code fragments. In this paper, we introduceGNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilizeAbstract Syntax Tree (AST). We make the first attempt to study howGNN-integrated Transformer can promote the development of semantic retrievaltasks by capturing the structural and semantic features of code. We furtherpropose an innovative graph pooling method tailored for AST, utilizing thenumber of child nodes as a key feature to highlight the intrinsic topologicalrelationships within the AST. This design effectively integrates bothsequential and hierarchical representations, enhancing the model's ability tocapture code structure and semantics. Additionally, we introduce the MeanAngular Margin (MAM), a novel metric for quantifying the uniformity of codeembedding distributions, providing a standardized measure of featureseparability. The proposed method achieves a lower MAM, indicating a morediscriminative feature representation. This underscores GNN-Coder's superiorability to distinguish between code snippets, thereby enhancing retrievalaccuracy. Experimental results show that GNN-Coder significantly boostsretrieval performance, with a 1\%-10\% improvement in MRR on the CSN dataset,and a notable 20\% gain in zero-shot performance on the CosQA dataset.</description>
      <author>example@mail.com (Yufan Ye, Pu Pang, Ting Zhang, Hua Huang)</author>
      <guid isPermaLink="false">2502.15202v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Context Transformer for Multi-level Semantic Scene Understanding</title>
      <link>http://arxiv.org/abs/2502.15184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by the IEEE TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种多层次语义场景理解（MSSU）的方法，用于手术场景的理解，并设计了一个层次化上下文变换器（HCT）网络。&lt;h4&gt;背景&lt;/h4&gt;在开发基于上下文感知的计算机辅助系统中，对手术场景的全面理解和显式认识至关重要。然而，很少有研究提供系统的分析以实现分层的手术场景理解。&lt;h4&gt;目的&lt;/h4&gt;提出一个多层级语义场景理解的方法和层次化上下文变换器网络来解决现有不足，并探索不同级别任务之间的关系。&lt;h4&gt;方法&lt;/h4&gt;设计了一个层次化相关聚合模块（HRAM），同时关联多级交互信息内部条目，然后增强特定于任务的特征。为了进一步促进各种任务的表示学习，提出了跨任务对比学习（ICL）以引导模型通过吸收其他任务提供的补充信息来学习具有任务特性的功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在白内障数据集和公共可用的内窥镜PSI-AVA数据集上的广泛实验，显示了该方法卓越的性能，并且始终大幅超越现有最佳方法。&lt;h4&gt;结论&lt;/h4&gt;提出的HCT+能够利用空间和时间适配器，在大量可调参数减少的情况下实现竞争力的表现。这些发现表明该方法在手术场景理解方面具有重要潜力。&lt;h4&gt;翻译&lt;/h4&gt;对手术场景进行全面而明确的理解对开发基于上下文感知的计算机辅助系统至关重要，但关于分层手术场景理解的研究较少提供系统分析。这项工作提出了将任务集表示为多层次语义场景理解（MSSU），并提出了一种新型层次化上下文变换器（HCT）网络来解决这一问题，并彻底探索了不同级别任务之间的关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A comprehensive and explicit understanding of surgical scenes plays a vitalrole in developing context-aware computer-assisted systems in the operatingtheatre. However, few works provide systematical analysis to enablehierarchical surgical scene understanding. In this work, we propose torepresent the tasks set [phase recognition --&gt; step recognition --&gt; action andinstrument detection] as multi-level semantic scene understanding (MSSU). Forthis target, we propose a novel hierarchical context transformer (HCT) networkand thoroughly explore the relations across the different level tasks.Specifically, a hierarchical relation aggregation module (HRAM) is designed toconcurrently relate entries inside multi-level interaction information and thenaugment task-specific features. To further boost the representation learning ofthe different tasks, inter-task contrastive learning (ICL) is presented toguide the model to learn task-wise features via absorbing complementaryinformation from other tasks. Furthermore, considering the computational costsof the transformer, we propose HCT+ to integrate the spatial and temporaladapter to access competitive performance on substantially fewer tunableparameters. Extensive experiments on our cataract dataset and a publiclyavailable endoscopic PSI-AVA dataset demonstrate the outstanding performance ofour method, consistently exceeding the state-of-the-art methods by a largemargin. The code is available at https://github.com/Aurora-hao/HCT.</description>
      <author>example@mail.com (Luoying Hao, Yan Hu, Yang Yue, Li Wu, Huazhu Fu, Jinming Duan, Jiang Liu)</author>
      <guid isPermaLink="false">2502.15184v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Expansion for Hypergraph Learning</title>
      <link>http://arxiv.org/abs/2502.15564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了新的自适应扩展方法AdE，该方法通过基于团的扩展方式将超图转换为加权图，并利用全局模拟网络和距离感知核函数来保持高阶结构信息。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着对捕获更高阶关系能力的要求，超图受到了广泛关注。许多超图表示学习方法也随之出现。&lt;h4&gt;目的&lt;/h4&gt;为了克服经典扩展方法在固定边权重设计中导致的信息丢失或冗余问题，提出了一种新的自适应扩展方法AdE。&lt;h4&gt;方法&lt;/h4&gt;通过引入全局模拟网络选择每个超边中的两个代表性节点，并将同一超边中的其余节点连接到相应的选定节点。设计了距离感知核函数，动态调整边缘权重以确保相似的节点具有更大的重量。&lt;h4&gt;主要发现&lt;/h4&gt;AdE相比经典的扩展模型在理论合理性、泛化能力和有效性方面都表现出色&lt;h4&gt;结论&lt;/h4&gt;提出的AdE方法能够更好地保持和利用超图中的高阶结构信息&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraph, with its powerful ability to capture higher-order relationships,has gained significant attention recently. Consequently, many hypergraphrepresentation learning methods have emerged to model the complex relationshipsamong hypergraphs. In general, these methods leverage classic expansion methodsto convert hypergraphs into weighted or bipartite graphs, and further employmessage passing mechanisms to model the complex structures within hypergraphs.However, classical expansion methods are designed in straightforward mannerswith fixed edge weights, resulting in information loss or redundancy. In lightof this, we design a novel clique expansion-based Adaptive Expansion methodcalled AdE to adaptively expand hypergraphs into weighted graphs that preservethe higher-order structure information. Specifically, we introduce a novelGlobal Simulation Network to select two representative nodes for adaptivelysymbolizing each hyperedge and connect the rest of the nodes within the samehyperedge to the corresponding selected nodes. Afterward, we design adistance-aware kernel function, dynamically adjusting edge weights to ensuresimilar nodes within a hyperedge are connected with larger weights. Extensivetheoretical justifications and empirical experiments over seven benchmarkhypergraph datasets demonstrate that AdE has excellent rationality,generalization, and effectiveness compared to classic expansion models.</description>
      <author>example@mail.com (Tianyi Ma, Yiyue Qian, Shinan Zhang, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2502.15564v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>mStyleDistance: Multilingual Style Embeddings and their Evaluation</title>
      <link>http://arxiv.org/abs/2502.15168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2410.12757&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种多语言风格嵌入模型mStyleDistance，该模型使用合成数据和对比学习进行训练。', '背景': '现有的风格嵌入仅限于英语，缺乏对多种语言的支持。', '目的': '开发一种能够在多种语言中有效工作的风格嵌入模型，并用于评估其质量和性能。', '方法': '利用来自九种不同语言的数据训练mStyleDistance模型；创建一个多语言的STEL或内容基准测试来评估嵌入的质量；在跨语种作者身份验证任务中应用该模型。', '主要发现': '实验结果表明，mStyleDistance风格嵌入优于现有模型，在多语言风格基准上表现出色，并且能够很好地推广到未见过的语言和特征。', '结论': '展示了mStyleDistance的潜在价值及其在跨语种分析中的优势；源代码已公开发布。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新的多语言风格嵌入模型，名为Multilingual Style Distance (mStyleDistance)，该模型通过合成数据和对比学习进行了训练，并用于评估其质量和性能。研究显示了该方法在处理多种语言的风格分析任务中的优越性，并且能够很好地推广到新场景中去使用。研究成果已经公开分享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Style embeddings are useful for stylistic analysis and style transfer;however, only English style embeddings have been made available. We introduceMultilingual StyleDistance (mStyleDistance), a multilingual style embeddingmodel trained using synthetic data and contrastive learning. We train the modelon data from nine languages and create a multilingual STEL-or-Content benchmark(Wegmann et al., 2022) that serves to assess the embeddings' quality. We alsoemploy our embeddings in an authorship verification task involving differentlanguages. Our results show that mStyleDistance embeddings outperform existingmodels on these multilingual style benchmarks and generalize well to unseenfeatures and languages. We make our model publicly available athttps://huggingface.co/StyleDistance/mstyledistance .</description>
      <author>example@mail.com (Justin Qiu, Jiacheng Zhu, Ajay Patel, Marianna Apidianaki, Chris Callison-Burch)</author>
      <guid isPermaLink="false">2502.15168v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2502.15635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;为了评估端到端的自动驾驶系统，基于新颖视图合成（NVS）技术的模拟环境是必不可少的。这种模拟可以生成新的车辆姿态下的真实感图像和点云数据，尤其是在跨车道场景中。&lt;h4&gt;目的&lt;/h4&gt;由于现有的合成场景基础的NVS数据集在现实性和捕捉到的图像及点云的真实度方面仍然存在不足，因此开发一个多车道的数据集和基准是必要的。该研究旨在基于NeRF和3DGS方法进一步评估现有方法的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新方法来创建第一个多车道数据集，通过并行扫描真实世界获取25组相关序列，其中包括16,000张前置视图图像、64,000张环视图像以及16,000个LiDAR帧。所有帧都被标记以区分移动物体和静态元素。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用该数据集，在不同车道和距离的多种测试场景中评估现有方法的表现，并提供解决多传感器姿态问题的方法，实现跨模式数据对齐。&lt;h4&gt;结论&lt;/h4&gt;该研究公开了一个名为Paralane的数据集（网址：https://nizqleo.github.io/paralane-dataset/），用于持续添加新的序列以测试现有的方法在不同场景中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;为了评估端到端的自动驾驶系统，一个基于新颖视图合成技术的模拟环境是必要的。该研究提出了一种新型多车道数据集和基准，旨在进一步检验NeRF和3DGS等现有方法的表现。新创建的数据集包括来自真实世界扫描并行扫描25组相关序列，含16000张前置视角图像、64,000环视图像及16,000个LiDAR帧，并且提供了解决跨模式数据对齐的多传感器姿态问题的方法。研究者计划持续添加新的序列以测试现有方法在不同场景中的泛化能力，数据集公开于项目页面：https://nizqleo.github.io/paralane-dataset/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To evaluate end-to-end autonomous driving systems, a simulation environmentbased on Novel View Synthesis (NVS) techniques is essential, which synthesizesphoto-realistic images and point clouds from previously recorded sequencesunder new vehicle poses, particularly in cross-lane scenarios. Therefore, thedevelopment of a multi-lane dataset and benchmark is necessary. While recentsynthetic scene-based NVS datasets have been prepared for cross-lanebenchmarking, they still lack the realism of captured images and point clouds.To further assess the performance of existing methods based on NeRF and 3DGS,we present the first multi-lane dataset registering parallel scans specificallyfor novel driving view synthesis dataset derived from real-world scans,comprising 25 groups of associated sequences, including 16,000 front-viewimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames arelabeled to differentiate moving objects from static elements. Using thisdataset, we evaluate the performance of existing approaches in various testingscenarios at different lanes and distances. Additionally, our method providesthe solution for solving and assessing the quality of multi-sensor poses formulti-modal data alignment for curating such a dataset in real-world. We planto continually add new sequences to test the generalization of existing methodsacross different scenarios. The dataset is released publicly at the projectpage: https://nizqleo.github.io/paralane-dataset/.</description>
      <author>example@mail.com (Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang)</author>
      <guid isPermaLink="false">2502.15635v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom in Epilepsy Patients</title>
      <link>http://arxiv.org/abs/2502.15198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;预测癫痫患者无发作状态对于个性化治疗至关重要。然而，传统的预测方法难以在不同类型的患者群体中实现准确的预测。&lt;h4&gt;背景&lt;/h4&gt;难治性癫痫患者的预后评估需要更有效的工具来提高治疗效果和减少副作用。&lt;h4&gt;目的&lt;/h4&gt;开发基于深度学习的图神经网络（GNN）模型以预测从立体脑电图（sEEG）数据中获得的无发作状态。&lt;h4&gt;方法&lt;/h4&gt;利用15名儿科难治性癫痫患者的高质量sEEG数据训练模型，使用图形卷积和多尺度注意力机制捕捉局部与全局连接性。&lt;h4&gt;主要发现&lt;/h4&gt;{'准确性': '在二元分类分析、患者级分析及多类分析中分别达到了92.4%、86.6%和81.4%的准确率；', '关键区域': '前扣带皮层与额极是预测无发作状态的关键脑区，并且这些区域更可能对应于癫痫发作起始区。', '模型识别': '模型标识出的节点更有可能重合于癫痫发作起始区，强调了基于连接性的新深度学习模型对于提高无发作状态预测、定位癫痫发作起始区及大脑在癫痫期间的连接性分析的重要性。'}&lt;h4&gt;结论&lt;/h4&gt;新型基于连接性的深度学习模型如GNN为改善难治性癫痫患者的个性化治疗提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;预测癫痫患者无发作状态对于个性化治疗至关重要。然而，传统的预测方法难以在不同类型的患者群体中实现准确的预测。本研究开发了一种基于深度学习的图神经网络（GNN）模型以预测从立体脑电图（sEEG）数据获得的无发作状态结果，并加深了对癫痫起始区大脑连接性的理解。该模型结合局部和全局连接性使用图形卷积与多尺度注意力机制来捕捉如丘脑及运动区域这样难以研究区域之间的连接，成功提高了预测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting seizure freedom is essential for tailoring epilepsy treatment. Butaccurate prediction remains challenging with traditional methods, especiallywith diverse patient populations. This study developed a deep learning-basedgraph neural network (GNN) model to predict seizure freedom from stereoelectroencephalography (sEEG) data in patients with refractory epilepsy. Weutilized high-quality sEEG data from 15 pediatric patients to train a deeplearning model that can accurately predict seizure freedom outcomes and advanceunderstanding of brain connectivity at the seizure onset zone. Our modelintegrates local and global connectivity using graph convolutions withmulti-scale attention mechanisms to capture connections betweendifficult-to-study regions such as the thalamus and motor regions. The modelachieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wiseanalysis, and 81.4% in multi-class analysis. Node and edge-level featureanalysis highlighted the anterior cingulate and frontal pole regions as keycontributors to seizure freedom outcomes. The nodes identified by our modelwere also more likely to coincide with seizure onset zones. Our findingsunderscore the potential of new connectivity-based deep learning models such asGNNs for enhancing the prediction of seizure freedom, predicting seizure onsetzones, connectivity analysis of the brain during seizure, as well as informingAI-assisted personalized epilepsy treatment planning.</description>
      <author>example@mail.com (Artur Agaronyan, Syeda Abeera Amir, Nunthasiri Wittayanakorn, John Schreiber, Marius G. Linguraru, William Gaillard, Chima Oluigbo, Syed Muhammad Anwar)</author>
      <guid isPermaLink="false">2502.15198v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning foundation models of materials interatomic potentials with frozen transfer learning</title>
      <link>http://arxiv.org/abs/2502.15582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习的原子间势能是通过提供训练数据覆盖范围内准确且可扩展的预测来革新原子材料模拟，但生成一个精确且稳健的数据集仍然是个挑战。本文展示了使用迁移学习可以提高基础模型的准确性，并构建了一个更为高效的人工智能模拟工作流程。&lt;h4&gt;背景&lt;/h4&gt;机器学习的原子间势能能够提供训练数据覆盖范围内准确和可扩展的预测，然而，要生成这种精准而健壮的数据集，通常需要数千次第一原理计算。现在开始出现一种旨在创建可以适用于广泛材料领域的基础模型。&lt;h4&gt;目的&lt;/h4&gt;展示通过迁移学习提高基础模型势能准确性，并构建一个改进了数据效率和计算效率的人工智能模拟工作流程。&lt;h4&gt;方法&lt;/h4&gt;利用部分冻结的权重和偏差进行微调，使用两个具有挑战性的表面反应化学以及三元合金稳定性和弹性性质的数据集作为案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;通过迁移学习，在只使用少量（几百个数据点）的情况下可以达到与从零开始训练模型相媲美的精度。此外，还可以利用这种经过调整的势能构建一个准确度相当但更高效的代理模型。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种改进了机器学习潜在效率的人工智能模拟工作流程，该流程提高了数据和计算使用率。&lt;h4&gt;翻译&lt;/h4&gt;机器学习原子间势场通过提供训练数据范围内的精确且可扩展的预测来革新原子材料模拟。生成准确且稳健的数据集仍然是一个挑战，通常需要数千次第一性原理计算才能获得高精度。现在正在出现一种基础模型，其目标是在广泛的材料中通用适用潜在场。虽然这些基础模型可以是健壮和转移的，但它们尚未达到预测反应势垒、相变以及物质稳定性所需的精确度。这项工作展示了通过使用部分冻结权重和偏差进行迁移学习微调的基础模型潜力可以实现化学精度。对于两个具有挑战性的数据集：表面反应化学与三元合金稳定性和弹性性质的研究表明，在使用10-20%的数据（几百个数据点）情况下，冻结转移学习达到的准确度与从头开始训练模型相仿（需要数千个数据点）。此外还展示了可以建立一个同样精确但计算更高效的替代模型，该模型以迁移学习后的势能为真值。综合来看，我们提出了一种改进了机器学习潜在效率的人工智能模拟工作流程，此流程提高了数据和计算使用率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned interatomic potentials are revolutionising atomisticmaterials simulations by providing accurate and scalable predictions within thescope covered by the training data. However, generation of an accurate androbust training data set remains a challenge, often requiring thousands offirst-principles calculations to achieve high accuracy. Foundation models havestarted to emerge with the ambition to create universally applicable potentialsacross a wide range of materials. While foundation models can be robust andtransferable, they do not yet achieve the accuracy required to predict reactionbarriers, phase transitions, and material stability. This work demonstratesthat foundation model potentials can reach chemical accuracy when fine-tunedusing transfer learning with partially frozen weights and biases. For twochallenging datasets on reactive chemistry at surfaces and stability andelastic properties of tertiary alloys, we show that frozen transfer learningwith 10-20% of the data (hundreds of datapoints) achieves similar accuracies tomodels trained from scratch (on thousands of datapoints). Moreover, we showthat an equally accurate, but significantly more efficient surrogate model canbe built using the transfer learned potential as the ground truth. Incombination, we present a simulation workflow for machine learning potentialsthat improves data efficiency and computational efficiency.</description>
      <author>example@mail.com (Mariia Radova, Wojciech G. Stark, Connor S. Allen, Reinhard J. Maurer, Albert P. Bartók)</author>
      <guid isPermaLink="false">2502.15582v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors</title>
      <link>http://arxiv.org/abs/2502.15540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a Spotlight Paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文建立了表示学习算法的期望和尾部泛化误差界限，并通过相对熵来描述训练集和测试集中提取出的表征分布之间的差异。&lt;h4&gt;背景&lt;/h4&gt;当前对于表示学习算法的泛化误差研究较少，现有的界限无法充分反映编码器结构与简单性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的数据依赖性对称先验（即训练和测试数据集潜在变量的最小描述长度）来建立更精确的泛化误差界限。&lt;h4&gt;方法&lt;/h4&gt;利用期望边界设计了一种合适的数据依赖正则项，并提出了同时学习数据依赖高斯混合先验并用其作为正则化的系统性方法，显示出了加权注意力机制的自然出现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法优于流行的变分信息瓶颈（VIB）和最近的类别依赖的VIB（CDVIB）。&lt;h4&gt;结论&lt;/h4&gt;新提出的界限和学习先验的方法有助于改善表示学习算法的表现。&lt;h4&gt;翻译&lt;/h4&gt;我们建立了表示学习类型算法的期望和尾部泛化误差界，这些边界以训练集和“测试”数据集中提取出的表征分布之间的相对熵来描述，并且相对于一个数据依赖性对称先验（即，对于训练和测试数据集潜在变量的最小描述长度）。我们证明了我们的界限反映了编码器的“结构”和“简单性”，并且显著改进了现有的少量边界。然后，我们将期望界用于设计合适的数据依赖正则项；并详细探讨了重要的先验选择问题。我们提出了一种系统方法，可以同时学习数据依赖高斯混合先验，并将其用作正则化器。有趣的是，在此过程中自然地出现了加权注意力机制。我们的实验表明，我们提出的方法优于流行的变分信息瓶颈（VIB）方法以及最近的类别依赖的VIB（CDVIB）方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We establish in-expectation and tail bounds on the generalization error ofrepresentation learning type algorithms. The bounds are in terms of therelative entropy between the distribution of the representations extracted fromthe training and "test'' datasets and a data-dependent symmetric prior, i.e.,the Minimum Description Length (MDL) of the latent variables for the trainingand test datasets. Our bounds are shown to reflect the "structure" and"simplicity'' of the encoder and significantly improve upon the few existingones for the studied model. We then use our in-expectation bound to devise asuitable data-dependent regularizer; and we investigate thoroughly theimportant question of the selection of the prior. We propose a systematicapproach to simultaneously learning a data-dependent Gaussian mixture prior andusing it as a regularizer. Interestingly, we show that a weighted attentionmechanism emerges naturally in this procedure. Our experiments show that ourapproach outperforms the now popular Variational Information Bottleneck (VIB)method as well as the recent Category-Dependent VIB (CDVIB).</description>
      <author>example@mail.com (Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski)</author>
      <guid isPermaLink="false">2502.15540v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>PDeepPP:A Deep learning framework with Pretrained Protein language for peptide classification</title>
      <link>http://arxiv.org/abs/2502.15610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, submitted to arXiv&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;蛋白质后翻译修饰（PTMs）和生物活性肽（BPs）在各种生物学过程中起着关键作用，并具有重要的治疗潜力。然而，通过实验方法识别这些位点既耗时又成本高昂。&lt;h4&gt;背景&lt;/h4&gt;现有计算工具，尤其是基于深度学习的方法，在预测PTM位点和肽类生物活性方面表现出色，但仍然面临蛋白质序列复杂性和跨不同数据集提供高质量预测的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合预训练蛋白质语言模型与融合Transformer和CNN的神经网络框架，以提高特征提取能力和预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该框架应用于多项任务中，包括PTM位点及生物活性肽预测，并通过大规模数据集提升模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在33项任务比较中，此模型在其中25项达到最先进水平，超越现有方法并展示出跨不同数据集的多功能性。&lt;h4&gt;结论&lt;/h4&gt;这种新方法为大规模肽类发现和PTM分析提供了可扩展且有效的方法，开启了更高效地肽类分类及功能注释的新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein post-translational modifications (PTMs) and bioactive peptides (BPs)play critical roles in various biological processes and have significanttherapeutic potential. However, identifying PTM sites and bioactive peptidesthrough experimental methods is often labor-intensive, costly, andtime-consuming. As a result, computational tools, particularly those based ondeep learning, have become effective solutions for predicting PTM sites andpeptide bioactivity. Despite progress in this field, existing methods stillstruggle with the complexity of protein sequences and the challenge ofrequiring high-quality predictions across diverse datasets.  To address these issues, we propose a deep learning framework that integratespretrained protein language models with a neural network combining transformerand CNN for peptide classification. By leveraging the ability of pretrainedmodels to capture complex relationships within protein sequences, combined withthe predictive power of parallel networks, our approach improves featureextraction while enhancing prediction accuracy.  This framework was applied to multiple tasks involving PTM site and bioactivepeptide prediction, utilizing large-scale datasets to enhance the model'srobustness. In the comparison across 33 tasks, the model achievedstate-of-the-art (SOTA) performance in 25 of them, surpassing existing methodsand demonstrating its versatility across different datasets. Our resultssuggest that this approach provides a scalable and effective solution forlarge-scale peptide discovery and PTM analysis, paving the way for moreefficient peptide classification and functional annotation.</description>
      <author>example@mail.com (Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Xueying Wang, Dan Huang)</author>
      <guid isPermaLink="false">2502.15610v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>P2W: From Power Traces to Weights Matrix -- An Unconventional Transfer Learning Approach</title>
      <link>http://arxiv.org/abs/2502.14968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的非传统迁移学习方法，用于在嵌入式SoC中训练机器学习模型。该方法通过提取已部署在SoC中的现有ML模型的权重来初始化新模型，而不是直接访问这些模型。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习（ML）模型在嵌入式芯片系统（SoCs）上的快速部署，医疗保健和自动驾驶汽车等领域发生了变革性的变化。然而，在这些场景中训练嵌入式ML模型的一个主要挑战是缺乏高质量的公开训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有迁移学习方法需要直接访问已存在模型这一限制问题，特别是在嵌入式SoC上运行的情况下。&lt;h4&gt;方法&lt;/h4&gt;通过从执行机器学习模型的SoC捕获功耗测量值并将这些值转换为权重矩阵来初始化新的ML模型。这种方法不需要直接获取嵌入式系统中的现有模型的具体信息。&lt;h4&gt;主要发现&lt;/h4&gt;新方法可以显著提高在数据稀缺环境下的模型准确性和预测性能，相比传统训练方式，在使用相同数量的受限训练数据的情况下，可以使新模型的准确性提升高达3倍。&lt;h4&gt;结论&lt;/h4&gt;提出的方法提供了一种有效的途径来利用现有的嵌入式ML模型的知识，以改进新模型的学习效率和预测表现。这种方法对于那些难以直接访问已有模型场景下的机器学习应用来说特别有用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在芯片上的系统（SoCs）上部署的机器学习（ML）模型数量快速增长已经对医疗保健、自动驾驶汽车等领域带来了变革性的变化。然而，在这些领域训练嵌入式ML模型的一个主要挑战是缺乏高质量的公共可用训练数据。迁移学习方法通过利用现有ML模型中的知识作为起点来应对这一挑战，从而用于训练新的ML模型。但是，现有的迁移学习方法需要直接访问现有的模型，这在许多情况下是不可行的，尤其是在部署在嵌入式SoC上的ML模型的情况下。因此，在本文中，我们提出了一种新颖的方法：通过提取并使用一个运行在嵌入式SoC中的现有ML模型的权重来训练一个新的ML模型，而不需要直接访问该模型。我们的方法采集了执行ML模型时从SoC捕捉到的能量消耗测量值，并将其转换为用于初始化新ML模型的大致权重矩阵。这提高了新模型的学习效率和预测性能，尤其是在可用来培训模型的数据量有限的情况下。与使用相同数量的受限训练数据的传统训练方式相比，我们新颖的方法可以有效提高新ML模型的准确度高达3倍。&lt;h4&gt;其他信息&lt;/h4&gt;{'关键词': ['迁移学习', '嵌入式SoC', '机器学习', '功耗测量', '权重初始化']}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of deploying machine learning (ML) models within embeddedsystems on a chip (SoCs) has led to transformative shifts in fields likehealthcare and autonomous vehicles. One of the primary challenges for trainingsuch embedded ML models is the lack of publicly available high-quality trainingdata. Transfer learning approaches address this challenge by utilizing theknowledge encapsulated in an existing ML model as a starting point for traininga new ML model. However, existing transfer learning approaches require directaccess to the existing model which is not always feasible, especially for MLmodels deployed on embedded SoCs. Therefore, in this paper, we introduce anovel unconventional transfer learning approach to train a new ML model byextracting and using weights from an existing ML model running on an embeddedSoC without having access to the model within the SoC. Our approach capturespower consumption measurements from the SoC while it is executing the ML modeland translates them to an approximated weights matrix used to initialize thenew ML model. This improves the learning efficiency and predictive performanceof the new model, especially in scenarios with limited data available to trainthe model. Our novel approach can effectively increase the accuracy of the newML model up to 3 times compared to classical training methods using the sameamount of limited training data.</description>
      <author>example@mail.com (Roozbeh Siyadatzadeh, Fatemeh Mehrafrooz, Nele Mentens, Todor Stefanov)</author>
      <guid isPermaLink="false">2502.14968v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Riemannian Sparse Representation Learning Network for Polarimetric SAR Image Classification</title>
      <link>http://arxiv.org/abs/2502.15302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的Riemannian稀疏表示学习网络(SRSR CNN)用于极化合成孔径雷达(PolSAR)图像分类。&lt;h4&gt;背景&lt;/h4&gt;深度学习是Polarimetric SAR图像分类的有效方法，但缺乏相关的数学原理指导，并且通常将复数协方差矩阵转换为欧几里得空间中的向量输入，这可能破坏了矩阵结构和通道关系。&lt;h4&gt;目的&lt;/h4&gt;通过引入Riemannian度量来更好地处理PolSAR的复杂矩阵结构，以提高分类准确性和边缘细节准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个基于超像素的Riemannian稀疏表示模型(SRSR)，该模型能够在黎曼空间中学习几何结构和稀疏特征。然后将其展开为一个网络，可以自动地学习稀疏系数和字典原子，并添加了CNN增强模块以提高上下文高级特征的学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的方法在保持准确的边缘细节和正确的区域同质性方面优于现有的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的基于SR指导的深度学习模型可以直接使用协方差矩阵作为网络输入，并且可以利用黎曼度量来学习复数矩阵在黎曼空间中的几何结构和稀疏特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning is an effective end-to-end method for Polarimetric SyntheticAperture Radar(PolSAR) image classification, but it lacks the guidance ofrelated mathematical principle and is essentially a black-box model. Inaddition, existing deep models learn features in Euclidean space, where PolSARcomplex matrix is commonly converted into a complex-valued vector as thenetwork input, distorting matrix structure and channel relationship. However,the complex covariance matrix is Hermitian positive definite (HPD), and resideson a Riemannian manifold instead of a Euclidean one. Existing methods cannotmeasure the geometric distance of HPD matrices and easily cause somemisclassifications due to inappropriate Euclidean measures. To address theseissues, we propose a novel Riemannian Sparse Representation Learning Network(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian SparseRepresentation (SRSR) model is designed to learn the sparse features withRiemannian metric. Then, the optimization procedure of the SRSR model isinferred and further unfolded into an SRSRnet, which can automatically learnthe sparse coefficients and dictionary atoms. Furthermore, to learn contextualhigh-level features, a CNN-enhanced module is added to improve classificationperformance. The proposed network is a Sparse Representation (SR) guided deeplearning model, which can directly utilize the covariance matrix as the networkinput, and utilize Riemannian metric to learn geometric structure and sparsefeatures of complex matrices in Riemannian space. Experiments on three realPolSAR datasets demonstrate that the proposed method surpasses state-of-the-arttechniques in ensuring accurate edge details and correct region homogeneity forclassification.</description>
      <author>example@mail.com (Junfei Shi, Mengmeng Nie, Weisi Lin, Haiyan Jin, Junhuai Li, Rui Wang)</author>
      <guid isPermaLink="false">2502.15302v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.15516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文讨论了毫米波雷达和相机在自动驾驶环境感知中的互补作用，通过结合4D毫米波雷达和深度感知的摄像机图像来提高3D物体检测精度。&lt;h4&gt;背景&lt;/h4&gt;安全性和可靠性对于公众接受自动驾驶至关重要。传统的3D毫米波雷达只能提供目标的距离、多普勒频移及方位信息，在恶劣天气条件下仍能保持良好性能，但点云稀疏。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过结合4D毫米波雷达和相机的优势来增强环境感知的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;['利用4D毫米波雷达提供的深度感知数据与相机图像进行融合，采用注意机制在鸟瞰图视角下将丰富的纹理图像与深度信息相结合', '提出了一种基于GAN的方法，在没有深度传感器的情况下从雷达频谱生成深度图像']&lt;h4&gt;主要发现&lt;/h4&gt;['通过结合4D毫米波雷达和相机可以有效提升环境感知的准确性，特别是在恶劣天气条件下。', '使用注意力机制能够更好地融合不同类型的感知数据，提高3D物体检测精度。', '基于GAN的方法有助于在缺乏直接深度信息的情况下生成有效的深度图像']&lt;h4&gt;结论&lt;/h4&gt;该研究证明了将4D毫米波雷达与相机结合可以显著提升自动驾驶系统的环境感知能力，并提出了一种新颖的解决方案来克服现有技术局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety and reliability are crucial for the public acceptance of autonomousdriving. To ensure accurate and reliable environmental perception, intelligentvehicles must exhibit accuracy and robustness in various environments.Millimeter-wave radar, known for its high penetration capability, can operateeffectively in adverse weather conditions such as rain, snow, and fog.Traditional 3D millimeter-wave radars can only provide range, Doppler, andazimuth information for objects. Although the recent emergence of 4Dmillimeter-wave radars has added elevation resolution, the radar point cloudsremain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,cameras offer rich semantic details but are sensitive to lighting and weatherconditions. Hence, this paper leverages these two highly complementary andcost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4Dradar spectra with depth-aware camera images and employing attentionmechanisms, we fuse texture-rich images with depth-rich radar data in theBird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,we propose using GAN-based networks to generate depth images from radar spectrain the absence of depth sensors, further improving detection accuracy.</description>
      <author>example@mail.com (Yue Sun, Yeqiang Qian, Chunxiang Wang, Ming Yang)</author>
      <guid isPermaLink="false">2502.15516v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Decoding lithium's subtle phase stability with a machine learning force field</title>
      <link>http://arxiv.org/abs/2502.15190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了锂金属在锂-金属电池阳极中的相稳定性，揭示了量子效应和非谐性对锂的热力学性质的重要性。&lt;h4&gt;背景&lt;/h4&gt;锂金属在作为锂电池阳极材料时表现出复杂的多晶型特性，这对优化其性能至关重要。然而，这种看似简单的金属具有平坦的能量地形图，需要考虑量子效应、声子重整化和热膨胀效应来准确描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图形神经网络的机器学习势场，并进行高效自洽声子计算，以研究在近环境条件下bcc-，fcc-和9R-Li相的稳定性。&lt;h4&gt;方法&lt;/h4&gt;利用了图神经网络机器学习力场以及声子重整化效应并考虑量子、热力学膨胀影响下的自洽声子计算来模拟锂的不同晶型结构。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明非谐性在决定Li的热力学性质中起着重要作用。fcc-Li在零温度和压力下被确认为基态，预测的bcc-fcc相边界与实验相变线具有定性的匹配。&lt;h4&gt;结论&lt;/h4&gt;这些研究提供了锂多晶型复杂特性的关键见解，并建立了一种有效的计算方法来模拟更现实条件下锂的大规模原子尺度仿真，以支持实际能源存储应用。&lt;h4&gt;翻译&lt;/h4&gt;理解元素锂的相稳定性对于优化其在锂-金属电池阳极中的性能至关重要。然而，这种看似简单的金属表现出复杂的多晶型特性，需要准确考虑量子效应和非谐性来捕捉平坦能量地形图中的细微差别。为了应对这一挑战，我们开发了一种基于图形神经网络的机器学习力场，并进行了高效的自洽声子计算，在近环境条件下对bcc-、fcc-和9R-Li进行了研究，将量子效应、声子重整化以及热膨胀效应结合考虑在内。我们的研究表明非谐性在决定锂的热力学性质中起着重要作用。这些相之间的自由能差值（特别是fcc-与9R-Li）仅几毫电子伏特/原子，解释了实验上难以获得纯相样品的问题，并暗示了堆垛层错和相关缺陷形成的可能性。在零温度和压力下确认了fcc-Li为基态，预测的bcc-fcc相边界虽然低估了相变温度和压力斜率，但与实验相变线具有定性的匹配。这些发现提供了锂复杂多晶型的关键见解，并建立了一种有效的计算方法来模拟更现实条件下锂的大规模原子尺度仿真，以支持实际能源存储应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1039/D4TA08860C&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the phase stability of elemental lithium (Li) is crucial foroptimizing its performance in lithium-metal battery anodes, yet this seeminglysimple metal exhibits complex polymorphism that requires proper accounting forquantum and anharmonic effects to capture the subtleties in its flat energylandscape. Here we address this challenge by developing an accurate graphneural network-based machine learning force field and performing efficientself-consistent phonon calculations for bcc-, fcc-, and 9R-Li undernear-ambient conditions, incorporating quantum, phonon renormalization andthermal expansion effects. Our results reveal the important role ofanharmonicity in determining Li's thermodynamic properties. The free energydifferences between these phases, particularly fcc- and 9R-Li are found to beonly a few meV/atom, explaining the experimental challenges in obtainingphase-pure samples and suggesting a propensity for stacking faults and relateddefect formation. fcc-Li is confirmed as the ground state at zero temperatureand pressure, and the predicted bcc-fcc phase boundary qualitatively matchesexperimental phase transition lines, despite overestimation of the transitiontemperature and pressure slope. These findings provide crucial insights intoLi's complex polymorphism and establish an effective computational approach forlarge-scale atomistic simulations of Li in more realistic settings forpractical energy storage applications.</description>
      <author>example@mail.com (Yiheng Shen, Wei Xie)</author>
      <guid isPermaLink="false">2502.15190v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个新的多模态行人场景数据集PFSD，旨在解决半结构化环境中行人感知和预测的挑战。同时提出了一种新的混合多尺度融合网络(HMFN)，以提高在复杂半结构化环境中的3D行人检测性能。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶感知系统在处理车辆主导的结构化环境时表现出色，但在存在更多动态行人的半结构化环境中表现不佳，这主要是由于高质量数据集的缺乏，特别是在涉及行人场景的数据集中。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态、全面标注的数据集PFSD，并提出了一种新的方法HMFN来应对半结构化环境中行人检测和预测的问题。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含130,000多个人行实例的PFSD数据集，该数据集在nuScenes格式下对点云进行了详细的分割、检测和对象ID跟踪。此外，提出了一个混合多尺度融合网络(HMFN)，用于处理高密度人群场景中的行人检测问题。&lt;h4&gt;主要发现&lt;/h4&gt;HMFN通过捕获并融合多种规模的特征显著提高了3D行人检测的性能，在PFSD数据集上达到了更高的平均精度(mAP)。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了高质量的数据和先进的算法在解决半结构化环境中行人感知挑战方面的必要性，并为未来的相关工作奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;最近，自主驾驶领域的感知技术取得了显著进展，尤其是在以车辆为主的结构化环境中。然而，目前的感知模型在半结构化的场景中面临重大局限性，特别是在存在动态行人的复杂和多样运动模式以及遮挡情况下表现不佳。我们归因于高质量数据集的缺乏，尤其是关于行人感知的数据集。在此研究中，我们提出了一个多模态行人焦点场景数据集PFSD，在nuScenes格式下为半结构化的场景提供了全面的多模态数据标注，包括点云分割、检测和对象ID用于追踪。该数据集涵盖了超过130,000个在各种情况下捕捉到的行人实例，包括不同的密度、移动模式及遮挡情况。为了证明解决多样且复杂的半结构化环境中的挑战的重要性，我们提出了一种新颖的混合多尺度融合网络(HMFN)。具体而言，在处理高密度人群场景中的行人检测时，我们的方法通过精心设计的混合框架有效捕捉并融合了多种规模的特征，该框架集成了稀疏和常规卷积。在PFSD上的广泛实验表明，HMFN在平均精度(mAP)上优于现有的方法，从而证明其在复杂半结构化环境中3D行人检测方面的有效性。代码和基准测试可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking machine learning for bowel sound pattern classification from tabular features to pretrained models</title>
      <link>http://arxiv.org/abs/2502.15607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了电子听诊器和可穿戴记录传感器的发展如何推动了肠鸣音信号的自动化分析，从而通过数据驱动的方法研究肠鸣音模式及其与不同病理的关系。&lt;h4&gt;背景&lt;/h4&gt;随着电子听诊器和可穿戴记录设备的进步，可以自动分析肠鸣音（BS）信号，这为基于数据的研究肠鸣音模式、它们之间的相互关系以及它们与各种疾病的相关性提供了可能性。&lt;h4&gt;目的&lt;/h4&gt;利用来自16名健康受试者的标注良好的BS数据集来评估机器学习模型在检测和/或分类BS模式方面的性能。&lt;h4&gt;方法&lt;/h4&gt;该研究使用了基于表格特征的模型，以光谱图为输入的卷积神经网络（CNN），以及预训练的大规模音频数据集上的模型，并对其进行了性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明，特别是对于样本较少的类别的检测任务中，预训练模型表现出了明显的优越性。使用HuBERT模型在区分肠鸣音信号和非肠鸣音信号上实现了AUC为0.89；使用Wav2Vec 2.0模型在不同肠鸣音模式之间区分时也达到了AUC为0.89。&lt;h4&gt;结论&lt;/h4&gt;这些结果为进一步理解肠鸣音及其潜在的机器学习辅助诊断应用铺平了道路，特别是在胃肠检查方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of electronic stethoscopes and wearable recording sensorsopened the door to the automated analysis of bowel sound (BS) signals. Thisenables a data-driven analysis of bowel sound patterns, their interrelations,and their correlation to different pathologies. This work leverages a BSdataset collected from 16 healthy subjects that was annotated according to fourestablished BS patterns. This dataset is used to evaluate the performance ofmachine learning models to detect and/or classify BS patterns. The selection ofconsidered models covers models using tabular features, convolutional neuralnetworks based on spectrograms and models pre-trained on large audio datasets.The results highlight the clear superiority of pre-trained models, particularlyin detecting classes with few samples, achieving an AUC of 0.89 indistinguishing BS from non-BS using a HuBERT model and an AUC of 0.89 indifferentiating bowel sound patterns using a Wav2Vec 2.0 model. These resultspave the way for an improved understanding of bowel sounds in general andfuture machine-learning-driven diagnostic applications for gastrointestinalexaminations</description>
      <author>example@mail.com (Zahra Mansour, Verena Uslar, Dirk Weyhe, Danilo Hollosi, Nils Strodthoff)</author>
      <guid isPermaLink="false">2502.15607v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>GiGL: Large-Scale Graph Neural Networks at Snapchat</title>
      <link>http://arxiv.org/abs/2502.15054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文介绍了GiGL，一个用于大规模分布式图机器学习的开源库。&lt;h4&gt;背景&lt;/h4&gt;随着图神经网络（GNNs）的发展，它们在商业应用中的兴趣日益增长。然而，由于规模挑战，工业界对GNNs的应用仍然落后于研究领域。&lt;h4&gt;目的&lt;/h4&gt;分享Snapchat采用GiGL进行大规模分布式图机器学习的方法和经验。&lt;h4&gt;方法&lt;/h4&gt;开发了GiGL库以解决大型社交数据上的图形ML的规模化问题，并简化内部实践者在建模方面的工作，同时支持与学术界常用的开源GNN建模库（如PyTorch Geometric）的接口。&lt;h4&gt;主要发现&lt;/h4&gt;GiGL已在多个生产环境中使用，在过去的两年中推动了超过35个跨多种业务领域的发布，包括好友推荐、内容推荐和广告领域。&lt;h4&gt;结论&lt;/h4&gt;论文详细描述了GiGL的设计、提供的工具、缩放属性以及在大规模社交图上的应用案例研究，并总结了一些关键经验教训。&lt;h4&gt;翻译&lt;/h4&gt;近期的图机器学习（ML）进展引入了图形神经网络（GNNs），这引发了将这些方法应用于商业规模应用的兴趣。GNNs使根据给定的图结构进行端到端(E2E)模型参数微分学习成为可能，从而优化流行节点、边（链接）和图级任务的目标函数。虽然在新GNN层和训练策略方面取得了迅速的研究创新，但由于大规模图形ML问题所特有的规模挑战，工业界对GNNs的应用明显滞后。在这项工作中，我们分享了Snapchat在培训、推理以及利用GNN时的方法。为此，我们介绍了GiGL（Gigantic Graph Learning），这是一个开源库，旨在使大型分布式图机器学习能够服务于研究人员、ML工程师和实践者的需求。我们在内部使用GiGL来处理GNN工作流程的繁重任务，包括从关系数据库中进行图数据预处理、子图采样、分布式训练、推理以及编排。GiGL的设计目的是清晰地与学术界常用的开源GNN建模库（如PyTorch Geometric）接口，并解决规模和生产化挑战，以使内部实践者能够专注于模型构建。GiGL在多个生产环境中使用，在过去的两年中推动了超过35个跨多种业务领域的发布，包括好友推荐、内容推荐以及广告领域。本工作详细描述了该库的高级设计和工具提供情况，扩展特性，并在各种行业规模图中的实际应用案例研究，以及在大规模社交数据上采用图形ML的关键经验教训。GiGL在https://github.com/snap-research/GiGL开源可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in graph machine learning (ML) with the introduction of GraphNeural Networks (GNNs) have led to a widespread interest in applying theseapproaches to business applications at scale. GNNs enable differentiableend-to-end (E2E) learning of model parameters given graph structure whichenables optimization towards popular node, edge (link) and graph-level tasks.While the research innovation in new GNN layers and training strategies hasbeen rapid, industrial adoption and utility of GNNs has lagged considerably dueto the unique scale challenges that large-scale graph ML problems create. Inthis work, we share our approach to training, inference, and utilization ofGNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), anopen-source library to enable large-scale distributed graph ML to the benefitof researchers, ML engineers, and practitioners. We use GiGL internally atSnapchat to manage the heavy lifting of GNN workflows, including graph datapreprocessing from relational DBs, subgraph sampling, distributed training,inference, and orchestration. GiGL is designed to interface cleanly withopen-source GNN modeling libraries prominent in academia like PyTorch Geometric(PyG), while handling scaling and productionization challenges that make iteasier for internal practitioners to focus on modeling. GiGL is used inmultiple production settings, and has powered over 35 launches across multiplebusiness domains in the last 2 years in the contexts of friend recommendation,content recommendation and advertising. This work details high-level design andtools the library provides, scaling properties, case studies in diversebusiness settings with industry-scale graphs, and several key lessons learnedin employing graph ML at scale on large social data. GiGL is open-sourced athttps://github.com/snap-research/GiGL.</description>
      <author>example@mail.com (Tong Zhao, Yozen Liu, Matthew Kolodner, Kyle Montemayor, Elham Ghazizadeh, Ankit Batra, Zihao Fan, Xiaobin Gao, Xuan Guo, Jiwen Ren, Serim Park, Peicheng Yu, Jun Yu, Shubham Vij, Neil Shah)</author>
      <guid isPermaLink="false">2502.15054v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Realm: Real-Time Line-of-Sight Maintenance in Multi-Robot Navigation with Unknown Obstacles</title>
      <link>http://arxiv.org/abs/2502.15162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, accepted by IEEE ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多机器人导航框架，该框架在未知且复杂环境中通过实时点云测量来维护线视（LoS）连接约束。提出了一个基于点云可见性分析的新LoS距离度量方法，并设计了融合函数以确保机器人之间的协作移动和保持LoS连接。&lt;h4&gt;背景&lt;/h4&gt;多机器人系统在复杂环境中的导航需要依赖于机器人之间的通信与相互观测，而以前的研究工作大多是在已知的环境中进行的，难以应用于未知且复杂的场景中。&lt;h4&gt;目的&lt;/h4&gt;研究解决未知复杂环境中多机器人导航问题的方法，并提出一种新的LoS距离度量方法和融合函数来保持机器人的连接性。&lt;h4&gt;方法&lt;/h4&gt;通过实时点云测量直接定义了机器人之间的线视（LoS）约束，利用点云可见性分析技术量化了由于潜在的机器人移动而可能失去LoS的重要性与敏感程度。设计了一个新的融合函数以确保两个机器人之间丢失LoS的需求平衡，并将LoS约束编码到势能函数中。&lt;h4&gt;主要发现&lt;/h4&gt;提出了新颖的基于点云的LoS距离度量方法，能够同时考虑保持连接性和紧急性；设计了一种新的融合功能来处理两台机器人的紧迫感不均衡的问题；实现了结合上述方法的多机器人探索框架，并通过分布式传感和通信确保了未知环境下的持续导航。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的框架在复杂且未知的环境中，通过点云测量实现LoS约束的有效维护，增强了机器人的合作能力和实时感知能力。此成果对于未来自主系统的开发具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;多机器人系统在复杂环境中的导航需要依靠机器人之间的通信和相互观察来协调并提高态势感知能力。本文研究了未知环境中基于视距（LoS）连接限制的多机器人导航问题，而以前的工作仅限于从已知的环境模型中推导出LoS约束条件，本论文通过实时点云测量直接建立了这些约束，并利用点云可见性分析技术来实现这一点。我们提出了一种新的LoS距离度量方法，该方法可以量化由于潜在的机器人移动而可能失去视距的重要性和敏感性。此外，为了应对两个机器人之间丢失视距时紧迫感不均衡的问题，设计了一个融合功能以捕捉整体紧迫感并生成有利于保持视距的协作运动梯度。&lt;h4&gt;开源链接&lt;/h4&gt;https://github.com/bairuofei/LoS_constrained_navigation&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-robot navigation in complex environments relies on inter-robotcommunication and mutual observations for coordination and situationalawareness. This paper studies the multi-robot navigation problem in unknownenvironments with line-of-sight (LoS) connectivity constraints. While previousworks are limited to known environment models to derive the LoS constraints,this paper eliminates such requirements by directly formulating the LoSconstraints between robots from their real-time point cloud measurements,leveraging point cloud visibility analysis techniques. We propose a novelLoS-distance metric to quantify both the urgency and sensitivity of losing LoSbetween robots considering potential robot movements. Moreover, to address theimbalanced urgency of losing LoS between two robots, we design a fusionfunction to capture the overall urgency while generating gradients thatfacilitate robots' collaborative movement to maintain LoS. The LoS constraintsare encoded into a potential function that preserves the positivity of theFiedler eigenvalue of the robots' network graph to ensure connectivity.Finally, we establish a LoS-constrained exploration framework that integratesthe proposed connectivity controller. We showcase its applications inmulti-robot exploration in complex unknown environments, where robots canalways maintain the LoS connectivity through distributed sensing andcommunication, while collaboratively mapping the unknown environment. Theimplementations are open-sourced athttps://github.com/bairuofei/LoS_constrained_navigation.</description>
      <author>example@mail.com (Ruofei Bai, Shenghai Yuan, Kun Li, Hongliang Guo, Wei-Yun Yau, Lihua Xie)</author>
      <guid isPermaLink="false">2502.15162v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and efficient machine learning interatomic potentials for finite temperature modeling of molecular crystals</title>
      <link>http://arxiv.org/abs/2502.15530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了机器学习（ML）势能模型在分子晶体计算中的应用，特别是用于准确高效地计算升华焓。通过利用化学和材料科学领域的基础模型以及量子扩散蒙特卡洛基准测试的最新进展，该研究展示了使用少量高质量数据结构生成高精度MLIP的能力。&lt;h4&gt;背景&lt;/h4&gt;机器学习势能（MLIP）在模拟分子晶体方面具有革命性意义，但准确高效地计算升华焓仍面临挑战。现有方法需要大量的高精度参考结构，并且依赖于可能不够可靠的密度泛函理论来产生这些数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够使用较少的数据生成准确的机器学习势能模型的方法，以改进分子晶体在有限温度和压力下的描述能力。&lt;h4&gt;方法&lt;/h4&gt;利用化学和材料科学领域的基础模型以及量子扩散蒙特卡洛（QDMC）基准测试，通过训练数据集中的约200个高质量结构来构建MLIP模型。该框架还考虑了非谐性和核量子效应，并应用于X23数据集。&lt;h4&gt;主要发现&lt;/h4&gt;生成的机器学习势能模型在计算升华焓时达到了次化学精度水平，甚至可以推广到具有药物相关性的晶体系统中，准确捕捉核量子效应（如对天青酸的研究）。&lt;h4&gt;结论&lt;/h4&gt;这项工作为研究药理学和生物学系统的环境条件下的精确模拟铺平了道路。通过减少所需的数据量，它不仅提高了计算效率，还提供了关于药物分子稳定性的重要见解。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在模拟分子晶体方面的潜力巨大，但准确计算升华焓仍存在挑战。本文提出了一种新方法，使用更少的高质量数据结构生成精确的MLIP模型，并通过X23数据集验证了该框架的有效性及对药物相关系统的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As with many parts of the natural sciences, machine learning interatomicpotentials (MLIPs) are revolutionizing the modeling of molecular crystals.However, challenges remain for the accurate and efficient calculation ofsublimation enthalpies - a key thermodynamic quantity measuring the stabilityof a molecular crystal. Specifically, two key stumbling blocks are: (i) theneed for thousands of ab initio quality reference structures to generatetraining data; and (ii) the sometimes unreliable nature of density functionaltheory, the main technique for generating such data. Exploiting recentdevelopments in foundational models for chemistry and materials sciencealongside accurate quantum diffusion Monte Carlo benchmarks, offers a promisingpath forward. Herein, we demonstrate the generation of MLIPs capable ofdescribing molecular crystals at finite temperature and pressure withsub-chemical accuracy, using as few as $\sim 200$ data structures; an order ofmagnitude improvement over the current state-of-the-art. We apply thisframework to compute the sublimation enthalpies of the X23 dataset, accountingfor anharmonicity and nuclear quantum effects, achieving sub-chemical accuracywith respect to experiment. Importantly, we show that our framework can begeneralized to crystals of pharmaceutical relevance, including paracetamol andaspirin. Nuclear quantum effects are also accurately captured as shown for thecase of squaric acid. By enabling accurate modeling at ambient conditions, thiswork paves the way for deeper insights into pharmaceutical and biologicalsystems.</description>
      <author>example@mail.com (Flaviano Della Pia, Benjamin X. Shi, Venkat Kapil, Andrea Zen, Dario Alfè, Angelos Michaelides)</author>
      <guid isPermaLink="false">2502.15530v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Android Malware Detection: Rethinking the Role of Traditional and Deep Learning Models</title>
      <link>http://arxiv.org/abs/2502.15041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文系统地评估了Android恶意软件检测模型，比较了传统的机器学习和深度学习方法在不同数据集上的表现。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用传统机器学习（ML）和深度学习（DL）技术对Android恶意软件进行检测的研究得到了广泛的关注。然而，尽管基于DL的方法声称具有优越的性能，它们往往依赖于有限的对比测试，并且缺乏与传统ML模型在多样化数据集上的全面基准比较。&lt;h4&gt;目的&lt;/h4&gt;通过使用四个不同数据集来评估不同的Android恶意软件检测模型（包括传统的随机森林和CatBoost以及先进的Capsule Graph Neural Networks等），论文旨在探讨各种模型的表现，并为未来的研究提供更加全面的基准测试。&lt;h4&gt;方法&lt;/h4&gt;该研究实施了一系列传统机器学习模型，如Random Forests (RF) 和 CatBoost，同时对比了先进的深度学习模型，例如CapsGNN、BERT和ExcelFormer等。使用的数据集包括三个最近发布的公开可用的数据集以及一个大规模的数据集（作者系统地收集的）。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，尽管高级DL模型可以实现强大的性能，但它们通常只与少量的传统ML基线进行比较。在许多情况下，更简单且计算效率更高的传统ML模型实现了可比甚至更好的性能。&lt;h4&gt;结论&lt;/h4&gt;论文强调了Android恶意软件检测研究中需要更加严格基准测试的重要性，并建议未来的研究应进行全面的对比研究以确保对检测能力的准确评估。此外，为促进进一步的研究，作者提供了其数据集的访问权限。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Android malware detection has been extensively studied using both traditionalmachine learning (ML) and deep learning (DL) approaches. While manystate-of-the-art detection models, particularly those based on DL, claimsuperior performance, they often rely on limited comparisons, lackingcomprehensive benchmarking against traditional ML models across diversedatasets. This raises concerns about the robustness of DL-based approaches'performance and the potential oversight of simpler, more efficient ML models.In this paper, we conduct a systematic evaluation of Android malware detectionmodels across four datasets: three recently published, publicly availabledatasets and a large-scale dataset we systematically collected. We implement arange of traditional ML models, including Random Forests (RF) and CatBoost,alongside advanced DL models such as Capsule Graph Neural Networks (CapsGNN),BERT-based models, and ExcelFormer based models. Our results reveal that whileadvanced DL models can achieve strong performance, they are often comparedagainst an insufficient number of traditional ML baselines. In many cases,simpler and more computationally efficient ML models achieve comparable or evensuperior performance. These findings highlight the need for rigorousbenchmarking in Android malware detection research. We encourage future studiesto conduct more comprehensive benchmarking comparisons between traditional andadvanced models to ensure a more accurate assessment of detection capabilities.To facilitate further research, we provide access to our dataset, including appIDs, hash values, and labels.</description>
      <author>example@mail.com (Guojun Liu, Doina Caragea, Xinming Ou, Sankardas Roy)</author>
      <guid isPermaLink="false">2502.15041v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation</title>
      <link>http://arxiv.org/abs/2502.15466v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个针对时间序列分析（TSA）的数据生成机制和预训练模型，旨在克服数据稀缺性和不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;在时间序列分析领域，基础模型正受到越来越多的关注。然而，由于数据稀缺和数据不平衡等问题的存在，其发展受到了阻碍。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，研究者们提出了一种新的方法来建模复杂系统，并引入了一系列-符号（S2）双模式数据生成机制。&lt;h4&gt;方法&lt;/h4&gt;通过这种机制，可以无限制地创建高质量的时间序列数据并配以相应的符号表示。基于这些数据，他们开发了SymTime预训练基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;SymTime在五个主要时间序列分析任务中表现出竞争力，并且其性能与直接用真实世界数据集进行预训练的基础模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了双模态数据生成和预训练机制在克服数据稀缺性、提升任务性能方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;基础模型用于时间序列分析（TSA）吸引了大量的关注。然而，诸如数据稀缺和不平衡等问题依然阻碍着其发展。为解决这些问题，我们考虑通过符号表达式来建模复杂系统，这些符号表达式可以作为时间序列的语义描述符。在此基础上，我们引入了一种系列-符号（S2）双模式性数据生成机制，能够无限制地创建高质量的时间序列数据及其相应的符号表示。利用S2数据集，我们开发了SymTime，这是一个为TSA设计的预训练基础模型。当在下游任务中进行微调时，SymTime在五个主要TSA任务中的表现是竞争性的，并且其性能可以与直接基于真实世界数据集预训练的基础模型相媲美。这种方法强调了双模态数据生成和预训练机制在克服数据稀缺性和提升任务性能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for time series analysis (TSA) have attracted significantattention. However, challenges such as data scarcity and data imbalancecontinue to hinder their development. To address this, we consider modelingcomplex systems through symbolic expressions that serve as semantic descriptorsof time series. Building on this concept, we introduce a series-symbol (S2)dual-modulity data generation mechanism, enabling the unrestricted creation ofhigh-quality time series data paired with corresponding symbolicrepresentations. Leveraging the S2 dataset, we develop SymTime, a pre-trainedfoundation model for TSA. SymTime demonstrates competitive performance acrossfive major TSA tasks when fine-tuned with downstream task, rivaling foundationmodels pre-trained on real-world datasets. This approach underscores thepotential of dual-modality data generation and pretraining mechanisms inovercoming data scarcity and enhancing task performance.</description>
      <author>example@mail.com (Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang, Jing Liu)</author>
      <guid isPermaLink="false">2502.15466v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment</title>
      <link>http://arxiv.org/abs/2502.15012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work is accepted by DAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为GNNVault的安全策略，用于在边缘设备上部署基于TEE的图神经网络模型。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型在边缘设备上的广泛应用使得其知识产权和数据隐私面临威胁。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于TEE的解决方案来保护图神经网络(GNN)模型及其使用的私有图形数据的安全性。&lt;h4&gt;方法&lt;/h4&gt;GNNVault采用'训练前划分'的设计理念，并结合了私人GNN校正器与公共骨干模型。在推理过程中，重要的GNN参数和使用到的私有图表均被安全地隔离在TEE中。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界的应用（例如Intel SGX环境）证明，GNNVault能够有效防御最先进的链接盗窃攻击，并且对精度的影响微乎其微（小于2%）。&lt;h4&gt;结论&lt;/h4&gt;提出的GNNVault方案为保护部署于边缘设备上的图神经网络模型提供了一种新颖而有效的安全机制。&lt;h4&gt;翻译&lt;/h4&gt;广泛地在边缘设备上部署机器学习模型已经使得这些模型的知识产权和数据隐私变得脆弱。我们提出了基于可信执行环境(TEE)的第一个安全图形神经网络(GNN)部署策略，称为GNNVault。该策略遵循'训练前划分'的设计理念，并且包括了一个私人GNN校正器来补充公共骨干模型。通过这种方式，在推理过程中使用的关键性GNN模型参数和私有图都被保护在安全的TEE隔间中。基于Intel SGX的真实世界实现表明，GNNVault可以有效地抵御最先进的链接盗窃攻击，同时不会导致精度显著下降（小于2%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wide deployment of machine learning models on edge devices has rendered themodel intellectual property (IP) and data privacy vulnerable. We proposeGNNVault, the first secure Graph Neural Network (GNN) deployment strategy basedon Trusted Execution Environment (TEE). GNNVault follows the design of'partition-before-training' and includes a private GNN rectifier to complementwith a public backbone model. This way, both critical GNN model parameters andthe private graph used during inference are protected within secure TEEcompartments. Real-world implementations with Intel SGX demonstrate thatGNNVault safeguards GNN inference against state-of-the-art link stealingattacks with negligible accuracy degradation (&lt;2%).</description>
      <author>example@mail.com (Ruyi Ding, Tianhong Xu, Aidong Adam Ding, Yunsi Fei)</author>
      <guid isPermaLink="false">2502.15012v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving Scenarios</title>
      <link>http://arxiv.org/abs/2502.15076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint, to appear in ROBOVIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文旨在通过改进仿真数据集生成流程，提高从虚拟环境到真实世界场景的自主驾驶系统物体检测模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在推进自动驾驶系统的进程中，模拟技术是一个重要因素。然而，目前尚无显著进展解决虚拟与现实之间转换的问题，特别是在基于LiDAR点云进行3D目标检测方面。&lt;h4&gt;目的&lt;/h4&gt;该研究重新审视了从仿真数据到真实世界应用的转化问题，并提出了一种新的数据集生成管道以增强模型在真实环境中的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;采用CARLA模拟器，结合领域随机化策略和细致建模技术来训练基于合成数据的目标检测模型。同时对比不同虚拟传感器变体，探究哪些传感器属性是导致域间隙的主要因素。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用少量真实世界的数据进行微调，该模型几乎可以达到基准性能；而当使用完整的真实训练集时，则能够超过基准表现。&lt;h4&gt;结论&lt;/h4&gt;利用精心设计的模拟数据生成流程和领域随机化策略可以使仿真训练的目标检测器在现实世界的任务中表现出色。进一步的研究应该集中在如何最小化传感器属性差异，以进一步提高模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：推动自动驾驶系统发展的关键因素之一是模拟技术的应用。然而，在虚拟环境向真实世界场景迁移的问题上进展甚微。我们重新审视了针对基于LiDAR点云进行3D物体检测任务中的领域转移问题，提出了一种基于CARLA仿真器的生成数据集管道。通过采用领域随机化策略和精心建模方法，我们在合成数据上训练了一个目标探测器，并展示了其在KITTI数据集上的强大泛化能力。此外，我们对比了不同虚拟传感器变体以获得洞见，哪些传感器特性可能造成显著的域间隙现象。最后，在少量真实世界数据的基础上进行微调几乎可以达到基准性能水平；而使用完整的真实训练集时则超过了该基准线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; An important factor in advancing autonomous driving systems is simulation.Yet, there is rather small progress for transferability between the virtual andreal world. We revisit this problem for 3D object detection on LiDAR pointclouds and propose a dataset generation pipeline based on the CARLA simulator.Utilizing domain randomization strategies and careful modeling, we are able totrain an object detector on the synthetic data and demonstrate stronggeneralization capabilities to the KITTI dataset. Furthermore, we comparedifferent virtual sensor variants to gather insights, which sensor attributescan be responsible for the prevalent domain gap. Finally, fine-tuning with asmall portion of real data almost matches the baseline and with the fulltraining set slightly surpasses it.</description>
      <author>example@mail.com (Richard Marcus, Christian Vogel, Inga Jatzkowski, Niklas Knoop, Marc Stamminger)</author>
      <guid isPermaLink="false">2502.15076v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning</title>
      <link>http://arxiv.org/abs/2502.15436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Raghav Singhal and Kaustubh Ponkshe contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Fed-SB，一种使用LoRA-SB方法进行联邦微调的新颖方式。该方法通过学习两个适配器之间的小型正方形矩阵来优化低秩适应过程，并直接平均该矩阵以减少通信成本和保证精确更新。&lt;h4&gt;背景&lt;/h4&gt;低秩适应（LoRA）已成为有效调整基础模型的普遍技术，但使用LoRA进行联邦微调存在挑战，因为传统的个体适配器平均方法会导致次优更新。现有解决方案要么导致高昂的通信成本，要么由于表达能力受限而性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的联邦微调方法，既能减少通信开销又能保证高性能。&lt;h4&gt;方法&lt;/h4&gt;提出Fed-SB，该方法基于最近提出的LoRA-SB低秩适应技术。在两个适配器之间学习一个小型正方形矩阵R，并直接平均这个矩阵以确保精确更新和降低通信成本。&lt;h4&gt;主要发现&lt;/h4&gt;Fed-SB在常识推理、算术推理和语言推断任务上实现了最先进的性能，同时将通信开销最多减少了230倍。此外，在私人设置下，通过减少可训练参数数量来提高隐私保护，并避免其他方法引入的噪声放大问题。&lt;h4&gt;结论&lt;/h4&gt;总体而言，Fed-SB在通信成本与性能之间的权衡中开辟了一个新的帕累托前沿，为私有和非私有的联邦微调提供了高效且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Lo-Rank 适应（LoRA）已成为有效调整基础模型的标准技术。然而，使用 LoRA 进行联邦细调具有挑战性，因为传统的个体适配器平均会导致次优更新。现有的解决方法要么导致高昂的通信成本，要么由于表达能力受限而降低性能表现。我们介绍了一种新的方法 Fed-SB，它利用了最近提出的低秩适应技术 LoRA-SB 来进行大规模语言模型（LLMs）的联邦细调。LoRA-SB 通过在适配器之间学习一个小方阵矩阵 R，并保持其他组件不变来优化适应过程。直接平均这个小方阵保证了精确更新，大大降低了通信成本，使其不受客户端数量的影响，从而实现了可扩展性。Fed-SB 在常识推理、算术推理和语言推断任务上达到了最先进的性能，同时将通信成本最多减少了230倍。在私人设置中，Fed-SB 通过减少训练参数的数量来提高差分隐私的噪声需求，并避免了其他方法引入的噪声放大问题，进一步提高了表现。总体而言，Fed-SB 在通信和性能之间的权衡上开辟了一个新的帕累托前沿，为私有和非私有的联邦细调提供了高效且可扩展的解决方案。我们的代码公开可用：https://github.com/CERT-Lab/fed-sb。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/CERT-Lab/fed-sb&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuningfoundation models. However, federated fine-tuning using LoRA is challenging dueto suboptimal updates arising from traditional federated averaging ofindividual adapters. Existing solutions either incur prohibitively highcommunication cost that scales linearly with the number of clients or sufferfrom performance degradation due to limited expressivity. We introduceFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning ofLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SBoptimally aligns the optimization trajectory with the ideal low-rank fullfine-tuning projection by learning a small square matrix (R) between adapters Band A, keeping other components fixed. Direct averaging of R guarantees exactupdates, substantially reducing communication cost, which remains independentof the number of clients, and enables scalability. Fed-SB achievesstate-of-the-art performance across commonsense reasoning, arithmeticreasoning, and language inference tasks while reducing communication costs byup to 230x. In private settings, Fed-SB further improves performance by (1)reducing trainable parameters, thereby lowering the noise required fordifferential privacy and (2) avoiding noise amplification introduced by othermethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoffbetween communication and performance, offering an efficient and scalablesolution for both private and non-private federated fine-tuning. Our code ispublicly available at https://github.com/CERT-Lab/fed-sb.</description>
      <author>example@mail.com (Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Lav R. Varshney, Praneeth Vepakomma)</author>
      <guid isPermaLink="false">2502.15436v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Design Principles of Link Prediction in Directed Settings</title>
      <link>http://arxiv.org/abs/2502.15008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图表示学习中有向链接预测的挑战，提出了适用于该任务的有效启发式方法，并展示了这些改进能够与为无向图设计的最佳图神经网络相媲美。&lt;h4&gt;背景&lt;/h4&gt;传统的图表示学习理论基于对称邻接矩阵假设，即认为数据是无方向性的。然而，现实世界中的关系经常包含通过方向传达的重要信息，这限制了现有模型捕捉复杂有向交互的能力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决有向链接预测的问题，并提出适用于该任务的有效方法。&lt;h4&gt;方法&lt;/h4&gt;在论文中评估了一些成功应用于无向图的关键启发式算法，然后对其进行了简单但有效的改进以适应有向链接预测的任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列广泛的实验研究，作者开发了一个新颖的框架来解决有向链接预测问题。该框架不仅超过了基线方法，在多个基准上的性能也优于为无向图设计的最佳图神经网络。&lt;h4&gt;结论&lt;/h4&gt;这项工作表明，对现有启发式算法进行简单的调整可以在有向图任务中实现与复杂神经网络模型相竞争的性能，并且这些改进可以提供对图表示学习框架发展的宝贵见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701716.3717803&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a widely studied task in Graph Representation Learning(GRL) for modeling relational data. The early theories in GRL were based on theassumption of a symmetric adjacency matrix, reflecting an undirected setting.As a result, much of the following state-of-the-art research has continued tooperate under this symmetry assumption, even though real-world data ofteninvolve crucial information conveyed through the direction of relationships.This oversight limits the ability of these models to fully capture thecomplexity of directed interactions. In this paper, we focus on the challengeof directed link prediction by evaluating key heuristics that have beensuccessful in undirected settings. We propose simple but effective adaptationsof these heuristics to the directed link prediction task and demonstrate thatthese modifications produce competitive performance compared to the leadingGraph Neural Networks (GNNs) originally designed for undirected graphs. Throughan extensive set of experiments, we derive insights that inform the developmentof a novel framework for directed link prediction, which not only surpassesbaseline methods but also outperforms state-of-the-art GNNs on multiplebenchmarks.</description>
      <author>example@mail.com (Jun Zhai, Muberra Ozmen, Thomas Markovich)</author>
      <guid isPermaLink="false">2502.15008v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Chitrarth: Bridging Vision and Language for a Billion People</title>
      <link>http://arxiv.org/abs/2502.15392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Chitrarth，这是一个针对印度10种语言的包容性视觉-语言模型。该模型结合了最先进的多语言大型语言模型和视觉模块，并主要在多语言图像文本数据上进行训练。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态基础模型主要是基于英语或高资源欧洲语言的数据集进行训练，这限制了它们在中低资源语言中的应用。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一局限性，研究者们引入了一个名为Chitrarth的视觉-语言模型，旨在支持印度丰富的语言多样性及跨10种主要印度语言的视觉推理。&lt;h4&gt;方法&lt;/h4&gt;研究团队使用最先进的多语言大型语言模型与视觉模块相结合的方法，并且这种结合主要是通过在包含多种语言图像文本数据集上进行训练来实现的。&lt;h4&gt;主要发现&lt;/h4&gt;该模型不仅在低资源语言基准测试中取得了最佳结果，同时在英语中的效率也得以保持。此外，他们还提出了BharatBench框架用于评估跨不同印度语言的视觉-语言模型的表现。&lt;h4&gt;结论&lt;/h4&gt;通过这项研究，研究者们旨在为多语种和多模态能力设置新的基准，并为未来的发展提供基础。&lt;h4&gt;翻译&lt;/h4&gt;最近的多种模态基础模型主要是基于英语或高资源欧洲语言的数据进行训练，这限制了它们在中低资源语言中的应用。为了应对这一局限性，我们介绍了Chitrarth（图像：图片；意义：含义），这是一个面向10种主要印度语言的语言多样性及视觉推理问题的目标包容性视觉-语言模型(VLM)。我们的模型有效地集成了最先进的多语言大型语言模型和一个视觉模块，主要是基于多种语言的图文数据进行训练。此外，我们还引入了BharatBench，这个框架用于在不同印度语中评估VLM的表现，最终促进了更加多样化的AI系统的发展。我们的模型在低资源语言基准测试中取得了最佳结果，并保持了其在英语中的效率。通过我们的研究，我们旨在为多语种和多模态能力设立新的基准，并对现有模型进行实质性的改进，从而为基础未来在这个领域的进步奠定基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent multimodal foundation models are primarily trained on English or highresource European language data, which hinders their applicability to othermedium and low-resource languages. To address this limitation, we introduceChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model(VLM), specifically targeting the rich linguistic diversity and visualreasoning across 10 prominent Indian languages. Our model effectivelyintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)with a vision module, primarily trained on multilingual image-text data.Furthermore, we also introduce BharatBench, a comprehensive framework forevaluating VLMs across various Indian languages, ultimately contributing tomore diverse and effective AI systems. Our model achieves SOTA results forbenchmarks across low resource languages while retaining its efficiency inEnglish. Through our research, we aim to set new benchmarks inmultilingual-multimodal capabilities, offering substantial improvements overexisting models and establishing a foundation to facilitate future advancementsin this arena.</description>
      <author>example@mail.com (Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal)</author>
      <guid isPermaLink="false">2502.15392v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CrossOver: 3D Scene Cross-Modal Alignment</title>
      <link>http://arxiv.org/abs/2502.15011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: sayands.github.io/crossover/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrossOver是一种用于多模态3D场景理解的新框架，通过灵活的场景级别模式对齐来解决传统方法需要所有对象实例都具有严格对齐的模式数据的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的方法通常假设完全的数据可用性以及各模式之间的刚性对齐，这在实际应用中可能难以实现。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的框架CrossOver，该框架旨在通过灵活、场景级别的模式对齐进行跨模态3D场景理解，并且无需显式的对象语义即可学习统一的多模式无关嵌入空间。&lt;h4&gt;方法&lt;/h4&gt;利用特定于维度的编码器和多阶段训练管道，CrossOver支持具有缺失模式的数据场景检索和物体定位任务。该框架包括RGB图像、点云、CAD模型、平面图以及文本描述等多种类型的数据输入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果在ScanNet和3RScan数据集上表现出色，显示了其在各种指标上的优越性能，并强调了适应现实世界应用的能力。&lt;h4&gt;结论&lt;/h4&gt;CrossOver框架展示了它在多模态3D场景理解中的强大能力，尤其是在处理不完整或缺失的模式时的表现。这为未来的研究提供了坚实的基础和新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;跨模式三维物体理解已经获得了相当的关注，然而当前的方法往往假设完全的数据可用性以及各模式之间的刚性对齐。我们提出了一种新的框架CrossOver，该框架通过灵活、场景级别的模式对齐进行跨模态3D场景理解，并且无需显式的对象语义即可学习统一的多模式无关嵌入空间。实验结果在ScanNet和3RScan数据集上表现出色，显示了其适应现实世界应用的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal 3D object understanding has gained significant attention, yetcurrent approaches often assume complete data availability and rigid alignmentacross all modalities. We present CrossOver, a novel framework for cross-modal3D scene understanding via flexible, scene-level modality alignment. Unliketraditional methods that require aligned modality data for every objectinstance, CrossOver learns a unified, modality-agnostic embedding space forscenes by aligning modalities - RGB images, point clouds, CAD models,floorplans, and text descriptions - with relaxed constraints and withoutexplicit object semantics. Leveraging dimensionality-specific encoders, amulti-stage training pipeline, and emergent cross-modal behaviors, CrossOversupports robust scene retrieval and object localization, even with missingmodalities. Evaluations on ScanNet and 3RScan datasets show its superiorperformance across diverse metrics, highlighting adaptability for real-worldapplications in 3D scene understanding.</description>
      <author>example@mail.com (Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni)</author>
      <guid isPermaLink="false">2502.15011v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation</title>
      <link>http://arxiv.org/abs/2502.15309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种动态、高保真和开放词汇的场景图生成系统（DynamicGSG），用于机器人在不断变化的环境中有效理解和适应。&lt;h4&gt;背景&lt;/h4&gt;现实世界中，环境由于代理或人类活动的变化使得机器人执行长期任务变得非常具有挑战性。感知系统需要提取实例级别的语义信息并根据环境变化更新内存中的表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够构建层次化场景图、优化高保真重建以及动态适应长时环境变化的系统。&lt;h4&gt;方法&lt;/h4&gt;{'构造层次化场景图': '使用先进的视觉基础模型来表示环境中对象的空间和语义关系', '设计联合特征损失': '为了增量式地提高高质量重建，优化高斯地图', '更新高斯地图与场景图': '根据实际环境变化进行动态更新'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能实验': '在语义分割、语言引导的对象检索和重建质量方面展示了所提方法的效能。', '真实实验室验证': '验证了系统动态更新能力的有效性'}&lt;h4&gt;结论&lt;/h4&gt;DynamicGSG能够有效提高机器人在复杂多变环境中的适应性和执行长期任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一个叫做DynamicGSG的新提出的用于机器人感知系统的解决方案，它能帮助机器人在不断变化的环境中更高效地理解和工作。该系统主要由三个部分构成：一是使用先进的视觉基础模型构建描述物体空间和语义关系的层次化场景图；二是通过设计联合特征损失来优化高斯地图，以获得增量式的高质量重建效果；三是根据真实环境的变化更新高斯地图和场景图，实现长时间内的环境适应。实验结果表明该方法在关键任务如语义分割、语言引导对象检索以及重建质量方面表现出色，并且其动态更新能力已经在实验室环境中得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world scenarios, the environment changes caused by agents or humanactivities make it extremely challenging for robots to perform variouslong-term tasks. To effectively understand and adapt to dynamic environments,the perception system of a robot needs to extract instance-level semanticinformation, reconstruct the environment in a fine-grained manner, and updateits environment representation in memory according to environment changes. Toaddress these challenges, We propose \textbf{DynamicGSG}, a dynamic,high-fidelity, open-vocabulary scene graph generation system leveragingGaussian splatting. Our system comprises three key components: (1) constructinghierarchical scene graphs using advanced vision foundation models to representthe spatial and semantic relationships of objects in the environment, (2)designing a joint feature loss to optimize the Gaussian map for incrementalhigh-fidelity reconstruction, and (3) updating the Gaussian map and scene graphaccording to real environment changes for long-term environment adaptation.Experiments and ablation studies demonstrate the performance and efficacy ofthe proposed method in terms of semantic segmentation, language-guided objectretrieval, and reconstruction quality. Furthermore, we have validated thedynamic updating capabilities of our system in real laboratory environments.The source code will be releasedat:~\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/DynamicGSG}.</description>
      <author>example@mail.com (Luzhou Ge, Xiangyu Zhu, Zhuo Yang, Xuesong Li)</author>
      <guid isPermaLink="false">2502.15309v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Lung-DDPM: Semantic Layout-guided Diffusion Models for Thoracic CT Image Synthesis</title>
      <link>http://arxiv.org/abs/2502.15204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code and pretrained models are available at  https://github.com/Manem-Lab/Lung-DDPM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种基于AI的胸部CT影像合成方法Lung-DDPM，旨在解决肺癌早期筛查中数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能技术的发展，AI辅助医学影像分析在肺癌早期筛查方面表现出色。然而，高昂的数据标注成本和隐私问题限制了大规模医疗数据集的构建，阻碍了AI在医疗领域的进一步应用。&lt;h4&gt;目的&lt;/h4&gt;为了应对肺癌筛查中的数据稀缺问题，提出了一种胸部CT图像合成方法Lung-DDPM，以生成高质量的3D合成CT影像，并应用于下游肺结节分割任务中。&lt;h4&gt;方法&lt;/h4&gt;该方法基于语义布局引导去噪扩散概率模型（DDPM），能够从不完整的语义布局中生成解剖学合理的、无缝且一致的样本图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Lung-DDPM在图像质量评估和下游肺结节分割任务方面优于其他最先进的生成模型。具体而言，在验证队列中FID为0.0047、MMD为0.0070、MSE为0.0024，分别比第二好的竞争对手高7.4倍、3.1倍和29.5倍。此外，结合真实数据与Lung-DDPM生成的数据训练的肺结节分割模型在Dice系数和敏感性方面分别优于单独使用真实数据模型8.8%和18.6%。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明Lung-DDPM具有更广泛的应用潜力，例如肿瘤分割、癌症生存估计以及风险预测等方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着人工智能（AI）的快速发展，基于AI的医学影像分析在肺癌早期筛查中展现出显著的效果。然而，高昂的数据标注成本和隐私问题限制了大规模医疗数据集的构建，阻碍了AI在医疗领域的进一步应用。为了应对肺癌筛查中的数据稀缺问题，本文提出了一种胸部CT图像合成方法Lung-DDPM，该方法能够有效生成高保真的3D合成CT影像，在下游肺结节分割任务中证明很有帮助。本研究基于语义布局引导去噪扩散概率模型（DDPM），即使从不完整的语义布局也能生成解剖学合理的、无缝且一致的样本图像。实验结果显示，该方法在图像质量评估和下游肺结节分割任务方面优于其他最先进的生成模型。具体而言，在验证队列中分别取得了FID为0.0047、MMD为0.0070以及MSE为0.0024的成绩，并且这些结果分别为第二好的竞争对手的7.4倍、3.1倍和29.5倍更好。此外，结合真实数据与Lung-DDPM生成的数据训练的肺结节分割模型在Dice系数和敏感性方面分别达到了0.3914和0.4393的成绩，比单独使用真实数据模型高8.8%和18.6%。实验结果表明Lung-DDPM具有更广泛的应用潜力，例如肿瘤分割、癌症生存估计以及风险预测等方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of artificial intelligence (AI), AI-assistedmedical imaging analysis demonstrates remarkable performance in early lungcancer screening. However, the costly annotation process and privacy concernslimit the construction of large-scale medical datasets, hampering the furtherapplication of AI in healthcare. To address the data scarcity in lung cancerscreening, we propose Lung-DDPM, a thoracic CT image synthesis approach thateffectively generates high-fidelity 3D synthetic CT images, which prove helpfulin downstream lung nodule segmentation tasks. Our method is based on semanticlayout-guided denoising diffusion probabilistic models (DDPM), enablinganatomically reasonable, seamless, and consistent sample generation even fromincomplete semantic layouts. Our results suggest that the proposed methodoutperforms other state-of-the-art (SOTA) generative models in image qualityevaluation and downstream lung nodule segmentation tasks. Specifically,Lung-DDPM achieved superior performance on our large validation cohort, with aFr\'echet inception distance (FID) of 0.0047, maximum mean discrepancy (MMD) of0.0070, and mean squared error (MSE) of 0.0024. These results were 7.4$\times$,3.1$\times$, and 29.5$\times$ better than the second-best competitors,respectively. Furthermore, the lung nodule segmentation model, trained on adataset combining real and Lung-DDPM-generated synthetic samples, attained adice coefficient (Dice) of 0.3914 and sensitivity of 0.4393. This represents8.8\% and 18.6\% improvements in DICE and sensitivity compared to the modeltrained solely on real samples. The experimental results highlight Lung-DDPM'spotential for a broader range of medical imaging applications, such as generaltumor segmentation, cancer survival estimation, and risk prediction.</description>
      <author>example@mail.com (Yifan Jiang, Yannick Lemaréchal, Josée Bafaro, Jessica Abi-Rjeile, Philippe Joubert, Philippe Després, Venkata Manem)</author>
      <guid isPermaLink="false">2502.15204v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models</title>
      <link>http://arxiv.org/abs/2502.14940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for GeoSpatial Week 2025, ISPRS Annals&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了FacaDiffy，这是一种利用个性化稳定扩散模型填补2D冲突地图中未见立面部分的方法，从而提高高精度3D语义建筑重建中的开口位置检测率。&lt;h4&gt;背景&lt;/h4&gt;在创建高细节的三维建筑物模型时，2D冲突图用于识别建筑物外墙上的开口位置。然而，在实际激光扫描过程中，这些2D冲突图由于障碍物的影响往往是不完整的。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来填补2D冲突地图中未见立面部分，并使用个性化稳定扩散模型来完成冲突地图的绘制。&lt;h4&gt;方法&lt;/h4&gt;{'1': '首先提出了一个确定性的光线分析方法，从现有的3D建筑模型和对应的激光扫描点云数据中推导出2D冲突图', '2': '利用个性化稳定扩散模型的能力将未见立面对象填充到这些2D冲突图中。', '3': '为了补充真实世界训练数据的不足，开发了一条可扩展的数据生成流水线，使用随机城市模型生成器和标记后的立面图像来创建合成冲突地图。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，FacaDiffy在与各种填补基线相比时，在2D冲突图完成方面达到了最先进的性能，并且当应用到高精度3D语义建筑重建中时，检测率提高了22%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过个性化稳定扩散模型有效地解决了实际激光扫描过程中遇到的建筑物外墙部分缺失的问题，为高细节三维模型创建提供了强有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;详细摘要的中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-detail semantic 3D building models are frequently utilized in robotics,geoinformatics, and computer vision. One key aspect of creating such models isemploying 2D conflict maps that detect openings' locations in building facades.Yet, in reality, these maps are often incomplete due to obstacles encounteredduring laser scanning. To address this challenge, we introduce FacaDiffy, anovel method for inpainting unseen facade parts by completing conflict mapswith a personalized Stable Diffusion model. Specifically, we first propose adeterministic ray analysis approach to derive 2D conflict maps from existing 3Dbuilding models and corresponding laser scanning point clouds. Furthermore, wefacilitate the inpainting of unseen facade objects into these 2D conflict mapsby leveraging the potential of personalizing a Stable Diffusion model. Tocomplement the scarcity of real-world training data, we also develop a scalablepipeline to produce synthetic conflict maps using random city model generatorsand annotated facade images. Extensive experiments demonstrate that FacaDiffyachieves state-of-the-art performance in conflict map completion compared tovarious inpainting baselines and increases the detection rate by $22\%$ whenapplying the completed conflict maps for high-definition 3D semantic buildingreconstruction. The code is be publicly available in the corresponding GitHubrepository: https://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects</description>
      <author>example@mail.com (Thomas Froech, Olaf Wysocki, Yan Xia, Junyu Xie, Benedikt Schwab, Daniel Cremers, Thomas H. Kolbe)</author>
      <guid isPermaLink="false">2502.14940v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>TransMamba: Fast Universal Architecture Adaption from Transformers to Mamba</title>
      <link>http://arxiv.org/abs/2502.15130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了利用现有的Transformer模型如LLaVA、CLIP和DEIT的知识，通过跨架构训练来增强Mamba架构的性能。提出了TransMamba方法，并采用两阶段策略加速新Mamba模型的训练。&lt;h4&gt;背景&lt;/h4&gt;Transformer因其注意力模块在单模态和多模态基础模型中的灵活扩展性而受到青睐，但从头开始为特定任务培训专门的次二次架构既耗时又耗费资源。&lt;h4&gt;目的&lt;/h4&gt;探索如何通过跨架构训练将现有的预训练Transformer模型的知识转移到Mamba架构上，以提升其性能并减少所需的训练时间和计算资源。&lt;h4&gt;方法&lt;/h4&gt;采用两种主要策略：一是引入Weight Subcloning and Adaptive Bidirectional distillation (WSAB) 方法来实现不受层数限制的知识迁移；二是设计了一个跨模态学习模块——cross-Mamba模块，该模块将语言感知与Mamba的视觉特征相结合，从而增强其处理跨模态任务的能力。&lt;h4&gt;主要发现&lt;/h4&gt;TransMamba在使用不到常规从头训练所需75%的数据量的情况下，在各种网络架构和下游任务中（如图像分类、视觉问答、文本视频检索等）表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过将现有的Transformer模型的知识转移到Mamba架构上，可以显著提高后者的性能，并且代码将在未来公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have been favored in both uni-modal and multi-modal foundationmodels for their flexible scalability in attention modules. Consequently, anumber of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, arepublicly available. Recent research has introduced subquadratic architectureslike Mamba, which enables global awareness with linear complexity.Nevertheless, training specialized subquadratic architectures from scratch forcertain tasks is both resource-intensive and time-consuming. As a motivator, weexplore cross-architecture training to transfer the ready knowledge in existingTransformer models to alternative architecture Mamba, termed TransMamba. Ourapproach employs a two-stage strategy to expedite training new Mamba models,ensuring effectiveness in across uni-modal and cross-modal tasks. Concerningarchitecture disparities, we project the intermediate features into an alignedlatent space before transferring knowledge. On top of that, a Weight Subcloningand Adaptive Bidirectional distillation method (WSAB) is introduced forknowledge transfer without limitations on varying layer counts. For cross-modallearning, we propose a cross-Mamba module that integrates language awarenessinto Mamba's visual features, enhancing the cross-modal interactioncapabilities of Mamba architecture. Despite using less than 75% of the trainingdata typically required for training from scratch, TransMamba boastssubstantially stronger performance across various network architectures anddownstream tasks, including image classification, visual question answering,and text-video retrieval. The code will be publicly available.</description>
      <author>example@mail.com (Xiuwei Chen, Sihao Lin, Xiao Dong, Zisheng Chen, Meng Cao, Jianhua Han, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.15130v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning</title>
      <link>http://arxiv.org/abs/2502.15082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/Vaidehi99/UPCORE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UPCORE是一种用于减轻遗忘过程中附带损害的数据选择框架。该方法通过减少模型在忘记集上的表示方差来最小化模型退化。&lt;h4&gt;背景&lt;/h4&gt;当需要从预训练模型中删除特定信息时，通常会导致模型性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够平衡信息删除和保留模型其他能力的方法，避免因失去平衡导致的模型不可用或效果不佳。&lt;h4&gt;方法&lt;/h4&gt;UPCORE是一种与方法无关的数据选择框架。通过选择性修剪忘记集来减少离群值，以最小化遗忘后的模型退化。&lt;h4&gt;主要发现&lt;/h4&gt;在三种标准删除方法上评估了UPCORE的表现，并引入了一个新的度量标准（AUC），用于衡量其效果优于其他现有技术。&lt;h4&gt;结论&lt;/h4&gt;UPCORE不仅提高了标准指标和AUC的得分，还通过减少负面迁移并利用核心集与修剪点之间的积极迁移效应来增强模型性能。&lt;h4&gt;翻译&lt;/h4&gt;用户规格或法律法规通常要求从预训练模型（包括大型语言模型）中移除信息。这需要删除或“忘记”一组已训练模型中的数据点，通常会导致其在其他数据点上的表现下降。因此，在去除信息和保持模型其余功能之间必须找到平衡，否则可能导致较差的删除效果或不可用的模型。为此，我们提出了UPCORE（Utility-Preserving Coreset Selection），这是一种用于减轻遗忘过程中附带损害的方法无关的数据选择框架。发现模型损坏与忘记集上表示方差相关，通过选择性地修剪该集合来去除离群值从而最小化遗忘后的退化。我们在三种标准删除方法中评估了UPCORE，并始终达到在删除有效性和模型保留之间竞争目标的优越平衡。为了更好地衡量这种权衡，我们引入了一个新的度量指标（AUC），其结果表明UPCORE提高了标准指标和AUC的得分，通过减少负面迁移并利用核心集与修剪点之间的积极迁移效应来增强模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User specifications or legal frameworks often require information to beremoved from pretrained models, including large language models (LLMs). Thisrequires deleting or "forgetting" a set of data points from an already-trainedmodel, which typically degrades its performance on other data points. Thus, abalance must be struck between removing information and keeping the model'sother abilities intact, with a failure to balance this trade-off leading topoor deletion or an unusable model. To this end, we propose UPCORE(Utility-Preserving Coreset Selection), a method-agnostic data selectionframework for mitigating collateral damage during unlearning. Finding that themodel damage is correlated with the variance of the model's representations onthe forget set, we selectively prune the forget set to remove outliers, therebyminimizing model degradation after unlearning. We evaluate UPCORE across threestandard unlearning methods consistently achieving a superior balance betweenthe competing objectives of deletion efficacy and model preservation. To betterevaluate this trade-off, we introduce a new metric, measuring thearea-under-the-curve (AUC) across standard metrics. We find that UPCOREimproves both standard metrics and AUC, benefitting from positive transferbetween the coreset and pruned points while reducing negative transfer from theforget set to points outside of it.</description>
      <author>example@mail.com (Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal)</author>
      <guid isPermaLink="false">2502.15082v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Physics-Guided Foundation Models</title>
      <link>http://arxiv.org/abs/2502.15013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的物理引导基础模型（PGFM），旨在将广泛领域的物理知识集成到传统基础模型中。&lt;h4&gt;背景&lt;/h4&gt;传统的基础模型是通过大规模数据集预训练的，目的是减少微调大量下游任务所需的资源。&lt;h4&gt;目的&lt;/h4&gt;解决传统基础模型在处理分布外预测时的问题，并避免产生不现实或物理上不可行的输出。&lt;h4&gt;方法&lt;/h4&gt;提出将广泛领域的（例如科学领域）通用知识集成到基础模型中的方法，形成物理引导基础模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的基础模型通过大规模数据集进行预训练，以减少在广泛的下游任务中微调所需的资源（如时间、能量和标记样本）。然而，传统的基础模型难以处理分布外预测，并且会产生不现实或物理上不可行的输出。我们提出了物理引导基础模型（PGFM）的概念，即将适用于广泛下游任务的广义或通用领域知识（例如科学领域的知识）集成到基础模型中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional foundation models are pre-trained on broad datasets to reduce thetraining resources (e.g., time, energy, labeled samples) needed for fine-tuninga wide range of downstream tasks. However, traditional foundation modelsstruggle with out-of-distribution prediction and can produce outputs that areunrealistic and physically infeasible. We propose the notation ofphysics-guided foundation models (PGFM), that is, foundation models integratedwith broad or general domain (e.g., scientific) physical knowledge applicableto a wide range of downstream tasks.</description>
      <author>example@mail.com (Majid Farhadloo, Arun Sharma, Mingzhou Yang, Bharat Jayaprakash, William Northrop, Shashi Shekhar)</author>
      <guid isPermaLink="false">2502.15013v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models</title>
      <link>http://arxiv.org/abs/2502.15010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的版权协议强调了对语言模型复制受版权保护的内容的能力进行精确控制的需求。提出了一种名为Obliviate的新方法，该方法是一种新型的后训练技术，能够选择性地防止特定文本的逐字复制，同时保持语义理解。&lt;h4&gt;背景&lt;/h4&gt;AI公司与内容创作者之间的最近版权协议强调了对语言模型在复制受版权保护的内容时进行精确控制的需求。现有方法依赖于通过去学习或简单输出过滤来完全移除概念。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的后训练技术Obliviate，该技术能够选择性地防止逐字复制特定文本，同时保持语义理解。&lt;h4&gt;方法&lt;/h4&gt;Obliviate通过在记忆序列中选择标记并修改模型的概率分布以防止精确复制来工作，同时维持上下文理解。评估了多个大型语言模型（LLaMA-3.1 8B、LLaMA-3.1-instruct 8B、Qwen-2.5-7B 和 Yi-1.5 6B）在合成记忆任务和有机版权内容上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Obliviate实现了逐字记忆的数个数量级（例如100倍）的减少，同时保持了模型性能与基准线相比仅下降不到1%。这对于解决预训练模型中的版权问题尤其有效，而不损害其通用能力。&lt;h4&gt;结论&lt;/h4&gt;由于在解决语言模型中出现的版权复制风险方面表现出了显著的效果，并且能够在不影响整体功能的情况下大幅度降低逐字记忆的风险，Obliviate非常适合于实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;最近的版权协议强调了对AI语言模型复制受版权保护内容能力进行精确控制的需求。现有方法依赖完全移除概念或简单输出过滤来解决此问题。然而，这项研究提出了一种名为Obliviate的新后训练技术，该技术能够选择性地防止逐字复现特定文本，同时保持语义理解。通过在大型语言模型上进行了测试，证明了这种方法可以大幅度减少复制风险，而不会影响模型的性能和通用能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent copyright agreements between AI companies and content creators havehighlighted the need for precise control over language models' ability toreproduce copyrighted content. While existing approaches rely on eithercomplete concept removal through unlearning or simple output filtering, wepropose Obliviate, a novel post-training technique that selectively preventsverbatim reproduction of specific text while preserving semantic understanding.  Obliviate operates by selecting tokens within memorized sequences andmodifying the model's probability distribution to prevent exact reproductionwhile maintaining contextual understanding. We evaluate Obliviate on multiplelarge language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, andYi-1.5 6B) across both synthetic memorization tasks and organic copyrightcontent. Our results demonstrate that Obliviate achieves orders of magnitudereduction, e.g., 100x, in verbatim memorization while maintaining modelperformance within 1% of baseline on standard benchmarks (HellaSwag, MMLU,TruthfulQA, and Winogrande). This makes Obliviate particularly suitable forpractical deployment scenarios where companies need to efficiently addresscopyright concerns in pretrained models without compromising their generalcapabilities.</description>
      <author>example@mail.com (Mark Russinovich, Ahmed Salem)</author>
      <guid isPermaLink="false">2502.15010v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models in Medical Image Analysis: Advances and Challenges</title>
      <link>http://arxiv.org/abs/2502.14584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了视觉基础模型在医学图像分割领域适应性的最新研究进展，重点讨论了域适应、模型压缩和联邦学习的挑战，并提出了未来的研究方向。&lt;h4&gt;背景&lt;/h4&gt;随着Vision Foundation Models (VFMs)的发展，特别是ViT和SAM模型，在医疗影像分析中的应用显示出卓越的能力。然而，将这些大型模型应用于医学图像分析面临多个挑战，包括医学图像与自然图像之间的领域差异、高效的适应策略需求以及小规模数据集的限制。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在提供关于视觉基础模型在医学图像分割中适应性的最新研究进展的一个全面概述，并指出未来研究的关键领域以推动下一轮创新。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了基于适配器改进的方法、知识蒸馏技术以及多尺度上下文特征建模的发展。&lt;h4&gt;主要发现&lt;/h4&gt;最新的发展包括通过新兴的技术如联邦学习和模型压缩，增强了视觉基础模型在医学图像分析领域的潜力。&lt;h4&gt;结论&lt;/h4&gt;文中强调了VFMs的未来应用前景，并指出了克服现有瓶颈的关键方法。文章呼吁研究者们关注这些领域以推动医疗影像分割技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了Vision Foundation Models (VFMs)迅速发展，特别是在医学图像分析中展示了出色的能力。但是将它们应用于医疗图像存在许多挑战，如领域差异、模型适应策略效率需求和小规模数据集的限制。本文综述了相关最新研究，并提出了未来的研究方向以克服这些瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Vision Foundation Models (VFMs), particularly VisionTransformers (ViT) and Segment Anything Model (SAM), has sparked significantadvances in the field of medical image analysis. These models have demonstratedexceptional capabilities in capturing long-range dependencies and achievinghigh generalization in segmentation tasks. However, adapting these large modelsto medical image analysis presents several challenges, including domaindifferences between medical and natural images, the need for efficient modeladaptation strategies, and the limitations of small-scale medical datasets.This paper reviews the state-of-the-art research on the adaptation of VFMs tomedical image segmentation, focusing on the challenges of domain adaptation,model compression, and federated learning. We discuss the latest developmentsin adapter-based improvements, knowledge distillation techniques, andmulti-scale contextual feature modeling, and propose future directions toovercome these bottlenecks. Our analysis highlights the potential of VFMs,along with emerging methodologies such as federated learning and modelcompression, to revolutionize medical image analysis and enhance clinicalapplications. The goal of this work is to provide a comprehensive overview ofcurrent approaches and suggest key areas for future research that can drive thenext wave of innovation in medical image segmentation.</description>
      <author>example@mail.com (Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang)</author>
      <guid isPermaLink="false">2502.14584v2</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>M2LADS Demo: A System for Generating Multimodal Learning Analytics Dashboards</title>
      <link>http://arxiv.org/abs/2502.15363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Workshop on Innovation and Responsibility in  AI-Supported Education (iRAISE25) at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;介绍了一种基于Web的系统M2LADS，该系统旨在整合、同步并可视化在计算机学习过程中通过生物传感器记录的多模态数据。&lt;h4&gt;目的&lt;/h4&gt;提供详细的生理和活动相关的指标见解，并帮助数据科学家通过综合视图展示参与者的体验以及简化错误活动信息的数据重标记过程。&lt;h4&gt;方法&lt;/h4&gt;使用EEG数据评估注意力和大脑活动，心率指标、眼动追踪数据测量视觉注意，网络摄像头视频录制以及监控任务的日志记录等多模态数据进行可视化。&lt;h4&gt;主要发现&lt;/h4&gt;M2LADS系统能将多种生物信号及视频同步，并通过基于Web的仪表板展示参与者的行为和生理数据，使研究人员能够更详细地了解学习者在不同活动中的表现。&lt;h4&gt;结论&lt;/h4&gt;该系统的使用为教育研究领域提供了新的视角，增强了对计算机辅助学习环境中学生行为和生理状态的理解。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为M2LADS的基于Web的系统（用于生成多模态学习分析仪表板），旨在整合、同步、可视化并分析在使用生物传感器记录计算机辅助学习过程中的多模态数据。该系统在一个Web仪表板上展示了广泛的生物测量和行为数据，提供了对各种生理和活动相关指标的深入了解。可视化的多模态数据包括用于评估注意力和大脑活动的脑电图（EEG）数据、心率指标、通过眼动追踪来衡量视觉关注的数据、网络摄像头视频记录以及监控任务的日志。M2LADS旨在以两种关键方式帮助数据科学家：(1)提供参与者体验的综合视图，将所有数据按参与者的活动进行分类展示；(2)同步所有的生物信号和视频，使在错误活动中更容易重标记数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a demonstration of a web-based system called M2LADS ("System forGenerating Multimodal Learning Analytics Dashboards"), designed to integrate,synchronize, visualize, and analyze multimodal data recorded duringcomputer-based learning sessions with biosensors. This system presents a rangeof biometric and behavioral data on web-based dashboards, providing detailedinsights into various physiological and activity-based metrics. The multimodaldata visualized include electroencephalogram (EEG) data for assessing attentionand brain activity, heart rate metrics, eye-tracking data to measure visualattention, webcam video recordings, and activity logs of the monitored tasks.M2LADS aims to assist data scientists in two key ways: (1) by providing acomprehensive view of participants' experiences, displaying all datacategorized by the activities in which participants are engaged, and (2) bysynchronizing all biosignals and videos, facilitating easier data relabeling ifany activity information contains errors.</description>
      <author>example@mail.com (Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Julian Fierrez)</author>
      <guid isPermaLink="false">2502.15363v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Research advances on fish feeding behavior recognition and intensity quantification methods in aquaculture</title>
      <link>http://arxiv.org/abs/2502.15311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了基于计算机视觉、声学和传感器单一模态的鱼类进食行为识别与强度量化方法的研究进展，并探讨了当前新兴多模态融合技术在鱼类进食行为识别与强度量化的应用。&lt;h4&gt;背景&lt;/h4&gt;鱼饲料投喂行为的识别与定量分析是水产养殖管理的关键部分，对于监测鱼类健康、指导饲喂工作和提高水产养殖效率具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;为了更好地进行未来相关研究，本论文回顾了基于单一模态技术（计算机视觉、声学及传感器）的研究进展，并探讨多模态融合在该领域的应用。此外还分析了各种方法的优缺点并展望了未来的研发方向。&lt;h4&gt;方法&lt;/h4&gt;论文首先总结了单模态技术如计算机视觉、声学和传感器技术在此研究领域内的研究成果，随后详细介绍了新兴的多模态融合技术的应用情况。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比分析不同技术的优点与不足之处，揭示未来的研究趋势和方向。&lt;h4&gt;结论&lt;/h4&gt;基于对已有工作的回顾以及当前研究状况的理解，论文认为结合多种数据源和技术手段（特别是多模态方法）来解决鱼类进食行为识别及强度量化问题具有广阔的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key part of aquaculture management, fish feeding behavior recognitionand intensity quantification has been a hot area of great concern toresearchers, and it plays a crucial role in monitoring fish health, guidingbaiting work and improving aquaculture efficiency. In order to better carry outthe related work in the future, this paper firstly reviews the researchadvances of fish feeding behavior recognition and intensity quantificationmethods based on computer vision, acoustics and sensors in a single modality.Then the application of the current emerging multimodal fusion in fish feedingbehavior recognition and intensity quantification methods is expounded.Finally, the advantages and disadvantages of various techniques are comparedand analyzed, and the future research directions are envisioned.</description>
      <author>example@mail.com (Shulong Zhang, Daoliang Li, Jiayin Zhao, Mingyuan Yao, Yingyi Chen, Yukang Huo, Xiao Liu, Haihua Wang)</author>
      <guid isPermaLink="false">2502.15311v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.15488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的量化感知位置嵌入变换(Q-PETR)方法，以提高多视角3D物体检测模型在INT8推理时的精度。&lt;h4&gt;背景&lt;/h4&gt;PETR系列的方法在3D感知领域占据主导地位，并成为现代自动驾驶系统的关键组件。但是，在需要INT8推理的情况下，这些模型的量化性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的量化感知位置嵌入变换方法(Q-PETR)，以解决PETR系列方法在INT8推理时精度下降的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了一种针对多视角3D物体检测任务的量化友好的位置嵌入转换机制，即Q-PETR，它可以在保持原始性能的同时提供更友好的部署环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在标准的8位定点后训练量化中，该方法将mAP和NDS下降限制在1%以内。此外，该方法还超过了原PETR模型在浮点精度上的表现，并且具有广泛的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Q-PETR提供了一种同时解决性能和部署问题的有效解决方案，在INT8推理时能够大幅缩小与FP32推理之间的准确度差距。&lt;h4&gt;翻译&lt;/h4&gt;基于PETR的方法已经在3D感知领域占据主导地位，越来越成为现代自动驾驶系统中的关键组件。然而，当需要进行INT8推理时，它们的量化表现显著下降，例如在NuScenes数据集上分别导致了58.2% mAP和36.9% NDS的性能损失。为了解决这一问题，我们提出了一种针对多视角3D物体检测任务的量化感知位置嵌入变换方法(Q-PETR)，它提供了更加友好的量化部署环境同时保持了PETR的原始性能。此外，该方法在标准8位定点后训练量化下大幅缩小了INT8和FP32推理之间的准确度差距，并且在浮点精度上超过了原PETR模型的表现。通过针对多种PETR系列模型进行广泛的实验验证了其泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PETR-based methods have dominated benchmarks in 3D perception and areincreasingly becoming a key component in modern autonomous driving systems.However, their quantization performance significantly degrades when INT8inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS onthe NuScenes dataset. To address this issue, we propose a quantization-awareposition embedding transformation for multi-view 3D object detection, termedQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendlyarchitecture while preserving the original performance of PETR. Itsubstantially narrows the accuracy gap between INT8 and FP32 inference forPETR-series methods. Without bells and whistles, our approach reduces the mAPand NDS drop to within 1% under standard 8-bit per-tensor post-trainingquantization. Furthermore, our method exceeds the performance of the originalPETR in terms of floating-point precision. Extensive experiments across avariety of PETR-series models demonstrate its broad generalization.</description>
      <author>example@mail.com (Jiangyong Yu, Changyong Shu, Dawei Yang, Zichen Yu, Xing Hu, Yan Chen)</author>
      <guid isPermaLink="false">2502.15488v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.14891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;合作3D物体检测在自动驾驶领域具有重要意义，它通过多代理系统之间的信息交换极大地增强了单个代理的感知能力。&lt;h4&gt;背景&lt;/h4&gt;由于姿态估计误差和时间延迟的影响，在实际应用中，跨多个代理的信息融合通常会导致带有空间和时间噪声的功能表示，并导致检测错误。&lt;h4&gt;目的&lt;/h4&gt;为了应对多代理系统之间存在的噪音问题，我们探索了使用扩散模型来净化嘈杂样本并将其转化为理想数据的可能性。&lt;h4&gt;方法&lt;/h4&gt;提出了CoDiff框架，该框架利用预训练的自编码器的强大潜在空间将高维特征图转换为低维度，并通过条件引导的方式让各个代理的信息指导扩散模型进行采样。这一过程可以去除粗糙特征图中的噪声，并逐步细化融合后的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界数据集上的实验研究表明，所提出的CoDiff框架在合作物体检测性能方面比现有的相关方法更加出色，尤其当代理的姿态信息和延迟带有高水平的噪音时，其表现出高度期望的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这是首次将扩散模型应用于多代理协作感知的工作。该工作表明了扩散模型解决多代理系统中噪声问题的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了合作3D物体检测在自动驾驶中的重要性，指出当前方法面临的挑战，并提出了一种新的框架CoDiff，利用扩散模型来提高特征表示的质量和清晰度，实验结果证明其优越性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative 3D object detection holds significant importance in the fieldof autonomous driving, as it greatly enhances the perception capabilities ofeach individual agent by facilitating information exchange among multipleagents. However, in practice, due to pose estimation errors and time delays,the fusion of information across agents often results in featurerepresentations with spatial and temporal noise, leading to detection errors.Diffusion models naturally have the ability to denoise noisy samples to theideal data, which motivates us to explore the use of diffusion models toaddress the noise problem between multi-agent systems. In this work, we proposeCoDiff, a novel robust collaborative perception framework that leverages thepotential of diffusion models to generate more comprehensive and clearerfeature representations. To the best of our knowledge, this is the first workto apply diffusion models to multi-agent collaborative perception.Specifically, we project high-dimensional feature map into the latent space ofa powerful pre-trained autoencoder. Within this space, individual agentinformation serves as a condition to guide the diffusion model's sampling. Thisprocess denoises coarse feature maps and progressively refines the fusedfeatures. Experimental study on both simulated and real-world datasetsdemonstrates that the proposed framework CoDiff consistently outperformsexisting relevant methods in terms of the collaborative object detectionperformance, and exhibits highly desired robustness when the pose and delayinformation of agents is with high-level noise.</description>
      <author>example@mail.com (Zhe Huang, Shuo Wang, Yongcai Wang, Lei Wang)</author>
      <guid isPermaLink="false">2502.14891v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Network Resource Optimization for ML-Based UAV Condition Monitoring with Vibration Analysis</title>
      <link>http://arxiv.org/abs/2502.15491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Networking Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着智慧城市的发展，无人飞行器（UAVs）及其可靠性变得越来越重要。本文通过优化基于机器学习的UAV条件监测框架中的网络资源利用来提高其在边缘计算环境下的效率。&lt;h4&gt;背景&lt;/h4&gt;智慧城市的构建推动了对UAV可靠性的需求，其中机器学习模型用于识别异常和不利条件是关键环节之一。&lt;h4&gt;目的&lt;/h4&gt;探索如何最小化下一代边缘网络中珍贵的网络资源使用，并优化基于ML的UAV状态监测框架中的网络资源配置。&lt;h4&gt;方法&lt;/h4&gt;开发了一种利用实验数据并调整特征提取聚合间隔来选择最有效机器学习模型的方法，同时采用维度降低技术减少了99.9%的网络资源消耗。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述方法，在保证准确性的前提下显著降低了网络资源使用量。&lt;h4&gt;结论&lt;/h4&gt;提出的框架能够有效地减少基于ML的UAV状态监测系统所需的网络资源，并提高了在有限资源条件下的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;随着智慧城市的发展，无人飞行器（UAVs）及其可靠性变得越来越重要。本文通过优化基于机器学习的UAV条件监测框架中的网络资源利用来提高其在边缘计算环境下的效率。研究指出，在资源受限的下一代边缘网络环境中，需要尽可能地减少珍贵的网络资源使用量。所提出的方法通过调整特征提取聚合间隔，并采用维度降低技术实现了这一目标，同时保证了模型性能和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As smart cities begin to materialize, the role of Unmanned Aerial Vehicles(UAVs) and their reliability becomes increasingly important. One aspect ofreliability relates to Condition Monitoring (CM), where Machine Learning (ML)models are leveraged to identify abnormal and adverse conditions. Given theresource-constrained nature of next-generation edge networks, the utilizationof precious network resources must be minimized. This work explores theoptimization of network resources for ML-based UAV CM frameworks. The developedframework uses experimental data and varies the feature extraction aggregationinterval to optimize ML model selection. Additionally, by leveragingdimensionality reduction techniques, there is a 99.9% reduction in networkresource consumption.</description>
      <author>example@mail.com (Alexandre Gemayel, Dimitrios Michael Manias, Abdallah Shami)</author>
      <guid isPermaLink="false">2502.15491v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>BOSS: Benchmark for Observation Space Shift in Long-Horizon Task</title>
      <link>http://arxiv.org/abs/2502.15679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于评估观察空间移位（OSS）对长时任务影响的新基准测试BOSS，并展示了几种模仿学习算法在面对此类问题时的性能下降情况。&lt;h4&gt;背景&lt;/h4&gt;视觉伺服机器人旨在完成前所未见的长期任务，而分层方法通过执行由任务计划器安排的技能组合来实现这一目标。然而，在简单如技巧串联的任务中，观察空间移位的问题会破坏单独训练的技能策略的表现。&lt;h4&gt;目的&lt;/h4&gt;提出并验证BOSS基准测试，评估模仿学习算法在面对观察空间移位问题时的性能下降情况，并探索解决OSS的方法。&lt;h4&gt;方法&lt;/h4&gt;引入了BOSS（观察空间移位基准）来衡量和评估观察空间移位对长时任务的影响。BOSS包括三个不同的挑战：单一谓词转移、累积谓词转移和技巧串联，用于测试不同方面的负面影响。此外，作者还测试了几种流行的模仿学习算法在BOSS上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在最简单的挑战下，各种算法性能下降显著，分别为67%，35%，34%和54%。增加训练数据的规模以解决OSS问题的方法并未达到预期效果。&lt;h4&gt;结论&lt;/h4&gt;观察空间移位对长时任务中技能策略的表现具有负面影响，而现有解决方案不足以完全解决这一问题。&lt;h4&gt;翻译&lt;/h4&gt;机器人技术长期以来一直致力于开发能够完成未见过的长期任务的视觉伺服机器人。分层方法通过执行由任务计划器安排的技能组合来实现这个目标，并且每个视觉运动技能都使用特定的模仿学习（IL）算法预先训练。然而，即使在简单的长期任务如技巧串联中，分层方法也常常因观察空间移位问题而难以实现目标。为了验证这一问题并评估其对长期任务的影响，我们引入了BOSS基准测试来衡量这个问题。BOSS包括三个不同的挑战：“单一谓词转移”、“累积谓词转移”和“技巧串联”，每个挑战都旨在评估OSS的负面影响的不同方面。我们在BOSS上评估了几种最近流行的IL算法，其中包括三种行为克隆方法和视觉语言动作模型OpenVLA。即使在最简单的挑战中，我们观察到当技能性能在有无观察空间移位的情况下对比时，平均性能下降分别为67%，35%，34%和54%。此外，我们研究了一种解决OSS的潜在解决方案，即通过使用更大且视觉上更加多样的示例数据集来增加每个技能训练数据的规模，但结果显示这种方法不足以解决问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotics has long sought to develop visual-servoing robots capable ofcompleting previously unseen long-horizon tasks. Hierarchical approaches offera pathway for achieving this goal by executing skill combinations arranged by atask planner, with each visuomotor skill pre-trained using a specific imitationlearning (IL) algorithm. However, even in simple long-horizon tasks like skillchaining, hierarchical approaches often struggle due to a problem we identifyas Observation Space Shift (OSS), where the sequential execution of precedingskills causes shifts in the observation space, disrupting the performance ofsubsequent individually trained skill policies. To validate OSS and evaluateits impact on long-horizon tasks, we introduce BOSS (a Benchmark forObservation Space Shift). BOSS comprises three distinct challenges: "SinglePredicate Shift", "Accumulated Predicate Shift", and "Skill Chaining", eachdesigned to assess a different aspect of OSS's negative effect. We evaluatedseveral recent popular IL algorithms on BOSS, including three BehavioralCloning methods and the Visual Language Action model OpenVLA. Even on thesimplest challenge, we observed average performance drops of 67%, 35%, 34%, and54%, respectively, when comparing skill performance with and without OSS.Additionally, we investigate a potential solution to OSS that scales up thetraining data for each skill with a larger and more visually diverse set ofdemonstrations, with our results showing it is not sufficient to resolve OSS.The project page is: https://boss-benchmark.github.io/</description>
      <author>example@mail.com (Yue Yang, Linfeng Zhao, Mingyu Ding, Gedas Bertasius, Daniel Szafir)</author>
      <guid isPermaLink="false">2502.15679v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>VaViM and VaVAM: Autonomous Driving through Video Generative Modeling</title>
      <link>http://arxiv.org/abs/2502.15672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and model: https://github.com/valeoai/VideoActionModel, project  page: https://valeoai.github.io/vavim-vavam/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了大规模生成式视频模型在自动驾驶中的潜力，介绍了开源的自回归视频模型（VaViM）和其辅助视频动作模型（VaVAM），以探索视频预训练如何应用于实际驾驶。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习的发展，视频生成技术被引入到自动驾驶领域，特别是在理解和预测复杂的动态场景方面具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;研究的目的是通过开发新的视频预训练模型来提升自动驾驶系统在真实世界中的表现和安全性。&lt;h4&gt;方法&lt;/h4&gt;VaViM是一个简单的自回归视频模型，通过时空令牌序列预测帧；VaVAM则利用VaViM学习到的表示生成驾驶轨迹。两个模型共同形成了从感知到动作的完整管道，并且研究者们对其进行了开放循环和闭环驾驶场景评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，基于视频的预训练对于自动驾驶具有前景，包括所学表示的语义丰富性、视频合成中规模效应的好处以及在闭环评估中的模型大小与数据之间的复杂关系及其对安全度量的影响。&lt;h4&gt;结论&lt;/h4&gt;通过发布代码和模型权重，研究团队希望促进相关领域的进一步发展，并鼓励其他研究人员探索该方向的潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们探讨了大规模生成式视频模型在自主驾驶中的潜力，介绍了开源自回归视频模型（VaViM）及其辅助动作视频模型（VaVAM），以探究视频预训练如何应用于现实世界的自动驾驶。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore the potential of large-scale generative video models forautonomous driving, introducing an open-source auto-regressive video model(VaViM) and its companion video-action model (VaVAM) to investigate how videopre-training transfers to real-world driving. VaViM is a simple auto-regressivevideo model that predicts frames using spatio-temporal token sequences. We showthat it captures the semantics and dynamics of driving scenes. VaVAM, thevideo-action model, leverages the learned representations of VaViM to generatedriving trajectories through imitation learning. Together, the models form acomplete perception-to-action pipeline. We evaluate our models in open- andclosed-loop driving scenarios, revealing that video-based pre-training holdspromise for autonomous driving. Key insights include the semantic richness ofthe learned representations, the benefits of scaling for video synthesis, andthe complex relationship between model size, data, and safety metrics inclosed-loop evaluations. We release code and model weights athttps://github.com/valeoai/VideoActionModel</description>
      <author>example@mail.com (Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, Éloi Zablocki, Andrei Bursuc, Eduardo Valle, Matthieu Cord)</author>
      <guid isPermaLink="false">2502.15672v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network</title>
      <link>http://arxiv.org/abs/2502.15662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于技能-环境贝叶斯网络(SEBN)的方法，以减少强化学习训练时间或提高目标任务的性能。&lt;h4&gt;背景&lt;/h4&gt;在强化学习中，自动生成课程以减少训练时间和提升性能是主要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;通过使用SEBN模型来预测代理在各种任务上的表现，并根据这些预测来加权可能的任务，从而开发一种算法来优化课程设置。&lt;h4&gt;方法&lt;/h4&gt;利用SEBN模型对代理成功概率的推断估计来评估下一个潜在任务的预期改进。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在三种不同环境中（离散格子世界、连续控制和模拟机器人）使用SEBN构建的课程比其他基准线更有效。&lt;h4&gt;结论&lt;/h4&gt;通过将技能与环境特征及奖励结构相关联，SEBN能够预测代理在各种任务上的表现并优化学习过程中的课程设置。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了SEBN模型及其用于减少训练时间和提升性能的方法，并展示了它优于传统基线的实验结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge for reinforcement learning is automatically generatingcurricula to reduce training time or improve performance in some target task.We introduce SEBNs (Skill-Environment Bayesian Networks) which model aprobabilistic relationship between a set of skills, a set of goals that relateto the reward structure, and a set of environment features to predict policyperformance on (possibly unseen) tasks. We develop an algorithm that uses theinferred estimates of agent success from SEBN to weigh the possible next tasksby expected improvement. We evaluate the benefit of the resulting curriculum onthree environments: a discrete gridworld, continuous control, and simulatedrobotics. The results show that curricula constructed using SEBN frequentlyoutperform other baselines.</description>
      <author>example@mail.com (Vincent Hsiao, Mark Roberts, Laura M. Hiatt, George Konidaris, Dana Nau)</author>
      <guid isPermaLink="false">2502.15662v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications</title>
      <link>http://arxiv.org/abs/2502.15649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted to be presented at IEEE SysCon 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种帮助减少仿真与现实差距、促进在真实世界机器人系统中开发和部署强化学习策略的流水线。&lt;h4&gt;背景&lt;/h4&gt;强化学习（RL）在解决复杂任务方面取得成功，特别是在机器人应用领域。然而，在物理机器人上实现它仍然充满挑战性，主要是由于安全风险和高昂的训练成本。为了解决这些问题，通常是在模拟器中训练RL代理，这又引入了关于仿真与现实之间差距的新问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种流水线来帮助减少仿真实验到现实操作之间的差距，并促进强化学习策略在实际机器人系统中的开发和部署。&lt;h4&gt;方法&lt;/h4&gt;该流程将RL的培训过程组织为初始的系统识别阶段，以及三个训练阶段：核心仿真培训、高保真仿真，然后是实地部署。每个阶段都会增加现实感的程度以减少模拟到真实之间的差距，并且通过迭代传递并改进策略来逐步达到所需性能。&lt;h4&gt;主要发现&lt;/h4&gt;该流水线的有效性在一项案例研究中得到证明，在这项研究中使用了Boston Dynamics Spot移动机器人执行监控应用。&lt;h4&gt;结论&lt;/h4&gt;提出的RL流程展示了通过各个阶段如何逐渐减少仿真与现实之间的差距，使开发的策略能够成功部署于真实环境中的机器人系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) has gained traction for its success in solvingcomplex tasks for robotic applications. However, its deployment on physicalrobots remains challenging due to safety risks and the comparatively high costsof training. To avoid these problems, RL agents are often trained onsimulators, which introduces a new problem related to the gap betweensimulation and reality. This paper presents an RL pipeline designed to helpreduce the reality gap and facilitate developing and deploying RL policies forreal-world robotic systems. The pipeline organizes the RL training process intoan initial step for system identification and three training stages: coresimulation training, high-fidelity simulation, and real-world deployment, eachadding levels of realism to reduce the sim-to-real gap. Each training stagetakes an input policy, improves it, and either passes the improved policy tothe next stage or loops it back for further improvement. This iterative processcontinues until the policy achieves the desired performance. The pipeline'seffectiveness is shown through a case study with the Boston Dynamics Spotmobile robot used in a surveillance application. The case study presents thesteps taken at each pipeline stage to obtain an RL agent to control the robot'sposition and orientation.</description>
      <author>example@mail.com (Jefferson Silveira, Joshua A. Marshall, Sidney N. Givigi Jr)</author>
      <guid isPermaLink="false">2502.15649v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Reduced-Order Model Guided Contact-Implicit Model Predictive Control for Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2502.15630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种结合简化的混合线性倒立摆模型（HLIP）和接触隐式模型预测控制（CI-MPC）优点的控制框架，旨在提高人形机器人的灵活性和实用性。&lt;h4&gt;背景&lt;/h4&gt;人形机器人在人类环境中操作具有巨大的应用潜力，但由于高维度非线性混合动力学的复杂性，部署面临挑战。虽然HLIP简化了模型，但丧失了全身表达能力；而CI-MPC能够处理多种接触模式下的规划问题，但仍存在局部最优和大量调优需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合HLIP与CI-MPC优点的新控制框架，以克服当前方法的局限性，并增强人形机器人的适应性和实用性。&lt;h4&gt;方法&lt;/h4&gt;该框架利用HLIP生成名义步态模式，同时使用CI-MPC处理全身动力学并根据需要调整接触序列。实验在24自由度的人形机器人Achilles上进行模拟测试。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的控制框架能够在粗糙地形行走、恢复外部干扰后的稳定性以及面对模型和状态不确定性时保持鲁棒性，同时能够与环境中的障碍物互动，并且以50Hz的频率实现实时在线运行。&lt;h4&gt;结论&lt;/h4&gt;结合HLIP和CI-MPC的优点可以显著提升人形机器人在复杂环境下的控制性能和适应能力。该框架为未来的人形机器人开发提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人的潜在应用领域因它们能在为人设计的环境中操作而广受期待，但其部署受到管理高维度非线性混合动力学挑战的影响。虽然简化的模型如HLIP简单且计算效率高，但这些模型缺乏全身表达能力。最近在CI-MPC上的进展使机器人能够通过多个混合接触模式进行规划，但仍容易陷入局部最优，并需要大量的调优工作。我们提出了一种结合HLIP和CI-MPC优点的控制框架：简化的模型产生名义步态，而CI-MPC管理全身动力学并根据需要调整接触安排。我们在模拟中使用一个新型24自由度的人形机器人Achilles展示了这种方法的有效性。我们的方法实现了粗糙地形行走、干扰恢复能力，在面对模型和状态不确定性时保持鲁棒性，并且能够与环境中的障碍物互动，所有这一切都在实时在线环境中以50Hz的频率运行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid robots have great potential for real-world applications due to theirability to operate in environments built for humans, but their deployment ishindered by the challenge of controlling their underlying high-dimensionalnonlinear hybrid dynamics. While reduced-order models like the Hybrid LinearInverted Pendulum (HLIP) are simple and computationally efficient, they losewhole-body expressiveness. Meanwhile, recent advances in Contact-Implicit ModelPredictive Control (CI-MPC) enable robots to plan through multiple hybridcontact modes, but remain vulnerable to local minima and require significanttuning. We propose a control framework that combines the strengths of HLIP andCI-MPC. The reduced-order model generates a nominal gait, while CI-MPC managesthe whole-body dynamics and modifies the contact schedule as needed. Wedemonstrate the effectiveness of this approach in simulation with a novel 24degree-of-freedom humanoid robot: Achilles. Our proposed framework achievesrough terrain walking, disturbance recovery, robustness under model and stateuncertainty, and allows the robot to interact with obstacles in theenvironment, all while running online in real-time at 50 Hz.</description>
      <author>example@mail.com (Sergio A. Esteban, Vince Kurtz, Adrian B. Ghansah, Aaron D. Ames)</author>
      <guid isPermaLink="false">2502.15630v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach</title>
      <link>http://arxiv.org/abs/2502.15613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Video and code are available at https://github.com/yaoxt3/GADP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于扩散的策略和混合学习优化框架，使机器人能在零样本条件下适应新的夹爪配置。&lt;h4&gt;背景&lt;/h4&gt;当前大多数抓取放置策略需要在训练和推理阶段保持一致的夹爪设置，这会导致高昂的成本，特别是在使用模仿学习方法时。当要适应新类型的末端执行器（即夹爪）时，这一问题尤为突出。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的策略以减少为适应不同夹爪而进行额外训练或微调的需求。&lt;h4&gt;方法&lt;/h4&gt;利用基于扩散的优化策略，在推理阶段动态地强制执行机械和安全约束。通过这种受限的去噪过程，该策略能够根据具体的夹爪参数（如工具中心点偏移量、颚宽）调整轨迹，同时确保碰撞避免和任务可行性。&lt;h4&gt;主要发现&lt;/h4&gt;在六种不同的夹爪配置上进行实验验证后，提出的方法实现了93.3%的平均任务成功率，而扩散政策基线方法的成功率仅为23.3-26.7%。该策略支持工具中心点偏移量从16至23.5厘米以及颚宽从7.5到11.5厘米的变化。&lt;h4&gt;结论&lt;/h4&gt;通过引入受限的扩散过程，可以实现跨夹爪操作的鲁棒性，并且保持了模仿学习方法的样本效率。这消除了针对特定夹爪进行重新训练的需求。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的具体中文翻译：当前抓取放置策略通常需要在训练和推理阶段维持一致的机械臂末端执行器设置，这一要求导致了高成本的再训练或微调需求，特别是在基于模仿学习的方法中。为解决这个问题，我们提出了一种扩散式的策略结合混合学习优化框架，使得机器人可以无须额外的数据收集便能在新的夹爪上进行零样本适应。在训练过程中，该政策通过使用基础夹爪采集的演示数据来学习抓取和放置的基本操作方法。而在推理阶段，基于扩散的优化策略动态地施加机械和安全约束，确保生成的动作轨迹与未见过的新夹爪的实际物理特性相匹配。这一过程通过一个受限去噪程序实现，该程序能够适应特定夹爪参数（例如工具中心点偏移量、颚宽）的同时保持碰撞避免和任务可行性。我们在Franka Panda机器人上进行了一系列实验测试，在六种不同的夹爪配置中验证了我们的方法的有效性，包括3D打印的手指末端执行器、柔软的硅胶抓手以及Robotiq 2F-85夹爪等。与扩散政策基线相比，我们提出的策略达到了93.3%的平均任务成功率（相比之下基线为23.3至26.7%），支持工具中心点偏移量从16到23.5厘米和颚宽范围从7.5到11.5厘米的变化。实验结果表明，受限扩散过程能够实现跨不同夹爪配置操作的鲁棒性同时维持了模仿学习方法的样本效率，并且无需为特定类型夹爪重新训练策略。代码与视频可在https://github.com/yaoxt3/GADP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current robotic pick-and-place policies typically require consistent gripperconfigurations across training and inference. This constraint imposes highretraining or fine-tuning costs, especially for imitation learning-basedapproaches, when adapting to new end-effectors. To mitigate this issue, wepresent a diffusion-based policy with a hybrid learning-optimization framework,enabling zero-shot adaptation to novel grippers without additional datacollection for retraining policy. During training, the policy learnsmanipulation primitives from demonstrations collected using a base gripper. Atinference, a diffusion-based optimization strategy dynamically enforceskinematic and safety constraints, ensuring that generated trajectories alignwith the physical properties of unseen grippers. This is achieved through aconstrained denoising procedure that adapts trajectories to gripper-specificparameters (e.g., tool-center-point offsets, jaw widths) while preservingcollision avoidance and task feasibility. We validate our method on a FrankaPanda robot across six gripper configurations, including 3D-printed fingertips,flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusionpolicy baselines), supporting tool-center-point variations of 16-23.5 cm andjaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusionenables robust cross-gripper manipulation while maintaining the sampleefficiency of imitation learning, eliminating the need for gripper-specificretraining. Video and code are available at https://github.com/yaoxt3/GADP.</description>
      <author>example@mail.com (Xiangtong Yao, Yirui Zhou, Yuan Meng, Liangyu Dong, Lin Hong, Zitao Zhang, Zhenshan Bing, Kai Huang, Fuchun Sun, Alois Knoll)</author>
      <guid isPermaLink="false">2502.15613v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Autonomous helicopter aerial refueling: controller design and performance guarantees</title>
      <link>http://arxiv.org/abs/2502.15562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于自主直升机空中加油的控制设计方法、稳定性标准和性能界限。&lt;h4&gt;背景&lt;/h4&gt;自主空中加油由于加油机尾流的影响、接触敏感操作特性和加油管运动不确定性而变得十分困难。此外，探针位于直升机重心之外，其位置与速度对直升机姿态及其角速率非常敏感。&lt;h4&gt;目的&lt;/h4&gt;为了提高自主空中加油的性能和稳定性，提出了一种新的外环位置控制器，并使用闭环误差动力学中的极限有界性特性来提供分析保证。&lt;h4&gt;方法&lt;/h4&gt;提出了一个将探针的位置和速度纳入反馈回路的新外环位置控制器。通过在高保真UH60直升机模型中进行仿真测试，验证了新控制策略的有效性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;新的控制方法能够显著减少2-范数对接误差，与现有标准控制器相比改进了36%。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了所提出的自主空中加油控制系统在复杂和动态环境中的有效性，并为未来的应用提供了理论基础和技术支持。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中，我们提出了一种用于无人直升机空中加油的控制设计方法、稳定性标准和性能界限。自主空中加油由于受到加油机尾流影响、操作接触敏感性和加油管运动不确定性的限制而变得非常困难。探针位置远离直升机重心，其位置（速度）对直升机姿态（角速率）极其敏感。此外，为了匹配加油机的速度，直升机需要高速运行并保持特定的姿态，这使得对接更加具有挑战性。我们提出了一种新的外环位置控制器，将探针的位置和速度纳入反馈回路中。通过闭环误差动态的极限有界特性，推导了关于对接性能与加油管运动不确定性及直升机角加速度之间的关系的分析保证。在考虑风力影响的情况下，利用高保真度UH60直升机模型进行了仿真测试，以验证新方法在现实场景中的有效性。高保真度模拟显示，相比于现有标准控制器，所提出的控制策略能将2-范数对接误差减少36%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a control design methodology, stability criteria,and performance bounds for autonomous helicopter aerial refueling. Autonomousaerial refueling is particularly difficult due to the aerodynamic interactionbetween the wake of the tanker, the contact-sensitive nature of the maneuver,and the uncertainty in drogue motion. Since the probe tip is locatedsignificantly away from the helicopter's center-of-gravity, its position (andvelocity) is strongly sensitive to the helicopter's attitude (and angularrates). In addition, the fact that the helicopter is operating at high speedsto match the velocity of the tanker forces it to maintain a particularorientation, making the docking maneuver especially challenging. In this paper,we propose a novel outer-loop position controller that incorporates the probeposition and velocity into the feedback loop. The position and velocity of theprobe tip depend both on the position (velocity) and on the attitude (angularrates) of the aircraft. We derive analytical guarantees for docking performancein terms of the uncertainty of the drogue motion and the angular accelerationof the helicopter, using the ultimate boundedness property of the closed-looperror dynamics. Simulations are performed on a high-fidelity UH60 helicoptermodel with a high-fidelity drogue motion under wind effects to validate theproposed approach for realistic refueling scenarios. These high-fidelitysimulations reveal that the proposed control methodology yields an improvementof 36% in the 2-norm docking error compared to the existing standardcontroller.</description>
      <author>example@mail.com (Damsara Jayarathne, Santiago Paternain, Sandipan Mishra)</author>
      <guid isPermaLink="false">2502.15562v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Probabilistic Collision Detection for Motion Planning Under Sensing Uncertainty</title>
      <link>http://arxiv.org/abs/2502.15525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了用于机器人在非结构化环境中的运动规划的增强概率碰撞检测（PCD）方法。&lt;h4&gt;背景&lt;/h4&gt;现有的PCD方法主要使用简化的几何模型，且仅考虑位置估计误差，未充分考虑到姿态估计误差和形状精度的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种改进的方法，以提高在感知不确定性下的鲁棒性，并减少路径长度与规划时间。&lt;h4&gt;方法&lt;/h4&gt;{'要点1': '利用超二次曲面（superquadrics）进行更精确的形状近似', '要点2': '考虑位置和姿态估计误差，通过扩大每个物体的表面来封装其观察到的所有旋转副本'}&lt;h4&gt;主要发现&lt;/h4&gt;该PCD方法比现有最佳方法更接近蒙特卡洛采样的基线，并且在减少路径长度和规划时间方面分别表现出色。&lt;h4&gt;结论&lt;/h4&gt;研究证明了考虑姿态估计误差的重要性，当仅考虑位置估计误差或忽略时，在仿真中执行计划路径的碰撞概率远高于此方法。&lt;h4&gt;翻译&lt;/h4&gt;概率碰撞检测（PCD）对于操作于非结构化环境中的机器人运动规划至关重要，通过考虑到感知不确定性有助于防止损坏。现有PCD方法主要使用简化的几何模型，并且仅解决位置估计误差问题。本文提出了一种增强的PCD方法，具有两个关键改进：(a) 使用超二次曲面进行更准确的形状近似；(b) 考虑到位置和姿态估计误差以提高在感知不确定性下的鲁棒性。该方法首先为每个对象计算一个扩大的表面，该表面封装了其观察到的所有旋转副本，从而解决了姿态估计误差问题。然后将位置估计误差下的碰撞概率作为机会约束问题进行公式化，并通过超二次曲面的正常参数化求解紧致上限。结果表明，与现有最佳PCD方法相比，该方法更接近蒙特卡洛采样的基线，并且在减少路径长度和规划时间方面表现出色。一种Real2Sim管道进一步验证了考虑姿态估计误差的重要性：执行仿真中计划路径的碰撞概率仅为2%，而仅考虑位置估计误差或完全不考虑时则分别为9% 和 29%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic collision detection (PCD) is essential in motion planning forrobots operating in unstructured environments, where considering sensinguncertainty helps prevent damage. Existing PCD methods mainly used simplifiedgeometric models and addressed only position estimation errors. This paperpresents an enhanced PCD method with two key advancements: (a) usingsuperquadrics for more accurate shape approximation and (b) accounting for bothposition and orientation estimation errors to improve robustness under sensinguncertainty. Our method first computes an enlarged surface for each object thatencapsulates its observed rotated copies, thereby addressing the orientationestimation errors. Then, the collision probability under the positionestimation errors is formulated as a chance-constraint problem that is solvedwith a tight upper bound. Both the two steps leverage the recently developednormal parameterization of superquadric surfaces. Results show that our PCDmethod is twice as close to the Monte-Carlo sampled baseline as the bestexisting PCD method and reduces path length by 30% and planning time by 37%,respectively. A Real2Sim pipeline further validates the importance ofconsidering orientation estimation errors, showing that the collisionprobability of executing the planned path in simulation is only 2%, compared to9% and 29% when considering only position estimation errors or none at all.</description>
      <author>example@mail.com (Xiaoli Wang, Sipu Ruan, Xin Meng, Gregory Chirikjian)</author>
      <guid isPermaLink="false">2502.15525v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Robust 4D Radar-aided Inertial Navigation for Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2502.15452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高效且鲁棒的误差状态卡尔曼滤波器(ESKF)雷达惯性导航系统的开发方案，用于无人驾驶飞行器(UAV)，并利用毫米波(MMW)雷达提供的稳健3D测距和多普勒速度测量来增强UAV在复杂环境下的导航能力。&lt;h4&gt;背景&lt;/h4&gt;随着激光雷达和摄像头在无人机上的广泛应用，在挑战性的环境中它们可能会变得不那么有效。相反，能够提供稳健的三维测距和多普勒速度测量的4D毫米波(MMW)雷达对于空中导航来说利用不够充分。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于ESKF的方法来提高UAV利用毫米波雷达进行导航时的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点对分布雷达扫描匹配技术，以提供具有适当不确定性资格的动作约束，并结合多普勒速度测量结果紧密耦合地更新导航状态。此外还设计了一个基于关键帧的方法来对抗先前地图（如果可用的话），从而限制累积的导航误差并提供高精度的雷达辅助全局定位解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的现实世界实验验证，该提出的雷达增强惯性导航方法在准确性和鲁棒性方面都超过了现有的最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;基于毫米波雷达和惯性传感器的数据融合技术可以提高UAV在复杂环境下的导航性能，并具有广阔的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While LiDAR and cameras are becoming ubiquitous for unmanned aerial vehicles(UAVs) but can be ineffective in challenging environments, 4D millimeter-wave(MMW) radars that can provide robust 3D ranging and Doppler velocitymeasurements are less exploited for aerial navigation. In this paper, wedevelop an efficient and robust error-state Kalman filter (ESKF)-basedradar-inertial navigation for UAVs. The key idea of the proposed approach isthe point-to-distribution radar scan matching to provide motion constraintswith proper uncertainty qualification, which are used to update the navigationstates in a tightly coupled manner, along with the Doppler velocitymeasurements. Moreover, we propose a robust keyframe-based matching schemeagainst the prior map (if available) to bound the accumulated navigation errorsand thus provide a radar-based global localization solution with high accuracy.Extensive real-world experimental validations have demonstrated that theproposed radar-aided inertial navigation outperforms state-of-the-art methodsin both accuracy and robustness.</description>
      <author>example@mail.com (Jinwen Zhu, Jun Hu, Xudong Zhao, Xiaoming Lang, Yinian Mao, Guoquan Huang)</author>
      <guid isPermaLink="false">2502.15452v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Horizon Robot Manipulation Skills via Privileged Action</title>
      <link>http://arxiv.org/abs/2502.15442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结构化的框架，利用特权动作和课程学习来解决长期接触密集型任务中的强化学习挑战。&lt;h4&gt;背景&lt;/h4&gt;在处理长时序的、高维度状态空间的任务时，传统强化学习方法由于稀疏奖励导致探索效率低下且容易陷入局部最优。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的学习框架，在不依赖大量人工设计奖励或参考轨迹的情况下，使策略能够掌握长期技能。&lt;h4&gt;方法&lt;/h4&gt;在模拟环境中使用特权动作进行训练，包括放松约束条件和虚拟力等手段来增强对象交互和探索。通过课程学习逐步移除这些特权以逼近真实世界情况。&lt;h4&gt;主要发现&lt;/h4&gt;成功完成了涉及非抓取姿势物体提升的复杂多阶段长期任务，展示了方法的一般性和奖励结构的简洁性，并且在多种环境中都能达到收敛。&lt;h4&gt;结论&lt;/h4&gt;实验表明所学技能可以转移到现实世界中表现得既稳健又细腻。相比于现有方法，在这些任务上的性能更优。&lt;h4&gt;翻译&lt;/h4&gt;长时序接触密集型任务由于高维状态空间和稀疏奖励的存在，对强化学习来说是一个挑战。传统的解决方案往往陷入局部最优并且需要针对特定任务进行复杂的奖励调参。为了解决这些问题，我们提出了一种使用特权动作与课程学习相结合的方法框架。该方法通过模拟中的特权训练提升了对象交互和探索效率，并且逐步移除这些特权以适应真实环境的约束条件。最终结果表明所提出的算法能够成功完成多种复杂任务，并在不同环境中展现出多样性和鲁棒性的行为模式，证明了其有效性和普遍性。此外，现实世界实验进一步验证了学到技能的有效转移能力以及在实际应用中的卓越性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon contact-rich tasks are challenging to learn with reinforcementlearning, due to ineffective exploration of high-dimensional state spaces withsparse rewards. The learning process often gets stuck in local optimum anddemands task-specific reward fine-tuning for complex scenarios. In this work,we propose a structured framework that leverages privileged actions withcurriculum learning, enabling the policy to efficiently acquire long-horizonskills without relying on extensive reward engineering or referencetrajectories. Specifically, we use privileged actions in simulation with ageneral training procedure that would be infeasible to implement in real-worldscenarios. These privileges include relaxed constraints and virtual forces thatenhance interaction and exploration with objects. Our results successfullyachieve complex multi-stage long-horizon tasks that naturally combinenon-prehensile manipulation with grasping to lift objects from non-graspableposes. We demonstrate generality by maintaining a parsimonious reward structureand showing convergence to diverse and robust behaviors across variousenvironments. Additionally, real-world experiments further confirm that theskills acquired using our approach are transferable to real-world environments,exhibiting robust and intricate performance. Our approach outperformsstate-of-the-art methods in these tasks, converging to solutions where othersfail.</description>
      <author>example@mail.com (Xiaofeng Mao, Yucheng Xu, Zhaole Sun, Elle Miller, Daniel Layeghi, Michael Mistry)</author>
      <guid isPermaLink="false">2502.15442v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Self-Mixing Laser Interferometry for Robotic Tactile Sensing</title>
      <link>http://arxiv.org/abs/2502.15390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种利用自混合干涉仪（SMI）技术的机器人指尖，用于检测物体滑动和外部接触。研究通过实验验证了该设计的有效性，并将其与声学传感器进行了比较。&lt;h4&gt;背景&lt;/h4&gt;自混合干涉仪因其在无需物理接触的情况下探测微振动的高度敏感性而受到赞誉。在机器人领域中，微振动通常被视为物体滑动的标志，最近也被认为是外接触的重要指标。&lt;h4&gt;目的&lt;/h4&gt;展示首个采用SMI技术检测滑动和外部接触信号的机器人指尖，并比较其与声学传感器的效果。&lt;h4&gt;方法&lt;/h4&gt;通过测量控制下的振动源进行设计验证，包括封装读取电路前后的情况。然后进行了三个实验将SMI指尖与声学传感相比较。&lt;h4&gt;主要发现&lt;/h4&gt;SMI对细微滑动事件更加敏感且在背景噪声下表现出显著更高的鲁棒性&lt;h4&gt;结论&lt;/h4&gt;将自混合干涉仪集成到机器人指尖中为触觉感应提供了新的有前景的分支技术&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-mixing interferometry (SMI) has been lauded for its sensitivity indetecting microvibrations, while requiring no physical contact with its target.In robotics, microvibrations have traditionally been interpreted as a markerfor object slip, and recently as a salient indicator of extrinsic contact. Wepresent the first-ever robotic fingertip making use of SMI for slip andextrinsic contact sensing. The design is validated through measurement ofcontrolled vibration sources, both before and after encasing the readoutcircuit in its fingertip package. Then, the SMI fingertip is compared toacoustic sensing through three experiments. The results are distilled into atechnology decision map. SMI was found to be more sensitive to subtle slipevents and significantly more robust against ambient noise. We conclude thatthe integration of SMI in robotic fingertips offers a new, promising branch oftactile sensing in robotics.</description>
      <author>example@mail.com (Remko Proesmans, Ward Goossens, Lowiek Van den Stockt, Lowie Christiaen, Francis wyffels)</author>
      <guid isPermaLink="false">2502.15390v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Online Learning of Hip Exoskeleton Assistance Preferences</title>
      <link>http://arxiv.org/abs/2502.15366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;髋部外骨骼装置在各种场景中表现出色，能够适应不同用户的需求。然而，个性化调整通常需要复杂的调参过程和计算密集型算法，并且大多数现有方法不考虑用户的反馈。&lt;h4&gt;背景&lt;/h4&gt;随着技术的发展，髋部外骨骼因其适应性广、适用性强而越来越受欢迎。但是，在个性化提供帮助方面仍存在挑战，如长时间的调整过程以及缺乏用户反馈整合机制。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于快速学习用户偏好来调整个体化辅助扭矩配置的方法。&lt;h4&gt;方法&lt;/h4&gt;通过随机生成不同助行方案进行成对比较，并主动向参与者提问以收集其偏好的信息。这些反馈被集成到一个优先级学习算法中，该算法根据个人行为动态更新奖励函数并相应调整外骨骼的助力模式。&lt;h4&gt;主要发现&lt;/h4&gt;来自八位健康受试者的实验数据显示了不同的最佳扭矩配置；用户的选择在面对微调后的方案时依然保持一致；用户偏好与个体步行策略有密切联系；助行力矩不会干扰运动学关节协同作用，且参与者倾向于选择与其步态模式同步的助力。&lt;h4&gt;结论&lt;/h4&gt;这一方法简单有效地实现了快速学习用户的偏好和奖励机制，为基于奖励的人机交互奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;髋部外骨骼由于其在各种场景中的有效性以及能够适应不同用户的能力而日益流行。然而，个性化调整通常需要长时间的调优过程和计算密集型算法，并且大多数现有方法没有整合用户反馈。本文提出了一种快速学习用户对髋部外骨骼辅助偏好并据此优化助动力矩配置的新方法。通过随机生成不同的助力方案进行成对比较，并收集参与者的选择偏好的方式，研究发现不同受试者拥有各自的最优扭矩模式；用户的偏好与他们的步行策略紧密相关；且被测的力矩不会破坏关节协同运动关系，用户更倾向于那些与其步态一致的辅助力矩。这种方法为未来基于奖励的人机交互的研究提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hip exoskeletons are increasing in popularity due to their effectivenessacross various scenarios and their ability to adapt to different users.However, personalizing the assistance often requires lengthy tuning proceduresand computationally intensive algorithms, and most existing methods do notincorporate user feedback. In this work, we propose a novel approach forrapidly learning users' preferences for hip exoskeleton assistance. We performpairwise comparisons of distinct randomly generated assistive profiles, andcollect participants preferences through active querying. Users' feedback isintegrated into a preference-learning algorithm that updates its belief, learnsa user-dependent reward function, and changes the assistive torque profilesaccordingly. Results from eight healthy subjects display distinct preferredtorque profiles, and users' choices remain consistent when compared to aperturbed profile. A comprehensive evaluation of users' preferences reveals aclose relationship with individual walking strategies. The tested torqueprofiles do not disrupt kinematic joint synergies, and participants favorassistive torques that are synchronized with their movements, resulting inlower negative power from the device. This straightforward approach enables therapid learning of users preferences and rewards, grounding future studies onreward-based human-exoskeleton interaction.</description>
      <author>example@mail.com (Giulia Ramella, Auke Ijspeert, Mohamed Bouri)</author>
      <guid isPermaLink="false">2502.15366v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions</title>
      <link>http://arxiv.org/abs/2502.15336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  81 pages, submitted to a journal for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文综述了嵌入式多模态大型模型（EMLMs）的发展，包括大语言模型、大视觉模型及其他相关模型，并探讨了这些模型在感知、导航和交互等方面的应用。&lt;h4&gt;背景介绍&lt;/h4&gt;近年来，由于EMLMs具有连接感知、认知和行动的潜力，在复杂现实环境中引起了广泛关注。EMLMs试图解决大规模环境下的多种挑战，如数据多样性与质量等。&lt;h4&gt;目的陈述&lt;/h4&gt;本文旨在详细分析EMLMs的发展历程及其面临的挑战，并探讨未来的方向，强调跨模态感知、推理及动作的重要性以促进更加自主系统的发展。&lt;h4&gt;方法概述&lt;/h4&gt;文章讨论了EMLMs的演化过程，重点关注嵌入式感知、导航、交互和模拟等方面。同时，对训练与评估这些模型所使用的数据集进行了深入分析，并指出了多样化高质量数据对于有效学习的重要意义。&lt;h4&gt;主要发现&lt;/h4&gt;文中指出了当前EMLMs面临的关键挑战，包括规模性问题、泛化能力和实时决策制定等方面的难题。&lt;h4&gt;未来方向&lt;/h4&gt;文章最后概述了未来的研究方向，强调多模态感知、推理和动作的集成是推进自主系统发展的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied multimodal large models (EMLMs) have gained significant attention inrecent years due to their potential to bridge the gap between perception,cognition, and action in complex, real-world environments. This comprehensivereview explores the development of such models, including Large Language Models(LLMs), Large Vision Models (LVMs), and other models, while also examiningother emerging architectures. We discuss the evolution of EMLMs, with a focuson embodied perception, navigation, interaction, and simulation. Furthermore,the review provides a detailed analysis of the datasets used for training andevaluating these models, highlighting the importance of diverse, high-qualitydata for effective learning. The paper also identifies key challenges faced byEMLMs, including issues of scalability, generalization, and real-timedecision-making. Finally, we outline future directions, emphasizing theintegration of multimodal sensing, reasoning, and action to advance thedevelopment of increasingly autonomous systems. By providing an in-depthanalysis of state-of-the-art methods and identifying critical gaps, this paperaims to inspire future advancements in EMLMs and their applications acrossdiverse domains.</description>
      <author>example@mail.com (Shoubin Chen, Zehao Wu, Kai Zhang, Chunyu Li, Baiyang Zhang, Fei Ma, Fei Richard Yu, Qingquan Li)</author>
      <guid isPermaLink="false">2502.15336v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models</title>
      <link>http://arxiv.org/abs/2502.15252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;理解集体行人运动对于人群管理、自主导航和人机交互至关重要。本文探讨了使用序列深度学习模型，包括循环神经网络（RNN）、长短期记忆（LSTM）网络和变压器，用于多行人轨迹中的实时群体检测。&lt;h4&gt;背景&lt;/h4&gt;理解和预测人群的行为对于许多应用如安全、交通规划以及机器人与人类的互动非常重要。&lt;h4&gt;目的&lt;/h4&gt;调查并开发基于序列深度学习模型的方法来识别多个人行进动轨迹中形成的集体运动模式（例如鸟群）。&lt;h4&gt;方法&lt;/h4&gt;{'两阶段过程': '首先，使用预训练的二元分类模型进行成对行人轨迹分类；其次，利用学到的表示动态地确定多代理群体。', '使用的模型': ['循环神经网络（RNN）', '长短期记忆网络（LSTM）', '变压器']}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验结果': '所提出的方法在真实世界的群体运动数据集上进行了验证，显示了其在不同序列长度和多样化移动模式下的鲁棒性。', '准确性与稳定性': '模型可以高精度且稳定地检测行人群体，即使是在动态和嘈杂的环境中也能表现出色。', '进一步应用': '该方法被扩展以识别其他形式的集体运动，如车队和蜂群，为更全面的多代理行为分析铺平了道路。'}&lt;h4&gt;结论&lt;/h4&gt;提出的基于序列深度学习的方法在检测行人群体及其动态变化方面展示了优异的表现，并为进一步研究提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;理解集体行人的移动模式对于人群管理、自主导航和人机交互至关重要。这项工作探讨了使用包括循环神经网络（RNN）、长短期记忆（LSTM）网络和变压器在内的序列深度学习模型，来实现实时群体检测在多行人轨迹中的应用。提出的方法包含两个阶段：首先利用预训练的二元分类模型对成对的人行进动轨迹进行分类；然后使用学到的表示动态地识别多代理群体。通过真实世界人群运动数据集验证了方法的有效性，证明其具有跨变序列长度和多样移动模式的稳健性。实验结果显示，该模型能在高精度和稳定性下检测行人群体，即使在动态且嘈杂的情况下也能保持良好的表现。此外，还扩展到识别其他形式的集体运动（如车队、蜂群），为多代理行为分析铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding collective pedestrian movement is crucial for applications incrowd management, autonomous navigation, and human-robot interaction. Thispaper investigates the use of sequential deep learning models, includingRecurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, andTransformers, for real-time flock detection in multi-pedestrian trajectories.Our proposed approach consists of a two-stage process: first, a pre-trainedbinary classification model is used for pairwise trajectory classification, andsecond, the learned representations are applied to identify multi-agent flocksdynamically.  We validate our method using real-world group movement datasets,demonstrating its robustness across varying sequence lengths and diversemovement patterns. Experimental results indicate that our model consistentlydetects pedestrian flocks with high accuracy and stability, even in dynamic andnoisy environments. Furthermore, we extend our approach to identify other formsof collective motion, such as convoys and swarms, paving the way for morecomprehensive multi-agent behavior analysis.</description>
      <author>example@mail.com (Amartaivan Sanjjamts, Hiroshi Morita, Togootogtokh Enkhtogtokh)</author>
      <guid isPermaLink="false">2502.15252v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework</title>
      <link>http://arxiv.org/abs/2502.15180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新颖的框架OccProphet，用于有效和高效地学习占用预测，显著降低计算需求并提高预测精度。&lt;h4&gt;背景&lt;/h4&gt;在复杂的交通环境中预测变化对于自动驾驶的安全性至关重要。最近的进步使通过观察历史2D图像来预测驾驶环境中的未来3D占用状态成为可能。然而，高计算需求使得占用预测在训练和推理阶段效率较低，限制了其在边缘设备上的可行性。&lt;h4&gt;目的&lt;/h4&gt;提出OccProphet框架以降低占用预测的计算要求，并提高预测精度。&lt;h4&gt;方法&lt;/h4&gt;OccProphet包含三个轻量级组件：观察者、预报器和精炼器。观察者通过提出的Efficient 4D Aggregation with Tripling-Attention Fusion从3D多帧体素中提取时空特征，而预报器和精炼器则有条件地预测并细化未来的占用状态。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes、Lyft-Level5和nuScenes-Occupancy数据集上的实验结果表明OccProphet训练友好且推理效率高。与最先进的Cam4DOcc相比，OccProphet减少了58%~78%的计算成本，并提高了2.6倍的速度；此外，它还实现了4%~18%相对更高的预测精度。&lt;h4&gt;结论&lt;/h4&gt;OccProphet在保持或提高预测准确性的同时显著降低了计算需求，显示出其部署到边缘设备中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;预测复杂的交通环境中变化对于自动驾驶的安全性至关重要。最近的进步使通过观察历史2D图像来预测驾驶环境中的未来3D占用状态成为可能。然而，高计算需求使得占用预测在训练和推理阶段效率较低，限制了其在边缘设备上的可行性。在这篇论文中，我们提出了一种新颖的框架OccProphet，用于有效且高效地学习占用预测，显著降低计算需求并提高预测精度。OccProphet包含三个轻量级组件：观察者、预报器和精炼器。观察者通过提出的Efficient 4D Aggregation with Tripling-Attention Fusion从3D多帧体素中提取时空特征，而预报器和精炼器则有条件地预测并细化未来的占用状态。实验结果表明OccProphet在nuScenes、Lyft-Level5和nuScenes-Occupancy数据集上训练友好且推理效率高。与最先进的Cam4DOcc相比，OccProphet减少了58%~78%的计算成本，并提高了2.6倍的速度；此外，它还实现了4%~18%相对更高的预测精度。代码和模型可在https://github.com/JLChen-C/OccProphet上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting variations in complex traffic environments is crucial for thesafety of autonomous driving. Recent advancements in occupancy forecasting haveenabled forecasting future 3D occupied status in driving environments byobserving historical 2D images. However, high computational demands makeoccupancy forecasting less efficient during training and inference stages,hindering its feasibility for deployment on edge agents. In this paper, wepropose a novel framework, i.e., OccProphet, to efficiently and effectivelylearn occupancy forecasting with significantly lower computational requirementswhile improving forecasting accuracy. OccProphet comprises three lightweightcomponents: Observer, Forecaster, and Refiner. The Observer extractsspatio-temporal features from 3D multi-frame voxels using the proposedEfficient 4D Aggregation with Tripling-Attention Fusion, while the Forecasterand Refiner conditionally predict and refine future occupancy inferences.Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasetsdemonstrate that OccProphet is both training- and inference-friendly.OccProphet reduces 58\%$\sim$78\% of the computational cost with a 2.6$\times$speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves4\%$\sim$18\% relatively higher forecasting accuracy. Code and models arepublicly available at https://github.com/JLChen-C/OccProphet.</description>
      <author>example@mail.com (Junliang Chen, Huaiyuan Xu, Yi Wang, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2502.15180v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models</title>
      <link>http://arxiv.org/abs/2502.15119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保自动驾驶系统的安全性是当前面临的关键挑战，尤其是在处理罕见但可能造成严重后果的安全临界场景时。尽管现有研究已经探讨了生成用于自主车辆（AV）测试的安全临界场景的方法，但在将这些场景有效地融入策略学习以提升安全性能方面的工作仍相对有限。此外，开发适应自主车辆行为模式和性能瓶颈变化的训练课程也尚未得到充分探索。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统中的安全性是目前研究的重点领域之一，特别是如何处理那些虽然罕见但可能导致严重事故的安全临界场景问题。&lt;h4&gt;目的&lt;/h4&gt;提出CurricuVLM框架以解决现有方法在生成安全临界测试场景和适应自主车辆行为模式方面存在的不足。该框架利用视觉语言模型（VLM）来实现个性化课程学习，从而提升自动驾驶系统的整体性能与安全性。&lt;h4&gt;方法&lt;/h4&gt;CurricuVLM通过运用VLM的多模态理解能力分析自动驾驶代理的行为、识别其性能弱点，并动态生成量身定制的训练场景进行课程适应。通过对不安全驾驶情况及其叙述性描述进行全面分析，该框架能够深入推理评估AV的能力并确定关键行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在Waymo Open Motion数据集上，CurricuVLM在常规及安全临界情景中均优于现有的基准方法，特别是在导航成功率、行驶效率和安全性指标方面表现更优。此外，进一步的分析揭示了CurricuVLM作为一种通用方法可以与各种强化学习算法相结合以增强自动驾驶系统。&lt;h4&gt;结论&lt;/h4&gt;CurricuVLM框架通过利用视觉语言模型的独特能力来改善自主驾驶代理的安全性和整体性能，并且它可以被广泛应用于不同的强化学习环境中。此外，该框架的源代码和演示视频可在GitHub页面上获取。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要已经以中文形式呈现，无需再次翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safety in autonomous driving systems remains a critical challenge,particularly in handling rare but potentially catastrophic safety-criticalscenarios. While existing research has explored generating safety-criticalscenarios for autonomous vehicle (AV) testing, there is limited work oneffectively incorporating these scenarios into policy learning to enhancesafety. Furthermore, developing training curricula that adapt to an AV'sevolving behavioral patterns and performance bottlenecks remains largelyunexplored. To address these challenges, we propose CurricuVLM, a novelframework that leverages Vision-Language Models (VLMs) to enable personalizedcurriculum learning for autonomous driving agents. Our approach uniquelyexploits VLMs' multimodal understanding capabilities to analyze agent behavior,identify performance weaknesses, and dynamically generate tailored trainingscenarios for curriculum adaptation. Through comprehensive analysis of unsafedriving situations with narrative descriptions, CurricuVLM performs in-depthreasoning to evaluate the AV's capabilities and identify critical behavioralpatterns. The framework then synthesizes customized training scenariostargeting these identified limitations, enabling effective and personalizedcurriculum learning. Extensive experiments on the Waymo Open Motion Datasetshow that CurricuVLM outperforms state-of-the-art baselines across both regularand safety-critical scenarios, achieving superior performance in terms ofnavigation success, driving efficiency, and safety metrics. Further analysisreveals that CurricuVLM serves as a general approach that can be integratedwith various RL algorithms to enhance autonomous driving systems. The code anddemo video are available at: https://zihaosheng.github.io/CurricuVLM/.</description>
      <author>example@mail.com (Zihao Sheng, Zilin Huang, Yansong Qu, Yue Leng, Sruthi Bhavanam, Sikai Chen)</author>
      <guid isPermaLink="false">2502.15119v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories</title>
      <link>http://arxiv.org/abs/2502.15043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用扩散模型生成机器人动态可接受轨迹的方法，名为DDAT。通过在训练和推理过程中将预测投影到动态可接受流形上，该方法解决了传统扩散模型与机器人动力学方程之间不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;扩散模型因其多模态生成能力而在图像和视频创建中表现出色，并逐渐被应用于机器人研究以生成机器人运动。然而，由于其随机性质，它难以满足描述可行机器人运动的精确动态方程。&lt;h4&gt;目的&lt;/h4&gt;解决利用扩散模型生成符合动力学约束的机器人轨迹的问题，提高长时域规划性能。&lt;h4&gt;方法&lt;/h4&gt;DDAT通过迭代采样预测状态前一个状态的可达集多面体下近似，并将预测状态投影到该集合中来确保动态可接受性。这种方法减少了扩散模型需要不断重新计划的需求，从而能够进行一次性长时间范围内的轨迹规划。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架在四旋翼飞行器和各种MuJoCo环境的广泛模拟以及Unitree GO1和GO2的真实世界实验中生成了更高品质的动态可接受机器人轨迹。&lt;h4&gt;结论&lt;/h4&gt;DDAT通过在扩散模型预测过程中引入动力学可行性约束，有效地解决了利用这种随机生成方法进行精确机器人运动规划的挑战。&lt;h4&gt;翻译&lt;/h4&gt;摘要：扩散模型因其多模态生成能力而擅长创建图像和视频，在机器人研究中也越来越流行，用于生成机器人运动。然而，扩散模型的本质随机性与描述可行机器人运动的动力学方程不一致。因此，利用扩散模型生成动态可接受的机器人轨迹是一个挑战。为解决这一问题，我们引入了DDAT：适用于动态可接受轨迹的扩散策略，以使用扩散模型对黑盒机器人系统进行能够被证明是可接受的轨迹生成。如果序列中的每个状态都属于其前驱者按照机器人运动方程计算出的可达集合，则称该序列是一条动力学上可接受的轨迹。为了生成这样的轨迹，我们的扩散策略在训练和推理过程中将预测投影到动态可接受流形上来使去噪神经网络的目标与动力学可行性约束对齐。这些预测的自回归性质以及机器人动力学的黑盒特性使得这种投影非常具有挑战性。因此，我们通过迭代采样状态可达集的一个多面体下近似，并将其预测的后继投影到该集合上，来强制执行可接受性；随后将此过程重复应用于经过投影后的后续状态。这种方法生成了准确轨迹，从而消除了扩散模型不断重新计划的需求，使得一次性长时域规划成为可能。我们通过广泛的四旋翼飞行器模拟和各种MuJoCo环境中的实验以及Unitree GO1和GO2的真实世界测试来证明我们的框架能够生成更高品质的动态可接受机器人轨迹。&lt;h4&gt;关键词&lt;/h4&gt;扩散模型, 动态可行性, 机器人运动规划&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at creating images and videos thanks to theirmultimodal generative capabilities. These same capabilities have made diffusionmodels increasingly popular in robotics research, where they are used forgenerating robot motion. However, the stochastic nature of diffusion models isfundamentally at odds with the precise dynamical equations describing thefeasible motion of robots. Hence, generating dynamically admissible robottrajectories is a challenge for diffusion models. To alleviate this issue, weintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories togenerate provably admissible trajectories of black-box robotic systems usingdiffusion models. A sequence of states is a dynamically admissible trajectoryif each state of the sequence belongs to the reachable set of its predecessorby the robot's equations of motion. To generate such trajectories, ourdiffusion policies project their predictions onto a dynamically admissiblemanifold during both training and inference to align the objective of thedenoiser neural network with the dynamical admissibility constraint. Theauto-regressive nature of these projections along with the black-box nature ofrobot dynamics render these projections immensely challenging. We thus enforceadmissibility by iteratively sampling a polytopic under-approximation of thereachable set of a state onto which we project its predicted successor, beforeiterating this process with the projected successor. By producing accuratetrajectories, this projection eliminates the need for diffusion models tocontinually replan, enabling one-shot long-horizon trajectory planning. Wedemonstrate that our framework generates higher quality dynamically admissiblerobot trajectories through extensive simulations on a quadcopter and variousMuJoCo environments, along with real-world experiments on a Unitree GO1 andGO2.</description>
      <author>example@mail.com (Jean-Baptiste Bouvier, Kanghyun Ryu, Kartik Nagpal, Qiayuan Liao, Koushil Sreenath, Negar Mehr)</author>
      <guid isPermaLink="false">2502.15043v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DEFT: Differentiable Branched Discrete Elastic Rods for Modeling Furcated DLOs in Real-Time</title>
      <link>http://arxiv.org/abs/2502.15037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的框架DEFT，用于实时建模复杂的分支柔性线性物体（BDLOs），解决了机器人自动化电线束装配中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的研究已经成功地对单一线性的可变形物体进行了建模，但对于具有复杂力交互和应变传播模式的分支结构来说，这些方法难以直接适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够准确预测分支柔性线性物体动态行为的方法，并实现高效的实时计算以及规划能力。&lt;h4&gt;方法&lt;/h4&gt;DEFT结合了基于物理的模型与机器学习框架，用于建模BDLO的动力学特性、动态传播和抓取操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列现实世界的实验展示了DEFT在准确性、计算速度和泛化性方面的优越性能。&lt;h4&gt;结论&lt;/h4&gt;DEFT为复杂柔性线性物体的自动化装配提供了强大的工具，并且展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;自主电线束组装要求机器人能够高精度地操作复杂的分支电缆。现有的研究虽然对单一线性的可变形对象建模取得了一定进展，但对于具有复杂力交互和应变传播模式的分支结构来说，这些方法难以直接适用。为了解决这一挑战，本文提出了一种新的框架DEFT，该框架结合了基于物理模型的方法与机器学习技术，能够准确地模拟BDLO的动力学特性，并实现了高效的实时计算及规划能力，在一系列现实世界的实验中证明了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous wire harness assembly requires robots to manipulate complexbranched cables with high precision and reliability. A key challenge inautomating this process is predicting how these flexible and branchedstructures behave under manipulation. Without accurate predictions, it isdifficult for robots to reliably plan or execute assembly operations. Whileexisting research has made progress in modeling single-threaded DeformableLinear Objects (DLOs), extending these approaches to Branched Deformable LinearObjects (BDLOs) presents fundamental challenges. The junction points in BDLOscreate complex force interactions and strain propagation patterns that cannotbe adequately captured by simply connecting multiple single-DLO models. Toaddress these challenges, this paper presents Differentiable discrete branchedElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel frameworkthat combines a differentiable physics-based model with a learning frameworkto: 1) accurately model BDLO dynamics, including dynamic propagation atjunction points and grasping in the middle of a BDLO, 2) achieve efficientcomputation for real-time inference, and 3) enable planning to demonstratedexterous BDLO manipulation. A comprehensive series of real-world experimentsdemonstrates DEFT's efficacy in terms of accuracy, computational speed, andgeneralizability compared to state-of-the-art alternatives. Projectpage:https://roahmlab.github.io/DEFT/.</description>
      <author>example@mail.com (Yizhou Chen, Xiaoyue Wu, Yeheng Zong, Anran Li, Yuzhen Chen, Julie Wu, Bohao Zhang, Ram Vasudevan)</author>
      <guid isPermaLink="false">2502.15037v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions</title>
      <link>http://arxiv.org/abs/2502.15006v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'问题描述': '在实践中使用模型预测控制（MPC）时，满足超出预测范围的安全规范是一个常见问题。', '现有方法的局限性': '理论研究表明可以通过施加合适的终端集约束或足够长的预测范围来保证安全性。然而这些技术难以应用且很少被实际操作者采用，特别是在处理一般非线性动态系统的情况下。', '提出的解决方案': '提出了一种新的方法，通过学习一个近似的离散时间控制屏障函数，并将其融入到变分推理MPC（VIMPC）中来解决上述问题。这种方法在精确递归可行性、计算可行性和适用于‘黑盒’动力学之间做出权衡。', '改进措施': '提出了一种新的采样策略，该策略显著减少了估计的最优控制方差，并提高了采样的效率，从而可以在CPU上实现实时规划。', '性能验证': 'Neural Shield-VIMPC（NS-VIMPC）控制器在模拟和实际硬件实验中均显示出比现有基于采样的MPC控制器更高的安全性改进。特别是在成本函数设计不佳的情况下也能获得显著的安全性提升。', '技术实现': '通过学习近似离散时间控制屏障函数，并将其集成到变分推理MPC（VIMPC）框架中的方式来解决上述问题，同时引入了一种新的采样策略以优化状态约束处理。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了在实际应用中使用模型预测控制时遇到的安全性保障难题以及现有的理论方法难以实施的问题。研究提出通过结合变分推理MPC与近似离散时间控制屏障函数的学习来解决这一挑战，同时引入了新的采样策略以提高计算效率和实时规划能力。实验表明，这种新方法在实际应用中表现出色，尤其是在处理复杂动态系统时有显著的安全性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common problem when using model predictive control (MPC) in practice is thesatisfaction of safety specifications beyond the prediction horizon. Whiletheoretical works have shown that safety can be guaranteed by enforcing asuitable terminal set constraint or a sufficiently long prediction horizon,these techniques are difficult to apply and thus are rarely used bypractitioners, especially in the case of general nonlinear dynamics. To solvethis problem, we impose a tradeoff between exact recursive feasibility,computational tractability, and applicability to ''black-box'' dynamics bylearning an approximate discrete-time control barrier function andincorporating it into a variational inference MPC (VIMPC), a sampling-based MPCparadigm. To handle the resulting state constraints, we further propose a newsampling strategy that greatly reduces the variance of the estimated optimalcontrol, improving the sample efficiency, and enabling real-time planning on aCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantialsafety improvements compared to existing sampling-based MPC controllers, evenunder badly designed cost functions. We validate our approach in bothsimulation and real-world hardware experiments.</description>
      <author>example@mail.com (Ji Yin, Oswin So, Eric Yang Yu, Chuchu Fan, Panagiotis Tsiotras)</author>
      <guid isPermaLink="false">2502.15006v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Ultra-High-Frequency Harmony: mmWave Radar and Event Camera Orchestrate Accurate Drone Landing</title>
      <link>http://arxiv.org/abs/2502.14992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ACM SenSys 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了实现精确、高效和安全的无人机着陆，地面平台需要实时且准确地定位下降中的无人机，并引导它们到达指定位置。虽然毫米波（mmWave）感应与相机结合可以提高定位精度，但传统帧照相机较低的采样频率相比毫米波雷达形成了系统吞吐量瓶颈。本文通过在地面平台设置中用新型事件摄像机取代传统的帧照相机来解决这一问题，并引入了针对无人机着陆设计的高度精确且低延迟的地面对准系统mmE-Loc。&lt;h4&gt;背景&lt;/h4&gt;传统的方法结合毫米波雷达和帧照相机构建定位系统，但其较低的采样频率限制了系统的吞吐量。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的高精度、低延迟地面定位系统（mmE-Loc）用于无人机着陆，以改善现有的瓶颈问题，并提高整体性能。&lt;h4&gt;方法&lt;/h4&gt;将事件摄像机与毫米波雷达结合使用，在这种设置中，采样频率得到了统一。为了充分利用这两种模式之间的时间一致性以及空间互补性，提出了两个创新模块：时间一致性指导的协作跟踪和基于图信息自适应联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实地实验表明，mmE-Loc在定位精度和延迟方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的mmE-Loc系统能够提供更精确、低延迟的无人机着陆支持，并且在实际应用中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For precise, efficient, and safe drone landings, ground platforms shouldreal-time, accurately locate descending drones and guide them to designatedspots. While mmWave sensing combined with cameras improves localizationaccuracy, the lower sampling frequency of traditional frame cameras compared tommWave radar creates bottlenecks in system throughput. In this work, we replacethe traditional frame camera with event camera, a novel sensor that harmonizesin sampling frequency with mmWave radar within the ground platform setup, andintroduce mmE-Loc, a high-precision, low-latency ground localization systemdesigned for drone landings. To fully leverage the \textit{temporalconsistency} and \textit{spatial complementarity} between these modalities, wepropose two innovative modules, \textit{consistency-instructed collaborativetracking} and \textit{graph-informed adaptive joint optimization}, for accuratedrone measurement extraction and efficient sensor fusion. Extensive real-worldexperiments in landing scenarios from a leading drone delivery companydemonstrate that mmE-Loc outperforms state-of-the-art methods in bothlocalization accuracy and latency.</description>
      <author>example@mail.com (Haoyang Wang, Jingao Xu, Xinyu Luo, Xuecheng Chen, Ting Zhang, Ruiyang Duan, Yunhao Liu, Xinlei Chen)</author>
      <guid isPermaLink="false">2502.14992v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A novel step-by-step procedure for the kinematic calibration of robots using a single draw-wire encoder</title>
      <link>http://arxiv.org/abs/2502.14983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;机器人定位精度在进行高精度制造任务时是一个关键因素。为了有效提高机械臂的精度，校准扮演着至关重要的角色。&lt;h4&gt;背景&lt;/h4&gt;现有的文献中提出了多种机器人校准方法，这些方法使用的测量系统和识别算法差异很大。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型逐步运动学校准程序，仅使用通过拉线编码器获取的一维距离测量数据来逐次估计参数。&lt;h4&gt;方法&lt;/h4&gt;为了实现这一目标，我们推导了一种分析方法，在这种方法中，对于每个未知参数，可以找到一组校准点，其中测得的距离与预测的距离之间的差异只依赖于那个未知参数。这减少了识别过程中的计算负担，并有可能提高其精度。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和实验测试中，该策略的有效性得到了证实。结果表明，所提出的逐步校准方法为标准校准方法提供了一种实用、成本效益高且计算需求较低的替代方案。&lt;h4&gt;结论&lt;/h4&gt;这种校准方法使得机器人校准更加易于实现，从而提高了机械臂执行任务时的精度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;机器人定位精度是进行精密制造作业的关键。为提高机械臂精度，提出了一种使用拉线编码器一维距离测量数据逐步校准的新方法，并通过推导分析法验证了其有效性。该方法是一种成本效益高、计算负担小的替代方案，使机器人校准更加简便和经济。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s00170-024-13219-1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot positioning accuracy is a key factory when performing high-precisionmanufacturing tasks. To effectively improve the accuracy of a manipulator,often up to a value close to its repeatability, calibration plays a crucialrole. In the literature, various approaches to robot calibration have beenproposed, and they range considerably in the type of measurement system andidentification algorithm used. Our aim was to develop a novel step-by-stepkinematic calibration procedure - where the parameters are subsequentlyestimated one at a time - that only uses 1D distance measurement data obtainedthrough a draw-wire encoder. To pursue this objective, we derived an analyticalapproach to find, for each unknown parameter, a set of calibration points wherethe discrepancy between the measured and predicted distances only depends onthat unknown parameter. This reduces the computational burden of theidentification process while potentially improving its accuracy. Simulationsand experimental tests were carried out on a 6 degrees-of-freedom robot arm:the results confirmed the validity of the proposed strategy. As a result, theproposed step-by-step calibration approach represents a practical,cost-effective and computationally less demanding alternative to standardcalibration approaches, making robot calibration more accessible and easier toperform.</description>
      <author>example@mail.com (Giovanni Boschetti, Teresa Sinico)</author>
      <guid isPermaLink="false">2502.14983v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</title>
      <link>http://arxiv.org/abs/2502.14795v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架Humanoid-VLA，该框架整合了语言理解、自我中心场景感知和运动控制，旨在实现通用的人形机器人控制。&lt;h4&gt;背景&lt;/h4&gt;当前人形机器人的控制系统主要依赖于反应机制，并且由于数据稀缺缺乏自主互动能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决现有局限性的新方法，即Humanoid-VLA框架，以提高人形机器人在理解和执行任务时的自主性和适应性。&lt;h4&gt;方法&lt;/h4&gt;该研究通过使用非自我中心的人体运动数据集与文本描述进行语言-动作预对齐来开始。然后利用参数高效的视频条件微调技术融入自我中心视觉上下文。此外还提出了一种自监督数据增强策略，可以直接从运动数据中生成伪标注。&lt;h4&gt;主要发现&lt;/h4&gt;Humanoid-VLA框架能够通过利用大规模未标记的视频数据进行训练，提高在物体互动和环境探索任务中的情境意识能力。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，基于全身控制架构的人形机器人控制系统实现了更像人类的行为表现，具备更强适应性和智能性交互能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此JSON格式包含对摘要内容的中文总结与分类&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the limitations of current humanoid robot controlframeworks, which primarily rely on reactive mechanisms and lack autonomousinteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novelframework that integrates language understanding, egocentric scene perception,and motion control, enabling universal humanoid control. Humanoid-VLA beginswith language-motion pre-alignment using non-egocentric human motion datasetspaired with textual descriptions, allowing the model to learn universal motionpatterns and action semantics. We then incorporate egocentric visual contextthrough a parameter efficient video-conditioned fine-tuning, enablingcontext-aware motion generation. Furthermore, we introduce a self-superviseddata augmentation strategy that automatically generates pseudoannotationsdirectly derived from motion data. This process converts raw motion sequencesinto informative question-answer pairs, facilitating the effective use oflarge-scale unlabeled video data. Built upon whole-body control architectures,extensive experiments show that Humanoid-VLA achieves object interaction andenvironment exploration tasks with enhanced contextual awareness, demonstratinga more human-like capacity for adaptive and intelligent engagement.</description>
      <author>example@mail.com (Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2502.14795v2</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Design of a Visual Pose Estimation Algorithm for Moon Landing</title>
      <link>http://arxiv.org/abs/2502.14942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 8 figures, Presented in 11th Nano-Satellite Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了实现月球精确着陆，提出了一种基于地形的绝对导航算法来估计航天器的位置和姿态。&lt;h4&gt;背景&lt;/h4&gt;在月球着陆任务中，需要提高航天器导航系统的准确性以减少惯性传感器引起的漂移误差。&lt;h4&gt;目的&lt;/h4&gt;通过使用预定的陨石坑数据库对拍摄到的陨石坑进行识别与匹配，并利用这些信息来校正导航偏差，从而实现精确导航。&lt;h4&gt;方法&lt;/h4&gt;该算法采用基于影像处理和地面特征识别的技术，但为了专注于估计算法的研究，在实验中跳过了图像处理及陨石坑匹配步骤。进行了仿真实验以评估算法的准确性以及使用不同数量的陨石坑进行估算的效果。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真验证了提出的绝对导航方法的有效性和准确性，并探讨了用于估计所需的陨石坑数量的影响。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种有效的地形绝对导航解决方案，可以显著提高月球着陆任务中航天器导航的精度。&lt;h4&gt;翻译&lt;/h4&gt;为了实现月球精确着陆，需要校正惯性传感器引起的导航漂移。本研究提出了一种基于地面特征识别技术的绝对导航方法，并通过仿真验证了其有效性及准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order to make a pinpoint landing on the Moon, the spacecraft's navigationsystem must be accurate. To achieve the desired accuracy, navigational driftcaused by the inertial sensors must be corrected. One way to correct this driftis to use absolute navigation solutions. In this study, a terrain absolutenavigation method to estimate the spacecraft's position and attitude isproposed. This algorithm uses the position of the craters below the spacecraftfor estimation. Craters seen by the camera onboard the spacecraft are detectedand identified using a crater database known beforehand. In order to focus onestimation algorithms, image processing and crater matching steps are skipped.The accuracy of the algorithm and the effect of the crater number used forestimation are inspected by performing simulations.</description>
      <author>example@mail.com (Atakan Süslü, Betül Rana Kuran, Halil Ersin Söken)</author>
      <guid isPermaLink="false">2502.14942v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference</title>
      <link>http://arxiv.org/abs/2502.13502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 1 figure, 12 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型PLDR-LLM通过学习一个奇异性条件，使得在推理时可以使用能量曲率张量G_{LM}来代替生成演绎输出的深度神经网络。&lt;h4&gt;背景&lt;/h4&gt;研究探讨了一种基于幂律解码表示（Power Law Decoder Representations）的大型语言模型PLDR-LLM，并揭示了其演绎输出具有高度不变性和可泛化性，且能够通过缓存机制提高推理效率。&lt;h4&gt;目的&lt;/h4&gt;展示PLDR-LLM作为一种基础模型的特点及其在不同条件下的表现特性；探讨演绎输出不变性的原因和影响；提出一种有效的推理框架来优化模型性能。&lt;h4&gt;方法&lt;/h4&gt;构建了一个学习奇异性条件的框架，该框架允许使用能量曲率张量G_{LM}替换原有的深度神经网络进行推理，并通过缓存机制（包括KV-cache和G-cache）加速推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;演绎输出具有高度不变性，在缓存后仍保持相同均方根误差(RMSE)及行列式值，且零样本基准分数未受影响；学习到的演绎输出与使用转移初始化、随机初始化或单位张量作为常数操作符预训练模型在损失和准确性上有不同特征。&lt;h4&gt;结论&lt;/h4&gt;观察到不变性特性引入了训练和推理阶段之间的新颖不对称关系，并为PLDR-LLM提供了一个有效的训练和推理框架，该框架利用KV-cache和G-cache来优化性能。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了基于幂律解码表示的大型语言模型(PLDR-LLM)是一种基础模型，其演绎输出在小扰动下是不变张量。PLDR-LLM学习了一个奇异性条件，使得生成演绎输出的深度神经网络（如幂定律图注意力PLGA）可以在推理阶段被能量曲率张量G_{LM}所替代。我们证明了可以通过简单的实现缓存机制来提高推理时间效率，包括用于G_{LM}和KV-cache的缓存。在缓存后，演绎输出具有非常高的不变性和可泛化性（例如RMSE和行列式的值保持到15位小数）。消融研究显示，学习得到的演绎输出与使用转移初始化、随机初始化或单位张量作为常数操作符预训练模型有不同的损失和准确性特征。观察到了一个由缓存所引入的新颖不对称关系在训练和推理阶段之间。我们提出了PLDR-LLM的学习奇异性条件的共同特性，并提供了一个带有KV-cache和G-cache的有效训练和推理框架实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We show that Large Language Model from Power Law Decoder Representations(PLDR-LLM) is a foundational model whose deductive outputs are invarianttensors up to a small perturbation. PLDR-LLM learns a singularity condition forthe deductive outputs that enable the once-inferred energy-curvature tensor$\mathbf{G}_{LM}$ to replace the deep neural network of power law graphattention (PLGA) generating the deductive outputs at inference. We demonstratethat a cache for $\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented ina straightforward manner to improve the inference time. The invariance andgeneralizable nature of deductive outputs is at a very high fidelity wheredeductive outputs have same RMSE and determinant values up to 15 decimal placesafter caching, and zero-shot benchmark scores remain unchanged. Ablationstudies show that learned deductive outputs have distinct loss and accuracycharacteristics from models pretrained with transferred, randomly initializedor identity tensors as a constant tensor operator and an LLM with scaled-dotproduct attention (SDPA) is a special case of PLDR-LLM where $\mathbf{G}_{LM}$is predefined as identity. The observed invariance characteristic introduces anovel asymmetry between training and inference phases with caching. We outlineobserved common characteristics of the deductive outputs for the learnedsingularity condition. We provide an implementation of a training and inferenceframework for PLDR-LLM with KV-cache and G-cache.</description>
      <author>example@mail.com (Burc Gokden)</author>
      <guid isPermaLink="false">2502.13502v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
  <item>
      <title>Large Language-Geometry Model: When LLM meets Equivariance</title>
      <link>http://arxiv.org/abs/2502.11149v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EquiLLM的新框架，该框架旨在准确预测物理系统的3D结构和动态。EquiLLM通过整合几何感知提示、等变编码器、大型语言模型（LLMs）以及等变适配器，实现了E(3)-等方性与LLMs能力的无缝融合。&lt;h4&gt;背景&lt;/h4&gt;在科学应用中，准确预测物理系统的3D结构和动态至关重要。现有的基于几何图神经网络（GNNs）的方法虽然有效执行了E(3)等方性，但难以充分利用广泛的外部信息。直接使用大型语言模型则可以融入外部知识，但在空间推理方面缺乏保证的等方性能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架EquiLLM，以解决现有方法在预测物理系统3D结构和动态时存在的局限性和不足。&lt;h4&gt;方法&lt;/h4&gt;EquiLLM主要由四个关键组件组成：几何感知提示、等变编码器、大型语言模型（LLMs）以及等变适配器。这些组件协同工作，使LLM能够作为高级不变特征处理器，并且3D方向信息则完全由等变的编码器和适配器处理。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EquiLLM在分子动力学模拟、人类运动仿真和抗体设计方面均显著优于先前的方法，展现了其出色的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了通过融合大型语言模型与等变性特征处理技术，可以更有效地预测物理系统的3D结构及其动态。这一方法为解决实际问题提供了新的途径，并且显示出在多个领域中的广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确地预测物理系统的三维结构和动力学是科学应用中至关重要的任务。现有的基于几何图神经网络（GNNs）的方法虽然成功实现了E(3)等方性，但往往难以充分利用广泛的外部信息。而直接使用大型语言模型可以融入外部知识，但在进行空间推理时缺乏保证的等方性能力。在本文中，我们提出了EquiLLM这一新框架，它能够无缝地将E(3)-等方性和大型语言模型的能力结合起来来表示三维物理系统。具体来说，EquiLLM包括四个关键部分：几何感知提示、等变编码器、一个大型语言模型以及等变适配器。在这种情况下，通过指导性提示引导的大型语言模型可以作为一个高级不变特征处理器，而3D方向信息则完全由等方性编码器和适配器处理模块来管理。实验结果表明，在分子动力学模拟、人类运动仿真和抗体设计方面，EquiLLM方法都显著优于先前的方法，这凸显了它的泛化能力的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D structures and dynamics of physical systems iscrucial in scientific applications. Existing approaches that rely on geometricGraph Neural Networks (GNNs) effectively enforce $\mathrm{E}(3)$-equivariance,but they often fall in leveraging extensive broader information. While directapplication of Large Language Models (LLMs) can incorporate external knowledge,they lack the capability for spatial reasoning with guaranteed equivariance. Inthis paper, we propose EquiLLM, a novel framework for representing 3D physicalsystems that seamlessly integrates E(3)-equivariance with LLM capabilities.Specifically, EquiLLM comprises four key components: geometry-aware prompting,an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, theLLM guided by the instructive prompt serves as a sophisticated invariantfeature processor, while 3D directional information is exclusively handled bythe equivariant encoder and adaptor modules. Experimental results demonstratethat EquiLLM delivers significant improvements over previous methods acrossmolecular dynamics simulation, human motion simulation, and antibody design,highlighting its promising generalizability.</description>
      <author>example@mail.com (Zongzhao Li, Jiacheng Cen, Bing Su, Wenbing Huang, Tingyang Xu, Yu Rong, Deli Zhao)</author>
      <guid isPermaLink="false">2502.11149v2</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Covering a Point Set by $m$ Disks with Minimum Total Area</title>
      <link>http://arxiv.org/abs/2502.13773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 Pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器人感知中的一个常见问题是放置传感器以确保对一组资产进行稳健监控。&lt;h4&gt;目的&lt;/h4&gt;研究如何在最小化总观察区域的前提下，通过特定数量的圆盘形状传感区来监测给定的一组资产。&lt;h4&gt;方法&lt;/h4&gt;提供并分析了一个快速启发式算法，并利用该算法初始化精确的整数规划解决方案。随后对整数程序进行修改，以强制传感器之间的分离约束。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一个有效的解决策略用于优化机器人系统中传感器布局问题。&lt;h4&gt;结论&lt;/h4&gt;通过将启发式方法与改进后的整数编程相结合，可以有效地确定传感器的位置，以便在给定条件下最小化监测所需的空间区域。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一个研究方向，旨在使用圆盘形状传感区的m个传感器来稳健地监控n个资产。目标是通过确保每个资产由至少kappa(p)数量的传感器监视的同时尽量减少总观察面积。为了解决这个问题，作者提出了一种快速启发式算法，并利用该方法初始化精确整数规划解决方案，从而进一步优化传感器布局以满足分离约束条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common robotics sensing problem is to place sensors to robustly monitor aset of assets, where robustness is assured by requiring asset $p$ to bemonitored by at least $\kappa(p)$ sensors. Given $n$ assets that must beobserved by $m$ sensors, each with a disk-shaped sensing region, where shouldthe sensors be placed to minimize the total area observed? We provide andanalyze a fast heuristic for this problem. We then use the heuristic toinitialize an exact Integer Programming solution. Subsequently, we enforceseparation constraints between the sensors by modifying the integer programformulation and by changing the disk candidate set.</description>
      <author>example@mail.com (Mariem Guitouni, Chek-Manh Loi, Sándor P. Fekete, Michael Perk, Aaron T. Becker)</author>
      <guid isPermaLink="false">2502.13773v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes</title>
      <link>http://arxiv.org/abs/2502.14721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在壳式建筑工地中，通过利用标准数据集和最新的Transformer模型架构来适应点云语义分割的方法。&lt;h4&gt;背景&lt;/h4&gt;构建新的训练数据集所需的重大努力阻碍了计算机视觉研究和建筑业中的机器学习发展。传统的室内物体分割方法无法有效应对复杂结构组件的语义分割挑战。&lt;h4&gt;目的&lt;/h4&gt;解决在AEC领域中复杂结构性件分割的问题，通过利用现有的大规模室内数据集进行跨域推理，并应用迁移学习以最小化新标注数据的需求来最大化分割性能。&lt;h4&gt;方法&lt;/h4&gt;建立了通过监督训练和自定义验证数据集建立基准的方法；评估了跨域推理与大型室内数据集的使用效果；并利用转移学习策略在小规模注释下最大化语义分割性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的Transformer架构即使经过最小限度的微调也能提供有效的建筑组件分割策略，这为自动化新未见过的数据标注以及频繁出现的对象分割提供了有前景的方法。&lt;h4&gt;结论&lt;/h4&gt;利用迁移学习和现有的大规模室内数据集进行训练可以有效地解决AEC领域中复杂结构性件语义分割的问题，并且能够显著减少新的标注工作量。这种方法对于构建更大规模的训练资源具有重要意义，同时也为自动化注释新类型的数据提供了一条可行的道路。&lt;h4&gt;翻译&lt;/h4&gt;创建用于建筑工地点云语义分割的新数据集是一个劳动密集型的过程，这阻碍了计算机视觉研究和建筑业中机器学习的发展。本文探索了使用标准数据集以及最近的Transformer模型架构来处理壳式建筑工地中的问题。与通常专注于建筑物内部家具物体分割的方法不同，本研究侧重于在AEC（建筑、工程和施工）领域内复杂结构组件的语义分割挑战。我们通过监督训练建立了一个基准，并用一个自定义验证数据集进行了评估；同时利用大规模室内数据集进行跨域推理，并尝试了转移学习策略以最小化新标注数据的需求来优化性能。研究发现表明，即使是经过少量微调，预训练的Transformer架构也能提供有效的建筑组件分割策略，这为构建更大规模的训练资源以及自动化注释未见过的数据类型提供了有前景的方法和可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The significant effort required to annotate data for new training datasetshinders computer vision research and machine learning in the constructionindustry. This work explores adapting standard datasets and the latesttransformer model architectures for point cloud semantic segmentation in thecontext of shell construction sites. Unlike common approaches focused on objectsegmentation of building interiors and furniture, this study addressed thechallenges of segmenting complex structural components in Architecture,Engineering, and Construction (AEC). We establish a baseline through supervisedtraining and a custom validation dataset, evaluate the cross-domain inferencewith large-scale indoor datasets, and utilize transfer learning to maximizesegmentation performance with minimal new data. The findings indicate that withminimal fine-tuning, pre-trained transformer architectures offer an effectivestrategy for building component segmentation. Our results are promising forautomating the annotation of new, previously unseen data when creating largertraining resources and for the segmentation of frequently recurring objects.</description>
      <author>example@mail.com (Lukas Rauch, Thomas Braml)</author>
      <guid isPermaLink="false">2502.14721v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2502.14525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Yannick W\"olker and Arash Hajisafi contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的图神经网络模型DeepStateGNN，用于分析交通数据，并展示了其在预测和重建任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;传统的GNN方法将每个交通传感器视为图节点，而新提出的DeepStateGNN通过相似性标准对传感器进行聚类形成更高层次的图节点（称为深层状态节点），从而固定了图中节点的数量。&lt;h4&gt;目的&lt;/h4&gt;提出并验证一种新的用于分析大规模交通数据的高效和准确的方法DeepStateGNN。&lt;h4&gt;方法&lt;/h4&gt;DeepStateGNN根据空间接近度、功能相似性和特定条件下行为相似性对传感器进行聚类，形成深层状态节点。这种聚类方式允许动态且适应性的节点分组，并支持更快更精确的大规模交通网络分析。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，DeepStateGNN在可扩展性、训练速度和预测及重建准确性方面均优于传统方法，在大规模传感器网络中表现出色。&lt;h4&gt;结论&lt;/h4&gt;DeepStateGNN作为一种创新的图神经网络模型，在处理复杂且动态变化的大规模交通数据时具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, foranalyzing traffic data, demonstrating its efficacy in two critical tasks:forecasting and reconstruction. Unlike typical GNN methods that treat eachtraffic sensor as an individual graph node, DeepStateGNN clusters sensors intohigher-level graph nodes, dubbed Deep State Nodes, based on various similaritycriteria, resulting in a fixed number of nodes in a Deep State graph. The term"Deep State" nodes is a play on words, referencing hidden networks of powerthat, like these nodes, secretly govern traffic independently of visiblesensors. These Deep State Nodes are defined by several similarity factors,including spatial proximity (e.g., sensors located nearby in the road network),functional similarity (e.g., sensors on similar types of freeways), andbehavioral similarity under specific conditions (e.g., traffic behavior duringrain). This clustering approach allows for dynamic and adaptive node grouping,as sensors can belong to multiple clusters and clusters may evolve over time.Our experimental results show that DeepStateGNN offers superior scalability andfaster training, while also delivering more accurate results than competitors.It effectively handles large-scale sensor networks, outperforming other methodsin both traffic forecasting and reconstruction accuracy.</description>
      <author>example@mail.com (Yannick Wölker, Arash Hajisafi, Cyrus Shahabi, Matthias Renz)</author>
      <guid isPermaLink="false">2502.14525v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.14235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D场景重建对于创建逼真的自动驾驶模拟环境至关重要。随着3DGaussian Splatting（3DGS）技术的进步，先前的研究已经将其应用于复杂动态驾驶场景的重建。然而，这些方法通常需要昂贵的LiDAR传感器和预注释的数据集来处理动态对象。为了解决这些问题，我们提出了一种新的方法OG-Gaussian。&lt;h4&gt;背景&lt;/h4&gt;准确且真实的3D场景重建对于创建逼真的自动驾驶模拟环境至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术对昂贵设备和复杂手动标注的依赖，研究者提出了一个新框架来优化动态驾驶场景的三维重建过程。&lt;h4&gt;方法&lt;/h4&gt;我们提出了一种新的方法OG-Gaussian，该方法使用来自周围视图相机图像生成的Occupancy Grids（OGs）代替LiDAR点云。通过在ONet的帮助下预测占位网格中的语义信息，我们的方法能够将动态车辆与静态街道背景分离，并将其转换为用于重建静态和动态对象的初始点云集合。此外，我们采用基于学习的方法来估计动态物体的轨迹和姿态。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，OG-Gaussian在Waymo Open数据集上，在重建质量和渲染速度方面与当前最先进技术相当，平均PSNR达到35.13，渲染速率达到143 FPS。同时，我们的方法显著降低了计算成本和经济负担。&lt;h4&gt;结论&lt;/h4&gt;提出的OG-Gaussian提供了一种高效且经济的方法来实现高质量的动态驾驶场景三维重建。&lt;h4&gt;翻译&lt;/h4&gt;准确且真实的3D场景重建对于创建逼真的自动驾驶模拟环境至关重要。随着3DGaussian Splatting（3DGS）技术的进步，先前的研究已经将其应用于复杂动态驾驶场景的重建。然而，这些方法通常需要昂贵的LiDAR传感器和预注释的数据集来处理动态对象。为了解决这些问题，我们提出了一种新的方法OG-Gaussian，该方法使用来自周围视图相机图像生成的Occupancy Grids（OGs）代替LiDAR点云，并在ONet的帮助下预测占位网格中的语义信息，将动态车辆与静态街道背景分离并转换为用于重建的初始点云集合。此外，我们采用基于学习的方法来估计动态物体的轨迹和姿态。实验结果表明，在Waymo Open数据集上，OG-Gaussian在重建质量和渲染速度方面表现优异（平均PSNR：35.13；FPS：143），同时显著降低了计算成本和经济负担。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and realistic 3D scene reconstruction enables the lifelike creationof autonomous driving simulation environments. With advancements in 3D GaussianSplatting (3DGS), previous studies have applied it to reconstruct complexdynamic driving scenes. These methods typically require expensive LiDAR sensorsand pre-annotated datasets of dynamic objects. To address these challenges, wepropose OG-Gaussian, a novel approach that replaces LiDAR point clouds withOccupancy Grids (OGs) generated from surround-view camera images usingOccupancy Prediction Network (ONet). Our method leverages the semanticinformation in OGs to separate dynamic vehicles from static street background,converting these grids into two distinct sets of initial point clouds forreconstructing both static and dynamic objects. Additionally, we estimate thetrajectories and poses of dynamic objects through a learning-based approach,eliminating the need for complex manual annotations. Experiments on Waymo Opendataset demonstrate that OG-Gaussian is on par with the currentstate-of-the-art in terms of reconstruction quality and rendering speed,achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, whilesignificantly reducing computational costs and economic overhead.</description>
      <author>example@mail.com (Yedong Shen, Xinran Zhang, Yifan Duan, Shiqi Zhang, Heng Li, Yilong Wu, Jianmin Ji, Yanyong Zhang)</author>
      <guid isPermaLink="false">2502.14235v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>DAG: Deep Adaptive and Generative $K$-Free Community Detection on Attributed Graphs</title>
      <link>http://arxiv.org/abs/2502.14294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by SIGKDD 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于社区检测的新模型DAG，该模型能够自动寻找最佳的社团数量，并无需人工指定社团的数量。通过在节点表示学习、社区隶属度读取和社团数量搜索三个模块的设计上进行创新，实现了端到端的学习过程。&lt;h4&gt;背景&lt;/h4&gt;带有丰富语义和拓扑信息的图上的社区检测对于现实世界网络分析尤其是在线游戏中的用户匹配具有巨大潜力。然而，在现有的深度图聚类方法中，确定最优社区数量需要昂贵的人工成本以及可能侵犯隐私的数据获取方式。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需事先指定社团数量K的方法来解决社区发现问题，即K-Free Community Detection问题。&lt;h4&gt;方法&lt;/h4&gt;设计了Deep Adaptive and Generative (DAG)模型。该模型包含三个关键组成部分：带掩码属性重建的节点表示学习模块、社区隶属度读取模块以及采用群稀疏策略寻找社团数量的搜索模块。通过这些创新，实现了端到端同时进行社区检测和社团数量搜索。&lt;h4&gt;主要发现&lt;/h4&gt;提出的DAG模型在五个公共数据集及一个真实在线手游数据集中表现出了优越性。特别是在腾讯的一个游戏中，相对于最佳竞争方法，DAG提高了7.35%的团队效率。&lt;h4&gt;结论&lt;/h4&gt;通过创新性的算法设计，所提出的方法克服了传统社区检测面临的参数选择难题，并展现了实际应用中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;带有丰富语义和拓扑信息的图上的社区发现为现实世界网络分析提供了巨大潜力，特别是在在线游戏用户匹配方面。近年来，图神经网络（GNN）使深度图聚类方法能够从语义和拓扑信息中学习社团分配。然而，其成功依赖于关于社群数量K的先验知识，这在获取成本高昂以及存在隐私问题的情况下是不现实的。本文探讨了无需先验知识确定社区数量的情况下的社区发现问题，并提出了一种新颖的深度自适应与生成模型（DAG），该模型能够在不指定社团数量的前提下进行社区检测。此外还设计了一个新指标EDGE，用于在标签难以获取的真实世界应用中评估社区检测方法的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Community detection on attributed graphs with rich semantic and topologicalinformation offers great potential for real-world network analysis, especiallyuser matching in online games. Graph Neural Networks (GNNs) have recentlyenabled Deep Graph Clustering (DGC) methods to learn cluster assignments fromsemantic and topological information. However, their success depends on theprior knowledge related to the number of communities $K$, which is unrealisticdue to the high costs and privacy issues of acquisition.In this paper, weinvestigate the community detection problem without prior $K$, referred to as$K$-Free Community Detection problem. To address this problem, we propose anovel Deep Adaptive and Generative model~(DAG) for community detection withoutspecifying the prior $K$. DAG consists of three key components, \textit{i.e.,}a node representation learning module with masked attribute reconstruction, acommunity affiliation readout module, and a community number search module withgroup sparsity. These components enable DAG to convert the process ofnon-differentiable grid search for the community number, \textit{i.e.,} adiscrete hyperparameter in existing DGC methods, into a differentiable learningprocess. In such a way, DAG can simultaneously perform community detection andcommunity number search end-to-end. To alleviate the cost of acquiringcommunity labels in real-world applications, we design a new metric, EDGE, toevaluate community detection methods even when the labels are not feasible.Extensive offline experiments on five public datasets and a real-world onlinemobile game dataset demonstrate the superiority of our DAG over the existingstate-of-the-art (SOTA) methods. DAG has a relative increase of 7.35\% in teamsin a Tencent online game compared with the best competitor.</description>
      <author>example@mail.com (Chang Liu, Yuwen Yang, Yue Ding, Hongtao Lu, Wenqing Lin, Ziming Wu, Wendong Bi)</author>
      <guid isPermaLink="false">2502.14294v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis</title>
      <link>http://arxiv.org/abs/2502.14807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为FetalCLIP的视觉-语言基础模型，该模型能够生成胎儿超声图像的通用表示。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在医疗领域变得越来越有效，但胎儿超声图像仍然是一个具有挑战性的领域，因为它们自身复杂性高，并且缺乏配对的多模态数据。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，引入了FetalCLIP这一视觉-语言基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用包含210,035张胎儿超声图像与文本配对的大规模多样本数据集进行预训练。这是迄今为止用于基础模型开发的最大规模的配对数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的基准测试中，FetalCLIP在分类、妊娠年龄估计、先天性心脏病检测和胎儿结构分割等关键胎儿超声应用中表现优于所有基线，并且展示了出色的泛化能力，在有限标签数据的情况下仍表现出色。&lt;h4&gt;结论&lt;/h4&gt;计划公开发布FetalCLIP模型以造福更广泛的科学界。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在医疗领域的有效性正在不断提升，提供了可以轻松适应下游任务的大型数据集上的预训练模型。尽管取得了进展，胎儿超声图像仍然是一个具有挑战性的领域，因为它们固有的复杂性通常需要大量的额外训练，并且由于多模态配对数据的缺乏而面临限制。为了克服这些挑战，我们引入了FetalCLIP这一视觉-语言基础模型，它可以生成胎儿超声图像的通用表示。FetalCLIP使用包含210,035张胎儿超声图像与文本配对的大规模多样本数据集进行预训练。这是迄今为止用于基础模型开发的最大规模的配对数据集。这种独特的培训方法使FetalCLIP能够有效地学习胎儿超声图像中复杂的解剖特征，从而产生可用于多种下游应用的强大表示。在广泛的基准测试中，包括分类、妊娠年龄估计、先天性心脏病(CHD)检测和胎儿结构分割等关键胎儿超声应用，FetalCLIP的表现优于所有基线，并且展示了出色的泛化能力，在有限标签数据的情况下仍表现出色。我们计划公开发布FetalCLIP模型以造福更广泛的科学界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are becoming increasingly effective in the medical domain,offering pre-trained models on large datasets that can be readily adapted fordownstream tasks. Despite progress, fetal ultrasound images remain achallenging domain for foundation models due to their inherent complexity,often requiring substantial additional training and facing limitations due tothe scarcity of paired multimodal data. To overcome these challenges, here weintroduce FetalCLIP, a vision-language foundation model capable of generatinguniversal representation of fetal ultrasound images. FetalCLIP was pre-trainedusing a multimodal learning approach on a diverse dataset of 210,035 fetalultrasound images paired with text. This represents the largest paired datasetof its kind used for foundation model development to date. This unique trainingapproach allows FetalCLIP to effectively learn the intricate anatomicalfeatures present in fetal ultrasound images, resulting in robustrepresentations that can be used for a variety of downstream applications. Inextensive benchmarking across a range of key fetal ultrasound applications,including classification, gestational age estimation, congenital heart defect(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed allbaselines while demonstrating remarkable generalizability and strongperformance even with limited labeled data. We plan to release the FetalCLIPmodel publicly for the benefit of the broader scientific community.</description>
      <author>example@mail.com (Fadillah Maani, Numan Saeed, Tausifa Saleem, Zaid Farooq, Hussain Alasmawi, Werner Diehl, Ameera Mohammad, Gareth Waring, Saudabi Valappi, Leanne Bricker, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2502.14807v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks</title>
      <link>http://arxiv.org/abs/2502.14158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WWW25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为SMILE的方法，用于在少量任务的情况下进行图的少样本学习。该方法通过双层mixup策略增强了元学习中的节点和任务多样性，并利用了图中节点度数提供的先验信息。&lt;h4&gt;背景&lt;/h4&gt;当前领先的图形模型需要大量的标记样本以避免在少样本场景下出现过拟合的情况，而最近的研究试图通过结合图学习与元学习来缓解这一问题。然而，这样的假设可能不现实，因为构建任务和涉及的成本很高。&lt;h4&gt;目的&lt;/h4&gt;提出一个简单有效的方法（SMILE），用于解决图形元学习中需要大量任务的问题，并在少样本场景下提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入双层mixup策略，包括在同一任务内的以及跨任务的混合技术。此外，利用图中的节点度数信息来编码更表达式的节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了SMILE能够增强模型的泛化能力；实验上，在所有评估数据集（无论是领域内还是跨域）中，SMILE均以显著优势超过了其他竞争性模型。&lt;h4&gt;结论&lt;/h4&gt;SMILE是一种简单有效的解决图形少样本学习问题的方法，通过丰富元学习中的节点和任务多样性，并利用图结构的先验信息来提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已经被证明可以在网络上有效地学习图数据并从中挖掘内容。然而，现有的领先图形模型需要大量标记样本以避免在少量样本场景下的过拟合问题。最近的研究试图通过结合图学习与元学习来缓解这一问题。但是，这样的假设可能不现实，因为构建任务和涉及的成本很高。因此，我们提出了一种称为SMILE的方法，用于解决图形少样本学习中的任务数量较少的问题。该方法引入了双层mixup策略，在丰富元学习中可获得的节点和任务的同时，利用图结构提供的先验信息（即节点度数），以编码表达式的节点表示。理论上证明了SMILE可以提高模型的泛化能力；在实验上，SMILE在所有评估的数据集下均优于其他竞争性模型。我们的匿名代码可以在给定链接处找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714905&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have been demonstrated as a powerful paradigm foreffectively learning graph-structured data on the web and mining content fromit.Current leading graph models require a large number of labeled samples fortraining, which unavoidably leads to overfitting in few-shot scenarios. Recentresearch has sought to alleviate this issue by simultaneously leveraging graphlearning and meta-learning paradigms. However, these graph meta-learning modelsassume the availability of numerous meta-training tasks to learn transferablemeta-knowledge. Such assumption may not be feasible in the real world due tothe difficulty of constructing tasks and the substantial costs involved.Therefore, we propose a SiMple yet effectIve approach for graph few-shotLearning with fEwer tasks, named SMILE. We introduce a dual-level mixupstrategy, encompassing both within-task and across-task mixup, tosimultaneously enrich the available nodes and tasks in meta-learning. Moreover,we explicitly leverage the prior information provided by the node degrees inthe graph to encode expressive node representations. Theoretically, wedemonstrate that SMILE can enhance the model generalization ability.Empirically, SMILE consistently outperforms other competitive models by a largemargin across all evaluated datasets with in-domain and cross-domain settings.Our anonymous code can be found here.</description>
      <author>example@mail.com (Yonghao Liu, Mengyu Li, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan)</author>
      <guid isPermaLink="false">2502.14158v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera</title>
      <link>http://arxiv.org/abs/2502.14503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Robotics and Automation Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了LXLv2，改进了之前最先进的4D雷达相机融合的3D目标检测方法（LXL），通过新的深度监督策略和基于通道与空间注意力机制的融合模块，提升了模型的准确性、速度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的LXL方法利用预测的图像深度分布图和雷达三维占用网格来辅助样本基础的图像视角转换。然而，该方法在深度预测的准确性和一致性上存在不足，并且基于拼接的融合方式影响了模型的健壮性。&lt;h4&gt;目的&lt;/h4&gt;改进现有技术中的缺陷，提高3D目标检测的精度、速度及鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;[{'一到多深度监督策略': '该策略通过雷达点的位置误差进行设计，并利用雷达散射截面（RCS）值调整监管区域以增强对象级别的深度一致性。'}, {'CSAFusion模块': '引入了基于通道和空间注意机制的融合模块，用于提高特征适应性。'}]&lt;h4&gt;主要发现&lt;/h4&gt;['在View-of-Delft和TJ4DRadSet数据集上的实验结果表明LXLv2优于原始方法LXL，在检测精度、推断速度以及鲁棒性方面均表现出显著优势。', '模型的改进策略有效提升了3D目标检测任务的整体性能。']&lt;h4&gt;结论&lt;/h4&gt;通过改进深度预测和特征融合机制，可以有效地解决现有雷达-相机融合技术中的问题，并提高其在复杂场景下的应用效果。&lt;h4&gt;翻译&lt;/h4&gt;作为之前的最先进的4D雷达-相机融合基础的3D对象检测方法LXL使用了预测的图像深度分布图和雷达三维占用网格来辅助基于采样的图像视角转换。然而，深度预测缺乏准确性和一致性，而LXL中的基于拼接的融合方式阻碍了模型的鲁棒性。在这项工作中，我们提出了LXLv2，在该版本中进行了修改以克服限制并提高性能。具体来说，考虑到雷达测量的位置误差，我们设计了一种通过雷达点实现的一对多深度监督策略，并利用雷达散射截面（RCS）值进一步调整监管区域以增强对象级别的深度一致性。此外，引入了一个名为CSAFusion的通道和空间注意机制融合模块来提高特征适应性。在View-of-Delft和TJ4DRadSet数据集上的实验结果表明所提出的LXLv2能够优于原始方法LXL，在检测精度、推理速度和鲁棒性方面都有改进，显示了模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3536840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the previous state-of-the-art 4D radar-camera fusion-based 3D objectdetection method, LXL utilizes the predicted image depth distribution maps andradar 3D occupancy grids to assist the sampling-based image viewtransformation. However, the depth prediction lacks accuracy and consistency,and the concatenation-based fusion in LXL impedes the model robustness. In thiswork, we propose LXLv2, where modifications are made to overcome thelimitations and improve the performance. Specifically, considering the positionerror in radar measurements, we devise a one-to-many depth supervision strategyvia radar points, where the radar cross section (RCS) value is furtherexploited to adjust the supervision area for object-level depth consistency.Additionally, a channel and spatial attention-based fusion module namedCSAFusion is introduced to improve feature adaptiveness. Experimental resultson the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 canoutperform LXL in detection accuracy, inference speed and robustness,demonstrating the effectiveness of the model.</description>
      <author>example@mail.com (Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu)</author>
      <guid isPermaLink="false">2502.14503v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors</title>
      <link>http://arxiv.org/abs/2502.14627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多语言音频-文本检索（ML-ATR）方案，旨在解决跨语言实例相似性匹配的不一致性问题。&lt;h4&gt;背景&lt;/h4&gt;现有的ML-ATR方案在跨语言相似性匹配中存在不一致性问题。这些问题源于跨模态对齐方向误差和权重误差。&lt;h4&gt;目的&lt;/h4&gt;理论分析了权重误差上限，并提出了一种基于1-to-k对比学习和音频-英语共锚对比学习的一致性ML-ATR方案，以减轻数据分布误差对召回率和一致性的负面影响。&lt;h4&gt;方法&lt;/h4&gt;使用1-to-k对比学习和音频-英语共锚对比学习来解决多语言模态对齐问题，并通过理论分析确定权重误差上限。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在翻译的AudioCaps和Clotho数据集上，该方案在八种主流语言（包括英语）上的召回率和一致性指标方面达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的ML-ATR方案有效提高了多语言音频-文本检索的一致性和召回率，并且源代码可在指定网址获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims toretrieve audio clips or multilingual texts from databases. However, existingML-ATR schemes suffer from inconsistencies for instance similarity matchingacross languages. We theoretically analyze the inconsistency in terms of bothmultilingual modal alignment direction error and weight error, and propose thetheoretical weight error upper bound for quantifying the inconsistency. Basedon the analysis of the weight error upper bound, we find that the inconsistencyproblem stems from the data distribution error caused by random sampling oflanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastivelearning and audio-English co-anchor contrastive learning, aiming to mitigatethe negative impact of data distribution error on recall and consistency inML-ATR. Experimental results on the translated AudioCaps and Clotho datasetsshow that our scheme achieves state-of-the-art performance on recall andconsistency metrics for eight mainstream languages, including English. Our codewill be available at https://github.com/ATRI-ACL/ATRI-ACL.</description>
      <author>example@mail.com (Yuguo Yin, Yuxin Xie, Wenyuan Yang, Dongchao Yang, Jinghan Ru, Xianwei Zhuang, Liming Liang, Yuexian Zou)</author>
      <guid isPermaLink="false">2502.14627v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2502.14189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了一种名为QUAD-LLM-MLTC的方法，该方法使用四种大型语言模型（LLMs）来处理多标签文本分类问题，尤其是在医疗领域的应用。&lt;h4&gt;背景描述&lt;/h4&gt;随着医疗领域内收集到的文本数据量不断增加，传统的机器学习模型难以有效应对这种大规模且复杂的数据集。此外，标记化的训练样本稀缺且具有复杂的语义特征。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种利用大型语言模型（LLMs）进行多标签文本分类的新方法，并证明这种方法在医疗文本分类上的有效性。&lt;h4&gt;采用的方法&lt;/h4&gt;引入了QUAD-LLM-MLTC框架，该框架结合了四个不同的预训练模型：BERT、PEGASUS、GPT-4o和BART。这些模型分别用于提取关键词、增强数据、执行分类以及提供标签概率。&lt;h4&gt;实验结果&lt;/h4&gt;通过三个标注文本样本的评估对比传统方法与单一模型的方法，结果显示在F1分数和一致性上均有显著提高（分别为78.17%和80.16%，标准偏差为0.025和0.011）。&lt;h4&gt;结论&lt;/h4&gt;此研究展示了大型语言模型在多标签文本分类上的强大潜力，并提出了一种无需额外训练即可快速准确地处理医疗领域内海量数据的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The escalating volume of collected healthcare textual data presents a uniquechallenge for automated Multi-Label Text Classification (MLTC), which isprimarily due to the scarcity of annotated texts for training and their nuancednature. Traditional machine learning models often fail to fully capture thearray of expressed topics. However, Large Language Models (LLMs) havedemonstrated remarkable effectiveness across numerous Natural LanguageProcessing (NLP) tasks in various domains, which show impressive computationalefficiency and suitability for unsupervised learning through promptengineering. Consequently, these LLMs promise an effective MLTC of medicalnarratives. However, when dealing with various labels, different prompts can berelevant depending on the topic. To address these challenges, the proposedapproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in whichBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, andBART provides topics' assignment probabilities, which results in fourclassifications, all in a 0-shot setting. The outputs are then combined usingensemble learning and processed through a meta-classifier to produce the finalMLTC result. The approach is evaluated using three samples of annotated texts,which contrast it with traditional and single-model methods. The results showsignificant improvements across the majority of the topics in theclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and80.16% with standard deviations of 0.025 and 0.011, respectively). Thisresearch advances MLTC using LLMs and provides an efficient and scalablesolution to rapidly categorize healthcare-related text data without furthertraining.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2502.14189v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Spiking Neural Networks from an Ensemble Learning Perspective</title>
      <link>http://arxiv.org/abs/2502.14218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;该论文提出了一种新的方法来提高脉冲神经网络（SNNs）的性能。&lt;h4&gt;背景信息&lt;/h4&gt;SNNs具有较高的能源效率，但存在性能不足的问题。研究者发现时间步长之间的初始状态差异过大是导致不稳定输出和性能下降的关键原因。&lt;h4&gt;主要目的&lt;/h4&gt;通过减少初始膜电位分布及其输出在不同时间步上的不一致性来提高SNN的稳定性和整体性能。&lt;h4&gt;提出方法&lt;/h4&gt;{'膜电位平滑': '一种改进措施，旨在促进信息向前传播并解决时间梯度消失问题。', '相邻子网络指导': '另一种策略，通过调整相邻的时间子网之间的关系以进一步提升稳定性。'}&lt;h4&gt;主要发现&lt;/h4&gt;该技术只需对脉冲神经元进行轻微修改而不需改变网络结构，并在1D语音、2D物体和3D点云识别任务中展示了稳定且一致的性能改进。&lt;h4&gt;具体成就&lt;/h4&gt;在CIFAR10-DVS数据集上，仅使用四个时间步就达到了83.20%的准确率。&lt;h4&gt;结论与意义&lt;/h4&gt;该研究为释放SNNs潜力提供了宝贵见解，并可能对神经形态计算领域产生深远影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks (SNNs) exhibit superior energy efficiency but sufferfrom limited performance. In this paper, we consider SNNs as ensembles oftemporal subnetworks that share architectures and weights, and highlight acrucial issue that affects their performance: excessive differences in initialstates (neuronal membrane potentials) across timesteps lead to unstablesubnetwork outputs, resulting in degraded performance. To mitigate this, wepromote the consistency of the initial membrane potential distribution andoutput through membrane potential smoothing and temporally adjacent subnetworkguidance, respectively, to improve overall stability and performance. Moreover,membrane potential smoothing facilitates forward propagation of information andbackward propagation of gradients, mitigating the notorious temporal gradientvanishing problem. Our method requires only minimal modification of the spikingneurons without adapting the network structure, making our method generalizableand showing consistent performance gains in 1D speech, 2D object, and 3D pointcloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset,we achieved 83.20\% accuracy with only four timesteps. This provides valuableinsights into unleashing the potential of SNNs.</description>
      <author>example@mail.com (Yongqi Ding, Lin Zuo, Mengmeng Jing, Pei He, Hanpu Deng)</author>
      <guid isPermaLink="false">2502.14218v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Harnessing PDF Data for Improving Japanese Large Multimodal Models</title>
      <link>http://arxiv.org/abs/2502.14778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用日本PDF数据训练大模态模型（LMM）以提高其在日本语言环境中的效果。&lt;h4&gt;背景&lt;/h4&gt;当前的大模态模型在英语中表现出色，但在日语环境中由于高质量训练数据的缺乏，性能受限。现有的日语大模态模型往往依赖于翻译后的英文数据集，限制了它们捕捉日本特定文化知识的能力。&lt;h4&gt;目的&lt;/h4&gt;探索利用日本PDF文档作为训练资源的可能性，并构建一个自动化流程来从这些文档中提取图像-文本对以丰富日语大模态模型的训练数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一个完全自动化的管道，该管道使用预训练模型通过版面分析、OCR和视觉语言配对技术从PDF文件中抽取图像-文本对。此外，还构建了指令数据集来进一步增强训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，基于PDF的数据显著提高了日语大模态模型的性能，在Heron-Bench上的表现提升了3.9%到13.8%不等。&lt;h4&gt;结论&lt;/h4&gt;PDF文档作为多模态资源对各种因素（如模型大小和语言模型）的影响得到了强化，证明了其在提高日语文本模型性能方面的价值。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要描述了一个旨在提升日本大模态模型训练效果的研究项目。该项目探索利用未充分利用的日本PDF数据集，并开发了一种自动化流程来从这些文档中提取高质量的数据对进行模型训练。研究结果表明，这种方法显著改善了日语大模态模型在特定基准上的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) have demonstrated strong performance inEnglish, but their effectiveness in Japanese remains limited due to the lack ofhigh-quality training data. Current Japanese LMMs often rely on translatedEnglish datasets, restricting their ability to capture Japan-specific culturalknowledge. To address this, we explore the potential of Japanese PDF data as atraining resource, an area that remains largely underutilized. We introduce afully automated pipeline that leverages pretrained models to extract image-textpairs from PDFs through layout analysis, OCR, and vision-language pairing,removing the need for manual annotation. Additionally, we construct instructiondata from extracted image-text pairs to enrich the training data. To evaluatethe effectiveness of PDF-derived data, we train Japanese LMMs and assess theirperformance on the Japanese LMM Benchmark. Our results demonstrate substantialimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.Further analysis highlights the impact of PDF-derived data on various factors,such as model size and language models, reinforcing its value as a multimodalresource for Japanese LMMs. We plan to make the source code and data publiclyavailable upon acceptance.</description>
      <author>example@mail.com (Jeonghun Baek, Akiko Aizawa, Kiyoharu Aizawa)</author>
      <guid isPermaLink="false">2502.14778v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains</title>
      <link>http://arxiv.org/abs/2502.14293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了AdaGraph-T3，一种针对跨领域图异常检测的测试时训练框架。&lt;h4&gt;背景&lt;/h4&gt;在新兴应用中，图结构数据中的标签异常通常稀缺，现有的监督式图异常检测方法由于分布偏移和异构特征空间的问题，在跨域情况下效果不佳或不可用。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，提出了一个新颖的测试时训练框架AdaGraph-T3，旨在解决跨领域图异常检测问题。&lt;h4&gt;方法&lt;/h4&gt;AdaGraph-T3结合了监督学习和自监督学习，并在测试阶段仅使用自监督学习进行领域适应，通过基于同质性的亲和力分数捕捉异常的域不变属性。引入四种关键创新：有效的自监督方案、基于注意力机制的学习边权重的方法、处理异构特征的特定领域的编码器以及解决不平衡问题的类别感知正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，AdaGraph-T3在多个跨领域设置下显著优于现有方法，在AUROC和AUPRC上平均分别提高了6.6%和7.9%。&lt;h4&gt;结论&lt;/h4&gt;AdaGraph-T3能够有效地应对标签稀缺的挑战，并且在不同的图结构数据异常检测任务中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Anomaly Detection (GAD) has demonstrated great effectiveness inidentifying unusual patterns within graph-structured data. However, whilelabeled anomalies are often scarce in emerging applications, existingsupervised GAD approaches are either ineffective or not applicable when movedacross graph domains due to distribution shifts and heterogeneous featurespaces. To address these challenges, we present AdaGraph-T3, a novel test-timetraining framework for cross-domain GAD. AdaGraph-T3 combines supervised andself-supervised learning during training while adapting to a new domain duringtest time using only self-supervised learning by leveraging a homophily-basedaffinity score that captures domain-invariant properties of anomalies. Ourframework introduces four key innovations to cross-domain GAD: an effectiveself-supervision scheme, an attention-based mechanism that dynamically learnsedge importance weights during message passing, domain-specific encoders forhandling heterogeneous features, and class-aware regularization to addressimbalance. Experiments across multiple cross-domain settings demonstrate thatAdaGraph-T3 significantly outperforms existing approaches, achieving averageimprovements of over 6.6% in AUROC and 7.9% in AUPRC compared to the bestcompeting model.</description>
      <author>example@mail.com (Delaram Pirhayati, Arlei Silva)</author>
      <guid isPermaLink="false">2502.14293v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2502.14495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18pages,13figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为HUCLNet的新框架，用于改进近岸区域水下目标检测（UTD）的准确性。该方法利用对比学习和自适应学习策略来提取受光谱畸变影响的数据中具有区分性的特征。&lt;h4&gt;背景&lt;/h4&gt;无人机携带的高光谱遥感技术在水下目标探测领域展现出巨大潜力，但在近岸环境中的光谱扭曲限制了其效果。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的网络框架以克服传统基于海底模型的高光谱水下目标检测方法面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为HUCLNet的新框架，该框架结合对比学习和自适应学习策略，从受畸变影响的数据中提取区分性特征。此外，还使用了可靠性引导的聚类策略来增强所学表示的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在新的近岸高光谱水下目标检测基准数据集ATR2-HUTD上进行广泛的实验表明，HUCLNet显著优于现有的先进方法。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效地提高无人机携带的高光谱遥感技术在复杂水域中进行水下目标探测的能力。&lt;h4&gt;翻译&lt;/h4&gt;无人机搭载的高光谱遥感技术已经成为一种有前景的水下目标检测（UTD）方法，但其在近岸环境中的效能受到光谱畸变的影响。这些畸变降低了传统基于海底模型的高光谱UTD方法的有效性，并增加了目标和背景光谱的不确定性。为了应对这一挑战，我们提出了一种新的框架——Hyperspectral Underwater Contrastive Learning Network (HUCLNet)，它结合了对比学习与自适应学习策略来增强近岸区域水下目标检测的鲁棒性。此外，在一系列实验中，通过一个新的基准数据集ATR2-HUTD证明了该方法的有效性，并显示出优于现有最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; UAV-borne hyperspectral remote sensing has emerged as a promising approachfor underwater target detection (UTD). However, its effectiveness is hinderedby spectral distortions in nearshore environments, which compromise theaccuracy of traditional hyperspectral UTD (HUTD) methods that rely onbathymetric model. These distortions lead to significant uncertainty in targetand background spectra, challenging the detection process. To address this, wepropose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), anovel framework that integrates contrastive learning with a self-paced learningparadigm for robust HUTD in nearshore regions. HUCLNet extracts discriminativefeatures from distorted hyperspectral data through contrastive learning, whilethe self-paced learning strategy selectively prioritizes the most informativesamples. Additionally, a reliability-guided clustering strategy enhances therobustness of learned representations.To evaluate the method effectiveness, weconduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering threediverse scenarios with varying water types and turbidity, and target types.Extensive experiments demonstrate that HUCLNet significantly outperformsstate-of-the-art methods. The dataset and code will be publicly available at:https://github.com/qjh1996/HUTD</description>
      <author>example@mail.com (Jiahao Qi, Chuanhong Zhou, Xingyue Liu, Chen Chen, Dehui Zhu, Kangcheng Bin, Ping Zhong)</author>
      <guid isPermaLink="false">2502.14495v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration</title>
      <link>http://arxiv.org/abs/2502.14156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为Mixed Signals的全面车辆到一切（V2X）数据集，该数据集旨在解决现有V2X数据集中存在的局限性问题。&lt;h4&gt;背景&lt;/h4&gt;现有的V2X数据集在范围、多样性和质量方面存在限制，这影响了单个车辆感知系统的性能。&lt;h4&gt;目的&lt;/h4&gt;为了填补这些空白，作者创建了一个名为Mixed Signals的数据集，它包含来自三辆配备两种不同类型的LiDAR传感器的连接自主汽车（CAVs）以及路边单元的数据。&lt;h4&gt;方法&lt;/h4&gt;该数据集提供了精确对齐的点云和跨越10类别的边界框注释，并且还进行了详细的统计分析和现有V2X方法的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;Mixed Signals V2X Dataset是目前公开可用的用于V2X感知研究的最高质量和大规模的数据集之一。&lt;h4&gt;结论&lt;/h4&gt;该数据集提供了高质量、多样的数据，有助于推动V2X协作感知技术的发展，并为未来的相关研究奠定了坚实的基础。&lt;h4&gt;翻译&lt;/h4&gt;车辆到一切（V2X）协作感知作为一种解决方案已出现，旨在解决单个车辆感知系统中的限制。然而，现有的V2X数据集在范围、多样性和质量方面存在局限性。为了填补这些空白，我们提出了Mixed Signals，这是一个综合的V2X数据集，其中包含45.1k点云和来自三个配备两种不同类型的LiDAR传感器的连接自主汽车（CAVs）以及一个带有双LiDAR的路边单元采集到的240.6k边界框。该数据集提供精确对齐的点云和跨越十类别的边界框注释，确保了用于感知训练的数据可靠且高质量。我们提供了关于数据集质量的详细统计分析，并对其进行了广泛的基准测试以评估现有的V2X方法。Mixed Signals V2X Dataset是目前公开可用的质量最高、规模最大的V2X感知研究数据集之一。详情请访问我们的网站 https://mixedsignalsdataset.cs.cornell.edu/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-everything (V2X) collaborative perception has emerged as apromising solution to address the limitations of single-vehicle perceptionsystems. However, existing V2X datasets are limited in scope, diversity, andquality. To address these gaps, we present Mixed Signals, a comprehensive V2Xdataset featuring 45.1k point clouds and 240.6k bounding boxes collected fromthree connected autonomous vehicles (CAVs) equipped with two different types ofLiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset providesprecisely aligned point clouds and bounding box annotations across 10 classes,ensuring reliable data for perception training. We provide detailed statisticalanalysis on the quality of our dataset and extensively benchmark existing V2Xmethods on it. Mixed Signals V2X Dataset is one of the highest quality,large-scale datasets publicly available for V2X perception research. Details onthe website https://mixedsignalsdataset.cs.cornell.edu/.</description>
      <author>example@mail.com (Katie Z Luo, Minh-Quan Dao, Zhenzhen Liu, Mark Campbell, Wei-Lun Chao, Kilian Q. Weinberger, Ezio Malis, Vincent Fremont, Bharath Hariharan, Mao Shan, Stewart Worrall, Julie Stephany Berrio Perez)</author>
      <guid isPermaLink="false">2502.14156v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies</title>
      <link>http://arxiv.org/abs/2502.14197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Anomaly Detection in Scientific Domains AAAI Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于学习时空交互的图神经网络的新方法。&lt;h4&gt;背景&lt;/h4&gt;传统的图神经网络依赖于预定义的图结构，这可能模糊了模型中被精确描述的关系。现有的方法通常基于固定的空间位置来定义节点，这种策略在动态环境中（如海洋环境）并不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的图表示方法，旨在明确捕捉时间依赖关系和空间交互，并适用于动态环境。&lt;h4&gt;方法&lt;/h4&gt;将时间戳作为独立节点建模，通过图边显式地捕获时间依赖性。这种方法被扩展以构建多船图，能够有效地捕捉空间互动同时保持图形稀疏性。使用图卷积网络层处理该图，用于捕获时空模式，并结合预测层和变分图自编码器进行特征预测及重构。&lt;h4&gt;主要发现&lt;/h4&gt;新方法能够在动态环境中有效学习时空交互，通过创新的节点定义策略增强对时间依赖性的捕捉能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在复杂且动态的环境下利用图神经网络建模提供了新的视角，并具有强大的异常检测能力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已经成为学习时空交互的强大工具。然而，传统的做法通常依赖于预定义的图结构，这可能会模糊模型中被精确描述的关系。此外，现有的方法一般基于固定的空间位置来定义节点，这种方法对于像海洋环境这样的动态环境是不合适的。我们的方法引入了一种创新的图表示方式，其中时间戳被建模为独立的节点，允许通过图边显式地捕获时间依赖性。此设置被扩展以构建一个多船图，能够有效地捕捉空间互动同时保持图形稀疏性。该图使用图卷积网络层来处理，用于捕捉时空模式，并结合预测层和变分图自编码器进行特征预测及重构，从而实现强大的异常检测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a powerful tool for learningspatiotemporal interactions. However, conventional approaches often rely onpredefined graphs, which may obscure the precise relationships being modeled.Additionally, existing methods typically define nodes based on fixed spatiallocations, a strategy that is ill-suited for dynamic environments like maritimeenvironments. Our method introduces an innovative graph representation wheretimestamps are modeled as distinct nodes, allowing temporal dependencies to beexplicitly captured through graph edges. This setup is extended to construct amulti-ship graph that effectively captures spatial interactions whilepreserving graph sparsity. The graph is processed using Graph ConvolutionalNetwork layers to capture spatiotemporal patterns, with a forecasting layer forfeature prediction and a Variational Graph Autoencoder for reconstruction,enabling robust anomaly detection.</description>
      <author>example@mail.com (Jeehong Kim, Minchan Kim, Jaeseong Ju, Youngseok Hwang, Wonhee Lee, Hyunwoo Park)</author>
      <guid isPermaLink="false">2502.14197v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Distribution Matching for Self-Supervised Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.14424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个新颖的自监督迁移学习方法——分布匹配(DM)，该方法在保持增强不变性的同时，驱动表示向预定义的参考分布靠拢。&lt;h4&gt;背景&lt;/h4&gt;现有的自监督迁移学习方法可能难以解释其内部结构和超参数的意义。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够提供直观且易于理解的表示空间的设计，并通过理论保证来证明该方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新的自监督转移学习方法，称为分布匹配(DM)，这种方法使得学习到的表示空间具有结构性并且其超参数可解释。此外，提供了DM的方法论上的保证，包括一个总体定理和端到端样本理论。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了在多个现实世界数据集上以及评估指标中，相对于现有的自监督迁移学习方法，DM在目标分类任务上有竞争力的表现。即使样本数量有限的情况下，如果未标记的样本足够大，DM也可以提供出色的分类性能。&lt;h4&gt;结论&lt;/h4&gt;通过理论和实验结果证实了分布匹配(DM)方法的有效性，并且其设计使得表示空间具有结构化、可解释的特性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种新颖的自监督迁移学习方法，称为分布匹配（DM），该方法在保持增强不变性的前提下驱动表示向预定义的参考分布靠拢。实验结果显示，在多个真实世界的数据集和评估指标上，与现有的自监督转移学习方法相比，DM在目标分类任务上有竞争力的表现。此外，我们还为DM提供了坚实的理论保证，包括总体定理和端到端样本定理。总体定理连接了自我监督学习任务和目标分类准确性之间的差距，而样本定理表明，即使从目标领域得到的样本数量有限，如果未标记的样本数量足够大，DM也能提供卓越的分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vincen-github/DM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel self-supervised transfer learning methodcalled Distribution Matching (DM), which drives the representation distributiontoward a predefined reference distribution while preserving augmentationinvariance. The design of DM results in a learned representation space that isintuitively structured and offers easily interpretable hyperparameters.Experimental results across multiple real-world datasets and evaluation metricsdemonstrate that DM performs competitively on target classification taskscompared to existing self-supervised transfer learning methods. Additionally,we provide robust theoretical guarantees for DM, including a population theoremand an end-to-end sample theorem. The population theorem bridges the gapbetween the self-supervised learning task and target classification accuracy,while the sample theorem shows that, even with a limited number of samples fromthe target domain, DM can deliver exceptional classification performance,provided the unlabeled sample size is sufficiently large.</description>
      <author>example@mail.com (Yuling Jiao, Wensen Ma, Defeng Sun, Hansheng Wang, Yang Wang)</author>
      <guid isPermaLink="false">2502.14424v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Token Adaptation via Side Graph Convolution for Temporally and Spatially Efficient Fine-tuning of 3D Point Cloud Transformers</title>
      <link>http://arxiv.org/abs/2502.14142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;参数高效的微调（PEFT）是3D点云分析中预训练的3D点云Transformer的一种有前景的技术。尽管现有的PEFT方法试图减少可调整参数的数量，但在微调过程中仍然存在较高的时间和空间计算成本。&lt;h4&gt;背景&lt;/h4&gt;当前基于PEFT的方法虽然减少了可调参数的数量，但是在微调过程中的时间与空间效率仍然不尽如人意。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的针对3D点云Transformer的PEFT算法STAG，以在不牺牲性能的前提下提高时间和空间效率。&lt;h4&gt;方法&lt;/h4&gt;采用图卷积侧网络（Side Token Adaptation on a neighborhood Graph, STAG）作为冻结主干Transformer的并行组件，通过连接、参数共享框架和高效的图卷积来实现高效率。同时提出Point Cloud Classification 13 (PCC13)基准测试集，涵盖多种公开的3D点云数据集。&lt;h4&gt;主要发现&lt;/h4&gt;STAG算法在保持现有方法分类准确率的同时，将可调参数数量减少至0.43M，并且显著降低了微调过程中的计算时间和内存消耗。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，STAG不仅实现了高精度和高效的PEFT，在多项预训练模型上的表现均优于其他方法。这一成果为未来的3D点云分析提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调（PEFT）是针对预训练的3D点云Transformer的一种有前景的技术，用于3D点云分析。尽管现有的PEFT方法尝试减少可调整参数的数量，但在精细调节过程中仍然存在高计算时间和空间成本的问题。本文提出了一种名为Side Token Adaptation on a neighborhood Graph (STAG) 的新颖PEFT算法，旨在实现卓越的时间和空间效率。STAG使用与冻结主干Transformer并行工作的图卷积侧网络来适应下游任务的令牌。通过连接、参数共享框架以及高效的图卷积，STAG的侧面网络实现了高效率。我们还提出了一个新的基准测试集Point Cloud Classification 13 (PCC13)，该基准集合成了多种公开可用的3D点云数据集，可以全面评估PEFT方法。使用多个预训练模型和PCC13进行广泛的实验表明了STAG的有效性。具体而言，STAG在保持现有方法的分类准确性的同时，将可调参数减少到仅0.43M，并显著减少了微调过程中的计算时间和内存消耗。代码和基准测试将在以下网址提供：https://github.com/takahikof/STAG&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloudTransformers has emerged as a promising technique for 3D point cloud analysis.While existing PEFT methods attempt to minimize the number of tunableparameters, they still suffer from high temporal and spatial computationalcosts during fine-tuning. This paper proposes a novel PEFT algorithm for 3Dpoint cloud Transformers, called Side Token Adaptation on a neighborhood Graph(STAG), to achieve superior temporal and spatial efficiency. STAG employs agraph convolutional side network that operates in parallel with a frozenbackbone Transformer to adapt tokens to downstream tasks. STAG's side networkrealizes high efficiency through three key components: connection with thebackbone that enables reduced gradient computation, parameter sharingframework, and efficient graph convolution. Furthermore, we present Point CloudClassification 13 (PCC13), a new benchmark comprising diverse publiclyavailable 3D point cloud datasets, enabling comprehensive evaluation of PEFTmethods. Extensive experiments using multiple pre-trained models and PCC13demonstrates the effectiveness of STAG. Specifically, STAG maintainsclassification accuracy comparable to existing methods while reducing tunableparameters to only 0.43M and achieving significant reductions in bothcomputational time and memory consumption for fine-tuning. Code and benchmarkwill be available at: https://github.com/takahikof/STAG</description>
      <author>example@mail.com (Takahiko Furuya)</author>
      <guid isPermaLink="false">2502.14142v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud Geometry Scalable Coding Using a Resolution and Quality-conditioned Latents Probability Estimator</title>
      <link>http://arxiv.org/abs/2502.14099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE and currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了一种新的点云（PC）编码可扩展解决方案，名为SRQH。这种方案能够在单个比特流中实现不同质量和分辨率的解码。&lt;h4&gt;背景&lt;/h4&gt;当前多媒体内容消费场景多样化，对网络、硬件和显示能力要求各异，传统的编码方法难以适应这些需求而不会导致存储和计算成本大幅增加。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于基于深度学习点云编码的标准JPEG Pleno的新可扩展编码方案SRQH。&lt;h4&gt;方法&lt;/h4&gt;该方案通过同时实现质量和分辨率的可扩展性来解决传统问题，能够建模由不同RD折衷训练的模型获得潜在变量之间的关系，并且在不同分辨率下仍有效。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SRQH能够在单个比特流中解码出不同质量和分辨率的点云，同时仅稍微增加复杂性和损失一些压缩效率。&lt;h4&gt;结论&lt;/h4&gt;SRQH是一种创新性的可扩展编码方案，在JPEG Pleno标准中的集成证明了其有效性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;在当今时代，用户以各种各样的网络、硬件和显示能力场景消费多媒体内容。一个简单的解决方案是为每种可能的客户需求生成独立流，但这种做法会大大增加存储和计算需求。通过使用能够生成渐进式比特流（包含基础层后跟多个增强层）的编码器可以避免这些问题，从而允许多次解码相同的比特流以满足不同重建和显示要求。虽然可扩展编码在传统图像和视频编解码器中已为人所知且得到解决，但本论文关注的是开发一种基于深度学习点云（PC）的新问题解决方案。由于这种3D表示的特性，实现灵活而不损害编解码器其他功能的方案难度很大。本文提出了一种名为可伸缩分辨率和质量超先验(SRQH)的联合质量和分辨率可扩展性方案，该方案与先前的方法不同，能够建模使用针对不同RD权衡训练模型获得潜在变量之间的关系，并且在不同分辨率下仍有效。实验结果表明，当将SRQH集成到新兴的JPEG Pleno学习PC编码标准中时，SRQH允许仅通过单个比特流解码具有不同质量和分辨率的PC，同时相较于需要针对每种编解码配置单独比特流的非可扩展性JPEG PCC只带来有限的RD惩罚和复杂度增量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the current age, users consume multimedia content in very heterogeneousscenarios in terms of network, hardware, and display capabilities. A naivesolution to this problem is to encode multiple independent streams, eachcovering a different possible requirement for the clients, with an obviousnegative impact in both storage and computational requirements. These drawbackscan be avoided by using codecs that enable scalability, i.e., the ability togenerate a progressive bitstream, containing a base layer followed by multipleenhancement layers, that allow decoding the same bitstream serving multiplereconstructions and visualization specifications. While scalable coding is awell-known and addressed feature in conventional image and video codecs, thispaper focuses on a new and very different problem, notably the development ofscalable coding solutions for deep learning-based Point Cloud (PC) coding. Thepeculiarities of this 3D representation make it hard to implement flexiblesolutions that do not compromise the other functionalities of the codec. Thispaper proposes a joint quality and resolution scalability scheme, namedScalable Resolution and Quality Hyperprior (SRQH), that, contrary to previoussolutions, can model the relationship between latents obtained with modelstrained for different RD tradeoffs and/or at different resolutions.Experimental results obtained by integrating SRQH in the emerging JPEG Plenolearning-based PC coding standard show that SRQH allows decoding the PC atdifferent qualities and resolutions with a single bitstream while incurringonly in a limited RD penalty and increment in complexity w.r.t. non-scalableJPEG PCC that would require one bitstream per coding configuration.</description>
      <author>example@mail.com (Daniele Mari, André F. R. Guarda, Nuno M. M. Rodrigues, Simone Milani, Fernando Pereira)</author>
      <guid isPermaLink="false">2502.14099v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Pretraining with Group-Level Data Influence Modeling</title>
      <link>http://arxiv.org/abs/2502.14709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Group-Level Data Influence Modeling (Group-MATES) 的新方法，用于高效的数据预训练。这种方法通过收集数据集对预训练模型的群体级影响，并利用关系加权聚合的个体影响力来优化群体级别的数据效用。&lt;h4&gt;背景&lt;/h4&gt;数据高效的预训练在提升缩放规律方面展示了巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，能够更有效地进行数据预训练，特别是在群组级别而不是独立数据点上优化数据的使用效率。&lt;h4&gt;方法&lt;/h4&gt;Group-MATES 方法包括：收集群体级别的影响力，通过本地探测预训练模型来获得；随后微调一个关系性数据影响模型以近似这些群体级影响力，并选择具有最大预测群体级影响力的子集数据。该过程还采用影响力感知聚类技术，以便更高效地进行推断。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Group-MATES 方法在22个下游任务上比DCLM-Baseline提高了10%的相对核心评分，比基于个体影响的方法提高5%，确立了新的状态-of-the-art。&lt;h4&gt;结论&lt;/h4&gt;关系性数据影响模型能够有效地捕捉到数据点之间的复杂交互作用，从而提升预训练模型的效果和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到数据高效预训练展现了巨大的潜力，提出一种新方法Group-Level Data Influence Modeling (Group-MATES)，该方法旨在通过收集和优化群组级别的数据效用来改进预训练过程。经过实验验证，这种方法在特定基准测试中表现出色，确立了新的最佳实践标准，并展示了捕捉复杂数据点交互的模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data-efficient pretraining has shown tremendous potential to elevate scalinglaws. This paper argues that effective pretraining data should be curated atthe group level, treating a set of data points as a whole rather than asindependent contributors. To achieve that, we propose Group-Level DataInfluence Modeling (Group-MATES), a novel data-efficient pretraining methodthat captures and optimizes group-level data utility. Specifically, Group-MATEScollects oracle group-level influences by locally probing the pretraining modelwith data sets. It then fine-tunes a relational data influence model toapproximate oracles as relationship-weighted aggregations of individualinfluences. The fine-tuned model selects the data subset by maximizing itsgroup-level influence prediction, with influence-aware clustering to enableefficient inference. Experiments on the DCLM benchmark demonstrate thatGroup-MATES achieves a 10% relative core score improvement on 22 downstreamtasks over DCLM-Baseline and 5% over individual-influence-based methods,establishing a new state-of-the-art. Further analyses highlight theeffectiveness of relational data influence models in capturing intricateinteractions between data points.</description>
      <author>example@mail.com (Zichun Yu, Fei Peng, Jie Lei, Arnold Overwijk, Wen-tau Yih, Chenyan Xiong)</author>
      <guid isPermaLink="false">2502.14709v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>A Macro- and Micro-Hierarchical Transfer Learning Framework for Cross-Domain Fake News Detection</title>
      <link>http://arxiv.org/abs/2502.14403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;跨域假新闻检测旨在通过在不同领域之间转移知识来减轻领域的偏移，从而改善检测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的方法是基于从源域到目标域的新闻内容和用户互动的知识迁移。然而，这些方法存在两个主要限制，阻碍了有效的知识传输及最优的假新闻检测性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的宏观和微观层级传递学习框架（MMHT），以解决现有方法在跨领域假新闻检测中的局限性。&lt;h4&gt;方法&lt;/h4&gt;{'微层级模块': '提出了一个用于从源域的新闻内容中区分事实相关和无关特征，从而改进目标领域的假新闻检测性能的微层级解构模块。', '宏观层级模块': '提出了一种基于不同领域之间常见用户共享行为生成参与度特征的宏观层级传递学习模块，以提高知识迁移的有效性。'}&lt;h4&gt;主要发现&lt;/h4&gt;通过在现实世界数据集上的广泛实验，证明了该框架显著优于现有基准。&lt;h4&gt;结论&lt;/h4&gt;提出的MMHT框架有效解决了跨域假新闻检测中的挑战，并展示了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714517&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain fake news detection aims to mitigate domain shift and improvedetection performance by transferring knowledge across domains. Existingapproaches transfer knowledge based on news content and user engagements from asource domain to a target domain. However, these approaches face two mainlimitations, hindering effective knowledge transfer and optimal fake newsdetection performance. Firstly, from a micro perspective, they neglect thenegative impact of veracity-irrelevant features in news content whentransferring domain-shared features across domains. Secondly, from a macroperspective, existing approaches ignore the relationship between userengagement and news content, which reveals shared behaviors of common usersacross domains and can facilitate more effective knowledge transfer. To addressthese limitations, we propose a novel macro- and micro- hierarchical transferlearning framework (MMHT) for cross-domain fake news detection. Firstly, wepropose a micro-hierarchical disentangling module to disentangleveracity-relevant and veracity-irrelevant features from news content in thesource domain for improving fake news detection performance in the targetdomain. Secondly, we propose a macro-hierarchical transfer learning module togenerate engagement features based on common users' shared behaviors indifferent domains for improving effectiveness of knowledge transfer. Extensiveexperiments on real-world datasets demonstrate that our framework significantlyoutperforms the state-of-the-art baselines.</description>
      <author>example@mail.com (Xuankai Yang, Yan Wang, Xiuzhen Zhang, Shoujin Wang, Huaxiong Wang, Kwok Yan Lam)</author>
      <guid isPermaLink="false">2502.14403v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models in Medical Image Analysis: Advances and Challenges</title>
      <link>http://arxiv.org/abs/2502.14584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本文综述了视觉基础模型（VFMs）在医学图像分析中的适应性研究，特别关注领域适应、模型压缩和联邦学习的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着Vision Foundation Models (VFMs)的发展，尤其是Vision Transformers(ViT)和Segment Anything Model(SAM)，医学影像分析领域取得了显著进展。这些模型展现了捕捉长距离依赖性和实现高泛化的卓越能力。&lt;h4&gt;目的&lt;/h4&gt;探讨将大型视觉基础模型适应于医学图像分割的挑战，并讨论最新的基于适配器的方法改进、知识蒸馏技术和多尺度上下文特征建模的发展，同时提出未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;本文回顾了VFMs在医学图像分割中的最新研究成果，重点关注领域适应、模型压缩和联邦学习的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;VFMs和其他新兴技术（如联邦学习和模型压缩）具有革新医学影像分析和增强临床应用的巨大潜力。论文强调了当前的研究方向，并指出了未来研究的关键领域。&lt;h4&gt;结论&lt;/h4&gt;该工作旨在提供一个全面的方法概述，为驱动医学图像分割领域的下一波创新提出关键建议。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型在医疗影像分析中的快速发展，特别是在Vision Transformers和Segment Anything Model的应用，已引发了该领域的显著进步。这些模型展现出了捕捉长距离依赖关系及实现高泛化的出色能力。然而，将大型视觉基础模型应用于医学图像分析面临着诸如医学与自然图像之间的领域差异、有效适应策略的需求以及小型医疗数据集限制等挑战。本文综述了VFMs在医学图像分割中的最新研究进展，重点探讨了领域适应、模型压缩和联邦学习的挑战，并讨论了基于适配器的改进方法、知识蒸馏技术及多尺度上下文特征建模的发展趋势，同时提出了未来的潜在发展方向。我们的分析强调了VFMs及其他新兴如联邦学习和模型压缩的技术在革新医学影像分析与提高临床应用方面所具有的巨大潜力。本文旨在为当前的研究方法提供全面概述，并建议未来关键研究领域以驱动下一轮创新浪潮的到来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Vision Foundation Models (VFMs), particularly VisionTransformers (ViT) and Segment Anything Model (SAM), has sparked significantadvances in the field of medical image analysis. These models have demonstratedexceptional capabilities in capturing long-range dependencies and achievinghigh generalization in segmentation tasks. However, adapting these large modelsto medical image analysis presents several challenges, including domaindifferences between medical and natural images, the need for efficient modeladaptation strategies, and the limitations of small-scale medical datasets.This paper reviews the state-of-the-art research on the adaptation of VFMs tomedical image segmentation, focusing on the challenges of domain adaptation,model compression, and federated learning. We discuss the latest developmentsin adapter-based improvements, knowledge distillation techniques, andmulti-scale contextual feature modeling, and propose future directions toovercome these bottlenecks. Our analysis highlights the potential of VFMs,along with emerging methodologies such as federated learning and modelcompression, to revolutionize medical image analysis and enhance clinicalapplications. The goal of this work is to provide a comprehensive overview ofcurrent approaches and suggest key areas for future research that can drive thenext wave of innovation in medical image segmentation.</description>
      <author>example@mail.com (Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang)</author>
      <guid isPermaLink="false">2502.14584v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection</title>
      <link>http://arxiv.org/abs/2502.14116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;硬件木马是在数字设计中由不可信供应链实体插入的恶意修改。这些木马可以导致信息泄露（例如MOLES木马）和拒绝服务等多样化的攻击向量。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测数字设计中的恶意修改的技术，特别是那些来自第三方知识产权供应商的设计。&lt;h4&gt;方法&lt;/h4&gt;提出了一个框架（SALTY），该框架使用新颖的图神经网络架构（利用跳跃知识机制生成初步预测）以及可解释的人工智能（XAI）方法进行后续处理来细化结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法能够达到98%的真实正样本率（TPR）和真实负样本率（TNR），在一系列标准基准测试中显著优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了现有检测技术的可扩展性问题，并且大大减少了误报的数量。&lt;h4&gt;翻译&lt;/h4&gt;硬件木马是数字设计中的一种恶意修改，可能由不可信供应链中的实体插入。它们会导致信息泄露（例如MOLES木马）和拒绝服务等多样化的攻击向量。这些攻击在关键系统（如医疗保健和航空领域）中可能导致人员伤亡及巨大的经济损失。已经开发了几种技术来检测数字设计中的这类恶意修改，尤其是从第三方知识产权供应商获取的设计。然而，大多数方法存在可扩展性问题（由于评估过程中不合理的假设），并且会产生大量的误报。我们的框架（SALTY）通过使用一种基于跳跃知识机制的新型图神经网络架构生成初步预测，并且利用可解释人工智能(XAI)的方法进行细化处理来缓解这些问题。实验结果表明，该方法在一系列标准基准测试中实现了98%的真实正样本率(TPR)和真实负样本率(TNR)，显著优于现有技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hardware Trojans are malicious modifications in digital designs that can beinserted by untrusted supply chain entities. Hardware Trojans can give rise todiverse attack vectors such as information leakage (e.g. MOLES Trojan) anddenial-of-service (rarely triggered bit flip). Such an attack in criticalsystems (e.g. healthcare and aviation) can endanger human lives and lead tocatastrophic financial loss. Several techniques have been developed to detectsuch malicious modifications in digital designs, particularly for designssourced from third-party intellectual property (IP) vendors. However, mosttechniques have scalability concerns (due to unsound assumptions duringevaluation) and lead to large number of false positive detections (falsealerts). Our framework (SALTY) mitigates these concerns through the use of anovel Graph Neural Network architecture (using Jumping-Knowledge mechanism) forgenerating initial predictions and an Explainable Artificial Intelligence (XAI)approach for fine tuning the outcomes (post-processing). Experiments show 98%True Positive Rate (TPR) and True Negative Rate (TNR), significantlyoutperforming state-of-the-art techniques across a large set of standardbenchmarks.</description>
      <author>example@mail.com (Tanzim Mahfuz, Pravin Gaikwad, Tasneem Suha, Swarup Bhunia, Prabuddha Chakraborty)</author>
      <guid isPermaLink="false">2502.14116v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Extending the RANGE of Graph Neural Networks: Relaying Attention Nodes for Global Encoding</title>
      <link>http://arxiv.org/abs/2502.13797v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RANGE是一个模型无关的框架，旨在解决图神经网络在处理长程相互作用时的信息瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）被广泛应用于分子物理、社会科学和经济学等领域。然而，GNN本质上是局部性的，并且当用于模拟具有长程相互作用的大规模分子系统时容易产生信息流动瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决大规模分子系统的计算成本高以及模型扩展性差的问题。&lt;h4&gt;方法&lt;/h4&gt;RANGE采用基于注意力的聚合-广播机制，该机制能够显著减少过度压缩效应，并以几乎可以忽略不计的计算成本捕捉长程相互作用。此外，这是首次在虚拟节点消息传递中整合注意力机制、位置编码和正则化来动态扩展虚拟表示的方法。&lt;h4&gt;主要发现&lt;/h4&gt;RANGE框架不仅大幅提高了对大规模分子系统中的长程相互作用建模的能力，而且保持了较低的计算成本。&lt;h4&gt;结论&lt;/h4&gt;该研究为下一代机器学习力场奠定了基础，并提供了既准确又高效的长期互动模型。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）经常用于模拟分子物理、社会科学和经济学中类似图形系统中的多体相互作用。然而，GNN本质上是局部性的并且在信息传递时可能会遭遇瓶颈问题。特别是在大规模分子系统的建模过程中，分散力和局部电场变化会驱动结构上的集体性改变，这使得问题更为复杂。现有的解决方案面临着计算成本高、可扩展性差的挑战。我们提出了一种模型无关的方法——RANGE，它采用了基于注意力机制的聚合-广播方法，大幅度减少了信息过度压缩的问题，并以极低的成本实现了长程相互作用的有效捕捉。值得注意的是，RANGE是首个将位置编码和正则化与注意力相结合，从而动态扩展虚拟表示的虚拟节点消息传递实现方案。这项研究为下一代机器学习力场奠定了基础，提供了一种既准确又高效的对大规模分子系统中的长程互动进行建模的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are routinely used in molecular physics, socialsciences, and economics to model many-body interactions in graph-like systems.However, GNNs are inherently local and can suffer from information flowbottlenecks. This is particularly problematic when modeling large molecularsystems, where dispersion forces and local electric field variations drivecollective structural changes. Existing solutions face challenges related tocomputational cost and scalability. We introduce RANGE, a model-agnosticframework that employs an attention-based aggregation-broadcast mechanism thatsignificantly reduces oversquashing effects, and achieves remarkable accuracyin capturing long-range interactions at a negligible computational cost.Notably, RANGE is the first virtual-node message-passing implementation tointegrate attention with positional encodings and regularization to dynamicallyexpand virtual representations. This work lays the foundation fornext-generation of machine-learned force fields, offering accurate andefficient modeling of long-range interactions for simulating large molecularsystems.</description>
      <author>example@mail.com (Alessandro Caruso, Jacopo Venturin, Lorenzo Giambagli, Edoardo Rolando, Frank Noé, Cecilia Clementi)</author>
      <guid isPermaLink="false">2502.13797v2</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling</title>
      <link>http://arxiv.org/abs/2502.14553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '字节构成了数字世界的基石，是多模态基础模型的潜在构建块。提出了Multiscale Byte Language Model (MBLM)，这是一种基于层次解码器堆栈、能够以全精度在单个GPU上使用5M字节上下文窗口进行训练的模型。', '背景': 'Byte语言模型（BLMs）最近出现，旨在克服分词问题，但字节流过长需要新的架构范式。MBLM是一个与模型无关的层次解码器堆栈，可以高效处理极长的字节序列并在生成效率上接近线性增长。', '目的': '介绍并评估Multiscale Byte Language Model (MBLM)，探讨其在视觉问答任务中的表现，并展示其适应各种数据表示的能力。', '方法': '通过Transformer和Mamba块对单模态和多模态任务进行全面性能测试，展示了混合架构的有效性。另外，首次评估了BLMs在视觉问答任务上的表现。', '主要发现': '尽管序列化图像且没有编码器，MBLM仅基于纯下一个令牌预测就可与专用的CNN-LSTM架构相匹配，后者具有指定分类头。', '结论': 'MBLM展示了强大的适应性，在集成各种数据表示（包括像素和图像文件流字节）方面表现出巨大潜力，这使其成为通用多模态基础模型的发展方向。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到了Bytes在数字世界中的重要性，并介绍了Multiscale Byte Language Model (MBLM)，这种模型可以高效处理极长的序列数据并适用于视觉问答任务等多模态场景。研究结果表明，MBLM在生成效率上接近线性增长，且其表现可与复杂的CNN-LSTM架构相媲美。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bytes form the basis of the digital world and thus are a promising buildingblock for multimodal foundation models. Recently, Byte Language Models (BLMs)have emerged to overcome tokenization, yet the excessive length of bytestreamsrequires new architectural paradigms. Therefore, we present the Multiscale ByteLanguage Model (MBLM), a model-agnostic hierarchical decoder stack that allowstraining with context windows of $5$M bytes on single GPU in full modelprecision. We thoroughly examine MBLM's performance with Transformer and Mambablocks on both unimodal and multimodal tasks. Our experiments demonstrate thathybrid architectures are efficient in handling extremely long byte sequencesduring training while achieving near-linear generational efficiency. To thebest of our knowledge, we present the first evaluation of BLMs on visual Q\&amp;Atasks and find that, despite serializing images and the absence of an encoder,a MBLM with pure next token prediction can match custom CNN-LSTM architectureswith designated classification heads. We show that MBLMs exhibit strongadaptability in integrating diverse data representations, including pixel andimage filestream bytes, underlining their potential toward omnimodal foundationmodels. Source code is publicly available at:https://github.com/ai4sd/multiscale-byte-lm</description>
      <author>example@mail.com (Eric Egli, Matteo Manica, Jannis Born)</author>
      <guid isPermaLink="false">2502.14553v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation</title>
      <link>http://arxiv.org/abs/2502.14214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个针对源无标签领域自适应（SFFSDA）场景的不对称协同训练(ACT)方法，该方法旨在解决传统无监督域自适应在缺乏大量目标数据时表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的无监督域自适应依赖于有标签的源数据的持续可用性。然而，在实际应用中获取大量的未标记目标数据是不现实的，因此需要寻求一种新的解决方案来克服这个限制。&lt;h4&gt;目的&lt;/h4&gt;提出了一种针对SFFSDA场景的有效方法，即不对称协同训练（ACT），通过该方法可以在少量有标签的目标数据的基础上对预训练模型进行适应性改进。&lt;h4&gt;方法&lt;/h4&gt;ACT方法首先采用弱强增强技术增加数据多样性。然后利用两步优化过程来训练目标模型：第一步优化标签平滑交叉熵损失、条件分布的熵以及反向熵损失，以提高模型区分能力并减少过拟合；第二步则通过最小化分类器确定性差异来降低输出空间中的冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准上的广泛实验表明，所提出的ACT方法优于现有的源无标签领域自适应（SFUDA）方法和迁移学习技术。&lt;h4&gt;结论&lt;/h4&gt;使用少量的有标签目标数据调整预训练模型可以提供一种实用且可靠的方法来解决域自适应问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于如何克服传统无监督域自适应方法依赖大量未标记目标数据的问题，提出了一种新的源无标签领域自适应（SFFSDA）方法。该方法通过不对称协同训练技术在少量有标签的目标数据基础上优化预训练模型性能，并展示了优于现有方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-free unsupervised domain adaptation (SFUDA) has gained significantattention as an alternative to traditional unsupervised domain adaptation(UDA), which relies on the constant availability of labeled source data.However, SFUDA approaches come with inherent limitations that are frequentlyoverlooked. These challenges include performance degradation when the unlabeledtarget data fails to meet critical assumptions, such as having a closed-setlabel distribution identical to that of the source domain, or when sufficientunlabeled target data is unavailable-a common situation in real-worldapplications. To address these issues, we propose an asymmetric co-training(ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents amore practical alternative to SFUDA, as gathering a few labeled targetinstances is more feasible than acquiring large volumes of unlabeled targetdata in many real-world contexts. Our ACT method begins by employing aweak-strong augmentation to enhance data diversity. Then we use a two-stepoptimization process to train the target model. In the first step, we optimizethe label smoothing cross-entropy loss, the entropy of the class-conditionaldistribution, and the reverse-entropy loss to bolster the model'sdiscriminative ability while mitigating overfitting. The second step focuses onreducing redundancy in the output space by minimizing classifier determinacydisparity. Extensive experiments across four benchmarks demonstrate thesuperiority of our ACT approach, which outperforms state-of-the-art SFUDAmethods and transfer learning techniques. Our findings suggest that adapting asource pre-trained model using only a small amount of labeled target dataoffers a practical and dependable solution. The code is available athttps://github.com/gengxuli/ACT.</description>
      <author>example@mail.com (Gengxu Li, Yuan Wu)</author>
      <guid isPermaLink="false">2502.14214v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks</title>
      <link>http://arxiv.org/abs/2502.14546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文摘要强调了在图学习应用于药物设计和分子属性预测方面的挑战，提出需要改进的基准测试方法以促进研究的进步。&lt;h4&gt;背景&lt;/h4&gt;机器学习在图上的应用显示出其在药物设计和分子性质预测中的潜力，但目前存在的大量基准测试问题阻碍了这些领域的进一步发展和相关性。&lt;h4&gt;目的&lt;/h4&gt;论文呼吁进行范式转变，采用更具意义的基准测试、严格的评估协议，并与领域专家加强合作，以推动有意义且可靠的图学习研究进展。&lt;h4&gt;方法&lt;/h4&gt;摘要中并未详细描述具体的方法论或实验过程。&lt;h4&gt;主要发现&lt;/h4&gt;当前的基准测试实践中倾向于关注狭窄的应用域如二维分子图，而不是更广泛和有影响力的应用领域。此外，许多基准数据集未能准确反映基础数据特征，导致抽象不充分且应用场景与实际脱节。&lt;h4&gt;结论&lt;/h4&gt;需要建立更加合理、更具代表性的基准体系来推动图学习技术在真实世界中的有效应用。&lt;h4&gt;翻译&lt;/h4&gt;虽然基于图的机器学习在药物设计和分子属性预测方面展现出了巨大潜力，但现有的基准测试挑战阻碍了其进一步的发展和实用性。当前的基准测试方法倾向于关注如二维分子图等狭窄领域，而不是更广泛且有影响力的应用领域，例如组合优化、关系数据库或芯片设计。此外，许多数据集未能准确反映实际基础数据特征，导致抽象不充分及应用场景与需求不符的问题加剧。这种碎片化的评估和对准确性过度重视的情况进一步促进了过拟合问题的出现，从而阻碍了通用性见解的发展。这些局限性已经阻止了真正有用的基础图模型的发展。这篇立场论文呼吁进行范式转变，采用更加有意义的基准测试标准、严格的评价流程，并与领域专家加强合作，以推动有影响力的和可靠的图学习研究进展，释放基于图的学习技术的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While machine learning on graphs has demonstrated promise in drug design andmolecular property prediction, significant benchmarking challenges hinder itsfurther progress and relevance. Current benchmarking practices often lack focuson transformative, real-world applications, favoring narrow domains liketwo-dimensional molecular graphs over broader, impactful areas such ascombinatorial optimization, relational databases, or chip design. Additionally,many benchmark datasets poorly represent the underlying data, leading toinadequate abstractions and misaligned use cases. Fragmented evaluations and anexcessive focus on accuracy further exacerbate these issues, incentivizingoverfitting rather than fostering generalizable insights. These limitationshave prevented the development of truly useful graph foundation models. Thisposition paper calls for a paradigm shift toward more meaningful benchmarks,rigorous evaluation protocols, and stronger collaboration with domain expertsto drive impactful and reliable advances in graph learning research, unlockingthe potential of graph learning.</description>
      <author>example@mail.com (Maya Bechler-Speicher, Ben Finkelshtein, Fabrizio Frasca, Luis Müller, Jan Tönshoff, Antoine Siraudin, Viktor Zaverkin, Michael M. Bronstein, Mathias Niepert, Bryan Perozzi, Mikhail Galkin, Christopher Morris)</author>
      <guid isPermaLink="false">2502.14546v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Precise Geolocation Inference Capabilities of Vision Language Models</title>
      <link>http://arxiv.org/abs/2502.14412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2025 Workshop DATASAFE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;视觉语言模型（VLMs）的广泛应用引发了关于隐私的问题，尤其是在图像信息越来越容易获得的时代。这项研究专注于评估这些基础模型从之前未见过的图像数据中推断地理位置的能力。&lt;h4&gt;背景&lt;/h4&gt;随着视觉信息变得日益丰富和易于获取，视觉语言模型（Vision-Language Models, VLMs）的应用变得广泛起来，这引发了一系列关于隐私保护的问题。基础VLM模型虽然展现出了广博的知识和学习能力，但研究人员特别关注它们从新图像中推断地理位置的能力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在评估基础VLM模型在未见过的图像数据上进行地理位置推测的有效性，并探讨其对在线隐私可能带来的风险。&lt;h4&gt;方法&lt;/h4&gt;研究团队创建了一个基准数据集，该数据集是从Google Street View收集的数据，涵盖了全球范围内的地理分布。这些基础模型被用来测试单张图片地理位置推断的能力。此外，研究人员还评估了具有额外工具访问权限的VLM '代理'的表现，并观察到了最高30.6%的距离误差减少。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，现代的基础视觉语言模型在没有专门训练的情况下也能成为强大的图像地理定位工具；它们可以有效地从新图片中推断出地理位置信息。当这些模型变得越来越容易获取时，这给在线隐私带来了更大的挑战和风险。&lt;h4&gt;结论&lt;/h4&gt;尽管基础VLM具有作为高效图像地理定位工具的潜力，但研究人员强调了这对个人数据保护可能造成的潜在威胁，并提出了进一步研究的方向来缓解这些问题&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The prevalence of Vision-Language Models (VLMs) raises important questionsabout privacy in an era where visual information is increasingly available.While foundation VLMs demonstrate broad knowledge and learned capabilities, wespecifically investigate their ability to infer geographic location frompreviously unseen image data. This paper introduces a benchmark datasetcollected from Google Street View that represents its global distribution ofcoverage. Foundation models are evaluated on single-image geolocationinference, with many achieving median distance errors of &lt;300 km. We furtherevaluate VLM "agents" with access to supplemental tools, observing up to a30.6% decrease in distance error. Our findings establish that modern foundationVLMs can act as powerful image geolocation tools, without being specificallytrained for this task. When coupled with increasing accessibility of thesemodels, our findings have greater implications for online privacy. We discussthese risks, as well as future work in this area.</description>
      <author>example@mail.com (Neel Jay, Hieu Minh Nguyen, Trung Dung Hoang, Jacob Haimes)</author>
      <guid isPermaLink="false">2502.14412v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images</title>
      <link>http://arxiv.org/abs/2502.14351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种专门用于PET图像分割的3D基础模型SegAnyPET，并通过大规模数据集PETS-5k验证了其性能。&lt;h4&gt;背景&lt;/h4&gt;正电子发射断层扫描（PET）成像在现代医学诊断中发挥重要作用，但PET图像的低对比度和边界模糊使其难以进行有效分割。现有的自然图像分割方法对于结构化的医疗影像表现出较差的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对PET图像的通用可提示分割基础模型，并解决其标注质量差异带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;构建了大规模PET图像分割数据集PETS-5k，包含超过1.3M张2D图像；提出SegAnyPET模型，采用交叉提示自信学习策略提高低质量和高质量标签之间的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有基础模型和特定任务的监督模型相比，SegAnyPET在分割精度和泛化能力方面表现出更佳性能，仅使用少量提示点即可正确分割已知及未知目标。&lt;h4&gt;结论&lt;/h4&gt;作为首个专门针对PET图像的基础模型，SegAnyPET将推进分子成像下游任务的应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Positron Emission Tomography (PET) imaging plays a crucial role in modernmedical diagnostics by revealing the metabolic processes within a patient'sbody, which is essential for quantification of therapy response and monitoringtreatment progress. However, the segmentation of PET images presents uniquechallenges due to their lower contrast and less distinct boundaries compared toother structural medical modalities. Recent developments in segmentationfoundation models have shown superior versatility across diverse natural imagesegmentation tasks. Despite the efforts of medical adaptations, these worksprimarily focus on structural medical images with detailed physiologicalstructural information and exhibit poor generalization ability when adapted tomolecular PET imaging. In this paper, we collect and construct PETS-5k, thelargest PET segmentation dataset to date, comprising 5,731 three-dimensionalwhole-body PET images and encompassing over 1.3M 2D images. Based on theestablished dataset, we develop SegAnyPET, a modality-specific 3D foundationmodel for universal promptable segmentation from PET images. To issue thechallenge of discrepant annotation quality of PET images, we adopt a crossprompting confident learning (CPCL) strategy with an uncertainty-guidedself-rectification process to robustly learn segmentation from high-qualitylabeled data and low-quality noisy labeled data. Experimental resultsdemonstrate that SegAnyPET can correctly segment seen and unseen targets usingonly one or a few prompt points, outperforming state-of-the-art foundationmodels and task-specific fully supervised models with higher accuracy andstrong generalization ability for universal segmentation. As the firstfoundation model for PET images, we believe that SegAnyPET will advance theapplications to various downstream tasks for molecular imaging.</description>
      <author>example@mail.com (Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi)</author>
      <guid isPermaLink="false">2502.14351v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective</title>
      <link>http://arxiv.org/abs/2502.14296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一套全面的框架，旨在解决生成式基础模型（GenFMs）在可信赖性方面面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;Generative Foundation Models (GenFMs) 作为变革性的工具出现，但其广泛应用引发了一系列关于信任度的重要问题。&lt;h4&gt;目的&lt;/h4&gt;通过系统地审查全球AI治理法规和政策、行业实践及标准，并提出一套整合了技术、伦理、法律和社会视角的指导原则来解决这些问题。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '全面评估现有法律法规与行业标准，提出跨学科合作产生的指导原则。', '第二部分': '引入TrustGen平台，这是一个动态基准测试工具，用于从多个维度和模型类型中评估可信赖性。', '第三部分': '深入讨论当前挑战及未来的方向，强调实用性和可信度之间的复杂权衡，以及针对不同下游应用的考虑。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'TrustGen平台功能': '通过模块化组件实现灵活、迭代的评估方法，并揭示了在多个维度上可信赖性的进步和持续存在的挑战。', '未来方向': '论文指出了可信生成式基础模型在未来研究中的复杂性和演变性质。'}&lt;h4&gt;结论&lt;/h4&gt;该工作为推进GenAI领域的信任度提供了全面框架，为进一步的研究和应用奠定了坚实的基础，并公开了动态评估工具包以促进社区发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含英文翻译内容。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Foundation Models (GenFMs) have emerged as transformative tools.However, their widespread adoption raises critical concerns regardingtrustworthiness across dimensions. This paper presents a comprehensiveframework to address these challenges through three key contributions. First,we systematically review global AI governance laws and policies fromgovernments and regulatory bodies, as well as industry practices and standards.Based on this analysis, we propose a set of guiding principles for GenFMs,developed through extensive multidisciplinary collaboration that integratestechnical, ethical, legal, and societal perspectives. Second, we introduceTrustGen, the first dynamic benchmarking platform designed to evaluatetrustworthiness across multiple dimensions and model types, includingtext-to-image, large language, and vision-language models. TrustGen leveragesmodular components--metadata curation, test case generation, and contextualvariation--to enable adaptive and iterative assessments, overcoming thelimitations of static evaluation methods. Using TrustGen, we reveal significantprogress in trustworthiness while identifying persistent challenges. Finally,we provide an in-depth discussion of the challenges and future directions fortrustworthy GenFMs, which reveals the complex, evolving nature oftrustworthiness, highlighting the nuanced trade-offs between utility andtrustworthiness, and consideration for various downstream applications,identifying persistent challenges and providing a strategic roadmap for futureresearch. This work establishes a holistic framework for advancingtrustworthiness in GenAI, paving the way for safer and more responsibleintegration of GenFMs into critical applications. To facilitate advancement inthe community, we release the toolkit for dynamic evaluation.</description>
      <author>example@mail.com (Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, Yuan Li, Han Bao, Zhaoyi Liu, Tianrui Guan, Dongping Chen, Ruoxi Chen, Kehan Guo, Andy Zou, Bryan Hooi Kuen-Yew, Caiming Xiong, Elias Stengel-Eskin, Hongyang Zhang, Hongzhi Yin, Huan Zhang, Huaxiu Yao, Jaehong Yoon, Jieyu Zhang, Kai Shu, Kaijie Zhu, Ranjay Krishna, Swabha Swayamdipta, Taiwei Shi, Weijia Shi, Xiang Li, Yiwei Li, Yuexing Hao, Yuexing Hao, Zhihao Jia, Zhize Li, Xiuying Chen, Zhengzhong Tu, Xiyang Hu, Tianyi Zhou, Jieyu Zhao, Lichao Sun, Furong Huang, Or Cohen Sasson, Prasanna Sattigeri, Anka Reuel, Max Lamparth, Yue Zhao, Nouha Dziri, Yu Su, Huan Sun, Heng Ji, Chaowei Xiao, Mohit Bansal, Nitesh V. Chawla, Jian Pei, Jianfeng Gao, Michael Backes, Philip S. Yu, Neil Zhenqiang Gong, Pin-Yu Chen, Bo Li, Xiangliang Zhang)</author>
      <guid isPermaLink="false">2502.14296v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization</title>
      <link>http://arxiv.org/abs/2502.14211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为Transfer-Prompting的新型框架，旨在改进大型语言模型（LLM）在不同任务之间的适应能力。&lt;h4&gt;背景&lt;/h4&gt;LLMs面临平衡生成连贯、相关且高质量响应与跨多种任务高效适应的重要挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种两阶段框架以增强提示生成过程中的跨任务适应性。&lt;h4&gt;方法&lt;/h4&gt;{'源提示构建': '通过在原始数据集上改进原始提示，创建具有更强泛化能力的源提示。', '目标提示生成': '通过对一组高分源提示进行微调，在特定于任务的数据集上增强目标提示的跨任务适应能力。', '反馈循环': '参考LLM根据历史提示-分数对和任务描述生成候选提示，并通过评分LLM使用多维度指标评估其有效性，从而形成一个促进持续改进的反馈回路。'}&lt;h4&gt;主要发现&lt;/h4&gt;在包括7个基础模型和18个专业化模型在内的25种不同LLMs上进行的广泛实验显示了Transfer-Prompting可以显著改善特定任务性能。&lt;h4&gt;结论&lt;/h4&gt;该框架对增强LLM中的跨任务适应性有潜在价值，并且代码已公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) face significant challenges when balancingmultiple high-level objectives, such as generating coherent, relevant, andhigh-quality responses while maintaining efficient task adaptation acrossdiverse tasks. To address these challenges, we introduce Transfer-Prompting, anovel two-stage framework designed to enhance cross-task adaptation in promptgeneration. The framework comprises two key components: (1) source promptconstruction, which refines the original prompts on source task datasets togenerate source prompts with enhanced generalization ability, and (2) targetprompt generation, which enhances cross-task adaptation of target prompts byfine-tuning a set of high-scored source prompts on task-specific datasets. Ineach optimization cycle, a reference LLM generates candidate prompts based onhistorical prompt-score pairs and task descriptions in our designed referenceprompt. These candidate prompts are refined iteratively, while a scorer LLMevaluates their effectiveness using the multi-dimensional metrics designed inthe objective prompts evaluator-a novel contribution in this work that providesa holistic evaluation of prompt quality and task performance. This feedbackloop facilitates continuous refinement, optimizing both prompt quality andtask-specific outcomes. We validate Transfer-Prompting through extensiveexperiments across 25 LLMs, including 7 foundational models and 18 specializedmodels, evaluated on 9 diverse datasets. The results demonstrate thatTransfer-Prompting significantly improves task-specific performance,highlighting its potential for enhancing cross-task adaptation in LLMs. Thecode is available at https://github.com/llm172/Transfer-Prompting.</description>
      <author>example@mail.com (Yupeng Chang, Yi Chang, Yuan Wu)</author>
      <guid isPermaLink="false">2502.14211v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging</title>
      <link>http://arxiv.org/abs/2502.14064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的3D MRI视觉基础模型Triad，该模型采用自编码器架构从大量MRI数据集中学习鲁棒的表示，并通过器官无关的成像描述来约束语义分布。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉基础模型主要针对CT图像进行了预训练，这使得它们在处理MRI特定应用时可能遇到性能和适应性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于3D MRI的新型视觉基础模型Triad，并评估其在多个下游任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;使用大规模MRI数据集（称为Triad-131K）对模型进行预训练，然后将其应用于多种下游任务并比较与未经过预训练的模型相比的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在器官和成像模式一致的情况下，采用Triad预训练权重的模型在分割、分类和图像配准等任务上取得了显著性能提升。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了针对MRI数据进行预训练可以最大化下游任务中的表现，并为开发专用于医学影像领域的视觉基础模型提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (VFMs) are pre-trained on extensive image datasetsto learn general representations for diverse types of data. These models cansubsequently be fine-tuned for specific downstream tasks, significantlyboosting performance across a broad range of applications. However, existingvision foundation models that claim to be applicable to various radiology tasksare mostly pre-trained on 3D computed tomography (CT), which benefits from theavailability of extensive 3D CT databases. Significant differences between CTand magnetic resonance imaging (MRI) in imaging principles, signalcharacteristics, and data distribution may hinder their practical performanceand versatility in MRI-specific applications. Here, we propose Triad, a visionfoundation model for 3D MRI. Triad adopts a widely used autoencoderarchitecture to learn robust representations from 131,170 3D MRI volumes anduses organ-independent imaging descriptions to constrain the semanticdistribution of the visual modality. The above pre-training dataset is calledTriad-131K, which is currently the largest 3D MRI pre-training dataset. Weevaluate Triad across three tasks, namely, organ/tumor segmentation,organ/cancer classification, and medical image registration, in two datamodalities (within-domain and out-of-domain) settings using 25 downstreamdatasets. By initializing models with Triad's pre-trained weights, nnUNet-Triadimproves segmentation performance by 6.88% compared to nnUNet-Scratch across 17datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch inclassification tasks across five datasets. SwinUNETR-Triad improves by 4.00%compared to SwinUNETR-Scratch in registration tasks across two datasets. Ourstudy demonstrates that pre-training can maximize performance when the datamodalities and organs of upstream and downstream tasks are consistent.</description>
      <author>example@mail.com (Shansong Wang, Mojtaba Safari, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, Justin Roper, David S. Yu, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2502.14064v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data</title>
      <link>http://arxiv.org/abs/2502.14044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Code: https://github.com/sycny/SelfSynthX&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个新颖的视觉拒绝采样框架，利用自合成数据提升大型多模态模型（LMMs）的认知和可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型在广泛的任务中表现出色，但在细粒度视觉推理上存在困难，难以提供充分合理的预测解释。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来改进大型多模态模型的细粒度视觉推理能力及其对特定领域目标的理解和可解释性。&lt;h4&gt;方法&lt;/h4&gt;通过合成易于解读的答案，这些答案包含基于专家定义的概念的人类可验证的视觉特征。在每一轮微调后使用无奖励模型过滤机制选出最高质量的易解答案以进行下一次微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法有效提高了特定视觉分类任务的准确性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过不断迭代的数据合成和微调，逐步提高大型多模态模型生成准确合理解释的能力。&lt;h4&gt;翻译&lt;/h4&gt;大规模多模态模型在一系列视觉任务中表现出色。然而，它们往往难以进行细粒度的视觉推理，并且无法提供合理的预测说明。为了解决这个问题，我们提出了一种新的基于自合成数据改进大模型认知能力和可解释性的视觉拒绝采样框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large multimodal models (LMMs) have shown impressive capabilities in a widerange of visual tasks. However, they often struggle with fine-grained visualreasoning, failing to identify domain-specific objectives and providejustifiable explanations for their predictions. To address this, we propose anovel visual rejection sampling framework to improve the cognition andexplainability of LMMs using self-synthesized data. Specifically, visualfine-tuning requires images, queries, and target answers. Our approach beginsby synthesizing interpretable answers that include human-verifiable visualfeatures. These features are based on expert-defined concepts, carefullyselected based on their alignment with the image content. After each round offine-tuning, we apply a reward model-free filtering mechanism to select thehighest-quality interpretable answers for the next round of tuning. Thisiterative process of data synthesis and fine-tuning progressively improves themodel's ability to generate accurate and reasonable explanations. Experimentalresults demonstrate the effectiveness of our method in improving both theaccuracy and explainability of specialized visual classification tasks.</description>
      <author>example@mail.com (Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, Ninghao Liu)</author>
      <guid isPermaLink="false">2502.14044v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>A Method to Simultaneously Facilitate All Jet Physics Tasks</title>
      <link>http://arxiv.org/abs/2502.14652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习在喷射物理中发挥着关键作用，特别是在处理高维数据的复杂性方面。通过专门设计的机器学习模型进行特定任务训练后，可以改进所有其他喷射物理学任务的准确度、精确性和速度。&lt;h4&gt;背景&lt;/h4&gt;由于喷射具有复杂的高维度性质，人工无法全面分析其特性，而神经网络能够从整体上探索这些特性。&lt;h4&gt;目的&lt;/h4&gt;展示专门构建的机器学习模型在特定喷射分类任务上的训练可以提高所有其他喷射物理学任务的表现，并介绍OmniLearn方法作为一个基础模型应用于喷射物理领域。&lt;h4&gt;方法&lt;/h4&gt;通过使用特定多类生成和分类任务进行训练，然后将学到的表示用于不同类型的生成和分类任务、具有不同的探测器模拟数据集的任务、来自不同碰撞系统的喷射任务（pp与ep）、似然比估计以及异常检测中。&lt;h4&gt;主要发现&lt;/h4&gt;专门构建的机器学习模型可以提高所有其他喷射物理任务的表现，且OmniLearn方法作为一个通用的基础模型适用于需要高精度分析的情况。&lt;h4&gt;结论&lt;/h4&gt;OmniLearn方法作为一种基础模型在任何需要顶尖精确度的涉及喷射及其结构分析的应用中公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning has become an essential tool in jet physics. Due to theircomplex, high-dimensional nature, jets can be explored holistically by neuralnetworks in ways that are not possible manually. However, innovations in allareas of jet physics are proceeding in parallel. We show that speciallyconstructed machine learning models trained for a specific jet classificationtask can improve the accuracy, precision, or speed of all other jet physicstasks. This is demonstrated by training on a particular multiclass generationand classification task and then using the learned representation for differentgeneration and classification tasks, for datasets with a different (full)detector simulation, for jets from a different collision system (pp versus ep),for generative models, for likelihood ratio estimation, and for anomalydetection. We consider, our OmniLearn approach thus as a jet-physics foundationmodel. It is made publicly available for use in any area where state-of-the-artprecision is required for analyses involving jets and their substructure.</description>
      <author>example@mail.com (Vinicius Mikuni, Benjamin Nachman)</author>
      <guid isPermaLink="false">2502.14652v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>SelfAge: Personalized Facial Age Transformation Using Self-reference Images</title>
      <link>http://arxiv.org/abs/2502.13987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于扩散模型的个性化面部年龄变换方法，旨在生成更接近个人实际年龄变化特征的人脸图像。&lt;h4&gt;背景&lt;/h4&gt;现有的深度学习方法虽然能够产生自然的老化效果，但无法准确反映个人因生活经历而产生的独特老化特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以利用少量自参考图片进行个性化训练的扩散模型，以实现更加个性化的面部年龄变换。&lt;h4&gt;方法&lt;/h4&gt;通过引入自我参照图像作为额外监督信息，对预训练的扩散模型进行微调，并设计有效的提示来提高效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在定量和定性评估中均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够更好地保留个人身份的同时实现更加自然且个性化的面部年龄变化。&lt;h4&gt;翻译&lt;/h4&gt;面部图像的年龄转换是一种编辑与年龄相关的人脸外观的技术，在保持个人识别性的前提下进行。现有的基于深度学习的方法可以再现自然的衰老过程；然而，它们只能再现平均化的过渡效果，而无法考虑到由于生活经历影响的独特个体特征。在本文中，我们提出了第一个基于扩散模型的个性化年龄变换方法。我们的扩散模型以面部图像和目标年龄作为输入，并生成经过年龄编辑后的面部图像输出。为了反映个人特定特征，我们将使用自我参照图片（即同一人的不同年龄段的照片）作为额外监督信息来微调预训练的扩散模型。我们还设计了一种有效的提示来增强年龄编辑效果和身份保存能力。实验结果表明，在定量与定性评估中，我们的方法都优于现有方法。代码及预训练模型可在 https://github.com/shiiiijp/SelfAge 查找。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Age transformation of facial images is a technique that edits age-relatedperson's appearances while preserving the identity. Existing deeplearning-based methods can reproduce natural age transformations; however, theyonly reproduce averaged transitions and fail to account for individual-specificappearances influenced by their life histories. In this paper, we propose thefirst diffusion model-based method for personalized age transformation. Ourdiffusion model takes a facial image and a target age as input and generates anage-edited face image as output. To reflect individual-specific features, weincorporate additional supervision using self-reference images, which arefacial images of the same person at different ages. Specifically, we fine-tunea pretrained diffusion model for personalized adaptation using approximately 3to 5 self-reference images. Additionally, we design an effective prompt toenhance the performance of age editing and identity preservation. Experimentsdemonstrate that our method achieves superior performance both quantitativelyand qualitatively compared to existing methods. The code and the pretrainedmodel are available at https://github.com/shiiiijp/SelfAge.</description>
      <author>example@mail.com (Taishi Ito, Yuki Endo, Yoshihiro Kanamori)</author>
      <guid isPermaLink="false">2502.13987v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>VB-Com: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception</title>
      <link>http://arxiv.org/abs/2502.14814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种复合框架VB-Com，该框架使类人机器人能够在感知不足的情况下决定何时依赖视觉策略和何时切换到盲策略。&lt;h4&gt;背景&lt;/h4&gt;腿足运动性能与状态观察的准确性和全面性密切相关。仅依靠本体感觉的盲策略被认为高度可靠，但限制了行走速度，并且经常需要通过碰撞地形来适应。相比之下，视觉策略允许机器人提前规划动作并积极应对未结构化的地形，但由于真实环境中的噪音、传感器故障以及当前模拟中动态或可变形地形的局限性，感知常常受到影响。&lt;h4&gt;目的&lt;/h4&gt;为了利用视觉策略和盲策略的优势，该研究旨在开发一种能够帮助类人机器人在感知不足的情况下做出决策的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为VB-Com的新框架，使类人机器人能够在动态或不可预测地形中进行导航时决定何时依赖于视觉信息，以及何时切换到仅使用本体感觉的策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，VB-Com能够帮助类人机器人克服由动态环境或感知噪声引起的障碍，成功穿越具有挑战性的地形和障碍物。&lt;h4&gt;结论&lt;/h4&gt;通过结合视觉策略与盲策略的优点，VB-Com框架提供了一种更鲁棒、高效的解决方案来应对真实世界中的复杂挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of legged locomotion is closely tied to the accuracy andcomprehensiveness of state observations. Blind policies, which rely solely onproprioception, are considered highly robust due to the reliability ofproprioceptive observations. However, these policies significantly limitlocomotion speed and often require collisions with the terrain to adapt. Incontrast, Vision policies allows the robot to plan motions in advance andrespond proactively to unstructured terrains with an online perception module.However, perception is often compromised by noisy real-world environments,potential sensor failures, and the limitations of current simulations inpresenting dynamic or deformable terrains. Humanoid robots, with high degreesof freedom and inherently unstable morphology, are particularly susceptible tomisguidance from deficient perception, which can result in falls or terminationon challenging dynamic terrains. To leverage the advantages of both vision andblind policies, we propose VB-Com, a composite framework that enables humanoidrobots to determine when to rely on the vision policy and when to switch to theblind policy under perceptual deficiency. We demonstrate that VB-Comeffectively enables humanoid robots to traverse challenging terrains andobstacles despite perception deficiencies caused by dynamic terrains orperceptual noise.</description>
      <author>example@mail.com (Junli Ren, Tao Huang, Huayi Wang, Zirui Wang, Qingwei Ben, Jiangmiao Pang, Ping Luo)</author>
      <guid isPermaLink="false">2502.14814v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Planning, scheduling, and execution on the Moon: the CADRE technology demonstration mission</title>
      <link>http://arxiv.org/abs/2502.14803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be presented at AAMAS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;NASA的CADRE任务计划于2025/2026年飞往月球Reiner Gamma地区，旨在演示多代理自主探索月球表面和次表层。该团队包括三台机器人和一个基站，将自主地在着陆器附近的一个区域内进行探索。&lt;h4&gt;背景&lt;/h4&gt;NASA的CADRE任务的目标是展示多个自治机器人如何协同工作来收集数据并绘制月球表面及其地下部分的地图。&lt;h4&gt;目的&lt;/h4&gt;演示多代理自主探索技术和分布式规划、调度与执行系统的功能。该系统确保在没有人类输入的情况下，机器人能够高效且安全地执行各种任务。&lt;h4&gt;方法&lt;/h4&gt;CADRE的软件架构基于一个新颖的自治、分布式的计划、调度和执行（PS&amp;E）系统。此系统采用集中式规划和分布式执行的概念，并利用选举领导者机制来增强系统的鲁棒性，确保在个别代理失效时仍能继续正常运作。&lt;h4&gt;主要发现&lt;/h4&gt;论文描述了CADRE PS&amp;E系统的架构、设计理由以及该系统在硬件上的验证与测试情况。&lt;h4&gt;结论&lt;/h4&gt;准备将PS&amp;E系统部署到月球上进行实际操作，并通过此次任务展示其有效性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE)mission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, isdesigned to demonstrate multi-agent autonomous exploration of the Lunar surfaceand sub-surface. A team of three robots and a base station will autonomouslyexplore a region near the lander, collecting the data required for 3Dreconstruction of the surface with no human input; and then autonomouslyperform distributed sensing with multi-static ground penetrating radars (GPR),driving in formation while performing coordinated radar soundings to create amap of the subsurface. At the core of CADRE's software architecture is a novelautonomous, distributed planning, scheduling, and execution (PS&amp;E) system. Thesystem coordinates the robots' activities, planning and executing tasks thatrequire multiple robots' participation while ensuring that each individualrobot's thermal and power resources stay within prescribed bounds, andrespecting ground-prescribed sleep-wake cycles. The system uses acentralized-planning, distributed-execution paradigm, and a leader electionmechanism ensures robustness to failures of individual agents. In this paper,we describe the architecture of CADRE's PS&amp;E system; discuss its designrationale; and report on verification and validation (V&amp;V) testing of thesystem on CADRE's hardware in preparation for deployment on the Moon.</description>
      <author>example@mail.com (Gregg Rabideau, Joseph Russino, Andrew Branch, Nihal Dhamani, Tiago Stegun Vaquero, Steve Chien, Jean-Pierre de la Croix, Federico Rossi)</author>
      <guid isPermaLink="false">2502.14803v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</title>
      <link>http://arxiv.org/abs/2502.14795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Humanoid-VLA框架，用于解决现有类人机器人控制框架在自主交互能力和数据稀缺性方面的局限性。该框架结合了语言理解、第一视角场景感知和运动控制。&lt;h4&gt;背景&lt;/h4&gt;当前的人形机器人控制框架主要依赖于反应机制，并且由于缺乏足够的数据而难以实现自主互动能力。&lt;h4&gt;目的&lt;/h4&gt;提出Humanoid-VLA框架，旨在通过整合多方面的能力来增强人形机器人的交互能力和环境适应性。&lt;h4&gt;方法&lt;/h4&gt;首先使用非第一视角的配有人类动作描述的数据集进行语言和运动的预对齐；然后引入参数高效的视频条件微调以结合第一视觉上下文；最后采用一种自监督数据增强策略生成伪注释，将原始运动序列转化为问题回答对。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于全身控制架构的人形机器人VLA框架能够执行对象交互和环境探索任务，并且具备更强的语境感知能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合语言理解、场景感知及运动控制，Humanoid-VLA展示了更接近人类的表现，即能够进行适应性和智能性的互动，从而克服了当前人形机器人在自主性方面的限制。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上述内容为其对应中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the limitations of current humanoid robot controlframeworks, which primarily rely on reactive mechanisms and lack autonomousinteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novelframework that integrates language understanding, egocentric scene perception,and motion control, enabling universal humanoid control. Humanoid-VLA beginswith language-motion pre-alignment using non-egocentric human motion datasetspaired with textual descriptions, allowing the model to learn universal motionpatterns and action semantics. We then incorporate egocentric visual contextthrough a parameter efficient video-conditioned fine-tuning, enablingcontext-aware motion generation. Furthermore, we introduce a self-superviseddata augmentation strategy that automatically generates pseudoannotationsdirectly derived from motion data. This process converts raw motion sequencesinto informative question-answer pairs, facilitating the effective use oflarge-scale unlabeled video data. Built upon whole-body control architectures,extensive experiments show that Humanoid-VLA achieves object interaction andenvironment exploration tasks with enhanced contextual awareness, demonstratinga more human-like capacity for adaptive and intelligent engagement.</description>
      <author>example@mail.com (Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2502.14795v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Coordination across Diverse Applications: A Survey</title>
      <link>http://arxiv.org/abs/2502.14743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文综述了多智能体系统（MAS）中的协调研究现状，通过回答四个基本的协调问题来提供统一的理解。&lt;h4&gt;背景&lt;/h4&gt;随着新兴应用和快速的人工智能发展，对多代理系统中趋势传播的基本机制的研究受到了越来越多的关注。&lt;h4&gt;目的&lt;/h4&gt;探索现有的协调思想和技术，并指出不同应用程序之间的联系以及未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;首先识别并分析了在各种应用程序中都至关重要的一般性协调问题。其次，综述了一系列的MAS应用案例。最后，分析和讨论了关于可扩展性、异质性和学习机制等开放挑战。&lt;h4&gt;主要发现&lt;/h4&gt;指出了分层与去中心化协调结合、人机协作以及基于大语言模型（LLM）的多智能体系统作为未来的有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;通过统一的理解，明确了当前MAS协调研究的状态，并为未来提出了具有潜力的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent coordination studies the underlying mechanism enabling thetrending spread of diverse multi-agent systems (MAS) and has receivedincreasing attention, driven by the expansion of emerging applications andrapid AI advances. This survey outlines the current state of coordinationresearch across applications through a unified understanding that answers fourfundamental coordination questions: (1) what is coordination; (2) whycoordination; (3) who to coordinate with; and (4) how to coordinate. Ourpurpose is to explore existing ideas and expertise in coordination and theirconnections across diverse applications, while identifying and highlightingemerging and promising research directions. First, general coordinationproblems that are essential to varied applications are identified and analyzed.Second, a number of MAS applications are surveyed, ranging from widely studieddomains, e.g., search and rescue, warehouse automation and logistics, andtransportation systems, to emerging fields including humanoid andanthropomorphic robots, satellite systems, and large language models (LLMs).Finally, open challenges about the scalability, heterogeneity, and learningmechanisms of MAS are analyzed and discussed. In particular, we identify thehybridization of hierarchical and decentralized coordination, human-MAScoordination, and LLM-based MAS as promising future directions.</description>
      <author>example@mail.com (Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, Yang Shen)</author>
      <guid isPermaLink="false">2502.14743v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Performance Scores: Directed Functional Connectivity as a Brain-Based Biomarker for Motor Skill Learning and Retention</title>
      <link>http://arxiv.org/abs/2502.14731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于脑电图（EEG）的定向功能连接性（dFC）作为新的生物标记物，用于评估运动技能学习和保持阶段。研究通过应用Fitts和Posner模型的不同阶段来展示dFC在神经机制中的作用，并展示了其在整个训练过程中及六周停训期后的稳定性和有效性。&lt;h4&gt;背景&lt;/h4&gt;传统的性能指标如执行时间和错误率对捕捉复杂的任务序列学习过程中的神经机制提供了有限的见解。这些指标难以深入理解技能习得和保持背后的认知变化。&lt;h4&gt;目的&lt;/h4&gt;引入基于EEG的dFC作为新的生物标记物，用以更好地评估运动技能的学习与保留情况，并提供对神经机制的新视角。&lt;h4&gt;方法&lt;/h4&gt;应用定向功能连接性（dFC）来映射Fitts和Posner模型的不同阶段，同时对比对照组，确保观察到的变化是由于训练而非其他因素引起的。&lt;h4&gt;主要发现&lt;/h4&gt;1. dFC能够有效地识别并追踪通过Fitts和Posner模型各个学习阶段的进展。2. 与传统方法相比，dFC捕捉到了神经信息流动的方向性和强度，提供了更全面的理解。3. 在六周停训期内，观察到dFC具有较高的稳定性，表明其在长期保持中的监测作用。4. 对照组未显示出显著变化，进一步确认了训练引起的特定神经适应性。&lt;h4&gt;结论&lt;/h4&gt;dFC作为一种强有力的生物标记物补充了传统的性能指标，对运动技能学习和保留提供了更深入的理解。它有助于个性化、靶向的训练协议的发展，特别是在外科教育等领域中至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容为中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor skill acquisition in fields like surgery, robotics, and sports involveslearning complex task sequences through extensive training. Traditionalperformance metrics, like execution time and error rates, offer limited insightas they fail to capture the neural mechanisms underlying skill learning andretention. This study introduces directed functional connectivity (dFC),derived from electroencephalography (EEG), as a novel brain-based biomarker forassessing motor skill learning and retention. For the first time, dFC isapplied as a biomarker to map the stages of the Fitts and Posner motor learningmodel, offering new insights into the neural mechanisms underlying skillacquisition and retention. Unlike traditional measures, it captures both thestrength and direction of neural information flow, providing a comprehensiveunderstanding of neural adaptations across different learning stages. Theanalysis demonstrates that dFC can effectively identify and track theprogression through various stages of the Fitts and Posner model. Furthermore,its stability over a six-week washout period highlights its utility inmonitoring long-term retention. No significant changes in dFC were observed ina control group, confirming that the observed neural adaptations were specificto training and not due to external factors. By offering a granular view of thelearning process at the group and individual levels, dFC facilitates thedevelopment of personalized, targeted training protocols aimed at enhancingoutcomes in fields where precision and long-term retention are critical, suchas surgical education. These findings underscore the value of dFC as a robustbiomarker that complements traditional performance metrics, providing a deeperunderstanding of motor skill learning and retention.</description>
      <author>example@mail.com (Anil Kamat, Rahul Rahul, Lora Cavuoto, Harry Burke, Matthew Hackett, Jack Norfleet, Steven Schwaitzberg, Suvranu De)</author>
      <guid isPermaLink="false">2502.14731v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.14684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;3D高斯点阵（3DGS）在新颖视图合成中表现出色，但其几何准确性受限于缺乏明确的几何约束。本文提出了一种新的方法CDGS来增强3DGS。&lt;h4&gt;背景&lt;/h4&gt;3DGS在渲染速度和图像质量方面具有优势，但在三维重建中的几何精度受到限制。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种基于深度正则化的信心感知方法（CDGS）以提高3DGS的几何细节保持能力和几何准确度。&lt;h4&gt;方法&lt;/h4&gt;使用多线索信心图来自单目深度估计和稀疏的结构从运动深度来适应性地调整优化过程中的深度监督。&lt;h4&gt;主要发现&lt;/h4&gt;在新颖视图合成的质量和几何精度方面都取得了竞争性的性能，特别是在训练早期阶段改善了细节保持能力，并且在Tanks and Temples基准数据集上的实验表明，该方法可以实现更稳定的收敛行为及更高的几何重建准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅提高了3DGS的效率和准确度，还可能为数字孪生创建、文化遗产保护或林业应用等现实世界中的高效和精确三维重建系统的发展提供帮助。&lt;h4&gt;翻译&lt;/h4&gt;摘要：3D高斯点阵（3DGS）在新颖视图合成中表现出显著的优势，特别是在实现高速渲染和高质量结果方面。然而，由于优化过程中缺乏明确的几何约束，其在三维重建中的几何准确性仍然有限。本文介绍了一种新的方法CDGS来增强3DGS。我们利用单目深度估计的多线索信心图以及稀疏结构从运动深度，在优化过程中自适应地调整深度监督。我们的方法展示了在训练早期阶段改善了细节保持能力，并且实现了新颖视图合成质量和几何精度方面的竞争性性能。在公开可用的Tanks and Temples基准数据集上的实验表明，该方法可以实现更稳定的收敛行为及更高的几何重建准确性，在PSNR方面提升了最多2.31 dB，而M3C2距离度量中的几何误差更低。值得注意的是，我们的方法仅用50%的训练迭代次数就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于开发高效的三维重建系统用于现实世界的应用，如数字孪生创建、文化遗产保护或林业应用等。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has shown significant advantages in novel viewsynthesis (NVS), particularly in achieving high rendering speeds andhigh-quality results. However, its geometric accuracy in 3D reconstructionremains limited due to the lack of explicit geometric constraints duringoptimization. This paper introduces CDGS, a confidence-aware depthregularization approach developed to enhance 3DGS. We leverage multi-cueconfidence maps of monocular depth estimation and sparse Structure-from-Motiondepth to adaptively adjust depth supervision during the optimization process.Our method demonstrates improved geometric detail preservation in earlytraining stages and achieves competitive performance in both NVS quality andgeometric accuracy. Experiments on the publicly available Tanks and Templesbenchmark dataset show that our method achieves more stable convergencebehavior and more accurate geometric reconstruction results, with improvementsof up to 2.31 dB in PSNR for NVS and consistently lower geometric errors inM3C2 distance metrics. Notably, our method reaches comparable F-scores to theoriginal 3DGS with only 50% of the training iterations. We expect this workwill facilitate the development of efficient and accurate 3D reconstructionsystems for real-world applications such as digital twin creation, heritagepreservation, or forestry applications.</description>
      <author>example@mail.com (Qilin Zhang, Olaf Wysocki, Steffen Urban, Boris Jutzi)</author>
      <guid isPermaLink="false">2502.14684v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Structure-from-Sherds++: Robust Incremental 3D Reassembly of Axially Symmetric Pots from Unordered and Mixed Fragment Collections</title>
      <link>http://arxiv.org/abs/2502.13986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种用于重新组装多个轴对称陶器的高效方法，该方法基于逐片迭代注册技术，通过利用多图束搜索来探索多种注册路径，并过滤出无法区分的虚假匹配。&lt;h4&gt;背景&lt;/h4&gt;碎片化陶片重新组装对于文化遗产保护至关重要，但薄且锋利的断裂面导致大量错误匹配，使得大规模拼图解决变得困难。现有的全局方法在处理多重混杂时面临局部极小值和可扩展性问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的重组成型方法，以克服现有技术挑战并提高轴对称陶器碎片重新组装的成功率。&lt;h4&gt;方法&lt;/h4&gt;受到基于多图像的三维重建结构从运动（SfM）方法启发，我们开发了一种基于逐片迭代注册的新方法Structure-from-Sherds++ (SfS++)。此方法通过探索多种可能的匹配路径来过滤错误匹配，并且不需要先验信息如底座或混合物体的数量。&lt;h4&gt;主要发现&lt;/h4&gt;在包含142个真实碎片和来自10种不同陶器的数据集上，我们的方法实现了87%的重新组装准确率，优于其他处理复杂裂纹图案和混杂数据的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的SfS++方法展示了其在文化遗产保护中的潜力，特别是在处理混合轴对称陶器碎片时。它克服了现有技术的局限性，并且无需任何先验信息即可有效重新组装多个物体。&lt;h4&gt;翻译&lt;/h4&gt;重新组装破碎的具有轴向对称性的陶器对于文化遗迹保存极为重要，但由于这些陶器碎片薄而尖锐，容易产生大量误匹配，使得大规模拼图解决成为一项巨大挑战。现有的全局方法和数据驱动模型在处理复杂情况时易陷入局部最小值，并且面对多重混杂的数据集时可扩展性较差。受结构从运动(SfM)技术的启发，我们提出了一种基于逐片迭代注册的重新组装方法Structure-from-Sherds++ (SfS++)，能够有效过滤出无法区分的误匹配并同时重构多个陶器而不需任何先验信息。在包含142个真实碎片和来自十种不同陶器的数据集上，我们的方法实现了87%的成功率，优于其他处理复杂裂纹图案与混合数据的方法，并且达到了当前的最佳水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reassembling multiple axially symmetric pots from fragmentary sherds iscrucial for cultural heritage preservation, yet it poses significant challengesdue to thin and sharp fracture surfaces that generate numerous false positivematches and hinder large-scale puzzle solving. Existing global approaches,which optimize all potential fragment pairs simultaneously or data-drivenmodels, are prone to local minima and face scalability issues when multiplepots are intermixed. Motivated by Structure-from-Motion (SfM) for 3Dreconstruction from multiple images, we propose an efficient reassembly methodfor axially symmetric pots based on iterative registration of one sherd at atime, called Structure-from-Sherds++ (SfS++). Our method extends beyond simplereplication of incremental SfM and leverages multi-graph beam search to exploremultiple registration paths. This allows us to effectively filter outindistinguishable false matches and simultaneously reconstruct multiple potswithout requiring prior information such as base or the number of mixedobjects. Our approach achieves 87% reassembly accuracy on a dataset of 142 realfragments from 10 different pots, outperforming other methods in handlingcomplex fracture patterns with mixed datasets and achieving state-of-the-artperformance. Code and results can be found in our project pagehttps://sj-yoo.info/sfs/.</description>
      <author>example@mail.com (Seong Jong Yoo, Sisung Liu, Muhammad Zeeshan Arshad, Jinhyeok Kim, Young Min Kim, Yiannis Aloimonos, Cornelia Fermuller, Kyungdon Joo, Jinwook Kim, Je Hyeong Hong)</author>
      <guid isPermaLink="false">2502.13986v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO</title>
      <link>http://arxiv.org/abs/2502.14669v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的两阶段训练框架，旨在为标准的大型语言模型（LLMs）赋予空间视觉推理能力，特别是在迷宫导航任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;尽管大模型在语言处理方面表现出色，但在需要真实的空间视觉推理的任务上存在困难。&lt;h4&gt;目的&lt;/h4&gt;通过一种创新的方法来增强现有语言模型的空间推理能力，特别是用于解决迷宫导航问题。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '使用监督微调（SFT）在包含标记化迷宫表示的数据集上训练模型以预测步骤指令', '第二阶段': '应用集团相对策略优化（GRPO），通过精心设计的奖励函数进一步提高模型的序列决策能力'}&lt;h4&gt;主要发现&lt;/h4&gt;{'基准测试结果': '基础模型无法导航迷宫，而SFT微调后的模型达到了86%的准确性', '增强效果': '进一步应用GRPO后，模型准确率提升至93%，显示出更稳健且自我纠正的空间推理能力'}&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了将语言模型与视觉空间任务相结合的巨大潜力，并为机器人、自主导航等领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在处理自然语言方面表现出色，但在需要真实空间视觉推理的任务中却表现不佳。本论文提出了一种创新的两阶段训练框架，旨在增强标准LLMs的空间视觉推理能力，特别是在迷宫导航任务上。该方法包括使用监督微调（SFT）和集团相对策略优化（GRPO），并在合成生成的迷宫实验中取得了86%至93%的准确率提升。研究结果表明，通过适当的技术改进，可以显著提高语言模型处理视觉空间问题的能力，并为机器人技术、自主导航等领域的应用提供了新的可能路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated impressive capabilities inlanguage processing, yet they often struggle with tasks requiring genuinevisual spatial reasoning. In this paper, we introduce a novel two-stagetraining framework designed to equip standard LLMs with visual reasoningabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)on a curated dataset of tokenized maze representations to teach the model topredict step-by-step movement commands. Next, we apply Group Relative PolicyOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully craftedreward function to refine the model's sequential decision-making and encourageemergent chain-of-thought behaviors. Experimental results on syntheticallygenerated mazes show that while a baseline model fails to navigate the maze,the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuningboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters morerobust and self-corrective reasoning, highlighting the potential of ourapproach to bridge the gap between language models and visual spatial tasks.These findings offer promising implications for applications in robotics,autonomous navigation, and other domains that require integrated visual andsequential reasoning.</description>
      <author>example@mail.com (Alan Dao, Dinh Bach Vu)</author>
      <guid isPermaLink="false">2502.14669v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion</title>
      <link>http://arxiv.org/abs/2502.14616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA(2025). The code is accessible through:  https://github.com/L-J-Yuan/MODEST&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种用于透明物体分割和深度估计的单目框架，该框架利用单一图像输入，并通过语义与几何融合模块及迭代策略优化预测结果。&lt;h4&gt;背景&lt;/h4&gt;透明物体的感知对于许多机器人任务至关重要。然而，由于其复杂的光学特性，准确地对透明物体进行分割并估算深度仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在只使用单张图像输入的情况下，在透明对象的分割和深度估计方面表现卓越的方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新颖的语义与几何融合模块，有效整合了任务之间的多尺度信息，并采用迭代策略逐步优化初始特征以获得更清晰的结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该模型在两个具有挑战性的合成数据集和真实世界数据集中超越了现有的单目、立体以及多视角方法，改进幅度达到了约38.8%-46.2%。&lt;h4&gt;结论&lt;/h4&gt;提出的框架是第一个能够在单一图像输入下同时优化透明物体分割与深度估计的方法，并且其效果显著优于现有技术。&lt;h4&gt;翻译&lt;/h4&gt;透明物体感知对于许多机器人任务来说至关重要。然而，由于复杂的光学特性，准确地对透明物体进行分割并估算其深度仍然具有挑战性。现有的方法主要专注于单独的任务，使用额外的输入或专用传感器，忽略了任务之间的有价值交互以及后续精炼过程，导致预测结果模糊且不理想。为了解决这些问题，我们提出了一种单目框架，这是第一个在仅用单张图像作为输入的情况下，在透明物体分割和深度估计方面表现出色的方法。具体来说，我们设计了一个新颖的语义与几何融合模块，有效整合了多尺度信息之间的任务，并受到人类感知对象方式的启发，进一步采用了迭代策略，逐步优化初始特征以获得更清晰的结果。在两个具有挑战性的合成数据集和真实世界数据集中进行的实验表明，我们的模型超越了现有的单目、立体以及多视角方法，在只有单一RGB输入的情况下，性能提高了约38.8%-46.2%。相关代码和模型可在https://github.com/L-J-Yuan/MODEST公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transparent object perception is indispensable for numerous robotic tasks.However, accurately segmenting and estimating the depth of transparent objectsremain challenging due to complex optical properties. Existing methodsprimarily delve into only one task using extra inputs or specialized sensors,neglecting the valuable interactions among tasks and the subsequent refinementprocess, leading to suboptimal and blurry predictions. To address these issues,we propose a monocular framework, which is the first to excel in bothsegmentation and depth estimation of transparent objects, with only asingle-image input. Specifically, we devise a novel semantic and geometricfusion module, effectively integrating the multi-scale information betweentasks. In addition, drawing inspiration from human perception of objects, wefurther incorporate an iterative strategy, which progressively refines initialfeatures for clearer results. Experiments on two challenging synthetic andreal-world datasets demonstrate that our model surpasses state-of-the-artmonocular, stereo, and multi-view methods by a large margin of about38.8%-46.2% with only a single RGB input. Codes and models are publiclyavailable at https://github.com/L-J-Yuan/MODEST.</description>
      <author>example@mail.com (Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou)</author>
      <guid isPermaLink="false">2502.14616v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Real-world Troublemaker: A Novel Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios</title>
      <link>http://arxiv.org/abs/2502.14574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,14 figures,2tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个名为Real-world Troublemaker的新型测试框架，用于生成对抗性目标对象运动轨迹，并促进被测车辆与环境之间的智能交互。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶系统的轨道测试场景通常是固定的和有限的，这是由于物体控制方法缺乏灵活性以及缺乏智能化互动行为造成的。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够产生对抗性目标物运动轨迹并促进车辆与环境间智能互动的新框架，从而创建更加真实且动态的测试环境。&lt;h4&gt;方法&lt;/h4&gt;利用云控制系统远程动态地操控对象以模拟真实的交通场景，并引入游戏理论结构下的互动具体场景生成法来实现智能化交互。&lt;h4&gt;主要发现&lt;/h4&gt;在同济大学智能网联汽车测评基地成功实施了此框架，结果显示它能够准确有效地执行动态交互测试。与传统方法相比，Troublemaker提高了场景再现精度65.2%，增加了目标车辆互动策略的多样性大约9.2倍，并将无保护左转安全临界情景的曝光频率提高3.5倍。&lt;h4&gt;结论&lt;/h4&gt;Real-world Troublemaker框架克服了现有自动驾驶系统测试中物体控制和智能交互方面的局限性，提供了更准确、高效且多样的场景再现能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Track testing plays a critical role in the safety evaluation of autonomousdriving systems (ADS), as it provides real-world object targets and asafety-controllable interaction environment. However, existing track testingscenarios are often pre-fixed and limited, primarily due to the inflexibilityof object target control methods and the lack of intelligent interactivebehaviors. To overcome this limitation, we propose a novel track testingframework, Real-world Troublemaker, which can generate adversarial objecttarget motion trajectories and facilitate intelligent interactions with thevehicle under test (VUT), creating a more realistic and dynamic testingenvironment. To enable flexible motion trajectories, cloud-controlledtechnology is utilized to remotely and dynamically control object targets tocreate a realistic traffic environment. To achieve intelligent interactions, aninteractive concrete scenario generation method is introduced within agame-theoretic structure. The proposed framework has been successfullyimplemented at the Tongji University Intelligent Connected Vehicle EvaluationBase. Field test results demonstrate that Troublemaker can perform dynamicinteractive testing of ADS accurately and effectively. Compared to traditionaltrack testing methods, Troublemaker improves scenario reproduction accuracy by65.2\%, increases the diversity of target vehicle interaction strategies byapproximately 9.2 times, and enhances exposure frequency of safety-criticalscenarios by 3.5 times in unprotected left-turn scenarios.</description>
      <author>example@mail.com (Xinrui Zhang, Lu Xiong, Peizhi Zhang, Junpeng Huang, Yining Ma)</author>
      <guid isPermaLink="false">2502.14574v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>A Mobile Robotic Approach to Autonomous Surface Scanning in Legal Medicine</title>
      <link>http://arxiv.org/abs/2502.14514v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted and accepted for presentation at CARS 2025. This preprint  has not undergone peer review or post-submission revisions. The final version  of this work will appear in the official CARS 2025 proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了一种移动机器人系统在法医学领域中的应用，该系统能够进行全身体表RGB-D扫描。&lt;h4&gt;背景&lt;/h4&gt;目前的法医文档记录主要依赖于手动操作，时间成本高且主观误差大。采用固定安装的机器人系统则需要大量空间和专用房间。&lt;h4&gt;目的&lt;/h4&gt;研究开发一种移动机器人系统用于尸体外部损伤的数字化文档记录，并评估其在实际应用中的有效性。&lt;h4&gt;方法&lt;/h4&gt;设计并实现了一种能够进行全身体表RGB-D扫描的移动机器人系统，通过实验室实验验证系统的环境配置参数及适用性。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在三个特定位置上使用该系统可以达到94.96%的身体覆盖范围，并且在模拟和实际尸体上的表面覆盖率分别为96.90%±3.16%和92.45%±1.43%，证明了系统的有效性。&lt;h4&gt;结论&lt;/h4&gt;移动机器人系统能够有效支持法医学中的RGB-D扫描，有助于提高文档记录的效率和自动化程度，并减少手动干预的需求。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：目的包括尸体内外部检查在内的全面法律医学文件记录通常由手动常规解剖过程中完成。特别地，外部伤口的数字化文档记录对于法医分析越来越重要。为此，引入了RGB表面扫描技术。然而，手持相机进行全表面扫描耗时且依赖操作者；而固定安装的机器人系统则需要大量空间和专用房间。因此，我们探讨了一种移动机器人系统的可行性用于尸体外层文档记录的方法开发：我们设计并实现了一个可全身RGB-D扫描的移动机器人系统，并通过实验室实验验证了其环境配置参数及实际应用效果。结果表明，在三个特定位置上的全身体表覆盖率为94.96%；模拟和真实尸体表面覆盖率分别为96.90±3.16%和92.45±1.43%，证明系统有效。结论：移动机器人系统的RGB-D扫描在法医领域显示出了极大的潜力，可以支持更高效的自动文档记录过程，并减少手动干预的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Comprehensive legal medicine documentation includes both an internalbut also an external examination of the corpse. Typically, this documentationis conducted manually during conventional autopsy. A systematic digitaldocumentation would be desirable, especially for the external examination ofwounds, which is becoming more relevant for legal medicine analysis. For thispurpose, RGB surface scanning has been introduced. While a manual full surfacescan using a handheld camera is timeconsuming and operator dependent, floor orceiling mounted robotic systems require substantial space and a dedicated room.Hence, we consider whether a mobile robotic system can be used for externaldocumentation. Methods: We develop a mobile robotic system that enablesfull-body RGB-D surface scanning. Our work includes a detailed configurationspace analysis to identify the environmental parameters that need to beconsidered to successfully perform a surface scan. We validate our findingsthrough an experimental study in the lab and demonstrate the system'sapplication in a legal medicine environment. Results: Our configuration spaceanalysis shows that a good trade-off between coverage and time is reached withthree robot base positions, leading to a coverage of 94.96 %. Experimentsvalidate the effectiveness of the system in accurately capturing body surfacegeometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43% for a body phantom and actual corpses, respectively. Conclusion: This workdemonstrates the potential of a mobile robotic system to automate RGB-D surfacescanning in legal medicine, complementing the use of post-mortem CT scans forinner documentation. Our results indicate that the proposed system cancontribute to more efficient and autonomous legal medicine documentation,reducing the need for manual intervention.</description>
      <author>example@mail.com (Sarah Grube, Sarah Latus, Martin Fischer, Vidas Raudonis, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer)</author>
      <guid isPermaLink="false">2502.14514v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control</title>
      <link>http://arxiv.org/abs/2502.14457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的基于强化学习的流水线，该流水线装备了可变阻抗控制和利用观测历史进行运动适应的方法，专门用于通用化的关节对象操纵。&lt;h4&gt;背景&lt;/h4&gt;关节物体操作相对于刚性物体操作具有独特的挑战，因为物体本身代表了一个动态环境。传统的视觉数据（RGBD/点云）通常作为策略输入直接使用，但这种做法会增加仿真到现实的差距。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的强化学习流水线，旨在实现零样本模拟到真实场景转换中的平滑且灵巧的动作操作。&lt;h4&gt;方法&lt;/h4&gt;[{'减少对视觉数据依赖': '通过现成模块提取有用的低维数据来间接使用视觉数据特征'}, {'利用观测历史': '推断物体运动及其内在属性以减轻仿真与现实的差距'}, {'阻抗控制': '在模拟和真实环境中均采用阻抗控制'}]&lt;h4&gt;主要发现&lt;/h4&gt;[{'训练设置': '设计了一个具有良好随机化和专门奖励系统的训练环境，使多阶段、端到端操作成为可能而无需启发式运动规划'}, {'实验结果': '通过广泛的未见过物体的实验，在真实世界中实现了84%的成功率，据我们所知这是首次报告的结果'}]&lt;h4&gt;结论&lt;/h4&gt;我们的策略是首个在广泛的真实对象上实现高效关节操纵的成功案例。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新颖的方法来解决关节物体操作的独特挑战，并通过广泛的实验验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Articulated object manipulation poses a unique challenge compared to rigidobject manipulation as the object itself represents a dynamic environment. Inthis work, we present a novel RL-based pipeline equipped with variableimpedance control and motion adaptation leveraging observation history forgeneralizable articulated object manipulation, focusing on smooth and dexterousmotion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap,our pipeline diminishes reliance on vision by not leveraging the vision datafeature (RGBD/pointcloud) directly as policy input but rather extracting usefullow-dimensional data first via off-the-shelf modules. Additionally, weexperience less sim-to-real gap by inferring object motion and its intrinsicproperties via observation history as well as utilizing impedance control bothin the simulation and in the real world. Furthermore, we develop awell-designed training setting with great randomization and a specializedreward system (task-aware and motion-aware) that enables multi-staged,end-to-end manipulation without heuristic motion planning. To the best of ourknowledge, our policy is the first to report 84\% success rate in the realworld via extensive experiments with various unseen objects.</description>
      <author>example@mail.com (Tan-Dzung Do, Nandiraju Gireesh, Jilong Wang, He Wang)</author>
      <guid isPermaLink="false">2502.14457v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Ground-aerial Transportation System for Pest Control Enabled by AI-based Autonomous Nano-UAVs</title>
      <link>http://arxiv.org/abs/2502.14455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;高效农作物生产需要早期检测害虫爆发并及时进行处理；本文提出了一种基于多架自主微型无人机（纳米UAV）的解决方案，用于视觉检测害虫，并由一辆较慢但功能强大的车辆运送处理物资。&lt;h4&gt;背景&lt;/h4&gt;农业生产中，及时发现和处理害虫爆发对于保证作物产量至关重要。然而，现有的方法在资源利用效率上存在不足。&lt;h4&gt;目的&lt;/h4&gt;设计一种基于纳米UAV的高效害虫监测与处理系统，以提高农作物生产的可持续性和经济效益。&lt;h4&gt;方法&lt;/h4&gt;['为应对纳米UAV上的极端限制（例如低分辨率传感器和计算能力），我们设计并优化了一个轻量级图像识别卷积神经网络(CNN)。该CNN在检测有害昆虫方面取得了0.79的平均精度(mAP)，同时减少了32倍的操作。', '为了处理田间意外障碍，采用了基于A*算法的全局+局部路径规划器。全局路径规划器确定纳米UAV的最佳飞行路线，而局部规划器可以达到50Hz的运行频率，通过调整近距离路径来防止碰撞。', '进行仿真实验，展示了一架25个纳米UAV组成的机队如何在200x200m的葡萄园中执行任务，并收集信息以优化拖拉机的最佳行进路线。']&lt;h4&gt;主要发现&lt;/h4&gt;['设计并实现了轻量级CNN，在低计算预算下，仍能实现高精度的害虫检测。', '提出了一种基于A*算法的路径规划方法，使纳米UAV能够避免障碍物，并高效完成飞行任务。', '实验表明，使用25架纳米UAV组成的机队可以比传统单一地面车辆节省高达20小时的工作时间。']&lt;h4&gt;结论&lt;/h4&gt;所提出的系统为农业生产提供了高效的害虫监测与处理方案，大幅提升了资源利用效率和作业速度。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3719210&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient crop production requires early detection of pest outbreaks andtimely treatments; we consider a solution based on a fleet of multipleautonomous miniaturized unmanned aerial vehicles (nano-UAVs) to visually detectpests and a single slower heavy vehicle that visits the detected outbreaks todeliver treatments. To cope with the extreme limitations aboard nano-UAVs,e.g., low-resolution sensors and sub-100 mW computational power budget, wedesign, fine-tune, and optimize a tiny image-based convolutional neural network(CNN) for pest detection. Despite the small size of our CNN (i.e., 0.58GOps/inference), on our dataset, it scores a mean average precision (mAP) of0.79 in detecting harmful bugs, i.e., 14% lower mAP but 32x fewer operationsthan the best-performing CNN in the literature. Our CNN runs in real-time at6.8 frame/s, requiring 33 mW on a GWT GAP9 System-on-Chip aboard a Crazyflienano-UAV. Then, to cope with in-field unexpected obstacles, we leverage aglobal+local path planner based on the A* algorithm. The global path plannerdetermines the best route for the nano-UAV to sweep the entire area, while thelocal one runs up to 50 Hz aboard our nano-UAV and prevents collision byadjusting the short-distance path. Finally, we demonstrate with in-simulatorexperiments that once a 25 nano-UAVs fleet has combed a 200x200 m vineyard,collected information can be used to plan the best path for the tractor,visiting all and only required hotspots. In this scenario, our efficienttransportation system, compared to a traditional single-ground vehicleperforming both inspection and treatment, can save up to 20 h working time.</description>
      <author>example@mail.com (Luca Crupi, Luca Butera, Alberto Ferrante, Alessandro Giusti, Daniele Palossi)</author>
      <guid isPermaLink="false">2502.14455v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2502.14420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为ChatVLA的新框架，用于解决视觉-语言-行动模型(VLA)中的两项关键挑战：虚假遗忘和任务干扰。该框架通过分阶段对齐训练和专家混合架构来克服这些问题，并展示了在多模态理解和机器人操作方面的优越性能。&lt;h4&gt;背景&lt;/h4&gt;人类具有感知、理解并与物理世界互动的统一认知能力，而大型语言模型难以复制这种综合理解能力。&lt;h4&gt;目的&lt;/h4&gt;探讨现有视觉-语言-行动模型中的训练模式存在的问题并提出改进方案。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地分析现有的VLA模型训练范式，识别了两个主要挑战：虚假遗忘和任务干扰。为解决这些问题，提出了一个称为ChatVLA的新框架，它包含分阶段对齐训练（Phased Alignment Training）以及专家混合架构(Mixture-of-Experts)。&lt;h4&gt;主要发现&lt;/h4&gt;ChatVLA在视觉问答数据集上表现出具有竞争力的性能，并且在多模态理解基准测试中显著超越了最先进的VLA方法。它以更加参数高效的架构设计，在MMMU和MMStar上的表现分别优于现有模型六倍和47.2%，并在25个实际机器人操作任务中超越了OpenVLA等现有的VLA方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了统一框架在实现强大的多模态理解和有效的机器人控制方面的潜力，这表明ChatVLA为这些目标提供了有效途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess a unified cognitive ability to perceive, comprehend, andinteract with the physical world. Why can't large language models replicatethis holistic understanding? Through a systematic analysis of existing trainingparadigms in vision-language-action models (VLA), we identify two keychallenges: spurious forgetting, where robot training overwrites crucialvisual-text alignments, and task interference, where competing control andunderstanding tasks degrade performance when trained jointly. To overcome theselimitations, we propose ChatVLA, a novel framework featuring Phased AlignmentTraining, which incrementally integrates multimodal data after initial controlmastery, and a Mixture-of-Experts architecture to minimize task interference.ChatVLA demonstrates competitive performance on visual question-answeringdatasets and significantly surpasses state-of-the-art vision-language-action(VLA) methods on multimodal understanding benchmarks. Notably, it achieves asix times higher performance on MMMU and scores 47.2% on MMStar with a moreparameter-efficient design than ECoT. Furthermore, ChatVLA demonstratessuperior performance on 25 real-world robot manipulation tasks compared toexisting VLA methods like OpenVLA. Our findings highlight the potential of ourunified framework for achieving both robust multimodal understanding andeffective robot control.</description>
      <author>example@mail.com (Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, Feifei Feng)</author>
      <guid isPermaLink="false">2502.14420v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images</title>
      <link>http://arxiv.org/abs/2502.14279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Australasian Conference on Robotics and  Automation, ACRA, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;单目深度估计是机器人感知的基础任务，近年来随着更准确和稳健的神经网络模型的发展以及不同类型数据集的应用，单目深度估计在性能和效率方面有了显著提升。然而，大多数相关研究集中在特定领域内，特别是在户外场景中的基准测试主要针对城市环境以改善自主驾驶设备，这与果园/葡萄园等农业环境差异巨大。&lt;h4&gt;背景&lt;/h4&gt;单目深度估计是机器人感知中的基础任务，并且该领域的进步依赖于更准确和稳健的神经网络模型的发展以及不同类型数据集的应用。然而，现有的研究大多集中在特定领域内（如城市环境），缺乏针对果园或葡萄园等农业环境的研究。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有单目深度估计方法在果园/葡萄园环境中性能不足的问题，提出一种填补该领域的空白的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的重训练方法，通过监控稠密深度图和稀疏点之间的连贯正则化来改进训练结果。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的 OrchardDepth 方法显著提高了单目相机在果园环境中的度量深度估计性能，将 RMSE 从 1.5337 减少到了 0.6738。&lt;h4&gt;结论&lt;/h4&gt;该研究通过提出一个新的方法和新的数据集填补了现有的单目深度估计模型中关于农业环境的空白，并证明了其有效性和应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth estimation is a rudimentary task in robotic perception.Recently, with the development of more accurate and robust neural networkmodels and different types of datasets, monocular depth estimation hassignificantly improved performance and efficiency. However, most of theresearch in this area focuses on very concentrated domains. In particular, mostof the benchmarks in outdoor scenarios belong to urban environments for theimprovement of autonomous driving devices, and these benchmarks have a massivedisparity with the orchard/vineyard environment, which is hardly helpful forresearch in the primary industry. Therefore, we propose OrchardDepth, whichfills the gap in the estimation of the metric depth of the monocular camera inthe orchard/vineyard environment. In addition, we present a new retrainingmethod to improve the training result by monitoring the consistentregularization between dense depth maps and sparse points. Our method improvesthe RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738,proving our method's validation.</description>
      <author>example@mail.com (Zhichao Zheng, Henry Williams, Bruce A MacDonald)</author>
      <guid isPermaLink="false">2502.14279v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation</title>
      <link>http://arxiv.org/abs/2502.14254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近，在大型语言模型（LLMs）和视觉-语言模型（VLMs）方面的进展使它们成为具身导航的强大工具，使得代理能够利用常识和空间推理来在不熟悉的环境中高效探索。&lt;h4&gt;背景&lt;/h4&gt;现有的基于LLM的方法将全局记忆，如语义或拓扑地图转换为语言描述以指导导航。这种方法提高了效率并减少了冗余的探索，但是用语言表示方式丢失了几何信息，这阻碍了复杂的环境中的空间推理。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于视觉-语言模型（VLM）的导航框架来解决这些问题，该框架通过从全局记忆模块中自适应检索任务相关的线索，并将其与代理的第一人称观察相结合，从而增强长期任务中的空间推理和决策制定。&lt;h4&gt;方法&lt;/h4&gt;利用动态对齐全局上下文信息与局部感知的技术来提高空间推理和决策的效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在对象导航任务中超越了以前最先进的方法，并为具身导航提供了一个更有效和可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;新的VLM框架通过结合全局记忆信息与第一人称视觉输入的优点，在复杂的导航环境中实现了更加高效和精准的空间推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Models (LLMs) and Vision-LanguageModels (VLMs) have made them powerful tools in embodied navigation, enablingagents to leverage commonsense and spatial reasoning for efficient explorationin unfamiliar environments. Existing LLM-based approaches convert globalmemory, such as semantic or topological maps, into language descriptions toguide navigation. While this improves efficiency and reduces redundantexploration, the loss of geometric information in language-basedrepresentations hinders spatial reasoning, especially in intricateenvironments. To address this, VLM-based approaches directly processego-centric visual inputs to select optimal directions for exploration.However, relying solely on a first-person perspective makes navigation apartially observed decision-making problem, leading to suboptimal decisions incomplex environments. In this paper, we present a novel vision-language model(VLM)-based navigation framework that addresses these challenges by adaptivelyretrieving task-relevant cues from a global memory module and integrating themwith the agent's egocentric observations. By dynamically aligning globalcontextual information with local perception, our approach enhances spatialreasoning and decision-making in long-horizon tasks. Experimental resultsdemonstrate that the proposed method surpasses previous state-of-the-artapproaches in object navigation tasks, providing a more effective and scalablesolution for embodied navigation.</description>
      <author>example@mail.com (Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Haoping Xu, Guowei Huang, Zhanpeng Zhang, Tongtong Cao, Weichao Qiu, Xingyue Quan, Jianye Hao, Yuzheng Zhuang, Yingxue Zhang)</author>
      <guid isPermaLink="false">2502.14254v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>No Minima, No Collisions: Combining Modulation and Control Barrier Function Strategies for Feasible Dynamical Collision Avoidance</title>
      <link>http://arxiv.org/abs/2502.14238v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了控制栅栏函数二次规划（CBF-QP）和动力学系统调制（Mod-DS）在实时安全关键反应控制系统中的应用，提出了结合两者优点的新方法。&lt;h4&gt;背景&lt;/h4&gt;CBF-QP适用于一般性控制仿射系统但会产生局部极小值；而Mod-DS可以减少甚至避免局部最小值，但在某些约束下难以实现最优解，并且只适用于完全驱动的系统。&lt;h4&gt;目的&lt;/h4&gt;揭示CBF-QP和Mod-DS之间的理论联系并提出一种结合两者优势的新方法来提高实时障碍物规避系统的性能。&lt;h4&gt;方法&lt;/h4&gt;通过数学证明正常调制动力学系统是CBF-QP的一种特殊情况，参考Mod-DS与CBF-QP之间存在一个连接的方程。基于此理论基础，提出了基于Mod的CBF-QP控制器。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的基于Mod的CBF-QP方法能够为控制仿射系统实现局部极小值免费的反应障碍物规避，并在模拟和真实世界实验中表现出了优于传统方法的效果。&lt;h4&gt;结论&lt;/h4&gt;结合CBF-QP与Mod-DS的方法可以有效地解决现有技术的问题，提高系统的安全性和性能。&lt;h4&gt;翻译&lt;/h4&gt;作为重要的实时安全关键响应控制技术，控制屏障函数二次规划(CBF-QPs)适用于一般的控制仿射系统，但会产生局部极小值，并不能确保达到目标。与此相反，动力学系统调制(Mod-DS)，包括常规、参考和在流形上的Mod-DS，可以实现具有很少甚至没有局部最小值的障碍物规避，但在最优地减少受约束与不受约束控制器输出之间的差异方面遇到困难，其应用仅限于完全驱动的系统。我们深入探讨了CBF-QP和Mod-DS的基础理论，并证明尽管它们来自不同的起源，常规Mod-DS是CBF-QP的一种特殊情况，而参考Mod-DS的解决方案通过一个方程与CBF-QP中的解存在数学联系。基于揭示出的CBF-QP和Mod-DS之间的理论联系，我们提出了参考Mod基CBF-QP和在流形上的Mod基CBF-QP控制器来结合这两种方法的优势，并为控制仿射系统实现局部最小值免费的反应障碍物规避。我们在模拟医院环境和使用Ridgeback完全驱动系统的实际世界实验中验证了我们的方法，同时也在Fetch机器人的欠驱动系统中进行了实验。在所有实验中，基于Mod的CBF-QP都超过了传统的CBF-QPs以及我们提出的最优约束执行的Mod-DS方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As prominent real-time safety-critical reactive control techniques, ControlBarrier Function Quadratic Programs (CBF-QPs) work for control affine systemsin general but result in local minima in the generated trajectories andconsequently cannot ensure convergence to the goals. Contrarily, Modulation ofDynamical Systems (Mod-DSs), including normal, reference, and on-manifoldMod-DS, achieve obstacle avoidance with few and even no local minima but havetrouble optimally minimizing the difference between the constrained and theunconstrained controller outputs, and its applications are limited tofully-actuated systems. We dive into the theoretical foundations of CBF-QP andMod-DS, proving that despite their distinct origins, normal Mod-DS is a specialcase of CBF-QP, and reference Mod-DS's solutions are mathematically connectedto that of the CBF-QP through one equation. Building on top of the unveiledtheoretical connections between CBF-QP and Mod-DS, reference Mod-based CBF-QPand on-manifold Mod-based CBF-QP controllers are proposed to combine thestrength of CBF-QP and Mod-DS approaches and realize local-minimum-freereactive obstacle avoidance for control affine systems in general. We validateour methods in both simulated hospital environments and real-world experimentsusing Ridgeback for fully-actuated systems and Fetch robots for underactuatedsystems. Mod-based CBF-QPs outperform CBF-QPs as well as the optimallyconstrained-enforcing Mod-DS approaches we proposed in all experiments.</description>
      <author>example@mail.com (Yifan Xue, Nadia Figueroa)</author>
      <guid isPermaLink="false">2502.14238v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Sampling-based Online Planning for Drone Interception</title>
      <link>http://arxiv.org/abs/2502.14231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025. Supplementary video:  https://youtu.be/dDdshfEAZpg&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了动态环境中高速在线规划问题，提出了一种基于神经网络的采样式算法来解决时间最优轨迹生成、计算约束以及环境不确定性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;在动态环境中，需要找到符合系统动力学的时间最优路径，并满足实时适应性的计算限制。同时还要考虑来自环境变化带来的不确定性影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够快速探索多种可能的不确定情况下的轨迹选择方法，解决无人机拦截问题中目标预测不完善和碰撞避免的问题。&lt;h4&gt;方法&lt;/h4&gt;采用采样为基础的在线规划算法结合神经网络推理技术来替代耗时的非线性路径优化过程。该算法可以平行生成多个潜在目标位置的轨迹，并评估这些轨迹的时间可达性以选择最优解。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的模拟和现实环境中的验证，证明了所提出方法具有高速率在线规划的能力以及在无结构场景中应对不可预测运动变化的良好适应性。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了利用神经网络辅助采样式算法的有效性和可行性，为动态环境中需要快速决策的任务提供了一种新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies high-speed online planning in dynamic environments. Theproblem requires finding time-optimal trajectories that conform to systemdynamics, meeting computational constraints for real-time adaptation, andaccounting for uncertainty from environmental changes. To address thesechallenges, we propose a sampling-based online planning algorithm thatleverages neural network inference to replace time-consuming nonlineartrajectory optimization, enabling rapid exploration of multiple trajectoryoptions under uncertainty. The proposed method is applied to the droneinterception problem, where a defense drone must intercept a target whileavoiding collisions and handling imperfect target predictions. The algorithmefficiently generates trajectories toward multiple potential target dronepositions in parallel. It then assesses trajectory reachability by comparingtraversal times with the target drone's predicted arrival time, ultimatelyselecting the minimum-time reachable trajectory. Through extensive validationin both simulated and real-world environments, we demonstrate our method'scapability for high-rate online planning and its adaptability to unpredictablemovements in unstructured settings.</description>
      <author>example@mail.com (Gilhyun Ryou, Lukas Lao Beyer, Sertac Karaman)</author>
      <guid isPermaLink="false">2502.14231v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition</title>
      <link>http://arxiv.org/abs/2502.14195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Text4VPR的方法，用于通过多视图（360度）文本-视觉注册来识别地点。这种方法首次完全利用了语言描述而非单一视角的图像信息。&lt;h4&gt;背景&lt;/h4&gt;移动机器人需要先进的自然语言理解能力以准确地识别位置并执行任务如包裹递送。然而传统的视觉地方识别方法依赖于单视图视觉信息，无法解读人类的语言描述。&lt;h4&gt;目的&lt;/h4&gt;目的是通过提出Text4VPR来克服现有方法的局限性，该方法将文本和图像结合起来，并使用冻结版T5语言模型以及Sinkhorn算法进行处理，以解决基于文本的地方识别任务中的挑战。&lt;h4&gt;方法&lt;/h4&gt;Text4VPR在训练阶段强调单个文本-图像对之间的精确描述。它还使用了级联交叉注意力余弦匹配（CCCA）来解决内部文本和图像组的不一致，并实现了通过语言描述与图像进行精准地点匹配。&lt;h4&gt;主要发现&lt;/h4&gt;Text4VPR方法首次建立了基于文字到图片地方识别任务的一个稳健基线，达到了57%的第一名精度以及92%的前十名精度（在测试集内半径为5米的情况下），这表明从文本描述定位至图像不仅是可行的，还有进一步发展的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;Text4VPR方法展示了将语言和视觉信息结合解决地方识别任务的有效性，并为未来研究提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nuozimiaowu/Text4VPR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robots necessitate advanced natural language understandingcapabilities to accurately identify locations and perform tasks such as packagedelivery. However, traditional visual place recognition (VPR) methods relysolely on single-view visual information and cannot interpret human languagedescriptions. To overcome this challenge, we bridge text and vision byproposing a multiview (360{\deg} views of the surroundings) text-visionregistration approach called Text4VPR for place recognition task, which is thefirst method that exclusively utilizes textual descriptions to match a databaseof images. Text4VPR employs the frozen T5 language model to extract globaltextual embeddings. Additionally, it utilizes the Sinkhorn algorithm withtemperature coefficient to assign local tokens to their respective clusters,thereby aggregating visual descriptors from images. During the training stage,Text4VPR emphasizes the alignment between individual text-image pairs forprecise textual description. In the inference stage, Text4VPR uses the CascadedCross-Attention Cosine Alignment (CCCA) to address the internal mismatchbetween text and image groups. Subsequently, Text4VPR performs precisely placematch based on the descriptions of text-image groups. On Street360Loc, thefirst text to image VPR dataset we created, Text4VPR builds a robust baseline,achieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92%within a 5-meter radius on the test set, which indicates that localization fromtextual descriptions to images is not only feasible but also holds significantpotential for further advancement, as shown in Figure 1.</description>
      <author>example@mail.com (Tianyi Shang, Zhenyu Li, Pengjie Xu, Jinwei Qiao, Gang Chen, Zihan Ruan, Weijun Hu)</author>
      <guid isPermaLink="false">2502.14195v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations</title>
      <link>http://arxiv.org/abs/2502.14185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and to appear in the IEEE/ACM Conference on Human Robot  Interaction 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为REFLEX的多模态数据集，该数据集记录了机器人在合作环境中因故障而向人类解释时引发的人类反应。&lt;h4&gt;背景&lt;/h4&gt;当前研究中缺乏对机器人出现故障及其后续解释过程中人类反应的系统性捕捉和分析。&lt;h4&gt;目的&lt;/h4&gt;为了促进对人机交互动态的研究，特别是针对初始失败、解释以及长期互动中的情感演变的探讨。&lt;h4&gt;方法&lt;/h4&gt;构建了一个丰富的数据集，包含了人类对于不同类型故障的反应，并且详细注释了不同的解释层次与策略变化。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集为开发更加稳健、适应性和用户满意的机器人系统提供了支持，这些系统能够在面对如重复性失败等挑战时维持积极的人机合作关系。&lt;h4&gt;结论&lt;/h4&gt;通过提供丰富的人类对不同故障类型的反应注释数据，REFLEX促进了更深入地理解和改善人机交互的机制。&lt;h4&gt;翻译&lt;/h4&gt;这项工作介绍了REFLEX（机器人解释给人类以应对失败和人类表达）：一个全面的多模态数据集，捕捉了合作环境中因机器人故障而引起的人类反应以及后续解释。它旨在促进对人机互动动态的研究，并解决需要研究初始失败、解释及长期互动中这些情感变化的需求。通过提供丰富的注释数据来描述人类对于不同类型的失败、不同的解释层次和策略的变化的响应，该数据集有助于开发更加稳健、适应性更强且满足用户需求的机器人系统，在面对如重复故障等挑战时仍能保持与人类合作者之间的积极关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents REFLEX: Robotic Explanations to FaiLures and HumanEXpressions, a comprehensive multimodal dataset capturing human reactions torobot failures and subsequent explanations in collaborative settings. It aimsto facilitate research into human-robot interaction dynamics, addressing theneed to study reactions to both initial failures and explanations, as well asthe evolution of these reactions in long-term interactions. By providing rich,annotated data on human responses to different types of failures, explanationlevels, and explanation varying strategies, the dataset contributes to thedevelopment of more robust, adaptive, and satisfying robotic systems capable ofmaintaining positive relationships with human collaborators, even duringchallenges like repeated failures.</description>
      <author>example@mail.com (Parag Khanna, Andreas Naoum, Elmira Yadollahi, Mårten Björkman, Christian Smith)</author>
      <guid isPermaLink="false">2502.14185v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>ModSkill: Physical Character Skill Modularization</title>
      <link>http://arxiv.org/abs/2502.14140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新型技能学习框架ModSkill，该框架将复杂的全身运动分解为独立的身体部位的模块化技能。&lt;h4&gt;背景&lt;/h4&gt;人类动作高度多样和动态变化，对模仿学习算法提出了挑战，这些算法旨在泛化控制模拟角色的运动技能。先前的方法通常依赖于全身体控制器来追踪参考动作或统一的全身心态嵌入空间，但难以在更大的运动数据集中进行泛化和扩展。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够处理大规模多样化人体动作并能有效学习模块化技能的新框架。&lt;h4&gt;方法&lt;/h4&gt;引入了ModSkill框架，该框架包括一个技能模块化注意层，将策略观察转换为引导各身体部位低级控制器的模块化技能嵌入。同时提出了一种带有生成适应性采样的主动技能学习方法，使用大规模动作生成模型在挑战性的追踪场景中增强策略学习。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在精确全身影子动作跟踪方面优于现有方法，并且能够为各种目标驱动的任务提供可重复使用的技能嵌入。&lt;h4&gt;结论&lt;/h4&gt;ModSkill框架通过分解复杂的身体运动技能，提高了模仿学习算法的泛化能力，在处理大规模人体动作数据集时表现出了优越性。&lt;h4&gt;翻译&lt;/h4&gt;人类的动作非常多样和动态变化，这对旨在将运动技能泛化到模拟角色控制中的模仿学习算法提出了挑战。先前的方法通常依赖于一个通用的全身体控制器来追踪参考动作或统一的全身心态嵌入空间。然而，在更大规模的数据集中这些方法往往难以实现泛化和扩展。在这项工作中，我们介绍了一种新的技能学习框架ModSkill，它将复杂的全身体运动分解为独立的身体部位模块化的技能。我们的框架包括一个技能模块化注意层，该层处理策略观察并将其转换为引导各身体部位低级控制器的模块化技能嵌入。此外，我们还提出了一种结合生成适应性采样（Generative Adaptive Sampling）的主动技能学习方法，使用大规模动作生成模型在挑战性的追踪场景中增强策略学习。我们的研究结果表明，该框架通过分解成模块化的技能学习并利用生成采样技术，在精确全身影子动作跟踪方面超过了现有的方法，并且能够为多样化目标驱动的任务提供可重复使用的技能嵌入。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human motion is highly diverse and dynamic, posing challenges for imitationlearning algorithms that aim to generalize motor skills for controllingsimulated characters. Previous methods typically rely on a universal full-bodycontroller for tracking reference motion (tracking-based model) or a unifiedfull-body skill embedding space (skill embedding). However, these approachesoften struggle to generalize and scale to larger motion datasets. In this work,we introduce a novel skill learning framework, ModSkill, that decouples complexfull-body skills into compositional, modular skills for independent body parts.Our framework features a skill modularization attention layer that processespolicy observations into modular skill embeddings that guide low-levelcontrollers for each body part. We also propose an Active Skill Learningapproach with Generative Adaptive Sampling, using large motion generationmodels to adaptively enhance policy learning in challenging tracking scenarios.Our results show that this modularized skill learning framework, enhanced bygenerative sampling, outperforms existing methods in precise full-body motiontracking and enables reusable skill embeddings for diverse goal-driven tasks.</description>
      <author>example@mail.com (Yiming Huang, Zhiyang Dou, Lingjie Liu)</author>
      <guid isPermaLink="false">2502.14140v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Visual Servoing of Tendon-driven Continuum Robots</title>
      <link>http://arxiv.org/abs/2502.14092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于控制肌腱驱动连续机器人（TDCR）的新型混合视觉伺服系统（HVS）。该方法结合了图像基视觉伺服(IBVS)和深度学习基视觉伺服(DLBVS)，以克服单一方法的局限性，提高整体性能。&lt;h4&gt;背景&lt;/h4&gt;在处理动态、无结构环境中的问题时，传统的IBVS和DLBVS各有优缺点。IBVS具有更高的精度和较快的收敛速度，而DLBVS则对干扰有更强的鲁棒性，并且工作空间更大。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够根据需要平滑过渡于IBVS与DLBVS之间控制方法，以提高处理复杂环境的能力。&lt;h4&gt;方法&lt;/h4&gt;混合视觉伺服系统(HVS)结合了IBVS和DLBVS的优点，通过模拟实验和真实世界测试验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;HVS相较于单独使用DLBVS，在迭代时间、收敛速度、最终误差和性能平滑性方面均有所改进。同时保留了DLBVS在诸如遮挡、光照变化等挑战条件下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的混合视觉伺服系统能够有效应对复杂多变的环境，展示了比单独使用IBVS或DLBVS更好的综合性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文介绍了一种针对肌腱驱动连续机器人控制的新颖混合视觉伺服（HVS）方法。该HVS系统结合了基于图像的视觉伺服(IBVS)与基于深度学习的视觉伺服(DLBVS)，克服了各自方法的局限性，以提升整体性能。IBVS在特征丰富环境中提供了更高的精度和更快的收敛速度；而DLBVS则增强了对抗干扰的能力，并拥有更大的工作空间。通过使IBVS与DLBVS之间实现平滑转换，所提出的HVS确保了复杂、无结构环境中的有效控制。该方法的有效性已通过仿真及真实世界实验得到验证，展示了相较于单独使用DLBVS时，HVS在减少迭代时间、加快收敛速度、降低最终误差以及改善性能方面的优势，同时保持了DLBVS面对诸如遮挡、光照变化、驱动器噪声和物理冲击等挑战条件下的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel Hybrid Visual Servoing (HVS) approach forcontrolling tendon-driven continuum robots (TDCRs). The HVS system combinesImage-Based Visual Servoing (IBVS) with Deep Learning-Based Visual Servoing(DLBVS) to overcome the limitations of each method and improve overallperformance. IBVS offers higher accuracy and faster convergence in feature-richenvironments, while DLBVS enhances robustness against disturbances and offers alarger workspace. By enabling smooth transitions between IBVS and DLBVS, theproposed HVS ensures effective control in dynamic, unstructured environments.The effectiveness of this approach is validated through simulations andreal-world experiments, demonstrating that HVS achieves reduced iteration time,faster convergence, lower final error, and smoother performance compared toDLBVS alone, while maintaining DLBVS's robustness in challenging conditionssuch as occlusions, lighting changes, actuator noise, and physical impacts.</description>
      <author>example@mail.com (Rana Danesh, Farrokh Janabi-Sharifi, Farhad Aghili)</author>
      <guid isPermaLink="false">2502.14092v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation</title>
      <link>http://arxiv.org/abs/2502.14061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于GDRNPP的快速且可扩展的姿态估计器集合，旨在实现实时应用中速度与精度之间的良好平衡。&lt;h4&gt;背景&lt;/h4&gt;在工业实时反馈应用场景（如质量控制和机器人操作）中，高准确度的姿态估计算法需求仍然非常重要。尽管目前有一些算法提高了姿态估计的速度和准确性，但在动态环境中实现高效计算能力和精确性的平衡仍面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于GDRNPP的快速、可扩展姿势估计算法集合，以满足或超越当前基准在准确性和鲁棒性方面的表现，特别是在实时场景中的效率-精度权衡问题上进行改进。&lt;h4&gt;方法&lt;/h4&gt;提出了AMIS算法来根据特定应用场景中推理时间和准确性之间的应用特定位平衡选择合适的模型。该研究展示了基于AMIS的模型选择在四个著名的基准数据集（LM-O、YCB-V、T-LESS和ITODD）上的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的AMIS方法可以在不同应用场景之间灵活调整，从而实现实时应用中速度与精度之间的良好平衡。&lt;h4&gt;结论&lt;/h4&gt;本文通过改进现有的姿态估计算法模型，并提出了新的AMIS方法来优化实时场景中的效率-准确性的权衡，为工业自动化和机器人技术等领域提供了更好的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In industrial applications requiring real-time feedback, such as qualitycontrol and robotic manipulation, the demand for high-speed and accurate poseestimation remains critical. Despite advances improving speed and accuracy inpose estimation, finding a balance between computational efficiency andaccuracy poses significant challenges in dynamic environments. Most currentalgorithms lack scalability in estimation time, especially for diversedatasets, and the state-of-the-art (SOTA) methods are often too slow. Thisstudy focuses on developing a fast and scalable set of pose estimators based onGDRNPP to meet or exceed current benchmarks in accuracy and robustness,particularly addressing the efficiency-accuracy trade-off essential inreal-time scenarios. We propose the AMIS algorithm to tailor the utilized modelaccording to an application-specific trade-off between inference time andaccuracy. We further show the effectiveness of the AMIS-based model choice onfour prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).</description>
      <author>example@mail.com (Zixuan Fang, Thomas Pöllabauer, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper)</author>
      <guid isPermaLink="false">2502.14061v1</guid>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Cadastral Boundary from Satellite Images Using U-Net model</title>
      <link>http://arxiv.org/abs/2502.11044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文利用深度学习方法，采用迁移学习训练具有ResNet34骨干网络的U-Net模型，通过三类语义分割（边界、农田和背景）来检测农地的土地权属界限。&lt;h4&gt;背景&lt;/h4&gt;土地管理中找到农地的土地权属界限是一个关键问题。使用卫星图像和无人机(UAV)图像进行该任务是必要的。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习方法加速并简化从卫星和UAV图像中提取土地权属边界的流程。&lt;h4&gt;方法&lt;/h4&gt;采用迁移学习训练具有ResNet34骨干网络的U-Net模型，用于三类语义分割以检测土地权属边界。使用的类别包括“边界”，“农田”以及“背景”。&lt;h4&gt;主要发现&lt;/h4&gt;在伊朗农业地区的两张卫星图像上评估了模型性能，分别得到了88%，75%和81%的精确度、召回率及F值。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该方法具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Finding the cadastral boundaries of farmlands is a crucial concern for landadministration. Therefore, using deep learning methods to expedite and simplifythe extraction of cadastral boundaries from satellite and unmanned aerialvehicle (UAV) images is critical. In this paper, we employ transfer learning totrain a U-Net model with a ResNet34 backbone to detect cadastral boundariesthrough three-class semantic segmentation: "boundary", "field", and"background". We evaluate the performance on two satellite images fromfarmlands in Iran using "precision", "recall", and "F-score", achieving highvalues of 88%, 75%, and 81%, respectively, which indicate promising results.</description>
      <author>example@mail.com (Neda Rahimpour Anaraki, Maryam Tahmasbi, Saeed Reza Kheradpisheh)</author>
      <guid isPermaLink="false">2502.11044v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
  <item>
      <title>Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2502.12414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors contributed equally as co-first authors. The  manuscript is 21 pages long and is a work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了大规模训练的语音基础模型在自动语音识别（ASR）任务中的性能评估挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的传统评价指标如WER和CER无法有效反映转录质量，尤其是伪造输出检测方面的问题。这使得这些模型在诸如医疗、法律及航空等高风险领域中可能隐藏严重错误。&lt;h4&gt;目的&lt;/h4&gt;研究自动语音识别（ASR）模型中的hallucination现象，并引入HER来量化这一问题。&lt;h4&gt;方法&lt;/h4&gt;分析了20个不同的ASR模型，探究分布变化、模型大小和架构对HER的影响。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '高WER可能掩盖低HER，而低WER也可能隐藏危险的hallucinations。', '2': '对抗性合成噪声（如白噪音、音调偏移和时间拉伸）会增加HER。', '3': '分布变化与HER之间有强相关性（α = 0.91）。'}&lt;h4&gt;结论&lt;/h4&gt;建议在评估ASR模型时，不仅要考虑传统的WER等指标，还要结合新的HER指标来更全面地衡量性能，特别是在高风险领域中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models trained at a massive scale, both in terms of modeland data size, result in robust systems capable of performing multiple speechtasks, including automatic speech recognition (ASR). These models transcendlanguage and domain barriers, yet effectively measuring their performanceremains a challenge. Traditional metrics like word error rate (WER) andcharacter error rate (CER) are commonly used to evaluate ASR performance butoften fail to reflect transcription quality in critical contexts, particularlywhen detecting fabricated outputs. This phenomenon, known as hallucination, isespecially concerning in high-stakes domains such as healthcare, legal, andaviation, where errors can have severe consequences. In our work, we addressthis gap by investigating hallucination in ASR models. We examine how factorssuch as distribution shifts, model size, and model architecture influence thehallucination error rate (HER), a metric we introduce to quantifyhallucinations. Our analysis of 20 ASR models reveals \numinsights~keyinsights: (1) High WERs can mask low hallucination rates, while low WERs mayconceal dangerous hallucinations. (2) Synthetic noise, both adversarial andcommon perturbations like white noise, pitch shift, and time stretching,increase HER. (3) Distribution shift correlates strongly with HER ($\alpha =0.91$). Our findings highlight the importance of incorporating HER alongsidetraditional metrics like WER to better assess ASR model performance,particularly in high-stakes domains.</description>
      <author>example@mail.com (Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj)</author>
      <guid isPermaLink="false">2502.12414v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise</title>
      <link>http://arxiv.org/abs/2502.13395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;分布式声学传感器(DAS)技术利用光纤电缆检测声信号，提供成本效益高且密集的监测能力。该技术具备多种优势，包括在极端条件下的耐受性、对电磁干扰的免疫性和精确探测能力。&lt;h4&gt;背景&lt;/h4&gt;DAS技术虽然具有诸多优点，但其信噪比(S/N)通常低于地震检波器，并且容易受到随机噪声、突发噪声、水平噪声和长周期噪声的影响。这些噪音可能会降低数据分析中反演和解释的质量。&lt;h4&gt;目的&lt;/h4&gt;为了改善DAS数据中的噪音问题，作者开发了一个基于上下文金字塔模块(Context-Pyramid-UNet, CP-UNet)的无监督学习(UL)网络模型，以抑制DAS数据中的突发噪声和随机噪声。&lt;h4&gt;方法&lt;/h4&gt;该CP-UNet模型利用了编码和解码过程中的Context Pyramid Module来提取特征并重构DAS数据。为了增强浅层与深层特征之间的连接性，在编码和解码部分加入了Connected Module(CM)。在训练过程中，用Layer Normalization(LN)代替常用的Batch Normalization(BN)，以加速模型的收敛速度，并防止梯度爆炸。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出的CP-UNet网络在二维合成数据和现场数据上都展示了出色的去噪性能，优于传统的去噪方法以及最新的无监督学习框架。&lt;h4&gt;结论&lt;/h4&gt;通过实验结果验证了基于上下文金字塔模块的无监督学习模型具有优秀的噪声抑制能力，能够有效提高DAS数据的质量。&lt;h4&gt;翻译&lt;/h4&gt;分布式声学传感器技术利用光纤电缆来检测声信号，并提供成本效益高、密度大的监测功能。虽然它具有许多优势，例如在极端条件下的抵抗力、对电磁干扰的免疫力以及准确的检测，但它通常表现出比地震检波器更低的信噪比(S/N)，并且容易受到各种噪声的影响，如随机噪声、不规则噪声、水平噪声和长周期噪声。这些减少的S/N可能会影响包含反演和解释的数据分析。虽然人工智能在去噪方面已经展示了出色的能力，但大多数现有方法依赖于监督学习并需要标签数据的支持。为了解决这一问题，我们开发了一个基于Context-Pyramid-UNet (CP-UNet)的无监督学习(UL)网络模型来抑制DAS数据中的不规则和随机噪声。该CP-UNet利用编码和解码过程中的上下文金字塔模块(Context Pyramid Module)提取特征并重构DAS数据。为了增强浅层与深层特征之间的连接性，我们在编码和解码部分加入了Connected Module (CM)。在训练过程中使用Layer Normalization (LN)替代常用的Batch Normalization (BN)，以加速模型的收敛速度，并防止梯度爆炸的发生。我们采用Huber损失作为我们的损失函数，其参数通过实验确定。该网络被应用于二维合成数据和现场数据上。与传统去噪方法以及最新无监督学习框架相比，所提出的去噪方法展示出了更好的噪声抑制性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed acoustic sensor (DAS) technology leverages optical fiber cablesto detect acoustic signals, providing cost-effective and dense monitoringcapabilities. It offers several advantages including resistance to extremeconditions, immunity to electromagnetic interference, and accurate detection.However, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared togeophones and is susceptible to various noise types, such as random noise,erratic noise, level noise, and long-period noise. This reduced S/N cannegatively impact data analyses containing inversion and interpretation. Whileartificial intelligence has demonstrated excellent denoising capabilities, mostexisting methods rely on supervised learning with labeled data, which imposesstringent requirements on the quality of the labels. To address this issue, wedevelop a label-free unsupervised learning (UL) network model based onContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DASdata. The CP-UNet utilizes the Context Pyramid Module in the encoding anddecoding process to extract features and reconstruct the DAS data. To enhancethe connectivity between shallow and deep features, we add a Connected Module(CM) to both encoding and decoding section. Layer Normalization (LN) isutilized to replace the commonly employed Batch Normalization (BN),accelerating the convergence of the model and preventing gradient explosionduring training. Huber-loss is adopted as our loss function whose parametersare experimentally determined. We apply the network to both the 2-D syntheticand filed data. Comparing to traditional denoising methods and the latest ULframework, our proposed method demonstrates superior noise reductionperformance.</description>
      <author>example@mail.com (Tianye Huang, Aopeng Li, Xiang Li, Jing Zhang, Sijing Xian, Qi Zhang, Mingkong Lu, Guodong Chen, Liangming Xiong, Xiangyun Hu)</author>
      <guid isPermaLink="false">2502.13395v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Video-Pose Pretraining for Operating Room Surgical Activity Recognition</title>
      <link>http://arxiv.org/abs/2502.13883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的无校准多视角多模态预训练框架PreViPS，用于手术活动识别，该框架能够将不同摄像机视图下的2D姿态和视觉嵌入对齐。&lt;h4&gt;背景&lt;/h4&gt;理解复杂手术室中临床医生与环境的交互需要深入理解外科程序的工作流程。现有的手术活动识别（SAR）模型通常无法准确捕捉细微的动作变化或充分利用多视角信息，或者它们需要精确校准的多视图摄像机设置和高级点云处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需多相机标定即可有效进行视频姿态和视觉特征预训练的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PreViPS的新框架。该模型采用CLIP风格的双编码器架构，一个用于处理视觉特性，另一个则负责人类姿态嵌入的生成。为了处理连续2D人体姿势坐标数据，引入了分词后的离散表示法将这些坐标转换为离散姿态嵌入，从而可以在双编码器框架中高效集成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在两个不同手术室数据集上的表现优于强基线模型，并且展示了其在多视图和单视图设置下的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究为复杂手术环境中的SAR任务提供了一种高效、实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解复杂的外科手术工作流程需要深入了解临床医生与其操作环境之间的交互作用。手术活动识别（SAR）是一项关键的计算机视觉任务，它从多视角摄像机记录中检测动作或阶段。现有的SAR模型往往无法准确捕捉细微的临床医生动作变化或多视角知识，或者它们需要精确校准的多视角摄像机设置和高级点云处理以获得更好的结果。在这项工作中，我们提出了一种新的无校准多视图多模态预训练框架PreViPS，该框架将不同摄像机视图下的2D姿态与视觉嵌入对齐。我们的模型采用了CLIP风格的双编码器架构：一个编码器处理视觉特征，另一个则用于编码人类姿势嵌入。为了处理连续的2D人体姿态坐标，我们引入了一种分词后的离散表示法，将连续的2D姿态坐标转换为离散的姿态嵌入，从而可以在双编码器框架中高效集成。为了弥合这两种模式之间的差距，我们提出了一些跨模态和同模态几何约束下的预训练目标，并采用掩码姿势令牌预测策略来增强表征学习能力。广泛的实验和消融研究表明，与强大的基线相比有所改进，并且在两个不同的手术室数据集上的数据效率测试进一步突显了该方法的有效性。我们强调这种方法对于多视图和单视图设置中的外科活动识别的益处，展示了其在复杂手术环境中的实际适用性。代码将在以下地址发布：https://github.com/CAMMA-public/PreViPS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the workflow of surgical procedures in complex operating roomsrequires a deep understanding of the interactions between clinicians and theirenvironment. Surgical activity recognition (SAR) is a key computer vision taskthat detects activities or phases from multi-view camera recordings. ExistingSAR models often fail to account for fine-grained clinician movements andmulti-view knowledge, or they require calibrated multi-view camera setups andadvanced point-cloud processing to obtain better results. In this work, wepropose a novel calibration-free multi-view multi-modal pretraining frameworkcalled Multiview Pretraining for Video-Pose Surgical Activity RecognitionPreViPS, which aligns 2D pose and vision embeddings across camera views. Ourmodel follows CLIP-style dual-encoder architecture: one encoder processesvisual features, while the other encodes human pose embeddings. To handle thecontinuous 2D human pose coordinates, we introduce a tokenized discreterepresentation to convert the continuous 2D pose coordinates into discrete poseembeddings, thereby enabling efficient integration within the dual-encoderframework. To bridge the gap between these two modalities, we propose severalpretraining objectives using cross- and in-modality geometric constraintswithin the embedding space and incorporating masked pose token predictionstrategy to enhance representation learning. Extensive experiments and ablationstudies demonstrate improvements over the strong baselines, whiledata-efficiency experiments on two distinct operating room datasets furtherhighlight the effectiveness of our approach. We highlight the benefits of ourapproach for surgical activity recognition in both multi-view and single-viewsettings, showcasing its practical applicability in complex surgicalenvironments. Code will be made available at:https://github.com/CAMMA-public/PreViPS.</description>
      <author>example@mail.com (Idris Hamoud, Vinkle Srivastav, Muhammad Abdullah Jamal, Didier Mutter, Omid Mohareri, Nicolas Padoy)</author>
      <guid isPermaLink="false">2502.13883v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Toward Robust Non-Transferable Learning: A Survey and Benchmark</title>
      <link>http://arxiv.org/abs/2502.13593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对非可传输学习（NTL）进行了首次全面的综述，并提出了评估NTL性能和鲁棒性的基准测试NLTBench。&lt;h4&gt;背景&lt;/h4&gt;过去几十年的研究主要集中在提高模型泛化能力上，较少关注调节这种泛化。然而，不良对手可以利用模型在未经授权或有害数据上的泛化能力，这可能违反了模型伦理。&lt;h4&gt;目的&lt;/h4&gt;解决现有NTL方法中缺乏全面综述和系统性分析的不足，并提出首个评估NTL性能及鲁棒性的统一框架NLTBench。&lt;h4&gt;方法&lt;/h4&gt;文章首先介绍了NTL的任务设置、通用框架以及评价标准。随后，总结现有的NTL方法并重点讨论了这些方法在面对破坏非可传输机制的各种攻击时所面临的鲁棒性问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用NLTBench进行实验验证，揭示现有NTL方法的局限性，尤其是在抵御不同攻击方面的鲁棒性不足。&lt;h4&gt;结论&lt;/h4&gt;本文还探讨了NTL的实际应用、未来研究方向及挑战，并强调了在实现NTL时需要注意的问题和面临的挑战。&lt;h4&gt;翻译&lt;/h4&gt;过去几十年的研究主要集中在提高模型泛化能力上。但是，这种泛化能力也可能被不良对手利用来造成未经授权或有害数据的使用，从而违背伦理原则。为了解决这个问题，研究人员提出了非可传输学习（NTL）任务，即重塑深度学习模型的泛化能力。尽管已经提出许多方法，但目前仍缺乏全面的综述和系统性分析。因此，本文填补了这一空白，并介绍了首个评估NTL性能及鲁棒性的基准NLTBench，在这个统一框架下进行了详细的实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decades, researchers have primarily focused on improving thegeneralization abilities of models, with limited attention given to regulatingsuch generalization. However, the ability of models to generalize to unintendeddata (e.g., harmful or unauthorized data) can be exploited by maliciousadversaries in unforeseen ways, potentially resulting in violations of modelethics. Non-transferable learning (NTL), a task aimed at reshaping thegeneralization abilities of deep learning models, was proposed to address thesechallenges. While numerous methods have been proposed in this field, acomprehensive review of existing progress and a thorough analysis of currentlimitations remain lacking. In this paper, we bridge this gap by presenting thefirst comprehensive survey on NTL and introducing NTLBench, the first benchmarkto evaluate NTL performance and robustness within a unified framework.Specifically, we first introduce the task settings, general framework, andcriteria of NTL, followed by a summary of NTL approaches. Furthermore, weemphasize the often-overlooked issue of robustness against various attacks thatcan destroy the non-transferable mechanism established by NTL. Experimentsconducted via NTLBench verify the limitations of existing NTL methods inrobustness. Finally, we discuss the practical applications of NTL, along withits future directions and associated challenges.</description>
      <author>example@mail.com (Ziming Hong, Yongli Xiang, Tongliang Liu)</author>
      <guid isPermaLink="false">2502.13593v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Extending the RANGE of Graph Neural Networks: Relaying Attention Nodes for Global Encoding</title>
      <link>http://arxiv.org/abs/2502.13797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;介绍了一种名为RANGE的新框架，该框架旨在解决图神经网络在处理大型分子系统时遇到的信息流瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)被广泛应用于建模物理、社会科学和经济学中的多体相互作用。然而，在模拟大型分子系统的分散力和局部电场变化导致的集体结构变化方面，GNN存在局限性，这使得现有的解决方案在计算成本和可扩展性上面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出了一种模型无关的方法RANGE框架，以有效地捕捉长距离交互并提高处理大规模分子系统的能力。&lt;h4&gt;方法&lt;/h4&gt;该框架采用了基于注意力的聚合-广播机制，通过引入虚拟节点消息传递来动态地扩大表示，并在计算成本低的情况下显著减少信息流瓶颈问题。&lt;h4&gt;主要发现&lt;/h4&gt;RANGE是第一个将注意和位置编码以及正则化整合到虚拟节点消息传递中的实现方案。这使它能够在捕捉长程相互作用方面表现出色，同时保持较低的计算开销。&lt;h4&gt;结论&lt;/h4&gt;这项工作为下一代机器学习力场奠定了基础，能够准确而有效地模拟大规模分子系统的长距离交互。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）被常规地用于建模分子物理学、社会科学和经济学中的图形系统多体相互作用。然而，由于其固有的局部性，它们在信息流方面可能遭受瓶颈问题，特别是在建模大型分子系统时，这会驱动集体结构变化的分散力和局部电场的变化尤为严重。现有的解决方案面临计算成本及可扩展性的挑战。我们介绍了RANGE模型无关框架，该框架采用基于注意力机制的聚合广播机制，大大减少了信息压缩效应，并以几乎可以忽略不计的计算开销实现了对长程相互作用的精确捕获。值得注意的是，RANGE是首个将注意力机制与位置编码及正则化结合来动态扩展虚拟表示的虚拟节点消息传递实现方案。这项工作为下一代机器学习力场奠定了基础，提供了准确而高效的建模方式以模拟大型分子系统的长程交互。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are routinely used in molecular physics, socialsciences, and economics to model many-body interactions in graph-like systems.However, GNNs are inherently local and can suffer from information flowbottlenecks. This is particularly problematic when modeling large molecularsystems, where dispersion forces and local electric field variations drivecollective structural changes. Existing solutions face challenges related tocomputational cost and scalability. We introduce RANGE, a model-agnosticframework that employs an attention-based aggregation-broadcast mechanism thatsignificantly reduces oversquashing effects, and achieves remarkable accuracyin capturing long-range interactions at a negligible computational cost.Notably, RANGE is the first virtual-node message-passing implementation tointegrate attention with positional encodings and regularization to dynamicallyexpand virtual representations. This work lays the foundation fornext-generation of machine-learned force fields, offering accurate andefficient modeling of long-range interactions for simulating large molecularsystems.</description>
      <author>example@mail.com (Alessandro Caruso, Jacopo Venturin, Lorenzo Giambagli, Edoardo Rolando, Frank Noé, Cecilia Clementi)</author>
      <guid isPermaLink="false">2502.13797v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Machine Learning Performance through Intelligent Data Quality Assessment: An Unsupervised Data-centric Framework</title>
      <link>http://arxiv.org/abs/2502.13198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个智能数据为中心的评估框架，该框架能够识别高质量的数据并提高机器学习系统的性能。&lt;h4&gt;背景&lt;/h4&gt;由于数据量和复杂性的增加，现代数据更容易受到质量低下的影响。这导致在将数据输入到ML管道之前需要进行大量繁琐且耗时的工作来准备和改进数据。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，提出了一种智能的数据为中心的评估框架，用于识别高质量数据并提高机器学习系统的性能。&lt;h4&gt;方法&lt;/h4&gt;该框架结合了质量测量的整理和无监督学习技术，能够区分高质量和低质量的数据。此外，该框架设计为具有灵活性和通用性，可以应用于各种领域和应用。&lt;h4&gt;主要发现&lt;/h4&gt;在实际案例中验证了所提出的框架的有效性，在分析化学领域的实验中，通过三个反义寡核苷酸数据集进行了测试，并咨询了相关领域的专家以确定相关的质量测量并评估框架的结果。结果表明该框架能够识别高质量数据的特性，从而指导高效的实验室实验进行，并提高机器学习系统的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的质量为中心的数据评估框架能够在多种应用场景中有效提升数据质量和机器学习模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;低质量的数据限制了机器学习（ML）的优势并削弱了高性能的ML软件系统。由于数据量和复杂性的增加，现代数据更容易受到质量问题的影响。因此，在将数据输入到ML管道之前需要进行大量繁琐且耗时的工作来准备和改进数据。为了应对这一挑战，我们提出了一种智能的数据为中心的评估框架，该框架能够识别高质量数据并提高机器学习系统的性能。该框架结合了质量测量的整理和无监督学习技术以区分高质量和低质量的数据。此外，该框架设计为具有灵活性和通用性，可以应用于各种领域和应用。为了验证所提出的框架的效果，在分析化学领域的实验中进行了实际案例测试，并通过咨询相关专家评估结果。结果显示，基于数据质量的评估框架能够识别出高质量数据的特点，从而指导实验室高效的实验操作并提高机器学习系统的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Poor data quality limits the advantageous power of Machine Learning (ML) andweakens high-performing ML software systems. Nowadays, data are more prone tothe risk of poor quality due to their increasing volume and complexity.Therefore, tedious and time-consuming work goes into data preparation andimprovement before moving further in the ML pipeline. To address thischallenge, we propose an intelligent data-centric evaluation framework that canidentify high-quality data and improve the performance of an ML system. Theproposed framework combines the curation of quality measurements andunsupervised learning to distinguish high- and low-quality data. The frameworkis designed to integrate flexible and general-purpose methods so that it isdeployed in various domains and applications. To validate the outcomes of thedesigned framework, we implemented it in a real-world use case from the fieldof analytical chemistry, where it is tested on three datasets of anti-senseoligonucleotides. A domain expert is consulted to identify the relevant qualitymeasurements and evaluate the outcomes of the framework. The results show thatthe quality-centric data evaluation framework identifies the characteristics ofhigh-quality data that guide the conduct of efficient laboratory experimentsand consequently improve the performance of the ML system.</description>
      <author>example@mail.com (Manal Rahal, Bestoun S. Ahmed, Gergely Szabados, Torgny Fornstedt, Jorgen Samuelsson)</author>
      <guid isPermaLink="false">2502.13198v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection</title>
      <link>http://arxiv.org/abs/2502.13628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络和双曲空间图神经网络作为环境声明检测任务中的轻量级替代方案，证明了这些基于结构化图的方法在保持高性能的同时具有更高的效率、可解释性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型主导着自然语言处理领域的各项任务，但其巨大的计算需求和缺乏透明性给实际应用带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;探索轻量级且有效的Graph Neural Networks (GNNs) 和Hyperbolic Graph Neural Networks (HGNNs) 作为环境声明检测中的替代方案，并重新定义该问题为图分类问题。&lt;h4&gt;方法&lt;/h4&gt;构造了依赖句法解析图，使用简单的词向量（word2vec）表示节点特征，将依存关系编码到边缘特征中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，这些基于结构化的图模型能够在参数减少30倍的情况下达到与最先进的Transformer模型相当或更好的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了在自然语言处理任务中使用结构化、可解释和计算效率高的图方法的潜力。&lt;h4&gt;翻译&lt;/h4&gt;变压器主导着如情感分析，机器翻译，声明验证等NLP任务，然而它们庞大的计算需求和缺乏透明性阻碍了对高效和透明度要求的应用。在这项工作中，我们探讨了图神经网络（GNNs）和双曲图神经网络（HGNNs）作为环境声明检测中轻量级且有效的替代方案，重新将其定义为一个图形分类问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based models dominate NLP tasks like sentiment analysis, machinetranslation, and claim verification. However, their massive computationaldemands and lack of interpretability pose challenges for real-worldapplications requiring efficiency and transparency. In this work, we exploreGraph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) aslightweight yet effective alternatives for Environmental Claim Detection,reframing it as a graph classification problem. We construct dependency parsinggraphs to explicitly model syntactic structures, using simple word embeddings(word2vec) for node features with dependency relations encoded as edgefeatures. Our results demonstrate that these graph-based models achievecomparable or superior performance to state-of-the-art transformers while using30x fewer parameters. This efficiency highlights the potential of structured,interpretable, and computationally efficient graph-based approaches.</description>
      <author>example@mail.com (Darpan Aswal, Manjira Sinha)</author>
      <guid isPermaLink="false">2502.13628v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Refining embeddings with fill-tuning: data-efficient generalised performance improvements for materials foundation models</title>
      <link>http://arxiv.org/abs/2502.13886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;预训练基础模型学习的嵌入可以用于广泛的任务，当在特定任务上表现不足时可以通过微调来改进。然而当前的方法会导致对所有分布外任务的表现下降。这项工作提出了一种新的方法'fill-tuning'，生成针对特定下游任务不合适的预训练数据集，并通过粗糙度分析技术改善模型的嵌入表示。&lt;h4&gt;背景&lt;/h4&gt;预训练基础模型在各种下游任务中表现出色，但是它们可能无法准确处理特定的任务而需要微调。传统的微调会导致对未见过的数据的表现下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法'fill-tuning'来生成针对不合适的下游任务的预训练数据集，并通过该方法提高模型在所有下游任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;'fill-tuning' 方法使用粗糙度分析技术识别并改善模型嵌入表示中质量较差的部分，从而有针对性地改进特定领域的性能。&lt;h4&gt;主要发现&lt;/h4&gt;应用 fill-tuning 到多个先进材料基础模型上，在添加仅仅 100 个数据点的情况下实现了所有下游任务表现平均提升接近 1% 的效果。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种在计算成本等于微调的情况下，对预训练基础模型进行整体改进的途径。&lt;h4&gt;翻译&lt;/h4&gt;预训练的基础模型学习到的嵌入可用于多种下游任务。这些嵌入优化了总体性能，如果在特定任务上不够准确，则可以通过微调来提升性能。然而，所有当前的方法都会导致其他未见过数据上的表现下降。在这项工作中我们提出了一种新的方法'fill-tuning'，用于生成针对不适合具体下游任务的预训练基础模型的数据集，而不是旨在纠正嵌入表示中的不良区域。我们展示了粗糙度分析如何应用于潜在空间拓扑结构，并说明了它可以用来推荐最有价值改进嵌入表示的数据点。我们将 fill-tuning 应用到一组最先进的材料基础模型上，这些模型是在 $O(10^9)$ 数据点的基础上训练的，在添加仅 100 条数据的情况下实现了所有下游任务中平均表现提高接近 1% 的效果。该方法提供了一种以与微调相当的计算成本来改进预训练基础模型的方法途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained foundation models learn embeddings that can be used for a widerange of downstream tasks. These embeddings optimise general performance, andif insufficiently accurate at a specific task the model can be fine-tuned toimprove performance. For all current methodologies this operation necessarilydegrades performance on all out-of-distribution tasks. In this work we present'fill-tuning', a novel methodology to generate datasets for continuedpretraining of foundation models that are not suited to a particular downstreamtask, but instead aim to correct poor regions of the embedding. We present theapplication of roughness analysis to latent space topologies and illustrate howit can be used to propose data that will be most valuable to improving theembedding. We apply fill-tuning to a set of state-of-the-art materialsfoundation models trained on $O(10^9)$ data points and show model improvementof almost 1% in all downstream tasks with the addition of only 100 data points.This method provides a route to the general improvement of foundation models atthe computational cost of fine-tuning.</description>
      <author>example@mail.com (Matthew P. Wilson, Edward O. Pyzer-Knapp, Nicolas Galichet, Luke Dicks)</author>
      <guid isPermaLink="false">2502.13886v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements</title>
      <link>http://arxiv.org/abs/2502.13522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '机器学习势能（MLPs）可以在显著降低计算成本的情况下实现从头算精度的模拟，但其有效性依赖于具有足够化学空间和热力学条件覆盖的大规模数据集。本文提出了在元素之间转移训练潜在能量面的方法。', '背景': 'MLPs的有效性需要大量的数据集来确保在整个化学空间中的稳健泛化，而这些大规模的数据集生成可能非常耗时。', '目的': '探索如何利用较少的数据进行MLPs的训练，特别是在缺乏大量初始数据的情况下。', '方法': '提出了转移学习的方法，即使用已训练好的硅原子MLP模型初始化并加速锗原子MLP模型的训练。', '主要发现': '与从零开始的传统训练相比，转移学习在力预测方面表现更佳，提高了模拟的稳定性，并且温度传递性更好。随着训练数据集的减小，这些优势变得更加明显。', '结论': '跨化学元素的迁移学习是一种开发准确和数值稳定的MLPs的有效技术，尤其是在数据稀少的情况下。', '翻译': '机器学习势能可以以数量级降低的成本实现从头算精度的模拟，但其有效性依赖于具有足够化学空间覆盖的大规模数据集。为此，文章提出了在相似元素之间迁移训练潜在能量面的方法，并通过实验验证了转移学习能够提高力预测准确性和温度传递性，尤其适用于小数据集情况下的模型开发。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine Learning Potentials (MLPs) can enable simulations of ab initioaccuracy at orders of magnitude lower computational cost. However, theireffectiveness hinges on the availability of considerable datasets to ensurerobust generalization across chemical space and thermodynamic conditions. Thegeneration of such datasets can be labor-intensive, highlighting the need forinnovative methods to train MLPs in data-scarce scenarios. Here, we introducetransfer learning of potential energy surfaces between chemically similarelements. Specifically, we leverage the trained MLP for silicon to initializeand expedite the training of an MLP for germanium. Utilizing classical forcefield and ab initio datasets, we demonstrate that transfer learning surpassestraditional training from scratch in force prediction, leading to more stablesimulations and improved temperature transferability. These advantages becomeeven more pronounced as the training dataset size decreases. The out-of-targetproperty analysis shows that transfer learning leads to beneficial butsometimes adversarial effects. Our findings demonstrate that transfer learningacross chemical elements is a promising technique for developing accurate andnumerically stable MLPs, particularly in a data-scarce regime.</description>
      <author>example@mail.com (Sebastien Röcken, Julija Zavadlav)</author>
      <guid isPermaLink="false">2502.13522v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</title>
      <link>http://arxiv.org/abs/2502.13942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多模态元学习框架，利用调优提示来连接两个预训练的大规模视觉和语言模型，以解决在少量数据训练时的领域差距问题。&lt;h4&gt;背景&lt;/h4&gt;大规模的视觉和语言预训练模型已经在大量的数据上编码了视觉和语言先验知识，使得生成更自然、更真实的图像和文本变得更加容易。然而，在小样本设置中（即只有非常有限的数据可用于训练），视觉和语言模态之间的领域差距仍然显著。&lt;h4&gt;目的&lt;/h4&gt;为了减轻这个领域差距问题，提出了一种多模态元学习框架来连接两个预训练的大规模视觉和语言模型，并引入一个可调的提示以促进两者的交互。&lt;h4&gt;方法&lt;/h4&gt;在少量样本图像描述任务中，现有的多模态元学习框架采用了一步式提示方案来积累输入图像的视觉特征并指导语言模型。然而，这种策略难以仅通过有限的训练样本生成准确的图像描述。因此，提出了一个链条思维（CoT）元学习方案作为多步骤图像描述程序，更有效地模仿人类如何描述图像，并进一步提出在每个CoT步骤中学习不同的子空间元参数以避免干扰。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在不同数据集上评估的方法优于基线方法，尤其是在少样本设置下的MSCOCO、Flickr8k和Flickr30k三个常用图像描述数据集中表现更为出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的链条思维子空间元学习策略在性能方面超越了基准模型，并且能够更好地处理视觉和语言模态之间的领域差距问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A large-scale vision and language model that has been pretrained on massivedata encodes visual and linguistic prior, which makes it easier to generateimages and language that are more natural and realistic. Despite this, there isstill a significant domain gap between the modalities of vision and language,especially when training data is scarce in few-shot settings, where only verylimited data are available for training. In order to mitigate this issue, amulti-modal meta-learning framework has been proposed to bridge the gap betweentwo frozen pretrained large vision and language models by introducing a tunableprompt connecting these two large models. For few-shot image captioning, theexisting multi-model meta-learning framework utilizes a one-step promptingscheme to accumulate the visual features of input images to guide the languagemodel, which struggles to generate accurate image descriptions with only a fewtraining samples. Instead, we propose a chain-of-thought (CoT) meta-learningscheme as a multi-step image captioning procedure to better imitate how humansdescribe images. In addition, we further propose to learn differentmeta-parameters of the model corresponding to each CoT step in distinctsubspaces to avoid interference. We evaluated our method on three commonly usedimage captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, underfew-shot settings. The results of our experiments indicate that ourchain-of-thought subspace meta-learning strategy is superior to the baselinesin terms of performance across different datasets measured by differentmetrics.</description>
      <author>example@mail.com (Hao Huang, Shuaihang Yuan, Yu Hao, Congcong Wen, Yi Fang)</author>
      <guid isPermaLink="false">2502.13942v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads</title>
      <link>http://arxiv.org/abs/2502.13963v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MuDAF方法来解决大型语言模型在长上下文问答任务中由于输入信息不相关而导致的注意力分散问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型常因输入中的无关信息而表现出注意力分散，严重损害了它们处理长上下文的能力。最近的研究表明检索头在长上下文事实性方面的有效性。&lt;h4&gt;目的&lt;/h4&gt;通过改进检索头直接解决这种分心问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一个多文档注意聚焦（MuDAF）的新颖方法，该方法通过对比学习显式优化头部级别的注意力分布。&lt;h4&gt;主要发现&lt;/h4&gt;根据实验结果，MuDAF可以显著提高大型语言模型在长上下文问答任务中的表现，特别是在多文档问答场景下。广泛的检索评分和注意可视化评估表明MuDAF具有使注意力头更专注于相关信息并减少注意力分散的潜力。&lt;h4&gt;结论&lt;/h4&gt;MuDAF通过优化头部级别的注意力分布来解决大型语言模型中存在的注意力分散问题，并在长上下文问答任务中显示出显著改善的表现，特别是在多文档问答方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) frequently show distracted attention due toirrelevant information in the input, which severely impairs their long-contextcapabilities. Inspired by recent studies on the effectiveness of retrievalheads in long-context factutality, we aim at addressing this distraction issuethrough improving such retrieval heads directly. We propose Multi-DocumentAttention Focusing (MuDAF), a novel method that explicitly optimizes theattention distribution at the head level through contrastive learning.According to the experimental results, MuDAF can significantly improve thelong-context question answering performance of LLMs, especially inmulti-document question answering. Extensive evaluations on retrieval scoresand attention visualizations show that MuDAF possesses great potential inmaking attention heads more focused on relevant information and reducingattention distractions.</description>
      <author>example@mail.com (Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang)</author>
      <guid isPermaLink="false">2502.13963v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Graph Embeddings for Session-based Recommendation with Item Features</title>
      <link>http://arxiv.org/abs/2502.13763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;论文&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合图神经网络和基于会话相似性的推荐算法的新方法——Graph Convolutional Network Extension (GCNext)，该方法通过在图卷积网络中直接集成物品特征来增强现有的序列推荐系统。&lt;h4&gt;背景&lt;/h4&gt;现有最先进的顺序推荐算法主要采用图神经网络建模会话或利用物品特性之间的相似性进行推荐。这些方法要么使用复杂的计算模型，要么依赖于大量的用户行为数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的序列推荐方法GCNext，该方法结合了两种当前主流的推荐技术的优点，通过在图表示中直接整合物品特征来改进会话基线推荐系统。&lt;h4&gt;方法&lt;/h4&gt;GCNext创建一个丰富物品共现图并利用无监督学习方式获取对应的物品嵌入。此外，研究团队还将这种方法与最近邻算法和神经网络模型相结合以验证其效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在三个数据集上将GCNext融入序列推荐算法可以显著提升近似邻居方法以及神经网络模型的性能，并且提高了MRR@20指标值高达12.79%。此外，该技术对于最先进的推荐方法来说易于集成并且具有灵活性。&lt;h4&gt;结论&lt;/h4&gt;通过直接整合物品特征到图卷积网络中的创新性设计，GCNext不仅改进了现有推荐系统的性能还提供了一种新的研究方向和策略用于未来的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;在基于会话的推荐系统中，预测是根据用户在此会话之前的行动进行的。最先进的顺序推荐算法要么使用图形神经网络来建模图中的会话，要么通过利用物品特征来挖掘会话之间的相似性。本文结合这两种方法并提出了一种新颖的方法Graph Convolutional Network Extension (GCNext)，它直接将项目特性纳入到图表示中。 GCNext创建了一个丰富的共现图，并在无监督的方式下学习对应的物品嵌入。我们在三个数据集上展示了将其融入顺序推荐算法可以显著提升最近邻方法和神经网络模型的性能，我们的灵活扩展易于在最先进的方法中集成并且可提高MRR@20达12.79%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In session-based recommender systems, predictions are based on the user'spreceding behavior in the session. State-of-the-art sequential recommendationalgorithms either use graph neural networks to model sessions in a graph orleverage the similarity of sessions by exploiting item features. In this paper,we combine these two approaches and propose a novel method, Graph ConvolutionalNetwork Extension (GCNext), which incorporates item features directly into thegraph representation via graph convolutional networks. GCNext creates afeature-rich item co-occurrence graph and learns the corresponding itemembeddings in an unsupervised manner. We show on three datasets thatintegrating GCNext into sequential recommendation algorithms significantlyboosts the performance of nearest-neighbor methods as well as neural networkmodels. Our flexible extension is easy to incorporate in state-of-the-artmethods and increases the MRR@20 by up to 12.79%.</description>
      <author>example@mail.com (Andreas Peintner, Marta Moscati, Emilia Parada-Cabaleiro, Markus Schedl, Eva Zangerle)</author>
      <guid isPermaLink="false">2502.13763v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Model Agnostic Social Influence Maximization in Hyperbolic Space</title>
      <link>http://arxiv.org/abs/2502.13571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了HIM，一种利用双曲表示学习来估计用户潜在影响力传播的新颖扩散模型无关方法。&lt;h4&gt;背景&lt;/h4&gt;传统影响最大化（IM）问题的方法依赖于具有已知参数的固定扩散模型，这限制了它们在现实场景中的应用。基于图表示的学习方法虽然能够克服这一局限性，但现有的研究建立在欧氏空间上，无法有效捕捉社会影响力分布的潜在层次特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的影响最大化（IM）问题解决方法，以更好地适应现实世界的社会网络。&lt;h4&gt;方法&lt;/h4&gt;HIM包括两个关键组成部分：一是双曲影响力表示模块，它从网络结构和历史影响力激活中编码出有表现力的双曲用户表示；二是自适应种子选择模块，该模块利用学习到的用户表示的位置信息灵活有效地选择种子用户。&lt;h4&gt;主要发现&lt;/h4&gt;在五个网络数据集上的广泛实验表明了HIM方法对于具有未知扩散模型参数的影响最大化问题的有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;通过使用双曲空间中的几何属性来反映用户的影响力大小，以及自适应的种子选择模块，HIM能够更准确地预测用户潜在的影响力传播，并在大规模现实世界的社会网络中展现出巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;影响最大化（IM）问题旨在寻找一组具有影响力的用户，以最大限度地扩大他们在社交网络中的影响力。传统的解决方案依赖于固定扩散模型和已知参数，这限制了它们在真实场景中的应用。基于图表示学习的方法虽然能够克服这一局限性，但现有的研究建立在欧氏空间上，无法有效捕捉社会影响力分布的潜在层次特征。为解决这些问题，我们提出了HIM，一种新颖的扩散模型无关方法，利用双曲表示学习从社交传播数据中估计用户潜在的影响传播。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Influence Maximization (IM) problem aims to find a small set ofinfluential users to maximize their influence spread in a social network.Traditional methods rely on fixed diffusion models with known parameters,limiting their generalization to real-world scenarios. In contrast, graphrepresentation learning-based methods have gained wide attention for overcomingthis limitation by learning user representations to capture influencecharacteristics. However, existing studies are built on Euclidean space, whichfails to effectively capture the latent hierarchical features of socialinfluence distribution. As a result, users' influence spread cannot beeffectively measured through the learned representations. To alleviate theselimitations, we propose HIM, a novel diffusion model agnostic method thatleverages hyperbolic representation learning to estimate users' potentialinfluence spread from social propagation data. HIM consists of two keycomponents. First, a hyperbolic influence representation module encodesinfluence spread patterns from network structure and historical influenceactivations into expressive hyperbolic user representations. Hence, theinfluence magnitude of users can be reflected through the geometric propertiesof hyperbolic space, where highly influential users tend to cluster near thespace origin. Second, a novel adaptive seed selection module is developed toflexibly and effectively select seed users using the positional information oflearned user representations. Extensive experiments on five network datasetsdemonstrate the superior effectiveness and efficiency of our method for the IMproblem with unknown diffusion model parameters, highlighting its potential forlarge-scale real-world social networks.</description>
      <author>example@mail.com (Hongliang Qiao)</author>
      <guid isPermaLink="false">2502.13571v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection</title>
      <link>http://arxiv.org/abs/2502.13171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Intelligent Cybersecurity Conference (ICSC2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;钓鱼攻击是当今最常见的网络攻击类型，被认为是数据泄露的主要源头，并对个人和企业造成重大后果。&lt;h4&gt;背景&lt;/h4&gt;网页基础的钓鱼攻击最为频繁，通过社交媒体帖子或包含链接到钓鱼网站的电子邮件来实施。这些现有的检测方法依赖于监督学习技术，需要大量数据进行训练，具有高计算需求，并且侵犯用户隐私。此外，由于生成式人工智能技术的发展，现有系统对于日益演变的安全威胁缺乏抵抗力。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督的学习方法，以解决现有钓鱼URL检测系统的不足，实现快速、可扩展的检测方案，同时保持用户的隐私。&lt;h4&gt;方法&lt;/h4&gt;采用了一种不涉及配对比较的新颖无监督学习方法，能够实时识别完整的钓鱼攻击活动，并且对于使用生成式AI技术创建的目标性更强的钓鱼URL也能有效防御。&lt;h4&gt;主要发现&lt;/h4&gt;该论文所提出的无监督学习方法能够在检测效率和准确性上超越现有解决方案。它可以在没有用户数据的前提下高效地工作，同时仍能提供高准确率的威胁识别。&lt;h4&gt;结论&lt;/h4&gt;这种新颖的方法为应对不断变化的安全环境提供了有力工具，并在保护用户隐私的同时增强了对抗生成式AI驱动钓鱼攻击的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了现今最常见且最具危害性的网络攻击——钓鱼攻击所带来的问题。当前基于监督学习技术的解决方案存在计算需求高、侵犯隐私及抵抗新兴威胁能力弱等问题。论文提出了一个新的无监督方法，旨在提高检测速度和准确性，同时保护用户隐私，并应对由生成式AI驱动的新一轮有针对性的钓鱼URL攻击浪潮。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Phishing is the most prevalent type of cyber-attack today and is recognizedas the leading source of data breaches with significant consequences for bothindividuals and corporations. Web-based phishing attacks are the most frequentwith vectors such as social media posts and emails containing links to phishingURLs that once clicked on render host systems vulnerable to more sinisterattacks. Research efforts to detect phishing URLs have involved the use ofsupervised learning techniques that use large amounts of data to train modelsand have high computational requirements. They also involve analysis offeatures derived from vectors including email contents thus affecting userprivacy. Additionally, they suffer from a lack of resilience against evolutionof threats especially with the advent of generative AI techniques to bypassthese systems as with AI-generated phishing URLs. Unsupervised methods such asclustering techniques have also been used in phishing detection in the past,however, they are at times unscalable due to the use of pair-wise comparisons.They also lack high detection rates while detecting phishing campaigns. In thispaper, we propose an unsupervised learning approach that is not only fast butscalable, as it does not involve pair-wise comparisons. It is able to detectentire campaigns at a time with a high detection rate while preserving userprivacy; this includes the recent surge of campaigns with targeted phishingURLs generated by malicious entities using generative AI techniques.</description>
      <author>example@mail.com (Muhammad Fahad Zia, Sri Harish Kalidass)</author>
      <guid isPermaLink="false">2502.13171v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models</title>
      <link>http://arxiv.org/abs/2502.13278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了使用表情符号进行情感分析和基于文本的情感分类，并采用分布式训练方式提高了模型的效率。&lt;h4&gt;背景&lt;/h4&gt;当前，数字世界中频繁使用表情符号来表达从简单到复杂的思想。因此，在情感分析和定向营销活动中也越来越多地使用它们。&lt;h4&gt;目的&lt;/h4&gt;研究使用不同嵌入模型对推特及Kaggle上的情感数据进行情感分类，并探索分布式训练方法以提高效率。&lt;h4&gt;方法&lt;/h4&gt;{'模型选择': '采用Universal Sentence Encoder (USE) 和Sentence Bidirectional Encoder Representations from Transformers (SBERT)生成句子嵌入，用于训练标准全连接神经网络（NN）和LSTM NN模型。', '数据处理': '对推特文本使用上述嵌入模型进行情感分类。同时，利用表情符号数据集作为验证集来评估模型性能。', '分布式训练': '采用分布式训练方法替代传统单线程模型以提高可扩展性，减少了大约15%的运行时间而不会牺牲准确性。', '解释性AI': '使用Shap算法解释模型行为并检查给定特征集中的潜在偏见。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'情感分类准确度': '对于推特文本的情感分析，全连接NN和LSTM NN模型的分类准确率都在98%左右。', '表情符号验证数据集': '当使用训练集中没有的表情符号作为验证集时，两个模型的准确率都急剧下降到70%左右。'}&lt;h4&gt;结论&lt;/h4&gt;虽然表情符号在情感分析中的准确性存在局限性，但采用分布式训练方法可以提高计算效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，在当前数字世界中，表情符号被广泛用于表达各种思想，并且在情感分析和定向营销活动中也得到了应用。研究者对Twitter数据及Kaggle上的表情符号数据进行了情感分类，使用了Universal Sentence Encoder (USE) 和Sentence Bidirectional Encoder Representations from Transformers (SBERT)，生成句子嵌入并训练标准全连接神经网络（NN）和LSTM NN模型。对于推特文本的测试集，两种模型的情感分类准确率均约为98%；然而，当验证数据集中包含未出现在训练中的表情符号时，两个模型的准确度大幅下降至70%左右。此外，研究还使用了分布式训练方法来提高模型可扩展性，并通过Shap算法进行了解释性AI的应用，以检查给定特征集上的潜在偏见。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.12720/jait.13.2.167-172&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emojis are being frequently used in todays digital world to express fromsimple to complex thoughts more than ever before. Hence, they are also beingused in sentiment analysis and targeted marketing campaigns. In this work, weperformed sentiment analysis of Tweets as well as on emoji dataset from theKaggle. Since tweets are sentences we have used Universal Sentence Encoder(USE) and Sentence Bidirectional Encoder Representations from Transformers(SBERT) end-to-end sentence embedding models to generate the embeddings whichare used to train the Standard fully connected Neural Networks (NN), and LSTMNN models. We observe the text classification accuracy was almost the same forboth the models around 98 percent. On the contrary, when the validation set wasbuilt using emojis that were not present in the training set then the accuracyof both the models reduced drastically to 70 percent. In addition, the modelswere also trained using the distributed training approach instead of atraditional singlethreaded model for better scalability. Using the distributedtraining approach, we were able to reduce the run-time by roughly 15% withoutcompromising on accuracy. Finally, as part of explainable AI the Shap algorithmwas used to explain the model behaviour and check for model biases for thegiven feature set.</description>
      <author>example@mail.com (Sirisha Velampalli, Chandrashekar Muniyappa, Ashutosh Saxena)</author>
      <guid isPermaLink="false">2502.13278v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics</title>
      <link>http://arxiv.org/abs/2502.13785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Helix-mRNA，一种用于优化mRNA疫苗序列的深度学习模型。该模型通过结构化的状态空间和注意力机制的结合，在处理UTR（非编码区）和编码区域时表现出色。&lt;h4&gt;背景&lt;/h4&gt;基于mRNA的疫苗在制药行业受到高度重视，但mRNA序列中包括编码区和非翻译区在内的各种因素共同决定了疫苗的有效性。当前深度学习模型主要关注于优化编码区而忽略了非翻译区。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时优化mRNA序列中的UTR和编码区域，并且具备高效处理长序列能力的深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;引入了Helix-mRNA，一个结合结构化状态空间和注意力机制的混合模型。该模型使用单核苷酸标记化及密码子分离来保持原始mRNA序列中的生物和结构性信息，在初次预训练之后进行了第二次高质数据预训练以提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;Helix-mRNA在分析UTR和编码区域特性方面超越了现有方法，能够处理比当前技术长六倍的序列且仅使用其他基础模型10%的参数。此外，其预测能力覆盖所有mRNA区域。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种高效的深度学习框架Helix-mRNA用于优化mRNA疫苗序列，它在准确性、效率和数据利用方面都有显著改进，并将源代码与权重开放共享以供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;基于mRNA的疫苗已成为制药业的主要焦点。编码序列以及非翻译区（UTRs）可以强烈影响蛋白质合成的效率、稳定性和降解等特性，这些共同决定了疫苗的效果。然而，优化mRNA序列仍然是一项复杂的挑战。现有的深度学习模型通常仅关注于编码区域的优化，而忽略了UTR。我们提出了Helix-mRNA，这是一种基于结构化的状态空间和注意力机制相结合的方法来解决这些问题。除了初步预训练外，通过高质量数据的二次预训练使模型专业化。采用单核苷酸标记法对mRNA序列进行处理，并分离密码子以确保保留原始mRNA序列中的生物和结构信息。Helix-mRNA在分析UTR和编码区域性质方面超过了现有方法的表现，在仅使用当前基础模型10%参数的情况下，可以处理比目前技术长六倍的序列。该模型具有广泛的预测能力，适用于所有mRNA区段。我们将开放源代码（https://github.com/helicalAI/helical）及模型权重（https://huggingface.co/helical-ai/helix-mRNA）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; mRNA-based vaccines have become a major focus in the pharmaceutical industry.The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA canstrongly influence translation efficiency, stability, degradation, and otherfactors that collectively determine a vaccine's effectiveness. However,optimizing mRNA sequences for those properties remains a complex challenge.Existing deep learning models often focus solely on coding region optimization,overlooking the UTRs. We present Helix-mRNA, a structured state-space-based andattention hybrid model to address these challenges. In addition to a firstpre-training, a second pre-training stage allows us to specialise the modelwith high-quality data. We employ single nucleotide tokenization of mRNAsequences with codon separation, ensuring prior biological and structuralinformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,outperforms existing methods in analysing both UTRs and coding regionproperties. It can process sequences 6x longer than current approaches whileusing only 10% of the parameters of existing foundation models. Its predictivecapabilities extend to all mRNA regions. We open-source the model(https://github.com/helicalAI/helical) and model weights(https://huggingface.co/helical-ai/helix-mRNA).</description>
      <author>example@mail.com (Matthew Wood, Mathieu Klop, Maxime Allard)</author>
      <guid isPermaLink="false">2502.13785v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>AI-Driven Discovery of High Performance Polymer Electrodes for Next-Generation Batteries</title>
      <link>http://arxiv.org/abs/2502.13899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 10 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;使用过渡族金属在电动电池中的应用，面临着锂、钴和镍等关键元素大量消耗的挑战，这些元素对环境造成了显著的影响。为了减少这种影响，用具有氧化还原活性的有机材料替代这些金属是一种有希望的方法，可以将电池的碳足迹降低一个数量级。&lt;h4&gt;背景&lt;/h4&gt;过渡族金属在电动电池中的广泛应用导致了锂、钴和镍等关键元素资源的巨大消耗，这对环境构成了严重挑战。为了减轻这一问题，研究转向寻找新的材料来取代这些金属。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于机器学习的电池信息学框架，以加速并优化具有氧化还原活性的有机材料的选择、优化与设计过程，从而克服其电压和特定容量方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;建立了一个结合数据融合和元学习模型的机器学习框架，该框架能够预测各种有机负极材料和载流子（正极材料）组合下的电池性能参数，包括电压和特定容量。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用先进的机器学习技术和广泛的电池数据库，研究成功地识别并设计出了适合可持续储能技术的候选材料。&lt;h4&gt;结论&lt;/h4&gt;该研究成果为加速具有氧化还原活性有机材料的研究提供了重要的手段，有助于推动更环保、高效的能源存储解决方案的发展。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要是关于探索使用红ox活性有机材料替代过渡族金属在电池中的应用。通过机器学习驱动的方法，研究旨在克服现有材料的局限性，并推进可持续储能技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of transition group metals in electric batteries requires extensiveusage of critical elements like lithium, cobalt and nickel, which posessignificant environmental challenges. Replacing these metals with redox-activeorganic materials offers a promising alternative, thereby reducing the carbonfootprint of batteries by one order of magnitude. However, this approach facescritical obstacles, including the limited availability of suitable redox-activeorganic materials and issues such as lower electronic conductivity, voltage,specific capacity, and long-term stability. To overcome the limitations forlower voltage and specific capacity, a machine learning (ML) driven batteryinformatics framework is developed and implemented. This framework utilizes anextensive battery dataset and advanced ML techniques to accelerate and enhancethe identification, optimization, and design of redox-active organic materials.In this contribution, a data-fusion ML coupled meta learning model capable ofpredicting the battery properties, voltage and specific capacity, for variousorganic negative electrodes and charge carriers (positive electrode materials)combinations is presented. The ML models accelerate experimentation, facilitatethe inverse design of battery materials, and identify suitable candidates fromthree extensive material libraries to advance sustainable energy-storagetechnologies.</description>
      <author>example@mail.com (Subhash V. S. Ganti, Lukas Woelfel, Christopher Kuenneth)</author>
      <guid isPermaLink="false">2502.13899v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets</title>
      <link>http://arxiv.org/abs/2502.13833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;合成数据作为一种隐私保护技术在医疗和金融等行业中受到关注。为了确保使用合成数据的实际应用中的隐私保障，本文提出了一种对比学习方法，该方法通过将数据嵌入到更具代表性的空间来改进对合成数据集的隐私评估。&lt;h4&gt;背景&lt;/h4&gt;利用合成数据提供保护保证是一个关键问题，尤其是在医疗和金融等敏感行业中。目前有两种主要的方法：基于相似度的方法以及基于攻击的方法。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的对比学习方法以提升合成数据集的隐私评估，并通过实验验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了将数据嵌入到更加代表性的空间中的对比学习方法，使其能够利用直观的距离测量指标来改进对隐私保护度的估计。同时，该研究还比较了基于相似性和攻击的方法在使用和不使用此嵌入技术时的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相对简单且易于实施的隐私评估指标可以与更复杂、专门针对GDPR条件设计的指标一样有效。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种改进合成数据集隐私保护度评估的新方法，并展示了其在多种公共可用数据集上的有效性。这种方法为实际应用中的合成数据分析提供了一个有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;合成数据作为增强隐私的技术，在医疗和金融领域等得到了广泛应用。为了保障将合成数据应用于实践时的隐私安全，研究文献中提出了两种处理表格数据的方法：基于相似度的方法旨在寻找训练数据与合成数据之间的相似性程度；而基于攻击的方法则故意对合成数据集进行攻击，通过成功概率来评估其安全性。本文介绍了一种对比方法，该方法通过将数据嵌入到更具代表性的空间来改善对生成数据隐私保护的评价，从而克服了不同类型和属性的数据面临的障碍，并使得使用直观的距离度量指标成为可能。在一系列公开可用数据集上进行的实验中，我们比较了基于相似性和攻击的方法，在使用与不使用对比学习基嵌入的情况下两种方法的表现情况。我们的结果表明，相对简单且容易实现的隐私评估指标可以像更高级、专门针对GDPR条件设计的隐私模型一样有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)in sectors such as healthcare and finance. When using synthetic data inpractical applications, it is important to provide protection guarantees. Inthe literature, two family of approaches are proposed for tabular data: on theone hand, Similarity-based methods aim at finding the level of similaritybetween training and synthetic data. Indeed, a privacy breach can occur if thegenerated data is consistently too similar or even identical to the train data.On the other hand, Attack-based methods conduce deliberate attacks on syntheticdatasets. The success rates of these attacks reveal how secure the syntheticdatasets are.  In this paper, we introduce a contrastive method that improves privacyassessment of synthetic datasets by embedding the data in a more representativespace. This overcomes obstacles surrounding the multitude of data types andattributes. It also makes the use of intuitive distance metrics possible forsimilarity measurements and as an attack vector. In a series of experimentswith publicly available datasets, we compare the performances ofsimilarity-based and attack-based methods, both with and without use of thecontrastive learning-based embeddings. Our results show that relativelyefficient, easy to implement privacy metrics can perform equally well as moreadvanced metrics explicitly modeling conditions for privacy referred to by theGDPR.</description>
      <author>example@mail.com (Milton Nicolás Plasencia Palacios, Sebastiano Saccani, Gabriele Sgroi, Alexander Boudewijn, Luca Bortolussi)</author>
      <guid isPermaLink="false">2502.13833v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Integrated Sensing and Communication for 6G Holographic Digital Twins</title>
      <link>http://arxiv.org/abs/2502.13352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了6G网络下全息通信的发展前景，特别是全息数字孪生（HDT）的应用。提出了一个基于感知与通信集成(ISAC)的四层架构来支持低成本、高精度环境数据收集以构建HDT。&lt;h4&gt;背景&lt;/h4&gt;随着6G网络的到来和终端设备分辨率的提升，全息通信逐渐成为可能。HDT是其关键应用之一，能够实时映射和预测物理实体状态，并进行空间信息三维再现。&lt;h4&gt;目的&lt;/h4&gt;提出一种集成感知与通信（ISAC）辅助架构以支持低成本、高精度环境数据收集用于构建HDT。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一个四层架构，通过探索超分辨率技术来增强感知解析度，同时研究多点协作感应在构建HDT中的应用，并详细回顾了四种关键技术：节点选择、多频带合作、协作波束形成和数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;论文提出并分析了几种可以提高全息数字孪生性能的关键技术和方法。其中强调了超分辨率技术和多点协同感应的重要性。&lt;h4&gt;结论&lt;/h4&gt;本文指出了几个未来研究的有趣方向，旨在指导和启发后续的工作。&lt;h4&gt;翻译&lt;/h4&gt;随着6G网络的到来，提供超高带宽和极低延迟，并且终端设备分辨率的提升，全息通信正逐渐成为现实。HDT被认为是全息通信中的关键应用之一，能够为物理实体的状态进行实时映射与预测，并实现空间信息的三维再现。在此背景下，感知与通信集成(ISAC)有望成为一个提供数据源给HDT的关键路径。本文提出了一种基于ISAC辅助四层架构的HDT方案，整合新兴范式和技术以实现低成本、高精度环境数据收集来构建HDT。具体而言，在提高感应分辨率方面，从参数估计和点云构造的角度探讨了超分辨率技术，并专注于多点协作感应在构建HDT中的应用，提供节点选择、多频带合作、协作波束形成和数据融合四种关键技术的全面回顾。最后指出了几个未来研究的方向以指导并启发后续工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the advent of 6G networks, offering ultra-high bandwidth and ultra-lowlatency, coupled with the enhancement of terminal device resolutions,holographic communication is gradually becoming a reality. Holographic digitaltwin (HDT) is considered one of key applications of holographic communication,capable of creating virtual replicas for real-time mapping and prediction ofphysical entity states, and performing three-dimensional reproduction ofspatial information. In this context, integrated sensing and communication(ISAC) is expected to be a crucial pathway for providing data sources to HDT.This paper proposes a four-layer architecture assisted by ISAC for HDT,integrating emerging paradigms and key technologies to achieve low-cost,high-precision environmental data collection for constructing HDT.Specifically, to enhance sensing resolution, we explore super-resolutiontechniques from the perspectives of parameter estimation and point cloudconstruction. Additionally, we focus on multi-point collaborative sensing forconstructing HDT, and provide a comprehensive review of four key techniques:node selection, multi-band collaboration, cooperative beamforming, and datafusion. Finally, we highlight several interesting research directions to guideand inspire future work.</description>
      <author>example@mail.com (Haijun Zhang, Ziyang Zhang, Xiangnan Liu, Wei Li, Haojin Li, Chen Sun)</author>
      <guid isPermaLink="false">2502.13352v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models</title>
      <link>http://arxiv.org/abs/2502.13656v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的句子嵌入生成和精炼的方法，该方法通过控制大规模语言模型（LLMs）在潜在空间中的生成方向来确保语义差异，并结合排名信息对现有句子嵌入模型进行改进。&lt;h4&gt;背景&lt;/h4&gt;对比学习方法在使用如NLI等标注数据集的情况下为许多自然语言处理任务提供了强大的句嵌入，但依赖人工标签限制了可扩展性。最近的研究利用大规模语言模型生成句子对以减少注释需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的控制大规模语言模型生成方向的方法，并结合排名信息来改进现有句子嵌入模型的性能。&lt;h4&gt;方法&lt;/h4&gt;通过控制LLMs在潜在空间中的生成方向，确保语义差异，并引入排名和语义信息来精炼现有的句子嵌入模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在适度增加句对合成成本的情况下，该方法达到了新的SOTA（State-of-the-Art）性能水平。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在不显著增加计算成本的前提下提高了句子嵌入的精度和语义区分能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sentence embedding is essential for many NLP tasks, with contrastive learningmethods achieving strong performance using annotated datasets like NLI. Yet,the reliance on manual labels limits scalability. Recent studies leverage largelanguage models (LLMs) to generate sentence pairs, reducing annotationdependency. However, they overlook ranking information crucial for fine-grainedsemantic distinctions. To tackle this challenge, we propose a method forcontrolling the generation direction of LLMs in the latent space. Unlikeunconstrained generation, the controlled approach ensures meaningful semanticdivergence. Then, we refine exist sentence embedding model by integratingranking information and semantic information. Experiments on multiplebenchmarks demonstrate that our method achieves new SOTA performance with amodest cost in ranking sentence synthesis.</description>
      <author>example@mail.com (Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen)</author>
      <guid isPermaLink="false">2502.13656v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective</title>
      <link>http://arxiv.org/abs/2502.13732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的联邦学习方法FedGSP，该方法通过挖掘图的谱性质来解决图联邦学习中的同质性异质性问题。&lt;h4&gt;背景&lt;/h4&gt;现有的许多方法主要关注于处理节点特征异质性和结构异质性，而忽视了同质性水平在不同客户端之间存在的显著变化，即同质性异质性。这个问题导致局部模型之间的合作效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的联邦学习方法FedGSP，以有效地解决图数据中的同质性异质性问题，并提高各客户端间的协作效率和性能。&lt;h4&gt;方法&lt;/h4&gt;引入谱Graph神经网络(GNN)，并通过挖掘图的谱性质来实现跨客户端共享通用的谱属性(即低频信息)。此外，允许客户端通过获取其缺乏的谱特性（即高频信息）来互补非通用的谱特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了FedGSP在处理同质性和异质性图数据时的表现优越性，并且在所有异质性数据集上比现有最佳方法平均高出3.28%。&lt;h4&gt;结论&lt;/h4&gt;通过挖掘和利用图形的谱性质，可以有效地解决联邦学习中的同质性异质性问题。FedGSP提供了一种有效的方法来提高各客户端间模型合作的效果。&lt;h4&gt;翻译&lt;/h4&gt;由于异质性是图联邦学习的基本挑战，许多现有方法主要关注于处理节点特征异质性和结构异质性。然而，它们忽视了关键的同质性异质性问题，即不同客户图数据中的同质水平存在显著变化。同质水平表示连接属于同一类别的节点之间的边的比例。由于适应各自的本地同质性，局部模型在不同的客户端之间捕获不一致的频谱特性，这大大减少了合作的有效性。具体来说，在高同质性的图上训练的局部模型倾向于捕捉低频信息，而那些在低同质性的图上训练的局部模型则倾向于捕捉高频信息。为了有效处理同质异质性问题，我们引入了谱Graph神经网络（GNN），并提出了一种通过挖掘图形谱特性进行联邦学习的新方法FedGSP。一方面，我们的提议FedGSP使客户端能够共享通用的频谱属性（即低频信息），从而使所有参与者都能从合作中受益。另一方面，受到理论发现的启发，我们提出的FedGSP允许客户端通过获取它们所缺乏的频谱特征来补充非通用的谱特性（即高频信息），从而获得额外的信息增益。在六种同质性和五种异质性图数据集上的广泛实验表明了我们的方法优于现有的11个最先进的方法。值得注意的是，我们的FedGSP在所有异质性数据集中比第二好的方法平均高出3.28%的性能差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since heterogeneity presents a fundamental challenge in graph federatedlearning, many existing methods are proposed to deal with node featureheterogeneity and structure heterogeneity. However, they overlook the criticalhomophily heterogeneity, which refers to the substantial variation in homophilylevels across graph data from different clients. The homophily level representsthe proportion of edges connecting nodes that belong to the same class. Due toadapting to their local homophily, local models capture inconsistent spectralproperties across different clients, significantly reducing the effectivenessof collaboration. Specifically, local models trained on graphs with highhomophily tend to capture low-frequency information, whereas local modelstrained on graphs with low homophily tend to capture high-frequencyinformation. To effectively deal with homophily heterophily, we introduce thespectral Graph Neural Network (GNN) and propose a novel Federated learningmethod by mining Graph Spectral Properties (FedGSP). On one hand, our proposedFedGSP enables clients to share generic spectral properties (i.e.,low-frequency information), allowing all clients to benefit throughcollaboration. On the other hand, inspired by our theoretical findings, ourproposed FedGSP allows clients to complement non-generic spectral properties byacquiring the spectral properties they lack (i.e., high-frequency information),thereby obtaining additional information gain. Extensive experiments conductedon six homophilic and five heterophilic graph datasets, across bothnon-overlapping and overlapping settings, validate the superiority of ourmethod over eleven state-of-the-art methods. Notably, our FedGSP outperformsthe second-best method by an average margin of 3.28% on all heterophilicdatasets.</description>
      <author>example@mail.com (Wentao Yu)</author>
      <guid isPermaLink="false">2502.13732v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2502.13555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种黑盒上下文驱动的图数据增强方法，该方法利用大型语言模型（LLM）生成的知识图谱来丰富原始图数据。&lt;h4&gt;背景&lt;/h4&gt;图表示学习中由于图数据稀疏性和噪声问题，需要进行数据增强。现有的大多数增强方法忽视了来自数据集的上下文信息，并且依赖于图结构单一地进行数据增强。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于LLM指导的数据增强方法，以便更好地利用图数据中的上下文知识并提高预测性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;该方法通过使用文本提示作为与上下文相关的信息来引导LLM生成知识图谱，并设计了一种动态合并策略以随机地将生成的知识图谱合并到原始图中。同时，提出了一种粒度感知的指令微调模块，根据数据集的不同粒度级别无缝生成文本提示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在各种图学习任务上都优于现有图数据增强方法，并且特别适用于电子健康记录（EHR）等场景。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够最大化地利用上下文知识，从而提升预测性能和可解释性。此方法可以克服现有LLM封闭源代码的问题，使之为更多人所用。&lt;h4&gt;翻译&lt;/h4&gt;为了克服这些限制，我们提出了一个黑盒上下文驱动的图数据增强方法——DemoGraph，在大型语言模型的指导下进行操作。通过使用文本提示作为与上下文相关的信息，我们让大模型生成知识图谱，这使我们可以捕捉到从文本输出中的结构交互。然后，我们在训练过程中设计了一个动态合并策略，随机地将这些由LLM生成的知识图谱整合进原始图中。为了控制增强后的图的稀疏性，我们还开发了一种粒度感知提示策略和指令微调模块，这可以根据数据集的不同粒度级别无缝生成文本提示。在各种图学习任务上的广泛实验验证了我们的方法优于现有的图数据增强方法。特别地，在涉及电子健康记录（EHR）的情境下，我们的方法表现出色，表明其能够充分运用上下文知识，从而提升预测性能和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data augmentation is necessary for graph representation learning due to thescarcity and noise present in graph data. Most of the existing augmentationmethods overlook the context information inherited from the dataset as theyrely solely on the graph structure for augmentation. Despite the success ofsome large language model-based (LLM) graph learning methods, they are mostlywhite-box which require access to the weights or latent features from theopen-access LLMs, making them difficult to be democratized for everyone asexisting LLMs are mostly closed-source for commercial considerations. Toovercome these limitations, we propose a black-box context-driven graph dataaugmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging thetext prompt as context-related information, we task the LLM with generatingknowledge graphs (KGs), which allow us to capture the structural interactionsfrom the text outputs. We then design a dynamic merging schema tostochastically integrate the LLM-generated KGs into the original graph duringtraining. To control the sparsity of the augmented graph, we further devise agranularity-aware prompting strategy and an instruction fine-tuning module,which seamlessly generates text prompts according to different granularitylevels of the dataset. Extensive experiments on various graph learning tasksvalidate the effectiveness of our method over existing graph data augmentationmethods. Notably, our approach excels in scenarios involving electronic healthrecords (EHRs), which validates its maximal utilization of contextualknowledge, leading to enhanced predictive performance and interpretability.</description>
      <author>example@mail.com (Yushi Feng, Tsai Hor Chan, Guosheng Yin, Lequan Yu)</author>
      <guid isPermaLink="false">2502.13555v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Towards Invariance to Node Identifiers in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.13660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2411.02271&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的正则化方法，用于增强图神经网络（GNN）对于节点ID不变性的能力，提高了模型的泛化性能。&lt;h4&gt;背景&lt;/h4&gt;消息传递结构导致了图神经网络表达力受限的问题。添加唯一节点标识符可以打破限制表达力的基本对称性。&lt;h4&gt;目的&lt;/h4&gt;解决现有ID框架的关键局限，并提出一种新的方法来实现节点ID不变性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的正则化技术，该技术能有效增强GNN对于节点ID的不变性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在现实世界和合成任务上，所提出的模型改善了节点ID不变性，进而提高了泛化性能。&lt;h4&gt;结论&lt;/h4&gt;新的正则化方法增强了图神经网络模型对节点标识符变化的鲁棒性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;消息传递图神经网络（GNN）由于其消息传递结构而被认为具有有限的表达力。一种规避这种限制的方法是添加独特的节点标识符，这打破了导致表达力受限的基本对称性。在这项工作中，我们指出了ID框架的关键局限，并提出了解决方案。我们首先观察到最终输出不应依赖于特定的ID。然后显示在实践中这是不成立的，这意味着学习网络并不具备所需的结构属性。可以通过几种方式强制执行节点ID不变性，我们将讨论它们的理论性质。接着我们提出了一种新的正则化方法，有效地增强了对网络中的ID不变性的要求。大量的真实世界和合成任务评估表明，我们的方法显著提高了ID不变性，并且在实践中通常也提升了泛化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message-Passing Graph Neural Networks (GNNs) are known to have limitedexpressive power, due to their message passing structure. One mechanism forcircumventing this limitation is to add unique node identifiers (IDs), whichbreak the symmetries that underlie the expressivity limitation. In this work,we highlight a key limitation of the ID framework, and propose an approach foraddressing it. We begin by observing that the final output of the GNN shouldclearly not depend on the specific IDs used. We then show that in practice thisdoes not hold, and thus the learned network does not possess this desiredstructural property. Such invariance to node IDs may be enforced in severalways, and we discuss their theoretical properties.  We then propose a novel regularization method that effectively enforces IDinvariance to the network. Extensive evaluations on both real-world andsynthetic tasks demonstrate that our approach significantly improves IDinvariance and, in turn, often boosts generalization performance.</description>
      <author>example@mail.com (Maya Bechler-Speicher, Moshe Eliasof, Carola-Bibiane Schonlieb, Ran Gilad-Bachrach, Amir Globerson)</author>
      <guid isPermaLink="false">2502.13660v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models</title>
      <link>http://arxiv.org/abs/2502.13734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, Submitted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Confidence-Aware Regression Estimation (CARE)的新模型，用于解决像素级回归任务中的置信度量化和评估问题。&lt;h4&gt;背景&lt;/h4&gt;在深度神经网络的实际应用中，准确地量化和评估信心是非常重要的。然而，在像素级别的回归任务（如建筑密度估计）中，这一方面还没有得到充分的研究。&lt;h4&gt;目的&lt;/h4&gt;开发并验证一种用于解决像素级回归问题的新模型CARE，并展示其相对于其他方法的优势。&lt;h4&gt;方法&lt;/h4&gt;提出了名为Confidence-Aware Regression Estimation (CARE)的模型。该模型可以为回归输出结果计算和分配置信度，从而更好地处理像素级回归任务。&lt;h4&gt;主要发现&lt;/h4&gt;在使用Copernicus Sentinel-2卫星数据进行实验时，显示提出的CARE方法能够成功应用于回归问题，并且其性能优于其他方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在解决像素级回归任务的置信度量化和评估方面表现出色，为实际应用中深度神经网络模型的表现改进提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;执行准确的信心量化和评估对于深度神经网络预测故障、提高性能以及增强其在现实世界中的能力至关重要。然而，与分类任务（如语义分割）相比，在像素级回归任务上这方面的问题并没有得到很好的解决。为了应对这一挑战，我们提出并训练了名为Confidence-Aware Regression Estimation (CARE)的模型。该模型计算并分配给回归输出结果信心值，并聚焦于作为地球观测(AI Foundation Model for Earth Observation, EO)下游任务的回归问题。实验结果显示，在估计建筑密度的任务上，所提出的CARE方法可以成功应用于回归问题，并且在Copernicus Sentinel-2卫星数据集上的表现优于其他方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Performing accurate confidence quantification and assessment is important fordeep neural networks to predict their failures, improve their performance andenhance their capabilities in real-world applications, for their practicaldeployment in real life. For pixel-wise regression tasks, confidencequantification and assessment has not been well addressed in the literature, incontrast to classification tasks like semantic segmentation. The softmax outputlayer is not used in deep neural networks that solve pixel-wise regressionproblems. In this paper, to address these problems, we develop, train andevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Ourmodel CARE computes and assigns confidence to regression output results. Wefocus on solving regression problems as downstream tasks of an AI FoundationModel for Earth Observation (EO). We evaluate the proposed model CARE andexperimental results on data from the Copernicus Sentinel-2 satelliteconstellation for estimating the density of buildings show that the proposedmethod can be successfully applied to regression problems. We also show thatour approach outperforms other methods.</description>
      <author>example@mail.com (Nikolaos Dionelis, Jente Bosmans, Nicolas Longépé)</author>
      <guid isPermaLink="false">2502.13734v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Random Forest Autoencoders for Guided Representation Learning</title>
      <link>http://arxiv.org/abs/2502.13257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Random Forest Autoencoders (RF-AE)的新框架，用于解决监督可视化中的扩展性问题。&lt;h4&gt;背景&lt;/h4&gt;长期以来，无监督数据可视化方法已经非常成熟，而基于专家标签指导表示的监督可视化研究相对较少。尽管最近一种基于扩散和随机森林的方法RF-PHATE取得了一些进展，但其缺乏显式映射函数限制了它在大规模数据集和标签稀疏场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效地将新样本纳入现有模型，并保持高准确性和可解释性的框架。&lt;h4&gt;方法&lt;/h4&gt;RF-AE结合了自编码器的灵活性、随机森林的监督学习优势以及RF-PHATE所捕捉到的信息几何特性，形成了一种新的神经网络基础架构。它解决了RF-PHATE在扩展性上的问题并改善了性能。&lt;h4&gt;主要发现&lt;/h4&gt;RF-AE不仅在准确性和可解释性方面优于现有方法和标准内核扩展的RF-PHATE，还具有对超参数选择的鲁棒性和通用性。&lt;h4&gt;结论&lt;/h4&gt;通过利用自编码器、随机森林以及信息几何的优势，RF-AE为监督数据可视化提供了一种新的强大工具。它不仅能解决现存的方法在大规模或标签稀疏场景下的局限性问题，还能提高可视化的质量和实用性。&lt;h4&gt;翻译&lt;/h4&gt;数十年的研究产生了稳健的无监督数据可视化方法，而以专家标签指导表示的监督可视化研究仍然较少，因为大多数监督方法优先考虑分类而非可视化。最近，RF-PHATE，一种利用随机森林和信息几何的基于扩散的流形学习方法，在监督可视化方面取得了显著进展。然而，其缺乏显式映射函数限制了可扩展性和应用于未见数据的能力，对大型数据集和标签稀少场景提出了挑战。为了克服这些局限性，我们引入了一种名为Random Forest Autoencoders (RF-AE)的框架，这是一种基于神经网络的方法，结合了自编码器的灵活性、随机森林的学习优势以及RF-PHATE所捕捉到的信息几何特性。RF-AE使高效的未见数据监督可视化成为可能，并在准确性和可解释性方面超越了包括RF-PHATE的标准内核扩展在内的现有方法。此外，RF-AE对超参数的选择具有鲁棒性，并可以推广应用于任何基于内核的降维方法中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decades of research have produced robust methods for unsupervised datavisualization, yet supervised visualization$\unicode{x2013}$where expert labelsguide representations$\unicode{x2013}$remains underexplored, as most supervisedapproaches prioritize classification over visualization. Recently, RF-PHATE, adiffusion-based manifold learning method leveraging random forests andinformation geometry, marked significant progress in supervised visualization.However, its lack of an explicit mapping function limits scalability andprevents application to unseen data, posing challenges for large datasets andlabel-scarce scenarios. To overcome these limitations, we introduce RandomForest Autoencoders (RF-AE), a neural network-based framework for out-of-samplekernel extension that combines the flexibility of autoencoders with thesupervised learning strengths of random forests and the geometry captured byRF-PHATE. RF-AE enables efficient out-of-sample supervised visualization andoutperforms existing methods, including RF-PHATE's standard kernel extension,in both accuracy and interpretability. Additionally, RF-AE is robust to thechoice of hyper-parameters and generalizes to any kernel-based dimensionalityreduction method.</description>
      <author>example@mail.com (Adrien Aumon, Shuang Ni, Myriam Lizotte, Guy Wolf, Kevin R. Moon, Jake S. Rhodes)</author>
      <guid isPermaLink="false">2502.13257v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method</title>
      <link>http://arxiv.org/abs/2502.13725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;时间序列建模在许多现实世界的应用中具有重要意义，并且已经被广泛研究。虽然预训练基础模型在自然语言处理和计算机视觉领域取得了显著进展，但在时间序列领域的开发受到数据稀疏性的限制。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，大型语言模型（LLMs）具备识别复杂令牌序列模式的稳健能力和推理能力。&lt;h4&gt;目的&lt;/h4&gt;为了实现时间序列和自然语言模态之间的高质量平衡，并保持推断效率，本文提出了Time-LlaMA框架。&lt;h4&gt;方法&lt;/h4&gt;首先，通过线性标记机制将时间序列输入转换为令牌嵌入。其次，对齐时间序列令牌嵌入与文本提示。第三，开发了一种动态低秩适应技术（D-LoRA），该技术在Transformer骨干网络的每一层中根据每个时间序列输入动态选择最合适的LoRA模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Time-LlaMA框架提出的改进方法在一系列具有挑战性的现实世界时间序列任务上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过上述方法和创新技术的应用，该研究展示了预训练语言模型在时间序列建模中的强大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于时间序列建模的重要性和现状，并提出了一个新的框架Time-LlaMA来解决当前文献中存在的问题。该框架包括将时间序列输入转换为令牌嵌入、对齐文本提示和动态低秩适应技术等方法，从而实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series modeling holds significant importance in many real-worldapplications and has been extensively studied. While pre-trained foundationmodels have made impressive strides in the fields of natural languageprocessing (NLP) and computer vision (CV), their development in time seriesdomains has been constrained by data sparsity. A series of recent studies havedemonstrated that large language models (LLMs) possess robust patternrecognition and reasoning abilities over complex sequences of tokens. However,the current literature have yet striked a high-quality balance between (a)effectively aligning the time series and natural language modalities, and (b)keeping the inference efficiency. To address the above issues, we now proposethe Time-LlaMA framework. Time-LlaMA first converts the time series input intotoken embeddings through a linear tokenization mechanism. Second, the timeseries token embeddings are aligned with the text prompts. Third, to furtheradapt the LLM backbone for time series modeling, we have developed a dynamiclow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the mostsuitable LoRA modules at each layer of the Transformer backbone for each timeseries input, enhancing the model's predictive capabilities. Our experimentalresults on an extensive collection of challenging real-world time series tasksconfirm that our proposed method achieves the state-of-the-art (SOTA)performance.</description>
      <author>example@mail.com (Juyuan Zhang, Wei Zhu, Jiechao Gao)</author>
      <guid isPermaLink="false">2502.13725v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization</title>
      <link>http://arxiv.org/abs/2502.13180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新型推荐系统框架，该框架借鉴了自动驾驶汽车的分类方法，并将其应用于推荐系统的自主性水平上。这个框架旨在提升用户个性化体验的同时减少负面效应如回音室效应。&lt;h4&gt;背景&lt;/h4&gt;传统的推荐系统研究主要关注提高推荐准确率，这往往会带来诸如形成回音室等意外后果，限制用户的互动方式。&lt;h4&gt;目的&lt;/h4&gt;通过引入一个动态的多目标优化方法来响应不同用户的多样化需求（例如准确性、多样性和平等性），促进更加智能和伦理化的个性化推荐体验。&lt;h4&gt;方法&lt;/h4&gt;开发了一个基于贝叶斯优化框架的方法来处理不确定性并捕捉用户在多个目标之间的个人偏好，同时利用正交元学习范式提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了一种有效的方式以根据个体用户的不确定多目标需求进行最优化。&lt;h4&gt;结论&lt;/h4&gt;研究表明新的推荐系统方法能够适应性和集中于用户体验的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender systems (RSs) play a crucial role in shaping our digitalinteractions, influencing how we access and engage with information acrossvarious domains. Traditional research has predominantly centered on maximizingrecommendation accuracy, often leading to unintended side effects such as echochambers and constrained user experiences. Drawing inspiration from autonomousdriving, we introduce a novel framework that categorizes RS autonomy into fivedistinct levels, ranging from basic rule-based accuracy-driven systems tobehavior-aware, uncertain multi-objective RSs - where users may have varyingneeds, such as accuracy, diversity, and fairness. In response, we propose anapproach that dynamically identifies and optimizes multiple objectives based onindividual user preferences, fostering more ethical and intelligentuser-centric recommendations. To navigate the uncertainty inherent inmulti-objective RSs, we develop a Bayesian optimization (BO) framework thatcaptures personalized trade-offs between different objectives while accountingfor their uncertain interdependencies. Furthermore, we introduce an orthogonalmeta-learning paradigm to enhance BO efficiency and effectiveness by leveragingshared knowledge across similar tasks and mitigating conflicts among objectivesthrough the discovery of orthogonal information. Finally, extensive empiricalevaluations demonstrate the effectiveness of our method in optimizing uncertainmulti-objectives for individual users, paving the way for more adaptive anduser-focused RSs.</description>
      <author>example@mail.com (Hongxu Wang, Zhu Sun, Yingpeng Du, Lu Zhang, Tiantian He, Yew-Soon Ong)</author>
      <guid isPermaLink="false">2502.13180v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>AS-GCL: Asymmetric Spectral Augmentation on Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TMM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;图对比学习（GCL）已经成为图结构数据自监督学习的主要方法。然而，现有的GCL方法通常依赖于一致的随机增强，这忽视了它们对谱域内固有结构的影响，从而限制了模型的有效泛化能力。&lt;h4&gt;背景&lt;/h4&gt;图对比学习是一种用于图结构数据的自监督学习方法，通过从各种增强视图中学习鲁棒表示来减少对标记数据的依赖。然而，现有的GCL方法忽视了它们对谱域内固有结构的影响。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，提出了一种新的范式——AS-GCL，它将不对称光谱增强引入到图对比学习中。&lt;h4&gt;方法&lt;/h4&gt;我们的方法在数据增强、视图编码和对比损失三个关键组成部分上都进行了显著改进。具体来说，在数据增强方面，我们采用了基于谱的增强来最小化谱变异性，并减少噪声；在编码过程中，使用具有不同扩散操作符的参数共享编码器生成多样且抗噪的图视图；对于对比损失，引入了上限损失函数以保持类内和类间距离分布的平衡。&lt;h4&gt;主要发现&lt;/h4&gt;我们是首次利用不对称编码器对谱域中的增强视图进行编码的方法。大量的实验在八种基准数据集上证明了所提出方法的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的AS-GCL范式通过引入不对称光谱增强和改进的对比损失函数，提高了模型的泛化能力，并在多个节点级任务上的表现优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) has emerged as the foremost approach forself-supervised learning on graph-structured data. GCL reduces reliance onlabeled data by learning robust representations from various augmented views.However, existing GCL methods typically depend on consistent stochasticaugmentations, which overlook their impact on the intrinsic structure of thespectral domain, thereby limiting the model's ability to generalizeeffectively. To address these limitations, we propose a novel paradigm calledAS-GCL that incorporates asymmetric spectral augmentation for graph contrastivelearning. A typical GCL framework consists of three key components: graph dataaugmentation, view encoding, and contrastive loss. Our method introducessignificant enhancements to each of these components. Specifically, for dataaugmentation, we apply spectral-based augmentation to minimize spectralvariations, strengthen structural invariance, and reduce noise. With respect toencoding, we employ parameter-sharing encoders with distinct diffusionoperators to generate diverse, noise-resistant graph views. For contrastiveloss, we introduce an upper-bound loss function that promotes generalization bymaintaining a balanced distribution of intra- and inter-class distance. To ourknowledge, we are the first to encode augmentation views of the spectral domainusing asymmetric encoders. Extensive experiments on eight benchmark datasetsacross various node-level tasks demonstrate the advantages of the proposedmethod.</description>
      <author>example@mail.com (Ruyue Liu, Rong Yin, Yong Liu, Xiaoshuai Hao, Haichao Shi, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2502.13525v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views</title>
      <link>http://arxiv.org/abs/2502.13277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于超图视角的多模态对比学习框架HyperGCL，该框架通过自适应拓扑增强技术来改进图表示。&lt;h4&gt;背景&lt;/h4&gt;现有Graph Contrastive Learning（GCL）方法依赖于预定义的数据增强方式，在任务相关的信息保留和对不同输入数据的适应性方面存在不足。同时，负样本的选择问题也未得到充分研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于超图视角的多模态GCL框架HyperGCL，以解决现有模型在图表示改进中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HyperGCL通过联合利用输入图结构和属性信息构造三个不同的超图视图，并使用可学习的自适应拓扑增强技术来优化这些视图。引入特定于视图的编码器来捕捉每个视图的关键特征，同时采用网络感知对比损失定义正负样本。&lt;h4&gt;主要发现&lt;/h4&gt;HyperGCL在基准数据集上的实验结果表明，其在节点分类任务上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的框架通过创新地融合了超图和多模态学习策略，在改进图表示方面展示了显著的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Graph Contrastive Learning (GCL) have demonstratedremarkable effectiveness in improving graph representations. However, relyingon predefined augmentations (e.g., node dropping, edge perturbation, attributemasking) may result in the loss of task-relevant information and a lack ofadaptability to diverse input data. Furthermore, the selection of negativesamples remains rarely explored. In this paper, we introduce HyperGCL, a novelmultimodal GCL framework from a hypergraph perspective. HyperGCL constructsthree distinct hypergraph views by jointly utilizing the input graph'sstructure and attributes, enabling a comprehensive integration of multiplemodalities in contrastive learning. A learnable adaptive topology augmentationtechnique enhances these views by preserving important relations and filteringout noise. View-specific encoders capture essential characteristics from eachview, while a network-aware contrastive loss leverages the underlying topologyto define positive and negative samples effectively. Extensive experiments onbenchmark datasets demonstrate that HyperGCL achieves state-of-the-art nodeclassification performance.</description>
      <author>example@mail.com (Khaled Mohammed Saifuddin, Jonathan Shihao Ji, Esra Akbas)</author>
      <guid isPermaLink="false">2502.13277v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>The impact of conformer quality on learned representations of molecular conformer ensembles</title>
      <link>http://arxiv.org/abs/2502.13220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了机器学习模型预测分子构象集合性质时输入的3D构象质量对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;训练机器学习模型来预测分子构象集合的性质已成为加速药物类似小分子、反应性有机底物和均相催化剂构象分析的一种流行策略。特别是对于高通量分析，经过训练的代理模型可以帮助规避依赖昂贵构象搜索和几何优化的传统方法。&lt;h4&gt;目的&lt;/h4&gt;研究较低质量的3D构象如何影响对高质量构象性质预测的效果；探讨随机构象的几何优化精度在编码中的重要性；探究包含引发目标属性活性构象的集合对于模型准确性的影响；比较代理模型预测与直接从便宜的构象集合估计性质的方法。&lt;h4&gt;方法&lt;/h4&gt;在基于密度泛函理论优化的构象集合中，研究了Sterimol参数的预测情况。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明3D表示学习模型的表现取决于输入数据的质量。较低质量的3D构象可能无法准确预测高质量构象的特性；随机构象的几何精度对于编码的效果至关重要；当活性构象存在时，能够提高模型的准确性；直接从便宜集合估计性质的方法有时可以与代理模型预测的结果相媲美。&lt;h4&gt;结论&lt;/h4&gt;虽然具体问题的答案会因情况而异，但研究提供了关于3D表示学习模型和构象质量重要性的宝贵见解，并提出了实际考虑因素。&lt;h4&gt;翻译&lt;/h4&gt;训练机器学习模型以预测分子构象集合的性质已成为加速药物类似小分子、反应性有机底物和均相催化剂构象分析的一种流行策略。尤其是高通量分析，经过训练的代理模型可以帮助规避依赖昂贵构象搜索和几何优化的传统方法。在这里，我们质疑所用3D构象质量对预测单一活性构象依赖性质的性能的影响。较低质量的构象能否准确反映高质量构象的性质？随机构象编码时，几何优化精度是否重要？对于将一系列构象作为输入的模型来说，目标属性引发活性构象的存在如何影响准确性？代理模型的预测与从便宜集合中估计性质的方法相比如何？我们在此背景下探讨了基于密度泛函理论优化的构象集合Sterimol参数的预测情况。虽然答案会因具体情况而异，但我们的分析提供了关于3D表示学习模型的重要视角，并提出了实际考虑因素，即何时构象质量很重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training machine learning models to predict properties of molecular conformerensembles is an increasingly popular strategy to accelerate the conformationalanalysis of drug-like small molecules, reactive organic substrates, andhomogeneous catalysts. For high-throughput analyses especially, trainedsurrogate models can help circumvent traditional approaches to conformationalanalysis that rely on expensive conformer searches and geometry optimizations.Here, we question how the performance of surrogate models for predicting 3Dconformer-dependent properties (of a single, active conformer) is affected bythe quality of the 3D conformers used as their input. How well do lower-qualityconformers inform the prediction of properties of higher-quality conformers?Does the fidelity of geometry optimization matter when encoding randomconformers? For models that encode sets of conformers, how does the presence ofthe active conformer that induces the target property affect model accuracy?How do predictions from a surrogate model compare to estimating the propertiesfrom cheap ensembles themselves? We explore these questions in the context ofpredicting Sterimol parameters of conformer ensembles optimized with densityfunctional theory. Although answers will be case-specific, our analyses providea valuable perspective on 3D representation learning models and raise practicalconsiderations regarding when conformer quality matters.</description>
      <author>example@mail.com (Keir Adams, Connor W. Coley)</author>
      <guid isPermaLink="false">2502.13220v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges</title>
      <link>http://arxiv.org/abs/2502.13476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  FEMA  [https://www.fema.gov/openfema-data-page/disaster-declarations-summaries-v2]  National Oceanic and Atmospheric Administration  [https://www.ncdc.noaa.gov/stormevents/details.jsp] packages Pytorch  [https://pytorch.org/] RLib [https://docs.ray.io/en/latest/rllib/index.html]  Neo4j [https://neo4j.com/] Apache Kafka [https://kafka.apache.org/]&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种新型的代理人工智能（AAI）框架，用于关键任务的安全应用。该框架具有多层架构，并通过详细实现的AAI层将网络基础设施与关键任务应用程序连接起来。&lt;h4&gt;背景&lt;/h4&gt;随着AI技术的进步，特别是基础模型的发展，AI已经成为许多依赖自动化服务交付的应用程序的重要组成部分之一，其中包括对公共安全至关重要的使命级应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于代理的人工智能框架用于关键任务应用，以解决传统AI系统中人机交互的问题和在维持情境感知的同时缺乏适应动态条件的能力问题。&lt;h4&gt;方法&lt;/h4&gt;本研究设计了一个具有多层架构的新型AAI框架，并实现了一种新的AAI层，该层连接网络基础设施与关键任务应用程序，使其能够快速分析文本数据并迅速适应环境变化。&lt;h4&gt;主要发现&lt;/h4&gt;初步分析表明，所提出的AAI框架可以将初始响应时间平均减少5.6分钟，警报生成时间平均缩短15.6秒，并且资源分配提高了最多13.4%。此外，它还可以使并发操作数增加40个，从而最多可减少恢复时间至多5.2分钟。&lt;h4&gt;结论&lt;/h4&gt;本文展示了AAI框架在关键任务应用中的有效性及优势，但同时也指出了实施过程中需考虑的一些问题和挑战。&lt;h4&gt;翻译&lt;/h4&gt;我们正处于一个变革的时代，人工智能（AI）的进步，尤其是基础模型的发展，一直在新闻中占据重要位置。AI已经成为许多依赖自动化服务交付的应用程序的重要组成部分之一，其中包括对公共安全至关重要的使命级应用。关键任务型AI应用程序的问题在于人机交互系统及在保持情境感知的同时缺乏适应动态条件的能力。由于代理人工智能（AAI）能够通过上下文分析文本数据并快速适应环境变化，因此最近它得到了广泛关注。在此背景下，本文提出了一种用于关键任务应用的AAI框架。我们提出了一个具有多层架构的新框架来实现AAI，并展示了一个详细的AAI实施，以弥合网络基础设施与关键任务应用程序之间的差距。我们的初步分析表明，AAI平均减少了5.6分钟的初始响应时间，警报生成时间平均缩短了15.6秒，资源分配最多提高了13.4%。我们还展示了AAI方法使并发操作数增加40个，从而最多可减少恢复时间至多5.2分钟。最后，我们强调了一些在实现AAI框架时需要考虑的问题和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We are in a transformative era, and advances in Artificial Intelligence (AI),especially the foundational models, are constantly in the news. AI has been anintegral part of many applications that rely on automation for servicedelivery, and one of them is mission-critical public safety applications. Theproblem with AI-oriented mission-critical applications is the humanin-the-loopsystem and the lack of adaptability to dynamic conditions while maintainingsituational awareness. Agentic AI (AAI) has gained a lot of attention recentlydue to its ability to analyze textual data through a contextual lens whilequickly adapting to conditions. In this context, this paper proposes an AAIframework for mission-critical applications. We propose a novel framework witha multi-layer architecture to realize the AAI. We also present a detailedimplementation of AAI layer that bridges the gap between network infrastructureand missioncritical applications. Our preliminary analysis shows that the AAIreduces initial response time by 5.6 minutes on average, while alert generationtime is reduced by 15.6 seconds on average and resource allocation is improvedby up to 13.4%. We also show that the AAI methods improve the number ofconcurrent operations by 40, which reduces the recovery time by up to 5.2minutes. Finally, we highlight some of the issues and challenges that need tobe considered when implementing AAI frameworks.</description>
      <author>example@mail.com (Sunder Ali Khowaja, Kapal Dev, Muhammad Salman Pathan, Engin Zeydan, Merouane Debbah)</author>
      <guid isPermaLink="false">2502.13476v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization</title>
      <link>http://arxiv.org/abs/2502.12534v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: see https://theialab.github.io/noksr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种新的大规模点云曲面重建方法，通过开发了一个高效的框架将不规则的点云转换为带符号的距离场（Signed Distance Field, SDF）。&lt;h4&gt;背景&lt;/h4&gt;当前基于Transformer架构的方法能够处理点云数据，但需要有效的序列化以保持局部性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的高效框架用于大规模点云曲面重建，并通过序列化方法将点云转换为SDF。&lt;h4&gt;方法&lt;/h4&gt;采用最近的基于Transformer的架构（如PointTransformerV3），该架构能够按保序的方式序列化点云。在预测某一点处的SDF值时，可以利用附近token的有效聚集来快速近似找到邻居。同时，在不同的层级/尺度下序列化点云，并通过非线性地聚合特征来预测SDF值。&lt;h4&gt;主要发现&lt;/h4&gt;跨多个尺度进行聚合对于克服序列化引入的近似误差（即局部邻域中的假阴性）至关重要，这有助于提高重建精度和效率。该框架在准确性和效率上均优于现有方法，在户外数据集上尤其表现出色，尤其是在稀疏网格方法表现不佳的情况下。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过结合序列化点云和多尺度特征聚合的方法，实现了前所未有的性能优势，并且具有更简单的实现方式。&lt;h4&gt;翻译&lt;/h4&gt;We present a novel approach to large-scale point cloud surface reconstruction by developing an efficient framework that converts an irregular point cloud into a signed distance field (SDF). Our backbone builds upon recent transformer-based architectures (i.e., PointTransformerV3), that serializes the point cloud into a locality-preserving sequence of tokens. We efficiently predict the SDF value at a point by aggregating nearby tokens, where fast approximate neighbors can be retrieved thanks to the serialization. We serialize the point cloud at different levels/scales, and non-linearly aggregate a feature to predict the SDF value. We show that aggregating across multiple scales is critical to overcome the approximations introduced by the serialization (i.e., false negatives in the neighborhood). Our framework sets the new state-of-the-art in terms of accuracy and efficiency (better or similar performance with half the latency of the best prior method, coupled with a simpler implementation), particularly on outdoor datasets where sparse-grid methods have shown limited performance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel approach to large-scale point cloud surface reconstructionby developing an efficient framework that converts an irregular point cloudinto a signed distance field (SDF). Our backbone builds upon recenttransformer-based architectures (i.e., PointTransformerV3), that serializes thepoint cloud into a locality-preserving sequence of tokens. We efficientlypredict the SDF value at a point by aggregating nearby tokens, where fastapproximate neighbors can be retrieved thanks to the serialization. Weserialize the point cloud at different levels/scales, and non-linearlyaggregate a feature to predict the SDF value. We show that aggregating acrossmultiple scales is critical to overcome the approximations introduced by theserialization (i.e. false negatives in the neighborhood). Our frameworks setsthe new state-of-the-art in terms of accuracy and efficiency (better or similarperformance with half the latency of the best prior method, coupled with asimpler implementation), particularly on outdoor datasets where sparse-gridmethods have shown limited performance.</description>
      <author>example@mail.com (Zhen Li, Weiwei Sun, Shrisudhan Govindarajan, Shaobo Xia, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi)</author>
      <guid isPermaLink="false">2502.12534v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Are Large Language Models In-Context Graph Learners?</title>
      <link>http://arxiv.org/abs/2502.13562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '大型语言模型（LLMs）在处理各种任务中表现出了出色的上下文推理能力，尤其是在面对非结构化数据时。然而，在处理图形等结构化数据方面存在不足。', '背景': 'LLMs缺乏对非欧几里得结构的理解，导致它们无法有效处理图学习任务中的结构化数据，并且其性能远低于专门的图神经网络（GNNs）。', '目的': '提出一种基于检索增强生成（RAG）过程的概念框架来改进LLMs在图形学习任务中的上下文学习能力。', '方法': '将图形数据的学习过程视为一个查询和从图中检索出背景信息的过程，进而开发了一系列增强LLM性能的RAG框架。', '主要发现': '实验表明这些提出的RAG框架显著提升了LLM在基于图形的任务上的表现，特别是在需要使用预训练的LLM而无需修改或通过API访问的情况下。', '结论': '该方法为改进大型语言模型处理结构化数据的能力提供了一种有效途径。', '翻译': '大型语言模型（LLMs）在各种任务中展示了显著的上下文推理能力，尤其是对于非结构化的输入如语言和图像等。然而，在无额外微调的情况下，它们难以处理图形这样的结构化数据，并且其性能远低于专门用于图学习任务的图神经网络（GNNs）。本文展示了一种通过将基于图的数据的学习看作是检索增强生成（RAG）过程来提升LLM在图学习上的上下文学习能力的方法。一系列实验表明，这些提出的RAG框架能够显著改善LLMs处理图形相关任务的表现，尤其适用于无需修改的预训练LLM或API访问场景中。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated remarkable in-contextreasoning capabilities across a wide range of tasks, particularly withunstructured inputs such as language or images. However, LLMs struggle tohandle structured data, such as graphs, due to their lack of understanding ofnon-Euclidean structures. As a result, without additional fine-tuning, theirperformance significantly lags behind that of graph neural networks (GNNs) ingraph learning tasks. In this paper, we show that learning on graph data can beconceptualized as a retrieval-augmented generation (RAG) process, wherespecific instances (e.g., nodes or edges) act as queries, and the graph itselfserves as the retrieved context. Building on this insight, we propose a seriesof RAG frameworks to enhance the in-context learning capabilities of LLMs forgraph learning tasks. Comprehensive evaluations demonstrate that our proposedRAG frameworks significantly improve LLM performance on graph-based tasks,particularly in scenarios where a pretrained LLM must be used withoutmodification or accessed via an API.</description>
      <author>example@mail.com (Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Liang Chen, Zibin Zheng)</author>
      <guid isPermaLink="false">2502.13562v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.13471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the draft before submitting to any journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了特征交互在图神经网络（GNN）模型中的重要性，通过实验揭示了有效的特征交互建模需要保留相互作用的特征之间的边缘，并避免非交互边缘带来的噪音。&lt;h4&gt;背景&lt;/h4&gt;特征交互对于预测机器学习模型至关重要，因为它捕捉到了影响模型性能的特征之间关系。此工作关注成对交互并研究其在构建用于GNN的特征图中的重要性。&lt;h4&gt;目的&lt;/h4&gt;利用现有的GNN模型和工具来探索特征图结构与其建模交互有效性的关系，并提供稀疏特征选择的理论支持。&lt;h4&gt;方法&lt;/h4&gt;通过对合成数据集进行实验，研究了边缘对GNN在建模特征交互上的影响。使用MDL原理来证明保持必要相互作用边界的特征图比完全图更高效且可解释性更强。&lt;h4&gt;主要发现&lt;/h4&gt;1. 互作用特征之间的边对于有效建模特征交互是重要的。2. 包含非互动边缘会引入噪音，降低模型性能。3. 使用MDL原理可以有效地选择稀疏的特征图。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了有关设计能够提高GNN模型性能和可解释性的特征图的理论见解和实用指南。&lt;h4&gt;翻译&lt;/h4&gt;特性交互在预测机器学习模型中至关重要，因为它捕获了影响模型性能的特性之间的关系。在这项工作中，我们专注于成对互动，并探讨其在为图神经网络（GNN）构建特征图中的重要性。我们并没有提出新的方法，而是利用现有的GNN模型和工具来探索特征图结构及其建模交互的有效性的关系。通过合成数据集上的实验，我们发现相互作用的特性之间的边缘对于使GNN能够有效建模特性互动是重要的。我们也观察到包括非互动边可以作为噪音降低模型性能。此外，我们使用最小描述长度（MDL）原则提供了稀疏特征图选择的理论依据。我们证明了只保留必要交互边界的特征图比完整图更高效且可解释性更强，这与奥卡姆剃刀原理一致。我们的发现为设计改进GNN模型性能和可解释性的特征图提供理论见解和实用指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature interaction is crucial in predictive machine learning models, as itcaptures the relationships between features that influence model performance.In this work, we focus on pairwise interactions and investigate theirimportance in constructing feature graphs for Graph Neural Networks (GNNs).Rather than proposing new methods, we leverage existing GNN models and tools toexplore the relationship between feature graph structures and theireffectiveness in modeling interactions. Through experiments on synthesizeddatasets, we uncover that edges between interacting features are important forenabling GNNs to model feature interactions effectively. We also observe thatincluding non-interaction edges can act as noise, degrading model performance.Furthermore, we provide theoretical support for sparse feature graph selectionusing the Minimum Description Length (MDL) principle. We prove that featuregraphs retaining only necessary interaction edges yield a more efficient andinterpretable representation than complete graphs, aligning with Occam's Razor.  Our findings offer both theoretical insights and practical guidelines fordesigning feature graphs that improve the performance and interpretability ofGNN models.</description>
      <author>example@mail.com (Phaphontee Yamchote, Saw Nay Htet Win, Chainarong Amornbunchornvej, Thanapon Noraset)</author>
      <guid isPermaLink="false">2502.13471v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>How Expressive are Knowledge Graph Foundation Models?</title>
      <link>http://arxiv.org/abs/2502.13339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文对知识图谱基础模型（KGFMs）的表达能力进行了严格的理论研究，揭示了其在处理不同关系词汇的新知识图谱时具有的泛化能力，并发现最典型的学习动机是二元性的，这限制了模型的表现力。设计并验证了使用更丰富的学习动机构建的KGFMs可以提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;知识图谱基础模型（KGFMs）在深度学习领域的知识图谱处理中具有前沿地位，能够对全新的、关系词汇不同的知识图谱进行泛化处理。然而尽管其实验效果良好，对其理论理解仍十分有限。&lt;h4&gt;目的&lt;/h4&gt;探讨并明确KGFMs的表达能力及其依赖的学习动机的关系，提出使用更丰富动机设计更具表现力的KGFMs，并通过实证研究验证这些理论发现。&lt;h4&gt;方法&lt;/h4&gt;首先分析现有文献中典型的学习动机（二元性），然后设计基于三元关系交互来学习关系表示的更加丰富的学习动机，并构建相应的KGFMs。进行广泛的实验，以测试和验证不同的模型在不同领域的数据集上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;KGFMs的表现力直接依赖于用于学习关系表示的学习动机。二元性动机限制了表现力；基于三元关系交互的更丰富的动机可以提高模型的表现能力。&lt;h4&gt;结论&lt;/h4&gt;使用更丰富和复杂动机设计的知识图谱基础模型能够显著提升在各种领域数据集上的性能，这为KGFMs的设计提供了新的理论指导。&lt;h4&gt;翻译&lt;/h4&gt;Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs, as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs used for learning relation representations. We then observe that the most typical motifs used in existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits model expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, necessitating learning relation representations based on how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graph Foundation Models (KGFMs) are at the frontier for deeplearning on knowledge graphs (KGs), as they can generalize to completely novelknowledge graphs with different relational vocabularies. Despite theirempirical success, our theoretical understanding of KGFMs remains very limited.In this paper, we conduct a rigorous study of the expressive power of KGFMs.Specifically, we show that the expressive power of KGFMs directly depends onthe motifs that are used to learn the relation representations. We then observethat the most typical motifs used in the existing literature are binary, as therepresentations are learned based on how pairs of relations interact, whichlimits the model's expressiveness. As part of our study, we design moreexpressive KGFMs using richer motifs, which necessitate learning relationrepresentations based on, e.g., how triples of relations interact with eachother. Finally, we empirically validate our theoretical findings, showing thatthe use of richer motifs results in better performance on a wide range ofdatasets drawn from different domains.</description>
      <author>example@mail.com (Xingyue Huang, Pablo Barceló, Michael M. Bronstein, İsmail İlkan Ceylan, Mikhail Galkin, Juan L Reutter, Miguel Romero Orth)</author>
      <guid isPermaLink="false">2502.13339v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fusing Point Cloud and Visual Representations for Imitation Learning</title>
      <link>http://arxiv.org/abs/2502.12320v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的模仿学习方法FPV-Net，该方法能够有效地结合点云和RGB图像的优点，在复杂的RoboCasa基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;手动操作的学习需要使用可以访问丰富的感官信息（如点云或RGB图象）的策略。点云在捕获几何结构方面非常有效，而RGB图像则提供了重要的纹理和语义信息。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法将2D图像特征映射到点云时丢失全局上下文信息的问题，提出了一种新的模仿学习方法FPV-Net。&lt;h4&gt;方法&lt;/h4&gt;该方法通过使用自适应层范数条件化使点云编码器依赖于全局和局部图象令牌，从而结合了两种模态的优点。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在复杂基准测试中，仅依靠单一模式存在局限性，并且FPV-Net在所有任务上都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法通过有效融合点云和RGB图像的优势解决了现有技术的不足，展示了其广泛的适用性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;模仿学习需要使用具有丰富感觉信息（如点云或RGB图象）访问策略。点云有效地捕获几何结构，在模拟学习中的操作任务中至关重要。相比之下，RGB图象提供重要的纹理和语义信息，对于某些任务来说是必不可少的。现有融合两种模态的方法将2D图像特征分配给点云，但往往会丢失原始图像的整体上下文信息。在本文工作中，我们提出了FPV-Net，这是一种新的模仿学习方法，有效结合了点云和RGB模式的优点。我们的方法通过使用自适应层范数条件化使点云编码器依赖于全局和局部图象令牌来利用两种模式的有利特性。在具有挑战性的RoboCasa基准测试中进行广泛的实验表明，在复杂任务中仅依靠一种模态存在局限性，并且我们证明了该方法在整个任务上都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning for manipulation requires using policies that have access to richsensory information such as point clouds or RGB images. Point cloudsefficiently capture geometric structures, making them essential formanipulation tasks in imitation learning. In contrast, RGB images provide richtexture and semantic information that can be crucial for certain tasks.Existing approaches for fusing both modalities assign 2D image features topoint clouds. However, such approaches often lose global contextual informationfrom the original images. In this work, we propose FPV-Net, a novel imitationlearning method that effectively combines the strengths of both point cloud andRGB modalities. Our method conditions the point-cloud encoder on global andlocal image tokens using adaptive layer norm conditioning, leveraging thebeneficial properties of both modalities. Through extensive experiments on thechallenging RoboCasa benchmark, we demonstrate the limitations of relying oneither modality alone and show that our method achieves state-of-the-artperformance across all tasks.</description>
      <author>example@mail.com (Atalay Donat, Xiaogang Jia, Xi Huang, Aleksandar Taranovic, Denis Blessing, Ge Li, Hongyi Zhou, Hanyi Zhang, Rudolf Lioutikov, Gerhard Neumann)</author>
      <guid isPermaLink="false">2502.12320v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction</title>
      <link>http://arxiv.org/abs/2502.13344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;K-Paths 是一种从大规模生物医学知识图谱中提取结构化、多样性和生物学意义路径的检索框架，该框架可促进大语言模型和图神经网络有效预测未观察到的药物间及药物与疾病间的相互作用。&lt;h4&gt;背景&lt;/h4&gt;药物发现是一个复杂且耗时的过程，需要识别和验证新的治疗候选者。利用大规模生物医学知识图谱（KGs）的计算方法为加速这一过程提供了可能的解决方案，但从中提取有意义的信息仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以与大语言模型和其他模型兼容的方法来从大规模 KGs 中提取结构化路径，并提高药物再定位和相互作用严重性预测的零样本性能。&lt;h4&gt;方法&lt;/h4&gt;K-Paths 框架利用 Yen 算法的一种多样性感知变体，检索查询中实体之间的 K 条最短无环路径，优先考虑生物学相关的多样关系。将这些路径转换为结构化格式以供大语言模型直接处理。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验表明，K-Paths 能够显著提高 Llama 8.1B 和 Llama 70B 的 F1 分数，在药物再定位和相互作用严重性预测中分别提高了12.45分和6.18分。此外，它还能有效降低知识图谱的规模并维持强大的预测性能。&lt;h4&gt;结论&lt;/h4&gt;K-Paths 提供了一种连接 KGs 和 LLM 的新方法，为药物发现提供了可解释的基础，并且在提高零样本性能的同时也提高了监督训练效率。&lt;h4&gt;翻译&lt;/h4&gt;药物发现是一个复杂和耗时的过程，需要识别和验证新的治疗候选者。计算方法利用大规模生物医学知识图谱（KGs）作为解决方案来加速这一过程显示出巨大潜力，但挑战在于如何从这些复杂的图结构中提取有意义的信息。现有的基于子图的方法主要针对图神经网络设计，导致它们与大语言模型等其他类型模型不兼容。为此，我们提出了一种名为 K-Paths 的检索框架，它可以从 KGs 中抽取结构化、多样化且生物上具有意义的路径，并让这些路径便于大语言模型和 GNN 用于预测未观察到的药物相互作用及药物与疾病之间的关系。与其他传统的基于路径排名的方法不同，K-Paths 检索并转换路径以一种大语言模型可以直接处理的形式呈现出来，从而实现可解释性推理。该方法利用了一种改进的 Yen 算法来优先考虑生物学相关的多样化的关联，并检索查询实体间 K 条最短无环路径。实验结果表明，在药物再定位和相互作用严重程度预测任务上，K-Paths 显著提升了零样本性能。此外，对于监督学习任务，它还能显著提高训练效率并保持强大的预测效果。这些特性表明 K-Paths 是一种高效的数据驱动药物发现工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug discovery is a complex and time-intensive process that requiresidentifying and validating new therapeutic candidates. Computational approachesusing large-scale biomedical knowledge graphs (KGs) offer a promising solutionto accelerate this process. However, extracting meaningful insights fromlarge-scale KGs remains challenging due to the complexity of graph traversal.Existing subgraph-based methods are tailored to graph neural networks (GNNs),making them incompatible with other models, such as large language models(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,diverse, and biologically meaningful paths from KGs. Integrating these pathsenables LLMs and GNNs to effectively predict unobserved drug-drug anddrug-disease interactions. Unlike traditional path-ranking approaches, K-Pathsretrieves and transforms paths into a structured format that LLMs can directlyprocess, facilitating explainable reasoning. K-Paths employs a diversity-awareadaptation of Yen's algorithm to retrieve the K shortest loopless paths betweenentities in an interaction query, prioritizing biologically relevant anddiverse relationships. Our experiments on benchmark datasets show that K-Pathsimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points ondrug repurposing and 13.42 points on interaction severity prediction. We alsoshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,respectively. K-Paths also improves the supervised training efficiency ofEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintainingstrong predictive performance. Beyond its scalability and efficiency, K-Pathsuniquely bridges the gap between KGs and LLMs, providing explainable rationalesfor predicted interactions. These capabilities show that K-Paths is a valuabletool for efficient data-driven drug discovery.</description>
      <author>example@mail.com (Tassallah Abdullahi, Ioanna Gemou, Nihal V. Nayak, Ghulam Murtaza, Stephen H. Bach, Carsten Eickhoff, Ritambhara Singh)</author>
      <guid isPermaLink="false">2502.13344v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations</title>
      <link>http://arxiv.org/abs/2502.13221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在基础模型能力日益增强的时代，求职者开始利用生成式AI工具来提升他们的申请材料。然而，这种不平等的访问权限和知识水平可能导致招聘决策的准确性降低，并且给一些候选人带来不公平的优势。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型功能的提高，越来越多的人使用这些技术来改进简历等求职文件。但是，不同人之间获取生成式AI工具的能力存在差异，这可能会影响招聘过程中的公平性和准确度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的策略分类框架变体，以解决由于利用大规模语言模型而产生的操纵问题，并为此类操作制定相应规则。&lt;h4&gt;方法&lt;/h4&gt;引入了一个名为“双票制”的方案，在该方案中，招聘算法会对提交的每份简历进行额外的操作处理，并与原始版本一起考虑。此外还扩展了这一方法为更一般的n-票制度。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明表明，这种'双票制'方案能够提升招聘决策的准确性和公平性。并且，当正向预测率最大化并限制错误阳性发生时，最终的招聘结果将趋于稳定且不依赖于群体差异。&lt;h4&gt;结论&lt;/h4&gt;通过使用开源简历筛选工具对真实简历进行实验验证了该框架及其性能的有效性，这表明这种双票制方案能够在一定程度上缓解由于大型语言模型访问权的不同而引起的不公平现象。&lt;h4&gt;翻译&lt;/h4&gt;在技术不断进步的时代背景下，论文提出了一种新的策略分类方法来改善招聘过程中的公平性和准确性问题。通过引入一种名为‘双票制’的机制，并理论上证明了其有效性，最终通过实验验证表明这种方法能够解决由于大型语言模型使用差异带来的不公平现象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In an era of increasingly capable foundation models, job seekers are turningto generative AI tools to enhance their application materials. However, unequalaccess to and knowledge about generative AI tools can harm both employers andcandidates by reducing the accuracy of hiring decisions and giving somecandidates an unfair advantage. To address these challenges, we introduce a newvariant of the strategic classification framework tailored to manipulationsperformed using large language models, accommodating varying levels ofmanipulations and stochastic outcomes. We propose a ``two-ticket'' scheme,where the hiring algorithm applies an additional manipulation to each submittedresume and considers this manipulated version together with the originalsubmitted resume. We establish theoretical guarantees for this scheme, showingimprovements for both the fairness and accuracy of hiring decisions when thetrue positive rate is maximized subject to a no false positives constraint. Wefurther generalize this approach to an $n$-ticket scheme and prove that hiringoutcomes converge to a fixed, group-independent decision, eliminatingdisparities arising from differential LLM access. Finally, we empiricallyvalidate our framework and the performance of our two-ticket scheme on realresumes using an open-source resume screening tool.</description>
      <author>example@mail.com (Lee Cohen, Jack Hsieh, Connie Hong, Judy Hanwen Shen)</author>
      <guid isPermaLink="false">2502.13221v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Myna: Masking-Based Contrastive Learning of Musical Representations</title>
      <link>http://arxiv.org/abs/2502.12511v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Myna 是一种简单的自监督音乐表示学习方法，基于对比学习框架，并引入了两个关键创新：使用 Vision Transformer (ViT) 处理频谱图和新颖的 token masking 数据增强策略。&lt;h4&gt;背景&lt;/h4&gt;音乐领域的自监督学习需要有效的数据表征方式以及高效的训练策略以提高模型性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法 Myna，能够通过对比学习框架提升音乐表示的学习效果，并且在有限资源下实现优秀的模型表现。&lt;h4&gt;方法&lt;/h4&gt;Myna 使用 ViT 作为主要架构并引入 token masking 数据增强技术。使用垂直补丁来捕捉关键特征，同时提高音高敏感性以改善任务性能。Myna-22M-Hybrid 版本同时处理不同大小的频谱图块，并且在单 GPU 上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;(i) Token masking 技术使每张 GPU 的批量大小从之前的 48 或者 120 增加到 4096，提高了效率。(ii) Myna 避免了传统增强方法，保持音高敏感性，从而在诸如关键检测的任务中表现更佳。(iii) 使用垂直补丁能更好地捕捉用于关键检测的特征。&lt;h4&gt;结论&lt;/h4&gt;Myna-22M-Hybrid 版本经过单 GPU 训练，在性能上超过了 MULE (62M)，并且与训练于 16 和 64 张 GPU 的 MERT-95M 相媲美，甚至在使用公开数据时表现更佳。&lt;h4&gt;翻译&lt;/h4&gt;摘要文本的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Myna, a simple yet effective approach for self-supervised musicalrepresentation learning. Built on a contrastive learning framework, Mynaintroduces two key innovations: (1) the use of a Vision Transformer (ViT) onmel-spectrograms as the backbone and (2) a novel data augmentation strategy,token masking, that masks 90 percent of spectrogram tokens. These innovationsdeliver both effectiveness and efficiency: (i) Token masking enables asignificant increase in per-GPU batch size, from 48 or 120 in prior methods(CLMR, MULE) to 4096. (ii) By avoiding traditional augmentations, Myna retainspitch sensitivity, enhancing performance in tasks like key detection. (iii) Theuse of vertical patches allows the model to better capture critical featuresfor key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and128x2 patches, achieving state-of-the-art results. Trained on a single GPU, itoutperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16and 64 GPUs, respectively. Additionally, it surpasses MERT-95M-public,establishing itself as the best-performing model trained on publicly availabledata. We release our code and models to promote reproducibility and facilitatefuture research.</description>
      <author>example@mail.com (Ori Yonay, Tracy Hammond, Tianbao Yang)</author>
      <guid isPermaLink="false">2502.12511v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for Databases: A Survey</title>
      <link>http://arxiv.org/abs/2502.12908v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A survey focus on GNNs and databases. 9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了图神经网络（GNN）在数据库系统中的应用，提出了一个将现有方法分类为两类的新体系结构：关系型数据库和图形数据库。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图数据方面展示了强大的能力，并且在数据库社区中引起了越来越多的关注。然而，在如何通过基于GNN的方法改进数据库系统的全面理解和综述方面还存在空白。&lt;h4&gt;目的&lt;/h4&gt;本文旨在填补这一知识空白，提供一种结构化和深入的关于GNN在数据库系统中的应用的概述。&lt;h4&gt;方法&lt;/h4&gt;提出了一个分类体系结构，将现有方法分为两大类：（1）关系型数据库，包括性能预测、查询优化以及文本到SQL转换等任务；（2）图数据库，解决高效图形查询处理及相似性计算等问题。同时对各类关键方法进行了系统性的回顾。&lt;h4&gt;主要发现&lt;/h4&gt;概述了各种GNN在不同类型的数据库中应用的关键贡献和实际影响，并提出了将GNN进一步集成进数据库系统的潜在路径。&lt;h4&gt;结论&lt;/h4&gt;论文强调了图神经网络在改进数据库性能方面的潜力，同时也指出了一些未来的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful deep learning models forgraph-structured data, demonstrating remarkable success across diverse domains.Recently, the database (DB) community has increasingly recognized thepotentiality of GNNs, prompting a surge of researches focusing on improvingdatabase systems through GNN-based approaches. However, despite notableadvances, There is a lack of a comprehensive review and understanding of howGNNs could improve DB systems. Therefore, this survey aims to bridge this gapby providing a structured and in-depth overview of GNNs for DB systems.Specifically, we propose a new taxonomy that classifies existing methods intotwo key categories: (1) Relational Databases, which includes tasks likeperformance prediction, query optimization, and text-to-SQL, and (2) GraphDatabases, addressing challenges like efficient graph query processing andgraph similarity computation. We systematically review key methods in eachcategory, highlighting their contributions and practical implications. Finally,we suggest promising avenues for integrating GNNs into Database systems.</description>
      <author>example@mail.com (Ziming Li, Youhuan Li, Yuyu Luo, Guoliang Li, Chuxu Zhang)</author>
      <guid isPermaLink="false">2502.12908v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond</title>
      <link>http://arxiv.org/abs/2502.12048v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文综述了基于脑电图（EEG）的多模态生成技术的发展现状，重点关注通过生成对抗网络（GANs）、变分自编码器（VAEs）和扩散模型进行的EEG到图像生成，以及利用Transformer语言模型和对比学习方法实现的EEG到文本生成。&lt;h4&gt;背景&lt;/h4&gt;脑机接口（BCI）与生成人工智能（GenAI）的结合开启了大脑信号解码的新领域，实现了辅助通信、神经表示学习及多模态集成。特别是基于脑电图（EEG）的非侵入性方法，在将神经活动转化为有意义的输出方面发挥了重要作用。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供基于EEG生成人工智能领域的结构化概述，为研究人员和从业者提供有关神经解码、增强辅助技术以及扩展人机交互边界的见解。&lt;h4&gt;方法&lt;/h4&gt;综述了通过GANs、VAEs和扩散模型实现的EEG到图像生成，以及借助Transformer语言模型和对比学习方法进行的EEG到文本生成。同时探讨了新兴领域——EEG到语音合成，并重点讨论关键数据集、使用案例、挑战及EEG特征编码方法。&lt;h4&gt;主要发现&lt;/h4&gt;近年来深度学习技术的进步显著改善了基于脑电图（EEG）产生图像、文本和语音的效果，特别是GANs和基于Transformer的大规模语言模型（LLMs）。&lt;h4&gt;结论&lt;/h4&gt;通过提供结构化概述，本文为推动神经解码的进展、增强辅助技术和拓展脑机交互领域提供了重要的研究视角。&lt;h4&gt;翻译&lt;/h4&gt;将脑电图（EEG）与生成人工智能结合的技术发展开辟了大脑信号解析的新方向。论文回顾了这些技术的应用现状，并指出了关键挑战和未来发展方向，强调其在神经解码、辅助通信及多模态数据融合方面的重要作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integration of Brain-Computer Interfaces (BCIs) and Generative ArtificialIntelligence (GenAI) has opened new frontiers in brain signal decoding,enabling assistive communication, neural representation learning, andmultimodal integration. BCIs, particularly those leveragingElectroencephalography (EEG), provide a non-invasive means of translatingneural activity into meaningful outputs. Recent advances in deep learning,including Generative Adversarial Networks (GANs) and Transformer-based LargeLanguage Models (LLMs), have significantly improved EEG-based generation ofimages, text, and speech. This paper provides a literature review of thestate-of-the-art in EEG-based multimodal generation, focusing on (i)EEG-to-image generation through GANs, Variational Autoencoders (VAEs), andDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer basedlanguage models and contrastive learning methods. Additionally, we discuss theemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. Wehighlight key datasets, use cases, challenges, and EEG feature encoding methodsthat underpin generative approaches. By providing a structured overview ofEEG-based generative AI, this survey aims to equip researchers andpractitioners with insights to advance neural decoding, enhance assistivetechnologies, and expand the frontiers of brain-computer interaction.</description>
      <author>example@mail.com (Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury)</author>
      <guid isPermaLink="false">2502.12048v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Graph Structure Learning</title>
      <link>http://arxiv.org/abs/2502.12618v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by TheWebConf 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '图神经网络（GNNs）已成为处理图结构数据学习的一种重要方法，但在图结构不理想时其效果会大打折扣。', '目的': '为了解决现有图结构学习（Graph Structure Learning, GSL）方法的两个关键局限性：一是忽视节点信息质量的重要性；二是构建的图结构通常受到对称性的限制，影响模型灵活性和有效性。', '方法': '提出了一种不确定性感知图结构学习（Uncertainty-aware Graph Structure Learning, UnGSL）策略。UnGSL通过估计节点信息的不确定性并利用它调整方向连接强度来减少具有高不确定性的节点的影响，并且可以无缝集成到现有的GSL方法中，几乎无需额外计算成本。', '主要发现': '实验显示，在将UnGSL应用于六种代表性GSL方法时，性能得到了一致的改进。', '结论': '通过引入不确定性感知机制和非对称连接调整策略，UnGSL能够提升现有图结构学习方法的有效性和灵活性。'}&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have become a prominent approach for learning from graph-structured data. However, their effectiveness can be significantly compromised when the graph structure is suboptimal. To address this issue, Graph Structure Learning (GSL) has emerged as a promising technique that refines node connections adaptively. Nevertheless, we identify two key limitations in existing GSL methods: 1) Most methods primarily focus on node similarity to construct relationships, while overlooking the quality of node information. Blindly connecting low-quality nodes and aggregating their ambiguous information can degrade the performance of other nodes. 2) The constructed graph structures are often constrained to be symmetric, which may limit the model's flexibility and effectiveness. To overcome these limitations, we propose an Uncertainty-aware Graph Structure Learning (UnGSL) strategy. UnGSL estimates the uncertainty of node information and utilizes it to adjust the strength of directional connections, where the influence of nodes with high uncertainty is adaptively reduced. Importantly, UnGSL serves as a plug-in module that can be seamlessly integrated into existing GSL methods with minimal additional computational cost. In our experiments, we implement UnGSL into six representative GSL methods, demonstrating consistent performance improvements.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a prominent approach for learningfrom graph-structured data. However, their effectiveness can be significantlycompromised when the graph structure is suboptimal. To address this issue,Graph Structure Learning (GSL) has emerged as a promising technique thatrefines node connections adaptively. Nevertheless, we identify two keylimitations in existing GSL methods: 1) Most methods primarily focus on nodesimilarity to construct relationships, while overlooking the quality of nodeinformation. Blindly connecting low-quality nodes and aggregating theirambiguous information can degrade the performance of other nodes. 2) Theconstructed graph structures are often constrained to be symmetric, which maylimit the model's flexibility and effectiveness. To overcome these limitations,we propose an Uncertainty-aware Graph Structure Learning (UnGSL) strategy.UnGSL estimates the uncertainty of node information and utilizes it to adjustthe strength of directional connections, where the influence of nodes with highuncertainty is adaptively reduced. Importantly, UnGSL serves as a plug-inmodule that can be seamlessly integrated into existing GSL methods with minimaladditional computational cost. In our experiments, we implement UnGSL into sixrepresentative GSL methods, demonstrating consistent performance improvements.</description>
      <author>example@mail.com (Shen Han, Zhiyao Zhou, Jiawei Chen, Zhezheng Hao, Sheng Zhou, Gang Wang, Yan Feng, Chun Chen, Can Wang)</author>
      <guid isPermaLink="false">2502.12618v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>ExoMiner++ on TESS with Transfer Learning from Kepler: Transit Classification and Vetting Catalog for 2-min Data</title>
      <link>http://arxiv.org/abs/2502.09790v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ExoMiner++是一种改进的深度学习模型，用于提高TESS（凌日系外行星巡天卫星）2分钟数据中的凌日信号分类准确性。&lt;h4&gt;背景&lt;/h4&gt;基于ExoMiner的成功经验，研究人员开发了ExoMiner++以更好地处理凌日信号，并区分出更多的假阳性源。&lt;h4&gt;目的&lt;/h4&gt;通过整合多种诊断输入和利用从开普勒太空望远镜中获得的高质量标签数据进行迁移学习来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;ExoMiner++引入了额外的诊断信息，包括周期图、通量趋势、差分图像、展开后的通量以及航天器姿态控制数据。&lt;h4&gt;主要发现&lt;/h4&gt;在147,568个未标记的凌日候选体中（TCE），ExoMiner++识别出了7330个为行星候选，其余则被分类为假阳性。这其中包括了与已知TESS目标对象相匹配的和新提出的社区TESS目标对象。&lt;h4&gt;结论&lt;/h4&gt;通过ExoMiner++的高准确性和优秀的排名质量，后续调查可以更加集中于最有希望的候选体上，从而提高发现行星的整体效率。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ExoMiner++，这是一种改进型深度学习模型，在2分钟TESS数据中提升凌日信号分类。该模型利用周期图、通量趋势、差分图像等作为额外诊断输入，并通过从开普勒空间望远镜高质量标签数据进行迁移学习来优化性能。ExoMiner++在多种分类和排名指标上达到了高精度，显著缩小了后续调查的搜索范围以确认新的行星。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ExoMiner++, an enhanced deep learning model that builds on thesuccess of ExoMiner to improve transit signal classification in 2-minute TESSdata. ExoMiner++ incorporates additional diagnostic inputs, includingperiodogram, flux trend, difference image, unfolded flux, and spacecraftattitude control data, all of which are crucial for effectively distinguishingtransit signals from more challenging sources of false positives. To furtherenhance performance, we leverage transfer learning from high-quality labeleddata from the Kepler space telescope, mitigating the impact of TESS's noisierand more ambiguous labels. ExoMiner++ achieves high accuracy across variousclassification and ranking metrics, significantly narrowing the search spacefor follow-up investigations to confirm new planets. To serve the exoplanetcommunity, we introduce new TESS catalogs containing ExoMiner++ classificationsand confidence scores for each transit signal. Among the 147,568 unlabeledTCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainderclassified as false positives. These 7,330 planet candidates correspond to1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects ofInterest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIspreviously labeled as planet candidates in ExoFOP are classified as planetcandidates by ExoMiner++. This reduction in plausible candidates combined withthe excellent ranking quality of ExoMiner++ allows the follow-up efforts to befocused on the most likely candidates, increasing the overall planet yield.</description>
      <author>example@mail.com (Hamed Valizadegan, Miguel J. S. Martinho, Jon M. Jenkins, Joseph D. Twicken, Douglas A. Caldwell, Patrick Maynard, Hongbo Wei, William Zhong, Charles Yates, Sam Donald, Karen A. Collins, David Latham, Khalid Barkaoui, Perry Berlind, Michael L. Calkins, Kylee Carden, Nikita Chazov, Gilbert A. Esquerdo, Tristan Guillot, Vadim Krushinsky, Grzegorz Nowak, Benjamin V. Rackham, Amaury Triaud, Richard P. Schwarz, Denise Stephens, Chris Stockdale, Jiaqi Wang, Cristilyn N. Watkins, Francis P. Wilkin)</author>
      <guid isPermaLink="false">2502.09790v3</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models</title>
      <link>http://arxiv.org/abs/2502.13187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 6 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度强化学习（RL）在机器人、交通系统和推荐系统等多个领域被证明对于解决决策任务是有效且实用的。它通过与环境互动并根据收集到的经验更新策略来学习。&lt;h4&gt;背景&lt;/h4&gt;受限于真实世界的有限数据以及执行有害行为可能带来的不可接受后果，RL策略的学习主要局限于模拟器中进行。这确保了在学习过程中的安全性但带来了部署时的模拟现实差距（sim-to-real gap），导致性能下降和潜在风险。&lt;h4&gt;目的&lt;/h4&gt;这篇综述论文旨在首次为解决不同领域的模拟到真实问题的技术提供一个基于马尔可夫决策过程关键元素（状态、行动、转换和奖励）的形式框架。并涵盖了从经典方法到最近由大基础模型支持的高级方法在内的全面文献，同时讨论了各种领域中值得特别注意的特点。&lt;h4&gt;方法&lt;/h4&gt;根据提出的方法框架，对模拟现实性能进行了正式评估过程，并使用可访问代码或基准进行总结。&lt;h4&gt;主要发现&lt;/h4&gt;论文还概述了解决模拟到真实问题所面临的挑战以及未来研究的机会。&lt;h4&gt;结论&lt;/h4&gt;作者积极维护一个资源库以包含最新的模拟到真实的研究成果来帮助研究人员的工作。&lt;h4&gt;翻译&lt;/h4&gt;深度强化学习已被探索和验证为在机器人、运输系统和推荐系统等各个领域解决决策任务的有效方法。它通过与环境互动并根据收集的经验更新策略来进行学习。然而，由于现实世界数据的限制以及执行有害行为可能带来的不可接受后果，RL策略的学习主要局限于模拟器中进行。这虽然保证了学习过程中的安全性但带来了部署时的模拟现实差距（sim-to-real gap），导致性能下降和潜在风险。在不同领域中有各种技术尝试解决这个问题，特别是在大型基础模型或语言模型等新兴技术时代，这些问题得到了更多关注。&lt;h4&gt;关键词&lt;/h4&gt;['深度强化学习', '马尔可夫决策过程', '模拟到真实问题', '模拟器']&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Reinforcement Learning (RL) has been explored and verified to beeffective in solving decision-making tasks in various domains, such asrobotics, transportation, recommender systems, etc. It learns from theinteraction with environments and updates the policy using the collectedexperience. However, due to the limited real-world data and unbearableconsequences of taking detrimental actions, the learning of RL policy is mainlyrestricted within the simulators. This practice guarantees safety in learningbut introduces an inevitable sim-to-real gap in terms of deployment, thuscausing degraded performance and risks in execution. There are attempts tosolve the sim-to-real problems from different domains with various techniques,especially in the era with emerging techniques such as large foundations orlanguage models that have cast light on the sim-to-real. This survey paper, tothe best of our knowledge, is the first taxonomy that formally frames thesim-to-real techniques from key elements of the Markov Decision Process (State,Action, Transition, and Reward). Based on the framework, we cover comprehensiveliterature from the classic to the most advanced methods including thesim-to-real techniques empowered by foundation models, and we also discuss thespecialties that are worth attention in different domains of sim-to-realproblems. Then we summarize the formal evaluation process of sim-to-realperformance with accessible code or benchmarks. The challenges andopportunities are also presented to encourage future exploration of thisdirection. We are actively maintaining a to include the most up-to-datesim-to-real research outcomes to help the researchers in their work.</description>
      <author>example@mail.com (Longchao Da, Justin Turnau, Thirulogasankar Pranav Kutralingam, Alvaro Velasquez, Paulo Shakarian, Hua Wei)</author>
      <guid isPermaLink="false">2502.13187v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models</title>
      <link>http://arxiv.org/abs/2502.13179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为PTQ1.61的极低比特量化（Post-Training Quantization，简称PTQ）方法，首次实现将权重量化到1.61比特。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在面对极其低比特（小于2比特）量化时性能严重下降。现有的一些次2位后训练量化方法使用混合精度方案，并引入了额外的0.5或更多的比特以区分显著权重。&lt;h4&gt;目的&lt;/h4&gt;探索PTQ的真实极限，开发一种极低比特量化的方法。&lt;h4&gt;方法&lt;/h4&gt;引入了一维结构化掩码和基于输入激活降低量化误差上限的方法，使显著权重通道能分配到4位。对于非显著通道二值化，提出高效的块级缩放因子优化框架，考虑隐含的行相关性和角度偏差。此外，论文还提出了量化预处理的概念。&lt;h4&gt;主要发现&lt;/h4&gt;PTQ1.61在极低比特量化中实现了最先进的性能，并展示了代码库的位置。&lt;h4&gt;结论&lt;/h4&gt;通过新的结构化掩码和高效优化框架，首次实现了模型权重到1.61位的量化，在极端低比特环境下保持了良好的性能。同时强调了量化预处理对于提高低比特PTQ效果的重要性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在面对极低比特（小于2比特）量化时表现出严重性能下降。现有的一些次2位后训练量化方法通过混合精度方案使用未结构化的细粒度掩码来区分显著权重，而这种方法会为每个权重引入额外的0.5个或更多的比特。为了探索PTQ的真实极限，我们提出了一种称为PTQ1.61的极低比特量化方法，首次实现了将权重量化到1.61比特的能力。具体而言，我们首先基于输入激活量从减少量化误差上限的角度引入了一个一维结构化掩码，每个权重仅增加几乎可以忽略不计的0.0002个比特，并允许相应的显著权重重分配至4位。对于非显著通道二值化，提出了一种高效的块级缩放因子优化框架来考虑隐含的行相关性和角度偏差。不同于以往的工作集中于调整量化方法论，我们进一步提出了量化预处理的新范式，即在量化前转换预先训练模型的权重分布可以缓解极低比特PTQ中的困难。广泛的实验表明我们的PTQ1.61在极低比特量化中达到了最先进的性能。代码可在https://github.com/zjq0455/PTQ1.61获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) suffer severe performance degradation whenfacing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bitpost-training quantization (PTQ) methods utilize a mix-precision scheme byleveraging an unstructured fine-grained mask to explicitly distinguish salientweights, while which introduces an extra 1-bit or more per weight. To explorethe real limit of PTQ, we propose an extremely low-bit PTQ method calledPTQ1.61, which enables weight quantization to 1.61-bit for the first time.Specifically, we first introduce a one-dimensional structured mask withnegligibly additional 0.0002-bit per weight based on input activations from theperspective of reducing the upper bound of quantization error to allocatecorresponding salient weight channels to 4-bit. For non-salient channelsbinarization, an efficient block-wise scaling factors optimization framework isthen presented to take implicit row-wise correlations and angular biases intoaccount. Different from prior works that concentrate on adjusting quantizationmethodologies, we further propose a novel paradigm called quantizationpreprocessing, where we argue that transforming the weight distribution of thepretrained model before quantization can alleviate the difficulty inper-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61achieves state-of-the-art performance in extremely low-bit quantization. Codesare available at https://github.com/zjq0455/PTQ1.61.</description>
      <author>example@mail.com (Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Min Zhang)</author>
      <guid isPermaLink="false">2502.13179v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects</title>
      <link>http://arxiv.org/abs/2502.13964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://arjung128.github.io/svm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为视觉模型伺服控制（SVM）的新框架，用于移动机械臂在现实世界中处理需要精准操作小物体的任务。该方法无需训练，通过利用先进的视觉模型和腕部RGB-D相机实现目标的可靠检测与定位。&lt;h4&gt;背景&lt;/h4&gt;日常生活中许多任务都需要对小物件进行精确的操作，例如打开柜子或按下开关等。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需先期培训且适用于多种环境下的移动机械臂操作框架，以应对涉及小物体精准处理的任务。&lt;h4&gt;方法&lt;/h4&gt;使用RGB-D腕部相机和视觉伺服控制技术；采用先进的计算机视觉模型来计算3D目标位置；利用‘出画’（out-painting）技术减轻因末端执行器造成遮挡的影响，并提高目标定位的准确性；通过开放词汇的对象检测算法识别语义上的目标物体，用户点击确定交互点。&lt;h4&gt;主要发现&lt;/h4&gt;借助于‘出画’方法，无需训练的方法在处理未见过的对象时达到了85%的成功率，在真实世界环境中显著优于开环控制及基于模仿学习的基线模型（后者经过1000多次演示训练）。&lt;h4&gt;结论&lt;/h4&gt;SVM框架展示了其强大的泛化能力以及对未知环境和任务的有效适应，为移动机器人在实际应用中的操作提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many everyday mobile manipulation tasks require precise interaction withsmall objects, such as grasping a knob to open a cabinet or pressing a lightswitch. In this paper, we develop Servoing with Vision Models (SVM), aclosed-loop training-free framework that enables a mobile manipulator to tacklesuch precise tasks involving the manipulation of small objects. SVM employs anRGB-D wrist camera and uses visual servoing for control. Our novelty lies inthe use of state-of-the-art vision models to reliably compute 3D targets fromthe wrist image for diverse tasks and under occlusion due to the end-effector.To mitigate occlusion artifacts, we employ vision models to out-paint theend-effector thereby significantly enhancing target localization. Wedemonstrate that aided by out-painting methods, open-vocabulary objectdetectors can serve as a drop-in module to identify semantic targets (e.g.knobs) and point tracking methods can reliably track interaction sitesindicated by user clicks. This training-free method obtains an 85% zero-shotsuccess rate on manipulating unseen objects in novel environments in the realworld, outperforming an open-loop control method and an imitation learningbaseline trained on 1000+ demonstrations by an absolute success rate of 50%.</description>
      <author>example@mail.com (Arjun Gupta, Rishik Sathua, Saurabh Gupta)</author>
      <guid isPermaLink="false">2502.13964v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360$^\circ$ Cameras</title>
      <link>http://arxiv.org/abs/2502.12545v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们提出了一种用于360°相机的新型3D重建流水线，旨在进行室内环境的3D映射和渲染。&lt;h4&gt;背景&lt;/h4&gt;传统的基于运动结构（SfM）的方法可能不适合大规模的室内场景，因为这些场景中存在无纹理和重复区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服传统SfM方法在处理大型室内场景中的不足。&lt;h4&gt;方法&lt;/h4&gt;{'IM360方法': '利用全景图像的广阔视野，并将球形相机模型集成到SfM流水线的核心组件中。引入神经隐式曲面重建技术，从稀疏输入数据生成高质量表面；使用基于网格的神经渲染技术来细化纹理图并准确捕捉依赖视点属性。', '评价': '在Matterport3D和Stanford2D3D数据集中的大规模室内场景上进行评估。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': 'IM360在纹理网格重建方面优于现有最佳方法（SOTA）。', '改进的准确性': '观察到相机定位、注册以及渲染高频细节方面的准确度有所提高。'}&lt;h4&gt;结论&lt;/h4&gt;我们的研究为大型室内场景的高质量3D重建提供了一种有前景的方法，展示出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的360°摄像机3D重建流水线，用于大型室内环境的3D映射和渲染。传统的基于运动结构（SfM）方法在大规模无纹理且重复区域较多的场景中表现不佳。为了克服这些挑战，我们的IM360方法利用了全景图像的广阔视野，并将球形相机模型集成到了SfM流水线的核心组件中。此外，我们还整合了一个神经隐式表面重建技术来从稀疏输入数据生成高质量曲面，并且采用了一种基于网格的神经渲染方式来细化纹理图并准确捕捉依赖视点属性。我们在Matterport3D和Stanford2D3D数据集的大规模室内场景中评估了我们的流水线，IM360在纹理网格重建方面表现出比现有最佳方法（SOTA）更好的性能，同时我们还观察到了相机定位、注册以及渲染高频细节方面的准确度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel 3D reconstruction pipeline for 360$^\circ$ cameras for 3Dmapping and rendering of indoor environments. Traditional Structure-from-Motion(SfM) methods may not work well in large-scale indoor scenes due to theprevalence of textureless and repetitive regions. To overcome these challenges,our approach (IM360) leverages the wide field of view of omnidirectional imagesand integrates the spherical camera model into every core component of the SfMpipeline. In order to develop a comprehensive 3D reconstruction solution, weintegrate a neural implicit surface reconstruction technique to generatehigh-quality surfaces from sparse input data. Additionally, we utilize amesh-based neural rendering approach to refine texture maps and accuratelycapture view-dependent properties by combining diffuse and specular components.We evaluate our pipeline on large-scale indoor scenes from the Matterport3D andStanford2D3D datasets. In practice, IM360 demonstrate superior performance interms of textured mesh reconstruction over SOTA. We observe accuracyimprovements in terms of camera localization and registration as well asrendering high frequency details.</description>
      <author>example@mail.com (Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha)</author>
      <guid isPermaLink="false">2502.12545v2</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>3D Gaussian Splatting aided Localization for Large and Complex Indoor-Environments</title>
      <link>http://arxiv.org/abs/2502.13803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了通过增加渲染图像来显著提高视觉定位精度和可靠性的新方法。&lt;h4&gt;背景&lt;/h4&gt;视觉定位领域经过几十年的研究，已经找到了许多实际应用。然而，在某些挑战性情况下，现有方法仍然表现不佳。&lt;h4&gt;目的&lt;/h4&gt;改进现有的视觉定位技术的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;首先使用现代视觉SLAM（Simultaneous Localization and Mapping）方法生成3D Gaussian Splatting (3DGS) 基于的地图作为参考数据；然后通过在随机采样的姿态下从3DGS渲染图像来丰富参考数据，以提升基于几何的视觉定位和Scene Coordinate Regression (SCR) 方法的表现。&lt;h4&gt;主要发现&lt;/h4&gt;增加渲染图像可以显著提高基于几何的方法和SCRe方法的性能，在工业环境中进行了全面评估，并分析了加入额外渲染视图对性能的影响。&lt;h4&gt;结论&lt;/h4&gt;通过添加渲染图像，能够有效提升视觉定位技术在复杂环境中的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位领域经过几十年的研究，已经找到了许多实际应用。然而，尽管该领域取得了巨大进展，在某些挑战情况下，现有方法仍然表现出局限性。我们提出了一种改进传统视觉定位方法精度和可靠性的新途径，即通过添加渲染图像实现。具体而言，首先使用现代的视觉SLAM技术生成3D Gaussian Splatting (3DGS) 基的地图作为参考数据；然后通过从该地图随机采样的姿态下进行渲染来丰富这些参考数据，并证明这种做法可以显著提高基于几何的定位方法和Scene Coordinate Regression (SCR) 方法的表现。在大型工业环境中进行了全面评估，分析了添加额外渲染视图对性能的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of visual localization has been researched for several decades andhas meanwhile found many practical applications. Despite the strong progress inthis field, there are still challenging situations in which established methodsfail. We present an approach to significantly improve the accuracy andreliability of established visual localization methods by adding renderedimages. In detail, we first use a modern visual SLAM approach that provides a3D Gaussian Splatting (3DGS) based map to create reference data. We demonstratethat enriching reference data with images rendered from 3DGS at randomlysampled poses significantly improves the performance of both geometry-basedvisual localization and Scene Coordinate Regression (SCR) methods. Throughcomprehensive evaluation in a large industrial environment, we analyze theperformance impact of incorporating these additional rendered views.</description>
      <author>example@mail.com (Vincent Ress, Jonas Meyer, Wei Zhang, David Skuddis, Uwe Soergel, Norbert Haala)</author>
      <guid isPermaLink="false">2502.13803v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>The NavINST Dataset for Multi-Sensor Autonomous Navigation</title>
      <link>http://arxiv.org/abs/2502.13863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 20 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NavINST实验室开发了一套全面的多感官数据集，该数据集包含了城市环境中的各种道路测试轨迹，并涵盖了不同的照明条件，包括室内车库场景以及密集3D地图。&lt;h4&gt;背景&lt;/h4&gt;为了支持高精度定位、导航、制图、计算机视觉和多传感器融合等领域的高级研究，NavINST实验室创建了一个包含多个商用级IMU和高端战术级IMU的数据集。&lt;h4&gt;目的&lt;/h4&gt;该数据集旨在为自动驾驶车辆算法的开发和验证提供丰富的、多传感器信息。它包括了各种感知基础传感器以及准确后处理过的高精度GNSS/IMU数据，以提供精确的位置定位和导航信息。&lt;h4&gt;方法&lt;/h4&gt;NavINST实验室创建的数据集使用多种商用级惯性测量单元（IMUs）和高端战术级IMU，还包括固态激光雷达、机械式激光雷达、四个电子扫描雷达、单目摄像头以及两个立体摄像机。此外，该数据集还包含了从车辆里程表中提取的前向速度信息。&lt;h4&gt;主要发现&lt;/h4&gt;NavINST数据集是首个包含固态激光雷达的数据集之一，并且提供了精确的地面真实位置和导航信息，适用于开发和验证自主驾驶汽车的鲁棒算法。&lt;h4&gt;结论&lt;/h4&gt;该数据集已完全集成到ROS中，确保了对研究社区的易用性和可访问性。整个数据集及其开发工具可通过https://navinst.github.io获取。&lt;h4&gt;翻译&lt;/h4&gt;NavINST实验室在城市环境中通过各种道路测试轨迹创建了一个全面的多感官数据集，涵盖了不同的照明条件，包括带有密集3D地图的室内车库场景。该数据集包含多个商用级IMU和高端战术级IMU以及多种感知基础传感器（如固态激光雷达、机械式激光雷达、四个电子扫描雷达、单目摄像头及两个立体摄像机）。此外，还包括从车辆里程表中提取的前向速度信息，并提供高精度后处理后的GNSS/IMU数据。NavINST数据集用于支持高精度定位、导航、制图、计算机视觉和多传感器融合等领域的高级研究，为自动驾驶汽车算法的发展与验证提供了丰富的多传感器数据。整个数据集已完全集成到ROS中，确保了对研究社区的易用性和可访问性，并可通过https://navinst.github.io获取其全部内容及开发工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The NavINST Laboratory has developed a comprehensive multisensory datasetfrom various road-test trajectories in urban environments, featuring diverselighting conditions, including indoor garage scenarios with dense 3D maps. Thisdataset includes multiple commercial-grade IMUs and a high-end tactical-gradeIMU. Additionally, it contains a wide array of perception-based sensors, suchas a solid-state LiDAR - making it one of the first datasets to do so - amechanical LiDAR, four electronically scanning RADARs, a monocular camera, andtwo stereo cameras. The dataset also includes forward speed measurementsderived from the vehicle's odometer, along with accurately post-processedhigh-end GNSS/IMU data, providing precise ground truth positioning andnavigation information. The NavINST dataset is designed to support advancedresearch in high-precision positioning, navigation, mapping, computer vision,and multisensory fusion. It offers rich, multi-sensor data ideal for developingand validating robust algorithms for autonomous vehicles. Finally, it is fullyintegrated with the ROS, ensuring ease of use and accessibility for theresearch community. The complete dataset and development tools are available athttps://navinst.github.io.</description>
      <author>example@mail.com (Paulo Ricardo Marques de Araujo, Eslam Mounier, Qamar Bader, Emma Dawson, Shaza I. Kaoud Abdelaziz, Ahmed Zekry, Mohamed Elhabiby, Aboelmagd Noureldin)</author>
      <guid isPermaLink="false">2502.13863v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Minimally sufficient structures for information-feedback policies</title>
      <link>http://arxiv.org/abs/2502.13852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 16th International Workshop on the Algorithmic Foundations of  Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;论文探讨了机器人在物理世界中的任务需求，这些任务需要通过有限感知、记忆和计算来实现。&lt;h4&gt;目的&lt;/h4&gt;设计一个过滤器来维持对物理世界的有用表示，并根据这个表示制定策略。&lt;h4&gt;方法&lt;/h4&gt;将过滤器看作机器人基于其感官信息的物理世界的视角。研究了内部系统（如过滤器）与外部物理世界如何通过传感器映射和信息反馈政策相互作用，以实现给定任务。&lt;h4&gt;主要发现&lt;/h4&gt;论文建立了使信息反馈政策存在的必要条件，并证明在轻微假设下存在唯一最小化内部系统来表示特定的规划/策略。&lt;h4&gt;结论&lt;/h4&gt;结果应用于确定距离最优导航所需的结构，在多边形环境中特别有效。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们考虑了机器人需要完成的任务，这些任务要求在一个物理世界中实现期望的结果。为了达到这一目标，我们需要设计一个过滤器来维持对这个物理世界的有用表示，并在此基础上制定策略。过滤器被视为机器人基于有限感官、记忆和计算的视角，并通过传感器映射和信息反馈政策将内部系统（如过滤器）与外部物理世界联系起来。论文确立了使这些结构存在的必要条件，以及在轻微假设下唯一最小化内部系统的存在性和独特性。最后，研究结果应用于确定多边形环境中的距离最优导航所需的足够结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we consider robotic tasks which require a desirable outcome tobe achieved in the physical world that the robot is embedded in and interactingwith. Accomplishing this objective requires designing a filter that maintains auseful representation of the physical world and a policy over the filterstates. A filter is seen as the robot's perspective of the physical world basedon limited sensing, memory, and computation and it is represented as atransition system over a space of information states. To this end, theinteractions result from the coupling of an internal and an external system, afilter, and the physical world, respectively, through a sensor mapping and aninformation-feedback policy. Within this setup, we look for sufficientstructures, that is, sufficient internal systems and sensors, for accomplishinga given task. We establish necessary and sufficient conditions for thesestructures to satisfy for information-feedback policies that can be definedover the states of an internal system to exist. We also show that under mildassumptions, minimal internal systems that can represent a particularplan/policy described over the action-observation histories exist and areunique. Finally, the results are applied to determine sufficient structures fordistance-optimal navigation in a polygonal environment.</description>
      <author>example@mail.com (Basak Sakcak, Vadim K. Weinstein, Kalle G. Timperi, Steven M. LaValle)</author>
      <guid isPermaLink="false">2502.13852v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>An Online Optimization-Based Trajectory Planning Approach for Cooperative Landing Tasks</title>
      <link>http://arxiv.org/abs/2502.13823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对由四旋翼和地面移动机器人组成的异构多机器人系统的实时轨迹规划方案，用于协作着陆任务。&lt;h4&gt;背景&lt;/h4&gt;现有系统缺乏对协同作业的灵活性和自主性的支持，特别是在需要动态调整以满足用户需求的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来实现基于可行性与用户规范的自动决定着陆位置、时间和机器人协调的任务。&lt;h4&gt;方法&lt;/h4&gt;利用互补性约束作为决策制定工具，并将其应用于协作降落场景中。该方案在模拟和实际应用中进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;通过将地面移动机器人用作移动充电站并与需要充电的四旋翼进行实时协同，实现了安全有效的会合与着陆。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架展示了其实时能力，并为类似协作任务提供了潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已翻译成中文并进行了分点总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a real-time trajectory planning scheme for aheterogeneous multi-robot system (consisting of a quadrotor and a ground mobilerobot) for a cooperative landing task, where the landing position, landingtime, and coordination between the robots are determined autonomously under theconsideration of feasibility and user specifications. The proposed frameworkleverages the potential of the complementarity constraint as a decision-makerand an indicator for diverse cooperative tasks and extends it to thecollaborative landing scenario. In a potential application of the proposedmethodology, a ground mobile robot may serve as a mobile charging station andcoordinates in real-time with a quadrotor to be charged, facilitating a safeand efficient rendezvous and landing. We verified the generated trajectories insimulation and real-world applications, demonstrating the real-timecapabilities of the proposed landing planning framework.</description>
      <author>example@mail.com (Jingshan Chen, Lihan Xu, Henrik Ebel, Peter Eberhard)</author>
      <guid isPermaLink="false">2502.13823v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Embodied Emotional Communication: A Human-oriented Review of Mediated Social Touch</title>
      <link>http://arxiv.org/abs/2502.13816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is 41 pages long, including references and appendices, and  contains 8 figures. The manuscript has been accepted for publication in CCF  Transactions on Pervasive Computing and Interaction but has not yet been  officially published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文通过回顾文献，为中介社会触摸（MST）提供了以人为主体的理解框架。&lt;h4&gt;研究背景&lt;/h4&gt;涵盖了触觉接口、情感信息、映射机制以及人际和人机交互的动态变化。&lt;h4&gt;研究目的&lt;/h4&gt;通过对37个选定的MST案例进行现有及探索性映射策略的研究，构建了容纳各种情绪的情感表达空间，并探讨了如何将情感线索转化为触觉信号。此外，还根据MST的表现力构建了一个设计空间。&lt;h4&gt;主要发现&lt;/h4&gt;建立了基于类别模型和效价-唤醒模型结合的情感表达空间；提出了包括工作流程、评估方法及伦理与文化考虑在内的多种MST设计方案；指出了未来的研究方向。&lt;h4&gt;结论&lt;/h4&gt;论文旨在为设计研究人员和从业者提供一个全面的参考，扩大情感交流范围，促进情感触觉应用探索，增强触觉交互的自然性和社交性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：This paper offers a structured understanding of mediated social touch (MST)using a human-oriented approach, through an extensive review of literaturespanning tactile interfaces, emotional information, mapping mechanisms, and thedynamics of human-human and human-robot interactions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper offers a structured understanding of mediated social touch (MST)using a human-oriented approach, through an extensive review of literaturespanning tactile interfaces, emotional information, mapping mechanisms, and thedynamics of human-human and human-robot interactions. By investigating theexisting and exploratory mapping strategies of the 37 selected MST cases, weestablished the emotional expression space of MSTs that accommodated a diversespectrum of emotions by integrating the categorical and Valence-arousal models,showcasing how emotional cues can be translated into tactile signals. Based onthe expressive capacity of MSTs, a practical design space was structuredencompassing factors such as the body locations, device form, tactilemodalities, and parameters. We also proposed various design strategies for MSTsincluding workflow, evaluation methods, and ethical and culturalconsiderations, as well as several future research directions. MSTs' potentialis reflected not only in conveying emotional information but also in fosteringempathy, comfort, and connection in both human-human and human-robotinteractions. This paper aims to serve as a comprehensive reference for designresearchers and practitioners, which helps expand the scope of emotionalcommunication of MSTs, facilitating the exploration of diverse applications ofaffective haptics, and enhancing the naturalness and sociability of hapticinteraction.</description>
      <author>example@mail.com (Liwen He, Zichun Guo, Yanru Mo, Yue Wen, Yun Wang)</author>
      <guid isPermaLink="false">2502.13816v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Muscle Activation Estimation by Optimzing the Musculoskeletal Model for Personalized Strength and Conditioning Training</title>
      <link>http://arxiv.org/abs/2502.13760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个全身肌肉骨骼模型用于力量和体能训练，并通过基于表面肌电图的优化方法校准相关肌肉参数。&lt;h4&gt;背景&lt;/h4&gt;肌肉骨骼模型在康复与抗阻训练领域中对于分析肌肉状况至关重要。然而，个体间肌肉骨骼参数差异及一些内部生物力学变量无法直接测量的问题导致个性化建模非常困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的全身肌肉骨骼模型，并通过基于表面肌电图的优化方法校准相关的肌肉参数，以更准确地估计肌肉激活情况，从而分析训练表现。&lt;h4&gt;方法&lt;/h4&gt;使用基于表面肌电图（EMG）的优化方法来个性化调整全身肌肉骨骼模型中的相关肌肉参数。利用这个个性化的肌肉骨骼模型，研究者能够随后估算肌肉激活情况，进而分析运动表现。&lt;h4&gt;主要发现&lt;/h4&gt;通过选择杠铃卧推和硬拉作为实验验证的方法，证实了该方法的有效性，并展示了如何更准确地估计肌肉激活和性能。&lt;h4&gt;结论&lt;/h4&gt;开发的全身肌肉骨骼模型及其优化参数对于个性化训练方案设计具有重要意义。这些成果为未来康复医学和运动科学的研究提供了新的视角和工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Musculoskeletal models are pivotal in the domains of rehabilitation andresistance training to analyze muscle conditions. However, individualvariability in musculoskeletal parameters and the immeasurability of someinternal biomechanical variables pose significant obstacles to accuratepersonalized modelling. Furthermore, muscle activation estimation can bechallenging due to the inherent redundancy of the musculoskeletal system, wheremultiple muscles drive a single joint. This study develops a whole-bodymusculoskeletal model for strength and conditioning training and calibratesrelevant muscle parameters with an electromyography-based optimization method.By utilizing the personalized musculoskeletal model, muscle activation can besubsequently estimated to analyze the performance of exercises. Bench press anddeadlift are chosen for experimental verification to affirm the efficacy ofthis approach.</description>
      <author>example@mail.com (Xi Wu, Chenzui Li, Kehan Zou, Ning Xi, Fei Chen)</author>
      <guid isPermaLink="false">2502.13760v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Active Illumination for Visual Ego-Motion Estimation in the Dark</title>
      <link>http://arxiv.org/abs/2502.13708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;视觉里程计（VO）和视觉同步定位与地图构建（V-SLAM）系统在低光或黑暗环境中表现不佳，因为缺乏稳健的视觉特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型主动照明框架，用于增强这些算法在挑战性条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;{'主动照明框架': '动态控制移动光源以照亮具有高度纹理的区域', '探测器块': '结合深度学习增强网络来识别具有相关特征的区域', '云台控制器': '负责将光线引导至这些区域，提供富含信息的图像给自运动估计算法'}&lt;h4&gt;主要发现&lt;/h4&gt;在实际机器人平台上进行实验的结果显示，与传统的固定照明技术相比，所提出的方法可减少姿态估计误差高达75%。&lt;h4&gt;结论&lt;/h4&gt;该主动照明框架显著提高了VO和V-SLAM系统在低光条件下的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了视觉里程计（VO）和视觉同步定位与地图构建（V-SLAM）技术在低光或黑暗环境中存在的挑战，并提出了一种基于动态控制移动光源的主动照明框架来提高这些算法的表现，通过实验验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Odometry (VO) and Visual SLAM (V-SLAM) systems often struggle inlow-light and dark environments due to the lack of robust visual features. Inthis paper, we propose a novel active illumination framework to enhance theperformance of VO and V-SLAM algorithms in these challenging conditions. Thedeveloped approach dynamically controls a moving light source to illuminatehighly textured areas, thereby improving feature extraction and tracking.Specifically, a detector block, which incorporates a deep learning-basedenhancing network, identifies regions with relevant features. Then, a pan-tiltcontroller is responsible for guiding the light beam toward these areas, sothat to provide information-rich images to the ego-motion estimation algorithm.Experimental results on a real robotic platform demonstrate the effectivenessof the proposed method, showing a reduction in the pose estimation error up to75% with respect to a traditional fixed lighting technique.</description>
      <author>example@mail.com (Francesco Crocetti, Alberto Dionigi, Raffaele Brilli, Gabriele Costante, Paolo Valigi)</author>
      <guid isPermaLink="false">2502.13708v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Human-Like Robot Impedance Regulation Skill Learning from Human-Human Demonstrations</title>
      <link>http://arxiv.org/abs/2502.13707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种创新的阻抗调节技能学习框架，旨在实现多种物理协作任务中的人机合作（HRC）。该框架通过调整机器人与人类伙伴状态相适应的顺应性来工作，并根据人与人的示范提供的参考轨迹进行操作。&lt;h4&gt;背景&lt;/h4&gt;人类在基于对同伴状态和任务需求感知的基础上调节顺从行为方面擅长协作。让机器人掌握这种技能可以促进更高效的人机协作（HRC）。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架，通过调整机器人的顺应性来实现有效的人机物理合作。&lt;h4&gt;方法&lt;/h4&gt;收集并分析人类肌肉的肌电图(EMG)信号以提取肢体阻抗；采用概率学习方法捕捉和表示人体端点运动，并创建参考轨迹和相应的阻抗配置文件。使用LSTM模块开发任务导向型阻抗调节策略，同时提出一种针对类人机器人的全身阻抗控制器。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证表明，在协作运输任务以及两个互动太极推手任务中，与恒定阻抗控制方法相比，本文所提出的框架从交互力的角度表现出更优性能。&lt;h4&gt;结论&lt;/h4&gt;该研究通过开发适应性更好的人机合作技能，促进了更加高效和自然的人机物理协作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans are experts in collaborating with others physically by regulatingcompliance behaviors based on the perception of their partner states and thetask requirements. Enabling robots to develop proficiency in humancollaboration skills can facilitate more efficient human-robot collaboration(HRC). This paper introduces an innovative impedance regulation skill learningframework for achieving HRC in multiple physical collaborative tasks. Theframework is designed to adjust the robot compliance to the human partnerstates while adhering to reference trajectories provided by human-humandemonstrations. Specifically, electromyography (EMG) signals from human musclesare collected and analyzed to extract limb impedance, representing compliancebehaviors during demonstrations. Human endpoint motions are captured andrepresented using a probabilistic learning method to create referencetrajectories and corresponding impedance profiles. Meanwhile, an LSTMbasedmodule is implemented to develop task-oriented impedance regulation policies bymapping the muscle synergistic contributions between two demonstrators.Finally, we propose a wholebody impedance controller for a human-like robot,coordinating joint outputs to achieve the desired impedance and referencetrajectory during task execution. Experimental validation was conducted througha collaborative transportation task and two interactive Tai Chi pushing handstasks, demonstrating superior performance from the perspective of interactiveforces compared to a constant impedance control method.</description>
      <author>example@mail.com (Chenzui Li, Xi Wu, Junjia Liu, Tao Teng, Yiming Chen, Sylvain Calinon, Darwin Caldwell, Fei Chen)</author>
      <guid isPermaLink="false">2502.13707v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for Semantics-based Situational Awareness during Mobile Robot Deployments</title>
      <link>http://arxiv.org/abs/2502.13677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在危险环境中部署机器人时，人机团队合作中情景意识的高级语义信息理解和获取的方法。提出了一种可扩展框架，用于远程部署移动机器人的多模态语义级情景意识采集和整合，并通过灾难响应机器人中的搜索与救援应用来演示该框架。&lt;h4&gt;背景&lt;/h4&gt;在危险环境中部署机器人通常采用人机团队合作模式，其中人类监督者与远程操作的机器人协同工作。情景意识对于支持导航、规划和决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究不同自主程度下语义信息的重要性和差异，并提出一种获取和整合多模态语义级情景意识的通用框架。&lt;h4&gt;方法&lt;/h4&gt;提出了“环境语义指标”，这些指标可以反映风险指示或人类活动迹象等不同类型的语义信息。基于这些指标，设计了一种称为“情境语义丰富度（SSR）”的综合评估指标，该指标结合了多种语义指示来总结整体情况。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示提出的语义指标对不同场景中各种模态的语义信息变化敏感，且SSR度量能够反映遇到的情境的整体语义变化。这表明信息丰富和复杂的情况可能需要高级推理能力和专家人类操作员的关注。&lt;h4&gt;结论&lt;/h4&gt;该框架为远程部署移动机器人提供了强大的情景意识支持，有助于提高人机团队合作的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deployment of robots into hazardous environments typically involves a``Human-Robot Teaming'' (HRT) paradigm, in which a human supervisor interactswith a remotely operating robot inside the hazardous zone. SituationalAwareness (SA) is vital for enabling HRT, to support navigation, planning, anddecision-making. This paper explores issues of higher-level ``semantic''information and understanding in SA. In semi-autonomous, or variable-autonomyparadigms, different types of semantic information may be important, indifferent ways, for both the human operator and an autonomous agent controllingthe robot. We propose a generalizable framework for acquiring and combiningmultiple modalities of semantic-level SA during remote deployments of mobilerobots. We demonstrate the framework with an example application of search andrescue (SAR) in disaster response robotics. We propose a set of ``environmentsemantic indicators" that can reflect a variety of different types of semanticinformation, e.g. indicators of risk, or signs of human activity, as the robotencounters different scenes. Based on these indicators, we propose a metric todescribe the overall situation of the environment called ``Situational SemanticRichness (SSR)". This metric combines multiple semantic indicators to summarisethe overall situation. The SSR indicates if an information-rich and complexsituation has been encountered, which may require advanced reasoning for robotsand humans and hence the attention of the expert human operator. The frameworkis tested on a Jackal robot in a mock-up disaster response environment.Experimental results demonstrate that the proposed semantic indicators aresensitive to changes in different modalities of semantic information indifferent scenes, and the SSR metric reflects overall semantic changes in thesituations encountered.</description>
      <author>example@mail.com (Tianshu Ruan, Aniketh Ramesh, Hao Wang, Alix Johnstone-Morfoisse, Gokcenur Altindal, Paul Norman, Grigoris Nikolaou, Rustam Stolkin, Manolis Chiou)</author>
      <guid isPermaLink="false">2502.13677v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>An Adaptive Data-Enabled Policy Optimization Approach for Autonomous Bicycle Control</title>
      <link>http://arxiv.org/abs/2502.13676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一控制框架，该框架结合了内环的反馈线性化（FL）控制器和外环的自适应数据增强策略优化（DeePO）控制器来平衡自主自行车。&lt;h4&gt;背景&lt;/h4&gt;由于自主自行车系统本质上是不稳定且非线性的，因此使用FL控制器可以稳定并部分线性化系统。然而，性能会因未建模动态效应以及时间变化特性而受到影响。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些限制，并增强系统的适应性和鲁棒性，引入了DeePO控制器来与FL控制器协同工作。&lt;h4&gt;方法&lt;/h4&gt;初始控制策略通过离线的持续激励输入和状态数据获得。利用一种促进鲁棒性的正则化器改进初始策略，同时增加一个遗忘因子以提升适应时间变化动态的能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和实际实验验证了DeePO+FL方法的有效性，并表明其在跟踪参考倾斜角度和倾斜速率的精确度上优于仅使用FL的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的控制框架能够显著提高自主自行车系统的稳定性和性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要的内容描述了一种结合反馈线性化和自适应数据增强策略优化来平衡自主自行车的新方法，展示了该方法在模拟和实际实验中的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a unified control framework that integrates a FeedbackLinearization (FL) controller in the inner loop with an adaptive Data-EnabledPolicy Optimization (DeePO) controller in the outer loop to balance anautonomous bicycle. While the FL controller stabilizes and partially linearizesthe inherently unstable and nonlinear system, its performance is compromised byunmodeled dynamics and time-varying characteristics. To overcome theselimitations, the DeePO controller is introduced to enhance adaptability androbustness. The initial control policy of DeePO is obtained from a finite setof offline, persistently exciting input and state data. To improve stabilityand compensate for system nonlinearities and disturbances, arobustness-promoting regularizer refines the initial policy, while the adaptivesection of the DeePO framework is enhanced with a forgetting factor to improveadaptation to time-varying dynamics. The proposed DeePO+FL approach isevaluated through simulations and real-world experiments on an instrumentedautonomous bicycle. Results demonstrate its superiority over the FL-onlyapproach, achieving more precise tracking of the reference lean angle and leanrate.</description>
      <author>example@mail.com (Niklas Persson, Feiran Zhao, Mojtaba Kaheni, Florian Dörfler, Alessandro V. Papadopoulos)</author>
      <guid isPermaLink="false">2502.13676v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis</title>
      <link>http://arxiv.org/abs/2502.13641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7pages, 6figures, accepted at IEEE International Conference on  Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SLAMSpoof是一种针对自动驾驶车辆定位系统的LiDAR欺骗攻击，通过评估LiDAR在实际环境中的易受攻击性来检验其对自主驾驶的影响。&lt;h4&gt;背景&lt;/h4&gt;现代全自动驾驶服务依赖于地图交通信息进行精确的车道形状、交通灯位置和标志识别。为了实现这一点，需要厘米级的定位精度，而目前只有LiDAR传感器能够达到这一要求。然而，由于LiDAR容易受到激光欺骗攻击的影响，这种安全威胁引起了人们的关注。&lt;h4&gt;目的&lt;/h4&gt;设计SLAMSpoof来评估实际场景中对基于LiDAR的自动驾驶车辆定位系统进行欺骗攻击的安全性影响。&lt;h4&gt;方法&lt;/h4&gt;通过扫描匹配脆弱性评分（SMVS）找到有效的攻击位置，并在真实环境中测试了该攻击的有效性，证明其能够在所有流行的LiDAR定位算法上引入超过4.2米的位置误差。&lt;h4&gt;主要发现&lt;/h4&gt;SLAMSpoof能够有效利用基于LiDAR的定位系统中的漏洞，在现实世界场景中诱导出显著的位置错误。&lt;h4&gt;结论&lt;/h4&gt;论文指出了自动驾驶车辆在使用LiDAR时可能面临的安全威胁，并提出了评估该类攻击实际影响的方法。同时，讨论了对抗此类攻击的潜在对策。&lt;h4&gt;翻译&lt;/h4&gt;精确定位对于实现现代全自动驾驶服务至关重要。这些服务高度依赖于基于地图的交通信息来减少识别车道形状、交通信号灯位置和标志等不确定性的程度。为了达到这种对地图数据的高度信赖，需要厘米级精度的定位能力，目前只有激光雷达（LiDAR）传感器可以提供这样的精确度。然而，由于激光雷达容易受到恶意发射激光欺骗其读数的安全威胁，一旦定位系统被攻破，可能导致车辆偏离道路或忽视交通信号灯等严重后果。鉴于此类攻击所带来的安全问题，研究团队设计了SLAMSpoof，这是首个针对自动驾驶车辆定位系统的实际LiDAR欺骗攻击方法，用以评估这种攻击对自主驾驶汽车的实际影响。通过扫描匹配脆弱性评分（SMVS），该技术能够找到有效的攻击位置，并在实地测试中证明其能够在所有流行的基于激光雷达的定位算法上引入显著的位置误差。研究团队还讨论了针对此类攻击可能采取的安全措施。代码可在https://github.com/Keio-CSG/slamspoof获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for enabling modern full self-drivingservices. These services heavily rely on map-based traffic information toreduce uncertainties in recognizing lane shapes, traffic light locations, andtraffic signs. Achieving this level of reliance on map information requirescentimeter-level localization accuracy, which is currently only achievable withLiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacksthat emit malicious lasers against LiDAR to overwrite its measurements. Oncelocalization is compromised, the attack could lead the victim off roads or makethem ignore traffic lights. Motivated by these serious safety implications, wedesign SLAMSpoof, the first practical LiDAR spoofing attack on localizationsystems for self-driving to assess the actual attack significance on autonomousvehicles. SLAMSpoof can effectively find the effective attack location based onour scan matching vulnerability score (SMVS), a point-wise metric representingthe potential vulnerability to spoofing attacks. To evaluate the effectivenessof the attack, we conduct real-world experiments on ground vehicles and confirmits high capability in real-world scenarios, inducing position errors of$\geq$4.2 meters (more than typical lane width) for all 3 popular LiDAR-basedlocalization algorithms. We finally discuss the potential countermeasures ofthis attack. Code is available at https://github.com/Keio-CSG/slamspoof</description>
      <author>example@mail.com (Rokuto Nagata, Kenji Koide, Yuki Hayakawa, Ryo Suzuki, Kazuma Ikeda, Ozora Sako, Qi Alfred Chen, Takami Sato, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2502.13641v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.13569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于遗传算法的模型进化框架（MEGA），旨在通过调整模型结构以适应不同任务难度，从而提升多任务强化学习中单一策略的泛化能力和效率。&lt;h4&gt;背景&lt;/h4&gt;在多任务强化学习中，单个策略用于完成多个任务，并通过参数共享来提高代理的学习效率。现有的方法通常使用路由网络为每个任务生成特定路径，并将一组模块重组以同时完成多种任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够根据任务难度自动调整模型结构的框架，提升多任务强化学习中的资源分配和学习效率。&lt;h4&gt;方法&lt;/h4&gt;引入了基于遗传算法的模型进化框架（MEGA），该框架允许在训练过程中根据任务难度动态调整模型。具体而言，采用二进制序列作为基因型策略进行模型重建，并使用非梯度遗传算法优化这些基因型策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的MEGA框架通过自适应地添加模块和动态调整结构，在机器人抓取等多任务场景中实现了优越的性能表现。&lt;h4&gt;结论&lt;/h4&gt;论文证明了基于遗传算法的模型进化方法的有效性，并计划向公众开放源代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了一种新的研究方法，该方法通过采用遗传算法优化策略来改进强化学习中的多任务处理能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-task reinforcement learning employs a single policy to complete varioustasks, aiming to develop an agent with generalizability across differentscenarios. Given the shared characteristics of tasks, the agent's learningefficiency can be enhanced through parameter sharing. Existing approachestypically use a routing network to generate specific routes for each task andreconstruct a set of modules into diverse models to complete multiple taskssimultaneously. However, due to the inherent difference between tasks, it iscrucial to allocate resources based on task difficulty, which is constrained bythe model's structure. To this end, we propose a Model Evolution framework withGenetic Algorithm (MEGA), which enables the model to evolve during trainingaccording to the difficulty of the tasks. When the current model isinsufficient for certain tasks, the framework will automatically incorporateadditional modules, enhancing the model's capabilities. Moreover, to adapt toour model evolution framework, we introduce a genotype module-level model,using binary sequences as genotype policies for model reconstruction, whileleveraging a non-gradient genetic algorithm to optimize these genotypepolicies. Unlike routing networks with fixed output dimensions, our approachallows for the dynamic adjustment of the genotype policy length, enabling it toaccommodate models with a varying number of modules. We conducted experimentson various robotics manipulation tasks in the Meta-World benchmark. Ourstate-of-the-art performance demonstrated the effectiveness of the MEGAframework. We will release our source code to the public.</description>
      <author>example@mail.com (Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li)</author>
      <guid isPermaLink="false">2502.13569v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>MILE: Model-based Intervention Learning</title>
      <link>http://arxiv.org/abs/2502.13519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Conference on Robotics and Automation (ICRA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种模型，能够从少量专家干预中学习策略，并展示了如何在模拟环境和实际机器人任务中应用此方法。&lt;h4&gt;背景&lt;/h4&gt;模仿学习技术在现实世界的控制场景（例如机器人）中非常有效，但这些方法容易出现累积误差问题，且需要人类专家提供完整的轨迹数据。现有互动方法仅利用干预期间的数据，忽视了非干预时间步长中的反馈信号。&lt;h4&gt;目的&lt;/h4&gt;创建一种模型来描述在这种情况下如何发生干预，并展示可以通过少量的专家干预学习策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一个能够从专家反馈中获取关于当前状态质量和所选动作最优性的关键信息的方法，无论是否有干预都适用。该方法在离散和连续模拟环境、实际机器人操作任务以及人类主题研究中进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;可以通过少量的专家干预学习出有效策略，并且可以利用非干预时间段中的反馈信号进行更有效的学习。&lt;h4&gt;结论&lt;/h4&gt;新的模型能够通过较少的数据实现更高效的模仿学习，为现实世界控制场景提供了改进的方法。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning techniques have been shown to be highly effective inreal-world control scenarios, such as robotics. However, these approaches notonly suffer from compounding error issues but also require human experts toprovide complete trajectories. Although there exist interactive methods wherean expert oversees the robot and intervenes if needed, these extensions usuallyonly utilize the data collected during intervention periods and ignore thefeedback signal hidden in non-intervention timesteps. In this work, we create amodel to formulate how the interventions occur in such cases, and show that itis possible to learn a policy with just a handful of expert interventions. Ourkey insight is that it is possible to get crucial information about the qualityof the current state and the optimality of the chosen action from expertfeedback, regardless of the presence or the absence of intervention. Weevaluate our method on various discrete and continuous simulation environments,a real-world robotic manipulation task, as well as a human subject study.Videos and the code can be found at https://liralab.usc.edu/mile .</description>
      <author>example@mail.com (Yigit Korkmaz, Erdem Bıyık)</author>
      <guid isPermaLink="false">2502.13519v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation</title>
      <link>http://arxiv.org/abs/2502.13508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a conference paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了VLAS，一种将语音识别直接集成到机器人策略模型中的新型端到端vision-language-action (VLA) 模型。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型主要依赖于仅支持文本指令的视觉-语言模型(VLM)，忽略了更适合人机交互的自然语音模式。传统的语音融合方法通常涉及一个独立的语音识别系统，这会增加复杂性并引入错误传播。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的问题，提出了一种新的端到端VLA模型（VLAS），它直接将语音识别集成到机器人策略模型中。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了VLAS，该模型可以直接理解口语指令并通过内部语音-文本对齐来生成相应的动作。2. 创建了两个新数据集SQA和CSI，支持针对语音命令的三阶段微调过程，使VLAS能够跨文本、图像、语音和机器人动作进行多模式交互。3. 设计了一个基于检索增强生成（RAG）的方法，让模型可以有效处理需要特定知识的任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，VLAS能够在广泛的口语命令下有效地完成机器人操作任务，并提供无缝且个性化的交互体验。&lt;h4&gt;结论&lt;/h4&gt;提出了一种全新的VLA框架，直接在机器人策略中集成语音识别功能，显著提高了人机对话的自然性和效率。该模型能够处理多样化的语音指令并成功执行任务，开启了未来机器人和人类更紧密合作的可能性。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language-action 模型（VLAs）由于其端到端的设计和卓越性能，在机器人操作中越来越受欢迎。然而，现有的VLAs主要依赖于仅支持文本指令的视觉-语言模型（VLMs），忽略了更适合人机交互的自然语音模式。传统的语音融合方法通常涉及一个独立的语音识别系统，这会增加复杂性并引入错误传播。此外，转录过程可能会丢失原始语音中的非语义信息，例如声纹，在机器人完成定制任务时这些信息可能至关重要。为了解决上述挑战，我们提出了VLAS，一种新型端到端VLA模型，它直接将语音识别集成到机器人策略模型中。VLAS允许机器人通过内部的语音-文本对齐来理解口头命令，并生成相应动作以执行任务。我们还介绍了两个新数据集SQA和CSI，支持针对语音指令的三阶段微调过程，使VLAS能够跨文本、图像、语音和机器人操作进行多模式交互。更进一步地，设计了一种基于检索增强生成（RAG）的方法来使我们的模型可以有效处理需要特定知识的任务。广泛的实验表明，VLAS能够使用多样化的语音命令有效地完成机器人操作任务，并提供无缝且个性化的交互体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action models (VLAs) have become increasingly popular inrobot manipulation for their end-to-end design and remarkable performance.However, existing VLAs rely heavily on vision-language models (VLMs) that onlysupport text-based instructions, neglecting the more natural speech modalityfor human-robot interaction. Traditional speech integration methods usuallyinvolves a separate speech recognition system, which complicates the model andintroduces error propagation. Moreover, the transcription procedure would losenon-semantic information in the raw speech, such as voiceprint, which may becrucial for robots to successfully complete customized tasks. To overcome abovechallenges, we propose VLAS, a novel end-to-end VLA that integrates speechrecognition directly into the robot policy model. VLAS allows the robot tounderstand spoken commands through inner speech-text alignment and producescorresponding actions to fulfill the task. We also present two new datasets,SQA and CSI, to support a three-stage tuning process for speech instructions,which empowers VLAS with the ability of multimodal interaction across text,image, speech, and robot actions. Taking a step further, a voiceretrieval-augmented generation (RAG) paradigm is designed to enable our modelto effectively handle tasks that require individual-specific knowledge. Ourextensive experiments show that VLAS can effectively accomplish robotmanipulation tasks with diverse speech commands, offering a seamless andcustomized interaction experience.</description>
      <author>example@mail.com (Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang)</author>
      <guid isPermaLink="false">2502.13508v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Improving Collision-Free Success Rate For Object Goal Visual Navigation Via Two-Stage Training With Collision Prediction</title>
      <link>http://arxiv.org/abs/2502.13498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了基于深度强化学习的端到端导航模型在目标物体导向视觉导航中的碰撞问题，并提出了一种两阶段训练方法来提高现有RGB观测下的导航模型的无碰撞成功率。&lt;h4&gt;背景&lt;/h4&gt;现有的深度强化学习导航模型虽然能较好地发现和到达目标对象，但在导航过程中的碰撞问题仍未解决。碰撞通常被忽略不计，在评估成功时不会对其施加负反馈，导致模型过于保守且难以有效避开障碍物。&lt;h4&gt;目的&lt;/h4&gt;引入无碰撞成功率的概念来衡量导航模型找到通往目标物体的无碰撞路径的能力，并提出一种新的两阶段训练方法以改善现有RGB观测下的导航模型的性能。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段训练策略：在第一阶段，通过监督代理人在探索过程中的碰撞状态学习预测可能发生的碰撞；在第二阶段，利用训练好的碰撞预测模块引导代理人学会在没有碰撞的情况下到达目标物体。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的两阶段训练方法能够显著提高现有导航模型的无碰撞成功率，并优于其他类似的方法。&lt;h4&gt;结论&lt;/h4&gt;新方法提高了基于深度强化学习的目标导向视觉导航系统的性能，在实际应用中可能具有更高的稳定性和安全性。&lt;h4&gt;翻译&lt;/h4&gt;目标导向视觉导航任务是通过第一人称视角的视觉观察来定位特定目标物体。本文针对该任务中的碰撞问题提出了解决方案，即利用两阶段训练策略提高现有模型的无碰撞成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The object goal visual navigation is the task of navigating to a specifictarget object using egocentric visual observations. Recent end-to-endnavigation models based on deep reinforcement learning have achieved remarkableperformance in finding and reaching target objects. However, the collisionproblem of these models during navigation remains unresolved, since thecollision is typically neglected when evaluating the success. Althoughincorporating a negative reward for collision during training appearsstraightforward, it results in a more conservative policy, thereby limiting theagent's ability to reach targets. In addition, many of these models utilizeonly RGB observations, further increasing the difficulty of collision avoidancewithout depth information. To address these limitations, a new concept --collision-free success is introduced to evaluate the ability of navigationmodels to find a collision-free path towards the target object. A two-stagetraining method with collision prediction is proposed to improve thecollision-free success rate of the existing navigation models using RGBobservations. In the first training stage, the collision prediction modulesupervises the agent's collision states during exploration to learn to predictthe possible collision. In the second stage, leveraging the trained collisionprediction, the agent learns to navigate to the target without collision. Theexperimental results in the AI2-THOR environment demonstrate that the proposedmethod greatly improves the collision-free success rate of different navigationmodels and outperforms other comparable collision-avoidance methods.</description>
      <author>example@mail.com (Shiwei Lian, Feitian Zhang)</author>
      <guid isPermaLink="false">2502.13498v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Ephemerality meets LiDAR-based Lifelong Mapping</title>
      <link>http://arxiv.org/abs/2502.13452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6+2 pages, 11 figures, accepted at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ELite是一种基于LiDAR的终生地图构建框架，能够无缝地对多时段数据进行校准、移除动态物体并更新地图。&lt;h4&gt;背景&lt;/h4&gt;长期部署机器人在动态环境中的关键在于终身制图能力。常规的地图元素分类为静态或动态不能完全满足需求，例如停车的车辆需要更详细的类别划分。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够准确区分瞬时与持续性地图元素，并维护可靠、更新及时静态地图的方法。&lt;h4&gt;方法&lt;/h4&gt;通过将世界建模为两阶段$extit{ephemerality}$（瞬时性）的概率模型，该模型在两个不同的时间尺度内表示映射点的易逝性。利用瞬时性所编码的空间和时间上下文信息，ELite能够更精细地校准新数据并提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;ELite框架可以准确推断出瞬时地图元素，并维持可靠的更新静态地图。&lt;h4&gt;结论&lt;/h4&gt;该系统的有效性和鲁棒性已在长期的现实世界实验中得到了验证。开源代码可供机器人社区使用。&lt;h4&gt;翻译&lt;/h4&gt;终身制图对于在动态环境中长时间部署机器人至关重要。本文提出了ELite，一种基于LiDAR的终生映射框架，能够无缝地对多时段数据进行校准、移除动态物体并更新地图。地图元素通常被分类为静态或动态，但像停放车辆的情况表明需要比二元更详细类别的划分。我们的方法的核心是将世界建模为两阶段$extit{ephemerality}$的概率模型，它表示了映射点在两个不同时间尺度内的易逝性。通过利用瞬时性所编码的空间和时间上下文信息，ELite能够准确推断出瞬时地图元素，并维持可靠的更新静态地图，并且以更精细的方式提高新数据校准的鲁棒性。广泛的现实世界长期实验展示了我们系统的有效性和鲁棒性。开源代码可供机器人社区使用：https://github.com/dongjae0107/ELite&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lifelong mapping is crucial for the long-term deployment of robots in dynamicenvironments. In this paper, we present ELite, an ephemerality-aidedLiDAR-based lifelong mapping framework which can seamlessly align multiplesession data, remove dynamic objects, and update maps in an end-to-end fashion.Map elements are typically classified as static or dynamic, but cases likeparked cars indicate the need for more detailed categories than binary. Centralto our approach is the probabilistic modeling of the world into two-stage$\textit{ephemerality}$, which represent the transiency of points in the mapwithin two different time scales. By leveraging the spatiotemporal contextencoded in ephemeralities, ELite can accurately infer transient map elements,maintain a reliable up-to-date static map, and improve robustness in aligningthe new data in a more fine-grained manner. Extensive real-world experiments onlong-term datasets demonstrate the robustness and effectiveness of our system.The source code is publicly available for the robotics community:https://github.com/dongjae0107/ELite.</description>
      <author>example@mail.com (Hyeonjae Gil, Dongjae Lee, Giseop Kim, Ayoung Kim)</author>
      <guid isPermaLink="false">2502.13452v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.13451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为MapNav的新颖端到端视觉和语言导航模型，该模型在构建Annotated Semantic Map（ASM）时利用了语义地图来替代历史帧，并通过文本标签增强了结构化的导航信息。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉和语言导航方法依赖于大量的时空上下文存储以进行决策，导致显著的存储与计算开销。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型MapNav，旨在减轻传统方法中对大量历史观测数据的需求并改进性能。&lt;h4&gt;方法&lt;/h4&gt;在每次任务开始时构建一个基于顶视图的语义地图，并且随着每一步的时间步更新该地图。通过文本标签为关键区域添加明确的导航提示来生成ASM。利用VLM的强大端到端能力进行导航。&lt;h4&gt;主要发现&lt;/h4&gt;MapNav模型在模拟和真实世界环境中均表现出色，达到了最先进的性能水平。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效减少存储与计算资源需求，并且通过开放源代码和数据集进一步促进了研究的可重复性。&lt;h4&gt;贡献&lt;/h4&gt;为视觉语言导航提供了一种新的记忆表示方法，有望推动该领域未来的研究发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-language navigation (VLN) is a key task in Embodied AI, requiringagents to navigate diverse and unseen environments while following naturallanguage instructions. Traditional approaches rely heavily on historicalobservations as spatio-temporal contexts for decision making, leading tosignificant storage and computational overhead. In this paper, we introduceMapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map(ASM) to replace historical frames. Specifically, our approach constructs atop-down semantic map at the start of each episode and update it at eachtimestep, allowing for precise object mapping and structured navigationinformation. Then, we enhance this map with explicit textual labels for keyregions, transforming abstract semantics into clear navigation cues andgenerate our ASM. MapNav agent using the constructed ASM as input, and use thepowerful end-to-end capabilities of VLM to empower VLN. Extensive experimentsdemonstrate that MapNav achieves state-of-the-art (SOTA) performance in bothsimulated and real-world environments, validating the effectiveness of ourmethod. Moreover, we will release our ASM generation source code and dataset toensure reproducibility, contributing valuable resources to the field. Webelieve that our proposed MapNav can be used as a new memory representationmethod in VLN, paving the way for future research in this field.</description>
      <author>example@mail.com (Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, Renjing Xu)</author>
      <guid isPermaLink="false">2502.13451v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Aware Robotic Palletization with Online Masking Inference</title>
      <link>http://arxiv.org/abs/2502.13443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种基于强化学习的方法来解决在线环境下的箱子堆放问题，这种方法考虑了盒子的物理属性，并在实际应用中证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;现代仓储和物流管理中的物品堆叠计划是一个重要挑战。现有解决方案通常处理不同尺寸的盒子，但忽略它们的密度、刚度等内在和物理特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用强化学习并结合动作空间屏蔽的新方法来解决箱子堆放问题，使其更加适应实际场景的需求。&lt;h4&gt;方法&lt;/h4&gt;使用强化学习并通过动作空间掩码机制动态训练有效动作，不需要人为设计启发式规则。这种方法能够在线学习盒子的物理特性，并据此规划堆叠策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了新提出的方法在性能上超越现有的最先进算法；并且将该方法部署到一个实际应用中的机器人托盘装载机中进行测试。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于强化学习的方法有效地解决了在线环境下的箱子堆放问题，并展示了其在真实世界操作设置中的实用性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The efficient planning of stacking boxes, especially in the online settingwhere the sequence of item arrivals is unpredictable, remains a criticalchallenge in modern warehouse and logistics management. Existing solutionsoften address box size variations, but overlook their intrinsic and physicalproperties, such as density and rigidity, which are crucial for real-worldapplications. We use reinforcement learning (RL) to solve this problem byemploying action space masking to direct the RL policy toward valid actions.Unlike previous methods that rely on heuristic stability assessments which aredifficult to assess in physical scenarios, our framework utilizes onlinelearning to dynamically train the action space mask, eliminating the need formanual heuristic design. Extensive experiments demonstrate that our proposedmethod outperforms existing state-of-the-arts. Furthermore, we deploy ourlearned task planner in a real-world robotic palletizer, validating itspractical applicability in operational settings.</description>
      <author>example@mail.com (Tianqi Zhang, Zheng Wu, Yuxin Chen, Yixiao Wang, Boyuan Liang, Scott Moura, Masayoshi Tomizuka, Mingyu Ding, Wei Zhan)</author>
      <guid isPermaLink="false">2502.13443v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Generative Predictive Control: Flow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks</title>
      <link>http://arxiv.org/abs/2502.13406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种新的机器人控制策略——生成式预测控制，旨在解决现有的基于行为克隆的生成政策方法在获取专家演示数据上的时间和成本问题以及处理快速动态任务时的局限性。&lt;h4&gt;背景&lt;/h4&gt;近年来，通过扩散或流匹配产生的行动序列的方法为机器人技术带来了重大进展。然而，这些方法存在两个主要限制：需要专家提供的大量时间与资金以获取高质量的演示数据，并且现有方法仅限于处理相对缓慢、准静态的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决当前生成策略局限性的新框架——生成式预测控制（Generative Predictive Control），用于处理快速动态任务，这类任务虽然难以展示但易于模拟。&lt;h4&gt;方法&lt;/h4&gt;利用采样基础的预测控制与生成模型之间的紧密联系。该论文介绍了如何在运行时通过流匹配政策进行热启动来保持时间一致性并实现快速反馈速率。&lt;h4&gt;主要发现&lt;/h4&gt;生成式预测控制能够提供一种不同于现有行为克隆方法的新途径，并有望为超越准静态任务领域的通才策略铺平道路。&lt;h4&gt;结论&lt;/h4&gt;该论文提出了一种新的机器人控制框架，通过结合采样基础的预测控制和生成模型的方法来处理快速动态的任务。这种方法在理论上解决了基于演示数据的传统生成式政策的关键限制，从而开辟了未来研究的新方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：生成性控制策略最近为机器人技术带来了重大突破。这些方法通过扩散或流匹配产生行动序列，并利用演示数据进行训练。然而，在困难的操作问题上虽然取得了相当大的成功，但这种策略却存在两个主要限制：首先，行为克隆需要专家示范，获取起来耗时且昂贵；其次，现有方法仅限于相对缓慢、准静态的任务。在本文中，我们借助采样基础的预测控制与生成建模之间的紧密联系来解决这些问题。具体来说，我们引入了一种用于具有快速动态特性的任务的新框架——生成式预测控制（Generative Predictive Control），这些任务易于模拟但难以演示。接着展示了如何在运行时通过训练好的流匹配政策进行热启动以保持时间一致性并实现高速反馈率。我们认为，生成式预测控制为现有的行为克隆方法提供了一种互补的方法，并希望它能开启超越准静态展示导向型任务的通才策略之路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative control policies have recently unlocked major progress inrobotics. These methods produce action sequences via diffusion or flowmatching, with training data provided by demonstrations. But despite enjoyingconsiderable success on difficult manipulation problems, generative policiescome with two key limitations. First, behavior cloning requires expertdemonstrations, which can be time-consuming and expensive to obtain. Second,existing methods are limited to relatively slow, quasi-static tasks. In thispaper, we leverage a tight connection between sampling-based predictive controland generative modeling to address each of these issues. In particular, weintroduce generative predictive control, a supervised learning framework fortasks with fast dynamics that are easy to simulate but difficult todemonstrate. We then show how trained flow-matching policies can bewarm-started at run-time, maintaining temporal consistency and enabling fastfeedback rates. We believe that generative predictive control offers acomplementary approach to existing behavior cloning methods, and hope that itpaves the way toward generalist policies that extend beyond quasi-staticdemonstration-oriented tasks.</description>
      <author>example@mail.com (Vince Kurtz, Joel W. Burdick)</author>
      <guid isPermaLink="false">2502.13406v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Object-Pose Estimation With Neural Population Codes</title>
      <link>http://arxiv.org/abs/2502.13403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了机器人装配任务中物体姿态估计的问题，特别是对于避免昂贵机械约束的任务。提出了使用神经群体编码表示对象旋转的方法，克服了现有方法计算量大的问题。&lt;h4&gt;背景&lt;/h4&gt;在进行机器人的装配作业时，准确地对齐物体的姿势是必要的，特别是在需要减少机械限制成本的情况下。然而，当面对具有镜像或旋转对称性的物体时，这种对齐过程变得复杂且难以直接将感官输入映射到对象旋转上。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过引入神经网络群体编码来实现更为高效准确的物体姿态估计方法，并避免传统解决方法中大量计算开销的问题。&lt;h4&gt;方法&lt;/h4&gt;利用神经网络群体编码表示物体旋转，这使得可以直接对齐输入图像与目标旋转角度，从而能够端到端地训练模型。该方法通过概率分布预测多个假设的可能性来克服先前提出的解决方案所面临的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在T-LESS数据集上采用灰度图像作为唯一输入的情况下，使用群体编码的模型能够在Apple M1 CPU上实现3.2毫秒内的推断速度，并达到84.7%的最大对称性感知表面距离精度。相比之下，直接映射到姿态的方法仅能达到69.7%的准确性。&lt;h4&gt;结论&lt;/h4&gt;神经网络群体编码为物体旋转提供了有效的表示方法，从而使得机器人装配任务中的对象姿态估计变得更快更准确。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经进行了中文翻译，并且上述所有项目均已根据原始英文摘要的内容进行填充。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic assembly tasks require object-pose estimation, particularly for tasksthat avoid costly mechanical constraints. Object symmetry complicates thedirect mapping of sensory input to object rotation, as the rotation becomesambiguous and lacks a unique training target. Some proposed solutions involveevaluating multiple pose hypotheses against the input or predicting aprobability distribution, but these approaches suffer from significantcomputational overhead. Here, we show that representing object rotation with aneural population code overcomes these limitations, enabling a direct mappingto rotation and end-to-end learning. As a result, population codes facilitatefast and accurate pose estimation. On the T-LESS dataset, we achieve inferencein 3.2 milliseconds on an Apple M1 CPU and a Maximum Symmetry-Aware SurfaceDistance accuracy of 84.7% using only gray-scale image input, compared to 69.7%accuracy when directly mapping to pose.</description>
      <author>example@mail.com (Heiko Hoffmann, Richard Hoffmann)</author>
      <guid isPermaLink="false">2502.13403v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Reflection of Episodes: Learning to Play Game from Expert and Self Experiences</title>
      <link>http://arxiv.org/abs/2502.13388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StarCraft II是一款复杂且动态的即时战略游戏环境，适用于人工智能和强化学习研究。为解决大型语言模型在复杂环境中通过自我反思进行学习的问题，提出了一种基于专家经验和自我经验的反射事件（ROE）框架。&lt;h4&gt;背景&lt;/h4&gt;StarCraft II是一个高度复杂的实时策略游戏环境，非常适合用于人工智能和强化学习的研究领域。&lt;h4&gt;目的&lt;/h4&gt;为了应对大型语言模型在复杂环境下通过自我反思来学习这一挑战，开发了一种新的框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于专家经验和自我经验的反射事件(ROE)框架。该框架首先通过关键帧选择法获取游戏中的关键信息，然后根据这些信息做出决策；每完成一局游戏后，进行回顾以获得新的自我经验。&lt;h4&gt;主要发现&lt;/h4&gt;实验中，我们的方法在TextStarCraft II的非常难难度下击败了机器人。&lt;h4&gt;结论&lt;/h4&gt;详细分析了大型语言模型在游戏中各个阶段的数据，验证了该方法的有效性&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已翻译为中文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; StarCraft II is a complex and dynamic real-time strategy (RTS) gameenvironment, which is very suitable for artificial intelligence andreinforcement learning research. To address the problem of Large LanguageModel(LLM) learning in complex environments through self-reflection, we proposea Reflection of Episodes(ROE) framework based on expert experience andself-experience. This framework first obtains key information in the gamethrough a keyframe selection method, then makes decisions based on expertexperience and self-experience. After a game is completed, it reflects on theprevious experience to obtain new self-experience. Finally, in the experiment,our method beat the robot under the Very Hard difficulty in TextStarCraft II.We analyze the data of the LLM in the process of the game in detail, verifiedits effectiveness.</description>
      <author>example@mail.com (Xiaojie Xu, Zongyuan Li, Chang Lu, Runnan Qi, Yanan Ni, Lumin Jiang, Xiangbei Liu, Xuebo Zhang, Yongchun Fang, Kuihua Huang, Xian Guo, Zhanghua Wu, Zhenya Li)</author>
      <guid isPermaLink="false">2502.13388v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Low-Complexity Cooperative Payload Transportation for Nonholonomic Mobile Robots Under Scalable Constraints</title>
      <link>http://arxiv.org/abs/2502.13366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;合作运输是物流网络中的一个关键环节，在这种情况下，通常使用分布式控制和基于优化的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的非完整移动机器人的合作运输方法来克服传统方法的缺点，该方法能够有效处理约束且具有可扩展性。&lt;h4&gt;方法&lt;/h4&gt;改进了传统的编队控制方式，提出了一个新型的合作运输方法。这个方法分为两个部分：机器人轨迹生成和轨迹跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的基于控制的方法不仅容易扩展到多个约束条件上，还降低了优化方法的时间复杂度，从多项式降至线性。&lt;h4&gt;结论&lt;/h4&gt;通过仿真实验验证了所提合作运输方法的可行性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种改进传统编队控制的合作运输方法的应用于非完整移动机器人领域。该方法具有分布式特性，时间复杂度低，并能适应可扩展约束条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative transportation, a key aspect of logistics  cyber-physical systems (CPS), is typically approached using dis tributedcontrol and optimization-based methods. The distributed  control methods consume less time, but poorly handle and extend  to multiple constraints. Instead, optimization-based methods  handle constraints effectively, but they are usually centralized,  time-consuming and thus not easily scalable to numerous robots.  To overcome drawbacks of both, we propose a novel cooperative  transportation method for nonholonomic mobile robots by im provingconventional formation control, which is distributed, has  a low time-complexity and accommodates scalable constraints.  The proposed control-based method is testified on a cable suspended payloadand divided into two parts, including robot  trajectory generation and trajectory tracking. Unlike most time consumingtrajectory generation methods, ours can generate  trajectories with only constant time-complexity, needless of global  maps. As for trajectory tracking, our control-based method not  only scales easily to multiple constraints as those optimization basedmethods, but reduces their time-complexity from poly nomial to linear.Simulations and experiments can verify the  feasibility of our method.</description>
      <author>example@mail.com (Renhe Guan, Yuanzhe Wang, Tao Liu, Yan Wang)</author>
      <guid isPermaLink="false">2502.13366v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>BoundPlanner: A convex-set-based approach to bounded manipulator trajectory planning</title>
      <link>http://arxiv.org/abs/2502.13286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于凸集的在线轨迹规划框架，包括新的笛卡尔路径规划器BoundPlanner和扩展后的在线轨迹规划器BoundMPC。该框架能够在复杂环境中快速为机器人制定合适的轨迹，并考虑机器人的动力学限制和碰撞问题。&lt;h4&gt;背景&lt;/h4&gt;现有的许多机器人轨迹规划方法虽然适用于已知环境，但在实时计算方面效率较低，无法在具有挑战性的场景中找到满足机器人约束条件的路径。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的在线轨迹规划框架，能够为机器人制定合适的、符合其物理限制且考虑碰撞问题的路径。&lt;h4&gt;方法&lt;/h4&gt;{'BoundPlanner': '利用凸集探索和映射无障碍空间，并计算出具有边界的参考路径。', 'BoundMPC': '扩展了模型预测控制算法以处理路径偏移时的凸集合，使机器人能够在边界内最优地跟随路径并考虑其动力学特性。', '碰撞避免方法': '开发了一种独立于障碍物数量的新颖凸集基碰撞规避公式，来应对机器人连杆之间的碰撞问题。'}&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和实验中验证了所提规划器的性能优于现有先进技术，并且该框架适用于具有7个自由度的机械臂。&lt;h4&gt;结论&lt;/h4&gt;新提出的轨迹规划方法能够有效解决在线环境中的挑战性问题，为机器人提供了快速反应能力和路径优化方案。源代码可在GitHub上获得，实验视频可通过提供的网站链接观看。&lt;h4&gt;翻译&lt;/h4&gt;在线轨迹规划使机器人能够迅速应对变化的工作环境或任务需求。许多现有的机器人轨迹规划器仅适用于已知环境，并且通常无法满足实时计算的要求。目前的方法在处理具有挑战性的场景时不能找到符合机器人类型限制并考虑碰撞的合适路径。本文提出了一种新的轨迹规划框架，包括基于凸集的新笛卡尔路径规划器BoundPlanner和在线轨迹规划器BoundMPC（扩展版）。BoundPlanner利用凸集合探索无障碍空间，并计算出具有边界的参考路径。BoundMPC被扩展来处理路径偏差时的凸集合问题，使机器人能够在其边界内最优地跟随路径同时考虑机器人的动力学特性。我们提出了一种新的基于凸集的碰撞规避公式独立于障碍物的数量，从而考虑到机器人连杆之间的碰撞问题。通过与最先进的方法进行比较，在一个具有7个自由度机械臂上的仿真和实验展示了所提出的规划器的表现能力。源代码可在GitHub（github.com/Thieso/BoundPlanner）上获得，并且在提供的网站链接www.acin.tuwien.ac.at/42d4可找到实验视频。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online trajectory planning enables robot manipulators to react quickly tochanging environments or tasks. Many robot trajectory planners exist for knownenvironments but are often too slow for online computations. Current methods inonline trajectory planning do not find suitable trajectories in challengingscenarios that respect the limits of the robot and account for collisions. Thiswork proposes a trajectory planning framework consisting of the novel Cartesianpath planner based on convex sets, called BoundPlanner, and the onlinetrajectory planner BoundMPC. BoundPlanner explores and maps the collision-freespace using convex sets to compute a reference path with bounds. BoundMPC isextended in this work to handle convex sets for path deviations, which allowsthe robot to optimally follow the path within the bounds while accounting forthe robot's kinematics. Collisions of the robot's kinematic chain areconsidered by a novel convex-set-based collision avoidance formulationindependent on the number of obstacles. Simulations and experiments with a7-DoF manipulator show the performance of the proposed planner compared tostate-of-the-art methods. The source code is available atgithub.com/Thieso/BoundPlanner and videos of the experiments can be found atwww.acin.tuwien.ac.at/42d4</description>
      <author>example@mail.com (Thies Oelerich, Christian Hartl-Nesic, Florian Beck, Andreas Kugi)</author>
      <guid isPermaLink="false">2502.13286v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>PCB Renewal: Iterative Reuse of PCB Substrates for Sustainable Electronic Making</title>
      <link>http://arxiv.org/abs/2502.13255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了PCB Renewal技术，该技术通过在旧PCB上选择性地沉积导电环氧树脂来擦除和重新配置电路路径，从而减少电子制造过程中的材料浪费。&lt;h4&gt;背景&lt;/h4&gt;印刷电路板（PCB）基材通常是一次性的，这导致了电子产品的材料浪费问题。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的技术——PCB Renewal，该技术能够通过沉积导电环氧树脂将旧的PCB进行改造和再利用。&lt;h4&gt;方法&lt;/h4&gt;展示了PCB Renewal的工作流程，并对其电气性能、机械耐久性进行了评估。还建立了一个软件插件来指导环氧树脂的沉积过程以及生成更新后的PCB配置文件。&lt;h4&gt;主要发现&lt;/h4&gt;通过四个设计迭代实例（包括相机滚轮、WiFi电台和ESPboy游戏控制台）展示了技术的有效性和多功能性，同时还展示了一块外包生产的双层PCB从LED手表转换为互动猫玩具的过程。&lt;h4&gt;结论&lt;/h4&gt;文章总结了该方法的局限性，并探讨了未来的研究方向。这项技术在减少材料浪费方面具有巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：PCB（印刷电路板）基材通常是一次性的，导致电子制造中的材料浪费问题。我们介绍了PCB Renewal这一新技术，它通过选择性沉积导电环氧树脂来擦除和重新配置PCB迹线，将孤立路径转变为支持新迹线的导电平面。文章展示了PCB Renewal的工作流程，并对其电气性能、机械耐久性和可持续性影响进行了评估，包括材料使用情况、成本、能源消耗和时间节省等方面。还开发了一个软件插件来指导环氧树脂沉积过程、生成更新后的PCB配置文件以及计算资源使用情况。为了证明该技术的有效性和多功能性，我们在三个项目中进行了一块PCB的四个设计迭代：相机滚轮、WiFi电台和ESPboy游戏控制台；此外，还展示了如何将一个外包生产的双层PCB从LED手表重新配置为互动猫玩具的过程。文章最后总结了限制条件并探讨了未来的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3706598.3714276&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PCB (printed circuit board) substrates are often single-use, leading tomaterial waste in electronics making. We introduce PCB Renewal, a noveltechnique that "erases" and "reconfigures" PCB traces by selectively depositingconductive epoxy onto outdated areas, transforming isolated paths intoconductive planes that support new traces. We present the PCB Renewal workflow,evaluate its electrical performance and mechanical durability, and model itssustainability impact, including material usage, cost, energy consumption, andtime savings. We develop a software plug-in that guides epoxy deposition,generates updated PCB profiles, and calculates resource usage. To demonstratePCB Renewal's effectiveness and versatility, we repurpose a single PCB acrossfour design iterations spanning three projects: a camera roller, a WiFi radio,and an ESPboy game console. We also show how an outsourced double-layer PCB canbe reconfigured, transforming it from an LED watch to an interactive cat toy.The paper concludes with limitations and future directions.</description>
      <author>example@mail.com (Zeyu Yan, Advait Vartak, Jiasheng Li, Zining Zhang, Huaishu Peng)</author>
      <guid isPermaLink="false">2502.13255v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent Soft Matter: Towards Embodied Intelligence</title>
      <link>http://arxiv.org/abs/2502.13224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概述&lt;/h4&gt;智能软物质位于材料科学、物理学和认知科学的交汇点，旨在改变我们设计与使用材料的方式。&lt;h4&gt;背景&lt;/h4&gt;传统材料通常执行静态或预定义的功能，而智能软物质能够动态地与其环境相互作用。它结合了多种感官输入，保留经验并作出决策以优化其反应。&lt;h4&gt;目的&lt;/h4&gt;通过借鉴生物系统，这些材料旨在利用柔软物质的固有属性（如灵活性、自演化和响应性）来执行模仿认知过程的功能，并展望智能软物质如何被构建。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种前瞻性的视角，讨论了设计传感、记忆与动作之间的集成以及内部低功耗操作的新途径，并探讨了具有“智能行为”的材料在实际应用中的挑战。&lt;h4&gt;结论&lt;/h4&gt;这些方法勾勒出一条通往更具鲁棒性、多功能性和可扩展性材料的道路，这种材料能够通过其固有的内在物质行为来行动、计算和“思考”，超越传统的需要外部控制的智能技术。&lt;h4&gt;翻译&lt;/h4&gt;智能软物质处于材料科学、物理学及认知科学的交汇点，承诺将改变我们设计与互动使用材料的方式。这一变革性的领域旨在创造具备生命特征能力（如感知、学习、记忆和适应性行为）的材料。与传统材料通常执行静态或预定义功能不同，智能软物质能与其环境动态交互。它融合了多种感官输入，保留经验并作出决策以优化其响应。受生物系统的启发，这些材料旨在利用柔软物质的固有属性（如灵活性、自演化及响应性）来实现模仿认知过程的功能。通过整合当前研究趋势和展望未来的发展方向，本文提供了一种前瞻性观点，即智能软物质如何构建，并意在激发包括生物医学设备和适应性机器人在内的领域的创新。文中还强调了将传感设计、记忆与动作的集成以及内部低功耗操作的新途径，并讨论了具有'智能行为'材料的实际应用挑战。这些方法描绘出了一条通往更为鲁棒、多功能及可扩展材料的道路，这种材料能够通过其内在固有的物质特性来行动、计算并“思考”，超越依赖外部控制的传统智能技术的范畴。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent soft matter stands at the intersection of materials science,physics, and cognitive science, promising to change how we design and interactwith materials. This transformative field seeks to create materials thatpossess life-like capabilities, such as perception, learning, memory, andadaptive behavior. Unlike traditional materials, which typically perform staticor predefined functions, intelligent soft matter dynamically interacts with itsenvironment. It integrates multiple sensory inputs, retains experiences, andmakes decisions to optimize its responses. Inspired by biological systems,these materials intend to leverage the inherent properties of soft matter:flexibility, self-evolving, and responsiveness to perform functions that mimiccognitive processes. By synthesizing current research trends and projectingtheir evolution, we present a forward-looking perspective on how intelligentsoft matter could be constructed, with the aim of inspiring innovations infields such as biomedical devices, adaptive robotics, and beyond. We highlightnew pathways for integrating design of sensing, memory and action with internallow-power operations and discuss challenges for practical implementation ofmaterials with "intelligent behavior". These approaches outline a path towardsto more robust, versatile and scalable materials that can potentially act,compute, and "think" by their inherent intrinsic material behaviour beyondtraditional smart technologies relying on external control.</description>
      <author>example@mail.com (Vladimir A. Baulin, Achille Giacometti, Dmitry Fedosov, Stephen Ebbens, Nydia R. Varela-Rosales, Neus Feliu, Mithun Chowdhury, Minghan Hu, Rudolf Füchslin, Marjolein Dijkstra, Matan Mussel, René van Roij, Dong Xie, Vassil Tzanov, Mengjie Zu, Samuel Hidalgo-Caballero, Ye Yuan, Luca Cocconi, Cheol-Min Ghim, Cécile Cottin-Bizonne, M. Carmen Miguel, Maria Jose Esplandiu, Juliane Simmchen, Wolfgang J. Parak, Marco Werner, Gerhard Gompper, Martin M. Hanczyc)</author>
      <guid isPermaLink="false">2502.13224v1</guid>
      <pubDate>Thu, 20 Feb 2025 16:54:09 +0800</pubDate>
    </item>
    </channel>
</rss>