<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 17 Oct 2025 15:23:43 +0800</lastBuildDate>
    <item>
      <title>Programmatic Representation Learning with Language Models</title>
      <link>http://arxiv.org/abs/2510.14825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at https://github.com/gpoesia/leapr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种称为'学习程序化表示'(LeaPR)的模型，它结合了决策树和通过大型语言模型(LLMs)合成的特征函数，能够在不依赖神经网络的情况下实现高质量的预测，同时保持模型的可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统监督机器学习模型（如决策树）是高效且可解释的预测器，但其质量高度依赖于输入特征的选择。虽然神经网络可以直接从原始数据（如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。&lt;h4&gt;目的&lt;/h4&gt;探索一种新的模型类LeaPR，它将表示为代码（从数据点到标量的函数）的任意特征与决策树预测器堆叠，从而在保持可解释性的同时实现高质量的预测。&lt;h4&gt;方法&lt;/h4&gt;1. 使用大型语言模型(LLMs)合成特征函数，利用它们在广泛领域的丰富先验知识和使用现有领域特定库编写代码的能力；2. 提出两种算法从监督数据中学习LeaPR模型：设计了FunSearch的适配版本来学习特征而非直接生成预测器；开发了经典ID3算法的新变体用于决策树学习，在分割叶节点时按需生成新特征。&lt;h4&gt;主要发现&lt;/h4&gt;在从国际象棋位置评估到图像和文本分类的实验中，该方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;h4&gt;翻译&lt;/h4&gt;传统监督机器学习的经典模型，如决策树，是高效且可解释的预测器，但其质量高度依赖于特定输入特征的选择。虽然神经网络可以直接从原始数据（例如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。在本文中，我们探索了一个称为学习程序化表示的假设类，它将表示为代码的任意特征（从数据点到标量的函数）与决策树预测器堆叠。我们使用大型语言模型合成特征函数，这些模型在广泛领域拥有丰富的先验知识，并且使用现有领域特定库编写代码的能力令人瞩目。我们提出了两种算法从监督数据中学习LeaPR模型。首先，我们设计了FunSearch的适配版本来学习特征而非直接生成预测器。然后，我们开发了经典ID3算法用于决策树学习的新变体，其中在分割叶节点时按需生成新特征。从国际象棋位置评估到图像和文本分类的实验中，我们的方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。我们的研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical models for supervised machine learning, such as decision trees, areefficient and interpretable predictors, but their quality is highly dependenton the particular choice of input features. Although neural networks can learnuseful representations directly from raw data (e.g., images or text), thiscomes at the expense of interpretability and the need for specialized hardwareto run them efficiently. In this paper, we explore a hypothesis class we callLearned Programmatic Representations (LeaPR) models, which stack arbitraryfeatures represented as code (functions from data points to scalars) anddecision tree predictors. We synthesize feature functions using Large LanguageModels (LLMs), which have rich prior knowledge in a wide range of domains and aremarkable ability to write code using existing domain-specific libraries. Wepropose two algorithms to learn LeaPR models from supervised data. First, wedesign an adaptation of FunSearch to learn features rather than directlygenerate predictors. Then, we develop a novel variant of the classical ID3algorithm for decision tree learning, where new features are generated ondemand when splitting leaf nodes. In experiments from chess position evaluationto image and text classification, our methods learn high-quality, neuralnetwork-free predictors often competitive with neural networks. Our worksuggests a flexible paradigm for learning interpretable representationsend-to-end where features and predictions can be readily inspected andunderstood.</description>
      <author>example@mail.com (Gabriel Poesia, Georgia Gabriela Sampaio)</author>
      <guid isPermaLink="false">2510.14825v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
  <item>
      <title>Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning</title>
      <link>http://arxiv.org/abs/2510.14819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PRTraj是一种新颖的轨迹表示学习框架，通过统一环境感知和路线选择建模来有效学习轨迹表示，解决了现有方法将轨迹视为孤立时空序列的局限。&lt;h4&gt;背景&lt;/h4&gt;现有轨迹表示学习方法将轨迹视为孤立的时空序列，忽略了形成轨迹的外部环境和内部路线选择行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够综合考虑外部环境和内部路线选择行为的轨迹表示学习框架，以生成更准确、更有效的轨迹嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;PRTraj框架包含环境感知模块和路线选择编码器：环境感知模块通过捕获周围POI分布的多粒度环境语义增强道路网络；路线选择编码器将轨迹的组成路段转换建模为决策序列来捕获路线选择行为；最后将路线选择感知表示聚合形成全局轨迹嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力；PRTraj展现出强大的数据效率，在少样本场景下仍能保持稳健性能。&lt;h4&gt;结论&lt;/h4&gt;PRTraj通过结合环境感知和路线选择建模，显著提升了轨迹表示学习的效果，为各种下游任务提供了更高质量的轨迹嵌入。&lt;h4&gt;翻译&lt;/h4&gt;轨迹表示学习旨在将原始轨迹编码为低维向量，这些向量可在各种下游任务中利用，包括行程时间估计、位置预测和轨迹相似性分析。然而，现有的轨迹表示学习方法存在一个关键疏忽：将轨迹视为孤立的时空序列，而没有考虑支配其形成的外部环境和内部路线选择行为。为了弥合这一差距，我们提出了一种新颖的框架，统一了全面的环境感知和明确的路线选择建模，用于有效的轨迹表示学习，称为PRTraj。具体而言，PRTraj首先引入环境感知模块，通过捕获周围POI分布的多粒度环境语义来增强道路网络。基于这种环境感知骨干网络，路线选择编码器通过将轨迹的组成路段转换建模为决策序列来捕获每条轨迹固有的路线选择行为。这些路线选择感知表示最终被聚合形成全局轨迹嵌入。在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力。此外，PRTraj展现出强大的数据效率，在少样本场景下保持稳健性能。我们的代码可在以下网址获取：https://anonymous.4open.science/r/PRTraj。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory Representation Learning (TRL) aims to encode raw trajectories intolow-dimensional vectors, which can then be leveraged in various downstreamtasks, including travel time estimation, location prediction, and trajectorysimilarity analysis. However, existing TRL methods suffer from a key oversight:treating trajectories as isolated spatio-temporal sequences, withoutconsidering the external environment and internal route choice behavior thatgovern their formation. To bridge this gap, we propose a novel framework thatunifies comprehensive environment \textbf{P}erception and explicit\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representationlearning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces anEnvironment Perception Module to enhance the road network by capturingmulti-granularity environmental semantics from surrounding POI distributions.Building on this environment-aware backbone, a Route Choice Encoder thencaptures the route choice behavior inherent in each trajectory by modeling itsconstituent road segment transitions as a sequence of decisions. Theseroute-choice-aware representations are finally aggregated to form the globaltrajectory embedding. Extensive experiments on 3 real-world datasets across 5downstream tasks validate the effectiveness and generalizability of PRTraj.Moreover, PRTraj demonstrates strong data efficiency, maintaining robustperformance under few-shot scenarios. Our code is available at:https://anonymous.4open.science/r/PRTraj.</description>
      <author>example@mail.com (Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song)</author>
      <guid isPermaLink="false">2510.14819v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval</title>
      <link>http://arxiv.org/abs/2510.14535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,3 figures, 3 tables. Accepted at 2025 IEEE International  Conference on Systems, Man, and Cybernetics (IEEE SMC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PL-SE-ADA的域调和框架，通过双编码器结构和对抗训练实现医学图像的域调和与可解释表示学习，同时保留与疾病相关的信息。&lt;h4&gt;背景&lt;/h4&gt;医学图像（如磁共振扫描）常因扫描仪和协议差异在不同成像站点间表现出域偏移，降低了机器学习在疾病分类等任务中的性能。现有方法虽能提取域不变和域特定特征，但缺乏医学应用所需的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的域调和框架，实现可解释的表示学习，同时保留脑磁共振图像中与疾病相关的信息。&lt;h4&gt;方法&lt;/h4&gt;提出PL-SE-ADA框架，包含两个编码器分别提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像。&lt;h4&gt;主要发现&lt;/h4&gt;PL-SE-ADA在图像重建、疾病分类和域识别方面实现了与先前方法相当或更好的性能，同时能够可视化域独立的脑特征和域特定成分，提供了高可解释性。&lt;h4&gt;结论&lt;/h4&gt;PL-SE-ADA是一种有效的域调和框架，不仅提高了医学图像处理任务的性能，还提供了必要的可解释性，解决了医学应用中的实际问题。&lt;h4&gt;翻译&lt;/h4&gt;医学图像如磁共振扫描通常因扫描仪和协议差异在不同成像站点间表现出域偏移，这降低了机器学习在疾病分类等任务中的性能。域调和因此成为关键研究焦点。近期方法将脑图像编码到低维潜在空间并分离为域不变和域特定成分，但往往缺乏医学应用所需的可解释性。我们提出PL-SE-ADA框架，包含两个编码器提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像，确保调和效果和信息保留。与先前方法相比，PL-SE-ADA在图像重建、疾病分类和域识别方面表现相当或更好，同时提供了高可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical images like MR scans often show domain shifts across imaging sitesdue to scanner and protocol differences, which degrade machine learningperformance in tasks such as disease classification. Domain harmonization isthus a critical research focus. Recent approaches encode brain images$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, thendisentangle it into $\boldsymbol{z_u}$ (domain-invariant) and$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, thesemethods often lack interpretability$-$an essential requirement in medicalapplications$-$leaving practical issues unresolved. We proposePseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), ageneral framework for domain harmonization and interpretable representationlearning that preserves disease-relevant information in brain MR images.PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image$f_D$, and a domain predictor $g_D$. Beyond adversarial training between theencoder and domain predictor, the model learns to reconstruct the input image$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Comparedto prior methods, PL-SE-ADA achieves equal or better performance in imagereconstruction, disease classification, and domain recognition. It also enablesvisualization of both domain-independent brain features and domain-specificcomponents, offering high interpretability across the entire framework.</description>
      <author>example@mail.com (Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi)</author>
      <guid isPermaLink="false">2510.14535v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Revisit Modality Imbalance at the Decision Layer</title>
      <link>http://arxiv.org/abs/2510.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Some Insights in Balanced Multimodal Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态学习面临模态不平衡问题，这种不平衡不仅存在于表示学习阶段，也在决策层显著存在。研究表明，即使在充分预训练后，模型仍表现出对某些模态的系统偏见，这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致。作者建议在决策层引入自适应权重分配机制以实现更平衡的模态融合。&lt;h4&gt;背景&lt;/h4&gt;多模态学习整合不同模态信息以增强模型性能，但常面临模态不平衡问题，即主导模态在联合优化过程中掩盖较弱模态。&lt;h4&gt;目的&lt;/h4&gt;揭示模态不平衡不仅在表示学习阶段存在，也在决策层显著表现，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在预训练和平衡优化后对模态的偏见，研究特征空间和决策权重分布的差异。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模态不平衡不仅存在于表示学习阶段，也在决策层显著存在；2) 即使在充分预训练和平衡优化后，模型仍表现出对某些模态（如音频）的系统偏见；3) 这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致；4) 在融合阶段聚合未校准的模态输出会导致决策层的加权偏差。&lt;h4&gt;结论&lt;/h4&gt;未来的多模态系统应该在决策层更多地纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡，从而有效利用较弱模态的贡献。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习整合来自不同模态的信息以增强模型性能，但它常常遭受模态不平衡的影响，在联合优化过程中主导模态会掩盖较弱的模态。本文揭示这种不平衡不仅发生在表示学习阶段，而且在决策层也显著表现。在音频-视觉数据集（CREMAD和Kinetic-Sounds）上的实验表明，即使在广泛的预训练和平衡优化后，模型仍然表现出对某些模态（如音频）的系统偏见。进一步分析表明，这种偏见源于特征空间和决策权重分布的内在差异，而不仅仅是优化动态。我们认为，在融合阶段聚合未校准的模态输出会导致决策层的加权偏差，阻碍较弱模态的有效贡献。为此，我们建议未来的多模态系统应该更注重在决策层纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning integrates information from different modalities toenhance model performance, yet it often suffers from modality imbalance, wheredominant modalities overshadow weaker ones during joint optimization. Thispaper reveals that such an imbalance not only occurs during representationlearning but also manifests significantly at the decision layer. Experiments onaudio-visual datasets (CREMAD and Kinetic-Sounds) show that even afterextensive pretraining and balanced optimization, models still exhibitsystematic bias toward certain modalities, such as audio. Further analysisdemonstrates that this bias originates from intrinsic disparities infeature-space and decision-weight distributions rather than from optimizationdynamics alone. We argue that aggregating uncalibrated modality outputs at thefusion stage leads to biased decision-layer weighting, hindering weakermodalities from contributing effectively. To address this, we propose thatfuture multimodal systems should focus more on incorporate adaptive weightallocation mechanisms at the decision layer, enabling relative balancedaccording to the capabilities of each modality.</description>
      <author>example@mail.com (Xiaoyu Ma, Hao Chen)</author>
      <guid isPermaLink="false">2510.14411v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis</title>
      <link>http://arxiv.org/abs/2510.14403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DCMIL模型，用于处理全切片图像(WSI)进行癌症预后预测，解决了计算瓶颈和注释稀缺的问题，并在多种癌症类型中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算病理学新兴学科利用WSI量化形态异性和开发癌症预后模型，但受千兆像素级输入的计算瓶颈和密集手动注释稀缺的阻碍，且当前方法忽视了多倍率WSI中的细粒度信息和肿瘤微环境变异。&lt;h4&gt;目的&lt;/h4&gt;开发一个易于到难的正向表示学习模型(DCMIL)，高效处理WSI用于癌症预后预测，不依赖密集注释，并能直接将千兆像素级WSI转化为结果预测。&lt;h4&gt;方法&lt;/h4&gt;提出名为双课程对比多实例学习(DCMIL)的模型，是一种正向表示学习模型，能高效处理WSI，不需要密集注释，可直接将大型WSI图像转化为预后预测。&lt;h4&gt;主要发现&lt;/h4&gt;在12种癌症类型(5,954名患者，1,254万张图像块)的实验中，DCMIL优于标准WSI预后模型；能识别细粒度预后显著区域；提供稳健实例不确定性估计；捕获正常与肿瘤组织形态差异；有潜力产生新生物学见解。&lt;h4&gt;结论&lt;/h4&gt;DCMIL模型在癌症预后预测方面表现出色，不需要密集注释，能直接处理大型WSI图像，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蓬勃发展的计算病理学学科显示出利用全切片图像(WSIs)量化形态异质性并为人类癌症开发客观预后模型的希望。然而，千兆像素级输入的计算瓶颈和密集手动注释的稀缺阻碍了进展。当前方法常常忽视了多倍率WSI中的细粒度信息和肿瘤微环境的变异。在这里，我们提出一个易于到难的正向表示学习模型，称为双课程对比多实例学习(DCMIL)，以高效处理WSI用于癌症预后。该模型不依赖于密集注释，并能将千兆像素级WSI直接转化为结果预测。在十二种癌症类型(5,954名患者，1,254万张图像块)的大量实验中证明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能识别细粒度的预后显著区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，有潜力产生新的生物学见解。所有代码已在https://github.com/tuuuc/DCMIL上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The burgeoning discipline of computational pathology shows promise inharnessing whole slide images (WSIs) to quantify morphological heterogeneityand develop objective prognostic modes for human cancers. However, progress isimpeded by the computational bottleneck of gigapixel-size inputs and thescarcity of dense manual annotations. Current methods often overlookfine-grained information across multi-magnification WSIs and variations intumor microenvironments. Here, we propose an easy-to-hard progressiverepresentation learning model, termed dual-curriculum contrastivemulti-instance learning (DCMIL), to efficiently process WSIs for cancerprognosis. The model does not rely on dense annotations and enables the directtransformation of gigapixel-size WSIs into outcome predictions. Extensiveexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)demonstrate that DCMIL outperforms standard WSI-based prognostic models.Additionally, DCMIL identifies fine-grained prognosis-salient regions, providesrobust instance uncertainty estimation, and captures morphological differencesbetween normal and tumor tissues, with the potential to generate new biologicalinsights. All codes have been made publicly accessible athttps://github.com/tuuuc/DCMIL.</description>
      <author>example@mail.com (Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning)</author>
      <guid isPermaLink="false">2510.14403v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection</title>
      <link>http://arxiv.org/abs/2510.14344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了BINCTX，一种多模态学习方法，用于检测移动应用中的不良行为，通过结合代码级语义、行为触发方式和第三方库使用信息，实现了高准确率的检测。&lt;h4&gt;背景&lt;/h4&gt;移动应用市场有数百万个应用，但不良行为（如干扰性广告、非法重定向、支付欺诈）难以被发现，因为这些行为通常不依赖权限保护的API，且可通过UI或元数据编辑轻易伪装。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测移动应用中不良行为的机器学习方法，提高检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;BINCTX构建应用的三种视图：全局字节码图像视图（捕获代码级语义和家族模式）、上下文视图（显示行为触发方式）和第三方库使用视图（总结组件间调用路径上的调用频率），然后将这三种视图嵌入并融合，训练上下文感知分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界恶意软件和良性应用上，BINCTX达到94.73%的宏观F1值，比强大基线方法至少高出14.92%；在商业混淆下保持84%的F1值；比最先进的仅字节码系统更能抵抗对抗样本。&lt;h4&gt;结论&lt;/h4&gt;BINCTX通过多模态表示学习，有效结合了代码级语义、行为上下文和第三方库使用信息，显著提高了移动应用不良行为的检测性能，并增强了对混淆技术和对抗攻击的抵抗力。&lt;h4&gt;翻译&lt;/h4&gt;移动应用市场托管着数百万个应用，但不良行为（例如干扰性广告、非法重定向、支付欺诈）仍然难以被发现，因为它们通常不依赖于权限保护的API，并且可以通过UI或元数据编辑轻松伪装。我们提出了BINCTX，一种学习方法，它从(i)全局字节码图像视图捕获代码级语义和家族模式，(ii)上下文视图（显示的操作、组件、声明的权限、URL/IP常量）指示行为如何被触发，以及(iii)第三方库使用视图总结组件间调用路径上的调用频率，构建应用的多模态表示。这三个视图被嵌入并融合，训练一个上下文感知分类器。在真实世界的恶意软件和良性应用上，BINCTX实现了94.73%的宏观F1值，比强大的基线方法至少高出14.92%。它在商业混淆下保持鲁棒性（混淆后F1为84%），并且比最先进的仅字节码系统更能抵抗对抗样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile app markets host millions of apps, yet undesired behaviors (e.g.,disruptive ads, illegal redirection, payment deception) remain hard to catchbecause they often do not rely on permission-protected APIs and can be easilycamouflaged via UI or metadata edits. We present BINCTX, a learning approachthat builds multi-modal representations of an app from (i) a globalbytecode-as-image view that captures code-level semantics and family-stylepatterns, (ii) a contextual view (manifested actions, components, declaredpermissions, URL/IP constants) indicating how behaviors are triggered, and(iii) a third-party-library usage view summarizing invocation frequencies alonginter-component call paths. The three views are embedded and fused to train acontextual-aware classifier. On real-world malware and benign apps, BINCTXattains a macro F1 of 94.73%, outperforming strong baselines by at least14.92%. It remains robust under commercial obfuscation (F1 84%post-obfuscation) and is more resistant to adversarial samples thanstate-of-the-art bytecode-only systems.</description>
      <author>example@mail.com (Zichen Liu, Shao Yang, Xusheng Xiao)</author>
      <guid isPermaLink="false">2510.14344v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm</title>
      <link>http://arxiv.org/abs/2510.14321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为大型推理嵌入模型(LREM)的新方法，通过将推理过程整合到表示学习中，解决了电子商务搜索系统中困难查询的语义匹配问题，显著提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;在现代电子商务搜索系统中，密集检索是重要组成部分。主流嵌入模型已从BERT转向大型语言模型(LLMs)，但仍采用直接嵌入方法，语义准确性不足。对比学习虽被使用，但模型倾向于捕获统计共现模式，偏向浅层词汇和语义匹配，导致对困难查询的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中，以解决困难查询与目标物品之间的语义匹配问题，提高检索准确性。&lt;h4&gt;方法&lt;/h4&gt;LREM对困难查询先进行推理以深入理解查询，然后生成推理增强的查询嵌入用于检索。采用两阶段训练：第一阶段在Query-CoT-Item三元组上使用SFT和InfoNCE损失优化LLM；第二阶段通过强化学习进一步优化推理轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;推理过程有效桥接了原始查询和目标物品间的语义差距，显著提高了检索准确性。大量离线和在线实验验证了LREM的有效性。&lt;h4&gt;结论&lt;/h4&gt;LREM已被成功部署在中国最大的电子商务平台上，自2025年8月起，证明了其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。通过计算查询和物品(产品)嵌入之间的相似性，它能够从大规模存储库中高效地选择候选产品。随着大型语言模型(LLMs)的突破，主流嵌入模型已逐渐从BERT转向LLMs以实现更准确的文本建模。然而，这些模型仍采用直接嵌入方法，嵌入的语义准确性仍然不足。因此，对比学习被大量使用来实现正对之间的紧密语义对齐。结果，这些模型倾向于捕获训练数据中的统计共现模式，偏向于浅层词汇和语义匹配。对于与目标物品存在明显词汇差异的困难查询，性能显著下降。在这项工作中，我们提出了大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以实现对原始查询的深入理解，然后生成推理增强的查询嵌入用于检索。这一推理过程有效地桥接了原始查询和目标物品之间的语义差距，显著提高了检索准确性。具体而言，我们采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品(Query-CoT-Item)三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习(RL)进一步优化推理轨迹。大量的离线和在线实验验证了LREM的有效性，使其自2025年8月起被部署在中国最大的电子商务平台上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern e-commerce search systems, dense retrieval has become anindispensable component. By computing similarities between query and item(product) embeddings, it efficiently selects candidate products fromlarge-scale repositories. With the breakthroughs in large language models(LLMs), mainstream embedding models have gradually shifted from BERT to LLMsfor more accurate text modeling. However, these models still adoptdirect-embedding methods, and the semantic accuracy of embeddings remainsinadequate. Therefore, contrastive learning is heavily employed to achievetight semantic alignment between positive pairs. Consequently, such models tendto capture statistical co-occurrence patterns in the training data, biasingthem toward shallow lexical and semantic matches. For difficult queriesexhibiting notable lexical disparity from target items, the performancedegrades significantly. In this work, we propose the Large Reasoning EmbeddingModel (LREM), which novelly integrates reasoning processes into representationlearning. For difficult queries, LREM first conducts reasoning to achieve adeep understanding of the original query, and then produces areasoning-augmented query embedding for retrieval. This reasoning processeffectively bridges the semantic gap between original queries and target items,significantly improving retrieval accuracy. Specifically, we adopt a two-stagetraining process: the first stage optimizes the LLM on carefully curatedQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminaryreasoning and embedding capabilities, and the second stage further refines thereasoning trajectories via reinforcement learning (RL). Extensive offline andonline experiments validate the effectiveness of LREM, leading to itsdeployment on China's largest e-commerce platform since August 2025.</description>
      <author>example@mail.com (Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu)</author>
      <guid isPermaLink="false">2510.14321v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</title>
      <link>http://arxiv.org/abs/2510.14139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review in Frontiers in Bioinformatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用的准确预测对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型(PLMs)的直接序列嵌入，或使用图神经网络(GNNs)处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架用于下游PPI预测，通过链接预测实现。&lt;h4&gt;方法&lt;/h4&gt;引入一个两阶段的图表示学习框架ProtGram-DirectGCN。第一阶段开发ProtGram，将蛋白质的一级结构建模为全局推断的n-gram图层次结构，其中残基转移概率定义边权重。第二阶段提出DirectGCN，一种定制的有向图卷积神经网络，通过入向、出向和无向路径的转换处理信息，并通过可学习的门控机制结合这些路径。&lt;h4&gt;主要发现&lt;/h4&gt;DirectGCN在标准节点分类基准上表现良好，性能与已建立的方法相当，尤其在具有密集、异质结构的有向复杂图中表现出色。完整的ProtGram-DirectGCN框架应用于PPI预测时提供了强大的预测能力，即使在有限的训练数据下也能保持。&lt;h4&gt;结论&lt;/h4&gt;ProtGram-DirectGCN框架是一种有效的PPI预测方法，在计算资源有限的情况下也能保持良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;引言：准确预测蛋白质相互作用对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型的直接序列嵌入，或使用图神经网络处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。我们引入了一种通过链接预测进行下游PPI预测的新框架。方法：我们引入了一个两阶段的图表示学习框架ProtGram-DirectGCN。首先，我们开发了ProtGram，该方法将蛋白质的一级结构建模为全局推断的n-gram图层次结构。在这些图中，残基转移概率定义边权重，每条边在有向图中连接一对残基，这些概率从大量序列集合中聚合。其次，我们提出了DirectGCN，一种定制的有向图卷积神经网络，该模型具有独特的卷积层，通过入向、出向和无向的特定路径转换处理信息，同时应用共享转换，这些路径通过可学习的门控机制结合。我们将DirectGCN应用于ProtGram图以学习残基级嵌入，并通过注意力池化生成蛋白质级嵌入进行预测。结果：我们首先在标准节点分类基准上建立了DirectGCN的有效性，其在一般数据集上的性能与已建立的方法相当，该模型在具有密集、异质结构的有向复杂图中表现出色。当应用于PPI预测时，完整的ProtGram-DirectGCN框架提供了强大的预测能力，即使在有限的训练数据下，这种强大的性能仍然保持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3389/fbinf.2025.1651623&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction Accurate prediction of protein-protein interactions (PPIs) iscrucial for understanding cellular functions and advancing drug development.Existing in-silico methods use direct sequence embeddings from Protein LanguageModels (PLMs). Others use Graph Neural Networks (GNNs) for 3D proteinstructures. This study explores less computationally intensive alternatives. Weintroduce a novel framework for downstream PPI prediction through linkprediction. Methods We introduce a two-stage graph representation learningframework, ProtGram-DirectGCN. First, we developed ProtGram. This approachmodels a protein's primary structure as a hierarchy of globally inferred n-gramgraphs. In these graphs, residue transition probabilities define edge weights.Each edge connects a pair of residues in a directed graph. The probabilitiesare aggregated from a large corpus of sequences. Second, we propose DirectGCN,a custom directed graph convolutional neural network. This model features aunique convolutional layer. It processes information through separatepath-specific transformations: incoming, outgoing, and undirected. A sharedtransformation is also applied. These paths are combined via a learnable gatingmechanism. We apply DirectGCN to ProtGram graphs to learn residue-levelembeddings. These embeddings are pooled via attention to generate protein-levelembeddings for prediction. Results We first established the efficacy ofDirectGCN on standard node classification benchmarks. Its performance matchesestablished methods on general datasets. The model excels at complex, directedgraphs with dense, heterophilic structures. When applied to PPI prediction, thefull ProtGram-DirectGCN framework delivers robust predictive power. This strongperformance holds even with limited training data.</description>
      <author>example@mail.com (Islam Akef Ebeid, Haoteng Tang, Pengfei Gu)</author>
      <guid isPermaLink="false">2510.14139v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
      <link>http://arxiv.org/abs/2510.14112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STEMS的新型安全约束多智能体强化学习框架，用于协调建筑能源管理，有效解决了多建筑系统中时空依赖关系利用和操作安全性的挑战。&lt;h4&gt;背景&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的安全约束多智能体强化学习框架，解决多建筑协调能源管理中的挑战，特别是在利用时空依赖关系和确保操作安全方面。&lt;h4&gt;方法&lt;/h4&gt;STEMS框架整合了两个核心组件：(1)时空图表示学习框架，使用GCN-Transformer融合架构捕捉建筑间关系和时间模式；(2)安全约束多智能体RL算法，结合控制屏障函数提供数学安全保证。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下表现出强大的鲁棒性，并在不同类型建筑中保持有效性。&lt;h4&gt;结论&lt;/h4&gt;STEMS框架成功解决了多建筑能源管理中的关键挑战，通过整合时空信息利用和安全约束，实现了显著的能源成本降低和排放减少，同时确保了系统安全和居住者舒适度。&lt;h4&gt;翻译&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。协调建筑能源管理在利用时空依赖关系的同时确保多建筑系统运行安全方面面临关键挑战。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。本文提出STEMS，一种用于协调建筑能源管理的新型安全约束多智能体强化学习框架。STEMS整合了两个核心组件：(1)使用GCN-Transformer融合架构的时空图表示学习框架，用于捕捉建筑间关系和时间模式；(2)结合控制屏障函数的安全约束多智能体RL算法，提供数学安全保证。在真实建筑数据集上的大量实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，同时将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下也表现出强大的鲁棒性，并且在不同类型建筑中保持有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building energy management is essential for achieving carbon reduction goals,improving occupant comfort, and reducing energy costs. Coordinated buildingenergy management faces critical challenges in exploiting spatial-temporaldependencies while ensuring operational safety across multi-building systems.Current multi-building energy systems face three key challenges: insufficientspatial-temporal information exploitation, lack of rigorous safety guarantees,and system complexity. This paper proposes Spatial-Temporal Enhanced SafeMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agentreinforcement learning framework for coordinated building energy management.STEMS integrates two core components: (1) a spatial-temporal graphrepresentation learning framework using a GCN-Transformer fusion architectureto capture inter-building relationships and temporal patterns, and (2) asafety-constrained multi-agent RL algorithm incorporating Control BarrierFunctions to provide mathematical safety guarantees. Extensive experiments onreal-world building datasets demonstrate STEMS's superior performance overexisting methods, showing that STEMS achieves 21% cost reduction, 18% emissionreduction, and dramatically reduces safety violations from 35.1% to 5.6% whilemaintaining optimal comfort with only 0.13 discomfort proportion. The frameworkalso demonstrates strong robustness during extreme weather conditions andmaintains effectiveness across different building types.</description>
      <author>example@mail.com (Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet)</author>
      <guid isPermaLink="false">2510.14112v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
      <link>http://arxiv.org/abs/2510.14049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一个新的因果表征学习(CRL)基准，使用高保真模拟视觉数据，既保留真实视觉复杂性又能访问真实因果生成过程，包含约20万张图像和300万视频帧，涵盖四个领域的24个子场景。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习旨在揭示数据生成过程并识别潜在因果变量和关系，但评估具有挑战性，因为需要已知的真实因果变量和结构。现有评估方法要么依赖简化合成数据集，要么依赖现实世界任务中的下游性能，在真实性和评估精度间面临两难困境。&lt;h4&gt;目的&lt;/h4&gt;创建一个既保留真实视觉复杂性又能访问真实因果生成过程的新CRL基准，解决现有评估方法在真实性和评估精度之间的两难困境。&lt;h4&gt;方法&lt;/h4&gt;构建包含约20万张图像和300万视频帧的数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通情况分析四个领域的24个子场景，提供对底层因果结构的灵活访问，允许用户修改或配置以符合CRL假设要求。&lt;h4&gt;主要发现&lt;/h4&gt;利用此基准评估了不同范式的代表性CRL方法，提供了实证见解，帮助实践者和新手选择或扩展适当的CRL框架，以解决可以从CRL视角受益的现实问题。&lt;h4&gt;结论&lt;/h4&gt;该基准有望弥合严格评估和实际应用之间的差距，为CRL研究提供更全面、更真实的测试平台。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已为中文，无需额外翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal Representation Learning (CRL) aims to uncover the data-generatingprocess and identify the underlying causal variables and relations, whoseevaluation remains inherently challenging due to the requirement of knownground-truth causal variables and causal structure. Existing evaluations oftenrely on either simplistic synthetic datasets or downstream performance onreal-world tasks, generally suffering a dilemma between realism and evaluativeprecision. In this paper, we introduce a new benchmark for CRL usinghigh-fidelity simulated visual data that retains both realistic visualcomplexity and, more importantly, access to ground-truth causal generatingprocesses. The dataset comprises around 200 thousand images and 3 million videoframes across 24 sub-scenes in four domains: static image generation, dynamicphysical simulations, robotic manipulations, and traffic situation analysis.These scenarios range from static to dynamic settings, simple to complexstructures, and single to multi-agent interactions, offering a comprehensivetestbed that hopefully bridges the gap between rigorous evaluation andreal-world applicability. In addition, we provide flexible access to theunderlying causal structures, allowing users to modify or configure them toalign with the required assumptions in CRL, such as available domain labels,temporal dependencies, or intervention histories. Leveraging this benchmark, weevaluated representative CRL methods across diverse paradigms and offeredempirical insights to assist practitioners and newcomers in choosing orextending appropriate CRL frameworks to properly address specific types of realproblems that can benefit from the CRL perspective. Welcome to visit our:Project page:https://causal-verse.github.io/,Dataset:https://huggingface.co/CausalVerse.</description>
      <author>example@mail.com (Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Sheng, Zijian Li, Kun Zhang)</author>
      <guid isPermaLink="false">2510.14049v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2510.14470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了结合语言模型的图基础模型在文本属性图上的安全漏洞，特别是后门攻击问题，并提出了一种双触发攻击框架。&lt;h4&gt;背景&lt;/h4&gt;图基础模型，特别是结合语言模型的模型，已革新图学习并在文本属性图上表现出色，但相比传统GNN引入了新的安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;解决LM赋能的GFMs在无安全提示调整阶段的安全漏洞问题，特别是在属性不可访问的约束TAG系统中的后门攻击挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过利用预先建立的文本池实现无需显式优化触发节点文本属性的有效攻击。&lt;h4&gt;主要发现&lt;/h4&gt;传统图后门攻击在属性不可访问的约束TAG系统中性能显著下降；所提双触发攻击框架能保持优越的干净准确率并取得出色的攻击成功率。&lt;h4&gt;结论&lt;/h4&gt;LM赋能的GFMs在网络部署中存在关键后门风险，研究为基础模型时代开源平台开发更强大的监督机制提供了贡献。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型的出现，特别是那些结合语言模型的模型，已经革新了图学习并在文本属性图上表现出色。然而，与传统GNN相比，这些由语言模型赋能的图基础模型在无安全提示调整阶段引入了独特的安全漏洞，这些漏洞在当前研究中尚未得到充分研究。通过实证研究，我们发现在属性不可访问的约束文本属性图系统中，当没有显式优化触发节点属性时，传统图后门攻击的性能会显著下降。为此，我们提出了一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过战略性地利用预先建立的文本池，无需显式优化触发节点文本属性即可实现有效攻击。大量实验评估表明，我们的攻击方法在保持优越的干净准确率的同时，取得了出色的攻击成功率，包括在高度隐蔽的单触发节点场景中。我们的工作强调了在网络上部署的由语言模型赋能的图基础模型中的关键后门风险，并为基础模型时代开源平台开发更强大的监督机制做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of graph foundation models (GFMs), particularly thoseincorporating language models (LMs), has revolutionized graph learning anddemonstrated remarkable performance on text-attributed graphs (TAGs). However,compared to traditional GNNs, these LM-empowered GFMs introduce unique securityvulnerabilities during the unsecured prompt tuning phase that remainunderstudied in current research. Through empirical investigation, we reveal asignificant performance degradation in traditional graph backdoor attacks whenoperating in attribute-inaccessible constrained TAG systems without explicittrigger node attribute optimization. To address this, we propose a noveldual-trigger backdoor attack framework that operates at both text-level andstruct-level, enabling effective attacks without explicit optimization oftrigger node text attributes through the strategic utilization of apre-established text pool. Extensive experimental evaluations demonstrate thatour attack maintains superior clean accuracy while achieving outstanding attacksuccess rates, including scenarios with highly concealed single-trigger nodes.Our work highlights critical backdoor risks in web-deployed LM-empowered GFMsand contributes to the development of more robust supervision mechanisms foropen-source platforms in the era of foundation models.</description>
      <author>example@mail.com (Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou)</author>
      <guid isPermaLink="false">2510.14470v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis</title>
      <link>http://arxiv.org/abs/2510.14336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的图变换器架构DARTS-GT，通过不对称注意力和可微分架构搜索实现深度异质性，并开发了首个图变换器的定量可解释性框架。实验表明该方法在多个数据集上达到最先进水平，且发现的异构架构比基线更可解释，证明图变换器无需在性能和可解释性间做出取舍。&lt;h4&gt;背景&lt;/h4&gt;图变换器(GTs)是处理图结构数据的有力架构，但受限于刚性设计和缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，且复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;重新设计GT注意力机制通过不对称性解耦结构编码与特征表示；使用DARTS在每层选择最优GNN算子；开发首个GT的定量可解释性框架；探索GT是否需要在性能和可解释性之间做出选择。&lt;h4&gt;方法&lt;/h4&gt;重新设计GT注意力：查询来自节点特征，键和值来自GNN变换；使用DARTS在transformer注意力内部实现深度异质性(DARTS-GT)；通过因果消融开发GT的定量可解释性框架；提出Head-deviation、Specialization和Focus指标；在8个基准数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;DARTS-GT在4个数据集上达到最先进水平，在其他数据集上保持竞争力；发现的架构揭示了数据集特定模式；可视化注意力和因果重要性并不总是相关，表明常用可视化方法可能忽略真正重要的组件；DARTS-GT发现的异构架构比基线产生更可解释的模型。&lt;h4&gt;结论&lt;/h4&gt;Graph Transformers不需要在性能和可解释性之间做出选择。异构架构可以同时提高性能和可解释性，证明性能和可解释性并非相互排斥的目标。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)已成为处理图结构数据的有力架构，但仍受限于刚性设计且缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，同时其复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。我们通过不对称性重新设计GT注意力，解耦结构编码与特征表示：查询来自节点特征，而键和值来自GNN变换。在此框架内，我们使用可微分架构搜索(DARTS)在每层选择最优GNN算子，在transformer注意力内部实现深度异质性(DARTS-GT)。为了理解发现的架构，我们通过因果消融开发了首个GT的定量可解释性框架。我们的指标(Head-deviation、Specialization和Focus)识别出哪些头和节点驱动预测，同时实现模型比较。在八个基准数据集上的实验显示，DARTS-GT在四个数据集上达到最先进水平，在其他数据集上保持竞争力，且发现的架构揭示了数据集特定模式。我们的可解释性分析表明，可视化注意力和因果重要性并不总是相关，表明广泛使用的可视化方法可能忽略实际重要的组件。重要的是，DARTS-GT发现的异构架构始终比基线产生更可解释的模型，证明图变换器无需在性能和可解释性之间做出选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as powerful architectures forgraph-structured data, yet remain constrained by rigid designs and lackquantifiable interpretability. Current state-of-the-art GTs commit to fixed GNNtypes across all layers, missing potential benefits of depth-specific componentselection, while their complex architectures become opaque where performancegains cannot be distinguished between meaningful patterns and spuriouscorrelations. We redesign GT attention through asymmetry, decoupling structuralencoding from feature representation: queries derive from node features whilekeys and values come from GNN transformations. Within this framework, we useDifferentiable ARchiTecture Search (DARTS) to select optimal GNN operators ateach layer, enabling depth-wise heterogeneity inside transformer attentionitself (DARTS-GT). To understand discovered architectures, we develop the firstquantitative interpretability framework for GTs through causal ablation. Ourmetrics (Head-deviation, Specialization, and Focus), identify which heads andnodes drive predictions while enabling model comparison. Experiments acrosseight benchmarks show DARTS-GT achieves state-of-the-art on four datasets whileremaining competitive on others, with discovered architectures revealingdataset-specific patterns. Our interpretability analysis reveals that visualattention salience and causal importance do not always correlate, indicatingwidely used visualization approaches may miss components that actually matter.Crucially, heterogeneous architectures found by DARTS-GT consistently producedmore interpretable models than baselines, establishing that Graph Transformersneed not choose between performance and interpretability.</description>
      <author>example@mail.com (Shruti Sarika Chakraborty, Peter Minary)</author>
      <guid isPermaLink="false">2510.14336v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network</title>
      <link>http://arxiv.org/abs/2510.14243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submited to IEEE journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间计算通信(SCC)的框架，用于解决多用户沉浸式VR应用在分布式MEC网络中的延迟和能源效率问题。通过MO-CMPO算法，结合监督学习和强化学习，实现了帕累托最优的资源部署方案。&lt;h4&gt;背景&lt;/h4&gt;沉浸式VR应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。现有的分布式移动边缘计算(MEC)网络难以满足这些需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来满足多用户VR在分布式MEC网络上的延迟和能源需求，并实现资源的高效部署。&lt;h4&gt;方法&lt;/h4&gt;提出空间计算通信(SCC)框架，将资源部署任务表述为多目标组合优化问题，并设计MO-CMPO算法，结合监督学习和强化学习，利用稀疏图神经网络生成帕累托最优解。&lt;h4&gt;主要发现&lt;/h4&gt;MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。以延迟为导向的解决方案倾向于本地MEC执行，而以能源为导向的解决方案则最小化冗余部署。&lt;h4&gt;结论&lt;/h4&gt;SCC框架和MO-CMPO算法能够有效解决多用户VR应用在分布式MEC网络中的资源部署问题，平衡延迟和能源消耗。&lt;h4&gt;翻译&lt;/h4&gt;沉浸式虚拟现实(VR)应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。为应对这些挑战，我们引入了空间计算通信(SCC)的概念，这是一个旨在满足分布式移动边缘计算(MEC)网络上多用户VR延迟和能源需求的框架。SCC使用用户动态和资源需求的概率模型，联合表示由用户和基站定义的物理空间，以及代表共享沉浸式环境的虚拟空间。然后，资源部署任务被表述为多目标组合优化(MOCO)问题，同时最小化分布式MEC资源上的系统延迟和能源消耗。为解决这个问题，我们提出了MO-CMPO，这是一种基于策略优化的多目标一致性模型，集成了监督学习和由偏好权重引导的强化学习(RL)微调。利用稀疏图神经网络(GNN)，MO-CMPO有效生成帕累托最优解。使用真实的新无线电基站数据集进行的模拟表明，MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。此外，分析揭示了实际的部署模式：以延迟为导向的解决方案倾向于本地MEC执行以减少传输延迟，而以能源为导向的解决方案则最小化冗余部署以节省能源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immersive virtual reality (VR) applications impose stringent requirements onlatency, energy efficiency, and computational resources, particularly inmulti-user interactive scenarios. To address these challenges, we introduce theconcept of spatial computing communications (SCC), a framework designed to meetthe latency and energy demands of multi-user VR over distributed mobile edgecomputing (MEC) networks. SCC jointly represents the physical space, defined byusers and base stations, and the virtual space, representing shared immersiveenvironments, using a probabilistic model of user dynamics and resourcerequirements. The resource deployment task is then formulated as amulti-objective combinatorial optimization (MOCO) problem that simultaneouslyminimizes system latency and energy consumption across distributed MECresources. To solve this problem, we propose MO-CMPO, a multi-objectiveconsistency model with policy optimization that integrates supervised learningand reinforcement learning (RL) fine-tuning guided by preference weights.Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generatesPareto-optimal solutions. Simulations with real-world New Radio base stationdatasets demonstrate that MO-CMPO achieves superior hypervolume performance andsignificantly lower inference latency than baseline methods. Furthermore, theanalysis reveals practical deployment patterns: latency-oriented solutionsfavor local MEC execution to reduce transmission delay, while energy-orientedsolutions minimize redundant placements to save energy.</description>
      <author>example@mail.com (Caolu Xu, Zhiyong Chen, Meixia Tao, Li Song, Wenjun Zhang)</author>
      <guid isPermaLink="false">2510.14243v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks</title>
      <link>http://arxiv.org/abs/2510.14137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了解耦图卷积网络(D-GCN)来解决异构多跳无线网络中吞吐量预测的挑战。D-GCN通过分离节点自身传输概率与邻居干扰效应，使用可学习注意力替代平均聚合，实现了更准确的预测和可解释性，实验表明其显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;p持续CSMA协议是随机接入MAC分析的核心，但在异构多跳无线网络中预测饱和吞吐量仍是一个难题。简化的单一共享干扰域模型会低估吞吐量48-62%，而精确的马尔可夫链分析计算复杂度高，对大型网络不实用。&lt;h4&gt;目的&lt;/h4&gt;开发可扩展的吞吐量预测方法，解决异构多跳无线网络中的计算障碍，适用于一般网络拓扑的结构化机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出解耦图卷积网络(D-GCN)，一种新型架构，明确分离节点自身的传输概率与邻居干扰效应的处理。用可学习的注意力替代平均聚合，产生可解释的每邻居贡献权重，同时捕获复杂的多跳干扰模式。&lt;h4&gt;主要发现&lt;/h4&gt;D-GCN实现了3.3%的归一化平均绝对误差(NMAE)，显著优于标准GCN的63.94% NMAE。D-GCN性能优于强基线方法，即使在精确分析方法计算上不可行的情况下仍然可扩展，且使基于梯度的网络优化达到理论最优值的1%以内。&lt;h4&gt;结论&lt;/h4&gt;D-GCN通过解耦处理和注意力机制，能够更准确地捕获网络中的复杂干扰模式，有效解决了异构多跳无线网络吞吐量预测问题。&lt;h4&gt;翻译&lt;/h4&gt;p持续CSMA协议、饱和吞吐量、异构多跳无线网络、干扰域、马尔可夫链分析、结构化机器学习、图卷积网络(GNNs)、图卷积网络(GCN)、归一化平均绝对误差(NMAE)、对称归一化、级联效应、解耦图卷积网络(D-GCN)、可学习注意力、多跳干扰模式、基于梯度的网络优化&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The p-persistent CSMA protocol is central to random-access MAC analysis, butpredicting saturation throughput in heterogeneous multi-hop wireless networksremains a hard problem. Simplified models that assume a single, sharedinterference domain can underestimate throughput by 48--62\% in sparsetopologies. Exact Markov-chain analyses are accurate but scale exponentially incomputation time, making them impractical for large networks. Thesecomputational barriers motivate structural machine learning approaches likeGNNs for scalable throughput prediction in general network topologies. Yetoff-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized meanabsolute error (NMAE) on heterogeneous networks because symmetric normalizationconflates a node's direct interference with higher-order, cascading effectsthat pertain to how interference propagates over the network graph.  Building on these insights, we propose the Decoupled Graph ConvolutionalNetwork (D-GCN), a novel architecture that explicitly separates processing of anode's own transmission probability from neighbor interference effects. D-GCNreplaces mean aggregation with learnable attention, yielding interpretable,per-neighbor contribution weights while capturing complex multihop interferencepatterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remainstractable even when exact analytical methods become computationally infeasible,and enables gradient-based network optimization that achieves within 1\% oftheoretical optima.</description>
      <author>example@mail.com (Faezeh Dehghan Tarzjani, Bhaskar Krishnamachari)</author>
      <guid isPermaLink="false">2510.14137v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>On the expressivity of sparse maxout networks</title>
      <link>http://arxiv.org/abs/2510.14068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了稀疏maxout网络的表达能力，建立了这类网络与虚拟多面体之间的对偶关系，分析了网络深度和宽度对表达能力的影响。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于稀疏maxout网络，其中每个神经元从前一层接收固定数量的输入并采用maxout激活函数，这种结构类似于卷积神经网络或图神经网络的关键特征。&lt;h4&gt;目的&lt;/h4&gt;目的是理解稀疏maxout网络的表达能力，特别是网络深度、宽度和稀疏性如何影响其计算能力。&lt;h4&gt;方法&lt;/h4&gt;通过建立稀疏maxout网络可计算函数与虚拟多面体之间的对偶关系，推导出相关多面体维度的紧界，并基于此构建深度层次结构序列。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现足够深的稀疏maxout网络具有通用性，但如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;h4&gt;结论&lt;/h4&gt;稀疏maxout网络的表达能力不仅取决于宽度，还与深度密切相关，深度不足时宽度无法完全补偿稀疏性的限制。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了稀疏maxout网络的表达能力，其中每个神经元从前一层接收固定数量的输入，并采用可能有多参数的maxout激活函数。这种设置捕捉了卷积神经网络或图神经网络的关键特征。我们建立了此类网络可计算的函数与一类虚拟多面体之间的对偶关系，将它们的几何形状与网络表达能力的问题联系起来。特别是，我们推导出相关多面体维度的紧界，作为我们分析的中心工具。在此基础上，我们构建了一个深度层次结构序列。虽然足够深的稀疏maxout网络是通用的，但我们证明，如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the expressivity of sparse maxout networks, where each neuron takesa fixed number of inputs from the previous layer and employs a, possiblymulti-argument, maxout activation. This setting captures key characteristics ofconvolutional or graph neural networks. We establish a duality betweenfunctions computable by such networks and a class of virtual polytopes, linkingtheir geometry to questions of network expressivity. In particular, we derive atight bound on the dimension of the associated polytopes, which serves as thecentral tool for our analysis. Building on this, we construct a sequence ofdepth hierarchies. While sufficiently deep sparse maxout networks areuniversal, we prove that if the required depth is not reached, width alonecannot compensate for the sparsity of a fixed indegree constraint.</description>
      <author>example@mail.com (Moritz Grillo, Tobias Hofmann)</author>
      <guid isPermaLink="false">2510.14068v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations</title>
      <link>http://arxiv.org/abs/2510.14035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages content. 2 pages references&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GammaZero是一种以动作为中心的图表示框架，用于在部分可观察马尔可夫决策过程(POMDPs)中指导规划学习，解决了现有方法在可扩展性和泛化能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有方法需要特定领域的神经网络架构，并且难以处理大规模问题，限制了在POMDPs中的规划学习能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的图表示框架，使学习到的策略能够在不同规模的问题间泛化，并减少对领域特定架构的需求。&lt;h4&gt;方法&lt;/h4&gt;GammaZero将信念状态转换为以动作为中心的图，使用图神经网络结合解码器架构从专家演示中学习价值函数和策略，然后应用这些启发式指导蒙特卡洛树搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在相同规模问题上，GammaZero性能与BetaZero相当；同时能够实现零样本泛化，处理比训练时所见大2-4倍的问题，并在减少搜索需求的同时保持解决方案质量。&lt;h4&gt;结论&lt;/h4&gt;GammaZero通过统一的图表示框架有效解决了POMDPs中的规划学习问题，实现了更好的泛化能力和可扩展性，为处理大规模部分可观察环境提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种以动作为中心的图表示框架，用于学习在部分可观察马尔可夫决策过程(POMDPs)中指导规划。与需要特定领域神经网络架构且难以扩展的现有方法不同，GammaZero利用统一的基于图的信念表示，使问题能够在领域内跨规模泛化。我们的关键见解是信念状态可以系统地转换为以动作为中心的图，其中在小问题上学习的结构模式可以转移到更大的实例上。我们采用具有解码器架构的图神经网络，从计算可行问题上的专家演示中学习价值函数和策略，然后将这些学习到的启发式应用于指导更大问题上的蒙特卡洛树搜索。在标准POMDP基准测试上的实验结果表明，当在相同规模问题上训练和测试时，GammaZero与BetaZero相当，同时能够独特地实现零样本泛化到比训练时所见大2-4倍的问题，在减少搜索需求的同时保持解决方案质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce an action-centric graph representation framework for learning toguide planning in Partially Observable Markov Decision Processes (POMDPs).Unlike existing approaches that require domain-specific neural architecturesand struggle with scalability, GammaZero leverages a unified graph-based beliefrepresentation that enables generalization across problem sizes within adomain. Our key insight is that belief states can be systematically transformedinto action-centric graphs where structural patterns learned on small problemstransfer to larger instances. We employ a graph neural network with a decoderarchitecture to learn value functions and policies from expert demonstrationson computationally tractable problems, then apply these learned heuristics toguide Monte Carlo tree search on larger problems. Experimental results onstandard POMDP benchmarks demonstrate that GammaZero achieves comparableperformance to BetaZero when trained and tested on the same-sized problems,while uniquely enabling zero-shot generalization to problems 2-4 times largerthan those seen during training, maintaining solution quality with reducedsearch requirements.</description>
      <author>example@mail.com (Rajesh Mangannavar, Prasad Tadepalli)</author>
      <guid isPermaLink="false">2510.14035v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters</title>
      <link>http://arxiv.org/abs/2510.14250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PhysAttnNet的新型物理先验引导双流注意力网络，通过引入衰减双向自注意力和相位差引导的双向交叉注意力模块，有效解决了传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题。实验证明该模型在波浪槽数据集上表现优异，且对未见环境具有良好的适应性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。&lt;h4&gt;目的&lt;/h4&gt;克服传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题，开发一种能够更好处理未见海况的预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PhysAttnNet的物理先验引导双流注意力网络，包含三个关键模块：1)衰减双向自注意力(DBSA)模块，通过可学习的时间衰减模拟自然衰减现象；2)相位差引导的双向交叉注意力(PDG-BCA)模块，明确捕获波与结构之间的双向相互作用和相位关系；3)全局上下文融合(GCF)模块，协同整合两个流。模型使用混合时频损失函数进行训练，同时最小化时域预测误差和频域频谱差异。&lt;h4&gt;主要发现&lt;/h4&gt;在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性。&lt;h4&gt;结论&lt;/h4&gt;PhysAttnNet有潜力作为开发海洋工程复杂系统预测模型的框架，能够有效解决传统深度学习模型在海洋环境预测中面临的泛化能力有限问题。&lt;h4&gt;翻译&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。为克服这些挑战，本研究提出了一种新颖的物理先验引导双流注意力网络(PhysAttnNet)。首先，衰减双向自注意力(DBSA)模块纳入了可学习的时间衰减，为最近的状态分配更高的权重，旨在模拟自然衰减现象。同时，相位差引导的双向交叉注意力(PDG-BCA)模块使用基于余弦的偏差在双向交叉计算范式中明确捕获波与结构之间的双向相互作用和相位关系。这些流通过全局上下文融合(GCF)模块协同整合。最后，PhysAttnNet使用混合时频损失进行训练，该损失函数同时最小化时域预测误差和频域频谱差异。在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性，突显了其作为开发海洋工程复杂系统预测模型的框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion response prediction for elastic Bragg breakwaters is criticalfor their structural safety and operational integrity in marine environments.However, conventional deep learning models often exhibit limited generalizationcapabilities when presented with unseen sea states. These deficiencies stemfrom the neglect of natural decay observed in marine systems and inadequatemodeling of wave-structure interaction (WSI). To overcome these challenges,this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) moduleincorporates a learnable temporal decay to assign higher weights to recentstates, aiming to emulate the natural decay phenomenon. Meanwhile, the phasedifferences guided bidirectional cross-attention (PDG-BCA) module explicitlycaptures the bidirectional interaction and phase relationship between waves andthe structure using a cosine-based bias within a bidirectionalcross-computation paradigm. These streams are synergistically integratedthrough a global context fusion (GCF) module. Finally, PhysAttnNet is trainedwith a hybrid time-frequency loss that jointly minimizes time-domain predictionerrors and frequency-domain spectral discrepancies. Comprehensive experimentson wave flume datasets demonstrate that PhysAttnNet significantly outperformsmainstream models. Furthermore,cross-scenario generalization tests validate themodel's robustness and adaptability to unseen environments, highlighting itspotential as a framework to develop predictive models for complex systems inocean engineering.</description>
      <author>example@mail.com (Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang)</author>
      <guid isPermaLink="false">2510.14250v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为动态人格完善框架(DPRF)的新方法，用于优化大型语言模型角色扮演代理的行为与目标个体行为的一致性，通过迭代识别认知差异并完善人格配置文件，显著提高了行为对齐度。&lt;h4&gt;背景&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。&lt;h4&gt;目的&lt;/h4&gt;解决LLM RPAs行为与目标个体行为不一致的问题，通过优化LLM RPAs的行为与目标个体行为的对齐度。&lt;h4&gt;方法&lt;/h4&gt;提出动态人格完善框架(DPRF)，通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异。在四个多样化的行为预测场景(正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论)中使用五个大型语言模型评估DPRF。&lt;h4&gt;主要发现&lt;/h4&gt;DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。为解决这一限制，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在四个多样化的行为预测场景中使用五个大型语言模型评估DPRF：正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论。DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。我们的研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation</title>
      <link>http://arxiv.org/abs/2510.12815v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)在离线设置下面临数据效率低和依赖预收集轨迹的挑战。本文提出了一种名为DAC4Rec的新框架，整合扩散过程与强化学习，有效解决噪声数据处理和长期用户偏好捕捉问题。&lt;h4&gt;背景&lt;/h4&gt;基于强化学习的推荐系统能够适应用户的动态偏好，但在离线设置下面临数据效率低和依赖预收集轨迹的挑战。离线强化学习方法利用大量数据解决这些问题，但往往难以处理嘈杂数据且无法捕捉长期用户偏好。&lt;h4&gt;目的&lt;/h4&gt;克服现有离线强化学习推荐系统的局限性，提出一种新的框架来更有效地建模复杂的用户偏好。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Diffusion-enhanced Actor-Critic for Offline RL4RS (DAC4Rec)的新框架，该框架整合了扩散过程与强化学习。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，还引入了一种基于能量的采样策略来减少推荐生成过程中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在六个真实世界离线数据集和在线模拟环境中的大量实验验证了DAC4Rec的有效性，证明其能够优化长期用户偏好。此外，提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，展示了其多功能性和广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;DAC4Rec框架通过整合扩散过程与强化学习，有效解决了离线强化学习推荐系统中的数据效率、噪声处理和长期偏好捕捉等问题。&lt;h4&gt;翻译&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)因其能够适应动态用户偏好而受到关注。然而，这些系统面临挑战，特别是在离线设置中，数据效率低下和对预收集轨迹的依赖限制了它们的广泛应用。虽然离线强化学习方法利用大量数据来解决这些问题，但它们通常难以处理嘈杂数据且无法捕捉长期用户偏好，导致次优的推荐策略。为了克服这些局限性，我们提出了用于离线RL4RS的扩散增强型Actor-Critic(DAC4Rec)，这是一个将扩散过程与强化学习相结合的新颖框架，能够更有效地建模复杂的用户偏好。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，我们引入了一种基于能量的采样策略来减少推荐生成过程中的随机性，确保更有针对性和可靠的结果。我们在六个真实世界的离线数据集和在线模拟环境中通过大量实验验证了DAC4Rec的有效性，证明了其优化长期用户偏好的能力。此外，我们表明所提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，突显了其多功能性和广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning-based recommender systems (RL4RS) have gainedattention for their ability to adapt to dynamic user preferences. However,these systems face challenges, particularly in offline settings, where datainefficiency and reliance on pre-collected trajectories limit their broaderapplicability. While offline reinforcement learning methods leverage extensivedatasets to address these issues, they often struggle with noisy data and failto capture long-term user preferences, resulting in suboptimal recommendationpolicies. To overcome these limitations, we propose Diffusion-enhancedActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integratesdiffusion processes with reinforcement learning to model complex userpreferences more effectively. DAC4Rec leverages the denoising capabilities ofdiffusion models to enhance the robustness of offline RL algorithms andincorporates a Q-value-guided policy optimization strategy to better handlesuboptimal trajectories. Additionally, we introduce an energy-based samplingstrategy to reduce randomness during recommendation generation, ensuring moretargeted and reliable outcomes. We validate the effectiveness of DAC4Recthrough extensive experiments on six real-world offline datasets and in anonline simulation environment, demonstrating its ability to optimize long-termuser preferences. Furthermore, we show that the proposed diffusion policy canbe seamlessly integrated into other commonly used RL algorithms in RL4RS,highlighting its versatility and wide applicability.</description>
      <author>example@mail.com (Xiaocong Chen, Siyu Wang, Lina Yao)</author>
      <guid isPermaLink="false">2510.12815v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: Synthetic LiDAR Datasets (LUMPI, V2X-Real-IC, and TUMTraf-I)</title>
      <link>http://arxiv.org/abs/2509.06781v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本，每个包含10K个注释帧，具有丰富的标注信息，能够有效支持深度学习模型训练。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据集进行模型训练，但真实数据集获取和标注成本高，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高保真合成数据集，能够独立使用或增强现有数据集，用于激光雷达感知任务，并探索其能否替代同领域真实世界数据集。&lt;h4&gt;方法&lt;/h4&gt;基于实际位置的几何特征、道路对齐和交通模式构建数字孪生环境，使用模拟激光雷达传感器合成数据，添加3D边界框、实例分割和语义分割等标注，并通过统计和结构相似性分析评估数据质量。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据集与真实数据高度相似，仅使用合成数据训练的模型在真实未见数据上表现优于使用真实数据训练的模型，数据集通过增加样本量和场景多样性有效增强了基准数据集。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替换同领域真实世界数据集的数字合成数据集，提供了高保真数据副本，支持自定义场景测试，已公开可供使用。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本。每个UrbanTwin数据集包含10K个注释帧，对应一个公共数据集。注释包括6个类别的3D边界框、实例分割标签和跟踪ID，以及9个类别的语义分割标签。这些数据集使用模拟激光雷达传感器在真实数字孪生中合成，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆移动模式进行建模。由于精确的数字孪生建模，合成数据集与真实数据集很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高质量激光雷达数据集创建困难的问题。真实世界数据收集和标注成本高、耗时长，限制了智能交通系统感知算法的发展。这个问题很重要，因为激光雷达是智能交通系统中的关键技术，而高质量数据集对于训练和评估3D感知算法至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有模拟环境虽然功能强大，但与真实世界存在差距。他们借鉴了数字孪生概念，结合了CARLA模拟器和现有路边激光雷达数据集的特点。作者强调需要同时建模静态元素(如几何结构)和动态行为(如交通模式)，而非仅依赖手工制作的3D资产和简化的物理假设。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用数字孪生技术创建真实世界场景的高保真虚拟副本，模拟激光雷达传感器生成与真实数据相似的点云，并提供丰富一致的标注。实现流程包括：1)使用卫星图像和真实位置数据构建环境；2)配置虚拟传感器匹配真实规格；3)随机生成符合交通规则的动态元素；4)在CARLA模拟器中生成10K帧带标注的合成数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门为路边激光雷达应用创建合成数据集；采用高保真数字孪生同时整合静态和动态元素；合成数据与真实数据高度相似；证明完全在模拟数据上训练的模型可匹敌真实数据训练效果。相比之前工作，UrbanTwin专门增强真实世界基准而非通用模拟，在模拟过程中而非事后缩小sim-to-real差距，提供完整标注支持多种感知任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过创建基于数字孪生的高保真合成激光雷达数据集，成功解决了真实世界数据集创建成本高昂且sim-to-real差距大的问题，使模型能在合成数据上训练并有效应用于真实世界感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets, high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC}}, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor Unlearning by Linear Task Decomposition</title>
      <link>http://arxiv.org/abs/2510.14845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决了基础模型中后门攻击的安全问题，提出了一种基于后门与良性任务解耦特性的简单遗忘方法，能够在不损害模型通用能力的情况下有效移除后门。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。&lt;h4&gt;目的&lt;/h4&gt;回答后门是否可以在不损害模型通用能力的情况下被移除这一问题，并研究后门如何在模型权重空间中被编码。&lt;h4&gt;方法&lt;/h4&gt;研究后门与良性任务在模型权重空间中的解耦特性，基于这种分离开发一种简单的遗忘方法，能够隔离和擦除后门对模型的影响，同时保持干净性能。通过基于CLIP的模型和常见对抗触发器进行大量实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;后门与其他良性任务是解耦的；给定攻击知识的情况下，方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率；即使当攻击及其存在未知时，方法也能通过反向工程触发器的适当估计成功遗忘后门；与当前最先进的防御相比，方法始终产生更好的遗忘和干净准确率权衡。&lt;h4&gt;结论&lt;/h4&gt;该方法在移除后门的同时，有效保留了模型的通用能力，为解决基础模型的安全问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。现有的后门移除方法依赖于昂贵的微调来覆盖有害行为，并且通常会降低在其他不相关任务上的性能。这引发了一个问题：后门是否可以在不损害模型通用能力的情况下被移除。在本研究中，我们解决了这个问题，并研究了后门如何在模型权重空间中被编码，发现它们与其他良性任务是解耦的。具体而言，这种分离使得能够隔离和擦除后门对模型的影响，同时对干净性能的影响最小。基于这一见解，我们引入了一种利用这种解耦特性的简单遗忘方法。通过对基于CLIP的模型和常见对抗触发器的大量实验，我们表明，给定攻击知识的情况下，我们的方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率。此外，我们证明即使当攻击及其存在未知时，我们的方法也能通过使用反向工程触发器的适当估计成功遗忘后门。总体而言，与当前最先进的防御相比，我们的方法始终产生更好的遗忘和干净准确率权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have revolutionized computer vision by enabling broadgeneralization across diverse tasks. Yet, they remain highly susceptible toadversarial perturbations and targeted backdoor attacks. Mitigating suchvulnerabilities remains an open challenge, especially given that thelarge-scale nature of the models prohibits retraining to ensure safety.Existing backdoor removal approaches rely on costly fine-tuning to override theharmful behavior, and can often degrade performance on other unrelated tasks.This raises the question of whether backdoors can be removed withoutcompromising the general capabilities of the models. In this work, we addressthis question and study how backdoors are encoded in the model weight space,finding that they are disentangled from other benign tasks. Specifically, thisseparation enables the isolation and erasure of the backdoor's influence on themodel with minimal impact on clean performance. Building on this insight, weintroduce a simple unlearning method that leverages such disentanglement.Through extensive experiments with CLIP-based models and common adversarialtriggers, we show that, given the knowledge of the attack, our method achievesapproximately perfect unlearning, while retaining, on average, 96% of cleanaccuracy. Additionally, we demonstrate that even when the attack and itspresence are unknown, our method successfully unlearns backdoors by properestimation using reverse-engineered triggers. Overall, our method consistentlyyields better unlearning and clean accuracy tradeoffs when compared to presentstate-of-the-art defenses.</description>
      <author>example@mail.com (Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard)</author>
      <guid isPermaLink="false">2510.14845v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&amp;E Whole Slide Images</title>
      <link>http://arxiv.org/abs/2510.14800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为PRISM的新型可解释AI模型，用于结直肠癌预后预测。该模型通过整合连续变异性谱的形态学信息，能够更准确地捕捉肿瘤的渐进式进化过程，并在III期结直肠癌患者中展现出优异的预后预测性能。&lt;h4&gt;背景&lt;/h4&gt;结直肠癌是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。当前计算机病理学中的基础模型主要采用任务无关的方法学，可能忽略器官特定的关键形态学模式，而这些模式对肿瘤行为、治疗反应和患者结局有重要影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），纳入每种不同形态内的连续变异性谱以表征表型多样性，反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。&lt;h4&gt;方法&lt;/h4&gt;PRISM模型在874万张组织学图像上进行训练，这些图像来自424名III期结直肠癌患者的手术切除标本。模型整合了空间形态学信息，以捕捉肿瘤的形态学变异性。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM在五年总生存期(OS)预后方面表现优越：AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；风险比(HR) = 3.34，95% CI = 2.28-4.90，p &lt; 0.0001。模型优于现有的结直肠癌特异性方法15%，比AI基础模型高约23%的准确率。PRISM显示性别无关的稳健性，在临床病理亚组中表现稳定，在不同治疗方案间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;h4&gt;结论&lt;/h4&gt;PRISM模型在结直肠癌预后预测方面表现优异，能够更好地捕捉肿瘤的形态学变异性，对不同治疗方案的患者预后有稳定的预测能力，为临床决策提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;结直肠癌(CRC)仍然是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。最近，计算病理学中基础模型的进展主要是由任务无关的方法学推动的，这些方法可能忽略器官特定的关键形态学模式，这些模式代表不同的生物学过程，能从根本上影响肿瘤行为、治疗反应和患者结局。本研究旨在开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），该模型纳入了每种不同形态内的连续变异性谱，以表征表型多样性，并反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。PRISM在从424名III期CRC患者的手术切除标本中提取的874万张组织学图像上进行训练。PRISM在五年OS预后方面取得了优异的性能（AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；HR = 3.34，95% CI = 2.28-4.90；p &lt; 0.0001），比现有的CRC特异性方法高出15%，比AI基础模型高出约23%的准确率。它显示出性别无关的稳健性（AUC差值 = 0.02；准确率差值 = 0.15%），并在临床病理亚组中表现稳定，在5FU/LV和CPT-11/5FU/LV治疗方案之间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer (CRC) remains the third most prevalent malignancy globally,with approximately 154,000 new cases and 54,000 projected deaths anticipatedfor 2025. The recent advancement of foundation models in computationalpathology has been largely propelled by task agnostic methodologies that canoverlook organ-specific crucial morphological patterns that represent distinctbiological processes that can fundamentally influence tumor behavior,therapeutic response, and patient outcomes. The aim of this study is to developa novel, interpretable AI model, PRISM (Prognostic Representation of IntegratedSpatial Morphology), that incorporates a continuous variability spectrum withineach distinct morphology to characterize phenotypic diversity and reflectingthe principle that malignant transformation occurs through incrementalevolutionary processes rather than abrupt phenotypic shifts. PRISM is trainedon 8.74 million histological images extracted from surgical resection specimensof 424 patients with stage III CRC. PRISM achieved superior prognosticperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;HR = 3.34, 95% CI = 2.28-4.90; p &lt; 0.0001), outperforming existing CRC-specificmethods by 15% and AI foundation models by ~23% accuracy. It showedsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stableperformance across clinicopathological subgroups, with minimal accuracyfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,replicating the Alliance cohort finding of no survival difference betweentreatments.</description>
      <author>example@mail.com (Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi)</author>
      <guid isPermaLink="false">2510.14800v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes</title>
      <link>http://arxiv.org/abs/2510.14763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在创意写作方面的局限性，特别是在非英语环境中的不足，提出了一个新颖的中文创意写作数据集COIG-Writer，并通过实验确定了创意写作的双组分模型及其关键发现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在创意写作方面存在系统性缺陷，特别是在非英语语境中，训练数据稀缺且缺乏过程层面的监督。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多样化输出及其背后思维过程的中文创意写作数据集，并研究创意写作的构成要素和优化方法。&lt;h4&gt;方法&lt;/h4&gt;创建了COIG-Writer数据集，包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含逆向工程提示、详细创意推理和最终文本。通过全面实验分析创意写作的构成要素和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 过程监督非常有效，但需要通用数据稳定化，至少需要一个创意样本对应十二个通用样本才能实现最佳性能；2. 创意能力具有文化局限性，没有跨语言迁移能力，中文和英文表现之间有89.26百分点的差距；3. 词汇多样性与创意质量呈负相关（TTR悖论），高多样性信号表明对逻辑缺陷的补偿行为。&lt;h4&gt;结论&lt;/h4&gt;创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。创意写作需要过程监督和通用数据的适当平衡。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在创意写作方面表现出系统性缺陷，特别是在非英语环境中，训练数据稀缺且缺乏过程层面的监督。我们提出了COIG-Writer，这是一个新颖的中文创意写作数据集，通过对高质量文本进行系统性的逆向工程，捕捉多样化的输出及其背后的思维过程。与仅提供输入-输出对的数据集不同，COIG-Writer包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含：(1)逆向工程提示，(2)详细创意推理记录决策过程，(3)最终文本。通过全面实验，我们确定了创意写作的双组分模型：叙事逻辑（由过程监督提供）和语言表达（由通用数据维持）。我们的研究揭示了三个关键见解：(1)过程监督非常有效，但需要通用数据稳定化。至少需要一个创意样本对应十二个通用样本的比例才能实现最佳性能；低于此阈值，胜率会逐渐下降（从62.75%降至35.78%）；(2)创意能力具有文化局限性，没有跨语言迁移能力（中文和英文表现之间有89.26百分点的差距）；(3)词汇多样性与创意质量呈负相关（TTR悖论），表明高多样性信号表明对逻辑缺陷的补偿行为。这些发现表明，创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models exhibit systematic deficiencies in creative writing,particularly in non-English contexts where training data is scarce and lacksprocess-level supervision. We present COIG-Writer, a novel Chinese creativewriting dataset that captures both diverse outputs and their underlying thoughtprocesses through systematic reverse-engineering of high-quality texts. Unlikeexisting datasets that provide only input-output pairs, COIG-Writer comprises1,665 meticulously curated triplets spanning 51 genres, each containing: (1) areverse-engineered prompt, (2) detailed creative reasoning documentingdecision-making processes, and (3) the final text. Through comprehensiveexperiments, we identify a two-component model of creative writing: narrativelogic (provided by process supervision) and linguistic expression (maintainedby general-purpose data). Our findings reveal three critical insights: (1)Process supervision is highly effective but requires stabilization with generaldata. A ratio of at least one creative sample to twelve general samples isneeded to achieve optimal performance; below this threshold, the win rateprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilitiesare culturally-bound with no cross-lingual transfer (89.26pp gap betweenChinese and English performance), and (3) lexical diversity inverselycorrelates with creative quality (TTR paradox), suggesting high diversitysignals compensatory behavior for logical deficiencies. These findingsestablish that creative excellence emerges from the interaction between logicalscaffolding and linguistic grounding, analogous to how mathematical reasoningenhances but cannot replace linguistic competence in foundation models.</description>
      <author>example@mail.com (Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang)</author>
      <guid isPermaLink="false">2510.14763v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>State-Space Models for Tabular Prior-Data Fitted Networks</title>
      <link>http://arxiv.org/abs/2510.14573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用Hydra（一种双向线性时间结构状态空间模型）替代TabPFN中的Transformer架构，以解决Transformer的二次复杂度问题，同时保持预测性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在表格数据领域取得了进展，如TabPFN展示了预训练Transformer架构可以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。&lt;h4&gt;目的&lt;/h4&gt;研究Hydra作为TabPFN中Transformer替代方案的潜力，解决SSM对输入标记顺序的固有敏感性这一关键挑战，特别是在表格数据集中行顺序语义无意义的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究双向方法能在多大程度上保持效率并实现对称上下文聚合，以减少SSM对输入顺序的依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型相当的预测性能。&lt;h4&gt;结论&lt;/h4&gt;双向Hydra模型可以作为TabPFN中Transformer的有效替代方案，在保持预测性能的同时提高效率。&lt;h4&gt;翻译&lt;/h4&gt;最近在表格数据基础模型方面的进展，如TabPFN，表明预训练的Transformer架构可以以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。在这项工作中，我们研究了使用Hydra（一种双向线性时间结构状态空间模型SSM）作为TabPFN中Transformer替代方案的潜力。一个关键挑战在于SSM对输入标记顺序的固有敏感性——对于行顺序在语义上无意义的表格数据集来说，这是一个不希望有的特性。我们研究了双向方法在多大程度上可以保持效率并实现对称上下文聚合。我们的实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型具有竞争力的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models for tabular data, such as TabPFN,demonstrated that pretrained Transformer architectures can approximate Bayesianinference with high predictive performance. However, Transformers suffer fromquadratic complexity with respect to sequence length, motivating theexploration of more efficient sequence models. In this work, we investigate thepotential of using Hydra, a bidirectional linear-time structured state spacemodel (SSM), as an alternative to Transformers in TabPFN. A key challenge liesin SSM's inherent sensitivity to the order of input tokens - an undesirableproperty for tabular datasets where the row order is semantically meaningless.We investigate to what extent a bidirectional approach can preserve efficiencyand enable symmetric context aggregation. Our experiments show that thisapproach reduces the order-dependence, achieving predictive performancecompetitive to the original TabPFN model.</description>
      <author>example@mail.com (Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler)</author>
      <guid isPermaLink="false">2510.14573v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</title>
      <link>http://arxiv.org/abs/2510.14532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DentVFM是首个专为牙科设计的视觉基础模型系列，解决了现有牙科AI系统的局限性，通过自监督学习和大规模多模态数据集训练，展现出卓越的泛化能力和跨模态诊断性能。&lt;h4&gt;背景&lt;/h4&gt;口腔颌面放射学在牙科医疗中至关重要，但受专业人才短缺限制。现有牙科AI系统因单一模态关注、任务特定设计和依赖标记数据而泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有AI系统局限性的牙科视觉基础模型，实现更广泛的应用和更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;创建DentVFM模型系列，使用DentVista数据集(约160万多模态放射图像)进行自监督学习，基于Vision Transformer架构开发2D和3D变体，并建立DentBench基准测试涵盖8个牙科亚专科。&lt;h4&gt;主要发现&lt;/h4&gt;DentVFM表现出通用智能，能推广到多种牙科任务；显著优于各类基线模型；提供更好的泛化能力、标签效率和可扩展性；在跨模态诊断中表现优于经验丰富的牙医。&lt;h4&gt;结论&lt;/h4&gt;DentVFM为牙科AI树立新范式，提供可扩展、适应性强且标签高效的模型，有助于改善智能牙科医疗保健并解决全球口腔医疗保健差距。&lt;h4&gt;翻译&lt;/h4&gt;口腔颌面放射学在牙科医疗保健中起着重要作用，但放射图像解读受到训练专业人员短缺的限制。虽然AI方法显示出前景，但现有牙科AI系统受限于其单一模态关注、任务特定设计和依赖昂贵的标记数据，阻碍了它们在多样化临床场景中的泛化能力。为解决这些挑战，我们引入了DentVFM，这是首个为牙科设计的视觉基础模型系列。DentVFM为广泛的牙科应用生成任务无关的视觉表示，并在DentVista上使用自监督学习，这是一个精心策划的大型牙科成像数据集，包含来自不同医疗中心的约160万张多模态放射图像。DentVFM基于Vision Transformer架构包含2D和3D变体。为解决牙科智能评估和基准测试的空白，我们引入了DentBench，这是一个全面的基准测试，涵盖八个牙科亚专科、更多疾病、成像方式和广泛的地理分布。DentVFM表现出令人印象深刻的通用智能，展示了向多样化牙科任务的稳健泛化能力，如疾病诊断、治疗分析、生物标志物识别以及解剖标志物检测和分割。实验结果表明，DentVFM显著优于监督、自监督和弱监督基线，提供更好的泛化能力、标签效率和可扩展性。此外，DentVFM实现跨模态诊断，在常规成像不可用的情况下提供比经验丰富的牙医更可靠的结果。DentVFM为牙科AI树立了新范式，提供可扩展、适应性强且标签高效的模型，以改善智能牙科医疗保健并解决全球口腔医疗保健中的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oral and maxillofacial radiology plays a vital role in dental healthcare, butradiographic image interpretation is limited by a shortage of trainedprofessionals. While AI approaches have shown promise, existing dental AIsystems are restricted by their single-modality focus, task-specific design,and reliance on costly labeled data, hindering their generalization acrossdiverse clinical scenarios. To address these challenges, we introduce DentVFM,the first family of vision foundation models (VFMs) designed for dentistry.DentVFM generates task-agnostic visual representations for a wide range ofdental applications and uses self-supervised learning on DentVista, a largecurated dental imaging dataset with approximately 1.6 million multi-modalradiographic images from various medical centers. DentVFM includes 2D and 3Dvariants based on the Vision Transformer (ViT) architecture. To address gaps indental intelligence assessment and benchmarks, we introduce DentBench, acomprehensive benchmark covering eight dental subspecialties, more diseases,imaging modalities, and a wide geographical distribution. DentVFM showsimpressive generalist intelligence, demonstrating robust generalization todiverse dental tasks, such as disease diagnosis, treatment analysis, biomarkeridentification, and anatomical landmark detection and segmentation.Experimental results indicate DentVFM significantly outperforms supervised,self-supervised, and weakly supervised baselines, offering superiorgeneralization, label efficiency, and scalability. Additionally, DentVFMenables cross-modality diagnostics, providing more reliable results thanexperienced dentists in situations where conventional imaging is unavailable.DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, andlabel-efficient model to improve intelligent dental healthcare and addresscritical gaps in global oral healthcare.</description>
      <author>example@mail.com (Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang)</author>
      <guid isPermaLink="false">2510.14532v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision Mamba for Permeability Prediction of Porous Media</title>
      <link>http://arxiv.org/abs/2510.14516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，并证明了其相比ViTs和CNNs的优势。&lt;h4&gt;背景&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这使得Vision Mamba在计算和内存效率方面更具优势。此外，Vision Mamba比传统卷积神经网络(CNN)需要更少的可训练参数，因此内存效率更高。&lt;h4&gt;目的&lt;/h4&gt;首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，比较Vision Mamba与ViT和CNN模型在渗透率预测多个方面的性能，并进行消融研究以评估其组件对准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;构建了一个使用Vision Mamba作为主干网络的神经网络来预测三维多孔介质的渗透率，并与ViT和CNN模型进行了性能比较，进行了消融研究评估组件对准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有计算效率高、内存占用少、参数量少等优势。&lt;h4&gt;结论&lt;/h4&gt;作者认为提出的框架有潜力集成到使用Vision Mamba替代ViTs的大型视觉模型中，并已公开源代码以促进可重复性并使其他研究人员能够在此基础上进行扩展。&lt;h4&gt;翻译&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这一特性提高了计算和内存效率。此外，Vision Mamba比传统卷积神经网络(CNN)需要少得多的可训练参数，因此可以更节省内存。由于这些特性，我们首次引入了一个使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络。我们在渗透率预测的多个方面比较了Vision Mamba与ViT和CNN模型的性能，并进行了消融研究以评估其组件对准确性的影响。我们通过实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有上述优势。我们公开源代码以促进可重复性，并使其他研究人员能够在此基础上进行扩展和延伸。我们认为，在Vision Mamba替代ViTs的大型视觉模型中，所提出的框架具有集成潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Mamba has recently received attention as an alternative to VisionTransformers (ViTs) for image classification. The network size of Vision Mambascales linearly with input image resolution, whereas ViTs scale quadratically,a feature that improves computational and memory efficiency. Moreover, VisionMamba requires a significantly smaller number of trainable parameters thantraditional convolutional neural networks (CNNs), and thus, they can be morememory efficient. Because of these features, we introduce, for the first time,a neural network that uses Vision Mamba as its backbone for predicting thepermeability of three-dimensional porous media. We compare the performance ofVision Mamba with ViT and CNN models across multiple aspects of permeabilityprediction and perform an ablation study to assess the effects of itscomponents on accuracy. We demonstrate in practice the aforementionedadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction ofthree-dimensional porous media. We make the source code publicly available tofacilitate reproducibility and to enable other researchers to build on andextend this work. We believe the proposed framework has the potential to beintegrated into large vision models in which Vision Mamba is used instead ofViTs.</description>
      <author>example@mail.com (Ali Kashefi, Tapan Mukerji)</author>
      <guid isPermaLink="false">2510.14516v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review</title>
      <link>http://arxiv.org/abs/2510.14462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新研究进展，涵盖了2018-2025年间的49项研究，表明这些模型在大局灶性病变检测和微妙异常识别方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。&lt;h4&gt;目的&lt;/h4&gt;综合关于无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型，并比较其性能指标和架构设计选择。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA指导的范围综述方法，系统检索并分析了2018-2025年间发表的49项研究，这些研究应用了各种生成模型于脑MRI和CT影像，用于检测肿瘤、中风、多发性硬化和小血管疾病等多种病理。&lt;h4&gt;主要发现&lt;/h4&gt;生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展；其关键优势是能够产生可解释的伪健康重建，这在注释数据稀缺的情况下（如罕见或异质性疾病）特别有价值。&lt;h4&gt;结论&lt;/h4&gt;这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射；未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;h4&gt;翻译&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的替代性有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。共确定了2018-2025年间发表的49项研究，涵盖了脑MRI和较少见的CT应用，应用于肿瘤、中风、多发性硬化和小血管疾病等多种病理。报告的性能指标与架构设计选择进行了比较。在纳入的研究中，生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展。生成模型的一个关键优势是它们能够产生可解释的伪健康（也称为反事实）重建，这在注释数据稀缺时（如罕见或异质性疾病）特别有价值。展望未来，这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射。为实现临床影响，未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised deep generative models are emerging as a promising alternativeto supervised methods for detecting and segmenting anomalies in brain imaging.Unlike fully supervised approaches, which require large voxel-level annotateddatasets and are limited to well-characterised pathologies, these models can betrained exclusively on healthy data and identify anomalies as deviations fromlearned normative brain structures. This PRISMA-guided scoping reviewsynthesises recent work on unsupervised deep generative models for anomalydetection in neuroimaging, including autoencoders, variational autoencoders,generative adversarial networks, and denoising diffusion models. A total of 49studies published between 2018 - 2025 were identified, covering applications tobrain MRI and, less frequently, CT across diverse pathologies such as tumours,stroke, multiple sclerosis, and small vessel disease. Reported performancemetrics are compared alongside architectural design choices. Across theincluded studies, generative models achieved encouraging performance for largefocal lesions and demonstrated progress in addressing more subtleabnormalities. A key strength of generative models is their ability to produceinterpretable pseudo-healthy (also referred to as counterfactual)reconstructions, which is particularly valuable when annotated data are scarce,as in rare or heterogeneous diseases. Looking ahead, these models offer acompelling direction for anomaly detection, enabling semi-supervised learning,supporting the discovery of novel imaging biomarkers, and facilitating within-and cross-disease deviation mapping in unified end-to-end frameworks. Torealise clinical impact, future work should prioritise anatomy-aware modelling,development of foundation models, task-appropriate evaluation metrics, andrigorous clinical validation.</description>
      <author>example@mail.com (Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi)</author>
      <guid isPermaLink="false">2510.14462v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Centric Activation and Coordination for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.14349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了VaCo方法，通过视觉中心激活和多视觉基础模型的协调来优化多模态大语言模型(MLLMs)的表示，提高模型在视觉理解方面的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)通过整合视觉编码器的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。&lt;h4&gt;目的&lt;/h4&gt;解决主流MLLMs忽视关键视觉中心信息的问题，通过引入视觉中心激活和协调机制，优化MLLMs的表示，提高其视觉理解能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VaCo方法，包括：视觉判别对齐整合从多个视觉基础模型(VFMs)中提取的任务感知特征；可学习的模块化任务查询(MTQs)在多种VFMs的监督下激活特定视觉信号；视觉对齐层(VALs)整合到MLLMs中；标记网关掩码(TGM)限制多组MTQs之间的信息流，协调VFMs之间的表示冲突。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;h4&gt;结论&lt;/h4&gt;VaCo通过有效整合多种视觉基础模型的特征，解决了主流MLLMs忽视视觉中心信息的问题，显著提升了模型在视觉理解任务上的表现。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)整合视觉编码器中的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。为了解决这一困境，我们引入了VaCo，它通过多个视觉基础模型(VFMs)的视觉中心激活和协调来优化MLLM表示。VaCo引入视觉判别对齐来整合从VFMs中提取的任务感知特征，从而统一MLMs中文本和视觉输出的优化。具体来说，我们将可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)整合到MLLMs中，在多种VFMs的监督下激活特定的视觉信号。为了协调VFMs之间的表示冲突，精心设计的标记网关掩码(TGM)限制了多组MTQs之间的信息流。大量实验证明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) integrate image features from visualencoders with LLMs, demonstrating advanced comprehension capabilities. However,mainstream MLLMs are solely supervised by the next-token prediction of textualtokens, neglecting critical vision-centric information essential for analyticalabilities. To track this dilemma, we introduce VaCo, which optimizes MLLMrepresentations through Vision-Centric activation and Coordination frommultiple vision foundation models (VFMs). VaCo introduces visual discriminativealignment to integrate task-aware perceptual features extracted from VFMs,thereby unifying the optimization of both textual and visual outputs in MLLMs.Specifically, we incorporate the learnable Modular Task Queries (MTQs) andVisual Alignment Layers (VALs) into MLLMs, activating specific visual signalsunder the supervision of diverse VFMs. To coordinate representation conflictsacross VFMs, the crafted Token Gateway Mask (TGM) restricts the informationflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCosignificantly improves the performance of different MLLMs on variousbenchmarks, showcasing its superior capabilities in visual comprehension.</description>
      <author>example@mail.com (Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin)</author>
      <guid isPermaLink="false">2510.14349v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</title>
      <link>http://arxiv.org/abs/2510.14270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GauSSmart的混合方法，通过结合2D基础模型和3D高斯飞溅重建技术，解决了Gaussian Splatting在捕捉精细细节和稀疏覆盖区域保持真实感方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;场景重建是计算机视觉中的核心挑战，NeRF和Gaussian Splatting等方法取得了显著进展。但Gaussian Splatting在大规模数据集上表现良好时，往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法，以提升场景重建的质量和细节表现。&lt;h4&gt;方法&lt;/h4&gt;集成成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，利用2D分割先验和高维特征嵌入，指导高斯飞溅的密集化和细化，改善代表性不足区域的覆盖，并保持复杂的结构细节。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的验证表明，GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法，能够更好地捕捉场景细节并提高稀疏覆盖区域的重建质量。&lt;h4&gt;结论&lt;/h4&gt;混合2D-3D方法具有巨大潜力，将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法的固有局限性。&lt;h4&gt;翻译&lt;/h4&gt;场景重建已成为计算机视觉中的一个核心挑战，诸如神经辐射场和高斯飞溅等方法已取得显著进展。虽然高斯飞溅在大规模数据集上表现出色，但它往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。在本工作中，我们提出了GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法。我们的方法集成了成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，以增强基于高斯的场景重建。通过利用2D分割先验和高维特征嵌入，我们的方法指导高斯飞溅的密集化和细化，改善了代表性不足区域的覆盖并保持了复杂的结构细节。我们在三个数据集上验证了我们的方法，其中GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法。我们的结果证明了混合2D-3D方法的巨大潜力，强调了如何将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法所固有的局限性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景重建中细节捕捉不足和稀疏覆盖区域真实感差的问题。这个问题很重要，因为高质量的3D重建对虚拟现实、增强现实、自动驾驶等应用至关重要，而现有方法在处理细节和稀疏区域时存在局限性，限制了重建质量和技术应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何结合2D基础模型和3D重建的优势，认识到2D视觉技术（如分割和特征提取）成熟而3D方法擅长空间建模。他们借鉴了DINO等基础模型的语义特征、SAM的图像分割能力，以及凸包过滤技术，并将这些2D方法与3D高斯溅射流程巧妙融合，形成互补优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型的语义信息指导3D高斯溅射优化，改善点云质量并增强稀疏区域。整体流程包括：1)使用凸包过滤去除点云异常值；2)通过相机聚类选择代表性图像；3)应用SAM进行图像分割并关联3D点；4)基于分割掩码有针对性地增强点云密度；5)引入DINOv3特征嵌入损失提高语义一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)凸包引导的异常值去除方法；2)感知的点云增强策略，考虑语义区域重要性；3)基于DINOv3的嵌入对齐训练损失。相比之前工作，不同之处在于：不是简单拼接2D和3D方法，而是设计真正融合框架；利用语义先验指导3D重建；点云增强考虑语义区域重要性；使用特征嵌入损失而非仅传统光度损失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GauSSmart通过融合2D基础模型的语义理解与3D高斯溅射的空间建模能力，有效提升了3D场景重建中的细节捕捉和稀疏区域真实感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene reconstruction has emerged as a central challenge in computer vision,with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splattingachieving remarkable progress. While Gaussian Splatting demonstrates strongperformance on large-scale datasets, it often struggles to capture fine detailsor maintain realism in regions with sparse coverage, largely due to theinherent limitations of sparse 3D training data.  In this work, we propose GauSSmart, a hybrid method that effectively bridges2D foundational models and 3D Gaussian Splatting reconstruction. Our approachintegrates established 2D computer vision techniques, including convexfiltering and semantic feature supervision from foundational models such asDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2Dsegmentation priors and high-dimensional feature embeddings, our method guidesthe densification and refinement of Gaussian splats, improving coverage inunderrepresented areas and preserving intricate structural details.  We validate our approach across three datasets, where GauSSmart consistentlyoutperforms existing Gaussian Splatting in the majority of evaluated scenes.Our results demonstrate the significant potential of hybrid 2D-3D approaches,highlighting how the thoughtful combination of 2D foundational models with 3Dreconstruction pipelines can overcome the limitations inherent in eitherapproach alone.</description>
      <author>example@mail.com (Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang)</author>
      <guid isPermaLink="false">2510.14270v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals</title>
      <link>http://arxiv.org/abs/2510.14254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基础模型在时间序列分析中的应用，特别是比较了专家模型和通用模型在生理信号处理（特别是PPG信号）上的性能差异。&lt;h4&gt;背景&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练后可适应各种下游任务，已广泛应用于自然语言处理和计算机视觉领域。时间序列分析领域，特别是生理信号处理，正逐渐受到关注，但大多数时间序列基础模型是专家模型，只在同类型数据上预训练和测试，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT尝试训练跨多个领域的通用时间序列基础模型。&lt;h4&gt;目的&lt;/h4&gt;进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。&lt;h4&gt;方法&lt;/h4&gt;通过总共51个任务组成的测试套件进行评估，包括心脏状态评估、实验室值估计和跨模态推理。在七个维度上全面评估两种模型：获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉模型在不同微调策略下的能力、适应性和效率。在完整微调场景下比较模型性能，并提供泛化、公平性、注意力可视化和训练数据选择重要性的进一步分析。&lt;h4&gt;主要发现&lt;/h4&gt;在完整微调场景下，专家模型的获胜分数比通用模型高27%。&lt;h4&gt;结论&lt;/h4&gt;论文提供了专家模型和通用模型在多样化下游场景中的优势和局限性的全面理解。&lt;h4&gt;翻译&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练，并可适应各种下游任务。它们已广泛应用于自然语言处理和计算机视觉任务，如GPT、BERT和CLIP等模型。现在，时间序列分析领域，特别是生理信号处理，也日益受到关注。然而，大多数时间序列基础模型是专家模型，其预训练和测试使用相同类型的数据，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT，使用来自多个领域（如天气、交通和电力）的数据训练通用时间序列基础模型。本文旨在进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。通过涵盖心脏状态评估、实验室值估计和跨模态推理的51个任务，我们在七个维度上全面评估了两种模型，包括获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉了模型在不同微调策略下的能力、适应性和效率，为它们在多样化下游场景中的优势和局限性提供了全面理解。在完整微调场景下，我们证明专家模型的获胜分数高出27%。最后，我们对泛化、公平性、注意力可视化和训练数据选择的重要性进行了进一步分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are large-scale machine learning models that arepre-trained on massive amounts of data and can be adapted for variousdownstream tasks. They have been extensively applied to tasks in NaturalLanguage Processing and Computer Vision with models such as GPT, BERT, andCLIP. They are now also increasingly gaining attention in time-series analysis,particularly for physiological sensing. However, most time series foundationmodels are specialist models - with data in pre-training and testing of thesame type, such as Electrocardiogram, Electroencephalogram, andPhotoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist timeseries foundation model with data from multiple domains, such as weather,traffic, and electricity. This paper aims to conduct a comprehensivebenchmarking study to compare the performance of generalist and specialistmodels, with a focus on PPG signals. Through an extensive suite of total 51tasks covering cardiac state assessment, laboratory value estimation, andcross-modal inference, we comprehensively evaluate both models across sevendimensions, including win score, average performance, feature quality, tuninggain, performance variance, transferability, and scalability. These metricsjointly capture not only the models' capability but also their adaptability,robustness, and efficiency under different fine-tuning strategies, providing aholistic understanding of their strengths and limitations for diversedownstream scenarios. In a full-tuning scenario, we demonstrate that thespecialist model achieves a 27% higher win score. Finally, we provide furtheranalysis on generalization, fairness, attention visualizations, and theimportance of training data choice.</description>
      <author>example@mail.com (Saurabh Kataria, Yi Wu, Zhaoliang Chen, Hyunjung Gloria Kwak, Yuhao Xu, Lovely Yeswanth Panchumarthi, Ran Xiao, Jiaying Lu, Ayca Ermis, Anni Zhao, Runze Yan, Alex Federov, Zewen Liu, Xu Wu, Wei Jin, Carl Yang, Jocelyn Grunwell, Stephanie R. Brown, Amit Shah, Craig Jabaley, Tim Buchman, Sivasubramanium V Bhavani, Randall J. Lee, Xiao Hu)</author>
      <guid isPermaLink="false">2510.14254v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization</title>
      <link>http://arxiv.org/abs/2510.14217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures, 3 tables, SI: 8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了不同分子表示方法（分子指纹、预训练Transformer、全局和局部3D表示）在七种分子属性上的谱特性，发现更丰富的谱特征并不总能提高准确性，主要特征值捕获了最有信息量的特征。&lt;h4&gt;背景&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。然而，对分子核的系统性谱分析仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;提供对QM9数据集上核岭回归的首次全面谱分析，研究不同分子表示方法在七种分子属性上的谱特性，探索谱特性与预测性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;使用四种不同的谱指标测量谱丰富度，实施截断核方法探究谱与预测性能的关系，分析七种分子属性，比较分子指纹、预训练Transformer、全局和局部3D表示等不同表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;1) 更丰富的谱特征并不一致地提高准确性；2) 对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关；3) 在许多核中，仅保留前2%的特征值就能恢复几乎所有性能；4) 主要特征值捕获了最有信息量的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明表示、核特征和预测性能之间存在微妙的关系，挑战了关于谱丰富度与性能关系的传统观点。这些发现对如何在数据有限的科学和实际任务中评估核方法和自监督学习方法提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。尽管机器学习中核谱的研究广泛，但对分子核的系统性谱分析仍然稀缺。在这项工作中，我们首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了分子指纹、预训练Transformer、全局和局部3D表示在七种分子属性上的谱特性。令人惊讶的是，通过四种不同的谱指标测量的更丰富的谱特征并不一致地提高准确性。皮尔逊相关性测试进一步表明，对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关。我们还实现了截断核来探究谱与预测性能之间的关系：在许多核中，仅保留前2%的特征值就能恢复几乎所有性能，这表明主要特征值捕获了最有信息量的特征。我们的结果挑战了'更丰富的谱产生更好的泛化'这一常见启发式方法，并突出了表示、核特征和预测性能之间的微妙关系。除了分子属性预测外，这些发现还指导了如何在数据有限的科学和实际任务中评估核方法和自监督学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the spectral properties of kernels offers a principledperspective on generalization and representation quality. While deep modelsachieve state-of-the-art accuracy in molecular property prediction, kernelmethods remain widely used for their robustness in low-data regimes andtransparent theoretical grounding. Despite extensive studies of kernel spectrain machine learning, systematic spectral analyses of molecular kernels arescarce. In this work, we provide the first comprehensive spectral analysis ofkernel ridge regression on the QM9 dataset, molecular fingerprint, pretrainedtransformer-based, global and local 3D representations across seven molecularproperties. Surprisingly, richer spectral features, measured by four differentspectral metrics, do not consistently improve accuracy. Pearson correlationtests further reveal that for transformer-based and local 3D representations,spectral richness can even have a negative correlation with performance. Wealso implement truncated kernels to probe the relationship between spectrum andpredictive performance: in many kernels, retaining only the top 2% ofeigenvalues recovers nearly all performance, indicating that the leadingeigenvalues capture the most informative features. Our results challenge thecommon heuristic that "richer spectra yield better generalization" andhighlight nuanced relationships between representation, kernel features, andpredictive performance. Beyond molecular property prediction, these findingsinform how kernel and self-supervised learning methods are evaluated indata-limited scientific and real-world tasks.</description>
      <author>example@mail.com (Asma Jamali, Tin Sum Cheng, Rodrigo A. Vargas-Hernández)</author>
      <guid isPermaLink="false">2510.14217v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ARM-FM是一种利用基础模型高级推理能力的框架，用于强化学习中自动化、组合式的奖励设计，解决了强化学习算法对奖励函数设定敏感的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;强化学习算法对奖励函数的设定高度敏感，这仍然是限制其广泛应用的核心挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ARM-FM框架，实现强化学习中自动化、组合式的奖励设计，利用基础模型的高级推理能力来自动构建奖励机。&lt;h4&gt;方法&lt;/h4&gt;使用奖励机(RMs)作为强化学习目标设定的机制，通过基础模型自动构建奖励机；将语言嵌入与每个奖励机自动机状态相关联以实现跨任务泛化；在多样化挑战性环境中评估框架效果。&lt;h4&gt;主要发现&lt;/h4&gt;ARM-FM框架在多样化的挑战性环境中展现出有效性，包括实现零样本泛化的能力；基础模型能够从自然语言规范自动生成奖励机；结构化的奖励机形式化方法能实现有效的任务分解。&lt;h4&gt;结论&lt;/h4&gt;基础模型与奖励机的结构化形式化方法相结合，能够实现有效的自动化奖励设计，促进强化学习在更广泛领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;强化学习(RL)算法对奖励函数的设定高度敏感，这仍然是限制其广泛适用性的核心挑战。我们提出了ARM-FM：基于基础模型的自动奖励机，这是一个用于强化学习中自动化、组合式奖励设计的框架，利用了基础模型(FMs)的高级推理能力。奖励机(RMs)——一种基于自动机的奖励规范形式化方法——被用作强化学习目标设定的机制，并通过基础模型的使用自动构建。奖励机的结构化形式化方法能够实现有效的任务分解，而基础模型的使用则允许用自然语言进行目标规范。具体而言，我们(i)使用基础模型从自然语言规范自动生成奖励机；(ii)将语言嵌入与每个奖励机自动机状态相关联，以实现跨任务泛化；(iii)在一系列多样化的挑战性环境中提供了ARM-FM有效性的实证证据，包括零样本泛化的证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) algorithms are highly sensitive to rewardfunction specification, which remains a central challenge limiting their broadapplicability. We present ARM-FM: Automated Reward Machines via FoundationModels, a framework for automated, compositional reward design in RL thatleverages the high-level reasoning capabilities of foundation models (FMs).Reward machines (RMs) -- an automata-based formalism for reward specification-- are used as the mechanism for RL objective specification, and areautomatically constructed via the use of FMs. The structured formalism of RMsyields effective task decompositions, while the use of FMs enables objectivespecifications in natural language. Concretely, we (i) use FMs to automaticallygenerate RMs from natural language specifications; (ii) associate languageembeddings with each RM automata-state to enable generalization across tasks;and (iii) provide empirical evidence of ARM-FM's effectiveness in a diversesuite of challenging environments, including evidence of zero-shotgeneralization.</description>
      <author>example@mail.com (Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth)</author>
      <guid isPermaLink="false">2510.14176v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title>
      <link>http://arxiv.org/abs/2510.14133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种代理AI系统的统一建模框架，由主机代理模型和任务生命周期模型组成，解决了当前代理间通信生态系统碎片化的问题，为多AI代理系统提供了形式化验证基础。&lt;h4&gt;背景&lt;/h4&gt;代理AI系统利用多个自主代理和大语言模型解决复杂多步骤任务，在高风险应用中安全性和功能性至关重要。当前代理间通信生态系统碎片化，各种协议被孤立分析，造成语义鸿沟，阻碍系统属性严格分析并引入架构不协调等风险。&lt;h4&gt;目的&lt;/h4&gt;解决代理AI系统中因通信碎片化导致的语义鸿沟问题，提供统一语义框架实现多AI代理系统行为的推理，支持系统化分析、设计和部署正确、可靠、稳健的代理AI系统。&lt;h4&gt;方法&lt;/h4&gt;引入由两个基础模型组成的框架：主机代理模型（正式化顶层实体与用户交互、任务分解和执行协调）和任务生命周期模型（详细说明子任务状态和转换）。基于此框架定义31个属性（主机代理17个，任务生命周期14个），分为活性、安全性、完整性和公平性四类，用时态逻辑表达以实现形式化验证。&lt;h4&gt;主要发现&lt;/h4&gt;两个基础模型共同为多AI代理系统行为推理提供统一语义框架，定义的属性能实现系统行为形式化验证，检测协调边缘情况，防止死锁和安全漏洞。&lt;h4&gt;结论&lt;/h4&gt;引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，确保系统正确性、可靠性和稳健性。&lt;h4&gt;翻译&lt;/h4&gt;代理AI系统，即利用多个自主代理和大语言模型的系统，正被越来越多地用于解决复杂的多步骤任务。这些系统的安全性、安全性和功能性至关重要，特别是在高风险应用中。然而，当前代理间通信生态系统是碎片化的，诸如用于工具访问的模型上下文协议和用于协调的代理到代理等协议被孤立地分析。这种碎片化造成了语义鸿沟，阻碍了对系统属性的严格分析，并引入了架构不协调和可利用的协调问题等风险。为应对这些挑战，我们引入了一个由两个基础模型组成的代理AI系统建模框架。第一个是主机代理模型，它正式化与用户交互、分解任务并通过利用外部代理和工具协调执行的最高级别实体。第二个是任务生命周期模型，它详细说明从创建到完成的各个子任务的状态和转换，提供细粒度的任务管理和错误处理视图。这两个模型共同为多AI代理系统行为推理提供了统一的语义框架。基于此框架，我们为主机代理定义了17个属性，为任务生命周期定义了14个属性，分为活性、安全性、完整性和公平性四类。用时态逻辑表达的这些属性，能够实现系统行为的正式验证，检测协调边缘情况，并防止死锁和安全漏洞。通过这项工作，我们引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，以确保正确、可靠和稳健的系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agentic AI systems, which leverage multiple autonomous agents and LargeLanguage Models (LLMs), are increasingly used to address complex, multi-steptasks. The safety, security, and functionality of these systems are critical,especially in high-stakes applications. However, the current ecosystem ofinter-agent communication is fragmented, with protocols such as the ModelContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocolfor coordination being analyzed in isolation. This fragmentation creates asemantic gap that prevents the rigorous analysis of system properties andintroduces risks such as architectural misalignment and exploitablecoordination issues. To address these challenges, we introduce a modelingframework for agentic AI systems composed of two foundational models. Thefirst, the host agent model, formalizes the top-level entity that interactswith the user, decomposes tasks, and orchestrates their execution by leveragingexternal agents and tools. The second, the task lifecycle model, details thestates and transitions of individual sub-tasks from creation to completion,providing a fine-grained view of task management and error handling. Together,these models provide a unified semantic framework for reasoning about thebehavior of multi-AI agent systems. Grounded in this framework, we define 17properties for the host agent and 14 for the task lifecycle, categorized intoliveness, safety, completeness, and fairness. Expressed in temporal logic,these properties enable formal verification of system behavior, detection ofcoordination edge cases, and prevention of deadlocks and securityvulnerabilities. Through this effort, we introduce the first rigorouslygrounded, domain-agnostic framework for the systematic analysis, design, anddeployment of correct, reliable, and robust agentic AI systems.</description>
      <author>example@mail.com (Edoardo Allegrini, Ananth Shreekumar, Z. Berkay Celik)</author>
      <guid isPermaLink="false">2510.14133v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Shadow Molecular Dynamics for Flexible Multipole Models</title>
      <link>http://arxiv.org/abs/2510.14132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将阴影分子动力学扩展到柔性多极模型，处理长程静电相互作用，提供稳定高效的原子模拟框架。&lt;h4&gt;背景&lt;/h4&gt;阴影分子动力学是处理具有长程静电相互作用的柔性电荷模型的高效稳定原子模拟框架，但之前实现仅限于原子单极电荷分布。&lt;h4&gt;目的&lt;/h4&gt;扩展阴影分子动力学方法以支持柔性多极模型，实现更准确的原子相互作用模拟。&lt;h4&gt;方法&lt;/h4&gt;推导阴影能量函数、势能和力项的详细表达式，明确包含单极-单极、偶极-单极和偶极-偶极相互作用；将原子单极和偶极视为扩展动力学变量；提出单极固定而偶极柔性的分子动力学方案。&lt;h4&gt;主要发现&lt;/h4&gt;引入额外偶极自由度保留了仅单极阴影分子动力学模拟的稳定性和准确性；扩展的阴影动力学为涉及柔性多极长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。&lt;h4&gt;结论&lt;/h4&gt;该方法与现代人工智能和机器学习技术结合特别有意义，有助于开发可转移的高精度原子相互作用表示，适用于各种分子系统。&lt;h4&gt;翻译&lt;/h4&gt;阴影分子动力学为具有长程静电相互作用的柔性电荷模型提供了一种高效稳定的原子模拟框架。虽然之前的实现仅限于原子单极电荷分布，但我们将这种方法扩展到了柔性多极模型。我们推导了阴影能量函数、势能和力项的详细表达式，明确包含了单极-单极、偶极-单极和偶极-偶极相互作用。在我们的公式中，原子单极和原子偶极都被视为扩展的动力学变量，与核自由度的传播一起处理。我们证明引入额外的偶极自由度保留了之前在仅单极阴影分子动力学模拟中看到的稳定性和准确性。此外，我们提出了一种阴影分子动力学方案，其中单极电荷保持固定，而偶极保持柔性。我们的扩展阴影动力学为涉及柔性多极之间长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。这与现代人工智能和机器学习技术结合特别有意义，这些技术越来越多地用于开发原子模拟的物理信息驱动和数据驱动的基础模型。这些模型旨在提供可转移的高精度原子相互作用表示，适用于各种分子系统，这需要准确处理长程电荷相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shadow molecular dynamics provide an efficient and stable atomisticsimulation framework for flexible charge models with long-range electrostaticinteractions. While previous implementations have been limited to atomicmonopole charge distributions, we extend this approach to flexible multipolemodels. We derive detailed expressions for the shadow energy functions,potentials, and force terms, explicitly incorporating monopole-monopole,dipole-monopole, and dipole-dipole interactions. In our formulation, bothatomic monopoles and atomic dipoles are treated as extended dynamical variablesalongside the propagation of the nuclear degrees of freedom. We demonstratethat introducing the additional dipole degrees of freedom preserves thestability and accuracy previously seen in monopole-only shadow moleculardynamics simulations. Additionally, we present a shadow molecular dynamicsscheme where the monopole charges are held fixed while the dipoles remainflexible. Our extended shadow dynamics provide a framework for stable,computationally efficient, and versatile molecular dynamics simulationsinvolving long-range interactions between flexible multipoles. This is ofparticular interest in combination with modern artificial intelligence andmachine learning techniques, which are increasingly used to developphysics-informed and data-driven foundation models for atomistic simulations.These models aim to provide transferable, high-accuracy representations ofatomic interactions that are applicable across diverse sets of molecularsystems, which requires accurate treatment of long-range charge interactions.</description>
      <author>example@mail.com (Rae A. Corrigan Grove, Robert Stanton, Michael E. Wall, Anders M. N. Niklasson)</author>
      <guid isPermaLink="false">2510.14132v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Exploratory Causal Inference in SAEnce</title>
      <link>http://arxiv.org/abs/2510.14073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Neural Effect Search的新方法，可以直接从数据中发现未知的因果效应，解决了传统随机对照试验的局限性。&lt;h4&gt;背景&lt;/h4&gt;随机对照试验是科学的重要支柱，但它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。&lt;h4&gt;目的&lt;/h4&gt;直接从数据中发现治疗的未知效应。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的基础模型将试验中的非结构化数据转换为有意义的表示，通过稀疏自编码器解释这些表示，并引入Neural Effect Search这一新颖的递归过程，通过渐进分层解决多重测试问题和效应纠缠问题。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成实验中评估了算法的稳健性，并在实验生态学背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;h4&gt;结论&lt;/h4&gt;Neural Effect Search方法成功解决了在神经水平发现显著因果效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随机对照试验是科学的重要支柱；然而，它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。我们提出直接从数据中发现治疗的未知效应。为此，我们通过预训练的基础模型将试验中的非结构化数据转换为有意义的表示，并通过稀疏自编码器解释它们。然而，由于多重测试问题和效应纠缠，在神经水平发现显著的因果效应并不简单。为了解决这些挑战，我们引入了Neural Effect Search，这是一种新颖的递归过程，通过渐进分层解决了这两个问题。在半合成实验中评估了我们算法的稳健性后，我们在实验生态学的背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Randomized Controlled Trials are one of the pillars of science; nevertheless,they rely on hand-crafted hypotheses and expensive analysis. Such constraintsprevent causal effect estimation at scale, potentially anchoring on popular yetincomplete hypotheses. We propose to discover the unknown effects of atreatment directly from data. For this, we turn unstructured data from a trialinto meaningful representations via pretrained foundation models and interpretthem via a sparse autoencoder. However, discovering significant causal effectsat the neural level is not trivial due to multiple-testing issues and effectsentanglement. To address these challenges, we introduce Neural Effect Search, anovel recursive procedure solving both issues by progressive stratification.After assessing the robustness of our algorithm on semi-synthetic experiments,we showcase, in the context of experimental ecology, the first successfulunsupervised causal effect identification on a real-world scientific trial.</description>
      <author>example@mail.com (Tommaso Mencattini, Riccardo Cadei, Francesco Locatello)</author>
      <guid isPermaLink="false">2510.14073v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Context-Selective State Space Models: Feedback is All You Need</title>
      <link>http://arxiv.org/abs/2510.14027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了COFFEE模型，一种新颖的时变状态空间模型，通过状态反馈实现上下文相关的选择性，有效捕获长距离依赖关系，并在多项任务上取得了优于现有S6模型的结果。&lt;h4&gt;背景&lt;/h4&gt;Transformers模型基于注意力机制，是大多数基础模型的骨干，但它们具有二次复杂度，并且在处理输入序列中的长距离依赖关系时存在困难。状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块在长序列基准测试上取得了最先进的结果。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理长距离依赖关系的高效序列模型，解决Transformers模型的二次复杂度问题，并超越现有状态空间模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，结合状态反馈以实现上下文相关的选择性。与S6不同，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示，使模型能够根据积累的上下文调节其动态。此外，采用高效的模型参数化方法消除冗余，实现更紧凑和可训练的公式。&lt;h4&gt;主要发现&lt;/h4&gt;在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性；在MNIST上，仅用3585个参数就达到了97%的准确率，大大优于S6在相同架构上的表现。&lt;h4&gt;结论&lt;/h4&gt;状态反馈是构建可扩展和高效序列模型的关键机制，COFFEE模型通过结合状态反馈和高效参数化，显著提升了序列建模能力，特别是在处理长距离依赖关系方面。&lt;h4&gt;翻译&lt;/h4&gt;Transformers模型由注意力机制驱动，是大多数基础模型的骨干，但它们受二次复杂度的困扰，并且在处理输入序列中的长距离依赖关系时存在困难。最近的研究表明，状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块作为Mamba架构的核心，在长序列基准测试上取得了最先进的结果。在本文中，我们介绍了COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，它结合了状态反馈以实现上下文相关的选择性，同时仍允许并行实现。而S6的选择性机制仅依赖于当前输入，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示。这种转变使模型能够根据积累的上下文调节其动态，提高其捕获长距离依赖关系的能力。除了状态反馈外，我们还采用了一种高效的模型参数化方法，消除了S6中存在的冗余，导致更紧凑和可训练的公式。在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性。在MNIST上，COFFEE在相同架构上大大优于S6，仅用3585个参数就达到了97%的准确率。这些结果展示了状态反馈作为构建可扩展和高效序列模型的关键机制的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers, powered by the attention mechanism, are the backbone of mostfoundation models, yet they suffer from quadratic complexity and difficultiesin dealing with long-range dependencies in the input sequence. Recent work hasshown that state space models (SSMs) provide an efficient alternative, with theS6 module at the core of the Mamba architecture achieving state-of-the-artresults on long-sequence benchmarks. In this paper, we introduce the COFFEE(COntext From FEEdback) model, a novel time-varying SSM that incorporates statefeedback to enable context-dependent selectivity, while still allowing forparallel implementation. Whereas the selectivity mechanism of S6 only dependson the current input, COFFEE computes it from the internal state, which servesas a compact representation of the sequence history. This shift allows themodel to regulate its dynamics based on accumulated context, improving itsability to capture long-range dependencies. In addition to state feedback, weemploy an efficient model parametrization that removes redundancies present inS6 and leads to a more compact and trainable formulation. On the induction headtask, COFFEE achieves near-perfect accuracy with two orders of magnitude fewerparameters and training sequences compared to S6. On MNIST, COFFEE largelyoutperforms S6 within the same architecture, reaching 97% accuracy with only3585 parameters. These results showcase the role of state feedback as a keymechanism for building scalable and efficient sequence models.</description>
      <author>example@mail.com (Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi)</author>
      <guid isPermaLink="false">2510.14027v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的跨模态生成和多轮交互，克服了现有自回归架构的局限性。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心，但现有多模态模型受限于自回归架构，无法平衡整合理解与生成能力。混合和解耦策略虽被探索，但其冗余设计限制了在广泛场景如跨模态检索中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的理解和生成，并扩展应用场景。&lt;h4&gt;方法&lt;/h4&gt;利用度量诱导的概率路径和动力学最优速度，原生支持任何到任何的理解和生成，增强响应效率；通过简洁的统一表示而非任务解耦设计实现更广泛应用；在大规模交错文本、图像、视频和音频数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多模态交互和跨模态检索方面优于之前的统一模型，展现了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;发布训练细节、数据协议，并开源代码和模型检查点，以促进多模态基础模型领域的进一步研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任何到任何跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组成部分，在人机交互中发挥关键作用。然而，大多数现有多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。虽然混合和解耦策略已被探索用于在统一框架内分别解决这些问题，但它们的冗余、非集成设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在这项工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任何到任何的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，增强响应效率。在大规模交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多模态交互和跨模态检索方面优于之前的统一模型，凸显了其作为下一代多模态基础模型的架构优势。为进一步推进研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation modelthat achieves unified modeling through discrete flow paradigms. By leveragingmetric-induced probability paths and kinetic optimal velocities, NExT-OMNInatively supports any-to-any understanding and generation with enhancedresponse efficiency, while enabling broader application scenarios throughconcise unified representations rather than task-decoupled designs. Trained onlarge-scale interleaved text, image, video, and audio data, NExT-OMNI deliverscompetitive performance on multimodal generation and understanding benchmarks,while outperforming prior unified models in multi-turn multimodal interactionand cross-modal retrieval, highlighting its architectural advantages as anext-generation multimodal foundation model. To advance further research, werelease training details, data protocols, and open-source both the code andmodel checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning</title>
      <link>http://arxiv.org/abs/2510.13909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种知识推理语言模型(KRLM)，用于解决归纳知识图谱推理中LLM知识与KG上下文协调的问题。通过设计KRL指令格式、KRL分词器、KRL注意力层和结构感知的下一个实体预测器，模型能够在KGR过程中实现LLM知识与KG上下文的统一协调，有效约束LLM的生成幻觉，提高推理结果的可信度。&lt;h4&gt;背景&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型来处理这种不确定性，而大型语言模型在开放域知识推理方面展示了强大能力。最新的研究集中在基于LLM的KGFMs上，这些模型整合了LLM知识与KG上下文进行归纳KGR。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LLM的KGR方法中LLM知识被稀疏KG上下文掩盖导致知识扭曲的问题，以及难以完全约束LLM生成幻觉的问题，提出一个知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。&lt;h4&gt;方法&lt;/h4&gt;设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示；提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文；提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。&lt;h4&gt;主要发现&lt;/h4&gt;在25个真实世界的归纳KGR数据集上进行了广泛的实验，结果表明所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;h4&gt;结论&lt;/h4&gt;KRLM模型有效地解决了LLM知识与KG上下文协调的问题，通过结构感知的下一个实体预测器提高了推理结果的可信度，在多个数据集上表现优异，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型，这些模型学习跨知识图谱的结构不变性来处理这种不确定性。最近，大型语言模型在开放域知识推理方面展示了强大的能力。因此，最新的研究集中在基于LLM的知识图谱基础模型上，这些模型整合了LLM知识与KG上下文进行归纳KGR。然而，LLM的内在知识可能被稀疏的KG上下文掩盖，导致LLM知识扭曲，这可能对模型推理造成不可逆的损害。此外，现有的基于LLM的KGR方法仍然难以完全约束LLM中的生成幻觉，严重限制了推理结果的可信度。为解决这些局限性，我们提出了一种知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。具体来说，我们设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示。然后，我们提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文。最后，提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。在25个真实世界的归纳KGR数据集上的广泛实验结果表明，所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inductive Knowledge Graph Reasoning (KGR) aims to discover facts inopen-domain KGs containing unknown entities and relations, which poses achallenge for KGR models in comprehending uncertain KG components. Existingstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learnstructural invariances across KGs to handle this uncertainty. Recently, LargeLanguage Models (LLMs) have demonstrated strong capabilities for open-domainknowledge reasoning. As a result, the latest research has focused on LLM-basedKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,leading to LLM knowledge distortion, which can cause irreversible damage tomodel reasoning. Moreover, existing LLM-based KGR methods still struggle tofully constrain generative hallucinations in LLMs, severely limiting thecredibility of reasoning results. To address these limitations, we propose aKnowledge Reasoning Language Model (KRLM) that achieves unified coordinationbetween LLM knowledge and KG context throughout the KGR process. Specifically,we design a Knowledge Reasoning Language (KRL) instruction format and a KRLtokenizer to align LLM knowledge with KG representations. Then, we propose aKRL attention layer that coordinates intrinsic LLM knowledge with additional KGcontext through a dynamic knowledge memory mechanism. Finally, astructure-aware next-entity predictor is proposed, which strictly constrainsthe reasoning results within a trustworthy knowledge domain. Extensiveexperimental results on 25 real-world inductive KGR datasets demonstrate thesignificant superiority of the proposed KRLM\footnote{Our source codes areavailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shotreasoning and fine-tuning scenarios.</description>
      <author>example@mail.com (Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu)</author>
      <guid isPermaLink="false">2510.13909v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SPHeRe（结构投影Hebbian表示）方法，一种新型无监督学习技术，通过整合正交性和结构信息保留解决了传统Hebbian学习在机器学习中的局限性，在多个任务中取得了优异表现。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种描述神经元通过重复刺激调整连接的生物原理，但在机器学习应用中存在连接更新无约束和缺乏反馈中介考虑等问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服传统Hebbian学习局限性的无监督学习方法，使其能够有效扩展到复杂网络架构和任务中。&lt;h4&gt;方法&lt;/h4&gt;SPHeRe通过局部的辅助非线性块整合正交性和结构信息保留，结构信息保留的损失通过辅助轻量级投影反向传播到输入（充当反馈中介），正交性约束则确保更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中达到无监督突触可塑性方法的最新性能；在持续学习和迁移学习场景中表现有效；图像重建任务证明了提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖严格反向传播的高效且受生物启发的学习算法的可能性，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新的无约束性和缺乏对反馈中介的考虑，它存在严重问题。这些缺点限制了其在复杂网络架构和任务中的有效扩展。为此，我们在此引入结构投影Hebbian表示（SPHeRe），一种新型无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，这个投影在概念上充当反馈中介，而正交性约束则考虑了更新幅度的有界性。大量实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试的无监督突触可塑性方法中达到了最先进性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效且受生物启发的学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning to Recognize Quantum Phases of Matter</title>
      <link>http://arxiv.org/abs/2510.14742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用无监督学习方法来确定多体系统的量子相图，该方法能够自主识别并可能揭示量子物质的新相位。&lt;h4&gt;背景&lt;/h4&gt;将多体系统的量子相图绘制视为学习问题，需要根据某种分类标准对其基态进行标记以定义不同的相。&lt;h4&gt;目的&lt;/h4&gt;采用无监督学习方法来确定多体系统的量子相图，算法无需访问任何预先标记的状态。&lt;h4&gt;方法&lt;/h4&gt;算法直接处理量子态，基于量子态之间的保真度相似性标准对基态配置进行分组。使用基于谱聚类的无监督学习算法，并结合'轮廓'和'肘部'方法来确定相位的最佳数量。&lt;h4&gt;主要发现&lt;/h4&gt;通过两个具体的自旋-1/2链进行基准测试，发现基于谱聚类的无监督学习算法能够准确重现相图。&lt;h4&gt;结论&lt;/h4&gt;无监督学习可以自主识别并可能揭示量子物质的新相位，为量子相图的确定提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;在哈密顿量参数空间中绘制多体系统的量子相图可以被视为一个学习问题，这需要根据定义相位的某种分类标准来标记相应的基态。在本工作中，我们采用无监督学习方法，其中算法无法访问任何预先标记的状态，作为确定多体系统量子相图的一种工具。该算法直接处理量子态：给定不同哈密顿量参数的基态配置，该过程基于量子态之间保真度的相似性标准揭示了对它们进行分组的最重要的方式，这种标准即使通过实验也容易估计。我们使用两个特定的自旋-1/2链来基准测试我们的方法，其状态通过张量网络技术确定。我们发现，基于谱聚类的无监督学习算法，结合用于确定相位最佳数量的'轮廓'和'肘部'方法，可以准确重现相图。我们的结果表明，无监督学习如何能够自主识别并可能揭示量子物质的新相位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drawing the quantum phase diagram of a many-body system in the parameterspace of its Hamiltonian can be seen as a learning problem, which implieslabelling the corresponding ground states according to some classificationcriterium that defines the phases. In this work we adopt unsupervised learning,where the algorithm has no access to any priorly labeled states, as a tool fordetermining quantum phase diagrams of many-body systems. The algorithm directlyworks with quantum states: given the ground-state configurations for differentvalues of the Hamiltonian parameters, the process uncovers the most significantway of grouping them based on a similarity criterion that refers to thefidelity between quantum states, that can be easily estimated, evenexperimentally. We benchmark our method with two specific spin-$\frac{1}{2}$chains, with states determined via tensor network techniques. We find thatunsupervised learning algorithms based on spectral clustering, combined with``silhouette'' and ``elbow'' methods for determining the optimal number ofphases, can accurately reproduce the phase diagrams. Our results show howunsupervised learning can autonomously recognize and possibly unveil novelphases of quantum matter.</description>
      <author>example@mail.com (Mehran Khosrojerdi, Alessandro Cuccoli, Paola Verrucchi, Leonardo Banchi)</author>
      <guid isPermaLink="false">2510.14742v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>http://arxiv.org/abs/2510.14415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文开发了一个敏感性分析框架，将完全观测网络中的源数据的平均总处理效应转移到网络完全未知的目标数据中，以估计政策的平均社会影响。&lt;h4&gt;背景&lt;/h4&gt;研究假设源数据和目标数据共享相同的条件均值结果（协变量漂移类型假设），但由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。&lt;h4&gt;目的&lt;/h4&gt;解决目标网络未观测情况下如何估计ATTE的问题，通过基于目标网络度分布不确定性的敏感性分析来构建ATTE的界限。&lt;h4&gt;方法&lt;/h4&gt;考虑基于目标网络度分布不确定性的敏感性分析，不确定性程度由给定参考度分布的Wasserstein距离衡量；使用基于线性规划的估计量构建目标ATTE的界限；通过函数delta方法推导界限估计量的极限分布；开发wild bootstrap方法来近似该分布。&lt;h4&gt;主要发现&lt;/h4&gt;构建了目标ATTE的界限估计量，推导了其极限分布，并开发了wild bootstrap方法来近似该分布。&lt;h4&gt;结论&lt;/h4&gt;该框架允许在目标网络完全未知的情况下，通过敏感性分析来估计政策的平均社会影响。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文开发了一个敏感性分析框架，将具有完全观测网络的源数据中的平均总处理效应（ATTE）转移到网络完全未知的目标数据中。ATTE代表了对数据集中每个个体实施政策的平均社会影响。我们提出了一个协变量漂移类型的假设，即源数据和目标数据共享相同的条件均值结果。然而，由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。为了解决这个问题，我们考虑了基于目标网络度分布不确定性的敏感性分析，其中不确定性程度由给定参考度分布的Wasserstein距离来衡量。然后，我们使用基于线性规划的估计量构建了目标ATTE的界限。通过函数delta方法推导了界限估计量的极限分布，并开发了wild bootstrap方法来近似该分布。作为一个实证说明，我们重新研究了Cai等人（2015）关于中国农民天气保险采用的社会网络实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper develops a sensitivity analysis framework that transfers theaverage total treatment effect (ATTE) from source data with a fully observednetwork to target data whose network is completely unknown. The ATTE representsthe average social impact of a policy that assigns the treatment to everyindividual in the dataset. We postulate a covariate-shift type assumption thatboth source and target datasets share the same conditional mean outcome.However, because the target network is unobserved, this assumption alone is notsufficient to pin down the ATTE for the target data. To address this issue, weconsider a sensitivity analysis based on the uncertainty of the targetnetwork's degree distribution, where the extent of uncertainty is measured bythe Wasserstein distance from a given reference degree distribution. We thenconstruct bounds on the target ATTE using a linear programming-based estimator.The limiting distribution of the bound estimator is derived via the functionaldelta method, and we develop a wild bootstrap approach to approximate thedistribution. As an empirical illustration, we revisit the social networkexperiment on farmers' weather insurance adoption in China by Cai et al.(2015).</description>
      <author>example@mail.com (Tadao Hoshino)</author>
      <guid isPermaLink="false">2510.14415v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Glitch noise classification in KAGRA O3GK observing data using unsupervised machine learning</title>
      <link>http://arxiv.org/abs/2510.14291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, accepted to Physics Letters B&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了使用无监督机器学习方法对KAGRA O3GK数据中的非平稳噪声进行图像分类的有效性，成功识别出八种不同的故障噪声类别，提高了引力波观测的可靠性。&lt;h4&gt;背景&lt;/h4&gt;引力波干涉仪受到各种非平稳噪声（称为故障噪声）的干扰，这些噪声影响数据分析和干涉仪的灵敏度。&lt;h4&gt;目的&lt;/h4&gt;准确识别和分类故障噪声，以提高引力波观测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器(VAE)结合谱聚类的无监督机器学习方法，对KAGRA O3GK数据中的非平稳噪声图像进行分类，将潜在变量降维后在三维空间中可视化并进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;成功识别出八种不同的故障噪声类别，并更好地理解了KAGRA在O3GK期间的故障噪声特征。&lt;h4&gt;结论&lt;/h4&gt;无监督学习在故障噪声分类方面显示出潜力，这有助于干涉仪升级和未来第三代引力波天文台的发展。&lt;h4&gt;翻译&lt;/h4&gt;引力波干涉仪受到各种类型的非平稳噪声干扰，称为故障噪声，这些噪声影响数据分析和干涉仪灵敏度。准确识别和分类故障噪声对于提高引力波观测的可靠性至关重要。在本研究中，我们展示了无监督机器学习在KAGRA O3GK数据中分类含有非平稳噪声图像的有效性。使用变分自编码器(VAE)结合谱聚类，我们识别出八种不同的故障噪声类别。从VAE获得的潜在变量被降维，在三维空间中进行可视化，并使用谱聚类进行分类，以便更好地理解KAGRA在O3GK期间的故障噪声特征。我们的结果强调了无监督学习在高效故障噪声分类方面的潜力，这可能反过来促进干涉仪升级和未来第三代引力波天文台的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gravitational wave interferometers are disrupted by various types ofnonstationary noise, referred to as glitch noise, that affect data analysis andinterferometer sensitivity. The accurate identification and classification ofglitch noise are essential for improving the reliability of gravitational waveobservations. In this study, we demonstrated the effectiveness of unsupervisedmachine learning for classifying images with nonstationary noise in the KAGRAO3GK data. Using a variational autoencoder (VAE) combined with spectralclustering, we identified eight distinct glitch noise categories. The latentvariables obtained from VAE were dimensionally compressed, visualized inthree-dimensional space, and classified using spectral clustering to betterunderstand the glitch noise characteristics of KAGRA during the O3GK period.Our results highlight the potential of unsupervised learning for efficientglitch noise classification, which may in turn potentially facilitateinterferometer upgrades and the development of future third-generationgravitational wave observatories.</description>
      <author>example@mail.com (Shoichi Oshino, Yusuke Sakai, Marco Meyer-Conde, Takashi Uchiyama, Yousuke Itoh, Yutaka Shikano, Yoshikazu Terada, Hirotaka Takahashi)</author>
      <guid isPermaLink="false">2510.14291v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data</title>
      <link>http://arxiv.org/abs/2510.14145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的稳健非参数聚类验证框架HD-BWDM，用于解决高维或受污染数据中确定适当聚类数量的问题。&lt;h4&gt;背景&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，在高维或受污染数据中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的稳健的非参数聚类验证框架HD-BWDM，将BWDM标准扩展到高维空间，解决传统方法在高维数据中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HD-BWDM整合随机投影和主成分分析缓解维度诅咒，应用修剪聚类和基于medoid的距离确保对离群点的稳健性。作者推导了理论结果，证明在Johnson-Lindenstrauss嵌入下的一致性和收敛性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟表明，HD-BWDM在高维投影和污染情况下保持稳定性和可解释性，为传统基于质心的验证标准提供了稳健的替代方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;h4&gt;翻译&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，因此在高维或受污染数据中表现不佳。本文提出了一种新的稳健的非参数聚类验证框架，即高维组内组间距离中位数（HD-BWDM），将最近引入的BWDM标准扩展到高维空间。HD-BWDM整合了随机投影和主成分分析来缓解维度诅咒，并应用修剪聚类和基于medoid的距离以确保对离群点的稳健性。作者推导了理论结果，证明了在Johnson-Lindenstrauss嵌入下的一致性和收敛性。广泛的模拟表明，在高维投影和污染情况下，HD-BWDM保持稳定性和可解释性，为传统的基于质心的验证标准提供了一个稳健的替代方案。所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate number of clusters in unsupervised learning is acentral problem in statistics and data science. Traditional validity indicessuch as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend oncentroid-based distances and therefore degrade in high-dimensional orcontaminated data. This paper proposes a new robust, nonparametric clusteringvalidation framework, the High-Dimensional Between-Within Distance Median(HD-BWDM), which extends the recently introduced BWDM criterion tohigh-dimensional spaces. HD-BWDM integrates random projection and principalcomponent analysis to mitigate the curse of dimensionality and applies trimmedclustering and medoid-based distances to ensure robustness against outliers. Wederive theoretical results showing consistency and convergence underJohnson-Lindenstrauss embeddings. Extensive simulations demonstrate thatHD-BWDM remains stable and interpretable under high-dimensional projections andcontamination, providing a robust alternative to traditional centroid-basedvalidation criteria. The proposed method provides a theoretically grounded,computationally efficient stopping rule for nonparametric clustering in modernhigh-dimensional applications.</description>
      <author>example@mail.com (Mohammed Baragilly, Hend Gabr)</author>
      <guid isPermaLink="false">2510.14145v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为RoboGPT-R1的两阶段微调框架，用于提升具身智能体的推理能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和基于监督微调的视觉语言模型在规划任务中取得成功，但在复杂现实环境中执行长时程操作任务时仍面临挑战，原因是它们有限的常识和推理能力。将通用视觉语言模型通过监督微调对齐到机器人规划任务存在泛化能力差和对物理理解不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，提升具身智能体在复杂环境中的推理和规划能力，特别是完成长时程操作任务的能力。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，包含两个阶段：首先通过监督训练使用专家序列获取基础知识，然后利用强化学习解决模型在视觉空间理解和推理方面的不足。同时设计了基于规则的奖励函数，考虑长时程性能和环境动作约束，并在Qwen2.5-VL-3B上训练推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;在EmbodiedBench基准测试上，训练的推理模型显著优于更大规模的GPT-4o-mini模型，性能高出21.33%；同时超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提升了具身智能体的推理能力和规划能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;翻译&lt;/h4&gt;提升具身智能体的推理能力对于机器人在长时程操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大型语言模型和视觉语言模型在规划任务中取得了成功，但由于常识和推理能力的限制，它们在复杂现实环境中执行长时程操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型对齐到机器人规划任务存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，这是一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长时程性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，高出21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决提升具身智能体在复杂长视野操作任务中的推理能力问题。当前基于监督微调的大语言模型在真实世界环境中执行长期任务时面临泛化能力不足和物理理解有限的问题。这一问题在现实中非常重要，因为机器人需要处理如'打扫厨房'或'准备晚餐'等复杂、长期的指令，而现有方法难以在动态环境中有效适应和自我纠正。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SFT-only范式的局限性，包括缺乏环境适应能力和奖励函数设计不足。他们借鉴了强化学习在其他领域（如视频推理、数学推理）的成功应用，以及DeepSeek-R1中的'aha moment'概念。具体设计上，作者结合了REBP项目中的数据集和GRPO算法，同时创新性地设计了包含格式奖励和LCS准确奖励的奖励函数，以解决多步推理任务中的物理理解和动作序列一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段训练框架结合监督微调和强化学习的优势，并设计针对具身任务的奖励函数。整体流程包括：1)数据准备阶段，使用从Gemini-2.0-flash提炼的SFT数据集和增强的RFT数据集；2)两阶段训练，第一阶段SFT赋予模型基础规划能力，第二阶段使用GRPO进行强化微调提升推理和泛化能力；3)奖励设计，结合格式奖励(评估结构完整性和动作有效性)和LCS准确奖励(关注动作序列顺序)；4)在EmbodiedBench基准上评估性能，包括域内(EB-ALFRED)和域外(EB-Habitat)场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段训练框架，结合SFT和GRPO强化学习；2)创新的奖励函数设计，包含格式奖励和LCS准确奖励；3)仅使用3B参数的小型模型实现高性能；4)采用零样本处理提高训练效率和泛化能力。相比之前的工作，RoboGPT-R1在EB-ALFRED基准上比GPT-4o-mini高21.33%，比其他基于Qwen2.5-VL-7B的工作高20.33%，特别是在长视野任务上达到50%的准确率，显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboGPT-R1通过结合监督微调和强化学习的两阶段训练框架，以及针对具身任务设计的基于规则的奖励函数，显著提升了小型视觉语言模型在复杂长视野机器人规划任务中的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2510.14647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SaTA的空间锚定触觉感知方法，用于解决灵巧操作中的高精度几何推理问题。该方法通过将触觉特征锚定到手部运动学框架，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;背景&lt;/h4&gt;灵巧操作需要精确的几何推理，但现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而传统基于模型的方法可以轻松处理这些任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用触觉信号的感知丰富性及其与手部运动学空间关系的框架，以实现高精度的灵巧操作。&lt;h4&gt;方法&lt;/h4&gt;提出了SaTA（Spatially-anchored Tactile Awareness for dexterous manipulation）框架，一种端到端策略框架，通过正向运动学将触觉特征锚定到手部运动学框架中。&lt;h4&gt;主要发现&lt;/h4&gt;空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。SaTA在多个基准测试中显著优于强视觉-触觉基线，成功率提高高达30个百分点，任务完成时间减少27%。&lt;h4&gt;结论&lt;/h4&gt;SaTA通过将触觉特征锚定到手部运动学框架，成功解决了现有学习框架在处理高精度灵巧操作任务时的局限性，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;翻译&lt;/h4&gt;灵巧操作需要精确的几何推理，然而现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而这些任务对于传统基于模型的方法来说则是常规操作。我们确定了一个关键限制：虽然触觉传感器提供了丰富的接触信息，但现有学习框架未能有效利用触觉信号的感知丰富性及其与手部运动学的空间关系。我们认为理想的触觉表示应将接触测量明确地锚定在稳定的参考框架中，同时保留详细的感官信息，使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们引入了SaTA（用于灵巧操作的空间锚定触觉感知），一种端到端策略框架，通过正向运动学将触觉特征明确锚定到手部运动学框架，无需物体模型或显式姿态估计即可实现准确的几何推理。我们的关键见解是空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们在具有挑战性的灵巧操作任务上验证了SaTA，包括自由空间中的双臂USB-C连接（需要亚毫米级对齐精度）、需要精确螺纹啮合和旋转控制的灯泡安装，以及需要精细力调制和角度精度的卡片滑动。这些任务由于其严格的精度要求，对基于学习的方法构成了重大挑战。在多个基准测试中，SaTA显著优于强视觉-触觉基线，成功率提高高达30个百分点，同时任务完成时间减少27%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灵巧操作中如何有效利用触觉信号进行精确几何推理的问题，特别是在需要亚毫米级精度的任务中。这个问题很重要，因为在多指多接触场景中，毫米级误差就可能导致任务失败（如USB连接器无法插入），而在接触关键时刻，视觉信息常被遮挡或失效，精确的几何信息对成功操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出触觉传感器虽提供丰富信息但现有学习框架未能有效利用其感知丰富性和空间关系这一关键限制。他们认为理想的触觉表示应将接触测量稳定在参考框架中，同时保留详细感官信息。设计方法借鉴了ACT框架作为基础架构，使用FiLM机制整合空间上下文与触觉特征，应用Fourier特征编码捕获多尺度几何变化，并采用模仿学习策略使用专家演示数据进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将触觉特征锚定到手部运动学坐标系中，同时保留完整几何信息，使策略能准确推断接触状态和物体几何形状，直接输出精确操作动作。整体流程是：接收多模态输入（RGB图像、触觉图像、关节角度）；通过正向运动学计算触觉传感器6D姿态；用Fourier特征编码空间信息；通过FiLM整合空间上下文与触觉特征；将多模态信息通过Transformer处理；生成动作序列实现亚毫米精度操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：空间锚定触觉表示方法；端到端操作策略框架SaTA；高精度灵巧操作任务的成功验证。相比之前工作，SaTA将触觉测量显式锚定到手部坐标系而非处理为抽象特征；保留了完整触觉图像特征而非转换为简化几何形式；直接输出操作动作而非专注于感知重建；不依赖显式物体模型或离线优化过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SaTA通过空间锚定触觉表示，使基于学习的方法能够实现亚毫米级精度的灵巧操作，成功解决了传统视觉-触觉学习方法在需要高精度几何推理任务中的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous manipulation requires precise geometric reasoning, yet existingvisuo-tactile learning methods struggle with sub-millimeter precision tasksthat are routine for traditional model-based approaches. We identify a keylimitation: while tactile sensors provide rich contact information, currentlearning frameworks fail to effectively leverage both the perceptual richnessof tactile signals and their spatial relationship with hand kinematics. Webelieve an ideal tactile representation should explicitly ground contactmeasurements in a stable reference frame while preserving detailed sensoryinformation, enabling policies to not only detect contact occurrence but alsoprecisely infer object geometry in the hand's coordinate system. We introduceSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), anend-to-end policy framework that explicitly anchors tactile features to thehand's kinematic frame through forward kinematics, enabling accurate geometricreasoning without requiring object models or explicit pose estimation. Our keyinsight is that spatially grounded tactile representations allow policies tonot only detect contact occurrence but also precisely infer object geometry inthe hand's coordinate system. We validate SaTA on challenging dexterousmanipulation tasks, including bimanual USB-C mating in free space, a taskdemanding sub-millimeter alignment precision, as well as light bulbinstallation requiring precise thread engagement and rotational control, andcard sliding that demands delicate force modulation and angular precision.These tasks represent significant challenges for learning-based methods due totheir stringent precision requirements. Across multiple benchmarks, SaTAsignificantly outperforms strong visuo-tactile baselines, improving successrates by up to 30 percentage while reducing task completion times by 27percentage.</description>
      <author>example@mail.com (Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang)</author>
      <guid isPermaLink="false">2510.14647v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps</title>
      <link>http://arxiv.org/abs/2510.14546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICRA 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用视觉语言模型嵌入表示机器人地图语义的方法，通过自然语言同义词和反义词来训练分类器，解决机器人确定环境中与查询相关部分的挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型的嵌入表示被越来越多地用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统有限标签的表示方法。&lt;h4&gt;目的&lt;/h4&gt;解决机器人确定环境中与查询相关部分的关键挑战，提高地图和图像的查询能力。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器来将环境划分为匹配和不匹配的部分。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验表明，该方法能够显著提高地图和图像的查询能力，且该查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了机器人确定环境中与查询相关部分的挑战，提高了地图和图像的查询能力，具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型的嵌入表示越来越多地被用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统的有限标签。嵌入表示通过相似度比较将嵌入的用户文本提示与地图嵌入，实现按需查询。执行查询任务的关键挑战是机器人必须确定环境中与查询相关的部分。本文提出了这一挑战的解决方案。我们利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器将环境划分为匹配和不匹配的部分。我们通过大量实验评估了该方法，包括对地图和标准图像基准的查询。结果表明地图和图像的查询能力得到了提高。我们的查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何更有效地从视觉-语言模型(VLM)的嵌入表示中查询相关信息的问题。具体来说，当机器人需要根据自然语言查询在地图或图像中找到相关物体或区域时，现有方法无法准确确定环境中与查询相关的部分。这个问题很重要，因为随着视觉-语言模型的发展，机器人地图能够包含更丰富的语义信息，开放词汇的场景理解能力对机器人执行复杂任务至关重要，而现有方法在匹配查询与地图嵌入时性能有限，限制了机器人对环境的理解和交互能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：现有方法主要采用阈值化余弦相似度或使用单个互补查询（如'other'）的策略，但这些方法假设所有维度对查询的重要性相同，且仅使用单个查询和单个负例无法准确估计相关区域的范围。基于这些分析，作者设计了QuASH方法，利用自然语言同义词和反义词来估计与查询相关的语言空间，通过启发式方法生成语义相关的同义词和反义词，并基于这些样本训练一个分类器。该方法借鉴了现有工作中的视觉-语言嵌入表示和查询机制，但改进了查询策略，不再依赖简单的阈值或单个负例比较。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用自然语言语义知识，通过生成查询的同义词和反义词样本来训练一个分类器，从而更准确地确定嵌入空间中与查询相关的区域。整体实现流程包括：1) 给定文本查询，生成一组语义同义词和反义词，并添加通用的负例查询；2) 使用嵌入函数将所有文本转换为嵌入表示；3) 使用这些嵌入表示作为训练数据，训练一个分类器；4) 给定一个地图，使用训练好的分类器对地图进行分类，得到与查询匹配的区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了一种新的查询形式化方法，将查询过程视为在嵌入空间中的分类问题；2) 设计了QuASH方法，利用自然语言启发式方法生成同义词和反义词样本来训练分类器；3) 该方法不依赖于特定的嵌入表示或编码器，具有通用性；4) 通过非线性分类器而非简单的相似度阈值或线性分割来估计相关区域。相比之前的工作，不同之处在于不再依赖单一的查询嵌入和单一的负例嵌入进行比较，不使用固定的相似度阈值，而是通过训练的分类器动态确定决策边界，考虑了嵌入空间中不同维度可能具有不同语义重要性的事实，方法更加灵活，可以适应不同的视觉-语言模型和编码器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; QuASH通过利用自然语言启发式方法生成同义词和反义词样本来训练分类器，显著提高了机器人地图和图像中基于自然语言查询的准确性，同时保持了方法的通用性和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embeddings from Visual-Language Models are increasingly utilized to representsemantics in robotic maps, offering an open-vocabulary scene understanding thatsurpasses traditional, limited labels. Embeddings enable on-demand querying bycomparing embedded user text prompts to map embeddings via a similarity metric.The key challenge in performing the task indicated in a query is that the robotmust determine the parts of the environment relevant to the query.  This paper proposes a solution to this challenge. We leveragenatural-language synonyms and antonyms associated with the query within theembedding space, applying heuristics to estimate the language space relevant tothe query, and use that to train a classifier to partition the environment intomatches and non-matches. We evaluate our method through extensive experiments,querying both maps and standard image benchmarks. The results demonstrateincreased queryability of maps and images. Our querying technique is agnosticto the representation and encoder used, and requires limited training.</description>
      <author>example@mail.com (Matti Pekkanen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2510.14546v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Preference Rewarding for MLLMs Spatial Understanding</title>
      <link>http://arxiv.org/abs/2510.14374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SPR（空间偏好奖励）方法，通过奖励多模态大语言模型生成具有精确物体定位的详细响应，增强其细粒度空间理解能力，实验证明该方法有效且训练开销小。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已展现出空间理解能力，但在细粒度空间感知方面仍有不足，如无法生成详细区域描述或准确定位物体，且常无法满足用户对细粒度空间理解的需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有MLLM方法缺乏对实际响应直接监督的问题，通过SPR方法提升MLLM的细粒度空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;SPR方法通过随机选择图像区域和描述，引入语义和定位分数评估MLLM生成描述的质量；使用高定位精度描述完善MLLM输出，并将最佳完善与初始最低分描述配对进行直接偏好优化，增强与视觉输入的细粒度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在标准引用和定位基准上的大量实验表明，SPR有效提高了MLLM的空间理解能力，同时训练开销最小。&lt;h4&gt;结论&lt;/h4&gt;SPR方法能够显著增强MLLM的细粒度空间理解能力，相关数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已展现出有希望的空间理解能力，如引用和定位物体描述。尽管取得了成功，MLLMs在细粒度空间感知能力方面仍有不足，例如生成详细的区域描述或准确定位物体。此外，它们经常无法响应用户对所需细粒度空间理解的要求。这个问题可能是因为现有方法主要专注于调整MLLMs以建模预标注的指令数据来注入空间知识，而没有直接监督MLLMs的实际响应。我们通过SPR（空间偏好奖励）方法解决这个问题，通过奖励MLLMs具有精确物体定位的详细响应，而不是模糊或不准确的响应，从而增强MLLMs的空间能力。使用从MLLMs中随机选择的图像区域和区域描述，SPR引入语义和定位分数来全面评估MLLM生成描述中的文本质量和定位质量。我们还使用更好的定位精度来完善MLLM描述，并将得分最高的完善与初始得分最低的描述配对，用于直接偏好优化，从而增强与视觉输入的细粒度对齐。在标准引用和基准测试上的大量实验表明，SPR有效地提高了MLLM的空间理解能力，同时训练开销最小。数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models~(MLLMs) have demonstrated promising spatialunderstanding capabilities, such as referencing and grounding objectdescriptions. Despite their successes, MLLMs still fall short in fine-grainedspatial perception abilities, such as generating detailed region descriptionsor accurately localizing objects. Additionally, they often fail to respond tothe user's requirements for desired fine-grained spatial understanding. Thisissue might arise because existing approaches primarily focus on tuning MLLMsto model pre-annotated instruction data to inject spatial knowledge, withoutdirect supervision of MLLMs' actual responses. We address this issue by SPR, aSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatialcapabilities by rewarding MLLMs' detailed responses with precise objectlocalization over vague or inaccurate responses. With randomly selected imageregions and region descriptions from MLLMs, SPR introduces semantic andlocalization scores to comprehensively evaluate the text quality andlocalization quality in MLLM-generated descriptions. We also refine the MLLMdescriptions with better localization accuracy and pair the best-scoredrefinement with the initial descriptions of the lowest score for directpreference optimization, thereby enhancing fine-grained alignment with visualinput. Extensive experiments over standard referring and grounding benchmarksshow that SPR improves MLLM spatial understanding capabilities effectively withminimal overhead in training. Data and code will be released athttps://github.com/hanqiu-hq/SPR</description>
      <author>example@mail.com (Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu)</author>
      <guid isPermaLink="false">2510.14374v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2510.14357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SUM-AgriVLN方法，通过空间理解记忆模块改进农业视觉语言导航，解决了现有方法忽略过去经验提供空间上下文的问题，在A2A基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，但仍严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。&lt;h4&gt;目的&lt;/h4&gt;解决现有AgriVLN方法将每个导航指令视为独立片段而忽略过去经验提供空间上下文的问题，特别是在农业场景中经常出现重复导航指令的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出空间理解记忆用于农业视觉语言导航(SUM-AgriVLN)方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。&lt;h4&gt;主要发现&lt;/h4&gt;在A2A基准测试上，SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;SUM-AgriVLN方法有效利用了空间记忆来改进农业视觉语言导航性能，证明了在农业机器人导航中考虑历史经验的重要性。&lt;h4&gt;翻译&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，然而，它们仍然严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。在实际农业场景中，导航指令经常重复出现，但AgriVLN将每个指令视为独立片段，忽略了过去经验为后续指令提供空间上下文的潜力。为了弥合这一差距，我们提出了用于农业视觉语言导航的空间理解记忆方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。在A2A基准测试上评估时，我们的SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。代码：https://github.com/AlexTraveling/SUM-AgriVLN。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业机器人在视觉语言导航任务中缺乏空间长期记忆的问题。这个问题很重要，因为实际农业场景中经常需要重复执行相似导航指令，而现有方法将每个指令视为独立事件，无法利用过去经验提供的空间上下文，导致机器人导航效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类导航时会自发形成空间记忆并在后续任务中利用，而现有农业机器人缺乏这种能力。他们受日常生活中第一次和第二次去陌生地方的差异启发，设计出空间理解记忆模块。该方法借鉴了VGGT视觉编码器用于3D重建，参考了结构运动和多视图立体等3D重建技术，并在AgriVLN基础上进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入空间理解记忆模块，使机器人能够保存和利用空间记忆。整体流程包括：1)空间理解：从相机图像集中采样10帧，用VGGT生成3D重建；2)空间记忆：将3D重建渲染为点云，提取正面和倾斜两种视角的2D RGB表示并存储；3)基础模型集成：将SUM模块融入AgriVLN，在每一步加载空间记忆，结合语言指令和视觉输入预测行动；4)任务执行：持续更新子任务列表直到满足结束条件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将空间记忆引入农业视觉语言导航；2)通过3D重建和表示保存空间记忆，而非传统基于图的方法；3)提取多视角空间表示提供丰富上下文；4)能够利用任务间经验。相比之前工作，SUM-AgriVLN不同于传统VLN方法专注于农业场景，不同于AgriVLN将任务视为独立事件，不同于现有空间记忆方法依赖图结构，也不同于传统3D重建只关注几何准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的SUM-AgriVLN方法通过引入空间理解记忆模块，使农业机器人能够保存和利用空间记忆，显著提高了在重复导航任务中的成功率，从0.47提升到0.54。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agricultural robots are emerging as powerful assistants across a wide rangeof agricultural tasks, nevertheless, still heavily rely on manual operation orfixed rail systems for movement. The AgriVLN method and the A2A benchmarkpioneeringly extend Vision-and-Language Navigation (VLN) to the agriculturaldomain, enabling robots to navigate to the target positions following thenatural language instructions. In practical agricultural scenarios, navigationinstructions often repeatedly occur, yet AgriVLN treat each instruction as anindependent episode, overlooking the potential of past experiences to providespatial context for subsequent ones. To bridge this gap, we propose the methodof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation(SUM-AgriVLN), in which the SUM module employs spatial understanding and savespatial memory through 3D reconstruction and representation. When evaluated onthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,demonstrating the state-of-the-art performance in the agricultural domain.Code: https://github.com/AlexTraveling/SUM-AgriVLN.</description>
      <author>example@mail.com (Xiaobei Zhao, Xingqi Lyu, Xiang Li)</author>
      <guid isPermaLink="false">2510.14357v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration</title>
      <link>http://arxiv.org/abs/2510.14354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, accepted at ICRA 2024 (International Conference on Robotics  and Automation)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用未标记RGB-D数据进行场景几何推理的新方法，通过循环一致的关键点和结合GRU循环单元的姿态模块，提高了RGB-D配准的准确性。&lt;h4&gt;背景&lt;/h4&gt;随着消费级深度相机的普及，大量未标记的RGB-D数据变得可用，如何有效利用这些数据进行场景几何推理成为一个重要问题。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用未标记的RGB-D数据进行场景的几何推理，提高RGB-D配准的准确性。&lt;h4&gt;方法&lt;/h4&gt;不同于传统的基于几何和特征相似性的RGB-D配准方法，作者使用循环一致的关键点作为显著点强制执行空间一致性约束，并引入结合GRU循环单元和变换同步的姿态模块来融合历史和多视图数据。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet和3DMatch数据集上，该方法超越了之前的自监督配准方法，甚至优于一些旧的监督方法；将组件集成到现有方法中也证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;通过创新的循环一致关键点和姿态模块设计，有效提高了RGB-D配准的准确性，为未标记RGB-D数据的利用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;随着消费级深度相机的兴起，大量未标记的RGB-D数据变得可用。这引发了一个问题：如何利用这些数据进行场景的几何推理。虽然许多RGB-D配准方法依赖于几何和基于特征的相似性，我们采取了不同的方法。我们使用循环一致的关键点作为显著点，在匹配过程中强制执行空间一致性约束，提高对应点准确性。此外，我们引入了一个新的姿态模块，将GRU循环单元与变换同步相结合，融合历史和多视图数据。我们的方法在ScanNet和3DMatch上超越了之前的自监督配准方法，甚至优于一些旧的监督方法。我们还将我们的组件集成到现有方法中，证明了它们的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大量无标签的RGB-D数据进行场景几何推理，特别是RGB-D配准问题。这个问题很重要，因为随着消费级深度相机的普及，有大量无标签RGB-D数据可用，而RGB-D数据在机器人任务(如SLAM、无人机导航和物体姿态估计)中非常关键。传统配准方法在有噪声或特征稀少环境下表现不佳，且现有自监督方法未充分利用场景中的显著点信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考利用场景中的显著点作为锚点，这些点在多视角下易被识别且循环一致。通过空间一致性约束改善对应关系搜索，并结合GRU循环单元和变换同步融合历史和多视图信息。作者借鉴了多项现有工作：使用ResNet-18作为特征提取网络，采用LofTr的匹配策略，利用Sinkhorn归一化，参考矩阵分解方法获得循环一致匹配，受启发于GRU单元和变换同步方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用循环一致的显著点(锚点)施加空间一致性约束提高对应准确性，结合GRU和变换同步进行姿态估计。整体流程：1)使用ResNet-18提取特征；2)通过Sinkhorn归一化获得软匹配并转换为循环一致的锚点；3)使用锚点距离编码修改自注意力模块；4)定义空间一致性成本函数；5)迭代进行像素级匹配和姿态更新；6)结合GRU和变换同步改进姿态估计；7)通过内部迭代(20次)和外部迭代(3次)优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)循环一致关键点匹配模块，施加空间约束；2)无RANSAC的姿态估计方法，结合GRU和变换同步；3)空间一致性成本函数；4)迭代优化框架。不同之处：大多数自监督方法依赖特征相似性或几何信息，而本文利用场景显著点；之前循环一致性方法应用于所有像素，本文仅用于定位显著点；与[24]不同，本文用空间约束学习锚点而非修剪离群值；结合GRU和变换同步，而非仅使用一种方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过循环一致锚点和空间一致性约束提出新自监督RGB-D配准方法，显著提高配准精度，超越之前自监督方法并接近一些有监督方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA57147.2024.10610738&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data hasbecome available. This prompts the question of how to utilize this data forgeometric reasoning of scenes. While many RGB-D registration meth- ods rely ongeometric and feature-based similarity, we take a different approach. We usecycle-consistent keypoints as salient points to enforce spatial coherenceconstraints during matching, improving correspondence accuracy. Additionally,we introduce a novel pose block that combines a GRU recurrent unit withtransformation synchronization, blending historical and multi-view data. Ourapproach surpasses previous self- supervised registration methods on ScanNetand 3DMatch, even outperforming some older supervised methods. We alsointegrate our components into existing methods, showing their effectiveness.</description>
      <author>example@mail.com (Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy)</author>
      <guid isPermaLink="false">2510.14354v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.13993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 8 tables. To be published in Applied AI Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了结合传统视觉模型与视觉语言模型(VLMs)以增强遥感图像分析，特别是在飞机检测和场景理解方面的应用。通过集成YOLO与LLaVA、ChatGPT和Gemini等VLMs，实现了更准确和具有上下文意识的图像解释。&lt;h4&gt;背景&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等领域的关键工具，数据量显著增加。然而，传统视觉模型受限于需要大量领域特定标记数据且在理解复杂环境上下文方面能力有限。视觉语言模型虽能整合视觉和文本数据，但在遥感领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型与VLMs的结合，以增强遥感图像分析，专注于飞机检测和场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;集成YOLO与VLMs(如LLaVA、ChatGPT和Gemini)，在标记和未标记的遥感数据以及退化图像场景上评估性能，旨在实现更准确和具有上下文意识的图像解释。&lt;h4&gt;主要发现&lt;/h4&gt;在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性平均提高了48.46%。在遥感图像的全面理解方面，CLIPScore提高了6.17%。&lt;h4&gt;结论&lt;/h4&gt;结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别在少样本学习场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等跨领域的关键工具。尽管生成的数据量显著增加，但传统视觉模型通常受限于需要大量领域特定标记数据及其在理解复杂环境中上下文能力的有限性。视觉语言模型通过整合视觉和文本数据提供了一种互补方法；然而，它们在遥感领域的应用仍未得到充分探索，特别是考虑到它们的通用性质。本研究探讨了结合视觉模型和VLMs以增强遥感图像分析，专注于飞机检测和场景理解。将YOLO与LLaVA、ChatGPT和Gemini等VLMs的集成旨在实现更准确和具有上下文意识的图像解释。性能在标记和未标记的遥感数据以及退化图像场景上进行了评估，这些场景对遥感至关重要。研究显示，在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性在各类模型中平均提高了48.46%。在遥感图像的全面理解方面，获得了6.17%的CLIPScore提升。结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别是在少样本学习场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing has become a vital tool across sectors such as urban planning,environmental monitoring, and disaster response. While the volume of datagenerated has increased significantly, traditional vision models are oftenconstrained by the requirement for extensive domain-specific labelled data andtheir limited ability to understand the context within complex environments.Vision Language Models offer a complementary approach by integrating visual andtextual data; however, their application to remote sensing remainsunderexplored, particularly given their generalist nature. This workinvestigates the combination of vision models and VLMs to enhance imageanalysis in remote sensing, with a focus on aircraft detection and sceneunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, andGemini aims to achieve more accurate and contextually aware imageinterpretation. Performance is evaluated on both labelled and unlabelled remotesensing data, as well as degraded image scenarios which are crucial for remotesensing. The findings show an average MAE improvement of 48.46% across modelsin the accuracy of aircraft detection and counting, especially in challengingconditions, in both raw and degraded scenarios. A 6.17% improvement inCLIPScore for comprehensive understanding of remote sensing images is obtained.The proposed approach combining traditional vision models and VLMs paves theway for more advanced and efficient remote sensing image analysis, especiallyin few-shot learning scenarios.</description>
      <author>example@mail.com (Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania)</author>
      <guid isPermaLink="false">2510.13993v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.07944v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CVD-STORM是一个跨视图视频扩散模型，利用空间-时间重建变分自编码器生成长期多视图视频并具备4D重建能力。&lt;h4&gt;背景&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟、未来状态预测。随着自动驾驶的发展，对高质量视频生成以及深度估计等多样化有意义信息的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;提出CVD-STORM模型，能够在各种控制输入下生成长期多视图视频，具备4D重建能力。&lt;h4&gt;方法&lt;/h4&gt;首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力；然后将这个VAE集成到视频扩散过程中提高生成质量；联合训练的高斯溅射解码器有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在FID和FVD指标上都取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;CVD-STORM模型能够在各种控制条件下生成高质量的多视图视频，并有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;翻译&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟以及未来状态预测。随着自动驾驶的发展，不仅需要高质量的视频生成，还需要产生多样化和有意义的信息如深度估计。为此，我们提出了CVD-STORM，这是一个利用空间-时间重建变分自编码器的跨视图视频扩散模型，能够在各种控制输入下生成具有4D重建能力的长期多视图视频。我们的方法首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力。随后，我们将这个VAE集成到视频扩散过程中，显著提高了生成质量。实验结果表明，我们的模型在FID和FVD指标上都取得了显著改进。此外，联合训练的高斯溅射解码器有效地重建动态场景，为全面场景理解提供了有价值的几何信息。我们的项目页面是https://sensetime-fvg.github.io/CVD-STORM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中高质量视频生成和4D场景重建的问题。具体来说，现有方法难以同时生成长期、多视角的视频并提供准确的深度信息，这限制了自动驾驶系统对环境的模拟和未来状态的预测能力。这个问题在现实中非常重要，因为自动驾驶需要准确的环境模拟来训练决策算法和验证规划输出，而深度信息对于理解场景的3D结构至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有视频扩散模型在生成长期、多视角视频方面的局限性，以及缺乏明确3D信息的问题。他们借鉴了多项现有工作：基于现有的扩散模型架构（如DiT），参考了STORM模型的空间-时间重建方法，采用了UniMLVG的多模态DiT架构和训练策略，利用了3D高斯溅射技术进行场景重建，并结合了VAE进行表示学习。作者通过整合这些技术，设计了一个两阶段训练策略：先学习场景重建，再训练条件世界模型，以实现高质量的视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过STORM-VAE（一个扩展的VAE模型，集成了高斯溅射解码器）进行4D场景重建，并利用CVD-STORM框架同时生成多视角视频和重建4D场景。整体实现流程分为三部分：1）STORM-VAE训练：使用预训练的图像VAE，添加高斯溅射解码器分支，通过多视图图像和相机姿态进行训练；2）CVD-STORM训练：使用STORM-VAE作为潜在编码器，在扩散模型中集成STORM-VAE，使用三个不同的transformer块处理不同维度；3）推理过程：生成长期六视角视频，高斯溅射解码器直接从生成的潜在表示重建4D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）STORM-VAE：一个能进行4D场景重建的扩展VAE模型；2）CVD-STORM：统一框架同时生成多视角视频和重建4D场景；3）两阶段训练策略：先学习场景重建，再训练条件世界模型；4）增强的表示学习：通过空间-时间重建模型提高生成质量；5）单阶段训练策略：简化训练过程，降低计算成本。相比之前的工作，CVD-STORM实现了真正的端到端交互（不同于MagicDrive3D的两阶段流水线），提供绝对深度估计（不同于UniFuture和GEM的相对深度），重建过程对生成模型有直接影响，并能同时完成多视角视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVD-STORM通过引入STORM-VAE和统一的生成-重建框架，实现了高质量的多视角视频生成和准确的4D场景重建，为自动驾驶提供了更强大的世界模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models have been widely applied to world modeling for environmentsimulation and future state prediction. With advancements in autonomousdriving, there is a growing demand not only for high-fidelity video generationunder various controls, but also for producing diverse and meaningfulinformation such as depth estimation. To address this, we propose CVD-STORM, across-view video diffusion model utilizing a spatial-temporal reconstructionVariational Autoencoder (VAE) that generates long-term, multi-view videos with4D reconstruction capabilities under various control inputs. Our approach firstfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing itsability to encode 3D structures and temporal dynamics. Subsequently, weintegrate this VAE into the video diffusion process to significantly improvegeneration quality. Experimental results demonstrate that our model achievessubstantial improvements in both FID and FVD metrics. Additionally, thejointly-trained Gaussian Splatting Decoder effectively reconstructs dynamicscenes, providing valuable geometric information for comprehensive sceneunderstanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.</description>
      <author>example@mail.com (Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu)</author>
      <guid isPermaLink="false">2510.07944v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</title>
      <link>http://arxiv.org/abs/2510.14672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了VTimeCoT框架，一种无需训练的方法，用于解决多模态大语言模型在视频时序定位和推理方面的缺陷，通过引入进度条视觉工具和跨模态推理过程，实现了显著的性能提升和可解释的推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频问答基于多模态大语言模型近年来受到关注，但这类模型在视频时序定位和推理方面存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个简单而有效的无需训练框架，用于高性能的视频时序定位和推理，解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出VTimeCoT框架，包含两个新颖的视觉工具：即插即用的进度条集成工具和高效率高亮工具，同时引入整合视频和文本跨模态推理的视觉时序思考链过程。&lt;h4&gt;主要发现&lt;/h4&gt;在视频时序定位和基于推理的问答任务中，该方法对Qwen2VL-7B和GPT4o基线模型显示出显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架实现了可组合且可解释的推理过程，有效提升了视频理解系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于多模态大语言模型(MLLM)的视频问答因其受益于LLMs的显著进步而受到广泛关注。然而，这些模型在视频时序定位和推理领域存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。受人类使用视频播放器与进度条交互以理解视频的启发，我们引入了VTimeCoT，一个简单而有效的无需训练框架，专为高性能视频定位和推理而设计。该框架包含两个新颖的进度条视觉工具：即插即用的进度条集成工具和高效率高亮工具。此外，为解决传统基于文本的思考链(CoT)方法的局限性，我们引入了一个整合视频和文本跨模态推理的视觉时序思考链过程。我们的方法在视频时序定位和基于推理的问答任务中，对Qwen2VL-7B和GPT4o基线模型均显示出显著的性能改进。最后，我们展示了所提出的框架实现了可组合且可解释的推理过程。项目页面：https://vtimecot.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, video question answering based on multimodal large languagemodels (MLLM) has garnered considerable attention, due to the benefits from thesubstantial advancements in LLMs. However, these models have a notabledeficiency in the domains of video temporal grounding and reasoning, posingchallenges to the development of effective real-world video understandingsystems. Inspired by how humans use video players to interact with the progressbar for video comprehension, we introduce VTimeCoT, a simple yet effectivetraining-free framework, designed for high-performance video grounding andreasoning. The proposed framework incorporates two novel visual tools of theprogress bar: a plug-and-play progress bar integration tool and ahigh-efficiency highlighting tool. In addition, to address the limitations ofconventional text-based chain-of-thought (CoT) approaches, we introduce avisuotemporal CoT process that integrates cross-modality reasoning across bothvideo and text. Our approach demonstrates significant performance improvementson both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding andreasoning-based question answering. Finally, we showcase that the proposedframework achieves a compositional and interpretable reasoning process. Projectpage: https://vtimecot.github.io</description>
      <author>example@mail.com (Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma)</author>
      <guid isPermaLink="false">2510.14672v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.14032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight). Webpage at  https://xiaoqian-shen.github.io/Vgent&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Vgent，一种基于图的检索-推理-增强生成框架，用于增强大型视频语言模型对长视频的理解能力。通过结构化图表示视频和引入中间推理步骤，有效解决了长视频处理中的时间依赖性问题和检索噪声问题，在多个基准测试上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，主要因为难以处理超出上下文窗口密集的视频token，并保留长期顺序信息。检索增强生成(RAG)虽然对处理长上下文有效，但应用于长视频时面临时间依赖性被打乱和包含无关信息等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架增强LVLMs对长视频的理解能力，解决长视频处理中的时间依赖性问题和检索噪声问题，提高模型在长视频理解任务中的准确性和上下文感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出Vgent框架，包含两个关键创新：(1)使用结构化图表示视频，保留视频片段间的语义关系以提高检索效果；(2)引入中间推理步骤，利用结构化验证减少检索噪声，促进相关信息片段的显式聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在MLVU基准测试上，与基础模型相比，总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;h4&gt;结论&lt;/h4&gt;Vgent框架通过结构化图表示和中间推理步骤，有效解决了长视频理解中的关键挑战，显著提升了LVLMs的性能，为长视频理解任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，因为难以处理超出上下文窗口密集的视频token并保留长期顺序信息。检索增强生成(RAG)在处理大型语言模型(LLMs)的长上下文方面已显示出有效性；然而，将RAG应用于长视频面临时间依赖性被打乱和包含无关信息等挑战，这些都会妨碍准确推理。为解决这些局限性，我们提出了Vgent，一种新颖的基于图的检索-推理-增强生成框架，用于增强LVLMs对长视频的理解能力。我们的方法引入了两个关键创新：(i)它通过保留视频片段间的语义关系，使用结构化图表示视频，以提高检索效果。(ii)它引入中间推理步骤，缓解LVLMs的推理局限性，利用结构化验证减少检索噪声，促进相关信息的显式聚合，从而产生更准确和上下文感知的响应。我们在三个长视频理解基准测试上使用各种开源LVLMs全面评估了我们的框架。与基础模型相比，我们的方法在MLVU上总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。我们的代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning over long videos pose significant challenges forlarge video language models (LVLMs) due to the difficulty in processingintensive video tokens beyond context window and retaining long-term sequentialinformation. Retrieval-Augmented Generation (RAG) has demonstratedeffectiveness in processing long context for Large Language Models (LLMs);however, applying RAG to long video faces challenges such as disrupted temporaldependencies and inclusion of irrelevant information that can hinder accuratereasoning. To address these limitations, we propose Vgent, a novel graph-basedretrieval-reasoning-augmented generation framework to enhance LVLMs for longvideo understanding. Our approach introduces two key innovations: (i) Itrepresents videos by structured graphs with semantic relationships across videoclips preserved to improve retrieval effectiveness. (ii) It introduces anintermediate reasoning step to mitigate the reasoning limitation of LVLMs,which leverages structured verification to reduce retrieval noise andfacilitate the explicit aggregation of relevant information across clips,resulting in more accurate and context-aware responses. We comprehensivelyevaluate our framework with various open-source LVLMs on three long-videounderstanding benchmarks. Our approach yielded an overall performanceimprovement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformedstate-of-the-art video RAG methods by $8.6\%$. Our code is publicly availableat https://xiaoqian-shen.github.io/Vgent.</description>
      <author>example@mail.com (Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny)</author>
      <guid isPermaLink="false">2510.14032v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了时空视频动作定位(SVAG)任务，要求模型同时检测、跟踪和基于自然语言描述对视频中的相关对象进行时空定位。研究团队构建了SVAG-Bench基准数据集，提出了SVAGFormer基线框架，并开发了SVAGEVal评估工具。实验表明现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确时空定位是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在联合检测、跟踪和时空定位视频中的相关对象方面的不足，推进细粒度动作理解和对象-动作交互的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出了时空视频动作定位(SVAG)任务，构建了SVAG-Bench大型基准数据集（包含688个视频、19,590条标注记录和903个独特动词），提出了SVAGFormer基线框架（适应最先进的视觉语言模型进行联合时空定位），并开发了SVAGEVal标准化评估工具包。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这凸显了对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;该研究为细粒度视频理解和对象-动作交互建立了新的基准和评估框架，强调了开发能够处理复杂场景和长视频中高级推理能力的模型的重要性。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位其在空间和时间中对应的执行者是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，因此忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。为解决这一差距，我们引入了时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时空定位视频中所有相关对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，它适应了最先进的视觉语言模型进行联合时空定位，并引入了SVAGEVal，这是一个标准化的评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这凸显了需要对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding</title>
      <link>http://arxiv.org/abs/2510.13891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态大语言模型在长视频理解方面面临上下文窗口和计算成本限制，现有关键帧选择方法存在信息丢失和场景连续性问题。作者提出K-frames方法，通过预测语义连贯的视频片段而非单个帧，保持时间连续性，支持灵活的多尺度关键帧选择。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在图像理解方面表现出色，但在长视频理解方面受到上下文窗口和计算成本的限制。均匀采样通常会导致大量信息丢失。&lt;h4&gt;目的&lt;/h4&gt;解决现有关键帧选择方法的问题，提出一种能够保持时间连续性的场景驱动关键帧选择方法。&lt;h4&gt;方法&lt;/h4&gt;K-frames方法预测语义连贯、与查询相关的视频片段而非单个帧，支持任意数量的关键帧选择。作者构建了包含20万个基于查询条件的视频亮点的PeakClips数据集，并采用三阶段渐进式课程学习：两个监督微调阶段（时间定位和关键片段感知）和一个强化学习阶段（优化场景驱动的预测策略）。&lt;h4&gt;主要发现&lt;/h4&gt;在主要的长视频理解基准上的大量实验表明，K-frames在各种规模的关键帧选择方面提供了有效、可解释且即插即用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;K-frames方法解决了长视频理解中的关键帧选择问题，作者公开的数据集和模型将为该领域提供支持。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在图像理解方面已展现出显著能力，但在长视频处理中受限于上下文窗口和计算成本。均匀采样常导致大量信息丢失。同时，现有的关键帧选择方法如文本-帧检索或基于强化学习的帧优化通常产生稀疏且时间上不连续的帧，忽略了场景连续性，缺乏多尺度帧选择的灵活性。为解决这些问题，我们引入K-frames，一种保持时间连续性的场景驱动关键帧选择新范式。K-frames不选择单个帧，而是预测语义连贯、与查询相关的片段，支持任意数量的关键帧选择以满足不同用户需求。为实现这一方法，我们首先引入PeakClips数据集，包含20万个基于查询条件的视频亮点。基于此数据集，K-frames使用三阶段渐进式课程学习clip2frame选择，包括两个监督微调阶段（用于时间定位和关键片段感知）和一个强化学习阶段（直接优化场景驱动的预测策略，无需额外注释）。在主要长视频理解基准上的大量实验表明，K-frames为各种规模的关键帧选择提供了有效、可解释且即插即用的解决方案。我们的数据集和模型将会公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantcapabilities in image understanding, but long-video are constrained by contextwindows and computational cost. Uniform frame sampling often leads tosubstantial information loss. Meanwhile existing keyframe selection methodssuch as text-frame retrieval or RL-based frame optimization typically yieldsparse and temporally disjointed frames, overlooking scene continuity andlacking flexibility for multi-scale frame selection. To address theselimitations, we introduce K-frames, a novel paradigm for scene-driven keyframeselection that preserves temporal continuity. Instead of selecting individualframes, K-frames predicts semantically coherent, query-relevant clips, whichenables any-k keyframes selection to meet diverse user budgets. To achieve thisapproach, we first introduce PeakClips, a dataset of 200K video highlightsconditioned by query. Building on this dataset, K-frames learns clip2frameselection using a three-stage progressive curriculum. It involves twoSupervised Fine-Tuning stages for temporal grounding and key-clip perception,followed by a Reinforcement Learning stage that directly optimizes thescene-driven prediction policy for downstream task without further annotations.Extensive experiments on major long-video understanding benchmarks demonstratethat K-frames provides an effective, interpretable, and plug-and-play solutionfor keyframe selection at various scales. Our dataset and model will beavailable.</description>
      <author>example@mail.com (Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang)</author>
      <guid isPermaLink="false">2510.13891v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ChangingGrounding: 3D Visual Grounding in Changing Scenes</title>
      <link>http://arxiv.org/abs/2510.14965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ChangingGrounding基准和Mem-ChangingGrounder方法，用于解决动态场景中3D视觉目标定位问题，通过利用过去观察信息减少探索成本。&lt;h4&gt;背景&lt;/h4&gt;现实世界机器人需要从自然语言指令中定位物体，同时周围场景不断变化。现有3D视觉目标定位方法假设已重建且最新的点云，这导致需要昂贵的重新扫描，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;将3DVG表述为主动的、内存驱动的问题，引入ChangingGrounding基准来衡量代理如何有效利用过去观察、只在需要处探索，并在变化场景中提供精确3D边界框。&lt;h4&gt;方法&lt;/h4&gt;提出Mem-ChangingGrounder零样本方法，结合跨模态检索与轻量级多视图融合：识别物体类型、检索相关记忆指导动作、高效探索目标、操作无效时回退、多视图扫描目标、融合多视图证据获取准确边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在ChangingGrounding基准上评估不同基线方法，Mem-ChangingGrounder实现最高定位精度，同时显著降低探索成本。&lt;h4&gt;结论&lt;/h4&gt;希望该基准和方法能推动面向实际应用的、以内存为中心的3DVG研究转变。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的机器人从自然语言指令中定位物体，同时周围场景不断变化。然而，大多数现有的3D视觉目标定位方法仍然假设已重建且最新的点云，这种假设迫使昂贵的重新扫描并阻碍部署。我们认为3DVG应表述为主动的、内存驱动的问题，并引入ChangingGrounding，这是第一个明确衡量代理如何有效利用过去观察、只在需要处探索并在变化场景中提供精确3D边界框的基准。为设定强参考点，我们还提出了Mem-ChangingGrounder，这是一种针对此任务的零样本方法，它结合了跨模态检索与轻量级多视图融合：识别查询暗示的物体类型，检索相关记忆指导动作，然后在场景中高效探索目标，在先前操作无效时回退，对目标进行多视图扫描，并将多视图扫描的融合证据投影以获得准确的物体边界框。我们在ChangingGrounding上评估了不同的基线方法，我们的Mem-ChangingGrounder实现了最高的定位精度，同时大大减少了探索成本。我们希望这个基准和方法能够推动面向实际应用的、以内存为中心的3DVG研究转变。项目页面：https://hm123450.github.io/CGB/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉定位在动态变化场景中的挑战。现有方法假设场景静态且拥有完整点云，但真实环境中物体会移动或被遮挡，导致机器人需要频繁重新扫描整个场景，这非常耗时且成本高昂。这个问题在现实中很重要，因为它限制了机器人在动态环境（如家庭、办公室）中的实用性，增加了能耗并降低了效率，而人类却能利用过去记忆快速适应变化环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类认知方式获得灵感，人类在动态环境中会利用过去记忆高效定位目标。作者将3D视觉定位重新定义为'主动的、记忆驱动的问题'。他们借鉴了VLM-Grounder的框架（使用2D图像而非点云）、3RScan数据集（提供不同时间点的场景扫描和物体对应关系），以及视觉语言模型和开放词汇检测器等现有技术。设计的Mem-ChangingGrounder方法结合了记忆检索、智能探索和回退策略，以应对场景变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用机器人对过去场景的记忆来指导在当前变化场景中的高效探索，避免盲目扫描整个场景。整体流程包括：1)查询分类：将查询分为'可验证'（即使目标移动，记忆中的目标仍匹配查询）和'不可验证'（记忆中的目标可能不再匹配）；2)记忆检索与定位：根据查询类型选择策略，使用全景扫描或空间关系感知扫描寻找目标；3)回退策略：当主策略失败时，从记忆检索目标类别并进行360度搜索；4)多视图投影：结合多视图信息生成精确3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次定义动态场景中的3D视觉定位任务，强调利用过去记忆；2)提出ChangingGrounding基准数据集，包含267K个参照性描述，评估定位准确性和探索成本；3)设计Mem-ChangingGrounder方法，结合记忆检索和智能探索；4)引入探索成本指标，强调效率。相比之前工作，本文不再假设场景静态，而是设计基于智能体的方法，利用2D图像和记忆避免昂贵的点云重建，同时关注准确性和效率的平衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个面向动态场景的3D视觉定位基准和方法，通过结合记忆检索和智能探索策略，实现了在变化环境中高效且准确的物体定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robots localize objects from natural-language instructions whilescenes around them keep changing. Yet most of the existing 3D visual grounding(3DVG) method still assumes a reconstructed and up-to-date point cloud, anassumption that forces costly re-scans and hinders deployment. We argue that3DVG should be formulated as an active, memory-driven problem, and we introduceChangingGrounding, the first benchmark that explicitly measures how well anagent can exploit past observations, explore only where needed, and stilldeliver precise 3D boxes in changing scenes. To set a strong reference point,we also propose Mem-ChangingGrounder, a zero-shot method for this task thatmarries cross-modal retrieval with lightweight multi-view fusion: it identifiesthe object type implied by the query, retrieves relevant memories to guideactions, then explores the target efficiently in the scene, falls back whenprevious operations are invalid, performs multi-view scanning of the target,and projects the fused evidence from multi-view scans to get accurate objectbounding boxes. We evaluate different baselines on ChangingGrounding, and ourMem-ChangingGrounder achieves the highest localization accuracy while greatlyreducing exploration cost. We hope this benchmark and method catalyze a shifttoward practical, memory-centric 3DVG research for real-world applications.Project page: https://hm123450.github.io/CGB/ .</description>
      <author>example@mail.com (Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu)</author>
      <guid isPermaLink="false">2510.14965v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://lei-kun.github.io/RL-100/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RL-100，一个基于扩散视觉运动策略的真实世界强化学习训练框架，通过三阶段流程实现高效可靠的机器人操作，并在多个任务上达到100%成功率。&lt;h4&gt;背景&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。&lt;h4&gt;目的&lt;/h4&gt;开发一个真实世界的强化学习训练框架，实现高效、可靠且通用的机器人操作能力。&lt;h4&gt;方法&lt;/h4&gt;RL-100框架采用三阶段流程：首先通过模仿学习利用人类先验知识；其次使用离线策略评估(OPE)进行迭代离线强化学习；最后通过在线强化学习消除剩余失败模式。此外，添加轻量级一致性蒸馏头将多步采样压缩为单步策略，实现高频控制并降低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实机器人任务上评估，包括动态刚体控制、流体倾倒、布料折叠、拧螺丝和橙汁制作等，RL-100实现了900/900的100%成功率，包括连续250次试验全部成功。该方法达到接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，可连续运行长达两小时。&lt;h4&gt;结论&lt;/h4&gt;RL-100是一个与任务、具身和表示无关的通用框架，支持多种输入和机器人平台，能够实现与人类相当或更好的机器人操作性能。&lt;h4&gt;翻译&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。我们提出了RL-100，一个基于通过监督学习训练的扩散视觉运动策略构建的真实世界强化学习训练框架。RL-100引入了一个三阶段流程。首先，模仿学习利用人类先验知识。其次，迭代离线强化学习使用离线策略评估(OPE)程序来筛选PPO风格的更新，并在去噪过程中应用这些更新，以实现保守可靠的改进。第三，在线强化学习消除剩余的失败模式。此外，添加的轻量级一致性蒸馏头将扩散中的多步采样过程压缩为单步策略，实现了高频控制，同时延迟减少一个数量级，并保留了任务性能。该框架与任务、具身和表示无关，支持3D点云和2D RGB输入，各种机器人平台，以及单步和动作块策略。我们在七个真实机器人任务上评估了RL-100，包括动态刚体控制（如推-T和敏捷保龄球）、流体和颗粒倾倒、可变形布料折叠、精确灵巧拧螺丝和多阶段橙汁制作。RL-100在总共900个评估试验中实现了100%成功率，包括在一个任务上连续250次试验全部成功。该方法实现了接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，不间断运行时间长达两小时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robotic manipulation in homes and factories demands reliability,efficiency, and robustness that approach or surpass skilled human operators. Wepresent RL-100, a real-world reinforcement learning training framework built ondiffusion visuomotor policies trained bu supervised learning. RL-100 introducesa three-stage pipeline. First, imitation learning leverages human priors.Second, iterative offline reinforcement learning uses an Offline PolicyEvaluation procedure, abbreviated OPE, to gate PPO-style updates that areapplied in the denoising process for conservative and reliable improvement.Third, online reinforcement learning eliminates residual failure modes. Anadditional lightweight consistency distillation head compresses the multi-stepsampling process in diffusion into a single-step policy, enablinghigh-frequency control with an order-of-magnitude reduction in latency whilepreserving task performance. The framework is task-, embodiment-, andrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, avariety of robot platforms, and both single-step and action-chunk policies. Weevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,such as Push-T and Agile Bowling, fluids and granular pouring, deformable clothfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100attains 100\% success across evaluated trials for a total of 900 out of 900episodes, including up to 250 out of 250 consecutive trials on one task. Themethod achieves near-human teleoperation or better time efficiency anddemonstrates multi-hour robustness with uninterrupted operation lasting up totwo hours.</description>
      <author>example@mail.com (Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu)</author>
      <guid isPermaLink="false">2510.14830v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery</title>
      <link>http://arxiv.org/abs/2510.14768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为接触感知动态恢复(CADRE)的强化学习框架，用于在灵巧操作中处理意外错误和干扰，特别是接住下落物体并系统重置以恢复主要任务。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。&lt;h4&gt;方法&lt;/h4&gt;提出接触感知动态恢复(CADRE)框架，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征，直接推理手指-物体对应关系并适应不同物体几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。&lt;h4&gt;结论&lt;/h4&gt;CADRE框架可以零样本泛化到具有不同几何形状的未见物体上，证明了其在实际应用中的有效性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。为了应对这一挑战，我们专注于在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。我们提出了接触感知动态恢复(CADRE)，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征。与仅依赖物体姿态或点云输入的方法相比，NDF可以直接推理手指-物体对应关系并适应不同的物体几何形状。实验表明，整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。此外，我们证明了CADRE可以零样本泛化到具有不同几何形状的未见物体上。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在灵巧操作中遇到意外错误和干扰时的恢复问题，特别是如何抓住掉落的物体并恢复到有利于继续主要操作任务的状态。这个问题在现实中很重要，因为机器人执行实际任务时经常遇到意外干扰，如螺丝卡住导致产生意外扭矩，可能导致物体掉落。仅仅抓住物体是不够的，还需要恢复到适合继续主要任务的状态，例如从强力抓取切换到精确抓取，这对实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到动态恢复问题的重要性，特别是抓住掉落物体并恢复到有利于继续主要操作任务的状态。他们观察到接触在灵巧操作中的重要性，并认为保持不同物体几何形状间一致的接触行为是成功泛化的基本因素。作者借鉴了Neural Descriptor Fields (NDF)来提取隐式接触特征，NDF能够捕获3D坐标和物体点云之间的几何对应关系。他们设计了Contact-Aware Dynamic Recovery (CADRE)框架，将NDF特征作为隐式接触信息整合到强化学习中。同时，他们借鉴了强化学习(特别是PPO算法)、点云表示方法以及DexPoint中利用接触信息提高泛化的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用神经描述场(NDF)提取隐式接触特征，使机器人能够推理手指-物体对应关系并适应不同物体几何形状，同时不仅关注抓住掉落物体，还关注恢复到有利于继续主要操作任务的状态。整体实现流程包括：1)使用预训练的NDF模型提取接触特征，在机器人手上预定义关键点并查询这些点的NDF特征形成抓取特征；2)设计强化学习框架，将观察(机器人关节角度、物体姿态、物体速度)和抓取特征结合作为输入，使用PPO算法优化策略，并设计多目标奖励函数；3)在螺丝刀和螺母恢复任务上评估方法，测试泛化能力，并在真实机器人硬件上部署验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通过抓取进行恢复的问题，机器人不仅要抓住掉落的物体，还要实现能够无缝恢复主要操作任务的抓取配置；2)开发了基于NDF的隐式接触表示，用于接触丰富的灵巧操作，有效捕获手部和操作物体之间的几何对应关系；3)提出了用于动态恢复的强化学习框架，利用这种表示实现成功的抓取和有利于后续操作任务的状态；4)证明了这种接触表示能够在不同几何形状的动态恢复任务中实现有效的泛化。相比之前工作，我们的方法不仅关注稳定抓取，还考虑后续操作任务的需求；NDF可以直接推理手指-物体对应关系并适应不同物体几何形状；提供了比点云方法更全面的接触建模；相比DexPoint，我们的方法区分了应该接触和应该避免接触的区域。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CADRE，一种利用神经描述场提取隐式接触特征的强化学习框架，使机器人能够在灵巧操作中从掉落物体中恢复并回到有利于继续主要操作任务的状态，同时实现了对不同物体几何形状的零样本泛化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world dexterous manipulation often encounters unexpected errors anddisturbances, which can lead to catastrophic failures, such as dropping themanipulated object. To address this challenge, we focus on the problem ofcatching a falling object while it remains within grasping range and,importantly, resetting the system to a configuration favorable for resuming theprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), areinforcement learning framework that incorporates a Neural Descriptor Field(NDF)-inspired module to extract implicit contact features. Compared to methodsthat rely solely on object pose or point cloud input, NDFs can directly reasonabout finger-object correspondence and adapt to different object geometries.Our experiments show that incorporating contact features improves trainingefficiency, enhances convergence performance for RL training, and ultimatelyleads to more successful recoveries. Additionally, we demonstrate that CADREcan generalize zero-shot to unseen objects with different geometries.</description>
      <author>example@mail.com (Fan Yang, Zixuan Huang, Abhinav Kumar, Sergio Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson)</author>
      <guid isPermaLink="false">2510.14768v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning</title>
      <link>http://arxiv.org/abs/2510.14584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的可放置性度量方法，可以直接从嘈杂点云评估放置姿态，无需任何形状先验知识。该方法联合评分稳定性、可抓取性和间隙，实现无需模型的统一抓取-放置推理。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然具有挑战性。现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验；实现统一抓取-放置推理；在未见过的真实物体和非平面物体支撑上提供准确的稳定性预测和物理合理的放置结果。&lt;h4&gt;方法&lt;/h4&gt;引入通用的可放置性度量方法；从原始几何形状中提取物体的支撑表面；生成多样化的多方向放置候选；采样满足碰撞和稳定性约束的接触点；将抓取分数与每个候选放置相关联，实现无需模型的统一抓取-放置推理。&lt;h4&gt;主要发现&lt;/h4&gt;在未见过的真实物体和非平面物体支撑上，该方法与CAD模型相当的准确性预测稳定性损失；比基于学习的方法产生更物理合理的放置结果。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够实现无需模型的统一抓取-放置推理；在现实世界的噪声条件下，能够准确预测稳定性损失并产生物理合理的放置结果；克服了现有方法对强物体先验和平面支撑假设的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然是一项具有挑战性的任务，因为现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。在这项工作中，我们引入了一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验。该度量方法联合评分稳定性、可抓取性和间隙。从原始几何形状中，我们提取物体的支撑表面，生成多样化的多方向放置候选，并采样满足碰撞和稳定性约束的接触点。通过将抓取分数与每个候选放置相关联，我们提出的方法实现了无需模型的统一抓取-放置推理，并选择导致稳定、无碰撞放置的抓取-放置对。在未见过的真实物体和非平面物体支撑上，我们的度量方法在预测稳定性损失方面提供了与CAD模型相当的准确性，并且通常比基于学习方法的预测器产生更物理合理的放置结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在现实世界感知噪声下可靠地抓取和放置未知物体的问题。这个问题很重要，因为抓取和放置能力对仓库物流、家庭辅助和医疗保健等机器人应用至关重要，而现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了在复杂和噪声环境中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：大多数方法使用强物体先验评估放置稳定性，或只评估少量预定义的放置姿态。作者借鉴了统一抓取和放置推理的思想，从点云处理中学习物体重建方法，并改进了稳定性评估以处理部分和噪声观测。新方法设计了一个通用的可放置性度量，直接从嘈杂点云评估放置姿态，融合物理可行性和机器人约束，实现无模型统一的抓取和放置推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个通用的可放置性度量，直接从传感器数据评估放置姿态，融合稳定性、放置条件下的可抓取性和间隙三个因素，通过统一评分抓取和放置候选，选择最佳组合。整体流程包括：1)感知阶段重建工作空间和物体点云；2)生成和评分候选抓取；3)生成多样化放置候选；4)计算可放置性评分（稳定性、PCG、间隙）；5)统一推理选择最佳抓取-放置组合进行执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)通用可放置性度量，直接从嘈杂点云评估放置姿态；2)无模型统一抓取和放置推理，在共同物体框架中评估；3)物理有效性验证，在非平面支撑和边缘情况表现优异；4)任务驱动的放置评估，支持操作偏好。相比之前工作：不需要CAD模型或平面支撑假设；能处理边缘附近的物体；提供通用分数而非单一稳定平面；适合在线部署，不依赖重型预测网络。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通用的可放置性度量，使机器人能够直接从嘈杂点云评估未知物体的稳定放置，实现无模型统一的抓取和放置推理，显著提高了在复杂环境中的操作成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reliably pick and place unknown objects under real-world sensing noiseremains a challenging task, as existing methods rely on strong object priors(e.g., CAD models), or planar-support assumptions, limiting generalization andunified reasoning between grasping and placing. In this work, we introduce ageneralized placeability metric that evaluates placement poses directly fromnoisy point clouds, without any shape priors. The metric jointly scoresstability, graspability, and clearance. From raw geometry, we extract thesupport surfaces of the object to generate diverse candidates formulti-orientation placement and sample contacts that satisfy collision andstability constraints. By conditioning grasp scores on each candidateplacement, our proposed method enables model-free unified pick-and-placereasoning and selects grasp-place pairs that lead to stable, collision-freeplacements. On unseen real objects and non-planar object supports, our metricdelivers CAD-comparable accuracy in predicting stability loss and generallyproduces more physically plausible placements than learning-based predictors.</description>
      <author>example@mail.com (Benno Wingender, Nils Dengler, Rohit Menon, Sicong Pan, Maren Bennewitz)</author>
      <guid isPermaLink="false">2510.14584v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification</title>
      <link>http://arxiv.org/abs/2510.14576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别任务。&lt;h4&gt;背景&lt;/h4&gt;车辆重识别面临的主要挑战是从三维点云中学习判别性和互补性特征来区分不同车辆。&lt;h4&gt;目的&lt;/h4&gt;提出CALM-Net模型，通过整合曲率感知信息提高车辆重识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入（用于表征点云中的局部表面变化），学习丰富的几何和上下文特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，CALM-Net相比最强基线方法，平均重识别准确率提高了约1.97个百分点。&lt;h4&gt;结论&lt;/h4&gt;将曲率信息整合到深度学习架构中并采用多分支特征学习，能有效提升基于激光雷达点云的车辆重识别性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别。所提出的模型解决了从三维点云中学习判别性和互补性特征以区分车辆这一挑战。CALM-Net采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入，后者用于表征点云中的局部表面变化。通过结合这些机制，模型学习更适合重识别任务的丰富几何和上下文特征。在大型nuScenes数据集上的实验评估表明，CALM-Net比我们研究中最强的基线方法平均提高了约1.97个百分点的重识别准确率。结果证实了将曲率信息整合到深度学习架构中的有效性，并突出了多分支特征学习对基于激光雷达点云的车辆重识别的益处。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于LiDAR点云的车辆重识别问题，即如何从三维点云数据中学习判别性特征来区分不同车辆。这个问题在智能交通系统中至关重要，因为它支持跨摄像头跟踪、交通分析和自动驾驶安全，能够解决传统运动跟踪在遮挡、轨迹碎片化等情况下的失败问题，同时点云数据提供准确的3D几何信息，相比摄像头数据对光照变化和视角变化具有更好的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基于摄像头的方法在复杂环境下的局限性，认识到点云数据的优势。他们借鉴了车辆重识别领域的多种方法，包括视角感知学习、多分支特征融合和注意力机制，同时借鉴了机器人学中利用特征值确定车辆方向的思想。基于这些现有工作，作者设计了CALM-Net，整合边缘卷积（处理局部几何）、点注意力（捕获全局上下文）和曲率嵌入（编码表面变化）三种机制，并通过混合采样策略（训练时随机采样，推理时最远点采样）进一步提升了性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CALM-Net的核心思想是通过多分支神经网络同时捕捉点云数据中的局部几何结构、全局上下文信息和表面曲率特征，学习对视角和环境变化鲁棒的车辆嵌入。整体流程包括：1)输入点云进行下采样；2)并行处理三个分支-边缘卷支提取局部几何、点注意力捕获全局依赖、曲率嵌入编码表面变化；3)将各分支特征融合并通过卷积和批归一化处理；4)通过ReLU激活得到最终嵌入；5)使用二元交叉熵损失进行训练，判断两个点云是否对应同一车辆。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在LiDAR点云重识别中引入曲率感知机制，通过可学习的曲率嵌入实现细粒度几何推理；2)设计多分支架构，同时处理局部、上下文和结构特征；3)提出混合点下采样策略，结合随机采样的数据增强和FPS的结构保持优势；4)精心设计特征融合方法。相比之前工作，本文专注于点云而非图像数据，引入曲率嵌入这一新特征，采用多分支而非单一架构，并使用混合采样策略，在nuScenes数据集上实现了比最强基线高1.97%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CALM-Net，一个结合边缘卷积、点注意力和曲率嵌入的多分支神经网络，通过从LiDAR点云中学习判别性和几何驱动的特征，显著提高了车辆重识别的准确率，特别是在复杂城市环境中的视角变化和光照变化情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents CALM-Net, a curvature-aware LiDAR point cloud-basedmulti-branch neural network for vehicle re-identification. The proposed modeladdresses the challenge of learning discriminative and complementary featuresfrom three-dimensional point clouds to distinguish between vehicles. CALM-Netemploys a multi-branch architecture that integrates edge convolution, pointattention, and a curvature embedding that characterizes local surface variationin point clouds. By combining these mechanisms, the model learns richergeometric and contextual features that are well suited for there-identification task. Experimental evaluation on the large-scale nuScenesdataset demonstrates that CALM-Net achieves a mean re-identification accuracyimprovement of approximately 1.97\% points compared with the strongest baselinein our study. The results confirms the effectiveness of incorporating curvatureinformation into deep learning architectures and highlight the benefit ofmulti-branch feature learning for LiDAR point cloud-based vehiclere-identification.</description>
      <author>example@mail.com (Dongwook Lee, Sol Han, Jinwhan Kim)</author>
      <guid isPermaLink="false">2510.14576v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Order Meshfree Surface Integration, Including Singular Integrands</title>
      <link>http://arxiv.org/abs/2510.14236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发并测试了针对表面点云的高阶积分方法，解决了在任意分段光滑表面上进行精确积分的问题。&lt;h4&gt;背景&lt;/h4&gt;表面积分在工程和科学领域的多种应用中至关重要，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得；而无网格方法通常需要在感兴趣域上精确积分一组函数，但这些积分在大多数表面上没有闭式形式。&lt;h4&gt;目的&lt;/h4&gt;开发能够在任意、分段光滑表面（有边界或无边界）上进行高精度积分的方法，且不需要特定的点排列或初始三角剖分。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种完全无网格的积分方法，适用于任意分段光滑表面。这些方法不需要特定的点排列或表面的初始三角剖分。此外，作者还展示了如何扩展这些方法以处理奇异积分。&lt;h4&gt;主要发现&lt;/h4&gt;1. 开发了两种在任意分段光滑表面上进行积分的方法；2. 这些方法完全无网格，不需要特定的点排列或初始三角剖分；3. 方法可以处理奇异积分，同时保持高精度；4. 无需在奇点附近改变点密度即可维持高精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在任意表面上进行高阶积分提供了有效解决方案，克服了传统网格方法和无网格方法的局限性，并能处理奇异积分情况。&lt;h4&gt;翻译&lt;/h4&gt;我们开发并测试了针对表面点云的高阶积分方法。在表面上积分函数的任务在工程和科学的一系列应用中出现，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得，而大多数无网格方法需要在感兴趣域上精确积分一组函数（如径向基函数）；这些积分在大多数表面上通常没有闭式形式。我们描述了两种在任意、分段光滑表面（有边界或无边界）上进行积分的方法。我们的方法不需要特定的点排列或表面的初始三角剖分，使它们完全无网格。我们还展示了如何扩展这些方法以处理奇异积分，同时保持高精度，而无需在奇点附近改变点密度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop and test high-order methods for integration on surface pointclouds. The task of integrating a function on a surface arises in a range ofapplications in engineering and the sciences, particularly those involvingvarious integral methods for partial differential equations. Mesh-based methodsrequire a curved mesh for high-order convergence, which can be difficult toreliably obtain on many surfaces, and most meshfree methods require the abilityto integrate a set of functions (such as radial basis functions) exactly on thedomain of interest; these integrals are generally not known in closed form onmost surfaces. We describe two methods for integrating on arbitrary,piecewise-smooth surfaces with or without boundary. Our approaches do notrequire a particular arrangement of points or an initial triangulation of thesurface, making them completely meshfree. We also show how the methods can beextended to handle singular integrals while maintaining high accuracy withoutchanging the point density near singularities.</description>
      <author>example@mail.com (Daniel R. Venn, Steven J. Ruuth)</author>
      <guid isPermaLink="false">2510.14236v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space</title>
      <link>http://arxiv.org/abs/2510.14234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的无模型方法，用于带有关键点约束的三维可变形物体形状控制。该方法通过深度学习从点云中提取关键点作为特征向量，保留了物体的空间信息同时降低了特征空间维度。将操控问题简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学，并通过结合障碍李雅普诺夫函数的预设性能控制方法提高控制精度。实验验证了该方法的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，主要因为可变形物体具有无限维状态空间和复杂的变形动力学特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的无模型方法，用于带有关键点约束的可变形物体形状控制，提高操控的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;不同于依赖特征降维的现有方法，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。通过提取关键点，将可变形物体操控简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学。同时，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束，提高控制精度。&lt;h4&gt;主要发现&lt;/h4&gt;通过提取关键点，成功降低了特征空间维度同时保留了物体空间信息；结合障碍李雅普诺夫函数的预设性能控制方法有效提高了控制精度；实验结果验证了所提出方法的有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的无模型方法通过深度学习提取关键点并结合预设性能控制，有效解决了三维可变形物体形状控制问题，具有较好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，因为它们具有无限维状态空间和复杂的变形动力学。本文提出了一种新型的带有关键点约束的无模型形状控制方法。与依赖特征降维的现有方法不同，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。这种方法不仅降低了特征空间的维度，还保留了物体的空间信息。通过提取关键点，可变形物体的操控被简化为一个视觉伺服问题，其中形状动力学使用变形雅可比矩阵描述。为了提高控制精度，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束。使用李雅普诺夫方法严格分析并验证了闭环系统的稳定性。实验结果进一步证明了所提出方法的有效性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作三维可变形物体的控制挑战。可变形物体（如海绵、布料等）由于其形状可以无限变化，状态空间维度极高，且具有复杂的变形动力学特性，使得传统的机器人控制方法难以有效处理。这个问题在现实中非常重要，因为可变形物体操作在医疗手术、工业焊接、自动折叠衣物等领域有广泛应用，提高机器人对这类物体的操作能力可以扩展机器人在这些领域的应用，提高自动化水平，减少人工干预，并提高任务执行的精度和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计方法时，首先分析了现有可变形物体操作方法的局限性，包括基于模型的方法依赖物理模型但参数估计困难，以及无模型方法面临高维状态空间的挑战。作者借鉴了现有工作中的深度学习方法提取关键点、基于Jacobian的视觉伺服控制以及规定性能控制（PPC）和障碍Lyapunov函数（BLF）等技术。作者的创新点在于将PPC方法从已知Jacobian矩阵的视觉伺服任务迁移到Jacobian矩阵完全未知的可变形物体操作任务中，并结合关键点提取方法，在保留空间信息的同时降低特征维度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从可变形物体的3D点云中提取关键点作为特征向量，这些关键点保留了物体的空间信息同时降低了维度；将可变形物体操作简化为视觉伺服问题，使用变形Jacobian矩阵描述形状动力学；设计规定性能控制器，通过障碍Lyapunov函数强制执行关键点的约束；使用神经网络近似未知的Jacobian矩阵。整体实现流程包括：使用Key-Grid神经网络从点云中提取关键点；计算关键点误差；使用规定性能函数定义误差边界；将误差转换为转换误差；设计基于Jacobian的控制器，使用神经网络近似Jacobian矩阵；应用自适应律更新神经网络权重；通过Lyapunov分析确保系统稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出一种新的无模型方法，使用关键点坐标作为特征向量，保留了空间信息同时降低了维度；将规定性能控制方法从视觉伺服任务迁移到可变形物体操作任务，其中Jacobian矩阵完全未知；设计障碍Lyapunov函数来确保关键点误差的边界约束；结合深度学习和自适应控制方法，提高了控制精度和鲁棒性。相比之前工作，该方法直接从3D点云提取关键点，而不是使用手动标记的关键点；使用改进的PPC框架，而不是基于图网络的MPC控制器；与传统降维方法相比，保留了物理和空间信息；与其他避免潜在抽象的方法相比，不局限于二维结构、刚性假设或强模型依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于空间潜在空间的关键点约束规定性能控制方法，有效解决了三维可变形物体操作中的高维状态空间和复杂变形动力学挑战，显著提高了控制精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulating three-dimensional (3D) deformable objects presents significantchallenges for robotic systems due to their infinite-dimensional state spaceand complex deformable dynamics. This paper proposes a novel model-freeapproach for shape control with constraints imposed on key points. Unlikeexisting methods that rely on feature dimensionality reduction, the proposedcontroller leverages the coordinates of key points as the feature vector, whichare extracted from the deformable object's point cloud using deep learningmethods. This approach not only reduces the dimensionality of the feature spacebut also retains the spatial information of the object. By extracting keypoints, the manipulation of deformable objects is simplified into a visualservoing problem, where the shape dynamics are described using a deformationJacobian matrix. To enhance control accuracy, a prescribed performance controlmethod is developed by integrating barrier Lyapunov functions (BLF) to enforceconstraints on the key points. The stability of the closed-loop system isrigorously analyzed and verified using the Lyapunov method. Experimentalresults further demonstrate the effectiveness and robustness of the proposedmethod.</description>
      <author>example@mail.com (Ning Han, Gu Gong, Bin Zhang, Yuexuan Xu, Bohan Yang, Yunhui Liu, David Navarro-Alarcon)</author>
      <guid isPermaLink="false">2510.14234v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction</title>
      <link>http://arxiv.org/abs/2510.14147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图，在各种数据集和度量标准下表现出卓越的性能和并行扩展性。&lt;h4&gt;背景&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。&lt;h4&gt;目的&lt;/h4&gt;解决现有并行近邻搜索工作在精确解和非欧几里得度量方面的局限性，提供一个可扩展的稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。&lt;h4&gt;方法&lt;/h4&gt;提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法。提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后介绍了两种分布式内存算法：简单的点分区策略和空间分区策略，它们利用每个节点上的覆盖树算法。&lt;h4&gt;主要发现&lt;/h4&gt;算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;h4&gt;结论&lt;/h4&gt;该算法能够有效处理大规模数据集的近邻图计算，在多种数据集和度量标准下表现出良好的并行扩展性。&lt;h4&gt;翻译&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。现有的并行近邻搜索工作在最近邻和近似最近邻搜索问题上取得了很大进展，特别关注欧几里得空间。然而，许多应用程序需要精确解和非欧几里得度量。本文提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。我们提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后，我们介绍了用于近邻图问题的两种分布式内存算法：一种简单的点分区策略和一种空间分区策略，它们利用每个节点上的覆盖树算法。我们的算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了比最先进方法高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing fixed-radius near-neighbor graphs is an important first step formany data analysis algorithms. Near-neighbor graphs connect points that areclose under some metric, endowing point clouds with a combinatorial structure.As computing power and data acquisition methods advance, diverse sources oflarge scientific datasets would greatly benefit from scalable solutions to thiscommon subroutine for downstream analysis. Prior work on parallel nearestneighbors has made great progress in problems like k-nearest and approximatenearest neighbor search problems, with particular attention on Euclideanspaces. Yet many applications need exact solutions and non-Euclidean metrics.This paper presents a scalable sparsity-aware distributed memory algorithmusing cover trees to compute near-neighbor graphs in general metric spaces. Weprovide a shared-memory algorithm for cover tree construction and demonstrateits competitiveness with state-of-the-art fixed-radius search data structures.We then introduce two distributed-memory algorithms for the near-neighbor graphproblem, a simple point-partitioning strategy and a spatial-partitioningstrategy, which leverage the cover tree algorithm on each node. Our algorithmsexhibit parallel scaling across a variety of real and synthetic datasets forboth traditional and non-traditional metrics. On real world high dimensionaldatasets with one million points, we achieve speedups up to 678.34x over thestate-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (onaverage), and up to 1590.99x using 4096 cores for graphs with 500 neighbors pervertex (on average).</description>
      <author>example@mail.com (Gabriel Raulet, Dmitriy Morozov, Aydin Buluc, Katherine Yelick)</author>
      <guid isPermaLink="false">2510.14147v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Geometric local parameterization for solving Hele-Shaw problems with surface tension</title>
      <link>http://arxiv.org/abs/2510.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种解决二维Hele-Shaw自由边界问题（带有表面张力）的新型计算框架。该方法使用点云表示移动边界，无需全局参数化，并通过广义移动最小二乘法构建局部几何图表，实现高阶几何量近似。研究提供了严格的收敛分析，并通过数值实验验证了方法的有效性，展示了复杂形状在表面张力作用下向圆形平衡状态的正确演化。&lt;h4&gt;背景&lt;/h4&gt;Hele-Shaw自由边界问题是流体力学中的重要问题，特别是在研究具有表面张力的界面动力学时。传统方法通常需要全局参数化来表示移动边界，这在处理复杂几何形状时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的计算框架，能够高效、准确地解决带有表面张力的二维Hele-Shaw自由边界问题，克服传统方法在处理复杂几何形状时的局限性，并实现高阶收敛精度。&lt;h4&gt;方法&lt;/h4&gt;1. 使用点云表示移动边界，消除全局参数化的需求；2. 应用广义移动最小二乘法构建局部几何图表；3. 直接从点云数据高阶近似几何量（如曲率）；4. 使用局部参数化离散化控制边界积分方程；5. 包含奇异积分的解析公式；6. 进行严格的收敛分析，建立一致性和稳定性条件。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所提出的方法实现了高阶空间收敛；2. 获得了预期的时域收敛率；3. 误差界限与均匀采样点云数据大小、边界光滑度和数值积分规则阶数相关；4. 复杂初始形状在表面张力作用下正确演变为圆形平衡状态。&lt;h4&gt;结论&lt;/h4&gt;该新型计算框架为解决二维Hele-Shaw自由边界问题提供了有效方法，点云表示和局部几何图表的构建使得方法能够处理复杂几何形状，同时保持高阶收敛精度。数值实验验证了理论分析的正确性和方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了一种解决带有表面张力的二维Hele-Shaw自由边界问题的新型计算框架。移动边界由点云表示，消除了对全局参数化的需求。我们的方法利用广义移动最小二乘法构建局部几何图表，能够直接从点云数据高阶近似几何量（如曲率）。这种局部参数化被系统地用于离散化控制边界积分方程，包括奇异积分的解析公式。我们为所提出的空间离散化提供了严格的收敛分析，在特定条件下建立了一致性和稳定性。导出的误差界限基于移动边界上均匀采样点云数据的大小、边界的光滑度和数值积分规则的阶数。数值实验验证了理论结果，展示了高阶空间收敛和预期的时域收敛率。通过复杂初始形状的模拟进一步说明了该方法的有效性，这些形状在表面张力的影响下正确地演变为圆形平衡状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce a novel computational framework for solving thetwo-dimensional Hele-Shaw free boundary problem with surface tension. Themoving boundary is represented by point clouds, eliminating the need for aglobal parameterization. Our approach leverages Generalized Moving LeastSquares (GMLS) to construct local geometric charts, enabling high-orderapproximations of geometric quantities such as curvature directly from thepoint cloud data. This local parameterization is systematically employed todiscretize the governing boundary integral equation, including an analyticalformula of the singular integrals. We provide a rigorous convergence analysisfor the proposed spatial discretization, establishing consistency and stabilityunder certain conditions. The resulting error bound is derived in terms of thesize of the uniformly sampled point cloud data on the moving boundary, thesmoothness of the boundary, and the order of the numerical quadrature rule.Numerical experiments confirm the theoretical findings, demonstratinghigh-order spatial convergence and the expected temporal convergence rates. Themethod's effectiveness is further illustrated through simulations of complexinitial shapes, which correctly evolve towards circular equilibrium statesunder the influence of surface tension.</description>
      <author>example@mail.com (Zengyan Zhang, Wenrui Hao, John Harlim)</author>
      <guid isPermaLink="false">2510.14088v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</title>
      <link>http://arxiv.org/abs/2510.14190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ConDA(对比扩散对齐)框架，通过对比学习在扩散嵌入中组织潜在空间，使其与系统动力学对齐，从而实现更可控和可解释的生成操作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在生成任务上表现出色，但其潜在空间没有被明确组织用于可解释的控制，限制了其在需要精确控制的应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来组织扩散模型的潜在空间，使其能够支持忠实插值、外推和可控生成，同时保持生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出ConDA框架，应用对比学习于扩散嵌入中，将潜在几何结构与系统动力学对齐，使遍历方向反映潜在的动力学因素，并支持非线性轨迹遍历。&lt;h4&gt;主要发现&lt;/h4&gt;在流体动力学、神经钙成像、治疗性神经刺激和面部表情等多个基准测试中，ConDA产生了比线性遍历和基于条件的基线更具可解释性的潜在表示，同时提高了可控性。&lt;h4&gt;结论&lt;/h4&gt;扩散潜变量编码了与动力学相关的结构，但要有效利用这种结构，需要沿着潜在流形进行潜在组织和遍历。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型在生成方面表现出色，但它们的潜在空间没有被明确组织用于可解释的控制。我们引入了ConDA(对比扩散对齐)，这是一个在扩散嵌入中应用对比学习的框架，将潜在几何结构与系统动力学对齐。受最近进展的启发，这些进展表明对比目标可以恢复更多解缠和结构化的表示，ConDA组织扩散潜变量，使得遍历方向反映潜在的动力学因素。在这个对比结构化的空间中，ConDA支持非线性轨迹遍历，实现忠实插值、外推和可控生成。在流体动力学、神经钙成像、治疗性神经刺激和面部表情的基准测试中，ConDA与线性遍历和基于条件的基线相比，产生了具有改进可解释性的潜在表示。这些结果表明扩散潜变量编码了动力学相关的结构，但利用这种结构需要沿着潜在流形进行潜在组织和遍历。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at generation, but their latent spaces are notexplicitly organized for interpretable control. We introduce ConDA (ContrastiveDiffusion Alignment), a framework that applies contrastive learning withindiffusion embeddings to align latent geometry with system dynamics. Motivatedby recent advances showing that contrastive objectives can recover moredisentangled and structured representations, ConDA organizes diffusion latentssuch that traversal directions reflect underlying dynamical factors. Withinthis contrastively structured space, ConDA enables nonlinear trajectorytraversal that supports faithful interpolation, extrapolation, and controllablegeneration. Across benchmarks in fluid dynamics, neural calcium imaging,therapeutic neurostimulation, and facial expression, ConDA producesinterpretable latent representations with improved controllability compared tolinear traversals and conditioning-based baselines. These results suggest thatdiffusion latents encode dynamics-relevant structure, but exploiting thisstructure requires latent organization and traversal along the latent manifold.</description>
      <author>example@mail.com (Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick)</author>
      <guid isPermaLink="false">2510.14190v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一个创新的机器人操作框架，通过视觉到触觉的生成解决了触觉传感器限制问题，结合视觉和生成触觉数据通过强化学习实现高性能的机器人推操作，在模拟和真实实验中表现出色，成功率高达86%。&lt;h4&gt;背景&lt;/h4&gt;机器人推操作需要触觉反馈来捕捉末端执行器和物体之间的接触力和动力学，但真实触觉传感器面临高成本、脆弱性、校准困难和传感器差异等挑战，而仅基于视觉的策略难以获得满意性能。&lt;h4&gt;目的&lt;/h4&gt;提出ViTacGen框架，用于视觉机器人推操作，在强化学习中实现视觉到触觉的生成，消除对高分辨率真实触觉传感器的依赖，实现视觉系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen框架能够在不依赖高分辨率触觉传感器的情况下实现有效的机器人推操作，通过视觉到触觉的生成实现了在视觉系统上的零样本部署。&lt;h4&gt;翻译&lt;/h4&gt;机器人推操作是一种基础的操作任务，需要触觉反馈来捕捉末端执行器和物体之间的微妙接触力和动力学。然而，真实的触觉传感器通常面临硬件限制，如高成本和脆弱性，以及部署挑战，包括校准和不同传感器之间的差异，而仅基于视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一个新颖的机器人操作框架，专为视觉机器人推操作设计，在强化学习中实现视觉到触觉的生成，以消除对高分辨率真实触觉传感器的依赖，实现仅在视觉系统上的有效零样本部署。具体而言，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是机器人推动任务中依赖昂贵且脆弱的触觉传感器的问题。这个问题很重要，因为触觉反馈对捕捉物体间细微接触力和动态至关重要，但真实触觉传感器成本高、易损坏、需要精确校准，且不同传感器间存在差异，限制了高性能机器人操作系统的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类能从视觉推断触觉状态的启发，观察到现有方法要么依赖昂贵触觉传感器，要么仅使用视觉但性能不足。他们设计了编码器-解码器的视觉到触觉生成网络(VT-Gen)和强化学习策略网络(VT-Con)。借鉴了人类视觉-触觉交互能力、Soft Actor-Critic强化学习算法、MoCo对比学习框架、Tactile Gym模拟平台、注意力机制和VGG损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模拟人类从视觉推断触觉的能力，让机器人仅通过视觉'感知'触觉，使用生成的触觉接触深度图像作为标准化表示，并通过对比学习对齐视觉和触觉特征。整体流程：1)在模拟环境中收集配对的视觉和触觉数据；2)训练VT-Gen网络从视觉生成触觉深度图像；3)冻结VT-Gen，训练VT-Con强化学习策略；4)零样本部署到真实视觉-only机器人系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出ViTacGen框架消除对触觉传感器的依赖；2)设计VT-Gen生成标准化触觉表示；3)提出VT-Con通过对比学习融合视觉-触觉特征；4)实现零样本部署；5)使用接触深度图解决传感器差异问题。不同之处：相比仅视觉方法提供更丰富感知；相比触觉传感器方法降低成本复杂度；相比简单特征拼接实现更有效跨模态对齐；相比校准方法具有更好通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ViTacGen通过模拟人类从视觉推断触觉的能力，实现了仅使用视觉信息的机器人精确推动，消除了对昂贵触觉传感器的依赖，同时保持了高性能操作能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D数据集和Cylinder Mamba Diffusion (CymbaDiff)方法，用于从手绘草图生成高质量的3D室外语义场景。&lt;h4&gt;背景&lt;/h4&gt;室外3D语义场景生成在都市仿真和自动驾驶等领域有重要应用，但该领域的发展受到缺乏公开可用、良好注释的数据集的制约。&lt;h4&gt;目的&lt;/h4&gt;引入首个大规模基准数据集SketchSem3D，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景，并提出一种增强空间一致性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出了Cylinder Mamba Diffusion (CymbaDiff)模型，该方法施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SketchSem3数据集和CymbaDiff方法为室外3D语义场景生成提供了新的基准和解决方案，有助于推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;室外3D语义场景生成为都市仿真和自动驾驶等应用生成真实且语义丰富的环境。然而，这一方向的发展受到缺乏公开可用、良好注释的数据集的限制。我们引入了SketchSem3D，这是第一个大规模基准，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景。SketchSem3D包含两个子集：基于语义的KITTI草图和基于KITTI-360的草图（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了圆柱形Mamba扩散模型（CymbaDiff），显著增强了室外3D场景生成的空间一致性。CymbaDiff施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于草图的3D户外语义场景生成问题，特别是缺乏公开的大规模标注数据集和现有方法在户外场景中的局限性。这个问题在现实中非常重要，因为高质量的城市场景生成对自动驾驶模拟、城市规划等应用至关重要，而传统方法要么依赖昂贵的传感器数据，要么无法生成复杂且语义丰富的户外环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D户外场景生成方法的局限性，如鸟瞰图(BEV)方法缺乏3D结构信息，多尺度方法计算复杂。他们借鉴了状态空间模型(SSMs)在图像处理和点云分析中的成功应用，结合扩散模型在生成任务中的优势。作者还利用了CLIP和SAM等现有模型进行数据集构建，但创新性地将这些技术整合到一个专门针对户外场景的框架中，通过圆柱坐标系统改进了空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散模型，结合笛卡尔和圆柱坐标系统的优势，增强3D场景生成的空间连贯性。整体流程包括：1)构建SketchSem3D数据集，包含草图、卫星图像和3D体素；2)使用场景结构估计网络(SSEN)提取结构信息；3)通过潜在映射网络(LMN)压缩输入条件；4)利用CymbaDiff去噪网络，结合三重Mamba模块和圆柱Mamba层进行生成；5)从噪声逐步去噪，最终生成高质量的3D语义场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建首个专门的大规模基准数据集SketchSem3D；3)提出CymbaDiff模型，结合圆柱Mamba块增强空间连贯性；4)引入圆柱坐标系统来更好地表示户外场景的空间关系。相比之前工作，CymbaDiff避免了多尺度方法的计算复杂性，解决了BEV方法缺乏3D结构信息的问题，并首次将基于草本的3D生成扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过CymbaDiff方法和SketchSem3D数据集，首次实现了从简单草图和卫星图像生成高质量、语义连贯的大规模3D城市场景，为自动驾驶和城市规划等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration</title>
      <link>http://arxiv.org/abs/2510.13729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Symposium on Visual Computing (ISVC)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新数据集，提供同步图像序列和高精度姿态数据，用于严格评估多相机光场配准方法。&lt;h4&gt;背景&lt;/h4&gt;现有光场数据集仅限于单相机设置，通常缺乏外部真实值，限制了多相机光场配准方法的评估。&lt;h4&gt;目的&lt;/h4&gt;创建一个独特的多相机光场数据集，结合高分辨率光场相机图像和精确的6自由度姿态数据，以实现多相机光场配准方法的严格评估。&lt;h4&gt;方法&lt;/h4&gt;提供两种互补的配准方法：1)基于RANSAC的鲁棒3D变换估计，使用跨视点点云；2)从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法与真实值显示出良好的对齐，支持可靠的多视点光场处理。&lt;h4&gt;结论&lt;/h4&gt;LiFMCR数据集及其配套方法为多相机光场配准提供了基准，能够准确且可扩展地进行多相机配准。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新颖数据集。虽然现有的光场数据集仅限于单相机设置且通常缺乏外部真实值，但LiFMCR提供了来自两个高分辨率Raytrix R32光场相机的同步图像序列，以及由Vicon动作捕捉系统记录的高精度6自由度姿态。这种独特组合能够严格评估多相机光场配准方法。作为基准，我们提供了两种互补的配准方法：一种基于RANSAC的鲁棒3D变换估计，使用跨视点点云；以及一种从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型，实现准确且可扩展的多相机配准。实验显示与真实值有良好的对齐，支持可靠的多视点光场处理。项目页面：https://lifmcr.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多相机光场相机（plenoptic cameras）注册缺乏标准数据集和基准测试的问题。这个问题很重要，因为准确的3D重建对自主系统和机器人应用至关重要，而结合多个光场相机可以通过立体视觉优势扩展深度范围和精度，提高深度感知能力和场景理解能力。现有的光场数据集通常局限于单相机设置且缺乏外部真实值，限制了多相机注册方法的评估和改进。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有光场数据集的局限性，特别是缺乏多相机设置和精确真实值的问题。他们借鉴了现有工作：使用LiFCal进行内参校准，采用SIFT特征提取和匹配，以及基于RANSAC的3D变换估计方法。作者设计了两种互补的注册方法：一种基于3D点云对齐的RANSAC方法，另一种是首次应用于光场数据的PnP算法。两种方法都明确集成了光场相机模型，以准确处理光场相机的特殊光学和几何特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供一个包含同步多视角光场数据和精确6-DoF姿态真实值的数据集，并设计两种互补的多相机注册方法，一种基于3D点云对齐，另一种基于光场PnP算法，都考虑光场相机的特殊光学特性。3D RANSAC方法流程：内参校准→点云生成→SIFT特征提取与匹配→3D RANSAC对齐→计算相机间相对变换。光场PnP方法流程：仅参考相机点云→镜头畸变校正→光场模型透视投影→特征匹配→鲁棒基础矩阵估计→RANSAC PnP→Levenberg-Marquardt优化细化姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出LiFMCR数据集，首次提供同步多视角光场序列和精确6-DoF真实值；2)提出两种互补注册方法，包括基于RANSAC的3D变换估计和首个光场PnP算法；3)两种方法都明确集成光场相机模型；4)提供完整内参和外参校准流程；5)使用Vicon系统提供亚毫米级精度真实值。相比之前工作：现有数据集多为单相机且缺乏真实值，而LiFMCR提供多相机同步数据和精确姿态；现有方法不专门针对光场多相机注册，而本文方法考虑了光场相机特性；首次将PnP算法应用于光场数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了LiFMCR数据集和两种互补的光场多相机注册方法，填补了多视角光场数据与精确姿态真实值结合的空白，为光场相机在3D重建、SLAM等应用中的可靠使用提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiFMCR, a novel dataset for the registration of multiple microlens array (MLA)-based light field cameras. While existing light field datasetsare limited to single-camera setups and typically lack external ground truth,LiFMCR provides synchronized image sequences from two high-resolution RaytrixR32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)poses recorded by a Vicon motion capture system. This unique combinationenables rigorous evaluation of multi-camera light field registration methods.  As a baseline, we provide two complementary registration approaches: a robust3D transformation estimation via a RANSAC-based method using cross-view pointclouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses fromsingle light field images. Both explicitly integrate the plenoptic cameramodel, enabling accurate and scalable multi-camera registration. Experimentsshow strong alignment with the ground truth, supporting reliable multi-viewlight field processing.  Project page: https://lifmcr.github.io/</description>
      <author>example@mail.com (Aymeric Fleith, Julian Zirbel, Daniel Cremers, Niclas Zeller)</author>
      <guid isPermaLink="false">2510.13729v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
  <item>
      <title>Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization</title>
      <link>http://arxiv.org/abs/2510.13619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the paper published in: Proceedings  of the 37th International Technical Meeting of the Satellite Division of The  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is  available at https://doi.org/10.33012/2024.19864&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可视化方法，用于辅助分析师分类影响激光雷达扫描匹配的逆境模式，通过生成矢量场图揭示点云数据中的差异模式。&lt;h4&gt;背景&lt;/h4&gt;激光雷达扫描匹配过程中存在多种逆境模式影响数据质量，分析师需要有效方法来识别和分类这些模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种离线可视化分析方法，帮助分析师识别和理解影响激光雷达扫描匹配的逆境机制。&lt;h4&gt;方法&lt;/h4&gt;提出一种生成矢量场图的可视化方法，该图能描述一对已配准点云之间的局部差异，揭示难以从原始数据中提取的模式。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟研究和现场实验验证，该方法能够帮助分析师识别和迭代移除逆境机制，逐步聚焦于更细微的数据差异。&lt;h4&gt;结论&lt;/h4&gt;所提出的可视化方法有效辅助了分析师对激光雷达扫描匹配中逆境模式的分类和分析，提高了数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种可视化方法，用于帮助人类分析师分类影响激光雷达扫描匹配的逆境模式。我们的方法适用于离线分析而非实时分析。该方法生成一个矢量场图，用于描述一对已配准点云之间的局部差异。矢量场图能够揭示分析师难以从原始点云数据中提取的模式。在介绍我们的方法后，我们将该过程应用于两个概念验证示例：一个是模拟研究，另一个是现场实验。对于这两个数据集，人类分析师能够推理一系列逆境机制，并从原始数据中迭代地移除这些机制，以帮助将注意力集中在逐渐变小的差异上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.33012/2024.19864&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we introduce a visualization methodology to aid a human analystin classifying adversity modes that impact lidar scan matching. Our methodologyis intended for offline rather than real-time analysis. The method generates avector-field plot that characterizes local discrepancies between a pair ofregistered point clouds. The vector field plot reveals patterns that would bedifficult for the analyst to extract from raw point-cloud data. Afterintroducing our methodology, we apply the process to two proof-of-conceptexamples: one a simulation study and the other a field experiment. For bothdata sets, a human analyst was able to reason about a series of adversitymechanisms and iteratively remove those mechanisms from the raw data, to helpfocus attention on progressively smaller discrepancies.</description>
      <author>example@mail.com (Daniel Choate, Jason Rife)</author>
      <guid isPermaLink="false">2510.13619v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构因果模型（SCM）的点云分割新类别发现方法，通过因果表示与推理的联合学习，解决仅使用已标记类别监督信息对新类别进行分割的问题。&lt;h4&gt;背景&lt;/h4&gt;点云分割中的新类别发现（3D-NCD）是一个挑战性问题，需要仅使用已标记（基础）3D类别的监督信息来学习能够分割未标记（新）3D类别的模型。&lt;h4&gt;目的&lt;/h4&gt;学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型（SCM）重新形式化3D-NCD问题，提出因果表示与推理的联合学习方法。通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系；设计消除混杂因素的因果表示原型；使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆；通过引入因果约束可以准确发现与类别对应的点云表示；所提出的方法在3D和2D NCD语义分割任务上表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;基于结构因果模型的方法能够有效解决点云分割中的新类别发现问题，通过因果表示与推理的联合学习，实现仅使用已标记类别监督信息对新类别的准确分割。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现（3D-NCD），旨在学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。这项任务的关键在于建立点表示与基础类别标签之间的准确相关性，以及基础类别和新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果在学习过程中施加因果关系作为强相关约束，应该能够准确发现与类别对应的本质点云表示。为此，我们引入结构因果模型（SCM）重新形式化3D-NCD问题，并提出一种新方法，即因果表示与推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了我们方法的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是点云分割中的新类别发现问题，即如何仅使用已知类别的监督信息来训练模型，使其能够分割场景中未标记的新类别物体。这个问题在自动驾驶、机器人感知等真实场景中非常重要，因为这些环境中可能出现各种未预先定义的物体类别，传统'封闭世界'假设的方法无法应对这种开放世界环境，而新类别发现能够减少人工标注负担，使模型能够适应动态变化的环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性，指出它们倾向于学习捷径特征而非本质特征，且忽视了基类与新类别之间的因果关系。基于此，作者引入结构因果模型重新形式化问题，并借鉴了因果表示学习、结构因果模型、图卷积网络和生成对抗网络等现有工作。通过这些借鉴，作者设计了因果表示原型学习来消除混杂因素，并使用图结构建模基类与新类别间的因果关系，实现了从未知类别中学习更准确的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习消除点云数据中的非因果特征，捕获基类的本质表示，并建立基类和新类别之间的因果关系模型，实现从已知到未知的知识迁移。整体流程分为三部分：1)因果表示原型学习，使用对抗训练消除混杂因素，生成基类的因果表示原型；2)因果推理图构建，创建包含基类和新类原型的图结构，设计因果自适应邻接矩阵和约束优化图结构；3)基于GCN的伪标签生成，利用图卷积网络处理优化后的图，通过多层传播和邻居聚合为新类别生成高质量伪标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将因果学习引入3D NCD领域，专注于学习因果关系而非统计相关性；2)提出因果表示原型学习，通过对抗机制消除混杂因素；3)提出基于图的因果推理方法，显式建模基类到新类别的因果路径。相比之前的工作，本文方法能够处理点云数据中的复杂因果关系，而非仅依赖表面特征相似性，通过因果推理更好地处理新类别的语义关系，减少错误分类，提高了在开放世界环境中的适应性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入因果表示学习和因果推理，首次解决了点云分割中新类别发现问题中的因果机制建模，实现了从未知类别中学习更准确、更鲁棒的语义分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</title>
      <link>http://arxiv.org/abs/2510.13287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IROS Active Perception Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAMM-LOAM的新型LiDAR SLAM系统，通过点云分类和退化感知算法解决了特征稀疏环境下的定位建图问题，显著提高了室内环境中的导航精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR SLAM系统对精确导航和环境重建至关重要。当前点对平面ICP算法在结构化、特征丰富的环境中表现良好，但在特征稀疏、重复几何结构和高频运动场景下表现不佳，导致6自由度姿态估计退化。虽然最先进算法通过添加额外传感模态应对挑战，但纯LiDAR解决方案在这种条件下仍有限制。&lt;h4&gt;目的&lt;/h4&gt;解决特征稀疏、重复几何结构和高频运动场景下的SLAM退化问题，提出一种新颖的退化感知多度量LiDAR里程计与建图(DAMM-LOAM)模块。&lt;h4&gt;方法&lt;/h4&gt;通过基于表面法线和邻域分析的点云分类提高建图精度，将点分类为地面、墙壁、屋顶、边缘和非平面点以实现准确对应；应用基于退化的加权最小二乘ICP算法进行精确里程计估计；实现基于ScanContext的后端以支持稳健的回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;DAMM-LOAM在里程计准确性方面有显著改进，特别是在长走廊等室内环境中表现突出。&lt;h4&gt;结论&lt;/h4&gt;DAMM-LOAM系统有效解决了传统LiDAR SLAM在特定场景下的退化问题，通过创新的点云分类和退化感知算法，提高了室内环境中的导航精度。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达同步定位与建图系统对于在各种应用中实现精确导航和环境重建至关重要。尽管当前点对平面ICP算法在结构化、特征丰富的环境中能有效工作，但在特征稀疏、重复几何结构和高频运动场景下表现不佳。这会导致6自由度姿态估计的退化。大多数最先进算法通过结合额外的传感模态来解决这些挑战，但纯激光雷达解决方案在这种条件下仍然面临限制。为解决这些问题，我们提出了一种新颖的退化感知多度量激光雷达里程计与建图模块。我们的系统通过基于表面法线和邻域分析的点云分类提高了建图精度。点被分类为地面、墙壁、屋顶、边缘和非平面点，从而实现准确的对应关系。然后应用基于退化的加权最小二乘ICP算法进行精确的里程计估计。此外，实现了基于扫描上下文的后端以支持稳健的回环闭合。DAMM-LOAM在里程计准确性方面表现出显著改进，特别是在长走廊等室内环境中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR SLAM系统在特征稀疏、重复几何结构和高频运动等场景下的退化问题，导致6自由度位姿估计不准确。这个问题很重要，因为许多实际应用场景（如走廊、隧道）都存在特征稀疏问题，而机器人导航、自动驾驶等领域需要在复杂环境中进行精确定位和地图构建，当前系统在这些挑战性场景中表现不佳，限制了技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统点对点或点对平面ICP算法的局限性，认识到在变化几何结构和特征稀疏环境下表现不佳的问题。设计方法时借鉴了现有工作：利用NV-LIOM的球形投影法线提取方法，但进一步进行几何分类；借鉴了条件数和特征值分析来检测退化，但设计了新的点级加权方案；并整合了现有的Scan Context算法作为后端。作者不是完全重新发明方法，而是在现有基础上进行了改进和整合创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合几何特征分类和退化感知的自适应加权来提高LiDAR SLAM系统在特征稀疏环境中的鲁棒性和准确性。整体流程包括：1)几何特征提取：将点云投影到球形范围图像，估计表面法线，并进行五类几何分类（地面、墙壁、屋顶、边缘、非平面点）；2)点云处理：自适应下采样并建立类别对应的点对；3)退化感知的位姿估计：分析Hessian矩阵特征值，为点分配权重，结合点对点和点对平面残差进行优化；4)后端处理：使用Scan Context进行回环检测和全局优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于法线图的语义特征提取：将点云分为五类而非传统的平面/非平面二分类；2)退化感知的逐点自适应加权：基于Hessian特征值分析为每个点分配权重，而非仅基于点类型数量；3)多度量残差整合：结合点对点和点对平面残差并动态调整权重；4)完整的端到端框架：整合几何特征提取、自适应加权优化和回环检测。相比之前工作，该方法提供了更细致的语义信息、更精确的退化处理和更全面的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAMM-LOAM通过基于法线图的语义特征分类和退化感知的自适应加权，显著提高了LiDAR SLAM系统在特征稀疏环境（如长走廊）中的定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential forenabling precise navigation and environmental reconstruction across variousapplications. Although current point-to-plane ICP algorithms perform effec-tively in structured, feature-rich environments, they struggle in scenarioswith sparse features, repetitive geometric structures, and high-frequencymotion. This leads to degeneracy in 6- DOF pose estimation. Moststate-of-the-art algorithms address these challenges by incorporatingadditional sensing modalities, but LiDAR-only solutions continue to facelimitations under such conditions. To address these issues, we propose a novelDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.Our system improves mapping accuracy through point cloud classification basedon surface normals and neighborhood analysis. Points are classified intoground, walls, roof, edges, and non-planar points, enabling accuratecorrespondences. A Degeneracy-based weighted least squares-based ICP algorithmis then applied for accurate odom- etry estimation. Additionally, a ScanContext based back-end is implemented to support robust loop closures.DAMM-LOAM demonstrates significant improvements in odometry accuracy,especially in indoor environments such as long corridors</description>
      <author>example@mail.com (Nishant Chandna, Akshat Kaushal)</author>
      <guid isPermaLink="false">2510.13287v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-based Adaptation in Large-scale Vision Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是一篇关于视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)的综合调查，提出了一种称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架，对现有方法进行了分类，并探讨了PA在不同领域的应用、挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉领域，VP和VPT作为大规模视觉模型适应的轻量级有效替代方法，在'预训练后微调'范式中迅速发展。然而，当前研究中VP和VPT经常被互换使用，缺乏系统区分这些技术及其各自应用的明确界限。&lt;h4&gt;目的&lt;/h4&gt;重新审视VP和VPT的设计，将它们概念化为一个统一的PA框架，提供清晰的方法分类，并探索PA在不同领域的应用、挑战和未来方向，为研究人员和实践者提供明确的路线图。&lt;h4&gt;方法&lt;/h4&gt;提供了一种分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。同时检查了PA在医学成像、3D点云和视觉语言任务等领域的整合，以及其在测试时适应和可信AI中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;PA在医学成像、3D点云和视觉语言任务等不同领域有广泛应用，并且在测试时适应和可信AI中发挥重要作用。作者总结了当前基准，并确定了关键挑战和未来方向。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是第一篇专门针对PA的方法和应用的综合调查，旨在为研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;h4&gt;翻译&lt;/h4&gt;在计算机视觉中，视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)最近已经出现作为轻量级且有效的替代方法，用于在'预训练后微调'范式中适应大规模视觉模型。然而，尽管进展迅速，它们的概念边界仍然模糊，因为VP和VPT在当前研究中经常被互换使用，反映了这些技术及其各自应用之间缺乏系统区分。在本调查中，我们从基本原理重新审视VP和VPT的设计，并将它们概念化为一个称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架。我们提供了一个分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。除了核心方法外，我们检查了PA在医学成像、3D点云和视觉语言任务等不同领域的整合，以及其在测试时适应和可信AI中的作用。我们还总结了当前基准，并确定了关键挑战和未来方向。据我们所知，我们是第一个专门针对PA的方法和应用的全面调查，考虑其独特特征。我们的调查旨在为所有领域的研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) haverecently emerged as lightweight and effective alternatives to full fine-tuningfor adapting large-scale vision models within the ``pretrain-then-finetune''paradigm. However, despite rapid progress, their conceptual boundaries remainblurred, as VP and VPT are frequently used interchangeably in current research,reflecting a lack of systematic distinction between these techniques and theirrespective applications. In this survey, we revisit the designs of VP and VPTfrom first principles, and conceptualize them within a unified framework termedPrompt-based Adaptation (PA). We provide a taxonomy that categorizes existingmethods into learnable, generative, and non-learnable prompts, and furtherorganizes them by injection granularity -- pixel-level and token-level. Beyondthe core methodologies, we examine PA's integrations across diverse domains,including medical imaging, 3D point clouds, and vision-language tasks, as wellas its role in test-time adaptation and trustworthy AI. We also summarizecurrent benchmarks and identify key challenges and future directions. To thebest of our knowledge, we are the first comprehensive survey dedicated to PA'smethodologies and applications in light of their distinct characteristics. Oursurvey aims to provide a clear roadmap for researchers and practitioners in allarea to understand and explore the evolving landscape of PA-related research.</description>
      <author>example@mail.com (Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han)</author>
      <guid isPermaLink="false">2510.13219v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>ADPerf: Investigating and Testing Performance in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2510.13078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, accepted by ASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了自动驾驶系统中障碍物检测模块的性能和延迟问题，开发了一个名为ADPerf的工具用于测试和暴露检测延迟，并评估了其对系统整体可靠性的影响。&lt;h4&gt;背景&lt;/h4&gt;障碍物检测对自动驾驶系统运行至关重要，系统依赖多种传感器结合深度学习模型进行时间敏感决策。然而，障碍物检测模块的延迟及其对LiDAR点云数据变化的适应性尚未被充分了解。&lt;h4&gt;目的&lt;/h4&gt;首次全面测量和建模Apollo和Autoware两个行业级自动驾驶系统中障碍物检测模块的性能，开发ADPerf工具生成测试用例以暴露检测延迟增加，并评估其对后续模块的影响。&lt;h4&gt;方法&lt;/h4&gt;对Apollo和Autoware系统中的障碍物检测模块进行性能测量和建模，开发ADPerf工具生成真实点云数据测试用例，对3D障碍物检测模块进行压力测试，并评估这些测试对轨迹预测模块的传播影响。&lt;h4&gt;主要发现&lt;/h4&gt;障碍物检测组件（特别是3D障碍物检测）的性能测试非常必要，障碍物检测可能成为自动驾驶系统延迟增加的主要瓶颈，延迟增加的不利影响会传播到其他模块，降低系统整体可靠性。&lt;h4&gt;结论&lt;/h4&gt;需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们是自动驾驶系统延迟增加的主要瓶颈，会进一步影响其他模块，降低整体系统可靠性。&lt;h4&gt;翻译&lt;/h4&gt;障碍物检测对自动驾驶系统的运行至关重要，这些系统依赖多种传感器（如摄像头和LiDAR）结合代码逻辑和深度学习模型来检测障碍物，以便进行时间敏感的决策。因此，障碍物检测延迟对自动驾驶系统的安全性和有效性至关重要。然而，障碍物检测模块的延迟及其对LiDAR点云数据各种变化的适应性尚未被充分了解。在这项工作中，我们首次对两个行业级自动驾驶系统（即Apollo和Autoware）中的障碍物检测模块性能进行了全面的测量和建模研究。从这项研究中，我们引入了ADPerf，这是一个旨在生成真实点云数据测试用例的工具，这些测试用例可以暴露检测延迟的增加。延迟降低会减少检测到障碍物的可用性，并对自动驾驶系统中后续模块的能力造成压力，即这些模块可能受到障碍物检测延迟增加的负面影响。我们将ADPerf应用于压力测试自动驾驶系统中广泛使用的3D障碍物检测模块的性能，以及此类测试对轨迹预测模块的传播影响。我们的评估强调了需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们可能成为自动驾驶系统延迟增加的主要瓶颈。这种不利结果还会进一步传播到其他模块，降低自动驾驶系统的整体可靠性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中障碍物检测模块的性能问题，特别是延迟(latency)问题。这个问题在现实中非常重要，因为障碍物检测延迟直接影响自动驾驶系统的安全性和有效性；延迟过大会导致系统无法及时做出决策，就像未检测到障碍物一样危险。同时，现有研究大多关注检测器的准确性和鲁棒性，而对其性能的研究相对不足，这导致自动驾驶系统在实际部署中可能存在未被发现性能瓶颈的风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶系统架构，识别出感知模块(特别是3D障碍物检测)是性能瓶颈。他们通过排队网络和排队Petri网对Apollo和Autoware系统进行性能建模，确认了3D障碍物检测的延迟问题。基于这些发现，他们设计了ADPerf工具，通过三种简单方法修改点云数据来增加检测延迟：添加障碍物边界外的噪声、添加新障碍物、移动现有障碍物。作者借鉴了现有的性能测试技术和障碍物检测鲁棒性测试方法，但专注于性能而非准确性，与现有的对抗攻击方法(如SlowLidar)相比，ADPerf采用更简单、更现实的修改方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过生成能增加3D障碍物检测延迟的测试场景，评估自动驾驶系统在性能压力下的行为及其对后续模块的影响。整体实现流程包括：1)数据准备，从真实世界驾驶场景数据集中提取点云表示和障碍物历史数据；2)测试场景生成，通过添加噪声、添加障碍物或移动障碍物来修改点云；3)模型执行与延迟测量，在修改和未修改的点云上运行检测模型并测量延迟；4)帧可用性估计，基于检测延迟估计哪些帧会被丢弃；5)轨迹预测评估，分析检测延迟对轨迹预测模块的级联影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次对自动驾驶系统障碍物检测模块性能进行综合研究；2)提出ADPerf工具，专门用于生成性能测试用例；3)研究性能问题的级联影响，关注检测延迟对整个系统的影响；4)采用更现实的测试方法。相比之前的工作，本文的关注点从准确性转向性能，方法上从复杂的对抗攻击转向简单的点云修改，评估范围从单个模块扩展到整个系统，且生成的测试场景更接近真实世界，具有更高的实用价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ADPerf工具，通过生成增加障碍物检测延迟的测试场景，首次系统性地研究了自动驾驶系统中障碍物检测模块的性能瓶颈及其对整体系统可靠性的影响。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Obstacle detection is crucial to the operation of autonomous driving systems,which rely on multiple sensors, such as cameras and LiDARs, combined with codelogic and deep learning models to detect obstacles for time-sensitivedecisions. Consequently, obstacle detection latency is critical to the safetyand effectiveness of autonomous driving systems. However, the latency of theobstacle detection module and its resilience to various changes in the LiDARpoint cloud data are not yet fully understood. In this work, we present thefirst comprehensive investigation on measuring and modeling the performance ofthe obstacle detection modules in two industry-grade autonomous drivingsystems, i.e., Apollo and Autoware. Learning from this investigation, weintroduce ADPerf, a tool that aims to generate realistic point cloud data testcases that can expose increased detection latency. Increasing latency decreasesthe availability of the detected obstacles and stresses the capabilities ofsubsequent modules in autonomous driving systems, i.e., the modules may benegatively impacted by the increased latency in obstacle detection.  We applied ADPerf to stress-test the performance of widely used 3D obstacledetection modules in autonomous driving systems, as well as the propagation ofsuch tests on trajectory prediction modules. Our evaluation highlights the needto conduct performance testing of obstacle detection components, especially 3Dobstacle detection, as they can be a major bottleneck to increased latency ofthe autonomous driving system. Such an adverse outcome will also furtherpropagate to other modules, reducing the overall reliability of autonomousdriving systems.</description>
      <author>example@mail.com (Tri Minh-Triet Pham, Diego Elias Costa, Weiyi Shang, Jinqiu Yang)</author>
      <guid isPermaLink="false">2510.13078v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.13774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanFusion是一个地理基础模型(GeoFM)，采用随机多模态融合(SMF)技术，能够有效整合多种地理空间数据，在预测城市现象方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定模型，而最近的用于空间表示的基础模型通常只支持有限模态，缺乏多模态融合能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，开发一个能够处理多种地理数据模态并具有强大泛化能力的地理基础模型。&lt;h4&gt;方法&lt;/h4&gt;UrbanFusion采用模态特定编码器处理街景图像、遥感数据、地图和兴趣点(POIs)数据，并通过基于Transformer的融合模块整合这些多模态输入，学习统一的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在全球56个城市41个任务的评估中，UrbanFusion表现出强大的泛化能力和预测性能：1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。&lt;h4&gt;结论&lt;/h4&gt;UrbanFusion可在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性，所有源代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定的模型，而最近的用于空间表示的基础模型通常只支持有限的模态，且缺乏多模态融合能力。为了克服这些挑战，我们提出了UrbanFusion，这是一个具有随机多模态融合(SMF)的地理基础模型(GeoFM)。该框架采用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图和兴趣点(POIs)数据。这些多模态输入通过基于Transformer的融合模块进行整合，学习统一的表示。在全球56个城市41个任务的广泛评估中，UrbanFusion与最先进的GeoAI模型相比表现出强大的泛化能力和预测性能。具体来说，它1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。UrbanFusion可以在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性。所有源代码均可通过https://github.com/DominikM198/UrbanFusion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting urban phenomena such as housing prices and public healthindicators requires the effective integration of various geospatial data.Current methods primarily utilize task-specific models, while recent foundationmodels for spatial representations often support only limited modalities andlack multimodal fusion capabilities. To overcome these challenges, we presentUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic MultimodalFusion (SMF). The framework employs modality-specific encoders to processdifferent types of inputs, including street view imagery, remote sensing data,cartographic maps, and points of interest (POIs) data. These multimodal inputsare integrated via a Transformer-based fusion module that learns unifiedrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwidedemonstrates UrbanFusion's strong generalization and predictive performancecompared to state-of-the-art GeoAI models. Specifically, it 1) outperformsprior foundation models on location-encoding, 2) allows multimodal input duringinference, and 3) generalizes well to regions unseen during training.UrbanFusion can flexibly utilize any subset of available modalities for a givenlocation during both pretraining and inference, enabling broad applicabilityacross diverse data availability scenarios. All source code is available athttps://github.com/DominikM198/UrbanFusion.</description>
      <author>example@mail.com (Dominik J. Mühlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann)</author>
      <guid isPermaLink="false">2510.13774v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Vision Transformers for Functional MRI with Flat Maps</title>
      <link>http://arxiv.org/abs/2510.13768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop, Foundation Models for the Brain and Body;  Code: https://github.com/MedARC-AI/fmri-fm; Discord:  https://discord.gg/tVR4TWnRM9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了将现代深度学习架构适应功能磁共振成像(fMRI)的方法，通过将4D体积fMRI数据转换为2D平面图视频，并使用时空掩码自编码器框架训练视觉Transformer模型。&lt;h4&gt;背景&lt;/h4&gt;现代深度学习架构如何适应功能磁共振成像(fMRI)是一个关键问题，需要解决fMRI与自然图像之间的模态差距。&lt;h4&gt;目的&lt;/h4&gt;研究如何将fMRI数据表示为模型输入，构建fMRI数据的基础模型。&lt;h4&gt;方法&lt;/h4&gt;将4D体积fMRI数据转换为2D fMRI活动平面图视频，使用时空掩码自编码器框架在人类连接体项目的2.3K小时fMRI平面图视频上训练视觉Transformer，并进行掩码建模和下游分类基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;掩码fMRI建模性能随数据集大小严格遵循幂律缩放规律而提高；模型能够学习丰富的表示，支持跨受试者的精细状态解码和跨脑状态变化的受试者特异性特征解码。&lt;h4&gt;结论&lt;/h4&gt;这是构建fMRI数据基础模型的开放科学项目的一部分，代码和数据已公开共享。&lt;h4&gt;翻译&lt;/h4&gt;将现代深度学习架构适应功能磁共振成像(fMRI)的一个关键问题是如何为模型输入表示数据。为弥合fMRI与自然图像之间的模态差距，我们将4D体积fMRI数据转换为2D fMRI活动平面图视频。我们在人类连接体项目的2.3K小时fMRI平面图视频上使用时空掩码自编码器框架训练视觉Transformer。我们观察到，根据严格的幂律缩放规律，掩码fMRI建模性能随数据集大小增加而提高。下游分类基准测试表明，我们的模型学习了丰富的表示，既支持跨受试者的精细状态解码，也支持跨脑状态变化的受试者特异性特征解码。这项工作是构建fMRI数据基础模型的开放科学项目的一部分。我们的代码和数据可在https://github.com/MedARC-AI/fmri-fm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key question for adapting modern deep learning architectures to functionalMRI (fMRI) is how to represent the data for model input. To bridge the modalitygap between fMRI and natural images, we transform the 4D volumetric fMRI datainto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3Khours of fMRI flat map videos from the Human Connectome Project using thespatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRImodeling performance improves with dataset size according to a strict powerscaling law. Downstream classification benchmarks show that our model learnsrich representations supporting both fine-grained state decoding acrosssubjects, as well as subject-specific trait decoding across changes in brainstate. This work is part of an ongoing open science project to build foundationmodels for fMRI data. Our code and datasets are available athttps://github.com/MedARC-AI/fmri-fm.</description>
      <author>example@mail.com (Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti)</author>
      <guid isPermaLink="false">2510.13768v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任意到任意的跨模态理解与生成，在多模态生成、理解、多轮交互和跨模态检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心组件，但现有多模态模型受限于自回归架构，难以平衡理解与生成能力；混合和解耦策略虽有探索，但冗余设计限制了在广泛场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个支持任何到任何跨模态生成和多轮交互的多模态基础模型，克服现有模型的局限性，实现更高效、更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;引入NExT-OMNI模型，利用度量诱导的概率路径和动力学最优速度，通过离散流范式实现统一建模，并在大规模交错文本、图像、视频和音频数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;NExT-OMNI通过简洁的统一表示而非任务解耦设计，实现了更广泛的应用场景，为促进进一步研究，已公开训练细节、数据协议，并开源了代码和模型检查点。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任意到任意跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组件，在人机交互中发挥关键作用。然而，大多数现有的多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。尽管已经探索了混合和解耦策略来在统一框架内分别解决这些问题，但这些冗余、非集成的设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在本工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任意到任意的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，并提高响应效率。在大型交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了促进进一步研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modalretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodalfoundation model that achieves unified modeling through discrete flowparadigms. By leveraging metric-induced probability paths and kinetic optimalvelocities, NExT-OMNI natively supports any-to-any understanding and generationwith enhanced response efficiency, while enabling broader application scenariosthrough concise unified representations rather than task-decoupled designs.Trained on large-scale interleaved text, image, video, and audio data,NExT-OMNI delivers competitive performance on multimodal generation andunderstanding benchmarks, while outperforming prior unified models inmulti-turn multimodal interaction and cross-modal retrieval, highlighting itsarchitectural advantages as a next-generation multimodal foundation model. Toadvance further research, we release training details, data protocols, andopen-source both the code and model checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Axial Neural Networks for Dimension-Free Foundation Models</title>
      <link>http://arxiv.org/abs/2510.13665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轴向神经网络(XNN)架构，解决了在物理数据上训练基础模型时面临的维度变化挑战，使模型能够有效处理不同维度的偏微分方程数据，同时保持计算效率和性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，在零样本推理和上下文学习方面表现出色。然而，在物理数据（包括偏微分方程PDEs的解）上训练此类模型面临独特挑战，因为不同系统的维度各不相同。&lt;h4&gt;目的&lt;/h4&gt;提出一种维度不可知的神经网络架构，解决传统方法在处理不同维度数据时效率低下的问题。&lt;h4&gt;方法&lt;/h4&gt;提出轴向神经网络(XNN)，受Deep Sets和图神经网络等参数共享结构的启发。将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;XNN架构解决了在物理数据上训练基础模型的维度挑战，同时保持了性能和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，使零样本推理和上下文学习能力显著提升。然而，在包括偏微分方程(PDEs)解在内的物理数据上训练此类模型，由于不同系统间维度的变化，带来了独特挑战。传统方法要么固定最大维度，要么为不同维度使用单独的编码器，导致效率低下。为此，我们提出了一种维度不可知的神经网络架构——轴向神经网络(XNN)，其灵感来自Deep Sets和图神经网络等参数共享结构。XNN能够在保持计算效率的同时，推广到变化的张量维度。我们将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估其性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of foundation models in AI has significantly advancedgeneral-purpose learning, enabling remarkable capabilities in zero-shotinference and in-context learning. However, training such models on physicsdata, including solutions to partial differential equations (PDEs), poses aunique challenge due to varying dimensionalities across different systems.Traditional approaches either fix a maximum dimension or employ separateencoders for different dimensionalities, resulting in inefficiencies. Toaddress this, we propose a dimension-agnostic neural network architecture, theAxial Neural Network (XNN), inspired by parameter-sharing structures such asDeep Sets and Graph Neural Networks. XNN generalizes across varying tensordimensions while maintaining computational efficiency. We convert existing PDEfoundation models into axial neural networks and evaluate their performanceacross three training scenarios: training from scratch, pretraining on multiplePDEs, and fine-tuning on a single PDE. Our experiments show that XNNs performcompetitively with original models and exhibit superior generalization tounseen dimensions, highlighting the importance of multidimensional pretrainingfor foundation models.</description>
      <author>example@mail.com (Hyunsu Kim, Jonggeon Park, Joan Bruna, Hongseok Yang, Juho Lee)</author>
      <guid isPermaLink="false">2510.13665v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Time Series Foundation Models: Benchmarking Challenges and Requirements</title>
      <link>http://arxiv.org/abs/2510.13654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)是一种新的时间序列预测范式，具有零样本预测能力，但其评估面临多个挑战，包括数据集代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。与大型语言模型(LLMs)类似，随着训练集的不断扩大，确保基准测试数据的完整性变得越来越困难。&lt;h4&gt;目的&lt;/h4&gt;调查现有TSFM评估的挑战，并提出改进评估方法的建议，以保障TSFM评估的完整性。&lt;h4&gt;方法&lt;/h4&gt;通过调查现有TSFM评估实践，分析数据分区问题，并提出新的评估方法建议，如在真正外来的未来数据上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;现有TSFM评估存在多个挑战，包括基准数据集的代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。此外，关于数据分区的普遍混乱可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。&lt;h4&gt;结论&lt;/h4&gt;需要开发强大的评估方法来防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的评估方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。然而，与大型语言模型(LLMs)一样，评估TSFMs很棘手，因为随着训练集的不断扩展，确保基准测试数据的完整性变得越来越具有挑战性。我们对现有TSFM评估的调查揭示了多个挑战，从基准数据集的代表性、缺乏时空评估，到由于数据集重叠和不透明导致的信息泄露风险，以及由经济危机或疫情等外部冲击引起的全局模式记忆问题。我们的发现揭示了关于数据分区的普遍混乱，这可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。我们呼吁开发强大的评估方法，以防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) represent a new paradigm for timeseries forecasting, offering zero-shot forecasting capabilities without theneed for domain-specific pre-training or fine-tuning. However, as with LargeLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensivetraining sets, it becomes more and more challenging to ensure the integrity ofbenchmarking data. Our investigation of existing TSFM evaluation highlightsmultiple challenges, ranging from the representativeness of the benchmarkdatasets, over the lack of spatiotemporal evaluation, to risks of informationleakage due to overlapping and obscure datasets, and the memorization of globalpatterns caused by external shocks like economic crises or pandemics. Ourfindings reveal widespread confusion regarding data partitions, riskinginflated performance estimates and incorrect transfer of global knowledge tolocal time series. We argue for the development of robust evaluationmethodologies to prevent pitfalls already observed in LLM and classical timeseries benchmarking, and call upon the research community to design new,principled approaches, such as evaluations on truly out-of-sample future data,to safeguard the integrity of TSFM assessment.</description>
      <author>example@mail.com (Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Oliver Müller)</author>
      <guid isPermaLink="false">2510.13654v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.13643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基于DINOv2等基础模型的小样本异常检测器的对抗性扰动敏感性和不确定性校准问题。作者通过在冻结的DINOv2特征上附加轻量级线性头创建对抗性攻击，评估了FGSM攻击的影响，并发现微小扰动可显著降低检测性能。同时，原始异常分数校准性较差，通过应用Platt缩放方法，作者提出了实用的攻击检测机制并降低了校准误差。&lt;h4&gt;背景&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大性能，但两个关键问题尚未得到研究：(1)这些检测器对抗性扰动的敏感性如何；(2)它们的异常分数在多大程度上反映了校准的不确定性。&lt;h4&gt;目的&lt;/h4&gt;研究DINOv2等基础模型在小样本异常检测中的对抗性鲁棒性和不确定性校准问题，并提出实用的攻击检测机制，以提高异常检测系统的可信度和安全性。&lt;h4&gt;方法&lt;/h4&gt;基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），作者在冻结的DINOv2特征上附加轻量级线性头仅用于创建对抗性扰动。评估FGSM攻击在MVTec-AD和VisA数据集上的影响，并应用后验Platt缩放方法对异常分数进行不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;1) 微小对抗性扰动显著降低检测性能(F1、AUROC、AP和G-mean指标均下降)；2) 扰动可在特征空间翻转最近邻关系，导致有把握的错误分类；3) 原始异常分数校准性较差，置信度与正确性存在差距；4) Platt缩放得到的校验后验分布在对抗性扰动输入上产生更高预测熵；5) 该方法可用于实用攻击检测机制，同时降低校准误差(ECE)。&lt;h4&gt;结论&lt;/h4&gt;DINOv2基础的小样本异常检测器存在具体脆弱性，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大的性能，但两个关键问题尚未得到研究：(i)这些检测器对抗性扰动的敏感性如何；(ii)它们的异常分数在多大程度上反映了校准的不确定性。基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），我们进行了此设置中对抗性攻击和不确定性估计的首次系统性研究之一。为了在保持测试时间行为的同时实现白盒梯度攻击，我们仅在创建扰动时为冻结的DINOv2特征附加了一个轻量级线性头。使用这种启发式方法，我们评估了FGSM在MVTec-AD和VisA数据集上的影响，并观察到F1、AUROC、AP和G-mean指标的一致下降，表明微小的扰动可以在特征空间中翻转最近邻关系，导致有把握的错误分类。除了鲁棒性外，我们还探测了可靠性，发现原始异常分数的校准性较差，揭示了置信度与正确性之间的差距，这限制了安全关键应用。作为迈向可信度的简单、强基线，我们对异常分数应用了后验Platt缩放进行不确定性估计。所得的校验后验分布在对抗性扰动输入上产生显著更高的预测熵，能够用于实用的攻击检测机制，同时降低校准误差（ECE）。我们的研究结果揭示了DINOv2基础小样本异常检测器的具体脆弱性，并为鲁棒、不确定性感知的异常检测建立了评估协议和基线。我们认为，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as DINOv2 have shown strong performance in few-shotanomaly detection, yet two key questions remain unexamined: (i) how susceptibleare these detectors to adversarial perturbations; and (ii) how well do theiranomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, atraining-free deep nearest-neighbor detector over DINOv2 features, we presentone of the first systematic studies of adversarial attacks and uncertaintyestimation in this setting. To enable white-box gradient attacks whilepreserving test-time behavior, we attach a lightweight linear head to frozenDINOv2 features only for crafting perturbations. Using this heuristic, weevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observeconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptibleperturbations can flip nearest-neighbor relations in feature space to induceconfident misclassification. Complementing robustness, we probe reliability andfind that raw anomaly scores are poorly calibrated, revealing a gap betweenconfidence and correctness that limits safety-critical use. As a simple, strongbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomalyscores for uncertainty estimation. The resulting calibrated posteriors yieldsignificantly higher predictive entropy on adversarially perturbed inputs thanon clean ones, enabling a practical flagging mechanism for attack detectionwhile reducing calibration error (ECE). Our findings surface concretevulnerabilities in DINOv2-based few-shot anomaly detectors and establish anevaluation protocol and baseline for robust, uncertainty-aware anomalydetection. We argue that adversarial robustness and principled uncertaintyquantification are not optional add-ons but essential capabilities if anomalydetection systems are to be trustworthy and ready for real-world deployment.</description>
      <author>example@mail.com (Akib Mohammed Khan, Bartosz Krawczyk)</author>
      <guid isPermaLink="false">2510.13643v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Computing Resources in Publishing Foundation Model Research</title>
      <link>http://arxiv.org/abs/2510.13621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了计算资源与基础模型科学进展之间的关系，发现增加计算资源与国家资金分配和引用量相关，但与研究环境、领域或研究方法无强相关性。&lt;h4&gt;背景&lt;/h4&gt;前沿的人工智能研究需要大量资源，包括图形处理器(GPU)、数据和人力资源。&lt;h4&gt;目的&lt;/h4&gt;评估这些资源与基础模型科学进展之间的关系。&lt;h4&gt;方法&lt;/h4&gt;回顾了2022年至2024年间发表的6517篇基础模型论文，并调查了229位第一作者关于计算资源对科研产出影响的情况。&lt;h4&gt;主要发现&lt;/h4&gt;增加的计算资源与国家资金分配和引用量相关，但未发现与研究环境(学术界或工业界)、领域或研究方法有强相关性。&lt;h4&gt;结论&lt;/h4&gt;建议个人和机构专注于创建共享且负担得起的计算机会，以减少资源不足研究者的入门障碍，这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。&lt;h4&gt;翻译&lt;/h4&gt;尖端的人工智能研究需要大量资源，包括图形处理器、数据和人力资源。在本文中，我们评估了这些资源与基础模型科学进展之间的关系。我们回顾了2022年至2024年间发表的6517篇基础模型论文，并对229位第一作者进行了调查，了解计算资源对科研产出的影响。我们发现，增加的计算资源与国家资金分配和引用量相关，但我们的研究结果未观察到与研究环境(学术界或工业界)、领域或研究方法有强相关性。我们建议个人和机构专注于创建共享且负担得起的计算机会，以降低资源不足研究者的入门门槛。这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。数据将在https://mit-calc.csail.mit.edu/提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge research in Artificial Intelligence (AI) requires considerableresources, including Graphics Processing Units (GPUs), data, and humanresources. In this paper, we evaluate of the relationship between theseresources and the scientific advancement of foundation models (FM). We reviewed6517 FM papers published between 2022 to 2024, and surveyed 229 first-authorsto the impact of computing resources on scientific output. We find thatincreased computing is correlated with national funding allocations andcitations, but our findings don't observe the strong correlations with researchenvironment (academic or industrial), domain, or study methodology. We advisethat individuals and institutions focus on creating shared and affordablecomputing opportunities to lower the entry barrier for under-resourcedresearchers. These steps can help expand participation in FM research, fosterdiversity of ideas and contributors, and sustain innovation and progress in AI.The data will be available at: https://mit-calc.csail.mit.edu/</description>
      <author>example@mail.com (Yuexing Hao, Yue Huang, Haoran Zhang, Chenyang Zhao, Zhenwen Liang, Paul Pu Liang, Yue Zhao, Lichao Sun, Saleh Kalantari, Xiangliang Zhang, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.13621v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment</title>
      <link>http://arxiv.org/abs/2510.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ICPADS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLSDA的新型泛化框架，利用预训练大型基础模型的语义先验来增强WiFi手势识别的泛化能力和语义表达能力，通过双路径CSI编码、多尺度语义编码、语义感知软监督和鲁棒双蒸馏策略，实现了在域内和跨域场景中的高性能手势识别。&lt;h4&gt;背景&lt;/h4&gt;WiFi手势识别作为一种有前途的RF传感范式，可在AIoT环境中实现非接触式和隐私保护的人机交互。然而，现有方法因信道状态信息的域敏感特性和高级手势抽象的缺乏，面临泛化能力和语义表达能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有WiFi手势识别方法泛化能力有限和语义表达能力不足的问题，提出一种能够增强域内和跨域场景中手势表示学习的新型框架。&lt;h4&gt;方法&lt;/h4&gt;1) 设计双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获手势模式；2) 开发多尺度语义编码器，学习时序嵌入并通过跨模态注意力机制与手势语义对齐；3) 引入语义感知软监督方案，编码类间相关性并减少标签模糊性；4) 开发鲁棒双蒸馏策略，将对齐模型压缩为轻量级网络。&lt;h4&gt;主要发现&lt;/h4&gt;在Widar3.0基准上的实验表明，GLSDA在域内和跨域手势识别任务中均优于现有最先进方法，同时显著减小了模型大小和推理延迟。&lt;h4&gt;结论&lt;/h4&gt;GLSDA为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于WiFi的手势识别已成为一种有前途的RF传感范式，能够在AIoT环境中实现非接触式和隐私保护的人机交互。然而，由于信道状态信息的域敏感特性和高级手势抽象的缺乏，现有方法通常面临泛化能力和语义表达能力有限的问题。为解决这些挑战，我们提出了一种名为Large-Model-Aware Semantic Distillation and Alignment (GLSDA)的新型泛化框架，它利用预训练大型基础模型的语义先验来增强域内和跨域场景中的手势表示学习。具体而言，我们首先设计了一个双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获几何和动态手势模式。然后将这些表示输入多尺度语义编码器，学习鲁棒的时序嵌入，并通过跨模态注意力机制将其与手势语义对齐。为进一步增强类别区分度，我们引入了一种语义感知软监督方案，编码类间相关性并减少标签模糊性，特别是对于语义相似的手势。最后，我们开发了一种鲁棒双蒸馏策略，将对齐的模型压缩为轻量级学生网络，从教师模型联合蒸馏中间特征和语义感知软标签。在Widar3.0基准上的大量实验表明，GLSDA在域内和跨域手势识别任务中始终优于最先进的方法，同时显著减小了模型大小和推理延迟。我们的方法为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; WiFi-based gesture recognition has emerged as a promising RF sensing paradigmfor enabling non-contact and privacy-preserving human-computer interaction inAIoT environments. However, existing methods often suffer from limitedgeneralization and semantic expressiveness due to the domain-sensitive natureof Channel State Information and the lack of high-level gesture abstraction. Toaddress these challenges, we propose a novel generalization framework, termedLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leveragesthe semantic prior of pre-trained large foundation models to enhance gesturerepresentation learning in both in-domain and cross-domain scenarios.Specifically, we first design a dual-path CSI encoding pipeline that capturesgeometric and dynamic gesture patterns via CSI-Ratio phase sequences andDoppler spectrograms. These representations are then fed into a MultiscaleSemantic Encoder, which learns robust temporal embeddings and aligns them withgesture semantics through cross-modal attention mechanisms. To further enhancecategory discrimination, we introduce a Semantic-Aware Soft Supervision schemethat encodes inter-class correlations and reduces label ambiguity, especiallyfor semantically similar gestures. Finally, we develop a RobustDual-Distillation strategy to compress the aligned model into a lightweightstudent network, jointly distilling intermediate features and semantic-informedsoft labels from the teacher model. Extensive experiments on the Widar3.0benchmark show that GLSDA consistently outperforms state-of-the-art methods inboth in-domain and cross-domain gesture recognition tasks, while significantlyreducing model size and inference latency. Our method offers a scalable anddeployable solution for generalized RF-based gesture interfaces in real-worldAIoT applications.</description>
      <author>example@mail.com (Feng-Qi Cui, Yu-Tong Guo, Tianyue Zheng, Jinyang Huang)</author>
      <guid isPermaLink="false">2510.13390v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Document Intelligence in the Era of Large Language Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了Document AI (DAI)领域在大语言模型(LLMs)影响下的发展，探讨了多模态、多语言和检索增强DAI的进展与挑战，并提出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;Document AI已成为重要应用领域，大语言模型的出现显著改变了这一领域，从早期的编码器-解码器架构发展为仅使用解码器的LLMs。&lt;h4&gt;目的&lt;/h4&gt;提供DAI演变的全面概述，突出LLMs在该领域的当前研究和未来前景，为DAI的最先进技术提供结构化分析。&lt;h4&gt;方法&lt;/h4&gt;通过综述形式，探索多模态、多语言和检索增强DAI的关键进展和挑战，并提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;解码器-only LLMs彻底改变了DAI，带来了理解和生成方面的显著进步；多模态、多语言和检索增强DAI面临关键进展和挑战；基于代理的方法和文档特定基础模型是有前景的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;DAI在大语言模型的影响下正在快速发展，为学术和实践应用提供了新的可能性和挑战。&lt;h4&gt;翻译&lt;/h4&gt;文档人工智能(DAI)已成为一个重要的应用领域，并因大型语言模型(LLMs)的出现而发生了显著变化。虽然早期方法依赖于编码器-解码器架构，但仅使用解码器的LLMs彻底改变了DAI，在理解和生成方面带来了显著进步。本综述提供了DAI演变的全面概述，突出了LLMs在该领域的当前研究和未来前景。我们探索了多模态、多语言和检索增强DAI的关键进展和挑战，同时提出了未来研究方向，包括基于代理的方法和文档特定基础模型。本文旨在为DAI的最先进技术提供结构化分析，及其对学术和实践应用的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Document AI (DAI) has emerged as a vital application area, and issignificantly transformed by the advent of large language models (LLMs). Whileearlier approaches relied on encoder-decoder architectures, decoder-only LLMshave revolutionized DAI, bringing remarkable advancements in understanding andgeneration. This survey provides a comprehensive overview of DAI's evolution,highlighting current research attempts and future prospects of LLMs in thisfield. We explore key advancements and challenges in multimodal, multilingual,and retrieval-augmented DAI, while also suggesting future research directions,including agent-based approaches and document-specific foundation models. Thispaper aims to provide a structured analysis of the state-of-the-art in DAI andits implications for both academic and practical applications.</description>
      <author>example@mail.com (Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier)</author>
      <guid isPermaLink="false">2510.13366v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generative model for information metamaterial design</title>
      <link>http://arxiv.org/abs/2510.13264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为InfoMetaGen的通用生成模型，用于信息超材料设计，结合预训练基础模型和轻量级功能适配器，能够智能生成从元原子到任意空间编码模式的人工结构，相比传统方法具有更高的效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;生成式模型如AlphaFold和MatterGen可直接生成具有理想特性的新型材料结构，AlphaFold专注于蛋白质预测，MatterGen专注于预测周期性晶体结构，而超材料的通用设计更为复杂，需要设计元原子及其在空间中的任意非均匀分布。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用的生成式模型InfoMetaGen用于信息超材料设计，解决超材料设计中的复杂性问题，实现从元原子到任意空间编码模式的智能生成。&lt;h4&gt;方法&lt;/h4&gt;InfoMetaGen结合预训练基础模型和轻量级功能适配器，通过微调轻量级适配器使单一通用生成模型能够切换不同功能，避免了传统方法需要为特定功能训练专用模型的局限。&lt;h4&gt;主要发现&lt;/h4&gt;InfoMetaGen能够加速新型超材料的多样化发现，在超材料性能方面取得突破，填补了设计人工材料时通用生成框架的空白。&lt;h4&gt;结论&lt;/h4&gt;该工作将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造，为生成模型在材料设计领域开辟了前所未有的机会。&lt;h4&gt;翻译&lt;/h4&gt;生成模型如AlphaFold和MatterGen可以直接生成具有理想特性的新型材料结构，加速新材料发现并将材料设计范式从传统的试错方法转变为智能按需生成。AlphaFold专注于具有特定非周期结构的蛋白质预测；而MatterGen专注于预测周期性和稳定的晶体结构。超材料的通用设计要复杂得多，因为它涉及设计元原子（类似于周期结构）及其在空间中的任意非均匀分布。在此，我们提出了InfoMetaGen，一种用于信息超材料设计的通用生成模型，它结合了预训练基础模型和轻量级功能适配器，智能生成从元原子到任意空间编码模式的人工结构。与需要为特定功能训练专用模型的传统智能超材料设计方法相比，InfoMetaGen使单个通用生成模型能够通过微调轻量级适配器切换不同功能，显著提高了效率和泛化能力。实验结果表明，InfoMetaGen不仅可以加速新型超材料的多样化发现，还能在超材料性能方面取得突破。这项工作填补了设计人工材料时通用生成框架的空白，并为将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造开辟了前所未有的机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决信息超材料的智能设计问题。传统超材料设计依赖于试错法，周期长、效率低，且现有智能方法多局限于单一功能。超材料能实现自然材料中不存在的奇特物理特性，在无线通信、传感和超分辨率成像等领域有广泛应用，因此高效设计方法对推动这些领域发展至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了AlphaFold和MatterGen等生成模型的成功经验，认识到超材料设计比蛋白质或晶体设计更复杂，因为它涉及设计超原子和它们在空间中的任意非均匀分布。他们采用两阶段训练策略：首先预训练无条件扩散模型捕获不同设计空间中的功能编码模式，然后引入条件适配器模块微调模型引导生成过程。这种方法冻结基础模型参数，只微调轻量级适配器，使单个模型能处理多种功能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个预训练的基础模型结合轻量级功能适配器，智能生成从单个超原子到整个超阵列的结构配置，实现多功能的统一生成。流程包括：1)预训练阶段训练无条件扩散模型捕获不同功能的设计模式；2)微调阶段引入条件适配器，冻结基础模型参数只微调适配器；3)生成阶段根据特定功能条件将随机噪声转换为功能数字比特流；4)应用阶段实现超原子设计、波束成形、电磁聚焦和全息成像等多种功能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出InfoMetaGen通用生成框架填补设计空白；2)实现多尺度设计能力从超原子到超阵列；3)通过轻量级适配器实现高效多任务处理；4)创新比特表示方法解决离散编码与连续扩散模型兼容性；5)强大生成能力产生新颖超原子和1/3位非均匀超阵列。相比之前工作，它专注于宏观人工材料而非微观自然材料，使用单一通用模型替代专用模型，实现多功能的统一生成，并能突破训练数据集限制生成高性能宽带超原子。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfoMetaGen开创了信息超材料设计的通用生成范式，通过结合预训练基础模型与轻量级功能适配器，实现了从超原子到超阵列的多尺度、多功能智能生成，显著加速了新超材料的发现并突破了超材料性能的极限。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models such as AlphaFold and MatterGen can directly generate novelmaterial structures with desired properties, accelerating the new materialsdiscovery and revolutionizing the material design paradigm from traditionaltrial-and-error approach to intelligent on-demand generation. AlphaFold isfocused on protein prediction with specific aperiodic structures; whileMatterGen is focused on predicting periodic and stable crystal structures. Theuniversal design of metamaterials is much more complicated, since it involvesto design meta-atoms (similar to the periodic structures) and their arbitrarilyinhomogeneous distributions in space. Here, we propose InfoMetaGen, a universalgenerative model for information metamaterial design, which combines apre-trained foundation model with lightweight functional adapters tointelligently generate artificial structures on-demand spanning from meta-atomsto arbitrary space coding patterns. In contrast to conventional intelligentmetamaterial design methods that require training dedicated models for specificfunctionalities, InfoMetaGen enables a single universal generative modelcapable of switching across diverse functionalities by fine-tuning thelightweight adapters, significantly improving both efficiency andgeneralizability. Experimental results demonstrate that InfoMetaGen can notonly accelerate the diverse discovery of new metamaterials, but also achievebreakthroughs in metamaterial performance. This work fills the gap of universalgenerative framework in designing artificial materials, and opens upunprecedented opportunities to expand the capability of generative models fromthe passive discovery of microscopic natural material to the active creation ofmacroscopic artificial materials.</description>
      <author>example@mail.com (Jun Ming Hou, Long Chen, Xuan Zheng, Jia Wei Wu, Jian Wei You, Zi Xuan Cai, Jiahan Huang, Chen Xu Wu, Jian Lin Su, Lianlin Li, Jia Nan Zhang, Tie Jun Cui)</author>
      <guid isPermaLink="false">2510.13264v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</title>
      <link>http://arxiv.org/abs/2510.13068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NeuroRVQ的新型脑电图基础模型，通过改进的信号令牌化方法解决了现有模型在高频动态处理和信号重建方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。现有的EEG基础模型在信号令牌化方面存在不足，无法保持高频动态，限制了信号重建的保真度。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获完整频率神经频谱、支持高分辨率编码并实现高效训练的EEG信号令牌化器，以提高EEG信号的表示能力和重建质量。&lt;h4&gt;方法&lt;/h4&gt;NeuroRVQ令牌化器包含三个关键组件：多尺度特征提取模块，捕获完整频率神经频谱；分层残差矢量量化(RVQ)码本，用于高分辨率编码；以及EEG信号相位和幅度感知损失函数，用于高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;NeuroRVQ设计支持所有频段的准确重建，同时实现高效的EEG压缩，实现了强大的生成掩码建模。实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的大脑波模型。&lt;h4&gt;结论&lt;/h4&gt;NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，有望推动神经解码、生成建模和多模态生物信号集成等领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。最近，经过训练以预测掩码信号令牌的EEG基础模型在学习可泛化表示方面显示出前景。然而，它们的性能受到信号令牌化模块的限制。现有的神经令牌化器无法保持高频动态，限制了它们以高保真度重建EEG信号的能力。我们引入了NeuroRVQ，这是一个基于码本令牌化器的可扩展大脑波模型(LBM)。我们的令牌化器整合了：(i)捕获完整频率神经频谱的多尺度特征提取模块；(ii)用于高分辨率编码的分层残差矢量量化(RVQ)码本；以及(iii)用于高效训练的EEG信号相位和幅度感知损失函数。这种设计支持所有频段的准确重建，同时实现高效的EEG压缩，从而实现强大的生成掩码建模。我们的实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的LBM。更广泛地说，NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，推动了神经解码、生成建模和多模态生物信号集成的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) captures neural activity across multipletemporal and spectral scales, yielding signals that are rich but complex forrepresentation learning. Recently, EEG foundation models trained to predictmasked signal-tokens have shown promise for learning generalizablerepresentations. However, their performance is hindered by their signaltokenization modules. Existing neural tokenizers fail to preservehigh-frequency dynamics, limiting their ability to reconstruct EEG signals withhigh fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)centered on a codebook-based tokenizer. Our tokenizer integrates: (i)multi-scale feature extraction modules that capture the full frequency neuralspectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks forhigh-resolution encoding; and, (iii) an EEG signal phase- and amplitude-awareloss function for efficient training. This design enables efficient EEGcompression while supporting accurate reconstruction across all frequencybands, leading to robust generative masked modeling. Our empirical resultsdemonstrate that NeuroRVQ achieves lower reconstruction error and outperformsexisting LBMs on a variety of downstream tasks. More broadly, NeuroRVQtokenizer establishes a strong prior for codebook-based general-purposebrainwave models, enabling advances in neural decoding, generative modeling andmultimodal biosignal integration.</description>
      <author>example@mail.com (Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou)</author>
      <guid isPermaLink="false">2510.13068v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</title>
      <link>http://arxiv.org/abs/2510.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了FetalMind，一个专为胎儿超声检查设计的医学AI系统，用于报告生成和诊断。通过引入显著认识解耦（SED）方法和构建大规模数据集FetalSigma-1M，FetalMind在所有妊娠阶段都优于现有基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上表现出色，但大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳。胎儿超声面临多视图图像推理、疾病多样性和图像多样性的挑战。&lt;h4&gt;目的&lt;/h4&gt;弥合现有医学视觉-语言模型在胎儿超声领域的应用差距，开发一个专门针对胎儿超声检查的AI系统，用于报告生成和诊断。&lt;h4&gt;方法&lt;/h4&gt;在临床工作流程指导下提出显著认识解耦（SED）方法，将专家构建的二分图注入模型以解耦视图-疾病关联，并通过强化学习引导偏好选择。同时构建了FetalSigma-1M数据集，包含来自十二个医疗中心的20K份胎儿超声报告，解决了领域数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;FetalMind是一个有效的胎儿超声AI系统，通过SED方法和大规模数据集训练，成功解决了胎儿超声图像推理的挑战，在报告生成和诊断方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上显示出潜力。然而，大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳，这带来了多视图图像推理、疾病多样性和图像多样性的挑战。为了弥合这一差距，我们引入了FetalMind，一个专为胎儿超声设计的医学AI系统，用于报告生成和诊断。在临床工作流程的指导下，我们提出了显著认识解耦（SED）方法，将专家构建的二分图注入模型中，解耦视图-疾病关联，并通过强化学习引导偏好选择，遵循临床忠实步骤。这种设计减轻了疾病间的变异性和视图间的异质性，减少了学习瓶颈，同时使模型的推理与产科实践保持一致。为了大规模训练FetalMind，我们整理了FetalSigma-1M数据集，这是第一个大规模胎儿超声报告语料库，包含来自十二个医疗中心的20K份报告，解决了领域数据稀缺的问题。大量实验表明，FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent medical vision-language models have shown promise on tasks such asVQA, report generation, and anomaly detection. However, most are adapted tostructured adult imaging and underperform in fetal ultrasound, which poseschallenges of multi-view image reasoning, numerous diseases, and imagediversity. To bridge this gap, we introduce FetalMind, a medical AI systemtailored to fetal ultrasound for both report generation and diagnosis. Guidedby clinical workflow, we propose Salient Epistemic Disentanglement (SED), whichinjects an expert-curated bipartite graph into the model to decoupleview-disease associations and to steer preference selection along clinicallyfaithful steps via reinforcement learning. This design mitigates variabilityacross diseases and heterogeneity across views, reducing learning bottleneckswhile aligning the model's inference with obstetric practice. To trainFetalMind at scale, we curate FetalSigma-1M dataset, the first large-scalefetal ultrasound report corpus, comprising 20K reports from twelve medicalcenters, addressing the scarcity of domain data. Extensive experiments showthat FetalMind outperforms open- and closed-source baselines across allgestational stages, achieving +14% average gains and +61.2% higher accuracy oncritical conditions while remaining efficient, stable, and scalable. ProjectPage: https://hexiao0275.github.io/FetalMind.</description>
      <author>example@mail.com (Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du)</author>
      <guid isPermaLink="false">2510.12953v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>An Investigation of Memorization Risk in Healthcare Foundation Models</title>
      <link>http://arxiv.org/abs/2510.12950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一套用于评估在电子健康记录上训练的基础模型中隐私相关记忆风险的黑盒评估测试，包括在嵌入层和生成层探测记忆的方法，并发布了开源工具包促进医疗AI中的隐私评估。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大型去标识化的电子健康记录上训练有临床应用前景，但它们可能记住患者信息，引发隐私问题。&lt;h4&gt;目的&lt;/h4&gt;引入一套黑盒评估测试来评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，包括在嵌入层和生成层探测记忆的方法，旨在区分模型泛化和有害记忆，特别是在临床相关环境中。&lt;h4&gt;主要发现&lt;/h4&gt;将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。&lt;h4&gt;结论&lt;/h4&gt;在公开可用的EHR基础模型上验证了这种方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;h4&gt;翻译&lt;/h4&gt;在大型去标识化电子健康记录(EHRs)上训练的基础模型在临床应用方面具有潜力。然而，它们记忆患者信息的能力引发了重要的隐私问题。在这项工作中，我们引入了一套黑盒评估测试，用于评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。我们的框架包括在嵌入层和生成层探测记忆的方法，旨在区分临床相关环境中的模型泛化和有害记忆。我们将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。我们在公开可用的EHR基础模型上验证了我们的方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale de-identified electronic healthrecords (EHRs) hold promise for clinical applications. However, their capacityto memorize patient information raises important privacy concerns. In thiswork, we introduce a suite of black-box evaluation tests to assessprivacy-related memorization risks in foundation models trained on structuredEHR data. Our framework includes methods for probing memorization at both theembedding and generative levels, and aims to distinguish between modelgeneralization and harmful memorization in clinically relevant settings. Wecontextualize memorization in terms of its potential to compromise patientprivacy, particularly for vulnerable subgroups. We validate our approach on apublicly available EHR foundation model and release an open-source toolkit tofacilitate reproducible and collaborative privacy assessments in healthcare AI.</description>
      <author>example@mail.com (Sana Tonekaboni, Lena Stempfle, Adibvafa Fallahpour, Walter Gerych, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.12950v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <link>http://arxiv.org/abs/2510.12709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-Embedding是一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决了现有多模态嵌入模型在现实应用中面临的挑战，包括模态支持有限、训练机制不稳定和工业领域差距等问题。&lt;h4&gt;背景&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示以支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决现有多模态嵌入模型面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出多阶段训练方案提高表示学习效果，包括内容感知渐进训练增强模型对多样化下游任务的适应性，协作感知推荐增强训练使多模态表示适应推荐场景，以及开发随机专业化和数据集驱动的模式匹配增强模型训练的灵活性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SAIL-Embedding在不同检索任务中实现了SOTA性能。在线实验显示，Lifetime (LT)显著增加，在抖音精选场景中带来+0.5%的7天LT增益，为抖音feed排序模型带来+0.1%的AUC增益。&lt;h4&gt;结论&lt;/h4&gt;SAIL-Embedding是一个有效的全模态嵌入基础模型，通过定制化训练策略和架构设计，在各种场景中表现出色，特别是在推荐系统中。&lt;h4&gt;翻译&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示，支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定和工业领域差距等。在本工作中，我们介绍了SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决这些问题。在优化过程中，我们提出多阶段训练方案以提高表示学习的多方面有效性。具体而言，内容感知渐进训练旨在增强模型对多样化下游任务的适应性，掌握丰富的跨模态能力。协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中提炼知识，同时挖掘用户历史兴趣，使多模态表示适应推荐场景。同时，我们开发了随机专业化和数据集驱动的模式匹配，增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中与其他方法相比实现了SOTA性能。在各种集成我们模型的现实场景的在线实验中，我们观察到作为推荐体验关键指标的Lifetime (LT)显著增加。例如，在抖音精选场景中，模型带来了+0.5%的7天LT增益。对于抖音feed排序模型，SAIL-Embedding产生的匹配特征带来了+0.1%的AUC增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal embedding models aim to yield informative unified representationsthat empower diverse cross-modal tasks. Despite promising developments in theevolution from CLIP-based dual-tower architectures to large vision-languagemodels, prior works still face unavoidable challenges in real-worldapplications and business scenarios, such as the limited modality support,unstable training mechanisms, and industrial domain gaps. In this work, weintroduce SAIL-Embedding, an omni-modal embedding foundation model thataddresses these issues through tailored training strategies and architecturaldesign. In the optimization procedure, we propose a multi-stage training schemeto boost the multifaceted effectiveness of representation learning.Specifically, the content-aware progressive training aims to enhance themodel's adaptability to diverse downstream tasks and master enrichedcross-modal proficiency. The collaboration-aware recommendation enhancementtraining further adapts multimodal representations for recommendation scenariosby distilling knowledge from sequence-to-item and ID-to-item embeddings whilemining user historical interests. Concurrently, we develop the stochasticspecialization and dataset-driven pattern matching to strengthen model trainingflexibility and generalizability. Experimental results show that SAIL-Embeddingachieves SOTA performance compared to other methods in different retrievaltasks. In online experiments across various real-world scenarios integratedwith our model, we observe a significant increase in Lifetime (LT), which is acrucial indicator for the recommendation experience. For instance, the modeldelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For theDouyin feed rank model, the match features produced by SAIL-Embedding yield a+0.1% AUC gain.</description>
      <author>example@mail.com (Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng)</author>
      <guid isPermaLink="false">2510.12709v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB</title>
      <link>http://arxiv.org/abs/2510.13404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于合成短波红外（SWIR）的多模态融合框架，用于改善恶劣能见度条件下的场景理解，解决了传统成像模态在融合时难以提供全面场景信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统是一个关键挑战。传统的RGB和热红外成像模态在融合时难以在大气干扰或照明不足条件下提供全面场景信息。&lt;h4&gt;目的&lt;/h4&gt;解决传统成像模态的局限性，克服SWIR成像发展和实施中因缺乏公开SWIR数据集而面临的障碍，提高融合图像质量。&lt;h4&gt;方法&lt;/h4&gt;利用先进对比度增强技术从现有长波红外（LWIR）数据合成类似SWIR的结构/对比度线索图像，提出多模态融合框架集成合成SWIR、LWIR和RGB模态，采用优化的编码器-解码器神经网络架构和softmax门控融合头。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共RGB-LWIR基准和私有真实RGB-MWIR-SWIR数据集上的实验表明，该框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能，并优于其他三模态融合方法。&lt;h4&gt;结论&lt;/h4&gt;合成SWIR增强的多模态融合框架在监控和自主系统中有实际应用的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统仍然是一个关键挑战。传统的成像模态，如RGB和热红外（中波/长波），在融合时往往难以提供全面的场景信息，特别是在大气干扰或照明不足的条件下。为解决这些限制，短波红外（SWIR）成像因其能够穿透大气干扰并以更清晰的分辨率区分材料而成为一种有前景的模态。然而，基于SWIR系统的发展和广泛实施面临重大障碍，主要是由于缺乏公开可访问的SWIR数据集。为应对这一挑战，我们的研究提出了一种方法，利用先进的对比度增强技术从现有的LWIR数据中合成类似SWIR的结构/对比度线索图像（不声称光谱再现）。然后我们提出了一个多模态融合框架，集成合成SWIR、LWIR和RGB模态，采用具有模态特定编码器和softmax门控融合头的优化编码器-解码器神经网络架构。在公共RGB-LWIR基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和额外的私有真实RGB-MWIR-SWIR数据集上的全面实验表明，我们的合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能。我们还添加了公平的三模态基线（LP、LatLRR、GFF）和U2Fusion/SwinFusion的级联三模态变体，采用统一协议。结果突显了在监控和自主系统中实际应用的巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在不良能见度条件下增强场景理解的问题，这对监控系统和自主导航系统至关重要。传统RGB和热红外成像融合在恶劣天气（如雾、烟、低光）下难以提供全面场景信息，而短波红外（SWIR）虽能穿透大气干扰并更好区分材料，但因公开SWIR数据集稀缺限制了其应用。这一问题在自动驾驶、安防监控等领域尤为重要，因为系统需要在各种环境条件下保持可靠的环境感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到RGB和热红外融合的局限性，注意到SWIR的优势但受限于数据集稀缺。他们借鉴了之前自己的LightFusion工作（使用灰度作为第三模态），但发现灰度无法充分体现SWIR优势。作者还参考了传统多尺度变换技术、深度学习方法（自编码器、CNN、GAN）以及红外-可见光图像融合（IVIF）领域的研究。基于这些思考，他们设计了一种使用对比度受限自适应直方图均衡化（CLAHE）从LWIR合成SWIR的方法，并开发了三模态融合框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过合成SWIR图像解决数据稀缺问题，并融合RGB、热红外和合成SWIR三种模态以提升场景理解能力。整体流程包括：1) 数据预处理（统一分辨率）；2) 使用CLAHE技术从LWIR生成合成SWIR图像；3) 使用三个模态特定编码器（RGB、MWIR和合成SWIR）独立提取特征，每个编码器采用轻量级梯度残块（Light-GRLB）；4) 通过softmax门控融合头整合多模态特征；5) 使用解码器重建高质量融合图像；6) 采用语义损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 使用CLAHE从LWIR合成SWIR图像解决数据稀缺问题；2) 设计三模态融合框架（RGB、LWIR/MWIR和合成SWIR）；3) 开发轻量级梯度残块（Light-GRLB）进行高效特征提取；4) 为每种模态使用独立编码器确保精确特征提取；5) 采用softmax门控融合头加权整合多模态特征。相比之前工作（如作者自己的LightFusion），不同之处在于使用合成SWIR替代灰度作为第三模态，采用专门设计的Light-GRLB而非标准卷积层，以及引入三重语义一致性损失函数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的多模态图像融合方法，通过合成生成短波红外图像并与RGB和热红外图像融合，显著提升了在不良能见度条件下的场景理解能力，同时保持了实时处理的效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing scene understanding in adverse visibility conditions remains acritical challenge for surveillance and autonomous navigation systems.Conventional imaging modalities, such as RGB and thermal infrared (MWIR /LWIR), when fused, often struggle to deliver comprehensive scene information,particularly under conditions of atmospheric interference or inadequateillumination. To address these limitations, Short-Wave Infrared (SWIR) imaginghas emerged as a promising modality due to its ability to penetrate atmosphericdisturbances and differentiate materials with improved clarity. However, theadvancement and widespread implementation of SWIR-based systems facesignificant hurdles, primarily due to the scarcity of publicly accessible SWIRdatasets. In response to this challenge, our research introduces an approach tosynthetically generate SWIR-like structural/contrast cues (without claimingspectral reproduction) images from existing LWIR data using advanced contrastenhancement techniques. We then propose a multimodal fusion frameworkintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimizedencoder-decoder neural network architecture with modality-specific encoders anda softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIRbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private realRGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusionframework improves fused-image quality (contrast, edge definition, structuralfidelity) while maintaining real-time performance. We also add fair trimodalbaselines (LP, LatLRR, GFF) and cascaded trimodal variants ofU2Fusion/SwinFusion under a unified protocol. The outcomes highlightsubstantial potential for real-world applications in surveillance andautonomous systems.</description>
      <author>example@mail.com (Muhammad Ishfaq Hussain, Ma Van Linh, Zubia Naz, Unse Fatima, Yeongmin Ko, Moongu Jeon)</author>
      <guid isPermaLink="false">2510.13404v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.13375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVLA是一种简单而有效的VLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力，但在需要精确空间推理的任务上表现不佳，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力造成的。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型在需要精确空间推理的任务上表现不佳的问题，提高模型的空间理解和推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出DepthVLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成端到端模型。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。&lt;h4&gt;结论&lt;/h4&gt;DepthVLA通过明确整合空间感知能力，显著提高了VLA模型在需要精确空间推理任务上的表现，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力。然而，在需要精确空间推理的任务上，它们的性能会下降，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力。现有的VLA依赖于大量的动作数据预训练来将VLMs定位在3D空间中，这降低了训练效率，并且仍然不足以进行准确的空间理解。在这项工作中，我们提出了DepthVLA，一种简单而有效的VLA架构，通过预训练的深度预测模块明确地整合了空间感知能力。DepthVLA采用混合变压器设计，统一了VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。我们的代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决Vision-Language-Action(VLA)模型在需要精确空间推理的任务上性能下降的问题。这个问题很重要，因为机器人需要精确的空间感知能力来完成精细操作，如抓取小物体、执行精确操作或避免碰撞。现有的VLA模型依赖大量动作数据预训练来将VLMs嵌入3D空间，这降低了训练效率，且仍无法满足精确空间理解的需求，限制了机器人在实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有VLA模型在空间推理方面的局限性，以及现有方法（如大量动作数据预训练或生成世界模型）的不足来设计新方法。他们借鉴了π0的mixture-of-transformers(MoT)设计，并利用3D感知领域的最新进展，特别是Depth Anything V2作为深度专家的基础。作者提出通过预训练的深度预测模块显式整合空间感知能力，采用混合transformers架构统一VLM、深度专家和动作专家，并使用块状掩码保持预训练模块的学习能力，同时允许每个组件在不同数据集上分别预训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的深度预测模块显式整合空间推理能力到VLA模型中，利用混合transformers架构统一三个专家（VLM、深度专家和动作专家），在保持预训练知识的同时融合语义和空间线索以生成精确动作。整体实现流程包括：1)模型架构设计，包含VLM专家（编码图像和指令）、深度专家（处理图像推断几何信息）和动作专家（生成连续动作）；2)首先在多样化3D数据集上预训练深度专家；3)然后在具身动作数据上训练整个DepthVLA模型，使用模仿学习目标和流动匹配损失；4)在推理过程中，三个专家并行处理输入，共享注意力机制，动作专家基于融合的特征生成连续动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DepthVLA架构，首次将预训练的深度预测专家集成到VLA框架中；2)按专家预训练策略，允许每个专家在不同数据集上分别预训练；3)在所有中间层执行空间推理，提供更丰富的几何特征；4)端到端联合优化空间推理和动作生成。与之前工作的不同之处在于：相比现有VLA模型，不依赖大量动作数据预训练；相比SpatialVLA，深度专家是端到端优化的；相比生成世界模型，明确编码当前场景的3D知识且无高延迟；相比CoT推理方法，避免了自回归生成空间令牌的高延迟问题，推理时间仅增加20毫秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVLA通过集成预训练的深度专家到混合transformers框架中，显著提升了机器人在需要精确空间推理任务上的性能，同时保持了高效的训练和推理速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have recently shown impressivegeneralization and language-guided manipulation capabilities. However, theirperformance degrades on tasks requiring precise spatial reasoning due tolimited spatial reasoning inherited from Vision-Language Models (VLMs).Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3Dspace, which reduces training efficiency and is still insufficient for accuratespatial understanding. In this work, we present DepthVLA, a simple yeteffective VLA architecture that explicitly incorporates spatial awarenessthrough a pretrained depth prediction module. DepthVLA adopts amixture-of-transformers design that unifies a VLM, a depth transformer, and anaction expert with fully shared attentions, forming an end-to-end model withenhanced spatial reasoning. Extensive evaluations in both real-world andsimulated environments show that DepthVLA outperforms state-of-the-artapproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.Our code will be made publicly available.</description>
      <author>example@mail.com (Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao)</author>
      <guid isPermaLink="false">2510.13375v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding</title>
      <link>http://arxiv.org/abs/2510.13243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures, 10 tables, data and code available&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlyAwareV2是一个新的多模态数据集，包含真实和合成的无人机图像，专为城市场景理解任务设计，解决了真实数据收集和标注的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市环境中无人机应用的计算机视觉算法开发严重依赖于大规模、准确标注的数据集，但收集和标注真实世界无人机数据极其困难且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决真实数据收集和标注的局限性，提供一个包含真实和合成无人机图像的多模态数据集，用于城市场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;基于SynDrone和FlyAware数据集开发FlyAwareV2，引入多模态数据(RGB、深度、语义标签)覆盖不同环境条件；通过最先进单目深度估计计算真实样本深度图；提供RGB和多模态语义分割基准；研究合成到真实域适应以评估模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;FlyAwareV2具有丰富的标注和环境多样性，为基于无人机的3D城市场景理解研究提供了宝贵资源。&lt;h4&gt;结论&lt;/h4&gt;FlyAwareV2通过其丰富的标注集和环境多样性，为基于无人机的3D城市场景理解研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;针对城市环境中无人机应用开发的计算机视觉算法严重依赖于具有准确标注的大规模数据集的可用性。然而，收集和标注真实世界的无人机数据极其具有挑战性和成本高昂。为了解决这一局限性，我们提出了FlyAwareV2，这是一个新颖的多模态数据集，包含专为城市场景理解任务定制的真实和合成无人机图像。基于最近引入的SynDrone和FlyAware数据集，FlyAwareV2引入了几个新的关键贡献：1)跨不同环境条件的多模态数据(RGB、深度、语义标签)，包括变化的天气和白天时间；2)通过最先进的单目深度估计计算的真实样本深度图；3)基于标准架构的RGB和多模态语义分割基准；4)关于合成到真实域适应的研究，以评估在合成数据上训练的模型的泛化能力。凭借其丰富的标注集和环境多样性，FlyAwareV2为基于无人机的3D城市场景理解研究提供了宝贵的资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of computer vision algorithms for Unmanned Aerial Vehicle(UAV) applications in urban environments heavily relies on the availability oflarge-scale datasets with accurate annotations. However, collecting andannotating real-world UAV data is extremely challenging and costly. To addressthis limitation, we present FlyAwareV2, a novel multimodal dataset encompassingboth real and synthetic UAV imagery tailored for urban scene understandingtasks. Building upon the recently introduced SynDrone and FlyAware datasets,FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,depth, semantic labels) across diverse environmental conditions includingvarying weather and daytime; 2) Depth maps for real samples computed viastate-of-the-art monocular depth estimation; 3) Benchmarks for RGB andmultimodal semantic segmentation on standard architectures; 4) Studies onsynthetic-to-real domain adaptation to assess the generalization capabilitiesof models trained on the synthetic data. With its rich set of annotations andenvironmental diversity, FlyAwareV2 provides a valuable resource for researchon UAV-based 3D urban scene understanding.</description>
      <author>example@mail.com (Francesco Barbato, Matteo Caligiuri, Pietro Zanuttigh)</author>
      <guid isPermaLink="false">2510.13243v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>True Self-Supervised Novel View Synthesis is Transferable</title>
      <link>http://arxiv.org/abs/2510.13063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了XFactor，第一个无需几何信息且能实现真正新颖视图合成的自监督模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;背景&lt;/h4&gt;先前关于自监督新颖视图合成的工作分析表明，它们预测的姿态不具备可转移性——相同姿态在不同3D场景中会导致不同的相机轨迹。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现真正新颖视图合成的模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;方法&lt;/h4&gt;XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理，使用不受约束的潜在姿态变量，无需任何3D归纳偏置或多视图几何概念。&lt;h4&gt;主要发现&lt;/h4&gt;XFactor实现了姿态表示的可转移性；引入了一种新的可转移性量化指标；大规模实验表明XFactor显著优于之前无需姿态的NVS变换器；探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;结论&lt;/h4&gt;XFactor是第一个无需几何信息且能实现真正新颖视图合成的自监督模型，通过结合成对姿态估计和输入输出增强方案，成功实现了姿态表示的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们确定判断一个模型是否真正具备新颖视图合成(NVS)能力的关键标准是可转移性：即从一段视频序列中提取的任何姿态表示是否可用于在另一场景中重新渲染相同的相机轨迹。我们分析了之前关于自监督NVS的工作，发现它们预测的姿态不具备可转移性：相同的姿态集在不同3D场景中会导致不同的相机轨迹。在这里，我们提出了XFactor，这是第一个无需几何信息且能实现真正NVS的自监督模型。XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理。值得注意的是，我们证明XFactor使用不受约束的潜在姿态变量实现了可转移性，无需任何3D归纳偏置或多视图几何概念——例如将姿态显式参数化为SE(3)的元素。我们引入了一种新的量化可转移性的指标，并通过大规模实验证明XFactor显著优于之前无需姿态的NVS变换器，并通过探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自监督新视角合成(NVS)的可转移性问题，即能否从一个视频序列中提取的姿态表示用于重新渲染另一个视频序列中的相同相机轨迹。这个问题很重要，因为真正的NVS应该允许用户控制视角，相同的相机姿态应该总是渲染相同的视角。如果模型无法做到这一点，它就不是真正的NVS模型，而只是帧插值器，限制了用户在任意场景中定义想要渲染的视图的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定可转移性是NVS的关键标准，然后分析现有自监督方法发现它们预测的姿态不能跨场景转移。作者提出两个关键见解：1)通过从必须外推的双视图模型开始训练来防止插值；2)将可转移性明确为训练目标，使用保持相机姿态但最小化像素内容重叠的增强方案。作者借鉴了RUST的无几何方法和CroCo的单目渲染思想，但通过创新的训练目标和架构设计解决了可转移性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是XFactor模型，通过成对姿态估计和输入输出的简单增强方案相结合，解耦相机姿态和场景内容，实现无几何约束的可转移NVS。整体流程：1)训练立体-单目模型(只用一对图像)，消除插值路径；2)应用可转移性目标训练，确保一个序列的姿态能用于渲染另一序列；3)使用保持相机姿态的增强方案(如逆掩码)生成像素差异大的图像对；4)通过二次训练将立体模型扩展为多视图模型，支持更复杂的场景渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出可转移性作为真正NVS的标准并引入TPS指标；2)识别现有方法实际是插值而非推理视角；3)提出促进可转移性的训练目标和增强策略；4)提出XFactor，首个完全自监督的可转移NVS模型；5)通过大规模实验验证有效性。相比RayZer(使用SE(3)参数化但降低可转移性)和RUST(仍受插值偏差影响)，XFactor不需要任何几何先验，实现了真正的跨场景姿态控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XFactor提出了第一个完全自监督且无几何的新视角合成模型，通过可转移性训练目标实现了真正的相机姿态控制，使相同的相机姿态能够在不同场景间产生一致的视角渲染。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we identify that the key criterion for determining whether amodel is truly capable of novel view synthesis (NVS) is transferability:Whether any pose representation extracted from one video sequence can be usedto re-render the same camera trajectory in another. We analyze prior work onself-supervised NVS and find that their predicted poses do not transfer: Thesame set of poses lead to different camera trajectories in different 3D scenes.Here, we present XFactor, the first geometry-free self-supervised model capableof true NVS. XFactor combines pair-wise pose estimation with a simpleaugmentation scheme of the inputs and outputs that jointly enablesdisentangling camera pose from scene content and facilitates geometricreasoning. Remarkably, we show that XFactor achieves transferability withunconstrained latent pose variables, without any 3D inductive biases orconcepts from multi-view geometry -- such as an explicit parameterization ofposes as elements of SE(3). We introduce a new metric to quantifytransferability, and through large-scale experiments, we demonstrate thatXFactor significantly outperforms prior pose-free NVS transformers, and showthat latent poses are highly correlated with real-world poses through probingexperiments.</description>
      <author>example@mail.com (Thomas W. Mitchel, Hyunwoo Ryu, Vincent Sitzmann)</author>
      <guid isPermaLink="false">2510.13063v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages</title>
      <link>http://arxiv.org/abs/2510.12845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一个新的多语言基准测试VLURes，用于评估视觉语言模型(VLMs)在细粒度视觉和语言理解能力方面的表现，特别是在长文本设置下。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要，但目前的评估主要局限于以英语为中心的基准测试，且图像-文本对仅包含短文本。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在四种语言(英语、日语、斯瓦希里语和乌尔都语)的长文本设置下的细粒度能力，特别是物体识别、场景理解和关系理解等对智能体至关重要的任务。&lt;h4&gt;方法&lt;/h4&gt;开发了包含八个视觉语言任务和一个不相关性任务的多语言基准测试VLURes；从目标语言网页资源收集包含十个不同图像类别和丰富文本上下文的数据集；通过提示VLMs生成回答和理由，并由自动系统和母语人士进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了VLMs在不同语言和任务上的表现差异；表现最好的模型GPT-4o总体准确率为90.8%，比人类表现低6.7%；开源模型与人类表现之间的差距更大。&lt;h4&gt;结论&lt;/h4&gt;VLURes基准测试在开发能够处理多模态视觉推理的智能体方面发挥着关键作用，特别是在低资源语言环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要。然而，对VLMs的评估仍然主要局限于以英语为中心的基准测试，其中图像-文本对仅包含短文本。为了评估VLMs在四种语言下的细粒度能力，特别是在长文本设置下，我们引入了一个新的多语言基准测试VLURes，包含八个视觉语言任务和一个开创性的不相关性任务，用于探测VLMs在英语、日语以及低资源语言斯瓦希里语和乌尔都语中的细粒度视觉和语言理解能力。我们的数据集从目标语言的网页资源中精心策划，包含十个不同的图像类别和丰富的文本上下文，为斯瓦希里语和乌尔都语引入了宝贵的视觉语言资源。通过提示VLMs生成回答和理由，并由自动系统和母语人士评估，我们揭示了VLMs在不同语言和任务上的表现差异，这些任务对智能体至关重要，如物体识别、场景理解和关系理解。我们使用VLURes评估了十个VLMs。表现最好的模型GPT-4o总体准确率达到90.8%，比人类表现低6.7%，尽管开源模型之间的差距更大。这一差距凸显了VLURes在开发能够处理多模态视觉推理的智能体方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) are pivotal for advancing perception inintelligent agents. Yet, evaluation of VLMs remains limited to predominantlyEnglish-centric benchmarks in which the image-text pairs comprise short texts.To evaluate VLM fine-grained abilities, in four languages under long-textsettings, we introduce a novel multilingual benchmark VLURes featuring eightvision-and-language tasks, and a pioneering unrelatedness task, to probe thefine-grained Visual and Linguistic Understanding capabilities of VLMs acrossEnglish, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,curated from web resources in the target language, encompass ten diverse imagecategories and rich textual context, introducing valuable vision-languageresources for Swahili and Urdu. By prompting VLMs to generate responses andrationales, evaluated automatically and by native speakers, we uncoverperformance disparities across languages and tasks critical to intelligentagents, such as object recognition, scene understanding, and relationshipunderstanding. We conducted evaluations of ten VLMs with VLURes. The bestperforming model, GPT-4o, achieves an overall accuracy of 90.8% and lags humanperformance by 6.7%, though the gap is larger for open-source models. The gaphighlights VLURes' critical role in developing intelligent agents to tacklemulti-modal visual reasoning.</description>
      <author>example@mail.com (Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka)</author>
      <guid isPermaLink="false">2510.12845v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vincentgu2000.github.io/u0project/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为USIM的水下机器人模拟数据集和一个名为U0的VLA模型，它们共同解决了水下环境中机器人操作面临的挑战，特别是在数据稀缺的情况下，通过提供大规模高质量数据集和有效的多任务学习方法。&lt;h4&gt;背景&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，作者引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。&lt;h4&gt;方法&lt;/h4&gt;USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，作者提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，我们提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决水下机器人自主执行多任务的困难问题，原因是水下环境存在复杂流体动力学、有限可见性和受限通信等挑战，同时大规模高质量水下数据集稀缺。这个问题很重要，因为水下环境覆盖地球71%的面积，涉及海洋生态调查、资源开发、管道检查等多种应用，而水下操作对人类来说危险且困难，自主水下机器人能大幅提高效率和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了水下机器人面临的特殊挑战，然后选择使用仿真环境解决真实数据收集成本高的问题。他们基于现有Isaac-GR00T N1.5模型进行改进，而非从头训练，并整合了双目视觉和其他传感器模态。借鉴了Stonefish模拟器构建环境、ROS框架进行数据收集、Vision-Language Model和Diffusion Transformer架构，以及PID控制器和MoveIt进行机械手控制等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过仿真环境构建大规模水下VLA数据集，开发适应水下环境的VLA模型，建立可扩展的数据到任务框架。流程包括：1)用Stonefish模拟器构建9种水下场景和BlueROV2机器人模型；2)收集20个任务的数据，共561K帧和15.6小时交互数据；3)基于Isaac-GR00T N1.5开发U0模型，整合多模态传感器数据和CAP感知增强模块；4)通过开环离线评估和闭环在线测试验证模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个大规模水下多任务VLA数据集USIM，覆盖20个任务和9种场景；2)首个专为水下机器人设计的VLA模型U0，整合多模态传感器数据和CAP模块；3)使用以机器人为中心的坐标系表示目标位置。相比之前工作，USIM是首个多任务VLA数据集，而现有数据集多为特定任务；U0是首个专门针对水下环境的VLA模型，考虑了水下视觉退化和特殊运动特性，能处理多种任务而非单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务Vision-Language-Action数据集USIM和开发专门的水下机器人通用模型U0，解决了水下机器人高质量数据稀缺和通用任务执行能力不足的问题，为构建可扩展的水下智能机器人框架奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng Wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v3</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</title>
      <link>http://arxiv.org/abs/2510.13740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Third Learning on Graphs  Conference (LoG 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的图构建方法Logarithmic Scalable Graph Construction (LSGC)和混合CNN-GNN模型LogViG，用于解决视觉图神经网络在大图像上计算成本高的问题，并通过引入高分辨率分支和多尺度特征融合提升了性能。&lt;h4&gt;背景&lt;/h4&gt;Vision graph neural networks (ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案在视觉任务中显示出潜力，但常见的图构建方法如k近邻(KNN)在大图像上计算成本高，而现有的Sparse Vision Graph Attention (SVGA)方法存在固定步长导致的过度压缩和错过多连接的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图构建方法，通过限制长距离链接的数量来提高视觉图神经网络的性能，同时降低计算复杂度，并构建一个结合CNN和GNN优势的混合模型。&lt;h4&gt;方法&lt;/h4&gt;提出Logarithmic Scalable Graph Construction (LSGC)方法来增强性能，并设计了LogViG这一新型混合CNN-GNN模型；同时引入高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。&lt;h4&gt;主要发现&lt;/h4&gt;LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构；最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，比Vision GNN高1.7%，参数减少24.3%，GMACs减少35.3%。&lt;h4&gt;结论&lt;/h4&gt;通过提出的LSGC方法在ViG中利用长距离链接可以超过当前最先进ViG的性能，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉图神经网络(ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案，在视觉任务中显示出前景；然而，常见的图构建方法如k近邻(KNN)在大图像上可能计算成本高昂。虽然Sparse Vision Graph Attention (SVGA)等方法显示出前景，但SVGA的固定步长可能导致过度压缩和错过多个连接，无法从长距离链接中获取相同信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建(LSGC)，通过限制长距离链接的数量来增强性能。为此，我们提出了LogViG，一种利用LSGC的新型混合CNN-GNN模型。此外，受多尺度和高分辨率架构成功的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。大量实验表明，LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构。我们的最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，标准差为0.2%，比Vision GNN高1.7%的平均准确率，参数减少24.3%，GMACs减少35.3%。我们的工作表明，通过我们提出的LSGC在ViG中利用长距离链接可以超过当前最先进ViG的性能。代码可在https://github.com/mmunir127/LogViG-Official获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision graph neural networks (ViG) have demonstrated promise in vision tasksas a competitive alternative to conventional convolutional neural nets (CNN)and transformers (ViTs); however, common graph construction methods, such ask-nearest neighbor (KNN), can be expensive on larger images. While methods suchas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed stepscale can lead to over-squashing and missing multiple connections to gain thesame information that could be gained from a long-range link. Through thisobservation, we propose a new graph construction method, Logarithmic ScalableGraph Construction (LSGC) to enhance performance by limiting the number oflong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN modelthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale andhigh-resolution architectures, we introduce and apply a high-resolution branchand fuse features between our high-resolution and low-resolution branches for amulti-scale high-resolution Vision GNN network. Extensive experiments show thatLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,GMACs, and parameters on image classification and semantic segmentation tasks.Our smallest model, Ti-LogViG, achieves an average top-1 accuracy onImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher averageaccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%reduction in GMACs. Our work shows that leveraging long-range links in graphconstruction for ViGs through our proposed LSGC can exceed the performance ofcurrent state-of-the-art ViGs. Code is available athttps://github.com/mmunir127/LogViG-Official.</description>
      <author>example@mail.com (Mustafa Munir, Alex Zhang, Radu Marculescu)</author>
      <guid isPermaLink="false">2510.13740v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Message Passing on the Edge: Towards Scalable and Expressive GNNs</title>
      <link>http://arxiv.org/abs/2510.13615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EB-1WL和EB-GNN，一种基于边的颜色细化测试和相应的图神经网络架构。该架构受经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。研究表明，EB-1WL比1-WL具有更强的表达能力，同时保持了接近线性的时间和内存复杂度。实验证明，EB-GNN是一种高效的通用架构，显著优于简单MPNN，且与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNN)领域存在对更具表达力且计算效率高的架构的需求，之前的提案在计算效率上存在问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于边的颜色细化测试(EB-1WL)和相应的GNN架构(EB-GNN)，以提高表达能力同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出EB-1WL（基于边的颜色细化测试）和EB-GNN架构，受Chiba和Nishizeki的经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。&lt;h4&gt;主要发现&lt;/h4&gt;EB-1WL比1-WL具有更强的表达能力；提供了基于一阶逻辑的EB-1WL完整逻辑表征和基于同态计数的匹配区分度结果；EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存；EB-GNN显著优于简单MPNN，与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;EB-GNN是一种高效、通用的GNN架构，在表达能力与计算效率之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了EB-1WL，一种基于边的颜色细化测试，以及相应的GNN架构EB-GNN。我们的架构受到Chiba和Nishizeki经典三角形计数算法的启发，并在消息传递过程中明确使用三角形。我们取得了以下结果：(1) EB-1WL比1-WL具有显著更强的表达能力。此外，我们基于一阶逻辑提供了EB-1WL的完整逻辑表征，并基于同态计数提供了匹配的区分度结果。(2) 与之前提出的更具表达力的GNN架构的重要区别在于，EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存。(3) 从经验上看，我们表明EB-GNN是一种高效的通用架构：它显著优于简单的MPNN，并且在计算效率方面远高于任务专用的GNN的同时，与它们保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose EB-1WL, an edge-based color-refinement test, and a correspondingGNN architecture, EB-GNN. Our architecture is inspired by a classic trianglecounting algorithm by Chiba and Nishizeki, and explicitly uses triangles duringmessage passing. We achieve the following results: (1)~EB-1WL is significantlymore expressive than 1-WL. Further, we provide a complete logicalcharacterization of EB-1WL based on first-order logic, and matchingdistinguishability results based on homomorphism counting. (2)~In an importantdistinction from previous proposals for more expressive GNN architectures,EB-1WL and EB-GNN require near-linear time and memory on practical graphlearning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficientgeneral-purpose architecture: It substantially outperforms simple MPNNs, andremains competitive with task-specialized GNNs while being significantly morecomputationally efficient.</description>
      <author>example@mail.com (Pablo Barceló, Fabian Jogl, Alexander Kozachinskiy, Matthias Lanzinger, Stefan Neumann, Cristóbal Rojas)</author>
      <guid isPermaLink="false">2510.13615v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</title>
      <link>http://arxiv.org/abs/2510.13401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Workshop on New Approaches for Addressing the Computing  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为F-BFQ的灵活块浮点量化加速器，用于提高BFP量化大语言模型在边缘设备上的推理效率，能够在两种BFP量化变体间动态切换而无需重新配置。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)在日常任务中应用广泛，通过llama.cpp等推理框架的优化（如KV缓存和量化），使在边缘设备上部署LLMs变得更加可行。量化技术是使LLMs在资源受限的边缘设备上运行的关键，llama.cpp采用块浮点(BFP)量化来减小模型权重和输入张量的位宽、内存占用和计算需求。通常，LLMs在各层采用混合BFP量化以减少精度损失。&lt;h4&gt;目的&lt;/h4&gt;为了高效加速BFP量化LLMs的各层计算，需要开发一种专门的加速器，使其能够支持不同的BFP量化变体而无需重新配置。&lt;h4&gt;方法&lt;/h4&gt;作者提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，该加速器可以动态切换两种BFP量化变体并执行矩阵乘法(MatMul)操作。&lt;h4&gt;主要发现&lt;/h4&gt;在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;h4&gt;结论&lt;/h4&gt;F-BFQ加速器有效地提高了BFP量化LLMs在边缘设备上的推理效率，通过支持多种BFP量化变体的动态切换，无需重新配置即可加速模型各层的计算。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)在日常任务中日益突出，从改善语音转文本翻译到生成最新视频游戏的额外帧等。借助llama.cpp等支持KV缓存和量化等优化的LLM推理框架，现在在边缘设备上部署LLMs比以往任何时候都更容易。量化是使LLMs在资源受限的边缘设备上运行的基本技术，llama.cpp利用块浮点(BFP)量化大幅减小权重和输入张量的位宽、内存占用以及运行LLMs所需的计算能力。LLMs通常在模型各层采用混合BFP量化，以减少量化导致的模型精度损失。因此，为了高效加速BFP量化LLMs的各层，专门的加速器需要支持不同的BFP变体而无需重新配置。为解决这一问题，我们提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，它可以在两种BFP量化变体间动态切换并执行矩阵乘法(MatMul)操作。我们在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，同时实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have become increasingly prominent for dailytasks, from improving sound-totext translation to generating additional framesfor the latest video games. With the help of LLM inference frameworks, such asllama.cpp, which support optimizations such as KV-caching and quantization, itis now easier than ever to deploy LLMs on edge devices. Quantization isfundamental to enable LLMs on resource-constrained edge devices, and llama.cpputilizes block floating point (BFP) quantization to drastically reduce the bitwidth of weights and input tensors, the memory footprint, and the computationalpower required to run LLMs. LLMs are typically quantized with mixed BFPquantization across the model layers to reduce the loss of model accuracy dueto quantization. Therefore, to efficiently accelerate across the layers ofBFP-quantized LLMs, specialized accelerators need to support different BFPvariants without reconfiguration. To address this issue, we propose a FlexibleBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamicallyswitch between two BFP quantization variants and perform matrix multiplication(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMDKria board, reduces inference time by 1.4x on average over the Arm NEON-basedCPU execution across three BFP quantized LLMs while achieving 5.2 tokens persecond (~3.9 words per second).</description>
      <author>example@mail.com (Jude Haris, José Cano)</author>
      <guid isPermaLink="false">2510.13401v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.13391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 8 figures, 11-page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种使用图神经网络(GNN)来近似计算网络流游戏中Banzhaf值的方法，解决了传统方法在处理大规模系统时的计算效率问题。&lt;h4&gt;背景&lt;/h4&gt;Banzhaf值用于量化多智能体系统中智能体的影响力，应用领域广泛，但精确计算对于超过约20个智能体的系统由于指数级复杂度而不可行；蒙特卡洛采样方法虽然可提供估计但存在样本复杂度高且无法跨网络配置泛化的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来近似计算网络流游戏中的Banzhaf值，使其能够处理大规模和动态系统，并具备良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)将Banzhaf值计算问题框架化为图级预测任务，直接从网络拓扑和控制结构中学习智能体影响力的模式；比较了三种GNN架构(GAT、GINE和EdgeConv)在大型合成数据集上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;训练后的GNN模型实现了高保真的Banzhaf值近似，计算速度比传统方法快数量级；模型展示了强大的零样本泛化能力，能够在未见过的网络结构上准确预测Banzhaf值而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;图神经网络可以作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;h4&gt;翻译&lt;/h4&gt;计算网络流游戏中的Banzhaf值对于量化多智能体系统中的智能体影响力至关重要，应用范围从网络安全到基础设施规划。然而，对于超过约20个智能体的系统，由于指数级复杂度，精确计算是不可行的。虽然蒙特卡洛采样方法可以提供统计估计，但它们存在样本复杂度高的问题，并且无法在不同网络配置之间转移知识，使其对于大规模或动态系统不切实际。我们提出了一种基于学习的新方法，使用图神经网络(GNN)来近似基数网络流游戏中的Banzhaf值。通过将问题框架化为图级预测任务，我们的方法直接从网络拓扑和控制结构中学习智能体影响力的可泛化模式。我们进行了全面的实证研究，比较了三种最先进的GNN架构-图注意力网络(GAT)、具有边特征的图同构网络(GINE)和EdgeConv-在每个配置200,000个图的大规模合成数据集上的性能，数据集在大小(20-100个节点)、智能体数量(5-20)和边概率(0.5-1.0)上有所不同。我们的结果表明，训练后的GNN模型实现了高保真的Banzhaf值近似，与精确和基于采样的方法相比，速度提高了数量级。最重要的是，我们展示了强大的零样本泛化能力：在特定大小和拓扑的图上训练的模型，能够准确预测具有完全不同结构特性的全新网络的Banzhaf值，而无需重新训练。这项工作确立了GNN作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing the Banzhaf value in network flow games is fundamental forquantifying agent influence in multi-agent systems, with applications rangingfrom cybersecurity to infrastructure planning. However, exact computation isintractable for systems with more than $\sim20$ agents due to exponentialcomplexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods providestatistical estimates, they suffer from high sample complexity and cannottransfer knowledge across different network configurations, making themimpractical for large-scale or dynamic systems. We present a novellearning-based approach using Graph Neural Networks (GNNs) to approximateBanzhaf values in cardinal network flow games. By framing the problem as agraph-level prediction task, our method learns generalisable patterns of agentinfluence directly from network topology and control structure. We conduct acomprehensive empirical study comparing three state-of-the-art GNNarchitectures-Graph Attention Networks (GAT), Graph Isomorphism Networks withEdge features (GINE), and EdgeConv-on a large-scale synthetic dataset of200,000 graphs per configuration, varying in size (20-100 nodes), agent count(5-20), and edge probability (0.5-1.0). Our results demonstrate that trainedGNN models achieve high-fidelity Banzhaf value approximation withorder-of-magnitude speedups compared to exact and sampling-based methods. Mostsignificantly, we show strong zero-shot generalisation: models trained ongraphs of a specific size and topology accurately predict Banzhaf values forentirely new networks with different structural properties, without requiringretraining. This work establishes GNNs as a practical tool for scalablecooperative game-theoretic analysis of complex networked systems.</description>
      <author>example@mail.com (Benjamin Kempinski, Tal Kachman)</author>
      <guid isPermaLink="false">2510.13391v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective</title>
      <link>http://arxiv.org/abs/2510.13254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ECML-PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FracNet是一种频率感知对比图网络，通过频谱分析将图分解为高频和低频成分，解决了图神经网络在领域适应中的挑战，通过对比学习框架改善了领域适应的模糊边界问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来更好地理解和处理领域适应中的分布变化，特别是通过频谱分析来识别和利用不同频率成分中的模式，以提高图神经网络在领域适应中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。通过与对比学习框架集成，改善了领域适应的模糊边界问题。&lt;h4&gt;主要发现&lt;/h4&gt;领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。通过分解图的不同频率成分，可以更有效地进行领域适应。&lt;h4&gt;结论&lt;/h4&gt;FracNet通过频谱分析和对比学习显著提高了领域适应的性能，实验证明其优于最先进的方法。研究不仅提供了实际应用价值，还提供了严格的理论证明来证明FracNet的优越性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。我们的关键见解是，领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。因此，我们提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。此外，通过与对比学习框架集成，改善了领域适应的模糊边界问题。除了实际应用意义外，我们还提供了严格的理论证明来证明FracNet的优越性。大量实验进一步证明了其优于最先进方法的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-06106-5_26&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in variousdomains, yet they often struggle with domain adaptation due to significantstructural distribution shifts and insufficient exploration of transferablepatterns. One of the main reasons behind this is that traditional approaches donot treat global and local patterns discriminatingly so that some local detailsin the graph may be violated after multi-layer GNN. Our key insight is thatdomain shifts can be better understood through spectral analysis, wherelow-frequency components often encode domain-invariant global patterns, andhigh-frequency components capture domain-specific local details. As such, wepropose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with twosynergic modules to decompose the original graph into high-frequency andlow-frequency components and perform frequency-aware domain adaption. Moreover,the blurring boundary problem of domain adaptation is improved by integratingwith a contrastive learning framework. Besides the practical implication, wealso provide rigorous theoretical proof to demonstrate the superiority ofFracNet. Extensive experiments further demonstrate significant improvementsover state-of-the-art approaches.</description>
      <author>example@mail.com (Haoyu Zhang, Yuxuan Cheng, Wenqi Fan, Yulong Chen, Yifan Zhang)</author>
      <guid isPermaLink="false">2510.13254v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universally Invariant Learning in Equivariant GNNs</title>
      <link>http://arxiv.org/abs/2510.13169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种理论上健全的框架，用于构建高效且实用的完备等变图神经网络(GNNs)，通过两个关键组件实现：完备的标量函数和满秩的可转向基集。&lt;h4&gt;背景&lt;/h4&gt;等变图神经网络在各种应用中显示出显著成功。为了实现完备性（即在等变函数空间上的通用逼近性质），网络必须有效捕捉不同节点之间复杂的多体相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论上健全的框架，用于构建高效且实用的完备等变GNN，解决现有方法计算成本高且没有多项式时间解决方案的问题。&lt;h4&gt;方法&lt;/h4&gt;证明完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。基于这一发现，提出了基于EGNN和TFN两种常见模型的高效算法。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果表明，该模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为构建高效且实用的完备等变GNN提供了理论基础，解决了现有方法的计算效率问题。&lt;h4&gt;翻译&lt;/h4&gt;等变图神经网络在各种应用中已显示出显著成功。为了实现完备性——即在等变函数空间上的通用逼近性质——网络必须有效捕捉不同节点之间复杂的多体相互作用。现有方法通过更深层次的结构、增强的体阶数或增加可转向特征的维度来实现这一目标，通常计算成本高且没有多项式时间解决方案。在这项工作中，我们提出了一个理论上健全的框架，用于构建高效且实用的完备等变GNN。我们证明，完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。利用这一发现，我们提出了基于两种常见模型(EGNN和TFN)的构建完备等变GNN的高效算法。实证结果表明，我们的模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决等变图神经网络(GNN)的完备性问题，即模型能否近似任何连续函数的能力。这个问题在科学计算和物理模拟中至关重要，因为它决定了模型能否准确捕捉复杂几何数据（如分子结构、蛋白质等）中的多体相互作用。现有方法需要通过增加网络深度、提高阶数或增加特征维度来获得更好的表达能力，但这会导致计算成本大幅增加，限制了等变GNN在实际应用中的使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先重新审视了现有等变GNN模型（如EGNN、TFN、MACE等），将它们统一为基于多体高阶基函数扩展的形式，从而识别出当前方法的局限性。然后，作者从输出空间角度提出新的动态方法，借鉴了几何同构问题的研究成果，提出完全等变GNN需要两个关键组件：几何图的规范形式和满秩基集。作者还借鉴了FastEGNN等工作中关于虚拟节点学习的思想，并基于四点定位原理提出了多项式时间算法来构建规范形式。此外，作者借鉴了不对称图上的着色理论，证明了在非对称图上总能构建满秩基集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件构建完全等变GNN：几何图的规范形式（完全标量函数）和满秩的可引导基集，而不需要通过增加网络深度、阶数或特征维度。实现流程包括：1) 构建几何图的规范形式，对于一般图使用四点定位原理（O(N^6)复杂度），对于非对称图使用E(3)等变函数生成参考点（O(N^2)复杂度）；2) 构建满秩基集，通过节点着色（⊕或⊗方法）使每个节点具有唯一特征；3) 实际模型实现，如EGNNcpl和TFNcpl，通过着色、构建虚拟节点、更新特征和全局操作来实现单层完备模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出新的完备性框架，将完全等变GNN构建转化为规范形式和满秩基集两个组件；2) 提出几何同构问题的多项式时间算法，为一般图提供O(N^6)算法，为非对称图提供O(N^2)算法；3) 证明在非对称几何图上总能构建任意度数的满秩基集；4) 提出EGNN/TFNcpl-global和EGNN/TFNcpl-local两种实际实现。相比之前的工作，传统方法需要通过增加阶数、层数或特征维度来实现完备性，计算成本高且无法保证在有限复杂度下实现完备性，而本文方法通过动态方法在保持低计算复杂度的同时实现了完备性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为Uni-EGNN的高效框架，通过几何图的规范形式和满秩基集两个关键组件，使等变图神经网络在保持低计算复杂度的同时实现了完备性，显著提升了模型在科学计算和物理模拟任务中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Equivariant Graph Neural Networks (GNNs) have demonstrated significantsuccess across various applications. To achieve completeness -- that is, theuniversal approximation property over the space of equivariant functions -- thenetwork must effectively capture the intricate multi-body interactions amongdifferent nodes. Prior methods attain this via deeper architectures, augmentedbody orders, or increased degrees of steerable features, often at highcomputational cost and without polynomial-time solutions. In this work, wepresent a theoretically grounded framework for constructing completeequivariant GNNs that is both efficient and practical. We prove that a completeequivariant GNN can be achieved through two key components: 1) a completescalar function, referred to as the canonical form of the geometric graph; and2) a full-rank steerable basis set. Leveraging this finding, we propose anefficient algorithm for constructing complete equivariant GNNs based on twocommon models: EGNN and TFN. Empirical results demonstrate that our modeldemonstrates superior completeness and excellent performance with only a fewlayers, thereby significantly reducing computational overhead while maintainingstrong practical efficacy.</description>
      <author>example@mail.com (Jiacheng Cen, Anyi Li, Ning Lin, Tingyang Xu, Yu Rong, Deli Zhao, Zihe Wang, Wenbing Huang)</author>
      <guid isPermaLink="false">2510.13169v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering</title>
      <link>http://arxiv.org/abs/2510.12959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种后验流行度去偏置(PPD)方法，用于纠正基于图神经网络的协同过滤中的流行度偏见问题。&lt;h4&gt;背景&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号，但训练数据通常呈现长尾分布，导致模型学习流行度偏见，降低推荐质量。图神经网络虽然有效，但其聚合过程会进一步传播和放大这种偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接对抗GNN邻域聚合过程中传播的流行度偏见的方法，提高推荐的个性化程度和质量。&lt;h4&gt;方法&lt;/h4&gt;提出PPD方法，该方法在预训练嵌入上操作，不需要重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，从而减少偏见同时保留用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法通过修改训练目标解决偏见问题，但无法直接对抗GNN邻域聚合过程中的偏见传播；在聚合过程中对交互应用权重可能缓解问题，但由于训练早期节点表示不稳定，可能导致模型学习扭曲。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，PPD方法在基于GNN的CF的流行度偏见纠正方面优于现有最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号。然而，训练数据通常呈现长尾分布，只有少数项目拥有大部分交互。直接在这种不平衡数据上训练的CF模型容易学习流行度偏见，降低个性化程度，导致次优的推荐质量。图神经网络(GNN)由于其消息传递机制对CF有效，但可以通过聚合过程进一步传播和放大流行度偏见。现有方法通常通过修改训练目标来解决流行度偏见，但未能直接对抗GNN在邻域聚合过程中传播的偏见。在聚合过程中对交互应用权重可以帮助缓解此问题，但由于训练早期阶段节点表示不稳定，可能会扭曲模型学习。在本文中，我们提出了一种后验流行度去偏置(PPD)方法，用于纠正基于GNN的CF中的流行度偏见，并直接在预训练嵌入上操作，无需重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，PPD减少了偏见同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF的流行度偏见纠正方面优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User historical interaction data is the primary signal for learning userpreferences in collaborative filtering (CF). However, the training data oftenexhibits a long-tailed distribution, where only a few items have the majorityof interactions. CF models trained directly on such imbalanced data are proneto learning popularity bias, which reduces personalization and leads tosuboptimal recommendation quality. Graph Neural Networks (GNNs), whileeffective for CF due to their message passing mechanism, can further propagateand amplify popularity bias through their aggregation process. Existingapproaches typically address popularity bias by modifying training objectivesbut fail to directly counteract the bias propagated during GNN's neighborhoodaggregation. Applying weights to interactions during aggregation can helpalleviate this problem, yet it risks distorting model learning due to unstablenode representations in the early stages of training. In this paper, we proposea Post-hoc Popularity Debiasing (PPD) method that corrects for popularity biasin GNN-based CF and operates directly on pre-trained embeddings withoutrequiring retraining. By estimating interaction-level popularity and removingpopularity components from node representations via a popularity directionvector, PPD reduces bias while preserving user preferences. Experimentalresults show that our method outperforms state-of-the-art approaches forpopularity bias correction in GNN-based CF.</description>
      <author>example@mail.com (Md Aminul Islam, Elena Zheleva, Ren Wang)</author>
      <guid isPermaLink="false">2510.12959v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的物理信息图神经网络方法，结合极值分析技术，有效提高了泰国地区降雨预测的准确性，特别是对极端事件的预测能力。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。泰国地区的站点降雨预测面临特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发结合物理信息的图神经网络和极值分析技术，改进泰国地区的站点降雨预测，特别是提高极端事件的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;使用图结构表示站点捕捉时空模式；预处理影响区域降雨的气候指标；提出Attention-LSTM模型，利用地形降水物理公式推导边特征；采用空间季节感知广义帕累托分布方法进行阈值超限映射，解决极值预测问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在大多数地区优于成熟的基线模型，包括易发生极端事件的区域；与最先进方法保持竞争力；相比SEAS5业务预报系统，显著改进了极端事件预测；提供高分辨率地图支持长期水资源管理决策。&lt;h4&gt;结论&lt;/h4&gt;该方法在实际应用中提高了极端事件的预测能力，为长期水资源管理中的决策提供了实用增强。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。本文提出了新颖的物理信息图神经网络结合极值分析技术，以改进泰国地区的站点降雨预测。该模型利用站点图的图结构表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理了可能影响区域降雨的相关气候指标。提出的带有长短期记忆的图注意力网络使用简单地形降水物理公式推导的初始边特征应用注意力机制。嵌入随后由LSTM层处理。为解决极值问题，我们使用新颖的空间季节感知广义帕累托分布方法进行阈值超限映射，这克服了传统机器学习模型的局限性。实验表明，我们的方法在大多数地区都优于成熟的基线模型，包括易发生极端事件的区域，并且与最先进的方法保持强劲竞争力。与业务预报系统SEAS5相比，我们的实际应用改进了极端事件的预测，并提供了实用增强，以支持长期水资源管理中的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producefine-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis</title>
      <link>http://arxiv.org/abs/2510.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为循环自监督扩散(CSS-Diff)的框架，用于从低场MRI合成高质量高场MRI图像，解决了现有方法中存在的临床保真度差距问题。&lt;h4&gt;背景&lt;/h4&gt;低场MRI具有成本低、可及性高、安全性好等优点，但存在分辨率低和信噪比差的问题。从低场MRI合成高质量图像可以减少对昂贵采集的依赖并扩大数据可用性。&lt;h4&gt;目的&lt;/h4&gt;解决从低场MRI合成高场MRI时存在的临床保真度差距，保留解剖保真度，增强细粒度结构细节，弥合图像对比度中的域差距。&lt;h4&gt;方法&lt;/h4&gt;提出循环自监督扩散(CSS-Diff)框架，在循环一致性约束下重新制定基于扩散的合成过程，强制在整个生成过程中保持解剖结构。框架包含两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在跨场合成任务上取得了最先进的性能，包括PSNR、SSIM和LPIPS指标的提升。与原始低场MRI相比，保留了细粒度解剖结构，左脑白质误差从12.1%降至2.1%，皮层从4.2%降至3.7%。&lt;h4&gt;结论&lt;/h4&gt;CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;h4&gt;翻译&lt;/h4&gt;从低场MRI合成高质量图像具有巨大潜力。低场MRI更便宜、更易获取且更安全，但分辨率低且信噪比差。这种合成过程可以减少对昂贵采集的依赖并扩大数据可用性。然而，从低场MRI合成高场MRI仍存在临床保真度差距。需要保留解剖保真度，增强细粒度结构细节，并弥合图像对比度中的域差距。为解决这些问题，我们提出了一个用于从真实低场MRI数据合成高场MRI的循环自监督扩散(CSS-Diff)框架。我们的核心思想是在循环一致性约束下重新制定基于扩散的合成过程。它在整个生成过程中强制保持解剖结构，而不仅仅依赖成对的像素级监督。CSS-Diff框架还进一步整合了两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。在跨场合成任务上的广泛实验证明了我们方法的有效性，取得了最先进的性能。除了像素级保真度外，与原始低场MRI相比，我们的方法还保留了细粒度解剖结构。总之，我们的CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthesizing high-quality images from low-field MRI holds significantpotential. Low-field MRI is cheaper, more accessible, and safer, but suffersfrom low resolution and poor signal-to-noise ratio. This synthesis process canreduce reliance on costly acquisitions and expand data availability. However,synthesizing high-field MRI still suffers from a clinical fidelity gap. Thereis a need to preserve anatomical fidelity, enhance fine-grained structuraldetails, and bridge domain gaps in image contrast. To address these issues, wepropose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework forhigh-field MRI synthesis from real low-field MRI data. Our core idea is toreformulate diffusion-based synthesis under a cycle-consistent constraint. Itenforces anatomical preservation throughout the generative process rather thanjust relying on paired pixel-level supervision. The CSS-Diff framework furtherincorporates two novel processes. The slice-wise gap perception network alignsinter-slice inconsistencies via contrastive learning. The local structurecorrection network enhances local feature restoration throughself-reconstruction of masked and perturbed patches. Extensive experiments oncross-field synthesis tasks demonstrate the effectiveness of our method,achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wisefidelity, our method also preserves fine-grained anatomical structures comparedwith the original low-field MRI (e.g., left cerebral white matter error dropsfrom 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, ourCSS-Diff can synthesize images that are both quantitatively reliable andanatomically consistent.</description>
      <author>example@mail.com (Zhenxuan Zhang, Peiyuan Jing, Zi Wang, Ula Briski, Coraline Beitone, Yue Yang, Yinzhe Wu, Fanwen Wang, Liutao Yang, Jiahao Huang, Zhifan Gao, Zhaolin Chen, Kh Tohidul Islam, Guang Yang, Peter J. Lally)</author>
      <guid isPermaLink="false">2510.13735v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.13675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KnowCoL（Knowledge-guided Contrastive Learning）的框架，用于开放域视觉实体识别，通过结合图像和文本描述，利用维基数据的结构化信息，将视觉和文本输入抽象到概念层面，支持零样本实体识别。&lt;h4&gt;背景&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布，导致任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决开放域视觉实体识别任务中的挑战，包括有限的监督、高视觉歧义性和语义消歧的需要，特别是针对训练过程中未见到的实体。&lt;h4&gt;方法&lt;/h4&gt;提出KnowCoL框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。&lt;h4&gt;主要发现&lt;/h4&gt;在OVEN基准测试上评估显示，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。最小的模型相比最先进的方法在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;h4&gt;结论&lt;/h4&gt;KnowCoL框架有效地解决了开放域视觉实体识别中的挑战，特别是在处理稀有和未见实体方面表现出色，同时模型尺寸显著减小。&lt;h4&gt;翻译&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与具有固定标签集的传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布。这使任务本身具有挑战性，因为监督有限、视觉歧义度高，且需要语义消歧。在这项工作中，我们提出了一个知识引导的对比学习框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。我们在OVEN基准上评估了我们的方法，OVEN是一个大规模开放域视觉识别数据集，以维基数据ID作为标签空间。我们的实验表明，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。与最先进的方法相比，我们的最小模型在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-domain visual entity recognition aims to identify and link entitiesdepicted in images to a vast and evolving set of real-world concepts, such asthose found in Wikidata. Unlike conventional classification tasks with fixedlabel sets, it operates under open-set conditions, where most target entitiesare unseen during training and exhibit long-tail distributions. This makes thetask inherently challenging due to limited supervision, high visual ambiguity,and the need for semantic disambiguation. In this work, we propose aKnowledge-guided Contrastive Learning (KnowCoL) framework that combines bothimages and text descriptions into a shared semantic space grounded bystructured information from Wikidata. By abstracting visual and textual inputsto a conceptual level, the model leverages entity descriptions, typehierarchies, and relational context to support zero-shot entity recognition. Weevaluate our approach on the OVEN benchmark, a large-scale open-domain visualrecognition dataset with Wikidata IDs as the label space. Our experiments showthat using visual, textual, and structured knowledge greatly improves accuracy,especially for rare and unseen entities. Our smallest model improves theaccuracy on unseen entities by 10.5% compared to the state-of-the-art, despitebeing 35 times smaller.</description>
      <author>example@mail.com (Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab)</author>
      <guid isPermaLink="false">2510.13675v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2510.13368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;目的&lt;/h4&gt;提出一种结合对比学习的依赖建模和异常检测方法，解决云服务环境中的异常检测问题&lt;h4&gt;方法&lt;/h4&gt;将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，使用图卷积机制聚合邻域信息实现上下文感知的服务表示，引入对比学习框架构建正负样本对增强正常和异常模式可分性，设计时间一致性约束保持表示稳定性，结合对比损失和时间一致性损失进行整体优化&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上从超参数、环境和数据敏感性角度系统评估了该方法，在精确率、召回率、F1分数和AUC等关键指标上显著优于现有方法，在稀疏标记、监控噪声和流量波动条件下保持鲁棒性&lt;h4&gt;结论&lt;/h4&gt;验证了将依赖建模与对比学习结合的有效性，为云服务异常检测提供了完整的技术解决方案，在复杂环境中表现出强大的适应性和稳定性&lt;h4&gt;翻译&lt;/h4&gt;本文通过提出一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战。该方法将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，并采用图卷积机制聚合邻域信息以实现上下文感知的服务表示。随后引入对比学习框架，构建正负样本对以增强正常和异常模式在表示空间中的可分性。此外，设计了时间一致性约束以保持跨时间步的表示稳定性，减少短期波动和噪声的影响。整体优化结合了对比损失和时间一致性损失，确保在多维度特征下的稳定可靠检测。在公共数据集上从超参数、环境和数据敏感性角度对该方法进行了系统评估。结果表明，在精确率、召回率、F1分数和AUC等关键指标上，所提出的方法显著优于现有方法，同时在稀疏标记、监控噪声和流量波动条件下保持鲁棒性。本研究验证了结合依赖建模与对比学习的有效性，为云服务异常检测提供了完整的技术解决方案，并在复杂环境中表现出强大的适应性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of complex dependencies and diverseanomaly patterns in cloud service environments by proposing a dependencymodeling and anomaly detection method that integrates contrastive learning. Themethod abstracts service interactions into a dependency graph, extractstemporal and structural features through embedding functions, and employs agraph convolution mechanism to aggregate neighborhood information forcontext-aware service representations. A contrastive learning framework is thenintroduced, constructing positive and negative sample pairs to enhance theseparability of normal and abnormal patterns in the representation space.Furthermore, a temporal consistency constraint is designed to maintainrepresentation stability across time steps and reduce the impact of short-termfluctuations and noise. The overall optimization combines contrastive loss andtemporal consistency loss to ensure stable and reliable detection acrossmulti-dimensional features. Experiments on public datasets systematicallyevaluate the method from hyperparameter, environmental, and data sensitivityperspectives. Results show that the proposed approach significantly outperformsexisting methods on key metrics such as Precision, Recall, F1-Score, and AUC,while maintaining robustness under conditions of sparse labeling, monitoringnoise, and traffic fluctuations. This study verifies the effectiveness ofintegrating dependency modeling with contrastive learning, provides a completetechnical solution for cloud service anomaly detection, and demonstrates strongadaptability and stability in complex environments.</description>
      <author>example@mail.com (Yue Xing, Yingnan Deng, Heyao Liu, Ming Wang, Yun Zi, Xiaoxuan Sun)</author>
      <guid isPermaLink="false">2510.13368v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universal Image Restoration Pre-training via Masked Degradation Classification</title>
      <link>http://arxiv.org/abs/2510.13282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种掩码退化分类预训练方法（MaskDCPT），用于图像退化类型分类，从而实现全面的图像恢复预训练。该方法使用图像退化类型作为弱监督，同时利用图像重建增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器，分别用于特征提取、退化类型分类和高质量图像重建。该方法结合了掩码图像建模和对比学习的优势，显著提升了CNN和Transformer在图像恢复任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统预训练方法在图像恢复任务中存在局限性，需要一种能够处理多种退化类型的通用图像恢复方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够分类图像退化类型的预训练方法，实现全面的图像恢复预训练，提高模型在通用图像恢复任务中的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出MaskDCPT方法，使用图像退化类型作为弱监督。构建包含一个编码器和两个解码器的架构：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型；重建解码器重建对应的高质量图像。利用掩码图像建模和对比学习的好处。构建UIR-2.5M数据集，包含250万对恢复样本，覆盖19种退化类型和200多种退化水平。&lt;h4&gt;主要发现&lt;/h4&gt;MaskDCPT显著提高了CNN和Transformer的性能，在5D全一恢复任务中PSNR至少提高3.77分贝，在真实退化场景中PIQE减少34.8%。模型对未见过的退化类型和水平表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MaskDCPT是一种简单而强大的预训练方法，可用于通用图像恢复，能够处理多种退化类型并在各种场景中表现出色。发布的UIR-2.5M数据集、源代码和模型可供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了一种掩码退化分类预训练方法（MaskDCPT），旨在促进输入图像中退化类型的分类，从而实现全面的图像恢复预训练。与传统预训练方法不同，MaskDCPT使用图像的退化类型作为极弱监督，同时利用图像重建来增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型，而重建解码器旨在重建相应的高质量图像。这种设计使预训练能够受益于掩码图像建模和对比学习，从而生成适合恢复任务的通用表示。得益于简单而强大的MaskDCPT，预训练的编码器可用于解决通用图像恢复并取得卓越性能。实施MaskDCPT显著提高了卷积神经网络（CNN）和Transformer的性能，在5D全一恢复任务中PSNR最小提高3.77分贝，在真实退化场景中与基线相比PIQE减少34.8%。它还对以前未见过的退化类型和水平表现出强大的泛化能力。此外，我们整理并发布了UIR-2.5M数据集，包含250万对恢复样本，涵盖19种退化类型和200多种退化水平，包括合成和真实世界数据。该数据集、源代码和模型可在https://github.com/MILab-PKU/MaskDCPT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a Masked Degradation Classification Pre-Training method(MaskDCPT), designed to facilitate the classification of degradation types ininput images, leading to comprehensive image restoration pre-training. Unlikeconventional pre-training methods, MaskDCPT uses the degradation type of theimage as an extremely weak supervision, while simultaneously leveraging theimage reconstruction to enhance performance and robustness. MaskDCPT includesan encoder and two decoders: the encoder extracts features from the maskedlow-quality input image. The classification decoder uses these features toidentify the degradation type, whereas the reconstruction decoder aims toreconstruct a corresponding high-quality image. This design allows thepre-training to benefit from both masked image modeling and contrastivelearning, resulting in a generalized representation suited for restorationtasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trainedencoder can be used to address universal image restoration and achieveoutstanding performance. Implementing MaskDCPT significantly improvesperformance for both convolution neural networks (CNNs) and Transformers, witha minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task anda 34.8% reduction in PIQE compared to baseline in real-world degradationscenarios. It also emergences strong generalization to previously unseendegradation types and levels. In addition, we curate and release the UIR-2.5Mdataset, which includes 2.5 million paired restoration samples across 19degradation types and over 200 degradation levels, incorporating both syntheticand real-world data. The dataset, source code, and models are available athttps://github.com/MILab-PKU/MaskDCPT.</description>
      <author>example@mail.com (JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu)</author>
      <guid isPermaLink="false">2510.13282v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding</title>
      <link>http://arxiv.org/abs/2510.13244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure. demo page: https://motionbeat2025.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MotionBeat是一个运动对齐的音乐表示学习框架，通过具身对比损失和结构节奏对齐损失，以及小节等变相旋转和接触引导注意力等创新架构，成功捕捉了音乐的具身维度，在音乐到舞蹈生成和多种音频处理任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，但现有音频表示忽略了这种具身维度，限制了捕捉驱动运动的节奏和结构线索的能力。&lt;h4&gt;目的&lt;/h4&gt;提出MotionBeat框架，用于学习能够捕捉音乐运动特性的音乐表示。&lt;h4&gt;方法&lt;/h4&gt;采用两个训练目标：具身对比损失(ECL)实现细粒度节奏区分，结构节奏对齐损失(SRAL)确保节奏一致性；架构上引入小节等变相旋转捕捉循环节奏模式，以及接触引导注意力强调与音乐重音同步的运动事件。&lt;h4&gt;主要发现&lt;/h4&gt;MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并在节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务中有效迁移。&lt;h4&gt;结论&lt;/h4&gt;MotionBeat框架成功捕捉了音乐的具身维度，提高了音乐表示的质量，在多种音频处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示忽略了这种具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐的音乐表示学习框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失(ECL)，一种具有速度感知和节拍抖动负样本的增强型InfoNCE公式，用于实现细粒度节奏区分；以及结构节奏对齐损失(SRAL)，通过将音乐重音与相应运动事件对齐来确保节奏一致性。在架构上，MotionBeat引入了小节等变相旋转来捕捉循环节奏模式，以及接触引导注意力来强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并有效迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务。我们的项目演示页面：https://motionbeat2025.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music is both an auditory and an embodied phenomenon, closely linked to humanmotion and naturally expressed through dance. However, most existing audiorepresentations neglect this embodied dimension, limiting their ability tocapture rhythmic and structural cues that drive movement. We proposeMotionBeat, a framework for motion-aligned music representation learning.MotionBeat is trained with two newly proposed objectives: the EmbodiedContrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware andbeat-jitter negatives to achieve fine-grained rhythmic discrimination, and theStructural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency byaligning music accents with corresponding motion events. Architecturally,MotionBeat introduces bar-equivariant phase rotations to capture cyclicrhythmic patterns and contact-guided attention to emphasize motion eventssynchronized with musical accents. Experiments show that MotionBeat outperformsstate-of-the-art audio encoders in music-to-dance generation and transferseffectively to beat tracking, music tagging, genre and instrumentclassification, emotion recognition, and audio-visual retrieval. Our projectdemo page: https://motionbeat2025.github.io/.</description>
      <author>example@mail.com (Xuanchen Wang, Heng Wang, Weidong Cai)</author>
      <guid isPermaLink="false">2510.13244v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning</title>
      <link>http://arxiv.org/abs/2510.13176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一个创新的编译器自动调优框架，通过利用通道协同性和加权评分方法缩小搜索空间，使用对比学习和相似感知聚类创建程序嵌入，并在聚类内进行进化搜索，生成针对未见程序具有强泛化能力的核心集通道序列。实验表明，GRACE在LLVM IR指令计数优化方面达到了最先进的性能，同时保持了高效的调优时间。&lt;h4&gt;背景&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的编译器自动调优框架，能够在保持快速调优时间的同时，为未见程序提供高质量的优化解决方案，特别是在LLVM IR指令计数优化方面。&lt;h4&gt;方法&lt;/h4&gt;GRACE框架首先利用通道协同性和加权评分方法生成初始高质量候选序列和通道池，有效缩小搜索空间。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在七个不同的数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架成功解决了编译器自动调优中的搜索空间过大和泛化能力不足的问题，通过结合通道协同性、加权评分、对比学习和进化搜索等技术，实现了在保持高效调优时间的同时，为未见程序提供高质量优化的能力，在LLVM IR指令计数优化方面达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。本文介绍了GRACE，一个用于编译器自动调优的新颖框架，已在LLVM IR指令计数优化中得到验证。GRACE通过利用通道协同性和加权评分方法有效缩小搜索空间，生成初始高质量候选序列和通道池。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。在七个不同数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compiler pass selection and phase ordering present a significant challenge inachieving optimal program performance, particularly for objectives like codesize reduction. Standard compiler heuristics offer general applicability butoften yield suboptimal, program-specific results due to their one-size-fits-allnature. While iterative compilation can find tailored solutions, itsprohibitive search cost limits practical use. Machine learning approachespromise faster inference but frequently struggle with generalization to unseenprograms. This paper introduces GRACE, a novel framework for compilerauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACEeffectively curtails the search space by leveraging pass synergies and aweighted scoring method to generate initial high-quality candidate sequencesand a pass pool. It then employs contrastive learning, using passsequence-based data augmentation, to create program embeddings that facilitatesimilarity-aware clustering. Evolutionary search within these clusters yields acoreset of $k$ specialized pass sequences designed for robust generalization tounseen programs. At test time, GRACE efficiently selects the best coresetsequence and refines it using lightweight techniques. Experimental results onseven diverse datasets show that GRACE reduces LLVM IR instruction count by anaverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,while incurring an average tuning time of less than 1s per program,demonstrating its state-of-the-art performance and practical effectiveness.</description>
      <author>example@mail.com (Haolin Pan, Chao Zha, Jinyuan Dong, Mingjie Xing, Yanjun Wu)</author>
      <guid isPermaLink="false">2510.13176v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VCTR: A Transformer-Based Model for Non-parallel Voice Conversion</title>
      <link>http://arxiv.org/abs/2510.12964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VCTR的高效非并行语音转换方法，结合了混合感知块和双剪枝自注意力机制，采用基于对比学习的对抗方法，解决了现有方法中存在的长距离依赖捕获不足的问题。&lt;h4&gt;背景&lt;/h4&gt;非并行语音转换技术旨在无需配对训练数据的情况下将源语音域转换为目标语音域。现有的CycleGAN、VAE和CVC等方法在训练效果和语义捕获方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获语音中长距离依赖关系的高效非并行语音转换方法，以提升转换质量和全局语义表达能力。&lt;h4&gt;方法&lt;/h4&gt;提出VCTR方法，结合了Hybrid Perception Block (HPB)和Dual Pruned Self-Attention (DPSA)技术，采用基于对比学习的对抗训练框架，能够更好地捕获语音中的长距离依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;基于CNN的生成器虽然能捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力；所提出的VCTR方法通过结合HPB和DPSA有效解决了这一问题。&lt;h4&gt;结论&lt;/h4&gt;VCTR是一种高效的非并行语音转换方法，通过创新的网络结构和训练策略，显著提升了语音转换的质量和全局语义表达能力。&lt;h4&gt;翻译&lt;/h4&gt;非并行语音转换旨在无需配对训练数据的情况下将语音从源域转换到目标域。循环一致性生成对抗网络(CycleGAN)和变分自编码器(VAE)已被用于此任务，但这些模型面临训练困难和结果不理想的问题。后来，对比语音转换(Contrastive Voice Conversion, CVC)被提出，利用基于对比学习的方法解决这些问题。然而，这些方法使用基于CNN的生成器，虽然可以捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力。在本文中，我们提出了VCTR，一种用于非并行语音转换的高效方法，它结合了混合感知块(HPB)和双剪枝自注意力(DPSA)，以及基于对比学习的对抗方法。代码可在https://github.com/Maharnab-Saikia/VCTR找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-parallel voice conversion aims to convert voice from a source domain to atarget domain without paired training data. Cycle-Consistent GenerativeAdversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have beenused for this task, but these models suffer from difficult training andunsatisfactory results. Later, Contrastive Voice Conversion (CVC) wasintroduced, utilizing a contrastive learning-based approach to address theseissues. However, these methods use CNN-based generators, which can capturelocal semantics but lacks the ability to capture long-range dependenciesnecessary for global semantics. In this paper, we propose VCTR, an efficientmethod for non-parallel voice conversion that leverages the Hybrid PerceptionBlock (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastivelearning-based adversarial approach. The code can be found inhttps://github.com/Maharnab-Saikia/VCTR.</description>
      <author>example@mail.com (Maharnab Saikia)</author>
      <guid isPermaLink="false">2510.12964v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D，第一个从抽象手绘草图和卫星图像伪标签注释生成3D户外语义场景的大规模基准数据集，以及Cylinder Mamba Diffusion (CymbaDiff)方法，显著增强了户外3D场景生成的空间连贯性。&lt;h4&gt;背景&lt;/h4&gt;户外3D语义场景生成技术为城市模拟和自动驾驶等应用提供逼真且语义丰富的环境，但该领域发展受限于缺乏公开可用的、良好注释的数据集。&lt;h4&gt;目的&lt;/h4&gt;引入SketchSem3D基准数据集，用于从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景。&lt;h4&gt;方法&lt;/h4&gt;SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像）。提出Cylinder Mamba Diffusion (CymbaDiff)方法，施加结构化空间排序，捕获圆柱连续性和垂直层次结构，保持物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;翻译&lt;/h4&gt;户外3D语义场景生成为城市模拟和自动驾驶等应用生成逼真且语义丰富的环境。然而，这一方向的进展受到缺乏公开可用、良好注释的数据集的限制。我们引入SketchSem3D，这是第一个从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景的大规模基准。SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了Cylinder Mamba Diffusion (CymbaDiff)，显著增强了户外3D场景生成的空间连贯性。CymbaDiff施加结构化空间排序，明确捕获圆柱连续性和垂直层次结构，并在生成的场景中保持物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外3D语义场景生成的问题，特别是从手绘草图和伪标记卫星图像注释生成3D城市场景。这个问题很重要，因为生成逼真且语义丰富的户外环境对城市模拟和自动驾驶等应用至关重要，但该领域缺乏公开、良好标注的数据集，且现有方法难以处理户外场景的高语义多样性、复杂空间结构和动态上下文依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到户外3D场景生成的重要性及现有方法的局限性，然后构建了SketchSem3D数据集作为基础。方法设计借鉴了状态空间模型(SSMs)在捕获长程依赖关系方面的优势，以及扩散模型在生成任务中的成功经验。作者创新性地结合了笛卡尔和圆柱坐标系统，设计了圆柱Mamba块(CylMa)来增强空间一致性，同时保留了三重Mamba模块以保持精确几何距离。整体架构包括场景结构估计网络、潜在映射网络和去噪网络，通过多尺度特征提取和维度分解残差块来提升性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散结合圆柱和笛卡尔坐标系统的优势，增强户外3D场景生成的空间一致性。整体流程包括：1)数据预处理，生成草图和伪标记卫星图像注释；2)使用场景结构估计网络提取抽象结构信息；3)通过变分自编码器将输入条件压缩为潜在表示；4)在潜在空间中使用圆柱Mamba块进行去噪扩散；5)融合三重Mamba和圆柱Mamba的特征，结合径向和轴对齐的空间线索；6)生成最终的3D语义场景，每个体素被分配语义类标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建SketchSem3D数据集，提供更高分辨率(256×256×32)、更多语义类别(20类)和更丰富的地理空间语义；3)设计圆柱Mamba扩散(CymbaDiff)模型；4)通过结构化空间排序捕获圆柱连续性和垂直层次结构；5)利用草图和伪标记卫星图像注释作为多模态条件输入。相比之前工作，CymbaDiff结合了圆柱和笛卡尔坐标系统，更好地表示户外场景结构；通过状态空间模型和扩散模型结合，更高效地捕获长程依赖；将草图生成从孤立对象和简单室内场景扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了SketchSem3D数据集和CymbaDiff方法，首次实现了从手绘草图和伪标记卫星图像注释生成高质量、空间一致的3D户外语义场景，为城市模拟和自动驾驶等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</title>
      <link>http://arxiv.org/abs/2510.13747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InteractiveOmni是一个统一的开源多模态大语言模型，参数规模从4B到8B，专注于音频视觉多轮交互，整合了视觉编码器、音频编码器、大语言模型和语音解码器，采用多阶段训练策略，具有强大的跨模态能力和类人长期对话能力。&lt;h4&gt;背景&lt;/h4&gt;轻量级模型领域需要全面的多模态理解和语音生成能力，现有模型可能在这一方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的、开源的多模态大语言模型，引领轻量级模型领域，提供全面的多模态理解和语音生成能力。&lt;h4&gt;方法&lt;/h4&gt;将视觉编码器、音频编码器、大语言模型和语音解码器整合到统一模型中；设计多阶段训练策略：预训练用于多模态理解，然后进行语音对话和视听交互的后训练；精心策划多轮训练数据集，增强处理复杂多轮交互的能力；构建多模态多轮记忆基准和多轮语音交互基准，用于评估多轮记忆和语音交互能力。&lt;h4&gt;主要发现&lt;/h4&gt;InteractiveOmni显著优于领先的开源模型；InteractiveOmni-4B在通用基准上可与更大的Qwen2.5-Omni-7B模型相媲美；InteractiveOmni-4B仅使用50%的模型大小就能保留InteractiveOmni-8B 97%的性能；在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;InteractiveOmni是下一代智能交互系统的可访问开源基础模型，提供了更智能的多轮音频视觉体验，特别是在长期记忆能力方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了InteractiveOmni，这是一个统一的开源多模态大语言模型，参数规模从4B到8B，专为音频视觉多轮交互设计，通过提供全面的多模态理解和语音生成能力引领轻量级模型领域。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器整合到一个统一模型中，用于理解和生成任务。我们设计了一个多阶段训练策略，以确保强大的跨模态能力，包括预训练用于多模态理解，随后进行语音对话和视听交互的后训练。为了实现类人长期对话能力，我们精心策划了一个多轮训练数据集，增强模型处理复杂多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验证明，InteractiveOmni显著优于领先的开源模型，提供了更智能的多轮音频视觉体验，特别是在其长期记忆能力方面。值得注意的是，InteractiveOmni-4B在通用基准上可与大得多的Qwen2.5-Omni-7B模型相媲美，同时仅利用50%的模型大小就能保留InteractiveOmni-8B 97%的性能。在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果，InteractiveOmni是下一代智能交互系统的可访问开源基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce InteractiveOmni, a unified and open-source omni-modal largelanguage model for audio-visual multi-turn interaction, ranging from 4B to 8Bparameters, designed to lead the field of lightweight models by offeringcomprehensive omni-modal understanding and speech generation capabilities. Toachieve this, we integrate the vision encoder, audio encoder, large languagemodel, and speech decoder into a unified model for understanding and generationtasks. We design a multi-stage training strategy to ensure robust cross-modalcapabilities, including pre-training for omni-modal understanding, followed bypost-training with speech conversation and audio-visual interaction. To enablehuman-like long-term conversational ability, we meticulously curate amulti-turn training dataset that enhances the model's ability to handle complexand multi-turn interactions. To effectively evaluate the multi-turn memory andspeech interaction capabilities, we construct the multi-modal multi-turn memorybenchmark and the multi-turn speech interaction benchmark. Experimentsdemonstrate that InteractiveOmni significantly outperforms leading open-sourcemodels and provides a more intelligent multi-turn audio-visual experience,particularly in its long-term memory capabilities. Notably, InteractiveOmni-4Bis comparable to the much larger model like Qwen2.5-Omni-7B on generalbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8Bwhile utilizing only 50% of the model size. Achieving state-of-the-art resultsagainst similarly sized models across image, audio, video understanding, andspeech generation tasks, InteractiveOmni is an accessible, open-sourcefoundation for next-generation intelligent interactive systems.</description>
      <author>example@mail.com (Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu)</author>
      <guid isPermaLink="false">2510.13747v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping</title>
      <link>http://arxiv.org/abs/2510.13672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用贝叶斯分层时空模型分析了巴西累西腓市2015-2024年的登革热病例，探讨了多种社会环境和气候因素对登革热风险的影响，并识别了高风险区域。&lt;h4&gt;背景&lt;/h4&gt;登革热是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。&lt;h4&gt;目的&lt;/h4&gt;分析2015-2024年累西腓市的登革热确诊病例，评估多种因素对登革热风险的影响。&lt;h4&gt;方法&lt;/h4&gt;使用R-INLA实现的贝叶斯分层时空模型，结合BYM2空间结构和RW1时间成分。纳入的协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。&lt;h4&gt;主要发现&lt;/h4&gt;人口密度和家庭规模增加登革热风险；收入和排水渠道具有保护作用；滞后降水量增加风险；高温显示反向关联，表明媒介活动的热阈值；模型拟合良好，收敛稳定；北部和西部存在持续高风险集群，与高密度和社会脆弱性区域重叠。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯模型支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;h4&gt;翻译&lt;/h4&gt;登革热仍然是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。本研究使用R-INLA实现的贝叶斯分层时空模型分析了2015-2024年累西腓市的登革热确诊病例，结合了BYM2空间结构和RW1时间成分。协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。人口密度和家庭规模对登革热风险有正向影响，而收入和渠道存在具有保护作用。滞后降水量增加风险，较高温度显示反向关联，表明媒介活动的热阈值。模型拟合良好，收敛稳定，具有中等程度的残差空间自相关和2016-2019年间平滑的时间趋势。空间时间估计显示累西腓北部和西部持续存在高风险集群，与较高密度和社会脆弱性区域重叠。除了重现历史模式外，贝叶斯模型还支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dengue remains one of Brazil's major epidemiological challenges, marked bystrong intra-urban inequalities and the influence of climatic andsocio-environmental factors. This study analyzed confirmed dengue cases inRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal modelimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporalcomponent. Covariates included population density, household size, income,drainage channels, lagged precipitation, and mean temperature. Populationdensity and household size had positive effects on dengue risk, while incomeand channel presence were protective. Lagged precipitation increased risk, andhigher temperatures showed an inverse association, suggesting thermalthresholds for vector activity. The model achieved good fit (DIC=65817;WAIC=64506) and stable convergence, with moderate residual spatialautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.Spatio-temporal estimates revealed persistent high-risk clusters in northernand western Recife, overlapping with areas of higher density and socialvulnerability. Beyond reproducing historical patterns, the Bayesian modelsupports probabilistic forecasting and early warning systems. Compared withclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertaintyand spatial-temporal dependence, offering credible interval inference fordecision-making in urban health management.</description>
      <author>example@mail.com (Marcílio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo)</author>
      <guid isPermaLink="false">2510.13672v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning</title>
      <link>http://arxiv.org/abs/2510.13614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MemoTime是一种记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习解决大型语言模型在时间理解方面的挑战，显著提升了模型在时间问答任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已展现出强大的推理能力，但在处理涉及多个实体、复合运算符和演变事件序列的时间理解问题时存在困难。时间知识图谱虽提供了结构化的时间事实，但现有基于TKG的LLM推理方法仍面临四大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有TKG-based LLM推理方法面临的四大挑战：保持多跳推理的时间忠实性、实现多实体时间同步、适应不同时间运算符的检索、重用先验推理经验以提高稳定性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出MemoTime框架，将复杂时间问题分解为层次化的时间树，实现运算符感知推理；包含动态证据检索层自适应选择策略；以及自我演化的经验记忆存储已验证推理轨迹、工具包决策和子问题嵌入用于跨类型重用。&lt;h4&gt;主要发现&lt;/h4&gt;在多个时间问答基准上，MemoTime取得了最先进的结果，比强基线模型高出24.0%；使较小模型(如Qwen3-4B)能达到与GPT-4-Turbo相当的推理性能。&lt;h4&gt;结论&lt;/h4&gt;MemoTime有效解决了现有方法面临的四大挑战，显著提升了大型语言模型在时间理解方面的能力，并使较小模型也能达到高性能水平。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)已经取得了令人印象深刻的推理能力，但在时间理解方面存在困难，特别是当问题涉及多个实体、复合运算符和不断演变的事件序列时。时间知识图谱(TKGs)以结构化格式捕获大量时间事实，为时间推理提供了可靠来源。然而，现有的基于TKG的LLM推理方法仍面临四大挑战：在多跳推理中保持时间忠实性、实现多实体时间同步、使检索适应不同的时间运算符、重用先前的推理经验以提高稳定性和效率。为解决这些问题，我们提出了MemoTime，这是一个记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习来增强LLM推理。MemoTime将复杂的时间问题分解为层次化的时间树，实现运算符感知推理，强制单调时间戳并在统一时间边界下共同约束多个实体。动态证据检索层自适应地选择特定运算符的检索策略，而自我演化的经验记忆存储已验证的推理轨迹、工具包决策和子问题嵌入用于跨类型重用。在多个时间问答基准上的综合实验显示，MemoTime取得了最先进的结果，比强大的基线模型高出24.0%。此外，MemoTime使较小的模型(如Qwen3-4B)能够实现与GPT-4-Turbo相当的推理性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved impressive reasoning abilities,but struggle with temporal understanding, especially when questions involvemultiple entities, compound operators, and evolving event sequences. TemporalKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in astructured format, offer a reliable source for temporal reasoning. However,existing TKG-based LLM reasoning methods still struggle with four majorchallenges: maintaining temporal faithfulness in multi-hop reasoning, achievingmulti-entity temporal synchronization, adapting retrieval to diverse temporaloperators, and reusing prior reasoning experience for stability and efficiency.To address these issues, we propose MemoTime, a memory-augmented temporalknowledge graph framework that enhances LLM reasoning through structuredgrounding, recursive reasoning, and continual experience learning. MemoTimedecomposes complex temporal questions into a hierarchical Tree of Time,enabling operator-aware reasoning that enforces monotonic timestamps andco-constrains multiple entities under unified temporal bounds. A dynamicevidence retrieval layer adaptively selects operator-specific retrievalstrategies, while a self-evolving experience memory stores verified reasoningtraces, toolkit decisions, and sub-question embeddings for cross-type reuse.Comprehensive experiments on multiple temporal QA benchmarks show that MemoTimeachieves overall state-of-the-art results, outperforming the strong baseline byup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) toachieve reasoning performance comparable to that of GPT-4-Turbo.</description>
      <author>example@mail.com (Xingyu Tan, Xiaoyang Wang, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang)</author>
      <guid isPermaLink="false">2510.13614v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</title>
      <link>http://arxiv.org/abs/2510.13251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 28 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了视频大语言模型（VideoLLMs）在视频问答任务中的内部工作机制和信息流动模式。通过可解释性技术分析，研究者发现了VideoLLMs处理视频和文本信息的特定阶段和模式，并展示了如何通过选择有效信息通路来保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究VideoLLMs的内部信息流动机制，特别是它们在视频问答任务中如何进行时序推理以及如何整合视频和文本信息。&lt;h4&gt;方法&lt;/h4&gt;研究者使用可解释性技术来分析VideoLLMs的内部信息流动模式。&lt;h4&gt;主要发现&lt;/h4&gt;1. 时序推理从早期到中间层开始，涉及帧间积极交互；2. 随后在中间层进行视频-语言逐步整合，这得益于视频表示与包含时间概念的词嵌入之间的对齐；3. 完成整合后，模型在中间到后期层准备生成正确答案；4. 通过选择有效信息通路并抑制大量注意力边缘（例如在LLaVA-NeXT-7B-Video-FT中为58%），VideoLLMs可以保持其视频问答性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现为理解VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。在本研究中，我们使用可解释性技术研究了VideoLLMs的内部信息流动。我们的分析揭示了跨不同视频问答任务的一致模式：（1）VideoLLMs中的时序推理从早期到中间层的帧间积极交互开始，（2）随后在中间层进行视频-语言逐步整合。这得益于视频表示与包含时间概念的词嵌入之间的对齐。（3）完成此整合后，模型在中间到后期层准备生成正确答案。（4）基于我们的分析，我们表明VideoLLMs可以通过选择这些有效信息通路同时抑制大量注意力边缘来保持其视频问答性能，例如在LLaVA-NeXT-7B-Video-FT中为58%。这些发现为VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。我们的项目页面和源代码可在https://map-the-flow.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) extend the capabilities ofvision-language models to spatiotemporal inputs, enabling tasks such as videoquestion answering (VideoQA). Despite recent advances in VideoLLMs, theirinternal mechanisms on where and how they extract and propagate video andtextual information remain less explored. In this study, we investigate theinternal information flow of VideoLLMs using mechanistic interpretabilitytechniques. Our analysis reveals consistent patterns across diverse VideoQAtasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frameinteractions in early-to-middle layers, (2) followed by progressivevideo-language integration in middle layers. This is facilitated by alignmentbetween video representations and linguistic embeddings containing temporalconcepts. (3) Upon completion of this integration, the model is ready togenerate correct answers in middle-to-late layers. (4) Based on our analysis,we show that VideoLLMs can retain their VideoQA performance by selecting theseeffective information pathways while suppressing a substantial amount ofattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide ablueprint on how VideoLLMs perform temporal reasoning and offer practicalinsights for improving model interpretability and downstream generalization.Our project page with the source code is available athttps://map-the-flow.github.io</description>
      <author>example@mail.com (Minji Kim, Taekyung Kim, Bohyung Han)</author>
      <guid isPermaLink="false">2510.13251v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation</title>
      <link>http://arxiv.org/abs/2510.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Edit-Your-Interest的轻量级、文本驱动、零样本视频编辑方法，通过时空特征内存和特征传播技术解决了现有视频编辑方法计算开销大、内存消耗高和视觉保真度低的问题。&lt;h4&gt;背景&lt;/h4&gt;现有文本到图像扩散模型在视频编辑方面取得了显著进展，但现有视频编辑方法受高计算开销和内存消耗的限制，且往往牺牲视觉保真度，导致时间不一致性和模糊、马赛克状伪影等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级、文本驱动、零样本的视频编辑方法，以提高效率和视觉保真度。&lt;h4&gt;方法&lt;/h4&gt;Edit-Your-Interest方法包含三个核心技术：1)时空特征内存(SFM)缓存来自先前帧的关键图像标记；2)特征最相似传播(FMP)方法将最相关标记从前一帧传播到后续帧；3)SFM更新算法持续刷新缓存特征。此外，还利用交叉注意图自动提取感兴趣实例的掩码，并将其集成到扩散去噪过程中实现细粒度控制。&lt;h4&gt;主要发现&lt;/h4&gt;SFM显著减少了计算开销；FMP保留了时间一致性；SFM更新算法确保了特征的长期相关性和有效性；掩码集成方法实现了对目标对象的高度准确编辑，同时保持背景完整性。&lt;h4&gt;结论&lt;/h4&gt;Edit-Your-Interest在效率和视觉保真度上都优于现有最先进方法，验证了其优越的有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;文本到图像(T2I)扩散模型最近在视频编辑方面展示了显著进展。然而，现有的视频编辑方法受到高计算开销和内存消耗的严重限制。此外，这些方法通常牺牲视觉保真度，导致不期望的时间不一致性和伪影，如模糊和明显的马赛克状图案。我们提出了Edit-Your-Interest，一种轻量级、文本驱动、零样本的视频编辑方法。Edit-Your-Interest引入了一个时空特征内存来缓存来自先前帧的特征，与完整序列时空建模方法相比显著减少了计算开销。具体来说，我们首先引入了一个时空特征内存库(SFM)，它被设计用来高效缓存和保留由空间注意力处理的关键图像标记。其次，我们提出了特征最相似传播(FMP)方法。FMP将最相关的标记从先前帧传播到后续帧，保持时间一致性。最后，我们引入了一个SFM更新算法，它不断刷新缓存的特征，确保它们在整个视频序列中的长期相关性和有效性。此外，我们利用交叉注意图自动提取感兴趣实例的掩码。这些掩码无缝集成到扩散去噪过程中，实现对目标对象的细粒度控制，并允许Edit-Your-Interest在稳健保持背景完整性的同时执行高度准确的编辑。大量实验明确证明，所提出的Edit-Your-Interest在效率和视觉保真度上都优于最先进的方法，验证了其优越的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image (T2I) diffusion models have recently demonstrated significantprogress in video editing.  However, existing video editing methods are severely limited by their highcomputational overhead and memory consumption.  Furthermore, these approaches often sacrifice visual fidelity, leading toundesirable temporal inconsistencies and artifacts such as blurring andpronounced mosaic-like patterns.  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot videoediting method.  Edit-Your-Interest introduces a spatio-temporal feature memory to cachefeatures from previous frames, significantly reducing computational overheadcompared to full-sequence spatio-temporal modeling approaches.  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),which is designed to efficiently cache and retain the crucial image tokensprocessed by spatial attention.  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMPpropagates the most relevant tokens from previous frames to subsequent ones,preserving temporal consistency.  Finally, we introduce an SFM update algorithm that continuously refreshes thecached features, ensuring their long-term relevance and effectivenessthroughout the video sequence.  Furthermore, we leverage cross-attention maps to automatically extract masksfor the instances of interest.  These masks are seamlessly integrated into the diffusion denoising process,enabling fine-grained control over target objects and allowingEdit-Your-Interest to perform highly accurate edits while robustly preservingthe background integrity.  Extensive experiments decisively demonstrate that the proposedEdit-Your-Interest outperforms state-of-the-art methods in both efficiency andvisual fidelity, validating its superior effectiveness and practicality.</description>
      <author>example@mail.com (Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao)</author>
      <guid isPermaLink="false">2510.13084v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了时空视频动作定位(SVAG)任务，旨在解决细粒度动作理解和对象时空定位的挑战，构建了大规模基准数据集SVAG-Bench，提出了基线框架SVAGFormer和评估工具包SVAGEval，发现现有模型在复杂场景中表现不佳，需要更高级的推理能力。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确定位其对应的时间和空间位置是推进下一代AI系统的基础能力。然而，现有视频理解方法主要处理粗粒度动作识别或通用目标跟踪，忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。&lt;h4&gt;目的&lt;/h4&gt;引入时空视频动作定位(SVAG)新任务，构建支持该任务的基准数据集，提出基线框架，并开发标准化评估工具包，以促进细粒度动作理解和对象时空定位的研究。&lt;h4&gt;方法&lt;/h4&gt;构建SVAG-Bench基准测试，包含688个视频、19,590条标注记录和903个独特动词；提出SVAGFormer框架，适配最先进的视觉语言模型进行联合时空定位；开发SVAGEval标准化评估工具包以确保公平和可复现的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;需要开发能够同时处理细粒度动作理解、对象跟踪和时空定位的AI系统，SVAG任务和基准测试为这一研究方向提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位它们在空间和时间中对应的执行者是推进下一代AI系统的基础能力，包括具身智能体、自主平台和人机交互框架。尽管最近视频理解取得了进展，但现有方法主要处理粗粒度动作识别或通用目标跟踪，从而忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。为解决这一差距，我们引入时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时域定位视频中的所有指代对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，适配最先进的视觉语言模型进行联合时空定位，并引入了SVAGEval，这是一个标准化评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets</title>
      <link>http://arxiv.org/abs/2510.13443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习的膝关节角度预测框架，使用轻量级注意力CNN-LSTM模型，仅需新受试者少量步态周期数据即可实现高精度预测。&lt;h4&gt;背景&lt;/h4&gt;肌电信号(EMG)广泛用于通过机器学习和深度学习预测身体关节角度，但现有方法面临实时应用性有限、测试条件不具代表性以及需要大量数据集等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅需少量新受试者数据即可预测膝关节角度的迁移学习框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用Georgia Tech、UCI和SMLE三个包含四个与膝关节运动相关EMG通道的数据集；开发轻量级基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上预训练后迁移到其他数据集；仅使用EMG输入，以及结合历史膝关节角度和多种传感器输入进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用EMG输入时，模型在异常受试者的一步和50步预测中NMAE分别为6.8%和13.7%；结合历史膝关节角度后，正常受试者NMAE降至3.1%和3.5%，异常受试者降至2.8%和7.5%；当使用EMG、运动学和相互作用力多种输入时，模型在一步和50步预测中NMAE分别达到1.09%和3.1%。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架在短期和长期康复场景中均表现出稳健的性能和强大的泛化能力，仅需少量新受试者数据即可实现高精度膝关节角度预测。&lt;h4&gt;翻译&lt;/h4&gt;肌电(EMG)信号被广泛用于通过机器学习(ML)和深度学习(DL)方法预测身体关节角度。然而，这些方法通常面临实时应用性有限、测试条件不具代表性以及需要大量数据集才能实现最佳性能等挑战。本文提出了一个膝关节角度预测的迁移学习框架，只需要新受试者几个步态周期的数据。利用了三个数据集——Georgia Tech、加州大学欧文分校(UCI)和Sharif机械实验室外骨骼(SMLE)，这些数据集包含四个与膝关节运动相关的EMG通道。开发了一个轻量级的基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上进行预训练，然后转移到UCI和SMLE数据集。所提出的模型仅使用EMG输入，在异常受试者的一步和50步预测中实现了6.8%和13.7%的归一化平均绝对误差(NMAE)。结合历史膝关节角度将正常受试者的NMAE降低到3.1%和3.5%，异常受试者降低到2.8%和7.5%。当进一步适应SMLE外骨骼，使用EMG、运动学和相互作用力输入时，模型在一步和50步预测中分别实现了1.09%和3.1%的NMAE。这些结果表明模型在短期和长期康复场景中都具有稳健的性能和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are widely used for predicting body jointangles through machine learning (ML) and deep learning (DL) methods. However,these approaches often face challenges such as limited real-time applicability,non-representative test conditions, and the need for large datasets to achieveoptimal performance. This paper presents a transfer-learning framework for kneejoint angle prediction that requires only a few gait cycles from new subjects.Three datasets - Georgia Tech, the University of California Irvine (UCI), andthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channelsrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTMmodel was developed and pre-trained on the Georgia Tech dataset, thentransferred to the UCI and SMLE datasets. The proposed model achievedNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent forone-step and 50-step predictions on abnormal subjects using EMG inputs alone.Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormalsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, andinteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAEfor one- and 50-step predictions, respectively. These results demonstraterobust performance and strong generalization for both short- and long-termrehabilitation scenarios.</description>
      <author>example@mail.com (Mojtaba Mollahossein, Gholamreza Vossoughi, Mohammad Hossein Rohban)</author>
      <guid isPermaLink="false">2510.13443v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.13023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端的机器学习工作流程，用于解决自动化超声波焊接检测中的数据有限和环境波动问题，通过结合降阶建模、扩散分布对齐和U-Net分割反演技术，实现了真实工业环境下的焊接缺陷检测。&lt;h4&gt;背景&lt;/h4&gt;自动化超声波焊接检测在无损评估领域面临两大挑战：训练数据有限（由于实验标本整理或高保真模拟的复杂性）和工业环境的环境波动性（导致实时测量数据损坏）。&lt;h4&gt;目的&lt;/h4&gt;开发一种在真实工业环境中进行声学焊接检测的端到端机器学习工作流程，克服数据整理和信号损坏问题。&lt;h4&gt;方法&lt;/h4&gt;提出的工作流程包括：1)基于Lamb波理论的降阶Helmholtz模型生成综合数据集；2)使用相对廉价的低阶解为反演模型提供训练数据，并通过迁移学习优化；3)利用引导扩散处理分布外实验LDV扫描数据，生成分布内表示供反演模型处理。&lt;h4&gt;主要发现&lt;/h4&gt;降阶模型能够生成全面的焊接异质性和裂纹缺陷数据集；迁移学习可有效利用有限的全3D弹性动力学模拟；扩散模型能够处理具有不可预测噪声分布的真实世界测量数据。&lt;h4&gt;结论&lt;/h4&gt;该集成框架为真实数据上的自动化焊接检测提供了有效的端到端解决方案，克服了传统方法在数据有限和环境波动情况下的局限性。&lt;h4&gt;翻译&lt;/h4&gt;自动化超声波焊接检测在无损评估领域仍是一个重大挑战，原因包括训练数据有限（由于整理实验标本或高保真模拟的复杂性）和许多工业环境的环境波动性（导致实时测量数据损坏）。因此，在真实（即工业）环境中进行声学焊接检测的端到端机器学习工作流程一直是一个难以实现的目标。本文通过提出包含降阶建模方案、基于扩散的分布对齐以及基于U-Net的分割和反演的工作流程，解决了数据整理和信号损坏的挑战。使用基于Lamb波理论的降阶Helmholtz模型，在变化的焊接异质性和裂纹缺陷上生成综合数据集。相对廉价的低阶解为反演模型提供了强大的训练数据集，这些模型通过使用有限的全3D弹性动力学模拟集的迁移学习阶段进行优化。为了处理具有变化且不可预测的噪声分布的分布外真实世界测量（即激光多普勒测振仪扫描），引导扩散生成OOD实验LDV扫描的分布内表示，随后由反演模型处理。这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated ultrasonic weld inspection remains a significant challenge in thenondestructive evaluation (NDE) community to factors such as limited trainingdata (due to the complexity of curating experimental specimens or high-fidelitysimulations) and environmental volatility of many industrial settings(resulting in the corruption of on-the-fly measurements). Thus, an end-to-endmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,industrial) settings has remained an elusive goal. This work addresses thechallenges of data curation and signal corruption by proposing workflowconsisting of a reduced-order modeling scheme, diffusion based distributionalignment, and U-Net-based segmentation and inversion. A reduced-orderHelmholtz model based on Lamb wave theory is used to generate a comprehensivedataset over varying weld heterogeneity and crack defects. The relativelyinexpensive low-order solutions provide a robust training dateset for inversionmodels which are refined through a transfer learning stage using a limited setof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)real-world measurements with varying and unpredictable noise distributions,i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distributionrepresentations of OOD experimental LDV scans which are subsequently processedby the inversion models. This integrated framework provides an end-to-endsolution for automated weld inspection on real data.</description>
      <author>example@mail.com (Joshua R. Tempelman, Adam J. Wachtor, Eric B. Flynn)</author>
      <guid isPermaLink="false">2510.13023v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.13809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://sihuiji.github.io/PhysMaster-Page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysMaster是一个通过物理知识表示指导视频生成模型的框架，利用输入图像中的物理先验信息，通过强化学习和人类反馈优化物理表示，能够生成物理上更合理的视频。&lt;h4&gt;背景&lt;/h4&gt;当前视频生成模型虽能生成视觉逼真的视频，但常常不遵守物理定律，限制了其生成物理合理视频的能力，使其无法成为有效的'世界模型'。&lt;h4&gt;目的&lt;/h4&gt;提出PhysMaster模型，通过捕获物理知识作为表示来指导视频生成模型，增强其物理感知能力，使其能够生成物理上合理的视频。&lt;h4&gt;方法&lt;/h4&gt;PhysMaster基于图像到视频任务，设计PhysEncoder从输入图像编码物理信息作为额外条件；采用强化学习与人类反馈相结合的方法，使用直接偏好优化(DPO)以端到端方式优化物理表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，在简单代理任务上证明了其能力，并展现出广泛物理场景的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可作为物理感知视频生成的通用即插即用解决方案，具有更广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;当今的视频生成模型能够生成视觉上逼真的视频，但常常不遵守物理定律，限制了它们生成物理上合理的视频的能力，使其无法成为'世界模型'。为解决这一问题，我们提出了PhysMaster，它将物理知识捕获为一种表示，用于指导视频生成模型增强其物理感知能力。具体而言，PhysMaster基于图像到视频任务，模型需要从输入图像预测物理上合理的动态。由于输入图像提供了物理先验信息，如场景中物体的相对位置和潜在交互，我们设计了PhysEncoder从中编码物理信息作为额外条件，将物理知识注入视频生成过程。除了外观之外，模型物理性能缺乏适当的监督，这促使PhysEncoder应用强化学习与人类反馈相结合的方法进行物理表示学习，利用生成模型的反馈通过直接偏好优化(DPO)以端到端方式优化物理表示。PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，从而提高了视频生成的物理合理性，在简单代理任务上证明了其能力，并具有广泛物理场景的泛化能力。这意味着我们的PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可以作为物理感知视频生成和更广泛应用的通用即插即用解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视频生成模型不遵守物理规律的问题。这个问题很重要，因为物理真实性是视频生成模型能否作为'世界模型'的关键，限制了它们在模拟真实世界场景、预测物理交互等应用场景中的实用性，也阻碍了视频生成模型从内容创作者向世界模拟器的转变。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了视频生成模型在物理规律遵循方面的两个主要挑战：MSE损失关注外观拟合而非物理理解，以及生成模型难以从图像中提取物理知识。他们提出学习物理表示作为桥梁，借鉴了基于物理仿真和无仿真方法的思路，但避免了它们的局限性。同时采用了大型语言模型中的RLHF框架和DPO训练方法，设计了三阶段训练pipeline：SFT微调基础模型和PhysEncoder，然后两阶段DPO分别优化DiT模型和PhysEncoder，利用生成反馈改进物理表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习物理表示作为物理知识和视频生成之间的桥梁，通过PhysEncoder从输入图像提取物理特征作为额外条件指导视频生成。整体流程包括：1)基于DiT的扩散模型架构，结合3D VAE和T5编码器；2)PhysEncoder基于DINOv2编码器和物理头部设计；3)三阶段训练pipeline - SFT阶段同时训练DiT和PhysEncoder，DPO阶段先优化DiT模型再优化PhysEncoder；4)从'自由落体'代理任务开始，验证后扩展到一般开放世界场景；5)使用PisaBench和VIDEOPHY等评估方法验证效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)物理表示学习作为物理知识和视频生成的桥梁；2)自上而下的优化策略，基于最终视频的物理合理性优化物理编码器；3)三阶段训练pipeline结合SFT和DPO；4)插件式物理知识注入实现通用物理属性学习；5)从特定任务到开放世界场景的泛化能力。相比之前工作，PhysMaster不依赖特定物理仿真引擎，能处理更广泛物理现象；不依赖大规模物理数据集或昂贵人工注释；专注于优化物理编码器而非整个模型；效率更高，生成5秒视频仅需26秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhysMaster通过学习物理表示并作为插件注入视频生成模型，利用强化学习优化物理编码器，显著提升了视频生成模型的物理合理性，使其能够从内容创作者转变为遵循物理规律的世界模拟器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models nowadays are capable of generating visually realisticvideos, but often fail to adhere to physical laws, limiting their ability togenerate physically plausible videos and serve as ''world models''. To addressthis issue, we propose PhysMaster, which captures physical knowledge as arepresentation for guiding video generation models to enhance theirphysics-awareness. Specifically, PhysMaster is based on the image-to-video taskwhere the model is expected to predict physically plausible dynamics from theinput image. Since the input image provides physical priors like relativepositions and potential interactions of objects in the scenario, we devisePhysEncoder to encode physical information from it as an extra condition toinject physical knowledge into the video generation process. The lack of propersupervision on the model's physical performance beyond mere appearancemotivates PhysEncoder to apply reinforcement learning with human feedback tophysical representation learning, which leverages feedback from generationmodels to optimize physical representations with Direct Preference Optimization(DPO) in an end-to-end manner. PhysMaster provides a feasible solution forimproving physics-awareness of PhysEncoder and thus of video generation,proving its ability on a simple proxy task and generalizability to wide-rangingphysical scenarios. This implies that our PhysMaster, which unifies solutionsfor various physical processes via representation learning in the reinforcementlearning paradigm, can act as a generic and plug-in solution for physics-awarevideo generation and broader applications.</description>
      <author>example@mail.com (Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.13809v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
      <link>http://arxiv.org/abs/2510.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UniME-V2的新型通用多模态嵌入模型，通过利用大型多模态语言模型的先进理解能力来增强表示学习。该方法通过MLLM-as-a-Judge机制评估语义对齐，生成软语义匹配分数，用于高质量困难负样本挖掘和模型优化，显著提升了模型的判别能力，并在多个检索任务上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的通用多模态嵌入模型通常采用批内负样本挖掘方法测量查询-候选对的相似性，但这些方法存在几个局限：难以捕捉候选者之间的细微语义差异，负样本缺乏多样性，以及在区分错误负样本和困难负样本方面的判别能力有限。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有多模态嵌入模型的局限性，提高其捕捉细微语义差异的能力，增加负样本的多样性，并增强模型对困难负样本的判别能力，从而提升通用多模态嵌入模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;研究团队提出了一种名为UniME-V2的新型通用多模态嵌入模型，主要方法包括：1) 通过全局检索构建潜在困难负样本集；2) 引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐并生成软语义匹配分数；3) 将这些分数作为困难负样本挖掘的基础，减轻错误负样本影响，识别多样化高质量困难负样本；4) 将语义匹配分数作为软标签，缓解一对一映射约束；5) 通过对齐相似度矩阵与软语义匹配分数矩阵，学习候选者间的语义区别；6) 提出UniME-V2-Reranker重排序模型，通过联合成对和列表级优化方法训练。&lt;h4&gt;主要发现&lt;/h4&gt;在MMEB基准和多个检索任务上的全面实验表明，该方法在所有任务上平均达到了最先进的性能，证明了所提出方法的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;通过利用大型多模态语言模型的先进理解能力和创新的负样本挖掘方法，UniME-V2模型显著提高了通用多模态嵌入模型的性能，特别是在捕捉细微语义差异和区分困难负样本方面，为多模态表示学习领域提供了新的思路和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;通用多模态嵌入模型是各种任务的基础。现有方法通常采用批内负样本挖掘来测量查询-候选对的相似性。然而，这些方法往往难以捕捉候选者之间的细微语义差异，且负样本缺乏多样性。此外，嵌入模型在区分错误负样本和困难负样本方面的判别能力有限。在本文中，我们利用大型多模态语言模型的先进理解能力来增强表示学习，并提出了一种新颖的通用多模态嵌入模型。我们的方法首先通过全局检索构建潜在的困难负样本集。然后我们引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐情况，并生成软语义匹配分数。这些分数作为困难负样本挖掘的基础，减轻了错误负样本的影响，并能够识别出多样化的高质量困难负样本。此外，语义匹配分数还被用作软标签，以缓解严格的一对一映射约束。通过将相似度矩阵与软语义匹配分数矩阵对齐，模型能够学习候选者之间的语义区别，显著提高其判别能力。为了进一步提高性能，我们提出了UniME-V2-Reranker，这是一个通过联合成对和列表级优化方法在我们挖掘的困难负样本上训练的重排序模型。我们在MMEB基准和多个检索任务上进行了全面实验，证明我们的方法在所有任务上平均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal multimodal embedding models are foundational to various tasks.Existing approaches typically employ in-batch negative mining by measuring thesimilarity of query-candidate pairs. However, these methods often struggle tocapture subtle semantic differences among candidates and lack diversity innegative samples. Moreover, the embeddings exhibit limited discriminativeability in distinguishing false and hard negatives. In this paper, we leveragethe advanced understanding capabilities of MLLMs to enhance representationlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.Our approach first constructs a potential hard negative set through globalretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizesMLLMs to assess the semantic alignment of query-candidate pairs and generatesoft semantic matching scores. These scores serve as a foundation for hardnegative mining, mitigating the impact of false negatives and enabling theidentification of diverse, high-quality hard negatives. Furthermore, thesemantic matching scores are used as soft labels to mitigate the rigidone-to-one mapping constraint. By aligning the similarity matrix with the softsemantic matching score matrix, the model learns semantic distinctions amongcandidates, significantly enhancing its discriminative capacity. To furtherimprove performance, we propose UniME-V2-Reranker, a reranking model trained onour mined hard negatives through a joint pairwise and listwise optimizationapproach. We conduct comprehensive experiments on the MMEB benchmark andmultiple retrieval tasks, demonstrating that our method achievesstate-of-the-art performance on average across all tasks.</description>
      <author>example@mail.com (Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing)</author>
      <guid isPermaLink="false">2510.13515v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.13497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述进行癫痫检测，并通过知识蒸馏方法创建轻量级学生模型，在多个数据集上实现了超过97%的准确率。&lt;h4&gt;背景&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动，由异常放电引起。目前大多数癫痫检测的深度学习方法仅依赖单模态的脑电图信号，忽视了多模态信息的潜在优势。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的多模态模型DistilCLIP-EEG，基于CLIP框架，整合脑电图信号和文本描述，以捕捉癫痫发作的全面特征，并通过知识蒸馏方法提高效率和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。同时引入知识蒸馏方法，训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型。&lt;h4&gt;主要发现&lt;/h4&gt;在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。&lt;h4&gt;结论&lt;/h4&gt;该模型突显了在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;h4&gt;翻译&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动 episodes，由异常放电引起，可能导致一些精神障碍。目前大多数用于癫痫检测的深度学习方法仅依赖单模态的脑电图(EEG)信号，忽视了多模态信息的潜在优势。为此，我们提出了一种新颖的多模态模型 DistilCLIP-EEG，基于CLIP框架，整合了脑电图信号和文本描述，以捕捉癫痫发作的全面特征。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及我们提出的可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。为了提高效率和适应性，我们引入了一种知识蒸馏方法，其中训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型，以降低训练复杂度和时间。在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。此外，学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。这些结果突显了我们提出的模型在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JBHI.2025.3603022&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Epilepsy is a prevalent neurological disorder marked by sudden, briefepisodes of excessive neuronal activity caused by abnormal electricaldischarges, which may lead to some mental disorders. Most existing deeplearning methods for epilepsy detection rely solely on unimodal EEG signals,neglecting the potential benefits of multimodal information. To address this,we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIPframework, which integrates both EEG signals and text descriptions to capturecomprehensive features of epileptic seizures. The model involves an EEG encoderbased on the Conformer architecture as a text encoder, the proposed LearnableBERT (BERT-LP) as prompt learning within the encoders. Both operate in a sharedlatent space for effective cross-modal representation learning. To enhanceefficiency and adaptability, we introduce a knowledge distillation method wherethe trained DistilCLIP-EEG serves as a teacher to guide a more compact studentmodel to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MITdatasets, both the teacher and student models achieved accuracy rates exceeding97%. Across all datasets, the F1-scores were consistently above 0.94,demonstrating the robustness and reliability of the proposed framework.Moreover, the student model's parameter count and model size are approximately58.1% of those of the teacher model, significantly reducing model complexityand storage requirements while maintaining high performance. These resultshighlight the potential of our proposed model for EEG-based epilepsy detectionand establish a solid foundation for deploying lightweight models inresource-constrained settings.</description>
      <author>example@mail.com (Zexin Wang, Lin Shi, Haoyu Wu, Junru Luo, Xiangzeng Kong, Jun Qi)</author>
      <guid isPermaLink="false">2510.13497v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Multi-Modal Diffusion Mamba</title>
      <link>http://arxiv.org/abs/2510.13253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MDM（多模态扩散Mamba）的新型架构，通过统一的变分自编码器实现多模态处理的统一，在多个任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器处理输入和输出信息，这种分离阻碍了不同模态的联合表示学习。&lt;h4&gt;目的&lt;/h4&gt;为了统一多模态处理，解决现有模型中不同模态处理分离的问题。&lt;h4&gt;方法&lt;/h4&gt;MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。&lt;h4&gt;主要发现&lt;/h4&gt;在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（如MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。&lt;h4&gt;结论&lt;/h4&gt;研究结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;h4&gt;翻译&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器来处理输入和输出信息。这种分离阻碍了不同模态的联合表示学习。为了统一多模态处理，我们提出了一种名为MDM（多模态扩散Mamba）的新型架构。MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。这种创新方法使MDM在处理高维数据时能够实现卓越的性能，特别是在同时生成高分辨率图像和扩展文本序列方面。我们在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。我们的结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current end-to-end multi-modal models utilize different encoders and decodersto process input and output information. This separation hinders the jointrepresentation learning of various modalities. To unify multi-modal processing,we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDMutilizes a Mamba-based multi-step selection diffusion model to progressivelygenerate and refine modality-specific information through a unified variationalautoencoder for both encoding and decoding. This innovative approach allows MDMto achieve superior performance when processing high-dimensional data,particularly in generating high-resolution images and extended text sequencessimultaneously. Our evaluations in areas such as image generation, imagecaptioning, visual question answering, text comprehension, and reasoning tasksdemonstrate that MDM significantly outperforms existing end-to-end models(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTAmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM'seffectiveness in unifying multi-modal processes while maintaining computationalefficiency, establishing a new direction for end-to-end multi-modalarchitectures.</description>
      <author>example@mail.com (Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo)</author>
      <guid isPermaLink="false">2510.13253v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Representation: Towards Graph-Based Abstract Code Generation</title>
      <link>http://arxiv.org/abs/2510.13163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于图的抽象代码生成，提出并评估了JSON表示方法，使大型语言模型能够高精度地执行此类任务。&lt;h4&gt;背景&lt;/h4&gt;大多数大型语言模型擅长生成原始顺序代码，但很少研究基于图的抽象代码生成，这种方法在可视化编程语言和原始源代码不可用的情况下很有价值。&lt;h4&gt;目的&lt;/h4&gt;提出并评估JSON表示方法，以实现高精度的基于图的抽象代码生成，并研究不同表示方法对生成准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用ScratchTest（基于Scratch Python重新实现的迷你基准测试）评估不同的JSON图表示方法，测试LLM在代码图空间中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型可以在单次通过中执行基于图的抽象代码生成任务，无需依赖专门或复杂的管道，且不同表示方法会导致显著不同的准确性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于图的抽象代码生成的表示学习奠定了基础，突显了适当表示方法的重要性。&lt;h4&gt;翻译&lt;/h4&gt;目前大多数大型语言模型擅长生成具有最小抽象和自定义结构的原始顺序代码。然而，很少有关于基于图的抽象代码生成的工作，其中重要逻辑被封装在预定义节点中，执行流程由边决定。这对于可视化编程语言，以及原始源代码对用户和LLM训练集不可用的情况相关。在这项工作中，我们提出并评估了用于图的JSON表示，以实现高精度的基于图的抽象代码生成。我们在ScratchTest上评估了这些表示，这是一个基于我们自定义的Scratch Python重新实现的迷你基准测试，用于测试LLM在代码图空间中的表现。我们的研究结果表明，LLM确实可以在单次通过中执行上述生成任务，而不依赖于专门的或复杂的管道，前提是使用正确的图表示。我们还表明，不同的表示会导致显著不同的准确性，突显了表示在此生成任务中的重要作用。总而言之，这项工作为基于图的抽象代码生成的表示学习建立了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most large language models (LLMs) today excel at generating raw, sequentialcode with minimal abstractions and custom structures. However, there has beenlittle work on graph-based abstract code generation, where significant logic isencapsulated in predefined nodes and execution flow is determined by edges.This is relevant for visual programming languages, and in cases where rawsource code is inaccessible to users and LLM training sets. In this work, wepropose and evaluate JSON representations for graphs to enable high accuracygraph-based abstract code generation. We evaluate these representations onScratchTest, a mini-benchmark based on our custom Python re-implementation ofScratch, which tests the LLM in code graph space. Our findings demonstrate thatLLMs can indeed perform the aforementioned generation task in a single passwithout relying on specialized or complex pipelines, given the correct graphrepresentations. We also show that different representations inducesignificantly different accuracies, highlighting the instrumental role ofrepresentations in this generation task. All in all, this work establishes thefirst steps towards representation learning for graph-based abstract codegeneration.</description>
      <author>example@mail.com (Nyx Iskandar, Hisham Bedri, Andy Tsen)</author>
      <guid isPermaLink="false">2510.13163v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Information Shapes Koopman Representation</title>
      <link>http://arxiv.org/abs/2510.13025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信息论视角重新思考Koopman学习，提出一种平衡表示简单性和表达性的新方法，解决了Koopman算子在深度架构中面临的子空间选择挑战。&lt;h4&gt;背景&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，受到机器学习社区日益关注，但其无限维特性使得识别合适的有限维子空间具有挑战性，特别是在深度架构中。&lt;h4&gt;目的&lt;/h4&gt;解决Koopman学习中次优表示学习的问题，平衡潜在变量在表达性和简单性之间的权衡，克服信息瓶颈困境。&lt;h4&gt;方法&lt;/h4&gt;提出一种信息论拉格朗日公式化，明确平衡简单性和表达性的权衡；基于该公式开发新算法，促进潜在互信息（简单性）和冯·诺依曼熵（表达性）的共同优化。&lt;h4&gt;主要发现&lt;/h4&gt;潜在互信息促进简单性但过度强调可能导致潜在空间崩溃；冯·诺依曼熵维持表达性并防止崩溃，鼓励模式多样性；所提方法产生稳定且可解释的Koopman表示。&lt;h4&gt;结论&lt;/h4&gt;通过信息论视角重新审视Koopman学习，提出的新方法在多种动力系统上验证优于现有方法，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，并吸引了机器学习界的日益关注。然而，其无限维特性使得识别合适的有限维子空间具有挑战性，特别是对于深度架构。我们认为这些困难来自于次优的表示学习，其中潜在变量无法平衡表达性和简单性。这种张力与信息瓶颈(IB)困境密切相关：构建既紧凑又有预测能力的压缩表示。通过这一视角重新思考Koopman学习，我们证明潜在互信息促进简单性，但过度强调简单性可能导致潜在空间崩溃到少数主导模式。相比之下，表达性由冯·诺依曼熵维持，防止这种崩溃并鼓励模式多样性。这一见解促使我们提出一种明确平衡这种权衡的信息论拉格朗日公式化。此外，我们基于该公式提出新算法，鼓励简单性和表达性，产生稳定且可解释的Koopman表示。除了定量评估外，我们还可视化了在我们表示下学习到的流形，观察到与理论预测一致的实证结果。最后，我们在多种动力系统上验证了我们的方法，展示了与现有Koopman学习方法相比的改进性能。实现已在https://github.com/Wenxuan52/InformationKoopman公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Koopman operator provides a powerful framework for modeling dynamicalsystems and has attracted growing interest from the machine learning community.However, its infinite-dimensional nature makes identifying suitablefinite-dimensional subspaces challenging, especially for deep architectures. Weargue that these difficulties come from suboptimal representation learning,where latent variables fail to balance expressivity and simplicity. Thistension is closely related to the information bottleneck (IB) dilemma:constructing compressed representations that are both compact and predictive.Rethinking Koopman learning through this lens, we demonstrate that latentmutual information promotes simplicity, yet an overemphasis on simplicity maycause latent space to collapse onto a few dominant modes. In contrast,expressiveness is sustained by the von Neumann entropy, which prevents suchcollapse and encourages mode diversity. This insight leads us to propose aninformation-theoretic Lagrangian formulation that explicitly balances thistradeoff. Furthermore, we propose a new algorithm based on the Lagrangianformulation that encourages both simplicity and expressiveness, leading to astable and interpretable Koopman representation. Beyond quantitativeevaluations, we further visualize the learned manifolds under ourrepresentations, observing empirical results consistent with our theoreticalpredictions. Finally, we validate our approach across a diverse range ofdynamical systems, demonstrating improved performance over existing Koopmanlearning methods. The implementation is publicly available athttps://github.com/Wenxuan52/InformationKoopman.</description>
      <author>example@mail.com (Xiaoyuan Cheng, Wenxuan Yuan, Yiming Yang, Yuanzhao Zhang, Sibo Cheng, Yi He, Zhuo Sun)</author>
      <guid isPermaLink="false">2510.13025v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning</title>
      <link>http://arxiv.org/abs/2510.12957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的多模态可解释AI框架，通过注意力增强特征融合、Grad-CAM++局部解释和Reveal-to-Revise反馈循环解决偏差检测和减轻问题，在多模态MNIST上实现了高准确率和解释保真度。&lt;h4&gt;背景&lt;/h4&gt;标准基准数据集如MNIST无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态可解释AI框架，实现偏差检测和减轻，提高AI系统的透明度和可信度。&lt;h4&gt;方法&lt;/h4&gt;统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，形成一个完整的偏差检测和减轻框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态扩展的MNIST上实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线方法；消融研究表明可解释性与偏差感知学习的结合增强了模型的鲁棒性和人类对齐。&lt;h4&gt;结论&lt;/h4&gt;该工作弥合了性能、透明度和公平性之间的差距，为敏感领域可信AI的实际应用提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;标准基准数据集如MNIST往往无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。我们提出了一种新颖的多模态可解释AI(XAI)框架，统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，用于偏差检测和减轻。在多模态扩展的MNIST上评估，我们的方法实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线。消融研究表明，将可解释性与偏差感知学习相结合可以增强鲁棒性和人类对齐。我们的工作弥合了性能、透明度和公平性之间的差距，突显了敏感领域可信AI的实际应用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard benchmark datasets, such as MNIST, often fail to expose latentbiases and multimodal feature complexities, limiting the trustworthiness ofdeep neural networks in high-stakes applications. We propose a novel multimodalExplainable AI (XAI) framework that unifies attention-augmented feature fusion,Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop forbias detection and mitigation. Evaluated on multimodal extensions of MNIST, ourapproach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%explanation fidelity (IoU-XAI), outperforming unimodal and non-explainablebaselines. Ablation studies demonstrate that integrating interpretability withbias-aware learning enhances robustness and human alignment. Our work bridgesthe gap between performance, transparency, and fairness, highlighting apractical pathway for trustworthy AI in sensitive domains.</description>
      <author>example@mail.com (Noor Islam S. Mohammad)</author>
      <guid isPermaLink="false">2510.12957v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment</title>
      <link>http://arxiv.org/abs/2510.12927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedGTEA框架，用于联邦增量学习，通过高斯任务嵌入和实现对任务特定知识和模型不确定性的高效捕捉，具有可扩展性和通信效率优势。&lt;h4&gt;背景&lt;/h4&gt;联邦增量学习领域需要有效捕捉任务特定知识和模型不确定性，同时确保可扩展性和通信效率。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够捕捉任务特定知识和模型不确定性，同时保持可扩展性和通信效率的联邦学习框架。&lt;h4&gt;方法&lt;/h4&gt;客户端使用Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，编码任务知识并解决统计异构性；服务器端利用2-Wasserstein距离衡量任务间差距，通过Wasserstein损失强制任务间分离，同时保护任务级隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在多个流行数据集上的实证评估显示，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;FedGTEA框架在联邦增量学习任务中表现优异，能够有效处理任务特定知识、模型不确定性，同时保持可扩展性和通信效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种联邦增量学习的新框架，称为联邦高斯任务嵌入与对齐（FedGTEA）。FedGTEA旨在以可扩展且通信高效的方式捕捉任务特定知识和模型不确定性。在客户端，Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，这些嵌入编码任务知识，解决统计异构性问题，并量化数据不确定性。重要的是，CATE保持固定参数大小，无论任务数量如何，这确保了长任务序列的可扩展性。在服务器端，FedGTEA利用2-Wasserstein距离来衡量高斯嵌入之间的任务间差距。我们制定Wasserstein损失以强制实现任务间分离。这种概率性表述不仅增强了表示学习，还通过避免直接传输潜在嵌入来保护任务级隐私，符合联邦学习中的隐私约束。在流行数据集上的大量实证评估表明，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有的强大基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel framework for Federated Class Incremental Learning,called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA isdesigned to capture task-specific knowledge and model uncertainty in a scalableand communication-efficient manner. At the client side, theCardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed taskembeddings that encode task knowledge, address statistical heterogeneity, andquantify data uncertainty. Importantly, CATE maintains a fixed parameter sizeregardless of the number of tasks, which ensures scalability across long tasksequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance tomeasure inter-task gaps between Gaussian embeddings. We formulate theWasserstein loss to enforce inter-task separation. This probabilisticformulation not only enhances representation learning but also preservestask-level privacy by avoiding the direct transmission of latent embeddings,aligning with the privacy constraints in federated learning. Extensiveempirical evaluations on popular datasets demonstrate that FedGTEA achievessuperior classification performance and significantly mitigates forgetting,consistently outperforming strong existing baselines.</description>
      <author>example@mail.com (Haolin Li, Hoda Bidkhori)</author>
      <guid isPermaLink="false">2510.12927v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model</title>
      <link>http://arxiv.org/abs/2510.09764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProtoMM，一种新的自监督学习框架，用于多模态时间序列数据建模，特别是在生物信号处理方面。该方法通过引入共享原型字典，将不同模态锚定在共同的嵌入空间中，克服了现有对比学习方法的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态时间序列数据建模对于捕捉系统级动态至关重要，特别是在生物信号领域，如ECG、PPG、EDA和加速度计等多种模态提供了关于相互关联生理过程的互补视角。尽管最近的自监督学习(SSL)进展已经改善了单模态表征学习，但现有的多模态方法往往依赖于CLIP风格的对比目标函数，这些方法容易过拟合到容易对齐的特征，并将有效的跨模态关系误分类为负样本，导致碎片化和不可泛化的嵌入。&lt;h4&gt;目的&lt;/h4&gt;克服现有多模态自监督学习方法中的局限性，特别是对比学习方法中存在的问题，如过拟合和对有效跨模态关系的错误分类。&lt;h4&gt;方法&lt;/h4&gt;提出ProtoMM，一种新的自监督学习框架，引入共享原型字典来将异构模态锚定在共同的嵌入空间中。通过围绕共享原型聚类表征，而不是显式的负采样，该方法能够捕获模态间的互补信息，并为生理信号提供连贯的'通用语言'。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用ProtoMM开发的Pulse Motion基础模型优于仅使用对比方法和先前的多模态SSL方法；2. 该方法实现了最先进的性能；3. 提供了更好的学习特征可解释性。&lt;h4&gt;结论&lt;/h4&gt;ProtoMM框架有效解决了多模态生物信号处理中的关键挑战，通过共享原型字典方法，不仅提高了性能，还增强了特征的可解释性，为生理信号处理提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;对多模态时间序列数据进行建模对于捕捉系统级动态至关重要，特别是在生物信号领域，如ECG、PPG、EDA和加速度计等多种模态提供了关于相互关联生理过程的互补视角。尽管最近的自监督学习(SSL)进展已经改善了单模态表征学习，但现有的多模态方法往往依赖于CLIP风格的对比目标函数，这些方法容易过拟合到容易对齐的特征，并将有效的跨模态关系误分类为负样本，导致碎片化和不可泛化的嵌入。为了克服这些局限性，我们提出了ProtoMM，一种新的SSL框架，引入共享原型字典将异构模态锚定在共同的嵌入空间中。通过围绕共享原型聚类表征而不是显式的负采样，我们的方法捕获了模态间的互补信息，并为生理信号提供了连贯的'通用语言'。在这项工作中，我们专注于使用ProtoMM开发Pulse Motion基础模型，并证明我们的方法优于仅使用对比方法和先前的多模态SSL方法，在实现最先进性能的同时，提供了更好的学习特征可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling multi-modal time-series data is critical for capturing system-leveldynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,and accelerometry provide complementary perspectives on interconnectedphysiological processes. While recent self-supervised learning (SSL) advanceshave improved unimodal representation learning, existing multi-modal approachesoften rely on CLIP-style contrastive objectives that overfit to easily alignedfeatures and misclassify valid cross-modal relationships as negatives,resulting in fragmented and non-generalizable embeddings. To overcome theselimitations, we propose ProtoMM, a novel SSL framework that introduces a sharedprototype dictionary to anchor heterogeneous modalities in a common embeddingspace. By clustering representations around shared prototypes rather thanexplicit negative sampling, our method captures complementary informationacross modalities and provides a coherent "common language" for physiologicalsignals. In this work, we focus on developing a Pulse Motion foundation modelwith ProtoMM and demonstrate that our approach outperforms contrastive-only andprior multimodal SSL methods, achieving state-of-the-art performance whileoffering improved interpretability of learned features.</description>
      <author>example@mail.com (Wanting Mao, Maxwell A Xu, Harish Haresamudram, Mithun Saha, Santosh Kumar, James Matthew Rehg)</author>
      <guid isPermaLink="false">2510.09764v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
  <item>
      <title>VideoLucy: Deep Memory Backtracking for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.12422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS-2025 Accepted Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoLucy是一种深度记忆回溯框架，用于解决长视频理解中的时序上下文捕捉和关键信息保留问题，通过分层记忆结构和智能体迭代回溯机制实现优越性能。&lt;h4&gt;背景&lt;/h4&gt;基于智能体的系统利用大型语言模型进行关键信息检索和整合已成为长视频理解的有前景的方法，但面临两大挑战：难以捕捉连续帧的时序上下文，以及稀疏帧采样可能导致丢弃关键信息。&lt;h4&gt;目的&lt;/h4&gt;克服现有长视频理解系统的局限性，提出VideoLucy框架以有效捕捉时序上下文并保留关键信息。&lt;h4&gt;方法&lt;/h4&gt;VideoLucy采用受人类从粗到细回忆过程启发的分层记忆结构，具有渐进粒度，在不同层次深度明确定义记忆的细节级别和时序范围。通过基于智能体的迭代回溯机制，系统性地挖掘视频范围内、问题相关的深度记忆，直到收集到足够信息提供自信答案。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明VideoLucy的优越性，基于开源模型构建的VideoLucy在多个长视频理解基准上显著优于最先进方法，性能甚至超过了最新的专有模型如GPT-4o。&lt;h4&gt;结论&lt;/h4&gt;VideoLucy通过创新的深度记忆回溯框架有效解决了长视频理解中的关键挑战，同时引入EgoMem基准用于全面评估模型能力，代码和数据集将公开。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，利用大型语言模型进行关键信息检索和整合的智能体系统已成为长视频理解的一种有前景的方法。然而，这些系统面临两大挑战。首先，它们通常在单帧上进行建模和推理，难以捕捉连续帧的时序上下文。其次，为降低密集帧级标注的成本，它们采用稀疏帧采样，这可能会丢弃关键信息。为克服这些局限性，我们提出了VideoLucy，一种用于长视频理解的深度记忆回溯框架。受人类从粗到细回忆过程的启发，VideoLucy采用具有渐进粒度的分层记忆结构。该结构在不同层次深度明确定义了记忆的细节级别和时序范围。通过基于智能体的迭代回溯机制，VideoLucy系统性地挖掘视频范围内、问题相关的深度记忆，直到收集到足够信息以提供自信的答案。这种设计能够有效理解连续帧的时序关系，同时保留关键细节。此外，我们引入了EgoMem，一个用于长视频理解的新基准。EgoMem旨在全面评估模型理解随时间展开的复杂事件和捕捉极长视频中细粒度细节的能力。大量实验证明了VideoLucy的优越性。基于开源模型构建的VideoLucy在多个长视频理解基准上显著优于最先进的方法，性能甚至超过了最新的专有模型如GPT-4o。我们的代码和数据集将在https://videolucy.github.io公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have shown that agent-based systems leveraging large languagemodels (LLMs) for key information retrieval and integration have emerged as apromising approach for long video understanding. However, these systems facetwo major challenges. First, they typically perform modeling and reasoning onindividual frames, struggling to capture the temporal context of consecutiveframes. Second, to reduce the cost of dense frame-level captioning, they adoptsparse frame sampling, which risks discarding crucial information. To overcomethese limitations, we propose VideoLucy, a deep memory backtracking frameworkfor long video understanding. Inspired by the human recollection process fromcoarse to fine, VideoLucy employs a hierarchical memory structure withprogressive granularity. This structure explicitly defines the detail level andtemporal scope of memory at different hierarchical depths. Through anagent-based iterative backtracking mechanism, VideoLucy systematically minesvideo-wide, question-relevant deep memories until sufficient information isgathered to provide a confident answer. This design enables effective temporalunderstanding of consecutive frames while preserving critical details. Inaddition, we introduce EgoMem, a new benchmark for long video understanding.EgoMem is designed to comprehensively evaluate a model's ability to understandcomplex events that unfold over time and capture fine-grained details inextremely long videos. Extensive experiments demonstrate the superiority ofVideoLucy. Built on open-source models, VideoLucy significantly outperformsstate-of-the-art methods on multiple long video understanding benchmarks,achieving performance even surpassing the latest proprietary models such asGPT-4o. Our code and dataset will be made publicly athttps://videolucy.github.io</description>
      <author>example@mail.com (Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao)</author>
      <guid isPermaLink="false">2510.12422v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling</title>
      <link>http://arxiv.org/abs/2510.12385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 7 figures and 5 tables in the main paper and one figure and  table in the appendix. To be published in Computer Vision and Image  Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了STORM-PSR模型，一种双流框架的程序步骤识别方法，通过结合空间和时间特征，有效解决了物体部分遮挡情况下的步骤识别问题。&lt;h4&gt;背景&lt;/h4&gt;现有的程序步骤识别模型仅依靠检测单个视频帧中的装配对象状态，忽略了时间特征，导致模型鲁棒性和准确性有限，尤其在物体部分遮挡时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种能够处理部分遮挡情况下的程序步骤识别方法，提高识别准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出STORM-PSR（时空遮挡弹性建模程序步骤识别）双流框架：装配状态检测流在物体无遮挡时有效工作；时空流捕捉空间和时间特征，即使在部分遮挡下也能识别步骤完成情况。时空流包含使用弱监督方法预训练的空间编码器和基于transformer的时间编码器。&lt;h4&gt;主要发现&lt;/h4&gt;在MECCANO和IndustReal数据集上评估，与之前方法相比，分别减少了11.2%和26.1%的实际和预测装配步骤完成之间的平均延迟。这种改进主要由时空流实现，它不依赖物体的无遮挡视图。&lt;h4&gt;结论&lt;/h4&gt;STORM-PSR能有效处理部分遮挡情况下的程序步骤识别问题，显著提高了识别准确性和及时性。相关代码和数据集已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;程序步骤识别旨在识别视频中程序任务中所有正确完成的步骤及其顺序。现有的最先进模型仅依靠检测单个视频帧中的装配对象状态。通过忽略时间特征，模型的鲁棒性和准确性受到限制，特别是当物体部分遮挡时。为克服这些限制，我们提出了用于程序步骤识别的时空遮挡弹性建模（STORM-PSR），这是一个用于程序步骤识别的双流框架，同时利用空间和时间特征。装配状态检测流在物体无遮挡视图下有效工作，而时空流捕捉空间和时间特征，即使在部分遮挡下也能识别步骤完成情况。该流包括一个空间编码器，使用新颖的弱监督方法预训练以捕获有意义的空间表示，以及一个基于transformer的时间编码器，学习这些空间特征随时间的关系。STORM-PSR在MECCANO和IndustReal数据集上进行了评估，与之前的方法相比，分别减少了11.2%和26.1%的实际和预测装配步骤完成之间的平均延迟。我们证明这种延迟减少是由时空流驱动的，它不依赖物体的无遮挡视图来推断完成的步骤。STORM-PSR的代码以及新标注的MECCANO标签已在https://timschoonbeek.github.io/stormpsr公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Procedure step recognition (PSR) aims to identify all correctly completedsteps and their sequential order in videos of procedural tasks. The existingstate-of-the-art models rely solely on detecting assembly object states inindividual video frames. By neglecting temporal features, model robustness andaccuracy are limited, especially when objects are partially occluded. Toovercome these limitations, we propose Spatio-Temporal Occlusion-ResilientModeling for Procedure Step Recognition (STORM-PSR), a dual-stream frameworkfor PSR that leverages both spatial and temporal features. The assembly statedetection stream operates effectively with unobstructed views of the object,while the spatio-temporal stream captures both spatial and temporal features torecognize step completions even under partial occlusion. This stream includes aspatial encoder, pre-trained using a novel weakly supervised approach tocapture meaningful spatial representations, and a transformer-based temporalencoder that learns how these spatial features relate over time. STORM-PSR isevaluated on the MECCANO and IndustReal datasets, reducing the average delaybetween actual and predicted assembly step completions by 11.2% and 26.1%,respectively, compared to prior methods. We demonstrate that this reduction indelay is driven by the spatio-temporal stream, which does not rely onunobstructed views of the object to infer completed steps. The code forSTORM-PSR, along with the newly annotated MECCANO labels, is made publiclyavailable at https://timschoonbeek.github.io/stormpsr .</description>
      <author>example@mail.com (Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen)</author>
      <guid isPermaLink="false">2510.12385v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding</title>
      <link>http://arxiv.org/abs/2510.12160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种状态空间提示方法，通过结合帧内和帧间提示来有效捕捉和传播视频中的时空信息，显著提升了视频分类性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的状态空间模型在视频分类方面展现出巨大潜力，它们以线性复杂度顺序压缩视频中的视觉标记，提高处理效率的同时保持高性能。提示学习被用于将这些强大模型高效适配到下游任务。&lt;h4&gt;目的&lt;/h4&gt;解决顺序压缩的视觉提示标记无法充分捕捉视频时空上下文信息的问题，以增强状态压缩模型中空间信息在帧内的传播以及时间信息在帧间的提取。&lt;h4&gt;方法&lt;/h4&gt;提出状态空间提示方法，结合帧内和帧间提示聚合和传播视频中的关键时空信息。具体包括帧内聚合模块和帧间扩散模块，通过自适应平衡和压缩帧内及帧间的关键时空信息，以互补方式有效传播判别性信息。&lt;h4&gt;主要发现&lt;/h4&gt;在四个视频基准数据集上的实验表明，该方法平均比现有最先进方法高出2.76%，同时减少了微调参数的开销。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合帧内和帧间提示，有效解决了状态空间模型在视频理解中时空信息捕捉不足的问题，在保持高性能的同时提高了处理效率并减少了微调参数。&lt;h4&gt;翻译&lt;/h4&gt;最近，预训练的状态空间模型在视频分类方面显示出巨大潜力，它们以线性复杂度顺序压缩视频中的视觉标记，从而提高视频数据的处理效率同时保持高性能。为了将强大的预训练模型应用于下游任务，提示学习被提出，只需少量微调参数即可实现高效的下游任务适应。然而，顺序压缩的视觉提示标记无法捕捉视频中的空间和时间上下文信息，从而限制了状态压缩模型内空间信息在视频帧内的有效传播以及帧间时间信息的提取和判别性信息的提取。为解决上述问题，我们提出了一种用于视频理解的状态空间提示方法，该方法结合帧内和帧间提示来聚合和传播视频中的关键时空信息。具体来说，设计了帧内聚合模块来聚合每个帧内的空间关键信息。此外，设计了帧间扩散模块来传播不同帧间的判别性时空信息。通过自适应平衡和压缩帧内及帧间的关键时空信息，我们的方法以互补方式有效传播视频中的判别性信息。在四个视频基准数据集上的大量实验验证了我们的方法平均比现有最先进方法高出2.76%，同时减少了微调参数的开销。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, pre-trained state space models have shown great potential for videoclassification, which sequentially compresses visual tokens in videos withlinear complexity, thereby improving the processing efficiency of video datawhile maintaining high performance. To apply powerful pre-trained models todownstream tasks, prompt learning is proposed to achieve efficient downstreamtask adaptation with only a small number of fine-tuned parameters. However, thesequentially compressed visual prompt tokens fail to capture the spatial andtemporal contextual information in the video, thus limiting the effectivepropagation of spatial information within a video frame and temporalinformation between frames in the state compression model and the extraction ofdiscriminative information. To tackle the above issue, we proposed a StateSpace Prompting (SSP) method for video understanding, which combinesintra-frame and inter-frame prompts to aggregate and propagate keyspatiotemporal information in the video. Specifically, an Intra-Frame Gathering(IFG) module is designed to aggregate spatial key information within eachframe. Besides, an Inter-Frame Spreading (IFS) module is designed to spreaddiscriminative spatio-temporal information across different frames. Byadaptively balancing and compressing key spatio-temporal information within andbetween frames, our SSP effectively propagates discriminative information invideos in a complementary manner. Extensive experiments on four video benchmarkdatasets verify that our SSP significantly outperforms existing SOTA methods by2.76% on average while reducing the overhead of fine-tuning parameters.</description>
      <author>example@mail.com (Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua)</author>
      <guid isPermaLink="false">2510.12160v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis</title>
      <link>http://arxiv.org/abs/2510.11907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种双模型框架，结合VideoLLaMA和Qwen2.5-VL的优势，通过分离训练字幕生成和视觉问答任务来最小化任务干扰，提高交通安全分析能力。实验证明该方法在AI城市挑战赛中表现优异，分离训练策略优于联合训练。&lt;h4&gt;背景&lt;/h4&gt;交通安全分析需要复杂的视频理解技术，以捕获细粒度的行为模式并生成全面的描述用于事故预防。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效进行交通安全分析的框架，通过结合不同模型的优势提高视频理解和分析能力，从而更好地捕捉交通行为模式和预防事故。&lt;h4&gt;方法&lt;/h4&gt;提出独特的双模型框架，战略性地利用VideoLLaMA和Qwen2.5-VL的互补优势。核心思路是分离字幕生成和视觉问答任务的训练，以最小化任务干扰并使每个模型能够更有效地专业化。通过在WTS数据集上进行大量实验评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;VideoLLaMA在时间推理方面特别有效，达到1.1001的CIDEr分数；Qwen2.5-VL在视觉理解方面表现出色，VQA准确率达到60.80%；该方法在2025年AI城市挑战赛第二赛道中达到45.7572的S2分数，排名第10位；分离训练策略比联合训练在VQA准确率上提高8.6%，同时保持字幕生成质量。&lt;h4&gt;结论&lt;/h4&gt;通过分离训练策略，双模型框架能够有效地结合不同模型的专长，提高交通安全分析的性能。VideoLLaMA擅长时间推理，而Qwen2.5-VL在视觉理解方面表现优异，为交通安全视频分析提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;交通安全分析需要复杂的视频理解来捕获细粒度的行为模式并生成全面的描述以预防事故。在这项工作中，我们提出了一种独特的双模型框架，通过针对特定任务的优化，战略性地利用VideoLLaMA和Qwen2.5-VL的互补优势来解决这一问题。我们方法的核心见解是，分离字幕生成和视觉问答任务的训练可以最小化任务干扰，并使每个模型能够更有效地专业化。实验结果表明，VideoLLaMA在时间推理方面特别有效，达到1.1001的CIDEr分数，而Qwen2.5-VL在视觉理解方面表现出色，VQA准确率为60.80%。通过在WTS数据集上的大量实验，我们的方法在2025年AI城市挑战赛第二赛道中实现了45.7572的S2分数，在挑战排行榜上排名第10位。消融研究验证了我们的分离训练策略在VQA准确率上比联合训练提高了8.6%，同时保持了字幕生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic safety analysis requires complex video understanding to capturefine-grained behavioral patterns and generate comprehensive descriptions foraccident prevention. In this work, we present a unique dual-model frameworkthat strategically utilizes the complementary strengths of VideoLLaMA andQwen2.5-VL through task-specific optimization to address this issue. The coreinsight behind our approach is that separating training for captioning andvisual question answering (VQA) tasks minimizes task interference and allowseach model to specialize more effectively. Experimental results demonstratethat VideoLLaMA is particularly effective in temporal reasoning, achieving aCIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with aVQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, ourmethod achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,placing 10th on the challenge leaderboard. Ablation studies validate that ourseparate training strategy outperforms joint training by 8.6\% in VQA accuracywhile maintaining captioning quality.</description>
      <author>example@mail.com (Blessing Agyei Kyem, Neema Jakisa Owor, Andrews Danyo, Joshua Kofi Asamoah, Eugene Denteh, Tanner Muturi, Anthony Dontoh, Yaw Adu-Gyamfi, Armstrong Aboah)</author>
      <guid isPermaLink="false">2510.11907v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Guided Visual Perception for Audio-Visual Navigation</title>
      <link>http://arxiv.org/abs/2510.11760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main paper (6 pages). Accepted for publication by International  Conference on Virtual Reality and Visualization 2025 (ICVRV 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视听具身导航(AVN)旨在使智能体能够使用听觉线索在未知3D环境中自主导航到声源。当前方法在已知声源上表现良好，但在面对新声源时泛化能力差。AGVP框架通过跨模态对齐和区域重加权解决了这一问题，提高了导航效率和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前的视听具身导航方法在分布内的声源上表现良好，但在跨声源泛化方面表现较差，当遇到未听过的声音或未见过的环境时，导航成功率大幅下降，搜索路径变得过长。&lt;h4&gt;目的&lt;/h4&gt;解决当前AVN方法在跨声源泛化方面的局限性，提高智能体在面对新声源时的导航效率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出AGVP框架，该框架首先通过音频自注意力提取全局听觉上下文，然后使用此上下文作为查询来引导视觉特征注意力，在特征级别突出显示与声源相关的区域，随后进行时间建模和策略优化。设计以可解释的跨模态对齐和区域重加权为中心，减少对特定声学指纹的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;AGVP框架通过将声音从策略可记忆的声学指纹线索转换为空间引导，解决了当前方法中缺乏听觉信号与相应视觉区域之间明确对齐机制的问题，避免了策略在训练期间记忆虚假的'声学指纹-场景'相关性。&lt;h4&gt;结论&lt;/h4&gt;AGVP框架提高了导航效率和鲁棒性，同时对先前未听过的声音实现了跨场景的优越泛化，减少了方法对特定声学指纹的依赖。&lt;h4&gt;翻译&lt;/h4&gt;视听具身导航旨在使智能体能够使用听觉线索在未知3D环境中自主导航到声源。虽然当前的AVN方法在分布内的声源上表现出色，但在跨声源泛化方面表现不佳：当智能体遇到未听过的声音或未见过的环境时，导航成功率大幅下降，搜索路径变得过长。这种限制源于缺乏听觉信号与相应视觉区域之间的明确对齐机制。策略倾向于在训练期间记忆虚假的'声学指纹-场景'相关性，当遇到新的声源时导致盲目探索。为解决此问题，我们提出了AGVP框架，将声音从策略可记忆的声学指纹线索转换为空间引导。该框架首先通过音频自注意力提取全局听觉上下文，然后使用此上下文作为查询来引导视觉特征注意力，在特征级别突出显示与声源相关的区域。随后进行时间建模和策略优化。这种以可解释的跨模态对齐和区域重加权为中心的设计，减少了对特定声学指纹的依赖。实验结果表明，AGVP提高了导航效率和鲁棒性，同时对先前未听过的声音实现了跨场景的优越泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决音频视觉导航(AVN)中的跨源泛化能力不足问题。当前方法在训练时听过的声音源上表现很好(成功率超过95%)，但在遇到未听过的声音源或未见过的环境时，导航成功率急剧下降，搜索路径变得过长。这个问题在现实中非常重要，因为现实世界中的声音源是多样化的，我们无法预训练智能体处理所有可能的声音。在紧急情况下(如火灾中求救声)，智能体需要能够有效定位未知声音源，提高跨场景泛化能力对于构建真正实用的自主导航系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前AVN方法的局限性：它们在策略级别连接视觉和听觉特征，缺乏声音源相关视觉区域与听觉信号之间的明确对齐，导致策略网络倾向于记忆'声纹-场景'的虚假相关性。作者从人类导航行为中获得灵感：在视觉受阻时，人类会先转向声音方向，锁定大致方向，然后关注声音可能出现的区域。基于此，作者设计了'声音优先，视觉跟随'的多模态融合机制。该方法借鉴了SoundSpaces平台、Transformer架构中的自注意力机制和引导注意力机制，以及GRU进行时间建模，同时参考了现有音频视觉导航方法如AV-WaN、SAVi等的优缺点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; AGVP框架的核心思想是将声音从策略可记忆的声纹线索转变为空间引导。在复杂视觉推理之前，使用音频上下文重新校准视觉特征图，突出显示与声音源最相关的区域，使音频决定'看哪里'，而视觉决定'如何看'。整体流程分为三阶段：1)观察阶段：智能体通过传感器获取视觉(深度图或RGB图像)和听觉(双通道频谱图)输入；2)观察编码阶段：音频特征通过自注意力构建全局上下文，然后作为查询引导视觉特征注意力，突出声音源相关区域，随后通过GRU进行时间建模；3)策略更新阶段：基于PPO的Actor-Critic头根据GRU隐藏状态生成动作分布和状态值，完成从感知到决策的闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)'声音优先，视觉跟随'的多模态融合机制，将融合从后端策略级别提升到感知特征级别；2)音频引导的视觉特征重新加权，使用音频上下文引导视觉注意力；3)通过可解释的跨模态对齐和区域重新加权，减少对特定声纹的依赖。相比之前工作，AGVP在特征级别实现明确的音频视觉对齐，而非传统方法在策略级别的简单连接；不再依赖'声纹-场景'的记忆映射，而是将声音转变为空间引导；在未听过的声音源任务上表现显著优于现有方法，如Replica数据集上实现66.5%的成功率，比之前最佳方法提高约15个百分点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AGVP框架通过在特征级别实现音频到视觉的明确对齐，将声音从可记忆的声纹线索转变为空间引导，显著提升了音频视觉导航系统在未知声音源和未见环境中的泛化能力和导航效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Embodied Navigation aims to enable agents to autonomouslynavigate to sound sources in unknown 3D environments using auditory cues. Whilecurrent AVN methods excel on in-distribution sound sources, they exhibit poorcross-source generalization: navigation success rates plummet and search pathsbecome excessively long when agents encounter unheard sounds or unseenenvironments. This limitation stems from the lack of explicit alignmentmechanisms between auditory signals and corresponding visual regions. Policiestend to memorize spurious \enquote{acoustic fingerprint-scenario} correlationsduring training, leading to blind exploration when exposed to novel soundsources. To address this, we propose the AGVP framework, which transforms soundfrom policy-memorable acoustic fingerprint cues into spatial guidance. Theframework first extracts global auditory context via audio self-attention, thenuses this context as queries to guide visual feature attention, highlightingsound-source-related regions at the feature level. Subsequent temporal modelingand policy optimization are then performed. This design, centered oninterpretable cross-modal alignment and region reweighting, reduces dependencyon specific acoustic fingerprints. Experimental results demonstrate that AGVPimproves both navigation efficiency and robustness while achieving superiorcross-scenario generalization on previously unheard sounds.</description>
      <author>example@mail.com (Yi Wang, Yinfeng Yu, Fuchun Sun, Liejun Wang, Wendong Zheng)</author>
      <guid isPermaLink="false">2510.11760v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding</title>
      <link>http://arxiv.org/abs/2510.12749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为SPORTS的新型框架，通过紧密集成视频全景分割、视觉里程计和场景渲染任务，实现整体场景理解，解决了现有方法中的分割不足、动态物体干扰、传感器数据稀疏和视角限制等问题。&lt;h4&gt;背景&lt;/h4&gt;场景感知、理解和模拟是具身AI代理的基础技术，但现有解决方案仍存在分割不足、动态物体干扰、传感器数据稀疏和视角限制等挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的框架，通过整合视频全景分割、视觉里程计和场景渲染任务，实现更准确的整体场景理解。&lt;h4&gt;方法&lt;/h4&gt;1) VPS部分：设计基于自适应注意力的几何融合机制，通过引入姿态、深度和光流模态对齐跨帧特征，并集成后匹配策略改进身份跟踪；2) VO部分：结合VPS的全景分割结果和光流图，提高动态物体置信度估计，增强相机姿态估计精度和深度图生成完整性；3) SR部分：将稀疏点云转换为神经场，合成高保真RGB视图和双重视图。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的实验表明，基于注意力的特征融合在里程计、跟踪、分割和新视角合成任务上优于大多数现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;SPORTS框架通过迭代和统一的视角整合多种任务，有效解决了场景理解中的多个挑战性问题，提升了整体性能。&lt;h4&gt;翻译&lt;/h4&gt;场景感知、理解和模拟是具身AI代理的基础技术，而现有解决方案仍然容易受到分割不足、动态物体干扰、传感器数据稀疏和视角限制等问题的影响。本文提出了一种名为SPORTS的新型框架，通过将视频全景分割、视觉里程计和场景渲染任务紧密集成到一个迭代和统一的视角中，实现整体场景理解。首先，VPS设计了一种基于自适应注意力的几何融合机制，通过引入姿态、深度和光流模态来对齐跨帧特征，自动调整不同解码阶段的特征图，并集成了后匹配策略以改进身份跟踪。在VO中，VPS的全景分割结果与光流图相结合，提高了动态物体的置信度估计，通过基于学习的方法增强了相机姿态估计的精度和深度图生成的完整性。此外，SR的点渲染受益于VO，将稀疏点云转换为神经场，以合成高保真的RGB视图和双重视图。在三个公共数据集上的大量实验证明，我们的基于注意力的特征融合在里程计、跟踪、分割和新视角合成任务上优于大多数现有的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有场景感知、理解和模拟技术中的四个关键问题：分割缺陷、动态物体干扰、传感器数据稀疏和视角限制。这些问题在现实中非常重要，因为随着自动驾驶车辆、四足机器人和人形机器人的普及，对城市场景的整体理解能力对这些智能体执行感知、定位和碰撞避免等任务至关重要。此外，整体场景理解可用于创建城市环境的数字孪生，作为智能体学习和验证意外情况的模拟平台，从而以较低成本提高自动驾驶安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法已开始解决整体场景理解中的信息孤岛问题，但VO和VPS性能仍需改进。他们注意到动态物体处理和稀疏点云重建是关键挑战。设计上，作者借鉴了Video K-Net的核学习机制、PVO的集成方法、DROID-SLAM的优化策略以及READ的点云渲染方法，但通过引入基于注意力的自适应几何融合机制和后匹配策略进行创新，解决了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出名为SPORTS的框架，通过紧密集成视频全景分割(VPS)、视觉里程计(VO)和场景渲染(SR)任务，实现迭代统一的城市场景理解。整体流程为：1)输入单目视频序列；2)VPS模块利用基于注意力的几何融合机制对齐跨帧特征，并加入后匹配策略提高跟踪质量；3)VO模块利用VPS结果增强动态物体置信度估计，提高姿态估计精度；4)SR模块将高精度姿态保证的稀疏点云转换为神经场，合成高保真场景；5)输出稀疏点云地图、相机姿态和合成的新场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次统一视觉里程计、渲染、物体跟踪和全景分割任务的框架；2)提出基于注意力的几何融合机制和后匹配策略，提高分割和跟踪质量3.07%；3)提出两阶段全景感知流感知深度传播模块，增强基于学习的视觉里程计；4)利用基础模型创建更多评估数据集，验证泛化能力。相比之前工作，SPORTS更充分地考虑了相邻帧特征，采用更先进的解码网络，解决了长序列中的误差传播问题，并更好地区分了静止可移动物体和真正移动的物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPORTS通过统一视觉里程计、渲染、物体跟踪和全景分割任务，并提出基于注意力的自适应几何融合机制和后匹配策略，实现了对城市场景的高效理解和精确重建，显著提升了动态环境下的感知、定位和场景渲染性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scene perception, understanding, and simulation are fundamentaltechniques for embodied-AI agents, while existing solutions are still prone tosegmentation deficiency, dynamic objects' interference, sensor data sparsity,and view-limitation problems. This paper proposes a novel framework, namedSPORTS, for holistic scene understanding via tightly integrating Video PanopticSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks intoan iterative and unified perspective. Firstly, VPS designs an adaptiveattention-based geometric fusion mechanism to align cross-frame features viaenrolling the pose, depth, and optical flow modality, which automaticallyadjust feature maps for different decoding stages. And a post-matching strategyis integrated to improve identities tracking. In VO, panoptic segmentationresults from VPS are combined with the optical flow map to improve theconfidence estimation of dynamic objects, which enhances the accuracy of thecamera pose estimation and completeness of the depth map generation via thelearning-based paradigm. Furthermore, the point-based rendering of SR isbeneficial from VO, transforming sparse point clouds into neural fields tosynthesize high-fidelity RGB views and twin panoptic views. Extensiveexperiments on three public datasets demonstrate that our attention-basedfeature fusion outperforms most existing state-of-the-art methods on theodometry, tracking, segmentation, and novel view synthesis tasks.</description>
      <author>example@mail.com (Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang)</author>
      <guid isPermaLink="false">2510.12749v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Gaussian Semantic Field for One-shot LiDAR Global Localization</title>
      <link>http://arxiv.org/abs/2510.12101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于轻量级三层场景图的一次性激光雷达全局定位算法，具有语义消歧能力。&lt;h4&gt;背景&lt;/h4&gt;基于地标语义注册的全局定位方法相比纯几何方法已经显示出有前景的性能提升，但地标可能是重复的且可能误导对应关系的建立。&lt;h4&gt;目的&lt;/h4&gt;通过使用高斯过程群体学习到的连续函数来建模语义分布，缓解地标重复和误导性的问题。&lt;h4&gt;方法&lt;/h4&gt;将连续函数作为中间层插入到物体层和度量-语义层之间，形成三层3D场景图，作为一次性定位的轻量级高性能后端。&lt;h4&gt;主要发现&lt;/h4&gt;与离散语义标签相比，连续函数能够捕获更细粒度的地理语义信息，并为对应关系建立提供更详细的度量信息。&lt;h4&gt;结论&lt;/h4&gt;将全局定位管道命名为Outram-GSF（高斯语义场），并在公开可用数据集上进行了广泛实验，验证了其与当前最先进方法相比的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于轻量级三层场景图的一次性激光雷达全局定位算法，具有语义消歧能力。虽然基于地标语义注册的方法与纯几何方法相比已经在全局定位中显示出有前景的性能提升，但地标可能是重复的且可能误导对应关系的建立。我们提出通过使用从高斯过程群体学习到的连续函数来建模语义分布来缓解这一问题。与离散语义标签相比，连续函数能够捕获更细粒度的地理语义信息，并为对应关系建立提供更详细的度量信息。我们将这个连续函数作为中间层插入到物体层和度量-语义层之间，形成三层3D场景图，作为一次性定位的轻量级高性能后端。我们将我们的全局定位管道命名为Outram-GSF（高斯语义场），并在公开可用数据集上进行了广泛实验，验证了其与当前最先进方法相比的优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决激光雷达全局定位中的语义歧义问题，即在具有重复结构的环境（如城市街道、停车场）中，传统方法难以区分相似但位置不同的地标。这个问题在现实中非常重要，因为精确的全局定位是自动驾驶和机器人导航的基础能力，而一次性定位方法对于机器人快速重新定位至关重要，特别是在GPS信号弱或不可用的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有基于场景图的全局定位方法主要依赖语义簇的质心和拓扑连接，忽略了簇内丰富的空间语义分布。他们借鉴了3D场景图概念、Outram的分层搜索思想以及高斯过程建模方法，在此基础上创新性地引入高斯语义场作为中间层，形成三层结构。作者通过连续建模空间语义分布来区分几何相似但语义分布不同的区域，解决语义歧义问题，特别是在重复语义结构环境中提高性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高斯语义场(GSF)建模连续的语义分布，而不是使用离散的语义标签，在传统3D场景图的对象层和语义点云层之间插入GSF中间层。整体流程包括：1)生成三层3D场景图，通过稀疏高斯过程创建高斯语义场层；2)使用网格探测方法生成度量语义特征；3)基于图的子结构匹配，利用Wasserstein距离进行相似性测量；4)通过最大团过程选择内点对应关系，生成最终姿态估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入高斯语义场作为3D场景图的中间层，实现连续语义分布建模；2)基于高斯过程的概率框架学习语义分布并提供相似度度量；3)三层场景图结构作为轻量级高性能定位后端；4)语义稳定性掩码处理场景变化。相比之前工作，Outram-GSF不再仅依赖实例级质心和拓扑关系，而是捕捉簇内丰富的空间语义分布，提供更详细的度量信息，在语义歧义环境中表现更好，特别是在重复结构场景中显著提高了定位性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于高斯语义场的一次性激光雷达全局定位方法，通过连续建模空间语义分布解决了传统方法在语义歧义环境中的局限性，显著提高了在重复结构场景中的定位性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a one-shot LiDAR global localization algorithm featuring semanticdisambiguation ability based on a lightweight tri-layered scene graph. Whilelandmark semantic registration-based methods have shown promising performanceimprovements in global localization compared with geometric-only methods,landmarks can be repetitive and misleading for correspondence establishment. Wepropose to mitigate this problem by modeling semantic distributions withcontinuous functions learned from a population of Gaussian processes. Comparedwith discrete semantic labels, the continuous functions capture finer-grainedgeo-semantic information and also provide more detailed metric information forcorrespondence establishment. We insert this continuous function as the middlelayer between the object layer and the metric-semantic layer, forming atri-layered 3D scene graph, serving as a light-weight yet performant backendfor one-shot localization. We term our global localization pipeline Outram-GSF(Gaussian semantic field) and conduct a wide range of experiments on publiclyavailable data sets, validating the superior performance against the currentstate-of-the-art.</description>
      <author>example@mail.com (Pengyu Yin, Shenghai Yuan, Haozhi Cao, Xingyu Ji, Ruofei Bai, Siyu Chen, Lihua Xie)</author>
      <guid isPermaLink="false">2510.12101v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning</title>
      <link>http://arxiv.org/abs/2510.11996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper was accepted at ICCV Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个专门的空间推理框架，通过将掩模尺寸嵌入输入提示中，增强模型对物体几何和布局的理解能力，并在四个特定问题类别上进行微调，最终在AI City Challenge的Track 3中排名第四，得分为73.0606，证明了该方法在现实工业环境中空间推理的有效性。&lt;h4&gt;背景&lt;/h4&gt;大规模3D环境（如仓库）中的空间推理对视觉语言系统仍然是一个重大挑战，主要困难包括场景杂乱、遮挡以及需要精确的空间理解。现有模型在这样的环境中往往难以泛化，因为它们严重依赖局部外观，缺乏明确的空间基础。&lt;h4&gt;目的&lt;/h4&gt;为2025年AI City Challenge的Track 3中介绍的Physical AI Spatial Intelligence Warehouse数据集引入一个专门的空间推理框架，以改善视觉语言系统在复杂3D环境中的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;通过将掩模尺寸以边界框坐标的形式直接嵌入输入提示中，增强空间理解能力，使模型能够对物体几何和布局进行推理。在四个问题类别上进行微调框架：距离估计、物体计数、多选基础和空间关系推理，使用任务特定的监督。为了进一步提高与评估系统的一致性，将标准化答案附加到训练集中的GPT响应中。&lt;h4&gt;主要发现&lt;/h4&gt;综合管道最终得分为73.0606，在公共排行榜上总体排名第四。这些结果表明，结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的，通过嵌入掩模尺寸和任务特定微调，可以显著提升视觉语言系统在复杂3D环境中的空间推理能力。&lt;h4&gt;翻译&lt;/h4&gt;在大规模3D环境（如仓库）中进行空间推理对于视觉语言系统来说仍然是一个重大挑战，因为场景杂乱、遮挡以及需要精确的空间理解。现有模型在这样的环境中往往难以泛化，因为它们严重依赖局部外观，缺乏明确的空间基础。在这项工作中，我们为2025年AI City Challenge的Track 3中介绍的Physical AI Spatial Intelligence Warehouse数据集引入了一个专门的空间推理框架。我们的方法通过将掩模尺寸以边界框坐标的形式直接嵌入输入提示中，增强空间理解能力，使模型能够对物体几何和布局进行推理。我们在四个问题类别上微调框架：距离估计、物体计数、多选基础和空间关系推理，使用任务特定的监督。为了进一步提高与评估系统的一致性，将标准化答案附加到训练集中的GPT响应中。我们的综合管道最终得分为73.0606，在公共排行榜上总体排名第四。这些结果表明，结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型3D环境（如仓库）中的空间推理挑战。视觉-语言系统在这些场景中面临杂乱环境、遮挡和需要精确空间理解的问题，现有模型往往难以泛化，因为它们过度依赖局部外观而缺乏明确的空间基础。这个问题在工业环境中至关重要，因为空间推理对仓库导航、库存管理和安全监控等任务必不可少，而这些环境具有不规则结构、多样物体类型和频繁遮挡等特点，需要系统能够同时捕捉细粒度视觉细节和场景的广泛空间组织。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有视觉-语言模型在空间推理方面的局限性，特别是在处理2D图像和缺乏几何基础方面。他们分析了仓库环境的复杂性，认为需要结合对象检测和空间理解的方法。作者借鉴了SpatialBot框架，因为它已证明在空间智能方面的优越性，能够将单目深度估计生成的深度信息整合到RGB输入中。在此基础上，作者设计了提示级别的增强，将区域掩码编码为边界框坐标，提供结构化空间线索，并添加标准化答案格式确保与评估系统一致。他们还采用了LoRA微调技术来减少训练时间和内存需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在提示中嵌入边界框坐标和掩码尺寸，将几何信息直接注入到视觉-语言模型的输入中，结合深度信息增强模型对空间关系的理解。整体实现流程包括：1) 基于SpatialBot的模型架构，处理RGB和深度输入；2) 提示增强，在输入中注入边界框坐标和区域ID；3) 答案标准化，添加模板后缀确保格式一致；4) 在仓库数据集上进行任务特定微调，使用LoRA技术优化训练过程。这种方法使模型能够推理物体几何和布局，提高在复杂仓库环境中的空间推理能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 专门针对3D工业环境的空间问答框架；2) 提示增强方法，将物体级别的几何特征嵌入提示；3) 在仓库数据集上扩展SpatialBot架构；4) 输出标准化模块确保预测一致性；5) 在AI City Challenge中取得第四名的优异表现。相比之前工作，该方法明确注入几何信息而非依赖纯2D图像，专门针对仓库环境复杂性优化，通过提示工程提供更精确的空间定位，并通过答案标准化确保输出一致性，超越了仅使用语言输入引导空间预测的传统方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过在提示中嵌入边界框坐标和深度信息，并应用答案标准化技术，显著提升了视觉-语言模型在复杂仓库环境中的细粒度空间推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial reasoning in large-scale 3D environments such as warehouses remains asignificant challenge for vision-language systems due to scene clutter,occlusions, and the need for precise spatial understanding. Existing modelsoften struggle with generalization in such settings, as they rely heavily onlocal appearance and lack explicit spatial grounding. In this work, weintroduce a dedicated spatial reasoning framework for the Physical AI SpatialIntelligence Warehouse dataset introduced in the Track 3 2025 AI CityChallenge. Our approach enhances spatial comprehension by embedding maskdimensions in the form of bounding box coordinates directly into the inputprompts, enabling the model to reason over object geometry and layout. Wefine-tune the framework across four question categories namely: DistanceEstimation, Object Counting, Multi-choice Grounding, and Spatial RelationInference using task-specific supervision. To further improve consistency withthe evaluation system, normalized answers are appended to the GPT responsewithin the training set. Our comprehensive pipeline achieves a final score of73.0606, placing 4th overall on the public leaderboard. These resultsdemonstrate the effectiveness of structured prompt enrichment and targetedoptimization in advancing spatial reasoning for real-world industrialenvironments.</description>
      <author>example@mail.com (Tanner Muturi, Blessing Agyei Kyem, Joshua Kofi Asamoah, Neema Jakisa Owor, Richard Dyzinela, Andrews Danyo, Yaw Adu-Gyamfi, Armstrong Aboah)</author>
      <guid isPermaLink="false">2510.11996v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.11340v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT3D是一个可扩展的零样本框架，将静态3D场景转换为具有一致几何形状的模拟就绪交互式副本，可直接用于各种下游任务，通过四个主要贡献实现高效处理，为具身智能研究提供了实用工具。&lt;h4&gt;背景&lt;/h4&gt;交互式3D场景对具身智能日益重要，但由于注释部分分割、运动类型和运动轨迹的劳动密集型过程，现有数据集仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出REACT3D框架，将静态3D场景转换为模拟就绪的交互式副本，使其能够直接用于各种下游任务，并降低关节场景理解大规模研究的门槛。&lt;h4&gt;方法&lt;/h4&gt;包括四个主要贡献：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在各种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了框架的有效性，并为可扩展的交互式场景生成提供了实用基础。&lt;h4&gt;结论&lt;/h4&gt;REACT3D为可扩展的交互式场景生成提供了实用基础，从而降低了关节场景理解大规模研究的门槛。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D场景对具身智能日益重要，但由于注释部分分割、运动类型和运动轨迹的劳动密集型过程，现有数据集仍然有限。我们提出了REACT3D，一个可扩展的零样本框架，将静态3D场景转换为具有一致几何形状的模拟就绪交互式副本，可直接用于各种下游任务。我们的贡献包括：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。我们在各种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了我们框架的有效性，并为可扩展的交互式场景生成提供了实用基础，从而降低了关节场景理解大规模研究的门槛。我们的项目页面是https://react3d.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将静态3D场景转换为交互式物理3D场景的问题。这个问题很重要，因为现有数据集在标注部分分割、运动类型和运动轨迹方面非常耗时，限制了交互式3D场景的发展。而交互式3D场景对虚拟现实、游戏制作和机器人系统训练等应用至关重要，这些应用需要大量既能提供照片级真实感渲染，又能支持物理上合理交互的3D数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到静态3D场景生成已相对成熟，但交互式场景生成仍处于早期阶段。他们提出利用视觉基础模型和视觉语言模型进行零样本转换，无需额外数据收集或计算密集型生成。他们借鉴了现有工作：使用RAM++进行对象识别，Grounded SAM进行分割，OPDMulti进行关节估计，以及类似DRAWER的多视图融合方法，但进行了改进以提高鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型和视觉语言模型从静态3D场景中恢复物体铰接，生成物理启发的3D场景，同时保持原始几何和外观。整体流程包括：1)开放词汇检测和分割，识别可打开物体并提取可移动部分；2)关节估计，确定运动类型和参数；3)隐藏几何生成，完成物体内部空腔；4)交互场景集成，将交互对象与静态背景结合并导出为兼容格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开放词汇检测减轻标签偏差；2)改进的2D到3D分割提高鲁棒性；3)关节细化基于物体几何提高准确性；4)隐藏几何生成使交互更真实；5)广泛的平台兼容性。相比之前工作，REACT3D无需多状态观察或用户交互，不需要仔细分割处理遮挡，比单图像方法提供更一致结果，且在把手缺失情况下表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REACT3D提出了一种可扩展的零样本框架，将静态3D场景转换为具有物理交互能力的数字孪生体，为具身智能研究提供了实用基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D scenes are increasingly vital for embodied intelligence, yetexisting datasets remain limited due to the labor-intensive process ofannotating part segmentation, kinematic types, and motion trajectories. Wepresent REACT3D, a scalable zero-shot framework that converts static 3D scenesinto simulation-ready interactive replicas with consistent geometry, enablingdirect use in diverse downstream tasks. Our contributions include: (i)openable-object detection and segmentation to extract candidate movable partsfrom static scenes, (ii) articulation estimation that infers joint types andmotion parameters, (iii) hidden-geometry completion followed by interactiveobject assembly, and (iv) interactive scene integration in widely supportedformats to ensure compatibility with standard simulation platforms. We achievestate-of-the-art performance on detection/segmentation and articulation metricsacross diverse indoor scenes, demonstrating the effectiveness of our frameworkand providing a practical foundation for scalable interactive scene generation,thereby lowering the barrier to large-scale research on articulated sceneunderstanding. Our project page is https://react3d.github.io/</description>
      <author>example@mail.com (Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys)</author>
      <guid isPermaLink="false">2510.11340v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>IP-Augmented Multi-Modal Malicious URL Detection Via Token-Contrastive Representation Enhancement and Multi-Granularity Fusion</title>
      <link>http://arxiv.org/abs/2510.12395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CURL-IP的新型多模态恶意URL检测框架，解决了现有方法在处理URL的非自然层次结构、字符级混淆以及整合网络级信号方面的局限性。该框架包含三个关键创新组件，能够同时保留细粒度的词汇线索、上下文语义并整合网络级信号。&lt;h4&gt;背景&lt;/h4&gt;恶意URL检测仍然是网络安全的关键挑战，因为攻击者越来越多地采用复杂的规避技术，包括混淆、字符级扰动和对抗性攻击。虽然预训练语言模型如BERT在URL分析任务中显示出潜力，但当前实现存在三个主要局限：无法有效建模URL的非自然层次结构、对字符级混淆的敏感性不足，以及缺乏整合辅助网络级信号（如IP地址）的机制。&lt;h4&gt;目的&lt;/h4&gt;解决当前恶意URL检测方法中的三个主要局限：无法有效建模URL的非自然层次结构、对字符级混淆的敏感性不足，以及缺乏整合网络级信号的机制。提出一个能够同时保留细粒度词汇线索、上下文语义并整合网络级信号的先进检测框架。&lt;h4&gt;方法&lt;/h4&gt;提出CURL-IP，一个先进的多模态检测框架，包含三个关键创新：(1) Token-Contrastive Representation Enhancer：通过令牌感知对比学习增强子词令牌表示，产生更具区分性和各向同性的嵌入；(2) Cross-Layer Multi-Scale Aggregator：通过卷积操作和门控MLP对Transformer输出进行层次聚合，捕获跨层的局部和全局语义模式；(3) Blockwise Multi-Modal Coupler：将URL-IP特征分解为局部块单元，并在块级别计算跨模态注意力权重，实现细粒度的跨模态交互。&lt;h4&gt;主要发现&lt;/h4&gt;在大型真实世界数据集上的评估显示，该框架在二元和多类分类任务中显著优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;CURL-IP框架通过其创新的架构设计，能够同时处理URL的细粒度特征、上下文语义和网络级信号，有效提高了恶意URL检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;恶意URL检测仍然是一个关键的网络安全挑战，因为对手越来越多地使用复杂的规避技术，包括混淆、字符级扰动和对抗性攻击。虽然像BERT这样的预训练语言模型在URL分析任务中显示出潜力，但当前实现中仍然存在三个局限性：无法有效建模URL的非自然层次结构，对字符级混淆的敏感性不足，以及缺乏整合辅助网络级信号（如IP地址）的机制——这些都是稳健检测所必需的。为了解决这些挑战，我们提出了CURL-IP，一个先进的多模态检测框架，包含三个关键创新：令牌对比表示增强器，通过令牌感知对比学习增强子词令牌表示；跨层多尺度聚合器，通过卷积操作和门控MLP对Transformer输出进行层次聚合；块级多模态耦合器，将URL-IP特征分解为局部块单元并计算跨模态注意力权重。这种架构能够同时保留细粒度的词汇线索、上下文语义和整合网络级信号。我们在大型真实世界数据集上的评估显示，该框架在二元和多类分类任务中显著优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious URL detection remains a critical cybersecurity challenge asadversaries increasingly employ sophisticated evasion techniques includingobfuscation, character-level perturbations, and adversarial attacks. Althoughpre-trained language models (PLMs) like BERT have shown potential for URLanalysis tasks, three limitations persist in current implementations: (1)inability to effectively model the non-natural hierarchical structure of URLs,(2) insufficient sensitivity to character-level obfuscation, and (3) lack ofmechanisms to incorporate auxiliary network-level signals such as IPaddresses-all essential for robust detection. To address these challenges, wepropose CURL-IP, an advanced multi-modal detection framework incorporatingthree key innovations: (1) Token-Contrastive Representation Enhancer, whichenhances subword token representations through token-aware contrastive learningto produce more discriminative and isotropic embeddings; (2) Cross-LayerMulti-Scale Aggregator, employing hierarchical aggregation of Transformeroutputs via convolutional operations and gated MLPs to capture both local andglobal semantic patterns across layers; and (3) Blockwise Multi-Modal Couplerthat decomposes URL-IP features into localized block units and computescross-modal attention weights at the block level, enabling fine-grainedinter-modal interaction. This architecture enables simultaneous preservation offine-grained lexical cues, contextual semantics, and integration ofnetwork-level signals. Our evaluation on large-scale real-world datasets showsthe framework significantly outperforms state-of-the-art baselines acrossbinary and multi-class classification tasks.</description>
      <author>example@mail.com (Ye Tian, Yanqiu Yu, Liangliang Song, Zhiquan Liu, Yanbin Wang, Jianguo Sun)</author>
      <guid isPermaLink="false">2510.12395v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?</title>
      <link>http://arxiv.org/abs/2510.12087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文本属性图(TAGs)上的表示学习结合了结构连接与丰富的文本语义，但在当前方法中，过度对齐会导致性能下降。作者提出了LLM4GTA框架，通过保留表示间隙来维持模态特定知识并提高迁移性能。&lt;h4&gt;背景&lt;/h4&gt;当前文本属性图表示学习方法主要依赖对比学习来最大化跨模态相似性，假设图和文本表示之间的更紧密耦合可以提高迁移性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中过度对齐导致的性能下降问题，提出一种保留表示间隙的框架，以维持模态特定知识并提高迁移性能。&lt;h4&gt;方法&lt;/h4&gt;提出LLM4GTA框架，包含自适应间隙保留模块防止过度对齐，以及模内补偿机制使用辅助分类器增强图空间的判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;经验分析表明，无论是自然间隙扩大还是强制间隙减小都会导致性能下降，这是因为图编码器捕获拓扑模式而文本编码器捕获语义结构，两者之间存在几何不兼容性。&lt;h4&gt;结论&lt;/h4&gt;保留表示间隙作为维持模态特定知识的几何必要性可以改善零样本和少样本场景下的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)上的表示学习将结构连接与丰富的文本语义相结合，使能够在多个领域应用。当前方法主要依赖对比学习来最大化跨模态相似性，假设图和文本表示之间的更紧密耦合可以提高迁移性能。然而，我们的经验分析显示，无论是自然间隙扩大还是强制间隙减小都会通过破坏预训练知识结构和损害泛化能力而导致性能下降。这是由于编码器之间的几何不兼容性造成的，其中图编码器捕获拓扑模式，而文本编码器捕获语义结构。过度对齐将这些不同的空间压缩到共享子空间中，导致结构崩溃，同时削弱了拓扑推理和语义理解。我们提出LLM4GTA，一个间隙感知的对齐框架，将表示间隙保留为维持模态特定知识和提高迁移性能的几何必要性。LLM4GTA包括自适应间隙保留模块，通过监控相似性演变防止过度对齐，以及模内补偿机制，使用图空间中的辅助分类器增强判别能力。大量实验表明，在零样本和少样本场景下，与现有方法相比显示出显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning on text-attributed graphs (TAGs) integratesstructural connectivity with rich textual semantics, enabling applications indiverse domains. Current methods largely rely on contrastive learning tomaximize cross-modal similarity, assuming tighter coupling between graph andtext representations improves transfer performance. However, our empiricalanalysis reveals that both natural gap expansion and forced gap reductionresult in performance degradation by disrupting pre-trained knowledgestructures and impairing generalization. This arises from the geometricincompatibility between encoders, where graph encoders capture topologicalpatterns, while text encoders capture semantic structures. Over-alignmentcompresses these distinct spaces into shared subspaces, causing structurecollapse that diminishes both topological reasoning and semantic understanding.We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preservesrepresentation gaps as geometric necessities for maintaining modality-specificknowledge and improving transfer performance. LLM4GTA includes an adaptive gappreservation module to prevent over-alignment by monitoring similarityevolution and an intra-modal compensation mechanism that boosts discriminativepower using auxiliary classifiers in graph space. Extensive experiments showsignificant improvements over existing methods in zero-shot and few-shotscenarios.</description>
      <author>example@mail.com (Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Zijian Zhang, Yilei Yuan, Hao Zhang, Jin Huang)</author>
      <guid isPermaLink="false">2510.12087v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2510.12085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出GraphShaper几何感知框架，通过多几何专业化解决图基础模型在结构边界处性能下降问题，实现零样本设置下在引用网络和社会网络上分别提高9.47%和7.63%的准确率。&lt;h4&gt;背景&lt;/h4&gt;图基础模型是一种在不同图域中学习可迁移表示的变革性范式。最近方法利用大型语言模型通过对比学习将图和文本模态统一到共享表示空间，但系统评估显示在结构边界处性能显著下降，准确率损失超过20个百分点。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够尊重图结构内在几何多样性的对齐框架，解决当前图基础模型在结构边界处性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;提出GraphShaper框架，采用针对不同几何空间的专业网络，动态计算融合权重，基于局部结构特征自适应地集成几何特性，在对齐文本嵌入前保持结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;当前方法假设所有图结构可在单一欧几里得空间编码，但实际上树结构需要双曲几何保持分层分支，循环模式依赖球面几何的闭合性质。在结构边界处，节点经历冲突的几何约束，统一编码空间无法解决。&lt;h4&gt;结论&lt;/h4&gt;GraphShaper通过多几何专业化和自适应融合策略，有效解决了图基础模型在结构边界处的性能下降问题，显著提高了零样本设置下的准确率。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型代表了一种在不同图域中学习可迁移表示的变革性范式。最近的方法利用大型语言模型通过对比学习将图和文本模态统一到共享表示空间中。然而，系统评估显示，在具有不同拓扑模式汇聚的结构边界处，性能显著下降，准确率损失超过20个百分点。这个问题源于一个关键限制：当前方法假设所有图结构都可以在单一欧几里得空间内编码。实际上，树结构需要双曲几何来保持分层分支，而循环模式则依赖于球面几何的闭合性质。在结构边界处，节点经历冲突的几何约束，统一的编码空间无法解决。这提出了一个关键挑战：能否设计尊重图结构内在几何多样性的对齐框架？我们介绍了GraphShaper，一个几何感知框架，通过多几何专业化增强图编码。我们的方法采用针对不同几何空间的专业网络，动态计算融合权重，基于局部结构特征自适应地集成几何特性。这种自适应融合在对齐文本嵌入之前保持结构完整性。大量实验证明，GraphShaper在零样本设置下在引用网络上实现了9.47%的准确率提升，在社会网络上实现了7.63%的准确率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models represent a transformative paradigm for learningtransferable representations across diverse graph domains. Recent methodsleverage large language models to unify graph and text modalities into a sharedrepresentation space using contrastive learning. However, systematicevaluations reveal significant performance degradation at structural boundarieswhere distinct topological patterns converge, with accuracy losses exceeding 20percentage points. This issue arises from a key limitation: current methodsassume all graph structures can be encoded within a single Euclidean space. Inreality, tree structures require hyperbolic geometry to preserve hierarchicalbranching, while cyclic patterns depend on spherical geometry for closureproperties. At structural boundaries, nodes experience conflicting geometricconstraints that uniform encoding spaces cannot resolve. This raises a crucialchallenge: \textbf{Can alignment frameworks be designed to respect theintrinsic geometric diversity of graph structures?} We introduce\textbf{GraphShaper}, a geometry-aware framework that enhances graph encodingthrough multi-geometric specialization. Our approach employs expert networkstailored to different geometric spaces, dynamically computing fusion weights toadaptively integrate geometric properties based on local structuralcharacteristics. This adaptive fusion preserves structural integrity beforealignment with text embeddings. Extensive experiments demonstrate thatGraphShaper achieves 9.47\% accuracy improvements on citation networks and7.63\% on social networks in zero-shot settings.</description>
      <author>example@mail.com (Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Haochen You, Zijian Zhang, Yilei Yuan, Jin Huang)</author>
      <guid isPermaLink="false">2510.12085v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging</title>
      <link>http://arxiv.org/abs/2510.12070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 page, 7 figures, uses IEEE.sty&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MEASURE的新型深度学习框架，用于解决睡眠分期模型在分布外场景中的泛化问题。该框架通过减少领域相关信息同时保留重要的时频特征，显著提升了模型在未见过的数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的自动睡眠分期技术近年来性能显著提升，对睡眠障碍诊断至关重要。然而，这些模型在处理不同受试者的生理信号时存在泛化困难，导致在分布外场景中性能下降。领域泛化方法，特别是对比学习，被研究用于解决这一问题，但现有方法往往无法充分提取领域不变特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减少领域相关信息同时保留睡眠分期所需关键时频特征的新型框架，以提高模型在未见数据上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了MEASURE（Multi-scale Minimal Sufficient Representation Learning）框架，该框架采用多尺度方法最小化充分表示学习，能够在减少领域相关信息的同时，保留多级特征中编码的多样时频信息。&lt;h4&gt;主要发现&lt;/h4&gt;在SleepEDF-20和MASS等公开睡眠分期基准数据集上的详尽实验表明，所提出的MEASURE方法持续优于当前最先进的方法，证明了其在处理领域差异方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;MEASURE框架通过针对性地减少过度领域相关信息同时保留关键特征，成功弥合了领域差距，显著提高了睡眠分期模型在分布外场景中的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的自动睡眠分期在性能上已取得显著进展，并在睡眠障碍诊断中起着关键作用。然而，由于生理信号的变异性，这些模型往往难以在未见过的受试者上泛化，导致在分布外场景中性能下降。为了解决这个问题，最近研究了领域泛化方法，以确保在训练期间对未见领域有泛化性能。在这些技术中，对比学习已证明通过在不同领域间对齐同类样本来学习领域不变特征的有效性。尽管有潜力，但许多现有方法不足以提取充分的领域不变表示，因为它们没有明确解决样本间非共享信息中嵌入的领域特征。在本文中，我们认为减轻这种领域相关属性（称为过度领域相关信息）是弥合领域差距的关键。然而，直接减轻领域相关属性的策略往往对高级信息特征过拟合，限制了利用多级特征中编码的多样时频信息的能力。为了解决这些局限性，我们提出了一个新颖的MEASURE（多尺度最小充分表示学习）框架，该框架在减少领域相关信息的同时，有效保留了睡眠分期分类的基本时频特征。在公开可用的睡眠分期基准数据集SleepEDF-20和MASS上进行的详尽实验中，我们提出的方法持续优于最先进的方法。我们的代码可在 https://github.com/ku-milab/Measure 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based automatic sleep staging has significantly advanced inperformance and plays a crucial role in the diagnosis of sleep disorders.However, those models often struggle to generalize on unseen subjects due tovariability in physiological signals, resulting in degraded performance inout-of-distribution scenarios. To address this issue, domain generalizationapproaches have recently been studied to ensure generalized performance onunseen domains during training. Among those techniques, contrastive learninghas proven its validity in learning domain-invariant features by aligningsamples of the same class across different domains. Despite its potential, manyexisting methods are insufficient to extract adequately domain-invariantrepresentations, as they do not explicitly address domain characteristicsembedded within the unshared information across samples. In this paper, weposit that mitigating such domain-relevant attributes-referred to as excessdomain-relevant information-is key to bridging the domain gap. However, thedirect strategy to mitigate the domain-relevant attributes often overfitsfeatures at the high-level information, limiting their ability to leverage thediverse temporal and spectral information encoded in the multiple featurelevels. To address these limitations, we propose a novel MEASURE (Multi-scalEminimAl SUfficient Representation lEarning) framework, which effectivelyreduces domain-relevant information while preserving essential temporal andspectral features for sleep stage classification. In our exhaustive experimentson publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,our proposed method consistently outperformed state-of-the-art methods. Ourcode is available at : https://github.com/ku-milab/Measure</description>
      <author>example@mail.com (Sangmin Jo, Jee Seok Yoon, Wootaek Jeong, Kwanseok Oh, Heung-Il Suk)</author>
      <guid isPermaLink="false">2510.12070v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images</title>
      <link>http://arxiv.org/abs/2510.11883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MammoDINO是一种专门为乳腺X线摄影设计的自监督学习框架，通过在大规模数据上预训练，实现了在乳腺癌筛查任务上的最先进性能，并具有良好的泛化能力，为计算机辅助诊断提供了无标注的基础。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已在一般领域的视觉编码器训练中取得变革性进展，但在医学影像领域应用不足，主要原因是数据有限和领域特定偏差。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于乳腺X线摄影的自监督学习框架，捕捉临床上有意义的特征，提高乳腺癌筛查的诊断效率，减轻放射科医生的工作量。&lt;h4&gt;方法&lt;/h4&gt;构建MammoDINO框架，在140万张乳腺X线图像上预训练；引入乳腺组织感知的数据增强采样器，用于图像级和补丁级监督；设计跨切片对比学习目标，利用3D数字乳腺断层合成结构进行2D预训练。&lt;h4&gt;主要发现&lt;/h4&gt;MammoDINO在多个乳腺癌筛查任务上取得了最先进的性能，并在五个基准数据集上表现出良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MammoDINO为乳腺X线摄影的多用途计算机辅助诊断工具提供了一种可扩展的无标注基础，有助于减轻放射科医生的工作量，提高乳腺癌筛查的诊断效率。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)已经在一般领域的视觉编码器训练中取得变革性进展，但由于数据有限和领域特定偏差，在医学影像中的应用仍然不足。我们提出了MammoDINO，这是一种用于乳腺X线摄影的新型SSL框架，在140万张乳腺X线图像上进行了预训练。为了捕捉临床上有意义的特征，我们引入了一种乳腺组织感知的数据增强采样器，用于图像级和补丁级监督，以及跨切片对比学习目标，利用3D数字乳腺断层合成(DBT)结构进行2D预训练。MammoDINO在多个乳腺癌筛查任务上取得了最先进的性能，并在五个基准数据集上表现出良好的泛化能力。它为乳腺X线摄影的多用途计算机辅助诊断(CAD)工具提供了一种可扩展的无标注基础，有助于减轻放射科医生的工作量，提高乳腺癌筛查的诊断效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督学习在乳腺X光摄影(mammography)领域的应用不足问题。这个问题很重要，因为乳腺癌是美国女性最常见的癌症，乳腺X光是主要筛查方式，但准确解读具有挑战性，且现有计算机辅助诊断工具依赖大量标注数据，而标注的乳腺X光数据有限，限制了模型效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有自监督学习框架在乳腺X光图像中的局限性：随机裁剪可能过度采样不相关背景，传统对比学习仅限单个2D切片无法捕捉3D DBT数据的跨切片结构连贯性。作者借鉴了DINOv2框架，参考了RAD-DINO和MedCoSS等医学影像自监督学习方法，并针对乳腺X光特点进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)确保模型专注于临床有意义的乳腺组织区域而非背景；2)利用3D DBT的跨切片结构连贯性。整体流程：收集140万乳腺X光图像→预处理→基于ViT架构的模型预训练→结合乳腺组织感知的DINO损失、iBOT损失和3D DBT相邻切片损失→在五个基准数据集上微调和评估多种乳腺癌筛查任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)乳腺组织感知的数据增强采样器，确保模型关注临床相关区域；2)3D DBT相邻切片损失，捕捉跨切片解剖连续性。不同之处：相比通用SSL框架(如DINOv2)，避免了过度采样背景和忽略3D结构；相比放射学定制SSL(如RadDINO)，专门针对乳腺解剖结构优化；相比文本引导方法(如BiomedCLIP)，无需文本监督仍能取得优越性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MammoDINO通过引入乳腺组织感知的数据增强和3D跨切片学习机制，解决了自监督学习在乳腺X光图像中的局限性，实现了乳腺癌筛查任务的先进性能，为开发更有效的计算机辅助诊断系统提供了无需标注的基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has transformed vision encoder training ingeneral domains but remains underutilized in medical imaging due to limiteddata and domain specific biases. We present MammoDINO, a novel SSL frameworkfor mammography, pretrained on 1.4 million mammographic images. To captureclinically meaningful features, we introduce a breast tissue aware dataaugmentation sampler for both image-level and patch-level supervision and across-slice contrastive learning objective that leverages 3D digital breasttomosynthesis (DBT) structure into 2D pretraining. MammoDINO achievesstate-of-the-art performance on multiple breast cancer screening tasks andgeneralizes well across five benchmark datasets. It offers a scalable,annotation-free foundation for multipurpose computer-aided diagnosis (CAD)tools for mammogram, helping reduce radiologists' workload and improvediagnostic efficiency in breast cancer screening.</description>
      <author>example@mail.com (Sicheng Zhou, Lei Wu, Cao Xiao, Parminder Bhatia, Taha Kass-Hout)</author>
      <guid isPermaLink="false">2510.11883v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements</title>
      <link>http://arxiv.org/abs/2510.11868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Thirteenth International Conference on Knowledge  Capture (K-CAP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的知识图谱嵌入方法，通过整合明确的否定陈述来改进知识嵌入学习过程，在链接预测和三元组分类任务上取得了优于现有模型的性能。&lt;h4&gt;背景&lt;/h4&gt;知识图谱以结构化三元组表示信息，是问答、链接预测和推荐系统等多种应用的基础。现有图嵌入方法大多依赖封闭世界假设，将缺失三元组视为错误，这与现实世界知识图谱的开放世界假设相矛盾，且很少考虑明确的否定陈述。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，将明确声明的否定陈述整合到知识嵌入学习过程中，以改进知识图谱的表示和预测能力。&lt;h4&gt;方法&lt;/h4&gt;采用双模型架构，两个嵌入模型并行训练：一个在正陈述上训练，另一个在负陈述上训练。训练过程中，每个模型通过破坏正样本生成负样本，并使用另一个模型的评分选择最可能的候选样本。&lt;h4&gt;主要发现&lt;/h4&gt;在通用和特定领域知识图谱上的大量实验表明，该方法在链接预测和三元组分类任务上优于最先进的嵌入模型，证明了整合有意义的负知识到嵌入学习中的价值。&lt;h4&gt;结论&lt;/h4&gt;通过整合明确的否定陈述，该方法有效提高了知识图谱嵌入的预测性能，为处理开放世界假设下的知识图谱提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱将信息表示为结构化三元组，并作为问答、链接预测和推荐系统等广泛应用的基础。探索知识图谱的一个主要研究方向是图嵌入方法，其中实体和关系在低维向量空间中表示，以捕获底层语义和结构。然而，大多数现有方法依赖于封闭世界假设或局部封闭世界假设等假设，将缺失的三元组视为错误。这与许多现实世界知识图谱所基于的开放世界假设形成对比。此外，虽然明确陈述的否定陈述有助于区分错误和未知的三元组，但它们很少被纳入知识图谱，在嵌入训练过程中也经常被忽视。在这项工作中，我们介绍了一种新方法，将明确声明的否定陈述整合到知识嵌入学习过程中。我们的方法采用双模型架构，两个嵌入模型并行训练，一个在正陈述上训练，另一个在负陈述上训练。在训练过程中，每个模型通过破坏正样本生成负样本，并使用另一个模型的评分选择最可能的候选样本。所提出的方法在通用和特定领域知识图谱上进行了评估，重点关注链接预测和三元组分类任务。大量实验表明，我们的方法优于最先进的嵌入模型，证明了将有意义的负知识整合到嵌入学习中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs represent information as structured triples and serve as thebackbone for a wide range of applications, including question answering, linkprediction, and recommendation systems. A prominent line of research forexploring knowledge graphs involves graph embedding methods, where entities andrelations are represented in low-dimensional vector spaces that captureunderlying semantics and structure. However, most existing methods rely onassumptions such as the Closed World Assumption or Local Closed WorldAssumption, treating missing triples as false. This contrasts with the OpenWorld Assumption underlying many real-world knowledge graphs. Furthermore,while explicitly stated negative statements can help distinguish between falseand unknown triples, they are rarely included in knowledge graphs and are oftenoverlooked during embedding training.  In this work, we introduce a novel approach that integrates explicitlydeclared negative statements into the knowledge embedding learning process. Ourapproach employs a dual-model architecture, where two embedding models aretrained in parallel, one on positive statements and the other on negativestatements. During training, each model generates negative samples bycorrupting positive samples and selecting the most likely candidates as scoredby the other model. The proposed approach is evaluated on both general-purposeand domain-specific knowledge graphs, with a focus on link prediction andtriple classification tasks. The extensive experiments demonstrate that ourapproach improves predictive performance over state-of-the-art embeddingmodels, demonstrating the value of integrating meaningful negative knowledgeinto embedding learning.</description>
      <author>example@mail.com (Rita T. Sousa, Heiko Paulheim)</author>
      <guid isPermaLink="false">2510.11868v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.11827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Janus框架通过联合利用欧几里得和双曲图神经网络，有效解决了节点级异常检测的挑战，通过对比学习对齐不同空间中的嵌入，识别难以协调的节点视图来检测异常。&lt;h4&gt;背景&lt;/h4&gt;节点级异常检测(NAD)具有挑战性，因为存在多样的结构模式和特征分布。NAD是一个关键任务，应用于欺诈检测、网络安全、推荐系统等多个领域。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为Janus的框架，用于有效解决节点级异常检测问题。&lt;h4&gt;方法&lt;/h4&gt;每个节点由两个视图描述：原始特征和从随机游走和度派生的结构特征，然后嵌入到欧几里得和双曲空间中。使用配备对比学习目标作为正则化项的多图自编码器框架，对齐欧几里得和双曲空间中的嵌入，突出显示那些视图难以协调的节点，这些节点可能是异常的。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上的实验表明，Janus始终优于浅层和深度基线方法， empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.&lt;h4&gt;结论&lt;/h4&gt;组合多种几何表示是一种稳健有效的方法，用于识别图中的微妙和复杂异常。&lt;h4&gt;翻译&lt;/h4&gt;节点级异常检测(NAD)具有挑战性，因为存在多样的结构模式和特征分布。因此，NAD是一个关键任务，应用于从欺诈检测、网络安全到推荐系统的多个领域。我们引入了Janus，一个联合利用欧几里得和双曲图神经网络来捕捉节点表示互补方面的框架。每个节点由两个视图描述，由原始特征和从随机游走和度派生的结构特征组成，然后嵌入到欧几里得和双曲空间中。配备对比学习目标作为正则化项的多图自编码器框架，对齐欧几里得和双曲空间中的嵌入，突出显示那些视图难以协调的节点，因此可能是异常的。在四个真实数据集上的实验表明，Janus始终优于浅层和深度基线方法， empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node-level anomaly detection (NAD) is challenging due to diverse structuralpatterns and feature distributions. As such, NAD is a critical task withseveral applications which range from fraud detection, cybersecurity, torecommendation systems. We introduce Janus, a framework that jointly leveragesEuclidean and Hyperbolic Graph Neural Networks to capture complementary aspectsof node representations. Each node is described by two views, composed by theoriginal features and structural features derived from random walks anddegrees, then embedded into Euclidean and Hyperbolic spaces. A multiGraph-Autoencoder framework, equipped with a contrastive learning objective asregularization term, aligns the embeddings across the Euclidean and Hyperbolicspaces, highlighting nodes whose views are difficult to reconcile and are thuslikely anomalous. Experiments on four real-world datasets show that Janusconsistently outperforms shallow and deep baselines, empirically demonstratingthat combining multiple geometric representations provides a robust andeffective approach for identifying subtle and complex anomalies in graphs.</description>
      <author>example@mail.com (Simone Mungari, Ettore Ritacco, Pietro Sabatino)</author>
      <guid isPermaLink="false">2510.11827v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare</title>
      <link>http://arxiv.org/abs/2510.12741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Symposium on Model Accountability, Sustainability and  Healthcare (SMASH) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的个性化联邦微调方法，通过学习正交LoRA适配器来解耦通用知识和客户特定知识，使医疗领域的每个参与方能够充分利用自己的数据和他人的数据，解决了医疗数据隐私保护与模型性能之间的矛盾。&lt;h4&gt;背景&lt;/h4&gt;基础模型为AI在医疗领域的应用开辟了新可能性，但即使在健康数据上预训练，仍需针对特定下游任务微调。尽管基础模型减少了训练数据需求，但获取足够数据仍具挑战性，部分原因是医疗数据共享和聚合受到患者隐私保护限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在保护患者隐私的前提下，有效利用多方医疗数据的联邦微调方法，使各参与机构能够充分利用自有数据和他人的数据来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的个性化联邦微调方法，学习正交LoRA适配器来解耦通用知识和客户特定知识，使每个客户端能够同时利用自有数据和来自其他参与方的数据。&lt;h4&gt;主要发现&lt;/h4&gt;在实际联邦医学成像任务上的初步结果表明，该方法与当前联邦微调方法具有竞争力，能够有效平衡数据隐私保护与模型性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过联邦学习框架下的正交LoRA适配器方法，解决了医疗AI中数据隐私保护与模型性能之间的矛盾，为医疗领域的基础模型应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基础模型为AI在医疗领域的应用开辟了新的可能性。然而，即使在健康数据上预训练，它们仍需要针对特定的下游任务进行微调。此外，尽管基础模型减少了实现良好性能所需的训练数据量，但获取足够的数据仍然是一个挑战。这部分是由于为了保护患者隐私，限制了不同来源数据的共享和聚合。一个可能的解决方案是通过多个参与方（即医院、诊所等）之间的联邦学习来微调基础模型。在这项工作中，我们提出了一种新的个性化联邦微调方法，学习正交LoRA适配器来解耦通用知识和客户特定知识，使每个客户能够充分利用自己的数据和他人数据。我们在实际联邦医学成像任务上的初步结果表明，我们的方法与当前的联邦微调方法具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models open up new possibilities for the use of AI in healthcare.However, even when pre-trained on health data, they still need to be fine-tunedfor specific downstream tasks. Furthermore, although foundation models reducethe amount of training data required to achieve good performance, obtainingsufficient data is still a challenge. This is due, in part, to restrictions onsharing and aggregating data from different sources to protect patients'privacy. One possible solution to this is to fine-tune foundation models viafederated learning across multiple participating clients (i.e., hospitals,clinics, etc.). In this work, we propose a new personalized federatedfine-tuning method that learns orthogonal LoRA adapters to disentangle generaland client-specific knowledge, enabling each client to fully exploit both theirown data and the data of others. Our preliminary results on real-worldfederated medical imaging tasks demonstrate that our approach is competitiveagainst current federated fine-tuning methods.</description>
      <author>example@mail.com (Adam Tupper, Christian Gagné)</author>
      <guid isPermaLink="false">2510.12741v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2510.12724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;T(R,O)Grasp是一种基于扩散的框架，能够高效生成准确和多样化的抓取动作，适用于多种机器人手，在实验中取得了94.83%的平均成功率，推理速度为0.21秒，每秒可处理41个抓取动作，显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;灵巧抓取在机器人学中仍然是一个核心挑战，这主要由于其高维状态和动作空间的复杂性。&lt;h4&gt;目的&lt;/h4&gt;引入T(R,O)Grasp，一种基于扩散的框架，用于高效生成准确和多样化的抓取动作，适用于多种机器人手。&lt;h4&gt;方法&lt;/h4&gt;核心是T(R,O)图，一种统一表示方法，建模机器人手和物体间的空间变换并编码其几何属性；结合图扩散模型和高效的逆运动学求解器，支持无条件和有条件的抓取合成。&lt;h4&gt;主要发现&lt;/h4&gt;在多种灵巧手上进行的实验显示，T(R,O)Grasp平均成功率达94.83%，推理速度0.21秒，在NVIDIA A100 40GB GPU上每秒处理41个抓取动作；该方法在不同实现上具有鲁棒性和泛化能力，显著减少内存消耗，高推理速度使闭环灵巧操作成为可能。&lt;h4&gt;结论&lt;/h4&gt;T(R,O)Grasp有潜力扩展为灵巧抓取的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;灵巧抓取由于高维状态和动作空间的复杂性，在机器人学中仍然是一个核心挑战。我们引入了T(R,O)Grasp，一种基于扩散的框架，能够高效生成准确和多样化的抓取动作，适用于多种机器人手。其核心是T(R,O)图，一种统一表示方法，建模机器人手和物体间的空间变换并编码其几何属性。结合图扩散模型和高效的逆运动学求解器，支持无条件和有条件的抓取合成。在多种灵巧手上的广泛实验表明，T(R,O)Grasp在NVIDIA A100 40GB GPU上达到94.83%的平均成功率、0.21秒的推理速度和每秒41个抓取的吞吐量，显著优于现有基线方法。此外，我们的方法在不同实现上具有鲁棒性和泛化能力，同时显著减少内存消耗。更重要的是，高推理速度使闭环灵巧操作成为可能，突显了T(R,O)Grasp扩展为灵巧抓取基础模型的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效生成精确且多样化的灵巧抓取动作（dexterous grasping）问题，特别是在跨机器人平台（cross-embodiment）情况下。这个问题很重要，因为灵巧抓取是实现人类级精确操作的基础能力，对于机器人完成日常任务至关重要；现有方法要么计算效率低下，要么难以在不同机器人平台间泛化；高效的抓取生成对于实时应用和闭环控制至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（机器人中心方法泛化能力差、物体中心方法计算成本高、交互中心方法如D(R,O)内存消耗大且依赖初始状态）设计了新方法。作者借鉴了扩散模型（DDPM/DDIM）、图神经网络、空间变换表示（SE(3)）和逆运动学求解等现有技术，但创新性地提出了T(R,O) Graph表示和图扩散模型，解决了效率和泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是T(R,O) Graph表示和图扩散模型。T(R,O) Graph将物体和机器人手表示为图中的节点，它们之间的空间变换表示为边；图扩散模型基于此表示生成抓取。整体流程：1) 构建T(R,O) Graph（物体节点和手节点及其空间变换）；2) 图扩散模型前向过程添加噪声；3) 去噪模型预测噪声；4) 反向过程恢复干净抓取；5) 使用Pyroki进行逆运动学求解得到关节值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) T(R,O) Graph统一表示，显著减少内存使用；2) 高效图扩散模型，支持非条件和条件生成；3) 不依赖初始状态，避免D(R,O)的局限性；4) 高效训练推理（内存减少57%，速度提高3倍）；5) 强大的跨平台泛化能力。相比D(R,O)，成功率更高（94.83% vs 87.53%），内存更少，速度更快，不依赖初始状态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; T(R,O) Grasp通过创新的图扩散模型和高效的T(R,O) Graph表示，实现了跨平台灵巧抓取的高效生成，在保持高成功率的同时显著提高了推理速度并降低了内存消耗，为实时闭环抓取提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping remains a central challenge in robotics due to thecomplexity of its high-dimensional state and action space. We introduce T(R,O)Grasp, a diffusion-based framework that efficiently generates accurate anddiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,a unified representation that models spatial transformations between robotichands and objects while encoding their geometric properties. A graph diffusionmodel, coupled with an efficient inverse kinematics solver, supports bothunconditioned and conditioned grasp synthesis. Extensive experiments on adiverse set of dexterous hands show that T(R,O) Grasp achieves average successrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps persecond on an NVIDIA A100 40GB GPU, substantially outperforming existingbaselines. In addition, our approach is robust and generalizable acrossembodiments while significantly reducing memory consumption. More importantly,the high inference speed enables closed-loop dexterous manipulation,underscoring the potential of T(R,O) Grasp to scale into a foundation model fordexterous grasping.</description>
      <author>example@mail.com (Xin Fei, Zhixuan Xu, Huaicong Fang, Tianrui Zhang, Lin Shao)</author>
      <guid isPermaLink="false">2510.12724v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction</title>
      <link>http://arxiv.org/abs/2510.12719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了化学预训练模型在药物发现中的应用，特别是在多任务学习框架下的微调效果。&lt;h4&gt;背景&lt;/h4&gt;化学预训练模型（基础模型）在药物发现领域受到广泛关注。从自监督训练中提取的一般化学知识有望提高对关键药物发现终点的预测，包括靶点效力和ADMET性质。多任务学习已被成功用于改进预测模型。&lt;h4&gt;目的&lt;/h4&gt;研究在化学预训练图神经网络模型（如KERMT和KGPT）的微调过程中启用多任务学习的效果。&lt;h4&gt;方法&lt;/h4&gt;通过比较多任务微调与非预训练图神经网络模型的性能差异，评估了Kinetic GROVER Multi-Task (KERMT)和Knowledge-guided Pre-training of Graph Transformer (KGPT)模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多任务微调显著提高了预训练图神经网络模型的性能；2. 数据量越大，多任务微调KERMT带来的性能提升越显著；3. 发布了两个多任务ADMET数据分割，便于更准确地基准测试多任务深度学习方法；4. 在GitHub上提供了KERMT模型的加速实现，支持工业药物发现工作流程。&lt;h4&gt;结论&lt;/h4&gt;多任务微调能显著提升化学预训练图神经网络模型在药物发现应用中的性能，特别是在大数据集上，并提供了相关工具和数据资源促进该领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;化学预训练模型，有时被称为基础模型，正在药物发现应用中引起相当大的关注。从自监督训练中提取的一般化学知识有可能提高对关键药物发现终点的预测，包括靶点效力和ADMET性质。多任务学习已被成功用于改进预测模型。在这里，我们表明，在化学预训练图神经网络模型的微调中启用多任务，如Kinetic GROVER Multi-Task (KERMT)（GROVER模型的增强版本）和Knowledge-guided Pre-training of Graph Transformer (KGPT)，显著优于非预训练的图神经网络模型。令人惊讶的是，我们发现以多任务方式微调KERMT带来的性能提升在数据量较大时最为显著。此外，我们发布了两个多任务ADMET数据分割，以便更准确地基准测试多任务深度学习方法用于药物性质预测。最后，我们在GitHub上提供了KERMT模型的加速实现，使工业药物发现工作流程中的大规模预训练、微调和推理成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chemical pretrained models, sometimes referred to as foundation models, arereceiving considerable interest for drug discovery applications. The generalchemical knowledge extracted from self-supervised training has the potential toimprove predictions for critical drug discovery endpoints, including on-targetpotency and ADMET properties. Multi-task learning has previously beensuccessfully leveraged to improve predictive models. Here, we show thatenabling multitasking in finetuning of chemical pretrained graph neural networkmodels such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of theGROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)significantly improves performance over non-pretrained graph neural networkmodels. Surprisingly, we find that the performance improvement from finetuningKERMT in a multitask manner is most significant at larger data sizes.Additionally, we publish two multitask ADMET data splits to enable moreaccurate benchmarking of multitask deep learning methods for drug propertyprediction. Finally, we provide an accelerated implementation of the KERMTmodel on GitHub, unlocking large-scale pretraining, finetuning, and inferencein industrial drug discovery workflows.</description>
      <author>example@mail.com (Matthew Adrian, Yunsie Chung, Kevin Boyd, Saee Paliwal, Srimukh Prasad Veccham, Alan C. Cheng)</author>
      <guid isPermaLink="false">2510.12719v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <link>http://arxiv.org/abs/2510.12709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-Embedding是一种全模态嵌入基础模型，通过定制的训练策略和架构设计解决了现有多模态嵌入模型在实际应用中面临的挑战，并在各种检索任务和实际业务场景中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示以支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展有前景，但现有模型在实际应用和业务场景中仍面临模态支持有限、训练机制不稳定和工业领域差距等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决现有多模态嵌入模型在实际应用中面临挑战的全模态嵌入基础模型，提高其在各种跨模态任务中的表现和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出SAIL-Embedding模型，采用多阶段训练方案：(1)内容感知渐进式训练增强模型对不同下游任务的适应性和跨模态能力；(2)协作感知推荐增强训练通过提取序列到项目和ID到项目嵌入知识并挖掘用户历史兴趣来优化推荐场景；(3)随机专业化和数据集驱动的模式匹配加强模型训练的灵活性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;SAIL-Embedding在不同检索任务中实现了最先进性能；在抖音精选场景中，7天LT增长+0.158%，14天LT增长+0.144%；在抖音feed排序模型中，匹配特征带来+0.08%的AUC增益。&lt;h4&gt;结论&lt;/h4&gt;SAIL-Embedding通过创新的训练策略和架构设计有效解决了现有多模态嵌入模型在实际应用中的局限性，显著提升了模型性能和业务指标。&lt;h4&gt;翻译&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示，支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展有前景，但先前的工作在实际应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定和工业领域差距。在这项工作中，我们引入了SAIL-Embedding，一个全模态嵌入基础模型，通过定制的训练策略和架构设计解决这些问题。在优化过程中，我们提出了多阶段训练方案，以提高表示学习的多方面有效性。具体而言，内容感知渐进式训练旨在增强模型对不同下游任务的适应性，掌握丰富的跨模态能力。协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中提取知识，同时挖掘用户历史兴趣，进一步使多模态表示适应推荐场景。同时，我们开发了随机专业化和数据集驱动的模式匹配，以加强模型训练的灵活性和泛化能力。实验结果表明，与其他方法相比，SAIL-Embedding在不同的检索任务中实现了最先进的性能。在我们模型集成的各种现实场景的在线实验中，我们观察到Lifetime (LT)显著增加，这是推荐体验的关键指标。例如，在抖音精选场景中，模型实现了7天LT增长+0.158%和14天LT增长+0.144%。对于抖音feed排序模型，SAIL-Embedding产生的匹配特征带来了+0.08%的AUC增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal embedding models aim to yield informative unified representationsthat empower diverse cross-modal tasks. Despite promising developments in theevolution from CLIP-based dual-tower architectures to large vision-languagemodels, prior works still face unavoidable challenges in real-worldapplications and business scenarios, such as the limited modality support,unstable training mechanisms, and industrial domain gaps. In this work, weintroduce SAIL-Embedding, an omni-modal embedding foundation model thataddresses these issues through tailored training strategies and architecturaldesign. In the optimization procedure, we propose a multi-stage training schemeto boost the multifaceted effectiveness of representation learning.Specifically, the content-aware progressive training aims to enhance themodel's adaptability to diverse downstream tasks and master enrichedcross-modal proficiency. The collaboration-aware recommendation enhancementtraining further adapts multimodal representations for recommendation scenariosby distilling knowledge from sequence-to-item and ID-to-item embeddings whilemining user historical interests. Concurrently, we develop the stochasticspecialization and dataset-driven pattern matching to strengthen model trainingflexibility and generalizability. Experimental results show that SAIL-Embeddingachieves SOTA performance compared to other methods in different retrievaltasks. In online experiments across various real-world scenarios integratedwith our model, we observe a significant increase in Lifetime (LT), which is acrucial indicator for the recommendation experience. For instance, the modeldelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in theDouyin-Selected scenario. For the Douyin feed rank model, the match featuresproduced by SAIL-Embedding yield a +0.08% AUC gain.</description>
      <author>example@mail.com (Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng)</author>
      <guid isPermaLink="false">2510.12709v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>CoRA: Covariate-Aware Adaptation of Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.12681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种协变量感知适应框架(CoRA)，用于增强时间序列基础模型(TSFMs)的性能，使其能够有效整合来自不同模态的外部协变量信息，显著提升预测质量。&lt;h4&gt;背景&lt;/h4&gt;当前大多数TSFMs在单变量时间序列上进行预训练，这限制了它们在真实世界预测任务中利用不同协变量中的重要信息。这种限制源于变量间依赖性的异质性和在大规模多变量数据集上的骨干模型扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;为了进一步提高TSFMs的性能，提出一个通用的协变量感知适应框架(CoRA)，使TSFMs能够有效整合来自时间序列、语言和图像等不同模态的外部协变量信息。&lt;h4&gt;方法&lt;/h4&gt;CoRA框架利用预训练的基础模型骨干作为冻结特征提取器，采用Granger因果嵌入(GCE)自动评估协变量相对于目标变量的因果预测能力，并通过零初始化的条件注入机制整合加权嵌入，避免灾难性遗忘并逐渐融入外部信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，TSFMs的CoRA超越了具有完全或少样本训练的最先进协变量感知深度预测器，在协变量感知预测上实现了31.1%的MSE降低。CoRA与各种先进TSFMs兼容性强，并将协变量范围扩展到其他模态。&lt;h4&gt;结论&lt;/h4&gt;CoRA为TSFMs的应用提供了实用范式，能够有效整合多模态协变量信息，显著提升预测性能，扩展了TSFMs在实际应用中的能力范围。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)已通过其模型容量、可扩展性和零样本泛化能力显示出显著影响。然而，由于变量间依赖性的异质性和在大规模多变量数据集上的骨干模型扩展性，大多数TSFMs通常在单变量时间序列上进行预训练。这一限制使它们在真实世界预测任务中忽略了来自不同协变量的关键信息。为了进一步提高TSFMs的性能，我们提出了一个通用的协变量感知适应(CoRA)框架。它利用基础模型的预训练骨干，同时有效整合来自时间序列、语言和图像等不同模态的外部协变量，以提高预测质量。技术上，CoRA在适应过程中保持初始化等价性和参数一致性。将基础模型的骨干作为冻结特征提取器，实证表明基础模型的输出嵌入比原始数据更具信息量。此外，CoRA采用了一种新的Granger因果嵌入(GCE)来自动评估协变量相对于目标变量的因果预测能力。我们将这些加权嵌入与零初始化的条件注入机制相结合，避免了对预训练基础模型的灾难性遗忘，并逐渐整合外部信息。大量实验表明，TSFMs的CoRA超越了具有完全或少样本训练的最先进协变量感知深度预测器，在协变量感知预测上实现了31.1%的MSE降低。与其他适应方法相比，CoRA与各种先进的TSFMs具有强大的兼容性，并将协变量的范围扩展到其他模态，为TSFMs的应用提供了实用的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have shown significant impact throughtheir model capacity, scalability, and zero-shot generalization. However, dueto the heterogeneity of inter-variate dependencies and the backbone scalabilityon large-scale multivariate datasets, most TSFMs are typically pre-trained onunivariate time series. This limitation renders them oblivious to crucialinformation from diverse covariates in real-world forecasting tasks. To furtherenhance the performance of TSFMs, we propose a general covariate-awareadaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones offoundation models while effectively incorporating exogenous covariates fromvarious modalities, including time series, language, and images, to improve thequality of predictions. Technically, CoRA maintains the equivalence ofinitialization and parameter consistency during adaptation. With preservedbackbones of foundation models as frozen feature extractors, the outcomeembeddings from foundation models are empirically demonstrated more informativethan raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)to automatically evaluate covariates regarding their causal predictability withrespect to the target variate. We incorporate these weighted embeddings with azero-initialized condition-injection mechanism, avoiding catastrophicforgetting of pre-trained foundation models and gradually integrates exogenousinformation. Extensive experiments show that CoRA of TSFMs surpassesstate-of-the-art covariate-aware deep forecasters with full or few-shottraining samples, achieving 31.1% MSE reduction on covariate-aware forecasting.Compared to other adaptation methods, CoRA exhibits strong compatibility withvarious advanced TSFMs and extends the scope of covariates to other modalities,presenting a practical paradigm for the application of TSFMs.</description>
      <author>example@mail.com (Guo Qin, Zhi Chen, Yong Liu, Zhiyuan Shi, Haixuan Liu, Xiangdong Huang, Jianmin Wang, Mingsheng Long)</author>
      <guid isPermaLink="false">2510.12681v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在开发简单高效的人体网格恢复(HMR)模型及其前置任务人体姿态估计(HPE)模型，通过利用分层视觉基础模型的早期阶段作为编码器，实现了在准确性和计算效率之间的更好权衡。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的HMR方法（如HMR2.0及其后续版本）依赖于大型、非分层的视觉Transformer作为编码器，这些编码器是从相应的HPE模型（如ViTPose）继承而来的。&lt;h4&gt;目的&lt;/h4&gt;开发简单高效的人体网格恢复(HMR)模型及其前置任务人体姿态估计(HPE)模型。&lt;h4&gt;方法&lt;/h4&gt;构建三种轻量级HMR2.0变体；提出利用Swin Transformer、GroupMixFormer和VMamba等分层视觉基础模型的早期阶段作为编码器；对27种基于分层VFMs的HMR和HPE模型进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用分层VFMs的前两或三个阶段就能达到与完整阶段模型相当的性能；截断后的模型在准确性和计算效率之间表现出比现有轻量级替代方案更好的权衡。&lt;h4&gt;结论&lt;/h4&gt;通过利用分层视觉基础模型的早期阶段，可以开发出既高效又准确的人体网格恢复和姿态估计模型。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们旨在开发用于人体网格恢复(HMR)及其前置任务人体姿态估计(HPE)的简单高效模型。最先进的HMR方法（如HMR2.0及其后续版本）依赖于大型、非分层的视觉Transformer作为编码器，这些编码器是从相应的HPE模型（如ViTPose）继承而来的。为了在不同计算预算下建立基线，我们首先通过调整相应的ViTPose模型构建了三种轻量级HMR2.0变体。此外，我们提出利用Swin Transformer、GroupMixFormer和VMamba等分层视觉基础模型(VFMs)的早期阶段作为编码器。这种设计灵感来自于分层VFMs的中间阶段产生的特征图分辨率与非分层模型相当或更高。我们对27种基于分层VFMs的HMR和HPE模型进行了全面评估，证明仅使用前两或三个阶段就能达到与完整阶段模型相当的性能。此外，我们表明与现有的轻量级替代方案相比，截断后的模型在准确性和计算效率之间表现出更好的权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we aim to develop simple and efficient models for human meshrecovery (HMR) and its predecessor task, human pose estimation (HPE).State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,non-hierarchical vision transformers as encoders, which are inherited from thecorresponding HPE models like ViTPose. To establish baselines across varyingcomputational budgets, we first construct three lightweight HMR2.0 variants byadapting the corresponding ViTPose models. In addition, we propose leveragingthe early stages of hierarchical vision foundation models (VFMs), includingSwin Transformer, GroupMixFormer, and VMamba, as encoders. This design ismotivated by the observation that intermediate stages of hierarchical VFMsproduce feature maps with resolutions comparable to or higher than those ofnon-hierarchical counterparts. We conduct a comprehensive evaluation of 27hierarchical-VFM-based HMR and HPE models, demonstrating that using only thefirst two or three stages achieves performance on par with full-stage models.Moreover, we show that the resulting truncated models exhibit better trade-offsbetween accuracy and computational efficiency compared to existing lightweightalternatives.</description>
      <author>example@mail.com (Shuhei Tarashima, Yushan Wang, Norio Tagawa)</author>
      <guid isPermaLink="false">2510.12660v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery</title>
      <link>http://arxiv.org/abs/2510.12640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于基础模型的新方法，用于分析科学领域中的时间序列事件数据，无需为每个新数据集重新训练模型，从而加速科学发现。&lt;h4&gt;背景&lt;/h4&gt;许多科学领域（从医学到地震学）依赖于分析随时间变化的事件序列来理解复杂系统。传统机器学习模型需要为每个新数据集从头构建和训练，这是一个缓慢且昂贵的过程。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的事件分析方法，使复杂事件分析更加易于访问，并加快科学发现的步伐。&lt;h4&gt;方法&lt;/h4&gt;创建一个单一的强大'基础模型'，在数百万个模拟事件序列上训练，学习事件数据的基本模式和事件如何展开的通用理解。&lt;h4&gt;主要发现&lt;/h4&gt;该模型可以即时分析新的科学数据，无需重新训练，只需查看数据集中的几个示例；同时可以快速微调以获得更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;这种方法使复杂事件分析更加易于访问，并加速了科学发现的步伐。&lt;h4&gt;翻译&lt;/h4&gt;许多科学领域，从医学到地震学，都依赖于分析随时间变化的事件序列来理解复杂系统。传统上，机器学习模型必须为每个新数据集从头构建和训练，这是一个缓慢且昂贵的过程。我们介绍了一种新方法：一个单一的、强大的模型，学习上下文中事件数据的基本模式。我们在数百万个模拟事件序列上训练了这个'基础模型'，教会它事件如何展开的通用理解。因此，我们的模型可以即时分析新的科学数据，无需重新训练，只需查看数据集中的几个示例。它还可以快速微调以获得更高的准确性。这种方法使复杂事件分析更加易于访问，并加快了科学发现的步伐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many scientific fields, from medicine to seismology, rely on analyzingsequences of events over time to understand complex systems. Traditionally,machine learning models must be built and trained from scratch for each newdataset, which is a slow and costly process. We introduce a new approach: asingle, powerful model that learns the underlying patterns of event data incontext. We trained this "foundation model" on millions of simulated eventsequences, teaching it a general-purpose understanding of how events canunfold. As a result, our model can analyze new scientific data instantly,without retraining, simply by looking at a few examples from the dataset. Itcan also be quickly fine-tuned for even higher accuracy. This approach makessophisticated event analysis more accessible and accelerates the pace ofscientific discovery.</description>
      <author>example@mail.com (David Berghaus, Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez)</author>
      <guid isPermaLink="false">2510.12640v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence</title>
      <link>http://arxiv.org/abs/2510.12579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合Plantnet、DinoV2和SAM的农业图像零样本分割方法，利用Plantnet的植物识别能力生成粗略掩码，再通过SAM细化得到详细分割结果，无需收集新数据集。&lt;h4&gt;背景&lt;/h4&gt;农业图像分割面临训练数据有限和田间条件复杂等挑战，这些因素常常阻碍纯监督方法的有效性，现有方法往往需要大量难以获取的标注数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需收集和标注新数据集的农业图像分割方法，解决农业场景中数据标注瓶颈问题，并在各种复杂农业场景中实现有效的分割。&lt;h4&gt;方法&lt;/h4&gt;利用Plantnet（大型植物分类模型）及其DinoV2主干网络，结合Segment Anything Model (SAM)，使用Plantnet的专门植物表示来识别植物区域并生成粗略分割掩码，然后通过SAM进一步细化掩码以获得详细分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用Plantnet微调的DinoV2相比基础DinoV2模型在Jaccard指数(IoU)测量上展现出一致的性能提升，结合基础模型与专门的植物中心模型可以缓解标注瓶颈问题，并在各种农业场景中实现有效分割。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与专门的植物中心模型相结合具有潜力，可以减轻农业图像分割中的标注负担，并在多样化的农业场景中实现有效的分割结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种农业图像的零样本分割方法，该方法结合了Plantnet（一种大规模植物分类模型）、其DinoV2主干网络和Segment Anything Model (SAM)。我们无需收集和标注新数据集，而是利用Plantnet专门的植物表示来识别植物区域并生成粗略分割掩码。然后，这些掩码通过SAM进行细化以产生详细分割结果。我们在四个公开可用数据集上进行了评估，这些数据集在对比度方面具有不同复杂度，包括一些训练数据有限且田间条件复杂常常阻碍纯监督方法的数据集。我们的结果显示，与基础DinoV2模型相比，使用Plantnet微调的DinoV2在Jaccard指数(IoU)测量上展现出一致的性能提升。这些发现强调了将基础模型与专门的植物中心模型相结合的潜力，可以减轻标注瓶颈，并在多样化的农业场景中实现有效分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a zero-shot segmentation approach for agricultural imagery thatleverages Plantnet, a large-scale plant classification model, in conjunctionwith its DinoV2 backbone and the Segment Anything Model (SAM). Rather thancollecting and annotating new datasets, our method exploits Plantnet'sspecialized plant representations to identify plant regions and produce coarsesegmentation masks. These masks are then refined by SAM to yield detailedsegmentations. We evaluate on four publicly available datasets of variouscomplexity in terms of contrast including some where the limited size of thetraining data and complex field conditions often hinder purely supervisedmethods. Our results show consistent performance gains when usingPlantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by theJaccard Index (IoU). These findings highlight the potential of combiningfoundation models with specialized plant-centric models to alleviate theannotation bottleneck and enable effective segmentation in diverse agriculturalscenarios.</description>
      <author>example@mail.com (Simon Ravé, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau)</author>
      <guid isPermaLink="false">2510.12579v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation</title>
      <link>http://arxiv.org/abs/2510.12515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HEAR，这是首个专门设计用于支持异构EEG设备的EEG基础模型，能够适应不同的电极布局和电极数量，通过可学习的基于坐标的空间嵌入和空间引导transformer实现统一表示空间，实验结果表明HEAR在支持异构EEG设备和跨任务跨主体泛化方面显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;EEG是神经科学研究和脑机接口应用的重要技术，近期开发的大规模EEG基础模型展现出强大的跨任务和跨主体泛化能力，但EEG设备的异质性阻碍了这些模型的广泛采用和进一步发展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够支持异构EEG设备的EEG基础模型，解决不同电极布局和电极数量带来的兼容性问题。&lt;h4&gt;方法&lt;/h4&gt;HEAR采用可学习的基于坐标的空间嵌入将不同布局和数量的电极映射到统一表示空间，并通过空间引导transformer处理这些表示以捕获电极间的时空依赖关系；同时构建了一个包含8,782小时数据、来自150多种电极布局的数据集来支持模型开发。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HEAR在支持异构EEG设备方面显著优于现有EEG基础模型，并在多样化的认知任务和主体间表现出良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;HEAR为解决EEG设备异质性挑战提供了有效方法，有助于EEG基础模型的广泛应用和进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;脑电图是神经科学研究和脑机接口应用的关键技术。最近，大规模EEG基础模型已被开发，展现出跨不同任务和主体的强大泛化能力。然而，EEG设备的异质性不仅阻碍了这些模型的广泛采用，也对其进一步扩展和发展提出了重大挑战。在本文中，我们介绍了HEAR，这是首个专门设计用于支持异构EEG设备的EEG基础模型，能够适应不同的电极布局和电极数量。HEAR采用可学习的基于坐标的空间嵌入，将具有不同布局和数量的电极映射到统一的表示空间。然后，这种统一的空间表示由新颖的空间引导transformer处理，有效捕获了电极间的时空依赖关系。为支持HEAR的开发，我们构建了一个包含8,782小时数据的大规模EEG数据集，数据来自150多种不同的电极布局，电极数量最多达1,132个。实验结果表明，HEAR在支持异构EEG设备方面显著优于现有的EEG基础模型，并能很好地泛化到多样化的认知任务和主体中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) is an essential technique for neuroscienceresearch and brain-computer interface (BCI) applications. Recently, large-scaleEEG foundation models have been developed, exhibiting robust generalizationcapabilities across diverse tasks and subjects. However, the heterogeneity ofEEG devices not only hinders the widespread adoption of these models but alsoposes significant challenges to their further scaling and development. In thispaper, we introduce HEAR, the first EEG foundation model explicitly designed tosupport heterogeneous EEG devices, accommodating varying electrode layouts andelectrode counts. HEAR employs a learnable, coordinate-based spatial embeddingto map electrodes with diverse layouts and varying counts into a unifiedrepresentational space. This unified spatial representation is then processedby a novel spatially-guided transformer, which effectively capturesspatiotemporal dependencies across electrodes. To support the development ofHEAR, we construct a large-scale EEG dataset comprising 8,782 hours of datacollected from over 150 distinct electrode layouts with up to 1,132 electrodes.Experimental results demonstrate that HEAR substantially outperforms existingEEG foundation models in supporting heterogeneous EEG devices and generalizingacross diverse cognitive tasks and subjects.</description>
      <author>example@mail.com (Zhige Chen, Chengxuan Qin, Wenlong You, Rui Liu, Congying Chu, Rui Yang, Kay Chen Tan, Jibin Wu)</author>
      <guid isPermaLink="false">2510.12515v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation</title>
      <link>http://arxiv.org/abs/2510.12498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人工智能虚拟细胞(AIVCs)旨在从多模态、多尺度数据中学习细胞状态模型，当前研究面临跨实验室迁移性有限、数据分割偏差、剂量时间效应处理不足以及跨尺度耦合受限等挑战。作者提出细胞状态潜在(CSL)视角，通过操作符语法组织学习，并建议改进评估方法和数据设计。&lt;h4&gt;背景&lt;/h4&gt;AIVCs致力于从多模态、多尺度测量中学习可执行的、决策相关的细胞状态模型。近期研究已引入单细胞和空间基础模型，改进跨模态对齐，扩展扰动图谱，并探索通路水平读出。&lt;h4&gt;目的&lt;/h4&gt;提出一种与模型无关的细胞状态潜在(CSL)视角，通过操作符语法组织学习，并建立跨模态、尺度、背景和干预的决策对齐评估蓝图。&lt;h4&gt;方法&lt;/h4&gt;采用操作符语法组织学习：测量、提升/投影(用于跨尺度耦合)和干预(用于剂量和调度)。强调功能空间读出，如通路活性、空间邻域和临床相关终点。&lt;h4&gt;主要发现&lt;/h4&gt;当前评估主要局限于单个数据集和设置；跨实验室和平台的可迁移性有限；某些数据分割易受泄漏和覆盖偏差影响；剂量、时间和组合效应未得到系统处理；跨尺度耦合受限，分子、细胞和组织水平的锚点稀少。&lt;h4&gt;结论&lt;/h4&gt;建议采用操作符感知的数据设计、抗泄漏分区和透明校准与报告，以实现可重复的、一对一的比较，改进AIVCs的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;人工智能虚拟细胞(AIVCs)旨在从多模态、多尺度测量中学习可执行的、决策相关的细胞状态模型。近期研究已引入单细胞和空间基础模型，改进跨模态对齐，扩展扰动图谱，并探索通路水平读出。然而，尽管保留验证是标准实践，评估仍主要局限于单个数据集和设置；证据表明跨实验室和平台的可迁移性通常有限，某些数据分割易受泄漏和覆盖偏差影响，剂量、时间和组合效应尚未得到系统处理。跨尺度耦合仍然受限，因为连接分子、细胞和组织水平的锚点稀少，且与科学或临床读出的对齐在不同研究中各不相同。我们提出了一种与模型无关的细胞状态潜在(CSL)视角，通过操作符语法组织学习：测量、提升/投影(用于跨尺度耦合)和干预(用于剂量和调度)。这一观点激发了跨模态、尺度、背景和干预的决策对齐评估蓝图，并强调功能空间读出，如通路活性、空间邻域和临床相关终点。我们建议采用操作符感知的数据设计、抗泄漏分区和透明校准与报告，以实现可重复的、一对一的比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,decision-relevant models of cell state from multimodal, multiscalemeasurements. Recent studies have introduced single-cell and spatial foundationmodels, improved cross-modality alignment, scaled perturbation atlases, andexplored pathway-level readouts. Nevertheless, although held-out validation isstandard practice, evaluations remain predominantly within single datasets andsettings; evidence indicates that transport across laboratories and platformsis often limited, that some data splits are vulnerable to leakage and coveragebias, and that dose, time and combination effects are not yet systematicallyhandled. Cross-scale coupling also remains constrained, as anchors linkingmolecular, cellular and tissue levels are sparse, and alignment to scientificor clinical readouts varies across studies. We propose a model-agnosticCell-State Latent (CSL) perspective that organizes learning via an operatorgrammar: measurement, lift/project for cross-scale coupling, and interventionfor dosing and scheduling. This view motivates a decision-aligned evaluationblueprint across modality, scale, context and intervention, and emphasizesfunction-space readouts such as pathway activity, spatial neighborhoods andclinically relevant endpoints. We recommend operator-aware data design,leakage-resistant partitions, and transparent calibration and reporting toenable reproducible, like-for-like comparisons.</description>
      <author>example@mail.com (Chengpeng Hu, Calvin Yu-Chian Chen)</author>
      <guid isPermaLink="false">2510.12498v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2510.12369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层量化框架，通过自加权机制实现跨尺度的任务自适应聚合，在保持编码器冻结的同时，通过轻量级门控过程调节信息流，实现参数高效的下游任务适应。&lt;h4&gt;背景&lt;/h4&gt;语言和视觉基础模型的进展表明，将复杂输入转换为紧凑序列的离散标记接口对大规模建模至关重要。将此范式扩展到图需要处理非欧几里得结构和多尺度依赖关系的标记化方案。&lt;h4&gt;目的&lt;/h4&gt;解决现有图标记化方法（线性化、连续和量化）在适应性和效率上的局限性，特别是解决基于量化的标记化方法在组织分层信息时缺乏任务自适应性的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种分层量化框架，引入自加权机制用于跨尺度的任务自适应聚合。该方法保持编码器冻结，通过轻量级门控过程调节信息流，实现参数高效的下游任务适应。&lt;h4&gt;主要发现&lt;/h4&gt;在节点分类和链接预测的基准数据集上，所提出的方法在可比的计算预算下，与强大的基线方法相比取得了持续改进。&lt;h4&gt;结论&lt;/h4&gt;该分层量化框架能够有效处理非欧几里得结构和多尺度依赖关系，通过自加权机制实现任务自适应信息聚合，为图建模提供了新的高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语言和视觉基础模型的最新进展表明，将复杂输入转换为紧凑序列的离散标记接口对大规模建模至关重要。将这一范式扩展到图需要一种能够高效处理非欧几里得结构和多尺度依赖关系的标记化方案。现有的图标记化方法，包括线性化、连续和量化方法，在适应性和效率方面仍然存在局限性。特别是，大多数当前基于量化的标记化方法以固定或任务无关的方式组织分层信息，这可能导致过度表示或未充分利用结构线索，并且无法在不重新编码器的情况下动态重新加权不同级别的贡献。本文提出了一种分层量化框架，引入了跨多个尺度进行任务自适应聚合的自加权机制。所提出的方法保持编码器冻结，同时通过轻量级门控过程调节信息流，实现参数高效地适应多样化的下游任务。在节点分类和链接预测的基准数据集上的实验表明，在可比的计算预算下，该方法比强大的基线方法取得了持续改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in language and vision foundation models demonstrates theimportance of discrete token interfaces that transform complex inputs intocompact sequences for large-scale modeling. Extending this paradigm to graphsrequires a tokenization scheme that handles non-Euclidean structures andmulti-scale dependencies efficiently. Existing approaches to graphtokenization, linearized, continuous, and quantized, remain limited inadaptability and efficiency. In particular, most current quantization-basedtokenizers organize hierarchical information in fixed or task-agnostic ways,which may either over-represent or under-utilize structural cues, and lack theability to dynamically reweight contributions from different levels withoutretraining the encoder. This work presents a hierarchical quantizationframework that introduces a self-weighted mechanism for task-adaptiveaggregation across multiple scales. The proposed method maintains a frozenencoder while modulating information flow through a lightweight gating process,enabling parameter-efficient adaptation to diverse downstream tasks.Experiments on benchmark datasets for node classification and link predictiondemonstrate consistent improvements over strong baselines under comparablecomputational budgets.</description>
      <author>example@mail.com (Yang Xiang, Li Fan, Chenke Yin, Chengtao Ji)</author>
      <guid isPermaLink="false">2510.12369v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models and Weakly Supervised Learning</title>
      <link>http://arxiv.org/abs/2510.12326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DeePAQ，一种基于深度学习的感知音频质量评估指标，用于评估通用音频质量。该方法结合了度量学习和音乐基础模型MERT，通过代理标签指导构建捕获音频失真强度的嵌入空间。研究首次在通用音频质量领域利用弱监督标签和度量学习微调音乐基础模型，使用低秩适应(LoRA)技术。实验表明，该方法在检测编码伪影方面优于现有指标，并能良好泛化到未见过的失真类型。&lt;h4&gt;背景&lt;/h4&gt;在音频质量评估领域，需要能够准确评估通用音频质量的指标。现有方法可能在处理不同类型的音频失真时存在局限性，特别是在编码伪影和源分离等场景中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的音频质量评估指标，能够准确捕捉通用音频中的失真强度，并在多种音频处理场景中表现良好。&lt;h4&gt;方法&lt;/h4&gt;研究采用度量学习结合音乐基础模型MERT的方法，通过代理标签指导，构建一个能够捕获通用音频失真强度的嵌入空间。该方法创新性地在通用音频质量领域应用弱监督标签和度量学习来微调音乐基础模型，并使用低秩适应(LoRA)技术进行参数高效调整。&lt;h4&gt;主要发现&lt;/h4&gt;在音频编码和源分离的听力测试中，DeePAQ超越了现有的最先进目标音频质量指标。特别是在检测编码伪影方面表现优异，并且对未见过的失真类型（如源分离）具有良好的泛化能力，展示了其鲁棒性和多功能性。&lt;h4&gt;结论&lt;/h4&gt;DeePAQ是一种创新的音频质量评估方法，通过结合度量学习和音乐基础模型，有效地解决了通用音频质量评估的挑战。其优越的性能和泛化能力表明该方法在音频处理领域具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了用于评估通用音频质量的基于深度学习的感知音频质量指标(DeePAQ)。我们的方法结合了度量学习和音乐基础模型MERT，通过代理标签指导，构建一个捕获通用音频中失真强度的嵌入空间。据我们所知，DeePAQ是通用音频质量领域中首个利用弱监督标签和度量学习来使用低秩适应(LoRA)微调音乐基础模型的方法，这一方向尚未被其他最先进方法探索。我们在涵盖音频编码和源分离的听力测试中，将所提出模型与最先进的目标音频质量指标进行了基准测试。结果表明，我们的方法在检测编码伪影方面超越了现有指标，并且对源分离等未见过的失真具有良好的泛化能力，突显了其鲁棒性和多功能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Deep learning-based Perceptual Audio Quality metric(DeePAQ) for evaluating general audio quality. Our approach leverages metriclearning together with the music foundation model MERT, guided by surrogatelabels, to construct an embedding space that captures distortion intensity ingeneral audio. To the best of our knowledge, DeePAQ is the first in the generalaudio quality domain to leverage weakly supervised labels and metric learningfor fine-tuning a music foundation model with Low-Rank Adaptation (LoRA), adirection not yet explored by other state-of-the-art methods. We benchmark theproposed model against state-of-the-art objective audio quality metrics acrosslistening tests spanning audio coding and source separation. Results show thatour method surpasses existing metrics in detecting coding artifacts andgeneralizes well to unseen distortions such as source separation, highlightingits robustness and versatility.</description>
      <author>example@mail.com (Guanxin Jiang, Andreas Brendel, Pablo M. Delgado, Jürgen Herre)</author>
      <guid isPermaLink="false">2510.12326v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <link>http://arxiv.org/abs/2510.12276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了空间强制(Spatial Forcing, SF)策略，一种简单有效的对齐方法，使视觉-语言-动作(VLA)模型能够隐式发展空间理解能力，无需依赖显式3D输入或深度估计器。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言-动作(VLA)模型在机器人执行语言指令和精确动作方面显示出强大潜力，但大多数VLA模型基于仅在2D数据上预训练的视觉-语言模型构建，缺乏准确的空间感知能力，限制了它们在3D物理世界中的操作能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单有效的对齐策略，使VLA模型能够在不依赖显式3D输入或深度估计器的情况下隐式发展空间理解能力，从而提高动作精确度。&lt;h4&gt;方法&lt;/h4&gt;提出了空间强制(SF)策略，将VLA的中间视觉嵌入与预训练的3D基础模型产生的几何表示对齐。通过在中间层强制对齐，引导VLA编码更丰富的空间表示。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实环境中的大量实验表明，SF实现了最先进的结果，超越了基于2D和3D的VLA模型。SF最多可加速训练3.8倍，并在多样化的机器人任务中提高了数据效率。&lt;h4&gt;结论&lt;/h4&gt;SF是一种有效的策略，可以增强VLA模型的空间理解能力，不需要显式的3D输入或深度估计器，在性能、训练速度和数据效率方面都有显著提升。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型最近在使机器人能够遵循语言指令并执行精确动作方面显示出强大潜力。然而，大多数VLA模型构建于仅在2D数据上预训练的视觉-语言模型之上，这些模型缺乏准确的空间感知能力，阻碍了它们在3D物理世界中的操作能力。现有解决方案尝试整合显式的3D传感器输入，如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中深度覆盖不完整，这些方法面临挑战。从2D图像估计3D线索的替代方法也受到深度估计器性能有限的困扰。我们提出了空间强制(SF)，一种简单而有效的对齐策略，隐式地强制VLA模型发展空间理解能力，而不依赖显式3D输入或深度估计器。SF将VLA的中间视觉嵌入与预训练的3D基础模型产生的几何表示对齐。通过在中间层强制对齐，SF引导VLA编码更丰富的空间表示，从而提高动作精确度。在模拟和真实环境中的大量实验表明，SF实现了最先进的结果，超越了基于2D和3D的VLA。SF最多可加速训练3.8倍，并在多样化的机器人任务中提高了数据效率。项目页面位于https://spatial-forcing.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动模型缺乏准确空间感知能力的问题。大多数VLA模型构建于仅在2D数据上预训练的视觉-语言模型之上，难以在3D物理世界中有效操作。这个问题很重要，因为机器人操作需要将语义推理与3D物理世界的精确空间控制相结合，缺乏空间感知能力会阻碍机器人在真实世界中的任务执行。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过深度探测实验发现现有VLA模型的视觉嵌入无法产生有意义的空间结构，从而假设需要隐式地增强模型的空间感知能力。作者借鉴了表示监督（representation supervision）的思路，特别是受到ROSS、REPA等工作的启发，采用表示对齐的方法。同时，作者利用了VGGT作为预训练的3D基础模型来生成空间表示，并借鉴了VLA模型中自回归机制的设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过空间强制（SF）策略隐式地强制VLA模型发展空间理解能力，而不依赖显式的3D输入。实现流程包括：1) 将多视角图像输入到VGGT 3D基础模型生成空间表示；2) 添加位置嵌入保留空间顺序；3) 使用余弦相似度对齐VLA的视觉标记与空间表示；4) 监督较深但不是最深的层（如第24层）；5) 结合动作生成损失和对齐损失进行训练；6) 推理时与标准VLA相同，无额外计算开销。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出空间强制（SF）对齐策略；2) 不依赖显式3D输入或深度估计器；3) 利用VGGT保证多视角一致性；4) 发现特定层（第24层）监督最有效。相比之前工作，SF避免了3D传感器噪声和硬件异构性问题，不依赖深度估计器性能限制，不仅提高了性能，还加速了训练（最高3.8倍）并提高了数据效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了空间强制策略，通过隐式对齐VLA模型的视觉嵌入与3D基础模型的空间表示，在不依赖显式3D输入的情况下，显著提升了模型的空间感知能力、训练效率和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have recently shown strong potential inenabling robots to follow language instructions and execute precise actions.However, most VLAs are built upon vision-language models pretrained solely on2D data, which lack accurate spatial awareness and hinder their ability tooperate in the 3D physical world. Existing solutions attempt to incorporateexplicit 3D sensor inputs such as depth maps or point clouds, but theseapproaches face challenges due to sensor noise, hardware heterogeneity, andincomplete depth coverage in existing datasets. Alternative methods thatestimate 3D cues from 2D images also suffer from the limited performance ofdepth estimators.We propose Spatial Forcing (SF), a simple yet effectivealignment strategy that implicitly forces VLA models to develop spatialcomprehension capabilities without relying on explicit 3D inputs or depthestimators. SF aligns intermediate visual embeddings of VLAs with geometricrepresentations produced by pretrained 3D foundation models. By enforcingalignment at intermediate layers, SF guides VLAs to encode richer spatialrepresentations that enhance action precision.Extensive experiments insimulation and real-world environments demonstrate that SF achievesstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF furtheraccelerates training by up to 3.8x and improves data efficiency across diverserobotic tasks. Project page is at https://spatial-forcing.github.io/</description>
      <author>example@mail.com (Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li)</author>
      <guid isPermaLink="false">2510.12276v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey</title>
      <link>http://arxiv.org/abs/2510.12178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述论文全面介绍了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）及其参数高效微调(PEFT)方法的发展历程，为机器学习研究人员和实践者提供了一站式资源。&lt;h4&gt;背景&lt;/h4&gt;Meta AI的LLaMA系列模型经历了快速演进，从最初的LLaMA 1发展到LLaMA 4，同时针对这些模型开发了一系列专门的参数高效微调方法。&lt;h4&gt;目的&lt;/h4&gt;提供对LLaMA模型家族和PEFT方法的全面概述，包括模型架构、性能特征、微调方法及其应用，帮助研究人员和实践者了解这一领域的最新进展。&lt;h4&gt;方法&lt;/h4&gt;描述了LLaMA基础模型（参数量从7B-65B到288B）、架构（包括原生多模态和专家混合变体）和关键性能特征；讨论了PEFT概念和五种应用于LLaMA的PEFT方法；分析了模型和适配器架构、参数量和基准测试结果；考察了实际应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;详细讨论了LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA等PEFT方法的机制、参数节省和应用示例；展示了微调后的LLaMA模型在某些任务上优于更大的基线模型；总结了LLaMA模型和PEFT在法律和医疗等领域的成功应用。&lt;h4&gt;结论&lt;/h4&gt;指出了当前面临的挑战和未来研究方向，如扩展到更大的上下文窗口和改进模型鲁棒性等，为后续研究提供了指导方向。&lt;h4&gt;翻译&lt;/h4&gt;本综述回顾了Meta AI的LLaMA（大型语言模型Meta AI）系列的快速演进历程 - 从LLaMA 1到LLaMA 4，以及为这些模型开发的专门参数高效微调(PEFT)方法。我们首先描述了LLaMA基础模型家族（7B-65B到288B参数）、它们的架构（包括原生多模态和专家混合变体）以及关键性能特征。然后我们描述并讨论了PEFT概念，它通过仅更新一小部分参数来适应大型预训练模型，并回顾了五种已应用于LLaMA的PEFT方法：LoRA（低秩自适应）、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA（量化LoRA）。我们讨论了每种方法的机制、参数节省以及在LLaMA上的应用示例（如指令微调、多模态任务）。我们对模型和适配器架构、参数量和基准测试结果进行了结构化讨论和分析（包括微调后的LLaMA模型优于更大基线模型的示例）。最后，我们考察了LLaMA模型和PEFT已成功应用的实际情况（如法律和医疗领域），并讨论了当前面临的挑战和未来研究方向（如扩展到更大的上下文和改进鲁棒性）。这篇综述论文为对LLaMA模型和高效微调策略感兴趣的机器学习研究人员和实践者提供了一站式资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review surveys the rapid evolution of Meta AI's LLaMA (Large LanguageModel Meta AI) series - from LLaMA 1 through LLaMA 4 and the specializedparameter-efficient fine-tuning (PEFT) methods developed for these models. Wefirst describe the LLaMA family of foundation models (7B-65B to 288Bparameters), their architectures (including native multimodal andMixtureof-Experts variants), and key performance characteristics. We thendescribe and discuss the concept of PEFT, which adapts large pre-trained modelsby updating only a small subset of parameters, and review five PEFT methodsthat have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method'smechanism, parameter savings, and example application to LLaMA (e.g.,instruction tuning, multimodal tasks). We provide structured discussion andanalysis of model and adapter architectures, parameter counts, and benchmarkresults (including examples where fine-tuned LLaMA models outperform largerbaselines). Finally, we examine real-world use cases where LLaMA-based modelsand PEFT have been successfully applied (e.g., legal and medical domains), andwe discuss ongoing challenges and future research directions (such as scalingto even larger contexts and improving robustness). This survey paper provides aone-stop resource for ML researchers and practitioners interested in LLaMAmodels and efficient fine-tuning strategies.</description>
      <author>example@mail.com (Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi)</author>
      <guid isPermaLink="false">2510.12178v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</title>
      <link>http://arxiv.org/abs/2510.12089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个基于DiT的框架和一种无需训练的多角色音频驱动动画方法，解决了现有技术在口型同步、长视频连贯性和多角色动画方面的挑战，实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;h4&gt;背景&lt;/h4&gt;扩散模型的最新进展显著提高了音频驱动人体视频生成的质量和可控性，超越了传统方法。然而，现有方法仍面临口型同步准确性、长视频生成的时间连贯性以及多角色动画的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成任意长度逼真说话视频的框架，以及一种无需训练的多角色音频驱动动画方法，以解决现有技术面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;采用基于LoRA的训练策略结合位置推理方法实现高效长视频生成；结合部分参数更新与奖励反馈增强口型同步和自然身体运动；提出无需训练的Mask分类器自由引导方法用于多角色动画，支持三个或更多角色的音频驱动动画。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法优于现有的最先进方法，以简单、高效和经济的方式实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;h4&gt;结论&lt;/h4&gt;所提出的DiT框架和Mask-CFG方法有效解决了音频驱动人体视频生成中的关键挑战，为高质量、时间连贯的多角色视频生成提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型的最新进展显著提高了音频驱动人体视频生成的质量，在质量和可控性方面超越了传统方法。然而，现有方法仍面临口型同步准确性、长视频生成的时间连贯性以及多角色动画的挑战。在这项工作中，我们提出了一个基于扩散变换器(DiT)的框架，用于生成任意长度的逼真说话视频，并引入了一种无需训练的多角色音频驱动动画方法。首先，我们采用基于LoRA的训练策略结合位置推理方法，使能够高效生成长视频同时保留基础模型能力。此外，我们将部分参数更新与奖励反馈相结合，以增强口型同步和自然的身体运动。最后，我们提出了一种无需训练的方法，即掩码分类器自由引导(Mask-CFG)，用于多角色动画，这不需要专门的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法优于现有的最先进方法，以简单、高效和经济的方式实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in diffusion models have significantly improved audio-drivenhuman video generation, surpassing traditional methods in both quality andcontrollability. However, existing approaches still face challenges in lip-syncaccuracy, temporal coherence for long video generation, and multi-characteranimation. In this work, we propose a diffusion transformer (DiT)-basedframework for generating lifelike talking videos of arbitrary length, andintroduce a training-free method for multi-character audio-driven animation.First, we employ a LoRA-based training strategy combined with a position shiftinference approach, which enables efficient long video generation whilepreserving the capabilities of the foundation model. Moreover, we combinepartial parameter updates with reward feedback to enhance both lipsynchronization and natural body motion. Finally, we propose a training-freeapproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-characteranimation, which requires no specialized datasets or model modifications andsupports audio-driven animation for three or more characters. Experimentalresults demonstrate that our method outperforms existing state-of-the-artapproaches, achieving high-quality, temporally coherent, and multi-characteraudio-driven video generation in a simple, efficient, and cost-effectivemanner.</description>
      <author>example@mail.com (Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang)</author>
      <guid isPermaLink="false">2510.12089v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Conjecturing: An Overlooked Step in Formal Mathematical Reasoning</title>
      <link>http://arxiv.org/abs/2510.11986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了数学自动形式化中的猜想步骤，创建了ConjectureBench数据集和评估框架，发现LLMs的自动形式化性能被高估，并提出Lean-FIRe方法实现了PutnamBench问题的端到端自动形式化。&lt;h4&gt;背景&lt;/h4&gt;自动形式化通常被视为直接翻译过程，忽略了关键的猜想步骤。许多数学问题需要先做出猜想才能形式化，而LLMs在自动形式化方面已存在困难，且对它们猜想能力的评估有限且常与自动形式化或证明纠缠。&lt;h4&gt;目的&lt;/h4&gt;创建专门评估LLMs猜想能力的框架，既作为独立任务也作为自动形式化流程的一部分，探究猜想对自动形式化的影响，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;创建ConjectureBench数据集，重新设计评估框架和指标评估LLMs的猜想能力，设计Lean-FIRe推理时间方法改进猜想和自动形式化性能。&lt;h4&gt;主要发现&lt;/h4&gt;当评估中考虑猜想时，GPT-4.1和DeepSeek-V3.1等基础模型的自动形式化性能被大大高估；Lean-FIRe方法首次实现了PutnamBench中13个问题(GPT-4.1)和7个问题(DeepSeek-V3.1)的端到端自动形式化。&lt;h4&gt;结论&lt;/h4&gt;虽然LLMs拥有生成准确猜想所需的知识，但提高自动形式化性能需要将猜想视为独立任务，并研究如何将其正确整合到自动形式化中。&lt;h4&gt;翻译&lt;/h4&gt;自动形式化是将非正式数学语言表达为正式数学语言的任务，通常被视为直接翻译过程。然而，这忽略了一个关键的先行步骤：猜想。许多数学问题不能直接形式化，需要先做出结论性猜想，如明确答案或特定界限。由于大型语言模型已经难以进行自动形式化，且对其猜想能力的评估有限且常与自动形式化或证明纠缠在一起，理解其影响尤其具有挑战性。为解决这一差距，我们扩充现有数据集创建了ConjectureBench，并重新设计了评估框架和指标，专门用于测量LLMs的猜想能力，既作为独立任务，也作为自动形式化流程的一部分。我们对基础模型的评估（包括GPT-4.1和DeepSeek-V3.1）显示，当评估中考虑猜想时，它们的自动形式化性能被大大高估。然而，不应假设猜想会预先提供。我们设计了一种推理时间方法Lean-FIRe来改进猜想和自动形式化，据我们所知，这首次实现了GPT-4.1对13个PutnamBench问题和DeepSeek-V3.1对7个问题的端到端自动形式化。我们证明，虽然LLMs拥有生成准确猜想所需的知识，但提高自动形式化性能需要将猜想视为独立任务，并进一步研究如何将其正确整合到自动形式化中。最后，我们提供前瞻性指导，引导未来研究关注改进猜想这一被忽视的正式数学推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoformalisation, the task of expressing informal mathematical statements informal language, is often viewed as a direct translation process. This,however, disregards a critical preceding step: conjecturing. Many mathematicalproblems cannot be formalised directly without first conjecturing a conclusionsuch as an explicit answer, or a specific bound. Since Large Language Models(LLMs) already struggle with autoformalisation, and the evaluation of theirconjecturing ability is limited and often entangled within autoformalisation orproof, it is particularly challenging to understand its effect. To address thisgap, we augment existing datasets to create ConjectureBench, and redesign theevaluation framework and metric specifically to measure the conjecturingcapabilities of LLMs both as a distinct task and within the autoformalisationpipeline. Our evaluation of foundational models, including GPT-4.1 andDeepSeek-V3.1, reveals that their autoformalisation performance issubstantially overestimated when the conjecture is accounted for duringevaluation. However, the conjecture should not be assumed to be provided. Wedesign an inference-time method, Lean-FIRe to improve conjecturing andautoformalisation, which, to the best of our knowledge, achieves the firstsuccessful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisiteknowledge to generate accurate conjectures, improving autoformalisationperformance requires treating conjecturing as an independent task, andinvestigating further how to correctly integrate it within autoformalisation.Finally, we provide forward-looking guidance to steer future research towardimproving conjecturing, an overlooked step of formal mathematical reasoning.</description>
      <author>example@mail.com (Jasivan Alex Sivakumar, Philipp Borchert, Ronald Cardenas, Gerasimos Lampouras)</author>
      <guid isPermaLink="false">2510.11986v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities</title>
      <link>http://arxiv.org/abs/2510.11842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at 39th Conference on Neural Information Processing Systems  (NeurIPS 2025) Workshop on Continual and Compatible Foundation Model Updates  (CCFM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了在语言模型持续预训练适应新任务时，如何平衡任务性能和知识保留的问题，特别关注了重放比率配置与计算预算之间的相互作用，并提供了基于实证的选择指南。&lt;h4&gt;背景&lt;/h4&gt;通过持续预训练使语言模型适应新任务面临一个基本权衡：模型必须学习新能力，同时避免对现有知识的灾难性遗忘。先前的研究已经研究了合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳重放比率仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究重放比率配置与计算预算在语言模型适应新任务时的相互作用，分析不同总令牌预算和重放比率配置对任务掌握和通用知识保留的影响，并提供基于实证的重放比率选择指南。&lt;h4&gt;方法&lt;/h4&gt;使用bAbI推理任务作为目标，应用合成数据生成方法，系统地评估不同的总令牌预算和重放比率配置，分析它们对任务掌握和通用知识保留的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验揭示了一种最优配置，能够平衡特定任务性能与通用知识保留。基于研究发现，研究提供了基于计算预算选择重放比率的实证指导，使实践者能够在显著降低训练成本的情况下实现强大的任务适应。&lt;h4&gt;结论&lt;/h4&gt;通过合理配置重放比率，实践者可以在有限的计算预算下实现有效的任务适应，同时保持模型的通用知识，从而显著降低训练成本。&lt;h4&gt;翻译&lt;/h4&gt;通过持续预训练使语言模型适应新任务面临一个基本权衡：模型必须学习新能力，同时避免对现有知识的灾难性遗忘。虽然先前的工作已经研究了合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳重放比率仍然知之甚少。我们提出了一个全面的实证研究，调查了在将语言模型适应新任务时，重放比率配置与计算预算之间的相互作用。使用bAbI推理任务作为我们的目标，我们应用合成数据生成方法，并系统评估不同的总令牌预算和重放比率配置。我们分析了它们对任务掌握和通用知识保留的影响。我们的实验揭示了一种平衡特定任务性能与通用知识保留的最优配置。基于我们的发现，我们提供了基于计算预算选择重放比率的实证指导，使实践者能够在显著降低训练成本的情况下实现强大的任务适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adapting language models to new tasks through continued pretraining faces afundamental trade-off: models must learn new capabilities while avoidingcatastrophic forgetting of existing knowledge. While prior work has studiedsynthetic data generation techniques, the optimal replay ratios for balancingtask performance and knowledge retention under computational constraints remainpoorly understood. We present a comprehensive empirical study investigating theinterplay between replay ratio configuration and computational budget whenadapting language models to new tasks. Using the bAbI reasoning tasks as ourtarget objective, we apply synthetic data generation and systematicallyevaluate different total token budgets and replay ratio configurations. Weanalyze their effects on both task mastery and general knowledge retention. Ourexperiments reveal an optimal configuration that balances task-specificperformance with general knowledge retention. Based on our findings, we provideempirically-grounded guidelines for selecting replay ratios based oncomputational budget, enabling practitioners to achieve strong task adaptationwith significantly reduced training costs.</description>
      <author>example@mail.com (Urs Spiegelhalter, Jörg K. H. Franke, Frank Hutter)</author>
      <guid isPermaLink="false">2510.11842v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</title>
      <link>http://arxiv.org/abs/2510.11576v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  currently being reviewed for WHISPERS conference ( Workshop on  Hyperspectral Image and Signal Processing: Evolution in Remote Sensing )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了三种基础模型在高光谱作物制图中的性能，发现SpectralEarth预训练模型表现最佳，准确率达到93.5%，强调了模型架构在跨区域和传感器平台泛化能力中的重要性。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变地球观测领域，但它们在高光谱作物制图方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;对比评估三种基础模型（HyperSigma、DOFA和SpectralEarth数据集预训练的Vision Transformers）用于高光谱作物制图的性能。&lt;h4&gt;方法&lt;/h4&gt;在训练区域的手动标记数据上对模型进行微调，在独立的测试区域评估模型性能，使用总体准确率、平均准确率和F1分数作为性能指标。&lt;h4&gt;主要发现&lt;/h4&gt;HyperSigma的OA为34.5%（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），SpectralEarth模型达到93.5%的OA（+/- 0.8%）；从头开始训练的紧凑型SpectralEarth变体达到91%的准确率，突显了模型架构对跨区域和传感器平台泛化能力的重要性。&lt;h4&gt;结论&lt;/h4&gt;这些结果为基础模型用于实际高光谱作物制图提供了系统评估，为未来模型开发指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在改变地球观测，但它们在高光谱作物制图方面的潜力尚未被充分探索。本研究使用高光谱图像对三种基础模型（HyperSigma、DOFA和SpectralEarth数据集预训练的Vision Transformers）进行了基准测试，用于谷物作物制图。模型在训练区域的手动标记数据上进行了微调，并在独立的测试区域进行了评估。性能通过总体准确率、平均准确率和F1分数进行衡量。HyperSigma达到34.5%的OA（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），SpectralEarth模型达到93.5%的OA（+/- 0.8%）。从零开始训练的紧凑型SpectralEarth变体达到91%，突显了模型架构对于在地理区域和传感器平台间实现强泛化能力的重要性。这些结果为基础模型用于实际高光谱作物制图提供了系统评估，并概述了未来模型开发的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are transforming Earth observation, but their potential forhyperspectral crop mapping remains underexplored. This study benchmarks threefoundation models for cereal crop mapping using hyperspectral imagery:HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarthdataset (a large multitemporal hyperspectral archive). Models were fine-tunedon manually labeled data from a training region and evaluated on an independenttest region. Performance was measured with overall accuracy (OA), averageaccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved91%, highlighting the importance of model architecture for stronggeneralization across geographic regions and sensor platforms. These resultsprovide a systematic evaluation of foundation models for operationalhyperspectral crop mapping and outline directions for future model development.</description>
      <author>example@mail.com (Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix)</author>
      <guid isPermaLink="false">2510.11576v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.12796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DriveVLA-W0是一种通过世界模型预测未来图像的训练范式，解决了VLA模型的监督不足问题，显著提升了驾驶智能性能，并随着数据量增加性能提升加速。&lt;h4&gt;背景&lt;/h4&gt;在大型数据上扩展视觉-语言-行动(VLA)模型是实现更通用驾驶智能的有前景路径，但VLA模型受到监督不足限制：模型容量大但仅由稀疏、低维度的行动监督，导致大部分表征能力未被利用。&lt;h4&gt;目的&lt;/h4&gt;解决VLA模型的监督不足问题，使模型能够更好地学习驾驶环境的基本动态，提高驾驶智能的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出DriveVLA-W0训练范式，采用世界模型预测未来图像生成密集自监督信号；为两种VLA架构实现：离散视觉令牌的自回归世界模型和连续视觉特征的扩散世界模型；引入轻量级行动专家解决实时部署推理延迟问题。&lt;h4&gt;主要发现&lt;/h4&gt;在NAVSIM v1/v2基准测试和680倍大的内部数据集上，DriveVLA-W0显著优于BEV和VLA基线；放大了数据扩展定律，表明随着训练数据集大小增加，性能提升加速。&lt;h4&gt;结论&lt;/h4&gt;DriveVLA-W0通过世界模型生成密集自监督信号有效解决VLA模型监督不足问题，提高模型表征能力同时通过轻量级行动专家解决实时部署问题，具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;在大型数据上扩展视觉-语言-行动(VLA)模型是实现更通用驾驶智能的有前景路径。然而，VLA模型受到监督不足的限制：模型容量巨大但仅由稀疏、低维度的行动监督，导致其大部分表征能力未被充分利用。为解决此问题，我们提出DriveVLA-W0训练范式，采用世界模型来预测未来图像。此任务生成密集的自监督信号，迫使模型学习驾驶环境的基本动态。我们通过为两种主导的VLA架构实现该范式来展示其多功能性：用于使用离散视觉令牌的VLA的自回归世界模型，以及用于在连续视觉特征上操作的VLA的扩散世界模型。基于从世界模型学到的丰富表征，我们引入轻量级行动专家以解决实时部署的推理延迟问题。在NAVSIM v1/v2基准测试和680倍大的内部数据集上进行的大量实验表明，DriveVLA-W0显著优于BEV和VLA基线。关键是，它放大了数据扩展定律，表明随着训练数据集大小的增加，性能提升会加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling Vision-Language-Action (VLA) models on large-scale data offers apromising path to achieving a more generalized driving intelligence. However,VLA models are limited by a ``supervision deficit'': the vast model capacity issupervised by sparse, low-dimensional actions, leaving much of theirrepresentational power underutilized. To remedy this, we propose\textbf{DriveVLA-W0}, a training paradigm that employs world modeling topredict future images. This task generates a dense, self-supervised signal thatcompels the model to learn the underlying dynamics of the driving environment.We showcase the paradigm's versatility by instantiating it for two dominant VLAarchetypes: an autoregressive world model for VLAs that use discrete visualtokens, and a diffusion world model for those operating on continuous visualfeatures. Building on the rich representations learned from world modeling, weintroduce a lightweight action expert to address the inference latency forreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a680x larger in-house dataset demonstrate that DriveVLA-W0 significantlyoutperforms BEV and VLA baselines. Crucially, it amplifies the data scalinglaw, showing that performance gains accelerate as the training dataset sizeincreases.</description>
      <author>example@mail.com (Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang)</author>
      <guid isPermaLink="false">2510.12796v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models</title>
      <link>http://arxiv.org/abs/2510.12618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解耦方法，利用基础推理模型(FIMs)来识别高维动态系统中的潜在变量，通过冻结FIM权重并只训练编码器-解码器映射，实现稳定高效的表示学习。&lt;h4&gt;背景&lt;/h4&gt;高维动态过程通常由更小的有效变量集合表征，这些变量在低维流形上演化。识别这些潜在动态需要解决两个交织的问题：发现适当的粗粒度变量和同时拟合控制方程。&lt;h4&gt;目的&lt;/h4&gt;将变量发现和动态拟合两个问题解耦，利用预训练的基础推理模型(FIMs)简化高维动态系统的分析过程。&lt;h4&gt;方法&lt;/h4&gt;通过利用预训练的基础推理模型(FIMs)来估计动态系统的无穷小生成器，使用具有冻结权重的FIM来推断动态，同时只训练编码器-解码器映射，定义了一个简单、模拟一致的损失函数来稳定表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在具有半圆扩散的随机双阱系统（嵌入到合成视频数据中）的概念验证实验表明，该方法具有快速和可重用的粗粒度处理流程的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过解耦变量发现和动态拟合的问题，并利用预训练的FIMs，提出了一种稳定且高效的表示学习方法，适用于高维动态系统的粗粒度化。&lt;h4&gt;翻译&lt;/h4&gt;高维动态过程的记录通常由更小的有效变量集合表征，这些变量在低维流形上演化。识别这些潜在动态需要解决两个交织的问题：发现适当的粗粒度变量和同时拟合控制方程。大多数机器学习方法通过联合训练自动编码器和强制动态一致性的模型来解决这些任务。我们提出通过利用最近引入的基础推理模型(FIMs)来解耦这两个问题。FIMs是预训练模型，可以零样本模式估计动态系统的无穷小生成器（例如随机微分方程的漂移和扩散）。通过使用具有冻结权重的FIM来推断动态，并且只训练编码器-解码器映射，我们定义了一个简单、模拟一致的损失函数，稳定了表示学习。在一个具有半圆扩散的随机双阱系统上进行的嵌入合成视频数据的概念证明，展示了这种方法在快速和可重用的粗粒度处理流程方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional recordings of dynamical processes are often characterized bya much smaller set of effective variables, evolving on low-dimensionalmanifolds. Identifying these latent dynamics requires solving two intertwinedproblems: discovering appropriate coarse-grained variables and simultaneouslyfitting the governing equations. Most machine learning approaches tackle thesetasks jointly by training autoencoders together with models that enforcedynamical consistency. We propose to decouple the two problems by leveragingthe recently introduced Foundation Inference Models (FIMs). FIMs are pretrainedmodels that estimate the infinitesimal generators of dynamical systems (e.g.,the drift and diffusion of a stochastic differential equation) in zero-shotmode. By amortizing the inference of the dynamics through a FIM with frozenweights, and training only the encoder-decoder map, we define a simple,simulation-consistent loss that stabilizes representation learning. A proof ofconcept on a stochastic double-well system with semicircle diffusion, embeddedinto synthetic video data, illustrates the potential of this approach for fastand reusable coarse-graining pipelines.</description>
      <author>example@mail.com (Manuel Hinz, Maximilian Mauel, Patrick Seifner, David Berghaus, Kostadin Cvejoski, Ramses J. Sanchez)</author>
      <guid isPermaLink="false">2510.12618v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>LayerSync: Self-aligning Intermediate Layers</title>
      <link>http://arxiv.org/abs/2510.12581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerSync是一种领域无关的方法，用于提高扩散模型的生成质量和训练效率。它通过利用模型自身的中间表示来正则化模型，减少对外部监督的需求，是一种自给自足、即插即用的正则化项，不需要预训练模型或额外数据，可应用于多种模态。&lt;h4&gt;背景&lt;/h4&gt;先前的研究已经指出扩散模型的生成质量与模型学习的表示之间存在联系，表明对模型中间表示的外部指导可以加速训练。然而，扩散模型不同层的表示质量存在差异。&lt;h4&gt;目的&lt;/h4&gt;重新构想扩散模型的训练范式，通过利用模型自身的中间表示来正则化模型，从而减少对外部监督的需求，提高生成质量和训练效率。&lt;h4&gt;方法&lt;/h4&gt;LayerSync基于扩散模型不同层表示质量不同的观察，将语义最丰富的表示作为较弱表示的内在指导。这是一种自给自足、即插即用的正则化项，不需要在扩散模型训练中增加额外开销，可以推广到视觉领域以外的其他模态。&lt;h4&gt;主要发现&lt;/h4&gt;LayerSync不需要预训练模型或额外数据；在图像生成上进行了广泛评估，并展示了其在音频、视频和运动生成等其他领域的适用性；它持续提高了生成质量和训练效率；例如，在ImageNet数据集上将基于流的变压器的训练速度提高了8.75倍以上，并将生成质量提高了23.6%。&lt;h4&gt;结论&lt;/h4&gt;LayerSync是一种有效的方法，可以提高扩散模型的生成质量和训练效率，适用于多种模态，且不需要额外的计算资源或数据。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LayerSync，一种领域无关的方法，用于提高扩散模型的生成质量和训练效率。先前的研究已经强调了扩散模型生成的质量与模型学习的表示之间的联系，表明对模型中间表示的外部指导可以加速训练。我们通过用模型的自身中间表示来正则化扩散模型，重新构想了这一范式。基于扩散模型不同层的表示质量存在差异的观察，我们证明语义最丰富的表示可以作为较弱表示的内在指导，减少对外部监督的需求。我们的方法LayerSync是一种自给自足、即插即用的正则化项，在扩散模型训练中没有额外开销，并且可以推广到视觉领域以外的其他模态。LayerSync不需要预训练模型或额外数据。我们在图像生成上广泛评估了该方法，并展示了其在音频、视频和运动生成等其他领域的适用性。我们表明它持续提高了生成质量和训练效率。例如，我们在ImageNet数据集上将基于流变压器的训练速度提高了8.75倍以上，并将生成质量提高了23.6%。代码可在https://github.com/vita-epfl/LayerSync获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose LayerSync, a domain-agnostic approach for improving the generationquality and the training efficiency of diffusion models. Prior studies havehighlighted the connection between the quality of generation and therepresentations learned by diffusion models, showing that external guidance onmodel intermediate representations accelerates training. We reconceptualizethis paradigm by regularizing diffusion models with their own intermediaterepresentations. Building on the observation that representation quality variesacross diffusion model layers, we show that the most semantically richrepresentations can act as an intrinsic guidance for weaker ones, reducing theneed for external supervision. Our approach, LayerSync, is a self-sufficient,plug-and-play regularizer term with no overhead on diffusion model training andgeneralizes beyond the visual domain to other modalities. LayerSync requires nopretrained models nor additional data. We extensively evaluate the method onimage generation and demonstrate its applicability to other domains such asaudio, video, and motion generation. We show that it consistently improves thegeneration quality and the training efficiency. For example, we speed up thetraining of flow-based transformer by over 8.75x on ImageNet dataset andimproved the generation quality by 23.6%. The code is available athttps://github.com/vita-epfl/LayerSync.</description>
      <author>example@mail.com (Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi)</author>
      <guid isPermaLink="false">2510.12581v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>I-DCCRN-VAE: An Improved Deep Representation Learning Framework for Complex VAE-based Single-channel Speech Enhancement</title>
      <link>http://arxiv.org/abs/2510.12485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的DCCRN-VAE单通道语音增强系统，通过移除预训练VAE中的跳跃连接、使用β-VAE进行预训练、以及让NSVAE同时生成语音和噪声潜在表示，提高了系统在不匹配数据集上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;最近提出了一种基于复杂变分自编码器(VAE)的单通道语音增强系统，该系统基于DCCRN架构。在这个系统中，噪声抑制VAE(NSVAE)使用预训练的干净语音和噪声VAE以及跳跃连接来从嘈杂语音中提取干净语音表示。&lt;h4&gt;目的&lt;/h4&gt;改进DCCRN-VAE系统，提高其在不同数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过三个关键改进：1) 移除预训练VAE中的跳跃连接，以鼓励更具信息性的语音和噪声潜在表示；2) 在预训练中使用β-VAE，以更好地平衡重建和潜在空间正则化；3) NSVAE同时生成语音和噪声潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;在匹配的DNS3数据集上，所提出的系统与DCCRN和DCCRN-VAE基线实现了相当的性能；在不匹配的数据集(WSJ0-QUT, Voicebank-DEMEND)上，优于基线，显示出改进的泛化能力；消融研究表明，可以使用传统的微调而非对抗训练实现类似性能，从而简化训练流程。&lt;h4&gt;结论&lt;/h4&gt;所提出的改进提高了系统在不匹配数据集上的泛化能力；简化的训练流程(使用传统微调而非对抗训练)也能获得类似性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，提出了一种基于复杂变分自编码器(VAE)的基于DCCRN架构的单通道语音增强系统。在该系统中，噪声抑制VAE(NSVAE)使用预训练的干净语音和噪声VAE以及跳跃连接，从嘈杂语音中学习提取干净语音表示。在本文中，我们通过三个关键改进来改进DCCRN-VAE：1) 移除预训练VAE中的跳跃连接，以鼓励更具信息性的语音和噪声潜在表示；2) 在预训练中使用β-VAE，以更好地平衡重建和潜在空间正则化；3) NSVAE生成语音和噪声潜在表示。实验表明，所提出的系统在匹配的DNS3数据集上实现了与DCCRN和DCCRN-VAE基线相当的性能，但在不匹配的数据集(WSJ0-QUT, Voicebank-DEMEND)上优于基线，显示出改进的泛化能力。此外，消融研究表明，可以使用传统的微调而非对抗训练实现类似性能，从而简化训练流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, a complex variational autoencoder (VAE)-based single-channel speechenhancement system based on the DCCRN architecture has been proposed. In thissystem, a noise suppression VAE (NSVAE) learns to extract clean speechrepresentations from noisy speech using pretrained clean speech and noise VAEswith skip connections. In this paper, we improve DCCRN-VAE by incorporatingthree key modifications: 1) removing the skip connections in the pretrainedVAEs to encourage more informative speech and noise latent representations; 2)using $\beta$-VAE in pretraining to better balance reconstruction and latentspace regularization; and 3) a NSVAE generating both speech and noise latentrepresentations. Experiments show that the proposed system achieves comparableperformance as the DCCRN and DCCRN-VAE baselines on the matched DNS3 datasetbut outperforms the baselines on mismatched datasets (WSJ0-QUT,Voicebank-DEMEND), demonstrating improved generalization ability. In addition,an ablation study shows that a similar performance can be achieved withclassical fine-tuning instead of adversarial training, resulting in a simplertraining pipeline.</description>
      <author>example@mail.com (Jiatong Li, Simon Doclo)</author>
      <guid isPermaLink="false">2510.12485v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression</title>
      <link>http://arxiv.org/abs/2510.12474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SMEC的新型训练框架，用于压缩大型语言模型的高维嵌入向量，解决了高维度带来的计算复杂度和存储需求问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型生成的高维嵌入向量虽然能捕捉丰富的语义和句法信息，但高维度加剧了计算复杂度和存储需求，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;解决高维嵌入带来的计算复杂度和存储需求问题，实现有效的维度压缩而不损失性能。&lt;h4&gt;方法&lt;/h4&gt;提出Sequential Matryoshka Embedding Compression (SMEC)框架，包含三个核心组件：Sequential Matryoshka Representation Learning (SMRL)方法减轻训练中的梯度方差，Adaptive Dimension Selection (ADS)模块减少维度修剪时的信息损失，Selectable Cross-batch Memory (S-XBM)模块增强高维和低维嵌入间的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;在图像、文本和多模态数据集上的实验表明，SMEC能在显著降低维度的同时保持性能。在BEIR数据集上，与Matryoshka-Adaptor和Search-Adaptor模型相比，SMEC将压缩后的LLM2Vec嵌入向量(256维)的性能分别提高了1.1分和2.7分。&lt;h4&gt;结论&lt;/h4&gt;SMEC框架能够有效压缩大型语言模型的高维嵌入向量，在减少计算复杂度和存储需求的同时保持或提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型生成的高维嵌入向量能够捕捉丰富的语义和句法信息。然而，高维嵌入向量加剧了计算复杂度和存储需求，从而阻碍了实际部署。为解决这些挑战，我们提出了一种名为Sequential Matryoshka Embedding Compression (SMEC)的新型训练框架。该框架引入了Sequential Matryoshka Representation Learning (SMRL)方法来减轻训练过程中的梯度方差，Adaptive Dimension Selection (ADS)模块来减少维度修剪过程中的信息损失，以及Selectable Cross-batch Memory (S-XBM)模块来增强高维和低维嵌入之间的无监督学习。在图像、文本和多模态数据集上的实验表明，SMEC在显著降低维度的同时保持了性能。例如，在BEIR数据集上，与Matryoshka-Adaptor和Search-Adaptor模型相比，我们的方法将压缩后的LLM2Vec嵌入向量(256维)的性能分别提高了1.1分和2.7分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) generate high-dimensional embeddings thatcapture rich semantic and syntactic information. However, high-dimensionalembeddings exacerbate computational complexity and storage requirements,thereby hindering practical deployment. To address these challenges, we proposea novel training framework named Sequential Matryoshka Embedding Compression(SMEC). This framework introduces the Sequential Matryoshka RepresentationLearning(SMRL) method to mitigate gradient variance during training, theAdaptive Dimension Selection (ADS) module to reduce information degradationduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) moduleto enhance unsupervised learning between high- and low-dimensional embeddings.Experiments on image, text, and multimodal datasets demonstrate that SMECachieves significant dimensionality reduction while maintaining performance.For instance, on the BEIR dataset, our approach improves the performance ofcompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 pointscompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.</description>
      <author>example@mail.com (Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng)</author>
      <guid isPermaLink="false">2510.12474v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Deep SPI: Safe Policy Improvement via World Models</title>
      <link>http://arxiv.org/abs/2510.12312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages main text, 17 pages appendix (excluding references)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究在线强化学习环境下的安全策略改进(SPI)理论，结合世界模型和表示学习，提出DeepSPI算法，在保持理论保证的同时实现了优异性能。&lt;h4&gt;背景&lt;/h4&gt;现有SPI保证主要关注离线、表格强化学习环境，缺乏在线设置下的理论支持。&lt;h4&gt;目的&lt;/h4&gt;开发理论框架，限制策略更新到当前策略的明确定义邻域，确保策略单调改进和收敛。&lt;h4&gt;方法&lt;/h4&gt;分析转换和奖励预测损失与表示质量的关系，开发在线、深度版本的经典SPI定理，提出DeepSPI算法，结合局部转换和奖励损失与正则化策略更新。&lt;h4&gt;主要发现&lt;/h4&gt;限制策略更新到当前策略的邻域可以确保单调改进和收敛；转换和奖励预测损失与表示质量相关联；DeepSPI在ALE-57基准测试中匹配或超过PPO和DeepMDPs等强基线方法。&lt;h4&gt;结论&lt;/h4&gt;DeepSPI算法在保持理论保证的同时，在实际应用中展现出与最先进方法相当或更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;安全策略改进(SPI)为策略更新提供了理论控制，但现有保证主要涉及离线、表格强化学习(RL)。我们研究结合世界模型和表示学习的在线设置下的SPI。我们开发了一个理论框架，显示将策略更新限制在当前策略的明确定义邻域内可以确保单调改进和收敛。该分析将转换和奖励预测损失与表示质量联系起来，产生了来自离线RL文献的经典SPI定理的在线、'深度'类比。基于这些结果，我们引入了DeepSPI，一种将局部转换和奖励损失与正则化策略更新相结合的原则性在线策略算法。在ALE-57基准测试中，DeepSPI匹配或超过了包括PPO和DeepMDPs在内的强基线方法，同时保留了理论保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safe policy improvement (SPI) offers theoretical control over policy updates,yet existing guarantees largely concern offline, tabular reinforcement learning(RL). We study SPI in general online settings, when combined with world modeland representation learning. We develop a theoretical framework showing thatrestricting policy updates to a well-defined neighborhood of the current policyensures monotonic improvement and convergence. This analysis links transitionand reward prediction losses to representation quality, yielding online, "deep"analogues of classical SPI theorems from the offline RL literature. Building onthese results, we introduce DeepSPI, a principled on-policy algorithm thatcouples local transition and reward losses with regularised policy updates. Onthe ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, includingPPO and DeepMDPs, while retaining theoretical guarantees.</description>
      <author>example@mail.com (Florent Delgrange, Raphael Avalos, Willem Röpke)</author>
      <guid isPermaLink="false">2510.12312v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs</title>
      <link>http://arxiv.org/abs/2510.12233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了IMDGA框架，一种统一的多维图攻击方法，针对图神经网络与大型语言模型结合的文本属性图，通过协调图结构和文本特征的多层次扰动，实现对Graph-LLMs的有效攻击，同时保持高度可解释性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为建模图结构数据的关键框架，通过整合大型语言模型，文本属性图利用丰富的文本语义显著提高了图学习能力。然而，这种协同作用也引入了关键漏洞，使Graph-LLMs容易受到对其结构拓扑和文本属性的对抗攻击。&lt;h4&gt;目的&lt;/h4&gt;虽然已有针对结构拓扑和文本属性的专门攻击方法，但缺乏统一的多维攻击框架。本研究旨在提出一种同时考虑图结构和文本特征的对抗攻击方法，并平衡攻击效果与可解释性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了可解释的多维图攻击（IMDGA）框架，该框架设计用于协调图结构和文本特征的多层次扰动。IMDGA利用三个紧密集成的模块构建攻击，平衡可解释性和影响力，帮助更深入理解Graph-LLM的漏洞。&lt;h4&gt;主要发现&lt;/h4&gt;通过在不同数据集和架构上的理论分析和实证评估，IMDGA在可解释性、攻击有效性、隐蔽性和鲁棒性方面均优于现有方法。研究揭示了TAG表示学习中的关键弱点，发现了Graph-LLMs中一个先前未被充分探索的语义维度漏洞。&lt;h4&gt;结论&lt;/h4&gt;通过暴露Graph-LLMs中的关键弱点，这项工作为提高系统弹性提供了有价值的见解，相关代码和资源已公开分享。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为建模图结构数据的关键框架，应用于从社交网络分析到分子化学的广泛领域。通过整合大型语言模型，文本属性图利用丰富的文本语义增强节点表示，显著提高了基于图的学习能力。然而，这种复杂的协同作用引入了关键漏洞，因为图-LLMs容易受到对其结构拓扑和文本属性的对抗攻击。虽然已经为这些方面的每个方面设计了专门的攻击方法，但还没有将它们统一为全面的方法。在本工作中，我们提出了可解释的多维图攻击（IMDGA），这是一种新型的人本主义对抗攻击框架，旨在协调图结构和文本特征的多层次扰动。IMDGA利用三个紧密集成的模块来构建攻击，平衡可解释性和影响力，使人们能够更深入地理解Graph-LLM的漏洞。通过在不同数据集和架构上进行严格的理论分析和全面的实证评估，IMDGA显示出比现有方法更好的可解释性、攻击有效性、隐蔽性和鲁棒性。通过揭示TAG表示学习中的关键弱点，这项工作揭示了Graph-LLMs中一个先前未被充分探索的语义维度漏洞，为提高它们的弹性提供了有价值的见解。我们的代码和资源已在https://anonymous.4open.science/r/IMDGA-7289公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a pivotal framework for modelinggraph-structured data, enabling a wide range of applications from socialnetwork analysis to molecular chemistry. By integrating large language models(LLMs), text-attributed graphs (TAGs) enhance node representations with richtextual semantics, significantly boosting the expressive power of graph-basedlearning. However, this sophisticated synergy introduces criticalvulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on boththeir structural topology and textual attributes. Although specialized attackmethods have been designed for each of these aspects, no work has yet unifiedthem into a comprehensive approach. In this work, we propose the InterpretableMulti-Dimensional Graph Attack (IMDGA), a novel human-centric adversarialattack framework designed to orchestrate multi-level perturbations across bothgraph structure and textual features. IMDGA utilizes three tightly integratedmodules to craft attacks that balance interpretability and impact, enabling adeeper understanding of Graph-LLM vulnerabilities. Through rigorous theoreticalanalysis and comprehensive empirical evaluations on diverse datasets andarchitectures, IMDGA demonstrates superior interpretability, attackeffectiveness, stealthiness, and robustness compared to existing methods. Byexposing critical weaknesses in TAG representation learning, this work uncoversa previously underexplored semantic dimension of vulnerability in Graph-LLMs,offering valuable insights for improving their resilience. Our code andresources are publicly available athttps://anonymous.4open.science/r/IMDGA-7289.</description>
      <author>example@mail.com (Bowen Fan, Zhilin Guo, Xunkai Li, Yihan Zhou, Bing Zhou, Zhenjun Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.12233v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification</title>
      <link>http://arxiv.org/abs/2510.12214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE BIBM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DE3S的医疗早期时间序列分类方法，通过双增强软稀疏形状学习解决了医疗场景中早期分类面临的准确性和早期性权衡问题。&lt;h4&gt;背景&lt;/h4&gt;早期时间序列分类在医疗应用中至关重要，特别是在ICU败血症预测等时间敏感场景中，延迟预测会导致大量死亡。ETSC可提高ICU资源利用效率和医疗精准度，但面临初始信号弱和类别不平衡的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决ETSC中准确性和早期性的冲突目标，捕捉早期细微模式，找到具有高可解释性的区分性子序列（形状）。&lt;h4&gt;方法&lt;/h4&gt;提出DE3S方法，包含三个创新：1)结合传统时间增强和基于注意力的全局时间增强的双增强策略；2)基于注意力分数的软形状稀疏化机制；3)双路径MoE和Inception模块融合架构。使用加权交叉熵损失处理类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实医疗数据集上进行了广泛实验，结果显示了最先进的性能，消融研究证实了各组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;DE3S方法成功解决了医疗早期时间序列分类中的准确性和早期性权衡问题，能够有效捕捉早期细微模式，提高ICU资源利用效率和医疗精准度。&lt;h4&gt;翻译&lt;/h4&gt;医疗应用中的早期时间序列分类(ETSC)对于ICU中败血症预测等时间敏感场景至关重要，大量死亡是由延迟预测引起的。ETSC可以显著提高ICU资源利用效率和医疗精准度。然而，它面临准确性和早期性的冲突目标，现有方法常常在两者之间权衡，由于初始信号弱和类别不平衡，难以捕捉早期的细微模式。解决这些挑战的关键是找到具有高可解释性的区分性子序列（或形状）。本文提出了用于医疗早期时间序列分类的双增强软稀疏形状学习(DE3S)，它引入了一种新的双增强软形状学习框架，通过三个创新精确找出形状：1)结合传统时间增强和基于注意力的全局时间增强的全面双增强策略，实现鲁棒的表示学习；2)基于注意力分数的软形状稀疏化机制，动态保留区分性模式，同时将不太重要的形状聚合成代表性标记；3)双路径专家混合网络(MoE)和Inception模块融合架构，其中MoE在形状内执行局部学习，多尺度Inception模块跨形状捕获全局模式。该框架使用加权交叉熵损失处理类别不平衡，并在主体一致性数据集上表现出鲁棒性。在六个真实医疗数据集上的广泛实验显示了最先进的性能，消融研究证实了组件的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early time-series classification (ETSC) in medical applications is crucialfor time-sensitive scenarios such as sepsis prediction in intensive care units(ICUs), where a large number of deaths are caused by delayed prediction. ETSCcan significantly improve ICU resource utilization efficiency and healthcareprecision. However, it faces conflicting goals of accuracy and earliness, withexisting methods often trading one for the other, struggling to capture subtleearly-stage patterns due to weak initial signals and class imbalance. The keyto solve these challenges is to find shapelets, which are discriminativesubsequences (or shapes) with high interpretability in time-seriesclassification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learningfor Medical Early Time-Series Classification (DE3S), which introduces a novelDual-Enhanced Soft-Shape Learning framework to figure out shapelets preciselythrough three innovations: (1) a comprehensive dual-enhancement strategycombines traditional temporal augmentation with attention-based global temporalenhancement for robust representation learning, (2) an attention-score-basedsoft shapelet sparsification mechanism dynamically preserves discriminativepatterns while aggregating less important shapelets into representative tokens,and (3) a dual-path Mixture of Experts Network (MoE) and Inception modulesfusion architecture where MoE performs local learning within shapelets andmulti-scale Inception modules capture global patterns across shapelets. Theframework employs weighted cross-entropy loss for class imbalance handling anddemonstrates robustness on subject-consistency datasets. Extensive experimentson six real-world medical datasets show state-of-the-art performance, withablation studies confirming component efficacy.</description>
      <author>example@mail.com (Tao Xie, Zexi Tan, Haoyi Xiao, Binbin Sun, Yiqun Zhang)</author>
      <guid isPermaLink="false">2510.12214v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SDGraph: Multi-Level Sketch Representation Learning by Sparse-Dense Graph Architecture</title>
      <link>http://arxiv.org/abs/2510.12192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对手绘草图的独特稀疏性和抽象性，提出了多级草图表示方案和SDGraph深度学习架构，有效利用草图中的有效信息，在多个下游任务中取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;手绘草图具有独特的稀疏性和抽象性，需要与图像不同的学习流程。然而，对于什么是有效的草图信息的研究有限，这限制了现有方法的性能。&lt;h4&gt;目的&lt;/h4&gt;系统地识别和利用草图中的有效信息，以提升草图学习方法的性能。&lt;h4&gt;方法&lt;/h4&gt;提出多级草图表示方案，将草图表示分为草图级、笔画级和点级三个层次；基于此开发了SDGraph深度学习架构，包含稀疏图和密集图两个互补模块，以及信息融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和实验评估，确定了草图中的有效信息；SDGraph在分类、检索和矢量草图生成任务上分别比最先进方法提高了1.15%、1.70%和36.58%的性能。&lt;h4&gt;结论&lt;/h4&gt;多级草图表示方案能够系统地识别有效信息，SDGraph架构能够有效利用这些信息，提升各种草图相关任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;手绘草图具有独特的稀疏性和抽象性，需要不同于图像的学习流程。对于草图学习方法，主要目标是充分利用草图中的有效信息。然而，关于什么是有效的草图信息的研究有限，这限制了现有方法的性能。为解决这一问题，我们首先提出了多级草图表示方案，系统地识别有效信息。该方案将草图表示分为三个层次：草图级、笔画级和点级。此设计基于分析元素的粒度，从粗（草图级）到细（点级），从而确保更全面地覆盖草图信息。对每个层次，我们进行了理论分析和实验评估，以识别和验证有效信息。基于上述研究，我们开发了SDGraph，这是一个深度学习架构，旨在利用三个层次中识别出的有效信息。SDGraph包含两个互补模块：稀疏图将笔画作为节点，用于草图级和笔画级表示学习；密集图将点作为节点，用于草图级和点级表示学习。两个模块都采用图卷积以及下采样和上采样操作，使其能够作为编码器和解码器。此外，信息融合模块连接两个图，进一步增强特征提取。SDGraph支持各种草图相关的下游任务，在分类和检索方面分别比最先进方法提高了1.15%和1.70%，在矢量草图生成质量上提高了36.58%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Freehand sketches exhibit unique sparsity and abstraction, necessitatinglearning pipelines distinct from those designed for images. For sketch learningmethods, the central objective is to fully exploit the effective informationembedded in sketches. However, there is limited research on what constituteseffective sketch information, which in turn constrains the performance ofexisting approaches. To tackle this issue, we first proposed the Multi-LevelSketch Representation Scheme to systematically identify the effectiveinformation. The scheme organizes sketch representation into three levels:sketch-level, stroke-level, and point-level. This design is based on thegranularity of analytical elements, from coarse (sketch-level) to fine(point-level), thereby ensuring more comprehensive coverage of the sketchinformation. For each level, we conducted theoretical analyses and experimentalevaluations to identify and validate the effective information. Building on theabove studies, we developed SDGraph, a deep learning architecture designed toexploit the identified effective information across the three levels. SDGraphcomprises two complementary modules: a Sparse Graph that treats strokes asnodes for sketch-level and stroke-level representation learning, and a DenseGraph that treats points as nodes for sketch-level and point-levelrepresentation learning. Both modules employ graph convolution along withdown-sampling and up-sampling operations, enabling them to function as bothencoder and decoder. Besides that, an information fusion module bridges the twographs to further enhance feature extraction. SDGraph supports a wide range ofsketch-related downstream tasks, achieving accuracy improvements of 1.15\% and1.70\% over the state-of-the-art in classification and retrieval, respectively,and 36.58\% improvement in vector sketch generation quality.</description>
      <author>example@mail.com (Xi Cheng, Pingfa Feng, Zhichao Liao, Mingyu Fan, Long Zeng)</author>
      <guid isPermaLink="false">2510.12192v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning</title>
      <link>http://arxiv.org/abs/2510.12107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了区分性表示学习(DRL)框架，通过增量并行适配器网络和解耦锚监督方法，解决了非重排类增量学习中的三大挑战，在保持高效率的同时显著提升了性能。&lt;h4&gt;背景&lt;/h4&gt;预训练模型(PTMs)在非重排类增量学习(CIL)研究中表现出色，但仍面临三大挑战：模型复杂度不断增加、增量学习过程中表示不平稳、以及阶段性子问题优化与全局推理之间不一致。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够有效解决非重排类增量学习中的三大挑战的框架，实现平稳的表示转移，并缩小阶段性局部优化与全局推理之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出区分性表示学习(DRL)框架，包含增量并行适配器(IPA)网络和解耦锚监督(DAS)机制。IPA网络基于预训练模型构建，通过学习轻量级适配器在每个增量阶段逐步增强模型；DAS机制通过分别比较正负样本与虚拟锚来解耦约束，促进区分性表示学习并实现对齐不同阶段特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准测试上的实验表明，DRL在整个CIL期间持续优于其他最先进方法，同时在训练和推理阶段都保持高效率，有效解决了模型复杂度、表示不平稳和优化不一致问题。&lt;h4&gt;结论&lt;/h4&gt;DRL框架通过创新性的网络架构和监督机制，成功解决了非重排类增量学习中的关键挑战，为该领域提供了高效且有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;凭借预训练模型(PTMs)的优秀表示能力，非重排类增量学习(CIL)研究取得了显著进展。然而，由于三个难题：日益增长的大模型复杂度、增量学习过程中不平稳的表示转移、以及阶段性子问题优化与全局推理之间的不一致性，这仍然是一个极具挑战性的任务。在这项工作中，我们提出了区分性表示学习(DRL)框架来专门解决这些挑战。为了有效且高效地进行增量学习，DRL的网络称为增量并行适配器(IPA)网络，它基于PTM构建，并通过在每个增量阶段学习轻量级适配器来逐步增强模型，参数学习开销小。该适配器负责使模型适应新类别，它可以通过它们之间的并行连接和传输门继承并传播当前模型的表示能力。因此，这种设计保证了不同增量阶段之间的平稳表示转移。此外，为了缓解不一致性并实现跨增量阶段可比的特征表示，我们设计了解耦锚监督(DAS)。它通过将正样本和负样本分别与虚拟锚进行比较来解耦它们的约束。这种解耦促进了区分性表示学习并对齐了在不同阶段学习的特征空间，从而缩小了在数据子集上进行阶段性局部优化与在所有类别上进行全局推理之间的差距。在六个基准测试上的大量实验表明，我们的DRL在整个CIL期间持续优于其他最先进方法，同时在训练和推理阶段都保持高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the excellent representation capabilities of Pre-Trained Models (PTMs),remarkable progress has been made in non-rehearsal Class-Incremental Learning(CIL) research. However, it remains an extremely challenging task due to threeconundrums: increasingly large model complexity, non-smooth representationshift during incremental learning and inconsistency between stage-wisesub-problem optimization and global inference. In this work, we propose theDiscriminative Representation Learning (DRL) framework to specifically addressthese challenges. To conduct incremental learning effectively and yetefficiently, the DRL's network, called Incremental Parallel Adapter (IPA)network, is built upon a PTM and increasingly augments the model by learning alightweight adapter with a small amount of parameter learning overhead in eachincremental stage. The adapter is responsible for adapting the model to newclasses, it can inherit and propagate the representation capability from thecurrent model through parallel connection between them by a transfer gate. As aresult, this design guarantees a smooth representation shift between differentincremental stages. Furthermore, to alleviate inconsistency and enablecomparable feature representations across incremental stages, we design theDecoupled Anchor Supervision (DAS). It decouples constraints of positive andnegative samples by respectively comparing them with the virtual anchor. Thisdecoupling promotes discriminative representation learning and aligns thefeature spaces learned at different stages, thereby narrowing the gap betweenstage-wise local optimization over a subset of data and global inference acrossall classes. Extensive experiments on six benchmarks reveal that our DRLconsistently outperforms other state-of-the-art methods throughout the entireCIL period while maintaining high efficiency in both training and inferencephases.</description>
      <author>example@mail.com (Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang)</author>
      <guid isPermaLink="false">2510.12107v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation</title>
      <link>http://arxiv.org/abs/2510.12054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为MIARec的学术论文推荐模型，通过基于引力的方法衡量学者之间的相互学术影响，并将其整合到图表示学习的特征聚合过程中，以解决传统基于图的推荐方法忽视学术网络中不对称学术影响的问题。&lt;h4&gt;背景&lt;/h4&gt;随着科学文献的快速扩张，学者们对精确且高质量的论文推荐需求日益增加。在各种推荐方法中，基于图的方法因其能有效利用学术网络中固有的结构特征而受到关注。然而，这些方法在学习图表示时往往忽视了学术网络中普遍存在的不对称学术影响。&lt;h4&gt;目的&lt;/h4&gt;解决传统基于图的推荐方法在学术网络推荐中忽视不对称学术影响的问题，提高科学论文推荐的准确性和质量。&lt;h4&gt;方法&lt;/h4&gt;提出Mutual-Influence-Aware Recommendation (MIARec)模型，采用基于引力的方法衡量学者之间的相互学术影响，并将这种影响整合到图表示学习中的消息传播过程的特征聚合中。此外，模型利用多通道聚合方法来捕获不同单一关系子网络的个体嵌入及其相互依赖的嵌入，从而能够更全面地理解异构学术网络。&lt;h4&gt;主要发现&lt;/h4&gt;在真实数据集上进行的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型，证明了其在科学论文推荐任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;MIARec模型通过考虑学者之间的相互学术影响和使用多通道聚合方法，能够更有效地进行科学论文推荐，其性能优于现有的基线模型。&lt;h4&gt;翻译&lt;/h4&gt;随着科学文献的快速扩张，学者们越来越需要精确和高质量的论文推荐。在各种推荐方法中，基于图的方法通过有效利用学术网络中固有的结构特征而受到关注。然而，这些方法在学习图表示时往往忽视了学术网络中普遍存在的不对称学术影响。为了解决这一局限，本研究提出了相互影响感知推荐(MIARec)模型，该模型采用基于引力的方法来衡量学者之间的相互学术影响，并将这种影响整合到图表示学习过程中消息传播的特征聚合中。此外，该模型利用多通道聚合方法来捕获不同单一关系子网络的个体嵌入及其相互依赖的嵌入，从而能够更全面地理解异构学术网络。在真实数据集上进行的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型，表明其在科学论文推荐任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid expansion of scientific literature, scholars increasinglydemand precise and high-quality paper recommendations. Among variousrecommendation methodologies, graph-based approaches have garnered attention byeffectively exploiting the structural characteristics inherent in scholarlynetworks. However, these methods often overlook the asymmetric academicinfluence that is prevalent in scholarly networks when learning graphrepresentations. To address this limitation, this study proposes theMutual-Influence-Aware Recommendation (MIARec) model, which employs agravity-based approach to measure the mutual academic influence betweenscholars and incorporates this influence into the feature aggregation processduring message propagation in graph representation learning. Additionally, themodel utilizes a multi-channel aggregation method to capture both individualembeddings of distinct single relational sub-networks and their interdependentembeddings, thereby enabling a more comprehensive understanding of theheterogeneous scholarly network. Extensive experiments conducted on real-worlddatasets demonstrate that the MIARec model outperforms baseline models acrossthree primary evaluation metrics, indicating its effectiveness in scientificpaper recommendation tasks.</description>
      <author>example@mail.com (Wenjin Xie, Tao Jia)</author>
      <guid isPermaLink="false">2510.12054v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition</title>
      <link>http://arxiv.org/abs/2510.11944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TopoAlign框架，利用代码仓库作为训练资源来提高大型语言模型在数学自动形式化任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在非正式和正式数学推理方面表现出色，但在将非正式数学陈述转换为正式陈述的自动形式化任务上仍有困难。当前数学LLMs的性能受限于包含非正式和正式陈述配对的大规模语料库的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;解决当前数学LLMs在自动形式化任务上的局限性，利用更广泛可用的代码仓库作为训练资源来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出TopoAlign框架，将代码分解为文档字符串、主函数和依赖函数，然后重新组装成结构上模仿正式陈述的类似物，产生结构对齐的代码数据用于训练。训练了DeepSeek-Math和Herald两个模型，并在minif2f、Putnam和ProofNet基准上评估。&lt;h4&gt;主要发现&lt;/h4&gt;TopoAlign显著提升了DeepSeek-Math的性能，在BEq@10上提高17.77%，在typecheck@10上提高68.82%。即使对于专业模型Herald，也在BEq@10和typecheck@10上分别提高了0.12%和1.09%，表明训练结构对齐的代码数据对各种模型都有益。&lt;h4&gt;结论&lt;/h4&gt;TopoAlign框架成功利用广泛可用的代码仓库作为训练资源，结构对齐的代码数据能有效提高数学LLMs在自动形式化任务上的性能，即使对专业模型也有改进作用。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在非正式和正式(如Lean 4)数学推理方面表现出色，但它们在自动形式化(即将非正式数学陈述转换为正式陈述的任务)方面仍有困难。自动形式化有助于将LLMs的非正式推理与形式证明助手配对，实现机器可验证的生成并减少幻觉。然而，当前数学LLMs的性能受限于大规模语料库的稀缺性，特别是包含非正式和正式陈述配对的语料库。尽管当前模型被训练为从自然语言指令生成代码，但这些代码与形式数学之间的结构和语法差异限制了有效的迁移学习。我们提出了TopoAlign框架，它解锁了广泛可用的代码仓库作为数学LLMs的训练资源。TopoAlign将代码分解为文档字符串、主函数和依赖函数，并将这些组件重新组装成结构上模仿正式陈述的类似物。这产生了结构对齐的代码数据，可用于训练数学LLMs而无需额外的人工注释。我们训练了两个最先进的模型DeepSeek-Math和Herald，并在minif2f、Putnam和ProofNet基准上评估它们。TopoAlign为DeepSeek-Math提供了显著提升，在BEq@10上提高17.77%，在typecheck@10上提高68.82%。尽管没有引入新的数学知识，我们的框架使Herald在BEq@10和typecheck@10上分别提高了0.12%和1.09%，证明了即使在专业模型上，训练结构对齐的代码数据也是有益的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)mathematical reasoning but still struggle with autoformalisation, the task oftransforming informal into formal mathematical statements. Autoformalisationhelps pair the informal reasoning of LLMs with formal proof assistants whichenable machine-verifiable generation and mitigate hallucinations. Yet, theperformance of current Math LLMs is constrained by the scarcity of large-scalecorpora, particularly those containing pairs of informal and formal statements.Although current models are trained to generate code from natural languageinstructions, structural and syntactic differences between these and formalmathematics limit effective transfer learning. We propose TopoAlign, aframework that unlocks widely available code repositories as training resourcesfor Math LLMs. TopoAlign decomposes code into docstrings, main functions, anddependency functions, and reassembles these components into analogues thatstructurally mirror formal statements. This produces structurally aligned codedata that can be used for training Math LLMs without requiring additional humanannotation. We train two state-of-the-art models, DeepSeek-Math and Herald, andevaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlignprovides substantial gains for DeepSeek-Math, improving performance by 17.77%on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematicalknowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10and typecheck@10, respectively, demonstrating that training on aligned codedata is beneficial even for specialized models.</description>
      <author>example@mail.com (Yupei Li, Philipp Borchert, Gerasimos Lampouras)</author>
      <guid isPermaLink="false">2510.11944v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer</title>
      <link>http://arxiv.org/abs/2510.11926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 12 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Locaris是一种仅解码器的大型语言模型，用于室内Wi-Fi定位，能够直接处理原始信号数据而无需预处理，在各种条件下表现出色且无需大量校准。&lt;h4&gt;背景&lt;/h4&gt;室内Wi-Fi定位具有挑战性，因为无线电信号对环境动态、信道传播特性和硬件异构性高度敏感。传统方法需要密集校准且在条件变化时性能迅速下降。&lt;h4&gt;目的&lt;/h4&gt;引入Locaris，一种仅解码器的大型语言模型，用于室内定位，解决传统方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Locaris将每个接入点测量视为token，摄取原始Wi-Fi遥测数据无需预处理。通过在不同Wi-Fi数据集上微调LLM，学习从原始信号到设备位置的轻量级且可泛化的映射。&lt;h4&gt;主要发现&lt;/h4&gt;Locaris匹配或超越现有技术；紧凑LLM可作为无校准回归模型；少样本适应实验显示高精度；亚米级精度仅需几百个样本；在缺少AP情况下仍稳健；支持所有可用遥测数据。&lt;h4&gt;结论&lt;/h4&gt;Locaris在室内定位实际应用中具有实用可行性，特别适用于大规模部署中广泛校准不可行的情况。&lt;h4&gt;翻译&lt;/h4&gt;室内Wi-Fi定位由于无线电信号对环境动态、信道传播特性和硬件异构性的高度敏感性而仍然是一个具有挑战性的问题。传统的指纹识别和基于模型的方法通常需要密集的校准，并且在设备、信道或部署条件变化时性能迅速下降。在本文中，我们引入了Locaris，一种用于室内定位的仅解码器大型语言模型（LLM）。Locaris将每个接入点（AP）测量视为一个token，能够摄取原始的Wi-Fi遥测数据而无需预处理。通过在不同的Wi-Fi数据集上微调其LLM，Locaris学习从原始信号直接到设备位置的轻量级且可泛化的映射。我们将Locaris与最先进的方法进行比较实验研究，一致表明Locaris在各种类型的遥测数据上匹配或超越现有技术。我们的结果表明，紧凑的LLM可以作为室内定位的无校准回归模型，在异构Wi-Fi部署中提供可扩展和稳健的跨环境性能。使用每个设备仅少数几个校准点的少样本适应实验进一步表明，当应用于未见过的设备和部署场景时，Locaris保持高精度。这仅需几百个样本就能实现亚米级精度，在缺少AP的情况下保持稳健性能，并支持所有可用的遥测数据。我们的发现突显了Locaris在现实场景室内定位中的实际可行性，特别是在广泛校准不可行的大规模部署中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor Wi-Fi positioning remains a challenging problem due to the highsensitivity of radio signals to environmental dynamics, channel propagationcharacteristics, and hardware heterogeneity. Conventional fingerprinting andmodel-based approaches typically require labor-intensive calibration and sufferrapid performance degradation when devices, channel or deployment conditionschange. In this paper, we introduce Locaris, a decoder-only large languagemodel (LLM) for indoor localization. Locaris treats each access point (AP)measurement as a token, enabling the ingestion of raw Wi-Fi telemetry withoutpre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locarislearns a lightweight and generalizable mapping from raw signals directly todevice location. Our experimental study comparing Locaris with state-of-the-artmethods consistently shows that Locaris matches or surpasses existingtechniques for various types of telemetry. Our results demonstrate that compactLLMs can serve as calibration-free regression models for indoor localization,offering scalable and robust cross-environment performance in heterogeneousWi-Fi deployments. Few-shot adaptation experiments, using only a handful ofcalibration points per device, further show that Locaris maintains highaccuracy when applied to previously unseen devices and deployment scenarios.This yields sub-meter accuracy with just a few hundred samples, robustperformance under missing APs and supports any and all available telemetry. Ourfindings highlight the practical viability of Locaris for indoor positioning inthe real-world scenarios, particularly in large-scale deployments whereextensive calibration is infeasible.</description>
      <author>example@mail.com (Nayan Sanjay Bhatia, Pranay Kocheta, Russell Elliott, Harikrishna S. Kuttivelil, Katia Obraczka)</author>
      <guid isPermaLink="false">2510.11926v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis</title>
      <link>http://arxiv.org/abs/2510.11829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了生成式AI与Schrödinger bridge问题的联系，提出了一种软约束方法来解决经典SBP的稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;生成式AI可视为学习将简单参考映射为复杂数据分布的模型，与Schrödinger bridge问题有强联系，因两者都通过熵正则化随机动力学在指定边际间插值。&lt;h4&gt;目的&lt;/h4&gt;解决经典SBP强制执行硬终端约束导致的不稳定性问题，特别是在高维或数据稀缺情况下。&lt;h4&gt;方法&lt;/h4&gt;采用软约束Schrödinger bridge问题(SCSBP)框架，将终端约束替换为一般惩罚函数，建立McKean-Vlasov类型随机控制公式，并证明随惩罚增加，控制和值函数以线性速率收敛到经典SBP。&lt;h4&gt;主要发现&lt;/h4&gt;建立了所有惩罚水平下最优解的存在性；首次为软约束桥提供定量收敛保证；揭示惩罚正则化如何实现鲁棒的生成建模、微调和迁移学习。&lt;h4&gt;结论&lt;/h4&gt;软约束SBP为解决经典SBP在高维和数据稀缺情况下的不稳定性提供了有效方法，对生成式AI领域有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;生成式AI可以被构建为学习将简单参考测度映射为复杂数据分布的问题，最近由于它们通过熵正则化随机动力学在指定边际之间插值的共同特性，它与经典的Schrödinger bridge问题理论有很强的联系。然而，经典SBP强制执行硬终端约束，这往往导致实际实现中的不稳定性，特别是在高维或数据稀缺的情况下。为应对这一挑战，我们遵循所谓的软约束Schrödinger bridge问题的思路，其中终端约束被一般惩罚函数所取代。这种松弛导致更灵活的McKean-Vlasov类型的随机控制公式。我们建立了所有惩罚水平下最优解的存在性，并证明随着惩罚的增加，控制和值函数以线性速率收敛到经典SBP。我们的分析基于Doob的h变换表示、Schrödinger势的稳定性结果、Gamma-收敛以及一种新的固定点论据，该论据将测度空间上的优化问题与辅助的熵最优传输问题耦合。这些结果不仅首次为软约束桥提供了定量收敛保证，还揭示了惩罚正则化如何实现鲁棒的生成建模、微调和迁移学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative AI can be framed as the problem of learning a model that mapssimple reference measures into complex data distributions, and it has recentlyfound a strong connection to the classical theory of the Schr\"odinger bridgeproblems (SBPs) due partly to their common nature of interpolating betweenprescribed marginals via entropy-regularized stochastic dynamics. However, theclassical SBP enforces hard terminal constraints, which often leads toinstability in practical implementations, especially in high-dimensional ordata-scarce regimes. To address this challenge, we follow the idea of theso-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which theterminal constraint is replaced by a general penalty function. This relaxationleads to a more flexible stochastic control formulation of McKean-Vlasov type.  We establish the existence of optimal solutions for all penalty levels andprove that, as the penalty grows, both the controls and value functionsconverge to those of the classical SBP at a linear rate. Our analysis builds onDoob's h-transform representations, the stability results of Schr\"odingerpotentials, Gamma-convergence, and a novel fixed-point argument that couples anoptimization problem over the space of measures with an auxiliary entropicoptimal transport problem. These results not only provide the firstquantitative convergence guarantees for soft-constrained bridges but also shedlight on how penalty regularization enables robust generative modeling,fine-tuning, and transfer learning.</description>
      <author>example@mail.com (Jin Ma, Ying Tan, Renyuan Xu)</author>
      <guid isPermaLink="false">2510.11829v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Denoised Diffusion for Object-Focused Image Augmentation</title>
      <link>http://arxiv.org/abs/2510.08955v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对数据稀缺条件下动物健康监测的数据增强框架，通过分割动物图像并进行变换和扩散合成，生成多样化场景，提高动物检测和监测性能。&lt;h4&gt;背景&lt;/h4&gt;现代农业依赖集成监测系统，其中基于无人机的动物健康监测是关键，但面临数据有限、动物体积小、被遮挡或部分可见等问题。迁移学习方法因缺乏反映特定农场条件的大型数据集而效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对特定问题、以动物为中心的数据增强策略，专为数据受限条件下的动物健康监测设计，解决数据稀缺与实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出面向对象的数据增强框架，将动物从背景中分割出来，通过变换和基于扩散的合成技术增强图像，创建真实、多样化的场景，以提高动物检测和监测性能。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，与基线模型相比，增强数据集在动物检测任务上表现更优。通过生成领域特定数据，该方法支持数据稀缺场景下的实时动物健康监测。&lt;h4&gt;结论&lt;/h4&gt;该数据增强方法能够弥合有限数据与实际应用之间的差距，即使在数据稀缺的情况下也能支持实时动物健康监测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现代农业生产操作越来越依赖于集成监测系统，这些系统结合多种数据源以优化农场运营。基于空中无人机的动物健康监测是关键组成部分，但面临数据可用性有限的问题，加上场景特定问题如动物体积小、被遮挡或部分可见。由于缺乏反映特定农场条件（包括动物品种、环境和行为的差异）的大型数据集，迁移学习方法通常无法解决这一限制。因此，需要开发一种针对特定问题、以动物为中心的数据增强策略，专门为这些独特挑战量身定制。为解决这一差距，我们提出了一种面向对象的数据增强框架，专门为数据受限条件下的动物健康监测设计。我们的方法将动物从背景中分割出来，并通过变换和基于扩散的合成来增强它们，创建真实、多样化的场景，以提高动物检测和监测性能。我们的初步实验表明，与基线模型相比，我们的增强数据集在动物检测任务上取得了更好的性能。通过生成领域特定的数据，我们的方法即使在数据稀缺的情况下也能支持实时动物健康监测解决方案，弥合了有限数据与实际应用之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern agricultural operations increasingly rely on integrated monitoringsystems that combine multiple data sources for farm optimization. Aerialdrone-based animal health monitoring serves as a key component but faceslimited data availability, compounded by scene-specific issues such as small,occluded, or partially visible animals. Transfer learning approaches often failto address this limitation due to the unavailability of large datasets thatreflect specific farm conditions, including variations in animal breeds,environments, and behaviors. Therefore, there is a need for developing aproblem-specific, animal-focused data augmentation strategy tailored to theseunique challenges. To address this gap, we propose an object-focused dataaugmentation framework designed explicitly for animal health monitoring inconstrained data settings. Our approach segments animals from backgrounds andaugments them through transformations and diffusion-based synthesis to createrealistic, diverse scenes that enhance animal detection and monitoringperformance. Our initial experiments demonstrate that our augmented datasetyields superior performance compared to our baseline models on the animaldetection task. By generating domain-specific data, our method empowersreal-time animal health monitoring solutions even in data-scarce scenarios,bridging the gap between limited data and practical applicability.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2510.08955v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>TopROI: A topology-informed network approach for tissue partitioning</title>
      <link>http://arxiv.org/abs/2510.12772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 11 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为TopROI的新方法，用于将点云数据分割为感兴趣区域(ROI)。该方法结合了几何感知网络和持续同调理论，能够同时保留局部几何结构和高级组织架构。在模拟腺体结构和结直肠癌活检数据上验证，该方法优于传统分割方法，能够更好地保留生物学上有意义的结构，并揭示了从健康到癌变的连续性组织变化。&lt;h4&gt;背景&lt;/h4&gt;哺乳动物组织架构对生物功能至关重要，其破坏是疾病的标志。医学成像技术可生成大型点云数据集捕捉疾病进展中的细胞变化，但传统感兴趣区域(ROI)定义方法基于象限(quadrat-based)，忽略了组织内在结构，可能导致有意义的特征碎片化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留局部几何结构和高级架构的点云分割方法，用于定义生物学上有意义的ROI，从而更准确地量化组织结构并获取与疾病进展相关结构变化的新见解。&lt;h4&gt;方法&lt;/h4&gt;TopROI是一种基于拓扑感知和网络的方法，将几何感知网络与持续同调(persistent homology)相结合，利用细胞邻域和多重尺度循环来指导社区检测，从而识别有意义的ROI。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在模拟腺体结构的合成点云上，TopROI优于传统方法，能维持生物学合理的ROI几何并保留真实结构；2) 在结直肠癌活检数据上，TopROI生成保留类似隐窝结构的ROI，允许进行持续同调分析；3) 研究揭示了从健康黏膜到癌变的连续性组织变化，反映了结构渐进性无序化。&lt;h4&gt;结论&lt;/h4&gt;TopROI为定义大型点云中生物学上有意义的ROI提供了原则性和灵活的框架，能更准确量化组织结构，并提供与疾病进展相关结构变化的新见解。&lt;h4&gt;翻译&lt;/h4&gt;哺乳动物组织架构对生物功能至关重要，其破坏是疾病的标志。医学成像技术可以生成大型点云数据集，捕捉疾病进展过程中组织细胞成分的变化。然而，感兴趣区域(ROI)通常基于象限方法定义，这些方法忽略了内在结构，可能导致有意义的特征碎片化。在此，我们介绍TopROI，一种基于拓扑感知的网络方法，用于将点云分割为ROI，同时保留局部几何结构和高级架构。TopROI将几何感知网络与持续同调相结合，利用细胞邻域和多重尺度循环来指导社区检测。应用于模拟腺体结构的合成点云时，TopROI通过维持生物学上合理的ROI几何形状和更好地保留真实结构，优于基于象限和纯粹几何的分割方法。应用于从人类结直肠癌活检获得的细胞点云时，TopROI生成保留类似隐窝结构的ROI，并允许对单个区域进行持续同调分析。本研究揭示了从健康黏膜到癌变的连续性 architectural 变化，反映了组织结构的渐进性无序化。因此，TopROI为定义大型点云中生物学上有意义的ROI提供了一个原则性和灵活的框架，能够更准确地量化组织结构并提供与疾病进展相关的结构变化的新见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学成像生成的大规模点云数据中如何定义'感兴趣区域'(ROIs)的问题。当前常用的基于网格的分区方法忽略了组织结构的内在特性，可能导致有意义的结构特征被分割。这个问题很重要，因为哺乳动物组织结构对生物功能至关重要，其破坏是疾病的标志，而准确的ROI定义对于研究疾病进展过程中组织结构变化、进行下游分析和诊断至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到传统网格方法无法保留组织拓扑结构，因此考虑结合计算几何和拓扑数据分析的最新进展。他们借鉴了Delaunay三角剖分来表示局部细胞关系、持久同调技术提取拓扑特征、以及Leiden算法进行社区检测。作者创新性地将这些现有方法整合到一个统一框架中，同时考虑几何和拓扑信息，从而更有效地定义ROI。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合几何信息和拓扑信息来定义ROI，使分区既能保留局部几何结构，又能捕捉更高阶的组织架构。流程包括：1)构建Delaunay几何网络并赋予权重；2)通过持久同调计算拓扑特征并构建拓扑网络；3)整合几何和拓扑网络为一个加权网络；4)使用Leiden算法进行社区检测，最终得到的社区即为定义的ROI。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创新性地整合几何网络与持久同调特征；2)提供多尺度分析能力；3)能保留如腺体结构等生物学相关结构；4)方法具有可扩展性和灵活性。相比之前工作，TopROI不同于传统网格方法(不考虑组织内在特性)、纯几何方法(无法捕捉高阶结构)和纯拓扑方法(缺乏几何细节)，是首个将计算几何、网络科学和拓扑数据分析整合用于ROI定义的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopROI通过整合几何网络和拓扑数据分析，提供了一种能够同时保留局部几何结构和更高阶组织架构的灵活框架，用于定义生物学上有意义的感兴趣区域，从而更准确地量化组织结构并揭示与疾病进展相关的结构变化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mammalian tissue architecture is central to biological function, and itsdisruption is a hallmark of disease. Medical imaging techniques can generatelarge point cloud datasets that capture changes in the cellular composition ofsuch tissues with disease progression. However, regions of interest (ROIs) areusually defined by quadrat-based methods that ignore intrinsic structure andrisk fragmenting meaningful features. Here, we introduce TopROI, atopology-informed, network-based method for partitioning point clouds into ROIsthat preserves both local geometry and higher-order architecture. TopROIintegrates geometry-informed networks with persistent homology, combining cellneighbourhoods and multiscale cycles to guide community detection. Applied tosynthetic point clouds that mimic glandular structure, TopROI outperformsquadrat-based and purely geometric partitions by maintaining biologicallyplausible ROI geometry and better preserving ground-truth structures. Appliedto cellular point clouds obtained from human colorectal cancer biopsies, TopROIgenerates ROIs that preserve crypt-like structures and enable persistenthomology analysis of individual regions. This study reveals a continuum ofarchitectural changes from healthy mucosa to carcinoma, reflecting progressivedisorganisation in tissue structure. TopROI thus provides a principled andflexible framework for defining biologically meaningful ROIs in large pointclouds, enabling more accurate quantification of tissue organization and newinsights into structural changes associated with disease progression.</description>
      <author>example@mail.com (Sergio Serrano de Haro Iváñez, Joshua W. Moore, Lucile Grzesiak, Eoghan J. Mullholand, Heather Harrington, Simon J. Leedham, Helen M. Byrne)</author>
      <guid isPermaLink="false">2510.12772v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points</title>
      <link>http://arxiv.org/abs/2510.12524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Voronoi-Assisted Diffusion (VAD)的轻量级、无网络方法，用于直接从无方向点云计算无符号距离场(UDF)。该方法通过双向法线分配、法线扩散和UDF梯度场积分实现，能够高效稳定地处理各种复杂几何结构。&lt;h4&gt;背景&lt;/h4&gt;无符号距离场(UDF)可以表示具有任意拓扑结构的3D形状，包括开放和封闭表面、可定向和不可定向几何以及非流形结构。然而，现有的神经方法在数值稳定性、计算成本和可控性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、计算稳定且高效的方法来直接从无方向点云计算UDF，解决现有神经方法的数值不稳定性和高计算成本问题。&lt;h4&gt;方法&lt;/h4&gt;VAD方法包括三个主要步骤：(1)基于Voronoi的几何标准通过能量函数引导为输入点分配双向法线；(2)将法线扩散形成近似UDF梯度场；(3)积分梯度场恢复最终UDF。&lt;h4&gt;主要发现&lt;/h4&gt;VAD能够稳健处理封闭和开放表面，以及复杂的非流形和不可定向几何，同时保持计算效率和稳定性。&lt;h4&gt;结论&lt;/h4&gt;VAD是一种有效的方法，可以克服现有神经方法在计算UDF时的数值不稳定性和高计算成本问题，同时提供更好的可控性。&lt;h4&gt;翻译&lt;/h4&gt;无符号距离场(UDF)为具有任意拓扑结构的3D形状提供了灵活的表示，包括开放和封闭表面、可定向和不可定向几何以及非流形结构。虽然最近的神经方法在学习UDF方面显示出潜力，但它们常常面临数值不稳定、计算成本高和可控性有限等问题。我们提出了一种轻量级、无网络的方法Voronoi-Assisted Diffusion (VAD)，用于直接从无方向点云计算UDF。我们的方法首先通过能量函数中编码的两个基于Voronoi的几何标准引导，为输入点分配双向法线以实现最佳对齐。然后将对齐的法线扩散形成近似UDF梯度场，随后积分恢复最终UDF。实验证明，VAD能够稳健处理封闭和开放表面，以及复杂的非流形和不可定向几何，同时保持计算效率和稳定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从无方向点云计算无符号距离场（UDFs）的问题。这个问题在3D重建、计算机图形学和几何处理等领域非常重要，因为UDFs可以表示具有任意拓扑的3D形状，包括开放和封闭表面、可定向和不可定向几何体以及非流形结构，而现有的神经学习方法往往面临数值不稳定、计算成本高和可控性有限的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到热方法（heat method）的启发，该方法用于在定向数据上计算有符号距离场。他们引入了投影距离场概念，并将其与Voronoi图联系起来，将点方向视为优化变量。作者借鉴了现有工作中的多个方面：热方法用于扩散法线、投影距离场概念、Voronoi图在几何处理中的应用、泊松表面重建框架以及基于广义回转数的方法，但将这些元素组合成一种新的方法来解决UDF计算问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用Voronoi辅助扩散（VAD）框架，首先对输入点分配双向法线，然后将对齐的法线扩散以形成近似的UDF梯度场，最后通过积分恢复最终的UDF。整体流程包括：1) 构建Voronoi图并初始化双向法线；2) 通过最小化Voronoi边界上的不连续性来优化双向法线；3) （可选）对于嘈杂输入，优化点位置；4) 将双向法线扩散并融合成一致的向量场；5) 求解泊松方程重建UDF。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Voronoi辅助框架优化双向法线；2) 投影距离场概念与Voronoi图结合；3) 将点方向视为优化变量；4) 提出轻量级、无网络方法；5) 能够处理复杂几何结构。相比之前的工作，该方法避免了神经网络的数值不稳定和高计算成本问题；不需要明确的内外区分，能够处理非封闭表面；不需要边界方向，能够处理非流形结构；相比其他UDF方法提供了更好的可控性和稳定性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Voronoi辅助扩散（VAD）方法，一种从无方向点云计算无符号距离场的轻量级、无网络方法，能够高效准确地处理开放表面、非流形结构和不可定向几何体等复杂情况。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsigned Distance Fields (UDFs) provide a flexible representation for 3Dshapes with arbitrary topology, including open and closed surfaces, orientableand non-orientable geometries, and non-manifold structures. While recent neuralapproaches have shown promise in learning UDFs, they often suffer fromnumerical instability, high computational cost, and limited controllability. Wepresent a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),for computing UDFs directly from unoriented point clouds. Our approach beginsby assigning bi-directional normals to input points, guided by twoVoronoi-based geometric criteria encoded in an energy function for optimalalignment. The aligned normals are then diffused to form an approximate UDFgradient field, which is subsequently integrated to recover the final UDF.Experiments demonstrate that VAD robustly handles watertight and open surfaces,as well as complex non-manifold and non-orientable geometries, while remainingcomputationally efficient and stable.</description>
      <author>example@mail.com (Jiayi Kong, Chen Zong, Junkai Deng, Xuhui Chen, Fei Hou, Shiqing Xin, Junhui Hou, Chen Qian, Ying He)</author>
      <guid isPermaLink="false">2510.12524v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Scene Coordinate Reconstruction Priors</title>
      <link>http://arxiv.org/abs/2510.12387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025, Project page: https://nianticspatial.github.io/scr-priors/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种对场景坐标回归(SCR)模型进行概率性重新解释的方法，通过引入高级重建先验来改善3D场景表示。研究团队探索了多种先验方法，并训练了3D点云扩散模型，这些先验有助于学习更好的场景表示，提高点云质量、配准率和相机姿态，并对下游任务产生积极影响。&lt;h4&gt;背景&lt;/h4&gt;场景坐标回归(SCR)模型已被证明是3D视觉中强大的隐式场景表示方法，能够实现视觉重定位和运动恢复。然而，这些模型是针对单个场景专门训练的，如果训练图像暗示了不足的多视图约束，SCR模型就会退化。&lt;h4&gt;目的&lt;/h4&gt;通过引入高级重建先验来改善SCR模型的学习过程，提高场景表示质量，解决在多视图约束不足情况下模型退化的问题。&lt;h4&gt;方法&lt;/h4&gt;研究团队提出了一种对SCR模型进行概率性重新解释的方法，并探索了多种先验技术：1)简单的深度值分布先验；2)学习合理的场景坐标配置先验；3)在大型室内扫描数据集上训练3D点云扩散模型。这些先验在每个训练步骤中将预测的3D场景点推向合理的几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;在三个室内数据集上，研究团队发现：1)引入的先验有助于学习更好的场景表示；2)产生了更一致的场景点云；3)提高了配准率；4)改善了相机姿态；5)对下游任务如新视图合成和相机重定位有积极影响。&lt;h4&gt;结论&lt;/h4&gt;通过概率性重新解释SCR模型并引入高级重建先验，可以显著改善场景表示的质量，即使在多视图约束不足的情况下也能有效工作，从而提高各种3D视觉任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;场景坐标回归(SCR)模型已被证明是3D视觉中强大的隐式场景表示方法，能够实现视觉重定位和运动恢复。SCR模型是针对单个场景专门训练的。如果训练图像暗示了不足的多视图约束，SCR模型就会退化。我们提出了一种对训练SCR模型进行概率性重新解释的方法，使我们能够注入高级重建先验。我们研究了多种这样的先验，从对重建深度值分布的简单先验，到对合理场景坐标配置的学习先验。对于后者，我们在大型室内扫描语料库上训练了一个3D点云扩散模型。我们的先验在每个训练步骤中将预测的3D场景点推向合理的几何形状，以提高它们的可能性。在三个室内数据集上，我们的先验有助于学习更好的场景表示，产生更一致的场景点云，更高的配准率和更好的相机姿态，对新视图合成和相机重定位等下游任务有积极影响。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决场景坐标回归(SCR)模型在多视图约束不足时的退化问题，特别是在纹理贫乏区域、重复结构等场景下出现的点云分散、相机姿态估计不准确等问题。这个问题在现实中很重要，因为它直接影响室内场景重建质量、相机重定位精度和新视图合成效果，进而影响AR/VR等应用的用户体验；在研究中，它代表了提升神经SfM模型鲁棒性的重要挑战，特别是在缺乏足够视觉重叠的场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了SCR模型在缺乏多视图约束时退化的原因，然后通过概率重新解释SCR训练过程，将重建先验作为负对数似然项融入训练目标。他们设计了三种先验：深度分布先验、深度先验(RGB-D)和3D点云扩散先验。该方法借鉴了ACE框架的架构，从DiffusioNeRF获取灵感但改为在3D空间直接正则化，并首次将扩散模型应用于场景级别的3D点云生成，而非仅限于单个对象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SCR训练重新表述为最大似然学习，引入重建先验来引导模型学习更合理的场景表示，使预测的3D场景点朝向合理的几何形状。整体流程包括：1)使用ACE框架进行场景坐标回归，包含特征提取器和回归头；2)将训练目标重新表述为最大化场景坐标的概率，添加负对数先验作为正则化项；3)实现三种先验：深度分布先验、深度先验和点云扩散先验；4)在训练过程中联合优化重投影误差和先验项，扩散先验只在训练迭代5k后应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)概率性重新解释SCR训练以融入重建先验；2)提出多种重建先验（深度分布先验、深度先验和3D点云扩散先验）；3)首次将扩散模型应用于场景级别的3D点云生成；4)开发有效的RGB-D版本的ACE。相比之前的工作，本文方法通过高级先验正则化SCR训练，而非简单依赖场景特定训练；与特征匹配方法相比，提供更强的几何约束；与ACE框架相比，联合优化重投影误差和先验项而非交替优化；与3D扩散模型相比，应用于整个室内场景而非单个对象。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过概率性重新解释场景坐标回归训练并引入多种重建先验，显著提升了室内场景重建质量和相机姿态估计准确性，同时保持了测试时的高效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene coordinate regression (SCR) models have proven to be powerful implicitscene representations for 3D vision, enabling visual relocalization andstructure-from-motion. SCR models are trained specifically for one scene. Iftraining images imply insufficient multi-view constraints SCR modelsdegenerate. We present a probabilistic reinterpretation of training SCR models,which allows us to infuse high-level reconstruction priors. We investigatemultiple such priors, ranging from simple priors over the distribution ofreconstructed depth values to learned priors over plausible scene coordinateconfigurations. For the latter, we train a 3D point cloud diffusion model on alarge corpus of indoor scans. Our priors push predicted 3D scene points towardsplausible geometry at each training step to increase their likelihood. On threeindoor datasets our priors help learning better scene representations,resulting in more coherent scene point clouds, higher registration rates andbetter camera poses, with a positive effect on down-stream tasks such as novelview synthesis and camera relocalization.</description>
      <author>example@mail.com (Wenjing Bian, Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann)</author>
      <guid isPermaLink="false">2510.12387v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation</title>
      <link>http://arxiv.org/abs/2510.12095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages main paper; 15 pages references and appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了IL3D，一个专为大型语言模型驱动的3D场景生成设计的大规模数据集，包含27,816个室内布局和29,215个高保真3D对象资源。&lt;h4&gt;背景&lt;/h4&gt;室内布局设计领域对多样化、高质量训练数据有迫切需求，现有数据集可能无法满足大型语言模型训练的要求。&lt;h4&gt;目的&lt;/h4&gt;创建一个支持多模态学习的3D场景生成数据集，用于提升大语言模型在视觉语言任务中的表现，并推动3D场景生成和具身智能的研究。&lt;h4&gt;方法&lt;/h4&gt;构建包含18种常见房间类型的室内布局数据集，添加实例级自然语言注释，建立严格的评估基准，测试监督微调方法在数据集上的效果。&lt;h4&gt;主要发现&lt;/h4&gt;在IL3D上对大型语言模型进行监督微调显著提高了模型的泛化能力，性能优于在其他数据集上的微调结果；数据集提供多种多模态数据导出格式，可适应各种视觉任务需求。&lt;h4&gt;结论&lt;/h4&gt;IL3D作为一个多功能且强大的资源，通过提供高保真场景数据，显著推动了3D场景生成和具身智能的研究进展，特别是支持了具身智能体的环境感知任务。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了IL3D，一个精心设计的大型数据集，用于大型语言模型驱动的3D场景生成，解决了室内布局设计中多样化、高质量训练数据的迫切需求。IL3D包含18种常见房间类型的27,816个室内布局和29,215个高保真3D对象资源库，并添加了实例级自然语言注释，以支持视觉语言任务的多模态学习。我们建立了严格的基准来评估LLM驱动的场景生成。实验结果表明，在IL3D上对LLM进行监督微调显著提高了泛化能力，并优于在其他数据集上的SFT性能。IL3D提供灵活的多模态数据导出功能，包括点云、3D边界框、多视图图像、深度图、法线图和语义掩码，能够无缝适应各种视觉任务。作为一个多功能且强大的资源，IL3D通过提供高保真场景数据来支持具身智能体的环境感知任务，显著推动了3D场景生成和具身智能的研究进展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏专门为LLM驱动的3D场景生成设计的大规模高质量室内布局数据集问题。这个问题很重要，因为3D室内场景生成是连接具身智能、智能家居设计、虚拟现实交互和机器人环境感知的关键技术，而精确的室内场景建模依赖于高质量的合成数据集，现有数据集在场景多样性、注释完整性和多模态适应性方面存在明显局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，然后整合了3D-FRONT和HSSD数据集资源，通过人工清理和有针对性的合成数据补充不足。方法设计上采用USD格式使LLM可直接读取场景信息，并使用Qwen3-VL生成详细实例级描述。作者借鉴了现有数据集的经验，整合了3D-FRONT和HSSD的资源，采用HOLODECK方法合成缺失场景类型，并在格式设计和评估指标方面参考了现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多样化的室内布局数据集，提供实例级自然语言注释以支持多模态学习，确保室内布局符合现实世界的功能逻辑，并覆盖不同面积和物体密度的室内场景。整体流程包括：整合现有数据集并人工清理；使用HOLODECK方法合成缺失场景类型；将数据转换为USDZ和USDA格式；为对象提供多级注释并使用Qwen3-VL生成详细描述；设计客观和主观评估指标；支持多种数据格式的灵活导出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：大规模数据集（27,816个室内布局和29,215个对象模型）；覆盖18种常见房间类型；提供实例级自然语言注释；支持多种数据格式导出；使用USD格式实现文本可读性。相比之前工作，IL3D规模更大（超过3D-FRONT和HSSD），提供更全面的注释（大多数现有数据集缺乏自然语言注释），确保更好的功能逻辑，具有更强的多模态适应性，并专为LLM设计使其可直接读取和解析场景信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IL3D数据集通过提供大规模、多样化的室内布局和丰富的自然语言注释，显著提升了LLM驱动的3D室内场景生成质量和3D感知任务的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we present IL3D, a large-scale dataset meticulously designedfor large language model (LLM)-driven 3D scene generation, addressing thepressing demand for diverse, high-quality training data in indoor layoutdesign. Comprising 27,816 indoor layouts across 18 prevalent room types and alibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched withinstance-level natural language annotations to support robust multimodallearning for vision-language tasks. We establish rigorous benchmarks toevaluate LLM-driven scene generation. Experimental results show that supervisedfine-tuning (SFT) of LLMs on IL3D significantly improves generalization andsurpasses the performance of SFT on other datasets. IL3D offers flexiblemultimodal data export capabilities, including point clouds, 3D bounding boxes,multiview images, depth maps, normal maps, and semantic masks, enablingseamless adaptation to various visual tasks. As a versatile and robustresource, IL3D significantly advances research in 3D scene generation andembodied intelligence, by providing high-fidelity scene data to supportenvironment perception tasks of embodied agents.</description>
      <author>example@mail.com (Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu)</author>
      <guid isPermaLink="false">2510.12095v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>dN/dx Reconstruction with Deep Learning for High-Granularity TPCs</title>
      <link>http://arxiv.org/abs/2510.10628v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Graph Point Transformer (GraphPT)的深度学习模型，用于高粒度时间投影室中的dN/dx重建，以提高粒子识别性能。该模型将TPC数据表示为点云，采用基于图神经网络的U-Net架构，并使用针对点云处理优化的注意力机制。实验表明，GraphPT模型在K/π粒子识别方面比传统方法提高了10%至20%的分离能力。&lt;h4&gt;背景&lt;/h4&gt;粒子识别(PID)对未来的粒子物理实验如圆形正负电子对撞机和未来圆形对撞机至关重要。高粒度时间投影室(TPC)不仅能提供精确的跟踪，还能实现dN/dx测量用于粒子识别。&lt;h4&gt;目的&lt;/h4&gt;引入一种深度学习模型Graph Point Transformer (GraphPT)用于dN/dx重建，解决准确重建面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;将TPC数据表示为点云，采用基于图神经网络的U-Net架构作为网络主干，并融入针对点云处理优化的注意力机制进行节点聚合。&lt;h4&gt;主要发现&lt;/h4&gt;GraphPT模型在PID性能上超越了传统的截断均值方法，特别是在5到20 GeV/c的动量区间内，K/π分离能力提高了约10%至20%。&lt;h4&gt;结论&lt;/h4&gt;GraphPT模型是dN/dx重建的有效方法，能显著提高粒子识别性能，对未来粒子物理实验具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;粒子识别对于未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高粒度时间投影室不仅能提供精确的跟踪，还能实现dN/dx测量用于粒子识别。dN/dx方法估计初级电离电子的数量，为PID性能提供了显著改进。然而，准确的重建仍然是该方法面临的主要挑战。在本文中，我们介绍了一种深度学习模型——图点变换器（GraphPT），用于dN/dx重建。在我们的方法中，TPC数据被表示为点云。然后网络主干采用基于图神经网络的U-Net架构，结合了针对点云处理优化的注意力机制进行节点聚合。所提出的GraphPT模型在PID性能上超越了传统的截断均值方法。特别是在5到20 GeV/c的动量区间内，K/π分离能力提高了约10%至20%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高粒度时间投影室(TPC)中的dN/dx重建挑战。dN/dx方法通过估计初级电离电子数量来提高粒子识别(PID)性能，这对未来粒子物理实验如环形正负电子对撞机至关重要。准确重建面临长漂移距离导致的电子扩散、重叠簇区分困难等问题，解决这些问题能显著提升粒子在高动量区域的区分能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了传统截断均值方法的局限性，然后借鉴了多个现有工作：受点变换器(Point Transformer)启发采用U-Net架构；将TPC数据表示为点云并使用图神经网络(GNN)处理；结合自注意力机制和GNN优势设计GraphPT模型；探索了减法算子和点积算子两种注意力机制，其中点积算子是本文创新。作者通过将轨迹表示为点云，利用图神经网络学习点间关系来区分初级和次级电子。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将TPC电子轨迹表示为点云，利用图神经网络和变换器架构学习点间关系，区分初级和次级电子。流程包括：1)将电子击中点表示为点云；2)构建k最近邻图；3)采用U-Net编码器-解码器结构；4)通过编码器提取高维特征，解码器映射回低维空间；5)融入变换器层使用注意力机制聚合信息；6)输出每个节点概率；7)根据概率和阈值计算dN/dx值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：提出GraphPT模型；将TPC数据表示为点云并利用GNN处理；设计U-Net架构用于点云；提出点积算子作为新注意力机制；将传统两步重建统一到单一模型；采用端到端训练。不同之处：之前工作主要处理一维波形，本文处理点云数据；之前多用规则方法或简单神经网络，本文使用先进图神经网络和变换器；点积算子比减法算子性能更好；在高粒度TPC上K/π分离能力提升显著。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出基于图点变换器的深度学习方法，通过将TPC数据表示为点云并利用图神经网络和注意力机制，显著提高了高粒度TPC中dN/dx重建的准确性，在K/π粒子识别能力上比传统方法提升了10%到20%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle identification (PID) is essential for future particle physicsexperiments such as the Circular Electron-Positron Collider and the FutureCircular Collider. A high-granularity Time Projection Chamber (TPC) not onlyprovides precise tracking but also enables dN/dx measurements for PID. ThedN/dx method estimates the number of primary ionization electrons, offeringsignificant improvements in PID performance. However, accurate reconstructionremains a major challenge for this approach. In this paper, we introduce a deeplearning model, the Graph Point Transformer (GraphPT), for dN/dxreconstruction. In our approach, TPC data are represented as point clouds. Thenetwork backbone adopts a U-Net architecture built upon graph neural networks,incorporating an attention mechanism for node aggregation specificallyoptimized for point cloud processing. The proposed GraphPT model surpasses thetraditional truncated mean method in PID performance. In particular, the$K/\pi$ separation power improves by approximately 10% to 20% in the momentuminterval from 5 to 20 GeV/c.</description>
      <author>example@mail.com (Guang Zhao, Yue Chang, Jinxian Zhang, Linghui Wu, Huirong Qi, Xin She, Mingyi Dong, Shengsen Sun, Jianchun Wang, Yifang Wang, Chunxu Yu)</author>
      <guid isPermaLink="false">2510.10628v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Learning with Graph-Tuple</title>
      <link>http://arxiv.org/abs/2510.10341v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TAG workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多视图图元组框架，用于解决图神经网络在密集图上的效率问题，通过将图划分为不相交的子图来捕捉多尺度交互信息，并在分子性质预测和宇宙学参数推断两个应用中展示了优越性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通常随图边数增加而扩展，适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见的稀疏化方法通过相似性阈值或距离修剪强制选择单一交互尺度，丢弃其他尺度的重要信息。&lt;h4&gt;目的&lt;/h4&gt;克服单一交互尺度的限制，保留多尺度信息，提高图神经网络在密集图上的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;引入多视图图元组框架，将图划分为不相交的子图捕捉主要局部相互作用和远程连接；通过受非交换算子理论启发的异构消息传递架构学习多视图表示；证明该框架比单图消息传递模型更具表达力并保证更低风险。&lt;h4&gt;主要发现&lt;/h4&gt;在分子性质预测(从特征稀缺的库仑矩阵)和宇宙学参数推断(从几何点云)两个应用中，多视图图元组模型都表现出比单图基线更好的性能。&lt;h4&gt;结论&lt;/h4&gt;多视图方法在处理密集图数据时具有强大的功能和通用性，能够有效捕捉多尺度交互信息，提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通常随图边数的增加而扩展，使其适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见的解决方案是通过相似性阈值或距离修剪来稀疏化图，但这强制选择单一交互尺度并丢弃其他尺度的重要信息。为克服这一限制，我们引入了多视图图元组框架。与单一图不同，我们的图元组框架将图划分为不相交的子图，捕捉主要局部相互作用和较弱的远程连接。然后，我们通过受非交换算子理论启发的异构消息传递架构从图元组中学习多视图表示，我们正式证明这比单图消息传递模型更具表达力，并保证更低的风险。我们在两个科学领域实例化了我们的框架：从特征稀缺的库仑矩阵进行分子性质预测，以及从几何点云进行宇宙学参数推断。在这两种应用中，我们的多视图图元组模型都表现出比单图基线更好的性能，突显了我们多视图方法的强大功能和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图神经网络（GNNs）在处理密集图（如点云或分子相互作用）时的效率问题。传统方法通过对图进行稀疏化（如相似性阈值化）来提高效率，但这会强制选择单一交互尺度并丢弃其他尺度的重要信息。这个问题在科学和现实应用中很重要，因为许多数据自然表现为密集图结构，如分子中的原子相互作用或宇宙学中的暗物质分布，传统方法会丢失关键信息从而影响模型性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到GNNs在密集图上的计算效率问题和传统稀疏化方法的信息丢失问题。他们考虑了现有解决方案如不变量特征模型和多视图方法，发现这些方法要么依赖低秩假设，要么专为异构图设计。作者设计思路是构建多视图图表示，将单个图根据交互强度划分为强连接图和弱连接图，并受GtNN框架启发，在单层中整合多个消息传递操作。他们借鉴了异构图学习、多视图表示学习和多尺度GNNs的思想，但进行了创新改进以适用于同构图和连续边特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是多视图图元组框架，将单个图划分为多个子图（视图）捕捉不同尺度的交互信息，并通过异构消息传递架构同时学习这些视图。实现流程包括：1) 图元组表示：将图G分解为图元组(G1,...,Gk)，每个子图在同一节点集上但具有不相交边集；2) 异构消息传递：在单层中整合尺度内操作（每个图视图内）和尺度间操作（跨不同图视图），公式为H(l+1) = H(l) + Σ(ci·Hi) + Σ(cij·Hi→j + cji·Hj→i)；3) 两种具体实现：GINE-Gt用于一般图，EGNN-Gt用于几何数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多视图图元组框架，同时保留不同尺度的交互信息；2) 异构消息传递架构，建模不同视图间的算子顺序；3) 理论保证，证明框架更具表现力且保证更低风险。相比之前工作，不同之处在于：不丢弃任何尺度信息（vs 传统稀疏化）；不依赖低秩假设（vs 不变量特征模型）；将异构图学习扩展到同构图，基于物理交互强度构建多视图（vs 多视图方法）；在同一节点集上定义多个图，避免跨级别对齐（vs 多尺度GNNs）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出的多视图图元框架通过异构消息传递架构同时学习不同尺度的图交互，解决了图神经网络在密集图上的效率和表示能力之间的权衡问题，并在科学应用中展示了优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) typically scale with the number of graph edges,making them well suited for sparse graphs but less efficient on dense graphs,such as point clouds or molecular interactions. A common remedy is to sparsifythe graph via similarity thresholding or distance pruning, but this forces anarbitrary choice of a single interaction scale and discards crucial informationfrom other scales. To overcome this limitation, we introduce a multi-viewgraph-tuple framework. Instead of a single graph, our graph-tuple frameworkpartitions the graph into disjoint subgraphs, capturing primary localinteractions and weaker, long-range connections. We then learn multi-viewrepresentations from the graph-tuple via a heterogeneous message-passingarchitecture inspired by the theory of non-commuting operators, which weformally prove is strictly more expressive and guarantees a lower oracle riskcompared to single-graph message-passing models. We instantiate our frameworkon two scientific domains: molecular property prediction from feature-scarceCoulomb matrices and cosmological parameter inference from geometric pointclouds. On both applications, our multi-view graph-tuple models demonstratebetter performance than single-graph baselines, highlighting the power andversatility of our multi-view approach.</description>
      <author>example@mail.com (Shiyu Chen, Ningyuan Huang, Soledad Villar)</author>
      <guid isPermaLink="false">2510.10341v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective</title>
      <link>http://arxiv.org/abs/2510.12763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Signal Processing Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了脑年龄差距作为神经退行性疾病生物标志物的应用，提出基于图信号处理和图神经网络的方法，特别是协方差神经网络(VNN)，以改进脑年龄差距预测模型的可靠性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;神经退行性疾病通常通过结构MRI显示的皮层厚度或脑体积减少来评估，但传统方法无法完全捕捉神经退行性病变在空间上的相关性和异质性。脑年龄差距作为一种数据驱动生物标志物虽有潜力，但其实际应用受限于方法学不明确和泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;提供BAGP的概述，并基于图信号处理的最新进展引入一个有原则的应用框架，特别是开发协方差神经网络(VNN)以实现稳健的脑年龄差距预测。&lt;h4&gt;方法&lt;/h4&gt;采用图信号处理、机器学习和网络神经学的综合视角，特别关注图神经网络(GNN)和协方差神经网络(VNN)，后者利用结构MRI推导的解剖协方差矩阵来提供理论基础和操作可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;脑年龄差距是脑健康的紧凑生物标志物，对疾病进展和严重程度具有预测效用。基于图神经网络和协方差神经网络的方法能够提供强大的理论支持和操作可解释性，实现可靠的脑年龄差距预测。&lt;h4&gt;结论&lt;/h4&gt;通过整合多学科视角，阐明了可靠和可解释的BAGP模型的发展路径，并指出了个性化医学的未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;神经退行性疾病以神经元结构或功能的进行性丧失为特征，临床上通常通过结构MRI显示的皮层厚度或脑体积减少来评估。虽然这些方法提供了信息，但传统方法缺乏足够的统计复杂性来完全捕捉神经退行性病变在空间上的相关性和异质性。为解决这些限制，脑年龄差距已成为一种有前途的脑健康数据驱动生物标志物。脑年龄差距预测模型估计从神经影像数据预测的脑年龄与实际年龄之间的差异。由此产生的脑年龄差距作为脑健康的紧凑生物标志物，最近的研究表明它对疾病进展和严重程度具有预测效用。然而，BAGP模型在实际应用中受到其方法学不明确和泛化能力有限的阻碍。本教程文章概述了BAGP，并基于图信号处理的最新进展，为这一应用引入了一个有原则的框架。特别是，我们关注图神经网络，并引入了协方差神经网络，它利用结构MRI推导的解剖协方差矩阵。VNN提供了坚实的理论基础和操作可解释性，能够实现可靠的脑年龄差距预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurodegeneration, characterized by the progressive loss of neuronalstructure or function, is commonly assessed in clinical practice throughreductions in cortical thickness or brain volume, as visualized by structuralMRI. While informative, these conventional approaches lack the statisticalsophistication required to fully capture the spatially correlated andheterogeneous nature of neurodegeneration, which manifests both in healthyaging and in neurological disorders. To address these limitations, brain agegap has emerged as a promising data-driven biomarker of brain health. The brainage gap prediction (BAGP) models estimate the difference between a person'spredicted brain age from neuroimaging data and their chronological age. Theresulting brain age gap serves as a compact biomarker of brain health, withrecent studies demonstrating its predictive utility for disease progression andseverity. However, practical adoption of BAGP models is hindered by theirmethodological obscurities and limited generalizability across diverse clinicalpopulations. This tutorial article provides an overview of BAGP and introducesa principled framework for this application based on recent advancements ingraph signal processing (GSP). In particular, we focus on graph neural networks(GNNs) and introduce the coVariance neural network (VNN), which leverages theanatomical covariance matrices derived from structural MRI. VNNs offer strongtheoretical grounding and operational interpretability, enabling robustestimation of brain age gap predictions. By integrating perspectives from GSP,machine learning, and network neuroscience, this work clarifies the pathforward for reliable and interpretable BAGP models and outlines future researchdirections in personalized medicine.</description>
      <author>example@mail.com (Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2510.12763v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2510.12703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE Consumer Communications &amp; Networking Conference  (CCNC) 2026 - Las Vegas, NV, USA 9 - 12 January 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶面临安全挑战，车辆传感器存在视野受限问题，车辆间通信特别是协作感知消息(CAM)可有效解决这一问题，本文提出的CAMNet模型证明了CAM数据在车辆轨迹预测中的有效性&lt;h4&gt;背景&lt;/h4&gt;自动驾驶任务具有挑战性，现代车辆虽配备LiDAR、摄像头和雷达等昂贵传感器，但存在视野和视线可能被其他车辆遮挡的固有局限性，从而降低态势感知能力&lt;h4&gt;目的&lt;/h4&gt;研究使用协作感知消息(CAM)数据进行车辆轨迹预测，评估CAM数据是否可以被有效利用&lt;h4&gt;方法&lt;/h4&gt;设计并训练名为CAMNet的神经网络模型，在广泛使用的运动预测数据集上进行训练，并在使用CAM数据从头创建的第二个数据集上进行评估&lt;h4&gt;主要发现&lt;/h4&gt;CAM数据确实可以支持车辆轨迹预测，CAMNet模型显示出有希望的结果&lt;h4&gt;结论&lt;/h4&gt;该方法存在一些局限性，这些局限性为未来研究提供了方向&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶仍然是一项具有挑战性的任务，主要由于安全问题。现代车辆通常配备昂贵的传感器，如LiDAR、摄像头和雷达，以降低事故风险。然而，这些传感器存在固有局限性：它们的视野和视线可能被其他车辆遮挡，从而降低态势感知能力。在此背景下，车辆间通信起着关键作用，因为它使车辆能够共享信息，即使在传感器被遮挡的情况下也能保持彼此的感知。实现这一点的一种方式是通过使用协作感知消息(CAM)。在本文中，我们研究使用CAM数据进行车辆轨迹预测。具体来说，我们在广泛使用的运动预测数据集上设计和训练了一个神经网络——基于协作感知消息的图神经网络(CAMNet)。然后，我们在使用协作感知消息从头创建的第二个数据集上评估该模型，以评估这种类型的数据是否可以被有效利用。我们的方法显示出有希望的结果，表明CAM确实可以支持车辆轨迹预测。同时，我们讨论了该方法的几种局限性，这些局限性突出了未来研究的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving remains a challenging task, particularly due to safetyconcerns. Modern vehicles are typically equipped with expensive sensors such asLiDAR, cameras, and radars to reduce the risk of accidents. However, thesesensors face inherent limitations: their field of view and line of sight can beobstructed by other vehicles, thereby reducing situational awareness. In thiscontext, vehicle-to-vehicle communication plays a crucial role, as it enablescars to share information and remain aware of each other even when sensors areoccluded. One way to achieve this is through the use of Cooperative AwarenessMessages (CAMs). In this paper, we investigate the use of CAM data for vehicletrajectory prediction. Specifically, we design and train a neural network,Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widelyused motion forecasting dataset. We then evaluate the model on a second datasetthat we created from scratch using Cooperative Awareness Messages, in order toassess whether this type of data can be effectively exploited. Our approachdemonstrates promising results, showing that CAMs can indeed support vehicletrajectory prediction. At the same time, we discuss several limitations of theapproach, which highlight opportunities for future research.</description>
      <author>example@mail.com (Mattia Grasselli, Angelo Porrello, Carlo Augusto Grazia)</author>
      <guid isPermaLink="false">2510.12703v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>PromoGuardian: Detecting Promotion Abuse Fraud with Multi-Relation Fused Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.12652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The final version of this paper is going to appear in IEEE Symposium  on Security and Privacy 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对电子商务平台中的促销滥用欺诈问题，提出了PROMOGUARDIAN模型，一种多关系融合图神经网络，通过整合交易数据的空间和时间信息来检测欺诈。实验表明该模型能有效识别促销滥用欺诈行为。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务平台的发展，欺诈活动日益增多，对平台的安全性和稳定性构成威胁。促销滥用是近年来增长最快的欺诈类型之一，用户通过利用促销活动从平台获取经济利益。&lt;h4&gt;目的&lt;/h4&gt;研究电子商务平台美团中的促销滥用欺诈问题，并提出有效的检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出PROMOGUARDIAN，一种新颖的多关系融合图神经网络，将交易数据的空间和时间信息整合到同质图中，以检测促销滥用欺诈。&lt;h4&gt;主要发现&lt;/h4&gt;促销滥用欺诈是基于群体的欺诈活动，包含囤积和返现滥用两种类型。与传统欺诈不同，它通常涉及普通客户进行合法交易，且两种欺诈活动常常相互交织。&lt;h4&gt;结论&lt;/h4&gt;在美团真实数据上的实验表明，该模型达到93.15%的精确度，能检测到2.1至5.0倍更多的欺诈者，在生产环境中可防止1.5至8.8倍更多的经济损失，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务平台的发展，欺诈活动日益增多，对平台的安全性和稳定性构成重大威胁。促销滥用是近年来增长最快的欺诈类型之一，其特点是用户利用促销活动从平台获取经济利益。为研究此问题，我们对电子商务平台美团进行了首次促销滥用欺诈研究。我们发现促销滥用欺诈是基于群体的欺诈活动，包含囤积和返现滥用两种类型。与虚假评论等传统欺诈活动不同，促销滥用欺诈通常涉及普通客户进行合法交易，且这两种欺诈活动常常相互交织。为解决此问题，我们提议利用空间和时间角度的额外信息来检测促销滥用欺诈。在本文中，我们介绍了PROMOGUARDIAN，一种新颖的多关系融合图神经网络，将交易数据的空间和时间信息整合到同质图中以检测促销滥用欺诈。我们在美团的现实数据上进行了广泛实验，结果表明我们提出的模型在促销滥用欺诈检测方面优于最先进的方法，达到93.15%的精确度，能检测到2.1至5.0倍更多的欺诈者，并在生产环境中防止1.5至8.8倍更多的经济损失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As e-commerce platforms develop, fraudulent activities are increasinglyemerging, posing significant threats to the security and stability of theseplatforms. Promotion abuse is one of the fastest-growing types of fraud inrecent years and is characterized by users exploiting promotional activities togain financial benefits from the platform. To investigate this issue, weconduct the first study on promotion abuse fraud in e-commerce platformsMEITUAN. We find that promotion abuse fraud is a group-based fraudulentactivity with two types of fraudulent activities: Stocking Up and CashbackAbuse. Unlike traditional fraudulent activities such as fake reviews, promotionabuse fraud typically involves ordinary customers conducting legitimatetransactions and these two types of fraudulent activities are oftenintertwined. To address this issue, we propose leveraging additionalinformation from the spatial and temporal perspectives to detect promotionabuse fraud. In this paper, we introduce PROMOGUARDIAN, a novel multi-relationfused graph neural network that integrates the spatial and temporal informationof transaction data into a homogeneous graph to detect promotion abuse fraud.We conduct extensive experiments on real-world data from MEITUAN, and theresults demonstrate that our proposed model outperforms state-of-the-artmethods in promotion abuse fraud detection, achieving 93.15% precision,detecting 2.1 to 5.0 times more fraudsters, and preventing 1.5 to 8.8 timesmore financial losses in production environments.</description>
      <author>example@mail.com (Shaofei Li, Xiao Han, Ziqi Zhang, Minyao Hua, Shuli Gao, Zhenkai Liang, Yao Guo, Xiangqun Chen, Ding Li)</author>
      <guid isPermaLink="false">2510.12652v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs</title>
      <link>http://arxiv.org/abs/2510.12401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在大规模异构图上预训练图神经网络的有效框架，解决了现有方法仅适用于同构图且未考虑语义不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络促进了图数据挖掘发展，但训练需要大量昂贵且有时不可用的有标签数据。现有自监督预训练方法主要针对同构图设计，而现实世界多为异构图，且未考虑语义不匹配问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个在大规模异构图上预训练GNNs的有效框架，解决语义不匹配问题并提高模型的可转移性。&lt;h4&gt;方法&lt;/h4&gt;设计了结构感知的预训练任务捕获异构图结构特性，以及语义感知的预训练任务解决语义不匹配。通过构建由语义邻居组成的扰动子空间，使模型更关注语义空间中的通用知识，学习具有更好可转移性的知识。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界大规模异构图上的大量实验表明，所提出的方法优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架能有效在大规模异构图上预训练GNNs，解决了现有方法在同构图和语义不匹配方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络促进了图数据挖掘的发展。然而，训练GNNs需要足够的有标签任务特定数据，这些数据昂贵且有时不可用。为减少对有标签数据的依赖，最近研究提出通过自监督方式预训练GNNs，然后在有有限标签数据的下游任务中应用预训练的GNNs。然而，大多数现有方法仅针对同构图设计（现实世界中的图大多是异构图），且未考虑语义不匹配问题（原始数据与包含更多可转移语义信息的理想数据之间的语义差异）。本文提出了一种在大规模异构图上预训练GNNs的有效框架。我们首先设计了一个结构感知的预训练任务，旨在捕获异构图中的结构特性。然后，设计了一个语义感知的预训练任务来解决不匹配问题。具体而言，我们构建了一个由语义邻居组成的扰动子空间，帮助处理语义不匹配。语义邻居使模型更专注于语义空间中的通用知识，进而帮助模型学习具有更好可转移性的知识。最后，在真实世界大规模异构图上进行了大量实验，证明了所提出方法优于最先进的基线方法。代码可在https://github.com/sunshy-1/PHE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have facilitated thedevelopment of graph data mining. However, training GNNs requires sufficientlabeled task-specific data, which is expensive and sometimes unavailable. To beless dependent on labeled data, recent studies propose to pre-train GNNs in aself-supervised manner and then apply the pre-trained GNNs to downstream taskswith limited labeled data. However, most existing methods are designed solelyfor homogeneous graphs (real-world graphs are mostly heterogeneous) and do notconsider semantic mismatch (the semantic difference between the original dataand the ideal data containing more transferable semantic information). In thispaper, we propose an effective framework to pre-train GNNs on the large-scaleheterogeneous graph. We first design a structure-aware pre-training task, whichaims to capture structural properties in heterogeneous graphs. Then, we designa semantic-aware pre-training task to tackle the mismatch. Specifically, weconstruct a perturbation subspace composed of semantic neighbors to help dealwith the semantic mismatch. Semantic neighbors make the model focus more on thegeneral knowledge in the semantic space, which in turn assists the model inlearning knowledge with better transferability. Finally, extensive experimentsare conducted on real-world large-scale heterogeneous graphs to demonstrate thesuperiority of the proposed method over state-of-the-art baselines. Codeavailable at https://github.com/sunshy-1/PHE.</description>
      <author>example@mail.com (Shengyin Sun, Chen Ma, Jiehao Chen)</author>
      <guid isPermaLink="false">2510.12401v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理信息的图神经网络与极值分析技术相结合的方法，用于提高泰国地区的降雨预测准确性，特别是对极端事件的预测。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预测，特别是极端事件的预测，在气候学和地球系统中仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，结合物理信息的图神经网络与极值分析技术，以提高泰国各气象站的降雨预测能力。&lt;h4&gt;方法&lt;/h4&gt;使用图结构表示气象站捕捉时空模式，预处理相关气候指标，提出Attention-LSTM模型，使用基于地形降水物理公式的边特征，并通过空间季节感知GPD方法进行POT映射处理极端值。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在大多数地区（包括易发生极端事件的地区）优于成熟的基线方法，并与最先进方法保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;与业务预测系统SEAS5相比，该方法改进了极端事件的预测，并为生产支持长期水资源管理的精细分辨率地图提供了实际改进。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预测，特别是对于极端事件，在气候学和地球系统中仍然是一个重大挑战。本文提出了一种新颖的物理信息图神经网络(GNNs)结合极值分析技术，以提高泰国各气象站的降雨预测。该模型利用气象站的图结构表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理了可能影响区域降雨的相关气候指标。所提出的图注意力网络与长短期记忆网络(Attention-LSTM)应用了注意力机制，使用基于简单地形降水物理公式推导的初始边特征。嵌入随后由LSTM层处理。为解决极值问题，我们使用新颖的空间季节感知广义帕累托分布(GPD)方法进行阈值超限(POT)映射，克服了传统机器学习模型的局限性。实验表明，我们的方法在大多数地区（包括易发生极端事件的地区）优于成熟的基线方法，并与最先进方法保持强劲竞争力。与业务预测系统SEAS5相比，我们的实际应用改进了极端事件的预测，并为生产支持长期水资源管理的精细分辨率地图提供了实际增强。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producefine-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Using STAR-IRS to Secure Indoor Communications Through Symbol-Level Random Phase Modulation</title>
      <link>http://arxiv.org/abs/2510.11925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于同时传输与反射智能反射面(STAR-IRS)的安全室内通信方案，通过动态分割电磁波并控制反射和传输信号来增强安全通信性能。&lt;h4&gt;背景&lt;/h4&gt;在室内通信环境中，发射方(Alice)需要向室内用户(Bob)发送机密信息，同时存在室外窃听者(Eves)的威胁，传统通信方案难以有效保障通信安全。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够保护传输免受窃听的通信方案，最大化安全速率，并通过硬件加速降低计算延迟。&lt;h4&gt;方法&lt;/h4&gt;部署STAR-IRS在墙壁或窗户上，将入射电磁波动态分割为透射和反射两个分量；控制反射信号增强Bob的接收质量，对透射信号进行符号级随机相位调制降低Eves的信号质量；提出基于图神经网络(GNN)的方案解决安全速率最大化问题；设计基于FPGA的GNN加速器减少计算延迟。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略在安全性方面优于传统方案和仅反射方案；GNN方法在解决优化问题时比MRT、ZF和MMSE等基准技术取得更优结果；基于FPGA的加速器实现了低推理延迟。&lt;h4&gt;结论&lt;/h4&gt;STAR-IRS结合GNN和FPGA加速器可以有效提高室内通信的安全性，为安全通信提供了一种高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于同时传输与反射智能反射面(STAR-IRS)的安全室内通信方案。具体而言，发射方(Alice)向室内目标用户(Bob)发送机密信息，同时有几个窃听者(Eves)潜伏在外部。为了保护传输免受窃听，在墙壁或窗户上部署了STAR-IRS。当电磁波撞击到STAR-IRS时，入射电磁波被动态分割为两个分量，实现通过表面的传输和表面的反射。反射信号被控制以增强Bob的接收，而透射信号则用符号级随机相移进行调制，以降低Eves的信号质量。基于这种设置，构建了安全速率最大化问题。为解决这一问题，开发了基于图神经网络(GNN)的方案。此外，还设计了一个基于FPGA的GNN加速器以减少计算延迟。仿真结果表明，所提出的策略在安全性方面优于传统方案和仅反射方案。此外，在解决优化问题时，GNN方法比最大比传输(MRT)、迫零(ZF)和最小均方误差(MMSE)等基准技术取得更好的结果。最后，实验评估确认基于FPGA的加速器实现了低推理延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a secure indoor communication scheme based onsimultaneous transmitting and reflecting intelligent reflecting surface(STAR-IRS). Specifically, a transmitter (Alice) sends confidential informationto its intended user (Bob) indoors, while several eavesdroppers (Eves) lurkoutside. To safeguard the transmission from eavesdropping, the STAR-IRS isdeployed on walls or windows. Upon impinging on the STAR-IRS, the incomingelectromagnetic wave is dynamically partitioned into two components, enablingboth transmission through and reflection from the surface. The reflected signalis controlled to enhance reception at Bob, while the transmitted signal ismodulated with symbol-level random phase shifts to degrade the signal qualityat Eves. Based on such a setting, the secrecy rate maximization problem isformulated. To solve it, a graph neural network (GNN)-based scheme isdeveloped. Furthermore, a field-programmable gate array (FPGA)-based GNNaccelerator is designed to reduce computational latency. Simulation resultsdemonstrate that the proposed strategy outperforms both the conventional schemeand the reflection-only scheme in terms of secrecy performance. Moreover, theGNN-based approach achieves superior results compared to benchmark techniquessuch as maximum ratio transmission (MRT), zero forcing (ZF), and minimum meansquare error (MMSE) in solving the optimization problem. Finally, experimentalevaluations confirm that the FPGA-based accelerator enables low inferencelatency.</description>
      <author>example@mail.com (Yanan Du, Zeyang Sun, Yilan Zhang, Sai Xu, Beiyuan Liu)</author>
      <guid isPermaLink="false">2510.11925v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
      <link>http://arxiv.org/abs/2406.11569v5</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 10 figures, submitted for possible journal publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了基于元学习的个性化联邦学习(meta-pFL)在无线环境中的泛化性能，分析了泛化到新代理和任务与收敛性之间的权衡关系。&lt;h4&gt;背景&lt;/h4&gt;现代AI应用如大型语言模型的训练范式已转变为预训练后微调；由于开放数据减少和AI模型访问民主化，预训练正从集中式部署转向联邦学习实现。&lt;h4&gt;目的&lt;/h4&gt;研究meta-pFL在无线环境中的泛化性能，探索对新代理和任务的泛化与收敛性之间的权衡。&lt;h4&gt;方法&lt;/h4&gt;采用空中计算技术，研究通过共享无线信道连接到服务器的无线环境中参与预训练阶段的代理的情况。&lt;h4&gt;主要发现&lt;/h4&gt;信道损伤可能会增强泛化能力同时降低收敛性，存在泛化与收敛性之间的权衡关系。&lt;h4&gt;结论&lt;/h4&gt;大量的数值结果验证了所提出的理论。&lt;h4&gt;翻译&lt;/h4&gt;对于现代人工智能应用（如大型语言模型），训练范式已转变为预训练后微调。此外，由于开放数据存储库减少以及AI模型访问民主化的努力，预训练预计将从当前集中式部署转向联邦学习实现。元学习提供了一个可以将预训练和微调形式化的通用框架。基于元学习的个性化联邦学习(meta-pFL)通过针对新代理和任务的泛化能力，超越了基本个性化。本文研究了meta-pFL在无线环境中的泛化性能，其中参与预训练阶段的代理通过共享无线信道连接到服务器。采用空中计算，我们研究了泛化到新代理和任务与收敛性之间的权衡。这种权衡源于信道损伤可能增强泛化同时降低收敛性的事实。大量的数值结果验证了该理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For modern artificial intelligence (AI) applications such as large languagemodels (LLMs), the training paradigm has recently shifted to pre-trainingfollowed by fine-tuning. Furthermore, owing to dwindling open repositories ofdata and thanks to efforts to democratize access to AI models, pre-training isexpected to increasingly migrate from the current centralized deployments tofederated learning (FL) implementations. Meta-learning provides a generalframework in which pre-training and fine-tuning can be formalized.Meta-learning-based personalized FL (meta-pFL) moves beyond basicpersonalization by targeting generalization to new agents and tasks. This paperstudies the generalization performance of meta-pFL for a wireless setting inwhich the agents participating in the pre-training phase, i.e., meta-learning,are connected via a shared wireless channel to the server. Adoptingover-the-air computing, we study the trade-off between generalization to newagents and tasks, on the one hand, and convergence, on the other hand. Thetrade-off arises from the fact that channel impairments may enhancegeneralization, while degrading convergence. Extensive numerical resultsvalidate the theory.</description>
      <author>example@mail.com (Haifeng Wen, Hong Xing, Osvaldo Simeone)</author>
      <guid isPermaLink="false">2406.11569v5</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
  <item>
      <title>High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.11017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新框架，通过扩展Mamba模型来分别学习全局和局部高分辨率时空表示，用于视频人体姿态估计(VHPE)。该框架包含全局时空Mamba和局部细化Mamba两个组件，有效解决了现有方法在平衡全局和局部动态建模方面的困难。&lt;h4&gt;背景&lt;/h4&gt;高分辨率时空表示建模对于视频人体姿态估计至关重要，需要同时考虑全局动态上下文和局部运动细节。当前最先进方法在单一建模结构中统一时空学习，难以平衡全局和局部动态建模，且在捕获全局依赖时具有二次复杂度，限制了在高分辨率序列中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，从两个方面扩展Mamba模型，分别学习VHPE的全局和局部高分辨率时空表示，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出全局时空Mamba，执行6D选择性时空扫描和时空调制扫描合并，从高分辨率序列中高效提取全局表示；引入基于窗口时空扫描的局部细化Mamba，增强局部关键点运动的高频细节。这种方法结合了Mamba的线性复杂度和处理长程上下文的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上的大量实验表明，所提出的模型优于最先进的VHPE方法，同时实现了更好的计算权衡。&lt;h4&gt;结论&lt;/h4&gt;通过分别建模全局和局部时空表示，可以提高VHPE性能。扩展Mamba框架可以有效地处理高分辨率视频数据，解决了现有方法在计算效率和表示能力之间的平衡问题。&lt;h4&gt;翻译&lt;/h4&gt;建模高分辨率时空表示，包括全局动态上下文(如整体人体运动趋势)和局部运动细节(如关键点的高频变化)，对于基于视频的人体姿态估计(VHPE)至关重要。当前最先进方法通常在单一类型的建模结构(卷积或基于注意力的块)中统一时空学习，这些方法本质上难以平衡全局和局部动态建模，可能导致网络偏向其中一种，从而产生次优性能。此外，现有VHPE模型在捕获全局依赖时具有二次复杂度，限制了它们在高分辨率序列中的应用。最近，状态空间模型(称为Mamba)在建模具有线性复杂度的长程上下文方面显示出巨大潜力；然而，它们仅限于1D序列数据。在本文中，我们提出了一种新框架，从两个方面扩展Mamba，分别学习VHPE的全局和局部高分辨率时空表示。具体而言，我们首先提出了全局时空Mamba，它执行6D选择性时空扫描和时空调制扫描合并，从高分辨率序列中高效提取全局表示。我们进一步引入了基于窗口时空扫描的局部细化Mamba，以增强局部关键点运动的高频细节。在四个基准数据集上的大量实验表明，所提出的模型优于最先进的VHPE方法，同时实现了更好的计算权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling high-resolution spatiotemporal representations, including bothglobal dynamic contexts (e.g., holistic human motion tendencies) and localmotion details (e.g., high-frequency changes of keypoints), is essential forvideo-based human pose estimation (VHPE). Current state-of-the-art methodstypically unify spatiotemporal learning within a single type of modelingstructure (convolution or attention-based blocks), which inherently havedifficulties in balancing global and local dynamic modeling and may bias thenetwork to one of them, leading to suboptimal performance. Moreover, existingVHPE models suffer from quadratic complexity when capturing globaldependencies, limiting their applicability especially for high-resolutionsequences. Recently, the state space models (known as Mamba) have demonstratedsignificant potential in modeling long-range contexts with linear complexity;however, they are restricted to 1D sequential data. In this paper, we present anovel framework that extends Mamba from two aspects to separately learn globaland local high-resolution spatiotemporal representations for VHPE.Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6Dselective space-time scan and spatial- and temporal-modulated scan merging toefficiently extract global representations from high-resolution sequences. Wefurther introduce a windowed space-time scan-based Local Refinement Mamba toenhance the high-frequency details of localized keypoint motions. Extensiveexperiments on four benchmark datasets demonstrate that the proposed modeloutperforms state-of-the-art VHPE approaches while achieving bettercomputational trade-offs.</description>
      <author>example@mail.com (Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao)</author>
      <guid isPermaLink="false">2510.11017v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2510.10365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PointMAC是一种创新的元学习框架，用于点云补全的测试时适应，通过自监督辅助目标和元辅助学习策略，解决了现有模型无法适应新结构模式和传感器失真的问题，实现了高质量的个体样本补全。&lt;h4&gt;背景&lt;/h4&gt;点云补全对机器人和增强现实等安全关键应用中的鲁棒3D感知至关重要。现有模型执行静态推理，严重依赖训练期间归纳偏置，限制了它们适应新结构模式和传感器引起失真的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型无法适应测试时新结构模式和传感器失真的问题，提出PointMAC框架实现测试时鲁棒适应。&lt;h4&gt;方法&lt;/h4&gt;提出PointMAC框架，通过两个自监督辅助目标模拟结构和传感器级别的不完整性；基于模型无关元学习的元辅助学习策略确保适应与主要任务一致；推理时通过优化辅助损失实时适应共享编码器；引入自适应λ校准机制平衡主要和辅助目标间的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在合成、模拟和真实世界数据集上的广泛实验表明，PointMAC通过单独细化每个样本来产生高质量补全，实现了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是首次将元辅助测试时适应应用于点云补全的工作。&lt;h4&gt;翻译&lt;/h4&gt;点云补全对于机器人和增强现实等安全关键应用中的鲁棒3D感知至关重要。然而，现有模型执行静态推理，并严重依赖训练期间学习的归纳偏置，限制了它们在测试时适应新结构模式和传感器引起失真的能力。为了解决这一限制，我们提出了PointMAC，一种用于点云补全的元学习框架，实现测试时的鲁棒适应。它无需额外监督即可实现样本特定细化。我们的方法在两个自监督辅助目标下优化补全模型，这些目标模拟结构和传感器级别的不完整性。基于模型无关元学习的元辅助学习策略确保由辅助目标驱动的适应与主要补全任务保持一致。在推理过程中，我们通过优化辅助损失实时适应共享编码器，同时保持解码器固定。为了进一步稳定适应，我们引入了自适应λ校准，一种用于平衡主要和辅助目标之间梯度的元学习机制。在合成、模拟和真实世界数据集上的广泛实验表明，PointMAC通过单独细化每个样本来产生高质量补全，实现了最先进的结果。据我们所知，这是首次将元辅助测试时适应应用于点云补全的工作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决点云补全中的静态推理问题，即现有模型在测试时使用固定推理方式，难以适应新的结构模式和传感器引起的失真。这个问题在现实中很重要，因为点云补全对机器人、自动驾驶和增强现实等安全关键应用至关重要，而现实世界中的点云常因遮挡、有限覆盖和传感器噪声而不完整，现有模型在处理这些情况时表现不佳，生成'通用补全'而非针对特定样本的'样本特定补全'。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从静态推理转向动态、样本特定适应的思路，利用测试时适应框架让模型自我调整。他们设计PointMAC框架，包含Bi-Aux Units执行自监督任务，并采用MAML规范适应过程。该方法借鉴了TTA在动态场景去模糊等领域的应用，元学习在少样本学习中的成功经验，以及点云补全领域的编码器-解码器架构和transformer模型设计思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过元辅助学习实现测试时样本特定适应，使模型能根据每个输入的独特几何形状和噪声动态调整内部表示。流程包括：1)网络架构(共享编码器、主要解码器和Bi-Aux Units)；2)训练流程(内部辅助适应、外部主要对齐和自适应λ校准)；3)推理流程(对每个测试样本执行自监督梯度步骤，细化共享编码器，生成样本特定补全)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个将元辅助学习和测试时适应应用于点云补全的框架；2)Bi-Aux Units设计自监督双辅助任务；3)基于MAML的元辅助学习策略；4)自适应λ校准机制。相比之前工作，PointMAC实现了从静态到动态的转变，采用自监督适应而非额外监督，解决了辅助任务与主要任务对齐问题，提高了泛化能力，并提供了端到端的框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMAC通过元辅助学习的测试时适应框架，使点云补全模型能动态调整每个输入样本的内部表示，在不依赖额外监督的情况下生成高质量、样本特定的补全结果，显著提高了模型对未见过的结构模式和传感器噪声的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is essential for robust 3D perception insafety-critical applications such as robotics and augmented reality. However,existing models perform static inference and rely heavily on inductive biaseslearned during training, limiting their ability to adapt to novel structuralpatterns and sensor-induced distortions at test time. To address thislimitation, we propose PointMAC, a meta-learned framework for robust test-timeadaptation in point cloud completion. It enables sample-specific refinementwithout requiring additional supervision. Our method optimizes the completionmodel under two self-supervised auxiliary objectives that simulate structuraland sensor-level incompleteness. A meta-auxiliary learning strategy based onModel-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliaryobjectives is consistently aligned with the primary completion task. Duringinference, we adapt the shared encoder on-the-fly by optimizing auxiliarylosses, with the decoder kept fixed. To further stabilize adaptation, weintroduce Adaptive $\lambda$-Calibration, a meta-learned mechanism forbalancing gradients between primary and auxiliary objectives. Extensiveexperiments on synthetic, simulated, and real-world datasets demonstrate thatPointMAC achieves state-of-the-art results by refining each sample individuallyto produce high-quality completions. To the best of our knowledge, this is thefirst work to apply meta-auxiliary test-time adaptation to point cloudcompletion.</description>
      <author>example@mail.com (Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, Yang Wang)</author>
      <guid isPermaLink="false">2510.10365v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View</title>
      <link>http://arxiv.org/abs/2510.11687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的、类别不可知的框架，可以从单个RGB-D图像同时预测物体的6D姿态、大小和密集形状，无需模板、CAD模型或类别标签，在多个基准数据集上实现了最先进性能，并展现出强大的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;从视觉输入估计物体的6D姿态、大小和形状是计算机视觉中的基础问题，在机器人抓取和操作中有关键应用。现有方法要么依赖特定于对象的先验，要么由于姿态-形状纠缠和多阶段管道而泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需对象特定先验的统一框架，能够从单个RGB-D图像同时预测6D姿态、大小和密集形状，并实现跨类别的强泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用Transformer编码器（由Mixture-of-Experts增强）融合来自视觉基础模型的密集2D特征和部分3D点云，采用并行解码器进行姿态-大小估计和形状重建，实现28 FPS的实时推理。仅在SOPE数据集的149个类别的合成数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同基准数据集（涵盖300多个类别）上评估，在已见类别上达到最先进精度，同时对未见到的真实世界物体表现出强大的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架为机器人和具身AI中的开放集6D理解建立了新标准，无需对象特定先验即可实现高精度和强泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;从视觉输入估计物体的6D姿态、大小和形状是计算机视觉中的一个基础问题，在机器人抓取和操作中具有关键应用。现有方法要么依赖于特定于对象的先验（如CAD模型或模板），要么由于姿态-形状纠缠和多阶段管道而在跨类别泛化方面受到限制。在这项工作中，我们提出了一个统一的、类别不可知的框架，可以从单个RGB-D图像同时预测6D姿态、大小和密集形状，测试时不需要模板、CAD模型或类别标签。我们的模型使用由Mixture-of-Experts增强的Transformer编码器融合来自视觉基础模型的密集2D特征和部分3D点云，并采用并行解码器进行姿态-大小估计和形状重建，实现28 FPS的实时推理。仅在SOPE数据集的149个类别的合成数据上进行训练后，我们的框架在四个不同的基准数据集SOPE、ROPE、ObjaversePose和HANDAL上进行了评估，涵盖300多个类别。它在已见类别上实现了最先进的精度，同时展现出对未见真实世界物体 remarkably强的零样本泛化能力，为机器人和具身AI中的开放集6D理解建立了新标准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单张RGB-D图像中估计物体6D位姿、大小和形状的问题，而不需要依赖特定模板、CAD模型或类别标签。这个问题在机器人抓取、操作以及具身AI领域至关重要，因为现有方法要么需要物体特定先验知识，要么在跨类别泛化能力上有限，限制了在开放场景下的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：实例级方法需要参考图像或CAD模型，类别级方法存在姿态-形状纠缠和多阶段流水线问题。他们设计了一个统一框架，融合视觉基础模型的密集2D特征与3D点云，使用增强Mixture-of-Experts的Transformer编码器，并采用并行解码器。该方法借鉴了DGCNN处理特征、Transformer架构、RADIOv2.5基础模型提取语义特征、DenseFusion的特征融合方式以及NOCS坐标系表示等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的、类别无关的框架，同时预测6D位姿、大小和形状，避免多阶段流水线，实现测试时无需模板或类别标签。整体流程：1)使用RADIOv2.5提取RGB图像的密集2D特征；2)将2D特征与3D点云坐标融合；3)通过DGCNN处理融合特征；4)使用带有Mixture-of-Experts的Transformer编码器生成全局表示；5)通过并行解码器进行姿态-大小直接回归和形状重建(采用粗到细策略)；6)结合多种损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个统一类别无关框架同时估计6D位姿、大小和形状；2)设计可扩展架构融合2D特征与3D点云，使用增强Mixture-of-Experts的Transformer；3)实现28 FPS实时推理；4)仅合成数据训练但展现强大零样本泛化；5)构建ObjaversePose数据集。不同之处：无需测试时模板/CAD模型/类别标签；统一端到端框架避免多阶段处理；同时处理位姿、大小和形状捕获相互依赖；使用MoE提高对不同形状分布建模能力；合成数据训练但能泛化到真实世界和未见类别。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种统一的、类别无关的框架，能够从单张RGB-D图像中实时估计物体的6D位姿、大小和完整形状，无需测试时的模板或类别标签，并在多种未见过的真实物体上展示了强大的零样本泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating an object's 6D pose, size, and shape from visual input is afundamental problem in computer vision, with critical applications in roboticgrasping and manipulation. Existing methods either rely on object-specificpriors such as CAD models or templates, or suffer from limited generalizationacross categories due to pose-shape entanglement and multi-stage pipelines. Inthis work, we propose a unified, category-agnostic framework thatsimultaneously predicts 6D pose, size, and dense shape from a single RGB-Dimage, without requiring templates, CAD models, or category labels at testtime. Our model fuses dense 2D features from vision foundation models withpartial 3D point clouds using a Transformer encoder enhanced by aMixture-of-Experts, and employs parallel decoders for pose-size estimation andshape reconstruction, achieving real-time inference at 28 FPS. Trained solelyon synthetic data from 149 categories in the SOPE dataset, our framework isevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,spanning over 300 categories. It achieves state-of-the-art accuracy on seencategories while demonstrating remarkably strong zero-shot generalization tounseen real-world objects, establishing a new standard for open-set 6Dunderstanding in robotics and embodied AI.</description>
      <author>example@mail.com (Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu)</author>
      <guid isPermaLink="false">2510.11687v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection</title>
      <link>http://arxiv.org/abs/2510.11632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NV3D的新型3D物体检测模型，利用从体素邻居获取的局部法向量特征来提高检测性能，并通过两种采样策略减少数据量同时保持性能。&lt;h4&gt;背景&lt;/h4&gt;近期自动驾驶车辆3D物体检测研究试图通过多模态设置或从LiDAR点云中提取局部模式来丰富特征，但多模态方法面临特征对齐挑战，局部特征获取对于复杂任务可能过于简化。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D物体检测模型NV3D，解决现有方法的局限性，提高检测性能并减少数据量。&lt;h4&gt;方法&lt;/h4&gt;NV3D利用基于K近邻和主成分分析计算每个体素的法向量作为局部特征，确定表面与目标实体间关系；提供两种采样策略：基于法向量密度的采样和基于视场感知的基于bin的采样；应用元素级注意力融合机制，将体素特征作为查询和值，法向量特征作为键。&lt;h4&gt;主要发现&lt;/h4&gt;使用两种采样策略可消除高达55%的数据同时保持性能；在KITTI数据集上，NV3D在汽车和骑行者检测方面优于基线；不使用采样时，NV3D比Voxel R-CNN分别高出2.61%和4.23% mAP；使用采样后，仍比基线高1.56% mAP，同时过滤了约55%体素。&lt;h4&gt;结论&lt;/h4&gt;NV3D模型通过利用法向量特征和有效采样策略，能够在减少数据量的同时提高3D物体检测性能，特别是在具有特定空间形状的物体检测上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近期关于自动驾驶车辆3D物体检测的研究试图通过多模态设置或从LiDAR点云中提取局部模式来丰富特征。然而，多模态方法面临特征对齐的重大挑战，而局部特征获取对于复杂的3D物体检测任务可能过于简化。在本文中，我们提出了一种新型模型NV3D，它利用从体素邻居获取的局部特征，作为使用K近邻和主成分分析按每个体素基础计算的法向量。这种信息丰富的特征使NV3D能够确定表面与相关目标实体之间的关系，包括汽车、行人或骑行者。在法向量提取过程中，NV3D提供两种不同的采样策略：基于法向量密度的采样和基于视场感知的基于bin的采样，允许消除高达55%的数据同时保持性能。此外，我们应用了元素级注意力融合，将体素特征作为查询和值，法向量特征作为键，类似于注意力机制。我们的方法在KITTI数据集上训练，并在汽车和骑行者检测方面表现出优越性能，这得益于它们的空间形状。在验证集上，不使用采样的NV3D达到86.60%和80.18%的平均精度均值(mAP)，分别比基线Voxel R-CNN高出2.61%和4.23% mAP。使用两种采样后，NV3D在汽车检测中达到85.54% mAP，比基线高出1.56% mAP，尽管大约55%的体素被过滤掉。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D目标检测中的特征提取效率和计算复杂度问题。在自动驾驶领域，精确检测车辆、行人和骑行者等对象至关重要，但现有方法要么依赖多模态数据融合面临特征对齐挑战，要么仅使用局部特征提取对复杂任务过于简化。提高检测精度和效率对于确保自动驾驶系统的安全性和可靠性具有现实意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察发现点云数据中存在冗余，特别是近距离密集点云集群，并提出几何特征可从邻近点提取的假设。他们借鉴了Voxel R-CNN作为基础架构，采用KNN和PCA方法提取法向量特征，受VirConv中关于远距离点影响更大的启发，并参考了注意力机制设计了元素级注意力融合。这种方法结合了传统几何处理和深度学习的优势，既减少了计算复杂度又保留了关键空间信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用局部体素特征计算法向量作为新特征表示，通过采样减少冗余数据，并融合体素和法向量特征。实现流程包括：1)将LiDAR点云转换为体素；2)使用KNN和PCA提取法向量特征；3)应用法向量密度采样(去除密度&gt;0.7的50%体素)和FOV感知的基于bin采样(保持空间连续性)；4)通过元素级注意力机制融合两种特征；5)将融合特征输入Voxel R-CNN架构进行目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入法向量作为3D目标检测的新特征表示；2)提出两种创新采样策略(法向量密度采样和FOV感知的基于bin采样)；3)设计元素级注意力融合机制。相比之前工作，NV3D专注于单模态LiDAR数据处理避免了特征对齐问题；使用法向量而非直接处理点云减少了计算复杂度；采样策略考虑了法向量和视野因素保留了更多有用信息；将局部特征转换为法向量表示提高了效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NV3D通过引入法向量特征和创新的采样策略，在保持高性能的同时显著提高了3D目标检测的效率，特别是在车辆和骑行者检测任务中表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in 3D object detection for autonomous vehicles aim to enrichfeatures through the utilization of multi-modal setups or the extraction oflocal patterns within LiDAR point clouds. However, multi-modal methods facesignificant challenges in feature alignment, and gaining features locally canbe oversimplified for complex 3D object detection tasks. In this paper, wepropose a novel model, NV3D, which utilizes local features acquired from voxelneighbors, as normal vectors computed per voxel basis using K-nearest neighbors(KNN) and principal component analysis (PCA). This informative feature enablesNV3D to determine the relationship between the surface and pertinent targetentities, including cars, pedestrians, or cyclists. During the normal vectorextraction process, NV3D offers two distinct sampling strategies: normal vectordensity-based sampling and FOV-aware bin-based sampling, allowing eliminationof up to 55% of data while maintaining performance. In addition, we appliedelement-wise attention fusion, which accepts voxel features as the query andvalue and normal vector features as the key, similar to the attentionmechanism. Our method is trained on the KITTI dataset and has demonstratedsuperior performance in car and cyclist detection owing to their spatialshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP incar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% ofvoxels being filtered out.</description>
      <author>example@mail.com (Krittin Chaowakarn, Paramin Sangwongngam, Nang Htet Htet Aung, Chalie Charoenlarpnopparut)</author>
      <guid isPermaLink="false">2510.11632v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SNAP: Towards Segmenting Anything in Any Point Cloud</title>
      <link>http://arxiv.org/abs/2510.11565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page, https://neu-vi.github.io/SNAP/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SNAP是一个统一的交互式3D点云分割模型，支持跨不同领域的点和文本提示，通过多数据集训练和领域自适应归一化实现跨领域通用性，在多个基准测试上取得了最先进或具有竞争力的结果。&lt;h4&gt;背景&lt;/h4&gt;交互式3D点云分割通过用户引导提示能够高效标注复杂3D场景，但当前方法通常局限于单一领域（室内或室外）和单一形式的用户交互（空间点击或文本提示）。在多个数据集上训练通常会导致负迁移，导致缺乏通用性的领域特定工具。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一模型，支持跨不同领域的基于点和基于文本的提示进行交互式3D分割，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出SNAP（Segment Anything in Any Point cloud）模型，通过在涵盖室内、室外和空中环境的7个数据集上训练实现跨领域通用性，使用领域自适应归一化来防止负迁移。对于文本提示的分割，自动生成掩码提案并与文本查询的CLIP嵌入进行匹配，支持全景和开放词汇分割。&lt;h4&gt;主要发现&lt;/h4&gt;SNAP在空间提示分割的9个零样本基准测试中，有8个达到了最先进的性能，在所有5个文本提示基准测试中展示了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;统一模型可以匹配或超越专门的领域特定方法，为可扩展的3D标注提供实用工具。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D点云分割通过用户引导提示能够高效标注复杂3D场景。然而，当前方法通常在范围上局限于单一领域（室内或室外），并且局限于单一形式的用户交互（空间点击或文本提示）。此外，在多个数据集上训练通常会导致负迁移，产生缺乏通用性的领域特定工具。为解决这些限制，我们提出了SNAP（Segment Anything in Any Point cloud），这是一个统一的交互式3D分割模型，支持跨不同领域的基于点和基于文本的提示。我们的方法通过在涵盖室内、室外和空中环境的7个数据集上训练实现跨领域通用性，同时采用领域自适应归一化来防止负迁移。对于文本提示的分割，我们无需人工干预自动生成掩码提案，并将其与文本查询的CLIP嵌入进行匹配，实现全景和开放词汇分割。大量实验证明，SNAP始终提供高质量的分割结果。我们在空间提示分割的9个零样本基准测试中的8个上取得了最先进的性能，并在所有5个文本提示基准测试上展示了具有竞争力的结果。这些结果表明，统一模型可以匹配或超越专门的领域特定方法，为可扩展的3D标注提供实用工具。项目页面位于https://neu-vi.github.io/SNAP/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前3D点云交互分割方法局限于单一领域（室内或室外）和单一交互形式（空间点击或文本提示）的问题，以及多数据集训练导致的负迁移问题。这个问题很重要，因为3D场景标注需要大量人工努力，而缺乏通用性的工具限制了它们作为高效标注工具的采用，领域特定工具需要单独训练增加了使用成本，缺乏灵活性也限制了用户适应不同的标注需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到2D图像中SAM成功的启发，认识到当前方法缺乏泛化能力和交互灵活性，因此决定创建一个统一的跨领域模型。他们设计时借鉴了现有工作：使用SAM的掩码解码器设计，借鉴AGILE3D和Interactive4D的点击采样策略，利用CLIP模型处理文本提示和嵌入匹配。核心创新是提出领域自适应归一化来解决跨领域训练的负迁移问题，并设计自动提示点生成算法实现无需人工干预的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的模型，支持在不同领域（室内、室外、空中）的3D点云上进行多模态（空间和文本）交互式分割。整体流程包括：1)点云编码：使用Point Transformer V3提取点嵌入并应用领域自适应归一化；2)空间提示分割：编码点击点，通过掩码解码器生成掩码；3)文本提示分割：自动生成提示点，生成掩码提案并匹配CLIP嵌入；4)训练：结合多种损失函数优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)领域自适应归一化解决跨领域训练负迁移；2)统一的多领域模型支持室内、室外和空中场景；3)多模态提示同时支持空间点击和文本描述；4)自动提示点生成算法实现无需人工干预的分割；5)开放词汇分割支持新类别。相比之前工作，SNAP突破了单一领域限制，支持多种提示方式，解决了负迁移问题，可直接处理点云无需RGB图像，并在多个零样本测试中达到最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SNAP是一个统一的、支持多模态提示的3D点云交互分割模型，通过领域自适应归一化实现了跨领域泛化能力，并在多种场景的分割任务中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D point cloud segmentation enables efficient annotation ofcomplex 3D scenes through user-guided prompts. However, current approaches aretypically restricted in scope to a single domain (indoor or outdoor), and to asingle form of user interaction (either spatial clicks or textual prompts).Moreover, training on multiple datasets often leads to negative transfer,resulting in domain-specific tools that lack generalizability. To address theselimitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3Dsegmentation that supports both point-based and text-based prompts acrossdiverse domains. Our approach achieves cross-domain generalizability bytraining on 7 datasets spanning indoor, outdoor, and aerial environments, whileemploying domain-adaptive normalization to prevent negative transfer. Fortext-prompted segmentation, we automatically generate mask proposals withouthuman intervention and match them against CLIP embeddings of textual queries,enabling both panoptic and open-vocabulary segmentation. Extensive experimentsdemonstrate that SNAP consistently delivers high-quality segmentation results.We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks forspatial-prompted segmentation and demonstrate competitive results on all 5text-prompted benchmarks. These results show that a unified model can match orexceed specialized domain-specific approaches, providing a practical tool forscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/</description>
      <author>example@mail.com (Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang)</author>
      <guid isPermaLink="false">2510.11565v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2510.11509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and  Code: https://github.com/RuipingL/Situat3DChange&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Situat3DChange数据集和SCReasoner方法，用于解决3D动态场景和情境理解的不完整问题，通过大规模数据集和高效3D MLLM方法提升动态环境变化的理解能力。&lt;h4&gt;背景&lt;/h4&gt;物理环境和情况本质上是动态的，但当前3D数据集和评估基准往往只专注于动态场景或动态情况的孤立研究，导致对动态环境的理解不完整。&lt;h4&gt;目的&lt;/h4&gt;克服现有3D数据集的局限性，引入支持情境感知变化理解任务的大规模数据集，并开发高效方法进行点云比较和动态场景理解。&lt;h4&gt;方法&lt;/h4&gt;构建Situat3DChange数据集，包含121K问答对、36K变化描述和17K重排指令；利用11K人类环境变化观察建立共享心智模型；融合自我中心和他者中心视角及空间关系；提出SCReasoner方法进行高效点云比较。&lt;h4&gt;主要发现&lt;/h4&gt;Situat3DChange任务上的评估突显了MLLMs在动态场景和情境理解方面的进展和局限性；数据扩展和跨域迁移实验证明了使用Situat3DChange作为训练数据集的任务无关有效性。&lt;h4&gt;结论&lt;/h4&gt;Situat3DChange数据集和SCReasoner方法为动态场景和情境理解提供了新的工具和视角，有助于提升AI对环境动态变化的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;物理环境和情况本质上是动态的，然而当前的3D数据集和评估基准往往只专注于动态场景或动态情况的孤立研究，导致理解不完整。为克服这些限制，我们引入Situat3DChange，一个支持三种情境感知变化理解任务的大规模数据集：121K问答对，36K用于感知任务的变化描述，以及17K用于行动任务的重排指令。为构建这一大规模数据集，Situat3DChange利用11K人类对环境变化的观察来建立人类-AI协作的共享心智模型和共享情境感知。这些观察融合了自我中心和他者中心视角以及分类和坐标空间关系，通过LLM集成以支持对情境变化的理解。为解决比较同一场景中具有微小变化的点云对这一挑战，我们提出SCReasoner，一种高效的3D MLLM方法，能够以最小的参数开销进行有效的点云比较，且语言解码器不需要额外令牌。在Situat3DChange任务上的全面评估突显了MLLMs在动态场景和情境理解方面的进展和局限性。关于数据扩展和跨域迁移的额外实验证明了使用Situat3DChange作为MLLMs训练数据集的任务无关有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前3D数据集和评估基准孤立关注动态场景或动态情境的问题，导致对环境变化的理解不完整。这个问题很重要，因为物理环境和情境本质上是动态的，即使是微小的位置变化对视障人士也可能造成障碍，有效的人机协作需要建立共享的心理模型和情境感知能力，而现有方法无法同时捕捉动态场景和情境感知。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析人类与机器人认知差异发现人类以柱面坐标感知环境而机器人以笛卡尔坐标感知，这导致无法共享心理地图。作者收集11K人类注释建立基于人类感知的共享模型，并整合自我中心和异中心视角以及分类和坐标空间关系。作者借鉴了3RScan数据集、3DSSG场景图、MSQA的情境采样方法、LEO框架，并使用GPT-4生成类人文本，同时采用Mamba的选择性状态空间模型和星操作进行token选择与融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建情境感知的3D变化理解数据集并开发高效的SCReasoner架构，通过结合人类注释和LLM生成保留人类感知框架。数据集构建流程包括情境采样、长文本生成、查询生成、问答生成和数据质量控制。SCReasoner实现流程使用共同编码器将两个点云嵌入token，从前一场景选择信息丰富token并与当前场景token融合，使用Mamba进行token选择，星操作进行token融合，构建在LEO框架上仅添加少量额外参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Situat3DChange数据集首个整合动态场景和情境感知，包含121K问答对、36K变化描述和17K重新排列指令；2) SCReasoner架构专门处理成对点云，使用Mamba和星操作实现高效比较；3) 基于人类感知框架整合自我中心和异中心视角。相比之前工作，该数据集同时关注动态场景和情境感知，SCReasoner专门设计用于点云比较并关注差异而非冗余，评估方法更全面且包含特殊距离评估指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了首个情境感知的3D变化理解数据集和高效的SCReasoner架构，通过整合人类感知与AI系统，实现了对动态环境和情境变化的全面理解，为人机协作在动态环境中的适应性交互提供了新的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physical environments and circumstances are fundamentally dynamic, yetcurrent 3D datasets and evaluation benchmarks tend to concentrate on eitherdynamic scenarios or dynamic situations in isolation, resulting in incompletecomprehension. To overcome these constraints, we introduce Situat3DChange, anextensive dataset supporting three situation-aware change understanding tasksfollowing the perception-action model: 121K question-answer pairs, 36K changedescriptions for perception tasks, and 17K rearrangement instructions for theaction task. To construct this large-scale dataset, Situat3DChange leverages11K human observations of environmental changes to establish shared mentalmodels and shared situational awareness for human-AI collaboration. Theseobservations, enriched with egocentric and allocentric perspectives as well ascategorical and coordinate spatial relations, are integrated using an LLM tosupport understanding of situated changes. To address the challenge ofcomparing pairs of point clouds from the same scene with minor changes, wepropose SCReasoner, an efficient 3D MLLM approach that enables effective pointcloud comparison with minimal parameter overhead and no additional tokensrequired for the language decoder. Comprehensive evaluation on Situat3DChangetasks highlights both the progress and limitations of MLLMs in dynamic sceneand situation understanding. Additional experiments on data scaling andcross-domain transfer demonstrate the task-agnostic effectiveness of usingSituat3DChange as a training dataset for MLLMs.</description>
      <author>example@mail.com (Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.11509v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces</title>
      <link>http://arxiv.org/abs/2510.11014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于采样的管道，利用大规模预训练生成模型在零样本方式下产生概率先验，捕捉环境不确定性和空间-语义关系，用于部分可观察性条件下的机器人规划。&lt;h4&gt;背景&lt;/h4&gt;在部分可观察性条件下进行规划时，先验信息非常重要，但在实践中难以获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用生成模型产生概率先验的方法，以零样本方式捕捉环境不确定性和空间-语义关系，并直接用于配置空间规划。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于采样的管道，基于部分观察条件恢复完整的RGB-D点云样本，包含占用信息和目标语义；建立Matterport3D基准测试，场景为只能通过门看到部分区域的房间，机器人需要导航到未观察到的目标物体。&lt;h4&gt;主要发现&lt;/h4&gt;有效先验必须表示未观察区域中的占用和目标位置不确定性；方法恢复了与真实情况一致的常识空间语义，生成了多样化的、干净的3D点云，可用于运动规划。&lt;h4&gt;结论&lt;/h4&gt;生成模型作为机器人规划中先验信息的丰富来源具有很大潜力。&lt;h4&gt;翻译&lt;/h4&gt;先验信息对于部分可观察性条件下的规划至关重要，但在实践中难以获取。我们提出了一种基于采样的管道，利用大规模预训练生成模型以零样本方式产生概率先验，捕捉环境不确定性和空间-语义关系。基于部分观察条件，该管道能够恢复包含占用信息和目标语义的完整RGB-D点云样本，可直接用于配置空间规划。我们建立了一个Matterport3D基准测试，场景为只能通过门看到部分区域的房间，机器人必须导航到未观察到的目标物体。此场景的有效先验必须表示未观察区域中的占用和目标位置不确定性。实验表明，我们的方法恢复了与真实情况一致的常识空间语义，生成了多样化的、干净的3D点云，可用于运动规划，突显了生成模型作为机器人规划中丰富先验来源的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人如何在部分可观测环境中（如只能通过门缝看到房间一部分）获取环境不确定性先验信息的问题。这个问题很重要，因为随着机器人应用扩展到真实世界环境，环境不确定性不可避免，而传统的规划方法依赖于难以获取且可能不准确的手工或预编程先验信息。准确的环境不确定性先验对机器人在未知区域进行有效导航和规划至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出在部分可观测环境下的规划需要先验信息但难以获取，然后注意到生成式视觉模型能生成符合数据分布的内容并可根据条件输入。他们设计了一个基于采样的流程，借鉴了多项现有工作：VLM用于图像分类和物体提示生成、FLUX图像外推模型用于场景扩展、单目深度估计器用于3D重建、以及现有的采样规划算法。作者将这些现有技术整合成一个完整的流水线，用于生成条件于部分观测的完整环境样本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模预训练的生成模型作为环境采样器，根据部分观测生成捕捉环境不确定性和空间-语义关系的3D环境样本，然后使用这些样本作为先验信息进行配置空间规划。整体流程包括：1) VLM提示机制对输入图像分类并生成相关物体提示；2) 基于图像的生成使用FLUX模型扩展场景图像；3) 物体分割和地板估计进行语义分割和对齐；4) 深度估计和对齐将RGB-D转换为点云；5) 配置空间规划使用采样先验进行运动规划。整个流程约需10.5秒生成一个样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 利用预训练生成模型采样环境不确定性先验的流程；2) 零样本方法直接生成有用先验；3) 空间-语义先验的形式化方法；4) 基于Matterport3D的新数据集；5) 通过模拟运动规划验证实用性。相比之前工作，本文与传统场景完成方法不同，后者追求单一一致重建，而本文捕捉多样性同时确保干净几何；区别于现有生成模型应用，后者专注于目标分布或策略优化，而本文构建环境先验；不同于显式环境先验，后者是空间-语义地图或场景完成模型，而本文采样扩展视野上的分布；也区别于高计算需求的3D生成方法，本文使用更高效的2D生成模型结合单目深度估计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种利用预训练生成模型从部分观测采样环境不确定性先验的创新方法，使机器人能够在未观测区域进行有效的空间-语义推理和规划。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Priors are vital for planning under partial observability, yet difficult toobtain in practice. We present a sampling-based pipeline that leverageslarge-scale pretrained generative models to produce probabilistic priorscapturing environmental uncertainty and spatio-semantic relationships in azero-shot manner. Conditioned on partial observations, the pipeline recoverscomplete RGB-D point cloud samples with occupancy and target semantics,formulated to be directly useful in configuration-space planning. We establisha Matterport3D benchmark of rooms partially visible through doorways, where arobot must navigate to an unobserved target object. Effective priors for thissetting must represent both occupancy and target-location uncertainty inunobserved regions. Experiments show that our approach recovers commonsensespatial semantics consistent with ground truth, yielding diverse, clean 3Dpoint clouds usable in motion planning, highlight the promise of generativemodels as a rich source of priors for robotic planning.</description>
      <author>example@mail.com (Subhransu S. Bhattacharjee, Hao Lu, Dylan Campbell, Rahul Shome)</author>
      <guid isPermaLink="false">2510.11014v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>rareboost3d: a synthetic lidar dataset with enhanced rare classes</title>
      <link>http://arxiv.org/abs/2510.10876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过引入合成数据集和跨域语义对齐方法解决了点云数据中的长尾问题，提高了激光雷达感知技术的性能。&lt;h4&gt;背景&lt;/h4&gt;真实世界点云数据集对基于激光雷达的感知技术（如自动驾驶中的物体分割）有重要贡献，但某些罕见类别的实例数量有限导致长尾问题仍然存在。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集中罕见类别实例不足的问题，通过合成数据补充真实世界数据。&lt;h4&gt;方法&lt;/h4&gt;提出了名为RareBoost3D的合成点云数据集，以及名为CS Loss的跨域语义对齐方法，用于对齐不同域中相同类别的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，跨域语义对齐方法显著提高了激光雷达点云分割模型在真实世界数据上的性能。&lt;h4&gt;结论&lt;/h4&gt;结合合成数据和真实世界数据，并应用跨域语义对齐方法，可以有效解决长尾问题，提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;真实世界的点云数据集对基于激光雷达的感知技术的发展做出了重大贡献，例如自动驾驶中的物体分割。然而，由于某些罕见类别中的实例数量有限，长尾问题仍然是现有数据集的主要挑战。为了解决这个问题，我们引入了一个名为RareBoost3D的新型合成点云数据集，通过为真实世界数据集中稀少的物体类别提供更多实例来补充现有的真实世界数据集。为了有效利用合成和真实世界数据，我们进一步提出了一个名为CS Loss的跨域语义对齐方法，用于对齐不同域中相同类别的特征表示。实验结果表明，这种对齐方法显著提高了激光雷达点云分割模型在真实世界数据上的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是LiDAR点云数据集中的'长尾问题'，即常见类别（如汽车）的实例数量远多于稀有类别（如行人、自行车等）。这个问题在自动驾驶领域非常重要，因为模型需要准确识别各种类别的对象，而稀有类别样本不足会导致对这些关键对象的识别能力下降，可能影响自动驾驶系统的安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到真实世界点云数据集的局限性：获取标注成本高、耗时长且存在类别不平衡。他们借鉴了多数据集联合训练和合成到真实领域迁移学习的思路，选择使用CARLA模拟器生成合成数据。特别的是，他们没有简单复制真实数据的分布，而是主动增加稀有类别的实例数量。为了解决合成与真实数据间的域差距，他们基于PointDR的跨域特征对齐方法，采用对比学习技术设计了CSC损失函数，这借鉴了对比学习在特征对齐方面的成功应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个专门增强稀有类别的大规模合成LiDAR数据集，并通过对比学习对齐合成与真实数据中相同类别的特征表示。实现流程包括：1) 使用CARLA模拟器生成8个不同地图的LiDAR序列，特别增加稀有类别的实例；2) 为真实和合成数据分别构建类别特征原型并存储在记忆库中；3) 使用对比学习使相同类别的特征在共享语义空间中接近，不同类别相互分离；4) 结合语义分割损失和对比损失进行模型训练，减少域差距的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了RareBoost3D数据集，不仅规模大，还特别增加了稀有类别的实例数量；2) 设计了跨域语义一致性(CSC)损失函数，通过对比学习而非复杂的对抗训练来实现域对齐；3) 实验证明调整稀有类别在合成数据中的比例可以显著提升这些类别的分割性能。相比之前的工作，不同之处在于：与SynthmanticLiDAR不同，RareBoost3D不复制而是主动改变类别分布；与SynLiDAR和ePointDA不同，CSC损失不需要对抗训练；传统数据增强方法主要在几何层面操作，而RareBoost3D引入了全新样本。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RareBoost3D数据集和跨域语义一致性损失函数通过增加稀有类别的样本数量并有效对齐合成与真实数据的特征表示，显著提升了LiDAR点云分割模型在稀有类别上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world point cloud datasets have made significant contributions to thedevelopment of LiDAR-based perception technologies, such as object segmentationfor autonomous driving. However, due to the limited number of instances in somerare classes, the long-tail problem remains a major challenge in existingdatasets. To address this issue, we introduce a novel, synthetic point clouddataset named RareBoost3D, which complements existing real-world datasets byproviding significantly more instances for object classes that are rare inreal-world datasets. To effectively leverage both synthetic and real-worlddata, we further propose a cross-domain semantic alignment method named CSCloss that aligns feature representations of the same class across differentdomains. Experimental results demonstrate that this alignment significantlyenhances the performance of LiDAR point cloud segmentation models overreal-world data.</description>
      <author>example@mail.com (Shutong Lin, Zhengkang Xiang, Jianzhong Qi, Kourosh Khoshelham)</author>
      <guid isPermaLink="false">2510.10876v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MATStruct: High-Quality Medial Mesh Computation via Structure-aware Variational Optimization</title>
      <link>http://arxiv.org/abs/2510.10751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的优化框架，用于计算中轴变换，同时保留中轴结构并确保高质量的中轴网格。该方法基于结构感知的粒子优化流程，由限制性幂图引导，通过球形二次误差度量和高斯核能量来约束和优化中轴球的分布。相比现有方法，该技术产生更清洁、准确的中轴结构，具有更好的几何保真度、拓扑正确性和明确的结构分解。&lt;h4&gt;背景&lt;/h4&gt;中轴结构由相互连接的薄片、接缝和连接点组成，为3D形状提供自然的体积分解。现有的中轴变换计算方法存在一些局限性，如特征保持方法(MATFP和MATTopo)产生的中轴结构不够清洁和准确，而基于体素、点云和变分的方法未能将结构感知集成到优化过程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的优化框架，能够同时保留中轴结构并确保高质量的中轴网格，克服现有方法的局限性，产生具有更好几何保真度、拓扑正确性和明确结构分解的中轴网格。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于结构感知的粒子优化流程，由限制性幂图(RPD)引导，将输入体积划分为凸单元，其对偶编码了中轴网格的连通性。通过球形二次误差度量(SQEM)投影强制执行结构感知，约束中轴球的移动，同时使用高斯核能量鼓励均匀的空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;1. 与特征保持方法(MATFP和MATTopo)相比，新方法产生更清洁、更准确的中轴结构，网格质量显著提高；2. 与基于体素、点云和变分的方法相比，该框架首次将结构感知集成到优化过程中；3. 产生的中轴网格具有优越的几何保真度、拓扑正确性和明确的结构分解。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种新的优化框架，能够有效计算中轴变换并生成高质量的中轴网格。该方法通过结构感知的粒子优化流程和限制性幂图引导，克服了现有方法的局限性，为中轴变换计算提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于计算中轴变换的新型优化框架，该框架同时保留中轴结构并确保高质量的中轴网格。中轴结构由相互连接的薄片、接缝和连接点组成，为3D形状提供自然的体积分解。我们的方法引入了一种基于结构感知的粒子优化流程，由限制性幂图(RPD)引导，该图将输入体积划分为凸单元，其对偶编码了中轴网格的连通性。通过球形二次误差度量(SQEM)投影强制执行结构感知，约束中轴球的移动，同时高斯核能量鼓励均匀的空间分布。与特征保持方法如MATFP和MATTopo相比，我们的方法产生更清洁、更准确的中轴结构，网格质量显著提高。与基于体素、点云和变分的方法相比，我们的框架是第一个将结构感知集成到优化过程中的方法，产生具有优越几何保真度、拓扑正确性和明确结构分解的中轴网格。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决计算高质量中轴网格（medial mesh）时难以同时保持中轴结构（medial structure）和网格质量的问题。中轴变换作为形状分析的基础，能捕捉形状的拓扑和几何特性，在形状分析、识别、匹配等下游应用中至关重要。特别是在CAD模型中，清晰的中轴结构对工程设计、制造和模拟有重要价值，而现有方法无法同时保证结构清晰度和几何质量，限制了中轴变换的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法如MATFP和MATTopo的局限性，特别是在球体分类和球体过度拥挤方面的问题。作者发现基于表面的RPD分类方法在球体偏离中轴时会导致误分类，因此提出使用体积RPD而非表面RPD来解决分类问题。作者借鉴了粒子优化方法来促进球体均匀分布，并设计了球形二次误差度量（SQEM）来约束球体移动，确保结构感知的优化。同时借鉴了MATTopo的拓扑保持策略和球形收缩算法用于投影回中轴，但进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构感知的变分优化同时保持中轴结构和确保高质量的中轴网格，使用体积RPD进行更准确的球体分类，并利用基于粒子的优化方案结合SQEM约束确保球体沿着正确的子结构移动。整体流程包括：1)初始化：使用球形收缩算法在形状表面均匀采样初始球体；2)迭代优化：计算RPD、采样RPC、计算球体间作用力和能量、梯度投影约束移动、L-BFGS优化；3)投影步骤：将优化后的球体投影回中轴；4)重复优化直到结构收敛；5)计算中轴网格作为RPD对偶；6)后处理修剪无效连接。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出全面的RPD-based优化框架生成高质量结构感知的3D中轴网格；2)提出基于体积RPD的鲁棒球体分类策略提高分类准确性；3)引入中轴结构误差率（MSER）作为新的定量评估指标。相比之前工作，不同之处在于：优化了插入球体的位置而非固定它们；使用体积RPD而非表面RPD进行分类；通过结构感知梯度投影确保球体沿正确子结构移动；在优化过程中促进球体均匀分布减少过度拥挤；引入MSER指标评估中轴结构准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATStruct通过结合结构感知的变分优化和基于体积RPD的球体分类，首次实现了高质量中轴网格的生成，同时保持了中轴结构的完整性和几何保真度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel optimization framework for computing the medial axistransform that simultaneously preserves the medial structure and ensures highmedial mesh quality. The medial structure, consisting of interconnected sheets,seams, and junctions, provides a natural volumetric decomposition of a 3Dshape. Our method introduces a structure-aware, particle-based optimizationpipeline guided by the restricted power diagram (RPD), which partitions theinput volume into convex cells whose dual encodes the connectivity of themedial mesh. Structure-awareness is enforced through a spherical quadraticerror metric (SQEM) projection that constrains the movement of medial spheres,while a Gaussian kernel energy encourages an even spatial distribution.Compared to feature-preserving methods such as MATFP and MATTopo, our approachproduces cleaner and more accurate medial structures with significantlyimproved mesh quality. In contrast to voxel-based, point-cloud-based, andvariational methods, our framework is the first to integrate structuralawareness into the optimization process, yielding medial meshes with superiorgeometric fidelity, topological correctness, and explicit structuraldecomposition.</description>
      <author>example@mail.com (Ningna Wang, Rui Xu, Yibo Yin, Zichun Zhong, Taku Komura, Wenping Wang, Xiaohu Guo)</author>
      <guid isPermaLink="false">2510.10751v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting</title>
      <link>http://arxiv.org/abs/2510.10726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page, code, and models will be publicly available soon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了WorldMirror，一个用于多样化3D几何预测任务的一体化前馈模型，能够整合多种几何先验信息并同时生成多种3D表示，在各种任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的3D几何预测方法要么仅限于图像输入，要么针对特定任务定制，缺乏灵活性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够灵活整合多种几何先验信息，同时生成多种3D表示的统一框架，解决结构歧义问题，并实现高效的3D几何预测。&lt;h4&gt;方法&lt;/h4&gt;提出WorldMirror模型，一种前馈架构，能够整合相机位姿、内参和深度图等几何先验信息，同时生成密集点云、多视角深度图、相机参数、表面法向量和3D高斯等多种3D表示。&lt;h4&gt;主要发现&lt;/h4&gt;WorldMirror在各种基准测试中实现了最先进的性能，包括相机估计、点图估计、深度估计、表面法向量估计和新视角合成，同时保持了前向推理的效率。&lt;h4&gt;结论&lt;/h4&gt;WorldMirror提供了一个优雅且统一的解决方案，能够利用可用的先验信息解决结构歧义，并在单次前向传播中生成几何一致的3D输出，为多样化3D几何预测任务提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了WorldMirror，这是一个用于多样化3D几何预测任务的一体化前馈模型。与仅限于图像输入或针对特定任务定制的现有方法不同，我们的框架灵活整合了多种几何先验信息，包括相机位姿、内参和深度图，同时生成多种3D表示：密集点云、多视角深度图、相机参数、表面法向量和3D高斯。这种优雅且统一的架构利用可用的先验信息解决结构歧义，并在单次前向传播中生成几何一致的3D输出。WorldMirror在从相机、点图、深度和表面法向量估计到新视角合成的各种基准测试中实现了最先进的性能，同时保持了前向推理的效率。代码和模型即将公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有3D重建方法的两个局限性：一是输入限制，大多数方法只能处理原始图像，无法利用校准内参、相机位姿和深度测量等有用的先验信息；二是输出限制，方法通常只针对单一任务优化，很少在统一框架内整合多个任务。这些问题在现实中很重要，因为先验信息可以解决尺度模糊、确保多视图一致性，并在图像线索不足区域提供基础；统一框架能更高效处理各种3D重建任务，确保不同输出间的几何一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法在输入和输出空间都存在局限性，提出关键问题：能否通过有效利用多样化先验知识，在通用3D重建架构中解决这些挑战？他们设计了多模态先验提示机制和通用几何预测架构，借鉴了DUSt3R、VGGT等前馈3D重建模型的思想，以及3D高斯溅射在新视图合成中的应用，同时参考了传统优化方法利用已知相机参数提高重建质量的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的前馈模型，能灵活整合多种几何先验信息，同时生成多种3D表示（点云、深度图、相机参数、表面法线、3D高斯），利用先验解决结构歧义，提供几何一致的3D输出。整体流程：1）多模态先验提示 - 将相机位姿、内参、深度图转换为令牌并整合；2）通用几何预测 - 使用DPT头和Transformer层预测各种几何属性；3）动态训练策略 - 随机采样不同先验组合，采用课程学习从简单到复杂优化训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）多模态先验提示机制，首次系统探索多模态几何先验注入；2）通用几何预测架构，支持全方位3D重建任务；3）动态先验注入方案，适应任意先验组合；4）系统课程学习策略，优化训练效率。相比之前工作：输入上能灵活利用多种先验而非仅图像；输出上能同时处理多种任务而非单一优化；性能上在多个基准测试实现最先进结果；同时保持前向推理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; WorldMirror是一个统一的前馈3D重建模型，通过灵活整合多种几何先验信息，在单次前向传播中同时生成多种高质量3D表示，实现了在各种3D重建任务上的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present WorldMirror, an all-in-one, feed-forward model for versatile 3Dgeometric prediction tasks. Unlike existing methods constrained to image-onlyinputs or customized for a specific task, our framework flexibly integratesdiverse geometric priors, including camera poses, intrinsics, and depth maps,while simultaneously generating multiple 3D representations: dense pointclouds, multi-view depth maps, camera parameters, surface normals, and 3DGaussians. This elegant and unified architecture leverages available priorinformation to resolve structural ambiguities and delivers geometricallyconsistent 3D outputs in a single forward pass. WorldMirror achievesstate-of-the-art performance across diverse benchmarks from camera, point map,depth, and surface normal estimation to novel view synthesis, while maintainingthe efficiency of feed-forward inference. Code and models will be publiclyavailable soon.</description>
      <author>example@mail.com (Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, Chunchao Guo)</author>
      <guid isPermaLink="false">2510.10726v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>dN/dx Reconstruction with Deep Learning for High-Granularity TPCs</title>
      <link>http://arxiv.org/abs/2510.10628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为GraphPT的深度学习模型，用于粒子物理实验中的dN/dx重建，该模型在粒子识别性能上超过了传统方法，特别是在K/π分离方面有显著提升。&lt;h4&gt;背景&lt;/h4&gt;粒子识别对未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高granularity时间投影室能提供精确跟踪和dN/dx测量用于粒子识别，但准确重建仍是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习模型（GraphPT）来解决dN/dx重建的挑战，提高粒子识别性能，特别是改善K/π粒子的分离能力。&lt;h4&gt;方法&lt;/h4&gt;将TPC数据表示为点云，采用基于图神经网络的U-Net架构作为网络主干，并融入针对点云处理优化的注意力机制用于节点聚合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的GraphPT模型在粒子识别性能上超过了传统的截断均值方法。在动量区间5到20 GeV/c时，K/π分离能力提高了约10%到20%。&lt;h4&gt;结论&lt;/h4&gt;GraphPT模型是一种有效的dN/dx重建方法，能够显著提高粒子物理实验中的粒子识别性能，特别是对于动量在5到20 GeV/c范围内的K/π粒子分离。&lt;h4&gt;翻译&lt;/h4&gt;粒子识别对未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高granularity时间投影室不仅提供精确的跟踪，还能实现dN/dx测量用于粒子识别。dN/dx方法估计初级电离电子的数量，显著提高了粒子识别性能。然而，准确的重建对于这种方法仍然是一个主要挑战。在本文中，我们介绍了一种深度学习模型——图点变换器，用于dN/dx重建。在我们的方法中，TPC数据被表示为点云。网络主干采用基于图神经网络的U-Net架构，并融入了针对点云处理优化的注意力机制用于节点聚合。所提出的GraphPT模型在粒子识别性能上超过了传统的截断均值方法。特别是在动量区间从5到20 GeV/c时，K/π分离能力提高了约10%到20%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高粒度时间投影室（TPC）中dN/dx重建的挑战。dN/dx指单位长度轨迹上的初级电离电子数，用于粒子识别（PID）。这个问题很重要，因为准确的PID对下一代粒子物理实验（如CEPC和FCC）至关重要，特别是高动量下的强子识别。dN/dx方法相比传统dE/dx能显著提高PID性能，因为它直接测量初级电离簇数量，抑制了次级电离和能量波动的干扰，但高粒度TPC中的电子漂移和扩散使得准确重建变得困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到传统规则方法难以应对高粒度TPC中的dN/dx重建挑战，而深度学习可以提取数据中的复杂特征。他们将TPC数据表示为点云，设计了基于图神经网络的U-Net架构，并引入针对点云处理优化的注意力机制。该方法借鉴了多个现有工作：自注意力机制和Transformer架构、PointNet和PointNet++的点云处理方法、PointTransformer的自适应自注意力机制，以及之前在dN/dx重建中使用LSTM和DGCNN的研究。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将TPC数据表示为点云，使用基于图神经网络的U-Net架构处理这些点云，并通过注意力机制优化节点聚合。整体流程包括：1) 将TPC轨迹表示为点云，每个点包含电荷和定时信息；2) 构建k近邻图；3) 使用U-Net编码器-解码器结构处理图数据；4) 在每个层使用Transformer层进行节点间信息聚合；5) 使用端到端训练优化模型；6) 根据输出概率进行dN/dx重建，计算轨迹上被分类为正的命中数除以轨迹长度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出GraphPT模型专门用于高粒度TPC的dN/dx重建；2) 将TPC数据表示为点云并用图神经网络处理；3) 引入针对点云优化的注意力机制；4) 研究减法和点积两种注意力操作符，发现点积结合多头机制效果更好；5) 将之前的两步处理统一为单一模型。相比传统截断均值法，它不依赖人工规则，能处理复杂数据模式，K/π分离能力提高10-20%；相比之前基于神经网络的dN/dx工作，它专门针对高粒度TPC的三维点云数据，使用更先进的图神经网络和Transformer架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于图神经网络和Transformer的GraphPT深度学习方法，显著提高了高粒度TPC中dN/dx重建的粒子识别性能，相比传统方法实现了10-20%的K/π分离能力提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle identification (PID) is essential for future particle physicsexperiments such as the Circular Electron-Positron Collider and the FutureCircular Collider. A high-granularity Time Projection Chamber (TPC) not onlyprovides precise tracking but also enables dN/dx measurements for PID. ThedN/dx method estimates the number of primary ionization electrons, offeringsignificant improvements in PID performance. However, accurate reconstructionremains a major challenge for this approach. In this paper, we introduce a deeplearning model, the Graph Point Transformer (GraphPT), for dN/dxreconstruction. In our approach, TPC data are represented as point clouds. Thenetwork backbone adopts a U-Net architecture built upon graph neural networks,incorporating an attention mechanism for node aggregation specificallyoptimized for point cloud processing. The proposed GraphPT model surpasses thetraditional truncated mean method in PID performance. In particular, the$K/\pi$ separation power improves by approximately 10% to 20% in the momentuminterval from 5 to 20 GeV/c.</description>
      <author>example@mail.com (Guang Zhao, Yue Chang, Jinxian Zhang, Linghui Wu, Huirong Qi, Xin She, Mingyi Dong, Shengsen Sun, Jianchun Wang, Yifang Wang, Chunxu Yu)</author>
      <guid isPermaLink="false">2510.10628v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams</title>
      <link>http://arxiv.org/abs/2510.10602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpikeGrasp的神经启发的机器人抓取框架，它模仿生物视觉运动通路，直接从立体尖峰摄像机的原始异步事件推断抓取姿态，无需重建点云，在杂乱和无纹理场景中表现优于传统方法，且数据效率高。&lt;h4&gt;背景&lt;/h4&gt;大多数机器人抓取系统依赖于将传感器数据转换为显式的3D点云，这是生物智能中不存在的计算步骤，表明现有方法与生物智能有根本差异。&lt;h4&gt;目的&lt;/h4&gt;探索一种 fundamentally different、神经启发的6-DoF抓取检测范式，模仿生物视觉运动通路，实现更高效、更自然的机器人抓取。&lt;h4&gt;方法&lt;/h4&gt;引入SpikeGrasp框架，处理来自立体尖峰摄像机的原始异步事件，融合这些立体尖峰流，使用循环尖峰神经网络迭代改进抓取假设，构建大规模合成基准数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;SpikeGrasp超越了基于点云的传统基线方法，特别在杂乱和无纹理场景中表现更好，展示了卓越的数据效率。&lt;h4&gt;结论&lt;/h4&gt;通过建立这种端到端的神经启发方法的可行性，SpikeGrasp为未来能够实现自然界中流畅高效操作的系统铺平了道路，特别是对于动态物体。&lt;h4&gt;翻译&lt;/h4&gt;大多数机器人抓取系统依赖于将传感器数据转换为显式的3D点云，这是生物智能中不存在的计算步骤。本文探索了一种根本不同的、神经启发的6-DoF抓取检测范式。我们引入了SpikeGrasp框架，它模仿生物视觉运动通路，处理来自立体尖峰摄像机的原始、异步事件（类似于视网膜），直接推断抓取姿态。我们的模型融合这些立体尖峰流，并使用循环尖峰神经网络（类似于高级视觉处理）来迭代改进抓取假设，而无需重建点云。为验证这一方法，我们构建了一个大规模合成基准数据集。实验表明，SpikeGrasp超越了传统的基于点云的基线方法，特别是在杂乱和无纹理场景中，并表现出卓越的数据效率。通过建立这种端到端的神经启发方法的可行性，SpikeGrasp为未来能够实现自然界中流畅高效操作的系统铺平了道路，特别是对于动态物体。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从立体尖峰相机的原始异步事件流中直接检测6自由度抓取姿态，而不需要将传感器数据转换为3D点云。这个问题很重要，因为当前大多数机器人抓取系统依赖点云重建，这是一个计算密集且易受噪声影响的步骤，而生物系统并不依赖显式点云表示来抓取物体。直接从原始事件流推断抓取姿态可以减少计算负担，提高系统在杂乱场景中的鲁棒性，更接近生物系统的效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到生物视觉运动系统的启发，将系统分为生物启发的组件：人工视网膜（立体尖峰相机）、视觉通路（左右尖峰流处理和融合）、整合皮层（循环尖峰神经网络）和运动系统。他们借鉴了尖峰相机在图像重建、目标检测等任务中的应用，以及基于点云的抓取检测方法的评估框架和表示方法，但避免了显式的点云处理步骤，直接从原始事件流推断抓取姿态。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模仿生物视觉运动系统，直接从原始异步事件流中推断抓取姿态，而不需要显式的3D点云重建。整体流程分为三部分：1）视觉通路网络：从左右尖峰流提取特征，计算相关性，使用循环尖峰神经网络迭代更新；2）可抓取网络：解码隐藏状态生成物体存在概率和可抓取性概率图；3）抓取检测网络：从隐藏状态和可抓取位置预测完整的6-DoF抓取配置，选择最高分数的抓取作为最终估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）生物启发的端到端框架，直接从原始立体尖峰流检测抓取姿态；2）构建了第一个用于6-DoF抓取姿态检测的大规模合成尖峰流数据集；3）使用循环尖峰神经网络处理时空信息并迭代完善抓取假设；4）表现出强大的数据效率。相比之前工作，SpikeGrasp避免了点云重建的中间步骤，能处理完整6-DoF抓取姿态，在杂乱场景中表现更好，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpikeGrasp通过引入一种受生物启发的端到端框架，首次实现了从原始立体尖峰流直接检测6-DoF抓取姿态，避免了点云重建的中间步骤，并在杂乱场景中表现出更高的准确性和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most robotic grasping systems rely on converting sensor data into explicit 3Dpoint clouds, which is a computational step not found in biologicalintelligence. This paper explores a fundamentally different, neuro-inspiredparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework thatmimics the biological visuomotor pathway, processing raw, asynchronous eventsfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.Our model fuses these stereo spike streams and uses a recurrent spiking neuralnetwork, analogous to high-level visual processing, to iteratively refine grasphypotheses without ever reconstructing a point cloud. To validate thisapproach, we built a large-scale synthetic benchmark dataset. Experiments showthat SpikeGrasp surpasses traditional point-cloud-based baselines, especiallyin cluttered and textureless scenes, and demonstrates remarkable dataefficiency. By establishing the viability of this end-to-end, neuro-inspiredapproach, SpikeGrasp paves the way for future systems capable of the fluid andefficient manipulation seen in nature, particularly for dynamic objects.</description>
      <author>example@mail.com (Zhuoheng Gao, Jiyao Zhang, Zhiyong Xie, Hao Dong, Zhaofei Yu, Rongmei Chen, Guozhang Chen, Tiejun Huang)</author>
      <guid isPermaLink="false">2510.10602v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2510.10471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAGLFNet的伪图像语义分割框架，用于高效处理非结构化点云并提取结构化语义信息，在保持实时性能的同时实现了高精度。&lt;h4&gt;背景&lt;/h4&gt;环境感知系统在高精度测绘和自主导航中扮演关键角色，LiDAR作为核心传感器提供准确的3D点云数据。然而，如何高效处理非结构化点云并提取结构化语义信息仍是一个重大挑战。现有的基于伪图像的表示方法往往忽略了点云的结构和语义细节，导致特征融合和判别性有限。&lt;h4&gt;目的&lt;/h4&gt;设计一个伪图像语义分割框架，提取判别性特征，平衡处理效率和性能，同时保持点云的结构和语义细节。&lt;h4&gt;方法&lt;/h4&gt;提出DAGLFNet框架，包含三个主要模块：1) 全局-局部特征融合编码模块，增强局部特征相关性并捕获全局上下文信息；2) 多分支特征提取网络，捕获更多邻域信息并增强轮廓特征的判别性；3) 基于深度特征引导的注意力机制的特征融合，提高跨通道特征融合的精度。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估显示，DAGLFNet在SemanticKITTI和nuScenes验证集上分别达到了69.83%和78.65%的性能，平衡了高性能与实时能力，展示了基于LiDAR实时应用的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;DAGLFNet方法通过创新的特征提取和融合机制，有效解决了点云处理中的效率和性能平衡问题，为基于LiDAR的实时应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;环境感知系统在高精度测绘和自主导航中起着关键作用，LiDAR作为提供准确3D点云数据的核心传感器。如何高效处理非结构化点云同时提取结构化语义信息仍是一个重大挑战，近年来，出现了许多基于伪图像的表示方法以在效率和性能之间取得平衡。然而，它们通常忽略了点云的结构和语义细节，导致特征融合和判别性有限。在这项工作中，我们提出了DAGLFNet，一种基于伪图像的语义分割框架，旨在提取判别性特征。首先，使用全局-局部特征融合编码模块来增强集合内局部特征之间的相关性并捕获全局上下文信息。其次，采用多分支特征提取网络来捕获更多邻域信息并增强轮廓特征的判别性。最后，引入基于深度特征引导的注意力机制进行特征融合，以提高跨通道特征融合的精度。实验评估表明，DAGLFNet在SemanticKITTI和nuScenes的验证集上分别达到了69.83%和78.65%。该方法平衡了高性能与实时能力，展示了基于LiDAR实时应用的巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效处理非结构化的LiDAR点云数据并提取结构化语义信息的问题，特别是在基于伪图像的点云分割方法中，如何解决结构扭曲、边界模糊和语义模糊等挑战。这个问题在现实世界中非常重要，因为环境感知系统是高精度地图和自主导航的核心，LiDAR作为关键传感器提供的3D点云数据需要被准确理解和解析，这对自动驾驶、机器人等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前点云分割的三种主要方法（点方法、体素方法和混合策略）及其局限性，然后关注到基于范围图像的方法在计算效率和性能之间的平衡。作者发现现有伪图像方法忽略了点云的结构和语义细节，导致特征融合和判别能力有限。针对这些问题，作者设计了DAGLFNet框架，借鉴了现有点云处理的基本方法、范围图像表示方式以及深度学习中的注意力机制和多分支网络等设计理念，但进行了创新性改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局-局部特征融合增强局部特征间的相关性并捕获全局上下文信息，使用多分支特征提取网络捕获更多邻域信息并增强边界特征的判别能力，以及引入深度特征引导的注意力机制提高跨通道特征融合的精度。整体实现流程包括：1)特征编码器将点云分组并提取点级和组级特征；2)图像特征提取器使用多分支架构捕获不同感受野的特征；3)特征更新模块通过深度引导的注意力机制融合点级和组级特征；4)融合头模块聚合多阶段特征并生成最终语义预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了DAGLFNet网络架构，将点云几何特征与二维伪图像表示紧密集成；2)提出了包含三个关键模块的全面特征增强策略：GL-FFE模块捕获长程依赖并稳定局部几何表示，MB-FE网络扩大感受野并加强边界特征表达，FFDFA机制利用距离感知加权提高跨通道特征集成精度。相比之前的工作，DAGLFNet不仅处理投影到图像上的点，还解决了多点映射冲突问题，考虑了遮挡点，保留了完整三维结构，并强调了对子集内特征关系的一致建模同时系统考虑了空间距离的影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAGLFNet通过全局-局部特征融合、多分支特征提取和深度特征引导的注意力机制，有效解决了伪图像点云分割中特征表达不足的问题，在保持实时性能的同时显著提高了语义分割精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environmental perception systems play a critical role in high-precisionmapping and autonomous navigation, with LiDAR serving as a core sensor thatprovides accurate 3D point cloud data. How to efficiently process unstructuredpoint clouds while extracting structured semantic information remains asignificant challenge, and in recent years, numerous pseudo-image-basedrepresentation methods have emerged to achieve a balance between efficiency andperformance. However, they often overlook the structural and semantic detailsof point clouds, resulting in limited feature fusion and discriminability. Inthis work, we propose DAGLFNet, a pseudo-image-based semantic segmentationframework designed to extract discriminative features. First, the Global-LocalFeature Fusion Encoding module is used to enhance the correlation among localfeatures within a set and capture global contextual information. Second, theMulti-Branch Feature Extraction network is employed to capture moreneighborhood information and enhance the discriminability of contour features.Finally, a Feature Fusion via Deep Feature-guided Attention mechanism isintroduced to improve the precision of cross-channel feature fusion.Experimental evaluations show that DAGLFNet achieves 69.83\% and 78.65\% on thevalidation sets of SemanticKITTI and nuScenes, respectively. The methodbalances high performance with real-time capability, demonstrating greatpotential for LiDAR-based real-time applications.</description>
      <author>example@mail.com (Chuang Chen, Wenyi Ge)</author>
      <guid isPermaLink="false">2510.10471v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Learning with Graph-Tuple</title>
      <link>http://arxiv.org/abs/2510.10341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TAG workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多视图图元组框架，解决了传统图神经网络在密集图上的效率问题&lt;h4&gt;背景&lt;/h4&gt;图神经网络通常随图边数扩展，适合稀疏图但在密集图(如点云或分子相互作用)上效率较低&lt;h4&gt;目的&lt;/h4&gt;克服传统稀疏化方法强制选择单一交互尺度并丢弃其他尺度信息的限制&lt;h4&gt;方法&lt;/h4&gt;将图划分为不相交的子图，捕获主要局部相互作用和远程连接；通过受非交换算子理论启发的异构消息传递架构学习多视图表示&lt;h4&gt;主要发现&lt;/h4&gt;多视图图元组模型在分子属性预测和宇宙学参数推断两个应用中均优于单图基线模型&lt;h4&gt;结论&lt;/h4&gt;多视图方法具有强大功能和通用性，能有效处理密集图数据&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通常随图边数扩展，使其适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见补救措施是通过相似度阈值或距离修剪稀疏化图，但这强制选择单一交互尺度并丢弃其他尺度的重要信息。为克服这一限制，我们引入了多视图图元组框架。我们的图元组框架将图划分为不相交的子图，捕获主要局部相互作用和较弱的远程连接。然后，我们通过受非交换算子理论启发的异构消息传递架构从图元组学习多视图表示，理论上证明这比单图消息传递模型更具表现力，并能保证更低的oracle风险。我们在两个科学领域实现了我们的框架：特征稀缺的库仑矩阵的分子属性预测和几何点云的宇宙学参数推断。在这两种应用中，我们的多视图图元组模型都表现出比单图基线模型更好的性能，突显了我们多视图方法的强大功能和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图神经网络（GNNs）在处理密集图（如点云或分子相互作用）时的效率问题。传统方法需要通过稀疏化图来提高计算效率，但这会强制选择单一交互尺度并丢失其他尺度的重要信息。这个问题在科学应用中尤为重要，因为分子、宇宙学等领域的密集数据包含多种尺度的交互信息，单一尺度方法无法同时捕捉局部强相互作用和全局弱相互作用，导致信息损失和性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了GNNs在密集图上的计算瓶颈和传统稀疏化方法的局限性。他们注意到现有多视图方法和异构图学习方法不适用于具有连续边特征的同构图。受异构图神经网络（如R-GCN、HAN）的启发，作者扩展了这些方法到同构图；借鉴了多视图表示学习的思想，但基于物理交互强度构建视图；受到多尺度GNNs的启发，但使用相同节点集的不同边集。作者还从数学上证明了新框架的理论优势，设计了异构消息传递架构，同时进行视图内和视图间的消息传递，以捕获不同尺度的交互信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将单一密集图分解为多个视图（子图），每个视图捕捉不同尺度的交互，通过异构消息传递架构同时学习这些视图的表示。实现流程包括：1) 构建图元组，将原始图分解为不相交的子图（如强连接图和弱连接图）；2) 进行异构消息传递，包括视图内消息传递（每个子图内计算节点表示）和视图间消息传递（跨子图计算表示）；3) 融合多视图表示，使用可学习的标量权重组合所有视图信息；4) 具体实现分为GINE-Gt（用于一般图）和EGNN-Gt（用于几何数据）两种架构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出多视图图元组表示，将单一密集图分解为多个子图，保留多尺度信息；2) 设计异构消息传递架构，同时进行视图内和视图间的消息传递，考虑操作顺序敏感性；3) 提供理论保证，证明新框架比单图模型更具表现力且风险更低；4) 在分子性质预测和宇宙学参数推断等科学应用中验证了框架的有效性。相比之前工作，不同之处在于：不丢弃任何尺度信息，避免任意选择单一交互尺度；扩展异构图方法到同构图；基于物理交互强度而非自监督构建视图；在相同节点集上定义多个图，避免跨级别对齐的复杂性；不依赖低秩假设，能更好保留原始数据信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个多视图图元组框架，通过分解密集图为多个子图并使用异构消息传递架构同时学习不同尺度的交互，有效解决了图神经网络在密集图上的计算效率与信息保留之间的权衡问题，并在科学应用中展示了优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) typically scale with the number of graph edges,making them well suited for sparse graphs but less efficient on dense graphs,such as point clouds or molecular interactions. A common remedy is to sparsifythe graph via similarity thresholding or distance pruning, but this forces anarbitrary choice of a single interaction scale and discards crucial informationfrom other scales. To overcome this limitation, we introduce a multi-viewgraph-tuple framework. Instead of a single graph, our graph-tuple frameworkpartitions the graph into disjoint subgraphs, capturing primary localinteractions and weaker, long-range connections. We then learn multi-viewrepresentations from the graph-tuple via a heterogeneous message-passingarchitecture inspired by the theory of non-commuting operators, which weformally prove is strictly more expressive and guarantees a lower oracle riskcompared to single-graph message-passing models. We instantiate our frameworkon two scientific domains: molecular property prediction from feature-scarceCoulomb matrices and cosmological parameter inference from geometric pointclouds. On both applications, our multi-view graph-tuple models demonstratebetter performance than single-graph baselines, highlighting the power andversatility of our multi-view approach.</description>
      <author>example@mail.com (Shiyu Chen, Ningyuan, Huang, Soledad Villar)</author>
      <guid isPermaLink="false">2510.10341v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.10097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Gesplat，一个基于3DGS的框架，能够从未配准的稀疏图像中进行鲁棒的新型视图合成和几何一致的3D重建&lt;h4&gt;背景&lt;/h4&gt;NeRF和3DGS已推动3D重建和新型视图合成发展，但严重依赖准确的相机姿态和密集视角覆盖，限制了在稀疏视图场景中的应用&lt;h4&gt;目的&lt;/h4&gt;克服现有方法在稀疏视图场景中的局限性，实现无需准确相机姿态的鲁棒3D重建和视图合成&lt;h4&gt;方法&lt;/h4&gt;利用VGGT基础模型替代COLMAP进行初始姿态估计；采用混合高斯表示结合视图间匹配一致性进行双位置-形状优化；引入图引导的属性细化模块增强场景细节；使用基于流的深度正则化提高深度估计准确性&lt;h4&gt;主要发现&lt;/h4&gt;通过定量和定性实验证明，相比其他无姿态方法，Ges在前向-facing和大规模复杂数据集上实现了更鲁棒的性能&lt;h4&gt;结论&lt;/h4&gt;Gesplat框架能够在稀疏视图条件下实现更鲁棒的3D重建和视图合成，拓展了NeRF和3DGS的应用范围&lt;h4&gt;翻译&lt;/h4&gt;神经辐射场和3D高斯飞溅已经推动了3D重建和新型视图合成的发展，但仍然严重依赖准确的相机姿态和密集的视角覆盖。这些要求限制了它们在稀疏视图场景中的应用，在这些场景中姿态估计变得不可靠且监督不足。为了克服这些挑战，我们引入了Gesplat，一个基于3DGS的框架，能够从未配准的稀疏图像中进行鲁棒的新型视图合成和几何一致的重建。与之前依赖COLMAP进行稀疏点云初始化的工作不同，我们利用VGGT基础模型获得更可靠的初始姿态和密集点云。我们的方法整合了几个关键创新：1) 通过视图间匹配一致性增强的双位置-形状优化的混合高斯表示；2) 增强场景细节的图引导属性细化模块；3) 基于流的深度正则化，提高深度估计准确性以实现更有效的监督。全面的定量和定性实验表明，与其他无姿态方法相比，我们的方法在前向-facing和大规模复杂数据集上实现了更鲁棒的性能&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从稀疏视角且没有相机位姿信息的图像中进行鲁棒的3D重建和新视角合成的问题。这个问题在现实中很重要，因为在实际场景中获取密集、覆盖良好的图像集通常不切实际且成本高昂，而稀疏视角的3D重建在自主导航、VR/AR和机器人技术等应用中至关重要。有限视角导致训练期间监督不足，会造成伪影和有缺陷的重建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有NeRF和3DGS方法在稀疏视角和无位姿设置下的局限性，特别是传统COLMAP方法在稀疏视角下的不可靠性。他们引入VGGT基础模型替代COLMAP进行初始点云和位姿估计，并设计了混合高斯表示结合普通和基于射线的高斯，通过多视图匹配一致性进行优化。作者借鉴了VGGT进行初始场景重建，受[28]启发采用混合高斯表示，使用图神经网络优化属性，并参考[31]的方法利用光流进行深度估计。整个设计思路是在保留3DGS高效性的同时，解决其在稀疏视角下的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入适当的几何先验约束场景结构，同时采用优化和正则化技术细化场景细节。整体流程包括：1)使用VGGT生成初始密集点云和相机位姿；2)采用混合高斯表示(普通高斯和基于射线的高斯)；3)利用多视图匹配一致性进行位置和形状双重优化；4)应用图神经网络优化高斯属性；5)使用基于流的深度正则化提高渲染质量；6)联合优化高斯参数和相机位姿。测试阶段使用训练好的高斯模型细化测试相机位姿。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合高斯表示，结合普通和基于射线的高斯，通过绑定高斯到匹配射线增强多视图一致性；2)图引导属性优化模块，使用图神经网络优化高斯属性；3)基于流的深度正则化，在极线几何框架内通过光流估计可靠深度图；4)使用VGGT替代COLMAP进行初始化，在稀疏视角下更可靠。相比之前工作，Gesplat不依赖密集视角覆盖和已知准确位姿，计算成本更低，能处理更稀疏的输入，且在几何一致性和细节质量上表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Gesplat通过引入混合高斯表示、图引导属性优化和基于流的深度正则化，实现了从稀疏视角无位姿图像中进行鲁棒3D重建和新视角合成，显著提高了场景几何一致性和细节质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced3D reconstruction and novel view synthesis, but remain heavily dependent onaccurate camera poses and dense viewpoint coverage. These requirements limittheir applicability in sparse-view settings, where pose estimation becomesunreliable and supervision is insufficient. To overcome these challenges, weintroduce Gesplat, a 3DGS-based framework that enables robust novel viewsynthesis and geometrically consistent reconstruction from unposed sparseimages. Unlike prior works that rely on COLMAP for sparse point cloudinitialization, we leverage the VGGT foundation model to obtain more reliableinitial poses and dense point clouds. Our approach integrates several keyinnovations: 1) a hybrid Gaussian representation with dual position-shapeoptimization enhanced by inter-view matching consistency; 2) a graph-guidedattribute refinement module to enhance scene details; and 3) flow-based depthregularization that improves depth estimation accuracy for more effectivesupervision. Comprehensive quantitative and qualitative experiments demonstratethat our approach achieves more robust performance on both forward-facing andlarge-scale complex datasets compared to other pose-free methods.</description>
      <author>example@mail.com (Jiahui Lu, Haihong Xiao, Xueyan Zhao, Wenxiong Kang)</author>
      <guid isPermaLink="false">2510.10097v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification</title>
      <link>http://arxiv.org/abs/2510.09367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Minkowski-MambaNet是一种创新的深度学习框架，能够直接从原始LiDAR数据估算森林木材体积和地上生物量，显著提高了森林生物量量化的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的森林生物量量化对碳循环监测至关重要。虽然机载LiDAR在捕捉森林三维结构方面表现出色，但由于难以建模区分树木所需的长程依赖关系，直接从点云估算木材体积和地上生物量(AGB)具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，能够直接从原始LiDAR数据估算体积和AGB，提高森林生物量量化的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出Minkowski-MambaNet，将Mamba模型的选择性状态空间模型(SSM)集成到Minkowski网络中，有效编码全局上下文和长程依赖关系以提高树木区分能力，并融入跳跃连接以增强特征并加速收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在丹麦国家森林清单LiDAR数据上评估，Minkowski-MambaNet显著优于最先进的方法，提供了更准确和稳健的估计。该方法不需要数字地形模型(DTM)，并且对边界伪影具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Minkowski-MambaNet为大规模森林生物量分析提供了强大的工具，推动了基于LiDAR的森林清查的发展。&lt;h4&gt;翻译&lt;/h4&gt;准确的森林生物量量化对碳循环监测至关重要。虽然机载LiDAR在捕捉森林三维结构方面表现出色，但由于难以建模区分树木所需的长程依赖关系，直接从点云估算木材体积和地上生物量(AGB)具有挑战性。我们提出了Minkowski-MambaNet，一种创新的深度学习框架，可直接从原始LiDAR估算体积和AGB。其关键创新是将Mamba模型的选择性状态空间模型(SSM)集成到Minkowski网络中，从而有效编码全局上下文和长程依赖关系，以提高树木区分能力。融入了跳跃连接以增强特征并加速收敛。在丹麦国家森林清单LiDAR数据上评估，Minkowski-MambaNet显著优于最先进的方法，提供了更准确和稳健的估计。重要的是，它不需要数字地形模型(DTM)，并且对边界伪影具有鲁棒性。这项工作为大规模森林生物量分析提供了强大的工具，推动了基于LiDAR的森林清查的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何直接从原始LiDAR点云数据中准确估算森林生物量（包括木材体积和地上生物量AGB）的问题。这个问题非常重要，因为森林是陆地生态系统中最大的碳库，约占全球陆地碳储量的40%，准确量化森林生物量对于全球碳循环监测、气候变化研究和森林管理至关重要。传统方法要么成本高昂、难以大范围应用，要么无法充分捕捉森林的三维结构信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统CNN处理点云效率低，基于局部卷积的方法难以捕获长距离依赖关系，而自注意力机制计算复杂度高。作者借鉴了Minkowski引擎（高效处理稀疏数据）和Mamba状态空间模型（高效序列建模）的工作，以MSENet50为基础骨干网络，设计了两个关键模块：Mamba-SEBottleneck（解决长距离依赖问题）和特征融合修改层（解决多尺度特征利用问题），形成了一个端到端的直接回归框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合Mamba的选择性状态空间模型与Minkowski稀疏卷积，有效捕获点云中的长距离依赖关系和多层次特征，直接从原始LiDAR点云估算森林生物量。实现流程包括：1)数据预处理（保留异质性、排除不一致样本、设置高度阈值）；2)基于MSENet50构建网络架构；3)Mamba-SEBottleneck模块将点特征转换为序列并处理，生成动态注意力权重；4)特征融合修改层通过跳跃连接融合中间层与深层特征；5)训练与评估使用丹麦国家森林调查数据，通过RMSE、R2等指标比较性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Mamba-SEBottleneck模块，将Mamba状态空间模型与Minkowski稀疏卷积结合，以线性复杂度捕获长距离依赖；2)特征融合修改层，通过跳跃连接保留多尺度特征；3)端到端直接回归方法，避免中间步骤。相比之前工作，它比传统方法不依赖手工特征，比基于局部卷积的方法更好地捕获全局结构，比自注意力机制计算效率更高，且无需数字地形模型预处理，对边界噪声更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Minkowski-MambaNet通过创新性地结合Mamba状态空间模型与Minkowski稀疏卷积，首次实现了从原始LiDAR点云中高效准确地直接估算森林生物量，为森林碳汇监测和资源管理提供了强大的新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate forest biomass quantification is vital for carbon cycle monitoring.While airborne LiDAR excels at capturing 3D forest structure, directlyestimating woody volume and Aboveground Biomass (AGB) from point clouds ischallenging due to difficulties in modeling long-range dependencies needed todistinguish trees.We propose Minkowski-MambaNet, a novel deep learningframework that directly estimates volume and AGB from raw LiDAR. Its keyinnovation is integrating the Mamba model's Selective State Space Model (SSM)into a Minkowski network, enabling effective encoding of global context andlong-range dependencies for improved tree differentiation. Skip connections areincorporated to enhance features and accelerate convergence.Evaluated on DanishNational Forest Inventory LiDAR data, Minkowski-MambaNet significantlyoutperforms state-of-the-art methods, providing more accurate and robustestimates. Crucially, it requires no Digital Terrain Model (DTM) and is robustto boundary artifacts. This work offers a powerful tool for large-scale forestbiomass analysis, advancing LiDAR-based forest inventories.</description>
      <author>example@mail.com (Jinxiang Tu, Dayong Ren, Fei Shi, Zhenhong Jia, Yahong Ren, Jiwei Qin, Fang He)</author>
      <guid isPermaLink="false">2510.09367v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes</title>
      <link>http://arxiv.org/abs/2510.09364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VAD-GS，一种针对具有挑战性城市场景的3DGS框架，通过体素可视性推理、多样性感知视图选择和基于补丁匹配的多视立体重建来解决3DGS在无界动态环境中初始化点云不完整导致的几何失真和伪影问题。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian splatting (3DGS)在合成高质量新视角方面表现出色，但其效果严重依赖于初始化点云的质量。在无界、动态的城市环境中，实现场景结构的均匀和完整点覆盖需要重叠的观察视锥，这一假设常常不成立，导致训练的高斯模型出现失真和伪影。&lt;h4&gt;目的&lt;/h4&gt;解决3DGS在无界动态城市环境中因初始化点云不完整导致的几何失真和伪影问题，提高静态和动态对象的几何重建质量。&lt;h4&gt;方法&lt;/h4&gt;提出VAD-GS框架，包含三个关键组件：1) 基于体素的可视性推理识别不可靠的几何结构；2) 通过多样性感知的视图选择选择信息量大的支持视图；3) 通过基于补丁匹配的多视立体重建恢复缺失结构。这种设计能够在缺乏初始点的区域中，由可靠的几何先验引导生成新的高斯基元。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo和nuScenes数据集上的实验表明，VAD-GS优于最先进的3DGS方法，并显著提高了静态和动态对象重建几何的质量。&lt;h4&gt;结论&lt;/h4&gt;VAD-GS框架能够有效解决3DGS在具有挑战性的城市环境中的几何恢复问题，即使在没有初始点的区域也能生成高质量的几何重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射(3DGS)在合成高质量新视角方面展示了令人印象深刻的性能。尽管如此，其有效性严重依赖于初始化点云的质量。具体来说，在底层场景结构上实现均匀和完整的点覆盖需要重叠的观察视锥，这一假设在无界、动态的城市环境中常常被违反。使用部分初始化的点云训练高斯模型通常会导致失真和伪影，因为相机射线可能无法与有效表面相交，导致与被遮挡或不可见几何相关联的高斯基元出现错误的梯度传播。此外，现有的密集化策略只是简单地从现有高斯基元克隆和分割，无法重建缺失的结构。为了解决这些局限性，我们提出了VAD-GS，一种针对具有挑战性的城市场景几何恢复的3DGS框架。我们的方法通过基于体素的可视性推理识别不可靠的几何结构，通过多样性感知的视图选择选择信息量大的支持视图，并通过基于补丁匹配的多视立体重建恢复缺失结构。这种设计使得即使在缺乏初始点的区域中，也能够由可靠的几何先验引导生成新的高斯基元。在Waymo和nuScenes数据集上的大量实验表明，VAD-GS优于最先进的3DGS方法，并显著提高了静态和动态对象重建几何的质量。源代码将在发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯泼溅(3DGS)在动态城市场景中重建完整几何结构的问题。具体来说，3DGS方法的有效性依赖于初始点云质量，但在无边界的动态城市环境中，难以实现均匀且完整的点覆盖，导致训练出的模型出现失真和伪影。这个问题在现实中对自动驾驶系统至关重要，因为它们需要高质量的场景重建来进行模拟和验证，而传统模拟器缺乏场景多样性和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3DGS在动态城市场景中的局限性，特别是初始点云不足导致的几何失真问题。他们注意到现有密集化方法只是简单克隆和分割现有高斯原语，无法处理未初始化区域。作者借鉴了多视图立体视觉(MVS)技术、体素化技术和z-buffer可见性推理，并结合实例分割方法来处理动态对象。通过整合这些技术，他们设计了一个主动评估结构完整性并选择性重建不完整区域的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可见性感知的密集化策略主动评估并重建缺失的几何结构，即使在初始点云不完整的区域也能生成新的高斯原语。整体流程包括：1)对初始点云进行体素化以获得均匀空间密度；2)进行基于体素的可见性推理，识别不可靠几何结构；3)使用多样性感知的视图选择策略选择信息量大的支持视图；4)通过基于块匹配的MVS算法重建深度和法线信息；5)使用这些几何先验指导高斯密集化和优化；6)结合颜色、法线和深度损失进行模型训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对动态城市场景的高斯泼溅框架，主动使用多摄像头、跨帧观测完成缺失几何；2)基于体素表面可见性推理方法，识别不可靠的静态和动态对象几何；3)多样性感知的采样策略，提高MVS重建质量；4)将MVS重建扩展到动态多摄像头驾驶场景。相比之前工作，VAD-GS不局限于现有高斯原语区域，能处理未初始化区域；能处理动态对象而非仅限于静态场景；利用多摄像头和跨帧观测而非仅单摄像头相邻帧；通过可见性推理区分可见和被遮挡几何，避免错误更新被遮挡的高斯原语。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VAD-GS通过可见性感知的密集化策略和多视图立体视觉重建，显著提高了3D高斯泼溅在动态城市场景中的几何质量和渲染保真度，解决了初始点云不完整导致的几何失真问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian splatting (3DGS) has demonstrated impressive performance insynthesizing high-fidelity novel views. Nonetheless, its effectivenesscritically depends on the quality of the initialized point cloud. Specifically,achieving uniform and complete point coverage over the underlying scenestructure requires overlapping observation frustums, an assumption that isoften violated in unbounded, dynamic urban environments. Training Gaussianmodels with partially initialized point clouds often leads to distortions andartifacts, as camera rays may fail to intersect valid surfaces, resulting inincorrect gradient propagation to Gaussian primitives associated with occludedor invisible geometry. Additionally, existing densification strategies simplyclone and split Gaussian primitives from existing ones, incapable ofreconstructing missing structures. To address these limitations, we proposeVAD-GS, a 3DGS framework tailored for geometry recovery in challenging urbanscenes. Our method identifies unreliable geometry structures via voxel-basedvisibility reasoning, selects informative supporting views throughdiversity-aware view selection, and recovers missing structures via patchmatching-based multi-view stereo reconstruction. This design enables thegeneration of new Gaussian primitives guided by reliable geometric priors, evenin regions lacking initial points. Extensive experiments on the Waymo andnuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGSapproaches and significantly improves the quality of reconstructed geometry forboth static and dynamic objects. Source code will be released upon publication.</description>
      <author>example@mail.com (Yikang Zhang, Rui Fan)</author>
      <guid isPermaLink="false">2510.09364v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.09254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从单个人工演示快速生成平滑、次优、无碰撞的三维笛卡尔轨迹的方法，通过结合动态运动基元和强化学习技术，实现了对多种障碍配置的高效轨迹规划。&lt;h4&gt;背景&lt;/h4&gt;基于学习的运动规划虽能快速生成次优轨迹，但通常需要大型训练数据集或昂贵的人类演示收集，限制了其在实际应用中的可行性。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代方法，仅从单个人工演示就能快速生成高质量的三维笛卡尔轨迹，减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;将演示编码为动态运动基元(DMP)，使用基于策略的强化学习迭代重塑DMP以创建多样化轨迹数据集，然后训练神经网络输入障碍物参数并输出相应的DMP参数。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实机器人实验中，该方法在计算时间、执行时间和轨迹长度方面均优于RRT-Connect基线算法，并能支持针对不同障碍几何形状和末端执行器尺寸的多模态轨迹生成。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了传统基于学习的运动规划对大量训练数据的依赖问题，为机器人轨迹规划提供了一种高效、实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于学习的运动规划可以快速生成次优轨迹。然而，它通常需要大型训练数据集或昂贵的人类演示收集工作。本文提出了一种替代方法，从单个人工演示快速生成平滑、次优、无碰撞的三维笛卡尔轨迹。该演示被编码为动态运动基元(DMP)，并使用基于策略的强化学习进行迭代重塑，为不同的障碍物配置创建多样化的轨迹数据集。该数据集用于训练神经网络，输入是从点云自动导出的描述障碍物尺寸和位置的任务参数，输出生成轨迹的DMP参数。该方法在仿真和真实机器人实验中得到验证，在计算和执行时间以及轨迹长度方面优于RRT-Connect基线，同时支持针对不同障碍几何形状和末端执行器尺寸的多模态轨迹生成。视频和实现代码可在https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人快速生成平滑、接近最优的无碰撞轨迹问题，特别是在避障场景中的应用。这个问题在现实中很重要，因为机器人需要在复杂环境中自主导航和操作，快速生成轨迹对于实时应用至关重要，而平滑的轨迹可以减少机械磨损并提高执行效率。此外，减少对大量演示数据的依赖可以降低部署成本，同时能够处理多种障碍物配置和末端执行器尺寸变化，增强了机器人的适应性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基于学习的方法在运动规划中的优势，但注意到它们通常需要大量训练数据。因此，他们想到使用动态运动基元（DMP）来编码和泛化演示轨迹，利用其良好的泛化能力。然后采用策略改进与路径积分（PI²）强化学习算法来迭代调整演示轨迹，生成多样化的避障轨迹数据集。最后将生成的轨迹数据集映射到一个神经网络中，实现快速在线轨迹生成。该方法借鉴了现有工作，包括使用DMP作为运动基元、PI²作为强化学习算法、点云处理技术来检测障碍物，以及参考现有避障方法作为比较基准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单个人工演示作为初始轨迹，通过强化学习迭代调整生成多样化的避障轨迹，训练神经网络将任务参数映射到DMP参数，并从点云中自动提取任务参数以适应新场景。整体实现流程分为两个阶段：1）离线训练阶段：将演示编码为DMP参数，使用PI²算法根据不同成本函数调整参数生成多样化轨迹数据集，训练神经网络将任务参数映射到DMP参数；2）在线执行阶段：从点云中提取任务参数，使用神经网络推断适合当前场景的DMP参数，生成并执行无碰撞轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）数据效率高，只需一个人工演示；2）能自动从点云提取任务参数；3）支持多模态轨迹生成；4）确保轨迹平滑；5）支持多达三个连续任务参数；6）实时性能好，在线生成时间仅0.2秒。相比之前的工作，该方法比传统采样方法（如RRT-Connect）计算更快，轨迹更平滑；比其他基于学习的方法需要更少训练数据；比仅使用PI²优化的方法将计算负担转移到离线阶段；比其他DMP方法支持更多任务参数和更复杂场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合动态运动基元和强化学习的高效避障方法，只需一个人工演示即可快速生成平滑、接近最优的无碰撞轨迹，并能自动适应不同的障碍物配置和末端执行器尺寸。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based motion planning can quickly generate near-optimaltrajectories. However, it often requires either large training datasets orcostly collection of human demonstrations. This work proposes an alternativeapproach that quickly generates smooth, near-optimal collision-free 3DCartesian trajectories from a single artificial demonstration. Thedemonstration is encoded as a Dynamic Movement Primitive (DMP) and iterativelyreshaped using policy-based reinforcement learning to create a diversetrajectory dataset for varying obstacle configurations. This dataset is used totrain a neural network that takes as inputs the task parameters describing theobstacle dimensions and location, derived automatically from a point cloud, andoutputs the DMP parameters that generate the trajectory. The approach isvalidated in simulation and real-robot experiments, outperforming a RRT-Connectbaseline in terms of computation and execution time, as well as trajectorylength, while supporting multi-modal trajectory generation for differentobstacle geometries and end-effector dimensions. Videos and the implementationcode are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2.</description>
      <author>example@mail.com (Dominik Urbaniak, Alejandro Agostini, Pol Ramon, Jan Rosell, Raúl Suárez, Michael Suppa)</author>
      <guid isPermaLink="false">2510.09254v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling</title>
      <link>http://arxiv.org/abs/2510.09088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MambaH-Fit，一种专门用于基于超曲面拟合的点云法线估计的状态空间建模框架。该方法通过注意力驱动的分层特征融合和基于块的状态空间模型，有效解决了现有方法在建模细粒度几何结构方面的不足，显著提高了点云法线估计的准确性、鲁棒性和灵活性。&lt;h4&gt;背景&lt;/h4&gt;现有的点云法线估计方法在建模细粒度几何结构方面存在不足，限制了预测法线的准确性。虽然状态空间模型(特别是Mamba)已显示出强大的建模能力，能够以线性复杂度捕捉长程依赖关系，但现有的基于Mamba的方法主要关注全局形状结构的理解，对局部、细粒度几何细节的建模探索不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模局部、细粒度几何细节的点云法线估计方法，以提高预测法线的准确性、鲁棒性和灵活性，解决现有方法在精细几何结构建模方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;首先提出了一种注意力驱动的分层特征融合(AHFF)方案，用于自适应融合多尺度点云块特征，显著增强局部点云邻域中的几何上下文学习。在此基础上，进一步提出了基于块的状态空间模型(PSSM)，通过状态动力学将点云块建模为隐式超曲面，实现法线预测的有效细粒度几何理解。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，MambaH-Fit方法在准确性、鲁棒性和灵活性方面均优于现有方法。消融研究进一步验证了所提出的AHFF和PSSM组件对方法性能的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;MambaH-Fit通过结合注意力驱动的分层特征融合和基于块的状态空间模型，成功解决了点云法线估计中细粒度几何结构建模的挑战，为点云处理提供了新的思路和方法，具有重要的理论和实践意义。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MambaH-Fit，一种专门用于基于超曲面拟合的点云法线估计的状态空间建模框架。现有的法线估计方法在建模细粒度几何结构方面往往表现不足，从而限制了预测法线的准确性。最近，状态空间模型(SSMs)，特别是Mamba，已经展示了强大的建模能力，能够以线性复杂度捕捉长程依赖关系，并启发了点云处理的适应性方法。然而，现有的基于Mamba的方法主要关注理解全局形状结构，而对局部、细粒度几何细节的建模探索不足。为了解决上述问题，我们首先引入了一种注意力驱动的分层特征融合(AHFF)方案，用于自适应融合多尺度点云块特征，显著增强了局部点云邻域中的几何上下文学习。在此基础上，我们进一步提出了基于块的状态空间模型(PSSM)，通过状态动力学将点云块建模为隐式超曲面，实现了法线预测的有效细粒度几何理解。在基准数据集上的大量实验表明，我们的方法在准确性、鲁棒性和灵活性方面优于现有方法。消融研究进一步验证了所提出组件的贡献。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云法向量估计问题，即从3D点云数据中准确预测每个点的表面法线方向。这个问题在3D视觉领域非常重要，因为准确的法向量是许多应用的基础，包括点云过滤、点云配准和表面重建等。原始点云缺乏连接信息且通常带有噪声，使得法向量估计变得困难，而现有方法在建模细粒度几何结构方面存在不足，限制了预测法向量的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出它们在建模细粒度几何结构方面的不足。他们借鉴了状态空间模型（特别是Mamba）在建模长程依赖关系方面的能力，这种模型最初在自然语言处理领域表现出色。同时，他们参考了HSurf-Net的隐式超表面拟合思想，但发现其残差块结构独立处理点特征，没有明确建模补丁内的点间关系。作者还注意到Transformer的自注意力机制虽然有效，但其二次方计算复杂度不适合处理大规模点云。基于这些观察，他们设计了两个关键模块：注意力驱动的分层特征融合模块和基于补丁的状态空间模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用状态空间模型（特别是Mamba）来建模点云补丁内的隐式超表面，从而更准确地估计点云法向量。方法通过注意力机制自适应地融合多尺度几何特征，并利用Mamba的高效序列建模能力捕捉点云补丁内的长程依赖关系。整体流程包括：1)从点云中提取局部邻域并进行归一化处理；2)使用点特征编码器提取几何特征；3)通过AHFF模块融合多尺度特征；4)使用PSSM模块将点特征作为序列输入，通过Mamba块建模隐式超表面；5)估计点权重并预测法向量；6)对预测结果进行归一化和方向调整。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)注意力驱动的分层特征融合模块，能自适应融合多尺度特征并学习相关几何区域；2)基于补丁的状态空间模型，首次将Mamba应用于点云法向量估计，有效建模补丁内点间关系。相比之前的工作，不同之处在于：不需要预定义多项式阶数（优于n-jet拟合）；明确建模点间关系（优于HSurf-Net的独立点处理）；计算复杂度为线性（优于Transformer的二次复杂度）；专注于局部细粒度几何细节（优于现有Mamba方法的全局关注）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaH-Fit首次将状态空间模型引入点云法向量估计领域，通过创新的注意力驱动的分层特征融合和基于补丁的状态空间模型，显著提高了法向量估计的准确性、鲁棒性和对复杂几何结构的适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MambaH-Fit, a state space modelling framework tailored forhyper-surface fitting-based point cloud normal estimation. Existing normalestimation methods often fall short in modelling fine-grained geometricstructures, thereby limiting the accuracy of the predicted normals. Recently,state space models (SSMs), particularly Mamba, have demonstrated strongmodelling capability by capturing long-range dependencies with linearcomplexity and inspired adaptations to point cloud processing. However,existing Mamba-based approaches primarily focus on understanding global shapestructures, leaving the modelling of local, fine-grained geometric detailslargely under-explored. To address the issues above, we first introduce anAttention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fusemulti-scale point cloud patch features, significantly enhancing geometriccontext learning in local point cloud neighbourhoods. Building upon this, wefurther propose Patch-wise State Space Model (PSSM) that models point cloudpatches as implicit hyper-surfaces via state dynamics, enabling effectivefine-grained geometric understanding for normal prediction. Extensiveexperiments on benchmark datasets show that our method outperforms existingones in terms of accuracy, robustness, and flexibility. Ablation studiesfurther validate the contribution of the proposed components.</description>
      <author>example@mail.com (Weijia Wang, Yuanzhi Su, Pei-Gen Ye, Yuan-Gen Wang, Xuequan Lu)</author>
      <guid isPermaLink="false">2510.09088v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels</title>
      <link>http://arxiv.org/abs/2510.09035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DuNe的双视图框架，用于解决带噪声标签的LiDAR语义分割领域泛化问题，在不同数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确感知对车辆安全至关重要，LiDAR是自动驾驶的关键技术。LiDAR标注常因传感器不完美、遮挡和人为错误而存在噪声，降低分割精度并在领域转移时进一步放大，威胁系统可靠性。点云的稀疏和不规则结构限制了2D噪声学习方法在3D LiDAR分割中的直接应用。&lt;h4&gt;目的&lt;/h4&gt;引入带噪声标签的LiDAR语义分割领域泛化(DGLSS-NL)这一新任务，建立首个基准，并提出有效方法解决现有噪声标签学习方法对LiDAR数据适应性差的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DuNe双视图框架，包含强分支和弱分支，强制执行特征级别的一致性，并基于置信感知的预测过滤应用交叉熵损失。&lt;h4&gt;主要发现&lt;/h4&gt;在10%对称标签噪声下，在SemanticKITTI上达到56.86% mIoU，在nuScenes上达到42.28% mIoU，在SemanticPOSS上达到52.58% mIoU，总体算术平均(AM)为49.57%，调和平均(HM)为48.50%。&lt;h4&gt;结论&lt;/h4&gt;DuNe框架展示了在DGLSS-NL任务中具有强大的领域泛化能力，代码已在项目页面公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的感知对车辆安全至关重要，LiDAR是自动驾驶的关键使能技术。为确保在不同环境、传感器类型和天气条件下的鲁棒性能且无需昂贵的重新标注，基于LiDAR的3D语义分割中的领域泛化是必不可少的。然而，由于传感器不完美、遮挡和人为错误，LiDAR标注通常存在噪声。这种噪声会降低分割精度，并在领域转移时进一步放大，威胁系统可靠性。虽然噪声标签学习在图像中已被广泛研究，但其扩展到领域泛化下的3D LiDAR分割基本上仍未被探索，因为点云的稀疏和不规则结构限制了2D方法的直接使用。为解决这一差距，我们引入了带噪声标签的LiDAR语义分割领域泛化这一新任务，并通过将三种代表性的噪声标签学习策略从图像分类调整到3D分割，建立了首个基准。然而，我们发现现有的噪声标签学习方法对LiDAR数据的适应性较差。因此，我们提出了DuNe，一个具有强分支和弱分支的双视图框架，强制执行特征级别的一致性，并基于置信感知的预测过滤应用交叉熵损失。我们的方法在SemanticKITTI上实现了56.86% mIoU，在nuScenes上实现了42.28%，在SemanticPOSS上实现了52.58%，在10%对称标签噪声下展示了最先进的性能，总体算术平均(AM)为49.57%，调和平均(HM)为48.50%，从而证明了在DGLSS-NL任务中具有强大的领域泛化能力。代码可在我们的项目页面上获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在带有不完美标签（噪声标签）条件下实现激光雷达语义分割的单域泛化问题。这个问题在现实中非常重要，因为自动驾驶安全依赖于准确的3D环境感知，而激光雷达是核心传感器；实际应用中激光雷达标注不可避免地存在噪声，且现有方法大多假设完美标注；噪声标签会降低分割精度，在域转移情况下这种影响会被放大，威胁系统可靠性；同时，重新标注不同环境下的数据成本高昂，域泛化可以避免这一成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有域泛化方法在噪声标签下的局限性，并观察到图像领域的噪声标签学习方法不能直接迁移到3D点云，因为点云是稀疏、不规则和无序的。作者借鉴了图像领域的三种代表性噪声标签学习方法（TCL、DISC、NPN），以及DGLSS框架中的稀疏不变特征一致性和语义相关性一致性，还借鉴了PolarMix数据增强技术。基于这些现有工作，作者设计了DuNe双视图框架，结合了几何感知的强视图和互补的弱视图，通过瓶颈一致性对齐它们，并采用基于置信度过滤的部分和负监督，针对性地解决了点云特性带来的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双视图学习（强视图和弱视图）来处理噪声标签下的域泛化问题。强视图使用几何混合生成增强点云用于构建噪声鲁棒标签，弱视图使用原始和稀疏增强版本形成配对输入并强制执行一致性损失。整体流程包括：1)使用PolarMix将点云增强为强视图和弱视图；2)通过稀疏增强生成四个派生视图；3)使用稀疏卷积网络编码特征；4)在强视图和弱视图中分别生成预测；5)结合DGLSS损失、NPN损失和双视图特征一致性损失进行训练；6)推理时仅使用强分支以提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出DGLSS-NL新任务并建立首个基准；2)设计DuNe双视图框架，结合几何感知视图、一致性学习和噪声感知监督；3)实现噪声感知域泛化，能抵抗标签污染和域转移。相比之前工作，不同之处在于：不同于DGLSS假设完美标注，本文处理噪声标签；不同于图像噪声学习方法，本文针对3D点云特性；不同于大多数3D方法只关注单一问题，本文同时处理域转移和噪声标签；方法设计上创新性地采用双视图、自适应选择策略和多种损失函数结合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了DuNe双视图框架，通过结合几何感知视图、一致性学习和噪声感知监督，首次实现了在噪声标签下具有强域泛化能力的激光雷达语义分割，并建立了首个DGLSS-NL基准，显著提升了自动驾驶感知系统在真实世界复杂环境中的可靠性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception is critical for vehicle safety, with LiDAR as a keyenabler in autonomous driving. To ensure robust performance acrossenvironments, sensor types, and weather conditions without costlyre-annotation, domain generalization in LiDAR-based 3D semantic segmentation isessential. However, LiDAR annotations are often noisy due to sensorimperfections, occlusions, and human errors. Such noise degrades segmentationaccuracy and is further amplified under domain shifts, threatening systemreliability. While noisy-label learning is well-studied in images, itsextension to 3D LiDAR segmentation under domain generalization remains largelyunexplored, as the sparse and irregular structure of point clouds limits directuse of 2D methods. To address this gap, we introduce the novel task DomainGeneralization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)and establish the first benchmark by adapting three representative noisy-labellearning strategies from image classification to 3D segmentation. However, wefind that existing noisy-label learning approaches adapt poorly to LiDAR data.We therefore propose DuNe, a dual-view framework with strong and weak branchesthat enforce feature-level consistency and apply cross-entropy loss based onconfidence-aware filtering of predictions. Our approach shows state-of-the-artperformance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and52.58% on SemanticPOSS under 10% symmetric label noise, with an overallArithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, therebydemonstrating robust domain generalization in DGLSS-NL tasks. The code isavailable on our project page.</description>
      <author>example@mail.com (Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.09035v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion</title>
      <link>http://arxiv.org/abs/2510.09020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  52 pages, 14 figures, 12 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MagicDock是一个创新的前瞻性框架，基于渐进流程和可微表面建模，解决了从头配体设计中的伪从头设计、有限对接建模和不灵活配体类型等限制问题。&lt;h4&gt;背景&lt;/h4&gt;从头配体设计是一项基础任务，旨在从头开始生成能够有效对接蛋白质受体并实现强结合亲和力的蛋白质或分子候选物。它对广泛的生物医学应用具有极其重要的意义。&lt;h4&gt;目的&lt;/h4&gt;解决现有从头配体设计研究中存在的伪从头设计、有限对接建模和不灵活配体类型三大限制问题。&lt;h4&gt;方法&lt;/h4&gt;MagicDock采用精心设计的梯度反转框架，整合受体和配体的通用对接知识；强调对接过程中的可微表面建模，利用可学习的3D点云表示精确捕获结合细节；为不同类型配体引入定制设计并整合到统一框架中。&lt;h4&gt;主要发现&lt;/h4&gt;在9种场景中的广泛实验表明，MagicDock比专门针对蛋白质或分子配体设计的最先进基线方法分别实现了27.1%和11.7%的平均改进。&lt;h4&gt;结论&lt;/h4&gt;MagicDock通过创新的方法解决了从头配体设计中的关键限制，在多种场景中表现出优越的性能，为生物医学应用提供了更有效的配体设计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从头配体设计是一项基础任务，旨在从头开始生成能够有效对接蛋白质受体并实现强结合亲和力的蛋白质或分子候选物。它对广泛的生物医学应用具有极其重要的意义。然而，大多数现有研究受限于伪从头设计、有限的对接建模和不灵活的配体类型。为解决这些问题，我们提出了MagicDock，一个基于渐进流程和可微表面建模的前瞻性框架。我们采用精心设计的梯度反转框架，首先将受体和配体的通用对接知识整合到骨干模型中，然后通过结合预测将对接知识实例化为反向梯度流，迭代指导配体的从头生成。我们强调对接过程中的可微表面建模，利用可学习的3D点云表示来精确捕获结合细节，确保生成的配体通过直接和可解释的空间指纹保持对接有效性。我们为不同类型的配体引入定制设计，并将它们整合到具有灵活触发器的统一梯度反转框架中，确保广泛适用性。此外，我们为MagicDock的每个组件提供严格的理论保证。在9种场景中的广泛实验表明，MagicDock比专门针对蛋白质或分子配体设计的最先进基线方法分别实现了27.1%和11.7%的平均改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; De novo ligand design is a fundamental task that seeks to generate protein ormolecule candidates that can effectively dock with protein receptors andachieve strong binding affinity entirely from scratch. It holds paramountsignificance for a wide spectrum of biomedical applications. However, mostexisting studies are constrained by the \textbf{Pseudo De Novo},\textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. Toaddress these issues, we propose MagicDock, a forward-looking frameworkgrounded in the progressive pipeline and differentiable surface modeling. (1)We adopt a well-designed gradient inversion framework. To begin with, generaldocking knowledge of receptors and ligands is incorporated into the backbonemodel. Subsequently, the docking knowledge is instantiated as reverse gradientflows by binding prediction, which iteratively guide the de novo generation ofligands. (2) We emphasize differentiable surface modeling in the dockingprocess, leveraging learnable 3D point-cloud representations to preciselycapture binding details, thereby ensuring that the generated ligands preservedocking validity through direct and interpretable spatial fingerprints. (3) Weintroduce customized designs for different ligand types and integrate them intoa unified gradient inversion framework with flexible triggers, thereby ensuringbroad applicability. Moreover, we provide rigorous theoretical guarantees foreach component of MagicDock. Extensive experiments across 9 scenariosdemonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\%over SOTA baselines specialized for protein or molecule ligand design,respectively.</description>
      <author>example@mail.com (Zekai Chen, Xunkai Li, Sirui Zhang, Henan Sun, Jia Li, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.09020v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.08849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FOLK的快速开放词汇3D实例分割方法，通过标签引导的知识蒸馏技术，解决了现有方法中因2D遮挡引入的噪声问题，并显著提高了推理速度。&lt;h4&gt;背景&lt;/h4&gt;开放词汇3D实例分割旨在分割和分类超出标注标签空间的实例。现有方法通常将3D实例映射到2D RGB-D图像，然后使用视觉语言模型进行分类，但这种映射策略会引入来自2D遮挡的噪声，并且在推理过程中需要大量计算和内存成本，降低了推理速度。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中由2D遮挡引入的噪声问题，减少计算和内存成本，加速推理过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于标签引导知识蒸馏的快速开放词汇3D实例分割方法(FOLK)。设计一个教师模型提取高质量的实例嵌入，并将其开放词汇知识蒸馏到3D学生模型中。教师模型为每个3D实例生成2D CLIP嵌入，结合可见性和视角多样性，作为蒸馏的学习目标。开发3D学生模型直接为每个3D实例生成3D嵌入。提出标签引导的蒸馏算法，将标签一致的2D嵌入中的开放词汇知识蒸馏到学生模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200和Replica数据集上进行了实验，在ScanNet200数据集上达到了最先进的性能，AP50得分为35.7，比之前的方法运行速度大约快6.0倍到152.2倍。&lt;h4&gt;结论&lt;/h4&gt;FOLK方法有效解决了现有方法中的噪声和计算效率问题，代码将在论文被接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇3D实例分割(Open-vocabulary 3D instance segmentation)：指能够分割和分类训练时未见过类别的3D实例的技术。知识蒸馏(Knowledge distillation)：将大型教师模型的知识转移到小型学生模型的技术。CLIP嵌入：由CLIP模型生成的表示文本和图像之间关系的向量。AP50：在IoU阈值为0.5时的平均精度，常用于评估实例分割性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇3D实例分割中的效率和准确性问题。现有方法通常将3D实例映射到2D图像再进行分类，但这种方式会受到遮挡引入噪声，同时计算量大导致推理速度慢。这个问题在现实中很重要，因为开放词汇3D实例分割能识别训练中未见过的物体类别，这对自动驾驶、机器人导航等需要处理多样化场景的应用至关重要，而现有方法的速度瓶颈限制了这些技术的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D映射引入噪声和计算效率低。他们提出核心思路是通过知识蒸馏将2D视觉语言模型(如CLIP)的知识转移到3D模型中，使3D模型能直接从点云分类。具体设计包括：1)教师模型使用多视图选择和密度引导掩码完成算法生成高质量2D嵌入；2)学生模型直接生成3D嵌入；3)标签引导蒸馏算法确保知识传递质量。作者借鉴了Mask3D用于3D提议生成、CLIP的表示能力、MaskCLIP++的掩码特征提取以及知识蒸馏技术，但进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过标签引导的知识蒸馏，将2D视觉语言模型的知识转移到3D学生模型中，使3D模型能直接从点云提取嵌入并分类，避免2D映射带来的噪声和计算开销。整体流程：1)教师模型阶段：对每个3D实例，选择多视图图像，生成精确掩码，提取高质量2D嵌入；2)学生模型阶段：从点云提取特征，生成3D实例嵌入；3)蒸馏阶段：过滤语义不一致的2D嵌入，通过对比损失和标签损失训练学生模型；4)推理阶段：仅用训练好的3D学生模型直接处理点云，高效生成分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)多视图选择算法：同时考虑可见性和视角多样性，选择代表性图像；2)密度引导掩码完成算法：从稀疏掩码生成精确密集掩码，减少背景噪声；3)标签引导蒸馏算法：过滤语义不一致的嵌入，确保知识质量；4)端到端3D嵌入提取：训练后直接从点云分类，无需2D映射。不同之处：传统方法需将3D映射到2D再分类，易受遮挡影响且计算量大；FOLK直接处理3D数据，避免了噪声问题，推理速度提高了6-152倍，同时保持了高精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出FOLK方法，通过标签引导的知识蒸馏将2D视觉语言模型的知识转移到3D学生模型中，实现了直接从点云进行高效准确的开放词汇3D实例分割，避免了传统2D映射方法的噪声和计算瓶颈问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D instance segmentation seeks to segment and classifyinstances beyond the annotated label space. Existing methods typically map 3Dinstances to 2D RGB-D images, and then employ vision-language models (VLMs) forclassification. However, such a mapping strategy usually introduces noise from2D occlusions and incurs substantial computational and memory costs duringinference, slowing down the inference speed. To address the above problems, wepropose a Fast Open-vocabulary 3D instance segmentation method via Label-guidedKnowledge distillation (FOLK). Our core idea is to design a teacher model thatextracts high-quality instance embeddings and distills its open-vocabularyknowledge into a 3D student model. In this way, during inference, the distilled3D model can directly classify instances from the 3D point cloud, avoidingnoise caused by occlusions and significantly accelerating the inferenceprocess. Specifically, we first design a teacher model to generate a 2D CLIPembedding for each 3D instance, incorporating both visibility and viewpointdiversity, which serves as the learning target for distillation. We thendevelop a 3D student model that directly produces a 3D embedding for each 3Dinstance. During training, we propose a label-guided distillation algorithm todistill open-vocabulary knowledge from label-consistent 2D embeddings into thestudent model. FOLK conducted experiments on the ScanNet200 and Replicadatasets, achieving state-of-the-art performance on the ScanNet200 dataset withan AP50 score of 35.7, while running approximately 6.0x to 152.2x faster thanprevious methods. All codes will be released after the paper is accepted.</description>
      <author>example@mail.com (Hongrui Wu, Zhicheng Gao, Jin Cao, Kelu Yao, Wen Shen, Zhihua Wei)</author>
      <guid isPermaLink="false">2510.08849v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery</title>
      <link>http://arxiv.org/abs/2506.00600v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SatDreamer360的框架，能够从单张卫星图像生成几何上多视角一致的地面全景图，解决了现有方法难以产生多视角一致序列的问题。&lt;h4&gt;背景&lt;/h4&gt;生成多视角一致的360度地面场景卫星影像是一个具有挑战性的任务，在模拟、自主导航和数字孪生城市等领域有广泛应用。现有方法主要专注于合成单个地面全景图，通常依赖高度图或手工制作的投影等辅助输入，难以产生多视角一致的序列。&lt;h4&gt;目的&lt;/h4&gt;提出SatDreamer360框架，从单张卫星图像生成几何上多视角一致的地面全景图，给定预定义的位置轨迹。&lt;h4&gt;方法&lt;/h4&gt;采用三平面表示法编码场景特征；设计基于射线的像素注意力机制从三平面检索特定视角特征；引入全景极线约束注意力模块根据已知相对姿态对齐跨帧特征；扩展VIGOR数据集创建VIGOR++，包含更多地面图像及其姿态标注。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SatDreamer360在卫星到地面对齐和多视角一致性方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SatDreamer360能够有效地从卫星图像生成多视角一致的360度地面场景，解决了卫星与地面图像之间的大视角差异问题。&lt;h4&gt;翻译&lt;/h4&gt;从卫星影像生成多视角一致的360度地面场景是一项具有挑战性的任务，在模拟、自主导航和数字孪生城市等领域有广泛应用。现有方法主要专注于合成单个地面全景图，通常依赖高度图或手工制作的投影等辅助输入，难以产生多视角一致的序列。本文提出SatDreamer360框架，能够从单张卫星图像生成几何上多视角一致的地面全景图，给定预定义的位置轨迹。为解决地面与卫星图像之间的大视角差异问题，我们采用三平面表示法编码场景特征，并设计基于射线的像素注意力机制从三平面中检索特定视角特征。为保持多帧一致性，我们引入全景极线约束注意力模块，根据已知的相对姿态对齐跨帧特征。为支持评估，我们通过增加更多地面图像及其姿态标注扩展了原始VIGOR数据集，创建了VIGOR++数据集。实验表明，SatDreamer360在卫星到地面对齐和多视角一致性方面都优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从卫星图像生成多视角一致的地面全景场景问题。这个问题在现实世界中非常重要，因为它有广泛的应用，包括模拟环境、自动驾驶和数字孪生城市建设。卫星图像覆盖范围广且获取成本低，但与地面视角差异巨大，现有方法难以生成连续、几何一致的地面场景序列，限制了这些应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括依赖辅助输入(如高度图)和难以保证多视角一致性。他们借鉴了三平面表示技术来编码3D场景特征，并从针孔相机的极线约束概念中获得灵感，将其扩展到全景图像。方法设计基于扩散模型，特别是Stable Diffusion 1.5，并添加了两个关键模块：基于射线的像素注意力机制和全景极线约束注意力模块。这些创新使模型能够从单个卫星图像生成连续且几何一致的地面场景序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用三平面表示编码卫星图像中的3D场景特征，并通过基于射线的像素注意力机制从三平面中检索视图特定特征，同时利用全景极线约束注意力模块确保多帧之间的一致性。整体流程是：首先将卫星图像转换为三平面表示；然后为每个地面像素定义3D射线并沿射线采样点；接着从三平面中提取这些点的特征；再利用极线约束对齐不同帧的特征；最后通过扩散模型迭代去噪生成连续的地面全景图像序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一框架SatDreamer360，可从单个卫星图像生成连续地面场景；2) 三平面表示编码场景几何，避免依赖高度图；3) 基于射线的像素注意力机制，实现像素级几何感知；4) 全景极线约束注意力模块，确保多视角一致性；5) 新建VIGOR++数据集。相比之前工作，不同之处在于：不需要辅助输入；只需单个卫星图像；能生成多视角一致序列；显式处理几何一致性；支持大规模场景生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SatDreamer360通过创新的三平面表示和极线约束注意力机制，实现了从单个卫星图像生成多视角一致的地面全景场景，为模拟、自动驾驶和数字孪生城市等应用提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating multiview-consistent $360^\circ$ ground-level scenes fromsatellite imagery is a challenging task with broad applications in simulation,autonomous navigation, and digital twin cities. Existing approaches primarilyfocus on synthesizing individual ground-view panoramas, often relying onauxiliary inputs like height maps or handcrafted projections, and struggle toproduce multiview consistent sequences. In this paper, we proposeSatDreamer360, a framework that generates geometrically consistent multi-viewground-level panoramas from a single satellite image, given a predefined posetrajectory. To address the large viewpoint discrepancy between ground andsatellite images, we adopt a triplane representation to encode scene featuresand design a ray-based pixel attention mechanism that retrieves view-specificfeatures from the triplane. To maintain multi-frame consistency, we introduce apanoramic epipolar-constrained attention module that aligns features acrossframes based on known relative poses. To support the evaluation, we introduce{VIGOR++}, a large-scale dataset for generating multi-view ground panoramasfrom a satellite image, by augmenting the original VIGOR dataset with moreground-view images and their pose annotations. Experiments show thatSatDreamer360 outperforms existing methods in both satellite-to-groundalignment and multiview consistency.</description>
      <author>example@mail.com (Xianghui Ze, Beiyi Zhu, Zhenbo Song, Jianfeng Lu, Yujiao Shi)</author>
      <guid isPermaLink="false">2506.00600v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Language-Centric Omnimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.11693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于多模态大语言模型(MLLMs)并通过对比学习(CL)微调的多模态嵌入方法优越性的根本原因，提出了一种以语言为中心的全模态嵌入框架LCO-Emb，并通过实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;最近基于多模态大语言模型(MLLMs)并通过对比学习(CL)微调的多模态嵌入方法显示出有希望的结果，但它们优越性的根本原因尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;探究MLLM方法优越性的根本原因，并基于此提出一种新的嵌入框架，提高多模态表示的性能。&lt;h4&gt;方法&lt;/h4&gt;通过各向异性和核相似性结构的分析，确认MLLM表示中存在潜在的对齐，使CL能够作为一个轻量级的精炼阶段。基于这一见解，提出了一个以语言为中心的全模态嵌入框架LCO-Emb，并在不同骨干网络和基准上进行了广泛实验。&lt;h4&gt;主要发现&lt;/h4&gt;1. MLLM方法的关键优势来自于生成预训练过程中实现的隐式跨模态对齐；2. 提出了表示能力-生成能力缩放定律(GRSL)，表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关；3. 提供了GRSL的理论解释，正式将MLLM的生成质量与其表示性能的上限联系起来。&lt;h4&gt;结论&lt;/h4&gt;提高生成能力是提升表示质量的有效范式，在CL之前进行持续的生成预训练可以进一步增强模型的嵌入能力潜力。&lt;h4&gt;翻译&lt;/h4&gt;最近利用通过对比学习(CL)微调的多模态大语言模型(MLLMs)的多模态嵌入方法显示出有希望的结果，但它们优越性的根本原因仍未被充分探索。本文认为，基于MLLM方法的关键优势来自于生成预训练过程中实现的隐式跨模态对齐，其中语言解码器学习在共享表示空间中利用多模态信号来生成单模态输出。通过各向异性和核相似性结构的分析，我们经验上确认潜在对齐出现在MLLM表示中，使CL能够作为一个轻量级的精炼阶段。利用这一见解，我们提出了一个以语言为中心的全模态嵌入框架，称为LCO-Emb。在不同骨干网络和基准上的广泛实验证明了其有效性，在各模态上实现了最先进的性能。此外，我们确定了表示能力-生成能力缩放定律(GRSL)，表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关。这表明提高生成能力是提升表示质量的有效范式。我们提供了GRSL的理论解释，正式将MLLM的生成质量与其表示性能的上限联系起来，并在一个具有挑战性的低资源视觉文档检索任务上验证了这一点，表明在CL之前进行持续的生成预训练可以进一步增强模型嵌入能力的潜力。代码、模型和资源可在 https://github.com/LCO-Embedding/LCO-Embedding 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent multimodal embedding approaches leveraging multimodal large languagemodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promisingresults, yet the underlying reasons behind their superiority remainunderexplored. This work argues that a crucial advantage of MLLM-basedapproaches stems from implicit cross-modal alignment achieved during generativepretraining, where the language decoder learns to exploit multimodal signalswithin a shared representation space for generating unimodal outputs. Throughanalysis of anisotropy and kernel similarity structure, we empirically confirmthat latent alignment emerges within MLLM representations, allowing CL to serveas a lightweight refinement stage. Leveraging this insight, we propose aLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensiveexperiments across diverse backbones and benchmarks demonstrate itseffectiveness, achieving state-of-the-art performance across modalities.Furthermore, we identify a Generation-Representation Scaling Law (GRSL),showing that the representational capabilities gained through contrastiverefinement scales positively with the MLLM's generative capabilities. Thissuggests that improving generative abilities evolves as an effective paradigmfor enhancing representation quality. We provide a theoretical explanation ofGRSL, which formally links the MLLM's generative quality to the upper bound onits representation performance, and validate it on a challenging, low-resourcevisual-document retrieval task, showing that continual generative pretrainingbefore CL can further enhance the potential of a model's embeddingcapabilities. Codes, models, and resources are available athttps://github.com/LCO-Embedding/LCO-Embedding.</description>
      <author>example@mail.com (Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong)</author>
      <guid isPermaLink="false">2510.11693v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.11541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于多跳问题检索的新型图表示学习框架，通过多信息级知识图和基于查询的图神经网络解决了RAG系统在处理复杂多跳问题时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成(RAG)能够通过整合外部知识源增强大语言模型的能力，但在处理需要识别多个知识目标形成综合答案的多跳问题时面临新挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种图表示学习框架，解决多跳问题中现有方法难以理解复杂语义结构和易受噪声影响的问题。&lt;h4&gt;方法&lt;/h4&gt;引入多信息级知识图(Multi-L KG)建模不同信息级别，设计基于查询的图神经网络(QSGNN)进行表示学习，采用层内/层间消息传递机制并由查询引导信息聚合，同时提出两种综合数据生成策略用于预训练QSGNN。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该框架在多跳场景中有效，特别是在高跳问题上改进可达33.8%。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能有效解决多跳问题中的挑战，提高RAG系统在复杂问题上的性能。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成(RAG)已证明其通过整合外部知识源增强大语言模型的能力。然而，需要识别多个知识目标以形成综合答案的多跳问题为RAG系统带来了新的挑战。在多跳设置下，现有方法往往难以完全理解具有复杂语义结构的问题，并且在检索多个信息目标时容易受到无关噪声的影响。为解决这些局限性，我们提出了一种用于多跳问题检索的新型图表示学习框架。我们首先引入多信息级知识图(Multi-L KG)来建模不同信息级别，以更全面地理解多跳问题。基于此，我们设计了基于查询的图神经网络(QSGNN)在Multi-L KG上进行表示学习。QSGNN采用层内/层间消息传递机制，每次信息聚合都由查询引导，这不仅促进了多粒度信息聚合，还显著减少了噪声的影响。为增强其学习鲁棒表示的能力，我们进一步提出了两种综合数据生成策略用于预训练QSGNN。广泛的实验结果证明了我们的框架在多跳场景中的有效性，特别是在高跳问题上改进可达33.8%。代码可在以下网址获取：https://github.com/Jerry2398/QSGNN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation (RAG) has demonstrated its ability to enhanceLarge Language Models (LLMs) by integrating external knowledge sources.However, multi-hop questions, which require the identification of multipleknowledge targets to form a synthesized answer, raise new challenges for RAGsystems. Under the multi-hop settings, existing methods often struggle to fullyunderstand the questions with complex semantic structures and are susceptibleto irrelevant noise during the retrieval of multiple information targets. Toaddress these limitations, we propose a novel graph representation learningframework for multi-hop question retrieval. We first introduce aMulti-information Level Knowledge Graph (Multi-L KG) to model variousinformation levels for a more comprehensive understanding of multi-hopquestions. Based on this, we design a Query-Specific Graph Neural Network(QSGNN) for representation learning on the Multi-L KG. QSGNN employsintra/inter-level message passing mechanisms, and in each message passing theinformation aggregation is guided by the query, which not only facilitatesmulti-granular information aggregation but also significantly reduces theimpact of noise. To enhance its ability to learn robust representations, wefurther propose two synthesized data generation strategies for pre-training theQSGNN. Extensive experimental results demonstrate the effectiveness of ourframework in multi-hop scenarios, especially in high-hop questions theimprovement can reach 33.8\%. The code is available at:https://github.com/Jerry2398/QSGNN.</description>
      <author>example@mail.com (Yuchen Yan, Zhihua Liu, Hao Wang, Weiming Li, Xiaoshuai Hao)</author>
      <guid isPermaLink="false">2510.11541v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.11369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为RALI的新算法，用于解决基于推理的图像质量评估模型的高能耗和高延迟问题。通过对比学习直接对齐图像与可泛化文本表示，该方法实现了与基于推理的模型相当的泛化性能，同时显著减少了模型参数和推理时间。&lt;h4&gt;背景&lt;/h4&gt;基于强化学习的推理图像质量评估模型表现出出色的泛化能力，但其背后的机制和关键因素尚未被充分探索。此外，这些模型虽然性能优越，但推理能耗和延迟比早期模型高出几个数量级，限制了它们在特定场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在阐明基于推理的IQA模型泛化能力的来源，并提出一种高效的新算法，以减少模型参数和推理时间，同时保持相当的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;通过大量实验验证，研究揭示了多模态大语言模型(MLLMs)通过强化学习训练，利用推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示，这是泛化能力的来源。基于此，作者提出了RALI算法，采用对比学习直接将图像与通过RL学习到的可泛化文本表示对齐，消除了对推理过程的依赖和加载大语言模型的必要性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过RL训练，MLLMs利用推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示。2. 这种转换是基于推理的IQA模型泛化能力的来源。3. RALI算法通过对比学习直接对齐图像与可泛化文本表示，消除了推理过程的依赖。4. RALI实现了与基于推理的模型相当的泛化性能，同时只需要不到5%的模型参数和推理时间。&lt;h4&gt;结论&lt;/h4&gt;RALI算法成功地解决了基于推理的IQA模型的高能耗和高延迟问题，通过直接对齐图像与文本表示，显著减少了模型复杂度和推理时间，同时保持了相当的泛化性能，为图像质量评估领域提供了一种高效的新方法。&lt;h4&gt;翻译&lt;/h4&gt;基于强化学习训练的推理图像质量评估模型表现出出色的泛化能力，但其背后的机制和关键驱动因素在当前研究中仍未得到充分探索。此外，尽管这些模型性能优越，但它们的推理能耗和延迟比早期模型高出几个数量级，限制了它们在特定场景中的部署。通过大量实验，本文验证并阐述，通过RL训练，多模态大语言模型利用其推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示。这种转换正是这些基于推理的IQA模型所表现出的泛化能力的来源。基于这一基本洞察，我们提出了RALI这一新颖算法，它采用对比学习直接将图像与通过RL学习到的可泛化文本表示对齐。这种方法消除了对推理过程的依赖，甚至不需要加载大语言模型。对于质量评分任务，该框架实现了与基于推理的模型相当的泛化性能，同时只需要不到5%的模型参数和推理时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning-based image quality assessment (IQA) models trained throughreinforcement learning (RL) exhibit exceptional generalization, yet theunderlying mechanisms and critical factors driving this capability remainunderexplored in current research. Moreover, despite their superiorperformance, these models incur inference energy usage and latency orders ofmagnitude higher than their earlier counterparts, restricting their deploymentin specific scenarios. Through extensive experiments, this paper verifies andelaborates that through RL training, MLLMs leverage their reasoning capabilityto convert redundant visual representations into compact, cross-domain alignedtext representations. This conversion is precisely the source of thegeneralization exhibited by these reasoning-based IQA models. Building on thisfundamental insight, we propose a novel algorithm, RALI, which employscontrastive learning to directly align images with these generalizable textrepresentations learned by RL. This approach eliminates the reliance onreasoning processes and even obviates the need to load an LLM. For the qualityscoring task, this framework achieves generalization performance comparable toreasoning-based models while requiring less than 5% of their model parametersand inference time.</description>
      <author>example@mail.com (Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang)</author>
      <guid isPermaLink="false">2510.11369v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data</title>
      <link>http://arxiv.org/abs/2510.11321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 39th Conference on Neural Information Processing Systems  (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自监督框架，用于学习层次化的操作概念，通过跨模态感官相关性和多级时间抽象捕捉不变的操作模式，无需人工标注。结合跨模态相关网络和多时间尺度预测器，使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。实验证明概念增强策略在模拟和实际环境中表现显著提升，学习到的概念类似于人类可解释的操作基元。&lt;h4&gt;背景&lt;/h4&gt;机器人操作中的有效泛化需要能够捕捉环境和任务间不变交互模式的表示。传统的机器人学习方法通常需要大量人工标注，且难以在不同环境和任务间有效迁移。&lt;h4&gt;目的&lt;/h4&gt;开发一种自监督学习方法，使机器人能够学习层次化的操作概念，捕捉跨环境和任务的不变交互模式，无需人工标注，从而提高机器人在复杂场景中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出结合两种主要组件的自监督框架：1) 跨模态相关网络：识别跨感官模态的持久模式；2) 多时间尺度预测器：在不同时间尺度上组织表示层次结构。这种双重结构使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。&lt;h4&gt;主要发现&lt;/h4&gt;1) 概念增强的策略在模拟基准测试和实际部署中表现出显著的性能改进；2) 学习到的概念类似于人类可解释的操作基元，尽管没有接受语义监督；3) 该框架能够捕捉跨环境和任务的不变操作模式。&lt;h4&gt;结论&lt;/h4&gt;这项研究不仅推进了对操作表示学习的理解，还为在复杂场景中增强机器人性能提供了实用方法。通过自监督学习层次化操作概念，机器人能够在无需人工标注的情况下实现更好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作中的有效泛化需要能够捕捉环境和任务间不变交互模式的表示。我们提出了一个自监督框架，用于学习层次化的操作概念，这些概念通过跨模态感官相关性和多级时间抽象来编码这些不变模式，无需人工标注。我们的方法结合了跨模态相关网络（识别跨感官模态的持久模式）和多时间尺度预测器（在不同时间尺度上组织表示层次结构）。通过这种双重结构学习到的操作概念，使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。在模拟基准测试和实际部署中的经验评估表明，我们的概念增强策略具有显著的性能改进。分析显示，学习到的概念类似于人类可解释的操作基元，尽管没有接受语义监督。这项工作不仅推进了对操作表示学习的理解，还为在复杂场景中增强机器人性能提供了实用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective generalization in robotic manipulation requires representationsthat capture invariant patterns of interaction across environments and tasks.We present a self-supervised framework for learning hierarchical manipulationconcepts that encode these invariant patterns through cross-modal sensorycorrelations and multi-level temporal abstractions without requiring humanannotation. Our approach combines a cross-modal correlation network thatidentifies persistent patterns across sensory modalities with a multi-horizonpredictor that organizes representations hierarchically across temporal scales.Manipulation concepts learned through this dual structure enable policies tofocus on transferable relational patterns while maintaining awareness of bothimmediate actions and longer-term goals. Empirical evaluation across simulatedbenchmarks and real-world deployments demonstrates significant performanceimprovements with our concept-enhanced policies. Analysis reveals that thelearned concepts resemble human-interpretable manipulation primitives despitereceiving no semantic supervision. This work advances both the understanding ofrepresentation learning for manipulation and provides a practical approach toenhancing robotic performance in complex scenarios.</description>
      <author>example@mail.com (Ruizhe Liu, Pei Zhou, Qian Luo, Li Sun, Jun Cen, Yibing Song, Yanchao Yang)</author>
      <guid isPermaLink="false">2510.11321v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2510.11084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 Figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CDRL4AD的因果解缠结表示学习方法，用于在多元时间序列中检测异常并识别因果关系，解决了传统方法无法在不同时间段明确推断因果关系的问题。&lt;h4&gt;背景&lt;/h4&gt;在多元时间序列分析中，数据变量之间的动态交互随时间变化，使因果关系的解释变得复杂。传统方法在无监督设置中假设变量间的统计独立性，而最近的方法通过图表示学习捕获特征相关性，但这些表示无法在不同时间段明确推断因果关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测异常并识别其因果关系的方法，特别是在多元时间序列数据中，解决现有方法无法明确推断不同时间段因果关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出CDRL4AD（用于异常检测的因果解缠结表示学习）方法，设计因果过程作为模型输入（包括时间异质图和因果关系），使表示能够识别不同时间段的因果关系并解缠结潜在变量以推断相应的因果因子。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的实验表明，CDRL4AD在准确性和根本原因分析方面优于最先进的方法；模型分析验证了超参数敏感性和CDRL4AD的时间复杂度；案例研究展示了该方法如何帮助人类专家诊断异常的根本原因。&lt;h4&gt;结论&lt;/h4&gt;CDRL4AD方法有效解决了多元时间序列中异常检测和因果关系推断的挑战，为异常诊断提供了更准确的工具，并能帮助人类专家理解异常的根本原因。&lt;h4&gt;翻译&lt;/h4&gt;解缠结复杂的因果关系对于准确检测异常很重要。在多元时间序列分析中，数据变量之间的动态交互随时间变化，使因果关系的解释变得复杂。传统方法在无监督设置中假设变量间的统计独立性，而最近的方法通过图表示学习捕获特征相关性。然而，这些表示无法在不同时间段明确推断因果关系。为解决这个问题，我们提出了用于异常检测的因果解缠结表示学习（CDRL4AD），用于在多元时间序列中检测异常并识别其因果关系。首先，我们将因果过程设计为模型输入，包括时间异质图和因果关系。其次，我们的表示能够识别不同时间段的因果关系，并解缠结潜在变量以推断相应的因果因子。第三，我们在真实世界数据集上的实验表明，CDRL4AD在准确性和根本原因分析方面优于最先进的方法。第四，我们的模型分析验证了超参数敏感性和CDRL4AD的时间复杂度。最后，我们进行了案例研究，展示我们的方法如何帮助人类专家诊断异常的根本原因。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Disentangling complex causal relationships is important for accuratedetection of anomalies. In multivariate time series analysis, dynamicinteractions among data variables over time complicate the interpretation ofcausal relationships. Traditional approaches assume statistical independencebetween variables in unsupervised settings, whereas recent methods capturefeature correlations through graph representation learning. However, theirrepresentations fail to explicitly infer the causal relationships overdifferent time periods. To solve the problem, we propose Causally DisentangledRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies andidentify their causal relationships in multivariate time series. First, wedesign the causal process as model input, the temporal heterogeneous graph, andcausal relationships. Second, our representation identifies causalrelationships over different time periods and disentangles latent variables toinfer the corresponding causal factors. Third, our experiments on real-worlddatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in termsof accuracy and root cause analysis. Fourth, our model analysis validateshyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conducta case study to show how our approach assists human experts in diagnosing theroot causes of anomalies.</description>
      <author>example@mail.com (Wonah Kim, Jeonghyeon Park, Dongsan Jun, Jungkyu Han, Sejin Chun)</author>
      <guid isPermaLink="false">2510.11084v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction</title>
      <link>http://arxiv.org/abs/2510.11066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了解耦多模态融合(DMF)方法，通过模态增强建模策略实现基于ID的协同表示和多模态表示的细粒度交互，用于用户兴趣建模。DMF构建目标感知特征桥接不同嵌入空间，设计推理优化注意力机制减轻计算瓶颈，并全面整合多模态表示。实验证明DMF有效，已在Lazada平台部署并取得显著业务指标提升。&lt;h4&gt;背景&lt;/h4&gt;现代工业推荐系统通过整合预训练模型的多模态表示到基于ID的点击率预测框架中来提高推荐性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法捕获内容语义和行为信号之间细粒度交互的问题，提高推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;提出解耦多模态融合(DMF)方法，包括：1)构建目标感知特征桥接不同嵌入空间的语义差距；2)设计推理优化注意力机制解耦计算；3)结合模态中心和模态增强建模策略下的用户兴趣表示实现全面多模态整合。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和工业数据集上的离线实验证明了DMF的有效性；在Lazada平台部署后，CTCVR提升5.30%，GMV提升7.43%，且计算开销可忽略不计。&lt;h4&gt;结论&lt;/h4&gt;DMF通过模态增强建模策略有效解决了现有方法无法捕获细粒度交互的问题，显著提升了推荐系统性能，且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;现代工业推荐系统通过将预训练模型的多模态表示整合到基于ID的点击率预测框架中来提高推荐性能。然而，现有方法通常采用模态中心建模策略，独立处理基于ID和多模态的嵌入，无法捕获内容语义和行为信号之间的细粒度交互。本文提出了解耦多模态融合(DMF)，它引入了模态增强建模策略，使基于ID的协同表示和多模态表示能够进行细粒度交互，用于用户兴趣建模。具体而言，我们构建目标感知特征来桥接不同嵌入空间之间的语义差距，并将其作为辅助信息来增强用户兴趣建模的效果。此外，我们设计了一种推理优化的注意力机制，在注意力层之前解耦目标感知特征和基于ID的嵌入的计算，从而减轻了引入目标感知特征带来的计算瓶颈。为了实现全面的多模态整合，DMF结合了在模态中心和模态增强建模策略下学习到的用户兴趣表示。在公共和工业数据集上的离线实验证明了DMF的有效性。此外，DMF已被部署在跨境电商平台Lazada的产品推荐系统上，实现了CTCVR提升5.30%和GMV提升7.43%，且计算开销可忽略不计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern industrial recommendation systems improve recommendation performanceby integrating multimodal representations from pre-trained models into ID-basedClick-Through Rate (CTR) prediction frameworks. However, existing approachestypically adopt modality-centric modeling strategies that process ID-based andmultimodal embeddings independently, failing to capture fine-grainedinteractions between content semantics and behavioral signals. In this paper,we propose Decoupled Multimodal Fusion (DMF), which introduces amodality-enriched modeling strategy to enable fine-grained interactions betweenID-based collaborative representations and multimodal representations for userinterest modeling. Specifically, we construct target-aware features to bridgethe semantic gap across different embedding spaces and leverage them as sideinformation to enhance the effectiveness of user interest modeling.Furthermore, we design an inference-optimized attention mechanism thatdecouples the computation of target-aware features and ID-based embeddingsbefore the attention layer, thereby alleviating the computational bottleneckintroduced by incorporating target-aware features. To achieve comprehensivemultimodal integration, DMF combines user interest representations learnedunder the modality-centric and modality-enriched modeling strategies. Offlineexperiments on public and industrial datasets demonstrate the effectiveness ofDMF. Moreover, DMF has been deployed on the product recommendation system ofthe international e-commerce platform Lazada, achieving relative improvementsof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.</description>
      <author>example@mail.com (Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang)</author>
      <guid isPermaLink="false">2510.11066v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Instruction-aware User Embedding via Synergistic Language and Representation Modeling</title>
      <link>http://arxiv.org/abs/2510.11016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InstructUE是一种指令感知的用户嵌入基础模型，利用大型语言模型生成通用和指令感知的用户表示，通过多编码器架构和对比-自回归训练框架解决了现有方法在跨领域泛化和噪声敏感性方面的问题。&lt;h4&gt;背景&lt;/h4&gt;用户表示建模对个性化应用日益重要，但现有方法在跨领域泛化能力和对噪声行为信号的敏感性方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出InstructUE模型，利用大型语言模型生成通用和指令感知的用户表示，提高用户建模的泛化能力和噪声鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入多编码器架构配备轻量级适配器处理异构数据；提出对比-自回归训练框架，通过UserQA数据集连接语言和表示空间，同时利用自回归学习捕获领域知识和对比学习对齐用户-文本嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;通过现实世界应用的广泛实验，证明InstructUE在用户预测、营销和推荐等多个场景中显著优于现有方法，实现了指令引导的用户信息去噪。&lt;h4&gt;结论&lt;/h4&gt;指令感知的用户建模可有效实现特定场景下用户信息的指令引导去噪，为更具泛化性和鲁棒性的用户表示学习铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;用户表示建模已成为个性化应用中日益重要的环节，然而现有方法在跨领域泛化能力和对噪声行为信号的敏感性方面存在挑战。我们提出了InstructUE，一种指令感知的用户嵌入基础模型，它利用大型语言模型生成通用且具有指令感知能力的用户表示。InstructUE引入了一个多编码器架构，配备轻量级适配器，能够高效处理来自六个不同来源的异构数据，同时保留其结构特征。此外，它提出了一种新颖的对比-自回归训练框架，通过精心策划的UserQA数据集连接语言和表示空间。该对比-自回归训练框架同时利用自回归学习捕获语言空间中的领域知识，以及对比学习对齐表示空间中的用户-文本嵌入，从而增强了用户嵌入的指令感知能力和噪声鲁棒性。通过在现实世界应用中的广泛实验，我们证明InstructUE在用户预测、营销和推荐等多个场景中显著优于现有方法。我们的结果表明，指令感知的用户建模可以在特定场景下有效实现用户信息的指令引导去噪，为更具泛化性和鲁棒性的用户表示学习铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User representation modeling has become increasingly crucial for personalizedapplications, yet existing approaches struggle with generalizability acrossdomains and sensitivity to noisy behavioral signals. We present InstructUE, aninstruction-aware user embedding foundation model that leverages large languagemodels (LLMs) to generate general and instruction-aware user representations.InstructUE introduces a multi-encoder architecture with a lightweight adapterthat efficiently processes heterogeneous data from six different sources whilepreserving their structural characteristics. Additionally, it proposes a novelcontrastive-autoregressive training framework that bridges language andrepresentation spaces through a curated UserQA dataset. Thecontrastive-autoregressive training framework simultaneously leveragesautoregressive learning to capture domain knowledge in language space andcontrastive learning to align user-text embeddings in representation space,thereby enhancing the instruction-awareness and noise-robustness of userembeddings. Through extensive experiments on real-world applications, wedemonstrate that InstructUE significantly outperforms existing methods acrossmultiple domains including user prediction, marketing, and recommendationscenarios. Our results show that instruction-aware user modeling caneffectively achieve instruction-guided denoising of user information inspecific scenarios, paving the way for more generalizable and robust userrepresentation learning.</description>
      <author>example@mail.com (Ziyi Gao, Yike Xu, Jiahao Yuan, Baokun Wang, Jinyong Wen, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie)</author>
      <guid isPermaLink="false">2510.11016v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unify Variables in Neural Scaling Laws for General Audio Representations via Embedding Effective Rank</title>
      <link>http://arxiv.org/abs/2510.10948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统探讨了通用音频表示学习中的缩放定律，引入嵌入有效秩（RankMe）作为统一指标，揭示了RankMe与表示质量之间的幂律关系，为音频基础模型的缩放策略提供了理论依据和实践框架。&lt;h4&gt;背景&lt;/h4&gt;缩放定律在计算机视觉和自然语言处理中对模型性能的理解有深远影响，但在通用音频表示学习中的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究通用音频表示学习中的缩放定律，探索如何评估和预测模型性能。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入有效秩（RankMe）作为统一指标，封装各种变量对表示质量的影响，在广泛的超参数空间（包括模型大小、训练数据量、计算预算、架构配置等）中检查缩放行为。&lt;h4&gt;主要发现&lt;/h4&gt;实证研究表明RankMe与表示质量之间存在一致的关系，表明嵌入有效秩可作为评估和预测音频表示学习中模型性能的可靠代理。&lt;h4&gt;结论&lt;/h4&gt;验证了经典缩放原理适用于通用音频领域，为音频基础模型的未来模型缩放策略提供了理论依据和经验上稳健的框架。&lt;h4&gt;翻译&lt;/h4&gt;缩放定律已经深刻地改变了我们在计算机视觉和自然语言处理中对模型性能的理解，但它们在通用音频表示学习中的应用仍然探索不足。一个关键挑战在于通用音频表示的多因素性质——表示质量受到音频长度、嵌入维度、模型深度、模型架构、数据量等多种变量的共同影响，其中许多变量难以分离或用分析方式表达。在这项工作中，我们通过使用嵌入有效秩（RankMe）作为统一指标，系统地研究了通用音频表示的缩放定律，该指标封装了各种变量对表示质量的影响。RankMe实现了对音频嵌入的无标签、信息论量化，使我们能够检查包括模型大小、训练数据量、计算预算、架构配置等在内的广泛超参数空间中的缩放行为。我们的实证发现揭示了RankMe与表示质量之间的一致的幂律关系，表明嵌入有效秩可作为评估和预测音频表示学习中模型性能的可靠代理。这项工作不仅验证了经典缩放原理适用于通用音频领域，还为音频基础模型的未来模型缩放策略提供了理论依据和经验上稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling laws have profoundly shaped our understanding of model performance incomputer vision and natural language processing, yet their application togeneral audio representation learning remains underexplored. A key challengelies in the multifactorial nature of general audiorepresentation-representation quality is jointly influenced by variables suchas audio length, embedding dimensionality, model depth, model architecture,data volume, etc., many of which are difficult to isolate or expressanalytically. In this work, we present a systematic study of scaling laws forgeneral audio representations by utilizing embedding effective rank (RankMe) asa unifying metric that encapsulates the impact of diverse variables onrepresentation quality. RankMe enables a label-free, information-theoreticquantification of audio embeddings, allowing us to examine scaling behaviorsacross a wide hyper-parameter space, including model size, training datavolume, computational budget, architectural configurations, etc. Our empiricalfindings reveal a consistent power-law relationship between RankMe andrepresentation quality, suggesting that embedding effective rank serves as areliable proxy for assessing and predicting model performance in audiorepresentation learning. This work not only validates the applicability ofclassical scaling principles to the general audio domain but also offers atheoretically grounded and empirically robust framework for guiding futuremodel scaling strategies in audio foundation models.</description>
      <author>example@mail.com (Xuyao Deng, Yanjie Sun, Yong Dou, Kele Xu)</author>
      <guid isPermaLink="false">2510.10948v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition</title>
      <link>http://arxiv.org/abs/2510.10927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为GapDNER的新型模型，用于生物医学领域中的不连续命名实体识别，通过关注实体片段间的上下文间隙解决了传统方法中的错误传播和解码歧义问题。&lt;h4&gt;背景&lt;/h4&gt;在生物医学领域，一个命名实体可能由一系列不相邻的标记组成，并与其他实体重叠。先前的方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临着错误传播和解码歧义的挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，作者深入探索不连续实体的结构，并提出一种有效的间隙感知网格标记模型（GapDNER）来提升不连续命名实体识别的性能。&lt;h4&gt;方法&lt;/h4&gt;GapDNER创新性地在实体片段间的上下文间隙上应用表示学习，将上下文间隙视为额外跨度类型，将跨度分类转换为标记对网格标记任务。设计了两个交互组件：内部跨度规则提取模块使用双仿射机制和线性注意力捕获每个跨度的内部规则；跨跨度关系增强模块利用交叉交叉注意力获取不同跨度间的语义关系。在推理阶段，为每个实体片段和上下文间隙分配有向边，使用BFS算法搜索有效路径。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的实验结果表明，GapDNER在不连续NER方面取得了新的最先进性能，并且在识别复杂实体结构方面表现出显著优势。&lt;h4&gt;结论&lt;/h4&gt;GapDNER模型通过创新地处理上下文间隙和设计专门的组件来建模实体间关系，有效解决了不连续命名实体识别中的挑战，显著提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;在生物医学领域，一个命名实体可能由一系列不相邻的标记组成，并与其他实体重叠。先前的方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临着错误传播和解码歧义的挑战。为了解决这些问题，我们深入探索了不连续实体的结构，并提出了一种有效的间隙感知网格标记模型用于不连续命名实体识别，称为GapDNER。我们的GapDNER创新性地在实体片段之间的上下文间隙上应用表示学习，以解决解码歧义并增强不连续NER性能。具体来说，我们将上下文间隙视为额外的跨度类型，并将跨度分类转换为标记对网格标记任务。随后，我们设计了两个交互组件，从内部和跨跨度两个角度全面建模标记对网格特征。内部跨度规则提取模块使用双仿射机制和线性注意力来捕获每个跨度的内部规则，而跨跨度关系增强模块则利用交叉交叉注意力来获取不同跨度之间的语义关系。在实体解码的推理阶段，我们为每个实体片段和上下文间隙分配有向边，然后使用BFS算法搜索网格中从头部到尾部的所有带有实体标签的有效路径。在三个数据集上的实验结果表明，我们的GapDNER在不连续NER方面取得了新的最先进性能，并且在识别复杂实体结构方面表现出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In biomedical fields, one named entity may consist of a series ofnon-adjacent tokens and overlap with other entities. Previous methods recognizediscontinuous entities by connecting entity fragments or internal tokens, whichface challenges of error propagation and decoding ambiguity due to the widevariety of span or word combinations. To address these issues, we deeplyexplore discontinuous entity structures and propose an effective Gap-aware gridtagging model for Discontinuous Named Entity Recognition, named GapDNER. OurGapDNER innovatively applies representation learning on the context gapsbetween entity fragments to resolve decoding ambiguity and enhancediscontinuous NER performance. Specifically, we treat the context gap as anadditional type of span and convert span classification into a token-pair gridtagging task. Subsequently, we design two interactive components tocomprehensively model token-pair grid features from both intra- and inter-spanperspectives. The intra-span regularity extraction module employs the biaffinemechanism along with linear attention to capture the internal regularity ofeach span, while the inter-span relation enhancement module utilizescriss-cross attention to obtain semantic relations among different spans. Atthe inference stage of entity decoding, we assign a directed edge to eachentity fragment and context gap, then use the BFS algorithm to search for allvalid paths from the head to tail of grids with entity tags. Experimentalresults on three datasets demonstrate that our GapDNER achieves newstate-of-the-art performance on discontinuous NER and exhibits remarkableadvantages in recognizing complex entity structures.</description>
      <author>example@mail.com (Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen)</author>
      <guid isPermaLink="false">2510.10927v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Topological Alignment of Shared Vision-Language Embedding Space</title>
      <link>http://arxiv.org/abs/2510.10889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures, 19 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ToMCLIP的拓扑感知框架，用于改进多语言视觉语言模型的跨模态对齐，通过拓扑保持约束增强嵌入空间的结构连贯性，提高零样本准确率和多语言检索性能。&lt;h4&gt;背景&lt;/h4&gt;对比视觉语言模型(VLMs)展示了强大的零样本能力，但由于有限的多语言多模态数据，其跨模态对齐偏向英语。现有多语言扩展虽缓解了这一问题，但仅关注实例级对齐，忽略了共享嵌入空间的几何结构。&lt;h4&gt;目的&lt;/h4&gt;解决多语言视觉语言模型中嵌入空间的全局几何结构对齐问题，通过拓扑感知方法提升多语言表示的结构连贯性和性能。&lt;h4&gt;方法&lt;/h4&gt;提出ToMCLIP框架，应用持续同调定义拓扑对齐损失，并利用图稀疏化策略近似持久图，确保在理论误差边界内实现拓扑保持的嵌入空间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;ToMCLIP增强了多语言表示的结构连贯性，在CIFAR-100上提高了零样本准确率，在xFlickr&amp;CO上增强了多语言检索性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的拓扑对齐方法不仅适用于视觉语言模型，还为表示学习中融入拓扑对齐提供了通用方法。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉语言模型(VLMs)已经展示了强大的零样本能力。然而，由于有限的多语言多模态数据，它们的跨模态对齐仍然偏向英语。最近的多语言扩展缓解了这一差距，但强制执行实例级对齐，同时忽略了共享嵌入空间的几何结构。我们通过引入ToMCLIP(多语言CLIP的拓扑对齐)，一种拓扑感知的框架，使用拓扑保持约束对齐嵌入空间，来解决这一问题。所提出的方法应用持续同调来定义拓扑对齐损失，并使用图稀疏化策略近似持久图，具有理论误差边界。这项工作验证了所提出的方法，展示了多语言表示增强的结构连贯性，在CIFAR-100上更高的零样本准确率，以及在xFlickr&amp;CO上更强的多语言检索性能。除了VLMs，所提出的方法为在表示学习中融入拓扑对齐提供了通用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shotcapabilities. However, their cross-modal alignment remains biased towardEnglish due to limited multilingual multimodal data. Recent multilingualextensions have alleviated this gap but enforce instance-level alignment whileneglecting the global geometry of the shared embedding space. We address thisproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), atopology-aware framework aligning embedding spaces with topology-preservingconstraints. The proposed method applies persistent homology to define atopological alignment loss and approximates persistence diagram withtheoretical error bounds using graph sparsification strategy. This workvalidates the proposed approach, showing enhanced structural coherence ofmultilingual representations, higher zero-shot accuracy on the CIFAR-100, andstronger multilingual retrieval performance on the xFlickr&amp;CO. Beyond VLMs, theproposed approach provides a general method for incorporating topologicalalignment into representation learning.</description>
      <author>example@mail.com (Junwon You, Dasol Kang, Jae-Hun Jung)</author>
      <guid isPermaLink="false">2510.10889v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning</title>
      <link>http://arxiv.org/abs/2510.10642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniCoD模型，结合了理解、规划和连续未来表示学习的优势，通过大规模预训练和微调，显著提升了机器人策略学习在多样化任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;构建能够在开放环境中处理多样化任务的全能机器人策略是机器人领域的核心挑战。现有方法通常基于视觉语言理解模型或生成模型，但语义理解和视觉动力学建模对具身机器人都至关重要。&lt;h4&gt;目的&lt;/h4&gt;利用最近出现的统一生成和理解模型的优势，结合理解、规划和连续未来表示学习，提升机器人策略学习的效果。&lt;h4&gt;方法&lt;/h4&gt;提出UniCoD模型，通过在超过100万个互联网规模的 instructional manipulation 视频上进行预训练，获得动态建模高维视觉特征的能力，然后在机器人具身收集的数据上进行微调，学习从预测表征到动作令牌的映射。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在模拟环境和真实世界分布外任务中，分别比基线方法高出9%和12%的性能。&lt;h4&gt;结论&lt;/h4&gt;UniCoD通过结合理解、规划和连续未来表示学习的优势，在机器人策略学习方面取得了显著的性能提升，为构建全能机器人策略提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在开放环境中处理多样化任务的全能机器人策略是机器人领域的中心挑战。为了利用大规模预训练的知识，先前的工作通常在视觉语言理解模型(VLMs)或生成模型的基础上构建全能策略。然而，来自视觉语言预训练的语义理解和来自视觉生成预训练的视觉动力学建模对具身机器人都至关重要。最近的统一生成和理解模型通过大规模预训练在理解和生成方面都展示了强大的能力。我们认为机器人策略学习同样可以从理解、规划和连续未来表示学习的综合优势中受益。基于这一见解，我们引入了UniCoD，它通过在超过100万个互联网规模的 instructional manipulation 视频上进行预训练，获得动态建模高维视觉特征的能力。随后，UniCoD在从机器人具身收集的数据上进行微调，实现了从预测表征到动作令牌的映射学习。大量实验表明，我们的方法在模拟环境和真实世界分布外任务中，始终比基线方法高出9%和12%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building generalist robot policies that can handle diverse tasks inopen-ended environments is a central challenge in robotics. To leverageknowledge from large-scale pretraining, prior work has typically builtgeneralist policies either on top of vision-language understanding models(VLMs) or generative models. However, both semantic understanding fromvision-language pretraining and visual dynamics modeling from visual-generationpretraining are crucial for embodied robots. Recent unified models ofgeneration and understanding have demonstrated strong capabilities in bothcomprehension and generation through large-scale pretraining. We posit thatrobotic policy learning can likewise benefit from the combined strengths ofunderstanding, planning and continuous future representation learning. Buildingon this insight, we introduce UniCoD, which acquires the ability to dynamicallymodel high-dimensional visual features through pretraining on over 1Minternet-scale instructional manipulation videos. Subsequently, UniCoD isfine-tuned on data collected from the robot embodiment, enabling the learningof mappings from predictive representations to action tokens. Extensiveexperiments show our approach consistently outperforms baseline methods interms of 9\% and 12\% across simulation environments and real-worldout-of-distribution tasks.</description>
      <author>example@mail.com (Jianke Zhang, Yucheng Hu, Yanjiang Guo, Xiaoyu Chen, Yichen Liu, Wenna Chen, Chaochao Lu, Jianyu Chen)</author>
      <guid isPermaLink="false">2510.10642v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</title>
      <link>http://arxiv.org/abs/2510.10604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FusionGen的新型EEG数据生成框架，通过解耦表征学习和特征融合技术解决脑机接口领域的数据稀缺和受试者间变异性问题，显著提高了EEG解码模型的分类准确性。&lt;h4&gt;背景&lt;/h4&gt;脑机接口(BCIs)通过脑电图(EEG)在大脑和外部设备间建立直接通信，应用范围从医疗康复到认知状态评估。然而，基于EEG的BCI受到数据稀缺和显著受试者间变异性的严重限制，阻碍了EEG解码模型在实际环境中的泛化和应用。&lt;h4&gt;目的&lt;/h4&gt;解决EEG数据稀缺和受试者间变异性问题，提高EEG解码模型在实际环境中的泛化能力和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出FusionGen框架，基于解耦表征学习和特征融合技术。通过特征匹配融合模块整合跨试验特征，并结合轻量级特征提取和重建管道，确保在有限数据条件下的数据多样性和可训练性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开EEG数据集上的实验表明，FusionGen显著优于现有增强技术，在分类准确性方面取得了明显改进。&lt;h4&gt;结论&lt;/h4&gt;FusionGen有效解决了BCI领域的数据稀缺和受试者间变异性挑战，是一种有前景的EEG数据生成方法。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCIs)通过脑电图(EEG)在大脑和外部设备之间建立直接通信途径，其应用范围从医疗康复到认知状态评估。然而，基于EEG的BCI受到数据稀缺和显著受试者间变异性的严重限制，这阻碍了EEG解码模型在实际环境中的泛化能力和适用性。为应对这些挑战，我们提出了FusionGen，一种基于解耦表征学习和特征融合的新型EEG数据生成框架。通过特征匹配融合模块整合跨试验特征，并与轻量级特征提取和重建管道相结合，FusionGen确保了在有限数据条件下的数据多样性和可训练性。在多个公开可用的EEG数据集上进行的大量实验表明，FusionGen显著优于现有的增强技术，在分类准确性方面取得了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) provide potential for applications rangingfrom medical rehabilitation to cognitive state assessment by establishingdirect communication pathways between the brain and external devices viaelectroencephalography (EEG). However, EEG-based BCIs are severely constrainedby data scarcity and significant inter-subject variability, which hinder thegeneralization and applicability of EEG decoding models in practical settings.To address these challenges, we propose FusionGen, a novel EEG data generationframework based on disentangled representation learning and feature fusion. Byintegrating features across trials through a feature matching fusion module andcombining them with a lightweight feature extraction and reconstructionpipeline, FusionGen ensures both data diversity and trainability under limiteddata constraints. Extensive experiments on multiple publicly available EEGdatasets demonstrate that FusionGen significantly outperforms existingaugmentation techniques, yielding notable improvements in classificationaccuracy.</description>
      <author>example@mail.com (Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu)</author>
      <guid isPermaLink="false">2510.10604v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Self-supervised Contrastive Learning through Supervised Objectives</title>
      <link>http://arxiv.org/abs/2510.10572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at TMLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了一种理论视角，将自监督表示学习表述为监督表示学习目标的近似，推导出与流行对比损失相关的损失函数，并引入原型表示偏差和平衡对比损失的概念，以解释和改进自监督学习算法的行为。&lt;h4&gt;背景&lt;/h4&gt;自监督表示学习已经取得了显著的实证成功，但其理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供理论视角，将自监督表示学习表述为监督表示学习目标的近似，深入理解对比损失函数的原理。&lt;h4&gt;方法&lt;/h4&gt;基于自监督表示学习作为监督表示学习目标近似的表述，推导损失函数，引入原型表示偏差和平衡对比损失的概念，并对应到对比学习的既定实践。&lt;h4&gt;主要发现&lt;/h4&gt;原型表示偏差和平衡对比损失的概念有助于解释和改进自监督学习算法的行为，理论框架的组成部分对应于对比学习中的既定实践，平衡正负样本对的交互具有实证效果。&lt;h4&gt;结论&lt;/h4&gt;通过理论推导和实证验证，本文提供了自监督表示学习的理论框架，所有理论证明在附录中提供，代码包含在补充材料中。&lt;h4&gt;翻译&lt;/h4&gt;自监督表示学习已经取得了令人印象深刻的实证成功，但其理论理解仍然有限。在这项工作中，我们通过将自监督表示学习表述为监督表示学习目标的近似，提供了一种理论视角。基于这一表述，我们推导出一个与流行的对比损失（如InfoNCE）密切相关的损失函数，揭示了它们的基本原理。我们的推导自然地引入了原型表示偏差和平衡对比损失的概念，这些概念有助于解释和改进自监督学习算法的行为。我们进一步展示了理论框架的组成部分如何对应于对比学习中的既定实践。最后，我们通过实证验证了平衡正负样本对交互的效果。所有理论证明都在附录中提供，我们的代码包含在补充材料中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised representation learning has achieved impressive empiricalsuccess, yet its theoretical understanding remains limited. In this work, weprovide a theoretical perspective by formulating self-supervised representationlearning as an approximation to supervised representation learning objectives.Based on this formulation, we derive a loss function closely related to popularcontrastive losses such as InfoNCE, offering insight into their underlyingprinciples. Our derivation naturally introduces the concepts of prototyperepresentation bias and a balanced contrastive loss, which help explain andimprove the behavior of self-supervised learning algorithms. We further showhow components of our theoretical framework correspond to established practicesin contrastive learning. Finally, we empirically validate the effect ofbalancing positive and negative pair interactions. All theoretical proofs areprovided in the appendix, and our code is included in the supplementarymaterial.</description>
      <author>example@mail.com (Byeongchan Lee)</author>
      <guid isPermaLink="false">2510.10572v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2510.10556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SICSRec是一种创新的顺序推荐模型，通过自监督表示学习和ID-Content模态对齐解决了内容顺序推荐中的三个关键挑战，在有限交互历史情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐模型通常基于用户历史交互的物品ID捕捉用户偏好，但在交互历史有限时表现不佳。基于内容的顺序推荐利用物品的文本和视觉特征增强偏好学习，但仍面临语义差距、偏好联合建模和表示对齐等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决内容顺序推荐中的三个关键挑战：减少不同内容模态表示间的语义差距；联合建模用户行为偏好和内容偏好；设计有效训练策略对齐ID表示和内容表示。&lt;h4&gt;方法&lt;/h4&gt;提出SICSRec模型，包含：基于LLM的样本构建方法和监督微调方法对齐物品级模态表示；基于Transformer的顺序模型，包括ID模态序列编码器、内容模态序列编码器和混合模态序列解码器；两步训练策略结合内容感知对比学习任务对齐模态表示和ID表示。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共视频流数据集上，SICSRec在NDCG@5上平均比最先进的ID模态顺序推荐器高出8.04%，在NDCG@10上平均高出6.62%。&lt;h4&gt;结论&lt;/h4&gt;SICSRec通过有效对齐ID表示和内容表示，成功解决了内容顺序推荐中的关键挑战，在有限交互历史情况下提供了更优的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐(SR)模型通常基于历史交互的物品ID来捕捉用户偏好，当交互历史有限时通常表现不佳。基于内容的顺序推荐最近已成为一种有前途的方向，利用物品的文本和视觉特征来增强偏好学习。然而，仍存在三个关键挑战：(i)如何减少不同内容模态表示之间的语义差距；(ii)如何联合建模用户行为偏好和内容偏好；(iii)如何设计有效的训练策略来对齐ID表示和内容表示。为应对这些挑战，我们提出了一种新模型，即带有ID-Content模态对齐的自监督表示学习，名为SICSRec。首先，我们提出了一种基于LLM的样本构建方法，并开发了监督微调方法来对齐物品级模态表示。其次，我们设计了一种新颖的基于Transformer的顺序模型，其中ID模态序列编码器捕捉用户行为偏好，内容模态序列编码器学习用户内容偏好，混合模态序列解码器把握这两种偏好之间的内在关系。第三，我们提出了一个包含内容感知对比学习任务的两步训练策略，用于对齐模态表示和ID表示，从而解耦内容模态依赖和物品协同依赖的训练过程。在四个公共视频流数据集上进行的广泛实验表明，我们的SICSRec在NDCG@5上平均比最先进的ID模态顺序推荐器高出8.04%，在NDCG@10上平均高出6.62%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation (SR) models often capture user preferences based onthe historically interacted item IDs, which usually obtain sub-optimalperformance when the interaction history is limited. Content-based sequentialrecommendation has recently emerged as a promising direction that exploitsitems' textual and visual features to enhance preference learning. However,there are still three key challenges: (i) how to reduce the semantic gapbetween different content modality representations; (ii) how to jointly modeluser behavior preferences and content preferences; and (iii) how to design aneffective training strategy to align ID representations and contentrepresentations. To address these challenges, we propose a novel model,self-supervised representation learning with ID-Content modality alignment,named SICSRec. Firstly, we propose a LLM-driven sample construction method anddevelop a supervised fine-tuning approach to align item-level modalityrepresentations. Secondly, we design a novel Transformer-based sequentialmodel, where an ID-modality sequence encoder captures user behaviorpreferences, a content-modality sequence encoder learns user contentpreferences, and a mix-modality sequence decoder grasps the intrinsicrelationship between these two types of preferences. Thirdly, we propose atwo-step training strategy with a content-aware contrastive learning task toalign modality representations and ID representations, which decouples thetraining process of content modality dependency and item collaborativedependency. Extensive experiments conducted on four public video streamingdatasets demonstrate our SICSRec outperforms the state-of-the-art ID-modalitysequential recommenders and content-modality sequential recommenders by 8.04%on NDCG@5 and 6.62% on NDCD@10 on average, respectively.</description>
      <author>example@mail.com (Donglin Zhou, Weike Pan, Zhong Ming)</author>
      <guid isPermaLink="false">2510.10556v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unified Open-World Segmentation with Multi-Modal Prompts</title>
      <link>http://arxiv.org/abs/2510.10524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了COSINE，一个统一的开放世界分割模型，整合了开放词汇分割和上下文分割功能，支持多模态提示。&lt;h4&gt;背景&lt;/h4&gt;现有的开放词汇分割和上下文分割方法存在架构差异、不同的学习目标和表示学习策略问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型来解决开放词汇分割和上下文分割的架构和策略不一致问题。&lt;h4&gt;方法&lt;/h4&gt;COSINE利用基础模型提取图像和多模态提示的表示，并通过SegDecoder对齐这些表示、建模交互，生成不同粒度的掩码。&lt;h4&gt;主要发现&lt;/h4&gt;COSINE在开放词汇和上下文分割任务中表现出显著的性能提升，且多模态提示的协同合作相比单模态方法提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;COSINE通过统一架构和策略，成功解决了现有开放世界分割方法的局限性，实现了更强大的分割性能。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了COSINE，一个统一的开放世界分割模型，它整合了开放词汇分割和上下文分割功能，并支持多模态提示（例如文本和图像）。COSINE利用基础模型提取输入图像和对应多模态提示的表示，并通过SegDecoder对齐这些表示、建模它们的交互，并获得不同粒度下由输入提示指定的掩码。这样，COSINE克服了先前开放词汇分割和上下文分割管道的架构差异、不同的学习目标和表示学习策略。全面的实验证明COSINE在开放词汇和上下文分割任务中都有显著的性能提升。我们的探索性分析强调，使用视觉和文本提示之间的协同合作相比单模态方法显著提高了泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present COSINE, a unified open-world segmentation model thatconsolidates open-vocabulary segmentation and in-context segmentation withmulti-modal prompts (e.g., text and image). COSINE exploits foundation modelsto extract representations for an input image and corresponding multi-modalprompts, and a SegDecoder to align these representations, model theirinteraction, and obtain masks specified by input prompts across differentgranularities. In this way, COSINE overcomes architectural discrepancies,divergent learning objectives, and distinct representation learning strategiesof previous pipelines for open-vocabulary segmentation and in-contextsegmentation. Comprehensive experiments demonstrate that COSINE has significantperformance improvements in both open-vocabulary and in-context segmentationtasks. Our exploratory analyses highlight that the synergistic collaborationbetween using visual and textual prompts leads to significantly improvedgeneralization over single-modality approaches.</description>
      <author>example@mail.com (Yang Liu, Yufei Yin, Chenchen Jing, Muzhi Zhu, Hao Chen, Yuling Xi, Bo Feng, Hao Wang, Shiyu Li, Chunhua Shen)</author>
      <guid isPermaLink="false">2510.10524v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes</title>
      <link>http://arxiv.org/abs/2510.10406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mesh-Gait是一种创新的端到端多模态步态识别框架，通过从2D剪影重建3D热图作为中间表示，有效结合了2D和3D表示的优势，在保持计算效率的同时实现了最先进的识别准确性。&lt;h4&gt;背景&lt;/h4&gt;步态识别是一种利用独特行走模式进行个人识别的生物识别技术，传统方法使用2D表示如剪影或骨架，但在视角变化、遮挡和噪声方面存在困难。结合3D身体形状信息的多模态方法虽能提高鲁棒性，但计算成本高，限制了实时应用的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效解决现有步态识别方法局限性，同时结合2D和3D优势的步态识别框架，提高识别准确性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;Mesh-Gait直接从2D剪影重建3D表示，使用3D热图作为中间表示，在训练过程中逐步重建并提高准确性，通过计算重建的3D关节、虚拟标记和3D网格与真实值之间的损失来确保精确对齐。该方法从剪影和重建的3D热图中提取判别性特征，使网络专注于运动动力学而非无关视觉细节。&lt;h4&gt;主要发现&lt;/h4&gt;Mesh-Gait能够以计算高效的方式捕获空间和结构步态特征，避免了从RGB视频直接进行3D重建的巨大开销，使网络能够专注于运动动力学而非无关的视觉细节。&lt;h4&gt;结论&lt;/h4&gt;大量实验证明Mesh-Gait达到了最先进的准确性，代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;步态识别是一种基础的生物识别技术，利用独特的行走模式进行个人识别，通常使用剪影或骨架等二维表示。然而，这些方法往往难以处理视角变化、遮挡和噪声问题。结合三维身体形状信息的多模态方法虽能提高鲁棒性，但计算成本高，限制了其在实时应用中的可行性。为解决这些挑战，我们引入了Mesh-Gait，一种新颖的端到端多模态步态识别框架，直接从二维剪影重建三维表示，有效结合了两种模态的优势。与现有方法相比，直接从三维关节或网格学习三维特征复杂且难以与基于剪影的步态特征融合。为克服这一问题，Mesh-Gait将三维热图重建为中间表示，使模型能够有效捕获三维几何信息，同时保持简单性和计算效率。在训练过程中，中间的三维热图被逐步重建，并在监督学习下变得越来越准确，通过计算重建的三维关节、虚拟标记和三维网格与其对应真实值之间的损失，确保精确的空间对齐和一致的三维结构。Mesh-Gait以计算高效的方式从剪影和重建的三维热图中提取判别性特征。这种设计使模型能够捕获空间和结构步态特征，同时避免了从RGB视频直接进行三维重建的巨大开销，使网络能够专注于运动动力学而非无关的视觉细节。大量实验证明Mesh-Gait达到了最先进的准确性。代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决步态识别(gait recognition)中传统2D方法在视角变化、遮挡和环境噪声方面表现不佳，以及多模态3D方法计算成本高、难以实时应用的问题。这个问题在现实中很重要，因为步态识别是一种非接触式生物识别技术，可在远距离识别个人，适用于监控、安全认证和法医分析等场景，但现有方法难以在实际复杂环境中保持高准确率和实时性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了2D方法的局限性和3D方法的计算成本问题，然后提出使用3D热图作为中间表示来桥接2D和3D信息。他们设计了一个双分支架构：一个处理2D轮廓特征，另一个处理重建的3D特征。在训练过程中使用监督学习逐步优化3D热图。该方法借鉴了HRNet作为3D估计器，受到虚拟标记概念的启发，并参考了现有的步态识别方法如GaitSet、GaitPart等，同时使用了多种损失函数的组合进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D热图作为中间表示，直接从2D轮廓重建3D信息，结合2D和3D特征进行多模态步态识别，并在推理阶段避免网格重建以提高效率。整体流程：1)输入2D轮廓序列；2)双分支处理：2D分支提取轮廓特征，3D分支使用HRNet估计3D热图；3)从热图重建3D关节、虚拟标记和网格；4)分别从2D轮廓和3D热图提取特征；5)拼接融合2D和3D特征；6)通过时间池化和金字塔池化处理；7)使用全连接层计算嵌入并进行识别；8)应用多种损失函数进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一框架直接从2D轮廓重建3D表示，消除复杂多视角需求；2)使用3D热图作为中间表示，便于特征提取和融合；3)监督学习渐进式优化3D热图，确保精确重建；4)推理阶段不需网格重建，计算效率高(比传统方法快72倍)。相比之前工作的不同：传统2D方法难以处理视角变化和遮挡；现有多模态方法需要额外3D重建模型且计算成本高；直接3D特征学习方法处理点云复杂且难以与2D特征融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mesh-Gait提出了一种创新的多模态步态识别框架，通过直接从2D轮廓重建3D热图表示，结合了2D和3D信息的优势，显著提高了识别准确率和鲁棒性，同时大幅降低了计算成本，使实时步态识别成为可能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gait recognition, a fundamental biometric technology, leverages uniquewalking patterns for individual identification, typically using 2Drepresentations such as silhouettes or skeletons. However, these methods oftenstruggle with viewpoint variations, occlusions, and noise. Multi-modalapproaches that incorporate 3D body shape information offer improved robustnessbut are computationally expensive, limiting their feasibility for real-timeapplications. To address these challenges, we introduce Mesh-Gait, a novelend-to-end multi-modal gait recognition framework that directly reconstructs 3Drepresentations from 2D silhouettes, effectively combining the strengths ofboth modalities. Compared to existing methods, directly learning 3D featuresfrom 3D joints or meshes is complex and difficult to fuse with silhouette-basedgait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as anintermediate representation, enabling the model to effectively capture 3Dgeometric information while maintaining simplicity and computationalefficiency. During training, the intermediate 3D heatmaps are graduallyreconstructed and become increasingly accurate under supervised learning, wherethe loss is calculated between the reconstructed 3D joints, virtual markers,and 3D meshes and their corresponding ground truth, ensuring precise spatialalignment and consistent 3D structure. Mesh-Gait extracts discriminativefeatures from both silhouettes and reconstructed 3D heatmaps in acomputationally efficient manner. This design enables the model to capturespatial and structural gait characteristics while avoiding the heavy overheadof direct 3D reconstruction from RGB videos, allowing the network to focus onmotion dynamics rather than irrelevant visual details. Extensive experimentsdemonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will bereleased upon acceptance of the paper.</description>
      <author>example@mail.com (Zhao-Yang Wang, Jieneng Chen, Jiang Liu, Yuxiang Guo, Rama Chellappa)</author>
      <guid isPermaLink="false">2510.10406v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Text2Token: Unsupervised Text Representation Learning with Token Target Prediction</title>
      <link>http://arxiv.org/abs/2510.10224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Text2Token的无监督文本表示学习框架，通过token目标预测任务构建高质量的目标token分布，在MTEB v2基准测试上取得了与最先进方法相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督文本表示学习是自然语言处理的基础任务，对利用网络未标记文本改进搜索和推荐系统至关重要。研究表明，高质量的文本表示与输入文本的关键词对齐，揭示了表示空间和词汇空间之间的潜在联系。&lt;h4&gt;目的&lt;/h4&gt;开发一个无监督生成框架Text2Token，用于文本表示学习，探索表示空间和词汇空间之间的联系，并通过token目标预测任务提升表示学习性能。&lt;h4&gt;方法&lt;/h4&gt;Text2Token框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。作者确定了两种关键token类别：文本中有意义的token和文本外语义派生的token，并提出了数据驱动和模型派生两种方法来构建合成token目标。&lt;h4&gt;主要发现&lt;/h4&gt;在MTEB v2基准测试上，Text2Token的性能与最先进的无监督对比学习方法LLM2Vec具有竞争力。词汇和表示空间在训练过程中共同优化并趋向最优解。&lt;h4&gt;结论&lt;/h4&gt;Text2Token框架成功探索了表示空间和词汇空间之间的联系，为无监督文本表示学习提供了新思路，证明了通过精心设计的token目标预测任务可以有效提升表示学习性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督文本表示学习(TRL)是自然语言处理的一项基础任务，它有助于利用网络上的未标记文本改进搜索和推荐系统。最近的一项实证研究发现，高质量的表示与输入文本的关键词对齐，揭示了表示空间和词汇空间之间的潜在联系。受这一发现的启发，我们重新审视了生成任务，并开发了一个用于TRL的无监督生成框架Text2Token。该框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。为了构建高质量的目标token分布，我们分析了与高级嵌入器的token对齐特性，并确定了两种关键token类别：(1)文本中有意义的token和(2)文本外语义派生的token。基于这些见解，我们提出了两种方法——数据驱动和模型派生——从数据或LLM骨干构建合成token目标。在MTEB v2基准测试上的实验表明，Text2Token的性能与最先进的无监督对比学习方法LLM2Vec具有竞争力。我们的进一步分析表明，词汇和表示空间在训练过程中共同优化并趋向最优解，为未来工作提供了新的思路和见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised text representation learning (TRL) is a fundamental task innatural language processing, which is beneficial for improving search andrecommendations with the web's unlabeled texts. A recent empirical study findsthat the high-quality representation aligns with the key token of the inputtext, uncovering the potential connection between representation space andvocabulary space. Inspired by the findings, we revisit the generative tasks anddevelop an unsupervised generative framework for TRL, Text2Token. The frameworkis based on the token target prediction task, utilizing carefully constructedtarget token distribution as supervisory signals. To construct the high-qualitytarget token distribution, we analyze the token-alignment properties withadvanced embedders and identify two essential categories of key tokens: (1) themeaningful tokens in the text and (2) semantically derived tokens beyond thetext. Based on these insights, we propose two methods -- data-driven andmodel-derived -- to construct synthetic token targets from data or the LLMbackbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Tokenachieves performance competitive with the state-of-the-art embedder withunsupervised contrastive learning, LLM2Vec. Our analysis further shows thatvocabulary and representation spaces optimize together and toward the optimumsolution during training, providing new ideas and insights for future work.</description>
      <author>example@mail.com (Ruize An, Richong Zhang, Zhijie Nie, Zhanyu Wu, Yanzhao Zhang, Dingkun Long)</author>
      <guid isPermaLink="false">2510.10224v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</title>
      <link>http://arxiv.org/abs/2510.10102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了PANTHER，一个混合生成-判别框架，用于用户行为建模和表示学习。通过生成式预训练从无标签行为数据中学习可迁移的表示，结合结构化标记化、序列模式识别、统一用户画像嵌入和实时可扩展性等技术，在微信支付的实际应用中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型能够通过生成式预训练将大量世界知识压缩到紧凑的标记表示中。然而，在建模用户交互历史中的行为知识方面存在局限。用户行为形成独特模态，每个行为（由时间、上下文和交易类型等多维属性定义）构成行为标记，建模这些高基数序列具有挑战性，判别模型在监督有限时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;将生成式预训练扩展到用户行为领域，类似于LLMs从文本中学习的方式，从无标签行为数据中学习可迁移的表示，开发能够实现大规模序列用户表示学习和实时推理的框架。&lt;h4&gt;方法&lt;/h4&gt;提出了PANTHER框架，包含四个主要组件：(1)结构化标记化：将多维交易属性压缩为可解释的词汇表；(2)序列模式识别模块(SPRM)：用于建模周期性交易模式；(3)统一用户画像嵌入：融合静态人口统计信息和动态交易历史；(4)实时可扩展性：通过预训练嵌入的离线缓存实现毫秒级推理。&lt;h4&gt;主要发现&lt;/h4&gt;在微信支付上部署PANTHER后，实现了下一交易预测的HitRate@1提升25.6%，欺诈检测召回率相对提高38.6%。在公共基准测试上显示出强大泛化能力，相比transformer基线最高实现21%的HitRate@1提升。&lt;h4&gt;结论&lt;/h4&gt;PANTHER被确立为一种可扩展、高性能的工业级序列用户行为建模框架，通过结合生成式预训练和判别式建模，有效解决了用户行为建模中的挑战，并在实际应用中取得了显著效果。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已经证明，生成式预训练可以将大量世界知识压缩到紧凑的标记表示中。虽然LLMs包含广泛的世界知识，但在建模用户交互历史中包含的行为知识方面仍然有限。用户行为形成一种独特的模态，其中每个行为（由时间、上下文和交易类型等多维属性定义）构成一个行为标记。建模这些高基数序列具有挑战性，判别模型在监督有限的情况下往往表现不佳。为了填补这一空白，我们将生成式预训练扩展到用户行为，类似于LLMs从文本中学习的方式，从无标签行为数据中学习可迁移的表示。我们提出了PANTHER，一个混合生成-判别框架，统一了用户行为预训练和下游适应，实现了大规模序列用户表示学习和实时推理。PANTHER引入了：(1)结构化标记化，将多维交易属性压缩为可解释的词汇表；(2)序列模式识别模块(SPRM)，用于建模周期性交易模式；(3)统一用户画像嵌入，融合静态人口统计信息和动态交易历史；(4)实时可扩展性，通过预训练嵌入的离线缓存实现毫秒级推理。在微信支付上全面部署和在线运行后，PANTHER相比基线实现了下一交易预测HitRate@1提升25.6%，欺诈检测召回率相对提高38.6%。在公共基准上的跨领域评估显示出强大的泛化能力，相比transformer基线最高实现21%的HitRate@1提升，确立了PANTHER作为工业级序列用户行为建模的可扩展、高性能框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have shown that generative pretraining candistill vast world knowledge into compact token representations. While LLMsencapsulate extensive world knowledge, they remain limited in modeling thebehavioral knowledge contained within user interaction histories. User behaviorforms a distinct modality, where each action, defined by multi-dimensionalattributes such as time, context, and transaction type, constitutes abehavioral token. Modeling these high-cardinality sequences is challenging, anddiscriminative models often falter under limited supervision. To bridge thisgap, we extend generative pretraining to user behavior, learning transferablerepresentations from unlabeled behavioral data analogous to how LLMs learn fromtext. We present PANTHER, a hybrid generative-discriminative framework thatunifies user behavior pretraining and downstream adaptation, enablinglarge-scale sequential user representation learning and real-time inference.PANTHER introduces: (1) Structured Tokenization to compress multi-dimensionaltransaction attributes into an interpretable vocabulary; (2) Sequence PatternRecognition Module (SPRM) for modeling periodic transaction motifs; (3) aUnified User-Profile Embedding that fuses static demographics with dynamictransaction histories; and (4) Real-time scalability enabled by offline cachingof pretrained embeddings for millisecond-level inference. Fully deployed andoperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost innext-transaction prediction HitRate@1 and a 38.6 percent relative improvementin fraud detection recall over baselines. Cross-domain evaluations on publicbenchmarks show strong generalization, achieving up to 21 percent HitRate@1gains over transformer baselines, establishing PANTHER as a scalable,high-performance framework for industrial sequential user behavior modeling.</description>
      <author>example@mail.com (Guilin Li, Yun Zhang, Xiuyuan Chen, Chengqi Li, Bo Wang, Linghe Kong, Wenjia Wang, Weiran Huang, Matthias Hwai Yong Tan)</author>
      <guid isPermaLink="false">2510.10102v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Cooperative Pseudo Labeling for Unsupervised Federated Classification</title>
      <link>http://arxiv.org/abs/2510.10100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FedCoPL（联邦合作伪标签）的新方法，首次将无监督联邦学习(UFL)扩展到分类问题，利用CLIP模型的零样本预测能力，通过客户端上传伪标签分布和服务器重新分配来解决类别不平衡问题，并引入部分提示聚合协议促进协作和个性化。&lt;h4&gt;背景&lt;/h4&gt;无监督联邦学习(UFL)旨在分布式客户端之间协作训练全局模型而不共享数据或标签信息。之前的UFL工作主要集中在表示学习和聚类任务上。视觉语言模型(如CLIP)因其强大的零样本预测能力而受到广泛关注，使UFL范式下的分类问题成为可能，但这一领域仍 largely未被探索。&lt;h4&gt;目的&lt;/h4&gt;将UFL扩展到分类问题，利用CLIP模型解决UFL中的分类挑战，并提出一种新的联邦学习方法来有效处理此类问题。&lt;h4&gt;方法&lt;/h4&gt;提出了FedCoPL方法：客户端估计并上传伪标签分布，服务器调整并重新分配以避免类别不平衡；引入部分提示聚合协议实现有效协作和个性化，其中视觉提示在服务器端聚合，文本提示保留在本地。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，FedCoPL与基线方法相比具有优越性能，成功解决了UFL范式下的分类问题。&lt;h4&gt;结论&lt;/h4&gt;FedCoPL成功将UFL扩展到分类问题，通过伪标签分布的协作和部分提示聚合，实现了有效的联邦学习，为UFL范式下的分类问题提供了新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无监督联邦学习(UFL)旨在分布式客户端之间协作训练全局模型，而不共享数据或访问标签信息。之前的UFL工作主要集中在表示学习和聚类任务上。最近，视觉语言模型（如CLIP）因其强大的零样本预测能力而受到广泛关注。利用这一进展，之前在UFL范式下被认为不可行的分类问题现在呈现出有希望的新机会，但仍然 largely未被探索。在本文中，我们首次将UFL扩展到使用CLIP的分类问题，并提出了一种新方法，联邦合作伪标签(FedCoPL)。具体来说，客户端估计并上传其伪标签分布，服务器调整并重新分配它们以避免类别之间的全局不平衡。此外，我们引入了部分提示聚合协议以实现有效的协作和个性化。特别是，包含通用图像特征的视觉提示在服务器端聚合，而编码个性化知识的文本提示则保留在本地。大量实验证明了我们的FedCoPL与基线方法相比的优越性能。我们的代码可在https://github.com/krumpguo/FedCoPL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised Federated Learning (UFL) aims to collaboratively train a globalmodel across distributed clients without sharing data or accessing labelinformation. Previous UFL works have predominantly focused on representationlearning and clustering tasks. Recently, vision language models (e.g., CLIP)have gained significant attention for their powerful zero-shot predictioncapabilities. Leveraging this advancement, classification problems that werepreviously infeasible under the UFL paradigm now present promising newopportunities, yet remain largely unexplored. In this paper, we extend UFL tothe classification problem with CLIP for the first time and propose a novelmethod, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative\underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}).Specifically, clients estimate and upload their pseudo label distribution, andthe server adjusts and redistributes them to avoid global imbalance amongclasses. Moreover, we introduce a partial prompt aggregation protocol foreffective collaboration and personalization. In particular, visual promptscontaining general image features are aggregated at the server, while textprompts encoding personalized knowledge are retained locally. Extensiveexperiments demonstrate the superior performance of our FedCoPL compared tobaseline methods. Our code is available at\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.</description>
      <author>example@mail.com (Kuangpu Guo, Lijun Sheng, Yongcan Yu, Jian Liang, Zilei Wang, Ran He)</author>
      <guid isPermaLink="false">2510.10100v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling</title>
      <link>http://arxiv.org/abs/2510.10060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Translution的新型操作，它结合了自注意力和卷积的优势，能够在计算机视觉和自然语言处理任务上实现更高的准确性。&lt;h4&gt;背景&lt;/h4&gt;在建模数据时，现有方法如自注意力和卷积各有优缺点。自注意力可以自适应识别相关元素，但依赖绝对位置嵌入；卷积以相对方式编码元素，但固定的核大小限制了其自适应选择能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一自注意力自适应识别能力和卷积相对编码优势的操作，同时解决参数数量过多的问题。&lt;h4&gt;方法&lt;/h4&gt;提出Translution操作，并结合其轻量级变体α-Translution，以减少参数数量，使其适合实际应用。&lt;h4&gt;主要发现&lt;/h4&gt;Translution（包括α-Translution）在计算机视觉和自然语言处理任务上实现了比自注意力更高的准确性，同时保持了计算效率。&lt;h4&gt;结论&lt;/h4&gt;Translution操作成功统一了自注意力和卷积的优势，为数据建模提供了新的有效方法，其轻量级变体使其能够在实际应用中部署。&lt;h4&gt;翻译&lt;/h4&gt;在建模给定类型的数据时，我们认为它涉及两个关键方面：1)识别与中心元素相关的元素（如卷积感受野中的图像像素）或与查询元素相关的元素（如自注意力中的文本单词），以及2)有效编码这些标记。自注意力可以自适应地识别这些元素，但依赖于绝对位置嵌入来进行结构表示学习。相比之下，卷积以相对方式编码元素，但其固定的核大小限制了它们自适应选择相关元素的能力。在本文中，我们引入了Translution，这是一种统一了自注意力自适应识别能力和卷积相对编码优势的操作。然而，这种整合导致参数数量大幅增加，超过了大多数现有计算资源的能力。因此，我们提出了Translution的轻量级变体，称为α-Translution。在计算机视觉和自然语言处理任务上的实验表明，Translution（包括α-Translution）实现了比自注意力更高的准确性。代码可在https://github.com/hehefan/Translution获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When modeling a given type of data, we consider it to involve two keyaspects: 1) identifying relevant elements (e.g., image pixels or textual words)to a central element, as in a convolutional receptive field, or to a queryelement, as in self-attention, and 2) encoding these tokens effectively.Self-attention can adaptively identify these elements but relies on absolutepositional embedding for structural representation learning. In contrast,convolution encodes elements in a relative manner, yet their fixed kernel sizelimits their ability to adaptively select the relevant elements. In this paper,we introduce Translution, an operation that unifies the adaptive identificationcapability of self-attention and the relative encoding advantage ofconvolution. However, this integration leads to a substantial increase in thenumber of parameters, exceeding most currently available computationalresources. Therefore, we propose a lightweight variant of Translution, named{\alpha}-Translution. Experiments on computer vision and natural languageprocessing tasks show that Translution (including {\alpha}-Translution)achieves superior accuracy compared to self-attention. The code is available athttps://github.com/hehefan/Translution.</description>
      <author>example@mail.com (Hehe Fan, Yi Yang, Mohan Kankanhalli, Fei Wu)</author>
      <guid isPermaLink="false">2510.10060v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.09894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AETHER是一个轻量级框架，通过兴趣点(POIs)引导的多模态对齐，将AlphaEarth模型扩展到以人为中心的urban分析，在土地使用分类和社会经济映射方面取得显著改进。&lt;h4&gt;背景&lt;/h4&gt;通用空间表示对构建可迁移的地理空间基础模型(GFMs)至关重要。AlphaEarthFoundation(AE)代表了地球表面全球统一表示的重要进展，但它主要编码物理和光谱模式，难以捕捉城市的功能和社会经济维度。&lt;h4&gt;目的&lt;/h4&gt;提出AETHER框架，通过兴趣点(POIs)引导的多模态对齐，使AlphaEarth适应以人为中心的urban分析，丰富其物理特征与语义线索。&lt;h4&gt;方法&lt;/h4&gt;AETHER将AE嵌入与POIs的文本表示对齐，用关于城市功能和社会经济语境的语义线索丰富基于物理的EO特征。它构建在预训练的AE之上，利用轻量级多模态对齐来丰富以人为中心的语义，同时保持计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在大伦敦地区，AETHER相对于AE基线实现了一致的改进：土地使用分类F1相对提高7.2%，社会经济映射的Kullback-Leibler散度相对减少23.6%。&lt;h4&gt;结论&lt;/h4&gt;通过将地球观测数据与以人为中心的语义相结合，AETHER推进了地理空间基础模型向通用城市表示发展，这些表示同时整合物理形态和功能意义。&lt;h4&gt;翻译&lt;/h4&gt;通用空间表示对于构建可迁移的地理空间基础模型(GFMs)至关重要。其中，AlphaEarthFoundation(AE)代表了地球表面全球统一表示的重要进展，它从多源地球观测(EO)数据中学习10米嵌入，捕捉不同景观中丰富的物理和环境模式。然而，这类EO驱动的表示在捕捉城市的功能和社会经济维度方面仍然有限，因为它们主要编码物理和光谱模式，而不是人类活动或空间功能。我们提出了AETHER(AlphaEarth-POI Enriched Representation Learning)，这是一个轻量级框架，通过兴趣点(POIs)引导的多模态对齐，使AlphaEarth适应以人为中心的urban分析。AETHER将AE嵌入与POIs的文本表示对齐，用关于城市功能和社会经济语境的语义线索丰富基于物理的EO特征。在大伦敦地区，AETHER相对于AE基线实现了一致的改进，土地使用分类F1相对提高7.2%，社会经济映射的Kullback-Leibler散度相对减少23.6%。构建在预训练的AE之上，AETHER利用轻量级多模态对齐来丰富它以人为中心的语义，同时保持计算效率和对城市应用的可扩展性。通过将EO与以人为中心的语义相结合，AETHER推进了地理空间基础模型向通用城市表示发展，这些表示同时整合物理形态和功能意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose spatial representations are essential for buildingtransferable geospatial foundation models (GFMs). Among them, the AlphaEarthFoundation (AE) represents a major step toward a global, unified representationof the Earth's surface, learning 10-meter embeddings from multi-source EarthObservation (EO) data that capture rich physical and environmental patternsacross diverse landscapes. However, such EO-driven representations remainlimited in capturing the functional and socioeconomic dimensions of cities, asthey primarily encode physical and spectral patterns rather than humanactivities or spatial functions. We propose AETHER (AlphaEarth-POI EnrichedRepresentation Learning), a lightweight framework that adapts AlphaEarth tohuman-centered urban analysis through multimodal alignment guided by Points ofInterest (POIs). AETHER aligns AE embeddings with textual representations ofPOIs, enriching physically grounded EO features with semantic cues about urbanfunctions and socioeconomic contexts. In Greater London, AETHER achievesconsistent gains over the AE baseline, with a 7.2% relative improvement inland-use classification F1 and a 23.6% relative reduction in Kullback-Leiblerdivergence for socioeconomic mapping. Built upon pretrained AE, AETHERleverages a lightweight multimodal alignment to enrich it with human-centeredsemantics while remaining computationally efficient and scalable for urbanapplications. By coupling EO with human-centered semantics, it advancesgeospatial foundation models toward general-purpose urban representations thatintegrate both physical form and functional meaning.</description>
      <author>example@mail.com (Junyuan Liu, Quan Qin, Guangsheng Dong, Xinglei Wang, Jiazhuang Feng, Zichao Zeng, Tao Cheng)</author>
      <guid isPermaLink="false">2510.09894v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>TAWRMAC: A Novel Dynamic Graph Representation Learning Method</title>
      <link>http://arxiv.org/abs/2510.09884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TAWRMAC框架，通过整合带重启的临时匿名游走、内存增强和邻居共现嵌入技术，解决了动态图表示学习中的三个关键挑战：嵌入过时、上下文感知不足和结构动态捕捉不充分。该方法在动态链接预测和节点分类任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;动态图表示学习对于分析社交网络、推荐系统和交通分析等领域中的演化网络至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有连续时间动态图表示学习方法面临的三个关键挑战：节点嵌入过时、邻域相关性捕获不足以及结构动态捕捉不充分。&lt;h4&gt;方法&lt;/h4&gt;提出TAWRMAC框架，整合带重启的临时匿名游走、内存增强GNN和邻居共现嵌入技术。通过固定时间编码的内存增强GNN提高嵌入稳定性，通过明确捕获邻居相关性改善上下文表示，通过区分重复交互节点和形成新连接的节点来更好地捕获结构动态。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，TAWRMAC在三种不同的负采样策略下，无论是在归纳还是直推设置中的动态链接预测和节点分类任务中，都始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;TAWRMAC通过提供稳定、可推广和上下文感知的嵌入，推进了连续时间动态图学习的最新技术水平。&lt;h4&gt;翻译&lt;/h4&gt;动态图表示学习已成为分析社交网络分析、推荐系统和交通分析等领域中演化网络的关键技术。然而，现有的连续时间方法面临三个关键挑战：(1)一些方法仅依赖节点特定内存而未有效整合邻居节点信息，导致嵌入过时；(2)大多数未能明确捕获节点邻域间的相关性，限制了上下文感知能力；(3)许多方法在缺乏丰富链接属性的情况下无法完全捕捉演化图的结构动态。为解决这些局限性，我们引入了TAWRMAC——一个整合了带重启的临时匿名游走、内存增强和邻居共现嵌入的新颖框架。TAWRMAC通过具有固定时间编码的内存增强GNN提高嵌入稳定性，并通过明确捕获邻居相关性来改善上下文表示。此外，其带重启的临时匿名游走机制区分了展示重复交互的节点和形成超出其直接邻域新连接的节点。这种方法能更好地捕获结构动态并支持强归纳学习。在多个基准数据集上的广泛实验表明，TAWRMAC在三种不同负采样策略下，无论是在归纳还是直推设置中的动态链接预测和节点分类任务中，都始终优于最先进的方法。通过提供稳定、可推广和上下文感知的嵌入，TAWRMAC推进了连续时间动态图学习的最新技术水平。代码可在https://anonymous.4open.science/r/tawrmac-A253获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graph representation learning has become essential for analyzingevolving networks in domains such as social network analysis, recommendationsystems, and traffic analysis. However, existing continuous-time methods facethree key challenges: (1) some methods depend solely on node-specific memorywithout effectively incorporating information from neighboring nodes, resultingin embedding staleness; (2) most fail to explicitly capture correlationsbetween node neighborhoods, limiting contextual awareness; and (3) many fail tofully capture the structural dynamics of evolving graphs, especially in absenceof rich link attributes. To address these limitations, we introduce TAWRMAC-anovel framework that integrates Temporal Anonymous Walks with Restart, MemoryAugmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embeddingstability through a memory-augmented GNN with fixedtime encoding and improvescontextual representation by explicitly capturing neighbor correlations.Additionally, its Temporal Anonymous Walks with Restart mechanism distinguishesbetween nodes exhibiting repetitive interactions and those forming newconnections beyond their immediate neighborhood. This approach capturesstructural dynamics better and supports strong inductive learning. Extensiveexperiments on multiple benchmark datasets demonstrate that TAWRMACconsistently outperforms state-of-the-art methods in dynamic link predictionand node classification under both transductive and inductive settings acrossthree different negative sampling strategies. By providing stable,generalizable, and context-aware embeddings, TAWRMAC advances the state of theart in continuous-time dynamic graph learning. The code is available athttps://anonymous.4open.science/r/tawrmac-A253 .</description>
      <author>example@mail.com (Soheila Farokhi, Xiaojun Qi, Hamid Karimi)</author>
      <guid isPermaLink="false">2510.09884v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems</title>
      <link>http://arxiv.org/abs/2510.09805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 1 table, 1 algorithm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对连续时间动力系统的自适应时间重参数化的隐空间公式，称为时间提升方法。&lt;h4&gt;背景&lt;/h4&gt;连续时间动力系统在处理近奇异行为时存在挑战，特别是在湍流等复杂系统中。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来规范底层流的近奇异行为，同时保持其守恒定律，使轨迹变得全局平滑。&lt;h4&gt;方法&lt;/h4&gt;引入一个平滑单调映射作为时间提升操作符，可以看作是连续时间归一化或时间扭曲算子。&lt;h4&gt;主要发现&lt;/h4&gt;在提升坐标中，不可压缩Navier-Stokes方程在环面上的轨迹变得全局平滑，可以稳定物理信息神经网络和其他AI系统中使用的潜在流架构。&lt;h4&gt;结论&lt;/h4&gt;该框架将解析正则性理论与用于刚性或湍流过程的表示学习方法联系起来。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了连续时间动力系统的自适应时间重参数化的隐空间公式。这种方法称为时间提升，引入了一个平滑单调映射，它可以规范底层流的近奇异行为，同时保持其守恒定律。在提升坐标中，如环面上不可压缩Navier-Stokes方程的轨迹变得全局平滑。从机器学习动力学的角度来看，时间提升作为连续时间归一化或时间扭曲算子，可以稳定物理信息神经网络和其他AI系统中使用的潜在流架构。该框架将解析正则性理论与用于处理刚性或湍流过程的表示学习方法联系起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a latent-space formulation of adaptive temporal reparametrizationfor continuous-time dynamical systems. The method, called *temporal lifting*,introduces a smooth monotone mapping $t \mapsto \tau(t)$ that regularizesnear-singular behavior of the underlying flow while preserving its conservationlaws. In the lifted coordinate, trajectories such as those of theincompressible Navier-Stokes equations on the torus $\mathbb{T}^3$ becomeglobally smooth. From the standpoint of machine-learning dynamics, temporallifting acts as a continuous-time normalization or time-warping operator thatcan stabilize physics-informed neural networks and other latent-flowarchitectures used in AI systems. The framework links analytic regularitytheory with representation-learning methods for stiff or turbulent processes.</description>
      <author>example@mail.com (Jeffrey Camlin)</author>
      <guid isPermaLink="false">2510.09805v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Combined Representation and Generation with Diffusive State Predictive Information Bottleneck</title>
      <link>http://arxiv.org/abs/2510.09784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Diffusive State Predictive Information Bottleneck (D-SPIB)的新方法，结合时间延迟信息瓶颈和扩散模型，用于分子科学中的生成建模，实现表征学习和生成目标的平衡。&lt;h4&gt;背景&lt;/h4&gt;生成建模在高维空间中变得日益数据密集型，而在分子科学中，数据收集成本高且重要事件稀少，因此压缩到低维流形对各种下游任务（包括生成）特别重要。&lt;h4&gt;目的&lt;/h4&gt;结合时间延迟信息瓶颈和扩散模型，在一个联合训练目标中实现表征学习和生成目标的平衡，构建灵活的架构，并学习热力学的连贯内部表征。&lt;h4&gt;方法&lt;/h4&gt;将时间延迟信息瓶颈与扩散模型结合在一个联合训练目标中，创建名为D-SPIB的协议，使模型能够结合来自不同分子模拟轨迹的温度信息。&lt;h4&gt;主要发现&lt;/h4&gt;D-SPIB能够在表征学习和生成目标之间取得平衡，且模型能够学习热力学的连贯且有用的内部表征。&lt;h4&gt;结论&lt;/h4&gt;在多个分子任务上对D-SPIB进行了基准测试，展示了其探索训练集外物理条件的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在高维空间中，生成建模变得越来越数据密集型。在分子科学领域，数据收集成本高昂且重要事件稀少，压缩到低维流形对各种下游任务（包括生成）尤为重要。我们将一种旨在表征分子重要表示的时间延迟信息瓶颈与扩散模型结合在一个联合训练目标中。由此产生的协议，我们称之为扩散状态预测信息瓶颈，能够在一种灵活的架构中平衡表征学习和生成目标。此外，该模型能够结合来自不同分子模拟轨迹的温度信息，学习热力学的连贯且有用的内部表征。我们在多个分子任务上对D-SPIB进行了基准测试，展示了其探索训练集外物理条件的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling becomes increasingly data-intensive in high-dimensionalspaces. In molecular science, where data collection is expensive and importantevents are rare, compression to lower-dimensional manifolds is especiallyimportant for various downstream tasks, including generation. We combine atime-lagged information bottleneck designed to characterize molecular importantrepresentations and a diffusion model in one joint training objective. Theresulting protocol, which we term Diffusive State Predictive InformationBottleneck (D-SPIB), enables the balancing of representation learning andgeneration aims in one flexible architecture. Additionally, the model iscapable of combining temperature information from different molecularsimulation trajectories to learn a coherent and useful internal representationof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcaseits potential for exploring physical conditions outside the training set.</description>
      <author>example@mail.com (Richard John, Yunrui Qiu, Lukas Herron, Pratyush Tiwary)</author>
      <guid isPermaLink="false">2510.09784v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network</title>
      <link>http://arxiv.org/abs/2510.09767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HeSRN是一种新型异构图表示学习网络，通过槽感知结构和基于保留的编码器解决了图变换器计算复杂度高和无法有效建模异构语义的问题，在节点分类任务上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;图变换器通过自注意力机制在图表示学习中取得了显著进展，但其二次方计算复杂度和无法有效建模异构语义严重限制了其在真实世界异构图上的可扩展性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出HeSRN，一种新型异构图槽感知保留网络，用于高效且表达性强的异构图表示学习，解决图变换器的计算复杂度和异构语义建模问题。&lt;h4&gt;方法&lt;/h4&gt;HeSRN引入了槽感知结构编码器，通过将异构特征投影到独立槽并使用槽归一化和基于保留的融合来分离节点类型语义；用基于保留的编码器取代自注意力机制，在线性时间复杂度内建模依赖关系；采用异构保留编码器通过多尺度保留层联合捕获局部结构信号和全局异构语义。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界异构图数据集上的实验表明，HeSRN在节点分类任务上始终优于最先进的异构图神经网络和图变换器基线，同时具有显著更低的计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;HeSRN通过创新的槽感知结构和基于保留的编码机制，有效解决了图变换器在异构图表示学习中的局限性，实现了高效且表达性强的学习性能。&lt;h4&gt;翻译&lt;/h4&gt;图变换器最近通过自注意力机制捕获长距离依赖关系，在图表示学习中取得了显著进展。然而，它们的二次方计算复杂度和无法有效建模异构语义严重限制了它们在真实世界异构图上的可扩展性和泛化能力。为了解决这些问题，我们提出了HeSRN，一种用于高效且表达性强的异构图表示学习的新型异构槽感知保留网络。HeSRN引入了一种槽感知结构编码器，通过将异构特征投影到独立槽并通过槽归一化和基于保留的融合来对齐它们的分布，从而显式地分离节点类型语义，有效缓解了先前基于Transformer模型中强制特征空间统一引起的语义纠缠。此外，我们用基于保留的编码器取代了自注意力机制，该编码器在线性时间复杂度内建模结构和上下文依赖关系，同时保持强大的表达能力。进一步采用异构保留编码器通过多尺度保留层联合捕获局部结构信号和全局异构语义。在四个真实世界异构图数据集上的大量实验表明，HeSRN在节点分类任务上始终优于最先进的异构图神经网络和图变换器基线，以显著更低的计算复杂度实现了更高的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers have recently achieved remarkable progress in graphrepresentation learning by capturing long-range dependencies throughself-attention. However, their quadratic computational complexity and inabilityto effectively model heterogeneous semantics severely limit their scalabilityand generalization on real-world heterogeneous graphs. To address these issues,we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network forefficient and expressive heterogeneous graph representation learning. HeSRNintroduces a slot-aware structure encoder that explicitly disentanglesnode-type semantics by projecting heterogeneous features into independent slotsand aligning their distributions through slot normalization and retention-basedfusion, effectively mitigating the semantic entanglement caused by forcedfeature-space unification in previous Transformer-based models. Furthermore, wereplace the self-attention mechanism with a retention-based encoder, whichmodels structural and contextual dependencies in linear time complexity whilemaintaining strong expressive power. A heterogeneous retentive encoder isfurther employed to jointly capture both local structural signals and globalheterogeneous semantics through multi-scale retention layers. Extensiveexperiments on four real-world heterogeneous graph datasets demonstrate thatHeSRN consistently outperforms state-of-the-art heterogeneous graph neuralnetworks and Graph Transformer baselines on node classification tasks,achieving superior accuracy with significantly lower computational complexity.</description>
      <author>example@mail.com (Yifan Lu, Ziyun Zou, Belal Alsinglawi, Islam Al-Qudah, Izzat Alsmadi, Feilong Tang, Pengfei Jiao, Shoaib Jameel)</author>
      <guid isPermaLink="false">2510.09767v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging</title>
      <link>http://arxiv.org/abs/2510.09593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 4 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STaTS是一种轻量级无监督框架，用于结构感知的时间序列总结，能自适应压缩时间序列为保留信息的令牌序列，实现高达30倍的压缩率，同时保留核心时间动态。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据常含潜在时间结构、状态转换、重复模式和变异性爆发，但现有模型通常处理原始或固定窗口序列，将所有时间步视为同等重要，导致在长序列或噪声序列中效率低下、鲁棒性差和可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个轻量级无监督框架STaTS，用于结构感知的时间序列总结，自适应压缩单变量和多变量时间序列为紧凑的信息保留令牌序列。&lt;h4&gt;方法&lt;/h4&gt;STaTS使用基于BIC的统计散度标准在多个时间分辨率上检测变化点，然后用简单函数（如均值）或生成模型（如GMM）对每个段进行总结，作为模型无关的预处理器可集成到现有无监督时间序列编码器中无需重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在150多个数据集上的实验表明，STaTS可实现全模型85-90%的性能，同时大幅降低计算成本；提高噪声下的鲁棒性，保留判别性结构；优于均匀和基于聚类的压缩基线。&lt;h4&gt;结论&lt;/h4&gt;STaTS是一种原则性的、通用的解决方案，用于高效、结构感知的时间序列建模。&lt;h4&gt;翻译&lt;/h4&gt;时间序列数据通常包含潜在的时间结构、局部平稳状态之间的转换、重复的motifs和变异性爆发，这些特征在标准表示学习管道中很少被利用。现有模型通常处理原始或固定窗口序列，将所有时间步视为同等重要，这导致在长序列或有噪声序列中效率低下、鲁棒性差和可扩展性有限。我们提出了STaTS，一种用于结构感知时间总结的轻量级无监督框架，能自适应地将单变量和多变量时间序列压缩为紧凑的、保留信息的令牌序列。STaTS使用基于BIC的统计散度标准在多个时间分辨率上检测变化点，然后使用简单函数如均值或生成模型如GMM对每个段进行总结。这一过程实现了高达30倍的序列压缩，同时保留核心时间动态。STaTS作为模型无关的预处理器，可以与现有无监督时间序列编码器集成而无需重新训练。在150多个数据集上的广泛实验，包括UCR-85、UCR-128和UEA-30档案上的分类任务，以及ETTh1和ETTh2、ETTm1和Electricity上的预测，表明STaTS可实现全模型85-90%的性能，同时显著降低计算成本。此外，STaTS提高了噪声下的鲁棒性并保留了判别性结构，优于均匀和基于聚类的压缩基线。这些结果将STaTS定位为一种原则性的、通用的解决方案，用于高效、结构感知的时间序列建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series data often contain latent temporal structure, transitions betweenlocally stationary regimes, repeated motifs, and bursts of variability, thatare rarely leveraged in standard representation learning pipelines. Existingmodels typically operate on raw or fixed-window sequences, treating all timesteps as equally informative, which leads to inefficiencies, poor robustness,and limited scalability in long or noisy sequences. We propose STaTS, alightweight, unsupervised framework for Structure-Aware Temporal Summarizationthat adaptively compresses both univariate and multivariate time series intocompact, information-preserving token sequences. STaTS detects change pointsacross multiple temporal resolutions using a BIC-based statistical divergencecriterion, then summarizes each segment using simple functions like the mean orgenerative models such as GMMs. This process achieves up to 30x sequencecompression while retaining core temporal dynamics. STaTS operates as amodel-agnostic preprocessor and can be integrated with existing unsupervisedtime series encoders without retraining. Extensive experiments on 150+datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,demonstrate that STaTS enables 85-90\% of the full-model performance whileoffering dramatic reductions in computational cost. Moreover, STaTS improvesrobustness under noise and preserves discriminative structure, outperforminguniform and clustering-based compression baselines. These results positionSTaTS as a principled, general-purpose solution for efficient, structure-awaretime series modeling.</description>
      <author>example@mail.com (Disharee Bhowmick, Ranjith Ramanathan, Sathyanarayanan N. Aakur)</author>
      <guid isPermaLink="false">2510.09593v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2510.09539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures. Submitted to IEEE ICASSP 2026. Copyright 2026  IEEE. Personal use of this material is permitted. Permission from IEEE must  be obtained for all other uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一个名为IF-D的大型惯性数据集，旨在支持IMU时间序列的自监督学习和基础学习。该数据集包含连续、长时间、多通道的记录，经过精心采集和校准，为鲁棒表示学习和下游任务提供了高质量数据支持。&lt;h4&gt;背景&lt;/h4&gt;惯性测量单元(IMU)时间序列数据在自动驾驶、运动分析和导航等领域有广泛应用。然而，现有数据集可能存在平台特定偏差，且缺乏多样化的运动模式，限制了模型的学习效果和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个大型惯性数据集IF-D，以减轻平台特定运动偏差，让模型接触物理动力学和典型测量噪声，从而促进鲁棒表示学习和支持下游任务如事件检测、运动模式识别和惯性导航。&lt;h4&gt;方法&lt;/h4&gt;使用UM7 IMU传感器，安装在3D打印的球形外壳内，在车辆行驶过程中采集加速度计、陀螺仪和磁力计数据。采样率为200Hz，收集时长约135分钟，产生约160万个样本。采用六方向加速度计校准、静止陀螺仪偏差估计和磁力计硬/软铁校正的椭球拟合等方法进行数据预处理和校准。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了一个包含九个传感器通道、约160万个样本的大型惯性数据集。通过精心设计的校准程序，有效减轻了平台特定偏差，并提供了定量的校准结果，验证了数据集的质量。&lt;h4&gt;结论&lt;/h4&gt;IF-D数据集通过多样化的自由旋转运动和全面的校准过程，为IMU时间序列的自监督学习和基础学习提供了高质量数据支持。该数据集能够促进鲁棒表示学习，支持事件检测、运动模式识别和惯性导航等多种下游任务。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了IF-D，这是一个大规模惯性数据集，旨在支持IMU时间序列的自监督学习和基础学习。IF-D包含连续、长时间、多通道记录（加速度计、陀螺仪、磁力计），采样率为200Hz，使用安装在3D打印球形外壳内的UM7 IMU采集，该外壳在车辆行驶期间促进多样化的自由旋转。收集时长约135分钟，产生约160万个样本，涵盖九个传感器通道。我们描述了数据采集设置、预处理和校准程序（六方向加速度计校准、静止陀螺仪偏差估计，以及磁力计硬/软铁校正的椭球拟合），并提供了定量的校准结果。IF-D旨在减轻平台特定运动偏差，让模型接触物理动力学和典型测量噪声，从而促进鲁棒表示学习和下游任务，如事件检测、运动模式识别和惯性导航。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present IF-D, a large-scale inertial dataset designed to enableself-supervised and foundational learning for IMU time series. IF-D comprisescontinuous, long-duration multichannel recordings (accelerometer, gyroscope,magnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printedspherical enclosure that promotes diverse, free rotations during vehicletraversal. The collection spans approximately 135 minutes of recording,yielding around 1.6 million samples across nine sensor channels. We describethe data acquisition setup, preprocessing, and calibration procedures(six-orientation accelerometer calibration, stationary gyroscope biasestimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction),and provide quantitative calibration results. IF-D is designed to mitigateplatform specific motion bias and expose models to both physical dynamics andtypical measurement noise, thereby facilitating robust representation learningand downstream tasks such as event detection, motion mode recognition, andinertial navigation.</description>
      <author>example@mail.com (Patrick Ferreira, Paula Costa)</author>
      <guid isPermaLink="false">2510.09539v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>What Do Temporal Graph Learning Models Learn?</title>
      <link>http://arxiv.org/abs/2510.09416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了七种时间图学习模型捕捉时间图链接结构八种基本属性的能力，揭示了模型在捕捉某些属性上的优势和局限性，为时间图学习模型的应用提供了实践见解。&lt;h4&gt;背景&lt;/h4&gt;时间图学习已成为图表示学习的中心主题，最先进的模型在多个基准测试中表现出色。然而，最近的研究对基准测试结果的可靠性提出了质疑，指出了常用评估协议的问题以及简单启发式方法的竞争力，引发了对模型实际利用底层图哪些特性的疑问。&lt;h4&gt;目的&lt;/h4&gt;探究时间图学习模型实际利用了底层图的哪些特性进行预测，系统评估模型捕捉时间图链接结构相关基本属性的能力。&lt;h4&gt;方法&lt;/h4&gt;系统地评估七种时间图学习模型在捕捉八种与时间图链接结构相关的基本属性方面的能力。这些属性包括结构特性（如密度）、时间模式（如近期性）和边形成机制（如同质性）。研究使用了合成和真实世界数据集进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果呈现出复杂的图景：模型能够很好地捕捉某些属性，但无法重现其他属性。这暴露了模型在时间图学习方面的重要局限性。&lt;h4&gt;结论&lt;/h4&gt;研究结果为时间图学习模型的应用提供了实践见解，并激励时间图学习研究进行更多可解释性驱动的评估，以更好地理解模型的工作原理和局限性。&lt;h4&gt;翻译&lt;/h4&gt;时间图学习已成为图表示学习的中心主题，众多基准测试表明最先进模型的强大性能。然而，最近的工作对基准测试结果的可靠性提出了担忧，指出了常用评估协议的问题以及简单启发式方法的惊人竞争力。这种对比引发了关于时间图学习模型实际上利用了底层图的哪些特性来进行预测的问题。我们通过系统地评估七种模型捕捉时间图链接结构相关八种基本属性的能力来解决这一问题。这些属性包括结构特性如密度、时间模式如近期性，以及边形成机制如同质性。使用合成和真实世界数据集，我们分析了模型学习这些属性的程度。我们的研究结果呈现出复杂的图景：模型能够很好地捕捉某些属性，但无法重现其他属性。通过这一点，我们暴露了重要的局限性。总体而言，我们相信我们的结果为时间图学习模型的应用提供了实践见解，并激励时间图学习研究进行更多可解释性驱动的评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on temporal graphs has become a central topic in graphrepresentation learning, with numerous benchmarks indicating the strongperformance of state-of-the-art models. However, recent work has raisedconcerns about the reliability of benchmark results, noting issues withcommonly used evaluation protocols and the surprising competitiveness of simpleheuristics. This contrast raises the question of which properties of theunderlying graphs temporal graph learning models actually use to form theirpredictions. We address this by systematically evaluating seven models on theirability to capture eight fundamental attributes related to the link structureof temporal graphs. These include structural characteristics such as density,temporal patterns such as recency, and edge formation mechanisms such ashomophily. Using both synthetic and real-world datasets, we analyze how wellmodels learn these attributes. Our findings reveal a mixed picture: modelscapture some attributes well but fail to reproduce others. With this, we exposeimportant limitations. Overall, we believe that our results provide practicalinsights for the application of temporal graph learning models, and motivatemore interpretability-driven evaluations in temporal graph learning research.</description>
      <author>example@mail.com (Abigail J. Hayes, Tobias Schumacher, Markus Strohmaier)</author>
      <guid isPermaLink="false">2510.09416v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training</title>
      <link>http://arxiv.org/abs/2510.09405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;射频指纹识别(RFFI)是一种利用设备制造过程中引入的硬件级不完善性来实现精确发射器识别的关键技术。然而，接收器引起的变异性限制了深度神经网络在实际部署中的应用。作者提出了一种对抗训练和风格转移相结合的框架，能够分离发射器和接收器特征，提高跨接收器变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;射频指纹识别是无线网络安全的关键技术，它利用设备制造过程中引入的硬件级不完善性来实现精确的发射器识别。深度神经网络在提取判别性特征方面表现出色，但它们的实际部署受到接收器引起的变化性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决接收器引起的特征偏移问题，防止RFFI模型过度拟合接收器特定模式，提高模型在不同接收器环境下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出一种对抗接收器变化的RFFI框架，整合对抗训练和风格转移，明确分离发射器和接收器特征。通过强制执行域不变表示学习，将真实的硬件签名与接收器伪影隔离，确保对接收器变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在多接收器数据集上的广泛实验表明，该方法始终优于最先进的基线方法，在各种接收器设置下平均准确率提高了高达10%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了接收器变化导致的性能下降问题，显著提高了RFFI系统在实际部署中的鲁棒性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;射频指纹识别是一种关键技术，用于无线网络安全，它利用设备制造过程中引入的硬件级内在不完善性来实现精确的发射器识别。虽然深度神经网络在提取判别性特征方面表现出色，但它们的实际部署受到接收器引起的变化性的阻碍。在实践中，射频指纹信号包含发射器特定特征以及信道失真和接收器引起的偏差。尽管信道均衡可以减轻信道噪声，但接收器引起的特征偏移仍然在很大程度上未得到解决，导致RFFI模型过度拟合接收器特定模式。当训练和评估使用相同的接收器时，这一限制尤其成问题，因为在部署中更换接收器可能导致性能大幅下降。为了应对这一挑战，我们提出了一种对抗接收器变化的RFFI框架，整合对抗训练和风格转移，明确分离发射器和接收器特征。通过强制执行域不变表示学习，我们的方法将真实的硬件签名与接收器伪影隔离，确保对接收器变化的鲁棒性。在多接收器数据集上的广泛实验表明，我们的方法始终优于最先进的基线方法，在各种接收器设置下平均准确率提高了高达10%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radio frequency fingerprint identification (RFFI) is a critical technique forwireless network security, leveraging intrinsic hardware-level imperfectionsintroduced during device manufacturing to enable precise transmitteridentification. While deep neural networks have shown remarkable capability inextracting discriminative features, their real-world deployment is hindered byreceiver-induced variability. In practice, RF fingerprint signals comprisetransmitter-specific features as well as channel distortions andreceiver-induced biases. Although channel equalization can mitigate channelnoise, receiver-induced feature shifts remain largely unaddressed, causing theRFFI models to overfit to receiver-specific patterns. This limitation isparticularly problematic when training and evaluation share the same receiver,as replacing the receiver in deployment can cause substantial performancedegradation. To tackle this challenge, we propose an RFFI framework robust tocross-receiver variability, integrating adversarial training and style transferto explicitly disentangle transmitter and receiver features. By enforcingdomain-invariant representation learning, our method isolates genuine hardwaresignatures from receiver artifacts, ensuring robustness against receiverchanges. Extensive experiments on multi-receiver datasets demonstrate that ourapproach consistently outperforms state-of-the-art baselines, achieving up to a10% improvement in average accuracy across diverse receiver settings.</description>
      <author>example@mail.com (Yuhao Pan, Xiucheng Wang, Nan Cheng, Wenchao Xu)</author>
      <guid isPermaLink="false">2510.09405v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Music Sample Identification with Multi-Track Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.11507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督学习的音频采样识别方法，通过多轨数据集创建人工混合正样本对，并设计新型对比学习目标，显著提高了采样内容检测和原始素材检索的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;采样是现代音乐制作中常见的技术，指利用现有音频片段创建新的音乐内容。自动识别采样内容是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动样本识别系统，能够检测采样内容并检索其原始素材来源。&lt;h4&gt;方法&lt;/h4&gt;采用自监督学习方法，利用多轨数据集创建人工混合的正样本对，设计了一种新的对比学习目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著优于先前最先进的基线方法，对各种音乐类型具有鲁棒性，并在参考数据库规模扩大时表现出良好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;详细分析了训练流程中不同组件的贡献，特别强调了高质量分离音轨对于此任务的必要性。&lt;h4&gt;翻译&lt;/h4&gt;采样，即利用现有音频片段创建新音乐内容的技术，在现代音乐制作中非常普遍。在本文中，我们解决了自动样本识别这一具有挑战性的任务，即检测采样内容并检索其原始素材。为此，我们采用了一种自监督学习方法，利用多轨数据集创建人工混合的正样本对，并设计了一种新颖的对比学习目标。我们证明该方法显著优于先前最先进的基线方法，对各种音乐类型具有鲁棒性，并且在增加参考数据库中的噪音歌曲数量时具有良好的可扩展性。此外，我们详细分析了训练流程中不同组件的贡献，并特别强调了高质量分离音轨对于此任务的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sampling, the technique of reusing pieces of existing audio tracks to createnew music content, is a very common practice in modern music production. Inthis paper, we tackle the challenging task of automatic sample identification,that is, detecting such sampled content and retrieving the material from whichit originates. To do so, we adopt a self-supervised learning approach thatleverages a multi-track dataset to create positive pairs of artificial mixes,and design a novel contrastive learning objective. We show that such methodsignificantly outperforms previous state-of-the-art baselines, that is robustto various genres, and that scales well when increasing the number of noisesongs in the reference database. In addition, we extensively analyze thecontribution of the different components of our training pipeline andhighlight, in particular, the need for high-quality separated stems for thistask.</description>
      <author>example@mail.com (Alain Riou, Joan Serrà, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2510.11507v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features</title>
      <link>http://arxiv.org/abs/2510.11223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了仅通过面部表情的动态成分而非静态面部外观是否能够识别个体。研究使用FLAME 3D可变形模型分离面部形状和表情动态，从对话视频中提取参数。在包含1,429名说话者的CANDOR数据集上，使用带有监督对比学习的Conformer模型实现61.14%的识别准确率，证明面部动态包含强烈的身份特征。研究还引入了漂移-噪声比率(DNR)指标来量化形状-表情分离的可靠性，发现DNR与识别性能呈负相关。研究结果表明对话面部动态中存在特定于个人的特征，对社会感知和临床评估具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;面部识别研究通常关注静态面部特征，而对面部表情动态与身份识别的关系研究较少。&lt;h4&gt;目的&lt;/h4&gt;探究是否仅通过面部表情的动态成分就能识别个体，而不依赖于静态面部外观。&lt;h4&gt;方法&lt;/h4&gt;使用FLAME 3D可变形模型实现面部形状和表情动态的分离，从对话视频中逐帧提取参数并仅保留表情和下颌系数。应用带有监督对比学习的Conformer模型进行1,429路分类任务。引入漂移-噪声比率(DNR)指标量化形状-表情分离的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 面部动态携带强烈的身份特征，识别准确率达到61.14%，是随机猜测的458倍；2) 漂移-噪声比率(DNR)与识别性能呈强负相关，表明不稳定的形状估计会损害动态识别能力；3) 对话面部动态中存在特定于个人的特征。&lt;h4&gt;结论&lt;/h4&gt;面部表情的动态成分包含足够的信息用于个体识别，这种能力对社会感知研究和临床评估具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查个体是否仅能通过面部表情的纯动态成分被识别，而独立于静态面部外观。我们利用FLAME 3D可变形模型来实现面部形状和表情动态之间的明确分离，从对话视频中逐帧提取参数，同时仅保留表情和下颌系数。在包含1,429名说话者在自然对话中的CANDOR数据集上，我们采用监督对比学习的Conformer模型在1,429路分类任务上达到61.14%的准确率——比随机猜测高458倍——证明了面部动态携带强烈的身份特征。我们引入了漂移-噪声比率(DNR)，通过测量跨会话形状变化相对于会话内变异性来量化形状-表情分离的可靠性。DNR与识别性能呈强负相关，证实不稳定的形状估计会损害动态识别。我们的发现揭示了对话面部动态中存在特定于个人的特征，对社会感知和临床评估有启示意义。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要研究是否可以仅通过面部表情的动态成分（即面部运动的方式）来识别一个人，而不依赖于静态的面部外观特征。这个问题在临床评估中很重要，因为可以区分个人特定的表情模式与神经系统症状；在社会互动研究中可以区分个人风格与情感内容；在技术系统中可以创建更自然的个性化虚拟角色动画。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过使用FLAME 3D可变形模型来实现面部形状和表情动态的显式分离，从对话视频中逐帧提取参数，只保留表情和下颌系数。他们借鉴了FLAME模型用于参数化面部，VGGHeads用于从2D视频估计参数，Conformer模型结合自注意力和卷积进行时间建模，以及监督对比学习来学习判别性表示。这些现有技术被创新性地组合以解决身份识别问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是每个人的面部表情动态（如微笑、说话和情绪表达的方式）具有独特的个体特征，这些特征可以作为身份识别的信号，即使不考虑静态的面部外观。整体流程包括：1)使用VGGHeads从视频帧提取FLAME参数；2)只保留动态成分（表情系数和下颌旋转参数）；3)使用监督对比学习训练Conformer模型处理这些动态序列；4)训练线性分类器进行身份识别；5)引入漂移-噪声比(DNR)量化特征分离质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次大规模证明纯面部动态（完全分离外观后）包含身份信号；2)证明Conformer的混合架构能最优捕获多尺度面部模式；3)提出漂移-噪声比(DNR)度量量化分离质量；4)全面分析基于动态识别的上下文和数据需求。相比之前工作，早期方法混合了外观和运动，无法确定识别是基于运动还是几何，而本文通过FLAME模型实现数学上的形状和表情分离，确保分析仅关注动态行为。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次证明可以通过纯面部动态特征（独立于静态面部外观）实现高精度身份识别，并提出了新的度量方法评估特征分离质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates whether individuals can be identified solely throughthe pure dynamical components of their facial expressions, independent ofstatic facial appearance. We leverage the FLAME 3D morphable model to achieveexplicit disentanglement between facial shape and expression dynamics,extracting frame-by-frame parameters from conversational videos while retainingonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakersin naturalistic conversations, our Conformer model with supervised contrastivelearning achieves 61.14\%accuracy on 1,429-way classification -- 458 timesabove chance -- demonstrating that facial dynamics carry strong identitysignatures. We introduce a drift-to-noise ratio (DNR) that quantifies thereliability of shape expression separation by measuring across-session shapechanges relative to within-session variability. DNR strongly negativelycorrelates with recognition performance, confirming that unstable shapeestimation compromises dynamic identification. Our findings revealperson-specific signatures in conversational facial dynamics, with implicationsfor social perception and clinical assessment.</description>
      <author>example@mail.com (Masoumeh Chapariniya, Pierre Vuillecard, Jean-Marc Odobez, Volker Dellwo, Teodora Vukovic)</author>
      <guid isPermaLink="false">2510.11223v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos</title>
      <link>http://arxiv.org/abs/2510.11204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at CVPR 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于类原型的监督对比学习方法，用于检测在线视频中的教育内容，特别是识字和数学两个类别。该方法使用多模态Transformer网络捕捉视频中的视觉和音频线索交互，并在新创建的APPROVE数据集上进行了验证，结果表明该方法优于现有基线。&lt;h4&gt;背景&lt;/h4&gt;儿童早期在线媒体消费的增长需要数据驱动工具来帮助教育工作者筛选适合的教育内容。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测在线视频中教育内容的方法，特别关注识字和数学两个广泛使用的教育内容类别。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于类原型的监督对比学习方法，将其视为细粒度多标签分类问题。使用多模态Transformer网络捕捉视频中的视觉和音频线索之间的交互。学习每个类别的类原型，通过损失函数最小化类原型与其样本之间的距离，同时最大化类原型与其他类别样本之间的距离。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在APPROVE数据集（包含193小时专家标注的视频，共19个类别）和其他基准测试（如Youtube-8M和COIN）上优于强大的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效处理多标签细粒度样本，为教育工作者筛选适合的教育内容提供了有效工具。APPROVE数据集已在https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE公开可用。&lt;h4&gt;翻译&lt;/h4&gt;儿童早期在线媒体消费的近期增长需要数据驱动工具，使教育工作者能够为幼儿筛选适当的教育内容。本文提出了一种检测在线视频中教育内容的方法。我们专注于两个广泛使用的教育内容类别：识字和数学。对于每个类别，我们基于共同核心标准选择突出的代码（子类）。例如，识字代码包括'字母名称'、'字母发音'，数学代码包括'计数'、'分类'。我们将此视为细粒度多标签分类问题，因为视频可能包含多种类型的教育内容，且内容类别可能在视觉上相似（例如，'字母名称'与'字母发音'）。我们提出了一种新颖的基于类原型的监督对比学习方法，能够处理与多个标签相关的细粒度样本。我们为每个类别学习一个类原型，并采用损失函数来最小化类原型与其类别样本之间的距离。同样，类原型与其他类别样本之间的距离被最大化。由于视觉和音频线索的 alignment 对有效理解至关重要，我们考虑使用多模态Transformer网络在视频学习嵌入的同时捕捉视频中视觉和音频线索之间的交互。为了评估，我们提出了一个数据集APPROVE，使用YouTube上的教育视频，由教育研究人员用细粒度教育类别进行标注。APPROVE包含193小时专家标注的视频，共19个类别。提出的方法在APPROVE和其他基准测试（如Youtube-8M和COIN）上优于强大的基线方法。数据集可在https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent growth in the consumption of online media by children during earlychildhood necessitates data-driven tools enabling educators to filter outappropriate educational content for young learners. This paper presents anapproach for detecting educational content in online videos. We focus on twowidely used educational content classes: literacy and math. For each class, wechoose prominent codes (sub-classes) based on the Common Core Standards. Forexample, literacy codes include `letter names', `letter sounds', and math codesinclude `counting', `sorting'. We pose this as a fine-grained multilabelclassification problem as videos can contain multiple types of educationalcontent and the content classes can get visually similar (e.g., `letter names'vs `letter sounds'). We propose a novel class prototypes based supervisedcontrastive learning approach that can handle fine-grained samples associatedwith multiple labels. We learn a class prototype for each class and a lossfunction is employed to minimize the distances between a class prototype andthe samples from the class. Similarly, distances between a class prototype andthe samples from other classes are maximized. As the alignment between visualand audio cues are crucial for effective comprehension, we consider amultimodal transformer network to capture the interaction between visual andaudio cues in videos while learning the embedding for videos. For evaluation,we present a dataset, APPROVE, employing educational videos from YouTubelabeled with fine-grained education classes by education researchers. APPROVEconsists of 193 hours of expert-annotated videos with 19 classes. The proposedapproach outperforms strong baselines on APPROVE and other benchmarks such asYoutube-8M, and COIN. The dataset is available athttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE</description>
      <author>example@mail.com (Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah)</author>
      <guid isPermaLink="false">2510.11204v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities</title>
      <link>http://arxiv.org/abs/2510.11110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysioME是一种鲁棒框架，能够在生理信号医疗应用中处理模态缺失问题，通过多模态自监督学习、专门设计的网络架构和恢复解码器，确保在各种缺失场景下的可靠性能。&lt;h4&gt;背景&lt;/h4&gt;生理信号在医疗应用中经常出现缺失或损坏的情况，这主要是由于硬件限制或运动伪影造成的。然而，大多数现有方法假设所有模态都可用，这导致在任何模态缺失时性能会显著下降。&lt;h4&gt;目的&lt;/h4&gt;克服模态缺失导致性能下降的局限性，提出PhysioME框架，确保在模态缺失条件下保持可靠性能。&lt;h4&gt;方法&lt;/h4&gt;PhysioME采用了三种主要方法：(1) 多模态自监督学习方法，结合对比学习和掩码预测；(2) 专门用于捕捉每种生理信号模态时间动态的Dual-Path NeuroNet主干网络；(3) 恢复解码器，用于重建缺失的模态令牌，能够灵活处理不完整的输入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PhysioME在各种模态缺失场景下都实现了高度一致性和泛化性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了PhysioME作为可靠工具的潜力，可以在数据不完善的现实环境中支持临床决策。&lt;h4&gt;翻译&lt;/h4&gt;缺失或损坏的模态在基于生理信号的医疗应用中很常见，这是由于硬件限制或运动伪影造成的。然而，大多数现有方法假设所有模态都可用，这导致在任何模态缺失时性能显著下降。为了克服这一局限性，本研究提出了PhysioME，一个鲁棒框架，旨在确保在模态缺失条件下保持可靠性能。PhysioME采用：(1) 多模态自监督学习方法，结合对比学习和掩码预测；(2) 专门用于捕捉每种生理信号模态时间动态的Dual-Path NeuroNet主干网络；(3) 恢复解码器，用于重建缺失的模态令牌，能够灵活处理不完整的输入。实验结果表明，PhysioME在各种模态缺失场景下都实现了高度一致性和泛化性能。这些发现强调了PhysioME作为可靠工具的潜力，可以在数据不完善的现实环境中支持临床决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Missing or corrupted modalities are common in physiological signal-basedmedical applications owing to hardware constraints or motion artifacts.However, most existing methods assume the availability of all modalities,resulting in substantial performance degradation in the absence of anymodality. To overcome this limitation, this study proposes PhysioME, a robustframework designed to ensure reliable performance under missing modalityconditions. PhysioME adopts: (1) a multimodal self-supervised learning approachthat combines contrastive learning with masked prediction; (2) aDual-PathNeuroNet backbone tailored to capture the temporal dynamics of eachphysiological signal modality; and (3) a restoration decoder that reconstructsmissing modality tokens, enabling flexible processing of incomplete inputs. Theexperimental results show that PhysioME achieves high consistency andgeneralization performance across various missing modality scenarios. Thesefindings highlight the potential of PhysioME as a reliable tool for supportingclinical decision-making in real-world settings with imperfect dataavailability.</description>
      <author>example@mail.com (Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim)</author>
      <guid isPermaLink="false">2510.11110v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Source-Free Object Detection with Detection Transformer</title>
      <link>http://arxiv.org/abs/2510.11090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Transactions on Image Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FRANCK(Feature Reweighting ANd Contrastive Learning NetworK)，一种专门为DETR架构设计的源域无目标检测(SFOD)框架，通过四个关键组件实现查询为中心的特征增强，有效提升了模型在目标域的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;源域无目标检测(SFOD)允许在不访问源数据的情况下，将知识从源域转移到无监督的目标域进行目标检测。然而，现有方法要么局限于传统目标检测模型(如Faster R-CNN)，要么缺乏针对新型目标检测架构(尤其是Detection Transformer/DETR)的专门适配。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门为DETR模型设计的SFOD框架，解决现有方法在新型目标检测架构上的局限性，提高目标检测在目标域的性能。&lt;h4&gt;方法&lt;/h4&gt;FRANCK框架包含四个关键组件：(1)基于目标性评分的样本重加权(OSSR)模块，通过计算注意力目标性评分并重加权检测损失来强调难以识别区域；(2)基于匹配的记忆库对比学习(CMMB)模块，集成多级特征到记忆库中增强类别对比学习；(3)不确定性加权的查询融合特征蒸馏(UQFD)模块，通过预测质量重加权和查询特征融合改进特征蒸馏；(4)改进的自训练流程，具有动态教师更新间隔(DTUI)以优化伪标签质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过这些组件，FRANCK有效将源预训练的DETR模型适应到目标域，显著增强了模型的鲁棒性和泛化能力。在多个基准测试上的广泛实验表明，该方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FRANCK是一种有效的、与基于DETR的SFOD模型兼容的方法，为目标检测领域特别是DETR架构的源域自适应提供了创新解决方案，具有显著的应用价值和兼容性。&lt;h4&gt;翻译&lt;/h4&gt;源域无目标检测(SFOD)使知识能够从源域转移到无监督的目标域进行目标检测，而无需访问源数据。大多数现有的SFOD方法要么局限于传统的目标检测(OD)模型(如Faster R-CNN)，要么被设计为通用解决方案，没有针对新型OD架构(尤其是Detection Transformer/DETR)进行专门适配。在本文中，我们引入了特征重加权与对比学习网络(FRANCK)，一种新型SFOD框架，专门为DETR执行以查询为中心的特征增强。FRANCK包含四个关键组件：(1)基于目标性评分的样本重加权(OSSR)模块，在多尺度编码器特征图上计算基于注意力的目标性评分，对检测损失进行重加权，以强调难以识别的区域；(2)基于匹配的记忆库对比学习(CMMB)模块，将多级特征集成到记忆库中，增强类别的对比学习；(3)不确定性加权的查询融合特征蒸馏(UQFD)模块，通过预测质量重加权和查询特征融合来改进特征蒸馏；(4)改进的自训练流程，具有动态教师更新间隔(DTUI)，优化伪标签质量。通过利用这些组件，FRANCK有效地将源预训练的DETR模型适应到目标域，增强了鲁棒性和泛化能力。在几个广泛使用的基准测试上的广泛实验表明，我们的方法达到了最先进的性能，突显了其有效性与基于DETR的SFOD模型的兼容性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TIP.2025.3607621&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-Free Object Detection (SFOD) enables knowledge transfer from a sourcedomain to an unsupervised target domain for object detection without access tosource data. Most existing SFOD approaches are either confined to conventionalobject detection (OD) models like Faster R-CNN or designed as general solutionswithout tailored adaptations for novel OD architectures, especially DetectionTransformer (DETR). In this paper, we introduce Feature Reweighting ANdContrastive Learning NetworK (FRANCK), a novel SFOD framework specificallydesigned to perform query-centric feature enhancement for DETRs. FRANCKcomprises four key components: (1) an Objectness Score-based Sample Reweighting(OSSR) module that computes attention-based objectness scores on multi-scaleencoder feature maps, reweighting the detection loss to emphasizeless-recognized regions; (2) a Contrastive Learning with Matching-based MemoryBank (CMMB) module that integrates multi-level features into memory banks,enhancing class-wise contrastive learning; (3) an Uncertainty-weightedQuery-fused Feature Distillation (UQFD) module that improves featuredistillation through prediction quality reweighting and query feature fusion;and (4) an improved self-training pipeline with a Dynamic Teacher UpdatingInterval (DTUI) that optimizes pseudo-label quality. By leveraging thesecomponents, FRANCK effectively adapts a source-pre-trained DETR model to atarget domain with enhanced robustness and generalization. Extensiveexperiments on several widely used benchmarks demonstrate that our methodachieves state-of-the-art performance, highlighting its effectiveness andcompatibility with DETR-based SFOD models.</description>
      <author>example@mail.com (Huizai Yao, Sicheng Zhao, Shuo Lu, Hui Chen, Yangyang Li, Guoping Liu, Tengfei Xing, Chenggang Yan, Jianhua Tao, Guiguang Ding)</author>
      <guid isPermaLink="false">2510.11090v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation</title>
      <link>http://arxiv.org/abs/2510.11036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;XGrasp是一个实时的夹爪感知抓取检测框架，能够高效处理多种夹爪配置，解决了传统机器人抓取方法仅针对单一夹爪类型的问题。&lt;h4&gt;背景&lt;/h4&gt;大多数机器人抓取方法通常针对单一夹爪类型设计，这在需要多样化末端执行器的现实世界场景中限制了它们的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理多种夹爪配置的实时抓取检测框架，提高机器人在多样化环境中的抓取能力。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地为现有数据集添加多夹爪注释解决数据稀缺问题；采用分层两阶段架构，包括使用全局场景信息和夹爪规格确定最佳位置的抓取点预测器(GPP)，以及使用局部特征细化抓取角度和宽度的角度-宽度预测器(AWP)；在AWP模块中使用对比学习实现零样本泛化；模块化框架与视觉基础模型无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示XGrasp在各种夹爪类型上具有竞争力的抓取成功率，同时与现有的夹爪感知方法相比，在推理速度上有了显著提高。&lt;h4&gt;结论&lt;/h4&gt;XGrasp提供了一个有效的解决方案，使机器人能够适应不同的夹爪配置，并在性能和计算效率上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;大多数机器人抓取方法通常针对单一夹爪类型设计，这限制了它们在需要多样化末端执行器的现实世界场景中的适用性。我们提出了XGrasp，一个实时的夹爪感知抓取检测框架，能够高效处理多种夹爪配置。该方法通过系统性地为现有数据集添加多夹爪注释来解决数据稀缺问题。XGrasp采用分层两阶段架构。在第一阶段，抓取点预测器(GPP)使用全局场景信息和夹爪规格确定最佳位置。在第二阶段，角度-宽度预测器(AWP)使用局部特征细化抓取角度和宽度。AWP模块中的对比学习通过学习基本抓取特征，实现对未见夹爪的零样本泛化。模块化框架与视觉基础模型无缝集成，为未来的视觉语言能力提供途径。实验结果表明，在各种夹爪类型上具有竞争力的抓取成功率，同时与现有的夹爪感知方法相比，在推理速度上取得了显著提高。项目页面：https://sites.google.com/view/xgrasp&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most robotic grasping methods are typically designed for single grippertypes, which limits their applicability in real-world scenarios requiringdiverse end-effectors. We propose XGrasp, a real-time gripper-aware graspdetection framework that efficiently handles multiple gripper configurations.The proposed method addresses data scarcity by systematically augmentingexisting datasets with multi-gripper annotations. XGrasp employs a hierarchicaltwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)identifies optimal locations using global scene information and gripperspecifications. In the second stage, an Angle-Width Predictor (AWP) refines thegrasp angle and width using local features. Contrastive learning in the AWPmodule enables zero-shot generalization to unseen grippers by learningfundamental grasping characteristics. The modular framework integratesseamlessly with vision foundation models, providing pathways for futurevision-language capabilities. The experimental results demonstrate competitivegrasp success rates across various gripper types, while achieving substantialimprovements in inference speed compared to existing gripper-aware methods.Project page: https://sites.google.com/view/xgrasp</description>
      <author>example@mail.com (Yeonseo Lee, Jungwook Mun, Hyosup Shin, Guebin Hwang, Junhee Nam, Taeyeop Lee, Sungho Jo)</author>
      <guid isPermaLink="false">2510.11036v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Joint Learning Approach to Hardware Caching and Prefetching</title>
      <link>http://arxiv.org/abs/2510.10862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ML for Systems Workshop at the 39th Conference on Neural  Information Processing Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了硬件缓存领域中缓存替换和预取策略之间的相互依赖关系，提出了一种联合学习方法，通过共享表示来同时训练这两种策略，并展示了两种实现方法的初步结果。&lt;h4&gt;背景&lt;/h4&gt;现代系统中已提出多种学习方法替代启发式算法用于调度、缓存等系统组件，这些模型利用多样特征、历史学习和行为预测来应对工作负载动态性和硬件发展。&lt;h4&gt;目的&lt;/h4&gt;研究缓存替换和预取策略之间的双向相互依赖关系，提出并验证联合训练这两种策略的方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于特征共享表示的联合学习方法，包括两种实现方式：基于联合编码器和基于嵌入对比学习的方法。&lt;h4&gt;主要发现&lt;/h4&gt;两种共享表示开发方法都展示了有希望的初步结果，证明了联合训练策略的有效性。&lt;h4&gt;结论&lt;/h4&gt;为基于相互依赖关系的系统策略联合研究方向制定了未来研究议程。&lt;h4&gt;翻译&lt;/h4&gt;已经提出了多种学习策略来替代现代系统中的调度、缓存和其他系统组件的启发式算法。通过利用多样特征、从历史趋势学习和预测未来行为，这类模型有望跟上不断增加的工作负载动态性和持续的硬件发展。然而，单独训练的策略在组合使用时可能仍然表现不佳。本文在硬件缓存领域研究了一个这样的实例——针对缓存替换和预取策略。我们认为这两种策略是双向相互依赖的，并论证了联合训练这两种策略的必要性。我们提出了一种联合学习方法，基于为两种策略使用的特征开发共享表示。我们介绍了两种开发这些共享表示的方法，一种基于联合编码器，另一种基于嵌入的对比学习，并展示了这两种方法都取得了有希望的初步结果。最后，我们为这一方向的未来研究制定了议程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several learned policies have been proposed to replace heuristics forscheduling, caching, and other system components in modern systems. Byleveraging diverse features, learning from historical trends, and predictingfuture behaviors, such models promise to keep pace with ever-increasingworkload dynamism and continuous hardware evolution. However, policies trainedin isolation may still achieve suboptimal performance when placed together. Inthis paper, we inspect one such instance in the domain of hardware caching --for the policies of cache replacement and prefetching. We argue that these twopolicies are bidirectionally interdependent and make the case for training thetwo jointly. We propose a joint learning approach based on developing sharedrepresentations for the features used by the two policies. We present twoapproaches to develop these shared representations, one based on a jointencoder and another based on contrastive learning of the embeddings, anddemonstrate promising preliminary results for both of these. Finally, we laydown an agenda for future research in this direction.</description>
      <author>example@mail.com (Samuel Yuan, Divyanshu Saxena, Jiayi Chen, Nihal Sharma, Aditya Akella)</author>
      <guid isPermaLink="false">2510.10862v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SS-DPPN: A self-supervised dual-path foundation model for the generalizable cardiac audio representation</title>
      <link>http://arxiv.org/abs/2510.10719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自监督双路径原型网络（SS-DPPN），用于从无标签数据中进行心脏音频表示和分类的基础模型，解决了监督式深度学习受限于专家标注数据稀缺性的问题。&lt;h4&gt;背景&lt;/h4&gt;心音图的自动分析对心血管疾病的早期诊断至关重要，但监督式深度学习常受限于专家标注数据的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从无标签数据中进行心脏音频表示和分类的基础模型，减少对专家标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出自监督双路径原型网络（SS-DPPN），采用基于双路径对比学习的架构，同时处理一维波形和二维频谱图，使用新型混合损失函数；下游任务采用基于度量学习的原型网络方法，提高敏感性并产生校准良好且可信赖的预测。&lt;h4&gt;主要发现&lt;/h4&gt;SS-DPPN在四个心脏音频基准测试上达到最先进性能；在数据效率方面表现出色，标注数据减少三倍；学习到的表示在肺部声音分类和心率估计任务上成功泛化。&lt;h4&gt;结论&lt;/h4&gt;实验和发现验证了SS-DPPN作为生理信号的一种强大、可靠且可扩展的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;心音图的自动分析对心血管疾病的早期诊断至关重要，然而监督式深度学习常受限于专家标注数据的稀缺性。在本文中，我们提出了自监督双路径原型网络（SS-DPPN），这是一种用于从无标签数据进行心脏音频表示和分类的基础模型。该框架引入了一种基于双路径对比学习的架构，同时使用新型混合损失函数处理一维波形和二维频谱图。对于下游任务，使用基于度量学习的原型网络方法，提高了敏感性并产生校准良好且可信赖的预测。SS-DPPN在四个心脏音频基准测试上实现了最先进的性能。该框架在数据效率方面表现出色，与全监督模型相比，标注数据减少三倍。最后，学习到的表示在肺部声音分类和心率估计任务上成功泛化。我们的实验和发现验证了SS-DPPN作为生理信号的一种强大、可靠且可扩展的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automated analysis of phonocardiograms is vital for the early diagnosisof cardiovascular disease, yet supervised deep learning is often constrained bythe scarcity of expert-annotated data. In this paper, we propose theSelf-Supervised Dual-Path Prototypical Network (SS-DPPN), a foundation modelfor cardiac audio representation and classification from unlabeled data. Theframework introduces a dual-path contrastive learning based architecture thatsimultaneously processes 1D waveforms and 2D spectrograms using a novel hybridloss. For the downstream task, a metric-learning approach using a PrototypicalNetwork was used that enhances sensitivity and produces well-calibrated andtrustworthy predictions. SS-DPPN achieves state-of-the-art performance on fourcardiac audio benchmarks. The framework demonstrates exceptional dataefficiency with a fully supervised model on three-fold reduction in labeleddata. Finally, the learned representations generalize successfully across lungsound classification and heart rate estimation. Our experiments and findingsvalidate SS-DPPN as a robust, reliable, and scalable foundation model forphysiological signals.</description>
      <author>example@mail.com (Ummy Maria Muna, Md Mehedi Hasan Shawon, Md Jobayer, Sumaiya Akter, Md Rakibul Hasan, Md. Golam Rabiul Alam)</author>
      <guid isPermaLink="false">2510.10719v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</title>
      <link>http://arxiv.org/abs/2510.10633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多智能体强化学习框架，用于解决多模态文本到图像生成中语义对齐和专业细节保持的难题。&lt;h4&gt;背景&lt;/h4&gt;多模态文本到图像生成面临保持语义对齐和专业级别细节的困难，特别是在不同视觉领域之间。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够协调领域专业化智能体的多智能体强化学习框架，提升跨视觉域的语义对齐和专业细节生成能力。&lt;h4&gt;方法&lt;/h4&gt;构建包含文本增强模块和图像生成模块的双耦合子系统，每个模块配备多模态集成组件；使用近端策略优化(PPO)训练智能体，通过复合奖励函数平衡语义相似性、视觉质量和内容多样性；采用对比学习、双向注意和迭代反馈实现跨模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;系统显著丰富了生成内容（字数增加1614%），同时降低ROUGE-1分数69.7%；基于Transformer的融合方法获得最高复合分数(0.521)，但存在稳定性问题；多模态集成一致性中等(0.444-0.481)，反映跨模态语义基础的持续挑战。&lt;h4&gt;结论&lt;/h4&gt;协作的、专业化驱动的架构在推进可靠的多模态生成系统方面显示出广阔前景。&lt;h4&gt;翻译&lt;/h4&gt;多模态文本到图像生成仍然受到在多样化视觉领域中保持语义对齐和专业级别细节的困难限制。我们提出了一种多智能体强化学习框架，在两个耦合子系统中协调领域专业化智能体（例如，专注于建筑、肖像和风景图像）：文本增强模块和图像生成模块，每个模块都增加了多模态集成组件。智能体在复合奖励函数下使用近端策略优化(PPO)进行训练，该函数平衡语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意和文本与图像之间的迭代反馈来强制跨模态对齐。在六种实验设置中，我们的系统显著丰富了生成内容（字数增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略获得最高复合分数(0.521)，尽管偶尔存在稳定性问题。多模态集成产生中等一致性（范围从0.444到0.481），反映了跨模态语义基础的持续挑战。这些研究结果强调了协作的、专业化驱动的架构在推进可靠多模态生成系统方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal text-to-image generation remains constrained by the difficulty ofmaintaining semantic alignment and professional-level detail across diversevisual domains. We propose a multi-agent reinforcement learning framework thatcoordinates domain-specialized agents (e.g., focused on architecture,portraiture, and landscape imagery) within two coupled subsystems: a textenhancement module and an image generation module, each augmented withmultimodal integration components. Agents are trained using Proximal PolicyOptimization (PPO) under a composite reward function that balances semanticsimilarity, linguistic visual quality, and content diversity. Cross-modalalignment is enforced through contrastive learning, bidirectional attention,and iterative feedback between text and image. Across six experimentalsettings, our system significantly enriches generated content (word countincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusionmethods, Transformer-based strategies achieve the highest composite score(0.521), despite occasional stability issues. Multimodal ensembles yieldmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistentchallenges of cross-modal semantic grounding. These findings underscore thepromise of collaborative, specialization-driven architectures for advancingreliable multimodal generative systems.</description>
      <author>example@mail.com (Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li)</author>
      <guid isPermaLink="false">2510.10633v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2510.10564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MGSD-WSS的多粒度序列去噪方法，通过弱监督信号同时处理项目粒度和兴趣粒度噪声，显著提升了序列推荐性能。&lt;h4&gt;背景&lt;/h4&gt;序列推荐基于用户历史交互序列预测下一个项目，但历史序列中的不相关噪声项严重影响推荐效果。现有无监督方法缺乏明确噪声标签，容易误判用户感兴趣项目，且仅关注项目粒度噪声而忽略兴趣粒度噪声。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中噪声标签缺乏导致的误判问题，以及忽略兴趣粒度噪声的问题，实现更全面有效的序列去噪推荐。&lt;h4&gt;方法&lt;/h4&gt;提出MGSD-WSS方法，包含多核感知器模块映射序列到共同表示空间，利用弱监督信号识别噪声项，通过带噪声加权对比学习的项目粒度去噪模块处理项目噪声，提取目标兴趣表示处理兴趣粒度噪声，最后基于去噪后的项目和兴趣表示预测下一项目。&lt;h4&gt;主要发现&lt;/h4&gt;在五个数据集上的大量实验表明，所提出的方法显著优于最先进的序列推荐和去噪模型。&lt;h4&gt;结论&lt;/h4&gt;MGSD-WSS能有效解决现有序列推荐中的噪声问题，提升推荐性能，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;Sequential recommendation aims to predict the next item based on user interests in historical interaction sequences. Historical interaction sequences often contain irrelevant noisy items, which significantly hinders the performance of recommendation systems. Existing research employs unsupervised methods that indirectly identify item-granularity irrelevant noise by predicting the ground truth item. Since these methods lack explicit noise labels, they are prone to misidentify users' interested items as noise. Additionally, while these methods focus on removing item-granularity noise driven by the ground truth item, they overlook interest-granularity noise, limiting their ability to perform broader denoising based on user interests. To address these issues, we propose Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation (MGSD-WSS). MGSD-WSS first introduces the Multiple Gaussian Kernel Perceptron module to map the original and enhance sequence into a common representation space and utilizes weakly supervised signals to accurately identify noisy items in the historical interaction sequence. Subsequently, it employs the item-granularity denoising module with noise-weighted contrastive learning to obtain denoised item representations. Then, it extracts target interest representations from the ground truth item and applies noise-weighted contrastive learning to obtain denoised interest representations. Finally, based on the denoised item and interest representations, MGSD-WSS predicts the next item. Extensive experiments on five datasets demonstrate that the proposed method significantly outperforms state-of-the-art sequence recommendation and denoising models. Our code is available at https://github.com/lalunex/MGSD-WSS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation aims to predict the next item based on userinterests in historical interaction sequences. Historical interaction sequencesoften contain irrelevant noisy items, which significantly hinders theperformance of recommendation systems. Existing research employs unsupervisedmethods that indirectly identify item-granularity irrelevant noise bypredicting the ground truth item. Since these methods lack explicit noiselabels, they are prone to misidentify users' interested items as noise.Additionally, while these methods focus on removing item-granularity noisedriven by the ground truth item, they overlook interest-granularity noise,limiting their ability to perform broader denoising based on user interests. Toaddress these issues, we propose Multi-Granularity Sequence Denoising withWeakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSSfirst introduces the Multiple Gaussian Kernel Perceptron module to map theoriginal and enhance sequence into a common representation space and utilizesweakly supervised signals to accurately identify noisy items in the historicalinteraction sequence. Subsequently, it employs the item-granularity denoisingmodule with noise-weighted contrastive learning to obtain denoised itemrepresentations. Then, it extracts target interest representations from theground truth item and applies noise-weighted contrastive learning to obtaindenoised interest representations. Finally, based on the denoised item andinterest representations, MGSD-WSS predicts the next item. Extensiveexperiments on five datasets demonstrate that the proposed method significantlyoutperforms state-of-the-art sequence recommendation and denoising models. Ourcode is available at https://github.com/lalunex/MGSD-WSS.</description>
      <author>example@mail.com (Liang Li, Zhou Yang, Xiaofei Zhu)</author>
      <guid isPermaLink="false">2510.10564v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction</title>
      <link>http://arxiv.org/abs/2510.10490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 Pages, Plus Appendices, EACL 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了VOLTAGE，一种基于对比学习的OCR方法，用于低资源语言（特别是Takri文字）的数字化保护，以防止语言灭绝。&lt;h4&gt;背景&lt;/h4&gt;全球7000种语言中有2500种被列为濒危语言，语言流失导致传统智慧、民间文学和社区本质的丧失。低资源语言面临更高的灭绝风险，而缺乏针对低资源语言的无监督OCR方法是阻碍其数字化的原因之一。&lt;h4&gt;目的&lt;/h4&gt;为低资源语言提供数字包容性，避免语言灭绝，开发适用于低资源语言的OCR方法。&lt;h4&gt;方法&lt;/h4&gt;提出VOLTAGE - 一种基于对比学习的OCR方法，利用自动字形特征推荐进行基于聚类的标记，使用图像转换和生成对抗网络增加标记数据的多样性和数量，使用Takri文字进行设计，并在多种印度文字上测试。&lt;h4&gt;主要发现&lt;/h4&gt;在Takri文字上实现了95%的机器印刷样本和87%的手写样本的准确率，进行了基线和消融研究，为Takri构建了下游应用案例，证明了工作的实用性。&lt;h4&gt;结论&lt;/h4&gt;VOLTAGE方法具有普适性，适用于不同类型的印度文字，有助于保护低资源语言的数字化。&lt;h4&gt;翻译&lt;/h4&gt;联合国教科文组织已将全球使用的7000种语言中的2500种列为濒危语言。语言的流失导致传统智慧、民间文学和使用该语言的社区本质的丧失。因此，必须为这些语言提供数字包容性，避免其灭绝。低资源语言面临更高的灭绝风险。缺乏针对低资源语言的无监督光学字符识别方法是阻碍其数字化的原因之一。我们提出了VOLTAGE - 一种基于对比学习的OCR方法，利用自动字形特征推荐进行基于聚类的标记。我们使用图像转换和生成对抗网络来增加标记数据的多样性和数量。VOLTAGE是使用Takri文字设计的 - 这是一组在16至20世纪印度喜马拉雅地区使用的文字。我们展示了Takri文字以及其他印度文字（包括低资源和高资源）的结果，以证明该方法的普适性。在Takri文字上，机器印刷样本的准确率达到95%，手写样本达到87%。我们进行了基线和消融研究，并为Takri构建了下游应用案例，证明了我们工作的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2024.eacl-long.53&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; UNESCO has classified 2500 out of 7000 languages spoken worldwide asendangered. Attrition of a language leads to loss of traditional wisdom, folkliterature, and the essence of the community that uses it. It is thereforeimperative to bring digital inclusion to these languages and avoid itsextinction. Low resource languages are at a greater risk of extinction. Lack ofunsupervised Optical Character Recognition(OCR) methodologies for low resourcelanguages is one of the reasons impeding their digital inclusion. We proposeVOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyphfeature recommendation for cluster-based labelling. We augment the labelleddata for diversity and volume using image transformations and GenerativeAdversarial Networks. Voltage has been designed using Takri - a family ofscripts used in 16th to 20th century in the Himalayan regions of India. Wepresent results for Takri along with other Indic scripts (both low and highresource) to substantiate the universal behavior of the methodology. Anaccuracy of 95% for machine printed and 87% for handwritten samples on Takriscript has been achieved. We conduct baseline and ablation studies along withbuilding downstream use cases for Takri, demonstrating the usefulness of ourwork.</description>
      <author>example@mail.com (Prawaal Sharma, Poonam Goyal, Vidisha Sharma, Navneet Goyal)</author>
      <guid isPermaLink="false">2510.10490v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Complementary and Contrastive Learning for Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2510.10051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCFormer的新型音频-视觉分割框架，结合了CNN和Transformer的优势，能够有效处理局部和全局信息并全面捕捉空间-时间上下文。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉分割(AVS)旨在生成与物体听觉信号相关的像素级分割图。传统CNN方法通过基本操作处理音频-视觉交互但受限于局部感受野；较新的Transformer方法将听觉作为查询增强帧内协作，但难以充分提取多模态系数和时间动态。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发一个能同时处理局部和全局信息并全面捕捉空间-时间上下文的音频-视觉分割框架。&lt;h4&gt;方法&lt;/h4&gt;提出CCFormer框架，包含：1)早期集成模块(EIM)，采用并行双边架构融合多尺度视觉特征与音频数据；2)多查询Transformer模块(MTM)，动态赋予音频查询学习能力并建模帧级和视频级关系；3)双模态对比学习(BCL)，促进统一特征空间中跨模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;通过有效结合这些设计，该方法在S4、MS3和AVSS数据集上设立了新的最先进基准，源代码和模型权重将在GitHub公开。&lt;h4&gt;结论&lt;/h4&gt;CCFormer框架通过创新设计成功解决了传统方法和现有Transformer方法在音频-视觉分割中的局限性，实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;音频-视觉分割(AVS)旨在生成与物体听觉信号相关的像素级分割图。该领域已取得显著进展，许多基于CNN和Transformer的方法提高了分割的准确性和鲁棒性。传统CNN方法通过填充和乘法等基本操作处理音频-视觉交互，但受限于CNN的有限局部感受野。较新的基于Transformer的方法将听觉线索作为查询，利用注意力机制增强帧内音频-视觉协作，但通常难以充分提取多模态系数和时间动态。为克服这些局限性，我们提出了互补和对比Transformer(CCFormer)，这是一个新型框架，擅长处理局部和全局信息，并全面捕捉空间-时间上下文。我们的CCFormer首先采用早期集成模块(EIM)，该模块采用并行双边架构，将多尺度视觉特征与音频数据融合，以增强跨模态互补性。为了提取帧内空间特征并促进时间相干性的感知，我们引入了多查询Transformer模块(MTM)，该模块动态赋予音频查询学习能力，同时建模帧级和视频级关系。此外，我们提出了双模态对比学习(BCL)，以促进统一特征空间中跨模态的对齐。通过有效结合这些设计，我们的方法在S4、MS3和AVSS数据集上设立了新的最先进基准。我们的源代码和模型权重将在https://github.com/SitongGong/CCFormer公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation mapsthat correlate with the auditory signals of objects. This field has seensignificant progress with numerous CNN and Transformer-based methods enhancingthe segmentation accuracy and robustness. Traditional CNN approaches manageaudio-visual interactions through basic operations like padding andmultiplications but are restricted by CNNs' limited local receptive field. Morerecently, Transformer-based methods treat auditory cues as queries, utilizingattention mechanisms to enhance audio-visual cooperation within frames.Nevertheless, they typically struggle to extract multimodal coefficients andtemporal dynamics adequately. To overcome these limitations, we present theComplementary and Contrastive Transformer (CCFormer), a novel framework adeptat processing both local and global information and capturing spatial-temporalcontext comprehensively. Our CCFormer initiates with the Early IntegrationModule (EIM) that employs a parallel bilateral architecture, mergingmulti-scale visual features with audio data to boost cross-modalcomplementarity. To extract the intra-frame spatial features and facilitate theperception of temporal coherence, we introduce the Multi-query TransformerModule (MTM), which dynamically endows audio queries with learning capabilitiesand models the frame and video-level relations simultaneously. Furthermore, wepropose the Bi-modal Contrastive Learning (BCL) to promote the alignment acrossboth modalities in the unified feature space. Through the effective combinationof those designs, our method sets new state-of-the-art benchmarks across theS4, MS3 and AVSS datasets. Our source code and model weights will be madepublicly available at https://github.com/SitongGong/CCFormer</description>
      <author>example@mail.com (Sitong Gong, Yunzhi Zhuge, Lu Zhang, Pingping Zhang, Huchuan Lu)</author>
      <guid isPermaLink="false">2510.10051v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning</title>
      <link>http://arxiv.org/abs/2510.09915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了通过微调大型语言模型来减少生成摘要中的不忠实片段，引入了一种新的数据集和三种微调技术，实验结果表明所有方法都能提高摘要忠实度，其中似然训练最有效。&lt;h4&gt;背景&lt;/h4&gt;使用大型语言模型进行摘要生成已成为信息压缩的重要工具，但这些模型有时会产生不忠实的摘要，包含单词、短语或概念级别的幻觉。&lt;h4&gt;目的&lt;/h4&gt;研究微调策略以减少生成摘要中不忠实片段的出现，提高摘要的忠实度。&lt;h4&gt;方法&lt;/h4&gt;首先使用多种LLM为训练集中的源文档自动生成摘要，然后使用GPT-4o标注检测到的片段级幻觉；利用这些标注，使用无幻觉摘要和标注的不忠实片段微调LLM；引入一个包含忠实和不忠实摘要以及片段级标签的新数据集；评估三种微调技术：梯度上升、似然训练和任务向量否定。&lt;h4&gt;主要发现&lt;/h4&gt;所有三种方法都成功利用片段级标注提高了忠实度，其中似然训练最有效。&lt;h4&gt;结论&lt;/h4&gt;通过使用片段级标注进行微调，可以有效提高LLM生成摘要的忠实度。&lt;h4&gt;翻译&lt;/h4&gt;使用大型语言模型进行抽象式摘要已成为信息压缩的重要工具。然而，尽管这些模型能够生成流畅的摘要，但有时会产生不忠实的摘要，在单词、短语或概念层面引入幻觉。现有的缓解策略，如后处理校正或使用合成生成的负样本进行对比学习，无法完全解决LLM生成摘要中可能出现的各种错误。在本文中，我们研究了微调策略以减少生成摘要中不忠实片段的出现。首先，我们使用多种LLM为训练集中的源文档自动生成摘要，然后使用GPT-4o标注其检测到的片段级幻觉。利用这些标注，我们使用无幻觉摘要和标注的不忠实片段来微调LLM，以提高模型的忠实度。在本文中，我们引入了一个包含忠实和不忠实摘要以及片段级标签的新数据集，并评估了三种微调技术来提高LLM生成摘要的忠实度：梯度上升、似然训练和任务向量否定。实验结果表明，所有三种方法都成功利用片段级标注提高了忠实度，其中似然训练最为有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Abstractive summarization using large language models (LLMs) has become anessential tool for condensing information. However, despite their ability togenerate fluent summaries, these models sometimes produce unfaithful summaries,introducing hallucinations at the word, phrase, or concept level. Existingmitigation strategies, such as post-processing corrections or contrastivelearning with synthetically generated negative samples, fail to fully addressthe diverse errors that can occur in LLM-generated summaries. In this paper, weinvestigate fine-tuning strategies to reduce the occurrence of unfaithful spansin generated summaries. First, we automatically generate summaries for the setof source documents in the training set with a variety of LLMs and then useGPT-4o to annotate any hallucinations it detects at the span-level. Leveragingthese annotations, we fine-tune LLMs with both hallucination-free summaries andannotated unfaithful spans to enhance model faithfulness. In this paper, weintroduce a new dataset that contains both faithful and unfaithful summarieswith span-level labels and we evaluate three techniques to fine-tuning a LLM toimprove the faithfulness of the resulting summarization: gradient ascent,unlikelihood training, and task vector negation. Experimental results show thatall three approaches successfully leverage span-level annotations to improvefaithfulness, with unlikelihood training being the most effective.</description>
      <author>example@mail.com (Sicong Huang, Qianqi Yan, Shengze Wang, Ian Lane)</author>
      <guid isPermaLink="false">2510.09915v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations</title>
      <link>http://arxiv.org/abs/2510.09293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DualCSE的句子嵌入方法，通过为每个句子分配两个嵌入向量（分别代表显式语义和隐式语义）来解决传统方法难以捕捉句子隐式语义的问题。&lt;h4&gt;背景&lt;/h4&gt;句子嵌入方法虽然取得了显著进展，但仍难以捕捉句子中的隐式语义，这归因于传统方法为每个句子只分配一个向量的固有局限性。&lt;h4&gt;目的&lt;/h4&gt;克服传统句子嵌入方法的局限性，开发一种能够同时捕捉句子显式和隐式语义的方法。&lt;h4&gt;方法&lt;/h4&gt;提出DualCSE方法，为每个句子分配两个嵌入：一个代表显式语义，另一个代表隐式语义，这两个嵌入共存于共享空间中，可根据特定任务需求选择合适的语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DualCSE能够有效编码句子的显式和隐式含义，并提高下游任务（如信息检索和文本分类）的性能。&lt;h4&gt;结论&lt;/h4&gt;DualCSE是一种有效的句子嵌入方法，通过双嵌入机制同时捕捉句子的显式和隐式语义，提升了下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;句子嵌入方法已经取得了显著进展，但仍然难以捕捉句子中的隐式语义。这可以归因于传统句子嵌入方法的固有局限性，即为每个句子只分配一个向量。为了克服这一局限性，我们提出了DualCSE，一种为每个句子分配两个嵌入的句子嵌入方法：一个代表显式语义，另一个代表隐式语义。这些嵌入共存于共享空间中，使得能够为特定目的（如信息检索和文本分类）选择所需的语义。实验结果表明，DualCSE能够有效编码显式和隐式含义，并提高下游任务的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sentence embedding methods have made remarkable progress, yet they stillstruggle to capture the implicit semantics within sentences. This can beattributed to the inherent limitations of conventional sentence embeddingmethods that assign only a single vector per sentence. To overcome thislimitation, we propose DualCSE, a sentence embedding method that assigns twoembeddings to each sentence: one representing the explicit semantics and theother representing the implicit semantics. These embeddings coexist in theshared space, enabling the selection of the desired semantics for specificpurposes such as information retrieval and text classification. Experimentalresults demonstrate that DualCSE can effectively encode both explicit andimplicit meanings and improve the performance of the downstream task.</description>
      <author>example@mail.com (Kohei Oda, Po-Min Chuang, Kiyoaki Shirai, Natthawut Kertkeidkachorn)</author>
      <guid isPermaLink="false">2510.09293v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Generative Data Augmentation in Graph Contrastive Learning for Recommendation</title>
      <link>http://arxiv.org/abs/2510.09129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 34th ACM International Conference on Information and Knowledge  Management&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GDA4Rec的新颖框架，用于在推荐系统中通过图对比学习和生成数据增强来学习高质量的用户-物品嵌入表示。&lt;h4&gt;背景&lt;/h4&gt;推荐系统已成为各种在线平台不可或缺的组成部分，但面临从稀疏用户-物品交互中学习有效嵌入表示的基本挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有随机数据增强方法在对比学习中常常改变原始语义信息的问题，提供高质量的增强视图和强大的自监督信号。&lt;h4&gt;方法&lt;/h4&gt;GDA4Rec采用噪声生成模块利用深度生成模型近似原始数据分布进行数据增强，提取物品互补矩阵表征物品间潜在相关性，并使用整合推荐、数据增强和对比学习的联合目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的广泛实验证明了该模型的优越性。&lt;h4&gt;结论&lt;/h4&gt;GDA4Rec通过生成高质量增强视图和提供强大自监督信号，解决了推荐系统中从稀疏交互学习有效嵌入的挑战。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统已成为各种在线平台中不可或缺的组成部分，从电子商务到流媒体服务。该领域的一个基本挑战是如何从稀疏的用户-物品交互中学习有效的嵌入表示。虽然对比学习最近已成为解决这个问题的有前景的方法，但通过大多数现有随机数据增强方法为对比学习生成增强视图常常导致原始语义信息的改变。在本文中，我们提出了一个新颖的框架GDA4Rec（用于推荐的图对比学习中的生成数据增强），以生成高质量的增强视图并提供强大的自监督信号。具体来说，我们采用了一个噪声生成模块，利用深度生成模型来近似原始数据的分布用于数据增强。此外，GDA4Rec进一步提取了一个物品互补矩阵来表征物品之间的潜在相关性，并提供额外的自监督信号。最后，使用一个整合了推荐、数据增强和对比学习的联合目标函数，强制模型学习更有效和信息量更大的嵌入。在三个公共数据集上进行了广泛的实验，以证明该模型的优越性。代码可在以下网址获取：https://github.com/MrYansong/GDA4Rec。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761248&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommendation systems have become indispensable in various online platforms,from e-commerce to streaming services. A fundamental challenge in this domainis learning effective embeddings from sparse user-item interactions. Whilecontrastive learning has recently emerged as a promising solution to thisissue, generating augmented views for contrastive learning through mostexisting random data augmentation methods often leads to the alteration oforiginal semantic information. In this paper, we propose a novel framework,GDA4Rec (Generative Data Augmentation in graph contrastive learning forRecommendation) to generate high-quality augmented views and provide robustself-supervised signals. Specifically, we employ a noise generation module thatleverages deep generative models to approximate the distribution of originaldata for data augmentation. Additionally, GDA4Rec further extracts an itemcomplement matrix to characterize the latent correlations between items andprovide additional self-supervised signals. Lastly, a joint objective thatintegrates recommendation, data augmentation and contrastive learning is usedto enforce the model to learn more effective and informative embeddings.Extensive experiments are conducted on three public datasets to demonstrate thesuperiority of the model. The code is available at:https://github.com/MrYansong/GDA4Rec.</description>
      <author>example@mail.com (Yansong Wang, Qihui Lin, Junjie Huang, Tao Jia)</author>
      <guid isPermaLink="false">2510.09129v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Biomedical Named Entity Recognition Framework with Large Language Models</title>
      <link>http://arxiv.org/abs/2510.08902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a short paper at BIBM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的统一生物医学命名实体识别框架，解决了嵌套实体、实体边界模糊和跨语言泛化问题，在多个数据集上实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;生物医学命名实体识别对医学信息提取和知识发现至关重要，但现有方法在处理嵌套实体、实体边界模糊和跨语言泛化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于大型语言模型的统一生物医学命名实体识别框架，提高识别准确性和跨语言泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将BioNER重新表述为文本生成任务，设计符号标记策略处理扁平和嵌套实体，通过中英文数据集进行双语联合微调，引入基于对比学习的实体选择器过滤错误预测。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集和两个未见语料库上的实验结果表明，该方法实现了最先进的性能，并在跨语言场景下展现出强大的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于大型语言模型的BioNER框架能有效处理复杂的生物医学实体识别任务，具有出色的跨语言泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的生物医学命名实体识别对医学信息提取和知识发现至关重要。然而，现有方法通常难以处理嵌套实体、实体边界模糊和跨语言泛化问题。在本文中，我们提出了一种基于大型语言模型的统一生物医学命名实体识别框架。我们首先将BioNER重新表述为文本生成任务，并设计了一种符号标记策略，通过明确的边界标注联合处理扁平实体和嵌套实体。为了增强多语言和多任务泛化能力，我们在多个中英文数据集上进行双语联合微调。此外，我们引入了一种基于对比学习的实体选择器，利用边界敏感的正负样本过滤不正确或虚假的预测。在四个基准数据集和两个未见语料库上的实验结果表明，我们的方法实现了最先进的性能，并在跨语言场景下展现出强大的零样本泛化能力。源代码可在https://github.com/dreamer-tx/LLMNER免费获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate recognition of biomedical named entities is critical for medicalinformation extraction and knowledge discovery. However, existing methods oftenstruggle with nested entities, entity boundary ambiguity, and cross-lingualgeneralization. In this paper, we propose a unified Biomedical Named EntityRecognition (BioNER) framework based on Large Language Models (LLMs). We firstreformulate BioNER as a text generation task and design a symbolic taggingstrategy to jointly handle both flat and nested entities with explicit boundaryannotation. To enhance multilingual and multi-task generalization, we performbilingual joint fine-tuning across multiple Chinese and English datasets.Additionally, we introduce a contrastive learning-based entity selector thatfilters incorrect or spurious predictions by leveraging boundary-sensitivepositive and negative samples. Experimental results on four benchmark datasetsand two unseen corpora show that our method achieves state-of-the-artperformance and robust zero-shot generalization across languages. The sourcecodes are freely available at https://github.com/dreamer-tx/LLMNER.</description>
      <author>example@mail.com (Tengxiao Lv, Ling Luo, Juntao Li, Yanhua Wang, Yuchen Pan, Chao Liu, Yanan Wang, Yan Jiang, Huiyi Lv, Yuanyuan Sun, Jian Wang, Hongfei Lin)</author>
      <guid isPermaLink="false">2510.08902v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>On the Alignment Between Supervised and Self-Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.08852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了自监督对比学习(CL)与负样本监督对比学习(NSCL)在表示层面的一致性，发现尽管两种方法的损失函数相似，但在训练过程中它们的表示确实保持对齐，而参数空间则可能出现指数级差异。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习已取得显著成功，其表示能力可与监督预训练相媲美。最近理论表明，当类别数量增加时，CL损失函数近似于监督代理NSCL损失函数。&lt;h4&gt;目的&lt;/h4&gt;探究CL和NSCL是否不仅在目标函数层面，而且在表示层面也保持一致，以及这种对齐如何随训练条件变化。&lt;h4&gt;方法&lt;/h4&gt;分析在相同初始化、批量和增强条件下训练的CL和NSCL模型的表示对齐情况，提供理论证明和实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;1) CL和NSCL的表示保持相似，相似度矩阵在现实条件下保持接近；2) 提供了CKA和RSA等对齐度量的高概率保证；3) 对齐随类别数量增加、温度升高而改善，且依赖于批量大小；4) 参数空间耦合不稳定，权重差异可能随训练时间呈指数级增长；5) 实验验证显示CL-NSCL对齐随规模和温度增强，NSCL比其他监督目标更紧密跟踪CL。&lt;h4&gt;结论&lt;/h4&gt;NSCL可作为自监督学习和监督学习之间的原则性桥梁，为理解自监督学习提供了理论框架。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习(CL)已取得显著的实证成功，其产生的表示通常能在下游任务上与监督预训练相媲美。最近的理论表明，当类别数量增加时，CL损失函数近似于监督代理NSCL损失函数。然而，这种损失层面的相似性留下了一个开放问题：CL和NSCL在训练过程中是否也在表示层面保持对齐，而不仅仅是在目标函数层面？我们通过分析在共享随机性(相同初始化、批量和增强)条件下训练的CL和NSCL模型的表示对齐来解决这个问题。首先，我们证明它们诱导的表示保持相似：具体而言，我们在现实条件下证明了CL和NSCL的相似度矩阵保持接近。我们的边界为CKA和RSA等对齐度量提供了高概率保证，并阐明了对齐如何随类别数量增加、温度升高而改善，以及其对批量大小的依赖性。相比之下，我们证明了参数空间耦合本质上是不稳定的：CL和NSCL权重之间的差异可能随训练时间呈指数级增长。最后，我们通过实验验证了这些预测，表明CL-NSCL对齐随规模和温度增强，NSCL比其他监督目标更紧密地跟踪CL。这使NSCL成为自监督学习和监督学习之间的原则性桥梁。我们的代码和项目页面可在[链接]获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (CL) has achieved remarkable empiricalsuccess, often producing representations that rival supervised pre-training ondownstream tasks. Recent theory explains this by showing that the CL lossclosely approximates a supervised surrogate, Negatives-Only SupervisedContrastive Learning (NSCL) loss, as the number of classes grows. Yet thisloss-level similarity leaves an open question: {\em Do CL and NSCL also remainaligned at the representation level throughout training, not just in theirobjectives?}  We address this by analyzing the representation alignment of CL and NSCLmodels trained under shared randomness (same initialization, batches, andaugmentations). First, we show that their induced representations remainsimilar: specifically, we prove that the similarity matrices of CL and NSCLstay close under realistic conditions. Our bounds provide high-probabilityguarantees on alignment metrics such as centered kernel alignment (CKA) andrepresentational similarity analysis (RSA), and they clarify how alignmentimproves with more classes, higher temperatures, and its dependence on batchsize. In contrast, we demonstrate that parameter-space coupling is inherentlyunstable: divergence between CL and NSCL weights can grow exponentially withtraining time.  Finally, we validate these predictions empirically, showing that CL-NSCLalignment strengthens with scale and temperature, and that NSCL tracks CL moreclosely than other supervised objectives. This positions NSCL as a principledbridge between self-supervised and supervised learning. Our code and projectpage are available at[\href{https://github.com/DLFundamentals/understanding_ssl_v2}{code},\href{https://dlfundamentals.github.io/cl-nscl-representation-alignment/}{projectpage}].</description>
      <author>example@mail.com (Achleshwar Luthra, Priyadarsi Mishra, Tomer Galanti)</author>
      <guid isPermaLink="false">2510.08852v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering</title>
      <link>http://arxiv.org/abs/2510.08791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR2025 Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种医学视觉问答(Med-VQA)框架，通过统一模态对齐、难例挖掘和门控交叉注意力模块解决现有方法的局限性，并在多个标准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉问答(Med-VQA)需要对医学图像和文本问题有深入理解，尽管近期基于医学视觉语言预训练(Med-VLP)的方法表现良好，但仍存在模态对齐不统一、难例探索不足以及知识融合可能引入无关信息等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架解决医学视觉问答中的模态对齐、难例处理和知识融合挑战。&lt;h4&gt;方法&lt;/h4&gt;提出三个关键贡献：(1)统一解决方案处理多级别、多模态、多视图和多阶段的异构模态对齐，利用对比学习和最优传输理论；(2)难例挖掘方法，使用软标签进行多模态对齐并强制区分难例对；(3)门控交叉注意力模块，将答案词汇作为先验知识集成并选择相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在RAD-VQA、SLAKE、PathVQA和VQA-2019等广泛使用的Med-VQA数据集上超越了之前的最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过统一模态对齐策略、有效的难例挖掘机制和门控交叉注意力模块，该框架显著提升了医学视觉问答任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉问答(Med-VQA)是一个具有挑战性的任务，需要对医学图像和文本问题有深入理解。尽管近期利用医学视觉语言预训练(Med-VLP)的工作已在Med-VQA任务上展现出强大性能，但仍没有统一的模态对齐解决方案，且难例问题尚未得到充分探索。此外，Med-VQA常用的知识融合技术可能引入无关信息。在这项工作中，我们通过三个关键贡献提出一个框架来解决这些挑战：(1)一种统一解决方案，用于处理多级别、多模态、多视图和多阶段的异构模态对齐，利用对比学习和最优传输理论等方法；(2)一种难例挖掘方法，使用软标签进行多模态对齐，并强制区分难例对；(3)一种用于Med-VQA的门控交叉注意力模块，将答案词汇作为先验知识集成，并从中选择相关信息。我们的框架在广泛使用的Med-VQA数据集(如RAD-VQA、SLAKE、PathVQA和VQA-2019)上超越了之前的最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Visual Question Answering (Med-VQA) is a challenging task thatrequires a deep understanding of both medical images and textual questions.Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP)have shown strong performance on the Med-VQA task, there is still no unifiedsolution for modality alignment, and the issue of hard negatives remainsunder-explored. Additionally, commonly used knowledge fusion techniques forMed-VQA may introduce irrelevant information. In this work, we propose aframework to address these challenges through three key contributions: (1) aunified solution for heterogeneous modality alignments across multiple levels,modalities, views, and stages, leveraging methods like contrastive learning andoptimal transport theory; (2) a hard negative mining method that employs softlabels for multi-modality alignments and enforces the hard negative pairdiscrimination; and (3) a Gated Cross-Attention Module for Med-VQA thatintegrates the answer vocabulary as prior knowledge and selects relevantinformation from it. Our framework outperforms the previous state-of-the-art onwidely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.</description>
      <author>example@mail.com (Yuanhao Zou, Zhaozheng Yin)</author>
      <guid isPermaLink="false">2510.08791v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Lecture Notes on Verifying Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.11617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇讲义回顾了图神经网络、Weisfeiler-Lehman测试与逻辑之间的联系，提出了一种包含计数模态的模态逻辑，用于图神经网络验证，并描述了相应的可满足性问题算法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络、Weisfeiler-Lehman测试与一阶逻辑、分级模态逻辑之间的联系&lt;h4&gt;目的&lt;/h4&gt;提出一种模态逻辑，其中计数模态以线性不等式形式出现，用于解决图神经网络的验证任务&lt;h4&gt;方法&lt;/h4&gt;描述了该逻辑可满足性问题的算法，该方法基于普通模态逻辑的tableau方法，并扩展了对无量化子句布尔代数与Presburger算术的推理&lt;h4&gt;主要发现&lt;/h4&gt;计数模态可以以线性不等式形式出现在模态逻辑中，用于解决图神经网络验证问题&lt;h4&gt;结论&lt;/h4&gt;通过扩展的tableau方法，可以有效地解决所提出模态逻辑的可满足性问题&lt;h4&gt;翻译&lt;/h4&gt;在这些讲义中，我们首先回顾了图神经网络、Weisfeiler-Lehman测试与一阶逻辑和分级模态逻辑等逻辑之间的联系。然后，我们提出了一个模态逻辑，其中计数模态以线性不等式的形式出现，用于解决图神经网络的验证任务。我们描述了该逻辑可满足性问题的算法。该算法受到普通模态逻辑的tableau方法的启发，并扩展了对无量化子句布尔代数与Presburger算术的推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In these lecture notes, we first recall the connection between graph neuralnetworks, Weisfeiler-Lehman tests and logics such as first-order logic andgraded modal logic. We then present a modal logic in which counting modalitiesappear in linear inequalities in order to solve verification tasks on graphneural networks. We describe an algorithm for the satisfiability problem ofthat logic. It is inspired from the tableau method of vanilla modal logic,extended with reasoning in quantifier-free fragment Boolean algebra withPresburger arithmetic.</description>
      <author>example@mail.com (François Schwarzentruber)</author>
      <guid isPermaLink="false">2510.11617v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity</title>
      <link>http://arxiv.org/abs/2510.11347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新颖的多视图特征传播(MFP)框架，用于解决图神经网络在节点分类任务中因特征稀疏和隐私问题导致的性能下降。该框架通过将特征划分为多个添加高斯噪声的视图，独立传播信息并聚合结果，提高了分类性能同时保护隐私。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在节点分类任务中表现出色，但其效果通常依赖于完整的节点特征。在现实场景中，特征矩阵往往高度稀疏或包含敏感信息，导致性能下降和隐私风险增加。直接暴露信息还可能导致意外数据泄露，使攻击者能够推断敏感信息。&lt;h4&gt;目的&lt;/h4&gt;解决特征稀疏问题，提高节点分类性能，同时促进隐私保护，平衡效用与隐私的关系。&lt;h4&gt;方法&lt;/h4&gt;提出多视图特征传播(MFP)框架，将可用特征划分为多个添加高斯噪声的视图，每个视图独立通过图拓扑传播信息，聚合后的表示生成具有表达能力和鲁棒性的节点嵌入。该框架创新性地提高了极端稀疏条件下的鲁棒性，并提供了平衡效用与隐私的原则性方法。&lt;h4&gt;主要发现&lt;/h4&gt;在图数据集上的大量实验表明，MFP在节点分类方面优于最先进的基线方法，同时显著减少了隐私泄露。传播的输出作为原始特征的替代插补值而非重建值，保留了效用而不损害隐私。全面的敏感性分析证实了MFP在不同场景下的稳定性和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;MFP为具有缺失或敏感特征的图学习领域提供了一个有效且具有隐私意识的框架。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在关系数据上的节点分类任务中已经显示出显著的成功，然而它们的有效性往往依赖于完整节点特征的可用性。然而，在许多现实场景中，特征矩阵高度稀疏或包含敏感信息，导致性能下降和隐私风险增加。此外，直接暴露信息可能导致意外数据泄露，使攻击者能够推断敏感信息。为应对这些挑战，我们提出了一种新颖的多视图特征传播(MFP)框架，该框架在特征稀疏条件下增强节点分类同时促进隐私保护。MFP通过将可用特征划分为多个添加高斯噪声的视图来扩展传统特征传播(FP)，每个视图独立通过图拓扑传播信息。聚合后的表示产生具有表达能力和鲁棒性的节点嵌入。该框架在两个方面具有创新性：它引入了一种在极端稀疏条件下提高鲁棒性的机制，并提供了一种平衡效用与隐私的原则性方法。在图数据集上进行的大量实验表明，MFP在节点分类方面优于最先进的基线方法，同时显著减少了隐私泄露。此外，我们的分析表明传播的输出作为原始特征的替代插补值而非重建值，保留了效用而不损害隐私。全面的敏感性分析进一步证实了MFP在不同场景下的稳定性和实际适用性。总体而言，MFP为具有缺失或敏感特征的图学习领域提供了一个有效且具有隐私意识的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable success in nodeclassification tasks over relational data, yet their effectiveness oftendepends on the availability of complete node features. In many real-worldscenarios, however, feature matrices are highly sparse or contain sensitiveinformation, leading to degraded performance and increased privacy risks.Furthermore, direct exposure of information can result in unintended dataleakage, enabling adversaries to infer sensitive information. To address thesechallenges, we propose a novel Multi-view Feature Propagation (MFP) frameworkthat enhances node classification under feature sparsity while promotingprivacy preservation. MFP extends traditional Feature Propagation (FP) bydividing the available features into multiple Gaussian-noised views, eachpropagating information independently through the graph topology. Theaggregated representations yield expressive and robust node embeddings. Thisframework is novel in two respects: it introduces a mechanism that improvesrobustness under extreme sparsity, and it provides a principled way to balanceutility with privacy. Extensive experiments conducted on graph datasetsdemonstrate that MFP outperforms state-of-the-art baselines in nodeclassification while substantially reducing privacy leakage. Moreover, ouranalysis demonstrates that propagated outputs serve as alternative imputationsrather than reconstructions of the original features, preserving utilitywithout compromising privacy. A comprehensive sensitivity analysis furtherconfirms the stability and practical applicability of MFP across diversescenarios. Overall, MFP provides an effective and privacy-aware framework forgraph learning in domains characterized by missing or sensitive features.</description>
      <author>example@mail.com (Etzion Harari, Moshe Unger)</author>
      <guid isPermaLink="false">2510.11347v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Event-Aware Prompt Learning for Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2510.11339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EVP的事件感知动态图提示学习框架，可作为现有方法的插件，增强其利用历史事件知识的能力。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的图通常通过一系列事件演化，建模跨领域对象之间的动态交互。动态图神经网络(DGNNs)已成为动态图学习的流行解决方案，而提示学习方法最近也被探索应用于动态图。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法专注于捕捉节点与时间关系而忽视历史事件影响的问题，提出能够增强现有方法利用历史事件知识能力的框架。&lt;h4&gt;方法&lt;/h4&gt;首先为每个节点提取一系列历史事件并引入事件适应机制对齐事件特征与下游任务；其次提出事件聚合机制将历史知识整合到节点表示中；最后在四个公共数据集上进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了EVP框架能够有效利用历史事件知识增强动态图学习性能。&lt;h4&gt;结论&lt;/h4&gt;EVP框架作为插件可以增强现有动态图学习方法的能力，特别在利用历史事件知识方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的图通常通过一系列事件演化，建模跨领域对象之间的动态交互。在动态图学习中，动态图神经网络(DGNNs)已成为流行的解决方案。最近，提示学习方法已被探索应用于动态图。然而，现有方法通常专注于捕捉节点与时间之间的关系，而忽视了历史事件的影响。在本文中，我们提出了EVP，一种事件感知的动态图提示学习框架，可作为现有方法的插件，增强它们利用历史事件知识的能力。首先，我们为每个节点提取一系列历史事件，并引入事件适应机制将这些事件的细粒度特征与下游任务对齐。其次，我们提出事件聚合机制，有效将历史知识整合到节点表示中。最后，我们在四个公共数据集上进行广泛的实验来评估和分析EVP。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world graph typically evolve via a series of events, modeling dynamicinteractions between objects across various domains. For dynamic graphlearning, dynamic graph neural networks (DGNNs) have emerged as popularsolutions. Recently, prompt learning methods have been explored on dynamicgraphs. However, existing methods generally focus on capturing the relationshipbetween nodes and time, while overlooking the impact of historical events. Inthis paper, we propose EVP, an event-aware dynamic graph prompt learningframework that can serve as a plug-in to existing methods, enhancing theirability to leverage historical events knowledge. First, we extract a series ofhistorical events for each node and introduce an event adaptation mechanism toalign the fine-grained characteristics of these events with downstream tasks.Second, we propose an event aggregation mechanism to effectively integratehistorical knowledge into node representations. Finally, we conduct extensiveexperiments on four public datasets to evaluate and analyze EVP.</description>
      <author>example@mail.com (Xingtong Yu, Ruijuan Liang, Xinming Zhang, Yuan Fang)</author>
      <guid isPermaLink="false">2510.11339v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids</title>
      <link>http://arxiv.org/abs/2510.11286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDEN（软件定义能源网络）的创新架构，用于解决现代电网中数据生成与处理位置不匹配的问题，通过统一边缘、雾和云计算资源，结合5G URLLC、SDN和NFV技术，实现能源、延迟和可靠性的协同优化。&lt;h4&gt;背景&lt;/h4&gt;现代电网面临数据生成位置与数据处理位置严重不匹配的挑战：保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而耗能大的工作负载通常位于远程云中，导致错过实时截止日期和浪费电力。&lt;h4&gt;目的&lt;/h4&gt;提出首个SDEN（软件定义能源网络）用于CaaS（计算即服务），将碎片化的电网计算转变为单一的、可编程的基础设施，提供可靠、节能的实时分析。&lt;h4&gt;方法&lt;/h4&gt;统一边缘、雾和云计算与5G URLLC（超可靠低延迟通信）、SDN（软件定义网络）和NFV（网络功能虚拟化）技术；提出联合任务卸载公式，在明确URLLC约束下将计算放置与网络容量耦合；开发可行性保持的轻量级贪婪启发式算法；构建分层AI管道，包括边缘反应式、雾预测性和云战略性层，使用隐私保护的联邦GNN（图神经网络）进行故障检测和微电网协调。&lt;h4&gt;主要发现&lt;/h4&gt;SDEN架构能够有效解决电网计算资源碎片化问题，通过协同优化能源消耗、网络延迟和系统可靠性，提供端到端的实时分析能力；所提出的轻量级贪婪启发式算法能够在可扩展性的同时，紧密跟踪最佳能源和延迟权衡；分层AI管道能够在不同层级提供不同特性的智能分析能力。&lt;h4&gt;结论&lt;/h4&gt;SDEN建立了首个软件定义的路径，实现了实用的、电网规模的CaaS（计算即服务），与仅边缘或仅云的方案不同，它能够将碎片化的电网计算资源整合为单一、可编程的基础设施，提供可靠、节能的实时分析能力。&lt;h4&gt;翻译&lt;/h4&gt;现代电网面临数据生成位置与数据处理位置严重不匹配的挑战：保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而耗能大的工作负载通常位于远程云中，导致错过实时截止日期和浪费电力。我们通过提出据我们所知首个用于CaaS（计算即服务）的SDEN（软件定义能源网络）来解决这一问题，该架构统一了边缘、雾和云计算资源，并结合5G URLLC（超可靠低延迟通信）、SDN（软件定义网络）和NFV（网络功能虚拟化）技术，共同优化端到端的能源、延迟和可靠性。我们的贡献有三方面：（i）联合任务卸载公式，在明确URLLC约束下将计算放置与网络容量耦合；（ii）可行性保持的轻量级贪婪启发式算法，可扩展并紧密跟踪最佳能源和延迟权衡；（iii）分层AI管道，边缘层具有反应性，雾层具有预测性，云层具有战略性，具有隐私保护的联邦GNN（图神经网络）用于故障检测和微电网协调。与仅边缘或仅云的方案不同，SDEN将碎片化的电网计算转变为单一的、可编程的基础设施，提供可靠、节能的实时分析，建立了首个软件定义的路径，实现实用的、电网规模的CaaS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern power grids face an acute mismatch between where data is generated andwhere it can be processed: protection relays, EV (Electric Vehicle) charging,and distributed renewables demand millisecond analytics at the edge, whileenergy-hungry workloads often sit in distant clouds leading to missed real-timedeadlines and wasted power. We address this by proposing, to our knowledge, thefirst-ever SDEN (Software Defined Energy Network) for CaaS(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5GURLLC (Ultra-Reliable Low-Latency Communications), SDN (Software DefinedNetworking), and NFV (Network Functions Virtualization) to co-optimize energy,latency, and reliability end-to-end. Our contributions are threefold: (i) ajoint task offloading formulation that couples computation placement withnetwork capacity under explicit URLLC constraints; (ii) a feasibilitypreserving, lightweight greedy heuristic that scales while closely trackingoptimal energy and latency trade-offs; and (iii) a tiered AI (ArtificialIntelligence) pipeline-reactive at the edge, predictive in the fog, strategicin the cloud-featuring privacy-preserving, federated GNNs (Graph NeuralNetworks) for fault detection and microgrid coordination. Unlike prioredge-only or cloud-only schemes, SDEN turns fragmented grid compute into asingle, programmable substrate that delivers dependable, energy-aware, realtime analytics establishing a first-ever, software defined path to practical,grid-scale CaaS.</description>
      <author>example@mail.com (Jack Jackman, David Ryan, Arun Narayanan, Pedro Nardelli, Indrakshi Dey)</author>
      <guid isPermaLink="false">2510.11286v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enforcing convex constraints in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.11227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ProjNet，一个满足输入依赖约束的图神经网络框架，结合了稀疏向量裁剪和CAD算法，并通过GPU加速实现高效处理大规模输入的能力。&lt;h4&gt;背景&lt;/h4&gt;许多机器学习应用需要满足复杂、动态约束的输出，但在图神经网络模型中，由于图结构数据的可变输出大小，这一任务尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够满足输入依赖约束的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;ProjNet结合稀疏向量裁剪方法和Component-Averaged Dykstra (CAD)算法，建立CAD的收敛结果，开发GPU加速实现，并引入计算效率高且适合优化的替代梯度用于端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在四类约束优化问题（线性规划、两类非凸二次规划和无线发射功率优化）上验证了ProjNet的有效性。&lt;h4&gt;结论&lt;/h4&gt;ProjNet在各种问题设置中表现出有效性，能够满足复杂、动态的约束要求。&lt;h4&gt;翻译&lt;/h4&gt;许多机器学习应用需要满足复杂、动态约束的输出。在图神经网络模型中，由于图结构数据的可变输出大小，这一任务尤其具有挑战性。本文介绍了ProjNet，一个满足输入依赖约束的图神经网络框架。ProjNet结合了稀疏向量裁剪方法和Component-Averaged Dykstra (CAD)算法，一种解决最佳逼近问题的迭代方案。我们建立了CAD的收敛结果，并开发了能够高效处理大规模输入的GPU加速实现。为了实现端到端训练，我们引入了一个计算效率高且比精确梯度更适合优化的替代梯度。我们在四类约束优化问题上验证了ProjNet：线性规划、两类非凸二次规划和无线发射功率优化，证明了其在各种问题设置中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many machine learning applications require outputs that satisfy complex,dynamic constraints. This task is particularly challenging in Graph NeuralNetwork models due to the variable output sizes of graph-structured data. Inthis paper, we introduce ProjNet, a Graph Neural Network framework whichsatisfies input-dependant constraints. ProjNet combines a sparse vectorclipping method with the Component-Averaged Dykstra (CAD) algorithm, aniterative scheme for solving the best-approximation problem. We establish aconvergence result for CAD and develop a GPU-accelerated implementation capableof handling large-scale inputs efficiently. To enable end-to-end training, weintroduce a surrogate gradient for CAD that is both computationally efficientand better suited for optimization than the exact gradient. We validate ProjNeton four classes of constrained optimisation problems: linear programming, twoclasses of non-convex quadratic programs, and radio transmit poweroptimization, demonstrating its effectiveness across diverse problem settings.</description>
      <author>example@mail.com (Ahmed Rashwan, Keith Briggs, Chris Budd, Lisa Kreusser)</author>
      <guid isPermaLink="false">2510.11227v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Multicast Routing for On-Demand Streaming Services in 6G Networks</title>
      <link>http://arxiv.org/abs/2510.11109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络(GNN)的多播路由框架，用于解决6G网络中带宽密集型应用的路由问题，能够同时最小化传输成本并支持用户特定的视频质量需求。&lt;h4&gt;背景&lt;/h4&gt;6G无线网络中带宽密集型应用（如实时体积流和多感官扩展现实）的增长需要智能多播路由解决方案，能够大规模提供差异化服务质量。&lt;h4&gt;目的&lt;/h4&gt;解决传统路由算法计算复杂、结构僵化、无法支持异构用户需求的问题，以及基于神经网络的方法缺乏拓扑泛化能力和可扩展性的局限。&lt;h4&gt;方法&lt;/h4&gt;将路由问题表述为约束最小流优化任务，开发强化学习算法顺序构建高效多播树；使用图注意力网络(GAT)作为编码器提取上下文感知节点嵌入，使用长短期记忆(LSTM)模块建模路由决策中的序列依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法接近基于动态规划的最优解，同时显著降低计算复杂度；对大规模和动态网络拓扑具有强泛化能力，适合6G多媒体交付场景的实时部署。&lt;h4&gt;结论&lt;/h4&gt;提出的GNN-based多播路由框架能有效解决6G网络中带宽密集型应用的路由问题，平衡了性能和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;随着第六代（6G）无线网络中带宽密集型应用的增加，如实时体积流和多感官扩展现实，需要能够大规模提供差异化服务质量（QoS）的智能多播路由解决方案。传统的最短路径和多播路由算法要么计算上不可行，要么结构上僵化，它们通常无法支持异构用户需求，导致资源利用次优。基于神经网络的方法虽然提供了改进的推理速度，但通常缺乏拓扑泛化能力和可扩展性。为了解决这些限制，本文提出了一个基于图神经网络（GNN）的多播路由框架，该框架同时最小化总传输成本并支持用户特定的视频质量要求。路由问题被表述为约束最小流优化任务，并开发了一种强化学习算法，通过重用路径和适应网络动态来顺序构建高效的多播树。采用图注意力网络（GAT）作为编码器来提取上下文感知的节点嵌入，同时使用长短期记忆（LSTM）模块来建模路由决策中的序列依赖关系。大量模拟表明，该方法接近最优的基于动态规划的解决方案，同时显著降低了计算复杂度。结果还证实了该方法对大规模和动态网络拓扑的强泛化能力，突显了该方法在6G多媒体交付场景中实时部署的潜力。代码可在https://github.com/UNIC-Lab/GNN-Routing获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increase of bandwidth-intensive applications in sixth-generation (6G)wireless networks, such as real-time volumetric streaming and multi-sensoryextended reality, demands intelligent multicast routing solutions capable ofdelivering differentiated quality-of-service (QoS) at scale. Traditionalshortest-path and multicast routing algorithms are either computationallyprohibitive or structurally rigid, and they often fail to support heterogeneoususer demands, leading to suboptimal resource utilization. Neural network-basedapproaches, while offering improved inference speed, typically lack topologicalgeneralization and scalability. To address these limitations, this paperpresents a graph neural network (GNN)-based multicast routing framework thatjointly minimizes total transmission cost and supports user-specific videoquality requirements. The routing problem is formulated as a constrainedminimum-flow optimization task, and a reinforcement learning algorithm isdeveloped to sequentially construct efficient multicast trees by reusing pathsand adapting to network dynamics. A graph attention network (GAT) is employedas the encoder to extract context-aware node embeddings, while a longshort-term memory (LSTM) module models the sequential dependencies in routingdecisions. Extensive simulations demonstrate that the proposed method closelyapproximates optimal dynamic programming-based solutions while significantlyreducing computational complexity. The results also confirm stronggeneralization to large-scale and dynamic network topologies, highlighting themethod's potential for real-time deployment in 6G multimedia deliveryscenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.</description>
      <author>example@mail.com (Xiucheng Wang, Zien Wang, Nan Cheng, Wenchao Xu, Wei Quan, Xuemin Shen)</author>
      <guid isPermaLink="false">2510.11109v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments</title>
      <link>http://arxiv.org/abs/2510.10954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 15th International Workshop on Structural Health Monitoring  (IWSHM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了图神经网络、卷积神经网络和标准前馈神经网络在预测人类空间偏好方面的可推广性，使用合成数据评估它们在未见环境中的表现。&lt;h4&gt;背景&lt;/h4&gt;预测人类在建成环境中的空间偏好对开发赛博物理社会基础设施系统至关重要，但偏好模型的可推广性是一个重大挑战，特别是在预测训练过程中未遇到的环境配置时。&lt;h4&gt;目的&lt;/h4&gt;确定哪种神经网络架构在推广到未见过的布局方面最有效，并进行不同神经网络架构的比较研究。&lt;h4&gt;方法&lt;/h4&gt;使用从简化的合成口袋公园环境生成的合成数据，评估模型预测受异构物理、环境和社会特征影响的偏好的能力，使用精确率-召回率曲线下面积计算可推广性分数。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型在学习复杂的空间和上下文依赖关系方面显示出潜力，但不同神经网络架构在推广到未见过的空间场景方面存在差异。&lt;h4&gt;结论&lt;/h4&gt;可推广性分数提供了关于每种神经网络架构在未见过的建成环境中进行偏好感知人类行为建模的适用性的见解。&lt;h4&gt;翻译&lt;/h4&gt;预测建成环境中人类空间偏好的能力对于开发赛博物理社会基础设施系统(CPSIS)至关重要。该领域的一个重大挑战是偏好模型的可推广性，特别是在预测训练过程中未遇到的环境配置时的有效性。虽然深度学习模型在学习复杂的空间和上下文依赖关系方面显示出潜力，但目前尚不清楚哪种神经网络架构在推广到未见过的布局方面最有效。为此，我们使用从简化的合成口袋公园环境生成的合成数据，对图神经网络、卷积神经网络和标准前馈神经网络进行了比较研究。从这个案例研究开始，可以控制分析每种模型将学习到的偏好模式转移到未见空间场景的能力。模型根据其预测受异构物理、环境和社会特征影响的偏好的能力进行评估。使用精确率-召回率曲线下面积计算可推广性分数，这种方法适用于不平衡数据，提供了关于每种神经网络架构在未见过的建成环境中进行偏好感知人类行为建模的适用性的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The capacity to predict human spatial preferences within built environmentsis instrumental for developing Cyber-Physical-Social Infrastructure Systems(CPSIS). A significant challenge in this domain is the generalizability ofpreference models, particularly their efficacy in predicting preferences withinenvironmental configurations not encountered during training. While deeplearning models have shown promise in learning complex spatial and contextualdependencies, it remains unclear which neural network architectures are mosteffective at generalizing to unseen layouts. To address this, we conduct acomparative study of Graph Neural Networks, Convolutional Neural Networks, andstandard feedforward Neural Networks using synthetic data generated from asimplified and synthetic pocket park environment. Beginning with thisillustrative case study, allows for controlled analysis of each model's abilityto transfer learned preference patterns to unseen spatial scenarios. The modelsare evaluated based on their capacity to predict preferences influenced byheterogeneous physical, environmental, and social features. Generalizabilityscore is calculated using the area under the precision-recall curve for theseen and unseen layouts. This generalizability score is appropriate forimbalanced data, providing insights into the suitability of each neural networkarchitecture for preference-aware human behavior modeling in unseen builtenvironments.</description>
      <author>example@mail.com (Maral Doctorarastoo, Katherine A. Flanigan, Mario Bergés, Christopher McComb)</author>
      <guid isPermaLink="false">2510.10954v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations</title>
      <link>http://arxiv.org/abs/2510.10864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图异质性与谱滤波器之间的关系，发现它们之间的联系比之前认为的更为复杂，提出了自适应滤波的必要性，并提出了一个简单而强大的GNN方法[METHOD NAME]，在实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;图异质性（连接节点具有不同标签）最近引起了广泛关注。大多数现有工作采用简化的方法——对同质性图使用低通滤波器，对异质性图使用高通滤波器。&lt;h4&gt;目的&lt;/h4&gt;探究图异质性与谱滤波器之间的复杂关系，设计能够适应不同异质性连接的自适应滤波器，以提高GNN在各类图上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了[METHOD NAME]，一个简单而强大的GNN方法，它能够提取异质性谱中的信息，并通过自适应混合来结合显著的表示。&lt;h4&gt;主要发现&lt;/h4&gt;图异质性与谱滤波器之间的关系更为复杂——最优滤波器响应在不同频率分量上有所不同，并且与异质性程度没有严格的单调相关性。GNNs的平均频率响应和图异质性程度之间没有严格的单调相关性。&lt;h4&gt;结论&lt;/h4&gt;需要自适应图滤波器来保证良好的泛化性能。[METHOD NAME]在同类质性和异质性图上相比领先基线实现了高达9.2%的准确率提升。&lt;h4&gt;翻译&lt;/h4&gt;图异质性，即连接的节点具有不同标签，最近引起了广泛关注。大多数现有工作采用简化的方法——对同质性图使用低通滤波器，对异质性图使用高通滤波器。然而，我们发现图异质性与谱滤波器之间的关系更为复杂——最优滤波器响应在不同频率分量上有所不同，并且与异质性程度没有严格的单调相关性。这一发现挑战了传统的固定滤波器设计，并表明需要自适应滤波来保持图嵌入的表达能力。正式地说，自然产生的问题有：给定一个异质性图G，G的异质性程度的变化如何以及会在多大程度上影响GNNs的性能？如何设计自适应滤波器来适应这些变化的异质性连接？我们的理论分析表明，GNNs的平均频率响应和图异质性程度之间没有严格的单调相关性，这需要自适应图滤波器来保证良好的泛化性能。因此，我们提出了[METHOD NAME]，一个简单而强大的GNN，它提取异质性谱中的信息，并通过自适应混合来结合显著的表示。[METHOD NAME]的优越性能在同类质性和异质性图上相比领先基线实现了高达9.2%的准确率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph heterophily, where connected nodes have different labels, has attractedsignificant interest recently. Most existing works adopt a simplified approach- using low-pass filters for homophilic graphs and high-pass filters forheterophilic graphs. However, we discover that the relationship between graphheterophily and spectral filters is more complex - the optimal filter responsevaries across frequency components and does not follow a strict monotoniccorrelation with heterophily degree. This finding challenges conventional fixedfilter designs and suggests the need for adaptive filtering to preserveexpressiveness in graph embeddings. Formally, natural questions arise: Given aheterophilic graph G, how and to what extent will the varying heterophilydegree of G affect the performance of GNNs? How can we design adaptive filtersto fit those varying heterophilic connections? Our theoretical analysis revealsthat the average frequency response of GNNs and graph heterophily degree do notfollow a strict monotonic correlation, necessitating adaptive graph filters toguarantee good generalization performance. Hence, we propose [METHOD NAME], asimple yet powerful GNN, which extracts information across the heterophilyspectrum and combines salient representations through adaptive mixing. [METHODNAME]'s superior performance achieves up to 9.2% accuracy improvement overleading baselines across homophilic and heterophilic graphs.</description>
      <author>example@mail.com (Shuaicheng Zhang, Haohui Wang, Junhong Lin, Xiaojie Guo, Yada Zhu, Si Zhang, Dongqi Fu, Dawei Zhou)</author>
      <guid isPermaLink="false">2510.10864v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion</title>
      <link>http://arxiv.org/abs/2510.10849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLANCE的自适应GNN-LLM融合框架，通过轻量级路由器选择性调用LLM来优化GNN预测，在异质节点上获得显著性能提升，同时保持整体性能最优。&lt;h4&gt;背景&lt;/h4&gt;文本属性图的学习促进了大型语言模型(LLMs)在图学习中的应用。然而，大多数融合策略在所有节点上统一应用，只获得较小的整体性能提升。&lt;h4&gt;目的&lt;/h4&gt;重新设计LLM-GNN融合框架，专注于GNN通常表现不佳的节点，提高图学习性能。&lt;h4&gt;方法&lt;/h4&gt;提出GLANCE(GNN with LLM Assistance for Neighbor- and Context-aware Embeddings)框架，使用轻量级路由器根据每个节点的低成本信号决定是否查询LLM。路由器使用基于优势的目标进行训练，比较查询LLM与仅依赖GNN的效用。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和LLM在性能上存在显著差异，各自在不同的结构模式上表现出色；GLANCE在节点子组之间实现了最佳的性能平衡，在异质节点上获得最高13%的性能提升，同时实现顶级整体性能。&lt;h4&gt;结论&lt;/h4&gt;自适应的、节点感知的GNN-LLM架构具有重要价值，选择性调用LLM enables在大图上的可扩展部署，而不会产生高计算成本。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图的学习激发了大型语言模型(LLMs)在图学习中的应用。然而，大多数融合策略统一应用于所有节点，仅获得较小的整体性能提升。我们认为这一结果源于聚合指标掩盖了LLMs何时提供益处，阻碍了新策略的可操作信号。在这项工作中，我们围绕GNN通常表现不佳的节点重新设计了LLM-GNN融合。我们首先展示了GNN和LLM在性能上可以显著不同，各自在不同的结构模式上表现出色，例如局部同质性。为了利用这一发现，我们提出了GLANCE(GNN with LLM Assistance for Neighbor- and Context-aware Embeddings)框架，该框架调用LLM来优化GNN的预测。GLANCE采用一个轻量级路由器，根据每个节点的低成本信号，决定是否查询LLM。由于LLM调用是不可微分的，路由器使用基于优势的目标进行训练，比较查询LLM与仅依赖GNN的效用。在多个基准测试中，GLANCE在节点子组之间实现了最佳的性能平衡，在异质节点上获得显著提升(最高+13%)，同时实现顶级整体性能。我们的研究结果表明自适应的、节点感知的GNN-LLM架构的价值，选择性调用LLM enables在大图上的可扩展部署，而不会产生高计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on text-attributed graphs has motivated the use of Large LanguageModels (LLMs) for graph learning. However, most fusion strategies are applieduniformly across all nodes and attain only small overall performance gains. Weargue this result stems from aggregate metrics that obscure when LLMs providebenefit, inhibiting actionable signals for new strategies. In this work, wereframe LLM-GNN fusion around nodes where GNNs typically falter. We first showthat performance can significantly differ between GNNs and LLMs, with eachexcelling on distinct structural patterns, such as local homophily. To leveragethis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- andContext-aware Embeddings), a framework that invokes an LLM to refine a GNN'sprediction. GLANCE employs a lightweight router that, given inexpensiveper-node signals, decides whether to query the LLM. Since the LLM calls arenon-differentiable, the router is trained with an advantage-based objectivethat compares the utility of querying the LLM against relying solely on theGNN. Across multiple benchmarks, GLANCE achieves the best performance balanceacross node subgroups, achieving significant gains on heterophilous nodes (upto $+13\%$) while simultaneously achieving top overall performance. Ourfindings highlight the value of adaptive, node-aware GNN-LLM architectures,where selectively invoking the LLM enables scalable deployment on large graphswithout incurring high computational costs.</description>
      <author>example@mail.com (Donald Loveland, Yao-An Yang, Danai Koutra)</author>
      <guid isPermaLink="false">2510.10849v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Fast and the Furious: Hot Starts in Pursuit-Evasion Games</title>
      <link>http://arxiv.org/abs/2510.10830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at AAMAS Workshop on Autonomous Robots and Multirobot  Systems (ARMS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合博弈论控制理论与图神经网络的新方法，用于解决在没有事先了解逃亡者位置情况下的追捕者部署问题。通过构建图特征空间和训练图卷积网络生成战略有效的初始配置，显著提高了追捕效率。&lt;h4&gt;背景&lt;/h4&gt;在追逃游戏中，没有事先了解逃亡者位置的情况下有效部署追捕者仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，使追捕者能够在没有先验知识的情况下有效地部署，提高追捕效率。&lt;h4&gt;方法&lt;/h4&gt;结合博弈论控制理论与图神经网络，将追捕者配置表示为图，通过多目标优化构建图特征空间识别帕累托最优配置，并在这些最优图上训练图卷积网络生成'热启动'配置。&lt;h4&gt;主要发现&lt;/h4&gt;经验评估表明，GCN生成的热启动相比随机配置具有显著优势；在多追捕者和多逃亡者场景中，该方法加速了逃亡者生存率下降，减少了追捕者移动距离，并增强了围捕效果。&lt;h4&gt;结论&lt;/h4&gt;该方法在追逃游戏中展示了明显的战略优势，能有效提高追捕效率。&lt;h4&gt;翻译&lt;/h4&gt;在没有逃亡者位置先验知识的情况下，在追逃游戏中有效部署追捕者仍然是一个重大挑战。本文介绍了一种结合博弈论控制理论与图神经网络的新方法。通过将追捕者配置概念化为战略安排并表示为图，通过多目标优化构建图特征空间以识别帕累托最优配置。在这些帕累托最优图上训练图卷积网络(GCN)，生成战略上有效的初始配置，称为'热启动'。经验评估表明，GCN生成的热启动相比随机配置具有显著优势。在考虑多个追捕者和逃亡者的场景中，这种方法加速了逃亡者生存率的下降，减少了追捕者的移动距离，并增强了围捕效果，展示了明显的战略优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effectively positioning pursuers in pursuit-evasion games without priorknowledge of evader locations remains a significant challenge. A novel approachthat combines game-theoretic control theory with Graph Neural Networks isintroduced in this work. By conceptualizing pursuer configurations as strategicarrangements and representing them as graphs, a Graph Characteristic Space isconstructed via multi-objective optimization to identify Pareto-optimalconfigurations. A Graph Convolutional Network (GCN) is trained on thesePareto-optimal graphs to generate strategically effective initialconfigurations, termed "hot starts". Empirical evaluations demonstrate that theGCN-generated hot starts provide a significant advantage over randomconfigurations. In scenarios considering multiple pursuers and evaders, thismethod hastens the decline in evader survival rates, reduces pursuer traveldistances, and enhances containment, showcasing clear strategic benefits.</description>
      <author>example@mail.com (Gabriel Smithline, Scott Nivison)</author>
      <guid isPermaLink="false">2510.10830v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction</title>
      <link>http://arxiv.org/abs/2510.10775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniGNN，一个基于注意力的多关系动态图神经网络，通过异构节点和边类型整合宏观经济背景，实现稳健的消息传递，特别在宏观经济冲击期间表现出色。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在金融市场中已被成功应用于建模关系数据，有效捕捉股票间的非线性依赖关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型在宏观经济冲击期间无法有效传播消息的问题，提出一个能够整合宏观经济背景的稳健图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;引入一个作为全局中介的行业节点实现快速冲击传播；利用图注意力网络(GAT)加权邻居贡献；采用Transformer捕捉多关系间的时间动态。&lt;h4&gt;主要发现&lt;/h4&gt;OmniGNN在公共数据集上的股票预测模型中表现优于现有模型，特别是在COVID-19期间表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;OmniGNN通过整合宏观经济背景和优化的消息传递机制，显著提升了股票预测模型的性能，特别是在宏观经济冲击期间的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;在金融市场中，图神经网络已被成功应用于建模关系数据，有效捕捉股票间的非线性依赖关系。然而，现有模型通常在宏观经济冲击期间无法有效传播消息。在本文中，我们提出了OmniGNN，一个基于注意力的多关系动态图神经网络，通过异构节点和边类型整合宏观经济背景，实现稳健的消息传递。OmniGNN的核心是一个作为全局中介的行业节点，使冲击能够在图中快速传播，而不依赖长距离多跳扩散。该模型利用图注意力网络(GAT)来加权邻居的贡献，并采用Transformer来捕捉多关系间的时间动态。实验表明，OmniGNN在公共数据集上的股票预测模型中优于现有模型，特别是在COVID-19期间表现出强大的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In financial markets, Graph Neural Networks have been successfully applied tomodeling relational data, effectively capturing nonlinear inter-stockdependencies. Yet, existing models often fail to efficiently propagate messagesduring macroeconomic shocks. In this paper, we propose OmniGNN, anattention-based multi-relational dynamic GNN that integrates macroeconomiccontext via heterogeneous node and edge types for robust message passing.Central to OmniGNN is a sector node acting as a global intermediary, enablingrapid shock propagation across the graph without relying on long-rangemulti-hop diffusion. The model leverages Graph Attention Networks (GAT) toweigh neighbor contributions and employs Transformers to capture temporaldynamics across multiplex relations. Experiments show that OmniGNN outperformsexisting stock prediction models on public datasets, particularly demonstratingstrong robustness during the COVID-19 period.</description>
      <author>example@mail.com (Amber Li, Aruzhan Abil, Juno Marques Oda)</author>
      <guid isPermaLink="false">2510.10775v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mapping the Urban Mobility Intelligence Frontier: A Scientometric Analysis of Data-Driven Pedestrian Trajectory Prediction and Simulation</title>
      <link>http://arxiv.org/abs/2510.10327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对数据驱动的行人轨迹预测和人群模拟进行了全面的文献计量分析，揭示了人工智能、城市信息和人群行为建模之间的融合趋势，并探讨了这些技术如何应用于城市移动设计、公共安全规划和智慧城市数字孪生开发。&lt;h4&gt;背景&lt;/h4&gt;理解和预测行人动力学对于塑造更安全、更响应性、以人为中心的城市环境至关重要。随着人工智能技术的发展，数据驱动的行人轨迹预测和人群模拟研究日益增多，需要系统梳理该领域的发展脉络。&lt;h4&gt;目的&lt;/h4&gt;通过文献计量分析，绘制数据驱动的行人轨迹预测和人群模拟研究的知识演进图和跨学科结构，识别主要趋势、有影响力的贡献者和新兴前沿领域。&lt;h4&gt;方法&lt;/h4&gt;使用Web of Science核心合集的文献计量数据，采用SciExplorer和Bibliometrix工具进行分析，识别该领域的主要研究趋势、有影响力的贡献者和新兴前沿领域。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现人工智能、城市信息和人群行为建模之间存在强烈的融合趋势，这种融合由图神经网络、transformers和生成模型驱动。除了技术进步外，该领域越来越多地为城市移动设计、公共安全规划和智慧城市数字孪生开发提供信息。然而，该领域在确保可解释性、包容性和跨领域可转移性方面仍然存在挑战。&lt;h4&gt;结论&lt;/h4&gt;通过将方法轨迹与城市应用联系起来，这项工作强调了数据驱动方法如何丰富城市治理，并为未来城市的自适应、社会负责的移动智能铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;理解和预测行人动力学对于塑造更安全、更具响应性、以人为中心的城市环境已成为必不可少的工作。本研究对数据驱动的行人轨迹预测和人群模拟研究进行了全面的文献计量分析，绘制了其知识演进和跨学科结构。利用Web of Science核心合集的文献计量数据，我们采用SciExplorer和Bibliometrix来识别主要趋势、有影响力的贡献者和新兴前沿。结果表明，人工智能、城市信息和人群行为建模之间存在强烈的融合趋势，这种趋势由图神经网络、transformers和生成模型驱动。除了技术进步外，该领域越来越多地影响着城市移动设计、公共安全规划和智慧城市数字孪生开发。然而，在确保可解释性、包容性和跨领域可转移性方面仍然存在挑战。通过将方法轨迹与城市应用联系起来，这项工作强调了数据驱动方法如何丰富城市治理，并为未来城市的自适应、社会负责的移动智能铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting pedestrian dynamics has become essential forshaping safer, more responsive, and human-centered urban environments. Thisstudy conducts a comprehensive scientometric analysis of research ondata-driven pedestrian trajectory prediction and crowd simulation, mapping itsintellectual evolution and interdisciplinary structure. Using bibliometric datafrom the Web of Science Core Collection, we employ SciExplorer and Bibliometrixto identify major trends, influential contributors, and emerging frontiers.Results reveal a strong convergence between artificial intelligence, urbaninformatics, and crowd behavior modeling--driven by graph neural networks,transformers, and generative models. Beyond technical advances, the fieldincreasingly informs urban mobility design, public safety planning, and digitaltwin development for smart cities. However, challenges remain in ensuringinterpretability, inclusivity, and cross-domain transferability. By connectingmethodological trajectories with urban applications, this work highlights howdata-driven approaches can enrich urban governance and pave the way foradaptive, socially responsible mobility intelligence in future cities.</description>
      <author>example@mail.com (Junhao Xu, Hui Zeng)</author>
      <guid isPermaLink="false">2510.10327v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Preference-driven Knowledge Distillation for Few-shot Node Classification</title>
      <link>http://arxiv.org/abs/2510.10116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种偏好驱动的知识蒸馏(PKD)框架，结合大型语言模型和图神经网络的优势，用于少样本节点分类任务。&lt;h4&gt;背景&lt;/h4&gt;图神经网络能高效处理带文本属性的图，但训练依赖人工标注标签；现实世界中TAGs节点的复杂多样局部拓扑结构使单一机制难以处理；大型语言模型在TAGs的零样本/少样本学习中表现良好，但面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;协同大型语言模型和多种图神经网络的优势，解决少样本节点分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出偏好驱动的知识蒸馏框架，包括：(1)GNN偏好驱动的节点选择器，促进从LLMs到教师GNN的预测蒸馏；(2)节点偏好驱动的GNN选择器，为每个节点识别最合适的教师GNN，实现定制化知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在真实世界TAGs的少样本节点分类任务中表现出色，能够有效处理节点复杂的局部拓扑结构。&lt;h4&gt;结论&lt;/h4&gt;PKD框架能够有效结合大型语言模型和图神经网络的优势，解决少样本节点分类问题，并在真实数据集上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)由于其消息传递机制能够高效处理带文本属性的图(TAGs)，但它们的训练严重依赖人工标注的标签。此外，现实世界中TAGs节点的复杂多样的局部拓扑结构使得单一机制难以处理。大型语言模型(LLMs)在TAGs的零样本/少样本学习中表现良好，但面临可扩展性挑战。因此，我们提出了一种偏好驱动的知识蒸馏(PKD)框架，协同大型语言模型和多种图神经网络的优势用于少样本节点分类。具体而言，我们开发了一个GNN偏好驱动的节点选择器，有效促进从LLMs到教师GNN的预测蒸馏。为进一步处理节点复杂的局部拓扑结构，我们开发了一个节点偏好驱动的GNN选择器，为每个节点识别最合适的教师GNN，从而促进从教师GNN到学生GNN的定制化知识蒸馏。大量实验验证了我们所提出的框架在真实世界TAGs的少样本节点分类中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) can efficiently process text-attributed graphs(TAGs) due to their message-passing mechanisms, but their training heavilyrelies on the human-annotated labels. Moreover, the complex and diverse localtopologies of nodes of real-world TAGs make it challenging for a singlemechanism to handle. Large language models (LLMs) perform well inzero-/few-shot learning on TAGs but suffer from a scalability challenge.Therefore, we propose a preference-driven knowledge distillation (PKD)framework to synergize the complementary strengths of LLMs and various GNNs forfew-shot node classification. Specifically, we develop a GNN-preference-drivennode selector that effectively promotes prediction distillation from LLMs toteacher GNNs. To further tackle nodes' intricate local topologies, we develop anode-preference-driven GNN selector that identifies the most suitable teacherGNN for each node, thereby facilitating tailored knowledge distillation fromteacher GNNs to the student GNN. Extensive experiments validate the efficacy ofour proposed framework in few-shot node classification on real-world TAGs.</description>
      <author>example@mail.com (Xing Wei, Chunchun Chen, Rui Fan, Xiaofeng Cao, Sourav Medya, Wei Ye)</author>
      <guid isPermaLink="false">2510.10116v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Structure-Aware Attention and Knowledge Graphs in Explainable Recommendation Systems</title>
      <link>http://arxiv.org/abs/2510.10109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文设计并实现了一个结合知识图谱和结构感知注意力机制的可解释推荐模型，该模型基于图神经网络，采用多跳邻居聚合策略，能够捕获用户隐式偏好关系。&lt;h4&gt;背景&lt;/h4&gt;推荐系统领域需要更有效地利用知识图谱信息来提高推荐性能，同时需要模型具有可解释性。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够整合知识图谱结构信息，并通过注意力机制动态分配邻居重要性的推荐模型，以提高推荐的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;构建基于图神经网络的推荐模型，将用户和项目嵌入统一图结构，利用知识图谱构建多级语义路径提取上下文信息，通过用户和项目表示交互生成推荐，使用二元交叉熵损失函数优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;在Amazon Books数据集上的实验表明，所提出模型在各种评估指标上表现优越，具有良好的收敛性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;结构感知注意力机制在知识图谱增强推荐中具有有效性和实用性，能够提高推荐性能并提供更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文设计并实现了一个可解释的推荐模型，该模型将知识图谱与结构感知注意力机制相结合。该模型基于图神经网络构建，并采用了多跳邻居聚合策略。通过整合知识图谱的结构信息，并通过注意力机制动态分配不同邻居的重要性，模型增强了捕获隐式偏好关系的能力。在所提出的方法中，用户和项目被嵌入到统一的图结构中。基于知识图谱中的实体和关系构建多级语义路径，以提取更丰富的上下文信息。在评分预测阶段，通过用户和目标项目表示之间的交互生成推荐。模型使用二元交叉熵损失函数进行优化。在Amazon Books数据集上进行的实验验证了所提出模型在各种评估指标上的优越性能。该模型还表现出良好的收敛性和稳定性。这些结果进一步证明了结构感知注意力机制在知识图谱增强推荐中的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper designs and implements an explainable recommendation model thatintegrates knowledge graphs with structure-aware attention mechanisms. Themodel is built on graph neural networks and incorporates a multi-hop neighboraggregation strategy. By integrating the structural information of knowledgegraphs and dynamically assigning importance to different neighbors through anattention mechanism, the model enhances its ability to capture implicitpreference relationships. In the proposed method, users and items are embeddedinto a unified graph structure. Multi-level semantic paths are constructedbased on entities and relations in the knowledge graph to extract richercontextual information. During the rating prediction phase, recommendations aregenerated through the interaction between user and target item representations.The model is optimized using a binary cross-entropy loss function. Experimentsconducted on the Amazon Books dataset validate the superior performance of theproposed model across various evaluation metrics. The model also shows goodconvergence and stability. These results further demonstrate the effectivenessand practicality of structure-aware attention mechanisms in knowledgegraph-enhanced recommendation.</description>
      <author>example@mail.com (Shuangquan Lyu, Ming Wang, Huajun Zhang, Jiasen Zheng, Junjiang Lin, Xiaoxuan Sun)</author>
      <guid isPermaLink="false">2510.10109v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation</title>
      <link>http://arxiv.org/abs/2510.10105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Lighter-X框架，解决了传统图神经网络推荐系统在大规模部署中的可扩展性问题，通过高效压缩和解耦框架实现了参数和计算复杂度的显著降低。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在推荐系统中表现优异，但传统方法如LightGCN需要为每个节点维护嵌入向量，导致参数复杂度为O(n×d)，其中n是用户和物品总数，这在大规模应用中面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效且模块化的框架，能够无缝集成到现有GNN推荐器架构中，减少参数大小和计算复杂度，同时保持理论保证和经验性能，实现大规模实际部署。&lt;h4&gt;方法&lt;/h4&gt;分析原始结构和参数中的固有冗余，提出稀疏邻接结构和高维嵌入矩阵的高效压缩方案，将参数复杂度从O(n×d)降低到O(h×d)（h&lt;&lt;n），并通过解耦框架优化模型，减少训练过程中的计算复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;Lighter-X在保持与基线模型相当性能的同时，显著减少了参数需求；在有数百万条边的大规模交互图上，仅使用LightGCN 1%的参数就能获得更好的结果。&lt;h4&gt;结论&lt;/h4&gt;Lighter-X框架能够在保持推荐性能的同时大幅降低参数需求，使大规模图神经网络推荐系统的实际部署成为可能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在推荐系统中已展现出显著的有效性。然而，传统的基于图的推荐系统，如LightGCN，需要为每个节点维护大小为d的嵌入，导致参数复杂度为O(n×d)，其中n代表用户和物品的总数。这种扩展模式在实际应用的大规模图部署中构成了重大挑战。为解决这一可扩展性限制，我们提出了Lighter-X，这是一个高效且模块化的框架，可以无缝集成到现有的基于GNN的推荐器架构中。我们的方法显著减少了参数大小和计算复杂度，同时保留了基础模型的理论保证和经验性能，从而实现了大规模的实际部署。具体而言，我们分析了原始结构和参数中的固有冗余，识别了优化机会。基于这一洞察，我们提出了稀疏邻接结构和高维嵌入矩阵的高效压缩方案，实现了O(h×d)的参数复杂度，其中h&lt;&lt;n。此外，模型通过解耦框架进行优化，减少了训练过程中的计算复杂度并提高了可扩展性。大量实验表明，Lighter-X以显著更少的参数实现了与基线模型相当的性能。特别是在有数百万条边的大规模交互图上，我们仅使用LightGCN 1%的参数就能获得更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness inrecommendation systems. However, conventional graph-based recommenders, such asLightGCN, require maintaining embeddings of size $d$ for each node, resultingin a parameter complexity of $\mathcal{O}(n \times d)$, where $n$ representsthe total number of users and items. This scaling pattern poses significantchallenges for deployment on large-scale graphs encountered in real-worldapplications. To address this scalability limitation, we propose\textbf{Lighter-X}, an efficient and modular framework that can be seamlesslyintegrated with existing GNN-based recommender architectures. Our approachsubstantially reduces both parameter size and computational complexity whilepreserving the theoretical guarantees and empirical performance of the basemodels, thereby enabling practical deployment at scale. Specifically, weanalyze the original structure and inherent redundancy in their parameters,identifying opportunities for optimization. Based on this insight, we proposean efficient compression scheme for the sparse adjacency structure andhigh-dimensional embedding matrices, achieving a parameter complexity of$\mathcal{O}(h \times d)$, where $h \ll n$. Furthermore, the model is optimizedthrough a decoupled framework, reducing computational complexity during thetraining process and enhancing scalability. Extensive experiments demonstratethat Lighter-X achieves comparable performance to baseline models withsignificantly fewer parameters. In particular, on large-scale interactiongraphs with millions of edges, we are able to attain even better results withonly 1\% of the parameter over LightGCN.</description>
      <author>example@mail.com (Yanping Zheng, Zhewei Wei, Frank de Hoog, Xu Chen, Hongteng Xu, Yuhang Ye, Jiadeng Huang)</author>
      <guid isPermaLink="false">2510.10105v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Rademacher Meets Colors: More Expressivity, but at What Cost ?</title>
      <link>http://arxiv.org/abs/2510.10101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过着色算法的视角，揭示了图神经网络表达能力和泛化能力之间的权衡关系，证明WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度，从而解释了为什么更强的表达能力往往导致更弱的泛化保证。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)的表达能力通常通过它们与图同构测试(如Weisfeiler-Leman层次结构)的对应关系来理解。更具表达能力的GNN能够区分更多种类的图，但也观察到它们有更高的泛化误差。&lt;h4&gt;目的&lt;/h4&gt;提供图神经网络表达能力和泛化能力之间权衡关系的理论解释，统一表达能力和泛化的研究，为增加表达能力以泛化为代价的现象提供原则性理解。&lt;h4&gt;方法&lt;/h4&gt;通过着色算法的视角，将表达能力和泛化能力联系起来，分析WL着色诱导的等价类数量对GNN的Rademacher复杂度的影响，并研究Rademacher复杂度在不同样本的颜色计数扰动下的稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;1) WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度；2) 更强的表达能力导致更高的复杂度，从而更弱的泛化保证；3) Rademacher复杂度在不同样本的颜色计数扰动下是稳定的，确保了数据集间采样变异性的鲁棒性；4) 该框架适用于任意GNN架构和表达能力度量。&lt;h4&gt;结论&lt;/h4&gt;图神经网络的表达能力和泛化能力之间存在权衡关系，增加表达能力通常以泛化为代价。这一发现为理解和设计GNN提供了重要指导，强调了在追求高表达能力的同时需要考虑泛化性能的重要性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)的表达能力通常通过它们与图同构测试(如Weisfeiler-Leman(WL)层次结构)的对应关系来理解。虽然更具表达能力的GNN能够区分更多种类的图，但也观察到它们有更高的泛化误差。这项工作通过着色算法的视角为这种权衡提供了理论解释。具体来说，我们证明WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度——这是泛化的一个关键数据相关度量。我们的分析表明，更强的表达能力导致更高的复杂度，从而更弱的泛化保证。此外，我们证明了Rademacher复杂度在不同样本的颜色计数扰动下是稳定的，确保了数据集间采样变异性的鲁棒性。重要的是，我们的框架不仅限于消息传递GNN或1-WL，还扩展到将图划分为等价类的任意GNN架构和表达能力度量。这些结果统一了GNN中表达能力和泛化的研究，为为什么增加表达能力通常以泛化为代价提供了原则性理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The expressive power of graph neural networks (GNNs) is typically understoodthrough their correspondence with graph isomorphism tests such as theWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish aricher set of graphs, they are also observed to suffer from highergeneralization error. This work provides a theoretical explanation for thistrade-off by linking expressivity and generalization through the lens ofcoloring algorithms. Specifically, we show that the number of equivalenceclasses induced by WL colorings directly bounds the GNNs Rademacher complexity-- a key data-dependent measure of generalization. Our analysis reveals thatgreater expressivity leads to higher complexity and thus weaker generalizationguarantees. Furthermore, we prove that the Rademacher complexity is stableunder perturbations in the color counts across different samples, ensuringrobustness to sampling variability across datasets. Importantly, our frameworkis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNNarchitectures and expressivity measures that partition graphs into equivalenceclasses. These results unify the study of expressivity and generalization inGNNs, providing a principled understanding of why increasing expressive poweroften comes at the cost of generalization.</description>
      <author>example@mail.com (Martin Carrasco, Caio Deberaldini Netto, Vahan A. Martirosyan, Aneeqa Mehrab, Ehimare Okoyomon, Caterina Graziani)</author>
      <guid isPermaLink="false">2510.10101v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection</title>
      <link>http://arxiv.org/abs/2510.09984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GeminiNet的统一神经网络方法，用于从函数调用图(FCGs)和进程调用图(PCGs)中学习联合嵌入，实现对软件系统的多视角分析。&lt;h4&gt;背景&lt;/h4&gt;软件系统可以表示为图来捕获函数和进程间的依赖关系，根据不同目标可构建不同类型的图，如函数调用图和进程交互图。虽然这些图相关但视角不同，提供互补信息。先前研究多关注单一图表示，联合建模两种图的方法研究不足。&lt;h4&gt;目的&lt;/h4&gt;探索对函数调用图和进程调用图进行联合建模，实现对软件系统更深层次、多视角的分析。&lt;h4&gt;方法&lt;/h4&gt;提出GeminiNet统一神经网络方法，构建包含635个Windows可执行文件(318个恶意和317个良性)的数据集，使用Ghidra提取FCGs，Any.Run沙箱提取PCGs。采用双图卷积分支和自适应门控机制，平衡静态和动态视图的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;联合嵌入方法优于单图模型，能够提供更全面的软件系统分析。&lt;h4&gt;结论&lt;/h4&gt;通过联合建模函数调用图和进程调用图，可以实现对软件系统更全面、准确的分析和理解，有助于软件行为分析和安全检测。&lt;h4&gt;翻译&lt;/h4&gt;软件系统可以表示为图，捕获函数和进程之间的依赖关系。软件系统的一个有趣方面是，根据提取目标和优先级的不同，它们可以表示为不同类型的图。例如，可以捕获软件内的函数调用以创建函数调用图，突出函数之间的关系和依赖。或者，可以对软件生成的进程进行建模，生成进程交互图，关注运行时行为和进程间通信。虽然这些图表示相关，但每个都捕获了系统的不同视角，提供了对其结构和操作的互补见解。虽然先前的研究利用图神经网络分析软件行为，但大多数工作只关注单一类型的图表示。函数调用图和进程交互图的联合建模在很大程度上仍未被探索，留下了对软件系统进行更深层次、多视角分析的机会。本文提出了一个构建和训练函数调用图和进程调用图以及学习联合嵌入的流程。我们证明联合嵌入优于单图模型。在本文中，我们提出了GeminiNet，一种统一的神经网络方法，用于从函数调用图和进程调用图中学习联合嵌入。我们构建了一个包含635个Windows可执行文件的新数据集，使用Ghidra提取函数调用图，使用Any.Run沙箱提取进程调用图。GeminiNet采用双图卷积分支和自适应门控机制，以平衡静态和动态视图的贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Software systems can be represented as graphs, capturing dependencies amongfunctions and processes. An interesting aspect of software systems is that theycan be represented as different types of graphs, depending on the extractiongoals and priorities. For example, function calls within the software can becaptured to create function call graphs, which highlight the relationshipsbetween functions and their dependencies. Alternatively, the processes spawnedby the software can be modeled to generate process interaction graphs, whichfocus on runtime behavior and inter-process communication. While these graphrepresentations are related, each captures a distinct perspective of thesystem, providing complementary insights into its structure and operation.While previous studies have leveraged graph neural networks (GNNs) to analyzesoftware behaviors, most of this work has focused on a single type of graphrepresentation. The joint modeling of both function call graphs and processinteraction graphs remains largely underexplored, leaving opportunities fordeeper, multi-perspective analysis of software systems. This paper presents apipeline for constructing and training Function Call Graphs (FCGs) and ProcessCall Graphs (PCGs) and learning joint embeddings. We demonstrate that jointembeddings outperform a single-graph model. In this paper, we proposeGeminiNet, a unified neural network approach that learns joint embeddings fromboth FCGs and PCGs. We construct a new dataset of 635 Windows executables (318malicious and 317 benign), extracting FCGs via Ghidra and PCGs via Any.Runsandbox. GeminiNet employs dual graph convolutional branches with an adaptivegating mechanism that balances contributions from static and dynamic views.</description>
      <author>example@mail.com (Kartikeya Aneja, Nagender Aneja, Murat Kantarcioglu)</author>
      <guid isPermaLink="false">2510.09984v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications</title>
      <link>http://arxiv.org/abs/2510.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了复值卷积神经网络(CVCNNs)在音频信号处理中的设计与应用，重点关注实值网络中常被忽略的相位信息的保留和利用。通过理论基础介绍、训练技术调整和三个阶段的实验评估，证明了复值架构的表现能力和相位作为音频处理中可利用特征的价值。&lt;h4&gt;背景&lt;/h4&gt;在音频信号处理中，相位信息通常被实值神经网络所忽略，而复值神经网络为保留和利用这些信息提供了可能。&lt;h4&gt;目的&lt;/h4&gt;研究复值卷积神经网络(CVCNNs)在音频信号处理中的设计和应用，探索如何有效利用通常被实值网络忽略的相位信息。&lt;h4&gt;方法&lt;/h4&gt;介绍CVCNNs的基础理论概念(复卷积、池化层、基于Wirtinger的微分和复值激活函数)，调整训练技术(复值批归一化和权重初始化方案)，并通过三个阶段实验评估：在图像数据集上基准测试、使用MFCC进行音频分类、引入GNN通过边权重建模相位信息。&lt;h4&gt;主要发现&lt;/h4&gt;CVCNNs在图像数据集上表现与实值CNN相当；在音频分类中，CVCNNs在实值MFCC上训练时略微优于实值CNN，但保留相位存在挑战；包含相位的GNNs在二元和多流派分类中带来可衡量的性能提升；心脏形激活函数等方法显示出前景。&lt;h4&gt;结论&lt;/h4&gt;复值架构具有强大的表达能力，相位是音频处理中一个有意义且可利用的特征。未来在相位感知设计方面的进步对于利用复表示在神经网络中的潜力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了复值卷积神经网络(CVCNNs)在音频信号处理中的设计与应用，重点关注实值网络中常被忽略的相位信息的保留和利用。我们首先介绍CVCNNs的基础理论概念，包括复卷积、池化层、基于Wirtinger的微分以及各种复值激活函数。这些理论概念辅以关键的训练技术调整，包括复值批归一化和权重初始化方案，以确保训练动力学的稳定性。实证评估分为三个阶段进行。首先，在标准图像数据集上对CVCNNs进行基准测试，它们表现出与实值CNNs相当的竞争力，即使在合成复扰动下也是如此。在第二个实验中，我们专注于使用梅尔频率倒谱系数(MFCC)进行音频分类。在实值MFCC上训练的CVCNNs略微优于实值CNN，而在输入工作流中保留相位则凸显了在没有架构修改的情况下利用相位的挑战。最后，第三个实验引入了GNNs通过边权重建模相位信息，其中包含相位在二元和多流派分类中都带来了可衡量的性能提升。这些结果强调了复值架构的表现能力，并确认了相位是音频处理应用中一个有意义且可利用的特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study explores the design and application of Complex-ValuedConvolutional Neural Networks (CVCNNs) in audio signal processing, with a focuson preserving and utilizing phase information often neglected in real-valuednetworks. We begin by presenting the foundational theoretical concepts ofCVCNNs, including complex convolutions, pooling layers, Wirtinger-baseddifferentiation, and various complex-valued activation functions. These arecomplemented by critical adaptations of training techniques, including complexbatch normalization and weight initialization schemes, to ensure stability intraining dynamics. Empirical evaluations are conducted across three stages.First, CVCNNs are benchmarked on standard image datasets, where theydemonstrate competitive performance with real-valued CNNs, even under syntheticcomplex perturbations. Although our focus is audio signal processing, we firstevaluate CVCNNs on image datasets to establish baseline performance andvalidate training stability before applying them to audio tasks. In the secondexperiment, we focus on audio classification using Mel-Frequency CepstralCoefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperformreal CNNs, while preserving phase in input workflows highlights challenges inexploiting phase without architectural modifications. Finally, a thirdexperiment introduces GNNs to model phase information via edge weighting, wherethe inclusion of phase yields measurable gains in both binary and multi-classgenre classification. These results underscore the expressive capacity ofcomplex-valued architectures and confirm phase as a meaningful and exploitablefeature in audio processing applications. While current methods show promise,especially with activations like cardioid, future advances in phase-awaredesign will be essential to leverage the potential of complex representationsin neural networks.</description>
      <author>example@mail.com (Naman Agrawal)</author>
      <guid isPermaLink="false">2510.09926v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering</title>
      <link>http://arxiv.org/abs/2510.09854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为NG-Router的新框架，用于解决营养问答系统中的推理能力有限和上下文过载问题，通过知识图引导的多智能体协作提高系统性能。&lt;h4&gt;背景&lt;/h4&gt;饮食在人类健康中起核心作用，营养问答系统为个性化饮食指导和预防饮食相关慢性疾病提供了有前景的路径。然而，现有方法面临单智能体系统推理能力有限、设计有效多智能体架构复杂以及上下文过载阻碍准确决策等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理复杂营养健康任务的多智能体推理框架，解决现有营养问答系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出Nutritional-Graph Router (NG-Router)框架，将营养问答视为有监督的、知识图引导的多智能体协作问题。该框架将智能体节点整合到异构知识图中，使用图神经网络学习任务感知的路由分布，并采用基于梯度的子图检索机制来解决上下文过载问题。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试和骨干模型上的实验表明，NG-Router始终优于单智能体和集成基线方法，能够有效增强多跳和关系推理能力。&lt;h4&gt;结论&lt;/h4&gt;NG-Router为复杂营养健康任务提供了一种有原则的领域感知多智能体推理方法，代表了营养问答系统的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;饮食在人类健康中扮演核心角色，营养问答为个性化饮食指导和预防饮食相关慢性疾病提供了有前景的路径。然而，现有方法面临两个基本挑战：单智能体系统的有限推理能力以及设计有效多智能体架构的复杂性，还有阻碍准确决策的上下文过载。我们引入了营养图路由器，这是一个新框架，将营养问答制定为一个有监督的、知识图引导的多智能体协作问题。营养图路由器将智能体节点整合到异构知识图中，并采用图神经网络来学习智能体上的任务感知路由分布，利用从经验智能体性能中获得的软监督。为了进一步解决上下文过载，我们提出了一种基于梯度的子图检索机制，在训练过程中识别显著证据，从而增强多跳和关系推理。在多个基准测试和骨干模型上的广泛实验表明，营养图路由器始终优于单智能体和集成基线，为复杂营养健康任务提供了一种有原则的领域感知多智能体推理方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diet plays a central role in human health, and Nutrition Question Answering(QA) offers a promising path toward personalized dietary guidance and theprevention of diet-related chronic diseases. However, existing methods face twofundamental challenges: the limited reasoning capacity of single-agent systemsand the complexity of designing effective multi-agent architectures, as well ascontextual overload that hinders accurate decision-making. We introduceNutritional-Graph Router (NG-Router), a novel framework that formulatesnutritional QA as a supervised, knowledge-graph-guided multi-agentcollaboration problem. NG-Router integrates agent nodes into heterogeneousknowledge graphs and employs a graph neural network to learn task-aware routingdistributions over agents, leveraging soft supervision derived from empiricalagent performance. To further address contextual overload, we propose agradient-based subgraph retrieval mechanism that identifies salient evidenceduring training, thereby enhancing multi-hop and relational reasoning.Extensive experiments across multiple benchmarks and backbone modelsdemonstrate that NG-Router consistently outperforms both single-agent andensemble baselines, offering a principled approach to domain-aware multi-agentreasoning for complex nutritional health tasks.</description>
      <author>example@mail.com (Kaiwen Shi, Zheyuan Zhang, Zhengqing Yuan, Keerthiram Murugesan, Vincent Galass, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2510.09854v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Geo-Aware Models for Stream Temperature Prediction across Different Spatial Regions and Scales</title>
      <link>http://arxiv.org/abs/2510.09500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Geo-STARS，一个地理感知时空建模框架，用于预测不同流域和空间尺度的河流水温，通过引入地理感知嵌入解决了现有模型在跨区域和尺度推广方面的问题。&lt;h4&gt;背景&lt;/h4&gt;理解环境生态系统对地球可持续管理至关重要，但现有基于物理和数据驱动的模型因数据异质性和有限观测样本而难以推广到不同空间区域和尺度。&lt;h4&gt;目的&lt;/h4&gt;开发Geo-STARS框架，实现跨流域和空间尺度的河流水温预测，解决现有模型的泛化问题。&lt;h4&gt;方法&lt;/h4&gt;Geo-STARS引入地理感知嵌入来捕捉跨空间区域和尺度的共享原则和模式，并将其整合到门控时空图神经网络中，使模型能够在地理和水文背景下学习复杂时空模式，即使数据稀疏或缺失。&lt;h4&gt;主要发现&lt;/h4&gt;在美国东部海岸多个流域37年真实数据集上评估，Geo-STARS在区域和尺度上都表现出优于最先进基线的泛化性能。&lt;h4&gt;结论&lt;/h4&gt;Geo-STARS为可扩展、数据高效的环境监测和决策制定提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解环境生态系统对我们星球的可持续管理至关重要。然而，现有的基于物理和数据驱动的模型往往由于现实环境生态系统中固有的数据异质性而无法推广到不同的空间区域和尺度。由于可用于模型训练的观测样本有限，这种推广问题进一步加剧。为解决这些问题，我们提出了Geo-STARS，一个用于预测不同流域和空间尺度河流水温的地理感知时空建模框架。Geo-STARS的主要创新是引入地理感知嵌入，它利用地理信息来明确捕捉跨空间区域和尺度的共享原则和模式。我们将地理感知嵌入进一步整合到门控时空图神经网络中。这种设计使模型能够在地理和水文背景的指导下学习复杂的时空模式，即使有稀疏或无观测数据也是如此。我们在预测河流水温方面评估了Geo-STARS的有效性，河流水质是水质量的主导因素。使用美国东部海岸多个流域跨越37年的真实世界数据集，Geo-STARS展示了其在区域和尺度上的优越泛化性能，优于最先进的基线。这些结果突显了Geo-STARS在可扩展、数据高效的环境监测和决策制定方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding environmental ecosystems is vital for the sustainablemanagement of our planet. However,existing physics-based and data-driven modelsoften fail to generalize to varying spatial regions and scales due to theinherent data heterogeneity presented in real environmental ecosystems. Thisgeneralization issue is further exacerbated by the limited observation samplesavailable for model training. To address these issues, we propose Geo-STARS, ageo-aware spatio-temporal modeling framework for predicting stream watertemperature across different watersheds and spatial scales. The majorinnovation of Geo-STARS is the introduction of geo-aware embedding, whichleverages geographic information to explicitly capture shared principles andpatterns across spatial regions and scales. We further integrate the geo-awareembedding into a gated spatio-temporal graph neural network. This designenables the model to learn complex spatial and temporal patterns guided bygeographic and hydrological context, even with sparse or no observational data.We evaluate Geo-STARS's efficacy in predicting stream water temperature, whichis a master factor for water quality. Using real-world datasets spanning 37years across multiple watersheds along the eastern coast of the United States,Geo-STARS demonstrates its superior generalization performance across bothregions and scales, outperforming state-of-the-art baselines. These resultshighlight the promise of Geo-STARS for scalable, data-efficient environmentalmonitoring and decision-making.</description>
      <author>example@mail.com (Shiyuan Luo, Runlong Yu, Shengyu Chen, Yingda Fan, Yiqun Xie, Yanhua Li, Xiaowei Jia)</author>
      <guid isPermaLink="false">2510.09500v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN</title>
      <link>http://arxiv.org/abs/2510.09495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于向量量化-变分自编码器(VQ-VAE)的鲁棒预编码方法，用于频率双工系统，解决了传统高斯混合模型(GMM)组件数量随反馈比特数指数增长的问题。&lt;h4&gt;背景&lt;/h4&gt;在频率双工系统中，鲁棒预编码的有效实现需要结合传播环境的统计信息，传统方法使用高斯混合模型和图神经网络设计特定站点的预编码器。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预编码框架，解决GMM组件数量随反馈比特数指数增长的问题，并实现端到端训练，以提高多用户无线系统的性能。&lt;h4&gt;方法&lt;/h4&gt;利用向量量化-变分自编码器(VQ-VAE)替代GMM，构建结合图神经网络(GNN)、VQ-VAE和pilot优化的端到端(E2E)模型。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多用户无线系统中实现了显著的速率增益，性能优于传统的子离散傅里叶变换(DFT) pilot 矩量和迭代预编码算法。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架支持使用更少pilot或反馈比特的系统部署，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;通过生成模型整合传播环境的学习统计信息，频率双工系统中的鲁棒预编码可以有效实现。我们基于先前成功结合高斯混合模型(GMM)和图神经网络(GNN)设计特定站点预编码器的工作。本文通过使用向量量化-变分自编码器(VQ-VAE)，避免了GMM的一个关键缺点，即GMM组件数量随反馈比特数呈指数增长。此外，VQ-VAE的深度学习架构允许我们将GNN与VQ-VAE以及pilot优化联合训练，形成一个端到端(E2E)模型，从而在多用户无线系统中实现显著的速率增益。仿真结果表明，所提出的框架优于涉及子离散傅里叶变换(DFT) pilot矩阵和迭代预编码算法的传统方法，能够支持部署具有更少pilot或反馈比特的系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust precoding is efficiently feasible in frequency division duplex (FDD)systems by incorporating the learnt statistics of the propagation environmentthrough a generative model. We build on previous work that successfullydesigned site-specific precoders based on a combination of Gaussian mixturemodels (GMMs) and graph neural networks (GNNs). In this paper, by utilizing avector quantized-variational autoencoder (VQ-VAE), we circumvent one of the keydrawbacks of GMMs, i.e., the number of GMM components scales exponentially tothe feedback bits. In addition, the deep learning architecture of the VQ-VAEallows us to jointly train the GNN together with VQ-VAE along with pilotoptimization forming an end-to-end (E2E) model, resulting in considerableperformance gains in sum rate for multi-user wireless systems. Simulationsdemonstrate the superiority of the proposed frameworks over the conventionalmethods involving the sub-discrete Fourier transform (DFT) pilot matrix anditerative precoder algorithms enabling the deployment of systems characterizedby fewer pilots or feedback bits.</description>
      <author>example@mail.com (Srikar Allaparapu, Michael Baur, Benedikt Böck, Michael Joham, Wolfgang Utschick)</author>
      <guid isPermaLink="false">2510.09495v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models</title>
      <link>http://arxiv.org/abs/2510.09735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种结合图神经网络和大语言模型的跨模态框架InterCorpRel-LLM，用于识别企业间的供应链和竞争关系，在关系识别任务上显著优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;识别企业间的供应链和竞争关系对财务分析和公司治理至关重要，但由于企业数据的规模、稀疏性和上下文依赖性，这一任务具有挑战性。基于图的方法能捕捉结构但缺乏语义深度，而大语言模型擅长文本处理但在表示关系依赖方面能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效建模企业网络结构和语义信息的框架，以准确识别企业间的供应链和竞争关系。&lt;h4&gt;方法&lt;/h4&gt;提出InterCorpRel-LLM，一个结合GNNs和LLMs的跨模态框架，使用来自FactSet供应链记录的专有数据集，并设计了三个定制训练任务：公司图匹配、行业分类和供应链关系预测。&lt;h4&gt;主要发现&lt;/h4&gt;InterCorpRel-LLM在供应链关系识别任务上显著优于强基线模型(包括GPT-5)，仅使用7B参数主干和轻量级训练就达到了0.8543的F分数(基线为0.2287)。该模型还能推广到零样本竞争者识别，展示了捕捉微妙企业间动态的能力。&lt;h4&gt;结论&lt;/h4&gt;InterCorpRel-LLM为分析师和战略家提供了绘制和推理复杂企业网络的强大工具，增强了动态市场中的决策制定和风险管理能力。&lt;h4&gt;翻译&lt;/h4&gt;识别企业间的供应链和竞争关系对财务分析和公司治理至关重要，但由于企业数据的规模、稀疏性和上下文依赖性，这一任务仍然具有挑战性。基于图的方法能捕捉结构但缺乏语义深度，而大语言模型擅长文本但在表示关系依赖方面能力有限。为此，我们提出了InterCorpRel-LLM，这是一个结合GNNs和LLMs的跨模态框架，支持来自FactSet供应链记录的专有数据集和三个定制训练任务：公司图匹配、行业分类和供应链关系预测。这种设计能够有效建模结构和语义的联合表示。实验表明，在供应链关系识别任务上，InterCorpRel-LLM显著优于强基线模型(包括GPT-5)，仅使用7B参数主干和轻量级训练就达到了0.8543的F分数(对比基线的0.2287)。该模型还能推广到零样本竞争者识别，突显了其捕捉微妙企业间动态的能力。因此，我们的框架为分析师和战略家提供了绘制和推理复杂企业网络的强大工具，增强了动态市场中的决策制定和风险管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying inter-firm relationships such as supply and competitive ties iscritical for financial analysis and corporate governance, yet remainschallenging due to the scale, sparsity, and contextual dependence of corporatedata. Graph-based methods capture structure but miss semantic depth, whilelarge language models (LLMs) excel at text but remain limited in their abilityto represent relational dependencies. To address this, we proposeInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,supported by a proprietary dataset derived from FactSet supply chain recordsand three tailored training tasks: company graph matching, industryclassification, and supply relation prediction. This design enables effectivejoint modeling of structure and semantics. Experiments show thatInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,on a supply relation identification task, achieving an F-score of 0.8543 vs.0.2287 with only a 7B-parameter backbone and lightweight training. The modelalso generalizes to zero-shot competitor identification, underscoring itsability to capture nuanced inter-firm dynamics. Our framework thus providesanalysts and strategists with a robust tool for mapping and reasoning aboutcomplex corporate networks, enhancing decision-making and risk management indynamic markets.</description>
      <author>example@mail.com (Qianyou Sun, Jiexin Zheng, Bohan Jin, Lihua Chen, Yijie Peng)</author>
      <guid isPermaLink="false">2510.09735v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Approach to SME Credit Scoring Integrating Transaction and Ownership Networks</title>
      <link>http://arxiv.org/abs/2510.09407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的中小企业信贷风险评估新方法，通过结合企业网络数据与传统数据提高了预测准确性，并揭示了企业间风险传染机制。&lt;h4&gt;背景&lt;/h4&gt;中小企业在经济增长、就业和创新中发挥重要作用，但面临获取信贷的挑战，包括有限财务历史、抵押品约束和宏观经济冲击风险。中小企业常在相互关联的网络中运营，违约风险可能通过网络传播。&lt;h4&gt;目的&lt;/h4&gt;提出并测试一种新的中小企业信贷风险建模方法，准确评估信贷风险，特别关注企业网络间的违约风险传播问题。&lt;h4&gt;方法&lt;/h4&gt;使用来自知名金融机构的独特大型中小企业贷款数据集，采用图神经网络预测中小企业违约，基于企业间共同所有权和金融交易的多层网络数据进行建模，并将网络数据与传统结构化数据结合。&lt;h4&gt;主要发现&lt;/h4&gt;结合网络数据和传统数据提高了申请评分性能；明确模拟了公司间的风险传染；连接的方向性和强度影响金融风险传染；网络数据具有预测能力；供应链网络使中小企业面临相关违约风险。&lt;h4&gt;结论&lt;/h4&gt;网络数据对预测中小企业违约风险具有重要作用，供应链网络在使中小企业面临相关违约风险方面扮演关键角色。&lt;h4&gt;翻译&lt;/h4&gt;中小企业(SMEs)在经济增长、就业和创新方面发挥着至关重要的作用。然而，由于有限的财务历史、抵押品约束和暴露于宏观经济冲击，它们在获取信贷方面往往面临重大挑战。这些挑战使得贷款人进行准确的信贷风险评估变得至关重要，特别是因为中小企业经常在相互关联的企业网络中运营，违约风险可以通过这些网络传播。本文提出并测试了一种新的中小企业信贷风险建模方法，使用来自知名金融机构的独特大型中小企业贷款数据集。具体而言，我们的方法采用图神经网络来预测中小企业违约，使用来自企业间共同所有权和金融交易的多层网络数据。我们表明，将此信息与传统结构化数据相结合不仅提高了申请评分性能，还明确模拟了公司之间的风险传染风险。进一步分析显示，这些连接的方向性和强度如何影响金融风险传染，从而提供了对潜在过程的更深入理解。我们的研究结果强调了网络数据的预测能力，以及供应链网络在使中小企业面临相关违约风险方面的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Small and Medium-sized Enterprises (SMEs) are known to play a vital role ineconomic growth, employment, and innovation. However, they tend to facesignificant challenges in accessing credit due to limited financial histories,collateral constraints, and exposure to macroeconomic shocks. These challengesmake an accurate credit risk assessment by lenders crucial, particularly sinceSMEs frequently operate within interconnected firm networks through whichdefault risk can propagate. This paper presents and tests a novel approach formodelling the risk of SME credit, using a unique large data set of SME loansprovided by a prominent financial institution. Specifically, our approachemploys Graph Neural Networks to predict SME default using multilayer networkdata derived from common ownership and financial transactions between firms. Weshow that combining this information with traditional structured data not onlyimproves application scoring performance, but also explicitly models contagionrisk between companies. Further analysis shows how the directionality andintensity of these connections influence financial risk contagion, offering adeeper understanding of the underlying processes. Our findings highlight thepredictive power of network data, as well as the role of supply chain networksin exposing SMEs to correlated default risk.</description>
      <author>example@mail.com (Sahab Zandi, Kamesh Korangi, Juan C. Moreno-Paredes, María Óskarsdóttir, Christophe Mues, Cristián Bravo)</author>
      <guid isPermaLink="false">2510.09407v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network</title>
      <link>http://arxiv.org/abs/2510.09350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGSPATIAL 2025 - GeoAI Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为XGeoAI的新框架，用于实时、可解释、多步列车延误预测，解决了现有研究中在网络规模多步自回归预测和实时可解释性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;铁路网络是现代经济的基石，但其运营效率持续受到列车延误级联效应的影响。准确预测延误传播对实时交通管理至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发并评估一种新的XGeoAI框架，用于实时、可解释、多步列车延误预测，为决策支持提供可靠工具。&lt;h4&gt;方法&lt;/h4&gt;构建了一个两阶段自回归图注意力网络(GAT)模型，使用覆盖荷兰铁路网络40%以上的真实世界数据集训练。该模型将系统表示为操作事件的时空图，并增加了站台和车站拥堵等细粒度特征。通过顺序的k步前预测协议进行评估，模拟真实世界中预测误差可能累积的情况。&lt;h4&gt;主要发现&lt;/h4&gt;提出的GATv2模型在纯误差指标(MAE)上比简单的持久性基线更具挑战性，但在分类延误事件方面实现了一贯的更高精度，这对可靠的决策支持工具至关重要。&lt;h4&gt;结论&lt;/h4&gt;XGeoAI框架能够提供实时、可解释的多步列车延误预测，特别适合作为决策支持工具，在延误事件分类方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;铁路网络的运营效率作为现代经济的基石，持续受到列车延误级联效应的破坏。准确预测这种延误传播是实时交通管理的关键挑战。尽管最近的研究利用图神经网络(GNNs)对铁路网络结构进行建模，但在开发能提供网络规模多步自回归预测的框架方面仍存在显著差距，同时缺乏决策支持所需的实时、可解释的解释。本文通过开发和评估一种新的XGeoAI框架来解决这一差距，该框架用于实时、可解释、多步列车延误预测。这项工作的核心是一个两阶段自回归图注意力网络(GAT)模型，在覆盖荷兰铁路网络40%以上的真实世界数据集上进行训练。该模型将系统表示为操作事件(到达和出发)的时空图，并增加了包括站台和车站拥堵在内的细粒度特征。为测试其在实时部署中的可行性，使用顺序的k步前预测协议对模型进行了严格评估，该协议模拟了预测误差可能累积的真实世界条件。结果表明，虽然提出的GATv2模型在纯误差指标(MAE)上比简单的持久性基线更具挑战性，但在分类延误事件方面实现了一贯的更高精度，这对可靠的决策支持工具至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3764912.3770828&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The operational efficiency of railway networks, a cornerstone of moderneconomies, is persistently undermined by the cascading effects of train delays.Accurately forecasting this delay propagation is a critical challenge forreal-time traffic management. While recent research has leveraged Graph NeuralNetworks (GNNs) to model the network structure of railways, a significant gapremains in developing frameworks that provide multi-step autoregressiveforecasts at a network-wide scale, while simultaneously offering the live,interpretable explanations needed for decision support. This paper addressesthis gap by developing and evaluating a novel XGeoAI framework for live,explainable, multi-step train delay forecasting. The core of this work is atwo-stage, autoregressive Graph Attention Network (GAT) model, trained on areal-world dataset covering over 40% of the Dutch railway network. The modelrepresents the system as a spatio-temporal graph of operational events(arrivals and departures) and is enriched with granular features, includingplatform and station congestion. To test its viability for live deployment, themodel is rigorously evaluated using a sequential, k-step-ahead forecastingprotocol that simulates real-world conditions where prediction errors cancompound. The results demonstrate that while the proposed GATv2 model ischallenged on pure error metrics (MAE) by a simpler Persistence baseline, itachieves consistently higher precision in classifying delay events -- a crucialadvantage for a reliable decision support tool.</description>
      <author>example@mail.com (Vu Duc Anh Nguyen, Ziyue Li)</author>
      <guid isPermaLink="false">2510.09350v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics</title>
      <link>http://arxiv.org/abs/2510.09082v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于复杂网络长期动态预测的高阶网络动力学识别方法，解决了现有方法只能处理成对关系且预测模型要么缺乏准确性要么缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;学习复杂网络动力学对于理解、建模和控制现实世界复杂系统至关重要。现有方法通常使用简单图来描述复杂网络中的关系，只能捕获成对关系，而网络中可能存在丰富的非成对结构化关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种用于复杂网络长期动态预测的高阶网络动力学识别方法，解决传统图机器学习只能处理成对关系的问题，同时提高预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入动态超图学习来捕获复杂网络中的高阶非成对关系，提高复杂网络建模的准确性；提出物理数据双驱动动态预测模块，利用Koopman算子理论将非线性动力学微分方程转化为线性系统求解，同时利用物理信息神经微分方程方法确保动态演化符合物理定律。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在公共数据集和自建产业链网络数据集上具有良好的预测精度和长期预测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过动态超图学习和双驱动动态预测模块，有效解决了复杂网络动态预测中的成对关系限制和预测准确性-可解释性权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;学习复杂网络动力学对于理解、建模和控制现实世界复杂系统至关重要。在预测复杂网络动态演化的任务中存在两个主要问题：一方面，现有方法通常使用简单图来描述复杂网络中的关系，然而这种方法只能捕获成对关系，而网络中可能存在丰富的非成对结构化关系。一阶GNN难以捕获动态非成对关系。另一方面，理论预测模型缺乏准确性，数据驱动预测模型缺乏可解释性。为解决上述问题，本文提出了一种用于复杂网络长期动态预测的高阶网络动力学识别方法。首先，为解决传统图机器学习只能处理成对关系的问题，引入动态超图学习来捕获复杂网络中的高阶非成对关系，提高复杂网络建模的准确性。然后，提出了物理数据双驱动动态预测模块。引入Koopman算子理论将复杂网络动态演化的非线性动力学微分方程转化为线性系统求解。同时，利用物理信息神经微分方程方法确保动态演化符合物理定律。双驱动动态预测模块确保了预测的准确性和可解释性。在公共数据集和自建产业链网络数据集上验证的实验结果表明，本文方法具有良好的预测精度和长期预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning complex network dynamics is fundamental to understanding, modellingand controlling real-world complex systems. There are two main problems in thetask of predicting the dynamic evolution of complex networks: on the one hand,existing methods usually use simple graphs to describe the relationships incomplex networks; however, this approach can only capture pairwiserelationships, while there may be rich non-pairwise structured relationships inthe network. First-order GNNs have difficulty in capturing dynamic non-pairwiserelationships. On the other hand, theoretical prediction models lack accuracyand data-driven prediction models lack interpretability. To address the aboveproblems, this paper proposes a higher-order network dynamics identificationmethod for long-term dynamic prediction of complex networks. Firstly, toaddress the problem that traditional graph machine learning can only deal withpairwise relations, dynamic hypergraph learning is introduced to capture thehigher-order non-pairwise relations among complex networks and improve theaccuracy of complex network modelling. Then, a dual-driven dynamic predictionmodule for physical data is proposed. The Koopman operator theory is introducedto transform the nonlinear dynamical differential equations for the dynamicevolution of complex networks into linear systems for solving. Meanwhile, thephysical information neural differential equation method is utilised to ensurethat the dynamic evolution conforms to the physical laws. The dual-drivedynamic prediction module ensures both accuracy and interpretability of theprediction. Validated on public datasets and self-built industrial chainnetwork datasets, the experimental results show that the method in this paperhas good prediction accuracy and long-term prediction performance.</description>
      <author>example@mail.com (Bicheng Wang, Junping Wang, Yibo Xue)</author>
      <guid isPermaLink="false">2510.09082v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A review of cultural heritage inspection: Toward terahertz from mid-infrared region</title>
      <link>http://arxiv.org/abs/2510.11521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述探讨了用于检测和分析文化遗产文物的非侵入式成像方法，覆盖中远红外到太赫兹光谱区域，并总结了这些技术的应用和最新信号处理进展。&lt;h4&gt;背景&lt;/h4&gt;文化遗产文物的保护和修复需要先进的无损检测技术，非侵入式成像(NII)方法在文化遗产研究中具有广泛应用。&lt;h4&gt;目的&lt;/h4&gt;总结NII技术在文化遗产研究中的应用，以及信号处理技术的最新进展，特别是深度学习在自动化分析中的革命性作用。&lt;h4&gt;方法&lt;/h4&gt;热红外域利用材料自发射特性；近红外通过外部照明增强表面细节；远红外和太赫兹技术以透射和反射模式穿透表面层；整合可见光和红外成像丰富诊断能力；应用深度学习进行自动分类、特征提取和缺陷检测。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习可通过监督和非监督学习可靠识别细微异常和材料变化，这些变化可能指示过去的修复或早期退化阶段；先进光谱成像、信号处理和神经网络的融合提供了更准确的数据驱动分析方法。&lt;h4&gt;结论&lt;/h4&gt;先进光谱成像、复杂信号处理和深度神经网络的结合为文化遗产分析提供了更准确、高效和数据驱动的途径，最终支持更明智的保护和修复决策。&lt;h4&gt;翻译&lt;/h4&gt;本综述探讨了覆盖中远红外至太赫兹光谱区域(最高约1000微米)的非侵入式成像(NII)方法，用于检测和分析文化遗产文物。在遵循普朗克定律的热红外域，材料的自发射揭示了材料的内在特性和内部退化。相比之下，在近红外范围内，外部照明增强了表面细节和颜料区分。远红外和太赫兹技术以透射和反射模式工作，通过穿透表面层提供亚表面结构和隐藏特征的补充见解。整合可见光和红外成像通过关联常规视觉评估与光谱信息进一步丰富了诊断能力。除了综述这些NII技术在文化遗产研究中的广泛应用外，本文还总结了信号处理的最新进展，包括硬件和软件发展。特别是，深度学习通过实现自动分类、特征提取、缺陷检测和超分辨率成像彻底改变了该领域。通过监督和非监督学习策略，神经网络可以可靠地识别指示过去修复或早期退化阶段的细微异常和材料变化。总之，先进光谱成像、复杂信号处理和深度神经网络的融合为文化遗产分析提供了更准确、高效和数据驱动的变革性途径，最终支持更明智的保护和修复决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review explores non-invasive imaging (NII) methods covering the mid- andfar-infrared to the terahertz spectral regions (up to approximately 1000 um)for the detection and analysis of cultural heritage artifacts. In the thermalinfrared domain, where radiation follows Planck's law, the self-emission ofmaterials reveals intrinsic properties and internal degradation. By contrast,in the near-infrared range, external illumination enhances surface details andpigment differentiation. Far-infrared and terahertz techniques, operating inboth transmission and reflection modes, provide complementary insights bypenetrating surface layers to uncover subsurface structures and concealedfeatures. Integrating visible and infrared imaging further enriches diagnosticcapabilities by correlating conventional visual assessments with spectralinformation. Beyond reviewing the wide applications of these NII techniques incultural heritage research, this work also summarizes recent advances in signalprocessing, encompassing both hardware and software developments. Inparticular, deep learning has revolutionized the field by enabling automatedclassification, feature extraction, defect detection, and super-resolutionimaging. Through supervised and unsupervised learning strategies, neuralnetworks can reliably identify subtle anomalies and material variationsindicative of past restorations or early stages of deterioration. Inconclusion, the convergence of advanced spectral imaging, sophisticated signalprocessing, and deep neural networks offers a transformative pathway towardmore accurate, efficient, and data-driven cultural heritage analysis,ultimately supporting more informed conservation and restoration decisions.</description>
      <author>example@mail.com (Pengfei Zhu, Hai Zhang, Stefano Sfarra, Dazhi Yang, Xavier Maldague)</author>
      <guid isPermaLink="false">2510.11521v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2510.11243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了尼泊尔手语的首个基准数据集，并评估了深度学习方法在手语识别中的有效性。&lt;h4&gt;背景&lt;/h4&gt;手语是听力和言语障碍人群的重要交流系统，但像尼泊尔手语这样的代表性不足的手语，其数字语言数据集资源仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;创建尼泊尔手语的基准数据集，并评估深度学习方法在手语识别任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含36个手势类别，每类1500个样本的数据集，并使用MobileNetV2和ResNet50架构进行微调来评估分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;MobileNetV2达到了90.45%的分类准确率，ResNet50达到了88.78%的分类准确率，证明卷积神经网络在手语识别任务中有效，特别是在资源有限的环境中。&lt;h4&gt;结论&lt;/h4&gt;这项工作代表了构建尼泊尔手语基准数据集和评估深度学习方法的第一系统性努力，突出了迁移学习和微调在推进代表性不足手语研究中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;手语是听力和言语障碍人士的重要交流系统。然而，对于代表性不足的手语，如尼泊尔手语，其数字语言数据集资源仍然稀缺。本研究引入了尼泊尔手语的第一个基准数据集，包含36个手势类别，每类1500个样本，旨在捕捉该语言的结构和视觉特征。为了评估识别性能，我们在数据集上微调了MobileNetV2和ResNet50架构，分别达到了90.45%和88.78%的分类准确率。这些发现证明了卷积神经网络在手语识别任务中的有效性，特别是在资源有限的环境中。据我们所知，这项工作代表了构建尼泊尔手语识别基准数据集和评估深度学习方法的第一系统性努力，突出了迁移学习和微调在推进代表性不足手语研究中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign languages serve as essential communication systems for individuals withhearing and speech impairments. However, digital linguistic dataset resourcesfor underrepresented sign languages, such as Nepali Sign Language (NSL), remainscarce. This study introduces the first benchmark dataset for NSL, consistingof 36 gesture classes with 1,500 samples per class, designed to capture thestructural and visual features of the language. To evaluate recognitionperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on thedataset, achieving classification accuracies of 90.45% and 88.78%,respectively. These findings demonstrate the effectiveness of convolutionalneural networks in sign recognition tasks, particularly within low-resourcesettings. To the best of our knowledge, this work represents the firstsystematic effort to construct a benchmark dataset and assess deep learningapproaches for NSL recognition, highlighting the potential of transfer learningand fine-tuning for advancing research in underexplored sign languages.</description>
      <author>example@mail.com (Birat Poudel, Satyam Ghimire, Sijan Bhattarai, Saurav Bhandari, Suramya Sharma Dahal)</author>
      <guid isPermaLink="false">2510.11243v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application</title>
      <link>http://arxiv.org/abs/2510.10870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于距离协方差的中心随机森林(CRF)方法，用于非参数回归的转移学习，特别是在源域和目标域回归函数在某些特征上稀疏不同的情况下。该方法通过两阶段CRF拟合过程，结合距离协方差特征权重，理论上证明了随机森林中转移学习的优势，并在模拟和实际医疗数据应用中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;随机森林是机器学习中的重要方法，在结构化表格数据方面优于其他竞争方法。然而，在源域和目标域分布不同的情况下，如何有效地应用随机森林进行转移学习仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于中心随机森林的转移学习方法，利用距离协方差的特征权重，以解决源域和目标域回归函数在某些特征上稀疏不同的问题，提高跨域预测性能。&lt;h4&gt;方法&lt;/h4&gt;首先使用源域训练的CRF预测目标域响应值并获取残差；然后使用另一个CRF拟合这些残差，特征分割概率与特征和残差之间的样本距离协方差成比例；推导均方误差率上界作为样本大小和差异维度的函数。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了随机森林中转移学习的好处；模拟结果表明CRF的结果在数值上也适用于标准随机森林；基于距离协方差的特征权重能提升RF性能；在预测ICU患者死亡率的应用中显示出显著优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的CRF方法结合距离协方差特征权重，能有效解决非参数回归中的转移学习问题，特别是在源域和目标域回归函数稀疏不同的情况下，为跨域预测提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随机森林是一种重要的机器学习方法，由于其在对结构化表格数据的广泛应用中优于其他竞争方法。我们提出了一种使用基于距离协方差的中心随机森林(CRF)进行非参数回归转移学习的方法，假设未知源域和目标域的回归函数在某些特征上有所不同(稀疏不同)。我们的方法首先使用源域训练的CRF预测目标域的响应值并获取残差。然后，我们使用另一个CRF拟合这些残差，但特征分割概率与特征和残差之间的样本距离协方差成比例。我们推导了该方法的均方误差率上界作为样本大小和差异维度的函数，理论上证明了随机森林中转移学习的好处。在模拟中，我们证明CRF的结果在数值上也适用于具有数据驱动特征分割选择的标准随机森林(SRF)方法。除了转移学习，我们的结果还显示了基于距离协方差的权重在某些情况下对RF性能的益处。我们的方法在使用包含20万ICU患者电子健康记录的大型多医院数据集预测小型床位目标医院ICU患者死亡率方面显示出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Random forest is an important method for ML applications due to its broadoutperformance over competing methods for structured tabular data. We propose amethod for transfer learning in nonparametric regression using a centeredrandom forest (CRF) with distance covariance-based feature weights, assumingthe unknown source and target regression functions are different for a fewfeatures (sparsely different). Our method first obtains residuals frompredicting the response in the target domain using a source domain-trained CRF.Then, we fit another CRF to the residuals, but with feature splittingprobabilities proportional to the sample distance covariance between thefeatures and the residuals in an independent sample. We derive an upper boundon the mean square error rate of the procedure as a function of sample sizesand difference dimension, theoretically demonstrating transfer learningbenefits in random forests. In simulations, we show that the results obtainedfor the CRFs also hold numerically for the standard random forest (SRF) methodwith data-driven feature split selection. Beyond transfer learning, our resultsalso show the benefit of distance-covariance-based weights on the performanceof RF in some situations. Our method shows significant gains in predicting themortality of ICU patients in smaller-bed target hospitals using a largemulti-hospital dataset of electronic health records for 200,000 ICU patients.</description>
      <author>example@mail.com (Chenze Li, Subhadeep Paul)</author>
      <guid isPermaLink="false">2510.10870v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Dataset Similarity to Guide Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.10866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为交叉学习分数(CLS)的新度量标准，用于衡量数据集相似性并提供迁移学习可迁移性的定量指导。该指标通过领域间的双向泛化性能评估数据集相似性，具有理论依据且计算高效。&lt;h4&gt;背景&lt;/h4&gt;迁移学习已成为现代机器学习的基石，可通过利用相关领域知识提高模型学习效果。然而，从对齐不良的数据进行迁移可能损害性能而非提高，因此在实施前确定迁移是否 beneficial 至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的度量标准来衡量数据集相似性，为迁移学习的可迁移性提供定量指导。&lt;h4&gt;方法&lt;/h4&gt;提出交叉学习分数(CLS)度量标准，通过领域间双向泛化性能衡量数据集相似性；建立CLS与目标/源数据集决策边界余弦相似性的理论联系；引入通用框架将源数据集分类为正/模糊/负迁移区；扩展至深度学习编码器-头部架构。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法主要关注特征分布而忽略标签信息和预测关系；CLS可可靠预测迁移是提高还是降低性能；CLS为迁移学习数据选择提供原则性工具。&lt;h4&gt;结论&lt;/h4&gt;通过在多种合成和真实世界任务上的广泛实验，证明CLS可有效预测迁移学习性能变化，指导数据选择决策。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习已成为现代机器学习的基石，因为它可以通过利用相关领域的知识来提高模型的学习效果。然而，从对齐不良的数据进行迁移可能会损害而非提高性能，因此在实施前确定迁移是否 beneficial 至关重要。本研究旨在通过提出一种创新的度量标准来衡量数据集相似性并提供可迁移性的定量指导来解决这一挑战。在现有文献中，现有方法主要关注特征分布而忽略了标签信息和预测关系，可能错失关键的可迁移性见解。相比之下，我们提出的度量标准交叉学习分数(CLS)通过领域之间的双向泛化性能来衡量数据集相似性。我们通过建立CLS与目标数据集和源数据集决策边界之间余弦相似性的联系，为CLS提供了理论依据。从计算角度看，CLS高效且计算快速，因为它绕过了高维问题中昂贵的分布估计问题。我们进一步引入了一个通用框架，根据CLS相对于基线误差将源数据集分类为正迁移区、模糊迁移区或负迁移区，从而能够做出明智的决策。此外，我们将这种方法扩展到深度学习中的编码器-头部架构，以更好地反映现代迁移流程。在多种合成和真实世界任务上的广泛实验表明，CLS可以可靠地预测迁移是提高还是降低性能，为迁移学习中的数据选择提供了原则性工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning has become a cornerstone of modern machine learning, as itcan empower models by leveraging knowledge from related domains to improvelearning effectiveness. However, transferring from poorly aligned data can harmrather than help performance, making it crucial to determine whether thetransfer will be beneficial before implementation. This work aims to addressthis challenge by proposing an innovative metric to measure dataset similarityand provide quantitative guidance on transferability. In the literature,existing methods largely focus on feature distributions while overlooking labelinformation and predictive relationships, potentially missing criticaltransferability insights. In contrast, our proposed metric, the Cross-LearningScore (CLS), measures dataset similarity through bidirectional generalizationperformance between domains. We provide a theoretical justification for CLS byestablishing its connection to the cosine similarity between the decisionboundaries for the target and source datasets. Computationally, CLS isefficient and fast to compute as it bypasses the problem of expensivedistribution estimation for high-dimensional problems. We further introduce ageneral framework that categorizes source datasets into positive, ambiguous, ornegative transfer zones based on their CLS relative to the baseline error,enabling informed decisions. Additionally, we extend this approach toencoder-head architectures in deep learning to better reflect modern transferpipelines. Extensive experiments on diverse synthetic and real-world tasksdemonstrate that CLS can reliably predict whether transfer will improve ordegrade performance, offering a principled tool for guiding data selection intransfer learning.</description>
      <author>example@mail.com (Shudong Sun, Hao Helen Zhang)</author>
      <guid isPermaLink="false">2510.10866v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2510.10671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Draft version, work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是对图像到视频迁移学习这一新兴领域的首次全面综述，探讨了如何将图像-语言基础模型(ILFM)的能力扩展到视频领域，以减轻从头训练视频-语言基础模型的数据和计算需求。&lt;h4&gt;背景&lt;/h4&gt;图像-语言基础模型在图像-文本理解和生成任务中表现出色，视频-文本研究的发展促使人们将基于图像的模型扩展到视频领域。图像到视频迁移学习范式可以显著降低训练成本。&lt;h4&gt;目的&lt;/h4&gt;提供对图像到视频迁移学习领域的全面回顾，建立基于现有ILFM推进视频-文本学习的结构化路线图，并启发未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;系统总结广泛使用的ILFM及其能力；将现有策略分为冻结特征和修改特征两类；详细阐述这些策略在从细粒度到粗粒度的各种视频-文本学习任务中的应用；通过实验分析不同迁移学习范式的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;图像到视频迁移学习能有效扩展ILFM能力到视频领域；不同迁移策略在各种视频任务上表现各异；从时空视频定位到视频问答等多种任务均可受益于这种迁移学习范式。&lt;h4&gt;结论&lt;/h4&gt;图像到视频迁移学习是视频-文本学习领域的有前途方向，现有ILFM可作为视频-文本学习的基础，该领域仍面临挑战需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;图像-语言基础模型(ILFM)在图像-文本理解/生成任务中展示了显著的成功，提供了可迁移的多模态表示，能够泛化到各种下游基于图像的任务。视频-文本研究的进步促使人们越来越有兴趣将基于图像的模型扩展到视频领域。这种被称为图像到视频迁移学习的范式，成功减轻了从头开始训练视频-语言基础模型以满足视频-文本学习的巨大数据和计算需求。本调查提供了这一新兴领域的首次全面回顾，首先总结了广泛使用的ILFM及其能力。然后我们将现有的图像到视频迁移学习策略系统地分为两类：冻结特征和修改特征，取决于是否保留ILFM的原始表示或进行修改。基于图像到视频迁移的任务特定性质，本调查系统地阐述这些策略，并详细说明了它们在从细粒度(如时空视频定位)到粗粒度(如视频问答)的多种视频-文本学习任务中的应用。我们进一步提供了详细的实验分析，研究不同的图像到视频迁移学习范式在多种下游视频理解任务上的有效性。最后，我们确定了当前面临的挑战并指出了未来研究的有希望方向。通过提供全面和结构化的概述，本调查旨在建立基于现有ILFM推进视频-文本学习的结构化路线图，并启发这一快速发展领域的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-Language Foundation Models (ILFM) have demonstrated remarkable successin image-text understanding/generation tasks, providing transferable multimodalrepresentations that generalize across diverse downstream image-based tasks.The advancement of video-text research has spurred growing interest inextending image-based models to the video domain. This paradigm, known asimage-to-video transfer learning, succeeds in alleviating the substantial dataand computational requirements associated with training video-languagefoundation models from scratch for video-text learning. This survey providesthe first comprehensive review of this emerging field, which begins bysummarizing the widely used ILFM and their capabilities. We then systematicallyclassify existing image-to-video transfer learning strategies into twocategories: frozen features and modified features, depending on whether theoriginal representations from ILFM are preserved or undergo modifications.Building upon the task-specific nature of image-to-video transfer, this surveymethodically elaborates these strategies and details their applications acrossa spectrum of video-text learning tasks, ranging from fine-grained (e.g.,spatio-temporal video grounding) to coarse-grained (e.g., video questionanswering). We further present a detailed experimental analysis to investigatethe efficacy of different image-to-video transfer learning paradigms on a rangeof downstream video understanding tasks. Finally, we identify prevailingchallenges and highlight promising directions for future research. By offeringa comprehensive and structured overview, this survey aims to establish astructured roadmap for advancing video-text learning based on existing ILFM,and to inspire future research directions in this rapidly evolving domain.</description>
      <author>example@mail.com (Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai)</author>
      <guid isPermaLink="false">2510.10671v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Towards Cybersickness Severity Classification from VR Gameplay Videos Using Transfer Learning and Temporal Modeling</title>
      <link>http://arxiv.org/abs/2510.10422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用迁移学习和长短期记忆网络预测虚拟现实网络疾病严重程度的方法，通过分析VR游戏视频实现68.4%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;虚拟现实技术快速发展并在医疗、教育和娱乐等领域广泛应用，但网络疾病(类似晕动症的症状)持续阻碍VR的广泛接受。&lt;h4&gt;目的&lt;/h4&gt;利用视频数据预测网络疾病严重程度，填补现有研究中基于视频特征预测网络疾病的空白。&lt;h4&gt;方法&lt;/h4&gt;使用在ImageNet数据集上预训练的InceptionV3模型从VR游戏视频中提取高级视觉特征，然后将这些特征传递给LSTM网络以捕捉VR体验的时间动态并预测网络疾病严重程度随时间的变化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有效利用了视频数据的时间序列特性，实现了68.4%的网络疾病严重度分类准确率，超越了仅使用视频数据训练的现有模型的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为VR开发者提供了评估和减轻虚拟环境中网络疾病的实用工具，并为未来基于视频的时间建模研究奠定基础，以增强VR应用中的用户舒适度。&lt;h4&gt;翻译&lt;/h4&gt;随着虚拟现实(VR)技术的快速发展，其在医疗、教育和娱乐等领域的应用显著增长。然而，持续存在的网络疾病问题(其症状类似于晕动症)继续阻碍VR的广泛接受。虽然近期研究探索了利用集成VR传感器(如眼睛和头部跟踪)数据的多模态深度学习方法，但使用基于视频特征预测网络疾病的研究仍然有限。在本研究中，我们通过使用在ImageNet数据集上预训练的InceptionV3模型，利用迁移学习从VR游戏视频中提取高级视觉特征，解决了这一研究空白。然后将这些特征传递给长短期记忆(LSTM)网络，以捕捉VR体验的时间动态并预测网络疾病严重程度随时间的变化。我们的方法有效利用了视频数据的时间序列特性，实现了68.4%的网络疾病严重度分类准确率。这超越了仅使用视频数据训练的现有模型的性能，为VR开发者提供了评估和减轻虚拟环境中网络疾病的实用工具。此外，这项工作为未来基于视频的时间建模研究奠定了基础，以增强VR应用中的用户舒适度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of virtual reality (VR) technology, its adoptionacross domains such as healthcare, education, and entertainment has grownsignificantly. However, the persistent issue of cybersickness, marked bysymptoms resembling motion sickness, continues to hinder widespread acceptanceof VR. While recent research has explored multimodal deep learning approachesleveraging data from integrated VR sensors like eye and head tracking, thereremains limited investigation into the use of video-based features forpredicting cybersickness. In this study, we address this gap by utilizingtransfer learning to extract high-level visual features from VR gameplay videosusing the InceptionV3 model pretrained on the ImageNet dataset. These featuresare then passed to a Long Short-Term Memory (LSTM) network to capture thetemporal dynamics of the VR experience and predict cybersickness severity overtime. Our approach effectively leverages the time-series nature of video data,achieving a 68.4% classification accuracy for cybersickness severity. Thissurpasses the performance of existing models trained solely on video data,providing a practical tool for VR developers to evaluate and mitigatecybersickness in virtual environments. Furthermore, this work lays thefoundation for future research on video-based temporal modeling for enhancinguser comfort in VR applications.</description>
      <author>example@mail.com (Jyotirmay Nag Setu, Kevin Desai, John Quarles)</author>
      <guid isPermaLink="false">2510.10422v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling Gamer Archetypes through Multi modal feature Correlations and Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.10263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Peer Review Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种综合数据驱动框架，结合心理测量、行为分析和机器学习来识别游戏玩家人格类型，通过结构化调查和多种分析方法确定了四种玩家原型。&lt;h4&gt;背景&lt;/h4&gt;游戏玩家画像研究对自适应游戏设计、行为理解和数字福祉至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个整合框架，揭示潜在游戏者人格类型，连接游戏动机与心理和健康结果。&lt;h4&gt;方法&lt;/h4&gt;对250名参与者（含113名活跃游戏玩家）进行结构化调查；整合特征工程、关联网络、知识图谱分析和无监督聚类；使用多种相关性统计和网络中心度分析；结合多种降维技术和聚类算法；使用多种评估指标优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;PCA与K-Means（k=4）模型达到最佳聚类质量，确定了四种玩家原型：沉浸式社交故事追求者、自律优化者、战略系统导航者和竞争团队建设者。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了将相关性驱动网络洞察与无监督学习联系的可复现流程，行为相关性网络与聚类的结合提高了分类准确性，并提供了理解游戏动机与心理健康结果的整体视角。&lt;h4&gt;翻译&lt;/h4&gt;游戏玩家画像为自适应游戏设计、行为理解和数字福祉提供了关键见解。本研究提出了一种综合的、数据驱动的框架，结合心理测量、行为分析和机器学习来揭示潜在的游戏者人格类型。对250名参与者（包括113名活跃游戏玩家）的结构化调查捕获了多维行为、动机和社会数据。分析流程整合了特征工程、关联网络、知识图谱分析和无监督聚类，以提取有意义的模式。相关性统计量化特征关联，网络中心度指导特征选择。将降维技术与聚类算法相结合，使用多种指数进行评估。PCA与K-Means模型实现了最佳聚类质量，确定了四种原型。这项研究贡献了一个可复现的流程，将相关性驱动的网络洞察与无监督学习联系起来。行为相关性网络与聚类的整合不仅提高了分类准确性，还提供了整体视角，将游戏动机与心理和健康结果联系起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Profiling gamers provides critical insights for adaptive game design,behavioral understanding, and digital well-being. This study proposes anintegrated, data-driven framework that combines psychological measures,behavioral analytics, and machine learning to reveal underlying gamer personas.A structured survey of 250 participants, including 113 active gamers, capturedmultidimensional behavioral, motivational, and social data. The analysispipeline integrated feature engineering, association-network, knowledge-graphanalysis, and unsupervised clustering to extract meaningful patterns.Correlation statistics uses Cramers V, Tschuprows T, Theils U, and Spearmansquantified feature associations, and network centrality guided featureselection. Dimensionality-reduction techniques such as PCA, SVD, t-SNE arecoupled with clustering algorithms like K-Means, Agglomerative, Spectral,DBSCAN, evaluated using Silhouette, Calinski Harabasz, and Davies Bouldinindices. The PCA with K-Means with k = 4 model achieved optimal cluster qualitywith Silhouette = 0.4, identifying four archetypes as Immersive SocialStory-Seekers, Disciplined Optimizers, Strategic Systems Navigators, andCompetitive Team-Builders. This research contributes a reproducible pipelinethat links correlation-driven network insights with unsupervised learning. Theintegration of behavioral correlation networks with clustering not onlyenhances classification accuracy but also offers a holistic lens to connectgameplay motivations with psychological and wellness outcomes.</description>
      <author>example@mail.com (Moona Kanwal, Muhammad Sami Siddiqui, Syed Anael Ali)</author>
      <guid isPermaLink="false">2510.10263v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning of the Biswas-Chatterjee-Sen Model</title>
      <link>http://arxiv.org/abs/2510.09446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures. arXiv admin note: text overlap with  arXiv:2509.14155&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习技术研究了动力学连续意见动力学模型的临界性质，通过神经网络、主成分分析和变分自编码器等方法成功识别了临界点并研究了相变行为。&lt;h4&gt;背景&lt;/h4&gt;动力学连续意见动力学模型是研究集体行为和相变的重要模型，其中系统由连续自旋变量组成，取值区间为[-1,1]，类似于自旋系统中的意见形成过程。&lt;h4&gt;目的&lt;/h4&gt;探究动力学连续意见动力学模型的临界性质，准确识别临界点，并研究相变行为，特别是使用深度学习技术来发现传统方法可能难以捕捉的规律。&lt;h4&gt;方法&lt;/h4&gt;使用深度神经网络在动力学蒙特卡洛模拟生成的自旋构型数据上进行训练；采用主成分分析进行无监督学习；实现变分自编码器并通过损失函数研究相变；定义真实数据与重构数据之间的相关函数。&lt;h4&gt;主要发现&lt;/h4&gt;深度神经网络能够准确识别方形和三角形晶格上的临界点；主成分分析可以重现磁化现象并估计临界指数；变分自编码器的损失函数可作为序参量；真实数据与重构数据之间的相关函数在临界点表现出普适性。&lt;h4&gt;结论&lt;/h4&gt;深度学习技术为研究动力学连续意见动力学模型的临界性质提供了有效工具，能够准确识别临界点并揭示相变过程中的普适性行为。&lt;h4&gt;翻译&lt;/h4&gt;我们使用深度学习技术研究动力学连续意见动力学模型的临界性质。该系统由区间[-1,1]内的N个连续自旋变量组成。在通过动力学蒙特卡洛模拟生成的自旋构型数据上训练密集神经网络，能够准确识别方形和三角形晶格上的临界点。使用主成分分析进行经典无监督学习重现了磁化现象，并允许估计临界指数。此外，实现变分自编码器通过损失函数研究相变，该损失函数表现为序参量。定义了真实数据和重构数据之间的相关函数，发现在临界点具有普适性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the critical properties of kinetic continuous opinion dynamicsusing deep learning techniques. The system consists of $N$ continuous spinvariables in the interval $[-1,1]$. Dense neural networks are trained on spinconfiguration data generated via kinetic Monte Carlo simulations, accuratelyidentifying the critical point on both square and triangular lattices.Classical unsupervised learning with principal component analysis reproducesthe magnetization and allows estimation of critical exponents. Additionally,variational autoencoders are implemented to study the phase transition throughthe loss function, which behaves as an order parameter. A correlation functionbetween real and reconstructed data is defined and found to be universal at thecritical point.</description>
      <author>example@mail.com (J. F. Silva Neto, D. S. M. Alencar, L. T. Brito, G. A. Alves, F. W. S. Lima, A. Macedo-Filho, R. S. Ferreira, T. F. A. Alves)</author>
      <guid isPermaLink="false">2510.09446v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>deep-REMAP: Probabilistic Parameterization of Stellar Spectra Using Regularized Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2510.09362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages. Accepted for publication in RASTI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了deep-REMAP，一种用于从观测光谱预测恒星大气参数的深度学习框架，结合正则化多任务学习和迁移学习，能够准确预测恒星的有效温度、表面重力和金属licity，具有可解释性和鲁棒性，可扩展到其他调查和合成库。&lt;h4&gt;背景&lt;/h4&gt;在天文学调查数据量爆炸式增长的时代，传统的光谱分析方法已达到其处理能力的极限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，用于从观测光谱中预测恒星大气参数。&lt;h4&gt;方法&lt;/h4&gt;在PHOENIX合成光谱库上训练深度卷积神经网络，使用迁移学习在MARVELS调查的一小部分观测FGK矮星光谱上微调模型，然后应用于732个未表征的FGK巨星候选体。结合非对称损失函数和嵌入损失，构建回归分类框架。&lt;h4&gt;主要发现&lt;/h4&gt;在30个MARVELS校准恒星上验证时，deep-REMAP准确恢复了有效温度、表面重力和金属licity，例如在有效温度上实现了约75 K的精度。该框架具有可解释性，对参数不平衡具有鲁棒性，能够捕捉非高斯不确定性。&lt;h4&gt;结论&lt;/h4&gt;虽然最初为MARVELS调查开发，但deep-REMAP框架可扩展到其他调查和合成库，为恒星特征表征提供了一种强大且自动化的方法。&lt;h4&gt;翻译&lt;/h4&gt;在调查量爆炸式增长的时代，传统的光谱分析方法已达到其极限。为此，我们开发了deep-REMAP，一种新颖的深度学习框架，利用正则化多任务方法从观测光谱预测恒星大气参数。我们在PHOENIX合成光谱库上训练深度卷积神经网络，并使用迁移学习在MARVELS调查的一小部分观测FGK矮星光谱上微调模型。然后我们将该模型应用于同一调查中的732个未表征的FGK巨星候选体。在30个MARVELS校准恒星上进行验证时，deep-REMAP准确恢复了有效温度、表面重力和金属licity，例如在有效温度上实现了约75 K的精度。通过结合非对称损失函数和嵌入损失，我们的回归分类框架具有可解释性，对参数不平衡具有鲁棒性，并且能够捕捉非高斯不确定性。虽然是为MARVELS开发的，但deep-REMAP框架可扩展到其他调查和合成库，展示了一种强大且自动化的恒星特征表征途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of exploding survey volumes, traditional methods of spectroscopicanalysis are being pushed to their limits. In response, we develop deep-REMAP,a novel deep learning framework that utilizes a regularized, multi-taskapproach to predict stellar atmospheric parameters from observed spectra. Wetrain a deep convolutional neural network on the PHOENIX synthetic spectrallibrary and use transfer learning to fine-tune the model on a small subset ofobserved FGK dwarf spectra from the MARVELS survey. We then apply the model to732 uncharacterized FGK giant candidates from the same survey. When validatedon 30 MARVELS calibration stars, deep-REMAP accurately recovers the effectivetemperature ($T_{\rm{eff}}$), surface gravity ($\log \rm{g}$), and metallicity([Fe/H]), achieving a precision of, for instance, approximately 75 K in$T_{\rm{eff}}$. By combining an asymmetric loss function with an embeddingloss, our regression-as-classification framework is interpretable, robust toparameter imbalances, and capable of capturing non-Gaussian uncertainties.While developed for MARVELS, the deep-REMAP framework is extensible to othersurveys and synthetic libraries, demonstrating a powerful and automated pathwayfor stellar characterization.</description>
      <author>example@mail.com (Sankalp Gilda)</author>
      <guid isPermaLink="false">2510.09362v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MPA-DNN: Projection-Aware Unsupervised Learning for Multi-period DC-OPF</title>
      <link>http://arxiv.org/abs/2510.09349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种MPA-DNN方法，解决了深度神经网络在最优潮流问题中无法满足关键操作约束的问题，特别是在涉及时间间耦合的情况下。该方法通过引入投影层强制物理可行性，实现了无需标记数据的端到端学习，实验表明其在变化负载条件下能实现接近最优性能并严格满足所有约束。&lt;h4&gt;背景&lt;/h4&gt;现代电力系统在高比例可再生能源和储能的情况下，最优潮流(OPF)操作的可行性和效率变得越来越重要。深度神经网络作为OPF求解器的快速代理很有前景，但常常无法满足关键的操作约束，特别是涉及时间间耦合的约束。&lt;h4&gt;目的&lt;/h4&gt;解决深度神经网络在最优潮流问题中无法满足关键操作约束的问题，特别是涉及时间间耦合的约束，如发电机爬坡限制和储能操作。&lt;h4&gt;方法&lt;/h4&gt;提出一个多周期投影感知深度神经网络(MPA-DNN)，它在网络中集成了一个用于多周期调度的投影层，通过投影强制物理可行性，实现端到端的符合约束的调度轨迹学习，无需依赖标记数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在变化的负载条件下实现了接近最优的性能，同时严格满足所有约束。&lt;h4&gt;结论&lt;/h4&gt;MPA-DNN方法能够确保最优潮流操作的可行性和效率，特别是在高比例可再生能源和储能的现代电力系统中。&lt;h4&gt;翻译&lt;/h4&gt;确保现代电力系统中高比例可再生能源和储能情况下的最优潮流(OPF)操作的可行性和效率变得越来越重要。虽然深度神经网络(DNN)已成为OPF求解器有前景的快速代理，但它们常常无法满足关键操作约束，特别是涉及时间间耦合的约束，如发电机爬坡限制和储能操作。为了解决这些问题，我们提出了一种多周期投影感知深度神经网络(MPA-DNN)，它在网络中集成了一个用于多周期调度的投影层。通过这样做，我们的模型通过投影强制物理可行性，使得无需依赖标记数据即可进行端到端的符合约束的调度轨迹学习。实验结果表明，所提出的方法在变化的负载条件下实现了接近最优的性能，同时严格满足所有约束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring both feasibility and efficiency in optimal power flow (OPF)operations has become increasingly important in modern power systems with highpenetrations of renewable energy and energy storage. While deep neural networks(DNNs) have emerged as promising fast surrogates for OPF solvers, they oftenfail to satisfy critical operational constraints, especially those involvinginter-temporal coupling, such as generator ramping limits and energy storageoperations. To deal with these issues, we propose a Multi-PeriodProjection-Aware Deep Neural Network (MPA-DNN) that incorporates a projectionlayer for multi-period dispatch into the network. By doing so, our modelenforces physical feasibility through the projection, enabling end-to-endlearning of constraint-compliant dispatch trajectories without relying onlabeled data. Experimental results demonstrate that the proposed methodachieves near-optimal performance while strictly satisfying all constraints invarying load conditions.</description>
      <author>example@mail.com (Yeomoon Kim, Minsoo Kim, Jip Kim)</author>
      <guid isPermaLink="false">2510.09349v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation</title>
      <link>http://arxiv.org/abs/2510.09306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LODi是一种利用成人脑部MRI分割模型先验知识增强婴儿脑部MRI分割性能的新框架，通过迁移学习和领域适应策略，实现了快速、准确、年龄自适应的分割，减轻了扫描仪和特定地点的偏差。&lt;h4&gt;背景&lt;/h4&gt;婴儿脑部MRI分割对于研究早期神经发育和诊断神经系统疾病至关重要，但面临受试者解剖结构不断变化、运动伪影以及高质量标记数据稀缺等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用成人脑部MRI分割模型先验知识来提高婴儿脑部MRI分割性能的方法，解决婴儿脑部MRI分割中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;LODi框架首先在大量成人脑部MRI数据上预训练分割模型，然后通过迁移学习和领域适应策略逐步适应0-2岁人群，利用弱监督学习和FreeSurfer获得的银标准真实标签进行调整，并引入分层特征细化和多级别一致性约束的新训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;在内部和外部数据集上的广泛实验表明，LODi方法优于传统监督学习和领域特定模型，能够实现快速、准确、年龄自适应的分割，同时减轻扫描仪和特定地点的偏差。&lt;h4&gt;结论&lt;/h4&gt;利用成人脑部先验作为年龄灵活神经成像分析的基础具有显著优势，为整个生命周期内更可靠和通用的脑部MRI分割铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;婴儿脑部MRI的精确分割对于研究早期神经发育和诊断神经系统疾病至关重要。然而，由于受试者解剖结构的持续变化、运动伪影以及高质量标记数据的稀缺，它仍然是一个基本挑战。在这项工作中，我们提出了LODi，一个新颖的框架，利用成人脑部MRI分割模型的先验知识来增强婴儿扫描的分割性能。鉴于大量公开可用的成人脑部MRI数据，我们在大型成人数据集上预训练分割模型作为起点。通过迁移学习和领域适应策略，我们将模型逐步适应0-2岁人群，使其能够考虑婴儿扫描中典型的解剖和成像变异性。成人模型的调整是通过在婴儿脑部扫描上进行弱监督学习进行的，利用使用FreeSurfer获得的银标准真实标签。通过引入一种结合分层特征细化和多级别一致性约束的新训练策略，我们的方法能够实现快速、准确、年龄自适应的分割，同时减轻扫描仪和特定地点的偏差。在内部和外部数据集上的广泛实验证明了我们的方法优于传统监督学习和领域特定模型。我们的研究结果突显了利用成人脑部先验作为年龄灵活神经成像分析基础的优势，为整个生命周期内更可靠和通用的脑部MRI分割铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of infant brain MRI is critical for studying earlyneurodevelopment and diagnosing neurological disorders. Yet, it remains afundamental challenge due to continuously evolving anatomy of the subjects,motion artifacts, and the scarcity of high-quality labeled data. In this work,we present LODi, a novel framework that utilizes prior knowledge from an adultbrain MRI segmentation model to enhance the segmentation performance of infantscans. Given the abundance of publicly available adult brain MRI data, wepre-train a segmentation model on a large adult dataset as a starting point.Through transfer learning and domain adaptation strategies, we progressivelyadapt the model to the 0-2 year-old population, enabling it to account for theanatomical and imaging variability typical of infant scans. The adaptation ofthe adult model is carried out using weakly supervised learning on infant brainscans, leveraging silver-standard ground truth labels obtained with FreeSurfer.By introducing a novel training strategy that integrates hierarchical featurerefinement and multi-level consistency constraints, our method enables fast,accurate, age-adaptive segmentation, while mitigating scanner and site-specificbiases. Extensive experiments on both internal and external datasetsdemonstrate the superiority of our approach over traditional supervisedlearning and domain-specific models. Our findings highlight the advantage ofleveraging adult brain priors as a foundation for age-flexible neuroimaginganalysis, paving the way for more reliable and generalizable brain MRIsegmentation across the lifespan.</description>
      <author>example@mail.com (Alemu Sisay Nigru, Michele Svanera, Austin Dibble, Connor Dalby, Mattia Savardi, Sergio Benini)</author>
      <guid isPermaLink="false">2510.09306v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study</title>
      <link>http://arxiv.org/abs/2510.09187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对板球击球分类进行了全面的基线研究，比较了七种不同的深度学习方法，发现在学术文献中报告的性能与实际实现结果之间存在显著差距，而现代架构结合系统优化可实现92.25%的准确率。&lt;h4&gt;背景&lt;/h4&gt;板球击球分类在体育视频分析中仍然是一个具有挑战性的问题，需要有效地建模空间和时间特征。&lt;h4&gt;目的&lt;/h4&gt;进行一项全面的基线研究，比较七种不同的深度学习方法，用于板球击球分类。&lt;h4&gt;方法&lt;/h4&gt;在统一的基准上实现了和系统评估了传统的CNN-LSTM架构、基于注意力的模型、视觉变换器、迁移学习方法以及现代的EfficientNet-GRU组合。&lt;h4&gt;主要发现&lt;/h4&gt;学术文献中报告的准确率（96%、99.2%和93%）与实际重新实现的准确率（46.0%、55.6%和57.7%）之间存在显著差距；现代最先进的方法（EfficientNet-B0与基于GRU的时间模型组合）达到了92.25%的准确率。&lt;h4&gt;结论&lt;/h4&gt;现代架构和系统优化可以带来实质性的改进；遵循现代MLOps实践并提供可重现的研究平台突显了标准化评估协议在体育视频分析研究中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;从视频序列中进行板球击球分类在体育视频分析中仍然是一个具有挑战性的问题，需要有效地建模空间和时间特征。本文首次进行了全面的基线研究，比较了四种不同研究范式下的七种深度学习方法。我们在统一的基准上实现了并系统评估了传统的CNN-LSTM架构、基于注意力的模型、视觉变换器、迁移学习方法以及现代的EfficientNet-GRU组合。我们研究的一个关键发现是学术文献中的声明与实际实现结果之间存在显著性能差距。虽然之前的论文报告了96%（Balaji LRCN）、99.2%（IJERCSE）和93%（Sensors）的准确率，但我们的标准化重新实现分别实现了46.0%、55.6%和57.7%的准确率。我们的现代最先进方法，结合EfficientNet-B0和基于GRU的时间模型，实现了92.25%的准确率，证明使用现代架构和系统优化可以实现实质性改进。所有实现都遵循使用PyTorch Lightning的现代MLOps实践，提供了一个可重现的研究平台，突显了标准化评估协议在体育视频分析研究中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cricket shot classification from video sequences remains a challengingproblem in sports video analysis, requiring effective modeling of both spatialand temporal features. This paper presents the first comprehensive baselinestudy comparing seven different deep learning approaches across four distinctresearch paradigms for cricket shot classification. We implement andsystematically evaluate traditional CNN-LSTM architectures, attention-basedmodels, vision transformers, transfer learning approaches, and modernEfficientNet-GRU combinations on a unified benchmark. A critical finding of ourstudy is the significant performance gap between claims in academic literatureand practical implementation results. While previous papers reported accuraciesof 96\% (Balaji LRCN), 99.2\% (IJERCSE), and 93\% (Sensors), our standardizedre-implementations achieve 46.0\%, 55.6\%, and 57.7\% respectively. Our modernSOTA approach, combining EfficientNet-B0 with a GRU-based temporal model,achieves 92.25\% accuracy, demonstrating that substantial improvements arepossible with modern architectures and systematic optimization. Allimplementations follow modern MLOps practices with PyTorch Lightning, providinga reproducible research platform that exposes the critical importance ofstandardized evaluation protocols in sports video analysis research.</description>
      <author>example@mail.com (Sungwoo Kang)</author>
      <guid isPermaLink="false">2510.09187v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans</title>
      <link>http://arxiv.org/abs/2510.09107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种专门用于医学图像分析的新型多分支ConvNeXt架构，通过整合三种并行分支特征提取方法，在COVID-19诊断任务上取得了优异性能，超越了之前报道的所有模型。&lt;h4&gt;背景&lt;/h4&gt;智能医学影像分析在辅助临床诊断中起着至关重要的作用，特别是在识别细微病理特征方面，但医学图像分析面临着独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对医学图像分析独特挑战的新型多分支ConvNeXt架构，应用于COVID-19诊断，并提供一个可推广的框架用于从CT扫描中分类广泛的病理。&lt;h4&gt;方法&lt;/h4&gt;提出包含严格端到端流程的模型，包括细致的数据预处理和增强，采用两阶段训练策略有效利用迁移学习，架构整合了全局平均池化、全局最大池化和新的注意力加权池化三个并行分支的特征，在2609个CT切片的组合数据集上训练和验证。&lt;h4&gt;主要发现&lt;/h4&gt;模型在验证集上表现出色，COVID-19病例的ROC-AUC达到0.9937，验证准确率为0.9757，F1得分为0.9825，超越了此数据集上之前报道的所有模型。&lt;h4&gt;结论&lt;/h4&gt;现代多分支架构与谨慎的数据处理相结合，可以实现与当代最先进模型相当或更好的性能，证明先进的深度学习技术为稳健医疗诊断提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学影像的智能分析在辅助临床诊断中起着至关重要的作用，特别是对于识别细微的病理特征。本文介绍了一种新型的多分支ConvNeXt架构，专门为医学图像分析的细微挑战而设计。虽然在此应用于COVID-19诊断的具体问题，但该方法为从CT扫描中广泛分类病理提供了一种可推广的框架。所提出的模型包含严格的端到端流程，从细致的数据预处理和增强到利用迁移学习有效利用的纪律性两阶段训练策略。该架构独特地整合了从三个并行分支提取的特征：全局平均池化、全局最大池化以及新的注意力加权池化机制。该模型在来自两个不同数据集的2609个CT切片的组合数据集上进行了训练和验证。实验结果表明，在验证集上表现出色，COVID-19病例的最终ROC-AUC达到0.9937，验证准确率为0.9757，F1得分为0.9825，超越了这个数据集上之前报道的所有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent analysis of medical imaging plays a crucial role in assistingclinical diagnosis, especially for identifying subtle pathological features.This paper introduces a novel multi-branch ConvNeXt architecture designedspecifically for the nuanced challenges of medical image analysis. Whileapplied here to the specific problem of COVID-19 diagnosis, the methodologyoffers a generalizable framework for classifying a wide range of pathologiesfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,from meticulous data preprocessing and augmentation to a disciplined two-phasetraining strategy that leverages transfer learning effectively. Thearchitecture uniquely integrates features extracted from three parallelbranches: Global Average Pooling, Global Max Pooling, and a newAttention-weighted Pooling mechanism. The model was trained and validated on acombined dataset of 2,609 CT slices derived from two distinct datasets.Experimental results demonstrate a superior performance on the validation set,achieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and anF1-score of 0.9825 for COVID-19 cases, outperforming all previously reportedmodels on this dataset. These findings indicate that a modern, multi-brancharchitecture, coupled with careful data handling, can achieve performancecomparable to or exceeding contemporary state-of-the-art models, therebyproving the efficacy of advanced deep learning techniques for robust medicaldiagnostics.</description>
      <author>example@mail.com (Irash Perera, Uthayasanker Thayasivam)</author>
      <guid isPermaLink="false">2510.09107v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission</title>
      <link>http://arxiv.org/abs/2510.09047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Asia Communications and Photonics Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于迁移学习的Transformer框架，用于C+L波段系统中的精确建模和拉曼泵浦设计。&lt;h4&gt;背景&lt;/h4&gt;C+L波段系统中的精确建模和拉曼泵浦设计面临挑战，需要新的方法来提高性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时实现精确建模和拉曼泵浦设计的框架，应用于C+L波段系统。&lt;h4&gt;方法&lt;/h4&gt;使用基于迁移学习的Transformer框架。&lt;h4&gt;主要发现&lt;/h4&gt;建模的均方根误差在0.22分贝以内，峰峰值光信噪比变化/偏差在0.86/0.1分贝以内。&lt;h4&gt;结论&lt;/h4&gt;该框架能够有效实现C+L波段系统中的精确建模和拉曼泵浦设计，性能指标达到预期。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于迁移学习的Transformer框架，用于在C+L波段系统中同时实现精确建模和拉曼泵浦设计。建模的均方根误差和峰峰值光信噪比变化/偏差分别在0.22分贝和0.86/0.1分贝以内。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a transfer learning-enabled Transformer framework tosimultaneously realize accurate modeling and Raman pump design in C+L-bandsystems. The RMSE for modeling and peak-to-peak GSNR variation/deviation iswithin 0.22 dB and 0.86/0.1 dB, respectively.</description>
      <author>example@mail.com (Jiaming Liu, Rui Wang, JinJiang Li, Hong Lin, Jing Zhang, Kun Qiu)</author>
      <guid isPermaLink="false">2510.09047v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language</title>
      <link>http://arxiv.org/abs/2510.09032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对数据有限的Chakma语言，引入了一个新的孟加拉语转写的Chakma语料库，并对多种transformer模型进行了微调。实验表明，微调后的多语言模型在适应Chakma时表现优异，且数据质量对模型性能有重要影响。&lt;h4&gt;背景&lt;/h4&gt;Chakma作为一种达罗毗荼语系语言，可用数据有限，在语言模型中代表性不足。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的上下文连贯的孟加拉语转写的Chakma语料库，并利用该数据集对多种模型进行微调，以提高Chakma语言在语言模型中的表现。&lt;h4&gt;方法&lt;/h4&gt;从Chakma文学中整理了一个经过母语人士验证的语料库，并使用该数据集在掩码语言建模任务上微调了六种基于编码器的多语言和区域transformer模型（mBERT、XLM-RoBERTa、DistilBERT、DeBERTaV3、BanglaBERT和IndicBERT）。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的多语言模型在适应孟加拉语转写的Chakma时优于其预训练版本，达到高达73.54%的标记准确度和低至2.90的困惑度。分析还强调了数据质量对模型性能的影响，以及OCR管道在形态丰富的印度文字方面的局限性。&lt;h4&gt;结论&lt;/h4&gt;孟加拉语转写的Chakma对于Chakma语言的迁移学习非常有效，研究人员发布了手动验证的单语数据集以鼓励对低资源语言的多语言语言建模进行进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;作为一种达罗毗荼语系语言且可用数据有限，Chakma在语言模型中仍然代表性不足。在这项工作中，我们引入了一个新颖的上下文连贯的孟加拉语转写的Chakma语料库，该语料库从Chakma文学中精心整理，并由母语人士验证。使用这个数据集，我们在掩码语言建模任务上微调了六种基于编码器的多语言和区域transformer模型（mBERT、XLM-RoBERTa、DistilBERT、DeBERTaV3、BanglaBERT和IndicBERT）。我们的实验表明，当适应到孟加拉语转写的Chakma时，微调后的多语言模型优于其预训练版本，达到高达73.54%的标记准确度和低至2.90的困惑度。我们的分析进一步强调了数据质量对模型性能的影响，并展示了OCR管道在形态丰富的印度文字方面的局限性。我们的研究证明了孟加拉语转写的Chakma对于Chakma语言的迁移学习非常有效，我们发布了手动验证的单语数据集以鼓励对低资源语言的多语言语言建模进行进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an Indo-Aryan language with limited available data, Chakma remains largelyunderrepresented in language models. In this work, we introduce a novel corpusof contextually coherent Bangla-transliterated Chakma, curated from Chakmaliterature, and validated by native speakers. Using this dataset, we fine-tunesix encoder-based multilingual and regional transformer models (mBERT,XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on maskedlanguage modeling (MLM) tasks. Our experiments show that fine-tunedmultilingual models outperform their pre-trained counterparts when adapted toBangla-transliterated Chakma, achieving up to 73.54% token accuracy and aperplexity as low as 2.90. Our analysis further highlights the impact of dataquality on model performance and shows the limitations of OCR pipelines formorphologically rich Indic scripts. Our research demonstrates thatBangla-transliterated Chakma can be very effective for transfer learning forChakma language, and we release our manually validated monolingual dataset toencourage further research on multilingual language modeling for low-resourcelanguages.</description>
      <author>example@mail.com (Adity Khisa, Nusrat Jahan Lia, Tasnim Mahfuz Nafis, Zarif Masud, Tanzir Pial, Shebuti Rayana, Ahmedul Kabir)</author>
      <guid isPermaLink="false">2510.09032v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Denoised Diffusion for Object-Focused Image Augmentation</title>
      <link>http://arxiv.org/abs/2510.08955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种面向对象的数据增强框架，专门用于解决数据受限条件下动物健康监测的问题，通过从背景中分割动物并进行变换和基于扩散的合成，创建真实多样的场景，提高动物检测和监测性能。&lt;h4&gt;背景&lt;/h4&gt;现代农业操作依赖集成监控系统进行农场优化，基于无人机的动物健康监测是关键组成部分，但面临数据有限的问题，包括动物小、被遮挡或部分可见等场景特定问题，且迁移学习方法因缺乏反映特定农场条件的大型数据集而效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发针对特定问题、以动物为中心的数据增强策略，应对数据有限条件下动物健康监测的挑战，弥合有限数据与实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一种面向对象的数据增强框架，专门为数据受限环境下的动物健康监测设计，通过从背景中分割动物，并利用变换和基于扩散的合成技术来增强动物，创建真实多样的场景。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，与基线模型相比，增强数据集在动物检测任务上表现出更好的性能；通过生成领域特定数据，即使在数据稀缺的情况下，该方法也能支持实时动物健康监测解决方案。&lt;h4&gt;结论&lt;/h4&gt;该数据增强方法成功解决了数据稀缺条件下动物健康监测的挑战，弥合了有限数据与实际应用之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;现代农业操作越来越多地依赖集成监控系统，结合多种数据源进行农场优化。基于空中无人机的动物健康监测是关键组成部分，但面临数据有限的问题，加之场景特定问题如动物小、被遮挡或部分可见。迁移学习方法通常无法解决这一限制，因为缺乏反映特定农场条件（包括动物品种、环境和行为变化）的大型数据集。因此，需要开发针对特定问题、以动物为中心的数据增强策略，应对这些独特挑战。为解决这一差距，我们提出了一种面向对象的数据增强框架，专门为数据受限条件下的动物健康监测设计。我们的方法从背景中分割动物，并通过变换和基于扩散的合成来增强它们，创建真实、多样的场景，提高动物检测和监测性能。初步实验表明，与基线模型相比，我们的增强数据集在动物检测任务上表现出更好的性能。通过生成领域特定数据，我们的方法即使在数据稀缺的情况下也能支持实时动物健康监测解决方案，弥合了有限数据与实际应用之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern agricultural operations increasingly rely on integrated monitoringsystems that combine multiple data sources for farm optimization. Aerialdrone-based animal health monitoring serves as a key component but faceslimited data availability, compounded by scene-specific issues such as small,occluded, or partially visible animals. Transfer learning approaches often failto address this limitation due to the unavailability of large datasets thatreflect specific farm conditions, including variations in animal breeds,environments, and behaviors. Therefore, there is a need for developing aproblem-specific, animal-focused data augmentation strategy tailored to theseunique challenges. To address this gap, we propose an object-focused dataaugmentation framework designed explicitly for animal health monitoring inconstrained data settings. Our approach segments animals from backgrounds andaugments them through transformations and diffusion-based synthesis to createrealistic, diverse scenes that enhance animal detection and monitoringperformance. Our initial experiments demonstrate that our augmented datasetyields superior performance compared to our baseline models on the animaldetection task. By generating domain-specific data, our method empowersreal-time animal health monitoring solutions even in data-scarce scenarios,bridging the gap between limited data and practical applicability.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2510.08955v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Structured Output Regularization: a framework for few-shot transfer learning</title>
      <link>http://arxiv.org/abs/2510.08728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了结构化输出正则化（SOR）框架，通过冻结网络结构并使用正则化方法，解决了传统迁移学习在适应领域特定特征和防止过拟合方面的局限性，在少样本医学影像分类任务中取得了与基准相当的结果。&lt;h4&gt;背景&lt;/h4&gt;传统迁移学习方法通过冻结预训练网络的部分权重并添加任务特定层来重用大型预训练网络，这种方法计算效率高，但限制了模型适应领域特定特征的能力，并且在数据有限时仍可能导致过拟合。&lt;h4&gt;目的&lt;/h4&gt;解决传统迁移学习方法在适应领域特定特征和防止过拟合方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出结构化输出正则化（SOR）框架，冻结内部网络结构（如卷积滤波器），同时使用组套索和L1惩罚的组合，使模型能够以最少的额外参数适应特定数据，并可以轻松应用于各种网络组件。&lt;h4&gt;主要发现&lt;/h4&gt;在三个少样本医学影像分类任务上评估了SOR，使用DenseNet121和EfficientNetB4作为基础模型，与已建立的基准相比取得了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;SOR框架是一种简单有效的方法，可以解决传统迁移学习在适应领域特定特征和防止过拟合方面的局限性，并且具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;传统的迁移学习通常通过冻结大型预训练网络的一些权重并添加任务特定层来重用这些网络。虽然这种方法计算效率高，但它限制了模型适应领域特定特征的能力，并且在数据非常有限的情况下仍可能导致过拟合。为了解决这些局限性，我们提出了结构化输出正则化（SOR），这是一个简单而有效的框架，它冻结内部网络结构（例如卷积滤波器），同时使用组套索和L1惩罚的组合。该框架使模型能够以最少的额外参数适应特定数据，并且可以轻松应用于各种网络组件，例如神经网络中的卷积滤波器或各种块，从而为迁移学习任务提供了广泛的适用性。我们在三个少样本医学影像分类任务上评估了SOR，使用DenseNet121和EfficientNetB4作为基础模型，与已建立的基准相比取得了具有竞争力的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional transfer learning typically reuses large pre-trained networks byfreezing some of their weights and adding task-specific layers. While thisapproach is computationally efficient, it limits the model's ability to adaptto domain-specific features and can still lead to overfitting with very limiteddata. To address these limitations, we propose Structured Output Regularization(SOR), a simple yet effective framework that freezes the internal networkstructures (e.g., convolutional filters) while using a combination of grouplasso and $L_1$ penalties. This framework tailors the model to specific datawith minimal additional parameters and is easily applicable to various networkcomponents, such as convolutional filters or various blocks in neural networksenabling broad applicability for transfer learning tasks. We evaluate SOR onthree few shot medical imaging classification tasks and we achieve competitiveresults using DenseNet121, and EfficientNetB4 bases compared to establishedbenchmarks.</description>
      <author>example@mail.com (Nicolas Ewen, Jairo Diaz-Rodriguez, Kelly Ramsay)</author>
      <guid isPermaLink="false">2510.08728v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</title>
      <link>http://arxiv.org/abs/2510.07545v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the EMNLP 2025 Industry Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型视觉-语言模型(LVLMs)作为图表理解任务中自动化评判者的能力，提出多标准提示和领域自适应迁移学习两种方法，成功使小型模型(2B参数)成为有效的图表评判者(ChartJudge)，实现了资源受限环境下的低成本评估。&lt;h4&gt;背景&lt;/h4&gt;具有70亿参数的大型视觉-语言模型(LVLMs)已显示出作为图表理解任务中自动化评判者的潜力。然而，小型模型(参数量≤2B)作为评判者时表现仍然不佳，限制了它们在资源受限环境中的实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决小型模型在图表理解任务中作为评判者表现不佳的问题，确保评估过程具有成本效益，使小型模型能够在资源受限环境中有效应用。&lt;h4&gt;方法&lt;/h4&gt;作者提出两种方法：(i)多标准提示：将单独的评估标准组合到一个查询中；(ii)领域自适应迁移学习：在图表数据集上的合成判断上微调一个具有20亿参数的LVLM，创建出ChartJudge模型。&lt;h4&gt;主要发现&lt;/h4&gt;多标准提示暴露了模型的鲁棒性差距，导致70亿参数模型(包括专业的LVLM评判者如LLaVA-Critic)性能大幅下降；小型LVLM(ChartJudge)可以有效将知识从一个数据集迁移到另一个数据集，使其成为更专业的模型；对不同图表类型和查询复杂性的细粒度分析提供了关于模型大小、提示设计和可迁移性之间权衡的可操作见解。&lt;h4&gt;结论&lt;/h4&gt;通过结合多标准提示和领域自适应迁移学习，ChartJudge模型能够在资源受限环境中实现可扩展的低成本评估，为图表推理任务提供有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;仅具有70亿参数的大型视觉-语言模型(LVLMs)已显示出作为图表理解任务中自动化评判者的潜力。然而，小型模型(参数量≤2B)作为评判者时仍然表现不佳，限制了它们在资源受限环境中的实际应用。为此，我们提出两种方法来确保成本高效的评估：(i)多标准提示，将单独的评估标准组合到一个查询中；(ii)领域自适应迁移学习，我们在图表数据集上的合成判断上微调一个20亿参数的LVLM，创建出ChartJudge。实验表明，多标准提示暴露了模型的鲁棒性差距，导致70亿参数模型(包括专业的LVLM评判者如LLaVA-Critic)性能大幅下降。此外，我们发现我们的小型LVLM(ChartJudge)可以有效地将知识从一个数据集迁移到另一个数据集，使其成为更专业的模型。我们对不同图表类型和查询复杂性的细粒度分析提供了关于模型大小、提示设计和可迁移性之间权衡的可操作见解，使图表推理任务能够实现可扩展的低成本评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) with only 7B parameters have shownpromise as automated judges in chart comprehension tasks. However, tiny models(&lt;=2B parameters) still perform poorly as judges, limiting their real-world usein resource-constrained settings. To address this, we propose two approaches toensure cost-efficient evaluation: (i) multi-criteria prompting, which combinesseparate evaluation criteria into a single query, and (ii) domain-adaptivetransfer learning, in which we fine-tune a 2B-parameter LVLM on syntheticjudgments in a chart dataset to create the ChartJudge. Experiments show thatmulti-criteria prompting exposes robustness gaps, which led to a huge drop inperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.In addition, we find that our tiny LVLM (ChartJudge) can effectively transferknowledge from one dataset to another to make it a more specialized model. Ourfine-grained analysis across chart types and query complexities offersactionable insights into trade-offs between model size, prompt design, andtransferability, enabling scalable, low-cost evaluation for chart reasoningtasks.</description>
      <author>example@mail.com (Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang)</author>
      <guid isPermaLink="false">2510.07545v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking</title>
      <link>http://arxiv.org/abs/2510.10287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了DualViewDistill混合检测和跟踪框架，结合透视视图和鸟瞰图特征，提高自动驾驶中的3D物体检测和跟踪性能。&lt;h4&gt;背景&lt;/h4&gt;基于相机的3D物体检测和跟踪对自动驾驶感知至关重要，但当前最先进方法通常仅依赖透视视图或鸟瞰图特征，限制了利用精细物体细节和空间结构化场景表示的能力。&lt;h4&gt;目的&lt;/h4&gt;开发能够同时利用透视视图和鸟瞰图特征优势的混合检测和跟踪框架，提升3D物体检测和跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;提出DualViewDistill框架，引入由基础模型指导的BEV地图，利用DINOv2特征通过新颖蒸馏过程生成BEV表示，并将PV特征与增强的BEV地图集成，通过可变形聚合增强3D物体检测和跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse 2基准测试上，DualViewDistill达到最先进性能，展示了基础模型BEV地图在实现更可靠自动驾驶感知方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;结合透视视图和鸟瞰图特征的优势，并利用基础模型提供的丰富语义和几何信息，显著提高了3D物体检测和跟踪性能，为自动驾驶感知提供了更可靠解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于相机的3D物体检测和跟踪对于自动驾驶感知至关重要。当前最先进的方法通常仅依赖于透视视图或鸟瞰图特征，限制了它们利用精细物体细节和空间结构化场景表示的能力。在这项工作中，我们提出了DualViewDistill，一个结合了PV和BEV相机图像特征的混合检测和跟踪框架，以利用它们的互补优势。我们的方法引入了由基础模型指导的BEV地图，利用描述性的DINOv2特征，并通过一种新颖的蒸馏过程将其蒸馏成BEV表示。通过将PV特征与由DINOv2的语义和几何特征增强的BEV地图集成，我们的模型通过可变形聚合利用这种混合表示来增强3D物体检测和跟踪。在nuScenes和Argoverse 2基准测试上的大量实验表明，DualViewDistill达到了最先进的性能。结果展示了基础模型BEV地图在实现更可靠自动驾驶感知方面的潜力。我们在https://dualviewdistill.cs.uni-freiburg.de提供了代码和预训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于相机的3D目标检测和跟踪方法中只依赖单一视图特征(透视视图PV或鸟瞰视图BEV)的问题，无法同时利用细粒度物体细节和空间结构化场景表示。这个问题在自动驾驶领域非常重要，因为可靠的3D感知系统需要既能识别物体细节，又能理解空间布局，这对安全驾驶至关重要。单一视图特征的限制导致现有方法难以同时满足这两方面的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法的局限性，发现BEV-based方法提供结构化空间表示但缺乏物体细节，而PV-based方法有丰富细节但空间推理能力有限。他们借鉴了BEVFormer、LSS等BEV方法的表示思想，以及Sparse4Dv3等查询设计方法，还参考了DINOv2等基础模型的一般特征能力和HDNet等地图感知方法的空间先验思想。在此基础上，作者创新性地设计了双视图融合框架和基础模型引导的BEV地图蒸馏方法，同时利用两种视图的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双视图融合与基础模型引导的特征蒸馏。方法同时利用透视视图(PV)的物体细节优势和鸟瞰视图(BEV)的空间结构优势，并通过DINOv2基础模型的特征蒸馏增强BEV表示。整体流程包括：1)输入多视角RGB图像；2)提取PV特征和DINOv2特征；3)使用LSS机制将PV特征提升到BEV空间；4)通过Transformer处理对象查询，结合可变形聚合与PV和BEV特征交互；5)将DINOv2特征投影到点云生成BEV伪标签并蒸馏；6)输出3D边界框和对象轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双视图检测框架，首次统一PV和BEV特征；2)基础模型引导的BEV地图，利用DINOv2特征蒸馏提供丰富的在线神经地图；3)可变形聚合机制有效融合两种视图特征；4)混合监督策略结合检测/跟踪监督和BEV特征蒸馏。相比之前工作，传统方法只使用单一视图特征或依赖手动标注的高清地图，而本文同时使用两种视图并通过基础模型在线生成神经地图，无需人工标注，在nuScenes和Argoverse 2上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DualViewDistill，一种结合透视视图和鸟瞰视图特征并利用DINOv2引导的BEV地图进行3D目标检测和跟踪的混合框架，显著提升了自动驾驶系统的感知性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-based 3D object detection and tracking are essential for perception inautonomous driving. Current state-of-the-art approaches often rely exclusivelyon either perspective-view (PV) or bird's-eye-view (BEV) features, limitingtheir ability to leverage both fine-grained object details and spatiallystructured scene representations. In this work, we propose DualViewDistill, ahybrid detection and tracking framework that incorporates both PV and BEVcamera image features to leverage their complementary strengths. Our approachintroduces BEV maps guided by foundation models, leveraging descriptive DINOv2features that are distilled into BEV representations through a noveldistillation process. By integrating PV features with BEV maps enriched withsemantic and geometric features from DINOv2, our model leverages this hybridrepresentation via deformable aggregation to enhance 3D object detection andtracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarksdemonstrate that DualViewDistill achieves state-of-the-art performance. Theresults showcase the potential of foundation model BEV maps to enable morereliable perception for autonomous driving. We make the code and pre-trainedmodels available at https://dualviewdistill.cs.uni-freiburg.de .</description>
      <author>example@mail.com (Markus Käppeler, Özgün Çiçek, Daniele Cattaneo, Claudius Gläser, Yakov Miron, Abhinav Valada)</author>
      <guid isPermaLink="false">2510.10287v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2510.06582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages (28 main text), 20 figures, 4 supplementary materials; links  to 3D point animations are included in the last table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种半自动的、不确定性感知的地面激光扫描点云语义分割管道，通过球面投影、特征丰富、集成学习和目标标注相结合，减少标注工作量的同时保持高准确性。构建了Mangrove3D数据集并评估了数据效率和特征重要性。&lt;h4&gt;背景&lt;/h4&gt;准确的地面激光扫描点云语义分割受限于昂贵的手动标注，需要一种方法来减少标注工作量同时保持高准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种半自动的、不确定性感知的管道，构建红树林森林语义分割TLS数据集(Mangrove3D)，并评估数据效率和特征重要性，回答需要多少标注数据以及哪些特征最重要的问题。&lt;h4&gt;方法&lt;/h4&gt;将3D点投影到2D球面网格，使用多源特征丰富像素，训练分割网络集成生成伪标签和不确定性地图，不确定性地图指导模糊区域标注，将2D输出投影回3D，开发三层可视化套件，构建Mangrove3D数据集，评估数据效率和特征重要性，通过跨数据集测试验证特征丰富化策略的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠捕获了几乎所有的判别能力，平均交并比(mIoU)稳定在约0.76。&lt;h4&gt;结论&lt;/h4&gt;提出了稳健的、不确定性感知的TLS标注管道和可视化工具，构建了Mangrove3D数据集，提供了关于数据效率和特征重要性的经验指导，使得生态监测等领域的可扩展、高质量的TLS点云分割成为可能。&lt;h4&gt;翻译&lt;/h4&gt;准确的地面激光扫描点云语义分割受限于昂贵的手动标注。我们提出了一种半自动的、不确定性感知的管道，结合球面投影、特征丰富、集成学习和目标标注，以减少标注工作量，同时保持高准确性。我们的方法将3D点投影到2D球面网格，用多源特征丰富像素，并训练分割网络集成来生成伪标签和不确定性地图，后者指导模糊区域的标注。将2D输出投影回3D，得到密集标注的点云，并配有三层可视化套件(2D特征图、3D着色点云和紧凑虚拟球体)用于快速分类和审阅者指导。使用此管道，我们构建了Mangrove3D，一个用于红树林森林的语义分割TLS数据集。我们进一步评估数据效率和特征重要性，以解决两个关键问题：(1)需要多少标注数据，(2)哪些特征最重要。结果表明，性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠捕获了几乎所有的判别能力，平均交并比(mIoU)稳定在约0.76。最后，我们通过在ForestSemantic和Semantic3D上的跨数据集测试，验证了我们的特征丰富化策略的泛化能力。我们的贡献包括：(i)带有可视化工具的稳健的、不确定性感知的TLS标注管道；(ii)Mangrove3D数据集；以及(iii)关于数据效率和特征重要性的经验指导，从而使得生态监测等领域的可扩展、高质量的TLS点云分割成为可能。数据集和处理脚本可在https://fz-rit.github.io/through-the-lidars-eye/公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决地面激光扫描(TLS)点云语义分割中高质量标注数据集稀缺的问题。这个问题很重要，因为手动标注全分辨率TLS扫描非常耗费人力，特别是在生态场景中，由于严重的遮挡、不规则的几何形状和交织的树结构，这一问题尤为突出。这阻碍了自动分析在林业和环境监测中的采用，尽管这些领域有迫切需求，而准确的语义分割是生态研究中分析树木指标、生物量和栖息地特征的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何降低标注复杂度并提高效率，考虑将不规则的3D数据转换为结构化的2D地图，集成特征增强的分割方法，并纳入不确定性分析来指导人工校正。作者借鉴了现有工作：球面投影概念来自地图学，应用于将3D点云转换为2D图像；主动学习范式用于迭代查询标注员获取信息量最大的样本；自训练策略让模型重用高置信度预测；特征融合方法利用几何结构、辐射度响应和位置上下文增强分割。作者的创新在于将这些方法整合并专门针对TLS点云和生态场景进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将3D点云投影到2D球面网格上，在2D空间中进行高效的标注和分割，然后再将结果投影回3D，从而降低标注复杂度并提高效率。整体流程分为三阶段：1)球面投影与可视化：将3D点云转换为2D球面网格，创建多通道特征图和三种可视化(2D特征图、3D彩色点云、虚拟球体)；2)混合标注：使用少量初始标注训练三个分割模型集合，生成伪标签和不确定性图，高不确定性区域人工精修，高置信度预测保留为伪标签；3)反向投影与精修：将2D分割结果投影回3D，应用几何平滑和特征驱动的修复，生成最终标注结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)不确定性感知的TLS标注流程，集成球面投影、特征增强和集成学习；2)三级可视化套件，便于标注和检查；3)系统化的特征增强策略，识别最优特征组合；4)创建首个针对复杂红树林生态系统的TLS数据集Mangrove3D。相比之前工作，不同之处在于：现有3D标注工具主要为简单场景设计；早期球面投影方法使用传统聚类而非深度学习；现有主动学习方法主要在RGB数据集上应用；PointPainting等方法依赖多传感器数据，而本文方法仅使用LiDAR数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合球面投影、特征增强和不确定性分析的半自动标注流程，显著降低了地面激光扫描点云语义分割的标注成本，同时创建了首个针对复杂红树林生态系统的TLS数据集，为生态监测等领域提供了高效、高质量的分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate semantic segmentation of terrestrial laser scanning (TLS) pointclouds is limited by costly manual annotation. We propose a semi-automated,uncertainty-aware pipeline that integrates spherical projection, featureenrichment, ensemble learning, and targeted annotation to reduce labelingeffort, while sustaining high accuracy. Our approach projects 3D points to a 2Dspherical grid, enriches pixels with multi-source features, and trains anensemble of segmentation networks to produce pseudo-labels and uncertaintymaps, the latter guiding annotation of ambiguous regions. The 2D outputs areback-projected to 3D, yielding densely annotated point clouds supported by athree-tier visualization suite (2D feature maps, 3D colorized point clouds, andcompact virtual spheres) for rapid triage and reviewer guidance. Using thispipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangroveforests. We further evaluate data efficiency and feature importance to addresstwo key questions: (1) how much annotated data are needed and (2) whichfeatures matter most. Results show that performance saturates after ~12annotated scans, geometric features contribute the most, and compactnine-channel stacks capture nearly all discriminative power, with the meanIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirmthe generalization of our feature-enrichment strategy through cross-datasettests on ForestSemantic and Semantic3D.  Our contributions include: (i) a robust, uncertainty-aware TLS annotationpipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)empirical guidance on data efficiency and feature importance, thus enablingscalable, high-quality segmentation of TLS point clouds for ecologicalmonitoring and beyond. The dataset and processing scripts are publiclyavailable at https://fz-rit.github.io/through-the-lidars-eye/.</description>
      <author>example@mail.com (Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt)</author>
      <guid isPermaLink="false">2510.06582v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation</title>
      <link>http://arxiv.org/abs/2509.15886v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了将视觉基础模型SAM2应用于LiDAR点云分割的range-view方法，通过结合2D特征提取与投影/反投影操作，实现了在保持2D方法效率的同时获得有竞争力的3D分割性能。&lt;h4&gt;背景&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心技术。目前基于体素和点的方法虽能捕获细粒度几何信息，但计算成本高，内存访问不规则，实时效率有限。相比之下，range-view方法相对未被充分探索，但可利用成熟的2D语义分割技术进行快速准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究当前最先进的视觉基础模型SAM2是否可作为LiDAR点云在range view中的强大backbone，探索VFMs作为3D感知通用backbone的可行性。&lt;h4&gt;方法&lt;/h4&gt;提出了首个适应SAM2进行3D分割的range-view框架，结合高效2D特征提取与标准投影/反投影操作处理点云。对编码器进行了三种架构修改：(1)强调LiDAR范围图像中水平空间依赖性的新模块；(2)针对球面投影几何特性的定制配置；(3)专门设计用于捕获range-view伪图像中独特空间模式和间断性的适应机制。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在SemanticKITTI数据集上实现了具有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简单性。&lt;h4&gt;结论&lt;/h4&gt;VFMs作为3D感知通用backbone具有可行性，为统一的基础模型驱动的LiDAR分割开辟了道路，使用VFMs的range-view分割方法取得了有希望的结果。&lt;h4&gt;翻译&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心。虽然最近的体素和基于点的方法因其与深度架构的兼容性和捕获细粒度几何的能力而主导研究，但它们通常带来高计算成本、不规则的内存访问和有限的实时效率。相比之下，range-view方法虽然相对未被充分探索，但可以利用成熟的2D语义分割技术进行快速准确的预测。受视觉基础模型(VFMs)在描述、零样本识别和多模态任务中快速进展的启发，我们研究了当前最先进的分割VFM SAM2是否可以作为LiDAR点云在range view中的强大backbone。据我们所知，我们提出了第一个适应SAM2进行3D分割的range-view框架，将高效的2D特征提取与标准投影/反投影相结合以处理点云。为了优化SAM2对range-view表示的处理，我们对编码器实现了几种架构修改：(1)一个强调LiDAR范围图像中固有水平空间依赖性的新模块；(2)针对球面投影几何特性的定制配置；(3)编码器主干中的一种适应机制，专门设计用于捕获range-view伪图像中存在的独特空间模式和间断性。我们的方法在SemanticKITTI上实现了具有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简单性。这项工作证明了VFMs作为3D感知通用backbone的可行性，并为统一的基础模型驱动的LiDAR分割开辟了道路。结果让我们得出结论，使用VFMs的range-view分割方法取得了有希望的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云分割的效率和准确性问题。现有的基于体素和点的方法计算成本高、内存访问不规则、运行效率有限。这个问题在自动驾驶和3D场景理解中至关重要，因为准确分割点云可以帮助车辆识别道路、车辆、行人等关键元素，确保安全导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，注意到范围视图方法可以利用成熟的2D语义分割技术。受SAM2等视觉基础模型在图像分割中成功的启发，作者将其应用于3D点云分割。为了适应范围视图表示，作者对编码器进行了关键修改：设计了新的Stem模块、自定义了Hiera块配置、调整了窗口注意力机制。该方法借鉴了SAM2模型、Receptive Field Blocks、k-NN插值和多种损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SAM2视觉基础模型应用于3D点云分割，通过将点云转换为范围视图表示，利用2D分割模型的能力，再投影回3D空间。流程包括：1)范围投影预处理，将3D点云转换为64×2048像素的范围图像；2)模型架构，包含Stem模块、基于Hiera的编码器和Receptive Field Block解码器；3)后处理，使用k-NN插值将标签传播回3D点云；4)训练，使用复合损失函数和数据增强技术。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个将SAM2适应3D分割的范围视图框架；2)针对范围视图的编码器修改(Stem模块、自定义Hiera块、调整的注意力机制)；3)多组件编码器架构结合感受野块；4)有效的k-NN插值后处理技术；5)复合损失函数。相比之前的工作，RangeSAM利用了最先进的SAM2模型，设计了非对称注意力窗口强调水平空间关系，发现多数据集训练比2D预训练更有效，实现了与现有方法相竞争的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM创新性地将SAM2视觉基础模型适应到范围视图表示的LiDAR点云分割中，通过专门设计的架构修改和训练策略，实现了与现有方法相竞争的性能，同时利用了2D分割模型的效率和可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation is central to autonomous driving and 3D sceneunderstanding. While voxel- and point-based methods dominate recent researchdue to their compatibility with deep architectures and ability to capturefine-grained geometry, they often incur high computational cost, irregularmemory access, and limited real-time efficiency. In contrast, range-viewmethods, though relatively underexplored - can leverage mature 2D semanticsegmentation techniques for fast and accurate predictions. Motivated by therapid progress in Visual Foundation Models (VFMs) for captioning, zero-shotrecognition, and multimodal tasks, we investigate whether SAM2, the currentstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone forLiDAR point cloud segmentation in the range view. We present , to ourknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,coupling efficient 2D feature extraction with standardprojection/back-projection to operate on point clouds. To optimize SAM2 forrange-view representations, we implement several architectural modifications tothe encoder: (1) a novel module that emphasizes horizontal spatial dependenciesinherent in LiDAR range images, (2) a customized configuration of tailored tothe geometric properties of spherical projections, and (3) an adapted mechanismin the encoder backbone specifically designed to capture the unique spatialpatterns and discontinuities present in range-view pseudo-images. Our approachachieves competitive performance on SemanticKITTI while benefiting from thespeed, scalability, and deployment simplicity of 2D-centric pipelines. Thiswork highlights the viability of VFMs as general-purpose backbones for 3Dperception and opens a path toward unified, foundation-model-driven LiDARsegmentation. Results lets us conclude that range-view segmentation methodsusing VFMs leads to promising results.</description>
      <author>example@mail.com (Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Dieter Fellner, Saptarshi Neil Sinha)</author>
      <guid isPermaLink="false">2509.15886v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</title>
      <link>http://arxiv.org/abs/2510.11649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM SIGGraphAsia 2025. Project website:  https://yuxuan-xue.com/physic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhySIC是一个用于物理合理的人体-场景交互和接触重建的框架，可以从单张RGB图像中恢复度量一致的3D人体和场景，处理遮挡和深度模糊等问题，实现高效且高质量的重建。&lt;h4&gt;背景&lt;/h4&gt;从单张图像重建具有度量准确性的三维人体及其周围场景对于虚拟现实、机器人和全面的3D场景理解至关重要。然而，现有方法面临深度模糊、遮挡和物理不一致接触等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法面临的深度模糊、遮挡和物理不一致接触等问题，实现从单张图像中重建物理合理的人体-场景交互。&lt;h4&gt;方法&lt;/h4&gt;PhySIC从单张RGB图像恢复度量一致的SMPL-X人体网格、密集场景表面和顶点级接触图；执行遮挡感知的图像修复；融合可见深度与未缩放几何形状；合成缺失支撑表面；通过加权置信度优化优化人体姿态、相机参数和全局尺度；使用显式遮挡掩码保护不可见区域；高效处理多个人体交互。&lt;h4&gt;主要发现&lt;/h4&gt;PhySIC在单图像基线上表现更优，将场景平均每顶点误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51；定性结果显示了真实的脚-地面交互、自然的坐姿以及对严重遮挡家具的合理重建。&lt;h4&gt;结论&lt;/h4&gt;通过将单张图像转换为物理合理的3D人体-场景对，PhySIC推动了可扩展的3D场景理解，实现仅需9秒的联合人体-场景优化和不到27秒的端到端处理。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像重建具有度量准确性的人体及其周围场景对于虚拟现实、机器人和全面的3D场景理解至关重要。然而，现有方法难以处理深度模糊、遮挡和物理不一致接触等问题。为应对这些挑战，我们引入了PhySIC，一个用于物理合理的人体-场景交互和接触重建的框架。PhySIC从单张RGB图像中恢复度量一致的SMPL-X人体网格、密集场景表面和顶点级接触图，并在共享坐标系中表示。从粗略的单目深度和人体估计开始，PhySIC执行遮挡感知的图像修复，将可见深度与未缩放的几何形状融合以获得稳健的度量支架，并合成缺失的支撑表面如地板。通过加权置信度优化，通过联合强制深度对齐、接触先验、避免穿透和2D重投影一致性来优化人体姿态、相机参数和全局尺度。显式遮挡掩码保护不可见区域避免不合理的配置。PhySIC效率高，仅需9秒进行联合人体-场景优化，端到端时间不到27秒。自然处理多个人体，能够重建多样化的交互。经验表明，PhySIC优于单图像基线方法，将场景平均每顶点误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51。定性结果显示了真实的脚-地面交互、自然的坐姿以及对严重遮挡家具的合理重建。通过将单张图像转换为物理合理的3D人体-场景对，PhySIC推动了可扩展的3D场景理解。我们的实现已在https://yuxuan-xue.com/physic公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单张RGB图像中重建度量准确的3D人体和场景几何，以及它们之间的物理交互关系的问题。这个问题在现实和研究中非常重要，因为对于虚拟现实、机器人和全面的3D场景理解至关重要，但现有方法难以处理深度歧义、遮挡和物理上不一致的接触等挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人体和场景是相互约束的：场景在物理上限制了可能的人体姿势，而人体姿势为估计场景几何和规模提供了重要线索。基于这一观察，作者借鉴了多种现有技术，包括使用SAM2进行人体分割、OmniEraser进行图像修复、DepthPro预测度量深度、MoGe获取详细几何、SMPL-X表示人体等。作者设计了一个三阶段方法：首先估计度量规模场景，然后重建人体并与场景对齐，最后通过联合优化确保物理合理性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是联合优化人体姿势、场景几何和全局尺度，产生物理上合理的人体-场景对，同时利用人体和场景之间的物理约束关系提高重建质量。整体流程分为三阶段：1)度量规模场景估计：通过图像修复、结合度量深度和详细几何、地面平面拟合和组合场景点来获取完整场景；2)人体重建与场景对齐：将人体点与场景点对齐，使用SMPL-X模型表示人体，并优化网格与场景对齐；3)联合人体-场景优化：通过接触损失、遮挡感知的穿透损失和正则化项确保物理合理性，并能处理多个人体情况。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个能处理多个人体、多样场景和交互类型的度量规模人体-场景重建方法；2)引入了强大的初始化策略和遮挡感知的联合优化；3)高效的重建管道，27秒内完成端到端重建；4)能处理复杂交互如坐姿和脚-地面接触。相比之前的工作，PhySIC不需要视频输入或多视图图像(如HSR和HSfM)，也不需要预定义的场景扫描(如PROX)，能处理室内外多种场景，而不仅限于特定室内环境(如HolisticMesh)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhySIC通过联合优化人体姿势、场景几何和接触信息，实现了从单张RGB图像中快速重建物理上合理的3D人体-场景交互，显著提高了重建的准确性和效率，为虚拟现实、机器人和3D场景理解提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763862&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing metrically accurate humans and their surrounding scenes from asingle image is crucial for virtual reality, robotics, and comprehensive 3Dscene understanding. However, existing methods struggle with depth ambiguity,occlusions, and physically inconsistent contacts. To address these challenges,we introduce PhySIC, a framework for physically plausible Human-SceneInteraction and Contact reconstruction. PhySIC recovers metrically consistentSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps withina shared coordinate frame from a single RGB image. Starting from coarsemonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,fuses visible depth with unscaled geometry for a robust metric scaffold, andsynthesizes missing support surfaces like floors. A confidence-weightedoptimization refines body pose, camera parameters, and global scale by jointlyenforcing depth alignment, contact priors, interpenetration avoidance, and 2Dreprojection consistency. Explicit occlusion masking safeguards invisibleregions against implausible configurations. PhySIC is efficient, requiring only9 seconds for joint human-scene optimization and under 27 seconds end-to-end.It naturally handles multiple humans, enabling reconstruction of diverseinteractions. Empirically, PhySIC outperforms single-image baselines, reducingmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,and improving contact F1 from 0.09 to 0.51. Qualitative results show realisticfoot-floor interactions, natural seating, and plausible reconstructions ofheavily occluded furniture. By converting a single image into a physicallyplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.Our implementation is publicly available at https://yuxuan-xue.com/physic.</description>
      <author>example@mail.com (Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll)</author>
      <guid isPermaLink="false">2510.11649v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.11567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新框架，利用扩散模型和不完美伪标签将合成数据适应到目标域，生成高保真图像，解决了合成数据与真实数据之间的差距问题，实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;合成数据集被广泛用于训练城市场景识别模型，但即使高度逼真的渲染图像与真实图像之间仍然存在明显差距。当适应特定目标领域（如Cityscapes）时，这种差距尤为明显，因为建筑、植被、物体外观和相机特性的差异限制了下游性能。&lt;h4&gt;目的&lt;/h4&gt;解决合成数据与真实数据之间的差距问题，避免使用昂贵的3D建模来缩小这一差距，因为这会违背低成本标记数据的目的。&lt;h4&gt;方法&lt;/h4&gt;提出一种新框架，使用不完美的伪标签将现成的扩散模型适应到目标域。训练后的模型可以从任何合成数据集的语义图中生成高保真、目标对齐的图像。该方法过滤次优生成结果，修正图像-标签不对齐问题，并标准化不同数据集的语义，将弱合成数据转换为具有竞争力的真实域训练集。&lt;h4&gt;主要发现&lt;/h4&gt;在五个合成数据集和两个真实目标数据集上的实验显示，与最先进的翻译方法相比，分割性能提升了高达+8.0% mIoU，使快速构建的合成数据集变得与需要大量手动设计的高投入、时间密集型合成数据集一样有效。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个有价值的协作范式，其中快速语义原型与生成模型相结合，为城市场景理解实现了可扩展的高质量训练数据创建。&lt;h4&gt;翻译&lt;/h4&gt;合成数据集被广泛用于训练城市场景识别模型，但即使高度逼真的渲染图像与真实图像之间仍然存在明显差距。当适应特定目标领域（如Cityscapes）时，这种差距尤为明显，因为建筑、植被、物体外观和相机特性的差异限制了下游性能。使用更详细的3D建模来缩小这一差距需要昂贵的资产和场景设计，违背了低成本标记数据的目的。为此，我们提出了一种新框架，仅使用不完美的伪标签将现成的扩散模型适应到目标域。一旦训练完成，它可以从任何合成数据集的语义图中生成高保真、目标对齐的图像，包括只需几小时而非数月创建的低投入来源。该方法过滤次优生成结果，修正图像-标签不对齐问题，并标准化不同数据集的语义，将弱合成数据转换为具有竞争力的真实域训练集。在五个合成数据集和两个真实目标数据集上的实验显示，与最先进的翻译方法相比，分割性能提升了高达+8.0% mIoU，使快速构建的合成数据集变得与需要大量手动设计的高投入、时间密集型合成数据集一样有效。这项工作展示了一个有价值的协作范式，其中快速语义原型与生成模型相结合，为城市场景理解实现了可扩展的高质量训练数据创建。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效生成高质量城市场景语义分割训练数据的问题，特别是如何将低成本的合成数据转换为真实世界场景的有效训练数据。这个问题重要是因为真实世界标注数据收集成本高、耗时长；即使最逼真的合成数据与真实图像间也存在明显差距，限制了下游模型性能；而传统3D建模方法需要昂贵资产和场景设计，违背了低成本标记数据的目的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是：认识到合成数据与真实数据差距问题→探索扩散模型替代昂贵光照建模→提出3D建模与生成建模协作范式→设计两阶段策略解决视觉风格学习和语义对齐两个竞争任务。借鉴了现有工作包括：使用预训练扩散模型(Stable Diffusion 2.1)作为基础；采用伪标签提供语义条件；使用分类器自由引导提高样本质量；结合深度图作为正则化输入；利用连通组件分析评估生成质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过微调现成扩散模型适应特定目标域，使用不完美伪标签生成高保真目标对齐图像，采用对象中心过滤策略优化结果，并标准化不同数据集间的语义。整体流程：1)两阶段微调-第一阶段学习目标域视觉风格，第二阶段实现语义布局对齐；2)正则化技术-使用粗略伪标签、伪深度图和零条件引导提高鲁棒性；3)自动数据生成-为单个语义图生成多样样本，用MCOC评分筛选高质量样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)源无关框架，能从任何合成数据集生成目标域图像；2)两阶段训练策略，分别学习视觉风格和语义控制；3)多种正则化技术组合提高鲁棒性；4)对象中心样本选择机制提高数据质量。相比不同：1)与传统I2I方法相比，无需成对数据，能生成多样化样本，对源质量要求低，图像质量更高；2)与UDA方法相比，明确生成目标域图像，提供透明度，解耦数据生成与下游模型；3)与其他扩散模型方法相比，针对特定目标域微调，使用伪标签而非真实标签，结合多种正则化技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于扩散模型的框架，通过利用不完美伪标签和两阶段训练策略，将低努力合成的语义布局高效转换为高质量、目标域对齐的训练图像，显著提升了城市场景语义分割性能，并展示了快速语义场景原型与生成模型协作创建大规模高质量训练数据的范式转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic datasets are widely used for training urban scene recognitionmodels, but even highly realistic renderings show a noticeable gap to realimagery. This gap is particularly pronounced when adapting to a specific targetdomain, such as Cityscapes, where differences in architecture, vegetation,object appearance, and camera characteristics limit downstream performance.Closing this gap with more detailed 3D modelling would require expensive assetand scene design, defeating the purpose of low-cost labelled data. To addressthis, we present a new framework that adapts an off-the-shelf diffusion modelto a target domain using only imperfect pseudo-labels. Once trained, itgenerates high-fidelity, target-aligned images from semantic maps of anysynthetic dataset, including low-effort sources created in hours rather thanmonths. The method filters suboptimal generations, rectifies image-labelmisalignments, and standardises semantics across datasets, transforming weaksynthetic data into competitive real-domain training sets. Experiments on fivesynthetic datasets and two real target datasets show segmentation gains of upto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidlyconstructed synthetic datasets as effective as high-effort, time-intensivesynthetic datasets requiring extensive manual design. This work highlights avaluable collaborative paradigm where fast semantic prototyping, combined withgenerative models, enables scalable, high-quality training data creation forurban scene understanding.</description>
      <author>example@mail.com (Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother)</author>
      <guid isPermaLink="false">2510.11567v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>mmWalk: Towards Multi-modal Multi-view Walking Assistance</title>
      <link>http://arxiv.org/abs/2510.11520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 Datasets and Benchmarks Track. Data and  Code: https://github.com/KediYing/mmWalk&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了mmWalk，一个针对盲人或低视力人群的多模态数据集，以及mmWalkVQA基准，用于户外安全导航辅助。研究评估了现有视觉语言模型的表现，并展示了所构建数据集的有效性。&lt;h4&gt;背景&lt;/h4&gt;盲人或低视力人群在极端或复杂环境中的行走辅助仍然是一个重大挑战，主要是因为缺乏整体场景理解能力。&lt;h4&gt;目的&lt;/h4&gt;为了满足盲人或低视力社区的实际需求，构建一个模拟的多模态数据集，用于户外安全导航。&lt;h4&gt;方法&lt;/h4&gt;构建了mmWalk数据集，包含120条手动控制的、场景分类的行走轨迹和62k个同步帧；收集了超过559k张RGB、深度和语义模态的全景图像；每条轨迹都包含户外极端情况和可访问性特定地标；创建了包含69k个视觉问答三元组的mmWalkVQA基准；评估了最先进的视觉语言模型在零样本和少样本设置下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视觉语言模型在风险评估和导航任务上表现不佳；在真实世界数据集上验证了mmWalk微调模型的有效性。&lt;h4&gt;结论&lt;/h4&gt;所构建的数据集对于推进多模态行走辅助技术有效。&lt;h4&gt;翻译&lt;/h4&gt;在极端或复杂环境中为盲人或低视力人群(BLV)提供行走辅助仍然是一个重大挑战，主要由于缺乏整体场景理解能力。受BLV社区的现实需求驱动，我们构建了mmWalk，这是一个模拟的多模态数据集，集成了多视图传感器和面向可访问性的特征，用于户外安全导航。我们的数据集包含120条手动控制、场景分类的行走轨迹，有62k个同步帧。它包含RGB、深度和语义模态下的超过559k张全景图像。此外，为了强调实际相关性，每条轨迹都包含户外极端情况和BLV用户特定的可访问性地标。此外，我们生成了mmWalkVQA，一个包含9个类别、超过69k个视觉问答三元组的VQA基准，专为安全和知情行走辅助而定制。我们使用零样本和少样本设置评估了最先进的视觉语言模型(VLMs)，发现它们在风险评估和导航任务上表现不佳。我们在真实世界数据集上验证了mmWalk微调模型，展示了我们的数据集在推进多模态行走辅助方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决盲人或低视力人士在极端或复杂环境中行走的安全辅助问题。这个问题很重要，因为全球有超过22亿人受盲症或低视力影响，超过63%的BLV人士在户外导航时受过伤害，7%的人每月至少摔倒一次，而现有导航系统未能充分识别危险状况、不平整表面和临时障碍物。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于BLV社区的真实需求设计方法，在Carla模拟器中手动收集多视角(步行者、导盲犬、无人机)和多模态(RGB、深度、语义分割)数据。借鉴了现有多视图辅助系统如OpenMPR和MSSP，以及视觉辅助数据集如VizWiz、GuideDog和SideGuide。同时参考了ATmaps统计中的地标信息，并定义了8种BLV相关的特殊情况。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个多模态、多视角的行走辅助数据集，特别关注BLV用户的安全需求，通过全景图像和多种传感器数据提供全面的场景理解。流程包括：在模拟器中收集120条轨迹和559k全景图像；标注轨迹元数据和特殊情况；使用GPT-4o生成69k视觉问答三元组；评估多种视觉语言模型；在真实世界数据集上验证微调模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个结合多视角和可访问性特征的多模态数据集；包含RGB、深度和语义分割的全景图像；专门针对BLV的特殊情况和导航地标；包含69k视觉问答三元组的基准测试；全面评估现有模型局限性。相比之前工作，它特别关注BLV安全需求，提供多视角同步数据，结合全景图像和多种传感器模态，且VQA数据集规模更大、类别更丰富。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; mmWalk论文贡献了一个多模态、多视角的行走辅助数据集和基准测试，特别关注盲人和低视力人士的安全需求，为开发更安全、更全面的行走辅助系统提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Walking assistance in extreme or complex environments remains a significantchallenge for people with blindness or low vision (BLV), largely due to thelack of a holistic scene understanding. Motivated by the real-world needs ofthe BLV community, we build mmWalk, a simulated multi-modal dataset thatintegrates multi-view sensor and accessibility-oriented features for outdoorsafe navigation. Our dataset comprises 120 manually controlled,scenario-categorized walking trajectories with 62k synchronized frames. Itcontains over 559k panoramic images across RGB, depth, and semantic modalities.Furthermore, to emphasize real-world relevance, each trajectory involvesoutdoor corner cases and accessibility-specific landmarks for BLV users.Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visualquestion-answer triplets across 9 categories tailored for safe and informedwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)using zero- and few-shot settings and found they struggle with our riskassessment and navigational tasks. We validate our mmWalk-finetuned model onreal-world datasets and show the effectiveness of our dataset for advancingmulti-modal walking assistance.</description>
      <author>example@mail.com (Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.11520v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.11340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT3D是一个可扩展的零样本框架，能够将静态3D场景转换为模拟就绪的交互式副本，具有一致的几何形状，可直接用于各种下游任务。&lt;h4&gt;背景&lt;/h4&gt;交互式3D场景对具身智能日益重要，但现有数据集有限，因为注释部分分割、运动类型和运动轨迹的过程是劳动密集型的。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的零样本框架，解决静态3D场景转换为交互式副本的难题，降低关节场景理解的大规模研究门槛。&lt;h4&gt;方法&lt;/h4&gt;包括四个主要贡献：可打开物体检测和分割；关节估计推断关节类型和运动参数；隐藏几何形状补全与交互式物体组装；交互式场景集成以确保与标准模拟平台的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的室内场景中，REACT3D在检测/分割和关节度量方面取得了最先进的性能，证明了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;REACT3D为可扩展的交互式场景生成提供了实际基础，为大规模关节场景理解研究创造了条件。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D场景对于具身智能越来越重要，然而现有的数据集仍然有限，因为注释部分分割、运动类型和运动轨迹的过程是劳动密集型的。我们提出了REACT3D，一个可扩展的零样本框架，将静态3D场景转换为模拟就绪的交互式副本，具有一致的几何形状，能够直接用于各种下游任务。我们的贡献包括：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。我们在多样化的室内场景中，在检测/分割和关节度量方面取得了最先进的性能，证明了我们框架的有效性，并为可扩展的交互式场景生成提供了实际基础，从而降低了关节场景理解的大规模研究门槛。我们的项目页面是react3d.github.io。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将静态3D场景转换为具有交互功能的物理3D场景，特别是识别和恢复场景中可动关节（如门、抽屉等可开合物体）的问题。这个问题在现实中很重要，因为交互式3D场景对虚拟现实、游戏、电影制作以及机器人系统开发至关重要，而现有数据集由于标注过程劳动密集而受限，自动化生成此类场景能有效扩展研究规模并降低使用门槛。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者设计REACT3D时借鉴了多项现有工作：使用RAM++和LLaVA进行语义识别，Grounded SAM进行分割，OPDMulti进行关节估计，并改进了DRAWER的多视图融合方法。作者认识到现有方法在处理开放词汇物体、关节估计精度和隐藏几何生成方面的不足，因此设计了结合视觉基础模型和视觉语言模型的零样本框架，通过开放词汇检测、关节细化、隐藏几何生成和场景集成四个步骤实现静态到交互场景的转换。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型和视觉语言模型从静态3D场景中恢复物体关节，生成物理启用的交互式数字孪生。整体流程分为四个主要步骤：1) 开放物体检测和分割，识别可动物体并提取可动部分；2) 关节估计，推断关节类型和运动参数并进行细化；3) 隐藏几何生成，完成物体内部腔体结构；4) 交互场景集成，将可动物体与静态背景结合，生成纹理并导出为兼容多种模拟平台的格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 开放词汇检测方法减轻了标签偏差，提高了对长尾可动物体的覆盖；2) 基于定向边界框的关节细化提高了参数准确性；3) 隐藏几何生成解决了物体内部结构缺失问题；4) 多平台兼容的导出格式确保了广泛适用性。相比前人工作，REACT3D是零样本方法，无需针对特定类别训练；改进了多视图融合策略；引入了几何驱动的关节细化；并考虑了隐藏几何生成，而不仅仅是表面几何。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REACT3D提供了一个创新的零样本框架，能够将静态3D场景转换为具有物理交互功能的数字孪生体，通过开放词汇检测、关节细化、隐藏几何生成和无缝集成，实现了在多种平台上就绪的交互式场景生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D scenes are increasingly vital for embodied intelligence, yetexisting datasets remain limited due to the labor-intensive process ofannotating part segmentation, kinematic types, and motion trajectories. Wepresent REACT3D, a scalable zero-shot framework that converts static 3D scenesinto simulation-ready interactive replicas with consistent geometry, enablingdirect use in diverse downstream tasks. Our contributions include: (i)openable-object detection and segmentation to extract candidate movable partsfrom static scenes, (ii) articulation estimation that infers joint types andmotion parameters, (iii) hidden-geometry completion followed by interactiveobject assembly, and (iv) interactive scene integration in widely supportedformats to ensure compatibility with standard simulation platforms. We achievestate-of-the-art performance on detection/segmentation and articulation metricsacross diverse indoor scenes, demonstrating the effectiveness of our frameworkand providing a practical foundation for scalable interactive scene generation,thereby lowering the barrier to large-scale research on articulated sceneunderstanding. Our project page is\textit{\hypersetup{urlcolor=black}\href{https://react3d.github.io/}{react3d.github.io}}.</description>
      <author>example@mail.com (Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys)</author>
      <guid isPermaLink="false">2510.11340v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Real2USD: Scene Representations in Universal Scene Description Language</title>
      <link>http://arxiv.org/abs/2510.10778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用通用场景描述（USD）语言作为大型语言模型（LLMs）机器人任务中环境表示的有效通用方法，并展示了'Real to USD'系统的实际应用。&lt;h4&gt;背景&lt;/h4&gt;现有的大语言模型机器人方法都是针对特定任务的，如用于导航的视觉语言模型、用于映射的语言引导神经辐射场等，缺乏通用性。&lt;h4&gt;目的&lt;/h4&gt;论证USD语言是LLM机器人任务中环境几何、光度学和语义信息的有效且通用的表示方法。&lt;h4&gt;方法&lt;/h4&gt;开发'Real to USD'系统，使用搭载LiDAR和RGB相机的Unitree Go2四足机器人构建室内环境USD表示，并用谷歌Gemini解析USD实现场景理解、推理和规划；同时在模拟仓库和医院环境中测试系统。&lt;h4&gt;主要发现&lt;/h4&gt;USD是基于XML的场景图，可被LLM和人类阅读，足够丰富以支持几乎所有任务，皮克斯开发此语言用于存储资产、场景甚至电影。&lt;h4&gt;结论&lt;/h4&gt;USD是LLM机器人任务中环境表示的有效通用方法，能够处理多样化物体和具有挑战性的环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）可以帮助机器人对抽象任务规范进行推理。这需要在机器人使用的经典环境表示基础上增加基于自然语言的先验知识。现有方法都是针对特定任务的，例如用于导航的视觉语言模型、用于映射的语言引导神经辐射场等。本文提出通用场景描述（USD）语言作为LLM机器人任务中环境几何、光度学和语义信息的有效且通用的表示方法。我们的论点很简单：USD是一种基于XML的场景图，可被LLM和人类阅读，足够丰富以支持几乎所有任务——皮克斯开发这种语言是为了存储资产、场景甚至电影。我们使用搭载LiDAR和RGB相机的Unitree Go2四足机器人展示了'Real to USD'系统，该系统能够（i）构建包含多样物体和大量玻璃等挑战性室内环境的显式USD表示，以及（ii）使用谷歌的Gemini解析USD以展示场景理解、复杂推理和规划能力。我们还使用Nvidia的Issac Sim在模拟仓库和医院环境中研究了该系统的不同方面。代码可在https://github.com/grasp-lyrl/Real2USD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何为机器人创建一个通用的场景表示方法，该方法能够结合几何、光学和语义信息，并且能被大型语言模型读取和理解。这个问题很重要，因为当前机器人系统主要使用度量表示进行环境建模，缺乏人类使用的丰富语义信息，而结合语义和度量信息对机器人构建有效的空间表示至关重要。现有方法通常针对特定任务设计，需要一种通用且有效的场景表示方法来支持基于自然语言的机器人任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到机器人需要结合语义和度量信息，然后提出使用USD（Universal Scene Description）语言作为解决方案，因为它是基于XML的场景图，可被LLMs和人类读取，且足够丰富支持各种任务。作者借鉴了3D度量-语义场景图方法（如SLAM、神经辐射场）和'Real to Sim to Real'研究思路，以及使用深度学习进行语义分割和基础模型进行大型数据库搜索的方法。作者设计了'Real2USD'系统，使用四足机器人携带传感器构建室内环境的USD表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用USD语言作为通用场景表示，将真实世界场景转换为USD格式，使其能被大型语言模型读取和理解，从而支持基于自然语言的任务规划。整体流程包括：1)资产识别和检索（使用YOLOE检测对象，CLIP和FAISS检索资产）；2)资产定位（使用模拟器生成资产点云，通过ICP算法配准）；3)资产协调（使用非极大值抑制和评分系统选择最佳资产，物理模拟确保合理性）；4)场景理解与任务规划（使用LLM解析USD，生成路径点和导航计划）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用USD作为通用场景表示；2)开发了完整的Real2USD系统；3)提出模拟循环配准方法提高准确性；4)设计物理协调机制确保场景合理性；5)实现与LLM的集成支持复杂语义任务。相比之前工作，本文方法更具通用性（而非针对特定任务），使用结构化资产表示（而非非结构化网格），可直接被LLM解析（无需额外注释），并能处理更大更复杂的场景（而非单个图像或文本提示生成的小场景）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出并验证了一种使用USD语言作为通用场景表示的方法，使机器人能够将真实世界环境转换为可被大型语言模型读取和理解的格式，从而支持复杂的语义任务规划。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) can help robots reason about abstract taskspecifications. This requires augmenting classical representations of theenvironment used by robots with natural language-based priors. There are anumber of existing approaches to doing so, but they are tailored to specifictasks, e.g., visual-language models for navigation, language-guided neuralradiance fields for mapping, etc. This paper argues that the Universal SceneDescription (USD) language is an effective and general representation ofgeometric, photometric and semantic information in the environment forLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scenegraph, readable by LLMs and humans alike, and rich enough to supportessentially any task -- Pixar developed this language to store assets, scenesand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USDrepresentation of indoor environments with diverse objects and challengingsettings with lots of glass, and (ii) parses the USD using Google's Gemini todemonstrate scene understanding, complex inferences, and planning. We alsostudy different aspects of this system in simulated warehouse and hospitalsettings using Nvidia's Issac Sim. Code is available athttps://github.com/grasp-lyrl/Real2USD .</description>
      <author>example@mail.com (Christopher D. Hsu, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2510.10778v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding</title>
      <link>http://arxiv.org/abs/2510.10194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;使用自然语言定位3D物体对机器人场景理解至关重要。然而，描述中通常涉及多个空间关系来区分相似物体，这使得3D-语言对齐变得困难。当前方法仅对成对物体建模关系，忽略了多模态关系理解中n元组合的全局感知显著性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在3D物体定位中只考虑成对关系而忽视n元组合全局感知的问题，提出一种新的渐进式关系学习框架。&lt;h4&gt;方法&lt;/h4&gt;将关系学习从二元扩展到n元，识别与参照描述全局匹配的视觉关系；设计分组监督损失促进n元关系学习（因训练数据中缺乏参照对象的特定标注）；在n元关系创建的场景图中，使用具有混合注意力机制的多模态网络进一步定位n元组合中的目标。&lt;h4&gt;主要发现&lt;/h4&gt;在ReferIt3D和ScanRefer基准上的实验和消融研究表明，该方法优于现有最先进技术，证明了n元关系感知在3D定位中的优势。&lt;h4&gt;结论&lt;/h4&gt;n元关系感知对3D物体定位至关重要，提出的渐进式关系学习框架有效解决了现有方法的局限性，提高了3D物体定位的准确性。&lt;h4&gt;翻译&lt;/h4&gt;使用自然语言定位3D物体对机器人场景理解至关重要。描述通常涉及多个空间关系来区分相似物体，这使得3D-语言对齐变得困难。当前方法仅对成对物体建模关系，忽略了多模态关系理解中n元组合的全局感知显著性。为解决这一问题，我们提出了一种用于3D物体定位的新型渐进式关系学习框架。我们将关系学习从二元扩展到n元，以识别与参照描述全局匹配的视觉关系。鉴于训练数据中缺乏参照对象的特定标注，我们设计了一种分组监督损失来促进n元关系学习。在使用n元关系创建的场景图中，我们使用具有混合注意力机制的多模态网络进一步定位n元组合中的目标。在ReferIt3D和ScanRefer基准上的实验和消融研究表明，我们的方法优于最先进技术，证明了n元关系感知在3D定位中的优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D物体定位问题，即在3D场景中根据自然语言描述来准确定位特定物体。这个问题在机器人场景理解中至关重要，因为现实世界任务通常需要自然语言指令来指导行动。当场景中有多个相似物体时，人们必须通过多个空间关系来描述目标位置，这需要模型能够同时理解多个关系，而当前方法仅能处理成对物体关系，难以应对复杂场景中的全局关系理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：当前方法仅建模成对物体关系，忽略了多关系描述的全局感知。作者借鉴了Transformer框架、场景图构建、图神经网络和大型语言模型等现有技术，但创新性地提出了从二元到n元的渐进式关系学习框架。作者特别利用LLM提取实体关系作为训练监督，并设计了分组监督损失来处理指代物体不确定性，从而实现了更全面的关系理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过渐进式关系学习从简单二元关系扩展到复杂n元关系，实现全局关系感知。整体流程包括：1) 编码文本和3D物体特征；2) 使用B2N-PRL模块进行二元关系建模，再基于此进行n元关系建模；3) 选择top K2个n元组合构建场景图；4) 通过混合注意力机制(自注意力、图注意力、交叉注意力)更新节点特征；5) 使用MLP输出目标置信度并定位。整个过程通过分组监督损失进行训练，优化关系学习和目标定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 从二元到n元的渐进式关系学习框架，实现全局关系感知；2) 分组监督损失设计，处理训练数据中指代物体不确定性；3) 基于n元关系而非所有相邻实体构建场景图，减少噪声；4) 混合注意力机制的多模态网络增强空间感知。相比之前工作，本文突破了仅处理成对关系的局限，通过全局n元关系理解显著提升了复杂多关系描述下的定位准确性，特别是在有相似物体干扰的场景中表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; B2N3D通过从二元到n元的渐进式关系学习和注意力驱动的图学习，显著提升了复杂多关系描述下的3D物体定位性能，实现了全局关系感知和更准确的目标定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localizing 3D objects using natural language is essential for robotic sceneunderstanding. The descriptions often involve multiple spatial relationships todistinguish similar objects, making 3D-language alignment difficult. Currentmethods only model relationships for pairwise objects, ignoring the globalperceptual significance of n-ary combinations in multi-modal relationalunderstanding. To address this, we propose a novel progressive relationallearning framework for 3D object grounding. We extend relational learning frombinary to n-ary to identify visual relations that match the referentialdescription globally. Given the absence of specific annotations for referredobjects in the training data, we design a grouped supervision loss tofacilitate n-ary relational learning. In the scene graph created with n-aryrelationships, we use a multi-modal network with hybrid attention mechanisms tofurther localize the target within the n-ary combinations. Experiments andablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that ourmethod outperforms the state-of-the-art, and proves the advantages of the n-aryrelational perception in 3D localization.</description>
      <author>example@mail.com (Feng Xiao, Hongbin Xu, Hai Ci, Wenxiong Kang)</author>
      <guid isPermaLink="false">2510.10194v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>CapGeo: A Caption-Assisted Approach to Geometric Reasoning</title>
      <link>http://arxiv.org/abs/2510.09302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出CapGeo框架，通过将几何图形转换为文本描述来提升多模态大语言模型的几何推理能力，并创建了CapGeo-Bench评估基准数据集。&lt;h4&gt;背景&lt;/h4&gt;几何推理是多模态大语言模型的核心挑战，即使是最先进的系统如GPT-O3和Gemini-2.5-Pro在解决几何问题时仍不可靠，尽管它们在文本推理任务上表现优异，表明瓶颈在于理解几何图形而非推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种将视觉内容转换为文本描述的方法，以提升MLLMs的几何推理能力，并创建相应的评估基准。&lt;h4&gt;方法&lt;/h4&gt;引入CapGeo标题辅助推理框架连接视觉和文本模态，提出CapGeo-Bench数据集包含4,641个精选图形-标题对，并开发基于关键点的评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;配备标题后模型性能显著提升：Qwen2.5-VL-72B从8.6%提升到59.0%，Claude-Opus-4从44.8%提升到73.0%。&lt;h4&gt;结论&lt;/h4&gt;CapGeo框架和CapGeo-Bench基准为提升多模态大语言模型中的几何推理能力提供了一条新途径。&lt;h4&gt;翻译&lt;/h4&gt;几何推理仍然是对多模态大语言模型的核心挑战。即使是最先进的闭源系统，如GPT-O3和Gemini-2.5-Pro，在解决几何问题时仍然不可靠，尽管它们在国际数学奥林匹克竞赛等任务上表现出强大的文本推理能力。这一差距表明，瓶颈在于理解几何图形而非推理本身。由于几何图形通常可以用简洁的文本形式忠实描述，将视觉内容转换为标题是一个有前景的方向。受此启发，我们引入了CapGeo，一种标题辅助推理框架，连接视觉和文本模态。实验表明，当模型配备标题时，性能有显著提升：Qwen2.5-VL-72B从仅使用视觉的8.6%提升到59.0%，而Claude-Opus-4从44.8%提升到73.0%。为了系统评估和识别高质量的几何标题生成模型，我们进一步提出了CapGeo-Bench，一个包含4,641个精选图形-标题对的数据集。重要的是，CapGeo-Bench包含一个基于关键点的评估指标，该指标与下游CapGeo性能高度相关，能够可靠评估几何标题生成能力。我们的框架和基准共同为提升多模态大语言模型中的几何推理能力指明了一条新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric reasoning remains a core challenge for Multimodal Large LanguageModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despiteexhibiting strong textual reasoning abilities on tasks like the InternationalMathematical Olympiad (IMO). This gap suggests that the bottleneck lies inunderstanding geometric diagrams rather than reasoning itself. Since geometricfigures can often be faithfully described in concise textual form, convertingvisual content into captions offers a promising direction. Motivated by thisinsight, we introduce CapGeo, a caption-assisted reasoning framework thatbridges visual and textual modalities. Experiments show substantialimprovements when models are equipped with captions: Qwen2.5-VL-72B improvesfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to73.0%. To systematically evaluate and identify high-quality geometriccaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curatedfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-basedevaluation metric that correlates strongly with downstream CapGeo performance,enabling reliable assessment of geometric captioning ability. Together, ourframework and benchmark highlight a new pathway toward advancing geometricreasoning in MLLMs.</description>
      <author>example@mail.com (Yuying Li, Siyi Qian, Hao Liang, Leqi Zheng, Ruichuan An, Yongzhen Guo, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.09302v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.09266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CFVBench基准和自适应视觉细化(AVR)框架，解决了多模态检索增强生成模型在捕捉细粒度多模态细节方面的瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;多模态检索增强生成(MRAG)使多模态大语言模型能够利用外部多模态证据生成响应，但现有基准在模态覆盖和格式多样性方面有限，常专注于单模态任务或粗粒度场景理解。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准在模态覆盖和格式多样性方面的局限性，引入一个大规模、人工验证的基准来评估模型在检索和生成阶段的能力。&lt;h4&gt;方法&lt;/h4&gt;构建CFVBench基准，包含599个公开视频产生5,360个开放式问答对，涵盖图表密集报告、新闻广播和软件教程等多种格式和领域；评估7种检索方法和14种MLLMs；提出自适应视觉细化(AVR)框架，自适应增加帧采样密度并选择性调用外部工具。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型(即使是GPT-4或Gemini)难以捕捉短暂但重要的细粒度多模态细节；AVR框架能够增强细粒度多模态理解，提高所有评估的MLLMs的性能。&lt;h4&gt;结论&lt;/h4&gt;AVR是一种简单而有效的框架，可以解决当前模型在捕捉细粒度多模态细节方面的瓶颈，提升多模态检索增强生成模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态检索增强生成(MRAG)使多模态大语言模型能够利用外部多模态证据生成响应，许多基于视频的MRAG基准已被提出以评估模型在检索和生成阶段的能力。然而，现有基准在模态覆盖和格式多样性方面仍然有限，常专注于单模态任务或粗粒度场景理解。为解决这些差距，我们引入了CFVBench，这是一个从599个公开视频中构建的大规模、人工验证的基准，产生5,360个开放式问答对。CFVBench涵盖图表密集报告、新闻广播和软件教程等高密度格式和领域，要求模型检索和推理长时间视频跨度，同时保持细粒度多模态信息。使用CFVBench，我们系统评估了7种检索方法和14种广泛使用的MLLMs，揭示了一个关键瓶颈：当前模型(即使是GPT-4或Gemini)难以捕捉短暂但重要的细粒度多模态细节。为缓解这一问题，我们提出了自适应视觉细化(AVR)，这是一个简单而有效的框架，自适应增加帧采样密度并在必要时选择性调用外部工具。实验表明，AVR一致地增强细粒度多模态理解，并提高所有评估的MLLMs的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal LargeLanguage Models (MLLMs) to generate responses with external multimodalevidence, and numerous video-based MRAG benchmarks have been proposed toevaluate model capabilities across retrieval and generation stages. However,existing benchmarks remain limited in modality coverage and format diversity,often focusing on single- or limited-modality tasks, or coarse-grained sceneunderstanding. To address these gaps, we introduce CFVBench, a large-scale,manually verified benchmark constructed from 599 publicly available videos,yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats anddomains such as chart-heavy reports, news broadcasts, and software tutorials,requiring models to retrieve and reason over long temporal video spans whilemaintaining fine-grained multimodal information. Using CFVBench, wesystematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealinga critical bottleneck: current models (even GPT5 or Gemini) struggle to capturetransient yet essential fine-grained multimodal details. To mitigate this, wepropose Adaptive Visual Refinement (AVR), a simple yet effective framework thatadaptively increases frame sampling density and selectively invokes externaltools when necessary. Experiments show that AVR consistently enhancesfine-grained multimodal comprehension and improves performance across allevaluated MLLMs</description>
      <author>example@mail.com (Kaiwen Wei, Xiao Liu, Jie Zhang, Zijian Wang, Ruida Liu, Yuming Yang, Xin Xiao, Xiao Sun, Haoyang Zeng, Changzai Pan, Yidan Zhang, Jiang Zhong, Peijin Wang, Yingchao Feng)</author>
      <guid isPermaLink="false">2510.09266v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities</title>
      <link>http://arxiv.org/abs/2510.08759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了BEAR基准测试，用于全面评估多模态大语言模型(MLLMs)的具身能力，并提出了BEAR-Agent模型来提升这些能力。研究显示现有MLLMs在具身能力方面存在局限，而新模型能显著改善性能。&lt;h4&gt;背景&lt;/h4&gt;具身能力是指代理感知、理解和与物理世界交互的基本能力。尽管MLLMs作为具身代理有潜力，但现有基准测试主要关注特定领域，缺乏对MLLMs具身能力的全面系统性评估。&lt;h4&gt;目的&lt;/h4&gt;弥补对MLLMs具身能力评估的空白，引入一个全面且细致的基准测试来评估MLLMs在基本具身能力方面的表现。&lt;h4&gt;方法&lt;/h4&gt;提出BEAR基准测试，包含6个类别14个领域的4469个交错图像-视频-文本条目，涵盖从低级指向、轨迹理解到高级规划的任务；评估20个代表性MLLMs；提出BEAR-Agent，一个整合预训练视觉模型的多模态可对话代理，以增强MLLMs的感知、3D理解和规划能力。&lt;h4&gt;主要发现&lt;/h4&gt;20个代表性MLLMs在所有具身能力领域都存在持续限制；BEAR-Agent显著提高了MLLMs在BEAR上的表现，在GPT-5上实现了9.12%的绝对增益和17.5%的相对改进；提高MLLMs的具身能力有利于模拟环境中的具身任务。&lt;h4&gt;结论&lt;/h4&gt;BEAR基准测试填补了对MLLMs具身能力系统评估的空白；BEAR-Agent模型能有效提升MLLMs在具身能力方面的表现，为具身AI研究提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;具身能力是指代理用于感知、理解和与物理世界交互的一系列基本能力。虽然多模态大语言模型(MLLMs)作为具身代理显示出潜力，但对它们具身能力的全面系统性评估仍然不足，因为现有基准测试主要关注规划或空间理解等特定领域。为了弥补这一差距，我们引入了BEAR，这是一个全面且细致的基准，用于评估MLLMs在基本具身能力方面的表现。BEAR包含6个类别14个领域中的4469个交错图像-视频-文本条目，包括从低级指向、轨迹理解、空间推理到高级规划的任务。对20个代表性MLLMs的广泛评估结果显示，它们在所有具身能力领域都存在持续的限制。为了解决这一不足，我们提出了BEAR-Agent，一个整合预训练视觉模型的多模态可对话代理，以增强MLLMs的感知、3D理解和规划能力。它在BEAR上显著提高了MLLMs在各种具身能力方面的表现，在GPT-5上实现了9.12%的绝对增益和17.5%的相对改进。此外，我们的实验表明，提高MLLMs的具身能力有利于模拟环境中的具身任务。项目网站：https://bear-official66.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)缺乏系统性的具身能力评估问题。这个问题很重要，因为具身能力是AI系统感知、理解和与物理世界交互的基础能力，对开发能在开放环境中有效工作的AI代理至关重要。现有评估基准主要关注特定领域，无法全面评估MLLMs的具身能力，限制了我们对它们潜力的理解和开发方向的指导。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有工作的局限性来设计BEAR基准。他们发现现有工作要么专注于特定领域(如指向、空间理解)，要么关注能力导向任务但未分解为逐步技能。作者从大规模具身家庭活动数据集(如BEHAVIOR-1K和ALFRED)中归纳分类，从人类认知过程中获取灵感，设计了将具身能力组织为6个类别和14个原子技能的基准。他们确实借鉴了现有工作，包括使用多种视觉模型和工具来增强BEAR-Agent，并参考了现有数据集和评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建全面基准(BEAR)系统评估MLLMs的具身能力，并基于评估结果开发增强型代理(BEAR-Agent)提升这些能力。BEAR基准包含4,469个交错图像-视频-文本条目，组织为6个类别和14个原子技能。BEAR-Agent是一个多模态可对话代理，通过对话与MLLM交互，提供工具增强视觉和空间能力，为不同类别提供特定模块(如对象检测、深度估计)，通过提供额外线索帮助模型生成更准确答案，最终提升MLLMs在具身任务中的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) BEAR基准首次系统评估MLLMs具身能力，将能力组织为6个类别和14个原子技能；2) 包含4,469个交错图像-视频-文本条目，首次将具身任务分解为面向技能的步骤；3) BEAR-Agent多模态可对话代理显著提升性能，GPT-5提升9.12%；4) 在模拟环境中也表现优异，提升超过20.17%。相比之前工作，BEAR首次提供全面细粒度评估，BEAR-Agent不仅提升离线评估能力，还改进实际任务执行，并提供详细失败分析揭示MLLMs瓶颈。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了BEAR基准首次系统评估多模态大语言模型的具身能力，并基于评估结果开发了BEAR-Agent，显著提升了这些能力在评估和实际任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied capabilities refer to a suite of fundamental abilities for an agentto perceive, comprehend, and interact with the physical world. While multimodallarge language models (MLLMs) show promise as embodied agents, a thorough andsystematic evaluation of their embodied capabilities remains underexplored, asexisting benchmarks primarily focus on specific domains such as planning orspatial understanding. To bridge this gap, we introduce BEAR, a comprehensiveand fine-grained benchmark that evaluates MLLMs on atomic embodiedcapabilities. BEAR comprises 4,469 interleaved image-video-text entries across14 domains in 6 categories, including tasks from low-level pointing, trajectoryunderstanding, spatial reasoning, to high-level planning. Extensive evaluationresults of 20 representative MLLMs reveal their persistent limitations acrossall domains of embodied capabilities. To tackle the shortfall, we proposeBEAR-Agent, a multimodal conversable agent that integrates pretrained visionmodels to strengthen MLLM perception, 3D understanding, and planningcapabilities. It substantially enhances MLLM performance across diverseembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relativeimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate thatimproving MLLM embodied capabilities can benefit embodied tasks in simulatedenvironments. Project website: https://bear-official66.github.io/</description>
      <author>example@mail.com (Yu Qi, Haibo Zhao, Ziyu Guo, Siyuan Ma, Ziyan Chen, Yaokun Han, Renrui Zhang, Zitiantao Lin, Shiji Xin, Yijian Huang, Kai Cheng, Peiheng Wang, Jiazheng Liu, Jiayi Zhang, Yizhe Zhu, Wenqing Wang, Yiran Qin, Xupeng Zhu, Haojie Huang, Lawson L. S. Wong)</author>
      <guid isPermaLink="false">2510.08759v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vincentgu2000.github.io/u0project/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了USIM数据集和U0模型，用于解决水下机器人面临的挑战，通过多任务视觉-语言-动作框架实现水下机器人的自主操作。&lt;h4&gt;背景&lt;/h4&gt;水下环境对机器人操作提出了独特挑战，包括复杂流体动力学、能见度有限和通信受限。虽然数据驱动方法已推动陆地机器人发展，但开发能自主执行多项任务的水下智能仍然困难，因为大规模高质量水下数据集稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，作者引入了USIM，这是一个基于模拟的水下机器人多任务视觉-语言-动作数据集。&lt;h4&gt;方法&lt;/h4&gt;USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9种不同场景中的20项任务。基于此数据集，作者提出了U0模型，通过多模态融合整合双目视觉和其他传感器模态，并采用基于卷积-注意力的感知焦点增强模块提高空间理解和移动操作能力。&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率；在具有挑战性的移动操作任务中，与基线方法相比，将到目标的距离减少了21.2%。&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展数据集构建、提高任务自主性和实现智能通用水下机器人的实际应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的可见性和受限的通信。尽管数据驱动方法已经推动了陆地机器人的具身智能发展，并使特定任务的水下自主机器人成为可能，但开发能够自主执行多项任务的水下智能仍然极具挑战性，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了USIM，这是一个基于模拟的水下机器人多任务视觉-语言-动作数据集。USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9种不同场景中的20项任务，范围从视觉导航到移动操作。基于此数据集，我们提出了U0，这是一个面向通用水下机器人的VLA模型，该模型通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知焦点增强模块（CAP）来提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展数据集构建、提高任务自主性和实现智能通用水下机器人的实际应用奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下机器人领域缺乏大规模高质量数据集的问题，导致难以开发能够自主执行多任务的通用水下智能。这个问题很重要，因为水下环境对人类操作极具挑战性，而海洋覆盖地球表面71%，开发自主水下机器人能极大扩展人类探索海洋的能力；同时，水下任务目前仍严重依赖人工远程操作，成本高且效率低，而真实水下环境收集数据既昂贵又有风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到水下环境的独特挑战（流体动力学、能见度限制、通信约束）和现有水下数据集的局限性，因此决定采用模拟环境来收集大规模数据。他们借鉴了室内具身智能领域的进展，如DROID、Open X-Embodiment等数据集和RT-2、GR00T N1.5等VLA模型，同时参考了Stonefish等水下模拟器。基于这些现有工作，作者设计了USIM数据集和U0模型，专门针对水下环境的特性进行优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建大规模模拟水下数据集和专门设计的视觉-语言-动作模型来解决水下机器人数据稀缺问题。整体流程包括：1) 使用Stonefish模拟器构建9种多样化水下场景；2) 在这些场景中收集20个任务的561K帧数据，形成USIM数据集；3) 基于Isaac-GR00T N1.5构建U0模型，整合多模态传感器数据融合和卷积-注意力感知焦点增强模块(CAP)；4) 通过离线评估和在线测试验证模型效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) USIM数据集：首个大规模多任务水下VLA数据集，覆盖9个场景中的20个任务；2) U0模型：专为水下机器人设计的VLA模型，整合多模态传感器融合和CAP模块；3) 可扩展的数据到任务框架。相比之前工作，USIM解决了现有水下数据集任务单一、多样性不足的问题；U0则针对水下环境的独特挑战进行了优化，特别是在空间理解和移动操作方面表现出色；整体框架实现了80%的任务成功率，比基线方法提升显著。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务视觉-语言-动作数据集USIM并提出专门针对水下环境的U0模型，为开发具有多任务自主能力的通用水下机器人奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs</title>
      <link>http://arxiv.org/abs/2510.08631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无监督的分布外(OOD)对象检测方法，通过深度神经网络特征空间中高斯混合模型参数的层次贝叶斯建模来提取认知不确定性，无需辅助数据或额外训练阶段即可实现显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;除了通过LiDAR点云的精确语义分割实现准确场景理解外，检测分布外(OOD)对象（即训练过程中未遇到的实例）对于防止将未知对象错误分配到已知类别至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督OOD检测方法中认知不确定性和偶然不确定性混淆的问题，避免将分布中的模糊区域错误分类为OOD。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于深度神经网络特征空间中高斯混合模型参数层次贝叶斯建模的无监督OOD检测方法，专门提取认知不确定性而非依赖预测熵。&lt;h4&gt;主要发现&lt;/h4&gt;在SemanticKITTI数据集上，该方法优于现有基于不确定性的方法，AUROC提高18%，AUPRC增加22%，FPR95降低36%（从76%降至40%）。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法无需辅助数据或额外训练阶段，即可有效区分已知和未知对象，显著提升了无监督OOD检测的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;除了通过LiDAR点云的精确语义分割实现准确场景理解外，检测分布外(OOD)对象（即在训练过程中未遇到的实例）对于防止将未知对象错误分配到已知类别至关重要。虽然监督式OOD检测方法依赖于辅助的OOD数据集，但无监督方法避免了这一需求，通常仅依赖于预测熵，即通过对集成或多个后验权重样本的平均获得的预测分布的熵。然而，这些方法经常将认知（模型）不确定性和偶然（数据）不确定性混淆，将分布中的模糊区域错误地分类为OOD。为解决这一问题，我们提出了一种无监督OOD检测方法，该方法采用基于深度神经网络特征空间中高斯混合模型参数层次贝叶斯建模的认知不确定性。无需辅助数据或额外训练阶段，我们的方法在SemanticKITTI数据集上优于现有的基于不确定性的方法，与先前工作中使用的预测熵方法相比，AUROC提高了18%，AUPRC增加了22%，FPR95降低了36%（从76%降至40%）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR语义分割中检测分布外(Out-of-Distribution, OOD)对象的问题，即那些在训练过程中未遇到的实例。这个问题在自动驾驶等安全关键应用中非常重要，因为现实世界环境中经常包含训练数据中未见过的对象，而深度模型往往对这些OOD对象做出过度自信的错误预测。有效的OOD检测可以防止系统将未知对象错误分类为已知类别，提高自主系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：监督方法需要辅助OOD数据集，而非监督方法通常依赖预测熵，但这种方法会将认识不确定性(模型不确定性)和偶然不确定性(数据不确定性)混为一谈，导致误判。作者借鉴了GMMSeg使用高斯混合模型(GMM)在特征空间中建模语义类的思想，以及分层贝叶斯不确定性建模方法。通过结合这些现有工作的优势，作者设计了一种基于认识不确定性的无监督OOD检测方法，能够在不需要额外数据或训练的情况下有效区分OOD样本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用从GMM参数的分层贝叶斯建模中获得的认识不确定性来进行OOD检测，避免传统预测熵方法将认识不确定性和偶然不确定性混合的问题。整体流程包括：1)使用深度神经网络提取LiDAR点云特征；2)在特征空间中使用类条件高斯混合模型建模每个语义类；3)通过分层贝叶斯方法对GMM参数进行建模；4)从参数后验分布中采样并计算认识不确定性；5)使用熵值作为不确定性的度量；6)将高不确定性像素识别为OOD样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于认识不确定性的无监督OOD检测方法；2)有效区分认识不确定性和偶然不确定性；3)不需要辅助数据或额外训练阶段；4)在SemanticKITTI数据集上实现了显著性能提升。相比之前的工作，与监督方法不同，它不需要辅助OOD数据；与传统预测熵方法不同，它能更好地区分两种不确定性；与MC Dropout和深度集合方法不同，它不需要重新训练网络；与原始GMMSeg相比，它增加了分层贝叶斯不确定性估计，提供了更强的检测能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于特征空间中高斯混合模型参数认识不确定性的无监督方法，有效解决了LiDAR语义分割中分布外对象检测问题，显著提高了检测准确性和系统安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In addition to accurate scene understanding through precise semanticsegmentation of LiDAR point clouds, detecting out-of-distribution (OOD)objects, instances not encountered during training, is essential to prevent theincorrect assignment of unknown objects to known classes. While supervised OODdetection methods depend on auxiliary OOD datasets, unsupervised methods avoidthis requirement but typically rely on predictive entropy, the entropy of thepredictive distribution obtained by averaging over an ensemble or multipleposterior weight samples. However, these methods often conflate epistemic(model) and aleatoric (data) uncertainties, misclassifying ambiguous indistribution regions as OOD. To address this issue, we present an unsupervisedOOD detection approach that employs epistemic uncertainty derived fromhierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters inthe feature space of a deep neural network. Without requiring auxiliary data oradditional training stages, our approach outperforms existing uncertainty-basedmethods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),compared to the predictive entropy approach used in prior works.</description>
      <author>example@mail.com (Hanieh Shojaei Miandashti, Claus Brenner)</author>
      <guid isPermaLink="false">2510.08631v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2510.08589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对比了传统CNN模型、零样本预训练的多模态大型语言模型(LLMs)和微调后的多模态LLMs在图像中人工文本叠加检测任务上的表现，发现LLMs在少量数据(少于1,000张图像)微调后可显著提升性能，达到36%的准确率提升，匹配或超过需要大量数据的CNN基线方法。&lt;h4&gt;背景&lt;/h4&gt;物体检测和理解领域受传统CNN模型(如ResNet和YOLO)和新兴多模态大型语言模型(LLMs)的共同推动发展。CNN模型在图像任务中仍然有效，而基于transformer的LLMs引入了动态上下文推理、语言引导提示和整体场景理解等新能力，但即用型LLMs在专业视觉任务中表现往往不佳。&lt;h4&gt;目的&lt;/h4&gt;通过对比微调的传统CNN、零样本预训练的多模态LLMs和微调的多模态LLMs，研究如何有效利用LLMs进行图像中人工文本叠加检测这一具有挑战性的任务，探索语言引导模型在最小监督下适应精确视觉理解的方法。&lt;h4&gt;方法&lt;/h4&gt;进行了全面的比较研究，探索了在少量数据(少于1,000张图像)上微调LLMs的方法，研究如何将语言引导模型适应精确视觉理解，并评估了这些方法在人工文本叠加检测任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs可以在非常有限的数据(少于1,000张图像)上进行有效微调，实现高达36%的准确率提升，匹配或超过基于CNN的基线方法，而这些方法通常需要数量级更多的数据。这表明基于LLM的方法在真实世界物体检测任务中具有高适应性和数据效率。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，通过少量数据微调，LLMs可以在专业视觉任务上取得优异表现，为在低资源视觉环境中应用多模态transformer提供了可行的指导。研究团队已将微调模型的代码公开在GitHub上，以支持该领域的持续进步。&lt;h4&gt;翻译&lt;/h4&gt;物体检测和理解领域正迅速发展，这既得益于传统基于CNN模型的进步，也得益于新兴的多模态大型语言模型(LLMs)的发展。虽然ResNet和YOLO等CNN模型在基于图像的任务中仍然非常有效，但最近的基于transformer的LLMs引入了动态上下文推理、语言引导提示和整体场景理解等新能力。然而，当即用型LLMs用于专业视觉任务时，其全部潜力仍未被充分开发，往往导致次优性能。在本工作中，我们在图像中人工文本叠加检测这一具有挑战性的任务上，对微调的传统CNN、零样本预训练的多模态LLMs和微调的多模态LLMs进行了全面比较。我们研究的一个关键贡献是证明了LLMs可以在非常有限的数据(少于1,000张图像)上进行有效微调，实现高达36%的准确率提升，匹配或超过通常需要数量级更多数据的基于CNN的基线方法。通过探索语言引导模型如何在最小监督下适应精确视觉理解，我们的研究为弥合视觉和语言的更广泛努力做出了贡献，为高效的跨模态学习策略提供了新的见解。这些研究结果突显了基于LLM的方法在真实世界物体检测任务中的适应性和数据效率，并提供了在低资源视觉环境中应用多模态transformer的可行指导。为了支持该领域的持续进步，我们已在GitHub上公开了用于微调模型的代码，以便未来改进和相关应用中的重用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of object detection and understanding is rapidly evolving, drivenby advances in both traditional CNN-based models and emerging multi-modal largelanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effectivefor image-based tasks, recent transformer-based LLMs introduce new capabilitiessuch as dynamic context reasoning, language-guided prompts, and holistic sceneunderstanding. However, when used out-of-the-box, the full potential of LLMsremains underexploited, often resulting in suboptimal performance onspecialized visual tasks. In this work, we conduct a comprehensive comparisonof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, andfine-tuned multi-modal LLMs on the challenging task of artificial text overlaydetection in images. A key contribution of our study is demonstrating that LLMscan be effectively fine-tuned on very limited data (fewer than 1,000 images) toachieve up to 36% accuracy improvement, matching or surpassing CNN-basedbaselines that typically require orders of magnitude more data. By exploringhow language-guided models can be adapted for precise visual understanding withminimal supervision, our work contributes to the broader effort of bridgingvision and language, offering novel insights into efficient cross-modallearning strategies. These findings highlight the adaptability and dataefficiency of LLM-based approaches for real-world object detection tasks andprovide actionable guidance for applying multi-modal transformers inlow-resource visual environments. To support continued progress in this area,we have made the code used to fine-tune the models available in our GitHub,enabling future improvements and reuse in related applications.</description>
      <author>example@mail.com (Nirmal Elamon, Rouzbeh Davoudi)</author>
      <guid isPermaLink="false">2510.08589v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</title>
      <link>http://arxiv.org/abs/2510.03342v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了Gemini Robotics模型家族的最新版本，包括Gemini Robotics 1.5和Gemini Robotics-ER1.5，通过三项创新提高了机器人的通用推理和任务执行能力。&lt;h4&gt;背景&lt;/h4&gt;通用机器人需要深入理解物理世界、高级推理能力和通用灵巧的控制能力。&lt;h4&gt;目的&lt;/h4&gt;介绍Gemini Robotics模型家族的最新版本，提高机器人解决复杂多步骤任务的能力。&lt;h4&gt;方法&lt;/h4&gt;三项主要创新：1) 采用新颖架构和运动转移机制，从异构多形态机器人数据中学习；2) 在自然语言中将动作与多级内部推理过程交错进行，实现'行动前思考'；3) 建立新的具身推理最先进水平，提升视觉空间理解、任务规划和进度估计能力。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini Robotics 1.5能够更好地分解和执行复杂多步骤任务，行为更具可解释性；Gemini Robotics-ER1.5在具身推理方面达到新水平。&lt;h4&gt;结论&lt;/h4&gt;这一系列模型使机器人能够感知、思考然后行动，朝着解决复杂多步骤任务的物理代理时代迈进。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人需要深入理解物理世界、高级推理能力和通用灵巧的控制。本报告介绍了Gemini Robotics模型家族的最新一代：Gemini Robotics 1.5，一个多形态视觉-语言-动作模型，以及Gemini Robotics-ER1.5，一个最先进的具身推理模型。我们带来了三大创新。首先，Gemini Robotics 1.5采用新颖架构和运动转移机制，使其能够从异构的多形态机器人数据中学习，使VLA更加通用。其次，Gemini Robotics 1.5在自然语言中将动作与多级内部推理过程交错进行，使机器人能够在行动前'思考'，显著提高其分解和执行复杂多步骤任务的能力，并使机器人的行为对用户更具可解释性。第三，Gemini Robotics-ER 1.5为具身推理建立了新的最先进水平，即对机器人至关重要的推理能力，如视觉和空间理解、任务规划和进度估计。总之，这一系列模型使我们迈向物理代理的新时代，使机器人能够感知、思考然后行动，从而解决复杂的多步骤任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发通用的机器人系统，使机器人能够深入理解物理世界、进行高级推理并执行灵活控制。这个问题非常重要，因为当前机器人大多只能执行特定任务，缺乏适应不同环境和任务的能力，而通用机器人可以大大扩展应用范围，从工业制造到家庭服务等多个领域，解决劳动力短缺和提高生活质量等问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到通用机器人需要三个核心能力：物理世界理解、高级推理和灵活控制。他们设计了一个包含两个主要模型的系统：Gemini Robotics 1.5(VLA模型)和Gemini Robotics-ER 1.5(ER模型)。作者借鉴了现有的Gemini模型基础架构、VLA模型范式和具身推理概念，同时引入了新的Motion Transfer机制和Thinking机制，使机器人能够在执行动作前进行思考，并实现不同机器人形态间的技能迁移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将机器人系统分为高级推理规划模块和低级执行模块，引入'Thinking'机制使机器人在执行前进行思考，并使用Motion Transfer技术实现跨形态技能迁移。整体流程是：用户输入任务→高级推理模块理解需求、制定计划并调用外部工具→低级执行模块接收指令、分解步骤、执行前思考并转化为具体动作→两个模块协同工作，形成完整智能体系统处理复杂多步骤任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Thinking VLA机制，提高复杂任务处理能力；2)Motion Transfer机制，实现跨形态机器人技能零样本迁移；3)高级具身推理能力，在多种推理任务上达到最先进性能；4)多形态通用机器人系统，单一模型控制多种不同机器人。相比之前工作，1.5版本引入思考机制提升复杂任务执行能力，能够处理多种机器人形态，在多步骤任务上有显著提升，并在保持通用能力的同时在具身推理任务上达到最先进水平。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Gemini Robotics 1.5通过引入思考机制、跨形态技能迁移和高级具身推理能力，显著提升了通用机器人在复杂物理世界中的感知、思考和行动能力，为实现真正的通用机器人系统提供了重要进展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose robots need a deep understanding of the physical world,advanced reasoning, and general and dexterous control. This report introducesthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing togetherthree major innovations. First, Gemini Robotics 1.5 features a novelarchitecture and a Motion Transfer (MT) mechanism, which enables it to learnfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.Second, Gemini Robotics 1.5 interleaves actions with a multi-level internalreasoning process in natural language. This enables the robot to "think beforeacting" and notably improves its ability to decompose and execute complex,multi-step tasks, and also makes the robot's behavior more interpretable to theuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art forembodied reasoning, i.e., for reasoning capabilities that are critical forrobots, such as visual and spatial understanding, task planning, and progressestimation. Together, this family of models takes us a step towards an era ofphysical agents-enabling robots to perceive, think and then act so they cansolve complex multi-step tasks.</description>
      <author>example@mail.com (Gemini Robotics Team, Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Amaris Paryag, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Peter Pastor Sampedro, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou, Yuxiang Zhou)</author>
      <guid isPermaLink="false">2510.03342v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</title>
      <link>http://arxiv.org/abs/2510.11107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025, project page:  https://jiahuilei.com/projects/momap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的像素对齐运动图(MoMap)表示方法，用于从真实世界视频中学习语义和功能上有意义的3D运动先验，实现从单图像预测未来3D场景运动。&lt;h4&gt;背景&lt;/h4&gt;从真实世界视频中学习语义和功能上有意义的3D运动先验是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;能够从单个输入图像预测未来的3D场景运动。&lt;h4&gt;方法&lt;/h4&gt;提出像素对齐的运动图(MoMap)表示方法，从超过50,000个真实视频中创建大规模MoMap数据库，并训练扩散模型；运动生成流程包括先生成MoMap，然后扭曲图像并完成扭曲的点基渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够生成合理且语义一致的3D场景运动。&lt;h4&gt;结论&lt;/h4&gt;通过MoMap表示和扩散模型训练，可以有效地从真实视频中学习3D运动先验，并实现从单图像预测未来3D场景运动。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了从真实世界视频中学习语义和功能上有意义的3D运动先验的挑战，目的是能够从单个输入图像预测未来的3D场景运动。我们提出了一种新颖的像素对齐的运动图(MoMap)表示方法用于3D场景运动，可以从现有的生成图像模型生成，以促进高效和有效的运动预测。为了学习有意义的运动分布，我们从超过50,000个真实视频中创建了大规模的MoMap数据库，并基于这些表示训练了一个扩散模型。我们的运动生成不仅在3D中合成轨迹，还为2D视频合成提出了新流程：先生成一个MoMap，然后相应地扭曲图像，并完成扭曲的点基渲染。实验结果表明，我们的方法能够生成合理且语义一致的3D场景运动。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从真实世界视频中学习语义上和功能上有意义的3D运动先验知识，以便从单个输入图像预测未来3D场景运动。这个问题在计算机视觉领域非常重要，因为理解、重建和预测物体在3D空间中的运动对于增强现实、自动驾驶和机器人等与物理环境交互的应用至关重要。现有方法要么局限于2D视频生成，要么只能处理小规模的短轨迹，缺乏大规模学习3D生成运动先验的有效方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到需要一种适合神经网络处理的3D场景运动表示方式，避免2D轨迹中常见的遮挡问题。他们受到近期重用图像扩散模型进行其他任务（如深度预测）的启发，提出像素对齐的Motion Map表示。设计过程中借鉴了多个现有工作：利用4D重建技术（MoSca）处理真实视频，采用视频深度估计（DepthCrafter）和3D点跟踪（SpaTracker）技术，并应用视频对象分割（DEVA）获取语义信息。核心创新在于巧妙地将强大的预训练图像扩散模型（如Stable Diffusion）重用于3D运动预测任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景运动表示为像素对齐的Motion Map（MoMap），这种'轨迹图像'保留了图像结构但编码了3D位置信息，使能利用大型预训练图像扩散模型进行运动预测。整体流程分为四步：1)数据准备：从真实视频中提取深度、跟踪3D点、优化轨迹并生成MoMap；2)MoMap压缩：将高维运动数据编码为紧凑的潜在特征；3)MoMap扩散：修改预训练U-Net生成未来运动；4)应用：通过渲染和图像完成生成视频，或使用视觉语言模型进行精细控制。这种方法解耦了相机和物体运动，减少了问题复杂度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出MoMap这一像素对齐的3D运动表示；2)构建了从5万+真实视频提取的大规模MoMap数据库；3)重用预训练图像扩散模型进行3D运动预测；4)提出先生成MoMap再完成视频的新范式；5)引入DSL语言实现VLM对运动的精细控制。相比之前工作，不同之处在于：专注于长期密集的3D运动而非短轨迹；直接学习3D轨迹而非通过像素变化隐式学习运动；使用真实世界大规模数据而非合成数据；利用预训练模型而非从头训练；实现了相机与物体运动的解耦。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为MoMap的像素对齐3D运动表示方法，利用大型预训练图像扩散模型从真实世界视频中学习语义上有意义的3D运动先验，实现了从单张输入图像预测未来3D场景运动，并展示了其在视频生成等应用中的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of learning semantically and functionallymeaningful 3D motion priors from real-world videos, in order to enableprediction of future 3D scene motion from a single input image. We propose anovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,which can be generated from existing generative image models to facilitateefficient and effective motion prediction. To learn meaningful distributionsover motion, we create a large-scale database of MoMaps from over 50,000 realvideos and train a diffusion model on these representations. Our motiongeneration not only synthesizes trajectories in 3D but also suggests a newpipeline for 2D video synthesis: first generate a MoMap, then warp an imageaccordingly and complete the warped point-based renderings. Experimentalresults demonstrate that our approach generates plausible and semanticallyconsistent 3D scene motion.</description>
      <author>example@mail.com (Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas)</author>
      <guid isPermaLink="false">2510.11107v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Seeing My Future: Predicting Situated Interaction Behavior in Virtual Reality</title>
      <link>http://arxiv.org/abs/2510.10742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://xy02-05.github.io/Seeing_My_Future/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层的、意图感知框架，用于在虚拟和增强现实系统中预测人类行为，通过理解人类意图和利用认知机制，实现了对未来情境行为的准确预测，并在实验中取得了优越性能。&lt;h4&gt;背景&lt;/h4&gt;虚拟和增强现实系统需要智能适应用户行为以增强交互体验。准确理解人类意图并预测未来的情境行为（如视线方向和物体交互）对创建响应式VR/AR环境和应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够对人类意图建模并预测详细情境行为的框架，以实现VR/AR系统对用户行为的智能适应，从而创建更响应式的交互环境。&lt;h4&gt;方法&lt;/h4&gt;引入了一个分层的、意图感知框架，利用认知机制对人类意图建模并预测情境行为。该框架基于历史人类动态和场景上下文观察，识别潜在交互目标并预测细粒度未来行为。采用动态图卷积网络(GCN)来捕捉人-环境关系。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的真实世界基准测试和实时VR环境上的实验表明，该方法在所有指标上都取得了优越性能，能够实现主动VR系统的实际应用，这些系统能够预测用户行为并相应调整虚拟环境。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了VR/AR系统中智能适应的关键挑战，通过理解人类意图和预测未来行为，使系统能够主动调整以提供更好的用户体验，为未来VR/AR应用的发展提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;虚拟和增强现实系统日益需要智能适应用户行为以增强交互体验。实现这一点需要准确理解人类意图并预测未来的情境行为——如视线方向和物体交互——这对于创建响应式的VR/AR环境和个性化助手等应用至关重要。然而，准确的行为预测需要对驱动人-环境交互的潜在认知过程进行建模。在本工作中，我们引入了一个分层的、意图感知框架，通过利用认知机制对人类意图建模并预测详细的情境行为。给定历史人类动态和场景上下文观察，我们的框架首先识别潜在的交互目标并预测细粒度的未来行为。我们提出了一种动态图卷积网络(GCN)来有效捕捉人-环境关系。在具有挑战性的真实世界基准测试和实时VR环境上的大量实验证明了我们方法的有效性，在所有指标上均取得了优越性能，并实现了主动VR系统的实际应用，这些系统能够预测用户行为并相应地调整虚拟环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在虚拟现实(VR)环境中准确预测用户的情境化交互行为问题，包括用户视线方向、移动轨迹和物体交互。这个问题很重要，因为VR/AR系统需要智能适应用户行为以增强交互体验，准确预测用户行为能创建响应式环境，支持个性化助手、游戏环境调整和人机协作等应用，使虚拟环境能主动适应而非被动响应人类行为。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从认知科学研究中获取灵感，注意到人类通常会先形成交互意图再执行具体动作，视线在交互意图形成中起关键作用。方法设计借鉴了现有工作：1)视线-身体相关性研究，利用视线信息提高预测准确性；2)视线预测技术，同时预测视线、轨迹和物体交互；3)物体交互预测方法，但创新性地采用符合人类认知的分层框架，先预测潜在目标再预测详细行为。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个分层、意图感知框架，模拟人类认知过程，先预测潜在交互目标再预测详细行为，使用动态图卷积网络捕获人-环境关系。整体流程：1)观察编码模块：将历史人类状态和场景上下文编码为时空特征；2)分层意图感知解码模块：先预测潜在交互目标的交互概率，再解码人类和物体的下一个状态；3)动态GCN模块：使用自适应权重矩阵建模视线、人体特征与物体间关系；4)多任务训练：使用多个损失函数监督所有预测输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)分层意图感知框架，首次模拟人类认知过程预测交互行为；2)动态GCN设计，通过自适应权重矩阵有效捕获人-环境关系；3)多任务预测，同时预测视线、轨迹和物体交互。相比之前工作的不同：现有方法如Pose2Gaze缺乏环境上下文，SIF3D等虽利用环境信息但缺乏人-环境关系建模，本文方法在所有指标上表现优越，特别是在识别下一个活跃物体方面有显著优势，且在真实VR环境中验证了实用性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种受认知科学启发的分层意图感知框架，通过动态图卷积网络建模人-环境关系，实现了在VR环境中对用户视线、移动轨迹和物体交互的准确预测，为主动式VR系统提供了新的技术基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual and augmented reality systems increasingly demand intelligentadaptation to user behaviors for enhanced interaction experiences. Achievingthis requires accurately understanding human intentions and predicting futuresituated behaviors - such as gaze direction and object interactions - which isvital for creating responsive VR/AR environments and applications likepersonalized assistants. However, accurate behavioral prediction demandsmodeling the underlying cognitive processes that drive human-environmentinteractions. In this work, we introduce a hierarchical, intention-awareframework that models human intentions and predicts detailed situated behaviorsby leveraging cognitive mechanisms. Given historical human dynamics and theobservation of scene contexts, our framework first identifies potentialinteraction targets and forecasts fine-grained future behaviors. We propose adynamic Graph Convolutional Network (GCN) to effectively capturehuman-environment relationships. Extensive experiments on challengingreal-world benchmarks and live VR environment demonstrate the effectiveness ofour approach, achieving superior performance across all metrics and enablingpractical applications for proactive VR systems that anticipate user behaviorsand adapt virtual environments accordingly.</description>
      <author>example@mail.com (Yuan Xu, Zimu Zhang, Xiaoxuan Ma, Wentao Zhu, Yu Qiao, Yizhou Wang)</author>
      <guid isPermaLink="false">2510.10742v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning</title>
      <link>http://arxiv.org/abs/2510.10451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度强化学习和反事实模拟的数据驱动多动物行为模拟器，解决了在生物学中实现真实多动物模拟的关键挑战，能够高保真地复现物种特异性行为，支持反事实行为预测和多个体建模。&lt;h4&gt;背景&lt;/h4&gt;动物运动模拟器在行为研究中扮演重要角色。模仿学习在机器人学中的进步为复制人类和动物运动提供了新的可能性。然而，在生物学中实现真实的多动物模拟面临关键挑战：弥合未知现实世界转换模型与其模拟对应物之间的差距。由于运动动态很少是已知的，仅依靠数学模型是不够的。&lt;h4&gt;目的&lt;/h4&gt;构建一个既能复现真实轨迹又支持奖励驱动优化的模拟器，解决高自由度运动引起的病态问题，实现物种特异性行为的准确复现，支持反事实行为预测和多个体建模。&lt;h4&gt;方法&lt;/h4&gt;研究团队基于深度强化学习和反事实模拟构建了数据驱动模拟器。他们通过在强化学习框架中将不完整转换模型的运动变量估计为动作，来解决运动中高自由度引起的病态问题。此外，他们使用基于距离的伪奖励来对齐和比较赛博空间与物理空间之间的状态。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在人工代理、苍蝇、蝾螈和家蚕上得到了验证，与标准模仿和强化学习方法相比，实现了更高的物种特异性行为可再现性和改进的奖励获取。此外，它支持在新实验环境中的反事实行为预测，并支持多个个体的建模以实现灵活的假设轨迹生成。&lt;h4&gt;结论&lt;/h4&gt;该数据驱动的多动物行为模拟器能够有效模拟和阐明复杂的多动物行为，为生物学研究提供了强大的工具，具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;动物运动模拟器在行为研究中扮演着重要角色。模仿学习在机器人学中的进步为复制人类和动物运动提供了新的可能性。在生物学中实现真实多动物模拟的一个关键挑战是弥合未知现实世界转换模型与其模拟对应物之间的差距。由于运动动态很少是已知的，仅依靠数学模型是不够的；构建一个既能复现真实轨迹又支持奖励驱动优化的模拟器仍然是一个开放问题。我们介绍了一种基于深度强化学习和反事实模拟的多动物行为数据驱动模拟器。我们通过在强化学习框架中将不完整转换模型的运动变量估计为动作，解决了运动中高自由度引起的病态问题。我们还使用基于距离的伪奖励来对齐和比较赛博空间与物理空间之间的状态。在人工代理、苍蝇、蝾螈和家蚕上的验证表明，与标准模仿和强化学习方法相比，我们的方法实现了更高的物种特异性行为可再现性和改进的奖励获取。此外，它支持在新实验环境中的反事实行为预测，并支持多个个体的建模以实现灵活的假设轨迹生成，表明其模拟和阐明复杂多动物行为的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulators of animal movements play a valuable role in studying behavior.Advances in imitation learning for robotics have expanded possibilities forreproducing human and animal movements. A key challenge for realisticmulti-animal simulation in biology is bridging the gap between unknownreal-world transition models and their simulated counterparts. Becauselocomotion dynamics are seldom known, relying solely on mathematical models isinsufficient; constructing a simulator that both reproduces real trajectoriesand supports reward-driven optimization remains an open problem. We introduce adata-driven simulator for multi-animal behavior based on deep reinforcementlearning and counterfactual simulation. We address the ill-posed nature of theproblem caused by high degrees of freedom in locomotion by estimating movementvariables of an incomplete transition model as actions within an RL framework.We also employ a distance-based pseudo-reward to align and compare statesbetween cyber and physical spaces. Validated on artificial agents, flies,newts, and silkmoth, our approach achieves higher reproducibility ofspecies-specific behaviors and improved reward acquisition compared withstandard imitation and RL methods. Moreover, it enables counterfactual behaviorprediction in novel experimental settings and supports multi-individualmodeling for flexible what-if trajectory generation, suggesting its potentialto simulate and elucidate complex multi-animal behaviors.</description>
      <author>example@mail.com (Keisuke Fujii, Kazushi Tsutsui, Yu Teshima, Makoto Itoh, Naoya Takeishi, Nozomi Nishiumi, Ryoya Tanaka, Shunsuke Shigaki, Yoshinobu Kawahara)</author>
      <guid isPermaLink="false">2510.10451v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?</title>
      <link>http://arxiv.org/abs/2510.10254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了大型视觉模型（LVM）在医学影像任务中的零样本学习能力，发现即使没有医学数据训练，该模型也能在器官分割、去噪、超分辨率和运动预测等任务上实现具有竞争力的性能，特别是在放射治疗运动预测中表现优异。&lt;h4&gt;背景&lt;/h4&gt;最近大型生成模型的发展表明，适当扩展的自回归公式可以在不同领域表现出强大的零样本泛化能力。这一趋势启发研究者探索自回归视频建模原则在医学影像领域的应用潜力。&lt;h4&gt;目的&lt;/h4&gt;研究旨在验证自回归视频建模原则是否可以直接应用于医学影像任务，评估大型视觉模型在从未接触过医学数据的情况下，在多种医学影像任务上的零样本性能。&lt;h4&gt;方法&lt;/h4&gt;研究评估了一个大型视觉模型（LVM）在四个代表性医学影像任务上的零样本性能：器官分割、去噪、超分辨率和运动预测。研究使用了来自122名患者的4D CT数据，总计超过1,820个3D CT体积进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 即使没有领域特定的微调，LVM也能在CT扫描中勾勒出解剖结构；2. LVM在分割、去噪和超分辨率任务上实现了具有竞争力的性能；3. 在放射治疗运动预测中，LVM能够直接从前4D CT扫描的前期阶段预测未来的3D CT阶段；4. 预测结果解剖上一致，能够以真实的时间连贯性捕捉患者特定的呼吸动力学；5. LVM在运动预测任务上超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。&lt;h4&gt;结论&lt;/h4&gt;这些发现揭示了医学视频建模中零样本能力的出现，突显了通用视频模型作为统一学习者和推理者的潜力，为建立在视频模型上的未来医学基础模型奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;最近大型生成模型的进展表明，适当地扩展简单的自回归公式可以在不同领域表现出强大的零样本泛化能力。受这一趋势启发，我们研究是否可以将自回归视频建模原则直接应用于医学影像任务，尽管该模型从未在医学数据上训练过。具体来说，我们在四个代表性任务上评估了一个大型视觉模型（LVM）的零样本性能：器官分割、去噪、超分辨率和运动预测。值得注意的是，即使没有领域特定的微调，LVM也能在CT扫描中勾勒出解剖结构，并在分割、去噪和超分辨率任务上实现具有竞争力的性能。最显著的是，在放射治疗运动预测中，该模型直接从前4D CT扫描的前期阶段预测未来的3D CT阶段，产生解剖上一致的预测，能够以真实的时间连贯性捕捉患者特定的呼吸动力学。我们在122名患者的4D CT数据上评估了LVM，总计超过1,820个3D CT体积。尽管没有预先接触医学数据，该模型在所有任务上都实现了强大的性能，并在运动预测方面超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。这些发现揭示了医学视频建模中零样本能力的出现，并突显了通用视频模型作为统一学习者和推理者的潜力，为建立在视频模型上的未来医学基础模型奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文探讨大型视频模型能否在未经医疗数据训练的情况下，直接应用于医疗影像任务并表现出色。这个问题很重要，因为传统医疗AI系统需要针对特定任务（如分割、去噪）专门训练，成本高昂且系统碎片化。如果通用视频模型能零样本应用于医疗领域，将大大降低医疗AI开发成本，提高系统通用性和适应性，使医疗AI更可扩展和实用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受大型语言模型展现的跨领域零样本泛化能力启发，思考自回归视频建模原则是否可直接应用于医疗影像。他们选择了大型视觉模型（LVM）作为基础，该模型在大规模自然图像和视频上训练，能通过提示适应不同视觉任务。作者借鉴了LLMs和VLMs的统一框架思想、自回归视频建模方法（如VQGAN和Transformer架构）、医疗分割框架（如nnUNet）和变形矢量场方法，创新性地将这些技术应用于医疗领域，特别是放疗运动预测任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是大型视频模型通过自回归学习时空表示，即使未经医疗数据训练，也能通过零样本学习在医疗影像任务上取得有竞争力表现。整体流程：1) 预处理：用nnUNet分割4D CT序列获取器官掩码；2) CT分词化：用VQGAN将CT图像编码为离散令牌序列；3) 序列建模：使用单向Transformer对CT阶段序列进行自回归建模，预测未来阶段；4) 评估：在分割、去噪、超分辨率和运动预测任务上评估性能，使用IoU、DSC等指标和定性可视化验证结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首次证明大型视频模型可在无医疗训练数据下直接应用于多种医疗任务；2) 展示单一模型可处理分割、去噪、超分辨率和运动预测等多种任务，无需任务特定重训练；3) 在放疗运动预测上超越专门方法，能准确预测未来CT阶段；4) 整合CT图像和分割掩码提高运动建模准确性。相比之前工作：传统医疗AI需针对每任务设计专门模型，而本文使用通用模型处理多任务；现有医疗基础模型多局限于静态影像，本文专注于动态医疗数据；传统方法依赖预计算的DVF，本文直接从图像序列学习运动模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次证明大型视频模型可以在未经医疗数据训练的情况下，通过零样本学习直接应用于多种医疗影像任务，特别是在放疗运动预测上超越了专门方法，为构建统一的医疗影像基础模型铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large generative models have shown that simpleautoregressive formulations, when scaled appropriately, can exhibit strongzero-shot generalization across domains. Motivated by this trend, weinvestigate whether autoregressive video modeling principles can be directlyapplied to medical imaging tasks, despite the model never being trained onmedical data. Specifically, we evaluate a large vision model (LVM) in azero-shot setting across four representative tasks: organ segmentation,denoising, super-resolution, and motion prediction. Remarkably, even withoutdomain-specific fine-tuning, the LVM can delineate anatomical structures in CTscans and achieve competitive performance on segmentation, denoising, andsuper-resolution. Most notably, in radiotherapy motion prediction, the modelforecasts future 3D CT phases directly from prior phases of a 4D CT scan,producing anatomically consistent predictions that capture patient-specificrespiratory dynamics with realistic temporal coherence. We evaluate the LVM on4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite noprior exposure to medical data, the model achieves strong performance acrossall tasks and surpasses specialized DVF-based and generative baselines inmotion prediction, achieving state-of-the-art spatial accuracy. These findingsreveal the emergence of zero-shot capabilities in medical video modeling andhighlight the potential of general-purpose video models to serve as unifiedlearners and reasoners laying the groundwork for future medical foundationmodels built on video models.</description>
      <author>example@mail.com (Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2510.10254v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</title>
      <link>http://arxiv.org/abs/2510.11606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Data &amp; Code: https://github.com/OpenGVLab/ExpVid&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了ExpVid基准测试，用于系统评估多模态大语言模型在科学实验视频上的能力，发现现有模型在细粒度感知、状态变化跟踪和科学推理方面存在明显不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型有望加速科学发现，但现有基准测试忽视了真实实验室工作的细粒度和长期特性，特别是在湿实验室环境中，导致对其真实能力的理解不足。&lt;h4&gt;目的&lt;/h4&gt;弥补这一差距，引入ExpVid基准测试，系统性评估MLLMs在科学实验视频上的能力。&lt;h4&gt;方法&lt;/h4&gt;ExpVid从同行评审的视频出版物中策划，包含三级任务层次结构：细粒度感知、程序理解和科学推理。采用视觉为中心的注释流程，结合自动生成和多学科专家验证，确保任务需要视觉基础。&lt;h4&gt;主要发现&lt;/h4&gt;评估19个领先MLLMs后发现，它们擅长粗粒度识别，但在区分细粒度细节、跟踪状态变化和将实验程序与科学结果联系方面存在困难。专有模型和开源模型间存在明显性能差距，特别是在高阶推理方面。&lt;h4&gt;结论&lt;/h4&gt;ExpVid不仅提供了诊断工具，还为开发能够成为科学实验可信伙伴的MLLMs绘制了路线图。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型（MLLMs）有望通过解释复杂的实验流程来加速科学发现。然而，它们真实的能力未被充分理解，因为现有的基准测试忽视了真实实验室工作的细粒度和长期特性，特别是在湿实验室环境中。为了弥补这一差距，我们引入了ExpVid，这是第一个旨在系统性评估MLLMs在科学实验视频上的基准测试。从同行评审的视频出版物中策划，ExpVid具有一个新的三级任务层次结构，反映科学过程：（1）对工具、材料和动作的细粒度感知；（2）对步骤顺序和完整性的程序理解；（3）将整个实验与其已发表结论联系起来的科学推理。我们的视觉为中心的注释流程，结合自动生成和多学科专家验证，确保任务需要视觉基础。我们在ExpVid上评估了19个领先的MLLMs，发现虽然它们擅长粗粒度识别，但在区分细粒度细节、跟踪随时间变化的状态以及将实验程序与科学结果联系起来方面存在困难。我们的结果揭示了专有模型和开源模型之间的显著性能差距，特别是在高阶推理方面。ExpVid不仅提供了诊断工具，还为开发能够成为科学实验可信伙伴的MLLMs绘制了路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) hold promise for acceleratingscientific discovery by interpreting complex experimental procedures. However,their true capabilities are poorly understood, as existing benchmarks neglectthe fine-grained and long-horizon nature of authentic laboratory work,especially in wet-lab settings. To bridge this gap, we introduce ExpVid, thefirst benchmark designed to systematically evaluate MLLMs on scientificexperiment videos. Curated from peer-reviewed video publications, ExpVidfeatures a new three-level task hierarchy that mirrors the scientific process:(1) Fine-grained Perception of tools, materials, and actions; (2) ProceduralUnderstanding of step order and completeness; and (3) Scientific Reasoning thatconnects the full experiment to its published conclusions. Our vision-centricannotation pipeline, combining automated generation with multi-disciplinaryexpert validation, ensures that tasks require visual grounding. We evaluate 19leading MLLMs on ExpVid and find that while they excel at coarse-grainedrecognition, they struggle with disambiguating fine details, tracking statechanges over time, and linking experimental procedures to scientific outcomes.Our results reveal a notable performance gap between proprietary andopen-source models, particularly in high-order reasoning. ExpVid not onlyprovides a diagnostic tool but also charts a roadmap for developing MLLMscapable of becoming trustworthy partners in scientific experimentation.</description>
      <author>example@mail.com (Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang)</author>
      <guid isPermaLink="false">2510.11606v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?</title>
      <link>http://arxiv.org/abs/2510.11549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ODI-Bench基准测试和Omni-CoT方法，用于提升多模态大语言模型对全景图像的理解能力。&lt;h4&gt;背景&lt;/h4&gt;全景图像提供360度全方位视角，广泛应用于VR、AR和具身智能，但多模态大语言模型对这类沉浸式环境的理解能力尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补多模态大语言模型在全景图像理解方面的研究空白，提供专门的基准测试和改进方法。&lt;h4&gt;方法&lt;/h4&gt;创建包含2000张全景图像和4000多个问答对的ODI-Bench基准，测试20个代表性MLLMs；提出Omni-CoT方法，通过思维链推理增强模型理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;当前MLLMs难以捕捉全景图像提供的沉浸式上下文信息。&lt;h4&gt;结论&lt;/h4&gt;研究将发布ODI-Bench基准测试和Omni-CoT代码，促进全景图像理解领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;全景图像(ODIs)提供360x180度的全方位视角，广泛应用于VR、AR和具身智能应用。虽然多模态大语言模型(MLLMs)在传统2D图像和视频理解基准测试上表现出色，但它们对ODIs捕捉的沉浸式环境的理解能力仍 largely未被探索。为解决这一差距，我们首先提出了ODI-Bench，这是一个专为全景图像理解设计的新型综合基准测试。ODI-Bench包含2000张高质量全景图像和4000多个人工标注的问答对，涵盖10个细粒度任务，包括一般层面和空间层面的全景图像理解。我们在封闭式和开放式两种设置下对20个代表性MLLMs进行了广泛测试，包括专有和开源模型。实验结果表明，当前MLLMs仍然难以捕捉全景图像提供的沉浸式上下文。为此，我们进一步引入了Omni-CoT，这是一种无需训练的方法，通过在文本信息和视觉线索之间进行思维链推理，显著增强MLLMs在全景环境中的理解能力。基准测试和代码将在发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决评估多模态大语言模型(MLLMs)对全方向图像(ODI)的理解能力问题。ODIs提供360°全景视野，广泛应用于VR、AR和具身智能等领域，但MLLMs对这类沉浸式环境的理解能力尚未被充分探索。这个问题很重要，因为ODIs包含比传统2D图像更丰富的空间信息，需要更高级的空间推理能力，对推进具身智能和交互式多模态系统发展至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有ODI基准的局限性(分辨率低、场景多样性有限、问题领域受限、视角限制)来设计方法。他们结合了自动化管道和人工标注两种QA构建方式，借鉴了传统2D图像理解任务设计了5个通用级任务，同时参考空间理解任务设计了5个空间级任务。作者还借鉴了链式思维(Chain-of-Thought)推理方法，设计了Omni-CoT框架来增强模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全面基准(ODI-Bench)评估MLLMs对全方向图像的理解，并通过Omni-CoT框架提升其理解能力。ODI-Bench包含2000张高质量ODIs和4000+问答对，涵盖10个细粒度任务。Omni-CoT框架包含三步：1)视角引导回答(将ODI投影为六个视角并生成描述)；2)裁剪线索的定位和细化(识别相关图像区域并过滤)；3)回答细化(结合视觉线索重新思考答案)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的ODI-Bench基准，同时评估通用级和空间级理解；2)高分辨率(&gt;8K)和多样化场景(室内+室外)的图像集；3)细粒度任务设计(10个任务涵盖属性识别、计数、方向判断等)；4)Omni-CoT训练增强框架显著提升模型性能。相比之前工作，ODI-Bench分辨率更高、场景更多样、任务更全面、标注质量更高，且采用封闭式和开放式双格式评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ODI-Bench全面基准和Omni-CoT增强框架，显著提升了多模态大语言模型对沉浸式全方向环境的理解能力，为评估和改进此类模型提供了新标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omnidirectional images (ODIs) provide full 360x180 view which are widelyadopted in VR, AR and embodied intelligence applications. While multi-modallarge language models (MLLMs) have demonstrated remarkable performance onconventional 2D image and video understanding benchmarks, their ability tocomprehend the immersive environments captured by ODIs remains largelyunexplored. To address this gap, we first present ODI-Bench, a novelcomprehensive benchmark specifically designed for omnidirectional imageunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images andover 4,000 manually annotated question-answering (QA) pairs across 10fine-grained tasks, covering both general-level and spatial-level ODIunderstanding. Extensive experiments are conducted to benchmark 20representative MLLMs, including proprietary and open-source models, under bothclose-ended and open-ended settings. Experimental results reveal that currentMLLMs still struggle to capture the immersive context provided by ODIs. To thisend, we further introduce Omni-CoT, a training-free method which significantlyenhances MLLMs' comprehension ability in the omnidirectional environmentthrough chain-of-thought reasoning across both textual information and visualcues. Both the benchmark and the code will be released upon the publication.</description>
      <author>example@mail.com (Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai)</author>
      <guid isPermaLink="false">2510.11549v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory</title>
      <link>http://arxiv.org/abs/2510.11129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了video-SALMONN S，一种流式视听LLM，能够在固定内存预算下以1 FPS和360p分辨率处理3小时长视频，通过测试时训练内存模块和提示依赖内存读取器实现高效处理，并在多个长视频基准测试上超越离线和流式基线。&lt;h4&gt;背景&lt;/h4&gt;连续、高帧率、高分辨率处理长视频流对未来AI代理至关重要，但当前视频理解LLM难以扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理长时间视频流的模型，解决现有方法中离线方法需要适应帧率、流式方法因合并或丢弃令牌导致信息丢失的问题。&lt;h4&gt;方法&lt;/h4&gt;提出video-SALMONN S，包含(i)测试时训练(TTT)内存模块，持续更新令牌表示以捕获长程依赖；(ii)提示依赖内存读取器，从固定大小内存中选择性检索上下文相关内容；使用无Hessian共轭梯度过程(TTT_HF)优化TTT模块。&lt;h4&gt;主要发现&lt;/h4&gt;在长视频基准测试(Video-MME, LVBench, VideoEvalPro)上，video-SALMONN S能够在包含10k帧和1M令牌的多小时视频上保持高质量理解。&lt;h4&gt;结论&lt;/h4&gt;80亿参数的video-SALMONN S模型在Video-MME长分集上达到74.2%总体得分和67.8%的得分，优于离线和流式基线，证明了处理长视频流的有效性。&lt;h4&gt;翻译&lt;/h4&gt;连续、高帧率、高分辨率处理长视频流对未来AI代理至关重要，但当前视频理解LLM难以扩展。离线时，固定帧数方法需要流长度适应帧率；流式方法通过合并或丢弃令牌来限制内存，导致信息丢失。我们提出了video-SALMONN S，一种流式视听LLM，据我们所知是首个在固定内存预算下以1 FPS和360p分辨率处理3小时视频的模型。我们的模型引入了(i)测试时训练(TTT)内存模块，通过替换令牌合并持续更新令牌表示以捕获长程依赖；(ii)提示依赖内存读取器，从固定大小内存中选择性检索上下文相关内容。TTT模块使用无Hessian共轭梯度过程(TTT_HF)进行优化，实现高效适应。在长视频基准测试(Video-MME, LVBench, VideoEvalPro)上，video-SALMONN S在包含10k帧和1M令牌的多小时视频上保持高质量理解。我们的80亿参数模型在Video-MME长分集上达到74.2%总体得分和67.8%的得分，优于离线和流式基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous, high-frame-rate, high-resolution processing of long video streamsis critical for future AI agents, yet current video-understanding LLMs struggleto scale. Offline, fixed-frame-number methods require the stream length toadapt frame rates; streaming methods constrain memory by merging or discardingtokens, losing information. We propose video-SALMONN S, a streamingaudio-visual LLM that, to our knowledge, is the first to process 3-hour videosat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces(i) a test-time-training (TTT) memory module that continually updates tokenrepresentations to capture long-range dependencies by replacing token merging,and (ii) a prompt-dependent memory reader that selectively retrievescontext-relevant content from fixed-size memory. The TTT module is optimisedwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficientadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),video-SALMONN S sustains high-quality understanding on multi-hour videos with10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and67.8% on the Video-MME long split, outperforming both offline and streamingbaselines.</description>
      <author>example@mail.com (Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang)</author>
      <guid isPermaLink="false">2510.11129v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Robust Photoplethysmography Signal Denoising via Mamba Networks</title>
      <link>http://arxiv.org/abs/2510.11058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的PPG去噪框架，通过DPNet网络和创新的损失函数设计，有效解决了PPG信号中的噪声问题，同时保留了重要的生理信息。&lt;h4&gt;背景&lt;/h4&gt;光电容积描记法(PPG)被广泛应用于可穿戴健康监测，但其可靠性常因噪声和运动伪影而降低，限制了心率(HR)估计等下游应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种深度学习框架用于PPG去噪，重点是保留生理信息。&lt;h4&gt;方法&lt;/h4&gt;提出DPNet，一种基于Mamba的去噪主干网络，专为有效的时间建模而设计；采用尺度不变信号失真比(SI-SDR)损失函数提高波形保真度；引入辅助心率预测器(HRP)通过基于心率的监督提供生理一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在BIDMC数据集上的实验表明，该方法对合成噪声和真实世界运动伪影都具有很强的鲁棒性，优于传统滤波和现有神经模型；能有效恢复PPG信号同时保持心率准确性；证明了SI-SDR损失和心率引导监督的互补作用。&lt;h4&gt;结论&lt;/h4&gt;该方法在可穿戴健康系统实际部署中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;光电容积描记法(PPG)被广泛应用于可穿戴健康监测，但其可靠性常因噪声和运动伪影而降低，限制了心率(HR)估计等下游应用。本文提出了一种深度学习框架用于PPG去噪，重点是保留生理信息。在该框架中，我们提出了DPNet，一种基于Mamba的去噪主干网络，专为有效的时间建模而设计。为进一步增强去噪性能，该框架还采用了尺度不变信号失真比(SI-SDR)损失函数来提高波形保真度，以及一个辅助心率预测器(HRP)，通过基于心率的监督提供生理一致性。在BIDMC数据集上的实验表明，我们的方法对合成噪声和真实世界运动伪影都具有很强的鲁棒性，优于传统滤波和现有神经模型。我们的方法能有效恢复PPG信号同时保持心率准确性，突显了SI-SDR损失和心率引导监督的互补作用。这些结果证明了我们的方法在可穿戴健康系统实际部署中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmography (PPG) is widely used in wearable health monitoring, butits reliability is often degraded by noise and motion artifacts, limitingdownstream applications such as heart rate (HR) estimation. This paper presentsa deep learning framework for PPG denoising with an emphasis on preservingphysiological information. In this framework, we propose DPNet, a Mamba-baseddenoising backbone designed for effective temporal modeling. To further enhancedenoising performance, the framework also incorporates a scale-invariantsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and anauxiliary HR predictor (HRP) that provides physiological consistency throughHR-based supervision. Experiments on the BIDMC dataset show that our methodachieves strong robustness against both synthetic noise and real-world motionartifacts, outperforming conventional filtering and existing neural models. Ourmethod can effectively restore PPG signals while maintaining HR accuracy,highlighting the complementary roles of SI-SDR loss and HR-guided supervision.These results demonstrate the potential of our approach for practicaldeployment in wearable healthcare systems.</description>
      <author>example@mail.com (I Chiu, Yu-Tung Liu, Kuan-Chen Wang, Hung-Yu Wei, Yu Tsao)</author>
      <guid isPermaLink="false">2510.11058v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mixup Helps Understanding Multimodal Video Better</title>
      <link>http://arxiv.org/abs/2510.10986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的多模态视频理解方法，通过动态调整模态混合比例来解决强模态过拟合问题，提高模型的泛化能力和多模态鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多模态视频理解在动作识别和情感分类等任务中至关重要，通过结合不同模态的信息。然而，多模态模型容易对强模态过拟合，导致强模态主导学习并抑制弱模态的贡献。&lt;h4&gt;目的&lt;/h4&gt;解决多模态模型中强模态过拟合问题，提高模型的泛化能力和多模态鲁棒性，同时考虑模态不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;首先提出Multimodal Mixup (MM)，在聚合的多模态特征级别应用Mixup策略生成虚拟特征-标签对以减轻过拟合；然后进一步提出Balanced Multimodal Mixup (B-MM)，根据各模态对学习目标的相对贡献动态调整每个模态的混合比例。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的广泛实验表明，所提出的方法能有效提高模型的泛化能力和多模态鲁棒性，解决了强模态过拟合和模态不平衡问题。&lt;h4&gt;结论&lt;/h4&gt;通过动态调整模态混合比例，B-MM方法能够有效平衡不同模态的贡献，减轻过拟合，提高多模态视频理解模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态视频理解通过结合不同模态的信息，在动作识别和情感分类等任务中发挥着关键作用。然而，多模态模型容易对强模态过拟合，这些强模态会主导学习并抑制弱模态的贡献。为了应对这一挑战，我们首先提出多模态混合(MM)，在聚合的多模态特征级别应用混合策略，通过生成虚拟特征-标签对来减轻过拟合。虽然MM有效提高了泛化能力，但它对所有模态一视同仁，没有考虑训练过程中的模态不平衡问题。基于MM，我们进一步引入平衡多模态混合(B-MM)，根据各模态对学习目标的相对贡献动态调整每个模态的混合比例。在多个数据集上的广泛实验证明了我们的方法在提高泛化能力和多模态鲁棒性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal video understanding plays a crucial role in tasks such as actionrecognition and emotion classification by combining information from differentmodalities. However, multimodal models are prone to overfitting strongmodalities, which can dominate learning and suppress the contributions ofweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),which applies the Mixup strategy at the aggregated multimodal feature level tomitigate overfitting by generating virtual feature-label pairs. While MMeffectively improves generalization, it treats all modalities uniformly anddoes not account for modality imbalance during training. Building on MM, wefurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjuststhe mixing ratios for each modality based on their relative contributions tothe learning objective. Extensive experiments on several datasets demonstratethe effectiveness of our methods in improving generalization and multimodalrobustness.</description>
      <author>example@mail.com (Xiaoyu Ma, Ding Ding, Hao Chen)</author>
      <guid isPermaLink="false">2510.10986v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph</title>
      <link>http://arxiv.org/abs/2510.10976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Video-STR，一种基于图的强化学习方法，用于解决多模态大语言模型在精确时空理解方面的不足，通过引入GRPO推理机制和构建STV-205k数据集，在各种基准测试上取得最先进结果，比基础模型提高13%。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在语义理解方面表现出色，但在精确时空理解方面存在困难。现有时空方法主要关注视频本身，忽略了视频中的物理信息（如多物体布局和运动），限制了MLLM在具身智能和VR等需要高精度的下游应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在精确时空理解方面的不足，开发一种能够进行精确视频时空推理的方法。&lt;h4&gt;方法&lt;/h4&gt;基于可验证奖励的强化学习(RLVR)提高模型能力，引入基于图的组相对策略优化(GRPO)推理机制，指导模型在思考过程中推断场景的潜在时空拓扑结构，并构建包含205k个问答对的STV-205k数据集，涵盖室内和室外环境中的动态多物体场景。&lt;h4&gt;主要发现&lt;/h4&gt;Video-STR在各种基准测试上取得了最先进的结果，在STI-Bench上比基础模型提高了13%，证明了该方法和数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;Video-STR成功解决了多模态大语言模型在精确时空理解方面的不足，代码、模型和数据将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展展示了强大的语义理解能力，但在执行精确时空理解方面存在困难。现有的时空方法主要关注视频本身，而忽略了视频中的物理信息，如多物体布局和运动。这些限制限制了MLLM在需要高精度的下游应用中的使用，包括具身智能和VR。为解决这个问题，我们提出了Video-STR，一种基于图的强化学习方法，用于精确的视频时空推理。基于可验证奖励的强化学习(RLVR)提高模型能力的能力，我们引入了一种使用基于图的组相对策略优化(GRPO)方法的推理机制，指导模型在思考过程中推断场景的潜在时空拓扑结构。为解决时空训练数据的缺乏，我们构建了包含205k个问答对的STV-205k数据集，涵盖室内和室外环境中的动态多物体场景，以支持模型训练。实验表明，Video-STR在各种基准测试上取得了最先进的结果，在STI-Bench上比基础模型提高了13%，证明了我们方法和数据集的有效性。代码、模型和数据将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型（MLLMs）在精确时空理解方面的不足。现有模型虽然语义理解能力强，但在理解视频中的物体位置、布局、运动轨迹等物理信息方面表现不佳。这个问题很重要，因为它限制了MLLMs在需要高精度的下游应用（如具身智能和VR）中的使用，而这些应用对物体间的空间关系和时间动态有严格要求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有时空方法的局限性，发现它们主要关注视频本身而忽视物理信息，或者使用像素级定位和2D认知图等方法，这些方法无法准确推断物体在物理空间中的布局和分布。基于这些分析，作者设计了一个基于图的表示方法，将物体建模为节点，物体间关系建模为边，这种方法具有旋转不变性且更鲁棒。作者借鉴了强化学习与可验证奖励（RLVR）框架和Group Relative Policy Optimization（GRPO）方法，但扩展了它们以适应视频时空推理任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的表示来建模多物体场景的拓扑结构，并通过强化学习框架训练模型理解时空关系。整体流程包括：1）从TAO、ScanNet和KITTI收集数据；2）构建STV-205k数据集（205k问答对）；3）设计多种可验证奖励函数（格式、多选、数值、IoU奖励）；4）引入图推理机制帮助模型理解空间拓扑；5）使用扩展的GRPO算法进行训练；6）在多个基准测试上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）构建STV-205k数据集，解决视频时空训练数据稀缺问题；2）首次使用物体间关系图表征多物体场景，扩展GRPO引入图推理机制；3）设计特定奖励函数和图推理机制监督模型理解时空信息。相比之前工作，本文强调视频嵌入的物理信息而非仅关注视频本身；使用图结构而非像素定位或2D认知图表示场景，具有旋转不变性；通过特定奖励函数更有效监督模型对时空信息的理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Video-STR通过引入基于图的时空推理机制和STV-205k数据集，显著提升了多模态大语言模型在视频时空推理任务上的性能，实现了现有方法的最佳效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Multimodal Large Language Models (MLLMs) has demonstratedstrong semantic understanding capabilities, but struggles to perform precisespatio-temporal understanding. Existing spatio-temporal methods primarily focuson the video itself, while overlooking the physical information within thevideo, such as multi-object layouts and motion. Such limitations restrict theuse of MLLMs in downstream applications that demand high precision, includingembodied intelligence and VR. To address this issue, we present Video-STR, anovel graph-based reinforcement method for precise Video Spatio-TemporalReasoning. Building upon the capacity of Reinforcement Learning with VerifiableReward (RLVR) to improve model abilities, we introduce a reasoning mechanismusing graph-based Group Relative Policy Optimization (GRPO) method to guide themodel in inferring the underlying spatio-temporal topology of scenarios duringthe thinking process. To resolve the lack of spatio-temporal training data, weconstruct the STV-205k dataset with 205k question-answering pairs, coveringdynamic multi-object scenes in both indoor and outdoor environments, to supportthe model training. Experiments show that Video-STR achieves state-of-the-artresults on various benchmarks, outperforming the base model by 13% onSTI-Bench, and demonstrating the effectiveness of our approach and dataset.Code, model, and data will be released.</description>
      <author>example@mail.com (Wentao Wang, Heqing Zou, Tianze Luo, Rui Huang, Yutian Zhao, Zhuochen Wang, Hansheng Zhang, Chengwei Qin, Yan Wang, Lin Zhao, Huaijian Zhang)</author>
      <guid isPermaLink="false">2510.10976v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs</title>
      <link>http://arxiv.org/abs/2510.10689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了OmniVideoBench，一个专门用于评估多模态大语言模型协同视听理解能力的大规模基准测试。该基准测试包含1000个高质量问答对，覆盖13种问题类型，评估结果显示当前模型与人类推理能力存在明显差距，开源模型表现尤其不佳。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视频理解方面显示出巨大潜力，但现有基准测试无法全面评估跨音频和视觉模态的协同推理能力，往往忽略其中一个模态或以逻辑不一致的方式整合它们。&lt;h4&gt;目的&lt;/h4&gt;弥补现有基准测试的不足，引入一个专门用于评估协同视听理解能力的大规模且精心设计的基准测试，强调模态互补性和逻辑一致性。&lt;h4&gt;方法&lt;/h4&gt;构建OmniVideoBench基准测试，包含1000个高质量问答对，每个标注有逐步推理轨迹；数据来源于628个多样化视频，时长从几秒到30分钟不等；包含13种精心设计的问题类型，涵盖时间推理、空间定位、计数、因果推断和总结等；所有数据经过人工验证确保正确性和唯一性。&lt;h4&gt;主要发现&lt;/h4&gt;在OmniVideoBench上对多个多模态大语言模型的评估显示，模型性能与人类推理之间存在明显差距；开源模型显著落后于闭源模型，这突显了真实视听推理的内在难度。&lt;h4&gt;结论&lt;/h4&gt;将发布OmniVideoBench基准测试以促进具有更强和更可泛化推理能力的多模态大语言模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近多模态大语言模型的进展在视频理解方面展示了巨大潜力。然而，现有基准测试无法全面评估跨音频和视觉模态的协同推理能力，常常忽略其中一个模态或以逻辑不一致的方式整合它们。为了弥补这一差距，我们引入了OmniVideoBench，一个大规模且精心设计的基准测试，专门用于评估协同视听理解，特别强调模态互补性和逻辑一致性。具体来说，OmniVideoBench包含1000个高质量的问答对，每个都标注了逐步推理轨迹，来源于628个从几秒到30分钟不等的多样化视频，并经过人工验证以确保完全正确性和唯一性。此外，OmniVideoBench包含13种精心设计的问题类型，涵盖时间推理、空间定位、计数、因果推断、总结等，从而捕捉视频理解的基本挑战。在OmniVideoBench上对多个多模态大语言模型的评估揭示了模型性能与人类推理之间的明显差距，其中开源模型显著落后于闭源模型，这突显了真实视听推理的内在难度。我们将发布OmniVideoBench以促进具有更强和更可泛化推理能力的多模态大语言模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal large language models (MLLMs) have demonstratedsubstantial potential in video understanding. However, existing benchmarks failto comprehensively evaluate synergistic reasoning capabilities across audio andvisual modalities, often neglecting either one of the modalities or integratingthem in a logically inconsistent manner. To bridge this gap, we introduceOmniVideoBench, a large-scale and rigorously designed benchmark dedicated toassessing synergistic audio-visual understanding, with a strong emphasis onmodality complementarity and logical consistency. Specifically, OmniVideoBenchcomprises 1000 high-quality question-answer(QA) pairs, each annotated withstep-by-step reasoning traces, derived from 628 diverse videos ranging fromseveral seconds to 30 minutes, and manually verified to guarantee completecorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefullydesigned question types, covering temporal reasoning, spatial localization,counting, causal inference, summarization, and beyond, thereby capturing theessential challenges of video understanding. Evaluation of multiple MLLMs onOmniVideoBench reveals a pronounced gap between model performance and humanreasoning, with open-source models lagging significantly behind theirclosed-source counterparts, underscoring the inherent difficulty of genuineaudio-visual reasoning. We will release OmniVideoBench to foster thedevelopment of MLLMs with stronger and more generalizable reasoningcapabilities.</description>
      <author>example@mail.com (Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu)</author>
      <guid isPermaLink="false">2510.10689v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction</title>
      <link>http://arxiv.org/abs/2510.10454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 GenAI4Health Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Traj-CoA是一个多智能体系统，通过智能体链处理电子健康记录数据，减少噪声并保留完整时间线，在患者轨迹建模任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型为患者轨迹建模提供了通用方法，但电子健康记录数据的时间推理存在数据冗长和嘈杂的问题。&lt;h4&gt;目的&lt;/h4&gt;解决EHR数据在时间推理中的长序列和噪声问题，提高患者轨迹建模的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Traj-CoA多智能体系统，使用工作智能体顺序处理EHR数据，将关键事件提炼到EHRMem记忆模块中，最后由管理智能体进行综合预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基于五年EHR数据的一年肺癌风险预测的零样本任务中，Traj-CoA优于四类基线方法，展现出与临床实践一致的时间推理能力。&lt;h4&gt;结论&lt;/h4&gt;Traj-CoA是一种有前途的、鲁棒且通用的方法，用于建模复杂患者轨迹。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型为患者轨迹建模提供了一种通用方法，但受电子健康记录数据在时间推理中冗长和嘈杂的特性所困扰。为应对这些挑战，我们引入了Traj-CoA，一个涉及智能体链的多智能体系统，用于患者轨迹建模。Traj-CoA采用一系列工作智能体顺序处理可管理的EHR数据块，将关键事件提炼到共享的长期记忆模块EHRMem中，以减少噪声并保留完整时间线。最终管理智能体综合工作智能体的摘要和EHRMem中提取的时间线进行预测。在基于五年EHR数据的一年肺癌风险预测的零样本任务中，Traj-CoA优于四类基线方法。分析表明，Traj-CoA展现出与临床实践一致的时间推理能力，使其成为建模复杂患者轨迹的一种前景广阔的鲁棒且通用的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) offer a generalizable approach for modelingpatient trajectories, but suffer from the long and noisy nature of electronichealth records (EHR) data in temporal reasoning. To address these challenges,we introduce Traj-CoA, a multi-agent system involving chain-of-agents forpatient trajectory modeling. Traj-CoA employs a chain of worker agents toprocess EHR data in manageable chunks sequentially, distilling critical eventsinto a shared long-term memory module, EHRMem, to reduce noise and preserve acomprehensive timeline. A final manager agent synthesizes the worker agents'summary and the extracted timeline in EHRMem to make predictions. In azero-shot one-year lung cancer risk prediction task based on five-year EHRdata, Traj-CoA outperforms baselines of four categories. Analysis reveals thatTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as apromisingly robust and generalizable approach for modeling complex patienttrajectories.</description>
      <author>example@mail.com (Sihang Zeng, Yujuan Fu, Sitong Zhou, Zixuan Yu, Lucas Jing Liu, Jun Wen, Matthew Thompson, Ruth Etzioni, Meliha Yetisgen)</author>
      <guid isPermaLink="false">2510.10454v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration</title>
      <link>http://arxiv.org/abs/2510.10395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://avocado-captioner.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AVoCaDO，一个由音频和视觉模态时间编排驱动的强大音视频视频字幕生成器，通过两阶段微调管道显著提升了字幕生成质量。&lt;h4&gt;背景&lt;/h4&gt;音视频视频字幕生成旨在生成语义丰富的描述，并使视觉和听觉事件之间保持时间对齐，从而有助于视频理解和生成。&lt;h4&gt;目的&lt;/h4&gt;开发一个强大的音视频视频字幕生成器，通过音频和视觉模态之间的时间编排来提高字幕生成的质量。&lt;h4&gt;方法&lt;/h4&gt;提出了一个两阶段的微调管道：(1) AVoCaDO SFT，在一个新整理的包含107K高质量、时间对齐的音视频字幕的数据集上微调模型；(2) AVoCaDO GRPO，利用定制化的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。&lt;h4&gt;主要发现&lt;/h4&gt;1. AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有的开源模型。2. AVoCaDO在仅视觉设置下的VDC和DREAM-1K基准测试中实现了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;AVoCaDO通过两阶段微调管道有效地提高了音视频视频字幕生成的质量，不仅在音视频设置下表现出色，而且在仅视觉设置下也能保持竞争力。&lt;h4&gt;翻译&lt;/h4&gt;音视频视频字幕生成旨在生成语义丰富的描述，并使视觉和听觉事件之间保持时间对齐，从而有助于视频理解和生成。在本文中，我们提出了AVoCaDO，一个由音频和视觉模态之间时间编排驱动的强大音视频视频字幕生成器。我们提出了一个两阶段的微调管道：(1) AVoCaDO SFT，在一个新整理的包含107K高质量、时间对齐的音视频字幕的数据集上微调模型；(2) AVoCaDO GRPO，利用定制化的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。实验结果表明，AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有的开源模型，并且在仅视觉设置下的VDC和DREAM-1K基准测试中也实现了具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual video captioning aims to generate semantically rich descriptionswith temporal alignment between visual and auditory events, thereby benefitingboth video understanding and generation. In this paper, we present AVoCaDO, apowerful audiovisual video captioner driven by the temporal orchestrationbetween audio and visual modalities. We propose a two-stage post-trainingpipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curateddataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)AVoCaDO GRPO, which leverages tailored reward functions to further enhancetemporal coherence and dialogue accuracy while regularizing caption length andreducing collapse. Experimental results demonstrate that AVoCaDO significantlyoutperforms existing open-source models across four audiovisual videocaptioning benchmarks, and also achieves competitive performance on the VDC andDREAM-1K benchmark under visual-only settings.</description>
      <author>example@mail.com (Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan)</author>
      <guid isPermaLink="false">2510.10395v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding</title>
      <link>http://arxiv.org/abs/2510.09274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一框架，用于联合优化时间句子定位和视频对象分割，解决了现有方法中忽视时间线索或增加系统复杂性的问题。&lt;h4&gt;背景&lt;/h4&gt;Referring Video Object Segmentation (RefVOS) 需要根据自然语言描述在视频中分割目标对象，这需要时间推理和细粒度的视觉理解能力。现有的基于LLM的采样策略通常依赖手工设计的启发式方法或外部关键帧模型，前者忽视重要时间线索，后者增加系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架，联合优化时间句子定位（TSG）和RefVOS，自然融合关键时刻定位能力。&lt;h4&gt;方法&lt;/h4&gt;1) 训练阶段：引入新的TSG范式，使用[FIND]标记通过时间标记相似度匹配识别关键时刻，避免外部时间戳编码；2) 推理阶段：设计以时刻为中心的采样(MCS)策略，密集采样信息丰富时刻，稀疏采样非必要帧；3) 开发双向锚点更新传播(BAP)，利用最相关时刻初始化高质量掩码，动态更新减轻累积误差。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合优化TSG和RefVOS，以及创新的采样策略和传播方法，能够有效保留运动细节和全局上下文，同时提高跟踪稳定性。&lt;h4&gt;结论&lt;/h4&gt;该框架解决了现有采样策略的局限性，通过自然集成关键时刻定位能力，实现了更高效的视频对象分割。&lt;h4&gt;翻译&lt;/h4&gt;该论文提出了一种引用视频对象分割方法，通过自然语言描述引导视频中的目标对象分割，同时需要时间推理和细粒度视觉理解能力。基于LLM的现有采样策略通常依赖手工设计启发式方法或外部关键帧模型。前者常常忽视重要时间线索，后者增加系统复杂性。为此，我们提出统一框架，联合优化时间句子定位和引用视频对象分割，自然融入关键时刻定位能力。训练阶段，我们引入新型TSG范式，使用专用[FIND]标记通过时间标记相似度匹配识别关键时刻，避免需要外部时间戳编码。推理阶段，我们设计以时刻为中心的采样策略，密集采样信息丰富时刻，同时稀疏采样非必要帧，保留运动细节和全局上下文。为进一步增强跟踪稳定性，我们开发双向锚点更新传播，利用最相关时刻作为高质量掩码初始化起点，并在采样点动态更新以减轻累积误差。代码和模型将发布于：https://github.com/Dmmm1997/MomentSeg&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RefVOS) seeks to segment target objectsin videos guided by natural language descriptions, demanding both temporalreasoning and fine-grained visual comprehension. Existing sampling strategiesfor LLM-based approaches typically rely on either handcrafted heuristics orexternal keyframe models. The former often overlooks essential temporal cues,while the latter increases system complexity. To address this, we propose aunified framework that jointly optimizes Temporal Sentence Grounding (TSG) andRefVOS, naturally incorporating key moment grounding capability. Duringtraining, we introduce a novel TSG paradigm that employs a dedicated\texttt{[FIND]} token for key moment identification through temporal tokensimilarity matching, thereby avoiding the need for external timestampencodings. For inference, we design a Moment-Centric Sampling (MCS) strategythat densely samples informative moments while sparsely sampling non-essentialframes, preserving both motion details and global context. To further enhancetracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),which leverages the most relevant moment as start point for high-quality maskinitialization and dynamically updates at sampled points to mitigateaccumulated errors. Code and model will be available at:https://github.com/Dmmm1997/MomentSeg</description>
      <author>example@mail.com (Ming Dai, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang)</author>
      <guid isPermaLink="false">2510.09274v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras</title>
      <link>http://arxiv.org/abs/2510.09230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于消费级设备视频的肩部疾病诊断框架HMVDx，利用多模态大语言模型分离动作理解和疾病诊断任务，显著提高了诊断准确率。&lt;h4&gt;背景&lt;/h4&gt;肩部疾病是全球常见疾病，在老年人和从事重复肩部任务的工作者中发病率高。在医疗资源稀缺地区，早期准确诊断面临挑战，需要低成本且易于扩展的辅助诊断方案。&lt;h4&gt;目的&lt;/h4&gt;引入消费级设备拍摄的视频作为诊断基础降低成本，研究多模态大语言模型在肩部疾病初步诊断中的应用，提出HMVDx框架。&lt;h4&gt;方法&lt;/h4&gt;HMVDx框架将动作理解和疾病诊断两个任务分开，由两个多模态大语言模型分别完成。提出'可用性指数'新型指标，基于医疗决策逻辑过程评估多模态大语言模型在医疗领域的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;HMVDx在诊断肩关节损伤方面的准确率比直接视频诊断提高了79.6%，显示低成本多模态大语言模型在医疗应用中的潜在价值。&lt;h4&gt;结论&lt;/h4&gt;HMVDx对多模态大语言模型在医学领域视频理解应用的研究具有重大技术贡献。&lt;h4&gt;翻译&lt;/h4&gt;肩部疾病，如冻结肩（又名粘连性关节囊炎），是影响全球人民健康的常见疾病，在老年人和从事重复肩部任务的工作者中发病率高。在医疗资源稀缺的地区，实现早期准确诊断面临重大挑战，迫切需要低成本且易于扩展的辅助诊断解决方案。本研究引入消费级设备拍摄的视频作为诊断基础，降低用户成本。我们专注于多模态大语言模型在肩部疾病初步诊断中的创新应用，并提出混合运动视频诊断框架。该框架将动作理解和疾病诊断两个任务分开，分别由两个多模态大语言模型完成。除了传统评估指标外，本研究还提出了一种名为'可用性指数'的新型指标，基于医疗决策的逻辑过程（动作识别、运动诊断和最终诊断）。该指数从整个医疗诊断路径的角度评估多模态大语言模型在医疗领域的有效性，揭示了低成本多模态大语言模型在医疗应用中对医疗从业者的潜在价值。在实验比较中，HMVDx在诊断肩关节损伤方面的准确率比直接视频诊断提高了79.6%，这是多模态大语言模型在医学领域视频理解应用研究中的重大技术贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),are common conditions affecting the health of people worldwide, and have a highincidence rate among the elderly and workers engaged in repetitive shouldertasks. In regions with scarce medical resources, achieving early and accuratediagnosis poses significant challenges, and there is an urgent need forlow-cost and easily scalable auxiliary diagnostic solutions. This researchintroduces videos captured by consumer-grade devices as the basis fordiagnosis, reducing the cost for users. We focus on the innovative applicationof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis ofshoulder disorders and propose a Hybrid Motion Video Diagnosis framework(HMVDx). This framework divides the two tasks of action understanding anddisease diagnosis, which are respectively completed by two MLLMs. In additionto traditional evaluation indicators, this work proposes a novel metric calledUsability Index by the logical process of medical decision-making (actionrecognition, movement diagnosis, and final diagnosis). This index evaluates theeffectiveness of MLLMs in the medical field from the perspective of the entiremedical diagnostic pathway, revealing the potential value of low-cost MLLMs inmedical applications for medical practitioners. In experimental comparisons,the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by79.6\% compared with direct video diagnosis, a significant technicalcontribution to future research on the application of MLLMs for videounderstanding in the medical field.</description>
      <author>example@mail.com (Jindong Hong, Wencheng Zhang, Shiqin Qiao, Jianhai Chen, Jianing Qiu, Chuanyang Zheng, Qian Xu, Yun Ji, Qianyue Wen, Weiwei Sun, Hao Li, Huizhen Li, Huichao Wang, Kai Wu, Meng Li, Yijun He, Lingjie Luo, Jiankai Sun)</author>
      <guid isPermaLink="false">2510.09230v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration</title>
      <link>http://arxiv.org/abs/2510.09048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出TW-GCN框架，结合图卷积网络和时间架构预测电动汽车充电需求，解决了充电设施分布不均和利用不规律对电网稳定性和投资规划的挑战&lt;h4&gt;背景&lt;/h4&gt;交通运输是温室气体的主要来源，向电动汽车等可持续替代品转型非常紧迫，但充电设施的空间分布不均和利用不规律对电网稳定性和投资规划构成挑战&lt;h4&gt;目的&lt;/h4&gt;开发TW-GCN框架，结合图卷积网络和时间架构，预测美国田纳西州的电动汽车充电需求&lt;h4&gt;方法&lt;/h4&gt;利用真实世界的交通流量、天气条件和美国最大的电动汽车基础设施公司提供的专有数据，捕捉空间依赖性和时间动态&lt;h4&gt;主要发现&lt;/h4&gt;中期（3小时）预测在响应性和稳定性之间取得最佳平衡；1DCNN在时间模型中表现持续优于其他模型；东、中、西田纳西州的预测准确性存在差异，反映了站点密度、人口和当地需求变异性对模型性能的影响&lt;h4&gt;结论&lt;/h4&gt;TW-GCN框架推动了数据驱动智能与电动汽车基础设施规划的整合，支持可持续交通转型和弹性电网管理&lt;h4&gt;翻译&lt;/h4&gt;交通仍然是温室气体的主要来源，这凸显了向电动汽车等可持续替代品过渡的紧迫性。然而，充电设施的空间分布不均和使用不规则对电网稳定性和投资规划构成了挑战。本研究引入了TW-GCN，一个结合图卷积网络和时间架构的时空预测框架，用于预测美国田纳西州的电动汽车充电需求。我们利用真实的交通流量、天气条件以及美国最大的电动汽车基础设施公司提供的专有数据，来捕捉空间依赖性和时间动态。在不同滞后时间、聚类策略和序列长度上的广泛实验表明，中期（3小时）预测在响应性和稳定性之间取得了最佳平衡，1DCNN持续优于其他时间模型。区域分析显示东、中、西田纳西州的预测准确性存在差异，反映了站点密度、人口和当地需求变异性如何影响模型性能。所提出的TW-GCN框架推动了数据驱动智能与电动汽车基础设施规划的整合，支持可持续交通转型和弹性电网管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transportation remains a major contributor to greenhouse gas emissions,highlighting the urgency of transitioning toward sustainable alternatives suchas electric vehicles (EVs). Yet, uneven spatial distribution and irregularutilization of charging infrastructure create challenges for both power gridstability and investment planning. This study introduces TW-GCN, aspatio-temporal forecasting framework that combines Graph ConvolutionalNetworks with temporal architectures to predict EV charging demand inTennessee, United States (U.S.). We utilize real-world traffic flows, weatherconditions, and proprietary data provided by one of the largest EVinfrastructure company in the U.S. to capture both spatial dependencies andtemporal dynamics. Extensive experiments across varying lag horizons,clustering strategies, and sequence lengths reveal that mid-horizon (3-hour)forecasts achieve the best balance between responsiveness and stability, with1DCNN consistently outperforming other temporal models. Regional analysis showsdisparities in predictive accuracy across East, Middle, and West Tennessee,reflecting how station density, population, and local demand variability shapemodel performance. The proposed TW-GCN framework advances the integration ofdata-driven intelligence into EV infrastructure planning, supporting bothsustainable mobility transitions and resilient grid management.</description>
      <author>example@mail.com (Jose Tupayachi, Mustafa C. Camur, Kevin Heaslip, Xueping Li)</author>
      <guid isPermaLink="false">2510.09048v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos</title>
      <link>http://arxiv.org/abs/2510.08936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了Ro-Bench，首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准，发现当前模型在面对被操纵的视频内容时鲁棒性不足，但通过反事实数据微调可显著提升性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在各种视频理解任务中表现出显著性能，但当面对被操纵的视频内容时，它们的鲁棒性在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;引入Ro-Bench，首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准。&lt;h4&gt;方法&lt;/h4&gt;Ro-Bench通过编辑风格、物体、背景及其组合，整合高质量、多样化的时间相关视频数据；评估了八个最近的视频多模态大语言模型；并通过反事实数据微调多模态大语言模型以增强鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型在面对反事实视频内容时，在Ro-Bench上表现出显著的性能下降；使用反事实数据微调多模态大语言模型可以增强鲁棒性，在Ro-Bench上实现了21.73%的性能提升，在MVBench数据集的20个任务上实现了12.78%的改进。&lt;h4&gt;结论&lt;/h4&gt;反事实数据在增强多模态大语言模型的视频理解能力方面具有显著有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，多模态大语言模型在各种视频理解任务中展示了显著的性能。然而，它们的鲁棒性，特别是在面对被操纵的视频内容时，在很大程度上仍未被探索。在本文中，我们引入了Ro-Bench，这是首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准。Ro-Bench通过编辑风格、物体、背景及其组合，整合了高质量、多样化且时间相关的视频数据。我们评估了八个最近的视频多模态大语言模型，发现当面对反事实视频内容时，当前模型在Ro-Bench上表现出显著的性能下降。此外，我们证明使用反事实数据微调多模态大语言模型可以增强鲁棒性，在Ro-Bench上实现了21.73%的性能提升，在MVBench数据集的20个任务上实现了12.78%的改进。这些发现强调了反事实数据在增强多模态大语言模型视频理解能力方面的有效性。代码和数据将很快发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Multi-modal Large Language Models (MLLMs) have demonstratedsignificant performance across various video understanding tasks. However,their robustness, particularly when faced with manipulated video content,remains largely unexplored. In this paper, we introduce Ro-Bench, the firstbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)counterfactual video test sets. Ro-Bench incorporates high-quality, diverse andtemporally relevant video data, by editing Style, Object, Background and theircompositions. We evaluated eight recent video MLLMs and found that currentmodels exhibit substantial performance degradation on Ro-Bench when exposed tocounterfactual video content. Furthermore, we demonstrate that fine-tuningMLLMs with counterfactual data enhances robustness, achieving a 21.73%performance increase on Ro-Bench and a 12.78% improvement across 20 tasks inthe MVBench dataset. These findings underscore the effectiveness ofcounterfactual data in enhancing the video understanding ability of MLLMs. Thecode and data will be released shortly.</description>
      <author>example@mail.com (Zixi Yang, Jiapeng Li, Muxi Diao, Yinuo Jing, Kongming Liang)</author>
      <guid isPermaLink="false">2510.08936v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition</title>
      <link>http://arxiv.org/abs/2510.08818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了D-CoDe，一个无需训练的适应框架，用于解决将图像预训练的视觉语言模型扩展到视频领域时面临的感知瓶颈和令牌过载问题。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在多样化的视频语言任务中表现出色，可以通过适应图像预训练的视觉语言模型来有效构建。然而，这种适应具有挑战性，因为需要处理密集且时间上延展的视觉输入，这超出了基于图像模型的处理能力。&lt;h4&gt;目的&lt;/h4&gt;解决将基于图像的视觉语言模型扩展到视频领域时面临的感知瓶颈和令牌过载这两个关键挑战。&lt;h4&gt;方法&lt;/h4&gt;提出D-CoDe，一个无需训练的适应框架，结合了动态压缩和问题分解两种技术。动态压缩通过自适应选择代表性帧和空间令牌的内容感知聚合来减轻感知瓶颈；问题分解通过将原始查询重新表述为子问题来缓解令牌过载。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明D-CoDe在各种基准测试中有效提高了视频理解能力，特别是在具有挑战性的长视频基准测试上表现出色，突显了其处理复杂视频语言任务的潜力。&lt;h4&gt;结论&lt;/h4&gt;D-CoDe框架能够有效解决视频理解中的感知瓶颈和令牌过载问题，为视频大语言模型的构建提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型在多样化的视频语言任务中表现出色，可以通过适应图像预训练的视觉语言模型来有效构建。然而，这种适应仍然具有挑战性，因为它需要处理密集且时间上延展的视觉输入，这超出了基于图像模型的处理能力。本文确定了感知瓶颈和令牌过载是将基于图像的视觉语言模型扩展到视频领域时的关键挑战。为了解决这些问题，我们提出了D-CoDe，一个无需训练的适应框架，结合了动态压缩和问题分解。具体而言，动态压缩通过自适应选择代表性帧和空间令牌的内容感知聚合来减轻感知瓶颈，从而减少冗余同时保留信息内容。同时，问题分解通过将原始查询重新表述为子问题来缓解令牌过载，指导模型关注视频的不同方面，实现更全面的理解。实验证明D-CoDe在各种基准测试中有效提高了视频理解能力。此外，在具有挑战性的长视频基准测试上的良好表现突显了D-CoDe处理复杂视频语言任务的潜力。代码可在https://github.com/hukcc/D-CoDe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs), which excel in diverse video-languagetasks, can be effectively constructed by adapting image-pretrainedvision-language models (VLMs). However, this adaptation remains challenging, asit requires processing dense and temporally extended visual inputs that exceedthe capacity of image-based models. This paper identifies the perceptionbottleneck and token overload as key challenges in extending image-based VLMsto the video domain. To address these issues, we propose D-CoDe, atraining-free adaptation framework that incorporates dynamic compression andquestion decomposition. Specifically, dynamic compression alleviates theperception bottleneck through adaptive selection of representative frames andcontent-aware aggregation of spatial tokens, thereby reducing redundancy whilepreserving informative content. In parallel, question decomposition mitigatestoken overload by reformulating the original query into sub-questions, guidingthe model to focus on distinct aspects of the video and enabling morecomprehensive understanding. Experiments demonstrate that D-CoDe effectivelyimproves video understanding across various benchmarks. Furthermore, strongperformance on the challenging long-video benchmark highlights the potential ofD-CoDe in handling complex video-language tasks. Code is available athttps://github.com/hukcc/D-CoDe.</description>
      <author>example@mail.com (Yiyang Huang, Yizhou Wang, Yun Fu)</author>
      <guid isPermaLink="false">2510.08818v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Edu-EmotionNet: Cross-Modality Attention Alignment with Temporal Feedback Loops</title>
      <link>http://arxiv.org/abs/2510.08802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 6 Figures, 3 Tables, Accepted as a Regular Research paper at  ICMLA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Edu-EmotionNet框架，用于在线教育中的学习者情绪识别，通过联合建模时间情绪演变和模态可靠性，实现了稳健的情感识别。&lt;h4&gt;背景&lt;/h4&gt;在线教育中理解学习者情绪对提高参与度和个性化教学至关重要。现有情绪识别方法通常采用静态融合策略，并假设模态输入始终可靠，这在真实学习环境中很少成立。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，联合建模时间情绪演变和模态可靠性，以实现稳健的情感识别，特别适用于在线教育环境。&lt;h4&gt;方法&lt;/h4&gt;该模型包含三个关键组件：跨模态注意力对齐模块用于动态跨模态上下文共享；模态重要性估计器为每个模态在每一步分配基于置信度的权重；时间反馈循环利用先前的预测来强制时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;Edu-EmotionNet在IEMOCAP和MOSEI的教育子集上取得了最先进的性能，并显示出对缺失或有噪声模态的强大鲁棒性。可视化证实了其捕捉情绪转变和自适应优先考虑可靠信号的能力。&lt;h4&gt;结论&lt;/h4&gt;该模型适合部署在实时学习系统中，能够有效识别学习者的情绪状态，为个性化教学提供支持。&lt;h4&gt;翻译&lt;/h4&gt;理解在线教育中的学习者情绪对于提高参与度和个性化教学至关重要。虽然先前在情绪识别方面的工作探索了多模态融合和时间建模，但现有方法通常依赖静态融合策略，并假设模态输入始终可靠，这在真实学习环境中很少成立。我们引入了Edu-EmotionNet，一个新颖的框架，联合建模时间情绪演变和模态可靠性，以实现稳健的情感识别。我们的模型包含三个关键组件：用于动态跨模态上下文共享的跨模态注意力对齐模块，为每个模态在每一步分配基于置信度的权重的模态重要性估计器，以及利用先前预测强制时间一致性的时间反馈循环。在为困惑、好奇、无聊和沮丧重新注释的教育子集IEMOCAP和MOSEI上评估，Edu-EmotionNet取得了最先进的性能，并显示出对缺失或有噪声模态的强大鲁棒性。可视化证实了其捕捉情绪转变和自适应优先考虑可靠信号的能力，使其非常适合部署在实时学习系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding learner emotions in online education is critical for improvingengagement and personalized instruction. While prior work in emotionrecognition has explored multimodal fusion and temporal modeling, existingmethods often rely on static fusion strategies and assume that modality inputsare consistently reliable, which is rarely the case in real-world learningenvironments. We introduce Edu-EmotionNet, a novel framework that jointlymodels temporal emotion evolution and modality reliability for robust affectrecognition. Our model incorporates three key components: a Cross-ModalityAttention Alignment (CMAA) module for dynamic cross-modal context sharing, aModality Importance Estimator (MIE) that assigns confidence-based weights toeach modality at every time step, and a Temporal Feedback Loop (TFL) thatleverages previous predictions to enforce temporal consistency. Evaluated oneducational subsets of IEMOCAP and MOSEI, re-annotated for confusion,curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-artperformance and demonstrates strong robustness to missing or noisy modalities.Visualizations confirm its ability to capture emotional transitions andadaptively prioritize reliable signals, making it well suited for deployment inreal-time learning systems</description>
      <author>example@mail.com (S M Rafiuddin)</author>
      <guid isPermaLink="false">2510.08802v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.07791v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Geo-Temporal Reasoning基准测试(GTR-Bench)，用于评估视觉-语言模型在结合图像/视频和图形上下文时的地理时空智能能力。评估显示当前最佳模型表现显著落后于人类，并揭示了三个主要缺陷。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型的时空智能在自动驾驶、具身AI和通用人工智能领域受到关注。现有基准测试主要关注自我视角推理或地理视角推理，无法评估结合图像/视频和图形上下文时的地理时空智能，这对交通管理和应急响应等领域很重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的不足，引入Geo-Temporal Reasoning基准测试(GTR-Bench)，用于评估大规模摄像头网络中移动目标的地理时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建GTR-Bench基准测试，要求模型在地图和视频之间进行多视角切换，对多个具有非重叠视野的视频进行联合推理，并对任何视频上下文都未观察到的时空区域进行推理。评估了10多种流行的视觉-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;即使最佳专有模型Gemini-2.5-Pro(34.9%)在地理时间推理上也显著落后于人类表现(78.61%)。当前模型存在三个主要缺陷：(1)时空上下文利用不平衡；(2)时间预测能力较弱，时间强调任务表现差；(3)缺乏理解或对齐地图数据与多视角视频输入的能力。&lt;h4&gt;结论&lt;/h4&gt;GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。基准测试和代码将在https://github.com/X-Luffy/GTR-Bench上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近，视觉-语言模型的时空智能因其对自动驾驶、具身AI和通用人工智能的重要性而受到广泛关注。现有的时空基准测试主要关注基于图像/视频上下文的自我视角推理，或基于图形上下文(如地图)的地理视角推理，因此无法评估VLMs在结合图像/视频和图形上下文时的地理时空智能，这对交通管理和应急响应等领域很重要。为解决这些差距，我们引入了Geo-Temporal Reasoning基准测试(GTR-Bench)，这是一个在大规模摄像头网络中对移动目标进行地理时间推理的新挑战。GTR-Bench更具挑战性，因为它需要在地图和视频之间进行多视角切换，对多个具有非重叠视野的视频进行联合推理，以及对任何视频上下文都未观察到的时空区域进行推理。对GTR-Bench上10多种流行VLMs的评估表明，即使是最佳专有模型Gemini-2.5-Pro(34.9%)在地理时间推理上也显著落后于人类表现(78.61%)。此外，我们对GTR-Bench的全面分析揭示了当前模型在地理时间推理方面的三个主要缺陷。(1)VLMs的推理受到时空上下文不平衡利用的影响。(2)VLMs在时间预测方面能力较弱，导致在时间强调任务上的表现比空间强调任务差。(3)VLMs缺乏理解或对齐地图数据与多视角视频输入的能力。我们相信GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。基准测试和代码将在https://github.com/X-Luffy/GTR-Bench上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently spatial-temporal intelligence of Visual-Language Models (VLMs) hasattracted much attention due to its importance for Autonomous Driving, EmbodiedAI and General Artificial Intelligence. Existing spatial-temporal benchmarksmainly focus on egocentric perspective reasoning with images/video context, orgeographic perspective reasoning with graphics context (eg. a map), thus failto assess VLMs' geographic spatial-temporal intelligence with both images/videoand graphics context, which is important for areas like traffic management andemergency response. To address the gaps, we introduce Geo-Temporal Reasoningbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning ofmoving targets in a large-scale camera network. GTR-Bench is more challengingas it requires multiple perspective switches between maps and videos, jointreasoning across multiple videos with non-overlapping fields of view, andinference over spatial-temporal regions that are unobserved by any videocontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate thateven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lagsbehind human performance (78.61%) on geo-temporal reasoning. Moreover, ourcomprehensive analysis on GTR-Bench reveals three primary deficiencies ofcurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired byan imbalanced utilization of spatial-temporal context. (2) VLMs are weak intemporal forecasting, which leads to worse performance on temporal-emphasizedtasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency tocomprehend or align the map data with multi-view video inputs. We believeGTR-Bench offers valuable insights and opens up new opportunities for researchand applications in spatial-temporal intelligence. Benchmark and code will bereleased at https://github.com/X-Luffy/GTR-Bench.</description>
      <author>example@mail.com (Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng)</author>
      <guid isPermaLink="false">2510.07791v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>From Captions to Keyframes: KeyScore for Multimodal Frame Scoring and Video-Language Understanding</title>
      <link>http://arxiv.org/abs/2510.06509v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了KeyScore和STACFP两种方法，用于选择信息量大的关键帧，提高视频理解效率和准确性。KeyScore是一种基于字幕感知的帧评分方法，STACFP是一种时空自适应聚类方法，两者结合可减少无用帧，保留关键内容，实现更快更准确的推理。&lt;h4&gt;背景&lt;/h4&gt;选择信息量大的关键帧对于高效的视频理解至关重要，但现有方法往往依赖于启发式方法，忽略语义，或者产生冗余帧。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够结合语义相似性、时间代表性和上下文下降影响三种互补信号的帧评分方法，生成帧级别的重要性分数，用于训练关键帧提取器或指导视频语言模型。&lt;h4&gt;方法&lt;/h4&gt;1. KeyScore：字幕感知的帧评分方法，结合三种互补信号：与字幕的语义相似性、时间代表性和上下文下降影响。2. STACFP：时空自适应聚类方法，在长视频中生成多样且紧凑的帧提案。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准视频语言基准（MSRVTT、MSVD、DiDeMo）上的实验表明，结合STACFP和KeyScore与全帧处理相比可实现高达99%的帧减少，同时在视频文本检索、关键帧提取和动作识别任务中优于均匀8帧编码器。&lt;h4&gt;结论&lt;/h4&gt;通过专注于语义相关的帧，该方法提高了效率和性能，实现了可扩展的、基于字幕的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;选择信息量大的关键帧对于高效的视频理解至关重要，但现有方法往往依赖于启发式方法，忽略语义，或者产生冗余帧。我们提出了KeyScore，一种字幕感知的帧评分方法，结合了三种互补信号：与字幕的语义相似性、时间代表性和上下文下降影响。应用于大规模视频字幕数据集，KeyScore生成帧级别的重要性分数，使能够训练关键帧提取器或指导视频语言模型。为此，我们还提出了STACFP，一种时空自适应聚类方法，能够在长视频中生成多样且紧凑的帧提案。KeyScore和STACFP共同减少无用帧，同时保留关键内容，实现更快更准确的推理。我们在三个标准视频语言基准（MSRVTT、MSVD、DiDeMo）上的实验表明，与全帧处理相比，结合STACFP和KeyScore可以实现高达99%的帧减少，同时在视频文本检索、关键帧提取和动作识别任务中优于均匀8帧编码器。通过专注于语义相关的帧，我们的方法提高了效率和性能，实现了可扩展的、基于字幕的视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting informative keyframes is critical for efficient videounderstanding, yet existing approaches often rely on heuristics, ignoresemantics, or produce redundant frames. We propose KeyScore, a caption-awareframe scoring method that combines three complementary signals: semanticsimilarity to captions, temporal representativeness, and contextual dropimpact. Applied to large-scale video-caption datasets, KeyScore generatesframe-level importance scores that enable training keyframe extractors orguiding video-language models. To support this, we also propose STACFP, aSpatio-Temporal Adaptive Clustering method that generates diverse and compactframe proposals across long videos. Together, KeyScore and STACFP reduceuninformative frames while preserving critical content, resulting in faster andmore accurate inference. Our experiments on three standard video-languagebenchmarks (MSRVTT, MSVD, DiDeMo) show that combining STACFP and KeyScoreenables up to 99% frame reduction compared to full-frame processing, whileoutperforming uniform 8-frame encoders in video-text retrieval, keyframeextraction, and action recognition tasks. By focusing on semantically relevantframes, our method enhances both efficiency and performance, enabling scalableand caption-grounded video understanding.</description>
      <author>example@mail.com (Shih-Yao Lin, Sibendu Paul, Caren Chen)</author>
      <guid isPermaLink="false">2510.06509v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>InfiniHuman: Infinite 3D Human Creation with Precise Control</title>
      <link>http://arxiv.org/abs/2510.11650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM SIGGRAPH Asia 2025. Project website:  https://yuxuan-xue.com/infini-human&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了InfiniHuman框架，通过利用现有基础模型生成丰富注释的3D人体数据，解决了大规模人体数据集收集和标注成本高的问题。该框架包括InfiniHumanData（自动数据生成管道）和InfiniHumanGen（基于扩散的生成管道），能够生成高质量、可精确控制的3D人体化身。&lt;h4&gt;背景&lt;/h4&gt;生成真实且可控的3D人体化身是一项长期挑战，尤其是在覆盖广泛属性范围（如种族、年龄、服装风格和详细身体形状）时。捕获和注释用于训练生成模型的大规模人体数据集成本极高，且在规模和多样性上受到限制。&lt;h4&gt;目的&lt;/h4&gt;研究核心问题是：现有基础模型是否可以被提炼，以生成理论上无限的、丰富注释的3D人体数据？论文旨在提出一种方法，以最低成本和理论上无限的扩展性生成丰富注释的人体数据。&lt;h4&gt;方法&lt;/h4&gt;提出了InfiniHuman框架协同提炼现有模型；InfiniHumanData作为全自动管道利用视觉语言和图像生成模型创建大规模多模态数据集；InfiniHumanGen作为基于扩散的生成管道以文本、身体形状和服装资源为条件。数据集包含111K个身份，每个身份都有多粒度文本描述、多视图RGB图像、详细服装图像和SMPL身体形状参数。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究表明自动生成的身份与扫描渲染无法区分；InfiniHumanData包含111K个身份，覆盖前所未有的多样性；实验证明在视觉质量、生成速度和可控性方面显著优于最先进方法；通过实用且经济实惠的解决方案实现了在有效无限规模下的高质量化身生成和细粒度控制。&lt;h4&gt;结论&lt;/h4&gt;InfiniHuman框架提供了一种实用且经济实惠的解决方案，能够以有效无限的规模生成高质量、细粒度控制的3D人体化身。研究团队将公开自动数据生成管道、全面的InfiniHumanData数据集和InfiniHumanGen模型。&lt;h4&gt;翻译&lt;/h4&gt;生成真实且可控的3D人体化身是一项长期存在的挑战，尤其是在覆盖广泛的属性范围（如种族、年龄、服装风格和详细的身体形状）时。捕获和注释用于训练生成模型的大规模人体数据集成本极高，且在规模和多样性上受到限制。我们在本文中要解决的核心问题是：现有的基础模型是否可以被提炼，以生成理论上无限的、丰富注释的3D人体数据？我们介绍了InfiniHuman，一个协同提炼这些模型以最低成本和理论上无限的扩展性生成丰富注释的人体数据的框架。我们提出了InfiniHumanData，一个全自动管道，利用视觉语言和图像生成模型创建大规模多模态数据集。用户研究表明，我们自动生成的身份与扫描渲染无法区分。InfiniHumanData包含111K个身份，覆盖前所未有的多样性。每个身份都注释有多粒度文本描述、多视图RGB图像、详细服装图像和SMPL身体形状参数。基于此数据集，我们提出了InfiniHumanGen，一个基于扩散的生成管道，以文本、身体形状和服装资源为条件。InfiniHumanGen能够实现快速、真实且精确可控的化身生成。大量实验表明，在视觉质量、生成速度和可控性方面显著优于最先进的方法。我们的方法通过实用且经济实惠的解决方案，实现了在有效无限规模下的高质量化身生成和细粒度控制。我们将在https://yuxuan-xue.com/infini-human公开自动数据生成管道、全面的InfiniHumanData数据集和InfiniHumanGen模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决生成真实且可控制的3D人类化身（avatar）的挑战，特别是在覆盖广泛属性如种族、年龄、服装风格和身体形状时。这个问题在虚拟现实、数字时尚、游戏和社会远程呈现等领域至关重要，因为这些领域对逼真且可定制的个性化化身需求日益增长，而现有方法要么依赖成本高昂的真实扫描数据，要么生成质量有限或控制性不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过'蒸馏'现有基础模型来生成大规模3D人类数据，设计了一个完全自动化的框架。他们利用了多种现有技术：使用GPT-4o生成文本描述，微调FLUX模型生成正交视图的'扫描式'图像，借鉴虚拟试衣技术提取服装图像，使用SMPL模型表示人体形状和姿势，应用扩散模型生成一致的多视图图像，并基于OminiControl2进行微调实现高分辨率生成。这种方法整合了多个领域的前沿技术，形成了一个协同工作的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合和重新利用现有基础模型创建一个完全自动化的框架，生成大规模、丰富注释的3D人类数据，并基于这些数据训练生成模型以实现精确控制。整体流程分为两部分：1) 数据生成（InfiniHumanData）：包括多粒度文本描述生成、正交文本到图像转换、虚拟试衣提取服装图像、单目身体拟合获取SMPL参数、正交多视图扩散生成高分辨率多视图图像；2) 生成模型（InfiniHumanGen）：包括Gen-Schnell（快速生成3D高斯点云）和Gen-HRes（高分辨率纹理网格生成），两者都支持基于文本、身体形状和服装图像的条件化生成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) InfiniHuman框架，实现完全自动化的无限3D人类数据生成；2) InfiniHumanData数据集，包含111K个多样化身份，具有前所未有的种族、年龄、服装风格等多样性；3) InfiniHumanGen生成模型，提供Gen-Schnell和Gen-HRes两种互补模型，支持精确控制。相比之前工作，该方法解决了视觉质量、生成速度和属性可控性的局限性，提供了对服装的精确控制，生成的身份在视觉上与真实扫描无法区分，数据集规模和多样性远超现有公开数据集，生成速度比现有高分辨率方法快8倍以上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfiniHuman通过整合现有基础模型创建了一个完全自动化的框架，能够生成大规模、多样化的3D人类数据集，并基于此实现了高质量、高速度且具有精确控制的3D人类化身生成，从而降低了高质量化身创建的门槛并实现了无限规模的扩展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763815&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic and controllable 3D human avatars is a long-standingchallenge, particularly when covering broad attribute ranges such as ethnicity,age, clothing styles, and detailed body shapes. Capturing and annotatinglarge-scale human datasets for training generative models is prohibitivelyexpensive and limited in scale and diversity. The central question we addressin this paper is: Can existing foundation models be distilled to generatetheoretically unbounded, richly annotated 3D human data? We introduceInfiniHuman, a framework that synergistically distills these models to producerichly annotated human data at minimal cost and with theoretically unlimitedscalability. We propose InfiniHumanData, a fully automatic pipeline thatleverages vision-language and image generation models to create a large-scalemulti-modal dataset. User study shows our automatically generated identitiesare undistinguishable from scan renderings. InfiniHumanData contains 111Kidentities spanning unprecedented diversity. Each identity is annotated withmulti-granularity text descriptions, multi-view RGB images, detailed clothingimages, and SMPL body-shape parameters. Building on this dataset, we proposeInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, bodyshape, and clothing assets. InfiniHumanGen enables fast, realistic, andprecisely controllable avatar generation. Extensive experiments demonstratesignificant improvements over state-of-the-art methods in visual quality,generation speed, and controllability. Our approach enables high-quality avatargeneration with fine-grained control at effectively unbounded scale through apractical and affordable solution. We will publicly release the automatic datageneration pipeline, the comprehensive InfiniHumanData dataset, and theInfiniHumanGen models at https://yuxuan-xue.com/infini-human.</description>
      <author>example@mail.com (Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll)</author>
      <guid isPermaLink="false">2510.11650v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</title>
      <link>http://arxiv.org/abs/2510.11576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Being reviewed for WHISPERS conference ( Workshop on Hyperspectral  Image and Signal Processing: Evolution in Remote Sensing )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了三种基础模型在超光谱作物制图中的性能，发现SpectralEarth预训练的Vision Transformer表现最佳，达到93.5%的总体准确率，强调了模型架构对跨区域泛化能力的重要性。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变地球观测领域，但它们在超光谱作物制图中的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;基准测试三种基础模型用于超光谱作物制图，评估它们在不同地理区域和传感器平台上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;测试了三种基础模型（HyperSigma、DOFA和在SpectralEarth数据集上预训练的Vision Transformer），在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能测量包括总体准确率(OA)、平均准确率(AA)和F1分数。&lt;h4&gt;主要发现&lt;/h4&gt;HyperSigma达到34.5%的OA（±1.8%），DOFA达到62.6%的OA（±3.5%），SpectralEarth模型达到93.5%的OA（±0.8%）。从头开始训练的紧凑型SpectralEarth变体达到了91%的性能，突显了模型架构对于在地理区域和传感器平台之间强泛化能力的重要性。&lt;h4&gt;结论&lt;/h4&gt;这些结果为操作超光谱作物制图的基础模型提供了系统评估，并概述了未来模型开发的方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在改变地球观测，但它们在超光谱作物制图中的潜力仍然未被充分探索。本研究使用超光谱图像对三种基础模型进行了基准测试，用于谷物作物制图：HyperSigma、DOFA以及在SpectralEarth数据集上预训练的Vision Transformer（一个大的多时相超光谱档案）。模型在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能通过总体准确率(OA)、平均准确率(AA)和F1分数来衡量。HyperSigma实现了34.5%的OA（±1.8%），DOFA达到62.6%（±3.5%），而SpectralEarth模型实现了93.5%的OA（±0.8%）。从头开始训练的紧凑型SpectralEarth变体达到了91%，突显了模型架构对于在地理区域和传感器平台之间强泛化能力的重要性。这些结果为操作超光谱作物制图的基础模型提供了系统评估，并概述了未来模型开发的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are transforming Earth observation, but their potential forhyperspectral crop mapping remains underexplored. This study benchmarks threefoundation models for cereal crop mapping using hyperspectral imagery:HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarthdataset (a large multitemporal hyperspectral archive). Models were fine-tunedon manually labeled data from a training region and evaluated on an independenttest region. Performance was measured with overall accuracy (OA), averageaccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved91%, highlighting the importance of model architecture for stronggeneralization across geographic regions and sensor platforms. These resultsprovide a systematic evaluation of foundation models for operationalhyperspectral crop mapping and outline directions for future model development.</description>
      <author>example@mail.com (Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix)</author>
      <guid isPermaLink="false">2510.11576v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>How many samples to label for an application given a foundation model? Chest X-ray classification study</title>
      <link>http://arxiv.org/abs/2510.11553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了胸部X光分类中基础模型对标注数据的需求量，发现XrayCLIP和XraySigLIP模型在显著少于传统ResNet-50基线模型的情况下仍能实现高性能，且仅需50个标注样本即可准确预测最终性能表现。&lt;h4&gt;背景&lt;/h4&gt;胸部X光分类对医学诊断至关重要，但传统方法需要大量标注数据，资源消耗大。基础模型虽可减少对标注数据的依赖，但具体需要多少标注样本尚不明确。&lt;h4&gt;目的&lt;/h4&gt;系统性评估使用幂律拟合方法预测达到特定ROC-AUC阈值所需训练样本量的可行性，旨在确定胸部X光分类任务中基础模型的最优标注样本数量。&lt;h4&gt;方法&lt;/h4&gt;研究测试了多种病理学和基础模型，应用幂律拟合来预测达到特定性能阈值所需的训练样本量，并验证了从少量标注样本(仅50个)的学习曲线斜率预测最终性能的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;XrayCLIP和XraySigLIP模型相比ResNet-50基线模型，在显著更少的标注样本下实现了同等或更好的性能；仅使用50个标注病例的学习曲线就能准确预测模型最终的性能平台期。&lt;h4&gt;结论&lt;/h4&gt;研究结果使医学影像从业者能够通过仅标注针对目标性能所必需的样本，有效降低数据标注成本，优化资源分配。&lt;h4&gt;翻译&lt;/h4&gt;胸部X光分类至关重要但资源密集，通常需要大量标注数据才能实现准确诊断。基础模型可以减少这种依赖，但需要多少标注样本尚不清楚。我们系统性评估了使用幂律拟合来预测达到特定ROC-AUC阈值所需训练样本量的方法。通过测试多种病理学和基础模型，我们发现XrayCLIP和XraySigLIP在显著少于ResNet-50基线的标注样本下实现了强大性能。重要的是，仅从50个标注病例的学习曲线斜率就能准确预测最终性能平台期。我们的结果使从业者能够通过仅标注针对目标性能所必需的样本来最小化标注成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chest X-ray classification is vital yet resource-intensive, typicallydemanding extensive annotated data for accurate diagnosis. Foundation modelsmitigate this reliance, but how many labeled samples are required remainsunclear. We systematically evaluate the use of power-law fits to predict thetraining size necessary for specific ROC-AUC thresholds. Testing multiplepathologies and foundation models, we find XrayCLIP and XraySigLIP achievestrong performance with significantly fewer labeled examples than a ResNet-50baseline. Importantly, learning curve slopes from just 50 labeled casesaccurately forecast final performance plateaus. Our results enablepractitioners to minimize annotation costs by labeling only the essentialsamples for targeted performance.</description>
      <author>example@mail.com (Nikolay Nechaev, Evgenia Przhezdzetskaya, Viktor Gombolevskiy, Dmitry Umerenkov, Dmitry Dylov)</author>
      <guid isPermaLink="false">2510.11553v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification</title>
      <link>http://arxiv.org/abs/2510.11537v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure. Submitted to VLSP 2025 and reviewed&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TextGraphFuseGAT的新型神经网络架构，结合了预训练的transformer编码器和图注意力网络，用于标记级分类任务，在多个越南语数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的标记级分类模型在处理标记间复杂关系时存在局限性，特别是在特定领域如医疗、COVID-19等需要精确识别实体类型的任务中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获标记间依赖关系并结合预训练语义特征的模型，以提高标记级分类任务在多个领域的性能。&lt;h4&gt;方法&lt;/h4&gt;提出TextGraphFuseGAT模型，在PhoBERT生成的标记嵌入上构建全连接图，使用GAT层捕获标记间依赖关系，并应用Transformer风格的自注意力层增强上下文化，最后通过分类头进行序列标注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个越南语基准数据集（PhoNER-COVID19、PhoDisfluency和VietMed-NER）上的实验表明，该方法始终优于强基线模型，包括仅使用transformer的模型和混合神经网络模型。&lt;h4&gt;结论&lt;/h4&gt;将预训练语义特征与基于图的建模相结合是提高跨领域标记分类性能的有效方法，特别是在处理具有专业词汇和领域特定表达的数据时。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为TextGraphFuseGAT的新型神经架构，该架构集成了预训练的transformer编码器（PhoBERT）和图注意力网络，用于标记级分类任务。所提出的模型在PhoBERT生成的标记嵌入上构建了一个全连接图，使GAT层能够捕获丰富的标记间依赖关系，而不仅仅是顺序上下文建模。为了进一步增强上下文化，在图增强的嵌入上应用了Transformer风格的自注意力层。最终的标记表示通过分类头进行序列标注。我们在三个越南语基准数据集上评估了我们的方法：PhoNER-COVID19用于COVID-19领域的命名实体识别，PhoDisfluency用于言语不流畅性检测，以及VietMed-NER用于医疗领域NER。VietMed-NER是第一个越南语医疗口语NER数据集，包含18种从真实医疗语音转录中收集的实体类型，并使用BIO标记方案进行标注。其专业词汇和领域特定表达使其成为标记级分类模型的一个具有挑战性的基准。实验结果表明，我们的方法始终优于强基线模型，包括仅使用transformer的模型和混合神经网络模型（如BiLSTM + CNN + CRF），证实了结合预训练语义特征和基于图的关系建模对提高跨领域标记分类的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel neural architecture named TextGraphFuseGAT, whichintegrates a pretrained transformer encoder (PhoBERT) with Graph AttentionNetworks for token-level classification tasks. The proposed model constructs afully connected graph over the token embeddings produced by PhoBERT, enablingthe GAT layer to capture rich inter-token dependencies beyond those modeled bysequential context alone. To further enhance contextualization, aTransformer-style self-attention layer is applied on top of the graph-enhancedembeddings. The final token representations are passed through a classificationhead to perform sequence labeling. We evaluate our approach on three Vietnamesebenchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19domain, PhoDisfluency for speech disfluency detection, and VietMed-NER formedical-domain NER. VietMed-NER is the first Vietnamese medical spoken NERdataset, featuring 18 entity types collected from real-world medical speechtranscripts and annotated with the BIO tagging scheme. Its specializedvocabulary and domain-specific expressions make it a challenging benchmark fortoken-level classification models. Experimental results show that our methodconsistently outperforms strong baselines, including transformer-only andhybrid neural models such as BiLSTM + CNN + CRF, confirming the effectivenessof combining pretrained semantic features with graph-based relational modelingfor improved token classification across multiple domains.</description>
      <author>example@mail.com (Ba-Quang Nguyen)</author>
      <guid isPermaLink="false">2510.11537v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation</title>
      <link>http://arxiv.org/abs/2510.11346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at ICCV Workshops 2025, "The 4th Workshop  on What is Next in Multimodal Foundation Models?" (MMFM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用未标记领域数据训练ControlNet的方法，通过引入不确定性概念到控制机制中，使模型能够创建具有高不确定性的目标领域标注数据，显著提高了分割结果，无需额外监督。&lt;h4&gt;背景&lt;/h4&gt;生成模型是创建高质量图像数据的有价值工具。ControlNet等受控扩散模型已允许创建标记分布，可用于增强原始训练分布。然而，ControlNet倾向于重现原始训练分布，限制了增强效果。以视网膜OCT为例，高质量的Spectralis图像有真实分割可用于训练，但Home-OCT设备产生的图像质量较低且存在较大域偏移，使得现成的分割网络无法应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法利用未标记领域数据训练ControlNet，通过引入不确定性概念，创建具有高不确定性的目标领域标注数据，解决域偏移问题，提高分割性能。&lt;h4&gt;方法&lt;/h4&gt;将不确定性概念引入控制机制中，不确定性表示给定图像不属于下游任务（如分割）的训练分布。最终网络结合两种控制：来自未标记数据集的不确定性控制和来自标记数据集的语义控制。这种方法允许创建来自目标领域的具有高不确定性的标注数据，即来自未标记分布的带标签合成数据。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ControlNet能够创建来自Home-OCT域的带注释图像，显著提高了分割结果，无需额外监督。与风格迁移相比，不确定性引导能够实现任意的域偏移，而无需严格学习图像风格，这一点在交通场景实验中也得到了验证。&lt;h4&gt;结论&lt;/h4&gt;通过引入不确定性概念到ControlNet的控制机制中，可以利用未标记领域数据训练模型，创建高质量的标注数据，有效解决域偏移问题，显著提高分割性能，且无需额外监督。&lt;h4&gt;翻译&lt;/h4&gt;生成模型是用于高质量图像数据受控创建的有价值工具。像ControlNet这样的受控扩散模型已允许创建标记分布。当训练判别模型（如语义分割）时，这类合成数据可以增强原始训练分布。然而，这种增强效果有限，因为ControlNet倾向于重现原始训练分布。这项工作引入了一种利用未标记领域数据训练ControlNet的方法，通过将不确定性概念引入控制机制中。不确定性表示给定图像不属于下游任务（如分割）的训练分布。因此，最终网络涉及两种控制：来自未标记数据集的不确定性控制和来自标记数据集的语义控制。所得到的ControlNet允许我们创建来自目标领域的高不确定性标注数据，即来自未标记分布的带标签合成数据。在我们的场景中，我们考虑视网膜OCT，通常高质量的Spectralis图像有给定的真实分割，可用于训练分割网络。然而，Home-OCT设备的最新发展产生了质量较低且存在较大域偏移的视网膜OCT，使得现成的分割网络无法应用于此类数据。使用所提出的方法合成来自Home-OCT域的带注释图像弥合了这一差距，并在不添加任何额外监督的情况下显著提高了分割结果。与风格迁移相比，不确定性引导的优势很明显：它能够实现任意的域偏移，而无需严格学习图像风格。这一点在交通场景实验中也得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Models are a valuable tool for the controlled creation ofhigh-quality image data. Controlled diffusion models like the ControlNet haveallowed the creation of labeled distributions. Such synthetic datasets canaugment the original training distribution when discriminative models, likesemantic segmentation, are trained. However, this augmentation effect islimited since ControlNets tend to reproduce the original training distribution.  This work introduces a method to utilize data from unlabeled domains to trainControlNets by introducing the concept of uncertainty into the controlmechanism. The uncertainty indicates that a given image was not part of thetraining distribution of a downstream task, e.g., segmentation. Thus, two typesof control are engaged in the final network: an uncertainty control from anunlabeled dataset and a semantic control from the labeled dataset. Theresulting ControlNet allows us to create annotated data with high uncertaintyfrom the target domain, i.e., synthetic data from the unlabeled distributionwith labels. In our scenario, we consider retinal OCTs, where typicallyhigh-quality Spectralis images are available with given ground truthsegmentations, enabling the training of segmentation networks. The recentdevelopment in Home-OCT devices, however, yields retinal OCTs with lowerquality and a large domain shift, such that out-of-the-pocket segmentationnetworks cannot be applied for this type of data. Synthesizing annotated imagesfrom the Home-OCT domain using the proposed approach closes this gap and leadsto significantly improved segmentation results without adding any furthersupervision. The advantage of uncertainty-guidance becomes obvious whencompared to style transfer: it enables arbitrary domain shifts without anystrict learning of an image style. This is also demonstrated in a traffic sceneexperiment.</description>
      <author>example@mail.com (Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova)</author>
      <guid isPermaLink="false">2510.11346v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Protein as a Second Language for LLMs</title>
      <link>http://arxiv.org/abs/2510.11188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main paper: 9 pages, 6 figures. With references and appendix: 18  pages, 9 figures total. Submitted to ICLR 2026 (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出'蛋白质作为第二语言'框架，将氨基酸序列重新表述为大型语言模型可解释的符号语言，通过自适应构建序列-问题-答案三元组在零样本设置中揭示蛋白质功能线索，无需额外训练。&lt;h4&gt;背景&lt;/h4&gt;解析未知蛋白质序列的功能是具有广泛科学影响的基本挑战，但现有方法大多依赖于特定任务适配器或大规模监督微调。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖特定任务适配器或大规模监督微调的蛋白质功能解析方法。&lt;h4&gt;方法&lt;/h4&gt;引入'蛋白质作为第二语言'框架，将氨基酸序列重新表述为新型符号语言中的句子，大型语言模型可通过上下文示例解释。该方法自适应构建序列-问题-答案三元组，在零样本设置中揭示功能线索。为此，整理了包含79,926个蛋白质-QA实例的双语语料库，涵盖属性预测、描述性理解和扩展推理。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在各种开源大型语言模型和GPT-4上取得了一致的改进，ROUGE-L最高提升17.2%（平均+7%），甚至超过了微调的蛋白质特定语言模型。&lt;h4&gt;结论&lt;/h4&gt;当用蛋白质作为语言的线索引导时，通用大型语言模型可以超越领域专用模型，为基础模型中的蛋白质理解提供了可扩展的途径。&lt;h4&gt;翻译&lt;/h4&gt;解析未知蛋白质序列的功能是一个具有广泛科学影响的基本挑战，但大多数现有方法依赖于特定任务的适配器或大规模监督微调。我们引入了'蛋白质作为第二语言'框架，将氨基酸序列重新表述为一种新型符号语言中的句子，大型语言模型可以通过上下文示例来解释。我们的方法自适应地构建序列-问题-答案三元组，在零样本设置中揭示功能线索，无需任何进一步训练。为此，我们整理了一个包含79,926个蛋白质-QA实例的双语语料库，涵盖属性预测、描述性理解和扩展推理。实验上，我们的方法在各种开源大型语言模型和GPT-4上取得了一致的改进，ROUGE-L最高提升17.2%（平均+7%），甚至超过了微调的蛋白质特定语言模型。这些结果表明，当用蛋白质作为语言的线索引导时，通用大型语言模型可以超越领域专用模型，为基础模型中的蛋白质理解提供了可扩展的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deciphering the function of unseen protein sequences is a fundamentalchallenge with broad scientific impact, yet most existing methods depend ontask-specific adapters or large-scale supervised fine-tuning. We introduce the"Protein-as-Second-Language" framework, which reformulates amino-acid sequencesas sentences in a novel symbolic language that large language models caninterpret through contextual exemplars. Our approach adaptively constructssequence-question-answer triples that reveal functional cues in a zero-shotsetting, without any further training. To support this process, we curate abilingual corpus of 79,926 protein-QA instances spanning attribute prediction,descriptive understanding, and extended reasoning. Empirically, our methoddelivers consistent gains across diverse open-source LLMs and GPT-4, achievingup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tunedprotein-specific language models. These results highlight that generic LLMs,when guided with protein-as-language cues, can outperform domain-specializedmodels, offering a scalable pathway for protein understanding in foundationmodels.</description>
      <author>example@mail.com (Xinhui Chen, Zuchao Li, Mengqi Gao, Yufeng Zhang, Chak Tou Leong, Haoyang Li, Jiaqi Chen)</author>
      <guid isPermaLink="false">2510.11188v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.11176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为G2L框架的新策略，通过知识蒸馏技术使大规模病理学模型（仅占giga-scale模型15%参数）在癌症特定任务上达到与giga-scale模型相当的性能，同时显著降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，扩大训练数据规模、增加癌症类型多样性和增大模型尺寸可提升病理学基础模型性能。然而，giga-scale基础模型（训练于数十万张玻片，覆盖数十种癌症类型，含数十亿参数）因开发和部署中的巨大计算成本，实际应用面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新策略(G2L框架)，使大规模基础模型（仅占giga-scale模型15%的参数）在癌症特定任务上达到与giga-scale模型相当的性能水平。&lt;h4&gt;方法&lt;/h4&gt;应用知识蒸馏技术，将giga-scale模型的能力转移到大规模模型，仅使用目标癌症（如乳腺癌、前列腺癌等）的1K张病理玻片进行知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;蒸馏后的模型在多个基准测试中优于同规模的最先进模型，甚至在某些测试中超过了giga-scale教师模型和huge-scale模型；蒸馏模型表现出更高的鲁棒性指数，对来自多个机构的图像变化具有更强的适应能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的蒸馏方法是一种数据和参数高效的方式，可以在癌症特定应用中达到giga-scale级别的性能，同时避免过高的计算负担。&lt;h4&gt;翻译&lt;/h4&gt;近期病理学基础模型研究表明，扩展训练数据、增加癌症类型多样性和提升模型尺寸能持续改善模型性能。然而，giga-scale基础模型（训练于覆盖数十种癌症类型的数十万张玻片，包含数十亿参数）因开发和部署中的巨大计算成本，给实际应用带来重大挑战。本研究提出了一种名为G2L框架的新策略，使大规模基础模型（仅占giga-scale模型15%的参数）在癌症特定任务上达到与giga-scale模型相当的性能水平。我们的方法应用知识蒸馏技术，将giga-scale模型的能力转移到大规模模型，仅使用目标癌症（如乳腺癌、前列腺癌等）的1K张病理玻片。所得蒸馏模型不仅在多个基准测试中优于同规模的最先进模型，而且有趣的是，在某些基准测试中甚至超过了giga-scale教师模型和huge-scale模型。此外，蒸馏模型表现出更高的鲁棒性指数，表明对来自多个机构的图像变化具有更强的适应能力。这些发现表明，针对大规模模型提出的蒸馏方法是实现giga-scale级别性能的数据和参数高效途径，且不会带来过高的计算负担。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in pathology foundation models have shown that scalingtraining data, diversifying cancer types, and increasing model sizeconsistently improve their performance. However, giga-scale foundation models,which are trained on hundreds of thousands of slides covering tens of cancertypes and contain billions of parameters, pose significant challenges forpractical use due to their tremendous computational costs in both developmentand deployment. In this work, we present a novel strategy, named the G2Lframework, to increase the performance of large-scale foundation models, whichconsist of only $15\%$ of the parameters of giga-scale models, to a comparableperformance level of giga-scale models in cancer-specific tasks. Our approachapplies knowledge distillation, transferring the capabilities of a giga-scalemodel to a large-scale model, using just 1K pathology slides of a target cancer(e.g., breast, prostate, etc.). The resulting distilled model not onlyoutperformed state-of-the-art models of the same size (i.e., large-scale)across several benchmarks but also, interestingly, surpassed the giga-scaleteacher and huge-scale models in some benchmarks. In addition, the distilledmodel exhibited a higher robustness index, indicating improved resilience toimage variations originating from multiple institutions. These findings suggestthat the proposed distillation approach for a large-scale model is a data- andparameter-efficient way to achieve giga-scale-level performance forcancer-specific applications without prohibitive computational burden.</description>
      <author>example@mail.com (Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin)</author>
      <guid isPermaLink="false">2510.11176v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times</title>
      <link>http://arxiv.org/abs/2510.11138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对Foundation Models (FMs)驱动的FMware生态系统进行了首次大规模分析，研究其应用领域、开发挑战和问题解决需求，为改进FMware工具、工作流程和社区支持提供指导。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models如OpenAI的GPT正在改变软件工程实践，催生了FMware（围绕这些模型构建的应用和基础设施）。FMware支持代码生成、自然语言交互、知识集成和多模态内容创建，但其设计、实现和演化在云平台和本地部署环境中带来了新的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究FMware在云平台和开源仓库中的开发情况，通过三个重点领域进行实证分析：(1)最常见的应用领域，(2)开发者遇到的关键挑战，(3)需要最大努力解决的问题类型。&lt;h4&gt;方法&lt;/h4&gt;从GitHub仓库和领先的FMware平台（包括HuggingFace、GPTStore、Ora和Poe）收集数据进行实证调查分析。&lt;h4&gt;主要发现&lt;/h4&gt;FMware强烈关注教育、内容创建和商业战略；在内存管理、依赖处理和tokenizer配置方面存在持久技术挑战；GitHub上最常报告的是错误报告和核心功能问题；最耗时的解决方案是代码审查、相似性搜索和提示模板设计。&lt;h4&gt;结论&lt;/h4&gt;通过揭示开发者的实践和痛点，研究指出了改进FMware工具、工作流程和社区支持的机会，提供了指导FMware开发未来的可行见解。&lt;h4&gt;翻译&lt;/h4&gt;基础模型（FMs），如OpenAI的GPT，正在从根本上改变软件工程的实践，使能够开发围绕这些模型的FMware——应用和基础设施。FMware系统现在支持代码生成、自然语言交互、知识集成和多模态内容创建等任务，凸显了它们对当前软件工程工作流程的颠覆性影响。然而，FMware的设计、实现和演化带来了显著的新挑战，特别是在云平台和本地部署平台上，这些平台的目标、流程和工具往往与传统软件开发不同。据我们所知，这是首次对云平台和开源仓库中的FMware开发进行大规模分析。我们通过三个重点领域对FMware生态系统进行了实证研究：(1)FMware最常见的应用领域，(2)开发者遇到的关键挑战，(3)需要最大努力解决的问题类型。我们的分析借鉴了GitHub仓库以及领先的FMware平台（包括HuggingFace、GPTStore、Ora和Poe）的数据。我们的研究结果显示，FMware强烈关注教育、内容创建和商业战略，同时在内存管理、依赖处理和tokenizer配置方面存在持久的技术挑战。在GitHub上，错误报告和核心功能问题是最常报告的问题，而代码审查、相似性搜索和提示模板设计是最耗时的解决方案。通过揭示开发者的实践和痛点，这项研究指出了改进FMware工具、工作流程和社区支持的机会，并提供了可行的见解，帮助指导FMware开发的未来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transformingthe practice of software engineering by enabling the development of\emph{FMware} -- applications and infrastructures built around these models.FMware systems now support tasks such as code generation, natural-languageinteraction, knowledge integration, and multi-modal content creation,underscoring their disruptive impact on current software engineering workflows.However, the design, implementation, and evolution of FMware presentsignificant new challenges, particularly across cloud-based and on-premiseplatforms where goals, processes, and tools often diverge from those oftraditional software development.  To our knowledge, this is the first large-scale analysis of FMwaredevelopment across both cloud-based platforms and open-source repositories. Weempirically investigate the FMware ecosystem through three focus areas: (1) themost common application domains of FMware, (2) the key challenges developersencounter, and (3) the types of issues that demand the greatest effort toresolve. Our analysis draws on data from GitHub repositories and from leadingFMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findingsreveal a strong focus on education, content creation, and business strategy,alongside persistent technical challenges in memory management, dependencyhandling, and tokenizer configuration. On GitHub, bug reports and corefunctionality issues are the most frequently reported problems, while codereview, similarity search, and prompt template design are the mosttime-consuming to resolve.  By uncovering developer practices and pain points, this study points toopportunities to improve FMware tools, workflows, and community support, andprovides actionable insights to help guide the future of FMware development.</description>
      <author>example@mail.com (Zitao Wang, Zhimin Zhao, Michael W. Godfrey)</author>
      <guid isPermaLink="false">2510.11138v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Improving AI Efficiency in Data Centres by Power Dynamic Response</title>
      <link>http://arxiv.org/abs/2510.11119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于创新方法的AI数据中心电力管理解决方案的能力和局限性，提出将部分输入电力动态化以提高可持续性。&lt;h4&gt;背景&lt;/h4&gt;人工智能近年来在大型语言模型和基础模型等复杂模型的推动下加速发展，但AI数据中心对电力需求极大，其电力管理问题对环境和可持续发展造成影响。&lt;h4&gt;目的&lt;/h4&gt;研究基于创新方法的AI数据中心电力管理解决方案的能力和局限性，即让部分输入电力与数据计算功能使用的电力一样动态化。&lt;h4&gt;方法&lt;/h4&gt;通过分析全球多个数据平台的电力趋势，量化比较被动设备和主动设备在计算增益、能源效率、资本支出减少和管理成本方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;这种动态电力管理策略代表了AI数据中心电力管理的一种范式转变，有潜力显著提高AI超算的可持续性。&lt;h4&gt;结论&lt;/h4&gt;该策略可以增强AI在环境、财务和社会领域的影响，促进AI的可持续发展。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的稳定增长近年来有所加速，这得益于大型语言模型和基础模型等复杂模型的发展。确保强大可靠的基础设施对于充分发挥AI的潜力至关重要。然而，AI数据中心极其耗电，使其电力管理问题成为焦点，特别是它们对环境和可持续发展的影响。在这项工作中，我们研究了基于创新方法的AI数据中心电力管理解决方案的能力和局限性，即使部分输入电源与用于数据计算功能的电源一样动态化。通过分析来自全球多个数据平台的电力趋势，我们以计算增益、能源效率、资本支出减少和管理成本为量化指标，比较了被动设备和主动设备的性能。这种策略代表了AI数据中心电力管理的一种范式转变，有潜力显著提高AI超算的可持续性，增强其在环境、财务和社会领域的影响力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The steady growth of artificial intelligence (AI) has accelerated in therecent years, facilitated by the development of sophisticated models such aslarge language models and foundation models. Ensuring robust and reliable powerinfrastructures is fundamental to take advantage of the full potential of AI.However, AI data centres are extremely hungry for power, putting the problem oftheir power management in the spotlight, especially with respect to theirimpact on environment and sustainable development. In this work, we investigatethe capacity and limits of solutions based on an innovative approach for thepower management of AI data centres, i.e., making part of the input power asdynamic as the power used for data-computing functions. The performance ofpassive and active devices are quantified and compared in terms ofcomputational gain, energy efficiency, reduction of capital expenditure, andmanagement costs by analysing power trends from multiple data platformsworldwide. This strategy, which identifies a paradigm shift in the AI datacentre power management, has the potential to strongly improve thesustainability of AI hyperscalers, enhancing their footprint on environmental,financial, and societal fields.</description>
      <author>example@mail.com (Andrea Marinoni, Sai Shivareddy, Pietro Lio', Weisi Lin, Erik Cambria, Clare Grey)</author>
      <guid isPermaLink="false">2510.11119v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts</title>
      <link>http://arxiv.org/abs/2510.11028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by PRCV&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的两阶段框架，用于工业异常检测中的零样本异常分割任务，有效结合了CLIP的异常定位能力和SAM的边界感知能力，解决了基础模型在下游任务中的引导问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型展现出强大的泛化能力，为零样本异常分割任务带来了新解决方案，但正确引导这些模型解决下游任务仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效利用CLIP和SAM优势的两阶段框架，提高零样本异常分割的准确性和精确性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出Co-Feature Point Prompt Generation (PPG)模块，协同使用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而非整个对象；2) 引入Cascaded Prompts for SAM (CPS)模块，采用混合提示与SAM轻量级解码器级联，优化分割结果并减少边界粗糙和孤立噪声。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上验证了该方法的有效性，取得了最先进的零样本异常分割结果，特别是在Visa数据集上，F1-max和AP指标分别比最先进方法高出10.3%和7.7%。&lt;h4&gt;结论&lt;/h4&gt;该两阶段框架成功解决了SAM在异常分割中的局限性，通过协同利用CLIP和SAM的优势，实现了异常区域的精确分割，为工业异常检测提供了有效的零样本解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近，基础模型展现出的强大泛化能力为零样本异常分割任务带来了新的解决方案。然而，正确引导这些基础模型解决下游任务仍然是一个挑战。本文提出了一种用于工业异常检测中零样本异常分割任务的新型两阶段框架。该框架出色地利用了CLIP的强大异常定位能力和SAM的边界感知能力。1) 为缓解SAM倾向于目标分割的问题，我们提出了Co-Feature Point Prompt Generation (PPG)模块。该模块协同使用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而非整个对象。2) 为进一步优化SAM的分割结果并缓解边界粗糙和孤立噪声，我们引入了Cascaded Prompts for SAM (CPS)模块。该模块采用与SAM轻量级解码器级联的混合提示，实现异常区域的精确分割。在多个数据集上的一致实验验证表明，我们的方法取得了最先进的零样本异常分割结果。特别值得注意的是，我们在Visa数据集上的表现，在F1-max和AP指标上分别比最先进方法高出10.3%和7.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, the powerful generalization ability exhibited by foundation modelshas brought forth new solutions for zero-shot anomaly segmentation tasks.However, guiding these foundation models correctly to address downstream tasksremains a challenge. This paper proposes a novel two-stage framework, forzero-shot anomaly segmentation tasks in industrial anomaly detection. Thisframework excellently leverages the powerful anomaly localization capability ofCLIP and the boundary perception ability of SAM.(1) To mitigate SAM'sinclination towards object segmentation, we propose the Co-Feature Point PromptGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM togenerate positive and negative point prompts, guiding SAM to focus onsegmenting anomalous regions rather than the entire object. (2) To furtheroptimize SAM's segmentation results and mitigate rough boundaries and isolatednoise, we introduce the Cascaded Prompts for SAM (CPS) module. This moduleemploys hybrid prompts cascaded with a lightweight decoder of SAM, achievingprecise segmentation of anomalous regions. Across multiple datasets, consistentexperimental validation demonstrates that our approach achievesstate-of-the-art zero-shot anomaly segmentation results. Particularlynoteworthy is our performance on the Visa dataset, where we outperform thestate-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and APmetrics, respectively.</description>
      <author>example@mail.com (Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu)</author>
      <guid isPermaLink="false">2510.11028v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2510.11005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种前景感知频谱分割(FASS)框架，用于解决医学图像中肿瘤和相邻正常组织分割的挑战，特别是在复杂、低对比度背景下。&lt;h4&gt;背景&lt;/h4&gt;在医学图像中准确分割肿瘤和相邻正常组织对手术规划和肿瘤分期至关重要。基础模型在分割任务中表现良好，但在复杂、低对比度背景下往往难以聚焦前景区域，因为某些恶性肿瘤与正常器官相似，难以区分。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂条件下提高分割鲁棒性和精细结构识别能力的分割框架。&lt;h4&gt;方法&lt;/h4&gt;FASS框架包含三个主要模块：1) 前景感知模块，增强背景与整个体积空间之间的区别；2) 基于小波变换的特征级频率增强模块，提取判别性高频特征以增强边界识别；3) 边缘约束模块，保持分割边界的几何连续性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个医学数据集上的实验表明，该框架在所有指标上表现优越，特别是在复杂条件下的鲁棒性和精细结构识别方面效果显著。&lt;h4&gt;结论&lt;/h4&gt;该框架显著提高了低对比度图像的分割效果，为更多样化和复杂的医学成像场景中的应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;医学图像中肿瘤和相邻正常组织的准确分割对于手术规划和肿瘤分期至关重要。尽管基础模型在分割任务中通常表现良好，但它们往往难以在复杂、低对比度背景下聚焦前景区域，因为某些恶性肿瘤与正常器官相似，使上下文区分复杂化。为解决这些挑战，我们提出了前景感知频谱分割(FASS)框架。首先，我们引入了前景感知模块来增强背景与整个体积空间之间的区别，使模型能够更有效地集中注意力在目标区域。其次，基于小波变换的特征级频率增强模块提取判别性高频特征，以增强边界识别和细节感知。最后，我们引入了边缘约束模块来保持分割边界的几何连续性。在多个医学数据集上的大量实验证明，所有指标上均表现出优越性能，验证了我们框架的有效性，特别是在复杂条件下的鲁棒性和精细结构识别方面。我们的框架显著提高了低对比度图像的分割效果，为更多样化和复杂的医学成像场景中的应用铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决低对比度腹部医学图像中肿瘤和正常组织的准确分割问题。这个问题在现实中非常重要，因为准确的肿瘤分割对手术计划和肿瘤分期至关重要，直接影响治疗效果和患者生存率。同时，手动标注非常耗时费力，需要大量临床专业知识，特别是在处理复杂解剖结构或模糊边界时。低对比度图像中肿瘤与周围组织灰度值相似，边界模糊，使得现有分割方法难以准确区分肿瘤和正常组织，导致分割结果不完整或边界断裂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了低对比度医学图像分割的关键挑战，识别出现有方法在处理复杂背景和相似组织时的局限性。他们从频率域角度思考解决方案，利用频率增强来放大高频成分，使模型能更好地捕捉细微特征。设计过程中，作者借鉴了深度学习在医学图像分割中的应用经验，参考了两阶段策略和单阶段方法的优缺点，并利用了小波变换在图像处理中的成功经验。同时，他们整合了注意力机制和对抗训练等现有技术，但创新性地将它们结合起来，形成了前景感知模块、特征级频率增强模块和边缘约束模块三个互补组件，共同解决低对比度分割的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用频率域信息增强来改善低对比度医学图像的分割效果，通过对抗训练使模型更专注于前景区域，并结合边缘约束确保分割结果的几何连续性。整体实现流程包括：1) 前景感知模块：通过对抗训练学习前景和背景特征的异质性，使模型能专注于目标区域；2) 特征级频率增强模块：利用小波变换将特征分解为不同频率成分，选择性增强判别性高频特征，提高边界识别和细节感知能力；3) 边缘约束模块：通过边界关键点集约束确保分割边界的几何连续性；4) 整体框架将这三个模块有机结合，通过端到端训练实现高质量的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 前景感知模块：创新性地使用对抗训练策略最大化背景和输入图像特征间的分布差异，使模型能有效抵抗复杂背景干扰；2) 特征级频率增强模块：首次将小波变换与交叉注意力机制结合，选择性增强和利用判别性高频信息，减少噪声干扰；3) 边缘约束模块：将物理模型先验知识整合到深度学习框架中，确保分割边界的几何连续性；4) 整体框架设计：首次将频率域分析与前景感知、边缘约束相结合。相比之前的工作，这种方法能更有效地处理低对比度图像中的细微差异，选择性利用判别性特征而非盲目增强所有高频成分，并通过边缘约束保持边界完整性，整体性能显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于频率域分析的前景感知谱分割框架，通过结合前景感知、特征级频率增强和边缘约束三个创新模块，显著提高了低对比度腹部医学图像中肿瘤和器官的分割精度和边界连续性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of tumors and adjacent normal tissues in medical imagesis essential for surgical planning and tumor staging. Although foundationmodels generally perform well in segmentation tasks, they often struggle tofocus on foreground areas in complex, low-contrast backgrounds, where somemalignant tumors closely resemble normal organs, complicating contextualdifferentiation. To address these challenges, we propose the Foreground-AwareSpectrum Segmentation (FASS) framework. First, we introduce a foreground-awaremodule to amplify the distinction between background and the entire volumespace, allowing the model to concentrate more effectively on target areas.Next, a feature-level frequency enhancement module, based on wavelet transform,extracts discriminative high-frequency features to enhance boundary recognitionand detail perception. Eventually, we introduce an edge constraint module topreserve geometric continuity in segmentation boundaries. Extensive experimentson multiple medical datasets demonstrate superior performance across allmetrics, validating the effectiveness of our framework, particularly inrobustness under complex conditions and fine structure recognition. Ourframework significantly enhances segmentation of low-contrast images, pavingthe way for applications in more diverse and complex medical imaging scenarios.</description>
      <author>example@mail.com (Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu)</author>
      <guid isPermaLink="false">2510.11005v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning in Astrophysics</title>
      <link>http://arxiv.org/abs/2510.10713v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to Annual Review of Astronomy and Astrophysics  for Volume 64. This is the authors' version. Revisions and the final version  will be available at https://www.annualreviews.org/content/journals/astro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度学习为天文学提供了多样化视角，通过将物理对称性、守恒定律和微分方程编码到网络架构中，扩展了数据分析工具包。尽管面临未标记数据庞大而确认样本稀少的挑战，深度学习仍通过架构设计整合领域知识，为天文学提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习在天文学中产生了多样化视角，支持者和怀疑者之间的持续讨论促使了这篇综述。天文学提供了独特机会，可以通过编码物理对称性、守恒定律和微分方程直接到架构中，创建能推广到训练数据之外的模型。&lt;h4&gt;目的&lt;/h4&gt;检查神经网络如何补充经典统计学，扩展现代调查的数据分析工具包；评估深度学习方法提供真正进步的领域和需要仔细审查的主张；展示如何通过架构设计将领域知识整合到深度学习中。&lt;h4&gt;方法&lt;/h4&gt;通过将物理对称性、守恒定律和微分方程直接编码到网络架构中，创建能推广到训练数据之外的模型；通过架构设计将领域知识整合到模型中，使模型朝着物理上有意义的解决方案发展；评估深度学习在天文学不同应用领域的效果。&lt;h4&gt;主要发现&lt;/h4&gt;神经架构通过将物理对称性和守恒定律编码到网络结构中，克服了可扩展性、表达能力和数据效率之间的权衡；基于模拟的推理和异常检测能够从复杂、非高斯分布中提取信息，使宇宙学场级分析和罕见现象的系统性发现成为可能；多尺度神经建模弥合了天文模拟中的分辨率差距，从高保真运行中学习有效的次网格物理；强化学习用于望远镜操作，基础模型从最少示例中学习，大型语言模型代理用于研究自动化等新兴范式显示出潜力。&lt;h4&gt;结论&lt;/h4&gt;深度学习通过将领域知识整合到架构设计中，为天文学提供了新的数据分析工具，能够处理大规模数据并从有限标记数据中学习。尽管面临数据挑战，但深度学习方法在多个天文应用领域显示出实质性进步，特别是在处理复杂分布、弥合分辨率差距和研究自动化方面。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在天文学中产生了多样化视角，支持者和怀疑者之间的持续讨论促使了这篇综述。我们检查了神经网络如何补充经典统计学，扩展了现代调查的数据分析工具包。天文学提供了独特机会，可以通过编码物理对称性、守恒定律和微分方程直接到架构中，创建能推广到训练数据之外的模型。然而挑战仍然存在，因为未标记观测数据数量达数十亿，而具有已知属性的确认样本仍然稀少且昂贵。这篇综述展示了深度学习如何通过架构设计整合领域知识，内置假设引导模型朝向物理上有意义的解决方案。我们评估了这些方法在哪些方面提供了真正进步，以及哪些主张需要仔细审查。神经架构通过将物理对称性和守恒定律编码到网络结构中，克服了可扩展性、表达能力和数据效率之间的权衡，能够从有限的标记数据中学习。基于模拟的推理和异常检测从复杂、非高斯分布中提取信息，在这些分布中分析似然失败，使宇宙学场级分析和罕见现象的系统性发现成为可能。多尺度神经建模弥合了天文模拟中的分辨率差距，从昂贵的高保真运行中学习有效的次网格物理，以增强直接计算仍然不可行的大体积计算。新兴范式——用于望远镜操作的强化学习，从最少示例中学习的基础模型，以及用于研究自动化的大型语言模型代理——显示出潜力，尽管在天文学应用中仍在发展中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has generated diverse perspectives in astronomy, with ongoingdiscussions between proponents and skeptics motivating this review. We examinehow neural networks complement classical statistics, extending our dataanalytical toolkit for modern surveys. Astronomy offers unique opportunitiesthrough encoding physical symmetries, conservation laws, and differentialequations directly into architectures, creating models that generalize beyondtraining data. Yet challenges persist as unlabeled observations number inbillions while confirmed examples with known properties remain scarce andexpensive. This review demonstrates how deep learning incorporates domainknowledge through architectural design, with built-in assumptions guidingmodels toward physically meaningful solutions. We evaluate where these methodsoffer genuine advances versus claims requiring careful scrutiny. - Neuralarchitectures overcome trade-offs between scalability, expressivity, and dataefficiency by encoding physical symmetries and conservation laws into networkstructure, enabling learning from limited labeled data. - Simulation-basedinference and anomaly detection extract information from complex, non-Gaussiandistributions where analytical likelihoods fail, enabling field-levelcosmological analysis and systematic discovery of rare phenomena. - Multi-scaleneural modeling bridges resolution gaps in astronomical simulations, learningeffective subgrid physics from expensive high-fidelity runs to enhancelarge-volume calculations where direct computation remains prohibitive. -Emerging paradigms-reinforcement learning for telescope operations, foundationmodels learning from minimal examples, and large language model agents forresearch automation-show promise though are still developing in astronomicalapplications.</description>
      <author>example@mail.com (Yuan-Sen Ting)</author>
      <guid isPermaLink="false">2510.10713v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection</title>
      <link>http://arxiv.org/abs/2510.10663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures, project page:  https://fsfm-3c.github.io/fsvfm.html&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的基本表示，通过结合掩码图像建模和实例判别，提高人脸安全任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;如何利用大量未标记的真实人脸图像学习鲁棒且可迁移的人脸表示，以提升各种人脸安全任务的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个可扩展的自监督预训练框架，学习真实人脸图像的基本表示，并在多种人脸安全任务上实现更好的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;引入3C学习目标，结合掩码图像建模(MIM)和实例判别(ID)；设计CRFR-P掩码策略；提出可靠的自蒸馏机制建立局部到全局的对应关系；使用普通视觉变压器(ViTs)作为下游任务的通用视觉基础模型；提出FS-Adapter轻量级即插即用瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;在11个公共基准测试上，FS-VFM在各种视觉基础模型中泛化能力更好，包括自然和人脸领域；在不同监督范式和ViT规模上都表现出色；甚至优于最先进的任务特定方法；FS-Adapter提供了出色的效率-性能权衡。&lt;h4&gt;结论&lt;/h4&gt;FS-VFM框架能够有效学习人脸表示，并在各种人脸安全任务上实现更好的泛化性能，代码和模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;利用大量未标记的真实人脸，我们如何学习鲁棒且可迁移的人脸表示来提高各种人脸安全任务的泛化能力？我们首次尝试并提出FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的基本表示。我们引入三个学习目标，即3C，协同结合掩码图像建模(MIM)和实例判别(ID)，使FS-VFM能够编码真实人脸的局部模式和全局语义。具体而言，我们为MIM制定了各种面部掩码策略，并设计了一种简单而有效的CRFR-P掩码，明确提示模型追求有意义的区域内一致性和挑战性的区域间连贯性。我们提出了一个可靠的自蒸馏机制，将MIM与ID无缝耦合，建立底层局部到全局的对应关系。预训练后，普通视觉变压器(ViTs)作为下游人脸安全任务的通用视觉基础模型：跨数据集深度伪造检测、跨域人脸防欺骗和未见扩散人脸取证。为了高效迁移预训练的FS-VFM，我们进一步提出FS-Adapter，一种新颖的真实锚点对比目标的轻量级即插即用瓶颈，位于冻结骨干网络之上。在11个公共基准上的广泛实验表明，我们的FS-VFM比各种视觉基础模型泛化能力更好，包括自然和人脸领域，完全、弱和自监督范式，小型、基础和大型的ViT规模，甚至优于最先进的任务特定方法，而FS-Adapter提供了出色的效率-性能权衡。代码和模型可在https://fsfm-3c.github.io/fsvfm.html获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With abundant, unlabeled real faces, how can we learn robust and transferablefacial representations to boost generalization across various face securitytasks? We make the first attempt and propose FS-VFM, a scalable self-supervisedpre-training framework, to learn fundamental representations of real faceimages. We introduce three learning objectives, namely 3C, that synergizemasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFMto encode both local patterns and global semantics of real faces. Specifically,we formulate various facial masking strategies for MIM and devise a simple yeteffective CRFR-P masking, which explicitly prompts the model to pursuemeaningful intra-region Consistency and challenging inter-region Coherency. Wepresent a reliable self-distillation mechanism that seamlessly couples MIM withID to establish underlying local-to-global Correspondence. After pre-training,vanilla vision transformers (ViTs) serve as universal Vision Foundation Modelsfor downstream Face Security tasks: cross-dataset deepfake detection,cross-domain face anti-spoofing, and unseen diffusion facial forensics. Toefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, alightweight plug-and-play bottleneck atop the frozen backbone with a novelreal-anchor contrastive objective. Extensive experiments on 11 publicbenchmarks demonstrate that our FS-VFM consistently generalizes better thandiverse VFMs, spanning natural and facial domains, fully, weakly, andself-supervised paradigms, small, base, and large ViT scales, and evenoutperforms SOTA task-specific methods, while FS-Adapter offers an excellentefficiency-performance trade-off. The code and models are available onhttps://fsfm-3c.github.io/fsvfm.html.</description>
      <author>example@mail.com (Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, Kui Ren)</author>
      <guid isPermaLink="false">2510.10663v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2510.10584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地探索了预训练视觉基础模型在分布外检测任务中的应用，发现DINOv2模型无需微调即可提供高度判别性的特征空间，并提出了MoFE模块和Dynamic-β Mixup策略来解决语义空间较大场景中的性能问题。&lt;h4&gt;背景&lt;/h4&gt;预训练视觉基础模型已改变众多计算机视觉任务，它们在学习判别性和可泛化特征方面能力强大，这些特征对分布外检测至关重要，但它们对这一任务的影响尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统性地研究代表性的视觉基础模型在分布外检测任务中的应用和性能。&lt;h4&gt;方法&lt;/h4&gt;研究预训练DINOv2模型在分布外检测中的表现；探索在领域内数据上微调基础模型如何增强分布外检测；提出Mixture of Feature Experts (MoFE)模块将特征划分为子空间；引入Dynamic-β Mixup策略从动态beta分布中采样插值权重。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的DINOv2模型无需在领域内数据上微调就能提供高度判别性的特征空间，性能媲美现有最先进方法；在语义空间较大的场景中，视觉基础模型的性能仍然不令人满意，这是因为类别数量增加导致决策边界复杂化，使优化过程变得复杂。&lt;h4&gt;结论&lt;/h4&gt;MoFE模块和Dynamic-β Mixup策略能有效捕获复杂数据分布并细化决策边界，大量实验证明该方法显著优于基线方法。&lt;h4&gt;翻译&lt;/h4&gt;预训练视觉基础模型已改变许多计算机视觉任务。尽管它们在学习对分布外检测至关重要的判别性和可泛化特征方面能力强大，它们对该任务的影响仍未被充分探索。受此差距启发，我们系统性地研究了代表性的视觉基础模型用于分布外检测。我们的发现表明，预训练的DINOv2模型即使在领域内数据上未进行微调，也能为分布外检测提供高度判别性的特征空间，实现与现有最先进方法相当的性能，而无需复杂设计。除此之外，我们探索了在领域内数据上微调基础模型如何增强分布外检测。然而，我们观察到在语义空间较大的场景中，视觉基础模型的性能仍然不令人满意。这是因为随着类别数量增加，决策边界的复杂度提高，使优化过程变得复杂。为缓解这一问题，我们提出了特征专家混合（MoFE）模块，该模块将特征划分为子空间，有效捕获复杂数据分布并细化决策边界。此外，我们引入了动态-β混合策略，从动态beta分布中采样插值权重。这使模型能够适应不同类别间的不同学习难度，提升更具挑战性类别的特征学习。大量实验证明了我们方法的有效性，显著优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained vision foundation models have transformed many computer visiontasks. Despite their strong ability to learn discriminative and generalizablefeatures crucial for out-of-distribution (OOD) detection, their impact on thistask remains underexplored. Motivated by this gap, we systematicallyinvestigate representative vision foundation models for OOD detection. Ourfindings reveal that a pre-trained DINOv2 model, even without fine-tuning onin-domain (ID) data, naturally provides a highly discriminative feature spacefor OOD detection, achieving performance comparable to existingstate-of-the-art methods without requiring complex designs. Beyond this, weexplore how fine-tuning foundation models on in-domain (ID) data can enhanceOOD detection. However, we observe that the performance of vision foundationmodels remains unsatisfactory in scenarios with a large semantic space. This isdue to the increased complexity of decision boundaries as the number ofcategories grows, which complicates the optimization process. To mitigate this,we propose the Mixture of Feature Experts (MoFE) module, which partitionsfeatures into subspaces, effectively capturing complex data distributions andrefining decision boundaries. Further, we introduce a Dynamic-$\beta$ Mixupstrategy, which samples interpolation weights from a dynamic beta distribution.This adapts to varying levels of learning difficulty across categories,improving feature learning for more challenging categories. Extensiveexperiments demonstrate the effectiveness of our approach, significantlyoutperforming baseline methods.</description>
      <author>example@mail.com (Shizhen Zhao, Jiahui Liu, Xin Wen, Haoru Tan, Xiaojuan Qi)</author>
      <guid isPermaLink="false">2510.10584v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment</title>
      <link>http://arxiv.org/abs/2510.10464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  81 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个用于TIPS预后的公开多中心数据集MultiTIPS，并基于此开发了一种新的多模态预后框架，包含双选项分割、多模态交互和多任务预测三个核心模块，解决了当前研究面临的ROI标注量大、单模态方法可靠性差和单终点评估不完整等挑战。&lt;h4&gt;背景&lt;/h4&gt;TIPS是治疗门静脉高压的既定方法，但存在生存结果差异大和频繁出现明显肝性脑病(OHE)的问题，需要准确的术前预后模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确、可靠的TIPS预后模型，解决当前研究中存在的ROI标注量大、单模态方法可靠性差和单终点评估不完整等挑战，并提供公开数据集促进该领域研究。&lt;h4&gt;方法&lt;/h4&gt;提出MultiTIPS数据集和一种新的多模态预后框架，该框架包含三个核心模块：(1)双选项分割：结合半监督和基础模型实现有限标注下的鲁棒ROI分割；(2)多模态交互：引入MGRA、POD和CGPE技术实现跨模态特征交互；(3)多任务预测：使用分阶段训练策略同时优化生存、PPG和OHE预测。&lt;h4&gt;主要发现&lt;/h4&gt;在MultiTIPS上的大量实验表明，所提出的方法优于最先进的方法，具有强大的跨域泛化性和可解释性，显示出临床应用的潜力。&lt;h4&gt;结论&lt;/h4&gt;MultiTIPS数据集和所提出的多模态预后框架为TIPS预后评估提供了有效解决方案，有望在临床实践中应用。&lt;h4&gt;翻译&lt;/h4&gt;经颈静脉肝内门体分流术(TIPS)是治疗门静脉高压的既定方法，但提供不同的生存结果和频繁明显的肝性脑病(OHE)，表明需要准确的术前预后建模。当前研究通常从术前CT图像或临床特征构建机器学习模型，但面临三个关键挑战：(1)劳动密集型的感兴趣区域(ROI)标注，(2)单模态方法的可靠性和泛化能力差，(3)单终点预测评估不完整。此外，缺乏公开可访问的数据集限制了该领域的研究。因此，我们提出了MultiTIPS，这是首个用于TIPS预后的公共多中心数据集，并基于它提出了一个新的多模态预后框架。该框架包含三个核心模块：(1)双选项分割，结合半监督和基于基础模型的流程，实现有限标注下的鲁棒ROI分割并促进后续特征提取；(2)多模态交互，引入多粒度放射组学注意力(MGRA)、渐进正交解耦(POD)和临床引导预后增强(CGPE)技术，实现跨模态特征交互和互补表示集成，从而提高模型准确性和鲁棒性；(3)多任务预测，使用分阶段训练策略对生存、门静脉压力梯度(PPG)和OHE预测进行稳定优化，实现全面预后评估。在MultiTIPS上的大量实验证明了所提出方法优于最先进的方法，同时具有强大的跨域泛化性和可解释性，表明其临床应用前景。该数据集和代码是公开可用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transjugular intrahepatic portosystemic shunt (TIPS) is an establishedprocedure for portal hypertension, but provides variable survival outcomes andfrequent overt hepatic encephalopathy (OHE), indicating the necessity ofaccurate preoperative prognostic modeling. Current studies typically buildmachine learning models from preoperative CT images or clinicalcharacteristics, but face three key challenges: (1) labor-intensiveregion-of-interest (ROI) annotation, (2) poor reliability and generalizabilityof unimodal methods, and (3) incomplete assessment from single-endpointprediction. Moreover, the lack of publicly accessible datasets constrainsresearch in this field. Therefore, we present MultiTIPS, the first publicmulti-center dataset for TIPS prognosis, and propose a novel multimodalprognostic framework based on it. The framework comprises three core modules:(1) dual-option segmentation, which integrates semi-supervised and foundationmodel-based pipelines to achieve robust ROI segmentation with limitedannotations and facilitate subsequent feature extraction; (2) multimodalinteraction, where three techniques, multi-grained radiomics attention (MGRA),progressive orthogonal disentanglement (POD), and clinically guided prognosticenhancement (CGPE), are introduced to enable cross-modal feature interactionand complementary representation integration, thus improving model accuracy androbustness; and (3) multi-task prediction, where a staged training strategy isused to perform stable optimization of survival, portal pressure gradient(PPG), and OHE prediction for comprehensive prognostic assessment. Extensiveexperiments on MultiTIPS demonstrate the superiority of the proposed methodover state-of-the-art approaches, along with strong cross-domain generalizationand interpretability, indicating its promise for clinical application. Thedataset and code are available.</description>
      <author>example@mail.com (Junhao Dong, Dejia Liu, Ruiqi Ding, Zongxing Chen, Yingjie Huang, Zhu Meng, Jianbo Zhao, Zhicheng Zhao, Fei Su)</author>
      <guid isPermaLink="false">2510.10464v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure</title>
      <link>http://arxiv.org/abs/2510.10366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BHI abstract extended&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了使用视觉基础模型(VFM)处理光电容积脉搏波描记术(PPG)信号的可能性，发现将一维PPG信号转换为二维图像表示后，视觉模型在血压估计等多种生理任务上达到最先进性能，并展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;光电容积脉搏波描记术(PPG)传感器在可穿戴和临床设备中能够以非侵入式和实时方式提供有价值的生理信息。目前通常使用专门的基础模型或重新利用的时间序列基础模型来基准化生理任务。&lt;h4&gt;目的&lt;/h4&gt;研究视觉基础模型(VFM)在PPG信号处理中的应用潜力，评估其在各种生理任务中的性能，并与现有时间序列基础模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;将一维PPG信号转换为二维图像表示，如短时傅里叶变换(STFT)，然后使用最新的视觉基础模型(如DINOv3和SIGLIP-2)进行微调。采用参数高效微调(PEFT)技术提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;视觉基础模型(VFM)在PPG信号处理中表现优异，特别是在血压估计任务上达到了最先进(SOTA)的性能。该方法在其他生命体征和血液实验室测量任务中也取得了有希望的结果，并且可以推广到STFT相位和递归图等其他2D输入表示。&lt;h4&gt;结论&lt;/h4&gt;提出的Vision4PPG方法解锁了一类新的基础模型用于PPG处理，能够实现最先进性能并具有良好的泛化能力。这些工具为临床科学家提供了计算效率高的新选择，通过参数高效微调技术实现。&lt;h4&gt;翻译&lt;/h4&gt;光电容积脉搏波描记术(PPG)传感器在可穿戴和临床设备中以非侵入式和实时方式提供有价值的生理洞见。专门的基础模型(FM)或重新利用的时间序列FM被用于基准化生理任务。我们对微调FM的实验表明，视觉FM(VFM)也可用于此目的，事实上在许多任务上(特别是血压估计)出乎意料地达到了最先进(SOTA)性能。我们通过简单地将一维PPG信号转换为类图像的二维表示(如短时傅里叶变换STFT)来利用VFMs。使用最新的VFMs(如DINOv3和SIGLIP-2)，我们在其他生命体征和血液实验室测量任务中也取得了有希望的性能。我们的提案Vision4PPG解锁了一类新的FM，实现了SOTA性能，并能显著推广到其他2D输入表示，包括STFT相位和递归图。我们的工作通过进行全面研究、将其与最先进的时间序列FM进行比较，并在六个额外任务上报告结果，改进了先前关于视觉模型用于PPG的研究。因此，我们为临床科学家提供了一套新的强大工具，由于参数高效微调(PEFT)技术，这些工具也具有计算效率高的特点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmography (PPG) sensor in wearable and clinical devices providesvaluable physiological insights in a non-invasive and real-time fashion.Specialized Foundation Models (FM) or repurposed time-series FMs are used tobenchmark physiological tasks. Our experiments with fine-tuning FMs reveal thatVision FM (VFM) can also be utilized for this purpose and, in fact,surprisingly leads to state-of-the-art (SOTA) performance on many tasks,notably blood pressure estimation. We leverage VFMs by simply transformingone-dimensional PPG signals into image-like two-dimensional representations,such as the Short-Time Fourier transform (STFT). Using the latest VFMs, such asDINOv3 and SIGLIP-2, we achieve promising performance on other vital signs andblood lab measurement tasks as well. Our proposal, Vision4PPG, unlocks a newclass of FMs to achieve SOTA performance with notable generalization to other2D input representations, including STFT phase and recurrence plots. Our workimproves upon prior investigations of vision models for PPG by conducting acomprehensive study, comparing them to state-of-the-art time-series FMs, anddemonstrating the general PPG processing ability by reporting results on sixadditional tasks. Thus, we provide clinician-scientists with a new set ofpowerful tools that is also computationally efficient, thanks toParameter-Efficient Fine-Tuning (PEFT) techniques.</description>
      <author>example@mail.com (Saurabh Kataria, Ayca Ermis, Lovely Yeswanth Panchumarthi, Minxiao Wang, Xiao Hu)</author>
      <guid isPermaLink="false">2510.10366v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs</title>
      <link>http://arxiv.org/abs/2510.10329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了结合预训练语音编码器和大型语言模型的端到端架构，用于同时进行语音识别和语音翻译，在英语到德语任务上取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;语音翻译是将一种语言的语音信号转换为另一种语言对应文本的机器翻译任务，存在传统级联方法和端到端方法两种不同途径。&lt;h4&gt;目的&lt;/h4&gt;探索一种结合预训练语音编码器和大型语言模型的端到端架构，实现同时进行自动语音识别和语音翻译。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的语音编码器和大型语言模型构建端到端架构，并在英语到德语的语言对上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型不仅比SeamlessM4T大型基础端到端多模态翻译模型取得更好的翻译结果，还能匹配使用Whisper和NLLB的级联系统性能，在COMET-DA22指标上获得高达8%的分数提升。&lt;h4&gt;结论&lt;/h4&gt;结合预训练语音编码器和大型语言模型的端到端架构在语音翻译任务上表现出色，超越了现有模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音翻译是一种机器翻译任务，涉及将一种语言的语音信号转换为另一种语言的对应文本；该任务有两种不同的方法，即传统的级联方法和最近的端到端方法。本文探索了结合预训练语音编码器和大型语言模型的端到端架构，用于同时执行自动语音识别和语音翻译。英语到德语语言对的实验表明，我们的最佳模型不仅能够比SeamlessM4T（大型基础端到端多模态翻译模型）获得更好的翻译结果，还能匹配使用Whisper和NLLB的级联系统性能，在COMET-DA22指标上获得高达8%的分数提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech Translation (ST) is a machine translation task that involvesconverting speech signals from one language to the corresponding text inanother language; this task has two different approaches, namely thetraditional cascade and the more recent end-to-end. This paper explores acombined end-to-end architecture of pre-trained speech encoders and LargeLanguage Models (LLMs) for performing both Automatic Speech Recognition (ASR)and ST simultaneously. Experiments with the English-to-German language pairshow that our best model not only can achieve better translation results thanSeamlessM4T, a large foundational end-to-end, multi-modal translation model,but can also match the performance of a cascaded system with Whisper and NLLB,with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.</description>
      <author>example@mail.com (Nam Luu, Ondřej Bojar)</author>
      <guid isPermaLink="false">2510.10329v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology</title>
      <link>http://arxiv.org/abs/2510.10196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了名为CerS-Path的宫颈癌亚专科病理诊断系统，通过两个预训练阶段构建，支持八种诊断功能，在前瞻性测试中达到99.38%的筛查敏感性，展示了优秀的泛化能力和临床应用潜力。&lt;h4&gt;背景&lt;/h4&gt;宫颈癌是一种主要恶性肿瘤，需要广泛复杂的组织病理学评估。现有深度学习模型缺乏准确性和泛化能力，通用基础模型在捕获亚专科特定特征和任务适应性方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对宫颈癌病理的诊断系统，提高宫颈癌病理诊断的准确性和泛化能力，解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发CerS-Path诊断系统，通过两个协同预训练阶段：1)自监督学习使用约1.9亿个组织块构建宫颈特异性特征提取器；2)多模态增强使用250万对图像-文本对进行增强，然后整合多个下游诊断功能。&lt;h4&gt;主要发现&lt;/h4&gt;CerS-Path在范围和临床适用性方面超越了以前的基础模型，全面评估显示在宫颈癌病理方面取得显著进展，在五个中心对3173例病例的前瞻性测试中保持99.38%的筛查敏感性和优秀泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CerS-Path在亚专科诊断转化和宫颈癌筛查方面具有潜力，代表了宫颈癌病理诊断的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;宫颈癌仍然是一种主要恶性肿瘤，需要广泛而复杂的组织病理学评估和全面的支持工具。尽管深度学习显示出前景，但这些模型仍然缺乏准确性和泛化能力。通用基础模型提供了更广泛的覆盖范围，但在捕获亚专科特定特征和任务适应性方面仍然存在局限。我们引入了宫颈亚专科病理学(CerS-Path)诊断系统，通过两个协同的预训练阶段开发：自监督学习来自约14万张幻灯片的1.9亿个组织块以构建宫颈特异性特征提取器，以及使用250万对图像-文本对进行多模态增强，随后整合多个下游诊断功能。支持包括罕见癌症分类和多模态问答在内的八种诊断功能，CerS-Path在范围和临床适用性方面超越了以前的基础模型。全面评估显示在宫颈癌病理方面取得了显著进展，在五个中心对3173例病例的前瞻性测试中保持了99.38%的筛查敏感性和优秀的泛化能力，突显了其在亚专科诊断转化和宫颈癌筛查方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cervical cancer remains a major malignancy, necessitating extensive andcomplex histopathological assessments and comprehensive support tools. Althoughdeep learning shows promise, these models still lack accuracy andgeneralizability. General foundation models offer a broader reach but remainlimited in capturing subspecialty-specific features and task adaptability. Weintroduce the Cervical Subspecialty Pathology (CerS-Path) diagnostic system,developed through two synergistic pretraining stages: self-supervised learningon approximately 190 million tissue patches from 140,000 slides to build acervical-specific feature extractor, and multimodal enhancement with 2.5million image-text pairs, followed by integration with multiple downstreamdiagnostic functions. Supporting eight diagnostic functions, including rarecancer classification and multimodal Q&amp;A, CerS-Path surpasses prior foundationmodels in scope and clinical applicability. Comprehensive evaluationsdemonstrate a significant advance in cervical pathology, with prospectivetesting on 3,173 cases across five centers maintaining 99.38% screeningsensitivity and excellent generalizability, highlighting its potential forsubspecialty diagnostic translation and cervical cancer screening.</description>
      <author>example@mail.com (Yizhi Wang, Li Chen, Qiang Huang, Tian Guan, Xi Deng, Zhiyuan Shen, Jiawen Li, Xinrui Chen, Bin Hu, Xitong Ling, Taojie Zhu, Zirui Huang, Deshui Yu, Yan Liu, Jiurun Chen, Lianghui Zhu, Qiming He, Yiqing Liu, Diwei Shi, Hanzhong Liu, Junbo Hu, Hongyi Gao, Zhen Song, Xilong Zhao, Chao He, Ming Zhao, Yonghong He)</author>
      <guid isPermaLink="false">2510.10196v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.10163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SparseUWSeg是一个新型框架，通过主动采样策略和混合方法解决水下图像语义分割中稀疏点标注的挑战，实现了比现有方法更高的分割精度。&lt;h4&gt;背景&lt;/h4&gt;语义分割对自动化水下图像分析和生态监测至关重要，但细粒度的水下场景分析仍是开放问题，获取密集专家标注标签成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决水下图像语义分割中稀疏点标注的选择和传播问题，提高分割模型的性能。&lt;h4&gt;方法&lt;/h4&gt;SparseUWSeg采用主动采样策略指导标注者选择有价值的点，并结合SAM2和基于超像素方法的混合技术传播稀疏标签。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同水下数据集上，SparseUWSeg相比最先进方法实现了最高5%的mIoU提升。&lt;h4&gt;结论&lt;/h4&gt;SparseUWSeg框架和其集成的交互式标注工具使生态研究人员能够高效利用基础模型和计算机视觉技术生成高质量分割掩模。&lt;h4&gt;翻译&lt;/h4&gt;语义分割对自动化水下图像分析和生态监测至关重要。不幸的是，即使对于最先进的分割模型，细粒度的水下场景分析仍然是一个开放问题。获取密集的、专家标注的分割标签成本很高，阻碍了该领域模型的监督学习。虽然稀疏点标签更容易获取，但它们在标注哪些点和如何传播稀疏信息方面带来了挑战。我们提出了SparseUWSeg，一个解决这两个问题的新型框架。SparseUWSeg采用主动采样策略指导标注者，最大化其点标签的价值。然后，它使用结合SAM2和基于超像素方法优点的混合方法传播这些稀疏标签。在两个不同水下数据集上的实验证明了SparseUWSeg相比最先进方法的优势，相比D+NN方法实现了最高5%的mIoU提升。我们的主要贡献是设计并发布了一个简单但有效的交互式标注工具，整合了我们的算法。它使生态研究人员能够利用基础模型和计算机视觉高效生成高质量的分割掩模来处理他们的数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation is essential to automate underwater imagery analysiswith ecology monitoring purposes. Unfortunately, fine grained underwater sceneanalysis is still an open problem even for top performing segmentation models.The high cost of obtaining dense, expert-annotated, segmentation labels hindersthe supervision of models in this domain. While sparse point-labels are easierto obtain, they introduce challenges regarding which points to annotate and howto propagate the sparse information. We present SparseUWSeg, a novel frameworkthat addresses both issues. SparseUWSeg employs an active sampling strategy toguide annotators, maximizing the value of their point labels. Then, itpropagates these sparse labels with a hybrid approach leverages both the bestof SAM2 and superpixel-based methods. Experiments on two diverse underwaterdatasets demonstrate the benefits of SparseUWSeg over state-of-the-artapproaches, achieving up to +5\% mIoU over D+NN. Our main contribution is thedesign and release of a simple but effective interactive annotation tool,integrating our algorithms. It enables ecology researchers to leveragefoundation models and computer vision to efficiently generate high-qualitysegmentation masks to process their data.</description>
      <author>example@mail.com (César Borja, Carlos Plou, Rubén Martinez-Cantín, Ana C. Murillo)</author>
      <guid isPermaLink="false">2510.10163v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework</title>
      <link>http://arxiv.org/abs/2510.10084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于视觉基础模型的新框架，能够通过将离散遥感图像转换为连续视频序列，实现大规模滑坡疤痕时空演化的连续追踪，为滑坡早期预警和灾害评估提供了有效工具。&lt;h4&gt;背景&lt;/h4&gt;追踪大规模滑坡疤痕的时空演化对于理解演化机制和破坏前兆、实现有效预警至关重要。然而，现有研究多关注单阶段或破坏前后的双阶段滑坡识别，难以追踪滑坡疤痕的时空演化过程。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以追踪滑坡疤痕时空演化的问题，提出一个新框架用于追踪大规模滑坡疤痕的时空演化。&lt;h4&gt;方法&lt;/h4&gt;使用视觉基础模型，将离散的光学遥感图像重建为连续的视频序列，使专为视频分割开发的视觉基础模型可用于追踪滑坡疤痕演化。该框架在知识引导、自动传播和交互式精炼的范式中运行，确保滑坡疤痕的连续和准确识别。&lt;h4&gt;主要发现&lt;/h4&gt;该框架通过白格滑坡和色拉滑坡(2017-2025)两个案例得到验证，能够连续追踪滑坡疤痕，捕捉对预警至关重要的破坏前兆，以及对评估次生灾害和长期稳定性至关重要的破坏后演化。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为滑坡疤痕的时空演化追踪提供了有效方法，有助于早期预警和灾害评估。&lt;h4&gt;翻译&lt;/h4&gt;追踪大规模滑坡疤痕的时空演化对于理解演化机制和破坏前兆、实现有效预警至关重要。然而，大多数现有研究只关注单阶段或破坏前后的双阶段滑坡识别。虽然这些方法能够确定破坏后的滑坡边界，但难以追踪滑坡疤痕的时空演化。为解决这一问题，本研究提出了一种新的通用框架，使用视觉基础模型追踪大规模滑坡疤痕的时空演化。该框架的关键思路是将离散的光学遥感图像重建为连续的视频序列。这种转换使得专为视频分割开发的视觉基础模型可以用于追踪滑坡疤痕的演化。该框架在知识引导、自动传播和交互式精炼的范式中运行，以确保滑坡疤痕的连续和准确识别。该框架已通过两个代表性案例的应用得到验证：破坏后的白格滑坡和活跃的色拉滑坡(2017-2025)。结果表明，所提出的框架能够连续追踪滑坡疤痕，捕捉对预警至关重要的破坏前兆，以及对评估次生灾害和长期稳定性至关重要的破坏后演化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking the spatiotemporal evolution of large-scale landslide scars iscritical for understanding the evolution mechanisms and failure precursors,enabling effective early-warning. However, most existing studies have focusedon single-phase or pre- and post-failure dual-phase landslide identification.Although these approaches delineate post-failure landslide boundaries, it ischallenging to track the spatiotemporal evolution of landslide scars. Toaddress this problem, this study proposes a novel and universal framework fortracking the spatiotemporal evolution of large-scale landslide scars using avision foundation model. The key idea behind the proposed framework is toreconstruct discrete optical remote sensing images into a continuous videosequence. This transformation enables a vision foundation model, which isdeveloped for video segmentation, to be used for tracking the evolution oflandslide scars. The proposed framework operates within a knowledge-guided,auto-propagation, and interactive refinement paradigm to ensure the continuousand accurate identification of landslide scars. The proposed framework wasvalidated through application to two representative cases: the post-failureBaige landslide and the active Sela landslide (2017-2025). Results indicatethat the proposed framework enables continuous tracking of landslide scars,capturing both failure precursors critical for early warning and post-failureevolution essential for assessing secondary hazards and long-term stability.</description>
      <author>example@mail.com (Meijun Zhou, Gang Mei, Zhengjing Ma, Nengxiong Xu, Jianbing Peng)</author>
      <guid isPermaLink="false">2510.10084v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow</title>
      <link>http://arxiv.org/abs/2510.05836v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV' 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Flow4Agent框架，通过引入光流运动先验来促进基于LLM的长视频理解，解决了长视频时空内容冗余和MLLMs上下文长度有限的问题。&lt;h4&gt;背景&lt;/h4&gt;长视频理解一直是一个具有挑战性的问题，因为时空内容中存在大量冗余，同时多模态大语言模型(MLLMs)的有限上下文长度进一步加剧了这一挑战。&lt;h4&gt;目的&lt;/h4&gt;解决长视频理解中的冗余问题，提出一种新颖的框架，利用光流运动先验来增强LLM对长视频的理解能力。&lt;h4&gt;方法&lt;/h4&gt;Flow4Agent框架包含两个核心模块：1) 时间粒度优化(TGO)自适应细化帧级层次结构，利用粗略光流先验分组相似视觉内容，再应用语义先验过滤无关场景信息；2) 运动标记剪枝(MTP)使用细粒度光流信息剪枝高冗余视频标记，进一步优化帧内视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;Flow4Agent在多个视频MLLM基准测试中优于现有方法，特别是在小时级视频理解任务中表现突出：在Video-MME上达到64.7%，在MLVU上达到71.4%，在LongVideoBench上达到60.4%。&lt;h4&gt;结论&lt;/h4&gt;Flow4Agent通过引入光流运动先验，有效解决了长视频理解中的时空冗余问题，在长视频理解任务中取得了显著性能提升。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解一直是一个具有挑战性的问题，因为时空内容中存在显著的冗余。多模态大语言模型(MLLMs)的有限上下文长度进一步加剧了这一挑战。为解决这一问题，许多先前的工作尝试提取关键视频信息，其中'关键'通常是语义感知的，并严重依赖CLIP模型作为先验。在本文中，我们提出了Flow4Agent，一个新颖的框架，开创性地引入光流运动先验来促进基于LLM的长视频理解。Flow4Agent通过两个核心模块在时空层面减轻长视频的冗余：时间粒度优化(TGO)自适应地细化帧级层次结构，首先利用粗略光流先验分组相似的视觉内容，然后应用语义先验过滤掉高度不相关的场景信息。运动标记剪枝(MTP)进一步细化帧内视觉表示，使用细粒度光流信息剪枝高冗余的视频标记。大量实验表明，我们的Flow4Agent在广泛的视频MLLM基准测试中优于现有方法，特别是在小时级视频理解任务中，在Video-MME上达到64.7%，在MLVU上达到71.4%，在LongVideoBench上达到60.4%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form video understanding has always been a challenging problem due tothe significant redundancy in both temporal and spatial contents. Thischallenge is further exacerbated by the limited context length of MultimodalLarge Language Models (MLLMs). To address this issue, many previous works haveattempted to extract key video information, where the "key" is typicallysemantic-aware and heavily dependent on the CLIP model as prior. In this paper,we propose Flow4Agent, a novel framework that pioneeringly incorporates motionpriors from optical flow to facilitate LLM-based long video understanding.Flow4Agent mitigates the redundancy in long videos at both temporal and spatiallevels through two core modules: Temporal Granularity Optimization (TGO)adaptively refines framelevel hierarchies, which first leverages coarse flowpriors to group similar visual contents and then applies semantic priors tofilter out highly irrelevant scene information. Motion Token Pruning (MTP)further refines the intra-frame visual representations, pruning high-redundancyvideo tokens using fine-grained optical flow information. Extensive experimentsdemonstrate that our Flow4Agent outperforms existing methods across a widerange of video MLLM benchmarks, especially for hour-level video understandingtasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.</description>
      <author>example@mail.com (Ruyang Liu, Shangkun Sun, Haoran Tang, Ge Li, Wei Gao)</author>
      <guid isPermaLink="false">2510.05836v1</guid>
      <pubDate>Mon, 13 Oct 2025 14:01:29 +0800</pubDate>
    </item>
  <item>
      <title>Long-tailed Recognition with Model Rebalancing</title>
      <link>http://arxiv.org/abs/2510.08177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORE的新型框架，通过直接重平衡模型参数空间来解决长尾识别问题，显著提高了模型对尾部类别的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;长尾识别在深度学习和基础模型下游微调中普遍存在且具有挑战性，偏斜的类别分布通常阻碍模型对尾部类别的泛化。&lt;h4&gt;目的&lt;/h4&gt;深入研究长尾情境下基本模型容量的影响，并提出一种有效的不平衡缓解方法。&lt;h4&gt;方法&lt;/h4&gt;提出MORE框架，引入低秩参数组件在定制化损失和正弦重加权计划指导下调解参数空间分配，不增加模型复杂度或推理成本。&lt;h4&gt;主要发现&lt;/h4&gt;MORE在多样化的长尾基准测试上显著提高了泛化能力，特别是对尾部类别，并能有效补充现有不平衡缓解方法。&lt;h4&gt;结论&lt;/h4&gt;MORE在长尾设置中可作为强大的即插即用模块，有效解决长尾识别问题。&lt;h4&gt;翻译&lt;/h4&gt;长尾识别在深度学习和基础模型的下游微调中普遍存在且具有挑战性，因为偏斜的类别分布通常阻碍模型对尾部类别的泛化能力。尽管之前从数据增强、损失重平衡和解耦训练等角度的方法有潜力，但在多标签长尾识别等广泛场景中取得一致的改进仍然困难。本研究深入探讨长尾情境下基本模型容量的影响，并提出一种名为MORE的新型框架，通过直接重平衡模型参数空间来缓解不平衡问题。具体而言，MORE引入低秩参数组件，在定制化损失和正弦重加权计划的指导下调解参数空间分配，同时不增加整体模型复杂度或推理成本。在多样化的长尾基准测试上进行的广泛实验（涵盖多类和多标签任务）表明，MORE显著提高了泛化能力，特别是对尾部类别，并有效补充了现有的不平衡缓解方法。这些结果突显了MORE在长尾设置中作为强大即插即用模块的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-tailed recognition is ubiquitous and challenging in deep learning andeven in the downstream finetuning of foundation models, since the skew classdistribution generally prevents the model generalization to the tail classes.Despite the promise of previous methods from the perspectives of dataaugmentation, loss rebalancing and decoupled training etc., consistentimprovement in the broad scenarios like multi-label long-tailed recognition isdifficult. In this study, we dive into the essential model capacity impactunder long-tailed context, and propose a novel framework, Model Rebalancing(MORE), which mitigates imbalance by directly rebalancing the model's parameterspace. Specifically, MORE introduces a low-rank parameter component to mediatethe parameter space allocation guided by a tailored loss and sinusoidalreweighting schedule, but without increasing the overall model complexity orinference costs. Extensive experiments on diverse long-tailed benchmarks,spanning multi-class and multi-label tasks, demonstrate that MORE significantlyimproves generalization, particularly for tail classes, and effectivelycomplements existing imbalance mitigation methods. These results highlightMORE's potential as a robust plug-and-play module in long-tailed settings.</description>
      <author>example@mail.com (Jiaan Luo, Feng Hong, Qiang Hu, Xiaofeng Cao, Feng Liu, Jiangchao Yao)</author>
      <guid isPermaLink="false">2510.08177v1</guid>
      <pubDate>Mon, 13 Oct 2025 14:01:29 +0800</pubDate>
    </item>
    <item>
      <title>Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization</title>
      <link>http://arxiv.org/abs/2510.06842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended Version of MAGR (ECCV 2024 Oral Presentation)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAGR++的自适应流形对齐图正则化方法，用于解决动作质量评估中的非平稳分布问题，通过持续学习减轻灾难性遗忘，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;动作质量评估（AQA）量化视频中的人类动作，支持体育评分、康复和技能评估等应用。主要挑战在于现实场景中质量分布的非平稳性，这限制了传统方法的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;引入持续动作质量评估（CAQA），使AQA具备持续学习能力，以处理不断变化的分布，同时减轻灾难性遗忘。&lt;h4&gt;方法&lt;/h4&gt;提出自适应流形对齐图正则化（MAGR++），将骨干网络微调（稳定浅层层，同时适应深层层）与两步特征校正流程相结合：流形投影器将偏离的历史特征转换到当前表示空间，以及图正则化器对齐局部和全局分布。&lt;h4&gt;主要发现&lt;/h4&gt;全参数微调（FPFT）对有效的表示学习是必要的，但未经控制的FPFT会导致过拟合和特征流形偏移，从而加剧遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;MAGR++在四个CAQA基准上实现了最先进的性能，与最强基线相比，离线平均相关性提高了3.6%，在线提高了12.2%，证实了其鲁棒性和有效性。代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;动作质量评估（AQA）量化视频中的人类动作，支持体育评分、康复和技能评估等应用。主要挑战在于现实场景中质量分布的非平稳性，这限制了传统方法的泛化能力。我们引入了持续动作质量评估（CAQA），使AQA具备持续学习（CL）能力，以处理不断变化的分布，同时减轻灾难性遗忘。尽管预训练模型的参数高效微调在图像分类的CL中显示出前景，但我们发现它对CAQA来说不足。我们的实证和理论分析揭示了两个见解：（i）全参数微调（FPFT）对有效的表示学习是必要的；（ii）未经控制的FPFT会导致过拟合和特征流形偏移，从而加剧遗忘。为解决这一问题，我们提出了自适应流形对齐图正则化（MAGR++），它将骨干网络微调（稳定浅层层，同时适应深层层）与两步特征校正流程相结合：流形投影器将偏离的历史特征转换到当前表示空间，以及图正则化器对齐局部和全局分布。我们从三个数据集构建了四个CAQA基准，包含定制的评估协议和强大的基线，实现了跨数据集的系统比较。大量实验表明MAGR++达到了最先进的性能，与最强基线相比，离线平均相关性提高了3.6%，在线提高了12.2%，证实了其鲁棒性和有效性。我们的代码可在 https://github.com/ZhouKanglei/MAGRPP 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Action Quality Assessment (AQA) quantifies human actions in videos,supporting applications in sports scoring, rehabilitation, and skillevaluation. A major challenge lies in the non-stationary nature of qualitydistributions in real-world scenarios, which limits the generalization abilityof conventional methods. We introduce Continual AQA (CAQA), which equips AQAwith Continual Learning (CL) capabilities to handle evolving distributionswhile mitigating catastrophic forgetting. Although parameter-efficientfine-tuning of pretrained models has shown promise in CL for imageclassification, we find it insufficient for CAQA. Our empirical and theoreticalanalyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) isnecessary for effective representation learning; yet (ii) uncontrolled FPFTinduces overfitting and feature manifold shift, thereby aggravating forgetting.To address this, we propose Adaptive Manifold-Aligned Graph Regularization(MAGR++), which couples backbone fine-tuning that stabilizes shallow layerswhile adapting deeper ones with a two-step feature rectification pipeline: amanifold projector to translate deviated historical features into the currentrepresentation space, and a graph regularizer to align local and globaldistributions. We construct four CAQA benchmarks from three datasets withtailored evaluation protocols and strong baselines, enabling systematiccross-dataset comparison. Extensive experiments show that MAGR++ achievesstate-of-the-art performance, with average correlation gains of 3.6% offlineand 12.2% online over the strongest baseline, confirming its robustness andeffectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.</description>
      <author>example@mail.com (Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang)</author>
      <guid isPermaLink="false">2510.06842v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
  <item>
      <title>Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.08442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://andrewcwlee.github.io/gaze-on-the-prize&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的视觉强化学习方法，通过引入受人类视觉注视启发的可学习注意力机制，解决了高维图像数据中样本效率低下的问题。该方法利用回报引导的对比学习，使代理能够关注与任务相关的特征，从而提高学习效率和稳定性。&lt;h4&gt;背景&lt;/h4&gt;视觉强化学习代理必须基于高维图像数据学习行动，其中只有一小部分像素与任务相关。这迫使代理在无关特征上浪费探索和计算资源，导致样本效率低下且学习不稳定。&lt;h4&gt;目的&lt;/h4&gt;解决视觉强化学习中样本效率低下和不稳定学习的问题，引入一个受人类视觉注视启发的框架，使代理能够专注于任务相关特征。&lt;h4&gt;方法&lt;/h4&gt;提出了'Gaze on the Prize'框架，通过可学习的中心注意力机制增强视觉RL，使用源自代理追求更高回报经验的自监督信号引导。通过回报差异揭示任务相关特征，实现回报引导的对比学习，训练注意力区分与成功和失败相关的特征，利用相似视觉表示的分组构建对比三元组提供训练信号。&lt;h4&gt;主要发现&lt;/h4&gt;方法实现了高达2.4倍的样本效率提升，可以解决基线方法无法学习的任务，在ManiSkill3基准测试的一系列操作任务上得到验证。&lt;h4&gt;结论&lt;/h4&gt;该框架无需修改底层算法或超参数即可显著提高视觉强化学习的效率和稳定性，使代理能够更有效地学习从高维图像数据中提取任务相关信息。&lt;h4&gt;翻译&lt;/h4&gt;视觉强化学习代理必须基于高维图像数据学习行动，其中只有一小部分像素与任务相关。这迫使代理在无关特征上浪费探索和计算资源，导致样本效率低下且学习不稳定。为此，受人类视觉注视的启发，我们引入了'Gaze on the Prize'。该框架通过可学习的中心注意力机制增强了视觉RL，并由源自代理追求更高回报经验的自监督信号引导。我们的关键见解是回报差异揭示了什么最重要：如果两个相似表示产生不同结果，它们的区分特征很可能与任务相关，注视应相应地关注它们。这是通过回报引导的对比学习实现的，该训练注意力区分与成功和失败相关的特征。我们根据回报差异将相似的视觉表示分组为正样本和负样本，并使用生成的标签构建对比三元组。这些三元组提供训练信号，教导注意力机制为与不同结果相关的状态生成可区分的表示。我们的方法在样本效率上实现了高达2.4倍的提升，并且可以解决基线方法无法学习的任务，这已在ManiSkill3基准测试的一系列操作任务上得到证明，且无需修改底层算法或超参数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Reinforcement Learning (RL) agents must learn to act based onhigh-dimensional image data where only a small fraction of the pixels istask-relevant. This forces agents to waste exploration and computationalresources on irrelevant features, leading to sample-inefficient and unstablelearning. To address this, inspired by human visual foveation, we introduceGaze on the Prize. This framework augments visual RL with a learnable fovealattention mechanism (Gaze), guided by a self-supervised signal derived from theagent's experience pursuing higher returns (the Prize). Our key insight is thatreturn differences reveal what matters most: If two similar representationsproduce different outcomes, their distinguishing features are likelytask-relevant, and the gaze should focus on them accordingly. This is realizedthrough return-guided contrastive learning that trains the attention todistinguish between the features relevant to success and failure. We groupsimilar visual representations into positives and negatives based on theirreturn differences and use the resulting labels to construct contrastivetriplets. These triplets provide the training signal that teaches the attentionmechanism to produce distinguishable representations for states associated withdifferent outcomes. Our method achieves up to 2.4x improvement in sampleefficiency and can solve tasks that the baseline fails to learn, demonstratedacross a suite of manipulation tasks from the ManiSkill3 benchmark, all withoutmodifying the underlying algorithm or hyperparameters.</description>
      <author>example@mail.com (Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani)</author>
      <guid isPermaLink="false">2510.08442v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Self-Supervised Learning at the Edge: An Energy Perspective</title>
      <link>http://arxiv.org/abs/2510.08374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了四种对比学习框架在资源受限设备上的部署可行性，重点关注能源消耗和训练数据减少条件下的表现，并发现SimCLR实际上具有最低的能源消耗。&lt;h4&gt;背景&lt;/h4&gt;对比学习在自监督表征学习中显示出巨大潜力，但在资源受限设备上的部署研究仍然不足。传统CL框架的训练需要大量计算资源，对能源消耗、数据可用性和内存使用构成挑战。&lt;h4&gt;目的&lt;/h4&gt;评估四种广泛使用的CL框架（SimCLR、MoCo、SimSiam和Barlow Twins）在边缘和雾计算环境中的实际可行性，并提供关于在处理能力有限的边缘/雾环境中部署CL的资源影响见解。&lt;h4&gt;方法&lt;/h4&gt;对四种CL框架进行系统评估，引入包括能耗分析和减少训练数据条件在内的基准测试策略，并评估轻量级神经网络架构与CL框架配对时的表现。&lt;h4&gt;主要发现&lt;/h4&gt;SimCLR尽管被认为计算成本高，但在各种数据条件下显示出最低的能源消耗，这与其普遍认知相反。&lt;h4&gt;结论&lt;/h4&gt;研究为在处理能力有限的边缘/雾环境中部署对比学习提供了资源影响见解，并为未来优化开辟了几个研究方向。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比学习(CL)在自监督表征学习中显示出巨大潜力，但其资源受限设备上的部署在很大程度上仍未被探索。传统CL框架训练所需的巨大计算需求带来了一系列挑战，特别是在能源消耗、数据可用性和内存使用方面。我们对四种广泛使用的CL框架进行了评估：SimCLR、MoCo、SimSiam和Barlow Twins。我们关注这些CL框架在边缘和雾计算部署中的实际可行性，并引入了一种包括能耗分析和减少训练数据条件的系统基准测试策略。我们的研究结果显示，SimCLR与其感知的计算成本相反，在各种数据条件下表现出最低的能源消耗。最后，我们还通过评估轻量级神经网络架构与CL框架配对时的表现扩展了我们的分析。我们的研究旨在提供关于在处理能力有限的边缘/雾环境中部署CL的资源影响见解，并为其未来优化开辟了几个研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While contrastive learning (CL) shows considerable promise in self-supervisedrepresentation learning, its deployment on resource-constrained devices remainslargely underexplored. The substantial computational demands required fortraining conventional CL frameworks pose a set of challenges, particularly interms of energy consumption, data availability, and memory usage. We conduct anevaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and BarlowTwins. We focus on the practical feasibility of these CL frameworks for edgeand fog deployment, and introduce a systematic benchmarking strategy thatincludes energy profiling and reduced training data conditions. Our findingsreveal that SimCLR, contrary to its perceived computational cost, demonstratesthe lowest energy consumption across various data regimes. Finally, we alsoextend our analysis by evaluating lightweight neural architectures when pairedwith CL frameworks. Our study aims to provide insights into the resourceimplications of deploying CL in edge/fog environments with limited processingcapabilities and opens several research directions for its future optimization.</description>
      <author>example@mail.com (Fernanda Famá, Roberto Pereira, Charalampos Kalalas, Paolo Dini, Lorena Qendro, Fahim Kawsar, Mohammad Malekzadeh)</author>
      <guid isPermaLink="false">2510.08374v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Channel Charting based Fast Beam Tracking Design and Implementation</title>
      <link>http://arxiv.org/abs/2510.08144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于信道映射的低开销波束跟踪算法，用于毫米波通信系统，显著减少了波束扫描时间同时保持高准确度。&lt;h4&gt;背景&lt;/h4&gt;在第五代移动通信技术（B5G）和即将到来的第六代移动通信技术（6G）无线通信系统中，毫米波技术是提供额外带宽资源和缓解频谱拥堵的很有前景的解决方案。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于信道映射的低开销波束跟踪算法，显著减少跟踪过程中的波束扫描时间。&lt;h4&gt;方法&lt;/h4&gt;通过将波束信息投影到信道映射中，将波束跟踪问题转换为在信道映射中获取波束簇；利用对比学习，将高维信道状态信息投影到保持空间邻近性的低维特征空间；使用动态候选波束获取策略，显著降低波束跟踪算法的复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法在模拟环境中实现了98.27%的准确度；与现有方法相比，可以减少高达55.9%的波束扫描时间；现场测试展示了移动过程中的优秀通信质量。&lt;h4&gt;结论&lt;/h4&gt;基于信道映射的低开销波束跟踪算法能够有效提高毫米波通信系统的波束跟踪性能，减少扫描时间同时保持高准确度。&lt;h4&gt;翻译&lt;/h4&gt;在第五代移动通信技术（B5G）和即将到来的第六代移动通信技术（6G）无线通信系统中，毫米波技术是提供额外带宽资源和缓解频谱拥堵的很有前景的解决方案。波束跟踪是毫米波通信系统中提供可靠通信服务的关键程序，面临的挑战是提供持续且准确的跟踪性能。在本研究中，我们引入了一种基于信道映射的低开销波束跟踪算法，显著减少了跟踪过程中的波束扫描时间。通过将波束信息投影到信道映射中，波束跟踪问题转换为在信道映射中获取波束簇。利用对比学习，所提出的信道映射将高维信道状态信息投影到保持空间邻近性的低维特征空间。使用动态候选波束获取策略，我们显著降低了波束跟踪算法的复杂度。所提出的算法在保持高预测准确度的同时显著降低了扫描复杂度，在模拟环境中实现了98.27%的准确度。与现有方法相比，所提出的方法可以减少高达55.9%的波束扫描时间。此外，我们还进行了现场测试，测量结果展示了移动过程中的优秀通信质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the beyond fifth-generation (B5G) and upcoming sixth-generation (6G)wireless communication systems, millimeter (mmWave) wave technology is apromising solution for offering additional bandwidth resources and mitigatingspectrum congestion. Beam tracking is an essential procedure for providingreliable communication services in the mmWave communication system, with thechallenge of providing consistent and accurate tracking performance. In thisstudy, we introduce a low-overhead beam tracking algorithm based on channelcharting, which significantly reduces beam scanning times during the trackingprocess. By projecting the beam information to the channel chart, the beamtracking problem is transformed into the acquisition of the beam cluster in thechannel chart. Leveraging contrastive learning, the proposed channel chartprojects high-dimensional channel state information into a low-dimensionalfeature space that preserves spatial proximities. Using a dynamic candidatebeam acquisition strategy, the complexity of our beam tracking algorithm issignificantly reduced. The proposed algorithm significantly reduces scanningcomplexity while maintaining high prediction accuracy, achieving an accuracy of98.27\% in simulation environments. Compared to existing methods, the proposedmethod can reduce beam scanning times by up to 55.9\%. In addition, we alsoperformed field tests, and the measured results demonstrated excellentcommunication quality during mobility.</description>
      <author>example@mail.com (Jiawei Zhang, Shihan Wang, Jienan Chen, Fan Wu, Jiyun Tao, Zheqi Gu)</author>
      <guid isPermaLink="false">2510.08144v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>SALAD-VAE: Semantic Audio Compression with Language-Audio Distillation</title>
      <link>http://arxiv.org/abs/2510.07592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SALAD-VAE是一种连续且高度紧凑的语义音频变分自编码器，在频域中运行，实现了低潜在帧率下的最先进压缩，同时揭示语义结构并产生高质量音频。&lt;h4&gt;背景&lt;/h4&gt;现代生成和多模态模型越来越依赖于紧凑的潜在表示，这些表示在语义丰富度和高保真度重建之间进行权衡和平衡。&lt;h4&gt;目的&lt;/h4&gt;引入SALAD-VAE，实现音频领域的高效压缩和高质量重建，同时保持语义结构。&lt;h4&gt;方法&lt;/h4&gt;在频域中操作，使用7.8Hz的极低潜在帧率，增强标准VAE的语义损失和数据增强，采用对比学习和基于CLAP的嵌入蒸馏，构建计算复杂度显著低于同类模型的架构。&lt;h4&gt;主要发现&lt;/h4&gt;SALAD-VAE能够在多样化的音频领域泛化，与最先进的VAE重建质量相匹配，同时在各种分类基准测试中持续优于它们，并提供训练好的CLAP投影层用于零样本任务。&lt;h4&gt;结论&lt;/h4&gt;SALAD-VAE实现了高效音频表示，在低潜在帧率下保持高质量重建，具有多功能性，可用于零样本音频标题和分类任务。&lt;h4&gt;翻译&lt;/h4&gt;现代生成和多模态模型越来越依赖于紧凑的潜在表示，这些表示在语义丰富度与高保真度重建之间进行权衡和平衡。我们引入了SALAD-VAE，一种连续且高度紧凑的语义音频变分自编码器，它在频域中运行，以极低的潜在帧率(7.8Hz)实现最先进的压缩，同时揭示语义结构并产生高质量的音频。我们增强了标准VAE的语义损失和数据增强，特别是对比学习和基于CLAP的嵌入蒸馏，使其能够泛化到多样化的音频领域。与可比的最先进VAE相比，SALAD-VAE的计算复杂度显著降低，同时匹配它们的重建质量，并在各种分类基准测试中持续优于它们。此外，提出的额外损失函数提供了训练好的CLAP投影层，可用于零样本音频标题和分类，与预训练的CLAP音频文本嵌入相匹配。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern generative and multimodal models increasingly rely on compact latentrepresentations that trade and balance semantic richness with high-fidelityreconstruction. We introduce SALAD-VAE, a continuous and highly compactsemantic Audio Variational Autoencoder, which operates in the frequency domainand achieves state-of-the-art compression with very low latent frame rate (7.8Hz) while surfacing semantic structure and producing high audio quality. Weenhance the standard VAE semantic losses and augmentation, specificallycontrastive learning and CLAP-based embedding distillation, enabling it togeneralize across diverse audio domains. With a significantly lesscomputational complex architecture than comparable state-of-the-art VAEs,SALAD-VAE matches their reconstruction quality while it consistentlyoutperforms them on a wide range of classification benchmarks. Furthermore, theproposed additional loss function provides a trained CLAP projection layer,which can be used zero-shot audio captioning and classification matchingpretrained CLAP audio-text embeddings.</description>
      <author>example@mail.com (Sebastian Braun, Hannes Gamper, Dimitra Emmanouilidou)</author>
      <guid isPermaLink="false">2510.07592v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.03690v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一框架，用于处理现实世界图数据集中的混合群体问题，通过图矩聚类和解混合成分，改进了图对比学习和数据增强技术，在无监督和监督学习中都取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;现实世界图数据集通常由多个不同的底层分布生成，形成混合群体结构。然而，现代图表示学习方法，如图对比学习和Mixup等增强方法，通常忽略了这种混合结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够明确建模图数据作为底层概率图生成模型混合的框架，并通过模型感知的分区改进图学习任务，包括数据增强和对比学习。&lt;h4&gt;方法&lt;/h4&gt;利用图矩（motif密度）来聚类来自同一模型的图，解混合成分并识别不同的生成机制。提出图混合感知的Mixup(GMAM)作为数据增强技术，以及模型自适应的对比学习框架MGCL，改进负采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;建立了图切割距离与图矩密度之间的理论保证，表明具有小切割距离的图具有相似motif密度。实验显示，MGCL在无监督学习中八个数据集上获得平均排名第一，GMAM在监督学习中七个数据集中的六个上达到最先进准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型感知框架有效解决了图数据中的混合群体问题，通过理论保证和实验验证了其在图学习任务中的优越性，为图表示学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;现实世界图数据集通常由混合群体组成，其中图是从多个不同的底层分布生成的。然而，现代表示学习方法，如图对比学习和Mixup等增强方法，通常忽略了这种混合结构。在这项工作中，我们提出了一个统一框架，明确地将数据建模为由图表示的底层概率图生成模型的混合。为了表征这些图，我们利用图矩（motif密度）来聚类来自同一模型的图。这使我们能够解混合成分并识别其不同的生成机制。这种模型感知的分区有利于两个关键的图学习任务：1) 它使图混合感知的Mixup(GMAM)成为可能，这是一种数据增强技术，在估计的图引导下在语义有效的空间中进行插值，而不是假设每类只有一个图。2) 对于GCL，它使模型自适应和有原则的增强成为可能。此外，通过引入新的模型感知目标，我们提出的方法（称为MGCL）通过将负样本限制为来自其他模型的图来改进负采样。我们建立了一个关键的理论保证：一个新的、更紧的界限表明，从具有小切割距离的图采样的图将具有相似的motif密度，具有高概率。在基准数据集上的广泛实验展示了强大的实证性能。在无监督学习中，MGCL取得了最先进的结果，在八个数据集上获得了平均排名第一。在监督学习中，GMAM始终优于现有策略，在7个数据集中的6个上取得了新的最先进准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world graph datasets often consist of mixtures of populations, wheregraphs are generated from multiple distinct underlying distributions. However,modern representation learning approaches, such as graph contrastive learning(GCL) and augmentation methods like Mixup, typically overlook this mixturestructure. In this work, we propose a unified framework that explicitly modelsdata as a mixture of underlying probabilistic graph generative modelsrepresented by graphons. To characterize these graphons, we leverage graphmoments (motif densities) to cluster graphs arising from the same model. Thisenables us to disentangle the mixture components and identify their distinctgenerative mechanisms. This model-aware partitioning benefits two key graphlearning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a dataaugmentation technique that interpolates in a semantically valid space guidedby the estimated graphons, instead of assuming a single graphon per class. 2)For GCL, it enables model-adaptive and principled augmentations. Additionally,by introducing a new model-aware objective, our proposed approach (termed MGCL)improves negative sampling by restricting negatives to graphs from othermodels. We establish a key theoretical guarantee: a novel, tighter boundshowing that graphs sampled from graphons with small cut distance will havesimilar motif densities with high probability. Extensive experiments onbenchmark datasets demonstrate strong empirical performance. In unsupervisedlearning, MGCL achieves state-of-the-art results, obtaining the top averagerank across eight datasets. In supervised learning, GMAM consistentlyoutperforms existing strategies, achieving new state-of-the-art accuracy in 6out of 7 datasets.</description>
      <author>example@mail.com (Ali Azizpour, Reza Ramezanpour, Ashutosh Sabharwal, Santiago Segarra)</author>
      <guid isPermaLink="false">2510.03690v2</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models</title>
      <link>http://arxiv.org/abs/2510.08492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  63 pages, 29 tables, and 47 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为UML（非配对多模态学习器）的新训练范式，利用辅助的非配对多模态数据增强目标模态的表示学习，无需显式配对数据。&lt;h4&gt;背景&lt;/h4&gt;传统多模态学习器为视觉问答等任务寻找统一表示，但严重依赖于配对数据集。&lt;h4&gt;目的&lt;/h4&gt;探索能否利用辅助的非配对多模态数据来直接增强目标模态中的表示学习。&lt;h4&gt;方法&lt;/h4&gt;提出UML（Unpaired Multimodal Learner），一种与模态无关的训练范式，其中单个模型交替处理来自不同模态的输入，同时在它们之间共享参数。这种设计利用了不同模态是共享底层现实投影的假设。&lt;h4&gt;主要发现&lt;/h4&gt;在线性数据生成假设下，证明非配对的辅助数据可以产生比单模态训练更严格地关于数据生成过程的信息表示；使用来自辅助模态的非配对数据可以持续改善各种单模态目标的下游性能。&lt;h4&gt;结论&lt;/h4&gt;UML能够有效利用非配对多模态数据增强表示学习，无需显式配对，为多模态学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;传统多模态学习器为视觉问答等任务寻找统一表示，但严重依赖于配对数据集。然而，一个被忽视但可能很强大的问题是：能否利用辅助的非配对多模态数据来直接增强目标模态中的表示学习？我们提出了UML：非配对多模态学习器，一种与模态无关的训练范式，其中单个模型交替处理来自不同模态的输入，同时在它们之间共享参数。这种设计利用了不同模态是共享底层现实投影的假设，使模型能够从跨模态结构中受益，而不需要显式的配对。理论上，在线性数据生成假设下，我们证明非配对的辅助数据可以产生比单模态训练更严格地关于数据生成过程的信息表示。实验上，我们表明，使用来自辅助模态的非配对数据（如文本、音频或图像）可以持续改善各种单模态目标（如图像和音频）的下游性能。我们的项目页面：https://unpaired-multimodal.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional multimodal learners find unified representations for tasks likevisual question answering, but rely heavily on paired datasets. However, anoverlooked yet potentially powerful question is: can one leverage auxiliaryunpaired multimodal data to directly enhance representation learning in atarget modality? We introduce UML: Unpaired Multimodal Learner, amodality-agnostic training paradigm in which a single model alternatelyprocesses inputs from different modalities while sharing parameters acrossthem. This design exploits the assumption that different modalities areprojections of a shared underlying reality, allowing the model to benefit fromcross-modal structure without requiring explicit pairs. Theoretically, underlinear data-generating assumptions, we show that unpaired auxiliary data canyield representations strictly more informative about the data-generatingprocess than unimodal training. Empirically, we show that using unpaired datafrom auxiliary modalities -- such as text, audio, or images -- consistentlyimproves downstream performance across diverse unimodal targets such as imageand audio. Our project page: https://unpaired-multimodal.github.io/</description>
      <author>example@mail.com (Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola)</author>
      <guid isPermaLink="false">2510.08492v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens</title>
      <link>http://arxiv.org/abs/2510.08222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文从因果角度重新审视推理任务，提出SR²框架，通过将估计的潜在变量作为反馈纳入选择机制，显著提高了模型在推理任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;推理任务因其内在复杂性，长期以来被视为评估机器学习模型（特别是大型语言模型）能力的严格基准。虽然人类可以轻松解决这些任务，但现有模型即使在大量预训练和后训练后，仍然无法可靠地执行推理。&lt;h4&gt;目的&lt;/h4&gt;从因果角度重新审视推理任务，试图理解它们在潜在空间中的行为，并为解决这些挑战提供见解。&lt;h4&gt;方法&lt;/h4&gt;将推理任务描述为选择机制，其中高级逻辑概念作为对给定观察的选择算子；提出SR²框架，包含三个关键模块：反思性表征学习、依赖自我细化和周期性中间对齐，将估计的潜在变量作为反馈纳入选择机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在推理准确性方面取得了显著提升，在数独和迷宫任务上，与最新进展相比，参数量减少8倍的情况下，性能提高了10%以上。&lt;h4&gt;结论&lt;/h4&gt;通过从因果角度重新审视推理任务，并引入SR²框架，能够有效提高模型在推理任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;由于其内在复杂性，推理任务长期以来一直被视为评估机器学习模型（特别是大型语言模型）能力的严格基准。虽然人类可以轻松解决这些任务，但现有模型即使在大量预训练和后训练后，仍然无法可靠地执行推理。在本文中，我们从因果角度重新审视推理任务，试图理解它们在潜在空间中的行为，并为解决这些挑战提供见解。具体来说，我们将推理任务描述为一种选择机制，其中高级逻辑概念作为对给定观察的选择算子，例如，在数学问题中识别正确答案或在数独中填写适当条目。我们强调了这个表述的两个关键特性，这些特性揭示了推理任务的难度。首先，即使正确答案完全由观察到的输入决定，潜在空间的复杂性也超过了观察空间。其次，与逻辑思维对应的潜在变量是密集结构的，并表现出强相关性。基于这一表述，我们引入了一个名为SR²的框架，该框架将估计的潜在变量作为反馈纳入选择机制，从而促进潜在表示之间密集依赖关系的学习。该框架包含三个关键模块：反思性表征学习、依赖自我细化和周期性中间对齐。实验表明，我们的方法在推理准确性方面取得了显著提升，例如，在数独和迷宫任务上，与最新进展相比，参数量减少8倍的情况下，性能提高了10%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to their inherent complexity, reasoning tasks have long been regarded asrigorous benchmarks for assessing the capabilities of machine learning models,especially large language models (LLMs). Although humans can solve these taskswith ease, existing models, even after extensive pre-training and post-trainingat scale, still fail to perform reasoning reliably. In this paper, we revisitreasoning tasks from a causal perspective, seeking to understand their behaviorin latent space and to offer insights for addressing their challenges.Specifically, we cast reasoning tasks as a selection mechanism, in whichhigh-level logical concepts function as selection operators on the givenobservations, such as, identifying the correct answer in a math problem orfilling the appropriate entry in Sudoku. We emphasize two key properties ofthis formulation that shed light on the difficulty of reasoning tasks. First,the latent space exceeds the observation space in complexity, even when thecorrect answer is fully determined by the observed input. Second, the latentvariables, corresponding to logical thought, are densely structured and exhibitstrong dependencies. Building on this formulation, we introduce a framework,called SR$^2$, that incorporates the estimated latent variables as feedbackinto the selection mechanism, thereby facilitating the learning of densedependencies among latent representations. The framework consists of three keymodules: reflective representation learning, dependency self-refinement, andperiodic intermediate alignment. Experimentally, we show that our approachyields significant gains in reasoning accuracy, for example, attaining over10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudokuand Maze tasks over the recent advances.</description>
      <author>example@mail.com (Yunlong Deng, Boyang Sun, Yan Li, Lingjing Kong, Zeyu Tang, Kun Zhang, Guangyi Chen)</author>
      <guid isPermaLink="false">2510.08222v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation</title>
      <link>http://arxiv.org/abs/2510.07910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Medical Image Computing and Computer-Assisted Intervention (MICCAI)  Predictive Intelligence in Medicine Workshop (MICCAI PRIME) 2025; 13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MMM的新型框架，通过整合三维量子化学信息到药物表示学习中，结合电子局域化函数(ELF)生成的三维电子密度图和二部图编码器，以捕捉药物分子的全局电子特性和局部子结构相互作用。实验结果表明，该方法在药物-药物相互作用预测上比基线模型有统计学上的显著改进，有潜力提高临床药物推荐的安全性。&lt;h4&gt;背景&lt;/h4&gt;药物推荐是机器学习支持的临床决策支持系统中的关键任务，但联合用药之间的药物-药物相互作用(DDI)风险仍然是一个重大挑战。之前的研究使用图神经网络(GNN)来表示药物结构，但其简化的离散形式无法完全捕捉分子结合亲和力和反应性。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为MMM的新型框架，将三维量子化学信息整合到药物表示学习中，以提高药物-药物相互作用的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;使用电子局域化函数(ELF)生成三维电子密度图，结合ELF衍生的特征(编码全局电子特性)和二部图编码器(建模局部子结构相互作用)，以学习药物分子的互补特性。在MIMIC-III数据集(250种药物，442个子结构)上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与基于GNN的SafeDrug模型相比，MMM在F1分数、Jaccard指数和DDI率方面显示出统计学上的显著改进。&lt;h4&gt;结论&lt;/h4&gt;ELF基础的3D表示具有提高预测准确率的潜力，可以支持临床实践中更安全的组合药物处方。&lt;h4&gt;翻译&lt;/h4&gt;药物推荐是基于机器学习的临床决策支持系统中的关键任务。然而，联合处方药物之间的药物-药物相互作用(DDI)风险仍然是一个重大挑战。先前的研究使用图神经网络(GNN)来表示药物结构。尽管如此，它们简化的离散形式无法完全捕捉分子结合亲和力和反应性。因此，我们提出了多模态DDI预测结合分子电子局域化函数(ELF)图(MMM)，这是一个新型框架，将三维量子化学信息整合到药物表示学习中。它使用ELF生成三维电子密度图。为了捕捉治疗相关性和相互作用风险，MMM结合了ELF衍生的特征，这些特征编码全局电子特性，以及一个建模局部子结构相互作用的二部图编码器。这种设计使学习药物分子的互补特性成为可能。我们在MIMIC-III数据集上评估MMM，并将其与几个基线模型进行比较。特别是，与基于GNN的SafeDrug模型的比较表明，在多个指标方面有统计学上的显著改进。这些结果表明，基于ELF的三维表示有可能提高预测准确性，并支持临床实践中更安全的组合药物处方。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug recommendation is an essential task in machine learning-based clinicaldecision support systems. However, the risk of drug-drug interactions (DDI)between co-prescribed medications remains a significant challenge. Previousstudies have used graph neural networks (GNNs) to represent drug structures.Regardless, their simplified discrete forms cannot fully capture the molecularbinding affinity and reactivity. Therefore, we propose Multimodal DDIPrediction with Molecular Electron Localization Function (ELF) Maps (MMM), anovel framework that integrates three-dimensional (3D) quantum-chemicalinformation into drug representation learning. It generates 3D electron densitymaps using the ELF. To capture both therapeutic relevance and interactionrisks, MMM combines ELF-derived features that encode global electronicproperties with a bipartite graph encoder that models local substructureinteractions. This design enables learning complementary characteristics ofdrug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442substructures), comparing it with several baseline models. In particular, acomparison with the GNN-based SafeDrug model demonstrates statisticallysignificant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),and the DDI rate (p = 0.0386). These results demonstrate the potential ofELF-based 3D representations to enhance prediction accuracy and support safercombinatorial drug prescribing in clinical practice.</description>
      <author>example@mail.com (Chongmyung Kwon, Yujin Kim, Seoeun Park, Yunji Lee, Charmgil Hong)</author>
      <guid isPermaLink="false">2510.07910v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials</title>
      <link>http://arxiv.org/abs/2510.07853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何利用自监督学习表征来有效识别毒物诱导的变化，并通过EmbryoNet数据集验证了该方法能够区分不同化合物的作用机制。&lt;h4&gt;背景&lt;/h4&gt;高通量毒性测试提供了一种快速且经济高效的方法来测试大量化合物，而自动化评估通过机器学习模型实现是这类系统的关键组成部分。&lt;h4&gt;目的&lt;/h4&gt;解决高通量毒性测试领域的关键挑战，并展示如何通过自监督学习学习的表征来有效识别毒物诱导的变化。&lt;h4&gt;方法&lt;/h4&gt;利用公开的EmbryoNet数据集进行概念验证，该数据集包含斑马鱼胚胎的十种表型，这些表型由针对早期胚胎发育中不同过程的各种化学化合物引起，并采用自监督学习方法学习表征。&lt;h4&gt;主要发现&lt;/h4&gt;使用自监督学习学习的表征能够有效区分不同化合物的作用机制。&lt;h4&gt;结论&lt;/h4&gt;讨论了在TOXBOX项目中机器学习模型与物理毒性测试设备的集成前景。&lt;h4&gt;翻译&lt;/h4&gt;高通量毒性测试提供了一种快速且经济高效的方法来测试大量化合物。这类系统的关键组成部分是通过机器学习模型进行自动化评估。在本文中，我们解决了该领域的关键挑战，并展示了如何通过自监督学习学习的表征来有效识别毒物诱导的变化。我们提供了一个概念验证，利用公开的EmbryoNet数据集，该数据集包含由针对早期胚胎发育中不同过程的各种化学化合物引起的十种斑马鱼胚胎表型。我们的分析显示，使用自监督学习学习的表征适合有效区分不同化合物的作用机制。最后，我们在TOXBOX项目的背景下讨论了机器学习模型与物理毒性测试设备的集成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-throughput toxicity testing offers a fast and cost-effective way to testlarge amounts of compounds. A key component for such systems is the automatedevaluation via machine learning models. In this paper, we address criticalchallenges in this domain and demonstrate how representations learned viaself-supervised learning can effectively identify toxicant-induced changes. Weprovide a proof-of-concept that utilizes the publicly available EmbryoNetdataset, which contains ten zebrafish embryo phenotypes elicited by variouschemical compounds targeting different processes in early embryonicdevelopment. Our analysis shows that the learned representations usingself-supervised learning are suitable for effectively distinguishing betweenthe modes-of-action of different compounds. Finally, we discuss the integrationof machine learning models in a physical toxicity testing device in the contextof the TOXBOX project.</description>
      <author>example@mail.com (Thomas Lautenschlager, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Katja Nau, Gaëlle Hayot, Thomas Dickmeis, Ralf Mikut)</author>
      <guid isPermaLink="false">2510.07853v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Causality Guided Representation Learning for Cross-Style Hate Speech Detection</title>
      <link>http://arxiv.org/abs/2510.07707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CADET是一种因果表示学习框架，通过解构仇恨言论为可解释的潜在因素并控制混杂因素，有效分离真实仇恨意图与表面语言线索，实现仇恨言论的稳健检测。&lt;h4&gt;背景&lt;/h4&gt;网络仇恨言论泛滥威胁网络和谐；显性仇恨易识别，但隐性仇恨（通过讽刺、反讽等表达）更难检测；现有模型依赖表面语言线索，难以泛化；不同平台仇恨言论针对不同群体且风格独特，导致虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;建立仇恨言论生成的因果图模型，包含上下文环境、创作者动机、目标和风格；提出CADET框架以提升仇恨言论检测的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;CADET框架将仇恨言论解构为可解释的潜在因素；控制混杂因素分离真实仇恨意图；通过在潜在空间中对风格进行干预实现反事实推理。&lt;h4&gt;主要发现&lt;/h4&gt;CADET在综合实验中表现出优越性能；因果先验在推进可泛化的仇恨言论检测方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;因果表示学习方法可以有效解决仇恨言论检测中的泛化问题，使模型能够稳健识别各种形式的仇恨言论。&lt;h4&gt;翻译&lt;/h4&gt;网络仇恨言论的泛滥对网络和谐构成严重威胁。虽然显性仇恨言论通过明显的侮辱性语言容易被识别，但隐性仇恨言论通常通过讽刺、反讽、刻板印象或编码语言表达——这使得它更难被检测。现有的仇恨言论检测模型主要依赖表面语言线索，无法在多样化的风格变化中有效泛化。此外，不同平台上传播的仇恨言论往往针对不同的群体并采用独特的风格，这可能导致它们与标签之间产生虚假相关性，进一步挑战了当前的检测方法。受这些观察的启发，我们假设仇恨言论的生成可以建模为一个包含关键因素的因果图：上下文环境、创作者动机、目标和风格。受此图指导，我们提出了CADET，一个因果表示学习框架，该框架将仇恨言论解构为可解释的潜在因素，然后控制混杂因素，从而将真实的仇恨意图与表面语言线索分离。此外，CADET允许通过在潜在空间中对风格进行干预来实现反事实推理，自然地引导模型以稳健地识别各种形式的仇恨言论。CADET在综合实验中表现出优越性能，突显了因果先验在推进可泛化的仇恨言论检测方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of online hate speech poses a significant threat to theharmony of the web. While explicit hate is easily recognized through overtslurs, implicit hate speech is often conveyed through sarcasm, irony,stereotypes, or coded language -- making it harder to detect. Existing hatespeech detection models, which predominantly rely on surface-level linguisticcues, fail to generalize effectively across diverse stylistic variations.Moreover, hate speech spread on different platforms often targets distinctgroups and adopts unique styles, potentially inducing spurious correlationsbetween them and labels, further challenging current detection approaches.Motivated by these observations, we hypothesize that the generation of hatespeech can be modeled as a causal graph involving key factors: contextualenvironment, creator motivation, target, and style. Guided by this graph, wepropose CADET, a causal representation learning framework that disentangleshate speech into interpretable latent factors and then controls confounders,thereby isolating genuine hate intent from superficial linguistic cues.Furthermore, CADET allows counterfactual reasoning by intervening on stylewithin the latent space, naturally guiding the model to robustly identify hatespeech in varying forms. CADET demonstrates superior performance incomprehensive experiments, highlighting the potential of causal priors inadvancing generalizable hate speech detection.</description>
      <author>example@mail.com (Chengshuai Zhao, Shu Wan, Paras Sheth, Karan Patwa, K. Selçuk Candan, Huan Liu)</author>
      <guid isPermaLink="false">2510.07707v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:45 +0800</pubDate>
    </item>
    <item>
      <title>Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging</title>
      <link>http://arxiv.org/abs/2510.07182v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Bridged Clustering的半监督学习框架，用于从无配对的输入X和输出Y数据集中学习预测器。&lt;h4&gt;背景&lt;/h4&gt;传统半监督学习和密集传输方法在处理无配对数据时存在局限性，前者没有充分利用仅输出数据，后者缺乏稀疏性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用无配对数据、保持稀疏性和可解释性的半监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;首先独立地对输入X和输出Y进行聚类，然后使用少量配对样本学习聚类之间的稀疏桥接。在推理时，将新输入分配到最近的输入聚类，然后返回链接的输出聚类的质心作为预测。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，在有界聚类错误和桥接错误率的情况下，该算法成为有效且高效的预测器。实验证明，该方法与最先进的方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;Bridged Clustering是一种简单、模型无关且在低监督设置中具有高标签效率的方法，能够有效处理无配对数据学习问题。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Bridged Clustering，一种半监督框架，用于从任意无配对的输入X和输出Y数据集中学习预测器。我们的方法首先独立地对X和Y进行聚类，然后仅使用少量配对样本学习聚类之间的稀疏、可解释的桥接。在推理时，新的输入x被分配到其最近的输入聚类，链接的输出聚类的质心被返回作为预测ŷ。与传统SSL不同，Bridged Clustering明确利用仅输出数据，与密集传输方法不同，它保持稀疏和可解释的对齐。通过理论分析，我们表明在有界聚类错误和桥接错误率的情况下，我们的算法成为有效且高效的预测器。经验上，我们的方法与最先进的方法具有竞争力，同时保持简单、模型无关，并且在低监督设置中具有高标签效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Bridged Clustering, a semi-supervised framework to learnpredictors from any unpaired input $X$ and output $Y$ dataset. Our method firstclusters $X$ and $Y$ independently, then learns a sparse, interpretable bridgebetween clusters using only a few paired examples. At inference, a new input$x$ is assigned to its nearest input cluster, and the centroid of the linkedoutput cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,Bridged Clustering explicitly leverages output-only data, and unlike densetransport-based methods, it maintains a sparse and interpretable alignment.Through theoretical analysis, we show that with bounded mis-clustering andmis-bridging rates, our algorithm becomes an effective and efficient predictor.Empirically, our method is competitive with SOTA methods while remainingsimple, model-agnostic, and highly label-efficient in low-supervision settings.</description>
      <author>example@mail.com (Patrick Peixuan Ye, Chen Shani, Ellen Vitercik)</author>
      <guid isPermaLink="false">2510.07182v2</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Limited Multi-Phase CT: Dual-Branch Prototype-Guided Framework for Early Recurrence Prediction in HCC</title>
      <link>http://arxiv.org/abs/2510.07347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DuoProto的双分支原型引导框架，用于从单相CT图像中增强肝细胞癌早期复发预测，通过在训练过程中利用有限的多相数据。&lt;h4&gt;背景&lt;/h4&gt;在肝细胞癌的根治性切除术后，早期复发预测仍是临床管理的重大挑战。临床指南推荐全多期对比增强CT，但并非所有机构都能提供完整期相覆盖。实践中，单相门静脉扫描常被单独使用，特别是在成像资源有限、采集协议不同或患者存在对比剂不耐受或运动伪影的情况下。这种差异导致理想化模型假设与实际部署约束不匹配，需要能有效利用有限多相数据的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从单相CT图像中有效预测肝细胞癌早期复发的方法，同时通过训练过程中利用多相数据提高预测准确性，解决实际临床环境中数据不完整的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DuoProto框架，采用双分支架构：主分支处理单相图像，辅助分支利用可用多期扫描通过跨域原型对齐引导表示学习。结构化原型表示作为类别锚点提高特征辨别能力，结合基于排序的监督机制纳入临床相关的复发风险因素。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，DuoProto优于现有方法，特别是在类别不平衡和缺失期相条件下。消融研究进一步验证了双分支、原型引导设计的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架符合当前临床应用需求，为肝细胞癌复发风险预测提供了通用解决方案，支持更明智的决策制定。&lt;h4&gt;翻译&lt;/h4&gt;在肝细胞癌的根治性切除术后，早期复发预测仍然是临床管理中的一个关键挑战。尽管临床指南推荐使用全多期采集的对比增强CT，并且在许多三级中心常规执行，但完整的期相覆盖并非在所有机构中都能一致可用。在实践中，单相门静脉扫描常常单独使用，特别是在成像资源有限、采集协议不同或患者相关因素(如对比剂不耐受或运动伪影)的情况下。这种差异性导致理想化模型假设与实际部署约束之间的不匹配，突显了需要能够有效利用有限多相数据的方法。为了应对这一挑战，我们提出了一个双分支原型引导框架，通过在训练过程中利用有限的多相数据，从单相CT中增强早期复发预测。该框架采用双分支架构：主分支处理单相图像，而辅助分支利用可用的多期扫描通过跨域原型对齐来引导表示学习。结构化的原型表示作为类别锚点来提高特征辨别能力，而基于排序的监督机制结合了临床相关的复发风险因素。大量实验证明，该框架优于现有方法，特别是在类别不平衡和缺失期相条件下。消融研究进一步验证了双分支、原型引导设计的有效性。我们的框架符合当前临床应用需求，并为肝细胞癌复发风险预测提供了通用解决方案，支持更明智的决策制定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early recurrence (ER) prediction after curative-intent resection remains acritical challenge in the clinical management of hepatocellular carcinoma(HCC). Although contrast-enhanced computed tomography (CT) with fullmulti-phase acquisition is recommended in clinical guidelines and routinelyperformed in many tertiary centers, complete phase coverage is not consistentlyavailable across all institutions. In practice, single-phase portal venous (PV)scans are often used alone, particularly in settings with limited imagingresources, variations in acquisition protocols, or patient-related factors suchas contrast intolerance or motion artifacts. This variability results in amismatch between idealized model assumptions and the practical constraints ofreal-world deployment, underscoring the need for methods that can effectivelyleverage limited multi-phase data. To address this challenge, we propose aDual-Branch Prototype-guided (DuoProto) framework that enhances ER predictionfrom single-phase CT by leveraging limited multi-phase data during training.DuoProto employs a dual-branch architecture: the main branch processessingle-phase images, while the auxiliary branch utilizes available multi-phasescans to guide representation learning via cross-domain prototype alignment.Structured prototype representations serve as class anchors to improve featurediscrimination, and a ranking-based supervision mechanism incorporatesclinically relevant recurrence risk factors. Extensive experiments demonstratethat DuoProto outperforms existing methods, particularly under class imbalanceand missing-phase conditions. Ablation studies further validate theeffectiveness of the dual-branch, prototype-guided design. Our framework alignswith current clinical application needs and provides a general solution forrecurrence risk prediction in HCC, supporting more informed decision-making.</description>
      <author>example@mail.com (Hsin-Pei Yu, Si-Qin Lyu, Yi-Hsien Hsieh, Weichung Wang, Tung-Hung Su, Jia-Horng Kao, Che Lin)</author>
      <guid isPermaLink="false">2510.07347v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>New Machine Learning Approaches for Intrusion Detection in ADS-B</title>
      <link>http://arxiv.org/abs/2510.08333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author's version of the work accepted for publication  Digital Avionics Systems Conference (DASC) 2025. The final version will be  available via IEEE Xplore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了利用机器学习模型提升ADS-B协议入侵检测系统性能的方法，比较了transformer编码器和xLSTM网络两种实现，结果表明xLSTM模型在检测性能上表现更优。&lt;h4&gt;背景&lt;/h4&gt;自动相关监视-广播(ADS-B)协议在空中交通管理(ATM)中的使用日益增加，但其存在安全漏洞，确保其安全性至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究新兴的机器学习模型和训练策略，提高基于AI的入侵检测系统(IDS)对ADS-B的保护能力，专注于地面ATM系统。&lt;h4&gt;方法&lt;/h4&gt;评估两种深度学习IDS实现：transformer编码器和xLSTM网络(首个基于xLSTM的ADS-B IDS)；采用迁移学习策略，在良性ADS-B消息上预训练，然后使用包含篡改消息的标记数据进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在识别微妙攻击方面优于现有方法；xLSTM-based IDS达到98.9%的F1分数，超过transformer-based模型的94.3%；对未见攻击的测试验证了xLSTM模型的泛化能力；xLSTM引入7.26秒延迟，在SSR刷新间隔内，但对时间关键操作有限制；transformer实现2.1秒延迟但检测性能较低。&lt;h4&gt;结论&lt;/h4&gt;基于xLSTM的IDS在检测性能方面表现优异，延迟时间在可接受范围内，但对于某些时间关键场景可能需要进一步优化。&lt;h4&gt;翻译&lt;/h4&gt;随着空中交通管理(ATM)对易受攻击的自动相关监视-广播(ADS-B)协议的依赖日益增长，确保安全至关重要。本研究调查了新兴的机器学习模型和训练策略，以改进用于ADS-B的基于AI的入侵检测系统(IDS)。专注于地面ATM系统，我们评估了两种深度学习IDS实现：一种使用transformer编码器，另一种使用扩展长短期记忆(xLSTM)网络，这是首个基于xLSTM的ADS-B IDS。采用迁移学习策略，包括在良性ADS-B消息上进行预训练，并使用包含篡改消息实例的标记数据进行微调。结果表明，这种方法优于现有方法，特别是在识别逐渐破坏态势感知的微妙攻击方面。基于xLSTM的IDS达到98.9%的F1分数，超过了基于transformer的模型的94.3%。对未见攻击的测试验证了xLSTM模型的泛化能力。推理延迟分析显示，xLSTM-based IDS引入的7.26秒延迟在辅助监视雷达(SSR)刷新间隔(5-12秒)内，但对于时间关键操作可能有限制。虽然基于transformer的IDS实现2.1秒延迟，但这是以较低的检测性能为代价的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing reliance on the vulnerable Automatic DependentSurveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),ensuring security is critical. This study investigates emerging machinelearning models and training strategies to improve AI-based intrusion detectionsystems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate twodeep learning IDS implementations: one using a transformer encoder and theother an extended Long Short-Term Memory (xLSTM) network, marking the firstxLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involvingpre-training on benign ADS-B messages and fine-tuning with labeled datacontaining instances of tampered messages. Results show this approachoutperforms existing methods, particularly in identifying subtle attacks thatprogressively undermine situational awareness. The xLSTM-based IDS achieves anF1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests onunseen attacks validated the generalization ability of the xLSTM model.Inference latency analysis shows that the 7.26-second delay introduced by thexLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refreshinterval (5-12 s), although it may be restrictive for time-critical operations.While the transformer-based IDS achieves a 2.1-second latency, it does so atthe cost of lower detection performance.</description>
      <author>example@mail.com (Mikaëla Ngamboé, Jean-Simon Marrocco, Jean-Yves Ouattara, José M. Fernandez, Gabriela Nicolescu)</author>
      <guid isPermaLink="false">2510.08333v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</title>
      <link>http://arxiv.org/abs/2510.07545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the EMNLP 2025 Industry Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出两种方法提高小型视觉-语言模型作为图表理解任务自动评估工具的性能，实现成本效益评估。&lt;h4&gt;背景&lt;/h4&gt;70亿参数的大型视觉-语言模型在图表理解任务中作为自动评估工具有良好表现，但小型模型(≤20亿参数)表现不佳，限制了在资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出两种方法确保成本效益评估，解决小型模型表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;1) 多标准提示：将单独的评估标准组合到单个查询中；2) 领域自适应迁移学习：在图表数据集上的合成判断上微调一个20亿参数的LVLM，创建ChartJudge模型。&lt;h4&gt;主要发现&lt;/h4&gt;多标准提示暴露了鲁棒性差距，导致70亿模型性能大幅下降，包括专业评估工具如LLaVA-Critic；小型LVLM (ChartJudge)能有效将知识从一个数据集转移到另一个数据集，使其更专业；对图表类型和查询复杂性的细粒度分析提供了模型大小、提示设计和可移植性之间权衡的可操作见解。&lt;h4&gt;结论&lt;/h4&gt;该工作实现了图表推理任务的可扩展、低成本评估。&lt;h4&gt;翻译&lt;/h4&gt;仅具有70亿参数的大型视觉-语言模型在图表理解任务中作为自动评估工具已显示出良好前景。然而，小型模型(≤20亿参数)作为评估工具时仍表现不佳，限制了它们在资源受限环境中的实际应用。为解决这一问题，我们提出了两种方法确保成本效益评估：(i)多标准提示，将单独的评估标准组合到单个查询中；(ii)领域自适应迁移学习，我们在图表数据集上的合成判断上微调一个20亿参数的LVLM，创建了ChartJudge。实验表明，多标准提示暴露了鲁棒性差距，导致70亿模型性能大幅下降，包括专业的LVLM评估工具如LLaVA-Critic。此外，我们发现我们的小型LVLM (ChartJudge)可以有效地将知识从一个数据集转移到另一个数据集，使其成为更专业的模型。我们对图表类型和查询复杂性的细粒度分析提供了模型大小、提示设计和可移植性之间权衡的可操作见解，实现了图表推理任务的可扩展、低成本评估。我们的代码和数据将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) with only 7B parameters have shownpromise as automated judges in chart comprehension tasks. However, tiny models(&lt;=2B parameters) still perform poorly as judges, limiting their real-world usein resource-constrained settings. To address this, we propose two approaches toensure cost-efficient evaluation: (i) multi-criteria prompting, which combinesseparate evaluation criteria into a single query, and (ii) domain-adaptivetransfer learning, in which we fine-tune a 2B-parameter LVLM on syntheticjudgments in a chart dataset to create the ChartJudge. Experiments show thatmulti-criteria prompting exposes robustness gaps, which led to a huge drop inperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.In addition, we find that our tiny LVLM (ChartJudge) can effectively transferknowledge from one dataset to another to make it a more specialized model. Ourfine-grained analysis across chart types and query complexities offersactionable insights into trade-offs between model size, prompt design, andtransferability, enabling scalable, low-cost evaluation for chart reasoningtasks. Our code and the data will be made publicly available.</description>
      <author>example@mail.com (Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang)</author>
      <guid isPermaLink="false">2510.07545v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>RayFusion: Ray Fusion Enhanced Collaborative Visual Perception</title>
      <link>http://arxiv.org/abs/2510.08017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为RayFusion的基于射线的融合方法，用于解决协作视觉感知中深度信息缺失的问题，提高自动驾驶系统中基于摄像头的感知性能。&lt;h4&gt;背景&lt;/h4&gt;协作视觉感知方法在自动驾驶领域受到广泛关注，能够解决传感器限制问题。然而，基于摄像头的感知系统（如3D物体检测）因缺乏明确的深度信息，难以生成准确预测。&lt;h4&gt;目的&lt;/h4&gt;减轻深度估计中的模糊性，提高纯摄像头协作感知系统的检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出RayFusion，一种基于射线的融合方法，利用协作者的射线占用信息，减少沿相机射线的冗余和假阳性预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过全面实验证明，该方法持续优于现有的最先进模型，显著提升了协作视觉感知的性能。&lt;h4&gt;结论&lt;/h4&gt;RayFusion有效解决了协作视觉感知中深度信息缺失的问题，为自动驾驶领域的感知系统提供了更准确的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，由于能够解决传感器限制问题，协作视觉感知方法在自动驾驶社区中获得了广泛关注。然而，缺乏明确的深度信息常常使得基于摄像头的感知系统（例如3D物体检测）难以生成准确的预测。为了减轻深度估计中的模糊性，我们提出了RayFusion，一种用于协作视觉感知的基于射线的融合方法。利用来自协作者的射线占用信息，RayFusion减少了沿相机射线的冗余和假阳性预测，增强了纯摄像头协作感知系统的检测性能。全面的实验表明，我们的方法持续优于现有的最先进模型，显著推进了协作视觉感知的性能。代码可在https://github.com/wangsh0111/RayFusion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative visual perception methods have gained widespread attention inthe autonomous driving community in recent years due to their ability toaddress sensor limitation problems. However, the absence of explicit depthinformation often makes it difficult for camera-based perception systems, e.g.,3D object detection, to generate accurate predictions. To alleviate theambiguity in depth estimation, we propose RayFusion, a ray-based fusion methodfor collaborative visual perception. Using ray occupancy information fromcollaborators, RayFusion reduces redundancy and false positive predictionsalong camera rays, enhancing the detection performance of purely camera-basedcollaborative perception systems. Comprehensive experiments show that ourmethod consistently outperforms existing state-of-the-art models, substantiallyadvancing the performance of collaborative visual perception. The code isavailable at https://github.com/wangsh0111/RayFusion.</description>
      <author>example@mail.com (Shaohong Wang, Bin Lu, Xinyu Xiao, Hanzhi Zhong, Bowen Pang, Tong Wang, Zhiyu Xiang, Hangguan Shan, Eryun Liu)</author>
      <guid isPermaLink="false">2510.08017v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.08531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://zju-real.github.io/SpatialLadder/ Code:  https://github.com/ZJU-REAL/SpatialLadder&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SpatialLadder的新方法，通过分阶段训练和新的数据集构建视觉语言模型的空间推理能力，取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;空间推理对视觉语言模型(VLMs)来说仍然是一个基本挑战，尽管最近有所进展，但现有方法难以实现稳健的性能。现有方法的局限性在于它们试图直接学习空间推理，而没有建立感知和理解的层次基础。&lt;h4&gt;目的&lt;/h4&gt;解决VLMs在空间推理方面的挑战，通过建立空间智能的渐进式方法论，提升模型在空间推理任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 引入了SpatialLadder-26k多模态数据集，包含26,610个样本，涵盖目标定位、单图像、多视图和视频空间推理任务；2. 设计了一个三阶段渐进训练框架：(1)通过目标定位建立空间感知，(2)通过多维空间任务发展空间理解，(3)通过具有可验证奖励的强化学习加强复杂推理；3. 基于此方法开发了SpatialLadder模型，一个拥有30亿参数的模型。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialLadder模型在空间推理基准测试上取得了最先进的性能，比基础模型平均提高了23.4%，比GPT-4o高出20.8%，比Gemini-2.0-Flash高出10.1%。在领域外基准测试上保持了强大的泛化能力，提高了7.2%。&lt;h4&gt;结论&lt;/h4&gt;从感知到推理的渐进式训练对于构建稳健的空间智能至关重要。&lt;h4&gt;翻译&lt;/h4&gt;空间推理对视觉语言模型(VLMs)来说仍然是一个基本挑战，尽管最近有所进展，但现有方法难以实现稳健的性能。我们确定这一局限性源于一个关键差距：现有方法试图直接学习空间推理，而没有建立感知和理解的层次基础。为了应对这一挑战，我们提出了一个构建空间智能的全面渐进式方法。我们引入了SpatialLadder-26k，这是一个包含26,610个样本的多模态数据集，涵盖目标定位、单图像、多视图和视频空间推理任务，通过标准化的管道构建，确保跨模态的系统覆盖。基于此数据集，我们设计了一个三阶段渐进训练框架，(1)通过目标定位建立空间感知，(2)通过多维空间任务发展空间理解，(3)通过具有可验证奖励的强化学习加强复杂推理。这种方法产生了SpatialLadder，一个拥有30亿参数的模型，在空间推理基准测试上取得了最先进的性能，比基础模型平均提高23.4%，超越GPT-4o 20.8%，超越Gemini-2.0-Flash 10.1%。值得注意的是，SpatialLadder在领域外基准测试上保持了强大的泛化能力，提高了7.2%，这表明从感知到推理的渐进式训练对于稳健的空间智能是必不可少的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型(VLMs)中的空间推理能力不足问题。当前VLMs在基础视觉任务上表现良好，但在空间推理方面存在显著瓶颈，这限制了它们在机器人导航、自动驾驶和虚拟现实等需要空间智能的应用中的部署。空间推理是人类理解视觉场景的基本能力，但对VLMs来说仍然是一个重大挑战，这个问题在现实世界中非常重要，因为它直接影响AI系统对物理环境的理解和交互能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过控制实验验证了假设：空间推理失败的原因是感知基础不足，而非推理能力本身。他们发现当提供渐进的感知提示(位置提示和方向提示)时，模型性能显著提升。基于这一发现，作者设计了层次化空间智能构建方法，借鉴了现有工作中的3D场景重建技术(ScanNet)、问题模板设计(VSI-Bench)和强化学习方法(GRPO)，但创新性地将这些技术整合到一个三阶段渐进式训练框架中，从感知基础开始，逐步发展到复杂推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是空间智能需要通过渐进式训练系统构建，从基础感知能力开始，逐步发展到复杂推理能力，而非一次性学习完整的空间推理能力。整体流程包括：1)构建SpatialLadder-26k多模态数据集，包含26,610个样本覆盖物体定位、单图像、多视图和视频空间推理任务；2)设计三阶段训练框架：阶段1通过物体定位建立空间感知，阶段2通过多维任务发展空间理解，阶段3通过强化学习强化复杂推理；3)使用可验证奖励函数和思维链技术确保推理质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SpatialLadder-26k数据集，首个系统覆盖从基础感知到复杂推理的全谱空间推理多模态数据集；2)三阶段渐进式训练框架，首次提出系统构建空间推理能力的层次化方法；3)显著提升的性能，在多个基准测试上超越现有方法。相比之前工作，不同之处在于：现有数据集碎片化且范围狭窄，而SpatialLadder-26k系统性地覆盖多种模态；现有方法直接优化推理输出，而SpatialLadder建立从感知到推理的层次结构；现有方法将空间推理视为单一能力，而SpatialLadder认识到需要渐进式构建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出一个基于渐进式训练的三阶段框架和构建SpatialLadder-26k多模态数据集，显著提升了视觉语言模型的空间推理能力，实现了从感知到理解再到推理的系统化空间智能构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial reasoning remains a fundamental challenge for Vision-Language Models(VLMs), with current approaches struggling to achieve robust performancedespite recent advances. We identify that this limitation stems from a criticalgap: existing methods attempt to learn spatial reasoning directly withoutestablishing the hierarchical foundations of perception and understanding. Toaddress this challenge, we present a comprehensive methodology for buildingspatial intelligence progressively. We introduce SpatialLadder-26k, amultimodal dataset containing 26,610 samples spanning object localization,single image, multi-view, and video spatial reasoning tasks, constructedthrough a standardized pipeline that ensures systematic coverage acrossmodalities. Building on this dataset, we design a three-stage progressivetraining framework that (1) establishes spatial perception through objectlocalization, (2) develops spatial understanding through multi-dimensionalspatial tasks, and (3) strengthens complex reasoning via reinforcement learningwith verifiable rewards. This approach yields SpatialLadder, a 3B-parametermodel that achieves state-of-the-art performance on spatial reasoningbenchmarks, with 23.4% average improvement over the base model, surpassingGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintainsstrong generalization with 7.2% improvement on out-of-domain benchmarks,demonstrating that progressive training from perception to reasoning isessential for robust spatial intelligence.</description>
      <author>example@mail.com (Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang)</author>
      <guid isPermaLink="false">2510.08531v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>The impact of abstract and object tags on image privacy classification</title>
      <link>http://arxiv.org/abs/2510.07976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了对象标签和抽象标签在图像隐私分类任务中的适用性，发现当标签预算有限时抽象标签更有效，而在标签数量充足时对象标签同样有用。&lt;h4&gt;背景&lt;/h4&gt;对象标签表示具体实体，是许多计算机视觉任务的核心；抽象标签捕获更高级别的信息，对需要上下文理解的任务相关；两者都有助于图像解释。&lt;h4&gt;目的&lt;/h4&gt;探索哪种类型的标签更适合图像隐私这一上下文相关且本质上主观的任务。&lt;h4&gt;方法&lt;/h4&gt;比较对象标签和抽象标签在图像隐私分类中的效果，考虑不同标签预算情况。&lt;h4&gt;主要发现&lt;/h4&gt;对象标签通常用于隐私分类；当标签预算有限时，抽象标签更有效；当每个图像有更多标签可用时，对象信息同样有用。&lt;h4&gt;结论&lt;/h4&gt;这些发现将指导未来研究开发更准确的图像隐私分类器，考虑标签类型和数量的作用。&lt;h4&gt;翻译&lt;/h4&gt;对象标签表示具体实体，是许多计算机视觉任务的核心，而抽象标签捕获更高级别的信息，与需要上下文理解的任务相关。从图像中提取的对象和抽象标签也有助于可解释性。在本文中，我们探索哪种类型的标签更适合图像隐私这一上下文相关且本质上主观的任务。虽然对象标签通常用于隐私分类，但我们表明当标签预算有限时，抽象标签更有效。相反，当每个图像有更多标签可用时，对象信息同样有用。我们相信这些发现将指导未来研究开发更准确的图像隐私分类器，考虑标签类型和数量的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object tags denote concrete entities and are central to many computer visiontasks, whereas abstract tags capture higher-level information, which isrelevant for tasks that require a contextual, potentially subjective sceneunderstanding. Object and abstract tags extracted from images also facilitateinterpretability. In this paper, we explore which type of tags is more suitablefor the context-dependent and inherently subjective task of image privacy.While object tags are generally used for privacy classification, we show thatabstract tags are more effective when the tag budget is limited. Conversely,when a larger number of tags per image is available, object-related informationis as useful. We believe that these findings will guide future research indeveloping more accurate image privacy classifiers, informed by the role of tagtypes and quantity.</description>
      <author>example@mail.com (Darya Baranouskaya, Andrea Cavallaro)</author>
      <guid isPermaLink="false">2510.07976v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.07944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CVD-STORM，一种利用空间-时间重建变分自编码器的跨视角视频扩散模型，能够在各种控制输入下生成具有4D重建能力的长期多视角视频，通过微调VAE和集成到视频扩散过程，显著提高了生成质量。&lt;h4&gt;背景&lt;/h4&gt;生成模型已被广泛应用于环境模拟和未来状态预测的世界建模。随着自动驾驶的发展，对高保真视频生成以及深度估计等多样化有意义信息的需求不断增长。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为CVD-STORM的跨视角视频扩散模型，生成具有4D重建能力的长期多视角视频，并接受各种控制输入，以满足自动驾驶领域对高质量视频和多样化信息的需求。&lt;h4&gt;方法&lt;/h4&gt;使用空间-时间重建变分自编码器，首先通过辅助4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力，然后将此VAE集成到视频扩散过程中，并采用联合训练的高斯溅射解码器来重建动态场景。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在FID和FVD指标上均取得了显著改进，联合训练的高斯溅射解码器能有效重建动态场景，为全面场景理解提供有价值的几何信息。&lt;h4&gt;结论&lt;/h4&gt;CVD-STORM模型能够满足自动驾驶领域对高保真视频生成和多样化信息的需求，通过4D重建能力和高质量视频生成，为场景理解和预测提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;生成模型已被广泛应用于环境模拟和未来状态预测的世界建模。随着自动驾驶的发展，不仅对高保真视频生成有需求，而且对深度估计等多样化有意义信息的需求也在增长。为此，我们提出了CVD-STORM，这是一种利用空间-时间重建变分自编码器的跨视角视频扩散模型，能够在各种控制输入下生成具有4D重建能力的长期多视角视频。我们的方法首先通过辅助4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力。随后将此VAE集成到视频扩散过程中，显著提高生成质量。实验结果表明，我们的模型在FID和FVD指标上均取得了显著改进。此外，联合训练的高斯溅射解码器能有效重建动态场景，为全面场景理解提供有价值的几何信息。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域中如何同时生成高质量长序列多视角视频并实现动态4D场景重建的问题。这个问题很重要，因为自动驾驶系统需要可靠的环境模拟和未来状态预测，而现有方法往往缺乏明确的3D信息，无法准确表示真实世界的几何结构，限制了它们作为世界模型的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视频生成方法在自动驾驶领域的局限性，然后提出将生成模型与重建任务相结合的思路。他们借鉴了STORM的4D重建方法，将其整合到VAE框架中创建了STORM-VAE；基于UniMLVG的多视角视频生成架构进行改进；使用Stable Diffusion 3.5作为基础模型，并应用DiT架构。整体采用两阶段训练策略：先学习场景重建，再训练条件世界模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合视频生成和4D场景重建任务，创建统一框架，使模型同时学习生成高质量视频和理解3D场景结构。整体流程分为三部分：1) STORM-VAE训练阶段：使用预训练VAE并添加高斯散射解码器，通过多视角图像和LiDAR数据训练；2) CVD-STORM训练阶段：使用STORM-VAE作为潜在编码器，构建基于DiT的视频扩散模型，采用三种transformer块处理不同维度数据；3) 推理阶段：生成六视角视频并重建动态3D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) STORM-VAE：扩展传统VAE架构，集成高斯散射解码器，能编码空间时间信息；2) CVD-STORM框架：统一多视角视频生成和4D场景重建，采用简化训练策略；3) 表示学习增强：通过辅助重建任务提升生成质量。相比之前工作，它实现了端到端联合训练，提供绝对深度估计而非相对深度，同时优化了生成质量和重建精度，并支持更丰富的条件输入。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVD-STORM通过结合跨视角视频生成和空间-时间重建模型，实现了自动驾驶场景中高质量多视角视频生成与精确4D场景重建的统一框架，显著提升了世界模型的表示能力和几何理解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models have been widely applied to world modeling for environmentsimulation and future state prediction. With advancements in autonomousdriving, there is a growing demand not only for high-fidelity video generationunder various controls, but also for producing diverse and meaningfulinformation such as depth estimation. To address this, we propose CVD-STORM, across-view video diffusion model utilizing a spatial-temporal reconstructionVariational Autoencoder (VAE) that generates long-term, multi-view videos with4D reconstruction capabilities under various control inputs. Our approach firstfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing itsability to encode 3D structures and temporal dynamics. Subsequently, weintegrate this VAE into the video diffusion process to significantly improvegeneration quality. Experimental results demonstrate that our model achievessubstantial improvements in both FID and FVD metrics. Additionally, thejointly-trained Gaussian Splatting Decoder effectively reconstructs dynamicscenes, providing valuable geometric information for comprehensive sceneunderstanding.</description>
      <author>example@mail.com (Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu)</author>
      <guid isPermaLink="false">2510.07944v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了USIM（基于模拟的多任务视觉-语言-行动数据集）和U0（适用于水下机器人的VLA模型），解决了水下机器人操作面临的挑战，在多种任务中表现出色，成功率达到80%，在移动操作任务中将目标距离减少了21.2%&lt;h4&gt;背景&lt;/h4&gt;水下环境为机器人操作带来独特挑战，包括复杂水动力学、有限能见度和受限通信。尽管数据驱动方法已推动陆地机器人具身智能发展，并实现特定任务水下机器人自主操作，但开发能自主执行多任务的水下智能仍面临挑战，因大规模高质量水下数据集稀缺&lt;h4&gt;目的&lt;/h4&gt;解决水下机器人面临的挑战，开发能自主执行多任务的水下智能，构建大规模高质量水下数据集，并基于此数据集开发适用于水下机器人的VLA模型&lt;h4&gt;方法&lt;/h4&gt;引入USIM数据集，包含来自1,852个轨迹的561K帧，总计15.6小时BlueROV2交互数据，涵盖9种场景下的20个任务；提出U0模型，通过多模态融合整合双目视觉和其他传感器模态，采用基于卷积-注意力的感知增强模块(CAP)提高空间理解和移动操作能力&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，框架实现80%成功率；在具有挑战性的移动操作任务中，与基线方法相比，目标距离减少21.2%&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可有效应用于水下机器人应用，为可扩展数据集构建、改进任务自主性和通用水下机器人的实际实现奠定基础&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来独特挑战，包括复杂水动力学、有限能见度和受限通信。尽管数据驱动方法已推动陆地机器人具身智能发展，并实现特定任务水下机器人自主操作，但开发能自主执行多任务的水下智能仍面临挑战，因大规模高质量水下数据集稀缺。为解决这些限制，我们引入USIM，一个基于模拟的多任务视觉-语言-行动数据集，专为水下机器人设计。USIM包含来自1,852个轨迹的561K帧，总计15.6小时BlueROV2交互数据，涵盖9种不同场景下的20个任务，从视觉导航到移动操作。基于此数据集，我们提出U0，一个适用于通用水下机器人的VLA模型，通过多模态融合整合双目视觉和其他传感器模态，并采用基于卷积-注意力的感知增强模块(CAP)提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，框架实现80%成功率；在具有挑战性的移动操作任务中，与基线方法相比，目标距离减少21.2%，证明其有效性。USIM和U0表明VLA模型可有效应用于水下机器人应用，为可扩展数据集构建、改进任务自主性和通用水下机器人的实际实现奠定基础&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决水下机器人缺乏大规模高质量数据集和通用智能模型的问题。这个问题很重要，因为海洋覆盖地球71%的面积，水下环境对人类操作极具挑战性，而现有水下机器人大多只能执行特定任务且依赖人工远程操作，缺乏自主执行多种任务的能力。这限制了人类对海洋的探索和开发，也使水下任务（如海洋生态调查、资源开发、管道检查等）效率低下且成本高昂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到数据驱动方法在陆地机器人领域的成功，注意到水下机器人面临的独特挑战（如流体动力学、能见度限制、通信受限）。他们选择在仿真环境中构建多样化的水下场景来高效安全地收集大规模数据，借鉴了Stonefish仿真器和室内机器人VLA数据集的构建方法，但针对水下环境进行了调整。模型设计上基于Isaac-GR00T N1.5架构，但增加了针对水下环境的多模态传感器融合和卷积-注意力感知焦点增强模块(CAP)，以提高水下目标检测和定位能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建大规模多样化的水下仿真数据集，开发针对水下环境的视觉-语言-动作(VLA)模型，整合多模态感知和语言理解能力，并通过卷积-注意力机制增强模型在水下环境中的感知能力。整体流程包括：1)使用Stonefish仿真器构建9种不同水下场景和BlueROV2机器人模型；2)在仿真环境中收集20个任务的机器人-环境交互数据，形成USIM数据集；3)基于Isaac-GR00T N1.5构建U0模型，整合多模态传感器数据和CAP模块；4)使用USIM数据集对模型进行微调；5)通过开环离线评估和闭环在线测试验证模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个大规模水下多任务VLA数据集USIM，包含561K帧和20个任务；2)首个针对水下机器人的通用VLA模型U0，集成多模态融合和CAP模块；3)以机器人为中心的坐标表示方法，增强动态场景理解；4)可扩展的数据到任务框架，实现80%的任务成功率和21.2%的移动操作性能提升。相比之前工作，USIM提供了统一的跨任务数据框架，而不仅是特定任务数据；U0是首个结合语言理解、视觉感知和动作执行的通用水下模型，专门针对水下环境特点进行了优化，而非直接应用陆地机器人模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务视觉-语言-动作数据集USIM和开发首个通用水下机器人VLA模型U0，为水下机器人的自主能力提升和智能化发展提供了新的基础和方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</title>
      <link>http://arxiv.org/abs/2510.07181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TIGeR框架将视觉语言模型从感知估计器转变为几何计算机，通过外部工具实现厘米级精度的几何计算，在机器人任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在空间推理方面表现出色，但仅限于定性精度，缺乏机器人技术所需的计算精度。当前方法未能利用深度传感器和相机校准的度量线索，无法提供机器人操作所需的厘米级精度。&lt;h4&gt;目的&lt;/h4&gt;提出TIGeR框架，使视觉语言模型能够通过外部工具生成和执行精确的几何计算，满足机器人应用的精度需求。&lt;h4&gt;方法&lt;/h4&gt;TIGeR使模型识别几何推理需求，合成计算代码，并调用专门库进行精确计算。引入TIGeR-300K数据集，涵盖点变换、姿态估计和空间兼容性验证。通过监督微调和具有分层奖励设计的强化微调两阶段训练管道进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;TIGeR在几何推理基准测试上实现了最先进性能，并在真实世界的机器人操作任务中展示了厘米级精度。&lt;h4&gt;结论&lt;/h4&gt;TIGeR成功地将视觉语言模型转变为能够执行精确几何计算的模型，为机器人应用提供了必要的精度。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在空间推理方面表现出色，但它们本质上仅限于定性精度，缺乏机器人技术所需的计算精度。当前方法未能利用深度传感器和相机校准的度量线索，而是将几何问题简化为无法提供机器人操作所需的厘米级精度的模式识别任务。我们提出TIGeR（工具集成几何推理）框架，将VLMs从感知估计器转变为几何计算机，使它们能够通过外部工具生成和执行精确的几何计算。TIGeR不是尝试在神经网络内部化复杂的几何操作，而是使模型能够识别几何推理需求，合成适当的计算代码，并调用专门库进行精确计算。为此，我们引入了TIGeR-300K数据集，涵盖点变换、姿态估计和空间兼容性验证，包含工具调用序列和中间计算。通过结合监督微调（SFT）和具有分层奖励设计的强化微调（RFT）的两阶段训练管道，TIGeR在几何推理基准测试上实现了最先进性能，并在真实世界的机器人操作任务中展示了厘米级精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决视觉语言模型(VLMs)在几何推理上的局限性，即它们只能提供定性的空间关系评估(如'在左边')，而无法进行精确的定量计算。这个问题很重要，因为机器人在物理世界中操作需要厘米级精度的几何推理能力，如计算3D姿态、旋转矩阵和无碰撞轨迹，缺乏这种能力会阻碍VLMs在现实世界机器人应用中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLMs在几何推理上的感知和输出局限性，然后提出让模型识别几何推理需求、生成计算代码并调用外部工具执行精确计算，而非将几何运算内化到神经网络中。该方法借鉴了工具集成推理(TIR)的三个类别(基于提示、SFT和RL的方法)以及空间理解和推理的数据驱动与工具方法，但创新性地提出了两阶段SFT-RFT训练流程和新的过程奖励函数，并整合了现有的视觉基础模型作为工具。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将VLMs从感知估计器转变为几何计算机，通过生成和执行精确的几何计算实现厘米级精度。整体流程包括：1)工具分类(视觉感知工具和几何计算工具)；2)数据生成(TIGeR-300K数据集，结合模板合成和大模型重写)；3)两阶段训练(SFT初始化工具使用能力，RFT使用GRPO算法增强能力)；4)层次化奖励设计(格式、工具调用、参数内容、代码生成和答案奖励)；5)推理过程(逐步推理、选择上下文、调用工具、生成代码、推导结果)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)概念与方法创新，强调几何推理的核心作用并引入工具集成框架；2)数据集创新，构建TIGeR-300K数据集提供完整工具调用序列；3)训练方法创新，开发两阶段SFT-RFT管道和层次化奖励设计；4)技术实现创新，分类工具并引入代码生成子程序。相比之前工作，TIGeR不将几何计算内化到神经网络中，而是利用外部工具实现精确计算；结合了结果导向和过程导向的奖励函数，提供细粒度监督；超越了纯数据驱动方法的定性限制和现有工具集成方法的准确性不足。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TIGeR通过工具集成几何推理框架，使视觉语言模型能够生成和执行精确代码，实现了厘米级精度的机器人几何推理，超越了传统方法的定性空间理解限制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown remarkable capabilities in spatialreasoning, yet they remain fundamentally limited to qualitative precision andlack the computational precision required for real-world robotics. Currentapproaches fail to leverage metric cues from depth sensors and cameracalibration, instead reducing geometric problems to pattern recognition tasksthat cannot deliver the centimeter-level accuracy essential for roboticmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novelframework that transforms VLMs from perceptual estimators to geometriccomputers by enabling them to generate and execute precise geometriccomputations through external tools. Rather than attempting to internalizecomplex geometric operations within neural networks, TIGeR empowers models torecognize geometric reasoning requirements, synthesize appropriatecomputational code, and invoke specialized libraries for exact calculations. Tosupport this paradigm, we introduce TIGeR-300K, a comprehensivetool-invocation-oriented dataset covering point transformations, poseestimation, and spatial compatibility verification, complete with toolinvocation sequences and intermediate computations. Through a two-stagetraining pipeline combining supervised fine-tuning (SFT) and reinforcementfine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achievesSOTA performance on geometric reasoning benchmarks while demonstratingcentimeter-level precision in real-world robotic manipulation tasks.</description>
      <author>example@mail.com (Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang)</author>
      <guid isPermaLink="false">2510.07181v2</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model</title>
      <link>http://arxiv.org/abs/2510.07345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SurgiFlowVid的稀疏且可控的视频扩散框架，用于生成代表性不足类别的手术视频，以解决手术视频数据集不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;手术视频数据集对于场景理解、程序建模和术中支持至关重要，但这些数据集通常严重不平衡，稀有动作和工具代表性不足，限制了下游模型的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;解决手术视频数据集中的数据不平衡问题，提高模型对稀有类别和工具的识别能力。&lt;h4&gt;方法&lt;/h4&gt;SurgiFlowVid框架包含双预测扩散模块，联合去噪RGB帧和光流，提供时间归纳偏置；以及稀疏视觉编码器，使用轻量级信号调节生成过程，实现可控性而不需要密集标注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个手术数据集上的实验表明，合成数据比竞争基线方法提高10-20%，验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;SurgiFlowVid是一种有前景的策略，可以缓解数据不平衡问题并推进手术视频理解方法的发展。&lt;h4&gt;翻译&lt;/h4&gt;手术视频数据集对于场景理解、程序建模和术中支持至关重要。然而，这些数据集通常严重不平衡，稀有动作和工具代表性不足，这限制了下游模型的鲁棒性。我们通过SurgiFlowVid解决了这一挑战，这是一种用于生成代表性不足类别手术视频的稀疏且可控的视频扩散框架。我们的方法引入了双预测扩散模块，联合去噪RGB帧和光流，提供时间归纳偏置，从有限样本改进运动建模。此外，稀疏视觉编码器使用轻量级信号（如稀疏分割掩码或RGB帧）调节生成过程，实现可控性而不需要密集标注。我们在三个手术数据集上验证了我们的方法，任务包括动作识别、工具存在检测和腹腔镜运动预测。我们方法生成的合成数据比竞争基线方法提高10-20%，确立了SurgiFlowVid作为一种有前景的策略，可以缓解数据不平衡并推进手术视频理解方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical video datasets are essential for scene understanding, enablingprocedural modeling and intra-operative support. However, these datasets areoften heavily imbalanced, with rare actions and tools under-represented, whichlimits the robustness of downstream models. We address this challenge with$SurgiFlowVid$, a sparse and controllable video diffusion framework forgenerating surgical videos of under-represented classes. Our approachintroduces a dual-prediction diffusion module that jointly denoises RGB framesand optical flow, providing temporal inductive biases to improve motionmodeling from limited samples. In addition, a sparse visual encoder conditionsthe generation process on lightweight signals (e.g., sparse segmentation masksor RGB frames), enabling controllability without dense annotations. We validateour approach on three surgical datasets across tasks including actionrecognition, tool presence detection, and laparoscope motion prediction.Synthetic data generated by our method yields consistent gains of 10-20% overcompetitive baselines, establishing $SurgiFlowVid$ as a promising strategy tomitigate data imbalance and advance surgical video understanding methods.</description>
      <author>example@mail.com (Danush Kumar Venkatesh, Adam Schmidt, Muhammad Abdullah Jamal, Omid Mohareri)</author>
      <guid isPermaLink="false">2510.07345v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games</title>
      <link>http://arxiv.org/abs/2510.07813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种在对抗环境中智能体如何平衡信息获取与风险暴露的决策框架&lt;h4&gt;背景&lt;/h4&gt;对抗环境中的智能体需要在获取信息以增强态势感知和暴露于威胁之间做出战略权衡&lt;h4&gt;目的&lt;/h4&gt;研究这种信息获取与风险暴露之间的紧张关系，并开发一个有效的决策框架&lt;h4&gt;方法&lt;/h4&gt;提出PEEC（追逃-暴露-隐藏博弈）框架和SHADOW（战争部分观察下的战略通信混合动作决策）多头序贯强化学习框架，整合连续导航控制、离散通信动作和对手建模&lt;h4&gt;主要发现&lt;/h4&gt;SHADOW追击者比六个竞争基线方法实现更高成功率；时间序列建模和对手建模对有效决策至关重要；学习到的策略在不同通信风险和智能体物理不对称性上具有良好的泛化能力&lt;h4&gt;结论&lt;/h4&gt;SHADOW框架能有效平衡对抗环境中的观察性和风险&lt;h4&gt;翻译&lt;/h4&gt;对抗环境需要智能体应对一个关键战略权衡：获取信息可以增强态势感知，但同时也可能使它们暴露于威胁之中。为了研究这种紧张关系，我们提出了一个追逃-暴露-隐藏博弈（PEEC），其中追击智能体必须决定何时通信以获取逃逸者的位置。每次通信都会暴露追击者的位置，增加被瞄准的风险。两个智能体都通过强化学习学习其移动策略，而追击者 additionally 学习一种通信策略以平衡观察性和风险。我们提出了SHADOW（战争部分观察下的战略通信混合动作决策），这是一个多头序贯强化学习框架，集成了连续导航控制、离散通信动作和对手建模以进行行为预测。经验评估显示，SHADOW追击者比六个竞争基线方法实现了更高的成功率。我们的消融研究证实时间序列建模和对手建模对有效决策至关重要。最后，我们的敏感性分析揭示学习到的策略在不同通信风险和智能体之间的物理不对称性上具有良好的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial environments require agents to navigate a key strategictrade-off: acquiring information enhances situational awareness, but maysimultaneously expose them to threats. To investigate this tension, weformulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursueragent must decide when to communicate in order to obtain the evader's position.Each communication reveals the pursuer's location, increasing the risk of beingtargeted. Both agents learn their movement policies via reinforcement learning,while the pursuer additionally learns a communication policy that balancesobservability and risk. We propose SHADOW (Strategic-communication HybridAction Decision-making under partial Observation for Warfare), a multi-headedsequential reinforcement learning framework that integrates continuousnavigation control, discrete communication actions, and opponent modeling forbehavior prediction. Empirical evaluations show that SHADOW pursuers achievehigher success rates than six competitive baselines. Our ablation studyconfirms that temporal sequence modeling and opponent modeling are critical foreffective decision-making. Finally, our sensitivity analysis reveals that thelearned policies generalize well across varying communication risks andphysical asymmetries between agents.</description>
      <author>example@mail.com (Valerio La Gatta, Dolev Mutzari, Sarit Kraus, VS Subrahmanian)</author>
      <guid isPermaLink="false">2510.07813v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Reconstructing the local density field with combined convolutional and point cloud architecture</title>
      <link>http://arxiv.org/abs/2510.08573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 1 table. Accepted at the NeurIPS 2025 Workshop:  ML4PS. Comments welcome!&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究构建了一种结合卷积U-Net和点云DeepSets的混合神经网络，用于根据暗物质晕的特殊速度重建局部暗物质密度场。这种混合方法比单独使用U-Net能更好地利用小尺度信息，提高了重建质量，特别是在小尺度上能更准确地恢复聚类幅度和相位。&lt;h4&gt;背景&lt;/h4&gt;暗物质密度场的重建是宇宙学研究中的重要问题。暗物质晕的特殊速度作为暗物质场的有偏差示踪物，提供了关于局部暗物质密度分布的信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种神经网络方法，能够根据暗物质晕的特殊速度有效重建局部暗物质密度场，特别是在小尺度上提高重建精度。&lt;h4&gt;方法&lt;/h4&gt;构建了一种混合神经网络架构，结合了卷积U-Net和点云DeepSets。U-Net擅长处理网格化数据，而DeepSets则能有效处理点云数据，这种组合能够充分利用小尺度信息。&lt;h4&gt;主要发现&lt;/h4&gt;混合神经网络比单独使用U-Net的方法能更好地重建暗物质密度场，特别是在小尺度上能更准确地恢复聚类幅度和相位。&lt;h4&gt;结论&lt;/h4&gt;结合卷积U-Net和点云DeepSets的混合神经网络架构是重建局部暗物质密度场的有效方法，特别是在处理小尺度信息方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;我们构建了一个神经网络，用于根据暗物质晕的视线特殊速度（暗物质场的有偏差示踪物）对局部暗物质密度场进行回归分析。我们的架构结合了卷积U-Net和点云DeepSets。这种组合能够有效利用小尺度信息，并相对于仅使用U-Net的方法提高了重建质量。具体来说，我们的混合网络在小尺度上比U-Net更好地恢复了聚类幅度和相位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We construct a neural network to perform regression on the local dark-matterdensity field given line-of-sight peculiar velocities of dark-matter halos,biased tracers of the dark matter field. Our architecture combines aconvolutional U-Net with a point-cloud DeepSets. This combination enablesefficient use of small-scale information and improves reconstruction qualityrelative to a U-Net-only approach. Specifically, our hybrid network recoversboth clustering amplitudes and phases better than the U-Net on small scales.</description>
      <author>example@mail.com (Baptiste Barthe-Gold, Nhat-Minh Nguyen, Leander Thiele)</author>
      <guid isPermaLink="false">2510.08573v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2510.08512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Robotics and Automation Letters  (RA-L). 8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于语义场景图的深度压缩框架，用于高效传输3D点云数据，在保持结构和语义保真度的同时，实现了高达98%的数据压缩率。&lt;h4&gt;背景&lt;/h4&gt;3D点云数据传输对于集中式和分散式多机器人系统的高级感知至关重要，特别是在如今越来越依赖边缘和云处理的背景下。然而，点云数据量大且复杂，在带宽受限和间歇性连接条件下造成挑战，常常降低系统性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效压缩3D点云数据的方法，解决在带宽受限和间歇性连接条件下的传输挑战，同时保持数据的质量和可用性。&lt;h4&gt;方法&lt;/h4&gt;将点云分解为语义连贯的块，使用基于特征线性调制(FiLM)的语义感知编码器将它们编码为紧凑的潜在表示，并通过由潜在特征和图节点属性引导的基于折叠的解码器实现结构准确的重建。&lt;h4&gt;主要发现&lt;/h4&gt;在SemanticKITTI和nuScenes数据集上的实验表明，该框架实现了最先进的压缩率，可将数据大小减少高达98%，同时保持结构和语义保真度；该框架还支持多机器人位姿图优化和地图合并等下游应用，实现的轨迹精度和地图对齐可与原始LiDAR扫描获得的结果相媲美。&lt;h4&gt;结论&lt;/h4&gt;基于语义场景图的深度压缩框架有效解决了3D点云数据在带宽受限环境下的传输问题，同时保持了数据的质量和可用性，为多机器人系统的高级感知提供了重要支持。&lt;h4&gt;翻译&lt;/h4&gt;3D点云数据的高效传输对于集中式和分散式多机器人系统中的高级感知至关重要，特别是在如今越来越依赖边缘和云处理的背景下。然而，点云数据量大且复杂的特性在带宽受限和间歇性连接条件下带来了挑战，常常降低系统性能。我们提出了一种基于语义场景图的深度压缩框架。该方法将点云分解为语义连贯的块，并使用由特征线性调制(FiLM)调节的语义感知编码器将它们编码为紧凑的潜在表示。基于折叠的解码器，由潜在特征和图节点属性引导，实现了结构准确的重建。在SemanticKITTI和nuScenes数据集上的实验表明，该框架实现了最先进的压缩率，在保持结构和语义保真度的同时，将数据大小减少了高达98%。此外，它支持多机器人位姿图优化和地图合并等下游应用，实现了与原始LiDAR扫描获得的轨迹精度和地图对齐相媲美的效果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云数据的高效传输问题。在多机器人系统中，点云数据对高级感知至关重要，但其庞大和复杂的特性在带宽受限和间歇性连接条件下会造成挑战，导致系统性能下降。随着机器人系统对边缘和云处理的依赖增长，解决这一问题对于实现高效的多机器人协作和实时感知具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，注意到语义场景图已被证明是导航和规划的有效表示，但现有压缩方法很少利用其结构信息来指导原始几何的有损编码。作者借鉴了FiLM（特征线性调制）技术进行语义条件化，以及基于折叠的解码器结构，同时结合了场景图表示方法和点云压缩技术，创造了一个新的框架，将语义场景图作为压缩的核心组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用语义场景图指导点云压缩，将点云分解为语义一致的块，并用语义感知编码器将这些块编码为紧凑的潜在表示，最后通过基于折叠的解码器在潜在特征和图节点属性指导下进行结构准确的重建。整体流程包括：1) 将原始点云转换为语义场景图；2) 将场景划分为特定层次的语义块；3) 使用基于transformer的自动编码器将每个块编码为紧凑的潜在向量；4) 解码这些潜在向量重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出基于图的点云深度压缩自动编码器，将点云分解为语义一致块并通过FiLM条件化的transformer编码；2) 将关系结构整合到压缩中，利用语义场景图指导整个压缩过程；3) 实现高达98%的数据减少，同时保持几何和语义保真度。相比之前工作，本文方法同时考虑了几何和语义保真度，将场景图作为压缩核心组件，并支持下游机器人任务如多机器人位姿图优化和地图合并。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于语义场景图的深度点云压缩方法，通过将点云分解为语义一致的块并使用语义感知编码器进行压缩，实现了高达98%的数据减少，同时保持了几何和语义保真度，并有效支持下游机器人任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient transmission of 3D point cloud data is critical for advancedperception in centralized and decentralized multi-agent robotic systems,especially nowadays with the growing reliance on edge and cloud-basedprocessing. However, the large and complex nature of point clouds createschallenges under bandwidth constraints and intermittent connectivity, oftendegrading system performance. We propose a deep compression framework based onsemantic scene graphs. The method decomposes point clouds into semanticallycoherent patches and encodes them into compact latent representations withsemantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). Afolding-based decoder, guided by latent features and graph node attributes,enables structurally accurate reconstruction. Experiments on the SemanticKITTIand nuScenes datasets show that the framework achieves state-of-the-artcompression rates, reducing data size by up to 98% while preserving bothstructural and semantic fidelity. In addition, it supports downstreamapplications such as multi-robot pose graph optimization and map merging,achieving trajectory accuracy and map alignment comparable to those obtainedwith raw LiDAR scans.</description>
      <author>example@mail.com (Nikolaos Stathoulopoulos, Christoforos Kanellakis, George Nikolakopoulos)</author>
      <guid isPermaLink="false">2510.08512v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge</title>
      <link>http://arxiv.org/abs/2510.08316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in process&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种语义基础学习范式，通过跨模态亲和力传输(CMAT)预训练策略和跨模态功能分割Transformer(CAST)，解决了3D功能分割中的语义边界不清晰问题，并在标准基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;功能分割旨在将3D对象解析为功能不同的部分，用于机器人操作、具身AI和AR应用。现有方法通常利用视觉或文本提示来指导这一过程，但它们往往将点云编码器作为通用特征提取器，忽视了3D数据固有的挑战，如稀疏性、噪声和几何模糊性。&lt;h4&gt;目的&lt;/h4&gt;解决3D功能分割中语义边界不清晰的问题，通过将大规模2D视觉基础模型(VFMs)的丰富语义知识转移到3D领域，提高3D功能分割的准确性和语义一致性。&lt;h4&gt;方法&lt;/h4&gt;提出了语义基础学习范式，具体包括：引入跨模态亲和力传输(CMAT)预训练策略，将3D编码器与提升的2D语义对齐，并联合优化重建、亲和力和多样性以产生语义组织的表示；设计跨模态功能分割Transformer(CAST)，将多模态提示与CMAT预训练特征集成，生成精确的、提示感知的分割图。&lt;h4&gt;主要发现&lt;/h4&gt;在标准基准测试上进行的广泛实验表明，该框架为3D功能分割建立了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;通过将2D视觉基础模型的语义知识转移到3D领域，并设计专门的预训练和分割模型，可以有效解决3D功能分割中的语义边界不清晰问题，提高分割的准确性和语义一致性。&lt;h4&gt;翻译&lt;/h4&gt;功能分割旨在将3D对象解析为功能不同的部分，为机器人操作、具身AI和AR应用中的识别与交互搭建桥梁。尽管最近的研究利用视觉或文本提示来指导这一过程，但它们通常依赖点云编码器作为通用特征提取器，忽视了3D数据固有的挑战，如稀疏性、噪声和几何模糊性。因此，孤立学习的3D特征通常缺乏清晰且语义一致的功能边界。为了解决这一瓶颈，我们提出了一种语义基础学习范式，将大规模2D视觉基础模型(VFMs)的丰富语义知识转移到3D领域。具体来说，我们引入了跨模态亲和力传输(CMAT)预训练策略，将3D编码器与提升的2D语义对齐，并联合优化重建、亲和力和多样性，以产生语义组织的表示。基于此主干，我们进一步设计了跨模态功能分割Transformer(CAST)，它将多模态提示与CMAT预训练特征集成，生成精确的、提示感知的分割图。在标准基准测试上的广泛实验表明，我们的框架为3D功能分割建立了新的最先进结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D功能分割中的语义模糊性问题，即如何将3D物体解析为功能不同的部分（如椅子的座位和腿）。这个问题在现实中至关重要，因为它使机器能够从被动感知转向主动交互，为机器人操作、具身AI和AR应用提供基础，使智能系统能够理解物体的'如何使用'而不仅仅是'是什么'。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D数据的内在挑战（稀疏性、噪声、几何模糊性），并发现现有方法将3D点云编码器作为通用特征提取器未能解决这些问题。他们借鉴了从2D视觉基础模型向3D领域转移语义知识的有前景范式，利用多视图特征提升技术。基于此，他们设计了三阶段框架：首先从2D模型提取语义指导，然后通过CMAT预训练3D编码器学习结构化特征，最后使用CAST架构融合多模态提示进行任务适应。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将2D视觉模型的丰富语义知识转移到3D领域，解决3D数据的语义模糊性。整体流程分为三阶段：1)基础语义基础：从预训练2D模型提取多视图特征并投影回3D点云；2)结构化表示学习：使用CMAT预训练策略，通过几何重建、语义对齐和特征多样性三个目标优化3D编码器；3)提示驱动任务适应：使用CAST架构融合预训练3D特征和多模态提示，生成功能分割图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一学习范式整合跨模态知识转移；2)CMAT预训练策略将2D语义知识蒸馏到3D编码器；3)CAST架构有效融合预训练特征和多模态提示。相比之前工作，本文直接解决3D数据的内在挑战，更明确地建模部分间关系而非仅特征对齐，实现了更精细的部分级区分，并支持更灵活的多模态提示融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将2D视觉模型的语义知识转移到3D领域，提出了一种新的语义基础学习范式，显著提升了3D功能分割的性能，使机器能够更准确地理解和分割物体的功能部分。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Affordance segmentation aims to parse 3D objects into functionally distinctparts, bridging recognition and interaction for applications in roboticmanipulation, embodied AI, and AR. While recent studies leverage visual ortextual prompts to guide this process, they often rely on point cloud encodersas generic feature extractors, overlooking the intrinsic challenges of 3D datasuch as sparsity, noise, and geometric ambiguity. As a result, 3D featureslearned in isolation frequently lack clear and semantically consistentfunctional boundaries. To address this bottleneck, we propose asemantic-grounded learning paradigm that transfers rich semantic knowledge fromlarge-scale 2D Vision Foundation Models (VFMs) into the 3D domain.Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-trainingstrategy that aligns a 3D encoder with lifted 2D semantics and jointlyoptimizes reconstruction, affinity, and diversity to yield semanticallyorganized representations. Building on this backbone, we further design theCross-modal Affordance Segmentation Transformer (CAST), which integratesmulti-modal prompts with CMAT-pretrained features to generate precise,prompt-aware segmentation maps. Extensive experiments on standard benchmarksdemonstrate that our framework establishes new state-of-the-art results for 3Daffordance segmentation.</description>
      <author>example@mail.com (Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen)</author>
      <guid isPermaLink="false">2510.08316v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Towards Precise Channel Knowledge Map: Exploiting Environmental Information from 2D Visuals to 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2510.08140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于三维环境信息的信道知识图(CKM)构建方法，解决传统导频信道探测资源消耗过大的问题，为未来6G网络提供可扩展的信道感知解决方案。&lt;h4&gt;背景&lt;/h4&gt;传统基于导频的信道探测消耗大量通信资源，给具有大规模信道维度、超宽带宽和密集用户部署的未来6G网络带来了严重的可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;利用三维环境信息构建高精度信道知识图(CKM)，解决传统方法资源消耗过大的问题，为未来6G网络提供可扩展的信道感知解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出一种新框架，通过混合模型和数据驱动方法将三维点云整合到CKM构建中，利用三维环境信息而非传统的二维视觉表示来构建高精度CKM。&lt;h4&gt;主要发现&lt;/h4&gt;基于具有语义理解的三维环境可以构建精确的CKM，这些CKM在下一代无线通信应用中展现出巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;三维环境信息对于构建高精度信道知识图至关重要，所提出的框架为未来6G网络的信道感知提供了可扩展解决方案。&lt;h4&gt;翻译&lt;/h4&gt;传统导频信道探测所消耗的大量通信资源带来了不可持续的开销，为具有大规模信道维度、超宽带宽和密集用户部署的未来6G网络带来了严重的可扩展性挑战。作为无线电地图的泛化，信道知识图(CKM)提供了一种范式转变，使无需全面测量即可访问位置标记的信道信息。为了充分利用CKM的潜力，本文强调了利用三维环境信息的必要性，超越传统的二维视觉表示，以构建高精度CKM。具体而言，我们提出了一种新框架，通过混合模型和数据驱动方法将三维点云整合到CKM构建中，并在真实场景案例研究中进行了广泛研究。实验结果表明，基于具有语义理解的三维环境构建精确CKM的潜力，以及它们在下一代无线通信中的应用。我们还发布了一个与高分辨率三维环境数据配对的实际测量信道数据集，以支持未来研究和验证。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建高精度的信道知识图(CKM)问题。传统方法依赖2D视觉信息，无法充分捕捉无线传播环境的复杂特性。这个问题在未来6G网络中至关重要，因为6G具有大规模信道维度、超宽带宽和密集用户部署的特点，传统基于导频的信道探测会消耗大量资源，带来不可持续的开销。CKM作为无线电图的泛化，可以在不进行大量测量的情况下提供位置标记的信道信息，对网络优化、波束成形等应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有CKM构建方法的局限性：传统插值方法需要大量样本且难以捕捉信号突变；射线追踪计算开销大；基于AI的方法大多操作在2D环境。作者认识到无线信道本质上由周围3D环境决定，因此提出利用3D点云信息。他们设计了一个混合模型和数据驱动的框架，借鉴了3D重建技术(如SfM、MVS)、语义理解技术(如开放集语义分割)、点云处理方法(如PointNet++)和无线信道建模原理，但将这些技术专门针对CKM构建问题进行了定制和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用高分辨率3D点云环境和语义理解构建更精确的CKM，采用混合模型和数据驱动方法。整体流程包括：1)3D环境重建：结合无人机图像和LiDAR扫描数据，使用SfM和MVS技术重建点云；2)环境语义理解：使用计算机视觉模型为3D点分配语义标签；3)CKM构建：包含点选择器(利用椭球几何原理过滤无关点)和神经信道增益估计器(学习环境特征到信道参数的映射)。这种方法结合了物理模型的解释能力和数据驱动方法的泛化能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)利用3D点云而非2D视觉信息，保留完整几何结构；2)混合模型和数据驱动的框架，结合物理原理和机器学习；3)创新的点选择器设计，基于共焦椭球壳原理过滤点云；4)环境语义理解，考虑材料特性对信号传播的影响。相比之前工作，不同之处在于：传统插值方法不需要大量样本但无法捕捉突变；射线追踪精度高但计算复杂；基于AI的方法使用3D信息而非2D投影；本文方法结合了物理模型和数据驱动优势，能更好处理复杂3D环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的混合框架，通过利用高分辨率3D点云环境和语义理解，显著提高了信道知识图的构建精度和鲁棒性，为未来6G网络中的环境感知通信提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The substantial communication resources consumed by conventional pilot-basedchannel sounding impose an unsustainable overhead, presenting a criticalscalability challenge for the future 6G networks characterized by massivechannel dimensions, ultra-wide bandwidth, and dense user deployments. As ageneralization of radio map, channel knowledge map (CKM) offers a paradigmshift, enabling access to location-tagged channel information withoutexhaustive measurements. To fully utilize the power of CKM, this workhighlights the necessity of leveraging three-dimensional (3D) environmentalinformation, beyond conventional two-dimensional (2D) visual representations,to construct high-precision CKMs. Specifically, we present a novel frameworkthat integrates 3D point clouds into CKM construction through a hybrid model-and data-driven approach, with extensive case studies in real-world scenarios.The experimental results demonstrate the potential for constructing preciseCKMs based on 3D environments enhanced with semantic understanding, togetherwith their applications in the next-generation wireless communications. We alsorelease a real-world dataset of measured channel paired with high-resolution 3Denvironmental data to support future research and validation.</description>
      <author>example@mail.com (Yancheng Wang, Chuan Huang, Songyang Zhang, Guanying Chen, Wei Guo, Shenglun Lan, Lexi Xu, Xinzhou Cheng, Xiongyan Tang, Shuguang Cui)</author>
      <guid isPermaLink="false">2510.08140v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.07636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Oral presentation at ICIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PIT-QMM的新型大型多模态模型，用于无参考点云质量评估，能够端到端处理文本、图像和点云数据，显著优于现有方法，并支持失真定位功能。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型在图像和视频质量评估领域已取得显著进展，但这些进步尚未在3D资产领域得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;利用大型多模态模型进行无参考点云质量评估，自动评估点云的感知质量而无需参考点云。&lt;h4&gt;方法&lt;/h4&gt;构建PIT-QMM模型，该模型能够处理文本描述、2D投影和3D点云视图这三种互补信息模态，端到端预测点云质量分数。&lt;h4&gt;主要发现&lt;/h4&gt;在流行基准测试上，所提出的方法以显著优势优于最先进方法，且训练迭代次数更少；该框架还能实现失真定位和识别，为模型可解释性和交互性开辟新途径。&lt;h4&gt;结论&lt;/h4&gt;PIT-QMM模型在点云质量评估方面表现出色，不仅提高了评估准确性，还提供了失真定位功能，代码和数据集已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;大型多模态模型最近在图像和视频质量评估领域取得了显著进展，但这一进步尚未在3D资产领域得到充分探索。我们有兴趣使用这些模型进行无参考点云质量评估，其目标是在没有参考点云的情况下自动评估点云的感知质量。我们从观察开始，不同模态的数据——文本描述、2D投影和3D点云视图——提供了关于点云质量的互补信息。然后我们构建了PIT-QMM，这是一种用于无参考点云质量评估的新型大型多模态模型，能够端到端地消费文本、图像和点云来预测质量分数。大量实验表明，在流行的基准测试上，我们提出的方法以显著优势优于最先进方法，且训练迭代次数更少。我们还展示了我们的框架能够实现失真定位和识别，这为模型可解释性和交互性开辟了新的前进道路。代码和数据集可在https://www.github.com/shngt/pit-qmm获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无参考点云质量评估问题，即自动评估点云感知质量而不需要参考原始高质量点云。这个问题在现实中很重要，因为点云是自动驾驶、沉浸式游戏和数字孪生等应用的基础，容易受传感器不准确、压缩和传输错误影响，而传统图像质量评估指标无法捕捉3D数据的复杂性，现有学习方法效果也不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同模态数据（文本、2D投影和3D点云）提供关于点云质量的互补信息，而现有模型要么擅长2D质量评估，要么擅长3D理解，但没有一个完全捕捉PCQA所需的两方面。他们借鉴了预训练基础模型（如Point-BERT、ViT-L/14）、Q-Align的离散化策略、两阶段训练策略和LoRA技术，但针对点云质量评估的特殊需求进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个端到端的多模态模型PIT-QMM，同时处理文本、图像和点云来预测质量分数，利用不同模态的互补优势：点云补丁捕获局部变化，图像投影提供全局视角，文本输入增加心理测量上下文。流程包括：1)数据准备（点云采样、图像投影、文本提示）；2)模型架构（图像编码器、点云编码器、点云投影仪和LLM主干）；3)两阶段训练（特征对齐和指令微调）；4)推理（离散化训练，加权平均映射到连续分数）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个端到端的点-图像-文本多模态模型用于PCQA；任务感知提示设计；高效的编码器感知点云采样策略；两阶段训练策略；失真识别和定位能力。相比之前工作，PIT-QMM在更少训练迭代中取得更好性能；保留局部变化信息；无需昂贵的预处理；是首个能进行失真定位的点云质量评估模型，增强了可解释性和交互性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PIT-QMM首次实现了端到端的点-图像-文本多模态模型，通过融合不同模态的互补信息，显著提升了无参考点云质量评估的性能，并开创了点云失真定位的新方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) have recently enabled considerable advances inthe realm of image and video quality assessment, but this progress has yet tobe fully explored in the domain of 3D assets. We are interested in using thesemodels to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), wherethe aim is to automatically evaluate the perceptual quality of a point cloud inabsence of a reference. We begin with the observation that different modalitiesof data - text descriptions, 2D projections, and 3D point cloud views - providecomplementary information about point cloud quality. We then construct PIT-QMM,a novel LMM for NR-PCQA that is capable of consuming text, images and pointclouds end-to-end to predict quality scores. Extensive experimentation showsthat our proposed method outperforms the state-of-the-art by significantmargins on popular benchmarks with fewer training iterations. We alsodemonstrate that our framework enables distortion localization andidentification, which paves a new way forward for model explainability andinteractivity. Code and datasets are available athttps://www.github.com/shngt/pit-qmm.</description>
      <author>example@mail.com (Shashank Gupta, Gregoire Phillips, Alan C. Bovik)</author>
      <guid isPermaLink="false">2510.07636v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Locality-Sensitive Hashing-Based Efficient Point Transformer for Charged Particle Reconstruction</title>
      <link>http://arxiv.org/abs/2510.07594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 Machine Learning and the Physical Sciences  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的 HEPTv2 方法，通过引入轻量级解码器消除了聚类阶段，直接预测轨迹分配，显著提高了带电粒子轨迹重建的速度。&lt;h4&gt;背景&lt;/h4&gt;带电粒子轨迹重建是 collider 实验的基础任务和主要计算瓶颈。图神经网络(GNNs)虽然表现良好，但存在计算成本高、计算不规则和随机内存访问模式等问题限制了其吞吐量。&lt;h4&gt;目的&lt;/h4&gt;提供对 HEPT 和基于 GNN 的管道在相同数据集和指标下的物理跟踪性能的统一、公平评估，并开发一种更高效的轨迹重建方法。&lt;h4&gt;方法&lt;/h4&gt;提出 HEPTv2，通过扩展 HEPT 添加轻量级解码器，消除聚类阶段，直接预测轨迹分配，保留了 HEPT 的规则、硬件友好的计算特性。&lt;h4&gt;主要发现&lt;/h4&gt;在 TrackML 数据集上，优化后的 HEPTv2 在 A100 上每事件处理时间约为 28 毫秒，同时保持了具有竞争力的跟踪效率。&lt;h4&gt;结论&lt;/h4&gt;HEPTv2 是一种实用、可扩展的替代方案，可以替代基于 GNN 的管道进行快速跟踪，在保持竞争性跟踪效率的同时显著提高了处理速度。&lt;h4&gt;翻译&lt;/h4&gt;带电粒子轨迹重建是 collider 实验的基础任务，也是粒子重建中的主要计算瓶颈。图神经网络(GNNs)在这个问题上表现出色，但昂贵的图构建、不规则的计算和随机内存访问模式大大限制了它们的吞吐量。最近提出的基于散列的高效点变换器(HEPT)通过在注意力计算中使用局部敏感散列(LSH)为大型点云处理提供了理论上保证的近线性复杂度；然而，它的评估主要集中在嵌入质量上，并且 HEPT 所依赖的对象凝聚管道需要一个后处理聚类步骤（如 DBScan），这可能会占用运行时间。在这项工作中，我们做出了两个贡献。首先，我们在相同的数据集和指标下，对 HEPT 和一个代表性的基于 GNN 的管道的物理跟踪性能进行了统一、公平的评估。其次，我们通过添加一个轻量级解码器扩展了 HEPT，该解码器消除了聚类阶段，直接预测轨迹分配。这种修改保留了 HEPT 的规则、硬件友好的计算特性，同时实现了超快的端到端推理。在 TrackML 数据集上，优化后的 HEPTv2 在 A100 上每事件处理时间约为 28 毫秒，同时保持了竞争性的跟踪效率。这些结果使 HEPTv2 成为基于 GNN 的管道用于快速跟踪的实用、可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Charged particle track reconstruction is a foundational task in colliderexperiments and the main computational bottleneck in particle reconstruction.Graph neural networks (GNNs) have shown strong performance for this problem,but costly graph construction, irregular computations, and random memory accesspatterns substantially limit their throughput. The recently proposedHashing-based Efficient Point Transformer (HEPT) offers a theoreticallyguaranteed near-linear complexity for large point cloud processing vialocality-sensitive hashing (LSH) in attention computations; however, itsevaluations have largely focused on embedding quality, and the objectcondensation pipeline on which HEPT relies requires a post-hoc clustering step(e.g., DBScan) that can dominate runtime. In this work, we make twocontributions. First, we present a unified, fair evaluation of physics trackingperformance for HEPT and a representative GNN-based pipeline under the samedataset and metrics. Second, we introduce HEPTv2 by extending HEPT with alightweight decoder that eliminates the clustering stage and directly predictstrack assignments. This modification preserves HEPT's regular,hardware-friendly computations while enabling ultra-fast end-to-end inference.On the TrackML dataset, optimized HEPTv2 achieves approximately 28 ms per eventon an A100 while maintaining competitive tracking efficiency. These resultsposition HEPTv2 as a practical, scalable alternative to GNN-based pipelines forfast tracking.</description>
      <author>example@mail.com (Shitij Govil, Jack P. Rodgers, Yuan-Tang Chou, Siqi Miao, Amit Saha, Advaith Anand, Kilian Lieret, Gage DeZoort, Mia Liu, Javier Duarte, Pan Li, Shih-Chieh Hsu)</author>
      <guid isPermaLink="false">2510.07594v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Human Action Recognition from Point Clouds over Time</title>
      <link>http://arxiv.org/abs/2510.05506v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于3D视频的新人类动作识别方法，利用点云数据作为第三种识别途径，结合基于点和稀疏卷积的技术，在NTU RGB-D 120数据集上达到了89.3%的准确率，超过了之前的点云动作识别方法。&lt;h4&gt;背景&lt;/h4&gt;最近的人类动作识别研究主要集中在骨骼动作识别和基于视频的方法。随着消费级深度传感器和激光雷达设备的普及，利用密集3D数据进行动作识别的机会正在增加，为动作识别提供了第三种可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种不同于骨骼动作识别和基于视频方法的第三种动作识别途径，通过3D视频识别动作，并提高识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新方法，包括一个流程，用于分割场景中人体点云与背景，跟踪个体随时间变化，并进行身体部位分割。该方法支持来自深度传感器和单目深度估计的点云。提出的HAR框架核心是一种新的3D动作识别骨干网络，结合了基于点技术和稀疏卷积网络。实验包括表面法线、颜色、红外强度和身体部位解析标签等辅助点特征。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB-D 120数据集上的评估表明，该方法与现有的骨骼动作识别算法具有竞争力。在集成设置中结合基于传感器和估计的深度输入，当考虑不同受试者进行训练和测试时，该方法达到89.3%的准确率，超过了之前的点云动作识别方法。&lt;h4&gt;结论&lt;/h4&gt;该方法利用3D点云数据提供了一种有效的人类动作识别方法，结合了基于点和稀疏卷积的技术，并能够处理来自不同来源的深度数据。&lt;h4&gt;翻译&lt;/h4&gt;最近的人类动作识别研究主要集中在骨骼动作识别和基于视频的方法。随着消费级深度传感器和激光雷达设备的日益普及，利用密集3D数据进行动作识别的机会正在增加，这为动作识别提供了第三种方法。本文通过引入一种流程，提出了一种从3D视频识别动作的新方法，该流程分割场景中人体点云与背景，跟踪个体随时间变化，并进行身体部位分割。该方法支持来自深度传感器和单目深度估计的点云。所提出的HAR框架核心是一种用于3D动作识别的新骨干网络，它将基于点技术与应用于体素映射点云序列的稀疏卷积网络相结合。实验包括辅助点特征，如表面法线、颜色、红外强度和身体部位解析标签，以提高识别准确性。在NTU RGB-D 120数据集上的评估表明，该方法与现有的骨骼动作识别算法具有竞争力。此外，在集成设置中结合基于传感器和估计的深度输入，当考虑不同受试者进行训练和测试时，该方法达到89.3%的准确率，超过了之前的点云动作识别方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于点云序列的人体动作识别问题。目前人体动作识别主要依赖骨骼数据和视频数据，而随着深度传感器普及，利用3D点云数据是新兴方向。这个问题在现实中非常重要，可用于监控中的异常检测和跌倒识别、自动视频标注（如体育分析）、以及自动驾驶中确保行人安全等场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：视频方法需降低帧大小影响精度，骨骼方法易受遮挡和噪声影响，点云方法缺乏人体分割和背景去除。作者借鉴了点云深度学习中的PointNet系列和稀疏卷积网络（如Minkowski Networks），以及动作识别领域的3DV、PSTNet等方法。作者设计了一个新流程，支持深度传感器和RGB视频两种输入方式，通过人体分割、跟踪和身体部位分割去除背景干扰，并开发了结合点基技术和稀疏卷积的混合架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用点云序列而非传统视频或骨骼数据进行人体动作识别，结合点基技术和稀疏卷积网络优势，通过人体分割去除背景干扰，并利用辅助特征提高精度。整体流程包括：1)人体点云获取（深度传感器或RGB+单目深度估计）；2)预处理（实例掩码去噪、3D点云去噪、人体跟踪、点采样、表面法线计算）；3)动作识别模型（T-Net嵌入、体素映射、稀疏CNN主干、全局稀疏最大池化、全连接分类）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新型人体点云获取流程，支持两种输入方式并进行完整人体分割；2)混合架构主干网络，结合点基技术和稀疏卷积；3)系统利用表面法线、红外强度/颜色、身体部位标签等辅助特征；4)高效的多人物处理方法。相比之前工作，本文方法提供了图像和3D空间中的人体分割，支持单目深度估计，采用混合架构而非单一技术路线，并系统探索了辅助特征的影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种新颖的基于点云序列的人体动作识别方法，通过结合人体分割、稀疏卷积网络和辅助特征，在NTU RGB-D 120数据集上实现了与现有骨骼动作识别算法相竞争的性能，并在跨主题设置下达到了新的最先进水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research into human action recognition (HAR) has focused predominantlyon skeletal action recognition and video-based methods. With the increasingavailability of consumer-grade depth sensors and Lidar instruments, there is agrowing opportunity to leverage dense 3D data for action recognition, todevelop a third way. This paper presents a novel approach for recognizingactions from 3D videos by introducing a pipeline that segments human pointclouds from the background of a scene, tracks individuals over time, andperforms body part segmentation. The method supports point clouds from bothdepth sensors and monocular depth estimation. At the core of the proposed HARframework is a novel backbone for 3D action recognition, which combinespoint-based techniques with sparse convolutional networks applied tovoxel-mapped point cloud sequences. Experiments incorporate auxiliary pointfeatures including surface normals, color, infrared intensity, and body partparsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D120 dataset demonstrates that the method is competitive with existing skeletalaction recognition algorithms. Moreover, combining both sensor-based andestimated depth inputs in an ensemble setup, this approach achieves 89.3%accuracy when different human subjects are considered for training and testing,outperforming previous point cloud action recognition methods.</description>
      <author>example@mail.com (James Dickens)</author>
      <guid isPermaLink="false">2510.05506v3</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</title>
      <link>http://arxiv.org/abs/2510.08450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 22 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络中的过压缩问题，从模型存储和检索容量角度重新审视此现象，引入新合成任务证明信息瓶颈会饱和容量，并借鉴序列建模思想开发新型GNN架构提升容量。&lt;h4&gt;背景&lt;/h4&gt;图神经网络利用图结构通过消息传递机制在节点间传输信息，但存在过压缩问题，即大量感受野的信息被压缩到固定大小的向量中，导致信息瓶颈。&lt;h4&gt;目的&lt;/h4&gt;重新审视过压缩现象，研究现有测量过压缩任务的局限性，引入新合成任务证明信息瓶颈的影响，并开发具有改进容量的新型GNN架构。&lt;h4&gt;方法&lt;/h4&gt;借鉴序列建模文献中的关联记忆、快速权重编程和xLSTM模型思想，开发新型GNN架构；在容量合成任务和多种真实世界图基准测试上评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;信息瓶颈会饱和模型的存储和检索容量；现有测量过压缩的任务存在局限性；新型GNN架构在容量任务和真实图基准上表现出强大性能。&lt;h4&gt;结论&lt;/h4&gt;通过从存储和检索容量角度研究过压缩问题，并借鉴序列建模思想，成功开发了具有改进容量的新型GNN架构，解决了过压缩导致的信息瓶颈问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)利用图结构在节点间传输信息，通常通过消息传递机制。虽然这些模型已找到广泛应用，但它们容易受到过压缩的影响，即大量感受野的节点表示信息被压缩到单个固定大小的向量中，导致信息瓶颈。本文我们从模型存储和检索容量的角度重新审视过压缩现象，将其定义为节点表示中可存储供后续使用的信息量。我们研究了现有用于测量过压缩的任务的一些局限性，并引入了一个新的合成任务来证明信息瓶颈会饱和此容量。此外，我们借鉴序列建模文献中关于关联记忆、快速权重编程和xLSTM模型的思想，开发了一种具有改进容量的新型GNN架构。我们在容量合成任务以及多种真实世界图基准测试上证明了该架构的强大性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) leverage the graph structure to transmitinformation between nodes, typically through the message-passing mechanism.While these models have found a wide variety of applications, they are known tosuffer from over-squashing, where information from a large receptive field ofnode representations is collapsed into a single fixed sized vector, resultingin an information bottleneck. In this paper, we re-examine the over-squashingphenomenon through the lens of model storage and retrieval capacity, which wedefine as the amount of information that can be stored in a node'srepresentation for later use. We study some of the limitations of existingtasks used to measure over-squashing and introduce a new synthetic task todemonstrate that an information bottleneck can saturate this capacity.Furthermore, we adapt ideas from the sequence modeling literature onassociative memories, fast weight programmers, and the xLSTM model to develop anovel GNN architecture with improved capacity. We demonstrate strongperformance of this architecture both on our capacity synthetic task, as wellas a range of real-world graph benchmarks.</description>
      <author>example@mail.com (Hugh Blayney, Álvaro Arroyo, Xiaowen Dong, Michael M. Bronstein)</author>
      <guid isPermaLink="false">2510.08450v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Verifying Graph Neural Networks with Readout is Intractable</title>
      <link>http://arxiv.org/abs/2510.08045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种用于量化聚合组合图神经网络(ACR-GNNs)的逻辑语言，证明了量化GNN验证任务的计算复杂性，并通过实验验证了量化ACR-GNN模型的效率和性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在许多领域有广泛应用，但量化GNN的验证问题在计算上是复杂的，需要研究如何确保基于GNN的系统的安全性。&lt;h4&gt;目的&lt;/h4&gt;开发一种逻辑语言来推理量化聚合组合图神经网络，并研究其验证任务的计算复杂性，同时探索量化模型的效率和性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种逻辑语言来表征量化ACR-GNNs，使用该逻辑语言证明了量化GNN验证任务的计算复杂性，并通过实验评估了量化模型的性能。&lt;h4&gt;主要发现&lt;/h4&gt;提供了量化ACR-GNNs的逻辑表征；证明了具有读取功能的量化GNN验证任务是(co)NEXPTIME完全的；量化ACR-GNN模型是轻量级的，同时保持良好的准确性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;量化GNN的验证在计算上是不可处理的，这促使了确保基于GNN的系统安全性的大量研究努力。量化ACR-GNN模型在保持性能的同时具有计算效率优势。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种用于推理具有全局读取功能的量化聚合组合图神经网络(ACR-GNNs)的逻辑语言。我们提供了逻辑表征并使用它证明了具有读取功能的量化GNN验证任务是(co)NEXPTIME完全的。这一结果表明量化GNN的验证在计算上是不可处理的，促使了大量研究努力以确保基于GNN的系统的安全性。我们还通过实验证明，量化ACR-GNN模型是轻量级的，同时与非量化模型相比保持良好的准确性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a logical language for reasoning about quantizedaggregate-combine graph neural networks with global readout (ACR-GNNs). Weprovide a logical characterization and use it to prove that verification tasksfor quantized GNNs with readout are (co)NEXPTIME-complete. This result impliesthat the verification of quantized GNNs is computationally intractable,prompting substantial research efforts toward ensuring the safety of GNN-basedsystems. We also experimentally demonstrate that quantized ACR-GNN models arelightweight while maintaining good accuracy and generalization capabilitieswith respect to non-quantized models.</description>
      <author>example@mail.com (Artem Chernobrovkin, Marco Sälzer, François Schwarzentruber, Nicolas Troquard)</author>
      <guid isPermaLink="false">2510.08045v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.07990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GraphEnet的图神经网络，用于基于事件相机的2D人体姿态估计，利用事件数据的稀疏特性和基于线条的中间表示实现高频姿态估计。&lt;h4&gt;背景&lt;/h4&gt;人体姿态估计是人机交互应用的关键模块，深度学习技术使RGB相机和商业GPU的鲁棒方法普及；基于事件的相机因其低延迟和低能耗优势在视觉研究社区受到关注，特别适合便携式电子设备和移动机器人等资源受限场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用事件相机稀疏输出特性的方法，通过基于线条的中间事件表示，实现单人2D人体姿态的高频率估计。&lt;h4&gt;方法&lt;/h4&gt;提出名为GraphEnet的图神经网络架构，采用新颖的偏移向量学习范式和基于置信度的池化技术进行人体姿态估计，这是首次将图神经网络应用于事件数据的人体姿态估计工作。&lt;h4&gt;主要发现&lt;/h4&gt;GraphEnet能够有效利用事件相机的稀疏特性，实现高频2D人体姿态估计，为资源受限场景提供了高效解决方案。&lt;h4&gt;结论&lt;/h4&gt;基于事件相机和图神经网络的人体姿态估计方法为低延迟、低能耗应用提供了新途径，相关代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;人体姿态估算是人机交互应用中的关键模块，特别是自深度学习技术兴起以来，使用RGB相机和商业GPU的鲁棒方法已可供消费者使用。另一方面，基于事件的相机在视觉研究社区中因其低延迟和低能耗优势而越来越受欢迎，这些优势使它们成为资源受限应用（如便携式电子设备和移动机器人）的理想选择。在这项工作中，我们提出了一种图神经网络GraphEnet，它利用事件相机输出的稀疏特性，通过基于线条的中间事件表示，以高频率估计单人的2D人体姿态。该架构结合了一种新颖的偏移向量学习范式和基于置信度的池化来估计人体姿态。这是首次将图神经网络应用于事件数据进行人体姿态估计的工作。代码已在https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Pose Estimation is a crucial module in human-machine interactionapplications and, especially since the rise in deep learning technology, robustmethods are available to consumers using RGB cameras and commercial GPUs. Onthe other hand, event-based cameras have gained popularity in the visionresearch community for their low latency and low energy advantages that makethem ideal for applications where those resources are constrained like portableelectronics and mobile robots. In this work we propose a Graph Neural Network,GraphEnet, that leverages the sparse nature of event camera output, with anintermediate line based event representation, to estimate 2D Human Pose of asingle person at a high frequency. The architecture incorporates a novel offsetvector learning paradigm with confidence based pooling to estimate the humanpose. This is the first work that applies Graph Neural Networks to event datafor Human Pose Estimation. The code is open-source athttps://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.</description>
      <author>example@mail.com (Gaurvi Goyal, Pham Cong Thuong, Arren Glover, Masayoshi Mizuno, Chiara Bartolozzi)</author>
      <guid isPermaLink="false">2510.07990v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning Based Few-Shot Graph-Level Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.07847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ARRML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MA-GAD的元学习框架，用于解决图级别异常检测在少样本条件下的挑战，通过结合图压缩模块和元学习方法提高异常检测性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图级别异常检测在欺诈检测、评论分类和生物化学等领域具有重要作用。虽然图神经网络(GNNs)在该领域取得了进展，但现有方法严重依赖大量标记数据，而现实场景中这类数据往往不可用。此外，基于GNN的少样本异常检测方法容易受到噪声干扰，导致嵌入质量差和模型鲁棒性降低。&lt;h4&gt;目的&lt;/h4&gt;解决现有图级别异常检测方法对大量标记数据的依赖问题，以及少样本条件下噪声干扰导致的性能下降问题，提高模型在有限样本情况下的异常检测性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出MA-GAD框架，包含三个关键组件：1)图压缩模块减少图的大小，减轻噪声干扰同时保留关键节点信息；2)元学习方法从相似网络中提取元异常信息，学习能快速适应新任务的初始化模型；3)偏置网络增强异常节点与正常节点之间的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;基于四个真实生物化学数据集的实验结果表明，在少样本条件下，MA-GAD在图级别异常检测任务上优于现有最先进的方法。在图异常和子图异常检测任务上的实验验证了该框架在真实数据集上的有效性。&lt;h4&gt;结论&lt;/h4&gt;MA-GAD框架通过结合图压缩和元学习方法，有效解决了少样本条件下图级别异常检测面临的挑战，提高了异常检测的性能和鲁棒性，为现实场景中的异常检测问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图级别异常检测旨在识别图数据集中的异常图或子图，在欺诈检测、评论分类和生物化学等多个领域发挥着重要作用。虽然图神经网络(GNNs)在该领域取得了显著进展，但现有方法严重依赖大量标记数据，而现实场景中这类数据往往不可用。此外，基于GNN的少样本异常检测方法容易受到噪声干扰，导致嵌入质量差和模型鲁棒性降低。为解决这些挑战，我们提出了一种新颖的基于元学习的图级别异常检测框架(MA-GAD)，包含一个图压缩模块，该模块减少图的大小，减轻噪声干扰同时保留基本节点信息。我们还利用元学习从相似网络中提取元异常信息，使模型能够学习一种初始化模型，该模型可以快速适应有限样本的新任务。这提高了目标图上的异常检测性能，并且使用偏置网络来增强异常节点与正常节点之间的区分度。基于四个真实生物化学数据集的实验结果表明，在少样本条件下，MA-GAD在图级别异常检测任务上优于现有最先进的方法。在图异常和子图异常检测任务上的实验验证了该框架在真实数据集上的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-level anomaly detection aims to identify anomalous graphs or subgraphswithin graph datasets, playing a vital role in various fields such as frauddetection, review classification, and biochemistry. While Graph Neural Networks(GNNs) have made significant progress in this domain, existing methods relyheavily on large amounts of labeled data, which is often unavailable inreal-world scenarios. Additionally, few-shot anomaly detection methods based onGNNs are prone to noise interference, resulting in poor embedding quality andreduced model robustness. To address these challenges, we propose a novelmeta-learning-based graph-level anomaly detection framework (MA-GAD),incorporating a graph compression module that reduces the graph size,mitigating noise interference while retaining essential node information. Wealso leverage meta-learning to extract meta-anomaly information from similarnetworks, enabling the learning of an initialization model that can rapidlyadapt to new tasks with limited samples. This improves the anomaly detectionperformance on target graphs, and a bias network is used to enhance thedistinction between anomalous and normal nodes. Our experimental results, basedon four real-world biochemical datasets, demonstrate that MA-GAD outperformsexisting state-of-the-art methods in graph-level anomaly detection underfew-shot conditions. Experiments on both graph anomaly and subgraph anomalydetection tasks validate the framework's effectiveness on real-world datasets.</description>
      <author>example@mail.com (Liting Li, Yumeng Wang, Yueheng Sun)</author>
      <guid isPermaLink="false">2510.07847v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title>
      <link>http://arxiv.org/abs/2510.07620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DGTEN是一种基于深度高斯的信任评估网络，通过统一图框架实现动态信任评估，能够捕捉变化关系、表达校准置信度并抵抗对手操纵。&lt;h4&gt;背景&lt;/h4&gt;在大型、快速演化的图中进行动态信任评估需要能够捕捉变化关系、表达校准置信度并抵抗对手操纵的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的图框架，实现不确定性感知的消息传递、表达性时间建模和对信任定向攻击的内置防御。&lt;h4&gt;方法&lt;/h4&gt;DGTEN将节点和边表示为高斯分布，使用混合绝对-高沙漏位置编码和基于Kolmogorov-Arnold网络的无偏多头注意力机制建模信任演化，采用基于常微分方程的残差学习模块捕捉突变和平滑趋势，并通过鲁棒的自适应集成系数分析降低可疑交互的权重。&lt;h4&gt;主要发现&lt;/h4&gt;在Bitcoin-Alpha上的单时隙预测中，MCC比最佳动态基线提高10.77%；在冷启动场景中，实现16.41%的MCC增益；在对抗性开关攻击下，MCC比基线高出高达11.63%。&lt;h4&gt;结论&lt;/h4&gt;DGTEN统一框架的有效性在两个签名比特币信任网络上得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;在大型、快速演化的图中进行动态信任评估需要能够捕捉变化关系、表达校准置信度并抵抗对手操纵的模型。DGTEN（基于深度高斯的信任评估网络）引入了一个统一的图框架，通过结合不确定性感知的消息传递、表达性时间建模和对信任定向攻击的内置防御，实现了这三个目标。它将节点和边表示为高斯分布，使语义信号和认知不确定性通过图神经网络传播，从而实现风险感知的信任决策，而非过度自信的猜测。为了建模信任的演化，它采用混合绝对-高沙漏（HAGH）位置编码和基于Kolmogorov-Arnold网络的无偏多头注意力， followed by an ordinary differential equation (ODE)-based residual learning module to jointly capture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity measures, mitigating reputation laundering, sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN delivers significant improvements: in single-timeslot prediction on Bitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the cold-start scenario, it achieves a 16.41% MCC gain - the largest across all tasks and datasets. Under adversarial on/off attacks, it surpasses the baseline by up to 11.63% MCC. These results validate the effectiveness of the unified DGTEN framework.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic trust evaluation in large, rapidly evolving graphs requires modelsthat can capture changing relationships, express calibrated confidence, andresist adversarial manipulation. DGTEN (Deep Gaussian-based Trust EvaluationNetwork) introduces a unified graph framework that achieves all three bycombining uncertainty-aware message passing, expressive temporal modeling, andbuilt-in defenses against trust-targeted attacks. It represents nodes and edgesas Gaussian distributions so that both semantic signals and epistemicuncertainty propagate through the graph neural network, enabling risk-awaretrust decisions rather than overconfident guesses. To model how trust evolves,it employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding withKolmogorov-Arnold network-based unbiased multi-head attention, followed by anordinary differential equation (ODE)-based residual learning module to jointlycapture abrupt shifts and smooth trends. Robust adaptive ensemble coefficientanalysis prunes or down-weights suspicious interactions using complementarycosine and Jaccard similarity measures, mitigating reputation laundering,sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTENdelivers significant improvements: in single-timeslot prediction onBitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in thecold-start scenario, it achieves a 16.41% MCC gain - the largest across alltasks and datasets. Under adversarial on/off attacks, it surpasses the baselineby up to 11.63% MCC. These results validate the effectiveness of the unifiedDGTEN framework.</description>
      <author>example@mail.com (Muhammad Usman, Yugyung Lee)</author>
      <guid isPermaLink="false">2510.07620v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2510.07426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE ICTAI 2025. Version 0.9. 10 pages, 5 figures.  Preprint differs from the published version in formatting and minor wording&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TESTAM+框架，通过引入空间语义专家整合物理道路拓扑和数据驱动特征相似性，显著提升了交通预测性能，同时减少了计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;交通预测对智能交通系统至关重要，可缓解拥堵并减少排放。然而，现有的混合专家框架如TESTAM缺乏对物理道路网络拓扑的明确整合，限制了其空间能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强的空间时间预测框架TESTAM+，通过结合物理道路拓扑和数据驱动特征相似性来提高交通预测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;引入了新的空间语义专家(SpatioSemantic Expert)，通过混合图构建整合物理道路拓扑和数据驱动特征相似性，改进了原有的TESTAM框架。&lt;h4&gt;主要发现&lt;/h4&gt;TESTAM+相比TESTAM有显著改进：在METR LA上MAE减少1.3%，在PEMS BAY上改进4.1%；战略专家选择优于简单集成聚合；单个专家表现出色，自适应专家和空间语义专家都达到1.63 MAE；最佳配置相比MegaCRN在METR LA上实现11.5%的MAE减少；推理延迟减少53.1%。&lt;h4&gt;结论&lt;/h4&gt;更少但战略设计的专家优于复杂的多专家集成，以优越的计算效率建立了新的最先进性能，适合实时部署。&lt;h4&gt;翻译&lt;/h4&gt;交通预测是智能交通系统的基础，能够在日益复杂的城市环境中实现拥堵缓解和排放减少。虽然最近的图神经网络方法已推进了空间时间建模，但现有的混合专家框架如时间增强空间时间注意力模型(TESTAM)缺乏对物理道路网络拓扑的明确整合，限制了其空间能力。我们提出了TESTAM+，一个增强的空间时间预测框架，引入了新的空间语义专家，通过混合图构建将物理道路拓扑与数据驱动特征相似性相结合。TESTAM+相比TESTAM实现了显著改进：在METR LA上MAE减少1.3%（3.10 vs 3.14），在PEMS BAY上改进4.1%（1.65 vs 1.72）。通过全面的消融研究，我们发现战略专家选择从根本上优于简单的集成聚合。单个专家表现出 remarkable 的有效性：自适应专家在PEMS BAY上达到1.63 MAE，优于原始三个专家的TESTAM（1.72 MAE），而空间语义专家以相同的1.63 MAE匹配这一性能。最佳的Identity + Adaptive配置相比最先进的MegaCRN在METR LA上实现了11.5%的MAE减少（2.99 vs 3.38），同时相比完整的四个专家TESTAM+减少了53.1%的推理延迟。我们的发现揭示出，更少但战略设计的专家优于复杂的多专家集成，以优越的计算效率建立了新的最先进性能，适合实时部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting is fundamental to intelligent transportation systems,enabling congestion mitigation and emission reduction in increasingly complexurban environments. While recent graph neural network approaches have advancedspatial temporal modeling, existing mixture of experts frameworks like TimeEnhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporationof physical road network topology, limiting their spatial capabilities. Wepresent TESTAM+, an enhanced spatio temporal forecasting framework thatintroduces a novel SpatioSemantic Expert integrating physical road topologywith data driven feature similarity through hybrid graph construction. TESTAM+achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Throughcomprehensive ablation studies, we discover that strategic expert selectionfundamentally outperforms naive ensemble aggregation. Individual expertsdemonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE onPEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while theSpatioSemantic Expert matches this performance with identical 1.63 MAE. Theoptimal Identity + Adaptive configuration achieves an 11.5% MAE reductioncompared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducinginference latency by 53.1% compared to the full four expert TESTAM+. Ourfindings reveal that fewer, strategically designed experts outperform complexmulti expert ensembles, establishing new state of the art performance withsuperior computational efficiency for real time deployment.</description>
      <author>example@mail.com (Walid Guettala, Yufan Zhao, László Gulyás)</author>
      <guid isPermaLink="false">2510.07426v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data</title>
      <link>http://arxiv.org/abs/2510.07350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了两种深度学习模型(GNN-RNN和MMST-ViT)在气候变化背景下农作物产量预测中的跨区域泛化能力，发现模型性能在不同地理区域间存在显著差异，GNN-RNN表现出更好的泛化能力和计算效率。&lt;h4&gt;背景&lt;/h4&gt;气候变化正在扰乱农业系统，准确的农作物产量预测对粮食安全至关重要。深度学习模型在使用卫星和天气数据进行产量预测方面显示出潜力，但它们跨地理区域和年份泛化的能力尚未得到充分检验。&lt;h4&gt;目的&lt;/h4&gt;在现实的分布外(OOD)条件下对两种最先进的模型GNN-RNN和MMST-ViT进行基准测试，评估其跨区域和跨年份的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用大规模CropNet数据集（涵盖2017-2022年美国1200多个县），在七个美国农业部农场资源区域进行留一集群交叉验证和提前一年预测场景，比较两种模型的性能。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-RNN表现出更好的泛化能力和计算效率（训练速度比MMST-ViT快135倍）；MMST-ViT在域内表现良好但在OOD条件下性能急剧下降；心脏地带和大平原北部地区转移稳定（RMSE&lt;10蒲式耳/英亩），而草原通道持续表现不佳（RMSE&gt;20蒲式耳/英亩），反映了气候和灌溉差异的影响。&lt;h4&gt;结论&lt;/h4&gt;空间-时间对齐是稳健泛化的关键因素，需要透明的OOD评估协议以确保公平可靠的气候感知农业预测系统。&lt;h4&gt;翻译&lt;/h4&gt;气候变化日益扰乱农业系统，使准确的农作物产量预测对粮食安全至关重要。虽然深度学习模型在使用卫星和天气数据进行产量预测方面显示出潜力，但它们跨地理区域和年份泛化的能力（对实际部署至关重要）尚未得到充分检验。我们使用大规模CropNet数据集（涵盖2017-2022年美国1200多个县）在现实的分布外(OOD)条件下对两种最先进的模型GNN-RNN和MMST-ViT进行基准测试。通过在七个美国农业部农场资源区域进行留一集群交叉验证和提前一年预测场景，我们确定了跨区域转移能力的显著变异性。GNN-RNN表现出更好的泛化能力，在地理变化下呈现正相关，而MMST-ViT在域内表现良好但在OOD条件下性能急剧下降。心脏地带和大平原北部地区显示出稳定的转移动态（大豆RMSE小于10蒲式耳/英亩），而草原通道在两种模型和作物中持续表现不佳（RMSE大于20蒲式耳/英亩），揭示了由半干旱气候、灌溉模式和光谱覆盖不完整导致的结构差异。除了准确性差异外，GNN-RNN的训练速度比MMST-ViT快135倍（14分钟对比31.5小时），使其更可持续部署。我们的研究结果强调，空间-时间对齐（不仅仅是模型复杂性或数据规模）是稳健泛化的关键，并需要透明的OOD评估协议以确保公平可靠的气候感知农业预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Climate change is increasingly disrupting agricultural systems, makingaccurate crop yield forecasting essential for food security. While deeplearning models have shown promise in yield prediction using satellite andweather data, their ability to generalize across geographic regions and years -critical for real-world deployment - remains largely untested. We benchmark twostate-of-the-art models, GNN-RNN and MMST-ViT, under realisticout-of-distribution (OOD) conditions using the large-scale CropNet datasetspanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-outcross-validation across seven USDA Farm Resource Regions and year-aheadprediction scenarios, we identify substantial variability in cross-regiontransferability. GNN-RNN demonstrates superior generalization with positivecorrelations under geographic shifts, while MMST-ViT performs well in-domainbut degrades sharply under OOD conditions. Regions like Heartland and NorthernGreat Plains show stable transfer dynamics (RMSE less than 10 bu/acre forsoybean), whereas Prairie Gateway exhibits persistent underperformance (RMSEgreater than 20 bu/acre) across both models and crops, revealing structuraldissimilarities likely driven by semi-arid climate, irrigation patterns, andincomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it moreviable for sustainable deployment. Our findings underscore thatspatial-temporal alignment - not merely model complexity or data scale - is keyto robust generalization, and highlight the need for transparent OOD evaluationprotocols to ensure equitable and reliable climate-aware agriculturalforecasting.</description>
      <author>example@mail.com (Aditya Chakravarty)</author>
      <guid isPermaLink="false">2510.07350v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption</title>
      <link>http://arxiv.org/abs/2510.08217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This preprint has not undergone peer review or any post-submission  improvements or corrections. The Version of Record of this contribution will  be published in "ECML PKDD Workshop 2025 - Advanced Analytics and Learning on  Temporal Data"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的船舶燃料消耗预测方法，通过引入新数据集、标准化基准和基础模型应用，实现了准确预测。&lt;h4&gt;背景&lt;/h4&gt;在航运业中，燃料消耗和排放是关键因素，对经济效率和环境影响重大。准确预测船舶燃料消耗对优化海运运营至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决船舶燃料消耗预测中存在的异构方法和有限高质量数据集问题，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;引入并发布包含三艘船运营和环境数据的新数据集；定义标准化基准覆盖表格回归和时间序列回归；研究使用TabPFN基础模型进行船舶消耗建模的上下文学习应用。&lt;h4&gt;主要发现&lt;/h4&gt;所有评估模型表现良好，支持船上数据驱动燃料预测的可行性；包含环境条件的模型优于仅依赖速度的基线；TabPFN略优于其他技术；包含时间背景可提高准确性。&lt;h4&gt;结论&lt;/h4&gt;具有上下文学习能力的基础模型在船舶燃料消耗预测中具有良好性能，为优化海运运营提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在航运业中，燃料消耗和排放是关键因素，因为它们对经济效率和环境影响重大。准确预测船舶燃料消耗对于进一步优化海运运营至关重要。然而，异构方法和有限的高质量数据集阻碍了建模方法的直接比较。本文做出三项关键贡献：(1)我们引入并发布了一个新数据集，包含三艘船的运营和环境数据；(2)我们定义了一个标准化基准，涵盖表格回归和时间序列回归；(3)我们研究了使用TabPFN基础模型进行船舶消耗建模的上下文学习应用-据我们所知，这是该领域的首次尝试。我们的结果表明所有评估的模型都表现出强大的性能，支持了船上、数据驱动的燃料预测的可行性。包含环境条件的模型始终优于仅依赖船舶速度的多项式基线。TabPFN略优于其他技术，突显了具有上下文学习能力的基础模型在表格预测中的潜力。此外，包含时间背景可以提高准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the shipping industry, fuel consumption and emissions are critical factorsdue to their significant impact on economic efficiency and environmentalsustainability. Accurate prediction of ship fuel consumption is essential forfurther optimization of maritime operations. However, heterogeneousmethodologies and limited high-quality datasets hinder direct comparison ofmodeling approaches. This paper makes three key contributions: (1) we introduceand release a new dataset(https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operationaland environmental data from three ships; (2) we define a standardized benchmarkcovering tabular regression and time-series regression (3) we investigate theapplication of in-context learning for ship consumption modeling using theTabPFN foundation model - a first in this domain to our knowledge. Our resultsdemonstrate strong performance across all evaluated models, supporting thefeasibility of onboard, data-driven fuel prediction. Models incorporatingenvironmental conditions consistently outperform simple polynomial baselinesrelying solely on vessel speed. TabPFN slightly outperforms other techniques,highlighting the potential of foundation models with in-context learningcapabilities for tabular prediction. Furthermore, including temporal contextimproves accuracy.</description>
      <author>example@mail.com (Justus Viga, Penelope Mueck, Alexander Löser, Torben Weis)</author>
      <guid isPermaLink="false">2510.08217v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection</title>
      <link>http://arxiv.org/abs/2510.08073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 spotlight&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于物理原理的AI生成视频检测方法，通过引入归一化时空梯度(Normalized Spatiotemporal Gradient, NSG)统计量，开发了一种高效的AI生成视频检测框架NSG-VD，实验证明该方法在召回率和F1分数上显著优于现有基线方法。&lt;h4&gt;背景&lt;/h4&gt;AI生成的视频已达到近乎完美的视觉真实感（如Sora模型），迫切需要可靠的检测机制。然而，这类视频的检测面临两大挑战：一是建模高维时空动力学，二是识别违反物理规律的细微异常。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于物理原理的AI生成视频检测方法，有效解决现有方法在检测高维时空动态和物理异常方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出基于概率流守恒原理的物理驱动检测范式；定义NSG统计量量化空间概率梯度与时间密度变化的比率；利用预训练扩散模型通过空间梯度近似和运动感知时间建模开发NSG估计器；提出NSG-VD方法计算测试视频与真实视频NSG特征间的最大均值差异作为检测指标；推导真实与生成视频NSG特征距离上界。&lt;h4&gt;主要发现&lt;/h4&gt;NSG-VD在召回率上比最先进基线方法高16.00%，在F1分数上高10.75%；生成视频因分布偏移表现出比真实视频更大的NSG特征差异；所提方法无需复杂运动分解同时保持物理约束。&lt;h4&gt;结论&lt;/h4&gt;基于物理原理的NSG-VD方法在AI生成视频检测任务中表现出色，显著优于现有方法，为应对AI生成内容带来的真实性挑战提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;AI生成的视频已经达到了近乎完美的视觉真实感（例如Sora模型），迫切需要可靠的检测机制。然而，这类视频的检测在建模高维时空动力学和识别违反物理规律的细微异常方面面临重大挑战。在本文中，我们提出了一种基于概率流守恒原理的物理驱动AI生成视频检测范式。具体而言，我们提出了一种称为归一化时空梯度(Normalized Spatiotemporal Gradient, NSG)的统计量，它量化了空间概率梯度与时间密度变化的比率，明确捕捉了自然视频动态的偏差。利用预训练的扩散模型，我们通过空间梯度近似和运动感知的时间建模开发了NSG估计器，无需复杂的运动分解同时保持物理约束。基于此，我们提出了一种基于NSG的视频检测方法(NSG-VD)，将测试视频和真实视频的NSG特征之间的最大均值差异(MMD)作为检测指标。最后，我们推导了真实视频和生成视频NSG特征距离的上界，证明生成视频由于分布偏移表现出放大的差异。大量实验证实，NSG-VD在召回率上比最先进的基线方法高出16.00%，在F1分数上高出10.75%，验证了NSG-VD的卓越性能。源代码可在https://github.com/ZSHsh98/NSG-VD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-generated videos have achieved near-perfect visual realism (e.g., Sora),urgently necessitating reliable detection mechanisms. However, detecting suchvideos faces significant challenges in modeling high-dimensional spatiotemporaldynamics and identifying subtle anomalies that violate physical laws. In thispaper, we propose a physics-driven AI-generated video detection paradigm basedon probability flow conservation principles. Specifically, we propose astatistic called Normalized Spatiotemporal Gradient (NSG), which quantifies theratio of spatial probability gradients to temporal density changes, explicitlycapturing deviations from natural video dynamics. Leveraging pre-traineddiffusion models, we develop an NSG estimator through spatial gradientsapproximation and motion-aware temporal modeling without complex motiondecomposition while preserving physical constraints. Building on this, wepropose an NSG-based video detection method (NSG-VD) that computes the MaximumMean Discrepancy (MMD) between NSG features of the test and real videos as adetection metric. Last, we derive an upper bound of NSG feature distancesbetween real and generated videos, proving that generated videos exhibitamplified discrepancies due to distributional shifts. Extensive experimentsconfirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recalland 10.75% in F1-Score, validating the superior performance of NSG-VD. Thesource code is available at https://github.com/ZSHsh98/NSG-VD.</description>
      <author>example@mail.com (Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, Mingkui Tan)</author>
      <guid isPermaLink="false">2510.08073v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</title>
      <link>http://arxiv.org/abs/2510.07915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MARC是一种创新的令牌压缩方法，通过结合结构化检索和强化学习蒸馏技术，显著减少了视频处理中的计算负担，同时保持了较高的准确性，适用于资源受限环境下的实时视频理解应用。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)的快速发展为多模态模型奠定了基础。然而，视觉语言模型(VLMs)在从图像扩展到视频时仍面临高昂的计算成本，这主要是由于高帧率和长持续时间导致的。&lt;h4&gt;目的&lt;/h4&gt;解决视觉语言模型在视频处理中的高计算成本问题，通过令牌压缩技术实现高效的视频理解，同时保持较高的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了MARC(基于记忆增强强化学习的令牌压缩)方法，结合了结构化检索和基于强化学习的蒸馏技术。采用'先检索后压缩'策略，使用视觉记忆检索器(VMR)选择关键片段，并使用压缩组相对策略优化(C-GRPO)框架将推理能力从教师模型蒸馏到学生模型。&lt;h4&gt;主要发现&lt;/h4&gt;在六个视频基准测试中，MARC仅使用一帧的令牌就能接近基线准确性；视觉令牌减少了95%，GPU内存减少了72%，延迟减少了23.9%。&lt;h4&gt;结论&lt;/h4&gt;MARC在资源受限的环境(如视频问答、监控和自动驾驶)中具有高效实时视频理解的潜力。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)的快速发展为多模态模型奠定了基础。然而，视觉语言模型(VLMs)在从图像扩展到视频时仍面临高昂的计算成本，这主要是由于高帧率和长持续时间导致的。令牌压缩是一个有前景的解决方案，但大多数现有的无需训练方法会导致信息丢失和性能下降。为了克服这一问题，我们提出了MARC(基于记忆增强强化学习的令牌压缩)，该方法结合了结构化检索和基于强化学习的蒸馏技术。MARC采用'先检索后压缩'策略，使用视觉记忆检索器(VMR)选择关键片段，并使用压缩组相对策略优化(C-GRPO)框架将推理能力从教师模型蒸馏到学生模型。在六个视频基准测试上的实验表明，MARC仅使用一帧的令牌就能接近基线准确性——视觉令牌减少了95%，GPU内存减少了72%，延迟减少了23.9%。这证明了它在资源受限环境(如视频问答、监控和自动驾驶)中进行高效实时视频理解的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress of large language models (LLMs) has laid the foundationfor multimodal models. However, visual language models (VLMs) still face heavycomputational costs when extended from images to videos due to high frame ratesand long durations. Token compression is a promising solution, yet mostexisting training-free methods cause information loss and performancedegradation. To overcome this, we propose \textbf{Memory-AugmentedReinforcement Learning-based Token Compression (MARC)}, which integratesstructured retrieval and RL-based distillation. MARC adopts a\textit{retrieve-then-compress} strategy using a \textbf{Visual MemoryRetriever (VMR)} to select key clips and a \textbf{Compression Group RelativePolicy Optimization (C-GRPO)} framework to distil reasoning ability from ateacher to a student model. Experiments on six video benchmarks show that MARCachieves near-baseline accuracy using only one frame's tokens -- reducingvisual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by\textbf{23.9\%}. This demonstrates its potential for efficient, real-time videounderstanding in resource-constrained settings such as video QA, surveillance,and autonomous driving.</description>
      <author>example@mail.com (Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen)</author>
      <guid isPermaLink="false">2510.07915v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.07791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Geo-Temporal Reasoning benchmark (GTR-Bench)，一个用于评估视觉语言模型(VLMs)地理时空推理能力的新基准测试。研究评估了10多个流行VLMs的表现，发现当前模型存在三个主要缺陷，为时空智能研究提供了新方向。&lt;h4&gt;背景&lt;/h4&gt;现有时空基准测试主要关注两种推理：基于图像/视频的第一人称视角推理和基于地图的地理视角推理。这些基准测试无法评估VLMs在同时使用图像/视频和图形上下文时的地理时空智能，而这种能力对交通管理和应急响应等领域非常重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的局限性，提出一个能够全面评估VLMs地理时空智能的新基准测试，特别关注大规模摄像头网络中移动目标的地理时间推理。&lt;h4&gt;方法&lt;/h4&gt;作者设计了Geo-Temporal Reasoning benchmark (GTR-Bench)，这是一个针对大规模摄像头网络中移动目标的地理时间推理的挑战。该基准测试要求模型在地图和视频之间进行多视角切换、跨越多个非重叠视野的视频进行联合推理，以及对未观察到的时空区域进行推理。&lt;h4&gt;主要发现&lt;/h4&gt;1. 即使是表现最好的专有模型Gemini-2.5-Pro(34.9%)也远低于人类表现(78.61%)2. 当前VLMs在地理时间推理方面存在三个主要缺陷：   - 时空上下文利用不平衡   - 时间预测能力较弱   - 缺乏对齐地图数据与多视图视频输入的能力&lt;h4&gt;结论&lt;/h4&gt;GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会，有助于推动VLMs在地理时空推理方面的发展。基准测试和代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;最近，视觉语言模型(VLMs)的时空智能因其在自动驾驶、具身人工智能和通用人工智能领域的重要性而受到广泛关注。现有的时空基准测试主要关注基于图像/视频上下文的第一人称视角推理，或基于图形上下文(如地图)的地理视角推理，因此无法评估VLMs在同时使用图像/视频和图形上下文时的地理时空智能，这对交通管理和应急响应等领域非常重要。为解决这些差距，我们引入了Geo-Temporal Reasoning基准测试(GTR-Bench)，这是一个针对大规模摄像头网络中移动目标的地理时间推理的新挑战。GTR-Bench更具挑战性，因为它需要在地图和视频之间进行多视角切换、跨越多个非重叠视野的视频进行联合推理，以及对任何视频上下文都未观察到的时空区域进行推理。对10多个流行VLMs在GTR-Bench上的评估表明，即使是表现最好的专有模型Gemini-2.5-Pro(34.9%)也远低于人类表现(78.61%)。此外，我们在GTR-Bench上的综合分析揭示了当前模型在地理时间推理方面的三个主要缺陷：(1)VLMs的推理受到时空上下文不平衡利用的损害。(2)VLMs在时间预测方面能力较弱，导致在时间强调任务上的表现比在空间强调任务上更差。(3)VLMs缺乏理解或对齐地图数据与多视图视频输入的能力。我们相信GTR-Bench为时空智能的研究和应用提供了有价值的见解和开辟了新的机会。基准测试和代码将在https://github.com/X-Luffy/GTR-Bench发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently spatial-temporal intelligence of Visual-Language Models (VLMs) hasattracted much attention due to its importance for Autonomous Driving, EmbodiedAI and General Artificial Intelligence. Existing spatial-temporal benchmarksmainly focus on egocentric perspective reasoning with images/video context, orgeographic perspective reasoning with graphics context (eg. a map), thus failto assess VLMs' geographic spatial-temporal intelligence with both images/videoand graphics context, which is important for areas like traffic management andemergency response. To address the gaps, we introduce Geo-Temporal Reasoningbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning ofmoving targets in a large-scale camera network. GTR-Bench is more challengingas it requires multiple perspective switches between maps and videos, jointreasoning across multiple videos with non-overlapping fields of view, andinference over spatial-temporal regions that are unobserved by any videocontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate thateven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lagsbehind human performance (78.61%) on geo-temporal reasoning. Moreover, ourcomprehensive analysis on GTR-Bench reveals three primary deficiencies ofcurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired byan imbalanced utilization of spatial-temporal context. (2) VLMs are weak intemporal forecasting, which leads to worse performance on temporal-emphasizedtasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency tocomprehend or align the map data with multi-view video inputs. We believeGTR-Bench offers valuable insights and opens up new opportunities for researchand applications in spatial-temporal intelligence. Benchmark and code will bereleased at https://github.com/X-Luffy/GTR-Bench.</description>
      <author>example@mail.com (Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng)</author>
      <guid isPermaLink="false">2510.07791v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</title>
      <link>http://arxiv.org/abs/2510.05034v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 1st version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对Video-LMMs（视频大型多模态模型）后训练方法的首次全面综述，涵盖了监督微调、强化学习和测试时扩展三个基本支柱，提供了结构化的分类法和关键设计原则，旨在为研究人员和实践者提供推进Video-LMMs能力的统一框架。&lt;h4&gt;背景&lt;/h4&gt;视频理解是计算机视觉领域最具挑战性的前沿，需要模型能够处理复杂的时空关系、长期依赖和多模态证据。Video-LMMs的出现展示了在视频理解任务中的显著能力，但将这些模型从基础感知系统转变为复杂推理引擎的后训练阶段在文献中仍然分散。&lt;h4&gt;目的&lt;/h4&gt;提供对Video-LMMs后训练方法的首次全面检查，建立统一框架，帮助研究人员和实践者推进Video-LMMs的能力，同时确定关键挑战并提供必要的评估工具。&lt;h4&gt;方法&lt;/h4&gt;研究通过三个基本支柱分析后训练方法：1）监督微调（SFT）与思维链；2）可验证目标的强化学习（RL）；3）通过增强推理计算的测试时扩展（TTS）。研究还提出了结构化的分类法，明确这些技术在视频理解中的角色和相互关系。&lt;h4&gt;主要发现&lt;/h4&gt;1) Video-LMMs在视频理解任务中展示了显著能力；2) 后训练方法对将模型转变为复杂推理引擎至关重要；3) 三个基本支柱各有独特作用和挑战；4) 视频特定挑战包括时间定位、时空定位、长视频效率和多模态证据集成；5) 关键设计原则包括奖励设计、可扩展性和成本性能优化。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为研究人员和实践者提供了推进Video-LMMs能力的统一框架，确定了关键挑战和必要的评估工具，有助于促进该领域的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;视频理解代表了计算机视觉中最具挑战性的前沿，需要模型能够推理复杂的时空关系、长期依赖和多模态证据。最近出现的Video-LMMs将视觉编码器与强大的基于解码器的语言模型相结合，在视频理解任务中展示了显著的能力。然而，将这些模型从基础感知系统转变为复杂推理引擎的关键阶段——后训练，在文献中仍然分散。这篇综述首次对Video-LMMs的后训练方法进行了全面检查，涵盖了三个基本支柱：思维链监督微调、可验证目标的强化学习和通过增强推理计算的测试时扩展。我们提出了一个结构化的分类法，阐明了这些技术的作用、相互关系和视频特定适应，解决了时间定位、时空定位、长视频效率和多模态证据集成等独特挑战。通过对代表性方法的系统分析，我们综合了关键设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding represents the most challenging frontier in computervision, requiring models to reason about complex spatiotemporal relationships,long-term dependencies, and multimodal evidence. The recent emergence ofVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoderswith powerful decoder-based language models, has demonstrated remarkablecapabilities in video understanding tasks. However, the critical phase thattransforms these models from basic perception systems into sophisticatedreasoning engines, post-training, remains fragmented across the literature.This survey provides the first comprehensive examination of post-trainingmethodologies for Video-LMMs, encompassing three fundamental pillars:supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)from verifiable objectives, and test-time scaling (TTS) through enhancedinference computation. We present a structured taxonomy that clarifies theroles, interconnections, and video-specific adaptations of these techniques,addressing unique challenges such as temporal localization, spatiotemporalgrounding, long video efficiency, and multimodal evidence integration. Throughsystematic analysis of representative methods, we synthesize key designprinciples, insights, and evaluation protocols while identifying critical openchallenges in reward design, scalability, and cost-performance optimization. Wefurther curate essential benchmarks, datasets, and metrics to facilitaterigorous assessment of post-training effectiveness. This survey aims to provideresearchers and practitioners with a unified framework for advancing Video-LMMcapabilities. Additional resources and updates are maintained at:https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</description>
      <author>example@mail.com (Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu)</author>
      <guid isPermaLink="false">2510.05034v3</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation</title>
      <link>http://arxiv.org/abs/2510.08569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了ArenaBencher，一个自动基准测试演进框架，旨在解决大语言模型基准测试中的数据泄露问题。该框架能够在保持可比性的同时更新测试用例，通过推断测试用例核心能力、生成候选问答对、验证正确性和意图，以及从多个模型聚合反馈来选择暴露共同弱点的候选。该框架应用于多个领域，能够产生经过验证的、多样化的和公平的更新，揭示新的失败模式，增加难度并提高模型可分离性。&lt;h4&gt;背景&lt;/h4&gt;基准测试对于衡量大语言模型的能力和指导模型开发至关重要。然而，从预训练语料库中的广泛数据泄露损害了基准测试的有效性。模型可以匹配记忆内容而非展示真正的泛化能力，这 inflated 了分数，扭曲了跨模型比较，并错误地代表了进展。&lt;h4&gt;目的&lt;/h4&gt;作者提出ArenaBencher框架的目的是创建一个与模型无关的自动基准测试演进方法，能够在保持可比性的同时更新测试用例，以解决数据泄露问题，确保基准测试能够真正衡量模型的能力而非记忆内容。&lt;h4&gt;方法&lt;/h4&gt;ArenaBencher框架的工作流程包括：给定现有基准测试和一组要评估的多样化模型，推断每个测试用例的核心能力；生成保留原始目标的候选问答对；使用LLM作为法官验证正确性和意图；从多个模型聚合反馈以选择暴露共同弱点的候选。该过程使用上下文演示迭代运行，引导生成更具挑战性和诊断性的案例。&lt;h4&gt;主要发现&lt;/h4&gt;作者将ArenaBencher应用于数学问题解决、常识推理和安全领域，发现它能够产生经过验证的、多样化的和公平的更新，这些更新能够揭示新的失败模式，增加测试难度同时保持测试目标对齐，并提高模型可分离性。&lt;h4&gt;结论&lt;/h4&gt;ArenaBencher框架为基础模型的快速进展提供了一条可扩展的持续发展基准测试的路径，有助于确保基准测试的有效性和准确性，从而更准确地评估和比较大语言模型的能力。&lt;h4&gt;翻译&lt;/h4&gt;基准测试对于衡量大语言模型的能力和指导模型开发至关重要，然而从预训练语料库中的广泛数据泄露损害了其有效性。模型可以匹配记忆内容而非展示真正的泛化能力，这 inflated 了分数，扭曲了跨模型比较，并错误地代表了进展。我们引入ArenaBencher，一个与模型无关的自动基准测试演进框架，它在保持可比性的同时更新测试用例。给定一个现有基准测试和一组要评估的多样化模型，ArenaBencher推断每个测试用例的核心能力，生成保留原始目标的候选问答对，使用LLM作为法官验证正确性和意图，并从多个模型聚合反馈以选择暴露共同弱点的候选。该过程使用上下文演示迭代运行，这些演示引导生成更具挑战性和诊断性的案例。我们将ArenaBencher应用于数学问题解决、常识推理和安全领域，并表明它产生了经过验证的、多样化的和公平的更新，这些更新揭示了新的失败模式，增加了难度同时保持测试目标对齐，并提高了模型可分离性。该框架为基础模型的快速进展提供了一条可扩展的持续发展基准测试的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benchmarks are central to measuring the capabilities of large language modelsand guiding model development, yet widespread data leakage from pretrainingcorpora undermines their validity. Models can match memorized content ratherthan demonstrate true generalization, which inflates scores, distortscross-model comparisons, and misrepresents progress. We introduce ArenaBencher,a model-agnostic framework for automatic benchmark evolution that updates testcases while preserving comparability. Given an existing benchmark and a diversepool of models to be evaluated, ArenaBencher infers the core ability of eachtest case, generates candidate question-answer pairs that preserve the originalobjective, verifies correctness and intent with an LLM as a judge, andaggregates feedback from multiple models to select candidates that exposeshared weaknesses. The process runs iteratively with in-context demonstrationsthat steer generation toward more challenging and diagnostic cases. We applyArenaBencher to math problem solving, commonsense reasoning, and safety domainsand show that it produces verified, diverse, and fair updates that uncover newfailure modes, increase difficulty while preserving test objective alignment,and improve model separability. The framework provides a scalable path tocontinuously evolve benchmarks in step with the rapid progress of foundationmodels.</description>
      <author>example@mail.com (Qin Liu, Jacob Dineen, Yuxi Huang, Sheng Zhang, Hoifung Poon, Ben Zhou, Muhao Chen)</author>
      <guid isPermaLink="false">2510.08569v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</title>
      <link>http://arxiv.org/abs/2510.08551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ARTDECO是一种统一框架，结合了前馈模型的效率和基于SLAM管道的可靠性，用于从单目图像序列进行实时3D重建。&lt;h4&gt;背景&lt;/h4&gt;从单目图像序列进行实时3D重建是计算机视觉中的一个长期挑战，对real-to-sim、AR/VR和机器人等应用至关重要。现有方法面临权衡：针对每个场景的优化计算成本高，而前馈基础模型实时性好但准确性和鲁棒性不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，结合前馈模型的效率和基于SLAM管道的可靠性，实现高质量、高效率的实时3D重建。&lt;h4&gt;方法&lt;/h4&gt;提出ARTDECO框架，使用3D基础模型进行姿态估计和点预测，结合高斯解码器将多尺度特征转换为结构化3D高斯，并设计层次化高斯表示和基于细节层次的渲染策略，以提高渲染保真度同时减少冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在八个多样化的室内和室外基准测试中，ARTDECO提供了与SLAM相当的交互性能、与前馈系统相似的鲁棒性，以及接近每个场景优化质量的重建质量。&lt;h4&gt;结论&lt;/h4&gt;ARTDECO为实时数字化真实世界环境提供了实用途径，既能实现精确几何又能保持高视觉保真度。&lt;h4&gt;翻译&lt;/h4&gt;从单目图像序列进行实时3D重建是计算机视觉中的一个长期挑战，对real-to-sim、AR/VR和机器人等应用至关重要。现有方法面临一个重大权衡：针对每个场景的优化能产生高保真度，但计算成本高，而前馈基础模型可以实现实时推理，但在准确性和鲁棒性方面存在问题。在这项工作中，我们提出了ARTDECO，这是一个统一框架，结合了前馈模型的效率和基于SLAM管道的可靠性。ARTDECO使用3D基础模型进行姿态估计和点预测，结合高斯解码器将多尺度特征转换为结构化3D高斯。为了在大规模下保持保真度和效率，我们设计了一种层次化高斯表示和基于细节层次的渲染策略，这提高了渲染保真度同时减少了冗余。在八个多样化的室内和室外基准测试中，ARTDECO提供了与SLAM相当的交互性能、与前馈系统相似的鲁棒性，以及接近每个场景优化质量的重建质量，为实时数字化具有精确几何和高视觉保真度的真实世界环境提供了实用途径。在我们的项目页面上探索更多演示：https://city-super.github.io/artdeco/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单目图像序列进行即时3D重建的挑战。这个问题在现实中非常重要，因为它关系到AR/VR应用、机器人导航、实时模拟和数字孪生等技术领域，这些领域都需要高效且高质量的3D场景重建能力。现有方法要么计算成本高但质量好，要么速度快但质量差，难以兼顾效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点：基于场景的优化方法质量高但计算昂贵，前馈模型速度快但准确性和鲁棒性不足。他们借鉴了SLAM管道的可靠性和前馈模型的效率，结合两者优势。具体来说，他们使用了3D基础模型进行姿态估计和点预测，借鉴了高斯溅射技术，并设计了分层高斯表示和基于细节级别的渲染策略，这些都是在现有工作基础上进行的创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; ARTDECO的核心思想是将前馈模型的效率与基于SLAM的管道的可靠性相结合，实现高效且高保真的即时3D重建。整体流程分为三个模块：1）前端模块：估计相对姿态并将帧分类为普通帧、映射帧或关键帧；2）后端模块：通过回环检测和全局束调整优化关键帧姿态；3）映射模块：从帧初始化3D高斯，并增量优化它们。特别地，系统使用分层半隐式高斯结构和基于细节级别的densification策略，平衡了重建质量和渲染效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ARTDECO的关键创新点包括：1）统一框架：将定位、重建和渲染整合到一个管道中，能在各种环境中稳健运行；2）模块化组件：将前馈基础模型用于姿态估计、回环检测和密集点预测；3）分层半隐式高斯表示：具有基于细节级别的densification策略，实现保真度和效率的平衡；4）实验验证：在多样化的室内和室外基准测试中，实现了SLAM级别的效率、前馈鲁棒性和接近场景优化质量的结果。相比之前的工作，ARTDECO不需要在每场景优化和前馈模型之间做权衡，而是结合了两者的优点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ARTDECO通过结合前馈模型的效率和基于SLAM的管道的可靠性，实现了从单目图像序列进行高效、高保真即时3D重建的创新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; On-the-fly 3D reconstruction from monocular image sequences is along-standing challenge in computer vision, critical for applications such asreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:per-scene optimization yields high fidelity but is computationally expensive,whereas feed-forward foundation models enable real-time inference but strugglewith accuracy and robustness. In this work, we propose ARTDECO, a unifiedframework that combines the efficiency of feed-forward models with thereliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for poseestimation and point prediction, coupled with a Gaussian decoder thattransforms multi-scale features into structured 3D Gaussians. To sustain bothfidelity and efficiency at scale, we design a hierarchical Gaussianrepresentation with a LoD-aware rendering strategy, which improves renderingfidelity while reducing redundancy. Experiments on eight diverse indoor andoutdoor benchmarks show that ARTDECO delivers interactive performancecomparable to SLAM, robustness similar to feed-forward systems, andreconstruction quality close to per-scene optimization, providing a practicalpath toward on-the-fly digitization of real-world environments with bothaccurate geometry and high visual fidelity. Explore more demos on our projectpage: https://city-super.github.io/artdeco/.</description>
      <author>example@mail.com (Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2510.08551v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Series-Symbol Data Generation for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.08445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  63 pages, NeurIPS 2025 accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于复杂动态系统理论的时间序列分析基础模型SymTime，通过系列-符号数据生成机制克服数据稀缺和不平衡问题，在五个主要TSA任务上展现出与真实数据预训练模型相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列分析基础模型已引起广泛关注，但训练数据稀缺和不平衡问题仍然阻碍其发展。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够无限制创建高质量时间序列数据及其对应符号表达式的数据生成机制，并开发利用这些数据对增强时间序列表示的基础模型。&lt;h4&gt;方法&lt;/h4&gt;受复杂动态系统理论启发，设计了系列-符号数据生成机制，并开发了名为SymTime的预训练基础模型，利用具有强相关性的系列-符号数据对增强时间序列表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;SymTime在五个主要时间序列分析任务上展现出有竞争力的性能，当使用下游任务微调时，与在真实数据集上预训练的基础模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;系列-符号数据生成和预训练机制在克服数据稀缺性和提高任务性能方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;时间序列分析的基础模型已引起广泛关注。然而，训练数据稀缺和不平衡等挑战继续阻碍其发展。受复杂动态系统理论的启发，我们设计了一系列-符号数据生成机制，能够无限制地创建高质量的时间序列数据及其对应的符号表达式。为了利用具有强相关性的系列-符号数据对，我们开发了SymTime，这是一个使用符号信息增强时间序列表示的预训练基础模型。当使用下游任务微调时，SymTime在五个主要的TSA任务上展现出有竞争力的性能，与在真实数据集上预训练的基础模型相媲美。这种方法强调了系列-符号数据生成和预训练机制在克服数据稀缺性和提高任务性能方面的潜力。代码可在https://github.com/wwhenxuan/SymTime获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for time series analysis (TSA) have attracted significantattention. However, challenges such as training data scarcity and imbalancecontinue to hinder their development. Inspired by complex dynamic systemtheories, we design a series-symbol data generation mechanism, enabling theunrestricted creation of high-quality time series data paired withcorresponding symbolic expressions. To leverage series-symbol data pairs withstrong correlations, we develop \texttt{SymTime}, a pre-trained foundationmodel for enhancing time series representation using symbolic information.\texttt{SymTime} demonstrates competitive performance across five major TSAtasks when fine-tunes with downstream tasks, rivaling foundation modelspre-trained on real-world datasets. This approach underscores the potential ofseries-symbol data generation and pretraining mechanisms in overcoming datascarcity and enhancing task performance. The code is available athttps://github.com/wwhenxuan/SymTime.</description>
      <author>example@mail.com (Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang)</author>
      <guid isPermaLink="false">2510.08445v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2510.08396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 accepted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlyLoRA是一种基于果蝇嗅觉电路启发的隐式MoE-based LoRA变体，通过秩级专家激活和隐式路由器设计解决了LoRA的参数干扰问题，并在多任务场景下有效缓解了任务间干扰。&lt;h4&gt;背景&lt;/h4&gt;Low-Rank Adaptation (LoRA)是一种广泛使用的参数高效微调方法，但存在参数干扰问题导致性能不理想。虽然Mixture-of-Experts (MoE)基础的LoRA变体在缓解单任务指令调优中的任务内相关性方面有前景，但引入了额外的路由器参数，并在多任务模型合并中仍然无效。&lt;h4&gt;目的&lt;/h4&gt;解决LoRA的参数干扰问题，缓解任务内相关性和任务间干扰，同时提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出FlyLoRA，包含两个关键组件：(1)在上投影矩阵中引入秩级专家激活；(2)设计隐式路由器统一专家路由和下投影，用冻结的稀疏随机投影矩阵替代传统的密集可训练版本。这一设计消除了对显式路由器的需求，并利用随机矩阵的正交特性内在缓解任务间干扰。&lt;h4&gt;主要发现&lt;/h4&gt;FlyLoRA解决了任务内解相关性和计算效率之间的权衡问题。在四个领域(通用知识理解、科学问答、数学推理和代码生成)的广泛实验中，相对于现有方法实现了持续的性能改进。&lt;h4&gt;结论&lt;/h4&gt;FlyLoRA不仅带来了经验上的改进，还展示了生物结构如何能够启发AI技术的创新。&lt;h4&gt;翻译&lt;/h4&gt;低秩适应(LoRA)是基础模型的广泛使用的参数高效微调方法，但它受到参数干扰的影响，导致次优性能。尽管基于专家混合(MoE)的LoRA变体在缓解单任务指令调优中的任务内相关性方面显示出前景，但它们引入了额外的路由器参数，并且在出现任务间干扰的多任务模型合并中仍然无效。受果蝇嗅觉电路的启发，我们提出了FlyLoRA，这是一种隐式MoE-based LoRA变体，它引入：(1)在上投影矩阵中进行秩级专家激活，以及(2)一个隐式路由器，统一专家路由和下投影，其中冻结的稀疏随机投影矩阵替代了传统的密集可训练版本。这种设计通过消除对显式路由器的需求，解决了任务内解相关性和计算效率之间的权衡，同时由于随机矩阵的正交特性，内在地缓解了任务间干扰。在四个领域(通用知识理解、科学问答、数学推理和代码生成)的广泛实验中，展示了相对于现有方法的持续性能改进。除了经验上的收益，FlyLoRA还展示了生物结构如何能够启发AI技术的创新。代码可在https://github.com/gfyddha/FlyLoRA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuningmethod for foundation models, but it suffers from parameter interference,resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-basedLoRA variants show promise in mitigating intra-task correlations in single-taskinstruction tuning, they introduce additional router parameters and remainineffective in multi-task model merging where inter-task interference arises.Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicitMoE-based LoRA variant that introduces: (1) rank-wise expert activation in theup-projection matrix, and (2) an implicit router that unifies expert routingand down-projection, where a frozen sparse random projection matrix replacesthe traditional dense trainable version. This design resolves the trade-offbetween intra-task decorrelation and computational efficiency by eliminatingthe need for an explicit router, while inherently mitigating inter-taskinterference due to the orthogonality property of random matrices. Extensiveexperiments across four domains -- general knowledge understanding, scientificquestion answering, mathematical reasoning, and code generation -- demonstrateconsistent performance improvements over existing methods. Beyond empiricalgains, FlyLoRA highlights how biological structures can inspire innovations inAI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.</description>
      <author>example@mail.com (Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji)</author>
      <guid isPermaLink="false">2510.08396v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution</title>
      <link>http://arxiv.org/abs/2510.08143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniMMVSR是一种创新的视频超分辨率框架，能够整合文本、图像和视频等多种模态条件，显著提升了生成视频的质量和与多模态条件的符合度，并实现了4K视频的多模态引导生成。&lt;h4&gt;背景&lt;/h4&gt;级联视频超分辨率技术是一种有前途的技术，可以解耦使用大型基础模型生成高分辨率视频的计算负担。但现有研究主要局限于文本到视频任务，无法利用文本以外的生成条件，这对于确保多模态视频生成的保真度至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出UniMMVSR，第一个统一生成视频超分辨率框架，可以整合混合模态条件，包括文本、图像和视频。&lt;h4&gt;方法&lt;/h4&gt;在潜在视频扩散模型中，对条件注入策略、训练方案和数据混合技术进行了全面探索。设计不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型，考虑到它们与目标视频的不同相关性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，UniMMVSR显著优于现有方法，生成的视频具有更优质的细节和更高的多模态条件符合度。验证了将UniMMVSR与基础模型结合以实现4K视频的多模态引导生成的可行性。&lt;h4&gt;结论&lt;/h4&gt;UniMMVSR代表了视频超分辨率技术的重要进步，通过整合多模态条件，实现了更高质量的视频生成，并能够生成4K视频。&lt;h4&gt;翻译&lt;/h4&gt;级联视频超分辨率已成为一种有前途的技术，用于解耦使用大型基础模型生成高分辨率视频的计算负担。然而，现有研究大多局限于文本到视频任务，无法利用文本以外的生成条件，而这些条件对于确保多模态视频生成的保真度至关重要。我们通过提出UniMMVSR来解决这个问题，这是第一个统一生成视频超分辨率框架，可以整合混合模态条件，包括文本、图像和视频。我们在潜在视频扩散模型中对条件注入策略、训练方案和数据混合技术进行了全面探索。一个关键挑战是设计不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型，考虑到它们与目标视频的不同相关性。我们的实验证明，UniMMVSR显著优于现有方法，生成的视频具有更优质的细节和更高的多模态条件符合度。我们还验证了将UniMMVSR与基础模型结合以实现多模态引导的4K视频生成的可行性，这是现有技术无法实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cascaded video super-resolution has emerged as a promising technique fordecoupling the computational burden associated with generating high-resolutionvideos using large foundation models. Existing studies, however, are largelyconfined to text-to-video tasks and fail to leverage additional generativeconditions beyond text, which are crucial for ensuring fidelity in multi-modalvideo generation. We address this limitation by presenting UniMMVSR, the firstunified generative video super-resolution framework to incorporate hybrid-modalconditions, including text, images, and videos. We conduct a comprehensiveexploration of condition injection strategies, training schemes, and datamixture techniques within a latent video diffusion model. A key challenge wasdesigning distinct data construction and condition utilization methods toenable the model to precisely utilize all condition types, given their variedcorrelations with the target video. Our experiments demonstrate that UniMMVSRsignificantly outperforms existing methods, producing videos with superiordetail and a higher degree of conformity to multi-modal conditions. We alsovalidate the feasibility of combining UniMMVSR with a base model to achievemulti-modal guided generation of 4K video, a feat previously unattainable withexisting techniques.</description>
      <author>example@mail.com (Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji)</author>
      <guid isPermaLink="false">2510.08143v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters</title>
      <link>http://arxiv.org/abs/2510.08059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Subject-Conditioned Layer的自适应层，用于解决脑电图解码中个体间分布差异对基础模型开发的阻碍问题。该层通过将权重分解为共享通用组件和受试者特定修正，使模型能够同时保持通用性和个性化适应性，实验证明其性能优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;个体间分布差异是开发脑电图解码基础模型的重要障碍，不同受试者的脑电信号存在显著差异，这限制了模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理个体间分布差异的自适应层，使基础模型能够同时具备通用知识和个性化适应能力，提高跨受试者脑电解码的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Subject-Conditioned Layer，一种可替代标准线性或卷积层的自适应层。该层通过将权重分解为共享的、不受试者影响的通用组件和轻量级的、低秩的、针对每个受试者的独特修正，实现通用知识与个性化适应的明确分离。&lt;h4&gt;主要发现&lt;/h4&gt;采用Subject-Conditioned Layer的模型在性能上超过了仅使用共享权重的模型(不受试者影响的模型)和单独训练的受试者特定模型的平均值，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;Subject-Conditioned Layer为构建有效的跨受试者脑电图基础模型提供了一种实用且可扩展的解决方案，能够使模型在保持通用性的同时具备个性化适应能力。&lt;h4&gt;翻译&lt;/h4&gt;受试者特定的分布差异是开发脑电图解码基础模型的重要障碍。为解决这一问题，我们提出了受试者条件化层，这是一种自适应层，可作为任何神经网络架构中标准线性或卷积层的替代品。我们的层通过将权重分解为共享的、不受试者影响的组件和轻量级的、低秩的、每个受试者独特的修正，来捕获受试者特定的变异性。这种将通用知识与个性化适应明确分离的方式，使现有模型能够对受试者差异具有鲁棒性。经验上，配备我们层的模型在性能上优于仅使用共享权重的模型(不受试者影响的模型)和单独训练的受试者特定模型的平均值。因此，受试者条件化层为构建有效的跨受试者脑电图基础模型提供了一种实用且可扩展的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Subject-specific distribution shifts represent an important obstacle to thedevelopment of foundation models for EEG decoding. To address this, we proposeSubject-Conditioned Layer,, an adaptive layer designed as a drop-in replacementfor standard linear or convolutional layers in any neural network architecture.Our layer captures subject-specific variability by decomposing its weights intoa shared, subject-invariant component and a lightweight, low-rank correctionunique to each subject. This explicit separation of general knowledge frompersonalized adaptation allows existing models to become robust to subjectshifts. Empirically, models equipped with our layer outperform both ashared-weight-only model (subject-agnostic model) and the average ofindividually trained subject-specific models. Consequently, theSubject-Conditioned Layer, offers a practical and scalable path towardsbuilding effective cross-subject foundation models for EEG.</description>
      <author>example@mail.com (Timon Klein, Piotr Minakowski, Sebastian Sager)</author>
      <guid isPermaLink="false">2510.08059v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</title>
      <link>http://arxiv.org/abs/2510.07940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://ttom-t2v.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Test-Time Optimization and Memorization (TTOM)框架，解决了Video Foundation Models (VFMs)在组合场景中的局限性，通过在推理过程中优化参数实现更好的文本-图像对齐。&lt;h4&gt;背景&lt;/h4&gt;Video Foundation Models (VFMs)在视觉生成方面表现出色，但在处理组合场景（如运动、计数和空间关系）时存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练的框架，在推理过程中将VFM输出与时空布局对齐，提高文本-图像对齐效果，并支持组合视频生成。&lt;h4&gt;方法&lt;/h4&gt;提出Test-Time Optimization and Memorization (TTOM)框架，集成并优化由布局-注意力目标引导的新参数；在流式设置下制定视频生成；使用参数化内存机制维护历史优化上下文，支持插入、读取、更新和删除等操作。&lt;h4&gt;主要发现&lt;/h4&gt;TTOM能够解耦组合世界知识，显示出强大的可转移性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TTOM被证明是一个有效、实用、可扩展且高效的框架，可以实现跨模态对齐和即时的组合视频生成。&lt;h4&gt;翻译&lt;/h4&gt;视频基础模型(VFMs)展现出卓越的视觉生成性能，但在组合场景（如运动、计数和空间关系）中表现不佳。在本工作中，我们提出了测试时优化和记忆(TTOM)，这是一个无需训练的框架，在推理过程中将VFM输出与时空布局对齐，以实现更好的文本-图像对齐。与现有工作中直接干预潜在表示或每个样本的注意力机制不同，我们集成并优化由通用布局-注意力目标引导的新参数。此外，我们在流式设置下制定视频生成，并使用参数化内存机制维护历史优化上下文，支持插入、读取、更新和删除等灵活操作。值得注意的是，我们发现TTOM能够解耦组合世界知识，显示出强大的可转移性和泛化能力。在T2V-CompBench和Vbench基准测试上的实验结果确立了TTOM作为一个有效、实用、可扩展且高效的框架，用于实现组合视频生成的跨模态对齐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Foundation Models (VFMs) exhibit remarkable visual generationperformance, but struggle in compositional scenarios (e.g., motion, numeracy,and spatial relation). In this work, we introduce Test-Time Optimization andMemorization (TTOM), a training-free framework that aligns VFM outputs withspatiotemporal layouts during inference for better text-image alignment. Ratherthan direct intervention to latents or attention per-sample in existing work,we integrate and optimize new parameters guided by a general layout-attentionobjective. Furthermore, we formulate video generation within a streamingsetting, and maintain historical optimization contexts with a parametric memorymechanism that supports flexible operations, such as insert, read, update, anddelete. Notably, we found that TTOM disentangles compositional world knowledge,showing powerful transferability and generalization. Experimental results onthe T2V-CompBench and Vbench benchmarks establish TTOM as an effective,practical, scalable, and efficient framework to achieve cross-modal alignmentfor compositional video generation on the fly.</description>
      <author>example@mail.com (Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.07940v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views</title>
      <link>http://arxiv.org/abs/2510.07839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AlignGS是一种新颖的框架，通过协同优化几何和语义，从稀疏视图中生成更连贯、更完整的3D模型。&lt;h4&gt;背景&lt;/h4&gt;室内场景语义丰富的3D模型需求快速增长，由增强现实、虚拟现实和机器人应用驱动，但从稀疏视图创建这些模型具有几何歧义性。&lt;h4&gt;目的&lt;/h4&gt;将语义理解作为主动引导力量，用于鲁棒的稀疏视图3D重建。&lt;h4&gt;方法&lt;/h4&gt;AlignGS框架从2D基础模型中提取丰富先验，通过深度一致性和多方面法线正则化等语义到几何的引导机制直接正则化3D表示。&lt;h4&gt;主要发现&lt;/h4&gt;在新视图合成方面取得最先进结果，产生具有更高几何精度的重建，证明利用语义先验作为几何正则化器可从有限输入视图产生更连贯、更完整的3D模型。&lt;h4&gt;结论&lt;/h4&gt;语义理解应作为主动引导力量，而非被动特征，用于3D重建过程。&lt;h4&gt;翻译&lt;/h4&gt;对室内场景语义丰富的3D模型需求正快速增长，这由增强现实、虚拟现实和机器人应用所驱动。然而，从稀疏视图创建这些模型仍然是一个挑战，因为存在几何歧义性。现有方法通常将语义作为被动特征，应用于已经形成且可能有缺陷的几何体上。我们认为，对于鲁棒的稀疏视图重建，语义理解应该成为主动的引导力量。本文介绍了AlignGS，这是一个新颖的框架，通过几何和语义的协同端到端优化实现了这一愿景。我们的方法从2D基础模型中提取丰富的先验知识，并通过一套新颖的语义到几何的引导机制（包括深度一致性和多方面法线正则化）直接正则化3D表示。在标准基准上的广泛评估表明，我们的方法在新视图合成方面取得了最先进的结果，并产生了具有更高几何精度的重建结果。这些结果验证了，利用语义先验作为几何正则化器可以从有限的输入视图中产生更连贯、更完整的3D模型。我们的代码可在https://github.com/MediaX-SJTU/AlignGS获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从稀疏视图进行鲁棒的室内场景3D重建问题。这个问题很重要，因为室内场景通常因复杂布局和频繁遮挡导致可用视角稀疏，而现有的3D重建方法在这种情况下难以产生准确完整的几何结构。同时，对语义丰富的3D室内模型的需求正在快速增长，由增强现实、虚拟现实和机器人等应用驱动，从稀疏视图创建这些模型对现实应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为传统方法将语义作为被动特征而非主动引导几何优化的力量，因此提出语义理解应该成为主动引导力量。作者借鉴了现有的2D基础模型(如DINOv2和Mask2Former)提取语义先验，同时采用3D高斯泼溅作为基础表示方法。方法设计包括使用VGGT进行无SfM初始化，为每个高斯添加语义向量，设计语义到几何的引导机制，以及采用双重监督策略转移2D语义知识到3D表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义理解作为主动引导几何优化的力量，通过2D先验知识直接约束3D表示，解决稀疏视图下的几何歧义。整体流程分为四步：1)初始化阶段：使用VGGT生成初始点云和相机姿态，初始化3D高斯模型；2)语义知识转移：通过双重监督策略将2D语义知识转移到3D表示；3)几何优化：应用深度一致性和多方面法线一致性约束；4)联合优化：同时优化所有几何和语义属性，协同改进场景的几何结构和语义理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端稀疏视图室内重建框架，利用2D先验直接正则化3D几何；2)语义引导的几何约束机制，包括深度一致性和边界感知法线一致性；3)鲁棒的无SfM初始化方法。相比之前工作，AlignGS将语义从被动特征转变为主动引导力量，采用端到端联合优化而非两阶段方法，使用无SfM初始化提高稀疏视图下的鲁棒性，并引入基于语义的高级先验作为几何正则化器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AlignGS通过将语义理解作为主动引导几何优化的力量，实现了从稀疏视图进行鲁棒室内重建，显著提高了3D重建的几何准确性和语义一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The demand for semantically rich 3D models of indoor scenes is rapidlygrowing, driven by applications in augmented reality, virtual reality, androbotics. However, creating them from sparse views remains a challenge due togeometric ambiguity. Existing methods often treat semantics as a passivefeature painted on an already-formed, and potentially flawed, geometry. Weposit that for robust sparse-view reconstruction, semantic understandinginstead be an active, guiding force. This paper introduces AlignGS, a novelframework that actualizes this vision by pioneering a synergistic, end-to-endoptimization of geometry and semantics. Our method distills rich priors from 2Dfoundation models and uses them to directly regularize the 3D representationthrough a set of novel semantic-to-geometry guidance mechanisms, includingdepth consistency and multi-faceted normal regularization. Extensiveevaluations on standard benchmarks demonstrate that our approach achievesstate-of-the-art results in novel view synthesis and produces reconstructionswith superior geometric accuracy. The results validate that leveraging semanticpriors as a geometric regularizer leads to more coherent and complete 3D modelsfrom limited input views. Our code is avaliable athttps://github.com/MediaX-SJTU/AlignGS .</description>
      <author>example@mail.com (Yijie Gao, Houqiang Zhong, Tianchi Zhu, Zhengxue Cheng, Qiang Hu, Li Song)</author>
      <guid isPermaLink="false">2510.07839v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling</title>
      <link>http://arxiv.org/abs/2510.07755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedBook，一种统一的联邦图基础码本，用于在联邦学习环境下构建强大的图基础模型。该方法通过两阶段过程（领域内协作和跨领域集成）系统性地聚合客户端的局部码本，既增强了领域内一致性，又保留了跨领域多样性。&lt;h4&gt;背景&lt;/h4&gt;Foundation models在语言和视觉领域表现出卓越的跨领域泛化能力，启发了图基础模型(GFMs)的发展。然而，现有GFMs通常假设可集中访问多领域图，这在实际应用中因隐私和机构限制而不可行。联邦图基础模型(FedGFMs)虽解决了这一问题，但有效构建全局码本仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够在联邦学习环境下构建强大的全局图基础码本，实现领域内一致性同时保持跨领域多样性，从而突破隐私和机构限制下的多领域图学习挑战。&lt;h4&gt;方法&lt;/h4&gt;提出FedBook，一种统一的联邦图基础码本，在服务器端联邦预训练期间系统性地聚合客户端局部码本。采用两阶段过程：(1)领域内协作：通过参考客户端间语义可靠的高频令牌改进低频令牌，增强领域特定一致性；(2)跨领域集成：根据码本语义独特性对客户端贡献加权，保留跨领域多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在8个跨多个领域和任务的基准测试上，FedBook持续优于21个基线方法，包括孤立监督学习、FL/FGL、集中式GFMs的联邦适配以及FedGFM技术，证明了其在联邦图学习中的优越性能。&lt;h4&gt;结论&lt;/h4&gt;FedBook有效地解决了联邦环境下构建强大图基础模型的挑战，通过系统性地聚合客户端码本，在保持跨领域多样性的同时增强了领域内一致性，为隐私保护下的多领域图学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在语言和视觉领域展示了卓越的跨领域泛化能力，启发了图基础模型(GFMs)的发展。然而，现有GFMs通常假设可以集中访问多领域图，这往往因隐私和机构限制而不可行。联邦图基础模型(FedGFMs)解决了这一限制，但其有效性根本上取决于构建一个强大的全局码本，该码本通过整合每个领域内相互强化的语义实现领域内一致性，同时通过保留跨领域的异构知识维持领域间多样性。为此，我们提出FedBook，一个统一的联邦图基础码本，在服务器端联邦预训练期间系统性地聚合客户端的局部码本。FedBook遵循两阶段过程：(1)领域内协作，通过参考客户端间语义上更可靠的高频令牌来改进低频令牌，增强领域特定一致性；(2)跨领域集成，在全局GFM聚合期间，根据码本的语义独特性对客户端贡献进行加权，从而保留跨领域多样性。在8个跨多个领域和任务的基准测试上的广泛实验表明，FedBook持续优于21个基线，包括孤立的监督学习、FL/FGL、集中式GFMs的联邦适配和FedGFM技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have shown remarkable cross-domain generalization inlanguage and vision, inspiring the development of graph foundation models(GFMs). However, existing GFMs typically assume centralized access tomulti-domain graphs, which is often infeasible due to privacy and institutionalconstraints. Federated Graph Foundation Models (FedGFMs) address thislimitation, but their effectiveness fundamentally hinges on constructing arobust global codebook that achieves intra-domain coherence by consolidatingmutually reinforcing semantics within each domain, while also maintaininginter-domain diversity by retaining heterogeneous knowledge across domains. Tothis end, we propose FedBook, a unified federated graph foundation codebookthat systematically aggregates clients' local codebooks during server-sidefederated pre-training. FedBook follows a two-phase process: (1) Intra-domainCollaboration, where low-frequency tokens are refined by referencing moresemantically reliable high-frequency tokens across clients to enhancedomain-specific coherence; and (2) Inter-domain Integration, where clientcontributions are weighted by the semantic distinctiveness of their codebooksduring the aggregation of the global GFM, thereby preserving cross-domaindiversity. Extensive experiments on 8 benchmarks across multiple domains andtasks demonstrate that FedBook consistently outperforms 21 baselines, includingisolated supervised learning, FL/FGL, federated adaptations of centralizedGFMs, and FedGFM techniques.</description>
      <author>example@mail.com (Zhengyu Wu, Yinlin Zhu, Xunkai Li, Ziang Qiu, Rong-Hua Li, Guoren Wang, Chenghu Zhou)</author>
      <guid isPermaLink="false">2510.07755v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Foundation Model for Cosmological Simulation Data</title>
      <link>http://arxiv.org/abs/2510.07684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个用于天体物理学星系数据的多模态基础模型，能够在模拟和观测数据之间进行映射。模型使用仅编码器transformer架构，支持从部分输入查询星系属性，并在红移估计和恒星质量推断方面表现出显著改进。&lt;h4&gt;背景&lt;/h4&gt;天体物理学研究中需要有效处理来自模拟和观测的星系数据，并建立两者之间的联系。现有方法可能难以有效处理多模态数据并支持多种任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理天体物理学星系数据的多模态基础模型，实现模拟和观测星系特征之间的映射，支持多任务训练，并能够从部分输入中查询任意星系属性。&lt;h4&gt;方法&lt;/h4&gt;使用仅编码器transformer架构，能够处理标量量（如红移、星系质量）和矢量（如恒星形成历史、光谱）数据。采用动态掩码策略，从部分输入中查询星系属性。使用来自千兆秒差距级宇宙模拟的185,000个模拟星系进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;当结合LSST和SPHEREx测光数据时，红移估计比仅使用LSST测光数据提高了50%；当结合晚期恒星形成历史与LSST测光数据时，恒星质量推断比结合早期恒星形成历史与LSST测光数据提高了63%。模型在多模态任务上表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法为连接模拟和观测提供了统一框架，为未来集成更高维度和结构化数据（如图像、合并树和3D场）奠定了基础，推动了可推广天体物理学基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于天体物理学星系数据的多模态基础模型，旨在映射基于模拟和观测的星系特征。我们的仅编码器transformer能够灵活地吸收标量量（如红移、星系质量）和矢量（如恒星形成历史、光谱），支持包括模态内重建和跨模态预测的多任务训练。通过动态掩码策略，模型可以从部分输入中查询任意星系属性——包括根据红移和质量预测光谱，或根据宽带星等估计测光红移——同时恢复模态内的缺失部分。模型在来自千兆秒差距级宇宙模拟的185,000个模拟星系上进行了训练，当结合LSST和SPHEREx测光数据时，红移估计比仅使用LSST测光数据提高了50%，当结合晚期恒星形成历史与LSST测光数据时，恒星质量推断比结合早期恒星形成历史与LSST测光数据提高了63%。该模型在多模态任务上表现出强大的泛化能力，为未来集成更高维度和结构化数据（如图像、合并树和3D场）奠定了基础。这种方法为连接模拟和观测提供了统一框架，推动了可推广天体物理学基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a multi-modal foundation model for astrophysical galaxy data,designed to map between simulation- and observation-based galactic features.Our encoder-only transformer flexibly ingests scalar quantities (e.g.,redshifts, galaxy masses) and vectors (e.g., star formation histories,spectra), supporting multi-task training that includes within-modalityreconstruction and cross-modality prediction. With a dynamic masking strategy,the model can query arbitrary galaxy properties from partial inputs --including predicting spectra from redshift and mass, or estimating photometricredshifts from broadband magnitudes -- while also recovering missing segmentswithin a modality. Trained on 185,000 simulated galaxies from agigaparsec-scale Cosmology simulation, the model yields a 50% improvement inredshift estimation when combining LSST and SPHEREx photometry over LSSTphotometry alone, and a 63% improvement in stellar mass inference whencombining late-time SFH with LSST photometry over early-time SFH with LSSTphotometry. The model demonstrates strong generalization across multi-modaltasks and lays the groundwork for future integration of higher-dimensional andstructured data such as images, merger trees, and 3D fields. This approachprovides a unified framework for connecting simulations and observations,advancing the development of generalizable astrophysical foundation models.</description>
      <author>example@mail.com (Bin Xia, Nesar Ramachandra, Azton I. Wells, Salman Habib, John Wise)</author>
      <guid isPermaLink="false">2510.07684v1</guid>
      <pubDate>Fri, 10 Oct 2025 15:04:46 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport</title>
      <link>http://arxiv.org/abs/2510.05751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于集合的管道，使用图神经网络模拟器来估计大气传输足迹、温室气体摩尔分数测量及其不确定性，实现了比传统方法快约1000倍的加速，同时能够量化预测的不确定性。&lt;h4&gt;背景&lt;/h4&gt;监测温室气体排放和评估国家清单需要高效、可扩展且可靠的推理方法。自上而下的方法结合卫星观测的新进展为评估大陆和全球尺度的排放提供了新机会，但传输模型仍是不确定性的主要来源，因其计算成本高且不确定性难以量化。&lt;h4&gt;目的&lt;/h4&gt;利用人工智能加速传输模拟并量化相关不确定性，开发一种基于集合的管道来估计大气传输足迹、温室气体摩尔分数测量及其不确定性。&lt;h4&gt;方法&lt;/h4&gt;使用拉格朗日粒子扩散模型(LPDM)的图神经网络模拟器，结合2016年巴西GOSAT观测数据进行演示，通过集合计算量化绝对和相对不确定性，揭示与预测误差的空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;模拟器实现了比NAME LPDM快约1000倍的加速，同时重现了大尺度足迹结构；集合展开突出了大气传输足迹和甲烷摩尔分数在空间和时间上的低置信度预测区域。&lt;h4&gt;结论&lt;/h4&gt;该方法可更广泛地应用于大气传输模型，支持不确定性感知的温室气体反演系统，提高基于卫星的排放监测的稳健性；随着进一步发展，基于集合的模拟器可以帮助探索系统性LPDM误差，为温室气体通量估计提供更全面的不确定性预算。&lt;h4&gt;翻译&lt;/h4&gt;监测温室气体排放和评估国家清单需要高效、可扩展且可靠的推理方法。自上而下的方法，结合最近卫星观测技术的进展，为评估大陆和全球尺度的排放提供了新机会。然而，这些方法中使用的传输模型仍然是不确定性的主要来源：大规模运行时计算成本高，且其不确定性难以表征。人工智能为加速传输模拟和量化相关不确定性提供了双重机会。我们提出了一种基于集合的管道，使用拉格朗日粒子扩散模型(LPDM)的图神经网络模拟器来估计大气传输'足迹'、温室气体摩尔分数测量及其不确定性。该方法使用2016年巴西GOSAT(温室气体观测卫星)观测数据进行演示。模拟器实现了比NAME LPDM快约1000倍的加速，同时重现了大尺度足迹结构。通过集合计算量化绝对和相对不确定性，揭示了与预测误差的空间相关性。结果表明，集合展开突出了大气传输足迹和甲烷摩尔分数在空间和时间上的低置信度预测。虽然本文演示的是LPDM模拟器，但该方法可更广泛地应用于大气传输模型，支持不确定性感知的温室气体反演系统，提高基于卫星的排放监测的稳健性。随着进一步发展，基于集合的模拟器也可以帮助探索系统性的LPDM误差，为温室气体通量估计提供更全面的不确定性预算，计算效率更高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monitoring greenhouse gas emissions and evaluating national inventoriesrequire efficient, scalable, and reliable inference methods. Top-downapproaches, combined with recent advances in satellite observations, providenew opportunities to evaluate emissions at continental and global scales.However, transport models used in these methods remain a key source ofuncertainty: they are computationally expensive to run at scale, and theiruncertainty is difficult to characterise. Artificial intelligence offers a dualopportunity to accelerate transport simulations and to quantify theirassociated uncertainty.  We present an ensemble-based pipeline for estimating atmospheric transport"footprints", greenhouse gas mole fraction measurements, and theiruncertainties using a graph neural network emulator of a Lagrangian ParticleDispersion Model (LPDM). The approach is demonstrated with GOSAT (GreenhouseGases Observing Satellite) observations for Brazil in 2016. The emulatorachieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scalefootprint structures. Ensembles were calculated to quantify absolute andrelative uncertainty, revealing spatial correlations with prediction error. Theresults show that ensemble spread highlights low-confidence spatial andtemporal predictions for both atmospheric transport footprints and methane molefractions.  While demonstrated here for an LPDM emulator, the approach could be appliedmore generally to atmospheric transport models, supporting uncertainty-awaregreenhouse gas inversion systems and improving the robustness ofsatellite-based emissions monitoring. With further development, ensemble-basedemulators could also help explore systematic LPDM errors, offering acomputationally efficient pathway towards a more comprehensive uncertaintybudget in greenhouse gas flux estimates.</description>
      <author>example@mail.com (Jeffrey N. Clark, Elena Fillola, Nawid Keshtmand, Raul Santos-Rodriguez, Matthew Rigby)</author>
      <guid isPermaLink="false">2510.05751v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
  <item>
      <title>AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</title>
      <link>http://arxiv.org/abs/2510.07293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 23 figures, the code is available at  \url{https://github.com/DabDans/AudioMarathon}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AudioMarathon基准测试，用于评估大型音频语言模型在长音频处理方面的理解和推理效率，针对长音频处理中的主要挑战。&lt;h4&gt;背景&lt;/h4&gt;处理长格式音频是大型音频语言模型(LALMs)面临的主要挑战。这些模型在处理注意力机制的二次方成本和建模长程时间依赖方面存在困难。现有的音频基准测试大多基于短片段构建，没有在现实的长上下文设置中评估模型。&lt;h4&gt;目的&lt;/h4&gt;为了解决长音频评估的研究空白，引入AudioMarathon基准测试，旨在评估模型在长音频处理方面的理解和推理效率。&lt;h4&gt;方法&lt;/h4&gt;AudioMarathon建立在三个支柱上：长上下文音频输入(90.0-300.0秒，对应2,250-7,500个音频标记)；语音、声音和音乐的完整领域覆盖；需要多跳推理的复杂推理任务。评估了最先进的LALMs，并研究了加速技术，分析了标记剪枝和KV缓存清除之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;随着音频长度的增加，最先进的LALMs性能明显下降。研究结果表明当前LALMs之间存在较大差距，突显了更好的时间推理和内存高效架构的需求。&lt;h4&gt;结论&lt;/h4&gt;AudioMarathon将推动音频和多模态研究社区开发能够解决复杂音频任务的高级音频理解模型。&lt;h4&gt;翻译&lt;/h4&gt;处理长格式音频是大型音频语言模型(LALMs)的主要挑战。这些模型在注意力机制的二次方成本和建模长程时间依赖方面存在困难。现有的音频基准测试大多基于短片段构建，没有在现实的长上下文设置中评估模型。为了解决这一研究空白，我们引入了AudioMarathon基准测试，旨在评估模型在长音频处理方面的理解和推理效率。AudioMarathon建立在三个支柱上：持续时间从90.0到300.0秒的长上下文音频输入，对应编码的2,250到7,500个音频标记；语音、声音和音乐的完整领域覆盖；以及需要多跳推理的复杂推理任务。我们评估了最先进的LALMs，并观察到随着音频长度的增加，性能明显下降。我们还研究了加速技术，分析了标记剪枝和KV缓存清除之间的权衡。结果表明当前LALMs之间存在较大差距，突显了更好的时间推理和内存高效架构的需求。我们相信AudioMarathon将推动音频和多模态研究社区开发能够解决复杂音频任务的高级音频理解模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Processing long-form audio is a major challenge for Large Audio Languagemodels (LALMs). These models struggle with the quadratic cost of attention($O(N^2)$) and with modeling long-range temporal dependencies. Existing audiobenchmarks are built mostly from short clips and do not evaluate models inrealistic long context settings. To address this gap, we introduceAudioMarathon, a benchmark designed to evaluate both understanding andinference efficiency on long-form audio. AudioMarathon provides a diverse setof tasks built upon three pillars: long-context audio inputs with durationsranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,sound, and music, and complex reasoning that requires multi-hop inference. Weevaluate state-of-the-art LALMs and observe clear performance drops as audiolength grows. We also study acceleration techniques and analyze the trade-offsof token pruning and KV cache eviction. The results show large gaps acrosscurrent LALMs and highlight the need for better temporal reasoning andmemory-efficient architectures. We believe AudioMarathon will drive the audioand multimodal research community to develop more advanced audio understandingmodels capable of solving complex audio tasks.</description>
      <author>example@mail.com (Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang)</author>
      <guid isPermaLink="false">2510.07293v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Online Generic Event Boundary Detection</title>
      <link>http://arxiv.org/abs/2510.06855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了在线通用事件边界检测(On-GEBD)任务，提出了一种名为Estimator的新框架，包含一致性事件预测器(CEA)和在线边界鉴别器(OBD)两个关键组件。实验表明，该框架优于所有基线模型，并在标准数据集上达到与离线方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;通用事件边界检测(GEBD)旨在从人类感知角度解释长视频，但现有方法需要处理完整视频帧才能预测，与人类在线实时处理数据的方式不同。&lt;h4&gt;目的&lt;/h4&gt;引入在线通用事件边界检测(On-GEBD)新任务，旨在立即检测流视频中通用事件的边界，弥合当前方法与人类感知方式的差距。&lt;h4&gt;方法&lt;/h4&gt;提出受事件分割理论(EST)启发的Estimator框架，包含两个组件：CEA基于先前的帧生成反映当前事件动态的未来帧预测；OBD测量预测误差并使用统计测试自适应调整阈值，以捕捉多样化的细微事件转换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Estimator优于从最近的在线视频理解模型改编的所有基线模型，并在Kinetics-GEBD和TAPOS数据集上实现了与先前离线GEBD方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;Estimator框架能够有效在线检测视频中的事件边界，性能接近离线方法，解决了当前GEBD方法无法实时处理视频的问题。&lt;h4&gt;翻译&lt;/h4&gt;通用事件边界检测(GEBD)旨在从人类感知的角度解释长视频。然而，当前的GEBD方法需要处理完整的视频帧才能做出预测，与人类在线和实时处理数据的方式不同。为了弥合这一差距，我们引入了一个新任务——在线通用事件边界检测(On-GEBD)，旨在立即检测流视频中通用事件的边界。该任务面临在无法访问未来帧的情况下，实时识别细微的、无分类的事件变化的独特挑战。为了应对这些挑战，我们提出了一个受事件分割理论(EST)启发的新型On-GEBD框架Estimator，该理论解释了人类如何通过利用预测信息和实际信息之间的差异将进行中的活动分割为事件。我们的框架包含两个关键组件：一致性事件预测器(CEA)和在线边界鉴别器(OBD)。具体而言，CEA仅基于先前的帧生成反映当前事件动态的未来帧预测。然后，OBD测量预测误差，并使用过去误差的统计测试自适应调整阈值，以捕捉多样化的、细微的事件转换。实验结果表明，Estimator优于从最近的在线视频理解模型改编的所有基线模型，并在Kinetics-GEBD和TAPOS数据集上实现了与先前离线GEBD方法相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generic Event Boundary Detection (GEBD) aims to interpret long-form videosthrough the lens of human perception. However, current GEBD methods requireprocessing complete video frames to make predictions, unlike humans processingdata online and in real-time. To bridge this gap, we introduce a new task,Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundariesof generic events immediately in streaming videos. This task faces uniquechallenges of identifying subtle, taxonomy-free event changes in real-time,without the access to future frames. To tackle these challenges, we propose anovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)which explains how humans segment ongoing activity into events by leveragingthe discrepancies between predicted and actual information. Our frameworkconsists of two key components: the Consistent Event Anticipator (CEA), and theOnline Boundary Discriminator (OBD). Specifically, the CEA generates aprediction of the future frame reflecting current event dynamics based solelyon prior frames. Then, the OBD measures the prediction error and adaptivelyadjusts the threshold using statistical tests on past errors to capturediverse, subtle event transitions. Experimental results demonstrate thatEstimator outperforms all baselines adapted from recent online videounderstanding models and achieves performance comparable to prior offline-GEBDmethods on the Kinetics-GEBD and TAPOS datasets.</description>
      <author>example@mail.com (Hyungrok Jung, Daneul Kim, Seunggyun Lim, Jeany Son, Jonghyun Choi)</author>
      <guid isPermaLink="false">2510.06855v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring</title>
      <link>http://arxiv.org/abs/2510.06509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种高效的视频语言理解方法，通过选择保留语义和上下文信息的小帧集来处理长视频。&lt;h4&gt;背景&lt;/h4&gt;视频语言理解需要从长视频中提取关键信息，但全帧处理效率低下，需要更有效的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够识别最具信息量帧的框架，用于视频检索、字幕生成和视频语言推理等下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出了KeyScore多模态帧评分框架，结合字幕和视觉上下文估计帧级重要性；同时引入STACFP（时空自适应聚类帧提案）生成紧凑多样的帧候选。&lt;h4&gt;主要发现&lt;/h4&gt;该方法相比全帧推理可实现高达99%的帧减少，在MSRVTT、MSVD和DiDeMo数据集上显著优于标准8帧编码器。&lt;h4&gt;结论&lt;/h4&gt;强调视觉和文本信号之间的多模态对齐可实现可扩展、高效且基于字幕的视频理解，无需明确的视频摘要步骤。&lt;h4&gt;翻译&lt;/h4&gt;高效的视频语言理解需要选择保留长视频中语义和上下文信息的小帧集。我们提出了KeyScore，一个多模态帧评分框架，联合利用字幕和视觉上下文来估计帧级重要性。通过结合语义相似性、时间多样性和上下文下降影响，KeyScore识别出用于检索、字幕生成和视频语言推理等下游任务的最具信息量的帧。为补充KeyScore，我们引入了STACFP（用于帧提案的时空自适应聚类），它为长视频生成紧凑且多样的帧候选。这些模块共同实现了相比全帧推理高达99%的帧减少，并在MSRVTT、MSVD和DiDeMo上显著优于标准8帧编码器。我们的结果表明，强调视觉和文本信号之间的多模态对齐可实现可扩展、高效且基于字幕的视频理解，无需明确的视频摘要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient video-language understanding requires selecting a small set offrames that retain semantic and contextual information from long videos. Wepropose KeyScore, a multimodal frame scoring framework that jointly leveragescaptions and visual context to estimate frame-level importance. By combiningsemantic similarity, temporal diversity, and contextual drop impact, KeyScoreidentifies the most informative frames for downstream tasks such as retrieval,captioning, and video-language reasoning. To complement KeyScore, we introduceSTACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), whichgenerates compact and diverse frame candidates for long-form videos. Together,these modules achieve up to 99\% frame reduction compared to full-frameinference and substantially outperform standard 8-frame encoders on MSRVTT,MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignmentbetween visual and textual signals enables scalable, efficient, andcaption-grounded video understanding -- without explicit video summarization.</description>
      <author>example@mail.com (Shih-Yao Lin, Sibendu Paul, Caren Chen)</author>
      <guid isPermaLink="false">2510.06509v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression</title>
      <link>http://arxiv.org/abs/2510.06293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlockGPT是一种新型生成式自回归transformer模型，使用批量标记化方法预测降水图，在准确性和推理速度方面均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;降水预测是复杂的时空建模任务，对减轻极端天气影响至关重要。短期降水预报需要准确且计算高效的实时模型。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法（如基于标记的自回归模型和扩散模型）的局限性，开发一种既准确又计算高效的降水预测模型。&lt;h4&gt;方法&lt;/h4&gt;BlockGPT采用批量标记化方法，在每个时间步预测完整的二维降水场，通过帧内自注意力和帧间因果注意力实现时空因子化，作为视频预测的模型无关范式实现。&lt;h4&gt;主要发现&lt;/h4&gt;在KNMI和SEVIR两个数据集上评估，BlockGPT比NowcastingGPT和DiffCast+Phydnet等基线模型实现更高准确性、更好的事件定位，推理速度提高高达31倍。&lt;h4&gt;结论&lt;/h4&gt;BlockGPT在降水预报任务中表现出色，在准确性、事件定位和推理速度方面均优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;预测降水图是一个高度复杂的时空建模任务，对于减轻极端天气事件的影响至关重要。短期降水预报或临近预报需要不仅准确而且计算高效的模型以实现实时应用。当前方法如基于标记的自回归模型往往存在错误的归纳偏置和缓慢的推理，而扩散模型可能计算密集。为解决这些限制，我们引入了BlockGPT，一种使用批量标记化方法的生成式自回归transformer，在每个时间步预测完整的二维场。作为视频预测的模型无关范式，BlockGPT通过在每帧内使用自注意力并在帧间使用因果注意力来分解时空；在本工作中，我们将其实例化用于降水临近预报。我们在两个降水数据集上评估BlockGPT，即荷兰的KNMI和美国的SEVIR，并将其与最先进的基线模型进行比较，包括基于标记的和基于扩散的模型。结果表明，BlockGPT实现了更高的准确性，通过分类度量的指标测量的事件定位，以及比可比基线模型快高达31倍的推理速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting precipitation maps is a highly complex spatiotemporal modelingtask, critical for mitigating the impacts of extreme weather events. Short-termprecipitation forecasting, or nowcasting, requires models that are not onlyaccurate but also computationally efficient for real-time applications. Currentmethods, such as token-based autoregressive models, often suffer from flawedinductive biases and slow inference, while diffusion models can becomputationally intensive. To address these limitations, we introduce BlockGPT,a generative autoregressive transformer using batched tokenization (Block)method that predicts full two-dimensional fields (frames) at each time step.Conceived as a model-agnostic paradigm for video prediction, BlockGPTfactorizes space-time by using self-attention within each frame and causalattention across frames; in this work, we instantiate it for precipitationnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselinesincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)models. The results show that BlockGPT achieves superior accuracy, eventlocalization as measured by categorical metrics, and inference speeds up to 31xfaster than comparable baselines.</description>
      <author>example@mail.com (Cristian Meo, Varun Sarathchandran, Avijit Majhi, Shao Hung, Carlo Saccardi, Ruben Imhoff, Roberto Deidda, Remko Uijlenhoet, Justin Dauwels)</author>
      <guid isPermaLink="false">2510.06293v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</title>
      <link>http://arxiv.org/abs/2510.05034v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 1st version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述首次全面考察了视频大多模态模型(Video-LMMs)的后训练方法，提供了统一框架来推进视频理解能力。&lt;h4&gt;背景&lt;/h4&gt;视频理解是计算机视觉领域最具挑战性的前沿，需要模型推理复杂时空关系、长期依赖和多模态证据。最近出现的Video-LMMs集成了视觉编码器和基于解码器的强大语言模型，在视频理解任务中展现出卓越能力。&lt;h4&gt;目的&lt;/h4&gt;填补文献中关于将Video-LMMs从基础感知系统转变为复杂推理引擎的后训练阶段研究的碎片化状态，提供首个全面的后训练方法考察。&lt;h4&gt;方法&lt;/h4&gt;涵盖三个基本支柱：使用思维链的监督微调、可验证目标的强化学习、通过增强推理计算实现的测试时缩放。提供结构化分类法阐明这些技术的角色和相互连接，分析代表性方法并综合关键设计原则和评估协议，同时整理必要的基准、数据集和指标。&lt;h4&gt;主要发现&lt;/h4&gt;视频理解面临独特挑战如时间定位、时空基础、长视频效率和多模态证据整合；识别出奖励设计、可扩展性和成本性能优化方面的关键开放性挑战。&lt;h4&gt;结论&lt;/h4&gt;为研究人员和从业人员提供推进Video-LMM能力的统一框架，相关资源和更新可在指定GitHub仓库获取。&lt;h4&gt;翻译&lt;/h4&gt;视频理解代表了计算机视觉中最具挑战性的前沿，需要模型推理复杂的时空关系、长期依赖和多模态证据。最近出现的视频大多模态模型(Video-LMMs)将视觉编码器与基于解码器的强大语言模型相结合，在视频理解任务中展示了卓越能力。然而，将这些模型从基础感知系统转变为复杂推理引擎的关键阶段——后训练，在文献中仍然分散。这篇综述首次对Video-LMMs的后训练方法进行了全面考察，涵盖三个基本支柱：使用思维链的监督微调、可验证目标的强化学习、通过增强推理计算实现的测试时缩放。我们提出了结构化分类法，阐明了这些技术的作用、相互连接和视频特定适应，解决了时间定位、时空基础、长视频效率和多模态证据集成等独特挑战。通过系统分析代表性方法，我们综合了关键设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。我们还整理了必要的基准、数据集和指标，以促进对后训练效果的严格评估。这篇综述旨在为研究人员和从业人员提供推进Video-LMM能力的统一框架。额外的资源和更新保存在：https://github.com/yunlong10/Awesome-Video-LMM-Post-Training&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding represents the most challenging frontier in computervision, requiring models to reason about complex spatiotemporal relationships,long-term dependencies, and multimodal evidence. The recent emergence ofVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoderswith powerful decoder-based language models, has demonstrated remarkablecapabilities in video understanding tasks. However, the critical phase thattransforms these models from basic perception systems into sophisticatedreasoning engines, post-training, remains fragmented across the literature.This survey provides the first comprehensive examination of post-trainingmethodologies for Video-LMMs, encompassing three fundamental pillars:supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)from verifiable objectives, and test-time scaling (TTS) through enhancedinference computation. We present a structured taxonomy that clarifies theroles, interconnections, and video-specific adaptations of these techniques,addressing unique challenges such as temporal localization, spatiotemporalgrounding, long video efficiency, and multimodal evidence integration. Throughsystematic analysis of representative methods, we synthesize key designprinciples, insights, and evaluation protocols while identifying critical openchallenges in reward design, scalability, and cost-performance optimization. Wefurther curate essential benchmarks, datasets, and metrics to facilitaterigorous assessment of post-training effectiveness. This survey aims to provideresearchers and practitioners with a unified framework for advancing Video-LMMcapabilities. Additional resources and updates are maintained at:https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</description>
      <author>example@mail.com (Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu)</author>
      <guid isPermaLink="false">2510.05034v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)</title>
      <link>http://arxiv.org/abs/2510.07285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This preprint was submitted to IEEE TrustCom 2025. The accepted  version will be published under copyright 2025 IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GTCN-G的新型深度学习框架，用于解决网络入侵检测系统中的网络威胁复杂性和数据类别不平衡问题。该框架结合了门控时序卷积网络和图卷积网络的优势，并通过残差学习机制提高对少数类恶意活动的检测敏感性。&lt;h4&gt;背景&lt;/h4&gt;现代入侵检测系统面临两大挑战：网络威胁日益复杂以及流量数据中的类别不平衡问题。图神经网络擅长建模拓扑结构，时序卷积网络能够捕捉时间序列依赖关系，但将两者有机结合并同时解决数据不平衡问题的框架仍然是一个开放性挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合图神经网络和时序卷积网络优势，同时明确解决数据不平衡问题的入侵检测框架，以提高对罕见恶意活动（少数类）的检测敏感性和整体检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为门控时序卷积网络与图（GTCN-G）的深度学习框架，该方法：1）融合门控时序卷积网络（G-TCN）用于从网络流量中提取分层时间特征；2）使用图卷积网络（GCN）从底层图结构中学习；3）集成通过图注意力网络（GAT）实现的残差学习机制，通过残差连接保留原始特征信息，以缓解类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;在UNSW-NB15和ToN-IoT两个公开基准数据集上的广泛实验表明，所提出的GTCN-G模型在二分类和多分类任务中都实现了最先进的性能，显著优于现有的基线模型。&lt;h4&gt;结论&lt;/h4&gt;GTCN-G框架成功结合了图神经网络和时序卷积网络的优势，并通过残差学习机制有效解决了数据不平衡问题，为现代入侵检测系统提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;网络威胁日益复杂和流量数据中固有的类别不平衡对现代入侵检测系统构成了严峻挑战。虽然图神经网络在建模拓扑结构方面表现出色，时序卷积网络擅长捕捉时间序列依赖关系，但能够协同整合两者并同时明确解决数据不平衡问题的框架仍然是一个开放性挑战。本文引入了一种名为门控时序卷积网络与图的新型深度学习框架，旨在克服这些局限性。我们的模型独特地融合了门控时序卷积网络用于从网络流量中提取分层时间特征，以及设计用于从底层图结构中学习的图卷积网络。核心创新在于集成了通过图注意力网络实现的残差学习机制。该机制通过残差连接保留原始特征信息，这对于缓解类别不平衡问题并增强对罕见恶意活动的检测敏感性至关重要。我们在两个公开基准数据集上进行了大量实验以验证我们的方法。实验结果表明，所提出的模型实现了最先进的性能，在二分类和多分类任务中都显著优于现有基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The escalating complexity of network threats and the inherent class imbalancein traffic data present formidable challenges for modern Intrusion DetectionSystems (IDS). While Graph Neural Networks (GNNs) excel in modeling topologicalstructures and Temporal Convolutional Networks (TCNs) are proficient incapturing time-series dependencies, a framework that synergistically integratesboth while explicitly addressing data imbalance remains an open challenge. Thispaper introduces a novel deep learning framework, named Gated TemporalConvolutional Network and Graph (GTCN-G), engineered to overcome theselimitations. Our model uniquely fuses a Gated TCN (G-TCN) for extractinghierarchical temporal features from network flows with a Graph ConvolutionalNetwork (GCN) designed to learn from the underlying graph structure. The coreinnovation lies in the integration of a residual learning mechanism,implemented via a Graph Attention Network (GAT). This mechanism preservesoriginal feature information through residual connections, which is criticalfor mitigating the class imbalance problem and enhancing detection sensitivityfor rare malicious activities (minority classes). We conducted extensiveexperiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, tovalidate our approach. The empirical results demonstrate that the proposedGTCN-G model achieves state-of-the-art performance, significantly outperformingexisting baseline models in both binary and multi-class classification tasks.</description>
      <author>example@mail.com (Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Qi Hu, Yan Li, Chang Liu)</author>
      <guid isPermaLink="false">2510.07285v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics</title>
      <link>http://arxiv.org/abs/2510.07109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication in IEEE Transactions on  Consumer Electronics. 10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的网络异常检测框架(GNN-NAD)，用于解决消费电子产品在物联网环境中面临的安全威胁问题。通过整合软件定义网络和计算优先网络架构，该框架能够有效检测网络异常并提供整体网络安全视图。&lt;h4&gt;背景&lt;/h4&gt;消费电子产品连接到物联网容易受到DDoS和基于网络的攻击，这些攻击可能导致设备功能受损和远程劫持。现有基于深度学习的流量异常检测系统在传统网络环境中表现良好，但往往过于复杂且依赖静态基础设施，需要手动配置和管理。&lt;h4&gt;目的&lt;/h4&gt;解决现有检测系统的局限性，提出一个可扩展的网络模型，整合软件定义网络和计算优先网络，用于增强下一代消费电子产品网络的安全性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于图神经网络的网络异常检测框架(GNN-NAD)，该框架整合了基于软件定义网络的消费电子产品网络并支持计算优先网络架构。GNN-NAD融合静态的、漏洞感知的攻击图与动态流量特征，框架核心是用于图表示学习的GNN模型(GSAGE)，结合随机森林分类器，形成GSAGE+RF设计。&lt;h4&gt;主要发现&lt;/h4&gt;在消费电子产品环境上的实验评估显示，GNN-NAD在准确性、召回率、精确度和F1分数等指标上表现出色。即使在样本量较小的情况下，其性能也超过了当前的网络异常检测方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过提出GNN-NAD框架，有效推进了下一代智能消费电子产品网络的安全性和效率，为物联网环境下的设备安全提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;消费电子产品连接到物联网容易受到各种攻击，包括DDoS和基于网络的威胁，这些攻击可能损害其功能并实现远程劫持。这些漏洞允许攻击者利用消费电子产品进行更广泛的系统攻击，同时恶意代码可以在消费电子产品网络中传播，导致设备故障。现有的基于深度学习的流量异常检测系统在传统网络环境中表现出高准确性，但往往过于复杂且依赖静态基础设施，需要手动配置和管理。为解决这些限制，我们提出了一种可扩展的网络模型，整合了软件定义网络和计算优先网络，用于下一代消费电子产品网络。在该网络模型中，我们提出了一个基于图神经网络的网络异常检测框架(GNN-NAD)，该框架整合了基于软件定义网络的消费电子产品网络并支持计算优先网络架构。GNN-NAD独特地融合了静态的、漏洞感知的攻击图与动态流量特征，提供网络安全的整体视图。框架的核心是一个用于图表示学习的GNN模型(GSAGE)，随后是随机森林分类器。这种设计相比现有的特征选择方法表现出优越性能。在消费电子产品环境上的实验评估显示，GNN-NAD在准确性、召回率、精确度和F1分数等指标上表现出色，即使在小样本情况下，也超过了当前网络异常检测方法的性能。这项工作推进了下一代智能消费电子产品网络的安全性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Consumer electronics (CE) connected to the Internet of Things are susceptibleto various attacks, including DDoS and web-based threats, which can compromisetheir functionality and facilitate remote hijacking. These vulnerabilitiesallow attackers to exploit CE for broader system attacks while enabling thepropagation of malicious code across the CE network, resulting in devicefailures. Existing deep learning-based traffic anomaly detection systemsexhibit high accuracy in traditional network environments but are often overlycomplex and reliant on static infrastructure, necessitating manualconfiguration and management. To address these limitations, we propose ascalable network model that integrates Software-defined Networking (SDN) andCompute First Networking (CFN) for next-generation CE networks. In this networkmodel, we propose a Graph Neural Networks-based Network Anomaly Detectionframework (GNN-NAD) that integrates SDN-based CE networks and enables the CFNarchitecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graphwith dynamic traffic features, providing a holistic view of network security.The core of the framework is a GNN model (GSAGE) for graph representationlearning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)demonstrates superior performance compared to existing feature selectionmethods. Experimental evaluations on CE environment reveal that GNN-NADachieves superior metrics in accuracy, recall, precision, and F1 score, evenwith small sample sizes, exceeding the performance of current network anomalydetection methods. This work advances the security and efficiency ofnext-generation intelligent CE networks.</description>
      <author>example@mail.com (Guan-Yan Yang, Farn Wang, Kuo-Hui Yeh)</author>
      <guid isPermaLink="false">2510.07109v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Node Affinity Prediction in Temporal Graphs</title>
      <link>http://arxiv.org/abs/2510.06940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为NAViS的节点亲和力预测模型，通过利用启发式方法和状态空间模型之间的等价性，并引入新型损失函数，在TGB数据集上实现了优于现有技术和启发式方法的性能。&lt;h4&gt;背景&lt;/h4&gt;节点亲和力预测是时序图学习中的常见任务，广泛应用于社交网络、金融网络和推荐系统等领域。尽管最近的研究尝试将先进的动态链接属性预测模型适应到节点亲和力预测中，但简单的启发式方法（如持久预测或移动平均）却表现更好。&lt;h4&gt;目的&lt;/h4&gt;分析当前时序图神经网络在节点亲和力预测训练中面临的挑战，并提出适当的解决方案以提升预测性能。&lt;h4&gt;方法&lt;/h4&gt;开发NAViS（使用虚拟状态的节点亲和力预测模型），通过结合提出的解决方案，利用启发式方法和状态空间模型之间的等价性，并引入一种专门用于节点亲和力预测的新型损失函数来解决训练难题。&lt;h4&gt;主要发现&lt;/h4&gt;在TGB数据集上的评估表明，NAViS模型性能优于现有最先进的方法，包括各种启发式方法。研究源代码已在GitHub上公开分享。&lt;h4&gt;结论&lt;/h4&gt;NAViS模型通过创新的虚拟状态方法和新型损失函数，成功解决了时序图神经网络在节点亲和力预测中的训练挑战，实现了比现有技术和简单启发式方法更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;节点亲和力预测是时序图学习中的常见任务，广泛应用于社交网络、金融网络、推荐系统等多个领域。最近的研究通过将最先进的动态链接属性预测模型适应到节点亲和力预测中来解决这一任务。然而，简单的启发式方法，如持久预测或移动平均，却优于这些模型。在本研究中，我们分析了当前时序图神经网络在节点亲和力预测训练中的挑战，并提出了适当的解决方案。结合这些解决方案，我们开发了NAViS——使用虚拟状态的节点亲和力预测模型，通过利用启发式方法和状态空间模型之间的等价性。虽然前景良好，但训练NAViS并不简单。因此，我们进一步引入了一种用于节点亲和力预测的新型损失函数。我们在TGB上评估了NAViS，结果表明它优于最先进的方法，包括启发式方法。我们的源代码可在https://github.com/orfeld415/NAVIS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node affinity prediction is a common task that is widely used in temporalgraph learning with applications in social and financial networks, recommendersystems, and more. Recent works have addressed this task by adaptingstate-of-the-art dynamic link property prediction models to node affinityprediction. However, simple heuristics, such as Persistent Forecast or MovingAverage, outperform these models. In this work, we analyze the challenges intraining current Temporal Graph Neural Networks for node affinity predictionand suggest appropriate solutions. Combining the solutions, we develop NAViS -Node Affinity prediction model using Virtual State, by exploiting theequivalence between heuristics and state space models. While promising,training NAViS is non-trivial. Therefore, we further introduce a novel lossfunction for node affinity prediction. We evaluate NAViS on TGB and show thatit outperforms the state-of-the-art, including heuristics. Our source code isavailable at https://github.com/orfeld415/NAVIS</description>
      <author>example@mail.com (Krishna Sri Ipsit Mantri, Or Feldman, Moshe Eliasof, Chaim Baskin)</author>
      <guid isPermaLink="false">2510.06940v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder</title>
      <link>http://arxiv.org/abs/2510.06880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多组学单细胞数据整合面临高维性和复杂跨模态关系的挑战，研究提出MoRE-GNN方法有效解决了这一问题&lt;h4&gt;背景&lt;/h4&gt;多组学单细胞数据整合具有挑战性，主要因为数据具有高维性和复杂的跨模态关系&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合多组学单细胞数据的方法，捕捉生物学上有意义的关系&lt;h4&gt;方法&lt;/h4&gt;提出MoRE-GNN（多组学关系边图神经网络），这是一种异构图自编码器，结合图卷积和注意力机制，可以直接从数据动态构建关系图&lt;h4&gt;主要发现&lt;/h4&gt;在六个公开数据集上的评估表明，MoRE-GNN能够捕获生物学上有意义的关系，优于现有方法，特别是在强跨模态相关性的设置中；此外，学习到的表示允许准确的下游跨模态预测&lt;h4&gt;结论&lt;/h4&gt;尽管性能可能因数据集复杂性而异，但MoRE-GNN为推进多组学整合提供了一个自适应、可扩展和可解释的框架&lt;h4&gt;翻译&lt;/h4&gt;多组学单细胞数据的整合由于其高维性和复杂的跨模态关系而仍然具有挑战性。为了解决这个问题，我们引入了MoRE-GNN（多组学关系边图神经网络），这是一种异构图自编码器，结合了图卷积和注意力机制，可以直接从数据中动态构建关系图。在六个公开可用数据集上的评估表明，MoRE-GNN能够捕获生物学上有意义的关系，并且优于现有方法，特别是在具有强跨模态相关性的设置中。此外，学习到的表示允许进行准确的下游跨模态预测。尽管性能可能因数据集的复杂性而有所不同，但MoRE-GNN为推进多组学整合提供了一个自适应、可扩展和可解释的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of multi-omics single-cell data remains challenging due tohigh-dimensionality and complex inter-modality relationships. To address this,we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), aheterogeneous graph autoencoder that combines graph convolution and attentionmechanisms to dynamically construct relational graphs directly from data.Evaluations on six publicly available datasets demonstrate that MoRE-GNNcaptures biologically meaningful relationships and outperforms existingmethods, particularly in settings with strong inter-modality correlations.Furthermore, the learned representations allow for accurate downstreamcross-modal predictions. While performance may vary with dataset complexity,MoRE-GNN offers an adaptive, scalable and interpretable framework for advancingmulti-omics integration.</description>
      <author>example@mail.com (Zhiyu Wang, Sonia Koszut, Pietro Liò, Francesco Ceccarelli)</author>
      <guid isPermaLink="false">2510.06880v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalization of Graph Neural Networks for AC Optimal Power Flow</title>
      <link>http://arxiv.org/abs/2510.06860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print has been submitted for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种混合异构消息传递神经网络（HH-MPNN），用于解决交流最优潮流（ACOPF）在大规模电力系统中的计算效率和拓扑适应性问题。&lt;h4&gt;背景&lt;/h4&gt;ACOPF对于大规模电力系统计算成本很高，传统求解器需要过长的求解时间。机器学习方法虽能提供计算加速，但在可扩展性和拓扑适应性方面存在问题，需要昂贵的重新训练。&lt;h4&gt;目的&lt;/h4&gt;实现跨电网规模的可扩展性，并使模型能够适应拓扑变化。&lt;h4&gt;方法&lt;/h4&gt;提出混合异构消息传递神经网络（HH-MPNN），将母线、发电机、负载、并联电抗器、输电线路和变压器建模为不同的节点或边类型，结合可扩展的变压器模型处理长距离依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;在14到2000母线的电网上，HH-MPNN在默认拓扑上实现了小于1%的最优性差距；零样本应用于数千个未见过的拓扑，实现了小于3%的最优性差距；在较小电网上预训练能改善较大电网的结果；与内点求解器相比，计算速度提升达到1000倍到10000倍。&lt;h4&gt;结论&lt;/h4&gt;这些结果推动了实时电力系统操作的实用、可泛化的机器学习发展。&lt;h4&gt;翻译&lt;/h4&gt;交流最优潮流（ACOPF）对于大规模电力系统计算成本很高，传统求解器需要过长的求解时间。机器学习方法虽能提供计算加速，但在可扩展性和拓扑适应性方面存在问题，需要昂贵的重新训练。为了实现跨电网规模的可扩展性和适应拓扑变化，我们提出了一种混合异构消息传递神经网络（HH-MPNN）。HH-MPNN将母线、发电机、负载、并联电抗器、输电线路和变压器建模为不同的节点或边类型，结合可扩展的变压器模型处理长距离依赖关系。在14到2000母线的电网上，HH-MPNN在默认拓扑上实现了小于1%的最优性差距。零样本应用于数千个未见过的拓扑，HH-MPNN实现了小于3%的最优性差距，尽管仅在默认拓扑上训练。在较小电网上预训练也能改善较大电网的结果。与内点求解器相比，计算速度提升达到1000倍到10000倍。这些结果推动了实时电力系统操作的实用、可泛化的机器学习发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AC Optimal Power Flow (ACOPF) is computationally expensive for large-scalepower systems, with conventional solvers requiring prohibitive solution times.Machine learning approaches offer computational speedups but struggle withscalability and topology adaptability without expensive retraining. To enablescalability across grid sizes and adaptability to topology changes, we proposea Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN modelsbuses, generators, loads, shunts, transmission lines and transformers asdistinct node or edge types, combined with a scalable transformer model forhandling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNNachieves less than 1% optimality gap on default topologies. Applied zero-shotto thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gapdespite training only on default topologies. Pre-training on smaller grids alsoimproves results on a larger grid. Computational speedups reach 1,000x to10,000x compared to interior point solvers. These results advance practical,generalizable machine learning for real-time power system operations.</description>
      <author>example@mail.com (Olayiwola Arowolo, Jochen L. Cremer)</author>
      <guid isPermaLink="false">2510.06860v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs</title>
      <link>http://arxiv.org/abs/2510.06290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Soft-Evidence Fusion Graph Neural Network (SEFGNN)框架，用于在多个生物网络中识别癌症驱动基因(CDGs)。该框架在决策层面而非特征层面融合多网络信息，使用Dempster-Shafer理论进行不确定性感知融合，并引入软证据平滑(SES)模块提高排名稳定性。实验表明SEFGNN在三个癌症数据集上优于现有方法，且在发现新型CDGs方面具有潜力。&lt;h4&gt;背景&lt;/h4&gt;识别癌症驱动基因(CDGs)对理解癌症机制和开发靶向治疗至关重要。图神经网络(GNNs)已被用于通过捕获生物相互作用网络中的模式来识别CDGs，但大多数方法仅依赖单一蛋白质-蛋白质相互作用(PPI)网络，忽略了其他生物网络的互补信息。一些研究通过特征对齐整合多网络，但这种方法假设跨网络基因关系一致，可能忽略网络异质性并引入冲突信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多个网络上识别CDGs的新框架，解决现有方法在网络异质性和冲突信息处理上的不足，通过在决策层面而非特征层面融合多网络信息，提高CDG识别准确性和发现新CDGs的能力。&lt;h4&gt;方法&lt;/h4&gt;提出Soft-Evidence Fusion Graph Neural Network (SEFGNN)框架，将每个生物网络视为独立证据源，在决策层面使用Dempster-Shafer理论(DST)进行不确定性感知融合，并引入软证据平滑(SES)模块来减轻DST过度自信的风险，提高排名稳定性同时保留判别性能。&lt;h4&gt;主要发现&lt;/h4&gt;在三个癌症数据集上的实验表明，SEFGNN始终优于最先进的基线方法；SEFGNN在发现新型CDGs方面展现出强大潜力；通过在决策层面融合多网络信息，能够更好地处理网络异质性；SES模块有效提高了排名稳定性而不影响判别性能。&lt;h4&gt;结论&lt;/h4&gt;SEFGNN是一个有效的多网络CDG识别框架，通过决策层面融合多网络信息，能够更好地处理网络异质性和冲突信息；引入的SES模块有效提高了方法稳定性；该方法在发现新型CDGs方面具有应用前景。&lt;h4&gt;翻译&lt;/h4&gt;识别癌症驱动基因(CDGs)对于理解癌症机制和开发靶向治疗至关重要。图神经网络(GNNs)最近已被用于通过捕获生物相互作用网络中的模式来识别CDGs。然而，大多数基于GNN的方法依赖于单一的蛋白质-蛋白质相互作用(PPI)网络，忽略了其他生物网络中的互补信息。一些研究通过将特征与一致性约束对齐来整合多个网络，学习统一的基因表示用于CDG识别。然而，这种表示级别的融合通常假设跨网络中的基因关系是一致的，这可能忽略网络异质性并引入冲突信息。为解决这一问题，我们提出了Soft-Evidence Fusion Graph Neural Network (SEFGNN)，这是一个在多个网络上识别CDGs的新型框架，在决策层面而非特征层面进行融合。SEFGNN不强制特征级别的一致性，而是将每个生物网络视为独立的证据源，并使用Dempster-Shafer理论(DST)在决策层面进行不确定性感知融合。为了减轻DST的过度自信风险，我们进一步引入了软证据平滑(SES)模块，它在保持判别性能的同时提高了排名稳定性。在三个癌症数据集上的实验表明，SEFGNN始终优于最先进的基线方法，并在发现新型CDGs方面展现出强大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying cancer driver genes (CDGs) is essential for understanding cancermechanisms and developing targeted therapies. Graph neural networks (GNNs) haverecently been employed to identify CDGs by capturing patterns in biologicalinteraction networks. However, most GNN-based approaches rely on a singleprotein-protein interaction (PPI) network, ignoring complementary informationfrom other biological networks. Some studies integrate multiple networks byaligning features with consistency constraints to learn unified generepresentations for CDG identification. However, such representation-levelfusion often assumes congruent gene relationships across networks, which mayoverlook network heterogeneity and introduce conflicting information. Toaddress this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), anovel framework for CDG identification across multiple networks at the decisionlevel. Instead of enforcing feature-level consistency, SEFGNN treats eachbiological network as an independent evidence source and performsuncertainty-aware fusion at the decision level using Dempster-Shafer Theory(DST). To alleviate the risk of overconfidence from DST, we further introduce aSoft Evidence Smoothing (SES) module that improves ranking stability whilepreserving discriminative performance. Experiments on three cancer datasetsshow that SEFGNN consistently outperforms state-of-the-art baselines andexhibits strong potential in discovering novel CDGs.</description>
      <author>example@mail.com (Bang Chen, Lijun Guo, Houli Fan, Wentao He, Rong Zhang)</author>
      <guid isPermaLink="false">2510.06290v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2510.07316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Pixel-Perfect Depth模型，一种基于像素空间扩散生成的单目深度估计模型，可直接生成高质量无飞像素的点云，避免了传统VAE压缩带来的边缘伪影问题。&lt;h4&gt;背景&lt;/h4&gt;当前生成式深度估计模型通过微调Stable Diffusion实现，但需要使用VAE将深度图压缩到潜在空间，这会在边缘和细节处引入飞像素伪影，影响点云质量。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接在像素空间进行扩散生成的深度估计模型，避免VAE诱导的伪影，生成高质量、无飞像素的点云，并提高效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出两种新颖设计：1) 语义提示扩散变换器(Sp-DiT)，将视觉基础模型的语义表示整合到DiT中以提示扩散过程；2) 级联DiT设计，逐步增加token数量以提高效率和准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在五个基准测试的所有已发布生成模型中取得了最佳性能，并且在边缘感知点云评估中显著优于所有其他模型，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;Pixel-Perfect Depth模型通过直接在像素空间进行扩散生成，成功避免了VAE诱导的伪影，结合语义提示和级联设计，实现了高质量深度估计和点云生成，为单目深度估计提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了Pixel-Perfect Depth，一种基于像素空间扩散生成的单目深度估计模型，可以从估计的深度图中生成高质量、无飞像素的点云。当前生成式深度估计模型通过微调Stable Diffusion实现，取得了令人印象深刻的性能。然而，它们需要使用VAE将深度图压缩到潜在空间，这不可避免地在边缘和细节处引入飞像素。我们的模型直接在像素空间执行扩散生成，避免了VAE引起的伪影。为了克服与像素空间生成相关的高复杂度，我们引入了两种新颖设计：1) 语义提示扩散变换器(Sp-DiT)，将视觉基础模型的语义表示整合到DiT中以提示扩散过程，从而保持全局语义一致性的同时增强细粒度视觉细节；2) 级联DiT设计，逐步增加token数量以进一步提高效率和准确性。我们的模型在五个基准测试的所有已发布生成模型中取得了最佳性能，并且在边缘感知点云评估中显著优于所有其他模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目深度估计中的'飞像素'问题，即深度图转换为点云时在物体边缘和细节处产生的几何伪影。这个问题很重要，因为深度估计是3D重建、新视角合成和机器人操作等应用的基础，飞像素问题限制了这些技术在自由视角广播、机器人操作和沉浸式内容创建等实际场景中的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析发现现有生成式深度模型使用VAE压缩深度图会导致飞像素问题，尝试直接在像素空间进行扩散生成。但发现这种方法计算复杂且难以优化，于是借鉴高分辨率图像生成研究，认识到主要困难在于建模全局图像结构。基于此，作者设计了Pixel-Perfect Depth框架，整合了视觉基础模型的语义提示和级联变压器设计。作者借鉴了扩散模型（特别是Flow Matching）、Transformer架构（DiT）和多种视觉基础模型（如DINOv2、Depth Anything v2）等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接在像素空间进行扩散生成，避免使用VAE压缩，同时通过语义提示和级联设计解决像素空间扩散的计算和优化挑战。整体流程：1)输入图像；2)用视觉基础模型提取高级语义表示；3)对语义进行归一化；4)采用Flow Matching进行扩散生成，早期使用大补丁尺寸建模全局结构，后期增加标记数生成细节；5)通过迭代去噪将噪声转换为最终深度图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)直接在像素空间进行扩散生成，避免VAE压缩导致的飞像素；2)Semantics-Prompted DiT整合高级语义提示，保留全局结构同时增强细节；3)Cascade DiT Design采用渐进式补丁大小策略，提高效率和准确性；4)提出边缘感知点云评估指标。相比之前工作，不依赖预训练Stable Diffusion，从头训练仍能实现优越性能；与判别式模型相比保留边缘清晰度；与其他生成式模型相比避免VAE伪影；采用纯Transformer架构，无卷积层。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Pixel-Perfect Depth通过直接在像素空间进行语义提示的扩散生成，结合级联变压器设计，实现了高质量且无飞像素的单目深度估计，显著优于现有生成式模型在点云质量方面的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Pixel-Perfect Depth, a monocular depth estimation modelbased on pixel-space diffusion generation that produces high-quality,flying-pixel-free point clouds from estimated depth maps. Current generativedepth estimation models fine-tune Stable Diffusion and achieve impressiveperformance. However, they require a VAE to compress depth maps into latentspace, which inevitably introduces \textit{flying pixels} at edges and details.Our model addresses this challenge by directly performing diffusion generationin the pixel space, avoiding VAE-induced artifacts. To overcome the highcomplexity associated with pixel-space generation, we introduce two noveldesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), whichincorporate semantic representations from vision foundation models into DiT toprompt the diffusion process, thereby preserving global semantic consistencywhile enhancing fine-grained visual details; and 2) Cascade DiT Design thatprogressively increases the number of tokens to further enhance efficiency andaccuracy. Our model achieves the best performance among all publishedgenerative models across five benchmarks, and significantly outperforms allother models in edge-aware point cloud evaluation.</description>
      <author>example@mail.com (Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang)</author>
      <guid isPermaLink="false">2510.07316v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.07313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WristWorld是首个仅从锚定视角生成腕部视角视频的4D世界模型，通过结合几何和跨视角先验解决极端视角转换问题，在多个数据集上实现了最先进的视频生成效果，并提高了VLA性能。&lt;h4&gt;背景&lt;/h4&gt;腕部视角观察对VLA模型至关重要，能捕捉精细的手物交互增强操作性能，但大规模数据集很少包含此类记录，导致丰富的锚定视角与稀缺的腕部视角之间存在显著差距。现有世界模型无法仅从锚定视角生成腕部视角视频。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够仅从锚定视角生成腕部视角视频的模型，解决锚定视角与腕部视角之间的差距问题，提升VLA模型的操作性能。&lt;h4&gt;方法&lt;/h4&gt;WristWorld在两个阶段运行：1)重建阶段，扩展VGGT并融入空间投影一致性损失，估计几何一致的腕部视角姿态和4D点云；2)生成阶段，使用视频生成模型从重建视角合成时间连贯的腕部视角视频。&lt;h4&gt;主要发现&lt;/h4&gt;在Droid、Calvin和Franka Panda数据集上的实验展示了最先进的视频生成效果，具有优越的空间一致性；同时提高了VLA性能，在Calvin上将平均任务完成长度提高了3.81%，缩小了42.4%的锚定-腕部视角差距。&lt;h4&gt;结论&lt;/h4&gt;WristWorld成功解决了仅从锚定视角生成腕部视角视频的挑战，通过结合几何和跨视角先验，实现了时间连贯且空间一致的腕部视角视频生成，显著提升了VLA模型的操作性能。&lt;h4&gt;翻译&lt;/h4&gt;腕部视角观察对VLA模型至关重要，因为它们捕捉了精细的手物交互，直接增强操作性能。然而，大规模数据集很少包含此类记录，导致丰富的锚定视角与稀缺的腕部视角之间存在显著差距。现有的世界模型无法弥合这一差距，因为它们需要腕部视角的第一帧，因此无法仅从锚定视角生成腕部视角视频。面对这一差距，最近的视觉几何模型如VGGT出现，具有几何和跨视角先验，使得解决极端视角转换成为可能。受这些见解启发，我们提出了WristWorld，这是首个仅从锚定视角生成腕部视角视频的4D世界模型。WristWorld在两个阶段运行：(i)重建，扩展VGGT并融入我们的空间投影一致性损失，以估计几何一致的腕部视角姿态和4D点云；(ii)生成，使用我们的视频生成模型从重建的视角合成时间连贯的腕部视角视频。在Droid、Calvin和Franka Panda上的实验展示了最先进的视频生成，具有优越的空间一致性，同时提高了VLA性能，在Calvin上将平均任务完成长度提高了3.81%，并缩小了42.4%的锚定-腕部视角差距。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从丰富的外部视角(anchor views)数据生成稀缺的手腕视角(wrist-view)视频数据。这个问题很重要，因为手腕视角能直接捕捉精细的手-物体交互，对提高机器人操作性能至关重要；而收集大规模手腕视角数据成本高昂，需要额外传感器和精确校准；现有世界模型无法填补这一视角差距，因为它们通常需要手腕视角的第一帧作为条件。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了手腕视角数据稀缺但重要的问题，发现现有世界模型无法从外部视角生成手腕视角视频。他们受到视觉几何模型(如VGGT)和基于扩散的视频生成方法的启发，设计了一个两阶段框架：重建阶段扩展VGGT并融入空间投影一致性(SPC)损失来估计几何一致的手腕姿态和4D点云；生成阶段使用扩散模型结合重建的几何条件和CLIP编码的外部视角语义来合成时间连贯的手腕视角视频。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个4D世界模型，能够仅从外部视角生成手腕视角视频，同时确保几何一致性和时间连贯性。整体流程分为两阶段：1)重建阶段：使用扩展的VGGT模型估计手腕相机姿态，应用SPC损失监督几何一致性，重建3D场景并投影到手腕视角形成条件图；2)生成阶段：基于扩散的视频生成模型以手腕视角投影条件和CLIP编码的外部视角特征为条件，生成时间连贯且几何一致的手腕视角视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个能仅从外部视角生成手腕视角视频的4D世界模型；2)专门的手腕头部设计和空间投影一致性(SPC)损失确保几何一致性；3)条件图生成提供结构指导；4)结合CLIP编码的外部视角语义弥补全局语义缺失。相比之前的工作，WristWorld不需要手腕视角的第一帧作为条件，同时兼顾几何一致性和时间连贯性，可作为即插即用模块扩展单视角世界模型，并在多个数据集上证明了优越性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; WristWorld提出了一种创新的两阶段4D世界模型，能够仅从外部视角生成几何一致且时间连贯的手腕视角视频，显著提升了视觉-语言-动作模型的操作性能，并可作为即插即用模块扩展现有单视角世界模型为多视角能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wrist-view observations are crucial for VLA models as they capturefine-grained hand-object interactions that directly enhance manipulationperformance. Yet large-scale datasets rarely include such recordings, resultingin a substantial gap between abundant anchor views and scarce wrist views.Existing world models cannot bridge this gap, as they require a wrist-viewfirst frame and thus fail to generate wrist-view videos from anchor viewsalone. Amid this gap, recent visual geometry models such as VGGT emerge withgeometric and cross-view priors that make it possible to address extremeviewpoint shifts. Inspired by these insights, we propose WristWorld, the first4D world model that generates wrist-view videos solely from anchor views.WristWorld operates in two stages: (i) Reconstruction, which extends VGGT andincorporates our Spatial Projection Consistency (SPC) Loss to estimategeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,which employs our video generation model to synthesize temporally coherentwrist-view videos from the reconstructed perspective. Experiments on Droid,Calvin, and Franka Panda demonstrate state-of-the-art video generation withsuperior spatial consistency, while also improving VLA performance, raising theaverage task completion length on Calvin by 3.81% and closing 42.4% of theanchor-wrist view gap.</description>
      <author>example@mail.com (Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang)</author>
      <guid isPermaLink="false">2510.07313v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis</title>
      <link>http://arxiv.org/abs/2510.07190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by SIGGRAPH Asia 2025 conference track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MV-Performer框架，一种用于从单目全身捕获中创建同步新视角视频的创新方法，专注于人体中心的4D新视角合成，实现了360度视角变化。&lt;h4&gt;背景&lt;/h4&gt;视频生成领域的最新突破表明，视频扩散模型可以作为隐式的4D新视角合成器，但当前方法主要集中在前视图的相机轨迹重定向，难以生成360度视角变化。&lt;h4&gt;目的&lt;/h4&gt;专注于人体为中心的子领域，提出MV-Performer框架，用于从单目全身捕获中创建同步的新视角视频，实现360度合成。&lt;h4&gt;方法&lt;/h4&gt;充分利用MVHumanNet数据集，引入信息丰富的条件信号，使用从定向部分点云渲染的相机依赖法线图减轻观察歧义，提出多视角人体为中心的视频扩散模型融合不同信息源，并提供针对野外视频案例的鲁棒推理程序。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的大量实验证明了MV-Performer的最先进有效性和鲁棒性，为人体中心的4D新视角合成建立了强大的模型。&lt;h4&gt;结论&lt;/h4&gt;MV-Performer成功解决了当前方法在360度视角变化方面的局限性，在人体中心的4D新视角合成方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近在视频生成领域的突破，由大规模数据集和扩散技术驱动，表明视频扩散模型可以作为隐式的4D新视角合成器。然而，当前方法主要集中在前视图内重定向相机轨迹，同时难以生成360度视角变化。在本文中，我们专注于人体为中心的子领域，提出了MV-Performer，一种用于从单目全身捕获中创建同步新视角视频的创新框架。为了实现360度合成，我们充分利用了MVHumanNet数据集并引入了信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的相机依赖法线图，有效减轻了可见和不可见观察之间的歧义。为了保持生成视频的同步性，我们提出了一个多视角人体为中心的视频扩散模型，融合参考视频、部分渲染和不同视角的信息。此外，我们为野外视频案例提供了鲁棒的推理程序，大大减轻了不完善单目深度估计引起的伪影。在三个数据集上的大量实验证明了我们MV-Performer的最先进有效性和鲁棒性，为人体中心的4D新视角合成建立了强大的模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单目视频中生成同步且一致的360度多视角人体表演视频的问题。这个问题在现实和研究中很重要，因为现有的多视角人体重建通常需要昂贵复杂的多视角相机系统，限制了应用范围。而单目输入的多视角合成可以大大降低数据采集成本，在媒体创作、虚拟现实、电影制作、自由视角视频和沉浸式体验等领域有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：当前方法难以处理360度视角变化，基于扩散模型的方法在处理大运动和保持时间细节方面有局限，而基于深度扭曲的方法在大视角变化时会产生不准确效果。作者借鉴了视频扩散模型（特别是WAN2.1）、基于深度的扭曲范式、多视角注意力和参考注意力机制等现有工作，但针对人体场景的特殊性进行了改进：使用MVHumanNet数据集，提出相机依赖法线图条件解决大视角歧义，设计多视角人体视频扩散模型确保同步性，并提供鲁棒推理过程减轻深度估计误差。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视频扩散模型作为强大生成器，结合基于深度的扭曲处理视角变化，引入相机依赖法线图作为条件信号区分可见和不可见区域，并通过多视角和参考注意力机制确保生成视频的同步性和一致性。整体流程：1)输入单目正面视频；2)使用MegaSaM和Sapiens估计深度和法线；3)对齐并优化深度；4)渲染部分几何条件和法线图；5)使用3D VAE编码特征；6)通过修改的DiT模型进行去噪，包含参考注意力和同步注意力；7)输出多视角同步视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个将单目视频转换为密集多视角视频的生成框架；2)提出由法线图指导的多视角视频扩散模型，实现大视角变化下的合成；3)相机依赖法线图条件解决大视角歧义；4)鲁棒推理过程减轻深度估计误差。相比之前工作：1)专门针对人体场景使用MVHumanNet数据集；2)使用显式几何先验而非隐式相机嵌入；3)通过注意力机制确保多视角同步性；4)针对不完善深度估计提供优化过程；5)实现真正的360度全方位合成而非小范围视角变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MV-Performer通过创新的相机依赖法线图条件和多视角注意力机制，首次实现了从单目视频中生成同步且一致的360度多视角人体表演视频，为沉浸式VR/AR和自由视角视频应用提供了强大的新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in video generation, powered by large-scale datasets anddiffusion techniques, have shown that video diffusion models can function asimplicit 4D novel view synthesizers. Nevertheless, current methods primarilyconcentrate on redirecting camera trajectory within the front view whilestruggling to generate 360-degree viewpoint changes. In this paper, we focus onhuman-centric subdomain and present MV-Performer, an innovative framework forcreating synchronized novel view videos from monocular full-body captures. Toachieve a 360-degree synthesis, we extensively leverage the MVHumanNet datasetand incorporate an informative condition signal. Specifically, we use thecamera-dependent normal maps rendered from oriented partial point clouds, whicheffectively alleviate the ambiguity between seen and unseen observations. Tomaintain synchronization in the generated videos, we propose a multi-viewhuman-centric video diffusion model that fuses information from the referencevideo, partial rendering, and different viewpoints. Additionally, we provide arobust inference procedure for in-the-wild video cases, which greatly mitigatesthe artifacts induced by imperfect monocular depth estimation. Extensiveexperiments on three datasets demonstrate our MV-Performer's state-of-the-arteffectiveness and robustness, setting a strong model for human-centric 4D novelview synthesis.</description>
      <author>example@mail.com (Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han)</author>
      <guid isPermaLink="false">2510.07190v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>The Stage Comes to You: A Real-Time Tele-Immersive System with 3D Point Clouds and Vibrotactile Feedback</title>
      <link>http://arxiv.org/abs/2510.07009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 1 figure. Accepted for presentation at SIGGRAPH Asia 2025  Posters. The final version will appear in the ACM Digital Library&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种低延迟的远程沉浸式娱乐系统，能够传输3D点云和脚步振动，创造舞台存在的感受。&lt;h4&gt;背景&lt;/h4&gt;远程沉浸式娱乐需要解决实时传输、渲染和触觉反馈的技术挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种低延迟的远程沉浸式娱乐系统，使远程观众能够获得接近现场表演的体验。&lt;h4&gt;方法&lt;/h4&gt;在快速变化的灯光下捕获动态点云，使用可穿戴加速度计感知脚步振动，通过总延迟小于100毫秒的系统进行处理、传输和渲染，使用大型3D LED墙和振动地板为远程观众提供视觉和触觉反馈。&lt;h4&gt;主要发现&lt;/h4&gt;系统成功实现了小于100毫秒的总延迟，在相距20公里的地点间进行了有效连接，观众能够观看现场舞蹈表演并与表演者实时互动而无明显延迟。&lt;h4&gt;结论&lt;/h4&gt;该系统能够有效创造远程沉浸式娱乐体验，使远程观众获得接近现场表演的感受。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种低延迟的远程沉浸式娱乐系统，该系统传输3D点云和表演者的脚步振动，创造舞台存在的感受。移动的表演者和周围环境在快速变化的灯光下被捕获为动态点云，然后在总延迟小于100毫秒的情况下进行处理、传输和渲染。在高环境噪声下，脚步振动通过可穿戴加速度计感知。实时视觉和触觉流被传送到远程场地，在那里大型3D LED墙和高效的振动地板环绕数十名观众。在2025年世博会上进行的公开试验连接了相距20公里的地点：观众观看了现场舞蹈表演，与表演者交谈，没有明显延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a low-latency tele-immersive entertainment system that streams 3Dpoint clouds and performers' footstep vibrations, creating the sense that thestage is present. Moving performers and their surroundings are captured asdynamic point clouds under rapidly changing lighting, then processed,transmitted, and rendered within a total latency of less than 100 ms. Underhigh ambient noise, footstep vibrations are sensed by wearable accelerometers.Real-time visual and haptic streams are delivered to a remote venue, where alarge 3D LED wall and a vibration-efficient haptic floor envelop dozens ofspectators. A public trial at Expo 2025 linked sites 20 km apart: visitorswatched a live dance show and conversed with performers without noticeabledelay.</description>
      <author>example@mail.com (Takahiro Matsumoto, Takahiro Kusabuka, Hiroshi Chigira, Kazuhiko Murasaki, Kakagu Komazaki, Masafumi Suzuki, Masakatsu Aoki)</author>
      <guid isPermaLink="false">2510.07009v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion</title>
      <link>http://arxiv.org/abs/2510.06687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态语义分割方法，通过整合光场和激光雷达数据来解决自动驾驶中遮挡等复杂条件下的场景理解挑战。作者创建了首个结合光场和点云数据的多模态语义分割数据集，并提出了Mlpfseg网络，包含特征补全和深度感知模块，实验表明该方法比单一模态方法表现更优。&lt;h4&gt;背景&lt;/h4&gt;语义分割是自动驾驶场景理解的基础，但在遮挡等复杂条件下仍面临显著挑战。光场和激光雷达模态提供了互补的视觉和空间线索，有助于鲁棒感知，但它们的有效融合受到视角多样性有限和模态差异固有问题的阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决多模态数据（光场和激光雷达）在语义分割中的有效融合问题，特别是在处理遮挡等复杂场景时，提高自动驾驶系统的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了首个整合光场数据和点云数据的多模态语义分割数据集。基于此数据集，设计了Mlpfseg网络（多模态光场点云融合分割网络），包含特征补全模块和深度感知模块。特征补全模块通过点云特征图的差分重建解决点云和图像像素之间的密度不匹配问题；深度感知模块通过强化注意力分数来提高对遮挡物体的分割效果。&lt;h4&gt;主要发现&lt;/h4&gt;该方法比仅使用图像的分割方法高出1.71%的平均交并比(mIoU)，比仅使用点云的分割方法高出2.38% mIoU，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;通过整合光场和激光雷达数据，并采用特征补全和深度感知技术，可以显著提高在复杂条件（如遮挡）下的语义分割性能，为自动驾驶场景理解提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语义分割作为自动驾驶场景理解的基石，但在遮挡等复杂条件下仍面临重大挑战。光场和激光雷达模态提供了互补的视觉和空间线索，有利于鲁棒感知；然而，它们的有效融合受到视角多样性有限和固有模态差异的阻碍。为解决这些挑战，本文提出了首个整合光场数据和点云数据的多模态语义分割数据集。基于此数据集，我们提出了多模态光场点云融合分割网络(Mlpfseg)，结合特征补全和深度感知，同时对相机图像和激光雷达点云进行分割。特征补全模块通过执行点云特征图的差分重建，解决点云与图像像素之间的密度不匹配问题，增强这些模态的融合。深度感知模块通过强化注意力分数来提高对遮挡物体的分割效果。我们的方法比仅图像分割高出1.71%平均交并比(mIoU)，比仅点云分割高出2.38% mIoU，证明了其有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决语义分割在复杂条件（如遮挡）下面临的挑战，特别是如何有效融合光场和激光雷达两种模态数据以提高分割准确性。这个问题在自动驾驶领域至关重要，因为准确的场景理解是安全驾驶的基础，而遮挡物体和小物体在实际交通场景中很常见，现有单一模态方法（图像或点云）各有局限性，难以有效处理这些情况。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括单一模态数据的不足和多模态融合的不充分。他们创建了首个结合光场和激光雷达的多模态数据集TrafficScene，使用3×3相机阵列提供更丰富的视角信息。在设计Mlpfseg网络时，借鉴了HRNet-48用于图像特征提取、SPVCNN用于点云特征提取，以及Mseg3D的多尺度特征融合方法。同时创新性地设计了点-像素特征融合模块解决密度不匹配问题，以及深度差异感知模块处理遮挡问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过同时分割光场图像和激光雷达点云两种模态数据，充分利用它们的优势互补，并解决点云稀疏性和遮挡感知问题。整体流程包括：1)双分支特征提取（光场图像分支用HRNet-48，点云分支用SPVCNN）；2)点-像素特征融合模块(PFFM)将点云特征投影到图像并进行插值填补；3)深度差异感知模块(DDPM)通过比较预测和真实深度图识别遮挡区域；4)融合特征输入分割头进行最终预测；5)计算多任务损失函数优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个多模态光场-激光雷达语义分割数据集TrafficScene，提供全视角标注；2)Mlpfseg网络架构，首次实现同时分割两种模态；3)点-像素特征融合模块(PFFM)，解决点云稀疏性问题；4)深度差异感知模块(DDPM)，专门处理遮挡物体。相比之前工作，不同之处在于：现有多模态方法通常只针对单一模态进行分割，且未充分考虑点云稀疏性和遮挡问题；而本文方法通过同时处理两种模态和专门设计的创新模块，显著提高了被遮挡物体和小物体的分割准确性，实验显示mIoU比之前方法提高1.71-2.38。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个结合光场图像和激光雷达点云的多模态语义分割数据集和网络，通过创新的特征融合和深度感知模块，显著提高了在复杂交通场景下特别是被遮挡物体和小物体的语义分割准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation serves as a cornerstone of scene understanding inautonomous driving but continues to face significant challenges under complexconditions such as occlusion. Light field and LiDAR modalities providecomplementary visual and spatial cues that are beneficial for robustperception; however, their effective integration is hindered by limitedviewpoint diversity and inherent modality discrepancies. To address thesechallenges, the first multimodal semantic segmentation dataset integratinglight field data and point cloud data is proposed. Based on this dataset, weproposed a multi-modal light field point-cloud fusion segmentationnetwork(Mlpfseg), incorporating feature completion and depth perception tosegment both camera images and LiDAR point clouds simultaneously. The featurecompletion module addresses the density mismatch between point clouds and imagepixels by performing differential reconstruction of point-cloud feature maps,enhancing the fusion of these modalities. The depth perception module improvesthe segmentation of occluded objects by reinforcing attention scores for betterocclusion awareness. Our method outperforms image-only segmentation by 1.71Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38mIoU, demonstrating its effectiveness.</description>
      <author>example@mail.com (Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan)</author>
      <guid isPermaLink="false">2510.06687v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2510.06582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种半自动的、不确定性感知的管道，用于地面激光扫描点云的语义分割，通过球形投影、特征增强、集成学习和目标标注相结合，减少标注工作量同时保持高准确性，并构建了Mangrove3D数据集。&lt;h4&gt;背景&lt;/h4&gt;准确的地面激光扫描点云语义分割受到昂贵的手动标注限制，这限制了大规模应用的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发减少标注工作量的半自动管道，构建红树林森林的语义分割TLS数据集，并评估数据效率和特征重要性。&lt;h4&gt;方法&lt;/h4&gt;将3D点投影到2D球形网格，使用多源特征丰富像素，训练集成分割网络生成伪标签和不确定性地图，通过不确定性地图指导模糊区域标注，将2D输出投影回3D，开发三层可视化套件，构建Mangrove3D数据集，评估数据效率和特征重要性，进行跨数据集测试验证泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠几乎捕获所有判别能力，平均交并比稳定在约0.76，特征增强策略在跨数据集测试中具有泛化能力。&lt;h4&gt;结论&lt;/h4&gt;研究贡献包括不确定性感知的TLS标注管道与可视化工具、Mangrove3D数据集、数据效率和特征重要性的经验指导，为生态监测等领域的可扩展高质量TLS点云分割提供支持。&lt;h4&gt;翻译&lt;/h4&gt;地面激光扫描点云的准确语义分割受到昂贵的手动标注限制。我们提出了一种半自动的、不确定性感知的管道，结合球形投影、特征增强、集成学习和目标标注，以减少标注工作量同时保持高准确性。我们的方法将3D点投影到2D球形网格，使用多源特征丰富像素，并训练集成分割网络生成伪标签和不确定性地图，后者指导模糊区域标注。2D输出被投影回3D，产生由三层可视化套件支持的密集标注点云，用于快速分类和审阅者指导。使用此管道，我们构建了Mangrove3D数据集。我们进一步评估数据效率和特征重要性，解决需要多少标注数据以及哪些特征最重要的问题。结果表明，性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠几乎捕获所有判别能力，平均交并比稳定在约0.76。最后，我们通过跨数据集测试确认了特征增强策略的泛化能力。研究贡献包括不确定性感知的TLS标注管道、Mangrove3D数据集、数据效率和特征重要性的经验指导，以及公开的数据集和处理脚本。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决地面激光扫描(TLS)点云语义分割中高质量手动标注成本高昂的问题。这个问题在生态监测和林业研究中至关重要，因为准确的点云分割是提取树木指标、生物量和栖息地特征的基础，但生态场景中严重的遮挡、不规则几何和交织的树结构使得手动标注极其困难和耗时，限制了自动化分析在林业和环境监测中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性：3D标注工具主要针对城市环境，球面投影方法主要用于自动驾驶，主动学习和自训练方法主要在RGB丰富的室内数据集上应用，特征融合方法依赖多传感器数据。在此基础上，作者借鉴了球面投影、主动学习、自训练和特征融合等现有工作，但进行了专门改进：扩展为多通道特征表示，专为没有配准图像的TLS设计，结合主动学习和自训练策略，并创建了专门针对红树林生态系统的数据集和三层次可视化工具。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将3D点云投影到2D球面网格降低标注难度，使用多源特征丰富表示增强模型理解，利用集成学习产生伪标签和不确定性图指导标注人员关注不确定性高的区域，结合主动学习和自训练减少人工标注工作量同时保持高准确性。整体流程分为三阶段：1)球面投影和可视化：将3D点云投影到2D球面网格，组织多通道特征，创建三层次可视化工具；2)混合标注：使用少量手动标注训练集成模型，生成分割掩码和不确定性图，高不确定性区域手动精化，高置信度预测保留为伪标签；3)3D空间反向投影和精化：将2D分割掩码反向投影到3D点云，应用几何平滑和特征驱动修复，创建紧凑虚拟球体用于快速检查。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)不确定性感知的TLS标注流程，首次将主动学习和自训练结合应用于全分辨率TLS球面投影图；2)多通道特征丰富表示，专为TLS设计不依赖外部图像；3)三层次可视化工具，加速数据分类和标注指导；4)创建了首个红树林TLS数据集Mangrove3D。相比之前工作的不同：与现有3D标注工具相比，专为复杂的TLS生态场景设计；与球面投影方法相比，扩展为多通道特征表示；与主动学习和自训练相比，针对TLS场景的挑战进行了改进；与特征融合方法相比，仅使用LiDAR衍生特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种半自动、不确定性感知的标注流程，通过球面投影、特征丰富、集成学习和目标标注相结合，显著降低了TLS点云语义分割的标注工作量，同时保持了高准确性，并创建了首个红树林TLS数据集Mangrove3D。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate semantic segmentation of terrestrial laser scanning (TLS) pointclouds is limited by costly manual annotation. We propose a semi-automated,uncertainty-aware pipeline that integrates spherical projection, featureenrichment, ensemble learning, and targeted annotation to reduce labelingeffort, while sustaining high accuracy. Our approach projects 3D points to a 2Dspherical grid, enriches pixels with multi-source features, and trains anensemble of segmentation networks to produce pseudo-labels and uncertaintymaps, the latter guiding annotation of ambiguous regions. The 2D outputs areback-projected to 3D, yielding densely annotated point clouds supported by athree-tier visualization suite (2D feature maps, 3D colorized point clouds, andcompact virtual spheres) for rapid triage and reviewer guidance. Using thispipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangroveforests. We further evaluate data efficiency and feature importance to addresstwo key questions: (1) how much annotated data are needed and (2) whichfeatures matter most. Results show that performance saturates after ~12annotated scans, geometric features contribute the most, and compactnine-channel stacks capture nearly all discriminative power, with the meanIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirmthe generalization of our feature-enrichment strategy through cross-datasettests on ForestSemantic and Semantic3D.  Our contributions include: (i) a robust, uncertainty-aware TLS annotationpipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)empirical guidance on data efficiency and feature importance, thus enablingscalable, high-quality segmentation of TLS point clouds for ecologicalmonitoring and beyond. The dataset and processing scripts are publiclyavailable at https://fz-rit.github.io/through-the-lidars-eye/.</description>
      <author>example@mail.com (Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt)</author>
      <guid isPermaLink="false">2510.06582v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Novel point cloud registration approach for noninvasive patient specific estimation of leaflet strain from 3D images of heart valves</title>
      <link>http://arxiv.org/abs/2510.06578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的特征跟踪框架，用于量化房室瓣的瓣膜应变，相比现有方法具有更高的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;心脏瓣膜疾病很常见，是心力衰竭的主要病因。瓣膜应变是评估瓣膜病理学发生和发展的潜在指标，但目前缺乏稳健且可推广的无创方法从临床图像中量化瓣膜应变。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的特征跟踪框架，用于使用三维超声心动图图像量化房室瓣的瓣膜应变。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的特征跟踪框架，用于使用儿科和成人患者的三维超声心动图图像量化房室瓣的瓣膜应变，并通过有限元基准验证其准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在评估心脏瓣膜的解剖变形和应变方面表现出比其他基于点的方法更高的准确性，能够在无需参数调整的情况下稳健跟踪高度可变形态的瓣膜跨阶段变形。第一主应变的中位数和四分位范围大于0.5与瓣膜膨出（脱垂）相关。&lt;h4&gt;结论&lt;/h4&gt;进一步研究心脏瓣膜疾病的生物力学特征可能有助于提高瓣膜疾病的预后评估和纵向评估。&lt;h4&gt;翻译&lt;/h4&gt;心脏瓣膜疾病很常见，是心力衰竭的主要病因。瓣膜应变是评估瓣膜病理学发生和发展的潜在指标。然而，从临床获取的患者图像中无创量化瓣膜应变的方法仍然有限。在本工作中，我们提出了一种新的特征跟踪框架，用于使用儿科和成人患者的三维超声心动图图像量化房室瓣的瓣膜应变。与其他基于点的方法相比，我们的方法在评估心脏瓣膜的解剖变形和应变方面表现出更高的准确性，并通过有限元基准验证。此外，我们的方法无需参数调整即可稳健地跟踪高度可变形态的瓣膜跨阶段变形。我们的分析显示，第一主应变的中位数和四分位范围大于0.5与瓣膜膨出（脱垂）相关。进一步研究心脏瓣膜疾病的生物力学特征可能有助于提高瓣膜疾病的预后评估和纵向评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Valvular heart disease is prevalent and a major contributor to heart failure.Valve leaflet strain is a promising metric for evaluating the mechanicsunderlying the initiation and progression of valvular pathology. However,robust and generalizable methods for noninvasively quantifying valvular strainfrom clinically acquired patient images remain limited. In this work, wepresent a novel feature-tracking framework for quantifying leaflet strain inatrioventricular valves using 3D echocardiographic images of pediatric andadult patients. Our method demonstrated superior accuracy in the assessment ofanatomical deformation and strain of heart valves compared to other point-basedapproaches, as verified against a finite element benchmark. Further, ourapproach can robustly track inter-phase deformation of valves across highlyvariable morphologies without parameter tuning. Our analysis revealed that amedian and interquartile range of the 1st principal strain greater than 0.5 isassociated with leaflet billow (prolapse). Further investigation of thebiomechanical signatures of heart valve disease has the potential to enhanceprognostic assessment and longitudinal evaluation of valvular disease.</description>
      <author>example@mail.com (Wensi Wu, Matthew Daemer, Jeffrey A. Weiss, Alison M. Pouch, Matthew A. Jolley)</author>
      <guid isPermaLink="false">2510.06578v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Terrain-Aided Navigation Using a Point Cloud Measurement Sensor</title>
      <link>http://arxiv.org/abs/2510.06470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了点云测量在地形辅助导航中的应用，通过比较两种测量模型并研究其可观测性特性，证明了点云测量优于雷达高度计，且选择哪种模型取决于计算资源。&lt;h4&gt;背景&lt;/h4&gt;地形辅助导航领域，以及惯性导航系统可能存在的精度问题。&lt;h4&gt;目的&lt;/h4&gt;通过探索生成有用的测量创新误差的方法，辅助惯性导航系统，实现有效的非线性状态估计。&lt;h4&gt;方法&lt;/h4&gt;比较了两种涉及数字地形高程模型扫描的测量模型：a)基于典型光线投射的模型，从给定姿态返回预测的点云测量；b)一种计算量较小不需要光线投射的滑动网格模型。此外，还研究了这两种测量模型的高度可观测性特性，并与雷达高度计进行了性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;点云测量优于雷达高度计的使用，且应使用的点云测量模型取决于计算资源。&lt;h4&gt;结论&lt;/h4&gt;点云测量优于雷达高度计，而应使用的点云测量模型取决于可用的计算资源。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了点云测量在地形辅助导航中的使用。我们的目标是通过探索生成有用的测量创新误差的方法来辅助惯性导航系统，实现有效的非线性状态估计。我们比较了两种涉及数字地形高程模型扫描的测量模型：a)一种基于从给定姿态出发的典型光线投射的模型，返回该姿态的预测点云测量；b)另一种计算量较小不需要光线投射的模型，我们在此称为滑动网格。除了需要姿态外，它还需要点云测量本身的模式，并返回预测的点云测量。我们进一步研究了这两种测量模型的高度可观测性特性。作为基线，我们将点云测量的性能与雷达高度计的使用进行了比较，并显示了精度的提高。我们最后得出结论，点云测量优于雷达高度计的使用，而应使用的点云测量模型取决于计算资源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用点云测量传感器来辅助惯性导航系统，提高在GPS拒绝环境下的导航精度问题。这个问题在现实中非常重要，因为GPS信号在许多环境（如城市峡谷、室内、军事区域）可能不可用或被干扰，而传统地形辅助导航主要依赖雷达高度计，在高倾斜角等情况下精度有限，无法满足现代自主导航系统的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了地形辅助导航的历史发展，从早期的TERCOM和SITAN系统到现代的粒子滤波应用。他们借鉴了[10]和[12]中使用点云测量进行状态估计的基本思路，但创新性地提出了两种不同的测量模型：射线投射和滑动网格。作者认识到传统扩展卡尔曼滤波难以处理高度非线性的测量模型，因此选择使用边缘化粒子滤波器来估计误差状态。他们还借鉴了现有的运动模型（如Pinson模型）和状态分解方法，将总状态导航动力学分解为标称状态和误差状态。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用点云测量提供丰富的地形信息，通过比较实际点云测量和预测点云测量生成误差信号，并使用边缘化粒子滤波器估计和校正惯性导航系统的误差状态。创新性地提出了滑动网格方法，避免计算密集的射线投射，显著提高效率。整体实现流程包括：1)获取IMU数据和点云测量；2)建立运动模型并分解状态；3)对每个粒子预测点云测量；4)计算预测与实际测量的误差；5)更新粒子权重；6)估计误差状态；7)与标称状态结合得到改进的导航解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出滑动网格方法避免计算密集的射线投射；2)使用较小尺寸的点云进行地形辅助导航；3)采用边缘化粒子滤波器直接估计误差状态；4)证明点云测量可观测高度信息，无需额外气压高度计；5)全面评估不同IMU等级下的性能。相比之前工作，本文的方法比传统雷达高度计提供更丰富的地形信息；相比[10]使用的大型机载激光扫描仪，使用更小的点云；相比[12]的可导向激光测量，使用固定模式点云；相比传统扩展卡尔曼滤波，更好地处理高度非线性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于点云测量的高效地形辅助导航方法，通过创新的滑动网格算法和边缘化粒子滤波器，显著提高了GPS拒绝环境下的导航精度，特别是在高倾斜角和复杂地形条件下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the use of a point cloud measurement in terrain-aidednavigation. Our goal is to aid an inertial navigation system, by exploring waysto generate a useful measurement innovation error for effective nonlinear stateestimation. We compare two such measurement models that involve the scanning ofa digital terrain elevation model: a) one that is based on typical ray-castingfrom a given pose, that returns the predicted point cloud measurement from thatpose, and b) another computationally less intensive one that does not requireraycasting and we refer to herein as a sliding grid. Besides requiring a pose,it requires the pattern of the point cloud measurement itself and returns apredicted point cloud measurement. We further investigate the observabilityproperties of the altitude for both measurement models. As a baseline, wecompare the use of a point cloud measurement performance to the use of a radaraltimeter and show the gains in accuracy. We conclude by showing that a pointcloud measurement outperforms the use of a radar altimeter, and the point cloudmeasurement model to use depends on the computational resources</description>
      <author>example@mail.com (Abdülbaki Şanlan, Fatih Erol, Murad Abu-Khalaf, Emre Koyuncu)</author>
      <guid isPermaLink="false">2510.06470v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Human Action Recognition from Point Clouds over Time</title>
      <link>http://arxiv.org/abs/2510.05506v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于3D视频的人类动作识别新方法，结合点云技术与稀疏卷积网络，实现了从密集3D数据中识别人体动作。&lt;h4&gt;背景&lt;/h4&gt;当前人类动作识别研究主要集中在骨骼动作识别和基于视频的方法。随着消费级深度传感器和激光雷达设备的普及，利用密集3D数据进行动作识别成为一个新的机会。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，利用3D视频进行动作识别，作为骨骼动作识别和视频方法的'第三种方式'。&lt;h4&gt;方法&lt;/h4&gt;提出一个处理流程，从场景背景中分割人体点云，跟踪个体随时间变化，并进行身体部位分割；支持来自深度传感器和单目深度估计的点云；核心是一种新的3D动作识别骨干网络，结合基于点云的技术和应用于体素映射点云序列的稀疏卷积网络；集成辅助点特征，包括表面法线、颜色、红外强度和身体部位解析标签，以提高识别准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB-D 120数据集上的评估表明，该方法与现有的骨骼动作识别算法具有竞争力；将基于传感器和估计的深度输入以集成方式结合，当考虑不同受试者进行训练和测试时，准确率达到89.3%；这种方法优于之前的点云动作识别方法。&lt;h4&gt;结论&lt;/h4&gt;提出的3D动作识别方法是一种有效的新方法，能够利用密集3D数据进行动作识别，并且在性能上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的人类动作识别研究主要集中在骨骼动作识别和基于视频的方法。随着消费级深度传感器和激光雷达设备的日益普及，利用密集3D数据进行动作识别，发展第三种方法的机会正在增加。本文提出了一种从3D视频识别动作的新方法，引入了一个流程，从场景背景中分割人体点云，随时间跟踪个体，并执行身体部位分割。该方法支持来自深度传感器和单目深度估计的点云。所提出的HAR框架的核心是一种用于3D动作识别的新型骨干网络，它将基于点云的技术与应用于体素映射点云序列的稀疏卷积网络相结合。实验集成了辅助点特征，包括表面法线、颜色、红外强度和身体部位解析标签，以提高识别准确性。在NTU RGB-D 120数据集上的评估表明，该方法与现有的骨骼动作识别算法具有竞争力。此外，在集成设置中同时结合基于传感器和估计的深度输入，当考虑不同受试者进行训练和测试时，这种方法达到89.3%的准确率，超越了之前的点云动作识别方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从3D点云序列识别人类动作的问题。随着消费级深度传感器和激光雷达设备的普及，利用3D点云数据进行动作识别成为一种有前景的'第三种方式'，补充了传统的基于视频和骨骼的方法。这个问题很重要，因为动作识别在监控、老年人跌倒检测、体育分析和自动驾驶等领域有广泛应用价值，而现有方法各有局限性：视频方法需要降低帧尺寸限制了精细动作识别，骨骼方法依赖可能出错的关键点估计。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后设计了适用于两种场景的流程：深度传感器获取的真实点云和通过单目深度估计从RGB视频生成的点云。方法借鉴了多项现有工作：使用M2FP模型进行人体分割，ByteTrack算法进行跟踪，迭代最远点采样进行点采样，DBSCAN进行去噪，以及Depth-Anything v2进行深度估计。在网络架构上借鉴了PointNet的T-Net模块和ResNet的残差设计。作者的创新在于将这些技术整合到一个完整的点云动作识别流程中，并设计了新的SP-HP-ConvoT骨干网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接利用3D点云序列进行动作识别，结合点处理技术和稀疏卷积网络的优势，并融合多种辅助特征提高识别精度。整体流程分为三部分：1) 点云获取：对深度传感器输入进行人体分割、3D投影和去噪；对RGB输入先进行单目深度估计再进行相同处理；2) 特征提取：计算表面法线等特征，使用迭代最远点采样固定点数；3) 动作识别：使用T-Net嵌入、体素映射、稀疏CNN骨干（包括MS-TCN层）、全局池化和全连接分类器进行最终分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 完整的点云处理流程，包含人体分割、跟踪和身体部位分割；2) 同时支持深度传感器和单目深度估计两种输入场景；3) 新的SP-HP-ConvoT骨干网络，结合点处理和稀疏卷积；4) 多模态特征融合，包括表面法线、颜色等；5) 高效处理多人场景。相比之前工作，不同之处在于：提供了更精确的人体和身体部位分割，结合了点处理和稀疏卷积方法，支持从RGB估计点云，不依赖可能出错的关键点估计，保留了更丰富的3D空间信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新颖的从点云序列识别人类动作的方法，通过结合点处理技术和稀疏卷积网络，并适应两种输入场景，在NTU RGB-D 120数据集上实现了与现有骨骼动作识别算法相竞争的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research into human action recognition (HAR) has focused predominantlyon skeletal action recognition and video-based methods. With the increasingavailability of consumer-grade depth sensors and Lidar instruments, there is agrowing opportunity to leverage dense 3D data for action recognition, todevelop a third way. This paper presents a novel approach for recognizingactions from 3D videos by introducing a pipeline that segments human pointclouds from the background of a scene, tracks individuals over time, andperforms body part segmentation. The method supports point clouds from bothdepth sensors and monocular depth estimation. At the core of the proposed HARframework is a novel backbone for 3D action recognition, which combinespoint-based techniques with sparse convolutional networks applied tovoxel-mapped point cloud sequences. Experiments incorporate auxiliary pointfeatures including surface normals, color, infrared intensity, and body partparsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D120 dataset demonstrates that the method is competitive with existing skeletalaction recognition algorithms. Moreover, combining both sensor-based andestimated depth inputs in an ensemble setup, this approach achieves 89.3%accuracy when different human subjects are considered for training and testing,outperforming previous point cloud action recognition methods.</description>
      <author>example@mail.com (James Dickens)</author>
      <guid isPermaLink="false">2510.05506v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</title>
      <link>http://arxiv.org/abs/2510.05070v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ResMimic是一种双阶段残差学习框架，用于从人类运动数据中学习精确且富有表现力的人形机器人全身运动控制，解决了现有通用运动跟踪方法在运动操作中缺乏精确性和物体意识的问题。&lt;h4&gt;背景&lt;/h4&gt;人形全身运动操作对于日常服务和仓库任务具有变革性潜力，但现有的通用运动跟踪(GMT)方法虽然能重现多样的人类运动，却缺乏运动操作所需的精确性和物体意识。&lt;h4&gt;目的&lt;/h4&gt;开发一种从人类运动数据中学习精确且富有表现力的人形机器人控制方法，以实现高效的运动操作任务。&lt;h4&gt;方法&lt;/h4&gt;ResMimic采用双阶段残差学习框架：第一阶段使用大规模纯人类运动数据训练的GMT策略作为基础生成类人全身运动；第二阶段学习高效但精确的残差策略优化GMT输出，改进运动并加入物体交互。还设计了三种辅助训练技术：基于点云的物体跟踪奖励、接触奖励和基于课程的虚拟物体控制器。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实Unitree G1人形机器人上的评估结果显示，ResMimic与强基线相比在任务成功率、训练效率和鲁棒性方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;ResMimic框架有效地解决了人形机器人运动操作中的精确性和物体意识问题，为机器人在日常服务和仓库任务中的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;人形全身运动操作承诺为日常服务和仓库任务带来变革性能力。虽然最近的通用运动跟踪(GMT)进展使人形机器人能够重现多样的人类运动，但这些策略缺乏运动操作所需的精确性和物体意识。为此，我们引入ResMimic，一种从人类运动数据中进行精确且富有表现力的人形机器人控制的双阶段残差学习框架。首先，在大型纯人类运动数据上训练的GMT策略作为生成类人全身运动的任务不可知基础。然后，学习一个高效但精确的残差策略来优化GMT输出，改进运动并整合物体交互。为进一步促进高效训练，我们设计了(i)基于点云的物体跟踪奖励，用于更平滑的优化；(ii)接触奖励，鼓励精确的人形身体-物体交互；以及(iii)基于课程的虚拟物体控制器，以稳定早期训练。我们在仿真和真实的Unitree G1人形机器人上评估了ResMimic。结果表明，与强基线相比，ResMimic在任务成功率、训练效率和鲁棒性方面有显著提升。视频可在https://resmimic.github.io/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid whole-body loco-manipulation promises transformative capabilitiesfor daily service and warehouse tasks. While recent advances in general motiontracking (GMT) have enabled humanoids to reproduce diverse human motions, thesepolicies lack the precision and object awareness required forloco-manipulation. To this end, we introduce ResMimic, a two-stage residuallearning framework for precise and expressive humanoid control from humanmotion data. First, a GMT policy, trained on large-scale human-only motion,serves as a task-agnostic base for generating human-like whole-body movements.An efficient but precise residual policy is then learned to refine the GMToutputs to improve locomotion and incorporate object interaction. To furtherfacilitate efficient training, we design (i) a point-cloud-based objecttracking reward for smoother optimization, (ii) a contact reward thatencourages accurate humanoid body-object interactions, and (iii) acurriculum-based virtual object controller to stabilize early training. Weevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Resultsshow substantial gains in task success, training efficiency, and robustnessover strong baselines. Videos are available at https://resmimic.github.io/ .</description>
      <author>example@mail.com (Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan)</author>
      <guid isPermaLink="false">2510.05070v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Platonic Transformers: A Solid Choice For Equivariance</title>
      <link>http://arxiv.org/abs/2510.03511v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为柏拉图Transformer的新型模型，通过引入柏拉图立体对称群参考帧的注意力机制，解决了传统Transformers缺乏几何对称性归纳偏置的问题，同时保持了模型的高效性和灵活性。&lt;h4&gt;背景&lt;/h4&gt;Transformers模型虽然应用广泛，但缺乏科学和计算机视觉中常见的几何对称性的归纳偏置。现有的等变方法通常通过复杂、计算密集型设计牺牲了Transformers原本的高效性和灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，使Transformers能够处理几何对称性问题，同时不损失其原有的效率和灵活性优势。&lt;h4&gt;方法&lt;/h4&gt;引入柏拉图Transformer，通过定义相对于柏拉图立体对称群参考帧的注意力机制，诱导出有原则的权重共享方案，使模型对连续平移和柏拉图对称性都具有等变性，同时保持标准Transformer的架构和计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的注意力机制在形式上等价于动态群卷积，揭示了模型能够学习自适应几何滤波器，并 enables一个高度可扩展的线性时间卷积变体。&lt;h4&gt;结论&lt;/h4&gt;在计算机视觉(CIFAR-10)、3D点云(ScanObjectNN)和分子性质预测(QM9, OMol25)等多样化基准测试中，柏拉图Transformer通过利用几何约束实现了具有竞争力的性能，且没有额外计算成本。&lt;h4&gt;翻译&lt;/h4&gt;虽然Transformer应用广泛，但缺乏科学和计算机视觉中常见的几何对称性的归纳偏置。现有的等变方法通常通过复杂、计算密集型设计牺牲了使Transformers如此有效的高效性和灵活性。我们引入柏拉图Transformer来解决这一权衡问题。通过定义相对于柏拉图立体对称群参考帧的注意力，我们的方法诱导出一种有原则的权重共享方案。这使得模型对连续平移和柏拉图对称性都具有等变性，同时保留了标准Transformer的精确架构和计算成本。此外，我们证明这种注意力在形式上等价于动态群卷积，这揭示了模型学习自适应几何滤波器，并 enables一个高度可扩展的线性时间卷积变体。在计算机视觉(CIFAR-10)、3D点云(ScanObjectNN)和分子性质预测(QM9, OMol25)等多样化基准测试中，柏拉图Transformer通过利用这些几何约束实现了具有竞争力的性能，且没有额外成本。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决Transformer模型缺乏处理几何对称性的归纳偏置问题。这一问题在科学和计算机视觉领域至关重要，因为物理、分子化学和3D计算机视觉等领域的数据具有固有的几何对称性。现有的等变方法虽然能处理对称性，但往往牺牲了Transformer的高效性和灵活性，导致难以扩展到大规模应用。没有几何对称性处理能力的模型在处理科学数据时效果不佳，限制了模型在分子预测、3D点云处理等任务中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到Transformer通过位置编码可以处理平移不变性，但无法处理旋转等更复杂的几何变换。他们受Rotary Position Embeddings (RoPE)启发，RoPE通过相对位置编码使Transformer具有平移等变性。作者将这一思路扩展到处理更复杂的几何变换，特别是旋转和反射。他们借鉴了RoPE的位置编码机制、群等变神经网络的群论处理方法以及多头自注意力机制的设计，但创新性地将柏拉图立体(Platonic solids)的对称性群作为参考帧，实现了在保持Transformer计算效率的同时引入几何对称性处理能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入柏拉图立体对称性群作为参考帧，在这些参考帧上并行计算注意力，并通过权重共享机制使模型能够从任何方向学习通用的注意力模式。整体实现流程包括：1) 特征提升：将输入特征提升为群函数，使其相对于每个参考帧有定义；2) 等变线性变换：使用群卷积代替标准矩阵乘法，确保线性层等变；3) 注意力计算：在多个参考帧上并行计算注意力，每个帧使用RoPE处理相对位置；4) 输出组合：将所有参考帧的输出组合，得到最终结果。这种方法保持了标准Transformer的计算图和计算成本，同时引入了几何对称性处理能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Platonic Transformer架构：首次将柏拉图立体对称性群集成到Transformer中；2) 权重共享机制：通过群卷积实现权重共享，在不增加计算成本的情况下引入几何偏置；3) 等变性注意力：实现了对连续平移和离散旋转/反射的等变性；4) 动态群卷积等价性：证明了RoPE注意力等价于动态群卷积，并提出了线性时间复杂度的变体；5) 计算效率：保持了标准Transformer的计算图和计算成本。相比之前的工作，它不像传统等变网络需要复杂的计算操作，不像混合架构需要打破对称性，不像不变注意力机制牺牲特征表示，也不像帧平均方法需要为每个帧元素单独前向传播。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Platonic Transformer通过引入柏拉图立体对称性群的参考帧和权重共享机制，在保持标准Transformer计算效率的同时，实现了对几何变换的等变性处理，解决了对称性感知与可扩展性之间的长期权衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While widespread, Transformers lack inductive biases for geometric symmetriescommon in science and computer vision. Existing equivariant methods oftensacrifice the efficiency and flexibility that make Transformers so effectivethrough complex, computationally intensive designs. We introduce the PlatonicTransformer to resolve this trade-off. By defining attention relative toreference frames from the Platonic solid symmetry groups, our method induces aprincipled weight-sharing scheme. This enables combined equivariance tocontinuous translations and Platonic symmetries, while preserving the exactarchitecture and computational cost of a standard Transformer. Furthermore, weshow that this attention is formally equivalent to a dynamic group convolution,which reveals that the model learns adaptive geometric filters and enables ahighly scalable, linear-time convolutional variant. Across diverse benchmarksin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecularproperty prediction (QM9, OMol25), the Platonic Transformer achievescompetitive performance by leveraging these geometric constraints at noadditional cost.</description>
      <author>example@mail.com (Mohammad Mohaiminul Islam, Rishabh Anand, David R. Wessels, Friso de Kruiff, Thijs P. Kuipers, Rex Ying, Clara I. Sánchez, Sharvaree Vadgama, Georg Bökman, Erik J. Bekkers)</author>
      <guid isPermaLink="false">2510.03511v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Unified Unsupervised Anomaly Detection via Matching Cost Filtering</title>
      <link>http://arxiv.org/abs/2510.03363v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  63 pages (main paper and supplementary material), 39 figures, 58  tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出统一成本过滤(UCF)框架，通过可学习过滤模块减轻匹配噪声并突出细微异常，在单模态和多模态UAD场景中均取得最先进结果。&lt;h4&gt;背景&lt;/h4&gt;无监督异常检测(UAD)在工业检测和医疗分析等领域有广泛应用，但因隐私问题和冷启动约束，异常样本稀缺。现有方法主要进行图像或特征级匹配，但匹配噪声被忽视，限制了检测能力。从单模态RGB UAD扩展到多模态场景，但这些研究方向相互隔离，阻碍了全面理解和知识转移。&lt;h4&gt;目的&lt;/h4&gt;从匹配角度提出统一单模态和多模态UAD的方法，开发通用后优化框架提升各种UAD模型性能。&lt;h4&gt;方法&lt;/h4&gt;构建测试样本与来自相同或不同模态正常样本间的匹配成本体积，使用测试样本的多层注意力引导的可学习过滤模块减轻匹配噪声并突出细微异常。&lt;h4&gt;主要发现&lt;/h4&gt;在22个多样化基准测试上，UCF能有效增强各种UAD方法，在单模态(RGB)和多模态(RGB-3D, RGB-Text)UAD场景中持续实现新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;UCF是适用于各种UAD模型的通用后优化方法，能够有效提升异常检测性能，代码和模型将在指定网址发布。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常检测(UAD)旨在仅使用正常训练数据识别图像和像素级别的异常，在工业检测和医疗分析等领域有广泛应用，但因隐私问题和冷启动约束，异常样本稀缺。现有方法，无论是基于重建（恢复正常对应物）还是基于嵌入（预训练表示），根本上都进行图像或特征级匹配来生成异常图。然而，匹配噪声在很大程度上被忽视，限制了它们的检测能力。除了早期基于单模态RGB的UAD关注外，最近的进展扩展到多模态场景，例如RGB-3D和RGB-Text，这得益于点云传感和视觉语言模型。尽管面临共同挑战，但这些研究方向在很大程度上仍然相互隔离，阻碍了全面理解和知识转移。在本文中，我们从匹配角度倡导统一单模态和多模态设置的UAD。在这一洞察下，我们提出了统一成本过滤(UCF)，这是一个通用的后优化细化框架，用于细化任何UAD模型的异常成本体积。成本体积是通过将测试样本与来自相同或不同模态的正常样本进行匹配构建的，然后是测试样本的多层注意力引导的可学习过滤模块，减轻匹配噪声并突出细微异常。在22个多样化基准测试上的全面实验证明了UCF在增强各种UAD方法方面的有效性，在单模态(RGB)和多模态(RGB-3D, RGB-Text)UAD场景中持续实现新的最先进结果。代码和模型将在https://github.com/ZHE-SAPI/CostFilter-AD发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无监督异常检测(UAD)中的匹配噪声问题。这个问题在现实中非常重要，因为UAD在工业检测、医疗分析等领域有广泛应用，但这些领域中异常样本由于隐私问题和冷启动约束而稀缺。匹配噪声会导致模糊的边界、假阳性和假阴性，特别是对于细微缺陷、低对比度或接近正常的区域，严重影响检测的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从匹配角度重新概念化单模态和多模态UAD，明确指出被忽视的匹配噪声问题。他们借鉴了立体匹配、深度估计、光流估计和光场渲染等领域的'匹配成本过滤'概念，将UAD重构为特征提取、异常成本体积构建和异常成本体积过滤的三步范式。UCF设计为通用后处理框架，可集成到各种UAD方法中。作者还使用了注意力机制、残差连接等深度学习技术，并引入多模板匹配和类感知适配器来增强检测能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将异常检测重新概念化为匹配成本过滤过程，通过构建和过滤异常成本体积来抑制匹配噪声，同时保留边缘结构和细微异常。整体流程包括：1)准备参考模板(重建RGB模板、3D点云或文本提示)；2)提取输入和模板的多层特征；3)构建异常成本体积(通过相似度匹配)；4)使用3D U-Net和双流注意力引导(输入特征和初始异常图)过滤成本体积；5)生成最终异常图。训练时使用合成异常掩码，推理时与基线响应加权结合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)重新概念化UAD，明确解决匹配噪声问题；2)提出三步范式(特征提取、成本体积构建和过滤)；3)设计通用UCF框架；4)引入双流注意力引导机制；5)采用多模板匹配策略；6)添加类感知适配器。相比之前工作，UCF明确处理匹配噪声问题，提供统一框架适用于所有模态，作为通用插件可集成到各种方法中，在22个基准测试上取得state-of-the-art结果，并能更好保留边缘结构和检测细微异常。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的统一成本过滤(UCF)框架通过构建和过滤异常成本体积有效抑制了匹配噪声，显著提升了单模态和多模态无监督异常检测的性能，为UAD领域提供了一个统一且通用的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomaly detection (UAD) aims to identify image- and pixel-levelanomalies using only normal training data, with wide applications such asindustrial inspection and medical analysis, where anomalies are scarce due toprivacy concerns and cold-start constraints. Existing methods, whetherreconstruction-based (restoring normal counterparts) or embedding-based(pretrained representations), fundamentally conduct image- or feature-levelmatching to generate anomaly maps. Nonetheless, matching noise has been largelyoverlooked, limiting their detection ability. Beyond earlier focus on unimodalRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB-3D andRGB-Text, enabled by point cloud sensing and vision-language models. Despiteshared challenges, these lines remain largely isolated, hindering acomprehensive understanding and knowledge transfer. In this paper, we advocateunified UAD for both unimodal and multimodal settings in the matchingperspective. Under this insight, we present Unified Cost Filtering (UCF), ageneric post-hoc refinement framework for refining anomaly cost volume of anyUAD model. The cost volume is constructed by matching a test sample againstnormal samples from the same or different modalities, followed by a learnablefiltering module with multi-layer attention guidance from the test sample,mitigating matching noise and highlighting subtle anomalies. Comprehensiveexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF inenhancing a variety of UAD methods, consistently achieving new state-of-the-artresults in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) UAD scenarios.Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.</description>
      <author>example@mail.com (Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu)</author>
      <guid isPermaLink="false">2510.03363v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs</title>
      <link>http://arxiv.org/abs/2510.06747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需训练和标签的短文本聚类方法，可应用于任何现有嵌入器。该方法基于迭代向量更新，通过代表性文本构建稀疏向量，并在LLM指导下进行迭代优化。&lt;h4&gt;背景&lt;/h4&gt;在面向客户的聊天机器人场景中，公司需要处理大量用户话语并根据其意图进行聚类。在这些商业环境中，通常没有标记数据可用，且聚类数量未知。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练和标签的短文本聚类方法，适用于任何现有嵌入器，能够在没有预先了解聚类或标签的情况下进行聚类，并且具有可扩展性和低资源适应性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于迭代向量更新的方法：首先基于代表性文本构建稀疏向量，然后通过LLM指导进行迭代优化这些向量。&lt;h4&gt;主要发现&lt;/h4&gt;1) 该方法在不预先了解聚类或标签的情况下，能够达到与使用对比学习的最先进方法相当或更好的结果；2) 在不同数据集和小型LLM上的实验表明，该方法与模型无关，可应用于任何嵌入器、小型LLM和不同聚类方法；3) 该方法可以扩展到大型数据集，降低LLM的计算成本。&lt;h4&gt;结论&lt;/h4&gt;这种低资源、可适应的设置以及方法的可扩展性，使其比现有的聚类方法更符合实际场景。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种无需训练和标签的短文本聚类方法，可以应用于任何现有的嵌入器。在面向客户的聊天机器人背景下，公司需要处理大量用户话语，这些话语需要根据其意图进行聚类。在这些商业环境中，通常没有标记数据可用，且聚类数量未知。我们的方法基于迭代向量更新：它基于代表性文本构建稀疏向量，然后通过LLM指导迭代优化这些向量。我们的方法在不假设预先了解聚类或标签的情况下，能够达到与使用对比学习的最先进方法相当或更好的结果。在不同数据集和小型LLM上的实验表明，我们的方法与模型无关，可以应用于任何嵌入器、小型LLM和不同的聚类方法。我们还表明，我们的方法可以扩展到大型数据集，降低LLM的计算成本。这种低资源、可适应的设置以及方法的可扩展性，使其比现有的聚类方法更符合实际场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a training-free and label-free method for shorttext clustering that can be used on top of any existing embedder. In thecontext of customer-facing chatbots, companies are dealing with large amountsof user utterances that need to be clustered according to their intent. Inthese commercial settings, no labeled data is typically available, and thenumber of clusters is not known. Our method is based on iterative vectorupdating: it constructs sparse vectors based on representative texts, and theniteratively refines them through LLM guidance. Our method achieves comparableor superior results to state-of-the-art methods that use contrastive learning,but without assuming prior knowledge of clusters or labels. Experiments ondiverse datasets and smaller LLMs show that our method is model agnostic andcan be applied to any embedder, with relatively small LLMs, and differentclustering methods. We also show that our method scales to large datasets,reducing the computational cost of the LLM. These low-resource, adaptablesettings and the scalability of our method make it more aligned with real-worldscenarios than existing clustering methods.</description>
      <author>example@mail.com (I-Fan Lin, Faegheh Hasibi, Suzan Verberne)</author>
      <guid isPermaLink="false">2510.06747v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</title>
      <link>http://arxiv.org/abs/2510.07181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TIGeR是一种新框架，将视觉语言模型从感知估计器转变为几何计算机，通过外部工具实现厘米级精度的几何计算，在几何推理基准测试上达到SOTA性能，并在现实机器人操作任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在空间推理方面显示出显著能力，但本质上仅限于定性精度，缺乏机器人技术所需的计算精度。当前方法未能利用深度传感器和相机校准的度量线索，而是将几何问题降级为无法实现机器人操作所需厘米级精度的模式识别任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，使视觉语言模型从感知估计器转变为几何计算机，通过外部工具使模型能够生成和执行精确的几何计算，实现机器人操作所需的厘米级精度。&lt;h4&gt;方法&lt;/h4&gt;TIGeR(工具集成几何推理)框架不试图在神经网络内部化复杂的几何操作，而是赋予模型识别几何推理需求、合成适当计算代码并调用专门库进行精确计算的能力。研究团队引入了TIGeR-300K数据集，包含点变换、姿态估计、轨迹生成和空间兼容性验证，并采用两阶段训练管道：监督微调(SFT)和强化微调(RFT)，配合分层奖励设计。&lt;h4&gt;主要发现&lt;/h4&gt;TIGeR在几何推理基准测试上实现了SOTA性能，在现实世界机器人操作任务中表现出厘米级精度。&lt;h4&gt;结论&lt;/h4&gt;TIGeR框架成功将视觉语言模型从感知估计器转变为几何计算机，通过外部工具实现了精确的几何计算，解决了视觉语言模型在机器人应用中的精度限制问题。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在空间推理方面显示出显著能力，但它们本质上仅限于定性精度，缺乏机器人技术所需的计算精度。当前方法未能利用深度传感器和相机校准的度量线索，而是将几何问题降级为无法实现机器人操作所需厘米级精度的模式识别任务。我们提出了TIGeR(工具集成几何推理)，这是一种新框架，通过使视觉语言模型能够通过外部工具生成和执行精确的几何计算，将其从感知估计器转变为几何计算机。TIGeR不试图在神经网络内部化复杂的几何操作，而是赋予模型识别几何推理需求、合成适当计算代码并调用专门库进行精确计算的能力。为支持这一范式，我们引入了TIGeR-300K，这是一个全面的面向工具调用的数据集，涵盖点变换、姿态估计、轨迹生成和空间兼容性验证，包含工具调用序列和中间计算。通过结合监督微调(SFT)和我们提出的分层奖励设计的强化微调(RFT)的两阶段训练管道，TIGeR在几何推理基准测试上实现了SOTA性能，同时在现实世界机器人操作任务中表现出厘米级精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型(VLMs)在精确几何计算方面的局限性，使其无法满足机器人操作所需的厘米级精度。这个问题很重要，因为现实世界中的机器人需要精确计算3D位置、距离和姿态等几何信息，而现有VLM只能提供'左侧'、'可到达'等定性评估，无法支持机器人完成精确操作任务，如物体抓取、放置和路径规划等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到VLMs在几何推理方面的根本局限，因此提出不要试图在神经网络内部复杂化几何运算，而是让模型识别几何推理需求，生成适当代码并调用外部工具执行。他们借鉴了工具集成推理(TIR)和空间理解领域的工作，但创新性地提出了新的基于过程的奖励函数和两阶段训练流程(SFT+RFT)，结合了模板合成数据和大模型重写数据，确保几何概念的全面覆盖和模型的泛化能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将VLM从感知估计器转变为几何计算机，通过外部工具实现精确几何计算。整体流程包括：1)工具分类(视觉感知工具和几何计算工具)；2)数据生成(TIGeR-300K数据集，包含模板合成和大模型重写两种数据)；3)两阶段训练(监督微调SFT初始化工具使用能力，强化微调RFT使用分层奖励函数提升精度)；4)推理过程(模型识别几何需求，生成代码，调用工具执行计算，返回结果)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)概念与方法创新，将VLM转变为几何计算机；2)TIGeR-300K数据集，专为几何推理设计；3)两阶段SFT→RFT训练流程和分层奖励设计。相比之前工作，不同之处在于：不试图在神经网络内部复杂化几何运算；提出新的基于过程的奖励函数；结合模板合成和大模型重写数据；在真实机器人任务中实现厘米级精度；支持跨视点的统一推理，即使多视角相机没有联合校准也能保持一致的数值推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TIGeR通过工具集成几何推理框架，使视觉语言模型能够从感知估计器转变为精确的几何计算机，实现了厘米级精度的机器人操作能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown remarkable capabilities in spatialreasoning, yet they remain fundamentally limited to qualitative precision andlack the computational precision required for real-world robotics. Currentapproaches fail to leverage metric cues from depth sensors and cameracalibration, instead reducing geometric problems to pattern recognition tasksthat cannot deliver the centimeter-level accuracy essential for roboticmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novelframework that transforms VLMs from perceptual estimators to geometriccomputers by enabling them to generate and execute precise geometriccomputations through external tools. Rather than attempting to internalizecomplex geometric operations within neural networks, TIGeR empowers models torecognize geometric reasoning requirements, synthesize appropriatecomputational code, and invoke specialized libraries for exact calculations. Tosupport this paradigm, we introduce TIGeR-300K, a comprehensivetool-invocation-oriented dataset covering point transformations, poseestimation, trajectory generation, and spatial compatibility verification,complete with tool invocation sequences and intermediate computations. Througha two-stage training pipeline combining supervised fine-tuning (SFT) andreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,TIGeR achieves SOTA performance on geometric reasoning benchmarks whiledemonstrating centimeter-level precision in real-world robotic manipulationtasks.</description>
      <author>example@mail.com (Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang)</author>
      <guid isPermaLink="false">2510.07181v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2510.04759v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://yanchi-3dv.github.io/PG-Occ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PG-Occ的创新渐进式高斯Transformer框架，用于实现开放词汇的3D占用预测，解决了稀疏表示难以捕捉小物体而密集表示计算开销大的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;3D占用预测任务近年来在基于视觉的自动驾驶系统中扮演关键角色。传统方法局限于固定语义类别，而最近的方法转向预测文本对齐的特征以实现开放词汇查询，但存在稀疏和密集表示的权衡问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉细粒度场景细节同时保持计算效率的3D占用预测框架，实现开放词汇场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出PG-Occ框架，采用渐进式在线密集化策略逐步增强3D高斯表示；引入各向异性感知采样策略结合时空融合，自适应分配不同尺度和阶段高斯的感受野，实现更有效的特征聚合和场景信息捕获。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛评估，PG-Occ实现了最先进的性能，比之前表现最好的方法相对提高了14.3%的mIoU。&lt;h4&gt;结论&lt;/h4&gt;PG-Over框架成功解决了文本对齐场景建模中的权衡问题，能够实现更精确和详细的场景理解，代码和预训练模型将在项目页面发布。&lt;h4&gt;翻译&lt;/h4&gt;3D占用预测任务近年来取得了显著进展，在基于视觉的自动驾驶系统中发挥着关键作用。虽然传统方法局限于固定的语义类别，但最近的方法已转向预测文本对齐的特征，以实现真实场景中的开放词汇文本查询。然而，在文本对齐的场景建模中存在权衡：稀疏高斯表示难以捕捉场景中的小物体，而密集表示则会带来显著的计算开销。为解决这些限制，我们提出了PG-Occ，一种创新的渐进式高斯Transformer框架，可实现开放词汇的3D占用预测。我们的框架采用渐进式在线密集化，这是一种前馈策略，能够逐步增强3D高斯表示以捕获细粒度的场景细节。通过迭代增强表示，框架实现越来越精确和详细的场景理解。另一个关键贡献是引入了各向异性感知采样策略结合时空融合，能够自适应地为不同尺度和阶段的高斯分配感受野，实现更有效的特征聚合和更丰富的场景信息捕获。通过广泛评估，我们证明PG-Occ实现了最先进的性能，比之前表现最好的方法相对提高了14.3%的mIoU。代码和预训练模型将在发布时在我们的项目页面上提供：https://yanchi-3dv.github.io/PG-Occ&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D占用预测中的开放词汇问题，即让系统能够识别和定位任意文本提示描述的物体，而不仅限于预定义的语义类别。这个问题在自动驾驶和 embodied intelligence 领域至关重要，因为现实世界中存在大量未预定义的物体类别，系统能够通过文本指令灵活识别这些物体对于安全导航和交互至关重要。此外，现有方法在捕捉小物体细节和计算效率之间存在权衡，限制了实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法受限于预定义类别，而开放词汇方法在稀疏表示难以捕捉小物体和密集表示计算开销大之间存在权衡。他们借鉴了3D高斯溅射技术用于场景表示，Transformer架构用于特征聚合，以及CLIP等开放词汇视觉-语言模型进行文本对齐。设计思路是采用渐进式方法，从粗到细逐步增强场景表示，同时通过非对称自注意力机制确保训练稳定性，利用各向异性感知采样提高特征提取效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过渐进式高斯建模和各向异性感知采样，实现高效的开放词汇3D占用预测，将场景表示为可扩展的文本对齐特征高斯点集。整体流程包括：1)初始化阶段从伪深度图生成初始高斯；2)渐进式高斯建模，包含基础层和多个渐进层，每层使用渐进式在线密度化添加新高斯；3)各向异性感知特征采样，根据高斯形状和方向进行特征提取；4)使用2D监督训练，结合深度和特征损失；5)最终将高斯表示转换为密集3D占用场，支持任意文本查询的语义定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)渐进式高斯Transformer框架，通过迭代增强表示捕捉细粒度细节；2)各向异性感知采样策略，根据高斯特性自适应分配感受野；3)非对称自注意力机制，防止新高斯干扰已优化高斯。相比之前工作，PG- adaptively 扩展高斯查询数量而非使用固定数量，能够更好地建模复杂场景；保持高斯表示的稀疏性降低计算开销；实现前馈推理无需离线优化；更有效地平衡开放词汇能力和计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PG-Occ通过渐进式高斯建模和各向异性感知采样，首次实现了高效且高精度的开放词汇3D占用预测，突破了预定义语义类别的限制，显著提升了自动驾驶系统的环境感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D occupancy prediction task has witnessed remarkable progress in recentyears, playing a crucial role in vision-based autonomous driving systems. Whiletraditional methods are limited to fixed semantic categories, recent approacheshave moved towards predicting text-aligned features to enable open-vocabularytext queries in real-world scenes. However, there exists a trade-off intext-aligned scene modeling: sparse Gaussian representation struggles tocapture small objects in the scene, while dense representation incurssignificant computational overhead. To address these limitations, we presentPG-Occ, an innovative Progressive Gaussian Transformer Framework that enablesopen-vocabulary 3D occupancy prediction. Our framework employs progressiveonline densification, a feed-forward strategy that gradually enhances the 3DGaussian representation to capture fine-grained scene details. By iterativelyenhancing the representation, the framework achieves increasingly precise anddetailed scene understanding. Another key contribution is the introduction ofan anisotropy-aware sampling strategy with spatio-temporal fusion, whichadaptively assigns receptive fields to Gaussians at different scales andstages, enabling more effective feature aggregation and richer sceneinformation capture. Through extensive evaluations, we demonstrate that PG-Occachieves state-of-the-art performance with a relative 14.3% mIoU improvementover the previous best performing method. Code and pretrained models will bereleased upon publication on our project page:https://yanchi-3dv.github.io/PG-Occ</description>
      <author>example@mail.com (Chi Yan, Dan Xu)</author>
      <guid isPermaLink="false">2510.04759v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.07210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyPlan的新型混合学习辅助规划方法，用于解决自动驾驶汽车在部分可观察交通环境中的无碰撞导航问题。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶汽车需要在部分可观察的交通环境中安全导航，这面临着挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够安全且高效导航的方法，减少执行时间而不影响驾驶安全。&lt;h4&gt;方法&lt;/h4&gt;HyPlan结合了多智能体行为预测、深度强化学习（使用近端策略优化）和近似在线部分可观察马尔可夫决策过程规划（使用基于启发式置信度的垂直剪枝）。&lt;h4&gt;主要发现&lt;/h4&gt;在CARLA-CTS2基准测试中，HyPlan在包含行人的关键交通场景测试中，比选定的相关基线方法导航更安全，并且比考虑的其他在线POMDP规划器显著更快。&lt;h4&gt;结论&lt;/h4&gt;HyPlan是一种有效的混合方法，能够在保证安全的同时提高导航效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为HyPlan的新型混合学习辅助规划方法，用于解决自动驾驶汽车在部分可观察交通环境中的无碰撞导航问题。HyPlan结合了多智能体行为预测方法、使用近端策略优化的深度强化学习和基于启发式置信度的垂直剪枝的近似在线部分可观察马尔可夫决策过程规划，以减少执行时间而不影响驾驶安全。我们在CARLA-CTS2基准测试中对包含行人的关键交通场景进行的实验性能分析表明，HyPlan可能比选定的相关基线导航更安全，并且比考虑的其他在线POMDP规划器显著更快。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel hybrid learning-assisted planning method, named HyPlan,for solving the collision-free navigation problem for self-driving cars inpartially observable traffic environments. HyPlan combines methods formulti-agent behavior prediction, deep reinforcement learning with proximalpolicy optimization and approximated online POMDP planning with heuristicconfidence-based vertical pruning to reduce its execution time withoutcompromising safety of driving. Our experimental performance analysis on theCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealedthat HyPlan may navigate safer than selected relevant baselines and performsignificantly faster than considered alternative online POMDP planners.</description>
      <author>example@mail.com (Donald Pfaffmann, Matthias Klusch, Marcel Steinmetz)</author>
      <guid isPermaLink="false">2510.07210v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Does Physics Knowledge Emerge in Frontier Models?</title>
      <link>http://arxiv.org/abs/2510.06251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures. Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究前沿视觉-语言模型在物理动态理解和预测方面的能力，发现尽管这些模型在视觉感知和一般推理方面表现出色，但在物理动态理解方面存在明显不足，感知和物理推理能力呈碎片化状态，未能有效结合形成因果理解。&lt;h4&gt;背景&lt;/h4&gt;前沿的视觉-语言模型(VLMs)在视觉感知和一般推理方面显示出强大结果，但它们理解和预测物理动态的能力仍然不清楚。&lt;h4&gt;目的&lt;/h4&gt;评估前沿VLMs在物理模拟任务中的表现，探究感知能力与物理推理能力之间的关系，揭示当前VLMs在物理动态理解方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;在三个物理模拟数据集(CLEVRER、Physion和Physion++)上对六个前沿VLMs进行基准测试，评估任务包括预测结果或假设替代情况；设计诊断子测试分离感知(物体、颜色、遮挡物)与物理推理(运动预测、空间关系)能力；分析感知/物理推理性能与评估准确性之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;模型在感知或物理推理方面的出色表现并不一致地转化为预测或反事实评估的更高准确性；感知和物理推理技能之间存在弱相关性；当前VLMs的感知和物理推理能力仍然是碎片化的，未能结合成因果理解。&lt;h4&gt;结论&lt;/h4&gt;当前VLMs存在一个核心局限：感知和物理推理技能未能有效结合形成因果理解，这表明需要开发能够更紧密地绑定感知和推理的架构。&lt;h4&gt;翻译&lt;/h4&gt;前沿视觉-语言模型(VLMs)在视觉感知和一般推理方面显示出强大的结果，但它们理解和预测物理动态的能力仍然不清楚。我们在三个物理模拟数据集 - CLEVRER、Physion和Physion++上对六个前沿VLMs进行了基准测试，其中评估任务测试模型是否可以预测结果或对替代情况提出假设。为了进行更深入的探究，我们设计了诊断子测试，将感知(物体、颜色、遮挡物)与物理推理(运动预测、空间关系)分离出来。直观地来看，更强的诊断性能应该支持更高的评估准确性。然而，我们的分析揭示了弱相关性：在感知或物理推理方面表现出色的模型并不总是在预测或反事实评估中表现更好。这种违反直觉的差距暴露了当前VLMs的一个核心局限：感知和物理技能仍然是碎片化的，未能结合成因果理解，这强调了需要开发能够更紧密地绑定感知和推理的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leading Vision-Language Models (VLMs) show strong results in visualperception and general reasoning, but their ability to understand and predictphysical dynamics remains unclear. We benchmark six frontier VLMs on threephysical simulation datasets - CLEVRER, Physion, and Physion++ - where theevaluation tasks test whether a model can predict outcomes or hypothesize aboutalternative situations. To probe deeper, we design diagnostic subtests thatisolate perception (objects, colors, occluders) from physics reasoning (motionprediction, spatial relations). Intuitively, stronger diagnostic performanceshould support higher evaluation accuracy. Yet our analysis reveals weakcorrelations: models that excel at perception or physics reasoning do notconsistently perform better on predictive or counterfactual evaluation. Thiscounterintuitive gap exposes a central limitation of current VLMs: perceptualand physics skills remain fragmented and fail to combine into causalunderstanding, underscoring the need for architectures that bind perception andreasoning more tightly.</description>
      <author>example@mail.com (Ieva Bagdonaviciute, Vibhav Vineet)</author>
      <guid isPermaLink="false">2510.06251v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects</title>
      <link>http://arxiv.org/abs/2510.06952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种文本到3D对抗生成方法(Phy3DAdvGen)，能够生成对LiDAR检测器不可见的3D物体模型，并在物理环境中实现，有效揭示了自动驾驶系统中LiDAR检测器的安全漏洞。&lt;h4&gt;背景&lt;/h4&gt;LiDAR 3D目标检测器是自动驾驶的基础，未能检测到物体可能带来严重的安全风险。开发有效的3D对抗攻击对于彻底测试这些检测系统并在实际部署前发现其漏洞至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入文本到3D对抗生成方法，实现物理上可实现的攻击，生成对LiDAR检测器真正不可见的3D物体模型，并且可以在现实世界中轻松实现。&lt;h4&gt;方法&lt;/h4&gt;通过系统研究行人3D模型的拓扑、连通性和强度对检测的影响，提出物理感知的文本到3D对抗生成方法，迭代优化文本提示生成LiDAR不可见的行人，并基于包含13个真实物体3D模型的物体池生成3D物体以确保物理可实现性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够生成3D行人，在CARLA模拟环境和物理环境中都能规避六种最先进的LiDAR 3D检测器，突显了安全关键应用中的漏洞。&lt;h4&gt;结论&lt;/h4&gt;物理上可实现的对抗攻击方法能够有效测试LiDAR检测系统，揭示其安全漏洞，强调了在实际部署前进行此类测试的重要性。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D目标检测器是自动驾驶的基础，未能检测到物体会带来严重的安全风险。开发有效的3D对抗攻击对于彻底测试这些检测系统并在实际部署前暴露其漏洞至关重要。然而，现有的向3D点添加优化扰动的对抗攻击有两个关键局限：它们很少导致物体完全消失，并且在物理环境中难以实现。我们引入了文本到3D对抗生成方法，这是一种新颖的方法，能够实现物理上可实现的攻击，生成对LiDAR检测器真正不可见的3D物体模型，并且可以在现实世界中轻松实现。具体而言，我们进行了第一个实证研究，通过操纵行人3D模型的拓扑、连通性和强度，并将行人与多个物体结合，系统研究了影响检测漏洞的因素。基于这些见解，我们提出了物理感知的文本到3D对抗生成(Phy3DAdvGen)方法，通过迭代优化动词、物体和姿势来生成对LiDAR不可见的行人。为确保物理可实现性，我们构建了一个包含13个真实物体3D模型的综合物体池，并限制Phy3DAdvGen基于该集合中的物体组合生成3D物体。大量实验表明，我们的方法能够生成3D行人，在CARLA模拟环境和物理环境中都能规避六种最先进的(SOTA)LiDAR 3D检测器，从而突显了安全关键应用中的漏洞。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成物理上可实现的3D对抗样本，使这些样本能够欺骗LiDAR 3D目标检测器，让检测器无法检测到这些物体。这个问题在现实中非常重要，因为自动驾驶系统中LiDAR检测是基础组件，如果无法检测到行人等物体可能导致严重安全事故；在研究中，开发有效的对抗攻击能帮助测试系统漏洞并提高鲁棒性，而现有方法很少能实现物体完全消失且难以在物理环境中部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先在CARLA仿真环境中系统研究了影响LiDAR检测漏洞的因素，发现物体组合对检测率影响最大。他们借鉴了文本到3D生成模型(如基于高斯溅射的LGM)和对抗攻击领域的工作，但创新性地将这些技术结合。设计方法包括构建动词-物体-姿势(VOP)文本提示空间，通过对抗优化寻找能生成LiDAR不可见物体的提示，并使用物理对象池确保物理可实现性，最终形成Phy3DAdvGen框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过优化文本提示来生成物理可实现且能规避LiDAR检测的3D物体。整体流程包括：1) 构建VOP文本提示；2) 使用检测器置信度作为目标进行对抗优化；3) 将优化后的提示输入文本到3D生成模型(如LGM)生成3D物体；4) 通过物理对象池和多视图渲染实现物理部署，调整真实物体的位置、旋转和缩放以模拟生成的对抗物体。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统研究3D物体在LiDAR场景中的可检测性因素；2) 提出Phy3DAdvGen框架，通过优化离散文本组件生成对抗性3D物体；3) 使用物理对象池确保现实世界可行性；4) 在模拟和物理环境中验证攻击效果。相比之前工作，本文方法不直接扰动点云或优化网格几何，而是通过文本提示生成对抗内容，提供了更大的优化空间、更好的物理可实现性和更强的攻击效果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于文本提示优化的物理可实现3D对抗生成方法，能生成对LiDAR检测器完全不可见的物体，并在真实环境中验证了其有效性，揭示了自动驾驶感知系统的重要安全漏洞。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D object detectors are fundamental to autonomous driving, wherefailing to detect objects poses severe safety risks. Developing effective 3Dadversarial attacks is essential for thoroughly testing these detection systemsand exposing their vulnerabilities before real-world deployment. However,existing adversarial attacks that add optimized perturbations to 3D points havetwo critical limitations: they rarely cause complete object disappearance andprove difficult to implement in physical environments. We introduce thetext-to-3D adversarial generation method, a novel approach enabling physicallyrealizable attacks that can generate 3D models of objects truly invisible toLiDAR detectors and be easily realized in the real world. Specifically, wepresent the first empirical study that systematically investigates the factorsinfluencing detection vulnerability by manipulating the topology, connectivity,and intensity of individual pedestrian 3D models and combining pedestrians withmultiple objects within the CARLA simulation environment. Building on theinsights, we propose the physically-informed text-to-3D adversarial generation(Phy3DAdvGen) that systematically optimizes text prompts by iterativelyrefining verbs, objects, and poses to produce LiDAR-invisible pedestrians. Toensure physical realizability, we construct a comprehensive object poolcontaining 13 3D models of real objects and constrain Phy3DAdvGen to generate3D objects based on combinations of objects in this set. Extensive experimentsdemonstrate that our approach can generate 3D pedestrians that evade sixstate-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation andphysical environments, thereby highlighting vulnerabilities in safety-criticalapplications.</description>
      <author>example@mail.com (Bing Li, Wuqi Wang, Yanan Zhang, Jingzheng Li, Haigen Min, Wei Feng, Xingyu Zhao, Jie Zhang, Qing Guo)</author>
      <guid isPermaLink="false">2510.06952v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder</title>
      <link>http://arxiv.org/abs/2510.07289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MolGA方法，通过灵活整合多样的分子领域知识，将预训练的2D图编码器适应到下游分子应用中，有效解决了现有方法在整合分子知识方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;分子图表示学习在化学和生物医学研究中广泛应用。虽然预训练的2D图编码器表现出强大性能，但它们忽视了与亚分子实例（原子和键）相关的丰富分子领域知识。而分子预训练方法虽将此类知识纳入预训练目标，但通常采用针对特定类型知识的设计，缺乏整合分子中多样知识的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够重用广泛可用且经验证的预训练2D编码器，同时在下游适应过程中整合分子领域知识，提供一种更实用的替代方案。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MolGA方法，包括：1)分子对齐策略，弥合预训练拓扑表示与领域知识表示之间的差距；2)条件适应机制，生成特定于实例的标记，实现分子领域知识的细粒度整合。&lt;h4&gt;主要发现&lt;/h4&gt;作者在十一个公共数据集上进行了广泛的实验，证明了MolGA的有效性。&lt;h4&gt;结论&lt;/h4&gt;MolGA通过灵活整合多样的分子领域知识，有效地将预训练的2D图编码器适应到下游分子应用中，是一种更实用的方法。&lt;h4&gt;翻译&lt;/h4&gt;分子图表示学习在化学和生物医学研究中得到广泛应用。虽然预训练的2D图编码器已展现出强大的性能，但它们忽视了与亚分子实例（原子和键）相关的丰富分子领域知识。尽管分子预训练方法将此类知识纳入其预训练目标，但它们通常采用针对特定类型知识的设计，缺乏整合分子中存在的多样知识的灵活性。因此，重用广泛可用且经验证的预训练2D编码器，同时在下游适应过程中整合分子领域知识，提供了一种更实用的替代方案。在这项工作中，我们提出了MolGA，它通过灵活整合多样的分子领域知识，将预训练的2D图编码器适应到下游分子应用中。首先，我们提出了一个分子对齐策略，弥合预训练拓扑表示与领域知识表示之间的差距。其次，我们引入了一个条件适应机制，生成特定于实例的标记，以实现分子领域知识在下游任务中的细粒度整合。最后，我们在十一个公共数据集上进行了广泛的实验，证明了MolGA的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular graph representation learning is widely used in chemical andbiomedical research. While pre-trained 2D graph encoders have demonstratedstrong performance, they overlook the rich molecular domain knowledgeassociated with submolecular instances (atoms and bonds). While molecularpre-training approaches incorporate such knowledge into their pre-trainingobjectives, they typically employ designs tailored to a specific type ofknowledge, lacking the flexibility to integrate diverse knowledge present inmolecules. Hence, reusing widely available and well-validated pre-trained 2Dencoders, while incorporating molecular domain knowledge during downstreamadaptation, offers a more practical alternative. In this work, we proposeMolGA, which adapts pre-trained 2D graph encoders to downstream molecularapplications by flexibly incorporating diverse molecular domain knowledge.First, we propose a molecular alignment strategy that bridge the gap betweenpre-trained topological representations with domain-knowledge representations.Second, we introduce a conditional adaptation mechanism that generatesinstance-specific tokens to enable fine-grained integration of molecular domainknowledge for downstream tasks. Finally, we conduct extensive experiments oneleven public datasets, demonstrating the effectiveness of MolGA.</description>
      <author>example@mail.com (Xingtong Yu, Chang Zhou, Xinming Zhang, Yuan Fang)</author>
      <guid isPermaLink="false">2510.07289v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</title>
      <link>http://arxiv.org/abs/2510.07191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了DINOv3自监督学习模型在胸部X光检查中的应用效果，与DINOv2和ImageNet初始化方法进行了比较，确定了最佳输入分辨率为512x512像素，并发现ConvNeXt-B骨干网络表现优于ViT-B/16。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在视觉表征学习方面取得了进展，但其在胸部X光检查这种具有精细发现的高容量成像模式中的价值尚不明确。Meta的DINOv3通过Gram锚定的自蒸馏扩展了早期的SSL模型，但这些设计选择是否改善了胸部X光检查的迁移学习尚未经过系统测试。&lt;h4&gt;目的&lt;/h4&gt;评估DINOv3在胸部X光检查中的应用效果，与DINOv2和ImageNet初始化进行比较，确定最佳输入分辨率，以及评估不同骨干网络的表现。&lt;h4&gt;方法&lt;/h4&gt;在七个数据集(超过814,000个样本)上进行了基准测试，评估了ViT-B/16和ConvNeXt-B两个骨干网络，分析了224x224、512x512和1024x1024像素的图像，还评估了来自7B模型的冻结特征，主要结果是标签的平均AUROC。&lt;h4&gt;主要发现&lt;/h4&gt;在224x224分辨率下，DINOv3和DINOv2在成人数据集上表现相当；将分辨率提高到512x512时，DINOv3一致优于DINOv2和ImageNet；在儿科队列中，不同初始化方法之间没有差异；在所有设置中，ConvNeXt-B都优于ViT-B/16；使用冻结特征的模型性能低于完全微调的骨干网络；扩展到1024x1024并没有进一步提高准确性；分辨率相关的增益在边界依赖性和小焦点异常方面最为明显。&lt;h4&gt;结论&lt;/h4&gt;在胸部X光检查中，更高的输入分辨率对于利用现代自监督模型的益处至关重要；512x512像素代表了一个实用的上限，其中DINOv3初始化的ConvNeXt-B网络提供最强性能；临床应用上，支持在512x512分辨率下使用微调的中型骨干网络进行胸部X光解释，预计在检测细微或边界为中心的病变时会有最大收益。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习在视觉表征学习方面取得了进展，但其在胸部X光检查这种具有精细发现的高容量成像模式中的价值尚不明确。Meta的DINOv3通过Gram锚定的自蒸馏扩展了早期的SSL模型。这些设计选择是否改善了胸部X光检查的迁移学习尚未经过系统测试。我们在七个数据集上对DINOv3与DINOv2和ImageNet初始化进行了基准测试（样本量超过814,000）。评估了两个代表性的骨干网络：ViT-B/16和ConvNeXt-B。分析了224x224、512x512和1024x1024像素的图像。我们还评估了来自7B模型的冻结特征。主要结果是标签的平均AUROC。在224x224分辨率下，DINOv3和DINOv2在成人数据集上表现相当。将分辨率提高到512x512时，DINOv3一致优于DINOv2和ImageNet。相比之下，儿科队列的结果显示不同初始化方法之间没有差异。在所有设置中，ConvNeXt-B都优于ViT-B/16。使用冻结DINOv3-7B特征的模型性能低于完全微调的86-89M参数骨干网络，突显了领域适应的重要性。扩展到1024x1024并没有进一步提高准确性。分辨率相关的增益在边界依赖性和小焦点异常方面最为明显。在胸部X光检查中，更高的输入分辨率对于利用现代自监督模型的益处至关重要。512x512像素代表了一个实用的上限，其中DINOv3初始化的ConvNeXt-B网络提供最强性能，而更大的输入提供的成本回报最小。从临床角度来看，这些发现支持在512x512分辨率下使用微调的中型骨干网络进行胸部X光解释，预计在检测与急诊和重症监护环境相关的细微或边界为中心的病变时会有最大收益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has advanced visual representation learning,but its value in chest radiography, a high-volume imaging modality withfine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSLmodels through Gram-anchored self-distillation. Whether these design choicesimprove transfer learning for chest radiography has not been systematicallytested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization acrossseven datasets (n&gt;814,000). Two representative backbones were evaluated:ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and1024x1024 pixels. We additionally assessed frozen features from a 7B model. Theprimary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2achieved comparable performance on adult datasets. Increasing resolution to512x512 yielded consistent improvements for DINOv3 over both DINOv2 andImageNet. In contrast, results in pediatric cohort showed no differences acrossinitializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Modelsusing frozen DINOv3-7B features underperformed relative to fully finetuned86-89M-parameter backbones, highlighting the importance of domain adaptation.Scaling to 1024x1024 did not further improve accuracy. Resolution-related gainswere most evident for boundary-dependent and small focal abnormalities. Inchest radiography, higher input resolution is critical for leveraging thebenefits of modern self-supervised models. 512x512 pixels represent a practicalupper limit where DINOv3-initialized ConvNeXt-B networks provide the strongestperformance, while larger inputs offer minimal return on cost. Clinically,these findings support use of finetuned, mid-sized backbones at 512x512 forchest radiograph interpretation, with the greatest gains expected in detectingsubtle or boundary-centered lesions relevant to emergency and critical caresettings.</description>
      <author>example@mail.com (Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn)</author>
      <guid isPermaLink="false">2510.07191v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging</title>
      <link>http://arxiv.org/abs/2510.07182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Bridged Clustering是一种半监督框架，通过独立聚类输入和输出数据，并使用少量配对示例学习聚类间的稀疏桥梁，实现高效预测。&lt;h4&gt;背景&lt;/h4&gt;传统半监督学习方法未充分利用输出数据，而密集传输方法缺乏稀疏性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出一种从任意未配对的输入X和输出Y数据集中学习预测器的半监督框架。&lt;h4&gt;方法&lt;/h4&gt;首先独立地对X和Y进行聚类，然后使用少量配对示例学习聚类间的稀疏、可解释桥梁；推理时将新输入分配到最近输入聚类，返回链接输出聚类的质心作为预测值。&lt;h4&gt;主要发现&lt;/h4&gt;方法明确利用仅输出数据，保持稀疏且可解释的对齐；在有界的错误聚类和错误桥接率下成为有效预测器；在低监督设置中与最先进方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;Bridged Clustering是一种有效的半监督学习方法，能够充分利用未配对的输入和输出数据，同时保持模型的可解释性和效率。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Bridged Clustering，一种半监督框架，可从任意未配对的输入X和输出Y数据集中学习预测器。我们的方法首先独立地对X和Y进行聚类，然后仅使用少量配对示例学习聚类之间的稀疏、可解释的桥梁。在推理时，新输入x被分配到其最近的输入聚类，链接输出聚类的质心被返回作为预测值ŷ。与传统SSL不同，Bridged Clustering明确利用仅输出数据，与密集传输方法不同，它保持稀疏且可解释的对齐。通过理论分析，我们表明在有界的错误聚类和错误桥接率下，我们的算法成为有效且高效的预测器。从经验上看，我们的方法与最先进方法具有竞争力，同时保持简单、模型无关和在低监督设置中的高标签效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Bridged Clustering, a semi-supervised framework to learnpredictors from any unpaired input $X$ and output $Y$ dataset. Our method firstclusters $X$ and $Y$ independently, then learns a sparse, interpretable bridgebetween clusters using only a few paired examples. At inference, a new input$x$ is assigned to its nearest input cluster, and the centroid of the linkedoutput cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,Bridged Clustering explicitly leverages output-only data, and unlike densetransport-based methods, it maintains a sparse and interpretable alignment.Through theoretical analysis, we show that with bounded mis-clustering andmis-bridging rates, our algorithm becomes an effective and efficient predictor.Empirically, our method is competitive with SOTA methods while remainingsimple, model-agnostic, and highly label-efficient in low-supervision settings.</description>
      <author>example@mail.com (Patrick Peixuan Ye, Chen Shani, Ellen Vitercik)</author>
      <guid isPermaLink="false">2510.07182v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration</title>
      <link>http://arxiv.org/abs/2510.07035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlexMol是一种灵活的分子预训练框架，能够学习统一的分子表示并支持单模态输入，克服了现有方法在缺少某种模态数据时的局限性&lt;h4&gt;背景&lt;/h4&gt;分子表示学习在药物发现和材料设计等应用中发挥关键作用，现有方法需要成对的2D和3D分子数据进行预训练以捕捉全面的分子结构信息&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理单模态输入的分子表示学习方法，解决现有方法在某种模态数据不可用或计算成本高昂时的局限性&lt;h4&gt;方法&lt;/h4&gt;提出FlexMol框架，为2D和3D分子数据使用单独模型，通过参数共享提高计算效率，利用解码器生成缺失模态特征，实现多阶段连续学习过程&lt;h4&gt;主要发现&lt;/h4&gt;FlexMol在多种分子属性预测任务上表现出色，实验证明其对不完整数据也有效&lt;h4&gt;结论&lt;/h4&gt;FlexMol成功实现了统一分子表示学习并支持单模态输入，为分子表示学习提供了更灵活的解决方案&lt;h4&gt;翻译&lt;/h4&gt;分子表示学习在推进药物发现和材料设计等应用中发挥着关键作用。现有工作利用分子信息的2D和3D模态进行预训练，旨在捕捉全面的结构和几何洞察。然而，这些方法需要成对的2D和3D分子数据来有效训练模型并防止其坍缩为单一模态，这在某种模态不可用或计算成本高昂的场景中存在限制。为了克服这一限制，我们提出了FlexMol，一个灵活的分子预训练框架，它学习统一的分子表示，同时支持单模态输入。具体而言，受视觉语言模型中统一结构的启发，我们的方法为2D和3D分子数据使用单独的模型，利用参数共享提高计算效率，并使用解码器生成缺失模态的特征。这使得多阶段连续学习过程成为可能，在训练期间两种模态协同贡献，同时确保在推理期间只有一种模态可用时的鲁棒性。大量实验表明，FlexMol在广泛的分子属性预测任务上实现了卓越的性能，我们也 empirically 证明了其在不完整数据上的有效性。我们的代码和数据可在 https://github.com/tewiSong/FlexMol 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761084&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular representation learning plays a crucial role in advancingapplications such as drug discovery and material design. Existing workleverages 2D and 3D modalities of molecular information for pre-training,aiming to capture comprehensive structural and geometric insights. However,these methods require paired 2D and 3D molecular data to train the modeleffectively and prevent it from collapsing into a single modality, posinglimitations in scenarios where a certain modality is unavailable orcomputationally expensive to generate. To overcome this limitation, we proposeFlexMol, a flexible molecule pre-training framework that learns unifiedmolecular representations while supporting single-modality input. Specifically,inspired by the unified structure in vision-language models, our approachemploys separate models for 2D and 3D molecular data, leverages parametersharing to improve computational efficiency, and utilizes a decoder to generatefeatures for the missing modality. This enables a multistage continuouslearning process where both modalities contribute collaboratively duringtraining, while ensuring robustness when only one modality is available duringinference. Extensive experiments demonstrate that FlexMol achieves superiorperformance across a wide range of molecular property prediction tasks, and wealso empirically demonstrate its effectiveness with incomplete data. Our codeand data are available at https://github.com/tewiSong/FlexMol.</description>
      <author>example@mail.com (Tengwei Song, Min Wu, Yuan Fang)</author>
      <guid isPermaLink="false">2510.07035v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Relational Database Distillation: From Structured Tables to Condensed Graph Data</title>
      <link>http://arxiv.org/abs/2510.06980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了关系数据库蒸馏(RDD)问题，旨在将大规模关系数据库压缩为紧凑的异构图，同时保留预测能力。通过保留多模态列信息和主-外键关系，并设计了基于核脊回归的目标函数，该方法显著减少了数据大小，同时保持了在分类和回归任务上的竞争性能。&lt;h4&gt;背景&lt;/h4&gt;关系数据库(RDBs)支撑着全球大多数数据管理系统，其中信息被结构化为多个相互依赖的表格。最近的进展利用图表示学习来捕获表格间的复杂关系作为多跳依赖，但这些方法由于数据库的巨大规模和表格间密集消息传递的计算负担，仍然面临存储开销过大和训练时间过长的问题。&lt;h4&gt;目的&lt;/h4&gt;解决关系数据库在预测任务中面临的存储和计算效率问题，通过蒸馏方法将大规模关系数据库压缩为紧凑的异构图，同时保留足够的预测能力，使图模型能够高效训练和使用。&lt;h4&gt;方法&lt;/h4&gt;提出了关系数据库蒸馏(RDD)方法，将大规模RDBs蒸馏为紧凑的异构图；通过节点特征保留多模态列信息；通过异构边编码主-外键关系，维护数据完整性和关系结构；设计基于核脊回归和伪标签的目标函数，避免传统低效的双级蒸馏框架，确保蒸馏后的图能适应多样化的下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界RDBs上的大量实验表明，该解决方案显著减少了数据大小，同时在分类和回归任务上保持了竞争性能，为RDBs的可扩展学习创造了有效途径。&lt;h4&gt;结论&lt;/h4&gt;关系数据库蒸馏方法能够有效解决大规模关系数据库在预测任务中的存储和计算效率问题，通过将数据库压缩为紧凑的异构图同时保留关键信息，为处理大规模关系数据库提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;关系数据库(RDBs)支撑着全球大多数数据管理系统，其中信息被结构化为多个相互依赖的表格。为了有效利用RDBs中的知识进行预测任务，最近的进展利用图表示学习来捕获表格间的复杂关系作为多跳依赖。尽管取得了最先进的性能，但由于数据库的巨大规模和表格间密集消息传递的计算负担，这些方法仍然受到存储开销过大和训练时间过长的限制。为了缓解这些问题，我们提出了并研究了关系数据库蒸馏(RDD)问题。具体而言，我们旨在将大规模RDBs蒸馏为紧凑的异构图，同时保留训练基于图模型的预测能力所需的效用。多模态列信息通过节点特征保留，主-外键关系通过异构边编码，从而维护数据完整性和关系结构。为确保在不采用传统低效的双级蒸馏框架的情况下适应多样化的下游任务，我们进一步设计了基于核脊回归和伪标签的目标函数，为蒸馏后的图生成高质量特征。在多个真实世界RDBs上的大量实验表明，我们的解决方案显著减少了数据大小，同时在分类和回归任务上保持了竞争性能，为RDBs的可扩展学习创造了有效途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational databases (RDBs) underpin the majority of global data managementsystems, where information is structured into multiple interdependent tables.To effectively use the knowledge within RDBs for predictive tasks, recentadvances leverage graph representation learning to capture complex inter-tablerelations as multi-hop dependencies. Despite achieving state-of-the-artperformance, these methods remain hindered by the prohibitive storage overheadand excessive training time, due to the massive scale of the database and thecomputational burden of intensive message passing across interconnected tables.To alleviate these concerns, we propose and study the problem of RelationalDatabase Distillation (RDD). Specifically, we aim to distill large-scale RDBsinto compact heterogeneous graphs while retaining the predictive power (i.e.,utility) required for training graph-based models. Multi-modal columninformation is preserved through node features, and primary-foreign keyrelations are encoded via heterogeneous edges, thereby maintaining both datafidelity and relational structure. To ensure adaptability across diversedownstream tasks without engaging the traditional, inefficient bi-leveldistillation framework, we further design a kernel ridge regression-guidedobjective with pseudo-labels, which produces quality features for the distilledgraph. Extensive experiments on multiple real-world RDBs demonstrate that oursolution substantially reduces the data size while maintaining competitiveperformance on classification and regression tasks, creating an effectivepathway for scalable learning with RDBs.</description>
      <author>example@mail.com (Xinyi Gao, Jingxi Zhang, Lijian Chen, Tong Chen, Lizhen Cui, Hongzhi Yin)</author>
      <guid isPermaLink="false">2510.06980v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Learning Global Representation from Queries for Vectorized HD Map Construction</title>
      <link>http://arxiv.org/abs/2510.06969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MapGR的新型架构，用于在线构建高清矢量地图，通过全局表示学习提高自动驾驶系统中地图构建的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;在线构建高清地图是现代自动驾驶系统的基石。当前最先进方法基于DETR框架，将其作为实例检测问题处理，但这些方法依赖于独立的可学习对象查询，导致主要采用局部查询视角，忽视了高清地图中固有的全局表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够学习和利用查询中全局表示的架构，以改进高清地图的构建质量。&lt;h4&gt;方法&lt;/h4&gt;提出MapGR（Global Representation learning for HD Map construction）架构，包含两个协同模块：1）全局表示学习模块，通过精心设计的整体分割任务使查询分布与全局地图更好对齐；2）全局表示指导模块，为每个查询提供显式的全局级上下文信息，促进优化过程。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse2数据集上的评估验证了MapGR方法的有效性，与最先进的基线相比，在平均精度均值方面有显著提高。&lt;h4&gt;结论&lt;/h4&gt;通过引入全局表示学习机制，MapGR成功解决了传统方法中忽视全局表示的问题，显著提升了高清地图构建的性能。&lt;h4&gt;翻译&lt;/h4&gt;在线构建高清矢量地图是现代自动驾驶系统的基石。最先进的方法，特别是基于DETR框架的方法，将其作为实例检测问题来处理。然而，它们依赖于独立的、可学习的对象查询，导致主要采用局部查询视角，忽视了高清地图中固有的全局表示。在这项工作中，我们提出了MapGR（高清地图的全局表示学习），这是一种旨在学习和利用查询中全局表示的架构。我们的方法引入了两个协同模块：全局表示学习模块，通过精心设计的整体分割任务，鼓励所有查询的分布更好地与全局地图对齐；以及全局表示指导模块，为每个单独的查询提供显式的全局级上下文信息，促进其优化。在nuScenes和Argoverse2数据集上的评估验证了我们方法的有效性，与最先进的基线相比，平均精度均值有显著提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在线高清地图（HD Map）构建中的全局表示学习问题。当前基于DETR框架的方法主要将地图构建视为实例检测问题，但它们依赖于独立的、可学习的对象查询，导致查询主要具有局部视角，忽略了高清地图中固有的全局表示。这个问题在现实中非常重要，因为在线高清地图构建是现代自动驾驶系统的基石，它为安全高效的导航提供高精度的地图元素感知。与传统高清地图相比，在线高清地图能更好地适应不断变化的道路条件，如施工区域、车道修改和意外障碍。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有DETR-like框架在HD地图构建中的局限性，特别是它们处理'连续分布'（HD地图在BEV空间中的空间连续性）时的不足，而DETR框架主要针对'独立分布'（独立目标的空间分布）进行了优化。作者发现现有方法通过手动实例分区来应用DETR框架，但这会导致信息损失并忽略全局结构信息。因此，作者提出从所有对象查询中直接学习全局HD地图表示，然后利用这个表示促进每个单独查询的学习。该方法借鉴了DETR框架及其变体（如Deformable DETR、Conditional DETR等）以及HD地图构建领域的现有工作（如MapTR、VectorMapNet等），但创新性地引入了全局表示学习思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从所有查询中学习全局地图表示，而不是仅仅关注单个实例，并通过全局表示来指导局部查询的优化，使每个查询在优化的同时保持全局视角。整体实现流程包括：1)输入处理：从多视角图像提取特征并投影到BEV空间；2)查询处理：使用多层Transformer解码器解码实例预测；3)全局表示学习(GRL)模块：将查询聚合为全局表示并计算与地面真实地图的损失；4)全局表示指导(GRG)模块：将全局信息融入每个查询并使用融合后的查询进行最终预测；5)输出：生成向量化的高清地图元素。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于全局表示学习的HD地图构建方法MapGR；2)设计全局表示学习(GRL)模块，增强查询的全局分布学习；3)提出全局表示指导(GRG)模块，通过全局信息指导单个查询优化。相比之前工作的不同：1)采用全局视角而非局部视角，考虑了地图元素间的空间关系和结构依赖性；2)通过全局表示将梯度传播到所有实例查询，而非仅成功匹配的预测；3)学习全局地图表示而非仅实例级表示；4)设计为即插即用模块，可与现有方法如MapTR系列兼容；5)专门针对HD地图的'连续分布'特性进行优化，而非DETR框架主要针对的'独立分布'。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种通过从查询中学习全局表示来增强高清地图构建的方法，引入了全局表示学习和全局表示指导两个互补模块，显著提升了地图构建的准确性和一致性，同时保持了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The online construction of vectorized high-definition (HD) maps is acornerstone of modern autonomous driving systems. State-of-the-art approaches,particularly those based on the DETR framework, formulate this as an instancedetection problem. However, their reliance on independent, learnable objectqueries results in a predominantly local query perspective, neglecting theinherent global representation within HD maps. In this work, we propose\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD\textbf{Map} construction), an architecture designed to learn and utilize aglobal representations from queries. Our method introduces two synergisticmodules: a Global Representation Learning (GRL) module, which encourages thedistribution of all queries to better align with the global map through acarefully designed holistic segmentation task, and a Global RepresentationGuidance (GRG) module, which endows each individual query with explicit,global-level contextual information to facilitate its optimization. Evaluationson the nuScenes and Argoverse2 datasets validate the efficacy of our approach,demonstrating substantial improvements in mean Average Precision (mAP) comparedto leading baselines.</description>
      <author>example@mail.com (Shoumeng Qiu, Xinrun Li, Yang Long, Xiangyang Xue, Varun Ojha, Jian Pu)</author>
      <guid isPermaLink="false">2510.06969v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Angular Constraint Embedding via SpherePair Loss for Constrained Clustering</title>
      <link>http://arxiv.org/abs/2510.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025, 6 Figures and 1 Table in Main text, 18  Figures and 5 Tables in Appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SpherePair的新型角度约束嵌入方法，用于深度受约束聚类(DCC)，解决了现有方法在端到端建模中受限于锚点或难以学习判别性欧几里得嵌入的问题。&lt;h4&gt;背景&lt;/h4&gt;受约束聚类通过成对约束整合领域知识，但现有深度受约束聚类方法要么受限于端到端建模中的锚点，要么难以学习判别性欧几里得嵌入，限制了它们的可扩展性和实际应用性。&lt;h4&gt;目的&lt;/h4&gt;为了避免现有DCC方法的缺陷，提出一种新颖的角度约束嵌入方法，实现更有效的聚类表示学习。&lt;h4&gt;方法&lt;/h4&gt;使用SpherePair损失和几何公式，该方法忠实地编码成对约束，产生在角度空间中聚类友好的嵌入，有效分离表示学习与聚类过程。&lt;h4&gt;主要发现&lt;/h4&gt;SpherePair能够在无冲突情况下保留成对关系，无需指定确切聚类数量，可推广到未见数据，能快速推断聚类数量，并有严格理论保证支持。&lt;h4&gt;结论&lt;/h4&gt;与最先进DCC方法在各种基准上的比较评估及理论见解的经验验证，证实了SpherePair的优越性能、可扩展性和实际有效性。&lt;h4&gt;翻译&lt;/h4&gt;受约束聚类通过成对约束整合领域知识。然而，现有的深度受约束聚类方法要么受限于端到端建模中的锚点，要么难以学习判别性欧几里得嵌入，限制了它们的可扩展性和实际应用性。为了避免各自的缺陷，我们提出了一种新颖的DCC角度约束嵌入方法，称为SpherePair。使用SpherePair损失和几何公式，我们的方法忠实地编码成对约束，并导致在角度空间中聚类友好的嵌入，有效地将表示学习与聚类分离。SpherePair在没有冲突的情况下保留成对关系，无需指定确切的聚类数量，可以推广到未见数据，能够快速推断聚类数量，并有严格的理论保证支持。与最先进的DCC方法在各种基准上的比较评估，以及对理论见解的经验验证，证实了其优越的性能、可扩展性和整体实际有效性。代码可在我们的仓库获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constrained clustering integrates domain knowledge through pairwiseconstraints. However, existing deep constrained clustering (DCC) methods areeither limited by anchors inherent in end-to-end modeling or struggle withlearning discriminative Euclidean embedding, restricting their scalability andreal-world applicability. To avoid their respective pitfalls, we propose anovel angular constraint embedding approach for DCC, termed SpherePair. Usingthe SpherePair loss with a geometric formulation, our method faithfully encodespairwise constraints and leads to embeddings that are clustering-friendly inangular space, effectively separating representation learning from clustering.SpherePair preserves pairwise relations without conflict, removes the need tospecify the exact number of clusters, generalizes to unseen data, enables rapidinference of the number of clusters, and is supported by rigorous theoreticalguarantees. Comparative evaluations with state-of-the-art DCC methods ondiverse benchmarks, along with empirical validation of theoretical insights,confirm its superior performance, scalability, and overall real-worldeffectiveness. Code is available at\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.</description>
      <author>example@mail.com (Shaojie Zhang, Ke Chen)</author>
      <guid isPermaLink="false">2510.06907v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Dual Goal Representations</title>
      <link>http://arxiv.org/abs/2510.06714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种用于目标条件强化学习的双重目标表示方法，该方法通过状态与其他所有状态的时间距离关系来表征状态，具有理论优势并能提升目标到达性能。&lt;h4&gt;背景&lt;/h4&gt;目标条件强化学习(GCRL)领域需要更有效的状态表示方法，以捕捉环境动态并提高目标到达性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于状态间时间距离关系的双重目标表示方法，该方法能够捕捉环境内在动态，提供足够信息以恢复最优目标到达策略，并过滤外部噪声。&lt;h4&gt;方法&lt;/h4&gt;提出双重目标表示，将状态表征为'从所有其他状态的时间距离的集合'；基于此概念开发了一种可结合任何现有GCRL算法的目标表示学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;双重目标表示具有两个重要理论特性：1)仅依赖于环境的内在动态，对原始状态表示不变；2)包含足够信息以恢复最优目标到达策略，同时能过滤外部噪声。在OGBench任务套件上的20个基于状态和像素的实验中，双重目标表示一致提高了离线目标到达性能。&lt;h4&gt;结论&lt;/h4&gt;双重目标表示是一种有效的目标条件强化学习方法，能够提升各种任务中的目标到达性能，且可与现有GCRL算法结合使用。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们为目标条件强化学习(GCRL)引入了双重目标表示。双重目标表示将状态定义为'从所有其他状态的时间距离的集合'；换句话说，它通过状态与其他每个状态的关系（以时间距离测量）来编码状态。这种表示提供了几个有吸引力的理论特性。首先，它仅依赖于环境的内在动态，且对原始状态表示不变。其次，它包含足够的信息来恢复最优的目标到达策略，同时能够过滤外部噪声。基于这一概念，我们开发了一种实用的目标表示学习方法，可以与任何现有的GCRL算法结合。通过在OGBench任务套件上的多样化实验，我们经验性地表明，双重目标表示在20个基于状态和像素的任务中一致地提高了离线目标到达性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce dual goal representations for goal-conditionedreinforcement learning (GCRL). A dual goal representation characterizes a stateby "the set of temporal distances from all other states"; in other words, itencodes a state through its relations to every other state, measured bytemporal distance. This representation provides several appealing theoreticalproperties. First, it depends only on the intrinsic dynamics of the environmentand is invariant to the original state representation. Second, it containsprovably sufficient information to recover an optimal goal-reaching policy,while being able to filter out exogenous noise. Based on this concept, wedevelop a practical goal representation learning method that can be combinedwith any existing GCRL algorithm. Through diverse experiments on the OGBenchtask suite, we empirically show that dual goal representations consistentlyimprove offline goal-reaching performance across 20 state- and pixel-basedtasks.</description>
      <author>example@mail.com (Seohong Park, Deepinder Mann, Sergey Levine)</author>
      <guid isPermaLink="false">2510.06714v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Latent Representation Learning in Heavy-Ion Collisions with MaskPoint Transformer</title>
      <link>http://arxiv.org/abs/2510.06691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, accepted at the NeurIPS 2025 workshop "Machine  Learning and the Physical Sciences"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的自编码器方法，通过两阶段训练范式从重离子碰撞数据中提取信息特征，显著提高了分类准确率并捕获了复杂非线性相关性。&lt;h4&gt;背景&lt;/h4&gt;高能核物理的核心挑战是从重离子碰撞的高维最终态数据中提取信息特征，传统方法依赖选定可观测量，可能遗漏数据中细微但物理相关的结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从重离子碰撞数据中提取信息丰富特征的方法，以实现可靠的下游分析和物理现象研究。&lt;h4&gt;方法&lt;/h4&gt;引入基于Transformer的自编码器，采用两阶段训练范式：1)自监督预训练，从无标签HIC数据学习潜在表示；2)监督微调，适应特定物理任务。应用主成分分析和SHAP解释评估特征质量。&lt;h4&gt;主要发现&lt;/h4&gt;1)该方法在大和小碰撞系统分类中显著优于PointNet；2)自编码器捕获了超越单个可观测量的复杂非线性相关性；3)产生的特征具有强判别力和解释力。&lt;h4&gt;结论&lt;/h4&gt;两阶段框架作为HIC中特征学习的通用和稳健基础，为分析夸克-胶子等离子体特性和其他新兴现象提供了更强大的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;高能核物理中的一个核心挑战是从重离子碰撞的高维最终态数据中提取信息特征，以实现可靠的下游分析。传统方法通常依赖于选定的可观测量，可能会遗漏数据中细微但物理相关的结构。为此，我们引入了一种基于Transformer的自编码器，采用两阶段范式进行训练：自监督预训练 followed by 监督微调。预训练编码器直接从无标签的HIC数据中学习潜在表示，提供紧凑且信息丰富的特征空间，可以适应不同的物理任务。作为案例研究，我们将该方法应用于区分大碰撞系统和小碰撞系统，其分类准确率显著高于PointNet。主成分分析和SHAP解释进一步表明，该自编码器捕获了超越单个可观测量的复杂非线性相关性，产生了具有强判别力和解释力的特征。这些结果建立了我们的两阶段框架作为HIC中特征学习的通用和稳健基础，为分析夸克-胶子等离子体特性和其他新兴现象打开了更强大分析的大门。实现在GitHub上公开：https://github.com/Giovanni-Sforza/MaskPoint-AMPT。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central challenge in high-energy nuclear physics is to extract informativefeatures from the high-dimensional final-state data of heavy-ion collisions(HIC) in order to enable reliable downstream analyses. Traditional approachesoften rely on selected observables, which may miss subtle but physicallyrelevant structures in the data. To address this, we introduce aTransformer-based autoencoder trained with a two-stage paradigm:self-supervised pre-training followed by supervised fine-tuning. The pretrainedencoder learns latent representations directly from unlabeled HIC data,providing a compact and information-rich feature space that can be adapted todiverse physics tasks. As a case study, we apply the method to distinguishbetween large and small collision systems, where it achieves significantlyhigher classification accuracy than PointNet. Principal component analysis andSHAP interpretation further demonstrate that the autoencoder captures complexnonlinear correlations beyond individual observables, yielding features withstrong discriminative and explanatory power. These results establish ourtwo-stage framework as a general and robust foundation for feature learning inHIC, opening the door to more powerful analyses of quark--gluon plasmaproperties and other emergent phenomena. The implementation is publiclyavailable at https://github.com/Giovanni-Sforza/MaskPoint-AMPT.</description>
      <author>example@mail.com (Jing-Zong Zhang, Shuang Guo, Li-Lin Zhu, Lingxiao Wang, Guo-Liang Ma)</author>
      <guid isPermaLink="false">2510.06691v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>The Effect of Label Noise on the Information Content of Neural Representations</title>
      <link>http://arxiv.org/abs/2510.06401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了标签噪声对深度学习模型隐藏表示的影响，发现隐藏表示的信息内容随网络参数数量变化呈现双下降行为，且过参数化网络的表示对标签噪声具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在监督分类任务中，模型训练用于预测数据点标签，但现实数据集中的标签常因标注错误而存在噪声。虽然标签噪声对深度学习模型性能的影响已被广泛研究，但它对网络隐藏表示的影响仍不明确。&lt;h4&gt;目的&lt;/h4&gt;填补标签噪声对网络隐藏表示影响的研究空白，通过系统比较隐藏表示来理解这一问题。&lt;h4&gt;方法&lt;/h4&gt;使用信息不平衡（Information Imbalance）作为条件互值的计算高效代理，系统比较不同情况下的隐藏表示。&lt;h4&gt;主要发现&lt;/h4&gt;1) 隐藏表示的信息内容随网络参数数量变化呈现双下降行为；2) 在欠参数化情况下，噪声标签学习的表示比干净标签学习的更具信息量；3) 在过参数化情况下，两种表示具有相同信息量；4) 过参数化网络的表示对标签噪声具有鲁棒性；5) 交叉熵损失可减少倒数第二层与softmax前层间的信息不平衡；6) 随机标签学习的表示比随机特征表现更差。&lt;h4&gt;结论&lt;/h4&gt;过参数化网络的表示对标签噪声具有鲁棒性，这为理解分类任务中的泛化提供了新视角。训练随机标签使网络超越懒学习，权重会适应以编码标签信息。&lt;h4&gt;翻译&lt;/h4&gt;在监督分类任务中，模型被训练来预测每个数据点的标签。在现实世界的数据集中，这些标签常常由于标注错误而存在噪声。虽然标签噪声对深度学习模型性能的影响已被广泛研究，但它对网络隐藏表示的影响仍知之甚少。我们通过使用信息不平衡（一种条件互值的计算高效代理）系统比较隐藏表示来填补这一知识空白。通过这种分析，我们观察到隐藏表示的信息内容作为网络参数数量的函数呈现出双下降行为，类似于测试误差的行为。我们进一步证明，在欠参数化情况下，使用噪声标签学习的表示比使用干净标签学习的表示更具信息量，而在过参数化情况下，这些表示具有相同的信息量。我们的结果表明，过参数化网络的表示对标签噪声具有鲁棒性。我们还发现在过参数化情况下，使用交叉熵损失时，倒数第二层和softmax前层之间的信息不平衡会减少。这为理解分类任务中的泛化提供了新视角。将我们的分析扩展到从随机标签学习的表示，我们表明这些表示比随机特征表现更差。这表明在随机标签上的训练使网络远远超出懒学习，因为权重会适应以编码标签信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In supervised classification tasks, models are trained to predict a label foreach data point. In real-world datasets, these labels are often noisy due toannotation errors. While the impact of label noise on the performance of deeplearning models has been widely studied, its effects on the networks' hiddenrepresentations remain poorly understood. We address this gap by systematicallycomparing hidden representations using the Information Imbalance, acomputationally efficient proxy of conditional mutual information. Through thisanalysis, we observe that the information content of the hidden representationsfollows a double descent as a function of the number of network parameters,akin to the behavior of the test error. We further demonstrate that in theunderparameterized regime, representations learned with noisy labels are moreinformative than those learned with clean labels, while in theoverparameterized regime, these representations are equally informative. Ourresults indicate that the representations of overparameterized networks arerobust to label noise. We also found that the information imbalance between thepenultimate and pre-softmax layers decreases with cross-entropy loss in theoverparameterized regime. This offers a new perspective on understandinggeneralization in classification tasks. Extending our analysis torepresentations learned from random labels, we show that these perform worsethan random features. This indicates that training on random labels drivesnetworks much beyond lazy learning, as weights adapt to encode labelsinformation.</description>
      <author>example@mail.com (Ali Hussaini Umar, Franky Kevin Nando Tezoh, Jean Barbier, Santiago Acevedo, Alessandro Laio)</author>
      <guid isPermaLink="false">2510.06401v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Type and Complexity Signals in Multilingual Question Representations</title>
      <link>http://arxiv.org/abs/2510.06304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Workshop on Multilingual Representation Learning at EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了多语言transformer模型如何表示问题的形态句法特性，通过引入QTC数据集和扩展探测方法，比较了不同模型的表现，并分析了上下文表示与统计基线的优劣。&lt;h4&gt;背景&lt;/h4&gt;多语言transformer模型在处理不同语言问题时，其表示形态句法特性的能力尚不清楚，需要系统性的研究。&lt;h4&gt;目的&lt;/h4&gt;研究多语言transformer模型对问题形态句法特性的表示能力，评估不同方法在问题分类和复杂性分析中的表现。&lt;h4&gt;方法&lt;/h4&gt;引入了包含七种语言的问题类型和复杂性(QTC)数据集，标注了类型信息和多种复杂性指标；扩展了探测方法，使用选择性控制处理回归标签；比较了冻结Glot500-m模型的分层探测、子词TF-IDF基线和微调模型的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在具有显式标记的语言中，统计特征能有效分类问题；神经探测能更好地捕获细粒度的结构性复杂性模式；研究结果帮助评估了上下文表示何时优于统计基线，以及参数更新对预训练语言信息可用性的影响。&lt;h4&gt;结论&lt;/h4&gt;多语言transformer模型能够有效表示问题的形态句法特性，但不同方法在不同语言和任务上有各自的优势，需要根据具体需求选择合适的方法。&lt;h4&gt;翻译&lt;/h4&gt;这项工作研究了多语言transformer模型如何表示问题的形态句法特性。我们引入了包含七种语言句子的问题类型和复杂性(QTC)数据集，并标注了类型信息和复杂性指标，包括依赖长度、树深度和词汇密度。我们的评估扩展了探测方法，使用选择性控制来处理回归标签，以量化泛化能力的提升。我们比较了在冻结的Glot500-m表示上的分层探测与子词TF-IDF基线以及微调模型的性能。结果表明，在具有显式标记的语言中，统计特征能有效分类问题，而神经探测能更好地捕获细粒度的结构性复杂性模式。我们使用这些结果来评估上下文表示何时优于统计基线，以及参数更新是否减少预训练语言信息的可用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates how a multilingual transformer model representsmorphosyntactic properties of questions. We introduce the Question Type andComplexity (QTC) dataset with sentences across seven languages, annotated withtype information and complexity metrics including dependency length, treedepth, and lexical density. Our evaluation extends probing methods toregression labels with selectivity controls to quantify gains ingeneralizability. We compare layer-wise probes on frozen Glot500-m (Imani etal., 2023) representations against subword TF-IDF baselines, and a fine-tunedmodel. Results show that statistical features classify questions effectively inlanguages with explicit marking, while neural probes capture fine-grainedstructural complexity patterns better. We use these results to evaluate whencontextual representations outperform statistical baselines and whetherparameter updates reduce the availability of pre-trained linguisticinformation.</description>
      <author>example@mail.com (Robin Kokot, Wessel Poelman)</author>
      <guid isPermaLink="false">2510.06304v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection</title>
      <link>http://arxiv.org/abs/2510.07277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at SIPAIM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了基础模型和传统CNN在糖尿病性黄斑水肿检测中的性能，发现轻量级CNN在大多数情况下表现更好。&lt;h4&gt;背景&lt;/h4&gt;糖尿病性黄斑水肿是糖尿病患者视力丧失的主要原因。虽然深度学习在自动检测方面有潜力，但受限于标注数据的稀缺性。基础模型被视为替代方案，但其在DME检测中的效果尚不明确。&lt;h4&gt;目的&lt;/h4&gt;系统比较不同基础模型和标准迁移学习方法在DME检测任务上的表现，特别是比较RETFound、FLAIR和EfficientNet-B0三种模型。&lt;h4&gt;方法&lt;/h4&gt;在IDRiD、MESSIDOR-2和OCT-and-Eye-Fundus-Images三个数据集上，采用不同的训练方案和评估设置对RETFound、FLAIR和EfficientNet-B0进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管基础模型规模较大，但并不始终优于微调的CNN。EfficientNet-B0在大多数评估中表现最佳，RETFound仅在OEFI数据集上表现良好，而FLAIR在零样本设置下表现出色。&lt;h4&gt;结论&lt;/h4&gt;基础模型可能不适合精细的眼科任务如DME检测，即使在微调后也是如此，轻量级CNN在数据稀缺环境中仍是强大的基线。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病性黄斑水肿(DME)是糖尿病视网膜病变(DR)患者视力丧失的主要原因。虽然深度学习在从眼底图像自动检测这种情况方面显示出有希望的结果，但由于标注数据的有限可用性，其应用仍然具有挑战性。基础模型(FM)已成为一种替代解决方案。然而，目前尚不清楚它们是否特别能够应对DME检测。在本文中，我们系统地比较了不同的基础模型和标准的迁移学习方法用于此任务。具体来说，我们在IDRiD、MESSIDOR-2和OCT-and-Eye-Fundus-Images(OEFI)上，比较了两种最流行的视网膜图像基础模型--RETFound和FLAIR，以及一个EfficientNet-B0骨干网络，采用不同的训练方案和评估设置。结果表明，尽管规模较大，基础模型在该任务上并不始终优于微调的CNN。特别是，EfficientNet-B0在大多数评估设置中，根据ROC曲线下面积和精确度/召回率曲线排名第一或第二，而RETFound仅在OEFI中显示出有希望的结果。另一方面，FLAIR展示了有竞争力的零样本性能，在适当提示时实现了显著的AUC-PR分数。这些发现表明，基础模型可能不是精细的眼科任务(如DME检测)的良好工具，即使在微调之后，这表明在数据稀缺环境中，轻量级CNN仍然是强大的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Macular Edema (DME) is a leading cause of vision loss among patientswith Diabetic Retinopathy (DR). While deep learning has shown promising resultsfor automatically detecting this condition from fundus images, its applicationremains challenging due the limited availability of annotated data. FoundationModels (FM) have emerged as an alternative solution. However, it is unclear ifthey can cope with DME detection in particular. In this paper, wesystematically compare different FM and standard transfer learning approachesfor this task. Specifically, we compare the two most popular FM for retinalimages--RETFound and FLAIR--and an EfficientNet-B0 backbone, across differenttraining regimes and evaluation settings in IDRiD, MESSIDOR-2 andOCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM donot consistently outperform fine-tuned CNNs in this task. In particular, anEfficientNet-B0 ranked first or second in terms of area under the ROC andprecision/recall curves in most evaluation settings, with RETFound only showingpromising results in OEFI. FLAIR, on the other hand, demonstrated competitivezero-shot performance, achieving notable AUC-PR scores when promptedappropriately. These findings reveal that FM might not be a good tool forfine-grained ophthalmic tasks such as DME detection even after fine-tuning,suggesting that lightweight CNNs remain strong baselines in data-scarceenvironments.</description>
      <author>example@mail.com (Franco Javier Arellano, José Ignacio Orlando)</author>
      <guid isPermaLink="false">2510.07277v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis</title>
      <link>http://arxiv.org/abs/2510.06632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Chem-NMF的新型非负矩阵分解方法，通过引入有界因子解决多层架构中的收敛性问题，并首次从物理化学角度分析NMF算法的收敛行为。&lt;h4&gt;背景&lt;/h4&gt;非负矩阵分解(NMF)是一种无监督学习方法，在音频处理、生物医学信号分析和图像识别等领域提供低秩表示。将α-散度融入NMF公式可提高优化灵活性，但扩展到多层架构时确保收敛性存在挑战。&lt;h4&gt;目的&lt;/h4&gt;解决NMF在多层架构中的收敛性问题，并从物理化学角度对NMF算法的收敛行为进行严格分析。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Chem-NMF的新方法，受到化学反应中能垒玻尔兹曼概率的启发，引入了有界因子来稳定收敛。从数学上证明了渐进收敛结果，并将其应用于真实数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的算法在生物医学信号上聚类准确率提高了5.6%±2.7%，在人脸图像上提高了11.1%±7.2%（平均值±标准差）。&lt;h4&gt;结论&lt;/h4&gt;Chem-NMF方法有效解决了NMF在多层架构中的收敛性问题，并首次从物理化学角度对NMF算法的收敛行为进行了严格分析。&lt;h4&gt;翻译&lt;/h4&gt;非负矩阵分解(NMF)是一种无监督学习方法，在音频处理、生物医学信号分析和图像识别等各个领域提供低秩表示。在NMF公式中融入α-散度提高了优化的灵活性，然而将这些方法扩展到多层架构时，确保收敛性存在挑战。为此，我们受化学反应中能垒玻尔兹曼概率的启发，引入了一种新方法来理论上进行收敛分析，称为Chem-NMF，它包含一个稳定收敛的有界因子。据我们所知，这是首次从物理化学角度对NMF算法的收敛行为进行严格分析的研究。我们从数学上证明了渐进收敛结果，然后展示了它们如何应用于真实数据。实验结果表明，所提出的算法在生物医学信号上的聚类准确率提高了5.6%±2.7%，在人脸图像上提高了11.1%±7.2%（平均值±标准差）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-Negative Matrix Factorization (NMF) is an unsupervised learning methodoffering low-rank representations across various domains such as audioprocessing, biomedical signal analysis, and image recognition. Theincorporation of $\alpha$-divergence in NMF formulations enhances flexibilityin optimization, yet extending these methods to multi-layer architecturespresents challenges in ensuring convergence. To address this, we introduce anovel approach inspired by the Boltzmann probability of the energy barriers inchemical reactions to theoretically perform convergence analysis. We introducea novel method, called Chem-NMF, with a bounding factor which stabilizesconvergence. To our knowledge, this is the first study to apply a physicalchemistry perspective to rigorously analyze the convergence behaviour of theNMF algorithm. We start from mathematically proven asymptotic convergenceresults and then show how they apply to real data. Experimental resultsdemonstrate that the proposed algorithm improves clustering accuracy by 5.6%$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean$\pm$ std).</description>
      <author>example@mail.com (Yasaman Torabi, Shahram Shirani, James P. Reilly)</author>
      <guid isPermaLink="false">2510.06632v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction</title>
      <link>http://arxiv.org/abs/2510.06611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UnrollINR的新型零样本自监督MRI重建框架，能够在不依赖外部训练数据的情况下实现特定扫描的MRI重建，结合了深度展开结构和隐式神经表示的优势，在高加速率下表现出色。&lt;h4&gt;背景&lt;/h4&gt;磁共振成像（MRI）是重要的临床诊断工具，但其广泛应用受到扫描时间长的限制。快速MRI重建技术通过从欠采样的k空间数据中重建高保真MR图像来有效减少采集时间。近年来，基于深度学习的方法在该领域取得了显著进展，特别是在难以获取完全采样数据的场景中，自监督和无监督学习方法证明特别有价值。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖外部训练数据的零样本自监督MRI重建框架，提高MRI重建的性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出名为UnrollINR的新型零样本自监督重建框架，采用物理引导的展开迭代重建架构，并将隐式神经表示（INR）作为正则化先验来有效约束解空间。通过结合深度展开结构与INR的强大隐式表示能力，增强了模型的解释性和重建性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，即使在10的高加速率下，UnrollINR相比监督学习方法也能实现优越的重建性能，验证了所提出方法的优越性。&lt;h4&gt;结论&lt;/h4&gt;UnrollINR框架通过结合深度展开结构和隐式神经表示，实现了无需外部训练数据的MRI重建，在高加速率下仍能保持高质量的重建效果。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（MRI）是重要的临床诊断工具，但其广泛应用受到扫描时间长的限制。快速MRI重建技术通过从欠采样的k空间数据中重建高保真MR图像来有效减少采集时间。近年来，基于深度学习的方法在该领域取得了显著进展，特别是在难以获取完全采样数据的场景中，自监督和无监督学习方法证明特别有价值。本文提出了一种名为UnrollINR的新型零样本自监督重建框架，它能够在不依赖外部训练数据的情况下实现特定扫描的MRI重建。该方法采用物理引导的展开迭代重建架构，并将隐式神经表示（INR）作为正则化先验来有效约束解空间。通过结合深度展开结构与INR的强大隐式表示能力，增强了模型的解释性和重建性能。实验结果表明，即使在10的高加速率下，UnrollINR相比监督学习方法也能实现优越的重建性能，验证了所提出方法的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet itswidespread application is limited by prolonged scan times. Fast MRIreconstruction techniques effectively reduce acquisition duration byreconstructing high-fidelity MR images from undersampled k-space data. Inrecent years, deep learning-based methods have demonstrated remarkable progressin this field, with self-supervised and unsupervised learning approachesproving particularly valuable in scenarios where fully sampled data aredifficult to obtain. This paper proposes a novel zero-shot self-supervisedreconstruction framework named UnrollINR, which enables scan-specific MRIreconstruction without relying on external training data. The method adopts aphysics-guided unrolled iterative reconstruction architecture and introducesImplicit Neural Representation (INR) as a regularization prior to effectivelyconstrain the solution space. By combining a deep unrolled structure with thepowerful implicit representation capability of INR, the model'sinterpretability and reconstruction performance are enhanced. Experimentalresults demonstrate that even at a high acceleration rate of 10, UnrollINRachieves superior reconstruction performance compared to the supervisedlearning method, validating the superiority of the proposed method.</description>
      <author>example@mail.com (Jingran Xu, Yuanyuan Liu, Yanjie Zhu)</author>
      <guid isPermaLink="false">2510.06611v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>TransFIRA: Transfer Learning for Face Image Recognizability Assessment</title>
      <link>http://arxiv.org/abs/2510.06353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://transfira.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TransFIRA是一个轻量级且无需注释的框架，将人脸图像可识别性直接嵌入到嵌入空间中，通过类中心相似度和类中心角分离度定义可识别性，实现了最先进的验证准确性，并扩展到人体识别领域。&lt;h4&gt;背景&lt;/h4&gt;在不受约束环境中进行人脸识别时，需应对姿态、模糊、光照和遮挡的极端变化，传统视觉质量指标无法预测输入对编码器的可识别性，现有FIQA方法通常依赖视觉启发式、人工注释或计算密集型生成流程，导致预测与编码器决策几何分离。&lt;h4&gt;目的&lt;/h4&gt;引入TransFIRA框架，将可识别性直接嵌入到嵌入空间中，提供一种无需注释的可识别性评估方法。&lt;h4&gt;方法&lt;/h4&gt;TransFIRA通过三个方面的进步实现目标：(i)使用类中心相似度(CCS)和类中心角分离度(CCAS)定义可识别性；(ii)采用可识别性感知的聚合策略，无需外部标签或特定训练；(iii)扩展到人脸以外的领域，包括编码器可解释性和可识别性感知的人体识别评估。&lt;h4&gt;主要发现&lt;/h4&gt;TransFIRA在BRIAR和IJB-C数据集上实现了最先进的验证准确性，与真实可识别性的相关性几乎翻倍，在人脸识别和人体识别方面均表现出色，且在跨数据集转移下具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;TransFIRA作为一个统一的、几何驱动的可识别性评估框架，特定于编码器、准确、可解释且可跨模态扩展，显著提高了FIQA的准确性、可解释性和应用范围。&lt;h4&gt;翻译&lt;/h4&gt;在不受约束的环境（如监控、视频和网络图像）中进行人脸识别必须应对姿态、模糊、光照和遮挡的极端变化，传统的视觉质量指标无法预测输入对部署的编码器是否真正可识别。现有的FIQA方法通常依赖视觉启发式、人工注释或计算密集型的生成流程，使其预测与编码器的决策几何分离。我们引入TransFIRA（人脸图像可识别性评估的迁移学习），一个轻量级且无需注释的框架，将可识别性直接嵌入到嵌入空间中。TransFIRA提供了三个方面的进步：(i)通过类中心相似度和类中心角分离度定义可识别性，产生第一个自然、决策边界对齐的过滤和加权标准；(ii)一种可识别性感知的聚合策略，在BRIAR和IJB-C上实现了最先进的验证准确性，同时与真实可识别性的相关性几乎翻倍，无需外部标签、启发式或特定于骨干网络的训练；以及(iii)超越人脸的新扩展，包括基于编码器的可解释性，揭示降级和主题特定因素如何影响可识别性，以及第一个可识别性感知的人体识别评估。实验验证了TransFIRA在人脸识别方面取得了最先进的结果，在人体识别方面表现出色，并且在跨数据集转移下具有鲁棒性。这些贡献共同确立了TransFIRA作为一个统一的、几何驱动的可识别性评估框架——特定于编码器、准确、可解释且可跨模态扩展——显著提高了FIQA的准确性、可解释性和范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Face recognition in unconstrained environments such as surveillance, video,and web imagery must contend with extreme variation in pose, blur,illumination, and occlusion, where conventional visual quality metrics fail topredict whether inputs are truly recognizable to the deployed encoder. ExistingFIQA methods typically rely on visual heuristics, curated annotations, orcomputationally intensive generative pipelines, leaving their predictionsdetached from the encoder's decision geometry. We introduce TransFIRA (TransferLearning for Face Image Recognizability Assessment), a lightweight andannotation-free framework that grounds recognizability directly in embeddingspace. TransFIRA delivers three advances: (i) a definition of recognizabilityvia class-center similarity (CCS) and class-center angular separation (CCAS),yielding the first natural, decision-boundary--aligned criterion for filteringand weighting; (ii) a recognizability-informed aggregation strategy thatachieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearlydoubling correlation with true recognizability, all without external labels,heuristics, or backbone-specific training; and (iii) new extensions beyondfaces, including encoder-grounded explainability that reveals how degradationsand subject-specific factors affect recognizability, and the firstrecognizability-aware body recognition assessment. Experiments confirmstate-of-the-art results on faces, strong performance on body recognition, androbustness under cross-dataset shifts. Together, these contributions establishTransFIRA as a unified, geometry-driven framework for recognizabilityassessment -- encoder-specific, accurate, interpretable, and extensible acrossmodalities -- significantly advancing FIQA in accuracy, explainability, andscope.</description>
      <author>example@mail.com (Allen Tu, Kartik Narayan, Joshua Gleason, Jennifer Xu, Matthew Meyn, Tom Goldstein, Vishal M. Patel)</author>
      <guid isPermaLink="false">2510.06353v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping</title>
      <link>http://arxiv.org/abs/2510.06299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种融合GEDI激光雷达和多模态SAR数据的深度学习框架，用于生成全球高分辨率的森林结构复杂性地图。&lt;h4&gt;背景&lt;/h4&gt;森林结构复杂性指标整合多个冠层属性为单一值，反映栖息地质量和生态系统功能。GEDI星载激光雷达虽能绘制温带和热带森林结构复杂性，但其稀疏采样限制了连续高分辨率制图。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的深度学习框架，融合GEDI观测与多模态SAR数据，生成全球高分辨率的森林结构复杂性地图。&lt;h4&gt;方法&lt;/h4&gt;使用调整的EfficientNetV2架构，在1.3亿多个GEDI足迹上训练，融合GEDI观测与多模态SAR数据集，生成25米分辨率的全覆盖森林结构复杂性地图。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现高性能(全局R2=0.82)，参数少于40万个；能在不同生物群落和时间周期内产生准确预测并校准不确定性；已生成2015-2022年的全球多时段森林结构复杂性数据集；可通过迁移学习扩展预测其他森林结构变量。&lt;h4&gt;结论&lt;/h4&gt;该方法支持全球森林结构动力学的连续监测，为气候变化背景下的生物多样性保护和生态系统管理提供工具，且计算成本较低。&lt;h4&gt;翻译&lt;/h4&gt;森林结构复杂性指标将多个冠层属性整合为一个单一值，反映栖息地质量和生态系统功能。全球生态系统动力学调查(GEDI)的星载激光雷达已经能够绘制温带和热带森林的结构复杂性图，但其稀疏采样限制了连续高分辨率制图。我们提出了一种可扩展的深度学习框架，融合GEDI观测与多模态合成孔径雷达(SAR)数据集，生成全球高分辨率(25米)的森林结构复杂性全覆盖地图。我们调整的EfficientNetV2架构，在超过1.3亿个GEDI足迹上训练，实现了高性能(全局R2=0.82)，参数少于40万个，使其成为可访问的工具，使研究人员能够在任何规模上处理数据集，而无需专门的计算基础设施。该模型在生物群落和时间周期内产生准确的预测，并校准不确定性估计，保留了精细尺度的空间模式。它已被用于生成2015年至2022年的全球多时段森林结构复杂性数据集。通过迁移学习，该框架可以扩展以预测其他森林结构变量，计算成本最小。这种方法支持全球森林结构动力学的连续、多时段监测，并为气候变化背景下的生物多样性和生态系统管理工作提供工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forest structural complexity metrics integrate multiple canopy attributesinto a single value that reflects habitat quality and ecosystem function.Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) hasenabled mapping of structural complexity in temperate and tropical forests, butits sparse sampling limits continuous high-resolution mapping. We present ascalable, deep learning framework fusing GEDI observations with multimodalSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25m) wall-to-wall maps of forest structural complexity. Our adaptedEfficientNetV2 architecture, trained on over 130 million GEDI footprints,achieves high performance (global R2 = 0.82) with fewer than 400,000parameters, making it an accessible tool that enables researchers to processdatasets at any scale without requiring specialized computing infrastructure.The model produces accurate predictions with calibrated uncertainty estimatesacross biomes and time periods, preserving fine-scale spatial patterns. It hasbeen used to generate a global, multi-temporal dataset of forest structuralcomplexity from 2015 to 2022. Through transfer learning, this framework can beextended to predict additional forest structural variables with minimalcomputational cost. This approach supports continuous, multi-temporalmonitoring of global forest structural dynamics and provides tools forbiodiversity conservation and ecosystem management efforts in a changingclimate.</description>
      <author>example@mail.com (Tiago de Conto, John Armston, Ralph Dubayah)</author>
      <guid isPermaLink="false">2510.06299v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.05753v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 13 figures, published in TMLR  https://openreview.net/forum?id=UligTUCgdt&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了不同成员推理攻击(MIAs)在迁移学习环境中的性能，帮助从业者识别用于隐私风险评估的最有效攻击方法。研究发现没有单一的MIA能捕捉迁移学习模型的所有隐私风险，不同攻击方法在不同情况下效果各异。&lt;h4&gt;背景&lt;/h4&gt;随着大规模基础模型的出现，机器学习训练范式正从从头训练转向迁移学习，这使得在敏感应用中使用小型领域特定数据集进行高效训练成为可能。成员推理攻击(MIAs)可用于评估机器学习模型的隐私泄露风险。&lt;h4&gt;目的&lt;/h4&gt;通过比较多样化MIA在迁移学习设置中的性能，帮助从业者识别用于隐私风险评估的最有效攻击方法。&lt;h4&gt;方法&lt;/h4&gt;比较不同类型的成员推理攻击(MIAs)在迁移学习环境中的表现，研究攻击效力与训练数据量的关系，以及不同攻击方法在不同数据集上的效果差异。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于分数的MIA攻击效力随训练数据增加而降低；2. 没有单一MIA能捕捉迁移学习模型的所有隐私风险；3. 似然比攻击(LiRA)在大多数实验场景中表现优异，但逆海森攻击(IHA)在针对使用PatchCamelyon数据集在高数据环境下微调的模型时更为有效。&lt;h4&gt;结论&lt;/h4&gt;在评估迁移学习模型的隐私风险时，需要考虑多种MIA方法，因为不同的攻击方法在不同情况下可能更有效，没有一种攻击方法能全面捕捉所有隐私风险。&lt;h4&gt;翻译&lt;/h4&gt;随着强大大规模基础模型的出现，训练范式正日益从从头开始训练转向迁移学习。这使得在敏感应用中使用典型的小型领域特定数据集进行高效训练成为可能。成员推理攻击(MIAs)为机器学习模型提供了隐私泄露的经验估计。然而，先前针对通过迁移学习微调的模型的MIA评估仅依赖于一小部分可能的攻击方法。我们通过比较多样化MIA在迁移学习环境中的性能来解决这个问题，帮助从业者识别用于隐私风险评估的最有效攻击。我们发现，对于基于分数的MIA，攻击效力随训练数据的增加而降低。我们发现没有单一的MIA能捕捉通过迁移学习训练的模型的所有隐私风险。尽管似然比攻击(LiRA)在大多数实验场景中表现优异，但逆海森攻击(IHA)被证明在针对使用PatchCamelyon数据集在高数据环境下微调的模型时更为有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emergence of powerful large-scale foundation models, the trainingparadigm is increasingly shifting from from-scratch training to transferlearning. This enables high utility training with small, domain-specificdatasets typical in sensitive applications. Membership inference attacks (MIAs)provide an empirical estimate of the privacy leakage by machine learningmodels. Yet, prior assessments of MIAs against models fine-tuned with transferlearning rely on a small subset of possible attacks. We address this bycomparing performance of diverse MIAs in transfer learning settings to helppractitioners identify the most efficient attacks for privacy risk evaluation.We find that attack efficacy decreases with the increase in training data forscore-based MIAs. We find that there is no one MIA which captures all privacyrisks in models trained with transfer learning. While the Likelihood RatioAttack (LiRA) demonstrates superior performance across most experimentalscenarios, the Inverse Hessian Attack (IHA) proves to be more effective againstmodels fine-tuned on PatchCamelyon dataset in high data regime.</description>
      <author>example@mail.com (Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, Antti Honkela)</author>
      <guid isPermaLink="false">2510.05753v2</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Prompting Matters: Rethinking Referring Video Object Segmentation</title>
      <link>http://arxiv.org/abs/2510.07319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个名为时间提示生成和选择(Tenet)的框架，用于解决视频对象分割任务，通过分解任务因素并利用现成的检测器和跟踪器生成时间提示，实现了高效模型适应。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数视频对象分割方法需要密集的掩码注释进行端到端训练，计算量大且可扩展性差。&lt;h4&gt;目的&lt;/h4&gt;重新思考视频对象分割问题，探究该任务的关键因素，并开发一种更高效的方法。&lt;h4&gt;方法&lt;/h4&gt;将RVOS任务分解为指代、视频和分割三个因素，提出Tenet框架解决指代和视频因素，利用现成的对象检测器和跟踪器生成与指代语句相关的时间提示，并通过提示偏好学习评估提示质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用生成的时间提示指导基础分割模型，能够为被指代对象生成高质量掩码，实现模型向视频对象分割的高效适应。&lt;h4&gt;结论&lt;/h4&gt;Tenet框架在RVOS基准测试上表现出有效性，提供了一种更高效的视频对象分割方法。&lt;h4&gt;翻译&lt;/h4&gt;引用视频对象分割旨在分割视频中由查询语句指代的对象。大多数现有方法需要密集的掩码注释进行端到端训练，这可能导致计算量大且可扩展性差。在本工作中，我们重新思考了RVOS问题，旨在探究该任务的关键。基于现有的基础分割模型，我们将RVOS任务分解为指代、视频和分割因素，并提出时间提示生成和选择(Tenet)框架来解决指代和视频因素，同时将分割问题留给基础模型。为了高效地将基于图像的基础分割模型适应到视频对象分割中，我们利用现成的对象检测器和跟踪器生成与指代语句相关的时间提示。虽然可以生成高质量的时间提示，但无法从置信度分数中轻易识别。为解决此问题，我们提出了提示偏好学习来评估生成的时间提示的质量。通过使用这些提示指导基于图像的基础分割模型，我们能够为被指代对象生成高质量掩码，实现模型向视频对象分割的高效适应。在RVOS基准测试上的实验证明了Tenet框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RVOS) aims to segment the objectreferred to by the query sentence in the video. Most existing methods requireend-to-end training with dense mask annotations, which could becomputation-consuming and less scalable. In this work, we rethink the RVOSproblem and aim to investigate the key to this task. Based on existingfoundation segmentation models, we decompose the RVOS task into referring,video, and segmentation factors, and propose a Temporal Prompt Generation andSelection (Tenet) framework to address the referring and video factors whileleaving the segmentation problem to foundation models. To efficiently adaptimage-based foundation segmentation models to referring video objectsegmentation, we leverage off-the-shelf object detectors and trackers toproduce temporal prompts associated with the referring sentence. Whilehigh-quality temporal prompts could be produced, they can not be easilyidentified from confidence scores. To tackle this issue, we propose PromptPreference Learning to evaluate the quality of the produced temporal prompts.By taking such prompts to instruct image-based foundation segmentation models,we would be able to produce high-quality masks for the referred object,enabling efficient model adaptation to referring video object segmentation.Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenetframework.</description>
      <author>example@mail.com (Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang)</author>
      <guid isPermaLink="false">2510.07319v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency</title>
      <link>http://arxiv.org/abs/2510.07119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MoRe，一种无需训练的单目几何细化方法，用于提高跨视图一致性和实现尺度对齐，通过图优化框架解决单目几何先验的尺度模糊性问题。&lt;h4&gt;背景&lt;/h4&gt;单目3D基础模型为感知任务提供了可扩展的解决方案，使其在更广泛的3D视觉应用中具有吸引力。&lt;h4&gt;目的&lt;/h4&gt;提高单目3D重建的跨视图一致性，实现尺度对齐，并改善3D重建和新视图合成的质量，特别是在稀疏视图渲染场景中。&lt;h4&gt;方法&lt;/h4&gt;提出MoRe方法，通过帧间特征匹配建立对应关系，采用基于图的优化框架，利用单目基础模型估计的3D点和表面法线进行局部平面近似，而非简单的最小二乘优化。&lt;h4&gt;主要发现&lt;/h4&gt;MoRe不仅提高了3D重建的质量，还改善了新视图合成，特别是在稀疏视图渲染场景中表现更为突出。&lt;h4&gt;结论&lt;/h4&gt;MoRe方法有效解决了单目几何先验中的尺度模糊性问题，同时保留了底层3D结构，为单目3D视觉应用提供了改进的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单目3D基础模型为感知任务提供了可扩展的解决方案，使其在更广泛的3D视觉应用中具有吸引力。在本文中，我们提出了MoRe，一种无需训练的单目几何细化方法，旨在提高跨视图一致性和实现尺度对齐。为了诱导帧间关系，我们的方法采用帧间特征匹配来建立对应关系。我们不是在这些匹配点上应用简单的最小二乘优化，而是制定了一个基于图的优化框架，使用单目基础模型估计的3D点和表面法线进行局部平面近似。这种方法解决了单目几何先验中固有的尺度模糊性问题，同时保留了底层3D结构。我们进一步证明，MoRe不仅提高了3D重建，还改善了新视图合成，特别是在稀疏视图渲染场景中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D基础模型在不同视角之间的一致性问题（cross-view consistency）。由于单目相机固有的尺度模糊性，不同视角预测的点云往往无法对齐，导致3D重建和新视图合成质量不佳。这个问题在机器人、AR/VR、自动驾驶等领域非常重要，因为这些领域广泛应用单目相机，而多视角一致性是3D场景理解和高质量重建的关键挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有单目3D基础模型的局限性，特别是多视角之间的尺度不一致问题。他们发现端到端方法缺乏模块化，难以集成外部传感器，而简单最小二乘优化容易引入噪声。因此，他们设计了两阶段方法：首先进行初始仿射变换对齐，然后引入基于图的优化使用局部平面近似来细化对齐。该方法借鉴了MoGe的并行化对齐求解器、MadPose的尺度参数化方法，以及Rossi等人的局部平面优化技术，并利用DKM等图像匹配算法建立稠密对应关系。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过基于图的优化提升单目3D点云的跨视角一致性，利用局部平面近似和表面法线信息来优化点云对齐，而不是简单地最小化对应点之间的欧氏距离。整体流程包括：1) 使用单目基础模型预测点云和法线；2) 通过图像匹配建立像素对应关系；3) 应用仿射变换进行初始对齐；4) 构建图结构并定义多种几何约束（帧内和跨帧平面约束、kNN约束、视线一致性约束等）；5) 使用多尺度策略优化点云位置；6) 输出优化后的跨视角一致点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 模块化框架，将点云对齐与完整3D场景重建解耦，允许灵活集成传统几何方法和外部传感器；2) 基于图的优化方法，使用局部平面近似和表面法线进行联合优化，减少噪声影响；3) 多种跨视角几何约束，确保优化后点云在多视角间保持一致性；4) 直接优化点云坐标而非深度图，允许在三个方向上调整。相比之前的工作，MoRe解决了端到端方法缺乏模块化的问题，提高了简单最小二乘优化的精度，并超越了单目方法如MoGe的尺度模糊性限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoRe通过创新的基于图优化和局部平面近似的跨视角对齐方法，有效解决了单目3D基础模型的尺度模糊性问题，实现了模块化、高精度的多视角一致3D重建，并在稀疏视图场景下显著提升了新视图合成的质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D foundation models offer an extensible solution for perceptiontasks, making them attractive for broader 3D vision applications. In thispaper, we propose MoRe, a training-free Monocular Geometry Refinement methoddesigned to improve cross-view consistency and achieve scale alignment. Toinduce inter-frame relationships, our method employs feature matching betweenframes to establish correspondences. Rather than applying simple least squaresoptimization on these matched points, we formulate a graph-based optimizationframework that performs local planar approximation using the estimated 3Dpoints and surface normals estimated by monocular foundation models. Thisformulation addresses the scale ambiguity inherent in monocular geometricpriors while preserving the underlying 3D structure. We further demonstratethat MoRe not only enhances 3D reconstruction but also improves novel viewsynthesis, particularly in sparse view rendering scenarios.</description>
      <author>example@mail.com (Dongki Jung, Jaehoon Choi, Yonghan Lee, Sungmin Eum, Heesung Kwon, Dinesh Manocha)</author>
      <guid isPermaLink="false">2510.07119v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report</title>
      <link>http://arxiv.org/abs/2510.07092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1X world model challenge technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了1X World Model Challenge，一个专注于人形机器人交互的开源基准，包含采样和压缩两个赛道。研究团队在采样赛道上适配了视频生成模型Wan-2.2 TI2V-5B，在压缩赛道上训练了时空Transformer模型，并在两个任务中均获得第一名。&lt;h4&gt;背景&lt;/h4&gt;World models是AI和机器人学中的强大范式，使智能体能够通过预测视觉观察或紧凑的潜在状态来推理未来。然而，缺乏针对真实世界人形机器人交互的开源基准。&lt;h4&gt;目的&lt;/h4&gt;创建并参与1X World Model Challenge，提供一个人形机器人交互的开源基准测试，包含两个互补赛道：采样（预测未来图像帧）和压缩（预测未来离散潜在代码）。&lt;h4&gt;方法&lt;/h4&gt;采样赛道：将视频生成基础模型Wan-2.2 TI2V-5B适应到视频状态条件下的未来帧预测，使用AdaLN-Zero条件化机器人状态，并通过LoRA进行微调。压缩赛道：从头开始训练Spatio-Temporal Transformer模型。&lt;h4&gt;主要发现&lt;/h4&gt;采样任务达到23.0 dB PSNR，压缩任务达到Top-500 CE为6.6386，在两个挑战中都获得第一名。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型在两个互补的任务上都表现出色，证明了World models在真实世界人形机器人交互中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;世界模型是AI和机器人学中的强大范式，使智能体能够通过预测视觉观察或紧凑的潜在状态来推理未来。1X World Model Challenge引入了一个真实世界人形机器人交互的开源基准，包含两个互补赛道：采样，专注于预测未来图像帧；压缩，专注于预测未来离散潜在代码。对于采样赛道，我们将视频生成基础模型Wan-2.2 TI2V-5B适应到视频状态条件下的未来帧预测。我们使用AdaLN-Zero将视频生成条件化为机器人状态，并使用LoRA进一步微调模型。对于压缩赛道，我们从零开始训练了一个时空Transformer模型。我们的模型在采样任务中达到23.0 dB PSNR，在压缩任务中达到Top-500 CE为6.6386，在两个挑战中都获得了第一名。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World models are a powerful paradigm in AI and robotics, enabling agents toreason about the future by predicting visual observations or compact latentstates. The 1X World Model Challenge introduces an open-source benchmark ofreal-world humanoid interaction, with two complementary tracks: sampling,focused on forecasting future image frames, and compression, focused onpredicting future discrete latent codes. For the sampling track, we adapt thevideo generation foundation model Wan-2.2 TI2V-5B to video-state-conditionedfuture frame prediction. We condition the video generation on robot statesusing AdaLN-Zero, and further post-train the model using LoRA. For thecompression track, we train a Spatio-Temporal Transformer model from scratch.Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386in the compression task, securing 1st place in both challenges.</description>
      <author>example@mail.com (Riccardo Mereu, Aidan Scannell, Yuxin Hou, Yi Zhao, Aditya Jitta, Antonio Dominguez, Luigi Acerbi, Amos Storkey, Paul Chang)</author>
      <guid isPermaLink="false">2510.07092v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Mixout: An Overlooked Path to Robust Finetuning</title>
      <link>http://arxiv.org/abs/2510.06982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究微调视觉基础模型时提高准确性与鲁棒性的平衡方法，提出GMixout技术。&lt;h4&gt;背景&lt;/h4&gt;微调视觉基础模型通常可以提高领域内准确性，但会降低在分布偏移情况下的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;重新审视Mixout随机正则化方法，通过单次运行、权重共享的隐式集合视角分析，提出改进方法以提高模型在分布偏移下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过单次运行、权重共享的隐式集合视角重新审视Mixout，发现控制鲁棒性的三个关键杠杆：掩码锚点、重采样频率和掩码稀疏性，并基于此提出GMixout方法，用指数移动平均快照替换固定锚点，并通过显式的重采样频率超参数调节掩码周期。采用稀疏核实现，仅更新一小部分参数，无需推理时间开销。&lt;h4&gt;主要发现&lt;/h4&gt;控制鲁棒性的三个关键杠杆是掩码锚点、重采样频率和掩码稀疏性；GMixout在提高领域内准确性的同时，能增强模型在分布偏移下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GMixout在多个基准测试中表现优异，在提高领域内准确性的同时，超越了Model Soups和强参数高效微调基线在分布偏移下的表现，且可在消费级GPU上训练。&lt;h4&gt;翻译&lt;/h4&gt;微调视觉基础模型通常可以提高领域内准确性，但会以在分布偏移下降低鲁棒性为代价。我们通过单次运行、权重共享的隐式集合的视角重新审视了Mixout，这是一种随机正则化方法，间歇性地用其预训练参考替换微调权重。这种视角揭示了控制鲁棒性的三个关键杠杆：掩码锚点、重采样频率和掩码稀疏性。在分析指导下，我们引入了GMixout，它(i)用适应训练过程中变化的指数移动平均快照替换固定锚点，以及(ii)通过显式的重采样频率超参数调节掩码周期。我们的稀疏核实现仅更新一小部分参数，无需推理时间开销， enabling在消费级GPU上训练。在涵盖协变量偏移、损坏和类别不平衡的基准测试中，包括ImageNet / ImageNet-LT、DomainNet、iWildCam和CIFAR100-C，GMixout在提高领域内准确性方面始终超越零样本性能，同时在分布偏移下超越了Model Soups和强参数高效微调基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Finetuning vision foundation models often improves in-domain accuracy butcomes at the cost of robustness under distribution shift. We revisit Mixout, astochastic regularizer that intermittently replaces finetuned weights withtheir pretrained reference, through the lens of a single-run, weight-sharingimplicit ensemble. This perspective reveals three key levers that governrobustness: the \emph{masking anchor}, \emph{resampling frequency}, and\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)replaces the fixed anchor with an exponential moving-average snapshot thatadapts during training, and (ii) regulates masking period via an explicitresampling-frequency hyperparameter. Our sparse-kernel implementation updatesonly a small fraction of parameters with no inference-time overhead, enablingtraining on consumer-grade GPUs. Experiments on benchmarks covering covariateshift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracybeyond zero-shot performance while surpassing both Model Soups and strongparameter-efficient finetuning baselines under distribution shift.</description>
      <author>example@mail.com (Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli)</author>
      <guid isPermaLink="false">2510.06982v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance</title>
      <link>http://arxiv.org/abs/2510.06809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为视觉-动作适配器(VA-Adapter)的参数高效方法，将超声基础模型知识应用于探头引导任务，帮助初级超声医师获取高质量心脏超声图像。&lt;h4&gt;背景&lt;/h4&gt;超声心动图是检测心脏疾病的关键工具，但心脏超声操作难度极高，导致高技能人员短缺，患者无法及时接受检查服务。获取高质量超声图像是准确诊断的前提。&lt;h4&gt;目的&lt;/h4&gt;将基础模型从大量数据集中学习到的医学知识适应到探头引导任务中，为初级超声医师提供实时操作建议，以获取高质量的超声图像。&lt;h4&gt;方法&lt;/h4&gt;设计了一个参数高效的视觉-动作适配器(VA-Adapter)，使基础模型的图像编码器能够编码视觉-动作序列，提高引导性能。该适配器具有内置的顺序推理能力和紧凑设计，仅微调一小部分参数即可使预训练的超声基础模型学习精确的探头调整策略。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VA-Adapter能够超越现有的强探头引导模型。&lt;h4&gt;结论&lt;/h4&gt;VA-Adapter有效解决了心脏超声操作难度高、专业人员短缺的问题，通过提供实时操作建议，帮助初级超声医师获取高质量超声图像。&lt;h4&gt;翻译&lt;/h4&gt;超声心动图是检测心脏疾病的关键工具。最近，超声基础模型在心脏超声图像分析方面表现出显著能力。然而，获取高质量超声图像是准确诊断的前提。由于心脏超声操作难度极高，缺乏高技能人员，导致患者无法及时接受检查服务。在本文中，我们旨在将基础模型从大量数据集中学习到的医学知识适应到探头引导任务中，该任务旨在为初级超声医师提供实时操作建议，以获取高质量的超声图像。此外，受专家根据过去探索优化行动决策的实践启发，我们精心设计了一个参数高效的视觉-动作适配器(VA-Adapter)，使基础模型的图像编码器能够编码视觉-动作序列，从而提高引导性能。凭借紧凑设计中的内置顺序推理能力，VA-Adapter使预训练的超声基础模型仅通过微调一小部分参数即可学习精确的探头调整策略。大量实验证明，VA-Adapter能够超越强大的探头引导模型。我们的代码将在接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Echocardiography is a critical tool for detecting heart diseases. Recently,ultrasound foundation models have demonstrated remarkable capabilities incardiac ultrasound image analysis. However, obtaining high-quality ultrasoundimages is a prerequisite for accurate diagnosis. Due to the exceptionally highoperational difficulty of cardiac ultrasound, there is a shortage of highlyskilled personnel, which hinders patients from receiving timely examinationservices. In this paper, we aim to adapt the medical knowledge learned byfoundation models from vast datasets to the probe guidance task, which isdesigned to provide real-time operational recommendations for juniorsonographers to acquire high-quality ultrasound images. Moreover, inspired bythe practice where experts optimize action decisions based on pastexplorations, we meticulously design a parameter-efficient Vision-ActionAdapter (VA-Adapter) to enable foundation model's image encoder to encodevision-action sequences, thereby enhancing guidance performance. With built-insequential reasoning capabilities in a compact design, the VA-Adapter enables apre-trained ultrasound foundation model to learn precise probe adjustmentstrategies by fine-tuning only a small subset of parameters. Extensiveexperiments demonstrate that the VA-Adapter can surpass strong probe guidancemodels. Our code will be released after acceptance.</description>
      <author>example@mail.com (Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Shiji Song, Gao Huang)</author>
      <guid isPermaLink="false">2510.06809v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene</title>
      <link>http://arxiv.org/abs/2510.06754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://sites.google.com/view/uniffield&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniFField是一种统一的不确定性感知神经特征场，结合视觉、语义和几何特征，能够预测每种模态的不确定性，适用于机器人任务执行和决策。&lt;h4&gt;背景&lt;/h4&gt;对3D场景进行全面的可视化、几何和语义理解对机器人任务执行至关重要，特别是在非结构化和复杂环境中。机器人需要评估感知信息的可靠性以做出稳健决策。现有的3D神经特征场方法有两个关键局限：通常是场景特定的，且缺乏对预测不确定性的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的不确定性感知神经特征场，结合多种特征模态并预测不确定性，使机器人能够零样本应用于新环境，并做出稳健决策。&lt;h4&gt;方法&lt;/h4&gt;提出UniFField，一种统一的神经特征场，结合视觉、语义和几何特征。使用基于体素的特征表示，随着机器人探索场景逐步整合RGB-D图像，同时更新不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;不确定性估计能够准确描述模型在场景重建和语义特征预测中的预测误差。利用特征预测及其相应的不确定性成功完成了主动物体搜索任务，展示了稳健决策的能力。&lt;h4&gt;结论&lt;/h4&gt;UniFField提供了一种有效的方法，使机器人能够在复杂环境中进行3D场景理解，并通过不确定性预测做出更稳健的决策。&lt;h4&gt;翻译&lt;/h4&gt;对3D场景进行全面的可视化、几何和语义理解对于机器人任务的成功执行至关重要，特别是在非结构化和复杂环境中。此外，为了做出稳健决策，机器人有必要评估感知信息的可靠性。尽管最近3D神经特征场的进展使机器人能够利用预训练基础模型的特征来完成语言引导的操控和导航等任务，但现有方法存在两个关键局限：(i)它们通常是场景特定的，(ii)它们缺乏对其预测不确定性的建模能力。我们提出了UniFField，一种统一的不确定性感知神经特征场，在单一可泛化表示中结合视觉、语义和几何特征，同时预测每种模态的不确定性。我们的方法可以零样本应用于任何新环境，随着机器人探索场景逐步将RGB-D图像整合到我们的基于体素的特征表示中，同时更新不确定性估计。我们评估了不确定性估计，以准确描述模型在场景重建和语义特征预测中的预测误差。此外，我们成功利用特征预测及其相应的不确定性，使用移动操作器机器人完成了主动物体搜索任务，展示了稳健决策的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个关键问题：1) 现有的3D神经特征场通常是场景特定的，无法很好地泛化到新场景；2) 现有方法缺乏对其预测的不确定性建模的能力。这两个问题在机器人技术中至关重要，因为机器人需要在未结构化和复杂的环境中执行任务，并需要评估感知信息的可靠性以做出稳健决策，特别是在部分可观察的环境中，理解预测的不确定性对机器人的主动感知和探索至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作的思想：参考了NeRF和Gaussian Splatting等3D神经表示方法，但解决了它们无法增量添加观察的问题；学习了通用特征场的工作，这些工作可以在多个场景上预训练并零样本应用到新场景；参考了增量神经表示的工作，随时间聚合信息；借鉴了贝叶斯方法如Dropout和集成方法用于不确定性建模；采用了类似GeFF的方法学习通用语义先验，并利用基于体素的特征表示。通过整合这些现有方法的思想并添加不确定性建模能力，设计了UniFField。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的神经特征场，能够结合视觉、语义和几何特征在一个表示中，预测每种模态的不确定性，支持零样本应用到任何新环境，并允许增量更新。实现流程：1) 构建统一特征场：从RGB-D图像提取特征并反投影到3D空间，创建图像特征体积和TSDF体积，添加不确定性指标；2) 解码统一特征场：构建三个解码网络映射到RGB值、语义特征、TSDF值及不确定性；3) 使用可微体积渲染将3D预测投影到2D进行训练；4) 使用不确定性感知的监督训练模型；5) 推理时直接构建新场景特征场，通过新RGB-D帧的统一特征体积的运行平均进行增量更新。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 通用统一神经特征场，结合多种模态在一个表示中；2) 能够预测每种模态的不确定性，使在部分可观察设置中进行稳健决策；3) 支持增量更新，适合连续探索场景的机器人；4) 设计了使用不确定性感知UniFField进行主动对象搜索任务的方法。相比之前工作，UniFField的不同在于：不仅处理视觉和几何信息，还整合语义信息；明确建模预测的不确定性；支持零样本应用和增量更新；能够处理不同类型的不确定性（认知和偶然不确定性）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniFField是一种通用统一的神经特征场，能够结合视觉、语义和几何特征并预测每种模态的不确定性，使机器人能够在任何场景中进行稳健的决策和主动探索，而无需针对每个场景进行优化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive visual, geometric, and semantic understanding of a 3D scene iscrucial for successful execution of robotic tasks, especially in unstructuredand complex environments. Additionally, to make robust decisions, it isnecessary for the robot to evaluate the reliability of perceived information.While recent advances in 3D neural feature fields have enabled robots toleverage features from pretrained foundation models for tasks such aslanguage-guided manipulation and navigation, existing methods suffer from twocritical limitations: (i) they are typically scene-specific, and (ii) they lackthe ability to model uncertainty in their predictions. We present UniFField, aunified uncertainty-aware neural feature field that combines visual, semantic,and geometric features in a single generalizable representation while alsopredicting uncertainty in each modality. Our approach, which can be appliedzero shot to any new environment, incrementally integrates RGB-D images intoour voxel-based feature representation as the robot explores the scene,simultaneously updating uncertainty estimation. We evaluate our uncertaintyestimations to accurately describe the model prediction errors in scenereconstruction and semantic feature prediction. Furthermore, we successfullyleverage our feature predictions and their respective uncertainty for an activeobject search task using a mobile manipulator robot, demonstrating thecapability for robust decision-making.</description>
      <author>example@mail.com (Christian Maurer, Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki)</author>
      <guid isPermaLink="false">2510.06754v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>LLM Company Policies and Policy Implications in Software Organizations</title>
      <link>http://arxiv.org/abs/2510.06718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Software Special Issue on AIware in the Foundation  Models Era&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了软件组织中采用大型语言模型(LLM)聊天机器人相关的风险及制定明确政策的必要性，调查了11家公司的政策制定过程及影响因素&lt;h4&gt;背景&lt;/h4&gt;软件组织在采用大型语言模型(LLM)聊天机器人时面临相关风险&lt;h4&gt;目的&lt;/h4&gt;帮助管理者将聊天机器人安全地集成到开发工作流程中&lt;h4&gt;方法&lt;/h4&gt;调查11家公司如何创建聊天机器人相关政策及影响这些政策的因素&lt;h4&gt;主要发现&lt;/h4&gt;软件组织采用LLM聊天机器人存在风险，需要制定明确政策；11家公司的政策创建方式及影响因素&lt;h4&gt;结论&lt;/h4&gt;需要明确的政策来管理LLM聊天机器人的采用，以确保安全集成到开发工作流程中&lt;h4&gt;翻译&lt;/h4&gt;软件组织中采用大型语言模型(LLM)聊天机器人相关的风险凸显了明确政策的必要性。我们研究了11家公司如何制定这些政策以及影响这些政策的因素，旨在帮助管理者将聊天机器人安全地集成到开发工作流程中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The risks associated with adopting large language model (LLM) chatbots insoftware organizations highlight the need for clear policies. We examine how 11companies create these policies and the factors that influence them, aiming tohelp managers safely integrate chatbots into development workflows.</description>
      <author>example@mail.com (Ranim Khojah, Mazen Mohamad, Linda Erlenhov, Francisco Gomes de Oliveira Neto, Philipp Leitner)</author>
      <guid isPermaLink="false">2510.06718v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training</title>
      <link>http://arxiv.org/abs/2510.06710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the technical report of the RLinf Team, focusing on the  algorithm side. For the system-level design, please refer to  arXiv:2509.15965. The open-sourced code link: https://github.com/RLinf/RLinf&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RLinf-VLA，一个统一且高效的框架，用于可扩展的VLA模型强化学习训练。该框架通过灵活的资源分配设计解决了在强化学习和视觉-语言-动作模型训练中整合渲染、训练和推理的挑战，实现了训练速度的显著提升。它支持多种VLA架构、强化学习算法和模拟器，在模拟环境中表现出色，并在真实机器人部署中显示出比监督微调更好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉和语言基础模型的进步显著推动了多模态理解、推理和生成能力的发展，激发了将此类能力扩展到具身环境中的兴趣，通过视觉-语言-动作模型实现。然而，大多数VLA模型仍使用监督微调进行训练，这种方法在分布变化下泛化能力有限，因为存在错误累积问题。强化学习提供了一个有前景的替代方案，但现有尝试仍然零散，缺乏统一的比较平台。&lt;h4&gt;目的&lt;/h4&gt;为了解决VLA模型训练中缺乏统一框架的问题，作者引入了RLinf-VLA，这是一个统一且高效的框架，用于VLA模型的可扩展强化学习训练。该系统旨在提供一个灵活的资源分配设计，解决在强化学习和视觉-语言-动作模型训练中整合渲染、训练和推理的挑战。&lt;h4&gt;方法&lt;/h4&gt;RLinf-VLA采用了一种高度灵活的资源分配设计，特别是对于GPU并行化的模拟器，实现了一种新颖的混合细粒度流水线分配模式，实现了1.61倍至1.88倍的训练速度提升。通过统一的接口，RLinf-VLA无缝支持多种VLA架构、多种强化学习算法以及各种模拟器。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟环境中，统一模型在130个LIBERO任务上达到了98.11%的成功率，在25个ManiSkill任务上达到了97.66%的成功率。研究还总结了一套将强化学习应用于VLA训练的最佳实践。在真实世界的Franka机器人上，强化学习训练的策略比使用监督微调训练的策略表现出更强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;作者将RLinf-VLA视为加速和标准化具身智能研究的基础。该框架提供了一个统一的平台，使研究人员能够公平和系统地比较不同模型架构和算法设计，从而推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉和语言基础模型的最新进展显著提升了多模态理解、推理和生成能力，激发了通过视觉-语言-动作模型将此类能力扩展到具身环境中的兴趣。然而，大多数VLA模型仍然使用监督微调进行训练，由于错误累积问题，这种方法在分布变化下难以泛化。强化学习通过交互直接优化任务性能，提供了一个有前景的替代方案。为了解决这一差距，我们引入了RLinf-VLA，一个用于VLA模型可扩展强化学习训练的统一高效框架。该系统采用了一种高度灵活的资源分配设计，解决了在强化学习和视觉-语言-动作模型训练中整合渲染、训练和推理的挑战。特别是，对于GPU并行化的模拟器，RLinf-VLA实现了一种新颖的混合细粒度流水线分配模式，实现了1.61倍至1.88倍的训练速度提升。通过统一的接口，RLinf-VLA无缝支持多种VLA架构、多种强化学习算法以及各种模拟器。在模拟环境中，统一模型在130个LIBERO任务上达到了98.11%的成功率，在25个ManiSkill任务上达到了97.66%的成功率。除了实证性能外，我们的研究还总结了一套将强化学习应用于VLA训练的最佳实践。此外，我们在真实世界的Franka机器人上进行了初步部署，发现强化学习训练的策略比使用监督微调训练的策略表现出更强的泛化能力。我们将RLinf-VLA视为加速和标准化具身智能研究的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in vision and language foundation models has significantlyadvanced multimodal understanding, reasoning, and generation, inspiring a surgeof interest in extending such capabilities to embodied settings throughvision-language-action (VLA) models. Yet, most VLA models are still trainedwith supervised fine-tuning (SFT), which struggles to generalize underdistribution shifts due to error accumulation. Reinforcement learning (RL)offers a promising alternative by directly optimizing task performance throughinteraction, but existing attempts remain fragmented and lack a unifiedplatform for fair and systematic comparison across model architectures andalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified andefficient framework for scalable RL training of VLA models. The system adopts ahighly flexible resource allocation design that addresses the challenge ofintegrating rendering, training, and inference in RL+VLA training. Inparticular, for GPU-parallelized simulators, RLinf-VLA implements a novelhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedupin training. Through a unified interface, RLinf-VLA seamlessly supports diverseVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, aunified model achieves 98.11\% across 130 LIBERO tasks and 97.66\% across 25ManiSkill tasks. Beyond empirical performance, our study distills a set of bestpractices for applying RL to VLA training and sheds light on emerging patternsin this integration. Furthermore, we present preliminary deployment on areal-world Franka robot, where RL-trained policies exhibit strongergeneralization than those trained with SFT. We envision RLinf-VLA as afoundation to accelerate and standardize research on embodied intelligence.</description>
      <author>example@mail.com (Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, Zhihao Liu, Kang Chen, Wenhao Tang, Quanlu Zhang, Weinan Zhang, Chao Yu, Yu Wang)</author>
      <guid isPermaLink="false">2510.06710v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>POME: Post Optimization Model Edit via Muon-style Projection</title>
      <link>http://arxiv.org/abs/2510.06627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为POME（后优化模型编辑）的新算法，它仅使用预训练和微调检查点即可提升大型语言模型的性能，无需额外数据或进一步优化。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在微调后可能仍有性能提升空间，特别是在不引入额外计算资源的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种简单的方法来提高已微调的大型语言模型的性能，同时保持与现有训练框架的兼容性。&lt;h4&gt;方法&lt;/h4&gt;对微调权重与预训练权重之间的差异ΔW应用μ子样投影，使用截断奇异值分解（SVD）平衡主导更新方向的影响，并修剪代表噪声的小奇异值。作为一种简单的后处理步骤，POME与训练流程完全解耦。&lt;h4&gt;主要发现&lt;/h4&gt;POME在GSM8K上平均性能提升+2.5%，在代码生成上提升+1.0%。该方法适用于从7B基础模型到72B RLHF指令模型的广泛范围。&lt;h4&gt;结论&lt;/h4&gt;POME是一种实用、零成本的微调流程增强方法，对任何优化器或分布式框架都是通用兼容的，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了后优化模型编辑（POME），一种新算法，它仅使用预训练和微调检查点即可增强大型语言模型的性能，无需额外数据或进一步优化。核心思想是对ΔW（微调权重与预训练权重之间的差异）应用μ子样投影。这种投影使用截断奇异值分解（SVD）来平衡主导更新方向的影响，并修剪小的奇异值，这些值通常代表噪声。作为一种简单的后处理步骤，POME与训练流程完全解耦。它需要零修改且不产生任何开销，使其与任何优化器或分布式框架普遍兼容。POME提供了一致的性能提升，在GSM8K上平均性能提高+2.5%，在代码生成上提高+1.0%。它的广泛适用性——从7B基础模型到72B RLHF指令模型——使其成为任何微调流程的实用、零成本增强。代码可在https://github.com/NUS-HPC-AI-Lab/POME获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Post-Optimization Model Edit (POME), a new algorithm thatenhances the performance of fine-tuned large language models using only theirpretrained and fine-tuned checkpoints, without requiring extra data or furtheroptimization. The core idea is to apply a muon-style projection to $\Delta W$,the difference between the fine-tuned and pretrained weights. This projectionuses truncated singular value decomposition (SVD) to equalize the influence ofdominant update directions and prune small singular values, which oftenrepresent noise. As a simple post-processing step, POME is completely decoupledfrom the training pipeline. It requires zero modifications and imposes nooverhead, making it universally compatible with any optimizer or distributedframework. POME delivers consistent gains, boosting average performance by+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from7B foundation models to 72B RLHF-instructed models -- establishes it as apractical, zero-cost enhancement for any fine-tuning pipeline. Code isavailable at https://github.com/NUS-HPC-AI-Lab/POME.</description>
      <author>example@mail.com (Yong Liu, Di Fu, Yang Luo, Zirui Zhu, Minhao Cheng, Cho-Jui Hsieh, Yang You)</author>
      <guid isPermaLink="false">2510.06627v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Cluster Paths: Navigating Interpretability in Neural Networks</title>
      <link>http://arxiv.org/abs/2510.06541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了cluster paths方法，通过聚类神经网络激活并使用序列表示输入来提高深度学习模型的可解释性。该方法通过多个指标评估，并在各种任务上展示了有效性和可扩展性，能够揭示模型决策背后的视觉概念。&lt;h4&gt;背景&lt;/h4&gt;现代深度神经网络在视觉任务中取得了令人印象深刻的性能，但它们的决策过程仍然不透明，存在不必要的信任风险、未检测到的偏差和意外故障。&lt;h4&gt;目的&lt;/h4&gt;开发一种后验可解释性方法，使深度神经网络的决策过程更加透明，帮助理解模型如何做出决策以及识别潜在问题。&lt;h4&gt;方法&lt;/h4&gt;在选定层对激活进行聚类，并将每个输入表示为其聚类ID序列。引入四个评估指标：路径复杂性（认知负荷）、加权路径纯度（类别对齐）、决策对齐保真度（预测保真度）和路径一致性（对扰动的稳定性）。将cluster paths扩展到concept paths，通过在大型语言模型上提示最小路径差异来推导。&lt;h4&gt;主要发现&lt;/h4&gt;在虚假线索的CIFAR-10实验中，cluster paths识别基于颜色的捷径；在五类CelebA发色任务中，实现了90%的保真度，并在高斯噪声下保持96%的一致性；能够揭示多网络深度的视觉概念，如调色板、纹理或对象上下文；可以作为有效的分布外(OOD)检测器。&lt;h4&gt;结论&lt;/h4&gt;cluster paths可以扩展到大型视觉模型，同时生成简洁且人类可读的解释，帮助理解模型决策过程并识别潜在问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然现代深度神经网络在视觉任务中取得了令人印象深刻的性能，但它们的决策过程仍然不透明，存在不必要的信任风险、未检测到的偏差和意外故障。我们提出了cluster paths，一种后验可解释性方法，它在选定层对激活进行聚类，并将每个输入表示为其聚类ID序列。为了评估这些cluster paths，我们引入了四个指标：路径复杂性（认知负荷）、加权路径纯度（类别对齐）、决策对齐保真度（预测保真度）和路径一致性（对扰动的稳定性）。在虚假线索的CIFAR-10实验中，cluster paths识别基于颜色的捷径，并在线索被移除时失效。在五类CelebA发色任务中，它们实现了90%的保真度，并在高斯噪声下保持96%的一致性，同时不牺牲准确性。扩展到ImageNet上预训练的Vision Transformer时，我们将cluster paths扩展为从大型语言模型提示中派生的concept paths。最后，我们表明cluster paths可以作为一种有效的分布外(OOD)检测器，在模型产生过度自信的预测之前可靠地标记异常样本。cluster paths在多个网络深度上揭示了视觉概念，如调色板、纹理或对象上下文，证明了cluster paths可以扩展到大型视觉模型，同时生成简洁且人类可读的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While modern deep neural networks achieve impressive performance in visiontasks, they remain opaque in their decision processes, risking unwarrantedtrust, undetected biases and unexpected failures. We propose cluster paths, apost-hoc interpretability method that clusters activations at selected layersand represents each input as its sequence of cluster IDs. To assess thesecluster paths, we introduce four metrics: path complexity (cognitive load),weighted-path purity (class alignment), decision-alignment faithfulness(predictive fidelity), and path agreement (stability under perturbations). In aspurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcutsand collapse when the cue is removed. On a five-class CelebA hair-color task,they achieve 90% faithfulness and maintain 96% agreement under Gaussian noisewithout sacrificing accuracy. Scaling to a Vision Transformer pretrained onImageNet, we extend cluster paths to concept paths derived from prompting alarge language model on minimal path divergences. Finally, we show that clusterpaths can serve as an effective out-of-distribution (OOD) detector, reliablyflagging anomalous samples before the model generates over-confidentpredictions. Cluster paths uncover visual concepts, such as color palettes,textures, or object contexts, at multiple network depths, demonstrating thatcluster paths scale to large vision models while generating concise andhuman-readable explanations.</description>
      <author>example@mail.com (Nicholas M. Kroeger, Vincent Bindschaedler)</author>
      <guid isPermaLink="false">2510.06541v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles</title>
      <link>http://arxiv.org/abs/2510.06475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究引入了PuzzlePlex基准测试，用于评估基础模型在推理和规划方面的能力及其在复杂环境中的可扩展性。该研究包含15种不同类型的谜题，支持扩展性，并开发了细粒度性能指标。研究发现推理模型在基于指令的设置中表现更佳，而基于代码的执行虽有挑战但更具可扩展性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在复杂、动态环境中的推理和规划能力及其可扩展性需要更全面的评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试来评估基础模型的推理、规划和泛化能力，并研究它们在不同设置下的表现和扩展极限。&lt;h4&gt;方法&lt;/h4&gt;引入PuzzlePlex基准测试，包含15种不同类型的谜题（确定性和随机性游戏，单人及双人场景）；实现定制化游戏策略；开发细粒度性能指标；在基于指令和基于代码的两种设置下评估前沿基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;推理模型在基于指令的设置中表现优于其他模型；基于代码的执行面临更大挑战，但提供了可扩展且高效的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PuzzlePlex基准测试能够实现有针对性的评估，并指导基础模型在推理、规划和泛化方面的未来改进。&lt;h4&gt;翻译&lt;/h4&gt;这项工作研究了基础模型在推理和规划方面的能力及其在复杂、动态环境中的可扩展性。我们引入了PuzzlePlex，一个通过多样化谜题集来评估这些能力的基准测试。PuzzlePlex包含15种类型的谜题，包括不同难度的确定性和随机性游戏，以及单人场景和双人场景。PuzzlePlex框架为每个游戏提供了全面的环境，并支持扩展性，可以根据基础模型的发展生成更具挑战性的实例。此外，我们实现了定制化的游戏策略进行比较。基于这个基准测试，我们开发了细粒度的指标来衡量性能，并对前沿基础模型在两种设置下进行了深入分析：基于指令的和基于代码的。此外，我们系统地研究了它们的扩展极限。我们的研究结果表明，在基于指令的设置中，推理模型优于其他模型，而基于代码的执行则面临更大挑战，但提供了可扩展且高效的替代方案。PuzzlePlex能够实现有针对性的评估，并指导基础模型在推理、规划和泛化方面的未来改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates the reasoning and planning capabilities of foundationmodels and their scalability in complex, dynamic environments. We introducePuzzlePlex, a benchmark designed to assess these capabilities through a diverseset of puzzles. PuzzlePlex consists of 15 types of puzzles, includingdeterministic and stochastic games of varying difficulty, as well assingle-player and two-player scenarios. The PuzzlePlex framework provides acomprehensive environment for each game, and supports extensibility to generatemore challenging instances as foundation models evolve. Additionally, weimplement customized game-playing strategies for comparison. Building on thisbenchmark, we develop fine-grained metrics to measure performance and conductan in-depth analysis of frontier foundation models across two settings:instruction-based and code-based. Furthermore, we systematically investigatetheir scaling limits. Our findings show that reasoning models outperform othersin instruction-based settings, while code-based execution presents greaterchallenges but offers a scalable and efficient alternative. PuzzlePlex enablestargeted evaluation and guides future improvements in reasoning, planning, andgeneralization for foundation models.</description>
      <author>example@mail.com (Yitao Long, Yuru Jiang, Hongjun Liu, Yilun Zhao, Jingchen Sun, Yiqiu Shen, Chen Zhao, Arman Cohan, Dennis Shasha)</author>
      <guid isPermaLink="false">2510.06475v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization</title>
      <link>http://arxiv.org/abs/2510.06434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过Hellinger局部化框架显著扩展了多轨迹设置中实例最优率的应用范围，提出了一种控制路径测度层面平方Hellinger距离的方法，并在参数空间中进行局部化，实现了在广泛条件下的实例最优边界。该方法在四个不同的案例研究中得到了验证，边界几乎匹配了来自渐近正态性的实例最优率，显著优于标准简化方法。&lt;h4&gt;背景&lt;/h4&gt;从时间相关数据学习是现代机器学习的核心方面，但在多轨迹设置中（数据由许多时间索引随机过程的独立实现组成），序列学习的理解仍然不完整。这种设置反映了现代大型基础模型的训练管道，并提供了在不需要典型混合假设的情况下进行学习的可能性。&lt;h4&gt;目的&lt;/h4&gt;解决多轨迹设置中实例最优边界的问题。目前只有带相关协变量的最小二乘回归具有实例最优边界，对于更一般的模型或损失函数，现有方法要么简化为i.i.d.学习，要么使用单轨迹结果，都存在有效样本量扩展的限制。&lt;h4&gt;方法&lt;/h4&gt;通过Hellinger局部化框架进行最大似然估计。首先通过简化为i.i.d.学习来控制路径测度层面的平方Hellinger距离，然后在参数空间中作为二次形式进行局部化，由轨迹Fisher信息加权。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的条件下，产生了与完整数据预算成比例的实例最优边界。在四个不同的案例研究中（简单马尔可夫链混合、非高斯噪声下的相关线性回归、具有非单调激活的广义线性模型以及线性注意力序列模型），边界几乎匹配了来自渐近正态性的实例最优率，显著优于标准简化方法。&lt;h4&gt;结论&lt;/h4&gt;该工作显著扩展了多轨迹设置中实例最优率的应用范围，为序列学习提供了更好的理论保证，特别是在处理更一般的模型或损失函数时。&lt;h4&gt;翻译&lt;/h4&gt;从时间相关数据学习是现代机器学习的核心方面。然而，我们对序列学习的理解仍然不完整，特别是在多轨迹设置中，数据由许多时间索引随机过程的独立实现组成。这种重要的设置既反映了现代大型基础模型的训练管道，又提供了在不采用单轨迹情况下典型混合假设的情况下进行学习的可能性。然而，目前只有带相关协变量的最小二乘回归具有实例最优边界；对于更一般的模型或损失函数，唯一广泛适用的保证要么简化为独立同分布学习，有效样本量仅随轨迹数量扩展，要么在每条单独轨迹混合时使用现有的单轨迹结果，有效样本量随总数据预算除以混合时间而扩展。在这项工作中，我们通过Hellinger局部化框架显著扩展了多轨迹设置中实例最优率的应用范围，这是最大似然估计的一种通用方法。我们的方法首先通过简化为独立同分布学习来控制路径测度层面的平方Hellinger距离，然后在参数空间中作为由轨迹Fisher信息加权的二次形式进行局部化。这产生了在广泛条件下与完整数据预算成比例的实例最优边界。我们在四个不同的案例研究中实现了我们的框架：简单马尔可夫链混合、非高斯噪声下的相关线性回归、具有非单调激活的广义线性模型以及线性注意力序列模型。在所有情况下，我们的边界几乎匹配了来自渐近正态性的实例最优率，显著优于标准简化方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning from temporally-correlated data is a core facet of modern machinelearning. Yet our understanding of sequential learning remains incomplete,particularly in the multi-trajectory setting where data consists of manyindependent realizations of a time-indexed stochastic process. This importantregime both reflects modern training pipelines such as for large foundationmodels, and offers the potential for learning without the typical mixingassumptions made in the single-trajectory case. However, instance-optimalbounds are known only for least-squares regression with dependent covariates;for more general models or loss functions, the only broadly applicableguarantees result from a reduction to either i.i.d. learning, with effectivesample size scaling only in the number of trajectories, or an existingsingle-trajectory result when each individual trajectory mixes, with effectivesample size scaling as the full data budget deflated by the mixing-time.  In this work, we significantly broaden the scope of instance-optimal rates inmulti-trajectory settings via the Hellinger localization framework, a generalapproach for maximum likelihood estimation. Our method proceeds by firstcontrolling the squared Hellinger distance at the path-measure level via areduction to i.i.d. learning, followed by localization as a quadratic form inparameter space weighted by the trajectory Fisher information. This yieldsinstance-optimal bounds that scale with the full data budget under a broad setof conditions. We instantiate our framework across four diverse case studies: asimple mixture of Markov chains, dependent linear regression under non-Gaussiannoise, generalized linear models with non-monotonic activations, andlinear-attention sequence models. In all cases, our bounds nearly match theinstance-optimal rates from asymptotic normality, substantially improving overstandard reductions.</description>
      <author>example@mail.com (Eliot Shekhtman, Yichen Zhou, Ingvar Ziemann, Nikolai Matni, Stephen Tu)</author>
      <guid isPermaLink="false">2510.06434v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.06419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了构建较小预训练预测模型组合作为单一大型模型的替代方案，通过集成或模型选择技术实现竞争性性能且使用更少参数。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型是否越大越好存在疑问，需要探索替代方案。&lt;h4&gt;目的&lt;/h4&gt;探索构建预测模型组合的策略，评估其在大型基准测试上的性能。&lt;h4&gt;方法&lt;/h4&gt;构建较小预训练预测模型组合，应用集成或模型选择技术，使用更少参数实现高性能。&lt;h4&gt;主要发现&lt;/h4&gt;专门模型组合始终优于独立训练的通用模型组合；对基础模型进行训练后处理是创建多样化专业模型的高效方法；集成和模型选择比测试时微调更计算高效。&lt;h4&gt;结论&lt;/h4&gt;构建较小专业模型的组合可以在保持竞争力的同时减少参数使用，提高计算效率。&lt;h4&gt;翻译&lt;/h4&gt;对于时间序列基础模型，越大总是越好吗？带着这个问题，我们探索了训练单一大型整体模型的替代方案：构建一组较小、预训练的预测模型组合。通过对这些模型组合应用集成或模型选择，我们使用更少的参数在大型基准测试上实现了竞争性性能。我们探索了设计此类模型组合的策略，发现专门模型的组合始终优于独立训练的通用模型组合。值得注意的是，我们证明了训练后处理基础模型是创建足够多样化专业模型的一种计算效率高的方法，并提供证据表明集成和模型选择比测试时微调更计算高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Is bigger always better for time series foundation models? With the questionin mind, we explore an alternative to training a single, large monolithicmodel: building a portfolio of smaller, pretrained forecasting models. Byapplying ensembling or model selection over these portfolios, we achievecompetitive performance on large-scale benchmarks using much fewer parameters.We explore strategies for designing such portfolios and find that collectionsof specialist models consistently outperform portfolios of independentlytrained generalists. Remarkably, we demonstrate that post-training a base modelis a compute-effective approach for creating sufficiently diverse specialists,and provide evidences that ensembling and model selection are morecompute-efficient than test-time fine-tuning.</description>
      <author>example@mail.com (Mert Kayaalp, Caner Turkmen, Oleksandr Shchur, Pedro Mercado, Abdul Fatir Ansari, Michael Bohlke-Schneider, Bernie Wang)</author>
      <guid isPermaLink="false">2510.06419v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings</title>
      <link>http://arxiv.org/abs/2510.06397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究揭示了非欧几里得基础模型中存在的一种特定于几何的脆弱性，即边界驱动的不对称性可以被后门触发器利用。作者分析了这一问题，提出了几何自适应触发器，并评估了不同任务和架构下的表现，同时指出了防御方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;非欧几里得基础模型越来越多地将表示放置在双曲几何等弯曲空间中。这种几何结构引入了新的安全挑战。&lt;h4&gt;目的&lt;/h4&gt;探究非欧几里得基础模型中的安全漏洞，特别是边界驱动的不对称性如何被后门触发器利用，并提出相应的防御方法。&lt;h4&gt;方法&lt;/h4&gt;作者通过理论分析形式化了边界附近微小输入变化导致模型表示空间大幅变化的现象，提出了几何自适应触发器，并在多种任务和架构上进行了实证评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在非欧几里得模型的边界附近，微小输入变化会导致模型表示空间中不成比例的大幅变化；2. 传统输入空间检测器难以检测这些边界附近的微妙变化；3. 通过沿半径向内拉点的方法可以抑制触发器，但会牺牲模型在同一方向上的有用敏感性；4. 攻击成功率随接近边界而增加，而传统检测器则减弱，与理论趋势一致。&lt;h4&gt;结论&lt;/h4&gt;非欧几里得模型中存在一种特定于几何的脆弱性，理解这种脆弱性对于设计和理解防御方法的局限至关重要。提出的几何自适应触发器展示了这种漏洞的实际影响。&lt;h4&gt;翻译&lt;/h4&gt;非欧几里得基础模型越来越多地将表示放置在双曲几何等弯曲空间中。我们表明，这种几何结构会产生一种边界驱动的不对称性，后门触发器可以加以利用。在边界附近，微小的输入变化对标准的输入空间检测器来说可能看起来很微妙，但在模型的表示空间中会产生不成比例的大幅变化。我们的分析形式化了这种效应，并揭示了防御方法的一个局限性：通过沿半径向内拉点的方法可以抑制此类触发器，但代价是在同一方向上牺牲了模型的有用敏感性。基于这些见解，我们提出了一种简单的几何自适应触发器，并在不同任务和架构上进行了评估。实验表明，攻击成功率随着接近边界而增加，而传统检测器则减弱，这与理论趋势一致。这些结果共同揭示了非欧几里得模型中的一种特定于几何的脆弱性，并为设计和理解防御的局限提供了基于分析的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-Euclidean foundation models increasingly place representations in curvedspaces such as hyperbolic geometry. We show that this geometry creates aboundary-driven asymmetry that backdoor triggers can exploit. Near theboundary, small input changes appear subtle to standard input-space detectorsbut produce disproportionately large shifts in the model's representationspace. Our analysis formalizes this effect and also reveals a limitation fordefenses: methods that act by pulling points inward along the radius cansuppress such triggers, but only by sacrificing useful model sensitivity inthat same direction. Building on these insights, we propose a simplegeometry-adaptive trigger and evaluate it across tasks and architectures.Empirically, attack success increases toward the boundary, whereas conventionaldetectors weaken, mirroring the theoretical trends. Together, these resultssurface a geometry-specific vulnerability in non-Euclidean models and offeranalysis-backed guidance for designing and understanding the limits ofdefenses.</description>
      <author>example@mail.com (Ali Baheri)</author>
      <guid isPermaLink="false">2510.06397v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</title>
      <link>http://arxiv.org/abs/2510.06377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint; under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了关系Transformer(RT)架构，能够在多样化的关系数据库上预训练并直接应用于未见过的数据集和任务，无需特定微调或上下文示例检索。&lt;h4&gt;背景&lt;/h4&gt;预训练transformer模型可以通过零样本提示适应新的序列建模任务，但关系领域仍缺乏跨数据集和任务的迁移架构。核心挑战在于关系数据的多样性，包括不同的异构模式、图结构和功能依赖。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预训练并在多样化关系数据库上应用，直接迁移到未见数据集和任务的架构，无需任务或数据集特定的微调。&lt;h4&gt;方法&lt;/h4&gt;关系Transformer(RT)架构：(1)使用表格/列元数据对单元格进行标记化，(2)通过掩码标记预测进行预训练，(3)利用新的关系注意机制处理列、行和主-外键链接。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench数据集上预训练后，RT在二元分类任务上达到完全监督AUROC的94%，仅使用2200万参数模型的一次前向传递，而270亿参数的LLM仅达到84%；微调能以高样本效率达到最先进结果；RT的零样本迁移利用了任务-表格上下文、关系注意模式和模式语义。&lt;h4&gt;结论&lt;/h4&gt;关系Transformer为关系数据的基础模型提供了实用路径。&lt;h4&gt;翻译&lt;/h4&gt;预训练transformer模型可以通过零样本提示轻松适应新的序列建模任务，但关系领域仍然缺乏能够在不同数据集和任务间迁移的架构。核心挑战在于关系数据的多样性，包括不同的异构模式、图结构和功能依赖。在本文中，我们提出了关系Transformer(RT)架构，可以在多样化的关系数据库上进行预训练，并直接应用于未见过的数据集和任务，无需任务或数据集特定的微调，或检索上下文示例。RT(i)使用表格/列元数据对单元格进行标记化，(ii)通过掩码标记预测进行预训练，(iii)利用一种新的关系注意机制处理列、行和主-外键链接。在RelBench数据集（涵盖流失和销售预测等任务）上预训练后，RT在二元分类任务上实现了强大的零样本性能，平均达到完全监督AUROC的94%，仅使用2200万参数模型的一次前向传递，而270亿参数的大语言模型仅达到84%。微调能以高样本效率达到最先进的结果。实验表明，RT的零样本迁移利用了任务-表格上下文、关系注意模式和模式语义。总体而言，RT为关系数据的基础模型提供了实用路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained transformers readily adapt to new sequence modeling tasks viazero-shot prompting, but relational domains still lack architectures thattransfer across datasets and tasks. The core challenge is the diversity ofrelational data, with varying heterogeneous schemas, graph structures andfunctional dependencies. In this paper, we present the Relational Transformer(RT) architecture, which can be pretrained on diverse relational databases anddirectly applied to unseen datasets and tasks without task- or dataset-specificfine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells withtable/column metadata, (ii) is pretrained via masked token prediction, and(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,rows, and primary-foreign key links. Pretrained on RelBench datasets spanningtasks such as churn and sales forecasting, RT attains strong zero-shotperformance, averaging 94% of fully supervised AUROC on binary classificationtasks with a single forward pass of a 22M parameter model, as opposed to 84%for a 27B LLM. Fine-tuning yields state-of-the-art results with high sampleefficiency. Our experiments show that RT's zero-shot transfer harnessestask-table context, relational attention patterns and schema semantics.Overall, RT provides a practical path toward foundation models for relationaldata.</description>
      <author>example@mail.com (Rishabh Ranjan, Valter Hudovernik, Mark Znidar, Charilaos Kanatsoulis, Roshan Upendra, Mahmoud Mohammadi, Joe Meyer, Tom Palczewski, Carlos Guestrin, Jure Leskovec)</author>
      <guid isPermaLink="false">2510.06377v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA</title>
      <link>http://arxiv.org/abs/2510.06371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Multimodal Foundation Models, Large Language Models, Native,  Multilingual, Language Diversity, Contextual Understanding, Culturally  Informed&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一个名为EverydayMMQA的框架，用于创建大规模、基于文化的数据集，以解决多模态模型在需要文化背景知识的问答任务中的局限性，并开发了OASIS数据集作为该框架的实例。&lt;h4&gt;背景&lt;/h4&gt;大规模多模态模型在视觉问答等任务上表现出色，但当查询需要基于文化、日常知识，特别是在低资源语言和代表性不足的语言中时，这些模型常常失败。&lt;h4&gt;目的&lt;/h4&gt;弥合多模态模型在文化背景知识问答方面的差距，引入日常多模态多语言问答(EverydayMMQA)框架，用于创建大规模、基于文化的口语和视觉问答(SVQA)数据集。&lt;h4&gt;方法&lt;/h4&gt;使用EverydayMMQA框架开发了OASIS多模态数据集，整合语音、图像和文本；包含约92万张图像和1480万个问答对，其中370万个口语问题；支持四种输入组合：仅语音、仅文本、语音+图像、文本+图像；专注于英语和阿拉伯语变体，涵盖18个国家；数据集内容反映多样化、真实世界的情境。&lt;h4&gt;主要发现&lt;/h4&gt;OASIS测试模型在超越对象识别的任务上，涉及语用、常识和文化感知推理；对四个闭源模型、三个开源模型和一个微调模型进行了基准测试。&lt;h4&gt;结论&lt;/h4&gt;EverydayMMQA和OASIS一起提供了一个基准和训练数据集，用于构建在文化背景下处理全面日常任务的多模态大语言模型；框架和数据集将对社区公开可用。&lt;h4&gt;翻译&lt;/h4&gt;大规模多模态模型在视觉问答(VQA)等任务上取得强劲成果，但当查询需要基于文化的日常知识，特别是在低资源和代表性不足的语言中时，它们往往会失败。为了弥合这一差距，我们引入了日常多模态多语言问答(EverydayMMQA)，这是一个用于创建大规模、基于文化的口语和视觉问答(SVQA)数据集的框架。使用此框架，我们开发了OASIS，一个整合语音、图像和文本的多模态数据集。OASIS包含约92万张图像和1480万个问答对，其中370万个口语问题，支持四种独特的输入组合：仅语音、仅文本、语音+图像和文本+图像。专注于英语和阿拉伯语变体，18个国家，数据集内容经过策划以反映多样化、真实世界的情境。OASIS测试模型在涉及语用、常识和文化感知推理的超越对象识别的任务。我们对四个闭源模型、三个开源模型和一个微调模型进行了基准测试。EverydayMMQA和OASIS一起提供了一个基准和训练数据集，用于构建在文化背景下处理全面日常任务的多模态大语言模型。该框架和数据集将向社区公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale multimodal models achieve strong results on tasks like VisualQuestion Answering (VQA), but they often fail when queries require culturallygrounded, everyday knowledge, particularly in low-resource and underrepresentedlanguages. To bridge this gap, we introduce Everyday Multimodal andMultilingual QA (EverydayMMQA), a framework for creating large-scale,culturally-grounded datasets for spoken and visual question answering (SVQA).Using this framework, we developed OASIS, a multimodal dataset integratingspeech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIScontains 3.7M spoken questions, enabling four unique input combinations:speech-only, text-only, speech+image, and text+image. Focused on English andArabic varieties, 18 countries, the dataset content is curated to reflectdiverse, real-world situations. OASIS tests models on tasks beyond objectrecognition that involve pragmatic, commonsense, and culturally awarereasoning. We benchmarked four closed-source models, three open-source models,and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmarkand training dataset for building multimodal LLMs for a comprehensive set ofeveryday tasks within cultural contexts. The framework and dataset will be madepublicly available to the community.</description>
      <author>example@mail.com (Firoj Alam, Ali Ezzat Shahroor, Md. Arid Hasan, Zien Sheikh Ali, Hunzalah Hassan Bhatti, Mohamed Bayan Kmainasi, Shammur Absar Chowdhury, Basel Mousi, Fahim Dalvi, Nadir Durrani, Natasa Milic-Frayling)</author>
      <guid isPermaLink="false">2510.06371v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks</title>
      <link>http://arxiv.org/abs/2510.06349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了基础模型在现实世界应用中的局限性，特别是在动态复杂系统决策方面的挑战，并提出了基于小型智能体网络(SANs)的去中心化架构作为替代方案。&lt;h4&gt;背景&lt;/h4&gt;基础模型迅速推动AI发展，但其决策是否超越人类策略尚不明确。尽管AI发展速度极快，但在许多与日常生活和社会相关的应用领域(如重症监护中动态发展疾病的诊断和治疗)只显示出适度的进展。适应复杂系统到动态环境是共同挑战，需要可靠、自适应的建模。&lt;h4&gt;目的&lt;/h4&gt;开发具有最少数据和有限机制知识的自适应AI模型方法，使AI能在复杂动态系统中展现出明显的优越性，然后才能承担更广泛的决策角色。&lt;h4&gt;方法&lt;/h4&gt;提出一种由交互式小型智能体网络(SANs)组成的去中心化架构，专注于代表系统专门子结构的智能体，每个智能体仅覆盖系统功能的一个子集。通过群体学习实现自适应决策。&lt;h4&gt;主要发现&lt;/h4&gt;维度灾难是高效自适应的基本障碍，单一基础模型在克服这一障碍方面面临概念上的限制。多样化群体中的群体学习可以使自适应SANs在动态环境中提供比单一基础模型更好的决策，但代价是细节的可重复性降低。&lt;h4&gt;结论&lt;/h4&gt;对于需要处理复杂动态系统的应用，基于SANs的去中心化架构可能比单一基础模型更有效，尽管在细节可重复性方面有所牺牲。&lt;h4&gt;翻译&lt;/h4&gt;基础模型迅速推动了人工智能的发展，引发了一个问题：它们的决策最终是否会超越人类在现实世界中的策略。人工智能发展的指数级甚至可能是超指数级的速度使得此类分析变得困难。然而，许多对日常生活和社会重要的应用领域迄今为止只显示出适度的进展；一个突出的例子是在重症监护中诊断和治疗动态发展的疾病。共同的挑战是将复杂系统适应动态环境。有效的策略必须在由强相互作用功能组成的系统中优化结果，同时避免共享的副作用；这需要可靠、自适应的建模。这些任务与构建高度复杂系统的数字孪生相一致，而这些系统的机制并未被完全或定量理解。因此，开发具有最少数据和有限机制知识的方法来构建自适应AI模型至关重要。由于这一挑战超出了医学范畴，AI在承担更广泛的决策角色之前，应该在这些环境中展示出明显的优越性。我们将维度灾难视为高效自适应的基本障碍，并论证单一的基础模型在克服它方面面临概念上的限制。作为替代方案，我们提出了一种由交互式小型智能体网络(SANs)组成的去中心化架构。我们专注于代表系统专门子结构的智能体，其中每个智能体仅覆盖系统功能的一个子集。基于SANs学习行为的数学结果和现有应用的证据，我们论证多样化群体中的群体学习可以使自适应SANs在动态环境中提供比单一基础模型更好的决策，尽管代价是细节的可重复性降低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have rapidly advanced AI, raising the question of whethertheir decisions will ultimately surpass human strategies in real-world domains.The exponential, and possibly super-exponential, pace of AI development makessuch analysis elusive. Nevertheless, many application areas that matter fordaily life and society show only modest gains so far; a prominent case isdiagnosing and treating dynamically evolving disease in intensive care.  The common challenge is adapting complex systems to dynamic environments.Effective strategies must optimize outcomes in systems composed of stronglyinteracting functions while avoiding shared side effects; this requiresreliable, self-adaptive modeling. These tasks align with building digital twinsof highly complex systems whose mechanisms are not fully or quantitativelyunderstood. It is therefore essential to develop methods for self-adapting AImodels with minimal data and limited mechanistic knowledge. As this challengeextends beyond medicine, AI should demonstrate clear superiority in thesesettings before assuming broader decision-making roles.  We identify the curse of dimensionality as a fundamental barrier to efficientself-adaptation and argue that monolithic foundation models face conceptuallimits in overcoming it. As an alternative, we propose a decentralizedarchitecture of interacting small agent networks (SANs). We focus on agentsrepresenting the specialized substructure of the system, where each agentcovers only a subset of the full system functions. Drawing on mathematicalresults on the learning behavior of SANs and evidence from existingapplications, we argue that swarm-learning in diverse swarms can enableself-adaptive SANs to deliver superior decision-making in dynamic environmentscompared with monolithic foundation models, though at the cost of reducedreproducibility in detail.</description>
      <author>example@mail.com (Moein E. Samadi, Andreas Schuppert)</author>
      <guid isPermaLink="false">2510.06349v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding</title>
      <link>http://arxiv.org/abs/2510.06308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 13 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Lumina-DiMOO是一个开源的基础模型，用于无缝的多模态生成和理解。&lt;h4&gt;背景&lt;/h4&gt;现有的统一多模态模型在处理效率和任务支持方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高效处理多种模态输入输出并支持广泛多模态任务的基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用全离散扩散建模来处理各种模态的输入和输出，区别于之前的自回归(AR)或混合AR-扩散范式。&lt;h4&gt;主要发现&lt;/h4&gt;实现了比先前AR或混合AR-扩散范式更高的采样效率；能够支持广泛的多模态任务，包括文本到图像生成、图像到图像生成（如图像编辑、主体驱动生成和图像修复等）以及图像理解；在多个基准测试上实现了最先进的性能，超越了现有的开源统一多模态模型。&lt;h4&gt;结论&lt;/h4&gt;为促进多模态和离散扩散模型研究的进一步发展，将代码和模型检查点发布给社区。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了Lumina-DiMOO，这是一个用于无缝多模态生成和理解的开源基础模型。Lumina-DiMOO通过使用全离散扩散建模来处理各种模态的输入和输出，从而区别于之前的统一模型。这种创新方法使Lumina-DiMOO能够比先前的自回归(AR)或混合AR-扩散范式实现更高的采样效率，并能够支持广泛的多模态任务，包括文本到图像生成、图像到图像生成（例如图像编辑、主体驱动生成和图像修复等），以及图像理解。Lumina-DiMOO在多个基准测试上实现了最先进的性能，超越了现有的开源统一多模态模型。为了促进多模态和离散扩散模型研究的进一步发展，我们将代码和模型检查点发布给社区。项目页面：https://synbol.github.io/Lumina-DiMOO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Lumina-DiMOO, an open-source foundational model for seamlessmulti-modal generation and understanding. Lumina-DiMOO sets itself apart fromprior unified models by utilizing a fully discrete diffusion modeling to handleinputs and outputs across various modalities. This innovative approach allowsLumina-DiMOO to achieve higher sampling efficiency compared to previousautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support abroad spectrum of multi-modal tasks, including text-to-image generation,image-to-image generation (e.g., image editing, subject-driven generation, andimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achievesstate-of-the-art performance on multiple benchmarks, surpassing existingopen-source unified multi-modal models. To foster further advancements inmulti-modal and discrete diffusion model research, we release our code andcheckpoints to the community. Project Page:https://synbol.github.io/Lumina-DiMOO.</description>
      <author>example@mail.com (Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo, Junjun He, Gen Luo, Tianbin Li, Ming Hu, Jin Ye, Shenglong Ye, Bo Zhang, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Bin Fu, Xiaohong Liu, Yu Qiao, Yihao Liu)</author>
      <guid isPermaLink="false">2510.06308v1</guid>
      <pubDate>Thu, 09 Oct 2025 16:40:05 +0800</pubDate>
    </item>
    <item>
      <title>Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning</title>
      <link>http://arxiv.org/abs/2510.03885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website can be found at  https://existentialrobotics.org/sbp_page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为'Seeing the Bigger Picture'(SBP)的端到端策略学习方法，该方法通过3D潜在地图增强移动操作策略的空间和时间推理能力，超越了仅依赖图像的传统方法。&lt;h4&gt;背景&lt;/h4&gt;移动操作任务中，机器人需要理解和操作复杂环境。传统的基于图像的方法在空间推理和长期记忆方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用3D潜在地图提供全局上下文和长期记忆的移动操作策略，提高机器人在复杂环境中的操作能力。&lt;h4&gt;方法&lt;/h4&gt;SBP方法直接在3D潜在特征地图上操作，通过增量融合多视角观察创建场景特定的潜在特征网格，使用预训练解码器重建目标嵌入，并在任务执行期间在线优化地图特征。策略将潜在地图视为状态变量，通过3D特征聚合器获取全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;SBP能够：(1)全局推理场景，(2)利用地图作为长期记忆，(3)在分布内和新场景中都优于基于图像的策略，例如在顺序操作任务中将成功率提高了25%。&lt;h4&gt;结论&lt;/h4&gt;基于3D潜在地图的方法为移动操作任务提供了更强大的空间和时间推理能力，显著提高了机器人在复杂环境中的操作性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们证明利用3D潜在地图的移动操作策略比仅依赖图像的策略具有更强的空间和时间推理能力。我们引入了'Seeing the Bigger Picture'(SBP)，一种直接在3D潜在特征地图上操作的端到端策略学习方法。在SBP中，地图扩展了感知范围，超越了机器人的当前视野，并长期聚合观察结果。我们的映射方法将多视角观察增量融合到场景特定的潜在特征网格中。预训练的、与场景无关的解码器从这些特征重建目标嵌入，并在任务执行期间 enables 在线优化地图特征。可以通过行为克隆或强化学习训练的策略将潜在地图视为状态变量，并使用通过3D特征聚合器从地图获取的全局上下文。我们在场景级移动操作和顺序桌面操作任务上评估了SBP。我们的实验证明SBP (i) 全局推理场景，(ii) 利用地图作为长期记忆，以及 (iii) 在分布内和新场景中都优于基于图像的策略，例如，将顺序操作任务的成功率提高了25%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决移动操作策略学习中基于2D图像的方法在空间和时间推理方面的局限性。现有方法在短时间动作预测上有效，但难以保持一致的3D理解和长期时间推理能力，而这对于需要全局空间理解和长期记忆的复杂任务（如长期移动操作）至关重要。随着机器人学习向更大规模环境扩展，这一问题变得尤为突出，因为它限制了机器人在现实世界中处理需要记住物体位置和任务状态的多步骤任务的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从机器人导航中持久3D地图的成功应用获得启发，注意到虽然3D地图在导航中已长期使用，但在操作策略学习中被忽视。他们设计了一种基于3D潜在特征地图的方法，能够增量构建、在线更新，并提供全局上下文。作者借鉴了多个领域的现有工作：3D场景表示（如将2D特征提升到3D或直接编码点云）、视觉-语言模型（如CLIP）的语义能力、神经场景表示技术，以及机器人导航中的持久地图概念。关键创新在于将这些概念结合，专门设计用于长期移动操作任务的端到端策略学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用增量构建的3D潜在特征地图作为机器人策略学习的状态变量，提供全局空间和长期时间上下文，让机器人能'看到更大的图景'。整体流程包括：1) 构建多分辨率3D潜在特征网格，通过三线性插值检索特征；2) 使用预训练解码器将潜在特征映射到目标空间；3) 通过3D特征聚合器将空间分布特征压缩为全局地图令牌；4) 将地图令牌作为额外状态输入策略网络，联合嵌入图像特征、本体感受状态和任务嵌入；5) 使用行为克隆或强化学习训练端到端策略；6) 在任务执行过程中在线更新地图以适应环境变化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将持久3D地图概念引入操作策略学习；2) 设计模块化地图架构，解耦场景特定表示与场景无关映射函数；3) 提出3D特征聚合器生成全局地图令牌；4) 实现端到端策略学习直接在3D潜在特征地图上操作。相比之前工作，不同之处在于：不是每帧重建场景而是构建持久地图；地图作为状态变量而非瞬时观察；支持在线更新以适应环境变化；在目标物体不在视野内时仍能完成任务；在分布内和分布外场景中均表现优异；结合全局空间理解和长期时间记忆处理多步骤任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了'Seeing the Bigger Picture'方法，通过增量构建的3D潜在特征地图作为机器人操作策略的状态变量，显著增强了机器人在复杂环境中的空间和时间推理能力，使其能够超越当前视野限制，利用长期记忆成功完成多步骤任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we demonstrate that mobile manipulation policies utilizing a3D latent map achieve stronger spatial and temporal reasoning than policiesrelying solely on images. We introduce Seeing the Bigger Picture (SBP), anend-to-end policy learning approach that operates directly on a 3D map oflatent features. In SBP, the map extends perception beyond the robot's currentfield of view and aggregates observations over long horizons. Our mappingapproach incrementally fuses multiview observations into a grid ofscene-specific latent features. A pre-trained, scene-agnostic decoderreconstructs target embeddings from these features and enables onlineoptimization of the map features during task execution. A policy, trainablewith behavior cloning or reinforcement learning, treats the latent map as astate variable and uses global context from the map obtained via a 3D featureaggregator. We evaluate SBP on scene-level mobile manipulation and sequentialtabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasonsglobally over the scene, (ii) leverages the map as long-horizon memory, and(iii) outperforms image-based policies in both in-distribution and novelscenes, e.g., improving the success rate by 25% for the sequential manipulationtask.</description>
      <author>example@mail.com (Sunghwan Kim, Woojeh Chung, Zhirui Dai, Dwait Bhatt, Arth Shukla, Hao Su, Yulun Tian, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2510.03885v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
  <item>
      <title>Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</title>
      <link>http://arxiv.org/abs/2510.03342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了Gemini Robotics模型家族的最新一代，包括Gemini Robotics 1.5多形态视觉-语言-动作模型和Gemini Robotics-ER 1.5先进具身推理模型，通过三个主要创新推动通用机器人技术的发展。&lt;h4&gt;背景&lt;/h4&gt;通用机器人需要对物理世界有深入理解、先进推理能力以及通用和灵巧的控制能力。当前机器人技术在处理复杂任务方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发新一代机器人模型，提升机器人的物理理解、推理能力和控制性能，使机器人能够解决复杂的多步骤任务。&lt;h4&gt;方法&lt;/h4&gt;1. 设计新颖的架构和运动传递机制，使模型能够从异构、多形态的机器人数据中学习；2. 将动作与多层次的内部推理过程（使用自然语言）交织，实现'先思考后行动'；3. 开发专门的具身推理模型，提升机器人在视觉和空间理解、任务规划和进度估计方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 新架构和运动传递机制使VLA模型更加通用，能够从多形态机器人数据中学习；2. 多层次内部推理过程提高了机器人分解和执行复杂多步骤任务的能力，增强了行为可解释性；3. Gemini Robotics-ER 1.5在具身推理方面建立了新的技术水平。&lt;h4&gt;结论&lt;/h4&gt;这一系列模型标志着向物理智能体时代迈进了一步，使机器人能够感知、思考和行动，以解决复杂的多步骤任务。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人需要对物理世界有深入理解、先进推理能力以及通用和灵巧的控制能力。本报告介绍了Gemini Robotics模型家族的最新一代：Gemini Robotics 1.5，一种多形态视觉-语言-动作模型，以及Gemini Robotics-ER1.5，一种先进的具身推理模型。我们带来了三大创新。首先，Gemini Robotics 1.5采用了新颖的架构和运动传递机制，使其能够从异构、多形态的机器人数据中学习，使VLA模型更加通用。其次，Gemini Robotics 1.5将动作与多层次的内部推理过程（使用自然语言）交织在一起，使机器人能够'先思考后行动'，显著提高其分解和执行复杂、多步骤任务的能力，同时也使机器人的行为对用户更具可解释性。第三，Gemini Robotics-ER 1.5在具身推理方面建立了新的技术水平，即对机器人至关重要的推理能力，如视觉和空间理解、任务规划和进度估计。总的来说，这一系列模型使我们向物理智能体时代迈进了一步——使机器人能够感知、思考和行动，从而解决复杂的多步骤任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何创建真正通用的机器人问题，这些机器人需要具备对物理世界的深刻理解、高级推理能力和通用的灵巧控制能力。这个问题在现实中很重要，因为通用机器人可以执行各种复杂任务，从家庭环境到工业环境，具有广泛应用前景；同时，当前机器人系统通常只针对特定任务优化，缺乏通用性和适应性，解决这个问题将推动人工智能在物理世界中的应用，实现更高级的人机协作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到通用机器人需要三个关键能力：对物理世界的理解、高级推理和通用控制。他们基于之前的Gemini Robotics工作进行扩展，借鉴了现有的Vision-Language-Action (VLA)模型和Embodied Reasoning (ER)模型概念。作者设计了两个互补模型：Gemini Robotics 1.5 (VLA)和Gemini Robotics-ER 1.5 (ER)，引入了'Thinking'机制和Motion Transfer机制，并构建了代理系统整合这些组件。他们借鉴了Gemini基础模型架构、多模态学习、迁移学习和thinking机制等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1)'Thinking before Acting'：机器人在行动前进行内部思考，分解复杂任务；2)'Motion Transfer'：从不同机器人数据中学习通用技能；3)'Embodied Reasoning'：结合高级物理理解和推理；4)'Agentic System'：整合高级推理和动作模型。实现流程：1)收集多机器人形态数据；2)分别训练GR-ER 1.5(具身推理)和GR 1.5(使用Motion Transfer)；3)构建编排器和动作模型组成的代理系统；4)执行任务时编排器分解任务，动作模型思考后执行；5)实施多层次安全保障。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)'Thinking VLA'机制：行动前内部思考和推理；2)Motion Transfer机制：实现跨机器人技能迁移；3)Gemini Robotics-ER 1.5：在具身推理任务上达到最先进水平；4)多层次代理系统架构：整合高级推理和低级控制。相比之前工作：1.5版本增加了思考和跨形态学习能力；可控制多种不同形态机器人；采用分层架构而非简单端到端模型；在保持通用能力的同时优化具身任务；更注重安全性和对齐问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入'Thinking before Acting'机制、Motion Transfer技能迁移方法和多层次代理系统，显著推进了通用机器人在具身推理、思考和动作迁移方面的发展，使机器人能够更有效地理解和执行复杂的多步骤任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose robots need a deep understanding of the physical world,advanced reasoning, and general and dexterous control. This report introducesthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing togetherthree major innovations. First, Gemini Robotics 1.5 features a novelarchitecture and a Motion Transfer (MT) mechanism, which enables it to learnfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.Second, Gemini Robotics 1.5 interleaves actions with a multi-level internalreasoning process in natural language. This enables the robot to "think beforeacting" and notably improves its ability to decompose and execute complex,multi-step tasks, and also makes the robot's behavior more interpretable to theuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art forembodied reasoning, i.e., for reasoning capabilities that are critical forrobots, such as visual and spatial understanding, task planning, and progressestimation. Together, this family of models takes us a step towards an era ofphysical agents-enabling robots to perceive, think and then act so they cansolve complex multi-step tasks.</description>
      <author>example@mail.com (Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Amaris Paryag, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Peter Pastor Sampedro, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou, Yuxiang Zhou)</author>
      <guid isPermaLink="false">2510.03342v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning</title>
      <link>http://arxiv.org/abs/2510.06068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于特征抓取的端到端框架，用于跨实体形态的灵巧抓取生成，解决了多指手抓取中的高维关节运动和优化成本问题，实现了在不同形态手之间的有效泛化。&lt;h4&gt;背景&lt;/h4&gt;灵巧的多指手抓取具有挑战性，主要由于高维关节运动和基于优化的管道的高成本。现有的端到端方法需要在特定手的大规模数据集上进行训练，限制了它们在不同实体形态之间泛化的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够跨不同实体形态进行抓取生成的端到端框架，无需针对每种手型进行大规模数据集训练。&lt;h4&gt;方法&lt;/h4&gt;从手的形态描述中推导形态嵌入和特征抓取集，结合物体点云和手腕姿态，通过振幅预测器在低维空间回归关节系数，再解码为完整关节运动。使用运动感知关节损失(KAL)进行监督学习，强调指尖相关运动并注入形态特定结构。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟环境中对三种灵巧手和未见物体测试，平均抓取成功率达91.9%，每次抓取推理时间少于0.4秒；通过少样本适应对未见手，在模拟中未见物体上达到85.6%成功率；真实世界实验中少样本泛化手达到87%成功率。&lt;h4&gt;结论&lt;/h4&gt;所提出的特征抓取端到端框架能够有效处理不同形态手的灵巧抓取问题，具有高泛化能力和计算效率，无需针对每种手型进行大规模数据集训练。&lt;h4&gt;翻译&lt;/h4&gt;灵巧的多指手抓取仍然具有挑战性，这是由于高维关节运动和基于优化的管道的高成本。现有的端到端方法需要在特定手的大规模数据集上进行训练，限制了它们在不同实体形态之间泛化的能力。我们提出了一种基于特征抓取的端到端框架，用于跨实体形态的抓取生成。从手的形态描述中，我们推导出形态嵌入和特征抓取集。基于这些，结合物体点云和手腕姿态，振幅预测器在低维空间回归关节系数，这些系数被解码为完整的关节运动。关节学习通过运动感知关节损失(KAL)进行监督，该损失强调与指尖相关的运动并注入形态特定的结构。在模拟中对三种灵巧手的未见物体进行测试，我们的模型平均抓取成功率达到91.9%，每次抓取推理时间少于0.4秒。通过对未见手的少样本适应，在模拟中未见物体上达到85.6%的成功率，而在这个少样本泛化手的真实世界实验中达到87%的成功率。代码和附加材料将在我们的项目网站上发布：https://connor-zh.github.io/cross_embodiment_dexterous_grasping。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping with multi-fingered hands remains challenging due tohigh-dimensional articulations and the cost of optimization-based pipelines.Existing end-to-end methods require training on large-scale datasets forspecific hands, limiting their ability to generalize across differentembodiments. We propose an eigengrasp-based, end-to-end framework forcross-embodiment grasp generation. From a hand's morphology description, wederive a morphology embedding and an eigengrasp set. Conditioned on these,together with the object point cloud and wrist pose, an amplitude predictorregresses articulation coefficients in a low-dimensional space, which aredecoded into full joint articulations. Articulation learning is supervised witha Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevantmotions and injects morphology-specific structure. In simulation on unseenobjects across three dexterous hands, our model attains a 91.9% average graspsuccess rate with less than 0.4 seconds inference per grasp. With few-shotadaptation to an unseen hand, it achieves 85.6% success on unseen objects insimulation, and real-world experiments on this few-shot generalized handachieve an 87% success rate. The code and additional materials will be madeavailable upon publication on our project websitehttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.</description>
      <author>example@mail.com (Heng Zhang, Kevin Yuchen Ma, Mike Zheng Shou, Weisi Lin, Yan Wu)</author>
      <guid isPermaLink="false">2510.06068v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Carré du champ flow matching: better quality-generalisation tradeoff in generative models</title>
      <link>http://arxiv.org/abs/2510.05930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Carré du champ flow matching (CDC-FM)的新方法，通过几何感知噪声正则化概率路径，改善了深度生成模型中样本质量和泛化能力之间的权衡关系。&lt;h4&gt;背景&lt;/h4&gt;深度生成模型通常面临基本权衡：高样本质量可能导致模型记忆化训练数据而非泛化到基础数据几何结构上。&lt;h4&gt;目的&lt;/h4&gt;引入CDC-FM方法，作为flow matching(FM)的泛化，通过几何感知噪声正则化概率路径，改善质量-泛化权衡。&lt;h4&gt;方法&lt;/h4&gt;用空间变化、各向异性高斯噪声替代FM中的同质、各向同性噪声，这种噪声的协方差能捕获潜在数据流形的局部几何结构；证明几何噪声可从数据中最优估计且可扩展到大数据；在多种数据集和神经网络架构上进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;CDC-FM始终提供更好的质量-泛化权衡；在数据稀缺环境和高度非均匀采样数据集中相比标准FM有显著改进；这些情况在AI科学应用中经常遇到。&lt;h4&gt;结论&lt;/h4&gt;该工作为研究生成模型中数据几何、泛化和记忆化之间的相互作用提供了数学框架；提供了一种稳健且可扩展的算法，可轻松集成到现有flow matching流程中。&lt;h4&gt;翻译&lt;/h4&gt;深度生成模型通常面临一个基本权衡：高样本质量可能以记忆化为代价，即模型重现训练数据而非泛化到基础数据几何结构上。我们引入了Carré du champ flow matching (CDC-FM)，这是flow matching(FM)的一种泛化，通过用几何感知噪声正则化概率路径来改善质量-泛化权衡。我们的方法用空间变化、各向异性高斯噪声替代FM中的同质、各向同性噪声，其协方差能够捕获潜在数据流形的局部几何结构。我们证明这种几何噪声可以从数据中最优估计，并且可扩展到大数据集。此外，我们在多种数据集（合成流形、点云、单细胞基因组学、动物运动捕捉和图像）以及各种神经网络架构（MLP、CNN和transformers）上提供了广泛的实验评估。我们证明CDC-FM始终提供更好的质量-泛化权衡。我们在数据稀缺环境和高度非均匀采样数据集中观察到相比标准FM的显著改进，这些情况在AI科学应用中经常遇到。我们的工作为研究生成模型中数据几何、泛化和记忆化之间的相互作用提供了数学框架，以及一种可以轻松集成到现有flow matching流程中的稳健且可扩展的算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep generative models often face a fundamental tradeoff: high sample qualitycan come at the cost of memorisation, where the model reproduces training datarather than generalising across the underlying data geometry. We introduceCarr\'e du champ flow matching (CDC-FM), a generalisation of flow matching(FM), that improves the quality-generalisation tradeoff by regularising theprobability path with a geometry-aware noise. Our method replaces thehomogeneous, isotropic noise in FM with a spatially varying, anisotropicGaussian noise whose covariance captures the local geometry of the latent datamanifold. We prove that this geometric noise can be optimally estimated fromthe data and is scalable to large data. Further, we provide an extensiveexperimental evaluation on diverse datasets (synthetic manifolds, point clouds,single-cell genomics, animal motion capture, and images) as well as variousneural network architectures (MLPs, CNNs, and transformers). We demonstratethat CDC-FM consistently offers a better quality-generalisation tradeoff. Weobserve significant improvements over standard FM in data-scarce regimes and inhighly non-uniformly sampled datasets, which are often encountered in AI forscience applications. Our work provides a mathematical framework for studyingthe interplay between data geometry, generalisation and memorisation ingenerative models, as well as a robust and scalable algorithm that can bereadily integrated into existing flow matching pipelines.</description>
      <author>example@mail.com (Jacob Bamberger, Iolo Jones, Dennis Duncan, Michael M. Bronstein, Pierre Vandergheynst, Adam Gosztolai)</author>
      <guid isPermaLink="false">2510.05930v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.05752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了ALISE，一种无需任何标注即可进行激光雷达实例分割的新型框架，通过视觉基础模型生成初始伪标签，结合时空投票模块和语义监督策略，在无监督3D实例分割任务上建立了新的最先进水平。&lt;h4&gt;背景&lt;/h4&gt;户外激光雷达点云的手动实例标注极其耗时且成本高昂。当前方法试图减轻这一负担，但仍依赖于某种形式的人工标注。&lt;h4&gt;目的&lt;/h4&gt;完全消除对人工标注的依赖，开发一种能够在没有任何标注的情况下进行激光雷达实例分割的方法。&lt;h4&gt;方法&lt;/h4&gt;1. 使用视觉基础模型(VFMs)在文本和图像指导下生成初始伪标签；2. 通过时空投票模块优化标签，结合2D和3D语义进行离线和在线优化；3. 引入基于2D先验的损失函数将视觉知识注入3D网络；4. 使用基于原型的对比损失利用3D语义一致性构建判别性特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;该综合设计带来了显著的性能提升，为无监督3D实例分割建立了新的最先进水平。ALISE甚至超过了使用真实2D边界框监督的MWSIS方法，在平均精度(mAP)上以2.53%的优势(50.95%对比48.42%)胜出。&lt;h4&gt;结论&lt;/h4&gt;ALISE框架成功地实现了无需任何标注的激光雷达实例分割，通过创新的伪标签生成和优化方法，以及语义监督策略，在性能上超越了现有方法，甚至超过了有监督的方法。&lt;h4&gt;翻译&lt;/h4&gt;户外激光雷达点云的手动实例标注极其耗时且成本高昂。当前方法试图减轻这一负担，但仍依赖于某种形式的人工标注。为了完全消除这种依赖，我们引入了ALISE，一种无需任何标注即可执行激光雷达实例分割的新型框架。核心挑战是以完全无监督的方式生成高质量的伪标签。我们的方法首先使用视觉基础模型(VFMs)，在文本和图像的指导下生成初始伪标签。然后，我们通过专门的时空投票模块优化这些标签，该模块结合2D和3D语义进行离线和在线优化。为了实现卓越的特征学习，我们进一步引入了两种形式的语义监督：一组基于2D先验的损失函数，将视觉知识注入3D网络；以及一种新颖的基于原型的对比损失，通过利用3D语义一致性构建判别性特征空间。这种全面的设计带来了显著的性能提升，为无监督3D实例分割建立了新的最先进水平。值得注意的是，我们的方法甚至超过了MWSIS(一种使用真实2D边界框监督的方法)，在平均精度(mAP)上以2.53%的优势(50.95%对比48.42%)胜出。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决激光雷达点云实例分割任务中依赖密集人工标注的问题。这个问题在现实中非常重要，因为3D点云标注成本极高、耗时极长，限制了自动驾驶感知技术的发展和应用范围。完全消除标注依赖可以大幅降低数据获取成本，加速自动驾驶技术的普及和应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有弱监督方法仍需某种形式人工标注的局限性，然后借鉴了视觉基础模型(VFMs)如GroundingDINO和SAM的能力，设计了ALISE框架。该方法融合了多阶段伪标签精炼(离线精炼OFR和在线精炼ONR)和多方面监督方案(VPD和PCL模块)，通过时序一致性和跨模态知识传递来提升性能。作者也参考了对比学习、知识蒸馏等现有技术，但创新性地将其组合应用于完全无标注的3D实例分割场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型生成高质量的3D点云实例分割伪标签，并通过多阶段精炼和知识蒸馏机制，使3D分割网络能够从这些伪标签中学习，实现无需任何人工标注的实例分割。整体流程包括：1)无监督伪标签生成(UPG)利用VFMs生成初始标签；2)两阶段精炼策略，包括利用相邻帧信息的离线精炼(OFR)和利用网络自身预测的在线精炼(ONR)；3)多方面监督训练，包括VFMs先验知识蒸馏(VPD)和基于原型的对比学习(PCL)；4)结合多个损失项进行网络训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全无标注框架，消除任何形式的人工标注依赖；2)创新的伪标签生成与精炼管道，保留VFMs的语义分布而非直接生成one-hot标签；3)多阶段精炼策略，结合离线和在线优化；4)多方面监督方案，包括软标签蒸馏和动态原型对比学习。相比之前工作，ALISE不仅超越了多种弱监督方法，甚至在某些情况下超过了使用GT 2D边界框监督的方法，且仅需少量标注数据微调就能超过全监督基线。之前的无监督方法通常生成硬标签且缺乏时序一致性考虑，而ALISE通过软标签和时序投票解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALISE提出了一种完全无标注的激光雷达实例分割框架，通过多阶段伪标签精炼和知识蒸馏技术，显著提升了无监督3D实例分割性能，甚至在某些情况下超越了弱监督方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The manual annotation of outdoor LiDAR point clouds for instance segmentationis extremely costly and time-consuming. Current methods attempt to reduce thisburden but still rely on some form of human labeling. To completely eliminatethis dependency, we introduce ALISE, a novel framework that performs LiDARinstance segmentation without any annotations. The central challenge is togenerate high-quality pseudo-labels in a fully unsupervised manner. Ourapproach starts by employing Vision Foundation Models (VFMs), guided by textand images, to produce initial pseudo-labels. We then refine these labelsthrough a dedicated spatio-temporal voting module, which combines 2D and 3Dsemantics for both offline and online optimization. To achieve superior featurelearning, we further introduce two forms of semantic supervision: a set of 2Dprior-based losses that inject visual knowledge into the 3D network, and anovel prototype-based contrastive loss that builds a discriminative featurespace by exploiting 3D semantic consistency. This comprehensive design resultsin significant performance gains, establishing a new state-of-the-art forunsupervised 3D instance segmentation. Remarkably, our approach evenoutperforms MWSIS, a method that operates with supervision from ground-truth(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).</description>
      <author>example@mail.com (Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu)</author>
      <guid isPermaLink="false">2510.05752v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
      <link>http://arxiv.org/abs/2510.05613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointNSP的自回归点云生成框架，通过多尺度分解解决了传统自回归方法中的序列偏差问题，首次实现了在自回归范式中达到最先进的点云生成质量。&lt;h4&gt;背景&lt;/h4&gt;自回归点云生成在质量上一直落后于基于扩散的方法，这是因为自回归模型对本质上无序的点集施加了人为排序，导致形状生成本质上是一系列局部预测的序列过程。&lt;h4&gt;目的&lt;/h4&gt;解决自回归点云生成中的序列偏差问题，使模型能够捕获长程依赖关系并强制执行全局结构属性，如对称性、一致拓扑和大尺度几何规律性。&lt;h4&gt;方法&lt;/h4&gt;受形状建模中细节层次（LOD）原则的启发，提出了PointNSP，一种从粗到细的生成框架。该框架在低分辨率保留全局形状结构，通过下一尺度预测范式逐步在高尺度细化精细几何，使自回归目标与点集的排列不变性质保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet上的实验表明，PointNSP首次在自回归范式内建立了最先进的生成质量；在参数、训练和推理效率方面超过了基于扩散的基线方法；在8,192点的密集生成中，其优势更加明显，突显了可扩展性潜力。&lt;h4&gt;结论&lt;/h4&gt;PointNSP通过多尺度分解成功解决了自回归点云生成中的序列偏差问题，实现了高质量、高效率的点云生成，为自回归方法在点云生成领域的应用开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;自回归点云生成在质量上一直落后于基于扩散的方法。这种性能差距源于自回归模型对本质上无序的点集施加了人为排序，迫使形状生成本质上作为一系列局部预测的序列过程。这种序列偏差强调短程连续性，但损害了模型捕获长程依赖关系的能力，阻碍了其强制执行全局结构属性（如对称性、一致拓扑和大尺度几何规律性）的能力。受形状建模中细节层次（LOD）原则的启发，我们提出了PointNSP，一种从粗到细的生成框架，在低分辨率保留全局形状结构，并通过下一尺度预测范式逐步在高尺度细化精细几何。这种多尺度分解使自回归目标与点集的排列不变性质保持一致，实现了丰富的尺度内交互，同时避免了脆弱的固定排序。ShapeNet上的实验表明，PointNSP首次在自回归范式内建立了最先进的生成质量。此外，它在参数、训练和推理效率方面超过了强大的基于扩散的基线方法。最后，在8,192点的密集生成中，PointNSP的优势变得更加明显，突显了其可扩展性潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云生成中自回归模型质量落后于扩散模型的问题。传统自回归方法对无序点集施加人为顺序，导致生成过程偏向局部预测而难以捕捉全局结构。这个问题很重要，因为点云是3D物体形状的基本表示，广泛应用于自动驾驶、机器人感知、计算机辅助设计等领域，高质量点云生成对于形状合成、重建等任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受形状建模中细节层次(LOD)原则启发，结合视觉自回归建模在2D图像生成中的成功经验，设计了从粗到细的生成框架。方法借鉴了VAR(视觉自回归模型)的多尺度预测思想，并采用Farthest Point Sampling(FPS)算法获取多尺度表示，同时创新性地设计了多尺度残差向量量化和位置感知的注意力机制，以适应3D点云的无序特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预测下一尺度的细节层次(LoD)来逐步生成3D点云，而非传统方法中的逐点预测。整体流程包括：1)使用FPS算法获取多尺度LoD序列；2)通过残差查询提取多尺度特征并使用RVQ量化为标记；3)训练自回归Transformer进行下一尺度预测，结合块对角因果掩码和位置感知软掩码建模尺度内和尺度间交互；4)从最粗尺度开始逐步生成，最终组合所有尺度重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从粗到细的生成策略，预测整个尺度而非单个点；2)保持点集的排列不变性；3)多尺度因子化对齐自回归目标与排列不变性质；4)高效的多尺度特征表示；5)位置感知的注意力机制。相比传统自回归方法，PointNSP保留了全局结构；相比扩散方法，它避免了高计算成本的迭代去噪；相比VAR方法，它专门针对3D点云特性进行了优化，并在参数效率和生成速度上有显著优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointNSP提出了一种从粗到细的自回归3D点云生成框架，通过预测下一尺度的细节层次，在保持排列不变性的同时实现了最先进的生成质量和显著的效率提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoregressive point cloud generation has long lagged behind diffusion-basedapproaches in quality. The performance gap stems from the fact thatautoregressive models impose an artificial ordering on inherently unorderedpoint sets, forcing shape generation to proceed as a sequence of localpredictions. This sequential bias emphasizes short-range continuity butundermines the model's capacity to capture long-range dependencies, hinderingits ability to enforce global structural properties such as symmetry,consistent topology, and large-scale geometric regularities. Inspired by thelevel-of-detail (LOD) principle in shape modeling, we propose PointNSP, acoarse-to-fine generative framework that preserves global shape structure atlow resolutions and progressively refines fine-grained geometry at higherscales through a next-scale prediction paradigm. This multi-scale factorizationaligns the autoregressive objective with the permutation-invariant nature ofpoint sets, enabling rich intra-scale interactions while avoiding brittle fixedorderings. Experiments on ShapeNet show that PointNSP establishesstate-of-the-art (SOTA) generation quality for the first time within theautoregressive paradigm. In addition, it surpasses strong diffusion-basedbaselines in parameter, training, and inference efficiency. Finally, in densegeneration with 8,192 points, PointNSP's advantages become even morepronounced, underscoring its scalability potential.</description>
      <author>example@mail.com (Ziqiao Meng, Qichao Wang, Zhiyang Dou, Zixing Song, Zhipeng Zhou, Irwin King, Peilin Zhao)</author>
      <guid isPermaLink="false">2510.05613v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Human Action Recognition from Point Clouds over Time</title>
      <link>http://arxiv.org/abs/2510.05506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新颖的3D视频动作识别方法，通过点云分割、跟踪和身体部位分割流程，结合基于点技术和稀疏卷积网络，实现了与现有骨骼动作识别算法相媲美的性能，并在集成设置中达到了89.3%的准确率。&lt;h4&gt;背景&lt;/h4&gt;当前人类动作识别研究主要集中在骨骼动作识别和基于视频的方法。随着消费级深度传感器和激光雷达设备的普及，有机会利用密集的3D数据进行动作识别，开发第三种方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用3D视频进行动作识别的新方法，形成不同于骨骼动作识别和基于视频方法的第三种途径。&lt;h4&gt;方法&lt;/h4&gt;提出了一种3D视频动作识别方法，引入了从场景背景分割人体点云、随时间跟踪个体、执行身体部位分割的流程。支持深度传感器和单目深度估计的点云，核心是一种结合基于点技术与稀疏卷积网络的新骨干网络。实验中结合了表面法线、颜色、红外强度和身体部位解析标签等辅助特征提高识别准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB-D 120数据集上的评估表明，该方法与现有骨骼动作识别算法具有竞争力。在集成设置中结合基于传感器和估计的深度输入，当训练和测试使用不同受试者时，达到89.3%的准确率，超过了之前的点云动作识别方法。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种利用密集3D数据进行动作识别的有效途径，结合不同来源的深度数据可以进一步提高性能。&lt;h4&gt;翻译&lt;/h4&gt;最近的人类动作识别研究主要集中在骨骼动作识别和基于视频的方法上。随着消费级深度传感器和激光雷达设备的日益普及，利用密集的3D数据进行动作识别的机会正在增加，从而开发第三种方法。本文通过引入一个从场景背景分割人体点云、随时间跟踪个体并执行身体部位分割的流程，提出了一种从3D视频识别动作的新方法。该方法支持来自深度传感器和单目深度估计的点云。所提出的HAR框架核心是一种新的3D动作识别骨干网络，它将基于点技术与应用于体素映射点云序列的稀疏卷积网络相结合。实验包括表面法线、颜色、红外强度和身体部位解析标签等辅助点特征，以提高识别准确性。在NTU RGB-D 120数据集上的评估表明，该方法与现有的骨骼动作识别算法具有竞争力。此外，在集成设置中结合基于传感器和估计的深度输入，当考虑不同受试者进行训练和测试时，该方法达到89.3%的准确率，超过了之前的点云动作识别方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从3D点云序列中识别人类动作的问题。现有的人体动作识别研究主要基于视频或骨骼数据，而随着深度传感器和激光雷达设备的普及，利用密集3D数据进行动作识别的机会增加。这个问题在现实中非常重要，因为它可以应用于监控系统中实现异常和暴力检测的自动化，帮助识别老年人跌倒，实现视频自动标注（特别是在体育分析中），以及在自动驾驶中确保行人和驾驶员的安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有两种主要动作识别方法（视频方法和骨骼方法）的局限性，然后指出现有点云动作识别方法的不足，包括缺乏人体分割、身体部位分割和辅助特征使用。基于这些分析，作者设计了一个新流程，适用于有深度传感器和只有RGB视频两种场景。作者借鉴了现有的深度学习技术，如M2FP模型进行实例分割、ByteTrack算法进行人物跟踪、迭代最远点采样进行点采样、T-Net进行点云嵌入，以及稀疏卷积神经网络进行特征提取。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用点云数据中的3D几何信息进行动作识别，结合点处理技术和稀疏卷积网络，通过人体分割和身体部位分割去除背景噪声，并提取更精细的特征。整体流程分为两个阶段：1) 数据预处理阶段：对输入图像进行实例和身体部分分割，进行掩码去噪，将实例投影到3D空间，去除异常点，进行人物跟踪，采样点云并计算表面法线；2) 动作识别模型阶段：使用T-Net嵌入层获取全局信息，将点云映射到体素网格，使用稀疏CNN骨干网络提取特征，进行全局池化，最后通过全连接层进行动作分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 全新的点云动作识别流程，同时进行实例和身体部分分割；2) 新颖的骨干网络架构SP-HP-ConvoT，结合点处理技术和稀疏卷积网络；3) 利用表面法线、红外/RGB颜色和身体部位标签等多种辅助特征；4) 适用于深度传感器和单目深度估计两种场景。相比之前的工作，该方法不依赖于关键点估计（避免误差），保留了更丰富的3D几何信息，进行了人体分割减少背景噪声，且不需要深度传感器也能工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新颖的从3D点云序列识别人类动作的方法，通过结合人体分割、身体部位分割和稀疏卷积网络，在NTU RGB-D 120数据集上达到了最先进的性能，并且适用于深度传感器和单目深度估计两种场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research into human action recognition (HAR) has focused predominantlyon skeletal action recognition and video-based methods. With the increasingavailability of consumer-grade depth sensors and Lidar instruments, there is agrowing opportunity to leverage dense 3D data for action recognition, todevelop a third way. This paper presents a novel approach for recognizingactions from 3D videos by introducing a pipeline that segments human pointclouds from the background of a scene, tracks individuals over time, andperforms body part segmentation. The method supports point clouds from bothdepth sensors and monocular depth estimation. At the core of the proposed HARframework is a novel backbone for 3D action recognition, which combinespoint-based techniques with sparse convolutional networks applied tovoxel-mapped point cloud sequences. Experiments incorporate auxiliary pointfeatures including surface normals, color, infrared intensity, and body partparsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D120 dataset demonstrates that the method is competitive with existing skeletalaction recognition algorithms. Moreover, combining both sensor-based andestimated depth inputs in an ensemble setup, this approach achieves 89.3%accuracy when different human subjects are considered for training and testing,outperforming previous point cloud action recognition methods.</description>
      <author>example@mail.com (James Dickens)</author>
      <guid isPermaLink="false">2510.05506v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Tensor-on-tensor Regression Neural Networks for Process Modeling with High-dimensional Data</title>
      <link>http://arxiv.org/abs/2510.05329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种新的张量对张量回归神经网络（TRNN），用于处理现代传感和计量系统产生的大规模异构高维数据。&lt;h4&gt;背景&lt;/h4&gt;现代传感和计量系统现在正在传输TB级的异构、高维数据，这些数据的自然表示是多向张量。&lt;h4&gt;目的&lt;/h4&gt;开发一种回归模型，既能保持张量几何结构，又能捕获主导工业和机械过程的显著非线性相互作用。&lt;h4&gt;方法&lt;/h4&gt;提出了一种张量对张量回归神经网络（TRNN），统一了基于张量的回归器和传统神经网络的范式。&lt;h4&gt;主要发现&lt;/h4&gt;现有的基于张量的回归器本质上是线性的，而传统神经网络在处理时会丢失空间结构并导致参数数量过多。&lt;h4&gt;结论&lt;/h4&gt;TRNN能够统一保持张量几何结构和表达非线性这两个范式，为处理高维数据提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现代传感和计量系统现在正在传输TB级的异构、高维数据，这些数据的自然表示是多向张量。理解这类数据需要保持张量几何结构的回归模型，同时要有足够的表达能力来捕获主导许多工业和机械过程的显著非线性相互作用。现有的基于张量的回归器满足第一个要求但本质上是线性的。相反，传统的神经网络只有在展平后才提供非线性，从而丢弃了空间结构并导致参数数量过多。本文引入了一个张量对张量回归神经网络（TRNN），它统一了这两个范式。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在处理高维数据时同时保留数据的张量结构（多路结构）并捕获数据间的非线性关系。这个问题在现实中非常重要，因为现代工业环境中传感器和测量系统产生大量异构高维数据（如数据曲线、图像和点云），这些数据自然表示为多路张量。理解这些数据需要既能保留张量几何结构又能表达复杂非线性相互作用的回归模型，这对实现更深层次的过程理解、更早的故障检测和更严格的质量控制至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：基于张量的回归器保留结构但本质上是线性的，而传统神经网络在展平后才能提供非线性，导致丢失空间结构和参数过多。作者认识到需要统一这两种范式，借鉴了自编码器的编码器-解码器架构，但进行了张量化改进。同时，作者借鉴了张量分解技术（特别是Tucker分解）来减少参数数量，并利用ReLU激活函数引入非线性，还展示了他们的一个线性特例可以简化为偏最小二乘法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用张量神经网络同时保留多路数据结构和捕捉非线性关系，采用类似自编码器的架构但使用张量操作代替向量操作，并在中间层引入收缩算子允许不同阶数的张量映射。整体流程分为三部分：1）编码器通过收缩Tucker层逐步减少输入张量维度，每层后接ReLU激活；2）收缩层（瓶颈）使用爱因斯坦收缩积将压缩特征映射到潜在表示，可改变张量阶数；3）解码器通过扩展ReLU层和Tucker层重建输出张量结构。训练时使用均方误差作为损失函数，通过推导的前向和反向传播公式进行端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个非线性张量-张量回归神经网络框架；2）张量化自编码器架构，使用可学习Tucker层保留多线性结构；3）瓶颈处引入收缩算子允许不同阶数张量映射；4）通过Tucker分解显著减少参数数量。相比之前工作，与线性张量回归方法相比能捕捉非线性关系，RMSE降低最多45%；与传统神经网络相比保留张量结构、减少参数、降低过拟合风险；与近期相关工作相比不强制输入输出张量同阶、不限制表达能力在特定阶段。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的张量-张量回归神经网络框架，能够在保留高维数据多路结构的同时有效捕捉非线性关系，显著提高了工业过程建模的准确性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern sensing and metrology systems now stream terabytes of heterogeneous,high-dimensional (HD) data profiles, images, and dense point clouds, whosenatural representation is multi-way tensors. Understanding such data requiresregression models that preserve tensor geometry, yet remain expressive enoughto capture the pronounced nonlinear interactions that dominate many industrialand mechanical processes. Existing tensor-based regressors meet the firstrequirement but remain essentially linear. Conversely, conventional neuralnetworks offer nonlinearity only after flattening, thereby discarding spatialstructure and incurring prohibitive parameter counts. This paper introduces aTensor-on-Tensor Regression Neural Network (TRNN) that unifies these twoparadigms.</description>
      <author>example@mail.com (Qian Wang, Mohammad N. Bisheh, Kamran Paynabar)</author>
      <guid isPermaLink="false">2510.05329v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Identification in source apportionment using geometry</title>
      <link>http://arxiv.org/abs/2510.03616v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究解决了源解析分析中非负矩阵分解(NMF)的非唯一性和依赖不可验证假设的问题，建立了更弱条件下源归因百分比矩阵的可识别性，并提出了一种几何估计方法。&lt;h4&gt;背景&lt;/h4&gt;源解析分析旨在量化多种空气污染物观测浓度对特定来源的归因，通常表述为非负矩阵分解(NMF)问题，但NMF是非唯一的，且依赖于不可验证的假设如稀疏性和不可解释的缩放比例。&lt;h4&gt;目的&lt;/h4&gt;建立源归因百分比矩阵在更弱和更现实条件下的可识别性，开发一种不依赖稀疏性或参数分布假设的几何估计方法。&lt;h4&gt;方法&lt;/h4&gt;引入源归因百分比矩阵的总体估计量，证明其尺度不变性和可识别性；将数据视为锥壳中的点云，开发几何估计量，无需稀疏性或参数分布假设，同时适应时空依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;源归因百分比矩阵的几何估计量在无需稀疏性或参数分布假设的情况下是一致的，能够适应时空依赖性，即使在NMF因子不可识别时仍然可识别。&lt;h4&gt;结论&lt;/h4&gt;数值实验验证了所提出的理论和方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;源解析分析旨在量化观测到的多种空气污染物浓度对特定来源的归因，可以表述为非负矩阵分解(NMF)问题。然而，NMF是非唯一的，并且通常依赖于不可验证的假设，如稀疏性和不可解释的缩放比例。在本手稿中，我们在更弱和更现实的条件下建立了源归因百分比矩阵的可识别性。我们引入了该矩阵的总体估计量，并证明即使NMF因子不可识别，它也是尺度不变的且可识别的。将数据视为锥壳中的一个点云，我们展示了源归因百分比矩阵的几何估计量是一致的，无需任何稀疏性或参数分布假设，同时能够适应时空依赖性。数值实验验证了这一理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source apportionment analysis, which aims to quantify the attribution ofobserved concentrations of multiple air pollutants to specific sources, can beformulated as a non-negative matrix factorization (NMF) problem. However, NMFis non-unique and typically relies on unverifiable assumptions such as sparsityand uninterpretable scalings. In this manuscript, we establish identifiabilityof the source attribution percentage matrix under much weaker and morerealistic conditions. We introduce the population-level estimand for thismatrix, and show that it is scale-invariant and identifiable even when the NMFfactors are not. Viewing the data as a point cloud in a conical hull, we showthat a geometric estimator of the source attribution percentage matrix isconsistent without any sparsity or parametric distributional assumptions, andwhile accommodating spatio-temporal dependence. Numerical experimentscorroborate the theory.</description>
      <author>example@mail.com (Bora Jin, Abhirup Datta)</author>
      <guid isPermaLink="false">2510.03616v2</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.06066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了深度图神经网络中过平滑现象的影响因素，通过提出新的MASED指标量化过平滑程度，并基于理论分析提出了G-Reg正则化方案，有效提高了深度GNN的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;深度图神经网络在处理图结构数据时会出现过平滑问题，即随着网络层数增加，节点嵌入逐渐趋同，导致模型性能下降。这种现象限制了GNN的深度和表达能力。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在量化分析过平滑现象的影响因素，提出减轻过平滑的方法，并探索深度GNN的有效训练策略，以提高节点分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了新的MASED指标来量化过平滑程度；2. 推导了MASED的逐层界限，聚合成全局上下界；3. 分析了节点嵌入范数和权重矩阵奇异值对过平滑的影响；4. 基于理论分析提出了G-Reg正则化方案；5. 通过实验验证了方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 过平滑程度随可训练权重矩阵数量和邻接矩阵数量的增加而增加；2. 通过G-Reg正则化方案增加MASED界限可以提高节点分类准确率；3. 减少深度网络中的过平滑可以使模型在某些任务上获得比浅层网络更好的结果；4. 存在感受野大小与性能之间的权衡关系，通过合理分配邻接跳跃可以避免欠参数化或过参数化。&lt;h4&gt;结论&lt;/h4&gt;通过理论分析和实验验证，本研究有效量化了图神经网络中的过平滑现象，并提出了减轻过平滑的正则化方案，使深度GNN能够在保持高性能的同时增加网络深度，为图神经网络的设计和训练提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们研究了导致深度图神经网络过平滑效应的因素。具体而言，我们的分析基于一个新的指标（平均平方距离-MASED）来量化过平滑的程度。我们推导了MASED的逐层界限，这些界限聚合成全局上下距离界限。基于对过平滑的量化，我们进一步分析了模型两个不同属性的重要性；即生成节点嵌入的范数，以及权重矩阵的最大和最小奇异值。基于从理论分析中获得的见解，我们展示了过平滑随着可训练权重矩阵数量和邻接矩阵数量的增加而增加。我们还使用推导出的MASED逐层界限提出了将跳跃次数（即邻接深度）与权重矩阵数量解耦的方案。特别是，我们引入了G-Reg，一种增加界限的正则化方案，并通过大量实验证明，这样做可以提高节点分类准确率，在大深度下实现鲁棒性。我们进一步表明，通过减少深度网络中的过平滑，我们可以在某些任务上获得比使用浅层网络更好的结果。具体来说，我们在'冷启动'场景下进行了实验，即当未标记节点没有特征信息时。最后，我们使用MASED界限 empirically 证明了感受野大小（即权重矩阵数量）与性能之间的权衡。这是通过将邻接跳跃分布在少量可训练层中实现的，避免了GNN的欠参数化或过参数化的极端情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we study the factors that contribute to the effect ofoversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysisis based on a new metric (Mean Average Squared Distance - $MASED$) to quantifythe extent of oversmoothing. We derive layer-wise bounds on $MASED$, whichaggregate to yield global upper and lower distance bounds. Based on thisquantification of oversmoothing, we further analyze the importance of twodifferent properties of the model; namely the norms of the generated nodeembeddings, along with the largest and smallest singular values of the weightmatrices. Building on the insights drawn from the theoretical analysis, we showthat oversmoothing increases as the number of trainable weight matrices and thenumber of adjacency matrices increases. We also use the derived layer-wisebounds on $MASED$ to form a proposal for decoupling the number of hops (i.e.,adjacency depth) from the number of weight matrices. In particular, weintroduce G-Reg, a regularization scheme that increases the bounds, anddemonstrate through extensive experiments that by doing so node classificationaccuracy increases, achieving robustness at large depths. We further show thatby reducing oversmoothing in deep networks, we can achieve better results insome tasks than using shallow ones. Specifically, we experiment with a ``coldstart" scenario, i.e., when there is no feature information for the unlabelednodes. Finally, we show empirically the trade-off between receptive field size(i.e., number of weight matrices) and performance, using the $MASED$ bounds.This is achieved by distributing adjacency hops across a small number oftrainable layers, avoiding the extremes of under- or over-parameterization ofthe GNN.</description>
      <author>example@mail.com (Dimitrios Kelesis, Dimitris Fotakis, Georgios Paliouras)</author>
      <guid isPermaLink="false">2510.06066v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A comprehensive comparison of neural operators for 3D industry-scale engineering designs</title>
      <link>http://arxiv.org/abs/2510.05995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了六个代表性的3D工业规模工程设计数据集，并对四种类型的神经算子变体进行了系统比较，为神经算子在工程设计中的应用提供了基准和指导。&lt;h4&gt;背景&lt;/h4&gt;神经算子已成为学习函数空间之间非线性映射的强大工具，能够实时预测科学和工程应用中的复杂动态。随着在工程设计评估中的广泛应用，已提出多种神经算子架构，但由于缺乏公平和全面的比较，模型选择仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出并标准化六个代表性的3D工业规模工程设计数据集，涵盖热分析、线性弹性、弹塑性、时变塑性问题和计算流体动力学，使数据集可直接用于各种神经算子架构的训练。&lt;h4&gt;方法&lt;/h4&gt;使用这些数据集，对四种类型的神经算子变体进行系统比较，包括受DeepONet启发的基于分支-主干神经算子、受图神经网络启发的基于图神经算子、受傅里叶神经算子启发的基于网格神经算子、以及受PointNet启发的基于点神经算子。研究引入实际增强以使这些模型适应不同的工程设置，并评估每个模型在预测性能、计算效率、内存使用和部署复杂性方面的优势和局限性。&lt;h4&gt;主要发现&lt;/h4&gt;研究提供了神经算子在多种工程问题上的性能比较，分析了不同神经算子架构的优缺点，为模型选择提供了依据。&lt;h4&gt;结论&lt;/h4&gt;研究结果为未来神经算子开发提供了可操作的见解，有助于推动神经算子在工程设计领域的应用和发展。&lt;h4&gt;翻译&lt;/h4&gt;神经算子已成为学习函数空间之间非线性映射的强大工具，能够实时预测科学和工程应用中的复杂动态。随着在工程设计评估中的广泛采用，已为各种问题环境提出了多种神经算子架构。然而，由于缺乏公平和全面的比较，模型选择仍然具有挑战性。为解决这一问题，我们提出并标准化了六个代表性的3D工业规模工程设计数据集，涵盖热分析、线性弹性、弹塑性、时变塑性问题和计算流体动力学。所有数据集包含完全预处理的输入和输出，用于模型训练，使其可直接用于各种神经算子架构。使用这些数据集，我们对四种类型的神经算子变体进行了系统比较，包括受DeepONet启发的基于分支-主干神经算子、受图神经网络启发的基于图神经算子、受傅里叶神经算子启发的基于网格神经算子、以及受PointNet启发的基于点神经算子。我们进一步引入了实际增强以使这些模型适应不同的工程设置，提高了比较的公平性。我们的基准研究评估了每个模型在预测性能、计算效率、内存使用和部署复杂性方面的优势和局限性。研究结果为未来神经算子开发提供了可操作的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators have emerged as powerful tools for learning nonlinearmappings between function spaces, enabling real-time prediction of complexdynamics in diverse scientific and engineering applications. With their growingadoption in engineering design evaluation, a wide range of neural operatorarchitectures have been proposed for various problem settings. However, modelselection remains challenging due to the absence of fair and comprehensivecomparisons. To address this, we propose and standardize six representative 3Dindustry-scale engineering design datasets spanning thermal analysis, linearelasticity, elasto-plasticity, time-dependent plastic problems, andcomputational fluid dynamics. All datasets include fully preprocessed inputsand outputs for model training, making them directly usable across diverseneural operator architectures. Using these datasets, we conduct a systematiccomparison of four types of neural operator variants, includingBranch-Trunk-based Neural Operators inspired by DeepONet, Graph-based NeuralOperators inspired by Graph Neural Networks, Grid-based Neural Operatorsinspired by Fourier Neural Operators, and Point-based Neural Operators inspiredby PointNet. We further introduce practical enhancements to adapt these modelsto different engineering settings, improving the fairness of the comparison.Our benchmarking study evaluates each model strengths and limitations in termsof predictive performance, computational efficiency, memory usage, anddeployment complexity. The findings provide actionable insights to guide futureneural operator development.</description>
      <author>example@mail.com (Weiheng Zhong, Qibang Liu, Diab Abueidda, Seid Koric, Hadi Meidani)</author>
      <guid isPermaLink="false">2510.05995v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases</title>
      <link>http://arxiv.org/abs/2510.05764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RareAgent的自进化多智能体系统，用于罕见疾病的药物重定位研究。该系统通过主动证据寻求推理而非被动模式识别，显著提高了药物与疾病关联的预测性能，并提供了透明的推理过程。&lt;h4&gt;背景&lt;/h4&gt;计算药物重定位对于罕见疾病特别具有挑战性，尤其是当药物和目标疾病之间没有先前的关联存在时。在这种情况下，知识图谱补全和消息传递图神经网络难以学习和传播可靠信号，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将罕见疾病药物重定位任务从被动模式识别转变为主动证据寻求推理的系统，以提高预测性能并提供可解释的推理过程。&lt;h4&gt;方法&lt;/h4&gt;RareAgent组织特定任务的对立辩论，其中智能体从不同角度动态构建证据图，以支持、反驳或蕴含假设。系统通过自进化循环分析推理策略，产生文本反馈以完善智能体策略，并将成功的推理路径提炼为可转移的启发式方法，以加速未来研究。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估显示，RareAgent比推理基线提高指示AUPRC 18.1%，且提供的推理链与临床证据一致，具有良好的可解释性。&lt;h4&gt;结论&lt;/h4&gt;RareAgent通过主动证据寻求推理有效改善了罕见疾病的药物重定位性能，不仅提高了预测准确率，还提供了透明的推理过程，有助于加速罕见疾病的治疗发现。&lt;h4&gt;翻译&lt;/h4&gt;对于罕见疾病，当药物与目标疾病之间没有先前的关联时，计算药物重定位尤其具有挑战性。因此，知识图谱补全和消息传递图神经网络几乎没有可靠的信号可以学习和传播，导致性能不佳。我们提出了RareAgent，这是一个自进化的多智能体系统，它将这一任务从被动的模式识别重新定义为主动的证据寻求推理。RareAgent组织特定任务的对立辩论，其中智能体从不同角度动态构建证据图，以支持、反驳或蕴含假设。推理策略在自进化循环中被事后分析，产生文本反馈以完善智能体策略，同时成功的推理路径被提炼为可转移的启发式方法，以加速未来的研究。全面的评估显示，RareAgent比推理基线提高了18.1%的指示AUPRC，并提供与临床证据一致的透明推理链。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational drug repurposing for rare diseases is especially challengingwhen no prior associations exist between drugs and target diseases. Therefore,knowledge graph completion and message-passing GNNs have little reliable signalto learn and propagate, resulting in poor performance. We present RareAgent, aself-evolving multi-agent system that reframes this task from passive patternrecognition to active evidence-seeking reasoning. RareAgent organizestask-specific adversarial debates in which agents dynamically constructevidence graphs from diverse perspectives to support, refute, or entailhypotheses. The reasoning strategies are analyzed post hoc in aself-evolutionary loop, producing textual feedback that refines agent policies,while successful reasoning paths are distilled into transferable heuristics toaccelerate future investigations. Comprehensive evaluations reveal thatRareAgent improves the indication AUPRC by 18.1% over reasoning baselines andprovides a transparent reasoning chain consistent with clinical evidence.</description>
      <author>example@mail.com (Lang Qin, Zijian Gan, Xu Cao, Pengcheng Jiang, Yankai Jiang, Jiawei Han, Kaishun Wu, Jintai Chen)</author>
      <guid isPermaLink="false">2510.05764v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective</title>
      <link>http://arxiv.org/abs/2510.05750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了异构图神经网络(HGNNs)的有效性，从模型架构和异构信息两个角度进行了系统分析。通过在21个数据集和20个基线上的系统复现，并结合全面的超参数调优，作者开发了一个因果效应估计框架来评估性能提升的来源。研究得出两个主要结论：模型架构和复杂性对性能没有因果影响，而异构信息通过增加同质性和局部-全局分布差异对性能产生正面因果效应，使节点类别更易区分。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在节点分类方面取得了显著成功。在此基础上，异构图神经网络(HGNNs)整合了关系类型和节点与边的语义，以利用异构信息。HGNNs的因果分析正在快速发展，旨在将真实的因果效应与虚假相关性分开。然而，HGNNs是否本质上有效尚未得到充分研究，大多数研究只是隐含地假设而非建立这种有效性。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在从两个角度(模型架构和异构信息)检查HGNNs的有效性，并确定性能提升的真正来源。&lt;h4&gt;方法&lt;/h4&gt;作者在21个数据集和20个基线上进行了系统复现，并结合了全面的超参数调优。为了进一步分离性能提升的来源，他们开发了一个因果效应估计框架，通过事实分析和反事实分析在标准假设下构建和评估候选因素，并通过最小充分调整集、跨方法一致性检查和敏感性分析验证了结果的稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现模型架构和复杂性对性能没有因果影响；异构信息通过增加同质性和局部-全局分布差异对性能产生正面因果效应，使节点类别更易区分。&lt;h4&gt;结论&lt;/h4&gt;HGNNs的性能提升主要来自于异构信息，而非模型架构或复杂性。异构信息通过增加同质性和局部-全局分布差异，使节点类别更易区分，从而提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在节点分类方面取得了显著成功。在此基础上，异构图神经网络(HGNNs)整合了关系类型和节点与边的语义，以利用异构信息。HGNNs的因果分析正在快速发展，旨在将真实的因果效应与虚假相关性分开。然而，HGNNs是否本质上有效尚未得到充分研究，大多数研究只是隐含地假设而非建立这种有效性。在本工作中，我们从两个角度检查HGNNs：模型架构和异构信息。我们在21个数据集和20个基线上进行了系统复现，并结合了全面的超参数调优。为了进一步分离性能提升的来源，我们开发了一个因果效应估计框架，通过事实分析和反事实分析在标准假设下构建和评估候选因素，并通过最小充分调整集、跨方法一致性检查和敏感性分析验证了结果的稳健性。我们的研究得出了两个结论。首先，模型架构和复杂性对性能没有因果影响。其次，异构信息通过增加同质性和局部-全局分布差异对性能产生正面因果效应，使节点类别更易区分。实现可在https://github.com/YXNTU/CausalHGNN公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in nodeclassification. Building on this progress, heterogeneous graph neural networks(HGNNs) integrate relation types and node and edge semantics to leverageheterogeneous information. Causal analysis for HGNNs is advancing rapidly,aiming to separate genuine causal effects from spurious correlations. However,whether HGNNs are intrinsically effective remains underexamined, and moststudies implicitly assume rather than establish this effectiveness. In thiswork, we examine HGNNs from two perspectives: model architecture andheterogeneous information. We conduct a systematic reproduction across 21datasets and 20 baselines, complemented by comprehensive hyperparameterretuning. To further disentangle the source of performance gains, we develop acausal effect estimation framework that constructs and evaluates candidatefactors under standard assumptions through factual and counterfactual analyses,with robustness validated via minimal sufficient adjustment sets, cross-methodconsistency checks, and sensitivity analyses. Our results lead to twoconclusions. First, model architecture and complexity have no causal effect onperformance. Second, heterogeneous information exerts a positive causal effectby increasing homophily and local-global distribution discrepancy, which makesnode classes more distinguishable. The implementation is publicly available athttps://github.com/YXNTU/CausalHGNN.</description>
      <author>example@mail.com (Xiao Yang, Xuejiao Zhao, Zhiqi Shen)</author>
      <guid isPermaLink="false">2510.05750v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes</title>
      <link>http://arxiv.org/abs/2510.05736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PoS(ICRC2025)752&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出并评估了三种基于深度学习的模型，用于在成像大气切伦科夫望远镜(IACTs)数据中从强子背景中识别γ射线，展示了结合卷积神经网络和图神经网络的方法在提高识别性能方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;在成像大气切伦科夫望远镜(IACTs)的地面探测中，从强子背景中识别γ射线是一个关键方面。当前方法在利用复杂数据相关性方面能力有限，难以有效处理复杂的观测数据。&lt;h4&gt;目的&lt;/h4&gt;设计具有与任务相关归纳偏见的模型架构，以解决基于深度学习的模型在稳健性和适用性方面面临的挑战，提高从强子背景中识别γ射线的性能。&lt;h4&gt;方法&lt;/h4&gt;提出、训练并评估了三种基于深度学习的模型：(1)结合图像和图数据的混合卷积和图神经网络模型(CNN-GNN)；(2)在图构建中纳入额外重建信息的增强版CNN-GNN变体；(3)使用图像矩作为基线的图神经网络(GNN)模型。所有模型均在模拟数据上进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;新的结合卷积和基于图的方法显示出比传统方法更好的性能。在图构建中纳入重建信息进一步提高了模型在真实观测数据上的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;结合卷积神经网络和图神经网络的方法为γ射线识别提供了有效途径，而包含重建信息则有助于提高模型在实际应用中的泛化性能，为未来研究提供了有价值的方向。&lt;h4&gt;翻译&lt;/h4&gt;使用成像大气切伦科夫望远镜(IACTs)进行地面探测时，从主要强子背景中识别γ射线是一个关键方面。虽然当前方法在利用复杂数据相关性方面能力有限，但基于深度学习的模型通过直接利用图像级信息提供了一种有前景的替代方案。然而，这类模型在稳健性和适用性方面仍面临几个挑战。设计具有与任务相关归纳偏见的模型架构可以帮助缓解这一问题。论文提出了三种基于深度学习的模型，在模拟数据上进行了训练和评估：(1)使用图像和图数据的混合卷积和图神经网络模型(CNN-GNN)；(2)在图构建中纳入额外重建信息的增强版CNN-GNN变体；(3)使用图像矩作为基线的图神经网络(GNN)模型。新的结合卷积和基于图的方法显示出比传统方法更好的性能，而包含重建信息则在真实观测数据的泛化能力方面提供了进一步的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.22323/1.501.0752&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The identification of $\gamma$-rays from the predominant hadronic-backgroundis a key aspect in their ground-based detection using Imaging AtmosphericCherenkov Telescopes (IACTs). While current methods are limited in theirability to exploit correlations in complex data, deep learning-based modelsoffer a promising alternative by directly leveraging image-level information.However, several challenges involving the robustness and applicability of suchmodels remain. Designing model architectures with inductive biases relevant forthe task can help mitigate the problem. Three such deep learning-based modelsare proposed, trained, and evaluated on simulated data: (1) a hybridconvolutional and graph neural network model (CNN-GNN) using both image andgraph data; (2) an enhanced CNN-GNN variant that incorporates additionalreconstructed information within the graph construction; and (3) a graph neuralnetwork (GNN) model using image moments serving as a baseline. The new combinedconvolution and graph-based approach demonstrates improved performance overtraditional methods, and the inclusion of reconstructed information offersfurther potential in generalization capabilities on real observational data.</description>
      <author>example@mail.com (Abhay Mehta, Dan Parsons, Tim Lukas Holch, David Berge, Matthias Weidlich)</author>
      <guid isPermaLink="false">2510.05736v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>QGraphLIME - Explaining Quantum Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.05683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QGraphLIME的模型无关、事后解释框架，用于解释量子图神经网络。该框架通过在保持图结构的扰动上拟合局部代理，并聚合代理属性及其离散度，提供不确定性感知的节点和边重要性排名。实证研究表明该方法能提供准确稳定的解释，且非线性代理建模具有明显优势。&lt;h4&gt;背景&lt;/h4&gt;量子图神经网络在图结构数据学习中提供了强大范式，但由于测量诱导的随机性和图结构的组合性质，它们的可解释性变得复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种模型无关的、事后的解释框架，为量子图模型提供不确定性感知的节点和边重要性排名。&lt;h4&gt;方法&lt;/h4&gt;QGraphLIME将模型解释视为在保持图结构的扰动上拟合的局部代理的分布。通过聚合代理属性及其离散度，该框架为量子图模型提供不确定性感知的节点和边重要性排名。该框架还提供了关于代理集大小的无分布、有限样本保证：Dvoretzky-Kiefer-Wolfowitz边界确保在标准独立假设下，以目标精度和置信度均匀逼近二元类别概率的诱导分布。&lt;h4&gt;主要发现&lt;/h4&gt;在对具有已知真实情况的受控合成图进行的实证研究中，展示了准确和稳定的解释，消融研究表明非线性代理建模的明显优势，并突出了对扰动设计的敏感性。&lt;h4&gt;结论&lt;/h4&gt;这些结果共同建立了一种原则性的、不确定性感知的、对结构敏感的量子图神经网络解释方法，并为扩展到更广泛的架构和真实世界数据奠定了基础，随着量子资源的成熟。&lt;h4&gt;翻译&lt;/h4&gt;量子图神经网络为图结构数据学习提供了强大的范式，然而它们的可解释性因测量诱导的随机性和图结构的组合性质而变得复杂。在本文中，我们引入了QuantumGraphLIME（QGraphLIME），一种模型无关的、事后的框架，该框架将模型解释视为在保持图结构的扰动上拟合的局部代理的分布。通过聚合代理属性及其离散度，QGraphLIME为量子图模型产生不确定性感知的节点和边重要性排名。该框架进一步提供了代理集大小的无分布、有限样本保证：Dvoretzky-Kiefer-Wolfowitz边界确保在标准独立假设下，以目标精度和置信度均匀逼近二元类别概率的诱导分布。在具有已知真实情况的受控合成图上的实证研究展示了准确和稳定的解释，消融研究表明非线性代理建模的明显优势，并突出了对扰动设计的敏感性。总的来说，这些结果建立了一种原则性的、不确定性感知的、对结构敏感的量子图神经网络解释方法，并为随着量子资源的成熟扩展到更广泛的架构和真实世界数据奠定了基础。代码可在https://github.com/smlab-niser/qglime获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum graph neural networks offer a powerful paradigm for learning ongraph-structured data, yet their explainability is complicated bymeasurement-induced stochasticity and the combinatorial nature of graphstructure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), amodel-agnostic, post-hoc framework that treats model explanations asdistributions over local surrogates fit on structure-preserving perturbationsof a graph. By aggregating surrogate attributions together with theirdispersion, QGraphLIME yields uncertainty-aware node and edge importancerankings for quantum graph models. The framework further provides adistribution-free, finite-sample guarantee on the size of the surrogateensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation ofthe induced distribution of a binary class probability at target accuracy andconfidence under standard independence assumptions. Empirical studies oncontrolled synthetic graphs with known ground truth demonstrate accurate andstable explanations, with ablations showing clear benefits of nonlinearsurrogate modeling and highlighting sensitivity to perturbation design.Collectively, these results establish a principled, uncertainty-aware, andstructure-sensitive approach to explaining quantum graph neural networks, andlay the groundwork for scaling to broader architectures and real-worlddatasets, as quantum resources mature. Code is available athttps://github.com/smlab-niser/qglime.</description>
      <author>example@mail.com (Haribandhu Jena, Jyotirmaya Shivottam, Subhankar Mishra)</author>
      <guid isPermaLink="false">2510.05683v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection</title>
      <link>http://arxiv.org/abs/2510.05676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新颖的归纳图梯度提升机器（G-GBM），用于解决异质性和动态图上的监督学习问题，特别是在保险欺诈检测领域。&lt;h4&gt;背景&lt;/h4&gt;基于图的方法在机器学习中越来越受欢迎，能够建模复杂的数据和关系。保险欺诈是一个典型用例，因为虚假索赔通常来自犯罪团伙策划事故或同一人在多个保单上提交错误索赔。然而，基于图的方法面临高类别不平衡和异质动态网络的挑战，导致表格数据上的梯度提升树方法仍占主导地位。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的归纳图梯度提升机器（G-GBM），用于异质性和动态图上的监督学习，特别是在保险欺诈检测场景中。&lt;h4&gt;方法&lt;/h4&gt;开发了一种图梯度提升机器（G-GBM），在模拟随机图实验中与流行图神经网络方法竞争，并在开源和专有真实世界数据集上展示其在保险欺诈检测中的能力。应用成熟可解释性方法来理解模型预测。&lt;h4&gt;主要发现&lt;/h4&gt;G-GBM能够与流行的图神经网络方法相竞争，在保险欺诈检测任务中显示出强大能力，同时通过可解释性方法提供了对模型决策的更好理解。&lt;h4&gt;结论&lt;/h4&gt;G-GBM为处理异质性和动态图上的监督学习提供了一种有效方法，特别是在保险欺诈检测领域，结合了图表示学习与梯度提升的优势。&lt;h4&gt;翻译&lt;/h4&gt;基于图的方法在机器学习中越来越受欢迎，因为它们能够建模复杂的数据和关系。保险欺诈是一个典型的用例，因为虚假索赔通常是犯罪团伙策划事故或同一人在多个保单上提交错误索赔的结果。一个挑战是，由于欺诈数据中存在高度类别不平衡，基于图的方法难以找到数据的有意义表示。另一个是，考虑到人员、公司和保单之间关系的变化，保险网络是异质和动态的。这就是为什么表格数据上的梯度提升树方法在该领域仍然占主导地位。因此，我们提出了一种新颖的归纳图梯度提升机器（G-GBM），用于异质和动态图上的监督学习。我们在使用各种模拟随机图的实验中表明，我们的估计器与流行的图神经网络方法相竞争。我们使用开源和真实世界的专有数据集展示了G-GBM在保险欺诈检测方面的能力。鉴于骨干模型是梯度提升森林，我们应用了成熟的可解释性方法，以更好地理解G-GBM所做的预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based methods are becoming increasingly popular in machine learning dueto their ability to model complex data and relations. Insurance fraud is aprime use case, since false claims are often the result of organised criminalsthat stage accidents or the same persons filing erroneous claims on multiplepolicies. One challenge is that graph-based approaches struggle to findmeaningful representations of the data because of the high class imbalancepresent in fraud data. Another is that insurance networks are heterogeneous anddynamic, given the changing relations among people, companies and policies.That is why gradient boosted tree approaches on tabular data still dominate thefield. Therefore, we present a novel inductive graph gradient boosting machine(G-GBM) for supervised learning on heterogeneous and dynamic graphs. We showthat our estimator competes with popular graph neural network approaches in anexperiment using a variety of simulated random graphs. We demonstrate the powerof G-GBM for insurance fraud detection using an open-source and a real-world,proprietary dataset. Given that the backbone model is a gradient boostingforest, we apply established explainability methods to gain better insightsinto the predictions made by G-GBM.</description>
      <author>example@mail.com (Félix Vandervorst, Bruno Deprez, Wouter Verbeke, Tim Verdonck)</author>
      <guid isPermaLink="false">2510.05676v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning</title>
      <link>http://arxiv.org/abs/2510.05583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 8 figures, 18 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个统一的、可重现的基准测试框架HydraGNN，用于系统性地评估图神经网络中全局注意力机制的实际效益，并比较了不同架构在原子尺度化合物建模中的表现。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）被广泛用于替代昂贵实验和第一性原理模拟，研究原子尺度化合物的行为。随着GNN架构复杂度的增加，大多数最新GNN结合了传统的消息传递神经网络（MPNNs）层和具有全局注意力机制的图变换器（GTs），但尚不清楚全局注意力机制相对于精心调优的MPNN层何时真正有益。&lt;h4&gt;目的&lt;/h4&gt;引入第一个统一的、可重现的基准测试框架，用于系统性地隔离和评估消息传递、全局注意力和基于编码器的特征增强在原子图学习中的贡献，并量化全局注意力机制的准确性-计算权衡。&lt;h4&gt;方法&lt;/h4&gt;构建了基于HydraGNN的统一基准测试框架，支持在四种受控模型类别之间无缝切换：MPNN、带有化学/拓扑编码器的MPNN、MPNN与全局注意力的GPS风格混合模型，以及带有编码器的完全融合的局部-全局模型。使用七个开源数据集进行回归和分类任务的基准测试，系统性地隔离不同组件的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;基于编码器增强的MPNNs形成了一个强大的基线，而融合的局部-全局模型在受长程相互作用效应影响的属性上表现出最明显的好处。研究还量化了注意力的准确性-计算权衡，报告了其在内存中的开销。&lt;h4&gt;结论&lt;/h4&gt;这些结果建立了原子图学习中全局注意力的首次受控评估，并为未来模型开发提供了可重现的测试平台，有助于理解不同架构在原子尺度化合物建模中的优势和适用场景。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）被广泛用作昂贵实验和第一性原理模拟的替代品，以研究原子尺度化合物的行为，其架构复杂度不断增加，以实现对复杂物理的建模。虽然最近的GNN大多将传统的消息传递神经网络（MPNNs）层与具有全局注意力机制的更先进的图变换器（GTs）结合，以分别建模短程和长程相互作用，但由于实现、特征或超参数调优的不一致，全局注意力机制相对于精心调优的MPNN层何时真正提供益处仍不清楚。我们引入了第一个统一的、可重现的基准测试框架 - 基于HydraGNN构建 - 它能够在四种受控模型类别之间无缝切换：MPNN、带有化学/拓扑编码器的MPNN、MPNN与全局注意力的GPS风格混合模型，以及带有编码器的完全融合的局部-全局模型。使用七个开源数据集进行回归和分类任务的基准测试，我们系统性地隔离了消息传递、全局注意力和基于编码器的特征增强的贡献。我们的研究表明，基于编码器增强的MPNNs形成了一个强大的基线，而融合的局部-全局模型在受长程相互作用效应影响的属性上产生最明显的好处。我们进一步量化了注意力的准确性-计算权衡，报告了其在内存中的开销。总之，这些结果建立了原子图学习中全局注意力的首次受控评估，并为未来模型开发提供了可重现的测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are widely used as surrogates for costlyexperiments and first-principles simulations to study the behavior of compoundsat atomistic scale, and their architectural complexity is constantly increasingto enable the modeling of complex physics. While most recent GNNs combine moretraditional message passing neural networks (MPNNs) layers to model short-rangeinteractions with more advanced graph transformers (GTs) with global attentionmechanisms to model long-range interactions, it is still unclear when globalattention mechanisms provide real benefits over well-tuned MPNN layers due toinconsistent implementations, features, or hyperparameter tuning. We introducethe first unified, reproducible benchmarking framework - built on HydraGNN -that enables seamless switching among four controlled model classes: MPNN, MPNNwith chemistry/topology encoders, GPS-style hybrids of MPNN with globalattention, and fully fused local - global models with encoders. Using sevendiverse open-source datasets for benchmarking across regression andclassification tasks, we systematically isolate the contributions of messagepassing, global attention, and encoder-based feature augmentation. Our studyshows that encoder-augmented MPNNs form a robust baseline, while fusedlocal-global models yield the clearest benefits for properties governed bylong-range interaction effects. We further quantify the accuracy - computetrade-offs of attention, reporting its overhead in memory. Together, theseresults establish the first controlled evaluation of global attention inatomistic graph learning and provide a reproducible testbed for future modeldevelopment.</description>
      <author>example@mail.com (Arindam Chowdhury, Massimiliano Lupo Pasini)</author>
      <guid isPermaLink="false">2510.05583v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection</title>
      <link>http://arxiv.org/abs/2510.05562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, ACM the web conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为生成动态图模型(GDGM)的新框架，用于金融交易中的欺骗检测，特别是针对复杂的共谋欺骗行为。该方法通过动态建模交易行为和节点间关系，有效捕捉了时间模式和不断变化的市场条件，并在实验和实际应用中表现出色。&lt;h4&gt;背景&lt;/h4&gt;金融交易中的欺骗检测至关重要，特别是识别复杂的共谋欺骗行为。传统机器学习方法主要关注孤立节点特征，忽略了互联节点的更广泛背景。基于图的技术如GNNs虽有进展，但在面对现实世界中动态、不规则的交易行为模式时仍面临挑战，难以捕捉动态和多样化的节点间关系复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为生成动态图模型(GDGM)的新框架，用于建模动态交易行为和节点间关系，学习用于共谋欺骗检测的表示，以解决现有方法难以捕捉动态关系的问题。&lt;h4&gt;方法&lt;/h4&gt;将原始交易数据转换为时间戳序列，使用神经常微分方程和门控循环单元建模交易行为，生成包含欺骗模式时间动态的表示，并采用伪标签生成和异构聚合技术收集相关信息以增强检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在欺骗检测数据集上的实验表明，该方法在检测准确性上优于最先进的模型。该欺骗检测系统已成功部署在世界上最大的交易市场之一，验证了其实际应用价值。&lt;h4&gt;结论&lt;/h4&gt;生成动态图模型(GDGM)能够有效捕捉动态交易行为和不断变化的节点间关系，在共谋欺骗检测任务中表现出色，具有良好的实际应用前景。&lt;h4&gt;翻译&lt;/h4&gt;金融交易中的欺骗检测至关重要，特别是在识别复杂的共谋欺骗行为方面。传统的机器学习方法主要关注孤立节点的特征，往往忽略了互联节点的更广泛背景。基于图的技术，特别是图神经网络(GNNs)，通过有效利用关系信息推动了该领域的发展。然而，在现实世界的欺骗检测数据集中，交易行为表现出动态、不规则的模式。现有的欺骗检测方法虽然在某些场景中有效，但难以捕捉动态和多样化、不断发展的节点间关系的复杂性。为解决这些挑战，我们提出了一种名为生成动态图模型(GDGM)的新框架，该框架通过建模动态交易行为和节点间关系来学习用于共谋欺骗检测的表示。具体而言，我们的方法集成了生成动态潜在空间来捕捉时间模式和不断变化的市场条件。原始交易数据首先被转换为带时间戳的序列。然后我们使用神经常微分方程和门控循环单元对交易行为进行建模，以生成包含欺骗模式时间动态的表示。此外，还采用了伪标签生成和异构聚合技术来收集相关信息并增强对共谋欺骗行为的检测性能。在欺骗检测数据集上进行的实验表明，我们的方法在检测准确性上优于最先进的模型。此外，我们的欺骗检测系统已成功部署在世界上最大的交易市场之一，进一步验证了所提出方法的实际应用性和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714518&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spoofing detection in financial trading is crucial, especially foridentifying complex behaviors such as conspiracy spoofing. Traditionalmachine-learning approaches primarily focus on isolated node features, oftenoverlooking the broader context of interconnected nodes. Graph-basedtechniques, particularly Graph Neural Networks (GNNs), have advanced the fieldby leveraging relational information effectively. However, in real-worldspoofing detection datasets, trading behaviors exhibit dynamic, irregularpatterns. Existing spoofing detection methods, though effective in somescenarios, struggle to capture the complexity of dynamic and diverse, evolvinginter-node relationships. To address these challenges, we propose a novelframework called the Generative Dynamic Graph Model (GDGM), which modelsdynamic trading behaviors and the relationships among nodes to learnrepresentations for conspiracy spoofing detection. Specifically, our approachincorporates the generative dynamic latent space to capture the temporalpatterns and evolving market conditions. Raw trading data is first convertedinto time-stamped sequences. Then we model trading behaviors using the neuralordinary differential equations and gated recurrent units, to generate therepresentation incorporating temporal dynamics of spoofing patterns.Furthermore, pseudo-label generation and heterogeneous aggregation techniquesare employed to gather relevant information and enhance the detectionperformance for conspiratorial spoofing behaviors. Experiments conducted onspoofing detection datasets demonstrate that our approach outperformsstate-of-the-art models in detection accuracy. Additionally, our spoofingdetection system has been successfully deployed in one of the largest globaltrading markets, further validating the practical applicability and performanceof the proposed method.</description>
      <author>example@mail.com (Sheng Xiang, Yidong Jiang, Yunting Chen, Dawei Cheng, Guoping Zhao, Changjun Jiang)</author>
      <guid isPermaLink="false">2510.05562v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Fundamental Limits of Crystalline Equivariant Graph Neural Networks: A Circuit Complexity Perspective</title>
      <link>http://arxiv.org/abs/2510.05494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过电路复杂度的透镜，表征了等变图神经网络在晶体结构预测中的内在计算和表达极限。研究表明，在特定条件下，这些模型可以被多项式大小的均匀阈值电路族模拟，为这类架构在现实资源约束下可解决的问题提供了具体上限。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为关系数据学习的核心范式。在材料科学中，等变图神经网络因其能够尊重欧几里得对称性和周期性边界条件，已成为晶体结构预测的有力骨干。尽管有强大的实证性能，它们在周期性、对称性约束环境下的表达能力仍然不为人知。&lt;h4&gt;目的&lt;/h4&gt;通过电路复杂度的透镜，表征等变图神经网络在晶体结构预测中的内在计算和表达极限，理解其在周期性、对称性约束环境下的表达能力。&lt;h4&gt;方法&lt;/h4&gt;分析等变图神经网络层在节点特征、原子坐标和晶格矩阵上进行的计算。在多项式精度下，证明对于特定数量的节点、层深度和宽度，这些模型可以被多项式大小的均匀阈值电路族模拟，并提供明确的常数深度界限。&lt;h4&gt;主要发现&lt;/h4&gt;1) 对于特定数量的节点、层深度和宽度，嵌入宽度有特定要求；2) 这些等变图神经网络模型可以被多项式大小的均匀阈值电路族模拟；3) 将这类模型置于特定复杂度类别中为架构在现实资源约束下可解决的问题提供了具体上限；4) 明确了需要增加深度、更丰富的几何基元或更宽的层等架构修改来超越这一范围。&lt;h4&gt;结论&lt;/h4&gt;该分析补充了Weisfeiler-Lehman风格的结果，这些结果不能直接转移到周期性晶体。研究为晶体系统上的对称感知图学习提供了复杂性理论基础，有助于理解这类神经网络的极限和改进方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为关系数据学习的核心范式。在材料科学中，等变图神经网络因其能够尊重欧几里得对称性和周期性边界条件，已成为晶体结构预测的有力骨干。尽管有强大的实证性能，它们在周期性、对称性约束环境下的表达能力仍然不为人知。本研究通过电路复杂度的透镜，表征了等变图神经网络在晶体结构预测中的内在计算和表达极限。我们分析了等变图神经网络层在节点特征、原子坐标和晶格矩阵上进行的计算，并证明在多项式精度下，对于特定数量的节点、层深度和宽度，嵌入宽度有特定要求的模型可以被多项式大小的均匀阈值电路族模拟(具有明确的常数深度界限)。将等变图神经网络置于特定复杂度类别中为这类架构在现实资源约束下可解决的决策和预测问题提供了具体上限，并明确了需要哪些架构修改(例如增加深度、更丰富的几何基元或更宽的层)来超越这一范围。该分析补充了Weisfeiler-Lehman风格的结果，这些结果不能直接转移到周期性晶体，并为晶体系统上的对称感知图学习提供了复杂性理论基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是确定用于晶体结构预测的等变图神经网络（EGNNs）的基本计算和表达能力的限制。这个问题在研究中很重要，因为尽管EGNNs在材料科学领域表现出强大性能，能够尊重欧几里得对称性和周期边界条件，但它们在周期性、对称约束条件下的理论表达能力仍不清楚。了解这些限制有助于确定哪些架构修改是必要的以超越这些限制，并为对称感知的晶体系统图学习提供理论基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从电路复杂性角度分析EGNNs的表达能力，将模型置于具体的电路类别中，以确定它们在现实约束下能解决的问题类型。他们首先形式化了EGNNs的结构，然后分析了EGNN层在节点特征、原子坐标和晶格矩阵上的计算，并量化了使用均匀阈值电路模拟这些计算所需的资源。作者借鉴了电路复杂性理论（特别是TC0电路类别）、现有的浮点数计算和矩阵乘法的电路实现结果，但区别于传统的Weisfeiler-Lehman分析（这些分析专注于离散图同构，抽象掉了连续坐标和对称约束）和其他架构（如Transformer）的电路分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过电路复杂性理论确定EGNNs在晶体结构预测中的基本计算限制，证明在多项式精度、特定架构条件下，EGNN可以被TC0电路模拟，从而为其能解决的问题类型提供具体上限。实现流程包括：1)定义晶体结构的单元胞表示和分数坐标矩阵；2)形式化EGNN架构，包括傅里叶变换、成对消息传递和层操作；3)分析基本EGNN构建块的电路复杂性，证明傅里叶变换、MLP和消息传递等可在TC0电路中计算；4)在给定假设条件下证明整个EGNN模型可被均匀TC0电路族实现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次对晶体EGNNs进行电路复杂性分析；形式化EGNNs结构为理论分析提供基础；确定TC0上限为EGNNs能解决的问题提供具体上限；提供超越限制的架构修改指导。相比之前工作的不同：区别于传统的Weisfeiler-Lehman分析（专注于离散图同构，抽象连续坐标和对称约束）；区别于其他架构（如Transformer）的电路分析；区别于现有几何深度学习研究（较少探索晶体结构中的基本局限性）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过电路复杂性理论确定了晶体结构预测中EGNNs的基本计算限制，证明其在特定条件下可被TC0电路模拟，为这类模型能解决的问题类型提供了理论上限并指出了超越这些限制的架构修改方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have become a core paradigm for learning onrelational data. In materials science, equivariant GNNs (EGNNs) have emerged asa compelling backbone for crystalline-structure prediction, owing to theirability to respect Euclidean symmetries and periodic boundary conditions.Despite strong empirical performance, their expressive power in periodic,symmetry-constrained settings remains poorly understood. This workcharacterizes the intrinsic computational and expressive limits of EGNNs forcrystalline-structure prediction through a circuit-complexity lens. We analyzethe computations carried out by EGNN layers acting on node features, atomiccoordinates, and lattice matrices, and prove that, under polynomial precision,embedding width $d=O(n)$ for $n$ nodes, $O(1)$ layers, and $O(1)$-depth,$O(n)$-width MLP instantiations of the message/update/readout maps, thesemodels admit a simulation by a uniform $\mathsf{TC}^0$ threshold-circuit familyof polynomial size (with an explicit constant-depth bound). Situating EGNNswithin $\mathsf{TC}^0$ provides a concrete ceiling on the decision andprediction problems solvable by such architectures under realistic resourceconstraints and clarifies which architectural modifications (e.g., increaseddepth, richer geometric primitives, or wider layers) are required to transcendthis regime. The analysis complements Weisfeiler-Lehman style results that donot directly transfer to periodic crystals, and offers a complexity-theoreticfoundation for symmetry-aware graph learning on crystalline systems.</description>
      <author>example@mail.com (Yang Cao, Zhao Song, Jiahao Zhang, Jiale Zhao)</author>
      <guid isPermaLink="false">2510.05494v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering</title>
      <link>http://arxiv.org/abs/2510.05445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为tAgentRouter的框架，通过知识图谱引导的多代理路由机制来优化问答任务性能，解决了在多种模型和代理策略中选择最佳配置的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和基于代理的框架发展迅速，但实践者在选择下游任务最佳配置时面临不确定性。研究表明不同代理和骨干模型具有互补优势，且更大模型并不总是更优。现有代理路由方法通常只强调成本效率，忽视了QA任务中固有的细粒度上下文和关系结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应路由机制，能够根据任务需求选择最适合的模型和代理配置，有效利用不同代理的互补优势来提高问答任务性能。&lt;h4&gt;方法&lt;/h4&gt;将多代理QA表述为知识图谱引导的路由问题，由经验性能信号监督。将QA实例转换为联合编码查询、上下文实体和代理的知识图谱，训练异构图神经网络在节点类型间传播信息并生成任务感知的路由分布。利用软监督和代理输出的加权聚合学习协作方案。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，该框架一致地优于单代理和集成基线，同时能够跨基准测试和LLM骨干模型进行有效泛化。&lt;h4&gt;结论&lt;/h4&gt;图监督的多代理路由在问答任务中具有显著的有效性和鲁棒性，能够有效捕捉不同代理的互补优势。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型和基于代理的框架发展迅速，支持多种应用。然而，随着模型和代理策略的激增，实践者在选择下游任务的最佳配置时面临重大不确定性。先前研究表明不同代理和骨干模型具有互补优势，且更大的模型并不总是更优，这凸显了对自适应路由机制的需求。然而，现有的代理路由方法通常只强调成本效率，而忽视了问答任务中固有的细粒度上下文和关系结构。在本文中，我们提出了tAgentRouter，一个将多代理QA表述为由经验性能信号监督的知识图谱引导的路由问题的框架。具体来说，我们将QA实例转换为联合编码查询、上下文实体和代理的知识图谱，然后训练异构图神经网络在节点类型间传播信息，并生成对代理的任务感知路由分布。通过利用软监督和代理输出的加权聚合，AgentRouter学习能够捕捉不同代理互补优势的有原则的协作方案。广泛的实验证明，我们的框架一致地优于单代理和集成基线，同时能够跨基准测试和LLM骨干模型进行泛化。这些结果突显了图监督的多代理路由在问答任务中的有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) and agent-based frameworks have advancedrapidly, enabling diverse applications. Yet, with the proliferation of modelsand agentic strategies, practitioners face substantial uncertainty in selectingthe best configuration for a downstream task. Prior studies show that differentagents and backbones exhibit complementary strengths, and that larger modelsare not always superior, underscoring the need for adaptive routing mechanisms.Existing approaches to agent routing, however, often emphasize cost efficiencywhile overlooking the fine-grained contextual and relational structure inherentin QA tasks. In this paper, we propose tAgentRouter, a framework thatformulates multi-agent QA as a knowledge-graph-guided routing problemsupervised by empirical performance signals. Specifically, we convert QAinstance into a knowledge graph that jointly encodes queries, contextualentities, and agents, and then train a heterogeneous graph neural network (GNN)to propagate information across node types and produce task-aware routingdistributions over agents. By leveraging soft supervision and weightedaggregation of agent outputs, AgentRouter learns principled collaborationschemes that capture the complementary strengths of diverse agents. Extensiveexperiments demonstrate that our framework consistently outperformssingle-agent and ensemble baselines, while generalizing across benchmarks andLLM backbones. These results highlight the effectiveness and robustness ofgraph-supervised multi-agent routing for question answering.</description>
      <author>example@mail.com (Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Zehong Wang, Tianyi Ma, Keerthiram Murugesan, Vincent Galassi, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2510.05445v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data</title>
      <link>http://arxiv.org/abs/2510.05888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了BioAutoML-NAS模型，一种基于多模态数据和神经架构搜索的昆虫分类方法，解决了昆虫特征复杂、类别不平衡和大数据集等挑战，在多个数据集上实现了高精度分类。&lt;h4&gt;背景&lt;/h4&gt;昆虫分类对农业管理和生态研究至关重要，直接影响作物健康和生产。然而，该任务面临昆虫特征复杂、类别不平衡及大规模数据集等挑战，难以有效进行。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理昆虫分类挑战的方法，通过结合多模态数据和自动架构搜索提高分类准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出BioAutoML-NAS模型，使用多模态数据（图像和元数据），应用神经架构搜索自动学习最佳网络结构，采用多模态融合模块结合视觉和生物信息，使用交替双层优化策略更新网络参数，并通过零操作移除不重要连接以产生高效架构。&lt;h4&gt;主要发现&lt;/h4&gt;在BIOSCAN-5M数据集上达到96.81%准确率、97.46%精确率、96.81%召回率和97.05% F1分数，优于现有方法约8-16%；在Insects-1M数据集上获得93.25%准确率、93.71%精确率、92.74%召回率和93.22% F1分数。&lt;h4&gt;结论&lt;/h4&gt;BioAutoML-NAS提供了准确可靠的昆虫分类方法，支持现代可持续农业实践。&lt;h4&gt;翻译&lt;/h4&gt;昆虫分类对农业管理和生态研究很重要，因为它直接影响作物健康和生产。然而，由于昆虫的复杂特征、类别不平衡和大规模数据集，这项任务仍然具有挑战性。为了解决这些问题，我们提出了BioAutoML-NAS，这是第一个使用多模态数据（包括图像和元数据）的BioAutoML模型，它应用神经架构搜索（NAS）来为图像自动学习每个单元内每个连接的最佳操作。多个单元堆叠形成完整网络，每个单元提取详细的图像特征表示。多模态融合模块将图像嵌入与元数据结合，使模型能够利用视觉和分类生物信息来分类昆虫。交替双层优化训练策略联合更新网络权重和架构参数，同时零操作移除不太重要的连接，产生稀疏、高效且高性能的架构。在BIOSCAN-5M数据集上的广泛评估表明，BioAutoML-NAS实现了96.81%的准确率、97.46%的精确率、96.81%的召回率和97.05%的F1分数，比最先进的迁移学习、transformer、AutoML和NAS方法分别高出约16%、10%和8%。在Insects-1M数据集上的进一步验证获得了93.25%的准确率、93.71%的精确率、92.74%的召回率和93.22%的F1分数。这些结果表明，BioAutoML-NAS提供了准确、可靠的昆虫分类，支持现代可持续农业。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Insect classification is important for agricultural management and ecologicalresearch, as it directly affects crop health and production. However, this taskremains challenging due to the complex characteristics of insects, classimbalance, and large-scale datasets. To address these issues, we proposeBioAutoML-NAS, the first BioAutoML model using multimodal data, includingimages, and metadata, which applies neural architecture search (NAS) for imagesto automatically learn the best operations for each connection within eachcell. Multiple cells are stacked to form the full network, each extractingdetailed image feature representations. A multimodal fusion module combinesimage embeddings with metadata, allowing the model to use both visual andcategorical biological information to classify insects. An alternating bi-leveloptimization training strategy jointly updates network weights and architectureparameters, while zero operations remove less important connections, producingsparse, efficient, and high-performing architectures. Extensive evaluation onthe BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperformingstate-of-the-art transfer learning, transformer, AutoML, and NAS methods byapproximately 16%, 10%, and 8% respectively. Further validation on theInsects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS providesaccurate, confident insect classification that supports modern sustainablefarming.</description>
      <author>example@mail.com (Arefin Ittesafun Abian, Debopom Sutradhar, Md Rafi Ur Rashid, Reem E. Mohamed, Md Rafiqul Islam, Asif Karim, Kheng Cher Yeo, Sami Azam)</author>
      <guid isPermaLink="false">2510.05888v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.05753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 13 figures, published in TMLR  https://openreview.net/forum?id=UligTUCgdt&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明，在迁移学习环境下评估隐私风险时，没有一种通用的成员推理攻击方法，实践者应根据具体应用场景和数据特点选择合适的攻击方法&lt;h4&gt;背景&lt;/h4&gt;强大的大规模基础模型的出现正在改变训练范式，训练方式正从从头开始训练转向迁移学习，这种转变使得在敏感应用中可以使用小型、领域特定的数据集进行高效训练&lt;h4&gt;目的&lt;/h4&gt;比较不同类型的成员推理攻击(MIAs)在迁移学习环境中的性能，帮助从业者识别用于隐私风险评估的最有效攻击方法&lt;h4&gt;方法&lt;/h4&gt;对比多种成员推理攻击在迁移学习设置中的表现，特别关注针对通过迁移学习微调的模型的攻击评估&lt;h4&gt;主要发现&lt;/h4&gt;基于分数的成员推理攻击的有效性随训练数据的增加而降低；没有一种单一的成员推理攻击能够捕捉迁移学习训练模型中的所有隐私风险；似然比攻击(LiRA)在大多数实验场景中表现优异；反向Hessian攻击(IHA)在针对PatchCamelyon数据集微调的高数据量模型上更有效&lt;h4&gt;结论&lt;/h4&gt;需要综合考虑多种成员推理攻击方法来全面评估迁移学习模型的隐私风险，不同攻击方法在不同场景下可能表现不同，应根据具体情况选择&lt;h4&gt;翻译&lt;/h4&gt;随着强大大规模基础模型的出现，训练范式正日益从从头开始训练转向迁移学习。这使得在敏感应用中，可以使用小型、领域特定的数据集进行高效训练。成员推理攻击(MIAs)为机器学习模型提供了隐私泄露的经验性估计。然而，先前针对通过迁移学习微调模型的成员推理攻击评估仅依赖于可能攻击的一小部分。我们通过比较迁移学习环境中不同成员推理攻击的性能来解决这一问题，以帮助从业者识别用于隐私风险评估的最有效攻击。我们发现，对于基于分数的成员推理攻击，攻击有效性随训练数据的增加而降低。我们发现，没有一种成员推理攻击能够捕捉迁移学习训练模型中的所有隐私风险。虽然似然比攻击(LiRA)在大多数实验场景中表现出优越性能，但反向Hessian攻击(IHA)被证明在针对PatchCamelyon数据集微调的高数据量模型上更有效&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emergence of powerful large-scale foundation models, the trainingparadigm is increasingly shifting from from-scratch training to transferlearning. This enables high utility training with small, domain-specificdatasets typical in sensitive applications.Membership inference attacks (MIAs)provide an empirical estimate of the privacy leakage by machine learningmodels. Yet, prior assessments of MIAs against models fine-tuned with transferlearning rely on a small subset of possible attacks. We address this bycomparing performance of diverse MIAs in transfer learning settings to helppractitioners identify the most efficient attacks for privacy risk evaluation.We find that attack efficacy decreases with the increase in training data forscore-based MIAs. We find that there is no one MIA which captures all privacyrisks in models trained with transfer learning. While the Likelihood RatioAttack (LiRA) demonstrates superior performance across most experimentalscenarios, the Inverse Hessian Attack (IHA) proves to be more effective againstmodels fine-tuned on PatchCamelyon dataset in high data regime.</description>
      <author>example@mail.com (Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, Antti Honkela)</author>
      <guid isPermaLink="false">2510.05753v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning on Edge Connecting Probability Estimation under Graphon Model</title>
      <link>http://arxiv.org/abs/2510.05527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GTRANS的迁移学习框架，用于在小规模目标图中提高图模型估计的准确性。该方法结合了邻域平滑和Gromov-Wasserstein最优传输，对齐和转移图之间的结构模式，并通过自适应去偏机制防止负迁移。&lt;h4&gt;背景&lt;/h4&gt;Graphon模型为估计网络中的潜在连接概率提供了一个灵活的非参数框架，支持链接预测和数据增强等下游应用。然而，准确的图模型估计通常需要大型图，而实践中往往只能观察到小型网络。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过迁移学习框架解决小型网络中图模型估计不准确的问题，利用大型相关源图的结构信息来改进小型目标图的估计。&lt;h4&gt;方法&lt;/h4&gt;提出了GTRANS方法，这是一个集成邻域平滑和Gromov-Wasserstein最优传输的迁移学习框架，用于对齐和转移图之间的结构模式。GTRANS包含一个自适应去偏机制，通过残差平滑识别并校正目标特定的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;研究提供了对估计对齐矩阵稳定性的理论保证，并通过大量合成和真实数据实验证明了GTRANS在提高目标图估计准确性方面的有效性。这些改进直接转化为下游应用（如图分类任务和链接预测任务）的性能提升。&lt;h4&gt;结论&lt;/h4&gt;GTRANS方法通过有效迁移结构信息并防止负迁移，显著提高了小规模网络中图模型估计的准确性，进而提升了下游应用性能。&lt;h4&gt;翻译&lt;/h4&gt;图模型(Graphon models)为估计网络中的潜在连接概率提供了一个灵活的非参数框架，支持链接预测和数据增强等一系列下游应用。然而，准确的图模型估计通常需要大型图，而实践中往往只能观察到小型网络。解决这一问题的一种方法是采用迁移学习框架，旨在利用来自更大相关源图的结构信息来改进小型目标图中的估计。在本文中，我们提出了一种新方法，即GTRANS，这是一个迁移学习框架，集成了邻域平滑和Gromov-Wasserstein最优传输，以对齐和转移图之间的结构模式。为防止负迁移，GTRANS包含一个自适应去偏机制，通过残差平滑识别并校正目标特定的偏差。我们提供了对估计对齐矩阵稳定性的理论保证，并通过大量合成和真实数据实验证明了GTRANS在提高目标图估计准确性方面的有效性。这些改进直接转化为下游应用的性能提升，如图分类任务和链接预测任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphon models provide a flexible nonparametric framework for estimatinglatent connectivity probabilities in networks, enabling a range of downstreamapplications such as link prediction and data augmentation. However, accurategraphon estimation typically requires a large graph, whereas in practice, oneoften only observes a small-sized network. One approach to addressing thisissue is to adopt a transfer learning framework, which aims to improveestimation in a small target graph by leveraging structural information from alarger, related source graph. In this paper, we propose a novel method, namelyGTRANS, a transfer learning framework that integrates neighborhood smoothingand Gromov-Wasserstein optimal transport to align and transfer structuralpatterns between graphs. To prevent negative transfer, GTRANS includes anadaptive debiasing mechanism that identifies and corrects for target-specificdeviations via residual smoothing. We provide theoretical guarantees on thestability of the estimated alignment matrix and demonstrate the effectivenessof GTRANS in improving the accuracy of target graph estimation throughextensive synthetic and real data experiments. These improvements translatedirectly to enhanced performance in downstream applications, such as the graphclassification task and the link prediction task.</description>
      <author>example@mail.com (Yuyao Wang, Yu-Hung Cheng, Debarghya Mukherjee, Huimin Cheng)</author>
      <guid isPermaLink="false">2510.05527v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating</title>
      <link>http://arxiv.org/abs/2510.05394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Workshop paper, AIP2025: Second Workshop on AI in Production (2025).  Licensed under CC BY 4.0&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型深度学习框架，用于PET预成型件在工业微波系统中预热过程的温度预测。通过迁移学习和模型融合技术，该方法能够在不同材料和设计条件下实现高效准确的温度预测，减少了重新训练的需求，并提高了泛化能力。&lt;h4&gt;背景&lt;/h4&gt;PET预成型件在工业微波系统中的预热过程对吹塑成型至关重要。准确且高效的温度预测是优化这一过程的关键，传统方法需要针对每种材料或设计变更进行大量重新训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种用于通用温度预测的新型深度学习框架，解决传统模型需要大量重新训练的问题，实现跨不同材料和设计条件的高效温度预测。&lt;h4&gt;方法&lt;/h4&gt;引入数据高效的神经网络架构，利用迁移学习和模型融合技术；预训练专门神经回归器处理不同条件（如回收PET热容量或变化的预成型件几何形状）；将表示集成到统一全局模型中；架构包含跳跃连接以增强稳定性和预测准确性；减少对大型模拟数据集的需求。&lt;h4&gt;主要发现&lt;/h4&gt;相比从头训练的模型，该方法实现了更优的性能；在材料变性和几何多样性两个案例研究上的实验验证表明泛化能力有显著提高；建立了一种可扩展的基于机器学习的制造环境智能热控制解决方案。&lt;h4&gt;结论&lt;/h4&gt;数据高效的泛化策略可以扩展到其他涉及有限数据复杂物理建模的工业应用，为制造环境中的智能热控制提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确且高效的温度预测对于优化PET预成型件在工业微波系统中吹塑成型前的预热过程至关重要。我们提出了一种用于通用温度预测的新型深度学习框架。与需要针对每种材料或设计变更进行大量重新训练的传统模型不同，我们的方法引入了一种数据高效的神经网络架构，利用迁移学习和模型融合来推广到未见过的场景。通过在不同条件下预训练专门的神经回归器，并将它们的表示集成到统一的全局模型中，我们创建了一个能够学习异构输入间共享热动力学的系统。该架构包含跳跃连接以增强稳定性和预测准确性。我们的方法减少了对大型模拟数据集的需求，同时实现了比从头训练的模型更优的性能。在两个案例研究上的实验验证表明泛化能力有显著提高，为制造环境中的智能热控制建立了可扩展的基于机器学习的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient temperature prediction is critical for optimizing thepreheating process of PET preforms in industrial microwave systems prior toblow molding. We propose a novel deep learning framework for generalizedtemperature prediction. Unlike traditional models that require extensiveretraining for each material or design variation, our method introduces adata-efficient neural architecture that leverages transfer learning and modelfusion to generalize across unseen scenarios. By pretraining specialized neuralregressor on distinct conditions such as recycled PET heat capacities orvarying preform geometries and integrating their representations into a unifiedglobal model, we create a system capable of learning shared thermal dynamicsacross heterogeneous inputs. The architecture incorporates skip connections toenhance stability and prediction accuracy. Our approach reduces the need forlarge simulation datasets while achieving superior performance compared tomodels trained from scratch. Experimental validation on two case studiesmaterial variability and geometric diversity demonstrates significantimprovements in generalization, establishing a scalable ML-based solution forintelligent thermal control in manufacturing environments. Moreover, theapproach highlights how data-efficient generalization strategies can extend toother industrial applications involving complex physical modeling with limiteddata.</description>
      <author>example@mail.com (Ahmad Alsheikh, Andreas Fischer)</author>
      <guid isPermaLink="false">2510.05394v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection</title>
      <link>http://arxiv.org/abs/2510.05326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Double column 6 pages, 10 figures, ieee conference style&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了五种预训练卷积神经网络模型在芒果叶疾病识别中的性能，发现DenseNet201表现最佳，为智能农业中的疾病检测提供了可靠解决方案。&lt;h4&gt;背景&lt;/h4&gt;芒果是南亚重要的水果作物，但其种植经常受到叶部疾病的阻碍，这些疾病严重影响产量和质量。&lt;h4&gt;目的&lt;/h4&gt;研究五种预训练卷积神经网络模型在芒果叶疾病多类识别中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;采用迁移学习策略和微调技术，测试DenseNet201、InceptionV3、ResNet152V2、SeResNet152和Xception五种模型，针对八类芒果叶疾病进行多类识别，并通过准确率、精确率、召回率、F1分数和混淆矩阵等标准评估指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet201表现最佳，准确率达99.33%，在各类别指标上表现一致，特别是在识别切象鼻虫和细菌性溃疡方面表现出色；ResNet152V2和SeResNet152提供了良好结果；InceptionV3和Xception在视觉相似的类别（如煤污病和白粉病）中表现较差；高性能模型的训练和验证图显示了稳定收敛。&lt;h4&gt;结论&lt;/h4&gt;微调的迁移学习模型能够为智能农业应用中的芒果叶疾病检测提供精确可靠的多类检测能力。&lt;h4&gt;翻译&lt;/h4&gt;芒果是南亚重要的水果作物，但其种植经常受到叶部疾病的阻碍，这些疾病严重影响产量和质量。本研究考察了五种预训练卷积神经网络模型（DenseNet201、InceptionV3、ResNet152V2、SeResNet152和Xception）在迁移学习策略和微调技术下，针对八类芒果叶疾病进行多类识别的性能。这些模型通过准确率、精确率、召回率、F1分数和混淆矩阵等标准评估指标进行评估。在测试的架构中，DenseNet201取得了最佳结果，准确率达到99.33%，各类别指标表现一致，特别是在识别切象鼻虫和细菌性溃疡方面表现出色。此外，ResNet152V2和SeResNet152提供了良好的结果，而InceptionV3和Xception在视觉相似的类别（如煤污病和白粉病）中表现较差。训练和验证图显示了高性能模型的稳定收敛。微调的迁移学习模型具备在智能农业应用中进行精确可靠的芒果叶疾病多类检测的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mango is an important fruit crop in South Asia, but its cultivation isfrequently hampered by leaf diseases that greatly impact yield and quality.This research examines the performance of five pre-trained convolutional neuralnetworks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, formulti-class identification of mango leaf diseases across eight classes using atransfer learning strategy with fine-tuning. The models were assessed throughstandard evaluation metrics, such as accuracy, precision, recall, F1-score, andconfusion matrices. Among the architectures tested, DenseNet201 delivered thebest results, achieving 99.33% accuracy with consistently strong metrics forindividual classes, particularly excelling in identifying Cutting Weevil andBacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strongoutcomes, whereas InceptionV3 and Xception exhibited lower performance invisually similar categories like Sooty Mould and Powdery Mildew. The trainingand validation plots demonstrated stable convergence for the highest-performingmodels. The capability of fine-tuned transfer learning models, for precise anddependable multi-class mango leaf disease detection in intelligent agriculturalapplications.</description>
      <author>example@mail.com (Jalal Ahmmed, Faruk Ahmed, Rashedul Hasan Shohan, Md. Mahabub Rana, Mahdi Hasan)</author>
      <guid isPermaLink="false">2510.05326v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A novel hallucination classification framework</title>
      <link>http://arxiv.org/abs/2510.05189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自动检测大型语言模型推理过程中产生幻觉的新方法，通过系统化分类学和提示工程实现幻觉类型的可控重现，并利用无监督学习技术分析幻觉数据。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在推理过程中会产生幻觉现象，这些幻觉可能包含错误或误导性信息，影响模型输出的可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动检测和区分LLM生成幻觉与准确响应的有效方法，提高模型输出的可靠性。&lt;h4&gt;方法&lt;/h4&gt;基于系统化分类学，通过提示工程控制性重现不同类型幻觉；使用嵌入模型将幻觉数据集映射到向量空间；在降维表示中应用无监督学习技术分析幻觉与真实响应；评估质心间距离的定量分析。&lt;h4&gt;主要发现&lt;/h4&gt;幻觉中信息失真的严重程度与它们从正确输出簇的空间偏离之间存在一致的相关性；简单的分类算法可以在单个LLM中可靠地区分幻觉和准确响应。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为提高模型可靠性提供了一种轻量级但有效的框架，即使使用简单算法也能可靠识别幻觉。&lt;h4&gt;翻译&lt;/h4&gt;这项工作引入了一种用于自动检测大型语言模型推理过程中产生的幻觉的新方法。所提出的方法基于系统化的分类学，并通过提示工程控制性地重现不同类型的幻觉。随后，使用嵌入模型将专门的幻觉数据集映射到向量空间，并在降维表示中，使用无监督学习技术分析幻觉与真实响应。对质心间距离的定量评估显示，幻觉中信息失真的严重程度与它们从正确输出簇的空间偏离之间存在一致的相关性。这些发现提供了理论和实证证据，表明即使在单个LLM中，简单的分类算法也可以可靠地区分幻觉和准确响应，从而为提高模型可靠性提供了一种轻量级而有效的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a novel methodology for the automatic detection ofhallucinations generated during large language model (LLM) inference. Theproposed approach is based on a systematic taxonomy and controlled reproductionof diverse hallucination types through prompt engineering. A dedicatedhallucination dataset is subsequently mapped into a vector space using anembedding model and analyzed with unsupervised learning techniques in areduced-dimensional representation of hallucinations with veridical responses.Quantitative evaluation of inter-centroid distances reveals a consistentcorrelation between the severity of informational distortion in hallucinationsand their spatial divergence from the cluster of correct outputs. Thesefindings provide theoretical and empirical evidence that even simpleclassification algorithms can reliably distinguish hallucinations from accurateresponses within a single LLM, thereby offering a lightweight yet effectiveframework for improving model reliability.</description>
      <author>example@mail.com (Maksym Zavhorodnii, Dmytro Dehtiarov, Anna Konovalenko)</author>
      <guid isPermaLink="false">2510.05189v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment</title>
      <link>http://arxiv.org/abs/2510.05157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 tables, 5 figures. 12th International Conference on Next  Generation Computing, Communication, Systems and Security&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过自定义OpenAI Gym环境，对网络安全的对抗性强化学习进行了受控研究，模拟了多端口服务上的暴力攻击和反应式防御策略。&lt;h4&gt;背景&lt;/h4&gt;网络环境中存在真实的安全权衡，包括背景流量噪声、渐进式利用机制、基于IP的规避策略、蜜罐陷阱和多级速率限制防御等复杂因素。&lt;h4&gt;目的&lt;/h4&gt;研究对抗性强化学习在网络安全中的应用，特别是在攻击者和防御者之间的零和奖励框架下，评估不同配置下防御者的有效性。&lt;h4&gt;方法&lt;/h4&gt;创建自定义OpenAI Gym环境模拟网络攻击和防御场景；使用深度Q网络(DQN)训练攻击者和防御者代理；在零和奖励框架下进行训练，成功利用获得大额终端奖励，增量操作产生小成本；系统性评估多种配置（变化陷阱检测概率、利用难度阈值和训练计划）。&lt;h4&gt;主要发现&lt;/h4&gt;防御者的可观测性和蜜罐的有效性对成功攻击构成重大障碍；奖励塑造和仔细的训练调度对于在这种对抗环境中学习稳定性至关重要；防御者在50,000+训练集中始终保持战略优势；当面对复杂的防御策略（包括自适应IP阻止和特定于端口的控制）时，性能增益会放大。&lt;h4&gt;结论&lt;/h4&gt;提供了完整的实现细节、可复现的超参数配置和架构指南，以支持未来在网络安全对抗性RL方面的研究；零和公式和真实的操作约束使该环境适合研究自主防御系统、攻击者-防御者共同进化和向真实世界网络安全场景的迁移学习。&lt;h4&gt;翻译&lt;/h4&gt;本文通过自定义OpenAI Gym环境对网络安全的对抗性强化学习进行了受控研究，该环境模拟了多端口服务上的暴力攻击和反应式防御。该环境捕捉了真实的安全权衡，包括背景流量噪声、渐进式利用机制、基于IP的规避策略、蜜罐陷阱和多级速率限制防御。使用深度Q网络(DQN)在零和奖励框架下训练竞争性的攻击者和防御者代理，成功的利用产生大的终端奖励，而增量操作产生小成本。通过系统评估多种配置（变化的陷阱检测概率、利用难度阈值和训练计划），结果表明防御者的可观测性和蜜罐的有效性对成功攻击构成重大障碍。实验揭示，奖励塑造和仔细的训练调度对于在这种对抗环境中的学习稳定性至关重要。防御者在50,000+训练集中始终保持战略优势，当面对复杂的防御策略（包括自适应IP阻止和特定于端口的控制）时，性能增益会放大。提供了完整的实现细节、可复现的超参数配置和架构指南，以支持未来在网络安全对抗性RL方面的研究。零和公式和真实的操作约束使该环境适合研究自主防御系统、攻击者-防御者共同进化和向真实世界网络安全场景的迁移学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a controlled study of adversarial reinforcement learningin network security through a custom OpenAI Gym environment that modelsbrute-force attacks and reactive defenses on multi-port services. Theenvironment captures realistic security trade-offs including background trafficnoise, progressive exploitation mechanics, IP-based evasion tactics, honeypottraps, and multi-level rate-limiting defenses. Competing attacker and defenderagents are trained using Deep Q-Networks (DQN) within a zero-sum rewardframework, where successful exploits yield large terminal rewards whileincremental actions incur small costs. Through systematic evaluation acrossmultiple configurations (varying trap detection probabilities, exploitationdifficulty thresholds, and training regimens), the results demonstrate thatdefender observability and trap effectiveness create substantial barriers tosuccessful attacks. The experiments reveal that reward shaping and carefultraining scheduling are critical for learning stability in this adversarialsetting. The defender consistently maintains strategic advantage across 50,000+training episodes, with performance gains amplifying when exposed to complexdefensive strategies including adaptive IP blocking and port-specific controls.Complete implementation details, reproducible hyperparameter configurations,and architectural guidelines are provided to support future research inadversarial RL for cybersecurity. The zero-sum formulation and realisticoperational constraints make this environment suitable for studying autonomousdefense systems, attacker-defender co-evolution, and transfer learning toreal-world network security scenarios.</description>
      <author>example@mail.com (Abrar Shahid, Ibteeker Mahir Ishum, AKM Tahmidul Haque, M Sohel Rahman, A. B. M. Alim Al Islam)</author>
      <guid isPermaLink="false">2510.05157v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection</title>
      <link>http://arxiv.org/abs/2510.05900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 26th International Conference on Web Information  Systems Engineering (WISE 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhishSSL是一种自监督对比学习框架，用于检测网络钓鱼网站，它消除了对标记钓鱼数据的依赖，通过混合表格增强和自适应特征注意力技术，在各种数据集上都优于无监督和自监督基线方法，显示出强大的泛化能力和可转移性，是应对动态网络环境中不断演变威胁的有效解决方案。&lt;h4&gt;背景&lt;/h4&gt;网络钓鱼网站通过模仿合法网站来窃取敏感用户信息，持续构成网络安全威胁。现有的基于机器学习的检测方法通常依赖标记数据的监督学习，这不仅带来大量的标注成本，还限制了适应新型攻击模式的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有监督学习方法面临的标注成本高和对新型攻击模式适应性差的问题，开发一种无需标记钓鱼数据训练的检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出PhishSSL，一种自监督对比学习框架，结合混合表格增强与自适应特征注意力，生成语义一致的视图并强调判别性属性，在三种具有不同特征组成的网络钓鱼数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;在所有数据集上，PhishSSL持续优于无监督和自监督基线方法；消融研究确认了每个组件的贡献；尽管特征集多样化，PhishSSL仍保持强大的性能，突显了其强大的泛化能力和可转移性。&lt;h4&gt;结论&lt;/h4&gt;PhishSSL为网络钓鱼网站检测提供了有前景的解决方案，特别有效于应对动态网络环境中不断演变的威胁。&lt;h4&gt;翻译&lt;/h4&gt;网络钓鱼网站通过模仿合法网站来窃取敏感用户信息，持续构成网络安全威胁。现有的基于机器学习的检测方法通常依赖标记数据的监督学习，这不仅带来大量的标注成本，还限制了适应新型攻击模式的能力。为应对这些挑战，我们提出了PhishSSL，一种自监督对比学习框架，消除了训练期间对标记钓鱼数据的需求。PhishSSL结合混合表格增强与自适应特征注意力，生成语义一致的视图并强调判别性属性。我们在三种具有不同特征组成的网络钓鱼数据集上评估了PhishSSL。在所有数据集上，PhishSSL持续优于无监督和自监督基线方法，同时消融研究确认了每个组件的贡献。此外，尽管特征集多样化，PhishSSL仍保持强大的性能，突显了其强大的泛化能力和可转移性。这些结果表明，PhishSSL为网络钓鱼网站检测提供了有前景的解决方案，特别有效于应对动态网络环境中不断演变的威胁。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Phishing websites remain a persistent cybersecurity threat by mimickinglegitimate sites to steal sensitive user information. Existing machinelearning-based detection methods often rely on supervised learning with labeleddata, which not only incurs substantial annotation costs but also limitsadaptability to novel attack patterns. To address these challenges, we proposePhishSSL, a self-supervised contrastive learning framework that eliminates theneed for labeled phishing data during training. PhishSSL combines hybridtabular augmentation with adaptive feature attention to produce semanticallyconsistent views and emphasize discriminative attributes. We evaluate PhishSSLon three phishing datasets with distinct feature compositions. Across alldatasets, PhishSSL consistently outperforms unsupervised and self-supervisedbaselines, while ablation studies confirm the contribution of each component.Moreover, PhishSSL maintains robust performance despite the diversity offeature sets, highlighting its strong generalization and transferability. Theseresults demonstrate that PhishSSL offers a promising solution for phishingwebsite detection, particularly effective against evolving threats in dynamicWeb environments.</description>
      <author>example@mail.com (Wenhao Li, Selvakumar Manickam, Yung-Wey Chong, Shankar Karuppayah, Priyadarsi Nanda, Binyong Li)</author>
      <guid isPermaLink="false">2510.05900v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust and Realible Multimodal Fake News Detection with Incomplete Modality</title>
      <link>http://arxiv.org/abs/2510.05839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MMLNet（多专家模态不完整学习网络），一种简单有效的多模态融合策略，用于解决多模态假新闻检测中模态不完整的问题。&lt;h4&gt;背景&lt;/h4&gt;随着社交媒体平台上大量多模态虚假内容的出现，多模态假新闻检测成为紧迫任务。现有研究主要关注复杂特征提取和融合，但忽略了实际应用中多媒体新闻在传播过程中可能丢失信息导致的模态不完整问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用且鲁棒的多模态融合策略，确保在模态信息缺失情况下仍能准确检测虚假新闻，有效遏制恶意虚假信息传播。&lt;h4&gt;方法&lt;/h4&gt;MMLNet包含三个关键步骤：(1)多专家协作推理：通过多个专家动态利用互补信息补偿缺失模态；(2)不完整模态适配器：利用新特征分布补偿缺失信息；(3)模态缺失学习：通过标签感知的自适应加权策略和对比学习学习鲁棒表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两种语言的三个真实世界基准测试中，MMLNet与最先进方法相比表现优越，同时保持相对简单的结构，能有效处理模态不完整的假新闻检测场景。&lt;h4&gt;结论&lt;/h4&gt;MMLNet通过解决多模态假新闻检测中的模态不完整问题，提高了检测的准确性和鲁棒性，有效遏制了恶意虚假信息的传播。代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;多模态假新闻检测（MFND）已成为一项紧迫任务，随着社交媒体平台上大量多模态虚假内容的出现。先前的研究主要集中在复杂特征提取和融合上，从多模态内容中学习判别性信息。然而，在实际应用中，多媒体新闻在传播过程中可能会自然丢失一些信息，导致模态不完整，这对现有模型的泛化能力和鲁棒性不利。为此，我们提出了一种新颖的通用且鲁棒的多模态融合策略，称为多专家模态不完整学习网络（MMLNet），它简单而有效。它包含三个关键步骤：(1) 多专家协作推理，通过多个专家动态利用互补信息来补偿缺失的模态。(2) 不完整模态适配器通过利用新的特征分布来补偿缺失的信息。(3) 模态缺失学习利用标签感知的自适应加权策略，通过对比学习学习鲁棒表示。我们在两种语言的三个真实世界基准上评估了MMLNet，与最先进的方法相比表现出优越的性能，同时保持了相对简单的结构。通过确保在信息传播导致的不完整模态场景下假新闻检测的准确性，MMLNet有效地遏制了恶意虚假信息的传播。代码已在GitHub公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal fake news detection (MFND) has become an urgent task with theemergence of huge multimodal fake content on social media platforms. Previousstudies mainly focus on complex feature extraction and fusion to learndiscriminative information from multimodal content. However, in real-worldapplications, multimedia news may naturally lose some information duringdissemination, resulting in modality incompleteness, which is detrimental tothe generalization and robustness of existing models. To this end, we propose anovel generic and robust multimodal fusion strategy, termed Multi-expertModality-incomplete Learning Network (MMLNet), which is simple yet effective.It consists of three key steps: (1) Multi-Expert Collaborative Reasoning tocompensate for missing modalities by dynamically leveraging complementaryinformation through multiple experts. (2) Incomplete Modality Adapterscompensates for the missing information by leveraging the new featuredistribution. (3) Modality Missing Learning leveraging an label-aware adaptiveweighting strategy to learn a robust representation with contrastive learning.We evaluate MMLNet on three real-world benchmarks across two languages,demonstrating superior performance compared to state-of-the-art methods whilemaintaining relative simplicity. By ensuring the accuracy of fake newsdetection in incomplete modality scenarios caused by information propagation,MMLNet effectively curbs the spread of malicious misinformation. Code ispublicly available at https://github.com/zhyhome/MMLNet.</description>
      <author>example@mail.com (Hengyang Zhou, Yiwei Wei, Jian Yang, Zhenyu Zhang)</author>
      <guid isPermaLink="false">2510.05839v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</title>
      <link>http://arxiv.org/abs/2510.05767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过推导非渐近谱带来界定平方InfoNCE梯度范数，并设计了谱感知批量选择方法，有效提高了训练效率。&lt;h4&gt;背景&lt;/h4&gt;研究关注InfoNCE梯度的谱特性以及批量选择对训练效率的影响。&lt;h4&gt;目的&lt;/h4&gt;推导非渐近谱带用于界定平方InfoNCE梯度范数，并设计基于谱特性的高效批量选择方法。&lt;h4&gt;方法&lt;/h4&gt;通过对齐、温度和批量谱推导非渐近谱带；使用有效秩作为各向异性代理设计谱感知批量选择；采用批量内白化促进各向同性。&lt;h4&gt;主要发现&lt;/h4&gt;恢复了1/τ²定律；在合成数据和ImageNet上密切跟踪批量均值梯度；Greedy-64在ImageNet-100上将训练时间缩短15%(相比随机)和24%(相比Pool-P3)；CIFAR-10显示类似改进；批量内白化减少50步梯度方差1.37倍，与理论上界匹配。&lt;h4&gt;结论&lt;/h4&gt;谱感知批量选择方法能有效提高训练效率；批量内白化能有效减少梯度方差。&lt;h4&gt;翻译&lt;/h4&gt;我们推导了通过对比度、温度和批量谱来界定平方InfoNCE梯度范数的非渐近谱带，恢复了1/τ²定律，并在合成数据和ImageNet上密切跟踪批量均值梯度。使用有效秩作为各向异性代理，我们设计了谱感知批量选择，包括一个快速贪婪构建器。在ImageNet-100上，Greedy-64在达到相同准确率的情况下，相比随机方法将时间缩短15%(相比Pool-P3缩短24%)；CIFAR-10显示出类似的改进。批量内白化促进各向同性并将50步梯度方差减少1.37倍，与我们的理论上限一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We derive non-asymptotic spectral bands that bound the squared InfoNCEgradient norm via alignment, temperature, and batch spectrum, recovering the\(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic dataand ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy,we design spectrum-aware batch selection, including a fast greedy builder. OnImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\%vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batchwhitening promotes isotropy and reduces 50-step gradient variance by\(1.37\times\), matching our theoretical upper bound.</description>
      <author>example@mail.com (Peter Ochieng)</author>
      <guid isPermaLink="false">2510.05767v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies</title>
      <link>http://arxiv.org/abs/2510.05692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL)的新框架，用于提高视觉运动政策学习的样本效率和渐进性能。&lt;h4&gt;背景&lt;/h4&gt;学习视觉运动政策的普遍方法是使用强化学习将高维视觉观察直接映射到动作命令，但高维视觉输入与敏捷机动输出的组合导致样本效率低下和显著的模拟到现实差距问题。&lt;h4&gt;目的&lt;/h4&gt;解决视觉运动政策学习中的样本效率和渐进性能问题，同时改善泛化能力。&lt;h4&gt;方法&lt;/h4&gt;OMC-RL框架将学习过程分为两个阶段：1）上游表征学习阶段：使用带掩码的Transformer模块进行时间建模和对比学习，从序列视觉输入中提取时间感知和任务相关的表征；2）下游政策学习阶段：利用拥有全局状态信息的oracle教师政策在早期训练中提供指导，随训练进展逐渐减少指导以促进独立探索。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界环境中的大量实验表明，OMC-RL实现了卓越的样本效率和渐进政策性能，同时提高了在多样性和感知复杂场景中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;OMC-RL框架有效解决了视觉运动政策学习中的样本效率和模拟到现实差距问题，为强化学习在视觉控制任务中的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;学习视觉运动政策的一种普遍方法是使用强化学习将高维视觉观察直接映射到动作命令。然而，高维视觉输入和敏捷机动输出的组合导致长期挑战，包括样本效率低下和显著的模拟到现实差距。为解决这些问题，我们提出了Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL)，这是一个新框架，旨在提高视觉运动政策学习的样本效率和渐进性能。OMC-RL明确将学习过程分为两个阶段：上游表征学习阶段和下游政策学习阶段。在上游阶段，带掩码的Transformer模块通过时间建模和对比学习进行训练，从序列视觉输入中提取时间感知和任务相关的表征。训练后，学习到的编码器被冻结用于从连续帧中提取视觉表征，而Transformer模块被丢弃。在下游阶段，拥有全局状态信息特权访问的oracle教师政策在早期训练中指导代理，提供信息性指导并加速早期政策学习。这种指导会逐渐减少，以允许随着训练进展进行独立探索。在模拟和真实世界环境中的大量实验表明，OMC-RL实现了卓越的样本效率和渐进政策性能，同时提高了在多样性和感知复杂场景中的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A prevailing approach for learning visuomotor policies is to employreinforcement learning to map high-dimensional visual observations directly toaction commands. However, the combination of high-dimensional visual inputs andagile maneuver outputs leads to long-standing challenges, including low sampleefficiency and significant sim-to-real gaps. To address these issues, wepropose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), anovel framework designed to improve the sample efficiency and asymptoticperformance of visuomotor policy learning. OMC-RL explicitly decouples thelearning process into two stages: an upstream representation learning stage anda downstream policy learning stage. In the upstream stage, a masked Transformermodule is trained with temporal modeling and contrastive learning to extracttemporally-aware and task-relevant representations from sequential visualinputs. After training, the learned encoder is frozen and used to extractvisual representations from consecutive frames, while the Transformer module isdiscarded. In the downstream stage, an oracle teacher policy with privilegedaccess to global state information supervises the agent during early trainingto provide informative guidance and accelerate early policy learning. Thisguidance is gradually reduced to allow independent exploration as trainingprogresses. Extensive experiments in simulated and real-world environmentsdemonstrate that OMC-RL achieves superior sample efficiency and asymptoticpolicy performance, while also improving generalization across diverse andperceptually complex scenarios.</description>
      <author>example@mail.com (Yuhang Zhang, Jiaping Xiao, Chao Yan, Mir Feroskhan)</author>
      <guid isPermaLink="false">2510.05692v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry</title>
      <link>http://arxiv.org/abs/2510.04631v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted to EMNLP 2025 (industry track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探索了将针对科学出版物设计的图感知邻域对比学习方法SciNCL应用于过程工业领域，通过使用图嵌入三元组微调语言模型，在专有基准测试上显著优于最先进模型，同时模型参数量更少。&lt;h4&gt;背景&lt;/h4&gt;最近NLP趋势利用知识图谱增强预训练语言模型，通过图结构中的额外知识学习领域特定术语或文档间关系。&lt;h4&gt;目的&lt;/h4&gt;探索如何将SciNCL应用于过程工业领域，该领域的文本日志包含日常操作的关键信息，通常被构造成稀疏知识图谱。&lt;h4&gt;方法&lt;/h4&gt;使用从图嵌入(GE)推导出的三元组对语言模型进行微调，在过程工业文本嵌入基准(PITEB)上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在PITEB上，使用图嵌入三元组微调的语言模型比最先进的mE5-large文本编码器高出9.8-14.3%(5.45-7.96p)，同时参数量减少3倍。&lt;h4&gt;结论&lt;/h4&gt;图感知对比学习方法在过程工业的文本嵌入任务上表现优异，同时模型更轻量，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;最近的NLP趋势利用知识图谱(KGs)来增强预训练语言模型，通过从图结构中纳入额外知识来学习领域特定术语或文档间关系，这些关系可能被忽略。本文探讨了如何将SciNCL(一种最初为科学出版物设计的图感知邻域对比学习方法)应用于过程工业领域，该领域的文本日志包含关于日常操作的关键信息，通常被构造成稀疏知识图谱。我们的实验证明，使用从图嵌入(GE)推导出的三元组进行微调的语言模型在专有的过程工业文本嵌入基准(PITEB)上比最先进的mE5-large文本编码器高出9.8-14.3%(5.45-7.96p)，同时参数量减少3倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrainedlanguage models by incorporating additional knowledge from the graph structuresto learn domain-specific terminology or relationships between documents thatmight otherwise be overlooked. This paper explores how SciNCL, a graph-awareneighborhood contrastive learning methodology originally designed forscientific publications, can be applied to the process industry domain, wheretext logs contain crucial information about daily operations and are oftenstructured as sparse KGs. Our experiments demonstrate that language modelsfine-tuned with triplets derived from graph embeddings (GE) outperform astate-of-the-art mE5-large text encoder by 9.8-14.3% (5.45-7.96p) on theproprietary process industry text embedding benchmark (PITEB) while having 3times fewer parameters.</description>
      <author>example@mail.com (Anastasia Zhukova, Jonas Lührs, Christian E. Lobmüller, Bela Gipp)</author>
      <guid isPermaLink="false">2510.04631v2</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data</title>
      <link>http://arxiv.org/abs/2510.05172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Transactions on Industrial Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于自监督预训练的电动汽车电池容量估计模型，利用大规模隐私友好型充电数据片段，通过片段相似性加权的掩码输入重建和对比学习技术，从特征较少且碎片化的数据中学习丰富、可泛化的表示，显著提高了容量估计的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的电池容量估计对缓解消费者对电动车电池性能和可靠性的担忧至关重要。然而，严格的隐私法规和标记数据短缺限制了通用容量估计模型的发展。现有的自监督学习技术不能有效从具有挑战性的现场数据或隐私友好型数据（通常特征较少且噪声更大）中学习有效信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于自监督预训练的容量估计模型，利用大规模隐私友好型充电数据片段，从特征较少且碎片化的隐私友好型数据中学习丰富、可泛化的表示，提高电池容量估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出片段相似性加权的掩码输入重建预训练框架，利用对比学习捕获片段之间的高层次相似性，结合片段级对比学习和相似性加权掩码重建，学习单个片段内的细粒度充电模式和不同片段间的高层次关联关系。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在具有挑战性的领域偏移设置下表现一致优于最先进的基线，与性能最好的基准相比，测试误差降低了31.9%，能够处理由制造商和年龄引起的分布偏移。&lt;h4&gt;结论&lt;/h4&gt;该研究首次提出基于自监督预训练的容量估计模型，能够从隐私友好型数据中学习丰富的表示，在实际应用中表现出色，特别是在处理具有挑战性的领域偏移情况时。&lt;h4&gt;翻译&lt;/h4&gt;准确的电池容量估计是缓解消费者对电动汽车电池性能和可靠性担忧的关键。然而，严格的隐私法规限制和标记数据短缺阻碍了能够保持对现实世界数据分布偏移具有鲁棒性的通用容量估计模型的发展。虽然自监督学习可以利用未标记数据，但现有技术并非特别设计用于从具有挑战性的现场数据中有效学习——更不用说从隐私友好型数据中学习了，这些数据通常特征较少且噪声更大。在这项工作中，我们提出了首个基于自监督预训练的容量估计模型，该模型基于从现实世界电动汽车操作中收集的大规模隐私友好型充电数据片段集进行开发。我们的预训练框架——片段相似性加权的掩码输入重建——旨在从特征较少且碎片化的隐私友好型数据中学习丰富、可泛化的表示。我们的关键创新在于利用对比学习首先捕获碎片化片段之间的高层次相似性，否则这些片段缺乏有意义的上下文。通过我们的片段级对比学习和后续的相似性加权掩码重建，我们能够学习单个片段内细粒度充电模式和不同片段间高层次关联关系的丰富表示。凭借这种丰富的表示学习，我们的模型一致优于最先进的基线，即使在受到制造商和年龄引起的分布偏移影响的具有挑战性的领域偏移设置下，也比性能最好的基准实现了31.9%的更低测试误差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TII.2025.3613385&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate battery capacity estimation is key to alleviating consumer concernsabout battery performance and reliability of electric vehicles (EVs). However,practical data limitations imposed by stringent privacy regulations and labeleddata shortages hamper the development of generalizable capacity estimationmodels that remain robust to real-world data distribution shifts. Whileself-supervised learning can leverage unlabeled data, existing techniques arenot particularly designed to learn effectively from challenging field data --let alone from privacy-friendly data, which are often less feature-rich andnoisier. In this work, we propose a first-of-its-kind capacity estimation modelbased on self-supervised pre-training, developed on a large-scale dataset ofprivacy-friendly charging data snippets from real-world EV operations. Ourpre-training framework, snippet similarity-weighted masked inputreconstruction, is designed to learn rich, generalizable representations evenfrom less feature-rich and fragmented privacy-friendly data. Our key innovationlies in harnessing contrastive learning to first capture high-levelsimilarities among fragmented snippets that otherwise lack meaningful context.With our snippet-wise contrastive learning and subsequent similarity-weightedmasked reconstruction, we are able to learn rich representations of bothgranular charging patterns within individual snippets and high-levelassociative relationships across different snippets. Bolstered by this richrepresentation learning, our model consistently outperforms state-of-the-artbaselines, achieving 31.9% lower test error than the best-performing benchmark,even under challenging domain-shifted settings affected by both manufacturerand age-induced distribution shifts.</description>
      <author>example@mail.com (Anushiya Arunan, Yan Qin, Xiaoli Li, U-Xuan Tan, H. Vincent Poor, Chau Yuen)</author>
      <guid isPermaLink="false">2510.05172v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer</title>
      <link>http://arxiv.org/abs/2510.06128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 25 tables, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了并行标记器(parallel tokenizers)新框架，通过确保语义等价的词在不同语言中获得一致的表示，改进多语言语言模型中的标记化过程，提高跨语言迁移学习效果。&lt;h4&gt;背景&lt;/h4&gt;现有多语言标记化方法无法支持有效的跨语言迁移，因为语义等价的词被分配到不同的嵌入表示中。例如，英语中的'I eat rice'和豪萨语中的'Ina cin shinkafa'通常被映射到不同的词汇索引，阻碍了共享表示和跨语言泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的标记化框架，确保语义等价的词在不同语言中获得一致的表示，从而改进多语言表示学习，特别是在低资源语言环境中。&lt;h4&gt;方法&lt;/h4&gt;提出并行标记器框架，首先单语训练标记器，然后使用双语词典或词到词翻译彻底对齐词汇表，确保语义等价的词具有一致的索引。在十三种低资源语言上预训练transformer编码器，并在情感分析、仇恨言论检测、情感分类和句子嵌入相似性任务上评估效果。&lt;h4&gt;主要发现&lt;/h4&gt;在所有评估任务中，使用并行标记器训练的模型都优于传统的多语言基线模型，证实了重新思考标记化对于推进多语言表示学习的重要性。&lt;h4&gt;结论&lt;/h4&gt;重新思考标记化方法对于推进多语言表示学习至关重要，特别是在低资源语言设置中。并行标记器框架确保语义等价的词在不同语言中获得一致的表示，提高了跨语言迁移学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;Tokenization: 标记化/分词；Multilingual language models: 多语言语言模型；Cross-lingual transfer: 跨语言迁移；Embeddings: 嵌入表示；Parallel tokenizers: 并行标记器；Bilingual dictionaries: 双语词典；Transformer encoder: Transformer编码器；Low-resource languages: 低资源语言；Sentiment analysis: 情感分析；Hate speech detection: 仇恨言论检测；Emotion classification: 情感分类；Sentence embedding similarity: 句子嵌入相似性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tokenization defines the foundation of multilingual language models bydetermining how words are represented and shared across languages. However,existing methods often fail to support effective cross-lingual transfer becausesemantically equivalent words are assigned distinct embeddings. For example, "Ieat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped todifferent vocabulary indices, preventing shared representations and limitingcross-lingual generalization. We introduce parallel tokenizers. This newframework trains tokenizers monolingually and then aligns their vocabulariesexhaustively using bilingual dictionaries or word-to-word translation, ensuringconsistent indices for semantically equivalent words. This alignment enforces ashared semantic space across languages while naturally improving fertilitybalance. To assess their effectiveness, we pretrain a transformer encoder fromscratch on thirteen low-resource languages and evaluate it on sentimentanalysis, hate speech detection, emotion classification, and sentence embeddingsimilarity. Across all tasks, models trained with parallel tokenizersoutperform conventional multilingual baselines, confirming that rethinkingtokenization is essential for advancing multilingual representationlearning--especially in low-resource settings.</description>
      <author>example@mail.com (Muhammad Dehan Al Kautsar, Fajri Koto)</author>
      <guid isPermaLink="false">2510.06128v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Trajectory Representation Learning for Travel Time Estimation</title>
      <link>http://arxiv.org/abs/2510.05840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态动态轨迹集成（MDTI）框架，通过整合GPS序列、网格轨迹和道路网络约束来提高行程时间估计（TTE）的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的行程时间估计在智能交通系统中至关重要，但由于异构数据源和复杂交通动态，这仍然具有挑战性。传统方法将轨迹转换为固定长度表示，忽略了现实世界轨迹的固有变异性，导致信息丢失或特征冗余。&lt;h4&gt;目的&lt;/h4&gt;解决行程时间估计中的挑战，提高准确性，通过整合多种数据源和动态建模来克服传统方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;MDTI框架采用模态特定编码器和跨模态交互模块捕获互补的空间、时间和拓扑语义，并使用动态轨迹建模机制自适应调节不同长度轨迹的信息密度。同时，通过对比对齐和掩码语言建模两种自监督预训练目标来加强多模态一致性和上下文理解。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的大量实验表明，MDTI框架始终优于最先进的基线方法，证实了其鲁棒性和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MDTI框架有效解决了行程时间估计中的挑战，通过整合多种数据源和动态建模显著提高了准确性，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的行程时间估计在智能交通系统中起着关键作用。然而，由于异构数据源和复杂的交通动态，这仍然具有挑战性。此外，传统方法通常将轨迹转换为固定长度的表示，忽略了现实世界轨迹的固有变异性，这往往导致信息丢失或特征冗余。为了应对这些挑战，本文引入了多模态动态轨迹集成（MDTI）框架——一种新颖的多模态轨迹表示学习方法，它集成GPS序列、网格轨迹和道路网络约束以提高TTE准确性。MDTI采用模态特定编码器和跨模态交互模块来捕获互补的空间、时间和拓扑语义，同时动态轨迹建模机制自适应调节不同长度轨迹的信息密度。两种名为对比对齐和掩码语言建模的自监督预训练目标进一步加强了多模态一致性和上下文理解。在三个真实世界数据集上的大量实验表明，MDTI始终优于最先进的基线方法，证实了其鲁棒性和强大的泛化能力。代码已在https://github.com/freshhxy/MDTI/公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate travel time estimation (TTE) plays a crucial role in intelligenttransportation systems. However, it remains challenging due to heterogeneousdata sources and complex traffic dynamics. Moreover, conventional approachestypically convert trajectories into fixed-length representations, neglectingthe inherent variability of real-world trajectories, which often leads toinformation loss or feature redundancy. To address these challenges, this paperintroduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--anovel multimodal trajectory representation learning approach that integratesGPS sequences, grid trajectories, and road network constraints to enhance TTEaccuracy. MDTI employs modality-specific encoders and a cross-modal interactionmodule to capture complementary spatial, temporal, and topological semantics,while a dynamic trajectory modeling mechanism adaptively regulates informationdensity for trajectories of varying lengths. Two self-supervised pretrainingobjectives, named contrastive alignment and masked language modeling, furtherstrengthen multimodal consistency and contextual understanding. Extensiveexperiments on three real-world datasets demonstrate that MDTI consistentlyoutperforms state-of-the-art baselines, confirming its robustness and stronggeneralization abilities. The code is publicly available at:https://github.com/freshhxy/MDTI/</description>
      <author>example@mail.com (Zhi Liu, Xuyuan Hu, Xiao Han, Zhehao Dai, Zhaolin Deng, Guojiang Shen, Xiangjie Kong)</author>
      <guid isPermaLink="false">2510.05840v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities</title>
      <link>http://arxiv.org/abs/2510.05717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为扩散序列解缠结自编码器(DiffSDA)的新框架，用于解决无监督表征学习中的序列解缠结问题，在多种真实世界数据模态上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;无监督表征学习特别是序列解缠结旨在分离数据中的静态和动态变化因素而不依赖标签。现有基于变分自编码器和生成对抗网络的方法存在优化复杂、应用于真实世界数据效果不佳以及缺乏成熟评估协议等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架来解决序列解缠结问题，使其能够有效应用于多种真实世界数据模态，并提供严格的评估协议。&lt;h4&gt;方法&lt;/h4&gt;提出DiffSDA框架，这是一种新颖的、与模态无关的方法，利用新的概率建模、潜在扩散和高效采样器，同时引入具有挑战性的评估协议进行严格测试。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的真实世界基准测试中，DiffSDA优于现有的最先进序列解缠结方法，证明了其在时间序列、视频和音频等多种数据模态上的有效性。&lt;h4&gt;结论&lt;/h4&gt;DiffSDA为序列解缠结问题提供了有效的解决方案，填补了扩散模型在这一应用领域的理论空白，并建立了新的评估标准。&lt;h4&gt;翻译&lt;/h4&gt;无监督表征学习，特别是序列解缠结，旨在不依赖标签的情况下分离数据中的静态和动态变化因素。这仍然是一个具有挑战性的问题，因为基于变分自编码器和生成对抗网络的现有方法通常依赖多个损失项，使优化过程复杂化。此外，序列解缠结方法应用于真实世界数据时面临挑战，目前还没有既定的评估协议来评估它们在这种情况下性能。最近，扩散模型已成为最先进的生成模型，但其在序列解缠结应用方面尚无理论形式化。在这项工作中，我们引入了扩散序列解缠结自编码器(DiffSDA)，这是一个新颖的、与模态无关的框架，在包括时间序列、视频和音频在内的各种真实世界数据模态中有效。DiffSDA利用新的概率建模、潜在扩散和高效采样器，同时纳入具有挑战性的评估协议进行严格测试。我们在各种真实世界基准上的实验证明，DiffSDA在序列解缠结方面优于最近的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised representation learning, particularly sequentialdisentanglement, aims to separate static and dynamic factors of variation indata without relying on labels. This remains a challenging problem, as existingapproaches based on variational autoencoders and generative adversarialnetworks often rely on multiple loss terms, complicating the optimizationprocess. Furthermore, sequential disentanglement methods face challenges whenapplied to real-world data, and there is currently no established evaluationprotocol for assessing their performance in such settings. Recently, diffusionmodels have emerged as state-of-the-art generative models, but no theoreticalformalization exists for their application to sequential disentanglement. Inthis work, we introduce the Diffusion Sequential Disentanglement Autoencoder(DiffSDA), a novel, modal-agnostic framework effective across diversereal-world data modalities, including time series, video, and audio. DiffSDAleverages a new probabilistic modeling, latent diffusion, and efficientsamplers, while incorporating a challenging evaluation protocol for rigoroustesting. Our experiments on diverse real-world benchmarks demonstrate thatDiffSDA outperforms recent state-of-the-art methods in sequentialdisentanglement.</description>
      <author>example@mail.com (Hedi Zisling, Ilan Naiman, Nimrod Berman, Supasorn Suwajanakorn, Omri Azencot)</author>
      <guid isPermaLink="false">2510.05717v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</title>
      <link>http://arxiv.org/abs/2510.05535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在联邦学习场景下进行特征选择的创新框架，解决了现有方法在特征交互捕捉、场景适应性、排列敏感性和凸性假设限制等方面的问题，同时处理了数据不平衡、异构性和隐私保护挑战。&lt;h4&gt;背景&lt;/h4&gt;现有特征选择方法难以捕捉复杂特征交互且适应性差；近期采用生成智能的方法仍受排列敏感性和凸性假设限制；实际分布式环境中数据高度不平衡、异构且受隐私法规限制，无法直接共享。&lt;h4&gt;目的&lt;/h4&gt;开发隐私保护的知识融合策略，在不共享敏感原始数据的情况下推导统一表示空间；引入样本感知的加权策略，解决异构本地客户端间的分布不平衡问题，提高特征选择在联邦学习中的效果。&lt;h4&gt;方法&lt;/h4&gt;提出整合排列不变嵌入与策略引导搜索的新框架；在期刊扩展版本中增加两方面改进：1)隐私保护的知识融合策略；2)样本感知的加权策略，以处理数据不平衡和异构性问题。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验验证了框架的有效性、鲁棒性和效率；结果表明该方法在联邦学习场景中具有强大的泛化能力，能够有效集成客户端间的特征选择知识而不暴露敏感信息。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架成功解决了分布式场景中的数据不平衡、异构性和隐私保护问题，为联邦学习环境下的特征选择提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;特征选择消除特征间的冗余，以提高下游任务性能同时减少计算开销。现有方法通常难以捕捉复杂的特征交互并适应多样化的应用场景。最近的进展采用生成智能来缓解这些缺点。然而，这些方法仍受嵌入中的排列敏感性和基于梯度搜索对凸性假设的依赖限制。为解决这些局限性，我们初步工作引入了一种整合排列不变嵌入与策略引导搜索的新框架。尽管有效，它仍存在适应真实分布式场景的改进空间。实际上，本地客户端的数据高度不平衡、异构，且受严格隐私法规限制，限制了直接共享。这些挑战凸显了需要一种能够在不暴露敏感信息的情况下集成客户端间特征选择知识的框架。在本扩展期刊版本中，我们从两个角度推进框架：1)开发隐私保护的知识融合策略，在不共享敏感原始数据的情况下推导统一表示空间；2)引入样本感知的加权策略，解决异构本地客户端间的分布不平衡问题。大量实验验证了我们框架的有效性、鲁棒性和效率。结果进一步证明了其在联邦学习场景中的强大泛化能力。代码和数据公开可用：https://anonymous.4open.science/r/FedCAPS-08BF。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature selection eliminates redundancy among features to improve downstreamtask performance while reducing computational overhead. Existing methods oftenstruggle to capture intricate feature interactions and adapt across diverseapplication scenarios. Recent advances employ generative intelligence toalleviate these drawbacks. However, these methods remain constrained bypermutation sensitivity in embedding and reliance on convexity assumptions ingradient-based search. To address these limitations, our initial workintroduces a novel framework that integrates permutation-invariant embeddingwith policy-guided search. Although effective, it still left opportunities toadapt to realistic distributed scenarios. In practice, data across localclients is highly imbalanced, heterogeneous and constrained by strict privacyregulations, limiting direct sharing. These challenges highlight the need for aframework that can integrate feature selection knowledge across clients withoutexposing sensitive information. In this extended journal version, we advancethe framework from two perspectives: 1) developing a privacy-preservingknowledge fusion strategy to derive a unified representation space withoutsharing sensitive raw data. 2) incorporating a sample-aware weighting strategyto address distributional imbalance among heterogeneous local clients.Extensive experiments validate the effectiveness, robustness, and efficiency ofour framework. The results further demonstrate its strong generalizationability in federated learning scenarios. The code and data are publiclyavailable: https://anonymous.4open.science/r/FedCAPS-08BF.</description>
      <author>example@mail.com (Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, Dongjie Wang)</author>
      <guid isPermaLink="false">2510.05535v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement</title>
      <link>http://arxiv.org/abs/2510.05295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AUREXA-SE（使用交叉注意力和Squeezeformer进行语音增强的视听统一表示交换架构），这是一个专门为视听语音增强设计的渐进式双模态框架。该模型通过结合音频和视觉信息，利用交叉注意力机制和Squeezeformer块，显著提高了嘈杂环境下的语音质量和可理解性。&lt;h4&gt;背景&lt;/h4&gt;语音增强是语音处理领域的重要任务，特别是在嘈杂环境下提高语音质量和可理解性。视听语音增强（AVSE）利用视觉信息（如说话者的口型）来辅助语音增强，这在嘈杂环境或低信噪比情况下尤为重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个先进的视听语音增强框架，有效结合音频和视觉信息，提高嘈杂环境下的语音质量和可理解性。&lt;h4&gt;方法&lt;/h4&gt;1. 使用基于U-Net的一维卷积编码器处理原始音频波形；2. 采用Swin Transformer V2进行视觉特征提取；3. 设计双向交叉注意力机制促进模态间深度上下文融合；4. 引入轻量级Squeezeformer块捕获融合嵌入中的时间依赖关系；5. 通过U-Net风格解码器进行波形重建。&lt;h4&gt;主要发现&lt;/h4&gt;1. 实验证明AUREXA-SE在嘈杂基线上有显著性能提升；2. 具体性能指标：STOI为0.516，PESQ为1.323，SI-SDR为-4.322 dB；3. 模型能够产生感知一致且可理解的语音输出。&lt;h4&gt;结论&lt;/h4&gt;AUREXA-SE是一个有效的视听语音增强框架，通过结合音频和视觉信息，利用交叉注意力机制和Squeezeformer块，显著提高了嘈杂环境下的语音质量和可理解性。该模型在多个评估指标上均表现出色，证明了其作为语音增强解决方案的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了AUREXA-SE（使用交叉注意力和Squeezeformer进行语音增强的视听统一表示交换架构），这是一个专门为视听语音增强设计的渐进式双模态框架。AUREXA-SE通过使用基于U-Net的一维卷积编码器处理音频，以及使用Swin Transformer V2进行高效且表达性强的视觉特征提取，共同利用原始音频波形和视觉线索。该架构的核心是一个新颖的双向交叉注意力机制，它促进了模态间的深度上下文融合，实现了丰富且互补的表示学习。为了捕获融合嵌入中的时间依赖关系，引入了一系列轻量级Squeezeformer块，结合了卷积和注意力模块。增强的嵌入随后通过U-Net风格解码器进行解码，直接进行波形重建，确保感知一致且可理解的语音输出。实验评估证明了AUREXA-SE的有效性，与嘈杂基线相比实现了显著的性能提升，STOI为0.516，PESQ为1.323，SI-SDR为-4.322 dB。AUREXA-SE的源代码可在https://github.com/mtanveer1/AVSEC-4-Challenge-2025获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose AUREXA-SE (Audio-Visual Unified RepresentationExchange Architecture with Cross-Attention and Squeezeformer for SpeechEnhancement), a progressive bimodal framework tailored for audio-visual speechenhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visualcues by employing a U-Net-based 1D convolutional encoder for audio and a SwinTransformer V2 for efficient and expressive visual feature extraction. Centralto the architecture is a novel bidirectional cross-attention mechanism, whichfacilitates deep contextual fusion between modalities, enabling rich andcomplementary representation learning. To capture temporal dependencies withinthe fused embeddings, a stack of lightweight Squeezeformer blocks combiningconvolutional and attention modules is introduced. The enhanced embeddings arethen decoded via a U-Net-style decoder for direct waveform reconstruction,ensuring perceptually consistent and intelligible speech output. Experimentalevaluations demonstrate the effectiveness of AUREXA-SE, achieving significantperformance improvements over noisy baselines, with STOI of 0.516, PESQ of1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available athttps://github.com/mtanveer1/AVSEC-4-Challenge-2025.</description>
      <author>example@mail.com (M. Sajid, Deepanshu Gupta, Yash Modi, Sanskriti Jain, Harshith Jai Surya Ganji, A. Rahaman, Harshvardhan Choudhary, Nasir Saleem, Amir Hussain, M. Tanveer)</author>
      <guid isPermaLink="false">2510.05295v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Provable Speech Attributes Conversion via Latent Independence</title>
      <link>http://arxiv.org/abs/2510.05191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的语音属性转换框架，通过理论分析和保证解决了现有方法缺乏理论基础的问题&lt;h4&gt;背景&lt;/h4&gt;信号转换和解缠表示学习在音频、图像和多模态生成等领域显示出潜力，但现有语音风格转换方法大多是经验性的，缺乏严格的理论基础来保证可靠和可解释的控制&lt;h4&gt;目的&lt;/h4&gt;提出一个通用的语音属性转换框架，并在合理假设下提供理论分析和保证&lt;h4&gt;方法&lt;/h4&gt;构建基于非概率自编码器架构的框架，在预测的潜变量和目标可控变量之间施加独立性约束，确保在观察到风格变量条件下的信号转换一致性，同时保留原始内容并修改所需属性&lt;h4&gt;主要发现&lt;/h4&gt;通过在说话人身份和情感等语音风格上的评估，证明了该方法的通用性；定量评估证实了所提出方法的有效性和通用性&lt;h4&gt;结论&lt;/h4&gt;该框架为语音属性转换提供了理论基础，确保了可靠和可解释的控制&lt;h4&gt;翻译&lt;/h4&gt;尽管信号转换和解缠表示学习在音频、图像和多模态生成等跨领域的数据属性操作方面显示出前景，但现有方法，特别是语音风格转换方面，大多是经验性的，缺乏严格的理论基础来保证可靠和可解释的控制。在这项工作中，我们提出了一个语音属性转换的通用框架，并在合理假设下提供了理论分析和保证。我们的框架构建在一个非概率自编码器架构上，预测的潜变量与目标可控变量之间具有独立性约束。这种设计确保了在观察到风格变量条件下的信号转换一致性，同时保留原始内容并修改所需属性。我们通过在语音风格（包括说话人身份和情感）上的评估进一步证明了我们方法的通用性。定量评估证实了所提出方法的有效性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While signal conversion and disentangled representation learning have shownpromise for manipulating data attributes across domains such as audio, image,and multimodal generation, existing approaches, especially for speech styleconversion, are largely empirical and lack rigorous theoretical foundations toguarantee reliable and interpretable control. In this work, we propose ageneral framework for speech attribute conversion, accompanied by theoreticalanalysis and guarantees under reasonable assumptions. Our framework builds on anon-probabilistic autoencoder architecture with an independence constraintbetween the predicted latent variable and the target controllable variable.This design ensures a consistent signal transformation, conditioned on anobserved style variable, while preserving the original content and modifyingthe desired attribute. We further demonstrate the versatility of our method byevaluating it on speech styles, including speaker identity and emotion.Quantitative evaluations confirm the effectiveness and generality of theproposed approach.</description>
      <author>example@mail.com (Jonathan Svirsky, Ofir Lindenbaum, Uri Shaham)</author>
      <guid isPermaLink="false">2510.05191v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach</title>
      <link>http://arxiv.org/abs/2510.05661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态自动视频编辑方法，特别针对古典音乐会多摄像头录像的编辑问题，将任务分解为'何时剪切'和'如何剪切'两个子任务，并在检测剪切点和视觉镜头选择方面取得了优于先前基线的结果。&lt;h4&gt;背景&lt;/h4&gt;自动视频编辑在计算机视觉和多媒体领域是一个探索不足的任务，与日益增长的视频生成和场景理解研究兴趣相比，视频编辑的研究相对较少。&lt;h4&gt;目的&lt;/h4&gt;解决多摄像头古典音乐会录像编辑的具体挑战，通过分解问题为两个关键子任务：何时剪切和如何剪切，以推进多模态自动视频编辑的技术水平。&lt;h4&gt;方法&lt;/h4&gt;为时间分割任务提出了一种新的多模态架构，整合音频信号的log-mel频谱图、可选图像嵌入和标量时间特征，通过轻量级卷积-Transformer管道处理；为空间选择任务使用基于CLIP的编码器替换旧的骨干网络，并将干扰项选择限制在同一音乐会的片段中；采用伪标记方法构建数据集，将原始视频数据自动聚类为连贯的镜头片段。&lt;h4&gt;主要发现&lt;/h4&gt;模型在检测剪切点方面优于之前的基线方法，同时提供了具有竞争力的视觉镜头选择能力。&lt;h4&gt;结论&lt;/h4&gt;该研究成功推进了多模态自动视频编辑领域的最先进技术，为自动视频编辑提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动视频编辑在计算机视觉和多媒体领域仍然是一个探索不足的任务，特别是与日益增长的视频生成和场景理解研究兴趣相比。在本研究中，我们通过将问题分解为两个关键子任务来解决古典音乐会多摄像头录像编辑的具体挑战：何时剪切和如何剪切。基于近期文献，我们为时间分割任务(何时剪切)提出了一种新的多模态架构，该架构通过轻量级卷积-Transformer管道整合了音频信号的log-mel频谱图、可选图像嵌入和标量时间特征。对于空间选择任务(如何剪切)，我们通过使用基于CLIP的编码器替换旧的骨干网络(如ResNet)并限制干扰项选择来自同一音乐会的片段来改进现有文献。我们的数据集采用伪标记方法构建，将原始视频数据自动聚类为连贯的镜头片段。我们证明，我们的模型在检测剪切点方面优于之前的基线，并提供了具有竞争力的视觉镜头选择，推进了多模态自动视频编辑的最先进技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746278.3759387&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated video editing remains an underexplored task in the computer visionand multimedia domains, especially when contrasted with the growing interest invideo generation and scene understanding. In this work, we address the specificchallenge of editing multicamera recordings of classical music concerts bydecomposing the problem into two key sub-tasks: when to cut and how to cut.Building on recent literature, we propose a novel multimodal architecture forthe temporal segmentation task (when to cut), which integrates log-melspectrograms from the audio signals, plus an optional image embedding, andscalar temporal features through a lightweight convolutional-transformerpipeline. For the spatial selection task (how to cut), we improve theliterature by updating from old backbones, e.g. ResNet, with a CLIP-basedencoder and constraining distractor selection to segments from the sameconcert. Our dataset was constructed following a pseudo-labeling approach, inwhich raw video data was automatically clustered into coherent shot segments.We show that our models outperformed previous baselines in detecting cut pointsand provide competitive visual shot selection, advancing the state of the artin multimodal automated video editing.</description>
      <author>example@mail.com (Daniel Gonzálbez-Biosca, Josep Cabacas-Maso, Carles Ventura, Ismael Benito-Altamirano)</author>
      <guid isPermaLink="false">2510.05661v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video</title>
      <link>http://arxiv.org/abs/2510.05560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://xiahongchi.github.io/HoloScene&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HoloScene是一种新颖的交互式3D重建框架，解决了现有方法在几何完整性、对象交互性、物理合理性、照片级真实感渲染和动态模拟物理属性方面的局限性，实现了全面的数字孪生重建。&lt;h4&gt;背景&lt;/h4&gt;将物理世界数字化为准确的模拟就绪虚拟环境在增强现实、虚拟现实、游戏和机器人技术等领域有重要应用机会，但当前3D重建和场景理解方法通常在一个或多个关键方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能同时满足几何完整性、对象交互性、物理合理性、照片级真实感渲染和可靠动态模拟物理属性的交互式3D重建框架。&lt;h4&gt;方法&lt;/h4&gt;HoloScene利用全面的交互式场景图表示法编码对象几何、外观和物理属性以及层次间和对象间关系；将重建表述为基于能量的优化问题，整合观测数据、物理约束和生成先验；通过结合基于采样的探索和基于梯度的细化的混合方法高效执行优化。&lt;h4&gt;主要发现&lt;/h4&gt;生成的数字孪生表现出完整精确的几何、物理稳定性和新视角的真实感渲染；在多个基准数据集上评估显示优越性能；交互式游戏和实时数字孪生操作的实际用例证明了其广泛适用性和有效性。&lt;h4&gt;结论&lt;/h4&gt;HoloScene成功解决了现有3D重建方法的局限性，实现了多种关键要求的统一，为物理世界的数字化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将物理世界数字化为准确的模拟就绪虚拟环境在增强现实、虚拟现实、游戏和机器人技术等多个领域提供了重要机会。然而，当前的3D重建和场景理解方法通常在一个或多个关键方面存在不足，如几何完整性、对象交互性、物理合理性、照片级真实感渲染或可靠动态模拟所需的物理属性。为解决这些局限性，我们引入了HoloScene，一种新颖的交互式3D重建框架，可同时满足这些要求。HoloScene利用全面的交互式场景图表示法，编码对象几何、外观和物理属性以及层次间和对象间关系。重建被表述为基于能量的优化问题，将观测数据、物理约束和生成先验整合到统一、连贯的目标中。优化通过结合基于采样的探索和基于梯度的细化的混合方法高效执行。生成的数字孪生表现出完整且精确的几何、物理稳定性和从新视角的真实感渲染。在多个基准数据集上进行的评估展示了优越性能，而交互式游戏和实时数字孪生操作的实际用例说明了HoloScene的广泛适用性和有效性。项目页面：https://xiahongchi.github.io/HoloScene。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从单个视频中重建出完整的、物理上合理的、可交互的3D数字孪生环境的问题。这个问题在现实中非常重要，因为数字化物理世界为增强现实、虚拟现实、游戏和机器人等领域提供了巨大机会，而现有方法无法同时满足几何完整性、物理合理性和交互性的需求，限制了数字孪生技术在机器人学习、自动驾驶和内容创作等领域的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有3D重建方法在几何完整性、物理合理性和交互性方面的不足，特别是在处理遮挡区域、推断对象间关系和确保物理稳定性方面存在挑战。他们分析了现有Real2Sim方法、Amodal重建和物理合理重建的局限性，然后设计了基于交互式场景图表示的方法，将场景图重建制定为基于能量的优化问题，整合观测数据、物理约束和生成先验。该方法借鉴了神经SDF表示几何形状、3D Gaussian Splatting技术进行渲染、图像到3D生成方法完成遮挡区域以及可微分物理和模拟工作确保稳定性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将场景表示为一个交互式3D场景图，编码对象几何、外观、物理属性以及层次化的对象间关系，通过基于能量的优化方法同时实现几何完整性、物理合理性和交互性。整体实现流程分为三个阶段：1) 基于梯度的优化初始化，优化对象几何和外观使其与观测数据匹配；2) 基于采样的优化，通过生成采样和树搜索完善形状和物理参数；3) 基于梯度的细化，精调高斯斑点确保真实外观。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：统一的交互式3D重建框架首次同时实现几何完整性、物理合理性和交互性；交互式场景图表示支持物理交互；基于能量的优化方法整合观测数据、物理约束和生成先验；混合优化策略结合采样和梯度细化；生成先验与物理约束的结合。相比之前工作，HoloScene执行多视图联合优化而非单图像生成；使用循环模拟器确保长期物理稳定性而非仅避免穿透；采用统一的基于能量公式而非多阶段管道；能够处理复杂室内场景中的对象间遮挡和交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HoloScene提出了一种创新的交互式3D重建框架，通过结合场景图表示、基于能量的优化和混合推理策略，首次实现了从单个视频中重建出几何完整、物理合理且可交互的3D数字孪生环境。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digitizing the physical world into accurate simulation-ready virtualenvironments offers significant opportunities in a variety of fields such asaugmented and virtual reality, gaming, and robotics. However, current 3Dreconstruction and scene-understanding methods commonly fall short in one ormore critical aspects, such as geometry completeness, object interactivity,physical plausibility, photorealistic rendering, or realistic physicalproperties for reliable dynamic simulation. To address these limitations, weintroduce HoloScene, a novel interactive 3D reconstruction framework thatsimultaneously achieves these requirements. HoloScene leverages a comprehensiveinteractive scene-graph representation, encoding object geometry, appearance,and physical properties alongside hierarchical and inter-object relationships.Reconstruction is formulated as an energy-based optimization problem,integrating observational data, physical constraints, and generative priorsinto a unified, coherent objective. Optimization is efficiently performed via ahybrid approach combining sampling-based exploration with gradient-basedrefinement. The resulting digital twins exhibit complete and precise geometry,physical stability, and realistic rendering from novel viewpoints. Evaluationsconducted on multiple benchmark datasets demonstrate superior performance,while practical use-cases in interactive gaming and real-time digital-twinmanipulation illustrate HoloScene's broad applicability and effectiveness.Project page: https://xiahongchi.github.io/HoloScene.</description>
      <author>example@mail.com (Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang)</author>
      <guid isPermaLink="false">2510.05560v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Sci-Phi: A Large Language Model Spatial Audio Descriptor</title>
      <link>http://arxiv.org/abs/2510.05542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Sci-Phi，一个能够全面描述声景的空间音频大型语言模型，可以估计声源和环境的完整参数集。&lt;h4&gt;背景&lt;/h4&gt;声景感知涉及描述声音的类型、时间、方向、距离、响度和混响。虽然音频语言模型在声音识别方面表现出色，但单通道输入限制了空间理解能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行全面空间场景描述的音频大型语言模型，克服单通道输入的空间理解限制。&lt;h4&gt;方法&lt;/h4&gt;提出了Sci-Phi，一个具有双空间和频谱编码器的空间音频大型语言模型，从超过4,000小时的合成一阶Ambisonics录音中学习，能够估计所有声源和周围环境的完整参数集。&lt;h4&gt;主要发现&lt;/h4&gt;Sci-Phi能够一次列举并描述多达四个方向性声源，以及非方向性背景声音和房间特性；在15个指标(涵盖内容、位置、时间、响度和混响)的评估中表现出色；能够推广到真实的房间脉冲响应，性能只有轻微下降。&lt;h4&gt;结论&lt;/h4&gt;Sci-Phi是第一个能够进行全面空间场景描述的音频LLM，具有强大的实际部署潜力。&lt;h4&gt;翻译&lt;/h4&gt;声景感知涉及描述声音的类型、时间、方向和距离，以及它们的响度和混响。虽然音频语言模型在声音识别方面表现出色，但单通道输入从根本上限制了空间理解。这项工作提出了Sci-Phi，一个具有双空间和频谱编码器的空间音频大型语言模型，可以估计所有声源和周围环境的完整参数集。从超过4,000小时的合成一阶Ambisonics录音(包括元数据)中学习，Sci-Phi一次列举并描述多达四个方向性声源，以及非方向性背景声音和房间特性。我们使用排列不变协议和15个指标(涵盖内容、位置、时间、响度和混响)评估了该模型，并分析了其在源数量、信噪比、混响水平以及声学、空间或时间上相似的源的挑战性混合物方面的鲁棒性。值得注意的是，Sci-Phi能够推广到真实的房间脉冲响应，性能只有轻微下降。总的来说，这项工作建立了第一个能够进行全面空间场景描述的音频LLM，具有强大的实际部署潜力。演示：https://sci-phi-audio.github.io/demo&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic scene perception involves describing the type of sounds, theirtiming, their direction and distance, as well as their loudness andreverberation. While audio language models excel in sound recognition,single-channel input fundamentally limits spatial understanding. This workpresents Sci-Phi, a spatial audio large language model with dual spatial andspectral encoders that estimates a complete parameter set for all sound sourcesand the surrounding environment. Learning from over 4,000 hours of syntheticfirst-order Ambisonics recordings including metadata, Sci-Phi enumerates anddescribes up to four directional sound sources in one pass, alongsidenon-directional background sounds and room characteristics. We evaluate themodel with a permutation-invariant protocol and 15 metrics covering content,location, timing, loudness, and reverberation, and analyze its robustnessacross source counts, signal-to-noise ratios, reverberation levels, andchallenging mixtures of acoustically, spatially, or temporally similar sources.Notably, Sci-Phi generalizes to real room impulse responses with only minorperformance degradation. Overall, this work establishes the first audio LLMcapable of full spatial-scene description, with strong potential for real-worlddeployment. Demo: https://sci-phi-audio.github.io/demo</description>
      <author>example@mail.com (Xilin Jiang, Hannes Gamper, Sebastian Braun)</author>
      <guid isPermaLink="false">2510.05542v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy &amp; Astrophysics (IOAA)</title>
      <link>http://arxiv.org/abs/2510.05016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures, to be submitted, comments are welcome.  Reproducibility details can be found at:  https://github.com/OSU-NLP-Group/LLM-IOAA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过国际天文和天体物理学奥林匹克竞赛(IOAA)考试系统评估了五种最先进的大型语言模型，发现Gemini 2.5 Pro和GPT-5在理论考试中达到金牌水平，接近人类顶尖表现，但在数据分析考试中表现差异较大，所有模型在概念推理、几何推理和空间可视化方面存在明显弱点。&lt;h4&gt;背景&lt;/h4&gt;当前基于特定任务演示的方法在将大型语言模型应用于自动化天文研究任务方面显示出早期成功，但仅提供了解决问题所需能力的不完整视图。现有的基准测试和评估主要集中在简单问答上，主要测试天文知识，未能评估现实研究中所需的复杂推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的局限性，通过IOAA考试系统评估大型语言模型在深入概念理解、多步推导和多模式分析方面的能力，以更全面地理解LLM的优势和局限性。&lt;h4&gt;方法&lt;/h4&gt;使用国际天文和天体物理学奥林匹克竞赛(IOAA)考试对五种最先进的大型语言模型进行系统基准测试，考试包括理论和数据分析部分，涵盖2022-2025年的四届比赛。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini 2.5 Pro和GPT-5分别获得85.6%和84.2%的理论考试平均分，达到金牌水平且排名参赛者前两名；在数据分析考试中，GPT-5平均得分为88.5%排名前10，其他模型性能下降至48-76%；所有模型在概念推理、几何推理和空间可视化方面准确率仅为52-79%，存在明显弱点。&lt;h4&gt;结论&lt;/h4&gt;尽管大型语言模型在理论考试中接近顶尖人类表现，但在它们能够作为天文学自主研究代理之前，必须解决概念推理、几何推理和空间可视化等关键能力差距。&lt;h4&gt;翻译&lt;/h4&gt;虽然特定任务的演示在将大型语言模型应用于自动化某些天文研究任务方面显示出早期成功，但它们仅提供了解决天文问题所需能力的片面视图，需要更全面地理解大型语言模型的优势和局限性。迄今为止，现有的基准测试和评估主要集中在简单的问答上，主要测试天文知识，未能评估该学科现实研究所需的复杂推理。在这里，我们通过在国际天文和天体物理学奥林匹克竞赛(IOAA)考试上系统性地对五种最先进的大型语言模型进行基准测试来解决这一差距，这些考试旨在检验深入的概念理解、多步推导和多模式分析能力。凭借85.6%和84.2%的平均分，Gemini 2.5 Pro和GPT-5（两个表现最佳的模型）不仅达到金牌水平，而且在所有四个IOAA理论考试（2022-2025年）中排名在前200-300名参赛者中的前两名。相比之下，数据分析考试的结果显示出更大的差异。GPT-5在这些考试中仍然表现出色，平均得分为88.5%，在最近四届IOAA中排名前10，而其他模型的性能下降到48-76%。此外，我们深入的错误分析强调概念推理、几何推理和空间可视化（52-79%的准确率）是所有大型语言模型的一致弱点。因此，尽管大型语言模型在理论考试中接近顶尖人类表现，但在它们能够作为天文学自主研究代理之前，必须解决关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While task-specific demonstrations show early success in applying largelanguage models (LLMs) to automate some astronomical research tasks, they onlyprovide incomplete views of all necessary capabilities in solving astronomyproblems, calling for more thorough understanding of LLMs' strengths andlimitations. So far, existing benchmarks and evaluations focus on simplequestion-answering that primarily tests astronomical knowledge and fails toevaluate the complex reasoning required for real-world research in thediscipline. Here, we address this gap by systematically benchmarking fivestate-of-the-art LLMs on the International Olympiad on Astronomy andAstrophysics (IOAA) exams, which are designed to examine deep conceptualunderstanding, multi-step derivations, and multimodal analysis. With averagescores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performingmodels) not only achieve gold medal level performance but also rank in the toptwo among ~200-300 participants in all four IOAA theory exams evaluated(2022-2025). In comparison, results on the data analysis exams show moredivergence. GPT-5 still excels in the exams with an 88.5% average score,ranking top 10 among the participants in the four most recent IOAAs, whileother models' performances drop to 48-76%. Furthermore, our in-depth erroranalysis underscores conceptual reasoning, geometric reasoning, and spatialvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,although LLMs approach peak human performance in theory exams, critical gapsmust be addressed before they can serve as autonomous research agents inastronomy.</description>
      <author>example@mail.com (Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun)</author>
      <guid isPermaLink="false">2510.05016v2</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</title>
      <link>http://arxiv.org/abs/2510.04704v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AtomWorld基准测试，用于评估大型语言模型在处理晶体学信息文件(CIFs)相关任务时的能力，特别是在材料科学领域的原子结构理解和空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在文本推理方面表现出色，并开始发展空间理解能力。在材料科学等领域，对3D原子结构的深入理解是基础，但缺乏标准化的基准来系统评估LLMs在多样化原子结构上的核心推理能力。&lt;h4&gt;目的&lt;/h4&gt;引入AtomWorld基准测试，用于评估LLMs基于晶体学信息文件(CIFs)的任务表现，CIFs是一种标准的结构表示格式。&lt;h4&gt;方法&lt;/h4&gt;开发了AtomWorld基准测试，包含基于CIFs的任务，如结构编辑、CIF感知和属性引导建模。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型虽然建立了有希望的基线，但在结构理解和空间推理方面存在明显局限。这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解方面也存在问题，可能导致后续分析和材料见解中的累积错误。&lt;h4&gt;结论&lt;/h4&gt;通过定义这些标准化任务，AtomWorld为推进LLMs向稳健的原子级建模发展奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在文本推理方面表现出色，并开始发展空间理解能力，这引发了一个问题：这些能力是否可以结合起来用于复杂、特定领域的任务。这个问题在材料科学等领域至关重要，因为对3D原子结构的深入理解是基础。虽然初步研究已成功将LLMs应用于涉及纯晶体生成或坐标理解的任务，但缺乏一个标准化的基准来系统评估它们在多样化原子结构上的核心推理能力。为解决这一差距，我们引入了AtomWorld基准测试，用于评估LLMs基于晶体学信息文件(CIFs)的任务表现，CIFs是一种标准的结构表示格式。这些任务包括结构编辑、CIF感知和属性引导建模，揭示了一个关键局限：当前模型尽管建立了有希望的基线，但在结构理解和空间推理方面持续失败。我们的实验显示，这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解方面也存在问题，可能导致后续分析和材料见解中的累积错误。通过定义这些标准化任务，AtomWorld为推进LLMs向稳健的原子级建模发展奠定了基础，这对加速材料研究和自动化科学工作流程至关重要。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏标准化基准来系统评估大型语言模型在处理晶体材料结构时的空间推理能力问题。这个问题在材料科学领域至关重要，因为深入理解3D原子结构是该领域的基础。当前模型在结构修改任务中经常出错，甚至在基本的CIF格式理解方面也存在问题，可能导致后续分析和材料洞察中的累积错误，影响材料研究的效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将LLMs处理CIF文件的能力分为三个阶段：运动技能（几何操作）、感知技能（识别模式）和认知技能（推理和创造力）。AtomWorld基准专门评估'运动技能'，据称是第一个检查晶体学这一基本能力的基准。作者借鉴了现有的CIF格式标准、传统晶体学软件（如Ovito和ASE）的工作方式，以及LLM4Mat-Bench等问答基准的设计思路，但专注于结构操作而非问答，形成了新的评估框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是AtomWorld作为一个可扩展的数据生成器，通过定义标准化的晶体结构操作任务来评估和训练LLMs的空间推理能力。整体流程包括：1)数据生成，创建'之前'和'之后'的CIF文件及动作提示；2)支持多种结构修改动作；3)评估流程，通过随机采样、初始化、结构操作、提示生成和结果比较来完成对LLM的评估。使用StructureMatcher比较生成结构与目标结构，计算成功率和最大距离等指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门评估LLMs晶体学运动技能的AtomWorld基准；2)设计互补基准(PointWorld、CIF-Gen等)全面评估不同能力维度；3)系统评估前沿模型在原子结构操作上的表现；4)提供可扩展数据生成器支持训练。相比之前工作，AtomWorld专注于实际的结构操作而非生成或问答，不仅评估性能还提供训练框架，并探索了工具增强LLM的潜力，填补了LLMs在材料科学空间推理评估方面的空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AtomWorld基准首次系统评估了大型语言模型在晶体材料空间推理和结构操作方面的能力，揭示了当前模型在理解和修改原子结构时的关键局限性，为未来开发更强大的原子级建模工具奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) excel at textual reasoning and are beginning todevelop spatial understanding, prompting the question of whether theseabilities can be combined for complex, domain-specific tasks. This question isessential in fields like materials science, where deep understanding of 3Datomic structures is fundamental. While initial studies have successfullyapplied LLMs to tasks involving pure crystal generation or coordinateunderstandings, a standardized benchmark to systematically evaluate their corereasoning abilities across diverse atomic structures has been notably absent.To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs ontasks based in Crystallographic Information Files (CIFs), a standard structurerepresentation format. These tasks, including structural editing, CIFperception, and property-guided modeling, reveal a critical limitation: currentmodels, despite establishing promising baselines, consistently fail instructural understanding and spatial reasoning. Our experiments show that thesemodels make frequent errors on structure modification tasks, and even in thebasic CIF format understandings, potentially leading to cumulative errors insubsequent analysis and materials insights. By defining these standardizedtasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scalemodeling, crucial for accelerating materials research and automating scientificworkflows.</description>
      <author>example@mail.com (Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Bram Hoex, Zhicheng Zhong, Tong Xie)</author>
      <guid isPermaLink="false">2510.04704v2</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Deforming Videos to Masks: Flow Matching for Referring Video Segmentation</title>
      <link>http://arxiv.org/abs/2510.06139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowRVS是一种将Referring Video Object Segmentation重新概念化为条件连续流问题的新框架，通过学习从视频整体表示到目标掩码的直接语言引导变形，实现了细粒度像素控制、文本-视频语义对齐和时间一致性，在所有主要RVOS基准测试中取得了新的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;Referring Video Object Segmentation需要根据自然语言描述在视频中分割特定对象，核心挑战是将抽象的语言概念锚定到特定像素并在视频动态中持续分割。先前方法采用'定位然后分割'的级联设计，简化语义为粗略几何提示，难以保持时间一致性。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的根本局限性，提出一种新的框架重新概念化RVOS问题，实现更精确的分割和时间一致性。&lt;h4&gt;方法&lt;/h4&gt;提出FlowRVS框架，将RVOS重新概念化为条件连续流问题，利用预训练T2V模型的固有优势，通过学习从视频整体表示到目标掩码的直接语言引导变形，采用单阶段生成方法而非传统从噪声生成掩码或直接预测掩码。&lt;h4&gt;主要发现&lt;/h4&gt;在所有主要RVOS基准测试中取得新最先进结果，在MeViS上达到51.1的J&amp;F分数（比之前SOTA高1.6），在零样本Ref-DAVIS17上达到73.3（高2.7）。&lt;h4&gt;结论&lt;/h4&gt;证明了将视频理解任务建模为连续变形过程具有巨大潜力，FlowRVS框架有效克服了传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;参考视频对象分割要求根据自然语言描述在视频中分割特定对象。RVOS的核心挑战是将抽象的语言概念锚定到特定的像素集合，并在视频的复杂动态中持续分割它们。面对这一困难，先前的工作通常将任务分解为一个实用的'定位然后分割'流水线。然而，这种级联设计通过将语义简化为粗略的几何提示（如点）创造了信息瓶颈，并且由于分割过程通常与初始语言解耦而难以保持时间一致性。为了克服这些根本限制，我们提出了FlowRVS，一个将RVOS重新概念化为条件连续流问题的新颖框架。这使我们能够利用预训练T2V模型的固有优势、细粒度像素控制、文本-视频语义对齐和时间一致性。我们不是通过从噪声生成掩码或直接预测掩码，而是通过学习从视频的整体表示到其目标掩码的直接语言引导变形来重新表述任务。我们单阶段的生成方法在所有主要的RVOS基准测试中取得了新的最先进结果。具体来说，在MeViS上达到51.1的J&amp;F（比之前SOTA高1.6），在零样本Ref-DAVIS17上达到73.3（高2.7），证明了将视频理解任务建模为连续变形过程的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RVOS) requires segmenting specificobjects in a video guided by a natural language description. The core challengeof RVOS is to anchor abstract linguistic concepts onto a specific set of pixelsand continuously segment them through the complex dynamics of a video. Facedwith this difficulty, prior work has often decomposed the task into a pragmatic`locate-then-segment' pipeline. However, this cascaded design creates aninformation bottleneck by simplifying semantics into coarse geometric prompts(e.g, point), and struggles to maintain temporal consistency as the segmentingprocess is often decoupled from the initial language grounding. To overcomethese fundamental limitations, we propose FlowRVS, a novel framework thatreconceptualizes RVOS as a conditional continuous flow problem. This allows usto harness the inherent strengths of pretrained T2V models, fine-grained pixelcontrol, text-video semantic alignment, and temporal coherence. Instead ofconventional generating from noise to mask or directly predicting mask, wereformulate the task by learning a direct, language-guided deformation from avideo's holistic representation to its target mask. Our one-stage, generativeapproach achieves new state-of-the-art results across all major RVOSbenchmarks. Specifically, achieving a $\mathcal{J}\&amp;\mathcal{F}$ of 51.1 inMeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),demonstrating the significant potential of modeling video understanding tasksas continuous deformation processes.</description>
      <author>example@mail.com (Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang)</author>
      <guid isPermaLink="false">2510.06139v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</title>
      <link>http://arxiv.org/abs/2510.06077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025, Project page:  https://vision.cs.utexas.edu/projects/video-ver/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了视频推理中思维链(CoT)机制的应用问题，发现CoT在视频推理中会导致性能下降，产生'视觉思维漂移'现象，并提出了视觉证据奖励(VER)框架来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;视频推理是让机器通过多步逻辑从动态视觉内容中推断信息的能力，对高级AI至关重要。虽然思维链(CoT)机制已增强基于文本任务的推理，但在视频理解中的应用仍探索不足。&lt;h4&gt;目的&lt;/h4&gt;分析CoT在视频推理中的问题，并提出解决方案以改善视频推理性能。&lt;h4&gt;方法&lt;/h4&gt;引入视觉证据奖励(VER)，一种新型强化学习框架，明确奖励可验证基于视觉证据的推理轨迹生成。&lt;h4&gt;主要发现&lt;/h4&gt;CoT在视频推理中通常会降低性能，产生冗长但误导性的内部独白，导致产生幻觉的视觉细节和覆盖正确的直觉，这种现象称为'视觉思维漂移'。CoT轨迹经常与实际视觉证据偏离，反而放大内部偏见或语言先验。&lt;h4&gt;结论&lt;/h4&gt;Video-VER在10个多样化的视频理解基准上取得了顶尖性能。这项工作揭示了以视频为中心推理的独特挑战，并鼓励开发能将其推断牢固建立在视觉证据上的AI。&lt;h4&gt;翻译&lt;/h4&gt;视频推理是让机器通过多步逻辑从动态视觉内容中推断信息的任务，对高级AI至关重要。虽然思维链(CoT)机制已增强基于文本任务的推理，但在视频理解中的应用仍探索不足。本文进行了系统分析，揭示CoT在视频推理中通常会降低性能，产生冗长但误导性的内部独白，导致产生幻觉的视觉细节和覆盖正确的直觉 - 我们将这种现象称为'视觉思维漂移'。我们通过贝叶斯视角解释这种漂移，认为CoT轨迹经常与实际视觉证据偏离，反而放大内部偏见或语言先验，导致模型讲故事而非进行基于证据的推理。为解决这一问题，我们引入视觉证据奖励(VER)，一种新型强化学习框架，明确奖励可验证基于视觉证据的推理轨迹生成。在10个多样化的视频理解基准上的全面评估表明，我们的Video-VER始终取得顶尖性能。我们的工作揭示了以视频为中心推理的独特挑战，并鼓励开发能将其推断牢固建立在视觉证据上的AI - 对于不仅'在回答前思考'，而且'在思考时看见'的大型多模态模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning, the task of enabling machines to infer from dynamic visualcontent through multi-step logic, is crucial for advanced AI. While theChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,its application to video understanding remains underexplored. This paperpresents a systematic analysis revealing that CoT often degrades performance invideo reasoning, generating verbose but misleading internal monologues, andleading to hallucinated visual details and overridden correct intuitions - aphenomenon we term "visual thinking drift". We explain this drift through aBayesian lens, positing that CoT traces often diverge from actual visualevidence, instead amplifying internal biases or language priors, causing modelsto storytell rather than engage in grounded reasoning. To counteract this, weintroduce Visual Evidence Reward (VER), a novel reinforcement learningframework that explicitly rewards the generation of reasoning traces that areverifiably grounded in visual evidence. Comprehensive evaluation across 10diverse video understanding benchmarks demonstrates that our Video-VERconsistently achieves top performance. Our work sheds light on the distinctchallenges of video-centric reasoning and encourages the development of AI thatrobustly grounds its inferences in visual evidence - for large multimodalmodels that not only "think before answering", but also "see while thinking".</description>
      <author>example@mail.com (Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman)</author>
      <guid isPermaLink="false">2510.06077v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</title>
      <link>http://arxiv.org/abs/2510.06040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VideoMiner的方法，用于解决长视频理解中的关键挑战，并引入了T-GRPO强化学习技术来精确定位关键帧。&lt;h4&gt;背景&lt;/h4&gt;理解长时间视频对于以人为中心的AI应用很重要，但使用多模态大语言模型进行端到端视频理解时，均匀采样视频帧会导致模型被大量无关信息淹没。现有的分层关键帧提取方法虽提高了准确性，但仍面临两个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键挑战：1) 如何减轻长视频中大量冗余信息的干扰；2) 如何让模型动态适应复杂的分层结构，同时准确识别关键帧。&lt;h4&gt;方法&lt;/h4&gt;提出VideoMiner方法，迭代地分割、标注和聚类长视频，形成分层树结构，从长视频到事件再到帧，同时保持时间连贯性。引入T-GRPO（基于树的组相对策略优化）强化学习方法，专为树结构设计，在事件级别整合时空信息，同时受问题引导。&lt;h4&gt;主要发现&lt;/h4&gt;在所有长视频理解任务中取得了优越的性能；T-GRPO意外地激励模型自发生成推理链；设计的树生长素动态调整扩展深度，获得准确性和效率提升。&lt;h4&gt;结论&lt;/h4&gt;VideoMiner和T-GRPO方法有效解决了长视频理解中的关键挑战，相关代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;理解长时间视频与多模态大语言模型(MM-LLMs)丰富了以人为中心的AI应用前景。然而，对于使用LLM进行端到端视频理解，随着视频长度增加，均匀采样视频帧会导致LLM被大量无关信息淹没。现有的分层关键帧提取方法提高了视频理解准确性，但仍面临两个关键挑战：1) 如何减轻长视频中大量冗余信息的干扰？2) 如何让模型动态适应复杂的分层结构，同时准确识别关键帧？为解决这些问题，我们提出了VideoMiner，它迭代地分割、标注和聚类长视频，形成分层树结构。所提出的VideoMiner从长视频到事件再到帧，同时保持时间连贯性，有效解决了第一个挑战。为了精确定位关键帧，我们引入了T-GRPO，一种基于树的组相对策略优化强化学习方法，该方法专为树结构设计，在事件级别整合时空信息，同时受问题引导，从而解决了第二个挑战。我们在所有长视频理解任务中取得了优越的性能，并发现了一些有趣的见解。我们提出的T-GRPO意外地激励模型自发生成推理链。此外，设计的树生长生长素动态调整扩展深度，获得了准确性和效率提升。代码已在https://github.com/caoxinye/VideoMiner公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding hour-long videos with multi-modal large language models(MM-LLMs) enriches the landscape of human-centered AI applications. However,for end-to-end video understanding with LLMs, uniformly sampling video framesresults in LLMs being overwhelmed by a vast amount of irrelevant information asvideo length increases. Existing hierarchical key frame extraction methodsimprove the accuracy of video understanding but still face two criticalchallenges. 1) How can the interference of extensive redundant information inlong videos be mitigated? 2) How can a model dynamically adapt to complexhierarchical structures while accurately identifying key frames? To addressthese issues, we propose VideoMiner, which iteratively segments, captions, andclusters long videos, forming a hierarchical tree structure. The proposedVideoMiner progresses from long videos to events to frames while preservingtemporal coherence, effectively addressing the first challenge. To preciselylocate key frames, we introduce T-GRPO, a tree-based group relative policyoptimization in reinforcement learning method that guides the exploration ofthe VideoMiner. The proposed T-GRPO is specifically designed for treestructures, integrating spatiotemporal information at the event level whilebeing guided by the question, thus solving the second challenge. We achievesuperior performance in all long-video understanding tasks and uncover severalinteresting insights. Our proposed T-GRPO surprisingly incentivizes the modelto spontaneously generate a reasoning chain. Additionally, the designed treegrowth auxin dynamically adjusts the expansion depth, obtaining accuracy andefficiency gains. The code is publicly available athttps://github.com/caoxinye/VideoMiner.</description>
      <author>example@mail.com (Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao)</author>
      <guid isPermaLink="false">2510.06040v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars</title>
      <link>http://arxiv.org/abs/2510.06200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了StarEmbed基准，用于评估时间序列基础模型在天文数据上的表现。结果表明，即使是在非天文数据上训练的模型，也能在天文任务上表现出色，这为天文学领域采用通用基础模型而非特定任务模型提供了依据。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)正被广泛采用作为通用的时间序列表示学习器，但其训练语料库不包括天文时间序列数据。恒星观测产生拍字节时间序列，具有独特挑战，包括不规则采样和异方差性。&lt;h4&gt;目的&lt;/h4&gt;介绍StarEmbed，这是第一个用于对恒星时间序列观测('光曲线')上的最先进TSFMs进行严格标准化评估的公共基准。&lt;h4&gt;方法&lt;/h4&gt;在三个科学动机的下游任务上进行基准测试：无监督聚类、监督分类和分布外源检测。StarEmbed整合了专家验证的标签目录与Zwicky Transient Facility的多变量光曲线，产生约40k个手工标记的光曲线，分布在七个天体物理学类别中。评估三个TSFMs(MOIRAI、Chronos、Chronos-Bolt)和一个领域特定转换器(Astromer)的零样本表示能力，与天文学文献中长期存在的基线手工制作的特征提取进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;这些TSFMs，特别是Chronos模型(在完全不同于天文观测的数据上训练)，在某些任务上可以超越既有的天文学特定基线，并有效推广到全新数据。特别是，TSFMs在分布外源检测基准上提供了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过第一个在天文时间序列数据上的TSFMs基准测试，研究测试了它们的泛化极限，并推动了时域天文学从使用任务特定的全监督管道转向采用通用基础模型表示来分析即将到来的天文台拍字节数据集的范式转变。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)正日益被采用作为高度通用的通用时间序列表示学习器。尽管它们的训练语料库庞大，但它们不包括天文时间序列数据。恒星观测产生拍字节时间序列，具有独特挑战，包括不规则采样和异方差性。我们介绍了StarEmbed，这是第一个用于对恒星时间序列观测('光曲线')上的最先进TSFMs进行严格标准化评估的公共基准。我们在三个科学动机的下游任务上进行基准测试：无监督聚类、监督分类和分布外源检测。StarEmbed整合了专家验证的标签目录与Zwicky Transient Facility的多变量光曲线，产生约40k个手工标记的光曲线，分布在七个天体物理学类别中。我们评估了三个TSFMs(MOIRAI、Chronos、Chronos-Bolt)和一个领域特定转换器(Astromer)的零样本表示能力，与天文学文献中长期存在的基线手工制作的特征提取进行比较。我们的结果表明，这些TSFMs，特别是Chronos模型(在完全不同于天文观测的数据上训练)，可以在某些任务上超越既有的天文学特定基线，并有效推广到全新数据。特别是，TSFMs在我们的分布外源检测基准上提供了最先进的性能。通过第一个在天文时间序列数据上的TSFMs基准测试，我们测试了它们的泛化极限，并推动了时域天文学从使用任务特定的全监督管道转向采用通用基础模型表示来分析即将到来的天文台拍字节数据集的范式转变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) are increasingly being adopted ashighly-capable general-purpose time series representation learners. Althoughtheir training corpora are vast, they exclude astronomical time series data.Observations of stars produce peta-scale time series with unique challengesincluding irregular sampling and heteroskedasticity. We introduce StarEmbed,the first public benchmark for rigorous and standardized evaluation ofstate-of-the-art TSFMs on stellar time series observations (``light curves'').We benchmark on three scientifically-motivated downstream tasks: unsupervisedclustering, supervised classification, and out-of-distribution sourcedetection. StarEmbed integrates a catalog of expert-vetted labels withmulti-variate light curves from the Zwicky Transient Facility, yielding ~40khand-labeled light curves spread across seven astrophysical classes. Weevaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) againsthandcrafted feature extraction, the long-standing baseline in the astrophysicsliterature. Our results demonstrate that these TSFMs, especially the Chronosmodels, which are trained on data completely unlike the astronomicalobservations, can outperform established astrophysics-specific baselines insome tasks and effectively generalize to entirely new data. In particular,TSFMs deliver state-of-the-art performance on our out-of-distribution sourcedetection benchmark. With the first benchmark of TSFMs on astronomical timeseries data, we test the limits of their generalization and motivate a paradigmshift in time-domain astronomy from using task-specific, fully supervisedpipelines toward adopting generic foundation model representations for theanalysis of peta-scale datasets from forthcoming observatories.</description>
      <author>example@mail.com (Weijian Li, Hong-Yu Chen, Qinjie Lin, Nabeel Rehemtulla, Ved G. Shah, Dennis Wu, Adam A. Miller, Han Liu)</author>
      <guid isPermaLink="false">2510.06200v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts</title>
      <link>http://arxiv.org/abs/2510.06162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的机器学习策略，通过在合成数据上持续预训练来扩展现有模型，以处理生物医学中的高维数据，同时保持可解释性。&lt;h4&gt;背景&lt;/h4&gt;机器学习在生物医学中用于揭示分子测量与病理学关系的新见解，但该领域数据通常只有少量观测值却有数千个潜在噪声特征，对传统机器学习方法构成挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维生物医学数据的大量特征的机器学习方法，同时保持特征重要性的可分析能力。&lt;h4&gt;方法&lt;/h4&gt;通过在从定制先验采样的合成数据上进行持续预训练来扩展现有基础模型，创建名为TabPFN-Wide的新模型。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN-Wide模型匹配或超过其基础模型的性能，提高对噪声的鲁棒性，可无缝扩展到50,000多个特征，同时保持固有的可解释性。&lt;h4&gt;结论&lt;/h4&gt;先验信息适应适合增强基础模型处理高维数据的能力，在生物医学应用中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;从分子测量与病理学之间的关系揭示新的见解，仍然是机器学习在生物医学中非常有影响力的应用。该领域的数据通常只包含少量观测值但有数千个潜在噪声特征，对传统机器学习方法构成挑战。虽然先验数据拟合网络作为表格数据的基础模型出现，但它们目前不适合处理大量特征。虽然特征减少使它们能够应用，但它妨碍了特征重要性分析。我们提出了一种策略，通过在从定制先验采样的合成数据上进行持续预训练来扩展现有模型。 resulting model, TabPFN-Wide, matches or exceeds its base model's performance while exhibiting improved robustness to noise. It seamlessly scales beyond 50,000 features, regardless of noise levels, while maintaining inherent interpretability, which is critical for biomedical applications. Our results show that prior-informed adaptation is suitable to enhance the capability of foundation models for high-dimensional data. On real-world biomedical datasets many of the most relevant features identified by the model overlap with previous biological findings, while others propose potential starting points for future studies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Revealing novel insights from the relationship between molecular measurementsand pathology remains a very impactful application of machine learning inbiomedicine. Data in this domain typically contain only a few observations butthousands of potentially noisy features, posing challenges for conventionalmachine learning approaches. While prior-data fitted networks emerge asfoundation models for tabular data, they are currently not suited to handlelarge feature counts (&gt;500). Although feature reduction enables theirapplication, it hinders feature importance analysis. We propose a strategy thatextends existing models through continued pre-training on synthetic datasampled from a customized prior. The resulting model, TabPFN-Wide, matches orexceeds its base model's performance while exhibiting improved robustness tonoise. It seamlessly scales beyond 50,000 features, regardless of noise levels,while maintaining inherent interpretability, which is critical for biomedicalapplications. Our results show that prior-informed adaptation is suitable toenhance the capability of foundation models for high-dimensional data. Onreal-world biomedical datasets many of the most relevant features identified bythe model overlap with previous biological findings, while others proposepotential starting points for future studies.</description>
      <author>example@mail.com (Christopher Kolberg, Katharina Eggensperger, Nico Pfeifer)</author>
      <guid isPermaLink="false">2510.06162v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</title>
      <link>http://arxiv.org/abs/2510.06131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeDiM是一个创新的医学离散扩散模型，能够学习跨模态的共享分布，无需模态特定组件。该模型统一了多种生成任务，通过离散扩散框架连接视觉和语言表示，并利用多模态大语言模型作为扩散骨干。&lt;h4&gt;背景&lt;/h4&gt;最近的生成式医学模型进展受到模态特定场景的限制，这些限制阻碍了从成像、病理学和临床笔记中整合互补证据。这种碎片化限制了它们发展为能够学习和推理整个生物医学数据谱系的基础模型。&lt;h4&gt;目的&lt;/h4&gt;提出MeDiM，第一个医学离散扩散模型，学习跨模态的共享分布，不需要模态特定组件，实现多种医学数据模态的统一生成和处理。&lt;h4&gt;方法&lt;/h4&gt;MeDiM基于离散扩散框架构建，通过共享概率空间连接视觉和语言表示。采用多模态大语言模型作为扩散骨干，利用其先验知识和跨模态推理能力。关键设计包括：移除因果注意力掩码以实现双向上下文，以及注入连续时间步嵌入以实现扩散感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明MeDiM能够生成高保真度的医学内容（在MIMIC-CXR上的FID为16.60，在PathGen上的FID为24.19）和准确的报告（METEOR分数为0.2650和0.2580）。联合生成的图像-报告对显著提高了下游性能（BLEU-1提高6.43%，BLEU-2提高18.57%，BLEU-3提高31.58%，METEOR提高4.80%）。&lt;h4&gt;结论&lt;/h4&gt;MeDiM支持连贯且临床上有依据的多模态输出，代表了医学生成模型的重要进步，能够统一处理多种医学数据模态，实现跨模态的学习和推理。&lt;h4&gt;翻译&lt;/h4&gt;最近的生成式医学模型进展受到模态特定场景的限制，这些限制阻碍了从成像、病理学和临床笔记中整合互补证据。这种碎片化限制了它们发展为能够学习和推理整个生物医学数据谱系的基础模型。我们提出了MeDiM，这是第一个医学离散扩散模型，它学习跨模态的共享分布，不需要模态特定组件。MeDiM统一了多种生成任务：图像和文本之间的转换，以及跨领域响应提示联合生成图像-报告对。基于离散扩散框架构建，MeDiM通过共享概率空间连接视觉和语言表示。为了实现统一和灵活的医学生成，我们采用多模态大语言模型作为扩散骨干，利用其先验知识和跨模态推理能力。引入了两个关键设计：(1)移除因果注意力掩码以实现双向上下文，(2)注入连续时间步嵌入以实现扩散感知。实验证明了高保真度的医学生成（在MIMIC-CXR上的FID为16.60，在PathGen上的FID为24.19）和准确的报告生成（METEOR分数为0.2650和0.2580）。联合生成的图像-报告对进一步提高了下游性能（BLEU-1提高6.43%，BLEU-2提高18.57%，BLEU-3提高31.58%，METEOR提高4.80%），表明MeDiM支持连贯且临床上有依据的多模态输出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative medical models are constrained bymodality-specific scenarios that hinder the integration of complementaryevidence from imaging, pathology, and clinical notes. This fragmentation limitstheir evolution into foundation models that can learn and reason across thefull spectrum of biomedical data. We propose MeDiM, the first medical discretediffusion model that learns shared distributions across modalities withoutmodality-specific components. MeDiM unifies multiple generative tasks:translating between images and text, and jointly producing image-report pairsacross domains in response to prompts. Built on a discrete diffusion framework,MeDiM bridges vision and language representations through a sharedprobabilistic space. To enable unified and flexible medical generation, weemploy a multimodal large language model (MLLM) as the diffusion backbone,leveraging its prior knowledge and cross-modal reasoning. Two key designs areintroduced: (1) removing the causal attention mask for bidirectional context,and (2) injecting continuous timestep embeddings for diffusion awareness.Experiments demonstrate high-fidelity medical generation (FID 16.60 onMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR0.2650 and 0.2580). Jointly generated image-report pairs further enhancedownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supportscoherent and clinically grounded multimodal outputs.</description>
      <author>example@mail.com (Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou)</author>
      <guid isPermaLink="false">2510.06131v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</title>
      <link>http://arxiv.org/abs/2510.05976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该调查论文提供了对扩散模型用于低光图像增强(LLIE)的最新批判性分析，包括与传统方法的对比评估、实际部署挑战和新兴范式的前景展望。&lt;h4&gt;背景&lt;/h4&gt;低光图像增强对安全关键应用如监控、自主导航和医疗成像至关重要，其中可见度下降会影响下游任务性能。扩散模型因其能够通过迭代去噪建模复杂图像分布而成为LLIE有前景的生成范式。&lt;h4&gt;目的&lt;/h4&gt;提供对扩散模型用于LLIE的最新批判性分析，对比评估扩散模型与生成对抗网络和基于Transformer的最先进方法，审查实际部署挑战，并展望基础模型等新兴范式的作用。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含六类(内在分解、频谱与潜在、加速、引导、多模态和自主)的多角度分类法，基于模型机制和条件信号的混合视图。评估定性的失败模式、基准不一致性以及可解释性、泛化能力和推理效率之间的权衡，并讨论实际部署约束和伦理考量。&lt;h4&gt;主要发现&lt;/h4&gt;扩散模型能够通过迭代去噪建模复杂图像分布；实际部署存在内存、能源使用等约束和伦理考量；基础模型具有潜力。&lt;h4&gt;结论&lt;/h4&gt;该调查旨在通过突出趋势和揭示开放研究问题来指导下一代基于扩散模型的LLIE研究，包括新型条件设置、实时适应和基础模型的潜力。&lt;h4&gt;翻译&lt;/h4&gt;低光图像增强(LLIE)对于监控、自主导航和医疗成像等安全关键应用至关重要，因为可见度下降会损害下游任务性能。最近，扩散模型已成为LLIE有前景的生成范式，因为它们能够通过迭代去噪建模复杂的图像分布。本调查提供了对扩散模型用于LLIE的最新批判性分析，特别强调了与生成对抗网络和基于Transformer的最先进方法的深入性能对比评估，对实际部署挑战的彻底检查，以及对基础模型等新兴范式作用的展望。我们提出了一个包含六类的多角度分类法：内在分解、频谱与潜在、加速、引导、多模态和自主；这些类别映射了跨越物理先验、条件方案和计算效率的增强方法。我们的分类法基于模型机制和条件信号的混合视图。我们评估了定性的失败模式、基准不一致性以及可解释性、泛化能力和推理效率之间的权衡。我们还讨论了实际部署约束(如内存、能源使用)和伦理考虑。本调查旨在通过突出趋势和揭示开放研究问题来指导下一代基于扩散模型的LLIE研究，包括新型条件设置、实时适应和基础模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-light image enhancement (LLIE) is vital for safety-critical applicationssuch as surveillance, autonomous navigation, and medical imaging, wherevisibility degradation can impair downstream task performance. Recently,diffusion models have emerged as a promising generative paradigm for LLIE dueto their capacity to model complex image distributions via iterative denoising.This survey provides an up-to-date critical analysis of diffusion models forLLIE, distinctively featuring an in-depth comparative performance evaluationagainst Generative Adversarial Network and Transformer-based state-of-the-artmethods, a thorough examination of practical deployment challenges, and aforward-looking perspective on the role of emerging paradigms like foundationmodels. We propose a multi-perspective taxonomy encompassing six categories:Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,and Autonomous; that map enhancement methods across physical priors,conditioning schemes, and computational efficiency. Our taxonomy is grounded ina hybrid view of both the model mechanism and the conditioning signals. Weevaluate qualitative failure modes, benchmark inconsistencies, and trade-offsbetween interpretability, generalization, and inference efficiency. We alsodiscuss real-world deployment constraints (e.g., memory, energy use) andethical considerations. This survey aims to guide the next generation ofdiffusion-based LLIE research by highlighting trends and surfacing openresearch questions, including novel conditioning, real-time adaptation, and thepotential of foundation models.</description>
      <author>example@mail.com (Eashan Adhikarla, Yixin Liu, Brian D. Davison)</author>
      <guid isPermaLink="false">2510.05976v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>StereoSync: Spatially-Aware Stereo Audio Generation from Video</title>
      <link>http://arxiv.org/abs/2510.05828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StereoSync是一个新颖高效的模型，用于生成与参考视频时间同步且空间对齐的音频。它利用预训练基础模型实现效率，同时保持高质量的合成，通过提取空间线索并使用交叉注意力条件，能够生成动态适应视频场景的立体声音频。&lt;h4&gt;背景&lt;/h4&gt;音频生成在近年来已被广泛研究，但与视频对齐的音频生成仍然是一个相对未被探索的前沿领域。&lt;h4&gt;目的&lt;/h4&gt;解决视频对齐音频生成的研究空白，开发能够生成与视频时间同步且空间对齐的音频模型。&lt;h4&gt;方法&lt;/h4&gt;StereoSync从深度图和边界框中提取空间线索，使用这些线索作为基于扩散的音频生成模型中的交叉注意力条件，实现立体声音频的生成，使其能够动态适应视频场景的空间结构和运动。&lt;h4&gt;主要发现&lt;/h4&gt;StereoSync在Walking The Maps数据集上实现了时间和空间对齐，在视频到音频生成方面取得了最先进的结果，产生了更加沉浸式和真实的音频体验。&lt;h4&gt;结论&lt;/h4&gt;StereoSync代表了视频对齐音频生成领域的重要进步，通过引入空间感知能力，超越了仅关注时间同步的现有方法。&lt;h4&gt;翻译&lt;/h4&gt;尽管音频生成在近年来已被广泛研究，但与视频对齐的音频生成仍然是一个相对未被探索的前沿领域。为解决这一空白，我们引入了StereoSync，这是一个新颖且高效的模型，旨在生成与参考视频时间同步且与其视觉空间上下文对齐的音频。此外，StereoSync通过利用预训练基础模型实现效率，减少了大量训练的需求，同时保持高质量的合成。与主要关注时间同步的现有方法不同，StereoSync通过将空间感知纳入视频对齐音频生成，引入了重大进展。实际上，给定输入视频，我们的方法从深度图和边界框中提取空间线索，将它们用作基于扩散的音频生成模型中的交叉注意力条件。这种方法使StereoSync能够超越简单的同步，产生动态适应视频场景空间结构的立体声音频。我们在Walking The Maps数据集上评估了StereoSync，这是一个包含视频游戏中角色穿过多样化环境行走视频的精选数据集。实验结果证明了StereoSync实现时间和空间对齐的能力，推动了视频到音频生成的最先进水平，并产生了显著更加沉浸式和真实的音频体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although audio generation has been widely studied over recent years,video-aligned audio generation still remains a relatively unexplored frontier.To address this gap, we introduce StereoSync, a novel and efficient modeldesigned to generate audio that is both temporally synchronized with areference video and spatially aligned with its visual context. Moreover,StereoSync also achieves efficiency by leveraging pretrained foundation models,reducing the need for extensive training while maintaining high-qualitysynthesis. Unlike existing methods that primarily focus on temporalsynchronization, StereoSync introduces a significant advancement byincorporating spatial awareness into video-aligned audio generation. Indeed,given an input video, our approach extracts spatial cues from depth maps andbounding boxes, using them as cross-attention conditioning in a diffusion-basedaudio generation model. Such an approach allows StereoSync to go beyond simplesynchronization, producing stereo audio that dynamically adapts to the spatialstructure and movement of a video scene. We evaluate StereoSync on Walking TheMaps, a curated dataset comprising videos from video games that featureanimated characters walking through diverse environments. Experimental resultsdemonstrate the ability of StereoSync to achieve both temporal and spatialalignment, advancing the state of the art in video-to-audio generation andresulting in a significantly more immersive and realistic audio experience.</description>
      <author>example@mail.com (Christian Marinoni, Riccardo Fosco Gramaccioni, Kazuki Shimada, Takashi Shibuya, Yuki Mitsufuji, Danilo Comminiello)</author>
      <guid isPermaLink="false">2510.05828v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation</title>
      <link>http://arxiv.org/abs/2510.05827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VCoT-Grasp的端到端抓取基础模型，通过视觉思维链推理增强视觉理解能力，解决了现有语言驱动抓取方法推理能力不足和泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取是机器人操作中最基础的任务之一，而抓取检测/生成一直是广泛研究的主题。最近，语言驱动的抓取生成因其实用的交互能力成为一个有前景的研究方向。&lt;h4&gt;目的&lt;/h4&gt;为了在复杂环境中保持强大的推理能力和泛化能力，作者提出了一种新的抓取基础模型，解决现有方法缺乏推理和泛化能力、依赖复杂模块化流程，以及当前抓取基础模型过度强调对话和对象语义导致性能低下且仅限于单目标抓取的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VCoT-Grasp，一种端到端的抓取基础模型，结合视觉思维链推理来增强视觉理解。该模型采用多轮处理范式，动态关注视觉输入，同时提供可解释的推理轨迹。此外，作者还创建并引入了一个大规模数据集VCoT-GraspSet，包含167K合成图像和1.36M+抓取，以及400+真实世界图像和1.2K+抓取，并标注了中间边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在VCoT-GraspSet和真实机器人上的广泛实验表明，该方法显著提高了抓取成功率，并能有效泛化到未见过的对象、背景和干扰物。&lt;h4&gt;结论&lt;/h4&gt;VCoT-Grasp通过视觉思维链推理和多轮处理范式，解决了现有语言驱动抓取方法的局限性，在复杂环境中实现了更强的推理能力和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取是机器人操作中最基础的任务之一，而抓取检测/生成长期以来一直是广泛研究的主题。最近，语言驱动的抓取生成因其实用的交互能力成为一个有前景的研究方向。然而，大多数现有方法要么缺乏足够的推理和泛化能力，要么依赖于复杂的模块化流程。此外，当前的抓取基础模型往往过度强调对话和对象语义，导致性能低下且仅限于单目标抓取。为了在复杂环境中保持强大的推理能力和泛化能力，我们提出了VCoT-Grasp，一种结合视觉思维链推理来增强视觉理解的端到端抓取基础模型。VCoT-Grasp采用多轮处理范式，动态关注视觉输入，同时提供可解释的推理轨迹。在训练方面，我们改进并引入了一个大规模数据集VCoT-GraspSet，包含167K合成图像和超过1.36M抓取，以及400+真实世界图像和超过1.2K抓取，并标注了中间边界框。在VCoT-GraspSet和真实机器人上的广泛实验表明，我们的方法显著提高了抓取成功率，并能有效泛化到未见过的对象、背景和干扰物。更多详情请访问https://zhanghr2001.github.io/VCoT-Grasp.github.io。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic grasping is one of the most fundamental tasks in roboticmanipulation, and grasp detection/generation has long been the subject ofextensive research. Recently, language-driven grasp generation has emerged as apromising direction due to its practical interaction capabilities. However,most existing approaches either lack sufficient reasoning and generalizationcapabilities or depend on complex modular pipelines. Moreover, current graspfoundation models tend to overemphasize dialog and object semantics, resultingin inferior performance and restriction to single-object grasping. To maintainstrong reasoning ability and generalization in cluttered environments, wepropose VCoT-Grasp, an end-to-end grasp foundation model that incorporatesvisual chain-of-thought reasoning to enhance visual understanding for graspgeneration. VCoT-Grasp adopts a multi-turn processing paradigm that dynamicallyfocuses on visual inputs while providing interpretable reasoning traces. Fortraining, we refine and introduce a large-scale dataset, VCoT-GraspSet,comprising 167K synthetic images with over 1.36M grasps, as well as 400+real-world images with more than 1.2K grasps, annotated with intermediatebounding boxes. Extensive experiments on both VCoT-GraspSet and real robotdemonstrate that our method significantly improves grasp success rates andgeneralizes effectively to unseen objects, backgrounds, and distractors. Moredetails can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.</description>
      <author>example@mail.com (Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Yuedi Zhang, Qi Zhang, Pengxiang Ding, Cheng Chi, Donglin Wang, Badong Chen)</author>
      <guid isPermaLink="false">2510.05827v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music</title>
      <link>http://arxiv.org/abs/2510.05756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to WASPAA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种三步框架，用于转录多声道音乐中吉他音轨的节奏模式，实现了高精度的节奏模式识别和人类可读的表示。&lt;h4&gt;背景&lt;/h4&gt;和弦转录在过去几十年受到广泛关注，但节奏模式的转录和编码研究相对较少。这一主题对节奏吉他等乐器尤为重要，因为这些乐器通常通过扫弦演奏重复和变化的节奏模式，且往往难以客观定义单一'正确'的节奏模式。&lt;h4&gt;目的&lt;/h4&gt;创建一个具有明确定义真实标签的数据集，让专业音乐家转录410首流行歌曲中的节奏模式，并录制遵循这些转录的吉他音轨封面版本。&lt;h4&gt;方法&lt;/h4&gt;提出一个三步框架：1)执行近似音源分离提取吉他部分；2)使用预训练的基础模型(MERT)检测分离的吉他音频中的单个扫弦；3)进行模式解码，将转录的吉他扫弦序列用专家策划的词汇中的模式表示。&lt;h4&gt;主要发现&lt;/h4&gt;可以以相当高的精度转录多声道音乐中吉他音轨的节奏模式，产生的表示是人类可读的，包括自动检测的小节线和拍号标记。&lt;h4&gt;结论&lt;/h4&gt;通过消融研究和错误分析，提出了一套评估指标来评估预测的节奏模式序列的准确性和可读性。&lt;h4&gt;翻译&lt;/h4&gt;虽然和弦转录在过去几十年中受到了相当多的关注，但投入到转录和编码歌曲中出现的节奏模式的工作却少得多。这一主题对于节奏吉他等乐器尤为重要，因为这些乐器通常通过扫弦演奏重复和随时间变化的节奏模式。然而，在许多情况下，无法客观地为给定的歌曲段落定义单一的'正确'节奏模式。为了创建一个具有明确定义真实标签的数据集，我们要求专业音乐家转录410首流行歌曲中的节奏模式，并录制吉他音轨遵循这些转录的封面版本。为了转录扫弦及其相应的节奏模式，我们提出了一个三步框架。首先，我们执行近似音源分离，从多声道混合中提取吉他部分。其次，我们使用预训练的基础模型(MERT)作为骨干网络，检测分离的吉他音频中的单个扫弦。最后，我们进行模式解码过程，将转录的吉他扫弦序列用专家策划的词汇中的模式表示。我们表明，可以以相当高的精度转录多声道音乐中吉他音轨的节奏模式，产生一种人类可读的表示，包括自动检测的小节线和拍号标记。我们进行了消融研究和错误分析，并提出了一套评估指标来评估预测的节奏模式序列的准确性和可读性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whereas chord transcription has received considerable attention during thepast couple of decades, far less work has been devoted to transcribing andencoding the rhythmic patterns that occur in a song. The topic is especiallyrelevant for instruments such as the rhythm guitar, which is typically playedby strumming rhythmic patterns that repeat and vary over time. However, in manycases one cannot objectively define a single "right" rhythmic pattern for agiven song section. To create a dataset with well-defined ground-truth labels,we asked expert musicians to transcribe the rhythmic patterns in 410 popularsongs and record cover versions where the guitar tracks followed thosetranscriptions. To transcribe the strums and their corresponding rhythmicpatterns, we propose a three-step framework. Firstly, we perform approximatestem separation to extract the guitar part from the polyphonic mixture.Secondly, we detect individual strums within the separated guitar audio, usinga pre-trained foundation model (MERT) as a backbone. Finally, we carry out apattern-decoding process in which the transcribed sequence of guitar strums isrepresented by patterns drawn from an expert-curated vocabulary. We show thatit is possible to transcribe the rhythmic patterns of the guitar track inpolyphonic music with quite high accuracy, producing a representation that ishuman-readable and includes automatically detected bar lines and time signaturemarkers. We perform ablation studies and error analysis and propose a set ofevaluation metrics to assess the accuracy and readability of the predictedrhythmic pattern sequence.</description>
      <author>example@mail.com (Aleksandr Lukoianov, Anssi Klapuri)</author>
      <guid isPermaLink="false">2510.05756v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2510.05746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的多智能体系统自动设计范式，通过优化思维链推理来显著提升系统性能，并开发出智能体推理模块(ARM)作为通用推理构建块，展现出优秀的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大语言模型驱动的多智能体系统在复杂推理任务上取得先进成果，但现有自动化MAS设计技术表现不佳，常与简单基线相当或更差，且需要计算昂贵的架构重新发现和数据标注。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MAS自动设计范式，将重点转移到优化思维链(CoT)推理上，因为简单的CoT推理通常与复杂系统具有竞争力。&lt;h4&gt;方法&lt;/h4&gt;开发了智能体推理模块(ARM)，这是CoT的智能体泛化，通过在代码空间上的树搜索发现，从简单CoT模块开始，基于执行轨迹反思的突变进行演化，可作为递归循环或元编排器中的子程序使用。&lt;h4&gt;主要发现&lt;/h4&gt;ARM方法显著优于手动设计的MAS和最先进的自动MAS设计方法，使用ARM构建的MAS在不同基础模型和任务域上保持高性能，无需进一步优化。&lt;h4&gt;结论&lt;/h4&gt;通过专注于优化思维链推理，可以开发出更有效的多智能体系统，ARM作为通用推理构建块具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLM)驱动的多智能体系统(MAS)已在各种复杂推理任务上取得了最先进的结果。最近的工作提出了自动化MAS设计的技术，消除了手动工程的需要。然而，这些技术表现不佳，通常只达到与简单基线相似或更差的表现。此外，它们需要为每个新的任务域进行计算昂贵的架构重新发现，并且在没有现有标记验证集的领域上进行昂贵的数据标注。一个关键的见解是，简单的思维链(CoT)推理通常与这些复杂系统具有竞争力，这表明MAS的基本推理单元CoT值得进一步研究。为此，我们提出了一种新的MAS自动设计范式，将重点转移到优化CoT推理上。我们引入了智能体推理模块(ARM)，这是CoT的一种智能体泛化，其中每个细粒度的推理步骤由专门的推理模块执行。该模块通过在代码空间上的树搜索被发现，从一个简单的CoT模块开始，并通过基于执行轨迹反思的突变进行演化。生成的ARM作为通用的推理构建块，可以直接用作递归循环或作为学习到的元编排器中的子程序。我们的方法显著优于手动设计的MAS和最先进的自动MAS设计方法。关键是，使用ARM构建的MAS表现出出色的泛化能力，在不同的基础模型和任务域上保持高性能，无需进一步优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Model (LLM)-powered Multi-agent systems (MAS) have achievedstate-of-the-art results on various complex reasoning tasks. Recent works haveproposed techniques to automate the design of MASes, eliminating the need formanual engineering. However, these techniques perform poorly, often achievingsimilar or inferior performance to simple baselines. Furthermore, they requirecomputationally expensive re-discovery of architectures for each new taskdomain and expensive data annotation on domains without existing labeledvalidation sets. A critical insight is that simple Chain of Thought (CoT)reasoning often performs competitively with these complex systems, suggestingthat the fundamental reasoning unit of MASes, CoT, warrants furtherinvestigation. To this end, we present a new paradigm for automatic MAS designthat pivots the focus to optimizing CoT reasoning. We introduce the AgenticReasoning Module (ARM), an agentic generalization of CoT where each granularreasoning step is executed by a specialized reasoning module. This module isdiscovered through a tree search over the code space, starting from a simpleCoT module and evolved using mutations informed by reflection on executiontraces. The resulting ARM acts as a versatile reasoning building block whichcan be utilized as a direct recursive loop or as a subroutine in a learnedmeta-orchestrator. Our approach significantly outperforms both manuallydesigned MASes and state-of-the-art automatic MAS design methods. Crucially,MASes built with ARM exhibit superb generalization, maintaining highperformance across different foundation models and task domains without furtheroptimization.</description>
      <author>example@mail.com (Bohan Yao, Shiva Krishna Reddy Malay, Vikas Yadav)</author>
      <guid isPermaLink="false">2510.05746v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</title>
      <link>http://arxiv.org/abs/2510.05740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project code: http://github.com/amir-aman/FusionDetect&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniGen基准测试和FusionDetect方法，用于解决生成图像检测中的跨生成器和跨视觉域泛化问题，实验表明新方法在多个基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;生成式模型的快速发展使得开发能够可靠检测合成图像的检测器变得日益重要。目前大多数工作集中在跨生成器泛化上，但这一观点过于局限。&lt;h4&gt;目的&lt;/h4&gt;解决检测合成图像中的另一个重要挑战：跨视觉域泛化能力。为此，提出OmniGen基准测试和FusionDetect方法。&lt;h4&gt;方法&lt;/h4&gt;介绍了FusionDetect方法，它利用两个冻结的基础模型(CLIP &amp; Dinov2)的优势，通过从这两个互补模型中提取特征，开发了一个能够自然适应生成器内容和设计变化的一致特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明FusionDetect比最接近的竞争对手准确率高3.87%，在已建立基准上的平均精度高6.13%，在OmniGen上准确率提高了4.48%，同时对常见的图像扰动表现出卓越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;不仅引入了一个高性能的检测器，还引入了一个新的基准和框架，用于推进通用AI图像检测。&lt;h4&gt;翻译&lt;/h4&gt;生成模型的快速发展使得开发能够可靠检测合成图像的检测器变得日益重要。尽管目前大多数工作集中在跨生成器泛化上，我们认为这一观点过于局限。检测合成图像涉及另一个同样重要的挑战：跨视觉域泛化。为弥合这一差距，我们提出了OmniGen基准测试。这个全面的评估数据集纳入了12种最先进的生成器，提供了一种更现实的方式来评估检测器在现实条件下的性能。此外，我们引入了一种新方法FusionDetect，旨在解决两个泛化向量。FusionDraw利用两个冻结的基础模型(CLIP &amp; Dinov2)的优势。通过从这两个互补模型中提取特征，我们开发了一个能够自然适应生成器内容和设计变化的一致特征空间。我们广泛的实验表明，FusionDetect不仅达到了新的最先进水平，比最接近的竞争对手准确率高3.87%，在已建立基准上的平均精度高6.13%，而且在OmniGen上准确率提高了4.48%，同时对常见的图像扰动表现出卓越的鲁棒性。我们不仅引入了一个高性能的检测器，还引入了一个新的基准和框架，用于推进通用AI图像检测。代码和数据集可在http://github.com/amir-aman/FusionDetect获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of generative models has made it increasingly crucialto develop detectors that can reliably detect synthetic images. Although mostof the work has now focused on cross-generator generalization, we argue thatthis viewpoint is too limited. Detecting synthetic images involves anotherequally important challenge: generalization across visual domains. To bridgethis gap,we present the OmniGen Benchmark. This comprehensive evaluationdataset incorporates 12 state-of-the-art generators, providing a more realisticway of evaluating detector performance under realistic conditions. In addition,we introduce a new method, FusionDetect, aimed at addressing both vectors ofgeneralization. FusionDetect draws on the benefits of two frozen foundationmodels: CLIP &amp; Dinov2. By deriving features from both complementary models,wedevelop a cohesive feature space that naturally adapts to changes in boththecontent and design of the generator. Our extensive experiments demonstratethat FusionDetect delivers not only a new state-of-the-art, which is 3.87% moreaccurate than its closest competitor and 6.13% more precise on average onestablished benchmarks, but also achieves a 4.48% increase in accuracy onOmniGen,along with exceptional robustness to common image perturbations. Weintroduce not only a top-performing detector, but also a new benchmark andframework for furthering universal AI image detection. The code and dataset areavailable at http://github.com/amir-aman/FusionDetect</description>
      <author>example@mail.com (Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee)</author>
      <guid isPermaLink="false">2510.05740v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment</title>
      <link>http://arxiv.org/abs/2510.05617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了InstaGeo，一个开源的端到端框架，用于解决地理空间基础模型(GFMs)部署中的两个主要限制：缺乏自动化的地理空间数据管道和微调模型体积过大。该框架整合了自动数据整理、任务特定模型蒸馏和交互式部署功能，将研究级GFMs转变为实用的低碳工具，用于实时、大规模的地球观测。&lt;h4&gt;背景&lt;/h4&gt;来自Landsat 8-9和Sentinel-2等任务的开放多光谱图像推动了地理空间基础模型在人道主义和环境应用方面的发展。然而，这些模型的部署仍然受到两个主要限制：(i)缺乏自动化的地理空间数据管道，以及(ii)微调模型体积过大。现有的GFMs缺乏处理原始卫星图像的工作流程，而下游调整通常保留了原始编码器的全部复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源的端到端框架，解决地理空间基础模型部署中的自动化数据管道缺失和模型体积过大的问题，将研究级GFMs转变为实用的低碳工具，用于实时、大规模的地球观测。&lt;h4&gt;方法&lt;/h4&gt;InstaGeo框架整合了三个主要组件：(1)自动数据整理，将原始图像转换为模型就绪的数据集；(2)任务特定模型蒸馏，生成紧凑、计算效率高的模型；(3)无缝部署为交互式网络地图应用。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用InstaGeo重现了三项已发表研究的数据集，训练的模型在洪水测绘中mIoU差异为-0.73 pp，作物分割中为-0.20 pp，沙漠蝗虫预测中为+1.79 pp；2) 蒸馏后的模型比标准微调对应模型小8倍，减少了FLOPs和CO2排放，同时精度损失最小；3) 利用InstaGeo的简化数据管道，整理了一个更大的作物分割数据集，实现了最先进的60.65% mIoU，比先前基线提高了12个百分点；4) InstaGeo使用户能够在单个工作日内从原始数据到模型部署。&lt;h4&gt;结论&lt;/h4&gt;通过统一数据准备、模型压缩和部署，InstaGeo将研究级GFMs转变为实用的、低碳的工具，用于实时、大规模的地球观测。这种方法将地理空间AI转向数据质量和应用驱动的创新。&lt;h4&gt;翻译&lt;/h4&gt;来自Landsat 8-9和Sentinel-2等任务的开放多光谱图像推动了地理空间基础模型(GFMs)在人道主义和环境应用方面的发展。然而，它们的部署仍然受到两个方面的限制：(i)缺乏自动化的地理空间数据管道，以及(ii)微调模型体积过大。现有的GFMs缺乏处理原始卫星图像的工作流程，而下游调整通常保留了原始编码器的全部复杂性。我们提出了InstaGeo，这是一个开源的端到端框架，通过整合以下内容解决了这些挑战：(1)自动数据整理，将原始图像转换为模型就绪的数据集；(2)任务特定模型蒸馏，生成紧凑、计算效率高的模型；(3)无缝部署为交互式网络地图应用。使用InstaGeo，我们重现了三项已发表研究的数据集，训练的模型在洪水测绘中mIoU差异为-0.73 pp，在作物分割中为-0.20 pp，在沙漠蝗虫预测中为+1.79 pp。蒸馏后的模型比标准微调对应模型小8倍，减少了FLOPs和CO2排放，同时精度损失最小。利用InstaGeo的简化数据管道，我们还整理了一个更大的作物分割数据集，实现了最先进的60.65% mIoU，比先前基线提高了12个百分点。此外，InstaGeo使用户能够在单个工作日内从原始数据到模型部署。通过统一数据准备、模型压缩和部署，InstaGeo将研究级GFMs转变为实用的、低碳的工具，用于实时、大规模的地球观测。这种方法将地理空间AI转向数据质量和应用驱动的创新。源代码、数据集和模型检查点可在以下网址获取：https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-access multispectral imagery from missions like Landsat 8-9 andSentinel-2 has fueled the development of geospatial foundation models (GFMs)for humanitarian and environmental applications. Yet, their deployment remainslimited by (i) the absence of automated geospatial data pipelines and (ii) thelarge size of fine-tuned models. Existing GFMs lack workflows for processingraw satellite imagery, and downstream adaptations often retain the fullcomplexity of the original encoder.  We present InstaGeo, an open-source, end-to-end framework that addressesthese challenges by integrating: (1) automated data curation to transform rawimagery into model-ready datasets; (2) task-specific model distillation toderive compact, compute-efficient models; and (3) seamless deployment asinteractive web-map applications. Using InstaGeo, we reproduced datasets fromthree published studies and trained models with marginal mIoU differences of-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp fordesert locust prediction. The distilled models are up to 8x smaller thanstandard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimalaccuracy loss.  Leveraging InstaGeo's streamlined data pipeline, we also curated a largercrop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 ppimprovement over prior baselines. Moreover, InstaGeo enables users to progressfrom raw data to model deployment within a single working day.  By unifying data preparation, model compression, and deployment, InstaGeotransforms research-grade GFMs into practical, low-carbon tools for real-time,large-scale Earth observation. This approach shifts geospatial AI toward dataquality and application-driven innovation. Source code, datasets, and modelcheckpoints are available at:https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git</description>
      <author>example@mail.com (Ibrahim Salihu Yusuf, Iffanice Houndayi, Rym Oualha, Mohamed Aziz Cherif, Kobby Panford-Quainoo, Arnu Pretorius)</author>
      <guid isPermaLink="false">2510.05617v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</title>
      <link>http://arxiv.org/abs/2510.05593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ShortCoTI框架，通过优化思维链推理过程，减少图像生成中的冗余提示，提高计算效率同时保持图像质量。&lt;h4&gt;背景&lt;/h4&gt;自回归多模态大语言模型在图像生成领域流行，新方法采用思维链推理增强对齐和细节，但这种方法可能引入不必要的冗余，增加计算成本并产生与原始提示矛盾的细节。&lt;h4&gt;目的&lt;/h4&gt;探索如何生成更简洁的思维链序列，实现更高效的图像生成。&lt;h4&gt;方法&lt;/h4&gt;介绍ShortCoTI轻量级优化框架，使用自适应函数奖励更简洁的提示，该函数根据任务估计难度调整，并将此奖励纳入强化学习范式。&lt;h4&gt;主要发现&lt;/h4&gt;ShortCoTI将提示推理长度减少54%，在多个基准测试中保持或略微提高质量指标，消除冗长解释和重复改进，生成既简洁又语义丰富的推理提示。&lt;h4&gt;结论&lt;/h4&gt;ShortCoTI提高计算效率，同时不损害生成图像的保真度或视觉吸引力。&lt;h4&gt;翻译&lt;/h4&gt;自回归多模态大语言模型最近在图像生成领域流行起来，这得益于基础模型的进展。为了增强对齐和细节，新方法采用思维链推理，在图像合成前将用户输入扩展为详细提示。然而，这种策略可能引入不必要的冗余——我们称之为视觉过度思考——这增加了计算成本，并可能引入与原始提示相矛盾的细节。在这项工作中，我们探索如何生成更简洁的思维链序列以实现更高效的图像生成。我们介绍ShortCoTI，一个轻量级优化框架，它鼓励更简洁的思维链，同时保持输出图像质量。ShortCoTI使用自适应函数奖励更简洁的提示，该函数根据每个任务的估计难度进行调整。将此奖励纳入强化学习范式，可以在多个基准测试中将提示推理长度减少54%，同时保持或略微提高质量指标。定性分析表明，我们的方法消除了冗长的解释和重复的改进，产生既简洁又语义丰富的推理提示。因此，ShortCoTI提高了计算效率，同时不损害生成图像的保真度或视觉吸引力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoregressive multimodal large language models have recently gainedpopularity for image generation, driven by advances in foundation models. Toenhance alignment and detail, newer approaches employ chain-of-thought (CoT)reasoning, expanding user inputs into elaborated prompts prior to imagesynthesis. However, this strategy can introduce unnecessary redundancy -- aphenomenon we call visual overthinking -- which increases computational costsand can introduce details that contradict the original prompt. In this work, weexplore how to generate more concise CoT sequences for more efficient imagegeneration. We introduce ShortCoTI, a lightweight optimization framework thatencourages more concise CoT while preserving output image quality. ShortCoTIrewards more concise prompts with an adaptive function that scales according toan estimated difficulty for each task. Incorporating this reward into areinforcement learning paradigm reduces prompt reasoning length by 54% whilemaintaining or slightly improving quality metrics across multiple benchmarks(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminatesverbose explanations and repetitive refinements, producing reasoning promptsthat are both concise and semantically rich. As a result, ShortCoTI improvescomputational efficiency without compromising the fidelity or visual appeal ofgenerated images.</description>
      <author>example@mail.com (Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang)</author>
      <guid isPermaLink="false">2510.05593v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics</title>
      <link>http://arxiv.org/abs/2510.05482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ATOM是一个预训练的Transformer神经算子，用于多任务分子动力学模拟，采用准等变设计和时间注意力机制，能够在无需明确分子图的情况下准确并行解码多个未来状态。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是现代计算药物发现、材料科学和生物化学的基础。最近的机器学习模型能够提供高保真的分子动力学预测，而无需重复求解量子力学力，相比传统流程有显著加速。然而，许多此类方法通常强制执行严格的等变性并依赖顺序滚动，限制了其灵活性和模拟效率。它们也通常是单任务的，针对单个分子和固定时间帧进行训练，这限制了它们对未见化合物和扩展时间步长的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有分子动力学模拟方法的限制，提高模型的灵活性、效率和泛化能力，特别是对未见化合物和不同时间跨度的预测。&lt;h4&gt;方法&lt;/h4&gt;提出ATOM（分子原子Transformer算子），一个预训练的Transformer神经算子，用于多任务分子动力学。ATOM采用准等变设计，不需要明确的分子图，并采用时间注意力机制，允许准确并行解码多个未来状态。为了支持跨化学品和时间尺度的算子预训练，作者创建了TG80数据集，这是一个包含80种化合物超过250万飞秒轨迹的大型、多样化且数值稳定的分子动力学数据集。&lt;h4&gt;主要发现&lt;/h4&gt;ATOM在既定的单任务基准测试（如MD17、RMD17和MD22）上取得了最先进的性能。在TG80上进行多任务预训练后，ATOM对跨不同时间跨度的未见分子表现出卓越的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ATOM代表了向准确、高效和可转移的分子动力学模型迈出的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学模拟支持现代计算药物发现、材料科学和生物化学。最近的机器学习模型提供高保真的分子动力学预测，无需重复求解量子力学力，相比传统流程实现了显著加速。然而，许多此类方法通常强制执行严格的等变性并依赖顺序滚动，从而限制了它们的灵活性和模拟效率。它们也通常是单任务的，针对单个分子和固定时间帧进行训练，这限制了它们对未见化合物和扩展时间步长的泛化能力。为解决这些问题，我们提出了ATOM（分子原子Transformer算子），这是一个用于多任务分子动力学的预训练Transformer神经算子。ATOM采用准等变设计，不需要明确的分子图，并采用时间注意力机制，允许准确并行解码多个未来状态。为了支持跨化学品和时间尺度的算子预训练，我们整理了TG80，这是一个包含80种化合物超过250万飞秒轨迹的大型、多样化且数值稳定的分子动力学数据集。ATOM在既定的单任务基准测试（如MD17、RMD17和MD22）上取得了最先进的性能。在TG80上进行多任务预训练后，ATOM对跨不同时间跨度的未见分子表现出卓越的零样本泛化能力。我们认为ATOM代表了向准确、高效和可转移的分子动力学模型迈出的重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations underpin modern computational drug dis-covery, materials science, and biochemistry. Recent machine learning modelsprovide high-fidelity MD predictions without the need to repeatedly solvequantum mechanical forces, enabling significant speedups over conventionalpipelines. Yet many such methods typically enforce strict equivariance and relyon sequential rollouts, thus limiting their flexibility and simulationefficiency. They are also com- monly single-task, trained on individualmolecules and fixed timeframes, which restricts generalization to unseencompounds and extended timesteps. To address these issues, we propose AtomisticTransformer Operator for Molecules (ATOM), a pretrained transformer neuraloperator for multitask molecular dynamics. ATOM adopts a quasi-equivariantdesign that requires no explicit molecular graph and employs a temporalattention mechanism, allowing for the accurate parallel decod- ing of multiplefuture states. To support operator pretraining across chemicals and timescales,we curate TG80, a large, diverse, and numerically stable MD dataset with over2.5 million femtoseconds of trajectories across 80 compounds. ATOM achievesstate-of-the-art performance on established single-task benchmarks, such asMD17, RMD17 and MD22. After multitask pretraining on TG80, ATOM showsexceptional zero-shot generalization to unseen molecules across varying timehori- zons. We believe ATOM represents a significant step toward accurate,efficient, and transferable molecular dynamics models</description>
      <author>example@mail.com (Luke Thompson, Davy Guan, Dai Shi, Slade Matthews, Junbin Gao, Andi Han)</author>
      <guid isPermaLink="false">2510.05482v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Vul-R2: A Reasoning LLM for Automated Vulnerability Repair</title>
      <link>http://arxiv.org/abs/2510.05480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures. This paper is accepted by ASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;当前自动漏洞修复(AVR)方法将问题表述为序列生成问题并利用大型语言模型解决，但面临缺乏高质量漏洞相关推理数据和难以验证中间修复过程两大挑战。&lt;h4&gt;背景&lt;/h4&gt;软件漏洞数量呈指数级增长，对自动漏洞修复解决方案有迫切需求。&lt;h4&gt;目的&lt;/h4&gt;解决当前自动漏洞修复方法面临的挑战，提高漏洞修复效果。&lt;h4&gt;方法&lt;/h4&gt;将自动漏洞修复表述为序列生成问题，利用大型语言模型生成漏洞修复方案。&lt;h4&gt;主要发现&lt;/h4&gt;当前方法面临两大挑战：(1)缺乏高质量、漏洞相关的推理数据，导致难以捕捉多样化的漏洞修复模式；(2)难以在LLM训练过程中验证中间的漏洞修复过程，因为缺乏可验证的中间反馈。&lt;h4&gt;结论&lt;/h4&gt;虽然当前自动漏洞修复方法展示了最先进的性能，但缺乏高质量漏洞相关推理数据和难以验证中间修复过程仍是主要挑战。&lt;h4&gt;翻译&lt;/h4&gt;软件漏洞的指数级增长已经对自动漏洞修复(AVR)解决方案产生了迫切需求。最近的研究将AVR表述为序列生成问题，并利用大型语言模型(LLMs)来解决这一问题。通常，这些方法提示或微调LLMs以直接生成漏洞修复方案。尽管这些方法展示了最先进的性能，但它们面临以下挑战：(1)缺乏高质量的、与漏洞相关的推理数据。当前方法主要依赖主要编码通用编程知识的基础模型。没有漏洞相关的推理数据，它们往往无法捕捉多样化的漏洞修复模式。(2)难以在LLM训练过程中验证中间的漏洞修复过程。现有的强化学习方法通常利用来自环境的中间执行反馈(例如基于沙盒的执行结果)来指导强化学习训练。相比之下，漏洞修复过程通常缺乏这种可验证的中间反馈，这给模型训练带来了额外的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The exponential increase in software vulnerabilities has created an urgentneed for automatic vulnerability repair (AVR) solutions. Recent research hasformulated AVR as a sequence generation problem and has leveraged largelanguage models (LLMs) to address this problem. Typically, these approachesprompt or fine-tune LLMs to generate repairs for vulnerabilities directly.Although these methods show state-of-the-art performance, they face thefollowing challenges: (1) Lack of high-quality, vulnerability-related reasoningdata. Current approaches primarily rely on foundation models that mainly encodegeneral programming knowledge. Without vulnerability-related reasoning data,they tend to fail to capture the diverse vulnerability repair patterns. (2)Hard to verify the intermediate vulnerability repair process during LLMtraining. Existing reinforcement learning methods often leverage intermediateexecution feedback from the environment (e.g., sandbox-based execution results)to guide reinforcement learning training. In contrast, the vulnerability repairprocess generally lacks such intermediate, verifiable feedback, which posesadditional challenges for model training.</description>
      <author>example@mail.com (Xin-Cheng Wen, Zirui Lin, Yijun Yang, Cuiyun Gao, Deheng Ye)</author>
      <guid isPermaLink="false">2510.05480v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts</title>
      <link>http://arxiv.org/abs/2510.05363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的MHA-RAG框架，通过将示例表示为软提示并使用多头注意力机制，在有限数据下有效适应基础模型到新领域，同时提高性能并降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;基础模型在有限训练数据下适应新领域具有挑战性且计算成本高。先前研究表明使用领域特定示例作为上下文演示是有效的，但纯文本表示可能不是最优方案。&lt;h4&gt;目的&lt;/h4&gt;调查将示例纯文本表示是否是最有效、高效和稳定的方法，并探索替代方案：将示例表示为软提示，并使用示例顺序不变的模型架构。&lt;h4&gt;方法&lt;/h4&gt;引入多头注意力检索增强生成(MHA-RAG)框架，使用注意力头数量作为简单超参数控制软提示生成，架构对示例顺序具有不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个问答基准和模型规模上，MHA-RAG比标准RAG实现了20点的性能提升，同时将推理成本降低了10倍XGFLOPs，提供了更高的准确性和效率。&lt;h4&gt;结论&lt;/h4&gt;MHA-RAG框架在准确性和效率方面都优于标准RAG，且对示例顺序具有不变性，是一种更有效的领域适应方法。&lt;h4&gt;翻译&lt;/h4&gt;在有限训练数据下将基础模型适应到新领域具有挑战性且计算成本高昂。虽然先前工作已经证明了使用领域特定示例作为上下文演示的有效性，但我们研究将示例纯文本表示是否是最有效、高效和稳定的方法。我们探索了一种替代方案：将示例表示为软提示，并采用示例顺序不变的模型架构。为此，我们引入了多头注意力检索增强生成(MHA-RAG)，这是一个框架，其中注意力头的数量作为简单超参数，用于控制不同任务中的软提示生成。在多个问答基准和模型规模上，MHA-RAG比标准RAG实现了20点的性能提升，同时将推理成本降低了10倍XGFLOPs—提供了更高的准确性和效率，且对示例顺序不变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adapting Foundation Models to new domains with limited training data ischallenging and computationally expensive. While prior work has demonstratedthe effectiveness of using domain-specific exemplars as in-contextdemonstrations, we investigate whether representing exemplars purely as text isthe most efficient, effective, and stable approach. We explore an alternative:representing exemplars as soft prompts with an exemplar order invariant modelarchitecture. To this end, we introduce Multi-Head AttentionRetrieval-Augmented Generation (MHA-RAG), a framework with the number ofattention heads serving as a simple hyperparameter to control softprompt-generation across different tasks. Across multiple question-answeringbenchmarks and model scales, MHA-RAG achieves a 20-point performance gain overstandard RAG, while cutting inference costs by a factor of 10XGFLOPs-delivering both higher accuracy and greater efficiency, invariant toexemplar order.</description>
      <author>example@mail.com (Abhinav Jain, Xinyu Yao, Thomas Reps, Christopher Jermaine)</author>
      <guid isPermaLink="false">2510.05363v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing</title>
      <link>http://arxiv.org/abs/2510.05213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VER(Vision Expert transformer for Robot learning)，一种用于机器人学习的视觉专家transformer模型，通过动态选择多个预训练视觉基础模型(VFMs)的专家，实现了跨任务的高性能机器人学习。&lt;h4&gt;背景&lt;/h4&gt;预训练视觉基础模型(VFMs)通过丰富的视觉表示推进了机器人学习，但单个VFM通常只在特定领域表现出色，限制了跨任务的泛化能力。将多个VFM蒸馏为统一表示用于策略可以缓解这一问题，但通常导致僵化的任务特定特征选择，并需要昂贵的完全重新训练来融入机器人领域知识。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够灵活选择任务相关特征并高效整合机器人领域知识的视觉表示模型，以提升机器人在多样化任务中的学习能力和泛化性能。&lt;h4&gt;方法&lt;/h4&gt;在预训练阶段将多个VFM蒸馏到视觉专家库中；仅微调一个轻量级路由网络(参数少于0.4%)，从预训练库中动态选择任务相关专家；引入基于课程Top-K退火的分块专家路由，提高动态专家选择的灵活性和精确度；支持参数高效的微调，实现可扩展的专家利用和自适应机器人领域知识整合。&lt;h4&gt;主要发现&lt;/h4&gt;在17个多样化的机器人任务和多个策略头中，VER实现了最先进的性能；VER减少了任务不相关区域(如背景)中的大范数异常值，并专注于任务关键区域。&lt;h4&gt;结论&lt;/h4&gt;VER有效解决了现有方法在机器人学习中的局限性，实现了更灵活的特征选择和更高效的知识整合，在多种任务上取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;预训练视觉基础模型(VFMs)通过丰富的视觉表示推进机器人学习，然而单个VFM通常只在特定领域表现出色，限制了跨任务的泛化能力。将多个VMs蒸馏为统一表示用于策略可以缓解这一限制，但通常产生僵化的任务特定特征选择，并需要昂贵的完全重新训练来融入机器人领域知识。我们提出了VER，一种用于机器人学习的视觉专家transformer。在预训练期间，VER将多个VFM蒸馏到视觉专家库中。然后，它仅微调一个轻量级路由网络(参数少于0.4%)，从预训练库中动态选择任务相关专家用于下游机器人任务。我们进一步引入了基于课程Top-K退火的分块专家路由，以提高动态专家选择的灵活性和精确度。此外，VER支持参数高效的微调，以实现可扩展的专家利用和自适应机器人领域知识整合。在17个多样化的机器人任务和多个策略头中，VER实现了最先进的性能。我们发现VER减少了任务不相关区域(如背景)中的大范数异常值，并专注于任务关键区域。可视化和代码可在https://yixiaowang7.github.io/ver_page/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained vision foundation models (VFMs) advance robotic learning via richvisual representations, yet individual VFMs typically excel only in specificdomains, limiting generality across tasks. Distilling multiple VFMs into aunified representation for policy can mitigate this limitation but often yieldsinflexible task-specific feature selection and requires costly full re-trainingto incorporate robot-domain knowledge. We propose VER, a Vision Experttransformer for Robot learning. During pretraining, VER distills multiple VFMsinto a vision expert library. It then fine-tunes only a lightweight routingnetwork (fewer than 0.4% of parameters) to dynamically select task-relevantexperts from the pretrained library for downstream robot tasks. We furtherintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improveboth flexibility and precision of dynamic expert selection. Moreover, VERsupports parameter-efficient finetuning for scalable expert utilization andadaptive robot-domain knowledge integration. Across 17 diverse robotic tasksand multiple policy heads, VER achieves state-of-the-art performance. We findthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,background) and concentrates on task-critical regions. Visualizations and codescan be found in https://yixiaowang7.github.io/ver_page/.</description>
      <author>example@mail.com (Yixiao Wang, Mingxiao Huo, Zhixuan Liang, Yushi Du, Lingfeng Sun, Haotian Lin, Jinghuan Shang, Chensheng Peng, Mohit Bansal, Mingyu Ding, Masayoshi Tomizuka)</author>
      <guid isPermaLink="false">2510.05213v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Representation Potentials of Foundation Models for Multimodal Alignment: A Survey</title>
      <link>http://arxiv.org/abs/2510.05184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了基础模型的表示潜力，即其学习到的表示在单一模态内捕获特定任务信息的能力，同时为跨模态对齐和统一提供可迁移的基础。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模多样化数据预学习高度可迁移的表示，研究表明这些表示在不同架构和模态间表现出显著的相似性。&lt;h4&gt;目的&lt;/h4&gt;调查基础模型的表示潜力，并分析其在跨模态迁移和对齐中的潜力。&lt;h4&gt;方法&lt;/h4&gt;回顾代表性基础模型和使对齐可测量的关键指标，综合来自视觉、语言、语音、多模态和神经科学研究中表示潜力的实证证据。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在其表示空间中通常表现出结构规律性和语义一致性，这使它们成为跨模态迁移和对齐的有力候选者。&lt;h4&gt;结论&lt;/h4&gt;分析促进表示潜力的关键因素，讨论开放性问题，并指出潜在挑战。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模多样化数据预学习高度可迁移的表示。越来越多的研究表明这些表示在不同架构和模态间表现出显著的相似性。在本综述中，我们调查了基础模型的表示潜力，定义为它们学习到的表示在单一模态内捕获特定任务信息的能力，同时为跨模态对齐和统一提供可迁移的基础。我们从回顾代表性的基础模型和使对齐可测量的关键指标开始。然后我们综合了来自视觉、语言、语音、多模态和神经科学研究中表示潜力的实证证据。证据表明基础模型在其表示空间中通常表现出结构规律性和语义一致性，这使它们成为跨模态迁移和对齐的有力候选者。我们进一步分析了促进表示潜力的关键因素，讨论了开放性问题，并指出了潜在挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models learn highly transferable representations throughlarge-scale pretraining on diverse data. An increasing body of researchindicates that these representations exhibit a remarkable degree of similarityacross architectures and modalities. In this survey, we investigate therepresentation potentials of foundation models, defined as the latent capacityof their learned representations to capture task-specific information within asingle modality while also providing a transferable basis for alignment andunification across modalities. We begin by reviewing representative foundationmodels and the key metrics that make alignment measurable. We then synthesizeempirical evidence of representation potentials from studies in vision,language, speech, multimodality, and neuroscience. The evidence suggests thatfoundation models often exhibit structural regularities and semanticconsistencies in their representation spaces, positioning them as strongcandidates for cross-modal transfer and alignment. We further analyze the keyfactors that foster representation potentials, discuss open questions, andhighlight potential challenges.</description>
      <author>example@mail.com (Jianglin Lu, Hailing Wang, Yi Xu, Yizhou Wang, Kuo Yang, Yun Fu)</author>
      <guid isPermaLink="false">2510.05184v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion</title>
      <link>http://arxiv.org/abs/2510.05957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于模型的强化学习框架，用于软体爬行机器人的自适应运动控制，通过从机载传感器推断的潜在动力学来指导运动策略优化。&lt;h4&gt;背景&lt;/h4&gt;软体爬行机器人利用软体变形和顺应性通过表面接触实现移动，但设计其控制策略面临模型不准确、传感器噪声和需要发现运动步态等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于模型的强化学习框架，使软体机器人能够仅依靠嘈杂的传感器反馈实现自适应运动。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于模型的强化学习框架，其中从机载传感器推断的潜在动力学作为预测模型，指导actor-critic算法优化运动策略。在模拟中使用惯性测量单元和飞行时间传感器作为观察值，对最小爬行模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;学习的潜在动力学能够实现短时间范围内的运动预测，而actor-critic算法发现了有效的运动策略。&lt;h4&gt;结论&lt;/h4&gt;基于潜在动力学的MB-RL方法在仅依靠嘈杂传感器反馈的基础上，为软体机器人的自适应运动提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;软体爬行机器人是利用软体变形和顺应性通过表面接触实现移动的移动机器人。由于模型不准确、传感器噪声以及需要发现运动步态等因素，为这类系统设计控制策略具有挑战性。在这项工作中，我们提出了一种基于模型的强化学习框架，其中从机载传感器推断的潜在动力学作为预测模型，指导actor-critic算法优化运动策略。我们在模拟中使用惯性测量单元和飞行时间传感器作为观察值，对最小爬行模型评估了该框架。学习的潜在动力学实现了短时间范围内的运动预测，而actor-critic发现了有效的运动策略。这种方法强调了基于潜在动力学的MB-RL在仅依靠嘈杂传感器反馈的基础上实现软体机器人自适应运动的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft robotic crawlers are mobile robots that utilize soft body deformabilityand compliance to achieve locomotion through surface contact. Designing controlstrategies for such systems is challenging due to model inaccuracies, sensornoise, and the need to discover locomotor gaits. In this work, we present amodel-based reinforcement learning (MB-RL) framework in which latent dynamicsinferred from onboard sensors serve as a predictive model that guides anactor-critic algorithm to optimize locomotor policies. We evaluate theframework on a minimal crawler model in simulation using inertial measurementunits and time-of-flight sensors as observations. The learned latent dynamicsenable short-horizon motion prediction while the actor-critic discoverseffective locomotor policies. This approach highlights the potential oflatent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotionbased solely on noisy sensor feedback.</description>
      <author>example@mail.com (Vaughn Gzenda, Robin Chhabra)</author>
      <guid isPermaLink="false">2510.05957v1</guid>
      <pubDate>Wed, 08 Oct 2025 15:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark</title>
      <link>http://arxiv.org/abs/2509.14574v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个用于测试视觉语言模型(VLMs)在城市感知能力上的小基准测试，使用100张蒙特利尔街道图像，收集了来自7个社区群体的12名参与者在30个维度上的230份标注表。研究发现模型在客观属性理解上优于主观评价，最佳系统(claude-sonnet)在多标签项上达到了宏观0.31和平均Jaccard 0.48的分数。&lt;h4&gt;背景&lt;/h4&gt;理解人们如何阅读城市场景可以为城市设计和规划提供重要信息，但目前缺乏针对视觉语言模型在城市感知能力上的标准化测试方法。&lt;h4&gt;目的&lt;/h4&gt;创建一个小型基准测试，用于评估视觉语言模型在理解城市场景方面的能力，特别是在区分客观物理属性和主观印象方面。&lt;h4&gt;方法&lt;/h4&gt;使用100张蒙特利尔街道图像（真实照片和逼真合成场景各半），收集7个社区群体12名参与者在30个维度上的标注表，将法语响应标准化为英语，在零样本设置下评估7个VLM模型，使用结构化提示和确定性解析器，对单选项目使用准确率，对多标签项目使用Jaccard重叠系数。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在可见的客观属性上比主观评价上有更好的对齐效果；2) 最佳系统(claude-sonnet)在多标签项上达到宏观0.31和平均Jaccard 0.48的分数；3) 人类一致性越高，模型得分越好；4) 合成图像略微降低了分数。&lt;h4&gt;结论&lt;/h4&gt;该基准测试为评估视觉语言模型在城市感知方面的能力提供了工具，结果显示模型在客观属性理解上优于主观评价，研究还提供了可复现和不确定性感知的评估工具，可用于参与式城市分析。&lt;h4&gt;翻译&lt;/h4&gt;理解人们如何阅读城市场景可以为设计和规划提供信息。我们引入了一个小型基准测试，用于使用100张蒙特利尔街道图像测试视觉语言模型(VLMs)在城市感知方面的能力，这些图像 evenly split between photographs和photorealistic synthetic scenes。来自7个社区群体的12名参与者在30个维度上提供了230份标注表，这些维度混合了物理属性和主观印象。法语响应被标准化为英语。我们在零样本设置下使用结构化提示和确定性解析器评估了7个VLM模型。对于单选项目我们使用准确率，对于多标签项目使用Jaccard重叠系数；人类一致性使用Krippendorff's alpha和成对Jaccard系数。结果表明，模型在可见的客观属性上比主观评价上有更好的对齐效果。最佳系统(claude-sonnet)在多标签项上达到了宏观0.31和平均Jaccard 0.48的分数。更高的人类一致性对应更好的模型得分。合成图像略微降低了分数。我们发布了基准测试、提示词和用于参与式城市分析的可复现、不确定性感知的评估工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how people read city scenes can inform design and planning. Weintroduce a small benchmark for testing vision-language models (VLMs) on urbanperception using 100 Montreal street images, evenly split between photographsand photorealistic synthetic scenes. Twelve participants from seven communitygroups supplied 230 annotation forms across 30 dimensions mixing physicalattributes and subjective impressions. French responses were normalized toEnglish. We evaluated seven VLMs in a zero-shot setup with a structured promptand deterministic parser. We use accuracy for single-choice items and Jaccardoverlap for multi-label items; human agreement uses Krippendorff's alpha andpairwise Jaccard. Results suggest stronger model alignment on visible,objective properties than subjective appraisals. The top system (claude-sonnet)reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher humanagreement coincides with better model scores. Synthetic images slightly lowerscores. We release the benchmark, prompts, and harness for reproducible,uncertainty-aware evaluation in participatory urban analysis.</description>
      <author>example@mail.com (Rashid Mushkani)</author>
      <guid isPermaLink="false">2509.14574v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
  <item>
      <title>ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</title>
      <link>http://arxiv.org/abs/2510.05070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ResMimic，一个用于人形机器人全身运动操作的两阶段残差学习框架，通过结合通用运动跟踪和精确残差策略，实现了从人类运动数据中学习精确且富有表现力的机器人控制。&lt;h4&gt;背景&lt;/h4&gt;人形机器人全身运动操作在日常生活服务和仓库任务中具有变革性潜力。尽管通用运动跟踪技术已使人形机器人能够复制人类动作，但这些策略缺乏运动操作所需的精确性和物体感知能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现精确且富有表现力的人形机器人控制系统，使机器人能够执行需要精确运动和物体交互的任务。&lt;h4&gt;方法&lt;/h4&gt;提出ResMimic两阶段框架：首先使用大规模人类运动数据训练通用运动跟踪策略作为基础；然后学习高效但精确的残差策略来优化GMT输出，改善运动并融入物体交互。还设计了基于点云的物体跟踪奖励、接触奖励和基于课程的虚拟物体控制器来促进高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实Unitree G1人形机器人上的评估表明，ResMimic与强大基线相比，在任务成功率、训练效率和鲁棒性方面都有显著提升。&lt;h4&gt;结论&lt;/h4&gt;ResMimic框架有效解决了人形机器人全身运动操作中精确性和物体感知的挑战，为人形机器人在实际服务任务中的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人全身运动操作有望为日常服务和仓库任务带来变革性能力。尽管最近在通用运动跟踪方面的进展使人形机器人能够复制各种人类动作，但这些策略缺乏运动操作所需的精确性和物体感知能力。为此，我们提出了ResMimic，一个从人类运动数据中实现精确且富有表现力的人形机器人控制的两阶段残差学习框架。首先，一个在仅基于人类大规模运动数据上训练的GMT策略，作为生成类人全身运动的与任务无关的基础。然后，学习一个高效但精确的残差策略来优化GMT输出，以改善运动并融入物体交互。为了进一步促进高效训练，我们设计了(i)基于点云的物体跟踪奖励，以实现更平滑的优化；(ii)接触奖励，鼓励精确的人形机器人-物体交互；(iii)基于课程的虚拟物体控制器，以稳定早期训练。我们在模拟和真实的Unitree G1人形机器人上评估了ResMimic。结果表明，与强大的基线相比，ResMimic在任务成功率、训练效率和鲁棒性方面都有显著提升。视频可在https://resmimic.github.io/ 查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid whole-body loco-manipulation promises transformative capabilitiesfor daily service and warehouse tasks. While recent advances in general motiontracking (GMT) have enabled humanoids to reproduce diverse human motions, thesepolicies lack the precision and object awareness required forloco-manipulation. To this end, we introduce ResMimic, a two-stage residuallearning framework for precise and expressive humanoid control from humanmotion data. First, a GMT policy, trained on large-scale human-only motion,serves as a task-agnostic base for generating human-like whole-body movements.An efficient but precise residual policy is then learned to refine the GMToutputs to improve locomotion and incorporate object interaction. To furtherfacilitate efficient training, we design (i) a point-cloud-based objecttracking reward for smoother optimization, (ii) a contact reward thatencourages accurate humanoid body-object interactions, and (iii) acurriculum-based virtual object controller to stabilize early training. Weevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Resultsshow substantial gains in task success, training efficiency, and robustnessover strong baselines. Videos are available at https://resmimic.github.io/ .</description>
      <author>example@mail.com (Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan)</author>
      <guid isPermaLink="false">2510.05070v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Discrete scalar curvature as a weighted sum of Ollivier-Ricci curvatures</title>
      <link>http://arxiv.org/abs/2510.04936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了离散几何中Ricci曲率和标量曲率之间的关系，针对点云和图结构定义了Ollivier-Ricci曲率的标量版本，并证明了其在特定条件下的收敛性质。&lt;h4&gt;背景&lt;/h4&gt;在离散几何中，传统的Ricci曲率和标量曲率需要被离散化以应用于点云和图结构。Ollivier-Ricci曲率是Ricci曲率的一种离散形式，而标量曲率在Riemannian流形中是Ricci曲率的迹。&lt;h4&gt;目的&lt;/h4&gt;定义一种离散的标量曲率版本，研究其与连续标量曲率的关系，并探索Ollivier-Ricci曲率到Ricci曲率的收敛性。&lt;h4&gt;方法&lt;/h4&gt;通过将标量曲率视为Ricci曲率的迹，定义了Ollivier-Ricci曲率的标量版本。研究最近邻图采样自流形时的收敛性，并证明Ollivier-Ricci曲率到Ricci曲率的收敛结果。&lt;h4&gt;主要发现&lt;/h4&gt;所定义的标量Ollivier-Ricci曲率对于通过从流形采样获得的最近邻图会收敛到标量曲率。同时证明了关于Ollivier-Ricci曲率收敛到Ricci曲率的新结果。&lt;h4&gt;结论&lt;/h4&gt;离散几何中可以合理地定义标量曲率概念，并且这些离散概念在适当的条件下会收敛到其连续对应物，为离散几何分析提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了为点云和图定义的Ricci曲率和标量曲率的离散类比之间的关系。在离散设置中，Ricci曲率被Ollivier-Ricci曲率所替代。标量曲率可以作为Riemannian流形上Ricci曲率的迹来计算；这促使我们定义了一种新的Ollivier-Ricci曲率的标量版本。我们展示了我们的定义对于通过从流形采样获得的最近邻图会收敛到标量曲率。我们还证明了一些关于Ollivier-Ricci曲率收敛到Ricci曲率的新结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the relationship between discrete analogues of Ricci and scalarcurvature that are defined for point clouds and graphs. In the discretesetting, Ricci curvature is replaced by Ollivier-Ricci curvature. Scalarcurvature can be computed as the trace of Ricci curvature for a Riemannianmanifold; this motivates a new definition of a scalar version of Ollivier-Riccicurvature. We show that our definition converges to scalar curvature fornearest neighbor graphs obtained by sampling from a manifold. We also provesome new results about the convergence of Ollivier-Ricci curvature to Riccicurvature.</description>
      <author>example@mail.com (Abigail Hickok, Andrew J. Blumberg)</author>
      <guid isPermaLink="false">2510.04936v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert</title>
      <link>http://arxiv.org/abs/2510.03896v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种以可泛化动作专家为中心的新框架，通过稀疏3D轨迹作为中间表示，连接VLM的高级规划能力和低级物理动作模块，解决了传统VLA模型泛化能力差和双系统方法中语义歧义问题。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language Models (VLM)虽展现出强大的规划和推理能力，但在物理世界中应用面临挑战。传统Vision-Language-Action (VLA)模型受限于稀缺窄域数据导致泛化能力差；近期双系统方法虽解耦了思考与行动，但受限于动作模块的语义歧义，使得大规模跨任务训练不可行，且系统间合作机制不明确。&lt;h4&gt;目的&lt;/h4&gt;解决传统VLA模型和双系统方法的局限性，提高系统在新环境中的适应性和泛化能力，并明确系统间的合作机制。&lt;h4&gt;方法&lt;/h4&gt;引入以可泛化动作专家为中心的框架，使用稀疏3D轨迹作为中间表示；在规划阶段，VLM生成粗略3D路径点，由动作专家通过采样环境实时点云观测细化为密集可执行动作序列；提出'动作预训练，点云微调'新范式，结合VLM的广泛泛化能力和动作专家的细粒度动作级别泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;新方法通过解耦高级规划和低级执行，有效解决了传统方法的语义歧义问题，提高了训练效率和鲁棒泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功结合了VLM在视觉理解和规划方面的广泛泛化能力与动作专家的细粒度、动作级别的泛化能力，为物理世界中的智能行动提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型(VLM)已展示了令人印象深刻的规划和推理能力，但这些能力在物理世界中的应用带来了重大挑战。整合推理和动作为一体的传统视觉语言动作(VLA)模型泛化能力差，受限于稀缺的窄域数据。虽然最近的双系统方法试图将'思考'与'行动'解耦，但它们常受限于动作模块内的语义歧义。这种歧义使得大规模、跨任务训练不可行。因此，这些系统通常在部署到新环境时需要对新收集的数据进行微调，且两个系统之间的合作机制仍然定义不明确。为解决这些局限性，我们首次引入了一个以可泛化动作专家为中心的框架。我们的方法使用稀疏3D轨迹作为中间表示，有效地连接了VLM的高级规划能力和低级物理动作模块。在规划阶段，VLM只需生成粗略的3D路径点。然后，这些路径点由我们的可泛化动作专家处理，通过采样环境的实时点云观测将其细化为密集的、可执行的动作序列。为提高训练效率和鲁棒泛化能力，我们引入了一种新颖的'动作预训练，点云微调'范式。我们的方法结合了VLM在视觉理解和规划方面的广泛泛化能力与动作专家的细粒度、动作级别泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型（VLM）难以将强大的规划和推理能力转化为实际物理行动的问题。传统方法要么将推理和动作整合到单一架构中导致泛化能力差，要么试图分离'思考'和'行动'但面临语义歧义问题。这个问题在现实中非常重要，因为它限制了AI系统在机器人控制、自动驾驶等需要物理交互领域的应用，使VLM的能力主要局限于数字世界而无法影响物理环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统VLA模型受限于稀缺数据导致泛化差，而双系统方法虽然分离了'思考'和'行动'，但动作模块仍需解释复杂语义信息，限制了大规模训练。作者借鉴了双系统框架和中间表示的概念，但创新性地使用稀疏3D轨迹作为明确接口，减轻了动作专家的语义负担。他们还参考了扩散模型架构，但提出了'动作预训练，点云微调'的新范式，使动作专家专注于几何精炼而非语义解释。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过稀疏3D轨迹作为高阶VLM规划和低阶动作专家之间的明确接口，实现真正的解耦。VLM只需生成简单的几何坐标（3D路径点），动作专家则专注于将这些路径点精炼为可执行动作序列。整体流程是：1) VLM接收语言指令和视觉输入，预测稀疏3D路径点；2) 将路径点从相机坐标系转换到机器人基座坐标系，用B样条插值形成连续轨迹；3) 动作专家接收引导姿态和实时点云观测，精炼为密集动作序列；4) 采用'动作预训练，点云微调'范式进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入围绕可泛化动作专家的框架，使用稀疏3D轨迹作为清晰接口；2) 提出'动作预训练，点云微调'训练策略；3) 在相机坐标系中直接预测路径点，保留VLM的视觉先验知识；4) 结合多种数据源并使用先进技术提高点云质量。相比之前的工作，本文方法完全解耦了规划和执行，解决了动作专家的语义负担问题，实现了真正的零样本部署，无需任务特定微调即可适应新环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于可泛化动作专家的新框架，通过稀疏3D轨迹作为接口解耦视觉语言模型的规划能力与低级物理动作，实现了无需任务特定微调的强泛化机器人控制。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Vision-Language Models (VLM) have demonstrated impressive planningand reasoning capabilities, translating these abilities into the physical worldintroduces significant challenges. Conventional Vision-Language-Action (VLA)models, which integrate reasoning and action into a monolithic architecture,generalize poorly because they are constrained by scarce, narrow-domain data.While recent dual-system approaches attempt to decouple "thinking" from"acting", they are often constrained by semantic ambiguities within the actionmodule. This ambiguity makes large-scale, cross-task training infeasible.Consequently, these systems typically necessitate fine-tuning on newlycollected data when deployed to novel environments, and the cooperationmechanism between the two systems remains ill-defined. To address theselimitations, we introduce, for the first time, a framework centered around ageneralizable action expert. Our approach utilizes sparse 3D trajectories as anintermediate representation, effectively bridging the high-level planningcapabilities of the VLM with the low-level physical action module. During theplanning phase, the VLM is only required to generate coarse 3D waypoints. Thesewaypoints are then processed by our generalizable action expert, which refinesthem into dense, executable action sequences by sampling real-time point cloudobservations of the environment. To promote training efficiency and robustgeneralization, we introduce a novel "Action Pre-training, PointcloudFine-tuning" paradigm. Our method combines the broad generalizationcapabilities of VLMs in visual understanding and planning with thefine-grained, action-level generalization of action expert.</description>
      <author>example@mail.com (Mingyu Liu, Zheng Huang, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Yating Wang, Haoyi Zhu, Hao Chen, Chunhua Shen)</author>
      <guid isPermaLink="false">2510.03896v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Identification in source apportionment using geometry</title>
      <link>http://arxiv.org/abs/2510.03616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究解决了源解析分析中非负矩阵分解的非唯一性问题，提出了在更弱条件下可识别的源归因百分比矩阵估计方法，并通过几何估计量实现了一致的源解析结果。&lt;h4&gt;背景&lt;/h4&gt;源解析分析旨在量化多种空气污染物观测浓度对特定来源的归因，传统方法通常使用非负矩阵分解(NMF)，但NMF具有非唯一性，依赖于不可验证的假设如稀疏性和难以解释的缩放。&lt;h4&gt;目的&lt;/h4&gt;建立源归因百分比矩阵在更弱和更现实条件下的可识别性，提出无需稀疏性或参数分布假设的几何估计方法。&lt;h4&gt;方法&lt;/h4&gt;将数据视为锥形锥中的一个点云，开发几何估计量来估计源归因百分比矩阵，该方法能够适应时空依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;源归因百分比矩阵的总体水平估计量是尺度不变的，即使NMF因子不可识别时也可识别；几何估计量是一致的，无需稀疏性或参数分布假设。&lt;h4&gt;结论&lt;/h4&gt;所提出的几何估计方法在源解析分析中具有理论优势，数值实验验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;源解析分析旨在量化观测到的多种空气污染物浓度对特定来源的归因，可以表述为非负矩阵分解问题。然而，NMF是非唯一的，通常依赖于不可验证的假设，如稀疏性和难以解释的缩放。在本研究中，我们在更弱和更现实的条件下建立了源归因百分比矩阵的可识别性。我们引入了这个矩阵的总体水平估计量，并证明它是尺度不变的，即使NMF因子不可识别时也是如此。将数据视为锥形锥中的一个点云，我们证明了源归因百分比矩阵的几何估计量是一致的，无需任何稀疏性或参数分布假设，同时能够适应时空依赖性。数值实验验证了这一理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source apportionment analysis, which aims to quantify the attribution ofobserved concentrations of multiple air pollutants to specific sources, can beformulated as a non-negative matrix factorization (NMF) problem. However, NMFis non-unique and typically relies on unverifiable assumptions such as sparsityand uninterpretable scalings. In this manuscript, we establish identifiabilityof the source attribution percentage matrix under much weaker and morerealistic conditions. We introduce the population-level estimand for thismatrix, and show that it is scale-invariant and identifiable even when the NMFfactors are not. Viewing the data as a point cloud in a conical hull, we showthat a geometric estimator of the source attribution percentage matrix isconsistent without any sparsity or parametric distributional assumptions, andwhile accommodating spatio-temporal dependence. Numerical experimentscorroborate the theory.</description>
      <author>example@mail.com (Bora Jin, Abhirup Datta)</author>
      <guid isPermaLink="false">2510.03616v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Platonic Transformers: A Solid Choice For Equivariance</title>
      <link>http://arxiv.org/abs/2510.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Platonic Transformer的新型模型，解决了Transformer在处理几何对称性方面的局限性，同时保持了其高效性和灵活性。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型虽然广泛应用，但缺乏科学和计算机视觉中常见的几何对称性的归纳偏置。现有的等变方法通常通过复杂、计算密集型设计牺牲了Transformer的高效性和灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决等变性与效率/灵活性之间的权衡问题，开发一种能够处理几何对称性同时保持Transformer优势的模型。&lt;h4&gt;方法&lt;/h4&gt;引入Platonic Transformer，通过定义相对于柏拉图立体对称群参考框架的注意力机制，诱导出有原则的权重共享方案，实现结合连续平移和柏拉图对称性的等变性，同时保留标准Transformer的精确架构和计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;该注意力机制在形式上等价于动态群卷积，使模型能够学习自适应几何滤波器，并实现高度可扩展的线性时间卷积变体。&lt;h4&gt;结论&lt;/h4&gt;在计算机视觉(CIFAR-10)、3D点云(ScanObjectNN)和分子属性预测(QM9, OMol25)等多样化基准测试中，Platonic Transformer通过利用几何约束实现了具有竞争力的性能，且没有额外计算成本。&lt;h4&gt;翻译&lt;/h4&gt;尽管Transformer应用广泛，但缺乏科学和计算机视觉中常见的几何对称性的归纳偏置。现有的等变方法通常通过复杂、计算密集型设计牺牲了Transformer的高效性和灵活性。我们引入Platonic Transformer来解决这种权衡。通过定义相对于柏拉图立体对称群参考框架的注意力，我们的方法诱导出一种有原则的权重共享方案。这实现了对连续平移和柏拉图对称性的组合等变性，同时保留了标准Transformer的精确架构和计算成本。此外，我们证明这种注意力在形式上等价于动态群卷积，这揭示了模型学习自适应几何滤波器的能力，并实现了一种高度可扩展的线性时间卷积变体。在计算机视觉(CIFAR-10)、3D点云(ScanObjectNN)和分子属性预测(QM9, OMol25)等多样化基准测试中，Platonic Transformer通过利用这些几何约束以无额外成本的方式实现了具有竞争力的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是如何在保持Transformer计算效率和灵活性的同时，为其添加几何对称性（等变性）的问题。这个问题在科学和计算机视觉领域非常重要，因为许多问题（如分子性质预测、3D点云处理）具有几何对称性，尊重这些对称性可以显著提高模型的性能、数据效率和鲁棒性，但现有的等变方法通常牺牲了Transformer的计算效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了Transformer在处理几何对称性方面的局限性，以及现有等变方法的不足。他们借鉴了Rotary Position Embeddings (RoPE)的工作，它已经为注意力机制提供了平移等变性；同时也借鉴了群等变卷积网络的思想。作者的核心思路是通过定义相对于柏拉图立体（Platonic solid）对称群参考帧的注意力，引入几何对称性，同时保持标准Transformer的计算图不变。他们通过权重共享和等变性线性层约束实现了这一目标。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入柏拉图立体的对称群作为参考帧，在保持标准Transformer架构的同时添加几何等变性。实现流程包括：1) 将输入特征提升为在群G上的函数；2) 修改RoPE操作使其依赖于参考帧；3) 在多个参考帧上并行计算注意力分数；4) 应用softmax得到注意力系数；5) 计算每个参考帧的输出；6) 确保所有线性层都是等变的；7) 选择柏拉图立体的对称群作为参考帧。整个过程保持了标准Transformer的计算图和计算成本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次在不改变标准Transformer架构的情况下实现几何等变性；2) 使用柏拉图立体的对称群作为结构化参考帧；3) 设计了权重共享的RoPE注意力机制；4) 揭示了注意力与动态群卷积的等价性；5) 提出了线性时间复杂度的卷积变体。相比之前的工作，Platonic Transformer不需要修改底层注意力机制，避免了计算开销，同时实现了完全等变的注意力（而非不变性注意力），并以更低成本超过了帧平均方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Platonic Transformer通过引入柏拉图立体对称群的参考帧和权重共享机制，首次在不改变标准Transformer架构和计算成本的情况下，实现了对连续平移和离散旋转-反射的等变性，解决了等变性与计算效率之间的权衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While widespread, Transformers lack inductive biases for geometric symmetriescommon in science and computer vision. Existing equivariant methods oftensacrifice the efficiency and flexibility that make Transformers so effectivethrough complex, computationally intensive designs. We introduce the PlatonicTransformer to resolve this trade-off. By defining attention relative toreference frames from the Platonic solid symmetry groups, our method induces aprincipled weight-sharing scheme. This enables combined equivariance tocontinuous translations and Platonic symmetries, while preserving the exactarchitecture and computational cost of a standard Transformer. Furthermore, weshow that this attention is formally equivalent to a dynamic group convolution,which reveals that the model learns adaptive geometric filters and enables ahighly scalable, linear-time convolutional variant. Across diverse benchmarksin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecularproperty prediction (QM9, OMol25), the Platonic Transformer achievescompetitive performance by leveraging these geometric constraints at noadditional cost.</description>
      <author>example@mail.com (Mohammad Mohaiminul Islam, Rishabh Anand, David R. Wessels, Friso de Kruiff, Thijs P. Kuipers, Rex Ying, Clara I. Sánchez, Sharvaree Vadgama, Georg Bökman, Erik J. Bekkers)</author>
      <guid isPermaLink="false">2510.03511v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching</title>
      <link>http://arxiv.org/abs/2510.03460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的机器人运动生成方法，利用流匹配模型以单视角点云为条件学习优化初始化，解决了传统方法在高维空间和复杂环境中的效率问题。&lt;h4&gt;背景&lt;/h4&gt;快速机器人运动生成在人机协作系统中至关重要，机器人需要实时响应动态环境并重新规划运动以确保安全交互和高效任务执行。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于采样的运动规划器难以扩展到高维配置空间和基于优化的规划器初始化敏感的问题，提高机器人运动生成的效率和成功率。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于流匹配模型的学习方法，以单视角点云为条件学习优化初始化的次优解，直接从单视角深度相机输入生成可行轨迹，无需先验环境知识。&lt;h4&gt;主要发现&lt;/h4&gt;在UR5e机械臂上的模拟研究表明，所提出的生成初始化器本身具有高成功率，显著提高了轨迹优化的成功率，需要更少的优化迭代，并对未见环境表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地解决了机器人运动生成中的初始化问题，提高了轨迹优化的效率和成功率，具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在杂乱工作空间中对UR5e机械臂进行的模拟研究表明，所提出的生成初始化器本身具有高成功率，与传统和基于学习的基准初始化器相比显著提高了轨迹优化的成功率，需要更少的优化迭代，并对未见环境表现出强大的泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人运动规划中的初始化问题。优化-based方法对初始化敏感，容易陷入局部最小值；而采样-based方法在高维空间扩展困难，生成的路径需要后处理。这个问题在人机协作系统中至关重要，因为机器人需要实时响应动态环境，持续观察并重新规划运动，确保安全互动和高效执行任务，特别是在共享工作空间高度动态的场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统运动规划方法的局限性，然后回顾了现有的神经增强方法，包括偏置采样、模仿学习、生成模型和优化热启动等。他们借鉴了Flow Matching这一生成模型，它学习从简单分布到数据分布的最优直线转换，比扩散模型需要更少的生成步骤。结合PointNet++处理点云数据和SE-Transformer架构，设计了一个条件Flow Matching模型，以单视角点云为条件，无需障碍物位置的先验知识。同时使用了cuRobo优化器进行并行优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用Flow Matching模型学习生成近优的轨迹初始化种子，同时生成多个多样性的种子，提高优化成功率并减少迭代次数。方法以单视角点云为条件，无需障碍物先验知识。整体流程：输入单视角点云、起始和目标配置；使用PointNet++提取点云特征；通过SE-Transformer处理轨迹表示和条件信息；通过Flow Matching生成多个初始种子；使用cuRobo优化器并行优化；输出平滑、无碰撞的可行轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于Flow Matching的随机神经初始化器，可同时生成多个多样性的近优种子；2) 以单视角点云为条件，无需障碍物先验知识；3) 结合GPU加速优化器支持批量并行优化；4) 在复杂环境中展示强大泛化能力。相比之前的工作，它比传统初始化方法显著提高成功率和收敛速度；比确定性神经初始化器能生成多样种子避免局部最小值；比扩散模型推理速度更快；比需要环境先验知识的方法可直接从深度观测生成轨迹。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于点云条件Flow Matching的机器人运动规划初始化方法，通过生成多个多样性的近优种子显著提高了轨迹优化的成功率和收敛速度，使机器人能够在动态环境中快速、安全地重新规划运动。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)systems, as robots need to respond to dynamic environments in real time bycontinuously observing their surroundings and replanning their motions toensure both safe interactions and efficient task execution. Currentsampling-based motion planners face challenges in scaling to high-dimensionalconfiguration spaces and often require post-processing to interpolate andsmooth the generated paths, resulting in time inefficiency in complexenvironments. Optimization-based planners, on the other hand, can incorporatemultiple constraints and generate smooth trajectories directly, making thempotentially more time-efficient. However, optimization-based planners aresensitive to initialization and may get stuck in local minima. In this work, wepresent a novel learning-based method that utilizes a Flow Matching modelconditioned on a single-view point cloud to learn near-optimal solutions foroptimization initialization. Our method does not require prior knowledge of theenvironment, such as obstacle locations and geometries, and can generatefeasible trajectories directly from single-view depth camera input. Simulationstudies on a UR5e robotic manipulator in cluttered workspaces demonstrate thatthe proposed generative initializer achieves a high success rate on its own,significantly improves the success rate of trajectory optimization comparedwith traditional and learning-based benchmark initializers, requires feweroptimization iterations, and exhibits strong generalization to unseenenvironments.</description>
      <author>example@mail.com (Sibo Tian, Minghui Zheng, Xiao Liang)</author>
      <guid isPermaLink="false">2510.03460v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Unified Unsupervised Anomaly Detection via Matching Cost Filtering</title>
      <link>http://arxiv.org/abs/2510.03363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  63 pages (main paper and supplementary material), 39 figures, 58  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine  Intelligence (TPAMI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一成本过滤(UCF)框架，用于增强无监督异常检测(UAD)方法，解决了匹配噪声问题，统一了单模态和多模态UAD场景。&lt;h4&gt;背景&lt;/h4&gt;无监督异常检测(UAD)仅使用正常训练数据识别异常，广泛应用于工业检测和医疗分析等领域，但异常样本稀缺。现有方法基于重建或嵌入，进行图像或特征级匹配，但匹配噪声被忽视，限制了检测能力。从单模态RGB UAD发展到多模态场景(RGB-3D, RGB-Text)，但这些研究方向相互隔离。&lt;h4&gt;目的&lt;/h4&gt;从匹配角度提出统一单模态和多模态UAD，开发一种通用的后处理优化框架来增强任何UAD模型的异常检测能力。&lt;h4&gt;方法&lt;/h4&gt;提出统一成本过滤(UCF)框架，通过构建成本体积将测试样本与正常样本进行匹配，然后使用具有来自测试样本多层注意力指导的可学习过滤模块，减轻匹配噪声并突出细微异常。&lt;h4&gt;主要发现&lt;/h4&gt;在22个多样化基准测试上进行的综合实验证明，UCF能够增强各种UAD方法，在单模态(RGB)和多模态(RGB-3D, RGB-Text)UAD场景中持续达到新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;UCF框架有效解决了UAD中的匹配噪声问题，统一了单模态和多模态UAD方法，代码和模型将在https://github.com/ZHE-SAPI/CostFilter-AD发布。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常检测(UAD)旨在仅使用正常训练数据识别图像和像素级异常，广泛应用于工业检测和医疗分析等领域，在这些领域中由于隐私问题和冷启动限制，异常样本稀缺。现有方法，无论是基于重建(恢复正常对应物)还是基于嵌入(预训练表示)，本质上都进行图像或特征级匹配以生成异常图。尽管如此，匹配噪声在很大程度上被忽视，限制了它们的检测能力。超越早期对单模态RGB-based UAD的关注，最近的进展扩展到多模态场景，例如RGB-3D和RGB-Text，这得益于点云传感和视觉-语言模型。尽管存在共同挑战，但这些研究方向在很大程度上相互隔离，阻碍了全面理解和知识转移。在本文中，我们从匹配角度倡导统一单模态和多模态设置的UAD。在此见解下，我们提出了统一成本过滤(UCF)，这是一个通用的后处理优化框架，用于优化任何UAD模型的异常成本体积。成本体积是通过将测试样本与来自相同或不同模态的正常样本进行匹配而构建的，然后是一个可学习的过滤模块，具有来自测试样本的多层注意力指导，减轻匹配噪声并突出细微异常。在22个多样化基准测试上的综合实验证明了UCF在增强各种UAD方法方面的有效性，在单模态(RGB)和多模态(RGB-3D, RGB-Text)UAD场景中持续达到新的最先进结果。代码和模型将在https://github.com/ZHE-SAPI/CostFilter-AD发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督异常检测（UAD）中的匹配噪声问题。这个问题在现实中非常重要，因为UAD广泛应用于工业检测和医疗分析等领域，这些领域由于隐私问题和冷启动约束，异常样本稀缺。匹配噪声会导致边界模糊、假阳性和假阴性，特别是对于细微异常、低对比度或接近正常区域的问题，严重影响检测准确性。同时，随着UAD从单模态扩展到多模态场景，这些挑战变得更加复杂，但不同模态的研究大多孤立进行，阻碍了全面理解和知识转移。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从匹配的视角重新概念化了单模态和多模态UAD，意识到匹配噪声是一个被忽视但关键的因素。他们受到立体匹配、深度估计、光流估计和光场渲染等领域中匹配成本过滤（成本体积过滤）概念的启发，将UAD重新构建为三步范式：特征提取、异常成本体积构建和异常成本体积过滤。他们借鉴了现有工作中的预训练模型（如CLIP、PointMAE）进行特征提取，多模态表示和特征匹配技术，以及3D U-Net架构和注意力机制，但创新性地将这些元素整合到一个统一的框架中专门解决匹配噪声问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将异常检测重新概念化为匹配成本过滤过程，通过多模板和多模态匹配构建异常成本体积，然后使用双流注意力引导的过滤网络来减少匹配噪声，同时保留边缘结构和细微异常的线索。整体实现流程分为四步：1) 特征提取：使用模态特定的预训练编码器从输入和参考模板中提取层次特征；2) 异常成本体积构建：通过执行补丁级内模态或跨模态匹配来构建多层成本体积；3) 异常成本体积过滤：引入一个过滤网络，以从测试样本中获得的多层注意力引导，逐步以粗到细的方式聚合多个模板的证据；4) 检测输出生成：通过沿匹配维度进行全局最小池化，然后通过卷积层和softmax生成正常-异常得分图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 重新概念化单模态和多模态UAD，明确解决内在匹配噪声问题；2) 提出统一成本过滤（UCF）框架，使用多层输入观察作为注意力查询来指导匹配去噪；3) 作为通用即插即用方法，可以灵活地构建和过滤来自RGB特征以及重建或基于嵌入的RGB、文本和点云表示的匹配成本体积；4) 提出类感知适配器，使用软分类logits动态调整分割损失，优先处理具有挑战性的样本并提高泛化能力。相比之前的工作，本文扩展到了多模态场景（RGB-3D和RGB-Text），而之前大多只关注单模态RGB UAD；本文明确解决了被忽略的匹配噪声问题；提供了一个统一的视角和方法，而之前的方法通常各自独立发展。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了统一成本过滤（UCF）框架，通过减少匹配噪声并利用多模态信息，显著提升了无监督异常检测在单模态和多模态场景下的性能，为异常检测提供了一个通用的即插即用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomaly detection (UAD) aims to identify image- and pixel-levelanomalies using only normal training data, with wide applications such asindustrial inspection and medical analysis, where anomalies are scarce due toprivacy concerns and cold-start constraints. Existing methods, whetherreconstruction-based (restoring normal counterparts) or embedding-based(pretrained representations), fundamentally conduct image- or feature-levelmatching to generate anomaly maps. Nonetheless, matching noise has been largelyoverlooked, limiting their detection ability. Beyond earlier focus on unimodalRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3Dand RGB--Text, enabled by point cloud sensing and vision--language models.Despite shared challenges, these lines remain largely isolated, hindering acomprehensive understanding and knowledge transfer. In this paper, we advocateunified UAD for both unimodal and multimodal settings in the matchingperspective. Under this insight, we present Unified Cost Filtering (UCF), ageneric post-hoc refinement framework for refining anomaly cost volume of anyUAD model. The cost volume is constructed by matching a test sample againstnormal samples from the same or different modalities, followed by a learnablefiltering module with multi-layer attention guidance from the test sample,mitigating matching noise and highlighting subtle anomalies. Comprehensiveexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF inenhancing a variety of UAD methods, consistently achieving new state-of-the-artresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UADscenarios. Code and models will be released athttps://github.com/ZHE-SAPI/CostFilter-AD.</description>
      <author>example@mail.com (Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu)</author>
      <guid isPermaLink="false">2510.03363v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps</title>
      <link>http://arxiv.org/abs/2510.02274v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Diffusion^2是一种基于扩散的射频信号传播建模方法，使用3D点云支持从Wi-Fi到毫米波的广泛频率范围，能够准确预测复杂环境中的RF信号行为。&lt;h4&gt;背景&lt;/h4&gt;射频信号传播建模对于理解环境至关重要，能提供比RGB相机更有价值的洞察，RGB相机受限于可见光谱、镜头覆盖和遮挡。RF信号还支持无线诊断、部署和优化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测复杂环境中射频信号传播的方法，解决信号与障碍物相互作用（如吸收和反射）带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出Diffusion^2方法，使用3D点云建模RF信号传播；开发RF-3D编码器从3D数据中捕获RF相关特征，封装3D几何复杂性和信号特定细节；通过多尺度嵌入模拟实际RF信号传播过程。&lt;h4&gt;主要发现&lt;/h4&gt;Diffusion^2能够准确估计各种频率波段和环境条件下的RF信号行为，误差仅为1.9 dB，比现有方法快27倍，标志着该领域的重大进步。&lt;h4&gt;结论&lt;/h4&gt;Diffusion^2为射频信号传播建模提供了有效解决方案，在准确性和计算效率方面均有显著提升。&lt;h4&gt;翻译&lt;/h4&gt;射频信号传播建模对于理解环境至关重要，因为RF信号能提供超越RGB相机能力范围的宝贵见解，RGB相机受限于可见光谱、镜头覆盖和遮挡。RF信号传播建模也有助于支持无线诊断、部署和优化。然而，由于与障碍物的相互作用（如吸收和反射），在复杂环境中准确预测RF信号仍然是一个挑战。我们提出了Diffusion^2，一种基于扩散的方法，使用3D点云建模从Wi-Fi到毫米波广泛频率范围内的RF信号传播。为了从3D数据中有效捕获RF相关特征，我们提出了RF-3D编码器，它封装了3D几何的复杂性以及信号特定细节。这些特征经过多尺度嵌入，以模拟实际的RF信号传播过程。基于合成和真实世界测量的评估表明，Diffusion^2准确估计了各种频率波段和环境条件下的RF信号行为，误差仅为1.9 dB，比现有方法快27倍，标志着该领域的重大进步。更多信息请访问https://rfvision-project.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确预测复杂3D环境中的射频(RF)信号传播问题。这个问题很重要，因为RF信号能提供比RGB相机更丰富的环境洞察，对无线网络优化、智能环境部署和物联网应用等至关重要。现有方法要么需要大量预测量数据，要么计算效率低下，难以在实际应用中部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到生成式AI最新进展(如Sora)的启发，思考是否将生成式AI扩展到可见光谱外用于RF预测。他们选择扩散模型作为基础，因为其能处理环境不确定性和提供良好可控性。作者创新设计了RF-3D编码器和RF-3D配对块，借鉴了扩散模型在图像生成领域的成功应用和3D场景理解技术，结合了物理模型和机器学习方法的优势，针对RF信号预测问题进行了专门改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用扩散模型将3D环境模型转换为RF热图，通过将复杂信号传播问题分解为多个去噪步骤来学习RF传播模式。整体流程包括：1)用智能手机采集3D环境模型和少量RF预测量数据；2)通过RF-3D编码器提取3D几何、2D图像和RF信号特征；3)执行前向扩散过程(添加噪声)和反向扩散过程(条件引导去噪)；4)使用RF-3D配对块融合特征与预测；5)训练网络并生成RF热图；6)对动态场景扩展为视频扩散。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个基于扩散模型的RF信号传播估计方法；2)RF-3D编码器有效提取多模态特征；3)RF-3D配对块实现跨模态融合；4)支持视频扩散适应动态环境；5)高精度且仅需少量预测量数据。相比之前工作，Diffusion2数据需求大幅降低(只需15个测量点vs数千个)，计算效率提高27倍以上，支持多频率和动态场景，无需环境变化时重新训练，泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Diffusion2首次将扩散模型应用于3D环境的射频信号传播预测，通过创新的RF-3D编码器和配对块设计，实现了仅需少量预测量数据的高精度、高效率RF热图生成，支持多频率和动态场景，为无线网络优化和智能环境部署提供了革命性工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling radio frequency (RF) signal propagation is essential forunderstanding the environment, as RF signals offer valuable insights beyond thecapabilities of RGB cameras, which are limited by the visible-light spectrum,lens coverage, and occlusions. It is also useful for supporting wirelessdiagnosis, deployment, and optimization. However, accurately predicting RFsignals in complex environments remains a challenge due to interactions withobstacles such as absorption and reflection. We introduce Diffusion^2, adiffusion-based approach that uses 3D point clouds to model the propagation ofRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.To effectively capture RF-related features from 3D data, we present the RF-3DEncoder, which encapsulates the complexities of 3D geometry along withsignal-specific details. These features undergo multi-scale embedding tosimulate the actual RF signal dissemination process. Our evaluation, based onsynthetic and real-world measurements, demonstrates that Diffusion^2 accuratelyestimates the behavior of RF signals in various frequency bands andenvironmental conditions, with an error margin of just 1.9 dB and 27x fasterthan existing methods, marking a significant advancement in the field. Refer tohttps://rfvision-project.github.io/ for more information.</description>
      <author>example@mail.com (Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang)</author>
      <guid isPermaLink="false">2510.02274v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment</title>
      <link>http://arxiv.org/abs/2510.03335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了扩散模型在点云数据训练中的旋转对齐问题，分析了Kabsch-Umeyama算法对齐步骤的有效性，并提出了一种在噪声水平较低时对最优去噪器的更好近似方法。&lt;h4&gt;背景&lt;/h4&gt;扩散模型是一类流行的生成模型，通过逆转从目标数据分布开始的加噪过程进行训练。当为分子和蛋白质等点云训练扩散模型时，通常无法分配规范的方向。为了捕获这种对称性，真实数据样本通常通过从SO(3)均匀采样的随机旋转进行增强。然后，在计算损失之前，去噪预测通常通过Kabsch-Umeyama算法旋转对齐到真实样本。&lt;h4&gt;目的&lt;/h4&gt;理解旋转对齐步骤对扩散模型训练的影响，并探索在噪声水平较低时对最优去噪器的更好近似方法。&lt;h4&gt;方法&lt;/h4&gt;研究表明最优去噪器可以用SO(3)上的矩阵Fisher分布表示。对齐对应于采样该分布的模式，并且对于小噪声水平而言是对齐的零阶近似。基于这一观点，研究人员推导出在噪声水平极限下对最优去噪器的更好近似方法。&lt;h4&gt;主要发现&lt;/h4&gt;对齐步骤对应于矩阵Fisher分布的采样模式；对于小噪声水平，对齐是零阶近似，解释了其有效性；提出了在噪声水平较低时对最优去噪器的更好近似方法。&lt;h4&gt;结论&lt;/h4&gt;对齐步骤对于扩散模型训练中最相关的噪声水平来说通常是一个'足够好'的近似。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型是一类流行的生成模型，经过训练可以逆转从目标数据分布开始的加噪过程。训练扩散模型包括学习如何在不同的噪声水平上去噪样本。当为分子和蛋白质等点云训练扩散模型时，通常无法分配规范的方向。为了捕获这种对称性，真实数据样本通常通过从SO(3)均匀采样的随机旋转进行增强。然后，在计算损失之前，去噪预测通常通过Kabsch-Umeyama算法旋转对齐到真实样本。然而，这种对齐步骤的影响尚未得到充分研究。在这里，我们表明最优去噪器可以用SO(3)上的矩阵Fisher分布表示。对齐对应于采样该分布的模式，并且对于小噪声水平而言是对齐的零阶近似，解释了其有效性。我们基于这一观点推导出在噪声水平极限下对最优去噪器的更好近似方法。我们的实验强调，对于扩散模型训练中最相关的噪声水平，对齐通常是一个'足够好'的近似。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云扩散模型训练中的旋转对齐问题。在处理分子、蛋白质等3D点云数据时，没有标准方向可分配，通常通过随机旋转增强数据，并在训练中使用Kabsch-Umeyama算法对齐去噪预测与真实样本。这个问题很重要，因为旋转对称性处理不当会影响模型性能，而现有方法缺乏对对齐步骤效果的理论分析，不清楚它是否引入偏差以及如何改进。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了旋转增强数据上训练扩散模型的挑战，推导了最优去噪器的数学表达式，证明其可表示为SO(3)上矩阵Fisher分布的期望。他们发现传统旋转对齐对应于该分布的众数，是小噪声水平下的零阶近似。基于此，他们使用Laplace方法推导了更高阶的校正项。作者借鉴了Karras等人的扩散模型理论、Kabsch-Umeyama算法、矩阵Fisher分布研究以及在分子结构上应用扩散模型的相关工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是最优去噪器可表示为旋转矩阵的期望，该旋转服从矩阵Fisher分布；传统对齐使用分布众数作为近似，而作者添加更高阶校正项以更好近似最优去噪器。流程包括：1)获取并旋转增强点云数据；2)在不同噪声水平下添加噪声；3)使用神经网络学习去噪器；4)计算最优旋转矩阵R*；5)应用零阶(直接对齐)、一阶(R*+σ²B1)或二阶(R*+σ²B1+σ⁴B2)校正；6)使用改进损失函数训练网络；7)训练好的模型用于生成新点云结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次分析旋转对齐的理论基础，证明最优去噪器是矩阵Fisher分布的期望；2)提出高阶校正方法改进对齐估计；3)通过数值实验和实际应用验证方法。相比之前工作，本文提供了对齐步骤的理论解释，分析了其潜在偏差，提出了自适应的、基于噪声水平的对齐改进方法，而非使用固定策略，同时保持了计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过理论分析揭示了点云扩散模型中旋转对齐的本质，并提出了一种基于矩阵Fisher分布的高阶校正方法，在保持计算效率的同时改进了去噪性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models are a popular class of generative models trained to reversea noising process starting from a target data distribution. Training adiffusion model consists of learning how to denoise noisy samples at differentnoise levels. When training diffusion models for point clouds such as moleculesand proteins, there is often no canonical orientation that can be assigned. Tocapture this symmetry, the true data samples are often augmented bytransforming them with random rotations sampled uniformly over $SO(3)$. Then,the denoised predictions are often rotationally aligned via the Kabsch-Umeyamaalgorithm to the ground truth samples before computing the loss. However, theeffect of this alignment step has not been well studied. Here, we show that theoptimal denoiser can be expressed in terms of a matrix Fisher distribution over$SO(3)$. Alignment corresponds to sampling the mode of this distribution, andturns out to be the zeroth order approximation for small noise levels,explaining its effectiveness. We build on this perspective to derive betterapproximators to the optimal denoiser in the limit of small noise. Ourexperiments highlight that alignment is often a `good enough' approximation forthe noise levels that matter most for training diffusion models.</description>
      <author>example@mail.com (Ameya Daigavane, YuQing Xie, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Tess Smidt)</author>
      <guid isPermaLink="false">2510.03335v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</title>
      <link>http://arxiv.org/abs/2510.05034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 1st version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文首次全面审视了Video-LMMs（视频大型多模态模型）的训练后方法，提供了结构化分类法和关键设计原则，旨在推进视频理解领域的研究与实践。&lt;h4&gt;背景&lt;/h4&gt;视频理解是计算机视觉中最具挑战性的前沿领域，需要模型推理复杂的时空关系、长期依赖和多模态证据。Video-LMMs整合视觉编码器与基于解码器的强大语言模型，已在视频理解任务中展现出显著能力。&lt;h4&gt;目的&lt;/h4&gt;提供对Video-LMMs训练后方法的首次全面检查，涵盖三个基本支柱：监督微调与思维链、可验证目标的强化学习以及通过增强推理计算进行的测试时扩展。&lt;h4&gt;方法&lt;/h4&gt;提出结构化分类法，阐明这些技术的作用、相互联系和视频特定适应，并解决时间定位、时空接地、长视频效率和 multimodal 证据集成等独特挑战。系统分析代表性方法，综合关键设计原则、见解和评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;确定了在奖励设计、可扩展性和成本性能优化方面的关键开放挑战。整理了必要的基准、数据集和指标，以促进对训练后效果的严格评估。&lt;h4&gt;结论&lt;/h4&gt;该调查为研究人员和从业者提供了一个统一的框架，以推进Video-LMM的能力。相关资源和更新可在GitHub仓库获取。&lt;h4&gt;翻译&lt;/h4&gt;视频理解代表了计算机视觉中最具挑战性的前沿，需要模型能够推理复杂的时空关系、长期依赖和多模态证据。最近出现的Video-LMMs（视频大型多模态模型）将视觉编码器与强大的基于解码器的语言模型相结合，在视频理解任务中展示了非凡能力。然而，将这些模型从基本感知系统转变为复杂推理引擎的关键阶段（训练后）在文献中仍然分散。本调查首次对Video-LMMs的训练后方法进行了全面检查，涵盖三个基本支柱：思维链监督微调（SFT）、可验证目标的强化学习（RL）以及通过增强推理计算进行的测试时扩展（TTS）。我们提出了一个结构化分类法，阐明了这些技术的作用、相互联系和视频特定适应，解决了时间定位、时空接地、长视频效率和 multimodal 证据集成等独特挑战。通过对代表性方法的系统分析，我们综合了关键设计原则、见解和评估协议，同时确定了在奖励设计、可扩展性和成本性能优化方面的关键开放挑战。我们还整理了必要的基准、数据集和指标，以促进对训练后效果的严格评估。本调查旨在为研究人员和从业者提供一个统一的框架，以推进Video-LMM的能力。额外资源和更新保存在：https://github.com/yunlong10/Awesome-Video-LMM-Post-Training&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding represents the most challenging frontier in computervision, requiring models to reason about complex spatiotemporal relationships,long-term dependencies, and multimodal evidence. The recent emergence ofVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoderswith powerful decoder-based language models, has demonstrated remarkablecapabilities in video understanding tasks. However, the critical phase thattransforms these models from basic perception systems into sophisticatedreasoning engines, post-training, remains fragmented across the literature.This survey provides the first comprehensive examination of post-trainingmethodologies for Video-LMMs, encompassing three fundamental pillars:supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)from verifiable objectives, and test-time scaling (TTS) through enhancedinference computation. We present a structured taxonomy that clarifies theroles, interconnections, and video-specific adaptations of these techniques,addressing unique challenges such as temporal localization, spatiotemporalgrounding, long video efficiency, and multimodal evidence integration. Throughsystematic analysis of representative methods, we synthesize key designprinciples, insights, and evaluation protocols while identifying critical openchallenges in reward design, scalability, and cost-performance optimization. Wefurther curate essential benchmarks, datasets, and metrics to facilitaterigorous assessment of post-training effectiveness. This survey aims to provideresearchers and practitioners with a unified framework for advancing Video-LMMcapabilities. Additional resources and updates are maintained at:https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</description>
      <author>example@mail.com (Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu)</author>
      <guid isPermaLink="false">2510.05034v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting</title>
      <link>http://arxiv.org/abs/2510.04401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉语言模型在多种任务上表现出色，但在计数物体方面存在局限性，特别是在多种形状组合的情况下。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型已成为AI社区的关注焦点，它们在从网络上获取的大规模视觉语言数据上训练后展现出了令人印象深刻的能力，在图像理解、视频理解、复杂视觉推理和具身AI等多种任务上表现出色。&lt;h4&gt;目的&lt;/h4&gt;探究视觉语言模型是否能正确计数物体。&lt;h4&gt;方法&lt;/h4&gt;引入了VLMCountBench基准测试，采用极简设置只包含基本几何形状及其组合，专注于计数任务；采用严格的独立变量控制，系统地研究颜色、大小和提示改进等简单属性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;当只有一种形状类型存在时，VLMs能够可靠计数；但当多种形状类型组合时(即组合计数)，它们表现出重大失败。&lt;h4&gt;结论&lt;/h4&gt;这揭示了当前视觉语言模型的基本经验局限性，并为未来研究指明了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型已成为当今AI社区的关注焦点，它们在从网络上获取的大规模视觉语言数据上训练后展现出了令人印象深刻的能力。这些模型在图像理解、视频理解、复杂视觉推理和具身AI等多种任务上表现出色。尽管取得了这些显著成功，一个基本问题仍然存在：视觉语言模型能否正确计数物体？在本文中，我们引入了一个简单而有效的基准测试VLMCountBench，在极简设置下设计，仅包含基本几何形状(如三角形、圆形)及其组合，专注于计数任务而不受其他因素干扰。我们采用严格的独立变量控制，并在消融研究中系统地研究了颜色、大小和提示改进等简单属性的影响。我们的实证结果表明，当只有一种形状类型存在时，VLMs能够可靠计数，但当多种形状类型组合时(即组合计数)，它们表现出重大失败。这突显了当前VLMs的基本经验局限性，并为未来研究指明了重要方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have become a central focus of today's AIcommunity, owing to their impressive abilities gained from training onlarge-scale vision-language data from the Web. These models have demonstratedstrong performance across diverse tasks, including image understanding, videounderstanding, complex visual reasoning, and embodied AI. Despite thesenoteworthy successes, a fundamental question remains: Can VLMs count objectscorrectly? In this paper, we introduce a simple yet effective benchmark,VLMCountBench, designed under a minimalist setting with only basic geometricshapes (e.g., triangles, circles) and their compositions, focusing exclusivelyon counting tasks without interference from other factors. We adopt strictindependent variable control and systematically study the effects of simpleproperties such as color, size, and prompt refinement in a controlled ablation.Our empirical results reveal that while VLMs can count reliably when only oneshape type is present, they exhibit substantial failures when multiple shapetypes are combined (i.e., compositional counting). This highlights afundamental empirical limitation of current VLMs and motivates importantdirections for future research.</description>
      <author>example@mail.com (Xuyang Guo, Zekai Huang, Zhenmei Shi, Zhao Song, Jiahao Zhang)</author>
      <guid isPermaLink="false">2510.04401v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation</title>
      <link>http://arxiv.org/abs/2510.04290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://research.nvidia.com/labs/toronto-ai/chronoedit&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ChronoEdit框架，将图像编辑重新构建为视频生成问题，通过时间推理确保编辑后物体的物理一致性，在视觉保真度和物理可能性方面超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;大型生成模型在图像编辑和上下文图像生成方面取得了显著进展，但在确保物理一致性方面存在关键差距，编辑后的物体必须保持连贯性，这对世界模拟相关任务尤为重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够确保编辑物体物理一致性的图像编辑框架，特别适用于需要物理一致性的场景。&lt;h4&gt;方法&lt;/h4&gt;ChronoEdit框架将输入和编辑后的图像视为视频的第一帧和最后一帧，利用预训练视频生成模型捕捉物体的外观和隐含物理规律；引入时间推理阶段，通过推理令牌联合去噪目标帧，想象合理的编辑轨迹约束解决方案；几步后丢弃推理令牌以避免全视频渲染的高计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;作者引入了PBench-Edit基准测试，包含需要物理一致性的图像-提示对，实验证明ChronoEdit在视觉保真度和物理可能性方面均超越了最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;ChronoEdit通过将图像编辑视为视频生成问题，有效解决了图像编辑中的物理一致性问题，框架的14B和2B变体代码和模型将在项目页面发布。&lt;h4&gt;翻译&lt;/h4&gt;最近大型生成模型的进展显著推动了图像编辑和上下文图像生成，但在确保物理一致性方面仍存在关键差距，编辑后的物体必须保持连贯。这种能力对世界模拟相关任务尤为重要。本文提出了ChronoEdit框架，将图像编辑重新构建为视频生成问题。首先，ChronoEdit将输入和编辑后的图像视为视频的第一帧和最后一帧，使其能够利用大型预训练视频生成模型，这些模型不仅捕捉物体外观，还通过学习的时间一致性捕获隐含的运动和交互物理规律。其次，ChronoEdit引入了时间推理阶段，在推理时显式执行编辑。在此设置下，目标帧与推理令牌联合去噪，想象合理的编辑轨迹，将解决方案空间约束为物理可行的变换。几步后丢弃推理令牌以避免全视频渲染的高计算成本。为验证ChronoEdit，我们引入了PBench-Edit，一个新的需要物理一致性的上下文图像-提示对基准，并证明ChronoEdit在视觉保真度和物理可能性方面均超越了最先进的基线方法。ChronoEdit的14B和2B变体的代码和模型将在项目页面发布：https://research.nvidia.com/labs/toronto-ai/chronoedit&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large generative models have significantly advanced imageediting and in-context image generation, yet a critical gap remains in ensuringphysical consistency, where edited objects must remain coherent. Thiscapability is especially vital for world simulation related tasks. In thispaper, we present ChronoEdit, a framework that reframes image editing as avideo generation problem. First, ChronoEdit treats the input and edited imagesas the first and last frames of a video, allowing it to leverage largepretrained video generative models that capture not only object appearance butalso the implicit physics of motion and interaction through learned temporalconsistency. Second, ChronoEdit introduces a temporal reasoning stage thatexplicitly performs editing at inference time. Under this setting, the targetframe is jointly denoised with reasoning tokens to imagine a plausible editingtrajectory that constrains the solution space to physically viabletransformations. The reasoning tokens are then dropped after a few steps toavoid the high computational cost of rendering a full video. To validateChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs forcontexts that require physical consistency, and demonstrate that ChronoEditsurpasses state-of-the-art baselines in both visual fidelity and physicalplausibility. Code and models for both the 14B and 2B variants of ChronoEditwill be released on the project page:https://research.nvidia.com/labs/toronto-ai/chronoedit</description>
      <author>example@mail.com (Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling)</author>
      <guid isPermaLink="false">2510.04290v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs</title>
      <link>http://arxiv.org/abs/2510.03955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 9 figures, 6 tables. Presents TimeWarp, a synthetic  preference data framework to improve temporal understanding in Video-LLMs,  showing consistent gains across seven benchmarks. Includes supplementary  material in the Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TimeWarp方法，通过创建有针对性的合成时间数据集来微调视频大语言模型(Video-LLMs)，提高其在时间理解任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在一般视频理解基准测试中表现良好，但在需要细粒度时间理解的任务上表现不佳。这种局限性源于当前微调数据集中缺乏视觉复杂性和时间细微差别，导致模型过度依赖基于语言的推理。&lt;h4&gt;目的&lt;/h4&gt;创建一个系统性的方法来生成针对性的合成时间数据集，微调Video-LLMs的响应，使其专注于给定的输入视频，提高模型对视频动态的时间理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出TimeWarp方法，创建了一个大规模的偏好数据集，捕捉通常被忽视的复杂时间动态，将模型响应基于视觉和时间信息进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;将TimeWarp方法应用于现有模型后，显著提高了时间理解基准测试的性能，在七个基准测试中实现了性能的绝对提升。&lt;h4&gt;结论&lt;/h4&gt;TimeWarp创建的数据集在推进Video-LLMs的时间理解方面是有效的，能够帮助模型更好地关注视频内容而非过度依赖语言推理。&lt;h4&gt;翻译&lt;/h4&gt;虽然视频大语言模型(Video-LLMs)在一般视频理解基准测试中表现出色，特别是在视频字幕和描述任务上，但在需要细粒度时间理解的任务上却表现不佳。这种局限性源于当前微调数据集中缺乏视觉复杂性和时间细微差别，导致这些模型过度依赖基于语言的推理，而不是真正理解视频动态。在这项工作中，我们提出了TimeWarp，这是一种系统性的方法，用于创建有针对性的合成时间数据集，以微调模型响应，鼓励模型专注于给定的输入视频。我们介绍了一个使用TimeWarp创建的大规模偏好数据集，捕捉了通常被忽视的复杂时间动态，将模型响应基于视觉和时间信息。我们证明，当我们的方法应用于现有模型时，它显著提高了时间理解基准测试的性能，突显了我们提出的数据集在推进Video-LLMs时间理解方面的有效性，在七个基准测试中实现了性能的绝对提升。代码可在https://github.com/sameepv21/timewarp获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Video Large Language Models (Video-LLMs) have demonstrated remarkableperformance across general video understanding benchmarks-particularly in videocaptioning and descriptive tasks-they consistently underperform on tasks thatrequire fine-grained temporal understanding. This limitation arises due to thelack of visual complexity and temporal nuance in current fine-tuning datasets,leading these models to rely heavily on language-based reasoning rather thantruly understanding video dynamics. In this work, we propose TimeWarp, asystematic method to create a targeted synthetic temporal dataset to fine-tunethe model's responses to encourage it to focus on the given input video. Weintroduce a large-scale preference dataset, created using TimeWarp, thatcaptures intricate temporal dynamics often overlooked, grounding the model'sresponses to visual and temporal information. We demonstrate that when ourmethod is applied to existing models, it significantly improves performance ontemporal understanding benchmarks, highlighting the effectiveness of ourproposed datasets in advancing temporal understanding in Video-LLMs, resultingin an absolute improvement in performance across seven benchmarks. Code isavailable at https://github.com/sameepv21/timewarp.</description>
      <author>example@mail.com (Sameep Vani, Shreyas Jena, Maitreya Patel, Chitta Baral, Somak Aditya, Yezhou Yang)</author>
      <guid isPermaLink="false">2510.03955v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiSwap for Zero-Shot Robot Imitation Learning</title>
      <link>http://arxiv.org/abs/2510.03706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Video link:  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为EmbodiSwap的方法，用于在人类视频上生成逼真的合成机器人覆盖层，并利用这种方法进行零样本模仿学习，成功训练了闭环机器人操作策略，创新性地将V-JEPA视觉模型应用于机器人模仿学习领域。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域，存在野生自中心人类视频与目标机器人 embodiment 之间的差距，需要有效的方法来弥合这一差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成逼真合成机器人覆盖层的方法，并利用该方法进行零样本模仿学习训练，实现机器人操作策略的有效学习。&lt;h4&gt;方法&lt;/h4&gt;提出EmbodiSwap方法生成合成机器人覆盖层，在生成的数据上训练闭环机器人操作策略，创新性地使用V-JEPA作为视觉主干，将其从视频理解领域重新应用于合成机器人视频的模仿学习。&lt;h4&gt;主要发现&lt;/h4&gt;采用V-JEPA作为视觉主干比传统机器人领域使用的替代视觉主干表现更好；在真实世界测试中，零样本训练的V-JEPA模型达到82%的成功率，优于少样本训练的π₀网络以及在EmbodiSwap生成的数据上训练的π₀。&lt;h4&gt;结论&lt;/h4&gt;EmbodiSwap方法结合V-JEPA视觉模型在机器人模仿学习领域取得了显著成果，通过发布代码、数据集和模型检查点，促进了该领域的可复现研究和广泛采用。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了EmbodiSwap - 一种用于在人类视频上生成逼真合成机器人覆盖层的方法。我们将EmbodiSwap用于零样本模仿学习，弥合了野生自中心人类视频与目标机器人 embodiment 之间的差距。我们在EmbodiSwap生成的数据上训练闭环机器人操作策略。我们创新性地使用V-JEPA作为视觉主干，将V-JEPA从视频理解领域重新用于合成机器人视频的模仿学习。采用V-JEPA比机器人领域更传统使用的替代视觉主干表现更好。在真实世界测试中，我们的零样本训练的V-JEPA模型达到82%的成功率，优于少样本训练的π₀网络以及在EmbodiSwap生成的数据上训练的π₀。我们发布了(i)用于生成合成机器人覆盖层的代码，输入为人类视频和任意机器人URDF，生成机器人数据集，(ii)在EPIC-Kitchens、HOI4D和Ego4D上合成的机器人数据集，以及(iii)模型检查点和推理代码，以促进可复现研究和广泛采用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce EmbodiSwap - a method for producing photorealistic syntheticrobot overlays over human video. We employ EmbodiSwap for zero-shot imitationlearning, bridging the embodiment gap between in-the-wild ego-centric humanvideo and a target robot embodiment. We train a closed-loop robot manipulationpolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as avisual backbone, repurposing V-JEPA from the domain of video understanding toimitation learning over synthetic robot videos. Adoption of V-JEPA outperformsalternative vision backbones more conventionally used within robotics. Inreal-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ successrate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$trained over data produced by EmbodiSwap. We release (i) code for generatingthe synthetic robot overlays which takes as input human videos and an arbitraryrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesizeover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inferencecode, to facilitate reproducible research and broader adoption.</description>
      <author>example@mail.com (Eadom Dessalene, Pavan Mantripragada, Michael Maynord, Yiannis Aloimonos)</author>
      <guid isPermaLink="false">2510.03706v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>FrameOracle: Learning What to See and How Much to See in Videos</title>
      <link>http://arxiv.org/abs/2510.03584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FrameOracle是一个轻量级即插即用模块，能够智能预测哪些帧与给定查询最相关以及需要多少帧，解决了视觉-语言模型在视频理解中受限于输入帧数的问题。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)在视频理解方面取得了进展，但其性能受限于能够处理的输入帧数。现有的帧采样策略如均匀选择或固定预算选择无法适应信息密度或任务复杂度的变化，导致效率低下和信息丢失。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级即插即用模块，用于预测(1)哪些帧与给定查询最相关，以及(2)需要多少帧，以提高视频理解的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;FrameOracle使用四阶段课程进行训练，前三个阶段依赖弱代理信号如跨模态相似性，最后阶段利用新数据集FrameOracle-41K的更强监督，这是首个提供关键帧标注的大型VideoQA集合，指定了回答每个问题所需的最小帧集。&lt;h4&gt;主要发现&lt;/h4&gt;在五个VLMs和六个基准测试上的实验表明，FrameOracle将16帧输入减少到平均10.4帧而没有任何精度损失；当从64帧候选开始时，将输入减少到平均13.9帧，同时提高精度1.4%，实现了可扩展视频理解的最先进效率-精度权衡。&lt;h4&gt;结论&lt;/h4&gt;FrameOracle通过智能选择相关帧和确定所需帧数，解决了现有视频理解模型中帧采样策略的局限性，实现了更高的效率和更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已推进视频理解，但其性能受限于它们可以处理的输入帧数。现有的帧采样策略，如均匀选择或固定预算选择，往往无法适应信息密度或任务复杂度的变化，导致效率低下和信息丢失。为解决这一问题，我们提出了FrameOracle，这是一个轻量级且即插即用的模块，可预测(1)哪些帧与给定查询最相关，以及(2)需要多少帧。FrameOracle使用四阶段课程进行训练，前三个阶段依赖弱代理信号如跨模态相似性。在最后阶段，它利用我们引入的新数据集FrameOracle-41K的更强监督，这是首个提供关键帧标注的大型VideoQA集合，指定了回答每个问题所需的最小帧集。在五个VLMs和六个基准测试上的广泛实验表明，FrameOracle将16帧输入减少到平均10.4帧而没有任何精度损失。当从64帧候选开始时，它将输入减少到平均13.9帧，同时提高精度1.4%，实现了可扩展视频理解的最先进效率-精度权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have advanced video understanding, but theirperformance is limited by the number of input frames they can process. Existingframe sampling strategies, such as uniform or fixed-budget selection, oftenfail to adapt to variations in information density or task complexity,resulting in inefficiency and information loss. To address this, we presentFrameOracle, a lightweight and plug-and-play module that predicts both (1)which frames are most relevant to a given query and (2) how many frames areneeded. FrameOracle is trained using a four-stage curriculum, with the firstthree stages relying on weak proxy signals such as cross-modal similarity. Inthe final stage, it leverages stronger supervision from a new dataset weintroduce, FrameOracle-41K, the first large-scale VideoQA collection to providekeyframe annotations specifying the minimal set of frames required to answereach question. Extensive experiments across five VLMs and six benchmarksdemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4frames without any loss in accuracy. When starting from 64-frame candidates, itreduces the input to an average of 13.9 frames while improving accuracy by1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalablevideo understanding.</description>
      <author>example@mail.com (Chaoyu Li, Tianzhi Li, Fei Tao, Zhenyu Zhao, Ziqian Wu, Maozheng Zhao, Juntong Song, Cheng Niu, Pooyan Fazli)</author>
      <guid isPermaLink="false">2510.03584v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Generalization of Graph Neural Network Models for Distribution Grid Fault Detection</title>
      <link>http://arxiv.org/abs/2510.03571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been submitted and accepted for IEEE SmartGridComm  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了电力配电网故障检测中不同图神经网络架构的性能比较，探索了GraphSAGE和图注意力网络在RNN+GNN管道中的应用，并评估了它们在不同拓扑设置中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;电力配电网的故障检测对确保系统可靠性和防止昂贵停电至关重要。故障检测方法需要对不断变化的电网拓扑结构保持鲁棒性，这些变化可能由重构、设备故障和分布式能源资源集成等因素引起。&lt;h4&gt;目的&lt;/h4&gt;系统地、一致地比较各种GNN架构在RNN+GNN管道模型中的性能，特别是探索GraphSAGE和图注意力网络在故障诊断中的应用，并评估这些模型在不同拓扑设置中的泛化潜力。&lt;h4&gt;方法&lt;/h4&gt;在IEEE 123节点配网络上进行实验，使用RNN+GNN管道模型，比较了包括GraphSAGE、图注意力网络(GAT, GATv2)、RGCN和纯RNN模型(特别是GRU)在内的多种架构的性能。&lt;h4&gt;主要发现&lt;/h4&gt;RGATv2模型展现出优异的泛化能力，在不同拓扑设置中F1分数仅下降约12%；相比之下，纯RNN模型F1分数下降高达约60%，其他RGNN变体F1分数下降约25%。&lt;h4&gt;结论&lt;/h4&gt;Graph注意力网络，特别是RGATv2，在电力配电网故障检测任务中表现出色，具有良好的泛化能力，能够适应不同的电网拓扑结构变化。&lt;h4&gt;翻译&lt;/h4&gt;电力配电网中的故障检测对确保系统可靠性和防止昂贵停电至关重要。此外，故障检测方法应对由重构、设备故障和分布式能源资源集成等因素引起的不断变化的电网拓扑结构保持鲁棒性。当前最先进的数据驱动方法使用循环神经网络进行时间建模，图神经网络进行空间学习，在RNN+GNN管道设置中。具体而言，对于电力系统故障诊断，已经采用了图卷积网络。然而，在电力系统领域之外，已经提出了和采用了各种更先进的GNN架构。在本文中，我们系统地、一致地比较了各种GNN架构在RNN+GNN管道模型中的性能。具体来说，据我们所知，我们首次提出在RGNN中使用GraphSAGE和图注意力进行故障诊断，以及与之前提出的RGNN解决方案和纯RNN模型进行全面比较，特别是探索它们在不同部署环境中的泛化潜力。我们在IEEE 123节点配电网上的实验结果表明，RGATv2具有优异的泛化能力，在不同拓扑设置中保持高性能，F1分数仅下降约12%。相比之下，纯RNN模型 largely 失败，F1分数下降高达约60%，而其他RGNN变体也表现出显著的性能下降，即F1分数低至约25%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fault detection in power distribution grids is critical for ensuring systemreliability and preventing costly outages. Moreover, fault detectionmethodologies should remain robust to evolving grid topologies caused byfactors such as reconfigurations, equipment failures, and Distributed EnergyResource (DER) integration. Current data-driven state-of-the-art methods useRecurrent Neural Networks (RNNs) for temporal modeling and Graph NeuralNetworks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN inshort). Specifically, for power system fault diagnosis, Graph ConvolutionalNetworks (GCNs) have been adopted. Yet, various more advanced GNN architectureshave been proposed and adopted in domains outside of power systems. In thispaper, we set out to systematically and consistently benchmark various GNNarchitectures in an RNN+GNN pipeline model. Specifically, to the best of ourknowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensivebenchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNNmodels (especially Gated Recurrent Unit (GRU)), particularly (iii) exploringtheir generalization potential for deployment in different settings than thoseused for training them. Our experimental results on the IEEE 123-nodedistribution network show that RGATv2 has superior generalization capabilities,maintaining high performance with an F1-score reduction of $\sim$12% acrossdifferent topology settings. In contrast, pure RNN models largely fail,experiencing an F1-score reduction of up to $\sim$60%, while other RGNNvariants also exhibit significant performance degradation, i.e., up to$\sim$25% lower F1-scores.</description>
      <author>example@mail.com (Burak Karabulut, Carlo Manna, Chris Develder)</author>
      <guid isPermaLink="false">2510.03571v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>SegMASt3R: Geometry Grounded Segment Matching</title>
      <link>http://arxiv.org/abs/2510.05051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The Thirty-Ninth Annual Conference on Neural Information  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用3D基础模型的空间理解能力来处理宽基线分割匹配的方法，能够在高达180度视点变化的图像对之间匹配分割区域。&lt;h4&gt;背景&lt;/h4&gt;分割匹配是计算机视觉中的重要中间任务，它在图像之间建立语义或几何上一致区域的对应关系。与关键点匹配不同，分割匹配捕捉结构化区域，对遮挡、光照变化和视点变化具有更强的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;解决宽基线分割匹配这一具有挑战性的任务，涉及极端视点变化下的图像分割区域匹配问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种利用3D基础模型归纳偏置的架构，能够在图像对之间匹配分割区域，支持高达180度的视点变化。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet++和Replica数据集上，与SAM2视频传播器和局部特征匹配方法等最先进方法相比，在AUPRC指标上提高了多达30%&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在宽基线分割匹配任务上优于现有方法，并在3D实例分割和图像目标导航等下游任务中展示了优势&lt;h4&gt;翻译&lt;/h4&gt;分割匹配是计算机视觉中的一个重要中间任务，它在图像之间建立语义或几何上一致区域的对应关系。与关键点匹配不同，分割匹配捕捉结构化区域，对遮挡、光照变化和视点变化具有更强的鲁棒性。在本文中，我们利用3D基础模型的空间理解能力来处理宽基线分割匹配，这是一个涉及极端视点变化的挑战性任务。我们提出了一种架构，利用这些3D基础模型的归纳偏置来匹配图像对之间的分割区域，支持高达180度的视点变化。大量实验表明，在ScanNet++和Replica数据集上，与SAM2视频传播器和局部特征匹配方法等最先进方法相比，我们的方法在AUPRC指标上提高了多达30%。我们进一步展示了所提出模型在相关下游任务中的优势，包括3D实例分割和图像目标导航。项目页面：https://segmast3r.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决宽基线(wide-baseline)图像片段匹配问题，即在不同视角的图像之间建立语义或几何上一致区域的对应关系。这个问题在现实中很重要，因为它支撑着视频目标跟踪、场景图构建和机器人导航等关键应用；在研究中也很重要，因为现有方法在极端视角变化(如180度旋转)下性能急剧下降，而片段匹配比关键点匹配对遮挡、光照变化和视角变化具有更强的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法在宽基线条件下的局限性，认识到需要利用几何先验来处理视角变化。他们选择3D基础模型MASt3R作为基础，因为它已经学习了场景的深度、形状和姿态等几何属性。设计上，他们冻结了MASt3R的骨干网络，添加了一个轻量级的片段特征头将块级特征转换为片段级描述符，并借鉴了SuperGlue的对比损失函数和可微匹配层设计。此外，他们还利用了Segment Anything Model(SAM)来获取片段掩码，并参考了Sinkhorn算法用于软对应关系计算。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D基础模型MASt3R的几何理解能力来处理宽基线片段匹配，通过添加一个适配器将MASt3R的块级特征转换为片段级特征，并使用可微的最优传输层建立片段对应关系。整体流程：1)输入一对宽基线图像；2)使用冻结的MASt3R提取块级特征；3)通过片段特征头将块级特征转换为片段级描述符；4)使用可微的最优传输层匹配片段，包括计算相似度、添加dustbin机制、使用Sinkhorn算法获得软对应关系；5)通过行级argmax获得最终匹配结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将3D基础模型MASt3R重新用于片段匹配任务；2)设计片段特征头将像素级表示转换为片段级描述符；3)引入可微的最优传输层和dustbin机制处理不匹配情况；4)实现端到端训练。相比之前的工作，不同之处在于：相比局部特征匹配方法，它匹配结构化区域而非点，对遮挡和外观变化更鲁棒；相比SAM2，它在极端视角变化下显式强制执行几何一致性；相比基于2D监督的方法，它利用3D几何先验处理近180度的视角变化；在AUPRC指标上比最先进方法高出最多30%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SegMASt3R通过利用3D基础模型的几何理解能力，实现了在极端视角变化下(最高180度旋转)的鲁棒图像片段匹配，显著提升了宽基线场景下的性能，并在3D实例映射和物体相对导航等下游任务中展现出实用价值。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segment matching is an important intermediate task in computer vision thatestablishes correspondences between semantically or geometrically coherentregions across images. Unlike keypoint matching, which focuses on localizedfeatures, segment matching captures structured regions, offering greaterrobustness to occlusions, lighting variations, and viewpoint changes. In thispaper, we leverage the spatial understanding of 3D foundation models to tacklewide-baseline segment matching, a challenging setting involving extremeviewpoint shifts. We propose an architecture that uses the inductive bias ofthese 3D foundation models to match segments across image pairs with up to 180degree view-point change. Extensive experiments show that our approachoutperforms state-of-the-art methods, including the SAM2 video propagator andlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++and Replica datasets. We further demonstrate benefits of the proposed model onrelevant downstream tasks, including 3D instance segmentation and image-goalnavigation. Project Page: https://segmast3r.github.io/</description>
      <author>example@mail.com (Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna)</author>
      <guid isPermaLink="false">2510.05051v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Achieve Gold Medal Performance at International Astronomy &amp; Astrophysics Olympiad</title>
      <link>http://arxiv.org/abs/2510.05016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures, to be submitted, comments are welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究评估了五种先进大型语言模型在国际天文学和天体物理学奥林匹克竞赛中的表现，发现这些模型在理论考试中接近人类顶尖水平，但在数据分析考试中表现差异大，且在概念推理、几何推理和空间可视化方面存在共同弱点。&lt;h4&gt;背景&lt;/h4&gt;当前针对特定任务的示范应用显示大型语言模型在自动化天文学研究任务方面有早期成功，但仅提供了解决天文学问题所需能力的不完整视角。现有基准测试主要关注简单问答，测试天文学知识，未能评估现实研究中的复杂推理能力。&lt;h4&gt;目的&lt;/h4&gt;通过在国际天文学和天体物理学奥林匹克竞赛考试上系统评估五种先进大型语言模型，填补现有评估方法的空白，因为这些考试检验深入概念理解、多步推导和多模态分析能力。&lt;h4&gt;方法&lt;/h4&gt;在国际天文学和天体物理学奥林匹克竞赛(IOAA)考试上对五种最先进的大型语言模型进行系统性基准测试，IOAA考试旨在检验深入概念理解、多步推导和多模态分析能力。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini 2.5 Pro和GPT-5平均得分分别为85.6%和84.2%，达到金牌水平，在理论考试中排名人类参赛者前两名；在数据分析考试中，GPT-5表现最佳(88.5%)，其他模型性能下降至48-76%；所有模型在概念推理、几何推理和空间可视化方面存在共同弱点(准确率52-79%)。&lt;h4&gt;结论&lt;/h4&gt;尽管大型语言模型在理论考试中接近顶尖人类水平，但在它们能够作为天文学自主研究代理之前，必须解决关键差距。&lt;h4&gt;翻译&lt;/h4&gt;虽然针对特定任务的示范应用显示将大型语言模型应用于自动化一些天文学研究任务方面显示出早期成功，但它们仅提供了解决天文学问题所需全部能力的不完整视角，这需要对大型语言模型的优势和局限性有更全面的理解。迄今为止，现有的基准测试和评估主要集中在简单的问答上，主要测试天文学知识，而未能评估该学科现实研究中所需的复杂推理能力。在这里，我们通过在国际天文学和天体物理学奥林匹克竞赛(IOAA)考试上系统性地对五种最先进的大型语言模型进行基准测试来解决这一差距，这些考试旨在检验深入的概念理解、多步推导和多模态分析能力。凭借85.6%和84.2%的平均分，Gemini 2.5 Pro和GPT-5（两种表现最佳的模型）不仅达到金牌水平，而且在所有四个评估的IOAA理论考试（2022-2025年）中排名在约200-300名参赛者的前两名。相比之下，数据分析考试的结果显示出更多差异。GPT-5在这些考试中仍然表现出色，平均得分为88.5%，在最近四届IOAA中排名前10，而其他模型的性能下降到48-76%。此外，我们深入的错误分析强调了概念推理、几何推理和空间可视化（52-79%的准确率）是所有大型语言模型中的一致性弱点。因此，尽管大型语言模型在理论考试中接近顶尖人类水平，但在它们能够作为天文学自主研究代理之前，必须解决关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While task-specific demonstrations show early success in applying largelanguage models (LLMs) to automate some astronomical research tasks, they onlyprovide incomplete views of all necessary capabilities in solving astronomyproblems, calling for more thorough understanding of LLMs' strengths andlimitations. So far, existing benchmarks and evaluations focus on simplequestion-answering that primarily tests astronomical knowledge and fails toevaluate the complex reasoning required for real-world research in thediscipline. Here, we address this gap by systematically benchmarking fivestate-of-the-art LLMs on the International Olympiad on Astronomy andAstrophysics (IOAA) exams, which are designed to examine deep conceptualunderstanding, multi-step derivations, and multimodal analysis. With averagescores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performingmodels) not only achieve gold medal level performance but also rank in the toptwo among ~200-300 participants in all four IOAA theory exams evaluated(2022-2025). In comparison, results on the data analysis exams show moredivergence. GPT-5 still excels in the exams with an 88.5% average score,ranking top 10 among the participants in the four most recent IOAAs, whileother models' performances drop to 48-76%. Furthermore, our in-depth erroranalysis underscores conceptual reasoning, geometric reasoning, and spatialvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,although LLMs approach peak human performance in theory exams, critical gapsmust be addressed before they can serve as autonomous research agents inastronomy.</description>
      <author>example@mail.com (Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun)</author>
      <guid isPermaLink="false">2510.05016v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2510.04759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://yanchi-3dv.github.io/PG-Occ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PG-Occ是一种创新的渐进式高斯变换器框架，用于开放词汇的3D占用率预测，通过渐进式在线密集化和各向异性感知采样策略解决了稀疏和密集表示之间的权衡问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;3D占用率预测在基于视觉的自动驾驶系统中扮演重要角色。传统方法局限于固定语义类别，而近期方法转向预测文本对齐特征以支持开放词汇文本查询。然而，文本对齐场景建模存在权衡：稀疏高斯表示难以捕捉小物体，密集表示则带来显著计算开销。&lt;h4&gt;目的&lt;/h4&gt;解决文本对齐场景建模中的权衡问题，实现开放词汇的3D占用率预测，同时能够捕捉细粒度的场景细节并有效控制计算开销。&lt;h4&gt;方法&lt;/h4&gt;提出PG-Progressive Gaussian Transformer Framework，采用渐进式在线密集化策略逐步增强3D高斯表示，并引入各向异性感知采样策略结合时空融合，自适应地为不同尺度和阶段的高斯分配感受野，实现更有效的特征聚合和更丰富的场景信息捕获。&lt;h4&gt;主要发现&lt;/h4&gt;通过迭代增强表示，框架实现了越来越精确和详细的场景理解。在广泛评估中，PG-Occ实现了最先进的性能，相比之前最佳方法相对提高了14.3%的mIoU。&lt;h4&gt;结论&lt;/h4&gt;PG-Occ成功解决了3D占用率预测中开放词汇表示与计算效率之间的权衡问题，为自动驾驶系统提供了更精确的场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，3D占用率预测任务取得了显著进展，在基于视觉的自动驾驶系统中发挥着关键作用。虽然传统方法仅限于固定的语义类别，但近期方法已转向预测文本对齐的特征，以支持在真实场景中进行开放词汇的文本查询。然而，在文本对齐的场景建模中存在权衡：稀疏高斯表示难以捕捉场景中的小物体，而密集表示则会带来显著的计算开销。为解决这些局限性，我们提出了PG-Occ，一种创新的渐进式高斯变换器框架，实现了开放词汇的3D占用率预测。我们的框架采用渐进式在线密集化，这是一种前馈策略，能够逐步增强3D高斯表示以捕捉细粒度的场景细节。通过迭代增强表示，该框架实现了越来越精确和详细的场景理解。另一个关键贡献是引入了具有时空融合的各向异性感知采样策略，该策略自适应地为不同尺度和阶段的高斯分配感受野，实现更有效的特征聚合和更丰富的场景信息捕获。通过广泛评估，我们证明PG-Occ实现了最先进的性能，比之前最佳方法相对提高了14.3%的mIoU。代码和预训练模型将在我们的项目页面上发布时公开：https://yanchi-3dv.github.io/PG-Occ&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D占用预测中的开放词汇语义识别问题。传统方法只能识别预定义的物体类别，无法应对现实世界中多样化的物体。在自动驾驶领域，系统需要能够识别和定位各种可能的物体，包括那些未预先定义的类别，这对安全决策至关重要。此外，现有方法在稀疏表示（计算效率高但细节不足）和密集表示（细节丰富但计算开销大）之间存在权衡，难以同时满足效率和精度的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：固定语义类别的限制、稀疏与密集表示的权衡、高维文本特征的计算开销等。受3D高斯溅射技术的启发，作者考虑使用稀疏高斯表示来平衡效率和细节捕捉。他们提出渐进式思路，先使用粗略的基础高斯模型捕捉全局场景，再逐步细化。同时借鉴了GaussTR等使用稀疏高斯表示的工作，以及Metric3D V2和MaskCLIP等用于深度估计和文本对齐的技术。作者还创新性地设计了各向异性感知采样和非对称自注意力机制来解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用渐进式高斯表示，从粗到细逐步增强场景理解能力，同时利用各向异性感知采样和非对称自注意力机制来提高特征提取的准确性和训练的稳定性。整体流程包括：1)初始化阶段使用伪深度图和远点采样选择初始高斯位置；2)基础层处理捕获粗略场景几何；3)渐进式细化包括识别表示不足区域并添加新高斯、处理高斯间关系、根据高斯各向异性特性采样特征点；4)特征聚合与解码；5)最终将高斯表示转换为密集3D占用场，支持开放词汇查询。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)渐进式高斯变换器框架，通过在线渐进密集化策略逐步增强场景表示；2)各向异性感知采样策略，根据高斯空间分布自适应分配感受野；3)非对称自注意力机制，防止新添加高斯干扰已优化高斯。相比之前的工作，PG- adaptively扩展查询数量而非使用固定数量，考虑了高斯的各向异性特性而非简单视为点，采用非对称自注意力而非标准自注意力，使用纯前馈方式实现实时推理而非离线优化，且在不使用LiDAR数据训练的情况下仍能超过使用LiDAR数据的先前最佳方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PG-Occ通过渐进式高斯变换和各向异性感知采样，实现了高效且准确的开放词汇3D占用预测，在保持计算效率的同时显著提升了场景细节捕捉能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D occupancy prediction task has witnessed remarkable progress in recentyears, playing a crucial role in vision-based autonomous driving systems. Whiletraditional methods are limited to fixed semantic categories, recent approacheshave moved towards predicting text-aligned features to enable open-vocabularytext queries in real-world scenes. However, there exists a trade-off intext-aligned scene modeling: sparse Gaussian representation struggles tocapture small objects in the scene, while dense representation incurssignificant computational overhead. To address these limitations, we presentPG-Occ, an innovative Progressive Gaussian Transformer Framework that enablesopen-vocabulary 3D occupancy prediction. Our framework employs progressiveonline densification, a feed-forward strategy that gradually enhances the 3DGaussian representation to capture fine-grained scene details. By iterativelyenhancing the representation, the framework achieves increasingly precise anddetailed scene understanding. Another key contribution is the introduction ofan anisotropy-aware sampling strategy with spatio-temporal fusion, whichadaptively assigns receptive fields to Gaussians at different scales andstages, enabling more effective feature aggregation and richer sceneinformation capture. Through extensive evaluations, we demonstrate that PG-Occachieves state-of-the-art performance with a relative 14.3% mIoU improvementover the previous best performing method. Code and pretrained models will bereleased upon publication on our project page:https://yanchi-3dv.github.io/PG-Occ</description>
      <author>example@mail.com (Chi Yan, Dan Xu)</author>
      <guid isPermaLink="false">2510.04759v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</title>
      <link>http://arxiv.org/abs/2510.04714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Code:  https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的3D语义场景图预测方法，通过优化对象特征质量和结合几何与语义特征，显著提升了场景图预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;3D语义场景图预测是检测3D场景中对象及其语义关系的关键技术，对机器人和AR/VR应用至关重要。先前研究虽解决了数据集限制问题并探索了多种方法，但常未能充分优化对象和关系特征的表示能力，过度依赖图神经网络而判别能力不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中对象特征质量不足的问题，提高场景图预测的整体准确性，并减少对图神经网络的过度依赖。&lt;h4&gt;方法&lt;/h4&gt;设计高判别性的对象特征编码器，采用对比预训练策略将对象表示学习与场景图预测解耦，并有效结合几何和语义特征以实现更优的关系预测。&lt;h4&gt;主要发现&lt;/h4&gt;对象特征质量对场景图整体准确性起关键作用；所提方法提高了对象分类准确性并改善了关系预测；将预训练编码器插入现有框架时，所有评估指标都观察到显著性能提升；结合几何和语义特征实现了优越的关系预测效果。&lt;h4&gt;结论&lt;/h4&gt;在3DSSG数据集上的综合实验表明，该方法显著优于先前最先进的方法，相关代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;3D语义场景图预测旨在检测3D场景中的对象及其语义关系，已成为机器人和AR/VR应用的关键技术。虽然先前研究解决了数据集限制问题并探索了包括开放词汇设置在内的各种方法，但它们经常未能优化对象和关系特征的表示能力，尽管判别能力不足却过度依赖图神经网络。在本工作中，我们通过大量分析证明对象特征质量对整体场景图准确性起着关键作用。为应对这一挑战，我们设计了一个高判别性的对象特征编码器，并采用对比预训练策略将对象表示学习与场景图预测解耦。这种设计不仅提高了对象分类准确性，也直接改善了关系预测。值得注意的是，当将我们的预训练编码器插入现有框架时，我们在所有评估指标上都观察到显著的性能提升。此外，由于现有方法未充分利用关系信息的整合，我们有效地结合了几何和语义特征以实现优越的关系预测。在3DSSG数据集上的综合实验表明，我们的方法显著优于先前最先进的方法。我们的代码已在https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D语义场景图预测中对象分类错误导致关系预测不准确的问题。现有方法过度依赖图神经网络进行关系推理，而忽视了对象特征质量的提升。这个问题在现实中很重要，因为准确的3D场景图理解对于机器人导航、物体操作和AR/VR交互等关键应用至关重要，能提高机器人在复杂环境中的感知能力和人机交互的自然度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析发现对象分类错误与关系预测错误高度相关，且对象特征质量对整体场景图准确性起决定性作用。基于这一观察，作者设计了判别性对象特征编码器和新型关系特征编码器。作者借鉴了CLIP的跨模态对比学习、PointNet的点云处理和图神经网络等现有工作，但创新性地将这些技术组合并优化，特别强调了对象表示学习与场景图预测的解耦。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提高对象特征的判别力可以直接改善关系预测。整体流程分为三个阶段：1)对象特征学习阶段：使用对比预训练结合3D点云、2D图像和文本描述训练对象编码器；2)关系特征学习阶段：设计结合对象特征和几何信息的关系编码器，并引入局部空间增强模块；3)图神经网络阶段：应用双向边门控机制和全局空间增强进行场景图预测，捕获对象间的空间关系和不对称性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)判别性对象特征编码器，通过对比预训练提高对象特征质量；2)新型关系特征编码器，结合语义和几何特征；3)双向边门控机制，显式建模主-客体不对称性；4)全局空间增强，整合整体空间上下文；5)解耦对象表示学习与场景图预测的策略。相比之前工作，本文优先提升对象表示质量而非直接优化关系预测，且方法可集成到现有框架中提升整体性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过提出判别性对象特征编码器和新型关系特征编码器，显著提高了3D语义场景图预测的准确性，在对象分类和关系预测方面实现了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Semantic Scene Graph Prediction aims to detect objects and their semanticrelationships in 3D scenes, and has emerged as a crucial technology forrobotics and AR/VR applications. While previous research has addressed datasetlimitations and explored various approaches including Open-Vocabulary settings,they frequently fail to optimize the representational capacity of object andrelationship features, showing excessive reliance on Graph Neural Networksdespite insufficient discriminative capability. In this work, we demonstratethrough extensive analysis that the quality of object features plays a criticalrole in determining overall scene graph accuracy. To address this challenge, wedesign a highly discriminative object feature encoder and employ a contrastivepretraining strategy that decouples object representation learning from thescene graph prediction. This design not only enhances object classificationaccuracy but also yields direct improvements in relationship prediction.Notably, when plugging in our pretrained encoder into existing frameworks, weobserve substantial performance improvements across all evaluation metrics.Additionally, whereas existing approaches have not fully exploited theintegration of relationship information, we effectively combine both geometricand semantic features to achieve superior relationship prediction.Comprehensive experiments on the 3DSSG dataset demonstrate that our approachsignificantly outperforms previous state-of-the-art methods. Our code ispublicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.</description>
      <author>example@mail.com (KunHo Heo, GiHyun Kim, SuYeon Kim, MyeongAh Cho)</author>
      <guid isPermaLink="false">2510.04714v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</title>
      <link>http://arxiv.org/abs/2510.04704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AtomWorld基准测试，用于评估大型语言模型在处理晶体学信息文件(CIFs)方面的能力，特别是在结构编辑、CIF感知和属性引导建模等任务上的表现。研究发现当前模型在结构理解和空间推理方面存在明显局限性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在文本推理方面表现出色，并开始发展空间理解能力。在材料科学等领域，对3D原子结构的深入理解是基础性的。虽然已有初步研究成功将LLMs应用于纯晶体生成或坐标理解任务，但缺乏一个标准化的基准来系统评估它们在多样化原子结构上的核心推理能力。&lt;h4&gt;目的&lt;/h4&gt;引入AtomWorld基准测试，用于评估大型语言模型基于晶体学信息文件(CIFs)的任务表现，CIFs是一种标准结构表示格式。&lt;h4&gt;方法&lt;/h4&gt;定义了一系列基于CIFs的任务，包括结构编辑、CIF感知和属性引导建模，用于系统评估LLMs的原子结构理解和空间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型尽管建立了有希望的基准线，但在结构理解和空间推理方面持续失败。实验表明，这些模型在结构修改任务中经常出错，甚至在基本的CIF格式理解方面也存在问题，可能导致后续分析和材料见解中的累积误差。&lt;h4&gt;结论&lt;/h4&gt;通过定义这些标准化任务，AtomWorld为推进大型语言模型向稳健的原子级建模奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在文本推理方面表现出色，并开始发展空间理解能力，这引发了一个问题：这些能力是否可以结合用于复杂、领域特定的任务。在材料科学等领域，这个问题尤为重要，因为对3D原子结构的深入理解是基础性的。虽然初步研究已成功将LLMs应用于涉及纯晶体生成或坐标理解的任务，但缺乏一个标准化的基准来系统评估它们在多样化原子结构上的核心推理能力。为解决这一差距，我们引入了AtomWorld基准测试，用于评估LLMs基于晶体学信息文件(CIFs)的任务表现，CIFs是一种标准结构表示格式。这些任务包括结构编辑、CIF感知和属性引导建模，揭示了一个关键局限：当前模型尽管建立了有希望的基准线，但在结构理解和空间推理方面持续失败。我们的实验表明，这些模型在结构修改任务中经常出错，甚至在基本的CIF格式理解方面也存在问题，可能导致后续分析和材料见解中的累积误差。通过定义这些标准化任务，AtomWorld为推进LLMs向稳健的原子级建模奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏标准化基准来系统评估大语言模型在处理晶体材料原子结构方面的空间推理能力问题。这个问题在材料科学等领域非常重要，因为深入理解3D原子结构是基础，而当前模型在结构理解和空间推理方面存在关键局限性，建立这一基准对于推动LLMs向稳健的原子级建模发展、加速材料研究和自动化科学工作流程至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将LLMs处理CIF文件的能力分为运动技能（几何操作）、感知技能（识别模式）和认知技能（推理与创造力）三个阶段，重点关注运动技能这一基础能力。他们设计了AtomWorld基准，基于晶体学信息文件（CIFs）创建多种原子结构操作任务。作者借鉴了Materials Project数据库中的CIF文件作为结构池，使用pymatgen库中的StructureMatcher进行结构比较，并参考了晶体学信息文件标准格式，但针对晶体材料领域进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个标准化的基准测试框架，系统评估LLMs在处理晶体材料原子结构方面的空间推理能力。整体实现流程包括：1) AtomWorld生成器从结构池随机选择结构，初始化操作模板，应用操作获得目标结构，并生成自然语言描述；2) 将输入结构和动作提示提供给LLM，生成修改后的结构并与目标结构比较；3) 使用互补基准测试（如PointWorld、CIF读写测试等）全面评估不同能力维度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个专门评估LLMs晶体学运动技能的基准；可扩展的数据生成器支持LLM训练；全面的任务设计涵盖多种原子结构操作；互补基准测试形成全面评估套件；系统化的多层级评估方法。相比之前工作，本文更专注于结构修改和空间推理能力而非仅晶体结构生成或问答；提供了标准化评估框架；不仅测试基本操作还评估复杂多步推理；直接针对材料科学研究中的实际操作需求设计任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AtomWorld基准首次系统评估了大型语言模型在晶体材料原子结构修改和空间推理方面的能力，揭示了当前模型在这一关键领域的局限性，并为未来开发更强大的原子级建模工具奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) excel at textual reasoning and are beginning todevelop spatial understanding, prompting the question of whether theseabilities can be combined for complex, domain-specific tasks. This question isessential in fields like materials science, where deep understanding of 3Datomic structures is fundamental. While initial studies have successfullyapplied LLMs to tasks involving pure crystal generation or coordinateunderstandings, a standardized benchmark to systematically evaluate their corereasoning abilities across diverse atomic structures has been notably absent.To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs ontasks based in Crystallographic Information Files (CIFs), a standard structurerepresentation format. These tasks, including structural editing, CIFperception, and property-guided modeling, reveal a critical limitation: currentmodels, despite establishing promising baselines, consistently fail instructural understanding and spatial reasoning. Our experiments show that thesemodels make frequent errors on structure modification tasks, and even in thebasic CIF format understandings, potentially leading to cumulative errors insubsequent analysis and materials insights. By defining these standardizedtasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scalemodeling, crucial for accelerating materials research and automating scientificworkflows.</description>
      <author>example@mail.com (Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Bram Hoex, Zhicheng Zhong, Tong Xie)</author>
      <guid isPermaLink="false">2510.04704v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation</title>
      <link>http://arxiv.org/abs/2510.03863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICLR 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SpatialCAPTCHA，一种新型人机验证框架，利用人类与多模态大语言模型在空间推理方面的根本差异，设计需要几何推理、视角转换、遮挡处理和心理旋转等能力的验证问题，有效抵御现代AI攻击。&lt;h4&gt;背景&lt;/h4&gt;在线服务依赖CAPTCHA作为抵御自动化滥用的第一道防线，但最近多模态大语言模型的进步已经削弱了专注于文本识别或2D图像理解的常规CAPTCHA设计的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了应对MLLMs对传统CAPTCHA的挑战，提出一种利用人类与AI在空间推理能力差异的新型验证框架，提高验证系统的安全性和有效性。&lt;h4&gt;方法&lt;/h4&gt;采用程序生成管道，包含基于约束的难度控制、自动正确性验证和人工循环验证，在Spatial-CAPTCHA-Bench基准上评估性能，并与Google reCAPTCHA进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;人类在Spatial-CAPTCHA-Bench上的表现远超10种最先进的MLLMs，最佳模型的Pass@1准确率仅为31.0%，证实了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;Spatial CAPTCHA通过利用人类与AI在空间推理能力上的差异，提供了有效抵御现代AI攻击的验证方法，同时也可作为评估AI空间推理能力的诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;在线服务依赖CAPTCHA作为抵御自动化滥用的第一道防线，然而最近多模态大语言模型的进步已经削弱了专注于文本识别或2D图像理解的常规设计的有效性。为了应对这一挑战，我们提出了SpatialCAPTCHA，一种新型的人机验证框架，它利用人类与MLLMs在空间推理方面的根本差异。与依赖容易被现代AI攻破的低级感知任务的现有CAPTCHA不同，Spatial CAPTCHA生成需要几何推理、视角转换、遮挡处理和心理旋转的动态问题。这些技能对人类来说是直观的，但对最先进的AI系统来说却很困难。该系统采用程序生成管道，包含基于约束的难度控制、自动正确性验证和人工循环验证，以确保可扩展性、鲁棒性和适应性。在相应的基准Spatial-CAPTCHA-Bench上的评估表明，人类的表现远超10种最先进的MLLMs，其中最佳模型的Pass@1准确率仅为31.0%。此外，我们将Spatial CAPTCHA与Google reCAPTCHA进行了比较，证实了其作为安全机制和AI空间推理诊断工具的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是随着多模态大语言模型的发展，传统CAPTCHA系统越来越容易被AI破解，无法有效保护在线服务免受自动化滥用。这个问题非常重要，因为CAPTCHA是保护网络服务（如Google、Facebook等）的第一道防线，防止自动化攻击（如凭证填充、内容抓取和垃圾信息）导致的经济损失和用户信任下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类天生具有强大的3D空间推理能力，而AI在这方面存在局限。他们借鉴了人类认知科学研究中的空间能力分类，设计了七种类型的空间推理任务。作者参考了经典心理测量工具和CAPTCHA技术的历史发展，但创新性地将这些空间能力转化为区分人类和AI的有效方法。他们开发了一个自主生成系统，集成了难度控制、自动验证和人机循环验证等机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用人类与AI在空间推理能力上的根本差异，通过设计需要3D空间理解、几何推理、视角转换和心像旋转等能力的任务来区分人类和机器。整体流程包括：定义七种空间推理任务类型；开发程序生成管道（场景生成、干扰项合成、验证）；构建提示和答案；组装成完整CAPTCHA实例；使用基于约束的难度控制确保任务对人类简单但对AI具有挑战性；通过自动验证和人机循环验证确保任务质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：基于人类认知能力的理论基础设计；专注于空间推理的七种任务类型；可扩展的程序生成管道；基于约束的难度控制；自动正确性验证；人机循环验证机制。相比之前工作，不同之处在于：传统CAPTCHA依赖文本或2D图像识别，而Spatial CAPTCHA专注于3D空间推理；不仅是评估工具，也是实用的安全机制；利用人类与AI在空间能力上的根本差异，创造了更大的人机性能差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Spatial CAPTCHA通过利用人类与AI在空间推理能力上的根本差异，提出了一种新型人机验证框架，能有效区分人类和先进AI模型，为在线服务提供更可靠的安全保护。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online services rely on CAPTCHAs as a first line of defense against automatedabuse, yet recent advances in multi-modal large language models (MLLMs) haveeroded the effectiveness of conventional designs that focus on text recognitionor 2D image understanding. To address this challenge, we present SpatialCAPTCHA, a novel human-verification framework that leverages fundamentaldifferences in spatial reasoning between humans and MLLMs. Unlike existingCAPTCHAs which rely on low-level perception tasks that are vulnerable to modernAI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,perspective-taking, occlusion handling, and mental rotation. These skills areintuitive for humans but difficult for state-of-the-art (SOTA) AI systems. Thesystem employs a procedural generation pipeline with constraint-baseddifficulty control, automated correctness verification, and human-in-the-loopvalidation to ensure scalability, robustness, and adaptability. Evaluation on acorresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastlyoutperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,which confirms its effectiveness as both a security mechanism and a diagnostictool for spatial reasoning in AI.</description>
      <author>example@mail.com (Arina Kharlamova, Bowei He, Chen Ma, Xue Liu)</author>
      <guid isPermaLink="false">2510.03863v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2510.03441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了SpatialViLT，一种增强的视觉语言模型，通过整合空间特征来解决3D场景和复杂物体配置中的空间推理挑战，并在视觉空间推理数据集上取得了最先进的准确性。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)在多模态推理方面取得了进展，但在3D场景和复杂物体配置的空间推理方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强的VLM，通过整合空间特征来提升模型对3D场景和复杂物体配置的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;通过多任务学习框架整合深度图、3D坐标和边缘图等空间特征，提出SpatialViLT和MaskedSpatialViLT两种变体，分别关注完整和遮蔽的对象区域，并创建SpatialEnsemble结合两种方法。&lt;h4&gt;主要发现&lt;/h4&gt;模型在方向关系、拓扑关系和邻近关系等空间推理类别方面表现出色，在具有挑战性的视觉空间推理(VSR)数据集上证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作在增强AI系统的空间智能方面迈出了重要一步，这对高级多模态理解和实际应用至关重要。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)已推进多模态推理，但在3D场景和复杂物体配置的空间推理方面仍面临挑战。为此，我们引入了SpatialViLT，一种通过多任务学习框架整合深度图、3D坐标和边缘图等空间特征的增强型VLM。这种方法通过空间理解丰富了多模态嵌入。我们提出了两种变体：SpatialViLT和MaskedSpatialViLT，分别关注完整和遮蔽的对象区域。此外，SpatialEnsemble结合了这两种方法，实现了最先进的准确性。我们的模型在方向关系、拓扑关系和邻近关系等空间推理类别方面表现出色，这在具有挑战性的视觉空间推理(VSR)数据集上得到了证明。这项工作在增强AI系统的空间智能方面代表了重要一步，对高级多模态理解和实际应用至关重要。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型（VLMs）在3D场景和复杂物体配置中的空间推理能力有限的问题。这个问题很重要，因为空间推理是人类认知的核心方面，对AI系统理解真实世界中的空间配置和交互至关重要。缺乏这种能力限制了AI系统在高级多模态理解和实际应用中的表现，如自动驾驶、机器人导航和增强现实等领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前VLMs在VSR数据集上的表现（不超过70%准确率，而人类可达95.4%），认识到现有模型在空间推理方面的局限性。他们借鉴了SpatialVLM等工作的思路，但发现仅靠数据集和微调不足。作者选择了ViLT作为基础模型，因为它适合集成各种空间特征。通过分析VSR数据集中不同空间关系元类别的需求，作者设计了多任务学习框架，同时预测深度图、3D坐标和边缘图三种空间特征，从而增强模型的空间理解能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务学习框架将空间特征（深度图、3D坐标、边缘图）整合到视觉语言模型中，增强模型的空间先验知识和3D理解能力。整体流程包括：1)特征提取管道，使用CLIPSeg进行对象分割，MiDaS生成深度图，计算3D坐标，Canny算法生成边缘图；2)基于ViLT构建模型架构，包含CNN编码器处理空间特征，解码器重建特征，分类器做预测；3)多任务训练，同时优化分类任务和空间特征重建任务；4)SpatialEnsemble技术，结合多个模型的预测，根据性能加权投票提高整体准确率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出SpatialViLT和MaskedSpatialViLT两种模型变体，前者处理全局空间信息，后者专注于物体特定区域；2)设计多任务学习框架，同时预测深度图、3D坐标和边缘图三种空间特征；3)创建SpatialEnsemble集成方法，结合多个专家模型的优势；4)实现了在VSR数据集上的最先进性能。相比之前工作，本文不仅整合了更多种类的空间特征，还通过多任务学习框架同时优化多种空间表示，而非单一任务训练，并且引入了关系感知的集成方法，针对不同空间关系元类别调整模型权重。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpatialViLT通过多任务学习框架整合空间特征并使用SpatialEnsemble技术，显著提升了视觉语言模型在3D场景中的空间推理能力，实现了在VSR数据集上的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have advanced multimodal reasoning but stillface challenges in spatial reasoning for 3D scenes and complex objectconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM thatintegrates spatial features like depth maps, 3D coordinates, and edge mapsthrough a multi-task learning framework. This approach enriches multimodalembeddings with spatial understanding. We propose two variants: SpatialViLT andMaskedSpatialViLT, focusing on full and masked object regions, respectively.Additionally, SpatialEnsemble combines both approaches, achievingstate-of-the-art accuracy. Our models excel in spatial reasoning categoriessuch as directional, topological, and proximity relations, as demonstrated onthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents asignificant step in enhancing the spatial intelligence of AI systems, crucialfor advanced multimodal understanding and real-world applications.</description>
      <author>example@mail.com (Chashi Mahiul Islam, Oteo Mamo, Samuel Jacob Chacko, Xiuwen Liu, Weikuan Yu)</author>
      <guid isPermaLink="false">2510.03441v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models</title>
      <link>http://arxiv.org/abs/2510.04933v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comments: 14 pages, 14 figures, 5 tables. Code available at:  https://github.com/sirraya-tech/Sirraya_LSD_Code&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了层级语义动力学(LSD)框架，用于检测大型语言模型中的幻觉现象。该方法通过分析transformer层中隐藏状态的语义演变，实现了高效准确的幻觉检测，仅需单次前向传播即可达到高精度。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)经常产生流畅但事实不正确的陈述，这种现象称为'幻觉'，在高风险领域会带来严重风险。现有的幻觉检测方法通常依赖多次采样或外部验证源，效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种几何框架来检测大型语言模型中的幻觉现象，分析transformer层中隐藏状态语义的演变，提供一种高效、内在的检测方法。&lt;h4&gt;方法&lt;/h4&gt;层级语义动力学(LSD)框架使用基于边距的对比学习，将隐藏激活与来自事实编码器的事实嵌入对齐。通过分析语义轨迹的分离情况来区分事实性响应和幻觉：事实性响应保持稳定对齐，而幻觉则在模型深度上表现出明显的语义漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在TruthfulQA和合成事实-幻觉数据集上评估，LSD实现了F1分数0.92、AUROC 0.96和聚类准确率0.89，性能优于SelfCheckGPT和语义熵基线方法。仅需单次前向传播，比基于采样的方法快5-20倍，同时保持了高精度和可解释性。&lt;h4&gt;结论&lt;/h4&gt;LSD提供了一种可扩展的、与模型无关的机制，可用于实时幻觉监控。该研究为大型语言模型中事实一致性的几何结构提供了新的见解，有助于理解和减轻模型幻觉问题。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)经常产生流畅但事实不正确的陈述—这种现象被称为'幻觉'—在高风险领域构成严重威胁。我们提出了层级语义动力学(LSD)，这是一种用于幻觉检测的几何框架，它分析transformer层中隐藏状态语义的演变。与依赖于多次采样或外部验证源的前沿方法不同，LSD在模型的表示空间内内在运行。使用基于边距的对比学习，LSD将隐藏激活与来自事实编码器的事实嵌入对齐，揭示出语义轨迹的明显分离：事实性响应保持稳定对齐，而幻觉则在深度上表现出明显的语义漂移。在TruthfulQA和合成事实-幻觉数据集上评估，LSD实现了F1分数0.92、AUROC 0.96和聚类准确率0.89，优于SelfCheckGPT和语义熵基线方法，同时仅需单次前向传播。这种效率比基于采样的方法快5-20倍，且不牺牲精度或可解释性。LSD为实时幻觉监控提供了一种可扩展的、与模型无关的机制，并为大型语言模型中事实一致性的几何结构提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) often produce fluent yet factually incorrectstatements-a phenomenon known as hallucination-posing serious risks inhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometricframework for hallucination detection that analyzes the evolution ofhidden-state semantics across transformer layers. Unlike prior methods thatrely on multiple sampling passes or external verification sources, LSD operatesintrinsically within the model's representational space. Using margin-basedcontrastive learning, LSD aligns hidden activations with ground-truthembeddings derived from a factual encoder, revealing a distinct separation insemantic trajectories: factual responses preserve stable alignment, whilehallucinations exhibit pronounced semantic drift across depth. Evaluated on theTruthfulQA and synthetic factual-hallucination datasets, LSD achieves anF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperformingSelfCheckGPT and Semantic Entropy baselines while requiring only a singleforward pass. This efficiency yields a 5-20x speedup over sampling-basedmethods without sacrificing precision or interpretability. LSD offers ascalable, model-agnostic mechanism for real-time hallucination monitoring andprovides new insights into the geometry of factual consistency within largelanguage models.</description>
      <author>example@mail.com (Amir Hameed Mir)</author>
      <guid isPermaLink="false">2510.04933v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation</title>
      <link>http://arxiv.org/abs/2510.04794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统比较了Vision-transformers (ViTs)和大规模卷积神经网络(CNNs)在几何估计任务上的性能，特别是在低数据场景下的表现。研究聚焦于两个任务：估计图像对间的2D刚性变换和预测立体图像对的基本矩阵。&lt;h4&gt;背景&lt;/h4&gt;Vision-transformers和大规模卷积神经网络通过预训练特征表示重塑了计算机视觉，实现了强大的迁移学习能力。然而，在涉及图像变形的低数据量几何估计任务中，它们作为骨干架构的效率仍是一个开放问题。&lt;h4&gt;目的&lt;/h4&gt;比较大规模CNNs(ResNet, EfficientNet, CLIP-ResNet)与ViT基础模型(CLIP-ViT变体和DINO)在各种数据量设置下的性能，包括少样本场景，评估它们在几何估计任务上的适用性。&lt;h4&gt;方法&lt;/h4&gt;系统比较预训练模型(针对分类或对比学习优化)在几何估计任务上的表现，这些模型通常专注于高级语义，但研究任务需要平衡局部和全局特征。在不同数据量设置下进行经验比较分析，包括少样本场景。&lt;h4&gt;主要发现&lt;/h4&gt;在大型下游数据场景中，ViTs在精炼阶段优于CNNs；在小数据场景中，CNNs的归纳偏差和较小容量使其性能能够与ViT相匹配；ViTs在跨域评估中表现出更强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;强调了为精炼仔细选择模型架构的重要性，促进了未来对混合架构的研究，这些架构能够平衡局部和全局表示。&lt;h4&gt;翻译&lt;/h4&gt;Vision-transformers (ViTs) 和大规模卷积神经网络 (CNNs) 通过预训练特征表示重塑了计算机视觉，使迁移学习能够在各种任务上实现强大性能。然而，在涉及图像变形的低数据量几何估计任务中，它们作为骨干架构的效率仍是一个开放问题。本研究考虑了两个这样的任务：1) 估计图像对之间的2D刚性变换；2) 预测立体图像对的基本矩阵，这是自主移动、机器人和3D场景重建等应用中的重要问题。通过系统地比较大规模CNNs(ResNet, EfficientNet, CLIP-ResNet)与基于ViT的基础模型(CLIP-ViT变体和DINO)在各种数据量设置下的表现(包括少样本场景)，本研究解决了这个有趣的问题。这些预训练模型针对分类或对比学习进行了优化，鼓励它们主要关注高级语义。所考虑的任务需要不同程度地平衡局部和全局特征，挑战了将这些模型直接作为骨干架构的简单采用。经验比较分析表明，类似于从头开始训练，在大型下游数据场景中，ViTs在精炼阶段优于CNNs。然而，在小数据场景中，CNNs的归纳偏差和较小容量提高了它们的性能，使其能够与ViT相匹配。此外，在数据分布发生变化的跨域评估中，ViTs表现出更强的泛化能力。这些结果强调了为精炼仔细选择模型架构的重要性，促进了未来对混合架构的研究，这些架构能够平衡局部和全局表示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在数据有限（特别是少样本）情况下，如何有效利用预训练的视觉Transformer（ViT）和卷积神经网络（CNN）进行几何估计任务的问题。具体研究两种任务：估计图像对间的2D刚性变换和预测立体图像对的基本矩阵。这个问题很重要，因为几何估计是3D场景重建、自动驾驶和机器人导航等应用的基础，而这些应用中往往标注数据有限，需要高效利用现有模型进行迁移学习。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先注意到ViT和CNN在图像分类等任务上的差异，但缺乏在几何估计任务上的系统比较。他们观察到几何估计需要平衡局部和全局特征，与传统的语义分类任务不同。因此设计了一个统一网络架构，包含特征提取模块和回归模块，使不同模型可以公平比较。他们借鉴了现有预训练模型（如CLIP-ViT、DINO-ViT、ResNet等）、基本矩阵估计的秩约束层、位置感知的最大池化技术，以及MSE和Huber损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是比较不同预训练模型在几何估计任务上的性能差异，探索数据量对性能的影响，以及不同预训练目标对下游任务的影响。整体流程包括：1) 特征提取模块：使用预训练ViT或CNN提取特征，通过卷积层和位置感知最大池化处理；2) 回归模块：针对不同任务设计特定结构（2D变换或基本矩阵估计）；3) 训练与评估：使用组合损失函数，在不同数据量条件下评估模型性能，包括少样本场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统比较ViT和CNN在几何估计任务上的性能，特别关注少样本场景；2) 揭示了数据量对模型选择的影响，发现小数据时CNN可媲美甚至优于ViT；3) 分析了不同预训练目标对下游任务的影响；4) 发现冻结ViT底层可减少小数据场景过拟合；5) 展示了ViT在跨域泛化方面的优势。相比之前工作，本文专注于迁移学习在几何任务中的应用，关注数据量影响，提供架构全面比较，并分析了模型选择的影响机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统比较视觉Transformer和卷积神经网络在少样本几何估计任务上的性能，揭示了不同数据场景下的最优模型选择，为几何任务中的迁移学习提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)have reshaped computer vision through pretrained feature representations thatenable strong transfer learning for diverse tasks. However, their efficiency asbackbone architectures for geometric estimation tasks involving imagedeformations in low-data regimes remains an open question. This work considerstwo such tasks: 1) estimating 2D rigid transformations between pairs of imagesand 2) predicting the fundamental matrix for stereo image pairs, an importantproblem in various applications, such as autonomous mobility, robotics, and 3Dscene reconstruction. Addressing this intriguing question, this worksystematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)with ViT-based foundation models (CLIP-ViT variants and DINO) in various datasize settings, including few-shot scenarios. These pretrained models areoptimized for classification or contrastive learning, encouraging them to focusmostly on high-level semantics. The considered tasks require balancing localand global features differently, challenging the straightforward adoption ofthese models as the backbone. Empirical comparative analysis shows that,similar to training from scratch, ViTs outperform CNNs during refinement inlarge downstream-data scenarios. However, in small data scenarios, theinductive bias and smaller capacity of CNNs improve their performance, allowingthem to match that of a ViT. Moreover, ViTs exhibit stronger generalization incross-domain evaluation where the data distribution changes. These resultsemphasize the importance of carefully selecting model architectures forrefinement, motivating future research towards hybrid architectures thatbalance local and global representations.</description>
      <author>example@mail.com (Alon Kaya, Igal Bilik, Inna Stainvas)</author>
      <guid isPermaLink="false">2510.04794v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry</title>
      <link>http://arxiv.org/abs/2510.04631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted to EMNLP 2025 (industry track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了如何将SciNCL图感知邻域对比学习方法应用于流程工业领域，通过使用从知识图谱中提取的三元组对语言模型进行微调，显著提高了文本嵌入性能并减小了模型大小。&lt;h4&gt;背景&lt;/h4&gt;自然语言处理(NLP)的最新趋势是利用知识图谱(KGs)来增强预训练语言模型，通过从图结构中获取额外知识，学习特定领域的术语或文档间的关系。&lt;h4&gt;目的&lt;/h4&gt;探索如何将SciNCL（一种针对科学出版物设计的图感知邻域对比学习方法）应用于流程工业领域，该领域的文本日志包含日常操作的关键信息，通常被构造成稀疏知识图谱。&lt;h4&gt;方法&lt;/h4&gt;使用从知识图谱中提取的三元组对语言模型进行微调（Graph-enhanced, GE）&lt;h4&gt;主要发现&lt;/h4&gt;使用GE三元组微调的语言模型在专有的流程工业文本嵌入基准(PITEB)上比最先进的mE5-large文本编码器高出9.8-14.3%（5.4-8.0个百分点），同时模型大小只有3-5倍小。&lt;h4&gt;结论&lt;/h4&gt;图感知邻域对比学习方法在特定领域（流程工业）的文本嵌入任务中表现优异，不仅提高了性能，还减小了模型大小。&lt;h4&gt;翻译&lt;/h4&gt;最近自然语言处理(NLP)的趋势是利用知识图谱(KGs)来增强预训练语言模型，通过从图结构中纳入额外知识来学习特定领域的术语或文档之间可能被忽视的关系。本文探讨了如何将SciNCL（一种最初为科学出版物设计的图感知邻域对比学习方法）应用于流程工业领域，该领域的文本日志包含关于日常操作的关键信息，通常被构造成稀疏知识图谱。我们的实验证明，使用从知识图谱中提取的三元组进行微调的语言模型在专有的流程工业文本嵌入基准(PITEB)上比最先进的mE5-large文本编码器高出9.8-14.3%（5.4-8.0个百分点），同时大小只有3-5倍小。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrainedlanguage models by incorporating additional knowledge from the graph structuresto learn domain-specific terminology or relationships between documents thatmight otherwise be overlooked. This paper explores how SciNCL, a graph-awareneighborhood contrastive learning methodology originally designed forscientific publications, can be applied to the process industry domain, wheretext logs contain crucial information about daily operations and are oftenstructured as sparse KGs. Our experiments demonstrate that language modelsfine-tuned with triplets derived from GE outperform a state-of-the-artmE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary processindustry text embedding benchmark (PITEB) while being 3-5 times smaller insize.</description>
      <author>example@mail.com (Anastasia Zhukova, Jonas Lührs, Christian E. Matt, Bela Gipp)</author>
      <guid isPermaLink="false">2510.04631v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Semantic Clones of Unseen Functionality</title>
      <link>http://arxiv.org/abs/2510.04143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, accepted for publication (to appear) in the 40th  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了语义代码克隆检测中模型对未见功能的泛化能力问题，提出了使用对比学习提高模型性能的方法。&lt;h4&gt;背景&lt;/h4&gt;许多神经模型在语义代码克隆检测任务上表现优异，但主要擅长检测与训练数据相似的克隆，难以泛化到未见功能。&lt;h4&gt;目的&lt;/h4&gt;评估现有模型在检测未见功能克隆方面的性能，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;重新评估六种最先进模型（任务特定模型和生成式LLMs），并应用对比学习技术（对比分类器和对比上下文学习）提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;任务特定模型在未见功能上的F1值平均下降31%，而LLMs平均仅下降3%；对比学习使任务特定模型F1值平均提高9%，LLMs平均提高3%。&lt;h4&gt;结论&lt;/h4&gt;对比学习能有效提高模型在检测未见功能克隆方面的性能，特别是对于任务特定模型。&lt;h4&gt;翻译&lt;/h4&gt;语义代码克隆检测是检测两个代码片段是否实现相同功能（如排序数组）的任务。最近，许多神经模型在这一任务上取得了近乎完美的性能。这些模型试图基于训练数据进行推理。因此，它们更擅长检测与训练过程中所见过的克隆相似的克隆，而可能难以检测那些未见过的克隆。当然，寻求克隆的开发者对两种类型的克隆都感兴趣。我们通过文献综述证实了这一观点，确定了三个实际的克隆检测任务，其中模型的目标是检测功能的克隆，即使它是在不同功能的克隆上训练的。基于这一发现，我们重新评估了六种最先进的模型，包括任务特定模型和生成式LLMs，在检测未见功能克隆方面的任务。我们的实验显示，任务特定模型的F1值最多下降48%（平均31%）。LLMs在没有专门针对克隆检测训练的情况下，与任务特定模型表现相当，但在未见功能上泛化能力更好，F1值最多下降5%（平均3%）。我们提出并评估了使用对比学习来提高现有模型在未见功能克隆上的性能。我们从计算机视觉和自然语言处理领域获取灵感，在这些领域中，对比学习擅长测量两个对象之间的相似性，即使它们来自训练中未见过的类别。我们用对比分类器替换了任务特定模型的最终分类器，而对于生成式LLMs，我们提出了对比上下文学习，引导LLMs专注于克隆与非克隆之间的差异。任务特定模型在未见功能克隆上的F1值最多提高了26%（平均9%），LLMs最多提高了5%（平均3%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic code clone detection is the task of detecting whether two snippetsof code implement the same functionality (e.g., Sort Array). Recently, manyneural models achieved near-perfect performance on this task. These models seekto make inferences based on their training data. Consequently, they betterdetect clones similar to those they have seen during training and may struggleto detect those they have not. Developers seeking clones are, of course,interested in both types of clones. We confirm this claim through a literaturereview, identifying three practical clone detection tasks in which the model'sgoal is to detect clones of a functionality even if it was trained on clones ofdifferent functionalities. In light of this finding, we re-evaluate sixstate-of-the-art models, including both task-specific models and generativeLLMs, on the task of detecting clones of unseen functionality. Our experimentsreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMsperform on par with task-specific models without explicit training for clonedetection, but generalize better to unseen functionalities, where F1 drops upto 5% (average 3%) instead. We propose and evaluate the use of contrastivelearning to improve the performance of existing models on clones of unseenfunctionality. We draw inspiration from the computer vision and naturallanguage processing fields where contrastive learning excels at measuringsimilarity between two objects, even if they come from classes unseen duringtraining. We replace the final classifier of the task-specific models with acontrastive classifier, while for the generative LLMs we propose contrastivein-context learning, guiding the LLMs to focus on the differences betweenclones and non-clones. The F1 on clones of unseen functionality is improved byup to 26% (average 9%) for task-specific models and up to 5% (average 3%) forLLMs.</description>
      <author>example@mail.com (Konstantinos Kitsios, Francesco Sovrano, Earl T. Barr, Alberto Bacchelli)</author>
      <guid isPermaLink="false">2510.04143v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation</title>
      <link>http://arxiv.org/abs/2510.03821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合对比学习和扩散模型的无配对图像到图像翻译方法，通过时间对比学习保留域不变特征并指导扩散模型进行翻译。&lt;h4&gt;背景&lt;/h4&gt;无配对图像到图像翻译涉及在没有对齐或对应样本的情况下学习源域和目标域之间的映射关系。基于分数的扩散模型在生成任务中表现出色，而对比学习可以在没有配对数据的情况下学习语义相似性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合对比学习和扩散模型的方法，提高无配对图像到图像翻译的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于时间对比学习方法，使用SimCLR训练模型，将图像及其域不变特征视为正样本对，从而保留域不变特征并丢弃域特定特征。学习的对比模型指导预训练SDE进行I2I翻译任务。&lt;h4&gt;主要发现&lt;/h4&gt;Contrastive-SDE在多个指标上达到了与最先进方法相当的结果，同时模型收敛速度显著加快，不需要标签监督或分类器训练。&lt;h4&gt;结论&lt;/h4&gt;结合对比学习和扩散模型的方法为无配对图像到图像翻译提供了一种更高效的替代方案，能够在保持高质量翻译的同时提高训练效率。&lt;h4&gt;翻译&lt;/h4&gt;无配对图像到图像翻译涉及在没有对齐或对应样本的情况下学习源域和目标域之间的映射关系。基于分数的扩散模型在生成任务中已展现出最先进的性能。它们通过随机微分方程近似复杂数据分布的能力，使其能够生成高质量和多样化的输出，特别适合无配对I2I设置。同时，对比学习提供了一种强大的框架，可以在没有明确监督或配对数据的情况下学习语义相似性。通过拉近语义相似样本的表示并推远不相似的样本，对比方法与无配对翻译的目标内在一致。在本文中，我们提出了一种基于时间对比学习方法，使用SimCLR训练模型，将图像及其域不变特征视为正样本对，从而保留域不变特征并丢弃域特定特征。学习到的对比模型随后指导预训练SDE进行I2I翻译任务。我们在三个常见的无配对I2I任务中，使用四个评估指标将Contrastive-SDE与几个基线方法进行了经验比较。Contrastive-SDE在多个指标上达到了与最先进方法相当的结果。此外，我们观察到模型收敛速度显著加快，不需要标签监督或分类器训练，使其成为该任务更高效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unpaired image-to-image translation involves learning mappings between sourcedomain and target domain in the absence of aligned or corresponding samples.Score based diffusion models have demonstrated state-of-the-art performance ingenerative tasks. Their ability to approximate complex data distributionsthrough stochastic differential equations (SDEs) enables them to generatehigh-fidelity and diverse outputs, making them particularly well-suited forunpaired I2I settings. In parallel, contrastive learning provides a powerfulframework for learning semantic similarities without the need for explicitsupervision or paired data. By pulling together representations of semanticallysimilar samples and pushing apart dissimilar ones, contrastive methods areinherently aligned with the objectives of unpaired translation. Its ability toselectively enforce semantic consistency at the feature level makes contrastivelearning particularly effective for guiding generation in unpaired scenarios.In this work, we propose a time-dependent contrastive learning approach where amodel is trained with SimCLR by considering an image and its domain invarientfeature as a positive pair, enabling the preservation of domain-invariantfeatures and the discarding of domain-specific ones. The learned contrastivemodel then guides the inference of a pretrained SDE for the I2I translationtask. We empirically compare Contrastive-SDE with several baselines acrossthree common unpaired I2I tasks, using four metrics for evaluation.Constrastive-SDE achieves comparable results to the state-of-the-art on severalmetrics. Furthermore, we observe that our model converges significantly fasterand requires no label supervision or classifier training, making it a moreefficient alternative for this task.</description>
      <author>example@mail.com (Venkata Narendra Kotyada, Revanth Eranki, Nagesh Bhattu Sristy)</author>
      <guid isPermaLink="false">2510.03821v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.03690v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一框架，明确将图数据建模为多个潜在概率图生成模型（graphons）的混合。利用图矩（motif密度）对来自同一模型的图进行聚类，从而分离混合成分并识别不同的生成机制。该方法支持两种关键应用：图混合感知的Mixup（GMAM）数据增强技术和模型自适应的图对比学习（GCL），并改进了负采样策略。&lt;h4&gt;背景&lt;/h4&gt;现实世界的图数据集通常包含多个不同群体的混合，这些图由多个不同的潜在分布生成。然而，现代的表示学习方法（如图对比学习和增强方法如Mixup）通常忽略了这种混合结构。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架，明确地将图数据建模为多个潜在概率图生成模型（以graphons表示）的混合，以解决现有方法忽略混合结构的问题。&lt;h4&gt;方法&lt;/h4&gt;利用图矩（motif密度）来聚类来自同一模型的图，分离混合成分并识别不同的生成机制。实现图混合感知的Mixup（GMAM）数据增强技术和模型自适应的图对比学习（GCL），并通过引入新的模型感知目标改进负采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;1) 建立了新的理论保证：从具有小切割距离的graphons中采样的图将以高概率具有相似的motif密度；2) 在无监督学习中，MGCL在八个数据集上获得平均排名第一；3) 在监督学习中，GMAM在7个数据集中的6个上实现了新的最先进准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效处理图数据中的混合结构，通过明确建模多个潜在概率图生成模型，显著提升了图表示学习的性能，在无监督和监督学习任务中均达到最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;现实世界的图数据集通常由多个群体的混合组成，其中图是从多个不同的潜在分布生成的。然而，现代的表示学习方法，如图对比学习（GCL）和增强方法如Mixup，通常忽略了这种混合结构。在这项工作中，我们提出了一个统一框架，明确地将数据建模为由graphons表示的多个潜在概率图生成模型的混合。为了表征这些graphons，我们利用图矩（motif密度）来聚类来自同一模型的图。这使我们能够分离混合成分并识别它们不同的生成机制。这种模型感知的分区受益于两个关键的图学习任务：1) 它实现了图混合感知的Mixup（GMAM），这是一种数据增强技术，在估计的graphons指导下在语义有效的空间中进行插值，而不是假设每个类只有一个graphon。2) 对于GCL，它实现了模型自适应和有原则的增强。此外，通过引入新的模型感知目标，我们提出的方法（称为MGCL）通过将负样本限制在其他模型的图中来改进负采样。我们建立了一个关键的理论保证：一个新的、更紧的界限表明，从具有小切割距离的graphons中采样的图将以高概率具有相似的motif密度。在基准数据集上的广泛实验展示了强大的实证性能。在无监督学习中，MGCL达到了最先进的结果，在八个数据集上获得了平均排名第一。在监督学习中，GMAM始终优于现有策略，在7个数据集中的6个上实现了新的最先进准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world graph datasets often consist of mixtures of populations, wheregraphs are generated from multiple distinct underlying distributions. However,modern representation learning approaches, such as graph contrastive learning(GCL) and augmentation methods like Mixup, typically overlook this mixturestructure. In this work, we propose a unified framework that explicitly modelsdata as a mixture of underlying probabilistic graph generative modelsrepresented by graphons. To characterize these graphons, we leverage graphmoments (motif densities) to cluster graphs arising from the same model. Thisenables us to disentangle the mixture components and identify their distinctgenerative mechanisms. This model-aware partitioning benefits two key graphlearning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a dataaugmentation technique that interpolates in a semantically valid space guidedby the estimated graphons, instead of assuming a single graphon per class. 2)For GCL, it enables model-adaptive and principled augmentations. Additionally,by introducing a new model-aware objective, our proposed approach (termed MGCL)improves negative sampling by restricting negatives to graphs from othermodels. We establish a key theoretical guarantee: a novel, tighter boundshowing that graphs sampled from graphons with small cut distance will havesimilar motif densities with high probability. Extensive experiments onbenchmark datasets demonstrate strong empirical performance. In unsupervisedlearning, MGCL achieves state-of-the-art results, obtaining the top averagerank across eight datasets. In supervised learning, GMAM consistentlyoutperforms existing strategies, achieving new state-of-the-art accuracy in 6out of 7 datasets.</description>
      <author>example@mail.com (Ali Azizpour, Reza Ramezanpour, Ashutosh Sabharwal, Santiago Segarra)</author>
      <guid isPermaLink="false">2510.03690v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology</title>
      <link>http://arxiv.org/abs/2510.03455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PEaRL是一种多模态框架，通过通路激活分数表示转录组学，使用transformer编码生物通路信号，并通过对比学习与组织学特征对齐，在三种癌症空间转录组学数据集上表现出色，超越了现有最先进的方法。&lt;h4&gt;背景&lt;/h4&gt;将组织病理学与空间转录组学结合为连接组织形态与分子功能提供了强大机会，但大多数现有的多模态方法依赖于少量高度可变的基因，这限制了预测范围，并忽视了塑造组织表型的协调生物程序。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态框架，能够通过通路激活分数表示转录组学，减少维度，提高可解释性，并加强跨模态对应关系。&lt;h4&gt;方法&lt;/h4&gt;PEaRL（Pathway Enhanced Representation Learning）是一种多模态框架，它使用ssGSEA计算通路激活分数来表示转录组学，使用transformer编码生物上连贯的通路信号，并通过对比学习将它们与组织学特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在三种癌症空间转录组学数据集（乳腺、皮肤和淋巴结）上，PEaRL持续超越最先进的方法，在基因和通路水平表达预测方面获得更高的准确性（与最先进方法相比，皮尔逊相关系数分别提高了高达58.9%和20.4%）。&lt;h4&gt;结论&lt;/h4&gt;将转录组表示建立在通路上，可以产生更符合生物学原理且可解释的多模态模型，推动计算病理学超越基因级嵌入的发展。&lt;h4&gt;翻译&lt;/h4&gt;将组织病理学与空间转录组学整合为连接组织形态与分子功能提供了强大机会。然而，大多数现有的多模态方法依赖于少量高度可变的基因，这限制了预测范围，并忽视了塑造组织表型的协调生物程序。我们提出了PEaRL（Pathway Enhanced Representation Learning），一种多模态框架，通过使用ssGSEA计算的通路激活分数来表示转录组学。通过使用transformer编码生物上连贯的通路信号，并通过对比学习将它们与组织学特征对齐，PEaRL减少了维度，提高了可解释性，并加强了跨模态对应关系。在三个癌症空间转录组学数据集（乳腺、皮肤和淋巴结）上，PEaRL持续超越最先进的方法，在基因和通路水平表达预测方面获得更高的准确性（与最先进方法相比，皮尔逊相关系数分别提高了高达58.9%和20.4%）。这些结果表明，将转录组表示建立在通路上可以产生更符合生物学原理且可解释的多模态模型，推动计算病理学超越基因级嵌入的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating histopathology with spatial transcriptomics (ST) provides apowerful opportunity to link tissue morphology with molecular function. Yetmost existing multimodal approaches rely on a small set of highly variablegenes, which limits predictive scope and overlooks the coordinated biologicalprograms that shape tissue phenotypes. We present PEaRL (Pathway EnhancedRepresentation Learning), a multimodal framework that representstranscriptomics through pathway activation scores computed with ssGSEA. Byencoding biologically coherent pathway signals with a transformer and aligningthem with histology features via contrastive learning, PEaRL reducesdimensionality, improves interpretability, and strengthens cross-modalcorrespondence. Across three cancer ST datasets (breast, skin, and lymph node),PEaRL consistently outperforms SOTA methods, yielding higher accuracy for bothgene- and pathway-level expression prediction (up to 58.9 percent and 20.4percent increase in Pearson correlation coefficient compared to SOTA). Theseresults demonstrate that grounding transcriptomic representation in pathwaysproduces more biologically faithful and interpretable multimodal models,advancing computational pathology beyond gene-level embeddings.</description>
      <author>example@mail.com (Sejuti Majumder, Saarthak Kapse, Moinak Bhattacharya, Xuan Xu, Alisa Yurovsky, Prateek Prasanna)</author>
      <guid isPermaLink="false">2510.03455v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.03375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了条件伪监督对比数据自由知识蒸馏(CPSC-DFKD)方法，解决了现有DFKD方法中的几个关键限制，提高了学生模型和生成器的性能。&lt;h4&gt;背景&lt;/h4&gt;数据自由知识蒸馏(DFKD)是解决模型压缩和传输限制同时保留隐私保护的有效方法，近年来受到广泛关注。现有方法主要使用生成器合成图像来支持蒸馏过程，但仍存在诸多问题。&lt;h4&gt;目的&lt;/h4&gt;探索DFKD中的伪监督范式；解决当前合成方法无法区分不同类别样本分布的问题；优化类别多样性的样本，帮助学生模型从多样样本中学习。&lt;h4&gt;方法&lt;/h4&gt;提出了条件伪监督对比数据自由知识蒸馏(CPSC-DFKD)方法；引入条件生成对抗网络合成特定类别的多样图像用于伪监督学习；改进生成器模块以区分不同类别的分布；提出基于教师和学生视图的伪监督对比学习以增强多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个常用数据集上的综合实验验证了CPSC-DFKD带来的学生模型和生成器性能提升。&lt;h4&gt;结论&lt;/h4&gt;CPSC-DFKD解决了现有DFKD方法的几个关键限制，包括无法区分不同类别样本分布和无法优化类别多样性样本的问题，通过创新的伪监督对比学习范式显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督知识蒸馏(DFKD)是解决模型压缩和传输限制同时保留隐私保护的有效方式，近年来引起了广泛关注。目前，大多数现有方法利用生成器合成图像来支持蒸馏过程。尽管当前方法已取得巨大成功，但仍有许多问题有待探索。首先，深度学习中监督学习的卓越性能促使我们在DFKD上探索伪监督范式。其次，当前合成方法无法区分不同类别样本的分布，从而产生可能被教师模型错误评估的模糊样本。此外，当前方法无法优化类别多样性的样本，这将阻碍学生模型从多样样本中学习并取得更好性能。在本文中，为解决上述限制，我们提出了一个新的学习范式，即条件伪监督对比数据自由知识蒸馏(CPSC-DFKD)。CPSC-DFKD的主要创新包括：(1)引入条件生成对抗网络合成特定类别的多样图像用于伪监督学习，(2)改进生成器模块以区分不同类别的分布，(3)提出基于教师和学生视图的伪监督对比学习以增强多样性。在三个常用数据集上的综合实验验证了CPSC-DFKD带来的学生模型和生成器性能提升。代码可在https://github.com/RoryShao/CPSC-DFKD.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.patcog.2023.109781&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data-free knowledge distillation~(DFKD) is an effective manner to solve modelcompression and transmission restrictions while retaining privacy protection,which has attracted extensive attention in recent years. Currently, themajority of existing methods utilize a generator to synthesize images tosupport the distillation. Although the current methods have achieved greatsuccess, there are still many issues to be explored. Firstly, the outstandingperformance of supervised learning in deep learning drives us to explore apseudo-supervised paradigm on DFKD. Secondly, current synthesized methodscannot distinguish the distributions of different categories of samples, thusproducing ambiguous samples that may lead to an incorrect evaluation by theteacher. Besides, current methods cannot optimize the category-wise diversitysamples, which will hinder the student model learning from diverse samples andfurther achieving better performance. In this paper, to address the abovelimitations, we propose a novel learning paradigm, i.e., conditionalpseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).The primary innovations of CPSC-DFKD are: (1) introducing a conditionalgenerative adversarial network to synthesize category-specific diverse imagesfor pseudo-supervised learning, (2) improving the modules of the generator todistinguish the distributions of different categories, and (3) proposingpseudo-supervised contrastive learning based on teacher and student views toenhance diversity. Comprehensive experiments on three commonly-used datasetsvalidate the performance lift of both the student and generator brought byCPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git</description>
      <author>example@mail.com (Renrong Shao, Wei Zhang, Jun wang)</author>
      <guid isPermaLink="false">2510.03375v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Latent Multi-view Learning for Robust Environmental Sound Representations</title>
      <link>http://arxiv.org/abs/2510.02500v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to DCASE 2025 Workshop. 4+1 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种多视图学习框架，整合对比原则到生成管道中，用于捕获声音源和设备信息，通过两个自监督目标实现更好的环境声音表示学习。&lt;h4&gt;背景&lt;/h4&gt;自监督学习方法（如对比和生成方法）已使用无标签数据推动了环境声音表示学习的进步，但这些方法如何在统一框架中相互补充仍较少研究。&lt;h4&gt;目的&lt;/h4&gt;研究对比方法和生成方法如何在统一框架中相互补充，并开发能捕获声音源和设备信息的多视图学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出多视图学习框架，将压缩音频潜在变量编码为视图特定和视图共享的子空间，由对比学习（促进子空间间信息流）和重建（保存整体信息）两个自监督目标指导。&lt;h4&gt;主要发现&lt;/h4&gt;在城市声音传感器网络数据集上的评估显示，该方法在声音源和传感器分类任务中优于传统自监督技术，且模型具有在结构化潜在空间中解离环境声音属性的潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的多视图学习框架成功整合了对比和生成方法，在环境声音表示学习中实现了更好的性能，并能解离环境声音属性。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习方法（如对比和生成方法）已经使用无标签数据推动了环境声音表示学习的进步。然而，这些方法如何在统一框架中相互补充仍然相对未被探索。在这项工作中，我们提出了一种多视图学习框架，将对比原则整合到生成管道中，以捕获声音源和设备信息。我们的方法将压缩音频潜在变量编码为视图特定和视图共享的子空间，由两个自监督目标指导：子空间之间的目标信息流的对比学习，以及整体信息保存的重建。我们在城市声音传感器网络数据集上评估了我们的方法，用于声音源和传感器分类，展示了与传统自监督技术相比改进的下游性能。此外，我们还研究了模型在变化训练配置下在结构化潜在空间中解离环境声音属性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) approaches, such as contrastive and generativemethods, have advanced environmental sound representation learning usingunlabeled data. However, how these approaches can complement each other withina unified framework remains relatively underexplored. In this work, we proposea multi-view learning framework that integrates contrastive principles into agenerative pipeline to capture sound source and device information. Our methodencodes compressed audio latents into view-specific and view-common subspaces,guided by two self-supervised objectives: contrastive learning for targetedinformation flow between subspaces, and reconstruction for overall informationpreservation. We evaluate our method on an urban sound sensor network datasetfor sound source and sensor classification, demonstrating improved downstreamperformance over traditional SSL techniques. Additionally, we investigate themodel's potential to disentangle environmental sound attributes within thestructured latent space under varied training configurations.</description>
      <author>example@mail.com (Sivan Ding, Julia Wilkins, Magdalena Fuentes, Juan Pablo Bello)</author>
      <guid isPermaLink="false">2510.02500v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval</title>
      <link>http://arxiv.org/abs/2510.03309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种轻量级的多模态表示对齐方法，通过使用轻量级投影头对齐化学和文本表示，无需训练完整的多模态模型。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型在药物发现和生物医学应用方面有潜力，但大多数现有方法依赖于大量预训练或大规模多模态语料库，计算成本较高。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以通过轻量级的对比桥梁对齐化学和文本表示，而不需要训练完整的多模态模型，以降低计算成本。&lt;h4&gt;方法&lt;/h4&gt;使用ChEMBL中的配对机制，通过双重线性投影和对比目标训练，将ECFP4分子指纹与生物医学句子嵌入进行对齐。为处理共享相同治疗靶点的药物，集成了困难负样本加权和边缘损失。&lt;h4&gt;主要发现&lt;/h4&gt;基于骨架分割的评估表明，该方法实现了非平凡的跨模态对齐，与冻结基线相比显著改善了靶点内辨别能力。&lt;h4&gt;结论&lt;/h4&gt;轻量级桥梁是大规模多模态预训练的高效替代方案，能够在精准医学中实现骨架感知的药物文本对齐和靶点特异性检索。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型在药物发现和生物医学应用方面有前景，但大多数现有方法依赖于大量预训练或大规模多模态语料库。我们研究了轻量级对比桥梁（冻结单模态编码器上的轻量级投影头）是否能够在不训练完整多模态模型的情况下对齐化学和文本表示。使用来自ChEMBL的配对机制，我们通过对比目标训练的双重线性投影将ECFP4分子指纹与生物医学句子嵌入进行对齐。为了更好地处理共享相同治疗靶点的药物，我们集成了困难负样本加权和边缘损失。基于骨架分割的评估（需要在不同化学核心之间泛化）表明，我们的方法实现了非平凡的跨模态对齐，并且与冻结基线相比显著改善了靶点内辨别能力。这些结果表明轻量级桥梁是大规模多模态预训练的高效替代方案，能够在精准医学中实现骨架感知的药物文本对齐和靶点特异性检索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal foundation models hold promise for drug discovery and biomedicalapplications, but most existing approaches rely on heavy pretraining or largescale multimodal corpora. We investigate whether thin contrastive bridges,lightweight projection heads over frozen unimodal encoders can align chemicaland textual representations without training a full multimodal model. Usingpaired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints withbiomedical sentence embeddings through dual linear projections trained with acontrastive objective. To better handle drugs sharing the same therapeutictarget, we incorporate hard negative weighting and a margin loss. Evaluationunder scaffold based splits, which require generalization across disjointchemical cores, demonstrates that our approach achieves non-trivial cross modalalignment and substantially improves within target discrimination compared tofrozen baselines. These results suggest that thin bridges offer a computeefficient alternative to large scale multimodal pretraining, enabling scaffoldaware drug text alignment and target specific retrieval in precision medicine.</description>
      <author>example@mail.com (Mallikarjuna Tupakula)</author>
      <guid isPermaLink="false">2510.03309v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation</title>
      <link>http://arxiv.org/abs/2510.05057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StaMo的无监督方法，通过轻量级编码器和预训练的扩散Transformer解码器学习高度压缩的双令牌状态表示，用于高效的世界建模和决策制定。该方法在LIBERO任务上提高14.3%性能，在真实世界任务中提高30%成功率，且推理开销小。研究发现，令牌间的差异可作为有效潜在动作，无需显式监督即可捕捉结构化动态，并增强策略协同训练，比先前方法提高10.4%。&lt;h4&gt;背景&lt;/h4&gt;具身智能面临的一个基本挑战是开发具有表现力和紧凑的状态表示，用于高效的世界建模和决策制定。然而，现有方法往往无法平衡这两点，产生的表示要么过于冗余，要么缺乏任务关键信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督方法，学习高度压缩的双令牌状态表示，用于高效的世界建模和决策制定，解决现有方法在表示冗余性和信息完整性之间的平衡问题。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级编码器和预训练的扩散Transformer (DiT) 解码器，利用其强大的生成先验，创建高效、可解释的状态表示，并无缝集成到现有的基于VLA的模型中。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在LIBERO上提高14.3%性能，在真实世界任务中提高30%成功率，推理开销最小；2) 通过潜在插值获得的令牌间差异可作为有效潜在动作，解码为可执行机器人动作；3) 这种能力表明表示在无需显式监督的情况下捕捉了结构化动态；4) 潜在动作增强了策略协同训练，比先前方法提高10.4%，可解释性更好。&lt;h4&gt;结论&lt;/h4&gt;StaMo方法能够从紧凑的状态表示学习可泛化的机器人运动，该状态表示从静态图像编码，挑战了学习潜在动作对复杂架构和视频数据的普遍依赖。该方法能有效扩展到各种数据源，包括真实机器人数据、模拟和人类第一人称视频。&lt;h4&gt;翻译&lt;/h4&gt;具身智能的一个基本挑战是开发具有表现力和紧凑的状态表示，用于高效的世界建模和决策制定。然而，现有方法往往无法实现这种平衡，产生的表示要么过于冗余，要么缺乏任务关键信息。我们提出一种无监督方法，使用轻量级编码器和预训练的扩散Transformer (DiT) 解码器学习高度压缩的双令牌状态表示，利用其强大的生成先验。我们的表示是高效的、可解释的，并能无缝集成到现有的基于VLA的模型中，在LIBERO上提高14.3%的性能，在真实世界任务成功率上提高30%，且推理开销最小。更重要的是，我们发现通过潜在插值获得的这两个令牌之间的差异自然地作为非常有效的潜在动作，可以进一步解码为可执行的机器人动作。这种涌现能力表明，我们的表示在无需显式监督的情况下捕捉了结构化动态。我们将此方法命名为StaMo，因为它能够从紧凑的状态表示学习可泛化的机器人运动，该状态表示从静态图像编码，挑战了学习潜在动作对复杂架构和视频数据的普遍依赖。产生的潜在动作也增强了策略协同训练，比先前方法高出10.4%，且可解释性更好。此外，我们的方法能够有效地扩展到各种数据源，包括真实机器人数据、模拟和人类第一人称视频。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fundamental challenge in embodied intelligence is developing expressive andcompact state representations for efficient world modeling and decision making.However, existing methods often fail to achieve this balance, yieldingrepresentations that are either overly redundant or lacking in task-criticalinformation. We propose an unsupervised approach that learns a highlycompressed two-token state representation using a lightweight encoder and apre-trained Diffusion Transformer (DiT) decoder, capitalizing on its stronggenerative prior. Our representation is efficient, interpretable, andintegrates seamlessly into existing VLA-based models, improving performance by14.3% on LIBERO and 30% in real-world task success with minimal inferenceoverhead. More importantly, we find that the difference between these tokens,obtained via latent interpolation, naturally serves as a highly effectivelatent action, which can be further decoded into executable robot actions. Thisemergent capability reveals that our representation captures structureddynamics without explicit supervision. We name our method StaMo for its abilityto learn generalizable robotic Motion from compact State representation, whichis encoded from static images, challenging the prevalent dependence to learninglatent action on complex architectures and video data. The resulting latentactions also enhance policy co-training, outperforming prior methods by 10.4%with improved interpretability. Moreover, our approach scales effectivelyacross diverse data sources, including real-world robot data, simulation, andhuman egocentric video.</description>
      <author>example@mail.com (Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, Chunhua Shen)</author>
      <guid isPermaLink="false">2510.05057v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns</title>
      <link>http://arxiv.org/abs/2510.05015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 11 figures, published on 2024 2nd International Conference  on Information and Communication Technology (ICICT 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于机器学习的帕金森病诊断方法，利用手绘螺旋和波形图像作为生物标志物，通过卷积神经网络、迁移学习和注意力机制构建模型，实现了93.3%的总体准确率。&lt;h4&gt;背景&lt;/h4&gt;帕金森病是一种进行性神经退行性疾病，由多巴胺能神经元死亡导致各种运动障碍症状。早期诊断对预防不良后果至关重要，但传统诊断方法通常繁琐且昂贵。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于机器学习的帕金森病检测方法，使用手绘螺旋和波形图像作为潜在生物标志物，提供一种非侵入性且经济有效的诊断解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用卷积神经网络、迁移学习和注意力机制提高模型性能和抗过拟合能力；通过数据增强增加图像数量提高多样性；架构分为三个阶段：使用预训练CNN、加入自定义卷积层、集成投票；采用硬投票聚合多个模型预测以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;螺旋图像的加权平均精确率、召回率和F1分数为90%；波形图像的加权平均精确率、召回率和F1分数为96.67%；通过集成硬投票组合预测后，总体准确率为93.3%。&lt;h4&gt;结论&lt;/h4&gt;机器学习在早期帕金森病诊断中具有潜力，提供了一种非侵入性且经济有效的解决方案，可以改善患者预后。&lt;h4&gt;翻译&lt;/h4&gt;帕金森病是一种进行性神经退行性疾病，其特点是多巴胺能神经元死亡，导致各种运动障碍症状。早期诊断对预防不良后果至关重要，但传统诊断方法通常繁琐且昂贵。本研究提出了一种基于机器学习方法，使用手绘螺旋和波形图像作为帕金森病检测的潜在生物标志物。我们的方法利用卷积神经网络、迁移学习和注意力机制来提高模型性能和抗过拟合能力。为了增强螺旋和波形类别的多样性和丰富性，训练数据集经过增强以增加图像数量。所提出的架构包含三个阶段：利用预训练CNN、加入自定义卷积层和集成投票。采用硬投票通过聚合多个模型的预测进一步提高了性能。实验结果显示了有希望的准确率。对于螺旋图像，加权平均精确率、召回率和F1分数为90%，对于波形图像为96.67%。通过集成硬投票组合预测后，总体准确率为93.3%。这些发现强调了机器学习在早期帕金森病诊断中的潜力，提供了一种非侵入性且经济有效的解决方案来改善患者预后。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parkinson's disease (PD) is a progressive neurodegenerative conditioncharacterized by the death of dopaminergic neurons, leading to various movementdisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,yet traditional diagnostic methods are often cumbersome and costly. In thisstudy, a machine learning-based approach is proposed using hand-drawn spiraland wave images as potential biomarkers for PD detection. Our methodologyleverages convolutional neural networks (CNNs), transfer learning, andattention mechanisms to improve model performance and resilience againstoverfitting. To enhance the diversity and richness of both spiral and wavecategories, the training dataset undergoes augmentation to increase the numberof images. The proposed architecture comprises three phases: utilizingpre-trained CNNs, incorporating custom convolutional layers, and ensemblevoting. Employing hard voting further enhances performance by aggregatingpredictions from multiple models. Experimental results show promising accuracyrates. For spiral images, weighted average precision, recall, and F1-score are90%, and for wave images, they are 96.67%. After combining the predictionsthrough ensemble hard voting, the overall accuracy is 93.3%. These findingsunderscore the potential of machine learning in early PD diagnosis, offering anon-invasive and cost-effective solution to improve patient outcomes.</description>
      <author>example@mail.com (Nabil Daiyan, Md Rakibul Haque)</author>
      <guid isPermaLink="false">2510.05015v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context</title>
      <link>http://arxiv.org/abs/2510.04912v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了四种目标检测模型在摩托车检测方面的性能，旨在为资源受限环境下的自动驾驶系统提供解决方案。&lt;h4&gt;背景&lt;/h4&gt;在卢旺达基加利，摩托车出租车是主要交通方式，它们行驶路线不可预测且经常无视交通规则，这对自动驾驶系统构成了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;比较四种目标检测模型（YOLOv5、Faster R-CNN、SSD和RetinaNet）在摩托车检测方面的性能，以评估它们在资源受限环境下的实时导航适用性。&lt;h4&gt;方法&lt;/h4&gt;使用在基加利收集的198张自定义数据集，在PyTorch框架下实现这些模型，并采用迁移学习。评估了模型的准确性、定位能力和推理速度。&lt;h4&gt;主要发现&lt;/h4&gt;确定了实施挑战，包括数据集限制和模型复杂性。&lt;h4&gt;结论&lt;/h4&gt;建议简化架构，以提高发展中国家如卢旺达的自动驾驶系统的可访问性。&lt;h4&gt;翻译&lt;/h4&gt;在卢旺达基加利，摩托车出租车是主要交通方式，它们常常不可预测地行驶且无视交通规则，对自动驾驶系统构成重大挑战。本研究比较了四种目标检测模型——YOLOv5、Faster R-CNN、SSD和RetinaNet——使用在基加利收集的198张自定义数据集进行摩托车检测。这些模型在PyTorch中通过迁移学习实现，并评估了其准确性、定位能力和推理速度，以评估它们在资源受限环境下实时导航的适用性。我们确定了实施挑战，包括数据集限制和模型复杂性，并建议简化架构，以提高发展中国家如卢旺达的自动驾驶系统的可访问性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,often navigating unpredictably and disregarding traffic rules, posingsignificant challenges for autonomous driving systems. This study compares fourobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--formotorbike detection using a custom dataset of 198 images collected in Kigali.Implemented in PyTorch with transfer learning, the models were evaluated foraccuracy, localization, and inference speed to assess their suitability forreal-time navigation in resource-constrained settings. We identifyimplementation challenges, including dataset limitations and modelcomplexities, and recommend simplified architectures for future work to enhanceaccessibility for autonomous systems in developing countries like Rwanda.</description>
      <author>example@mail.com (Ngeyen Yinkfu, Sunday Nwovu, Jonathan Kayizzi, Angelique Uwamahoro)</author>
      <guid isPermaLink="false">2510.04912v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements</title>
      <link>http://arxiv.org/abs/2510.04844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 15th International Workshop on Structural Health Monitoring  (IWSHM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种身势语识别框架，能够从3D骨骼关节数据中直接推断人类活动的交流功能，结合时空图卷积网络和卷积神经网络，利用迁移学习实现可扩展、准确且以人为本的行为建模。&lt;h4&gt;背景&lt;/h4&gt;理解人类与建成环境之间的动态关系是环境心理学到强化学习等多个学科的关键挑战。建模这些互动的主要障碍是无法以既可泛化又保护隐私的方式捕捉人类心理状态。传统方法依赖理论模型或问卷，存在范围有限、静态和劳动密集等限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接从3D骨骼关节数据推断人类活动交流功能(身势语)的框架，实现可扩展、准确且以人为本的行为建模，为增强基于强化学习的人类-环境互动模拟提供新途径。&lt;h4&gt;方法&lt;/h4&gt;提出一个身势语识别框架，结合时空图卷积网络(ST-GCN)和卷积神经网络(CNN)，利用迁移学习来绕过手动定义物理动作与心理类别之间映射的需求，保留用户匿名性的同时揭示反映认知和情绪状态的潜在身体运动结构。&lt;h4&gt;主要发现&lt;/h4&gt;在Dyadic User EngagemenT (DUET)数据集上的结果表明，该方法能够实现可扩展、准确且以人为本的行为建模，为增强基于强化学习的人类-环境互动模拟提供了新的途径。&lt;h4&gt;结论&lt;/h4&gt;该框架为理解人类与建成环境之间的动态关系提供了一种新方法，通过直接从身体动作数据推断心理状态，解决了传统方法的局限性，既保护了用户隐私，又能够捕捉人类心理状态。&lt;h4&gt;翻译&lt;/h4&gt;理解人类与建成环境之间的动态关系是环境心理学到强化学习(RL)等多个学科的关键挑战。建模这些互动的主要障碍是无法以既可泛化又保护隐私的方式捕捉人类心理状态。传统方法依赖理论模型或问卷，存在范围有限、静态和劳动密集等限制。我们提出一个身势语识别框架，能够直接从3D骨骼关节数据推断人类活动的交流功能——即身势语。结合时空图卷积网络(ST-GCN)和卷积神经网络(CNN)，该框架利用迁移学习来绕过手动定义物理动作与心理类别之间映射的需求。该方法保留用户匿名性，同时揭示反映认知和情绪状态的潜在身体运动结构。我们在Dyadic User EngagemenT (DUET)数据集上的结果表明，该方法能够实现可扩展、准确且以人为本的行为建模，为增强基于RL的人类-环境互动模拟提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the dynamic relationship between humans and the builtenvironment is a key challenge in disciplines ranging from environmentalpsychology to reinforcement learning (RL). A central obstacle in modeling theseinteractions is the inability to capture human psychological states in a waythat is both generalizable and privacy preserving. Traditional methods rely ontheoretical models or questionnaires, which are limited in scope, static, andlabor intensive. We present a kinesics recognition framework that infers thecommunicative functions of human activity -- known as kinesics -- directly from3D skeleton joint data. Combining a spatial-temporal graph convolutionalnetwork (ST-GCN) with a convolutional neural network (CNN), the frameworkleverages transfer learning to bypass the need for manually defined mappingsbetween physical actions and psychological categories. The approach preservesuser anonymity while uncovering latent structures in bodily movements thatreflect cognitive and emotional states. Our results on the Dyadic UserEngagemenT (DUET) dataset demonstrate that this method enables scalable,accurate, and human-centered modeling of behavior, offering a new pathway forenhancing RL-driven simulations of human-environment interaction.</description>
      <author>example@mail.com (Cheyu Lin, Katherine A. Flanigan)</author>
      <guid isPermaLink="false">2510.04844v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics</title>
      <link>http://arxiv.org/abs/2510.04753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探究了基于Transformer的架构在自然面对面对话场景中人物识别的性能，通过实现并评估一个双流框架，分别对关键点的空间配置和时间运动模式进行建模。&lt;h4&gt;背景&lt;/h4&gt;研究基于自然面对面对话场景下的人物识别，使用基于Transformer的架构。&lt;h4&gt;目的&lt;/h4&gt;探究基于Transformer的架构在自然对话场景中人物识别的性能。&lt;h4&gt;方法&lt;/h4&gt;实现并评估了一个双流框架，分别对133个COCO WholeBody关键点的空间配置和时间运动模式进行建模；从CANDOR对话语料库的子集中提取关键点；比较了预训练和从头开始的训练；研究了速度特征的使用；引入了多尺度时间Transformer进行分层运动建模。&lt;h4&gt;主要发现&lt;/h4&gt;领域特定的训练显著优于迁移学习；空间配置比时间动态包含更多的判别信息；空间Transformer达到95.74%的准确率；多尺度时间Transformer达到93.90%的准确率；特征级融合将性能提升到98.03%，证实了姿态和动态信息是互补的。&lt;h4&gt;结论&lt;/h4&gt;这些发现突出了Transformer架构在自然交互中人物识别的潜力，并为未来的多模态和跨文化研究提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了基于Transformer的架构在自然面对面对话场景下的人物识别性能。我们实现并评估了一个双流框架，分别对从CANDOR对话语料库子集中提取的133个COCO WholeBody关键点的空间配置和时间运动模式进行建模。我们的实验比较了预训练和从头开始的训练，研究了速度特征的使用，并引入了多尺度时间Transformer进行分层运动建模。结果表明，领域特定的训练显著优于迁移学习，且空间配置比时间动态包含更多的判别信息。空间Transformer达到95.74%的准确率，而多尺度时间Transformer达到93.90%的准确率。特征级融合将性能提升到98.03%，证实了姿态和动态信息是互补的。这些发现突出了Transformer架构在自然交互中人物识别的潜力，并为未来的多模态和跨文化研究提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the performance of transformer-based architecturesfor person identification in natural, face-to-face conversation scenario. Weimplement and evaluate a two-stream framework that separately models spatialconfigurations and temporal motion patterns of 133 COCO WholeBody keypoints,extracted from a subset of the CANDOR conversational corpus. Our experimentscompare pre-trained and from-scratch training, investigate the use of velocityfeatures, and introduce a multi-scale temporal transformer for hierarchicalmotion modeling. Results demonstrate that domain-specific trainingsignificantly outperforms transfer learning, and that spatial configurationscarry more discriminative information than temporal dynamics. The spatialtransformer achieves 95.74% accuracy, while the multi-scale temporaltransformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,confirming that postural and dynamic information are complementary. Thesefindings highlight the potential of transformer architectures for personidentification in natural interactions and provide insights for futuremultimodal and cross-cultural studies.</description>
      <author>example@mail.com (Masoumeh Chapariniya, Teodora Vukovic, Sarah Ebling, Volker Dellwo)</author>
      <guid isPermaLink="false">2510.04753v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing</title>
      <link>http://arxiv.org/abs/2510.04579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Wasserstein空间中Busemann函数的存在性和计算问题，并建立了两个重要情况下的闭式表达式，为概率分布设计了明确的投影方案，并定义了新的Sliced-Wasserstein距离。&lt;h4&gt;背景&lt;/h4&gt;Busemann函数在几何机器学习问题中受到广泛关注，它自然定义了Riemannian流形上的测地射线投影并推广了超平面概念。许多数据源可建模为概率分布，而Wasserstein空间具有由最优传输指标诱导的丰富形式Riemannian结构。&lt;h4&gt;目的&lt;/h4&gt;研究Wasserstein空间中Busemann函数的存在性和计算，该空间允许测地射线。&lt;h4&gt;方法&lt;/h4&gt;建立一维分布和高斯测度两种重要情况下的闭式表达式，为实数上的概率分布设计明确的投影方案，并定义高斯混合和标记数据集上的Sliced-Wasserstein距离。&lt;h4&gt;主要发现&lt;/h4&gt;在一维分布和高斯测度情况下获得了闭式表达式，设计了针对概率分布的明确投影方案，并定义了新的Sliced-Wasserstein距离。&lt;h4&gt;结论&lt;/h4&gt;在合成数据集和迁移学习问题上展示了这些原始方案的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Busemann函数最近在多种几何机器学习问题中引起了广泛关注，因为它自然地定义了Riemannian流形上的测地射线投影，并推广了超平面的概念。由于多个数据源可以方便地建模为概率分布，因此在具有由最优传输指标诱导的丰富形式Riemannian结构的Wasserstein空间中研究该函数是很自然的。在本工作中，我们研究了Wasserstein空间中Busemann函数的存在性和计算，该空间允许测地射线。我们在两个重要情况下建立了闭式表达式：一维分布和高斯测度。这些结果使得能够为实数上的概率分布设计明确的投影方案，进而使我们能够定义高斯混合和标记数据集上的新型Sliced-Wasserstein距离。我们在合成数据集以及迁移学习问题上展示了这些原始方案的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Busemann function has recently found much interest in a variety ofgeometric machine learning problems, as it naturally defines projections ontogeodesic rays of Riemannian manifolds and generalizes the notion ofhyperplanes. As several sources of data can be conveniently modeled asprobability distributions, it is natural to study this function in theWasserstein space, which carries a rich formal Riemannian structure induced byOptimal Transport metrics. In this work, we investigate the existence andcomputation of Busemann functions in Wasserstein space, which admits geodesicrays. We establish closed-form expressions in two important cases:one-dimensional distributions and Gaussian measures. These results enableexplicit projection schemes for probability distributions on $\mathbb{R}$,which in turn allow us to define novel Sliced-Wasserstein distances overGaussian mixtures and labeled datasets. We demonstrate the efficiency of thoseoriginal schemes on synthetic datasets as well as transfer learning problems.</description>
      <author>example@mail.com (Clément Bonet, Elsa Cazelles, Lucas Drumetz, Nicolas Courty)</author>
      <guid isPermaLink="false">2510.04579v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Categorical Invariants of Learning Dynamics</title>
      <link>http://arxiv.org/abs/2510.04376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种关于神经网络训练的新视角，将学习视为参数空间和已学习表征空间之间的结构保持变换，而非传统的梯度下降方法。&lt;h4&gt;背景&lt;/h4&gt;传统观点认为神经网络训练是在损失面上进行梯度下降的过程。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的理论框架来理解神经网络学习和泛化的本质。&lt;h4&gt;方法&lt;/h4&gt;使用范畴论中的函子概念描述学习过程，将学习定义为从参数空间到已学习表征空间的函子L，并通过持续同调和拉回构造等数学工具分析优化路径。&lt;h4&gt;主要发现&lt;/h4&gt;1) 不同训练过程若产生相似测试性能，通常属于优化路径的同一同伦类；2) 通过同伦轨迹收敛的网络泛化性能相差在0.5%准确率内，而非同伦路径则相差超过3%；3) 持续同调可识别与泛化相关的稳定最小值；4) 拉回构造形式化了迁移学习；5) 2-分类结构解释了不同优化算法产生功能等效模型的条件。&lt;h4&gt;结论&lt;/h4&gt;这种范畴论框架不仅提供了深度学习为何有效的理论见解，也为训练更鲁棒的网络提供了具体的算法原则。&lt;h4&gt;翻译&lt;/h4&gt;神经网络训练通常被视为在损失面上的梯度下降。我们提出了一种根本不同的视角：学习是参数空间和已学习表征空间之间的结构保持变换。这种范畴框架揭示，产生相似测试性能的不同训练过程通常属于优化路径的同一同伦类。我们通过实验证明，通过同伦轨迹收敛的网络泛化性能彼此相差在0.5%的准确率内，而非同伦路径则相差超过3%。该理论提供了实用工具：持续同调识别可预测泛化的稳定最小值，拉回构造形式化了迁移学习，2-分类结构解释了不同优化算法何时产生功能等效的模型。这些分类不变量既提供了深度学习为何有效的理论见解，也为训练更鲁棒的网络提供了具体的算法原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural network training is typically viewed as gradient descent on a losssurface. We propose a fundamentally different perspective: learning is astructure-preserving transformation (a functor L) between the space of networkparameters (Param) and the space of learned representations (Rep). Thiscategorical framework reveals that different training runs producing similartest performance often belong to the same homotopy class (continuousdeformation family) of optimization paths. We show experimentally that networksconverging via homotopic trajectories generalize within 0.5% accuracy of eachother, while non-homotopic paths differ by over 3%. The theory providespractical tools: persistent homology identifies stable minima predictive ofgeneralization (R^2 = 0.82 correlation), pullback constructions formalizetransfer learning, and 2-categorical structures explain when differentoptimization algorithms yield functionally equivalent models. These categoricalinvariants offer both theoretical insight into why deep learning works andconcrete algorithmic principles for training more robust networks.</description>
      <author>example@mail.com (Abdulrahman Tamim)</author>
      <guid isPermaLink="false">2510.04376v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>A Multilingual Framework for Dysarthria: Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation</title>
      <link>http://arxiv.org/abs/2510.03986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出一个基于AI的多语言统一框架，用于构音障碍的检测、分类、语音生成、语音转文本、情感检测和语音克隆，在英语、俄语和德语数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;构音障碍是一种运动性言语障碍，导致言语缓慢且难以理解，严重影响社交互动。它是帕金森病和ALS等神经系统疾病的特征，但现有工具缺乏跨语言和不同严重程度的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个解决构音障碍六个关键方面的AI多语言框架：二元构音障碍检测、严重程度分类、清晰语音生成、语音转文本转换、情感检测和语音克隆。&lt;h4&gt;方法&lt;/h4&gt;分析英语、俄语和德语数据集，使用基于频谱图的可视化和声学特征提取来指导模型训练，开发六个关键组件的模型并评估其性能。&lt;h4&gt;主要发现&lt;/h4&gt;二元检测模型在三种语言上达到97%准确率；严重程度分类模型也达到97%测试准确率；俄语翻译管道实现了低L1损失的可理解输出；通过跨语言迁移学习，英语模型微调后获得更好结果；语音转文本管道达到0.1367的词错误率。&lt;h4&gt;结论&lt;/h4&gt;该研究的结果和产品可用于诊断构音障碍，并改善不同语言患者的沟通和理解。&lt;h4&gt;翻译&lt;/h4&gt;研究训练了基于俄语构音障碍-清晰语音对的翻译管道，并探索了跨语言迁移学习，将俄语模型在英语数据上进行微调，实现了低资源设置下的有效应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dysarthria is a motor speech disorder that results in slow and oftenincomprehensible speech. Speech intelligibility significantly impactscommunication, leading to barriers in social interactions. Dysarthria is oftena characteristic of neurological diseases including Parkinson's and ALS, yetcurrent tools lack generalizability across languages and levels of severity. Inthis study, we present a unified AI-based multilingual framework that addressessix key components: (1) binary dysarthria detection, (2) severityclassification, (3) clean speech generation, (4) speech-to-text conversion, (5)emotion detection, and (6) voice cloning. We analyze datasets in English,Russian, and German, using spectrogram-based visualizations and acousticfeature extraction to inform model training. Our binary detection modelachieved 97% accuracy across all three languages, demonstrating stronggeneralization across languages. The severity classification model also reached97% test accuracy, with interpretable results showing model attention focusedon lower harmonics. Our translation pipeline, trained on paired Russiandysarthric and clean speech, reconstructed intelligible outputs with lowtraining (0.03) and test (0.06) L1 losses. Given the limited availability ofEnglish dysarthric-clean pairs, we fine-tuned the Russian model on English dataand achieved improved losses of 0.02 (train) and 0.03 (test), highlighting thepromise of cross-lingual transfer learning for low-resource settings. Ourspeech-to-text pipeline achieved a Word Error Rate of 0.1367 after threeepochs, indicating accurate transcription on dysarthric speech and enablingdownstream emotion recognition and voice cloning from transcribed speech.Overall, the results and products of this study can be used to diagnosedysarthria and improve communication and understanding for patients acrossdifferent languages.</description>
      <author>example@mail.com (Ananya Raghu, Anisha Raghu, Nithika Vivek, Sofie Budman, Omar Mansour)</author>
      <guid isPermaLink="false">2510.03986v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2510.03878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种多模态深度学习框架，整合临床、放射学和病理组织学图像，通过DenseNet-121卷积神经网络的加权集成，提高了口腔鳞状细胞癌的早期检测能力，总体准确率达到84.58%。&lt;h4&gt;背景&lt;/h4&gt;口腔鳞状细胞癌(OSCC)的晚期诊断导致其全球死亡率高，根据世界卫生组织统计，超过50%的病例在晚期阶段才被检测出来，5年生存率低于50%。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态深度学习框架，整合临床、放射学和病理组织学图像，使用加权集成DenseNet-121卷积神经网络，以提高OSCC的早期检测能力。&lt;h4&gt;方法&lt;/h4&gt;进行回顾性研究，使用公开可用的数据集代表三种不同的医学成像模态，通过迁移学习训练DenseNet-121 CNN，应用增强和模态特定预处理，使用验证加权集成策略融合预测，并使用准确率、精确率、召回率和F1分数进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;放射学(100%)和病理组织学(95.12%)模态的验证准确率很高，临床图像表现较低(63.10%)由于视觉异质性，集成模型在55个样本的多模态验证数据集上总体准确率达到84.58%，显示出改进的诊断鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;多模态集成框架通过提供非侵入性、AI辅助的分诊工具弥合了当前诊断工作流程中的差距，有助于增强高风险病变的早期识别，支持临床医生决策制定，符合全球肿瘤学指南，以减少诊断延迟并改善患者结局。&lt;h4&gt;翻译&lt;/h4&gt;目的：口腔鳞状细胞癌(OSCC)的晚期诊断对其全球高死亡率有显著贡献，根据世界卫生组织统计，超过50%的病例在晚期阶段才被检测出来，5年生存率低于50%。本研究旨在通过开发一个多模态深度学习框架来提高OSCC的早期检测，该框架整合了临床、放射学和病理组织学图像，使用DenseNet-121卷积神经网络(CNN)的加权集成。材料与方法：使用代表三种不同医学成像模态的公开可用数据集进行回顾性研究。每种模态特定的数据集用于通过迁移训练训练DenseNet-121 CNN。应用增强和模态特定的预处理以提高鲁棒性。使用验证加权集成策略融合预测。使用准确率、精确率、召回率和F1分数进行评估。结果：放射学(100%)和病理组织学(95.12%)模态的验证准确率很高，临床图像表现较低(63.10%)由于视觉异质性。集成模型在55个样本的多模态验证数据集上总体准确率为84.58%，显示出改进的诊断鲁棒性。结论：多模态集成框架通过提供非侵入性、AI辅助的分诊工具弥合了当前诊断工作流程中的差距，增强了高风险病变的早期识别。它支持临床医生决策制定，符合全球肿瘤学指南，以减少诊断延迟并改善患者结局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributessignificantly to its high global mortality rate, with over 50\% of casesdetected at advanced stages and a 5-year survival rate below 50\% according toWHO statistics. This study aims to improve early detection of OSCC bydeveloping a multimodal deep learning framework that integrates clinical,radiological, and histopathological images using a weighted ensemble ofDenseNet-121 convolutional neural networks (CNNs). Material and Methods Aretrospective study was conducted using publicly available datasetsrepresenting three distinct medical imaging modalities. Each modality-specificdataset was used to train a DenseNet-121 CNN via transfer learning.Augmentation and modality-specific preprocessing were applied to increaserobustness. Predictions were fused using a validation-weighted ensemblestrategy. Evaluation was performed using accuracy, precision, recall, F1-score.Results High validation accuracy was achieved for radiological (100\%) andhistopathological (95.12\%) modalities, with clinical images performing lower(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improveddiagnostic robustness with an overall accuracy of 84.58\% on a multimodalvalidation dataset of 55 samples. Conclusion The multimodal ensemble frameworkbridges gaps in the current diagnostic workflow by offering a non-invasive,AI-assisted triage tool that enhances early identification of high-risklesions. It supports clinicians in decision-making, aligning with globaloncology guidelines to reduce diagnostic delays and improve patient outcomes.</description>
      <author>example@mail.com (Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R)</author>
      <guid isPermaLink="false">2510.03878v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Searching for the Most Human-like Emergent Language</title>
      <link>http://arxiv.org/abs/2510.03467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 2025 Conference on Empirical Methods  in Natural Language Processing; 19 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文设计了一个基于信号博弈的涌现通信环境，通过超参数优化生成与人类语言高度相似的涌现语言，并研究了熵对涌现语言迁移学习性能的预测能力。&lt;h4&gt;背景&lt;/h4&gt;研究涌现通信系统的特性及其与人类语言的相似性，探索如何生成更接近人类语言的涌现语言。&lt;h4&gt;目的&lt;/h4&gt;设计能够产生与人类语言相似的涌现通信系统，并研究影响涌现语言质量的因素。&lt;h4&gt;方法&lt;/h4&gt;使用信号博弈构建涌现通信环境，采用XferBench作为目标函数进行超参数优化，XferBench通过测量深度迁移学习的适用性来量化涌现语言与人类语言的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;熵对涌现语言的迁移学习性能具有预测能力，涌现通信系统具有熵最小化特性，某些超参数能产生更真实的涌现语言（即更好地迁移到人类语言）。&lt;h4&gt;结论&lt;/h4&gt;通过超参数优化的信号博弈环境可以生成与人类语言相似的涌现语言，熵是评估涌现语言质量的有效指标。&lt;h4&gt;翻译&lt;/h4&gt;本文设计了一个基于信号博弈的涌现通信环境，以生成在相似性方面达到最先进水平的涌现语言。这是通过使用XferBench作为目标函数进行超参数优化完成的。XferBench通过测量涌现语言对人类语言深度迁移学习的适用性，来量化其与人类语言的统计相似性。此外，我们还证明了熵对涌现语言迁移学习性能的预测能力，并验证了先前关于涌现通信系统熵最小化特性的研究结果。最后，我们报告了哪些超参数能产生更真实的涌现语言，即那些能更好地迁移到人类语言的涌现语言。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we design a signalling game-based emergent communicationenvironment to generate state-of-the-art emergent languages in terms ofsimilarity to human language. This is done with hyperparameter optimization,using XferBench as the objective function. XferBench quantifies the statisticalsimilarity of emergent language to human language by measuring its suitabilityfor deep transfer learning to human language. Additionally, we demonstrate thepredictive power of entropy on the transfer learning performance of emergentlanguage as well as corroborate previous results on the entropy-minimizationproperties of emergent communication systems. Finally, we reportgeneralizations regarding what hyperparameters produce more realistic emergentlanguages, that is, ones which transfer better to human language.</description>
      <author>example@mail.com (Brendon Boldt, David Mortensen)</author>
      <guid isPermaLink="false">2510.03467v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Transfer Learning for High-Dimensional Linear Regression via Adaptive Shrinkage</title>
      <link>http://arxiv.org/abs/2510.03449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BLAST是一种贝叶斯多源迁移学习框架，用于高维线性回归。它结合全局-局部收缩先验和贝叶斯源选择，平衡信息共享和正则化，通过贝叶斯模型平均同时处理源选择和稀疏回归，使用Gibbs采样实现高效后验模拟。&lt;h4&gt;背景&lt;/h4&gt;在高维线性回归中，有效利用多源数据面临挑战，传统正则化方法可能无法充分利用多源信息，现有迁移学习方法在预测性能和不确定性量化方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合多源信息的高维线性回归框架，提高目标变量的后验推断准确性，同时保持良好的预测性能和不确定性量化能力。&lt;h4&gt;方法&lt;/h4&gt;提出BLAST框架，结合全局-局部收缩先验和贝叶斯源选择，通过贝叶斯模型平均处理源选择和稀疏回归，使用Gibbs采样进行后验模拟，并在模拟研究和TCGA肿瘤突变负担估计案例中验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;BLAST能提取最有用数据源同时排除导致负面迁移的偏差信息；相比仅基于目标数据的正则化方法，提供更准确的目标后验推断；相比当前最先进迁移学习方法，具有竞争性预测性能和更优的不确定性量化能力。&lt;h4&gt;结论&lt;/h4&gt;BLAST是计算上实用且推断上直接的方法，能有效处理高维线性回归中的多源迁移学习问题，在多个方面表现出优越性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了BLAST，即具有自适应收缩的贝叶斯线性回归用于迁移学习，这是一种用于高维线性回归的贝叶斯多源迁移学习框架。所提出的分析框架利用全局-局部收缩先验和贝叶斯源选择来平衡信息共享和正则化。我们展示了贝叶斯源选择如何允许提取最有用的数据源，同时排除可能导致负面迁移的偏差信息。在这个框架中，通过贝叶斯模型平均，源选择和稀疏回归在预测和推断中被同时考虑。我们的模型结构通过Gibbs采样算法允许高效的后验模拟，能够对目标回归系数进行完整后验推断，使BLAST在计算上实用且推断上直接。我们的方法为目标提供了比仅基于目标数据的正则化方法更准确的后验推断，同时与当前最先进的迁移学习方法相比提供了竞争性的预测性能和更优的不确定性量化。我们通过广泛的模拟研究验证了其有效性，并说明了当应用于从基因表达估计肿瘤突变负担的案例研究（使用癌症基因组图谱(TCGA)的数据）时的分析特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage forTransfer, a Bayesian multi-source transfer learning framework forhigh-dimensional linear regression. The proposed analytical framework leveragesglobal-local shrinkage priors together with Bayesian source selection tobalance information sharing and regularization. We show how Bayesian sourceselection allows for the extraction of the most useful data sources, whilediscounting biasing information that may lead to negative transfer. In thisframework, both source selection and sparse regression are jointly accountedfor in prediction and inference via Bayesian model averaging. The structure ofour model admits efficient posterior simulation via a Gibbs sampling algorithmallowing full posterior inference for the target regression coefficients,making BLAST both computationally practical and inferentially straightforward.Our method achieves more accurate posterior inference for the target thanregularization approaches based on target data alone, while offeringcompetitive predictive performance and superior uncertainty quantificationcompared to current state-of-the-art transfer learning methods. We validate itseffectiveness through extensive simulation studies and illustrate itsanalytical properties when applied to a case study on the estimation of tumormutational burden from gene expression, using data from The Cancer Genome Atlas(TCGA).</description>
      <author>example@mail.com (Parsa Jamshidian, Donatello Telesca)</author>
      <guid isPermaLink="false">2510.03449v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)</title>
      <link>http://arxiv.org/abs/2510.03355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于迁移学习的框架，使用长短期记忆网络(LSTMs)来预测铝合金的高周扭转S-N曲线，从而减少获取材料疲劳特性的成本和时间。&lt;h4&gt;背景&lt;/h4&gt;铝是一种广泛使用的合金，容易发生疲劳失效。表征材料的疲劳性能非常耗时且成本高昂，特别是对于高周疲劳数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于迁移学习的框架，以减少获取材料疲劳特性的成本和时间。&lt;h4&gt;方法&lt;/h4&gt;使用长短期记忆网络(LSTMs)构建了一个基于迁移学习的框架。首先基于纯轴向疲劳数据训练源LSTM模型（针对7075-T6铝合金），然后将该模型迁移用于预测高周扭转S-N曲线。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够准确预测更高周次范围的铝扭转S-N曲线。&lt;h4&gt;结论&lt;/h4&gt;该框架将有助于显著减少收集不同材料疲劳特性的成本，并帮助在更好的成本和时间约束下优先进行测试。&lt;h4&gt;翻译&lt;/h4&gt;铝是一种广泛使用的合金，容易发生疲劳失效。表征材料的疲劳性能非常耗时且成本高昂，特别是对于高周疲劳数据。为帮助缓解这一问题，研究人员开发了一种基于迁移学习的框架，使用长短期记忆网络(LSTMs)。在该框架中，基于7075-T6铝合金的纯轴向疲劳数据训练源LSTM模型，然后将其迁移用于预测高周扭转S-N曲线。该框架能够准确预测更高周次范围的铝扭转S-N曲线。研究人员相信，该框架将有助于显著减少收集不同材料疲劳特性的成本，并帮助在更好的成本和时间约束下优先进行测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aluminum is a widely used alloy, which is susceptible to fatigue failure.Characterizing fatigue performance for materials is extremely time and costdemanding, especially for high cycle data. To help mitigate this, a transferlearning based framework has been developed using Long short-term memorynetworks (LSTMs) in which a source LSTM model is trained based on pure axialfatigue data for Aluminum 7075-T6 alloy which is then transferred to predicthigh cycle torsional S-N curves. The framework was able to accurately predictAl torsional S-N curves for a much higher cycle range. It is the belief thatthis framework will help to drastically mitigate the cost of gathering fatiguecharacteristics for different materials and help prioritize tests with bettercost and time constraints.</description>
      <author>example@mail.com (Aryan Patel)</author>
      <guid isPermaLink="false">2510.03355v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies</title>
      <link>http://arxiv.org/abs/2510.03305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Supplement&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文分析了机器学习在气候建模中的应用案例，综合了不同项目中的工作流设计模式，旨在提供一个确保科学机器学习严谨性的框架，并促进跨学科合作。&lt;h4&gt;背景&lt;/h4&gt;机器学习在气候建模中的应用日益增加，包括系统模拟加速、数据驱动参数推断、预测和知识发现等领域，同时面临物理一致性、多尺度耦合、数据稀疏性、鲁棒泛化和与科学工作流集成等挑战。&lt;h4&gt;目的&lt;/h4&gt;分析机器学习在气候建模中的应用案例研究，重点关注设计选择和工作流结构，综合各种项目中的工作流设计模式，提供确保科学机器学习严谨性的框架，降低数据科学与气候建模跨学科合作的障碍。&lt;h4&gt;方法&lt;/h4&gt;通过分析一系列应用机器学习研究的案例，关注工作流设计模式而非技术细节，研究代理建模、机器学习参数化、概率编程、基于模拟的推断和物理信息迁移学习等多种工作流，分析这些工作流如何基于物理知识、模拟数据设计并集成观测数据。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习方法可应用于气候建模的不同方面，工作流设计需考虑物理知识、模拟数据和观测数据的集成，透明模型开发、关键评估、信息适应和可重复性对确保科学机器学习的严谨性至关重要。&lt;h4&gt;结论&lt;/h4&gt;通过提供透明模型开发、关键评估、信息适应和可重复性的框架，可以确保科学机器学习的严谨性，同时有助于降低数据科学和气候建模交叉学科合作的障碍。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在气候建模的系统模拟加速、数据驱动参数推断、预测和知识发现等方面应用日益广泛，解决了物理一致性、多尺度耦合、数据稀疏性、鲁棒泛化和与科学工作流集成等挑战。本文分析了机器学习在气候建模应用研究的一系列案例研究，重点关注设计选择和工作流结构。我们旨在综合机器学习赋能的气候建模中不同项目的工作流设计模式：从代理建模、机器学习参数化、概率编程，到基于模拟的推断和物理信息迁移学习。我们解析了这些工作流如何基于物理知识、由模拟数据提供信息并设计用于集成观测数据。我们旨在通过更透明的模型开发、关键评估、信息适应和可重复性提供确保科学机器学习严谨性的框架，并为降低数据科学与气候建模交叉学科合作的障碍做出贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning has been increasingly applied in climate modeling on systememulation acceleration, data-driven parameter inference, forecasting, andknowledge discovery, addressing challenges such as physical consistency,multi-scale coupling, data sparsity, robust generalization, and integrationwith scientific workflows. This paper analyzes a series of case studies fromapplied machine learning research in climate modeling, with a focus on designchoices and workflow structure. Rather than reviewing technical details, we aimto synthesize workflow design patterns across diverse projects in ML-enabledclimate modeling: from surrogate modeling, ML parameterization, probabilisticprogramming, to simulation-based inference, and physics-informed transferlearning. We unpack how these workflows are grounded in physical knowledge,informed by simulation data, and designed to integrate observations. We aim tooffer a framework for ensuring rigor in scientific machine learning throughmore transparent model development, critical evaluation, informed adaptation,and reproducibility, and to contribute to lowering the barrier forinterdisciplinary collaboration at the interface of data science and climatemodeling.</description>
      <author>example@mail.com (Tian Zheng, Subashree Venkatasubramanian, Shuolin Li, Amy Braverman, Xinyi Ke, Zhewen Hou, Peter Jin, Samarth Sanjay Agrawal)</author>
      <guid isPermaLink="false">2510.03305v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline</title>
      <link>http://arxiv.org/abs/2509.26440v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于Transformer的框架，用于动态对比增强MRI中乳腺病变的自动分类，通过SegFormer架构实现了高准确率，并创建了标准化数据集，为临床应用提供支持。&lt;h4&gt;背景&lt;/h4&gt;乳腺磁共振成像(MRI)是癌症检测和治疗规划的重要工具，但其临床应用受到特异性差的限制，导致假阳性率高和不必要的活检。&lt;h4&gt;目的&lt;/h4&gt;引入一个基于Transformer的框架，用于动态对比增强MRI中乳腺病变的自动分类，解决区分良性和恶性病变的挑战。&lt;h4&gt;方法&lt;/h4&gt;实现了SegFormer架构，通过语义分割量化恶性像素分布，创建了包含88名患者和133个注释病变的BreastDCEDL_AMBL数据集，并整合了超过1200名患者的训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;模型在病变级别分类中达到0.92的AUC，在患者水平上达到100%的敏感性和67%的特异性，有潜力在不遗漏恶性肿瘤的情况下消除三分之一的不必要活检。&lt;h4&gt;结论&lt;/h4&gt;公开发布的数据集、模型和评估协议为DCE-MRI病变分类提供了第一个标准化基准，促进了临床部署的方法学进展。&lt;h4&gt;翻译&lt;/h4&gt;乳腺磁共振成像是癌症检测和治疗规划的关键工具，但其临床效用受到特异性差的阻碍，导致高假阳性和不必要的活检。本研究引入了一种基于Transformer的框架，用于动态对比增强MRI中乳腺病变的自动分类，解决了区分良性和恶性发现的挑战。我们实现了SegFormer架构，在病变级别分类中达到0.92的AUC，在患者水平上达到100%的敏感性和67%的特异性 - 有可能在不遗漏恶性肿瘤的情况下消除三分之一的不必要活检。该模型通过语义分割量化恶性像素分布，产生可解释的空间预测，支持临床决策。为了建立可复现的基准，我们通过将癌症影像档案库的AMBL集合转换为标准化深度学习数据集，创建了BreastDCEDL_AMBL，包含88名患者和133个注释病变(89个良性，44个恶性)。这一资源解决了关键的基础设施差距，因为现有的公共数据集缺乏良性病变注释，限制了良恶性分类研究。训练通过与BreastDCEDL数据集整合，纳入了超过1200名患者的扩展队列，验证了仅使用原发性肿瘤注释的迁移学习方法。公开发布数据集、模型和评估协议为DCE-MRI病变分类提供了第一个标准化基准，推动了临床部署的方法学进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast magnetic resonance imaging is a critical tool for cancer detection andtreatment planning, but its clinical utility is hindered by poor specificity,leading to high false-positive rates and unnecessary biopsies. This studyintroduces a transformer-based framework for automated classification of breastlesions in dynamic contrast-enhanced MRI, addressing the challenge ofdistinguishing benign from malignant findings. We implemented a SegFormerarchitecture that achieved an AUC of 0.92 for lesion-level classification, with100% sensitivity and 67% specificity at the patient level - potentiallyeliminating one-third of unnecessary biopsies without missing malignancies. Themodel quantifies malignant pixel distribution via semantic segmentation,producing interpretable spatial predictions that support clinicaldecision-making. To establish reproducible benchmarks, we curatedBreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collectioninto a standardized deep learning dataset with 88 patients and 133 annotatedlesions (89 benign, 44 malignant). This resource addresses a key infrastructuregap, as existing public datasets lack benign lesion annotations, limitingbenign-malignant classification research. Training incorporated an expandedcohort of over 1,200 patients through integration with BreastDCEDL datasets,validating transfer learning approaches despite primary tumor-only annotations.Public release of the dataset, models, and evaluation protocols provides thefirst standardized benchmark for DCE-MRI lesion classification, enablingmethodological advancement toward clinical deployment.</description>
      <author>example@mail.com (Naomi Fridman, Anat Goldstein)</author>
      <guid isPermaLink="false">2509.26440v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>ResCP: Reservoir Conformal Prediction for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.05060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ResCP是一种基于储层计算的新型共形预测方法，通过动态重新加权一致性分数来处理时间序列数据，无需复杂的模型训练，适用于小样本和分布变化的情况。&lt;h4&gt;背景&lt;/h4&gt;共形预测提供了构建可交换数据分布无关预测区间的强大框架，但现有扩展到序列数据的方法依赖于拟合复杂模型捕获时间依赖性，这些方法在小样本时可能失效，且在分布变化时需要昂贵的重新训练。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种无需训练的共形预测方法，专门用于时间序列数据。&lt;h4&gt;方法&lt;/h4&gt;提出储层共形预测(ResCP)，利用储层计算效率和表示学习能力动态重新加权一致性分数，通过计算储层状态间相似性分数来自适应地重新加权每步的观测残差，从而在建模误差分布时考虑局部时间动态性。&lt;h4&gt;主要发现&lt;/h4&gt;在合理假设下，ResCP实现了渐近条件覆盖，并在多样化的预测任务中实证证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;ResCP能够有效处理时间序列数据，无需复杂训练，适用于小样本和分布变化的情况，为共形预测在序列数据应用中提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;共形预测为构建可交换数据的无分布预测区间提供了强大框架。现有将共形预测扩展到序列数据的方法依赖于拟合相对复杂的模型来捕获时间依赖性。然而，这些方法在样本量小时可能会失败，并且在底层分布变化时通常需要昂贵的重新训练。为克服这些局限性，我们提出了储层共形预测(ResCP)，一种用于时间序列的新型无需训练的共形预测方法。我们的方法利用储层计算的效率和表示学习能力来动态重新加权一致性分数。特别是，我们计算储层状态之间的相似性分数，并使用它们来自适应地重新加权每一步的观测残差。通过这种方法，ResCP使我们在建模误差分布时能够考虑局部时间动态性，同时不损害计算可扩展性。我们证明，在合理假设下，ResCP实现了渐近条件覆盖，并在多样化的预测任务中实证证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conformal prediction offers a powerful framework for buildingdistribution-free prediction intervals for exchangeable data. Existing methodsthat extend conformal prediction to sequential data rely on fitting arelatively complex model to capture temporal dependencies. However, thesemethods can fail if the sample size is small and often require expensiveretraining when the underlying data distribution changes. To overcome theselimitations, we propose Reservoir Conformal Prediction (ResCP), a noveltraining-free conformal prediction method for time series. Our approachleverages the efficiency and representation learning capabilities of reservoircomputing to dynamically reweight conformity scores. In particular, we computesimilarity scores among reservoir states and use them to adaptively reweightthe observed residuals at each step. With this approach, ResCP enables us toaccount for local temporal dynamics when modeling the error distributionwithout compromising computational scalability. We prove that, under reasonableassumptions, ResCP achieves asymptotic conditional coverage, and we empiricallydemonstrate its effectiveness across diverse forecasting tasks.</description>
      <author>example@mail.com (Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi)</author>
      <guid isPermaLink="false">2510.05060v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data</title>
      <link>http://arxiv.org/abs/2510.04927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为FedSSL-AMC的联邦自监督学习方法，用于自动调制分类(AMC)，解决了集中式训练的隐私问题和通信开销问题，同时提高了模型在信道变化和非独立同分布数据下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在集中式数据上训练自动调制分类模型会引发隐私问题，产生通信开销，并且往往无法对信道变化保持鲁棒性。联邦学习虽然避免了集中聚合，通过在分布式客户端上训练，但仍对类别不平衡、非独立同分布客户端分布和有限的标记样本敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决隐私问题、通信开销，同时对信道变化和非独立同分布数据具有鲁棒性的自动调制分类方法。&lt;h4&gt;方法&lt;/h4&gt;提出FedSSL-AMC方法，在各个客户端的无标记I/Q序列上训练具有三元组损失自监督的因果时间膨胀CNN，然后在小型标记集上为每个客户端训练SVM。建立了联邦表示学习过程的收敛性，以及对下游分类器在特征噪声下的可分性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据和实际空中数据集上的实验表明，在异构信噪比、载波频率偏移和非独立同分布标签分区条件下，与监督联邦学习基线相比，该方法取得了持续的性能提升。&lt;h4&gt;结论&lt;/h4&gt;FedSSL-AMC方法通过结合联邦学习和自监督学习，有效解决了AMC中的隐私、通信开销和鲁棒性问题，在各种信道条件下表现优于传统方法。&lt;h4&gt;翻译&lt;/h4&gt;在集中聚合数据上训练自动调制分类模型会引发隐私问题，产生通信开销，并且往往无法对信道变化保持鲁棒性。联邦学习通过在分布式客户端上训练避免了集中聚合，但仍对类别不平衡、非独立同分布客户端分布和有限的标记样本敏感。我们提出了FedSSL-AMC，它在各个客户端的无标记I/Q序列上训练具有三元组损失自监督的因果时间膨胀CNN，然后在小型标记集上为每个客户端训练SVM。我们建立了联邦表示学习过程的收敛性，以及对下游分类器在特征噪声下的可分性保证。在合成数据和实际空中数据集上的实验表明，在异构信噪比、载波频率偏移和非独立同分布标签分区条件下，与监督联邦学习基线相比，该方法取得了持续的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training automatic modulation classification (AMC) models on centrallyaggregated data raises privacy concerns, incurs communication overhead, andoften fails to confer robustness to channel shifts. Federated learning (FL)avoids central aggregation by training on distributed clients but remainssensitive to class imbalance, non-IID client distributions, and limited labeledsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN withtriplet-loss self-supervision on unlabeled I/Q sequences across clients,followed by per-client SVMs on small labeled sets. We establish convergence ofthe federated representation learning procedure and a separability guaranteefor the downstream classifier under feature noise. Experiments on synthetic andover-the-air datasets show consistent gains over supervised FL baselines underheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.</description>
      <author>example@mail.com (Usman Akram, Yiyue Chen, Haris Vikalo)</author>
      <guid isPermaLink="false">2510.04927v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</title>
      <link>http://arxiv.org/abs/2510.04876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Article under review by IJRR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个全面的多模态数据集，用于海洋底栖生境制图，包含约一百万个侧扫声纳瓦片及其相关数据，旨在为水下生境分类提供标准化基准。&lt;h4&gt;背景&lt;/h4&gt;海洋底栖生境制图对于理解海洋生态系统、指导保护工作和支持可持续资源管理至关重要。然而，大型标注数据集的稀缺限制了该领域机器学习模型的发展和基准测试。&lt;h4&gt;目的&lt;/h4&gt;创建一个全面的多模态数据集，建立水下生境制图的标准化基准，促进自主海底分类和多传感器集成技术的发展。&lt;h4&gt;方法&lt;/h4&gt;收集了约一百万个来自加泰罗尼亚海岸的侧扫声纳瓦片，配以水深图和自主水下航行器获取的光学图像。约36,000个SSS瓦片已手动标注分割掩码。研究将光学图像与SSS瓦片在空间上关联，促进自监督、跨模态表示学习，并提供开源的预处理和标注工具。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了一个大型多模态海洋底栖生境数据集，包含多种传感器数据，并提供了标注工具和数据关联方法，解决了多传感器数据融合的挑战。&lt;h4&gt;结论&lt;/h4&gt;这一资源为水下生境制图提供了标准化基准，有望促进自主海底分类和多传感器集成技术的进步，同时通过提供开源工具和数据增强了研究可访问性。&lt;h4&gt;翻译&lt;/h4&gt;海底生境制图对于理解海洋生态系统、指导保护工作和支持可持续资源管理至关重要。然而，大型标注数据集的稀缺限制了该领域机器学习模型的发展和基准测试。本文介绍了一个全面的多模态数据集，包含约一百万个沿加泰罗尼亚海岸收集的侧扫声纳瓦片，辅以水深图和一组使用自主水下航行器进行目标调查的配准光学图像。约36,000个SSS瓦片已手动标注分割掩码，用于监督分类模型的微调。所有原始传感器数据以及镶嵌图像也已发布，以支持进一步探索和算法开发。为解决AUV多传感器数据融合的挑战，我们在空间上将光学图像与相应的SSS瓦片关联，促进自监督、跨模态表示学习。提供了开源的预处理和标注工具，以提高可访问性和鼓励研究。这一资源旨在为水下生境制图建立标准化基准，促进自主海底分类和多传感器集成的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benthic habitat mapping is fundamental for understanding marine ecosystems,guiding conservation efforts, and supporting sustainable resource management.Yet, the scarcity of large, annotated datasets limits the development andbenchmarking of machine learning models in this domain. This paper introduces athorough multi-modal dataset, comprising about a million side-scan sonar (SSS)tiles collected along the coast of Catalonia (Spain), complemented bybathymetric maps and a set of co-registered optical images from targetedsurveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}of the SSS tiles have been manually annotated with segmentation masks to enablesupervised fine-tuning of classification models. All the raw sensor data,together with mosaics, are also released to support further exploration andalgorithm development. To address challenges in multi-sensor data fusion forAUVs, we spatially associate optical images with corresponding SSS tiles,facilitating self-supervised, cross-modal representation learning. Accompanyingopen-source preprocessing and annotation tools are provided to enhanceaccessibility and encourage research. This resource aims to establish astandardized benchmark for underwater habitat mapping, promoting advancementsin autonomous seafloor classification and multi-sensor integration.</description>
      <author>example@mail.com (Hayat Rajani, Valerio Franchi, Borja Martinez-Clavel Valles, Raimon Ramos, Rafael Garcia, Nuno Gracias)</author>
      <guid isPermaLink="false">2510.04876v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Compressed Concatenation of Small Embedding Models</title>
      <link>http://arxiv.org/abs/2510.04626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过连接多个小型嵌入模型并使用轻量级统一解码器来降低维度的方法，在保持高性能的同时实现模型压缩，适用于资源受限环境。&lt;h4&gt;背景&lt;/h4&gt;嵌入模型在密集检索、语义搜索和推荐系统中起核心作用，但其体积大，难以部署在资源受限环境如浏览器或边缘设备。&lt;h4&gt;目的&lt;/h4&gt;缩小小型嵌入模型与大型模型之间的性能差距，实现高效且高性能的模型部署。&lt;h4&gt;方法&lt;/h4&gt;通过连接多个小型模型的嵌入向量，并引入一个使用套娃表示学习(MRL)损失训练的轻量级统一解码器，将高维联合表示映射到低维空间，无需微调基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;连接多个小型模型可超越单个大型基线模型；虽然连接更多模型收益递减，但解码器表示在压缩和量化下的鲁棒性提高；在MTEB检索任务上，连接-编码-量化流程以48倍压缩因子恢复了89%的原始性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的连接-编码-量化管道能够在资源受限环境中实现高效部署，同时保持高性能，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;嵌入模型是密集检索、语义搜索和推荐系统的核心，但其体积通常使得在资源受限的环境中（如浏览器或边缘设备）部署变得不切实际。虽然较小的嵌入模型具有实际优势，但通常比大型模型性能较差。为了缩小这一差距，我们证明连接多个小型模型的原始嵌入向量可以在标准检索基准测试中超越单个大型基线模型。为了克服简单连接导致的高维度问题，我们引入了一个使用套娃表示学习(MRL)损失训练的轻量级统一解码器。这个解码器将高维联合表示映射到低维空间，在不需要微调基础模型的情况下保留了大部分原始性能。我们还表明，虽然连接更多基础模型会带来收益递减，但解码器表示在压缩和量化下的鲁棒性会提高。我们的实验显示，在MTEB检索任务的一个子集上，当将连接-编码-量化流程应用于四个小型嵌入模型时，该流程以48倍的压缩因子恢复了89%的原始性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760831&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embedding models are central to dense retrieval, semantic search, andrecommendation systems, but their size often makes them impractical to deployin resource-constrained environments such as browsers or edge devices. Whilesmaller embedding models offer practical advantages, they typicallyunderperform compared to their larger counterparts. To bridge this gap, wedemonstrate that concatenating the raw embedding vectors of multiple smallmodels can outperform a single larger baseline on standard retrievalbenchmarks. To overcome the resulting high dimensionality of naiveconcatenation, we introduce a lightweight unified decoder trained with aMatryoshka Representation Learning (MRL) loss. This decoder maps thehigh-dimensional joint representation to a low-dimensional space, preservingmost of the original performance without fine-tuning the base models. We alsoshow that while concatenating more base models yields diminishing gains, therobustness of the decoder's representation under compression and quantizationimproves. Our experiments show that, on a subset of MTEB retrieval tasks, ourconcat-encode-quantize pipeline recovers 89\% of the original performance witha 48x compression factor when the pipeline is applied to a concatenation offour small embedding models.</description>
      <author>example@mail.com (Mohamed Ayoub Ben Ayad, Michael Dinzinger, Kanishka Ghosh Dastidar, Jelena Mitrovic, Michael Granitzer)</author>
      <guid isPermaLink="false">2510.04626v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Conditional Representation Learning for Customized Tasks</title>
      <link>http://arxiv.org/abs/2510.04564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出条件表示学习(CRL)方法，能够根据用户指定标准提取定制化表示，无需高成本监督微调。该方法通过大语言模型生成描述性文本构建语义基，再利用视觉-语言模型将图像表示投影到条件特征空间，从而更好地捕获特定语义。&lt;h4&gt;背景&lt;/h4&gt;传统表示学习方法学习通用表示，主要捕获主导语义，可能与特定下游任务不完全一致。例如在动物栖息地分析中，研究人员关注场景相关特征，而通用嵌入强调类别语义，导致次优结果。&lt;h4&gt;目的&lt;/h4&gt;提出条件表示学习(CRL)，旨在提取符合任意用户指定标准的定制化表示。&lt;h4&gt;方法&lt;/h4&gt;基于空间语义由其基决定的原理，CRL首先使用大语言模型生成描述性文本来构建语义基，然后利用视觉-语言模型将图像表示投影到条件特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;条件表示能够更好地捕获特定标准的语义，可用于多种定制化任务。&lt;h4&gt;结论&lt;/h4&gt;在分类和检索任务上的大量实验证明了CRL的优越性和通用性。代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;传统的表示学习方法学习的是通用表示，主要捕获主导语义，这并不总是与定制的下游任务一致。例如，在动物栖息地分析中，研究人员优先考虑场景相关特征，而通用嵌入强调类别语义，导致次优结果。作为解决方案，现有方法采用监督微调，但这会产生高计算和标注成本。在本文中，我们提出条件表示学习(CRL)，旨在提取符合任意用户指定标准的定制化表示。具体来说，我们揭示空间的语义由其基决定，从而使得一组描述性词语可以近似定制特征空间的基。基于这一见解，给定用户指定的标准，CRL首先使用大语言模型(LLM)生成描述性文本来构建语义基，然后利用视觉-语言模型(VLM)将图像表示投影到这个条件特征空间。条件表示能够更好地捕获特定标准的语义，可用于多种定制化任务。在分类和检索任务上的大量实验证明了所提出的CRL的优越性和通用性。代码可在 https://github.com/XLearning-SCU/2025-NeurIPS-CRL 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional representation learning methods learn a universal representationthat primarily captures dominant semantics, which may not always align withcustomized downstream tasks. For instance, in animal habitat analysis,researchers prioritize scene-related features, whereas universal embeddingsemphasize categorical semantics, leading to suboptimal results. As a solution,existing approaches resort to supervised fine-tuning, which however incurs highcomputational and annotation costs. In this paper, we propose ConditionalRepresentation Learning (CRL), aiming to extract representations tailored toarbitrary user-specified criteria. Specifically, we reveal that the semanticsof a space are determined by its basis, thereby enabling a set of descriptivewords to approximate the basis for a customized feature space. Building uponthis insight, given a user-specified criterion, CRL first employs a largelanguage model (LLM) to generate descriptive texts to construct the semanticbasis, then projects the image representation into this conditional featurespace leveraging a vision-language model (VLM). The conditional representationbetter captures semantics for the specific criterion, which could be utilizedfor multiple customized tasks. Extensive experiments on classification andretrieval tasks demonstrate the superiority and generality of the proposed CRL.The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.</description>
      <author>example@mail.com (Honglin Liu, Chao Sun, Peng Hu, Yunfan Li, Xi Peng)</author>
      <guid isPermaLink="false">2510.04564v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Generative Representation Learning via Contrastive Policy Optimization</title>
      <link>http://arxiv.org/abs/2510.04506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一种新型框架，通过对比策略优化实现生成表征学习，将大型语言模型转变为可解释的代理，生成推理理由并基于这些理由进行对比学习，在MTEB基准测试中表现优异，显著提升了监督和无监督设置下的性能。&lt;h4&gt;背景&lt;/h4&gt;当前训练大型语言模型作为文本编码器的主流方法依赖于对比损失，这些方法将模型视为黑盒函数，放弃了其生成和推理能力，只使用静态嵌入。&lt;h4&gt;目的&lt;/h4&gt;引入GRACE（通过对比策略优化的生成表征学习）框架，重新构想对比信号不是要最小化的损失，而是引导生成策略的奖励，以保留大型语言模型的生成和推理能力。&lt;h4&gt;方法&lt;/h4&gt;在GRACE框架中，大型语言模型作为策略生成明确、可解释的推理理由，这些推理理由通过平均池编码为高质量嵌入，使用策略梯度优化，训练模型采用多组件奖励函数，该奖励函数最大化查询正对之间的相似性，同时最小化与负对的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;GRACE将大型语言模型从不透明的编码器转变为可解释的代理，其推理过程透明且可检查；在MTEB基准测试中，GRACE实现了广泛的跨类别提升，在监督设置下，四个骨干模型的平均总体得分比基线模型提高11.5%，无监督变体增加6.9%，同时保留了通用能力。&lt;h4&gt;结论&lt;/h4&gt;将对比目标视为推理理由上的奖励，统一了表征学习和生成，产生了更强的嵌入和透明的推理理由。&lt;h4&gt;翻译&lt;/h4&gt;目前训练大型语言模型作为文本编码器的方法依赖于对比损失，这些方法将模型视为黑盒函数，为了静态嵌入而放弃了其生成和推理能力。我们引入了GRACE（通过对比策略优化的生成表征学习），这是一个重新构想对比信号不是作为最小化的损失，而是作为引导生成策略的奖励的新型框架。在GRACE中，大型语言模型作为策略生成明确、人类可解释的推理理由——对其语义理解的结构化自然语言解释。然后这些推理理由通过平均池编码为高质量嵌入。使用策略梯度优化，我们训练模型采用多组件奖励函数，该函数最大化查询正对之间的相似性，同时最小化与负对的相似性。这使得大型语言模型从不透明的编码器转变为可解释的代理，其推理过程透明且可检查。在MTEB基准测试中，GRACE实现了广泛的跨类别提升：在四个骨干模型上平均，监督设置下比基线模型总体得分提高11.5%，无监督变体增加6.9%，同时保留了通用能力。这项工作将对比目标视为推理理由上的奖励，统一了表征学习和生成，以产生更强的嵌入和透明的推理理由。模型、数据和代码可在https://github.com/GasolSun36/GRACE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prevailing methods for training Large Language Models (LLMs) as text encodersrely on contrastive losses that treat the model as a black box function,discarding its generative and reasoning capabilities in favor of staticembeddings. We introduce GRACE (Generative Representation Learning viaContrastive Policy Optimization), a novel framework that reimagines contrastivesignals not as losses to be minimized, but as rewards that guide a generativepolicy. In GRACE, the LLM acts as a policy that produces explicit,human-interpretable rationales--structured natural language explanations of itssemantic understanding. These rationales are then encoded into high-qualityembeddings via mean pooling. Using policy gradient optimization, we train themodel with a multi-component reward function that maximizes similarity betweenquery positive pairs and minimizes similarity with negatives. This transformsthe LLM from an opaque encoder into an interpretable agent whose reasoningprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broadcross category gains: averaged over four backbones, the supervised settingimproves overall score by 11.5% over base models, and the unsupervised variantadds 6.9%, while preserving general capabilities. This work treats contrastiveobjectives as rewards over rationales, unifying representation learning withgeneration to produce stronger embeddings and transparent rationales. Themodel, data and code are available at https://github.com/GasolSun36/GRACE.</description>
      <author>example@mail.com (Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han)</author>
      <guid isPermaLink="false">2510.04506v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs</title>
      <link>http://arxiv.org/abs/2510.04241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAD-SGM的新方法，通过去噪扩散模型作为教师辅助，将自监督图神经网络的知识有效蒸馏到轻量级多层感知机中，提升了MLPs在图表示学习中的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在大规模应用中，人们倾向于用轻量级的多层感知机替代图神经网络，但在自监督图表示学习中，这种知识蒸馏更具挑战性，因为自监督学习的性能与模型的归纳偏置关系更密切。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的蒸馏方法，以弥合自监督图表示学习中图神经网络和多层感知机之间的巨大能力差距。&lt;h4&gt;方法&lt;/h4&gt;提出DAD-SGM(Diffusion-Assisted Distillation for Self-supervised Graph representation learning with MLPs)方法，采用去噪扩散模型作为教师辅助，更好地将教师GNN的知识蒸馏到学生MLP中。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，与最先进的GNN到MLP蒸馏方法相比，DAD-SGM能有效蒸馏自监督GNNs的知识，并显著增强MLPs在自监督图表示学习中的泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DAD-SGM是一种有效的自监督GNN知识蒸馏方法，相关实现代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;对于大规模应用，人们越来越有兴趣通过知识蒸馏用轻量级的多层感知机替代图神经网络。然而，在自监督图表示学习中将图神经网络蒸馏到多层感知机更具挑战性。这是因为自监督学习的性能比监督学习更依赖于模型的归纳偏置。这促使我们设计一种新的蒸馏方法，以弥合自监督图表示学习中图神经网络和多层感知机之间的巨大能力差距。在本文中，我们提出了DAD-SGM(用于多层感知机自监督图表示学习的扩散辅助蒸馏)。该方法采用去噪扩散模型作为教师辅助，以更好地将教师图神经网络的知识蒸馏到学生多层感知机中。这种方法增强了多层感知机在自监督图表示学习中的泛化能力和鲁棒性。大量实验表明，与最先进的图神经网络到多层感知机蒸馏方法相比，DAD-SGM能有效蒸馏自监督图神经网络的知识。我们的实现可在https://github.com/SeongJinAhn/DAD-SGM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TAI.2025.3598791&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For large-scale applications, there is growing interest in replacing GraphNeural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) viaknowledge distillation. However, distilling GNNs for self-supervised graphrepresentation learning into MLPs is more challenging. This is because theperformance of self-supervised learning is more related to the model'sinductive bias than supervised learning. This motivates us to design a newdistillation method to bridge a huge capacity gap between GNNs and MLPs inself-supervised graph representation learning. In this paper, we propose\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for\textbf{S}elf-supervised \textbf{G}raph representation learning with\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusionmodel as a teacher assistant to better distill the knowledge from the teacherGNN into the student MLP. This approach enhances the generalizability androbustness of MLPs in self-supervised graph representation learning. Extensiveexperiments demonstrate that DAD-SGM effectively distills the knowledge ofself-supervised GNNs compared to state-of-the-art GNN-to-MLP distillationmethods. Our implementation is available athttps://github.com/SeongJinAhn/DAD-SGM.</description>
      <author>example@mail.com (Seong Jin Ahn, Myoung-Ho Kim)</author>
      <guid isPermaLink="false">2510.04241v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition</title>
      <link>http://arxiv.org/abs/2510.04136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MoME（Mixture of Matryoshka Experts）的新型框架，将稀疏混合专家（MoE）集成到基于Matryoshka表示学习（MRL）的大语言模型中，用于音频-视觉语音识别。这种方法统一了MRL的适应性和MoE的效率，为资源感知的语音识别提供了可扩展且可解释的解决方案。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在音频-视觉语音识别方面显示出强大潜力，但其高计算需求和令牌粒度敏感性限制了在资源受限环境中的实用性。现有的令牌压缩方法需要在预先固定压缩率并产生固定长度的输出，无法灵活平衡信息密度和效率。Matryoshka表示学习虽能解决此问题，但当前方法在训练时独立处理每个尺度，限制了跨尺度泛化能力、高压缩下的鲁棒性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;克服现有基于MRL方法的局限性，提高跨尺度泛化能力、高压缩下的鲁棒性和可解释性，同时保持资源效率。&lt;h4&gt;方法&lt;/h4&gt;提出MoME框架，将稀疏混合专家集成到基于MRL的LLMs中。MoME通过冻结LLM并添加top-k路由和共享专家，允许跨尺度和模态的动态容量分配。共享路由器促进跨粒度的一致专家激活，使压缩序列能够受益于在较低压缩率下学习到的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在LRS2和LRS3上的实验表明，MoME在AVSR、ASR和VSR任务上实现了最先进的性能，同时需要显著更少的参数，并在噪声条件下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MoME统一了MRL的适应性和MoE的效率，为资源感知的语音识别提供了可扩展且可解释的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型最近在音频-视觉语音识别方面显示出强大的潜力，但其高计算需求和令牌粒度敏感性限制了它们在资源受限环境中的实用性。令牌压缩方法可以降低推理成本，但它们需要预先固定压缩率并产生单一固定长度的输出，无法在推理时灵活平衡信息密度和效率。Matryoshka表示学习通过使单个模型能够在多个令牌粒度上运行，允许动态调整压缩率来解决这个问题。然而，当前基于MRL的方法在训练时独立处理每个尺度，限制了跨尺度泛化能力、高压缩下的鲁棒性和可解释性。为了克服这些局限性，我们提出了MoME，一种将稀疏混合专家集成到基于MRL的LLMs中用于AVSR的新型框架。MoME通过冻结LLM并添加top-k路由和共享专家，允许跨尺度和模态的动态容量分配。共享路由器促进跨粒度的一致专家激活，使压缩序列能够受益于在较低压缩率下学习到的表示。在LRS2和LRS3上的实验表明，MoME在AVSR、ASR和VSR任务上实现了最先进的性能，同时需要显著更少的参数，并在噪声条件下保持鲁棒性。MoME统一了MRL的适应性和MoE的效率，为资源感知的语音识别提供了可扩展且可解释的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have recently shown strong potential inaudio-visual speech recognition (AVSR), but their high computational demandsand sensitivity to token granularity limit their practicality inresource-constrained settings. Token compression methods can reduce inferencecost, but they require fixing a compression rate in advance and produce asingle fixed-length output, offering no flexibility to balance informationdensity and efficiency at inference time. Matryoshka representation learning(MRL) addresses this by enabling a single model to operate across multipletoken granularities, allowing compression rates to be adjusted dynamically.However, current MRL-based methods treat each scale independently duringtraining, limiting cross-scale generalization, robustness at high compression,and interpretability. To overcome these limitations, we propose MoME (Mixtureof Matryoshka Experts), a novel framework that integrates sparseMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozenLLM with top-k routed and shared experts, allowing dynamic capacity allocationacross scales and modalities. A shared router promotes consistent expertactivation across granularities, enabling compressed sequences to benefit fromrepresentations learned at lower compression. Experiments on LRS2 and LRS3demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,and VSR tasks, while requiring significantly fewer parameters and maintainingrobustness under noise. MoME unifies the adaptability of MRL with theefficiency of MoE, offering a scalable and interpretable solution forresource-aware speech recognition.</description>
      <author>example@mail.com (Umberto Cappellazzo, Minsu Kim, Pingchuan Ma, Honglie Chen, Xubo Liu, Stavros Petridis, Maja Pantic)</author>
      <guid isPermaLink="false">2510.04136v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions</title>
      <link>http://arxiv.org/abs/2510.04126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ColdDTI的框架，用于冷启动药物-靶点相互作用(DTI)预测，该框架关注蛋白质的多层次结构，通过分层注意力机制挖掘蛋白质多层次结构与药物结构的相互作用，并融合不同级别的结构表示进行预测。&lt;h4&gt;背景&lt;/h4&gt;冷启动药物-靶点相互作用预测关注新型药物和蛋白质之间的相互作用。现有方法通常学习药物和蛋白质结构之间的可转移模式，但蛋白质组学研究表明蛋白质具有多层次结构(从初级到四级)，这些结构都影响DTI。然而，现有工作通常仅用初级结构表示蛋白质，限制了它们捕获涉及更高级结构相互作用的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个关注蛋白质多层次结构的框架用于冷启动DTI预测，以克服现有方法仅使用蛋白质初级结构的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出ColdDTI框架，采用分层注意力机制挖掘多层次蛋白质结构与药物结构在局部和全局粒度上的相互作用，然后利用这些相互作用融合不同级别的结构表示进行最终预测。&lt;h4&gt;主要发现&lt;/h4&gt;该设计捕获了生物学上可转移的先验知识，避免了对表示学习的过度依赖导致的过拟合风险。在基准数据集上的实验表明，ColdDTI在冷启动设置中始终优于以前的方法。&lt;h4&gt;结论&lt;/h4&gt;ColdDTI框架通过关注蛋白质多层次结构并采用分层注意力机制，有效提高了冷启动DTI预测的性能，为药物发现提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;冷启动药物-靶点相互作用(DTI)预测关注新型药物和蛋白质之间的相互作用。以前的方法通常学习药物和蛋白质结构之间的可转移相互作用模式来解决它。然而，蛋白质组学的见解表明蛋白质具有多层次结构，并且它们都影响DTI。现有工作通常仅用初级结构表示蛋白质，限制了它们捕获涉及更高级结构相互作用的能力。受这一见解的启发，我们提出了ColdDTI，一个关注蛋白质多层次结构用于冷启动DTI预测的框架。我们采用分层注意力机制挖掘多层次蛋白质结构(从初级到四级)与药物结构在局部和全局粒度上的相互作用。然后，我们利用挖掘到的相互作用融合不同级别的结构表示进行最终预测。我们的设计捕获了生物学上可转移的先验知识，避免了因过度依赖表示学习导致的过拟合风险。在基准数据集上的实验表明，ColdDTI在冷启动设置中始终优于以前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cold-start drug-target interaction (DTI) prediction focuses on interactionbetween novel drugs and proteins. Previous methods typically learn transferableinteraction patterns between structures of drug and proteins to tackle it.However, insight from proteomics suggest that protein have multi-levelstructures and they all influence the DTI. Existing works usually representprotein with only primary structures, limiting their ability to captureinteractions involving higher-level structures. Inspired by this insight, wepropose ColdDTI, a framework attending on protein multi-level structure forcold-start DTI prediction. We employ hierarchical attention mechanism to mineinteraction between multi-level protein structures (from primary to quaternary)and drug structures at both local and global granularities. Then, we leveragemined interactions to fuse structure representations of different levels forfinal prediction. Our design captures biologically transferable priors,avoiding the risk of overfitting caused by excessive reliance on representationlearning. Experiments on benchmark datasets demonstrate that ColdDTIconsistently outperforms previous methods in cold-start settings.</description>
      <author>example@mail.com (Ziying Zhang, Yaqing Wang, Yuxuan Sun, Min Ye, Quanming Yao)</author>
      <guid isPermaLink="false">2510.04126v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games</title>
      <link>http://arxiv.org/abs/2510.03591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 21st AAAI Conference on Artificial Intelligence and  Interactive Digital Entertainment (AIIDE 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合的Co-FineTuning (CFT)方法，用于解决视频游戏视觉缺陷检测中标记数据稀缺的问题，通过整合标记和非标记数据提高检测效率。&lt;h4&gt;背景&lt;/h4&gt;手动识别视频游戏视觉缺陷是一个资源密集且成本高昂的过程，需要专业知识。监督式视觉缺陷检测模型虽有前景，但依赖于大量标记数据，而这类缺陷很少出现，构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;克服监督式视觉缺陷检测模型对大量标记数据的依赖，提出一种能够有效整合标记和非标记数据的方法，减少对特定目标游戏标记样本的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出混合Co-FineTuning (CFT)方法，整合来自目标游戏和不同领域游戏的标记样本，同时利用未标记数据增强特征表示学习，最大化所有可用数据的效用。&lt;h4&gt;主要发现&lt;/h4&gt;开发的框架展示了增强的可扩展性和适应性，能在各种游戏标题中实现高效的视觉缺陷检测；实验结果表明该方法具有鲁棒性，与传统基线相比在多个游戏环境中表现优越；即使仅使用目标游戏50%的标记数据进行训练，CFT也能保持有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;CFT方法有效解决了视觉缺陷检测中标记数据稀缺的问题，具有良好的可扩展性和适应性，适用于多种游戏环境。&lt;h4&gt;翻译&lt;/h4&gt;手动识别视频游戏中的视觉缺陷是一个资源密集且成本高昂的过程，通常需要专业的领域知识。虽然监督式视觉缺陷检测模型提供了有前景的解决方案，但它们对大量标记数据的依赖由于此类缺陷的罕见性而构成了重大挑战。为克服这一限制，我们提出了一种混合的Co-FineTuning (CFT)方法，能够有效整合标记和非标记数据。我们的方法利用来自目标游戏和不同领域游戏的标记样本，同时结合未标记数据以增强特征表示学习。这种策略最大化了所有可用数据的效用，显著减少了对特定目标游戏标记样本的依赖。开发的框架展示了增强的可扩展性和适应性，促进了各种游戏标题中高效的视觉缺陷检测。我们的实验结果表明，所提出的方法对游戏视觉缺陷检测具有鲁棒性，在多个游戏环境中表现出优于传统基线的性能。此外，即使仅使用目标游戏50%的标记数据进行训练，CFT仍能保持有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manual identification of visual bugs in video games is a resource-intensiveand costly process, often demanding specialized domain knowledge. Whilesupervised visual bug detection models offer a promising solution, theirreliance on extensive labeled datasets presents a significant challenge due tothe infrequent occurrence of such bugs. To overcome this limitation, we proposea hybrid Co-FineTuning (CFT) method that effectively integrates both labeledand unlabeled data. Our approach leverages labeled samples from the target gameand diverse co-domain games, additionally incorporating unlabeled data toenhance feature representation learning. This strategy maximizes the utility ofall available data, substantially reducing the dependency on labeled examplesfrom the specific target game. The developed framework demonstrates enhancedscalability and adaptability, facilitating efficient visual bug detectionacross various game titles. Our experimental results show the robustness of theproposed method for game visual bug detection, exhibiting superior performancecompared to conventional baselines across multiple gaming environments.Furthermore, CFT maintains competitive performance even when trained with only50% of the labeled data from the target game.</description>
      <author>example@mail.com (Faliu Yi, Sherif Abdelfattah, Wei Huang, Adrian Brown)</author>
      <guid isPermaLink="false">2510.03591v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection</title>
      <link>http://arxiv.org/abs/2510.02610v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MINERVA是一种创新的监督特征选择方法，利用神经网络估计特征和目标之间的互信息，通过两阶段过程解耦表示学习和特征选择，能够捕捉高阶特征交互，解决了传统方法在处理复杂特征-目标关系时的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有的特征过滤器依赖于统计成对依赖性指标来建模特征-目标关系，但当目标依赖于高阶特征交互而非单个特征贡献时，这种方法可能失效。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的监督特征选择方法，能够捕捉复杂特征-目标关系，特别是当目标依赖于高阶特征交互时。&lt;h4&gt;方法&lt;/h4&gt;引入Mutual Information Neural Estimation Regularized Vetting Algorithm (MINERVA)，基于特征和目标之间互信息的神经估计进行监督特征选择；使用神经网络参数化互信息的近似，并使用精心设计的损失函数结合稀疏诱导正则化器进行特征选择；采用两阶段过程解耦表示学习与特征选择，通过评估特征子集集合来捕捉复杂特征-目标关系。&lt;h4&gt;主要发现&lt;/h4&gt;展示了文献中很少捕捉到的普遍依赖结构；所提出的方法能够有效捕捉这些复杂的特征-目标关系；在合成和真实欺诈数据集上的实验结果证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够执行精确解，具有更好的泛化能力，能够更准确地表达特征重要性。&lt;h4&gt;翻译&lt;/h4&gt;现有的特征过滤器依赖于统计成对依赖性指标来建模特征-目标关系，但当目标依赖于高阶特征交互而非单个特征贡献时，这种方法可能失效。我们引入了互信息神经估计正则化审查算法（MINERVA），这是一种基于特征和目标之间互信息神经估计的监督特征选择新方法。我们使用神经网络参数化互信息的近似，并使用精心设计的损失函数结合稀疏诱导正则化器进行特征选择。我们的方法通过两阶段过程实现，将表示学习与特征选择解耦，确保更好的泛化能力和更准确的特征重要性表达。我们展示了文献中很少捕捉到的普遍依赖结构，并通过评估特征子集集合证明我们提出的方法能够有效捕捉这些复杂的特征-目标关系。在合成和真实欺诈数据集上的实验结果证明了我们方法的有效性及其执行精确解的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing feature filters rely on statistical pair-wise dependence metrics tomodel feature-target relationships, but this approach may fail when the targetdepends on higher-order feature interactions rather than individualcontributions. We introduce Mutual Information Neural Estimation RegularizedVetting Algorithm (MINERVA), a novel approach to supervised feature selectionbased on neural estimation of mutual information between features and targets.We paramaterize the approximation of mutual information with neural networksand perform feature selection using a carefully designed loss functionaugmented with sparsity-inducing regularizers. Our method is implemented in atwo-stage process to decouple representation learning from feature selection,ensuring better generalization and a more accurate expression of featureimportance. We present examples of ubiquitous dependency structures that arerarely captured in literature and show that our proposed method effectivelycaptures these complex feature-target relationships by evaluating featuresubsets as an ensemble. Experimental results on synthetic and real-life frauddatasets demonstrate the efficacy of our method and its ability to performexact solutions.</description>
      <author>example@mail.com (Taurai Muvunza, Egor Kraev, Pere Planell-Morell, Alexander Y. Shestopaloff)</author>
      <guid isPermaLink="false">2510.02610v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations Through Contrastive Neural Model Checking</title>
      <link>http://arxiv.org/abs/2510.01853v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出对比神经网络模型检查(CNML)方法，将模型检查任务作为学习对齐表征的指导信号，通过自监督对比目标将逻辑规范和系统嵌入共享潜在空间，在工业启发检索任务上表现优异，且学习表征可有效迁移到下游任务并推广到复杂公式。&lt;h4&gt;背景&lt;/h4&gt;模型检查是验证安全关键系统是否符合形式规范的关键技术，深度学习应用显示出前景，但表征学习在形式验证领域仍探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索表征学习在形式验证领域的应用，利用模型检查任务作为学习表征的指导信号，提高形式验证效果。&lt;h4&gt;方法&lt;/h4&gt;提出对比神经网络模型检查(CNML)方法，通过自监督对比目标将逻辑规范和系统共同嵌入到共享的潜在空间中。&lt;h4&gt;主要发现&lt;/h4&gt;CNML在工业启发的检索任务上明显优于算法和神经基线；学习到的表征可以有效地迁移到下游任务；学习到的表征可以推广到更复杂的公式。&lt;h4&gt;结论&lt;/h4&gt;模型检查可以作为学习形式语言表征的目标函数。&lt;h4&gt;翻译&lt;/h4&gt;模型检查是验证安全关键系统是否符合形式规范的关键技术，最近深度学习的应用显示出前景。然而，尽管表征学习在视觉和语言领域无处不在，但在形式验证领域仍然探索不足。我们引入对比神经网络模型检查(CNML)，一种利用模型检查任务作为学习对齐表征指导信号的新方法。CNML通过自监督对比目标将逻辑规范和系统共同嵌入到共享的潜在空间中。在工业启发的检索任务上，CNML在跨模态和模态内设置上都明显优于算法和神经基线。我们进一步表明，学习到的表征可以有效地迁移到下游任务并推广到更复杂的公式。这些发现表明，模型检查可以作为学习形式语言表征的目标函数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model checking is a key technique for verifying safety-critical systemsagainst formal specifications, where recent applications of deep learning haveshown promise. However, while ubiquitous for vision and language domains,representation learning remains underexplored in formal verification. Weintroduce Contrastive Neural Model Checking (CNML), a novel method thatleverages the model checking task as a guiding signal for learning alignedrepresentations. CNML jointly embeds logical specifications and systems into ashared latent space through a self-supervised contrastive objective. Onindustry-inspired retrieval tasks, CNML considerably outperforms bothalgorithmic and neural baselines in cross-modal and intra-modal settings. Wefurther show that the learned representations effectively transfer todownstream tasks and generalize to more complex formulas. These findingsdemonstrate that model checking can serve as an objective for learningrepresentations for formal languages.</description>
      <author>example@mail.com (Vladimir Krsmanovic, Matthias Cosler, Mohamed Ghanem, Bernd Finkbeiner)</author>
      <guid isPermaLink="false">2510.01853v2</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets</title>
      <link>http://arxiv.org/abs/2510.03776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to the IEEE Robotics and Automation  Letters journal and presented at the 40th Anniversary of the IEEE  International Conference on Robotics and Automation, which was held in  Rotterdam, Netherlands on 23-26 September, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在复杂动态环境中导航的机器人如何通过预测周围智能体的未来行动和意图来高效到达目标并避免碰撞。研究提出并评估了基于类别的条件运动预测方法，特别是在数据有限或类别不平衡情况下的表现。&lt;h4&gt;背景&lt;/h4&gt;机器人和其他智能系统在复杂动态环境中导航时，需要准确预测周围智能体的未来行为以实现高效导航和避免碰撞。这些智能体的动态行为与其任务、角色或可观察标签密切相关。基于类别的运动预测可以减少预测不确定性，提高对不同类型智能体预测的准确性，但这一领域在现有研究中很少被探索，特别是在移动机器人和有限数据应用方面。&lt;h4&gt;目的&lt;/h4&gt;分析不同基于类别的轨迹预测方法在两个数据集上的表现，提出一套基于条件模式和高效深度学习的基线方法，并评估它们在机器人技术和户外数据集上的性能，特别关注在数据不平衡或新环境（数据不足）情况下的表现。&lt;h4&gt;方法&lt;/h4&gt;研究者在两个数据集上分析了不同的基于类别的轨迹预测方法，提出了一套基于条件模式和高效深度学习的基线方法，并在机器人技术和户外数据集（THOR-MAGNI和斯坦福无人机数据集）上评估了它们的性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在大多数情况下，考虑类别标签可以提高所有方法的准确性。在从不平衡数据集学习或在新环境中存在显著差异。具体来说，深度学习方法在平衡数据集上表现更好，但在数据有限的应用中（如机器人在新环境中的冷启动或类别不平衡），基于模式的方法可能更可取。&lt;h4&gt;结论&lt;/h4&gt;基于类别的条件运动预测可以提高预测准确性，但在不同数据分布和环境条件下，不同方法的表现有所差异。在数据有限或类别不平衡的情况下，基于模式的方法可能比深度学习方法更适用，这为实际应用中的方法选择提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;机器人和其他智能系统在复杂动态环境中导航时，应该预测周围智能体的未来行动和意图，以高效地达到目标并避免碰撞。这些智能体的动态行为很大程度上取决于它们的任务、角色或可观察标签。因此，基于类别的条件运动预测是一种减少预测不确定性并提高对异构智能体预测准确性的有吸引力的方法。然而，这在现有技术中很少被探索，特别是对于移动机器人和有限数据应用。在本文中，我们在两个数据集上分析了不同的基于类别的轨迹预测方法。我们提出了一套基于条件模式和高效深度学习的基线方法，并在机器人技术和户外数据集（THOR-MAGNI和斯坦福无人机数据集）上评估了它们的性能。我们的实验表明，在大多数情况下，当考虑类别标签时，所有方法都能提高准确性。更重要的是，我们观察到在从不平衡数据集学习或在新环境中（数据不足）存在显著差异。特别是，我们发现深度学习方法在平衡数据集上表现更好，但在数据有限的应用中，例如机器人在新环境中的冷启动或类别不平衡，基于模式的方法可能更可取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2024.3408510&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots and other intelligent systems navigating in complex dynamicenvironments should predict future actions and intentions of surrounding agentsto reach their goals efficiently and avoid collisions. The dynamics of thoseagents strongly depends on their tasks, roles, or observable labels.Class-conditioned motion prediction is thus an appealing way to reduce forecastuncertainty and get more accurate predictions for heterogeneous agents.However, this is hardly explored in the prior art, especially for mobile robotsand in limited data applications. In this paper, we analyse differentclass-conditioned trajectory prediction methods on two datasets. We propose aset of conditional pattern-based and efficient deep learning-based baselines,and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNIand Stanford Drone Dataset). Our experiments show that all methods improveaccuracy in most of the settings when considering class labels. Moreimportantly, we observe that there are significant differences when learningfrom imbalanced datasets, or in new environments where sufficient data is notavailable. In particular, we find that deep learning methods perform better onbalanced datasets, but in applications with limited data, e.g., cold start of arobot in a new environment, or imbalanced classes, pattern-based methods may bepreferable.</description>
      <author>example@mail.com (Tiago Rodrigues de Almeida, Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Johannes A. Stork, Martin Magnusson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2510.03776v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Modeling information acquisition via f-divergence and duality</title>
      <link>http://arxiv.org/abs/2510.03482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了一种基于多元统计差异理论的新实验成本函数f-information，推广了Sims的理性不注意模型和后验可分离成本函数类，并通过推导最优性条件来表征其行为预测。&lt;h4&gt;背景&lt;/h4&gt;f-information基于多元统计差异理论，是对现有理性不注意模型和后验可分离成本函数类的推广。&lt;h4&gt;目的&lt;/h4&gt;扩展Matejka和McKay (2015)以及Caplin、Dean和Leahy (2019)在互信息方面的工作，研究f-information在决策问题中的含义。&lt;h4&gt;方法&lt;/h4&gt;通过推导最优性条件来表征f-information的行为预测，并在多个典型决策问题中应用这些工具。&lt;h4&gt;主要发现&lt;/h4&gt;f-information框架可以使用微观经济学的熟悉方法进行分析，包括凸对偶和Arrow-Pratt预期效用方法。&lt;h4&gt;结论&lt;/h4&gt;f-information是一种通用的实验成本函数，能够扩展现有理论并应用于多种决策问题分析。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了一种基于多元统计差异理论的新实验成本函数f-information，它推广了Sims的经典理性不注意模型以及后验可分离成本函数类。我们通过推导最优性条件来表征其行为预测，这些条件扩展了Matejka和McKay (2015)以及Caplin、Dean和Leahy (2019)在互信息方面的工作。利用这些工具，我们研究了f-information在多个典型决策问题中的含义。该框架的一个优势是它可以使用微观经济学的熟悉方法进行分析：凸对偶和Arrow-Pratt预期效用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a new cost function over experiments, f-information, based onthe theory of multivariate statistical divergences, that generalizes Sims'sclassic model of rational inattention as well as the class ofposterior-separable cost functions. We characterize its behavioral predictionsby deriving optimality conditions that extend those of Matejka and McKay (2015)and Caplin, Dean, and Leahy (2019) beyond mutual information. Using thesetools, we study the implications of f-information in a number of canonicaldecision problems. A strength of the framework is that it can be analyzed usingfamiliar methods of microeconomics: convex duality and the Arrow-Pratt approachto expected utility.</description>
      <author>example@mail.com (Alex Bloedel, Tommaso Denti, Luciano Pomatto)</author>
      <guid isPermaLink="false">2510.03482v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration</title>
      <link>http://arxiv.org/abs/2510.05102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TopInG的拓扑可解释图学习框架，用于解决图神经网络在关键决策应用中缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在多个科学领域取得了显著成功，但在关键决策应用中常因缺乏可解释性而受限。现有的内在可解释GNN方法在处理复杂多样的基础理由子图时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的拓扑框架，能够识别持久的理由子图，以提高图神经网络的可解释性，同时保持预测性能。&lt;h4&gt;方法&lt;/h4&gt;TopInG采用持久同调来识别持久的理由子图，使用理由过滤学习方法建模理由子图的自回归生成过程，并引入拓扑差异作为自调整拓扑约束，强制理由子图与无关部分之间的持久拓扑区别。&lt;h4&gt;主要发现&lt;/h4&gt;在特定条件下，所提出的损失函数能被真实值唯一优化；广泛的实验表明，TopInG在处理各种理由子图、平衡预测性能与可解释性以及减轻虚假相关性方面有效；在预测准确性和解释质量上都优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;TopInG为图神经网络的可解释性提供了一种有效的解决方案，特别适用于处理复杂多样的理由子图，并能平衡预测性能与可解释性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在各个科学领域已展现出显著的成功，但它们在关键决策应用中的采用往往因缺乏可解释性而受到阻碍。最近，内在可解释的图神经网络已被研究用于通过识别图中的理由子结构来提供对模型预测的洞察。然而，当基础理由子图复杂多样时，现有方法面临挑战。在这项工作中，我们提出了TopInG：拓扑可解释图学习，这是一种利用持久同调来识别持久理由子图的新拓扑框架。TopInG采用理由过滤学习方法来建模理由子图的自回归生成过程，并引入了一种自调整拓扑约束，称为拓扑差异，以强制理由子图与无关对应物之间的持久拓扑区别。我们在特定条件下提供了理论保证，即我们的损失函数能被真实值唯一优化。广泛的实验证明了TopInG在处理关键挑战方面的有效性，如处理各种理由子图、平衡预测性能与可解释性以及减轻虚假相关性。结果表明，我们的方法在预测准确性和解释质量上都优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable success across variousscientific fields, yet their adoption in critical decision-making is oftenhindered by a lack of interpretability. Recently, intrinsically interpretableGNNs have been studied to provide insights into model predictions byidentifying rationale substructures in graphs. However, existing methods facechallenges when the underlying rationale subgraphs are complex and varied. Inthis work, we propose TopInG: Topologically Interpretable Graph Learning, anovel topological framework that leverages persistent homology to identifypersistent rationale subgraphs. TopInG employs a rationale filtration learningapproach to model an autoregressive generation process of rationale subgraphs,and introduces a self-adjusted topological constraint, termed topologicaldiscrepancy, to enforce a persistent topological distinction between rationalesubgraphs and irrelevant counterparts. We provide theoretical guarantees thatour loss function is uniquely optimized by the ground truth under specificconditions. Extensive experiments demonstrate TopInG's effectiveness intackling key challenges, such as handling variform rationale subgraphs,balancing predictive performance with interpretability, and mitigating spuriouscorrelations. Results show that our approach improves upon state-of-the-artmethods on both predictive accuracy and interpretation quality.</description>
      <author>example@mail.com (Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin Ding)</author>
      <guid isPermaLink="false">2510.05102v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>On the sensitivity of different galaxy properties to warm dark matter</title>
      <link>http://arxiv.org/abs/2510.05037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in The Astrophysical Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用DREAMS项目的1024个先进宇宙流体动力学模拟，探索温暗物质粒子质量对星系属性的影响。研究发现亚晕气体质量是约束WDM质量的最具信息量特征，确定系数高达0.9。通过比较MLP和GNN方法，表明预测能力主要来自全局描述符，晕级别信息提供有限增益。&lt;h4&gt;背景&lt;/h4&gt;研究温暗物质粒子质量如何影响星系属性，基于DREAMS项目的1024个最先进的宇宙流体动力学模拟。&lt;h4&gt;目的&lt;/h4&gt;探索温暗物质粒子质量对星系属性的影响，并开发有效的方法来约束WDM质量。&lt;h4&gt;方法&lt;/h4&gt;使用多层感知器结合标准化流分析星系群体的全局统计描述符；采用符号回归提取简单关系；使用图神经网络结合标准化流分析单个暗物质晕的属性来推断WDM质量。&lt;h4&gt;主要发现&lt;/h4&gt;亚晕气体质量是约束WDM质量的最具信息量特征，确定系数R平方等于0.9；基于全局特征的GNN方法仅比仅基于全局特征的MLP模型产生微小的改进。&lt;h4&gt;结论&lt;/h4&gt;预测WDM质量的能力主要存在于星系群体的全局描述符中，晕级别信息提供的额外增益有限。&lt;h4&gt;翻译&lt;/h4&gt;我们使用DREAMS项目的1024个最先进的宇宙流体动力学模拟研究温暗物质粒子质量对星系属性的影响。我们首先使用多层感知器结合标准化流来探索星系群体的全局统计描述符，如14种星系属性的平均值、标准差和直方图。我们发现亚晕气体质量是约束WDM质量的最具信息量的特征，确定系数R平方等于0.9。我们使用符号回归提取与WDM粒子质量的简单、可解释的关系。最后，我们采用更局部化的方法，选择单个暗物质晕，使用图神经网络结合标准化流来推断WDM质量，将亚晕属性作为节点特征，全局模拟统计作为图级特征。GNN方法仅比仅基于全局特征的MLP模型产生微小的改进，表明预测能力主要存在于全局描述符中，晕级别信息的增益有限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3847/1538-4357/ae0e6c&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the impact of warm dark matter (WDM) particle mass on galaxyproperties using 1,024 state-of-the-art cosmological hydrodynamical simulationsfrom the DREAMS project. We begin by using a Multilayer Perceptron (MLP)coupled with a normalizing flow to explore global statistical descriptors ofgalaxy populations, such as the mean, standard deviation, and histograms of 14galaxy properties. We find that subhalo gas mass is the most informativefeature for constraining the WDM mass, achieving a determination coefficient ofR^2 = 0.9. We employ symbolic regression to extract simple, interpretablerelations with the WDM particle mass. Finally, we adopt a more localizedapproach by selecting individual dark matter halos and using a Graph NeuralNetwork (GNN) with a normalizing flow to infer the WDM mass, incorporatingsubhalo properties as node features and global simulation statistics asgraph-level features. The GNN approach yields only a residual improvement overMLP models based solely on global features, indicating that most of thepredictive power resides in the global descriptors, with only marginal gainsfrom halo-level information.</description>
      <author>example@mail.com (Belén Costanza, Bonny Y. Wang, Francisco Villaescusa-Navarro, Alex M. Garcia, Jonah C. Rose, Mark Vogelsberger, Paul Torrey, Arya Farahi, Xuejian Shen, Ilem Leisher)</author>
      <guid isPermaLink="false">2510.05037v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection</title>
      <link>http://arxiv.org/abs/2510.04987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures (2 additional figures in Appendices)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NatGVD的新型攻击方法，用于生成自然对抗性漏洞代码，绕过基于图神经网络和图感知transformer的漏洞检测器，并通过实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;基于图的模型在代码分析任务中表现出色，但这些模型在漏洞检测中对对抗样本攻击的鲁棒性仍是一个开放问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为NatGVD的新型攻击方法，用于生成自然对抗性漏洞代码，绕过基于GNN和图感知transformer的漏洞检测器。&lt;h4&gt;方法&lt;/h4&gt;NatGVD采用一组代码转换方法，这些方法修改图结构同时保留代码语义。与之前的工作不同，NatGVD考虑了自然性要求：生成的样本不应被人类或程序分析工具轻易识别。&lt;h4&gt;主要发现&lt;/h4&gt;在最新的漏洞检测系统上对NatGVD进行广泛评估，结果显示在基于GNN的检测器和基于图感知transformer的检测器中，逃避率高达53.04%。&lt;h4&gt;结论&lt;/h4&gt;作者还探讨了潜在的防御策略，以增强这些系统对NatGVD的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;基于图的模型学习丰富的代码图结构信息，并在各种代码分析任务中表现出优越的性能。然而，在漏洞检测背景下，这些模型对抗对抗样本攻击的鲁棒性仍然是一个开放问题。本文提出了NatGVD，一种新颖的攻击方法，用于生成自然的对抗性漏洞代码，以规避基于GNN和图感知transformer的漏洞检测器。NatGVD采用一组代码转换，这些转换修改图结构同时保留代码语义。与之前注入死代码或不相关代码的工作不同，NatGVD考虑了自然性要求：生成的样本不应被人类或程序分析工具轻易识别。通过对最先进的漏洞检测系统进行广泛评估，结果显示在基于GNN的检测器和基于图感知transformer的检测器中，逃避率高达53.04%。我们还探讨了潜在的防御策略，以增强这些系统对NatGVD的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based models learn rich code graph structural information and presentsuperior performance on various code analysis tasks. However, the robustness ofthese models against adversarial example attacks in the context ofvulnerability detection remains an open question. This paper proposes NatGVD, anovel attack methodology that generates natural adversarial vulnerable code tocircumvent GNN-based and graph-aware transformer-based vulnerability detectors.NatGVD employs a set of code transformations that modify graph structure whilepreserving code semantics. Instead of injecting dead or unrelated code likeprevious works, NatGVD considers naturalness requirements: generated examplesshould not be easily recognized by humans or program analysis tools. Withextensive evaluation of NatGVD on state-of-the-art vulnerability detectionsystems, the results reveal up to 53.04% evasion rate across GNN-baseddetectors and graph-aware transformer-based detectors. We also explorepotential defense strategies to enhance the robustness of these systems againstNatGVD.</description>
      <author>example@mail.com (Avilash Rath, Weiliang Qi, Youpeng Li, Xinda Wang)</author>
      <guid isPermaLink="false">2510.04987v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study</title>
      <link>http://arxiv.org/abs/2510.04837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 10 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Bond Centered FingerPrint (BCFP)的分子指纹方法，作为Extended-Connectivity FingerPrints (ECFP)的补充。通过在BBBP分类任务上的评估，证明了结合使用这两种指纹可以显著提高预测性能，并提出了BCFP-Sort&amp;Slice特征组合方案以进一步提升效果。&lt;h4&gt;背景&lt;/h4&gt;Extended-Connectivity FingerPrints (ECFP)是分子描述的常用方法，但可能存在局限性。Bond Centered FingerPrint (BCFP)作为一种以化学键为中心的替代方法被提出，旨在补充ECFP的不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、以键为中心的分子描述符，能够与以原子为中心的圆形指纹(如ECFP)互补，提高Blood-Brain Barrier Penetration (BBBP)预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出静态BCFP，模仿有向消息传递GNN(如ChemProp)使用的键卷积；使用快速随机森林模型在BBBP分类任务上评估；通过分层交叉验证比较ECFP与BCFP组合的效果；提出BCFP-Sort&amp;Slice特征组合方案；将复合特征(键和原子特征)与MGTP预测方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;将ECFP与BCFP连接在一起比单独使用任何一种描述符都能提高AUROC和AUPRC；通过Turkey HSD多重比较分析确认了这一结果；半径r = 1表现最佳，r = 2未产生统计上可分离的增益；BCFP-Sort&amp;Slice方案保留了ECFP中的OOV信息，同时实现BCFP变体的紧凑连接；使用复合新特征在BBBP评估中优于MGTP预测。&lt;h4&gt;结论&lt;/h4&gt;轻量级的、以键为中心的描述符可以补充以原子为中心的圆形指纹，并为BBBP预测提供强大快速的基线。&lt;h4&gt;翻译&lt;/h4&gt;键中心指纹(BCFP)是对扩展连接性指纹(ECFP)的一种互补的、以键为中心的替代方案。我们引入了一种静态BCFP，它模仿了ChemProp等有向消息传递GNN使用的键卷积，并使用快速随机森林模型在血脑屏障穿透(BBBP)分类任务上对其进行了评估。在分层交叉验证中，将ECFP与BCFP连接在一起，比单独使用任何一种描述符都能提高AUROC和AUPRC，这一点通过Turkey HSD多重比较分析得到了确认。在半径方面，r = 1表现最佳；在相同测试下，r = 2没有产生统计上可分离的增益。我们进一步提出了BCFP-Sort&amp;Slice，这是一种简单的特征组合方案，它保留了ECFP计数向量中固有的词汇表外(OOV)计数信息，同时实现了BCFP变体的紧凑无哈希连接。我们还使用这种复合新特征(键和原子特征)在BBBP评估中优于MGTP预测。这些结果表明，轻量级的、以键为中心的描述符可以补充以原子为中心的圆形指纹，并为BBBP预测提供强大快速的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bond Centered FingerPrint (BCFP) are a complementary, bond-centricalternative to Extended-Connectivity Fingerprints (ECFP). We introduce a staticBCFP that mirrors the bond-convolution used by directed message-passing GNNslike ChemProp, and evaluate it with a fast rapid Random Forest model onBrain-Blood Barrier Penetration (BBBP) classification task. Across stratifiedcross-validation, concatenating ECFP with BCFP consistently improves AUROC andAUPRC over either descriptor alone, as confirmed by Turkey HSDmultiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does notyield statistically separable gains under the same test. We further proposeBCFP-Sort&amp;Slice, a simple feature-combination scheme that preserves theout-of-vocabulary (OOV) count information native to ECFP count vectors whileenabling compact unhashed concatenation of BCFP variants. We also outperformthe MGTP prediction on our BBBP evaluation, using such composite new featuresbond and atom features. These results show that lightweight, bond-centereddescriptors can complement atom-centered circular fingerprints and providestrong, fast baselines for BBBP prediction.</description>
      <author>example@mail.com (Guillaume Godin)</author>
      <guid isPermaLink="false">2510.04837v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning</title>
      <link>http://arxiv.org/abs/2510.04567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GILT框架，一种无LLM和无调整的架构，用于处理图数据的极端异质性，实现了高效的上下文学习和少样本分类任务。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在处理关系数据时强大，但在推广到未见过的图时存在困难，导致图基础模型(GFMs)的发展。当前GFMs面临图数据极端异质性的挑战，每个图可能具有独特的特征空间、标签集和拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;解决现有GFMs面临的异构性问题，克服两种主要范式的局限性：基于大型语言模型的方法难以处理图中的数值特征，而基于结构的模型适应新任务需要昂贵的逐图调整阶段。&lt;h4&gt;方法&lt;/h4&gt;提出GILT (Graph In-context Learning Transformer)框架，构建在无LLM和无调整的架构上，引入基于令牌的框架用于图中的上下文学习，将节点、边和图级别的分类任务重新框架化为统一框架，设计为处理通用数值特征，并能够从上下文中动态理解类语义。&lt;h4&gt;主要发现&lt;/h4&gt;GILT在少样本学习中表现更强，与基于LLM或基于调整的基线相比，用时显著减少，验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;GILT成功解决了现有GFMs面临的异构性问题，通过无LLM和无调整的架构，实现了高效的图学习和适应。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是处理关系数据的强大工具，但通常难以推广到未见过的图，这促进了图基础模型(GFMs)的发展。然而，当前GFMs面临图数据极端异质性的挑战，其中每个图可以具有独特的特征空间、标签集和拓扑结构。为此，出现了两种主要范式。第一种利用大型语言模型(LLMs)，但根本上依赖于文本，因此难以处理大量图中的数值特征。第二种预训练基于结构的模型，但适应新任务通常需要昂贵的逐图调整阶段，造成关键的效率瓶颈。在这项工作中，我们超越了这些限制，引入了GILT (Graph In-context Learning Transformer)，一个构建在无LLM和无调整架构上的框架。GILT引入了一种基于令牌的框架，用于图中的上下文学习(ICL)，将跨越节点、边和图级别的分类任务重新框架化为统一框架。这种机制是处理异质性的关键，因为它被设计为操作通用数值特征。此外，它从上下文动态理解类语义的能力实现了无调整适应。全面的实验表明，GILT比基于LLM或基于调整的基线实现更强的少样本性能，且用时显著减少，验证了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful tools for precessing relationaldata but often struggle to generalize to unseen graphs, giving rise to thedevelopment of Graph Foundational Models (GFMs). However, current GFMs arechallenged by the extreme heterogeneity of graph data, where each graph canpossess a unique feature space, label set, and topology. To address this, twomain paradigms have emerged. The first leverages Large Language Models (LLMs),but is fundamentally text-dependent, thus struggles to handle the numericalfeatures in vast graphs. The second pre-trains a structure-based model, but theadaptation to new tasks typically requires a costly, per-graph tuning stage,creating a critical efficiency bottleneck. In this work, we move beyond theselimitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-freearchitecture. GILT introduces a novel token-based framework for in-contextlearning (ICL) on graphs, reframing classification tasks spanning node, edgeand graph levels in a unified framework. This mechanism is the key to handlingheterogeneity, as it is designed to operate on generic numerical features.Further, its ability to understand class semantics dynamically from the contextenables tuning-free adaptation. Comprehensive experiments show that GILTachieves stronger few-shot performance with significantly less time thanLLM-based or tuning-based baselines, validating the effectiveness of ourapproach.</description>
      <author>example@mail.com (Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang)</author>
      <guid isPermaLink="false">2510.04567v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size</title>
      <link>http://arxiv.org/abs/2510.04440v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了基于分数热核动力学的标签传播和自训练新算法，并将其集成到图神经网络架构中，通过切比雪夫多项式近似使大规模图计算可行。该方法在标记样本有限的情况下表现优越。&lt;h4&gt;背景&lt;/h4&gt;研究基于信息论与抛物线演化方程物理学之间的经典对应关系，通过扩展经典扩散模型到拉普拉斯算子的分数幂，实现非局部交互和更全局的标签扩散。&lt;h4&gt;目的&lt;/h4&gt;开发一种在标记样本有限情况下有效的标签传播和自训练方法，增强图神经网络的表示能力，同时保持大规模图计算的可行性。&lt;h4&gt;方法&lt;/h4&gt;使用带源项的分数热核动力学进行标签传播和自训练，将分数热核集成到图卷积网络和图注意力网络等架构中，应用切比雪夫多项式近似处理大规模图计算。&lt;h4&gt;主要发现&lt;/h4&gt;通过扩展经典扩散模型到拉普拉斯算子的分数幂，非局部交互能够提供更全局的标签扩散，在已知标签监督和图上扩散之间的特殊平衡使该方法在标记样本有限情况下特别有效。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在标准数据集上证明了有效性，为半监督学习提供了一种新的解决方案，特别是在标记数据稀缺的场景下。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们引入了基于带源项的分数热核动力学的标签传播和自训练新算法。我们通过信息论与抛物线演化方程物理学之间的经典对应关系来论证该方法。我们将分数热核集成到图卷积网络和图注意力等图神经网络架构中，通过自适应的多跳扩散增强其表达能力。通过应用切比雪夫多项式近似，大规模图变得计算可行。变分公式的论证表明，通过将经典扩散模型扩展到拉普拉斯算子的分数幂，非局部交互提供了更全局的标签扩散。在已知标签监督和图上扩散之间的特殊平衡在只有少量标记训练样本的情况下特别有利。我们在标准数据集上证明了这种方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce novel algorithms for label propagation andself-training using fractional heat kernel dynamics with a source term. Wemotivate the methodology through the classical correspondence of informationtheory with the physics of parabolic evolution equations. We integrate thefractional heat kernel into Graph Neural Network architectures such as GraphConvolutional Networks and Graph Attention, enhancing their expressivenessthrough adaptive, multi-hop diffusion. By applying Chebyshev polynomialapproximations, large graphs become computationally feasible. Motivatingvariational formulations demonstrate that by extending the classical diffusionmodel to fractional powers of the Laplacian, nonlocal interactions deliver moreglobally diffusing labels. The particular balance between supervision of knownlabels and diffusion across the graph is particularly advantageous in the casewhere only a small number of labeled training examples are present. Wedemonstrate the effectiveness of this approach on standard datasets.</description>
      <author>example@mail.com (Farid Bozorgnia, Vyacheslav Kungurtsev, Shirali Kadyrov, Mohsen Yousefnezhad)</author>
      <guid isPermaLink="false">2510.04440v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields</title>
      <link>http://arxiv.org/abs/2510.04325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为FoilDiff的基于扩散模型的替代模型，用于预测翼型周围的流场。该模型采用混合主干去噪网络，结合卷积特征提取和基于Transformer的全局注意力机制，实现了更准确和适应性更强的流场结构表示。&lt;h4&gt;背景&lt;/h4&gt;翼型周围流场的准确预测对空气动力学设计和优化至关重要。计算流体动力学(CFD)模型虽然有效，但计算成本高昂，这促使了替代模型的发展以实现更快速的预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于扩散模型的替代模型，用于准确、高效地预测翼型周围的流场，同时提供更好的预测不确定性校准。&lt;h4&gt;方法&lt;/h4&gt;提出FoilDiff，一种基于扩散的替代模型，具有混合主干去噪网络。该设计结合了卷积特征提取和基于Transformer的全局注意力，以生成更适应性和准确的流场结构表示。使用去噪扩散隐式模型(DDIM)采样优化采样过程，同时不损害模型泛化能力。使用雷诺数、攻角和翼型几何的编码表示定义输入空间，以在广泛的空气动力学条件下实现泛化。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的模型相比，FoilDiff显示出显著的性能改进，在相同数据集上的平均预测误差减少了高达85%。结果表明，FoilDiff可以提供比现有基于扩散的模型更准确的预测和更好的预测不确定性校准。&lt;h4&gt;结论&lt;/h4&gt;FoilDiff作为基于扩散的替代模型，能够有效预测复杂流场，提供比现有方法更准确的预测结果和更好的不确定性估计，为空气动力学设计和优化提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;翼型周围流场的准确预测对空气动力学设计和优化至关重要。计算流体动力学(CFD)模型虽然有效，但计算成本高昂，这促使了替代模型的发展以实现更快速的预测。这些替代模型可以基于深度学习架构，如卷积神经网络(CNNs)、图神经网络(GNNs)和扩散模型(DMs)。扩散模型在预测复杂流场方面显示出巨大潜力。在这项工作中，我们提出了FoilDiff，一种具有混合主干去噪网络的基于扩散的替代模型。这种混合设计结合了卷积特征提取和基于Transformer的全局注意力，以生成更适应性和准确的流场结构表示。FoilDiff利用去噪扩散隐式模型(DDIM)采样，在不损害模型泛化能力的情况下优化采样过程的效率。我们使用雷诺数、攻角和翼型几何的编码表示来定义输入空间，以在广泛的空气动力学条件下实现泛化。与最先进的模型相比，FoilDiff显示出显著的性能改进，在相同数据集上的平均预测误差减少了高达85%。结果表明，FoilDiff可以提供比现有基于扩散的模型更准确的预测和更好的预测不确定性校准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate prediction of flow fields around airfoils is crucial foraerodynamic design and optimisation. Computational Fluid Dynamics (CFD) modelsare effective but computationally expensive, thus inspiring the development ofsurrogate models to enable quicker predictions. These surrogate models can bebased on deep learning architectures, such as Convolutional Neural Networks(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusionmodels have shown significant promise in predicting complex flow fields. Inthis work, we propose FoilDiff, a diffusion-based surrogate model with ahybrid-backbone denoising network. This hybrid design combines the power ofconvolutional feature extraction and transformer-based global attention togenerate more adaptable and accurate representations of flow structures.FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) samplingto optimise the efficiency of the sampling process at no additional cost tomodel generalisation. We used encoded representations of Reynolds number, angleof attack, and airfoil geometry to define the input space for generalisationacross a wide range of aerodynamic conditions. When evaluated againststate-of-the-art models, FoilDiff shows significant performance improvements,with mean prediction errors reducing by up to 85\% on the same datasets. Theresults have demonstrated that FoilDiff can provide both more accuratepredictions and better-calibrated predictive uncertainty than existingdiffusion-based models.</description>
      <author>example@mail.com (Kenechukwu Ogbuagu, Sepehr Maleki, Giuseppe Bruni, Senthil Krishnababu)</author>
      <guid isPermaLink="false">2510.04325v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid GNN-IZR Framework for Fast and Empirically Robust AC Power Flow Analysis in Radial Distribution Systems</title>
      <link>http://arxiv.org/abs/2510.04264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种结合图神经网络(GNN)和隐式Z总线递归(IZR)方法的混合框架，用于解决交流潮流计算中的速度与可靠性权衡问题。该框架通过物理信息GNN进行快速预测，并使用两级触发机制识别压力情况，调用IZR求解器作为故障保护，在测试中实现了100%的成功率。&lt;h4&gt;背景&lt;/h4&gt;交流潮流计算问题需要在数据驱动模型的速度和分析求解器的可靠性之间做出权衡。传统分析方法可靠但速度慢，而数据驱动模型速度快但可靠性不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合框架，结合数据驱动模型的速度和分析求解器的可靠性，实现近实时分析大量场景的能力。&lt;h4&gt;方法&lt;/h4&gt;结合图神经网络和隐式Z总线递归方法，使用物理信息GNN进行快速初始预测，通过两级触发机制识别压力情况，将失败案例(定义为最大功率失配超过0.1 p.u.)委托给IZR求解器处理。&lt;h4&gt;主要发现&lt;/h4&gt;在7,500个压力场景测试中，纯GNN模型失败率为13.11%，而混合框架实现了0.00%的失败率和100%的成功率；消融研究表明，物理信息训练和Z总线敏感特征对降低GNN失败率至关重要，将失败率从98.72%降至13.11%。&lt;h4&gt;结论&lt;/h4&gt;混合方法为实现分析求解器的经验可靠性同时利用GNN速度提供了一条实用路径，能够显著增加近实时可分析的场景数量。&lt;h4&gt;翻译&lt;/h4&gt;交流潮流计算问题需要在数据驱动模型的速度和分析求解器的可靠性之间做出权衡。本文引入了一种混合框架，将图神经网络与隐式Z总线递归方法相结合，这是一种用于辐射状配电网的稳健、非迭代求解器。该框架采用物理信息图神经网络进行快速初始预测，并通过两级触发机制调用IZR求解器作为压力情况的故障保护。失败被定义为任何最大功率失配超过0.1 p.u.的解，这是显著的运行偏差。在IEEE 33节点系统具有挑战性的7,500个压力场景测试集中，纯GNN模型在13.11%的案例中失败。相比之下，混合框架识别出所有潜在的失败案例，将它们委托给IZR求解器，实现了0.00%的失败率，在该特定测试集上 empirically 匹配了分析求解器的100%成功率。扩展的消融研究证实，物理信息训练和Z总线敏感特征都很关键，共同将GNN的失败率从98.72%(仅数据)降低到13.11%。混合方法为实现分析求解器的经验可靠性同时利用GNN速度提供了一条实用路径，能够显著增加近实时可分析的场景数量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Alternating Current Power Flow (ACPF) problem forces a trade-off betweenthe speed of data-driven models and the reliability of analytical solvers. Thispaper introduces a hybrid framework that synergizes a Graph Neural Network(GNN) with the Implicit Z-Bus Recursive (IZR) method, a robust, non-iterativesolver for radial distribution networks. The framework employs aphysics-informed GNN for rapid initial predictions and invokes the IZR solveras a failsafe for stressed cases identified by a two-stage trigger. A failureis defined as any solution with a maximum power mismatch exceeding 0.1 p.u., asignificant operational deviation. On a challenging test set of 7,500 stressedscenarios for the IEEE 33-bus system, the GNN-only model failed on 13.11 % ofcases. In contrast, the hybrid framework identified all potential failures,delegating them to the IZR solver to achieve a 0.00 % failure rate, empiricallymatching the 100 % success rate of the analytical solver on this specific testset. An expanded ablation study confirms that both physics-informed trainingand Z-bus sensitivity features are critical, collaboratively reducing the GNN'sfailure rate from 98.72 % (data-only) to 13.11 %. The hybrid approachdemonstrates a pragmatic path to achieving the empirical reliability of ananalytical solver while leveraging GNN speed, enabling a significant increasein the number of scenarios analyzable in near real-time.</description>
      <author>example@mail.com (Mohamed Shamseldein)</author>
      <guid isPermaLink="false">2510.04264v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling</title>
      <link>http://arxiv.org/abs/2510.04233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PAINET的新方法，用于建模多体系统中的3D动力学，通过捕捉未观察到的相互作用来提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;建模3D动力学是多体系统在科学和工程领域的基础问题，在轨迹预测和模拟中具有重要实际意义。现有的基于GNN的方法虽通过强制几何对称性、编码高阶特征或结合神经-ODE力学取得了强性能，但通常依赖于明确观察到的结构。&lt;h4&gt;目的&lt;/h4&gt;提出PAINET，一种用于学习多体系统中所有相互作用的SE(3)-等变神经架构，以捕捉现有方法无法获取的对复杂物理行为至关重要的未观察到的相互作用。&lt;h4&gt;方法&lt;/h4&gt;PAINET包含：(1)一种新颖的受物理学启发的注意力网络，源自能量函数的最小化轨迹；(2)一个并行解码器，在保持等变性的同时实现高效推理。&lt;h4&gt;主要发现&lt;/h4&gt;在人体动作捕捉、分子动力学和大规模蛋白质模拟等多样化真实世界基准测试中，PAINET始终优于最近提出的模型，在3D动力学预测中实现了4.7%到41.5%的错误减少，且在时间和内存计算成本方面具有可比性。&lt;h4&gt;结论&lt;/h4&gt;PAINET有效捕捉了多体系统中的未观察到的相互作用，在各种应用中表现出色，同时保持高计算效率。&lt;h4&gt;翻译&lt;/h4&gt;建模3D动力学是多体系统在科学和工程领域的基础问题，在轨迹预测和模拟中具有重要的实际意义。虽然最近的基于GNN的方法通过强制几何对称性、编码高阶特征或结合神经-ODE力学已经取得了强性能，但它们通常依赖于明确观察到的结构，并且本质上无法捕捉对复杂物理行为和动力学机制至关重要的未观察到的相互作用。在本文中，我们提出了PAINET，一种用于学习多体系统中所有相互作用的SE(3)-等变神经架构。该模型包括：(1)一种源自能量函数最小化轨迹的新颖的受物理学启发的注意力网络，和(2)一个保持等变性同时实现高效推理的并行解码器。在人体动作捕捉、分子动力学和大规模蛋白质模拟等多样化的真实世界基准测试中的经验结果表明，PAINET始终优于最近提出的模型，在3D动力学预测中实现了4.7%到41.5%的错误减少，同时在时间和内存计算成本方面具有可比性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是：在3D动力学建模中，现有的基于图神经网络的方法无法捕捉粒子之间未观察到的成对相互作用，而这些相互作用对于准确模拟复杂物理系统至关重要。这个问题在科学和工程领域非常重要，因为分子动力学、天体力学、物理模拟等多个领域都需要准确建模多体系统的3D动力学。忽略未观察到的相互作用会导致长期轨迹预测不准确，并掩盖长程相关性，在结晶、蛋白质折叠等情况下，基于固定观察结构的模型可能导致系统偏差和显著的误差积累。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从物理学中获取灵感，特别是能量最小化原理，将学习未观察相互作用的问题表述为最小化能量函数的问题。他们采用多项式势能形式（Landau-Ginzburg势能形式）推导出一个基于物理的前馈层。该方法借鉴了现有的图神经网络（如EGNN）和SE(3)-等变性概念，但创新点在于专注于捕捉未观察到的成对相互作用，而不仅仅是基于观察到的结构进行消息传递。作者设计了物理启发的注意力网络和并行等变解码器，在保持物理合理性的同时提高预测准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过能量最小化原理学习粒子之间的所有成对相互作用，包括未观察到的相互作用。整体实现流程包括：1) 初始化：输入初始粒子位置、速度和特征，通过MLP生成初始粒子嵌入；2) 编码阶段：递归应用物理启发的注意力网络更新粒子嵌入，使用自适应成对映射捕获特定于粒子类型的依赖关系；3) 解码阶段：使用并行等变解码器（基于EGNN）从初始状态生成多个时间步的预测位置，保持SE(3)-等变性；4) 训练和推理：以监督方式训练模型，最小化预测位置和真实位置之间的均方误差，推理时先计算粒子嵌入序列，再并行生成预测位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 能量基础的潜在结构学习公式，为将未观察到的相互作用纳入3D动力学建模提供原则性方法；2) 物理启发的注意力网络，具有自适应成对映射，能捕获超越观察结构的长程、特定于粒子类型的依赖关系；3) 并行等变解码器，在保持SE(3)-等变性的同时实现高效推理；4) 全局相互作用建模，能够建模所有粒子对之间的相互作用。相比之前的工作，PAINET不局限于局部邻域内的相互作用，而是显式地建模所有粒子对之间的未观察到的相互作用，结合了物理原理和深度学习的优势，在保持合理计算成本的同时实现了更高的准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAINET通过物理启发的能量最小化原理和自适应注意力机制，首次在3D动力学建模中有效捕捉了未观察到的全粒子对相互作用，显著提高了多体系统轨迹预测的准确性，同时保持了计算效率和物理合理性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling 3D dynamics is a fundamental problem in multi-body systems acrossscientific and engineering domains and has important practical implications intrajectory prediction and simulation. While recent GNN-based approaches haveachieved strong performance by enforcing geometric symmetries, encodinghigh-order features or incorporating neural-ODE mechanics, they typicallydepend on explicitly observed structures and inherently fail to capture theunobserved interactions that are crucial to complex physical behaviors anddynamics mechanism. In this paper, we propose PAINET, a principledSE(3)-equivariant neural architecture for learning all-pair interactions inmulti-body systems. The model comprises: (1) a novel physics-inspired attentionnetwork derived from the minimization trajectory of an energy function, and (2)a parallel decoder that preserves equivariance while enabling efficientinference. Empirical results on diverse real-world benchmarks, including humanmotion capture, molecular dynamics, and large-scale protein simulations, showthat PAINET consistently outperforms recently proposed models, yielding 4.7% to41.5% error reductions in 3D dynamics prediction with comparable computationcosts in terms of time and memory.</description>
      <author>example@mail.com (Kai Yang, Yuqi Huang, Junheng Tao, Wanyu Wang, Qitian Wu)</author>
      <guid isPermaLink="false">2510.04233v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs</title>
      <link>http://arxiv.org/abs/2510.04171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VBM-NET的学习方法，用于从场景的顶部正交投影中选择最优的移动基座姿态，实现高效物体抓取。该方法结合了等变性TransporterNet、图神经网络和强化学习，无需精确的物体姿态和环境模型信息。&lt;h4&gt;背景&lt;/h4&gt;在移动操作领域，选择最优的移动基座姿态对成功物体抓取至关重要。现有方法依赖经典规划或基于状态的策略学习，但都需要可靠的状态信息，如精确的物体姿态和环境模型。&lt;h4&gt;目的&lt;/h4&gt;研究直接从场景的顶部正交投影进行基座姿态规划，提供场景全局视角同时保留空间结构，并提出一种基于学习的方法VBM-NET用于基座姿态选择。&lt;h4&gt;方法&lt;/h4&gt;提出VBM-NET方法，使用等变性TransporterNet利用空间对称性高效学习候选基座姿态，采用图神经网络表示可变数量的候选姿态，并通过强化学习确定最优基座姿态。&lt;h4&gt;主要发现&lt;/h4&gt;VBM-NET在显著更少的计算时间内能产生与经典方法相当的结果；成功实现了从模拟到真实世界的策略迁移，验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;VBM-NET提供了一种高效的基座姿态选择方法，直接从场景的顶部正交投影中学习，无需精确的状态信息，同时保持与经典方法相当的性能但计算效率更高。&lt;h4&gt;翻译&lt;/h4&gt;在移动操作中，选择最优的移动基座姿态对于成功的物体抓取至关重要。先前的工作要么通过经典规划方法，要么通过学习基于状态的策略来解决这一问题。他们假设可以访问可靠的状态信息，如精确的物体姿态和环境模型。在这项工作中，我们直接从场景的顶部正交投影研究基座姿态规划，这种方法提供场景的全局视角，同时保留空间结构。我们提出了VBM-NET，一种使用这种顶部正交投影进行基座姿态选择的基于学习的方法。我们使用等变性TransporterNet利用空间对称性，并高效学习用于抓取的候选基座姿态。此外，我们使用图神经网络表示可变数量的候选基座姿态，并使用强化学习来确定它们之间的最优基座姿态。我们证明VBM-NET能够在显著更少的计算时间内产生与经典方法相当的结果。此外，我们通过将在模拟中训练的策略成功部署到真实世界的移动操作中，验证了模拟到真实的迁移学习。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决移动操作中机器人基座姿态选择问题，即在机器人需要移动到合适位置才能抓取物体时，如何确定最优的基座位置和方向。这个问题在现实中很重要，因为传统方法依赖精确的物体姿态估计和环境模型，而这些信息在实际应用中往往难以获取，尤其是在机器人距离物体较远时。有效的基座姿态规划能显著提高移动操作任务的效率和成功率，让机器人在没有精确状态信息的情况下也能完成抓取任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法和现有学习方法都依赖于精确状态估计的局限性，然后思考如何仅使用视觉输入来解决这个问题。他们选择使用正交俯视投影作为视觉表示，因为它能提供场景全局概览且保持空间结构。作者借鉴了等变性神经网络（ENN）中的TransporterNet来利用空间对称性，使用图神经网络（GNN）处理可变数量的候选基座姿态，并采用强化学习来确定最优姿态。整体设计是一个两阶段方法：先学习可能的基座姿态，再选择最优的一个。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用正交俯视投影的场景表示来学习基座姿态选择策略，不依赖精确状态估计，利用等变性神经网络处理空间对称性，使用图神经网络处理可变数量的候选基座姿态。整体流程分为两阶段：第一阶段使用等变性TransporterNet从场景正交投影中学习可能的基座姿态，确保抓取可行性和无碰撞；第二阶段使用图神经网络和强化学习从候选姿态中选择最优的一个，考虑导航成本。最后机器人导航到选定基座姿态并进行抓取操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次从视觉表示中学习基座姿态规划而不依赖精确状态估计；提出VBM-NET两阶段学习方法；使用等变性TransporterNet提高样本效率；使用图神经网络处理可变数量的候选姿态；成功实现模拟到现实迁移。相比之前工作，不同之处在于：传统方法依赖精确状态估计，而VBM-NET仅使用视觉输入；现有基于视觉的方法通常使用机器人自带的相机缺乏全局视野，而VBM-NET使用正交俯视投影；现有方法通常不考虑导航成本，而VBM-NET明确考虑了这一点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VBM-NET首次提出了一种仅使用正交俯视投影而不依赖精确状态估计的学习方法，通过结合等变性TransporterNet和图神经网络，实现了移动操作中考虑导航成本的最优基座姿态规划，并在模拟到现实中成功验证了其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Mobile Manipulation, selecting an optimal mobile base pose is essentialfor successful object grasping. Previous works have addressed this problemeither through classical planning methods or by learning state-based policies.They assume access to reliable state information, such as the precise objectposes and environment models. In this work, we study base pose planningdirectly from top-down orthographic projections of the scene, which provide aglobal overview of the scene while preserving spatial structure. We proposeVBM-NET, a learning-based method for base pose selection using such top-downorthographic projections. We use equivariant TransporterNet to exploit spatialsymmetries and efficiently learn candidate base poses for grasping. Further, weuse graph neural networks to represent a varying number of candidate base posesand use Reinforcement Learning to determine the optimal base pose among them.We show that VBM-NET can produce comparable solutions to the classical methodsin significantly less computation time. Furthermore, we validate sim-to-realtransfer by successfully deploying a policy trained in simulation to real-worldmobile manipulation.</description>
      <author>example@mail.com (Lakshadeep Naik, Adam Fischer, Daniel Duberg, Danica Kragic)</author>
      <guid isPermaLink="false">2510.04171v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity</title>
      <link>http://arxiv.org/abs/2510.03987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ICEPool，一种新的分层池化框架，用于增强模型对簇间连通性的理解，并保留原始图的结构完整性。ICEPool可与多种基于池化的图神经网络模型兼容，通过结合原有模型的优点和ICEPool强调簇间连通集成的能力，生成更全面和鲁棒的图级表示。&lt;h4&gt;背景&lt;/h4&gt;分层池化模型在分类图结构数据方面表现出色。虽然已经提出了许多创新方法来设计簇分配和粗化策略，但簇之间的关系往往被忽视。&lt;h4&gt;目的&lt;/h4&gt;引入ICEPool，一种新型分层池化框架，旨在增强模型对簇间连通性的理解，并保留原始图的结构完整性，解决传统模型忽视簇间关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出ICEPool（Inter-cluster Connectivity Enhancement Pooling），一种分层池化框架，强调簇间连通性的集成，可与多种基于池化的GNN模型兼容。通过理论分析证明ICEPool的图重构能力，展示其在学习传统模型忽视的簇间关系方面的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;ICEPool能够与多种模型兼容，并有望提升现有图神经网络架构的性能。理论分析证明ICEPool在图重构方面的能力，展示了其在学习簇间关系方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;ICEPool作为对现有模型的增强，有效结合了原始模型的优点和ICEPool强调簇间连通集成的能力，生成更全面和鲁棒的图级表示。实验结果表明ICEPool具有广泛的模型兼容性，并能提升现有图神经网络架构的性能。&lt;h4&gt;翻译&lt;/h4&gt;分层池化模型在分类图结构数据方面表现出色。虽然已经提出了许多创新方法来设计簇分配和粗化策略，但簇之间的关系往往被忽视。在本文中，我们介绍了ICEPool（Inter-cluster Connectivity Enhancement Pooling），一种新型分层池化框架，旨在增强模型对簇间连通性的理解以及保留原始图结构完整性的能力。ICEPool可与多种基于池化的GNN模型兼容。将ICEPool作为现有模型的增强，有效地结合了原始模型的优点和ICEPool强调簇间连通集成的能力，从而生成更全面和鲁棒的图级表示。此外，我们对ICEPool的图重构能力进行了理论分析，证明其在学习传统模型忽视的簇间关系方面的有效性。最后，实验结果表明了ICEPool与各种模型的兼容性及其提升现有图神经网络架构性能的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical Pooling Models have demonstrated strong performance inclassifying graph-structured data. While numerous innovative methods have beenproposed to design cluster assignments and coarsening strategies, therelationships between clusters are often overlooked. In this paper, weintroduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novelhierarchical pooling framework designed to enhance model's understanding ofinter-cluster connectivity and ability of preserving the structural integrityin the original graph. ICEPool is compatible with a wide range of pooling-basedGNN models. The deployment of ICEPool as an enhancement to existing modelseffectively combines the strengths of the original model with ICEPool'scapability to emphasize the integration of inter-cluster connectivity,resulting in a more comprehensive and robust graph-level representation.Moreover, we make theoretical analysis to ICEPool's ability of graphreconstruction to demonstrate its effectiveness in learning inter-clusterrelationship that is overlooked by conventional models. Finally, theexperimental results show the compatibility of ICEPool with wide varieties ofmodels and its potential to boost the performance of existing graph neuralnetwork architectures.</description>
      <author>example@mail.com (Michael Yang)</author>
      <guid isPermaLink="false">2510.03987v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.03923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了连续深度图神经网络(GNDEs)的收敛性分析，引入了Graphon神经微分方程(Graphon-NDEs)作为GNDEs的无限节点极限，证明了GNDE解到Graphon-NDE解的轨迹收敛性。研究推导了两种确定性图采样方案下的显式收敛率，并建立了大小转移界限，为GNDE模型在不同大小图之间的迁移提供了理论支持。&lt;h4&gt;背景&lt;/h4&gt;连续深度图神经网络(GNDEs)结合了图神经网络(GNNs)的结构归纳偏置和神经ODEs的连续深度架构，为在图上建模动力学提供了可扩展且原则性的框架。&lt;h4&gt;目的&lt;/h4&gt;对具有时变参数的GNDEs在无限节点极限情况下进行严格的收敛性分析，提供关于它们大小可转移性的理论见解。&lt;h4&gt;方法&lt;/h4&gt;引入Graphon神经微分方程(Graphon-NDEs)作为GNDEs的无限节点极限并建立其适定性；利用图论理论和动力系统工具证明GNDE解到Graphon-NDE解的轨迹收敛；推导两种确定性图采样方案下的显式收敛率；建立大小转移界限。&lt;h4&gt;主要发现&lt;/h4&gt;GNDEs在无限节点极限情况下收敛到Graphon-NDEs；在从平滑图子中采样的加权图和从{0,1}-值图子中采样的无权图两种方案下具有明确的收敛率；GNDE模型具有大小可转移性，可以将在中等规模图上训练的模型应用到更大图上而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;数值实验使用合成和真实数据支持了理论发现，验证了GNDEs的收敛性和大小可转移性。&lt;h4&gt;翻译&lt;/h4&gt;连续深度图神经网络，也称为图神经微分方程(GNDEs)，结合了图神经网络(GNNs)的结构归纳偏置和神经ODEs的连续深度架构，为在图上建模动力学提供了一个可扩展且原则性的框架。在本文中，我们提出了对具有时变参数的GNDEs在无限节点极限情况下的严格收敛性分析，为它们的大小可转移性提供了理论见解。为此，我们引入了图子神经微分方程(Graphon-NDEs)作为GNDEs的无限节点极限，并建立了它们的适定性。利用图子理论和动力系统工具，我们证明了GNDE解到Graphon-NDE解的轨迹收敛性。此外，我们在两种确定性图采样方案下推导了显式收敛率：(1)从平滑图子中采样的加权图，和(2)从{0,1}-值(不连续)图子中采样的无权图。我们进一步建立了大小转移界限，为将训练在中等规模图上的GNDE模型转移到更大、结构相似的图而无需重新训练的实际策略提供了理论依据。使用合成和真实数据的数值实验支持了我们的理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous-depth graph neural networks, also known as Graph NeuralDifferential Equations (GNDEs), combine the structural inductive bias of GraphNeural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,offering a scalable and principled framework for modeling dynamics on graphs.In this paper, we present a rigorous convergence analysis of GNDEs withtime-varying parameters in the infinite-node limit, providing theoreticalinsights into their size transferability. To this end, we introduce GraphonNeural Differential Equations (Graphon-NDEs) as the infinite-node limit ofGNDEs and establish their well-posedness. Leveraging tools from graphon theoryand dynamical systems, we prove the trajectory-wise convergence of GNDEsolutions to Graphon-NDE solutions. Moreover, we derive explicit convergencerates under two deterministic graph sampling regimes: (1) weighted graphssampled from smooth graphons, and (2) unweighted graphs sampled from$\{0,1\}$-valued (discontinuous) graphons. We further establish sizetransferability bounds, providing theoretical justification for the practicalstrategy of transferring GNDE models trained on moderate-sized graphs tolarger, structurally similar graphs without retraining. Numerical experimentsusing synthetic and real data support our theoretical findings.</description>
      <author>example@mail.com (Mingsong Yan, Charles Kulick, Sui Tang)</author>
      <guid isPermaLink="false">2510.03923v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.03351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CONCEPTNEURO的新型精神疾病诊断框架，通过结合大型语言模型和神经生物学领域知识，实现了高准确性和临床可解释性的精神疾病诊断。&lt;h4&gt;背景&lt;/h4&gt;近五分之一的青少年被诊断患有精神或行为健康问题，如焦虑、抑郁或行为障碍，凸显了开发准确且可解释诊断工具的紧迫性。静息态功能磁共振成像(rs-fMRI)提供了一种研究大规模功能连接的有力工具，但现有的图神经网络(GNN)方法仍存在黑盒问题，限制了其可靠性和临床应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于概念的诊断框架，解决现有GNN方法作为黑盒模型的问题，提高精神疾病诊断的可靠性和临床转化能力，同时保持高预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出CONCEPTNEURO框架，利用大型语言模型和神经生物学领域知识自动生成、过滤和编码可解释的功能连接概念。每个概念被表示为连接特定脑区的结构化子图，然后通过概念分类器进行处理，使预测基于临床上有意义的连接模式。&lt;h4&gt;主要发现&lt;/h4&gt;CONCEPTNEURO增强的GNN在多个精神疾病数据集上表现优于传统GNN，提高了诊断准确性，同时提供了透明且符合临床的解释。概念分析揭示了与专业知识一致且与特定疾病相关的连接模式，为未来研究提出了新假设。&lt;h4&gt;结论&lt;/h4&gt;CONCEPTNEURO是一个可解释、领域知情的精神疾病诊断框架，成功结合了高性能预测和临床可解释性，有望推动精神疾病诊断工具的临床应用。&lt;h4&gt;翻译&lt;/h4&gt;目前约五分之一的青少年被诊断患有精神或行为健康问题，如焦虑、抑郁或行为障碍，这凸显了开发准确且可解释诊断工具的紧迫性。静息态功能磁共振成像(rs-fMRI)为大规模功能连接提供了有力的视角，其中脑区被建模为节点，区域间同步性被建模为边，为精神障碍提供了临床相关的生物标志物。虽然先前的工作使用图神经网络(GNN)方法进行疾病预测，但它们仍然是复杂的黑盒，限制了其可靠性和临床转化。在这项工作中，我们提出了CONCEPTNEURO，一个基于概念的诊断框架，利用大型语言模型(LLMs)和神经生物学领域知识自动生成、过滤和编码可解释的功能连接概念。每个概念表示为连接特定脑区的结构化子图，然后通过概念分类器。我们的设计确保通过临床上有意义的连接模式进行预测，同时实现可解释性和强大的预测性能。在多个精神疾病数据集上的广泛实验表明，CONCEPTNEURO增强的GNN始终优于其传统版本，提高了准确性，同时提供透明、临床一致的解释。此外，概念分析突显了与专业知识一致的疾病特异性连接模式，并为未来研究提出了新假设，确立了CONCEPTNEURO作为精神疾病诊断的一种可解释、领域知情框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nearly one in five adolescents currently live with a diagnosed mental orbehavioral health condition, such as anxiety, depression, or conduct disorder,underscoring the urgency of developing accurate and interpretable diagnostictools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides apowerful lens into large-scale functional connectivity, where brain regions aremodeled as nodes and inter-regional synchrony as edges, offering clinicallyrelevant biomarkers for psychiatric disorders. While prior works use graphneural network (GNN) approaches for disorder prediction, they remain complexblack-boxes, limiting their reliability and clinical translation. In this work,we propose CONCEPTNEURO, a concept-based diagnosis framework that leverageslarge language models (LLMs) and neurobiological domain knowledge toautomatically generate, filter, and encode interpretable functionalconnectivity concepts. Each concept is represented as a structured subgraphlinking specific brain regions, which are then passed through a conceptclassifier. Our design ensures predictions through clinically meaningfulconnectivity patterns, enabling both interpretability and strong predictiveperformance. Extensive experiments across multiple psychiatric disorderdatasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperformtheir vanilla counterparts, improving accuracy while providing transparent,clinically aligned explanations. Furthermore, concept analyses highlightdisorder-specific connectivity patterns that align with expert knowledge andsuggest new hypotheses for future investigation, establishing CONCEPTNEURO asan interpretable, domain-informed framework for psychiatric disorder diagnosis.</description>
      <author>example@mail.com (Song Wang, Zhenyu Lei, Zhen Tan, Jundong Li, Javier Rasero, Aiying Zhang, Chirag Agarwal)</author>
      <guid isPermaLink="false">2510.03351v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Comparing fine-tuning strategies of MACE machine learning force field for modeling Li-ion diffusion in LiF for batteries</title>
      <link>http://arxiv.org/abs/2510.05020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了MACE机器学习模型与DeePMD势能在预测LiF中锂扩散率方面的性能，发现MACE模型在少量或无需训练数据的情况下能达到与DeePMD相当的准确性。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子间势能(MLIPs)正在改变材料科学和工程，使研究电池操作等复杂现象成为可能。&lt;h4&gt;目的&lt;/h4&gt;对MACE机器学习模型与训练良好的DeePMD势能进行基准测试，用于预测LiF中的间隙锂扩散率，LiF是锂离子电池固体电解质界面的关键组成部分。&lt;h4&gt;方法&lt;/h4&gt;通过分子动力学模拟来预测关键扩散特性，比较MACE模型与DeePMD模型的预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;MACE-MPA-0基础模型预测活化能为0.22 eV，仅用300个数据点微调的模型预测为0.20 eV，两者都与DeePMD模型的参考值0.24 eV接近；微调方法只需少量数据即可达到与DeePMD相当的性能。&lt;h4&gt;结论&lt;/h4&gt;微调方法只需要一小部分训练数据就能达到与超过40,000个主动学习数据训练的DeePMD势能相当的稳健性能。&lt;h4&gt;翻译&lt;/h4&gt;机器学习原子间势能(MLIPs)正在通过使研究复杂现象（如电池操作中至关重要的现象）而改变材料科学和工程。在这项工作中，我们将MACE机器学习模型与训练良好的DeePMD势能进行基准测试，用于预测LiF中的间隙锂扩散率，LiF是锂离子电池中固体电解质界面的关键组成部分。我们的结果表明，MACE-MPA-0基础模型在基于分子动力学模拟预测关键扩散特性方面，与训练良好的DeePMD模型具有相当的准确性，同时需要最少或无需训练数据。例如，MACE-MPA-0预测活化能Ea为0.22 eV，仅用300个数据点微调的模型预测Ea = 0.20 eV，两者都与DeePMD模型的参考值Ea = 0.24 eV显示出良好的一致性。在这项工作中，我们提供了一个坚实的测试案例，其中微调方法-无论是使用为DeePMD生成的数据还是由基础MACE模型本身产生的数据-都能产生与DeePMD势能相当的稳健性能，后者需要超过40,000个主动学习的训练数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials (MLIPs) are transforming materialsscience and engineering by enabling the study of complex phenomena, such asthose critical to battery operation. In this work, we benchmark the MACEmachine learning model against a well-trained DeePMD potential for predictinginterstitial lithium diffusivity in LiF, a key component in the solidelectrolyte interphase in Li ion batteries. Our results demonstrate that theMACE-MPA-0 foundational model achieves comparable accuracy to well-trainedDeePMD, in predicting key diffusion properties based on molecular dynamicssimulation, while requiring minimal or no training data. For instance, theMACE-MPA-0 predicts an activation energy Ea of 0.22 eV, the fine-tuned modelwith only 300 data points predicts Ea = 0.20 eV, both of which show goodagreement with the DeePMD model reference value of Ea = 0.24 eV. In this work,we provide a solid test case where fine-tuning approaches - whether using datagenerated for DeePMD or data produced by the foundational MACE model itself -yield similar robust performance to the DeePMD potential trained with over40,000 actively learned data, albeit requiring only a fraction of the trainingdata.</description>
      <author>example@mail.com (Nada Alghamdi, Paolo de Angelis, Pietro Asinari, Eliodoro Chiavazzo)</author>
      <guid isPermaLink="false">2510.05020v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>ActiveMark: on watermarking of visual foundation models via massive activations</title>
      <link>http://arxiv.org/abs/2510.04966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种视觉基础模型的所有权验证方法，通过微调模型的一小组表达性层和小型编码器-解码器网络，将数字水印嵌入到输入图像的内部表示中，使水印在模型的功能副本中仍可检测。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型在大数据集上训练后，可通过微调适应各种下游任务，在计算机视觉应用中表现出色。由于数据收集和训练计算成本高，模型所有者通常分发模型并附带许可证以保护知识产权。&lt;h4&gt;目的&lt;/h4&gt;开发可靠的所有权验证工具，以区分重新分发的模型副本和独立开发的模型。&lt;h4&gt;方法&lt;/h4&gt;微调视觉基础模型的一小组表达性层，同时使用一个小型编码器-解码器网络，将数字水印嵌入到保留集输入图像的内部表示中。&lt;h4&gt;主要发现&lt;/h4&gt;嵌入的水印在受保护模型的功能副本中仍然可检测，例如通过微调VFM用于特定下游任务时。&lt;h4&gt;结论&lt;/h4&gt;理论和实验表明，所提出的方法对未加水印模型的错误检测概率低，对加水印模型的错误漏检概率也低。&lt;h4&gt;翻译&lt;/h4&gt;在大规模数据集上训练的视觉基础模型可以通过微调适应各种下游任务，在各种计算机视觉应用中实现卓越的性能和效率。数据收集和训练的高计算成本促使一些视觉基础模型的所有者分发这些模型并附带许可证，以保护其知识产权。然而，受保护模型副本的不诚实用户可能会非法重新分发它，例如以获利为目的。因此，今天开发可靠的所有权验证工具非常重要，因为这些方法可用于区分受保护模型的重新分发副本和独立模型。在本文中，我们提出了一种通过微调视觉基础模型的一小组表达性层以及小型编码器-解码器网络来对所有权进行验证的方法，将数字水印嵌入到保留集输入图像的内部表示中。重要的是，嵌入的水印在受保护模型的功能副本中仍然可检测，例如通过将视觉基础模型微调用于特定下游任务时。我们在理论和实验上证明，所提出的方法对未加水印模型的错误检测概率低，对加水印模型的错误漏检概率也低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Being trained on large and vast datasets, visual foundation models (VFMs) canbe fine-tuned for diverse downstream tasks, achieving remarkable performanceand efficiency in various computer vision applications. The high computationcost of data collection and training motivates the owners of some VFMs todistribute them alongside the license to protect their intellectual propertyrights. However, a dishonest user of the protected model's copy may illegallyredistribute it, for example, to make a profit. As a consequence, thedevelopment of reliable ownership verification tools is of great importancetoday, since such methods can be used to differentiate between a redistributedcopy of the protected model and an independent model. In this paper, we proposean approach to ownership verification of visual foundation models byfine-tuning a small set of expressive layers of a VFM along with a smallencoder-decoder network to embed digital watermarks into an internalrepresentation of a hold-out set of input images. Importantly, the watermarksembedded remain detectable in the functional copies of the protected model,obtained, for example, by fine-tuning the VFM for a particular downstream task.Theoretically and experimentally, we demonstrate that the proposed methodyields a low probability of false detection of a non-watermarked model and alow probability of false misdetection of a watermarked model.</description>
      <author>example@mail.com (Anna Chistyakova, Mikhail Pautov)</author>
      <guid isPermaLink="false">2510.04966v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks</title>
      <link>http://arxiv.org/abs/2510.04898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyperVLA的新型Vision-Language-Action模型，通过基于超网络的架构显著降低推理成本，同时保持或提高性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action模型基于具有强大泛化能力的语言和视觉基础模型，在大规模机器人数据上训练，已成为学习通用机器人策略的有前景方法。但现有VLA的主要缺点是极高的推理成本。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型推理成本过高的问题，设计一种能在推理阶段减少计算量同时保持高性能的模型架构。&lt;h4&gt;方法&lt;/h4&gt;提出HyperVLA，采用基于超网络的架构，在推理阶段只激活小型特定任务策略，同时保留训练阶段的高模型容量。包含利用视觉基础模型先验知识、超网络归一化和动作生成策略等关键算法设计特征。&lt;h4&gt;主要发现&lt;/h4&gt;与整体式VLA相比，HyperVLA在零样本泛化和少样本适应方面达到相似或更高成功率，同时显著降低推理成本。与OpenVLA相比，HyperVLA测试时激活参数数量减少90倍，推理速度提高120倍。&lt;h4&gt;结论&lt;/h4&gt;HyperVLA通过创新的超网络架构成功解决了现有VLA模型推理成本过高的问题，在保持或提高性能的同时大幅降低了计算需求。&lt;h4&gt;翻译&lt;/h4&gt;基于具有强大泛化能力的语言和视觉基础模型，并在大规模机器人数据上训练，Vision-Language-Action模型最近已成为学习通用机器人策略的有前景的方法。然而，现有VLA的一个主要缺点是极高的推理成本。在本文中，我们提出HyperVLA来解决这个问题。与在训练和推理过程中激活整个模型的整体式VLA不同，HyperVLA使用一种新颖的基于超网络的架构，在推理过程中只激活小型的特定任务策略，同时在训练阶段仍保留容纳多样化多任务行为所需的高模型容量。代码已在https://github.com/MasterXiong/HyperVLA公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Built upon language and vision foundation models with strong generalizationability and trained on large-scale robotic data, Vision-Language-Action (VLA)models have recently emerged as a promising approach to learning generalistrobotic policies. However, a key drawback of existing VLAs is their extremelyhigh inference costs. In this paper, we propose HyperVLA to address thisproblem. Unlike existing monolithic VLAs that activate the whole model duringboth training and inference, HyperVLA uses a novel hypernetwork (HN)-basedarchitecture that activates only a small task-specific policy during inference,while still retaining the high model capacity needed to accommodate diversemulti-task behaviors during training. Successfully training an HN-based VLA isnontrivial so HyperVLA contains several key algorithm design features thatimprove its performance, including properly utilizing the prior knowledge fromexisting vision foundation models, HN normalization, and an action generationstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or evenhigher success rate for both zero-shot generalization and few-shot adaptation,while significantly reducing inference costs. Compared to OpenVLA, astate-of-the-art VLA model, HyperVLA reduces the number of activated parametersat test time by $90\times$, and accelerates inference speed by $120\times$.Code is publicly available at https://github.com/MasterXiong/HyperVLA</description>
      <author>example@mail.com (Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson)</author>
      <guid isPermaLink="false">2510.04898v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>A Clinical-grade Universal Foundation Model for Intraoperative Pathology</title>
      <link>http://arxiv.org/abs/2510.04861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CRISP是一个临床级基础模型，基于八个医疗中心的超过10万张冰冻切片开发，旨在为术中病理学提供临床级稳健支持，在真实临床环境中表现出高诊断准确性并能辅助医生减少工作量。&lt;h4&gt;背景&lt;/h4&gt;术中病理学对精准手术至关重要，但其临床应用受到诊断复杂性和高质量冰冻切片数据有限性的制约。尽管计算病理学取得显著进展，但缺乏大规模前瞻性验证阻碍了其在手术工作流程中的常规应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为CRISP的临床级基础模型，为病理学提供临床级稳健的术中支持，解决术中病理诊断的挑战。&lt;h4&gt;方法&lt;/h4&gt;在八个医疗中心的超过10万张冰冻切片上开发CRISP模型，并在近100个回顾性诊断任务上对超过15,000张术中切片进行全面评估，包括良恶性鉴别、关键术中决策和泛癌检测等。随后在超过2,000名患者的前瞻性队列中进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;CRISP模型在不同机构、肿瘤类型和解剖部位（包括未见过的部位和罕见癌症）上表现出强大的泛化能力；在真实世界条件下保持高诊断准确性，在92.6%的病例中直接指导手术决策；人机协作将诊断工作量减少35%，避免105项辅助检查，并以87.5%的准确性提高微转移检测率。&lt;h4&gt;结论&lt;/h4&gt;CRISP定位为AI驱动的术中病理学的临床级范式，弥合了计算进步与手术精确性之间的差距，加速了人工智能向常规临床实践的转化。&lt;h4&gt;翻译&lt;/h4&gt;术中病理学对精准手术至关重要，但其临床影响受到诊断复杂性和高质量冰冻切片数据有限性的制约。虽然计算病理学已取得显著进展，但缺乏大规模的前瞻性验证阻碍了其在手术工作流程中的常规应用。在此，我们介绍CRISP，一个在八个医疗中心超过10万张冰冻切片上开发临床级基础模型，专门设计用于为病理学提供临床级稳健的术中支持。CRISP在近100个回顾性诊断任务上对超过15,000张术中切片进行了全面评估，包括良恶性鉴别、关键术中决策和泛癌检测等。该模型在不同机构、肿瘤类型和解剖部位（包括未见过的部位和罕见癌症）上表现出强大的泛化能力。在超过2,000名患者的前瞻性队列中，CRISP在真实世界条件下保持高诊断准确性，在92.6%的病例中直接指导手术决策。人机协作进一步将诊断工作量减少35%，避免了105项辅助检查，并以87.5%的准确性提高了微转移的检测率。总之，这些发现将CRISP定位为AI驱动的术中病理学的临床级范式，弥合了计算进步与手术精确性之间的差距，加速了人工智能向常规临床实践的转化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intraoperative pathology is pivotal to precision surgery, yet its clinicalimpact is constrained by diagnostic complexity and the limited availability ofhigh-quality frozen-section data. While computational pathology has madesignificant strides, the lack of large-scale, prospective validation hasimpeded its routine adoption in surgical workflows. Here, we introduce CRISP, aclinical-grade foundation model developed on over 100,000 frozen sections fromeight medical centers, specifically designed to provide Clinical-grade RobustIntraoperative Support for Pathology (CRISP). CRISP was comprehensivelyevaluated on more than 15,000 intraoperative slides across nearly 100retrospective diagnostic tasks, including benign-malignant discrimination, keyintraoperative decision-making, and pan-cancer detection, etc. The modeldemonstrated robust generalization across diverse institutions, tumor types,and anatomical sites-including previously unseen sites and rare cancers. In aprospective cohort of over 2,000 patients, CRISP sustained high diagnosticaccuracy under real-world conditions, directly informing surgical decisions in92.6% of cases. Human-AI collaboration further reduced diagnostic workload by35%, avoided 105 ancillary tests and enhanced detection of micrometastases with87.5% accuracy. Together, these findings position CRISP as a clinical-gradeparadigm for AI-driven intraoperative pathology, bridging computationaladvances with surgical precision and accelerating the translation of artificialintelligence into routine clinical practice.</description>
      <author>example@mail.com (Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai)</author>
      <guid isPermaLink="false">2510.04861v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge</title>
      <link>http://arxiv.org/abs/2510.04772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A challenge report pre-print (31 pages), including 7 tables and 8  figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedSurg挑战是首个针对手术视频分类中联邦学习策略评估的基准测试，评估了模型在未见临床中心的泛化能力和本地微调适应能力，发现ViViT模型表现最佳，但存在泛化有限、类别不平衡敏感性高、超参数调优困难等问题，强调了架构选择、预处理和损失设计的重要性。&lt;h4&gt;背景&lt;/h4&gt;联邦学习在医疗领域特别是手术视频分析中的应用面临独特挑战，需要在保护患者隐私的同时实现多中心协作模型开发。&lt;h4&gt;目的&lt;/h4&gt;评估当前联邦学习方法在未见临床中心的泛化能力；评估通过本地微调进行中心特定适应的能力；在不共享患者数据的情况下促进协作模型开发；建立手术视频分类中联邦学习策略的评估基准。&lt;h4&gt;方法&lt;/h4&gt;使用多中心Appendix300视频数据集的初步版本；参与者开发策略来分类阑尾炎的炎症阶段；评估两个任务：未见中心的泛化和微调后的中心特定适应；提交的方法包括基础模型与线性探测、带三元组损失的度量学习、各种FL聚合方案（FedAvg、FedMedian、FedSAM）；使用F1分数和预期成本评估性能，通过bootstrap和统计测试评估排名稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;在泛化任务中，跨中心的性能有限；在适应任务中，所有团队在微调后都有所改善，但排名稳定性低；ViViT-based提交实现了最强的整体性能；挑战突显了泛化限制、对类别不平衡的敏感性以及去中心化训练中超参数调优的困难；时空建模和上下文感知预处理作为有前途的策略出现。&lt;h4&gt;结论&lt;/h4&gt;FedSurg挑战建立了评估手术视频分类中FL策略的首个基准；研究结果突显了本地个性化与全局稳健性之间的权衡；强调了架构选择、预处理和损失设计的重要性；该基准为未来在临床手术AI中开发不平衡感知、自适应和稳健的FL方法提供了参考点。&lt;h4&gt;翻译&lt;/h4&gt;目的：FedSurg挑战旨在评估联邦学习在手术视频分类领域的最新技术水平。其目标是评估当前方法在未见过的临床中心的泛化能力，并通过本地微调进行适应，同时在不共享患者数据的情况下实现协作模型开发。方法：参与者使用多中心Appendix300视频数据集的初步版本开发策略，以分类阑尾炎的炎症阶段。挑战评估了两个任务：对未见中心的泛化和微调后的中心特定适应。提交的方法包括带有线性探测的基础模型、带三元组损失的度量学习以及各种FL聚合方案（FedAvg、FedMedian、FedSAM）。使用F1分数和预期成本评估性能，并通过bootstrap和统计测试评估排名稳健性。结果：在泛化任务中，跨中心的性能有限。在适应任务中，所有团队在微调后都有所改善，尽管排名稳定性低。基于ViViT的提交实现了最强的整体性能。该挑战突显了泛化限制、对类别不平衡的敏感性以及去中心化训练中超参数调优的困难，而时空建模和上下文感知预处理作为有前途的策略出现。结论：FedSurg挑战建立了评估手术视频分类中FL策略的首个基准。研究结果突显了本地个性化与全局稳健性之间的权衡，并强调了架构选择、预处理和损失设计的重要性。这一基准为未来在临床手术AI中开发不平衡感知、自适应和稳健的FL方法提供了参考点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: The FedSurg challenge was designed to benchmark the state of the artin federated learning for surgical video classification. Its goal was to assesshow well current methods generalize to unseen clinical centers and adaptthrough local fine-tuning while enabling collaborative model developmentwithout sharing patient data. Methods: Participants developed strategies toclassify inflammation stages in appendicitis using a preliminary version of themulti-center Appendix300 video dataset. The challenge evaluated two tasks:generalization to an unseen center and center-specific adaptation afterfine-tuning. Submitted approaches included foundation models with linearprobing, metric learning with triplet loss, and various FL aggregation schemes(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score andExpected Cost, with ranking robustness evaluated via bootstrapping andstatistical testing. Results: In the generalization task, performance acrosscenters was limited. In the adaptation task, all teams improved afterfine-tuning, though ranking stability was low. The ViViT-based submissionachieved the strongest overall performance. The challenge highlightedlimitations in generalization, sensitivity to class imbalance, and difficultiesin hyperparameter tuning in decentralized training, while spatiotemporalmodeling and context-aware preprocessing emerged as promising strategies.Conclusion: The FedSurg Challenge establishes the first benchmark forevaluating FL strategies in surgical video classification. Findings highlightthe trade-off between local personalization and global robustness, andunderscore the importance of architecture choice, preprocessing, and lossdesign. This benchmarking offers a reference point for future development ofimbalance-aware, adaptive, and robust FL methods in clinical surgical AI.</description>
      <author>example@mail.com (Max Kirchner, Hanna Hoffmann, Alexander C. Jenke, Oliver L. Saldanha, Kevin Pfeiffer, Weam Kanjo, Julia Alekseenko, Claas de Boer, Santhi Raj Kolamuri, Lorenzo Mazza, Nicolas Padoy, Sophia Bano, Annika Reinke, Lena Maier-Hein, Danail Stoyanov, Jakob N. Kather, Fiona R. Kolbinger, Sebastian Bodenstedt, Stefanie Speidel)</author>
      <guid isPermaLink="false">2510.04772v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</title>
      <link>http://arxiv.org/abs/2510.04706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCVW 2025, Code: https://github.com/foivospar/Arc2Face&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于扩散的框架，能够在不损害身份一致性的情况下实现细粒度的面部表情控制，适用于AI驱动的故事讲述。&lt;h4&gt;背景&lt;/h4&gt;面向AI驱动故事讲述的人本生成模型需要两个核心能力：身份一致性和对人类表演的精确控制。尽管基于扩散的方法在保持面部身份方面取得了进展，但在不损害身份的情况下实现细粒度表情控制仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够忠实地在任意特定面部表情下重新构想任何主体的扩散框架。&lt;h4&gt;方法&lt;/h4&gt;基于ID一致的面部基础模型，采用组合设计，包含由FLAME blendshape参数引导的表情交叉注意力模块用于显式控制。在多样化的图像和视频数据上进行训练，能够泛化到基本情感之外的微妙微表情和表情转换。此外，还提供了可插拔的参考适配器用于真实图像的表情编辑。&lt;h4&gt;主要发现&lt;/h4&gt;大量的定量和定性评估表明，该模型在定制化和身份一致的表情生成方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架成功结合了身份一致性和细粒度表情控制，为AI驱动的故事讲述提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;面向AI驱动故事讲述的人本生成模型必须结合两个核心能力：身份一致性和对人类表演的精确控制。虽然最近的基于扩散的方法在保持面部身份方面取得了显著进展，但在不损害身份的情况下实现细粒度表情控制仍然具有挑战性。在这项工作中，我们提出了一个基于扩散的框架，能够在任何特定面部表情下忠实地重新构想任何主体。基于ID一致的面部基础模型，我们采用组合设计，包含由FLAME blendshape参数引导的表情交叉注意力模块，用于显式控制。在富含表情变化的多样化图像和视频数据上进行训练后，我们的适配器能够泛化到基本情感之外的微妙微表情和表情转换，这是先前工作所忽视的。此外，一个可插拔的参考适配器能够在合成过程中通过从参考帧转移外观来实现真实图像的表情编辑。大量的定量和定性评估表明，我们的模型在定制化和身份一致的表情生成方面优于现有方法。代码和模型可在https://github.com/foivospar/Arc2Face找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-centric generative models designed for AI-driven storytelling mustbring together two core capabilities: identity consistency and precise controlover human performance. While recent diffusion-based approaches have madesignificant progress in maintaining facial identity, achieving fine-grainedexpression control without compromising identity remains challenging. In thiswork, we present a diffusion-based framework that faithfully reimagines anysubject under any particular facial expression. Building on an ID-consistentface foundation model, we adopt a compositional design featuring an expressioncross-attention module guided by FLAME blendshape parameters for explicitcontrol. Trained on a diverse mixture of image and video data rich inexpressive variation, our adapter generalizes beyond basic emotions to subtlemicro-expressions and expressive transitions, overlooked by prior works. Inaddition, a pluggable Reference Adapter enables expression editing in realimages by transferring the appearance from a reference frame during synthesis.Extensive quantitative and qualitative evaluations show that our modeloutperforms existing methods in tailored and identity-consistent expressiongeneration. Code and models can be found athttps://github.com/foivospar/Arc2Face.</description>
      <author>example@mail.com (Foivos Paraperas Papantoniou, Stefanos Zafeiriou)</author>
      <guid isPermaLink="false">2510.04706v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI</title>
      <link>http://arxiv.org/abs/2510.04705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种标签高效的肝脏分割方法，通过结合基础模型适配和共同训练技术，解决了多相MRI中标记数据稀缺和分布不均的问题，实现了跨模态和跨供应商系统的稳健分割性能。&lt;h4&gt;背景&lt;/h4&gt;在多相MRI中进行准确的肝脏分割对于肝纤维化评估至关重要，但标记数据通常稀缺，并且在成像模式和供应商系统之间分布不均。实际条件下，肝胆期注释有限，非对比序列未标记，且空间错位和缺失相位常见。&lt;h4&gt;目的&lt;/h4&gt;提出一种标签高效的分割方法，促进在实际条件下的跨模态泛化能力，特别是在标记数据有限的情况下。&lt;h4&gt;方法&lt;/h4&gt;集成一个基础规模的3D分割主干并通过微调进行适配；使用交叉伪监督进行共同训练以利用未标记的体积数据；采用标准化预处理流程；无需空间配准即可实现跨MRI相位和供应商的泛化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标记和未标记域中均表现出稳健的分割性能；所提出的标签基线对于多相、多供应商MRI中的肝脏分割有效。&lt;h4&gt;结论&lt;/h4&gt;结合基础模型适配和共同训练对于实际临床成像任务具有潜力，能够有效解决多相MRI中肝脏分割面临的标记数据稀缺问题。&lt;h4&gt;翻译&lt;/h4&gt;在多相MRI中进行准确的肝脏分割对于肝纤维化评估至关重要，但标记数据通常稀缺，并且在成像模式和供应商系统之间分布不均。我们提出了一种标签高效的分割方法，促进在实际条件下的跨模态泛化，其中肝胆期注释有限，非对比序列未标记，且空间错位和缺失相位常见。我们的方法集成了一个通过微调适配的基础规模3D分割主干，使用交叉伪监督进行共同训练以利用未标记体积，以及标准化预处理流程。无需空间配准，模型能够跨MRI相位和供应商泛化，在标记和未标记域中均表现出稳健的分割性能。我们的结果展示了所提出的标签基线对于多相、多供应商MRI中肝脏分割的有效性，并突显了结合基础模型适配和共同训练对于实际临床成像任务的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多相MRI中肝脏分割的标签效率问题和跨模态泛化问题。具体来说，就是如何在标记数据稀缺（只有少量GED4肝胆期有标签）且分布不均的情况下，实现不同MRI相位和供应商设备间的肝脏准确分割。这个问题很重要，因为肝脏是人体关键器官，其准确分割对肝纤维化等疾病的诊断至关重要，而临床环境中获取大量标记数据成本高、耗时长，不同医院和设备的差异也增加了分割难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先明确定义了LiSeg任务面临的挑战，然后选择了STU-Net作为基础模型而非传统的nnU-Net，因为STU-Net具有更好的可扩展性和迁移能力。作者在TotalSegmentator数据集上预训练STU-Net，并在ATLAS肝脏分割数据集上进行微调以适应目标域。在半监督学习方面，作者采用了交叉伪监督(CPS)方法，结合了BCP和MiDSS等技术，让两个独立的网络互相提供伪标签。预处理方面借鉴了nnU-Net的标准化流程。整个方法综合了多种现有技术的优点，并针对医学图像特点进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模预训练模型作为基础，通过半监督学习同时利用有限的标记数据和大量的未标记数据，实现跨模态泛化。整体流程包括：1)使用LiQA多中心多供应商数据集；2)选择STU-Net作为基础模型并在ATLAS数据集上微调；3)采用双网络训练策略，两个网络互相提供伪标签；4)使用nnU-Net的预处理流程和增强技术；5)通过Dice相似系数和Hausdorff距离评估性能。这种方法不需要空间配准，就能处理不同MRI相位和供应商间的差异。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)标签效率与跨模态泛化的结合，特别适用于标记数据稀缺的多相MRI场景；2)使用具有更好迁移能力的STU-Net作为基础模型；3)采用交叉伪监督等多种半监督技术有效利用未标记数据；4)实现无需空间配准的跨模态泛化。相比之前工作，这种方法结合了多种技术的优点，不需要复杂的域适应技术，直接利用未标记数据进行端到端的跨模态泛化，在真实临床环境中表现更加鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合基础模型微调和半监督学习的标签高效框架，能够在标记数据稀缺和跨模态差异的情况下实现多相MRI中肝脏的准确分割，展示了在真实临床环境中应用的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate liver segmentation in multi-phase MRI is vital for liver fibrosisassessment, yet labeled data is often scarce and unevenly distributed acrossimaging modalities and vendor systems. We propose a label-efficientsegmentation approach that promotes cross-modality generalization underreal-world conditions, where GED4 hepatobiliary-phase annotations are limited,non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatialmisalignment and missing phases are common. Our method integrates afoundation-scale 3D segmentation backbone adapted via fine-tuning, co-trainingwith cross pseudo supervision to leverage unlabeled volumes, and a standardizedpreprocessing pipeline. Without requiring spatial registration, the modellearns to generalize across MRI phases and vendors, demonstrating robustsegmentation performance in both labeled and unlabeled domains. Our resultsexhibit the effectiveness of our proposed label-efficient baseline for liversegmentation in multi-phase, multi-vendor MRI and highlight the potential ofcombining foundation model adaptation with co-training for real-world clinicalimaging tasks.</description>
      <author>example@mail.com (Quang-Khai Bui-Tran, Minh-Toan Dinh, Thanh-Huy Nguyen, Ba-Thinh Lam, Mai-Anh Vu, Ulas Bagci)</author>
      <guid isPermaLink="false">2510.04705v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>MedPAO: A Protocol-Driven Agent for Structuring Medical Reports</title>
      <link>http://arxiv.org/abs/2510.04623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper published at "Agentic AI for Medicine" Workshop, MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MedPAO，一个用于结构化临床数据的新型智能框架，通过遵循临床协议确保准确性和可验证推理，解决了大型语言模型在临床数据结构化中的幻觉问题和无法遵循领域特定规则的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）在结构化临床数据方面的应用受到其倾向于产生幻觉事实和无法遵循领域特定规则的严重阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在临床数据结构化中的幻觉问题和无法遵循领域特定规则的问题，确保准确性和可验证推理。&lt;h4&gt;方法&lt;/h4&gt;引入MedPAO，一个新颖的智能框架，它将报告结构化任务分解为由计划-行动-观察（PAO）循环和专门工具管理的透明过程，并基于已建立的临床协议（如CXR分析的ABCDEF协议）进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;MedPAO在概念分类的关键子任务上实现了0.96的F1分数。专家放射科医生和临床医生对最终结构化输出的平均评分为4.52分（满分5分），表明其可靠性超过了仅依赖基于LLM的基础模型的基线方法。&lt;h4&gt;结论&lt;/h4&gt;MedPAO提供了一种可验证的替代方案，解决了传统单一模型在临床数据结构化中的问题，其可靠性和准确性超过了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）在结构化临床数据方面的部署受到其倾向于产生幻觉事实和无法遵循领域特定规则的严重阻碍。为解决这一问题，我们引入了MedPAO，一种新颖的智能框架，它通过基于已建立的临床协议（如CXR分析的ABCDEF协议）来确保操作准确性和可验证推理。MedPAO将报告结构化任务分解为由计划-行动-观察（PAO）循环和专门工具管理的透明过程。这种协议驱动的方法为不透明的单一模型提供了可验证的替代方案。我们通过严格的评估证明了我们方法的有效性：MedPAO在概念分类的关键子任务上实现了0.96的F1分数。值得注意的是，专家放射科医生和临床医生对最终结构化输出的平均评分为4.52分（满分5分），表明其可靠性超过了仅依赖基于LLM的基础模型的基线方法。代码可在以下网址获取：https://github.com/MiRL-IITM/medpao-agent&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-06004-4_4&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of Large Language Models (LLMs) for structuring clinical datais critically hindered by their tendency to hallucinate facts and theirinability to follow domain-specific rules. To address this, we introduceMedPAO, a novel agentic framework that ensures accuracy and verifiablereasoning by grounding its operation in established clinical protocols such asthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuringtask into a transparent process managed by a Plan-Act-Observe (PAO) loop andspecialized tools. This protocol-driven method provides a verifiablealternative to opaque, monolithic models. The efficacy of our approach isdemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96on the critical sub-task of concept categorization. Notably, expertradiologists and clinicians rated the final structured outputs with an averagescore of 4.52 out of 5, indicating a level of reliability that surpassesbaseline approaches relying solely on LLM-based foundation models. The code isavailable at: https://github.com/MiRL-IITM/medpao-agent</description>
      <author>example@mail.com (Shrish Shrinath Vaidya, Gowthamaan Palani, Sidharth Ramesh, Velmurugan Balasubramanian, Minmini Selvam, Gokulraja Srinivasaraja, Ganapathy Krishnamurthi)</author>
      <guid isPermaLink="false">2510.04623v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior</title>
      <link>http://arxiv.org/abs/2510.04587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为AI会话记录器的系统，用于捕获病理学家观察全切片图像的行为模式，并基于这些数据构建了Pathologist-o3代理系统，在病理诊断中表现优异。&lt;h4&gt;背景&lt;/h4&gt;诊断全切片图像是一个交互式多阶段过程，涉及放大倍数变化和视野间移动。尽管病理学基础模型强大，但缺乏能够决定下一个检查视野、调整放大倍数并提供可解释诊断的实用代理系统。主要障碍在于缺乏可扩展的、与临床一致的专家查看行为监督数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种将病理学家的日常查看日志转化为可扩展的、专家验证的监督数据的方法，构建实用的病理学代理系统，建立人类对齐的、可升级的临床AI路径。&lt;h4&gt;方法&lt;/h4&gt;1) 引入AI会话记录器，记录常规导航并转换为标准化行为命令；2) 通过轻量级人工审核将AI起草理由转化为病理学思维链数据集；3) 基于行为数据构建两阶段代理系统Pathologist-o3，先提出感兴趣区域，再进行行为引导推理。&lt;h4&gt;主要发现&lt;/h4&gt;1) AI会话记录器能有效捕获病理学家查看行为；2) 轻量级人工审核可将标记时间减少至传统方法的六分之一；3) Pathologist-o3在胃肠道淋巴结转移检测中达到84.5%精确率、100%召回率和75.4%准确率；4) 系统超越了OpenAI o3模型并能跨骨干架构泛化。&lt;h4&gt;结论&lt;/h4&gt;将日常查看日志转化为可扩展的、专家验证的监督，该框架使代理病理学变得实用，为人类对齐的、可升级的临床AI铺平道路，是病理学领域首个基于行为的代理系统之一。&lt;h4&gt;翻译&lt;/h4&gt;全切片图像诊断是一个涉及放大倍数变化和视野间移动的交互式多阶段过程。尽管最近的病理学基础模型表现强大，但仍然缺乏能够决定下一个检查视野、调整放大倍数并提供可解释诊断的实用代理系统。主要障碍在于数据：缺乏可扩展的、与临床一致的专家查看行为监督，这些行为是隐含的、基于经验的，未写在教科书或网络资源中，因此也缺失于大型语言模型的训练中。我们引入了AI会话记录器，它能够与标准WSI查看器配合使用，非侵入性地记录常规导航，并将查看器日志转换为标准化行为命令和边界框。轻量级人工循环审核将AI起草的理由转化为病理学思维链数据集，产生时间约为传统方法的六分之一。利用这些行为数据，我们构建了Pathologist-o3，在胃肠道淋巴结转移检测中表现优异，超越了最先进的OpenAI o3模型，并能跨骨干架构泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diagnosing a whole-slide image is an interactive, multi-stage processinvolving changes in magnification and movement between fields. Although recentpathology foundation models are strong, practical agentic systems that decidewhat field to examine next, adjust magnification, and deliver explainablediagnoses are still lacking. The blocker is data: scalable, clinically alignedsupervision of expert viewing behavior that is tacit and experience-based, notwritten in textbooks or online, and therefore absent from large language modeltraining. We introduce the AI Session Recorder, which works with standard WSIviewers to unobtrusively record routine navigation and convert the viewer logsinto standardized behavioral commands (inspect or peek at discretemagnifications) and bounding boxes. A lightweight human-in-the-loop reviewturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired"where to look" and "why it matters" supervision produced at roughly six timeslower labeling time. Using this behavioral data, we build Pathologist-o3, atwo-stage agent that first proposes regions of interest and then performsbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding thestate-of-the-art OpenAI o3 model and generalizing across backbones. To ourknowledge, this constitutes one of the first behavior-grounded agentic systemsin pathology. Turning everyday viewer logs into scalable, expert-validatedsupervision, our framework makes agentic pathology practical and establishes apath to human-aligned, upgradeable clinical AI.</description>
      <author>example@mail.com (Sheng Wang, Ruiming Wu, Charles Herndon, Yihang Liu, Shunsuke Koga, Jeanne Shen, Zhi Huang)</author>
      <guid isPermaLink="false">2510.04587v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators</title>
      <link>http://arxiv.org/abs/2510.04354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SureSim框架通过结合大规模仿真和小规模真实世界测试，为机器人操作策略的性能提供可靠统计推断，节省了20-25%的硬件评估工作量。&lt;h4&gt;背景&lt;/h4&gt;机器人模仿学习、基础模型和大规模数据集的快速发展使得机器人操作策略能够泛化到各种任务和环境，但这些策略的严格评估仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出SureSim框架，通过结合大规模仿真和相对小规模的真实世界测试，为策略在真实世界中的表现提供可靠的推断。&lt;h4&gt;方法&lt;/h4&gt;将真实和仿真评估的组合问题形式化为预测驱动的推断问题，使用少量配对的真实和仿真评估修正仿真偏差，并利用非渐近均值估计算法为策略性能提供置信区间。&lt;h4&gt;主要发现&lt;/h4&gt;在基于物理的仿真中评估扩散策略和多任务微调的π₀策略时，发现该方法可以节省超过20-25%的硬件评估工作量，同时获得类似的策略性能边界。&lt;h4&gt;结论&lt;/h4&gt;SureSim框架能够有效结合仿真和真实世界测试，减少硬件评估需求，并提供策略性能的可靠统计推断。&lt;h4&gt;翻译&lt;/h4&gt;模仿学习、基础模型和大规模数据集的快速发展使得机器人操作策略能够泛化到广泛多样的任务和环境。然而，对这些策略的严格评估仍然是一个挑战。通常在实践中，机器人策略往往只在少量硬件试验中进行评估，没有任何统计保证。我们提出了SureSim框架，通过将大规模仿真与相对小规模的真实世界测试相结合，为策略在真实世界中的表现提供可靠的推断。我们的核心思想是将结合真实和仿真评估的问题形式化为一个预测驱动的推断问题，其中使用少量配对的真实和仿真评估来修正大规模仿真中的偏差。然后，我们利用非渐近均值估计算法为策略性能均值提供置信区间。使用基于物理的仿真，我们在物体和初始条件的联合分布上评估了扩散策略和多任务微调的π₀策略，发现我们的方法可以节省超过20-25%的硬件评估工作量，同时获得类似的策略性能边界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid progress in imitation learning, foundation models, and large-scaledatasets has led to robot manipulation policies that generalize to a wide-rangeof tasks and environments. However, rigorous evaluation of these policiesremains a challenge. Typically in practice, robot policies are often evaluatedon a small number of hardware trials without any statistical assurances. Wepresent SureSim, a framework to augment large-scale simulation with relativelysmall-scale real-world testing to provide reliable inferences on the real-worldperformance of a policy. Our key idea is to formalize the problem of combiningreal and simulation evaluations as a prediction-powered inference problem, inwhich a small number of paired real and simulation evaluations are used torectify bias in large-scale simulation. We then leverage non-asymptotic meanestimation algorithms to provide confidence intervals on mean policyperformance. Using physics-based simulation, we evaluate both diffusion policyand multi-task fine-tuned \(\pi_0\) on a joint distribution of objects andinitial conditions, and find that our approach saves over \(20-25\%\) ofhardware evaluation effort to achieve similar bounds on policy performance.</description>
      <author>example@mail.com (Apurva Badithela, David Snyder, Lihan Zha, Joseph Mikhail, Matthew O'Kelly, Anushri Dixit, Anirudha Majumdar)</author>
      <guid isPermaLink="false">2510.04354v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</title>
      <link>http://arxiv.org/abs/2510.04331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Nghiem T. Diep, Hien Dang, and Tuan Truong contributed equally to  this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DoRAN，一种基于DoRA改进的参数高效微调方法，通过噪声注入和动态网络生成两个关键技术，实现了训练稳定性和样本效率的双重提升。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)已成为适应大规模模型的标准范式。其中，权重分解低秩自适应(DoRA)通过将预训练权重分解为幅度和方向分量，提高了原始LoRA方法的学习能力和训练稳定性。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的DoRA变体，进一步稳定训练过程并提升DoRA的样本效率。&lt;h4&gt;方法&lt;/h4&gt;包含两个关键阶段：(i)在DoRA权重分解的分母中注入噪声，作为自适应正则化器减轻不稳定性；(ii)用生成动态低秩矩阵的辅助网络替换静态低秩矩阵，实现层间参数耦合，提高样本效率。&lt;h4&gt;主要发现&lt;/h4&gt;在视觉和语言基准测试上的全面实验表明，DoRAN持续优于LoRA、DoRA和其他PEFT基线方法。&lt;h4&gt;结论&lt;/h4&gt;结合基于噪声的正则化与基于网络的参数生成，为基础模型的稳健高效微调提供了有前景的新方向。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)方法已成为适应大规模模型的标准范式。在这些技术中，权重分解低秩自适应(DoRA)通过将预训练权重显式分解为幅度和方向分量，已被证明可以提高原始低秩自适应(LoRA)方法的学习能力和训练稳定性。在这项工作中，我们提出了DoRAN，一种新的DoRA变体，旨在进一步稳定训练并提高DoRA的样本效率。我们的方法包括两个关键阶段：(i)在DoRA权重分解的分母中注入噪声，作为自适应正则化器来减轻不稳定性；(ii)用生成它们的辅助网络动态替换静态低秩矩阵，实现层间参数耦合，在理论和实践中都获得更好的样本效率。在视觉和语言基准测试上的全面实验表明，DoRAN持续优于LoRA、DoRA和其他PEFT基线。这些结果强调了通过基于噪声的正则化与基于网络的参数生成相结合的有效性，为基础模型的稳健高效微调提供了有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) methods have become the standardparadigm for adapting large-scale models. Among these techniques,Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both thelearning capacity and training stability of the vanilla Low-Rank Adaptation(LoRA) method by explicitly decomposing pre-trained weights into magnitude anddirectional components. In this work, we propose DoRAN, a new variant of DoRAdesigned to further stabilize training and boost the sample efficiency of DoRA.Our approach includes two key stages: (i) injecting noise into the denominatorof DoRA's weight decomposition, which serves as an adaptive regularizer tomitigate instabilities; and (ii) replacing static low-rank matrices withauxiliary networks that generate them dynamically, enabling parameter couplingacross layers and yielding better sample efficiency in both theory andpractice. Comprehensive experiments on vision and language benchmarks show thatDoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. Theseresults underscore the effectiveness of combining stabilization throughnoise-based regularization with network-based parameter generation, offering apromising direction for robust and efficient fine-tuning of foundation models.</description>
      <author>example@mail.com (Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho)</author>
      <guid isPermaLink="false">2510.04331v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws</title>
      <link>http://arxiv.org/abs/2510.04102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型在时间序列预测中的应用，特别关注它们在外推和长期预测方面的局限性，以及与物理定律的对比。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言建模中取得了显著成功，激发了其在时间序列预测领域的应用研究。这些模型对科学和工程具有变革性潜力，并在短期预测中表现出色。&lt;h4&gt;目的&lt;/h4&gt;确定并形式化一个基本属性，该属性表征统计学习模型在训练域外进行更准确预测的能力，解释深度学习模型在外推设置中性能下降的原因，并为设计能够掌握外推能力的下一代预测模型提供方向。&lt;h4&gt;方法&lt;/h4&gt;通过理论分析和实证研究，探讨神经网络结构与物理定律之间的根本差异，以及这种差异如何影响模型的外推能力。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在短期预测中表现良好，但在外推或长期预测方面表现不佳，甚至无法超越简单基线方法。研究确定了一个基本属性，解释了深度学习模型在外推设置中性能下降的原因。&lt;h4&gt;结论&lt;/h4&gt;研究结果阐明了外推差距的根本原因，并为设计能够掌握外推能力的下一代预测模型提供了方向。理解神经网络结构与物理定律之间的差异对于改进预测模型的外推能力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;受语言建模中基础模型的显著成功启发，人们对开发用于时间序列预测的基础模型越来越感兴趣，因为这些模型对科学和工程具有变革性潜力。这导致了基础模型在短期预测设置中取得显著成功。然而，对于外推或长期预测，基础模型仍然难以实现，甚至无法超越简单的基线方法。这与物理定律形成鲜明对比，因为物理定律具有很强的外推特性。这引发了关于神经网络结构与物理定律之间根本差异的问题。在这项工作中，作者确定并形式化了一个基本属性，该属性表征了统计学习模型在训练域外进行更准确预测的能力，从而解释了深度学习模型在外推设置中性能下降的原因。除了理论分析外，作者还展示了实证结果，说明了这一属性对当前深度学习架构的影响。研究结果不仅阐明了外推差距的根本原因，还为设计能够掌握外推能力的下一代预测模型提供了方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by the remarkable success of Foundation Models (FMs) in languagemodeling, there has been growing interest in developing FMs for time seriesprediction, given the transformative power such models hold for science andengineering. This culminated in significant success of FMs in short-rangeforecasting settings. However, extrapolation or long-range forecasting remainselusive for FMs, which struggle to outperform even simple baselines. Thiscontrasts with physical laws which have strong extrapolation properties, andraises the question of the fundamental difference between the structure ofneural networks and physical laws. In this work, we identify and formalize afundamental property characterizing the ability of statistical learning modelsto predict more accurately outside of their training domain, hence explainingperformance deterioration for deep learning models in extrapolation settings.In addition to a theoretical analysis, we present empirical results showcasingthe implications of this property on current deep learning architectures. Ourresults not only clarify the root causes of the extrapolation gap but alsosuggest directions for designing next-generation forecasting models capable ofmastering extrapolation.</description>
      <author>example@mail.com (Ramzi Dakhmouche, Hossein Gorji)</author>
      <guid isPermaLink="false">2510.04102v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Zephyrus: An Agentic Framework for Weather Science</title>
      <link>http://arxiv.org/abs/2510.04017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究构建了一个名为Zephyrus的气象科学智能体框架，结合了基础模型的数值数据处理能力和大型语言模型的文本理解能力，通过ZephyrusWorld环境与气象数据交互，并创建了ZephyrusBench基准测试评估性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在气象科学领域通过大量结构化数值数据预训练，优于传统天气预报系统，但缺乏基于语言的推理能力；而大型语言模型擅长理解和生成文本，却无法推理高维气象数据集。&lt;h4&gt;目的&lt;/h4&gt;弥合基础模型和大语言模型之间的差距，为气象科学构建一个新的智能体框架，结合两者的优势。&lt;h4&gt;方法&lt;/h4&gt;构建ZephyrusWorld基于Python代码的交互环境，包含WeatherBench 2数据集接口、地理掩码查询、天气预报和气候模拟等功能；设计Zephyrus多回合LLM气象智能体，迭代分析数据并通过对话反馈循环改进；创建ZephyrusBench基准测试和数据生成管道，构建多样化气象任务问答对。&lt;h4&gt;主要发现&lt;/h4&gt;Zephyrus智能体在正确性方面比纯文本基线高出多达35个百分点，但在更困难的任务上表现与纯文本基线相似，突显了基准测试的挑战性。&lt;h4&gt;结论&lt;/h4&gt;该框架为气象科学提供了有前途的方向，但未来工作需解决更复杂任务的挑战，以进一步提升智能体性能。&lt;h4&gt;翻译&lt;/h4&gt;气象科学的基础模型是在大量结构化数值数据上预训练的，并且优于传统的天气预报系统。然而，这些模型缺乏基于语言的推理能力，限制了它们在交互式科学工作流程中的实用性。大型语言模型(LLMs)擅长理解和生成文本，但无法推理高维气象数据集。我们通过为气象科学构建一个新的智能体框架来弥合这一差距。我们的框架包括一个基于Python代码的环境，供智能体(ZephyrusWorld)与气象数据交互，具有WeatherBench 2数据集接口、从自然语言查询地理掩码、天气预报和气候模拟等功能。我们设计了Zephyrus，这是一个多回合基于LLM的气象智能体，它迭代分析气象数据集，观察结果，并通过对话反馈循环改进其方法。我们为该智能体配备了新的基准测试ZephyrusBench，它具有可扩展的数据生成管道，能够构建跨气象相关任务的多样化问答对，从基本查询到高级预报、极端事件检测和反事实推理。在该基准测试上的实验表明，Zephyrus智能体在正确性方面比纯文本基线高出多达35个百分点。然而，在更困难的任务上，Zephyrus的表现与纯文本基线相似，突显了我们基准测试的挑战性，并为未来工作指出了有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for weather science are pre-trained on vast amounts ofstructured numerical data and outperform traditional weather forecastingsystems. However, these models lack language-based reasoning capabilities,limiting their utility in interactive scientific workflows. Large languagemodels (LLMs) excel at understanding and generating text but cannot reasonabout high-dimensional meteorological datasets. We bridge this gap by buildinga novel agentic framework for weather science. Our framework includes a Pythoncode-based environment for agents (ZephyrusWorld) to interact with weatherdata, featuring tools like an interface to WeatherBench 2 dataset, geoqueryingfor geographical masks from natural language, weather forecasting, and climatesimulation capabilities. We design Zephyrus, a multi-turn LLM-based weatheragent that iteratively analyzes weather datasets, observes results, and refinesits approach through conversational feedback loops. We accompany the agent witha new benchmark, ZephyrusBench, with a scalable data generation pipeline thatconstructs diverse question-answer pairs across weather-related tasks, frombasic lookups to advanced forecasting, extreme event detection, andcounterfactual reasoning. Experiments on this benchmark demonstrate the strongperformance of Zephyrus agents over text-only baselines, outperforming them byup to 35 percentage points in correctness. However, on harder tasks, Zephyrusperforms similarly to text-only baselines, highlighting the challenging natureof our benchmark and suggesting promising directions for future work.</description>
      <author>example@mail.com (Sumanth Varambally, Marshall Fisher, Jas Thakker, Yiwei Chen, Zhirui Xia, Yasaman Jafari, Ruijia Niu, Manas Jain, Veeramakali Vignesh Manivannan, Zachary Novack, Luyu Han, Srikar Eranky, Salva Rühling Cachay, Taylor Berg-Kirkpatrick, Duncan Watson-Parris, Yi-An Ma, Rose Yu)</author>
      <guid isPermaLink="false">2510.04017v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models</title>
      <link>http://arxiv.org/abs/2510.04009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了C^2-Eval，一个用于统一评估基础模型创造力的整体基准，区分收敛创造力和发散创造力，并使用实用性、原创性和惊喜度作为评估标准。&lt;h4&gt;背景&lt;/h4&gt;基础模型的能力已扩展到传统任务之外，创造力作为人类智能的标志和创新驱动力，现在被认为是生成式基础模型时代机器智能的关键维度，补充了传统准确性衡量标准。&lt;h4&gt;目的&lt;/h4&gt;解决现有创造力评估框架分散、缺乏理论基础的问题，引入一个统一的创造力评估基准。&lt;h4&gt;方法&lt;/h4&gt;C^2-Eval区分收敛创造力（有约束解决方案的任务）和发散创造力（开放式任务），使用源自社会科学理论的细粒度标准评估这两个维度，重点关注实用性、原创性和惊喜度。&lt;h4&gt;主要发现&lt;/h4&gt;通过对领先的专有和开源模型进行广泛实验，分析了它们在创造力能力方面的权衡，揭示了当前基础模型在追求创造性机器思维方面的优势和挑战。&lt;h4&gt;结论&lt;/h4&gt;C^2-Eval是检查创意AI不断发展的格局的有效视角。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的兴起极大地扩展了其能力范围，远超传统任务。创造力长期以来被视为人类智能的标志和创新驱动力，现在在生成式基础模型时代越来越被认可为机器智能的关键维度，补充了传统准确性衡量标准。然而，现有的创造力评估框架仍然分散，依赖于临时性指标，这些指标没有牢固地建立在既定理论基础上。为了解决这一差距，我们引入了C^2-Eval，这是一个用于统一评估基础模型创造力的整体基准。C^2-Eval区分了两种互补的创造力形式：收敛创造力（任务允许有约束的解决方案，如代码生成）和发散创造力（任务是开放式的，如讲故事）。它使用源自社会科学理论的细粒度标准评估这两个维度，重点关注实用性、原创性和惊喜度。通过对领先的专有和开源模型进行广泛实验，我们分析了它们在创造力能力方面的权衡。我们的结果强调了当前基础模型在追求创造性机器思维方面的优势和挑战，表明C^2-Eval是检查创意AI不断发展的格局的有效视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The meteoric rise of foundation models (FMs) has expanded their capabilitiesfar beyond conventional tasks. Creativity, long regarded as a hallmark of humanintelligence and a driver of innovation, is now increasingly recognized as acritical dimension of machine intelligence in the era of generative FMs,complementing traditional measures of accuracy. However, existing evaluationframeworks for creativity remain fragmented, relying on ad hoc metrics notfirmly grounded in established theories. To address this gap, we introduceC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.C^2-Eval distinguishes between two complementary forms of creativity:convergent creativity, where tasks admit constrained solutions (e.g., codegeneration), and divergent creativity, where tasks are open-ended (e.g.,storytelling). It evaluates both dimensions using fine-grained criteria derivedfrom social-science theory, focusing on Usefulness, Originality, and Surprise(U-O-S). Through extensive experiments on leading proprietary and open-sourcemodels, we analyze trade-offs in their creative capabilities. Our resultshighlight both the strengths and challenges of current FMs in pursuing acreative machine mind, showing that C^2-Eval is an effective lens for examiningthe evolving landscape of creative AI.</description>
      <author>example@mail.com (Zicong He, Boxuan Zhang, Weihao Liu, Ruixiang Tang, Lu Cheng)</author>
      <guid isPermaLink="false">2510.04009v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series</title>
      <link>http://arxiv.org/abs/2510.03911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Oral Presentation. AI4TS Workshop, IJCAI'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了THEMIS，一个利用基础模型预训练知识的时间序列异常检测新框架，通过提取Chronos模型的嵌入并应用异常检测技术，在多个数据集上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;时间序列异常检测在多个领域至关重要但面临挑战，包括数据具有季节性、趋势、噪声和演变模式，异常类型多样且罕见导致数据不平衡，以及高维、实时检测阈值设置和结果可解释性等问题。&lt;h4&gt;目的&lt;/h4&gt;开发强大、灵活且可解释的方法来应对时间序列异常检测的多方面挑战，提出THEMIS框架。&lt;h4&gt;方法&lt;/h4&gt;THEMIS利用基础模型的预训练知识，从Chronos时间序列基础模型的编码器中提取嵌入，并在自相似矩阵上应用局部异常因子和谱分解等异常检测技术来识别异常。&lt;h4&gt;主要发现&lt;/h4&gt;THEMIS在MSL数据集上取得最先进结果，在SMAP和SWAT数据上表现具有竞争力，超过了专门为异常检测训练的模型，具有超参数鲁棒性和默认可解释性。&lt;h4&gt;结论&lt;/h4&gt;提倡使用基础模型的预训练表示来进行高效且适应性强的异常检测。&lt;h4&gt;翻译&lt;/h4&gt;时间序列异常检测在多个领域中构成非常关键的区域，但带来了重大挑战。由于时间序列数据具有季节性、趋势、噪声和演变模式(概念漂移)，很难确定什么是正常行为的通用概念。异常本身可能是多样的，从单一异常值到上下文异常或集体异常，而且通常非常罕见，因此数据集严重不平衡。现代时间序列的高维问题、实时检测标准、设置适当的检测阈值以及获得可解释的结果等问题增加了额外的复杂性。为了应对这些多方面的挑战，需要非常强大、灵活且可解释的方法。本文提出了THEMIS，一个用于时间序列异常检测的新框架，它利用基础模型的预训练知识。THEMIS从Chronos时间序列基础模型的编码器中提取嵌入，并在自相似矩阵上应用局部异常因子和谱分解等异常检测技术，以发现数据中的异常。我们的实验表明，这种模块化方法在MSL数据集上取得了最先进的结果，并在SMAP和SWAT数据集上表现得相当有竞争力。值得注意的是，THEMIS超过了专门为异常检测训练的模型，具有超参数鲁棒性和默认的可解释性。本文提倡使用基础模型的预训练表示来对时间序列数据进行高效且适应性强的异常检测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series anomaly detection forms a very crucial area in several domainsbut poses substantial challenges. Due to time series data possessingseasonality, trends, noise, and evolving patterns (concept drift), it becomesvery difficult to set a general notion of what constitutes normal behavior.Anomalies themselves could be varied, ranging from a single outlier tocontextual or collective anomalies, and are normally very rare; hence, thedataset is largely imbalanced. Additional layers of complexities arise due tothe problems of increased dimensionality of modern time series, real-timedetection criteria, setting up appropriate detection thresholds, and arrivingat results that are interpretable. To embrace these multifaceted challenges,very strong, flexible, and interpretable approaches are required. This paperpresents THEMIS, a new framework for time series anomaly detection thatexploits pretrained knowledge from foundation models. THEMIS extractsembeddings from the encoder of the Chronos time series foundation model andapplies outlier detection techniques like Local Outlier Factor and SpectralDecomposition on the self-similarity matrix, to spot anomalies in the data. Ourexperiments show that this modular method achieves SOTA results on the MSLdataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.Notably, THEMIS exceeds models trained specifically for anomaly detection,presenting hyperparameter robustness and interpretability by default. Thispaper advocates for pretrained representations from foundation models forperforming efficient and adaptable anomaly detection for time series data.</description>
      <author>example@mail.com (Yadav Mahesh Lorik, Kaushik Sarveswaran, Nagaraj Sundaramahalingam, Aravindakumar Venugopalan)</author>
      <guid isPermaLink="false">2510.03911v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</title>
      <link>http://arxiv.org/abs/2510.03751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025 Workshop CrocoDL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为参考集微调(RSF)的新方法，通过利用测试时的参考集（地图）信息来微调视觉位置识别(VPR)模型，以提高模型在具有挑战性基准测试上的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉位置识别(VPR)的任务是根据查询图像从参考数据库中检索同一地点的图像，需要能够抵抗视角和外观变化。虽然使用视觉基础模型主干网络并在大规模VPR数据集上训练的方法已解决了一些基准测试，但当测试环境与训练数据集有显著差异时，这些方法仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;探索一种互补的、未被探索的信息来源来弥合训练-测试域差距，以提高最先进VPR方法在具有挑战性基准测试上的性能。&lt;h4&gt;方法&lt;/h4&gt;在测试时的参考集（地图）上进行简单的参考集微调(RSF)，因为参考集包含目标域的图像和姿态信息，且在某些VPR应用中必须在接收测试查询之前可用。&lt;h4&gt;主要发现&lt;/h4&gt;在地图上微调VPR模型可以提高在具有挑战性数据集上的性能；微调后的模型平均Recall@1指标提升了约2.3%；微调后的模型保留了泛化能力；RSF方法在多样化的测试数据集上都有效。&lt;h4&gt;结论&lt;/h4&gt;参考集微调(RSF)是一种有效的方法，可以利用测试时的参考集信息来提高VPR模型在具有挑战性基准测试上的性能，同时保持模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;给定一个查询图像，视觉位置识别(VPR)是从参考数据库中检索同一地点图像的任务，需要能够抵抗视角和外观变化的影响。最近的研究表明，一些VPR基准测试可以通过使用视觉基础模型主干网络并在大规模、多样化的VPR专用数据集上进行训练来解决。然而，一些基准测试仍然具有挑战性，特别是当测试环境与通常的VPR训练数据集有显著差异时。我们提出了一种互补的、未被探索的信息来源来弥合训练-测试域差距，这可以进一步提高最先进(SOTA)的VPR方法在这些具有挑战性的基准测试上的性能。具体来说，我们识别出测试时的参考集，即'地图'，包含目标域的图像和姿态信息，并且在几种VPR应用中必须在接收测试时查询之前可用。因此，我们提出在地图上进行简单的参考集微调(RSF)的VPR模型，在这些具有挑战性的数据集上将SOTA性能提升了约2.3%（平均Recall@1）。微调后的模型保留了泛化能力，并且RSF在多样化的测试数据集上都有效。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉地点识别（VPR）中的'训练-测试域差距'问题，即当测试环境与训练数据集差异显著时，即使是最先进的VPR方法表现也会下降。这个问题很重要，因为许多VPR应用（如地标检索、3D建模、图像搜索和基于地图的定位）要求测试时参考集在接收查询前就已可用，解决这个问题能显著提高VPR系统在真实世界不同环境中的性能和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到当前VPR方法在类似训练环境的数据集上表现良好，但在差异大的测试环境中表现不佳。他们注意到测试时参考集（地图）一直被忽视，但这个信息源在VPR应用中是可用的。作者设计了参考集微调（RSF）策略，利用测试前可用的参考图像来微调预训练模型。该方法借鉴了视觉基础模型（如DinoV2）、特征聚合方法（如BoQ）、域适应技术和数据增强方法，但将这些技术整合到一个新颖的应用场景中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用测试时已经可用的参考图像集来微调预训练的VPR模型，从而缩小训练-测试域差距，无需新的训练数据或更强的骨干网络。实现流程包括：1) 从测试参考集创建微调数据集，通过数据增强生成查询图像；2) 使用预训练模型对图像进行特征编码；3) 利用姿态信息进行三元组挖掘，找到正样本和困难负样本；4) 使用三元组损失函数进行模型微调；5) 在验证集和测试集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 发现并利用测试时参考集的价值来缩小训练-测试域差距；2) 提出简单有效的参考集微调（RSF）策略；3) 设计姿态感知的三元组挖掘方法；4) 创建自监督微调框架，无需额外标注数据。相比之前工作，这篇论文不仅关注查询-参考域差距（传统VPR焦点），还关注训练-测试域差距；利用测试时参考集而非需要额外标注数据；专注于可以离线使用参考集的应用场景；即使在没有姿态信息的情况下，RSF仍然有效。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种简单而有效的参考集微调（RSF）策略，利用测试时已经可用的参考图像集来缩小视觉地点识别中的训练-测试域差距，显著提高了模型在未见环境中的性能，同时保持了模型的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given a query image, Visual Place Recognition (VPR) is the task of retrievingan image of the same place from a reference database with robustness toviewpoint and appearance changes. Recent works show that some VPR benchmarksare solved by methods using Vision-Foundation-Model backbones and trained onlarge-scale and diverse VPR-specific datasets. Several benchmarks remainchallenging, particularly when the test environments differ significantly fromthe usual VPR training datasets. We propose a complementary, unexplored sourceof information to bridge the train-test domain gap, which can further improvethe performance of State-of-the-Art (SOTA) VPR methods on such challengingbenchmarks. Concretely, we identify that the test-time reference set, the"map", contains images and poses of the target domain, and must be availablebefore the test-time query is received in several VPR applications. Therefore,we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models onthe map, boosting the SOTA (~2.3% increase on average for Recall@1) on thesechallenging datasets. Finetuned models retain generalization, and RSF worksacross diverse test datasets.</description>
      <author>example@mail.com (Mubariz Zaffar, Liangliang Nan, Sebastian Scherer, Julian F. P. Kooij)</author>
      <guid isPermaLink="false">2510.03751v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap Between Multimodal Foundation Models and World Models</title>
      <link>http://arxiv.org/abs/2510.03727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了如何弥合多模态基础模型与世界模型之间的差距，通过改进推理能力和生成能力，使模型能够更好地理解和模拟动态物理过程。&lt;h4&gt;背景&lt;/h4&gt;人类通过整合多种感官模态理解世界，受此启发，多模态基础模型(MFMs)已成为多模态理解和生成的强大工具。然而，当前MFMs作为世界模型存在不足，缺乏关键能力。&lt;h4&gt;目的&lt;/h4&gt;研究如何提升多模态基础模型的能力，使其更接近世界模型的功能，能够进行反事实推理、动态模拟、时空理解等高级认知任务。&lt;h4&gt;方法&lt;/h4&gt;1) 通过区分性任务提高MFMs推理能力，赋予因果推断、反事实思维和时空推理等结构化推理技能；2) 探索图像和视频模态中的生成能力，引入结构化和可控生成框架；3) 利用场景图、多模态条件和多模态对齐策略指导生成过程；4) 将技术扩展到可控4D生成，实现时空交互式、可编辑和可变形对象合成。&lt;h4&gt;主要发现&lt;/h4&gt;通过赋予MFMs结构化推理能力和可控生成能力，可以使其超越表面相关性，理解视觉和文本数据中的深层关系，并实现与高级语义和细粒度用户意图一致的生成结果。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型通过增强推理和生成能力，可以逐步具备世界模型的功能，更好地模拟和理解动态物理过程。&lt;h4&gt;翻译&lt;/h4&gt;人类通过整合多种感官模态来理解世界，使他们能够感知、推理和想象动态物理过程。受这一能力启发，多模态基础模型(MFMs)已成为多模态理解和生成的强大工具。然而，当今的MFMs作为有效的世界模型尚有不足。它们缺乏基本能力，如进行反事实推理、模拟动态、理解时空信息、控制生成的视觉结果和执行多方面推理。我们研究弥合多模态基础模型与世界模型之间差距所需的条件。我们首先通过区分性任务提高MFMs的推理能力，赋予其因果推断、反事实思维和时空推理等结构化推理技能，使它们能够超越表面相关性，理解视觉和文本数据中的深层关系。接下来，我们探索多模态基础模型在图像和视频模态中的生成能力，引入新的结构化和可控生成框架。我们的方法结合场景图、多模态条件和多模态对齐策略来指导生成过程，确保与高级语义和细粒度用户意图保持一致。我们进一步将这些技术扩展到可控4D生成，实现随时间和空间的交互式、可编辑和可变形对象合成。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态基础模型与世界模型之间的差距问题。当前的多模态模型虽然能处理多种任务，但缺乏核心的世界建模能力，如反事实推理、动态模拟、时空信息理解、视觉结果控制和多方面推理。这个问题很重要，因为人类通过整合多种感官模态理解世界，而缺乏这些能力的模型无法以类人方式模拟、规划和与世界互动，限制了人工智能系统向更高级认知能力的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出多模态基础模型与世界模型之间的差距，然后从两个主要方向进行改进：一是提升感知和推理能力，二是增强生成能力。在推理方面，作者借鉴了参数高效学习、提示学习范式、因果推理和图结构等方法；在生成方面，作者借鉴了扩散模型、多模态控制和场景图等技术。作者将现有方法与新的创新相结合，系统地解决了多模态模型的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强多模态模型的推理能力和生成能力来弥合与世界模型之间的差距。在推理方面，通过反事实思考、因果推理、图结构等方法使模型能够进行更深层次的推理；在生成方面，通过可控的文本到图像生成、视频生成的动态控制和4D场景生成等方法使模型能够模拟世界的变化。整体流程是：首先分析差距，然后分别从判别式和生成式两个方向进行改进，最后引入评估基准来验证进展。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出反事实提示学习范式增强模型推理能力；2) 设计多模态图变换器进行结构化推理；3) 开发判别性扩散模型将生成模型用于感知任务；4) 引入MMWorld和VLM4D基准全面评估世界模型能力；5) 开发FlexEControl、Mojito和Morpho4D框架实现可控的2D、3D和4D生成。相比之前工作，这些创新不仅关注表面理解，还强调深层推理；将生成模型与判别任务结合；从2D扩展到4D生成；强调可控性和交互性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过增强多模态模型的推理和生成能力并引入全面评估基准，显著缩小了多模态基础模型与世界模型之间的差距，朝着开发能像人类一样推理、模拟和与世界的智能系统迈出了重要一步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans understand the world through the integration of multiple sensorymodalities, enabling them to perceive, reason about, and imagine dynamicphysical processes. Inspired by this capability, multimodal foundation models(MFMs) have emerged as powerful tools for multimodal understanding andgeneration. However, today's MFMs fall short of serving as effective worldmodels. They lack the essential ability such as perform counterfactualreasoning, simulate dynamics, understand the spatiotemporal information,control generated visual outcomes, and perform multifaceted reasoning. Weinvestigates what it takes to bridge the gap between multimodal foundationmodels and world models. We begin by improving the reasoning capabilities ofMFMs through discriminative tasks and equipping MFMs with structured reasoningskills, such as causal inference, counterfactual thinking, and spatiotemporalreasoning, enabling them to go beyond surface correlations and understanddeeper relationships within visual and textual data. Next, we exploregenerative capabilities of multimodal foundation models across both image andvideo modalities, introducing new frameworks for structured and controllablegeneration. Our approaches incorporate scene graphs, multimodal conditioning,and multimodal alignment strategies to guide the generation process, ensuringconsistency with high-level semantics and fine-grained user intent. We furtherextend these techniques to controllable 4D generation, enabling interactive,editable, and morphable object synthesis over time and space.</description>
      <author>example@mail.com (Xuehai He)</author>
      <guid isPermaLink="false">2510.03727v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis</title>
      <link>http://arxiv.org/abs/2510.03555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)的灵活集成框架，能够无缝整合多个Foundation models的特征，保留它们的互补优势，无需手动特征选择或大量任务特定的微调。该方法在三个癌症数据集的分类任务中表现优异，展示了其鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;Foundation models已经通过提供强大的通用特征提取器改变了计算病理学领域。然而，针对特定诊断任务调整和评估单个Foundation模型通常耗时且资源密集，特别是考虑到它们的规模和多样性。&lt;h4&gt;目的&lt;/h4&gt;解决Foundation models在适应特定诊断任务和基准测试时面临的耗时和资源密集型挑战，提供一种无需大量任务特定微调的解决方案。&lt;h4&gt;方法&lt;/h4&gt;引入Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)，这是一种灵活的集成框架，能够无缝整合多个Foundation models的特征，保留它们的互补优势，而无需手动特征选择或大量任务特定的微调。&lt;h4&gt;主要发现&lt;/h4&gt;在前列腺癌(PANDA)、卵巢癌(UBC-OCEAN)和乳腺癌(TCGA-BrCa)三个癌症数据集的分类任务中，GAS-MIL始终取得了优于或相当于单个Foundation models和既定MIL方法的性能，证明了其鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过高效整合异构的Foundation models，GAS-MIL简化了病理学模型部署，并为未来的多模态和精准肿瘤学应用提供了可扩展的基础。&lt;h4&gt;翻译&lt;/h4&gt;Foundation models已经通过提供强大的通用特征提取器改变了计算病理学领域。然而，针对特定诊断任务调整和评估单个Foundation模型通常耗时且资源密集，特别是考虑到它们的规模和多样性。为应对这一挑战，我们引入了Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)，这是一种灵活的集成框架，能够无缝整合多个Foundation models的特征，保留它们的互补优势，而无需手动特征选择或大量任务特定的微调。在前列腺癌(PANDA)、卵巢癌(UBC-OCEAN)和乳腺癌(TCGA-BrCa)三个癌症数据集的分类任务中，GAS-MIL始终取得了优于或相当于单个Foundation models和既定MIL方法的性能，证明了其鲁棒性和泛化能力。通过高效整合异构的Foundation models，GAS-MIL简化了病理学模型部署，并为未来的多模态和精准肿瘤学应用提供了可扩展的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have transformed computational pathology by providingpowerful, general-purpose feature extractors. However, adapting andbenchmarking individual FMs for specific diagnostic tasks is oftentime-consuming and resource-intensive, especially given their scale anddiversity. To address this challenge, we introduce Group-Aggregative SelectionMulti-Instance Learning (GAS-MIL), a flexible ensemble framework thatseamlessly integrates features from multiple FMs, preserving theircomplementary strengths without requiring manual feature selection or extensivetask-specific fine-tuning. Across classification tasks in three cancerdatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MILconsistently achieves superior or on-par performance relative to individual FMsand established MIL methods, demonstrating its robustness and generalizability.By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlinesmodel deployment for pathology and provides a scalable foundation for futuremultimodal and precision oncology applications.</description>
      <author>example@mail.com (Peiran Quan, Zifan Gu, Zhuo Zhao, Qin Zhou, Donghan M. Yang, Ruichen Rong, Yang Xie, Guanghua Xiao)</author>
      <guid isPermaLink="false">2510.03555v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization for Semantic Segmentation: A Survey</title>
      <link>http://arxiv.org/abs/2510.03540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR2025W&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面概述了领域泛化语义分割的快速发展主题，对现有方法进行了分类和回顾，并展示了基础模型对领域泛化的重大影响。&lt;h4&gt;背景&lt;/h4&gt;尽管深度神经网络近年来取得了巨大进步，但它们在未知领域的泛化能力仍然是一个重大挑战。领域泛化(DG)作为应对这一挑战的动态领域应运而生。与无监督领域适应不同，领域泛化无法访问或了解目标领域，而是旨在跨越多个不同的未见过的目标领域进行泛化。领域泛化对于语义分割任务特别重要，该任务在生物医学或自动驾驶等多个领域都有应用。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在推进领域泛化研究，并激励科学家探索新的研究方向。&lt;h4&gt;方法&lt;/h4&gt;作者对现有方法进行了分类和回顾，确定了向基于基础模型的领域泛化的范式转变，并对所有方法进行了广泛的性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;研究确定了领域泛化语义分割领域向基于基础模型的范式转变，并通过广泛的性能比较突出了基础模型对领域泛化的重大影响。&lt;h4&gt;结论&lt;/h4&gt;基础模型对领域泛化有显著影响，这一综述为领域泛化研究提供了全面概述，并鼓励科学家探索新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度神经网络近年来取得了巨大进步，但它们在未知领域的泛化能力仍然是一个重大挑战。因此，领域泛化(DG)这一动态领域应运而生。与无监督领域适应不同，领域泛化无法访问或了解目标领域，而是旨在跨越多个不同的未见过的目标领域进行泛化。领域泛化对于语义分割任务特别重要，该任务在生物医学或自动驾驶等多个领域都有应用。这篇综述对领域泛化语义分割这一快速发展的主题进行了全面概述。我们对现有方法进行了分类和回顾，并确定了向基于基础模型的领域泛化的范式转变。最后，我们对所有方法进行了广泛的性能比较，突出了基础模型对领域泛化的重大影响。这篇综述旨在推进领域泛化研究，并激励科学家探索新的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization of deep neural networks to unknown domains is a majorchallenge despite their tremendous progress in recent years. For this reason,the dynamic area of domain generalization (DG) has emerged. In contrast tounsupervised domain adaptation, there is no access to or knowledge about thetarget domains, and DG methods aim to generalize across multiple differentunseen target domains. Domain generalization is particularly relevant for thetask semantic segmentation which is used in several areas such as biomedicineor automated driving. This survey provides a comprehensive overview of therapidly evolving topic of domain generalized semantic segmentation. We clusterand review existing approaches and identify the paradigm shift towardsfoundation-model-based domain generalization. Finally, we provide an extensiveperformance comparison of all approaches, which highlights the significantinfluence of foundation models on domain generalization. This survey seeks toadvance domain generalization research and inspire scientists to explore newresearch directions.</description>
      <author>example@mail.com (Manuel Schwonberg, Hanno Gottschalk)</author>
      <guid isPermaLink="false">2510.03540v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning</title>
      <link>http://arxiv.org/abs/2510.03519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法TS-Reasoner，通过将时间序列基础模型的潜在表示与大型语言模型的文本输入对齐，解决了时间序列推理中的挑战，实现了高效且准确的时间序列理解和推理。&lt;h4&gt;背景&lt;/h4&gt;时间序列推理在金融、能源使用、交通、天气和科学发现等多个领域的决策中至关重要。现有的时间序列基础模型(TSFMs)能够捕获低级动态模式并提供准确的预测，但通常需要额外的背景知识和复杂的推理能力，而这些是大多数TSFMs所缺乏的。大型语言模型(LLMs)可以实现这些推理能力，但如果没有昂贵的后训练，LLMs通常难以理解时间序列数据的数值方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来整合TSFMs和LLMs，使两种模态在推理任务上保持一致。开发TS-Reasoner，将TSFMs的潜在表示与LLMs的文本输入对齐，用于下游理解/推理任务。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单但有效的方法，为对齐训练整理多样化、合成的时间序列和文本标题对。开发一个两阶段训练方案，在对齐预训练后应用指令微调。与现有训练LLM接受时间序列作为输入的工作不同，他们利用预训练的TSFM并在训练期间冻结它。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的大量实验表明，TS-Reasoner不仅优于各种现有的LLMs、视觉语言模型(VLMs)和时间序列LLMs。TS-Reasoner以显著的数据效率实现了这一优势，例如使用不到一半的训练数据。&lt;h4&gt;结论&lt;/h4&gt;TS-Reasoner成功地将时间序列基础模型与大型语言模型结合起来，实现了高效的时间序列推理。&lt;h4&gt;翻译&lt;/h4&gt;时间序列推理对金融、能源使用、交通、天气和科学发现等不同领域的决策至关重要。虽然现有的时间序列基础模型(TSFMs)可以捕获低级动态模式并提供准确的预测，但进一步分析通常需要额外的背景知识和复杂的推理能力，这些能力在大多数TSFMs中缺乏，但可以通过大型语言模型(LLMs)实现。另一方面，如果没有昂贵的后训练，LLMs通常难以理解时间序列数据的数值方面。虽然直观地整合这两种类型的模型是可行的，但开发有效的训练方法来使两种模态在推理任务上保持一致仍然是一个开放的挑战。为此，我们提出了TS-Reasoner，它将TSFMs的潜在表示与LLMs的文本输入对齐，用于下游理解/推理任务。具体来说，我们提出了一种简单而有效的方法，为对齐训练整理多样化、合成的时间序列和文本标题对。然后，我们开发了一个两阶段训练方案，在对齐预训练后应用指令微调。与现有训练LLM接受时间序列作为输入的工作不同，我们利用预训练的TSFM并在训练期间冻结它。在几个基准测试上的大量实验表明，TS-Reasoner不仅优于各种现有的LLMs、视觉语言模型(VLMs)和时间序列LLMs，而且以显著的数据效率实现了这一优势，例如使用不到一半的训练数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series reasoning is crucial to decision-making in diverse domains,including finance, energy usage, traffic, weather, and scientific discovery.While existing time series foundation models (TSFMs) can capture low-leveldynamic patterns and provide accurate forecasting, further analysis usuallyrequires additional background knowledge and sophisticated reasoning, which arelacking in most TSFMs but can be achieved through large language models (LLMs).On the other hand, without expensive post-training, LLMs often struggle withthe numerical understanding of time series data. Although it is intuitive tointegrate the two types of models, developing effective training recipes thatalign the two modalities for reasoning tasks is still an open challenge. Tothis end, we propose TS-Reasoner that aligns the latent representations ofTSFMs with the textual inputs of LLMs for downstream understanding/reasoningtasks. Specifically, we propose a simple yet effective method to curatediverse, synthetic pairs of time series and textual captions for alignmenttraining. We then develop a two-stage training recipe that applies instructionfinetuning after the alignment pretraining. Unlike existing works that train anLLM to take time series as inputs, we leverage a pretrained TSFM and freeze itduring training. Extensive experiments on several benchmarks demonstrate thatTS-Reasoner not only outperforms a wide range of prevailing LLMs, VisionLanguage Models (VLMs), and Time Series LLMs, but also achieves this withremarkable data efficiency, e.g., using less than half the training data.</description>
      <author>example@mail.com (Fangxu Yu, Hongyu Zhao, Tianyi Zhou)</author>
      <guid isPermaLink="false">2510.03519v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation models for equation discovery in high energy physics</title>
      <link>http://arxiv.org/abs/2510.03397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了大型语言模型在高能物理符号回归任务中的应用潜力，特别是在发现已知没有封闭形式表达式的观测量的函数形式方面。&lt;h4&gt;背景&lt;/h4&gt;基础模型是在广泛、多模态数据集上训练的大型机器学习模型，在科学应用中受到越来越多的关注。大型语言模型作为基础模型的突出实例，在文本和图像生成等任务上取得了显著成功。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型（特别是大型语言模型）在高能物理中方程发现的潜力，专注于符号回归方法。&lt;h4&gt;方法&lt;/h4&gt;应用LLM-SR方法，研究轻子角分布方程恢复的基准问题，以及大型强子对撞机电弱玻色子产生中角系数函数形式的发现。&lt;h4&gt;主要发现&lt;/h4&gt;LLM-SR能够在领域内和领域外的运动学区域中发现紧凑、准确且可解释的方程，能够有效融入嵌入的科学知识。&lt;h4&gt;结论&lt;/h4&gt;为高能物理中的方程发现提供了有前景的新方法。&lt;h4&gt;翻译&lt;/h4&gt;基础模型，即在广泛、多模态数据集上训练的大型机器学习模型，由于其能在多样化的下游任务上表现出色，在科学应用中正获得越来越多的关注。大型语言模型作为基础模型的突出实例，已在文本和图像生成等任务上取得了显著成功。在这项工作中，我们研究了它们在高能物理中方程发现的潜力，专注于符号回归。我们将LLM-SR方法应用于轻子角分布方程恢复的基准问题，以及大型强子对撞机电弱玻色子产生中角系数函数形式的发现，这些是具有高现象学相关性的可观测量，对于这些量，目前还不知道从第一性原理得出的封闭形式表达式。我们的结果表明，LLM-SR能够在领域内和领域外的运动学区域中发现紧凑、准确且可解释的方程，有效融入嵌入的科学知识，为高能物理中的方程发现提供了有前景的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large machine learning models trained on broad, multimodaldatasets, have been gaining increasing attention in scientific applications dueto their strong performance on diverse downstream tasks. Large Language Models(LLMs), a prominent instance of foundation models, have achieved remarkablesuccess in tasks such as text and image generation. In this work, weinvestigate their potential for equation discovery in high energy physics,focusing on symbolic regression. We apply the LLM-SR methodology both tobenchmark problems of equation recovery in lepton angular distributions and tothe discovery of functional forms for angular coefficients in electroweak bosonproduction at the Large Hadron Collider, observables of high phenomenologicalrelevance for which no closed-form expressions are known from first principles.Our results demonstrate that LLM-SR can uncover compact, accurate, andinterpretable equations across in-domain and out-of-domain kinematic regions,effectively incorporating embedded scientific knowledge and offering apromising new approach to equation discovery in high energy physics.</description>
      <author>example@mail.com (Manuel Morales-Alvarado)</author>
      <guid isPermaLink="false">2510.03397v1</guid>
      <pubDate>Tue, 07 Oct 2025 15:36:54 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics</title>
      <link>http://arxiv.org/abs/2510.03031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Robotics and Automation Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于动力学地图（MoDs）的长期人类运动预测（LHMP）框架，能够在长达60秒的时间范围内准确预测人类运动，显著提高了机器人在与人类共享环境中的安全性和实用性。&lt;h4&gt;背景&lt;/h4&gt;长期人类运动预测对自主机器人和车辆在与人类共享的环境中安全高效运行至关重要，准确的预测对运动规划、跟踪、人机交互和安全监控等应用具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;实现长达60秒的长期人类运动预测，提高机器人在实际应用中的实用性。&lt;h4&gt;方法&lt;/h4&gt;利用动力学地图（MoDs）将空间或时空运动模式编码为环境特征，提出MoD信息化的LHMP框架支持多种MoD类型并包含排序方法输出最可能预测轨迹，引入时间条件MoD捕捉不同时间段的运动模式变化，使用三种MoD类型实例化MoD-LHMP并在两个真实世界数据集上评估。&lt;h4&gt;主要发现&lt;/h4&gt;MoD信息化的方法优于基于学习的方法，平均位移误差提高了高达50%，时间条件变体整体上达到了最高精度。&lt;h4&gt;结论&lt;/h4&gt;基于动力学地图的长期人类运动预测方法表现出色，其中时间条件MoD变体效果最佳。&lt;h4&gt;翻译&lt;/h4&gt;长期人类运动预测对于自主机器人和车辆在与人类共享的环境中的安全高效运行非常重要。准确的预测对于包括运动规划、跟踪、人机交互和安全监控在内的应用很重要。在本文中，我们利用动力学地图（MoDs），将空间或时空运动模式编码为环境特征，实现长达60秒的长期人类运动预测。我们提出了一个MoD信息化的LHMP框架，支持各种类型的MoDs，并包含一种排序方法来输出最可能的预测轨迹，提高了机器人的实际实用性。此外，引入了时间条件MoD来捕捉在不同时间段变化的运动模式。我们评估了使用三种类型MoD实例化的MoD-LHMP。在两个真实世界数据集上的实验表明，MoD信息化的方法优于基于学习的方法，平均位移误差提高了高达50%，时间条件变体整体上达到了最高精度。项目代码可在https://github.com/test-bai-cpu/LHMP-with-MoDs.git获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决长期人类运动预测问题，即预测人类在未来长达60秒的运动轨迹。这个问题对于自主机器人和车辆在人类共享环境中的安全高效运行至关重要，应用于运动规划、跟踪、人机交互和安全监控等多个领域。长期预测特别重要，因为它需要考虑环境对人类运动的复杂影响，而不仅仅是当前状态。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到人类运动受个人意图和环境因素共同影响，长期预测需要更全面的建模。他们借鉴了现有的CLiFF-LHMP方法，扩展了其使用范围，使其能适用于各种类型的动力学地图(MoDs)。作者还参考了CLiFF-map、STeF-map等现有地图表示方法，并创新性地引入了时间条件概念，捕捉不同时段的运动模式变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用动力学地图(MoDs)编码环境中的运动模式，并利用这些模式来引导长期人类运动预测。整体流程包括：1)从历史轨迹估计当前速度；2)在每个预测步骤从当前位置的MoD中采样速度；3)用采样速度偏置恒定速度模型(CVM)的预测；4)通过高斯核函数控制MoD的影响程度；5)迭代预测直到达到预测时间范围；6)使用排名方法输出最可能的预测轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通用的MoD-LHMP框架，支持各种类型的MoDs；2)引入排名方法，输出最可能的预测轨迹；3)提出时间条件CLiFF-map，捕捉不同时段的运动模式；4)在两个真实世界数据集上全面评估。相比之前的工作，该方法能更好地处理长期预测，考虑时间变化因素，且比基于学习的方法在长期预测上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于动力学地图的长期人类运动预测框架，通过引入时间条件地图和排名机制，显著提高了长期预测的准确性，为机器人在人类共享环境中的安全导航提供了更可靠的预测能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term human motion prediction (LHMP) is important for the safe andefficient operation of autonomous robots and vehicles in environments sharedwith humans. Accurate predictions are important for applications includingmotion planning, tracking, human-robot interaction, and safety monitoring. Inthis paper, we exploit Maps of Dynamics (MoDs), which encode spatial orspatio-temporal motion patterns as environment features, to achieve LHMP forhorizons of up to 60 seconds. We propose an MoD-informed LHMP framework thatsupports various types of MoDs and includes a ranking method to output the mostlikely predicted trajectory, improving practical utility in robotics. Further,a time-conditioned MoD is introduced to capture motion patterns that varyacross different times of day. We evaluate MoD-LHMP instantiated with threetypes of MoDs. Experiments on two real-world datasets show that MoD-informedmethod outperforms learning-based ones, with up to 50\% improvement in averagedisplacement error, and the time-conditioned variant achieves the highestaccuracy overall. Project code is available athttps://github.com/test-bai-cpu/LHMP-with-MoDs.git</description>
      <author>example@mail.com (Yufei Zhu, Andrey Rudenko, Tomasz P. Kucner, Achim J. Lilienthal, Martin Magnusson)</author>
      <guid isPermaLink="false">2510.03031v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
  <item>
      <title>When Researchers Say Mental Model/Theory of Mind of AI, What Are They Really Talking About?</title>
      <link>http://arxiv.org/abs/2510.02660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员声称AI系统拥有心智理论或心智模型时，实际上是在讨论行为预测和偏差修正，而非真正的心理状态。当前讨论混淆了复杂的模式匹配与真实认知，忽略了模拟与经验之间的区别。大型语言模型在心智理论实验室任务中的表现仅基于行为模仿，而非真实理解。现有测试范式存在缺陷，不应简单将人类认知测试应用于AI系统。建议转向相互的心智理论框架，关注人类与AI的互动动态。&lt;h4&gt;背景&lt;/h4&gt;当前学术界对AI系统是否拥有心智理论的讨论存在混淆，研究者经常将AI系统的行为表现误认为真正的认知能力。&lt;h4&gt;目的&lt;/h4&gt;区分AI系统的行为模拟与真实认知，改进AI心智理论的测试方法，提出更合理的评估框架。&lt;h4&gt;方法&lt;/h4&gt;分析当前AI心智理论研究的局限性，批判现有测试范式，提出相互的心智理论框架。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在心智理论实验室任务中的表现仅基于行为模仿；现有测试范式在评估AI认知能力时存在根本性缺陷；孤立测试AI无法准确评估其真正的认知能力。&lt;h4&gt;结论&lt;/h4&gt;需要区分AI系统的行为预测与真正的心理状态；应将研究重点转向人类与AI之间的互动动态；采用相互的心智理论框架，同时考虑人类认知和AI算法的贡献。&lt;h4&gt;翻译&lt;/h4&gt;当研究人员声称AI系统拥有心智理论或心智模型时，他们实际上是在讨论行为预测和偏差修正，而不是真正的心理状态。这篇立场论文认为，当前的讨论将复杂的模式匹配与真实的认知混为一谈，忽略了模拟与经验之间的关键区别。尽管最近的研究显示大型语言模型在心智理论实验室任务中达到了人类水平的性能，但这些结果仅基于行为模仿。更重要的是，整个测试范式可能存在缺陷，它将人类个体认知测试应用于AI系统，而不是在人类与AI互动的瞬间直接评估人类认知。我建议将重点转向相互的心智理论框架，承认人类认知和AI算法的同时贡献，强调互动动态，而不是孤立地测试AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When researchers claim AI systems possess ToM or mental models, they arefundamentally discussing behavioral predictions and bias corrections ratherthan genuine mental states. This position paper argues that the currentdiscourse conflates sophisticated pattern matching with authentic cognition,missing a crucial distinction between simulation and experience. While recentstudies show LLMs achieving human-level performance on ToM laboratory tasks,these results are based only on behavioral mimicry. More importantly, theentire testing paradigm may be flawed in applying individual human cognitivetests to AI systems, but assessing human cognition directly in the moment ofhuman-AI interaction. I suggest shifting focus toward mutual ToM frameworksthat acknowledge the simultaneous contributions of human cognition and AIalgorithms, emphasizing the interaction dynamics, instead of testing AI inisolation.</description>
      <author>example@mail.com (Xiaoyun Yin, Elmira Zahmat Doost, Shiwen Zhou, Garima Arya Yadav, Jamie C. Gorman)</author>
      <guid isPermaLink="false">2510.02660v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.02469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SIMSplat，一种基于语言对齐高斯飞溅的预测性驾驶场景编辑器，能够通过自然语言提示实现直观的场景操控和精确的对象级编辑。&lt;h4&gt;背景&lt;/h4&gt;基于传感器数据的驾驶场景操控正在成为传统虚拟驾驶模拟器的一个有前景的替代方案，但现有框架由于编辑能力有限，难以高效生成真实场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够支持直观操控和精确编辑的驾驶场景编辑框架，解决现有方法在场景生成和编辑方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;SIMSplat是一种语言控制的编辑器，通过将语言与高斯重建的场景对齐，支持直接查询道路对象，实现精确和灵活的编辑。该方法提供详细的对象级编辑，包括添加新对象和修改车辆与行人轨迹，并通过多智能体运动预测进行预测路径优化，生成真实的场景交互。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo数据集上的实验表明，SIMSplat具有广泛的编辑能力和在各种场景中的适应性，能够高效生成真实感强的驾驶场景。&lt;h4&gt;结论&lt;/h4&gt;SIMSplat为驾驶场景操控提供了一个有效的解决方案，通过自然语言控制实现精确的对象级编辑，并生成真实的场景交互，为自动驾驶研究提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于传感器数据的驾驶场景操控正在成为传统虚拟驾驶模拟器的一个有前景的替代方案。然而，现有框架由于编辑能力有限，难以高效生成真实场景。为解决这些挑战，我们提出了SIMSplat，一种具有语言对齐高斯飞溅的预测性驾驶场景编辑器。作为语言控制的编辑器，SIMSplat能够使用自然语言提示进行直观操控。通过将语言与高斯重建的场景对齐，它进一步支持直接查询道路对象，实现精确和灵活的编辑。我们的方法提供详细的对象级编辑，包括添加新对象和修改车辆与行人的轨迹，同时通过多智能体运动预测进行预测路径优化，生成场景中所有智能体之间的真实交互。在Waymo数据集上的实验证明了SIMSplat广泛的编辑能力和在各种场景中的适应性。项目页面：https://sungyeonparkk.github.io/simsplat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决驾驶场景编辑的效率和真实性问题。现有方法在创建多样化驾驶场景时能力有限，难以实现细粒度的对象级编辑，且通常只验证自车和目标对象的可行性，忽略了周围所有代理的响应。这个问题很重要，因为驾驶模拟器是自动驾驶算法开发的关键测试平台，能够高效编辑和生成真实驾驶场景对于测试自动驾驶系统在各种情况下的表现至关重要，特别是对于创建难以在现实中模拟的安全关键场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法在对象级编辑、场景可行性验证和行人编辑方面的局限性，设计了一个统一的语言控制驾驶模拟器。他们借鉴了4D高斯溅射技术用于场景重建，参考了LangSplat等语言-场景对齐方法但进行了改进以适应动态驾驶场景，使用了场景图表示方法和SAM-2、CLIP等工具。关键创新在于提出了运动感知语言对齐，使系统能理解驾驶场景中的行为描述，并设计了多代理路径精确保证编辑后场景的全局一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将语言特征与4D高斯场景对齐，实现自然语言驱动的驾驶场景编辑，并使用多代理路径精确保证场景真实性。整体流程包括：1)使用场景图4D高斯溅射重建场景；2)通过外观和时间对齐将语言特征嵌入高斯；3)LLM代理解析用户提示并执行编辑操作；4)多代理路径精修确保所有对象自然响应变化；5)扩散修复润色修改区域，确保输出无缝真实。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)运动感知语言对齐，能理解驾驶场景中的行为描述；2)基于LLM的对象级编辑器，支持细粒度的车辆和行人修改；3)多代理路径精修，确保编辑后场景的全局一致性。相比ChatSim，SIMSplat支持行人编辑和修改现有对象；相比OmniRe，支持通过详细提示控制对象；相比LangSplat等，专注于动态驾驶场景且查询性能显著提升；相比SceneCrafter，支持动态代理的显式运动编辑且无需用户提供3D边界框。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SIMSplat通过结合运动感知语言对齐、基于LLM的对象级编辑和多代理路径精修，实现了自然语言驱动的真实驾驶场景编辑，支持细粒度的车辆和行人修改，并确保编辑后场景的全局一致性和真实性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving scene manipulation with sensor data is emerging as a promisingalternative to traditional virtual driving simulators. However, existingframeworks struggle to generate realistic scenarios efficiently due to limitedediting capabilities. To address these challenges, we present SIMSplat, apredictive driving scene editor with language-aligned Gaussian splatting. As alanguage-controlled editor, SIMSplat enables intuitive manipulation usingnatural language prompts. By aligning language with Gaussian-reconstructedscenes, it further supports direct querying of road objects, allowing preciseand flexible editing. Our method provides detailed object-level editing,including adding new objects and modifying the trajectories of both vehiclesand pedestrians, while also incorporating predictive path refinement throughmulti-agent motion prediction to generate realistic interactions among allagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat'sextensive editing capabilities and adaptability across a wide range ofscenarios. Project page: https://sungyeonparkk.github.io/simsplat/</description>
      <author>example@mail.com (Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang)</author>
      <guid isPermaLink="false">2510.02469v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning</title>
      <link>http://arxiv.org/abs/2510.03142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://pku-epic.github.io/MM-Nav-Web/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于视觉-语言-动作（VLA）模型的视觉导航方法，通过教师-学生学习方式从合成专家数据中学习多样化的导航能力，并在合成和真实环境中验证了其有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉导航策略被认为是有前途的方向，因为它通过使用以自我为中心的视觉观察来模拟人类导航行为。然而，视觉观察的光学信息难以像LiDAR点云或深度图那样被明确建模，这需要智能模型和大规模数据。&lt;h4&gt;目的&lt;/h4&gt;利用视觉-语言-动作（VLA）模型的智能，通过教师-学生方式从合成专家数据中学习多样化的导航能力。&lt;h4&gt;方法&lt;/h4&gt;实现VLA模型MM-Nav作为基于预训练大型语言模型和视觉基础模型的多视图VLA（具有360度观察）；从三个具有不同导航能力的挑战性定制环境中收集专家数据（到达、挤压和避免）；使用从RL专家在线收集的数据迭代训练VLA模型，并根据个体能力的性能动态平衡训练比例。&lt;h4&gt;主要发现&lt;/h4&gt;在合成环境中的实验表明模型实现了强大的泛化能力；学生VLA模型优于RL教师，展示了整合多种能力的协同效应。&lt;h4&gt;结论&lt;/h4&gt;大量的真实世界实验进一步证实了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉导航策略被认为是一个有前途的方向，因为它通过使用以自我为中心的视觉观察来模仿人类的导航行为。然而，视觉观察的光学信息难以像LiDAR点云或深度图那样被明确建模，这需要智能模型和大规模数据。为此，我们提出利用视觉-语言-动作（VLA）模型的智能，通过教师-学生方式从合成专家数据中学习多样化的导航能力。具体来说，我们实现了VLA模型MM-Nav，这是一个基于预训练大型语言模型和视觉基础模型的多视图VLA（具有360度观察）。对于大规模导航数据，我们从三个使用特权深度信息训练的强化学习（RL）专家中收集专家数据，这些专家数据来自三个针对不同导航能力（到达、挤压和避免）定制的具有挑战性的环境。我们使用从RL专家在线收集的数据迭代训练VLA模型，其中训练比例基于个体能力的性能进行动态平衡。通过在合成环境中的大量实验，我们证明我们的模型实现了强大的泛化能力。此外，我们发现我们的学生VLA模型优于RL教师，这展示了整合多种能力的协同效应。大量的真实世界实验进一步证实了我们方法的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人视觉导航中的挑战，特别是在复杂环境中如何让机器人仅通过视觉信息进行有效导航。这个问题很重要，因为视觉导航能让机器人像人类一样通过观察环境来移动，但视觉信息难以像激光雷达那样直接建模，需要智能模型和大量数据。解决这一问题将使机器人在更复杂、更具挑战性的环境中工作，扩大机器人的应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到视觉导航面临一个矛盾：真实世界导航数据缺乏极端挑战场景，而合成数据虽有挑战性但存在模拟到现实的差距。为了解决这个矛盾，作者借鉴了视觉-语言-动作模型、强化学习专家和教师-学生训练策略等现有工作，设计了MM-Nav方法。他们使用四个摄像头实现360度环境感知，并训练三个专门的强化学习专家分别学习'到达'、'挤压'和'躲避'三种能力，然后将这些专家的知识整合到一个统一的VLA模型中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多视图感知和多专家学习，让机器人从不同专业专家那里学习并整合多种导航能力。整体流程分为三步：首先，在三个定制环境中训练专门的强化学习专家；其次，收集这些专家的成功轨迹数据，用于初始训练VLA模型；最后，采用教师-学生迭代训练方法，在模拟环境中部署VLA模型并在线收集专家数据，通过能力平衡的数据聚合策略动态调整不同能力数据的训练比例，持续优化模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）多视图VLA模型，使用四个摄像头实现360度环境感知；2）多专家学习策略，从三个专业强化学习专家学习不同导航能力；3）能力平衡的训练方法，根据模型在不同能力上的表现差距动态调整训练数据比例；4）教师-学生迭代训练框架。相比之前的工作，MM-Nav不仅提供更全面的环境感知，还直接输出连续速度命令而非离散动作，并通过整合多种能力实现了比单一专家更强的导航性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MM-Nav通过多视图视觉-语言-动作模型和多专家学习策略，让机器人从多个专业强化学习专家中学习并整合不同导航能力，在复杂环境中展现出比单一专家更强的导航性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation policy is widely regarded as a promising direction, as itmimics humans by using egocentric visual observations for navigation. However,optical information of visual observations is difficult to be explicitlymodeled like LiDAR point clouds or depth maps, which subsequently requiresintelligent models and large-scale data. To this end, we propose to leveragethe intelligence of the Vision-Language-Action (VLA) model to learn diversenavigation capabilities from synthetic expert data in a teacher-student manner.Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360observations) based on pretrained large language models and visual foundationmodels. For large-scale navigation data, we collect expert data from threereinforcement learning (RL) experts trained with privileged depth informationin three challenging tailor-made environments for different navigationcapabilities: reaching, squeezing, and avoiding. We iteratively train our VLAmodel using data collected online from RL experts, where the training ratio isdynamically balanced based on performance on individual capabilities. Throughextensive experiments in synthetic environments, we demonstrate that our modelachieves strong generalization capability. Moreover, we find that our studentVLA model outperforms the RL teachers, demonstrating the synergistic effect ofintegrating multiple capabilities. Extensive real-world experiments furtherconfirm the effectiveness of our method.</description>
      <author>example@mail.com (Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2510.03142v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion</title>
      <link>http://arxiv.org/abs/2510.03110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://bb12346.github.io/GeoComplete/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoComplete是一种新型框架，通过引入显式3D结构指导来强制补全区域中的几何一致性，解决了目标视图与参考图像差异大时的图像补全挑战。&lt;h4&gt;背景&lt;/h4&gt;基于参考的图像补全技术在目标视图与参考图像差异显著时特别具有挑战性。现有生成方法仅依赖扩散先验，缺乏几何线索（如相机姿态或深度），常产生错位或不可信的内容。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为GeoComplete的新框架，在补全区域中强制执行几何一致性，区别于先前的仅基于图像的方法。&lt;h4&gt;方法&lt;/h4&gt;GeoComplete引入两个关键想法：将扩散过程条件化到投影点云上注入几何信息，以及应用目标感知掩码引导模型关注相关参考线索。框架采用双分支扩散架构，一个分支合成缺失区域，另一个提取几何特征，通过跨分支联合自注意力确保连贯准确的补全。此外，通过投影目标视图到参考图像中检测遮挡区域并掩码处理，指导模型关注有用线索。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合几何感知的双分支扩散架构和目标感知掩码策略，GeoComplete为几何条件图像补全提供了统一且强大的解决方案。实验表明，GeoComplete比最先进方法实现了17.1 PSNR的改进，显著提高了几何准确性同时保持高质量视觉效果。&lt;h4&gt;结论&lt;/h4&gt;GeoComplete是一种统一且强大的解决方案，用于几何条件图像补全，通过结合几何信息和目标感知掩码策略，在补全质量和几何准确性方面优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;基于参考的图像补全使用额外图像恢复目标视图中的缺失区域，当目标视图与参考图像差异显著时尤其具有挑战性。现有生成方法仅依赖扩散先验，没有几何线索如相机姿态或深度，常常产生错位或不可信的内容。我们提出了GeoComplete，一个新框架，通过引入显式3D结构指导来强制补全区域中的几何一致性，区别于先前的仅基于图像的方法。GeoComplete引入两个关键想法：将扩散过程条件化到投影点云上以注入几何信息，以及应用目标感知掩码引导模型关注相关参考线索。该框架采用双分支扩散架构，一个分支从掩码目标合成缺失区域，另一个分支从投影点云中提取几何特征。跨分支的联合自注意力确保连贯和准确的补全。为解决参考中可见但目标中缺失的区域，我们将目标视图投影到每个参考中以检测遮挡区域，然后在训练期间对这些区域进行掩码处理。这种目标感知掩码指导模型关注有用的线索，提高在困难场景中的性能。通过整合几何感知的双分支扩散架构和目标感知掩码策略，GeoComplete为几何条件图像补全提供了统一且强大的解决方案。实验表明，GeoComplete比最先进方法实现了17.1 PSNR的改进，显著提高了几何准确性同时保持高质量视觉效果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决参考驱动的图像补全中的几何一致性问题。当目标图像与参考图像视角差异较大时，现有生成方法往往会产生错位或不合理的内容。这个问题在现实中非常重要，因为它直接影响图像修复的质量和可信度，对于需要精确空间关系的应用（如自动驾驶、增强现实、图像编辑等）尤为关键。几何一致性是保持图像真实感和合理性的基础，缺乏这种一致性的补全结果会显得不自然甚至荒谬。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统几何方法采用多阶段流程（姿态估计、深度重建等），容易在复杂场景中出现错误累积；而生成方法如RealFill在目标视图与参考差异较大时表现不佳，因为没有几何线索。作者借鉴了扩散模型的强大生成能力、VGGT用于统一预测相机参数和深度图、LangSAM用于分割动态对象。在此基础上，作者设计了将显式3D结构指导整合到扩散模型中的方法，通过双分支架构和目标感知掩码策略来解决几何一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将显式3D几何信息注入扩散模型，强制补全区域保持几何一致性，并通过目标感知掩码引导模型关注有意义的参考线索。整体实现流程包括：1)点云生成：使用LangSAM过滤动态区域，用VGGT估计相机参数和深度图，构建并投影3D点云；2)目标感知掩码：将目标图像投影到参考视图，识别信息丰富区域，应用条件参考掩码和条件点云掩码；3)双分支扩散：目标分支处理掩码图像生成缺失内容，云分支处理投影点云提供几何指导，通过联合自注意力融合两个分支信息，并使用注意力掩码确保掩码标记直接接收几何线索。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双分支扩散架构：结合目标分支和云分支，通过联合自注意力融合视觉和几何信息；2)目标感知掩码策略：选择性掩码信息丰富区域，引导模型关注有意义的参考线索；3)几何信息注入：将显式3D几何信息整合到扩散模型中。相比之前的工作，GeoComplete避免了传统几何方法的多阶段错误累积，超越了纯图像方法的几何限制，不假设所有视图共享相同几何（区别于NeRF等方法），并通过LangSAM提高了动态场景的处理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoComplete通过将显式3D几何信息整合到双分支扩散模型中，并引入目标感知掩码策略，显著提升了参考驱动图像补全中的几何一致性和视觉质量，实现了比现有方法高17.1%的PSNR性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reference-driven image completion, which restores missing regions in a targetview using additional images, is particularly challenging when the target viewdiffers significantly from the references. Existing generative methods relysolely on diffusion priors and, without geometric cues such as camera pose ordepth, often produce misaligned or implausible content. We propose GeoComplete,a novel framework that incorporates explicit 3D structural guidance to enforcegeometric consistency in the completed regions, setting it apart from priorimage-only approaches. GeoComplete introduces two key ideas: conditioning thediffusion process on projected point clouds to infuse geometric information,and applying target-aware masking to guide the model toward relevant referencecues. The framework features a dual-branch diffusion architecture. One branchsynthesizes the missing regions from the masked target, while the otherextracts geometric features from the projected point cloud. Jointself-attention across branches ensures coherent and accurate completion. Toaddress regions visible in references but absent in the target, we project thetarget view into each reference to detect occluded areas, which are then maskedduring training. This target-aware masking directs the model to focus on usefulcues, enhancing performance in difficult scenarios. By integrating ageometry-aware dual-branch diffusion architecture with a target-aware maskingstrategy, GeoComplete offers a unified and robust solution forgeometry-conditioned image completion. Experiments show that GeoCompleteachieves a 17.1 PSNR improvement over state-of-the-art methods, significantlyboosting geometric accuracy while maintaining high visual quality.</description>
      <author>example@mail.com (Beibei Lin, Tingting Chen, Robby T. Tan)</author>
      <guid isPermaLink="false">2510.03110v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots</title>
      <link>http://arxiv.org/abs/2510.02885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures, accepted to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型运动规划算法，通过结合实时动态障碍物跟踪、控制屏障函数和非线性模型预测控制，实现了自主移动机器人在复杂环境中的安全导航。&lt;h4&gt;背景&lt;/h4&gt;自主移动机器人在动态环境中进行安全导航是一个挑战性问题，需要有效处理静态和动态障碍物。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理静态和动态障碍物的运动规划算法，确保机器人在复杂环境中的安全导航。&lt;h4&gt;方法&lt;/h4&gt;1. 集成实时动态障碍物跟踪和映射系统，将点云分类为动态和静态组件；2. 使用卡尔曼滤波器估计和预测动态点云的运动状态；3. 外推动态点云的未来状态并与静态点云合并构建前向时间域地图；4. 结合控制屏障函数与非线性模型预测控制进行障碍物避障；5. 基于预测状态与地图碰撞检测确定的风险点制定CBF约束。&lt;h4&gt;主要发现&lt;/h4&gt;1. 该算法在模拟和真实场景的复杂环境中表现出有效性；2. 与两种基线方法相比，该算法在障碍物避障的安全性和鲁棒性方面表现更优。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法通过有效整合动态障碍物预测和控制策略，显著提高了机器人在复杂环境中的安全导航能力。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了一种新颖的运动规划算法，以促进自主移动机器人的安全关键导航。所提出的算法集成了一个实时动态障碍物跟踪和映射系统，将点云分为动态和静态组件。对于动态点云，采用卡尔曼滤波器来估计和预测它们的运动状态。基于这些预测，我们外推动态点云的未来状态，随后与静态点云合并以构建前向时间域地图。通过将控制屏障函数与非线性模型预测控制相结合，所提出的算法使机器人能够有效避开静态和动态障碍物。CBF约束是基于通过预测未来状态与FTD地图之间的碰撞检测确定的风险点制定的。来自模拟和真实场景的实验结果证明了所提出的算法在复杂环境中的有效性。在模拟实验中，将所提出的算法与两种基线方法进行了比较，显示出在障碍物避障方面的安全性和鲁棒性方面的优越性能。源代码已发布供机器人社区参考。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自主移动机器人在同时包含静态和动态障碍物的复杂环境中进行安全导航的问题。这个问题在现实中非常重要，因为自主机器人需要在人类共享空间中安全工作，而现有方法通常将障碍物简化为椭球体，导致过度保守的避障行为，特别是在处理不规则形状障碍物时，限制了机器人的运动能力和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（如过度保守的障碍物建模、难以同时处理静态和动态障碍物）来设计新方法。他们借鉴了动态障碍物检测技术（如点云聚类、YOLO检测器）、控制屏障函数(CBF)理论和非线性模型预测控制(NMPC)框架，但进行了创新性整合。具体来说，他们改进了障碍物检测方法（YOLO-Fusion检测器），开发了基于点云的直接障碍物表示方法，并设计了前向时域(FTD)地图来整合当前和预测的未来障碍物状态。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将控制屏障函数(CBF)与非线模型预测控制(NMPC)相结合，直接使用点云数据表示障碍物，并构建包含预测信息的环境地图。整体流程包括：1)使用RGB-D相机获取环境数据并生成点云；2)通过YOLO-Fusion检测器识别动态障碍物；3)使用卡尔曼滤波预测动态障碍物运动状态；4)构建前向时域(FTD)地图，整合静态和动态点云；5)识别静态和动态风险点；6)基于风险点生成控制屏障函数；7)将CBF约束集成到NMPC框架中生成安全轨迹；8)实时执行控制指令。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于点云的直接障碍物表示，而非简化为几何形状；2)前向时域(FTD)地图整合当前和预测的未来障碍物状态；3)启发式高风险碰撞点识别方法，包括前向-逆向碰撞检测器和动态风险点识别；4)YOLO-Fusion检测器提高动态障碍物检测精度。相比之前的工作，这种方法避免了过度保守的避障行为，能够处理不规则形状障碍物（如有空心区域的黑板），同时能有效预测和避让动态障碍物，在复杂环境中表现出更高的安全性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于点云的控制屏障函数与非线模型预测控制相结合的新方法，通过构建前向时域地图和识别高风险碰撞点，实现了自主移动机器人在复杂动态环境中安全、高效地同时避让静态和动态障碍物。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we propose a novel motion planning algorithm to facilitatesafety-critical navigation for autonomous mobile robots. The proposed algorithmintegrates a real-time dynamic obstacle tracking and mapping system thatcategorizes point clouds into dynamic and static components. For dynamic pointclouds, the Kalman filter is employed to estimate and predict their motionstates. Based on these predictions, we extrapolate the future states of dynamicpoint clouds, which are subsequently merged with static point clouds toconstruct the forward-time-domain (FTD) map. By combining control barrierfunctions (CBFs) with nonlinear model predictive control, the proposedalgorithm enables the robot to effectively avoid both static and dynamicobstacles. The CBF constraints are formulated based on risk points identifiedthrough collision detection between the predicted future states and the FTDmap. Experimental results from both simulated and real-world scenariosdemonstrate the efficacy of the proposed algorithm in complex environments. Insimulation experiments, the proposed algorithm is compared with two baselineapproaches, showing superior performance in terms of safety and robustness inobstacle avoidance. The source code is released for the reference of therobotics community.</description>
      <author>example@mail.com (Faduo Liang, Yunfeng Yang, Shi-Lu Dai)</author>
      <guid isPermaLink="false">2510.02885v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>Visualizing Spatial Point Clouds: A Task-Oriented Taxonomy</title>
      <link>http://arxiv.org/abs/2510.02651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了3D点云可视化的设计空间，提出了一种分类法来映射四十年来可视化设计选择与现代应用挑战的关系，为开发更有效、可解释和以用户为中心的可视化技术提供了框架。&lt;h4&gt;背景&lt;/h4&gt;3D点云数据可视化在自主导航、环境监测和灾难响应等领域至关重要，这些领域的任务如物体识别、结构分析和时空探索依赖于清晰有效的视觉表示。尽管AI驱动处理技术有所进步，可视化仍然是解释复杂数据集的关键工具。然而，由于数据的稀疏性、密度变化和规模问题，设计有效的点云可视化面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;分析空间点云可视化的设计空间，并解决可视化技术与分析目标之间缺乏系统映射的问题。&lt;h4&gt;方法&lt;/h4&gt;引入一种分类法，对四十年来可视化设计选择进行分类，并将这些选择与现代应用中的基本挑战联系起来。基于数据类型、用户目标和可视化技术构建可视化策略框架。&lt;h4&gt;主要发现&lt;/h4&gt;在可视化技术与分析目标之间缺乏系统映射，需要一种将可视化设计选择与挑战联系起来的分类法。&lt;h4&gt;结论&lt;/h4&gt;该框架为推进更有效、可解释和以用户为中心的可视化技术提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;3D点云数据的可视化在自主导航、环境监测和灾难响应等领域至关重要，在这些领域中，物体识别、结构分析和时空探索等任务依赖于清晰有效的视觉表示。尽管AI驱动处理技术有所进步，可视化仍然是解释复杂数据集的关键工具。然而，由于数据的稀疏性、密度变化和规模问题，设计有效的点云可视化面临重大挑战。在这项工作中，我们分析了空间点云可视化的设计空间，突显了可视化技术与分析目标之间系统映射的差距。我们引入了一种分类法，对四十年来可视化设计选择进行分类，并将它们与现代应用中的基本挑战联系起来。通过基于数据类型、用户目标和可视化技术构建可视化策略，我们的框架为推进更有效、可解释和以用户为中心的可视化技术提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空间点云可视化的系统化分类和设计空间映射问题。这个问题很重要，因为点云数据在自动驾驶、环境监测和灾害响应等领域至关重要，这些领域的任务如物体识别和结构分析依赖于清晰有效的视觉表示。尽管AI处理技术有所进步，可视化仍是解释复杂数据集的关键工具，而点云数据的稀疏性、密度变化和规模使得设计有效可视化面临重大挑战，缺乏系统化框架阻碍了更有效可视化技术的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用系统化文献综述方法，从主要可视化期刊收集论文并使用关键词搜索，初始获得1200多篇论文后筛选出相关研究。他们采用What-Why-How框架组织知识，这一框架在可视化领域已建立。作者确实借鉴了现有工作，参考了点云处理计算方法的调查、特定可视化技术的调查，并融合了抽象点数据表示和基于深度学习的点云处理的分类法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过任务导向分类法系统化组织和理解空间点云可视化设计空间，强调可视化应紧密联系用户分析目标而非仅关注技术实现。整体流程分三部分：数据抽象(What)定义点云关键特征；任务抽象(Why)将用户任务分为部分-整体关系、空间关系和时间关系；设计选择(How)按渲染基元分为基于点、基于网格和基于几何三类技术，每类包含具体实现方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：创建首个系统化框架将四十年点云可视化技术映射到用户任务；采用以用户为中心的What-Why-How框架；进行全面的文献综述并连接可视化技术与机器学习方法；提出新的任务分类体系。相比之前工作，本文专注于空间点云而非非空间点可视化，强调低级视觉任务，采用系统化文献综述而非技术性综述，以用户任务而非技术为中心，同时考虑传统和基于深度学习的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建一个系统化的任务导向分类法，填补了点云可视化技术与用户分析目标之间的映射差距，为未来更有效、可解释和以用户为中心的点云可视化技术的发展奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The visualization of 3D point cloud data is essential in fields such asautonomous navigation, environmental monitoring, and disaster response, wheretasks like object recognition, structural analysis, and spatiotemporalexploration rely on clear and effective visual representation. Despiteadvancements in AI-driven processing, visualization remains a critical tool forinterpreting complex spatial datasets. However, designing effective point cloudvisualizations presents significant challenges due to the sparsity, densityvariations, and scale of the data. In this work, we analyze the design space ofspatial point cloud visualization, highlighting a gap in systematically mappingvisualization techniques to analytical objectives. We introduce a taxonomy thatcategorizes four decades of visualization design choices, linking them tofundamental challenges in modern applications. By structuring visualizationstrategies based on data types, user objectives, and visualization techniques,our framework provides a foundation for advancing more effective,interpretable, and user-centered visualization techniques.</description>
      <author>example@mail.com (Mahsa Partovi, Federico Iuricich)</author>
      <guid isPermaLink="false">2510.02651v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality</title>
      <link>http://arxiv.org/abs/2510.02464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Extended Reality Universal Planning Toolkit (ERUPT)，一个扩展现实(XR)交互式运动规划系统，允许用户在三维环境中创建、重新配置环境并规划机器人路径。&lt;h4&gt;背景&lt;/h4&gt;传统机器人路径规划通常在二维屏幕上进行，缺乏空间直观性和自然交互能力。扩展现实技术为机器人运动规划提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于扩展现实的交互式机器人运动规划系统，提供更直观的空间理解和更自然的交互方式，以提高规划效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;设计并实现了一个名为ERUPT的扩展现实系统，该系统允许用户在沉浸式三维环境中自然地与虚拟对象交互，集成了MoveIt操作规划框架，并提供多种交互模态。&lt;h4&gt;主要发现&lt;/h4&gt;通过扩展现实环境，用户能够获得更好的空间理解，使用自然交互方式调整环境对象，可视化机器人运动，并在无碰撞风险的环境中进行规划。&lt;h4&gt;结论&lt;/h4&gt;ERUPT系统通过结合扩展现实技术和机器人运动规划，提供了一种更直观、更自然的机器人路径规划方法，可以在虚拟环境中验证规划结果后部署到实际机器人。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了扩展现实通用规划工具包(ERUPT)，一个用于交互式运动规划的扩展现实(XR)系统。我们的系统允许用户在规划机器人路径时创建和动态重新配置环境。在沉浸式三维XR环境中，用户获得更好的空间理解。XR还解锁了更广泛的自然交互能力，允许用户像在现实世界中一样抓取和调整环境中的物体，而不是使用鼠标和键盘将场景投影到二维计算机屏幕上。我们的系统与MoveIt（操作规划框架）集成，允许用户发送运动规划请求并在虚拟或增强现实中可视化 resulting机器人路径。我们提供广泛的交互模态，允许用户修改环境中的对象并与虚拟机器人交互。我们的系统允许操作员可视化机器人运动，确保其在整个环境中移动时具有期望的行为，在虚拟空间内没有碰撞风险，然后将规划好的路径部署到现实世界中的物理机器人。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人运动规划中的交互式环境配置和路径规划问题。传统方法在二维屏幕上可视化环境，限制了用户的空间理解，难以准确感知机器人在三维空间中的行为。这个问题很重要，因为机器人正越来越多地应用于生活和工业环境，这些场景常需要人类重新配置环境，机器人需适应新环境。二维可视化增加了规划难度和错误风险，而直接在真实环境中测试路径可能存在碰撞风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统二维可视化的局限性，然后考察了扩展现实技术在机器人领域的应用潜力。他们注意到游戏引擎如Unity提供了创建XR应用的便利性，并且有ROS接口可实现与机器人操作系统的通信。作者确实借鉴了现有工作，包括使用ROS2作为基础框架，集成MoveIt作为运动规划平台，利用Unity游戏引擎和OpenXR作为XR接口，以及使用Unity ROS-TCP Connector实现通信。但他们将这些技术以新方式组合，创造了更直观的交互体验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩展现实技术提供沉浸式三维环境，让用户能自然地与虚拟机器人和环境物体交互，从而更直观地配置环境和规划路径。实现流程是：1)系统由XR界面和ROS节点组成；2)用户可在XR中导入机器人、添加/修改物体、与机器人交互、设置路径起点终点；3)XR中的变化通过ROS2同步到MoveIt规划场景；4)用户发起规划请求，请求被转发到MoveIt；5)MoveIt计算路径后返回并在XR中可视化；6)用户预览路径满意后可执行到物理机器人上。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开源XR系统用于交互式运动规划；2)沉浸式可视化运动规划反馈；3)与ROS2集成，支持虚拟到物理的路径执行；4)提供系统演示。相比之前工作，ERUPT完全基于ROS2构建，而大多数现有系统不支持；允许用户与环境中物体交互，而不仅仅是规划路径；支持创建和修改虚拟物体评估不同环境配置，无需停止系统；提供更自然的三维交互方式；实现了XR环境与MoveIt规划场景的实时同步。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ERUPT通过将扩展现实技术与机器人操作系统和运动规划框架相结合，创造了一个直观的三维交互环境，使用户能自然地配置环境、规划路径并预览运动，然后将结果部署到物理机器人上，大大提高了机器人路径规划的效率和安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Extended Reality Universal Planning Toolkit (ERUPT), anextended reality (XR) system for interactive motion planning. Our system allowsusers to create and dynamically reconfigure environments while they plan robotpaths. In immersive three-dimensional XR environments, users gain a greaterspatial understanding. XR also unlocks a broader range of natural interactioncapabilities, allowing users to grab and adjust objects in the environmentsimilarly to the real world, rather than using a mouse and keyboard with thescene projected onto a two-dimensional computer screen. Our system integrateswith MoveIt, a manipulation planning framework, allowing users to send motionplanning requests and visualize the resulting robot paths in virtual oraugmented reality. We provide a broad range of interaction modalities, allowingusers to modify objects in the environment and interact with a virtual robot.Our system allows operators to visualize robot motions, ensuring desiredbehavior as it moves throughout the environment, without risk of collisionswithin a virtual space, and to then deploy planned paths on physical robots inthe real world.</description>
      <author>example@mail.com (Isaac Ngui, Courtney McBeth, André Santos, Grace He, Katherine J. Mimnaugh, James D. Motes, Luciano Soares, Marco Morales, Nancy M. Amato)</author>
      <guid isPermaLink="false">2510.02464v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:25 +0800</pubDate>
    </item>
    <item>
      <title>MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition</title>
      <link>http://arxiv.org/abs/2510.03228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mixer的新型随机神经网络，用于纹理表示学习。该方法结合了传统技术和基于学习的方法的优势，通过超球面随机嵌入和双分支学习模块捕获通道内和通道间关系，并通过新制定的优化问题构建丰富的纹理表示。&lt;h4&gt;背景&lt;/h4&gt;随机神经网络在纹理识别任务中一直取得了显著成果，有效地结合了传统技术和基于学习的方法的优势。然而，现有方法主要集中在改进跨信息预测上，而没有对整体随机网络架构引入重大改进。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Mixer的新型随机神经网络，用于纹理表示学习，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;该方法的核心是利用超球面随机嵌入，结合双分支学习模块来捕获通道内和通道间的关系，并通过新制定的优化问题进一步增强，以构建丰富的纹理表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在几个纯纹理基准测试中取得了有趣的结果，每个基准测试都具有不同的特征和挑战。&lt;h4&gt;结论&lt;/h4&gt;Mixer方法在纹理表示学习方面表现出色，源代码将在发表后提供。&lt;h4&gt;翻译&lt;/h4&gt;随机神经网络用于表示学习在纹理识别任务中一直取得了显著成果，有效地结合了传统技术和基于学习的方法的优势。然而，现有方法到目前为止主要专注于改进跨信息预测，而没有对整体随机网络架构引入重大改进。在本文中，我们提出了Mixer，一种用于纹理表示学习的新型随机神经网络。其核心方法是利用超球面随机嵌入，结合双分支学习模块来捕获通道内和通道间的关系，并通过新制定的优化问题进一步增强，以构建丰富的纹理表示。实验结果表明，所提出的方法在几个纯纹理基准测试中取得了有趣的结果，每个基准测试都具有不同的特征和挑战。源代码将在发表后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Randomized neural networks for representation learning have consistentlyachieved prominent results in texture recognition tasks, effectively combiningthe advantages of both traditional techniques and learning-based approaches.However, existing approaches have so far focused mainly on improvingcross-information prediction, without introducing significant advancements tothe overall randomized network architecture. In this paper, we propose Mixer, anovel randomized neural network for texture representation learning. At itscore, the method leverages hyperspherical random embeddings coupled with adual-branch learning module to capture both intra- and inter-channelrelationships, further enhanced by a newly formulated optimization problem forbuilding rich texture representations. Experimental results have shown theinteresting results of the proposed approach across several pure texturebenchmarks, each with distinct characteristics and challenges. The source codewill be available upon publication.</description>
      <author>example@mail.com (Ricardo T. Fares, Lucas C. Ribas)</author>
      <guid isPermaLink="false">2510.03228v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>CVSM: Contrastive Vocal Similarity Modeling</title>
      <link>http://arxiv.org/abs/2510.03025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 tables, 8 figures. Submitted article at IEEE Trans. on  Audio, Speech and Language Proc. (pre-print version)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CVSM（对比声音相似性建模），一种用于音频领域音乐信号表示学习的对比自监督方法，可在音乐和声音相似性建模中有效应用。&lt;h4&gt;背景&lt;/h4&gt;大型无标注数据集在各个领域的可用性促进了多种通过自监督预训练学习表示方法的发展，这些方法能够为多个目标（下游）任务学习表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种用于音频领域音乐信号表示学习的对比自监督程序，可用于音乐和声音相似性建模。&lt;h4&gt;方法&lt;/h4&gt;CVSM在对比框架下运行，最大化相同声音片段和包含该声音的音乐混合体之间的相似性。设计了两种方案：基于标签的协议（利用艺术家身份信息采样对比对）和无标签方案（从随机采样的声音和伴奏片段创建人工混合体并与来自同一音频段的声音配对）。&lt;h4&gt;主要发现&lt;/h4&gt;通过CVSM学习到的表示在音乐和声音相似性建模中有效，在独立声音和完整音乐混合体上都优于多个基线。在预训练期间使用艺术家身份标签会导致更一致的性能，但无标签CVSM变体结合混合预训练也能达到相当的性能。&lt;h4&gt;结论&lt;/h4&gt;CVSM方法在声音相似性建模方面表现出色，即使在没有标签的情况下也能获得与有标签方法相当的性能。&lt;h4&gt;翻译&lt;/h4&gt;各个领域大型无标注数据集的可用性促进了多种通过自监督预学习为多个目标（下游）任务学习表示方法的发展。在本工作中，我们介绍了CVSM（对比声音相似性建模），一种用于音频领域音乐信号表示学习的对比自监督程序，可用于音乐和声音相似性建模。我们的方法在对比框架下运行，最大化相同声音片段和包含该声音的音乐混合体之间的相似性；我们设计了基于标签的协议，利用艺术家身份信息来采样对比对，以及无标签方案，涉及从随机采样的声音和伴奏片段创建人工混合体，这些混合体与来自同一音频段的声音配对。我们通过在一系列适当的下游任务上进行线性探测来客观评估声音相似性，并通过在基于查询的推荐设置中进行不同模型之间的成对比较的用户研究来主观评估。我们的结果表明，通过CVSM学习到的表示在音乐和声音相似性建模中有效，在独立声音和完整音乐混合体上都优于多个基线。此外，虽然在预训练期间使用艺术家身份标签会导致评估的下游任务和用户研究中整体更一致的性能，但结合真实和人工混合体的混合预训练的无标签CVSM变体在艺术家识别和感知声音相似性方面达到了与基于标签的方案相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The availability of large, unlabeled datasets across various domains hascontributed to the development of a plethora of methods that learnrepresentations for multiple target (downstream) tasks through self-supervisedpre-training. In this work, we introduce CVSM (Contrastive Vocal SimilarityModeling), a contrastive self-supervised procedure for music signalrepresentation learning in the audio domain that can be utilized for musicaland vocal similarity modeling. Our method operates under a contrastiveframework, maximizing the similarity between vocal excerpts and musicalmixtures containing the same vocals; we devise both a label-informed protocol,leveraging artist identity information to sample the contrastive pairs, and alabel-agnostic scheme, involving artificial mixture creation from randomlysampled vocal and accompaniment excerpts, which are paired with vocals from thesame audio segment. We evaluate our proposed method in measuring vocalsimilarity both objectively, through linear probing on a suite of appropriatedownstream tasks, and subjectively, via conducting a user study consisting ofpairwise comparisons between different models in a recommendation-by-querysetting. Our results indicate that the representations learned through CVSM areeffective in musical and vocal similarity modeling, outperforming numerousbaselines across both isolated vocals and complete musical mixtures. Moreover,while the availability of artist identity labels during pre-training leads tooverall more consistent performance both in the evaluated downstream tasks andthe user study, a label-agnostic CVSM variant incorporating hybrid pre-trainingwith real and artificial mixtures achieves comparable performance to thelabel-informed one in artist identification and perceived vocal similarity.</description>
      <author>example@mail.com (Christos Garoufis, Athanasia Zlatintsi, Petros Maragos)</author>
      <guid isPermaLink="false">2510.03025v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning</title>
      <link>http://arxiv.org/abs/2510.02763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SIT-FUSE的自监督机器学习框架，用于利用多传感器卫星数据检测和绘制有害藻华的严重程度和物种分布，无需每个仪器的标记数据集即可生成产品。&lt;h4&gt;背景&lt;/h4&gt;有害藻华监测对于海洋生态系统和人类健康至关重要，但在标记数据稀缺的环境中面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种自监督机器学习框架，用于检测和绘制有害藻华的严重程度和物种分布，减少对标记数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;融合VIIRS、MODIS、Sentinel-3和PACE等仪器的反射率数据与TROPOMI太阳诱导荧光数据；采用自监督表示学习和分层深度聚类方法；在墨西哥湾和南加利福尼亚(2018-2025)的实地数据上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;结果与总浮游植物、Karenia brevis、Alexandrium spp.和Pseudo-nitzschia spp.的测量值高度一致；该框架能够在标记稀缺环境中实现可扩展的有害藻华监测。&lt;h4&gt;结论&lt;/h4&gt;该工作通过分层嵌入实现了探索性分析，是迈向将自监督学习应用于全球水生生物地球化学操作化的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种自监督机器学习框架，用于利用多传感器卫星数据检测和绘制有害藻华的严重程度和物种分布。通过融合操作仪器(VIIRS、MODIS、Sentinel-3、PACE)的反射率数据与TROPOMI太阳诱导荧光，我们的框架名为SIT-FUSE，能够生成有害藻华严重程度和物种分布产品，而无需每个仪器的标记数据集。该框架采用自监督表示学习，通过分层深度聚类将浮游植物浓度和物种分割为可解释的类别，并在墨西哥湾和南加利福尼亚(2018-2025)的实地数据上进行了验证。结果显示与总浮游植物、Karenia brevis、Alexandrium spp.和Pseudo-nitzschia spp.的测量值高度一致。这项工作在标记稀缺环境中推进了可扩展的有害藻华监测，同时通过分层嵌入实现了探索性分析：这是将自监督学习操作化应用于全球水生生物地球化学的关键步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a self-supervised machine learning framework for detecting andmapping harmful algal bloom (HAB) severity and speciation using multi-sensorsatellite data. By fusing reflectance data from operational instruments (VIIRS,MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), ourframework, called SIT-FUSE, generates HAB severity and speciation productswithout requiring per-instrument labeled datasets. The framework employsself-supervised representation learning, hierarchical deep clustering tosegment phytoplankton concentrations and speciations into interpretableclasses, validated against in-situ data from the Gulf of Mexico and SouthernCalifornia (2018-2025). Results show strong agreement with total phytoplankton,Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. Thiswork advances scalable HAB monitoring in label-scarce environments whileenabling exploratory analysis via hierarchical embeddings: a critical steptoward operationalizing self-supervised learning for global aquaticbiogeochemistry.</description>
      <author>example@mail.com (Nicholas LaHaye, Kelly M. Luis, Michelle M. Gierach)</author>
      <guid isPermaLink="false">2510.02763v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering</title>
      <link>http://arxiv.org/abs/2510.02731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新型鲁棒属性图聚类方法(RAGC)，结合混合协同增强(HCA)和对比样本自适应差异感知(CSADA)策略，解决了现有对比属性图聚类方法中只关注节点级嵌入增强、忽略边级增强和样本对差异性的问题。&lt;h4&gt;背景&lt;/h4&gt;对比属性图聚类(CAGC)因其在自监督表示学习和聚类方面的强大能力取得了显著成功，主要依赖于有效的数据增强和对比目标设置。&lt;h4&gt;目的&lt;/h4&gt;解决现有CAGC方法忽略边级嵌入增强、不同粒度下节点级和边级嵌入增强交互以及平等对待所有对比样本对的问题，以提高方法的判别能力。&lt;h4&gt;方法&lt;/h4&gt;提出RAGC方法，同时执行节点级和边级嵌入表示和增强，建立更全面的相似性测量标准；利用高置信度伪标签信息设计CSADA策略，自适应识别并差异化处理对比样本对；HCA和CSADA模块在良性循环中相互强化。&lt;h4&gt;主要发现&lt;/h4&gt;同时考虑节点级和边级嵌入增强可以建立更全面的相似性测量标准；差异化处理难易正负样本对可以增强表示学习的判别能力；HCA和CSADA模块相互强化形成良性循环。&lt;h4&gt;结论&lt;/h4&gt;在六个基准数据集上的全面图聚类评估表明，所提出的RAGC方法相对于几种最先进的CAGC方法具有更好的效果。&lt;h4&gt;翻译&lt;/h4&gt;由于其强大的自监督表示学习和聚类能力，对比属性图聚类(CAGC)已取得巨大成功，这主要依赖于有效的数据增强和对比目标设置。然而，大多数CAGC方法使用边作为辅助信息来获取节点级嵌入表示，并且只关注节点级嵌入增强。这种方法忽略了边级嵌入增强以及不同粒度下节点级和边级嵌入增强之间的交互。此外，它们通常平等对待所有对比样本对，忽视了难易正负样本对之间的显著差异，这最终限制了它们的判别能力。为解决这些问题，提出了一种新型鲁棒属性图聚类(RAGC)，结合了混合协同增强(HCA)和对比样本自适应差异感知(CSADA)。首先，同时执行节点级和边级嵌入表示和增强，为后续对比学习建立更全面的相似性测量标准。反过来，判别相似性进一步有意识地指导边增强。其次，通过利用高置信度的伪标签信息，精心设计了CSADA策略，该策略自适应地识别所有对比样本对，并通过创新的权重调制函数差异化处理它们。HCA和CSADA模块在良性循环中相互强化，从而增强了表示学习的判别能力。在六个基准数据集上进行的全面图聚类评估证明了所提出的RAGC相对于几种最先进的CAGC方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to its powerful capability of self-supervised representation learning andclustering, contrastive attributed graph clustering (CAGC) has achieved greatsuccess, which mainly depends on effective data augmentation and contrastiveobjective setting. However, most CAGC methods utilize edges as auxiliaryinformation to obtain node-level embedding representation and only focus onnode-level embedding augmentation. This approach overlooks edge-level embeddingaugmentation and the interactions between node-level and edge-level embeddingaugmentations across various granularity. Moreover, they often treat allcontrastive sample pairs equally, neglecting the significant differencesbetween hard and easy positive-negative sample pairs, which ultimately limitstheir discriminative capability. To tackle these issues, a novel robustattributed graph clustering (RAGC), incorporating hybrid-collaborativeaugmentation (HCA) and contrastive sample adaptive-differential awareness(CSADA), is proposed. First, node-level and edge-level embeddingrepresentations and augmentations are simultaneously executed to establish amore comprehensive similarity measurement criterion for subsequent contrastivelearning. In turn, the discriminative similarity further consciously guidesedge augmentation. Second, by leveraging pseudo-label information with highconfidence, a CSADA strategy is elaborately designed, which adaptivelyidentifies all contrastive sample pairs and differentially treats them by aninnovative weight modulation function. The HCA and CSADA modules mutuallyreinforce each other in a beneficent cycle, thereby enhancing discriminabilityin representation learning. Comprehensive graph clustering evaluations over sixbenchmark datasets demonstrate the effectiveness of the proposed RAGC againstseveral state-of-the-art CAGC methods.</description>
      <author>example@mail.com (Tianxiang Zhao, Youqing Wang, Jinlu Wang, Jiapu Wang, Mingliang Cui, Junbin Gao, Jipeng Guo)</author>
      <guid isPermaLink="false">2510.02731v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking</title>
      <link>http://arxiv.org/abs/2510.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PGMEL，一种基于策略梯度的生成对抗网络，用于多模态实体链接，通过生成高质量负样本来改进表示学习，实验证明该方法优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;实体链接任务因其众多潜在应用而受到广泛关注。最近提出了各种多模态实体链接技术，旨在利用文本和视觉模态学习全面的嵌入表示。&lt;h4&gt;目的&lt;/h4&gt;填补现有文献中关于高质量负样本选择在多模态实体链接框架中未被探索的空白。&lt;h4&gt;方法&lt;/h4&gt;在生成对抗框架下解决多模态实体链接问题，生成器负责生成高质量负样本，判别器负责度量学习任务。由于生成样本是离散过程，使用策略梯度技术优化生成器，提出了基于策略梯度的生成对抗网络用于多模态实体链接(PGMEL)。&lt;h4&gt;主要发现&lt;/h4&gt;PGMEL通过选择具有挑战性的负样本来学习有意义的表示，在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上的实验结果证明PGMEL优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;PGMEL方法在多模态实体链接任务中表现优异，为负样本选择提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实体链接任务涉及将提及与知识图中的相应实体关联起来，由于其众多潜在应用而受到显著关注。最近，提出了各种多模态实体链接技术，旨在利用文本和视觉模态学习全面的嵌入表示。高质量负样本的选择可能在度量/表示学习中发挥关键作用。然而，据我们所知，这种可能性在现有文献的多模态实体链接框架中尚未被探索。为填补这一空白，我们在生成对抗设置下解决多模态实体链接问题，其中生成器负责生成高质量负样本，判别者负责度量学习任务。由于生成器参与生成样本，这是一个离散过程，我们使用策略梯度技术对其进行优化，并提出了基于策略梯度的生成对抗网络用于多模态实体链接(PGMEL)。基于Wiki-MEL、Richpedia-MEL和WikiDiverse数据集的实验结果表明，PGMEL通过选择具有挑战性的负样本来学习有意义的表示，并优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The task of entity linking, which involves associating mentions with theirrespective entities in a knowledge graph, has received significant attentiondue to its numerous potential applications. Recently, various multimodal entitylinking (MEL) techniques have been proposed, targeted to learn comprehensiveembeddings by leveraging both text and vision modalities. The selection ofhigh-quality negative samples can potentially play a crucial role inmetric/representation learning. However, to the best of our knowledge, thispossibility remains unexplored in existing literature within the framework ofMEL. To fill this gap, we address the multimodal entity linking problem in agenerative adversarial setting where the generator is responsible forgenerating high-quality negative samples, and the discriminator is assigned theresponsibility for the metric learning tasks. Since the generator is involvedin generating samples, which is a discrete process, we optimize it using policygradient techniques and propose a policy gradient-based generative adversarialnetwork for multimodal entity linking (PGMEL). Experimental results based onWiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learnsmeaningful representation by selecting challenging negative samples andoutperforms state-of-the-art methods.</description>
      <author>example@mail.com (KM Pooja, Cheng Long, Aixin Sun)</author>
      <guid isPermaLink="false">2510.02726v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>STSM-FiLM: A FiLM-Conditioned Neural Architecture for Time-Scale Modification of Speech</title>
      <link>http://arxiv.org/abs/2510.02672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为STSM-FILM的全神经架构，用于语音时间尺度修改，通过Feature-Wise Linear Modulation (FiLM)实现基于连续速度因子的条件化模型，能够在广泛的时间尺度因子范围内产生感知上一致的音频输出。&lt;h4&gt;背景&lt;/h4&gt;语音时间尺度修改(TSM)旨在改变音频播放速率而不改变音调。经典方法如WSOLA在非平稳或极端拉伸条件下常引入伪影。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的神经TSM架构，利用FiLM条件化提高模型在多种时间尺度因子下的表现。&lt;h4&gt;方法&lt;/h4&gt;使用WSOLA生成的输出来监督网络训练，使模型学习模仿对齐和合成行为；探索了四种编码器-解码器变体：STFT-HiFiGAN、WavLM-HiFiGAN、Whisper-HiFiGAN和EnCodec。&lt;h4&gt;主要发现&lt;/h4&gt;STSM-FILM能够在广泛的时间尺度因子范围内产生感知上一致的输出，基于FiLM的条件化提高了神经TSM模型的泛化能力和灵活性。&lt;h4&gt;结论&lt;/h4&gt;基于FiLM的条件化显示出改进神经TSM模型泛化能力和灵活性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;语音时间尺度修改(TSM)旨在改变音频的播放速率而不改变其音调。虽然像基于波形相似性的重叠相加(WSOLA)这样的经典方法提供了强大的基线，但它们在非平稳或极端拉伸条件下常常引入伪影。我们提出了STSM-FILM - 一种全神经架构，它结合了特征级线性调制(FiLM)，使模型能够根据连续的速度因子进行条件化。通过使用WSOLA生成的输出来监督网络，STSM-FILM学习模仿对齐和合成行为，同时受益于通过深度学习学习到的表示。我们探索了四种编码器-解码器变体：STFT-HiFiGAN、WavLM-HiFiGAN、Whisper-HiFiGAN和EnCodec，并证明STSM-FILM能够在广泛的时间尺度因子范围内产生感知上一致的输出。总体而言，我们的结果证明了基于FiLM的条件化在提高神经TSM模型的泛化能力和灵活性方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-Scale Modification (TSM) of speech aims to alter the playback rate ofaudio without changing its pitch. While classical methods like WaveformSimilarity-based Overlap-Add (WSOLA) provide strong baselines, they oftenintroduce artifacts under non-stationary or extreme stretching conditions. Wepropose STSM-FILM - a fully neural architecture that incorporates Feature-WiseLinear Modulation (FiLM) to condition the model on a continuous speed factor.By supervising the network using WSOLA-generated outputs, STSM-FILM learns tomimic alignment and synthesis behaviors while benefiting from representationslearned through deep learning. We explore four encoder-decoder variants:STFT-HiFiGAN, WavLM-HiFiGAN, Whisper-HiFiGAN, and EnCodec, and demonstrate thatSTSM-FILM is capable of producing perceptually consistent outputs across a widerange of time-scaling factors. Overall, our results demonstrate the potentialof FiLM-based conditioning to improve the generalization and flexibility ofneural TSM models.</description>
      <author>example@mail.com (Dyah A. M. G. Wisnu, Ryandhimas E. Zezario, Stefano Rini, Fo-Rui Li, Yan-Tsung Peng, Hsin-Min Wang, Yu Tsao)</author>
      <guid isPermaLink="false">2510.02672v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection</title>
      <link>http://arxiv.org/abs/2510.02610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MINERVA，一种基于互信息神经估计的监督特征选择方法，能有效处理高阶特征交互关系。&lt;h4&gt;背景&lt;/h4&gt;现有特征过滤器依赖统计成对依赖性指标建模特征-目标关系，但当目标依赖于高阶特征交互而非单个特征贡献时，这种方法可能失效。&lt;h4&gt;目的&lt;/h4&gt;引入Mutual Information Neural Estimation Regularized Vetting Algorithm (MINERVA)，一种基于特征和目标间互信息神经估计的新型监督特征选择方法。&lt;h4&gt;方法&lt;/h4&gt;使用神经网络参数化互信息近似，通过添加稀疏诱导正则化器的精心设计损失函数进行特征选择；采用两阶段流程解耦表示学习和特征选择；通过评估特征子集作为集成来捕获复杂关系。&lt;h4&gt;主要发现&lt;/h4&gt;展示了文献中很少捕捉到的普遍依赖结构，证明所提方法能有效捕获这些复杂的特征-目标关系。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实欺诈数据集上的实验结果验证了该方法的有效性及其执行精确解的能力。&lt;h4&gt;翻译&lt;/h4&gt;现有的特征过滤器依赖于统计成对依赖性指标来建模特征-目标关系，但当目标依赖于高阶特征交互而非单个特征贡献时，这种方法可能会失效。我们引入了互信息神经估计审查算法(MINERVA)，一种基于特征和目标之间互信息神经估计的新型监督特征选择方法。我们使用神经网络参数化互信息的近似，并使用精心设计的损失函数进行特征选择，该函数添加了稀疏诱导正则化器。我们的方法通过两阶段流程实现，将表示学习与特征选择解耦，确保更好的泛化能力和更准确的特征重要性表达。我们展示了文献中很少捕捉到的普遍依赖结构示例，并通过评估特征子集作为集成，证明我们提出的方法能有效捕获这些复杂的特征-目标关系。在合成和真实欺诈数据集上的实验结果证明了我们方法的有效性及其执行精确解的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing feature filters rely on statistical pair-wise dependence metrics tomodel feature-target relationships, but this approach may fail when the targetdepends on higher-order feature interactions rather than individualcontributions. We introduce Mutual Information Neural Estimation RegularizedVetting Algorithm (MINERVA), a novel approach to supervised feature selectionbased on neural estimation of mutual information between features and targets.We paramaterize the approximation of mutual information with neural networksand perform feature selection using a carefully designed loss functionaugmented with sparsity-inducing regularizers. Our method is implemented in atwo-stage process to decouple representation learning from feature selection,ensuring better generalization and a more accurate expression of featureimportance. We present examples of ubiquitous dependency structures that arerarely captured in literature and show that our proposed method effectivelycaptures these complex feature-target relationships by evaluating featuresubsets as an ensemble. Experimental results on synthetic and real-life frauddatasets demonstrate the efficacy of our method and its ability to performexact solutions.</description>
      <author>example@mail.com (Taurai Muvunzaa, Egor Kraev, Pere Planell-Morell, Alexander Y. Shestopaloff)</author>
      <guid isPermaLink="false">2510.02610v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Latent Multi-view Learning for Robust Environmental Sound Representations</title>
      <link>http://arxiv.org/abs/2510.02500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to DCASE 2025 Workshop. 4+1 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多视图学习框架，将对比原则整合到生成流程中，以捕捉声音源和设备信息，提高环境声音表征学习效果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)方法如对比方法和生成方法已使用未标记数据推动了环境声音表征学习的发展，但这些方法如何在统一框架中互补仍然相对未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个多视图学习框架，整合对比原则与生成方法，以更有效地捕捉声音源和设备信息。&lt;h4&gt;方法&lt;/h4&gt;该方法将压缩的音频潜在编码为视图特定和视图共享的子空间，由两个自监督目标引导：子空间间的对比学习以实现有针对性的信息流，以及整体信息保存的重建。&lt;h4&gt;主要发现&lt;/h4&gt;在城市声音传感器网络数据集上的评估表明，该方法在声音源和传感器分类任务中展现出优于传统SSL技术的下游性能；此外，该模型在变化训练配置下具有在结构化潜在空间中解离环境声音属性的潜力。&lt;h4&gt;结论&lt;/h4&gt;多视图学习框架结合对比学习和生成方法能够更好地捕捉声音源和设备信息，提高下游性能，并可能解离环境声音属性。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)方法，如对比方法和生成方法，已经使用未标记数据推动了环境声音表征学习的发展。然而，这些方法如何在统一框架中互补仍然相对未被探索。在这项工作中，我们提出了一种多视图学习框架，将对比原则整合到生成流程中以捕捉声音源和设备信息。我们的方法将压缩的音频潜在编码为视图特定和视图共享的子空间，由两个自监督目标引导：子空间之间的对比学习以实现有针对性的信息流，以及整体信息保存的重建。我们在城市声音传感器网络数据集上评估了我们的方法，用于声音源和传感器分类，展示了与传统SSL技术相比改进的下游性能。此外，我们还研究了模型在变化训练配置下在结构化潜在空间中解离环境声音属性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) approaches, such as contrastive and generativemethods, have advanced environmental sound representation learning usingunlabeled data. However, how these approaches can complement each other withina unified framework remains relatively underexplored. In this work, we proposea multi-view learning framework that integrates contrastive principles into agenerative pipeline to capture sound source and device information. Our methodencodes compressed audio latents into view-specific and view-common subspaces,guided by two self-supervised objectives: contrastive learning for targetedinformation flow between subspaces, and reconstruction for overall informationpreservation. We evaluate our method on an urban sound sensor network datasetfor sound source and sensor classification, demonstrating improved downstreamperformance over traditional SSL techniques. Additionally, we investigate themodel's potential to disentangle environmental sound attributes within thestructured latent space under varied training configurations.</description>
      <author>example@mail.com (Sivan Sing, Julia Wilkins, Magdalena Fuentes, Juan Pablo Bello)</author>
      <guid isPermaLink="false">2510.02500v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Interact in World Latent for Team Coordination</title>
      <link>http://arxiv.org/abs/2509.25550v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Web: https://dongsuleetech.github.io/projects/IWoL/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了交互世界潜变量(IWoL)这一新型表示学习框架，用于促进多智能体强化学习(MARL)中的团队协调。该框架通过直接建模通信协议，构建能够同时捕获智能体间关系和任务特定世界信息的可学习表示空间，实现完全去中心化的执行和隐式协调，同时避免显式消息传递的固有缺点。&lt;h4&gt;背景&lt;/h4&gt;在多智能体强化学习中，为团队协调构建有效表示具有挑战性，这源于多智能体交互产生的复杂动态以及由局部观察引起的不完整信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种表示学习框架，有效解决多智能体强化学习中的团队协调问题，同时避免显式消息传递的缺点。&lt;h4&gt;方法&lt;/h4&gt;构建可学习的表示空间，通过直接建模通信协议，联合捕获智能体间关系和任务特定的世界信息。这种表示既可以作为每个智能体的隐式潜变量，也可以作为通信的显式消息。&lt;h4&gt;主要发现&lt;/h4&gt;在四个具有挑战性的MARL基准测试中，评估了两种变体，结果表明IWoL为团队协调提供了简单而强大的关键。此外，该表示可以与现有MARL算法结合使用，进一步提高性能。&lt;h4&gt;结论&lt;/h4&gt;IWoL框架有效解决了多智能体强化学习中的团队协调问题，通过隐式协调避免了显式消息传递的缺点，并具有良好的适应性和扩展性。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了一种新的表示学习框架——交互世界潜变量(IWoL)，以促进多智能体强化学习(MARL)中的团队协调。为团队协调构建有效的表示是一个具有挑战性的问题，这是由于多智能体交互产生的复杂动态以及由局部观察引起的不完整信息。我们的关键见解是构建一个可学习的表示空间，通过直接建模通信协议，联合捕获智能体间关系和任务特定的世界信息。我们保持完全去中心化的执行和隐式协调，同时避免了显式消息传递的固有缺点，例如决策较慢、易受恶意攻击者攻击以及对带宽限制的敏感性。在实践中，我们的表示不仅可以作为每个智能体的隐式潜变量，也可以作为通信的显式消息。在四个具有挑战性的MARL基准测试中，我们评估了两种变体，并证明IWoL为团队协调提供了一个简单而强大的关键。此外，我们证明了我们的表示可以与现有的MARL算法结合，以进一步提高它们的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents a novel representation learning framework, interactiveworld latent (IWoL), to facilitate team coordination in multi-agentreinforcement learning (MARL). Building effective representation for teamcoordination is a challenging problem, due to the intricate dynamics emergingfrom multi-agent interaction and incomplete information induced by localobservations. Our key insight is to construct a learnable representation spacethat jointly captures inter-agent relations and task-specific world informationby directly modeling communication protocols. This representation, we maintainfully decentralized execution with implicit coordination, all while avoidingthe inherent drawbacks of explicit message passing, e.g., slowerdecision-making, vulnerability to malicious attackers, and sensitivity tobandwidth constraints. In practice, our representation can be used not only asan implicit latent for each agent, but also as an explicit message forcommunication. Across four challenging MARL benchmarks, we evaluate bothvariants and show that IWoL provides a simple yet powerful key for teamcoordination. Moreover, we demonstrate that our representation can be combinedwith existing MARL algorithms to further enhance their performance.</description>
      <author>example@mail.com (Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao)</author>
      <guid isPermaLink="false">2509.25550v3</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2510.03216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 4 tables; Submitted to IEEE Conference for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出Wave-GMS，一种轻量级且高效的多尺度生成模型，用于医学图像分割，可在有限内存的GPU上实现高性能和大批量训练。&lt;h4&gt;背景&lt;/h4&gt;为了在医疗机构公平部署AI工具，需要高性能的深度分割网络，这些网络可以在具有有限内存和大数据批处理规模的低成本GPU上进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级且高效的医学图像分割模型，能够在资源受限的环境中有效运行。&lt;h4&gt;方法&lt;/h4&gt;Wave-GMS模型具有较少的可训练参数，不需要加载内存密集型的预训练视觉基础模型，并支持在内存有限的GPU上使用大数据批处理规模进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公开数据集上进行的实验表明，Wave-GMS实现了最先进的分割性能，具有出色的跨域泛化能力，只需要约260万可训练参数。&lt;h4&gt;结论&lt;/h4&gt;Wave-GMS是一种轻量级且高效的医学图像分割解决方案，能够在有限的计算资源上实现高性能，有助于AI工具在医疗环境中的公平部署。&lt;h4&gt;翻译&lt;/h4&gt;为了在医院和医疗机构公平部署AI工具，我们需要高性能的深度分割网络，这些网络可以在具有有限内存和大数据批处理规模的低成本GPU上进行训练。在这项工作中，我们提出了Wave-GMS，一种用于医学图像分割的轻量级且高效的多尺度生成模型。Wave-GMS的可训练参数数量显著减少，不需要加载内存密集型的预训练视觉基础模型，并支持在内存有限的GPU上使用大数据批处理规模进行训练。我们在四个公开可用的数据集上进行了广泛的实验，证明Wave-GMS实现了最先进的分割性能，具有出色的跨域泛化能力，只需要约260万可训练参数。代码可在https://github.com/ATPLab-LUMS/Wave-GMS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For equitable deployment of AI tools in hospitals and healthcare facilities,we need Deep Segmentation Networks that offer high performance and can betrained on cost-effective GPUs with limited memory and large batch sizes. Inthis work, we propose Wave-GMS, a lightweight and efficient multi-scalegenerative model for medical image segmentation. Wave-GMS has a substantiallysmaller number of trainable parameters, does not require loadingmemory-intensive pretrained vision foundation models, and supports trainingwith large batch sizes on GPUs with limited memory. We conducted extensiveexperiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,and HAM10000), demonstrating that Wave-GMS achieves state-of-the-artsegmentation performance with superior cross-domain generalizability, whilerequiring only ~2.6M trainable parameters. Code is available athttps://github.com/ATPLab-LUMS/Wave-GMS.</description>
      <author>example@mail.com (Talha Ahmed, Nehal Ahmed Shaikh, Hassan Mohy-ud-Din)</author>
      <guid isPermaLink="false">2510.03216v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training</title>
      <link>http://arxiv.org/abs/2510.03189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对交互式3D生物医学图像分割的训练策略，结合动态体积提示生成和内容感知自适应裁剪，优化了图像编码器的使用，解决了单GPU上从顺序细化反馈学习的计算挑战。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型在交互式3D生物医学图像分割中存在局限，要么缺乏体积感知能力，要么交互能力有限，需要能够基于用户提示迭代优化预测的高效模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种训练策略，优化图像编码器的使用，模拟真实用户交互模式，并解决在单GPU上从顺序细化反馈学习的计算挑战。&lt;h4&gt;方法&lt;/h4&gt;提出结合动态体积提示生成与内容感知自适应裁剪的训练策略，使用nnInteractive分割模型的公开可用权重初始化网络。&lt;h4&gt;主要发现&lt;/h4&gt;在'交互式3D生物医学图像分割基础模型'竞赛中，方法表现强劲，平均最终Dice得分为0.6385，归一化表面距离为0.6614，Dice曲线下面积指标为2.4799，NSD曲线下面积指标为2.5671。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在交互式3D生物医学图像分割任务中表现出色，能够有效处理体积数据并响应用户交互。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D生物医学图像分割需要能够基于用户提示迭代优化预测的高效模型。当前的基础模型要么缺乏体积感知能力，要么交互能力有限。我们提出了一种结合动态体积提示生成和内容感知自适应裁剪的训练策略，以优化图像编码器的使用。我们的方法在训练过程中模拟真实的用户交互模式，同时解决了在单GPU上从顺序细化反馈学习的计算挑战。为了高效训练，我们使用nnInteractive分割模型的公开可用权重初始化我们的网络。在'交互式3D生物医学图像分割基础模型'竞赛中的评估显示出强劲的性能，平均最终Dice得分为0.6385，归一化表面距离为0.6614，Dice曲线下面积指标为2.4799，NSD曲线下面积指标为2.5671。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决交互式3D医学图像分割中高效模型训练的问题。当前的基础模型要么缺乏体积感知能力，要么交互能力有限。这个问题在现实中非常重要，因为准确的3D医学图像分割对临床诊断和治疗规划至关重要，而生物医学数据集的复杂性和体积不断增加，需要能够有效整合用户反馈的交互式分割方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：SAM/MedSAM缺乏迭代细化能力；SegVol不支持点击修正；VISTA3D不支持边界框；SAM-Med3D分辨率过低；nnInteractive将边界框限制在2D切片。作者借鉴了nnInteractive的架构，但针对其局限性进行了改进。设计思路是解决交互式模型训练中的循环依赖问题：生成真实训练信号需要迭代预测，而模型又需要从这些自生成交互中学习，为此作者设计了无梯度计算和有梯度计算的两阶段训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是动态提示生成和内容感知自适应裁剪。动态提示生成通过识别预测和真实值之间的最大错误区域来模拟用户点击；内容感知裁剪根据解剖结构大小动态调整处理区域。整体流程：1)采用3D残差编码器U-Net架构；2)将图像和各类提示(边界框、点击、先前分割)作为多通道输入；3)两阶段交互模拟-先无梯度生成点击提示，再有梯度更新模型；4)基于错误分析生成点击位置；5)动态裁剪确保捕获完整结构；6)使用Dice损失和交叉熵的组合损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)动态提示生成策略，模拟真实用户交互模式；2)内容感知动态裁剪，优化视场大小；3)两阶段交互模拟解决训练循环依赖问题。相比之前工作：不同于VISTA3D仅支持点击，支持边界框和迭代细化；不同于SegVol仅支持边界框，支持点击修正；不同于SAM-Med3D低分辨率，支持高精度3D分割；不同于nnInteractive的2D边界框，使用完整3D边界框充分利用体积信息。论文还解决了现有方法不充分利用所有可用提示信息的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种动态提示生成策略，通过模拟真实用户交互和内容感知自适应裁剪，显著提高了交互式3D医学图像分割的准确性和效率，特别是在有边界框初始化的情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D biomedical image segmentation requires efficient models thatcan iteratively refine predictions based on user prompts. Current foundationmodels either lack volumetric awareness or suffer from limited interactivecapabilities. We propose a training strategy that combines dynamic volumetricprompt generation with content-aware adaptive cropping to optimize the use ofthe image encoder. Our method simulates realistic user interaction patternsduring training while addressing the computational challenges of learning fromsequential refinement feedback on a single GPU. For efficient training, weinitialize our network using the publicly available weights from thennInteractive segmentation model. Evaluation on the \textbf{Foundation Modelsfor Interactive 3D Biomedical Image Segmentation} competition demonstratesstrong performance with an average final Dice score of 0.6385, normalizedsurface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)and 2.5671 (NSD).</description>
      <author>example@mail.com (Tidiane Camaret Ndir, Alexander Pfefferle, Robin Tibor Schirrmeister)</author>
      <guid isPermaLink="false">2510.03189v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields</title>
      <link>http://arxiv.org/abs/2510.03104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在辐射场中使用几何基础语义特征的效果，与纯视觉特征进行了比较，并提出了一个名为SPINE的新型辐射场反转框架。&lt;h4&gt;背景&lt;/h4&gt;语义蒸馏在辐射场中已显著推动了开放词汇机器人策略的发展，前人工作已证明视觉语义特征在高斯溅射和神经辐射场中的有效性，但几何基础在蒸馏场中的潜在优势仍不明确。&lt;h4&gt;目的&lt;/h4&gt;研究几何基础语义特征在蒸馏场中的优势，并回答三个关键问题：空间基础是否能产生更高保真度的几何感知语义特征？几何基础是否能改善语义目标定位？几何基础是否能实现更高精度的辐射场反转？&lt;h4&gt;方法&lt;/h4&gt;提出SPINE框架，用于在没有初始猜测的情况下反转辐射场，包含两个核心组件：使用蒸馏语义的粗反转和使用基于光度优化的细反转。&lt;h4&gt;主要发现&lt;/h4&gt;几何基础骨干网络的图像特征包含更精细的结构细节；在语义目标定位任务中没有观察到显著差异；姿态估计精度随着几何基础特征的使用而降低；视觉特征为更广泛的下游任务提供了更大的通用性。&lt;h4&gt;结论&lt;/h4&gt;虽然几何基础特征包含更多几何细节，但视觉-only特征在更广泛的下游任务中具有更好的通用性。未来需要探索有效的几何基础策略，以增强预训练语义特征的通用性和性能。&lt;h4&gt;翻译&lt;/h4&gt;辐射场中的语义蒸馏推动了开放词汇机器人策略的重大进展，如在操作和导航方面，这基于来自大型视觉模型的预训练语义。虽然先前的工作已经证明了视觉语义特征（如DINO和CLIP）在高斯溅射和神经辐射场中的有效性，但在蒸馏场中几何基础的潜在优势仍然是一个开放问题。原则上，视觉-几何特征对于空间任务（如姿态估计）似乎非常有前景，这引发了一个问题：几何基础的语义特征在蒸馏场中是否具有优势？具体来说，我们提出了三个关键问题：首先，空间基础是否能产生更高保真度的几何感知语义特征？我们发现，与对应特征相比，来自几何基础骨干网络的图像特征包含更精细的结构细节。其次，几何基础是否能改善语义目标定位？我们观察到此任务没有显著差异。第三，几何基础是否能实现更高精度的辐射场反转？鉴于先前工作的局限性及其语义集成的缺乏，我们提出了一种新颖的框架SPINE，用于在没有初始猜测的情况下反转辐射场，包含两个核心组件：使用蒸馏语义的粗反转和使用基于光度优化的细反转。令人惊讶的是，我们发现姿态估计精度随着几何基础特征的使用而降低。我们的结果表明，视觉-only特征为更广泛的下游任务提供了更大的通用性，尽管几何基础特征包含更多几何细节。值得注意的是，我们的研究结果强调了未来研究的必要性，即研究有效的几何基础策略，以增强预训练语义特征的通用性和性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要研究在蒸馏辐射场中，基于几何的语义特征(visual-geometry semantic features)与纯视觉语义特征(visual-only semantic features)相比是否具有优势。这个问题在机器人学和计算机视觉领域非常重要，因为辐射场结合预训练语义模型为机器人提供了理解环境的能力，而理解哪种类型的语义特征更适合各种下游任务对于构建更有效的机器人系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有工作主要使用纯视觉语义特征在辐射场中进行语义蒸馏，而基于几何的特征在空间任务中可能更有潜力，但其在蒸馏辐射场中的性能尚不清楚。他们设计了三个关键问题来系统比较这两种特征。该方法借鉴了现有的辐射场表示方法(如NeRF和Gaussian Splatting)、预训练的视觉模型(DINO、CLIP和VGGT)、语义蒸馏技术，以及视角n点(PnP)优化方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是研究视觉几何语义特征在蒸馏辐射场中的相对性能，评估它们在三个关键任务中的表现，并提出一种新的辐射场反转框架。整体流程包括：1)从纯视觉模型和视觉几何模型提取语义特征；2)将语义蒸馏到辐射场中，学习语义场映射3D点到特征空间；3)通过语义内容分析(PCA可视化和GFF指标)、语义目标定位(使用CLIP查询和评估指标)和辐射场反转(SPINE框架)来评估性能；4)在多个数据集上验证不同特征的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统性地比较视觉几何语义特征与纯视觉语义特征在蒸馏辐射场中的性能；2)提出SPINE框架，实现无需初始猜测的辐射场反转；3)引入几何保真度因子(GFF)量化语义特征中的几何内容；4)发现视觉几何特征在大多数下游任务中并不总是优于纯视觉特征。相比之前工作，这项研究首次将几何增强与语义蒸馏结合，提供了对几何增强语义特征在辐射场中实际性能的深入理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这项研究系统性地比较了视觉几何语义特征与纯视觉语义特征在蒸馏辐射场中的性能，发现尽管视觉几何特征包含更丰富的几何细节，但纯视觉特征在大多数下游任务中表现更好，并提出了无需初始猜测的辐射场反转新方法SPINE。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic distillation in radiance fields has spurred significant advances inopen-vocabulary robot policies, e.g., in manipulation and navigation, foundedon pretrained semantics from large vision models. While prior work hasdemonstrated the effectiveness of visual-only semantic features (e.g., DINO andCLIP) in Gaussian Splatting and neural radiance fields, the potential benefitof geometry-grounding in distilled fields remains an open question. Inprinciple, visual-geometry features seem very promising for spatial tasks suchas pose estimation, prompting the question: Do geometry-grounded semanticfeatures offer an edge in distilled fields? Specifically, we ask three criticalquestions: First, does spatial-grounding produce higher-fidelity geometry-awaresemantic features? We find that image features from geometry-grounded backbonescontain finer structural details compared to their counterparts. Secondly, doesgeometry-grounding improve semantic object localization? We observe nosignificant difference in this task. Thirdly, does geometry-grounding enablehigher-accuracy radiance field inversion? Given the limitations of prior workand their lack of semantics integration, we propose a novel framework SPINE forinverting radiance fields without an initial guess, consisting of two corecomponents: coarse inversion using distilled semantics, and fine inversionusing photometric-based optimization. Surprisingly, we find that the poseestimation accuracy decreases with geometry-grounded features. Our resultssuggest that visual-only features offer greater versatility for a broader rangeof downstream tasks, although geometry-grounded features contain more geometricdetail. Notably, our findings underscore the necessity of future research oneffective strategies for geometry-grounding that augment the versatility andperformance of pretrained semantic features.</description>
      <author>example@mail.com (Zhiting Mei, Ola Shorinwa, Anirudha Majumdar)</author>
      <guid isPermaLink="false">2510.03104v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Scalable and Consistent 3D Editing</title>
      <link>http://arxiv.org/abs/2510.02994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的3D编辑框架，通过改进数据集和模型方法解决了现有3D编辑技术的局限性，实现了更快速、精确且一致的3D资产编辑效果。&lt;h4&gt;背景&lt;/h4&gt;3D编辑（修改3D资产几何形状或外观）在沉浸式内容创作、数字娱乐和AR/VR中有广泛应用，但与2D编辑不同，它面临跨视图一致性、结构完整性和细粒度可控性等挑战。现有方法通常速度慢、易产生几何失真，或依赖易出错且不切实际的手动精确3D掩码。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D编辑方法的局限性，提高编辑的速度、质量和可控性，实现实用且可扩展的3D编辑解决方案。&lt;h4&gt;方法&lt;/h4&gt;在数据方面，引入3DEditVerse，最大的成对3D编辑基准，包含116,309个高质量训练对和1,500个精选测试对，通过姿势驱动的几何编辑和基础模型引导的外观编辑构建；在模型方面，提出3DEditFormer，一种3D结构保持的条件Transformer，通过双引导注意力和时间自适应门控增强图像到3D生成，无需辅助3D掩码即可分离可编辑区域与保留结构。&lt;h4&gt;主要发现&lt;/h4&gt;提出的3DEditFormer框架在定量和定性评估上都优于现有最先进的基线方法，实现了更精确、一致的3D编辑效果，同时保持了原始3D资产的结构完整性。&lt;h4&gt;结论&lt;/h4&gt;3DEditFormer和3DEditVerse为3D编辑领域建立了新的实用且可扩展的标准，相关数据集和代码将公开发布，项目地址为https://www.lv-lab.org/3DEditFormer/&lt;h4&gt;翻译&lt;/h4&gt;3D编辑——对3D资产几何形状或外观进行局部修改的任务——在沉浸式内容创作、数字娱乐和AR/VR中有广泛应用。然而，与2D编辑不同，由于需要跨视图一致性、结构完整性和细粒度可控性，3D编辑仍然具有挑战性。现有方法通常速度慢，容易产生几何失真，或依赖于容易出错且不切实际的手动精确3D掩码。为应对这些挑战，我们在数据和模型两方面都进行了创新。在数据方面，我们引入了3DEditVerse，迄今为止最大的成对3D编辑基准，包含116,309个高质量训练对和1,500个精选测试对。通过姿势驱动的几何编辑和基础模型引导的外观编辑的互补流程构建，3DEditVerse确保了编辑的局部性、多视图一致性和语义对齐。在模型方面，我们提出了3DEditFormer，一种3D结构保持的条件Transformer。通过双引导注意力和时间自适应门控增强图像到3D生成，3DEditFormer将可编辑区域与保留结构分离，无需辅助3D掩码即可实现精确且一致的编辑。大量实验证明，我们的框架在定量和定性上都优于最先进的基线，为实用且可扩展的3D编辑建立了新标准。数据集和代码将发布。项目：https://www.lv-lab.org/3DEditFormer/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D编辑的挑战，包括跨视角一致性、结构保真度和细粒度可控性。这个问题在现实中非常重要，因为3D编辑在沉浸式内容创作、数字娱乐和AR/VR等领域有广泛应用。与2D编辑相比，3D编辑更困难，而实用的3D编辑系统可以让用户像使用2D工具一样直观地进行修改，大大提高创作效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D编辑的两个基本瓶颈：缺乏大规模配对3D编辑数据集和难以实现结构保持的编辑。他们创建了3DEditVerse数据集，通过姿态驱动的几何编辑和基础模型引导的外观编辑两个互补管道构建。模型方面，他们基于现有的图像到3D的Trellis模型扩展，设计了3DEditFormer，借鉴了现有基础模型如DeepSeek-R1、Flux、Qwen-VL等，并利用了掩码引导的重绘策略和扩散模型思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过注入源资产的多阶段特征到目标生成中，分离可编辑区域和保持的结构。使用双引导注意力机制（一个关注细粒度结构特征，另一个关注语义过渡特征）和时间自适应门控机制来平衡不同扩散阶段的影响。整体流程包括：使用3DEditVerse数据集训练；从源资产提取细粒度结构和语义过渡特征；通过双引导注意力块将源特征注入目标生成；使用时间自适应门控机制平衡特征影响；最终实现无需手动3D掩码的精确且一致的3D编辑。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 3DEditVerse数据集，最大的配对3D编辑基准；2) 3DEditFormer模型，具有双引导注意力块、多阶段特征提取和时间自适应门控机制；3) 无需辅助3D掩码即可实现精确编辑。相比之前的工作：比SDS方法更快；比多视图编辑方法更好地保持跨视图一致性；比端到端3D生成模型不依赖易出错的3D掩码；比VoxHammer对掩码精度不那么敏感，且在3D指标上平均提高13%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出大规模3DEditVerse数据集和3DEditFormer模型，解决了3D编辑中跨视角一致性和结构保持的挑战，实现了无需辅助3D掩码的精确、一致且结构保持的3D编辑，为实用且可扩展的3D编辑建立了新标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D editing - the task of locally modifying the geometry or appearance of a 3Dasset - has wide applications in immersive content creation, digitalentertainment, and AR/VR. However, unlike 2D editing, it remains challengingdue to the need for cross-view consistency, structural fidelity, andfine-grained controllability. Existing approaches are often slow, prone togeometric distortions, or dependent on manual and accurate 3D masks that areerror-prone and impractical. To address these challenges, we advance both thedata and model fronts. On the data side, we introduce 3DEditVerse, the largestpaired 3D editing benchmark to date, comprising 116,309 high-quality trainingpairs and 1,500 curated test pairs. Built through complementary pipelines ofpose-driven geometric edits and foundation model-guided appearance edits,3DEditVerse ensures edit locality, multi-view consistency, and semanticalignment. On the model side, we propose 3DEditFormer, a3D-structure-preserving conditional transformer. By enhancing image-to-3Dgeneration with dual-guidance attention and time-adaptive gating, 3DEditFormerdisentangles editable regions from preserved structure, enabling precise andconsistent edits without requiring auxiliary 3D masks. Extensive experimentsdemonstrate that our framework outperforms state-of-the-art baselines bothquantitatively and qualitatively, establishing a new standard for practical andscalable 3D editing. Dataset and code will be released. Project:https://www.lv-lab.org/3DEditFormer/</description>
      <author>example@mail.com (Ruihao Xia, Yang Tang, Pan Zhou)</author>
      <guid isPermaLink="false">2510.02994v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Training-Free Out-Of-Distribution Segmentation With Foundation Models</title>
      <link>http://arxiv.org/abs/2510.02909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, 2 tables, ICOMP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型在语义分割中检测分布外(OoD)区域的能力，提出了一种无需训练的简单方法，利用InternImage特征和K-Means聚类识别未知对象，并在基准测试上超越了监督和无监督基线。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶等安全关键应用中，检测语义分割中的未知对象至关重要。大型视觉基础模型如DINOv2、InternImage和CLIP通过提供丰富的特征推动了视觉表征学习，但它们在检测分布外区域的能力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究经过分割数据集微调的基础模型是否能够在没有任何异常监督的情况下，本质地区分分布内(ID)和分布外(OoD)区域。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单、无需训练的方法，利用InternImage主干网络的特征，在原始解码器logits上应用K-Means聚类和置信度阈值来识别OoD聚类。&lt;h4&gt;主要发现&lt;/h4&gt;使用InternImage-L，在RoadAnomaly基准测试上达到50.02的平均精度，在ADE-OoD基准测试上达到48.77的平均精度，超越了多个监督和无监督基线。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，对于需要最少假设或额外数据的通用OoD分割方法来说，这是一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;在语义分割中检测未知对象对于自动驾驶等安全关键应用至关重要。包括DINOv2、InternImage和CLIP在内的大型视觉基础模型，通过提供能很好地跨不同任务泛化的丰富特征，推动了视觉表征学习的发展。虽然它们在闭集语义任务中的优势已经得到确立，但在语义分割中检测分布外(OoD)区域的能力尚未被充分探索。在本工作中，我们研究了经过分割数据集微调的基础模型是否能够在没有任何异常监督的情况下，本质地区分分布内(ID)和分布外(OoD)区域。我们提出了一种简单、无需训练的方法，利用InternImage主干网络的特征，并在原始解码器logits上应用K-Means聚类和置信度阈值来识别OoD聚类。我们的方法在使用InternImage-L的情况下，在RoadAnomaly基准测试上达到50.02的平均精度，在ADE-OoD基准测试上达到48.77的平均精度，超越了多个监督和无监督基线。这些结果表明，对于需要最少假设或额外数据的通用OoD分割方法来说，这是一个有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting unknown objects in semantic segmentation is crucial forsafety-critical applications such as autonomous driving. Large visionfoundation models, including DINOv2, InternImage, and CLIP, have advancedvisual representation learning by providing rich features that generalize wellacross diverse tasks. While their strength in closed-set semantic tasks isestablished, their capability to detect out-of-distribution (OoD) regions insemantic segmentation remains underexplored. In this work, we investigatewhether foundation models fine-tuned on segmentation datasets can inherentlydistinguish in-distribution (ID) from OoD regions without any outliersupervision. We propose a simple, training-free approach that utilizes featuresfrom the InternImage backbone and applies K-Means clustering alongsideconfidence thresholding on raw decoder logits to identify OoD clusters. Ourmethod achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77on the benchmark of ADE-OoD with InternImage-L, surpassing several supervisedand unsupervised baselines. These results suggest a promising direction forgeneric OoD segmentation methods that require minimal assumptions or additionaldata.</description>
      <author>example@mail.com (Laith Nayal, Hadi Salloum, Ahmad Taha, Yaroslav Kholodov, Alexander Gasnikov)</author>
      <guid isPermaLink="false">2510.02909v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions</title>
      <link>http://arxiv.org/abs/2510.02882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文关注地球观测数据在云处理过程中的能源效率问题，指出了当前处理平台中的能源效率差距，并提出了改进的研究方向。&lt;h4&gt;背景&lt;/h4&gt;地球观测数据量快速增长，云计算被用于处理大型数据集，但能源效率方面受到的关注较少。随着大数据处理中能源成本和碳足迹意识的提高，以及计算密集型基础模型的发展，这一问题变得尤为突出。&lt;h4&gt;目的&lt;/h4&gt;识别基于云的地球观测大数据(EOBD)处理中能源效率实践的差距，并提出改进的研究方向。&lt;h4&gt;方法&lt;/h4&gt;检查当前EOBD环境，关注需要基于云处理的要求，分析现有解决方案，研究其他大数据领域成功应用的能源效率策略。&lt;h4&gt;主要发现&lt;/h4&gt;现有EOBD处理平台主要关注数据可访问性和计算可行性而非能源效率；关键差距包括：不足的能源监测机制、数据管理中缺乏能源意识、能源感知资源分配实施不足、任务调度缺乏能源效率标准。&lt;h4&gt;结论&lt;/h4&gt;建议开发能源感知性能监测和基准测试框架，使用优化技术进行基础设施编排，采用能源高效的任务调度方法，以促进EOBD处理中的能源意识，减少能源消耗和环境影响，同时保持处理性能。&lt;h4&gt;翻译&lt;/h4&gt;地球观测数据量正在快速增长。虽然云计算现在被用于处理大型地球观测数据集，但这种处理的能源效率方面受到的关注较少。考虑到大数据处理中能源成本和碳足迹意识的不断提高，特别是在计算密集型基础模型受到更多关注的情况下，这个问题尤为突出。在本文中，我们确定了基于云的地球观测大数据处理中能源效率实践的差距，并提出了几项改进的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Earth observation (EO) data volumes are rapidly increasing. While cloudcomputing are now used for processing large EO datasets, the energy efficiencyaspects of such a processing have received much less attention. This issue isnotable given the increasing awareness of energy costs and carbon footprint inbig data processing, particularly with increased attention on compute-intensivefoundation models. In this paper we identify gaps in energy efficiencypractices within cloud-based EO big data (EOBD) processing and propose severalresearch directions for improvement. We first examine the current EOBDlandscape, focus on the requirements that necessitate cloud-based processingand analyze existing cloud-based EOBD solutions. We then investigate energyefficiency strategies that have been successfully employed in well-studied bigdata domains. Through this analysis, we identify several critical gaps inexisting EOBD processing platforms, which primarily focus on data accessibilityand computational feasibility, instead of energy efficiency. These gaps includeinsufficient energy monitoring mechanisms, lack of energy awareness in datamanagement, inadequate implementation of energy-aware resource allocation andlack of energy efficiency criteria on task scheduling. Based on these findings,we propose the development of energy-aware performance monitoring andbenchmarking frameworks, the use of optimization techniques for infrastructureorchestration, and of energy-efficient task scheduling approaches fordistributed cloud-based EOBD processing frameworks. These proposed approachesaim to foster more energy awareness in EOBD processing , potentially reducingpower consumption and environmental impact while maintaining or minimallyimpacting processing performance.</description>
      <author>example@mail.com (Adhitya Bhawiyuga, Serkan Girgin, Rolf A. de By, Raul Zurita-Milla)</author>
      <guid isPermaLink="false">2510.02882v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.02732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种运动自适应框架，用于解决从单目视频中动态3D重建的挑战，通过使控制点密度与运动复杂度相匹配来提高重建质量和效率。&lt;h4&gt;背景&lt;/h4&gt;从单目视频中进行动态3D重建仍然困难，原因是从有限视角推断3D运动存在歧义，以及对时变场景建模的计算需求大。&lt;h4&gt;目的&lt;/h4&gt;开发一个运动自适应框架，使控制点密度与运动复杂度相匹配，解决现有方法中静态冗余和动态不足的问题。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型的语义和运动先验，建立补丁-标记-节点对应关系，应用运动自适应压缩将控制点集中在动态区域同时抑制静态背景冗余，通过迭代体素化和运动倾向评分实现灵活的表示密度适应，并引入基于样条的轨迹参数化替代基于MLP的变形场。&lt;h4&gt;主要发现&lt;/h4&gt;该方法直接解决了控制点分配与运动复杂度之间的根本不匹配问题，能够更有效地分配计算资源到动态区域。&lt;h4&gt;结论&lt;/h4&gt;大量实验表明，与现有最先进的方法相比，该方法在重建质量和效率方面都有显著改进。&lt;h4&gt;翻译&lt;/h4&gt;从单目视频进行动态3D重建仍然困难，这是由于从有限视角推断3D运动存在歧义以及对时变场景建模的计算需求。虽然最近的稀疏控制方法通过将数百万个高斯函数减少到数千个控制点来减轻计算负担，但它们存在一个关键局限：它们完全基于几何分配点，导致静态冗余和动态不足。我们提出了一种运动自适应框架，使控制密度与运动复杂度相匹配。利用视觉基础模型的语义和运动先验，我们建立了补丁-标记-节点对应关系，并应用运动自适应压缩，将控制点集中在动态区域，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动倾向评分实现了灵活的表示密度适应，直接解决了控制点分配与运动复杂度之间的根本不匹配问题。为了捕捉时间演化，我们引入了基于样条的轨迹参数化，由2D轨迹初始化，替代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。大量实验证明了与现有最先进方法相比，在重建质量和效率方面的显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态3D重建中的两个核心问题：从有限视角推断3D运动的不确定性，以及建模时变场景的高计算需求。具体来说，现有稀疏控制方法仅基于几何分配控制点，导致静态区域冗余而动态区域表示不足。这个问题在虚拟现实、自主系统和内容创建等领域至关重要，因为这些应用需要从有限视角捕捉复杂物体运动和变形，同时保持实时渲染性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有稀疏控制方法的局限性：控制点分配不均衡，浪费在静态区域而无法充分表示动态区域。然后提出利用语义和运动先验来指导控制点分配的思路，使控制密度与运动复杂度相匹配。作者借鉴了3D高斯泼溅技术、视觉基础模型用于语义提取、节点表示方法控制场景变形、样条参数化表示轨迹以及双四元数混合技术进行变形。这些现有技术被创新性地组合，解决了传统方法中的静态冗余和动态不足问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用语义和运动先验指导控制点分配，使控制点密度与运动复杂度相匹配，在动态区域放置更多控制点，在静态区域减少冗余。整体实现流程分为五个阶段：1) 基础阶段从单目视频中提取语义和运动先验；2) 运动自适应节点初始化，通过补丁到节点的生成和运动自适应压缩创建优化的节点集；3) 样条参数化节点轨迹，使用三次Hermite样条表示并从2D轨迹初始化；4) 高斯到节点变形，通过双四元数混合将节点运动传播到高斯点；5) 优化阶段，使用多视图约束进行联合优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三方面：1) 运动自适应节点初始化方法，利用视觉基础模型的语义和运动先验使控制密度与运动复杂度匹配；2) 基于样条的节点轨迹参数化，提供紧凑、平滑且可微的运动基础；3) 完整的优化框架，实现比现有方法更优的重建质量和效率。相比之前的工作，不同之处在于：传统方法(如SC-GS、4D-Scaffold)仅基于几何均匀性分配控制点，不考虑运动复杂性；之前的方法使用MLP或网格变形场，而本文使用样条参数化提供更平滑的表示；本文利用视觉基础模型的语义先验指导控制点分配，而之前方法主要依赖几何信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于语义引导的运动自适应框架，通过视觉基础模型的先验知识优化控制点分配，并使用样条参数化实现更稳定、高效的动态3D高斯泼溅重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic 3D reconstruction from monocular videos remains difficult due to theambiguity inferring 3D motion from limited views and computational demands ofmodeling temporally varying scenes. While recent sparse control methodsalleviate computation by reducing millions of Gaussians to thousands of controlpoints, they suffer from a critical limitation: they allocate points purely bygeometry, leading to static redundancy and dynamic insufficiency. We propose amotion-adaptive framework that aligns control density with motion complexity.Leveraging semantic and motion priors from vision foundation models, weestablish patch-token-node correspondences and apply motion-adaptivecompression to concentrate control points in dynamic regions while suppressingredundancy in static backgrounds. Our approach achieves flexiblerepresentational density adaptation through iterative voxelization and motiontendency scoring, directly addressing the fundamental mismatch between controlpoint allocation and motion complexity. To capture temporal evolution, weintroduce spline-based trajectory parameterization initialized by 2D tracklets,replacing MLP-based deformation fields to achieve smoother motionrepresentation and more stable optimization. Extensive experiments demonstratesignificant improvements in reconstruction quality and efficiency over existingstate-of-the-art methods.</description>
      <author>example@mail.com (Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang)</author>
      <guid isPermaLink="false">2510.02732v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems</title>
      <link>http://arxiv.org/abs/2510.02668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgenticRAG是一个结合工具增强基础模型和检索增强生成的新框架，用于零样本可解释推荐。它通过整合外部工具调用、知识检索和思维链推理，实现无需任务特定训练的透明决策，实验证明其在多个数据集上优于现有方法，同时保持可解释性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;基础模型已经彻底改变了人工智能领域，但在推荐系统中的应用受到推理不透明性和知识限制的约束。&lt;h4&gt;目的&lt;/h4&gt;介绍AgenticRAG框架，结合工具增强的基础模型和检索增强生成，实现零样本可解释推荐。&lt;h4&gt;方法&lt;/h4&gt;整合外部工具调用、知识检索和思维链推理，创建能够进行透明决策的自主推荐代理，无需任务特定训练。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实验结果表明，AgenticRAG实现了持续改进：Amazon Electronics上NDCG@10提高0.4%，MovieLens-1M上提高0.8%，Yelp数据集上提高1.6%。框架展现出卓越的可解释性，同时保持与传统方法相当的计算效率。&lt;h4&gt;结论&lt;/h4&gt;AgenticRAG框架解决了基础模型在推荐系统中应用的两个主要限制：推理不透明性和知识约束，为推荐系统提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已经彻底改变了人工智能，然而它们在推荐系统中的应用仍然受到推理不透明性和知识限制的制约。本文介绍了AgenticRAG，一个结合工具增强基础模型和检索增强生成的新框架，用于零样本可解释推荐。我们的方法整合了外部工具调用、知识检索和思维链推理，创建了无需任务特定训练即可进行透明决策的自主推荐代理。在三个真实世界数据集上的实验结果表明，AgenticRAG与最先进的基线相比实现了持续改进，在Amazon Electronics上NDCG@10提高了0.4%，在MovieLens-1M上提高了0.8%，在Yelp数据集上提高了1.6%。该框架展现出卓越的可解释性，同时保持与传统方法相当的计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have revolutionized artificial intelligence, yet theirapplication in recommender systems remains limited by reasoning opacity andknowledge constraints. This paper introduces AgenticRAG, a novel framework thatcombines tool-augmented foundation models with retrieval-augmented generationfor zero-shot explainable recommendations. Our approach integrates externaltool invocation, knowledge retrieval, and chain-of-thought reasoning to createautonomous recommendation agents capable of transparent decision-making withouttask-specific training. Experimental results on three real-world datasetsdemonstrate that AgenticRAG achieves consistent improvements overstate-of-the-art baselines, with NDCG@10 improvements of 0.4\% on AmazonElectronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The frameworkexhibits superior explainability while maintaining computational efficiencycomparable to traditional methods.</description>
      <author>example@mail.com (Bo Ma, Hang Li, ZeHua Hu, XiaoFan Gui, LuYao Liu, Simon Liu)</author>
      <guid isPermaLink="false">2510.02668v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer</title>
      <link>http://arxiv.org/abs/2510.02625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabImpute是一种基于预训练transformer的零样本插补方法，解决了表格数据中缺失数据处理的挑战，它不需要在推理时进行拟合或超参数调整，且速度快、准确性高。&lt;h4&gt;背景&lt;/h4&gt;缺失数据是表格设置中的一个普遍问题。现有的解决方案从简单平均到复杂的生成对抗网络不等。然而，由于在实际领域中性能差异很大，且超参数调整耗时，目前没有默认的插补方法。&lt;h4&gt;目的&lt;/h4&gt;基于TabPFN（一种用于监督学习的表格基础模型）提出TabImpute，开发一个预训练的transformer模型，提供准确且快速的零样本插补，无需在推理时进行拟合或超参数调整。&lt;h4&gt;方法&lt;/h4&gt;提出TabImpute，一种预训练的transformer模型；引入(i) 表格设置中的逐要素特征化，比之前的TabPFN插补方法快100倍；(ii) 合成训练数据生成管道，融入真实的缺失模式，提高测试时性能；(iii) MissBench，一个包含42个OpenML数据集和13种缺失模式的插补方法综合评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;TabImpute在医学、金融和工程等多个领域展现了强大的性能，与11种成熟的插补方法相比表现良好。&lt;h4&gt;结论&lt;/h4&gt;TabImpute提供了一种准确、快速且无需超参数调整的缺失数据插补解决方案，通过MissBench基准测试验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;缺失数据是表格设置中的一个普遍问题。现有的解决方案从简单平均到复杂的生成对抗网络不等。然而，由于在实际领域中性能差异很大，且超参数调整耗时，目前没有默认的插补方法。基于TabPFN（一种用于监督学习的表格基础模型），我们提出了TabImpute，一种预训练的transformer模型，能够提供准确且快速的零样本插补，无需在推理时进行拟合或超参数调整。为了训练和评估TabImpute，我们引入了(i) 表格设置中的逐要素特征化，比之前的TabPFN插补方法快100倍，(ii) 合成训练数据生成管道，融入真实的缺失模式，提高测试时性能，以及(iii) MissBench，一个包含42个OpenML数据集和13种缺失模式的插补方法综合评估基准。MissBench涵盖医学、金融和工程等领域，展示了TabImpute与11种成熟插补方法相比的强大性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Missing data is a pervasive problem in tabular settings. Existing solutionsrange from simple averaging to complex generative adversarial networks.However, due to huge variance in performance across real-world domains andtime-consuming hyperparameter tuning, no default imputation method exists.Building on TabPFN, a recent tabular foundation model for supervised learning,we propose TabImpute, a pre-trained transformer that delivers accurate and fastzero-shot imputations requiring no fitting or hyperparameter tuning atinference-time. To train and evaluate TabImpute, we introduce (i) an entry-wisefeaturization for tabular settings, which enables a $100\times$ speedup overthe previous TabPFN imputation method, (ii) a synthetic training datageneration pipeline incorporating realistic missingness patterns, which booststest-time performance, and (iii) MissBench, a comprehensive benchmark forevaluation of imputation methods with $42$ OpenML datasets and $13$ missingnesspatterns. MissBench spans domains such as medicine, finance, and engineering,showcasing TabImpute's robust performance compared to $11$ establishedimputation methods.</description>
      <author>example@mail.com (Jacob Feitelberg, Dwaipayan Saha, Kyuseong Choi, Zaid Ahmad, Anish Agarwal, Raaz Dwivedi)</author>
      <guid isPermaLink="false">2510.02625v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Modal Imbalance in Multimodal Reasoning</title>
      <link>http://arxiv.org/abs/2510.02608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 10 figures, CoLM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了基础模型在处理跨模态冲突时的表现，发现模型在单模态上下文中能较好识别冲突，但在跨模态上下文中表现极差，原因是存在跨模态注意力不平衡问题。通过改进训练方法，可以显著提高模型在跨模态任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;基础模型在真实世界任务（如计算机使用代理）中需要整合多种模态，特别是在模态间相互作用形成跨模态上下文时。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型在跨模态冲突场景下的表现，探究模型是优先考虑某一模态还是联合推理解决冲突。&lt;h4&gt;方法&lt;/h4&gt;通过实验研究基础模型在跨模态冲突（不同模态间存在矛盾证据）场景下的表现，分析注意力机制，并提出改进的训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在单模态上下文中能90%时间识别冲突，但在跨模态上下文中识别率降至3%；跨语言上下文中也有类似现象；失败原因是跨模态注意力不平衡，模型过度优先考虑某些模态；仅扩大数据集无法解决问题，但通过在每个训练实例中明确组合多种模态可以显著减少注意力不平衡并提高下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;系统地解决跨模态上下文对构建可靠的基础模型至关重要。&lt;h4&gt;翻译&lt;/h4&gt;部署在计算机使用代理等真实世界任务中的基础模型必须整合多种模态。基础模型在执行联合推理（同时推理多种模态）方面的表现如何，特别是在模态相互作用并形成跨模态上下文时？为更好地理解这一问题，我们研究了基础模型在跨模态冲突场景下的表现：不同模态呈现矛盾证据的情况。这使我们能够检验基础模型是优先考虑某一模态还是联合推理来解决冲突。我们的实验显示，基础模型在单模态上下文中能90%时间识别冲突，但当证据分散在不同模态时，这一比例低至3%——在由多种语言组成的跨语言上下文中也有类似观察。我们将这一失败归因于跨模态注意力不平衡，表明基础模型在注意力分数上表现出极端不对称性，过度优先考虑某些模态。我们证明，仅盲目扩大多模态或多语言数据集无法解决跨模态注意力不平衡问题，因为它们缺乏明确要求跨模态推理的训练样本。我们证明，即使在每个训练实例中明确组合多种模态的简单可扩展方法也能显著减少注意力不平衡。减少注意力不平衡直接转化为多个视觉语言基准测试的下游性能提升。我们的发现强调了系统解决跨模态上下文对构建可靠基础模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) deployed in real-world tasks such as computer-useagents must integrate diverse modalities. How good are FMs at performing jointreasoning, simultaneously reasoning over multiple modalities, especially whenthe modalities interact and relate to each other to form cross-modal context?To better understand this problem, we study FMs on cross-modal conflicts:scenarios where conflicting evidence is presented across modalities. Thisallows us to examine whether FMs prioritize one modality over another or reasonjointly to reconcile the conflict. Our experiments reveal that FMs canrecognize conflicts in unimodal contexts, composed of a single modality, 90% ofthe time, but the ratio falls as low as 3% when evidence is split acrossmodalities -- similar observations hold in cross-lingual contexts, composed ofmultiple languages. We trace this failure to cross-modal attention imbalance,showing that FMs exhibit extreme asymmetry in attention scores,disproportionately prioritizing certain modalities. We show that cross-modalattention imbalance does not go away by simply scaling up multimodal ormultilingual datasets blindly, since they lack training examples thatexplicitly require cross-modal reasoning. We demonstrate that even a simple andscalable method of explicitly combining multiple modalities within eachtraining instance significantly reduces attention imbalance. Reduced attentionimbalance directly translates to improved downstream performance on severalvision-language benchmarks. Our findings underscore the importance ofsystematically addressing cross-modal contexts to build reliable foundationmodels.</description>
      <author>example@mail.com (Chen Henry Wu, Neil Kale, Aditi Raghunathan)</author>
      <guid isPermaLink="false">2510.02608v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction</title>
      <link>http://arxiv.org/abs/2510.02578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Flowr.root是一个等变流匹配模型，用于口袋感知的3D配体生成，具有结合亲和力预测和置信度估计功能。它支持从头生成、药效团条件采样、片段修饰和多端点亲和力预测。该模型结合大规模配体库和混合保真度的蛋白质-配体复合物进行训练，并在精选共晶数据集上精炼。&lt;h4&gt;背景&lt;/h4&gt;基于结构的药物设计需要能够生成高质量配体并预测其结合亲和力的工具，以加速从命中识别到先导优化的过程。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行3D配体生成并预测结合亲和力的模型，为基于结构的药物设计提供全面基础。&lt;h4&gt;方法&lt;/h4&gt;使用等变流匹配技术构建Flowr.root模型，结合大规模配体库和混合保真度的蛋白质-配体复合物进行训练，并在精选共晶数据集上精炼。使用参数高效微调进行项目特定适应。&lt;h4&gt;主要发现&lt;/h4&gt;Flowr.root在无条件3D分子生成和口袋条件配体设计中达到最先进性能，产生几何上真实的低应变结构。其集成亲和力预测模块在SPINDR测试集上表现出卓越准确性，在Schrodinger FEP+/OpenFE基准测试中优于最近模型。案例研究显示预测与量子力学计算结果具有强相关性。&lt;h4&gt;结论&lt;/h4&gt;Flowr.root通过整合结构感知生成、亲和力估计和属性引导采样，为基于结构的药物设计提供了全面基础，涵盖从命中识别到先导优化的整个过程。作为基础模型，它需要在项目特定数据集上进行微调以应对未见过的结构-活性景观。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Flowr.root，一个用于口袋感知3D配体生成的等变流匹配模型，具有结合亲和力预测和置信度估计功能。该模型支持从头生成、药效团条件采样、片段修饰和多端点亲和力预测。训练结合大规模配体库和混合保真度的蛋白质-配体复合物，随后在精选的共晶数据集上进行精炼，并通过参数高效微调进行项目特定适应。Flowr.root在无条件3D分子生成和口袋条件配体设计中达到最先进性能，产生几何上真实的低应变结构。集成的亲和力预测模块在SPINDR测试集上表现出卓越的准确性，并在Schrodinger FEP+/OpenFE基准测试中优于最近的模型，具有显著的速度优势。作为基础模型，Flowr.root需要在项目特定数据集上进行微调以应对未见过的结构-活性景观，与实验数据产生强相关性。联合生成和亲和力预测通过重要性采样实现推理时扩展，引导分子设计朝向更高亲和力的化合物。案例研究验证了这一点：针对CLK3的选择性CK2alpha配体生成显示预测与量子力学结合能之间的显著相关性，而ERalpha和TYK2骨架修饰与量子力学计算显示出强一致性。通过整合结构感知生成、亲和力估计和属性引导采样，Flowr.root为基于结构的药物设计提供了全面基础，涵盖从命中识别到先导优化的整个过程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Flowr.root, an equivariant flow-matching model for pocket-aware 3Dligand generation with joint binding affinity prediction and confidenceestimation. The model supports de novo generation, pharmacophore-conditionalsampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,pKi, pKd, pEC50). Training combines large-scale ligand libraries withmixed-fidelity protein-ligand complexes, followed by refinement on curatedco-crystal datasets and parameter-efficient finetuning for project-specificadaptation. Flowr.root achieves state-of-the-art performance in unconditional3D molecule generation and pocket-conditional ligand design, producinggeometrically realistic, low-strain structures. The integrated affinityprediction module demonstrates superior accuracy on the SPINDR test set andoutperforms recent models on the Schrodinger FEP+/OpenFE benchmark withsubstantial speed advantages. As a foundation model, Flowr.root requiresfinetuning on project-specific datasets to account for unseenstructure-activity landscapes, yielding strong correlation with experimentaldata. Joint generation and affinity prediction enable inference-time scalingthrough importance sampling, steering molecular design toward higher-affinitycompounds. Case studies validate this: selective CK2alpha ligand generationagainst CLK3 shows significant correlation between predicted andquantum-mechanical binding energies, while ERalpha and TYK2 scaffoldelaboration demonstrates strong agreement with QM calculations. By integratingstructure-aware generation, affinity estimation, and property-guided sampling,Flowr.root provides a comprehensive foundation for structure-based drug designspanning hit identification through lead optimization.</description>
      <author>example@mail.com (Julian Cremer, Tuan Le, Mohammad M. Ghahremanpour, Emilia Sługocka, Filipe Menezes, Djork-Arné Clevert)</author>
      <guid isPermaLink="false">2510.02578v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Geospatial Machine Learning Libraries</title>
      <link>http://arxiv.org/abs/2510.02572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Book chapter&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了地理空间机器学习(GeoML)库的全面概述，分析了它们的演变、核心功能和当前生态系统，介绍了流行库如TorchGeo、eo-learn和Raster Vision的架构和数据类型支持，讨论了数据预处理等方法论，并通过作物类型映射案例研究展示了实际应用，同时提出了最佳实践和未来方向。&lt;h4&gt;背景&lt;/h4&gt;机器学习的进步得益于特定领域软件库的出现，但地理空间机器学习领域面临地球观测数据可用性超过处理其独特挑战的领域库发展的问题，这些挑战包括变化的空间分辨率、光谱特性、时间频率、数据覆盖范围、坐标系统和文件格式。&lt;h4&gt;目的&lt;/h4&gt;提供GeoML库的全面概述，分析它们的演变、核心功能和当前生态系统，介绍流行的GeoML库及其架构、数据类型支持和ML框架集成，讨论数据预处理等方法论，通过案例研究展示实际应用，强调最佳实践，并讨论开放挑战和未来方向，特别是基础模型和开源地理空间软件治理。&lt;h4&gt;方法&lt;/h4&gt;分析GeoML库的演变、核心功能和生态系统；介绍并详细说明流行的GeoML库的架构、支持的数据类型和与ML框架的集成；讨论数据预处理、时空连接、基准测试和预训练模型使用的方法；通过作物类型映射的案例研究展示实际应用。&lt;h4&gt;主要发现&lt;/h4&gt;存在多种流行的GeoML库，如TorchGeo、eo-learn和Raster Vision，它们具有不同的架构和数据类型支持；数据预处理、时空连接、基准测试和预训练模型使用是GeoML中的常见方法；作物类型映射是GeoML的一个实际应用案例；软件设计、许可和测试有最佳实践可遵循；基础模型的兴起和开源地理空间软件治理是未来的重要方向。&lt;h4&gt;结论&lt;/h4&gt;本文旨在指导从业者、开发者和研究人员导航和贡献于快速发展的GeoML领域。&lt;h4&gt;翻译&lt;/h4&gt;最近的机器学习进展得益于特定领域软件库的出现，使工作流程更加高效且提高了可重复性。对于地理空间机器学习(GeoML)，地球观测数据的可用性已经超过了处理其独特挑战的领域库的发展，如变化的空间分辨率、光谱特性、时间频率、数据覆盖范围、坐标系统和文件格式。本章提供了GeoML库的全面概述，分析了它们的演变、核心功能和当前生态系统。它还介绍了流行的GeoML库，如TorchGeo、eo-learn和Raster Vision，详细说明了它们的架构、支持的数据类型以及与ML框架的集成。此外，它讨论了数据预处理、时空连接、基准测试和使用预训练模型的常见方法。通过作物类型映射的案例研究，它展示了这些工具的实际应用。软件设计、许可和测试的最佳实践被强调，同时还有开放挑战和未来方向，特别是基础模型的兴起和开源地理空间软件治理的需求。我们的目标是指导从业者、开发者和研究人员导航并贡献于快速发展的GeoML领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in machine learning have been supported by the emergence ofdomain-specific software libraries, enabling streamlined workflows andincreased reproducibility. For geospatial machine learning (GeoML), theavailability of Earth observation data has outpaced the development of domainlibraries to handle its unique challenges, such as varying spatial resolutions,spectral properties, temporal cadence, data coverage, coordinate systems, andfile formats. This chapter presents a comprehensive overview of GeoMLlibraries, analyzing their evolution, core functionalities, and the currentecosystem. It also introduces popular GeoML libraries such as TorchGeo,eo-learn, and Raster Vision, detailing their architecture, supported datatypes, and integration with ML frameworks. Additionally, it discusses commonmethodologies for data preprocessing, spatial--temporal joins, benchmarking,and the use of pretrained models. Through a case study in crop type mapping, itdemonstrates practical applications of these tools. Best practices in softwaredesign, licensing, and testing are highlighted, along with open challenges andfuture directions, particularly the rise of foundation models and the need forgovernance in open-source geospatial software. Our aim is to guidepractitioners, developers, and researchers in navigating and contributing tothe rapidly evolving GeoML landscape.</description>
      <author>example@mail.com (Adam J. Stewart, Caleb Robinson, Arindam Banerjee)</author>
      <guid isPermaLink="false">2510.02572v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction</title>
      <link>http://arxiv.org/abs/2510.02476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 workshop: 2nd Workshop on Multi-modal Foundation Models  and Large Language Models for Life Sciences&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了基于不确定性的模型选择策略在生物分子功效预测中的应用，发现使用简单序列特征的TabPFN模型可以超越专业预测器，且模型的不确定性可作为无需标签的启发式方法优化预测性能。&lt;h4&gt;背景&lt;/h4&gt;上下文学习器如TabPFN在生物分子功效预测方面前景广阔，其中已建立的分子特征集和相关实验结果可作为有力的上下文示例。然而，这些学习器的性能对提供的上下文高度敏感，导致在不同数据子集上训练的模型的后集成成为一种可行方法。&lt;h4&gt;目的&lt;/h4&gt;研究在没有真实标签的情况下，如何选择最佳模型进行集成，特别关注基于不确定性的模型选择策略。&lt;h4&gt;方法&lt;/h4&gt;在siRNA敲低功效任务上测试TabPFN模型，使用简单的基于序列的特征，评估模型预测的IQR（不确定性的度量）与真实预测误差的关系，并通过选择和平均具有最低平均IQR的模型进行集成。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用简单序列特征的TabPFN模型超越了专业的最先进预测器；2) 模型预测的IQR与真实预测误差呈负相关；3) 通过选择最低平均IQR的模型集成，比简单集成或使用在所有可用数据上训练的单个模型性能更好。&lt;h4&gt;结论&lt;/h4&gt;模型不确定性是优化生物分子功效预测的一个强大、无需标签的启发式方法，为模型选择提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;上下文学习器如TabPFN在生物分子功效预测方面很有前景，其中已建立的分子特征集和相关实验结果可以作为有力的上下文示例。然而，它们的性能对提供的上下文高度敏感，使得在不同数据子集上训练的模型的后集成成为一种可行方法。一个开放问题是，在没有真实标签的情况下如何为集成选择最佳模型。在本研究中，我们研究了一种基于不确定性的模型选择策略。我们在siRNA敲低功效任务上证明，使用简单序列特征的TabPFN模型可以超越专业的最先进预测器。我们还表明，模型预测的四分位距，即其不确定性的度量，与真实预测误差呈负相关。通过选择并平均具有最低平均四分位距的模型集成，我们实现了比简单集成或使用在所有可用数据上训练的单个模型更好的性能。这一发现突显了模型不确定性作为优化生物分子功效预测的强大、无需标签的启发式方法的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context learners like TabPFN are promising for biomolecule efficacyprediction, where established molecular feature sets and relevant experimentalresults can serve as powerful contextual examples. However, their performanceis highly sensitive to the provided context, making strategies like post-hocensembling of models trained on different data subsets a viable approach. Anopen question is how to select the best models for the ensemble without accessto ground truth labels. In this study, we investigate an uncertainty-guidedstrategy for model selection. We demonstrate on an siRNA knockdown efficacytask that a TabPFN model using simple sequence-based features can surpassspecialized state-of-the-art predictors. We also show that the model'spredicted inter-quantile range (IQR), a measure of its uncertainty, has anegative correlation with true prediction error. By selecting and averaging anensemble of models with the lowest mean IQR, we achieve superior performancecompared to naive ensembling or using a single model trained on all availabledata. This finding highlights model uncertainty as a powerful, label-freeheuristic for optimizing biomolecule efficacy predictions.</description>
      <author>example@mail.com (Jie Li, Andrew McCarthy, Zhizhuo Zhang, Stephen Young)</author>
      <guid isPermaLink="false">2510.02476v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models</title>
      <link>http://arxiv.org/abs/2510.02453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了顾问模型（Advisor Models），一种通过强化学习训练的轻量级参数化策略，用于动态优化黑盒基础模型的行为，克服了静态提示优化的局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型越来越多地作为黑盒服务部署，模型权重无法修改，定制仅限于提示工程。静态提示优化虽然显示出前景，但产生的是单一固定提示，无法适应不同的输入、用户或环境。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态适应不同输入、用户和环境的方法，用于优化黑盒基础模型的行为。&lt;h4&gt;方法&lt;/h4&gt;引入顾问模型，这是一种轻量化的参数化策略，通过强化学习训练，能够针对黑盒模型反应性地发出自然语言引导指令。顾问模型作为第二模型位于输入和目标模型之间，基于环境的奖励信号逐个实例地塑造行为。&lt;h4&gt;主要发现&lt;/h4&gt;顾问模型在多个涉及推理和个性化的领域超越了静态提示优化器，能够发现环境动态并改进下游任务性能。顾问模型可以跨黑盒模型迁移，显示出良好的泛化能力，同时能够实现专业化并保持对分布外输入的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过顾问模型对黑盒模型进行动态优化是实现个性化和环境适应性AI的有前景方向，能够为黑盒系统提供可学习的接口，顾问模型作为参数化、环境特定的记忆发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;基础模型越来越多地作为黑盒服务部署，其中模型权重无法修改，定制仅限于提示工程。虽然静态提示优化显示出前景，但它产生的单一固定提示无法适应不同的输入、用户或环境。我们引入了顾问模型，这是一种通过强化学习训练的轻量级参数化策略，能够针对黑盒模型反应性地发出自然语言引导指令。顾问是一个小型第二模型，位于输入和模型之间，使用来自环境的奖励信号逐个实例地塑造行为。在多个涉及推理和个性化的领域，我们表明顾问模型优于静态提示优化器，能够发现环境动态并改进下游任务性能。我们还通过跨黑盒模型迁移顾问模型证明了其泛化能力，以及框架在保持对分布外输入鲁棒性的同时实现专业化的能力。更广泛地看，顾问模型为黑盒系统提供了可学习的接口，其中顾问作为参数化、环境特定的记忆发挥作用。我们认为，通过顾问模型对黑盒模型进行动态优化是实现具有前沿能力的个性化和环境适应性AI的一个有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are increasingly deployed as black-box services, wheremodel weights cannot be modified and customization is limited to prompting.While static prompt optimization has shown promise, it produces a single fixedprompt that fails to adapt to different inputs, users, or environments. Weintroduce Advisor Models, lightweight parametric policies trained withreinforcement learning to reactively issue natural language steeringinstructions in-context to black-box models. The advisor is a second smallmodel that sits between the input and the model, shaping behavior on aper-instance basis using reward signals from the environment. Across multipledomains involving reasoning and personalization, we show that Advisor Modelsoutperform static prompt optimizers, discovering environment dynamics andimproving downstream task performance. We also demonstrate the generalizabilityof advisors by transferring them across black-box models, as well as theframework's ability to achieve specialization while retaining robustness toout-of-distribution inputs. Viewed more broadly, Advisor Models provide alearnable interface to black-box systems where the advisor acts as aparametric, environment-specific memory. We argue that dynamic optimization ofblack-box models via Advisor Models is a promising direction for enablingpersonalization and environment-adaptable AI with frontier-level capabilities.</description>
      <author>example@mail.com (Parth Asawa, Alan Zhu, Matei Zaharia, Alexandros G. Dimakis, Joseph E. Gonzalez)</author>
      <guid isPermaLink="false">2510.02453v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.02084v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了KAIROS，一个非自回归时间序列预测框架，直接对段级多峰分布进行建模，避免了误差累积并实现及时推理，在六个基准测试上表现出强大的零样本泛化能力，以较低推理成本提供与最先进模型相当的预测性能。&lt;h4&gt;背景&lt;/h4&gt;在万维网中，可靠的时间序列预测提供前瞻性信号，驱动资源规划、缓存放置和异常响应，使平台能够随着用户行为和内容分布的演变而高效运行。与其他领域相比，Web应用的时间序列预测需要更快的响应速度以支持实时决策。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够快速响应的时间序列预测框架，以满足Web应用中实时决策的需求，同时避免自回归方法中的误差累积问题。&lt;h4&gt;方法&lt;/h4&gt;提出KAIROS，一个非自回归时间序列预测框架，直接对段级多峰分布进行建模。与自回归方法不同，KAIROS避免了误差累积，实现了及时推理，同时改进了现有的非自回归模型，防止它们退化为过度平滑的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在大型语料库上训练后，KAIROS在六个广泛使用的基准测试上表现出强大的零样本泛化能力，以远低于最先进基础模型的推理成本提供了可比较的预测性能。&lt;h4&gt;结论&lt;/h4&gt;KAIROS强调了非自回归设计作为时间序列基础模型可扩展范式的重要性。&lt;h4&gt;翻译&lt;/h4&gt;在万维网中，可靠的时间序列预测提供了前瞻性信号，驱动资源规划、缓存放置和异常响应，使平台能够随着用户行为和内容分布的演变而高效运行。与其他领域相比，Web应用的时间序列预测需要更快的响应速度以支持实时决策。我们提出了KAIROS，一个非自回归时间序列预测框架，直接对段级多峰分布进行建模。与自回归方法不同，KAIROS避免了误差累积，实现了及时推理，同时改进了现有的非自回归模型，防止它们退化为过度平滑的预测。在大型语料库上训练后，KAIROS在六个广泛使用的基准测试上表现出强大的零样本泛化能力，以远低于最先进基础模型的推理成本提供了可比较的预测性能。除了实证结果外，KAIROS还强调了非自回归设计作为时间序列基础模型可扩展范式的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the World Wide Web, reliable time series forecasts provide theforward-looking signals that drive resource planning, cache placement, andanomaly response, enabling platforms to operate efficiently as user behaviorand content distributions evolve. Compared with other domains, time seriesforecasting for Web applications requires much faster responsiveness to supportreal-time decision making. We present KAIROS, a non-autoregressive time seriesforecasting framework that directly models segment-level multi-peakdistributions. Unlike autoregressive approaches, KAIROS avoids erroraccumulation and achieves just-in-time inference, while improving over existingnon-autoregressive models that collapse to over-smoothed predictions. Trainedon the large-scale corpus, KAIROS demonstrates strong zero-shot generalizationon six widely used benchmarks, delivering forecasting performance comparable tostate-of-the-art foundation models with similar scale, at a fraction of theirinference cost. Beyond empirical results, KAIROS highlights the importance ofnon-autoregressive design as a scalable paradigm for foundation models in timeseries.</description>
      <author>example@mail.com (Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan)</author>
      <guid isPermaLink="false">2510.02084v2</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition</title>
      <link>http://arxiv.org/abs/2510.03066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InsideOut框架，一个基于EfficientNetV2-S的可重现面部情感识别系统，通过迁移学习、强数据增强和不平衡感知优化，在FER2013数据集上实现了62.8%的准确率和0.590的宏平均F1分数。&lt;h4&gt;背景&lt;/h4&gt;面部情感识别是情感计算中的关键任务，应用于人机交互、电子学习、医疗和安全系统。尽管深度学习有所进步，FER仍面临遮挡、光照和姿势变化、类内差异细微以及数据不平衡等挑战，这些因素阻碍了对少数情感的识别。&lt;h4&gt;目的&lt;/h4&gt;开发一个可重现的FER框架，解决数据不平衡问题，并在FER2013数据集上实现有竞争力的性能。&lt;h4&gt;方法&lt;/h4&gt;构建基于EfficientNetV2-S的框架，采用迁移学习、强数据增强和不平衡感知优化技术。标准化FER2013图像，应用分层分割和增强，使用类别加权损失微调轻量级分类头以处理偏斜分布。&lt;h4&gt;主要发现&lt;/h4&gt;InsideOut在FER2013上达到62.8%的准确率和0.590的宏平均F1分数，与传统CNN基线相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;高效架构与定制的不平衡处理相结合可以提供实用、透明和可重现的FER解决方案。&lt;h4&gt;翻译&lt;/h4&gt;面部情感识别是情感计算中的关键任务，使人机交互、电子学习、医疗和安全系统的应用成为可能。尽管深度学习有所进步，FER仍然面临挑战，包括遮挡、光照和姿势变化、细微的类内差异以及数据不平衡，这些因素阻碍了对少数情感的识别。我们提出了InsideOut，这是一个基于EfficientNetV2-S的可重现FER框架，采用迁移学习、强数据增强和不平衡感知优化构建。该方法标准化FER2013图像，应用分层分割和增强，并使用类别加权损失微调轻量级分类头以解决偏斜分布问题。InsideOut在FER2013上实现了62.8%的准确率和0.590的宏平均F1分数，显示出与传统CNN基线相比具有竞争力的结果。新颖之处在于证明了高效架构与定制的不平衡处理相结合可以提供实用、透明和可重现的FER解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial Emotion Recognition (FER) is a key task in affective computing,enabling applications in human-computer interaction, e-learning, healthcare,and safety systems. Despite advances in deep learning, FER remains challengingdue to occlusions, illumination and pose variations, subtle intra-classdifferences, and dataset imbalance that hinders recognition of minorityemotions. We present InsideOut, a reproducible FER framework built onEfficientNetV2-S with transfer learning, strong data augmentation, andimbalance-aware optimization. The approach standardizes FER2013 images, appliesstratified splitting and augmentation, and fine-tunes a lightweightclassification head with class-weighted loss to address skewed distributions.InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,showing competitive results compared to conventional CNN baselines. The noveltylies in demonstrating that efficient architectures, combined with tailoredimbalance handling, can provide practical, transparent, and reproducible FERsolutions.</description>
      <author>example@mail.com (Ahsan Farabi, Israt Khandaker, Ibrahim Khalil Shanto, Md Abdul Ahad Minhaz, Tanisha Zaman)</author>
      <guid isPermaLink="false">2510.03066v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Quantum Geometry for Fractional Chern Insulators with unsupervised learning</title>
      <link>http://arxiv.org/abs/2510.03018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一种无监督机器学习框架，通过单粒子形状因子的分布直接建模相互作用的哈密顿量，成功区分并生成了具有不同拓扑特性的分数量子霍尔绝缘体态&lt;h4&gt;背景&lt;/h4&gt;莫尔材料中的分数量子霍尔绝缘体为探索强关联拓扑相提供了独特平台，但现实莫尔模型缺乏量子度量和贝里曲率的直接可调性，限制了理论和数值探索&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习方法来建模相互作用哈密顿量，克服传统方法在理想量子几何方面的局限性&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器(VAE)进行无监督学习，分析单粒子形状因子的分布，并结合主成分分析(PCA)揭示形状因子中的主导模式&lt;h4&gt;主要发现&lt;/h4&gt;无监督学习能区分FCI和非FCI态；能生成训练集中不存在的新形状因子；能生成和插值具有陈数|C|=1的拓扑平带形状因子；能发现如电荷密度波等未观测到的多体态；形状因子中的主导模式可分解为具有近似量化陈数的分量&lt;h4&gt;结论&lt;/h4&gt;机器学习能够泛化和建模拓扑量子系统，为设计具有定制量子几何和平带材料中多体相的形状因子开辟了新途径&lt;h4&gt;翻译&lt;/h4&gt;莫尔材料中的分数量子霍尔绝缘体(FCIs)为探索超越理想量子几何范式的强关联拓扑相提供了一个独特平台。虽然FCIs和分数量子霍尔态(FQHS)的解析方法通常依赖于理想的布洛赫波函数，但现实的莫尔模型缺乏量子度量和贝里曲率的直接可调性，限制了理论和数值探索。在这里，我们引入了一个无监督机器学习框架，通过单粒子形状因子的分布直接建模相互作用的哈密顿量。使用变分自编码器(VAE)，我们证明无监督学习不仅能区分FCI和非FCI态，还能生成训练集中不存在的新形状因子，具有不同的拓扑特性。这个潜在空间使得能够生成和插值具有陈数|C|=1的拓扑平带形状因子，能够发现如电荷密度波等未观测到的多体态。主成分分析(PCA)进一步揭示了形状因子中的主导模式——反映了布里渊区内的关联性——可以被分解为具有近似量化陈数的分量，为量子几何的全局和拓扑结构提供了新的见解。我们的结果强调了机器学习泛化和建模拓扑量子系统的能力，为设计具有定制量子几何和平带材料中多体相的形状因子开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fractional Chern insulators (FCIs) in moire materials present a uniqueplatform for exploring strongly correlated topological phases beyond theparadigm of ideal quantum geometry. While analytical approaches to FCIs andfractional quantum Hall states (FQHS) often rely on idealized Blochwavefunctions, realistic moire models lack direct tunability of quantum metricand Berry curvature, limiting theoretical and numerical exploration. Here, weintroduce an unsupervised machine learning framework to model interactingHamiltonians directly through the distribution of single-particle form factors.Using a variational autoencoder (VAE), we show that unsupervised learning cannot only distinguish FCI and non-FCI states, but also generate new form factorswith distinct topological character, not present in the training set. Thislatent space enables the generation and interpolation of form factors fortopological flatbands with Chern number $|C|=1$, enabling the discovery ofunobserved many-body states such as charge density waves. Principal componentanalysis (PCA) further reveals that the dominant patterns in the formfactors-reflecting correlations across the Brillouin zone-can be decomposedinto components with approximately quantized Chern numbers, providing newinsights into the global and topological structure of quantum geometry. Ourresults highlight the ability of machine learning to generalize and modeltopological quantum systems, paving the way for the inverse design of formfactors with tailored quantum geometry and many-body phases in flatbandmaterials.</description>
      <author>example@mail.com (Ang-Kun Wu, Louis Primeau, Jingtao Zhang, Kai Sun, Yang Zhang, Shi-Zeng Lin)</author>
      <guid isPermaLink="false">2510.03018v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime</title>
      <link>http://arxiv.org/abs/2510.03003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Keywords: transfer learning, shaft power prediction, noon reports,  sensor data, maritime&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的方法来预测船舶轴功率，通过结合高频传感器数据和低频日间报告数据，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;全球海运运输的增长使得能源优化对降低成本和确保运营效率变得至关重要。轴功率作为影响燃料消耗的关键因素，其准确预测对优化船舶性能具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确预测船舶轴功率的方法，以优化船舶性能和降低燃料消耗。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于迁移学习的轴功率预测方法，模型首先在高频数据上进行训练，然后使用来自其他船舶的低频日间报告进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;与仅使用日间报告数据训练的模型相比，该方法在不同类型船舶上的平均绝对百分比误差均有降低：姊妹船舶降低10.6%，相似船舶降低3.6%，不同船舶降低5.3%。&lt;h4&gt;结论&lt;/h4&gt;基于迁移学习的方法能有效预测船舶轴功率，特别是在具有相似配置的姊妹船舶上表现最佳。&lt;h4&gt;翻译&lt;/h4&gt;随着全球海运运输的增长，能源优化对于降低成本和确保运营效率已变得至关重要。轴功率是从发动机传递到轴的机械功率，直接影响燃料消耗，因此其准确预测是优化船舶性能的关键一步。功率消耗与船舶参数（如速度和每分钟轴转速）以及天气和海况高度相关。频繁获取这些运营数据可以提高预测准确性。然而，获取高质量传感器数据通常不可行且成本高昂，使得日间报告等替代来源成为可行选择。在本文中，我们提出了一种基于迁移学习的船舶轴功率预测方法，模型首先在一艘船舶的高频数据上进行初始训练，然后使用其他船舶的低频日间报告进行微调。我们在姊妹船舶（相同尺寸和配置）、相似船舶（稍大且发动机不同）和不同船舶（不同尺寸和配置）上测试了我们的方法。实验表明，与仅使用日间报告数据训练的模型相比，姊妹船舶的平均绝对百分比误差降低了10.6%，相似船舶降低了3.6%，不同船舶降低了5.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growth of global maritime transportation, energy optimization hasbecome crucial for reducing costs and ensuring operational efficiency. Shaftpower is the mechanical power transmitted from the engine to the shaft anddirectly impacts fuel consumption, making its accurate prediction a paramountstep in optimizing vessel performance. Power consumption is highly correlatedwith ship parameters such as speed and shaft rotation per minute, as well asweather and sea conditions. Frequent access to this operational data canimprove prediction accuracy. However, obtaining high-quality sensor data isoften infeasible and costly, making alternative sources such as noon reports aviable option. In this paper, we propose a transfer learning-based approach forpredicting vessels shaft power, where a model is initially trained onhigh-frequency data from a vessel and then fine-tuned with low-frequency dailynoon reports from other vessels. We tested our approach on sister vessels(identical dimensions and configurations), a similar vessel (slightly largerwith a different engine), and a different vessel (distinct dimensions andconfigurations). The experiments showed that the mean absolute percentage errordecreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,and 5.3 percent for a different vessel, compared to the model trained solely onnoon report data.</description>
      <author>example@mail.com (Akriti Sharma, Dogan Altan, Dusica Marijan, Arnbjørn Maressa)</author>
      <guid isPermaLink="false">2510.03003v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology</title>
      <link>http://arxiv.org/abs/2510.02760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HGCD-BT的新型脑肿瘤分类方法，结合层次聚类与对比学习，能够同时识别已知和未知类别的肿瘤类型。&lt;h4&gt;背景&lt;/h4&gt;准确的脑肿瘤分类对神经肿瘤手术中的术中决策至关重要，但现有方法局限于预定义类别，无法捕捉训练时未提供的肿瘤类型模式。无监督学习缺乏整合标记数据先验知识的能力，而半监督方法通常假设所有潜在类别都包含在标记数据中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够反映脑肿瘤分类层次结构的方法，解决现有分类方法无法识别未知类别的问题。&lt;h4&gt;方法&lt;/h4&gt;引入Hierarchical Generalized Category Discovery for Brain Tumor Classification (HGCD-BT)，这是一种结合层次聚类与对比学习的方法，通过扩展基于对比学习的GCD方法，纳入新的半监督层次聚类损失。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenSRH数据集上评估，HGCD-BT与最先进的GCD方法相比，在补丁级别分类上准确率提高了28%，特别是在识别先前未见过的肿瘤类别方面表现突出。此外，该方法在Digital Brain Tumor Atlas的苏木精-伊红染色全幻灯片图像的幻灯片级别分类上也展示了良好的可推广性。&lt;h4&gt;结论&lt;/h4&gt;HGCD-BT是一种创新的脑肿瘤分类方法，能够处理已知和未知类别，并在不同数据集和成像模式下都表现出良好的性能，为脑肿瘤分类提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的脑肿瘤分类对于神经肿瘤手术中的术中决策至关重要。然而，现有方法局限于预定义的固定类别集合，因此无法捕捉训练时未提供的肿瘤类型模式。无监督学习可以提取通用特征，但缺乏整合标记数据先验知识的能力，而半监督方法通常假设所有潜在类别都包含在标记数据中。广义类别发现(GCD)旨在通过标记未标记数据中的已知和未知类别来弥合这一差距。为了反映脑肿瘤分类法的层次结构，在本文中，我们引入了用于脑肿瘤分类的层次广义类别发现(HGCD-BT)，这是一种结合层次聚类与对比学习的新方法。我们的方法通过纳入新颖的半监督层次聚类损失，扩展了基于对比学习的GCD方法。我们在OpenSRH(一个模拟拉曼组织学脑肿瘤图像数据集)上评估了HGCD-BT，与最先进的GCD方法相比，在补丁级别分类上实现了28%的准确率提升，特别是在识别先前未见过的肿瘤类别方面。此外，我们在数字脑肿瘤图谱的苏木精和伊红染色全幻灯片图像的幻灯片级别分类上证明了HGCD-BT的可推广性，确认了其在不同成像模式下的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate brain tumor classification is critical for intra-operative decisionmaking in neuro-oncological surgery. However, existing approaches arerestricted to a fixed set of predefined classes and are therefore unable tocapture patterns of tumor types not available during training. Unsupervisedlearning can extract general-purpose features, but it lacks the ability toincorporate prior knowledge from labelled data, and semi-supervised methodsoften assume that all potential classes are represented in the labelled data.Generalized Category Discovery (GCD) aims to bridge this gap by categorizingboth known and unknown classes within unlabelled data. To reflect thehierarchical structure of brain tumor taxonomies, in this work, we introduceHierarchical Generalized Category Discovery for Brain Tumor Classification(HGCD-BT), a novel approach that integrates hierarchical clustering withcontrastive learning. Our method extends contrastive learning based GCD byincorporating a novel semi-supervised hierarchical clustering loss. We evaluateHGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,achieving a +28% improvement in accuracy over state-of-the-art GCD methods forpatch-level classification, particularly in identifying previously unseen tumorcategories. Furthermore, we demonstrate the generalizability of HGCD-BT onslide-level classification of hematoxylin and eosin stained whole-slide imagesfrom the Digital Brain Tumor Atlas, confirming its utility across imagingmodalities.</description>
      <author>example@mail.com (Matthias Perkonigg, Patrick Rockenschaub, Georg Göbel, Adelheid Wöhrer)</author>
      <guid isPermaLink="false">2510.02760v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</title>
      <link>http://arxiv.org/abs/2510.02778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了AdaRD-Key，一个无需训练的关键帧采样模块，用于查询驱动的长视频理解。它结合了查询相关性和视觉多样性，能够在长视频中高效选择信息丰富且不冗余的帧，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态大语言模型（MLLMs）在理解长视频时面临挑战，因为长视频具有广泛的时域长度和高信息密度。现有的关键帧选择方法存在局限性：均匀采样方法忽略关键时刻；严格时域间隔方法错过重要事件附近的精细线索；强调视觉多样性的方法则忽略查询相关性。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的关键帧采样模块，能够在长视频中智能选择与查询相关且具有视觉多样性的帧，提高视觉-语言模型对长视频的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出AdaRD-Key，一个无需训练的关键帧采样模块，最大化'相关性-多样性最大体积'（RD-MV）目标，结合查询条件的相关性评分和对数行列式多样性组件。采用轻量级相关性感知门控机制，当相关性分布弱时自动切换到纯多样性模式。该方法计算效率高，可在单个GPU上实时运行，并能与现有VLMs即插即用。&lt;h4&gt;主要发现&lt;/h4&gt;在LongVideoBench和Video-MME上的大量实验表明，AdaRD-Key在长视频理解任务上实现了最先进的性能。该方法能够在无需额外训练的情况下，有效平衡相关性和多样性，提高视频理解的准确性。&lt;h4&gt;结论&lt;/h4&gt;AdaRD-Key为长视频理解提供了一个高效、无需训练的解决方案，通过智能选择关键帧，显著提升了视觉-语言模型对长视频的理解能力，同时保持了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;理解长视频对于视觉-语言模型（VLMs）来说仍然是一个重大挑战，因为它们具有广泛的时域长度和高信息密度。大多数当前的多模态大语言模型（MLLMs）依赖于均匀采样，这常常忽略关键时刻，导致对查询的错误回答。同时，许多关键帧选择方法施加严格的时域间隔：一旦选择了一个帧，就会抑制相邻时间戳以减少冗余。虽然这种方法在限制重叠方面有效，但它经常错过重要事件附近的短时精细线索。其他方法则强调视觉多样性而忽略查询相关性。我们提出了AdaRD-Key，一个用于查询驱动的长视频理解的无训练关键帧采样模块。AdaRD-Key最大化了一个统一的'相关性-多样性最大体积'（RD-MV）目标，结合了查询条件的相关性评分和对数行列式多样性组件，以产生信息丰富且不冗余的帧。为了处理与视频对齐性弱的广泛查询，AdaRD-Key采用了一个轻量级的相关性感知门控机制；当相关性分布表明对齐性弱时，该方法无缝切换到纯多样性模式，无需额外监督即可提高覆盖率。我们的流水线无需训练，计算效率高（可在单个GPU上实时运行），并且可以即插即式地与现有VLMs兼容。在LongVideoBench和Video-MME上的大量实验展示了最先进的性能，特别是在长视频方面。代码可在https://github.com/Xian867/AdaRD-Key获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding long-form videos remains a significant challenge forvision--language models (VLMs) due to their extensive temporal length and highinformation density. Most current multimodal large language models (MLLMs) relyon uniform sampling, which often overlooks critical moments, leading toincorrect responses to queries. In parallel, many keyframe selection approachesimpose rigid temporal spacing: once a frame is chosen, an exclusion windowsuppresses adjacent timestamps to reduce redundancy. While effective atlimiting overlap, this strategy frequently misses short, fine-grained cues nearimportant events. Other methods instead emphasize visual diversity but neglectquery relevance. We propose AdaRD-Key, a training-free keyframe sampling modulefor query-driven long-form video understanding. AdaRD-Key maximizes a unifiedRelevance--Diversity Max-Volume (RD-MV) objective, combining aquery-conditioned relevance score with a log-determinant diversity component toyield informative yet non-redundant frames. To handle broad queries with weakalignment to the video, AdaRD-Key employs a lightweight relevance-aware gatingmechanism; when the relevance distribution indicates weak alignment, the methodseamlessly shifts into a diversity-only mode, enhancing coverage withoutadditional supervision. Our pipeline is training-free, computationallyefficient (running in real time on a single GPU), and compatible with existingVLMs in a plug-and-play manner. Extensive experiments on LongVideoBench andVideo-MME demonstrate state-of-the-art performance, particularly on long-formvideos. Code available at https://github.com/Xian867/AdaRD-Key.</description>
      <author>example@mail.com (Xian Zhang, Zexi Wu, Zinuo Li, Hongming Xu, Luqi Gong, Farid Boussaid, Naoufel Werghi, Mohammed Bennamoun)</author>
      <guid isPermaLink="false">2510.02778v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2510.00572v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IntrusionX是一种混合深度学习框架，通过结合CNN和LSTM网络，并使用松鼠搜索算法优化，解决了入侵检测系统面临的网络攻击演变、高维流量数据和类别不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;入侵检测系统面临持续挑战，包括不断演变的网络攻击、高维流量数据和基准数据集(如NSL-KDD)中严重的类别不平衡问题。&lt;h4&gt;目的&lt;/h4&gt;解决入侵检测系统面临的挑战，提高检测性能，特别是对稀有类别的检测能力。&lt;h4&gt;方法&lt;/h4&gt;提出IntrusionX混合深度学习框架，整合CNN用于局部特征提取和LSTM用于时序建模，使用松鼠搜索算法进行架构优化和超参数调优，并采用严格的预处理、分层数据分割和动态类别加权技术。&lt;h4&gt;主要发现&lt;/h4&gt;在NSL-KDD数据集上，IntrusionX在二元分类中达到98%的准确率，在5类分类中达到87%的准确率，少数类别的召回率显著提升(U2R: 71%，R2L: 93%)。&lt;h4&gt;结论&lt;/h4&gt;IntrusionX的创新点在于其可复制、不平衡感知的设计与元启发式优化相结合，有效解决了入侵检测系统中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;入侵检测系统由于不断演变的网络攻击、高维流量数据以及像NSL-KDD这样的基准数据集中的严重类别不平衡而面临持续挑战。为解决这些问题，我们提出了IntrusionX，一种混合深度学习框架，整合了用于局部特征提取的卷积神经网络和用于时序建模的长短期记忆网络。该架构使用松鼠搜索算法进一步优化，实现了有效的超参数调优同时保持计算效率。我们的流程包含严格的预处理、分层数据分割和动态类别加权，以增强稀有类别的检测。在NSL-KDD上的实验评估表明，IntrusionX在二元分类中达到98%的准确率，在5类分类中达到87%的准确率，少数类别的召回率有显著提升(U2R: 71%，R2L: 93%)。IntrusionX的创新点在于其可复制、不平衡感知的设计与元启发式优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intrusion Detection Systems (IDS) face persistent challenges due to evolvingcyberattacks, high-dimensional traffic data, and severe class imbalance inbenchmark datasets such as NSL-KDD. To address these issues, we proposeIntrusionX, a hybrid deep learning framework that integrates ConvolutionalNeural Networks (CNNs) for local feature extraction and Long Short-Term Memory(LSTM) networks for temporal modeling. The architecture is further optimizedusing the Squirrel Search Algorithm (SSA), enabling effective hyperparametertuning while maintaining computational efficiency. Our pipeline incorporatesrigorous preprocessing, stratified data splitting, and dynamic class weightingto enhance the detection of rare classes. Experimental evaluation on NSL-KDDdemonstrates that IntrusionX achieves 98% accuracy in binary classification and87% in 5-class classification, with significant improvements in minority classrecall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in itsreproducible, imbalance-aware design with metaheuristic optimization.</description>
      <author>example@mail.com (Ahsan Farabi, Muhaiminul Rashid Shad, Israt Khandaker)</author>
      <guid isPermaLink="false">2510.00572v2</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments</title>
      <link>http://arxiv.org/abs/2510.02788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 EMNLP Findings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了XTRA，一种新的跨语言主题建模框架，通过结合词袋建模和多语言嵌入，实现了主题和表示的双重对齐，显著提高了主题的连贯性、多样性和跨语言一致性。&lt;h4&gt;背景&lt;/h4&gt;跨语言主题建模旨在揭示不同语言之间的共享语义主题。虽然已有方法在主题多样性方面取得了一定进展，但往往难以确保高主题连贯性和跨语言一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时保证主题可解释性（连贯性和多样性）和跨语言良好对齐的跨语言主题建模方法。&lt;h4&gt;方法&lt;/h4&gt;提出XTRA框架，统一了词袋建模与多语言嵌入，包含两个核心组件：(1) 表示对齐：通过对比学习在共享语义空间中对齐文档-主题分布；(2) 主题对齐：将主题-词分布投影到同一空间以强制跨语言一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在多语料库上的实验表明，XTRA在主题连贯性、多样性和对齐质量方面显著优于强基线方法。&lt;h4&gt;结论&lt;/h4&gt;XTRA的双重机制能够学习到可解释且跨语言良好对齐的主题。&lt;h4&gt;翻译&lt;/h4&gt;跨语言主题建模旨在揭示不同语言之间的共享语义主题。已有多种方法解决这个问题，利用传统和神经方法。虽然先前方法在主题多样性方面取得了一些改进，但它们往往难以确保高主题连贯性和跨语言一致性。我们提出XTRA（跨语言主题建模与主题和表示对齐），一种统一词袋建模与多语言嵌入的新框架。XTRA引入两个核心组件：(1) 表示对齐，通过在共享语义空间中的对比学习对齐文档-主题分布；(2) 主题对齐，将主题-词分布投影到同一空间以强制跨语言一致性。这种双重机制使XTRA能够学习到可解释（连贯且多样）且跨语言良好对齐的主题。多语料库上的实验证实，XTRA在主题连贯性、多样性和对齐质量方面显著优于强基线方法。代码和可重现脚本可在https://github.com/tienphat140205/XTRA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-lingual topic modeling aims to uncover shared semantic themes acrosslanguages. Several methods have been proposed to address this problem,leveraging both traditional and neural approaches. While previous methods haveachieved some improvements in topic diversity, they often struggle to ensurehigh topic coherence and consistent alignment across languages. We propose XTRA(Cross-Lingual Topic Modeling with Topic and Representation Alignments), anovel framework that unifies Bag-of-Words modeling with multilingualembeddings. XTRA introduces two core components: (1) representation alignment,aligning document-topic distributions via contrastive learning in a sharedsemantic space; and (2) topic alignment, projecting topic-word distributionsinto the same space to enforce crosslingual consistency. This dual mechanismenables XTRA to learn topics that are interpretable (coherent and diverse) andwell-aligned across languages. Experiments on multilingual corpora confirm thatXTRA significantly outperforms strong baselines in topic coherence, diversity,and alignment quality. Code and reproducible scripts are available at https://github.com/tienphat140205/XTRA.</description>
      <author>example@mail.com (Tien Phat Nguyen, Vu Minh Ngo, Tung Nguyen, Linh Van Ngo, Duc Anh Nguyen, Sang Dinh, Trung Le)</author>
      <guid isPermaLink="false">2510.02788v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context</title>
      <link>http://arxiv.org/abs/2510.02742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一个基于对比学习编码器的评估框架和针对印度文化背景的新数据集IndiCASA，评估发现开源LLMs存在不同程度的刻板印象偏见，特别是与残疾相关的偏见，强调了开发更公平模型的必要性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)因其出色的上下文理解和生成能力而在关键领域获得广泛应用，这些模型越来越多地应用于高风险场景，需要严格评估其嵌入的偏见。在印度等文化多样性的背景下，现有的基于嵌入的偏见评估方法往往难以捕捉细微的刻板印象。&lt;h4&gt;目的&lt;/h4&gt;提出一个评估框架，用于捕捉大型语言模型中的细微偏见，并创建一个专门针对印度文化背景的新数据集。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于使用对比学习训练的编码器的评估框架，通过嵌入相似性来捕捉细粒度的偏见；引入了名为IndiCASA的新数据集，包含2575个人类验证的句子，涵盖五个人口统计维度：种姓、性别、宗教、残疾和社会经济地位。&lt;h4&gt;主要发现&lt;/h4&gt;对多个开源LLMs的评估显示，所有模型都表现出一定程度的刻板印象偏见，与残疾相关的偏见尤为明显且持续存在，宗教偏见普遍较低，可能是由于全球去偏见努力的结果。&lt;h4&gt;结论&lt;/h4&gt;研究表明需要开发更公平的模型，提出的框架和数据集有助于更准确地评估和减少模型偏见。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)因其出色的上下文理解和生成能力而在关键领域获得了显著的关注。然而，它们在高风险应用中的日益部署需要对嵌入偏见进行严格评估，特别是在像印度这样的文化多样性背景下，现有的基于嵌入的偏见评估方法往往难以捕捉细微的刻板印象。我们提出了一个基于使用对比学习训练的编码器的评估框架，通过嵌入相似性捕捉细粒度的偏见。我们还引入了一个名为IndiCASA(基于印度偏见的上下文对齐的刻板印象和反刻板印象)的新数据集，包含2575个人类验证的句子，涵盖五个人口统计维度：种姓、性别、宗教、残疾和社会经济地位。我们对多个开源LLMs的评估显示，所有模型都表现出一定程度的刻板印象偏见，与残疾相关的偏见尤为明显且持续存在，宗教偏见普遍较低，可能是由于全球去偏见努力的结果，这展示了开发更公平模型的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have gained significant traction across criticaldomains owing to their impressive contextual understanding and generativecapabilities. However, their increasing deployment in high stakes applicationsnecessitates rigorous evaluation of embedded biases, particularly in culturallydiverse contexts like India where existing embedding-based bias assessmentmethods often fall short in capturing nuanced stereotypes. We propose anevaluation framework based on a encoder trained using contrastive learning thatcaptures fine-grained bias through embedding similarity. We also introduce anovel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes andAnti-stereotypes) comprising 2,575 human-validated sentences spanning fivedemographic axes: caste, gender, religion, disability, and socioeconomicstatus. Our evaluation of multiple open-weight LLMs reveals that all modelsexhibit some degree of stereotypical bias, with disability related biases beingnotably persistent, and religion bias generally lower likely due to globaldebiasing efforts demonstrating the need for fairer model development.</description>
      <author>example@mail.com (Santhosh G S, Akshay Govind S, Gokul S Krishnan, Balaraman Ravindran, Sriraam Natarajan)</author>
      <guid isPermaLink="false">2510.02742v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.02484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为动作可控因子化(ACF)的对比学习方法，用于解决从高维观测中学习因子化表示的问题，结合了因子化MDP的样本效率和深度强化学习处理高维输入的能力。&lt;h4&gt;背景&lt;/h4&gt;基于因子化马尔可夫决策过程的算法比因子无关方法更具有样本效率，但它们假设因子化表示是预先已知的，这在智能体只能看到高维观测时失效；而深度强化学习能处理高维输入但无法利用因子化结构。&lt;h4&gt;目的&lt;/h4&gt;解决表示问题，使算法能够从高维观测中学习因子化表示，发现可独立控制的潜在变量。&lt;h4&gt;方法&lt;/h4&gt;提出动作可控因子化(ACF)对比学习方法，利用动作的稀疏性（动作通常只影响变量的一小部分，其余部分在环境动态下演化）为对比训练提供信息数据。&lt;h4&gt;主要发现&lt;/h4&gt;ACF能够直接从像素观测中恢复真实的可控因子，在Taxi、FourRooms和MiniGrid-DoorKey三个基准测试上表现一致优于基解纠缠算法。&lt;h4&gt;结论&lt;/h4&gt;ACF成功解决了因子化表示预先已知的要求与高维观测处理之间的矛盾，为强化学习中的表示学习提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;利用因子化马尔可夫决策过程的算法比因子无关方法具有更高的样本效率，但它们假设因子化表示是预先已知的——这一要求在智能体只能看到高维观测时会失效。相反，深度强化学习能够处理此类输入，但无法受益于因子化结构。我们通过动作可控因子化(ACF)解决这一表示问题，ACF是一种对比学习方法，能够发现可独立控制的潜在变量——即每个动作可以单独影响的状态分量。ACF利用稀疏性：动作通常只影响变量的一小部分，而其余部分在环境动态下演化，为对比训练提供信息数据。ACF能够在三个具有已知因子化结构的基准测试(Taxi、FourRooms和MiniGrid-DoorKey)上直接从像素观测中恢复真实的可控因子，并且一致性地优于基解纠缠算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Algorithms that exploit factored Markov decision processes are far moresample-efficient than factor-agnostic methods, yet they assume a factoredrepresentation is known a priori -- a requirement that breaks down when theagent sees only high-dimensional observations. Conversely, deep reinforcementlearning handles such inputs but cannot benefit from factored structure. Weaddress this representation problem with Action-Controllable Factorization(ACF), a contrastive learning approach that uncovers independently controllablelatent variables -- state components each action can influence separately. ACFleverages sparsity: actions typically affect only a subset of variables, whilethe rest evolve under the environment's dynamics, yielding informative data forcontrastive training. ACF recovers the ground truth controllable factorsdirectly from pixel observations on three benchmarks with known factoredstructure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistentlyoutperforming baseline disentanglement algorithms.</description>
      <author>example@mail.com (Rafael Rodriguez-Sanchez, Cameron Allen, George Konidaris)</author>
      <guid isPermaLink="false">2510.02484v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI</title>
      <link>http://arxiv.org/abs/2510.02120v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VarCoNet是一个创新的自监督框架，通过将大脑功能的个体间变异性视为有意义数据而非噪声，实现了从静息态fMRI数据中稳健提取功能连接组。该框架在主体指纹识别和自闭症谱系障碍分类等下游任务上表现出优越性、稳健性、可解释性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;精准医疗需要考虑大脑功能的个体间变异性，但传统方法通常将这种变异性视为噪声而非有价值的信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个增强的自监督框架VarCoNet，用于从静息态fMRI数据中稳健提取功能连接组，并将功能个体间变异性作为有意义的数据加以利用。&lt;h4&gt;方法&lt;/h4&gt;VarCoNet采用自监督对比学习利用内在的功能个体间变异性，作为大脑功能编码器生成可直接应用于下游任务的FC嵌入。它通过基于分割rs-fMRI信号的新型增强策略促进对比学习，核心是集成了1D-CNN-Transformer编码器进行高级时间序列处理，并采用贝叶斯超参数优化增强稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;VarCoNet在两个下游任务上得到验证：使用人类连接组计划数据进行主体指纹识别，以及使用ABIDE I和II数据集进行自闭症谱系障碍分类。与包括13种深度学习方法在内的最先进方法相比，VarCoNet展现出优越性、稳健性、可解释性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;VarCoNet为静息态fMRI中的功能连接组分析提供了一个通用且稳健的框架，能够有效利用大脑功能的个体间变异性。&lt;h4&gt;翻译&lt;/h4&gt;考虑大脑功能的个体间变异性对精准医疗至关重要。通过将功能个体间变异性视为有意义的数据而非噪声，我们引入了VarCoNet，一个增强的自监督框架，用于从静息态fMRI数据中稳健提取功能连接组。VarCoNet采用自监督对比学习来利用内在的功能个体间变异性，作为大脑功能编码器生成可直接应用于下游任务的FC嵌入，即使在没有标记数据的情况下。对比学习通过一种基于分割rs-fMRI信号的新型增强策略促进。其核心是集成了一个1D-CNN-Transformer编码器用于高级时间序列处理，增强了稳健的贝叶斯超参数优化。我们的VarCoNet框架在两个下游任务上进行了评估：(i)使用人类连接组计划的rs-fMRI数据进行主体指纹识别，和(ii)使用ABIDE I和ABIDE II数据集的rs-fMRI数据进行自闭症谱系障碍分类。使用不同的脑区分割方法，我们与包括13种深度学习方法在内的最先进方法进行了广泛测试，证明了VarCoNet的优越性、稳健性、可解释性和泛化能力。总体而言，VarCoNet为rs-fMRI中的FC分析提供了一个通用且稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accounting for inter-individual variability in brain function is key toprecision medicine. Here, by considering functional inter-individualvariability as meaningful data rather than noise, we introduce VarCoNet, anenhanced self-supervised framework for robust functional connectome (FC)extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employsself-supervised contrastive learning to exploit inherent functionalinter-individual variability, serving as a brain function encoder thatgenerates FC embeddings readily applicable to downstream tasks even in theabsence of labeled data. Contrastive learning is facilitated by a novelaugmentation strategy based on segmenting rs-fMRI signals. At its core,VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-seriesprocessing, enhanced with a robust Bayesian hyperparameter optimization. OurVarCoNet framework is evaluated on two downstream tasks: (i) subjectfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)autism spectrum disorder (ASD) classification, using rs-fMRI data from theABIDE I and ABIDE II datasets. Using different brain parcellations, ourextensive testing against state-of-the-art methods, including 13 deep learningmethods, demonstrates VarCoNet's superiority, robustness, interpretability, andgeneralizability. Overall, VarCoNet provides a versatile and robust frameworkfor FC analysis in rs-fMRI.</description>
      <author>example@mail.com (Charalampos Lamprou, Aamna Alshehhi, Leontios J. Hadjileontiadis, Mohamed L. Seghier)</author>
      <guid isPermaLink="false">2510.02120v2</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Node Feature Selection For Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.03096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种自适应节点特征选择方法，用于图神经网络，能够在训练过程中识别并移除不必要的特征，基于验证性能变化确定特征重要性，适用于不同图架构和挑战性图学习场景。&lt;h4&gt;背景&lt;/h4&gt;测量特征如何贡献于模型输出对于解释决策、降低维度和通过消除无用变量提高性能至关重要。然而，图结构数据引入了复杂的依赖关系，可能不适合传统的特征重要性指标。&lt;h4&gt;目的&lt;/h4&gt;开发一种模型和任务无关的方法，在训练过程中基于验证性能变化确定相关特征，提供特征重要性分数，并跟踪特征在连续被删除过程中相关性的演变。&lt;h4&gt;方法&lt;/h4&gt;提出基于干预的方法，通过置换特征值并观察验证性能变化来确定特征重要性。不仅训练结束后返回特征重要性分数，还跟踪特征在连续被删除过程中相关性的演变。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果表明，该方法能够灵活适应不同的图架构，并且能够适应更具挑战性的图学习设置。&lt;h4&gt;结论&lt;/h4&gt;该自适应节点特征选择方法能够有效识别和移除不必要的特征，提高图神经网络的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于图神经网络(GNNs)的自适应节点特征选择方法，该方法在训练过程中能够识别并移除不必要的特征。测量特征如何贡献于模型输出的能力对于解释决策、降低维度以及通过消除无用变量来提高性能至关重要。然而，图结构数据引入了复杂的依赖关系，这些关系可能不适合传统的特征重要性指标。受此挑战启发，我们提出了一种模型和任务无关的方法，该方法基于置换特征值后验证性能的变化来确定训练过程中的相关特征。我们通过表征GNN性能如何依赖于节点数据与图结构之间的关系，从理论上验证了我们基于干预的方法。我们不仅会在训练结束后返回特征重要性分数，还会跟踪特征在连续被删除过程中相关性的演变。因此，我们可以监控特征是否被有效消除，并使用此技术评估其他指标。我们的实证结果验证了我们的方法对不同图架构的灵活性，以及其对更具挑战性的图学习设置的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an adaptive node feature selection approach for graph neuralnetworks (GNNs) that identifies and removes unnecessary features duringtraining. The ability to measure how features contribute to model output is keyfor interpreting decisions, reducing dimensionality, and even improvingperformance by eliminating unhelpful variables. However, graph-structured dataintroduces complex dependencies that may not be amenable to classical featureimportance metrics. Inspired by this challenge, we present a model- andtask-agnostic method that determines relevant features during training based onchanges in validation performance upon permuting feature values. Wetheoretically motivate our intervention-based approach by characterizing howGNN performance depends on the relationships between node data and graphstructure. Not only do we return feature importance scores once trainingconcludes, we also track how relevance evolves as features are successivelydropped. We can therefore monitor if features are eliminated effectively andalso evaluate other metrics with this technique. Our empirical results verifythe flexibility of our approach to different graph architectures as well as itsadaptability to more challenging graph learning settings.</description>
      <author>example@mail.com (Ali Azizpour, Madeline Navarro, Santiago Segarra)</author>
      <guid isPermaLink="false">2510.03096v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs</title>
      <link>http://arxiv.org/abs/2510.03086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 10 figures, 12 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的链式图神经网络方法来解决图对齐问题，显著提高了图神经网络在组合问题上的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在组合问题上一直难以超越传统优化方法，限制了其实际应用影响。&lt;h4&gt;目的&lt;/h4&gt;解决图对齐这一NP-hard任务，仅使用结构信息在未标记图之间寻找最优节点对应关系。&lt;h4&gt;方法&lt;/h4&gt;引入链式程序，训练一系列图神经网络迭代改进相似度矩阵；结合在节点对上操作的架构，捕获全局结构模式；与传统优化方法结合作为后处理。&lt;h4&gt;主要发现&lt;/h4&gt;链式图神经网络在具有挑战性的实例上比现有方法提高3倍以上准确性；唯一解决了所有竞争方法都失效的正则图；与传统优化结合后显著优于最先进求解器。&lt;h4&gt;结论&lt;/h4&gt;通过链式图神经网络架构和节点对操作，成功解决了图神经网络在组合问题上的局限性，在图对齐问题上取得显著进展。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在组合问题上一直难以超越传统优化方法，限制了其实际应用。我们通过引入一种新颖的链式程序来解决图对齐问题，这是一个基础的NP-hard任务，仅使用结构信息在未标记图之间寻找最优节点对应关系。我们的方法训练一系列图神经网络，每个网络学习迭代改进前一个网络产生的相似度矩阵。在推理过程中，这创造了一种自举效应：每个图神经网络通过整合先前迭代中关于节点对齐质量的离散排名信息来改进部分解决方案。我们结合了一个强大的架构，该架构在节点对而非单个节点上操作，捕获了对齐至关重要的全局结构模式，这是标准消息传递网络无法表示的。在合成基准上的广泛实验表明了显著的改进：我们的链式图神经网络在具有挑战性的实例上比现有方法提高了3倍以上的准确性，并且唯一解决了所有竞争方法都失效的正则图。当与传统优化结合作为后处理时，我们的方法在图对齐基准上显著优于最先进的求解器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have struggled to outperform traditionaloptimization methods on combinatorial problems, limiting their practicalimpact. We address this gap by introducing a novel chaining procedure for thegraph alignment problem, a fundamental NP-hard task of finding optimal nodecorrespondences between unlabeled graphs using only structural information. Ourmethod trains a sequence of GNNs where each network learns to iterativelyrefine similarity matrices produced by previous networks. During inference,this creates a bootstrap effect: each GNN improves upon partial solutions byincorporating discrete ranking information about node alignment quality fromprior iterations. We combine this with a powerful architecture that operates onnode pairs rather than individual nodes, capturing global structural patternsessential for alignment that standard message-passing networks cannotrepresent. Extensive experiments on synthetic benchmarks demonstratesubstantial improvements: our chained GNNs achieve over 3x better accuracy thanexisting methods on challenging instances, and uniquely solve regular graphswhere all competing approaches fail. When combined with traditionaloptimization as post-processing, our method substantially outperformsstate-of-the-art solvers on the graph alignment benchmark.</description>
      <author>example@mail.com (Marc Lelarge)</author>
      <guid isPermaLink="false">2510.03086v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia</title>
      <link>http://arxiv.org/abs/2510.03004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript has been accepted by Biomedical Signal Processing and  Control and the code is available at  https://github.com/TianzhengHU/BrainIB_coding/tree/main/BrainIB_GIB&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为BrainIB++的端到端创新图神经网络框架，应用信息瓶颈原则识别最具信息量的脑区域作为子图，提高了精神障碍诊断模型的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;精神障碍诊断模型正在快速发展，基于rs-fMRI的机器学习分类器被用于识别区分精神障碍与健康对照的脑生物标志物。传统机器学习模型依赖大量特征工程引入偏差，而深度学习模型虽无需人工干预但缺乏可解释性，限制了临床应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端的创新图神经网络框架，在保持高诊断准确性的同时提高模型的可解释性，识别出与临床相关联的脑生物标志物。&lt;h4&gt;方法&lt;/h4&gt;引入名为BrainIB++的图神经网络框架，应用信息瓶颈原则在模型训练过程中识别最具信息量的数据驱动脑区域作为子图进行解释。在三个多队列精神分裂症数据集上评估性能，并与九种已建立的脑网络分类方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;BrainIB++模型在诊断准确性上表现一致优异，对未见过的数据具有泛化能力。模型识别的子图与精神分裂症已建立的生物标志物相对应，特别强调了视觉、感觉运动和更高认知脑功能网络的异常。&lt;h4&gt;结论&lt;/h4&gt;BrainIB++模型与临床生物标志物的一致性增强了其可解释性，强调了其在现实世界诊断应用中的相关性和潜力。&lt;h4&gt;翻译&lt;/h4&gt;精神障碍诊断模型的发展正在该领域获得关注。最近，基于静息态功能磁共振成像的机器学习分类器已被开发用于识别区分精神障碍与健康对照的脑生物标志物。然而，传统基于机器学习的诊断模型通常依赖大量特征工程，通过人工干预引入偏差。虽然深度学习模型预期无需人工操作，但它们缺乏可解释性，在获取可解释和可靠的脑生物标志物以支持诊断决策方面面临重大挑战，最终限制了它们的临床适用性。在本研究中，我们引入了一种名为BrainIB++的端到端创新图神经网络框架，该框架应用信息瓶颈原则在模型训练期间识别最具信息量的数据驱动脑区域作为子图进行解释。我们在三个多队列精神分裂症数据集上评估了我们的模型与九种已建立的脑网络分类方法的性能。它始终表现出优越的诊断准确性，并显示出对未见数据的泛化能力。此外，我们模型识别的子图也与精神分裂症中已建立的临床生物标志物相对应，特别强调了视觉、感觉运动和更高认知脑功能网络的异常。这种一致性增强了模型的可解释性，并强调了其对现实世界诊断应用的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of diagnostic models is gaining traction in the field ofpsychiatric disorders. Recently, machine learning classifiers based onresting-state functional magnetic resonance imaging (rs-fMRI) have beendeveloped to identify brain biomarkers that differentiate psychiatric disordersfrom healthy controls. However, conventional machine learning-based diagnosticmodels often depend on extensive feature engineering, which introduces biasthrough manual intervention. While deep learning models are expected to operatewithout manual involvement, their lack of interpretability poses significantchallenges in obtaining explainable and reliable brain biomarkers to supportdiagnostic decisions, ultimately limiting their clinical applicability. In thisstudy, we introduce an end-to-end innovative graph neural network frameworknamed BrainIB++, which applies the information bottleneck (IB) principle toidentify the most informative data-driven brain regions as subgraphs duringmodel training for interpretation. We evaluate the performance of our modelagainst nine established brain network classification methods across threemulti-cohort schizophrenia datasets. It consistently demonstrates superiordiagnostic accuracy and exhibits generalizability to unseen data. Furthermore,the subgraphs identified by our model also correspond with established clinicalbiomarkers in schizophrenia, particularly emphasizing abnormalities in thevisual, sensorimotor, and higher cognition brain functional network. Thisalignment enhances the model's interpretability and underscores its relevancefor real-world diagnostic applications.</description>
      <author>example@mail.com (Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu)</author>
      <guid isPermaLink="false">2510.03004v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.02813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for poster presentation at Forum Acusticum Euronoise 2025,  Malaga, Spain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出使用图神经网络(GNN)提升摄影测量重建网格分辨率，解决传统HRTFs获取方法成本高和专业要求高的问题，实现个体化HRTFs合成。&lt;h4&gt;背景&lt;/h4&gt;传统HRTFs获取方法依赖专业设备和声学专业知识，存在可及性挑战；高分辨率3D建模可通过数值方法合成HRTFs，但高级3D扫描仪成本高且可用性有限；摄影测量法生成3D头部网格虽可行，但分辨率不足限制了其在HRTF合成中的应用。&lt;h4&gt;目的&lt;/h4&gt;研究使用图神经网络和神经细分技术将低分辨率摄影测量重建网格提升为高分辨率网格的可行性，进而用于合成个体化的HRTFs。&lt;h4&gt;方法&lt;/h4&gt;使用Apple Photogrammetry API处理SONICOM数据集重建低分辨率头部网格；训练GNN网络通过基于Hausdorff距离的损失函数将低分辨率网格提升为高分辨率；通过几何验证和Mesh2HRTF生成的合成HRTFs评估性能；将合成HRTFs与高分辨率3D扫描计算结果、声学测量结果及KEMAR HRTF进行对比，使用感知相关数值分析和行为实验（包括定位和空间掩蔽释放任务）进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络能有效提升低分辨率摄影测量网格的分辨率；合成的HRTFs与从高分辨率3D扫描计算出的HRTFs、声学测量的HRTFs以及KEMAR HRTF具有可比性；在定位和空间掩蔽释放任务中表现良好。&lt;h4&gt;结论&lt;/h4&gt;使用图神经网络和神经细分技术提升低分辨率摄影测量重建网格的分辨率是可行的；这种方法可用于合成高质量的个体化HRTFs，无需依赖昂贵的3D扫描设备。&lt;h4&gt;翻译&lt;/h4&gt;传统的头相关传递函数获取方法依赖专业设备和声学专业知识，带来了可及性挑战。 alternatively, 高分辨率3D建模提供了使用边界元法等数值方法合成HRTFs的途径。然而，高级3D扫描仪的高成本和有限可用性限制了它们的适用性。摄影测量已被提出作为生成3D头部网格的解决方案，但其分辨率限制阻碍了其在HRTF合成中的应用。为解决这些限制，本研究探讨了使用图神经网络和神经细分技术将低分辨率摄影测量重建网格提升为高分辨率网格的可行性，这些网格随后可用于合成个体化的HRTFs。使用Apple Photogrammetry API处理SONICOM数据集中的摄影测量数据，以重建低分辨率头部网格。然后使用成对的高低分辨率网格数据集训练GNN，使用基于Hausdorff距离的损失函数将低分辨率输入提升为高分辨率输出。通过几何验证和通过Mesh2HRTF生成的合成HRTFs来验证GNN在未见过的摄影测量数据上的性能。合成的HRTFs与从高分辨率3D扫描计算的HRTFs、声学测量的HRTFs以及KEMAR HRTF进行比较，使用感知相关的数值分析和行为实验，包括定位和空间掩蔽释放任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过提高摄影测量重建（Photogrammetry Reconstruction）的分辨率来改进头相关传输函数（HRTF）的合成质量问题。这个问题重要是因为传统HRTF获取方法需要专业设备和声学知识，而高分辨率3D扫描又成本高昂，限制了普通用户获得个体化HRTF的能力，而个体化HRTF能提供更准确的空间音频体验，减少前后混淆和高度感知障碍。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统HRTF获取方法的局限性和摄影测量的优缺点，认识到提高摄影测量网格分辨率可能是解决方案。他们借鉴了Liu等人提出的数据驱动粗到细几何建模框架和Schmidt等人的表面同映射技术，设计了使用图神经网络（GNN）对低分辨率摄影测量网格进行上采样的方法，并使用基于Hausdorff距离的损失函数进行优化，最终通过HRTF合成和评估验证方法效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络（GNN）对摄影测量重建的低分辨率头部网格进行上采样，提高其分辨率和耳部细节，从而合成更高质量的个体化HRTF。整体流程包括：1)使用iPhone采集摄影测量数据和专业3D扫描作为参考；2)使用Apple摄影测量API重建低分辨率网格；3)计算低-高分辨率网格之间的双射映射；4)训练GNN学习网格上采样；5)使用Mesh2HRTF从不同网格合成HRTF；6)通过数值指标（如LSD）和感知实验（如定位任务）评估效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次应用GNN提高摄影测量重建头部网格分辨率，特别是耳部细节；2)创新性地使用表面同胚映射技术处理不同获取技术间的网格差异；3)使用基于Hausdorff距离的损失函数优化网络；4)结合数值和感知评估全面验证效果。相比之前工作，本文方法大幅降低了获取个体化HRTF的成本和复杂性，使用消费级设备即可实现，同时保持了与高成本方法相当的质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过应用图神经网络提高摄影测量重建头部网格的分辨率，特别是改进耳部细节，为经济可及地获取高质量个体化头相关传输函数（HRTF）提供了创新解决方案，显著提升了普通消费者获得沉浸式音频体验的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Head-Related Transfer Functions (HRTFs) acquisition methods relyon specialised equipment and acoustic expertise, posing accessibilitychallenges. Alternatively, high-resolution 3D modelling offers a pathway tonumerically synthesise HRTFs using Boundary Elements Methods and others.However, the high cost and limited availability of advanced 3D scannersrestrict their applicability. Photogrammetry has been proposed as a solutionfor generating 3D head meshes, though its resolution limitations restrict itsapplication for HRTF synthesis. To address these limitations, this studyinvestigates the feasibility of using Graph Neural Networks (GNN) using neuralsubdivision techniques for upsampling low-resolutionPhotogrammetry-Reconstructed (PR) meshes into high-resolution meshes, which canthen be employed to synthesise individual HRTFs. Photogrammetry data from theSONICOM dataset are processed using Apple Photogrammetry API to reconstructlow-resolution head meshes. The dataset of paired low- and high-resolutionmeshes is then used to train a GNN to upscale low-resolution inputs tohigh-resolution outputs, using a Hausdorff Distance-based loss function. TheGNN's performance on unseen photogrammetry data is validated geometrically andthrough synthesised HRTFs generated via Mesh2HRTF. Synthesised HRTFs areevaluated against those computed from high-resolution 3D scans, to acousticallymeasured HRTFs, and to the KEMAR HRTF using perceptually-relevant numericalanalyses as well as behavioural experiments, including localisation and SpatialRelease from Masking (SRM) tasks.</description>
      <author>example@mail.com (Ludovic Pirard, Katarina C. Poole, Lorenzo Picinali)</author>
      <guid isPermaLink="false">2510.02813v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns</title>
      <link>http://arxiv.org/abs/2510.02702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一个名为VisitHGNN的异构关系特定图神经网络，用于预测城市居民从社区到各个兴趣点(POIs)的访问概率，支持交通规划和公共卫生决策。&lt;h4&gt;背景&lt;/h4&gt;了解城市居民如何在社区和目的地之间出行对于交通规划、出行管理和公共卫生至关重要。通过分析城市间的出行模式，可以估计社区对城市人流量的贡献。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测从社区到各个兴趣点访问概率的方法，支持需求估计、可达性评估和多模式规划，为城市规划和交通政策提供决策支持。&lt;h4&gt;方法&lt;/h4&gt;引入VisitHGNN异构关系特定图神经网络，结合POIs的多种属性和人口普查区块组(CBGs)的社会经济人口变量，通过空间邻接和带距离注释的跨类型边缘连接，使用掩码Kullback-Leibler散度进行训练和预测。&lt;h4&gt;主要发现&lt;/h4&gt;VisitHGNN在富尔顿县数据上表现优异，平均KL散度0.287，MAE 0.008，Top-1准确率0.853，R平方0.892，显著优于基线方法，并与实际访问模式高度一致(NDCG@50 = 0.966; Recall@5 = 0.611)。&lt;h4&gt;结论&lt;/h4&gt;该模型能够高度准确地反映实际出行行为，在城市规划、交通政策、出行系统设计和公共卫生决策支持方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;了解城市居民如何在社区和目的地之间出行对于交通规划、出行管理和公共卫生至关重要。通过挖掘城市地点间具有空间、时间和功能关系的历史起点到终点流动模式，我们估计了从社区到特定目的地的访问概率。这些概率捕捉了社区层面城市车辆和人流量贡献，支持需求估计、可达性评估和多模式规划。特别是，我们引入了VisitHGNN，这是一种异构关系特定的图神经网络，专为预测单个兴趣点(POIs)的访问概率而设计。POIs使用数值、JSON派生和文本属性进行表征，并增加了POI-POI空间邻近度、时间共活动和品牌亲和力的固定摘要，而人口普查区块组(CBGs)则用72个社会经济人口变量描述。CBGs通过空间邻接连接，POIs和CBGs通过带距离注释的跨类型边缘连接。推理被限制在基于距离的可能来源CBG候选集内，训练最小化了掩码Kullback-Leibler(KL)散度，以在候选集上产生概率分布。使用美国乔治亚州富尔顿县的每周移动数据，VisitHGNN取得了强大的预测性能，平均KL散度为0.287，平均绝对误差为0.008，Top-1准确率为0.853，R平方为0.892，显著优于成对MLP和仅距离的基线方法，并与实际访问模式紧密一致(NDCG@50 = 0.966; Recall@5 = 0.611)。所得分布高度忠实地反映了观察到的出行行为，突显了该模型在城市规划、交通政策、出行系统设计和公共卫生决策支持方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how urban residents travel between neighborhoods anddestinations is critical for transportation planning, mobility management, andpublic health. By mining historical origin-to-destination flow patterns withspatial, temporal, and functional relations among urban places, we estimateprobabilities of visits from neighborhoods to specific destinations. Theseprobabilities capture neighborhood-level contributions to citywide vehicularand foot traffic, supporting demand estimation, accessibility assessment, andmultimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous,relation-specific graph neural network designed to predict visit probabilitiesat individual Points of interest (POIs). POIs are characterized usingnumerical, JSON-derived, and textual attributes, augmented with fixed summariesof POI--POI spatial proximity, temporal co-activity, and brand affinity, whilecensus block groups (CBGs) are described with 72 socio-demographic variables.CBGs are connected via spatial adjacency, and POIs and CBGs are linked throughdistance-annotated cross-type edges. Inference is constrained to adistance-based candidate set of plausible origin CBGs, and training minimizes amasked Kullback-Leibler (KL) divergence to yield probability distributionacross the candidate set. Using weekly mobility data from Fulton County,Georgia, USA, VisitHGNN achieves strong predictive performance with mean KLdivergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of0.892, substantially outperforming pairwise MLP and distance-only baselines,and aligning closely with empirical visitation patterns (NDCG@50 = 0.966);Recall@5 = 0.611). The resulting distributions closely mirror observed travelbehavior with high fidelity, highlighting the model's potential for decisionsupport in urban planning, transportation policy, mobility system design, andpublic health.</description>
      <author>example@mail.com (Lin Pang, Jidong J. Yang)</author>
      <guid isPermaLink="false">2510.02702v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Identifying Asymptomatic Nodes in Network Epidemics using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.02568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper presented in the 35th Brazilian Conference on Intelligent  Systems (BRACIS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用图神经网络(GNN)识别流行病中无症状个体的方法，该方法能够在不进行全面检测的情况下准确识别无症状感染者，且在不同网络环境下表现鲁棒。&lt;h4&gt;背景&lt;/h4&gt;某些流行病中的感染者可能保持无症状状态但仍能传播感染，这增加了疫情控制的难度。识别这些无症状个体对疫情监测至关重要，但定期广泛检测健康人群成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决在SI(易感-感染)网络流行病学模型中识别无症状个体的问题，这些个体的观察状态与易感节点相同，难以区分。&lt;h4&gt;方法&lt;/h4&gt;采用带有监督学习的图神经网络模型，从具有已观察感染节点的网络构建节点特征集，用于将健康节点分类为无症状或易感状态。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在不同网络模型、网络规模和观察感染比例的场景下均表现出鲁棒性，能够准确识别无症状节点，并具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的识别方法能有效解决无症状感染者识别难题，为疫情监测和控制提供了一种经济有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;某些流行病中的感染者可能保持无症状状态，但仍能携带和传播感染。这些个体促进了疫情的传播，对公共卫生政策构成重大挑战。识别无症状个体对测量和控制疫情至关重要，但定期广泛检测健康人群往往成本过高。本文考虑经典的SI(易感-感染)网络流行病学模型，解决识别无症状个体的问题，其中部分感染节点未被观察到(即它们的观察状态与易感节点相同)。为将健康节点分类为无症状或易感，采用了一种基于监督学习的图神经网络模型，从具有已观察感染节点的网络构建节点特征集。该方法在不同网络模型、网络大小和观察感染比例下进行了评估。结果表明，所提出的方法在不同场景下具有鲁棒性，能准确识别无症状节点，同时推广到不同网络大小和观察感染比例的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infected individuals in some epidemics can remain asymptomatic while stillcarrying and transmitting the infection. These individuals contribute to thespread of the epidemic and pose a significant challenge to public healthpolicies. Identifying asymptomatic individuals is critical for measuring andcontrolling an epidemic, but periodic and widespread testing of healthyindividuals is often too costly. This work tackles the problem of identifyingasymptomatic individuals considering a classic SI (Susceptible-Infected)network epidemic model where a fraction of the infected nodes are not observedas infected (i.e., their observed state is identical to susceptible nodes). Inorder to classify healthy nodes as asymptomatic or susceptible, a Graph NeuralNetwork (GNN) model with supervised learning is adopted where a set of nodefeatures are built from the network with observed infected nodes. The approachis evaluated across different network models, network sizes, and fraction ofobserved infections. Results indicate that the proposed methodology is robustacross different scenarios, accurately identifying asymptomatic nodes whilealso generalizing to different network sizes and fraction of observedinfections.</description>
      <author>example@mail.com (Conrado Catarcione Pinto, Amanda Camacho Novaes de Oliveira, Rodrigo Sapienza Luna, Daniel Ratton Figueiredo)</author>
      <guid isPermaLink="false">2510.02568v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>On The Expressive Power of GNN Derivatives</title>
      <link>http://arxiv.org/abs/2510.02565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高阶导数图神经网络(HOD-GNN)，通过利用基础模型的高阶节点导数来增强消息传递神经网络的表达能力，在多个图学习基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络(GNNs)取得了显著进展，但其表达能力仍然是一个基本挑战。关于GNN表达能力的研究已经产生了多种具有不同表达能力的架构层次。同时，GNN对节点特征的导数研究已在过度挤压和过度平滑现象、GNN可解释性等方面得到广泛研究。&lt;h4&gt;目的&lt;/h4&gt;探索利用GNN对节点特征的导数作为增强GNN表达能力的新方法，填补这一研究空白。&lt;h4&gt;方法&lt;/h4&gt;引入高阶导数GNN(HOD-GNN)，一种新颖方法，通过利用基础模型的高阶节点导数增强消息传递神经网络(MPNNs)的表达能力。这些导数生成具有结构感知能力的节点嵌入，由第二个GNN在端到端可训练架构中处理。同时开发了利用图稀疏性和并行性的消息传递算法以提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，HOD-GNN架构家族的表达能力与WL层次结构一致；发现了HOD-GNN、子图GNN和流行的结构编码方案之间的深层联系；在多个图学习基准测试上表现出强劲性能。&lt;h4&gt;结论&lt;/h4&gt;高阶导数GNN(HOD-GNN)为增强图神经网络表达能力提供了有效途径，通过利用高阶节点导数生成结构感知的节点嵌入，结合理论分析和实验验证，证明了其在图学习任务中的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络(GNNs)取得了显著进展，但其有限的表达能力仍然是一个基本挑战。关于GNN表达能力的研究已经产生了许多具有表达能力的架构，形成了具有越来越强表达能力模型的架构层次。同时，关于GNN对节点特征的导数研究已经在过度挤压和过度平滑现象、GNN可解释性等方面得到广泛研究。迄今为止，这些导数尚未被探索作为增强GNN表达能力的方法。在本文中，我们展示了这些导数为增强GNN表达能力提供了一种自然方式。我们引入了高阶导数GNN(HOD-GNN)，一种新颖方法，通过利用基础模型的高阶节点导数来增强消息传递神经网络(MPNNs)的表达能力。这些导数生成具有结构感知能力的节点嵌入，由第二个GNN在端到端可训练架构中处理。理论上，我们展示了所得到的架构家族的表达能力与WL层次结构一致。我们还发现了HOD-GNN、子图GNN和流行的结构编码方案之间的深层联系。为了计算效率，我们开发了一种用于计算MPNN高阶导数的消息传递算法，该算法利用了图稀疏性和并行性。在流行的图学习基准测试上的评估表明，HOD-GNN在流行的图学习任务上表现强劲。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant advances in Graph Neural Networks (GNNs), their limitedexpressivity remains a fundamental challenge. Research on GNN expressivity hasproduced many expressive architectures, leading to architecture hierarchieswith models of increasing expressive power. Separately, derivatives of GNNswith respect to node features have been widely studied in the context of theoversquashing and over-smoothing phenomena, GNN explainability, and more. Todate, these derivatives remain unexplored as a means to enhance GNNexpressivity. In this paper, we show that these derivatives provide a naturalway to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN(HOD-GNN), a novel method that enhances the expressivity of Message PassingNeural Networks (MPNNs) by leveraging high-order node derivatives of the basemodel. These derivatives generate expressive structure-aware node embeddingsprocessed by a second GNN in an end-to-end trainable architecture.Theoretically, we show that the resulting architecture family's expressivepower aligns with the WL hierarchy. We also draw deep connections betweenHOD-GNN, Subgraph GNNs, and popular structural encoding schemes. Forcomputational efficiency, we develop a message-passing algorithm for computinghigh-order derivatives of MPNNs that exploits graph sparsity and parallelism.Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strongperformance on popular graph learning tasks.</description>
      <author>example@mail.com (Yam Eitan, Moshe Eliasof, Yoav Gelberg, Fabrizio Frasca, Guy Bar-Shalom, Haggai Maron)</author>
      <guid isPermaLink="false">2510.02565v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads</title>
      <link>http://arxiv.org/abs/2510.02472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a preprint and has been submitted to Engineering with  Computers&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于异构图神经网络(HGNNs)的加筋板异构图表示方法，能够有效考虑几何变异性、非均匀边界条件和不同加载场景。通过将结构划分为多个结构单元并使用三种不同类型的节点表示，结合异构图变换器(HGT)，该方法能够准确预测加筋板上的冯·米塞斯应力和位移场。数值测试表明，与同构对应物相比，该方法具有优越性能，能有效捕捉结构行为模式和最大值。&lt;h4&gt;背景&lt;/h4&gt;代理模型在结构分析和优化中是必不可少的。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够考虑几何变异性、非均匀边界条件和不同加载场景的加筋板异构图表示方法，使用异构图神经网络(HGNNs)。&lt;h4&gt;方法&lt;/h4&gt;将结构划分为多个结构单元，如加强筋和它们之间的板，每个单元由三种不同类型的节点表示：几何节点、边界节点和加载节点；通过引入连接节点的局部方向和空间关系来引入边异构性；提出几种具有不同异构程度的异构图表示；将这些表示实现到异构图变换器(HGT)中，以预测加筋板上的冯·米塞斯应力和位移场。&lt;h4&gt;主要发现&lt;/h4&gt;对承受点载荷的板和由加筋板组成的箱梁在各种载荷条件下的数值测试表明，异构图表示与同构对应物相比表现出优越性能；消融分析评估了图异构性对HGT性能的影响；结果显示对位移和冯·米塞斯应力都具有强预测准确性，能有效捕捉结构行为模式和最大值。&lt;h4&gt;结论&lt;/h4&gt;所提出的异构图表示方法能够准确预测加筋板的应力和位移，有效捕捉结构行为模式。&lt;h4&gt;翻译&lt;/h4&gt;代理模型在结构分析和优化中是必不可少的。我们提出了一种加筋板的异构图表示方法，该方法考虑了几何变异性、非均匀边界条件和不同的加载场景，使用异构图神经网络(HGNNs)。结构被划分为多个结构单元，如加强筋和它们之间的板，每个单元由三种不同类型的节点表示：几何节点、边界节点和加载节点。通过引入连接节点的局部方向和空间关系来引入边异构性。提出了几种具有不同异构程度的异构图表示并进行了分析。这些表示被实现到异构图变换器(HGT)中，以预测加筋板上的冯·米塞斯应力和位移场，基于其边界上的载荷和自由度。为了评估我们方法的功效，我们对承受点载荷的板和由加筋板组成的在各种载荷条件下的箱梁进行了数值测试。将异构图表示与同构对应物进行了比较，显示出优越的性能。此外，进行了消融分析以评估图异构性对HGT性能的影响。结果显示对位移和冯·米塞斯应力都具有强预测准确性，有效捕捉了结构行为模式和最大值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surrogate models are essential in structural analysis and optimization. Wepropose a heterogeneous graph representation of stiffened panels that accountsfor geometrical variability, non-uniform boundary conditions, and diverseloading scenarios, using heterogeneous graph neural networks (HGNNs). Thestructure is partitioned into multiple structural units, such as stiffeners andthe plates between them, with each unit represented by three distinct nodetypes: geometry, boundary, and loading nodes. Edge heterogeneity is introducedby incorporating local orientations and spatial relationships of the connectingnodes. Several heterogeneous graph representations, each with varying degreesof heterogeneity, are proposed and analyzed. These representations areimplemented into a heterogeneous graph transformer (HGT) to predict von Misesstress and displacement fields across stiffened panels, based on loading anddegrees of freedom at their boundaries. To assess the efficacy of our approach,we conducted numerical tests on panels subjected to patch loads and box beamscomposed of stiffened panels under various loading conditions. Theheterogeneous graph representation was compared with a homogeneous counterpart,demonstrating superior performance. Additionally, an ablation analysis wasperformed to evaluate the impact of graph heterogeneity on HGT performance. Theresults show strong predictive accuracy for both displacement and von Misesstress, effectively capturing structural behavior patterns and maximum values.</description>
      <author>example@mail.com (Yuecheng Cai, Jasmin Jelovica)</author>
      <guid isPermaLink="false">2510.02472v1</guid>
      <pubDate>Mon, 06 Oct 2025 15:03:26 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks</title>
      <link>http://arxiv.org/abs/2510.02278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提供了更完整、更真实、更具挑战性的交通预测基准，通过发布两个主要城市的道路网络数据集，其中最大的包含近100,000个路段。研究还提出了一种替代方法，使用不包含专门时间序列处理模块的图神经网络，实现更好的可扩展性和更强的预测性能。&lt;h4&gt;背景&lt;/h4&gt;交通预测是机器学习社区关注的重点，时空图神经网络已成为最流行方法。然而，当前公开基准数据集存在显著缺点：缺乏道路连通性信息、道路属性信息有限、路段数量较少，且主要包含城际高速公路信息，而非更具挑战性的城市道路网络。&lt;h4&gt;目的&lt;/h4&gt;提供更完整、更真实、更具挑战性的交通预测基准，并开发能够处理大规模数据集的神经交通预测方法。&lt;h4&gt;方法&lt;/h4&gt;发布两个主要城市的道路网络数据集，包含丰富的道路特征和细粒度的交通流量与速度数据。提出一种不包含专门时间序列处理模块的图神经网络方法，以提升可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;大多数当前神经时空模型无法扩展到大规模数据集。提出的替代方法（不包含专门时间序列处理模块的GNN）实现了更好的可扩展性，同时展示了更强的预测性能。&lt;h4&gt;结论&lt;/h4&gt;研究团队的数据集和模型洞察将成为交通预测研究的有价值资源。&lt;h4&gt;翻译&lt;/h4&gt;道路网络上的交通预测是一项具有重大实际意义的复杂任务，最近引起了机器学习社区的极大关注，时空图神经网络已成为最流行的方法。当前公开可用的基准存在显著缺点，包括缺乏道路连通性信息、道路属性信息有限，以及路段数量相对较少。此外，当前数据集主要包含城际高速公路信息，而城市道路网络由于道路更密集和交通模式更复杂，实际上提出了更具挑战性的预测任务。在这项工作中，我们通过发布两个主要城市的道路网络数据集，为交通预测提供了一个更完整、更真实、更具挑战性的基准。我们的数据集包含丰富的道路特征，并提供关于交通流量和交通速度的细粒度数据。我们提出的替代方法使用不包含专门时间序列处理模块的GNN，实现了更好的可扩展性和更强的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting on road networks is a complex task of significantpractical importance that has recently attracted considerable attention fromthe machine learning community, with spatiotemporal graph neural networks(GNNs) becoming the most popular approach. The proper evaluation of trafficforecasting methods requires realistic datasets, but current publicly availablebenchmarks have significant drawbacks, including the absence of informationabout road connectivity for road graph construction, limited information aboutroad properties, and a relatively small number of road segments that fallsshort of real-world applications. Further, current datasets mostly containinformation about intercity highways with sparsely located sensors, while cityroad networks arguably present a more challenging forecasting task due to muchdenser roads and more complex urban traffic patterns. In this work, we providea more complete, realistic, and challenging benchmark for traffic forecastingby releasing datasets representing the road networks of two major cities, withthe largest containing almost 100,000 road segments (more than a 10-foldincrease relative to existing datasets). Our datasets contain rich roadfeatures and provide fine-grained data about both traffic volume and trafficspeed, allowing for building more holistic traffic forecasting systems. We showthat most current implementations of neural spatiotemporal models for trafficforecasting have problems scaling to datasets of our size. To overcome thisissue, we propose an alternative approach to neural traffic forecasting thatuses a GNN without a dedicated module for temporal sequence processing, thusachieving much better scalability, while also demonstrating strongerforecasting performance. We hope our datasets and modeling insights will serveas a valuable resource for research in traffic forecasting.</description>
      <author>example@mail.com (Fedor Velikonivtsev, Oleg Platonov, Gleb Bazhenov, Liudmila Prokhorenkova)</author>
      <guid isPermaLink="false">2510.02278v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
  <item>
      <title>Transformers Discover Molecular Structure Without Graph Priors</title>
      <link>http://arxiv.org/abs/2510.02259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了纯Transformer模型在分子机器学习中的应用，挑战了图神经网络(GNNs)中硬编码图的必要性。研究表明，直接在笛卡尔坐标上训练的Transformer可以近似分子能量和力，并且学习到物理上一致的注意力模式。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是分子机器学习的主导架构，特别用于分子性质预测和机器学习原子间势能。GNNs在预定义的图上进行消息传递，这些图通常由固定半径截断或k近邻方案诱导。这种设计虽然与许多分子任务中的局部性一致，但硬编码的图可能限制表达能力并减慢推理速度。&lt;h4&gt;目的&lt;/h4&gt;研究纯的、未经修改的Transformer（直接在笛卡尔坐标上训练，没有预定义图或物理先验）是否可以近似分子能量和力。&lt;h4&gt;方法&lt;/h4&gt;在匹配的训练计算预算下训练Transformer，使其在OMol25数据集上与最先进的等变GNN具有竞争力的能量和力平均绝对误差。使用标准Transformer架构进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;Transformer学习到物理上一致的模式，例如注意力权重随原子间距离呈反比衰减；由于没有硬编码的偏见，Transformer能够灵活地适应不同的分子环境；使用标准Transformer实现了可预测的性能提升，与在其他领域观察到的经验缩放定律一致。&lt;h4&gt;结论&lt;/h4&gt;许多GNN的有利特性可以在Transformer中自适应地出现，挑战了硬编码图归纳偏见的必要性，指向分子建模的标准化、可扩展架构。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是分子机器学习的主导架构，特别用于分子性质预测和机器学习原子间势能(MLIPs)。GNNs在预定义的图上进行消息传递，这些图通常由固定半径截断或k近邻方案诱导。虽然这种设计符合许多分子任务中的局部性，但硬编码的图可能由于固定的感受野而限制表达能力，并且稀疏图操作会减慢推理速度。在本工作中，我们研究纯的、未经修改的Transformer（直接在笛卡尔坐标上训练，没有预定义图或物理先验）是否可以近似分子能量和力。作为分析的起点，我们展示如何在匹配的训练计算预算下训练Transformer，使其在OMol25数据集上与最先进的等变GNN具有竞争力的能量和力平均绝对误差。我们发现Transformer学习到物理上一致的模式，例如注意力权重随原子间距离呈反比衰减，并且由于没有硬编码的偏见，能够灵活地适应不同的分子环境。使用标准Transformer还实现了可预测的性能提升，与其他领域观察到的经验缩放定律一致。我们的结果表明，GNN的许多有利特性可以在Transformer中自适应地出现，挑战了硬编码图归纳偏见的必要性，并指向分子建模的标准化、可扩展架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are the dominant architecture for molecularmachine learning, particularly for molecular property prediction and machinelearning interatomic potentials (MLIPs). GNNs perform message passing onpredefined graphs often induced by a fixed radius cutoff or k-nearest neighborscheme. While this design aligns with the locality present in many moleculartasks, a hard-coded graph can limit expressivity due to the fixed receptivefield and slows down inference with sparse graph operations. In this work, weinvestigate whether pure, unmodified Transformers trained directly on Cartesiancoordinates$\unicode{x2013}$without predefined graphs or physicalpriors$\unicode{x2013}$can approximate molecular energies and forces. As astarting point for our analysis, we demonstrate how to train a Transformer tocompetitive energy and force mean absolute errors under a matched trainingcompute budget, relative to a state-of-the-art equivariant GNN on the OMol25dataset. We discover that the Transformer learns physically consistentpatterns$\unicode{x2013}$such as attention weights that decay inversely withinteratomic distance$\unicode{x2013}$and flexibly adapts them across differentmolecular environments due to the absence of hard-coded biases. The use of astandard Transformer also unlocks predictable improvements with respect toscaling training resources, consistent with empirical scaling laws observed inother domains. Our results demonstrate that many favorable properties of GNNscan emerge adaptively in Transformers, challenging the necessity of hard-codedgraph inductive biases and pointing toward standardized, scalable architecturesfor molecular modeling.</description>
      <author>example@mail.com (Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan)</author>
      <guid isPermaLink="false">2510.02259v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement</title>
      <link>http://arxiv.org/abs/2510.01910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对图神经网络(GNNs)在真实场景下面临的复合缺陷问题，首次系统比较了传统方法和基于大型语言模型(LLMs)的增强方法，挑战了LLM增强始终优于传统方法的假设，并提出了RoGRAD框架通过检索增强对比学习迭代改进图表示，实验证明其性能显著优于基线方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)被广泛应用于网络相关应用，作为学习图结构数据的核心技术。然而，在真实场景中，这些图往往存在缺陷，严重影响了GNN的性能。先前研究虽然探索了对单个缺陷的鲁棒性，但对于图原生方法和LLM增强方法在复合缺陷下的行为，仍缺乏系统理解。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究空白，首次进行实证研究对比传统方法和LLM-on-graph框架在各种图缺陷下的表现，揭示被忽视的脆弱性，并挑战LLM增强始终优于传统方法的假设。&lt;h4&gt;方法&lt;/h4&gt;提出了Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD)框架。这是一种迭代范式，利用检索增强生成(RAG)通过提供类别一致、多样化的增强，并通过迭代图对比学习强制区分性表示，将图的LLM增强从静态信号注入转变为动态改进。&lt;h4&gt;主要发现&lt;/h4&gt;LLM增强并不总是优于传统方法，挑战了这一假设。RoGRAD框架在性能上显著优于传统GNN基线和LLM增强基线，平均提升了高达82.43%。&lt;h4&gt;结论&lt;/h4&gt;RoGRAD框架通过迭代地检索增强对比学习，有效地解决了图神经网络在复合缺陷下的鲁棒性问题，为图学习领域提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛应用于网络相关应用，作为学习图结构数据的核心技术。然而，在真实场景中，这些图存在缺陷，严重影响了GNN的性能。虽然先前基于GNN的增强研究已经探索了对单个缺陷的鲁棒性，但对于图原生方法和大型语言模型增强方法在复合缺陷下的行为，仍然缺乏系统理解。为填补这一空白，我们进行了首次实证研究，在多种图缺陷下对这些方法进行基准测试，揭示了被忽视的脆弱性，并挑战了LLM增强始终优于传统方法的假设。基于实证发现，我们提出了RoGRAD框架，这是一种迭代范式，利用检索增强生成通过提供类别一致、多样化的增强，并通过迭代图对比学习来强制区分性表示，将图的LLM增强从静态信号注入转变为动态改进。广泛的实验证明了RoGRAD在传统GNN基线和LLM增强基线上的优越性，平均提升了高达82.43%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely adopted in Web-related applications,serving as a core technique for learning from graph-structured data, such astext-attributed graphs. Yet in real-world scenarios, such graphs exhibitdeficiencies that substantially undermine GNN performance. While priorGNN-based augmentation studies have explored robustness against individualimperfections, a systematic understanding of how graph-native and LargeLanguage Models (LLMs) enhanced methods behave under compound deficiencies isstill missing. Specifically, there has been no comprehensive investigationcomparing conventional approaches and recent LLM-on-graph frameworks, leavingtheir merits unclear. To fill this gap, we conduct the first empirical studythat benchmarks these two lines of methods across diverse graph deficiencies,revealing overlooked vulnerabilities and challenging the assumption that LLMaugmentation is consistently superior. Building on empirical findings, wepropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD isthe first iterative paradigm that leverages Retrieval-Augmented Generation(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,diverse augmentations and enforcing discriminative representations throughiterative graph contrastive learning. It transforms LLM augmentation for graphsfrom static signal injection into dynamic refinement. Extensive experimentsdemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhancedbaselines, achieving up to 82.43% average improvement.</description>
      <author>example@mail.com (Zhaoyan Wang, Zheng Gao, Arogya Kharel, In-Young Ko)</author>
      <guid isPermaLink="false">2510.01910v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.01801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型生成的高说服力垃圾评论提出了一种混合检测模型FraudSquad，该模型在LLM生成的垃圾评论数据集上表现出色，比现有方法提高精确度44.22%，召回率43.01%，同时保持模型大小适中且训练资源需求低。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的发展使高度逼真、模仿人类写作的垃圾评论生成成为可能，这些评论对现有检测系统构成挑战，威胁在线平台的可信度。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测由大型语言模型生成的高说服力垃圾评论的检测系统。&lt;h4&gt;方法&lt;/h4&gt;创建三个使用不同LLM生成的垃圾评论数据集，由产品元数据和真实参考评论指导；提出FraudSquad混合检测模型，整合预训练语言模型的文本嵌入和门控图transformer进行垃圾节点分类，捕获语义和行为信号。&lt;h4&gt;主要发现&lt;/h4&gt;FraudSquad在三个LLM生成的数据集上比最先进基线模型提高精确度44.22%，召回率43.01%；在两个人类编写的垃圾评论数据集上也取得良好结果；模型大小适中，只需少量标记训练数据。&lt;h4&gt;结论&lt;/h4&gt;FraudSquad是现实世界应用的实用解决方案，贡献包括新的合成数据集、实用的检测框架以及强调适应LLM时代垃圾检测紧迫性的实证证据。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)的兴起使得能够生成高度说服力的垃圾评论，这些评论紧密模仿人类写作。这些评论对现有检测系统构成重大挑战，威胁在线平台的可信度。在这项工作中，我们首先使用三个不同的LLM创建了三个逼真的LLM生成垃圾评论数据集，每个数据集都由产品元数据和真实的参考评论指导。GPT-4.1的评估确认了这些评论的高说服力和欺骗潜力。为应对这一威胁，我们提出了FraudSquad，一个混合检测模型，集成了预训练语言模型的文本嵌入和用于垃圾节点分类的门控图transformer。FraudSquad捕获语义和行为信号，无需依赖手动特征工程或大量训练资源。实验表明，FraudSquad在三个LLM生成的数据集上比最先进的基线模型在精确度上提高高达44.22%，在召回率上提高43.01%，同时在两个人类编写的垃圾评论数据集上也取得了有希望的结果。此外，FraudSquad保持适中的模型大小，只需要最少的标记训练数据，使其成为现实世界应用的实用解决方案。我们的贡献包括新的合成数据集、实用的检测框架以及强调适应LLM时代垃圾检测紧迫性的实证证据。我们的代码和数据集可在以下网址获取：https://anonymous.4open.science/r/FraudSquad-5389/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of large language models (LLMs) has enabled the generation of highlypersuasive spam reviews that closely mimic human writing. These reviews posesignificant challenges for existing detection systems and threaten thecredibility of online platforms. In this work, we first create three realisticLLM-generated spam review datasets using three distinct LLMs, each guided byproduct metadata and genuine reference reviews. Evaluations by GPT-4.1 confirmthe high persuasion and deceptive potential of these reviews. To address thisthreat, we propose FraudSquad, a hybrid detection model that integrates textembeddings from a pre-trained language model with a gated graph transformer forspam node classification. FraudSquad captures both semantic and behavioralsignals without relying on manual feature engineering or massive trainingresources. Experiments show that FraudSquad outperforms state-of-the-artbaselines by up to 44.22% in precision and 43.01% in recall on threeLLM-generated datasets, while also achieving promising results on twohuman-written spam datasets. Furthermore, FraudSquad maintains a modest modelsize and requires minimal labeled training data, making it a practical solutionfor real-world applications. Our contributions include new synthetic datasets,a practical detection framework, and empirical evidence highlighting theurgency of adapting spam detection to the LLM era. Our code and datasets areavailable at: https://anonymous.4open.science/r/FraudSquad-5389/.</description>
      <author>example@mail.com (Xin Liu, Rongwu Xu, Xinyi Jia, Jason Liao, Jiao Sun, Ling Huang, Wei Xu)</author>
      <guid isPermaLink="false">2510.01801v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2510.01632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BioBlobs是一种即插即用、完全可微分的蛋白质表征学习模块，通过动态分割蛋白质为灵活大小的非重叠亚结构（blobs），并将其量化到共享码本中，生成与功能相关的蛋白质亚结构离散词汇表，从而提高蛋白质编码器性能并提供功能机制性见解。&lt;h4&gt;背景&lt;/h4&gt;蛋白质功能由大小和拓扑各异的相干亚结构驱动，而当前蛋白质表征学习模型依赖k-hop和固定半径邻域等刚性亚结构，扭曲了这些信号。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确捕捉蛋白质功能相关亚结构的表征方法，提高预测性能并提供功能机制性见解。&lt;h4&gt;方法&lt;/h4&gt;引入BioBlobs模块，通过动态分割蛋白质结构为灵活大小、不重叠的亚结构（blobs），将这些blobs量化到共享且可解释的码本中，生成与功能相关的蛋白质亚结构离散词汇表，用于计算蛋白质嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;BioBlobs表征显著提高了GVP-GNN等广泛使用的蛋白质编码器在各种蛋白质表征学习任务中的性能，证明了直接捕获功能相关蛋白质亚结构的架构价值。&lt;h4&gt;结论&lt;/h4&gt;直接捕获功能相关蛋白质亚结构的架构不仅能提高预测性能，还能为蛋白质功能提供机制性见解，具有重要的研究价值。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质功能是由相干的亚结构驱动的，这些亚结构在大小和拓扑上各不相同，然而当前的蛋白质表征学习模型（PRL）通过依赖k-hop和固定半径邻域等刚性亚结构而扭曲了这些信号。我们引入了BioBlobs，一个即插即用、完全可微分的模块，它通过动态地将结构分割成灵活大小、非重叠的亚结构（'blobs'）来表示蛋白质。生成的blobs被量化到一个共享且可解释的码本中，产生一个与功能相关的蛋白质亚结构的离散词汇表，用于计算蛋白质嵌入。我们证明，BioBlobs表征提高了广泛使用的蛋白质编码器（如GVP-GNN）在各种PRL任务中的性能。我们的方法强调了直接捕获功能相关蛋白质亚结构的架构的价值，既能提高预测性能，又能对蛋白质功能提供机制性见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein function is driven by coherent substructures which vary in size andtopology, yet current protein representation learning models (PRL) distortthese signals by relying on rigid substructures such as k-hop and fixed radiusneighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiablemodule that represents proteins by dynamically partitioning structures intoflexibly-sized, non-overlapping substructures ("blobs"). The resulting blobsare quantized into a shared and interpretable codebook, yielding a discretevocabulary of function-relevant protein substructures used to compute proteinembeddings. We show that BioBlobs representations improve the performance ofwidely used protein encoders such as GVP-GNN across various PRL tasks. Ourapproach highlights the value of architectures that directly capturefunction-relevant protein substructures, enabling both improved predictiveperformance and mechanistic insight into protein function.</description>
      <author>example@mail.com (Xin Wang, Carlos Oliver)</author>
      <guid isPermaLink="false">2510.01632v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets</title>
      <link>http://arxiv.org/abs/2510.01022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at the NeurIPS workshop on New Perspectives  in Advancing Graph Machine Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种新型的几何散射变换版本，用于处理包含标量和向量节点特征的几何图，该变换具有关于刚体旋转平移的对称性，可整合到几何GNN框架中，并且实证表明其基于等变性散射的GNN在参数数量大幅减少的情况下，与其他等变性消息传递GNN性能相当。&lt;h4&gt;背景&lt;/h4&gt;几何图处理是图神经网络领域的重要研究方向，特别是对于具有标量和向量节点特征的几何图。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有理想对称性的新型几何散射变换，并证明其在几何GNN框架中的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型的几何散射变换版本，该变换具有关于刚体旋转平移（SE(3)等变性）的对称性，并将其整合到几何GNN框架中。&lt;h4&gt;主要发现&lt;/h4&gt;基于等变性散射的GNN在参数数量大幅减少的情况下，与其他基于等变性消息传递的GNN实现了相当的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的等变性散射变换是一种有效的几何图处理方法，可以在保持性能的同时显著减少参数数量。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种用于处理包含标量和向量节点特征的几何图的新型几何散射变换版本。这种新的散射变换具有关于刚体旋转平移（即SE(3)等变性）的理想对称性，并且可以整合到几何GNN框架中。我们通过实证表明，我们基于等变性散射的GNN在参数数量大幅减少的情况下，与其他基于等变性消息传递的GNN实现了相当的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决几何图神经网络中处理向量值节点特征的旋转等变性问题，以及传统消息传递网络存在的过平滑和下达问题。这个问题在现实中非常重要，因为许多应用（如分子结构分析、3D点云处理）需要处理具有几何对称性的数据，而向量特征（如位置、速度）的旋转等变性对于保持物理和几何意义至关重要。同时，解决过平滑和下达问题可以构建更深、更强大的图神经网络，提高模型性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有的几何散射变换工作（如Zou and Lerman 2019, Gama et al. 2018），这些工作使用扩散小波捕获图的多尺度几何信息，避免了消息传递网络的局限性。作者意识到现有方法主要针对标量特征，因此扩展其处理向量特征的能力，通过向量扩散映射（Singer and Wu 2012）设计向量扩散矩阵Q。为每个节点构建局部正交基，设计特殊的激活函数和扩散操作确保旋转等变性，并将标量和向量特征处理分离后结合，充分利用两种特征信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计向量扩散小波来处理向量值节点特征，同时保持旋转等变性；为每个节点构建局部正交基，实现不同局部坐标系间的转换；分离处理标量和向量特征后结合结果。整体流程包括：1)输入几何图和节点特征；2)构建向量扩散矩阵Q，通过SVD为每个节点创建局部坐标系；3)定义向量扩散小波；4)计算标量和向量特征的散射系数；5)分别应用MLP进行特征混合，向量特征还使用门控机制；6)提取向量不变量；7)根据任务类型应用预测头。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将几何散射变换扩展到处理向量值节点特征；2)通过理论证明确保旋转等变性；3)设计局部坐标系转换机制处理向量特征；4)以显著更少的参数实现与传统等变性GNN相当的性能。相比之前工作，不同之处在于：避免了消息传递网络的过平滑和下达问题；不依赖底层流形离散化假设；参数效率更高（少90%以上）；使用扩散小波而非局部捕获多尺度信息；分离处理标量和向量特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于向量扩散小波的等变性几何散射网络，首次将几何散射变换扩展到处理向量值节点特征，在保持旋转等变性的同时，以显著更少的参数实现了与传统等变性图神经网络相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel version of the geometric scattering transform forgeometric graphs containing scalar and vector node features. This newscattering transform has desirable symmetries with respect to rigid-bodyroto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into ageometric GNN framework. We empirically show that our equivariantscattering-based GNN achieves comparable performance to other equivariantmessage-passing-based GNNs at a fraction of the parameter count.</description>
      <author>example@mail.com (David R. Johnson, Rishabh Anand, Smita Krishnaswamy, Michael Perlmutter)</author>
      <guid isPermaLink="false">2510.01022v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks in Large Scale Wireless Communication Networks: Scalability Across Random Geometric Graphs</title>
      <link>http://arxiv.org/abs/2510.00896v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无线系统中图神经网络(GNNs)的可转移性特性，为随机几何图(RGGs)上的可转移性提供了正式的理论基础，并通过功率分配任务的数值实验进行了验证。&lt;h4&gt;背景&lt;/h4&gt;无线系统日益复杂，推动了从传统方法向基于学习解决方案的转变。图神经网络特别适合无线系统，因为无线网络可自然表示为图。尽管实证研究表明GNN-based无线策略能有效转移，但现有理论保证无法解释这一现象，因为多数研究假设密集图，而无线系统实际上是稀疏的。&lt;h4&gt;目的&lt;/h4&gt;为随机几何图(RGGs)上的GNN可转移性提供正式的理论基础，RGGs是无线网络的一种稀疏且广泛使用的模型。&lt;h4&gt;方法&lt;/h4&gt;通过理论分析和随机几何图模型建立GNN可转移性的理论基础，并通过功率分配这一基本资源管理任务的数值实验验证结果。&lt;h4&gt;主要发现&lt;/h4&gt;GNN模型在一个图上训练后可以有效地泛化到更大的图上，性能损失很小，这种现象在稀疏的无线网络模型中得到了理论支持。&lt;h4&gt;结论&lt;/h4&gt;本文为无线系统中GNN的可转移性提供了理论保证，填补了现有理论与实证观察之间的差距，证明了GNN在无线资源管理任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;随着无线系统复杂性的日益增加，传统方法向基于学习解决方案的转变正在加速。图神经网络(GNNs)特别适合于此，因为无线网络可以自然地表示为图。GNN的一个关键特性是可转移性：在一个图上训练的模型通常可以很好地泛化到更大的图上，性能损失很小。尽管实证研究表明基于GNN的无线策略能够有效转移，但现有的理论保证并不能解释这一现象。大多数工作关注密集图，其中节点度随网络规模扩展，这一假设在无线系统中不成立。在本工作中，我们为随机几何图(RGGs)上的可转移性提供了正式的理论基础，RGGs是无线网络的一种稀疏且广泛使用的模型。我们进一步通过功率分配这一基本资源管理任务的数值实验验证了我们的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing complexity of wireless systems has accelerated the move fromtraditional methods to learning-based solutions. Graph Neural Networks (GNNs)are especially well-suited here, since wireless networks can be naturallyrepresented as graphs. A key property of GNNs is transferability: modelstrained on one graph often generalize to much larger graphs with littleperformance loss. While empirical studies have shown that GNN-based wirelesspolicies transfer effectively, existing theoretical guarantees do not capturethis phenomenon. Most works focus on dense graphs where node degrees scale withnetwork size, an assumption that fails in wireless systems. In this work, weprovide a formal theoretical foundation for transferability on Random GeometricGraphs (RGGs), a sparse and widely used model of wireless networks. We furthervalidate our results through numerical experiments on power allocation, afundamental resource management task.</description>
      <author>example@mail.com (Romina Garcia Camargo, Zhiyang Wang, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2510.00896v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>LEAP: Local ECT-Based Learnable Positional Encodings for Graphs</title>
      <link>http://arxiv.org/abs/2510.00757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LEAP的新型图位置编码方法，结合了欧拉特征变换的可微分近似及其局部变体，用于解决标准图神经网络在理论和实践上的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)主要依赖于消息传递范式，其中节点迭代地从邻居聚合信息。然而，标准消息传递神经网络(MPNNs)面临着理论和实践上的局限性。图位置编码(PE)已成为解决这些局限性的有前途的方向。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的端到端可训练的图局部结构位置编码方法，以克服标准图神经网络的局限性。&lt;h4&gt;方法&lt;/h4&gt;结合欧拉特征变换的可微分近似(DECT)及其局部变体(ℓ-ECT)，提出名为LEAP的新型图位置编码方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于LEAP的编码在多个真实世界数据集和合成任务上表现出了潜力，特别是在提取拓扑特征方面。&lt;h4&gt;结论&lt;/h4&gt;LEAP-based编码可以作为图表示学习流水线的强大组件，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)主要依赖于消息传递范式，其中节点迭代地从邻居聚合信息。然而，标准消息传递神经网络(MPNNs)面临着众所周知的理论和实践局限性。图位置编码(PE)已成为解决这些局限性的一个有前途的方向。欧拉特征变换(Euler Characteristic Transform, ECT)是一种可有效计算的几何-拓扑不变量，用于表征形状和图形。在这项工作中，我们将ECT的可微分近似(DECT)及其局部变体(ℓ-ECT)相结合，提出了LEAP，这是一种新的端到端可训练的图局部结构位置编码。我们在多个真实世界数据集以及一个旨在测试其提取拓扑特征能力的合成任务上评估了我们的方法。我们的结果强调了基于LEAP的编码作为图表示学习流水线强大组件的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图神经网络(GNNs)在处理图数据时面临的理论和实践限制，特别是消息传递神经网络(MPNNs)的局限性，包括在高直径图中丢失信号和无法有效利用子结构信息等问题。这个问题在现实中非常重要，因为图是许多科学领域处理二元关系的主要模态，而改进的图表示学习方法对图表示学习领域的发展至关重要，能够帮助更好地理解和分析各种复杂网络结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从MPNNs的局限性出发，受到Transformer架构中位置编码的启发，开始关注图的位置编码(PEs)和结构编码(SEs)。注意到现有方法大多基于几何或拓扑单一方面的信息，表达能力有限，因此决定结合两者优势。作者借鉴了现有的Euler Characteristic Transform (ECT)这一几何-拓扑不变量，以及可微分ECT近似(DECT)和局部变体(ℓ-ECT)，同时参考了Random Walk Positional Encoding (RWPE)和Laplacian Positional Encoding (LaPE)等现有图位置编码方法，最终设计出LEAP方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; LEAP的核心思想是利用局部欧拉特征变换(ℓ-ECT)创建一种可学习的图位置编码，结合了几何和拓扑信息。整体实现流程为：1)对图中的每个节点，计算其m跳子图；2)对子图中的节点特征进行归一化处理(均值中心化并除以最大范数)；3)计算不同方向和阈值下的可微分ECT近似，得到矩阵表示；4)使用可学习的投影函数将ECT矩阵映射到低维空间，得到节点的位置编码向量。这种方法能够捕捉图的局部结构信息，并支持端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出LEAP，一种基于局部ECT的可学习图位置编码；2)结合几何和拓扑信息提高表达能力；3)实现端到端可训练性；4)提出多种ECT投影策略(线性投影、一维卷积、DeepSets、注意力机制等)；5)支持学习方向向量而非仅使用固定方向。相比之前的工作，LEAP是可学习的而非静态预处理，专门设计为局部结构编码而非全局，能够在节点特征不具信息性时仍捕获结构信息，并提供多种投影策略选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了LEAP，一种基于局部欧拉特征变换的可学习图位置编码方法，它结合了几何和拓扑信息，能够在多种图神经网络架构和任务中提供强大的结构表示，并实现了端到端的可训练性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) largely rely on the message-passing paradigm,where nodes iteratively aggregate information from their neighbors. Yet,standard message passing neural networks (MPNNs) face well-documentedtheoretical and practical limitations. Graph positional encoding (PE) hasemerged as a promising direction to address these limitations. The EulerCharacteristic Transform (ECT) is an efficiently computablegeometric-topological invariant that characterizes shapes and graphs. In thiswork, we combine the differentiable approximation of the ECT (DECT) and itslocal variant ($\ell$-ECT) to propose LEAP, a new end-to-end trainable localstructural PE for graphs. We evaluate our approach on multiple real-worlddatasets as well as on a synthetic task designed to test its ability to extracttopological features. Our results underline the potential of LEAP-basedencodings as a powerful component for graph representation learning pipelines.</description>
      <author>example@mail.com (Juan Amboage, Ernst Röell, Patrick Schnider, Bastian Rieck)</author>
      <guid isPermaLink="false">2510.00757v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchy-Aware Neural Subgraph Matching with Enhanced Similarity Measure</title>
      <link>http://arxiv.org/abs/2510.00402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Knowledge and Data Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NC-Iso的新型GNN架构，用于解决子图匹配问题，解决了现有方法在处理图对尺度差异和特征相对位置方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;子图匹配具有挑战性，需要耗时的组合搜索。基于图神经网络的方法虽然显著缩短了响应时间，但存在两个主要问题：1) 编码过程中图对间存在尺度差异，因只关注特征计数而忽略节点根子树中特征的相对位置；2) 铰链距离度量对匹配图对缺乏判别力，影响排名应用。&lt;h4&gt;目的&lt;/h4&gt;提出NC-Iso架构，解决现有子图匹配方法的局限性，提供更具判别力的神经子图匹配解决方案，用于子图检索任务。&lt;h4&gt;方法&lt;/h4&gt;NC-Iso通过构建节点根子树内相邻层级间的层次依赖关系保留特征相对位置，确保匹配图对保持一致层次结构并符合特征计数包含约束。同时引入相似优势比率增强度量，量化图对间相似性对差异性的优势，提升匹配对的排名能力。&lt;h4&gt;主要发现&lt;/h4&gt;在九个数据集上的经验结果验证了NC-Iso的有效性、泛化能力、可扩展性和可转移性，同时保持时间效率，为子图检索提供了更具判别力的神经子图匹配解决方案。&lt;h4&gt;结论&lt;/h4&gt;NC-Iso通过保留特征相对位置和引入新的相似性度量，解决了现有子图匹配方法中的关键问题，在保持时间效率的同时提高了子图匹配的准确性。&lt;h4&gt;翻译&lt;/h4&gt;子图匹配具有挑战性，因为它需要耗时的组合搜索。最近的基于图神经网络（GNN）的方法通过使用GNN编码器提取图信息和使用铰链距离度量确保嵌入空间中的包含约束来解决这个问题。这些方法显著缩短了响应时间，使它们成为子图检索的 promising 解决方案。然而，它们在编码过程中图对之间存在尺度差异，因为它们只关注特征计数而忽略了节点根子树中特征的相对位置，导致包含约束被干扰和错误预测。此外，它们的铰链距离度量对匹配的图对缺乏判别力，阻碍了排名应用。我们提出了NC-Iso，一种用于神经子图匹配的新型GNN架构。NC-Iso通过构建节点根子树内相邻层级之间的层次依赖关系来保留特征的相对位置，确保匹配的图对保持一致的层次结构，同时符合特征计数中的包含约束。为了增强匹配对的排名能力，我们引入了一种新颖的相似优势比率增强度量，它量化了图对之间相似性对差异性的优势。在九个数据集上的经验结果验证了NC-Iso的有效性、泛化能力、可扩展性和可转移性，同时保持时间效率，为子图检索提供了更具判别力的神经子图匹配解决方案。代码可在https://github.com/liuzhouyang/NC-Iso获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Subgraph matching is challenging as it necessitates time-consumingcombinatorial searches. Recent Graph Neural Network (GNN)-based approachesaddress this issue by employing GNN encoders to extract graph information andhinge distance measures to ensure containment constraints in the embeddingspace. These methods significantly shorten the response time, making thempromising solutions for subgraph retrieval. However, they suffer from scaledifferences between graph pairs during encoding, as they focus on featurecounts but overlook the relative positions of features within node-rootedsubtrees, leading to disturbed containment constraints and false predictions.Additionally, their hinge distance measures lack discriminative power formatched graph pairs, hindering ranking applications. We propose NC-Iso, a novelGNN architecture for neural subgraph matching. NC-Iso preserves the relativepositions of features by building the hierarchical dependencies betweenadjacent echelons within node-rooted subtrees, ensuring matched graph pairsmaintain consistent hierarchies while complying with containment constraints infeature counts. To enhance the ranking ability for matched pairs, we introducea novel similarity dominance ratio-enhanced measure, which quantifies thedominance of similarity over dissimilarity between graph pairs. Empiricalresults on nine datasets validate the effectiveness, generalization ability,scalability, and transferability of NC-Iso while maintaining time efficiency,offering a more discriminative neural subgraph matching solution for subgraphretrieval. Code available at https://github.com/liuzhouyang/NC-Iso.</description>
      <author>example@mail.com (Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Menghan Jia, Dongsheng Li)</author>
      <guid isPermaLink="false">2510.00402v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification</title>
      <link>http://arxiv.org/abs/2510.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GDLNN，一种结合领域特定编程语言GDL与神经网络的新型图机器学习架构，用于图分类任务。GDLNN的核心优势在于其GDL层能够生成具有表现力和可解释性的图表示，在多个基准数据集上表现出色，且解释成本较低。&lt;h4&gt;背景&lt;/h4&gt;图机器学习在图分类任务中面临可解释性和性能平衡的挑战，现有方法如GNNs虽然性能良好但解释性不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图机器学习架构，既能保持高分类准确率，又能提供可解释的图表示，使现有模型解释技术可直接应用。&lt;h4&gt;方法&lt;/h4&gt;提出GDLNN架构，结合名为GDL的领域特定编程语言与神经网络，核心是GDL层，用于生成表达性强且可解释的图表示。&lt;h4&gt;主要发现&lt;/h4&gt;基于GDL的表示在大多数图分类基准数据集上实现高准确率，优于主流图学习方法如GNNs；应用现有模型解释技术能产生高质量的预测解释；包含解释成本时，GDLNN的成本较低。&lt;h4&gt;结论&lt;/h4&gt;GDLNN通过结合领域特定编程语言与神经网络，成功实现了高准确率和良好可解释性的图分类，为图机器学习提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了GDLNN，一种用于图分类任务的新型图机器学习架构。GDLNN将一种名为GDL的领域特定编程语言与神经网络相结合。GDLNN的主要优势在于其GDL层，能够生成具有表现力和可解释性的图表示。由于图表示具有可解释性，现有的模型解释技术可以直接应用于解释GDLNN的预测。我们的评估显示，基于GDL的表示在大多数图分类基准数据集上实现了高准确率，优于主流的图学习方法如GNNs。应用现有的模型解释技术也能产生高质量的GDLNN预测解释。此外，当包含解释成本时，GDLNN的成本较低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GDLNN, a new graph machine learning architecture, for graphclassification tasks. GDLNN combines a domain-specific programming language,called GDL, with neural networks. The main strength of GDLNN lies in its GDLlayer, which generates expressive and interpretable graph representations.Since the graph representation is interpretable, existing model explanationtechniques can be directly applied to explain GDLNN's predictions. Ourevaluation shows that the GDL-based representation achieves high accuracy onmost graph classification benchmark datasets, outperforming dominant graphlearning methods such as GNNs. Applying an existing model explanation techniquealso yields high-quality explanations of GDLNN's predictions. Furthermore, thecost of GDLNN is low when the explanation cost is included.</description>
      <author>example@mail.com (Minseok Jeon, Seunghyun Park)</author>
      <guid isPermaLink="false">2510.00374v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction</title>
      <link>http://arxiv.org/abs/2510.00080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SoREX是一种自解释的基于图神经网络的社交推荐框架，通过双塔结构和朋友推荐增强，独立建模社交关系和用户-项交互，并提供新颖的自我路径提取方法来生成解释，在多个基准数据集上验证了其预测准确性和解释有效性。&lt;h4&gt;背景&lt;/h4&gt;社交推荐已被证明可以通过利用社交网络解决用户-项交互建模中的数据稀疏性问题。近年来，图神经网络（GNNs）的整合提高了社交推荐算法的预测准确性，但许多基于GNN的社交推荐方法缺乏为其预测提供有意义解释的能力。&lt;h4&gt;目的&lt;/h4&gt;引入SoREX，一种自解释的基于GNN的社交推荐框架，以解决现有方法缺乏预测解释能力的问题。&lt;h4&gt;方法&lt;/h4&gt;SoREX采用由朋友推荐增强的双塔框架，独立建模社交关系和用户-项交互，同时联合优化辅助任务以强化社交信号。为提供解释，提出了一种新颖的自我路径提取方法，将目标用户的自我网络转换为多跳自我路径集合，提取特定因素和候选感知的自我路径子集作为解释，并进行解释重新聚合将解释与下游预测明确关联。&lt;h4&gt;主要发现&lt;/h4&gt;在四个广泛采用的基准数据集上的实验验证了SoREX在预测准确性方面的有效性。定性和定量分析确认了SoREX中提取的解释的有效性。&lt;h4&gt;结论&lt;/h4&gt;SoREX是一个有效的自解释GNN社交推荐框架，不仅具有高预测准确性，还能提供有意义的解释，使推荐结果更加透明和可信。&lt;h4&gt;翻译&lt;/h4&gt;社交推荐已被证明通过利用社交网络解决用户-项交互建模中的数据稀疏性问题。最近图神经网络（GNNs）的整合进一步提高了当代社交推荐算法的预测准确性。然而，社交推荐中的许多基于GNN的方法缺乏为其预测提供有意义解释的能力。在本研究中，我们通过引入SoREX（一种自解释的基于GNN的社交推荐框架）来应对这一挑战。SoREX采用一个由朋友推荐增强的双塔框架，独立建模社交关系和用户-项交互，同时联合优化辅助任务以强化社交信号。为提供解释，我们提出了一种新颖的自我路径提取方法。此方法涉及将目标用户的自我网络转换为多跳自我路径集合，从中我们提取特定因素和候选感知的自我路径子集作为解释。这一过程通过复杂的子结构分析，促进了不同候选项目之间详细比较解释的总结。此外，我们进行了解释重新聚合，将解释与下游预测明确关联，赋予我们的框架内在的自解释能力。在四个广泛采用的基准数据集上进行的大量实验验证了SoREX在预测准确性方面的有效性。此外，定性和定量分析确认了SoREX中提取解释的有效性。我们的代码和数据可在https://github.com/antman9914/SoREX获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social recommendation has been proven effective in addressing data sparsityin user-item interaction modeling by leveraging social networks. The recentintegration of Graph Neural Networks (GNNs) has further enhanced predictionaccuracy in contemporary social recommendation algorithms. However, manyGNN-based approaches in social recommendation lack the ability to furnishmeaningful explanations for their predictions. In this study, we confront thischallenge by introducing SoREX, a self-explanatory GNN-based socialrecommendation framework. SoREX adopts a two-tower framework enhanced by friendrecommendation, independently modeling social relations and user-iteminteractions, while jointly optimizing an auxiliary task to reinforce socialsignals. To offer explanations, we propose a novel ego-path extractionapproach. This method involves transforming the ego-net of a target user into acollection of multi-hop ego-paths, from which we extract factor-specific andcandidate-aware ego-path subsets as explanations. This process facilitates thesummarization of detailed comparative explanations among different candidateitems through intricate substructure analysis. Furthermore, we conductexplanation re-aggregation to explicitly correlate explanations with downstreampredictions, imbuing our framework with inherent self-explainability.Comprehensive experiments conducted on four widely adopted benchmark datasetsvalidate the effectiveness of SoREX in predictive accuracy. Additionally,qualitative and quantitative analyses confirm the efficacy of the extractedexplanations in SoREX. Our code and data are available athttps://github.com/antman9914/SoREX.</description>
      <author>example@mail.com (Hanze Guo, Yijun Ma, Xiao Zhou)</author>
      <guid isPermaLink="false">2510.00080v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction</title>
      <link>http://arxiv.org/abs/2509.25514v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了AGNOMIN，一种用于剥离二进制文件中多标签函数名预测的新型架构无关方法。该方法通过构建功能增强的分层图和分层图神经网络，实现了跨架构一致的函数表示，显著提高了函数名预测的精度和召回率，并在实际安全应用中得到了验证。&lt;h4&gt;背景&lt;/h4&gt;函数名预测对于理解软件逆向工程中的剥离二进制文件至关重要，是进行后续漏洞分析和修复的关键步骤。然而，现有方法通常面临特定架构限制、数据稀缺和多样化命名约定的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为AGNOMIN的新型架构无关方法，用于剥离二进制文件中的多标签函数名预测，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;AGNOMIN构建了功能增强的分层图（FEHGs），结合控制流图、函数调用图和动态学习的PCode特征。分层图神经网络处理这种增强结构，生成跨架构一致的函数表示。对于函数名预测，采用受Renée启发的解码器，增强了基于注意力的头部层和算法改进。&lt;h4&gt;主要发现&lt;/h4&gt;在包含三种架构的9000个ELF可执行文件的综合性数据集上评估AGNOMIN，结果显示其性能优于最先进的方法，在测试数据集中精度提高了27.17%，召回率提高了55.86%。此外，AGNOMIN在未见过的架构上泛化良好，召回率比最接近的基线高出5.89%。&lt;h4&gt;结论&lt;/h4&gt;AGNOMIN的实际效用已通过安全黑客马拉松得到验证，成功帮助逆向工程师分析和修补不同架构上的易受攻击的二进制文件，证明了其在实际安全应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;函数名预测对于理解软件逆向工程中的剥离二进制文件至关重要，这是进行后续漏洞分析和修复的关键步骤。然而，现有方法通常面临特定架构限制、数据稀缺和多样化命名约定的挑战。我们提出了AGNOMIN，一种用于剥离二进制文件中多标签函数名预测的新型架构无关方法。AGNOMIN构建了功能增强的分层图（FEHGs），结合了控制流图、函数调用图和动态学习的PCode特征。分层图神经网络处理这种增强结构，以生成跨架构一致的函数表示，这对可扩展的安全评估至关重要。对于函数名预测，AGNOMIN采用了受Renée启发的解码器，增强了基于注意力的头部层和算法改进。我们在包含三种架构的9000个ELF可执行文件的综合性数据集上评估AGNOMIN，展示了其相比最先进方法的优越性能，在测试数据集中精度提高了27.17%，召回率提高了55.86%。此外，AGNOMIN在未见过的架构上泛化良好，召回率比最接近的基线高出5.89%。AGNOMIN的实际效用已通过安全黑客马拉松得到验证，成功帮助逆向工程师分析和修补不同架构上的易受攻击的二进制文件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Function name prediction is crucial for understanding stripped binaries insoftware reverse engineering, a key step for \textbf{enabling subsequentvulnerability analysis and patching}. However, existing approaches oftenstruggle with architecture-specific limitations, data scarcity, and diversenaming conventions. We present AGNOMIN, a novel architecture-agnostic approachfor multi-label function name prediction in stripped binaries. AGNOMIN buildsFeature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs,Function Call Graphs, and dynamically learned \texttt{PCode} features. Ahierarchical graph neural network processes this enriched structure to generateconsistent function representations across architectures, vital for\textbf{scalable security assessments}. For function name prediction, AGNOMINemploys a Ren\'ee-inspired decoder, enhanced with an attention-based head layerand algorithmic improvements.  We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executablebinaries across three architectures, demonstrating its superior performancecompared to state-of-the-art approaches, with improvements of up to 27.17\% inprecision and 55.86\% in recall across the testing dataset. Moreover, AGNOMINgeneralizes well to unseen architectures, achieving 5.89\% higher recall thanthe closest baseline. AGNOMIN's practical utility has been validated throughsecurity hackathons, where it successfully aided reverse engineers in analyzingand patching vulnerable binaries across different architectures.</description>
      <author>example@mail.com (Yonatan Gizachew Achamyeleh, Tongtao Zhang, Joshua Hyunki Kim, Gabriel Garcia, Shih-Yuan Yu, Anton Kocheturov, Mohammad Abdullah Al Faruque)</author>
      <guid isPermaLink="false">2509.25514v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning</title>
      <link>http://arxiv.org/abs/2510.02091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了大型语言模型不同深度的层在各种评估协议、任务类别和模型架构下的利用情况，发现深度利用具有高度异质性和上下文依赖性。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，大型语言模型的深层对表征学习的贡献很小，通常可以被移除而不会造成显著的性能损失。然而，这些说法通常是基于狭隘的评估得出的，可能忽略了模型行为的重要方面。&lt;h4&gt;目的&lt;/h4&gt;对大型语言模型在不同维度上的深度利用进行系统研究，包括评估协议、任务类别和模型架构。&lt;h4&gt;方法&lt;/h4&gt;通过分析不同评估协议下模型各层的表现，研究深度利用的异质性和上下文依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在基于似然的指标且不涉及生成的评估中，修剪大部分层可以保持性能，只有最初几个层是关键的；2. 在基于生成的评估中，中间层和深层在促进推理和保持长程连贯性中不可或缺；3. 知识和检索集中在浅层组件中，而推理准确性则严重依赖于深层，但可以通过蒸馏来重塑。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型中的深度利用高度异质且依赖于上下文，因此在解释和压缩大型模型时需要考虑任务、指标和模型感知的视角。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，大型语言模型的深层对表征学习的贡献很小，通常可以被移除而不会造成显著的性能损失。然而，这些说法通常是基于狭隘的评估得出的，可能忽略了模型行为的重要方面。在这项工作中，我们对不同维度上的深度利用进行了系统研究，包括评估协议、任务类别和模型架构。我们的分析证实，非常深的层通常比早期层效果较差，但它们的贡献会随着评估设置而有显著变化。在基于似然的指标且不涉及生成的评估中，修剪大部分层可以保持性能，只有最初几个层是关键的。相比之下，基于生成的评估揭示了中间层和深层在促进推理和保持长程连贯性中不可或缺的作用。我们还发现，知识和检索集中在浅层组件中，而推理准确性则严重依赖于深层——但可以通过蒸馏来重塑。这些结果表明，大型语言模型中的深度利用高度异质且依赖于上下文，强调了在解释和压缩大型模型时需要考虑任务、指标和模型感知的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies suggest that the deeper layers of Large Language Models (LLMs)contribute little to representation learning and can often be removed withoutsignificant performance loss. However, such claims are typically drawn fromnarrow evaluations and may overlook important aspects of model behavior. Inthis work, we present a systematic study of depth utilization across diversedimensions, including evaluation protocols, task categories, and modelarchitectures. Our analysis confirms that very deep layers are generally lesseffective than earlier ones, but their contributions vary substantially withthe evaluation setting. Under likelihood-based metrics without generation,pruning most layers preserves performance, with only the initial few beingcritical. By contrast, generation-based evaluation uncovers indispensable rolesfor middle and deeper layers in enabling reasoning and maintaining long-rangecoherence. We further find that knowledge and retrieval are concentrated inshallow components, whereas reasoning accuracy relies heavily on deeper layers-- yet can be reshaped through distillation. These results highlight that depthusage in LLMs is highly heterogeneous and context-dependent, underscoring theneed for task-, metric-, and model-aware perspectives in both interpreting andcompressing large models.</description>
      <author>example@mail.com (Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu)</author>
      <guid isPermaLink="false">2510.02091v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data</title>
      <link>http://arxiv.org/abs/2510.02017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 - Reliable ML Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种针对表格数据的对比学习框架，通过战略性地选择正样本对并结合监督和自监督对比学习，显著减少了偏见，同时保持了较高的准确率，并在各种下游任务中表现良好。&lt;h4&gt;背景&lt;/h4&gt;随着AI系统日益融入日常生活，开发公平无偏的模型变得至关重要。学习公平稳健的表示已被证明是有效消除算法偏见并提高公平性的有力方法，尽管这些方法在表格数据应用中的公平性问题仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一个专门设计用于解决表格数据中偏见和学习公平表示的对比学习框架。&lt;h4&gt;方法&lt;/h4&gt;通过战略性地选择正样本对，并结合监督和自监督对比学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;与现有表格数据对比学习模型相比，该方法显著减少了偏见，在最小化准确率权衡方面有效减轻偏见，且利用学习到的公平表示在各种下游任务中表现良好。&lt;h4&gt;结论&lt;/h4&gt;对比学习框架能有效解决表格数据中的偏见问题，学习公平表示可在保持预测任务所需基本信息的同时提高算法公平性。&lt;h4&gt;翻译&lt;/h4&gt;随着AI系统越来越融入日常生活，开发公平无偏的模型变得至关重要。考虑AI系统的社会影响不仅是技术挑战，也是道德义务。正如多项研究所示，学习公平稳健的表示已被证明是有效消除算法偏见并提高公平性的有力方法，同时保持预测任务所需的基本信息。表示学习框架，特别是利用自监督和对比学习的框架，已在各种领域表现出更强的稳健性和泛化能力。尽管越来越有兴趣将这些方法应用于表格数据，但这些学习表示中的公平性问题仍未得到充分探索。在本研究中，我们引入了一个专门设计用于解决表格数据中偏见和学习公平表示的对比学习框架。通过战略性地选择正样本对并采用监督和自监督对比学习，我们显著减少了与现有表格数据最先进对比学习模型相比的偏见。我们的结果证明了该方法在最小化准确率权衡方面减轻偏见的有效性，以及利用学习到的公平表示在各种下游任务中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As AI systems become more embedded in everyday life, the development of fairand unbiased models becomes more critical. Considering the social impact of AIsystems is not merely a technical challenge but a moral imperative. Asevidenced in numerous research studies, learning fair and robustrepresentations has proven to be a powerful approach to effectively debiasingalgorithms and improving fairness while maintaining essential information forprediction tasks. Representation learning frameworks, particularly those thatutilize self-supervised and contrastive learning, have demonstrated superiorrobustness and generalizability across various domains. Despite the growinginterest in applying these approaches to tabular data, the issue of fairness inthese learned representations remains underexplored. In this study, weintroduce a contrastive learning framework specifically designed to addressbias and learn fair representations in tabular datasets. By strategicallyselecting positive pair samples and employing supervised and self-supervisedcontrastive learning, we significantly reduce bias compared to existingstate-of-the-art contrastive learning models for tabular data. Our resultsdemonstrate the efficacy of our approach in mitigating bias with minimumtrade-off in accuracy and leveraging the learned fair representations invarious downstream tasks.</description>
      <author>example@mail.com (Aida Tayebi, Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay)</author>
      <guid isPermaLink="false">2510.02017v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Quantum-Classical Walks for Graph Representation Learning in Community Detection</title>
      <link>http://arxiv.org/abs/2510.01918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. Accepted at the 2025 IEEE International Conference on  Quantum Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于混合量子-经典行走的量子启发式图表示学习算法，用于解决传统方法在处理复杂图关系时的局限性。&lt;h4&gt;背景&lt;/h4&gt;图表示学习已成为分析生物系统、社交网络和数据分析等领域复杂网络化数据的核心技术，但传统方法难以捕捉具有幂律分布或层次结构等非平凡结构特性的复杂图中的关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型量子启发式算法，以克服传统图表示学习方法在捕捉复杂图内 intricate 关系方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出利用混合量子-经典行走的算法，结合量子和经典动力学的优势，使行走者能够同时探索图中的高度局部和远距离连接。&lt;h4&gt;主要发现&lt;/h4&gt;网络社区检测案例研究的初步结果表明，这种混合动态使算法能够有效适应复杂的图拓扑结构。&lt;h4&gt;结论&lt;/h4&gt;该混合量子-经典行走算法为图表示学习任务提供了强大且通用的解决方案，特别适用于处理具有复杂结构特性的图。&lt;h4&gt;翻译&lt;/h4&gt;图表示学习已成为分析生物系统、社交网络和数据分析等不同领域中复杂网络化数据的核心技术。传统图表示学习方法通常难以捕捉复杂图中的 intricate 关系，特别是那些表现出幂律分布或层次结构等非平凡结构特性的图。本文介绍了一种用于图表示学习的量子启发式新算法，利用混合量子-经典行走来克服这些局限性。我们的方法结合了量子和经典动力学的优点，使行走者能够同时探索图中的高度局部和远距离连接。在网络社区检测的案例研究中，初步结果表明这种混合动态使算法能够有效适应复杂的图拓扑结构，为图表示学习任务提供了强大且通用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Representation Learning (GRL) has emerged as a cornerstone techniquefor analysing complex, networked data across diverse domains, includingbiological systems, social networks, and data analysis. Traditional GRL methodsoften struggle to capture intricate relationships within complex graphs,particularly those exhibiting non-trivial structural properties such aspower-law distributions or hierarchical structures. This paper introduces anovel quantum-inspired algorithm for GRL, utilizing hybrid Quantum-ClassicalWalks to overcome these limitations. Our approach combines the benefits of bothquantum and classical dynamics, allowing the walker to simultaneously exploreboth highly local and far-reaching connections within the graph. Preliminaryresults for a case study in network community detection shows that this hybriddynamic enables the algorithm to adapt effectively to complex graph topologies,offering a robust and versatile solution for GRL tasks.</description>
      <author>example@mail.com (Adrián Marın, Mauricio Soto-Gomez, Giorgio Valentini, Elena Casiraghi, Carlos Cano, Daniel Manzano)</author>
      <guid isPermaLink="false">2510.01918v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations Through Contrastive Neural Model Checking</title>
      <link>http://arxiv.org/abs/2510.01853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比神经模型检查(CNML)方法，将模型检查任务作为指导信号来学习对齐表征，在形式验证领域探索了表征学习的应用。&lt;h4&gt;背景&lt;/h4&gt;模型检查是验证安全关键系统符合形式规范的关键技术，最近深度学习的应用显示出了前景。然而，表征学习在形式验证领域仍然探索不足。&lt;h4&gt;目的&lt;/h4&gt;利用模型检查任务作为学习对齐表征的指导信号，探索表征学习在形式验证领域的应用。&lt;h4&gt;方法&lt;/h4&gt;提出对比神经模型检查(CNML)方法，通过自监督对比目标将逻辑规范和系统共同嵌入到一个共享的潜在空间中。&lt;h4&gt;主要发现&lt;/h4&gt;在受工业启发的检索任务中，CNML在跨模态和模态内部设置中都明显优于算法和神经基线；学习到的表征能有效地迁移到下游任务并推广到更复杂的公式。&lt;h4&gt;结论&lt;/h4&gt;模型检查可以作为学习形式语言表征的目标。&lt;h4&gt;翻译&lt;/h4&gt;模型检查是验证安全关键系统是否符合形式规范的关键技术，最近深度学习的应用显示出了前景。然而，尽管表征学习在视觉和语言领域无处不在，但在形式验证领域仍然探索不足。作者引入了对比神经模型检查(CNML)这一新方法，利用模型检查任务作为学习对齐表征的指导信号。CNML通过自监督对比目标将逻辑规范和系统共同嵌入到一个共享的潜在空间中。在受工业启发的检索任务中，CNML在跨模态和模态内部设置中都明显优于算法和神经基线。作者进一步展示了学习到的表征有效地迁移到下游任务并推广到更复杂的公式。这些发现表明模型检查可以作为学习形式语言表征的目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model checking is a key technique for verifying safety-critical systemsagainst formal specifications, where recent applications of deep learning haveshown promise. However, while ubiquitous for vision and language domains,representation learning remains underexplored in formal verification. Weintroduce Contrastive Neural Model Checking (CNML), a novel method thatleverages the model checking task as a guiding signal for learning alignedrepresentations. CNML jointly embeds logical specifications and systems into ashared latent space through a self-supervised contrastive objective. Onindustry-inspired retrieval tasks, CNML considerably outperforms bothalgorithmic and neural baselines in cross-modal and intra-modal settings.Wefurther show that the learned representations effectively transfer todownstream tasks and generalize to more complex formulas. These findingsdemonstrate that model checking can serve as an objective for learningrepresentations for formal languages.</description>
      <author>example@mail.com (Vladimir Krsmanovic, Matthias Cosler, Mohamed Ghanem, Bernd Finkbeiner)</author>
      <guid isPermaLink="false">2510.01853v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representation Regularization for Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2510.01711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Robot State-aware Contrastive Loss (RS-CL)的表示正则化方法，用于增强VLA模型在机器人操作中的性能，通过将视觉语言模型表示与机器人信号对齐来提高操作准确性。&lt;h4&gt;背景&lt;/h4&gt;VLA模型已经通过利用预训练视觉语言模型的丰富表示展示了在机器人操作方面的能力，但它们的表示仍然不够优化，缺乏对机器人信号（如控制动作和本体感受状态）的敏感性。&lt;h4&gt;目的&lt;/h4&gt;解决VLA模型表示不够优化的问题，通过引入RS-CL来弥合视觉语言模型表示与机器人信号之间的差距，使表示更接近机器人的本体感受状态。&lt;h4&gt;方法&lt;/h4&gt;提出了Robot State-aware Contrastive Loss (RS-CL)，一种简单有效的VLA模型表示正则化方法。它使用状态之间的相对距离作为软监督，将表示更紧密地与机器人的本体感受状态对齐。RS-CL补充了原始的动作预测目标，同时保持轻量级且完全兼容标准的VLA训练流程。&lt;h4&gt;主要发现&lt;/h4&gt;RS-CL显著提高了最先进VLA模型的操作性能；在RoboCasa-Kitchen的拾取和放置任务中，将最先进的结果从30.8%提高到41.5%，通过在抓取和放置过程中实现更精确的定位；在具有挑战性的真实机器人操作任务中，将成功率从45.0%提高到58.3%。&lt;h4&gt;结论&lt;/h4&gt;RS-CL是一种简单而有效的方法，能够增强控制相关表示的学习，显著提升VLA模型在机器人操作任务中的性能，同时保持轻量级和与标准训练流程的兼容性。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作模型已经通过利用预训练视觉语言模型的丰富表示展示了在机器人操作方面的能力。然而，它们的表示被认为仍然不够优化，缺乏对机器人信号（如控制动作和本体感受状态）的敏感性。为了解决这个问题，我们引入了机器人状态感知对比损失，这是一种简单有效的视觉-语言-动作模型表示正则化方法，旨在弥合视觉语言模型表示与机器人信号之间的差距。特别是，该损失通过使用状态之间的相对距离作为软监督，使表示更紧密地与机器人的本体感受状态对齐。除了原始的动作预测目标外，机器人状态感知对比损失有效地增强了控制相关表示的学习，同时保持轻量级并完全兼容标准的视觉-语言-动作训练流程。我们的实证结果表明，机器人状态感知对比损失显著提高了最先进视觉-语言-动作模型的操作性能；它通过在抓取和放置过程中实现更精确的定位，将RoboCasa-Kitchen中拾取和放置任务的最先进结果从30.8%提高到41.5%，并将具有挑战性的真实机器人操作任务的成功率从45.0%提高到58.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have shown its capabilities in robotmanipulation by leveraging rich representations from pre-trainedVision-Language Models (VLMs). However, their representations arguably remainsuboptimal, lacking sensitivity to robotic signals such as control actions andproprioceptive states. To address the issue, we introduce Robot State-awareContrastive Loss (RS-CL), a simple and effective representation regularizationfor VLA models, designed to bridge the gap between VLM representations androbotic signals. In particular, RS-CL aligns the representations more closelywith the robot's proprioceptive states, by using relative distances between thestates as soft supervision. Complementing the original action predictionobjective, RS-CL effectively enhances control-relevant representation learning,while being lightweight and fully compatible with standard VLA trainingpipeline. Our empirical results demonstrate that RS-CL substantially improvesthe manipulation performance of state-of-the-art VLA models; it pushes theprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,through more accurate positioning during grasping and placing, and boostssuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.</description>
      <author>example@mail.com (Taeyoung Kim, Jimin Lee, Myungkyu Koo, Dongyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin)</author>
      <guid isPermaLink="false">2510.01711v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery</title>
      <link>http://arxiv.org/abs/2510.01662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为离散面部编码(DFE)的无监督、数据驱动方法，用于从3D网格序列中学习面部表情的紧凑且可解释的字典。&lt;h4&gt;背景&lt;/h4&gt;面部表情分析对理解人类行为至关重要，但现有的面部动作编码系统(FACS)存在覆盖范围有限和人工标注成本高的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代方案，克服FACS的局限性，提供更精确、更高效的面部表情分析方法。&lt;h4&gt;方法&lt;/h4&gt;使用3D Morphable Model提取身份不变的表情特征，然后通过Residual Vector Quantized Variational Autoencoder(RVQ-VAE)编码这些特征，生成离散令牌序列，每个令牌捕捉特定的面部变形模式。&lt;h4&gt;主要发现&lt;/h4&gt;DFE比FACS和其他面部编码替代方案捕获更精确的面部行为；在压力检测、性格预测和抑郁检测三个心理任务上表现优于基于FACS的流程和先进的图像视频表征模型；覆盖更广泛的面部表情显示。&lt;h4&gt;结论&lt;/h4&gt;DFE是FACS在心理和情感计算应用中的一种可扩展且有效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;面部表情分析对理解人类行为至关重要，但现有的编码系统如面部动作编码系统(FACS)受限于有限的覆盖范围和昂贵的手工标注。在这项工作中，我们引入了离散面部编码(DFE)，这是一种无监督、数据驱动的替代方案，通过残差向量量化变分自编码器(RVQ-VAE)从3D网格序列中学习紧凑且可解释的面部表情字典。我们的方法首先使用3D可变形模型(3DMM)从图像中提取身份不变的表情特征，有效地分离了头部姿态和面部几何等因素。然后我们使用RVQ-VAE对这些特征进行编码，从共享码本中生成离散令牌序列，其中每个令牌捕获特定的、可重用的面部变形模式，这些模式共同构成整体表情。通过大量实验，我们证明离散面部编码比FACS和其他面部编码替代方案捕获更精确的面部行为。我们在三个高级心理任务上评估了我们表征的实用性：压力检测、性格预测和抑郁检测。使用在学习的令牌之上构建的简单词袋模型，我们的系统持续优于基于FACS的流程以及强大的图像和视频表征学习模型，如掩码自编码器。进一步的分析显示，我们的表征覆盖了更广泛的面部表情显示，突显了其作为心理和情感计算应用中FACS的可扩展且有效替代方案的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial expression analysis is central to understanding human behavior, yetexisting coding systems such as the Facial Action Coding System (FACS) areconstrained by limited coverage and costly manual annotation. In this work, weintroduce Discrete Facial Encoding (DFE), an unsupervised, data-drivenalternative of compact and interpretable dictionary of facial expressions from3D mesh sequences learned through a Residual Vector Quantized VariationalAutoencoder (RVQ-VAE). Our approach first extracts identity-invariantexpression features from images using a 3D Morphable Model (3DMM), effectivelydisentangling factors such as head pose and facial geometry. We then encodethese features using an RVQ-VAE, producing a sequence of discrete tokens from ashared codebook, where each token captures a specific, reusable facialdeformation pattern that contributes to the overall expression. Throughextensive experiments, we demonstrate that Discrete Facial Encoding capturesmore precise facial behaviors than FACS and other facial encoding alternatives.We evaluate the utility of our representation across three high-levelpsychological tasks: stress detection, personality prediction, and depressiondetection. Using a simple Bag-of-Words model built on top of the learnedtokens, our system consistently outperforms both FACS-based pipelines andstrong image and video representation learning models such as MaskedAutoencoders. Further analysis reveals that our representation covers a widervariety of facial displays, highlighting its potential as a scalable andeffective alternative to FACS for psychological and affective computingapplications.</description>
      <author>example@mail.com (Minh Tran, Maksim Siniukov, Zhangyu Jin, Mohammad Soleymani)</author>
      <guid isPermaLink="false">2510.01662v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Representation Learning as Mutual Information Maximization</title>
      <link>http://arxiv.org/abs/2510.01345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究从第一性原理出发，探讨自监督表示学习(SSRL)算法的学习目标如何决定其优化策略和模型设计选择，通过变分互信息下界推导出SDMI和JMI两种训练范式，为现有SSRL方法的架构组件选择提供理论解释。&lt;h4&gt;背景&lt;/h4&gt;自监督表示学习已取得显著成功但基本原理尚未充分理解，现有研究多从信息论目标或防止表示崩溃的启发式方法角度统一SSRL方法，而架构元素如预测器网络、stop-gradient操作和统计正则化器常被视为经验驱动的补充。&lt;h4&gt;目的&lt;/h4&gt;采用第一性原理方法，探究SSRL算法的学习目标是否决定其可能的优化策略和模型设计选择。&lt;h4&gt;方法&lt;/h4&gt;从变分互信息(MI)下界出发，推导出自蒸馏互信息(SDMI)和联合互信息(JMI)两种训练范式，分析它们施加的不同结构约束及其与现有SSRL算法的关系。&lt;h4&gt;主要发现&lt;/h4&gt;SDMI需要交替优化使stop-gradient操作理论上成为必需；JMI可通过对称架构进行联合优化无需此类组件；预测器网络和统计正则化器分别是MI目标的可替代代理；许多现有SSRL方法可视为这两种范式的特定实例或近似。&lt;h4&gt;结论&lt;/h4&gt;本文为现有SSRL方法不同架构组件的选择提供了超越启发式便利性的理论解释。&lt;h4&gt;翻译&lt;/h4&gt;自监督表示学习(SSRL)已经取得了显著的实证成功，但其基本原理仍未被充分理解。虽然最近的工作试图通过检查其信息论目标或总结防止表示崩溃的启发式方法来统一SSRL方法，但预测器网络、stop-gradient操作和统计正则化器等架构元素通常被视为经验驱动的补充。在本文中，我们采用第一性原理方法，研究SSRL算法的学习目标是否决定其可能的优化策略和模型设计选择。特别是，从变分互信息(MI)下界出发，我们推导出两种训练范式，即自蒸馏互信息(SDMI)和联合互信息(JMI)，每种范式施加不同的结构约束并涵盖一系列现有的SSRL算法。SDMI本质上需要交替优化，使stop-gradient操作在理论上成为必需。相比之下，JMI可以通过对称架构进行联合优化，无需此类组件。在提出的公式中，SDMI中的预测器网络和JMI中的统计正则化器成为MI目标的可替代代理。我们表明，许多现有的SSRL方法都是这两种范式的特定实例或近似。本文为现有SSRL方法不同架构组件的选择提供了理论解释，超越了启发式便利性的范畴。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised representation learning (SSRL) has demonstrated remarkableempirical success, yet its underlying principles remain insufficientlyunderstood. While recent works attempt to unify SSRL methods by examining theirinformation-theoretic objectives or summarizing their heuristics for preventingrepresentation collapse, architectural elements like the predictor network,stop-gradient operation, and statistical regularizer are often viewed asempirically motivated additions. In this paper, we adopt a first-principlesapproach and investigate whether the learning objective of an SSRL algorithmdictates its possible optimization strategies and model design choices. Inparticular, by starting from a variational mutual information (MI) lower bound,we derive two training paradigms, namely Self-Distillation MI (SDMI) and JointMI (JMI), each imposing distinct structural constraints and covering a set ofexisting SSRL algorithms. SDMI inherently requires alternating optimization,making stop-gradient operations theoretically essential. In contrast, JMIadmits joint optimization through symmetric architectures without suchcomponents. Under the proposed formulation, predictor networks in SDMI andstatistical regularizers in JMI emerge as tractable surrogates for the MIobjective. We show that many existing SSRL methods are specific instances orapproximations of these two paradigms. This paper provides a theoreticalexplanation behind the choices of different architectural components ofexisting SSRL methods, beyond heuristic conveniences.</description>
      <author>example@mail.com (Akhlaqur Rahman Sabby, Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu)</author>
      <guid isPermaLink="false">2510.01345v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Approach for Improving Relational Aggregated Search</title>
      <link>http://arxiv.org/abs/2510.00966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了高级自然语言处理技术在阿拉伯语文本数据聚类中的应用，特别是堆叠自编码器和AraBERT嵌入技术，以改善聚合搜索环境中的搜索结果质量和相关性。&lt;h4&gt;背景&lt;/h4&gt;互联网信息爆炸导致需要开发能够提升各种格式内容检索和管理的聚合搜索系统。传统搜索引擎存在不精确、缺乏上下文相关性和个性化的问题。&lt;h4&gt;目的&lt;/h4&gt;改进阿拉伯语文本数据在聚合搜索环境中的聚类效果，提供更丰富、具有上下文感知能力的搜索结果表征。&lt;h4&gt;方法&lt;/h4&gt;研究应用了堆叠自编码器和AraBERT嵌入等高级自然语言处理技术，并使用K-means聚类算法来发现搜索结果中的特征和关系。通过不同阿拉伯语查询评估了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;堆叠自编码器在表示学习中适合聚类任务，能够显著改善聚类搜索结果。该方法提高了搜索结果的准确性和相关性。&lt;h4&gt;结论&lt;/h4&gt;通过超越传统搜索引擎的局限性，该研究提供了一种更有效的方法来聚类和表征阿拉伯语文本搜索结果，显著提升了搜索质量和用户体验。&lt;h4&gt;翻译&lt;/h4&gt;由于互联网上的信息爆炸，需要开发聚合搜索系统来提升各种格式内容的检索和管理。为了进一步改进聚合搜索环境中阿拉伯语文本数据的聚类，本研究调查了高级自然语言处理技术的应用，即堆叠自编码器和AraBERT嵌入。通过超越传统搜索引擎的不精确、缺乏上下文相关性和个性化等局限，我们提供了更丰富、具有上下文感知能力的搜索结果表征，因此我们使用K-means聚类算法来发现这些结果中的特征和关系，然后在不同阿拉伯语查询上使用我们的方法评估其有效性。我们的模型表明，在表示学习中使用堆叠自编码器适合聚类任务，并能显著改善聚类搜索结果。它还展示了搜索结果的准确性和相关性的提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to an information explosion on the internet, there is a need for thedevelopment of aggregated search systems that can boost the retrieval andmanagement of content in various formats. To further improve the clustering ofArabic text data in aggregated search environments, this research investigatesthe application of advanced natural language processing techniques, namelystacked autoencoders and AraBERT embeddings. By transcending the limitations oftraditional search engines, which are imprecise, not contextually relevant, andnot personalized, we offer more enriched, context-aware characterizations ofsearch results, so we used a K-means clustering algorithm to discoverdistinctive features and relationships in these results, we then used ourapproach on different Arabic queries to evaluate its effectiveness. Our modelillustrates that using stacked autoencoders in representation learning suitsclustering tasks and can significantly improve clustering search results. Italso demonstrates improved accuracy and relevance of search results.</description>
      <author>example@mail.com (Sara Saad Soliman, Ahmed Younes, Islam Elkabani, Ashraf Elsayed)</author>
      <guid isPermaLink="false">2510.00966v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>LLM Routing with Dueling Feedback</title>
      <link>http://arxiv.org/abs/2510.00841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于上下文对决老虎机的LLM路由方法，通过成对偏好反馈学习，并引入了CCFT表示学习方法和FGTS.CDB算法，实现了在用户满意度、模型专业性和推理成本之间的平衡。&lt;h4&gt;背景&lt;/h4&gt;在大型语言模型(LLM)应用中，为每个查询选择最佳模型是一个关键挑战，需要同时考虑用户满意度、模型专业性和推理成本等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效选择最佳LLM的路由方法，平衡用户满意度、模型专业性和推理成本，并通过成对偏好反馈实现标签效率和动态适应。&lt;h4&gt;方法&lt;/h4&gt;将路由问题表述为上下文对决老虎机，引入Category-Calibrated Fine-Tuning (CCFT)表示学习方法，使用带类别权重的对比微从未标记数据中导出模型嵌入，并实现Feel-Good Thompson Sampling for Contextual Dueling Bandits (FGTS.CDB)算法，提出了四种整合模型质量和成本的类别权重变体。&lt;h4&gt;主要发现&lt;/h4&gt;在RouterBench和MixInstruct数据集上的实验表明，所提出的方法实现了比强基线更低的累积遗憾和更快的收敛速度，具有更好的鲁棒性和性能-成本平衡。&lt;h4&gt;结论&lt;/h4&gt;基于上下文对决老虎机和CCFT的LLM路由方法能够有效平衡用户满意度、模型专业性和推理成本，是一种标签效率和动态适应的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们研究LLM路由问题，即在平衡用户满意度、模型专业性和推理成本的同时为每个查询选择最佳模型。我们将路由表述为上下文对决老虎机，从成对偏好反馈而非绝对分数中学习，从而实现标签效率和动态适应。基于这一表述，我们引入了类别校准微调(CCFT)，一种表示学习方法，使用带类别权重的对比从未标记数据中导出模型嵌入。这些嵌入使Feel-Good Thompson Sampling for Contextual Dueling Bandits(FGTS.CDB)成为可能，这是一个理论上合理的后验采样算法。我们提出了四种明确整合模型质量和成本的类别权重变体，并在RouterBench和MixInstruct数据集上对所提出的方法进行了实证评估。在两个基准测试中，我们的方法实现了比使用通用OpenAI嵌入模型构建的强基线更低的累积遗憾和更快的收敛速度，具有更好的鲁棒性和性能-成本平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study LLM routing, the problem of selecting the best model for each querywhile balancing user satisfaction, model expertise, and inference cost. Weformulate routing as contextual dueling bandits, learning from pairwisepreference feedback rather than absolute scores, thereby yieldinglabel-efficient and dynamic adaptation. Building on this formulation, weintroduce Category-Calibrated Fine-Tuning (CCFT), a representation-learningmethod that derives model embeddings from offline data using contrastivefine-tuning with categorical weighting. These embeddings enable the practicalinstantiation of Feel-Good Thompson Sampling for Contextual Dueling Bandits(FGTS.CDB), a theoretically grounded posterior-sampling algorithm. We proposefour variants of the categorical weighting that explicitly integrate modelquality and cost, and we empirically evaluate the proposed methods on theRouterBench and MixInstruct datasets. Across both benchmarks, our methodsachieve lower cumulative regret and faster convergence, with better robustnessand performance-cost balance than strong baselines built with a general-purposeOpenAI embedding model.</description>
      <author>example@mail.com (Chao-Kai Chiang, Takashi Ishida, Masashi Sugiyama)</author>
      <guid isPermaLink="false">2510.00841v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression</title>
      <link>http://arxiv.org/abs/2510.00621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FAME是一种创新的函数数据处理方法，结合了注意力机制和专家混合模型，能够有效处理函数数据的连续性和函数间依赖关系。&lt;h4&gt;背景&lt;/h4&gt;函数数据在科学和工程中起着关键作用，但其无限维特性使得表示学习具有挑战性。传统统计模型依赖于预选择的基展开或核函数，限制了数据驱动发现的灵活性；而许多深度学习方法将函数视为固定网格向量，忽略了固有的连续性。&lt;h4&gt;目的&lt;/h4&gt;引入一种端到端的、完全数据驱动的框架，用于函数对函数回归任务。&lt;h4&gt;方法&lt;/h4&gt;提出Functional Attention with a Mixture-of-Experts (FAME)方法，通过双向神经控制微分方程与MoE驱动的向量场耦合形成连续注意力以捕获函数内连续性，并通过多头交叉注意力进一步融合变化以捕获函数间依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界的函数回归基准测试中，FAME实现了最先进的准确性，并且对函数的任意采样离散观测具有强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;FAME方法为函数数据表示学习提供了一个有效的解决方案，能够处理函数的连续性和函数间依赖关系。&lt;h4&gt;翻译&lt;/h4&gt;函数数据在科学和工程中起着关键作用，但其无限维特性使得表示学习具有挑战性。传统统计模型依赖于预选择的基展开或核函数，限制了数据驱动发现的灵活性，而许多深度学习管道将函数视为固定网格向量，忽略了固有的连续性。在本文中，我们引入了具有专家混合的函数注意力(FAME)，这是一种用于函数对函数回归的端到端、完全数据驱动框架。FAME通过双向神经控制微分方程与MoE驱动的向量场耦合形成连续注意力，以捕获函数内连续性，并通过多头交叉注意力进一步融合变化以捕获函数间依赖关系。在合成和真实世界函数回归基准上的广泛实验表明，FAME实现了最先进的准确性，并对函数的任意采样离散观测具有强大的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional data play a pivotal role across science and engineering, yet theirinfinite-dimensional nature makes representation learning challenging.Conventional statistical models depend on pre-chosen basis expansions orkernels, limiting the flexibility of data-driven discovery, while manydeep-learning pipelines treat functions as fixed-grid vectors, ignoringinherent continuity. In this paper, we introduce Functional Attention with aMixture-of-Experts (FAME), an end-to-end, fully data-driven framework forfunction-on-function regression. FAME forms continuous attention by coupling abidirectional neural controlled differential equation with MoE-driven vectorfields to capture intra-functional continuity, and further fuses change tointer-functional dependencies via multi-head cross attention. Extensiveexperiments on synthetic and real-world functional-regression benchmarks showthat FAME achieves state-of-the-art accuracy, strong robustness to arbitrarilysampled discrete observations of functions.</description>
      <author>example@mail.com (Yifei Gao, Yong Chen, Chen Zhang)</author>
      <guid isPermaLink="false">2510.00621v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>VIRTUE: Visual-Interactive Text-Image Universal Embedder</title>
      <link>http://arxiv.org/abs/2510.00523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的视觉交互式文本图像通用嵌入器（VIRTUE），它将分割模型和视觉语言模型的能力扩展到表示学习领域，使嵌入模型能够处理视觉提示并精确定位图像中的特定区域，从而在复杂和模糊场景中实现更精确的处理。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习模型在复杂任务中表现出色，视觉语言模型（VLMs）的集成为嵌入模型提供了指令遵循能力。然而，现有嵌入模型缺乏视觉交互能力，无法处理用户指定的感兴趣区域（如点、边界框、掩码），而这些能力在生成模型中已被探索以扩大人机交互适用性。&lt;h4&gt;目的&lt;/h4&gt;为嵌入模型配备视觉交互能力，解锁基于用户意图局部化的新应用，并使模型能够学习图像中的实体级信息，以补充传统嵌入任务的全局表示。&lt;h4&gt;方法&lt;/h4&gt;提出VIRTUE模型，将分割模型和视觉语言模型的能力扩展到表示学习领域。分割模型可以处理视觉提示，精确定位图像中的特定区域，使嵌入器能够更精确地处理复杂和模糊的场景。&lt;h4&gt;主要发现&lt;/h4&gt;引入了一个大规模分割和场景标题检索（SCaR）基准，包含100万个样本，旨在通过同时考虑特定对象的实体和图像场景来检索文本标题。VIRTUE在36个通用MMEB任务上取得了显著改进（3.1%-8.5%），在五个视觉交互式SCaR任务上也取得了显著改进（15.2%-20.3%）。&lt;h4&gt;结论&lt;/h4&gt;VIRTUE成功地将视觉交互能力集成到嵌入模型中，使模型能够更好地处理复杂场景和用户指定的感兴趣区域，并在多个任务上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态表示学习模型已在复杂任务中展现出成功操作，并且视觉语言模型（VLMs）的集成进一步使嵌入模型具备了指令遵循能力。然而，现有的嵌入模型缺乏视觉交互能力，无法处理用户指定的感兴趣区域（例如点、边界框、掩码），这些能力在生成模型中已被探索，以扩大其人机交互适用性。为嵌入模型配备视觉交互能力不仅能够解锁基于用户意图局部化的新应用，这些应用尚未被探索，而且还能使模型学习图像中的实体级信息，以补充其在传统嵌入任务中的全局表示。在本文中，我们提出了一种新颖的视觉交互式文本图像通用嵌入器（VIRTUE），它将分割模型和视觉语言模型的能力扩展到表示学习领域。在VIRTUE中，分割模型可以处理视觉提示，精确定位图像中的特定区域，从而使嵌入器能够更精确地处理复杂和模糊的场景。为了评估VIRTUE的视觉交互能力，我们引入了一个包含100万个样本的大规模分割和场景标题检索（SCaR）基准，旨在通过同时考虑特定对象的实体和图像场景来检索文本标题。VIRTUE在36个通用MMEB任务（3.1%-8.5%）和五个视觉交互式SCaR任务（15.2%-20.3%）上持续取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning models have demonstrated successfuloperation across complex tasks, and the integration of vision-language models(VLMs) has further enabled embedding models with instruction-followingcapabilities. However, existing embedding models lack visual-interactivecapabilities to specify regions of interest from users (e.g., point, boundingbox, mask), which have been explored in generative models to broaden theirhuman-interactive applicability. Equipping embedding models with visualinteractions not only would unlock new applications with localized grounding ofuser intent, which remains unexplored, but also enable the models to learnentity-level information within images to complement their globalrepresentations for conventional embedding tasks. In this paper, we propose anovel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extendsthe capabilities of the segmentation model and the vision-language model to therealm of representation learning. In VIRTUE, the segmentation model can processvisual prompts that pinpoint specific regions within an image, thereby enablingthe embedder to handle complex and ambiguous scenarios more precisely. Toevaluate the visual-interaction ability of VIRTUE, we introduce a large-scaleSegmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samplesthat aims to retrieve the text caption by jointly considering the entity with aspecific object and image scene. VIRTUE consistently achieves astate-of-the-art performance with significant improvements across 36 universalMMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.</description>
      <author>example@mail.com (Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2510.00523v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Cutting the Skip: Training Residual-Free Transformers</title>
      <link>http://arxiv.org/abs/2510.00345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种方法，使无跳跃连接的transformers能够稳定高效地训练，证明了跳跃连接并非训练ViTs的基本要求。&lt;h4&gt;背景&lt;/h4&gt;Transformers在各种应用中取得了显著成功，通常归因于其可扩展性。然而，没有跳跃（残差）连接的训练仍然非常困难。&lt;h4&gt;目的&lt;/h4&gt;解决transformers在没有跳跃连接情况下的训练问题。&lt;h4&gt;方法&lt;/h4&gt;通过分析无跳跃transformer块的雅可比矩阵，解释跳跃连接如何改善条件，并揭示可以通过原则性初始化策略恢复跳跃连接的稳定益处。基于此，引入了第一种能够在不改变标准架构的情况下实现无跳跃transformers稳定高效训练的方法。&lt;h4&gt;主要发现&lt;/h4&gt;使用其初始化方法训练的无跳跃ViTs克服了通常的优化障碍，学习了更丰富的层次化表示，并在密集预测基准上优于包含跳跃连接的强基线模型。&lt;h4&gt;结论&lt;/h4&gt;跳跃连接不是训练ViTs的基本要求，并为视觉模型中的层次化表示学习开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Transformers在广泛的应用中取得了显著成功，这一成就通常归因于其可扩展性。然而，没有跳跃（残差）连接的训练仍然非常困难。虽然跳跃连接稳定了优化过程，但它们也破坏了表示的层次结构，引发了一个长期存在的问题：transformers是否可以在没有跳跃连接的情况下被有效训练。在本工作中，我们通过分析无跳跃transformer块的雅可比矩阵来解决这个问题，解释了跳跃连接如何改善条件，并揭示可以通过一种原则性的初始化策略来恢复跳跃连接的稳定益处。基于这一见解，我们引入了第一种方法，可以在不改变标准架构的情况下实现无跳跃transformers的稳定高效训练。我们在监督和自监督设置下的Vision Transformers (ViTs)上验证了我们的方法，证明使用我们的初始化方法训练的无跳跃ViTs克服了通常的优化障碍，学习了更丰富的层次化表示，并在密集预测基准上优于包含跳跃连接的强基线模型。这些结果表明，跳跃连接不是训练ViTs的基本要求，并为视觉模型中的层次化表示学习开辟了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have achieved remarkable success across a wide range ofapplications, a feat often attributed to their scalability. Yet training themwithout skip (residual) connections remains notoriously difficult. While skipsstabilize optimization, they also disrupt the hierarchical structure ofrepresentations, raising the long-standing question of whether transformers canbe trained efficiently without them. In this work, we address this problem byanalyzing the Jacobian of a skipless transformer block, showing why skipsimprove conditioning and revealing that their stabilization benefits can berecovered through a principled initialization strategy. Building on thisinsight, we introduce the first method that enables stable and efficienttraining of skipless transformers without altering the standard architecture.We validate our approach on Vision Transformers (ViTs) in both supervised andself-supervised settings, demonstrating that skipless ViTs trained with ourinitialization overcome the usual optimization barriers, learn richerhierarchical representations, and outperform strong baselines, that incorporateskip connections, on dense prediction benchmarks. These results show that skipconnections are not a fundamental requirement for training ViTs and open newavenues for hierarchical representation learning in vision models.</description>
      <author>example@mail.com (Yiping Ji, James Martens, Jianqiao Zheng, Ziqin Zhou, Peyman Moghadam, Xinyu Zhang, Hemanth Saratchandran, Simon Lucey)</author>
      <guid isPermaLink="false">2510.00345v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection</title>
      <link>http://arxiv.org/abs/2510.00303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS'25. 22 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了名为CROWD的组合开放世界检测框架，解决了现有OWOD方法中的语义混淆和灾难性遗忘问题，显著提高了已知类准确率和未知召回率。&lt;h4&gt;背景&lt;/h4&gt;开放世界目标检测(OWOD)通过人工指导能够持续发现和集成未知目标来丰富传统目标检测器。然而，现有的OWOD方法经常面临已知类和未知类之间的语义混淆，以及灾难性遗忘问题，导致未知目标召回率下降和已知类准确率降低。&lt;h4&gt;目的&lt;/h4&gt;克服OWOD方法中的语义混淆和灾难性遗忘挑战，提高未知目标召回率和已知类准确率。&lt;h4&gt;方法&lt;/h4&gt;提出CROWD统一框架，将未知目标发现和适应重新表述为交织的组合数据发现(CROWD-Discover)和表征学习(CROWD-Learn)任务。CROWD-Discover通过最大化子模态条件增益函数策略性挖掘未知实例；CROWD-Learn采用新颖组合目标，分离已知和未知表征同时保持已知类间的判别一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在OWOD基准评估中，CROWD在M-OWODB和S-OWODB上的已知类准确率分别提高了2.83%和2.05%，与最先进基线相比，未知召回率提高了约2.4倍。&lt;h4&gt;结论&lt;/h4&gt;CROWD框架有效解决了开放世界目标检测中的关键挑战，显著提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;开放世界目标检测(OWOD)通过人工指导实现未知目标的持续发现和集成，从而丰富了传统目标检测器。然而，现有的OWOD方法经常遭受已知类与未知类之间的语义混淆，以及灾难性遗忘问题，导致未知召回率降低和已知类准确率下降。为克服这些挑战，我们提出了组合开放世界检测(CROWD)，这是一个统一框架，将未知目标发现和适应重新表述为一个交织的组合(基于集合)的数据发现(CROWD-Discover)和表征学习(CROWD-Learn)任务。CROWD-Discover通过最大化子模态条件增益(SCG)函数策略性地挖掘未知实例，选择与已知目标明显不同的代表性示例。随后，CROWD-Learn采用新颖的组合目标，同时分离已知和未知表征，同时保持已知类之间的判别一致性，从而减轻混淆和遗忘。在OWOD基准上的广泛评估表明，CROWD在M-OWODB和S-OWODB上的已知类准确率分别提高了2.83%和2.05%，并且与最先进的基线相比，未知召回率提高了近2.4倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-World Object Detection (OWOD) enriches traditional object detectors byenabling continual discovery and integration of unknown objects via humanguidance. However, existing OWOD approaches frequently suffer from semanticconfusion between known and unknown classes, alongside catastrophic forgetting,leading to diminished unknown recall and degraded known-class accuracy. Toovercome these challenges, we propose Combinatorial Open-World Detection(CROWD), a unified framework reformulating unknown object discovery andadaptation as an interwoven combinatorial (set-based) data-discovery(CROWD-Discover) and representation learning (CROWD-Learn) task. CROWD-Discoverstrategically mines unknown instances by maximizing Submodular Conditional Gain(SCG) functions, selecting representative examples distinctly dissimilar fromknown objects. Subsequently, CROWD-Learn employs novel combinatorial objectivesthat jointly disentangle known and unknown representations while maintainingdiscriminative coherence among known classes, thus mitigating confusion andforgetting. Extensive evaluations on OWOD benchmarks illustrate that CROWDachieves improvements of 2.83% and 2.05% in known-class accuracy on M-OWODB andS-OWODB, respectively, and nearly 2.4x unknown recall compared to leadingbaselines.</description>
      <author>example@mail.com (Anay Majee, Amitesh Gangrade, Rishabh Iyer)</author>
      <guid isPermaLink="false">2510.00303v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder</title>
      <link>http://arxiv.org/abs/2509.25334v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为LEO-CVAE的局部熵引导过采样方法，用于解决高维生物医学数据中的类别不平衡问题，通过整合局部不确定性信息提高生成样本质量，从而改善分类性能。&lt;h4&gt;背景&lt;/h4&gt;类别不平衡是机器学习中的主要挑战，特别是在高维生物医学数据中，其非线性流形结构占主导地位。传统过采样方法如SMOTE依赖局部线性插值，产生不合理样本；而深度生成模型如CVAE虽能捕捉非线性分布，但未特别关注边界区域样本的重要性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够显式地将局部不确定性纳入表示学习和数据生成的生成过采样框架，以改善不平衡数据的学习效果，特别是在复杂非线性结构的数据中。&lt;h4&gt;方法&lt;/h4&gt;提出LEO-CVAE框架，通过计算样本邻域类分布的香农熵来量化不确定性，并利用两种机制：(1)局部熵加权损失(LEWL)强调在不确定区域的鲁棒学习；(2)熵引导的采样策略在类重叠区域集中生成样本。&lt;h4&gt;主要发现&lt;/h4&gt;在ADNI和TCGA肺癌临床基因组数据集上应用LEO-CVAE，结果显示该方法一致提高了分类器性能，优于传统过采样和生成基线方法。&lt;h4&gt;结论&lt;/h4&gt;在受复杂非线性结构（如组学数据）支配的领域中进行不平衡学习时，感知不确定性的生成过采样具有重要价值，LEO-CVAE通过整合局部不确定性信息有效提高了生成样本质量和分类性能。&lt;h4&gt;翻译&lt;/h4&gt;类别不平衡仍然是机器学习中的一个主要挑战，特别是在高维生物医学数据中，非线性流形结构占主导地位。传统的过采样方法如SMOTE依赖于局部线性插值，常常产生不合理的合成样本。深度生成模型如条件变分自编码器(CVAE)能够更好地捕捉非线性分布，但标准变体对所有少数类样本同等对待，忽略了边界区域样本的重要性，如Borderline-SMOTE和ADASYN等启发式方法所强调的那样。我们提出了一个结合了CVAE的局部熵引导过采样方法(LEO-CVAE)，这是一个生成过采样框架，明确地将局部不确定性纳入表示学习和数据生成中。为了量化不确定性，我们计算样本邻域中类分布的香农熵：高熵表示更大的类重叠，作为不确定性的代理。LEO-CVAE通过两种机制利用这一信号：(i)一种局部熵加权损失(LEWL)，强调在不确定区域的鲁棒学习；(ii)一种熵引导的采样策略，在这些信息丰富、类重叠的区域集中生成。应用于临床基因组数据集(ADNI和TCGA肺癌)时，LEO-CVAE一致地提高了分类器性能，优于传统的过采样和生成基线。这些结果强调了在受复杂非线性结构（如组学数据）支配的领域中进行不平衡学习时，感知不确定性的生成过采样的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Class imbalance remains a major challenge in machine learning, especially forhigh-dimensional biomedical data where nonlinear manifold structures dominate.Traditional oversampling methods such as SMOTE rely on local linearinterpolation, often producing implausible synthetic samples. Deep generativemodels like Conditional Variational Autoencoders (CVAEs) better capturenonlinear distributions, but standard variants treat all minority samplesequally, neglecting the importance of uncertain, boundary-region examplesemphasized by heuristic methods like Borderline-SMOTE and ADASYN.  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), agenerative oversampling framework that explicitly incorporates localuncertainty into both representation learning and data generation. To quantifyuncertainty, we compute Shannon entropy over the class distribution in asample's neighborhood: high entropy indicates greater class overlap, serving asa proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms:(i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning inuncertain regions, and (ii) an entropy-guided sampling strategy thatconcentrates generation in these informative, class-overlapping areas.  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAEconsistently improves classifier performance, outperforming both traditionaloversampling and generative baselines. These results highlight the value ofuncertainty-aware generative oversampling for imbalanced learning in domainsgoverned by complex nonlinear structures, such as omics data.</description>
      <author>example@mail.com (Amirhossein Zare, Amirhessam Zare, Parmida Sadat Pezeshki, Herlock, Rahimi, Ali Ebrahimi, Ignacio Vázquez-García, Leo Anthony Celi)</author>
      <guid isPermaLink="false">2509.25334v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps</title>
      <link>http://arxiv.org/abs/2510.02274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Diffusion^2是一种基于扩散的方法，使用3D点云建模从Wi-Fi到毫米波的广泛频率范围内的RF信号传播，通过RF-3D Encoder和多尺度嵌入有效捕获RF相关特征，实现了高精度和高效的RF信号预测。&lt;h4&gt;背景&lt;/h4&gt;RF信号传播建模对于理解环境至关重要，因为RF信号能提供RGB相机无法提供的宝贵见解，后者受限于可见光谱、镜头覆盖和遮挡。RF建模对于支持无线诊断、部署和优化也很有用，但在复杂环境中准确预测RF信号仍然是一个挑战，因为信号与障碍物存在相互作用，如吸收和反射。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测复杂环境中RF信号传播的方法，克服现有方法的局限性，提高预测精度和效率。&lt;h4&gt;方法&lt;/h4&gt;提出Diffusion^2，一种基于扩散的方法，使用3D点云来建模RF信号传播。引入RF-3D Encoder来从3D数据中有效捕获RF相关特征，结合多尺度嵌入来模拟实际的RF信号传播过程。&lt;h4&gt;主要发现&lt;/h4&gt;基于合成和真实世界测量的评估表明，Diffusion^2能够准确估计不同频段和环境条件下RF信号的行为，误差仅为1.9分贝，比现有方法快27倍。&lt;h4&gt;结论&lt;/h4&gt;Diffusion^2代表了RF信号预测领域的重大进展，为无线诊断、部署和优化提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;射频信号传播建模对于理解环境至关重要，因为RF信号提供了超越RGB相机能力的宝贵见解，而RGB相机受限于可见光谱、镜头覆盖和遮挡。RF建模对于支持无线诊断、部署和优化也很有用。然而，由于与障碍物的相互作用（如吸收和反射），准确预测复杂环境中的RF信号仍然是一个挑战。我们引入了Diffusion^2，一种基于扩散的方法，使用3D点云来建模从Wi-Fi到毫米波的广泛频率范围内的RF信号传播。为了从3D数据中有效捕获RF相关特征，我们提出了RF-3D Encoder，它封装了3D几何的复杂性以及信号特定的细节。这些特征经过多尺度嵌入，以模拟实际的RF信号传播过程。基于合成和真实世界测量的评估表明，Diffusion^2能够准确估计不同频段和环境条件下RF信号的行为，误差仅为1.9分贝，比现有方法快27倍，标志着该领域的重大进展。更多信息请访问https://rfvision-project.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D环境模型生成准确的射频(RF)信号热力图问题。这个问题很重要，因为RF信号能提供超越RGB相机能力的环境洞察，可用于无线网络优化、设备部署、智能环境建设等领域，而现有方法要么计算成本高，要么需要大量预测量数据，难以实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受生成式AI(如Sora)启发，思考能否让AI理解可见光谱外的RF信号。他们分析了现有射线追踪方法(计算复杂、需材料信息)和机器学习方法(需大量数据)的局限性，借鉴了扩散模型在图像生成中的成功应用，结合NeRF的场景表示思想，设计了条件引导的扩散框架，并创新性地提出了RF-3D编码器和RF-3D配对块来处理多模态信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型将3D环境模型转换为RF信号热力图，通过条件引导确保生成结果符合物理规律。流程包括：1)用智能手机捕获3D环境模型和少量预测量数据；2)RF-3D编码器提取3D点云、2D图像和RF信号特征；3)扩散过程(前向加噪、反向去噪)由RF-3D特征引导；4)RF-3D配对块融合噪声预测与环境特征；5)训练时使用信号损失函数；6)推理时生成静态热力图或动态热力图视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个基于扩散模型的RF热力图生成方法；2)RF-3D编码器提取多模态特征；3)RF-3D配对块实现跨模态融合；4)支持动态场景的视频扩散。相比不同：1)数据效率高(只需15个测量点vs.数千点)；2)计算速度快(27倍于AUTOMS)；3)支持多频率生成；4)环境变化时无需重新训练；5)同时支持静态和动态场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Diffusion2创新性地将扩散模型与3D环境建模相结合，仅需少量预测量数据就能快速准确地生成多频率的RF信号热力图，显著提高了无线网络规划和优化的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling radio frequency (RF) signal propagation is essential forunderstanding the environment, as RF signals offer valuable insights beyond thecapabilities of RGB cameras, which are limited by the visible-light spectrum,lens coverage, and occlusions. It is also useful for supporting wirelessdiagnosis, deployment, and optimization. However, accurately predicting RFsignals in complex environments remains a challenge due to interactions withobstacles such as absorption and reflection. We introduce Diffusion^2, adiffusion-based approach that uses 3D point clouds to model the propagation ofRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.To effectively capture RF-related features from 3D data, we present the RF-3DEncoder, which encapsulates the complexities of 3D geometry along withsignal-specific details. These features undergo multi-scale embedding tosimulate the actual RF signal dissemination process. Our evaluation, based onsynthetic and real-world measurements, demonstrates that Diffusion^2 accuratelyestimates the behavior of RF signals in various frequency bands andenvironmental conditions, with an error margin of just 1.9 dB and 27x fasterthan existing methods, marking a significant advancement in the field. Refer tohttps://rfvision-project.github.io/ for more information.</description>
      <author>example@mail.com (Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang)</author>
      <guid isPermaLink="false">2510.02274v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</title>
      <link>http://arxiv.org/abs/2510.02186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GeoPurify的新方法，用于解决将2D视觉语言模型特征转移到3D语义分割中的权衡问题，通过利用潜在的几何信息实现高效的数据利用和优越的性能。&lt;h4&gt;背景&lt;/h4&gt;当前将2D视觉语言模型(VLMs)的特征转移到3D语义分割中存在一个持续的权衡：直接投影2D特征到3D会产生嘈杂和碎片化的预测，而强制几何一致性需要昂贵的训练流程和大规模的3D标注数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够有效利用2D视觉语言模型的特征进行3D语义分割，同时避免传统方法中的权衡问题，实现更高的数据效率和性能。&lt;h4&gt;方法&lt;/h4&gt;提出GeoPurify方法，应用一个小型学生亲和网络来净化2D VLM生成的3D点特征，使用从3D自监督教师模型中提炼的几何先验；在推理阶段，设计了几何引导池化模块来进一步去噪点云并确保语义和结构一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在主要3D基准上的广泛实验表明，GeoPurify在使用仅约1.5%训练数据的情况下达到或超过了最先进的性能，有效缓解了传统方法中的权衡问题。&lt;h4&gt;结论&lt;/h4&gt;GeoPurify通过利用潜在的几何信息和学习的亲和网络，成功缓解了2D到3D特征转移中的权衡问题，实现了优越的数据效率和性能，为3D语义分割提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;最近尝试将2D视觉语言模型(VLMs)的特征转移到3D语义分割中暴露了一个持续的权衡。直接将2D特征投影到3D会产生嘈杂和碎片化的预测，而强制几何一致性需要昂贵的训练流程和大规模的3D标注数据。我们认为这种限制源于主导的分割和匹配范式，该范式无法调和2D语义与3D几何结构。几何线索在2D到3D转移过程中并未被消除，而是保留在嘈杂和视图聚合的特征中。为了利用这一特性，我们提出了GeoPurify，它应用一个小型学生亲和网络，使用从3D自监督教师模型中提炼的几何先验来净化2D VLM生成的3D点特征。在推理过程中，我们设计了一个几何引导池化模块来进一步去噪点云并确保语义和结构一致性。受益于潜在的几何信息和学习的亲和网络，GeoPurify有效缓解了权衡问题并实现了优越的数据效率。在主要3D基准上的广泛实验表明，GeoPurify在使用仅约1.5%训练数据的情况下达到或超过了最先进的性能。我们的代码和检查点可在https://github.com/tj12323/GeoPurify获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决开放词汇3D语义分割中的基本权衡问题：直接将2D视觉-语言模型(VLM)的特征投影到3D会产生噪声和碎片化的预测，而强制几何一致性则需要昂贵的训练流程和大规模的3D标注数据。这个问题很重要，因为传统3D场景理解受限于封闭世界假设，无法处理真实世界中多样复杂的物体；同时手动3D标注成本极高，劳动强度大；开放词汇3D理解对自动驾驶、机器人和增强现实等应用至关重要，但当前方法要么产生噪声输出，要么需要大量计算资源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到当前'分割和匹配'范式的局限性，它将几何和语义视为分离的问题。作者假设2D到3D的转换不会破坏几何信息，而是使其变得潜在(latent)，因此可以更有效地恢复潜在结构，而不是从头学习3D几何。作者借鉴了生物感知中的'分割即理解'范式。方法借鉴了现有工作：使用2D视觉-语言模型(X-Decoder)获取语义特征；利用3D自监督教师模型(如Sonata)提供几何先验；应用知识蒸馏技术让学生网络从教师模型学习几何关系；采用对比学习来学习点之间的几何亲和性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何对比蒸馏，利用小型学生亲和网络来净化2D VLM生成的3D点特征，使用从3D自监督教师模型中提炼的几何先验。在推理时，使用几何引导池化模块去噪点云并确保语义和结构一致性。整体流程：1)语义初始化：用冻结的2D VLM从多视图RGB图像生成初始3D特征；2)几何对比蒸馏：训练学生网络学习几何亲和性，使用教师模型作为指导，通过对比目标优化；3)几何引导池化：在推理时，用训练好的学生网络构建亲和矩阵，通过迭代池化传播语义信息，产生净化的特征集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)范式转变：从'分割和匹配'转向'分割即理解'；2)数据效率框架：仅用1.5%训练数据达到或超过最先进性能；3)几何对比蒸馏：让学生从未标记3D扫描中学习几何亲和性；4)几何引导池化：在推理时确保结构一致性。不同之处：不同于训练自由方法导致的噪声问题；不同于需要密集标注的高成本方法；不依赖大规模数据集隐式学习几何先验；采用解耦训练过程学习与语义无关的几何先验，提高跨域泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoPurify通过几何对比蒸馏和几何引导池化，解决了开放词汇3D语义分割中语义丰富性与几何一致性的基本权衡，实现了在极少量数据上达到或超过最先进性能的数据高效3D场景理解框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to3D semantic segmentation expose a persistent trade-off. Directly projecting 2Dfeatures into 3D yields noisy and fragmented predictions, whereas enforcinggeometric coherence necessitates costly training pipelines and large-scaleannotated 3D data. We argue that this limitation stems from the dominantsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with3D geometric structure. The geometric cues are not eliminated during the2D-to-3D transfer but remain latent within the noisy and view-aggregatedfeatures. To exploit this property, we propose GeoPurify that applies a smallStudent Affinity Network to purify 2D VLM-generated 3D point features usinggeometric priors distilled from a 3D self-supervised teacher model. Duringinference, we devise a Geometry-Guided Pooling module to further denoise thepoint cloud and ensure the semantic and structural consistency. Benefiting fromlatent geometric information and the learned affinity network, GeoPurifyeffectively mitigates the trade-off and achieves superior data efficiency.Extensive experiments on major 3D benchmarks demonstrate that GeoPurifyachieves or surpasses state-of-the-art performance while utilizing only about1.5% of the training data. Our codes and checkpoints are available at[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).</description>
      <author>example@mail.com (Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen)</author>
      <guid isPermaLink="false">2510.02186v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions</title>
      <link>http://arxiv.org/abs/2510.02104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LangGrasp框架，通过整合微调的大语言模型和点云定位模块，解决语言驱动抓取中处理模糊指令隐含意图的问题，实现从对象级到部件级的高精度抓取。&lt;h4&gt;背景&lt;/h4&gt;现有的语言驱动抓取方法难以处理包含隐含意图的模糊指令。&lt;h4&gt;目的&lt;/h4&gt;提出LangGrasp框架，解决模糊指令中隐含意图的处理问题。&lt;h4&gt;方法&lt;/h4&gt;提出LangGrasp框架，整合微调的大语言模型(LLMs)利用其常识理解和环境感知能力，从语言指令中推断隐含意图；设计了由2D部件分割引导的点云定位模块，实现场景中部分点云定位。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LangGrasp框架能够准确解决模糊指令中的隐含意图，识别出任务完成所需的关键操作和目标信息；通过整合环境信息动态选择最优抓取姿态。&lt;h4&gt;结论&lt;/h4&gt;LangGrasp实现了从对象级到部件级的高精度抓取，显著提高了机器人在非结构化环境中的适应性和任务执行效率。&lt;h4&gt;翻译&lt;/h4&gt;现有的语言驱动抓取方法难以处理包含隐含意图的模糊指令。为了应对这一挑战，我们提出了LangGrasp，一种新颖的语言交互式机器人抓取框架。该框架整合了微调的大语言模型(LLMs)，利用其强大的常识理解和环境感知能力，从而从语言指令中推断隐含意图，并阐明任务要求以及目标操作对象。此外，我们设计的由2D部件分割引导的点云定位模块，能够在场景中进行部分点云定位，从而将抓取操作从粗粒度的对象级扩展到细粒度的部件级操作。实验结果表明，LangGrasp框架能够准确解决模糊指令中的隐含意图，识别出任务完成所需的关键操作和目标信息，而这些信息在指令中未明确说明但对任务完成至关重要。此外，它通过整合环境信息动态选择最优抓取姿态，实现了从对象级到部件级操作的高精度抓取，显著提高了机器人在非结构化环境中的适应性和任务执行效率。更多信息 and code are available here: https://github.com/wu467/LangGrasp.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有语言驱动抓取方法无法处理包含隐含意图的模糊指令，以及现有框架仅限于对象级别抓取而忽略部件功能区别的问题。这个问题很重要，因为随着机器人在日常环境中的部署增加，基于自然语言的人机交互能显著提高效率，但在动态非结构化环境中准确解释模糊指令，特别是包含隐含意图的指令，仍是关键挑战，机器人需要具备高效的语言理解和视觉感知能力来实现常识驱动的意图消除歧义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法在处理模糊指令方面的局限性，注意到大型语言模型具有强大的常识理解和环境感知能力，可以利用这些能力推断隐含意图。他们设计了三个主要模块：感知与推理、点云定位和抓取姿态检测。通过微调LLMs来保留其广泛知识同时增强推理能力。作者借鉴了LLMs作为机器人任务规划器的现有工作，语言驱动机器人抓取的研究，以及任务导向抓取(TOG)的相关方法，但针对抓取任务进行了专门优化和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用微调的大型语言模型处理模糊复杂指令，推断隐含意图，并将抓取从对象级别扩展到部件级别，通过结构化输出直接提取任务信息。整体流程分为三阶段：1)感知与推理：使用微调LLM基于场景和多轮对话生成JSON格式动作序列；2)点云定位：用预训练2D部件分割模型定位目标，应用扩展策略确保覆盖边缘和背景信息，将深度图转换为点云并聚焦目标区域；3)抓取姿态检测：对定位点云预测6-DoF抓取姿态，基于置信度选择最优姿态，生成执行轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出LangGrasp框架，利用微调LLMs处理模糊指令并实现部件级别抓取；2)构建专门用于机器人抓取任务的LLMs微调数据集；3)设计点云定位模块支持精细抓取。相比之前工作，LangGrasp能处理含隐含意图的模糊指令，实现从对象到部件的细粒度抓取，通过微调生成结构化输出无需额外解析，并使用更简洁方法实现任务导向抓取，避免了大规模数据集训练的需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LangGrasp通过微调大型语言模型并设计创新的点云定位策略，使机器人能够准确理解模糊语言指令并实现从对象级别到部件级别的精细抓取操作，显著提高了机器人在非结构化环境中的适应性和任务执行效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The existing language-driven grasping methods struggle to fully handleambiguous instructions containing implicit intents. To tackle this challenge,we propose LangGrasp, a novel language-interactive robotic grasping framework.The framework integrates fine-tuned large language models (LLMs) to leveragetheir robust commonsense understanding and environmental perceptioncapabilities, thereby deducing implicit intents from linguistic instructionsand clarifying task requirements along with target manipulation objects.Furthermore, our designed point cloud localization module, guided by 2D partsegmentation, enables partial point cloud localization in scenes, therebyextending grasping operations from coarse-grained object-level to fine-grainedpart-level manipulation. Experimental results show that the LangGrasp frameworkaccurately resolves implicit intents in ambiguous instructions, identifyingcritical operations and target information that are unstated yet essential fortask completion. Additionally, it dynamically selects optimal grasping poses byintegrating environmental information. This enables high-precision graspingfrom object-level to part-level manipulation, significantly enhancing theadaptability and task execution efficiency of robots in unstructuredenvironments. More information and code are available here:https://github.com/wu467/LangGrasp.</description>
      <author>example@mail.com (Yunhan Lin, Wenqi Wu, Zhijie Zhang, Huasong Min)</author>
      <guid isPermaLink="false">2510.02104v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</title>
      <link>http://arxiv.org/abs/2510.02034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GaussianMorphing的新框架，用于从多视角图像进行语义感知的3D形状和纹理变形。&lt;h4&gt;背景&lt;/h4&gt;以往的方法通常依赖于点云或需要为未纹理数据预定义同胚映射，存在一定的局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的限制，实现高质量的3D形状和纹理变形，同时保持几何一致性和纹理保真度。&lt;h4&gt;方法&lt;/h4&gt;利用网格引导的3D高斯溅射技术进行高保真几何和外观建模，通过统一的变形策略将3D高斯锚定到重建的网格补片上，并利用网格拓扑作为几何先验建立无监督语义对应。&lt;h4&gt;主要发现&lt;/h4&gt;在提出的TexMorph基准测试上，GaussianMorphing显著优于之前的2D/3D方法，将颜色一致性误差降低了22.2%，将EI指标降低了26.2%。&lt;h4&gt;结论&lt;/h4&gt;这种集成方法在不需要标记数据的情况下，在整个变形过程中保持了局部细节和全局语义一致性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入GaussianMorphing，一种用于从多视角图像进行语义感知的3D形状和纹理变形的新框架。以往的方法通常依赖于点云或需要为未纹理数据预定义同胚映射。我们的方法通过利用网格引导的3D高斯溅射技术进行高保真几何和外观建模，克服了这些限制。我们框架的核心是一个统一的变形策略，将3D高斯锚定到重建的网格补片上，确保几何一致的变换，并通过拓扑感知约束保持纹理保真度。同时，我们的框架利用网格拓扑作为几何先验建立无监督语义对应，并通过物理合理的点轨迹保持结构完整性。这种集成方法在整个变形过程中保持了局部细节和全局语义一致性，而无需标记数据。在我们提出的TexMorph基准测试上，GaussianMorphing显著优于之前的2D/3D方法，将颜色一致性误差降低了22.2%，将EI降低了26.2%。项目页面：https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决语义感知的3D形状和纹理变形问题，即如何从多视角图像生成高质量、结构一致且纹理连贯的3D物体变形序列。这个问题在计算机动画、几何建模、形状分析和电影视觉特效等领域非常重要，因为变形技术是连接计算机视觉与计算机图形学的桥梁，能够实现物体之间的平滑过渡，为创意内容制作提供关键技术支持。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：图像导向方法缺乏3D几何推理，3D方法需要高质量网格输入且难以处理复杂纹理。他们借鉴了3D高斯泼溅(3DGS)的高效渲染能力和网格的结构优势，结合图卷积网络学习语义特征，并使用测地距离计算几何关系。通过将离散的高斯点与结构化网格桥接，作者设计出一种混合表示方法，既保留了高斯表示的渲染优势，又利用网格提供了必要的结构约束。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用网格作为拓扑支架来引导无结构高斯点的变形，确保几何一致性的同时保持纹理保真度。整体流程包括：1)从多视角图像生成混合网格-高斯表示，将高斯锚定到网格面上；2)通过图卷积网络学习网格顶点间的语义对应关系；3)使用神经网络预测连续变形场；4)随着网格变形更新绑定高斯的位置；5)通过多目标优化平衡几何结构、外观一致性和语义对齐，最终生成高质量的3D变形序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个使用3D高斯进行联合3D几何和纹理变形的框架；2)网格引导的变形机制，同时感知拓扑和语义；3)双域优化策略结合几何约束和纹理插值。相比之前的工作，本方法不需要预定义的3D数据或同胚映射，直接从图像生成完全纹理化的3D输出；解决了现有方法在几何鲁棒性、纹理一致性和输入可访问性间的权衡问题；在复杂拓扑和纹理丰富场景中表现出色，同时减少了高质量3D数据的依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GaussianMorphing通过结合网格引导的3D高斯泼溅与语义感知变形，实现了从多视角图像生成高质量纹理化3D变形的创新框架，显著优于现有方法在几何准确性、纹理一致性和结构完整性方面的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce GaussianMorphing, a novel framework for semantic-aware 3D shapeand texture morphing from multi-view images. Previous approaches usually relyon point clouds or require pre-defined homeomorphic mappings for untextureddata. Our method overcomes these limitations by leveraging mesh-guided 3DGaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.The core of our framework is a unified deformation strategy that anchors3DGaussians to reconstructed mesh patches, ensuring geometrically consistenttransformations while preserving texture fidelity through topology-awareconstraints. In parallel, our framework establishes unsupervised semanticcorrespondence by using the mesh topology as a geometric prior and maintainsstructural integrity via physically plausible point trajectories. Thisintegrated approach preserves both local detail and global semantic coherencethroughout the morphing process with out requiring labeled data. On ourproposed TexMorph benchmark, GaussianMorphing substantially outperforms prior2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/</description>
      <author>example@mail.com (Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen)</author>
      <guid isPermaLink="false">2510.02034v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2510.02028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures, 7 tables, Submitted to ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为LiLa-Net的3D自编码器架构，仅使用LiDAR点云从真实交通环境中编码高效特征，并成功实现了原始点云的准确重建。&lt;h4&gt;背景&lt;/h4&gt;研究使用配备Velodyne LiDAR的真实半自动驾驶车辆作为数据来源，针对交通环境中的点云数据处理需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效的3D自编码器架构，能够从交通环境中提取有效特征并准确重建原始点云。&lt;h4&gt;方法&lt;/h4&gt;提出LiLa-Net架构，利用跳跃连接概念提高性能，同时减少编码器层数和简化跳跃连接结构，以在保持高效性的同时产生具有代表性的潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在跳跃连接信息和潜在编码之间实现了有效平衡，提高了重建质量而不影响性能；模型展示了强大的泛化能力，能够重建与原始交通环境无关的物体。&lt;h4&gt;结论&lt;/h4&gt;LiLa-Net是一种资源高效的3D自编码器架构，能够在不使用大量资源的情况下，从交通环境中提取有效特征并准确重建点云，同时具备良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了一种名为LiLa-Net的3D自编码器架构，它仅使用LiDAR的点云从真实交通环境中编码高效特征。为此，我们使用了一辆配备有Velodyne LiDAR的真实半自动驾驶车辆。该系统利用跳跃连接概念来提高性能，而不像最先进的架构那样使用大量资源。关键变化包括减少编码器层数和简化跳跃连接，同时仍然产生高效的潜在空间，允许准确重建原始点云。此外，在跳跃连接和潜在编码携带的信息之间实现了有效平衡，提高了重建质量而不影响性能。最后，该模型展示了强大的泛化能力，成功重建了与原始交通环境无关的物体。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效处理自动驾驶车辆中LiDAR传感器生成的大量3D点云数据问题。这个问题很重要，因为自动驾驶需要准确理解周围环境，而点云数据量大，需要能高效提取特征同时减少计算和内存需求的模型，现有的基于Transformer的架构虽然有效但计算成本高，限制了实时系统中的部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者意识到现有方法计算资源需求高，因此设计了一种轻量级框架。他们借鉴了自监督学习和掩码建模策略，但简化了架构；采用了编码器-解码器结构类似传统自编码器；利用跳跃连接概念但简化了实现；使用Chamfer Distance作为重建目标。这些借鉴帮助他们创建了一个更高效、更轻量的点云处理模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计轻量级自编码器架构，直接在稀疏3D点上操作，通过减少编码器层数和简化跳跃连接提高性能，在跳跃连接信息和潜在编码间实现平衡，学习紧凑且具有表达能力的潜在表示。流程包括：1)数据预处理(移除地面点、范围过滤、下采样)；2)编码器(1D卷积层处理点云，生成特征)；3)潜在空间(生成1024维潜在表示)；4)跳跃连接(仅保留最后编码层连接)；5)解码器(将特征转换回3D坐标重建点云)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出LiLa-Net直接在稀疏3D点上操作；2)能高效处理大型密集点云；3)成功重建复杂交通环境；4)证明潜在表示强大泛化能力；5)消除预训练或掩码策略。不同之处：简化了跳跃连接结构；减少编码器层数使模型更轻量；实现跳跃连接和潜在编码的平衡；不需要预训练训练流程更直接；保持高质量重建同时减少计算和内存需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiLa-Net是一种轻量级自编码器，能够高效压缩和重建3D点云数据，同时学习具有强大泛化能力的紧凑潜在表示，解决了自动驾驶环境中处理大规模点云数据的计算和内存挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work proposed a 3D autoencoder architecture, named LiLa-Net, whichencodes efficient features from real traffic environments, employing only theLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,equipped with Velodyne LiDAR. The system leverage skip connections concept toimprove the performance without using extensive resources as thestate-of-the-art architectures. Key changes include reducing the number ofencoder layers and simplifying the skip connections, while still producing anefficient and representative latent space which allows to accuratelyreconstruct the original point cloud. Furthermore, an effective balance hasbeen achieved between the information carried by the skip connections and thelatent encoding, leading to improved reconstruction quality withoutcompromising performance. Finally, the model demonstrates strong generalizationcapabilities, successfully reconstructing objects unrelated to the originaltraffic environment.</description>
      <author>example@mail.com (Mario Resino, Borja Pérez, Jaime Godoy, Abdulla Al-Kaff, Fernando García)</author>
      <guid isPermaLink="false">2510.02028v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Efficient manifold evolution algorithm using adaptive B-Spline interpolation</title>
      <link>http://arxiv.org/abs/2510.01790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种在光滑流形上演化点云数据的高效拉格朗日方法，使用B样条作为基础函数，替代传统径向基函数方法，实现点云数据的灵活操作和高效更新。&lt;h4&gt;背景&lt;/h4&gt;传统径向基函数(RBF)方法在高维流形上处理点云数据存在局限性，需要频繁重新插值，特别是在点密度波动较大的区域。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代传统RBF方法的高效拉格朗日方法，用于在光滑流形上演化点云数据，实现点云数据的无缝添加和删除，特别是在点密度变化大的区域。&lt;h4&gt;方法&lt;/h4&gt;使用B样条作为所有局部插值的基础函数，利用其系数具有几何意义的特性，使系数可以像点一样被操作，实现快速更新插值函数，避免频繁重新插值。&lt;h4&gt;主要发现&lt;/h4&gt;数值结果证明了所提方法在几何量收敛方面的有效性，以及点云数据添加和删除操作的无缝性，特别是在点密度波动较大的区域表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;B样条作为拉格朗日方法的基础函数能够有效替代传统RBF方法，提供更高效、更灵活的点云数据处理方式，特别适合处理点密度变化大的场景。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了一种在光滑流形上演化点云数据的高效拉格朗日方法。在这项初步研究中，我们专注于分析平面曲线，最终目标是为高维流形提供一种替代传统径向基函数(RBF)的方法。特别是，我们使用B样条作为所有局部插值的基础函数。与RBF和其他光滑基函数一样，B样条能够近似法向量和曲率等几何特征。一旦正确设置，使用B样条的优势在于其系数具有几何意义，这使得系数可以像点一样被操作，便于快速更新插值函数，并消除了频繁重新插值的需要。因此，点云数据的添加和删除变得无缝进行，特别是在点密度波动较大的区域特别有利。数值结果证明了几何量的收敛性和该方法的有效性。最后，我们展示了曲率流的模拟，其速度取决于耦合反应-扩散系统模式形成的解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.enganabound.2025.106488&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores an efficient Lagrangian approach for evolving point clouddata on smooth manifolds. In this preliminary study, we focus on analyzingplane curves, and our ultimate goal is to provide an alternative to theconventional radial basis function (RBF) approach for manifolds in higherdimensions. In particular, we use the B-Spline as the basis function for alllocal interpolations. Just like RBF and other smooth basis functions, B-Splinesenable the approximation of geometric features such as normal vectors andcurvature. Once properly set up, the advantage of using B-Splines is that theircoefficients carry geometric meanings. This allows the coefficients to bemanipulated like points, facilitates rapid updates of the interpolant, andeliminates the need for frequent re-interpolation. Consequently, the removaland insertion of point cloud data become seamless processes, particularlyadvantageous in regions experiencing significant fluctuations in point density.The numerical results demonstrate the convergence of geometric quantities andthe effectiveness of our approach. Finally, we show simulations of curvatureflows whose speeds depend on the solutions of coupled reaction--diffusionsystems for pattern formation.</description>
      <author>example@mail.com (Muhammad Ammad, Leevan Ling)</author>
      <guid isPermaLink="false">2510.01790v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Reducing Simulation Dependence in Neutrino Telescopes with Masked Point Transformers</title>
      <link>http://arxiv.org/abs/2510.01733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, presented at the 39th International Cosmic Ray  Conference (ICRC2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了中微子望远镜的首个自监督学习训练流程，利用点云变换器和掩码自编码器，减少对模拟数据的依赖，从而提高事件重建和分类的准确性。&lt;h4&gt;背景&lt;/h4&gt;传统中微子物理中的机器学习技术依赖模拟数据获取真实标签，但模拟数据的准确性以及模拟与真实数据间的差异仍是重大问题，特别是在复杂自然介质中运行的大型中微子望远镜。&lt;h4&gt;目的&lt;/h4&gt;减少对标记数据集的依赖，降低模拟数据带来的系统不确定性。&lt;h4&gt;方法&lt;/h4&gt;开发首个中微子望远镜的自监督训练流程，利用点云变换器和掩码自编码器，将大部分训练转移到真实数据上。&lt;h4&gt;主要发现&lt;/h4&gt;通过将训练重点转向真实数据，显著减少了对模拟数据的依赖，从而减轻了相关的系统不确定性问题。&lt;h4&gt;结论&lt;/h4&gt;这代表了中微子望远镜中机器学习应用的范式转变，为事件重建和分类技术的重大改进开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;中微子物理中的机器学习技术传统上依赖于模拟数据，这可以提供真实标签的访问。然而，这些模拟的准确性以及模拟数据和真实数据之间的差异仍然是一个重大问题，特别是在复杂自然介质中运行的大型中微子望远镜。近年来，自监督学习已成为减少对标记数据集依赖的强大范式。在这里，我们提出了中微子望远镜的第一个自监督训练流程，利用点云变换器和掩码自编码器。通过将大部分训练转移到真实数据，这种方法最小化了对模拟的依赖，从而减轻了相关的系统不确定性。这代表了中微子望远镜中机器学习应用的根本性转变，为事件重建和分类的重大改进铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning techniques in neutrino physics have traditionally relied onsimulated data, which provides access to ground-truth labels. However, theaccuracy of these simulations and the discrepancies between simulated and realdata remain significant concerns, particularly for large-scale neutrinotelescopes that operate in complex natural media. In recent years,self-supervised learning has emerged as a powerful paradigm for reducingdependence on labeled datasets. Here, we present the first self-supervisedtraining pipeline for neutrino telescopes, leveraging point cloud transformersand masked autoencoders. By shifting the majority of training to real data,this approach minimizes reliance on simulations, thereby mitigating associatedsystematic uncertainties. This represents a fundamental departure from previousmachine learning applications in neutrino telescopes, paving the way forsubstantial improvements in event reconstruction and classification.</description>
      <author>example@mail.com (Felix J. Yu, Nicholas Kamp, Carlos A. Argüelles)</author>
      <guid isPermaLink="false">2510.01733v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</title>
      <link>http://arxiv.org/abs/2510.01665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为Con-NRSfM的新方法，用于解决单目视觉可变形SLAM中的非刚性结构从运动问题。该方法通过基于图框架优化的2D选择图像变形进行逐点重建，消除了现有方法的严格假设，能够准确计算局部共形尺度并解耦深度和共形尺度的约束，实现了更精确的深度估计。实验证明该方法在重建精度和鲁棒性上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;非刚性结构从运动技术是解决单目视觉可变形同时定位与地图构建中映射挑战的一种有前景的方法，近年来引起了越来越多的关注。&lt;h4&gt;目的&lt;/h4&gt;引入一种名为Con-NRSfM的新方法，用于处理共形变形下的非刚性结构从运动问题，消除现有方法的严格假设限制，准确计算局部共形尺度，解耦深度和共形尺度的约束，实现更精确的深度估计。&lt;h4&gt;方法&lt;/h4&gt;使用基于图框架优化的2D选择图像变形进行逐点重建，采用并行可分离迭代优化策略解决公式化问题的敏感性，集成自监督学习框架，使用编码器-解码器网络生成带纹理的密集3D点云。&lt;h4&gt;主要发现&lt;/h4&gt;该方法消除了对局部平面表面或局部线性变形的严格假设，能够恢复共形尺度，解耦了深度和共形尺度的约束使深度估计更精确，在合成和真实数据集上的实验结果表明，该方法在重建精度和鲁棒性方面超越了现有方法。&lt;h4&gt;结论&lt;/h4&gt;Con-NRSfM方法在非刚性结构从运动领域表现出色，代码将在项目网站上公开：https://sites.google.com/view/con-nrsfm。&lt;h4&gt;翻译&lt;/h4&gt;Non-rigid structure-from-motion (NRSfM): 非刚性结构从运动；monocular visual deformable simultaneous localization and mapping (SLAM): 单目视觉可变形同时定位与地图构建；conformal deformations: 共形变形；isometric deformations: 等距变形；point-wise reconstruction: 逐点重建；image warps: 图像变形；graph-based framework: 基于图框架；locally planar surfaces: 局部平面表面；locally linear deformations: 局部线性变形；conformal scale: 共形尺度；depth estimation: 深度估计；parallel separable iterative optimization strategy: 并行可分离迭代优化策略；self-supervised learning framework: 自监督学习框架；encoder-decoder network: 编码器-解码器网络；dense 3D point clouds: 密集3D点云；reconstruction accuracy: 重建精度；robustness: 鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决非刚性结构运动恢复（NRSfM）中的挑战，特别是在共形变形条件下的3D形状重建问题。这个问题在现实中非常重要，因为它对于单目视觉SLAM在变形环境中的应用至关重要，能够帮助机器人在动态变化的环境中（如医疗手术、软体物体交互等）进行精确导航和地图构建，解决了传统SLAM方法无法处理的非刚性场景问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有NRSfM方法的局限性，包括统计约束方法对复杂变形效果不佳、全局物理约束方法计算复杂度高、以及局部物理约束方法依赖不切实际的假设（如局部平面表面和局部线性变形）。基于微分几何理论，作者发现了共形变形下连接（connections）和活动标架（moving frames）的旋转不变性性质，证明了共形尺度和深度估计可以解耦。他们借鉴了现有局部物理约束方法（如[11]、[12]、[13]和[37]），但通过理论创新克服了这些方法的局限性，设计了并行可分离的迭代优化算法和自监督学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用微分几何理论中的旋转不变性性质，放弃现有方法中的局部平面表面和局部线性变形假设，将共形尺度和深度估计解耦，使用并行可分离的迭代优化策略提高鲁棒性。整体流程包括：1)构建完整加权图并选择连接良好的子图；2)基于选定的图像变换进行点级重建；3)通过并行可分离迭代优化（包括预步骤和四个主要步骤）优化深度、法线和共形尺度；4)可选地使用自监督编码器-解码器网络生成带纹理的密集3D点云；5)输出稀疏点云或密集带纹理的表面。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)理论创新，证明了共形变形下连接的旋转不变性，实现了共形尺度和深度估计的解耦；2)方法创新，放弃了局部平面表面和局部线性变形假设，设计了并行可分离的迭代优化算法，提出了基于共形尺度的物理约束；3)结合自监督学习框架生成密集点云。相比之前的工作，不同之处在于：不需要依赖不切实际的假设，能恢复共形尺度，将深度和共形尺度的约束解耦，使用更鲁棒的优化策略，并能生成更全面的密集点云表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过微分几何理论和并行可分离优化框架，提出了一种能够在共形变形条件下准确恢复3D形状并估计共形尺度的非刚性结构运动恢复方法，显著提高了变形物体重建的精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-rigid structure-from-motion (NRSfM), a promising technique for addressingthe mapping challenges in monocular visual deformable simultaneous localizationand mapping (SLAM), has attracted growing attention. We introduce a novelmethod, called Con-NRSfM, for NRSfM under conformal deformations, encompassingisometric deformations as a subset. Our approach performs point-wisereconstruction using 2D selected image warps optimized through a graph-basedframework. Unlike existing methods that rely on strict assumptions, such aslocally planar surfaces or locally linear deformations, and fail to recover theconformal scale, our method eliminates these constraints and accuratelycomputes the local conformal scale. Additionally, our framework decouplesconstraints on depth and conformal scale, which are inseparable in otherapproaches, enabling more precise depth estimation. To address the sensitivityof the formulated problem, we employ a parallel separable iterativeoptimization strategy. Furthermore, a self-supervised learning framework,utilizing an encoder-decoder network, is incorporated to generate dense 3Dpoint clouds with texture. Simulation and experimental results using bothsynthetic and real datasets demonstrate that our method surpasses existingapproaches in terms of reconstruction accuracy and robustness. The code for theproposed method will be made publicly available on the project website:https://sites.google.com/view/con-nrsfm.</description>
      <author>example@mail.com (Yongbo Chen, Yanhao Zhang, Shaifali Parashar, Liang Zhao, Shoudong Huang)</author>
      <guid isPermaLink="false">2510.01665v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion</title>
      <link>http://arxiv.org/abs/2510.01592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 12 figures, This work has been submitted to the IEEE for  possible publication. Copyright may be transfered without notice, after which  this version may no longer be accessible&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于GPU加速的高分辨率3D体素映射的实时多平面分割方法，用于足式机器人运动控制。&lt;h4&gt;背景&lt;/h4&gt;现有的在线平面映射方法难以平衡准确性和计算效率：直接深度图像分割存在时间积分差问题；基于高度图的方法无法表示悬挑等复杂3D结构；基于体素的平面分割在实时应用中尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;解决现有平面映射方法的局限性，开发一种能够快速准确提取3D平面区域的新框架。&lt;h4&gt;方法&lt;/h4&gt;结合基于顶点的连通分量标记与基于随机样本一致性的平面检测和凸包，利用GPU并行计算从高分辨率3D体素图中积累的点云中快速提取平面区域。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能够在0.01米的分辨率下实现超过30Hz更新率的快速准确的3D多平面分割，使检测到的平面能够实时用于运动控制任务。&lt;h4&gt;结论&lt;/h4&gt;通过在模拟环境和物理足式机器人平台上的实验验证了该方法的有效性，考虑3D平面结构时能够实现稳健的运动性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于GPU加速的高分辨率3D体素映射的实时多平面分割方法，用于足式机器人运动。现有的在线平面映射方法难以平衡准确性和计算效率：直接从特定传感器进行深度图像分割存在时间积分差的问题，基于高度图的方法无法表示悬挑等复杂3D结构，而基于体素的平面分割在实时应用中尚未被探索。为解决这些限制，我们开发了一种新的框架，结合基于顶点的连通分量标记与基于随机样本一致性的平面检测和凸包，利用GPU并行计算从高分辨率3D体素图中积累的点云中快速提取平面区域。实验结果表明，提出的方法能够在0.01米的分辨率下实现超过30Hz更新率的快速准确的3D多平面分割，使检测到的平面能够实时用于运动控制任务。此外，我们通过在模拟环境和物理足式机器人平台上的实验验证了该方法的有效性，考虑3D平面结构时能够实现稳健的运动性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决足式机器人在复杂3D环境中运动时，如何实现实时、高精度的多平面分割问题。这个问题很重要，因为足式机器人需要准确识别可站立区域和稳定平面表面才能安全运动，而现有方法要么无法表示悬挑等复杂结构（如高度图方法），要么计算效率太低无法实时处理（如传统体素方法），导致机器人在开放式楼梯、桌下等环境中容易发生碰撞和运动失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计新方法：直接传感器分割方法时间积分差，高度图方法无法表示复杂3D结构，传统体素分割无法实时处理。作者借鉴了GPU加速3D体素映射技术、基于高度图的环境感知方法、RANSAC平面检测和GPU图像分割等现有工作，但创新性地将它们整合到一个专门针对足式机器人运动优化的框架中，通过并行计算解决了准确性和实时性的平衡问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用GPU并行计算能力实现高分辨率三维体素映射基础上的实时多平面分割。整体流程分为两大部分：1) 3D体素映射模块：接收传感器点云和机器人姿态，高效累积到体素地图中并通过射线投射移除动态物体；2) 多平面分割模块：将体素分类为可站立点和物体点，对可站立点进行聚类，对每个聚类进行平面参数估计，最后生成边界多边形。整个流程充分利用GPU并行能力，实现了超过30Hz的更新速率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) GPU加速的3D多平面分割，利用连通分量标记聚类和并行平面边界估计；2) 综合框架整合3D体素映射与多平面分割；3) 在5m×5m×5m范围内实现0.01米分辨率的实时处理。相比之前工作，这篇论文完全基于GPU实现（而非CPU-GPU混合），支持多种传感器配置，并引入簇级并行处理显著提高了多平面分割效率，同时解决了准确性和实时性的平衡问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于GPU加速的高分辨率3D体素映射的实时多平面分割方法，使足式机器人能够准确感知复杂3D环境中的多层平面表面，从而安全高效地完成运动任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a real-time multi-plane segmentation method based onGPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.Existing online planar mapping approaches struggle to balance accuracy andcomputational efficiency: direct depth image segmentation from specific sensorssuffers from poor temporal integration, height map-based methods cannotrepresent complex 3D structures like overhangs, and voxel-based planesegmentation remains unexplored for real-time applications. To address theselimitations, we develop a novel framework that integrates vertex-basedconnected component labeling with random sample consensus based plane detectionand convex hull, leveraging GPU parallel computing to rapidly extract planarregions from point clouds accumulated in high-resolution 3D voxel maps.Experimental results demonstrate that the proposed method achieves fast andaccurate 3D multi-plane segmentation at over 30 Hz update rate even at aresolution of 0.01 m, enabling the detected planes to be utilized in real timefor locomotion tasks. Furthermore, we validate the effectiveness of ourapproach through experiments in both simulated environments and physical leggedrobot platforms, confirming robust locomotion performance when considering 3Dplanar structures.</description>
      <author>example@mail.com (Shun Niijima, Ryoichi Tsuzaki, Noriaki Takasugi, Masaya Kinoshita)</author>
      <guid isPermaLink="false">2510.01592v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.01433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AFFORD2ACT框架，一种从文本提示和单张图像中提取语义2D关键点的功能引导方法，实现了高效且可扩展的视觉机器人学习。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的机器人学习通常依赖密集图像或点云输入，计算量大且包含不相关背景特征；现有关键点方法虽轻量但依赖手动启发式或与任务耦合选择，限制了可扩展性和语义理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从文本提示和单张图像中提取最小语义2D关键点的框架，解决现有方法的局限性，提高数据效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;AFFORD2ACT框架采用三阶段流程：功能过滤、类别级别关键点构建和基于transformer的策略学习（带有嵌入式门控机制），生成38维状态策略，15分钟内完成训练。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在实时执行中表现良好，无需本体感受或密集表示；在多样化现实操作任务中持续提高数据效率，在未见物体、新类别、背景和干扰物上达到82%成功率。&lt;h4&gt;结论&lt;/h4&gt;AFFORD2ACT是一个有效的框架，能够从文本提示和单张图像中提取语义关键点，在各种现实操作任务中表现出色，具有高数据效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的机器人学习通常依赖于密集图像或点云输入，这些计算量大且包含不相关的背景特征。现有关键点方法虽然可以专注于操作相关的特征并且轻量级，但要么依赖于手动启发式规则，要么与任务耦合选择，限制了可扩展性和语义理解。为解决这一问题，我们提出了AFFORD2ACT，一个由功能引导的框架，可以从文本提示和单张图像中提取最小语义2D关键点。AFFORD2ACT遵循三阶段流程：功能过滤、类别级别关键点构建和基于transformer的策略学习（带有嵌入式门控机制），能够推理最相关的关键点，生成一个紧凑的38维状态策略，可以在15分钟内训练完成，在无需本体感受或密集表示的情况下实时表现良好。在多样化的现实操作任务中，AFFORD2ACT持续提高了数据效率，在未见过的物体、新类别、背景和干扰物上达到82%的成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based robot learning often relies on dense image or point-cloudinputs, which are computationally heavy and entangle irrelevant backgroundfeatures. Existing keypoint-based approaches can focus on manipulation-centricfeatures and be lightweight, but either depend on manual heuristics ortask-coupled selection, limiting scalability and semantic understanding. Toaddress this, we propose AFFORD2ACT, an affordance-guided framework thatdistills a minimal set of semantic 2D keypoints from a text prompt and a singleimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,category-level keypoint construction, and transformer-based policy learningwith embedded gating to reason about the most relevant keypoints, yielding acompact 38-dimensional state policy that can be trained in 15 minutes, whichperforms well in real-time without proprioception or dense representations.Across diverse real-world manipulation tasks, AFFORD2ACT consistently improvesdata efficiency, achieving an 82% success rate on unseen objects, novelcategories, backgrounds, and distractors.</description>
      <author>example@mail.com (Anukriti Singh, Kasra Torshizi, Khuzema Habib, Kelin Yu, Ruohan Gao, Pratap Tokekar)</author>
      <guid isPermaLink="false">2510.01433v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking</title>
      <link>http://arxiv.org/abs/2510.01349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A short version of this paper appeared at the ICLR AI4Mat workshop in  April 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对称性感知方法在机器学习中的应用，提出了一种评估数据集中对称性假设的方法，通过量化各向异性来分析对称性感知方法的效果，发现其效果取决于数据集特性。&lt;h4&gt;背景&lt;/h4&gt;对称性感知方法（如数据增强和等变架构）鼓励模型对所有原始数据集的变换（如旋转或排列）表现出正确行为，可提高泛化能力和样本效率，但其有效性依赖于变换后的数据点在测试分布下具有高概率或'重要性'的假设。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来批判性评估对称性感知方法所依赖的假设，即量化数据集中的各向异性或对称性破坏程度。&lt;h4&gt;方法&lt;/h4&gt;提出一种度量指标，通过两样本神经分类器测试区分原始数据集及其随机增强的等效版本，在合成数据集上验证该指标，并应用于分析几个基准点云数据集。&lt;h4&gt;主要发现&lt;/h4&gt;1) 理论上证明分布对称性破坏可阻止不变方法达到最优性能，即使底层标签真正不变；2) 实证研究发现对称性感知方法的效果是数据依赖的；3) 在几个基准点云数据集中发现了出人意料的高度对齐程度。&lt;h4&gt;结论&lt;/h4&gt;理解等变性（包括它何时有效以及为何有效）可能需要重新思考数据中的对称性偏差，对称性感知方法的效果取决于数据集特性。&lt;h4&gt;翻译&lt;/h4&gt;对称性感知的机器学习方法，如数据增强和等变架构，鼓励模型对所有原始数据集的变换（例如旋转或排列）表现出正确的行为。这些方法可以提高泛化能力和样本效率，其假设是变换后的数据点在测试分布下具有高概率或'重要性'。在本工作中，我们开发了一种方法来批判性评估这一假设。特别是，我们提出了一种度量指标，通过两样本神经分类器测试来区分原始数据集及其随机增强的等效版本，从而量化数据集中的各向异性或对称性破坏程度。我们在合成数据集上验证了我们的指标，然后使用它来揭示几个基准点云数据集中出人意料的高度对齐程度。我们从理论上证明，分布对称性破坏实际上可以阻止不变方法达到最优性能，即使底层标签真正是不变的，正如我们在无限特征限制下的不变岭回归中所展示的那样。从经验上看，我们发现对称性感知方法的含义是数据依赖的：等变方法仍然在某些各向异性数据集上带来好处，但在其他数据集上则没有。总的来说，这些发现表明，理解等变性——无论它何时有效以及为何有效——可能需要重新思考数据中的对称性偏差。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决数据增强和等变方法在应用中的核心假设问题，即变换后的数据点在测试分布下是否具有高概率或'重要性'。这个问题很重要，因为错误地应用对称性假设可能导致模型性能下降，理解数据中的对称性破坏有助于更有效地应用数据增强和等变方法，提高模型泛化能力和样本效率，同时指导模型设计。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别对称感知方法中隐含的假设局限性，从理论角度分析对称性破坏对等变方法的影响，进而提出基于双样本分类器测试的度量方法。作者借鉴了Lopez-Paz &amp; Oquab的双样本分类器测试方法用于分布差异检测，参考了Chiu &amp; Bloem-Reddy的对称性检测工作但避免了核选择问题，同时基于现有的等变机器学习方法进行研究但关注其在对称性破坏情况下的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过训练一个分类器来区分原始数据点和经过随机变换的数据点，分类器的准确率作为对称性破坏程度的度量。实现流程包括：1) 输入未标记数据集和群组；2) 将数据集分成两部分并对一部分应用随机变换；3) 构建二分类数据集(原始样本标记0，变换样本标记1)；4) 训练二分类器；5) 返回分类器在测试集上的准确率，值越接近1表示对称性破坏程度越高，越接近0.5表示数据越对称。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出无需特定领域知识的分布对称性破坏度量方法；2) 提供在对称性破坏情况下不变性岭回归的理论分析；3) 在多个基准数据集上发现高度对称性破坏并评估等变方法效果；4) 探索局部对称性破坏概念。相比之前工作，作者的方法避免了核选择问题，扩展了理论分析到非对称数据分布，在更广泛数据集上验证了方法，明确区分了分布对称性破坏和功能对称性破坏，并引入了任务相关的对称性破坏度量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种量化数据集中分布对称性破坏的新方法，并通过理论和实验揭示了等变机器学习方法在对称性破坏数据集上的复杂行为，为何时以及如何应用对称性假设提供了重要见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Symmetry-aware methods for machine learning, such as data augmentation andequivariant architectures, encourage correct model behavior on alltransformations (e.g. rotations or permutations) of the original dataset. Thesemethods can improve generalization and sample efficiency, under the assumptionthat the transformed datapoints are highly probable, or "important", under thetest distribution. In this work, we develop a method for critically evaluatingthis assumption. In particular, we propose a metric to quantify the amount ofanisotropy, or symmetry-breaking, in a dataset, via a two-sample neuralclassifier test that distinguishes between the original dataset and itsrandomly augmented equivalent. We validate our metric on synthetic datasets,and then use it to uncover surprisingly high degrees of alignment in severalbenchmark point cloud datasets. We show theoretically that distributionalsymmetry-breaking can actually prevent invariant methods from performingoptimally even when the underlying labels are truly invariant, as we show forinvariant ridge regression in the infinite feature limit. Empirically, we findthat the implication for symmetry-aware methods is dataset-dependent:equivariant methods still impart benefits on some anisotropic datasets, but notothers. Overall, these findings suggest that understanding equivariance -- bothwhen it works, and why -- may require rethinking symmetry biases in the data.</description>
      <author>example@mail.com (Hannah Lawrence, Elyssa Hofgard, Vasco Portilheiro, Yuxuan Chen, Tess Smidt, Robin Walters)</author>
      <guid isPermaLink="false">2510.01349v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Approximate mean curvature flows of a general varifold, and their limit spacetime Brakke flow</title>
      <link>http://arxiv.org/abs/2510.00746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于变分流形理论的近似方法，用于构建适用于非常一般初始数据的平均曲率流。&lt;h4&gt;背景&lt;/h4&gt;基于Brakke和Kim &amp; Tonegawa的工作，利用变分流形理论构建平均曲率流。&lt;h4&gt;目的&lt;/h4&gt;为任意维度和余维度的非常一般结构（从连续表面到离散点云）提供一种近似平均曲率流的概念。&lt;h4&gt;方法&lt;/h4&gt;通过迭代前向构造一个依赖于给定时间步长和近似参数的近似时间离散平均曲率流。&lt;h4&gt;主要发现&lt;/h4&gt;当时间步长趋于0时，时间离散流收敛到唯一的近似平均曲率流；该流满足稳定性、唯一性、Brakke型等式和质量衰减等性质；当近似参数趋于0时，与规范时间测度耦合收敛到广义平均曲率有界的时空极限测度。&lt;h4&gt;结论&lt;/h4&gt;在可求长性假设下，极限测度是一个时空Brakke流，为一般结构的平均曲率流提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过近似方法构建平均曲率流的构造方法，适用于非常一般的初始数据，遵循Brakke和Kim &amp; Tonegawa基于变分流形理论的工作精神。给定一个一般的变分流形，我们通过迭代前向构造一个依赖于给定时间步长和近似参数的近似时间离散平均曲率流。我们证明，当时间步长趋于0时，这个时间离散流收敛到一个唯一的极限流，我们称之为近似平均曲率流。我们方法的一个有趣特点是它的普适性，因为它为任意维度和余维度的非常一般的结构提供了平均曲率流的近似概念，从连续表面到离散点云。我们证明，我们的近似平均曲率流满足几个性质：稳定性、唯一性、Brakke型等式、质量衰减。通过将这种近似流与规范时间测度耦合，我们证明，当近似参数趋于0时，收敛到一个时空极限测度，其广义平均曲率有界。在额外的可求长性假设下，我们进一步证明这个极限测度是一个时空Brakke流。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a construction of mean curvature flows by approximation for verygeneral initial data, in the spirit of the works of Brakke and of Kim &amp;Tonegawa based on the theory of varifolds. Given a general varifold, weconstruct by iterated push-forwards an approximate time-discrete mean curvatureflow depending on both a given time step and an approximation parameter. Weshow that, as the time step tends to $0$, this time-discrete flow converges toa unique limit flow, which we call the approximate mean curvature flow. Aninteresting feature of our approach is its generality, as it provides anapproximate notion of mean curvature flow for very general structures of anydimension and codimension, ranging from continuous surfaces to discrete pointclouds. We prove that our approximate mean curvature flow satisfies severalproperties: stability, uniqueness, Brakke-type equality, mass decay. Bycoupling this approximate flow with the canonical time measure, we proveconvergence, as the approximation parameter tends to $0$, to a spacetime limitmeasure whose generalized mean curvature is bounded. Under an additionalrectifiability assumption, we further prove that this limit measure is aspacetime Brakke flow.</description>
      <author>example@mail.com (Blanche Buet, Gian Paolo Leonardi, Simon Masnou, Abdelmouksit Sagueni)</author>
      <guid isPermaLink="false">2510.00746v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review</title>
      <link>http://arxiv.org/abs/2510.01296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述论文调查了基于深度学习的3D MRI重建方法，重点关注四种主要方法：点云、基于网格、形状感知和体积模型，分析了它们的技术基础、限制和应用领域。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的3D形状重建从2D磁共振成像技术在医学疾病诊断、治疗计划和计算建模中变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;为研究人员提供当前3D重建方法的结构化概述，以识别推进深度学习向更强大、可推广和具有临床影响力的解决方案的机会。&lt;h4&gt;方法&lt;/h4&gt;分析四种主要3D重建方法（点云、基于网格、形状感知和体积模型）的最新技术、方法论基础和限制；考察这些方法在心脏、神经和肺部等不同解剖结构中的应用；评估模型对病理解剖的临床适用性及其训练和测试数据的影响；检查公开数据集、计算需求和评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;各种3D重建方法在不同解剖结构和疾病应用中各有优势和限制；训练数据的选择和模型计算需求对重建质量有显著影响；多模态集成和跨模态框架是新兴的研究方向。&lt;h4&gt;结论&lt;/h4&gt;深度学习在3D MRI重建领域展现出巨大潜力，未来的研究应关注提高方法的鲁棒性、可推广性和临床实用性，同时探索多模态融合等创新方向。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的从二维磁共振成像到三维形状重建在医学疾病诊断、治疗计划和计算建模中变得越来越重要。本综述调查了3D MRI重建的方法论现状，重点关注四种主要方法：点云、基于网格、形状感知和体积模型。对于每个类别，我们分析了当前最先进的技术、其方法论基础、限制以及在不同解剖结构中的应用。我们提供了从心脏、神经到肺部成像的广泛概述。我们还关注模型对病理解剖的临床适用性及其训练和测试数据的影响。我们检查了公开可用的数据集、计算需求和评估指标。最后，我们突出了包括多模态集成和跨模态框架在内的新兴研究方向。本综述旨在为研究人员提供当前3D重建方法的概述，以识别推进深度学习向更强大、可推广和具有临床影响力的解决方案的机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从2D MRI图像重建出准确的3D形状的问题。这个问题在现实中非常重要，因为准确的3D器官表示能为疾病状态和功能提供无与伦比的见解，使医生能够进行更精确、有效和个性化的治疗。虽然医生能够解释2D MRI图像并在脑海中构建3D解剖结构，但使用深度学习直接生成3D模型可以提高效率和准确性，特别是在处理扫描质量差异、解剖多样性和数据稀缺性等挑战时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这是一篇综述文章，作者通过系统性地回顾和分析现有研究来组织内容。作者将现有的3D MRI重建方法分为四类：点云方法、网格方法、形状感知方法和体积模型方法。对于每类方法，作者分析了最先进技术、方法基础、局限性和应用场景。作者借鉴了大量现有工作，包括CNN、GAN和扩散模型等架构，并参考了其他综述文章，同时指出了这些综述的局限性，如专注于特定器官（如心脏）或缺乏对临床适用性的讨论。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 由于这是一篇综述文章，没有单一的核心思想或实现流程。相反，论文综述了多种方法的核心思想和实现流程：1）点云方法将解剖结构表示为3D空间中的点集合，通过编码器-解码器架构生成全局和局部特征；2）网格方法生成连续表面表示，使用互连的顶点、边和面形成多边形网格；3）形状感知方法利用统计或学习先验约束输出空间，确保解剖合理性；4）体积方法生成基于体素的完整3D表示，通常结合数据驱动先验和基于模型的重建技术。这些方法通常从2D MRI切片开始，通过深度学习模型生成3D表示，最终输出可以是点云、网格或体积数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 作为一篇综述文章，其创新点在于综述的方法和视角：1）提供了更全面的综述范围，涵盖从心脏到神经到肺部成像的广泛应用，而非专注于特定器官；2）特别关注模型对病理解剖的临床适用性和训练数据的影响；3）强调多模态集成和跨模态框架等新兴研究方向；4）评估了当前评估指标的有效性并提供了未来研究建议；5）探讨了非MRI重建方法（如动画和自然图像处理）在MRI重建中的潜在应用。相比其他综述，本文更注重不同器官间的技术转移机会和临床实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了基于深度学习的3D MRI重建技术的全面综述，系统分析了四类主要方法，为研究人员指出了推进深度学习向更稳健、可泛化和具有临床影响力的3D重建解决方案的机会。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based 3-dimensional (3D) shape reconstruction from2-dimensional (2D) magnetic resonance imaging (MRI) has become increasinglyimportant in medical disease diagnosis, treatment planning, and computationalmodeling. This review surveys the methodological landscape of 3D MRIreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,shape-aware, and volumetric models. For each category, we analyze the currentstate-of-the-art techniques, their methodological foundation, limitations, andapplications across anatomical structures. We provide an extensive overviewranging from cardiac to neurological to lung imaging. We also focus on theclinical applicability of models to diseased anatomy, and the influence oftheir training and testing data. We examine publicly available datasets,computational demands, and evaluation metrics. Finally, we highlight theemerging research directions including multimodal integration andcross-modality frameworks. This review aims to provide researchers with astructured overview of current 3D reconstruction methodologies to identifyopportunities for advancing deep learning towards more robust, generalizable,and clinically impactful solutions.</description>
      <author>example@mail.com (Emma McMillian, Abhirup Banerjee, Alfonso Bueno-Orovio)</author>
      <guid isPermaLink="false">2510.01296v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI</title>
      <link>http://arxiv.org/abs/2510.02120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  My preview .pdf was not loading. Can you please share with me a  compiled .pdf file so I can confirm that the result is correct?&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为VarCoNet的新型自监督框架，用于从静息态功能磁共振成像数据中稳健提取功能连接组，该框架将个体间功能变异性视为有价值信息而非噪声。&lt;h4&gt;背景&lt;/h4&gt;个体间脑功能变异性是精准医疗的关键因素，但传统方法通常将其视为噪声而非有意义的数据。&lt;h4&gt;目的&lt;/h4&gt;开发能够有效利用个体间功能变异性来提取功能连接组的框架，生成适用于下游任务的FC嵌入，即使在缺乏标记数据的情况下也能工作。&lt;h4&gt;方法&lt;/h4&gt;VarCoNet采用自监督对比学习框架，结合基于静息态fMRI信号分割的新型增强策略，核心是1D-CNN-Transformer编码器用于时间序列处理，并集成了贝叶斯超参数优化。&lt;h4&gt;主要发现&lt;/h4&gt;在受试者指纹识别和自闭症谱系障碍分类两个下游任务上，使用不同脑区分割方法，与13种最先进方法相比，VarCoNet显示出优越性、稳健性、可解释性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;VarCoNet为静息态fMRI中的功能连接组分析提供了一个多功能且稳健的框架。&lt;h4&gt;翻译&lt;/h4&gt;考虑个体间脑功能变异性是精准医疗的关键。在这里，通过将功能个体间变异性视为有意义的数据而非噪声，我们引入了VarCoNet，这是一种增强的自监督框架，用于从静息态功能磁共振成像数据中稳健地提取功能连接组。VarCoNet采用自监督对比学习来利用内在的功能个体间变异性，充当脑功能编码器，生成可直接适用于下游任务的FC嵌入，即使在没有标记数据的情况下也是如此。对比学习通过一种基于静息态fMRI信号分割的新型增强策略得到促进。其核心是，VarCoNet集成了一个1D-CNN-Transformer编码器用于高级时间序列处理，并增强了稳健的贝叶斯超参数优化。我们的VarCoNet框架在两个下游任务上进行了评估：（i）使用人类连接组计划的rs-fMRI数据进行受试者指纹识别，以及（ii）使用ABIDE I和ABIDE II数据集的rs-fMRI数据进行自闭症谱系障碍分类。使用不同的脑区分割方法，我们与包括13种深度学习方法在内的最先进方法进行了广泛测试，证明了VarCoNet的优越性、稳健性、可解释性和泛化能力。总体而言，VarCoNet为rs-fMRI中的FC分析提供了一个多功能且稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accounting for inter-individual variability in brain function is key toprecision medicine. Here, by considering functional inter-individualvariability as meaningful data rather than noise, we introduce VarCoNet, anenhanced self-supervised framework for robust functional connectome (FC)extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employsself-supervised contrastive learning to exploit inherent functionalinter-individual variability, serving as a brain function encoder thatgenerates FC embeddings readily applicable to downstream tasks even in theabsence of labeled data. Contrastive learning is facilitated by a novelaugmentation strategy based on segmenting rs-fMRI signals. At its core,VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-seriesprocessing, enhanced with a robust Bayesian hyperparameter optimization. OurVarCoNet framework is evaluated on two downstream tasks: (i) subjectfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)autism spectrum disorder (ASD) classification, using rs-fMRI data from theABIDE I and ABIDE II datasets. Using different brain parcellations, ourextensive testing against state-of-the-art methods, including 13 deep learningmethods, demonstrates VarCoNet's superiority, robustness, interpretability, andgeneralizability. Overall, VarCoNet provides a versatile and robust frameworkfor FC analysis in rs-fMRI.</description>
      <author>example@mail.com (Charalampos Lamprou, Aamna Alshehhi, Leontios J. Hadjileontiadis, Mohamed L. Seghier)</author>
      <guid isPermaLink="false">2510.02120v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision</title>
      <link>http://arxiv.org/abs/2510.01860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SLAP模型，这是首个通过对比学习将语音与说话者和健康元数据的自然语言描述对齐的音频基础模型。该模型在多种语言和任务上展示了强大的零样本和分布外泛化能力，特别是在健康相关任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;语音编码了副语言学信息如人口统计、语音质量和健康信息，但目前没有音频基础模型支持这些任务的零样本或分布外（OOD）泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够将语音与自然语言描述对齐的模型，实现对副语言学信息的零样本和分布外泛化，特别是在人口统计、语音特征和健康评估任务上。&lt;h4&gt;方法&lt;/h4&gt;提出SLAP（Speaker contrastive Language-Audio Pretraining）模型，结合Vision Transformer音频编码器和文本编码器，通过对比学习对齐语音与自然语言描述。在9个数据集上使用超过3400小时的数据进行训练，涵盖多样化的说话者注释。&lt;h4&gt;主要发现&lt;/h4&gt;SLAP在零样本评估中实现62.9%的平均F1分数，比CLAP相对提高48%；展示了在未见语言和临床人群上的强OOD泛化能力；通过线性探测微调后总体达到69.3%的F1分数，在健康任务上实现57.9%的F1分数，表现优于更大的基础模型。&lt;h4&gt;结论&lt;/h4&gt;SLAP是首个能够有效对齐语音与自然语言描述的音频基础模型，在多种副语言学任务上展示了强大的零样本和分布外泛化能力，特别是在健康相关任务上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音编码了副语言学信息，如人口统计、语音质量和健康信息。然而，目前没有音频基础模型支持这些任务的零样本或分布外（OOD）泛化。我们介绍了SLAP（Speaker contrastive Language-Audio Pretraining），这是第一个通过对比学习将语音与说话者和健康元数据的自然语言描述对齐的模型。SLAP结合了Vision Transformer音频编码器和文本编码器，在9个具有不同说话者注释的数据集上进行了超过3400小时的训练。我们在7种语言的14个数据集上的38个二元分类任务（涵盖人口统计、语音特征和临床评估）上进行了评估。SLAP在零样本评估中实现了62.9%的平均F1分数，比CLAP（42.4%）相对提高了48%，同时展示了在未见过的语言和临床人群上的强OOD泛化能力。当通过线性探测微调时，SLAP总体上达到69.3%的F1分数，并在健康任务上实现了最先进的性能（57.9% F1），超过了更大的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech encodes paralinguistic information such as demographics, voicequality, and health. Yet no audio foundation model supports zero-shot orout-of-distribution (OOD) generalization to these tasks. We introduce SLAP(Speaker contrastive Language-Audio Pretraining), the first model aligningspeech with natural language descriptions of speaker and health metadatathrough contrastive learning. SLAP combines a Vision Transformer audio encoderwith text encoders, trained on more than 3400 hours across 9 datasets withdiverse speaker annotations. We evaluated on 38 binary classification tasksspanning demographics, voice characteristics, and clinical assessments across14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shotevaluation, a 48% relative improvement over CLAP (42.4%), while demonstratingstrong OOD generalization to unseen languages and clinical populations. Whenfine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achievesbest-in-class performance on health tasks (57.9% F1), surpassing largerfoundation models.</description>
      <author>example@mail.com (Angelika Ando, Auguste Crabeil, Adrien Lesage, Rachid Riad)</author>
      <guid isPermaLink="false">2510.01860v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention</title>
      <link>http://arxiv.org/abs/2510.01652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在大型语言模型中启用双向注意力机制，以克服单向注意力机制在文本嵌入任务和语义表示分析方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;自回归大型语言模型在语言理解和生成方面表现出色，但由于单向注意力机制的约束，在文本嵌入任务中的应用发展较慢，并且在探测任务中的语义表示分析也相对滞后。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以通过在大型语言模型中启用双向注意力来克服这些约束。&lt;h4&gt;方法&lt;/h4&gt;通过对Llama架构的不同变体进行额外的训练步骤，逐步启用双向注意力，并结合无监督/监督对比学习进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论。&lt;h4&gt;翻译&lt;/h4&gt;自回归大型语言模型在语言理解和生成方面表现出色。然而，由于单向注意力机制的约束，它们在文本嵌入任务中的应用相对较慢，同时在探测任务中对其语义表示的分析也较为滞后。本文旨在探索是否可以通过在大型语言模型中启用双向注意力来克服这些约束。我们通过对Llama架构的不同变体进行额外的训练步骤，逐步启用双向注意力，并结合无监督/监督对比学习进行测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoregressive Large Language Models (LLMs) demonstrate exceptionalperformance in language understanding and generation. However, theirapplication in text embedding tasks has been relatively slow, along with theanalysis of their semantic representation in probing tasks, due to theconstraints of the unidirectional attention mechanism.  This paper aims to explore whether such constraints can be overcome byenabling bidirectional attention in LLMs. We tested different variants of theLlama architecture through additional training steps, progressively enablingbidirectional attention and unsupervised/supervised contrastive learning.</description>
      <author>example@mail.com (Zhaoxin Feng, Jianfei Ma, Emmanuele Chersoni, Xiaojing Zhao, Xiaoyi Bao)</author>
      <guid isPermaLink="false">2510.01652v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>It Takes Two: Your GRPO Is Secretly DPO</title>
      <link>http://arxiv.org/abs/2510.00977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文挑战了GRPO算法需要大组大小的传统假设，通过将GRPO重新构建为对比学习形式，揭示了其与DPO的联系，并验证了仅需两个轮次的2-GRPO可以达到与16-GRPO相当的性能，同时大幅减少计算开销。&lt;h4&gt;背景&lt;/h4&gt;GRPO是一种用于训练后大型语言模型(LLMs)的重要强化学习算法。传统观点认为，GRPO需要较大的组大小以确保通过精确的统计估计实现稳定训练，但这会导致显著的计算开销。&lt;h4&gt;目的&lt;/h4&gt;挑战GRPO需要大组大小的假设，重新构建GRPO为对比学习形式，揭示其与DPO的联系，并调查之前被认为不可行的最小两轮案例(2-GRPO)的可行性。&lt;h4&gt;方法&lt;/h4&gt;将GRPO重新构建为对比学习形式，提供严格的理论分析验证2-GRPO，并进行实证研究比较2-GRPO与16-GRPO的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;2-GRPO(使用最小化的两个轮次)与16-GRPO性能相当，仅使用1/8的轮次，同时将训练时间减少了70%以上。&lt;h4&gt;结论&lt;/h4&gt;GRPO不一定需要大组大小也能实现良好性能，2-GRPO是一种高效可行的替代方案，显著降低了计算成本。&lt;h4&gt;翻译&lt;/h4&gt;组相对策略优化(GRPO)是一种用于训练后大型语言模型(LLMs)的突出强化学习算法。人们普遍认为，GRPO需要较大的组大小来确保通过精确的统计估计实现稳定训练，这会导致大量的计算开销。在这项工作中，我们通过将GRPO重新构建为对比学习形式来挑战这一假设，这揭示了GRPO与直接偏好优化(DPO)的基本联系。受DPO实证成功的启发，我们调查了最小化的两轮案例(2-GRPO)，这是一种之前被认为不可行的配置。我们提供了严格的理论分析来验证2-GRPO，并通过实证证明，尽管仅使用1/8的轮次并将训练时间减少70%以上，2-GRPO的性能与16-GRPO相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group Relative Policy Optimization (GRPO) is a prominent reinforcementlearning algorithm for post-training Large Language Models (LLMs). It iscommonly believed that GRPO necessitates a large group size to ensure stabletraining via precise statistical estimation, which incurs substantialcomputational overhead. In this work, we challenge this assumption by reframingGRPO as a form of contrastive learning, which reveals a fundamental connectionto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,we investigate the minimal two-rollout case (2-GRPO), a configurationpreviously deemed infeasible. We provide a rigorous theoretical analysis tovalidate 2-GRPO and demonstrate empirically that it achieves performance on parwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training timeby over 70%.</description>
      <author>example@mail.com (Yihong Wu, Liheng Ma, Lei Ding, Muzhi Li, Xinyu Wang, Kejia Chen, Zhan Su, Zhanguang Zhang, Chenyang Huang, Yingxue Zhang, Mark Coates, Jian-Yun Nie)</author>
      <guid isPermaLink="false">2510.00977v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration</title>
      <link>http://arxiv.org/abs/2510.00890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Sci-SpanDet框架，一个用于检测学术文本中AI生成内容的结构感知方法。该方法结合了基于章节的条件风格建模和多层次对比学习，捕捉人类与AI文本的细微差异，同时减少主题依赖性。此外，它集成了BIO-CRF序列标记和置信度校准，实现精确的片段级检测。实验表明，该框架在跨学科数据集上达到最先进性能，并在对抗性重写下表现出强鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在科学写作中的快速采用引发了关于作者完整性和学术出版物可靠性的严重担忧。现有检测方法主要依赖文档级分类或表面统计线索，但忽略了细粒度片段定位，校准能力弱，且难以跨学科和跨生成器泛化。&lt;h4&gt;目的&lt;/h4&gt;解决现有AI生成文本检测方法的局限性，包括忽视细粒度片段定位、校准能力弱以及跨学科和跨生成器泛化能力差的问题。&lt;h4&gt;方法&lt;/h4&gt;提出Sci-SpanDet框架，结合基于章节的条件风格建模与多层次对比学习，捕捉人类与AI文本的细微差异，同时缓解主题依赖。集成BIO-CRF序列标记、基于指针的边界解码和置信度校准，实现精确的片段级检测和可靠概率估计。&lt;h4&gt;主要发现&lt;/h4&gt;在包含100,000个标注样本的跨学科数据集上，Sci-SpanDet达到最先进性能：F1(AI)为80.17，AUROC为92.63，Span-F1为74.36。该方法在对抗性重写下表现出强鲁棒性，在IMRaD部分和不同学科中保持平衡准确率，显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;Sci-SpanDet是有效的AI生成学术文本检测框架，能精确识别AI生成内容并在各种条件下保持高准确率。为促进该领域研究，整理好的数据集和源代码将在发表后公开。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在科学写作中的快速采用引发了关于作者完整性和学术出版物可靠性的严重担忧。现有的检测方法主要依赖于文档级分类或表面级别的统计线索；然而，它们忽略了细粒度的片段定位，校准能力弱，并且往往无法跨学科和跨生成器泛化。为了解决这些局限性，我们提出了Sci-SpanDet，一个用于检测AI生成学术文本的结构感知框架。该方法结合了基于章节的条件风格建模与多层次对比学习，捕捉人类与AI生成文本的细微差异，同时缓解主题依赖，从而增强跨领域鲁棒性。此外，它还集成了BIO-CRF序列标记、基于指针的边界解码和置信度校准，以实现精确的片段级检测和可靠的概率估计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid adoption of large language models (LLMs) in scientific writingraises serious concerns regarding authorship integrity and the reliability ofscholarly publications. Existing detection approaches mainly rely ondocument-level classification or surface-level statistical cues; however, theyneglect fine-grained span localization, exhibit weak calibration, and oftenfail to generalize across disciplines and generators. To address theselimitations, we present Sci-SpanDet, a structure-aware framework for detectingAI-generated scholarly texts. The proposed method combines section-conditionedstylistic modeling with multi-level contrastive learning to capture nuancedhuman-AI differences while mitigating topic dependence, thereby enhancingcross-domain robustness. In addition, it integrates BIO-CRF sequence labelingwith pointer-based boundary decoding and confidence calibration to enableprecise span-level detection and reliable probability estimates. Extensiveexperiments on a newly constructed cross-disciplinary dataset of 100,000annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, withF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it showsstrong resilience under adversarial rewriting and maintains balanced accuracyacross IMRaD sections and diverse disciplines, substantially surpassingexisting baselines. To ensure reproducibility and to foster further research onAI-generated text detection in scholarly documents, the curated dataset andsource code will be publicly released upon publication.</description>
      <author>example@mail.com (Zhen Yin, Shenghua Wang)</author>
      <guid isPermaLink="false">2510.00890v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Feature Identification for Hierarchical Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.00837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种新颖的分层对比学习方法，通过建模类别间关系和不均衡分布，在多个层级实现细粒度聚类，并在标准数据集上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;分层分类是许多应用中的关键任务，对象被组织成多个类别的层次结构。&lt;h4&gt;目的&lt;/h4&gt;解决传统分类方法忽视不同层次类别间内在关系的问题，开发能充分利用层次结构信息的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出两种分层对比学习方法：第一种利用高斯混合模型（G-HMLC），第二种使用注意力机制捕获层次特定特征（A-HMLC），模仿人类处理过程。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法明确建模了类别间关系和高层级的不均衡类别分布，实现了所有层级的细粒度聚类，在CIFAR100和ModelNet40数据集上线性评估准确率比现有方法提高2个百分点。&lt;h4&gt;结论&lt;/h4&gt;方法通过定量和定性结果证明了有效性，突显了其在计算机视觉及其他领域应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;分层分类是许多应用中的关键任务，其中对象被组织成多个类别的层次结构。然而，传统分类方法常常忽视不同层次类别间的内在关系，从而错过了重要的监督信号。因此，我们提出了两种新颖的分层对比学习方法。第一种利用高斯混合模型，第二种使用注意力机制捕获层次特定特征，模仿人类处理过程。我们的方法明确建模了类别间关系和高层级的不均衡类别分布，实现了所有层级的细粒度聚类。在具有竞争力的CIFAR100和ModelNet40数据集上，我们的方法在线性评估中取得了最先进的性能，在准确率上比现有的分层对比学习方法提高了2个百分点。我们方法的有效性得到了定量和定性结果的支持，突显了其在计算机视觉及其他领域应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical classification is a crucial task in many applications, whereobjects are organized into multiple levels of categories. However, conventionalclassification approaches often neglect inherent inter-class relationships atdifferent hierarchy levels, thus missing important supervisory signals. Thus,we propose two novel hierarchical contrastive learning (HMLC) methods. Thefirst, leverages a Gaussian Mixture Model (G-HMLC) and the second uses anattention mechanism to capture hierarchy-specific features (A-HMLC), imitatinghuman processing. Our approach explicitly models inter-class relationships andimbalanced class distribution at higher hierarchy levels, enabling fine-grainedclustering across all hierarchy levels. On the competitive CIFAR100 andModelNet40 datasets, our method achieves state-of-the-art performance in linearevaluation, outperforming existing hierarchical contrastive learning methods by2 percentage points in terms of accuracy. The effectiveness of our approach isbacked by both quantitative and qualitative results, highlighting its potentialfor applications in computer vision and beyond.</description>
      <author>example@mail.com (Julius Ott, Nastassia Vysotskaya, Huawei Sun, Lorenzo Servadei, Robert Wille)</author>
      <guid isPermaLink="false">2510.00837v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy</title>
      <link>http://arxiv.org/abs/2510.00695v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://myungkyukoo.github.io/hamlet/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAMLET是一种可扩展的框架，使视觉-语言-动作模型(VLAs)能够在动作预测过程中关注历史上下文，通过引入时刻令牌和轻量级记忆模块，显著提高了在历史依赖任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;机器人操作任务本质上是依赖于历史的，利用过去的上下文可能是有益的。然而，大多数现有的VLAs在设计时没有考虑这一方面，它们仅依赖于当前观察，而忽略先前的上下文。&lt;h4&gt;目的&lt;/h4&gt;提出HAMLET框架，使VLAs能够在动作预测过程中关注历史上下文，提高在历史依赖任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;引入时刻令牌紧凑编码每个时间步的感知信息，通过时间对比学习初始化这些表示，并采用轻量级记忆模块将过去时间步的时刻令牌整合为记忆特征，用于动作预测。&lt;h4&gt;主要发现&lt;/h4&gt;HAMLET成功将最先进的VLA转变为历史感知策略，在历史依赖的真实世界任务上实现了76.4%的平均成功率，比基线提高了47.2%；在RoboCasa Kitchen上将性能从64.1%提高到66.4%；在LIBERO上将性能从95.6%提高到97.7%。&lt;h4&gt;结论&lt;/h4&gt;HAMLET是一个有效的框架，能够使VLAs关注历史上下文，在各种任务上取得了显著的性能提升，特别是在长周期和历史依赖任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本质上，机器人操作任务是依赖于历史的：利用过去的上下文可能是有益的。然而，大多数现有的视觉-语言-动作模型(VLAs)在设计时没有考虑这一方面，即它们仅依赖于当前观察，而忽略先前的上下文。在本文中，我们提出了HAMLET，一个可扩展的框架，使VLAs能够在动作预测过程中关注历史上下文。具体来说，我们引入了时刻令牌，它们紧凑地编码每个时间步的感知信息。它们的表示通过时间对比学习进行初始化，使其能够更好地捕捉时间上独特的方面。接下来，我们采用一个轻量级记忆模块，将过去时间步的时刻令牌整合为记忆特征，然后利用这些特征进行动作预测。通过经验评估，我们表明HAMLET成功地将最先进的VLA转变为历史感知策略，特别是在需要历史上下文的长周期任务上显示出显著改进。特别是在基于GR00T N1.5的实验中，HAMLET在历史依赖的真实世界任务上实现了76.4%的平均成功率，超过了基线性能47.2%。此外，HAMLET将RoboCasa Kitchen(100-demo设置)上的先前艺术性能从64.1%提高到66.4%，将LIBERO上的性能从95.6%提高到97.7%，突显了其在通用机器人操作基准测试中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inherently, robotic manipulation tasks are history-dependent: leveraging pastcontext could be beneficial. However, most existing Vision-Language-Actionmodels (VLAs) have been designed without considering this aspect, i.e., theyrely solely on the current observation, ignoring preceding context. In thispaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to thehistorical context during action prediction. Specifically, we introduce momenttokens that compactly encode perceptual information at each timestep. Theirrepresentations are initialized with time-contrastive learning, allowing themto better capture temporally distinctive aspects. Next, we employ a lightweightmemory module that integrates the moment tokens across past timesteps intomemory features, which are then leveraged for action prediction. Throughempirical evaluation, we show that HAMLET successfully transforms astate-of-the-art VLA into a history-aware policy, especially demonstratingsignificant improvements on long-horizon tasks that require historical context.In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of76.4% on history-dependent real-world tasks, surpassing the baselineperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% onLIBERO, highlighting its effectiveness even under generic robot-manipulationbenchmarks.</description>
      <author>example@mail.com (Myungkyu Koo, Daewon Choi, Taeyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin)</author>
      <guid isPermaLink="false">2510.00695v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>ARIONet: An Advanced Self-supervised Contrastive Representation Network for Birdsong Classification and Future Frame Prediction</title>
      <link>http://arxiv.org/abs/2510.00522v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ARIONet的自监督对比网络，用于自动鸟类声音分类，通过联合优化对比分类和未来帧预测来学习具有区分性的物种特定表示，无需大规模标注数据。&lt;h4&gt;背景&lt;/h4&gt;自动鸟类声音分类对生态监测和生物多样性研究至关重要，但现有方法存在严重依赖标记数据、使用有限特征表示、忽略物种识别所需时间动态等问题。&lt;h4&gt;目的&lt;/h4&gt;设计一个自监督对比网络ARIONet，通过增强音频表示联合优化对比分类和未来帧预测，并在基于transformer的编码器模型中集成多种互补音频特征。&lt;h4&gt;方法&lt;/h4&gt;ARIONet实现两个关键目标：(1)通过最大化同一音频段增强视图间的相似性同时分离不同样本来学习区分性物种表示；(2)通过预测未来音频帧建模时间动态，两者均无需大规模标注。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上验证了框架性能，分类准确率分别为98.41%、93.07%、91.89%和91.58%，F1分数分别为97.84%、94.10%、91.29%和90.94%，未来帧预测任务中余弦相似度高达95%。&lt;h4&gt;结论&lt;/h4&gt;自监督学习策略能有效捕捉复杂声学模式和时序依赖，该方法在生态保护和监测方面具有实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;自动鸟类声音分类对推进生态监测和生物多样性研究至关重要。尽管最近取得了进展，但现有方法通常严重依赖标记数据，使用有限的特征表示，并忽略了准确物种识别所必需的时间动态。在这项工作中，我们提出了一个自监督对比网络ARIONet（帧间目标网络的声音表示），通过增强音频表示联合优化对比分类和未来帧预测。该模型在基于transformer的编码器模型中同时集成多种互补音频特征。我们的框架设计有两个关键目标：(1)通过最大化同一音频段增强视图之间的相似性同时推开不同样本来学习具有区分性的物种特定表示用于对比学习；(2)通过预测未来音频帧来建模时间动态，两者都不需要大规模标注。我们在四个多样化的鸟类声音数据集上验证了我们的框架，包括英国鸟类声音数据集、鸟类声音数据集和两个扩展的Xeno-Canto子集（A-M和N-Z）。我们的方法一致优于现有基线，分别实现了98.41%、93.07%、91.89%和91.58%的分类准确率，以及97.84%、94.10%、91.29%和90.94%的F1分数。此外，它在未来帧预测任务中显示出低的平均绝对误差和高余弦相似度，高达95%。大量实验进一步证实了我们的自监督学习策略在捕捉复杂声学模式和时序依赖方面的有效性，以及其在生态保护和监测中的实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated birdsong classification is essential for advancing ecologicalmonitoring and biodiversity studies. Despite recent progress, existing methodsoften depend heavily on labeled data, use limited feature representations, andoverlook temporal dynamics essential for accurate species identification. Inthis work, we propose a self-supervised contrastive network, ARIONet (AcousticRepresentation for Interframe Objective Network), that jointly optimizescontrastive classification and future frame prediction using augmented audiorepresentations. The model simultaneously integrates multiple complementaryaudio features within a transformer-based encoder model. Our framework isdesigned with two key objectives: (1) to learn discriminative species-specificrepresentations for contrastive learning through maximizing similarity betweenaugmented views of the same audio segment while pushing apart differentsamples, and (2) to model temporal dynamics by predicting future audio frames,both without requiring large-scale annotations. We validate our framework onfour diverse birdsong datasets, including the British Birdsong Dataset, BirdSong Dataset, and two extended Xeno-Canto subsets (A-M and N-Z). Our methodconsistently outperforms existing baselines and achieves classificationaccuracies of 98.41%, 93.07%, 91.89%, and 91.58%, and F1-scores of 97.84%,94.10%, 91.29%, and 90.94%, respectively. Furthermore, it demonstrates low meanabsolute errors and high cosine similarity, up to 95%, in future frameprediction tasks. Extensive experiments further confirm the effectiveness ofour self-supervised learning strategy in capturing complex acoustic patternsand temporal dependencies, as well as its potential for real-worldapplicability in ecological conservation and monitoring.</description>
      <author>example@mail.com (Md. Abdur Rahman, Selvarajah Thuseethan, Kheng Cher Yeo, Reem E. Mohamed, Sami Azam)</author>
      <guid isPermaLink="false">2510.00522v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.00346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种域鲁棒生物声学学习(DR-BioL)框架，结合对比学习和分布对齐，解决了蚊子物种分类中域特征干扰问题，提高了跨域分类的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;蚊子物种分类对媒介监测和疾病控制至关重要，但蚊子生物声学数据收集受活动季节和野外工作限制。不同区域、栖息地和实验室的蚊子录音包含非生物学变化的域特征，干扰分类准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种域鲁棒生物声学学习方法，解决直接在含域特征的音频上训练的模型依赖域信息而非物种声学线索的问题，提高跨域泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出域鲁棒生物声学学习(DR-BioL)框架，结合对比学习促进同一物种内部凝聚力和减轻域间差异，以及物种条件分布对齐增强跨域物种表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多域蚊子生物声学数据集上的实验表明，DR-BioL提高了基线模型的准确性和鲁棒性，展现了在现实世界中进行可靠跨域蚊子物种分类的潜力。&lt;h4&gt;结论&lt;/h4&gt;DR-BioL框架能有效解决域特征干扰问题，提高跨域蚊子物种分类的准确性和鲁棒性，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;蚊子物种分类(MSC)对媒介监测和疾病控制至关重要。蚊子生物声学数据的收集通常受蚊子活动季节和野外工作的限制。来自不同区域、栖息地和实验室的蚊子录音通常包含记录环境带来的非生物学变化，我们称之为域特征。研究发现，直接在包含域特征的音频录音上训练的模型往往依赖域信息而非物种声学线索进行识别，导致看似良好的性能但实际上跨域泛化能力差。为此，我们提出了一种域鲁棒生物声学学习(DR-BioL)框架，结合对比学习和分布对齐。对比学习旨在促进同一物种内部的凝聚力并减轻域间差异，物种条件分布对齐进一步增强了跨域物种表示。在来自不同环境的多域蚊子生物声学数据集上的实验表明，DR-BioL提高了基线模型的准确性和鲁棒性，突显了其在现实世界中进行可靠的跨域蚊子物种分类的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mosquito Species Classification (MSC) is crucial for vector surveillance anddisease control. The collection of mosquito bioacoustic data is often limitedby mosquito activity seasons and fieldwork. Mosquito recordings across regions,habitats, and laboratories often show non-biological variations from therecording environment, which we refer to as domain features. This study findsthat models directly trained on audio recordings with domain features tend torely on domain information rather than the species' acoustic cues foridentification, resulting in illusory good performance while actuallyperforming poor cross-domain generalization. To this end, we propose aDomain-Robust Bioacoustic Learning (DR-BioL) framework that combinescontrastive learning with distribution alignment. Contrastive learning aims topromote cohesion within the same species and mitigate inter-domaindiscrepancies, and species-conditional distribution alignment further enhancescross-domain species representation. Experiments on a multi-domain mosquitobioacoustic dataset from diverse environments show that the DR-BioL improvesthe accuracy and robustness of baselines, highlighting its potential forreliable cross-domain MSC in the real world.</description>
      <author>example@mail.com (Yuanbo Hou, Zhaoyi Liu, Xin Shen, Stephen Roberts)</author>
      <guid isPermaLink="false">2510.00346v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2510.01483v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VL-KnG，一个视觉场景理解系统，通过时空知识图谱构建和计算高效查询处理，解决了视觉语言模型在机器人导航中的基本局限性。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在机器人导航方面显示出潜力，但存在基本局限性：缺乏持久的场景记忆，空间推理能力有限，且随着视频持续时间增加无法有效扩展以实现实时应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够克服VLMs在机器人导航中局限性的系统，特别是解决场景记忆缺失、空间推理不足和实时应用扩展性问题。&lt;h4&gt;方法&lt;/h4&gt;提出VL-KnG系统，将视频序列分块处理利用现代VLMs，创建持久知识图谱以保持对象身份随时间变化，通过可查询的图结构实现可解释的空间推理，并引入WalkieKnowledge基准测试包含8条不同轨迹的约200个手动标注问题。&lt;h4&gt;主要发现&lt;/h4&gt;在差速驱动机器人上的实际部署展示了实际应用能力，方法实现了77.27%的成功率和76.92%的答案准确率，与Gemini 2.5 Pro性能相匹配，同时提供知识图谱支持的可解释推理和计算效率。&lt;h4&gt;结论&lt;/h4&gt;VL-KnG系统有效解决了VLMs在机器人导航中的基本局限性，提供了可解释的推理能力和计算效率，适用于实时应用。代码和数据集将在接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)在机器人导航方面显示出潜力，但遇到基本局限性：它们缺乏持久的场景记忆，空间推理能力有限，并且随着视频持续时间增加无法有效扩展以实现实时应用。我们提出了VL-KnG，一个视觉场景理解系统，它通过时空知识图谱构建和用于导航目标识别的计算高效查询处理来解决这些挑战。我们的方法利用现代VLMs处理视频序列块，创建保持对象身份随时间变化的持久知识图谱，并通过可查询的图结构实现可解释的空间推理。我们还引入了WalkieKnowledge，一个新的基准测试，包含约200个手动标注问题，跨越8条不同轨迹，约100分钟的视频数据，使结构化方法和通用VLMs之间的公平比较成为可能。在差速驱动机器人上的实际部署展示了实际应用能力，我们的方法实现了77.27%的成功率和76.92%的答案准确率，与Gemini 2.5 Pro性能相匹配，同时提供知识图谱支持的可解释推理，具有计算效率，可实现不同任务(如定位、导航和规划)的实时部署。代码和数据集将在接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人导航中视觉场景理解的挑战，具体是让机器人通过视觉语言模型更好地识别导航目标。这个问题很重要，因为机器人导航是机器人融入人类日常生活的关键能力，需要理解复杂的空间关系和时间对象动态，以实现自然语言引导的目标导向行为。当前视觉语言模型存在持久场景记忆缺乏、空间推理有限和无法随视频时长扩展以实现实时应用的三大基本限制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有方法不足（顺序处理失去时间一致性或直接VLM推理缺乏结构化能力）提出关键见解：持久结构化表示（如知识图谱）能提供直接VLM推理无法比拟的优势，特别是在可解释性、计算效率和跨任务适应性方面。作者确实借鉴了现有工作，包括视觉语言导航研究、多模态3D映射方法（VLMaps、ConceptFusion）、场景图构建（ConceptGraphs）、3D图表示（Hydra、Clio）、基于图像的拓扑图、CLIP检索和检索增强生成(RAG)技术，并综合这些方法的最佳实践。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建时空知识图谱来表示和理解环境，通过结构化知识表示和高效查询处理支持导航目标识别。整体流程包括：1)时空知识图谱构建：将视频分块处理，用视觉语言模型提取对象描述符，迭代构建知识图谱；2)时空对象关联：使用基于语义的关联机制维持跨时间对象身份，利用大语言模型推理建立对象对应关系；3)导航查询处理：采用基于GraphRAG的方法进行子图检索和推理，包括查询分解、子图检索和推理定位三步；4)实际部署：与机器人导航系统集成，提供目标位置估计，支持实时应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于语义的对象关联机制，维持跨时间唯一对象身份；2)综合对象描述符系统，捕获颜色、材质、大小等丰富语义信息；3)时空知识图谱系统，实现持久场景表示和可查询空间推理；4)WalkieKnowledge评估基准，提供结构化方法和通用VLM的公平比较；5)实时部署能力，在实际机器人平台上验证实用性。相比之前工作，VL-KnG结合了结构化知识表示和VLM推理优势，提供可解释推理过程，在保持高性能同时实现实时计算效率，通过知识图谱实现跨时间对象一致性跟踪，并引入新评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VL-KnG通过构建时空知识图谱和高效查询处理机制，解决了视觉语言模型在机器人导航中的持久场景记忆、空间推理和实时性挑战，实现了与最先进VLM相当的性能但提供了更好的可解释性和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have shown potential for robot navigation butencounter fundamental limitations: they lack persistent scene memory, offerlimited spatial reasoning, and do not scale effectively with video duration forreal-time application. We present VL-KnG, a Visual Scene Understanding systemthat tackles these challenges using spatiotemporal knowledge graph constructionand computationally efficient query processing for navigation goalidentification. Our approach processes video sequences in chunks utilizingmodern VLMs, creates persistent knowledge graphs that maintain object identityover time, and enables explainable spatial reasoning through queryable graphstructures. We also introduce WalkieKnowledge, a new benchmark with about 200manually annotated questions across 8 diverse trajectories spanningapproximately 100 minutes of video data, enabling fair comparison betweenstructured approaches and general-purpose VLMs. Real-world deployment on adifferential drive robot demonstrates practical applicability, with our methodachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5Pro performance while providing explainable reasoning supported by theknowledge graph, computational efficiency for real-time deployment acrossdifferent tasks, such as localization, navigation and planning. Code anddataset will be released after acceptance.</description>
      <author>example@mail.com (Mohamad Al Mdfaa, Svetlana Lukina, Timur Akhtyamov, Arthur Nigmatzyanov, Dmitrii Nalberskii, Sergey Zagoruyko, Gonzalo Ferrer)</author>
      <guid isPermaLink="false">2510.01483v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>KeySG: Hierarchical Keyframe-Based 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2510.01049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KeySG是一种新的框架，通过层次化图表示和多模态信息增强，解决了现有3D场景图构建方法的语义局限性和上下文窗口问题，在多个基准测试上表现优异。&lt;h4&gt;背景&lt;/h4&gt;3D场景图作为强大的世界表示方法，结合大型语言模型可让机器人在复杂环境中推理和导航，但当前方法语义局限于预定义关系，且大环境序列化易超出LLM上下文窗口。&lt;h4&gt;目的&lt;/h4&gt;开发一种更通用的、与任务无关的3D场景表示方法，克服现有方法的语义局限性和扩展性问题，使系统能处理复杂模糊查询。&lt;h4&gt;方法&lt;/h4&gt;KeySG将3D场景表示为包含楼层、房间、物体和功能元素的层次化图，节点通过优化选择的关键帧提取多模态信息增强，利用VLM提取场景信息避免显式建模物体关系，采用层次化RAG管道处理大规模场景图。&lt;h4&gt;主要发现&lt;/h4&gt;KeySG在四个基准测试(包括3D物体分割和复杂查询检索)上优于先前方法，展示了卓越的语义丰富性和效率。&lt;h4&gt;结论&lt;/h4&gt;KeySG通过创新的设计有效解决了现有3D场景图方法的局限性，能够处理复杂模糊查询并缓解大规模场景图的扩展性问题，为机器人在复杂环境中的推理和导航提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，3D场景图已成为一种强大的世界表示方法，既提供几何准确性又具备语义丰富性。将3D场景图与大型语言模型结合，使机器人在以人为中心的环境中能够推理、规划和导航。然而，当前构建3D场景图的方法在语义上仅限于预定义的关系集合，且在大环境中的序列化容易超出LLM的上下文窗口。我们引入了KeySG框架，将3D场景表示为由楼层、房间、物体和功能元素组成的层次化图，其中节点通过从关键帧中提取的多模态信息进行增强，这些关键帧经过选择以优化几何和视觉覆盖率。关键帧使我们能够有效利用VLM提取场景信息，避免了对物体间关系边进行显式建模的需求，从而实现更通用、与任务无关的推理和规划。我们的方法可以处理复杂和模糊的查询，同时通过利用层次化检索增强生成(RAG)管道从图中提取相关上下文，缓解与大规模场景图相关的扩展性问题。在四个不同的基准测试上(包括3D物体分割和复杂查询检索)评估，KeySG在大多数指标上优于先前的方法，展示了其卓越的语义丰富性和效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前3D场景图方法的两个关键问题：一是语义局限性，即只能支持预定义的关系集合，限制了任务多样性；二是可扩展性问题，即大型环境场景图可能超过大型语言模型的上下文窗口限制。这些问题在机器人领域尤为重要，因为要让机器人在复杂人类环境中有效工作，需要一种既能提供精确几何细节又支持高级推理的世界表示方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出3D场景图的局限性，然后设计了一个分层结构来捕捉环境的多层次抽象。他们借鉴了关键帧视觉SLAM的思想，但将其应用于视觉覆盖而非几何重建；使用了现有的场景分割算法和视觉语言模型；并引入了分层检索增强生成(RAG)管道来解决可扩展性问题。这些设计共同构成了KeySG框架，它通过关键帧采样和多模态信息增强来构建一个更灵活、可扩展的场景表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过关键帧采样和多模态信息增强构建分层3D场景图，不显式建模物体关系边，而是将丰富场景信息存储在关键帧及其描述中。整体流程包括：1)分层场景分割(重建点云并分割为楼层和房间)；2)关键帧采样(选择能提供全面视觉覆盖的代表帧)；3)物体和功能元素分割(利用VLM提取标签并进行3D分割)；4)场景描述生成(用LLM创建房间和楼层摘要)；5)分层RAG查询(处理用户查询并返回相关上下文)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)五层分层的3D场景图结构(建筑、楼层、房间、物体和功能元素)；2)关键帧驱动的多模态信息增强；3)分层场景描述生成；4)分层检索增强生成(RAG)管道。相比之前的工作，KeySG不显式建模预定义关系边，而是通过关键帧隐式捕获关系；解决了大型环境中的可扩展性问题；支持更通用、任务无关的推理；能处理更复杂的查询；提供了更丰富的语义信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; KeySG提出了一种基于关键帧的分层3D场景图框架，通过多模态信息增强和分层检索增强生成技术，解决了现有3D场景图在语义表达和可扩展性方面的局限性，实现了更通用、高效的机器人环境表示和推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, 3D scene graphs have emerged as a powerful worldrepresentation, offering both geometric accuracy and semantic richness.Combining 3D scene graphs with large language models enables robots to reason,plan, and navigate in complex human-centered environments. However, currentapproaches for constructing 3D scene graphs are semantically limited to apredefined set of relationships, and their serialization in large environmentscan easily exceed an LLM's context window. We introduce KeySG, a framework thatrepresents 3D scenes as a hierarchical graph consisting of floors, rooms,objects, and functional elements, where nodes are augmented with multi-modalinformation extracted from keyframes selected to optimize geometric and visualcoverage. The keyframes allow us to efficiently leverage VLM to extract sceneinformation, alleviating the need to explicitly model relationship edgesbetween objects, enabling more general, task-agnostic reasoning and planning.Our approach can process complex and ambiguous queries while mitigating thescalability issues associated with large scene graphs by utilizing ahierarchical retrieval-augmented generation (RAG) pipeline to extract relevantcontext from the graph. Evaluated across four distinct benchmarks -- including3D object segmentation and complex query retrieval -- KeySG outperforms priorapproaches on most metrics, demonstrating its superior semantic richness andefficiency.</description>
      <author>example@mail.com (Abdelrhman Werby, Dennis Rotondi, Fabio Scaparro, Kai O. Arras)</author>
      <guid isPermaLink="false">2510.01049v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>L4P: Towards Unified Low-Level 4D Vision Perception</title>
      <link>http://arxiv.org/abs/2502.13078v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;L4P是一种通用的前馈架构，通过预训练的ViT视频编码器和轻量级任务头部，在统一框架下高效解决多种低级4D感知任务，性能与专业化方法相当，且可同时处理所有任务。&lt;h4&gt;背景&lt;/h4&gt;视频像素之间的时空关系对低级4D感知任务至关重要。目前大多数最先进的方法依赖于针对特定任务的专业化架构。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的前馈架构L4P，能够在统一框架下解决低级4D感知任务。&lt;h4&gt;方法&lt;/h4&gt;L4P利用预训练的基于ViT的视频编码器，结合轻量级的特定任务头部，因此不需要大量训练。&lt;h4&gt;主要发现&lt;/h4&gt;尽管L4P是通用和前馈的，但它在密集任务（如深度或光流估计）和稀疏任务（如2D/3D跟踪）上都能与现有的专业化方法相媲美。&lt;h4&gt;结论&lt;/h4&gt;L4P能够一次性解决所有任务，所需时间与单任务方法相当。&lt;h4&gt;翻译&lt;/h4&gt;视频像素之间的时空关系对低级4D感知任务携带关键信息。能够推理这种关系的单一模型应该能够很好地解决多种此类任务。然而，大多数最先进的方法依赖于针对特定任务的专业化架构。我们提出了L4P，一种前馈、通用架构，在统一框架下解决低级4D感知任务。L4P利用预训练的基于ViT的视频编码器，并结合轻量级的特定任务头部，因此不需要大量训练。尽管其通用和前馈的表述，我们的方法在密集任务（如深度或光流估计）和稀疏任务（如2D/3D跟踪）上都能与现有专业化方法相媲美。此外，它一次性解决了所有任务，所需时间与单任务方法相当。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决低层次4D视觉感知任务缺乏统一框架的问题。现有方法通常需要针对每个任务（如深度估计、光流估计、跟踪等）设计专门的架构，无法有效利用视频数据中的时空关系信息。这个问题很重要，因为视频像素间的时空关系对理解动态场景至关重要，统一的框架能更高效地处理多种任务，减少重复计算，并增强模型在不同任务间的知识迁移能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到预训练视频模型（如VideoMAE）能捕捉丰富的时空特征，但其在低层次4D感知任务中的应用未被充分探索。他们设计了一个通用架构，结合预训练视频编码器与轻量级任务特定头：对于密集任务（如深度估计），扩展了DPT架构；对于稀疏任务（如跟踪），借鉴了SAM的提示机制并添加记忆机制。作者采用了多阶段训练策略，先在单窗口上端到端训练，再通过冻结部分参数和展开窗口训练优化跟踪性能。该方法明显借鉴了VideoMAE、DPT和CoTracker等现有工作，但进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用预训练的视频编码器作为通用特征提取器，配合轻量级任务特定头，实现统一的低层次4D视觉感知。整体流程为：1) 视频输入经VideoMAE编码器生成时空特征；2) 密集任务（如深度估计）通过扩展的DPT头部处理，将视频标记映射到3D特征图并引入时间推理；3) 稀疏任务（如跟踪）通过特殊头部处理，使用双向注意力机制和记忆机制实现长时间跟踪；4) 采用多阶段训练策略，先端到端训练再优化特定任务；5) 推理时使用滑动窗口处理长视频，确保重叠区域一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一框架同时处理密集和稀疏任务，打破传统方法需专门架构的限制；2) 轻量级任务特定头设计，只需少量训练参数即可扩展新任务；3) 记忆机制解决视频编码器有限上下文问题，支持长时间跟踪；4) 多阶段训练策略平衡不同任务性能；5) 强大的跨数据集泛化能力。相比之前工作，L4P无需针对每个任务重新训练整个模型，计算效率更高（处理16帧视频约300ms），且能同时输出多种任务结果，而传统方法通常只能处理单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L4P提出了一种统一的低层次4D视觉感知框架，通过结合预训练视频编码器和轻量级任务特定头，实现了在密集和稀疏任务上的高性能，同时支持长时间跟踪和新任务的轻松扩展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The spatio-temporal relationship between the pixels of a video carriescritical information for low-level 4D perception tasks. A single model thatreasons about it should be able to solve several such tasks well. Yet, moststate-of-the-art methods rely on architectures specialized for the task athand. We present L4P, a feedforward, general-purpose architecture that solveslow-level 4D perception tasks in a unified framework. L4P leverages apre-trained ViT-based video encoder and combines it with per-task heads thatare lightweight and therefore do not require extensive training. Despite itsgeneral and feedforward formulation, our method is competitive with existingspecialized methods on both dense tasks, such as depth or optical flowestimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves alltasks at once in a time comparable to that of single-task methods.</description>
      <author>example@mail.com (Abhishek Badki, Hang Su, Bowen Wen, Orazio Gallo)</author>
      <guid isPermaLink="false">2502.13078v3</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.01970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Moon是一种监督模态转换的多元时间序列异常检测框架，通过多变量马尔可夫转移场技术将时间序列数据转换为图像表示，结合多模态CNN融合数值和图像数据，并使用SHAP解释器提高异常检测的可解释性。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列异常检测旨在识别每个时间戳包含多个变量的异常模式。现有方法分为基于重建、基于预测和基于分类器三类，但面临无监督方法依赖误差阈值导致不准确、半监督方法未充分利用异常标签、监督方法无法捕获局部关系且计算成本高等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有多元时间序列异常检测方法的局限性，提高异常检测的效率和准确性，并提供详细的异常分析报告。&lt;h4&gt;方法&lt;/h4&gt;Moon框架包含三个关键组件：1)多变量马尔可夫转移场技术将数值时间序列转换为图像表示；2)多模态CNN通过参数共享的特征融合模型整合数值和图像数据；3)基于SHAP的异常解释器识别导致异常的关键变量。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实世界多元时间序列数据集上的实验表明，Moon在效率上比六种最先进的方法高93%，在准确性上高4%，在解释性能上高10.8%。&lt;h4&gt;结论&lt;/h4&gt;Moon通过模态转换和多模态融合有效提高了异常检测的效率和准确性，同时提供详细的异常分析报告，解决了现有方法的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列异常检测识别每个时间戳包含多个变量的异常模式。现有的多元时间序列异常检测方法分为三类：基于重建的方法、基于预测的方法和基于分类器的方法。然而，这些方法面临两个关键挑战：(1)无监督学习方法，如基于重建和预测的方法，依赖误差阈值，可能导致不准确；(2)半监督方法主要建模正常数据且常常未充分利用异常标签，限制了细微异常的检测；(3)监督学习方法，如基于分类器的方法，往往无法捕获局部关系，计算成本高，且受限于标记数据的稀缺性。为解决这些局限性，我们提出了Moon，一种监督模态转换的多元时间序列异常检测框架。Moon提高了异常检测的效率和准确性，同时提供详细的异常分析报告。首先，Moon引入了一种新颖的多变量马尔可夫转移场技术，将数值时间序列数据转换为图像表示，捕获变量和时间戳之间的关系。由于数值数据保留了无法仅通过图像转换完全捕获的独特模式，Moon采用多模态CNN通过参数共享的特征融合模型整合数值和图像数据，提高训练效率。最后，基于SHAP的异常解释器识别导致异常的关键变量，提高可解释性。在六个真实世界多元时间序列数据集上的大量实验表明，Moon在效率上比六种最先进的方法高93%，在准确性上高4%，在解释性能上高10.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) anomaly detection identifies abnormal patternswhere each timestamp contains multiple variables. Existing MTS anomalydetection methods fall into three categories: reconstruction-based,prediction-based, and classifier-based methods. However, these methods face twokey challenges: (1) Unsupervised learning methods, such as reconstruction-basedand prediction-based methods, rely on error thresholds, which can lead toinaccuracies; (2) Semi-supervised methods mainly model normal data and oftenunderuse anomaly labels, limiting detection of subtle anomalies;(3) Supervisedlearning methods, such as classifier-based approaches, often fail to capturelocal relationships, incur high computational costs, and are constrained by thescarcity of labeled data. To address these limitations, we propose Moon, asupervised modality conversion-based multivariate time series anomaly detectionframework. Moon enhances the efficiency and accuracy of anomaly detection whileproviding detailed anomaly analysis reports. First, Moon introduces a novelmultivariate Markov Transition Field (MV-MTF) technique to convert numeric timeseries data into image representations, capturing relationships acrossvariables and timestamps. Since numeric data retains unique patterns thatcannot be fully captured by image conversion alone, Moon employs aMultimodal-CNN to integrate numeric and image data through a feature fusionmodel with parameter sharing, enhancing training efficiency. Finally, aSHAP-based anomaly explainer identifies key variables contributing toanomalies, improving interpretability. Extensive experiments on six real-worldMTS datasets demonstrate that Moon outperforms six state-of-the-art methods byup to 93% in efficiency, 4% in accuracy and, 10.8% in interpretationperformance.</description>
      <author>example@mail.com (Yuanyuan Yao, Yuhan Shi, Lu Chen, Ziquan Fang, Yunjun Gao, Leong Hou U, Yushuai Li, Tianyi Li)</author>
      <guid isPermaLink="false">2510.01970v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review</title>
      <link>http://arxiv.org/abs/2510.01145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇系统文献综述探讨了非洲低资源语言的自动语音识别研究现状，分析了数据集、模型、训练方法、评估技术和面临的挑战，并提出了未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;自动语音识别技术在全球取得了显著进展，但非洲低资源语言仍然严重缺乏代表性，阻碍了非洲大陆的数字化包容性，非洲大陆拥有超过2000种语言。&lt;h4&gt;目的&lt;/h4&gt;探索非洲语言自动语音识别的研究现状，重点关注数据集、模型和训练方法、评估技术、挑战，并提出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA 2020程序，在多个学术数据库中检索2020年1月至2025年7月发表的研究，筛选出71篇高质量论文，记录了74个数据集，涵盖111种语言和约11,206小时的语音数据。&lt;h4&gt;主要发现&lt;/h4&gt;1) 少于15%的研究提供了可复现材料；2) 数据集许可不明确；3) 自监督和迁移学习技术有前景但受限于预训练数据和方言覆盖；4) 研究主要使用词错误率，很少考虑声调和形态丰富的语言特性；5) ASR系统研究证据不一致，受到数据集质量和基准测试限制。&lt;h4&gt;结论&lt;/h4&gt;社区驱动倡议和方法进步为改进提供了途径，未来可持续发展需要利益相关方合作、创建伦理平衡的数据集、使用轻量级建模技术和加强基准测试。&lt;h4&gt;翻译&lt;/h4&gt;ASR已取得显著的全球进展，然而非洲低资源语言仍然严重缺乏代表性，阻碍了非洲大陆的数字化包容，非洲大陆有超过2000种语言。这篇系统文献综述探索了非洲语言ASR的研究，重点关注数据集、模型和训练方法、评估技术、挑战，并推荐未来方向。我们采用PRISMA 2020程序，在DBLP、ACM Digital Library、Google Scholar、Semantic Scholar和arXiv中搜索2020年1月至2025年7月发表的研究。我们纳入与非洲语言ASR数据集、模型或指标相关的研究，同时排除非非洲研究、重复研究和低质量研究(评分&lt;3/5)。我们从2,062条记录中筛选出71条，记录了涵盖111种语言的74个数据集，包含约11,206小时的语音。少于15%的研究提供了可复现材料，数据集许可不明确。自监督和迁移学习技术有前景，但受到预训练数据有限、方言覆盖不足和资源可用性的阻碍。大多数研究人员使用词错误率(WER)，很少使用语言学评分如字符错误率(CER)或变音符号错误率(DER)，因此在声调和形态丰富的语言中应用有限。关于ASR系统的现有证据不一致，受到数据集可用性、注释质量、许可不确定性和有限基准测试等问题的影响。尽管如此，社区驱动倡议和方法进步的兴起表明了改进的途径。该领域的可持续发展还将包括利益相关方合作、创建伦理平衡的数据集、使用轻量级建模技术和积极的基准测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ASR has achieved remarkable global progress, yet African low-resourcelanguages remain rigorously underrepresented, producing barriers to digitalinclusion across the continent with more than +2000 languages. This systematicliterature review (SLR) explores research on ASR for African languages with afocus on datasets, models and training methods, evaluation techniques,challenges, and recommends future directions. We employ the PRISMA 2020procedures and search DBLP, ACM Digital Library, Google Scholar, SemanticScholar, and arXiv for studies published between January 2020 and July 2025. Weinclude studies related to ASR datasets, models or metrics for Africanlanguages, while excluding non-African, duplicates, and low-quality studies(score &lt;3/5). We screen 71 out of 2,062 records and we record a total of 74datasets across 111 languages, encompassing approximately 11,206 hours ofspeech. Fewer than 15% of research provided reproducible materials, and datasetlicensing is not clear. Self-supervised and transfer learning techniques arepromising, but are hindered by limited pre-training data, inadequate coverageof dialects, and the availability of resources. Most of the researchers useWord Error Rate (WER), with very minimal use of linguistically informed scoressuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus withlimited application in tonal and morphologically rich languages. The existingevidence on ASR systems is inconsistent, hindered by issues like datasetavailability, poor annotations, licensing uncertainties, and limitedbenchmarking. Nevertheless, the rise of community-driven initiatives andmethodological advancements indicates a pathway for improvement. Sustainabledevelopment for this area will also include stakeholder partnership, creationof ethically well-balanced datasets, use of lightweight modelling techniques,and active benchmarking.</description>
      <author>example@mail.com (Sukairaj Hafiz Imam, Tadesse Destaw Belay, Kedir Yassin Husse, Ibrahim Said Ahmad, Idris Abdulmumin, Hadiza Ali Umar, Muhammad Yahuza Bello, Joyce Nakatumba-Nabende, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad)</author>
      <guid isPermaLink="false">2510.01145v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.00956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was submitted to IEEE ICC 2026. 7 Pages, 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合迁移学习的混合方法，通过微调预训练模型将模拟数据与少量真实数据结合，显著提高了网络行为预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;基于机器学习的网络模型能够快速准确地预测复杂网络行为，但需要大量训练数据。从真实网络收集这些数据通常成本高昂且有限，特别是在故障等关键场景中。&lt;h4&gt;目的&lt;/h4&gt;解决模型在真实环境中部署时因依赖模拟数据而导致的准确性降低问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种利用迁移学习结合模拟和真实数据的混合方法，使用RouteNet-Fermi模型，通过少量真实数据微调预训练模型来提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;使用OMNeT++和自定义测试台的实验将数据包延迟预测的平均绝对百分比误差降低了高达88%。仅使用10个真实场景，MAPE下降37%；使用50个场景，MAPE下降48%。&lt;h4&gt;结论&lt;/h4&gt;通过迁移学习结合少量真实数据与模拟数据，可以显著提高网络行为预测模型在真实环境中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;基于机器学习的网络模型能够为复杂的网络行为提供快速而准确的预测，但需要大量的训练数据。从真实网络收集此类数据通常成本高昂且有限，特别是在故障等关键场景中。因此，研究人员通常依赖模拟数据，但这会导致模型在真实环境中部署时准确性降低。我们提出了一种利用迁移学习结合模拟和真实数据的混合方法。使用RouteNet-Fermi，我们展示了用少量真实数据微调预训练模型能显著提高性能。我们在OMNeT++和自定义测试台上的实验将数据包延迟预测的平均绝对百分比误差降低了高达88%。仅使用10个真实场景，MAPE就下降了37%，使用50个场景则下降了48%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine Learning (ML)-based network models provide fast and accuratepredictions for complex network behaviors but require substantial trainingdata. Collecting such data from real networks is often costly and limited,especially for critical scenarios like failures. As a result, researcherscommonly rely on simulated data, which reduces accuracy when models aredeployed in real environments. We propose a hybrid approach leveraging transferlearning to combine simulated and real-world data. Using RouteNet-Fermi, weshow that fine-tuning a pre-trained model with a small real datasetsignificantly improves performance. Our experiments with OMNeT++ and a customtestbed reduce the Mean Absolute Percentage Error (MAPE) in packet delayprediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, andwith 50 scenarios, by 48%.</description>
      <author>example@mail.com (Carlos Güemes-Palau, Miquel Ferriol-Galmés, Jordi Paillisse-Vilanova, Albert López-Brescó, Pere Barlet-Ros, Albert Cabellos-Aparicio)</author>
      <guid isPermaLink="false">2510.00956v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Intuitions of Machine Learning Researchers about Transfer Learning for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.00902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了医学影像迁移学习中源数据集选择的决策过程，发现选择是任务依赖性的，受社区实践、数据集属性和相似性影响，且相似性与预期性能并不总是对齐。&lt;h4&gt;背景&lt;/h4&gt;迁移学习对医学影像至关重要，但源数据集的选择通常基于研究者直觉而非系统原则，这会影响算法的可推广性和患者结果。&lt;h4&gt;目的&lt;/h4&gt;从人类中心的人机交互(HCI)角度了解机器学习从业者如何选择源数据集，为更系统的源选择提供实用见解。&lt;h4&gt;方法&lt;/h4&gt;对机器学习从业者进行基于任务的调查，采用人类中心的HCI视角研究源数据集选择决策，而非传统的模型和实验设置基准测试方法。&lt;h4&gt;主要发现&lt;/h4&gt;1)选择是任务依赖性的；2)受社区实践、数据集属性和计算或感知相似性影响；3)相似性评级与预期性能并不总是对齐，挑战了'越相似越好'的传统观点；4)参与者常使用模糊术语，表明需要更清晰的定义和HCI工具。&lt;h4&gt;结论&lt;/h4&gt;通过阐明这些启发式方法，该工作为迁移学习中更系统的源数据集选择提供了实用见解，有助于改善算法的可推广性和患者结果。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习对医学影像至关重要，然而源数据集的选择——这可能影响算法的可推广性，进而影响患者结果——通常依赖于研究者的直觉而非系统原则。本研究通过一项针对机器学习从业者的基于任务的调查来研究这些决策。与之前对模型和实验设置进行基准测试的工作不同，我们采用人类中心的HCI视角来了解从业者如何选择源数据集。我们的发现表明，选择是任务依赖性的，并受到社区实践、数据集属性以及计算(数据嵌入)或感知视觉或语义相似性的影响。然而，相似性评级和预期性能并不总是对齐，挑战了传统的'越相似越好'的观点。参与者经常使用模糊的术语，这表明需要更清晰的定义和HCI工具来使其明确和可用。通过阐明这些启发式方法，这项工作为迁移学习中更系统的源选择提供了实用见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is crucial for medical imaging, yet the selection of sourcedatasets - which can impact the generalizability of algorithms, and thuspatient outcomes - often relies on researchers' intuition rather thansystematic principles. This study investigates these decisions through atask-based survey with machine learning practitioners. Unlike prior work thatbenchmarks models and experimental setups, we take a human-centered HCIperspective on how practitioners select source datasets. Our findings indicatethat choices are task-dependent and influenced by community practices, datasetproperties, and computational (data embedding), or perceived visual or semanticsimilarity. However, similarity ratings and expected performance are not alwaysaligned, challenging a traditional "more similar is better" view. Participantsoften used ambiguous terminology, which suggests a need for clearer definitionsand HCI tools to make them explicit and usable. By clarifying these heuristics,this work provides practical insights for more systematic source selection intransfer learning.</description>
      <author>example@mail.com (Yucheng Lu, Hubert Dariusz Zając, Veronika Cheplygina, Amelia Jiménez-Sánchez)</author>
      <guid isPermaLink="false">2510.00902v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>LVLMs as inspectors: an agentic framework for category-level structural defect annotation</title>
      <link>http://arxiv.org/abs/2510.00603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ADPT的新型自动结构缺陷标注框架，该框架结合大视觉语言模型、语义模式匹配和迭代自提问完善机制，能够在无需人工监督的情况下将原始视觉数据转换为高质量语义标注的缺陷数据集。&lt;h4&gt;背景&lt;/h4&gt;自动化结构缺陷标注对于确保基础设施安全至关重要，同时可以最大限度地减少人工标注的高成本和低效率问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工监督的高效、准确的结构缺陷自动标注方法，构建高质量数据集，支持结构损伤评估中的迁移学习和领域适应等下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出了ADPT框架，整合了大视觉语言模型(LVLMs)、语义模式匹配模块和迭代自提问完善机制，通过优化的领域特定提示和递归验证流程实现自动标注。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ADPT在区分缺陷和非缺陷图像时准确率高达98%，在四类缺陷的平衡类别设置下标注准确率为85%-98%，在不平衡类别数据集上准确率为80%-92%。&lt;h4&gt;结论&lt;/h4&gt;ADPT框架为高保真数据集构建提供了可扩展且经济高效的解决方案，为结构损伤评估等下游任务提供了强有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;自动化结构缺陷标注对于确保基础设施安全至关重要，同时可以最大限度地减少人工标注的高成本和低效率问题。本文介绍了一种新颖的代理标注框架——基于代理的缺陷模式标记器(ADPT)，该框架整合了大视觉语言模型、语义模式匹配模块和迭代自提问完善机制。通过利用优化的领域特定提示和递归验证流程，ADPT能够在没有任何人工监督的情况下将原始视觉数据转换为高质量的语义标注缺陷数据集。实验结果表明，ADPT在区分缺陷图像和非缺陷图像时准确率高达98%，在四类缺陷的平衡类别设置下标注准确率为85%-98%，在不平衡类别数据集上准确率为80%-92%。该框架为高保真数据集构建提供了可扩展且经济高效的解决方案，为迁移学习和领域适应等下游任务提供了强有力的支持，特别是在结构损伤评估领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated structural defect annotation is essential for ensuringinfrastructure safety while minimizing the high costs and inefficiencies ofmanual labeling. A novel agentic annotation framework, Agent-based DefectPattern Tagger (ADPT), is introduced that integrates Large Vision-LanguageModels (LVLMs) with a semantic pattern matching module and an iterativeself-questioning refinement mechanism. By leveraging optimized domain-specificprompting and a recursive verification process, ADPT transforms raw visual datainto high-quality, semantically labeled defect datasets without any manualsupervision. Experimental results demonstrate that ADPT achieves up to 98%accuracy in distinguishing defective from non-defective images, and 85%-98%annotation accuracy across four defect categories under class-balancedsettings, with 80%-92% accuracy on class-imbalanced datasets. The frameworkoffers a scalable and cost-effective solution for high-fidelity datasetconstruction, providing strong support for downstream tasks such as transferlearning and domain adaptation in structural damage assessment.</description>
      <author>example@mail.com (Sheng Jiang, Yuanmin Ning, Bingxi Huang, Peiyin Chen, Zhaohui Chen)</author>
      <guid isPermaLink="false">2510.00603v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment</title>
      <link>http://arxiv.org/abs/2510.00048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究开发了一种混合深度学习集成框架，使用结构磁共振成像数据来区分阿尔茨海默病、轻度认知障碍和正常对照组，通过结合多种预训练CNN和集成学习策略，实现了高准确率，并通过可解释AI技术提高了诊断的可解释性。&lt;h4&gt;背景&lt;/h4&gt;早期准确诊断阿尔茨海默病对有效临床干预至关重要，需要将阿尔茨海默病与轻度认知障碍区分开，后者是一种以细微结构变化为特征的早期阶段。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合深度学习集成框架，用于阿尔茨海默病分类，使用结构磁共振成像作为数据源。&lt;h4&gt;方法&lt;/h4&gt;使用灰质和白质切片作为输入，采用三种预训练的卷积神经网络（ResNet50、NASNet和MobileNet），通过端到端过程对每个模型进行微调，并采用堆叠集成学习策略，包含元学习器和加权平均，以最优方式组合基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;在阿尔茨海默病神经影像计划数据集上评估，所提出的方法实现了最先进的准确率：阿尔茨海默病与轻度认知障碍之间为99.21%，轻度认知障碍与正常对照组之间为91.0%，优于传统的迁移学习和基线集成方法；集成了可解释人工智能技术，通过梯度加权类激活生成热图和归因图，突出灰质和白质切片中的关键区域，揭示了影响模型决策的结构生物标志物。&lt;h4&gt;结论&lt;/h4&gt;结果突显了该框架在神经退行性疾病诊断中具有稳健和可扩展的临床决策支持潜力。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病的早期准确诊断对有效临床干预至关重要，特别是在将其与轻度认知障碍区分方面，后者是一种以细微结构变化为特征的早期阶段。在本研究中，我们提出了一种混合深度学习集成框架，使用结构磁共振成像进行阿尔茨海默病分类。灰质和白质切片被用作三种预训练的卷积神经网络的输入，如ResNet50、NASNet和MobileNet，每个网络都通过端到端过程进行微调。为进一步提高性能，我们采用堆叠集成学习策略，结合元学习器和加权平均，以最优方式组合基础模型。在阿尔茨海默病神经影像计划数据集上评估，所提出的方法实现了最先进的准确率：阿尔茨海默病与轻度认知障碍之间为99.21%，轻度认知障碍与正常对照组之间为91.0%，优于传统的迁移学习和基线集成方法。为了提高基于图像诊断的可解释性，我们通过梯度加权类激活集成了可解释人工智能技术，生成热图和归因图，突出灰质和白质切片中的关键区域，揭示了影响模型决策的结构生物标志物。这些结果突显了该框架在神经退行性疾病诊断中具有稳健和可扩展的临床决策支持潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early and accurate diagnosis of Alzheimer Disease is critical for effectiveclinical intervention, particularly in distinguishing it from Mild CognitiveImpairment, a prodromal stage marked by subtle structural changes. In thisstudy, we propose a hybrid deep learning ensemble framework for AlzheimerDisease classification using structural magnetic resonance imaging. Gray andwhite matter slices are used as inputs to three pretrained convolutional neuralnetworks such as ResNet50, NASNet, and MobileNet, each fine tuned through anend to end process. To further enhance performance, we incorporate a stackedensemble learning strategy with a meta learner and weighted averaging tooptimally combine the base models. Evaluated on the Alzheimer DiseaseNeuroimaging Initiative dataset, the proposed method achieves state of the artaccuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and91.0% for Mild Cognitive Impairment vs. Normal Controls, outperformingconventional transfer learning and baseline ensemble methods. To improveinterpretability in image based diagnostics, we integrate Explainable AItechniques by Gradient weighted Class Activation, which generates heatmaps andattribution maps that highlight critical regions in gray and white matterslices, revealing structural biomarkers that influence model decisions. Theseresults highlight the frameworks potential for robust and scalable clinicaldecision support in neurodegenerative disease diagnostics.</description>
      <author>example@mail.com (Fahad Mostafa, Kannon Hossain, Hafiz Khan)</author>
      <guid isPermaLink="false">2510.00048v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>VideoNSA: Native Sparse Attention Scales Video Understanding</title>
      <link>http://arxiv.org/abs/2510.02295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://enxinsong.com/VideoNSA-web/, Code:  https://github.com/Espere-1119-Song/VideoNSA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoNSA是一种针对视频语言模型优化的方法，通过本地稀疏注意力技术解决了视频理解中的上下文长度限制问题，在长视频理解、时间推理和空间分析任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;多模态语言模型中的视频理解受到上下文长度的限制，模型经常错过关键转换帧，难以在长时间尺度上保持连贯性。&lt;h4&gt;目的&lt;/h4&gt;解决多模态语言模型中视频理解的上下文长度限制问题，提高模型在长时间视频上的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出VideoNSA方法，通过在包含21.6万个视频指令的数据集上进行端到端训练，将本地稀疏注意力（NSA）适配到视频语言模型Qwen2.5-VL中。采用硬件感知的混合注意力方法，为文本保留密集注意力，同时为视频使用NSA。&lt;h4&gt;主要发现&lt;/h4&gt;与基于token压缩和无需训练的稀疏基线方法相比，VideoNSA在长视频理解、时间推理和空间基准测试中实现了改进的性能。消融分析揭示了四个关键发现：(1)可靠扩展到12.8万个token；(2)在固定预算下的最优全局-局部注意力分配；(3)任务相关的分支使用模式；(4)可学习的组合稀疏注意力有助于诱导动态注意力汇。&lt;h4&gt;结论&lt;/h4&gt;VideoNSA通过适配本地稀疏注意力到视频语言模型中，有效解决了多模态语言模型中视频理解的上下文长度限制问题，并在多个基准测试中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;多模态语言模型中的视频理解受到上下文长度的限制：模型经常错过关键转换帧，难以在长时间尺度上保持连贯性。为解决这一问题，我们将本地稀疏注意力（NSA）适配到视频语言模型中。我们的方法VideoNSA通过在21.6万个视频指令数据集上进行端到端训练来适配Qwen2.5-VL。我们采用了一种硬件感知的混合注意力方法，为文本保留密集注意力，同时为视频使用NSA。与基于token压缩和无需训练的稀疏基线方法相比，VideoNSA在长视频理解、时间推理和空间基准测试中实现了改进的性能。进一步的消融分析揭示了四个关键发现：(1)可靠扩展到12.8万个token；(2)在固定预算下的最优全局-局部注意力分配；(3)任务相关的分支使用模式；(4)可学习的组合稀疏注意力有助于诱导动态注意力汇。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding in multimodal language models remains limited by contextlength: models often miss key transition frames and struggle to maintaincoherence across long time scales. To address this, we adapt Native SparseAttention (NSA) to video-language models. Our method, VideoNSA, adaptsQwen2.5-VL through end-to-end training on a 216K video instruction dataset. Weemploy a hardware-aware hybrid approach to attention, preserving denseattention for text, while employing NSA for video. Compared totoken-compression and training-free sparse baselines, VideoNSA achievesimproved performance on long-video understanding, temporal reasoning, andspatial benchmarks. Further ablation analysis reveals four key findings: (1)reliable scaling to 128K tokens; (2) an optimal global-local attentionallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)the learnable combined sparse attention help induce dynamic attention sinks.</description>
      <author>example@mail.com (Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu)</author>
      <guid isPermaLink="false">2510.02295v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</title>
      <link>http://arxiv.org/abs/2510.02262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为F2C的训练-free方法，通过从孤立的关键帧扩展到关键片段来改善视频理解，同时采用自适应分辨率策略保持固定的计算预算。实验表明该方法在三个长视频基准测试中显著优于均匀采样。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型(VLMs)在各种视觉语言任务上取得了显著成果，但实际应用受到'大海捞针'问题的限制：从原始视频帧产生的大量视觉tokens耗尽了模型的上下文窗口。现有解决方案通过选择稀疏帧集缓解此问题，但这种帧级选择丢弃了基本的时间动态，导致对运动和事件连续性的次优推理。&lt;h4&gt;目的&lt;/h4&gt;系统探索时间信息的影响，证明从孤立的关键帧扩展到关键片段可以改善视频理解，同时保持固定计算预算并适应更大的片段token占用空间。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的token数量恒定。该方法从帧级选择扩展到片段级选择，保留了时间连贯性，同时通过调整分辨率来控制计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在三个长视频基准测试(Video-MME、LongVideoBench和MLVU)上，F2C方法分别比均匀采样高出8.1%、5.6%和10.3%。这些结果突显了在帧选择中保持时间连贯性的重要性。&lt;h4&gt;结论&lt;/h4&gt;帧选择中保持时间连贯性对视频理解至关重要，从关键帧到关键片段的转变结合自适应分辨率策略，为扩展视频LLM到实际应用提供了实用路径。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(VLMs)在各种视觉语言任务上取得了显著成果，但它们的实际应用受到'大海捞针'问题的限制：从原始视频帧产生的大量视觉tokens耗尽了模型的上下文窗口。现有的解决方案通过选择稀疏的帧集来缓解这一问题，但这种帧级选择丢弃了基本的时间动态，导致对运动和事件连续性的次优推理。在本工作中，我们系统性地探索了时间信息的影响，并证明将选择从孤立的关键帧扩展到关键片段(短时间连贯的片段)可以改善视频理解。为了在适应片段更大的token占用空间的同时保持固定的计算预算，我们提出了一种自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的token数量恒定。在三个长视频基准测试上的实验表明，我们的训练-free方法F2C在Video-MME、LongVideoBench和MLVU基准测试上分别比均匀采样高出8.1%、5.6%和10.3%。这些结果突显了在帧选择中保持时间连贯性的重要性，并为扩展视频LLM到真实世界视频理解应用提供了实用途径。项目网页可在https://guangyusun.com/f2c获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VLMs) have achieved remarkable results on avariety of vision language tasks, yet their practical use is limited by the"needle in a haystack" problem: the massive number of visual tokens producedfrom raw video frames exhausts the model's context window. Existing solutionsalleviate this issue by selecting a sparse set of frames, thereby reducingtoken count, but such frame-wise selection discards essential temporaldynamics, leading to suboptimal reasoning about motion and event continuity. Inthis work we systematically explore the impact of temporal information anddemonstrate that extending selection from isolated key frames to key clips,which are short, temporally coherent segments, improves video understanding. Tomaintain a fixed computational budget while accommodating the larger tokenfootprint of clips, we propose an adaptive resolution strategy that dynamicallybalances spatial resolution and clip length, ensuring a constant token countper video. Experiments on three long-form video benchmarks demonstrate that ourtraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. Theseresults highlight the importance of preserving temporal coherence in frameselection and provide a practical pathway for scaling Video LLMs to real worldvideo understanding applications. Project webpage is available athttps://guangyusun.com/f2c .</description>
      <author>example@mail.com (Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler)</author>
      <guid isPermaLink="false">2510.02262v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>TimeGazer: Temporal Modeling of Predictive Gaze Stabilization for AR Interaction</title>
      <link>http://arxiv.org/abs/2510.01561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了TimeGazer，一种用于增强现实环境中视线稳定的新方法，通过时间回归模型预测理想注视轨迹，提高交互准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;在沉浸式AR环境中，视线稳定对实现流畅、准确和高效的交互至关重要。然而，活动视线任务中的注视序列常表现出不规则分散和系统性偏差，主要由人眼运动生理学、AR设备精度不足和环境干扰引起，损害了交互性能。&lt;h4&gt;目的&lt;/h4&gt;解决AR环境中视线不稳定的问题，提高交互准确性和视觉参与度，增强任务驱动AR交互中的注意力一致性和响应性。&lt;h4&gt;方法&lt;/h4&gt;将视线稳定重新表述为序列到序列的时间回归问题，从搜索阶段的历史视线动态中预测目标注视阶段的理想化注视轨迹。提出合成数据生成和混合策略，产生空间集中、以目标为中心的注视参考，丰富训练空间并增强模型泛化能力。在54名参与者通过Microsoft HoloLens 2收集的数据集上训练和评估TimeGazer。&lt;h4&gt;主要发现&lt;/h4&gt;统计结果表明TimeGazer显著提高了交互准确性并减少了完成时间，证实了预测性视线稳定的时间建模可以增强任务驱动AR交互中的注意力一致性和响应性。&lt;h4&gt;结论&lt;/h4&gt;TimeGazer在推进自适应基于视线的界面和沉浸式系统时间建模研究方面具有广泛潜力，为解决AR环境中视线不稳定问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视线稳定对于在沉浸式增强现实(AR)环境中实现流畅、准确和高效的交互至关重要，特别是在任务导向的视觉行为期间。然而，在活动视线任务中捕获的注视序列通常表现出不规则分散和从目标位置的系统性偏差，这种变异性主要由人眼运动生理学、AR头戴式设备跟踪和校准精度不足以及环境干扰的综合效应引起，这损害了交互性能和视觉参与度。为了解决这个问题，我们提出了TimeGazer，它将视线稳定重新表述为序列到序列的时间回归问题，从搜索阶段的历史视线动态中预测目标注视阶段的理想化注视轨迹。我们提出了一种合成数据生成和混合策略，产生空间集中、以目标为中心的注视参考，与任务目标保持一致，大大丰富了训练空间并增强了模型泛化能力。我们在54名参与者通过Microsoft HoloLens 2收集的真实和增强视线序列的混合数据集上训练和评估了TimeGazer，并在多个预测时间范围内进行了测试。通过用户研究，统计结果表明TimeGazer显著提高了交互准确性并减少了完成时间，证实了预测性视线稳定的时间建模可以增强任务驱动AR交互中的注意力一致性和响应性。这些发现突显了TimeGazer在推进自适应基于视线的界面和沉浸式系统时间建模研究方面的更广泛潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaze stabilization is critical for enabling fluid, accurate, and efficientinteraction in immersive augmented reality (AR) environments, particularlyduring task-oriented visual behaviors. However, fixation sequences captured inactive gaze tasks often exhibit irregular dispersion and systematic deviationsfrom target locations, a variability primarily caused by the combined effectsof human oculomotor physiology, insufficient AR headset tracking andcalibration accuracy, and environmental disturbances, undermining interactionperformance and visual engagement. To address this issue, we propose TimeGazer,which reformulates gaze stabilization as a sequence-to-sequence temporalregression problem, predicting idealized fixation trajectories for thetarget-fixation phase from historical gaze dynamics in the search phase. Wepresent a synthetic data generation and blending strategy that producesspatially concentrated, target-centered fixation references aligned with taskobjectives, substantially enriching the training space and enhancing modelgeneralization. We train and evaluate TimeGazer on a hybrid dataset of real andaugmented gaze sequences collected via Microsoft HoloLens 2 from 54participants across multiple prediction horizons. Through the user study,statistical results demonstrate that TimeGazer significantly improvesinteraction accuracy and reduces completion time, confirming that temporalmodeling of predictive gaze stabilization can strengthen attentionalconsistency and responsiveness in task-driven AR interaction. These findingshighlight the broader potential of TimeGazer for advancing adaptive gaze-basedinterfaces and temporal modeling research in immersive systems.</description>
      <author>example@mail.com (Yaozheng Xia, Zaiping Zhu, Bo Pang, Shaorong Wang, Sheng Li)</author>
      <guid isPermaLink="false">2510.01561v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies</title>
      <link>http://arxiv.org/abs/2510.01391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in *sem 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为TAG-EQA的提示框架，通过将因果事件图注入大型语言模型输入，增强模型对事件问答特别是因果和时间推理的能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在通用语言任务上表现出色，但在处理基于事件的问答问题时，尤其是需要因果或时间推理的问题时，往往表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个提示框架，通过注入因果事件图来增强大型语言模型对事件推理的能力，无需微调即可提高问答准确性。&lt;h4&gt;方法&lt;/h4&gt;提出TAG-EQA框架，将结构化关系转换为自然语言语句，包含九种提示配置，结合三种策略（零样本、少样本、思维链）和三种输入模态（纯文本、纯图、文本+图），系统分析结构化知识何时以及如何辅助推理。&lt;h4&gt;主要发现&lt;/h4&gt;在TORQUESTRA基准测试中，TAG-EQA相比纯文本基线平均提高5%的准确率，零样本设置中最高提高12%，图增强的思维链提示有效时提高幅度可达18%。&lt;h4&gt;结论&lt;/h4&gt;因果图可以在不进行微调的情况下增强大型语言模型中的事件推理能力，为在基于提示的问答中编码结构提供了一种灵活的方式。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在通用语言任务上表现出色，但在处理基于事件的问答问题时，尤其是那些需要因果或时间推理的问题时，往往表现不佳。我们引入了TAG-EQA（Text-And-Graph for Event Question Answering），一种提示框架，通过将结构化关系转换为自然语言语句，将因果事件图注入到大型语言模型的输入中。TAG-EQA包含九种提示配置，结合了三种策略（零样本、少样本、思维链）和三种输入模态（纯文本、纯图、文本+图），能够系统分析结构化知识何时以及如何辅助推理。在TORQUESTRA基准测试中，TAG-EQA相比纯文本基线平均提高了5%的准确率，在零样本设置中最高可提高12%，当图增强的思维链提示有效时，提高幅度可达18%。虽然性能因模型和配置而异，但我们的研究表明，因果图可以在不进行微调的情况下增强大型语言模型中的事件推理能力，为在基于提示的问答中编码结构提供了一种灵活的方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at general language tasks but oftenstruggle with event-based questions-especially those requiring causal ortemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event QuestionAnswering), a prompting framework that injects causal event graphs into LLMinputs by converting structured relations into natural-language statements.TAG-EQA spans nine prompting configurations, combining three strategies(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,graph-only, text+graph), enabling a systematic analysis of when and howstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQAimproves accuracy by 5% on average over text-only baselines, with gains up to12% in zero-shot settings and 18% when graph-augmented CoT prompting iseffective. While performance varies by model and configuration, our findingsshow that causal graphs can enhance event reasoning in LLMs withoutfine-tuning, offering a flexible way to encode structure in prompt-based QA.</description>
      <author>example@mail.com (Maithili Kadam, Francis Ferraro)</author>
      <guid isPermaLink="false">2510.01391v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Augmenting LLMs for General Time Series Understanding and Prediction</title>
      <link>http://arxiv.org/abs/2510.01111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的时间序列增强大型语言模型(TsLLM)，结合了时间序列数据处理能力和自然语言理解能力，解决了传统时间序列模型无法处理文本信息以及大型语言模型难以处理数值型时间序列数据的问题。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据在医疗保健、金融和环境科学等关键领域的决策中非常重要，但传统时间序列模型缺乏处理文本信息的能力，无法整合非结构化上下文信息、回答领域特定问题或生成自然语言解释。同时，大型语言模型虽然擅长上下文推理和知识整合，却因基于文本的表示效率低下及预训练中时间数据接触有限而难以处理数值型时间序列数据。&lt;h4&gt;目的&lt;/h4&gt;解决传统时间序列模型和大型语言模型之间的能力差距，通过专门的感知能力增强LLM以处理时间序列数据，创建一种能够同时处理数值计算和自然语言理解的新型时间序列分析范式。&lt;h4&gt;方法&lt;/h4&gt;采用基于补丁的编码器-解码器架构增强大型语言模型，创建时间序列增强的大型语言模型(TsLLM)。在包含超过200万个交错时间序列和文本示例的大型语料库上训练，涵盖多种分析任务：具有上下文信息的预测、时间序列问答、模式解释、具有自然语言输出的分类和报告生成。&lt;h4&gt;主要发现&lt;/h4&gt;TsLLM能够同时利用其语言理解能力和新获得的时序推理能力。虽然在传统基准测试中不是为了超越专业模型，但在需要时间序列分析与自然语言整合的任务上表现出色，这是现有方法无法提供的。&lt;h4&gt;结论&lt;/h4&gt;这项工作建立了一种新的时间序列分析范式，弥合了数值计算和自然语言理解之间的鸿沟，通过自然语言交互使复杂的时序推理变得普及。&lt;h4&gt;翻译&lt;/h4&gt;时间序列数据是医疗保健、金融和环境科学等许多关键领域决策的基础。然而，分析这些数据通常需要整合非结构化的上下文信息，回答领域特定问题，以及生成自然语言解释——这些能力是传统时间序列模型所缺乏的，因为它们无法处理文本。虽然大型语言模型擅长上下文推理和知识整合，但由于基于文本的表示效率低下以及在预训练过程中对时间数据的接触有限，它们难以处理数值型时间序列数据。我们通过基于补丁的编码器-解码器架构增强LLM，专门的时间序列感知能力来解决这一差距。我们在包含超过200万个交错时间序列和文本示例的大型语料库上训练这个时间序列增强的LLM(TsLLM)，涵盖各种分析任务：具有上下文信息的预测、时间序列问答、模式解释、具有自然语言输出的分类和报告生成。这种训练使TsLLM能够同时利用其语言理解能力和新获得的时序推理能力。虽然不是为了在传统基准测试中超越专业模型而设计的，但TsLLM在需要时间序列分析与自然语言整合的任务上表现出色，这是现有方法无法提供的。我们的工作建立了一种新的时间序列分析范式，弥合了数值计算和自然语言理解之间的鸿沟，通过自然语言交互使复杂的时序推理变得普及。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series data is fundamental to decision-making in many crucial domainsincluding healthcare, finance, and environmental science. However, analyzingthis data often requires incorporating unstructured contextual information,answering domain-specific questions, and generating natural languageexplanations -- capabilities that traditional time series models lack due totheir inability to process text. While Large Language Models (LLMs) excel atcontextual reasoning and knowledge integration, they struggle with numericaltime series due to inefficient text-based representations and limited exposureto temporal data during pretraining. We address this gap by augmenting an LLMwith specialized time series perception through a patch-based encoder-decoderarchitecture. We train this Time Series-augmented LLM (TsLLM) on a large corpusof over 2 million interleaved time series and text examples spanning diverseanalysis tasks: forecasting with contextual information, time seriesquestion-answering, pattern explanation, classification with natural languageoutputs, and report generation. This training enables TsLLM to leverage bothits language understanding and newly acquired temporal reasoning capabilities.While not designed to surpass specialized models on traditional benchmarks,TsLLM demonstrates strong performance on tasks requiring the integration oftime series analysis with natural language -- capabilities that existingapproaches cannot provide. Our work establishes a new paradigm for time seriesanalysis that bridges numerical computation and natural language understanding,democratizing access to sophisticated temporal reasoning through naturallanguage interaction.</description>
      <author>example@mail.com (Felix Parker, Nimeesha Chan, Chi Zhang, Kimia Ghobadi)</author>
      <guid isPermaLink="false">2510.01111v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling</title>
      <link>http://arxiv.org/abs/2510.01025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了监督多维尺度变换（SMDS）方法，用于自动发现语言模型中的特征流形，并通过时间推理案例研究揭示了这些几何结构的多方面特性。&lt;h4&gt;背景&lt;/h4&gt;线性表征假设认为语言模型将概念编码为其潜在空间中的方向，形成有组织的多维流形。先前的研究专注于为特定特征发现特定几何结构，因此缺乏泛化性。&lt;h4&gt;目的&lt;/h4&gt;引入一种模型无关的方法来自动发现特征流形，并应用于时间推理以揭示这些结构的特性。&lt;h4&gt;方法&lt;/h4&gt;开发了监督多维尺度变换（SMDS）方法，这是一种模型无关的方法，用于自动发现语言模型中的特征流形。将其应用于时间推理作为案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;不同特征形成各种几何结构，如圆形、线和簇。这些结构一致地反映所表示概念的特性；在不同模型族和规模中保持稳定；主动支持模型中的推理；并根据上下文变化动态重塑。&lt;h4&gt;结论&lt;/h4&gt;这些发现揭示了特征流形的功能作用，支持了一种基于实体的推理模型，在该模型中，语言模型编码和转换结构化表示。&lt;h4&gt;翻译&lt;/h4&gt;线性表征假设认为语言模型将概念编码为其潜在空间中的方向，形成有组织的、多维流形。先前的研究专注于为特定特征发现特定几何结构，因此缺乏泛化性。我们引入了监督多维尺度变换（SMDS），这是一种模型无关的方法，用于自动发现特征流形。我们将SMDS应用于时间推理作为案例研究，发现不同特征形成各种几何结构，如圆形、线和簇。SMDS揭示了这些结构的多个见解：它们一致地反映所表示概念的特性；在不同模型族和规模中保持稳定；主动支持模型中的推理；并根据上下文变化动态重塑。总之，我们的发现揭示了特征流形的功能作用，支持了一种基于实体的推理模型，在该模型中，语言模型编码和转换结构化表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The linear representation hypothesis states that language models (LMs) encodeconcepts as directions in their latent space, forming organized,multidimensional manifolds. Prior efforts focus on discovering specificgeometries for specific features, and thus lack generalization. We introduceSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method toautomatically discover feature manifolds. We apply SMDS to temporal reasoningas a case study, finding that different features form various geometricstructures such as circles, lines, and clusters. SMDS reveals many insights onthese structures: they consistently reflect the properties of the concepts theyrepresent; are stable across model families and sizes; actively supportreasoning in models; and dynamically reshape in response to context changes.Together, our findings shed light on the functional role of feature manifolds,supporting a model of entity-based reasoning in which LMs encode and transformstructured representations.</description>
      <author>example@mail.com (Federico Tiblias, Irina Bigoulaeva, Jingcheng Niu, Simone Balloccu, Iryna Gurevych)</author>
      <guid isPermaLink="false">2510.01025v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.00726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and data available at https://github.com/iit-DLSLab/croSTAta&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种交叉状态转换注意力Transformer模型，通过新颖的状态转换注意力机制(STA)来调节标准注意力权重，使机器人策略能够更好地根据执行历史调整行为，并在训练中结合结构化注意力和时序掩码以提高鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;通过监督学习从演示中学习机器人操作策略在遇到训练中未明确涵盖的执行变化时仍然具有挑战性。虽然通过注意力机制整合历史上下文可以提高鲁棒性，但标准方法处理序列中的所有过去状态，没有明确建模演示可能包含的时序结构，如失败和恢复模式。&lt;h4&gt;目的&lt;/h4&gt;解决机器人策略在遇到训练中未明确涵盖的执行变化时的适应性问题，更好地建模演示中的时序结构，特别是失败和恢复模式。&lt;h4&gt;方法&lt;/h4&gt;提出交叉状态转换注意力Transformer，使用新颖的状态转换注意力(STA)机制基于学习到的状态演化模式调节标准注意力权重，在训练中结合结构化注意力和时序掩码，从最近的timesteps中随机移除视觉信息，鼓励从历史上下文进行时序推理。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟评估中，STA在所有任务中一致优于标准的交叉注意力和时序建模方法(如TCN和LSTM网络)，在精度关键任务上比交叉注意力提高了2倍以上。&lt;h4&gt;结论&lt;/h4&gt;所提出的STA机制能够有效建模状态之间的时序关系，特别是失败和恢复模式，通过结合结构化注意力和时序掩码训练，提高了机器人策略的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;通过从演示中进行监督学习来学习机器人操作策略，当策略遇到训练中未明确涵盖的执行变化时仍然具有挑战性。虽然通过注意力机制整合历史上下文可以提高鲁棒性，但标准方法处理序列中的所有过去状态，没有明确建模演示可能包含的时序结构，如失败和恢复模式。我们提出了一种交叉状态转换注意力Transformer，它采用了一种新颖的状态转换注意力(STA)机制，基于学习到的状态演化模式来调节标准注意力权重，使策略能够更好地根据执行历史调整其行为。我们的方法在训练中将这种结构化注意力与时序掩码相结合，从最近的timesteps中随机移除视觉信息，鼓励从历史上下文进行时序推理。在模拟中的评估表明，STA在所有任务中一致优于标准的交叉注意力和时序建模方法，如TCN和LSTM网络，在精度关键任务上比交叉注意力提高了2倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robotic manipulation policies through supervised learning fromdemonstrations remains challenging when policies encounter execution variationsnot explicitly covered during training. While incorporating historical contextthrough attention mechanisms can improve robustness, standard approachesprocess all past states in a sequence without explicitly modeling the temporalstructure that demonstrations may include, such as failure and recoverypatterns. We propose a Cross-State Transition Attention Transformer thatemploys a novel State Transition Attention (STA) mechanism to modulate standardattention weights based on learned state evolution patterns, enabling policiesto better adapt their behavior based on execution history. Our approachcombines this structured attention with temporal masking during training, wherevisual information is randomly removed from recent timesteps to encouragetemporal reasoning from historical context. Evaluation in simulation shows thatSTA consistently outperforms standard cross-attention and temporal modelingapproaches like TCN and LSTM networks across all tasks, achieving more than 2ximprovement over cross-attention on precision-critical tasks.</description>
      <author>example@mail.com (Giovanni Minelli, Giulio Turrisi, Victor Barasuol, Claudio Semini)</author>
      <guid isPermaLink="false">2510.00726v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs</title>
      <link>http://arxiv.org/abs/2510.00705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无需训练的框架，利用多模态大语言模型的内在不确定性作为主动指导信号，解决了MLLMs在细粒度感知任务中的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在细粒度感知方面存在困难，如在高分辨率图像中识别小物体或在长视频中找到关键时刻。现有方法通常依赖于复杂且任务特定的微调，限制了泛化能力并增加了模型复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的、无需训练的框架，利用MLLMs的内在不确定性作为主动指导信号，增强其在细粒度多模态任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;基于模型输出熵在接收到相关视觉信息时会降低的核心见解，引入了一种统一的机制，通过响应不确定性对候选视觉输入进行评分，使模型能够自主关注最显著的数据。&lt;h4&gt;主要发现&lt;/h4&gt;将这一简单原则应用于视觉搜索、长视频理解和时间定位三种复杂视觉任务，使现成的MLLMs能够实现与专门的微调方法相竞争的性能。&lt;h4&gt;结论&lt;/h4&gt;利用内在不确定性是增强细粒度多模态性能的一种强大且通用的策略。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通常在细粒度感知方面存在困难，例如在高分辨率图像中识别小物体或在长视频中找到关键时刻。现有工作通常依赖于复杂、任务特定的微调，这限制了它们的泛化能力并增加了模型复杂性。在本工作中，我们提出了一种有效的、无需训练的框架，利用MLLMs的内在不确定性作为主动指导信号。我们的核心见解是，当模型接收到相关的视觉信息时，其输出熵会降低。我们引入了一种统一的机制，通过响应不确定性对候选视觉输入进行评分，使模型能够自主关注最显著的数据。我们将这一简单原则应用于三种复杂的视觉任务：视觉搜索、长视频理解和时间定位，使现成的MLLMs能够实现与专门的微调方法相竞争的性能。我们的工作验证了利用内在不确定性是增强细粒度多模态性能的一种强大且通用的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) often struggle with fine-grainedperception, such as identifying small objects in high-resolution images orfinding key moments in long videos. Existing works typically rely oncomplicated, task-specific fine-tuning, which limits their generalizability andincreases model complexity. In this work, we propose an effective,training-free framework that uses an MLLM's intrinsic uncertainty as aproactive guidance signal. Our core insight is that a model's output entropydecreases when presented with relevant visual information. We introduce aunified mechanism that scores candidate visual inputs by response uncertainty,enabling the model to autonomously focus on the most salient data. We applythis simple principle to three complex visual tasks: Visual Search, Long VideoUnderstanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieveperformance competitive with specialized, fine-tuned methods. Our workvalidates that harnessing intrinsic uncertainty is a powerful, general strategyfor enhancing fine-grained multimodal performance.</description>
      <author>example@mail.com (Sanghwan Kim, Rui Xiao, Stephan Alaniz, Yongqin Xian, Zeynep Akata)</author>
      <guid isPermaLink="false">2510.00705v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2510.00572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IntrusionX是一种混合深度学习框架，通过结合CNN和LSTM网络，并使用松鼠搜索算法进行优化，有效解决了入侵检测系统面临的网络攻击演变、高维流量数据和严重类别不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;入侵检测系统（IDS）面临持续挑战，包括不断演变的网络攻击、高维流量数据，以及基准数据集如NSL-KDD中存在的严重类别不平衡问题。&lt;h4&gt;目的&lt;/h4&gt;解决入侵检测系统面临的挑战，提高检测性能，特别是对罕见类别的检测能力。&lt;h4&gt;方法&lt;/h4&gt;提出IntrusionX，一种混合深度学习框架，集成CNN进行局部特征提取，LSTM进行时序建模，使用松鼠搜索算法进行超参数优化，并采用严格的预处理、分层数据分割和动态类别加权技术。&lt;h4&gt;主要发现&lt;/h4&gt;在NSL-KDD数据集上，IntrusionX在二元分类中达到98%的准确率，在5类分类中达到87%的准确率，显著提高了少数类别的召回率（U2R: 71%，R2L: 93%）。&lt;h4&gt;结论&lt;/h4&gt;IntrusionX的创新点在于其可复现的、不平衡感知的设计和元启发式优化，能有效处理入侵检测系统中的复杂挑战。&lt;h4&gt;翻译&lt;/h4&gt;入侵检测系统（IDS）由于网络攻击的不断演变、高维流量数据以及基准数据集（如NSL-KDD）中严重的类别不平衡问题而面临持续挑战。为解决这些问题，我们提出了IntrusionX，一种混合深度学习框架，集成了卷积神经网络（CNN）用于局部特征提取和长短期记忆（LSTM）网络用于时序建模。该架构使用松鼠搜索算法（SSA）进一步优化，实现了有效的超参数调同时保持计算效率。我们的流程包含严格的预处理、分层数据分割和动态类别加权，以增强对罕见类别的检测。在NSL-KDD上的实验评估表明，IntrusionX在二元分类中达到98%的准确率，在5类分类中达到87%的准确率，少数类别召回率显著提高（U2R: 71%，R2L: 93%）。IntrusionX的创新点在于其可复现的、不平衡感知的设计和元启发式优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intrusion Detection Systems (IDS) face persistent challenges due to evolvingcyberattacks, high-dimensional traffic data, and severe class imbalance inbenchmark datasets such as NSL-KDD. To address these issues, we proposeIntrusionX, a hybrid deep learning framework that integrates ConvolutionalNeural Networks (CNNs) for local feature extraction and Long Short-Term Memory(LSTM) networks for temporal modeling. The architecture is further optimizedusing the Squirrel Search Algorithm (SSA), enabling effective hyperparametertuning while maintaining computational efficiency. Our pipeline incorporatesrigorous preprocessing, stratified data splitting, and dynamic class weightingto enhance the detection of rare classes. Experimental evaluation on NSL-KDDdemonstrates that IntrusionX achieves 98% accuracy in binary classification and87% in 5-class classification, with significant improvements in minority classrecall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in itsreproducible, imbalance-aware design with metaheuristic optimization.</description>
      <author>example@mail.com (Ahsan Farabi, Muhaiminul Rashid Shad, Israt Khandaker)</author>
      <guid isPermaLink="false">2510.00572v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?</title>
      <link>http://arxiv.org/abs/2510.00520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了CardioBench，一个用于超声心动图基础模型的全面基准，解决了该领域缺乏标准化评估工具的问题。通过整合八个公共数据集并涵盖多种任务类型，研究人员评估了不同类型的基础模型，发现了各模型的互补优势和局限性，为未来超声心动图基础模型的设计提供了指导。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在重塑医学影像领域，但在超声心动图中的应用仍然有限。虽然最近出现了几种专门针对超声心动图的基础模型，但缺乏标准化的基准来评估它们。超声心动图存在独特挑战，包括噪声采集、高帧冗余和有限的公共数据集，且大多数现有解决方案在私有数据上评估，限制了结果的可比性。&lt;h4&gt;目的&lt;/h4&gt;引入CardioBench，一个全面的超声心动图基础模型基准，以解决该领域缺乏标准化评估工具的问题，促进不同模型之间的公平比较和未来发展。&lt;h4&gt;方法&lt;/h4&gt;CardioBench将八个公共数据集统一为一个标准化套件，涵盖四个回归和五个分类任务，涉及功能、结构、诊断和视图识别终点。研究在一致的零样本、探测和校准协议下评估了多种领先的基础模型，包括心脏特定、生物医学和通用目的编码器，并发布了预处理、分割和公共评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型家族具有互补优势：时间建模对功能回归至关重要，检索在分布变化下提供鲁棒性，领域特定的文本编码器能捕获生理上有意义的轴。通用编码器迁移能力强，通常缩小与探测的差距，但在细粒度区分(如视图分类和细微病理识别)方面存在困难。&lt;h4&gt;结论&lt;/h4&gt;通过发布预处理、分割和公共评估流程，CardioBench建立了可重现的参考点，并提供了有价值的见解，以指导未来超声心动图基础模型的设计，推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)正在重塑医学影像领域，但其在超声心动图中的应用仍然有限。虽然最近已引入几种专门针对超声心动图的基础模型，但尚无标准化基准来评估它们。超声心动图存在独特挑战，包括噪声采集、高帧冗余和有限的公共数据集。大多数现有解决方案在私有数据上进行评估，限制了可比性。为解决这一问题，我们引入了CardioBench，一个用于超声心动图基础模型的全面基准。CardioBench将八个公共数据集统一为一个标准化套件，涵盖四个回归和五个分类任务，涉及功能、结构、诊断和视图识别终点。我们在一致的零样本、探测和校准协议下评估了几种领先的基础模型，包括心脏特定、生物医学和通用目的编码器。我们的结果突显了不同模型家族的互补优势：时间建模对功能回归至关重要，检索在分布变化下提供鲁棒性，领域特定的文本编码器捕获生理上有意义的轴。通用编码器迁移能力强，通常缩小与探测的差距，但在细粒度区分如视图分类和细微病理识别方面存在困难。通过发布预处理、分割和公共评估流程，CardioBench建立了可重现的参考点，并提供了有价值的见解，以指导未来超声心动图基础模型的设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) are reshaping medical imaging, yet their applicationin echocardiography remains limited. While several echocardiography-specificFMs have recently been introduced, no standardized benchmark exists to evaluatethem. Echocardiography poses unique challenges, including noisy acquisitions,high frame redundancy, and limited public datasets. Most existing solutionsevaluate on private data, restricting comparability. To address this, weintroduce CardioBench, a comprehensive benchmark for echocardiography FMs.CardioBench unifies eight publicly available datasets into a standardized suitespanning four regression and five classification tasks, covering functional,structural, diagnostic, and view recognition endpoints. We evaluate severalleading FM, including cardiac-specific, biomedical, and general-purposeencoders, under consistent zero-shot, probing, and alignment protocols. Ourresults highlight complementary strengths across model families: temporalmodeling is critical for functional regression, retrieval provides robustnessunder distribution shift, and domain-specific text encoders capturephysiologically meaningful axes. General-purpose encoders transfer strongly andoften close the gap with probing, but struggle with fine-grained distinctionslike view classification and subtle pathology recognition. By releasingpreprocessing, splits, and public evaluation pipelines, CardioBench establishesa reproducible reference point and offers actionable insights to guide thedesign of future echocardiography foundation models.</description>
      <author>example@mail.com (Darya Taratynova, Ahmed Aly, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2510.00520v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation</title>
      <link>http://arxiv.org/abs/2510.00806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了TrajVLM-Gen，一个两阶段的物理感知图像到视频生成框架，通过视觉语言模型预测运动轨迹并基于注意力机制进行视频生成，解决了现有视频生成模型中物理不一致的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的视频生成模型产生的运动在物理上不一致，违反了真实世界的动力学规律。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成符合物理规律的动态视频的框架，解决现有视频生成模型中物理不一致的问题。&lt;h4&gt;方法&lt;/h4&gt;提出TrajVLM-Gen两阶段框架：首先使用视觉语言模型预测与真实世界物理保持一致的粗粒度运动轨迹；其次，通过基于注意力的机制引导视频生成，进行细粒度运动优化。同时，基于具有真实运动模式的视频跟踪数据构建了轨迹预测数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF-101和MSR-VTT数据集上的实验表明，TrajVLM-Gen优于现有方法，在UCF-101上实现了545的FVD分数，在MSR-VTT上实现了539的FVD分数。&lt;h4&gt;结论&lt;/h4&gt;TrajVLM-Gen框架通过结合物理感知的运动轨迹预测和基于注意力的视频生成，能够生成更符合物理规律的动态视频，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;当前的视频生成模型产生的运动在物理上不一致，违反了真实世界的动力学规律。我们提出了TrajVLM-Gen，一个用于物理感知图像到视频生成的两阶段框架。首先，我们采用视觉语言模型预测与真实世界物理保持一致的粗粒度运动轨迹。其次，这些轨迹通过基于注意力的机制引导视频生成，进行细粒度运动优化。我们基于具有真实运动模式的视频跟踪数据构建了轨迹预测数据集。在UCF-101和MSR-VTT上的实验表明，TrajVLM-Gen优于现有方法，在UCF-101上实现了545的竞争性FVD分数，在MSR-VTT上实现了539的FVD分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current video generation models produce physically inconsistent motion thatviolates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework forphysics-aware image-to-video generation. First, we employ a Vision LanguageModel to predict coarse-grained motion trajectories that maintain consistencywith real-world physics. Second, these trajectories guide video generationthrough attention-based mechanisms for fine-grained motion refinement. We builda trajectory prediction dataset based on video tracking data with realisticmotion patterns. Experiments on UCF-101 and MSR-VTT demonstrate thatTrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of545 on UCF-101 and 539 on MSR-VTT.</description>
      <author>example@mail.com (Fan Yang, Zhiyang Chen, Yousong Zhu, Xin Li, Jinqiao Wang)</author>
      <guid isPermaLink="false">2510.00806v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations</title>
      <link>http://arxiv.org/abs/2510.00405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了EgoTraj-Bench基准和BiFlow模型，用于解决从自我中心视角进行可靠轨迹预测的问题，通过考虑真实世界中的感知约束，提高了模型的鲁棒性和预测精度。&lt;h4&gt;背景&lt;/h4&gt;从自我中心视角进行可靠的轨迹预测对于在以人为中心的环境中机器人导航至关重要。然而，现有方法通常假设理想化的观察历史，没有考虑第一视觉中固有的感知伪影，如遮挡、ID切换和跟踪漂移。&lt;h4&gt;目的&lt;/h4&gt;弥合训练假设与部署现实之间的差距，开发能够应对真实世界自我中心感知挑战的轨迹预测系统。&lt;h4&gt;方法&lt;/h4&gt;引入EgoTraj-Bench基准，将嘈杂的第一视觉历史与清晰的鸟瞰图未来轨迹相关联；提出BiFlow双流流匹配模型，通过共享潜在表示同时去噪历史观察和预测未来运动；引入EgoAnchor机制，通过特征调制将预测解码器条件化为提取的历史特征。&lt;h4&gt;主要发现&lt;/h4&gt;BiFlow实现了最先进的性能，将minADE和minFDE平均降低了10-15%，并展示了卓越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;EgoTraj-Bench基准和BiFlow模型为开发真正能够抵御真实世界自我中心感知挑战的轨迹预测系统提供了关键基础。&lt;h4&gt;翻译&lt;/h4&gt;从以自我为中心的视角进行可靠的轨迹预测对于在以人为中心的环境中机器人导航至关重要。然而，现有方法通常假设理想化的观察历史，未能考虑第一视觉中固有的感知伪影，如遮挡、ID切换和跟踪漂移。这种训练假设与部署现实之间的差异严重限制了模型的鲁棒性。为了弥合这一差距，我们引入了EgoTraj-Bench，这是第一个将嘈杂的第一视觉历史与清晰的鸟瞰图未来轨迹相关联的真实世界基准，使模型能够在真实的感知约束下进行鲁棒学习。基于此基准，我们提出了BiFlow，一种双流流匹配模型，通过利用共享的潜在表示同时去噪历史观察和预测未来运动。为了更好地建模智能体意图，BiFlow融入了我们的EgoAnchor机制，该机制通过特征调制将预测解码器条件化为提取的历史特征。大量实验表明，BiFlow实现了最先进的性能，将minADE和minFDE平均降低了10-15%，并展示了卓越的鲁棒性。我们期望，我们的基准和模型将为开发真正能够抵御真实世界自我中心感知挑战的轨迹预测系统提供关键基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable trajectory prediction from an ego-centric perspective is crucial forrobotic navigation in human-centric environments. However, existing methodstypically assume idealized observation histories, failing to account for theperceptual artifacts inherent in first-person vision, such as occlusions, IDswitches, and tracking drift. This discrepancy between training assumptions anddeployment reality severely limits model robustness. To bridge this gap, weintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,first-person visual histories in clean, bird's-eye-view future trajectories,enabling robust learning under realistic perceptual constraints. Building onthis benchmark, we propose BiFlow, a dual-stream flow matching model thatconcurrently denoises historical observations and forecasts future motion byleveraging a shared latent representation. To better model agent intent, BiFlowincorporates our EgoAnchor mechanism, which conditions the prediction decoderon distilled historical features via feature modulation. Extensive experimentsshow that BiFlow achieves state-of-the-art performance, reducing minADE andminFDE by 10-15% on average and demonstrating superior robustness. Weanticipate that our benchmark and model will provide a critical foundation fordeveloping trajectory forecasting systems truly resilient to the challenges ofreal-world, ego-centric perception.</description>
      <author>example@mail.com (Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, Junwei Liang)</author>
      <guid isPermaLink="false">2510.00405v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting</title>
      <link>http://arxiv.org/abs/2510.00401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PINCoDE的物理信息神经控制微分方程模型，用于长时间范围的多机器人运动预测，该模型能够处理大规模多机器人系统并显著提高预测准确性。&lt;h4&gt;背景&lt;/h4&gt;长时间范围的多机器人运动预测面临非线性智能体交互、累积预测误差和动力学连续时间演化的挑战。学习此类系统的动力学对行程时间预测、预测引导规划和生成式仿真等应用具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于多智能体目标条件的高效轨迹预测模型，实现长时间范围的多机器人运动预测。&lt;h4&gt;方法&lt;/h4&gt;基于神经控制微分方程(CDEs)构建模型，与离散时间方法不同，它在连续时间运行，能够结合物理约束和偏差共同建模多机器人动力学。PINCoDE学习微分方程参数，从初始条件预测多智能体系统轨迹，并基于未来目标条件强制执行物理约束。采用可扩展策略使模型从10个机器人扩展到100个机器人，无需额外参数，并使用课程学习进行渐进式训练。&lt;h4&gt;主要发现&lt;/h4&gt;对于1分钟的时间范围，模型预测的平均位移误差低于0.5米；与分析模型相比，PINCoDE在4分钟时间范围内的预测姿态误差减少了2.7倍。&lt;h4&gt;结论&lt;/h4&gt;PINCoDE是一种有效的长时间范围多机器人运动预测方法，能够处理大规模多机器人系统，并通过结合物理约束和课程学习显著提高预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;对于多个自主机器人的长时间范围运动预测具有挑战性，这是由于非线性智能体交互、累积预测误差和动力学的连续时间演化。此类系统的学习动力学在各种应用中很有用，如行程时间预测、预测引导规划和生成式仿真。在本工作中，我们的目标是开发一个基于多智能体目标条件的高效轨迹预测模型。受最近物理学引导深度学习在部分已知动力学系统中成功的启发，我们开发了一种基于神经控制微分方程(CDEs)的模型，用于长时间范围的运动预测。与RNNs和transformers等离散时间方法不同，神经CDEs在连续时间运行，使我们能够结合物理信息约束和偏差来共同建模多机器人动力学。我们的方法名为PINCoDE(物理信息神经控制微分方程)，学习微分方程参数，可用于从初始条件预测多智能体系统的轨迹。PINCoDE基于未来目标条件，并强制执行机器人运动在长时间内的物理约束。我们采用一种策略，使模型能够从10个机器人扩展到100个机器人，而无需额外模型参数，同时产生1分钟时间范围内平均位移误差低于0.5米的预测。此外，使用课程学习对我们的PINCoDE模型进行渐进式训练，与分析模型相比，在4分钟时间范围内的预测姿态误差减少了2.7倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon motion forecasting for multiple autonomous robots is challengingdue to non-linear agent interactions, compounding prediction errors, andcontinuous-time evolution of dynamics. Learned dynamics of such a system can beuseful in various applications such as travel time prediction,prediction-guided planning and generative simulation. In this work, we aim todevelop an efficient trajectory forecasting model conditioned on multi-agentgoals. Motivated by the recent success of physics-guided deep learning forpartially known dynamical systems, we develop a model based on neuralControlled Differential Equations (CDEs) for long-horizon motion forecasting.Unlike discrete-time methods such as RNNs and transformers, neural CDEs operatein continuous time, allowing us to combine physics-informed constraints andbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE(Physics-Informed Neural Controlled Differential Equations), learnsdifferential equation parameters that can be used to predict the trajectoriesof a multi-agent system starting from an initial condition. PINCoDE isconditioned on future goals and enforces physics constraints for robot motionover extended periods of time. We adopt a strategy that scales our model from10 robots to 100 robots without the need for additional model parameters, whileproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.Furthermore, progressive training with curriculum learning for our PINCoDEmodel results in a 2.7X reduction of forecasted pose error over 4 minutehorizons compared to analytical models.</description>
      <author>example@mail.com (Shounak Sural, Charles Kekeh, Wenliang Liu, Federico Pecora, Mouhacine Benosman)</author>
      <guid isPermaLink="false">2510.00401v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.01206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种将分子动力学模拟作为时间序列预测问题的新方法，通过位移而非绝对位置预测原子轨迹，结合物理信息损失机制，显著提高了模拟效率与准确性。&lt;h4&gt;背景&lt;/h4&gt;传统的密度泛函理论方法计算成本高，限制了长期模拟的可行性，而分子动力学模拟对于理解材料科学和生物物理学中的原子尺度过程至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的分子动力学模拟方法，克服传统DFT方法的计算限制，实现长期原子尺度过程的准确模拟。&lt;h4&gt;方法&lt;/h4&gt;将MD模拟表述为时间序列预测问题，基于DFT参数化的成对Morse势函数构建物理信息损失和推理机制，通过惩罚非物理原子接近来确保物理合理性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多种材料的模拟准确性上一致优于标准基线方法，能够在几分钟内稳定建模数千个MD步骤，为昂贵的DFT模拟提供了一种可扩展的替代方案。&lt;h4&gt;结论&lt;/h4&gt;融入物理知识对于提高原子轨迹预测的可靠性和精确度至关重要，该方法为材料科学和生物物理学中的原子尺度过程研究提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;高效的分子动力学模拟对于理解材料科学和生物物理学中的原子尺度过程至关重要。传统的密度泛函理论方法计算成本高，限制了长期模拟的可行性。我们提出了一种新方法，将MD模拟表述为时间序列预测问题，使先进的预测模型能够通过位移而非绝对位置来预测原子轨迹。我们基于DFT参数化的成对Morse势函数，融入了物理信息损失和推理机制，通过惩罚非物理原子接近来确保物理合理性。我们的方法在多种材料的模拟准确性上一致优于标准基线方法。结果强调了融入物理知识以提高原子轨迹预测可靠性和精确度的重要性。值得注意的是，它能够在几分钟内稳定建模数千个MD步骤，为昂贵的DFT模拟提供了一种可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient molecular dynamics (MD) simulation is vital for understandingatomic-scale processes in materials science and biophysics. Traditional densityfunctional theory (DFT) methods are computationally expensive, which limits thefeasibility of long-term simulations. We propose a novel approach thatformulates MD simulation as a time-series forecasting problem, enablingadvanced forecasting models to predict atomic trajectories via displacementsrather than absolute positions. We incorporate a physics-informed loss andinference mechanism based on DFT-parametrised pair-wise Morse potentialfunctions that penalize unphysical atomic proximity to enforce physicalplausibility. Our method consistently surpasses standard baselines insimulation accuracy across diverse materials. The results highlight theimportance of incorporating physics knowledge to enhance the reliability andprecision of atomic trajectory forecasting. Remarkably, it enables stablemodeling of thousands of MD steps in minutes, offering a scalable alternativeto costly DFT simulations.</description>
      <author>example@mail.com (Hung Le, Sherif Abbas, Minh Hoang Nguyen, Van Dai Do, Huu Hiep Nguyen, Dung Nguyen)</author>
      <guid isPermaLink="false">2510.01206v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.01829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对三维目标检测器的分类任务提出了一种置信度校准方法，通过引入辅助正则化损失项来改善预测置信度的校准效果。&lt;h4&gt;背景&lt;/h4&gt;在自主系统中，精确的目标检测和不确定性估计对于系统的自我感知和安全运行至关重要。然而，三维目标检测器的置信度校准仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决三维目标检测器分类任务的置信度校准问题，提出能够捕捉主要和次要类别预测校准情况的度量指标，并开发相应的校准方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种辅助正则化损失项：一种针对主要预测的校准，另一种针对完整预测向量的校准。将这些方法与等度规回归相结合，应用于CenterPoint、PillarNet和DSVT-Pillar三种三维目标检测模型。&lt;h4&gt;主要发现&lt;/h4&gt;将完整类别预测的正则化损失项与等度规回归相结合，对CenterPoint和PillarNet的主要和次要类别预测实现了最佳校准效果。然而，DSVT-Pillar无法使用相同的方法同时校准主要和次要预测。&lt;h4&gt;结论&lt;/h4&gt;通过提出的正则化损失项与等度规回归的结合，可以有效改善三维目标检测器的置信度校准，但不同模型可能需要采用不同的校准策略才能达到最佳效果。&lt;h4&gt;翻译&lt;/h4&gt;在自主系统中，精确的目标检测和不确定性估计对于自我感知和安全运行至关重要。这项工作解决了三维目标检测器分类任务的置信度校准问题。我们认为有必要对所有类别的完整预测置信度分布进行校准，并提出了一种能够捕捉主要和次要类别预测校准情况的度量指标。我们提出了两个辅助的正则化损失项，它们分别引入主要预测的校准或完整预测向量的校准作为训练目标。我们评估了一系列应用于CenterPoint、PillarNet和DSVT-Pillar的事后和训练时间方法，发现将完整类别预测的正则化损失项与等度规回归相结合，对于CenterPoint和PillarNet的主要和次要类别预测都能实现最佳校准。我们还发现DSVT-Pillar无法使用相同的方法同时校准主要和次要预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D目标检测器的置信度校准问题，特别是针对自动驾驶场景中目标分类的不确定性估计。这个问题在现实中非常重要，因为自动驾驶系统需要准确评估其预测的不确定性，以便在规划行为时考虑可能的环境感知错误。如果系统对自己的预测置信度估计不准确，可能会导致危险决策。此外，完整预测分布（包括主导和次要类别）的校准对安全关键应用尤为重要，例如当物体被分类为车辆或弱势道路使用者的置信度相近时，系统需要了解这种不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从强校准条件出发，认为理想的校准应该考虑所有类别的预测概率分布，而不仅仅是主导类别。他们借鉴了现有的置信度校准方法，包括分箱方法（直方图分箱、等度回归）和缩放方法（Platt缩放、温度缩放），以及训练时校准方法如标签平滑和Focal Loss。在此基础上，作者设计了新的校准度量（Full D-ECE）和两种辅助损失函数（L_DECE和L_FullDECE），用于在训练过程中引入校准目标。作者还系统地评估了这些方法与现有训练后校准方法的组合效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是全面校准3D目标检测器的预测分布，不仅校准主导类别的置信度，还校准所有类别（包括次要类别）的预测分布。整体实现流程包括：1) 使用Waymo数据集训练三种3D目标检测器；2) 将D-ECE或Full D-ECE作为辅助损失函数添加到标准分类损失中进行训练时校准；3) 应用训练后校准方法（温度缩放、Platt缩放和等度回归）；4) 评估不同组合方法的校准性能（使用D-ECE和Full D-ECE指标）和检测性能（mAP和mAPH指标）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出完整检测期望校准误差（Full D-ECE）指标，评估所有类别预测的校准质量；2) 设计两种辅助损失函数（L_DECE和L_FullDECE），在训练过程中引入校准目标；3) 提出全面校准策略，同时考虑主导和次要类别预测；4) 系统性评估多种训练时和训练后校准方法的组合效果。相比之前的工作，这篇论文专门针对3D目标检测器，而不仅仅是2D检测器或一般分类器；它关注完整预测分布而非仅主导类别；提出的校准度量专门为3D检测场景设计；作者还分析了不同架构（如Transformer架构）对校准方法的影响，发现需要特定策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种针对3D目标检测器的全面置信度校准方法，通过新的校准度量和辅助损失函数，结合训练时和训练后校准技术，显著提高了自动驾驶系统中目标检测的不确定性估计质量，同时保持了检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV64158.2025.11097526&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous systems, precise object detection and uncertainty estimationare critical for self-aware and safe operation. This work addresses confidencecalibration for the classification task of 3D object detectors. We argue thatit is necessary to regard the calibration of the full predictive confidencedistribution over all classes and deduce a metric which captures thecalibration of dominant and secondary class predictions. We propose twoauxiliary regularizing loss terms which introduce either calibration of thedominant prediction or the full prediction vector as a training goal. Weevaluate a range of post-hoc and train-time methods for CenterPoint, PillarNetand DSVT-Pillar and find that combining our loss term, which regularizes forcalibration of the full class prediction, and isotonic regression lead to thebest calibration of CenterPoint and PillarNet with respect to both dominant andsecondary class predictions. We further find that DSVT-Pillar can not bejointly calibrated for dominant and secondary predictions using the samemethod.</description>
      <author>example@mail.com (Cornelius Schröder, Marius-Raphael Schlüter, Markus Lienkamp)</author>
      <guid isPermaLink="false">2510.01829v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Inferring Dynamic Physical Properties from Video Foundation Models</title>
      <link>http://arxiv.org/abs/2510.02311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究从视频中预测动态物理属性的方法，收集了新数据集并探索了三种不同的推断方法。&lt;h4&gt;背景&lt;/h4&gt;动态物理属性（如弹性、粘度和摩擦力）需要时间信息才能推断，传统方法可能难以准确捕捉这些属性。&lt;h4&gt;目的&lt;/h4&gt;开发能够从视频中准确预测动态物理属性的方法，包括弹性、粘度和摩擦力。&lt;h4&gt;方法&lt;/h4&gt;收集了新的视频数据集，并探索了三种推断方法：基于计算机视觉的oracle方法、基于预训练视频模型的提示机制、以及多模态大型语言模型的提示策略。&lt;h4&gt;主要发现&lt;/h4&gt;视频基础模型在生成或自监督训练下表现相似，但略低于oracle方法；MLLMs目前表现较差，但通过适当提示可改善。&lt;h4&gt;结论&lt;/h4&gt;视频基础模型在动态物理属性预测方面具有潜力，而MLLMs需要进一步优化才能达到类似性能。&lt;h4&gt;翻译&lt;/h4&gt;我们研究从视频中预测动态物理属性的任务。更具体地说，我们考虑需要时间信息才能推断的物理属性：弹跳物体的弹性、流动液体的粘度和物体在表面上滑动的动态摩擦力。为此，我们做出以下贡献：(i)我们为每个物理属性收集了一个新的视频数据集，包括合成的训练和测试分割，以及用于现实世界评估的真实分割。(ii)我们探索了三种从视频中推断物理属性的方法：(a)oracle方法，我们使用经典计算机视觉技术提供反映该属性固有视觉线索；(b)使用视觉提示和可训练提示向量在预训练的视频生成和自监督模型上进行交叉注意力的简单读取机制；(c)多模态大型语言模型(MLLMs)的提示策略。(iii)我们表明，以生成或自监督方式训练的视频基础模型实现了类似oracle方法的性能，尽管略低于oracle方法，而MLLMs目前劣于其他模型，但通过适当的提示可以提高其性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the task of predicting dynamic physical properties from videos. Morespecifically, we consider physical properties that require temporal informationto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,and dynamic friction of an object sliding on a surface. To this end, we makethe following contributions: (i) We collect a new video dataset for eachphysical property, consisting of synthetic training and testing splits, as wellas a real split for real world evaluation. (ii) We explore three ways to inferthe physical property from videos: (a) an oracle method where we supply thevisual cues that intrinsically reflect the property using classical computervision techniques; (b) a simple read out mechanism using a visual prompt andtrainable prompt vector for cross-attention on pre-trained video generative andself-supervised models; and (c) prompt strategies for Multi-modal LargeLanguage Models (MLLMs). (iii) We show that video foundation models trained ina generative or self-supervised manner achieve a similar performance, thoughbehind that of the oracle, and MLLMs are currently inferior to the othermodels, though their performance can be improved through suitable prompting.</description>
      <author>example@mail.com (Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman)</author>
      <guid isPermaLink="false">2510.02311v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data</title>
      <link>http://arxiv.org/abs/2510.02294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;F2LLM是一种新型的嵌入模型套件，包含0.6B、1.7B和4B三种参数规模版本，通过从基础模型直接微调而非使用对比预训练和合成数据的方式训练，在保持高性能的同时降低了训练成本。&lt;h4&gt;背景&lt;/h4&gt;现有的顶级嵌入模型通常需要大规模对比预训练、复杂的训练流程和昂贵的合成训练数据，这限制了其广泛应用和研究。&lt;h4&gt;目的&lt;/h4&gt;开发一种在训练成本、模型大小和嵌入性能之间取得平衡的高效嵌入模型，并提供可复现且经济实惠的基线。&lt;h4&gt;方法&lt;/h4&gt;从基础模型直接微调F2LLM，使用从开源非合成数据集中筛选的600万查询-文档-负元组进行训练，避免了复杂的对比预训练和昂贵的合成数据需求。&lt;h4&gt;主要发现&lt;/h4&gt;在MTEB英语排行榜上，F2LLM-4B在约40亿参数模型中排名第2，总体排名第7；F2LLM-1.7B在10-20亿参数规模模型中排名第1，证明了其高效性能。&lt;h4&gt;结论&lt;/h4&gt;F2LLM通过创新的训练方法，成功在训练成本、模型大小和性能之间取得了平衡，为嵌入模型领域提供了一个强大、可复现且经济实惠的基线。&lt;h4&gt;翻译&lt;/h4&gt;我们引入F2LLM - 基础到特性大型语言模型，这是一套包含三种尺寸的最先进嵌入模型：0.6B、1.7B和4B。与之前需要大量对比预训练、复杂训练流程和昂贵合成训练数据的顶级嵌入模型不同，F2LLM是从基础模型直接微调而来，使用了从开源非合成数据集中筛选的600万查询-文档-负元组，在训练成本、模型大小和嵌入性能之间取得了良好平衡。在MTEB英语排行榜上，F2LLM-4B在约40亿参数模型中排名第2，总体排名第7，而F2LLM-1.7B在10-20亿参数规模模型中排名第1。为了促进该领域未来的研究，我们发布了模型、训练数据集和代码，使F2LLM成为未来工作的强大、可复现且经济实惠的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce F2LLM - Foundation to Feature Large Language Models, a suite ofstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlikeprevious top-ranking embedding models that require massive contrastivepretraining, sophisticated training pipelines, and costly synthetic trainingdata, F2LLM is directly finetuned from foundation models on 6 millionquery-document-negative tuples curated from open-source, non-syntheticdatasets, striking a strong balance between training cost, model size, andembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2ndamong models with approximately 4B parameters and 7th overall, while F2LLM-1.7Branks 1st among models in the 1B-2B size range. To facilitate future researchin the field, we release the models, training dataset, and code, positioningF2LLM as a strong, reproducible, and budget-friendly baseline for future works.</description>
      <author>example@mail.com (Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang)</author>
      <guid isPermaLink="false">2510.02294v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Anchoring for Discrete Diffusion Posterior Sampling</title>
      <link>http://arxiv.org/abs/2510.02291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用预训练的离散扩散基础模型进行后验采样，从噪声测量中恢复图像而无需重新训练特定任务模型&lt;h4&gt;背景&lt;/h4&gt;扩散模型在生成建模方面取得显著成功，但大多依赖连续高斯扩散；离散扩散为文本和图像等分类数据提供统一建模框架，具有更快推理、更精细控制和无需训练的贝叶斯推断优势&lt;h4&gt;目的&lt;/h4&gt;解决离散扩散后验采样面临的挑战：无导数引导产生稀疏信号、连续松弛限制适用性、分裂吉布斯采样器受维度诅咒影响&lt;h4&gt;方法&lt;/h4&gt;引入锚定后验采样(APS)，用于掩码扩散基础模型，基于两个关键创新：离散嵌入空间中的量化期望（用于梯度类引导）和锚定重新掩码（用于自适应解码）&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准基准上针对线性和非线性逆问题在离散扩散采样器中实现了最先进的性能&lt;h4&gt;结论&lt;/h4&gt;该方法在无需训练的风格化和文本引导编辑中展示了其优势&lt;h4&gt;翻译&lt;/h4&gt;我们研究使用预训练的离散扩散基础模型进行后验采样的问题，旨在从噪声测量中恢复图像而无需重新训练特定任务模型。虽然扩散模型在生成建模方面取得了显著成功，但大多数进展依赖于连续高斯扩散。相比之下，离散扩散为文本和图像等分类数据提供了统一的建模框架。除了统一性，离散扩散提供了更快的推理、更精细的控制和无需训练的贝叶斯推断，使其特别适合后验采样。然而，现有的离散扩散后验采样方法面临严重挑战：无导数引导产生稀疏信号，连续松弛限制了适用性，分裂吉布斯采样器受到维度诅咒的影响。为了克服这些限制，我们为掩码扩散基础模型引入了锚定后验采样(APS)，基于两个关键创新——离散嵌入空间中的量化期望（用于梯度类引导）和锚定重新掩码（用于自适应解码）。我们的方法在标准基准上针对线性和非线性逆问题在离散扩散采样器中实现了最先进的性能。我们进一步展示了该方法在无需训练的风格化和文本引导编辑中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the problem of posterior sampling using pretrained discretediffusion foundation models, aiming to recover images from noisy measurementswithout retraining task-specific models. While diffusion models have achievedremarkable success in generative modeling, most advances rely on continuousGaussian diffusion. In contrast, discrete diffusion offers a unified frameworkfor jointly modeling categorical data such as text and images. Beyondunification, discrete diffusion provides faster inference, finer control, andprincipled training-free Bayesian inference, making it particularly well-suitedfor posterior sampling. However, existing approaches to discrete diffusionposterior sampling face severe challenges: derivative-free guidance yieldssparse signals, continuous relaxations limit applicability, and split Gibbssamplers suffer from the curse of dimensionality. To overcome theselimitations, we introduce Anchored Posterior Sampling (APS) for maskeddiffusion foundation models, built on two key innovations -- quantizedexpectation for gradient-like guidance in discrete embedding space, andanchored remasking for adaptive decoding. Our approach achievesstate-of-the-art performance among discrete diffusion samplers across linearand nonlinear inverse problems on the standard benchmarks. We furtherdemonstrate the benefits of our approach in training-free stylization andtext-guided editing.</description>
      <author>example@mail.com (Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman)</author>
      <guid isPermaLink="false">2510.02291v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals</title>
      <link>http://arxiv.org/abs/2510.02276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为BioX-Bridge的新框架，用于生物信号的无监督跨模态知识转移，通过训练轻量级桥接网络对齐不同模态生物信号的中间表示，实现信息流动。&lt;h4&gt;背景&lt;/h4&gt;生物信号能提供人体生理状态的宝贵见解，不同模态信号相互关联但功能各异。现有方法基于知识蒸馏，需要同时运行教师模型，计算和内存开销大，且基础模型虽性能优越但尺寸大。&lt;h4&gt;目的&lt;/h4&gt;解决生物信号跨模态知识转移中的高计算开销问题，开发一种高效的无监督跨模态知识转移方法，减少参数数量同时保持或提高性能。&lt;h4&gt;方法&lt;/h4&gt;训练轻量级桥接网络对齐中间表示，促进基础模型间和跨模态信息流动；引入高效策略选择桥接位置；采用灵活的原型网络作为桥接架构。&lt;h4&gt;主要发现&lt;/h4&gt;BioX-Bridge在多个生物信号模态、任务和数据集上实验表明，可将可训练参数数量减少88-99%，同时与最先进方法相比保持或提高了转移性能。&lt;h4&gt;结论&lt;/h4&gt;BioX-Bridge框架有效解决了生物信号跨模态知识转移中的计算效率问题，显著减少了参数数量同时保持了或提高了性能，为健康监测系统提供了更高效、更可访问的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生物信号为人体生理状态提供了宝贵的洞察。尽管生物信号模态在功能、信号保真度、传感器舒适度和成本方面有所不同，但它们通常是相互关联的，反映了人体生理的整体性和互联性。这使得使用替代的生物信号模态执行相同任务成为可能，从而提高健康监测系统的可访问性、可用性和适应性。然而，针对特定任务和感兴趣的模态训练模型的挑战在于标记数据集的有限可用性。无监督跨模态知识转移提供了一种有前途的解决方案，它利用现有模态的知识来支持新模态的模型训练。现有方法通常基于知识蒸馏，这需要 alongside 学生模型训练运行教师模型，导致高计算和内存开销。这个挑战随着最近基础模型的发展而进一步加剧，这些模型在跨任务方面表现出卓越的性能和泛化能力，但代价是模型尺寸大。为此，作者探索了一种新的生物信号无监督跨模态知识转移框架，通过训练轻量级桥接网络来对齐中间表示，并促进基础模型之间和跨模态的信息流动。具体来说，他们引入了一种选择桥接位置的策略，以及一个灵活的原型网络作为桥接架构。在多个生物信号模态、任务和数据集上的广泛实验表明，BioX-Bridge将可训练参数数量减少了88-99%，同时与最先进的方法相比保持了甚至提高了转移性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biosignals offer valuable insights into the physiological states of the humanbody. Although biosignal modalities differ in functionality, signal fidelity,sensor comfort, and cost, they are often intercorrelated, reflecting theholistic and interconnected nature of human physiology. This opens up thepossibility of performing the same tasks using alternative biosignalmodalities, thereby improving the accessibility, usability, and adaptability ofhealth monitoring systems. However, the limited availability of large labeleddatasets presents challenges for training models tailored to specific tasks andmodalities of interest. Unsupervised cross-modal knowledge transfer offers apromising solution by leveraging knowledge from an existing modality to supportmodel training for a new modality. Existing methods are typically based onknowledge distillation, which requires running a teacher model alongsidestudent model training, resulting in high computational and memory overhead.This challenge is further exacerbated by the recent development of foundationmodels that demonstrate superior performance and generalization across tasks atthe cost of large model sizes. To this end, we explore a new framework forunsupervised cross-modal knowledge transfer of biosignals by training alightweight bridge network to align the intermediate representations and enableinformation flow between foundation models and across modalities. Specifically,we introduce an efficient strategy for selecting alignment positions where thebridge should be constructed, along with a flexible prototype network as thebridge architecture. Extensive experiments across multiple biosignalmodalities, tasks, and datasets show that BioX-Bridge reduces the number oftrainable parameters by 88--99\% while maintaining or even improving transferperformance compared to state-of-the-art methods.</description>
      <author>example@mail.com (Chenqi Li, Yu Liu, Timothy Denison, Tingting Zhu)</author>
      <guid isPermaLink="false">2510.02276v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.02224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于copula的方法，用于从现有的多步时间序列基础模型中高效生成准确的相关样本路径，只需一次前向传递。该方法比自回归采样快几个数量级，并通过减轻误差累积现象提高了样本路径质量。&lt;h4&gt;背景&lt;/h4&gt;时间序列应用通常需要访问样本路径形式的多步预测轨迹。最近，时间序列基础模型利用多步前向预测来提高多步预测的质量和效率。然而，这些模型只预测每个时间步的独立边际分布，而不是完整的联合预测分布。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法在生成具有相关结构的预测样本路径时效率低下的问题，提出一种比自回归采样更快且质量更高的方法。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于copula的方法，可以从现有的多步时间序列基础模型中高效生成准确的相关样本路径，只需一次前向传递。&lt;h4&gt;主要发现&lt;/h4&gt;copula方法比自回归采样快几个数量级，并且通过减轻误差累积现象提高了样本路径质量。&lt;h4&gt;结论&lt;/h4&gt;基于copula的方法是生成相关时间序列预测样本路径的高效替代方案，在速度和质量上都优于传统的自回归采样方法。&lt;h4&gt;翻译&lt;/h4&gt;许多时间序列应用需要访问样本路径形式的多步预测轨迹。最近，时间序列基础模型利用多步前向预测来提高多步预测的质量和效率。然而，这些模型只预测每个时间步的独立边际分布，而不是完整的联合预测分布。为了生成具有相关结构的预测样本路径，通常采用自回归采样，但这可能非常昂贵。在本文中，我们提出了一种基于copula的方法，可以从现有的多步时间序列基础模型中高效生成准确的相关样本路径，只需一次前向传递。我们的copula方法比自回归采样快几个数量级，并且通过减轻误差累积现象提高了样本路径质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many time series applications require access to multi-step forecasttrajectories in the form of sample paths. Recently, time series foundationmodels have leveraged multi-step lookahead predictions to improve the qualityand efficiency of multi-step forecasts. However, these models only predictindependent marginal distributions for each time step, rather than a full jointpredictive distribution. To generate forecast sample paths with realisticcorrelation structures, one typically resorts to autoregressive sampling, whichcan be extremely expensive. In this paper, we present a copula-based approachto efficiently generate accurate, correlated sample paths from existingmulti-step time series foundation models in one forward pass. Our copula-basedapproach generates correlated sample paths orders of magnitude faster thanautoregressive sampling, and it yields improved sample path quality bymitigating the snowballing error phenomenon.</description>
      <author>example@mail.com (Ethan Baron, Boris Oreshkin, Ruijun Ma, Hanyu Zhang, Kari Torkkola, Michael W. Mahoney, Andrew Gordon Wilson, Tatiana Konstantinova)</author>
      <guid isPermaLink="false">2510.02224v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</title>
      <link>http://arxiv.org/abs/2510.02114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Master Thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了FFREEDG任务和FRIEREN框架，解决联邦学习在语义分割中面临的域偏移挑战，特别是在客户端仅有未标记数据的情况下，通过整合视觉和语言模态来提升性能。&lt;h4&gt;背景&lt;/h4&gt;联邦学习为语义分割任务提供隐私保护解决方案，但面临域偏移挑战，尤其是客户端数据未标记时。现有方法假设可访问客户端标记数据或未能充分利用现代视觉基础模型。&lt;h4&gt;目的&lt;/h4&gt;解决FFREEDG任务：模型在服务器标记源数据集预训练后，仅使用客户端未标记数据在客户端间训练，且不再重新访问源数据。&lt;h4&gt;方法&lt;/h4&gt;提出FRIEREN框架，利用视觉基础模型知识，整合视觉和语言模态。采用由CLIP文本嵌入引导的视觉语言解码器改善语义消歧，并使用弱到强一致性学习策略对伪标签进行鲁棒局部训练。&lt;h4&gt;主要发现&lt;/h4&gt;在合成到真实和清晰到恶劣天气的基准测试中，该框架有效解决了新任务，与现有领域泛化和适应方法相比取得有竞争力性能，为未来研究设定了强基线。&lt;h4&gt;结论&lt;/h4&gt;该研究解决了联邦学习中的重要挑战，提出的方法有效利用视觉基础模型知识，为处理未标记数据场景下的语义分割提供了创新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)为语义分割(SS)任务提供了一种隐私保护解决方案，可以适应新领域，但当客户端数据未标记时，面临着来自这些域偏移的显著挑战。然而，大多数现有的联邦学习方法不切实际地假设可以访问远程客户端的标记数据，或者未能利用现代视觉基础模型(VFMs)的力量。在这里，我们提出了一个新颖且具有挑战性的任务FFREEDG，其中模型在服务器的标记源数据集上预训练，随后仅使用客户端的未标记数据在客户端间进行训练，且不再重新访问源数据。为了解决FFREEDG，我们提出了FRIEREN框架，该框架通过整合视觉和语言模态来利用视觉基础模型的知识。我们的方法采用由基于CLIP的文本嵌入引导的视觉语言解码器来改善语义消歧，并使用弱到强一致性学习策略对伪标签进行鲁棒局部训练。我们在合成到真实和清晰到恶劣天气的基准测试中的实验表明，我们的框架有效地解决了这一新任务，与现有的领域泛化和适应方法相比取得了有竞争力的性能，并为未来研究设定了强有力的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federeated Learning (FL) offers a privacy-preserving solution for SemanticSegmentation (SS) tasks to adapt to new domains, but faces significantchallenges from these domain shifts, particularly when client data isunlabeled. However, most existing FL methods unrealistically assume access tolabeled data on remote clients or fail to leverage the power of modern VisionFoundation Models (VFMs). Here, we propose a novel and challenging task,FFREEDG, in which a model is pretrained on a server's labeled source datasetand subsequently trained across clients using only their unlabeled data,without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, aframework that leverages the knowledge of a VFM by integrating vision andlanguage modalities. Our approach employs a Vision-Language decoder guided byCLIP-based text embeddings to improve semantic disambiguation and uses aweak-to-strong consistency learning strategy for robust local training onpseudo-labels. Our experiments on synthetic-to-real andclear-to-adverse-weather benchmarks demonstrate that our framework effectivelytackles this new task, achieving competitive performance against establisheddomain generalization and adaptation methods and setting a strong baseline forfuture research.</description>
      <author>example@mail.com (Ding-Ruei Shen)</author>
      <guid isPermaLink="false">2510.02114v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.02084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KAIROS是一种非自回归时间序列预测框架，直接对段级多峰分布进行建模，避免了误差累积并实现了及时推理，在保持高性能的同时大幅降低了推理成本。&lt;h4&gt;背景&lt;/h4&gt;在万维网中，可靠的时间序列预测为资源规划、缓存放置和异常响应提供前瞻性信号，使平台能够随着用户行为和内容分布的演变而高效运行。与其他领域相比，Web应用的时间序列预测需要更快的响应速度以支持实时决策。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够快速响应的时间序列预测框架，以满足Web应用实时决策的需求，同时避免自回归方法的误差累积问题。&lt;h4&gt;方法&lt;/h4&gt;提出KAIROS框架，一种非自回归时间序列预测方法，直接对段级多峰分布进行建模，避免了自回归方法的误差累积，实现了及时推理，并改进了现有非自回归模型过度平滑的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模语料库上训练的KAIROS在六个广泛使用的基准测试上展示了强大的零样本泛化能力，其预测性能可与规模相当的最先进基础模型相媲美，但推理成本仅为这些模型的一小部分。&lt;h4&gt;结论&lt;/h4&gt;非自回归设计可作为基础模型在时间序列领域的一种重要可扩展范式，KAIROS框架证明了这一设计在保持高性能的同时显著降低计算成本的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在万维网中，可靠的时间序列预测为资源规划、缓存放置和异常响应提供前瞻性信号，使平台能够随着用户行为和内容分布的演变而高效运行。与其他领域相比，Web应用的时间序列预测需要更快的响应速度以支持实时决策。我们提出了KAIROS，一种非自回归时间序列预测框架，直接对段级多峰分布进行建模。与自回归方法不同，KAIROS避免了误差累积并实现了及时推理，同时改进了现有的会退化为过度平滑预测的非自回归模型。在大规模语料库上训练后，KAIROS在六个广泛使用的基准测试上展示了强大的零样本泛化能力，以远低于这些模型推理成本的价格，提供了与规模相当的最先进基础模型相当的预测性能。除了实证结果外，KAIROS还强调了非自回归设计作为基础模型在时间序列中可扩展范式的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the World Wide Web, reliable time series forecasts provide theforward-looking signals that drive resource planning, cache placement, andanomaly response, enabling platforms to operate efficiently as user behaviorand content distributions evolve. Compared with other domains, time seriesforecasting for Web applications requires much faster responsiveness to supportreal-time decision making. We present KAIROS, a non-autoregressive time seriesforecasting framework that directly models segment-level multi-peakdistributions. Unlike autoregressive approaches, KAIROS avoids erroraccumulation and achieves just-in-time inference, while improving over existingnon-autoregressive models that collapse to over-smoothed predictions. Trainedon the large-scale corpus, KAIROS demonstrates strong zero-shot generalizationon six widely used benchmarks, delivering forecasting performance comparable tostate-of-the-art foundation models with similar scale, at a fraction of theirinference cost. Beyond empirical results, KAIROS highlights the importance ofnon-autoregressive design as a scalable paradigm for foundation models in timeseries.</description>
      <author>example@mail.com (Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan)</author>
      <guid isPermaLink="false">2510.02084v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Foundation Models for Early Disease Detection</title>
      <link>http://arxiv.org/abs/2510.01899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态基础模型，通过基于注意力机制的transformer框架整合多样化医疗数据，用于早期疾病诊断。&lt;h4&gt;背景&lt;/h4&gt;医疗健康领域产生多种数据流（电子健康记录、医学影像、基因数据和可穿戴设备监测数据），传统诊断模型通常单独分析这些数据源，限制了识别跨模态关联的能力，而这些关联对早期疾病诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个多模态基础模型，整合多样化患者数据，用于提高早期疾病诊断的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于注意力机制的transformer框架的多模态基础模型。该模型首先通过专用编码器将每种模态数据转换到共享的潜在空间，然后使用多头注意力和残差归一化进行组合。该架构支持在多项任务上进行预训练，使其能够轻松适应新的疾病和数据集。研究还提供了一种实验策略，使用肿瘤学、心脏病学和神经病学基准数据集测试早期检测任务。&lt;h4&gt;主要发现&lt;/h4&gt;该框架包括数据治理和模型管理工具，提高了透明度、可靠性和临床可解释性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法旨在为精准诊断建立单一基础模型，可以提高预测准确性并帮助医生做出决策。&lt;h4&gt;翻译&lt;/h4&gt;医疗健康领域产生多样化的数据流，包括电子健康记录、医学影像、基因数据以及来自可穿戴设备的持续监测数据。传统的诊断模型经常单独分析这些数据源，这限制了它们识别跨模态关联的能力，而这些关联对早期疾病诊断至关重要。我们的研究提出了一个多模态基础模型，通过基于注意力的transformer框架整合多样化的患者数据。首先，专用编码器将每种模态数据转换到共享的潜在空间。然后，使用多头注意力和残差归一化将它们组合。该架构设计用于在多项任务上进行预训练，使其能够轻松适应新的疾病和数据集，只需额外少量工作。我们提供了一种实验策略，使用肿瘤学、心脏病学和神经病学基准数据集，以测试早期检测任务。除了技术性能外，该框架还包括数据治理和模型管理工具，以提高透明度、可靠性和临床可解释性。所提出的方法旨在为精准诊断建立一个单一基础模型，这可以提高预测准确性并帮助医生做出决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare generates diverse streams of data, including electronic healthrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearabledevices. Traditional diagnostic models frequently analyze these sources inisolation, which constrains their capacity to identify cross-modal correlationsessential for early disease diagnosis. Our research presents a multimodalfoundation model that consolidates diverse patient data through anattention-based transformer framework. At first, dedicated encoders put eachmodality into a shared latent space. Then, they combine them using multi-headattention and residual normalization. The architecture is made for pretrainingon many tasks, which makes it easy to adapt to new diseases and datasets withlittle extra work. We provide an experimental strategy that uses benchmarkdatasets in oncology, cardiology, and neurology, with the goal of testing earlydetection tasks. The framework includes data governance and model managementtools in addition to technological performance to improve transparency,reliability, and clinical interpretability. The suggested method works toward asingle foundation model for precision diagnostics, which could improve theaccuracy of predictions and help doctors make decisions.</description>
      <author>example@mail.com (Md Talha Mohsin, Ismail Abdulrashid)</author>
      <guid isPermaLink="false">2510.01899v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>AI Foundation Model for Time Series with Innovations Representation</title>
      <link>http://arxiv.org/abs/2510.01560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于创新表示的生成式预训练转换器（TS-GPT），专门用于工程应用中的时间序列分析，解决了基于大型语言模型的AI基础模型在遵循物理规律而非语言规律的时间序列数据中的局限性。&lt;h4&gt;背景&lt;/h4&gt;工程时间序列由物理规律而非语言规律支配，因此大型语言模型为基础的AI基础模型在工程应用中可能效果不佳或效率不高。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对工程时间序列的AI基础模型，用于需要因果操作的实时监控和控制任务。&lt;h4&gt;方法&lt;/h4&gt;基于Wiener、Kallianpur和Rosenblatt的经典创新表示理论，提出时间序列GPT（TS-GPT）——一种基于创新表示的生成式预训练转换器，并采用概率生成预测方法，从条件概率分布中生成未来时间序列样本。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用美国独立系统运营商的历史数据进行实时位置边际价格预测，证明了TS-GPT的有效性。&lt;h4&gt;结论&lt;/h4&gt;TS-GPT作为一种专门为工程时间序列设计的基础模型，能够有效处理物理规律支配的数据，为工程监控和控制任务提供可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种用于工程应用中时间序列的人工智能基础模型，在实时监控和控制中需要因果操作。由于工程时间序列遵循物理规律而非语言规律，基于大型语言模型的AI基础模型可能效果不佳或效率不高。基于Wiener、Kallianpur和Rosenblatt的经典创新表示理论，我们提出了时间序列GPT（TS-GPT）——一种基于创新表示的生成式预训练转换器，用于工程监控和控制。作为基础模型适应的例子，我们考虑了概率生成预测，它根据给定的过去实现，从条件概率分布中生成未来时间序列样本。我们通过使用美国独立系统运营商的历史数据进行实时位置边际价格预测，证明了TS-GPT的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an Artificial Intelligence (AI) foundation model fortime series in engineering applications, where causal operations are requiredfor real-time monitoring and control. Since engineering time series aregoverned by physical, rather than linguistic, laws, large-language-model-basedAI foundation models may be ineffective or inefficient. Building on theclassical innovations representation theory of Wiener, Kallianpur, andRosenblatt, we propose Time Series GPT (TS-GPT) -- aninnovations-representation-based Generative Pre-trained Transformer forengineering monitoring and control. As an example of foundation modeladaptation, we consider Probabilistic Generative Forecasting, which producesfuture time series samples from conditional probability distributions givenpast realizations. We demonstrate the effectiveness of TS-GPT in forecastingreal-time locational marginal prices using historical data from U.S.independent system operators.</description>
      <author>example@mail.com (Lang Tong, Xinyi Wang)</author>
      <guid isPermaLink="false">2510.01560v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs</title>
      <link>http://arxiv.org/abs/2510.01527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为往返强化学习(RTRL)的新框架，用于提高大型语言模型在计算化学任务中的往返一致性，实验证明该方法能有效提升模型性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正在成为计算化学的多功能基础模型，能够处理反应预测和逆合成分析等双向任务，但这些模型通常缺乏往返一致性，即无法从自身生成的文本中准确重建原始分子结构。&lt;h4&gt;目的&lt;/h4&gt;将往返一致性重新定义为模型改进的直接目标，开发一种能够提高模型一致性的训练方法。&lt;h4&gt;方法&lt;/h4&gt;提出往返强化学习(RTRL)框架，使用往返变换的成功作为奖励信号来训练模型；进一步提出迭代变体，让前向和反向映射在自我改进循环中交替训练，这种方法对数据效率很高，特别适用于化学中常见的海量未标记数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，RTRL在监督、自监督和合成数据方案中都显著提高了性能和一致性，证明往返一致性不仅是一个理想属性，而且是一个可训练的目标。&lt;h4&gt;结论&lt;/h4&gt;往返一致性为构建更强大可靠的基础模型提供了一条新路径，通过将一致性作为训练目标可以显著提升模型表现。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)正在成为计算化学的多功能基础模型，能够处理反应预测和逆合成分析等双向任务。然而，这些模型通常缺乏往返一致性。例如，最先进的化学LLM可能能够成功描述一个分子，但无法从其自身生成的文本中准确重建原始结构。这种不一致性表明模型正在学习单向记忆而非灵活掌握。确实，最近的研究已经证明模型的往返一致性与其主要任务表现之间存在强相关性。这种强相关性将一致性重新定义为模型改进的直接目标。因此，我们引入了往返强化学习(RTRL)，这是一个新框架，通过使用往返变换的成功作为奖励信号来训练模型以提高其一致性。我们进一步提出了一种迭代变体，其中前向和反向映射在自我改进循环中交替训练，这个过程对数据效率很高，并且在化学中常见的海量未标记数据上特别有效。实验证明，RTRL在监督、自监督和合成数据方案中都显著提高了性能和一致性。这项工作表明，往返一致性不仅是一个理想属性，而且是一个可训练的目标，为构建更强大可靠的基础模型提供了新路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are emerging as versatile foundation models forcomputational chemistry, handling bidirectional tasks like reaction predictionand retrosynthesis. However, these models often lack round-trip consistency.For instance, a state-of-the-art chemical LLM may successfully caption amolecule, yet be unable to accurately reconstruct the original structure fromits own generated text. This inconsistency suggests that models are learningunidirectional memorization rather than flexible mastery. Indeed, recent workhas demonstrated a strong correlation between a model's round-trip consistencyand its performance on the primary tasks. This strong correlation reframesconsistency into a direct target for model improvement. We therefore introduceRound-Trip Reinforcement Learning (RTRL), a novel framework that trains a modelto improve its consistency by using the success of a round-trip transformationas a reward signal. We further propose an iterative variant where forward andreverse mappings alternately train each other in a self-improvement loop, aprocess that is highly data-efficient and notably effective with the massiveamount of unlabelled data common in chemistry. Experiments demonstrate thatRTRL significantly \textbf{boosts performance and consistency} over strongbaselines across supervised, self-supervised, and synthetic data regimes. Thiswork shows that round-trip consistency is not just a desirable property but atrainable objective, offering a new path toward more robust and reliablefoundation models.</description>
      <author>example@mail.com (Lecheng Kong, Xiyuan Wang, Yixin Chen, Muhan Zhang)</author>
      <guid isPermaLink="false">2510.01527v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.01521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CarbonX，一个利用时间序列基础模型(TSFMs)进行多种脱碳任务的开源工具，能够仅使用历史碳强度数据在全球范围内提供准确的碳强度预测，克服了现有工具的局限性。&lt;h4&gt;背景&lt;/h4&gt;计算脱碳旨在减少计算系统和社会系统（如数据中心、交通和建筑环境）的碳排放，但现有工具存在三大局限性：需要特定电网的电力组合数据、依赖单独的特定电网模型难以提供全球覆盖、以及提供的预测没有不确定性估计。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够克服现有工具局限性的碳强度预测工具，实现全球范围内的准确碳强度预测，为下游碳感知应用提供可靠支持。&lt;h4&gt;方法&lt;/h4&gt;提出CarbonX工具，利用时间序列基础模型(TSFMs)的多功能性，仅使用历史碳强度数据和单个通用模型进行碳强度预测和插补任务。&lt;h4&gt;主要发现&lt;/h4&gt;CarbonX在全球214个电网中实现了15.82%的零样本预测平均绝对百分比误差；在13个基准电网上的性能与当前最先进技术相当，平均MAPE为9.59%，尾部预测MAPE为16.54%，并提供95%覆盖率的预测区间；可提供长达21天的预测，精度下降最小；完全微调后，在插补任务上优于统计基线1.2-3.9倍。&lt;h4&gt;结论&lt;/h4&gt;CarbonX可以在有限数据的情况下轻松应用于任何电网并仍能提供强大性能，是全球规模脱碳的实用工具。&lt;h4&gt;翻译&lt;/h4&gt;计算脱碳旨在减少计算系统和社会系统（如数据中心、交通和建筑环境）中的碳排放。这需要准确、细粒度的碳强度预测，然而现有工具有几个关键局限性：(i)它们需要特定电网的电力组合数据，限制了在无法获取此类信息的地方的使用；(ii)它们依赖于单独的特定电网模型，难以提供全球覆盖；以及(iii)它们提供的预测没有不确定性估计，限制了下游碳感知应用的可靠性。在本文中，我们提出了CarbonX，一个利用时间序列基础模型(TSFMs)进行多种脱碳任务的开源工具。CarbonX利用TSFMs的多功能性，在多种任务（如碳强度预测和插补）和不同电网间提供强大性能。仅使用历史碳强度数据和单个通用模型，我们的工具在全球214个电网中实现了15.82%的零样本预测平均绝对百分比误差。在13个基准电网中，CarbonX的性能与当前最先进技术相当，平均MAPE为9.59%，尾部预测MAPE为16.54%，同时还提供95%覆盖率的预测区间。CarbonX可以提供长达21天的预测，精度下降最小。此外，当完全微调后，CarbonX在插补任务上优于统计基线1.2-3.9倍。总体而言，这些结果表明CarbonX可以在有限数据的情况下轻松应用于任何电网并仍能提供强大性能，是全球规模脱碳的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational decarbonization aims to reduce carbon emissions in computingand societal systems such as data centers, transportation, and builtenvironments. This requires accurate, fine-grained carbon intensity forecasts,yet existing tools have several key limitations: (i) they require grid-specificelectricity mix data, restricting use where such information is unavailable;(ii) they depend on separate grid-specific models that make it challenging toprovide global coverage; and (iii) they provide forecasts without uncertaintyestimates, limiting reliability for downstream carbon-aware applications.  In this paper, we present CarbonX, an open-source tool that leverages TimeSeries Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonXutilizes the versatility of TSFMs to provide strong performance across multipletasks, such as carbon intensity forecasting and imputation, and across diversegrids. Using only historical carbon intensity data and a single general model,our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonXperformance is comparable with the current state-of-the-art, with an averageMAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providingprediction intervals with 95% coverage. CarbonX can provide forecasts for up to21 days with minimal accuracy degradation. Further, when fully fine-tuned,CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputationtask. Overall, these results demonstrate that CarbonX can be used easily on anygrid with limited data and still deliver strong performance, making it apractical tool for global-scale decarbonization.</description>
      <author>example@mail.com (Diptyaroop Maji, Kang Yang, Prashant Shenoy, Ramesh K Sitaraman, Mani Srivastava)</author>
      <guid isPermaLink="false">2510.01521v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Flock: A Knowledge Graph Foundation Model via Learning on Random Walks</title>
      <link>http://arxiv.org/abs/2510.01510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Flock，一种基于概率节点-关系等变性的知识图谱基础模型，用于解决零次链接预测问题，在多个知识图谱上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;知识图谱基础模型(KGFMs)通过强制节点和关系的等变性来解决零次链接预测问题，但传统确定性等变性限制了模型的表达能力，无法区分结构相似但语义不同的关系。&lt;h4&gt;目的&lt;/h4&gt;克服传统确定性等变性的内在限制，引入概率节点-关系等变性，提高知识图谱基础模型的表达能力，使其能够区分结构相似但语义不同的关系。&lt;h4&gt;方法&lt;/h4&gt;提出Flock模型，它迭代采样随机游走，通过记录协议将它们编码为序列，使用序列模型嵌入，并通过学习的池化聚合节点和关系的表示。Flock尊重概率节点-关系等变性，是知识图谱上同构不变链接级函数的通用逼近器。&lt;h4&gt;主要发现&lt;/h4&gt;Flock完美解决了当前KGFMs失败的新诊断数据集Petals，并在54个不同领域的知识图谱上的实体和关系预测任务上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;概率节点-关系等变性可以有效克服传统确定性等变性的限制，提高知识图谱基础模型的表达能力和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了知识图谱上的零次链接预测问题，这要求模型能够泛化到新的实体和新的关系。知识图谱基础模型通过强制节点和关系的等变性来解决这一任务，学习节点和关系的结构特性，然后将其转移到具有相似结构特性的新图谱中。然而，传统确定性等变性的概念对知识图谱基础模型的表达能力有内在限制，使它们无法区分结构相似但语义不同的关系。为了克服这一限制，我们引入了概率节点-关系等变性，在保持分布等变性的同时，引入了合理的随机化来打破推理过程中的对称性。基于这一原理，我们提出了Flock，一种知识图谱基础模型，它通过迭代采样随机游走，通过记录协议将它们编码为序列，使用序列模型嵌入它们，并通过学习的池化聚合节点和关系的表示。关键是，Flock尊重概率节点-关系等变性，并且是知识图谱上同构不变链接级函数的通用逼近器。实验上，Flock完美解决了当前知识图谱基础模型失败的新诊断数据集Petals，并在54个不同领域的知识图谱上的实体和关系预测任务上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the problem of zero-shot link prediction on knowledge graphs (KGs),which requires models to generalize over novel entities and novel relations.Knowledge graph foundation models (KGFMs) address this task by enforcingequivariance over both nodes and relations, learning from structural propertiesof nodes and relations, which are then transferable to novel graphs withsimilar structural properties. However, the conventional notion ofdeterministic equivariance imposes inherent limits on the expressive power ofKGFMs, preventing them from distinguishing structurally similar butsemantically distinct relations. To overcome this limitation, we introduceprobabilistic node-relation equivariance, which preserves equivariance indistribution while incorporating a principled randomization to break symmetriesduring inference. Building on this principle, we present Flock, a KGFM thatiteratively samples random walks, encodes them into sequences via a recordingprotocol, embeds them with a sequence model, and aggregates representations ofnodes and relations via learned pooling. Crucially, Flock respectsprobabilistic node-relation equivariance and is a universal approximator forisomorphism-invariant link-level functions over KGs. Empirically, Flockperfectly solves our new diagnostic dataset Petals where current KGFMs fail,and achieves state-of-the-art performances on entity- and relation predictiontasks on 54 KGs from diverse domains.</description>
      <author>example@mail.com (Jinwoo Kim, Xingyue Huang, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail İlkan Ceylan)</author>
      <guid isPermaLink="false">2510.01510v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2510.01428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BIOVERSE是一种两阶段方法，通过预训练的生物医学基础模型作为模态编码器，并使用轻量级的模态特定投影层将它们与大型语言模型对齐，实现跨模态生物医学推理。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和生物医学基础模型在生物文本推理、分子建模和单细胞分析方面取得了显著成果，但它们仍然被隔离在分离的嵌入空间中，限制了跨模态推理能力。&lt;h4&gt;目的&lt;/h4&gt;将生物医学数据与嵌入在大型语言模型中的知识统一起来，实现零样本标注、跨模态问答和交互式可解释对话。&lt;h4&gt;方法&lt;/h4&gt;BIOVERSE采用两阶段方法：首先将每个模态通过独立训练的投影对齐到共享的大型语言模型空间，使它们能够自然互操作；然后使用多模态数据应用标准指令调优，将它们结合用于下游推理。&lt;h4&gt;主要发现&lt;/h4&gt;在细胞类型注释、分子描述和蛋白质功能推理等跨任务中，紧凑的BIOVERSE配置超过了更大的大型语言模型基线，同时能够生成比现有生物医学基础模型更丰富、更具生成性的输出。&lt;h4&gt;结论&lt;/h4&gt;BIOVERSE为有原则的多模态生物医学推理奠定了基础，通过统一原始生物医学数据与大型语言模型中嵌入的知识，实现了跨模态推理能力。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型和生物医学基础模型的最新进展已在生物文本推理、分子建模和单细胞分析方面取得了显著成果，但它们仍然被隔离在分离的嵌入空间中，限制了跨模态推理能力。我们提出了BIOVERSE（生物医学向量嵌入重新对齐以实现语义参与），这是一种两阶段方法，它将预训练的生物医学基础模型作为模态编码器，并通过轻量级的模态特定投影层将它们与大型语言模型对齐。该方法首先通过独立训练的投影将每个模态对齐到共享的大型语言模型空间，使它们能够自然互操作，然后使用多模态数据应用标准指令调优，将它们结合用于下游推理。通过将原始生物医学数据与嵌入在大型语言模型中的知识统一起来，该方法实现了零样本标注、跨模态问答和交互式可解释对话。在跨越细胞类型注释、分子描述和蛋白质功能推理的任务中，紧凑的BIOVERSE配置超过了更大的大型语言模型基线，同时能够生成比现有生物医学基础模型更丰富、更具生成性的输出，为有原则的多模态生物医学推理奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large language models (LLMs) and biomedical foundationmodels (BioFMs) have achieved strong results in biological text reasoning,molecular modeling, and single-cell analysis, yet they remain siloed indisjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE(Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stageapproach that adapts pretrained BioFMs as modality encoders and aligns themwith LLMs through lightweight, modality-specific projection layers. Theapproach first aligns each modality to a shared LLM space through independentlytrained projections, allowing them to interoperate naturally, and then appliesstandard instruction tuning with multi-modal data to bring them together fordownstream reasoning. By unifying raw biomedical data with knowledge embeddedin LLMs, the approach enables zero-shot annotation, cross-modal questionanswering, and interactive, explainable dialogue. Across tasks spanningcell-type annotation, molecular description, and protein function reasoning,compact BIOVERSE configurations surpass larger LLM baselines while enablingricher, generative outputs than existing BioFMs, establishing a foundation forprincipled multi-modal biomedical reasoning.</description>
      <author>example@mail.com (Ching-Huei Tsou, Michal Ozery-Flato, Ella Barkan, Diwakar Mahajan, Ben Shapira)</author>
      <guid isPermaLink="false">2510.01428v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation</title>
      <link>http://arxiv.org/abs/2510.01388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VENTURA是一个视觉-语言导航系统，通过微调互联网预训练的图像扩散模型进行路径规划，将自然语言指令转化为多样化的机器人行为，在真实世界评估中表现出色。&lt;h4&gt;背景&lt;/h4&gt;机器人必须适应多样化的人类指令并在非结构化的开放世界环境中安全操作。近期的视觉-语言模型(VLMs)为语言和感知提供了强大的先验知识，但由于动作空间和预训练目标的差异，这些模型难以用于导航任务，限制了其在机器人任务中的可转移性。&lt;h4&gt;目的&lt;/h4&gt;解决VLMs在机器人导航中的局限性，开发一个能够处理自然语言指令并生成多样化机器人行为的系统。&lt;h4&gt;方法&lt;/h4&gt;引入VENTURA，微调互联网预训练的图像扩散模型进行路径规划，在图像空间生成路径掩码(视觉计划)而非直接预测低级行动，使用轻量级行为克隆策略将这些视觉计划转化为可执行的轨迹，通过自监督跟踪模型与VLM增强的标题相结合生成的路径掩码进行监督训练，避免了手动像素级注释。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界评估中，VENTURA在物体抓取、障碍物避让和地形偏好任务上优于最先进的基础模型基线，在已见和未见的场景中，成功率提高了33%，碰撞减少了54%，并且能够推广到未见过的不同任务的组合，展现出组合能力。&lt;h4&gt;结论&lt;/h4&gt;VENTURA是一个有效的视觉-语言导航系统，通过微调图像扩散模型并使用路径掩码作为中间表示，系统能够将自然语言指令转化为多样化的机器人行为，并在多种任务上表现出色，具有泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人必须适应多样化的人类指令并在非结构化的开放世界环境中安全操作。近期的视觉-语言模型(VLMs)为语言和感知提供了强大的先验知识，但由于动作空间和预训练目标的差异，这些模型难以用于导航任务，限制了其在机器人任务中的可转移性。为此，我们引入了VENTURA，一个视觉-语言导航系统，它微调了互联网预训练的图像扩散模型进行路径规划。VENTURA不在图像空间直接预测低级行动，而是生成路径掩码(即视觉计划)，捕捉细粒度的、具有上下文感知的导航行为。轻量级行为克隆策略将这些视觉计划转化为可执行的轨迹，形成一个遵循自然语言指令生成多样化机器人行为的界面。为扩展训练，我们使用自监督跟踪模型与VLM增强的标题相结合衍生的路径掩码进行监督，避免了手动像素级注释或高度工程化的数据收集设置。在广泛的真实世界评估中，VENTURA在物体抓取、障碍物避让和地形偏好任务上优于最先进的基础模型基线，在已见和未见的场景中，成功率提高了33%，碰撞减少了54%。值得注意的是，我们发现VENTURA能够推广到未见过的不同任务的组合，展现出组合能力。视频、代码和额外材料：https://venturapath.github.io&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人如何理解和执行多样化语言指令在非结构化环境中进行安全导航的问题。这个问题在现实中非常重要，因为机器人需要在建筑检查、城市维护和配送等场景中适应不断变化的人类偏好和环境上下文，而现有视觉-语言导航系统难以将复杂语言指令（如'保持与儿童的安全距离'）精确转化为机器人运动，限制了机器人在开放世界中的适应性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有视觉-语言模型的局限性，认识到需要一种能将互联网规模先验知识转化为精确导航规划的方法。他们借鉴了多个现有工作：利用预训练图像扩散模型（如Stable Diffusion）的强大视觉生成能力；采用现成的点跟踪技术（Co-Tracker）自动生成路径标签；使用轻量级行为克隆策略将视觉计划转换为动作；并借鉴了图像扩散模型中的分类器自由引导训练方法。作者设计了两阶段训练流程，首先训练扩散模型生成路径掩码，然后训练策略网络将其转换为可执行动作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; VENTURA的核心思想是将导航规划问题重新表述为图像生成问题，利用预训练扩散模型根据语言指令生成路径掩码（视觉计划），再通过轻量级策略转换为机器人动作。整体流程：1)接收相机图像和语言指令；2)使用预训练编码器提取特征；3)扩散模型逐步去噪生成路径掩码；4)ResNet-34结合当前观测和路径掩码预测xyz路径点；5)转换为机器人可执行动作。训练分两阶段：先训练扩散模型生成路径掩码，再训练策略网络将掩码转换为动作。数据收集方面，使用点跟踪技术从演示视频中自动生成路径标签，并用VLM生成多样化语言描述增强数据集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将互联网预训练的图像扩散模型首次用于多任务路径规划；2)引入路径掩码作为视觉计划表示，捕获细粒度导航行为；3)设计可扩展的自动标注管道，从非结构化演示中生成高质量路径标签；4)采用混合训练策略，结合任务无关和任务导向演示。相比之前工作，VENTURA能处理更复杂语言指令而非仅定位目标；生成更精细视觉计划而非粗略规划；不需要机器人里程计监督，更具可扩展性；直接在图像空间规划完整轨迹而非生成子目标或直接合成动作，支持多样化语言条件任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VENTURA创新性地将互联网预训练的图像扩散模型适应用于机器人导航，通过生成语言条件化的视觉计划并转换为精确动作，显著提升了机器人在开放环境中理解和执行多样化语言指令的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots must adapt to diverse human instructions and operate safely inunstructured, open-world environments. Recent Vision-Language models (VLMs)offer strong priors for grounding language and perception, but remain difficultto steer for navigation due to differences in action spaces and pretrainingobjectives that hamper transferability to robotics tasks. Towards addressingthis, we introduce VENTURA, a vision-language navigation system that finetunesinternet-pretrained image diffusion models for path planning. Instead ofdirectly predicting low-level actions, VENTURA generates a path mask (i.e. avisual plan) in image space that captures fine-grained, context-awarenavigation behaviors. A lightweight behavior-cloning policy grounds thesevisual plans into executable trajectories, yielding an interface that followsnatural language instructions to generate diverse robot behaviors. To scaletraining, we supervise on path masks derived from self-supervised trackingmodels paired with VLM-augmented captions, avoiding manual pixel-levelannotation or highly engineered data collection setups. In extensive real-worldevaluations, VENTURA outperforms state-of-the-art foundation model baselines onobject reaching, obstacle avoidance, and terrain preference tasks, improvingsuccess rates by 33% and reducing collisions by 54% across both seen and unseenscenarios. Notably, we find that VENTURA generalizes to unseen combinations ofdistinct tasks, revealing emergent compositional capabilities. Videos, code,and additional materials: https://venturapath.github.io</description>
      <author>example@mail.com (Arthur Zhang, Xiangyun Meng, Luca Calliari, Dong-Ki Kim, Shayegan Omidshafiei, Joydeep Biswas, Ali Agha, Amirreza Shaban)</author>
      <guid isPermaLink="false">2510.01388v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs</title>
      <link>http://arxiv.org/abs/2510.01370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为Small PDE U-Net Solver (SPUS)的紧凑高效基础模型，作为统一的神经算子用于求解广泛的偏微分方程。相比基于大型Transformer架构的现有方法，SPUS采用轻量级残差U-Net架构，并通过自回归预训练策略学习底层物理规律。&lt;h4&gt;背景&lt;/h4&gt;现有的PDE基础模型主要基于大型复杂Transformer架构，具有高计算和参数开销。残差U-Net架构作为基础模型架构在PDE求解领域尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种紧凑高效的神经算子基础模型，用于求解广泛的偏微分方程，同时减少参数需求和计算开销。&lt;h4&gt;方法&lt;/h4&gt;提出基于残差U-Net架构的轻量级SPUS模型；使用简单而强大的自回归预训练策略模拟数值求解器行为；在多样化流体动力学PDEs上预训练；在6个具有挑战性的未见过的下游PDEs上评估。&lt;h4&gt;主要发现&lt;/h4&gt;SPUS在下游任务上实现了最先进的泛化能力；需要显著更少的参数；只需要最少的微调数据；作为高度参数效率的PDE求解基础模型具有巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;轻量级残差U-Net架构可作为PDE求解的基础模型，在保持高性能的同时显著减少参数需求和计算开销，为PDE求解提供了高效的新方法。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了小型PDE U-Net求解器（SPUS），这是一种紧凑高效的基础模型（FM），被设计为用于求解广泛偏微分方程（PDE）的统一神经算子。与现有的最先进的PDE基础模型（主要基于具有高计算和参数开销的大型复杂Transformer架构）不同，SPUS利用了一种轻量级的残差U-Net架构，该架构在这一领域作为基础模型架构在很大程度上尚未得到探索。为了在这个最小化框架中实现有效学习，我们使用了一种简单而强大的自回归预训练策略，该策略紧密复制了数值求解器的行为以学习底层物理规律。SPUS在多样化的流体动力学PDE集上进行了预训练，并在跨越各种物理系统的6个具有挑战性的未见过的下游PDE上进行了评估。实验结果表明，使用基于残差U-Net架构的SPUS在这些下游任务上实现了最先进的泛化能力，同时需要显著更少的参数和最少的微调数据，突显了其作为求解多样化PDE系统的高度参数效率的FM的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Small PDE U-Net Solver (SPUS), a compact and efficientfoundation model (FM) designed as a unified neural operator for solving a widerange of partial differential equations (PDEs). Unlike existingstate-of-the-art PDE FMs-primarily based on large complex transformerarchitectures with high computational and parameter overhead-SPUS leverages alightweight residual U-Net-based architecture that has been largelyunderexplored as a foundation model architecture in this domain. To enableeffective learning in this minimalist framework, we utilize a simple yetpowerful auto-regressive pretraining strategy which closely replicates thebehavior of numerical solvers to learn the underlying physics. SPUS ispretrained on a diverse set of fluid dynamics PDEs and evaluated across 6challenging unseen downstream PDEs spanning various physical systems.Experimental results demonstrate that SPUS using residual U-Net basedarchitecture achieves state-of-the-art generalization on these downstream taskswhile requiring significantly fewer parameters and minimal fine-tuning data,highlighting its potential as a highly parameter-efficient FM for solvingdiverse PDE systems.</description>
      <author>example@mail.com (Abu Bucker Siddik, Diane Oyen, Alexander Most, Michal Kucer, Ayan Biswas)</author>
      <guid isPermaLink="false">2510.01370v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving</title>
      <link>http://arxiv.org/abs/2510.00919v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 (Findings)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了检索增强生成（RAG）基础模型在解决奥林匹克级别物理问题方面的能力，提出了PhoPile多模态数据集，并对多种检索增强的基础模型进行了基准测试，结果表明检索与物理语料库的集成可以提升模型性能。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成（RAG）与基础模型已在多种任务上展现出强大性能，但在专家级推理能力（如解决奥林匹克级别的物理问题）方面的应用仍 largely unexplored。受学生通过复习往届问题准备竞赛的启发，研究者探索了RAG增强基础模型物理推理能力的可能性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索RAG技术能否增强基础模型在奥林匹克级别物理问题上的推理能力，并系统研究基于检索的物理推理方法。&lt;h4&gt;方法&lt;/h4&gt;研究者引入了PhoPile，一个专门为奥林匹克级别物理设计的高质量多模态数据集，包含图表、图形和方程，捕捉了物理问题解决的多模态本质。使用PhoPile，他们对多种检索增强的基础模型进行了基准测试，包括大型语言模型（LLMs）和大视觉语言模型（LMMs），并使用了多种检索器。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，将检索与物理语料库集成可以提高模型在奥林匹克级别物理问题上的性能，同时也指出了需要进一步研究的挑战。&lt;h4&gt;结论&lt;/h4&gt;检索增强生成技术有潜力提升基础模型在物理推理方面的能力，特别是在解决专家级别物理问题上，为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成（RAG）与基础模型已在各种任务上取得强大性能，但它们在专家级推理方面的能力——如解决奥林匹克级别的物理问题——在很大程度上尚未被探索。受学生通过复习往届问题准备竞赛的方式启发，我们研究了RAG增强基础模型物理推理能力的潜力。我们引入了PhoPile，一个专门为奥林匹克级别物理设计的高质量多模态数据集，使基于检索的推理能够得到系统研究。PhoPile包含图表、图形和方程，捕捉了物理问题解决的内在多模态特性。使用PhoPile，我们对检索增强的基础模型进行了基准测试，涵盖了使用多种检索器的大型语言模型（LLMs）和大视觉语言模型（LMMs）。我们的结果表明，将检索与物理语料库集成可以改进模型性能，同时也指出了激励检索增强物理推理进一步研究的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation (RAG) with foundation models has achievedstrong performance across diverse tasks, but their capacity for expert-levelreasoning-such as solving Olympiad-level physics problems-remains largelyunexplored. Inspired by the way students prepare for competitions by reviewingpast problems, we investigate the potential of RAG to enhance physics reasoningin foundation models. We introduce PhoPile, a high-quality multimodal datasetspecifically designed for Olympiad-level physics, enabling systematic study ofretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,capturing the inherently multimodal nature of physics problem solving. UsingPhoPile, we benchmark RAG-augmented foundation models, covering both largelanguage models (LLMs) and large multimodal models (LMMs) with multipleretrievers. Our results demonstrate that integrating retrieval with physicscorpora can improve model performance, while also highlighting challenges thatmotivate further research in retrieval-augmented physics reasoning.</description>
      <author>example@mail.com (Shunfeng Zheng, Yudi Zhang, Meng Fang, Zihan Zhang, Zhitan Wu, Mykola Pechenizkiy, Ling Chen)</author>
      <guid isPermaLink="false">2510.00919v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging</title>
      <link>http://arxiv.org/abs/2510.01298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MorphGen是一种先进的基于扩散的生成模型，用于荧光显微镜，能够在多种细胞类型和干预条件下生成高质量图像，保留细胞器特定细节，支持细粒度形态分析，并且与真实图像具有生物一致性。&lt;h4&gt;背景&lt;/h4&gt;计算模拟细胞对干预的反应是加速基于高含量图像的筛选的有前途的方向，这对药物发现和基因编辑的推进至关重要。&lt;h4&gt;目的&lt;/h4&gt;介绍MorphGen，一种能够在多种细胞类型和干预条件下进行可控生成的荧光显微镜生成模型。&lt;h4&gt;方法&lt;/h4&gt;MorphGen使用对齐损失进行训练，使其表示与OpenPhenom的表型嵌入相匹配，以捕获有生物学意义的模式。与之前将多通道染色压缩为RGB图像的方法不同，MorphGen联合生成完整的荧光通道集，保留每个细胞器的结构。&lt;h4&gt;主要发现&lt;/h4&gt;通过CellProfiler特征证明与真实图像的生物一致性，MorphGen的FID分数比之前最先进的MorphoDiff低35%以上，后者仅生成单一细胞类型的RGB图像。&lt;h4&gt;结论&lt;/h4&gt;MorphGen是一种先进的生成模型，能够在多种细胞类型和干预条件下生成高质量的荧光显微镜图像，保留细胞器特定细节，支持细粒度形态分析，并且与真实图像具有生物一致性。&lt;h4&gt;翻译&lt;/h4&gt;计算模拟细胞对干预的反应是加速基于高含量图像的筛选的一个有前途的方向，对推进药物发现和基因编辑至关重要。为此，我们介绍了MorphGen，这是一种最先进的基于扩散的生成模型，用于荧光显微镜，能够在多种细胞类型和干预条件下进行可控生成。为了捕获与已知细胞形态一致的有生物学意义的模式，MorphGen使用对齐损失进行训练，使其表示与OpenPhenom（一种最先进的生物基础模型）的表型嵌入相匹配。与之前将多通道染色压缩为RGB图像（从而牺牲细胞器特定细节）的方法不同，MorphGen联合生成完整的荧光通道集，保留每个细胞器的结构，并支持对生物解释至关重要的细粒度形态分析。我们通过CellProfiler特征证明了与真实图像的生物一致性，并且MorphGen的FID分数比之前最先进的MorphoDiff低35%以上，后者仅生成单一细胞类型的RGB图像。代码可在https://github.com/czi-ai/MorphGen获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating in silico cellular responses to interventions is a promisingdirection to accelerate high-content image-based assays, critical for advancingdrug discovery and gene editing. To support this, we introduce MorphGen, astate-of-the-art diffusion-based generative model for fluorescent microscopythat enables controllable generation across multiple cell types andperturbations. To capture biologically meaningful patterns consistent withknown cellular morphologies, MorphGen is trained with an alignment loss tomatch its representations to the phenotypic embeddings of OpenPhenom, astate-of-the-art biological foundation model. Unlike prior approaches thatcompress multichannel stains into RGB images -- thus sacrificingorganelle-specific detail -- MorphGen generates the complete set of fluorescentchannels jointly, preserving per-organelle structures and enabling afine-grained morphological analysis that is essential for biologicalinterpretation. We demonstrate biological consistency with real images viaCellProfiler features, and MorphGen attains an FID score over $35\%$ lower thanthe prior state-of-the-art MorphoDiff, which only generates RGB images for asingle cell type. Code is available at https://github.com/czi-ai/MorphGen.</description>
      <author>example@mail.com (Berker Demirel, Marco Fumero, Theofanis Karaletsos, Francesco Locatello)</author>
      <guid isPermaLink="false">2510.01298v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Can World Models Benefit VLMs for World Dynamics?</title>
      <link>http://arxiv.org/abs/2510.00855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://dyva-worldlm.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新型视觉语言模型架构，称为世界语言模型（WorldLMs），通过将视频扩散模型作为生成编码器，显著提升了视觉推理能力，特别是在空间推理和多帧任务上。&lt;h4&gt;背景&lt;/h4&gt;在互联网规模视频数据上训练的世界模型能够生成一致且合理的动态，这引发了它们是否可能取代传统视觉编码器范式的问题。然而，现有研究缺乏对世界模型在通用多模态任务上的系统探索。&lt;h4&gt;目的&lt;/h4&gt;研究世界模型先验知识转移到视觉语言模型时的能力，探索生成编码器在下游视觉理解任务中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;将视频扩散模型重新用作生成编码器，执行单步去噪，并将得到的潜变量作为视觉嵌入。提出了一种称为动态视觉对齐器（DyVA）的最佳变体，并通过一系列视觉推理任务进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 生成编码器可以捕获对下游理解有用的潜变量，这些潜变量与传统编码器有所不同；2. DyVA方法显著增强了空间推理能力，使单图像模型能够执行多帧推理；3. DyVA超越了开源和专有基线，达到了最先进或可比的性能；4. 这些改进归因于WorldLM从视频预训练中继承的运动一致性内化。&lt;h4&gt;结论&lt;/h4&gt;研究为利用世界模型先验的新一代视觉语言模型铺平了道路，这些模型朝着通用视觉学习者的方向发展，具有广阔的前景。&lt;h4&gt;翻译&lt;/h4&gt;在互联网规模的视频数据上训练生成的世界模型正日益被视为强大的世界模拟器，能够在结构、运动和物理方面生成一致且合理的动态。这自然引发了一个问题：随着强大的视频基础模型的兴起，它们是否会取代传统的视觉编码器范式，用于通用多模态理解？尽管最近的研究开始探索世界模型在常见视觉任务上的潜力，但这些探索通常缺乏对通用多模态任务的系统研究。在这项工作中，我们努力研究将世界模型先验知识转移到视觉语言模型时的能力：我们将视频扩散模型重新用作生成编码器，执行单步去噪，并将得到的潜变量视为一组视觉嵌入。我们经验性地研究了这类模型，称之为世界语言模型（WorldLMs），发现生成编码器可以捕获对下游理解有用的潜变量，这些潜变量与传统编码器有所不同。我们将性能最佳的变体命名为动态视觉对齐器（DyVA），进一步发现这种方法显著增强了空间推理能力，并使单图像模型能够执行多帧推理。通过一系列视觉推理任务的整理，我们发现DyVA超越了开源和专有基线，达到了最先进或可比的性能。我们将这些改进归因于WorldLM从视频预训练中继承的运动一致性内化。最后，我们系统性地探索了广泛的模型设计，以突出未来工作的有前途方向。我们希望我们的研究能够为利用世界模型先验的新一代VLMs铺平道路，并朝着通用视觉学习者的有希望前进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要探讨世界模型（特别是视频生成模型）能否增强视觉语言模型（VLMs）对世界动态的理解能力。这个问题很重要，因为当前主流VLMs依赖静态图像编码器（如CLIP），在处理动态场景、空间推理和时序理解方面存在局限，而世界模型通过大规模视频数据训练，能够理解和预测物体的运动、空间布局和物理规律，如果能将这种动态理解能力整合到VLMs中，将大大增强模型对现实世界动态场景的理解和推理能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到世界模型具有强大的动态建模能力，能够生成连贯、合理的未来场景预测，思考这种生成能力是否也意味着对视觉动态的语义理解，并假设这种理解可以迁移到其他任务中。他们设计将视频扩散模型（Stable Video Diffusion）重新用作生成编码器，通过单步去噪提取动态特征，并与静态语义特征融合。该方法借鉴了VLMs的基本架构、视频扩散模型的U-Net网络以及Prismatic-VLMs的单阶段训练策略，但创新性地将生成模型作为编码器使用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用世界模型的动态理解能力提取包含动态信息的视觉特征，并将其与静态语义特征融合，使VLMs既能理解静态内容，又能理解动态变化和空间关系。整体流程包括：1) 使用SigLIP提取图像语义特征，同时用SVD进行单步去噪提取动态特征；2) 将两种特征通过投影层映射到语言模型嵌入空间并融合；3) 将融合特征与文本提示拼接输入语言模型生成输出；4) 采用单阶段训练策略训练投影层和语言模型；5) 推理时处理单图像或多图像输入，融合静态和动态特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 范式转变，从静态描述转向动态预测；2) 实现零样本多帧适应，仅用单图像训练获得多帧推理能力；3) 提出动态视觉对齐器（DyVA）架构，有效融合静态和动态特征；4) 建立系统研究框架，从范式比较、基准诊断到设计空间探索全面研究。相比之前的工作，本文创新性地使用生成编码器而非静态编码器，融合动态和静态特征而非仅使用静态特征，训练效率更高（仅需10.3小时），且显著增强了空间推理和多帧理解能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种新的World-Language Model架构，通过融合世界模型的动态特征与传统静态视觉特征，显著增强了视觉语言模型对空间关系和动态场景的理解与推理能力，实现了仅用单图像训练即可获得多帧推理的突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trained on internet-scale video data, generative world models areincreasingly recognized as powerful world simulators that can generateconsistent and plausible dynamics over structure, motion, and physics. Thisraises a natural question: with the advent of strong video foundational models,might they supplant conventional vision encoder paradigms for general-purposemultimodal understanding? While recent studies have begun to explore thepotential of world models on common vision tasks, these explorations typicallylack a systematic investigation of generic, multimodal tasks. In this work, westrive to investigate the capabilities when world model priors are transferredinto Vision-Language Models: we re-purpose a video diffusion model as agenerative encoder to perform a single denoising step and treat the resultinglatents as a set of visual embedding. We empirically investigate this class ofmodels, which we refer to as World-Language Models (WorldLMs), and we find thatgenerative encoders can capture latents useful for downstream understandingthat show distinctions from conventional encoders. Naming our best-performingvariant Dynamic Vision Aligner (DyVA), we further discover that this methodsignificantly enhances spatial reasoning abilities and enables single-imagemodels to perform multi-frame reasoning. Through the curation of a suite ofvisual reasoning tasks, we find DyVA to surpass both open-source andproprietary baselines, achieving state-of-the-art or comparable performance. Weattribute these gains to WorldLM's inherited motion-consistency internalizationfrom video pre-training. Finally, we systematically explore extensive modeldesigns to highlight promising directions for future work. We hope our studycan pave the way for a new family of VLMs that leverage priors from worldmodels and are on a promising path towards generalist vision learners.</description>
      <author>example@mail.com (Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, Shanghang Zhang)</author>
      <guid isPermaLink="false">2510.00855v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Are Time Series Foundation Models Susceptible to Catastrophic Forgetting?</title>
      <link>http://arxiv.org/abs/2510.00809v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了时间序列基础模型(TSFMs)在持续适应过程中的鲁棒性问题，特别关注了灾难性遗忘现象。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)已在多种预测任务中展现出有前途的零样本泛化能力，但它们对持续适应的鲁棒性尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;调查TSFMs在连续微调多个数据集时是否会遭受灾难性遗忘，并衡量适应新数据与保留先前知识之间的权衡。&lt;h4&gt;方法&lt;/h4&gt;使用具有不同程度周期结构的合成数据集，通过实验测量模型在新任务上的适应能力与对先前知识的保留程度。&lt;h4&gt;主要发现&lt;/h4&gt;虽然微调提高了TSFMs在新任务上的性能，但它常常导致先前学习任务的性能显著下降，体现了基本的稳定性-可塑性困境。&lt;h4&gt;结论&lt;/h4&gt;TSFMs在持续适应过程中存在稳定性-可塑性困境，需要在适应新数据和保留旧知识之间找到平衡。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)已在多样化的预测任务中展现出有前途的零样本泛化能力。然而，它们对持续适应的鲁棒性仍处于探索不足的状态。在本工作中，我们研究了TSFMs在连续微调多个数据集时遭受灾难性遗忘的程度。使用具有不同程度周期结构的合成数据集，我们衡量了适应新数据与保留先前知识之间的权衡。我们的实验揭示，虽然微调提高了在新任务上的性能，但它常常导致先前学习任务的显著性能下降，说明了基本的稳定性-可塑性困境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have shown promising zero-shotgeneralization across diverse forecasting tasks. However, their robustness tocontinual adaptation remains underexplored. In this work, we investigate theextent to which TSFMs suffer from catastrophic forgetting when fine-tunedsequentially on multiple datasets. Using synthetic datasets designed withvarying degrees of periodic structure, we measure the trade-off betweenadaptation to new data and retention of prior knowledge. Our experiments revealthat, while fine-tuning improves performance on new tasks, it often causessignificant degradation on previously learned ones, illustrating a fundamentalstability-plasticity dilemma.</description>
      <author>example@mail.com (Nouha Karaouli, Denis Coquenet, Elisa Fromont, Martial Mermillod, Marina Reyboz)</author>
      <guid isPermaLink="false">2510.00809v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models</title>
      <link>http://arxiv.org/abs/2510.00797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SF-SPA框架，通过计算机视觉和人工智能技术自动将街景照片转化为光伏部署的定量评估，解决了城市建筑立面光伏潜力评估的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市建筑立面是密集城市环境中太阳能发电的重要未开发资源，但由于复杂几何形状和语义组件，评估其光伏潜力具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;引入SF-SPA（语义立面光伏评估）自动化框架，将街景照片转换为光伏部署的定量评估，提高评估效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;结合计算机视觉和人工智能技术，采用四阶段流程：几何校正、零样本语义分割、大型语言模型引导的空间推理和能量模拟，解决透视失真校正、立面元素语义理解和光伏布局优化三个关键挑战。&lt;h4&gt;主要发现&lt;/h4&gt;在四个国家的80栋建筑上验证，平均面积估计误差为6.2%±2.8%，每栋建筑评估约需100秒，效率远高于手动方法；模拟发电量预测证实了该方法在区域潜力研究、城市能源规划和建筑一体化光伏部署中的可靠性和适用性。&lt;h4&gt;结论&lt;/h4&gt;SF-SPA框架能够有效评估城市建筑立面的光伏潜力，为城市能源规划和光伏部署提供支持。&lt;h4&gt;翻译&lt;/h4&gt;建筑立面代表了密集城市环境中太阳能发电的重要未开发资源，但由于复杂几何形状和语义组件，评估其光伏潜力仍然具有挑战性。本研究引入了SF-SPA（语义立面光伏评估），这是一个自动化框架，将街景照片转换为光伏部署的定量评估。该方法结合计算机视觉和人工智能技术，解决三个关键挑战：透视失真校正、立面元素的语义理解和光伏布局优化的空间推理。我们的四阶段流程通过几何校正、零样本语义分割、大型语言模型引导的空间推理和能量模拟来处理图像。在四个国家的80栋建筑上的验证表明，与专家注释相比，平均面积估计误差为6.2%±2.8%，性能稳健。自动化评估每栋建筑约需100秒，效率比手动方法大幅提高。模拟的发电量预测证实了该方法在区域潜力研究、城市能源规划和建筑一体化光伏部署中的可靠性和适用性。代码可在https://github.com/CodeAXu/Solar-PV-Installation获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决建筑物立面太阳能光伏安装潜力的评估问题。这个问题很重要，因为在密集城市环境中，建筑物立面是未被充分利用的太阳能资源，其面积往往超过屋顶面积的3-5倍。传统方法主要关注屋顶安装，忽略立面潜力会导致对城市总可再生能源容量的严重低估，同时立面评估面临复杂几何形状、语义组件和透视失真等独特挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：屋顶导向方法忽略立面潜力，整体建筑评估方法将立面简化为均匀表面，现有方法需要昂贵的3D数据或专门训练的模型。基于这些局限，作者设计了结合几何校正、语义理解和空间推理的解决方案。方法借鉴了计算机视觉和人工智能技术，利用了现有基础模型如Grounding-DINO和Segment Anything Model，以及光伏性能模拟工具pvlib，但创新性地将它们组合用于立面光伏评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将街道级别建筑物立面照片自动转换为可量化的光伏部署评估，结合计算机视觉和人工智能技术解决透视失真校正、立面元素语义理解和光伏布局优化三个关键挑战。整体流程分为四个阶段：1)立面图像获取和几何校正，使用语义关键点进行单应性变换校正透视失真并建立物理尺度；2)视觉基础模型的语义感知，使用零样本语义分割区分建筑元素；3)大型语言模型驱动的光伏布局推理，通过提示链将语义掩码转换为符合规范的布局；4)辐照度和能量模拟，使用pvlib估算年发电量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自动化立面光伏评估框架，从单个街道视图图像量化可安装面积并预测发电量；2)失真感知校正，使用语义关键点执行单应性变换校正透视失真；3)零样本立面解析，利用视觉-语言模型进行像素级分割无需专门训练数据；4)提示引导的空间推理，通过'描述→分区→过滤→总结'提示链引导大型语言模型生成实用光伏布局。相比之前工作，此方法不再依赖屋顶导向或简化立面模型，不需要昂贵3D数据，结合了最新基础模型，提供了完全自动化的从2D图像到精确光伏布局和能量估计的流程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的自动化框架SF-SPA，通过结合视觉-语言基础模型和大型语言模型，能够从单个街道级别建筑物立面照片中准确评估太阳能光伏安装潜力，为城市能源规划和建筑集成光伏系统部署提供了高效、可靠的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building facades represent a significant untapped resource for solar energygeneration in dense urban environments, yet assessing their photovoltaic (PV)potential remains challenging due to complex geometries and semantic components. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), anautomated framework that transforms street-view photographs into quantitativePV deployment assessments. The approach combines com puter vision andartificial intelligence techniques to address three key challenges: perspectivedistortion correction, semantic understanding of facade elements, and spatialreasoning for PV layout optimization. Our four-stage pipeline processes imagesthrough geometric rectification, zero-shot semantic segmentation, LargeLanguage Model (LLM) guided spatial reasoning, and energy simulation.Validation across 80 buildings in four countries demonstrates ro bustperformance with mean area estimation errors of 6.2% &amp;#177; 2.8% compared toexpert annotations. The auto mated assessment requires approximately 100seconds per building, a substantial gain in efficiency over manual methods.Simulated energy yield predictions confirm the method's reliability andapplicability for regional poten tial studies, urban energy planning, andbuilding-integrated photovoltaic (BIPV) deployment. Code is available at:https:github.com/CodeAXu/Solar-PV-Installation</description>
      <author>example@mail.com (Ruyu Liu, Dongxu Zhuang, Jianhua Zhang, Arega Getaneh Abate, Per Sieverts Nielsen, Ben Wang, Xiufeng Liu)</author>
      <guid isPermaLink="false">2510.00797v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>How Foundational are Foundation Models for Time Series Forecasting?</title>
      <link>http://arxiv.org/abs/2510.00742v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Workshop on Recent Advances in Time Series  Foundation Models (BERT2S)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了时间序列数据构建Foundation模型的适用性问题，指出时间序列数据的内在特性使其不适合作为Foundation模型的基础，并通过预测任务验证了这一观点。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models通常作为多功能嵌入机器设计，具有强大的零样本能力和在微调后的优越泛化性能，这在语言和视觉Foundation模型中表现尤为突出。&lt;h4&gt;目的&lt;/h4&gt;作者旨在论证时间序列数据的内在多样性使其不适合构建有效的Foundation模型，并通过预测任务来验证这一观点。&lt;h4&gt;方法&lt;/h4&gt;作者使用预测作为下游任务，评估时间序列Foundation模型的零样本能力和微调后的性能，并将其与针对特定预测任务定制的小型模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;时间序列Foundation模型的零样本能力受到其预训练领域的显著影响；当应用于未见过的真实世界时间序列数据时，微调后的Foundation模型并不比针对特定任务定制的小型模型一致地产生更好的结果，尽管参数数量和内存占用更大。&lt;h4&gt;结论&lt;/h4&gt;时间序列数据由于其内在多样性，不太适合构建有效的Foundation模型，至少在预测任务方面是这样。&lt;h4&gt;翻译&lt;/h4&gt;基础模型被设计为多功能的嵌入机器，具有强大的零样本能力和在多样化下游任务上微调后的优越泛化性能。虽然这在语言和视觉基础模型中很大程度上成立，但我们认为时间序列数据的内在多样性使得它们不太适合构建有效的基础模型。我们使用预测作为下游任务来证明这一点。我们展示了时间序列基础模型的零样本能力受到其预训练领域的显著影响和限制。此外，当应用于未见过的真实世界时间序列数据时，微调后的基础模型并没有比针对特定预测任务定制的小型模型一致地产生明显更好的结果，考虑到其增加的参数数量和内存占用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models are designed to serve as versatile embedding machines, withstrong zero shot capabilities and superior generalization performance whenfine-tuned on diverse downstream tasks. While this is largely true for languageand vision foundation models, we argue that the inherent diversity of timeseries data makes them less suited for building effective foundation models. Wedemonstrate this using forecasting as our downstream task. We show that thezero-shot capabilities of a time series foundation model are significantlyinfluenced and tied to the specific domains it has been pretrained on.Furthermore, when applied to unseen real-world time series data, fine-tunedfoundation models do not consistently yield substantially better results,relative to their increased parameter count and memory footprint, than smaller,dedicated models tailored to the specific forecasting task at hand.</description>
      <author>example@mail.com (Nouha Karaouli, Denis Coquenet, Elisa Fromont, Martial Mermillod, Marina Reyboz)</author>
      <guid isPermaLink="false">2510.00742v2</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Flexible Uncertainty Calibration for Machine-Learned Interatomic Potentials</title>
      <link>http://arxiv.org/abs/2510.00721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种灵活的不确定性校准框架，用于机器学习原子势能(MLIPs)中的不确定性量化，解决了现有共形预测技术在准确性、可扩展性和适应性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;可靠的不确定性量化(UQ)对开发用于预测原子模拟的机器学习原子势能至关重要。共形预测(CP)虽能提供覆盖保证，但现有技术常缺乏准确性、可扩展性及对原子环境复杂性的适应性。&lt;h4&gt;目的&lt;/h4&gt;为MLIPs开发一个灵活的不确定性校准框架，提高预测区间的准确性、可扩展性和适应性，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种受CP启发但重新表述为参数化优化问题的框架，直接学习环境相关的分位数函数，以最小计算成本产生更锐利和自适应的预测区间。使用MACE-MP-0基础模型在离子晶体、催化表面和分子系统等多种基准中进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;实现了不确定性-误差相关性数量级的改进，提高了主动学习中的数据效率，展现出强大的泛化性能，并能可靠地在不同交换关联泛函间转移校准的不确定性。&lt;h4&gt;结论&lt;/h4&gt;该工作为MLIPs中的不确定性校准建立了原则性和数据高效的方法，为更可靠和可转移的原子模拟提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;可靠的不确定性量化(UQ)对于开发用于预测原子模拟的机器学习原子势能(MLIPs)至关重要。共形预测(CP)是一种统计框架，能在最小假设下构建具有覆盖保证的预测区间，使其成为UQ的有吸引力的工具。然而，现有的CP技术虽然提供了正式的覆盖保证，但通常缺乏准确性、可扩展性以及对原子环境复杂性的适应性。在这项工作中，我们为MLIPs提出了一个灵活的不确定性校准框架，受CP启发但重新表述为参数化优化问题。这种表述能够直接学习环境相关的分位数函数，以最小的计算成本产生更锐利和自适应的预测区间。使用MACE-MP-0基础模型作为代表性案例，我们在离子晶体、催化表面和分子系统等多种基准中展示了该框架。我们的结果显示不确定性-误差相关性有数量级的改进，主动学习中的数据效率得到增强，同时具有强大的泛化性能，以及在不同的交换关联泛函之间可靠转移校准的不确定性。这项工作为MLIPs中的不确定性校准建立了一个原则性和数据高效的方法，为更可靠和可转移的原子模拟提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable uncertainty quantification (UQ) is essential for developingmachine-learned interatomic potentials (MLIPs) in predictive atomisticsimulations. Conformal prediction (CP) is a statistical framework thatconstructs prediction intervals with guaranteed coverage under minimalassumptions, making it an attractive tool for UQ. However, existing CPtechniques, while offering formal coverage guarantees, often lack accuracy,scalability, and adaptability to the complexity of atomic environments. In thiswork, we present a flexible uncertainty calibration framework for MLIPs,inspired by CP but reformulated as a parameterized optimization problem. Thisformulation enables the direct learning of environment-dependent quantilefunctions, producing sharper and more adaptive predictive intervals atnegligible computational cost. Using the foundation model MACE-MP-0 as arepresentative case, we demonstrate the framework across diverse benchmarks,including ionic crystals, catalytic surfaces, and molecular systems. Ourresults show order-of-magnitude improvements in uncertainty-error correlation,enhanced data efficiency in active learning, and strong generalizationperformance, together with reliable transfer of calibrated uncertainties acrossdistinct exchange-correlation functionals. This work establishes a principledand data-efficient approach to uncertainty calibration in MLIPs, providing apractical route toward more trustworthy and transferable atomistic simulations.</description>
      <author>example@mail.com (Cheuk Hin Ho, Christoph Ortner, Yangshuai Wang)</author>
      <guid isPermaLink="false">2510.00721v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>ProtoMask: Segmentation-Guided Prototype Learning</title>
      <link>http://arxiv.org/abs/2510.00683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ProtoMask的新型模型架构，利用图像分割基础模型提高可解释性，通过将注意力图计算限制在预定义的语义图像块上，减少可视化的不确定性。&lt;h4&gt;背景&lt;/h4&gt;XAI(可解释人工智能)近年来获得了相当大的重要性。基于原型案例推理的方法在可解释性方面显示出有希望的提升，但这些方法通常依赖于额外的后期显著性技术来解释学习到的原型的语义，而这些技术的可靠性和质量受到了多方批评。&lt;h4&gt;目的&lt;/h4&gt;研究使用突出的图像分割基础模型来提高嵌入空间与输入空间之间映射的真实性，旨在将显著性图的计算区域限制在预定义的语义图像块上，以减少此类可视化的不确定性。&lt;h4&gt;方法&lt;/h4&gt;使用每个生成的分割掩码的边界框来裁剪图像，以便感知整个图像的信息。每个掩码在名为ProtoMask的新型模型架构中产生一个单独的输入。&lt;h4&gt;主要发现&lt;/h4&gt;在三个流行的细粒度分类数据集上使用广泛的指标进行实验，提供了关于可解释性特征的详细概述。与其他流行模型的比较表明，该模型具有竞争性的性能和独特的可解释性特征。&lt;h4&gt;结论&lt;/h4&gt;ProtoMask模型通过利用图像分割基础模型改进了可解释性，特别是在减少可视化不确定性方面表现出色，同时保持了与其他模型相当的分类性能。&lt;h4&gt;翻译&lt;/h4&gt;XAI近年来获得了相当大的重要性。基于原型案例推理的方法在可解释性方面显示出有希望的提升。然而，这些方法通常依赖于额外的后期显著性技术来解释学习到的原型的语义。关于此类技术的可靠性和质量已有多方批评。因此，我们研究了使用突出的图像分割基础模型来提高嵌入空间与输入空间之间映射的真实性。我们旨在将显著性图的计算区域限制在预定义的语义图像块上，以减少此类可视化的不确定性。为了感知整个图像的信息，我们使用每个生成的分割掩码的边界框来裁剪图像。每个掩码在名为ProtoMask的新型模型架构中产生一个单独的输入。我们在三个流行的细粒度分类数据集上使用广泛的指标进行实验，提供了关于可解释性特征的详细概述。与其他流行模型的比较表明，该模型具有竞争性的性能和独特的可解释性特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; XAI gained considerable importance in recent years. Methods based onprototypical case-based reasoning have shown a promising improvement inexplainability. However, these methods typically rely on additional post-hocsaliency techniques to explain the semantics of learned prototypes. Multiplecritiques have been raised about the reliability and quality of suchtechniques. For this reason, we study the use of prominent image segmentationfoundation models to improve the truthfulness of the mapping between embeddingand input space. We aim to restrict the computation area of the saliency map toa predefined semantic image patch to reduce the uncertainty of suchvisualizations. To perceive the information of an entire image, we use thebounding box from each generated segmentation mask to crop the image. Each maskresults in an individual input in our novel model architecture named ProtoMask.We conduct experiments on three popular fine-grained classification datasetswith a wide set of metrics, providing a detailed overview on explainabilitycharacteristics. The comparison with other popular models demonstratescompetitive performance and unique explainability features of our model.https://github.com/uos-sis/quanproto</description>
      <author>example@mail.com (Steffen Meinert, Philipp Schlinge, Nils Strodthoff, Martin Atzmueller)</author>
      <guid isPermaLink="false">2510.00683v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>U-DFA: A Unified DINOv2-Unet with Dual Fusion Attention for Multi-Dataset Medical Segmentation</title>
      <link>http://arxiv.org/abs/2510.00585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为U-DFA的医学图像分割方法，通过整合DINOv2和Unet架构，并引入局部-全局融合适配器(LGFA)来有效融合局部和全局特征，在多个数据集上实现了最先进的性能，同时显著减少了可训练参数。&lt;h4&gt;背景&lt;/h4&gt;准确的医学图像分割在诊断中至关重要，但CNN模型存在局部感受野限制，无法捕获全局上下文；结合CNN和transformer的方法未能有效融合局部和全局特征；而现有的VLMs和基础模型虽可用于医学影像任务，但存在领域差距和高计算成本问题。&lt;h4&gt;目的&lt;/h4&gt;提出U-DFA架构，通过集成局部-全局融合适配器(LGFA)来增强医学图像分割性能，有效融合高级语义特征和空间特征。&lt;h4&gt;方法&lt;/h4&gt;U-DFA采用统一的DINOv2-Unet编码器-解码器架构，LGFA模块将基于CNN的空间模式适配器(SPA)的空间特征注入到多个阶段的冻结DINOv2块中，实现局部和全局特征的有效融合。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在Synapse和ACDC数据集上实现了最先进的性能，同时仅需33%的可训练模型参数，证明了其高效性。&lt;h4&gt;结论&lt;/h4&gt;U-DFA是跨多种模态的医学图像分割的稳健且可扩展的框架，能够在保持高性能的同时显著减少计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;准确的医学图像分割在整体诊断中起着至关重要的作用，是诊断流程中最关键的任务之一。尽管CNN-based模型被广泛使用，但它们存在局部感受野问题，无法捕获全局上下文。结合CNN和transformer的常见方法试图弥补这一差距，但未能有效融合局部和全局特征。随着最近VLMs和基础模型的出现，它们已被适应用于下游医学影像任务；然而，它们存在固有的领域差距和高计算成本。为此，我们提出了U-DFA，一个统一的DINOv2-Unet编码器-解码器架构，集成了新的局部-全局融合适配器(LGFA)以增强分割性能。LGFA模块将基于CNN的空间模式适配器(SPA)模块的空间特征注入到多个阶段的冻结DINOv2块中，实现了高级语义特征和空间特征的有效融合。我们的方法在Synapse和ACDC数据集上仅使用33%的可训练模型参数就实现了最先进的性能。这些结果表明，U-DFA是跨多种模态的医学图像分割的稳健且可扩展的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate medical image segmentation plays a crucial role in overall diagnosisand is one of the most essential tasks in the diagnostic pipeline. CNN-basedmodels, despite their extensive use, suffer from a local receptive field andfail to capture the global context. A common approach that combines CNNs withtransformers attempts to bridge this gap but fails to effectively fuse thelocal and global features. With the recent emergence of VLMs and foundationmodels, they have been adapted for downstream medical imaging tasks; however,they suffer from an inherent domain gap and high computational cost. To thisend, we propose U-DFA, a unified DINOv2-Unet encoder-decoder architecture thatintegrates a novel Local-Global Fusion Adapter (LGFA) to enhance segmentationperformance. LGFA modules inject spatial features from a CNN-based SpatialPattern Adapter (SPA) module into frozen DINOv2 blocks at multiple stages,enabling effective fusion of high-level semantic and spatial features. Ourmethod achieves state-of-the-art performance on the Synapse and ACDC datasetswith only 33\% of the trainable model parameters. These results demonstratethat U-DFA is a robust and scalable framework for medical image segmentationacross multiple modalities.</description>
      <author>example@mail.com (Zulkaif Sajjad, Furqan Shaukat, Junaid Mir)</author>
      <guid isPermaLink="false">2510.00585v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Assessing Foundation Models for Mold Colony Detection with Limited Training Data</title>
      <link>http://arxiv.org/abs/2510.00561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 figures, accepted as oral presentation at GCPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了如何使用少量标注数据训练视觉基础模型来量化培养皿上的霉菌菌落，替代了传统上需要大量标注数据的方法。&lt;h4&gt;背景&lt;/h4&gt;量化培养皿上的霉菌菌落对评估室内空气质量至关重要，高菌落数量可能表示潜在健康风险和通风系统缺陷。传统自动化方法依赖于大型数据集的手动标注和模型训练。&lt;h4&gt;目的&lt;/h4&gt;证明在处理新的视觉任务时，详尽的标注不再是先决条件，展示数据高效的基础模型如何匹配传统方法。&lt;h4&gt;方法&lt;/h4&gt;编译包含5000张培养皿图像的数据集，用边界框标注，模拟传统数据收集方法以及少样本和低样本场景，对比三种视觉基础模型与传统基线的性能。&lt;h4&gt;主要发现&lt;/h4&gt;MaskDINO模型在仅150张图像上微调后达到与 extensively 训练的YoloV9模型几乎相当的性能；即使使用仅25张图像，仍能在约70%的样本上保持可靠性能。&lt;h4&gt;结论&lt;/h4&gt;数据高效的基础模型只需传统方法所需数据的一小部分就能匹配传统方法，使自动化微生物系统能更早开发并更快迭代改进，且具有更高的上限性能。&lt;h4&gt;翻译&lt;/h4&gt;量化培养皿样本上霉菌菌落的过程对评估室内空气质量至关重要，因为高菌落数量可能表示潜在的健康风险和通风系统缺陷。传统上，这种劳动密集型过程的自动化以及微生物学中的其他任务依赖于大型数据集的手动标注和后续的模型训练（如YoloV9）。为了证明在处理新的视觉任务时详尽的标注不再是先决条件，我们编译了一个包含5000张培养皿图像的代表数据集，用边界框进行标注，模拟了传统数据收集方法以及少样本和低样本场景，具有精心策划的子集和实例级掩码。我们在反映现实世界实际需求的特定任务指标上，将三种视觉基础模型与传统基线进行对比。值得注意的是，MaskDINO在仅150张图像上微调后就达到了与 extensively 训练的YoloV9模型几乎相当的性能，即使使用仅25张图像仍能保持有竞争力的性能，在约70%的样本上仍然可靠。我们的结果表明，数据高效的基础模型只需传统方法所需数据的一小部分就能匹配传统方法，使自动化微生物系统能够更早开发并更快迭代改进，且具有比传统模型更高的上限性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The process of quantifying mold colonies on Petri dish samples is of criticalimportance for the assessment of indoor air quality, as high colony counts canindicate potential health risks and deficiencies in ventilation systems.Conventionally the automation of such a labor-intensive process, as well asother tasks in microbiology, relies on the manual annotation of large datasetsand the subsequent extensive training of models like YoloV9. To demonstratethat exhaustive annotation is not a prerequisite anymore when tackling a newvision task, we compile a representative dataset of 5000 Petri dish imagesannotated with bounding boxes, simulating both a traditional data collectionapproach as well as few-shot and low-shot scenarios with well curated subsetswith instance level masks. We benchmark three vision foundation models againsttraditional baselines on task specific metrics, reflecting realistic real-worldrequirements. Notably, MaskDINO attains near-parity with an extensively trainedYoloV9 model while finetuned only on 150 images, retaining competitiveperformance with as few as 25 images, still being reliable on $\approx$ 70% ofthe samples. Our results show that data-efficient foundation models can matchtraditional approaches with only a fraction of the required data, enablingearlier development and faster iterative improvement of automatedmicrobiological systems with a superior upper-bound performance thantraditional models would achieve.</description>
      <author>example@mail.com (Henrik Pichler, Janis Keuper, Matthew Copping)</author>
      <guid isPermaLink="false">2510.00561v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation Models</title>
      <link>http://arxiv.org/abs/2510.00487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了跨提示基础模型(CPFM)，用于解决黑盒时间序列领域适应(BBTSDA)问题，通过双分支网络结构和独特的提示设计，有效捕捉时间序列数据的时空特征，并在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;黑盒领域适应(BBDA)旨在解决隐私安全问题，其中只有源模型的API可用。现有研究多集中于视觉应用，不适用于具有独特时空特征的时间序列应用，且尚未探索基础模型在黑盒时间序列领域适应中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种针对黑盒时间序列领域适应(BBTSDA)问题的跨提示基础模型(CPFM)，构建能处理时间序列数据时空动态的基础模型。&lt;h4&gt;方法&lt;/h4&gt;CPFM采用双分支网络结构构建，每个分支配备独特提示以捕捉数据分布的不同特征；在领域适应阶段，开发提示级别和输入级别的重建学习阶段；所有构建都基于时间序列基础模型，以克服时空动态问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过严格实验证明CPFM的优势，在三个不同应用领域的时间序列数据集上，以明显优势优于竞争对手，取得改进结果。&lt;h4&gt;结论&lt;/h4&gt;CPFM是有效的黑盒时间序列领域适应方法，基础模型在处理时间序列数据的时空特征方面具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;黑盒领域适应(BBDA)主题的发展是为了解决隐私和安全问题，其中只有源模型的应用程序接口(API)可用于领域适应。尽管BBDA主题已引起越来越多的研究关注，但现有工作大多针对视觉应用，不适用于具有独特时空特征的时间序列应用。此外，现有方法中没有任何一种探索了基础模型在黑盒时间序列领域适应(BBTSDA)中的优势。本文提出了针对BBTSDA问题的跨提示基础模型(CPFM)概念。CPFM在双分支网络结构下构建，每个分支配备独特提示以捕捉数据分布的不同特征。在领域适应阶段，开发了提示级别和输入级别的重建学习阶段。所有这些都构建在时间序列基础模型之上，以克服时空动态。我们的严格实验证明了CPFM的优势，在三个不同应用领域的时间序列数据集上，以明显优势优于竞争对手，取得了改进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The black-box domain adaptation (BBDA) topic is developed to address theprivacy and security issues where only an application programming interface(API) of the source model is available for domain adaptations. Although theBBDA topic has attracted growing research attentions, existing works mostlytarget the vision applications and are not directly applicable to thetime-series applications possessing unique spatio-temporal characteristics. Inaddition, none of existing approaches have explored the strength of foundationmodel for black box time-series domain adaptation (BBTSDA). This paper proposesa concept of Cross-Prompt Foundation Model (CPFM) for the BBTSDA problems. CPFMis constructed under a dual branch network structure where each branch isequipped with a unique prompt to capture different characteristics of datadistributions. In the domain adaptation phase, the reconstruction learningphase in the prompt and input levels is developed. All of which are built upona time-series foundation model to overcome the spatio-temporal dynamic. Ourrigorous experiments substantiate the advantage of CPFM achieving improvedresults with noticeable margins from its competitors in three time-seriesdatasets of different application domains.</description>
      <author>example@mail.com (M. T. Furqon, Mahardhika Pratama, Igor Skrjanc, Lin Liu, Habibullah Habibullah, Kutluyil Dogancay)</author>
      <guid isPermaLink="false">2510.00487v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs</title>
      <link>http://arxiv.org/abs/2510.00419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ZO Fine-tuner的基于学习的零阶优化器，用于微调大型语言模型，通过自动学习高效的扰动策略，显著减少了GPU内存消耗，并可在不同下游任务间重用。&lt;h4&gt;背景&lt;/h4&gt;零阶优化器作为微调大型语言模型的实用方法，比传统一阶方法显著减少GPU内存消耗，但现有方法依赖手工制作的静态采样策略，无法适应特定模型结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种学习型的零阶优化器，能够自动学习高效的扰动策略，通过紧凑且内存高效的设计解决现有方法的局限性，并支持一次训练后跨任务重用。&lt;h4&gt;方法&lt;/h4&gt;基于只有少数基础模型及其变体在实践中被广泛采用的观察，设计ZO Fine-tuner将学习到学习(L2L)扩展到基础模型时代，支持每个LLM只需一次训练，开销最小。&lt;h4&gt;主要发现&lt;/h4&gt;在4个LLMs和7个数据集上的实验表明，ZO Fine-tuner在82.1%的任务-模型组合中优于先前的零阶基线，展示了高效LLMs微调的强大性能和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;ZO Fine-tuner是一种有效的零阶优化器，通过学习高效的扰动策略，比传统方法更节省内存，并可在不同下游任务间重用，提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;零阶优化器最近已成为微调大型语言模型(LLMs)的一种实用方法，与传统一阶方法相比显著减少了GPU内存消耗。然而，现有的零阶方法依赖于手工制作的静态采样策略，这些策略不能适应特定模型的结构。为此，我们提出了ZO Fine-tuner，一种基于学习的LLMs零阶优化器，通过紧凑且内存高效的设计自动学习高效的扰动策略。关键的是，我们的方法基于一个观察：在实践中只有少数基础模型及其变体被广泛采用。因此，为给定的LLM学习一次优化器并在各种下游任务中重用是可行且高度可取的。相应地，ZO Fine-tuner通过支持每个LLM只需一次训练且开销最小，将学习到学习(L2L)扩展到基础模型时代。在4个LLMs和7个数据集上的实验表明，ZO Fine-tuner在82.1%的任务-模型组合中优于先前的零阶基线，从而展示了高效LLMs微调的强大性能和可扩展性。我们的代码可在https://github.com/ASTRAL-Group/ZO_Fine_tuner.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zeroth-order optimizers have recently emerged as a practical approach forfine-tuning large language models (LLMs), significantly reducing GPU memoryconsumption compared to traditional first-order methods. Yet, existingzeroth-order methods rely on hand-crafted, static sampling strategies that arenot adaptable to model-specific structures. To address this, we propose ZOFine-tuner, a learning-based zeroth-order optimizer for LLMs that automaticallylearns efficient perturbation strategies through a compact and memory-efficientdesign. Crucially, our approach is motivated by the observation that only asmall number of foundation models and their derivatives are widely adopted inpractice. Therefore, learning the optimizer once for a given LLM and reusing itacross diverse downstream tasks is both feasible and highly desirable.Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to thefoundation-model era by supporting one-time training per LLM with minimaloverhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuneroutperforms prior zeroth-order baselines in 82.1\% of task-model combinations,thereby demonstrating strong performance and scalability for efficient LLMfine-tuning. Our code is available athttps://github.com/ASTRAL-Group/ZO_Fine_tuner.git.</description>
      <author>example@mail.com (Kairun Zhang, Haoyu Li, Yanjun Zhao, Yifan Sun, Huan Zhang)</author>
      <guid isPermaLink="false">2510.00419v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology Cases Unaddressed by Previous Foundation Models</title>
      <link>http://arxiv.org/abs/2510.01287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了最新AI细胞基础模型在肾脏病理学细胞核分割任务上的性能，通过比较多种模型和融合方法，发现最新模型显著提高了分割准确性，融合模型尤其有效，解决了大多数具有挑战性的案例。&lt;h4&gt;背景&lt;/h4&gt;准确的细胞核分割对肾脏病理学的下游任务至关重要，但由于肾脏组织的形态多样性和成像变异性，这仍然是一个重大挑战。之前的研究评估了早期一代的AI细胞基础模型，但最近细胞基础模型的有效性尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;基准测试先进的AI细胞基础模型（2025年），包括CellViT++变体和Cellpose-SAM，与2024年之前开发的三种广泛使用的细胞基础模型进行比较，评估它们在肾脏图像分割任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;使用包含人类参与循环评分框架的多样化大规模肾脏图像补丁集进行评估，进行基于融合的集成评估和模型一致性分析，使用2,091个具有挑战性的样本进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;CellViT++ [Virchow]在独立性能方面表现最佳，40.3%的预测被评为'好'；融合模型实现了62.2%的'好'预测和仅0.4%的'坏'预测，显著减少了分割错误；融合模型成功解决了之前研究中大多数未解决的具有挑战性案例。&lt;h4&gt;结论&lt;/h4&gt;这些发现证明了AI细胞基础模型在肾脏病理学中的开发潜力，提供了一个具有挑战性样本的精选数据集，以支持未来肾脏特定模型的改进。&lt;h4&gt;翻译&lt;/h4&gt;准确的细胞核分割对肾脏病理学的下游任务至关重要，并由于肾脏组织的形态多样性和成像变异性，仍然是一个重大挑战。虽然我们之前的工作已经评估了该领域中早期一代的AI细胞基础模型，但最近的细胞基础模型的有效性仍然不清楚。在这项研究中，我们使用包含人类参与循环评分框架的多样化大规模肾脏图像补丁集，基准测试了先进的AI细胞基础模型（2025年），包括CellViT++变体和Cellpose-SAM，与2024年之前开发的三种广泛使用的细胞基础模型进行比较。我们进一步进行了基于融合的集成评估和模型一致性分析，以评估不同模型的分割能力。我们的结果显示，CellViT++ [Virchow]在独立性能方面表现最佳，在2,091个精选的具有挑战性样本中，40.3%的预测被评为'好'，优于所有之前的模型。此外，我们的融合模型实现了62.2%的'好'预测和仅0.4%的'坏'预测，显著减少了分割错误。值得注意的是，融合模型（2025年）成功解决了我们之前研究中大多数未解决的具有挑战性案例。这些发现证明了AI细胞基础模型在肾脏病理学中的开发潜力，并提供了一个具有挑战性样本的精选数据集，以支持未来肾脏特定模型的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cell nuclei segmentation is critical for downstream tasks in kidneypathology and remains a major challenge due to the morphological diversity andimaging variability of renal tissues. While our prior work has evaluatedearly-generation AI cell foundation models in this domain, the effectiveness ofrecent cell foundation models remains unclear. In this study, we benchmarkadvanced AI cell foundation models (2025), including CellViT++ variants andCellpose-SAM, against three widely used cell foundation models developed priorto 2024, using a diverse large-scale set of kidney image patches within ahuman-in-the-loop rating framework. We further performed fusion-based ensembleevaluation and model agreement analysis to assess the segmentation capabilitiesof the different models. Our results show that CellViT++ [Virchow] yields thehighest standalone performance with 40.3% of predictions rated as "Good" on acurated set of 2,091 challenging samples, outperforming all prior models. Inaddition, our fused model achieves 62.2% "Good" predictions and only 0.4%"Bad", substantially reducing segmentation errors. Notably, the fusion model(2025) successfully resolved the majority of challenging cases that remainedunaddressed in our previous study. These findings demonstrate the potential ofAI cell foundation model development in renal pathology and provide a curateddataset of challenging samples to support future kidney-specific modelrefinement.</description>
      <author>example@mail.com (Runchen Wang, Junlin Guo, Siqi Lu, Ruining Deng, Zhengyi Lu, Yanfan Zhu, Yuechen Yang, Chongyu Qu, Yu Wang, Shilin Zhao, Catie Chang, Mitchell Wilkes, Mengmeng Yin, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2510.01287v1</guid>
      <pubDate>Fri, 03 Oct 2025 15:44:46 +0800</pubDate>
    </item>
    </channel>
</rss>