<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 22 May 2025 14:27:07 +0800</lastBuildDate>
    <item>
      <title>Policy Contrastive Decoding for Robotic Foundation Models</title>
      <link>http://arxiv.org/abs/2505.13255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCD（Policy Contrastive Decoding）的新方法，旨在提高机器人策略的泛化能力，通过对比原始视觉输入和对象掩码视觉输入导出的动作概率分布，使机器人策略更加关注与对象相关的视觉线索。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人策略在训练数据之外的学习泛化能力较弱，容易从预训练轨迹中学习到虚假的相关性。&lt;h4&gt;目的&lt;/h4&gt;提出PCD方法，解决现有机器人策略泛化能力不足的问题。&lt;h4&gt;方法&lt;/h4&gt;PCD方法通过对比原始和对象掩码的视觉输入导出的动作概率分布，引导机器人策略关注对象相关的视觉线索，且无需对模型进行微调或访问模型权重。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenVLA、Octo和π_0等开源机器人策略上进行的实验表明，PCD方法在模拟和真实世界环境中都提高了机器人策略的性能，例如在模拟环境中将π_0策略的性能提高了8%，在真实世界环境中提高了108%。&lt;h4&gt;结论&lt;/h4&gt;PCD方法是一种灵活且有效的机器人策略改进方法，可以显著提高机器人策略的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为PCD（策略对比解码）的新方法，旨在提高机器人策略的泛化能力。通过对比原始视觉输入和对象掩码视觉输入导出的动作概率分布，该方法使机器人策略更加关注与对象相关的视觉线索。实验结果表明，PCD方法在模拟和真实世界环境中都提高了机器人策略的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/Koorye/PCD&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic foundation models, or generalist robot policies, hold immensepotential to enable flexible, general-purpose and dexterous robotic systems.Despite their advancements, our empirical experiments reveal that existingrobot policies are prone to learning spurious correlations from pre-trainingtrajectories, adversely affecting their generalization capabilities beyond thetraining data. To tackle this, we propose a novel Policy Contrastive Decoding(PCD) approach, which redirects the robot policy's focus toward object-relevantvisual clues by contrasting action probability distributions derived fromoriginal and object-masked visual inputs. As a training-free method, our PCDcan be used as a plugin to improve different types of robot policies withoutneeding to finetune or access model weights. We conduct extensive experimentson top of three open-source robot policies, including the autoregressive policyOpenVLA and the diffusion-based policies Octo and $\pi_0$. The obtained resultsin both simulation and real-world environments prove PCD's flexibility andeffectiveness, e.g., PCD enhances the state-of-the-art policy $\pi_0$ by 8% inthe simulation environment and by 108% in the real-world environment. Code anddemos are publicly available at: https://Koorye.github.io/proj/PCD.</description>
      <author>example@mail.com (Shihan Wu, Ji Zhang, Xu Luo, Junlin Xie, Jingkuan Song, Heng Tao Shen, Lianli Gao)</author>
      <guid isPermaLink="false">2505.13255v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
  <item>
      <title>Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum</title>
      <link>http://arxiv.org/abs/2505.12191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需在推理或下游微调阶段使用去噪器的自监督学习框架，以实现噪声鲁棒的表示学习。&lt;h4&gt;背景&lt;/h4&gt;尽管自监督学习（SSL）在从无标签数据中提取丰富表示方面变得非常强大，但其研究主要集中在干净、精心整理和高质量的数据集上。&lt;h4&gt;目的&lt;/h4&gt;克服将SSL应用于噪声数据的挑战，这对于天体物理学、医学成像、地球物理学或金融等应用至关重要。&lt;h4&gt;方法&lt;/h4&gt;该方法首先在噪声数据上训练一个SSL去噪器，然后使用它来构建一个去噪到噪声数据课程（即先在去噪样本上训练，然后是噪声样本），以预训练SSL骨干（例如DINOv2），并结合教师引导的正则化，将噪声嵌入锚定到其去噪对应物。&lt;h4&gt;主要发现&lt;/h4&gt;去噪器可以在预训练后丢弃，简化部署。在ImageNet-1k上，使用ViT-B在极端高斯噪声（σ=255，信噪比SNR = 0.72 dB）下，该方法将DINOv2的线性探测精度提高了4.8%，证明了无去噪器的鲁棒性可以来自噪声感知预训练。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了通过噪声感知预训练实现无去噪器的鲁棒性，为噪声数据的自监督学习提供了一种有效途径。&lt;h4&gt;翻译&lt;/h4&gt;Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise (σ=255, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at https://github.com/wenquanlu/noisy_dinov2.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/wenquanlu/noisy_dinov2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-Supervised Learning (SSL) has become a powerful solution to extract richrepresentations from unlabeled data. Yet, SSL research is mostly focused onclean, curated and high-quality datasets. As a result, applying SSL on noisydata remains a challenge, despite being crucial to applications such asastrophysics, medical imaging, geophysics or finance. In this work, we presenta fully self-supervised framework that enables noise-robust representationlearning without requiring a denoiser at inference or downstream fine-tuning.Our method first trains an SSL denoiser on noisy data, then uses it toconstruct a denoised-to-noisy data curriculum (i.e., training first ondenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),combined with a teacher-guided regularization that anchors noisy embeddings totheir denoised counterparts. This process encourages the model to internalizenoise robustness. Notably, the denoiser can be discarded after pretraining,simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge fromnoise-aware pretraining. The code is available athttps://github.com/wenquanlu/noisy_dinov2.</description>
      <author>example@mail.com (Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, Randall Balestriero)</author>
      <guid isPermaLink="false">2505.12191v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models</title>
      <link>http://arxiv.org/abs/2505.14454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Our code is available at https://github.com/xuyang-liu16/VidCom2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VidCom2的视频压缩框架，用于解决视频大语言模型（VideoLLM）在视频理解中的效率问题。&lt;h4&gt;背景&lt;/h4&gt;VideoLLM在视频理解方面表现出色，但由于视觉标记的二次复杂度，其效率面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过分析标记压缩方法，旨在解决VideoLLM在压缩过程中的信息丢失和实施限制问题。&lt;h4&gt;方法&lt;/h4&gt;提出了三个设计原则，并设计了VidCom2框架，通过量化每帧的独特性，自适应调整压缩强度。&lt;h4&gt;主要发现&lt;/h4&gt;VidCom2在保持视频序列中关键信息的同时，减少了冗余，并在各种VideoLLM和基准测试中展现出优越的性能和效率。&lt;h4&gt;结论&lt;/h4&gt;VidCom2在仅使用25%视觉标记的情况下，实现了99.6%的原有性能，同时减少了70.8%的LLM生成延迟，且其帧压缩调整策略与其它标记压缩方法兼容。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a video compression framework called VidCom2 to address the efficiency challenges faced by VideoLLM in video understanding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/xuyang-liu16/vidcom2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (VideoLLM) excel at video understanding, but faceefficiency challenges due to the quadratic complexity of abundant visualtokens. Our systematic analysis of token compression methods for VideoLLMsreveals two critical issues: (i) overlooking distinctive visual signals acrossframes, leading to information loss; (ii) suffering from implementationconstraints, causing incompatibility with modern architectures or efficientoperators. To address these challenges, we distill three design principles forVideoLLM token compression and propose a plug-and-play inference accelerationframework "Video Compression Commander" (VidCom2). By quantifying each frame'suniqueness, VidCom2 adaptively adjusts compression intensity across frames,effectively preserving essential information while reducing redundancy in videosequences. Extensive experiments across various VideoLLMs and benchmarksdemonstrate the superior performance and efficiency of our VidCom2. With only25% visual tokens, VidCom2 achieves 99.6% of the original performance onLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our FrameCompression Adjustment strategy is compatible with other token compressionmethods to further improve their performance. Our code is available athttps://github.com/xuyang-liu16/VidCom2.</description>
      <author>example@mail.com (Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang)</author>
      <guid isPermaLink="false">2505.14454v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>On the creation of narrow AI: hierarchy and nonlocality of neural network skills</title>
      <link>http://arxiv.org/abs/2505.15811v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了创建强大但专一的AI系统的问题，探讨了神经网络学习特性和表示结构中的两个挑战。&lt;h4&gt;背景&lt;/h4&gt;虽然近期AI的进步主要依赖于大型通用基础模型的训练，但为特定领域创建小型专用模型可能在效率和安全性方面有价值。&lt;h4&gt;目的&lt;/h4&gt;研究创建强大而专一的AI系统的方法，并解决神经网络学习特性和表示结构中的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过实验，研究了从零开始训练窄模型的可能性，以及如何将特定技能从大型通用模型转移到小型专用模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 当技能之间存在层次依赖关系时，在广泛的数据分布上训练网络有时是必要的，这可以加速学习。2. 基于剪枝的方法在技能迁移方面优于知识蒸馏。3. 使用正则化目标可以在去除不必要的技能的同时，将所需技能与可剪枝组件对齐。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法可以有效地创建强大而专一的AI系统，并解决神经网络学习特性和表示结构中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the problem of creating strong, yet narrow, AI systems. While recentAI progress has been driven by the training of large general-purpose foundationmodels, the creation of smaller models specialized for narrow domains could bevaluable for both efficiency and safety. In this work, we explore twochallenges involved in creating such systems, having to do with basicproperties of how neural networks learn and structure their representations.The first challenge regards when it is possible to train narrow models fromscratch. Through experiments on a synthetic task, we find that it is sometimesnecessary to train networks on a wide distribution of data to learn certainnarrow skills within that distribution. This effect arises when skills dependon each other hierarchically, and training on a broad distribution introduces acurriculum which substantially accelerates learning. The second challengeregards how to transfer particular skills from large general models into smallspecialized models. We find that model skills are often not perfectly localizedto a particular set of prunable components. However, we find that methods basedon pruning can still outperform distillation. We investigate the use of aregularization objective to align desired skills with prunable components whileunlearning unnecessary skills.</description>
      <author>example@mail.com (Eric J. Michaud, Asher Parker-Sartori, Max Tegmark)</author>
      <guid isPermaLink="false">2505.15811v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2505.14866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://nisarganc.github.io/UPTor-page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种统一的方法来预测人类关键点动力学和运动轨迹，该方法基于输入姿势序列进行预测。&lt;h4&gt;背景&lt;/h4&gt;许多研究要么专注于全身姿态预测，要么专注于运动轨迹预测，但只有少数研究尝试将它们结合起来。&lt;h4&gt;目的&lt;/h4&gt;提出了一种运动变换技术，同时在一个全局坐标系中预测全身姿态和轨迹关键点。&lt;h4&gt;方法&lt;/h4&gt;使用了现成的3D人体姿态估计模块、图注意力网络来编码骨骼结构，以及适用于实时运动预测的紧凑、非自回归的变换器。&lt;h4&gt;主要发现&lt;/h4&gt;引入了人类导航数据集“DARKO”，重点关注对人类感知移动机器人导航相关的导航活动。&lt;h4&gt;结论&lt;/h4&gt;在Human3.6M、CMU-Mocap和DARKO数据集上进行了广泛的评估，结果显示该方法紧凑、实时且准确。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种统一的方法来预测人类关键点动力学和基于短序列输入姿态的运动轨迹。虽然许多研究关注全身体态预测或运动轨迹预测，但只有少数研究尝试将它们合并。我们提出了一种运动变换技术，以同时预测全局坐标系中的全身体态和轨迹关键点。我们利用了一个现成的3D人体姿态估计模块、一个图注意力网络来编码骨骼结构，以及一个适用于实时运动预测的紧凑、非自回归的变换器。我们引入了人类导航数据集“DARKO”，特别关注对人类感知移动机器人导航相关的导航活动。我们在Human3.6M、CMU-Mocap和我们的DARKO数据集上进行了广泛的评估。与先前的工作相比，我们的方法在所有数据集上预测人类导航运动时既紧凑又准确。结果动画、我们的数据集和代码可在https://nisarganc.github.io/UPTor-page/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a unified approach to forecast the dynamics of human keypointsalong with the motion trajectory based on a short sequence of input poses.While many studies address either full-body pose prediction or motiontrajectory prediction, only a few attempt to merge them. We propose a motiontransformation technique to simultaneously predict full-body pose andtrajectory key-points in a global coordinate frame. We utilize an off-the-shelf3D human pose estimation module, a graph attention network to encode theskeleton structure, and a compact, non-autoregressive transformer suitable forreal-time motion prediction for human-robot interaction and human-awarenavigation. We introduce a human navigation dataset ``DARKO'' with specificfocus on navigational activities that are relevant for human-aware mobile robotnavigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and ourDARKO dataset. In comparison to prior work, we show that our approach iscompact, real-time, and accurate in predicting human navigation motion acrossall datasets. Result animations, our dataset, and code will be available athttps://nisarganc.github.io/UPTor-page/</description>
      <author>example@mail.com (Nisarga Nilavadi, Andrey Rudenko, Timm Linder)</author>
      <guid isPermaLink="false">2505.14866v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning</title>
      <link>http://arxiv.org/abs/2505.15703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的运动预测框架 HAMF，用于自动驾驶系统中的场景上下文编码和未来运动状态预测。&lt;h4&gt;背景&lt;/h4&gt;运动预测是自动驾驶系统中的关键挑战，需要准确预测周围代理的未来轨迹。现有方法通过从历史代理轨迹和道路布局中提取的场景上下文特征来预测未来运动状态，但存在信息退化的问题。&lt;h4&gt;目的&lt;/h4&gt;提出 HAMF 框架以解决现有方法的限制，通过联合学习场景上下文编码和未来运动表示，以协调场景理解和未来运动状态预测。&lt;h4&gt;方法&lt;/h4&gt;首先将观察到的代理状态和地图信息嵌入到一维标记序列中，并将目标多模态未来运动特征作为一组可学习的标记。然后设计了一个统一的基于注意力的编码器，它协同结合自注意力和交叉注意力机制来建模场景上下文信息并聚合未来运动特征。在解码阶段，实现 Mamba 模块以进一步保持学习到的未来运动表示的一致性和相关性，生成准确且多样化的最终轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在 Argoverse 2 基准测试上，所提出的混合注意力-Mamba 模型实现了最先进的运动预测性能，同时具有简单轻量级的架构。&lt;h4&gt;结论&lt;/h4&gt;HAMF 框架通过联合学习和注意力机制，有效提高了运动预测的准确性，为自动驾驶系统提供了更可靠的预测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion forecasting represents a critical challenge in autonomous drivingsystems, requiring accurate prediction of surrounding agents' futuretrajectories. While existing approaches predict future motion states with theextracted scene context feature from historical agent trajectories and roadlayouts, they suffer from the information degradation during the scene featureencoding. To address the limitation, we propose HAMF, a novel motionforecasting framework that learns future motion representations with the scenecontext encoding jointly, to coherently combine the scene understanding andfuture motion state prediction. We first embed the observed agent states andmap information into 1D token sequences, together with the target multi-modalfuture motion features as a set of learnable tokens. Then we design a unifiedAttention-based encoder, which synergistically combines self-attention andcross-attention mechanisms to model the scene context information and aggregatefuture motion features jointly. Complementing the encoder, we implement theMamba module in the decoding stage to further preserve the consistency andcorrelations among the learned future motion representations, to generate theaccurate and diverse final trajectories. Extensive experiments on Argoverse 2benchmark demonstrate that our hybrid Attention-Mamba model achievesstate-of-the-art motion forecasting performance with the simple and lightweightarchitecture.</description>
      <author>example@mail.com (Xiaodong Mei, Sheng Wang, Jie Cheng, Yingbing Chen, Dan Xu)</author>
      <guid isPermaLink="false">2505.15703v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Clapper: Compact Learning and Video Representation in VLMs</title>
      <link>http://arxiv.org/abs/2505.15529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Clapper方法，该方法通过使用慢-快策略和TimePerceiver模块来优化视频语言模型（VLMs）对短长视频的理解能力。&lt;h4&gt;背景&lt;/h4&gt;当前VLMs在视频理解应用中表现出色，但设计这些模型需要有效地建模时间维度和平衡处理短长视频的需求。&lt;h4&gt;目的&lt;/h4&gt;提高VLMs对短长视频输入的有效建模能力。&lt;h4&gt;方法&lt;/h4&gt;提出Clapper方法，该方法使用慢-快策略对视频表示，并引入TimePerceiver模块，用于在现有VLM框架中进行高效的时空编码。&lt;h4&gt;主要发现&lt;/h4&gt;Clapper方法实现了每帧13倍的视频标记压缩（平均每帧61个标记）而不影响问答（QA）准确性。&lt;h4&gt;结论&lt;/h4&gt;在VideoMME、MLVU和TempCompass数据集上，Clapper分别达到了62.0%、69.8%和67.4%的准确率，每个视频使用少于6,000个视觉标记。&lt;h4&gt;翻译&lt;/h4&gt;Current vision-language models (VLMs) have demonstrated remarkable capabilities across diverse video understanding applications. Designing VLMs for video inputs requires effectively modeling the temporal dimension (i.e., capturing dependencies across frames) and balancing the processing of short and long videos. Specifically, short videos demand preservation of fine-grained details, whereas long videos require strategic compression of visual information to handle extensive temporal contexts efficiently. However, our empirical analysis reveals a critical limitation: most existing VLMs suffer severe performance degradation in long video understanding tasks when compressing visual tokens below a quarter of their original visual tokens. To enable more effective modeling of both short and long video inputs, we propose Clapper, a method that utilizes a slow-fast strategy for video representation and introduces a novel module named TimePerceiver for efficient temporal-spatial encoding within existing VLM backbones. By using our method, we achieve 13x compression of visual tokens per frame (averaging 61 tokens/frame) without compromising QA accuracy. In our experiments, Clapper achieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all with fewer than 6,000 visual tokens per video. The code will be publicly available on the homepage.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current vision-language models (VLMs) have demonstrated remarkablecapabilities across diverse video understanding applications. Designing VLMsfor video inputs requires effectively modeling the temporal dimension (i.e.capturing dependencies across frames) and balancing the processing of short andlong videos. Specifically, short videos demand preservation of fine-graineddetails, whereas long videos require strategic compression of visualinformation to handle extensive temporal contexts efficiently. However, ourempirical analysis reveals a critical limitation: most existing VLMs suffersevere performance degradation in long video understanding tasks whencompressing visual tokens below a quarter of their original visual tokens. Toenable more effective modeling of both short and long video inputs, we proposeClapper, a method that utilizes a slow-fast strategy for video representationand introduces a novel module named TimePerceiver for efficienttemporal-spatial encoding within existing VLM backbones. By using our method,we achieves 13x compression of visual tokens per frame (averaging 61tokens/frame) without compromising QA accuracy. In our experiments, Clapperachieves 62.0% on VideoMME, 69.8% on MLVU, and 67.4% on TempCompass, all withfewer than 6,000 visual tokens per video. The code will be publicly availableon the homepage.</description>
      <author>example@mail.com (Lingyu Kong, Hongzhi Zhang, Jingyuan Zhang, Jianzhao Huang, Kunze Li, Qi Wang, Fuzheng Zhang)</author>
      <guid isPermaLink="false">2505.15529v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2505.15528v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，人工智能在生成合成3D物体方面取得了显著进步，但在生成复杂3D物体，如植物方面仍存在较大挑战。&lt;h4&gt;背景&lt;/h4&gt;当前生成式3D模型在生成植物方面与生成一般物体相比存在困难，限制了其在需要精细细节和准确几何形状的植物分析工具中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出PlantDreamer，一种新的3D合成植物生成方法，以实现比现有文本到3D模型更高的复杂植物几何和纹理的真实感。&lt;h4&gt;方法&lt;/h4&gt;新的生成流程利用深度控制网络、微调的低秩适应和可适应的高斯剔除算法，直接提高生成3D植物模型的纹理真实性和几何完整性。PlantDreamer还通过利用L-系统生成的网格实现纯合成植物生成，并通过将现实世界植物点云转换为3D高斯Splats来增强它们。&lt;h4&gt;主要发现&lt;/h4&gt;通过与最先进的文本到3D模型进行比较，PlantDreamer在生成高保真合成植物方面优于现有方法。结果表明，该方法不仅推进了合成植物生成，还有助于升级旧点云数据集，使其成为3D表型应用的有价值工具。&lt;h4&gt;结论&lt;/h4&gt;PlantDreamer是一种有效的3D合成植物生成工具，能够提高植物分析工具的性能，并有助于3D表型应用的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，在利用人工智能生成合成3D物体方面取得了显著进展。然而，生成复杂的3D物体，如植物，仍然是一个相当大的挑战。与生成一般物体相比，当前的生成式3D模型在生成植物方面存在困难，这限制了它们在需要精细细节和准确几何形状的植物分析工具中的应用。我们引入了PlantDreamer，这是一种新的3D合成植物生成方法，它能够实现比现有的文本到3D模型更高的复杂植物几何和纹理的真实感。为了实现这一点，我们新的生成流程利用了深度控制网络、微调的低秩适应和可适应的高斯剔除算法，这些算法直接提高了生成3D植物模型的纹理真实性和几何完整性。此外，PlantDreamer通过利用L-系统生成的网格实现纯合成植物生成，并通过将现实世界植物点云转换为3D高斯Splats来增强它们。我们通过将其输出与最先进的文本到3D模型进行比较来评估我们的方法，结果表明PlantDreamer在生成高保真合成植物方面优于现有方法。我们的结果表明，我们的方法不仅推进了合成植物生成，还有助于升级旧点云数据集，使其成为3D表型应用的有价值工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen substantial improvements in the ability to generatesynthetic 3D objects using AI. However, generating complex 3D objects, such asplants, remains a considerable challenge. Current generative 3D models strugglewith plant generation compared to general objects, limiting their usability inplant analysis tools, which require fine detail and accurate geometry. Weintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,which can achieve greater levels of realism for complex plant geometry andtextures than available text-to-3D models. To achieve this, our new generationpipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and anadaptable Gaussian culling algorithm, which directly improve textural realismand geometric integrity of generated 3D plant models. Additionally,PlantDreamer enables both purely synthetic plant generation, by leveragingL-System-generated meshes, and the enhancement of real-world plant point cloudsby converting them into 3D Gaussian Splats. We evaluate our approach bycomparing its outputs with state-of-the-art text-to-3D models, demonstratingthat PlantDreamer outperforms existing methods in producing high-fidelitysynthetic plants. Our results indicate that our approach not only advancessynthetic plant generation, but also facilitates the upgrading of legacy pointcloud datasets, making it a valuable tool for 3D phenotyping applications.</description>
      <author>example@mail.com (Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound)</author>
      <guid isPermaLink="false">2505.15528v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>MMaDA: Multimodal Large Diffusion Language Models</title>
      <link>http://arxiv.org/abs/2505.15809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project: https://github.com/Gen-Verse/MMaDA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MMaDA是一种新型的多模态扩散基础模型，旨在实现跨多个领域如文本推理、多模态理解和文本到图像生成等领域的优越性能。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态模型在处理不同数据类型时存在整合和处理的困难。&lt;h4&gt;目的&lt;/h4&gt;设计MMaDA模型以实现跨领域的高性能。&lt;h4&gt;方法&lt;/h4&gt;MMaDA的关键创新包括：采用统一的扩散架构，实现不同数据类型之间的无缝整合；实施混合长思维链（CoT）微调策略，促进文本和视觉领域之间的推理过程对齐；提出UniGRPO，一种专门针对扩散基础模型的政策梯度强化学习算法。&lt;h4&gt;主要发现&lt;/h4&gt;MMaDA-8B作为统一的多模态基础模型，展现出强大的泛化能力，在文本推理、多模态理解和文本到图像生成方面均优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;MMaDA有效连接了预训练和后训练阶段，为未来的研究和开发提供了一个全面的框架。&lt;h4&gt;翻译&lt;/h4&gt;We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gen-verse/mmada&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MMaDA, a novel class of multimodal diffusion foundation modelsdesigned to achieve superior performance across diverse domains such as textualreasoning, multimodal understanding, and text-to-image generation. The approachis distinguished by three key innovations: (i) MMaDA adopts a unified diffusionarchitecture with a shared probabilistic formulation and a modality-agnosticdesign, eliminating the need for modality-specific components. Thisarchitecture ensures seamless integration and processing across different datatypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuningstrategy that curates a unified CoT format across modalities. By aligningreasoning processes between textual and visual domains, this strategyfacilitates cold-start training for the final reinforcement learning (RL)stage, thereby enhancing the model's ability to handle complex tasks from theoutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithmspecifically tailored for diffusion foundation models. Utilizing diversifiedreward modeling, UniGRPO unifies post-training across both reasoning andgeneration tasks, ensuring consistent performance improvements. Experimentalresults demonstrate that MMaDA-8B exhibits strong generalization capabilitiesas a unified multimodal foundation model. It surpasses powerful models likeLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X inmultimodal understanding, and excels over SDXL and Janus in text-to-imagegeneration. These achievements highlight MMaDA's effectiveness in bridging thegap between pretraining and post-training within unified diffusionarchitectures, providing a comprehensive framework for future research anddevelopment. We open-source our code and trained models at:https://github.com/Gen-Verse/MMaDA</description>
      <author>example@mail.com (Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang)</author>
      <guid isPermaLink="false">2505.15809v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Transfer of Structural Knowledge from Synthetic Languages</title>
      <link>http://arxiv.org/abs/2505.15769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures and 3 tables to be published in ACL 2025 Workshop  XLLM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了从多种合成语言到英语的迁移学习，分析了微调模型的嵌入结构、包含的信息和简单语言任务上的能力，并引入了一种新的合成语言，其在向英语迁移方面优于之前研究使用的语言。此外，还介绍了Tiny-Cloze基准，这是一个新的自然语言理解合成基准，对性能较弱的模型更有信息量。通过Tiny-Cloze基准评估了微调模型，证明在新的合成语言上进行微调可在多种任务上实现更好的性能。&lt;h4&gt;背景&lt;/h4&gt;迁移学习从多种合成语言到英语。&lt;h4&gt;目的&lt;/h4&gt;研究微调模型的嵌入结构、信息内容和能力，引入新的合成语言，并评估Tiny-Cloze基准。&lt;h4&gt;方法&lt;/h4&gt;分析微调模型的嵌入结构、信息内容和能力，引入新的合成语言，使用Tiny-Cloze基准进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;新的合成语言在向英语迁移方面表现优于之前使用的语言；在Tiny-Cloze基准上的评估显示，新的合成语言上的微调模型在多种任务上表现更好。&lt;h4&gt;结论&lt;/h4&gt;新的合成语言有助于提高从多种合成语言到英语的迁移学习效果；Tiny-Cloze基准为评估微调模型提供了有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;这项工作探讨了从多种合成语言到英语的迁移学习。我们研究了微调模型中的嵌入结构、它们包含的信息以及微调模型在简单语言任务上的能力。我们还引入了一种新的合成语言，它比先前研究中使用的语言在向英语迁移方面更好。最后，我们引入了Tiny-Cloze基准——一个针对自然语言理解的新合成基准，对于性能较弱的模型来说更有信息量。我们使用Tiny-Cloze基准评估了多个领域的微调模型，证明在新的合成语言上进行微调可以在各种任务上实现更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores transfer learning from several synthetic languages toEnglish. We investigate the structure of the embeddings in the fine-tunedmodels, the information they contain, and the capabilities of the fine-tunedmodels on simple linguistic tasks. We also introduce a new synthetic languagethat leads to better transfer to English than the languages used in previousresearch. Finally, we introduce Tiny-Cloze Benchmark - a new syntheticbenchmark for natural language understanding that is more informative for lesspowerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models inseveral domains demonstrating that fine-tuning on a new synthetic languageallows for better performance on a variety of tasks.</description>
      <author>example@mail.com (Mikhail Budnikov, Ivan Yamshchikov)</author>
      <guid isPermaLink="false">2505.15769v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2505.15576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Joint Conference on Artificial  Intelligence (IJCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了自适应硬负样本扰动学习（AHNPL）方法，用于提升视觉语言模型（VLMs）在复合推理（CR）任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的VLMs方法主要通过生成基于文本的硬负样本来微调模型，忽视了图像负样本的重要性，导致视觉编码器训练不足，影响了模型的整体性能。&lt;h4&gt;目的&lt;/h4&gt;针对上述问题，旨在提升VLMs在CR任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;AHNPL方法将基于文本的硬负样本转换为视觉域，生成语义扰动的图像负样本以训练模型，并引入了对比学习方法和动态边缘损失，以增强模型对困难样本对的区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在三个公共数据集上有效地提升了VLMs在复杂CR任务上的性能。&lt;h4&gt;结论&lt;/h4&gt;AHNPL方法通过改进负样本的生成和样本难度调整，有效提升了VLMs在复合推理任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model's discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs' performance on complex CR tasks. The source code is available at https://github.com/nynu-BDAI/AHNPL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nynu-bdai/ahnpl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) are essential for multimodal tasks, especiallycompositional reasoning (CR) tasks, which require distinguishing fine-grainedsemantic differences between visual and textual embeddings. However, existingmethods primarily fine-tune the model by generating text-based hard negativesamples, neglecting the importance of image-based negative samples, whichresults in insufficient training of the visual encoder and ultimately impactsthe overall performance of the model. Moreover, negative samples are typicallytreated uniformly, without considering their difficulty levels, and thealignment of positive samples is insufficient, which leads to challenges inaligning difficult sample pairs. To address these issues, we propose AdaptiveHard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hardnegatives into the visual domain to generate semantically disturbed image-basednegatives for training the model, thereby enhancing its overall performance.AHNPL also introduces a contrastive learning approach using a multimodal hardnegative loss to improve the model's discrimination of hard negatives withineach modality and a dynamic margin loss that adjusts the contrastive marginaccording to sample difficulty to enhance the distinction of challenging samplepairs. Experiments on three public datasets demonstrate that our methodeffectively boosts VLMs' performance on complex CR tasks. The source code isavailable at https://github.com/nynu-BDAI/AHNPL.</description>
      <author>example@mail.com (Xin Huang, Ruibin Li, Tong Jia, Wei Zheng, Ya Wang)</author>
      <guid isPermaLink="false">2505.15576v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Higher-order Structure Boosts Link Prediction on Temporal Graphs</title>
      <link>http://arxiv.org/abs/2505.15746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高级结构时序图神经网络（HTGN），用于建模和预测时序图中的结构。该方法通过结合超图表示来学习时序图，有效提高了模型的性能并减少了内存成本。&lt;h4&gt;背景&lt;/h4&gt;现有的时序图神经网络（TGNNs）主要关注成对交互，忽略了现实世界中时序图链接形成和演变中的高级结构，并且存在效率瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出HTGN旨在解决现有TGNNs在处理高级结构时的不足，同时提高模型的效率和表达力。&lt;h4&gt;方法&lt;/h4&gt;HTGN通过以下方法实现：1. 识别潜在的更高阶结构；2. 将多个边缘特征聚合到超边表示中，以减少训练过程中的内存成本。&lt;h4&gt;主要发现&lt;/h4&gt;HTGN在动态链接预测任务上表现出色，与现有方法相比，内存成本可降低高达50%。&lt;h4&gt;结论&lt;/h4&gt;HTGN通过引入超图表示和优化训练过程，显著提升了时序图神经网络的表达能力和效率。&lt;h4&gt;翻译&lt;/h4&gt;Temporal Graph Neural Networks (TGNNs) have gained growing attention for modeling and predicting structures in temporal graphs. However, existing TGNNs primarily focus on pairwise interactions while overlooking higher-order structures that are integral to link formation and evolution in real-world temporal graphs. Meanwhile, these models often suffer from efficiency bottlenecks, further limiting their expressive power. To tackle these challenges, we propose a Higher-order structure Temporal Graph Neural Network, which incorporates hypergraph representations into temporal graph learning. In particular, we develop an algorithm to identify the underlying higher-order structures, enhancing the model's ability to capture the group interactions. Furthermore, by aggregating multiple edge features into hyperedge representations, HTGN effectively reduces memory cost during training. We theoretically demonstrate the enhanced expressiveness of our approach and validate its effectiveness and efficiency through extensive experiments on various real-world temporal graphs. Experimental results show that HTGN achieves superior performance on dynamic link prediction while reducing memory costs by up to 50% compared to existing methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Graph Neural Networks (TGNNs) have gained growing attention formodeling and predicting structures in temporal graphs. However, existing TGNNsprimarily focus on pairwise interactions while overlooking higher-orderstructures that are integral to link formation and evolution in real-worldtemporal graphs. Meanwhile, these models often suffer from efficiencybottlenecks, further limiting their expressive power. To tackle thesechallenges, we propose a Higher-order structure Temporal Graph Neural Network,which incorporates hypergraph representations into temporal graph learning. Inparticular, we develop an algorithm to identify the underlying higher-orderstructures, enhancing the model's ability to capture the group interactions.Furthermore, by aggregating multiple edge features into hyperedgerepresentations, HTGN effectively reduces memory cost during training. Wetheoretically demonstrate the enhanced expressiveness of our approach andvalidate its effectiveness and efficiency through extensive experiments onvarious real-world temporal graphs. Experimental results show that HTGNachieves superior performance on dynamic link prediction while reducing memorycosts by up to 50\% compared to existing methods.</description>
      <author>example@mail.com (Jingzhe Liu, Zhigang Hua, Yan Xie, Bingheng Li, Harry Shomer, Yu Song, Kaveh Hassani, Jiliang Tang)</author>
      <guid isPermaLink="false">2505.15746v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets</title>
      <link>http://arxiv.org/abs/2505.15517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Robo2VLM，一个用于视觉语言模型（VLMs）的视觉问答（VQA）数据集生成框架，通过丰富的机器人轨迹数据来增强和评估VLMs的能力。&lt;h4&gt;背景&lt;/h4&gt;VLMs通过互联网规模的图像-文本语料库获得现实世界的知识和通用推理能力，可以增强机器人系统的场景理解和任务规划，并协助基于机器人轨迹数据训练的视觉运动策略。&lt;h4&gt;目的&lt;/h4&gt;探索使用丰富的、真实的、多模态的机器人轨迹数据来增强和评估VLMs。&lt;h4&gt;方法&lt;/h4&gt;Robo2VLM从非视觉和非描述性的感官模态（如末端执行器姿态、夹持器开口和力感应）中提取地面实况，并基于这些模态将机器人轨迹分割成一系列操作阶段。在每个阶段，Robo2VLM使用场景和交互理解来识别机器人的3D属性、任务目标和目标对象。这些属性用于生成基于空间、目标条件和交互推理问题的代表性VQA查询。&lt;h4&gt;主要发现&lt;/h4&gt;Robo2VLM-1是一个包含684,710个问题、覆盖463个不同场景和3,396个机器人操作任务的大规模真实世界数据集。结果表明，Robo2VLM-1可以衡量并提高VLMs在空间和交互推理方面的能力。&lt;h4&gt;结论&lt;/h4&gt;Robo2VLM-1数据集能够作为VLMs能力评估和提升的基准，特别是在空间和交互推理方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) acquire real-world knowledge and generalreasoning ability through Internet-scale image-text corpora. They can augmentrobotic systems with scene understanding and task planning, and assistvisuomotor policies that are trained on robot trajectory data. We explore thereverse paradigm - using rich, real, multi-modal robot trajectory data toenhance and evaluate VLMs. In this paper, we present Robo2VLM, a VisualQuestion Answering (VQA) dataset generation framework for VLMs. Given a humantele-operated robot trajectory, Robo2VLM derives ground-truth from non-visualand non-descriptive sensory modalities, such as end-effector pose, gripperaperture, and force sensing. Based on these modalities, it segments the robottrajectory into a sequence of manipulation phases. At each phase, Robo2VLM usesscene and interaction understanding to identify 3D properties of the robot,task goal, and the target object. The properties are used to generaterepresentative VQA queries - images with textural multiple-choice questions -based on spatial, goal-conditioned, and interaction reasoning questiontemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710questions covering 463 distinct scenes and 3,396 robotic manipulation tasksfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 canbenchmark and improve VLM capabilities in spatial and interaction reasoning.</description>
      <author>example@mail.com (Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Ken Goldberg)</author>
      <guid isPermaLink="false">2505.15517v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference</title>
      <link>http://arxiv.org/abs/2505.15381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, 3 tables, accepted at EMBC2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯方法的跨个体差异迁移学习方法，用于电磁肌电图（EMG）运动识别，以解决大量数据收集的负担。&lt;h4&gt;背景&lt;/h4&gt;在基于EMG的运动识别中，通常需要收集大量的标注数据来训练个体特定的分类器，这给受试者带来了负担。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，通过利用多个受试者的预训练信息来训练目标受试者，以减少数据收集的负担。&lt;h4&gt;方法&lt;/h4&gt;该方法基于一个简单假设：尽管不同受试者的EMG特征均值差异很大，但它们的方差可能表现出相似的规律。通过贝叶斯更新框架将来自多个源受试者的方差信息迁移到目标受试者，并引入一个系数来调整迁移信息量，以实现高效的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用两个EMG数据集的实验评估证明了所提出的方差迁移策略的有效性，并且与现有方法相比具有优越性。&lt;h4&gt;结论&lt;/h4&gt;所提出的跨个体差异迁移学习方法能够有效地减少数据收集的负担，并提高基于EMG的运动识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;在基于电磁肌电图（EMG）的运动识别中，通常需要使用足够的标注数据来训练针对特定个体的分类器。然而，这个过程需要收集大量的数据，这给受试者带来了负担。为了解决这个问题，利用多个受试者的预训练信息来训练目标受试者的信息可能是有益的。本文提出了一种基于贝叶斯方法的跨个体差异迁移学习方法。该方法基于一个简单假设：尽管不同受试者的EMG特征均值差异很大，但它们的方差可能表现出相似的规律。我们的方法通过贝叶斯更新框架将来自多个源受试者的方差信息迁移到目标受试者，并引入一个系数来调整迁移信息量，以实现高效的迁移学习。使用两个EMG数据集的实验评估证明了我们方差迁移策略的有效性，并且与现有方法相比具有优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/EMBC53108.2024.10782091&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In electromyogram (EMG)-based motion recognition, a subject-specificclassifier is typically trained with sufficient labeled data. However, thisprocess demands extensive data collection over extended periods, burdening thesubject. To address this, utilizing information from pre-training on multiplesubjects for the training of the target subject could be beneficial. This paperproposes an inter-subject variance transfer learning method based on a Bayesianapproach. This method is founded on the simple hypothesis that while the meansof EMG features vary greatly across subjects, their variances may exhibitsimilar patterns. Our approach transfers variance information, acquired throughpre-training on multiple source subjects, to a target subject within a Bayesianupdating framework, thereby allowing accurate classification using limitedtarget calibration data. A coefficient was also introduced to adjust the amountof information transferred for efficient transfer learning. Experimentalevaluations using two EMG datasets demonstrated the effectiveness of ourvariance transfer strategy and its superiority compared to existing methods.</description>
      <author>example@mail.com (Seitaro Yoneda, Akira Furui)</author>
      <guid isPermaLink="false">2505.15381v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation</title>
      <link>http://arxiv.org/abs/2505.15373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无监督的3D场景理解框架，能够同时构建精确的3D地图并保持语义一致性，支持实时自然语言交互。&lt;h4&gt;背景&lt;/h4&gt;现有的3D语义映射系统在重建和识别预定义对象实例方面表现出色，但在构建开放词汇语义地图方面缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练的统一系统，能够实时构建准确的3D地图，同时保持语义一致性并支持自然语言交互。&lt;h4&gt;方法&lt;/h4&gt;通过在线实例级语义嵌入融合，将GPU加速的几何重建与开放词汇视觉语言模型无缝集成，并通过层次化对象关联和空间索引进行指导。&lt;h4&gt;主要发现&lt;/h4&gt;该系统通过增量处理和统一的几何-语义更新实现了优越的性能，同时能够鲁棒地处理2D分割的不一致性。&lt;h4&gt;结论&lt;/h4&gt;该框架可以用于零样本3D实例检索、分割和对象检测等任务，以推理未见过的对象并解释自然语言查询。&lt;h4&gt;翻译&lt;/h4&gt;Mapping and understanding complex 3D environments is fundamental to how autonomous systems perceive and interact with the physical world, requiring both precise geometric reconstruction and rich semantic comprehension. While existing 3D semantic mapping systems excel at reconstructing and identifying predefined object instances, they lack the flexibility to efficiently build semantic maps with open-vocabulary during online operation. Although recent vision-language models have enabled open-vocabulary object recognition in 2D images, they haven't yet bridged the gap to 3D spatial understanding. The critical challenge lies in developing a training-free unified system that can simultaneously construct accurate 3D maps while maintaining semantic consistency and supporting natural language interactions in real time. In this paper, we develop a zero-shot framework that seamlessly integrates GPU-accelerated geometric reconstruction with open-vocabulary vision-language models through online instance-level semantic embedding fusion, guided by hierarchical object association with spatial indexing. Our training-free system achieves superior performance through incremental processing and unified geometric-semantic updates, while robustly handling 2D segmentation inconsistencies. The proposed general-purpose 3D scene understanding framework can be used for various tasks including zero-shot 3D instance retrieval, segmentation, and object detection to reason about previously unseen objects and interpret natural language queries. The project page is available at https://razer-3d.github.io.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mapping and understanding complex 3D environments is fundamental to howautonomous systems perceive and interact with the physical world, requiringboth precise geometric reconstruction and rich semantic comprehension. Whileexisting 3D semantic mapping systems excel at reconstructing and identifyingpredefined object instances, they lack the flexibility to efficiently buildsemantic maps with open-vocabulary during online operation. Although recentvision-language models have enabled open-vocabulary object recognition in 2Dimages, they haven't yet bridged the gap to 3D spatial understanding. Thecritical challenge lies in developing a training-free unified system that cansimultaneously construct accurate 3D maps while maintaining semanticconsistency and supporting natural language interactions in real time. In thispaper, we develop a zero-shot framework that seamlessly integratesGPU-accelerated geometric reconstruction with open-vocabulary vision-languagemodels through online instance-level semantic embedding fusion, guided byhierarchical object association with spatial indexing. Our training-free systemachieves superior performance through incremental processing and unifiedgeometric-semantic updates, while robustly handling 2D segmentationinconsistencies. The proposed general-purpose 3D scene understanding frameworkcan be used for various tasks including zero-shot 3D instance retrieval,segmentation, and object detection to reason about previously unseen objectsand interpret natural language queries. The project page is available athttps://razer-3d.github.io.</description>
      <author>example@mail.com (Naman Patel, Prashanth Krishnamurthy, Farshad Khorrami)</author>
      <guid isPermaLink="false">2505.15373v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>DC-Scene: Data-Centric Learning for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.15232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DC-Scene的数据中心框架，旨在提高3D场景理解的学习效率。&lt;h4&gt;背景&lt;/h4&gt;3D场景理解在机器人、自动驾驶和增强现实等领域至关重要，但受限于大规模场景的复杂性和高质量标注数据的稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发更有效的学习范式，以解决3D场景理解中的挑战。&lt;h4&gt;方法&lt;/h4&gt;DC-Scene框架包括一个CLIP驱动的双重指标质量（DIQ）过滤器，结合视觉-语言对齐分数和标题损失困惑度，以及一个课程调度器，逐步扩大训练池从场景-标题对的顶部25%到75%。&lt;h4&gt;主要发现&lt;/h4&gt;DC-Scene在ScanRefer和Nr3D数据集上实现了最先进的性能，同时将训练成本降低了约三分之二。&lt;h4&gt;结论&lt;/h4&gt;高质量的样本集可以超越全面训练，DC-Scene框架能够有效提高3D场景理解的学习效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：3D场景理解在视觉应用中扮演着基础角色，如机器人、自动驾驶和增强现实等。然而，由于3D场景的规模和复杂性较大，以及高质量标注的3D数据集比2D视觉数据集稀缺得多，基于学习的3D场景理解的发展仍然具有挑战性。这些挑战强调了需要更有效的学习范式。在本工作中，我们提出了一种名为DC-Scene的数据中心框架，专门用于3D场景理解，强调提高数据质量和训练效率。具体来说，我们引入了一种CLIP驱动的双重指标质量（DIQ）过滤器，结合视觉-语言对齐分数和标题损失困惑度，以及一个课程调度器，逐步将训练池从场景-标题对的顶部25%扩展到75%。这种策略过滤掉了噪声样本，并显著减少了对于大规模标注3D数据的依赖。在ScanRefer和Nr3D数据集上的大量实验表明，DC-Scene实现了最先进的性能（使用顶部75%子集的86.1 CIDEr，与完整数据集的85.4相比），同时将训练成本降低了约三分之二，证实了高质量样本集可以超越全面训练。代码将在https://github.com/AIGeeksGroup/DC-Scene上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/aigeeksgroup/dc-scene&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene understanding plays a fundamental role in vision applications suchas robotics, autonomous driving, and augmented reality. However, advancinglearning-based 3D scene understanding remains challenging due to two keylimitations: (1) the large scale and complexity of 3D scenes lead to highercomputational costs and slower training compared to 2D counterparts; and (2)high-quality annotated 3D datasets are significantly scarcer than thoseavailable for 2D vision. These challenges underscore the need for moreefficient learning paradigms. In this work, we propose DC-Scene, a data-centricframework tailored for 3D scene understanding, which emphasizes enhancing dataquality and training efficiency. Specifically, we introduce a CLIP-drivendual-indicator quality (DIQ) filter, combining vision-language alignment scoreswith caption-loss perplexity, along with a curriculum scheduler thatprogressively expands the training pool from the top 25% to 75% ofscene-caption pairs. This strategy filters out noisy samples and significantlyreduces dependence on large-scale labeled 3D data. Extensive experiments onScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-artperformance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)while reducing training cost by approximately two-thirds, confirming that acompact set of high-quality samples can outperform exhaustive training. Codewill be available at https://github.com/AIGeeksGroup/DC-Scene.</description>
      <author>example@mail.com (Ting Huang, Zeyu Zhang, Ruicheng Zhang, Yang Zhao)</author>
      <guid isPermaLink="false">2505.15232v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.15447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ViaRL的视频理解框架，该框架利用基于规则的强化学习来优化意图驱动的视频理解中的帧选择，通过实验证明了其在多个视频理解任务中的有效性和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;视频理解需要基于用户的意图来关注相关的帧，但现有的视频理解框架缺乏直接训练信号来识别相关帧，常用的方法成本高且可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为ViaRL的框架，以解决视频理解中帧选择的问题，提高视频理解的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;ViaRL框架采用迭代放大策略，在视频CoT系统中进行交替循环训练，并通过试错法利用下游模型的答案准确率作为奖励信号来训练帧选择器，从而无需昂贵的标注。&lt;h4&gt;主要发现&lt;/h4&gt;ViaRL在多个基准测试中表现出色，包括VideoMME、LVBench和MLVU，在Needle QA任务上实现了近15%的改进，证明了其在不同视频理解任务中的有效性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;ViaRL是一种有效的视频理解框架，能够通过优化帧选择来提高视频理解的准确性和鲁棒性，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding is inherently intention-driven-humans naturally focus onrelevant frames based on their goals. Recent advancements in multimodal largelanguage models (MLLMs) have enabled flexible query-driven reasoning; however,video-based frameworks like Video Chain-of-Thought lack direct training signalsto effectively identify relevant frames. Current approaches often rely onheuristic methods or pseudo-label supervised annotations, which are both costlyand limited in scalability across diverse scenarios. To overcome thesechallenges, we introduce ViaRL, the first framework to leverage rule-basedreinforcement learning (RL) for optimizing frame selection in intention-drivenvideo understanding. An iterated amplification strategy is adopted to performalternating cyclic training in the video CoT system, where each componentundergoes iterative cycles of refinement to improve its capabilities. ViaRLutilizes the answer accuracy of a downstream model as a reward signal to traina frame selector through trial-and-error, eliminating the need for expensiveannotations while closely aligning with human-like learning processes.Comprehensive experiments across multiple benchmarks, including VideoMME,LVBench, and MLVU, demonstrate that ViaRL consistently delivers superiortemporal grounding performance and robust generalization across diverse videounderstanding tasks, highlighting its effectiveness and scalability. Notably,ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, whichis required to search a specific needle within a long video and regarded as oneof the most suitable benchmarks for evaluating temporal grounding.</description>
      <author>example@mail.com (Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo)</author>
      <guid isPermaLink="false">2505.15447v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems</title>
      <link>http://arxiv.org/abs/2505.15685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了三种构建机器人系统的范式，并通过对复杂指令理解和跨模态消歧的任务评估以及通过VLA微调实现技能迁移的对象操作任务，评估了这些范式。&lt;h4&gt;背景&lt;/h4&gt;基础模型（FMs）越来越多地用于连接语言和具有身体感知的智能体，但不同FM集成策略的操作特性仍被低估。&lt;h4&gt;目的&lt;/h4&gt;研究复杂指令跟随和多变环境中的灵活动作生成。&lt;h4&gt;方法&lt;/h4&gt;研究了端到端视觉-语言-动作（VLA）模型和模块化管道，这些管道包含视觉-语言模型（VLMs）或多模态大型语言模型（LLMs）。&lt;h4&gt;主要发现&lt;/h4&gt;在零样本和少样本设置中的实验揭示了泛化和数据效率之间的权衡。&lt;h4&gt;结论&lt;/h4&gt;通过探索性能限制，本文总结了开发语言驱动的物理智能体的设计启示，并概述了基于FM的机器人在现实条件下的新兴挑战和机遇。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了三种构建机器人系统的范式：端到端视觉-语言-动作（VLA）模型，这些模型隐式地整合了感知和规划；以及包含视觉-语言模型（VLMs）或多模态大型语言模型（LLMs）的模块化管道。通过两个聚焦案例研究评估了这些范式：一个复杂的指令归基任务，用于评估细粒度指令理解和跨模态消歧；以及一个针对通过VLA微调实现技能迁移的对象操作任务。在零样本和少样本设置中的实验揭示了泛化和数据效率之间的权衡。通过探索性能限制，本文总结了开发语言驱动的物理智能体的设计启示，并概述了基于FM的机器人在现实条件下的新兴挑战和机遇。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) are increasingly used to bridge language and actionin embodied agents, yet the operational characteristics of different FMintegration strategies remain under-explored -- particularly for complexinstruction following and versatile action generation in changing environments.This paper examines three paradigms for building robotic systems: end-to-endvision-language-action (VLA) models that implicitly integrate perception andplanning, and modular pipelines incorporating either vision-language models(VLMs) or multimodal large language models (LLMs). We evaluate these paradigmsthrough two focused case studies: a complex instruction grounding taskassessing fine-grained instruction understanding and cross-modaldisambiguation, and an object manipulation task targeting skill transfer viaVLA finetuning. Our experiments in zero-shot and few-shot settings revealtrade-offs in generalization and data efficiency. By exploring performancelimits, we distill design implications for developing language-driven physicalagents and outline emerging challenges and opportunities for FM-poweredrobotics in real-world conditions.</description>
      <author>example@mail.com (Xiuchao Sui, Daiying Tian, Qi Sun, Ruirui Chen, Dongkyu Choi, Kenneth Kwok, Soujanya Poria)</author>
      <guid isPermaLink="false">2505.15685v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Graph Conditional Flow Matching for Relational Data Generation</title>
      <link>http://arxiv.org/abs/2505.15668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages of main content, submitted to a conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;数据合成作为隐私增强技术正在兴起，该论文提出了一种用于生成关系数据的生成模型，能够处理复杂的关系结构。&lt;h4&gt;背景&lt;/h4&gt;目前的多表数据生成方法在灵活性和表达性方面不足，难以捕捉复杂的关系结构，尤其是在长距离依赖和复杂的外键关系方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种生成模型，通过学习外键关系图来生成关系数据集的内容。&lt;h4&gt;方法&lt;/h4&gt;使用流匹配学习整个关系数据库的内容，神经网络通过图神经网络从相关记录中获取信息，从而实现去噪记录。&lt;h4&gt;主要发现&lt;/h4&gt;该方法灵活且表达性强，能够生成具有复杂结构的关系数据，且每个记录的生成可受到同一连通组件内任何其他记录的影响。&lt;h4&gt;结论&lt;/h4&gt;该方法在多个基准数据集上的评估中显示出在合成数据保真度方面达到最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;数据合成作为一种增强隐私的技术正在逐渐受到关注。尽管单表表格数据的生成已经取得了很多进展，但现有的多表数据生成方法通常缺乏灵活性和表达性，无法有效地捕捉复杂的关系结构。特别是，它们在处理长距离依赖和复杂的外键关系方面存在困难，例如具有多个父表或相同对表之间具有多种类型的链接的表。我们提出了一种关系数据的生成模型，它根据外键关系图生成关系数据集的内容。我们通过流匹配学习整个关系数据库的内容，训练用于去噪记录的神经网络利用图神经网络从连接的记录中获取信息。我们的方法是灵活的，因为它可以支持具有复杂结构的关系数据集，并且是表达性的，因为每个记录的生成可以受到同一连通组件中任何其他记录的影响。我们在多个基准数据集上评估了我们的方法，并显示出在合成数据保真度方面达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data synthesis is gaining momentum as a privacy-enhancing technology. Whilesingle-table tabular data generation has seen considerable progress, currentmethods for multi-table data often lack the flexibility and expressivenessneeded to capture complex relational structures. In particular, they strugglewith long-range dependencies and complex foreign-key relationships, such astables with multiple parent tables or multiple types of links between the samepair of tables. We propose a generative model for relational data thatgenerates the content of a relational dataset given the graph formed by theforeign-key relationships. We do this by learning a deep generative model ofthe content of the whole relational database by flow matching, where the neuralnetwork trained to denoise records leverages a graph neural network to obtaininformation from connected records. Our method is flexible, as it can supportrelational datasets with complex structures, and expressive, as the generationof each record can be influenced by any other record within the same connectedcomponent. We evaluate our method on several benchmark datasets and show thatit achieves state-of-the-art performance in terms of synthetic data fidelity.</description>
      <author>example@mail.com (Davide Scassola, Sebastiano Saccani, Luca Bortolussi)</author>
      <guid isPermaLink="false">2505.15668v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Enhanced Trajectory Matching for Small-Scale Dataset Distillation</title>
      <link>http://arxiv.org/abs/2505.15267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对资源受限环境中的机器学习模型部署，提出了一个结合对比学习的新颖数据集蒸馏方法，以生成更丰富、多样化的合成数据样本。&lt;h4&gt;背景&lt;/h4&gt;在边缘设备或快速原型场景中部署机器学习模型时，需要将大型数据集压缩成更小但信息量大的合成数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集蒸馏技术，特别是轨迹匹配方法在样本稀缺情况下无法充分保留语义丰富性的问题。&lt;h4&gt;方法&lt;/h4&gt;提出的方法在图像合成过程中集成对比学习，通过显式最大化实例级特征辨别来生成更丰富的合成样本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，这种方法在非常小规模的合成数据集上训练的模型性能得到显著提升，不仅提高了特征表示的有效性，也显著改善了合成图像的视觉保真度。&lt;h4&gt;结论&lt;/h4&gt;该方法在合成数据极为有限的情况下，相比现有蒸馏技术实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;Deploying machine learning models in resource-constrained environments, such as edge devices or rapid prototyping scenarios, increasingly demands distillation of large datasets into significantly smaller yet informative synthetic datasets. Current dataset distillation techniques, particularly Trajectory Matching methods, optimize synthetic data so that the model's training trajectory on synthetic samples mirrors that on real data. While demonstrating efficacy on medium-scale synthetic datasets, these methods fail to adequately preserve semantic richness under extreme sample scarcity. To address this limitation, we propose a novel dataset distillation method integrating contrastive learning during image synthesis. By explicitly maximizing instance-level feature discrimination, our approach produces more informative and diverse synthetic samples, even when dataset sizes are significantly constrained. Experimental results demonstrate that incorporating contrastive learning substantially enhances the performance of models trained on very small-scale synthetic datasets. This integration not only guides more effective feature representation but also significantly improves the visual fidelity of the synthesized images. Experimental results demonstrate that our method achieves notable performance improvements over existing distillation techniques, especially in scenarios with extremely limited synthetic data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying machine learning models in resource-constrained environments, suchas edge devices or rapid prototyping scenarios, increasingly demandsdistillation of large datasets into significantly smaller yet informativesynthetic datasets. Current dataset distillation techniques, particularlyTrajectory Matching methods, optimize synthetic data so that the model'straining trajectory on synthetic samples mirrors that on real data. Whiledemonstrating efficacy on medium-scale synthetic datasets, these methods failto adequately preserve semantic richness under extreme sample scarcity. Toaddress this limitation, we propose a novel dataset distillation methodintegrating contrastive learning during image synthesis. By explicitlymaximizing instance-level feature discrimination, our approach produces moreinformative and diverse synthetic samples, even when dataset sizes aresignificantly constrained. Experimental results demonstrate that incorporatingcontrastive learning substantially enhances the performance of models trainedon very small-scale synthetic datasets. This integration not only guides moreeffective feature representation but also significantly improves the visualfidelity of the synthesized images. Experimental results demonstrate that ourmethod achieves notable performance improvements over existing distillationtechniques, especially in scenarios with extremely limited synthetic data.</description>
      <author>example@mail.com (Wenmin Li, Shunsuke Sakai, Tatsuhito Hasegawa)</author>
      <guid isPermaLink="false">2505.15267v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization</title>
      <link>http://arxiv.org/abs/2505.15379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了P$^3$数据集，这是一个大规模的多模态基准数据集，用于构建矢量化，由空中激光雷达点云、高分辨率空中图像和矢量化的二维建筑轮廓构成，数据来自三个大洲。&lt;h4&gt;背景&lt;/h4&gt;现有的数据集主要关注图像模态，而P$^3$通过结合密集的3D信息提供了互补的视角。&lt;h4&gt;目的&lt;/h4&gt;构建一个用于矢量化的多模态基准数据集，并评估激光雷达点云在预测建筑多边形中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用空中激光雷达点云、高分辨率空中图像和矢量化的二维建筑轮廓构建数据集，并在混合和端到端学习框架中测试激光雷达点云在预测建筑多边形中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;激光雷达点云是预测建筑多边形的一个稳健的模态，并且融合空中激光雷达和图像可以进一步提高预测多边形的准确性和几何质量。&lt;h4&gt;结论&lt;/h4&gt;P$^3$数据集公开可用，并提供了用于建筑多边形预测的三个最先进模型的代码和预训练权重。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了P$^3$数据集，这是一个大规模的多模态基准数据集，用于构建矢量化，由空中激光雷达点云、高分辨率空中图像和矢量化的二维建筑轮廓构成，数据来自三个大洲。虽然许多现有数据集主要关注图像模态，但P$^3$通过结合密集的3D信息提供了互补的视角。我们证明了激光雷达点云是预测建筑多边形的一个稳健的模态，在混合和端到端学习框架中均有表现。此外，融合空中激光雷达和图像进一步提高了预测多边形的准确性和几何质量。P$^3$数据集是公开可用的，并提供了用于建筑多边形预测的三个最先进模型的代码和预训练权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/raphaelsulzer/pixelspointspolygons&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the P$^3$ dataset, a large-scale multimodal benchmark for buildingvectorization, constructed from aerial LiDAR point clouds, high-resolutionaerial imagery, and vectorized 2D building outlines, collected across threecontinents. The dataset contains over 10 billion LiDAR points withdecimeter-level accuracy and RGB images at a ground sampling distance of 25centimeter. While many existing datasets primarily focus on the image modality,P$^3$ offers a complementary perspective by also incorporating dense 3Dinformation. We demonstrate that LiDAR point clouds serve as a robust modalityfor predicting building polygons, both in hybrid and end-to-end learningframeworks. Moreover, fusing aerial LiDAR and imagery further improves accuracyand geometric quality of predicted polygons. The P$^3$ dataset is publiclyavailable, along with code and pretrained weights of three state-of-the-artmodels for building polygon prediction athttps://github.com/raphaelsulzer/PixelsPointsPolygons .</description>
      <author>example@mail.com (Raphael Sulzer, Liuyun Duan, Nicolas Girard, Florent Lafarge)</author>
      <guid isPermaLink="false">2505.15379v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.15581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UWSAM的模型，用于高效准确地分割水下实例，并构建了UIIS10K数据集，旨在解决大规模水下实例分割任务中的性能限制。&lt;h4&gt;背景&lt;/h4&gt;由于缺乏水下领域专业知识，SAM及其变体在水下实例分割任务中存在性能限制，并且高计算需求限制了它们在水下场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的水下实例分割模型，并构建相应的数据集，以解决水下实例分割中的性能和计算问题。&lt;h4&gt;方法&lt;/h4&gt;构建了包含10,048张图像的UIIS10K数据集，并提出了UWSAM模型，该模型通过Mask GAT基于的水下知识蒸馏（MG-UKD）方法，将SAM ViT-Huge图像编码器的知识蒸馏到较小的ViT-Small图像编码器中。此外，设计了端到端水下提示生成器（EUPG）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，UWSAM模型在多个水下实例数据集上取得了显著的性能提升，超过了最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;UWSAM模型能够有效地提高水下实例分割的性能，并具有高效的知识蒸馏和提示生成机制。&lt;h4&gt;翻译&lt;/h4&gt;With recent breakthroughs in large-scale modeling, the Segment Anything Model(SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at https://github.com/LiamLian0727/UIIS10K.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/liamlian0727/uiis10k&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With recent breakthroughs in large-scale modeling, the Segment Anything Model(SAM) has demonstrated significant potential in a variety of visualapplications. However, due to the lack of underwater domain expertise, SAM andits variants face performance limitations in end-to-end underwater instancesegmentation tasks, while their higher computational requirements furtherhinder their application in underwater scenarios. To address this challenge, wepropose a large-scale underwater instance segmentation dataset, UIIS10K, whichincludes 10,048 images with pixel-level annotations for 10 categories. Then, weintroduce UWSAM, an efficient model designed for automatic and accuratesegmentation of underwater instances. UWSAM efficiently distills knowledge fromthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via theMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effectivevisual representation learning. Furthermore, we design an End-to-end UnderwaterPrompt Generator (EUPG) for UWSAM, which automatically generates underwaterprompts instead of explicitly providing foreground points or boxes as prompts,thus enabling the network to locate underwater instances accurately forefficient segmentation. Comprehensive experimental results show that our modelis effective, achieving significant performance improvements overstate-of-the-art methods on multiple underwater instance datasets. Datasets andcodes are available at https://github.com/LiamLian0727/UIIS10K.</description>
      <author>example@mail.com (Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Sam Kwong)</author>
      <guid isPermaLink="false">2505.15581v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off</title>
      <link>http://arxiv.org/abs/2505.15594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted at the 33rd European Signal Processing Conference  (EUSIPCO 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了提升基础模型鲁棒性的方法，特别是Diffusion Denoised Smoothing技术，并通过实验分析其在不同对抗攻击下的性能表现。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在多种任务上表现出色，但它们对对抗性输入仍然脆弱。&lt;h4&gt;目的&lt;/h4&gt;研究Diffusion Denoised Smoothing技术在分类以外的下游任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;分析三个数据集，在三个不同的对抗攻击算法下，对四个不同的下游任务进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型对常规变换有较强的抵抗力，但高噪声扩散去噪对无扭曲的清洁图像的性能降低了高达57%。低噪声扩散设置虽然能保持性能，但无法对所有攻击类型提供足够的保护。此外，引入了一种针对扩散过程本身的新的攻击策略，能够在低噪声环境下绕过防御。&lt;h4&gt;结论&lt;/h4&gt;对抗鲁棒性和性能之间的权衡仍然是一个需要解决的问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models demonstrate impressive performance across varioustasks, they remain vulnerable to adversarial inputs. Current research exploresvarious approaches to enhance model robustness, with Diffusion DenoisedSmoothing emerging as a particularly promising technique. This method employs apretrained diffusion model to preprocess inputs before model inference. Yet,its effectiveness remains largely unexplored beyond classification. We aim toaddress this gap by analyzing three datasets with four distinct downstreamtasks under three different adversarial attack algorithms. Our findings revealthat while foundation models maintain resilience against conventionaltransformations, applying high-noise diffusion denoising to clean imageswithout any distortions significantly degrades performance by as high as 57%.Low-noise diffusion settings preserve performance but fail to provide adequateprotection across all attack types. Moreover, we introduce a novel attackstrategy specifically targeting the diffusion process itself, capable ofcircumventing defenses in the low-noise regime. Our results suggest that thetrade-off between adversarial robustness and performance remains a challenge tobe addressed.</description>
      <author>example@mail.com (Yury Belousov, Brian Pulfer, Vitaliy Kinakh, Slava Voloshynovskiy)</author>
      <guid isPermaLink="false">2505.15594v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Diffusion Transformers Efficiently via $μ$P</title>
      <link>http://arxiv.org/abs/2505.15270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 10 figures, 15 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将标准μP推广到扩散Transformer的方法，并通过大规模实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;扩散Transformer在视觉生成模型中作为基础，但其可扩展性受到大规模超参数调整成本高的限制。&lt;h4&gt;目的&lt;/h4&gt;研究μP在vanilla Transformers中的成功是否可以扩展到扩散Transformer。&lt;h4&gt;方法&lt;/h4&gt;本文对主流扩散Transformer（如DiT、U-ViT、PixArt-α和MMDiT）的μP进行了严格证明，并展示了DiT-μP的鲁棒超参数迁移性。通过将PixArt-α和MMDiT的规模从0.04B和0.18B扩展到0.61B和18B，验证了μP在文本到图像生成中的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;μP可以成功地应用于主流扩散Transformer，显著减少了超参数调整成本，并提高了模型的收敛速度。&lt;h4&gt;结论&lt;/h4&gt;μP是一个原理性和高效的框架，可以用于扩展扩散Transformer。&lt;h4&gt;翻译&lt;/h4&gt;摘要：扩散Transformer已经成为视觉生成模型的基础，但其可扩展性受到大规模超参数调整成本高的限制。最近，针对vanilla Transformers提出了最大更新参数化（μP），它能够实现从小到大型语言模型之间的稳定超参数迁移，并大大降低了调整成本。然而，vanilla Transformers的μP是否可以扩展到在架构和目标上都有所不同的扩散Transformer，尚不清楚。在本工作中，我们将标准μP推广到扩散Transformer，并通过大规模实验验证了其有效性。首先，我们严格证明了主流扩散Transformer（包括DiT、U-ViT、PixArt-α和MMDiT）的μP与vanilla Transformer的μP一致，使得现有的μP方法可以直接应用。利用这一结果，我们系统地证明了DiT-μP具有鲁棒的超参数迁移性。值得注意的是，使用迁移学习率的DiT-XL-2-μP比原始的DiT-XL-2收敛速度快2.9倍。最后，通过将PixArt-α从0.04B扩展到0.61B和将MMDiT从0.18B扩展到18B，我们验证了μP在文本到图像生成中的有效性。在这两种情况下，μP下的模型都优于各自的基线，同时调整成本很小，PixArt-α仅需5.5%的一次训练运行成本，MMDiT-18B仅需3%的人专家消耗。这些结果将μP确立为扩展扩散Transformer的原理性和高效框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion Transformers have emerged as the foundation for vision generativemodels, but their scalability is limited by the high cost of hyperparameter(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P)was proposed for vanilla Transformers, which enables stable HP transfer fromsmall to large language models, and dramatically reduces tuning costs. However,it remains unclear whether $\mu$P of vanilla Transformers extends to diffusionTransformers, which differ architecturally and objectively. In this work, wegeneralize standard $\mu$P to diffusion Transformers and validate itseffectiveness through large-scale experiments. First, we rigorously prove that$\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer,enabling the direct application of existing $\mu$P methodologies. Leveragingthis result, we systematically demonstrate that DiT-$\mu$P enjoys robust HPtransferability. Notably, DiT-XL-2-$\mu$P with transferred learning rateachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, wevalidate the effectiveness of $\mu$P on text-to-image generation by scalingPixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,models under $\mu$P outperform their respective baselines while requiring smalltuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% ofconsumption by human experts for MMDiT-18B. These results establish $\mu$P as aprincipled and efficient framework for scaling diffusion Transformers.</description>
      <author>example@mail.com (Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li)</author>
      <guid isPermaLink="false">2505.15270v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Rate-Accuracy Bounds in Visual Coding for Machines</title>
      <link>http://arxiv.org/abs/2505.14980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, IEEE MIPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了计算机视觉模型自动化分析视觉信号（如图像、视频和点云）的需求，提出了针对这些信号的分析压缩策略，并探讨了视觉编码领域的问题。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉模型在交通监控、机器人、自动驾驶、智能家居等领域的应用日益增多，对视觉信号的压缩策略需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;开发针对视觉信号的压缩策略，以满足分析需求而非重建需求，即所谓的“为机器编码”领域。&lt;h4&gt;方法&lt;/h4&gt;通过将视觉编码问题与离散无记忆源的损失性编码进行类比，推导了几个流行问题的速率-精度界限，并与文献中的最先进结果进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;比较结果显示，当前结果在所需比特率方面至少落后于理论界限一个数量级，在某些情况下落后两个或三个数量级，以达到一定水平的精度。&lt;h4&gt;结论&lt;/h4&gt;这表明，在视觉编码领域，当前方法仍有很大的改进空间。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着计算机视觉模型在图像、视频和点云等视觉信号上的自动化分析需求的增加，包括交通监控、机器人、自动驾驶、智能家居等多个领域。这一趋势导致了对这些信号的分析压缩策略的需求，通常被称为“为机器编码”。通过将视觉编码问题与离散无记忆源的损失性编码进行类比，本文推导了几个流行问题的速率-精度界限，并将其与文献中的最先进结果进行了比较。比较表明，当前结果在所需比特率方面至少落后于理论界限一个数量级，在某些情况下落后两个或三个数量级，以达到一定水平的精度。这反过来意味着，在视觉编码领域，当前方法仍有很大的改进空间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasingly, visual signals such as images, videos and point clouds arebeing captured solely for the purpose of automated analysis by computer visionmodels. Applications include traffic monitoring, robotics, autonomous driving,smart home, and many others. This trend has led to the need to developcompression strategies for these signals for the purpose of analysis ratherthan reconstruction, an area often referred to as "coding for machines." Bydrawing parallels with lossy coding of a discrete memoryless source, in thispaper we derive rate-accuracy bounds on several popular problems in visualcoding for machines, and compare these with state-of-the-art results from theliterature. The comparison shows that the current results are at least an orderof magnitude -- and in some cases two or three orders of magnitude -- away fromthe theoretical bounds in terms of the bitrate needed to achieve a certainlevel of accuracy. This, in turn, means that there is much room for improvementin the current methods for visual coding for machines.</description>
      <author>example@mail.com (Ivan V. Bajić)</author>
      <guid isPermaLink="false">2505.14980v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN</title>
      <link>http://arxiv.org/abs/2505.15368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Neurodyne的神经网络基音调整系统，用于改善音乐制作中的音高调整过程，通过对抗性表示学习和循环一致性训练，提高了音高调整的合成质量。&lt;h4&gt;背景&lt;/h4&gt;音高调整在音乐制作中非常重要，基于神经网络的音高调整系统因其合成质量优于传统的数字信号处理方法而受到欢迎。&lt;h4&gt;目的&lt;/h4&gt;提出Neurodyne系统旨在解决现有神经网络音高调整系统在特征解耦不准确和缺乏调音数据的问题。&lt;h4&gt;方法&lt;/h4&gt;Neurodyne系统采用对抗性表示学习来学习与音高无关的潜在表示，并通过循环一致性训练来隐式创建配对训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Neurodyne系统在全局键和基于模板的音高调整任务中表现出有效性，提高了合成质量并保持了原始歌手身份。&lt;h4&gt;结论&lt;/h4&gt;Neurodyne系统通过对抗性学习和循环一致性训练，有效地解决了现有音高调整系统的局限性，为音乐制作提供了更高质量的音高调整解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pitch manipulation is the process of producers adjusting the pitch of anaudio segment to a specific key and intonation, which is essential in musicproduction. Neural-network-based pitch-manipulation systems have been popularin recent years due to their superior synthesis quality compared to classicalDSP methods. However, their performance is still limited due to theirinaccurate feature disentanglement using source-filter models and the lack ofpaired in- and out-of-tune training data. This work proposes Neurodyne toaddress these issues. Specifically, Neurodyne uses adversarial representationlearning to learn a pitch-independent latent representation to avoid inaccuratedisentanglement and cycle-consistency training to create paired training dataimplicitly. Experimental results on global-key and template-based pitchmanipulation demonstrate the effectiveness of the proposed system, markingimproved synthesis quality while maintaining the original singer identity.</description>
      <author>example@mail.com (Yicheng Gu, Chaoren Wang, Zhizheng Wu, Lauri Juvela)</author>
      <guid isPermaLink="false">2505.15368v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback</title>
      <link>http://arxiv.org/abs/2505.15572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于强化学习的微调框架，用于改进数据到方程任务中基础模型的领域适应性，以提高方程生成的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;数据到方程任务旨在发现将观测值映射到标签的解析数学方程，提供物理洞察和广泛的应用。现有的遗传编程和深度学习方法在搜索效率和泛化能力方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;增强基础模型在数据到方程任务中的领域适应性，解决现有方法在特定领域任务中效果不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于强化学习的微调框架，通过从下游数值适应性中获得的奖励信号直接优化预训练模型的生成策略。&lt;h4&gt;主要发现&lt;/h4&gt;该方法允许模型适应特定的复杂数据分布，并生成具有数学意义的方程，实验表明该方法在复杂分布下提高了方程生成的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法有效提升了数据到方程任务中方程生成的质量和效率，为该领域的研究提供了新的思路和方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The data-to-equation (Data2Eqn) task aims to discover interpretablemathematical equations that map observed values to labels, offering physicalinsights and broad applicability across academic and industrial domains.Genetic programming and traditional deep learning-based approaches suffer fromsearch inefficiency and poor generalization on small task-specific datasets.Foundation models showed promise in this area, but existing approaches sufferfrom: 1) They are pretrained on general-purpose data distributions, making themless effective for domain-specific tasks; and 2) their training objectivesfocus on token-level alignment, overlooking mathematical semantics, which canlead to inaccurate equations. To address these issues, we aim to enhance thedomain adaptability of foundation models for Data2Eqn tasks. In this work, wepropose a reinforcement learning-based finetuning framework that directlyoptimizes the generation policy of a pretrained model through reward signalsderived from downstream numerical fitness. Our method allows the model to adaptto specific and complex data distributions and generate mathematicallymeaningful equations. Extensive experiments demonstrate that our approachimproves both the accuracy and robustness of equation generation under complexdistributions.</description>
      <author>example@mail.com (Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Haifeng Chen, Yanjie Fu)</author>
      <guid isPermaLink="false">2505.15572v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer</title>
      <link>http://arxiv.org/abs/2505.15241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GAMA++，一种新的几何感知领域自适应框架，旨在解决现有方法在任务相关和任务无关流形维度解耦不足以及刚性扰动方案忽略类间对齐不对称性等问题。&lt;h4&gt;背景&lt;/h4&gt;尽管在几何感知领域自适应方面取得进展，但现有方法如GAMA仍存在两个未解决的问题：任务相关和任务无关流形维度解耦不足，以及刚性扰动方案忽略类间对齐不对称性。&lt;h4&gt;目的&lt;/h4&gt;提出GAMA++框架，以解决现有方法的不足，提高领域自适应的性能。&lt;h4&gt;方法&lt;/h4&gt;GAMA++引入了以下方法：(i) 潜在空间解耦以隔离标签一致流形方向和干扰因素；(ii) 自适应对比扰动策略，根据类特定的流形曲率和对齐差异调整流形内和流形外的探索；(iii) 提出跨领域对比一致性损失，鼓励局部语义簇对齐同时保持域内多样性。&lt;h4&gt;主要发现&lt;/h4&gt;GAMA++在DomainNet、Office-Home和VisDA基准测试中实现了最先进的成果，在标准设置和少样本设置下均表现出显著的类级对齐精度和边界鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GAMA++为迁移学习中的语义几何对齐设定了新的标准。&lt;h4&gt;翻译&lt;/h4&gt;尽管在几何感知领域自适应方面取得进展，但当前方法如GAMA仍存在两个未解决的问题：一是任务相关和任务无关流形维度的解耦不足；二是刚性扰动方案忽略了类间的对齐不对称性。为了解决这个问题，我们提出了GAMA++，一种新的框架，它引入了：(i) 潜在空间解耦以隔离标签一致的流形方向和干扰因素；(ii) 自适应对比扰动策略，根据类特定的流形曲率和对齐差异调整流形内和流形外的探索；(iii) 提出跨领域对比一致性损失，鼓励局部语义簇对齐同时保持域内多样性。我们的方法在DomainNet、Office-Home和VisDA基准测试中实现了最先进的成果，在标准设置和少样本设置下均表现出显著的类级对齐精度和边界鲁棒性。GAMA++为迁移学习中的语义几何对齐设定了新的标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite progress in geometry-aware domain adaptation, current methods such asGAMA still suffer from two unresolved issues: (1) insufficient disentanglementof task-relevant and task-irrelevant manifold dimensions, and (2) rigidperturbation schemes that ignore per-class alignment asymmetries. To addressthis, we propose GAMA++, a novel framework that introduces (i) latent spacedisentanglement to isolate label-consistent manifold directions from nuisancefactors, and (ii) an adaptive contrastive perturbation strategy that tailorsboth on- and off-manifold exploration to class-specific manifold curvature andalignment discrepancy. We further propose a cross-domain contrastiveconsistency loss that encourages local semantic clusters to align whilepreserving intra-domain diversity. Our method achieves state-of-the-art resultson DomainNet, Office-Home, and VisDA benchmarks under both standard andfew-shot settings, with notable improvements in class-level alignment fidelityand boundary robustness. GAMA++ sets a new standard for semantic geometryalignment in transfer learning.</description>
      <author>example@mail.com (Kim Yun, Hana Satou, F Monkey)</author>
      <guid isPermaLink="false">2505.15241v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes</title>
      <link>http://arxiv.org/abs/2505.15559v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Moonbeam是一个基于Transformer的符号音乐基础模型，通过在大量的MIDI数据上预训练，包括81.6K小时的音乐和180亿个token，提出了音乐领域的归纳偏置。该模型通过引入新颖的领域知识启发式标记化和多维相对注意力（MRA）来捕捉相对音乐信息，不增加额外的可训练参数。通过微调Moonbeam，提出了两种具有完全预见能力的微调架构，针对符号音乐理解和条件音乐生成（包括音乐填充）两个下游任务。在四个数据集上进行的三个下游音乐分类任务中，该模型在准确性和F1分数方面优于其他大规模预训练音乐模型，并且在条件音乐生成模型上优于一个具有REMI-like标记器的强大Transformer基线。代码、预训练模型和生成的样本已开源。&lt;h4&gt;背景&lt;/h4&gt;Moonbeam是基于Transformer的符号音乐基础模型，通过预训练大量MIDI数据来捕捉音乐领域的知识。&lt;h4&gt;目的&lt;/h4&gt;提高符号音乐理解和条件音乐生成的性能，并提出具有完全预见能力的微调架构。&lt;h4&gt;方法&lt;/h4&gt;引入新颖的领域知识启发式标记化和多维相对注意力（MRA）来捕捉相对音乐信息，并通过微调Moonbeam来实现对下游任务的优化。&lt;h4&gt;主要发现&lt;/h4&gt;Moonbeam在三个下游音乐分类任务上优于其他大规模预训练音乐模型，且在条件音乐生成模型上优于一个基线。&lt;h4&gt;结论&lt;/h4&gt;Moonbeam在音乐理解和生成任务上表现出色，且其代码和模型已开源。&lt;h4&gt;翻译&lt;/h4&gt;Moonbeam is a transformer-based foundation model for symbolic music, pretrained on a large and diverse collection of MIDI data totaling 81.6K hours of music and 18 billion tokens. Moonbeam incorporates music-domain inductive biases by capturing both absolute and relative musical attributes through the introduction of a novel domain-knowledge-inspired tokenization method and Multidimensional Relative Attention (MRA), which captures relative music information without additional trainable parameters. Leveraging the pretrained Moonbeam, we propose 2 finetuning architectures with full anticipatory capabilities, targeting 2 categories of downstream tasks: symbolic music understanding and conditional music generation (including music infilling). Our model outperforms other large-scale pretrained music models in most cases in terms of accuracy and F1 score across 3 downstream music classification tasks on 4 datasets. Moreover, our finetuned conditional music generation model outperforms a strong transformer baseline with a REMI-like tokenizer. We open-source the code, pretrained model, and generated samples on Github.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Moonbeam is a transformer-based foundation model for symbolic music,pretrained on a large and diverse collection of MIDI data totaling 81.6K hoursof music and 18 billion tokens. Moonbeam incorporates music-domain inductivebiases by capturing both absolute and relative musical attributes through theintroduction of a novel domain-knowledge-inspired tokenization method andMultidimensional Relative Attention (MRA), which captures relative musicinformation without additional trainable parameters. Leveraging the pretrainedMoonbeam, we propose 2 finetuning architectures with full anticipatorycapabilities, targeting 2 categories of downstream tasks: symbolic musicunderstanding and conditional music generation (including music infilling). Ourmodel outperforms other large-scale pretrained music models in most cases interms of accuracy and F1 score across 3 downstream music classification taskson 4 datasets. Moreover, our finetuned conditional music generation modeloutperforms a strong transformer baseline with a REMI-like tokenizer. Weopen-source the code, pretrained model, and generated samples on Github.</description>
      <author>example@mail.com (Zixun Guo, Simon Dixon)</author>
      <guid isPermaLink="false">2505.15559v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval</title>
      <link>http://arxiv.org/abs/2505.15269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiveVLM的训练免费框架，旨在解决视频大语言模型在处理长视频序列时的内存使用和响应速度问题。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型主要关注离线视频问答，忽略了在实际应用中的内存使用和响应速度问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出LiveVLM框架，用于在线视频理解和实时交互。&lt;h4&gt;方法&lt;/h4&gt;LiveVLM构建了一个创新的流式KV缓存，以实时处理视频流，保留长期视频细节并消除冗余KV，确保对用户查询的快速响应。对于连续视频流，LiveVLM生成和压缩视频键值张量（视频KV），以保留视觉信息并提高内存效率。此外，当提出新问题时，LiveVLM结合在线问答过程，高效地获取短期和长期视觉信息，同时最小化冗余上下文的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LiveVLM使基础LLaVA-OneVision模型在同一设备上处理了44倍的帧数，与SoTA在线方法相比，在输入256帧时实现了5倍的响应速度提升，同时保持了相同的或更好的模型性能。&lt;h4&gt;结论&lt;/h4&gt;LiveVLM框架在保持模型性能的同时，显著提高了视频大语言模型的处理速度和内存效率，适用于需要实时视频理解和交互的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in Video Large Language Models (Video LLMs) have enabledmodels to process long video sequences and demonstrate remarkable performance.Nonetheless, studies predominantly focus on offline video question answering,neglecting memory usage and response speed that are essential in variousreal-world applications, such as Deepseek services, autonomous driving, androbotics. To mitigate these challenges, we propose $\textbf{LiveVLM}$, atraining-free framework specifically designed for streaming, online videounderstanding and real-time interaction. Unlike existing works that processvideos only after one question is posed, LiveVLM constructs an innovativestreaming-oriented KV cache to process video streams in real-time, retainlong-term video details and eliminate redundant KVs, ensuring prompt responsesto user queries. For continuous video streams, LiveVLM generates and compressesvideo key-value tensors (video KVs) to reserve visual information whileimproving memory efficiency. Furthermore, when a new question is proposed,LiveVLM incorporates an online question-answering process that efficientlyfetches both short-term and long-term visual information, while minimizinginterference from redundant context. Extensive experiments demonstrate thatLiveVLM enables the foundation LLaVA-OneVision model to process 44$\times$number of frames on the same device, and achieves up to 5$\times$ speedup inresponse speed compared with SoTA online methods at an input of 256 frames,while maintaining the same or better model performance.</description>
      <author>example@mail.com (Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, Jieru Zhao)</author>
      <guid isPermaLink="false">2505.15269v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives</title>
      <link>http://arxiv.org/abs/2505.15103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Graph Contrastive Learning, Self-supervised Learning,  Kolmogorov-Arnold Network, Representation Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Khan-GCL的图对比学习（GCL）新框架，旨在解决传统GCL方法的两个关键限制，并通过实验证明其在多个数据集和任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;GCL在从无标签数据中学习可泛化的图表示方面展现出巨大潜力，但传统方法存在两个主要问题：MLP编码器的表达能力有限和负样本选择不优。&lt;h4&gt;目的&lt;/h4&gt;提出Khan-GCL框架，旨在增强GCL编码器的表达能力，并生成具有语义意义的硬负样本。&lt;h4&gt;方法&lt;/h4&gt;1. 将Kolmogorov-Arnold网络（KAN）集成到GCL编码器架构中，提高其表达能力；2. 利用KAN系数参数中的丰富信息，开发两种新的关键特征识别技术，以生成具有语义意义的硬负样本。&lt;h4&gt;主要发现&lt;/h4&gt;Khan-GCL框架通过强调图之间的关键语义差异，引导编码器学习更具判别性的特征，从而在多个数据集和任务上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;Khan-GCL框架在GCL领域取得了显著进展，为图表示学习提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has demonstrated great promise for learning generalizable graph representations from unlabeled data. However, conventional GCL approaches face two critical limitations: (1) the restricted expressive capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal negative samples that either from random augmentations-failing to provide effective 'hard negatives'-or generated hard negatives without addressing thesemantic distinctions crucial for discriminating graph data. To this end, we propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold Network (KAN) into the GCL encoder architecture, substantially enhancing its representational capacity. Furthermore, we exploit the rich information embedded within KAN coefficient parameters to develop two novel critical feature identification techniques that enable the generation of semantically meaningful hard negative samples for each graph representation. These strategically constructed hard negatives guide the encoder to learn more discriminative features by emphasizing critical semantic differences between graphs. Extensive experiments demonstrate that our approach achieves state-of-the-art performance compared to existing GCL methods across a variety of datasets and tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has demonstrated great promise for learninggeneralizable graph representations from unlabeled data. However, conventionalGCL approaches face two critical limitations: (1) the restricted expressivecapacity of multilayer perceptron (MLP) based encoders, and (2) suboptimalnegative samples that either from random augmentations-failing to provideeffective 'hard negatives'-or generated hard negatives without addressing thesemantic distinctions crucial for discriminating graph data. To this end, wepropose Khan-GCL, a novel framework that integrates the Kolmogorov-ArnoldNetwork (KAN) into the GCL encoder architecture, substantially enhancing itsrepresentational capacity. Furthermore, we exploit the rich informationembedded within KAN coefficient parameters to develop two novel criticalfeature identification techniques that enable the generation of semanticallymeaningful hard negative samples for each graph representation. Thesestrategically constructed hard negatives guide the encoder to learn morediscriminative features by emphasizing critical semantic differences betweengraphs. Extensive experiments demonstrate that our approach achievesstate-of-the-art performance compared to existing GCL methods across a varietyof datasets and tasks.</description>
      <author>example@mail.com (Zihu Wang, Boxun Xu, Hejia Geng, Peng Li)</author>
      <guid isPermaLink="false">2505.15103v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking</title>
      <link>http://arxiv.org/abs/2505.15637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OMNI的口腔和面部自然图像数据集，旨在提高牙齿错颌问题的图像分析研究。&lt;h4&gt;背景&lt;/h4&gt;牙齿错颌是正畸学中的主要挑战，其复杂的呈现和多样的临床表现使得准确的定位和诊断尤为重要。目前，牙科图像分析领域的一个主要缺点是缺乏针对错颌问题的大规模、准确标注的数据集，这限制了牙科自动化诊断的发展，导致临床实践中的诊断准确性和效率低下。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过提出OMNI数据集，推动牙科图像分析在牙齿错颌问题研究中的应用。&lt;h4&gt;方法&lt;/h4&gt;OMNI数据集包含4166张多视图图像，由384名参与者提供，并由专业牙医进行标注。此外，对OMNI数据集进行了综合验证，包括三种基于CNN的方法、两种基于Transformer的方法和一种基于GNN的方法，并进行了错颌问题的自动化诊断实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，OMNI数据集可以促进错颌问题的自动化诊断研究，并为该领域的研究提供一个新的基准。&lt;h4&gt;结论&lt;/h4&gt;OMNI数据集和基线代码已公开，可用于推动牙科图像分析在错颌问题研究中的应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：牙齿错颌是正畸学中的主要挑战，其复杂的呈现和多样的临床表现使得准确的定位和诊断尤为重要。目前，牙科图像分析领域的一个主要缺点是缺乏针对错颌问题的大规模、准确标注的数据集，这限制了牙科自动化诊断的发展，导致临床实践中的诊断准确性和效率低下。因此，本研究提出了口腔和面部自然图像（OMNI）数据集，这是一个针对牙齿错颌问题分析的新颖而全面的数据集。具体来说，该数据集包含由384名参与者提供的4166张多视图图像，并由专业牙医进行标注。此外，我们对创建的OMNI数据集进行了综合验证，包括三种基于CNN的方法、两种基于Transformer的方法和一种基于GNN的方法，并进行了错颌问题的自动化诊断实验。实验结果表明，OMNI数据集可以促进错颌问题的自动化诊断研究，并为该领域的研究提供一个新的基准。我们的OMNI数据集和基线代码可在https://github.com/RoundFaceJ/OMNI上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/roundfacej/omni&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malocclusion is a major challenge in orthodontics, and its complexpresentation and diverse clinical manifestations make accurate localization anddiagnosis particularly important. Currently, one of the major shortcomingsfacing the field of dental image analysis is the lack of large-scale,accurately labeled datasets dedicated to malocclusion issues, which limits thedevelopment of automated diagnostics in the field of dentistry and leads to alack of diagnostic accuracy and efficiency in clinical practice. Therefore, inthis study, we propose the Oral and Maxillofacial Natural Images (OMNI)dataset, a novel and comprehensive dental image dataset aimed at advancing thestudy of analyzing dental images for issues of malocclusion. Specifically, thedataset contains 4166 multi-view images with 384 participants in datacollection and annotated by professional dentists. In addition, we performed acomprehensive validation of the created OMNI dataset, including three CNN-basedmethods, two Transformer-based methods, and one GNN-based method, and conductedautomated diagnostic experiments for malocclusion issues. The experimentalresults show that the OMNI dataset can facilitate the automated diagnosisresearch of malocclusion issues and provide a new benchmark for the research inthis field. Our OMNI dataset and baseline code are publicly available athttps://github.com/RoundFaceJ/OMNI.</description>
      <author>example@mail.com (Pujun Xue, Junyi Ge, Xiaotong Jiang, Siyang Song, Zijian Wu, Yupeng Huo, Weicheng Xie, Linlin Shen, Xiaoqin Zhou, Xiaofeng Liu, Min Gu)</author>
      <guid isPermaLink="false">2505.15637v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</title>
      <link>http://arxiv.org/abs/2505.14910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TCSinger 2是一种多任务多语言零样本歌唱声音合成（SVS）模型，具有风格迁移和基于不同提示的风格控制功能，旨在提高音乐创作和短视频配音中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;现有的SVS模型过度依赖音素和音符边界标注，导致在零样本场景下的鲁棒性不足，且音素和音符之间的过渡效果不佳。&lt;h4&gt;目的&lt;/h4&gt;克服现有SVS模型的局限性，提高其零样本场景下的鲁棒性和风格控制能力。&lt;h4&gt;方法&lt;/h4&gt;TCSinger 2主要包括三个关键模块：1）模糊边界内容（BBC）编码器，预测持续时间，扩展内容嵌入，并应用掩码以实现平滑过渡；2）定制音频编码器，使用对比学习从歌唱、语音和文本提示中提取对齐表示；3）基于流的定制Transformer，利用Cus-MOE和F0监督，增强生成的歌唱声音的合成质量和风格建模。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TCSinger 2在多个相关任务的主观和客观指标上均优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;TCSinger 2是一种有效的多语言零样本歌唱声音合成模型，能够显著提高音乐创作和短视频配音中的应用效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Customizable multilingual zero-shot singing voice synthesis (SVS) has variouspotential applications in music composition and short video dubbing. However,existing SVS models overly depend on phoneme and note boundary annotations,limiting their robustness in zero-shot scenarios and producing poor transitionsbetween phonemes and notes. Moreover, they also lack effective multi-levelstyle control via diverse prompts. To overcome these challenges, we introduceTCSinger 2, a multi-task multilingual zero-shot SVS model with style transferand style control based on various prompts. TCSinger 2 mainly includes threekey modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,extends content embedding, and applies masking to the boundaries to enablesmooth transitions. 2) Custom Audio Encoder, uses contrastive learning toextract aligned representations from singing, speech, and textual prompts. 3)Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,enhancing both the synthesis quality and style modeling of the generatedsinging voice. Experimental results show that TCSinger 2 outperforms baselinemodels in both subjective and objective metrics across multiple related tasks.</description>
      <author>example@mail.com (Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao)</author>
      <guid isPermaLink="false">2505.14910v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework</title>
      <link>http://arxiv.org/abs/2505.15245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Findings of the Association for Computational Linguistics: ACL  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GETER的新型结构感知生成框架，用于可解释的时序推理，并通过实验证明了其在时序推理中的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型（LLMs）在时序推理方面具有巨大潜力，但大多数现有工作过于关注性能提升，而忽视了推理过程的可解释性。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文提出一个涵盖广泛时序粒度的全面基准，旨在系统地评估LLMs在可解释时序推理方面的能力。&lt;h4&gt;方法&lt;/h4&gt;本文首先利用时序知识图开发了一个时序编码器，用于捕获查询的结构信息。随后，引入了一个结构-文本前缀适配器，将图结构特征映射到文本嵌入空间。最后，LLMs通过无缝集成软图标记和指令调整提示标记来生成解释文本。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，LLMs在仅依赖文本信息时难以提供令人信服的解释。&lt;h4&gt;结论&lt;/h4&gt;GETER在性能上达到了最先进水平，同时证明了其有效性和强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporalknowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefixadapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/carrytatum/geter&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While large language models (LLMs) show great potential in temporalreasoning, most existing work focuses heavily on enhancing performance, oftenneglecting the explainable reasoning processes underlying the results. Toaddress this gap, we introduce a comprehensive benchmark covering a wide rangeof temporal granularities, designed to systematically evaluate LLMs'capabilities in explainable temporal reasoning. Furthermore, our findingsreveal that LLMs struggle to deliver convincing explanations when relyingsolely on textual information. To address challenge, we propose GETER, a novelstructure-aware generative framework that integrates Graph structures with textfor Explainable TEmporal Reasoning. Specifically, we first leverage temporalknowledge graphs to develop a temporal encoder that captures structuralinformation for the query. Subsequently, we introduce a structure-text prefixadapter to map graph structure features into the text embedding space. Finally,LLMs generate explanation text by seamlessly integrating the soft graph tokenwith instruction-tuning prompt tokens. Experimental results indicate that GETERachieves state-of-the-art performance while also demonstrating itseffectiveness as well as strong generalization capabilities. Our dataset andcode are available at https://github.com/carryTatum/GETER.</description>
      <author>example@mail.com (Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng)</author>
      <guid isPermaLink="false">2505.15245v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows</title>
      <link>http://arxiv.org/abs/2505.15329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FINE的可逆神经网络架构，它在保持紧凑性、可解释性和信息保留特性的同时，学习一维非线性波相互作用和精确圆周平移对称性的低维表示。&lt;h4&gt;背景&lt;/h4&gt;可逆神经网络因其紧凑性、可解释性和信息保留特性而受到关注。&lt;h4&gt;目的&lt;/h4&gt;提出FINE架构，用于学习一维非线性波相互作用和精确圆周平移对称性的低维表示。&lt;h4&gt;方法&lt;/h4&gt;FINE结合了可逆的单调激活函数和可逆的滤波器结构，并可以通过可逆ResNets扩展。该架构通过在潜在空间中的傅里叶截断步骤来实现降维，同时保持平移等变性和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;FINE在性能上显著优于离散傅里叶变换（DFT）和正交分解（POD）等经典线性方法，并且比具有卷积层的传统深度自动编码器（CNN）具有更高的重建精度。同时，FINE使用了规模更小的模型，并提供了优越的物理可解释性。&lt;h4&gt;结论&lt;/h4&gt;可逆单神经元网络与频谱截断相结合，为学习物理数据集的紧凑和可解释的表示提供了一个有希望的框架，并有助于物理学信息机器学习中的对称性感知表示学习。&lt;h4&gt;翻译&lt;/h4&gt;摘要：可逆神经网络架构最近因其紧凑性、可解释性和信息保留特性而受到关注。在这项工作中，我们提出了傅里叶可逆神经网络编码器（FINE），它结合了可逆的单调激活函数和可逆的滤波器结构，可以通过可逆ResNets扩展。该架构在学习和一维非线性波相互作用以及精确圆周平移对称性的低维表示方面进行了检验。维度在层间得到保留，除了潜在空间中的傅里叶截断步骤，这使降维成为可能，同时保持了平移等变性和可解释性。我们的结果表明，FINE在性能上显著优于离散傅里叶变换（DFT）和正交分解（POD）等经典线性方法，并且达到了比具有卷积层的传统深度自动编码器（CNN）更好的重建精度——同时使用规模更小的模型，并提供了优越的物理可解释性。这些发现表明，可逆单神经元网络与频谱截断相结合，为学习物理数据集的紧凑和可解释的表示提供了一个有希望的框架，并有助于物理学信息机器学习中的对称性感知表示学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Invertible neural architectures have recently attracted attention for theircompactness, interpretability, and information-preserving properties. In thiswork, we propose the Fourier-Invertible Neural Encoder (FINE), which combinesinvertible monotonic activation functions with reversible filter structures,and could be extended using Invertible ResNets. This architecture is examinedin learning low-dimensional representations of one-dimensional nonlinear waveinteractions and exact circular translation symmetry. Dimensionality ispreserved across layers, except for a Fourier truncation step in the latentspace, which enables dimensionality reduction while maintaining shiftequivariance and interpretability. Our results demonstrate that FINEsignificantly outperforms classical linear methods such as Discrete FourierTransformation (DFT) and Proper Orthogonal Decomposition (POD), and achievesreconstruction accuracy better than conventional deep autoencoders withconvolutional layers (CNN) - while using substantially smaller models andoffering superior physical interpretability. These findings suggest thatinvertible single-neuron networks, when combined with spectral truncation,offer a promising framework for learning compact and interpretablerepresentations of physics datasets, and symmetry-aware representation learningin physics-informed machine learning.</description>
      <author>example@mail.com (Anqiao Ouyang, Hongyi Ke, Qi Wang)</author>
      <guid isPermaLink="false">2505.15329v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network</title>
      <link>http://arxiv.org/abs/2505.15203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, accepted at IEEE EMBC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合领域对抗训练、卷积神经网络（CNN）和双向长短期记忆（BiLSTM）的癫痫发作检测框架，以提高癫痫发作检测的准确性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;癫痫发作检测在EEG模式存在显著个体差异的背景下具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以泛化到新患者的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 通过领域对抗训练，CNN提取局部患者不变特征。2. BiLSTM捕捉提取特征中的时间依赖性，以建模癫痫发作的演化模式。&lt;h4&gt;主要发现&lt;/h4&gt;使用20名局灶性癫痫患者的EEG记录进行评估，该方法在非对抗方法之上表现出卓越的性能，实现了跨患者的较高检测准确率。&lt;h4&gt;结论&lt;/h4&gt;对抗训练与时间建模的结合使得跨患者癫痫发作检测变得鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;Automated epileptic seizure detection from electroencephalogram (EEG) remains challenging due to significant individual differences in EEG patterns across patients. While existing studies achieve high accuracy with patient-specific approaches, they face difficulties in generalizing to new patients. To address this, we propose a detection framework combining domain adversarial training with a convolutional neural network (CNN) and a bidirectional long short-term memory (BiLSTM). First, the CNN extracts local patient-invariant features through domain adversarial training, which optimizes seizure detection accuracy while minimizing patient-specific characteristics. Then, the BiLSTM captures temporal dependencies in the extracted features to model seizure evolution patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy demonstrated superior performance over non-adversarial methods, achieving high detection accuracy across different patients. The integration of adversarial training with temporal modeling enables robust cross-patient seizure detection.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated epileptic seizure detection from electroencephalogram (EEG) remainschallenging due to significant individual differences in EEG patterns acrosspatients. While existing studies achieve high accuracy with patient-specificapproaches, they face difficulties in generalizing to new patients. To addressthis, we propose a detection framework combining domain adversarial trainingwith a convolutional neural network (CNN) and a bidirectional long short-termmemory (BiLSTM). First, the CNN extracts local patient-invariant featuresthrough domain adversarial training, which optimizes seizure detection accuracywhile minimizing patient-specific characteristics. Then, the BiLSTM capturestemporal dependencies in the extracted features to model seizure evolutionpatterns. Evaluation using EEG recordings from 20 patients with focal epilepsydemonstrated superior performance over non-adversarial methods, achieving highdetection accuracy across different patients. The integration of adversarialtraining with temporal modeling enables robust cross-patient seizure detection.</description>
      <author>example@mail.com (Rina Tazaki, Tomoyuki Akiyama, Akira Furui)</author>
      <guid isPermaLink="false">2505.15203v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</title>
      <link>http://arxiv.org/abs/2505.15191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAADA框架，用于解决域迁移下的迁移学习问题，通过分解对抗性扰动，同时捕捉语义变化和模型脆弱性，并在多个数据集上证明了其优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;域迁移下的迁移学习因源数据和目标数据流形之间的差异而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出MAADA框架，以提高迁移学习在域迁移情况下的性能。&lt;h4&gt;方法&lt;/h4&gt;MAADA框架分解对抗性扰动为流形内和流形外成分，同时使用几何感知对齐损失最小化源和目标流形之间的测地线差异。&lt;h4&gt;主要发现&lt;/h4&gt;MAADA在流形内一致性约束下降低了假设复杂性并提高了泛化能力，而在流形外正则化下则平滑了低密度区域的决策边界。&lt;h4&gt;结论&lt;/h4&gt;实验表明，MAADA在无监督和少样本设置中均优于现有方法，表现出优异的结构鲁棒性和跨域泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在域迁移下的迁移学习中，由于源和目标数据流形之间的差异，这是一个基本的挑战。在本文中，我们提出了MAADA（流形感知对抗性数据增强），这是一个新颖的框架，将对抗性扰动分解为流形内和流形外成分，以同时捕捉语义变化和模型脆弱性。我们理论证明了强制流形内一致性可以降低假设复杂性并提高泛化，而流形外正则化可以平滑低密度区域的决策边界。此外，我们引入了一种几何感知的对齐损失，以最小化源和目标流形之间的测地线差异。在DomainNet、VisDA和Office-Home上的实验表明，MAADA在无监督和少样本设置中均优于现有的对抗性和自适应方法，证明了其优异的结构鲁棒性和跨域泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning under domain shift remains a fundamental challenge due tothe divergence between source and target data manifolds. In this paper, wepropose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel frameworkthat decomposes adversarial perturbations into on-manifold and off-manifoldcomponents to simultaneously capture semantic variation and model brittleness.We theoretically demonstrate that enforcing on-manifold consistency reduceshypothesis complexity and improves generalization, while off-manifoldregularization smooths decision boundaries in low-density regions. Moreover, weintroduce a geometry-aware alignment loss that minimizes geodesic discrepancybetween source and target manifolds. Experiments on DomainNet, VisDA, andOffice-Home show that MAADA consistently outperforms existing adversarial andadaptation methods in both unsupervised and few-shot settings, demonstratingsuperior structural robustness and cross-domain generalization.</description>
      <author>example@mail.com (Hana Satou, Alan Mitkiy, F Monkey)</author>
      <guid isPermaLink="false">2505.15191v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts</title>
      <link>http://arxiv.org/abs/2505.15506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in TMLR (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了预训练的视觉语言基础模型在针对分布和类别与训练数据差异很大的目标数据集上的适应性问题，并提出了PromptMargin方法来优化这些模型在少量标注样本上的表现。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练的视觉语言模型在零样本泛化方面表现出色，但在面对与训练数据分布和类别差异很大的目标数据集时，直接微调这些模型面临着过拟合和泛化能力下降的问题。&lt;h4&gt;目的&lt;/h4&gt;探索在仅有少量标注样本的情况下，如何评估进一步微调是否能够提升模型在目标数据集上的表现。&lt;h4&gt;方法&lt;/h4&gt;通过分析视觉语言嵌入空间，提出了一种名为PromptMargin的新方法，该方法直接在少量目标样本上调整大规模视觉语言模型。PromptMargin包括两个主要模块：一是使用选择性增强策略来补充每个任务中的少量训练样本；二是通过引入一种新颖的多模态边缘正则化器，增加类间边缘以改善类别的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;在十五个目标基准数据集上进行的广泛实验和分析表明，所提出的PromptMargin框架优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;PromptMargin方法有效地提高了大规模视觉语言模型在少量标注样本上的性能，为解决此类问题提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Recently, Vision-Language foundation models like CLIP and ALIGN, which are pre-trained on large-scale data, have shown remarkable zero-shot generalization to diverse datasets with different classes and even domains. In this work, we take a step further and analyze whether these models can be adapted to target datasets having very different distributions and classes compared to what these models have been trained on, using only a few labeled examples from the target dataset. In such scenarios, finetuning large pretrained models is challenging due to problems of overfitting as well as loss of generalization, and has not been well explored in prior literature. Since, the pre-training data of such models are unavailable, it is difficult to comprehend the performance on various downstream datasets. First, we try to answer the question: Given a target dataset with a few labelled examples, can we estimate whether further fine-tuning can enhance the performance compared to zero-shot evaluation? by analyzing the common vision-language embedding space. Based on the analysis, we propose a novel prompt-tuning method, PromptMargin for adapting such large-scale VLMs directly on the few target samples. PromptMargin effectively tunes the text as well as visual prompts for this task, and has two main modules: 1) Firstly, we use a selective augmentation strategy to complement the few training samples in each task; 2) Additionally, to ensure robust training in the presence of unfamiliar class names, we increase the inter-class margin for improved class discrimination using a novel Multimodal Margin Regularizer. Extensive experiments and analysis across fifteen target benchmark datasets, with varying degrees of distribution shifts from natural images, shows the effectiveness of the proposed framework over the existing state-of-the-art approaches applied to this setting. github.com/debarshigit/PromptMargin.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/debarshigit/promptmargin&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Vision-Language foundation models like CLIP and ALIGN, which arepre-trained on large-scale data have shown remarkable zero-shot generalizationto diverse datasets with different classes and even domains. In this work, wetake a step further and analyze whether these models can be adapted to targetdatasets having very different distributions and classes compared to what thesemodels have been trained on, using only a few labeled examples from the targetdataset. In such scenarios, finetuning large pretrained models is challengingdue to problems of overfitting as well as loss of generalization, and has notbeen well explored in prior literature. Since, the pre-training data of suchmodels are unavailable, it is difficult to comprehend the performance onvarious downstream datasets. First, we try to answer the question: Given atarget dataset with a few labelled examples, can we estimate whether furtherfine-tuning can enhance the performance compared to zero-shot evaluation? byanalyzing the common vision-language embedding space. Based on the analysis, wepropose a novel prompt-tuning method, PromptMargin for adapting suchlarge-scale VLMs directly on the few target samples. PromptMargin effectivelytunes the text as well as visual prompts for this task, and has two mainmodules: 1) Firstly, we use a selective augmentation strategy to complement thefew training samples in each task; 2) Additionally, to ensure robust trainingin the presence of unfamiliar class names, we increase the inter-class marginfor improved class discrimination using a novel Multimodal Margin Regularizer.Extensive experiments and analysis across fifteen target benchmark datasets,with varying degrees of distribution shifts from natural images, shows theeffectiveness of the proposed framework over the existing state-of-the-artapproaches applied to this setting. github.com/debarshigit/PromptMargin.</description>
      <author>example@mail.com (Debarshi Brahma, Anuska Roy, Soma Biswas)</author>
      <guid isPermaLink="false">2505.15506v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Foundation Models for Multimodal Graph-Based Action Recognition</title>
      <link>http://arxiv.org/abs/2505.15192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的新型框架，用于解决精细的双手操作动作识别问题，该框架结合了视觉-语言基础模型，利用VideoMAE进行动态视觉编码和BERT进行上下文文本嵌入。&lt;h4&gt;背景&lt;/h4&gt;多模态视频理解迎来了新的时代，通过提取丰富的时空和语义表示。&lt;h4&gt;目的&lt;/h4&gt;为了解决精细双手操作动作识别的挑战。&lt;h4&gt;方法&lt;/h4&gt;该方法构建了一个自适应的多模态图，其中节点代表帧、对象和文本注释，边编码空间、时间和语义关系。这些图结构根据学习到的交互动态演变，允许灵活和上下文感知的推理。图注意力网络中的任务特定注意力机制通过根据动作语义调节边的重要性来进一步增强推理。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的广泛评估表明，该方法在动作识别方面一致优于最先进的基线。&lt;h4&gt;结论&lt;/h4&gt;结合基础模型与动态图推理对于鲁棒和可泛化的动作识别具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have ushered in a new era for multimodal videounderstanding by enabling the extraction of rich spatiotemporal and semanticrepresentations. In this work, we introduce a novel graph-based framework thatintegrates a vision-language foundation, leveraging VideoMAE for dynamic visualencoding and BERT for contextual textual embedding, to address the challenge ofrecognizing fine-grained bimanual manipulation actions. Departing fromconventional static graph architectures, our approach constructs an adaptivemultimodal graph where nodes represent frames, objects, and textualannotations, and edges encode spatial, temporal, and semantic relationships.These graph structures evolve dynamically based on learned interactions,allowing for flexible and context-aware reasoning. A task-specific attentionmechanism within a Graph Attention Network further enhances this reasoning bymodulating edge importance based on action semantics. Through extensiveevaluations on diverse benchmark datasets, we demonstrate that our methodconsistently outperforms state-of-the-art baselines, underscoring the strengthof combining foundation models with dynamic graph-based reasoning for robustand generalizable action recognition.</description>
      <author>example@mail.com (Fatemeh Ziaeetabar, Florentin Wörgötter)</author>
      <guid isPermaLink="false">2505.15192v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation</title>
      <link>http://arxiv.org/abs/2505.14986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个用于学习跨形态操作的新基准，重点关注两种基础任务：跨越不同形态的抓取和推动。该基准旨在测试泛化能力，包括插值、外推和组合三个维度。研究评估了不同强化学习策略在多种形态上的学习能力和泛化能力，旨在探讨形态感知训练是否优于单一形态基线，以及零样本泛化到未见形态的可行性。&lt;h4&gt;背景&lt;/h4&gt;将控制策略泛化到新的形态是机器人可扩展和可迁移学习的基本挑战。尽管先前的研究在运动方面有所探索，但在操作任务中的系统研究仍然有限，部分原因是缺乏标准化的基准。&lt;h4&gt;目的&lt;/h4&gt;引入一个用于学习跨形态操作的基准，评估不同强化学习策略在多种形态上的学习能力和泛化能力，并探讨形态感知训练的优越性以及零样本泛化的可行性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个基准，包含跨越不同形态的抓取和推动任务，并从插值、外推和组合三个维度测试泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示了多形态学习的当前局限性，并提供了关于架构和训练设计选择如何影响策略泛化的见解。&lt;h4&gt;结论&lt;/h4&gt;形态感知训练可能优于单一形态基线，零样本泛化到未见形态是可行的，但多形态学习的泛化能力存在局限性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：将控制策略泛化到新的形态是使机器人实现可扩展和可迁移学习的基本挑战。虽然先前的工作在运动方面进行了探索，但在操作任务中的系统研究仍然有限，部分原因是缺乏标准化的基准。在本文中，我们引入了一个用于学习跨形态操作的基准，重点关注两种基础任务——跨越不同形态的抓取和推动。该基准旨在测试沿着三个轴的泛化：插值（测试在同一机器人类别中共享相同链结构的表现）、外推（测试在不同链结构上的机器人）和组合（测试链结构的组合）。在该基准上，我们评估了不同RL策略从多种形态中学习以及泛化到新形态的能力。我们的研究旨在回答形态感知训练是否能优于单一形态基线，是否可以实现零样本泛化到未见形态，以及这些模式在不同泛化制度下的一致性如何。结果突出了多形态学习的当前局限性，并提供了关于架构和训练设计选择如何影响策略泛化的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing control policies to novel embodiments remains a fundamentalchallenge in enabling scalable and transferable learning in robotics. Whileprior works have explored this in locomotion, a systematic study in the contextof manipulation tasks remains limited, partly due to the lack of standardizedbenchmarks. In this paper, we introduce a benchmark for learningcross-embodiment manipulation, focusing on two foundational tasks-reach andpush-across a diverse range of morphologies. The benchmark is designed to testgeneralization along three axes: interpolation (testing performance within arobot category that shares the same link structure), extrapolation (testing ona robot with a different link structure), and composition (testing oncombinations of link structures). On the benchmark, we evaluate the ability ofdifferent RL policies to learn from multiple morphologies and to generalize tonovel ones. Our study aims to answer whether morphology-aware training canoutperform single-embodiment baselines, whether zero-shot generalization tounseen morphologies is feasible, and how consistently these patterns holdacross different generalization regimes. The results highlight the currentlimitations of multi-embodiment learning and provide insights into howarchitectural and training design choices influence policy generalization.</description>
      <author>example@mail.com (Meenal Parakh, Alexandre Kirchmeyer, Beining Han, Jia Deng)</author>
      <guid isPermaLink="false">2505.14986v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations</title>
      <link>http://arxiv.org/abs/2505.15405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HOPSE的Topological Deep Learning框架，旨在解决现有方法在处理高阶关系数据时的可扩展性问题。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在建模关系数据方面非常有效，但它们无法完全捕捉复杂现实系统中自然存在的多向关系。&lt;h4&gt;目的&lt;/h4&gt;提出HOPSE框架，以克服现有TDL方法在高阶交互处理中的可扩展性挑战。&lt;h4&gt;方法&lt;/h4&gt;HOPSE使用Hasse图分解，不依赖消息传递，从而在任意高阶域上提供高效且具有表达力的编码。&lt;h4&gt;主要发现&lt;/h4&gt;HOPSE在保持表达力和排列等变性的同时，其性能与现有最佳方法相当或更优，且在速度上比基于HOMP的模型快7倍。&lt;h4&gt;结论&lt;/h4&gt;HOPSE为可扩展的TDL开辟了新的途径，克服了现有方法的可扩展性限制。&lt;h4&gt;翻译&lt;/h4&gt;While Graph Neural Networks (GNNs) have proven highly effective at modeling relational data, pairwise connections cannot fully capture multi-way relationships naturally present in complex real-world systems. In response to this, Topological Deep Learning (TDL) leverages more general combinatorial representations -- such as simplicial or cellular complexes -- to accommodate higher-order interactions. Existing TDL methods often extend GNNs through Higher-Order Message Passing (HOMP), but face critical scalability challenges due to (i) a combinatorial explosion of message-passing routes, and (ii) significant complexity overhead from the propagation mechanism. To overcome these limitations, we propose HOPSE (Higher-Order Positional and Structural Encoder) -- a message passing-free framework that uses Hasse graph decompositions to derive efficient and expressive encodings over arbitrary higher-order domains. Notably, HOPSE scales linearly with dataset size while preserving expressive power and permutation equivariance. Experiments on molecular, expressivity, and topological benchmarks show that HOPSE matches or surpasses state-of-the-art performance while achieving up to 7 times speedups over HOMP-based models, opening a new path for scalable TDL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Neural Networks (GNNs) have proven highly effective at modelingrelational data, pairwise connections cannot fully capture multi-wayrelationships naturally present in complex real-world systems. In response tothis, Topological Deep Learning (TDL) leverages more general combinatorialrepresentations -- such as simplicial or cellular complexes -- to accommodatehigher-order interactions. Existing TDL methods often extend GNNs throughHigher-Order Message Passing (HOMP), but face critical \emph{scalabilitychallenges} due to \textit{(i)} a combinatorial explosion of message-passingroutes, and \textit{(ii)} significant complexity overhead from the propagationmechanism. To overcome these limitations, we propose HOPSE (Higher-OrderPositional and Structural Encoder) -- a \emph{message passing-free} frameworkthat uses Hasse graph decompositions to derive efficient and expressiveencodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scaleslinearly with dataset size while preserving expressive power and permutationequivariance. Experiments on molecular, expressivity and topological benchmarksshow that HOPSE matches or surpasses state-of-the-art performance whileachieving up to 7 $times$ speedups over HOMP-based models, opening a new pathfor scalable TDL.</description>
      <author>example@mail.com (Martin Carrasco, Guillermo Bernardez, Marco Montagna, Nina Miolane, Lev Telyatnikov)</author>
      <guid isPermaLink="false">2505.15405v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Piecewise-linear Ricci curvature flows on weighted graphs</title>
      <link>http://arxiv.org/abs/2505.15395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络中的社区检测问题，提出了基于Ricci曲率流的算法，并证明了其全局存在性和唯一性，以及在不同Ricci曲率下的性质，并在社区检测问题上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;社区检测是图神经网络中的一个重要问题，基于Ricci曲率流的算法近年来受到关注，已有相关数学理论的发展。&lt;h4&gt;目的&lt;/h4&gt;提出统一的分段线性Ricci曲率流算法，用于社区检测，并证明其性质和优越性。&lt;h4&gt;方法&lt;/h4&gt;提出了基于任意选择Ricci曲率的分段线性Ricci曲率流，并证明了其全局存在性和唯一性，以及在不同Ricci曲率下的性质，并将其应用于社区检测问题。&lt;h4&gt;主要发现&lt;/h4&gt;1. 提出的分段线性Ricci曲率流具有全局存在性和唯一性；2. 当Ricci曲率是同质的，经过多次手术后，演化图在每个连通分量上具有常数的Ricci曲率；3. 在三个真实世界数据集上，该方法优于基线模型和现有方法。&lt;h4&gt;结论&lt;/h4&gt;提出的算法在社区检测问题上具有优越性，不需要在每个迭代中进行曲率计算，且迭代过程收敛。&lt;h4&gt;翻译&lt;/h4&gt;摘要：社区检测是图神经网络中的一个重要问题。最近，基于Ricci曲率流的算法受到了广泛关注。Ollivier（2009年）提出了这一理论，Ni等（2019年）和Lai等（2022年）将其应用于社区检测。其数学理论由Bai等（2024年）和Li-M"unch（2025年）发展。特别是，这些流的一些解具有存在性、唯一性和收敛性。然而，该领域尚未建立统一的理论框架。在当前研究中，我们提出了几个关于任意选择的Ricci曲率的统一分段线性Ricci曲率流。首先，我们证明了这些流具有全局存在性和唯一性。其次，我们表明，如果使用的Ricci曲率是同质的，那么经过多次手术，演化图在每个连通分量上都具有常数的Ricci曲率。值得注意的是，五种常用的Ricci曲率（分别由Ollivier、Lin-Lu-Yau、Forman、Menger和Haantjes定义）都是同质的，并且所有这些结果的证明与特定Ricci曲率的选择无关。第三，作为应用，我们将离散分段线性Ricci曲率流与手术应用于社区检测问题。在三个真实世界数据集上，该流始终优于基线模型和现有方法。在合成图上的补充实验进一步证实了其可扩展性和鲁棒性。与现有算法相比，我们的算法有两个优点：它不需要在每个迭代中进行曲率计算，且迭代过程收敛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Community detection is an important problem in graph neural networks.Recently, algorithms based on Ricci curvature flows have gained significantattention. It was suggested by Ollivier (2009), and applied to communitydetection by Ni et al (2019) and Lai et al (2022). Its mathematical theory wasdue to Bai et al (2024) and Li-M\"unch (2025). In particular, solutions to someof these flows have existence, uniqueness and convergence. However, a unifiedtheoretical framework has not yet been established in this field.  In the current study, we propose several unified piecewise-linear Riccicurvature flows with respect to arbitrarily selected Ricci curvatures. First,we prove that the flows have global existence and uniqueness. Second, we showthat if the Ricci curvature being used is homogeneous, then after undergoingmultiple surgeries, the evolving graph has a constant Ricci curvature on eachconnected component. Note that five commonly used Ricci curvatures, which wererespectively defined by Ollivier, Lin-Lu-Yau, Forman, Menger and Haantjes, areall homogeneous, and that the proof of all these results is independent of thechoice of the specific Ricci curvature. Third, as an application, we apply thediscrete piecewise-linear Ricci curvature flow with surgeries to the problem ofcommunity detection. On three real-world datasets, the flow consistentlyoutperforms baseline models and existing methods. Complementary experiments onsynthetic graphs further confirm its scalability and robustness. Compared withexisting algorithms, our algorithm has two advantages: it does not requirecurvature calculations at each iteration, and the iterative process converges.</description>
      <author>example@mail.com (Jicheng Ma, Yunyan Yang)</author>
      <guid isPermaLink="false">2505.15395v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection</title>
      <link>http://arxiv.org/abs/2505.15184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AuxDet的新型红外小目标检测器，通过结合文本元数据，解决Omni-domain红外小目标检测的挑战。&lt;h4&gt;背景&lt;/h4&gt;Omni-domain红外小目标检测需要模型适应不同的成像系统、分辨率和光谱波段，现有方法主要依赖视觉建模，存在背景干扰和泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过结合文本元数据，提高红外小目标检测的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个名为AuxDet的多模态框架，使用多层感知器融合元数据语义和视觉特征，并通过一维卷积块增强模块进一步优化特征。&lt;h4&gt;主要发现&lt;/h4&gt;结合辅助信息可以显著提高红外小目标检测的性能，验证了辅助信息在提高鲁棒性和准确性方面的关键作用。&lt;h4&gt;结论&lt;/h4&gt;AuxDet在广泛的红外小目标检测任务中优于现有方法，证明了辅助信息的重要性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：全领域红外小目标检测（Omni-domain infrared small target detection, IRSTD）面临着严峻挑战，因为单个模型必须无缝适应不同的成像系统、不同的分辨率和多个光谱波段。当前的方法主要依赖于仅视觉建模的范式，不仅难以处理复杂的背景干扰和内在稀缺的目标特征，而且在复杂的全场景环境中表现出有限的泛化能力，这些环境中存在显著的领域转移和外观变化。在这项工作中，我们揭示了现有范式中的一个关键疏忽：忽略了描述成像参数和采集条件的可用辅助元数据，例如光谱波段、传感器平台、分辨率和观测视角。为了解决这一局限性，我们提出了辅助元数据驱动的红外小目标检测器（AuxDet），这是一个新颖的多模态框架，通过结合文本元数据对场景进行感知优化，从根本上重新构思了IRSTD范式。通过基于多层感知器（MLPs）的高维融合模块，AuxDet动态地将元数据语义与视觉特征相结合，引导每个样本的适应性表示学习。此外，我们设计了一个使用1D卷积块进行轻量级预初始化的增强模块，以进一步优化融合特征并恢复细粒度目标线索。在具有挑战性的WideIRSTD-Full基准上的大量实验表明，AuxDet始终优于最先进的方法，验证了辅助信息在提高全领域IRSTD任务鲁棒性和准确性方面的关键作用。代码可在https://github.com/GrokCV/AuxDet处获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/grokcv/auxdet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omni-domain infrared small target detection (IRSTD) poses formidablechallenges, as a single model must seamlessly adapt to diverse imaging systems,varying resolutions, and multiple spectral bands simultaneously. Currentapproaches predominantly rely on visual-only modeling paradigms that not onlystruggle with complex background interference and inherently scarce targetfeatures, but also exhibit limited generalization capabilities across complexomni-scene environments where significant domain shifts and appearancevariations occur. In this work, we reveal a critical oversight in existingparadigms: the neglect of readily available auxiliary metadata describingimaging parameters and acquisition conditions, such as spectral bands, sensorplatforms, resolution, and observation perspectives. To address thislimitation, we propose the Auxiliary Metadata Driven Infrared Small TargetDetector (AuxDet), a novel multi-modal framework that fundamentally reimaginesthe IRSTD paradigm by incorporating textual metadata for scene-awareoptimization. Through a high-dimensional fusion module based on multi-layerperceptrons (MLPs), AuxDet dynamically integrates metadata semantics withvisual features, guiding adaptive representation learning for each individualsample. Additionally, we design a lightweight prior-initialized enhancementmodule using 1D convolutional blocks to further refine fused features andrecover fine-grained target cues. Extensive experiments on the challengingWideIRSTD-Full benchmark demonstrate that AuxDet consistently outperformsstate-of-the-art methods, validating the critical role of auxiliary informationin improving robustness and accuracy in omni-domain IRSTD tasks. Code isavailable at https://github.com/GrokCV/AuxDet.</description>
      <author>example@mail.com (Yangting Shi, Renjie He, Le Hui, Xiang Li, Jian Yang, Ming-Ming Cheng, Yimian Dai)</author>
      <guid isPermaLink="false">2505.15184v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration</title>
      <link>http://arxiv.org/abs/2505.15180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCAI 20205&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NeuBM（Neutral Bias Mitigation）是一种通过中性输入校准来减轻图神经网络（GNN）模型偏差的新方法，显著提高了少数类的平衡准确率和召回率，同时保持了整体性能。&lt;h4&gt;背景&lt;/h4&gt;GNN在多个领域表现出色，但往往存在模型偏差，尤其是在类别不平衡的情况下，这可能导致对少数类的不公平预测。&lt;h4&gt;目的&lt;/h4&gt;提出NeuBM以减轻GNN中的模型偏差，提高模型对少数类的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;NeuBM利用动态更新的中性图来估计和纠正模型的固有偏差，通过从输入图的logits中减去中性图的logits来重新校准模型的预测。&lt;h4&gt;主要发现&lt;/h4&gt;NeuBM可以无缝集成到现有的GNN架构和训练过程中，对计算开销影响最小。在多个基准数据集上的实验表明，NeuBM显著提高了少数类的平衡准确率和召回率，特别是在类别不平衡和标签数据有限的情况下。&lt;h4&gt;结论&lt;/h4&gt;NeuBM不仅调整了最终预测，还影响了网络中平衡特征表示的学习，为偏差缓解提供了理论见解，并将其与表示平衡的概念联系起来。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have shown remarkable performance across various domains, yet they often struggle with model bias, particularly in the presence of class imbalance. This bias can lead to suboptimal performance and unfair predictions, especially for underrepresented classes. We introduce NeuBM (Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs through neutral input calibration. NeuBM leverages a dynamically updated neutral graph to estimate and correct the inherent biases of the model. By subtracting the logits obtained from the neutral graph from those of the input graph, NeuBM effectively recalibrates the model's predictions, reducing bias across different classes. Our method integrates seamlessly into existing GNN architectures and training procedures, requiring minimal computational overhead. Extensive experiments on multiple benchmark datasets demonstrate that NeuBM significantly improves the balanced accuracy and recall of minority classes, while maintaining strong overall performance. The effectiveness of NeuBM is particularly pronounced in scenarios with severe class imbalance and limited labeled data, where traditional methods often struggle. We provide theoretical insights into how NeuBM achieves bias mitigation, relating it to the concept of representation balancing. Our analysis reveals that NeuBM not only adjusts the final predictions but also influences the learning of balanced feature representations throughout the network.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance across variousdomains, yet they often struggle with model bias, particularly in the presenceof class imbalance. This bias can lead to suboptimal performance and unfairpredictions, especially for underrepresented classes. We introduce NeuBM(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNsthrough neutral input calibration. NeuBM leverages a dynamically updatedneutral graph to estimate and correct the inherent biases of the model. Bysubtracting the logits obtained from the neutral graph from those of the inputgraph, NeuBM effectively recalibrates the model's predictions, reducing biasacross different classes. Our method integrates seamlessly into existing GNNarchitectures and training procedures, requiring minimal computationaloverhead. Extensive experiments on multiple benchmark datasets demonstrate thatNeuBM significantly improves the balanced accuracy and recall of minorityclasses, while maintaining strong overall performance. The effectiveness ofNeuBM is particularly pronounced in scenarios with severe class imbalance andlimited labeled data, where traditional methods often struggle. We providetheoretical insights into how NeuBM achieves bias mitigation, relating it tothe concept of representation balancing. Our analysis reveals that NeuBM notonly adjusts the final predictions but also influences the learning of balancedfeature representations throughout the network.</description>
      <author>example@mail.com (Jiawei Gu, Ziyue Qiao, Xiao Luo)</author>
      <guid isPermaLink="false">2505.15180v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets</title>
      <link>http://arxiv.org/abs/2505.15176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的框架，旨在提高跨域步态识别的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;由于视角、外观和环境的严重域偏移，广义步态识别是一个具有挑战性的问题。&lt;h4&gt;目的&lt;/h4&gt;通过解决域偏移带来的问题，实现跨域步态识别的鲁棒性能。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种解耦的三元组损失，以隔离不同数据集之间的监督信号，缓解优化过程中的梯度冲突。2. 引入了一种有针对性的数据集蒸馏策略，基于特征冗余和预测不确定性过滤掉最少信息量的20%训练样本，提高数据效率。&lt;h4&gt;主要发现&lt;/h4&gt;在CASIA-B、OU-MVLP、Gait3D和GREW数据集上的实验表明，该方法显著提高了GaitBase和DeepGaitV2主干网络的跨数据集识别能力，同时不牺牲源域的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效提高跨域步态识别的性能，将在https://github.com/li1er3/Generalized_Gait发布代码。&lt;h4&gt;翻译&lt;/h4&gt;Generalized gait recognition, which aims to achieve robust performance across diverse domains, remains a challenging problem due to severe domain shifts in viewpoints, appearances, and environments. While mixed-dataset training is widely used to enhance generalization, it introduces new obstacles including inter-dataset optimization conflicts and redundant or noisy samples, both of which hinder effective representation learning. To address these challenges, we propose a unified framework that systematically improves cross-domain gait recognition. First, we design a disentangled triplet loss that isolates supervision signals across datasets, mitigating gradient conflicts during optimization. Second, we introduce a targeted dataset distillation strategy that filters out the least informative 20% of training samples based on feature redundancy and prediction uncertainty, enhancing data efficiency. Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that our method significantly improves cross-dataset recognition for both GaitBase and DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will be released at https://github.com/li1er3/Generalized_Gait.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized gait recognition, which aims to achieve robust performance acrossdiverse domains, remains a challenging problem due to severe domain shifts inviewpoints, appearances, and environments. While mixed-dataset training iswidely used to enhance generalization, it introduces new obstacles includinginter-dataset optimization conflicts and redundant or noisy samples, both ofwhich hinder effective representation learning. To address these challenges, wepropose a unified framework that systematically improves cross-domain gaitrecognition. First, we design a disentangled triplet loss that isolatessupervision signals across datasets, mitigating gradient conflicts duringoptimization. Second, we introduce a targeted dataset distillation strategythat filters out the least informative 20\% of training samples based onfeature redundancy and prediction uncertainty, enhancing data efficiency.Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate thatour method significantly improves cross-dataset recognition for both GaitBaseand DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code willbe released at https://github.com/li1er3/Generalized_Gait.</description>
      <author>example@mail.com (Qian Zhou, Xianda Guo, Jilong Wang, Chuanfu Shen, Zhongyuan Wang, Hua Zou, Qin Zou, Chao Liang, Chen Long, Gang Wu)</author>
      <guid isPermaLink="false">2505.15176v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection</title>
      <link>http://arxiv.org/abs/2505.15173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AvatarShield的新框架，用于检测以人为中心的虚假视频，旨在解决AIGC技术快速发展带来的信息完整性、身份安全和公众信任威胁。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能生成内容（AIGC）技术的迅速发展，特别是视频生成领域，虽然带来了前所未有的创意能力，但也增加了信息完整性、身份安全和公众信任的威胁。&lt;h4&gt;目的&lt;/h4&gt;针对以人为中心的虚假视频检测的挑战，旨在提出一种能够有效识别和防范虚假视频的方法。&lt;h4&gt;方法&lt;/h4&gt;AvatarShield是基于可解释的多语言语言模型（MLLM）的框架，通过组相对策略优化（GRPO）进行增强。它通过精心设计的准确度检测奖励和时间补偿奖励，避免了高成本文本标注数据的依赖，并实现了精确的时间建模和伪造检测。同时，采用双编码器架构，结合高级语义推理和低级痕迹放大，引导MLLM进行有效的伪造检测。&lt;h4&gt;主要发现&lt;/h4&gt;AvatarShield在领域内和跨领域检测方面均显著优于现有方法，为以人为中心的视频取证设定了新的标准。&lt;h4&gt;结论&lt;/h4&gt;AvatarShield框架为以人为中心的虚假视频检测提供了一种有效解决方案，有助于维护信息安全和公众信任。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人工智能生成内容（AIGC）技术的快速发展，特别是在视频生成领域，带来了前所未有的创意能力，但也增加了对信息完整性、身份安全和公众信任的威胁。尽管现有的检测方法在一般场景下有效，但对于以人为中心的视频，由于其真实性和潜在的非法和道德滥用风险，缺乏稳健的解决方案。此外，当前的检测方法往往存在泛化能力差、可扩展性有限和依赖劳动密集型监督微调的问题。为了解决这些挑战，我们提出了AvatarShield，这是第一个基于可解释的多语言语言模型（MLLM）的框架，通过组相对策略优化（GRPO）进行增强。通过我们精心设计的准确度检测奖励和时间补偿奖励，它有效地避免了使用高成本的文本标注数据，实现了精确的时间建模和伪造检测。同时，我们设计了一个双编码器架构，结合高级语义推理和低级痕迹放大，以引导MLLM进行有效的伪造检测。我们进一步收集了FakeHumanVid，这是一个大规模的人为中心的视频基准，包括由姿态、音频和文本输入指导的合成方法，使得检测方法可以在现实场景中得到严格评估。广泛的实验表明，AvatarShield在领域内和跨领域检测方面都显著优于现有方法，为以人为中心的视频取证设定了新的标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of Artificial Intelligence Generated Content (AIGC)technologies, particularly in video generation, has led to unprecedentedcreative capabilities but also increased threats to information integrity,identity security, and public trust. Existing detection methods, whileeffective in general scenarios, lack robust solutions for human-centric videos,which pose greater risks due to their realism and potential for legal andethical misuse. Moreover, current detection approaches often suffer from poorgeneralization, limited scalability, and reliance on labor-intensive supervisedfine-tuning. To address these challenges, we propose AvatarShield, the firstinterpretable MLLM-based framework for detecting human-centric fake videos,enhanced via Group Relative Policy Optimization (GRPO). Through our carefullydesigned accuracy detection reward and temporal compensation reward, iteffectively avoids the use of high-cost text annotation data, enabling precisetemporal modeling and forgery detection. Meanwhile, we design a dual-encoderarchitecture, combining high-level semantic reasoning and low-level artifactamplification to guide MLLMs in effective forgery detection. We further collectFakeHumanVid, a large-scale human-centric video benchmark that includessynthesis methods guided by pose, audio, and text inputs, enabling rigorousevaluation of detection methods in real-world scenes. Extensive experimentsshow that AvatarShield significantly outperforms existing approaches in bothin-domain and cross-domain detection, setting a new standard for human-centricvideo forensics.</description>
      <author>example@mail.com (Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang)</author>
      <guid isPermaLink="false">2505.15173v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2505.15334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效框架，用于对多光谱基础模型SpectralGPT进行微调，以适应高光谱图像分类（HSIC）。通过实验验证了所提出的方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;多光谱基础模型在遥感领域取得了显著成功，但针对高光谱图像的分类任务，由于高光谱图像具有大量光谱波段，其应用仍相对较少。此外，微调模型对下游任务具有挑战性，需要大量内存和存储。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的微调方法，以适应高光谱图像分类任务，并减少所需的内存和存储。&lt;h4&gt;方法&lt;/h4&gt;研究并应用了多种参数高效微调（PEFT）方法，包括低秩适应（LoRA）、基于克罗内克矩阵的适应（KronA）、低秩克罗内克（LoKr）和LoRA+。受LoRA+的启发，引入了KronA+，它将类似的机制应用于克罗内克矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在五个不同传感器的数据集上表现出与最先进的高光谱图像模型相竞争的性能。SpectralGPT的完整微调（FFT）设置在某些数据集上甚至优于专门的高光谱基础模型，同时所需的训练轮数仅为四分之一。在相同数量的训练轮次下，KronA+达到了类似性能，但可训练参数仅为0.056%，并且仅增加了大约0.2兆字节的存储空间，成为测试中效果最佳的PEFT方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在微调SpectralGPT以适应高光谱图像分类任务方面是有效的，并且通过使用KronA+等方法，可以显著减少所需的资源和计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have achieved great success across diverse domains,including remote sensing (RS), thanks to their versatility and stronggeneralization abilities. However, most RS foundation models are designed formultispectral data, while hyperspectral imagery (HSI) - with its hundreds ofspectral bands - remains less explored. Fine-tuning such models for downstreamtasks is also challenging, often demanding considerable memory and storage. Inthis paper, we propose an efficient framework to fine-tune SpectralGPT, amultispectral foundation model, for hyperspectral image classification (HSIC).We explore several Parameter-Efficient Fine-Tuning (PEFT) methods, includingLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-RankKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates forlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduceKronA+, which applies a similar mechanism to the Kronecker matrices. Weevaluate our approach on five datasets from different sensors, showingcompetitive performance with state-of-the-art HSI models. Our full fine-tuning(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectralfoundation model on some datasets while requiring only a quarter of thetraining epochs. Under the same number of epochs, KronA+ reaches similarperformance with far fewer trainable parameters - just 0.056 percent - and addsonly approximately 0.2 megabytes of storage, making it the most effective PEFTmethod tested.</description>
      <author>example@mail.com (Bernardin Ligan, Khalide Jbilou, Fahd Kalloubi, Ahmed Ratnani)</author>
      <guid isPermaLink="false">2505.15334v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks</title>
      <link>http://arxiv.org/abs/2505.14951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对地球观测数据的灵活的多模态、多任务预训练策略，通过预训练模型提高了迁移学习能力，并在多个地球观测数据集的分类和分割任务中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多模态地球观测数据为改进深度学习模型的迁移学习能力提供了巨大机遇。然而，现有方法在将学习转移到下游任务时面临挑战，因为这些任务中可用数据的结构与预训练期间使用的结构不同。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在迁移学习中的局限性，探索一种更灵活的多模态、多任务预训练策略。&lt;h4&gt;方法&lt;/h4&gt;采用多模态多任务掩码自编码器（MultiMAE），通过重建包括光谱、高程和分割数据在内的多种输入模态进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型展现出强大的迁移学习能力，在多个地球观测数据集的分类和分割任务中优于现有方法。该方法具有显著灵活性，可以处理多种输入配置，无需针对特定模态的预训练模型。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在地球观测数据迁移学习中具有显著优势，未来将在GitHub上提供代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/josesosajs/multimae-meets-eo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal data in Earth Observation (EO) presents a huge opportunity forimproving transfer learning capabilities when pre-training deep learningmodels. Unlike prior work that often overlooks multi-modal EO data, recentmethods have started to include it, resulting in more effective pre-trainingstrategies. However, existing approaches commonly face challenges ineffectively transferring learning to downstream tasks where the structure ofavailable data differs from that used during pre-training. This paper addressesthis limitation by exploring a more flexible multi-modal, multi-taskpre-training strategy for EO data. Specifically, we adopt a Multi-modalMulti-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructingdiverse input modalities, including spectral, elevation, and segmentation data.The pre-trained model demonstrates robust transfer learning capabilities,outperforming state-of-the-art methods on various EO datasets forclassification and segmentation tasks. Our approach exhibits significantflexibility, handling diverse input configurations without requiringmodality-specific pre-trained models. Code will be available at:https://github.com/josesosajs/multimae-meets-eo.</description>
      <author>example@mail.com (Jose Sosa, Danila Rukhovich, Anis Kacem, Djamila Aouada)</author>
      <guid isPermaLink="false">2505.14951v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2505.12448v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SSR的空间感知与推理方法，通过将原始深度数据转化为可解释的文本推理，显著提升了视觉语言模型的空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;尽管视觉语言模型在多模态任务上取得了显著进步，但它们对RGB输入的依赖限制了精确的空间理解。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过整合空间线索来增强视觉语言模型的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SSR的方法，该方法将原始深度数据转换为结构化的文本推理，并利用知识蒸馏将生成的推理压缩为紧凑的潜在嵌入，以便高效集成到现有的视觉语言模型中。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个基准测试上的实验，证明SSR显著提高了深度数据的利用率并增强了空间推理能力。&lt;h4&gt;结论&lt;/h4&gt;SSR方法推进了视觉语言模型向更类似人类的多模态理解发展。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型在多模态任务上取得了显著的进步，但它们对RGB输入的依赖限制了精确的空间理解。现有的空间线索整合方法，如点云或深度信息，要么需要专门的传感器，要么无法有效地利用深度信息进行高级推理。为此，我们提出了一种名为SSR的新颖的空间感知与推理方法，该方法将原始深度数据转换为可解释的文本推理。这些文本推理作为有意义的中间表示，可以显著提高空间推理能力。此外，我们利用知识蒸馏将生成的推理压缩为紧凑的潜在嵌入，从而实现资源高效的集成到现有的视觉语言模型中，无需重新训练。为了进行全面的评估，我们引入了一个名为SSR-CoT的新数据集，这是一个包含百万规模视觉语言推理数据集，并丰富了中间空间推理注释。我们还提出了SSRBench，一个综合的多任务基准。在多个基准测试上的大量实验表明，SSR显著提高了深度数据的利用率并增强了空间推理能力，从而推动了视觉语言模型向更类似人类的多模态理解发展。我们的项目页面在https://yliu-cs.github.io/SSR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite impressive advancements in Visual-Language Models (VLMs) formulti-modal tasks, their reliance on RGB inputs limits precise spatialunderstanding. Existing methods for integrating spatial cues, such as pointclouds or depth, either require specialized sensors or fail to effectivelyexploit depth information for higher-order reasoning. To this end, we propose anovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework thattransforms raw depth data into structured, interpretable textual rationales.These textual rationales serve as meaningful intermediate representations tosignificantly enhance spatial reasoning capabilities. Additionally, we leverageknowledge distillation to compress the generated rationales into compact latentembeddings, which facilitate resource-efficient and plug-and-play integrationinto existing VLMs without retraining. To enable comprehensive evaluation, weintroduce a new dataset named SSR-CoT, a million-scale visual-languagereasoning dataset enriched with intermediate spatial reasoning annotations, andpresent SSRBench, a comprehensive multi-task benchmark. Extensive experimentson multiple benchmarks demonstrate SSR substantially improves depth utilizationand enhances spatial reasoning, thereby advancing VLMs toward more human-likemulti-modal understanding. Our project page is athttps://yliu-cs.github.io/SSR.</description>
      <author>example@mail.com (Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2505.12448v2</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</title>
      <link>http://arxiv.org/abs/2505.14449v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种隐式人口统计学推断（IDI）模块，用于减少语音情感识别（SER）中的子群体差异和性能偏差，以提升SER的公平性。&lt;h4&gt;背景&lt;/h4&gt;尽管子群体差异和性能偏差在计算研究中日益受到关注，但分类语音情感识别（SER）中的公平性仍被低估。现有的方法通常依赖于难以获取的显式人口统计学标签。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一限制，提出了一种隐式人口统计学推断（IDI）模块，该模块利用预训练模型的伪标签和k-means聚类进行无监督学习，以减轻SER中的偏差。&lt;h4&gt;方法&lt;/h4&gt;IDI模块结合了伪标签和无监督学习技术，通过k-means聚类来推断人口统计学信息，从而减少SER中的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，伪标签IDI减少了子群体差异，公平性指标提高了33%以上，而SER准确率下降了不到3%。无监督IDI在公平性指标上提高了26%以上，SER性能下降了不到4%。进一步的分析显示，无监督IDI可以持续减轻种族和年龄差异。&lt;h4&gt;结论&lt;/h4&gt;IDI模块在无法获取显式人口统计学信息的情况下具有潜力，能够有效减轻SER中的偏差，提高公平性。&lt;h4&gt;翻译&lt;/h4&gt;While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 26% improvement in fairness metrics with a drop of less than 4% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential in scenarios where explicit demographic information is unavailable.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While subgroup disparities and performance bias are increasingly studied incomputational research, fairness in categorical Speech Emotion Recognition(SER) remains underexplored. Existing methods often rely on explicitdemographic labels, which are difficult to obtain due to privacy concerns. Toaddress this limitation, we introduce an Implicit Demography Inference (IDI)module that leverages pseudo-labeling from a pre-trained model and unsupervisedlearning using k-means clustering to mitigate bias in SER. Our experiments showthat pseudo-labeling IDI reduces subgroup disparities, improving fairnessmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, theunsupervised IDI yields more than a 26% improvement in fairness metrics with adrop of less than 4% in SER performance. Further analyses reveal that theunsupervised IDI consistently mitigates race and age disparities, demonstratingits potential in scenarios where explicit demographic information isunavailable.</description>
      <author>example@mail.com (Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee)</author>
      <guid isPermaLink="false">2505.14449v2</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Towards Pre-training an Effective Respiratory Audio Foundation Model</title>
      <link>http://arxiv.org/abs/2505.15307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 4 tables, Accepted by Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了呼吸声音的预训练方法，比较了多种预训练音频模型，发现使用AudioSet进行预训练比专门针对呼吸声音的预训练更有效，并且结合AudioSet和呼吸声音数据集进行进一步预训练可以提升性能。&lt;h4&gt;背景&lt;/h4&gt;基于基础模型在呼吸音频领域的应用引起了研究兴趣，但将传统预训练方案应用于小规模且缺乏多样性的数据集的有效性尚未得到充分验证。&lt;h4&gt;目的&lt;/h4&gt;旨在探索更好的呼吸声音预训练实践。&lt;h4&gt;方法&lt;/h4&gt;通过比较多种预训练音频模型来进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;模型在AudioSet（一个通用音频数据集）上预训练比专门在呼吸声音上预训练的效果更好；结合AudioSet和呼吸声音数据集进行进一步预训练可以增强性能；在聚合特征时保留频率信息是至关重要的。&lt;h4&gt;结论&lt;/h4&gt;本研究在OPERA基准上建立了新的最先进水平，为呼吸音频基础模型的进步做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;The study aims to explore better pre-training practices for respiratory sounds by comparing numerous pre-trained audio models. Our investigation reveals that models pre-trained on AudioSet, a general audio dataset, are more effective than the models specifically pre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory sound datasets for further pre-training enhances performance, and preserving the frequency-wise information when aggregating features is vital. Along with more insights found in the experiments, we establish a new state-of-the-art for the OPERA benchmark, contributing to advancing respiratory audio foundation models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models have sparked interest in respiratoryaudio foundation models. However, the effectiveness of applying conventionalpre-training schemes to datasets that are small-sized and lack diversity hasnot been sufficiently verified. This study aims to explore better pre-trainingpractices for respiratory sounds by comparing numerous pre-trained audiomodels. Our investigation reveals that models pre-trained on AudioSet, ageneral audio dataset, are more effective than the models specificallypre-trained on respiratory sounds. Moreover, combining AudioSet and respiratorysound datasets for further pre-training enhances performance, and preservingthe frequency-wise information when aggregating features is vital. Along withmore insights found in the experiments, we establish a new state-of-the-art forthe OPERA benchmark, contributing to advancing respiratory audio foundationmodels. Our code is available online athttps://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.</description>
      <author>example@mail.com (Daisuke Niizumi, Daiki Takeuchi, Masahiro Yasuda, Binh Thien Nguyen, Yasunori Ohishi, Noboru Harada)</author>
      <guid isPermaLink="false">2505.15307v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression</title>
      <link>http://arxiv.org/abs/2505.15140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了联邦图学习中的标签分布攻击问题，提出了一种新的攻击方法EC-LDA，并通过实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;GNNs在图分析中广泛应用，Federated Graph Learning (FGL)允许从多个客户端协同训练图数据。然而，模型参数的上传给服务器提供了泄露客户端数据隐私的机会。&lt;h4&gt;目的&lt;/h4&gt;旨在攻击FGL中客户端的标签分布，特别是针对标签分布攻击（LDAs）。&lt;h4&gt;方法&lt;/h4&gt;首先，分析了节点嵌入在GNNs中的方差与LDA有效性的关系，接着提出了压缩节点嵌入的新攻击方法EC-LDA，并在六个常用图数据集上进行了广泛的实验。&lt;h4&gt;主要发现&lt;/h4&gt;EC-LDA通过压缩节点嵌入显著提高了攻击的有效性，在节点分类和链接预测任务中优于现有的SOTA LDAs，并在CoraFull和LastFM数据集上达到了最优值。&lt;h4&gt;结论&lt;/h4&gt;EC-LDA在差分隐私保护下具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have been widely used for graph analysis. Federated Graph Learning (FGL) is an emerging learning framework to collaboratively train graph data from various clients. However, since clients are required to upload model parameters to the server in each round, this provides the server with an opportunity to infer each client's data privacy. In this paper, we focus on label distribution attacks (LDAs) that aim to infer the label distributions of the clients' local data. We take the first step to attack client's label distributions in FGL. Firstly, we observe that the effectiveness of LDA is closely related to the variance of node embeddings in GNNs. Next, we analyze the relation between them and we propose a new attack named EC-LDA, which significantly improves the attack effectiveness by compressing node embeddings. Thirdly, extensive experiments on node classification and link prediction tasks across six widely used graph datasets show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal values under both Cos-sim and JS-div evaluation metrics in the CoraFull and LastFM datasets. Finally, we explore the robustness of EC-LDA under differential privacy protection.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have been widely used for graph analysis.Federated Graph Learning (FGL) is an emerging learning framework tocollaboratively train graph data from various clients. However, since clientsare required to upload model parameters to the server in each round, thisprovides the server with an opportunity to infer each client's data privacy. Inthis paper, we focus on label distribution attacks(LDAs) that aim to infer thelabel distributions of the clients' local data. We take the first step toattack client's label distributions in FGL. Firstly, we observe that theeffectiveness of LDA is closely related to the variance of node embeddings inGNNs. Next, we analyze the relation between them and we propose a new attacknamed EC-LDA, which significantly improves the attack effectiveness bycompressing node embeddings. Thirdly, extensive experiments on nodeclassification and link prediction tasks across six widely used graph datasetsshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimalvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull andLastFM datasets. Finally, we explore the robustness of EC-LDA underdifferential privacy protection.</description>
      <author>example@mail.com (Tong Cheng, Fu Jie, Xinpeng Ling, Huifa Li, Zhili Chen)</author>
      <guid isPermaLink="false">2505.15140v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2505.14753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了TransMedSeg，一个用于半监督医学图像分割的可迁移语义框架，通过有效利用有限的标记数据在医学图像分割领域取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;现有的半监督医学图像分割方法主要依赖于一致性正则化和伪标签技术，但往往忽略了不同临床领域和成像模态间的可迁移语义关系。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够有效利用可迁移语义关系的半监督医学图像分割方法。&lt;h4&gt;方法&lt;/h4&gt;TransMedSeg引入了一个可迁移语义增强（TSA）模块，通过跨域分布匹配和域内结构保持来隐式增强特征表示。具体来说，TransMedSeg构建了一个统一的特征空间，其中教师网络的特征通过一个轻量级记忆模块自适应地增强以接近学生网络的语义，实现隐式语义转换而不需要显式数据生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过在增强的教师分布上计算预期的可迁移交叉熵损失，并最小化这个损失的上界，实现了隐式增强。实验表明，TransMedSeg在医学图像数据集上优于现有的半监督方法。&lt;h4&gt;结论&lt;/h4&gt;TransMedSeg为医学图像分析中的可迁移表示学习开辟了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;Semi-supervised learning (SSL) has achieved significant progress in medical image segmentation (SSMIS) through effective utilization of limited labeled data. While current SSL methods for medical images predominantly rely on consistency regularization and pseudo-labeling, they often overlook transferable semantic relationships across different clinical domains and imaging modalities. To address this, we propose TransMedSeg, a novel transferable semantic framework for semi-supervised medical image segmentation. Our approach introduces a Transferable Semantic Augmentation (TSA) module, which implicitly enhances feature representations by aligning domain-invariant semantics through cross-domain distribution matching and intra-domain structural preservation. Specifically, TransMedSeg constructs a unified featurespace where teacher network features are adaptively augmented towards student network semantics via a lightweight memory module, enabling implicit semantic transformation without explicit data generation. Interestingly, this augmentation is implicitly realized through an expected transferable cross-entropy loss computed over the augmented teacher distribution. An upper bound of the expected loss is theoretically derived and minimized during training, incurring negligible computational overhead. Extensive experiments on medical image datasets demonstrate that TransMedSeg outperforms existing semi-supervised methods, establishing a new direction for transferable representation learning in medical image analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised learning (SSL) has achieved significant progress in medicalimage segmentation (SSMIS) through effective utilization of limited labeleddata. While current SSL methods for medical images predominantly rely onconsistency regularization and pseudo-labeling, they often overlooktransferable semantic relationships across different clinical domains andimaging modalities. To address this, we propose TransMedSeg, a noveltransferable semantic framework for semi-supervised medical image segmentation.Our approach introduces a Transferable Semantic Augmentation (TSA) module,which implicitly enhances feature representations by aligning domain-invariantsemantics through cross-domain distribution matching and intra-domainstructural preservation. Specifically, TransMedSeg constructs a unified featurespace where teacher network features are adaptively augmented towards studentnetwork semantics via a lightweight memory module, enabling implicit semantictransformation without explicit data generation. Interestingly, thisaugmentation is implicitly realized through an expected transferablecross-entropy loss computed over the augmented teacher distribution. An upperbound of the expected loss is theoretically derived and minimized duringtraining, incurring negligible computational overhead. Extensive experiments onmedical image datasets demonstrate that TransMedSeg outperforms existingsemi-supervised methods, establishing a new direction for transferablerepresentation learning in medical image analysis.</description>
      <author>example@mail.com (Mengzhu Wang, Jiao Li, Shanshan Wang, Long Lan, Huibin Tan, Liang Yang, Guoli Yang)</author>
      <guid isPermaLink="false">2505.14753v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2505.15139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 47th Annual International Conference of the IEEE  Engineering in Medicine and Biology Society (EMBC) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ConneX的多模态融合方法，用于提高神经精神疾病如精神分裂症的诊断性能。&lt;h4&gt;背景&lt;/h4&gt;神经科学研究长期以来一直关注大脑的结构和功能机制，特别是在理解和治疗精神分裂症等神经精神疾病方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够充分利用结构和功能连接组学数据互补特性的方法，以增强诊断性能。&lt;h4&gt;方法&lt;/h4&gt;ConneX方法集成了交叉注意力机制和多层感知器（MLP）-Mixer，用于精细的特征融合。该方法首先使用模态特定的骨干图神经网络（GNNs）来获取每个模态的特征表示，然后引入一个统一的跨模态注意力网络来融合这些嵌入，同时MLP-Mixer层通过利用高阶依赖关系来细化全局和局部特征，以多头联合损失进行端到端分类。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同的临床数据集上进行了广泛的评估，表明所提出的框架提高了性能，突出了其鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;ConneX方法在提高神经精神疾病诊断性能方面表现出色，为神经科学研究和临床应用提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Gaining insights into the structural and functional mechanisms of the brain has been a longstanding focus in neuroscience research, particularly in the context of understanding and treating neuropsychiatric disorders such as Schizophrenia (SZ). Nevertheless, most of the traditional multimodal deep learning approaches fail to fully leverage the complementary characteristics of structural and functional connectomics data to enhance diagnostic performance. To address this issue, we proposed ConneX, a multimodal fusion method that integrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer for refined feature fusion. Modality-specific backbone graph neural networks (GNNs) were firstly employed to obtain feature representation for each modality. A unified cross-modal attention network was then introduced to fuse these embeddings by capturing intra- and inter-modal interactions, while MLP-Mixer layers refined global and local features, leveraging higher-order dependencies for end-to-end classification with a multi-head joint loss. Extensive evaluations demonstrated improved performance on two distinct clinical datasets, highlighting the robustness of our proposed framework.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaining insights into the structural and functional mechanisms of the brainhas been a longstanding focus in neuroscience research, particularly in thecontext of understanding and treating neuropsychiatric disorders such asSchizophrenia (SZ). Nevertheless, most of the traditional multimodal deeplearning approaches fail to fully leverage the complementary characteristics ofstructural and functional connectomics data to enhance diagnostic performance.To address this issue, we proposed ConneX, a multimodal fusion method thatintegrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer forrefined feature fusion. Modality-specific backbone graph neural networks (GNNs)were firstly employed to obtain feature representation for each modality. Aunified cross-modal attention network was then introduced to fuse theseembeddings by capturing intra- and inter-modal interactions, while MLP-Mixerlayers refined global and local features, leveraging higher-order dependenciesfor end-to-end classification with a multi-head joint loss. Extensiveevaluations demonstrated improved performance on two distinct clinicaldatasets, highlighting the robustness of our proposed framework.</description>
      <author>example@mail.com (Badhan Mazumder, Lei Wu, Vince D. Calhoun, Dong Hye Ye)</author>
      <guid isPermaLink="false">2505.15139v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models</title>
      <link>http://arxiv.org/abs/2505.15185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoSplat是一种新的框架，通过利用预训练的单目深度基础模型中的丰富视觉先验，实现了鲁棒的Gaussian重建，提高了对不熟悉视觉内容的处理能力。&lt;h4&gt;背景&lt;/h4&gt;现有的3D Gaussian Splatting方法在实时高保真渲染中表现出色，但在处理新场景的不熟悉视觉内容时，由于泛化能力有限，仍然存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出MonoSplat框架，以解决现有方法在处理新场景时泛化能力不足的问题。&lt;h4&gt;方法&lt;/h4&gt;MonoSplat包括两个主要组件：Mono-Multi Feature Adapter，用于将单目特征转换为多视图表示；Integrated Gaussian Prediction模块，用于有效地融合两种特征类型以生成精确的Gaussian原型。&lt;h4&gt;主要发现&lt;/h4&gt;通过轻量级的注意力机制，Adapter能够在保留单目先验的同时，无缝地对齐和聚合跨视图的特征，使Prediction模块能够生成具有精确几何和外观的Gaussian原型。实验表明，MonoSplat在多样化的真实世界数据集上实现了比现有方法更优的重建质量和泛化能力，同时保持了计算效率，并具有最少的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;MonoSplat在保持计算效率的同时，显著提高了对不熟悉视觉内容的处理能力，为实时高保真渲染提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/cuhk-aim-group/monosplat&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generalizable 3D Gaussian Splatting have demonstratedpromising results in real-time high-fidelity rendering without per-sceneoptimization, yet existing approaches still struggle to handle unfamiliarvisual content during inference on novel scenes due to limitedgeneralizability. To address this challenge, we introduce MonoSplat, a novelframework that leverages rich visual priors from pre-trained monocular depthfoundation models for robust Gaussian reconstruction. Our approach consists oftwo key components: a Mono-Multi Feature Adapter that transforms monocularfeatures into multi-view representations, coupled with an Integrated GaussianPrediction module that effectively fuses both feature types for preciseGaussian generation. Through the Adapter's lightweight attention mechanism,features are seamlessly aligned and aggregated across views while preservingvaluable monocular priors, enabling the Prediction module to generate Gaussianprimitives with accurate geometry and appearance. Through extensive experimentson diverse real-world datasets, we convincingly demonstrate that MonoSplatachieves superior reconstruction quality and generalization capability comparedto existing methods while maintaining computational efficiency with minimaltrainable parameters. Codes are available athttps://github.com/CUHK-AIM-Group/MonoSplat.</description>
      <author>example@mail.com (Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan)</author>
      <guid isPermaLink="false">2505.15185v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks</title>
      <link>http://arxiv.org/abs/2505.14717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于脑动脉瘤研究的大规模、高保真计算流体动力学（CFD）数据集，旨在促进高效机器学习算法的发展，并推动脑动脉瘤研究以及生物流体、生物医学工程和临床风险评估中的数据驱动方法。&lt;h4&gt;背景&lt;/h4&gt;颅内动脉瘤（IAs）是严重的脑血管病变，在一般人群中约占5%。其破裂可能导致高死亡率。目前评估IA风险的方法主要关注形态学和患者特异性因素，但关于血流动力学对IA发展和破裂的影响尚不明确。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，研究人员构建了一个大规模、高保真脑动脉瘤CFD数据集，以促进高效机器学习算法的开发，并推动相关领域的研究。&lt;h4&gt;方法&lt;/h4&gt;基于427个真实的动脉瘤几何形状，通过控制变形合成了10,660个3D形状来模拟动脉瘤的演变。神经外科医生验证了这些合成形状的真实性。在每种形状下，进行了八种稳态质量流量条件下的CFD计算，生成了85,280条血液流动动力学数据，覆盖了关键参数。此外，数据集还包括分割掩码，可以支持使用图像、点云或其他多模态数据作为输入的任务。此外，还引入了一个用于估计流动参数的基准，以评估当前的建模方法。&lt;h4&gt;主要发现&lt;/h4&gt;数据集包括分割掩码，可以支持使用图像、点云或其他多模态数据作为输入的任务。同时，引入的基准可用于评估现有建模方法。&lt;h4&gt;结论&lt;/h4&gt;该数据集旨在推动脑动脉瘤研究，并促进生物流体、生物医学工程和临床风险评估中的数据驱动方法。&lt;h4&gt;翻译&lt;/h4&gt;颅内动脉瘤（IAs）是严重的脑血管病变，在一般人群中约占5%。其破裂可能导致高死亡率。目前评估IA风险的方法主要关注形态学和患者特异性因素，但关于血流动力学对IA发展和破裂的影响尚不明确。为了解决这一挑战，研究人员构建了一个大规模、高保真脑动脉瘤CFD数据集，以促进高效机器学习算法的开发，并推动相关领域的研究。基于427个真实的动脉瘤几何形状，通过控制变形合成了10,660个3D形状来模拟动脉瘤的演变。神经外科医生验证了这些合成形状的真实性。在每种形状下，进行了八种稳态质量流量条件下的CFD计算，生成了85,280条血液流动动力学数据，覆盖了关键参数。此外，数据集还包括分割掩码，可以支持使用图像、点云或其他多模态数据作为输入的任务。此外，还引入了一个用于估计流动参数的基准，以评估现有的建模方法。该数据集旨在推动脑动脉瘤研究，并促进生物流体、生物医学工程和临床风险评估中的数据驱动方法。代码和数据集可在以下网址找到：https://github.com/Xigui-Li/Aneumo。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/xigui-li/aneumo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intracranial aneurysms (IAs) are serious cerebrovascular lesions found inapproximately 5\% of the general population. Their rupture may lead to highmortality. Current methods for assessing IA risk focus on morphological andpatient-specific factors, but the hemodynamic influences on IA development andrupture remain unclear. While accurate for hemodynamic studies, conventionalcomputational fluid dynamics (CFD) methods are computationally intensive,hindering their deployment in large-scale or real-time clinical applications.To address this challenge, we curated a large-scale, high-fidelity aneurysm CFDdataset to facilitate the development of efficient machine learning algorithmsfor such applications. Based on 427 real aneurysm geometries, we synthesized10,660 3D shapes via controlled deformation to simulate aneurysm evolution. Theauthenticity of these synthetic shapes was confirmed by neurosurgeons. CFDcomputations were performed on each shape under eight steady-state mass flowconditions, generating a total of 85,280 blood flow dynamics data covering keyparameters. Furthermore, the dataset includes segmentation masks, which cansupport tasks that use images, point clouds or other multimodal data as input.Additionally, we introduced a benchmark for estimating flow parameters toassess current modeling methods. This dataset aims to advance aneurysm researchand promote data-driven approaches in biofluids, biomedical engineering, andclinical risk assessment. The code and dataset are available at:https://github.com/Xigui-Li/Aneumo.</description>
      <author>example@mail.com (Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Chen Jiang, Tan Pan, Xingmeng Zhang, Cenyu Liu, Zeyun Miao, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Yichi Zhang, Wenbo Zhang, Fengping Zhu, Limei Han, Yuan Qi, Chensen Lin, Yuan Cheng)</author>
      <guid isPermaLink="false">2505.14717v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Guided Multi-View Graph Neural Network for Schizophrenia Classification via Structural-Functional Coupling</title>
      <link>http://arxiv.org/abs/2505.15135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented at the 7th International Workshop on  PRedictive Intelligence in MEdicine (Held in Conjunction with MICCAI 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的物理引导深度学习框架，用于分析神经精神疾病如精神分裂症中的脑结构连接（SC）和功能连接（FC）。&lt;h4&gt;背景&lt;/h4&gt;临床研究表明，神经精神疾病如精神分裂症中存在脑结构连接和功能连接的破坏。传统方法可能仅依赖于SC，由于功能数据的有限可用性，这阻碍了对精神分裂症患者认知和行为障碍的理解，忽视了SC-FC之间的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，本研究旨在提出一种新的方法，通过同时生成FC来利用SC，并实现对精神分裂症患者的分类。&lt;h4&gt;方法&lt;/h4&gt;本研究提出的方法利用SC从系统动力学角度学习SC-FC耦合，并采用了一种新的多视角图神经网络（GNN）以及联合损失来实现基于相关性的SC-FC融合和精神分裂症患者的分类。&lt;h4&gt;主要发现&lt;/h4&gt;在临床数据集上进行的实验表明，所提出的方法提高了性能，证明了该方法的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的方法能够有效分析神经精神疾病中的脑结构连接和功能连接，为理解和治疗这些疾病提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Clinical studies reveal disruptions in brain structural connectivity (SC) and functional connectivity (FC) in neuropsychiatric disorders such as schizophrenia (SZ). Traditional approaches might rely solely on SC due to limited functional data availability, hindering comprehension of cognitive and behavioral impairments in individuals with SZ by neglecting the intricate SC-FC interrelationship. To tackle the challenge, we propose a novel physics-guided deep learning framework that leverages a neural oscillation model to describe the dynamics of a collection of interconnected neural oscillators, which operate via nerve fibers dispersed across the brain's structure. Our proposed framework utilizes SC to simultaneously generate FC by learning SC-FC coupling from a system dynamics perspective. Additionally, it employs a novel multi-view graph neural network (GNN) with a joint loss to perform correlation-based SC-FC fusion and classification of individuals with SZ. Experiments conducted on a clinical dataset exhibited improved performance, demonstrating the robustness of our proposed approach.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-74561-4_6&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical studies reveal disruptions in brain structural connectivity (SC) andfunctional connectivity (FC) in neuropsychiatric disorders such asschizophrenia (SZ). Traditional approaches might rely solely on SC due tolimited functional data availability, hindering comprehension of cognitive andbehavioral impairments in individuals with SZ by neglecting the intricate SC-FCinterrelationship. To tackle the challenge, we propose a novel physics-guideddeep learning framework that leverages a neural oscillation model to describethe dynamics of a collection of interconnected neural oscillators, whichoperate via nerve fibers dispersed across the brain's structure. Our proposedframework utilizes SC to simultaneously generate FC by learning SC-FC couplingfrom a system dynamics perspective. Additionally, it employs a novel multi-viewgraph neural network (GNN) with a joint loss to perform correlation-based SC-FCfusion and classification of individuals with SZ. Experiments conducted on aclinical dataset exhibited improved performance, demonstrating the robustnessof our proposed approach.</description>
      <author>example@mail.com (Badhan Mazumder, Ayush Kanyal, Lei Wu, Vince D. Calhoun, Dong Hye Ye)</author>
      <guid isPermaLink="false">2505.15135v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines</title>
      <link>http://arxiv.org/abs/2505.15151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Time Tracker模型，用于在多变量时间序列数据上实现更准确的预测。&lt;h4&gt;背景&lt;/h4&gt;近年来，时间序列基础模型在预测精度上取得了显著成果，但实际时间序列数据在时间模式和领域上存在显著多样性，使得单一模型架构难以适应所有复杂场景。此外，时间序列数据可能包含多个变量，它们之间存在复杂的相互关系。&lt;h4&gt;目的&lt;/h4&gt;为了更好地处理多变量时间序列数据的预测问题。&lt;h4&gt;方法&lt;/h4&gt;Time Tracker模型采用以下方法：1. 在Transformer中使用稀疏专家混合（MoE）来处理多样化的时间序列模式；2. 提出任意变量注意力机制，使统一模型结构能够无缝处理单变量和多变量时间序列；3. 设计了一个图学习模块，通过频率域特征构建序列之间的关系，为捕获序列间的依赖关系提供更精确的指导。&lt;h4&gt;主要发现&lt;/h4&gt;Time Tracker模型在预测精度、模型泛化能力和适应性方面实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;Time Tracker模型通过上述创新方法，在处理多变量时间序列数据时表现出色，为时间序列预测提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在过去的几年里，时间序列基础模型在预测精度上取得了显著的成果。然而，现实中的时间序列数据在不同的时间段和领域往往表现出显著的多样性，这使得单一模型架构难以适应所有复杂场景。此外，时间序列数据可能包含多个变量，它们之间存在复杂的相互关系。为了更好地处理多变量时间序列数据的预测问题，我们提出了Time Tracker模型。首先，我们在Transformer中利用稀疏专家混合（MoE）来处理多样化的时间序列模式，从而减轻单个模型的学习难度并提高其泛化能力。此外，我们提出了任意变量注意力机制，使统一模型结构能够无缝处理单变量和多变量时间序列，从而在预训练阶段实现通道无关建模，在微调阶段实现通道混合建模。进一步地，我们设计了一个图学习模块，通过频率域特征构建序列之间的关系，为在通道混合建模中捕获序列间的依赖关系提供更精确的指导。基于这些进展，Time Tracker模型在预测精度、模型泛化能力和适应性方面实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past few years, time series foundation models have achieved superiorpredicting accuracy. However, real-world time series often exhibit significantdiversity in their temporal patterns across different time spans and domains,making it challenging for a single model architecture to fit all complexscenarios. In addition, time series data may have multiple variables exhibitingcomplex correlations between each other. Recent mainstream works have focusedon modeling times series in a channel-independent manner in both pretrainingand finetuning stages, overlooking the valuable inter-series dependencies. Tothis end, we propose \textbf{Time Tracker} for better predictions onmultivariate time series data. Firstly, we leverage sparse mixture of experts(MoE) within Transformers to handle the modeling of diverse time seriespatterns, thereby alleviating the learning difficulties of a single model whileimproving its generalization. Besides, we propose Any-variate Attention,enabling a unified model structure to seamlessly handle both univariate andmultivariate time series, thereby supporting channel-independent modelingduring pretraining and channel-mixed modeling for finetuning. Furthermore, wedesign a graph learning module that constructs relations among sequences fromfrequency-domain features, providing more precise guidance to captureinter-series dependencies in channel-mixed modeling. Based on theseadvancements, Time Tracker achieves state-of-the-art performance in predictingaccuracy, model generalization and adaptability.</description>
      <author>example@mail.com (Xiaohou Shi, Ke Li, Aobo Liang, Yan Sun)</author>
      <guid isPermaLink="false">2505.15151v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.15147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 14 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文回顾了基于深度学习的遥感图像语义分割（RSISS）的发展历程，对现有方法进行了分类，并对其性能进行了评估。&lt;h4&gt;背景&lt;/h4&gt;遥感图像（RSI）捕捉地球表面的自然和人为变化，是环境监测、城市规划和资源管理的重要数据来源。语义分割（SS）在遥感分析中扮演着关键角色。&lt;h4&gt;目的&lt;/h4&gt;通过深度学习（DL）自动化特征提取并提高分割精度，解决传统方法在处理大量遥感图像时效率低下和精度不足的问题。&lt;h4&gt;方法&lt;/h4&gt;将现有方法分为四个阶段：早期基于像素的方法、流行的基于块和瓦片的技术，以及由基础模型驱动的基于图像的策略。从特征提取和学习策略的角度分析这些发展，并通过对近40种高级技术进行综合评估来量化其性能和适用性。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习在RSISS领域取得了显著进展，从像素级到瓦片级，从单模态到多模态分割，分割技术不断进步。&lt;h4&gt;结论&lt;/h4&gt;本文提供了一个关于基于深度学习的遥感图像语义分割的全面视角，突出了关键进展、比较见解和开放挑战，以指导未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;This paper reviews the evolution of deep learning-based remote sensing image semantic segmentation (RSISS), categorizes existing methods, and evaluates their performance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing images (RSIs) capture both natural and human-induced changeson the Earth's surface, serving as essential data for environmental monitoring,urban planning, and resource management. Semantic segmentation (SS) of RSIsenables the fine-grained interpretation of surface features, making it acritical task in remote sensing analysis. With the increasing diversity andvolume of RSIs collected by sensors on various platforms, traditionalprocessing methods struggle to maintain efficiency and accuracy. In response,deep learning (DL) has emerged as a transformative approach, enablingsubstantial advances in remote sensing image semantic segmentation (RSISS) byautomating feature extraction and improving segmentation accuracy acrossdiverse modalities. This paper revisits the evolution of DL-based RSISS bycategorizing existing approaches into four stages: the early pixel-basedmethods, the prevailing patch-based and tile-based techniques, and the emergingimage-based strategies enabled by foundation models. We analyze thesedevelopments from the perspective of feature extraction and learningstrategies, revealing the field's progression from pixel-level to tile-leveland from unimodal to multimodal segmentation. Furthermore, we conduct acomprehensive evaluation of nearly 40 advanced techniques on a unified datasetto quantitatively characterize their performance and applicability. This reviewoffers a holistic view of DL-based SS for RS, highlighting key advancements,comparative insights, and open challenges to guide future research.</description>
      <author>example@mail.com (Quanwei Liu, Tao Huang, Yanni Dong, Jiaqi Yang, Wei Xiang)</author>
      <guid isPermaLink="false">2505.15147v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data</title>
      <link>http://arxiv.org/abs/2505.15132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多模态LLM-agent框架，旨在解决材料科学数据多样性和增长的问题，通过结合不同模态数据来提高检索准确性和材料发现效率。&lt;h4&gt;背景&lt;/h4&gt;材料科学数据种类繁多，包括高分辨率显微镜图像、动态模拟视频、表格实验日志和文献档案等。现有的AI方法通常处理单一模态数据，未能充分利用跨模态关联，且多模态基础模型需要大量重训练或微调，多智能体系统在材料信息学中仅解决特定子任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个由专业LLM代理组成的协调团队，每个代理配备领域自适应的提示和插件，以实现跨模态数据的统一推理，同时不修改底层LLM权重。&lt;h4&gt;方法&lt;/h4&gt;设计了一个动态门控机制，用于加权合并代理的输出，从而在共享嵌入空间中进行统一推理。&lt;h4&gt;主要发现&lt;/h4&gt;在挑战性案例研究中验证了该方法，与单一模态和零样本基线相比，检索准确率提高了85%，标题忠实度提高了，综合覆盖范围提高了35%。&lt;h4&gt;结论&lt;/h4&gt;该方法为AI数字研究人员搭建了桥梁，能够跨越数据孤岛，加速材料发现周期。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种受材料科学数据增长和多样性驱动的多模态LLM-agent框架。虽然最近的AI努力加速了诸如属性预测或图像分类等个别任务，但它们通常将每个模态孤立对待，未能探索丰富的跨模态关联，并迫使研究人员进行繁琐的手动集成。此外，现有的多模态基础模型通常需要在领域数据上进行昂贵的重训练或微调，而当前的多智能体系统在材料信息学中仅解决狭窄的子任务。为了克服这些障碍，我们设计了一支由专业LLM代理组成的协调团队，每个代理都配备了领域自适应的提示和插件，将它们的输出投影到一个共享的嵌入空间中。然后，一个动态门控机制对这些见解进行加权合并，从而在不修改底层LLM权重的情况下，实现异构输入的统一推理。我们在具有挑战性的案例研究中验证了我们的方法，与单一模态和零样本基线相比，检索准确率（85%）、标题忠实度以及综合覆盖范围（35%）都有显著提高。我们的工作为能够跨越数据孤岛并加速材料发现周期的AI数字研究人员铺平了道路。代码可在https://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a multicrossmodal LLM-agent framework motivated by the growingvolume and diversity of materials-science data ranging from high-resolutionmicroscopy and dynamic simulation videos to tabular experiment logs andsprawling literature archives. While recent AI efforts have acceleratedindividual tasks such as property prediction or image classification, theytypically treat each modality in isolation, leaving rich cross-modalcorrelations unexplored and forcing researchers to perform laborious manualintegration. Moreover, existing multimodal foundation models often requireexpensive retraining or fine-tuning on domain data, and current multi-agentsystems in materials informatics address only narrow subtasks. To overcomethese obstacles, we design a coordinated team of specialized LLM agents, eachequipped with domain-adapted prompts and plugins that project their outputsinto a shared embedding space. A dynamic gating mechanism then weights andmerges these insights, enabling unified reasoning over heterogeneous inputswithout ever modifying the underlying LLM weights. We validate our approach onchallenging case studies and demonstrate substantial gains in retrievalaccuracy (85%), captioning fidelity, and integrated coverage (35%) compared tosingle-modality and zero-shot baselines. Our work paves the way for AI digitalresearchers capable of bridging data silos and accelerating thematerials-discovery cycle. The code is available athttps://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.</description>
      <author>example@mail.com (Adib Bazgir, Rama chandra Praneeth Madugula, Yuwen Zhang)</author>
      <guid isPermaLink="false">2505.15132v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction</title>
      <link>http://arxiv.org/abs/2505.14747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了LiDAR数据在Level of Detail 1 (LOD1)精度下进行3D建筑重建的潜力，并从这些模型中提取形态学特征。&lt;h4&gt;背景&lt;/h4&gt;三维重建在城市规划、城市环境研究和优化交通网络设计等应用中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究旨在评估深度语义分割模型在从LiDAR数据中提取建筑足迹方面的性能，并探究分割精度对3D建筑模型质量和形态学特征准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;研究了U-Net、Attention U-Net、U-Net3+和DeepLabV3+四种深度语义分割模型，并应用迁移学习技术。使用多种统计指标（如最大值、范围、众数、中位数和90分位数）来估计建筑高度，从而生成LOD1级别的3D模型。&lt;h4&gt;主要发现&lt;/h4&gt;U-Net3+和Attention U-Net在性能上优于其他模型，分别达到了0.833和0.814的IoU分数。分割精度对3D模型质量和形态学特征（如建筑面积和外墙面积）的估计有显著影响。&lt;h4&gt;结论&lt;/h4&gt;UNet3+方法，利用90分位数和中位数指标，能够实现对建筑高度的准确估计并提取形态学特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.rsase.2025.101534&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional reconstruction of buildings, particularly at Level ofDetail 1 (LOD1), plays a crucial role in various applications such as urbanplanning, urban environmental studies, and designing optimized transportationnetworks. This study focuses on assessing the potential of LiDAR data foraccurate 3D building reconstruction at LOD1 and extracting morphologicalfeatures from these models. Four deep semantic segmentation models, U-Net,Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learningto extract building footprints from LiDAR data. The results showed that U-Net3+and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and0.814, respectively. Various statistical measures, including maximum, range,mode, median, and the 90th percentile, were used to estimate building heights,resulting in the generation of 3D models at LOD1. As the main contribution ofthe research, the impact of segmentation accuracy on the quality of 3D buildingmodeling and the accuracy of morphological features like building area andexternal wall surface area was investigated. The results showed that theaccuracy of building identification (segmentation performance) significantlyaffects the 3D model quality and the estimation of morphological features,depending on the height calculation method. Overall, the UNet3+ method,utilizing the 90th percentile and median measures, leads to accurate heightestimation of buildings and the extraction of morphological features.</description>
      <author>example@mail.com (Fatemeh Chajaei, Hossein Bagheri)</author>
      <guid isPermaLink="false">2505.14747v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Graph Foundation Models: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2505.15116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github Repo:  https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages,  438 references&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了图基础模型（GFMs），这是将大规模预训练和泛化能力扩展到图结构数据的尝试，从而在图中心任务和领域中实现广泛的迁移。&lt;h4&gt;背景&lt;/h4&gt;图结构数据在社交网络、生物系统、知识图谱和推荐系统等领域广泛应用。尽管基础模型已经改变了自然语言处理、视觉和跨模态学习，但将这些能力扩展到具有非欧几里得结构和复杂关系语义的图上仍然存在挑战。&lt;h4&gt;目的&lt;/h4&gt;Graph Foundation Models (GFMs)旨在为结构化数据带来可扩展的通用智能，使图中心任务和领域之间能够实现广泛的迁移。&lt;h4&gt;方法&lt;/h4&gt;本文提供了一个关于GFMs的全面概述，统一了包含三个关键组件（骨干架构、预训练策略和适应机制）的模块化框架。文章按照泛化范围对GFMs进行了分类，并回顾了每个类别中的代表性方法、关键创新和理论见解。&lt;h4&gt;主要发现&lt;/h4&gt;除了方法论之外，文章还考察了理论基础，包括可迁移性和涌现能力，并强调了关键挑战，如结构对齐、异构性、可扩展性和评估。&lt;h4&gt;结论&lt;/h4&gt;GFMs位于图学习和通用人工智能的交汇处，有望成为在结构化数据上开放式推理的基础设施。本文总结了当前的研究进展，并概述了未来的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;Graph-structured data is pervasive in domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data pervades domains such as social networks, biologicalsystems, knowledge graphs, and recommender systems. While foundation modelshave transformed natural language processing, vision, and multimodal learningthrough large-scale pretraining and generalization, extending thesecapabilities to graphs -- characterized by non-Euclidean structures and complexrelational semantics -- poses unique challenges and opens new opportunities. Tothis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purposeintelligence to structured data, enabling broad transfer across graph-centrictasks and domains. This survey provides a comprehensive overview of GFMs,unifying diverse efforts under a modular framework comprising three keycomponents: backbone architectures, pretraining strategies, and adaptationmechanisms. We categorize GFMs by their generalization scope -- universal,task-specific, and domain-specific -- and review representative methods, keyinnovations, and theoretical insights within each category. Beyond methodology,we examine theoretical foundations including transferability and emergentcapabilities, and highlight key challenges such as structural alignment,heterogeneity, scalability, and evaluation. Positioned at the intersection ofgraph learning and general-purpose AI, GFMs are poised to become foundationalinfrastructure for open-ended reasoning over structured data. This surveyconsolidates current progress and outlines future directions to guide researchin this rapidly evolving field. Resources are available athttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.</description>
      <author>example@mail.com (Zehong Wang, Zheyuan Liu, Tianyi Ma, Jiazheng Li, Zheyuan Zhang, Xingbo Fu, Yiyang Li, Zhengqing Yuan, Wei Song, Yijun Ma, Qingkai Zeng, Xiusi Chen, Jianan Zhao, Jundong Li, Meng Jiang, Pietro Lio, Nitesh Chawla, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2505.15116v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing</title>
      <link>http://arxiv.org/abs/2505.15015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MSH-GNN（多尺度谐波图神经网络）是一种新型架构，通过节点特定的谐波投影进行特征自适应消息传递，能够识别细粒度、方向特定的特征相关性。&lt;h4&gt;背景&lt;/h4&gt;传统的图神经网络（GNNs）在聚合邻居嵌入时作为整体向量，缺乏识别细粒度、方向特定特征相关性的能力。&lt;h4&gt;目的&lt;/h4&gt;提出MSH-GNN，以增强模型在图和节点分类任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;MSH-GNN通过节点特定的谐波投影动态地将邻居特征投影到由目标节点自身表示确定的频率敏感方向上。这些投影使用可学习的正弦编码在多个频率上进行调制，以捕获不同尺度上的平滑和振荡结构模式。引入频率感知的注意力池化机制，以强调读出过程中光谱和结构上显著的节点。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，MSH-GNN近似了平移不变核，并与1-Weisfeiler-Lehman（1-WL）测试的表达能力相匹配。在实证研究中，MSH-GNN在各种图和节点分类任务上持续优于最先进的模型。在涉及图拓扑和光谱频率联合变化的困难分类设置中，MSH-GNN擅长捕捉结构不对称和高频调制，从而实现更准确的图区分。&lt;h4&gt;结论&lt;/h4&gt;MSH-GNN在图和节点分类任务中表现出色，特别是在处理复杂结构时，能够提供更精确的图区分能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings asholistic vectors, lacking the ability to identify fine-grained,direction-specific feature relevance. We propose MSH-GNN (Multi-Scale HarmonicGraph Neural Network), a novel architecture that performs feature-wise adaptivemessage passing through node-specific harmonic projections. For each node,MSH-GNN dynamically projects neighbor features onto frequency-sensitivedirections determined by the target node's own representation. Theseprojections are further modulated using learnable sinusoidal encodings atmultiple frequencies, enabling the model to capture both smooth and oscillatorystructural patterns across scales. A frequency-aware attention poolingmechanism is introduced to emphasize spectrally and structurally salient nodesduring readout. Theoretically, we prove that MSH-GNN approximatesshift-invariant kernels and matches the expressive power of the1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperformsstate-of-the-art models on a wide range of graph and node classification tasks.Furthermore, in challenging classification settings involving joint variationsin graph topology and spectral frequency, MSH-GNN excels at capturingstructural asymmetries and high-frequency modulations, enabling more accurategraph discrimination.</description>
      <author>example@mail.com (Longlong Li, Cunquan Qu, Guanghui Wang)</author>
      <guid isPermaLink="false">2505.15015v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents</title>
      <link>http://arxiv.org/abs/2505.14727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了从直觉驱动投资到AI驱动系统追求超额收益的演变过程，提出了一个涵盖手动策略、统计模型、经典机器学习、深度学习和基于大型语言模型的代理架构的五阶段分类法。&lt;h4&gt;背景&lt;/h4&gt;投资追求超越市场基准的超额收益经历了深刻变化，从直觉驱动投资转向自主的AI系统。&lt;h4&gt;目的&lt;/h4&gt;提供一套综合的五阶段分类法，追踪投资策略的演变过程，并评估成熟度、基础设施的协调以及下一代alpha系统的负责任发展。&lt;h4&gt;方法&lt;/h4&gt;采用系统水平的方法，整合了表示学习、多模态数据融合和工具增强的大型语言模型代理的进展。&lt;h4&gt;主要发现&lt;/h4&gt;强调了从静态预测器到具备实时推理、场景模拟和跨模态决策能力的上下文感知金融代理的战略转变，并考察了可解释性、数据脆弱性、治理和监管合规等关键挑战。&lt;h4&gt;结论&lt;/h4&gt;提出的分类法为评估成熟度、协调基础设施和指导下一代alpha系统的负责任发展提供了一个统一的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pursuit of alpha returns that exceed market benchmarks has undergone aprofound transformation, evolving from intuition-driven investing toautonomous, AI powered systems. This paper introduces a comprehensive fivestage taxonomy that traces this progression across manual strategies,statistical models, classical machine learning, deep learning, and agenticarchitectures powered by large language models (LLMs). Unlike prior surveysfocused narrowly on modeling techniques, this review adopts a system levellens, integrating advances in representation learning, multimodal data fusion,and tool augmented LLM agents. The strategic shift from static predictors tocontextaware financial agents capable of real time reasoning, scenariosimulation, and cross modal decision making is emphasized. Key challenges ininterpretability, data fragility, governance, and regulatory compliance areascritical to production deployment are examined. The proposed taxonomy offers aunified framework for evaluating maturity, aligning infrastructure, and guidingthe responsible development of next generation alpha systems.</description>
      <author>example@mail.com (Mohammad Rubyet Islam)</author>
      <guid isPermaLink="false">2505.14727v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Flattening Hierarchies with Policy Bootstrapping</title>
      <link>http://arxiv.org/abs/2505.14975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的算法，通过自举子目标条件策略和优势加权重要性采样来训练平坦的（非层次化）目标条件策略，以解决离线目标条件强化学习（GCRL）在长时程任务中遇到的挑战。&lt;h4&gt;背景&lt;/h4&gt;离线GCRL在大型奖励免费轨迹数据集上预训练通用策略是一个有前景的方法，类似于用于训练计算机视觉和自然语言处理基础模型的自我监督目标。然而，由于奖励稀疏和折现的组合，将其扩展到更长的时程仍然具有挑战性。层次化强化学习方法在长时程目标达成任务上取得了强实验结果，但它们依赖于模块化、时间尺度特定的策略和子目标生成，这引入了额外的复杂性，并阻碍了向高维目标空间的扩展。&lt;h4&gt;目的&lt;/h4&gt;提出一种算法，通过自举子目标条件策略和优势加权重要性采样来训练平坦的非层次化目标条件策略，以解决GCRL在长时程任务中的扩展问题。&lt;h4&gt;方法&lt;/h4&gt;该方法消除了在（子）目标空间上使用生成模型的必要性，这是在大型状态空间中实现高维控制的关键。进一步地，展示了现有层次化和基于自举的方法对应于推导中的特定设计选择。&lt;h4&gt;主要发现&lt;/h4&gt;在一系列基于状态和像素的移动和操作基准测试中，该方法与最先进的离线GCRL算法相匹配或超过，并能扩展到先前方法失败的复杂、长时程任务。&lt;h4&gt;结论&lt;/h4&gt;提出的算法能够有效地扩展离线GCRL，使其适用于更复杂的任务，并且在多个基准测试中显示出优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Offline goal-conditioned reinforcement learning (GCRL) is a promisingapproach for pretraining generalist policies on large datasets of reward-freetrajectories, akin to the self-supervised objectives used to train foundationmodels for computer vision and natural language processing. However, scalingGCRL to longer horizons remains challenging due to the combination of sparserewards and discounting, which obscures the comparative advantages of primitiveactions with respect to distant goals. Hierarchical RL methods achieve strongempirical results on long-horizon goal-reaching tasks, but their reliance onmodular, timescale-specific policies and subgoal generation introducessignificant additional complexity and hinders scaling to high-dimensional goalspaces. In this work, we introduce an algorithm to train a flat(non-hierarchical) goal-conditioned policy by bootstrapping onsubgoal-conditioned policies with advantage-weighted importance sampling. Ourapproach eliminates the need for a generative model over the (sub)goal space,which we find is key for scaling to high-dimensional control in large statespaces. We further show that existing hierarchical and bootstrapping-basedapproaches correspond to specific design choices within our derivation. Acrossa comprehensive suite of state- and pixel-based locomotion and manipulationbenchmarks, our method matches or surpasses state-of-the-art offline GCRLalgorithms and scales to complex, long-horizon tasks where prior approachesfail.</description>
      <author>example@mail.com (John L. Zhou, Jonathan C. Kao)</author>
      <guid isPermaLink="false">2505.14975v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Topology-aware Detection and Localization of Distributed Denial-of-Service Attacks in Network-on-Chips</title>
      <link>http://arxiv.org/abs/2505.14898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用图神经网络（GNN）进行网络片（NoC）中的分布式拒绝服务（DDoS）攻击检测和定位的框架。&lt;h4&gt;背景&lt;/h4&gt;网络片（NoC）在现代芯片设计中实现核心之间的芯片内通信，但由于其共享通信结构，NoC成为了各种安全威胁的焦点，特别是异构和高性能计算平台。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以检测和定位NoC中的DDoS攻击，以应对其分布式特性和动态流量模式，这些特性常使得静态检测规则或简单分析无效。&lt;h4&gt;方法&lt;/h4&gt;将NoC建模为图，方法利用时空流量特征，通过GNN直接在原始的数据包延迟数据上学习复杂的流量依赖关系，无需人工干预。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够以高达99%的准确率检测和定位DDoS攻击，且在不同攻击策略下保持一致的性能。该方法对恶意IP的数量和位置、数据包注入速率、应用工作负载和架构配置（包括2D网格和3D TSV NoC）具有强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个可扩展、灵活且架构无关的防御机制，显著提高了未来SoC设计中芯片内通信的可用性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：网络片（NoC）允许现代系统芯片（SoC）设计中不同核心之间进行芯片内通信。由于其共享的通信结构，NoC已成为各种安全威胁的焦点，尤其是在异构和高性能计算平台上。在这些攻击中，分布式拒绝服务（DDoS）攻击发生在多个恶意实体合作以压倒和干扰对关键系统组件的访问时，可能导致严重的性能下降或服务完全中断。由于其在NoC中的分布式特性和动态流量模式，这些攻击特别难以检测，常常逃避静态检测规则或简单的分析。本文提出了一种使用图神经网络（GNN）进行拓扑感知的DDoS攻击检测和定位的框架，通过分析NoC流量模式。具体来说，通过将NoC建模为图，我们的方法利用时空流量特征以有效地识别和定位DDoS攻击。与依赖手工特征或阈值检测的先前工作不同，我们的基于GNN的方法直接在原始的数据包延迟数据上运行，学习复杂的流量依赖关系，无需人工干预。实验结果表明，我们的方法可以以高达99%的准确率检测和定位DDoS攻击，同时在不同的攻击策略下保持一致的性能。此外，该方法对恶意IP的数量和位置、数据包注入速率、应用工作负载和架构配置（包括2D网格和3D TSV NoC）具有强大的鲁棒性。我们的工作提供了一种可扩展、灵活且架构无关的防御机制，显著提高了未来SoC设计中芯片内通信的可用性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network-on-Chip (NoC) enables on-chip communication between diverse cores inmodern System-on-Chip (SoC) designs. With its shared communication fabric, NoChas become a focal point for various security threats, especially inheterogeneous and high-performance computing platforms. Among these attacks,Distributed Denial of Service (DDoS) attacks occur when multiple maliciousentities collaborate to overwhelm and disrupt access to critical systemcomponents, potentially causing severe performance degradation or completedisruption of services. These attacks are particularly challenging to detectdue to their distributed nature and dynamic traffic patterns in NoC, whichoften evade static detection rules or simple profiling. This paper presents aframework to conduct topology-aware detection and localization of DDoS attacksusing Graph Neural Networks (GNNs) by analyzing NoC traffic patterns.Specifically, by modeling the NoC as a graph, our method utilizesspatiotemporal traffic features to effectively identify and localize DDoSattacks. Unlike prior works that rely on handcrafted features orthreshold-based detection, our GNN-based approach operates directly on rawinter-flit delay data, learning complex traffic dependencies without manualintervention. Experimental results demonstrate that our approach can detect andlocalize DDoS attacks with high accuracy (up to 99\%) while maintainingconsistent performance under diverse attack strategies. Furthermore, theproposed method exhibits strong robustness across varying numbers andplacements of malicious IPs, different packet injection rates, applicationworkloads, and architectural configurations, including both 2D mesh and 3DTSV-based NoCs. Our work provides a scalable, flexible, andarchitecture-agnostic defense mechanism, significantly improving theavailability and trustworthiness of on-chip communication in future SoCdesigns.</description>
      <author>example@mail.com (Hansika Weerasena, Xiaoguo Jia, Prabhat Mishra)</author>
      <guid isPermaLink="false">2505.14898v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning</title>
      <link>http://arxiv.org/abs/2505.14938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SMS是一种结合3D高斯分层、视觉基础模型、视觉-语言模型和物理模拟的统一框架，用于实现无结构真实环境中自主机器人的物理推理和对象中心规划。&lt;h4&gt;背景&lt;/h4&gt;在无结构、真实世界环境中，自主机器人需要推理其行为对物理世界的影响，以有效运作。&lt;h4&gt;目的&lt;/h4&gt;提供一种无需重新学习基础物理动力学的通用物理推理和对象中心规划方法。&lt;h4&gt;方法&lt;/h4&gt;SMS结合了3D高斯分层进行场景重建、视觉基础模型进行语义分割、视觉-语言模型进行材料属性推断以及物理模拟来预测动作结果。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟领域迁移和真实世界实验中，SMS在台球灵感的操作任务和具有挑战性的多旋翼着陆场景中表现出稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;SMS展示了使用可微分渲染进行场景重建、基础模型进行语义理解和基于物理的模拟来实现不同环境中物理基础机器人规划的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robots must reason about the physical consequences of theiractions to operate effectively in unstructured, real-world environments. Wepresent Scan, Materialize, Simulate (SMS), a unified framework that combines 3DGaussian Splatting for accurate scene reconstruction, visual foundation modelsfor semantic segmentation, vision-language models for material propertyinference, and physics simulation for reliable prediction of action outcomes.By integrating these components, SMS enables generalizable physical reasoningand object-centric planning without the need to re-learn foundational physicaldynamics. We empirically validate SMS in a billiards-inspired manipulation taskand a challenging quadrotor landing scenario, demonstrating robust performanceon both simulated domain transfer and real-world experiments. Our resultshighlight the potential of bridging differentiable rendering for scenereconstruction, foundation models for semantic understanding, and physics-basedsimulation to achieve physically grounded robot planning across diversesettings.</description>
      <author>example@mail.com (Amine Elhafsi, Daniel Morton, Marco Pavone)</author>
      <guid isPermaLink="false">2505.14938v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Foundations of Unknown-aware Machine Learning</title>
      <link>http://arxiv.org/abs/2505.14933v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文针对开放世界部署中机器学习模型的可靠性和安全性问题，提出了算法和理论基础，以解决由分布不确定性和未知类别引起的可靠性问题，包括从标准神经网络到现代基础模型如大型语言模型（LLMs）。&lt;h4&gt;背景&lt;/h4&gt;确保机器学习模型在开放世界部署中的可靠性和安全性是人工智能安全领域的核心挑战。&lt;h4&gt;目的&lt;/h4&gt;开发新的框架，联合优化分布内准确性和对未见数据的可靠性，并使模型能够识别和处理新颖的输入，而无需标记的未知数据。&lt;h4&gt;方法&lt;/h4&gt;提出了新的异常值合成方法VOS、NPOS和DREAM-OOD，以在训练过程中生成信息丰富的未知数据。基于此，提出了SAL框架，利用未标记的野外数据在现实部署条件下增强异常值检测。还开发了HaloScope用于LLMs中的幻觉检测，MLLMGuard用于防御多模态模型中的恶意提示，以及数据清洗方法以去除用于更好对齐的人类反馈。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法表明，丰富的未标记数据可以用来识别和适应不可预见的数据，提供正式的可靠性保证。&lt;h4&gt;结论&lt;/h4&gt;这些贡献推动了未知感知学习作为一种新范式，并希望它能够在最小的人为努力下提高人工智能系统的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to address key reliability issues arising from distributional uncertainty and unknown classes, from standard neural networks to modern foundation models like large language models (LLMs). Traditional learning paradigms, such as empirical risk minimization (ERM), assume no distribution shift between training and inference, often leading to overconfident predictions on out-of-distribution (OOD) inputs. This thesis introduces novel frameworks that jointly optimize for in-distribution accuracy and reliability to unseen data. A core contribution is the development of an unknown-aware learning framework that enables models to recognize and handle novel inputs without labeled OOD data. We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to generate informative unknowns during training. Building on this, we present SAL, a theoretical and algorithmic framework that leverages unlabeled in-the-wild data to enhance OOD detection under realistic deployment conditions. These methods demonstrate that abundant unlabeled data can be harnessed to recognize and adapt to unforeseen inputs, providing formal reliability guarantees. The thesis also extends reliable learning to foundation models. We develop HaloScope for hallucination detection in LLMs, MLLMGuard for defending against malicious prompts in multimodal models, and data cleaning methods to denoise human feedback used for better alignment. These tools target failure modes that threaten the safety of large-scale models in deployment. Overall, these contributions promote unknown-aware learning as a new paradigm, and we hope it can advance the reliability of AI systems with minimal human efforts.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring the reliability and safety of machine learning models in open-worlddeployment is a central challenge in AI safety. This thesis develops bothalgorithmic and theoretical foundations to address key reliability issuesarising from distributional uncertainty and unknown classes, from standardneural networks to modern foundation models like large language models (LLMs).  Traditional learning paradigms, such as empirical risk minimization (ERM),assume no distribution shift between training and inference, often leading tooverconfident predictions on out-of-distribution (OOD) inputs. This thesisintroduces novel frameworks that jointly optimize for in-distribution accuracyand reliability to unseen data. A core contribution is the development of anunknown-aware learning framework that enables models to recognize and handlenovel inputs without labeled OOD data.  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, togenerate informative unknowns during training. Building on this, we presentSAL, a theoretical and algorithmic framework that leverages unlabeledin-the-wild data to enhance OOD detection under realistic deploymentconditions. These methods demonstrate that abundant unlabeled data can beharnessed to recognize and adapt to unforeseen inputs, providing formalreliability guarantees.  The thesis also extends reliable learning to foundation models. We developHaloScope for hallucination detection in LLMs, MLLMGuard for defending againstmalicious prompts in multimodal models, and data cleaning methods to denoisehuman feedback used for better alignment. These tools target failure modes thatthreaten the safety of large-scale models in deployment.  Overall, these contributions promote unknown-aware learning as a newparadigm, and we hope it can advance the reliability of AI systems with minimalhuman efforts.</description>
      <author>example@mail.com (Xuefeng Du)</author>
      <guid isPermaLink="false">2505.14933v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity</title>
      <link>http://arxiv.org/abs/2505.14725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了HR-VILAGE-3K3M数据库，这是一个集成了大量RNA-seq数据的资源，用于研究呼吸道病毒免疫。&lt;h4&gt;背景&lt;/h4&gt;呼吸道病毒感染是全球健康负担的来源，但细胞免疫反应的具体机制尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;开发一个AI准备好的、严格编目的数据库，以解决自然感染队列数据缺乏和疫苗接种试验数据分散的问题。&lt;h4&gt;方法&lt;/h4&gt;HR-VILAGE-3K3M数据库整合了来自66项研究的14,136个RNA-seq样本，包括来自全血、PBMCs和鼻拭子的微阵列、bulk RNA-seq和单细胞RNA-seq数据。&lt;h4&gt;主要发现&lt;/h4&gt;数据库支持疫苗接种反应者的预测建模和批次效应校正方法的评估，适用于系统免疫学应用和特征选择及迁移学习算法的基准测试。&lt;h4&gt;结论&lt;/h4&gt;HR-VILAGE-3K3M是呼吸道病毒免疫的最大纵向转录组资源，为可重复的AI驱动研究提供了一个平台，加速了系统免疫学和针对新兴病毒威胁的疫苗开发。&lt;h4&gt;翻译&lt;/h4&gt;摘要：呼吸道病毒感染是全球健康负担的来源，但驱动保护或病理的细胞免疫反应尚不清楚。自然感染队列通常缺乏暴露前的基线数据以及结构化的时间序列采样。相比之下，疫苗接种和疫苗试验产生了有洞察力的纵向转录组数据。然而，这些数据集在平台上的分散，以及不一致的元数据和预处理程序，阻碍了AI驱动的发现。为了解决这些挑战，我们开发了人类呼吸道病毒免疫纵向基因表达（HR-VILAGE-3K3M）数据库：一个AI准备好的、严格编目的数据集，集成了来自66项研究的3,178名受试者的14,136个RNA-seq配置文件，涵盖了超过2.56百万个细胞。该数据集涵盖了疫苗接种、接种和混合暴露，包括来自GEO、ImmPort和ArrayExpress的全血、PBMCs和鼻拭子的微阵列、bulk RNA-seq和单细胞RNA-seq数据。我们协调了受试者级别的元数据，标准化了结果指标，应用了统一的预处理流程，并严格的质量控制，并将所有数据对齐到官方基因符号。为了证明HR-VILAGE-3K3M的实用性，我们进行了疫苗接种反应者的预测建模，并评估了批次效应校正方法。除了这些初步演示之外，它支持多样化的系统免疫学应用和特征选择及迁移学习算法的基准测试。其规模和异质性也使其成为预训练人类免疫反应基础模型和多模态学习框架的理想选择。作为人类呼吸道病毒免疫的最大纵向转录组资源，它为可重复的AI驱动研究提供了一个平台，加速了系统免疫学和针对新兴病毒威胁的疫苗开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Respiratory viral infections pose a global health burden, yet the cellularimmune responses driving protection or pathology remain unclear. Naturalinfection cohorts often lack pre-exposure baseline data and structured temporalsampling. In contrast, inoculation and vaccination trials generate insightfullongitudinal transcriptomic data. However, the scattering of these datasetsacross platforms, along with inconsistent metadata and preprocessing procedure,hinders AI-driven discovery. To address these challenges, we developed theHuman Respiratory Viral Immunization LongitudinAl Gene Expression(HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset thatintegrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studiesencompassing over 2.56 million cells. Spanning vaccination, inoculation, andmixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cellRNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort,and ArrayExpress. We harmonized subject-level metadata, standardized outcomemeasures, applied unified preprocessing pipelines with rigorous qualitycontrol, and aligned all data to official gene symbols. To demonstrate theutility of HR-VILAGE-3K3M, we performed predictive modeling of vaccineresponders and evaluated batch-effect correction methods. Beyond these initialdemonstrations, it supports diverse systems immunology applications andbenchmarking of feature selection and transfer learning algorithms. Its scaleand heterogeneity also make it ideal for pretraining foundation models of thehuman immune response and for advancing multimodal learning frameworks. As thelargest longitudinal transcriptomic resource for human respiratory viralimmunization, it provides an accessible platform for reproducible AI-drivenresearch, accelerating systems immunology and vaccine development againstemerging viral threats.</description>
      <author>example@mail.com (Xuejun Sun, Yiran Song, Xiaochen Zhou, Ruilie Cai, Yu Zhang, Xinyi Li, Rui Peng, Jialiu Xie, Yuanyuan Yan, Muyao Tang, Prem Lakshmanane, Baiming Zou, James S. Hagood, Raymond J. Pickles, Didong Li, Fei Zou, Xiaojing Zheng)</author>
      <guid isPermaLink="false">2505.14725v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Evaluation of Graph-based News Detection Using Network Structural Information</title>
      <link>http://arxiv.org/abs/2505.14453v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Proceedings of the ACM SIGKDD Conference on Knowledge  Discovery and Data Mining 2025 (KDD 2025). 14 pages, 7 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SI2AF的新型对抗攻击框架，用于评估和增强基于图神经网络的虚假新闻检测器的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在虚假新闻检测中显示出巨大潜力，但它们在社会网络中容易受到对抗性操纵。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来挑战基于图的检测器，并进一步探究其检测鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入结构熵来量化社交互动中的动态不确定性，并识别包含所有用户账户和新闻帖子的层次社区。2. 提出影响力指标来衡量每个账户进行随机互动的概率，以设计管理不同恶意账户的多个代理。3. 通过关联子图内的多代理协作，为每个目标新闻开发三种攻击策略，以优化对黑盒检测器的规避。4. 通过SI2AF生成的对抗性操纵丰富原始网络结构，并细化基于图的检测器以提高其对抗攻击的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;SI2AF在攻击有效性方面显著优于最先进的基线，平均提高了16.71%，并且平均提高了41.54%的基于GNN的检测鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SI2AF框架能够有效挑战基于图的检测器，并显著提高其对抗攻击的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络（GNNs）在虚假新闻检测中显示出巨大的潜力，但它们在社会网络中仍然容易受到对抗性操纵。现有的方法主要在恶意账户和个体目标新闻之间建立联系，以研究基于图检测器的脆弱性，但它们忽略了围绕目标的结构关系，限制了其在鲁棒性评估中的有效性。在本工作中，我们提出了一种名为SI2AF的新型结构信息原理指导的对抗攻击框架，该框架有效地挑战了基于图的检测器，并进一步探究了其检测鲁棒性。具体来说，引入了结构熵来量化社交互动中的动态不确定性，并识别包含所有用户账户和新闻帖子的层次社区。提出了一种影响力指标来衡量每个账户进行随机互动的概率，以设计管理不同恶意账户的多个代理。对于每个目标新闻，通过关联子图内的多代理协作开发了三种攻击策略，以优化对黑盒检测器的规避。通过整合SI2AF生成的对抗性操纵，丰富了原始网络结构，并细化了基于图的检测器，以提高其对抗攻击的鲁棒性。广泛的评估表明，SI2AF在攻击有效性方面显著优于最先进的基线，平均提高了16.71%，并且平均提高了41.54%的基于GNN的检测鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Graph Neural Networks (GNNs) have shown promising potential in fakenews detection, they remain highly vulnerable to adversarial manipulationswithin social networks. Existing methods primarily establish connectionsbetween malicious accounts and individual target news to investigate thevulnerability of graph-based detectors, while they neglect the structuralrelationships surrounding targets, limiting their effectiveness in robustnessevaluation. In this work, we propose a novel Structural Informationprinciples-guided Adversarial Attack Framework, namely SI2AF, which effectivelychallenges graph-based detectors and further probes their detection robustness.Specifically, structural entropy is introduced to quantify the dynamicuncertainty in social engagements and identify hierarchical communities thatencompass all user accounts and news posts. An influence metric is presented tomeasure each account's probability of engaging in random interactions,facilitating the design of multiple agents that manage distinct maliciousaccounts. For each target news, three attack strategies are developed throughmulti-agent collaboration within the associated subgraph to optimize evasionagainst black-box detectors. By incorporating the adversarial manipulationsgenerated by SI2AF, we enrich the original network structure and refinegraph-based detectors to improve their robustness against adversarial attacks.Extensive evaluations demonstrate that SI2AF significantly outperformsstate-of-the-art baselines in attack effectiveness with an average improvementof 16.71%, and enhances GNN-based detection robustness by 41.54% on average.</description>
      <author>example@mail.com (Xianghua Zeng, Hao Peng, Angsheng Li)</author>
      <guid isPermaLink="false">2505.14453v2</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>This Time is Different: An Observability Perspective on Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2505.14766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Toto，一个具有1.51亿参数的时间序列预测基础模型，并提出了BOOM，一个包含2807个真实世界时间序列的350百万观察值的大规模基准。&lt;h4&gt;背景&lt;/h4&gt;针对多变量可观测时间序列数据中的特定挑战，Toto采用了一种现代的仅解码器架构，并结合了创新性的架构设计。&lt;h4&gt;目的&lt;/h4&gt;旨在通过Toto模型实现时间序列预测的高性能，并通过BOOM基准评估模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;Toto使用混合了可观测数据、开放数据集和合成数据的预训练语料库，其规模是领先时间序列基础模型的4-10倍。BOOM基准的数据来自Datadog的内部可观测性指标。&lt;h4&gt;主要发现&lt;/h4&gt;Toto在BOOM基准和现有的通用时间序列预测基准上均达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;Toto和BOOM的模型权重、推理代码、评估脚本以及BOOM的数据和评估代码都作为开源项目在Apache 2.0许可下提供。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10 times larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general-purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/datadog/toto&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Toto, a time series forecasting foundation model with 151million parameters. Toto uses a modern decoder-only architecture coupled witharchitectural innovations designed to account for specific challenges found inmultivariate observability time series data. Toto's pre-training corpus is amixture of observability data, open datasets, and synthetic data, and is4-10$\times$ larger than those of leading time series foundation models.Additionally, we introduce BOOM, a large-scale benchmark consisting of 350million observations across 2,807 real-world time series. For both Toto andBOOM, we source observability data exclusively from Datadog's own telemetry andinternal observability metrics. Extensive evaluations demonstrate that Totoachieves state-of-the-art performance on both BOOM and on established generalpurpose time series forecasting benchmarks. Toto's model weights, inferencecode, and evaluation scripts, as well as BOOM's data and evaluation code, areall available as open source under the Apache 2.0 License available athttps://huggingface.co/Datadog/Toto-Open-Base-1.0 andhttps://github.com/DataDog/toto.</description>
      <author>example@mail.com (Ben Cohen, Emaad Khwaja, Youssef Doubli, Salahidine Lemaachi, Chris Lettieri, Charles Masson, Hugo Miccinilli, Elise Ramé, Qiqi Ren, Afshin Rostamizadeh, Jean Ogier du Terrail, Anna-Monica Toon, Kan Wang, Stephan Xie, David Asker, Ameet Talwalkar, Othmane Abou-Amal)</author>
      <guid isPermaLink="false">2505.14766v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Cooperative Causal GraphSAGE</title>
      <link>http://arxiv.org/abs/2505.14748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CoCa-GraphSAGE（合作因果图SAGE），这是一种结合合作博弈论与因果图SAGE的改进方法，以解决Causal GraphSAGE忽视采样节点间合作关系的问题。&lt;h4&gt;背景&lt;/h4&gt;GraphSAGE是一个广泛使用的图神经网络，引入因果推理后的Causal GraphSAGE提高了其鲁棒性能，但Causal GraphSAGE主要关注个体节点的因果权重，而忽略了采样节点整体的合作关系。&lt;h4&gt;目的&lt;/h4&gt;提出CoCa-GraphSAGE的目的是为了解决Causal GraphSAGE在采样节点整体合作关系上的不足，提高模型在复杂网络中的性能。&lt;h4&gt;方法&lt;/h4&gt;本文构建了一个基于图结构的合作因果结构模型，并提出了CoCa-sampling算法，该算法使用Shapley值根据节点的因果权重计算合作贡献。CoCa-sampling引导选择具有显著合作因果效应的节点，在邻域采样过程中整合具有合作关系的邻域特征，从而以整体采样的节点生成更稳定的目标节点嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法与比较方法具有可比的分类性能，并且在扰动下表现更优，这证明了CoCa-sampling在提高鲁棒性方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;CoCa-GraphSAGE通过结合合作博弈论和因果推理，有效提高了图神经网络的鲁棒性能，特别是在处理复杂网络中的采样节点合作关系时表现突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GraphSAGE is a widely used graph neural network. The introduction of causalinference has improved its robust performance and named as Causal GraphSAGE.However, Causal GraphSAGE focuses on measuring causal weighting amongindividual nodes, but neglecting the cooperative relationships among samplingnodes as a whole. To address this issue, this paper proposes Cooperative CausalGraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with CausalGraphSAGE. Initially, a cooperative causal structure model is constructed inthe case of cooperation based on the graph structure. Subsequently, CooperativeCausal sampling (CoCa-sampling) algorithm is proposed, employing the Shapleyvalues to calculate the cooperative contribution based on causal weights of thenodes sets. CoCa-sampling guides the selection of nodes with significantcooperative causal effects during the neighborhood sampling process, thusintegrating the selected neighborhood features under cooperative relationships,which takes the sampled nodes as a whole and generates more stable target nodeembeddings. Experiments on publicly available datasets show that the proposedmethod has comparable classification performance to the compared methods andoutperforms under perturbations, demonstrating the robustness improvement byCoCa-sampling.</description>
      <author>example@mail.com (Zaifa Xue, Tao Zhang, Tuo Xu, Huaixin Liang, Le Gao)</author>
      <guid isPermaLink="false">2505.14748v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis</title>
      <link>http://arxiv.org/abs/2505.14716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种分布式混合量子经典管道，用于提高骨骨折诊断的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;骨骨折是全球主要的致残原因，给医疗系统带来了巨大的临床和经济负担。传统的X射线解读耗时且易出错，而现有的机器学习和深度学习解决方案通常需要大量的特征工程、大型标注数据集和高计算资源。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了一个分布式混合量子经典管道，以提高骨骨折诊断的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;首先使用主成分分析（PCA）进行降维，然后利用4量子比特振幅编码电路进行特征增强。通过融合8个PCA导出的特征和8个量子增强特征形成一个16维向量，然后使用不同的机器学习模型进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;使用公共多区域X射线数据集，在99%的准确率上与最先进的迁移学习模型相当，同时将特征提取时间减少了82%。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法在提高骨骨折诊断准确性的同时，显著减少了特征提取时间，具有临床和经济效益。&lt;h4&gt;翻译&lt;/h4&gt;Bone fractures are a leading cause of morbidity and disability worldwide, imposing significant clinical and economic burdens on healthcare systems. Traditional X-ray interpretation is time consuming and error prone, while existing machine learning and deep learning solutions often demand extensive feature engineering, large, annotated datasets, and high computational resources. To address these challenges, a distributed hybrid quantum classical pipeline is proposed that first applies Principal Component Analysis (PCA) for dimensionality reduction and then leverages a 4 qubit quantum amplitude encoding circuit for feature enrichment. By fusing eight PCA derived features with eight quantum enhanced features into a 16 dimensional vector and then classifying with different machine learning models achieving 99% accuracy using a public multi region X-ray dataset on par with state of the art transfer learning models while reducing feature extraction time by 82%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bone fractures are a leading cause of morbidity and disability worldwide,imposing significant clinical and economic burdens on healthcare systems.Traditional X ray interpretation is time consuming and error prone, whileexisting machine learning and deep learning solutions often demand extensivefeature engineering, large, annotated datasets, and high computationalresources. To address these challenges, a distributed hybrid quantum classicalpipeline is proposed that first applies Principal Component Analysis (PCA) fordimensionality reduction and then leverages a 4 qubit quantum amplitudeencoding circuit for feature enrichment. By fusing eight PCA derived featureswith eight quantum enhanced features into a 16 dimensional vector and thenclassifying with different machine learning models achieving 99% accuracy usinga public multi region X ray dataset on par with state of the art transferlearning models while reducing feature extraction time by 82%.</description>
      <author>example@mail.com (Sahil Tomar, Rajeshwar Tripathi, Sandeep Kumar)</author>
      <guid isPermaLink="false">2505.14716v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Multivariate Long-Term History Representation for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.14737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LMHR的框架，用于多变量时间序列（MTS）预测，以解决现有方法在建模长期空间时间相似性和相关性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测在工业和学术领域有广泛应用，而空间时间图神经网络（STGNN）在建模空间时间相关性方面取得了进展，但大多数STGNN在计算复杂度限制下主要关注短期和局部空间时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，提出了一种名为LMHR的框架，旨在提高MTS预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;该框架包括一个长期历史编码器（LHEncoder）来有效地将长期历史编码为段级上下文表示并减少点级噪声；一个非参数分层表示检索器（HRetriever）来包含空间信息并提取最有价值的表示；以及一个基于Transformer的聚合器（TAggregator），它根据排名位置嵌入有效地融合稀疏检索到的上下文表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LMHR在平均预测范围内比典型的STGNN提高了10.72%，在多个真实世界数据集上比最先进的方法提高了4.12%，并且在数据集的前10%快速变化模式上，预测准确性提高了9.8%。&lt;h4&gt;结论&lt;/h4&gt;LMHR框架有效地提高了MTS预测的准确性，尤其是在处理长期空间时间依赖性方面表现突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate Time Series (MTS) forecasting has a wide range of applicationsin both industry and academia. Recent advances in Spatial-Temporal Graph NeuralNetwork (STGNN) have achieved great progress in modelling spatial-temporalcorrelations. Limited by computational complexity, most STGNNs for MTSforecasting focus primarily on short-term and local spatial-temporaldependencies. Although some recent methods attempt to incorporate univariatehistory into modeling, they still overlook crucial long-term spatial-temporalsimilarities and correlations across MTS, which are essential for accurateforecasting. To fill this gap, we propose a framework called the Long-termMultivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectivelyencode the long-term history into segment-level contextual representations andreduce point-level noise. A non-parametric Hierarchical RepresentationRetriever (HRetriever) is designed to include the spatial information in thelong-term spatial-temporal dependency modelling and pick out the most valuablerepresentations with no additional training. A Transformer-based Aggregator(TAggregator) selectively fuses the sparsely retrieved contextualrepresentations based on the ranking positional embedding efficiently.Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%on the average prediction horizons and state-of-the-art methods by 4.12% onseveral real-world datasets. Additionally, it consistently improves predictionaccuracy by 9.8% on the top 10% of rapidly changing patterns across thedatasets.</description>
      <author>example@mail.com (Huiliang Zhang, Di Wu, Arnaud Zinflou, Stephane Dellacherie, Mouhamadou Makhtar Dione, Benoit Boulet)</author>
      <guid isPermaLink="false">2505.14737v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection</title>
      <link>http://arxiv.org/abs/2505.15519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于网络数字孪生和人工智能技术的高频无线系统阻塞检测（视距识别）方法，通过整合信息新鲜度度量（AoI）和光线追踪技术，提高了动态无线环境中的实时阻塞检测能力。&lt;h4&gt;背景&lt;/h4&gt;可靠的高频通信链路，尤其是易受遮挡的链路，视距识别至关重要。&lt;h4&gt;目的&lt;/h4&gt;提高高频无线系统（如6GHz以上）的阻塞检测（视距识别）的可靠性。&lt;h4&gt;方法&lt;/h4&gt;通过整合AoI度量，增强网络数字孪生，实现动态无线环境中的可靠实时阻塞检测。利用光线追踪技术自动收集和标记大规模信道数据，针对环境变化进行优化。将AoI与损失函数结合，优先处理最近信息，以微调深度学习模型应对性能退化。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市模拟中，所提出的方法展示了输入分辨率、计算成本和模型性能之间的权衡。信道样本尺寸从（32, 1024）减少到4x8，沿角度和子载波维度提高了32倍的计算速度。通过仅使用1%的数据样本，成功缓解了性能退化，实现了自动和快速的模型漂移缓解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高高频无线系统的阻塞检测能力，同时优化了计算效率和数据处理。&lt;h4&gt;翻译&lt;/h4&gt;摘要：视距（LoS）识别对于确保可靠的高频通信链路至关重要，尤其是那些易受遮挡的链路。网络数字孪生和人工智能是使高频无线系统（例如，6GHz以上）的阻塞检测（LoS识别）成为可能的关键技术。在这项工作中，我们通过整合信息新鲜度（AoI）度量，一种对状态更新新鲜度的量化，增强了网络数字孪生，使得在动态无线环境中能够实现可靠的实时阻塞检测（LoS识别）。通过整合光线追踪技术，我们自动收集和标记大规模信道数据，针对环境变化进行优化。引入的AoI与损失函数结合，以优先处理最近信息，以便在性能退化（模型漂移）的情况下微调深度学习模型。所提出解决方案的有效性在真实城市模拟中得到证明，突出了输入分辨率、计算成本和模型性能之间的权衡。沿角度和子载波维度从原始信道样本大小（32, 1024）减少到4x8的分辨率降低了4倍，计算速度提高了32倍。所提出的微调成功地缓解了性能退化，同时仅需要可用数据样本的1%，实现了自动和快速的模型漂移缓解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Line-of-Sight (LoS) identification is crucial to ensure reliablehigh-frequency communication links, especially those vulnerable to blockages.Network Digital Twins and Artificial Intelligence are key technologies enablingblockage detection (LoS identification) for high-frequency wireless systems,e.g., 6&gt;GHz. In this work, we enhance Network Digital Twins by incorporatingAge of Information (AoI) metrics, a quantification of status update freshness,enabling reliable real-time blockage detection (LoS identification) in dynamicwireless environments. By integrating raytracing techniques, we automatelarge-scale collection and labeling of channel data, specifically tailored tothe evolving conditions of the environment. The introduced AoI is integratedwith the loss function to prioritize more recent information to fine-tune deeplearning models in case of performance degradation (model drift). Theeffectiveness of the proposed solution is demonstrated in realistic urbansimulations, highlighting the trade-off between input resolution, computationalcost, and model performance. A resolution reduction of 4x8 from an originalchannel sample size of (32, 1024) along the angle and subcarrier dimensionresults in a computational speedup of 32 times. The proposed fine-tuningsuccessfully mitigates performance degradation while requiring only 1% of theavailable data samples, enabling automated and fast mitigation of model drifts.</description>
      <author>example@mail.com (Michele Zhu, Francesco Linsalata, Silvia Mura, Lorenzo Cazzella, Damiano Badini, Umberto Spagnolini)</author>
      <guid isPermaLink="false">2505.15519v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>A fast and automated approach for urban CFD simulations: integration with meteorological predictions and its application to drone flights</title>
      <link>http://arxiv.org/abs/2505.14703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种快速自动的重建城市环境中空气流动的方法，利用LiDAR和地籍数据结合计算流体动力学（CFD）模拟，并应用于城市风模拟研究。&lt;h4&gt;背景&lt;/h4&gt;近年来，有多个研究提出了城市风模拟的新方法和应用。&lt;h4&gt;目的&lt;/h4&gt;目的是模拟城市环境中风与建筑物、植被、水域和地形形态之间的复杂相互作用。&lt;h4&gt;方法&lt;/h4&gt;方法将气象预测与计算技术相结合，引入基于气象预测的精确边界条件，简化了CFD城市模拟中的几何创建过程。同时，使用气象站的真实数据验证了模拟结果，并通过风洞方法验证了无人机与提取的风流之间的相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果与CFD模型生成的结果高度一致，风向和风速的一致性相关系数分别达到0.985和0.853。使用该方法进行风洞模拟，与将无人机嵌入完整城市景观的最直接方法相比，计算时间有了显著提高。&lt;h4&gt;结论&lt;/h4&gt;这项研究推动了城市CFD建模的进步，并对各种应用具有重大意义，为城市开发提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在过去的几年里，有多个研究提出了城市风模拟的新方法和应用。在本文中，我们提出了一种使用LiDAR和地籍数据结合计算流体动力学（CFD）模拟的快速自动方法，用于重建城市环境中的空气流动。我们的方法将气象预测与计算技术相结合，引入基于气象预测的精确边界条件，简化了CFD城市模拟中的几何创建过程，这是CFD城市模拟中最普遍的问题之一。将模拟结果与从气象站获得的真实数据进行了比较，结果显示与所提出的CFD模型生成的结果高度一致，风向和风速的一致性相关系数分别达到0.985和0.853。然后，使用这些模拟结果验证了一种风洞方法，该方法模仿了移动无人机与提取的风流之间的相互作用，与将无人机嵌入完整城市景观的最直接方法相比，计算时间有了显著提高。这项研究推动了城市CFD建模的进步，并对各种应用具有重大意义，为城市开发提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In past years, several studies have proposed new methods and applications forurban wind simulations. In this article, we present a fast and automaticmethodology for reconstructing airflows within urban environments using LiDARand cadastral data coupled with Computational Fluid Dynamics (CFD) simulations.Our approach integrates meteorological predictions with computationaltechniques to simulate the complex interactions between wind currents,buildings, vegetation, water zones and terrain morphology within urbanenvironments. Accurate boundary conditions based on meteorological predictionsare introduced into a coupled methodology that directly creates the terrainshape inside the simulation environment, simplifying the geometry creationprocess, which is one of the most prevalent problems in CFD urban simulations.The simulation results are confronted against ground-truth real data obtainedfrom a meteorological station, showing strong agreement with the outcomesgenerated by the proposed CFD model, with a concordance correlation coefficientup to $\rho_c = 0.985$ for the wind direction and $\rho_c = 0.853$ for the windspeed. The results from these simulations are then used for validating a windtunnel approach that mimics the interaction between a moving drone and theextracted wind currents, demonstrating a great improvement in computation timeswhen compared to the most straightforward approach that consists in embeddingthe drone within the full urban landscape. This research contributes to theadvancement of urban CFD modeling, and it has significant implications forvarious applications, providing valuable insights for urban development.</description>
      <author>example@mail.com (Marcos Suárez-Vázquez, Sylvana Varela Ballesta, Alberto Otero-Cacho, Alberto P. Muñuzuri, Jorge Mira)</author>
      <guid isPermaLink="false">2505.14703v1</guid>
      <pubDate>Thu, 22 May 2025 14:27:07 +0800</pubDate>
    </item>
    <item>
      <title>Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics</title>
      <link>http://arxiv.org/abs/2505.12709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Gadget，这是一种针对非独立同分布图数据的渐进式领域自适应（GDA）框架，旨在解决图神经网络在分布转移上的脆弱性，特别是在源图和目标图之间存在大范围转移的情况下。&lt;h4&gt;背景&lt;/h4&gt;图神经网络虽然在性能上表现出色，但容易受到图上分布转移的影响。现有的图领域自适应方法通常隐式地假设源图和目标图之间存在轻微的转移，这限制了它们在存在大范围转移的真实世界场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在提出一种适用于非独立同分布图数据的GDA框架，能够处理大范围的图上分布转移。&lt;h4&gt;方法&lt;/h4&gt;本文采用融合的Gromov-Wasserstein（FGW）距离作为非独立同分布图的领域差异度量，并基于此推导出误差界限，表明目标域误差与路径长度成正比。此外，通过误差界限的引导，识别出FGW测地线作为最优路径，并提出了一个有效的算法来生成这个路径。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，生成的路径可以无缝地集成到现有的图领域自适应方法中，以处理图上的大范围转移，从而显著提高节点分类的准确性，在实际数据集上比最先进的图领域自适应方法提高了高达6.8%。&lt;h4&gt;结论&lt;/h4&gt;本文提出的Gadget框架能够有效地处理非独立同分布图数据的大范围分布转移，为图神经网络在真实世界场景中的应用提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks, despite their impressive performance, are highlyvulnerable to distribution shifts on graphs. Existing graph domain adaptation(graph DA) methods often implicitly assume a \textit{mild} shift between sourceand target graphs, limiting their applicability to real-world scenarios with\textit{large} shifts. Gradual domain adaptation (GDA) has emerged as apromising approach for addressing large shifts by gradually adapting the sourcemodel to the target domain via a path of unlabeled intermediate domains.Existing GDA methods exclusively focus on independent and identicallydistributed (IID) data with a predefined path, leaving their extension to\textit{non-IID graphs without a given path} an open challenge. To bridge thisgap, we present Gadget, the first GDA framework for non-IID graph data. First(\textit{theoretical foundation}), the Fused Gromov-Wasserstein (FGW) distanceis adopted as the domain discrepancy for non-IID graphs, based on which, wederive an error bound revealing that the target domain error is proportional tothe length of the path. Second (\textit{optimal path}), guided by the errorbound, we identify the FGW geodesic as the optimal path, which can beefficiently generated by our proposed algorithm. The generated path can beseamlessly integrated with existing graph DA methods to handle large shifts ongraphs, improving state-of-the-art graph DA methods by up to 6.8\% in nodeclassification accuracy on real-world datasets.</description>
      <author>example@mail.com (Zhichen Zeng, Ruizhong Qiu, Wenxuan Bao, Tianxin Wei, Xiao Lin, Yuchen Yan, Tarek F. Abdelzaher, Jiawei Han, Hanghang Tong)</author>
      <guid isPermaLink="false">2505.12709v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
  <item>
      <title>Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</title>
      <link>http://arxiv.org/abs/2505.14449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为隐式人口统计学推断（IDI）的模块，用于在分类语音情感识别（SER）中提高公平性，并通过伪标签化和无监督学习减少偏差。&lt;h4&gt;背景&lt;/h4&gt;随着计算研究对子群体差异和性能偏差的关注增加，分类语音情感识别（SER）中的公平性仍被低估。现有方法通常依赖于难以获取的显式人口统计学标签。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在提高分类语音情感识别（SER）中的公平性，同时减少子群体差异。&lt;h4&gt;方法&lt;/h4&gt;本文引入了隐式人口统计学推断（IDI）模块，该模块利用预训练模型的伪标签化和k-means聚类进行无监督学习，以减少SER中的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，伪标签化IDI减少了子群体差异，公平性指标提高了超过33%，同时SER准确率下降了不到3%。无监督IDI在公平性指标上提高了超过26%，同时SER性能下降了不到4%。进一步分析显示，无监督IDI一致地减轻了种族和年龄差异，展示了在显式人口统计学信息不可用场景中的潜力。&lt;h4&gt;结论&lt;/h4&gt;隐式人口统计学推断（IDI）模块在提高分类语音情感识别（SER）的公平性方面具有潜力，即使在显式人口统计学信息不可用的情况下也能有效减少偏差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While subgroup disparities and performance bias are increasingly studied incomputational research, fairness in categorical Speech Emotion Recognition(SER) remains underexplored. Existing methods often rely on explicitdemographic labels, which are difficult to obtain due to privacy concerns. Toaddress this limitation, we introduce an Implicit Demography Inference (IDI)module that leverages pseudo-labeling from a pre-trained model and unsupervisedlearning using k-means clustering to mitigate bias in SER. Our experiments showthat pseudo-labeling IDI reduces subgroup disparities, improving fairnessmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, theunsupervised IDI yields more than a 26% improvement in fairness metrics with adrop of less than 4% in SER performance. Further analyses reveal that theunsupervised IDI consistently mitigates race and age disparities, demonstratingits potential in scenarios where explicit demographic information isunavailable.</description>
      <author>example@mail.com (Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee)</author>
      <guid isPermaLink="false">2505.14449v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Egocentric Action-aware Inertial Localization in Point Clouds</title>
      <link>http://arxiv.org/abs/2505.14346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EgocentricAction-aware Inertial Localization (EAIL)的新型惯性定位框架，该框架利用头戴式IMU信号中的自体动作线索来在3D点云中定位目标个体。&lt;h4&gt;背景&lt;/h4&gt;由于IMU传感器噪声导致轨迹漂移，以及人类动作的多样性引入了各种运动模式，使得人类惯性定位具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出EAIL框架，通过学习IMU信号中的自体动作线索与空间环境结构之间的相关性，以补偿定位漂移。&lt;h4&gt;方法&lt;/h4&gt;EAIL框架通过分层多模态对齐学习这些相关性，并假设环境的三维点云可用，通过对比学习来学习模态编码器，将IMU信号中的短期自体动作线索与点云中的局部环境特征对齐。然后，这些编码器用于推理时间和空间上的IMU数据和点云以执行惯性定位。&lt;h4&gt;主要发现&lt;/h4&gt;这些编码器还可以用于识别相应的动作序列作为副产品。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验证明了所提出框架在惯性定位和惯性动作识别方面优于现有基准。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为EgocentricAction-aware Inertial Localization (EAIL)的新型惯性定位框架，该框架利用头戴式IMU信号中的自体动作线索来在3D点云中定位目标个体。由于IMU传感器噪声导致轨迹漂移以及人类动作的多样性引入了各种运动模式，使得人类惯性定位具有挑战性。EAIL框架通过学习IMU信号中的自体动作线索与空间环境结构之间的相关性，以补偿定位漂移。该框架通过分层多模态对齐学习这些相关性，并假设环境的三维点云可用，通过对比学习来学习模态编码器，将IMU信号中的短期自体动作线索与点云中的局部环境特征对齐。然后，这些编码器用于推理时间和空间上的IMU数据和点云以执行惯性定位。有趣的是，这些编码器还可以用于识别相应的动作序列作为副产品。广泛的实验证明了所提出框架在惯性定位和惯性动作识别方面优于现有基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mf-zhang/ego-inertial-localization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel inertial localization framework named EgocentricAction-aware Inertial Localization (EAIL), which leverages egocentric actioncues from head-mounted IMU signals to localize the target individual within a3D point cloud. Human inertial localization is challenging due to IMU sensornoise that causes trajectory drift over time. The diversity of human actionsfurther complicates IMU signal processing by introducing various motionpatterns. Nevertheless, we observe that some actions observed through thehead-mounted IMU correlate with spatial environmental structures (e.g., bendingdown to look inside an oven, washing dishes next to a sink), thereby serving asspatial anchors to compensate for the localization drift. The proposed EAILframework learns such correlations via hierarchical multi-modal alignment. Byassuming that the 3D point cloud of the environment is available, itcontrastively learns modality encoders that align short-term egocentric actioncues in IMU signals with local environmental features in the point cloud. Theseencoders are then used in reasoning the IMU data and the point cloud over timeand space to perform inertial localization. Interestingly, these encoders canfurther be utilized to recognize the corresponding sequence of actions as aby-product. Extensive experiments demonstrate the effectiveness of the proposedframework over state-of-the-art inertial localization and inertial actionrecognition baselines.</description>
      <author>example@mail.com (Mingfang Zhang, Ryo Yonetani, Yifei Huang, Liangyang Ouyang, Ruicong Liu, Yoichi Sato)</author>
      <guid isPermaLink="false">2505.14346v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Emerging Properties in Unified Multimodal Pretraining</title>
      <link>http://arxiv.org/abs/2505.14683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BAGEL是一个开源的基础模型，支持多模态理解和生成，在标准基准测试中显著优于开源统一模型，展现了自由图像操作、未来帧预测、3D操作和世界导航等高级多模态推理能力。&lt;h4&gt;背景&lt;/h4&gt;多模态理解和生成在高端私有系统中表现出色。&lt;h4&gt;目的&lt;/h4&gt;引入BAGEL模型，以支持多模态理解和生成。&lt;h4&gt;方法&lt;/h4&gt;BAGEL是一个统一、仅解码器模型，在万亿个从大规模混合文本、图像、视频和网页数据中精心挑选的标记上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;BAGEL在复杂多模态推理方面展现出新兴能力，包括自由图像操作、未来帧预测、3D操作和世界导航。&lt;h4&gt;结论&lt;/h4&gt;为了促进多模态研究，研究者分享了关键发现、预训练细节、数据创建协议，并发布了代码和检查点。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一个名为BAGEL的开源基础模型，该模型原生支持多模态理解和生成。BAGEL是一个统一的、仅解码器模型，在万亿个来自大规模混合文本、图像、视频和网页数据中精心挑选的标记上进行预训练。当使用多样化的多模态混合数据扩展时，BAGEL在复杂多模态推理方面展现出新兴能力。因此，它在标准基准测试中在多模态生成和理解方面显著优于开源统一模型，并展示了自由图像操作、未来帧预测、3D操作和世界导航等高级多模态推理能力。为了促进进一步的多模态研究机会，研究者分享了关键发现、预训练细节、数据创建协议，并将代码和检查点发布到社区。项目页面位于https://bagel-ai.org/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unifying multimodal understanding and generation has shown impressivecapabilities in cutting-edge proprietary systems. In this work, we introduceBAGEL, an open0source foundational model that natively supports multimodalunderstanding and generation. BAGEL is a unified, decoder0only model pretrainedon trillions of tokens curated from large0scale interleaved text, image, video,and web data. When scaled with such diverse multimodal interleaved data, BAGELexhibits emerging capabilities in complex multimodal reasoning. As a result, itsignificantly outperforms open-source unified models in both multimodalgeneration and understanding across standard benchmarks, while exhibitingadvanced multimodal reasoning abilities such as free-form image manipulation,future frame prediction, 3D manipulation, and world navigation. In the hope offacilitating further opportunities for multimodal research, we share the keyfindings, pretraining details, data creation protocal, and release our code andcheckpoints to the community. The project page is at https://bagel-ai.org/</description>
      <author>example@mail.com (Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan)</author>
      <guid isPermaLink="false">2505.14683v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime</title>
      <link>http://arxiv.org/abs/2505.14323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了训练数据重建攻击，通过构造神经网络学习逆映射来恢复模型训练数据。分析了在大量训练数据下，差分隐私技术可以防御此类攻击，但在小数据集下，DP-SGD方法会严重影响分类器准确性。&lt;h4&gt;背景&lt;/h4&gt;训练数据重建攻击允许攻击者恢复发布模型的部分训练数据。已有研究表明，有信息的攻击者可以利用发布模型的权重和除一个训练数据点外的所有数据点实现高质量的重建。&lt;h4&gt;目的&lt;/h4&gt;研究在更现实的情况下，即攻击者只知道小训练数据集的分布，并攻击在此数据集上训练的迁移学习神经网络，探讨如何防御此类攻击。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种攻击方法，在现实威胁模型下有效，并使用Neyman-Pearson引理构建接收器操作特性曲线，以评估重建的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;在小数据集情况下，DP-SGD方法无法防御攻击而不严重影响分类器准确性。攻击对VGG、EfficientNet和ResNet图像分类器有效，这些分类器分别迁移学习于MNIST、CIFAR-10和CelebA数据集。&lt;h4&gt;结论&lt;/h4&gt;当保护训练数据至关重要时，使用此类迁移学习分类器存在严重风险。常用的重建成功率指标无法可靠地量化实际重建效果，而应使用Neyman-Pearson引理评估。&lt;h4&gt;翻译&lt;/h4&gt;Training data reconstruction attacks enable adversaries to recover portions of a released model's training data. We consider the attacks where a constructor neural network learns to invert the (random) mapping between training data and model weights. Prior work has shown that an informed adversary with access to released model's weights and all but one training data point can achieve high-quality reconstructions in this way. However, differential privacy can defend against such an attack with little to no loss in model's utility when the amount of training data is sufficiently large. In this work we consider a more realistic adversary who only knows the distribution from which a small training dataset has been sampled and who attacks a transfer-learned neural network classifier that has been trained on this dataset. We exhibit an attack that works in this realistic threat model and demonstrate that in the small-data regime it cannot be defended against by DP-SGD without severely damaging the classifier accuracy. This raises significant concerns about the use of such transfer-learned classifiers when protection of training-data is paramount. We demonstrate the effectiveness and robustness of our attack on VGG, EfficientNet and ResNet image classifierstransfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we point out that the commonly used (true-positive) reconstruction success rate metric fails to reliably quantify the actual reconstruction effectiveness. Instead, we make use of the Neyman-Pearson lemma to construct the receiver operating characteristic curve and consider the associated true-positive reconstruction rate at a fixed level of the false-positive reconstruction rate.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training data reconstruction attacks enable adversaries to recover portionsof a released model's training data. We consider the attacks where areconstructor neural network learns to invert the (random) mapping betweentraining data and model weights. Prior work has shown that an informedadversary with access to released model's weights and all but one training datapoint can achieve high-quality reconstructions in this way. However,differential privacy can defend against such an attack with little to no lossin model's utility when the amount of training data is sufficiently large. Inthis work we consider a more realistic adversary who only knows thedistribution from which a small training dataset has been sampled and whoattacks a transfer-learned neural network classifier that has been trained onthis dataset. We exhibit an attack that works in this realistic threat modeland demonstrate that in the small-data regime it cannot be defended against byDP-SGD without severely damaging the classifier accuracy. This raisessignificant concerns about the use of such transfer-learned classifiers whenprotection of training-data is paramount. We demonstrate the effectiveness androbustness of our attack on VGG, EfficientNet and ResNet image classifierstransfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, wepoint out that the commonly used (true-positive) reconstruction success ratemetric fails to reliably quantify the actual reconstruction effectiveness.Instead, we make use of the Neyman-Pearson lemma to construct the receiveroperating characteristic curve and consider the associated true-positivereconstruction rate at a fixed level of the false-positive reconstruction rate.</description>
      <author>example@mail.com (Tomasz Maciążek, Robert Allison)</author>
      <guid isPermaLink="false">2505.14323v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>InstanceBEV: Unifying Instance and BEV Representation for Global Modeling</title>
      <link>http://arxiv.org/abs/2505.13817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为InstanceBEV的新方法，用于构建基于鸟瞰图（BEV）的感知模型，以解决多视角相机构建占用网格图时数据复杂度增长的问题。&lt;h4&gt;背景&lt;/h4&gt;占用网格图在导航中被广泛使用，但由于数据复杂度的问题，现有的基于多视角相机的占用网络方法存在性能瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出InstanceBEV方法，以解决BEV方法在大规模全局建模中所需的工程优化问题。&lt;h4&gt;方法&lt;/h4&gt;InstanceBEV首次引入了实例级维度缩减，使用Transformer进行全局特征聚合，直接将全局特征图采样到3D空间中，而不依赖稀疏化或加速操作。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenOcc-NuScenes数据集上的实验表明，InstanceBEV在保持简单、高效的框架的同时，实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;InstanceBEV方法为自主驾驶中的全局建模提供了一种有效且高效的解决方案，为BEV方法的应用开辟了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy Grid Maps are widely used in navigation for their ability torepresent 3D space occupancy. However, existing methods that utilize multi-viewcameras to construct Occupancy Networks for perception modeling suffer fromcubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspectiveoffers a more practical solution for autonomous driving, as it provides highersemantic density and mitigates complex object occlusions. Nonetheless,BEV-based approaches still require extensive engineering optimizations toenable efficient large-scale global modeling. To address this challenge, wepropose InstanceBEV, the first method to introduce instance-leveldimensionality reduction for BEV, enabling global modeling with transformerswithout relying on sparsification or acceleration operators. Different fromother BEV methods, our approach directly employs transformers to aggregateglobal features. Compared to 3D object detection models, our method samplesglobal feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset showthat InstanceBEV achieves state-of-the-art performance while maintaining asimple, efficient framework without requiring additional optimizations.</description>
      <author>example@mail.com (Feng Li, Kun Xu, Zhaoyue Wang, Yunduan Cui, Mohammad Masum Billah, Jia Liu)</author>
      <guid isPermaLink="false">2505.13817v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium</title>
      <link>http://arxiv.org/abs/2505.14463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对抗攻击在图分析中的应用，并从三个角度探讨了图的内在对抗鲁棒性状态及其寻找方法。&lt;h4&gt;背景&lt;/h4&gt;对抗攻击在图分析领域受到越来越多的关注，已有两种对抗措施从图本身或图神经网络的角度来抵抗各种图对抗攻击。&lt;h4&gt;目的&lt;/h4&gt;探讨是否存在图的内在对抗鲁棒性状态，以及如何找到这种关键状态。&lt;h4&gt;方法&lt;/h4&gt;1) 将图上的对抗学习过程视为复杂的多目标动力学系统，并模拟对抗攻击行为；2) 提出一种广义理论框架，以证明存在临界对抗鲁棒性状态；3) 开发一个简化的单变量函数，以捕捉图在扰动下的动态变化，并通过求解动力系统的平衡点来确定临界状态。&lt;h4&gt;主要发现&lt;/h4&gt;通过多方面的实验，证明所提出的方法在五个常用的真实世界数据集和三种代表性攻击下，可以显著优于最先进的防御方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地识别和抵抗图对抗攻击，为图分析的安全性和鲁棒性提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Adversarial attacks to graph analytics are gaining increased attention. Todate, two lines of countermeasures have been proposed to resist various graph adversarial attacks from the perspectives of either graph per se or graph neural networks. Nevertheless, a fundamental question lies in whether there exists an intrinsic adversarial resilience state within a graph regime and how to find out such a critical state if exists. This paper contributes to tackle the above research questions from three unique perspectives: i) we regard the process of adversarial learning on graph as a complex multi-object dynamics system, and model the behavior of adversarial attack; ii) we propose a generalized theoretical framework to show the existence of critical adversarial resilience state; and iii) we develop a condensed one-dimensional function to capture the dynamic variation of graph regime under perturbations, and pinpoint the critical state through solving the equilibrium point of dynamic system. Multi-facet experiments are conducted to show our proposed approach can significantly outperform the state-of-the-art defense methods under five commonly-used real-world datasets and three representative attacks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial attacks to graph analytics are gaining increased attention. Todate, two lines of countermeasures have been proposed to resist various graphadversarial attacks from the perspectives of either graph per se or graphneural networks. Nevertheless, a fundamental question lies in whether thereexists an intrinsic adversarial resilience state within a graph regime and howto find out such a critical state if exists. This paper contributes to tacklethe above research questions from three unique perspectives: i) we regard theprocess of adversarial learning on graph as a complex multi-object dynamicsystem, and model the behavior of adversarial attack; ii) we propose ageneralized theoretical framework to show the existence of critical adversarialresilience state; and iii) we develop a condensed one-dimensional function tocapture the dynamic variation of graph regime under perturbations, and pinpointthe critical state through solving the equilibrium point of dynamic system.Multi-facet experiments are conducted to show our proposed approach cansignificantly outperform the state-of-the-art defense methods under fivecommonly-used real-world datasets and three representative attacks.</description>
      <author>example@mail.com (Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, Ling Liu)</author>
      <guid isPermaLink="false">2505.14463v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds</title>
      <link>http://arxiv.org/abs/2505.14366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late  Breaking Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种训练视觉语言模型（VLMs）进行视觉视角假设（VPT）的概念框架，这是实现人机交互（HRI）中具身认知的核心能力。为达到这一目标，我们引入了一个在NVIDIA Omniverse中生成的合成数据集，该数据集能够支持空间推理任务的监督学习。&lt;h4&gt;背景&lt;/h4&gt;视觉视角假设（VPT）是具身认知的核心能力，对于人机交互（HRI）至关重要。&lt;h4&gt;目的&lt;/h4&gt;构建一个概念框架，用于训练视觉语言模型（VLMs）以执行视觉视角假设（VPT），并开发一个支持空间推理任务的合成数据集。&lt;h4&gt;方法&lt;/h4&gt;创建一个合成数据集，包含RGB图像、自然语言描述和表示物体姿态的4x4变换矩阵。重点关注推断Z轴距离，未来将扩展到6个自由度（DOFs）的全推理。&lt;h4&gt;主要发现&lt;/h4&gt;提出的数据集公开可用，以支持进一步的研究，并为在交互式人机场景中具有空间理解能力的具身人工智能系统打下基础。&lt;h4&gt;结论&lt;/h4&gt;该研究为在交互式人机场景中实现空间理解能力的具身人工智能系统提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种训练视觉语言模型（VLMs）以执行视觉视角假设（VPT）的概念框架，这是实现具身认知的核心能力，对于人机交互（HRI）至关重要。作为这一目标的第一步，我们引入了一个在NVIDIA Omniverse中生成的合成数据集，该数据集能够支持空间推理任务的监督学习。每个实例包括一个RGB图像、一个自然语言描述和一个表示物体姿态的4x4变换矩阵。我们重点关注推断Z轴距离作为基础技能，未来将扩展到全6自由度（DOFs）推理。该数据集公开可用，以支持进一步的研究。这项工作为人机交互场景中具有空间理解能力的具身人工智能系统的发展奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a conceptual framework for training Vision-Language Models (VLMs)to perform Visual Perspective Taking (VPT), a core capability for embodiedcognition essential for Human-Robot Interaction (HRI). As a first step towardthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,that enables supervised learning for spatial reasoning tasks. Each instanceincludes an RGB image, a natural language description, and a ground-truth 4X4transformation matrix representing object pose. We focus on inferring Z-axisdistance as a foundational skill, with future extensions targeting full 6Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available tosupport further research. This work serves as a foundational step towardembodied AI systems capable of spatial understanding in interactive human-robotscenarios.</description>
      <author>example@mail.com (Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska)</author>
      <guid isPermaLink="false">2505.14366v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight</title>
      <link>http://arxiv.org/abs/2505.13921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为APEX的框架，旨在增强大型语言模型在物理交互建模方面的能力。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通过视觉语言模型或强化学习来整合感知和决策，但这些方法无法捕捉动态物体交互或需要特定任务的训练，限制了其在现实世界中的应用。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个框架，使LLMs能够进行基于物理的预见性任务规划。&lt;h4&gt;方法&lt;/h4&gt;APEX通过构建结构化图来识别和建模环境中最相关的动态交互，为LLMs提供明确的物理状态更新。同时，它提供低延迟的前向模拟，使LLMs能够根据预测结果而不是静态观察来选择最佳策略。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试中，APEX在感知、预测和决策方面均显著优于标准LLMs和基于VLM的模型，证明了显式物理推理在连接基于语言智能和现实世界任务执行之间的必要性。&lt;h4&gt;结论&lt;/h4&gt;APEX框架通过增强LLMs的物理预见性，提高了它们在现实世界任务执行中的表现。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces APEX (Anticipatory Physics-Enhanced Execution), a framework designed to enhance the physical interaction modeling capabilities of Large Language Models (LLMs). Existing methods, which integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. APEX equips LLMs with physics-driven foresight for real-time task planning by constructing structured graphs to identify and model the most relevant dynamic interactions in the environment, providing explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. The framework is evaluated on three benchmarks designed to assess perception, prediction, and decision-making, demonstrating significant improvements over standard LLMs and VLM-based models, highlighting the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) demonstrate strong reasoning and task planningcapabilities but remain fundamentally limited in physical interaction modeling.Existing approaches integrate perception via Vision-Language Models (VLMs) oradaptive decision-making through Reinforcement Learning (RL), but they fail tocapture dynamic object interactions or require task-specific training, limitingtheir real-world applicability. We introduce APEX (AnticipatoryPhysics-Enhanced Execution), a framework that equips LLMs with physics-drivenforesight for real-time task planning. APEX constructs structured graphs toidentify and model the most relevant dynamic interactions in the environment,providing LLMs with explicit physical state updates. Simultaneously, APEXprovides low-latency forward simulations of physically feasible actions,allowing LLMs to select optimal strategies based on predictive outcomes ratherthan static observations. We evaluate APEX on three benchmarks designed toassess perception, prediction, and decision-making: (1) Physics ReasoningBenchmark, testing causal inference and object motion prediction; (2) Tetris,evaluating whether physics-informed prediction enhances decision-makingperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,assessing the immediate integration of perception and action feasibilityanalysis. APEX significantly outperforms standard LLMs and VLM-based models,demonstrating the necessity of explicit physics reasoning for bridging the gapbetween language-based intelligence and real-world task execution. The sourcecode and experiment setup are publicly available athttps://github.com/hwj20/APEX_EXP .</description>
      <author>example@mail.com (Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh)</author>
      <guid isPermaLink="false">2505.13921v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation</title>
      <link>http://arxiv.org/abs/2505.14640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro,  Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的长视频理解（LVU）基准测试VideoEval-Pro，以解决现有LVU基准测试的不足，并评估了视频多模态模型（LMMs）在长视频理解方面的能力。&lt;h4&gt;背景&lt;/h4&gt;现有的LVU基准测试主要依赖于多项选择题（MCQs），其评估结果可能由于猜测正确答案而偏高，并且存在部分问题具有强烈的先验知识，使得模型无需阅读整个视频即可直接回答。&lt;h4&gt;目的&lt;/h4&gt;提出VideoEval-Pro基准测试，以更真实地评估LMMs的长视频理解能力。&lt;h4&gt;方法&lt;/h4&gt;VideoEval-Pro包含开放式简答题，真正需要理解整个视频才能回答。通过感知和推理任务评估段级别和全视频理解。&lt;h4&gt;主要发现&lt;/h4&gt;1. 视频LMMs在开放式问题上的表现比MCQs下降了25%以上；2. 在VideoEval-Pro上，MCQ分数高的模型不一定在开放式问题上有更高的分数；3. 与其他MCQ基准测试相比，VideoEval-Pro从增加输入帧数中获益更多。&lt;h4&gt;结论&lt;/h4&gt;VideoEval-Pro提供了一个更真实和可靠的长期视频理解度量，为该领域的发展提供了更清晰的视角。&lt;h4&gt;翻译&lt;/h4&gt;Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (&gt;25%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large multimodal models (LMMs) have recently emerged as a powerful tool forlong video understanding (LVU), prompting the development of standardized LVUbenchmarks to evaluate their performance. However, our investigation reveals arather sober lesson for existing LVU benchmarks. First, most existingbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluationresults are inflated due to the possibility of guessing the correct answer;Second, a significant portion of questions in these benchmarks have strongpriors to allow models to answer directly without even reading the input video.For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random framefrom a long video on Video-MME. We also observe that increasing the number offrames does not necessarily lead to improvement on existing benchmarks, whichis counterintuitive. As a result, the validity and robustness of current LVUbenchmarks are undermined, impeding a faithful assessment of LMMs' long-videounderstanding capability. To tackle this problem, we propose VideoEval-Pro, arealistic LVU benchmark containing questions with open-ended short-answer,which truly require understanding the entire video. VideoEval-Pro assesses bothsegment-level and full-video understanding through perception and reasoningtasks. By evaluating 21 proprietary and open-source video LMMs, we conclude thefollowing findings: (1) video LMMs show drastic performance ($&gt;$25\%) drops onopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores donot lead to higher open-ended scores on VideoEval-Pro; (3) compared to otherMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of inputframes. Our results show that VideoEval-Pro offers a more realistic andreliable measure of long video understanding, providing a clearer view ofprogress in this domain.</description>
      <author>example@mail.com (Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen)</author>
      <guid isPermaLink="false">2505.14640v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2505.14218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种灵活加权 Chamfer 距离（FCD）方法，用于指导点云生成，以解决传统 Chamfer 距离在点云补全任务中可能导致的整体性能看似很高但全局分布不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;Chamfer Distance（CD）在点云补全任务中被广泛用作生成点云与目标点云之间相似性的度量，同时由于其计算效率高，也常作为指导点云生成的目标函数。&lt;h4&gt;目的&lt;/h4&gt;为了解决使用固定权重计算 CD 作为目标函数时，可能导致的整体性能看似很高但全局分布不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种灵活加权 Chamfer 距离（FCD）方法，该方法对 CD 的全局分布组件赋予更高的权重，并采用灵活的加权策略来调整两个组件之间的平衡，旨在改善全局分布的同时保持整体性能的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FCD 在多个评估指标上均取得了优于传统方法的结果，包括 CD、EMD、DCD 和 F-Score，以及人类评估。&lt;h4&gt;结论&lt;/h4&gt;FCD 方法能够有效改善点云补全任务中的全局分布，同时保持整体性能的鲁棒性，是一种有效的点云生成指导方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chamfer Distance (CD) comprises two components that can evaluate the globaldistribution and local performance of generated point clouds, making it widelyutilized as a similarity measure between generated and target point clouds inpoint cloud completion tasks. Additionally, CD's computational efficiency hasled to its frequent application as an objective function for guiding pointcloud generation. However, using CD directly as an objective function withfixed equal weights for its two components can often result in seemingly highoverall performance (i.e., low CD score), while failing to achieve a goodglobal distribution. This is typically reflected in high Earth Mover's Distance(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor humanassessments. To address this issue, we propose a Flexible-Weighted ChamferDistance (FCD) to guide point cloud generation. FCD assigns a higher weight tothe global distribution component of CD and incorporates a flexible weightingstrategy to adjust the balance between the two components, aiming to improveglobal distribution while maintaining robust overall performance. Experimentalresults on two state-of-the-art networks demonstrate that our method achievessuperior results across multiple evaluation metrics, including CD, EMD, DCD,and F-Score, as well as in human evaluations.</description>
      <author>example@mail.com (Jie Li, Shengwei Tian, Long Yu, Xin Ning)</author>
      <guid isPermaLink="false">2505.14218v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Vox-Profile: A Speech Foundation Model Benchmark for Characterizing Diverse Speaker and Speech Traits</title>
      <link>http://arxiv.org/abs/2505.14648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Vox-Profile，这是一个用于描述说话者和语音特征的全面基准。&lt;h4&gt;背景&lt;/h4&gt;目前的研究工作多集中于说话者特征的单一维度。&lt;h4&gt;目的&lt;/h4&gt;Vox-Profile旨在提供全面的多维度说话者和语音特征描述。&lt;h4&gt;方法&lt;/h4&gt;该基准基于语音科学和语言学，由领域专家开发，用于准确索引说话者和语音特征。研究使用了超过15个公开的语音数据集和多种广泛使用的语音基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;Vox-Profile可以增强现有的语音识别数据集，以分析自动语音识别性能的变异性。它也被用作评估语音生成系统性能的工具。通过与人评估的比较，证明了自动生成的Vox-Profile的质量。&lt;h4&gt;结论&lt;/h4&gt;Vox-Profile是公开可用的，可以在https://github.com/tiantiaf0627/vox-profile-release找到。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Vox-Profile, a comprehensive benchmark to characterize richspeaker and speech traits using speech foundation models. Unlike existing worksthat focus on a single dimension of speaker traits, Vox-Profile providesholistic and multi-dimensional profiles that reflect both static speaker traits(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speechflow). This benchmark is grounded in speech science and linguistics, developedwith domain experts to accurately index speaker and speech characteristics. Wereport benchmark experiments using over 15 publicly available speech datasetsand several widely used speech foundation models that target various static anddynamic speaker and speech properties. In addition to benchmark experiments, weshowcase several downstream applications supported by Vox-Profile. First, weshow that Vox-Profile can augment existing speech recognition datasets toanalyze ASR performance variability. Vox-Profile is also used as a tool toevaluate the performance of speech generation systems. Finally, we assess thequality of our automated profiles through comparison with human evaluation andshow convergent validity. Vox-Profile is publicly available at:https://github.com/tiantiaf0627/vox-profile-release.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Vox-Profile, a comprehensive benchmark to characterize richspeaker and speech traits using speech foundation models. Unlike existing worksthat focus on a single dimension of speaker traits, Vox-Profile providesholistic and multi-dimensional profiles that reflect both static speaker traits(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speechflow). This benchmark is grounded in speech science and linguistics, developedwith domain experts to accurately index speaker and speech characteristics. Wereport benchmark experiments using over 15 publicly available speech datasetsand several widely used speech foundation models that target various static anddynamic speaker and speech properties. In addition to benchmark experiments, weshowcase several downstream applications supported by Vox-Profile. First, weshow that Vox-Profile can augment existing speech recognition datasets toanalyze ASR performance variability. Vox-Profile is also used as a tool toevaluate the performance of speech generation systems. Finally, we assess thequality of our automated profiles through comparison with human evaluation andshow convergent validity. Vox-Profile is publicly available at:https://github.com/tiantiaf0627/vox-profile-release.</description>
      <author>example@mail.com (Tiantian Feng, Jihwan Lee, Anfeng Xu, Yoonjeong Lee, Thanathai Lertpetchpun, Xuan Shi, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Dani Byrd, Najim Dehak, Shrikanth Narayanan)</author>
      <guid isPermaLink="false">2505.14648v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning for Semantic Alignment of Language, Audio, and Visual Modalities</title>
      <link>http://arxiv.org/abs/2505.14562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to European Signal Processing Conference (EUSIPCO 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种单阶段训练方法，使用对比学习框架语义对齐音频、视觉和文本三种模态。该方法利用大规模未标记数据学习共享表示，并展示了单阶段方法在音频视觉检索方面的优越性。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态对齐深度学习方法通常涉及两个阶段，分别对视觉-文本和音频-文本模态进行对齐，但这种方法由于数据分布不匹配，导致对齐效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种单阶段训练方法，通过对比学习框架语义对齐音频、视觉和文本三种模态，以提高多模态对齐的效果。&lt;h4&gt;方法&lt;/h4&gt;利用AVCaps数据集，该数据集提供了视频剪辑的音频、视觉和音频-视觉字幕，通过对比训练联合优化所有模态的表示。&lt;h4&gt;主要发现&lt;/h4&gt;单阶段方法在音频视觉检索方面优于两阶段方法，实现了两倍的性能提升，突出了统一的多模态表示学习的优势。&lt;h4&gt;结论&lt;/h4&gt;单阶段训练方法在多模态对齐中表现出色，能够有效提高音频视觉检索的性能，为多模态表示学习提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a single-stage training approach that semantically alignsthree modalities - audio, visual, and text using a contrastive learningframework. Contrastive training has gained prominence for multimodal alignment,utilizing large-scale unlabeled data to learn shared representations. Existingdeep learning approach for trimodal alignment involves two-stages, thatseparately align visual-text and audio-text modalities. This approach suffersfrom mismatched data distributions, resulting in suboptimal alignment.Leveraging the AVCaps dataset, which provides audio, visual and audio-visualcaptions for video clips, our method jointly optimizes the representation ofall the modalities using contrastive training. Our results demonstrate that thesingle-stage approach outperforms the two-stage method, achieving a two-foldimprovement in audio based visual retrieval, highlighting the advantages ofunified multimodal representation learning.</description>
      <author>example@mail.com (Parthasaarathy Sudarsanam, Irene Martín-Morató, Tuomas Virtanen)</author>
      <guid isPermaLink="false">2505.14562v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data</title>
      <link>http://arxiv.org/abs/2505.14272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的跨语言迁移学习方法，通过利用近邻检索技术来增强目标语言的少量标注数据，从而提高仇恨语言检测的性能。&lt;h4&gt;背景&lt;/h4&gt;检测仇恨语言的重要性日益凸显，但收集标注数据成本高且耗时，尤其在资源较少的语言中。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的跨语言迁移学习方法。&lt;h4&gt;方法&lt;/h4&gt;利用目标语言中少量标注数据，通过检索从大量多语言仇恨语言检测池中获取最相关的标注实例。&lt;h4&gt;主要发现&lt;/h4&gt;在八种语言上评估了该方法，发现它始终优于仅使用目标语言数据的模型，并且大多数情况下超过了当前最先进的方法。该方法在数据效率上表现卓越，在某些情况下只需检索200个实例即可保持优异的性能。此外，该方法可扩展，检索池可以轻松扩展，并且可以迅速适应新的语言和任务。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅数据效率高，而且可扩展，有助于提高仇恨语言检测的性能，尤其是在数据稀缺的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Considering the importance of detecting hateful language, labeled hate speechdata is expensive and time-consuming to collect, particularly for low-resourcelanguages. Prior work has demonstrated the effectiveness of cross-lingualtransfer learning and data augmentation in improving performance on tasks withlimited labeled data. To develop an efficient and scalable cross-lingualtransfer learning approach, we leverage nearest-neighbor retrieval to augmentminimal labeled data in the target language, thereby enhancing detectionperformance. Specifically, we assume access to a small set of labeled traininginstances in the target language and use these to retrieve the most relevantlabeled examples from a large multilingual hate speech detection pool. Weevaluate our approach on eight languages and demonstrate that it consistentlyoutperforms models trained solely on the target language data. Furthermore, inmost cases, our method surpasses the current state-of-the-art. Notably, ourapproach is highly data-efficient, retrieving as small as 200 instances in somecases while maintaining superior performance. Moreover, it is scalable, as theretrieval pool can be easily expanded, and the method can be readily adapted tonew languages and tasks. We also apply maximum marginal relevance to mitigateredundancy and filter out highly similar retrieved instances, resulting inimprovements in some languages.</description>
      <author>example@mail.com (Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser)</author>
      <guid isPermaLink="false">2505.14272v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions</title>
      <link>http://arxiv.org/abs/2505.14543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CHARM是一种用于多元时间序列的基础嵌入模型，它学习共享的、可迁移的和领域感知的表示。&lt;h4&gt;背景&lt;/h4&gt;传统的时序模型是任务特定的，通常依赖于特定数据集的培训和大量的特征工程。基于Transformer的架构虽然提高了可扩展性，但基础模型在时序领域的探索仍然有限，且主要限于预测。&lt;h4&gt;目的&lt;/h4&gt;CHARM旨在解决时序基础学习中的独特困难，并实现跨下游任务的最优性能。&lt;h4&gt;方法&lt;/h4&gt;CHARM通过整合通道级文本描述的架构创新，同时保持对通道顺序的不变性。模型使用联合嵌入预测架构（JEPA）进行训练，并采用新颖的增强方案和设计以改进可解释性和训练稳定性的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;CHARM在7M参数模型中实现了在多样化下游任务中的最先进性能，为时序表示学习设定了新的基准。&lt;h4&gt;结论&lt;/h4&gt;CHARM模型为时序基础学习提供了一种有效的方法，并在多个任务中达到了领先的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的时间序列模型是针对特定任务的，通常依赖于特定数据集的训练和大量的特征工程。虽然基于Transformer的架构提高了可扩展性，但在文本、视觉和音频中常见的基座模型在时序领域的探索仍然不足，并且主要限于预测。我们引入了CHARM，这是一种用于多元时间序列的基础嵌入模型，它学习共享的、可迁移的和领域感知的表示。为了解决时序基础学习中的独特困难，CHARM结合了架构创新，这些创新整合了通道级的文本描述，同时保持对通道顺序的不变性。该模型使用联合嵌入预测架构（JEPA）进行训练，并采用了新颖的增强方案和设计，以改进可解释性和训练稳定性。我们的7M参数模型在多样化的下游任务中实现了最先进的性能，为时序表示学习设定了新的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional time series models are task-specific and often depend ondataset-specific training and extensive feature engineering. WhileTransformer-based architectures have improved scalability, foundation models,commonplace in text, vision, and audio, remain under-explored for time seriesand are largely restricted to forecasting. We introduce $\textbf{CHARM}$, afoundation embedding model for multivariate time series that learns shared,transferable, and domain-aware representations. To address the uniquedifficulties of time series foundation learning, $\textbf{CHARM}$ incorporatesarchitectural innovations that integrate channel-level textual descriptionswhile remaining invariant to channel order. The model is trained using a JointEmbedding Predictive Architecture (JEPA), with novel augmentation schemes and aloss function designed to improve interpretability and training stability. Our$7$M-parameter model achieves state-of-the-art performance across diversedownstream tasks, setting a new benchmark for time series representationlearning.</description>
      <author>example@mail.com (Utsav Dutta, Sina Khoshfetrat Pakazad, Henrik Ohlsson)</author>
      <guid isPermaLink="false">2505.14543v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?</title>
      <link>http://arxiv.org/abs/2505.14321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文指出现有视频理解基准测试未能清晰区分模型的时间推理能力，并提出VBenchComp自动流程来分类问题，以更准确地评估视频LLM的能力。&lt;h4&gt;背景&lt;/h4&gt;现有视频理解基准测试将基于知识的和纯图像的问题混淆，未能明确隔离模型的时间推理能力，这是区分视频理解和其他模态的关键。&lt;h4&gt;目的&lt;/h4&gt;识别现有基准测试的局限性，并提出解决方案以更准确地评估视频LLM。&lt;h4&gt;方法&lt;/h4&gt;提出VBenchComp自动流程，将问题分为LLM-Answerable、Semantic和Temporal等不同领域，并分析模型在不同问题上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;发现模型在时间推理能力上的弱点，这些弱点在传统的整体评分中被隐藏。&lt;h4&gt;结论&lt;/h4&gt;VBenchComp能够实现视频LLM能力的细粒度评估，并为设计更准确的视频LLM基准测试提供见解和建议。&lt;h4&gt;翻译&lt;/h4&gt;Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating a model's temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that obscure whether higher scores truly indicate stronger understanding of the dynamic content in videos: (1) strong language priors, where models can answer questions without watching the video; and (2) shuffling invariance, where models maintain similar performance on certain questions even when video frames are temporally shuffled. To alleviate these issues, we propose VBenchComp, an automated pipeline that categorizes questions into different domains: LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions can be answered without viewing the video; Semantic questions remain answerable even when the video frames are shuffled; and Temporal questions require understanding the correct temporal order of frames. The rest of the questions are labeled as Others. This can enable fine-grained evaluation of different capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that are hidden by traditional overall scores, and we offer insights and recommendations for designing future benchmarks that more accurately assess video LLMs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing video understanding benchmarks often conflate knowledge-based andpurely image-based questions, rather than clearly isolating a model's temporalreasoning ability, which is the key aspect that distinguishes videounderstanding from other modalities. We identify two major limitations thatobscure whether higher scores truly indicate stronger understanding of thedynamic content in videos: (1) strong language priors, where models can answerquestions without watching the video; and (2) shuffling invariance, wheremodels maintain similar performance on certain questions even when video framesare temporally shuffled. To alleviate these issues, we propose VBenchComp, anautomated pipeline that categorizes questions into different domains:LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questionscan be answered without viewing the video; Semantic questions remain answerableeven when the video frames are shuffled; and Temporal questions requireunderstanding the correct temporal order of frames. The rest of the questionsare labeled as Others. This can enable fine-grained evaluation of differentcapabilities of a video LLM. Our analysis reveals nuanced model weaknesses thatare hidden by traditional overall scores, and we offer insights andrecommendations for designing future benchmarks that more accurately assessvideo LLMs.</description>
      <author>example@mail.com (Bo Feng, Zhengfeng Lai, Shiyu Li, Zizhen Wang, Simon Wang, Ping Huang, Meng Cao)</author>
      <guid isPermaLink="false">2505.14321v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Evaluation of Graph-based News Detection Using Network Structural Information</title>
      <link>http://arxiv.org/abs/2505.14453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SI2AF的新型对抗攻击框架，用于评估和增强基于图神经网络的虚假新闻检测器的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在虚假新闻检测方面显示出潜力，但它们在社交网络中容易受到对抗性攻击。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来有效挑战基于图神经网络的检测器，并进一步探究其检测鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入结构熵来量化社交互动中的动态不确定性，并识别包含所有用户账户和新闻帖子的层次社区。提出一个影响力指标来衡量每个账户参与随机互动的概率，以设计管理不同恶意账户的多个代理。通过在关联子图内进行多代理协作，为每个目标新闻开发三种攻击策略，以优化对黑盒检测器的逃避。通过整合SI2AF生成的对抗性操作，丰富原始网络结构，并细化基于图的检测器，以提高其对对抗性攻击的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;SI2AF在攻击有效性方面显著优于最先进的基线，平均提高了16.71%，并且平均提高了41.54%的基于GNN的检测鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SI2AF是一种有效的对抗攻击框架，可以显著提高基于图神经网络的虚假新闻检测器的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管图神经网络（GNNs）在虚假新闻检测方面显示出有希望的潜力，但它们在社交网络中仍然高度容易受到对抗性操作。现有的方法主要建立恶意账户和个体目标新闻之间的联系，以研究基于图检测器的脆弱性，而忽略了围绕目标的结构关系，限制了其在鲁棒性评估中的有效性。在这项工作中，我们提出了一种名为SI2AF的新型结构信息原则指导的对抗攻击框架，该框架有效地挑战了基于图的检测器，并进一步探究了它们的检测鲁棒性。具体来说，引入了结构熵来量化社交互动中的动态不确定性，并识别包含所有用户账户和新闻帖子的层次社区。提出了一种影响力指标来衡量每个账户参与随机互动的概率，以便设计管理不同恶意账户的多个代理。通过在相关子图内进行多代理协作，为每个目标新闻开发了三种攻击策略，以优化对黑盒检测器的逃避。通过整合SI2AF生成的对抗性操作，丰富了原始网络结构，并细化了基于图的检测器，以提高其对对抗性攻击的鲁棒性。广泛的评估表明，SI2AF在攻击有效性方面显著优于最先进的基线，平均提高了16.71%，并且平均提高了41.54%的基于GNN的检测鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Graph Neural Networks (GNNs) have shown promising potential in fakenews detection, they remain highly vulnerable to adversarial manipulationswithin social networks. Existing methods primarily establish connectionsbetween malicious accounts and individual target news to investigate thevulnerability of graph-based detectors, while they neglect the structuralrelationships surrounding targets, limiting their effectiveness in robustnessevaluation. In this work, we propose a novel Structural Informationprinciples-guided Adversarial Attack Framework, namely SI2AF, which effectivelychallenges graph-based detectors and further probes their detection robustness.Specifically, structural entropy is introduced to quantify the dynamicuncertainty in social engagements and identify hierarchical communities thatencompass all user accounts and news posts. An influence metric is presented tomeasure each account's probability of engaging in random interactions,facilitating the design of multiple agents that manage distinct maliciousaccounts. For each target news, three attack strategies are developed throughmulti-agent collaboration within the associated subgraph to optimize evasionagainst black-box detectors. By incorporating the adversarial manipulationsgenerated by SI2AF, we enrich the original network structure and refinegraph-based detectors to improve their robustness against adversarial attacks.Extensive evaluations demonstrate that SI2AF significantly outperformsstate-of-the-art baselines in attack effectiveness with an average improvementof 16.71%, and enhances GNN-based detection robustness by 41.54% on average.</description>
      <author>example@mail.com (Xianghua Zeng, Hao Peng, Angsheng Li)</author>
      <guid isPermaLink="false">2505.14453v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Solving Unit Commitment Problems with Graph Neural Network based Initial Commitment Prediction and Large Neighborhood Search</title>
      <link>http://arxiv.org/abs/2505.14408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种加速求解单位组合问题（UCP）的框架，并介绍了两种不同的图神经网络（GNN）策略的数据收集过程。&lt;h4&gt;背景&lt;/h4&gt;单位组合问题是电力市场决策的关键组成部分，但其计算复杂度要求高效的求解方法。&lt;h4&gt;目的&lt;/h4&gt;目的是通过提出的方法加速UCP的求解过程，并提高求解的准确性。&lt;h4&gt;方法&lt;/h4&gt;首先训练了一个神经初始承诺预测策略以获得UCP的初始承诺；其次，引入了启发式过程以恢复初始承诺的可行性；然后基于初始预测得到邻域，进行邻域搜索以改善承诺；最后，训练了一个神经邻域预测策略，在每个迭代中预测当前承诺的邻域，不断优化承诺，直到满足停止条件。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在80个单元的系统中训练的GNN策略在1080个单元的系统中优于商业求解器，LNS在更复杂的实例中表现优于商业求解器。&lt;h4&gt;结论&lt;/h4&gt;提出的方法可以产生高质量的初始承诺，可以迭代优化以满足更高的准确性要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unit commitment problem (UCP) is a critical component of power marketdecision-making. However, its computational complexity necessitates effi-cientsolution methods. In this work we propose a framework to accelerate the solvingprocess of the UCP, and the data collecting process for two dis-tinct graphneural network (GNN) policy. We at first train a Neural Initial CommitmentPrediction policy to obtain an initial commitment for UCP. Sec-ond, a heuristicprocess is introduced to restore the feasibility of the initial commitment.Third, get the neighborhood based on the initial prediction then neighborhoodsearch to improve the commitment. At last, we train a Neural neighborhoodPrediction policy to predict the neighborhood of the incum-bent commitment ateach iteration, continuously optimizing the commitment until the stoppingcondition is met. This approach produces high-quality ini-tial commitments thatcan be iteratively refined to meet higher accuracy re-quirements. Theexperimental results show that the GNN policies trained on the 80-unit systemoutperform commercial solvers on a 1080-unit system, and LNS performs betterthan commercial solver on more complex instanc-es.</description>
      <author>example@mail.com (Linfeng Yang, Peilun Li, Jinbao Jian)</author>
      <guid isPermaLink="false">2505.14408v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision</title>
      <link>http://arxiv.org/abs/2505.13905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为4D-ROLLS的弱监督占用估计方法，用于4D雷达，并使用激光雷达点云作为监督信号，旨在提高占用估计在恶劣环境下的性能。&lt;h4&gt;背景&lt;/h4&gt;占用估计在自动驾驶车辆中至关重要，但现有方法在烟雾、雨、雪和雾等恶劣环境下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的占用估计方法，以解决恶劣环境下的占用估计问题。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括：生成伪激光雷达标签，包括占用查询和激光雷达高度图，作为多阶段监督训练4D雷达占用估计模型；将模型与激光雷达生成的占用图对齐，以微调其在占用估计中的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;4D-ROLLS在恶劣环境下的鲁棒性和在跨数据集训练中的有效性得到了验证。该模型还可无缝迁移到下游任务，如BEV分割和点云占用预测，表明其具有更广泛的应用潜力。轻量级网络使得4D-ROLLS模型在4060 GPU上以约30 Hz的速度实现快速推理。&lt;h4&gt;结论&lt;/h4&gt;4D-ROLLS是一种有效的占用估计方法，适用于恶劣环境和多种下游任务，具有快速推理能力。&lt;h4&gt;翻译&lt;/h4&gt;A comprehensive understanding of 3D scenes is essential for autonomous vehicles (AVs), and among various perception tasks, occupancy estimation plays a central role by providing a general representation of drivable and occupied space. However, most existing occupancy estimation methods rely on LiDAR or cameras, which perform poorly in degraded environments such as smoke, rain, snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised occupancy estimation method for 4D radar using the LiDAR point cloud as the supervisory signal. Specifically, we introduce a method for generating pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as multi-stage supervision to train the 4D radar occupancy estimation model. Then the model is aligned with the occupancy map produced by LiDAR, fine-tuning its accuracy in occupancy estimation. Extensive comparative experiments validate the exceptional performance of 4D-ROLLS. Its robustness in degraded environments and effectiveness in cross-dataset training are qualitatively demonstrated. The model is also seamlessly transferred to downstream tasks BEV segmentation and point cloud occupancy prediction, highlighting its potential for broader applications. The lightweight network enables 4D-ROLLS model to achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of 4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A comprehensive understanding of 3D scenes is essential for autonomousvehicles (AVs), and among various perception tasks, occupancy estimation playsa central role by providing a general representation of drivable and occupiedspace. However, most existing occupancy estimation methods rely on LiDAR orcameras, which perform poorly in degraded environments such as smoke, rain,snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervisedoccupancy estimation method for 4D radar using the LiDAR point cloud as thesupervisory signal. Specifically, we introduce a method for generatingpseudo-LiDAR labels, including occupancy queries and LiDAR height maps, asmulti-stage supervision to train the 4D radar occupancy estimation model. Thenthe model is aligned with the occupancy map produced by LiDAR, fine-tuning itsaccuracy in occupancy estimation. Extensive comparative experiments validatethe exceptional performance of 4D-ROLLS. Its robustness in degradedenvironments and effectiveness in cross-dataset training are qualitativelydemonstrated. The model is also seamlessly transferred to downstream tasks BEVsegmentation and point cloud occupancy prediction, highlighting its potentialfor broader applications. The lightweight network enables 4D-ROLLS model toachieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.</description>
      <author>example@mail.com (Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou)</author>
      <guid isPermaLink="false">2505.13905v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards a Foundation Model for Communication Systems</title>
      <link>http://arxiv.org/abs/2505.14603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于通信数据的Transformer多模态模型，旨在解决通信数据处理的挑战，并展示了该模型在估计多个特征方面的成功。&lt;h4&gt;背景&lt;/h4&gt;人工智能在各个领域展现出前所未有的性能，其在通信系统中的应用成为研究热点。当前方法侧重于特定任务的解决方案，而人工智能的更广泛趋势是转向能够支持多个应用的大型通用模型。&lt;h4&gt;目的&lt;/h4&gt;构建一个用于通信数据的基础模型，该模型基于Transformer，能够直接处理通信数据。&lt;h4&gt;方法&lt;/h4&gt;提出了针对关键挑战的方法，包括分词、位置嵌入、多模态处理、可变特征大小和归一化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该模型能够成功估计多个特征，如传输等级、选择的预编码器、多普勒频移和延迟轮廓。&lt;h4&gt;结论&lt;/h4&gt;该Transformer多模态模型在通信数据处理方面具有潜力，能够有效估计多个关键特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial Intelligence (AI) has demonstrated unprecedented performanceacross various domains, and its application to communication systems is anactive area of research. While current methods focus on task-specificsolutions, the broader trend in AI is shifting toward large general modelscapable of supporting multiple applications. In this work, we take a steptoward a foundation model for communication data--a transformer-based,multi-modal model designed to operate directly on communication data. Wepropose methodologies to address key challenges, including tokenization,positional embedding, multimodality, variable feature sizes, and normalization.Furthermore, we empirically demonstrate that such a model can successfullyestimate multiple features, including transmission rank, selected precoder,Doppler spread, and delay profile.</description>
      <author>example@mail.com (Davide Buffelli, Sowmen Das, Yu-Wei Lin, Sattar Vakili, Chien-Yi Wang, Masoud Attarifar, Pritthijit Nath, Da-shan Shiu)</author>
      <guid isPermaLink="false">2505.14603v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.14471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscripts, accepted to KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为Citss的新型框架，用于解决学术引用分类中的挑战，并通过实验证明了其在基准数据集上的优越性。&lt;h4&gt;背景&lt;/h4&gt;引用分类对于学术分析至关重要，而预训练语言模型在引用分类数据集上的微调已被证明是一种有效的方法，但直接微调存在数据稀缺、上下文噪声和虚假关键词相关性等问题。&lt;h4&gt;目的&lt;/h4&gt;设计Citss框架以适应预训练语言模型，克服引用分类中的挑战。&lt;h4&gt;方法&lt;/h4&gt;Citss引入自监督对比学习来缓解数据稀缺，并配备两种特殊策略来获取对比对：句子级裁剪和关键词扰动。&lt;h4&gt;主要发现&lt;/h4&gt;Citss与基于编码器的PLMs和基于解码器的LLMs兼容，实验结果表明其在基准数据集上优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;Citss框架有效地解决了引用分类中的挑战，并在实验中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Citation classification, which identifies the intention behind academiccitations, is pivotal for scholarly analysis. Previous works suggestfine-tuning pretrained language models (PLMs) on citation classificationdatasets, reaping the reward of the linguistic knowledge they gained duringpretraining. However, directly fine-tuning for citation classification ischallenging due to labeled data scarcity, contextual noise, and spuriouskeyphrase correlations. In this paper, we present a novel framework, Citss,that adapts the PLMs to overcome these challenges. Citss introducesself-supervised contrastive learning to alleviate data scarcity, and isequipped with two specialized strategies to obtain the contrastive pairs:sentence-level cropping, which enhances focus on target citations within longcontexts, and keyphrase perturbation, which mitigates reliance on specifickeyphrases. Compared with previous works that are only designed forencoder-based PLMs, Citss is carefully developed to be compatible with bothencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlargedpretraining. Experiments with three benchmark datasets with both encoder-basedPLMs and decoder-based LLMs demonstrate our superiority compared to theprevious state of the art. Our code is available at: github.com/LITONG99/Citss</description>
      <author>example@mail.com (Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen)</author>
      <guid isPermaLink="false">2505.14471v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
      <link>http://arxiv.org/abs/2505.14125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为任务调制对比学习（TMCL）的新方法，该方法从生物大脑的学习机制中汲取灵感，旨在解决机器学习在自然学习设置中的灾难性遗忘问题，并通过少量标记数据提高学习效果。&lt;h4&gt;背景&lt;/h4&gt;生物大脑能够从无标签数据中持续学习，同时整合少量标记示例中的专业信息，而机器学习方法在这种自然学习设置中容易受到灾难性遗忘的影响，监督式专家微调会降低原始任务的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的学习方法，以解决机器学习在自然学习设置中的灾难性遗忘问题，并提高使用少量标记数据时的学习效果。&lt;h4&gt;方法&lt;/h4&gt;引入了任务调制对比学习（TMCL），该方法使用预测编码原理来持续且无监督地整合自上而下的信息，并利用对比损失构建一个视图不变的表现空间。当出现新类别的标记样本时，学习新的仿射调制来改善新类别与其他类别的分离，而不影响前馈权重。通过利用视图不变性学习机制，训练前馈权重以匹配数据样本的无调制表示与其调制对应物，从而在表现空间中引入调制不变性，并通过使用过去的调制来稳定它。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的无监督方法以及可比的监督方法相比，TMCL在类增量学习和迁移学习方面都有所改进，且只需使用1%的可用标签。&lt;h4&gt;结论&lt;/h4&gt;研究表明，自上而下的调制在平衡稳定性和可塑性方面起着至关重要的作用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：生物大脑能够从无标签数据流中持续学习，同时整合来自少量标记示例的专业信息，而不会牺牲其泛化能力。与此同时，机器学习方法在这种自然学习设置中容易受到灾难性遗忘的影响，监督式专家微调会降低原始任务的表现。我们引入了任务调制对比学习（TMCL），该方法从新皮层中的生物物理机制中汲取灵感，使用预测编码原理来持续且无监督地整合自上而下的信息。我们遵循这些原则构建一个视图不变的表现空间，并且可以使用对比损失来实现。然后，每当出现新类别的标记样本时，学习新的仿射调制来改善新类别与其他类别的分离，而不影响前馈权重。通过利用视图不变性学习机制，我们训练前馈权重以匹配数据样本的无调制表示与其调制对应物。这引入了调制不变性到表现空间中，并通过使用过去的调制来稳定它。我们的实验表明，在类增量学习和迁移学习方面，我们的方法与最先进的无监督方法以及可比的监督方法相比都有所改进，且只需使用1%的可用标签。总之，我们的工作表明，自上而下的调制在平衡稳定性和可塑性方面起着至关重要的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological brains learn continually from a stream of unlabeled data, whileintegrating specialized information from sparsely labeled examples withoutcompromising their ability to generalize. Meanwhile, machine learning methodsare susceptible to catastrophic forgetting in this natural learning setting, assupervised specialist fine-tuning degrades performance on the original task. Weintroduce task-modulated contrastive learning (TMCL), which takes inspirationfrom the biophysical machinery in the neocortex, using predictive codingprinciples to integrate top-down information continually and withoutsupervision. We follow the idea that these principles build a view-invariantrepresentation space, and that this can be implemented using a contrastiveloss. Then, whenever labeled samples of a new class occur, new affinemodulations are learned that improve separation of the new class from allothers, without affecting feedforward weights. By co-opting the view-invariancelearning mechanism, we then train feedforward weights to match the unmodulatedrepresentation of a data sample to its modulated counterparts. This introducesmodulation invariance into the representation space, and, by also using pastmodulations, stabilizes it. Our experiments show improvements in bothclass-incremental and transfer learning over state-of-the-art unsupervisedapproaches, as well as over comparable supervised approaches, using as few as1% of available labels. Taken together, our work suggests that top-downmodulations play a crucial role in balancing stability and plasticity.</description>
      <author>example@mail.com (Viet Anh Khoa Tran, Emre Neftci, Willem. A. M. Wybo)</author>
      <guid isPermaLink="false">2505.14125v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Graph Clustering with Deep Structural Entropy</title>
      <link>http://arxiv.org/abs/2505.14040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Proceedings of the ACM SIGKDD Conference on Knowledge  Discovery and Data Mining 2025 (KDD 2025). 13 pages, 10 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DeSE，一种结合了深度结构熵的全新无监督图聚类框架，用于改进图结构学习。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络、图注意力网络和对比学习方法在处理稀疏图或含噪声边时性能下降，且依赖于节点嵌入和传统聚类技术，可能无法完全捕捉节点间的图结构。&lt;h4&gt;目的&lt;/h4&gt;提出DeSE框架，旨在解决上述方法的局限性，通过引入深度结构熵来增强图的原始结构。&lt;h4&gt;方法&lt;/h4&gt;DeSE通过以下步骤实现：1）计算具有软分配的结构熵，以可微形式量化结构信息；2）设计结构学习层（SLL）从原始特征数据生成属性图，作为优化原始结构图的目标；3）聚类分配方法（ASS）基于图神经网络学习节点嵌入和软分配矩阵，在增强的图上进行聚类。&lt;h4&gt;主要发现&lt;/h4&gt;DeSE在四个基准数据集上与八个代表性的无监督图聚类基线进行了广泛的比较实验，证明了其在有效性和可解释性方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;DeSE框架能够有效提高图聚类任务的效果和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：关于图结构学习（GSL）的研究为基于图的聚类提供了关键见解。然而，当前的方法，如图神经网络（GNNs）、图注意力网络（GATs）和对比学习，通常严重依赖于原始图结构。当原始图的邻接矩阵过于稀疏或包含与聚类无关的噪声边时，它们的性能会下降。此外，这些方法依赖于学习节点嵌入并使用传统的k-means等技术来形成聚类，这可能无法完全捕捉节点间的潜在图结构。为了解决这些局限性，本文提出了一种名为DeSE的新颖无监督图聚类框架，该框架结合了深度结构熵。它通过增强原始图并使用深度神经网络来形成聚类，从而增强了原始图。具体而言，我们首先提出了一种使用软分配计算结构熵的方法，该方法以可微形式量化了结构信息。接下来，我们设计了一个结构学习层（SLL），它从原始特征数据生成属性图，作为增强和优化原始结构图的目标，从而减轻了图节点之间稀疏连接的问题。最后，我们的聚类分配方法（ASS）基于GNNs，学习节点嵌入和软分配矩阵，在增强的图上进行聚类。ASS层可以根据下游任务要求进行堆叠，最小化结构熵以实现稳定的聚类，并最大化节点与基于边的交叉熵损失的一致性。在四个基准数据集上对八个代表性的无监督图聚类基线进行了广泛的比较实验，证明了DeSE在有效性和可解释性方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Research on Graph Structure Learning (GSL) provides key insights forgraph-based clustering, yet current methods like Graph Neural Networks (GNNs),Graph Attention Networks (GATs), and contrastive learning often rely heavily onthe original graph structure. Their performance deteriorates when the originalgraph's adjacency matrix is too sparse or contains noisy edges unrelated toclustering. Moreover, these methods depend on learning node embeddings andusing traditional techniques like k-means to form clusters, which may not fullycapture the underlying graph structure between nodes. To address theselimitations, this paper introduces DeSE, a novel unsupervised graph clusteringframework incorporating Deep Structural Entropy. It enhances the original graphwith quantified structural information and deep neural networks to formclusters. Specifically, we first propose a method for calculating structuralentropy with soft assignment, which quantifies structure in a differentiableform. Next, we design a Structural Learning layer (SLL) to generate anattributed graph from the original feature data, serving as a target to enhanceand optimize the original structural graph, thereby mitigating the issue ofsparse connections between graph nodes. Finally, our clustering assignmentmethod (ASS), based on GNNs, learns node embeddings and a soft assignmentmatrix to cluster on the enhanced graph. The ASS layer can be stacked to meetdownstream task requirements, minimizing structural entropy for stableclustering and maximizing node consistency with edge-based cross-entropy loss.Extensive comparative experiments are conducted on four benchmark datasetsagainst eight representative unsupervised graph clustering baselines,demonstrating the superiority of the DeSE in both effectiveness andinterpretability.</description>
      <author>example@mail.com (Jingyun Zhang, Hao Peng, Li Sun, Guanlin Wu, Chunyang Liu, Zhengtao Yu)</author>
      <guid isPermaLink="false">2505.14040v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction</title>
      <link>http://arxiv.org/abs/2505.13856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SuperMapNet，一种用于构建长距离和高精度向量化的高精度地图（HD map）的方法。&lt;h4&gt;背景&lt;/h4&gt;向量化的HD地图对于自动驾驶至关重要。尽管近年来在此领域取得了显著进展，但仍存在一些主要问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了SuperMapNet。&lt;h4&gt;方法&lt;/h4&gt;SuperMapNet使用相机图像和LiDAR点云作为输入。它首先通过交叉注意力增强模块和基于流的差异对齐模块紧密耦合来自相机图像的语义信息和来自LiDAR点云的几何信息，以生成长距离的BEV特征。然后，通过三级交互紧密耦合局部特征和全局特征，进行高精度的分类和定位。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse2数据集上的实验表明，SuperMapNet的性能优于现有的SOTA方法，在困难/简单设置下的mAP分别超过14.9/8.8和18.5/3.1。&lt;h4&gt;结论&lt;/h4&gt;SuperMapNet能够有效地解决当前HD地图构建中的问题，为自动驾驶提供了更好的地图信息。&lt;h4&gt;翻译&lt;/h4&gt;Vectorized HD map is essential for autonomous driving. Significant progress has been made in this field in recent years, but there are still some major issues. In order to address these issues, this paper proposes SuperMapNet, a method for constructing long-range and high-accuracy vectorized HD map. SuperMapNet uses camera images and LiDAR point clouds as input, and tightly couples semantic information from camera images and geometric information from LiDAR point clouds to generate long-range BEV features. Then, local features and global features are tightly coupled through three-level interactions for high-accuracy classification and localization. Experiments on the nuScenes and Argoverse2 datasets demonstrate that SuperMapNet outperforms the existing SOTA methods, achieving an mAP of 14.9/8.8 and 18.5/3.1 under hard/easy settings, respectively. The code is publicly available.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vectorized HD map is essential for autonomous driving. Remarkable work hasbeen achieved in recent years, but there are still major issues: (1) in thegeneration of the BEV features, single modality-based methods are of limitedperception capability, while direct concatenation-based multi-modal methodsfail to capture synergies and disparities between different modalities,resulting in limited ranges with feature holes; (2) in the classification andlocalization of map elements, only point information is used without theconsideration of element infor-mation and neglects the interaction betweenpoint information and element information, leading to erroneous shapes andelement entanglement with low accuracy. To address above issues, we introduceSuperMapNet for long-range and high-accuracy vectorized HD map construction. Ituses both camera images and LiDAR point clouds as input, and first tightlycouple semantic information from camera images and geometric information fromLiDAR point clouds by a cross-attention based synergy enhancement module and aflow-based disparity alignment module for long-range BEV feature generation.And then, local features from point queries and global features from elementqueries are tightly coupled by three-level interactions for high-accuracyclassification and localization, where Point2Point interaction learns localgeometric information between points of the same element and of each point,Element2Element interaction learns relation constraints between differentelements and semantic information of each elements, and Point2Elementinteraction learns complement element information for its constituent points.Experiments on the nuScenes and Argoverse2 datasets demonstrate superiorperformances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP underhard/easy settings, respectively. The code is made publicly available1.</description>
      <author>example@mail.com (Ruqin Zhou, San Jiang, Wanshou Jiang, Yongsheng Zhang, Chenguang Dai)</author>
      <guid isPermaLink="false">2505.13856v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>RETRO: REthinking Tactile Representation Learning with Material PriOrs</title>
      <link>http://arxiv.org/abs/2505.14319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/weihaox/RETRO&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的触觉表征学习方法，通过引入材料感知先验，使触觉模型更好地捕捉和泛化表面纹理的细微差别，从而提高触觉反馈的准确性和丰富性。&lt;h4&gt;背景&lt;/h4&gt;现有触觉表征学习方法主要关注将触觉数据与视觉或文本信息对齐，而忽略了材料特性对触觉体验的重要性。&lt;h4&gt;目的&lt;/h4&gt;填补现有方法在触觉表征学习中的不足，通过引入材料感知先验来提高触觉模型的性能。&lt;h4&gt;方法&lt;/h4&gt;重新审视触觉表征学习框架，并在学习过程中融入材料感知先验，这些先验代表针对不同材料的预学习特性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法使触觉模型能够更准确地捕捉表面纹理的细微差别，并在不同材料和纹理上提供更丰富、更准确的触觉反馈。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在机器人、触觉反馈系统和材料编辑等实际应用中提高了触觉反馈的性能。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知深受接触物体表面特性的影响。然而，尽管这些材料特性在塑造触觉体验方面起着至关重要的作用，但它们在现有的触觉表征学习方法中却遭到了很大程度的忽视。大多数方法主要关注将触觉数据与视觉或文本信息对齐，而忽略了来自理解材料固有特性的丰富触觉反馈。在本研究中，我们通过重新审视触觉表征学习框架并融入材料感知先验来解决这个问题。这些先验代表针对不同材料的预学习特性，允许触觉模型更好地捕捉和泛化表面纹理的细微差别。我们的方法使触觉模型能够在不同材料和纹理上提供更准确、更丰富的触觉反馈，从而提高了在机器人、触觉反馈系统和材料编辑等现实应用中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile perception is profoundly influenced by the surface properties ofobjects in contact. However, despite their crucial role in shaping tactileexperiences, these material characteristics have been largely neglected inexisting tactile representation learning methods. Most approaches primarilyfocus on aligning tactile data with visual or textual information, overlookingthe richness of tactile feedback that comes from understanding the materials'inherent properties. In this work, we address this gap by revisiting thetactile representation learning framework and incorporating material-awarepriors into the learning process. These priors, which represent pre-learnedcharacteristics specific to different materials, allow tactile models to bettercapture and generalize the nuances of surface texture. Our method enables moreaccurate, contextually rich tactile feedback across diverse materials andtextures, improving performance in real-world applications such as robotics,haptic feedback systems, and material editing.</description>
      <author>example@mail.com (Weihao Xia, Chenliang Zhou, Cengiz Oztireli)</author>
      <guid isPermaLink="false">2505.14319v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning</title>
      <link>http://arxiv.org/abs/2505.13997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Spatiotemporal Preservation and Routing (StPR)的视频类增量学习框架，旨在解决在持续学习新动作类别时忘记先前知识的问题。&lt;h4&gt;背景&lt;/h4&gt;传统类增量学习方法难以在处理时空结构的同时避免灾难性遗忘，而现有方法要么依赖于示例重排，存在隐私和内存问题，要么是静态图像方法，忽略了时间建模。&lt;h4&gt;目的&lt;/h4&gt;提出一种无示例的统一框架，在保持先验知识的同时，有效捕获帧共享语义和时间动态。&lt;h4&gt;方法&lt;/h4&gt;引入了Frame-Shared Semantics Distillation (FSSD)来识别语义稳定和有意义的通道，并通过选择性正则化来维持先验知识。同时设计了基于时间分解的专家混合模型（TD-MoE），动态路由任务特定的专家。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在UCF101、HMDB51和Kinetics400数据集上优于现有基准，同时提高了可解释性和效率。&lt;h4&gt;结论&lt;/h4&gt;StPR框架有效地利用空间语义和时间动态，实现了无示例的视频类增量学习。&lt;h4&gt;翻译&lt;/h4&gt;Video Class-Incremental Learning (VCIL)试图开发模型，在时间上连续学习新的动作类别，而不忘记先前获得的知识。与传统的类增量学习（CIL）不同，VCIL引入了时空结构的附加复杂性，这使得在有效捕捉帧共享语义和时间动态的同时减轻灾难性遗忘变得特别具有挑战性。现有的方法要么依赖于示例重排，引发关于记忆和隐私的担忧，要么调整基于静态图像的方法，忽略了时间建模。为了解决这些限制，我们提出了Spatiotemporal Preservation and Routing (StPR)，这是一个统一的无示例VCIL框架，它明确地将时空信息分离并保留。首先，我们引入了Frame-Shared Semantics Distillation (FSSD)，通过联合考虑语义敏感性和分类贡献来识别语义稳定和有意义的通道。这些重要的语义通道被选择性地正则化，以保持先验知识的同时允许适应。其次，我们设计了基于时间分解的混合专家模型（TD-MoE），根据其时间动态动态路由特定任务的专家，使推理无需任务ID或存储的示例。Together, StPR有效地利用空间语义和时间动态，实现了统一的、无示例的VCIL框架。在UCF101、HMDB51和Kinetics400上的大量实验表明，我们的方法优于现有基线，同时提高了VCIL的可解释性和效率。代码可在补充材料中找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Class-Incremental Learning (VCIL) seeks to develop models thatcontinuously learn new action categories over time without forgettingpreviously acquired knowledge. Unlike traditional Class-Incremental Learning(CIL), VCIL introduces the added complexity of spatiotemporal structures,making it particularly challenging to mitigate catastrophic forgetting whileeffectively capturing both frame-shared semantics and temporal dynamics.Existing approaches either rely on exemplar rehearsal, raising concerns overmemory and privacy, or adapt static image-based methods that neglect temporalmodeling. To address these limitations, we propose Spatiotemporal Preservationand Routing (StPR), a unified and exemplar-free VCIL framework that explicitlydisentangles and preserves spatiotemporal information. First, we introduceFrame-Shared Semantics Distillation (FSSD), which identifies semanticallystable and meaningful channels by jointly considering semantic sensitivity andclassification contribution. These important semantic channels are selectivelyregularized to maintain prior knowledge while allowing for adaptation. Second,we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), whichdynamically routes task-specific experts based on their temporal dynamics,enabling inference without task ID or stored exemplars. Together, StPReffectively leverages spatial semantics and temporal dynamics, achieving aunified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51,and Kinetics400 show that our method outperforms existing baselines whileoffering improved interpretability and efficiency in VCIL. Code is available inthe supplementary materials.</description>
      <author>example@mail.com (Huaijie Wang, De Cheng, Guozhang Li, Zhipeng Xu, Lingfeng He, Jie Li, Nannan Wang, Xinbo Gao)</author>
      <guid isPermaLink="false">2505.13997v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction</title>
      <link>http://arxiv.org/abs/2505.13558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态数据的CAGRU模型，用于预测客户的购买意愿，旨在解决传统时间序列预测方法在处理不平衡客户群体时的局限性。&lt;h4&gt;背景&lt;/h4&gt;准确预测客户的购买意愿对商业策略的成功至关重要。现有研究主要关注分析客户可能购买的具体产品类型，而对客户是否参与回购行为的关注较少。&lt;h4&gt;目的&lt;/h4&gt;预测客户是否会进行下一次购买，这是一个经典的时间序列预测任务。本文旨在提出一种新的方法来解决现实购买行为中客户群体不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;本文提出的方法首先对客户进行特征分析并进行聚类，以区分具有相似特征的客户群体。然后，使用GRU神经网络提取不同客户群体的时间序列特征，并引入注意力机制以捕捉序列位置的重要性。此外，针对客户群体的头尾分布，模型对每个客户群体进行单独训练，以更准确地捕捉不同客户群体间的行为特征差异以及同一客户群体内客户的相似特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过构建四个数据集并开展广泛实验，本文证明了CAGRU方法在预测客户购买意愿方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;CAGRU模型能够有效解决传统时间序列预测方法在处理不平衡客户群体时的局限性，提高了客户购买意愿预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;Accurately predicting customers' purchase intentions is critical to the success of a business strategy. Current researches mainly focus on analyzing the specific types of products that customers are likely to purchase in the future, little attention has been paid to the critical factor of whether customers will engage in repurchase behavior. Predicting whether a customer will make the next purchase is a classic time series forecasting task. However, in real-world purchasing behavior, customer groups typically exhibit imbalance- i.e., there are a large number of occasional buyers and a small number of loyal customers. This head-to-tail distribution makes traditional time series forecasting methods face certain limitations when dealing with such problems. To address the above challenges, this paper proposes a unified Clustering and Attention mechanism GRU model (CAGRU) that leverages multi-modal data for customer purchase intention prediction. The framework first performs customer profiling with respect to the customer characteristics and clusters the customers to delineate the different customer clusters that contain similar features. Then, the time series features of different customer clusters are extracted by GRU neural network and an attention mechanism is introduced to capture the significance of sequence locations. Furthermore, to mitigate the head-to-tail distribution of customer segments, we train the model separately for each customer segment, to adapt and capture more accurately the differences in behavioral characteristics between different customer segments, as well as the similar characteristics of the customers within the same customer segment. We constructed four datasets and conducted extensive experiments to demonstrate the superiority of the proposed CAGRU approach.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting customers' purchase intentions is critical to thesuccess of a business strategy. Current researches mainly focus on analyzingthe specific types of products that customers are likely to purchase in thefuture, little attention has been paid to the critical factor of whethercustomers will engage in repurchase behavior. Predicting whether a customerwill make the next purchase is a classic time series forecasting task. However,in real-world purchasing behavior, customer groups typically exhibit imbalance- i.e., there are a large number of occasional buyers and a small number ofloyal customers. This head-to-tail distribution makes traditional time seriesforecasting methods face certain limitations when dealing with such problems.To address the above challenges, this paper proposes a unified Clustering andAttention mechanism GRU model (CAGRU) that leverages multi-modal data forcustomer purchase intention prediction. The framework first performs customerprofiling with respect to the customer characteristics and clusters thecustomers to delineate the different customer clusters that contain similarfeatures. Then, the time series features of different customer clusters areextracted by GRU neural network and an attention mechanism is introduced tocapture the significance of sequence locations. Furthermore, to mitigate thehead-to-tail distribution of customer segments, we train the model separatelyfor each customer segment, to adapt and capture more accurately the differencesin behavioral characteristics between different customer segments, as well asthe similar characteristics of the customers within the same customer segment.We constructed four datasets and conducted extensive experiments to demonstratethe superiority of the proposed CAGRU approach.</description>
      <author>example@mail.com (Yingjie Kuang, Tianchen Zhang, Zhen-Wei Huang, Zhongjie Zeng, Zhe-Yuan Li, Ling Huang, Yuefang Gao)</author>
      <guid isPermaLink="false">2505.13558v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents</title>
      <link>http://arxiv.org/abs/2505.14418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 10 figures, 12 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了AgentGhost框架，用于针对基于多模态大型语言模型（MLLMs）的图形用户界面（GUI）代理进行隐蔽的后门攻击，并开发了一种防御方法以降低攻击效果。&lt;h4&gt;背景&lt;/h4&gt;虽然基于MLLMs的GUI代理在人类交互中显示出巨大潜力，但由于高昂的微调成本，用户通常依赖开源GUI代理或AI提供商提供的API，这引入了供应链威胁：后门攻击。&lt;h4&gt;目的&lt;/h4&gt;揭示MLLMs驱动的GUI代理在交互层面的多个触发点，并开发一种有效的后门攻击框架。&lt;h4&gt;方法&lt;/h4&gt;提出AgentGhost框架，通过组合目标和交互层面的复合触发，使GUI代理在不影响任务功能的情况下无意中激活后门。通过最小-最大优化问题，使用监督对比学习来最大化样本类在表示空间中的特征差异，同时采用监督微调以最小化后门与清洁行为生成之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;AgentGhost在两个已建立的移动基准测试中对各种代理模型进行了广泛的评估，攻击精度达到99.7%，且只有1%的功能退化。&lt;h4&gt;结论&lt;/h4&gt;AgentGhost是一种有效且通用的后门攻击框架，同时开发了一种防御方法，将攻击精度降低到22.1%。&lt;h4&gt;翻译&lt;/h4&gt;Graphical user interface (GUI) agents powered by multimodal large languagemodels (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLML-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7% on three attack objectives, and shows stealthiness with only 1% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1%. Our code is available at anonymous.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) agents powered by multimodal large languagemodels (MLLMs) have shown greater promise for human-interaction. However, dueto the high fine-tuning cost, users often rely on open-source GUI agents orAPIs offered by AI providers, which introduces a critical but underexploredsupply chain threat: backdoor attacks. In this work, we first unveil thatMLLM-powered GUI agents naturally expose multiple interaction-level triggers,such as historical steps, environment states, and task progress. Based on thisobservation, we introduce AgentGhost, an effective and stealthy framework forred-teaming backdoor attacks. Specifically, we first construct compositetriggers by combining goal and interaction levels, allowing GUI agents tounintentionally activate backdoors while ensuring task utility. Then, weformulate backdoor injection as a Min-Max optimization problem that usessupervised contrastive learning to maximize the feature difference acrosssample classes at the representation space, improving flexibility of thebackdoor. Meanwhile, it adopts supervised fine-tuning to minimize thediscrepancy between backdoor and clean behavior generation, enhancingeffectiveness and utility. Extensive evaluations of various agent models in twoestablished mobile benchmarks show that AgentGhost is effective and generic,with attack accuracy that reaches 99.7\% on three attack objectives, and showsstealthiness with only 1\% utility degradation. Furthermore, we tailor adefense method against AgentGhost that reduces the attack accuracy to 22.1\%.Our code is available at \texttt{anonymous}.</description>
      <author>example@mail.com (Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Daizong Ding, Zhuosheng Zhang, Gongshen Liu)</author>
      <guid isPermaLink="false">2505.14418v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation of VLM for Soccer Video Understanding</title>
      <link>http://arxiv.org/abs/2505.13860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, accepted to the 11th IEEE International Workshop  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix  included as ancillary PDF&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了开放源代码视觉语言模型（VLMs）在不同领域的适应性，以足球为例，通过大规模数据集和指令遵循数据，对通用域VLM进行微调，实现了对足球特定任务的显著提升。&lt;h4&gt;背景&lt;/h4&gt;大多数视频理解VLM研究未针对特定领域，其迁移学习能力在专业领域未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索开放源代码VLMs对不同领域的适应性，以足球领域为例。&lt;h4&gt;方法&lt;/h4&gt;使用大规模足球数据集和大型语言模型创建指令遵循数据，以课程学习的方式对通用域VLM进行迭代微调，首先教授模型关键的足球概念，然后进行问答任务。&lt;h4&gt;主要发现&lt;/h4&gt;经过微调的模型在足球特定任务上表现出显著改进，视觉问答任务的相对改进为37.5%，下游足球动作分类任务的准确率从11.8%提升到63.5%。&lt;h4&gt;结论&lt;/h4&gt;通过针对特定领域的微调，开放源代码VLMs在足球等特定任务上可以取得显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) have demonstrated strong performance inmulti-modal tasks by effectively aligning visual and textual representations.However, most video understanding VLM research has been domain-agnostic,leaving the understanding of their transfer learning capability to specializeddomains under-explored. In this work, we address this by exploring theadaptability of open-source VLMs to specific domains, and focusing on soccer asan initial case study. Our approach uses large-scale soccer datasets and LLM tocreate instruction-following data, and use them to iteratively fine-tune thegeneral-domain VLM in a curriculum learning fashion (first teaching the modelkey soccer concepts to then question answering tasks). The final adapted model,trained using a curated dataset of 20k video clips, exhibits significantimprovement in soccer-specific tasks compared to the base model, with a 37.5%relative improvement for the visual question-answering task and an accuracyimprovement from 11.8% to 63.5% for the downstream soccer action classificationtask.</description>
      <author>example@mail.com (Tiancheng Jiang, Henry Wang, Md Sirajus Salekin, Parmida Atighehchian, Shinan Zhang)</author>
      <guid isPermaLink="false">2505.13860v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening</title>
      <link>http://arxiv.org/abs/2505.14033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and  Data Mining, KDD 2025 February Cycle&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于过滤的图神经网络（GNN）的新方法，通过结合图过滤和节点过滤来处理异质图，并提出了一种新的Coarsening-guided Partition-wise Filtering（CPF）方法，以增强模型的适应性和避免过拟合。&lt;h4&gt;背景&lt;/h4&gt;传统的基于过滤的GNN方法采用统一的图过滤范式，但在处理异质图时存在局限性。近期的研究引入了节点过滤，但缺乏统一框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种综合图过滤和节点过滤的框架，以处理具有同质性和异质性的图，并避免过度参数化和过拟合。&lt;h4&gt;方法&lt;/h4&gt;引入Coarsening-guided Partition-wise Filtering（CPF）方法，包括结构感知的节点分区过滤和特征感知的节点分区过滤。&lt;h4&gt;主要发现&lt;/h4&gt;CPF通过结合图过滤和节点过滤，提高了模型的适应性和分类性能，并揭示了节点过滤可能导致过度参数化和过拟合的风险。&lt;h4&gt;结论&lt;/h4&gt;CPF是一种有效的GNN过滤方法，适用于处理具有同质性和异质性的图，并通过实验验证了其效力和实用性。&lt;h4&gt;翻译&lt;/h4&gt;Filtering-based graph neural networks (GNNs) constitute a distinct class of GNNs that employ graph filters to handle graph-structured data, achieving notable success in various graph-related tasks. Conventional methods adopt a graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet recent findings suggest that this rigid paradigm struggles with heterophilic graphs. To overcome this, recent works have introduced node-wise filtering, which assigns distinct filters to individual nodes, offering enhanced adaptability. However, a fundamental gap remains: a comprehensive framework unifying these two strategies is still absent, limiting theoretical insights into the filtering paradigms. Moreover, through the lens of Contextual Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise filtering provides a sufficient solution for classification on graphs exhibiting both homophily and heterophily, suggesting the risk of excessive parameterization and potential overfitting with node-wise filtering. To address the limitations, this paper introduces Coarsening-guided Partition-wise Filtering (CPF). CPF innovates by performing filtering on node partitions. The method begins with structure-aware partition-wise filtering, which filters node partitions obtained via graph coarsening algorithms, and then performs feature-aware partition-wise filtering, refining node embeddings via filtering on clusters produced by $k$-means clustering over features. In-depth analysis is conducted for each phase of CPF, showing its superiority over other paradigms. Finally, benchmark node classification experiments, along with a real-world graph anomaly detection application, validate CPF's efficacy and practical utility.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Filtering-based graph neural networks (GNNs) constitute a distinct class ofGNNs that employ graph filters to handle graph-structured data, achievingnotable success in various graph-related tasks. Conventional methods adopt agraph-wise filtering paradigm, imposing a uniform filter across all nodes, yetrecent findings suggest that this rigid paradigm struggles with heterophilicgraphs. To overcome this, recent works have introduced node-wise filtering,which assigns distinct filters to individual nodes, offering enhancedadaptability. However, a fundamental gap remains: a comprehensive frameworkunifying these two strategies is still absent, limiting theoretical insightsinto the filtering paradigms. Moreover, through the lens of ContextualStochastic Block Model, we reveal that a synthesis of graph-wise and node-wisefiltering provides a sufficient solution for classification on graphsexhibiting both homophily and heterophily, suggesting the risk of excessiveparameterization and potential overfitting with node-wise filtering. To addressthe limitations, this paper introduces Coarsening-guided Partition-wiseFiltering (CPF). CPF innovates by performing filtering on node partitions. Themethod begins with structure-aware partition-wise filtering, which filters nodepartitions obtained via graph coarsening algorithms, and then performsfeature-aware partition-wise filtering, refining node embeddings via filteringon clusters produced by $k$-means clustering over features. In-depth analysisis conducted for each phase of CPF, showing its superiority over otherparadigms. Finally, benchmark node classification experiments, along with areal-world graph anomaly detection application, validate CPF's efficacy andpractical utility.</description>
      <author>example@mail.com (Guoming Li, Jian Yang, Yifan Chen)</author>
      <guid isPermaLink="false">2505.14033v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning</title>
      <link>http://arxiv.org/abs/2505.13812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于物理的自动监督学习方法，用于点云表示学习，该方法通过构建局部-整体力传播机制来捕捉部分与整体之间的关系。&lt;h4&gt;背景&lt;/h4&gt;现有的点云表示学习方法通常通过数据驱动的方法学习对象的几何分布，强调结构特征，而忽略了局部信息与整体结构之间的关系。&lt;h4&gt;目的&lt;/h4&gt;旨在通过捕捉局部特征与整体结构之间的关系，改进点云表示学习。&lt;h4&gt;方法&lt;/h4&gt;采用了一种双任务编码器-解码器框架，结合了隐式场的几何建模能力和物理驱动的弹性变形。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在对象分类、少样本学习和分割任务上优于现有方法，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效地捕捉点云的局部和整体几何形状，对点云表示学习具有改进作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing point cloud representation learning tend to learning the geometricdistribution of objects through data-driven approaches, emphasizing structuralfeatures while overlooking the relationship between the local information andthe whole structure. Local features reflect the fine-grained variations of anobject, while the whole structure is determined by the interaction andcombination of these local features, collectively defining the object's shape.In real-world, objects undergo elastic deformation under external forces, andthis deformation gradually affects the whole structure through the propagationof forces from local regions, thereby altering the object's geometricproperties. Inspired by this, we propose a physics-driven self-supervisedlearning method for point cloud representation, which captures the relationshipbetween parts and the whole by constructing a local-whole force propagationmechanism. Specifically, we employ a dual-task encoder-decoder framework,integrating the geometric modeling capability of implicit fields withphysics-driven elastic deformation. The encoder extracts features from thepoint cloud and its tetrahedral mesh representation, capturing both geometricand physical properties. These features are then fed into two decoders: onelearns the whole geometric shape of the point cloud through an implicit field,while the other predicts local deformations using two specifically designedphysics information loss functions, modeling the deformation relationshipbetween local and whole shapes. Experimental results show that our methodoutperforms existing approaches in object classification, few-shot learning,and segmentation, demonstrating its effectiveness.</description>
      <author>example@mail.com (Zhongyu Chen, Rong Zhao, Xie Han, Xindong Guo, Song Wang, Zherui Qiao)</author>
      <guid isPermaLink="false">2505.13812v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives</title>
      <link>http://arxiv.org/abs/2505.14361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Geoscience and Remote Sensing Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了基于两阶段范式的视觉语言模型（VLM）在遥感领域的进展，包括模型分类、网络架构、预训练目标、现有工作、数据集以及未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;VLM旨在弥合图像和自然语言之间的信息鸿沟，通过在大规模图像-文本对上进行预训练，然后在特定任务数据上进行微调，遥感领域的VLM取得了显著进展。&lt;h4&gt;目的&lt;/h4&gt;为遥感社区提供关于使用两阶段范式VLM发展的及时、全面综述。&lt;h4&gt;方法&lt;/h4&gt;首先，概述遥感领域VLM的分类，包括对比学习、视觉指令调整和文本条件图像生成。其次，对现有工作进行彻底回顾，包括基础模型和特定任务适应方法、架构升级、训练策略和模型能力。第三，总结用于VLM预训练、微调和评估的数据集，分析其构建方法（包括图像来源和字幕生成）和关键属性。最后，讨论未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;VLM模型从广泛的一般知识中受益，在多种遥感数据分析任务中表现出强大的性能，并且能够以对话方式与用户交互。&lt;h4&gt;结论&lt;/h4&gt;本文总结了VLM在遥感领域的最新进展，并提出了未来研究方向，包括跨模态表示对齐、模糊需求理解、解释驱动的模型可靠性、持续可扩展的模型能力以及具有更丰富模态和更大挑战的大规模数据集。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language modeling (VLM) aims to bridge the information gap betweenimages and natural language. Under the new paradigm of first pre-training onmassive image-text pairs and then fine-tuning on task-specific data, VLM in theremote sensing domain has made significant progress. The resulting modelsbenefit from the absorption of extensive general knowledge and demonstratestrong performance across a variety of remote sensing data analysis tasks.Moreover, they are capable of interacting with users in a conversationalmanner. In this paper, we aim to provide the remote sensing community with atimely and comprehensive review of the developments in VLM using the two-stageparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:contrastive learning, visual instruction tuning, and text-conditioned imagegeneration. For each category, we detail the commonly used network architectureand pre-training objectives. Second, we conduct a thorough review of existingworks, examining foundation models and task-specific adaptation methods incontrastive-based VLM, architectural upgrades, training strategies and modelcapabilities in instruction-based VLM, as well as generative foundation modelswith their representative downstream applications. Third, we summarize datasetsused for VLM pre-training, fine-tuning, and evaluation, with an analysis oftheir construction methodologies (including image sources and captiongeneration) and key properties, such as scale and task adaptability. Finally,we conclude this survey with insights and discussions on future researchdirections: cross-modal representation alignment, vague requirementcomprehension, explanation-driven model reliability, continually scalable modelcapabilities, and large-scale datasets featuring richer modalities and greaterchallenges.</description>
      <author>example@mail.com (Xingxing Weng, Chao Pang, Gui-Song Xia)</author>
      <guid isPermaLink="false">2505.14361v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Embedded Mean Field Reinforcement Learning for Perimeter-defense Game</title>
      <link>http://arxiv.org/abs/2505.14209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了大规模异构周界防御游戏，在三维环境中模拟现实元素，提出了一种基于嵌入平均场演员-评论员（EMFAC）框架的防御策略，并通过模拟和实际实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;无人机的快速发展和导弹技术的进步使得保护关键区域的攻防游戏变得更加复杂和重要，现有研究多集中于小型、简化的二维场景，忽略了现实环境中的复杂因素。&lt;h4&gt;目的&lt;/h4&gt;旨在研究大规模异构周界防御游戏，并提出有效的防御策略。&lt;h4&gt;方法&lt;/h4&gt;采用三维环境，引入运动动力学和风场等现实元素，推导攻击者和防御者的纳什均衡策略，并验证理论通过大量模拟。提出EMFAC框架，利用表示学习实现高层动作聚合，并引入基于奖励表示的轻量级注意力机制。&lt;h4&gt;主要发现&lt;/h4&gt;EMFAC在收敛速度和整体性能方面优于现有基线，且在实际场景中表现出良好的效果。&lt;h4&gt;结论&lt;/h4&gt;EMFAC框架能够有效应对大规模异构防御挑战，为周界防御游戏提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着无人机和导弹技术的快速发展，保护关键区域的攻击者和防御者之间的周界防御游戏在多个领域变得更加复杂和战略重要。然而，现有研究主要集中于小型、简化的二维场景，往往忽略了现实环境中的干扰、运动动力学和固有的异质性，这些因素对实际应用提出了重大挑战。为了弥合这一差距，我们在三维环境中研究大规模异构周界防御游戏，引入了运动动力学和风场等现实元素。我们推导了攻击者和防御者的纳什均衡策略，描述了胜利区域，并通过大量模拟验证了我们的理论发现。为了应对大规模异构防御策略中的控制挑战，我们提出了一种嵌入平均场演员-评论员（EMFAC）框架。EMFAC利用表示学习以平均场方式实现高层动作聚合，支持防御者之间的可扩展协调。此外，我们引入了一种基于奖励表示的轻量级注意力机制，该机制能够选择性地过滤观察和平均场信息，以增强决策效率和加速大规模任务中的收敛。跨不同规模的广泛模拟证明了EMFAC的有效性和适应性，其在收敛速度和整体性能方面均优于现有基线。为了进一步验证其实用性，我们在小规模实际实验中测试了EMFAC，并进行了详细分析，为框架在复杂场景中的有效性提供了更深入的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of unmanned aerial vehicles (UAVs) and missiletechnologies, perimeter-defense game between attackers and defenders for theprotection of critical regions have become increasingly complex andstrategically significant across a wide range of domains. However, existingstudies predominantly focus on small-scale, simplified two-dimensionalscenarios, often overlooking realistic environmental perturbations, motiondynamics, and inherent heterogeneity--factors that pose substantial challengesto real-world applicability. To bridge this gap, we investigate large-scaleheterogeneous perimeter-defense game in a three-dimensional setting,incorporating realistic elements such as motion dynamics and wind fields. Wederive the Nash equilibrium strategies for both attackers and defenders,characterize the victory regions, and validate our theoretical findings throughextensive simulations. To tackle large-scale heterogeneous control challengesin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)framework. EMFAC leverages representation learning to enable high-level actionaggregation in a mean-field manner, supporting scalable coordination amongdefenders. Furthermore, we introduce a lightweight agent-level attentionmechanism based on reward representation, which selectively filtersobservations and mean-field information to enhance decision-making efficiencyand accelerate convergence in large-scale tasks. Extensive simulations acrossvarying scales demonstrate the effectiveness and adaptability of EMFAC, whichoutperforms established baselines in both convergence speed and overallperformance. To further validate practicality, we test EMFAC in small-scalereal-world experiments and conduct detailed analyses, offering deeper insightsinto the framework's effectiveness in complex scenarios.</description>
      <author>example@mail.com (Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu)</author>
      <guid isPermaLink="false">2505.14209v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language</title>
      <link>http://arxiv.org/abs/2505.13784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 19th IEEE International Conference on Automatic Face and  Gesture Recognition 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究主要关注非手动特征，特别是口型在手语识别中的应用，并探索了从视觉语音识别到德语口型识别的迁移学习潜力。&lt;h4&gt;背景&lt;/h4&gt;手语识别系统主要关注手动手势，而口型等非手动特征提供了重要的语言信息。&lt;h4&gt;目的&lt;/h4&gt;直接对口型实例进行分类，并研究从视觉语音识别到德语口型识别的迁移学习潜力。&lt;h4&gt;方法&lt;/h4&gt;使用三个视觉语音识别数据集（一个英语，一个德语且包含无关词汇，一个德语且包含与口型数据集相同的目标词汇）来研究任务相似性对模型的影响。&lt;h4&gt;主要发现&lt;/h4&gt;多任务学习提高了口型识别和视觉语音识别的准确性以及模型的鲁棒性，表明口型识别应被视为与视觉语音识别相关但独立的任务。&lt;h4&gt;结论&lt;/h4&gt;该研究通过提出从视觉语音识别到手语识别数据集（有限口型标注）的知识迁移，为手语识别领域做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;This research focuses on the application of non-manual features, especially mouth movements, in Sign Language Recognition (SLR), and explores the potential of transfer learning from Visual Speech Recognition (VSR) to mouthing recognition in German Sign Language. The study uses three VSR datasets (one in English, one in German with unrelated words, and one in German containing the same target words as the mouthing dataset) to investigate the impact of task similarity on the model. The results show that multi-task learning improves the accuracy and robustness of both mouthing recognition and VSR, suggesting that mouthing recognition should be treated as a distinct but related task to VSR. This research contributes to the field of SLR by proposing knowledge transfer from VSR to SLR datasets with limited mouthing annotations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nphamdinh/transfer-learning-vsr-mouthing-sign-language&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Recognition (SLR) systems primarily focus on manual gestures,but non-manual features such as mouth movements, specifically mouthing, providevaluable linguistic information. This work directly classifies mouthinginstances to their corresponding words in the spoken language while exploringthe potential of transfer learning from Visual Speech Recognition (VSR) tomouthing recognition in German Sign Language. We leverage three VSR datasets:one in English, one in German with unrelated words and one in German containingthe same target words as the mouthing dataset, to investigate the impact oftask similarity in this setting. Our results demonstrate that multi-tasklearning improves both mouthing recognition and VSR accuracy as well as modelrobustness, suggesting that mouthing recognition should be treated as adistinct but related task to VSR. This research contributes to the field of SLRby proposing knowledge transfer from VSR to SLR datasets with limited mouthingannotations.</description>
      <author>example@mail.com (Dinh Nam Pham, Eleftherios Avramidis)</author>
      <guid isPermaLink="false">2505.13784v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.14005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025 AI4Tech Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OPEN的新方法，旨在提高图神经网络（GNN）的可解释性和透明度，解决现有GNN可解释性方法在捕捉决策逻辑和适用性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络的可解释性（XGNN）领域出现，旨在提高GNN的可靠性和可信度，但现有方法存在两大局限性：无法捕捉整个数据集样本空间中GNN的完整决策逻辑，以及对于边属性和GNN内部可访问性的严格要求。&lt;h4&gt;目的&lt;/h4&gt;提出OPEN方法，旨在克服现有XGNN方法的局限性，实现更全面、无前提条件的GNN可解释性。&lt;h4&gt;方法&lt;/h4&gt;OPEN方法首先将整个数据集样本空间推断并划分为多个环境，每个环境包含遵循不同分布的图。然后，通过从每个环境采样子图并分析其预测，学习GNN在不同分布下的决策逻辑，从而消除对严格前提条件的需求。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，OPEN方法几乎捕捉了GNN的完整决策逻辑，在保真度方面优于现有方法，同时保持了相似的效率，并在实际场景中增强了鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;OPEN方法为GNN的可解释性提供了新的解决方案，有助于提高GNN的可靠性和可信度，并增强其在实际应用中的表现。&lt;h4&gt;翻译&lt;/h4&gt;To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To enhance the reliability and credibility of graph neural networks (GNNs)and improve the transparency of their decision logic, a new field ofexplainability of GNNs (XGNN) has emerged. However, two major limitationsseverely degrade the performance and hinder the generalizability of existingXGNN methods: they (a) fail to capture the complete decision logic of GNNsacross diverse distributions in the entire dataset's sample space, and (b)impose strict prerequisites on edge properties and GNN internal accessibility.To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensiveand \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, asthe first work in the literature, can infer and partition the entire dataset'ssample space into multiple environments, each containing graphs that follow adistinct distribution. OPEN further learns the decision logic of GNNs acrossdifferent distributions by sampling subgraphs from each environment andanalyzing their predictions, thus eliminating the need for strictprerequisites. Experimental results demonstrate that OPEN captures nearlycomplete decision logic of GNNs, outperforms state-of-the-art methods infidelity while maintaining similar efficiency, and enhances robustness inreal-world scenarios.</description>
      <author>example@mail.com (Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam)</author>
      <guid isPermaLink="false">2505.14005v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts</title>
      <link>http://arxiv.org/abs/2505.13928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LoVR，一个针对长视频文本检索的基准，解决了现有基准在视频时长、字幕质量和标注粒度方面的限制。&lt;h4&gt;背景&lt;/h4&gt;长视频包含大量信息，视频文本检索是多媒体学习中的关键且具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;提出LoVR以解决现有基准的局限性，并提高视频文本检索方法的评估。&lt;h4&gt;方法&lt;/h4&gt;LoVR包含467个长视频和超过40,804个细粒度剪辑，以及高质量的字幕。通过集成VLM自动生成、字幕质量评分和动态细化，提出一个高效的字幕生成框架。此外，引入语义融合方法生成连贯的全视频字幕。&lt;h4&gt;主要发现&lt;/h4&gt;LoVR提出了更长的视频、更详细的字幕和更大规模的数据库，为视频理解和检索带来了新的挑战。&lt;h4&gt;结论&lt;/h4&gt;LoVR是一个具有挑战性的基准，揭示了当前方法的局限性，并为未来的研究提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces LoVR, a benchmark specifically designed for long video-text retrieval, which addresses the limitations of existing benchmarks in terms of video duration, caption quality, and annotation granularity. LoVR contains 467 long videos and over 40,804 fine-grained clips with high-quality captions. To overcome the issue of poor machine-generated annotations, an efficient caption generation framework is proposed, which integrates VLM automatic generation, caption quality scoring, and dynamic refinement. In addition, a semantic fusion method is introduced to generate coherent full-video captions without losing important contextual information. LoVR introduces longer videos, more detailed captions, and a larger-scale dataset, presenting new challenges for video understanding and retrieval. Extensive experiments on various advanced embedding models demonstrate that LoVR is a challenging benchmark, revealing the limitations of current approaches and providing valuable insights for future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/technomad-ds/lovr-benchmark&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long videos contain a vast amount of information, making video-text retrievalan essential and challenging task in multimodal learning. However, existingbenchmarks suffer from limited video duration, low-quality captions, and coarseannotation granularity, which hinder the evaluation of advanced video-textretrieval methods. To address these limitations, we introduce LoVR, a benchmarkspecifically designed for long video-text retrieval. LoVR contains 467 longvideos and over 40,804 fine-grained clips with high-quality captions. Toovercome the issue of poor machine-generated annotations, we propose anefficient caption generation framework that integrates VLM automaticgeneration, caption quality scoring, and dynamic refinement. This pipelineimproves annotation accuracy while maintaining scalability. Furthermore, weintroduce a semantic fusion method to generate coherent full-video captionswithout losing important contextual information. Our benchmark introduceslonger videos, more detailed captions, and a larger-scale dataset, presentingnew challenges for video understanding and retrieval. Extensive experiments onvarious advanced embedding models demonstrate that LoVR is a challengingbenchmark, revealing the limitations of current approaches and providingvaluable insights for future research. We release the code and dataset link athttps://github.com/TechNomad-ds/LoVR-benchmark</description>
      <author>example@mail.com (Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, Wentao Zhang)</author>
      <guid isPermaLink="false">2505.13928v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment</title>
      <link>http://arxiv.org/abs/2505.14204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Perceptual-Initialization (PI)的新方法，该方法在视觉表示学习初始化阶段就融入人类感知结构，而不是作为下游微调步骤。&lt;h4&gt;背景&lt;/h4&gt;传统方法通常在视觉表示学习的下游阶段使用人类感知数据来进行微调。&lt;h4&gt;目的&lt;/h4&gt;通过在初始化阶段结合人类感知结构，提高视觉语言系统的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将NIGHTS数据集的人类三元组嵌入初始化CLIP视觉编码器，然后进行YFCC15M上的自监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;在29个零样本分类和2个检索基准测试中，该方法在无需特定任务微调的情况下，显著提高了零样本性能。&lt;h4&gt;结论&lt;/h4&gt;这种方法挑战了将人类感知数据主要用于微调的传统观点，并证明了在早期表示学习阶段嵌入人类感知结构可以构建更强大、视觉语言对齐且泛化能力更强的系统。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了一种名为Perceptual-Initialization (PI)的范式转变，这种方法在视觉表示学习的初始化阶段结合人类感知结构，而不是作为下游微调步骤。通过整合来自NIGHTS数据集的人类三元组嵌入来初始化CLIP视觉编码器，随后在YFCC15M上进行自监督学习，我们的方法在29个零样本分类和2个检索基准测试中展示了显著的零样本性能提升，无需任何特定任务的微调。在ImageNet-1K上，零样本性能提升在约15个预训练周期后出现。这种方法在各种规模的数据集上观察到益处，改进在不同预训练阶段显现，具体取决于数据集的特征。我们的方法在多个评估任务中一致提高了零样本top-1准确率、top-5准确率和检索召回率（例如R@1、R@5），无需针对目标领域进行调整。这些发现挑战了使用人类感知数据主要用于微调的传统智慧，并表明在早期表示学习阶段嵌入人类感知结构可以构建更强大且视觉语言对齐的系统，能够立即泛化到未见过的任务。我们的工作表明，‘从你开始’，即从人类感知开始，为通用视觉语言智能提供了一个更坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perceptual-Initialization (PI), a paradigm shift in visualrepresentation learning that incorporates human perceptual structure during theinitialization phase rather than as a downstream fine-tuning step. Byintegrating human-derived triplet embeddings from the NIGHTS dataset toinitialize a CLIP vision encoder, followed by self-supervised learning onYFCC15M, our approach demonstrates significant zero-shot performanceimprovements, without any task-specific fine-tuning, across 29 zero shotclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gainsemerge after approximately 15 epochs of pretraining. Benefits are observedacross datasets of various scales, with improvements manifesting at differentstages of the pretraining process depending on dataset characteristics. Ourapproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, andretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,without requiring any adaptation to target domains. These findings challengethe conventional wisdom of using human-perceptual data primarily forfine-tuning and demonstrate that embedding human perceptual structure duringearly representation learning yields more capable and vision-language alignedsystems that generalize immediately to unseen tasks. Our work shows that"beginning with you", starting with human perception, provides a strongerfoundation for general-purpose vision-language intelligence.</description>
      <author>example@mail.com (Yang Hu, Runchen Wang, Stephen Chong Zhao, Xuhui Zhan, Do Hun Kim, Mark Wallace, David A. Tovar)</author>
      <guid isPermaLink="false">2505.14204v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Visual Instruction Bottleneck Tuning</title>
      <link>http://arxiv.org/abs/2505.13946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进多模态大语言模型（MLLM）在分布偏移下鲁棒性的方法。&lt;h4&gt;背景&lt;/h4&gt;尽管MLLM得到广泛应用，但在遇到分布偏移下的不熟悉查询时，它们的性能会下降。现有的提高MLLM泛化能力的方法通常需要更多的指令数据或更大的模型架构，这都会产生相当的人力和计算成本。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在从表示学习角度增强MLLM在分布偏移下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;受信息瓶颈（IB）原理的启发，本研究推导出MLLM的IB变分下界，并设计了一种实际可行的实现方法——视觉指令瓶颈调整（Vittle）。然后，通过揭示Vittle与MLLM信息理论鲁棒性指标之间的关系，对该方法提供了理论上的论证。&lt;h4&gt;主要发现&lt;/h4&gt;在包括30个偏移场景的45个数据集上对三个MLLM进行的实证验证表明，Vittle通过追求学习最小充分表示，一致地提高了MLLM在偏移下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效地提高MLLM在分布偏移情况下的鲁棒性，是一种经济高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大型语言模型（MLLM）被广泛采用，但当它们遇到分布偏移下的陌生查询时，其性能会下降。现有的提高MLLM泛化能力的方法通常需要更多的指令数据或更大的模型架构，这都会产生相当的人力和计算成本。在这项工作中，我们采用了一种从表示学习角度增强MLLM在分布偏移下鲁棒性的替代方法。受信息瓶颈（IB）原理的启发，我们推导了MLLM的IB变分下界，并设计了一种实际可行的实现方法，即视觉指令瓶颈调整（Vittle）。然后，我们通过揭示Vittle与MLLM信息理论鲁棒性指标之间的关系，对该方法提供了理论上的论证。在包括30个偏移场景的45个数据集上对三个MLLM进行的实证验证表明，Vittle通过追求学习最小充分表示，一致地提高了MLLM在偏移下的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite widespread adoption, multimodal large language models (MLLMs) sufferperformance degradation when encountering unfamiliar queries under distributionshifts. Existing methods to improve MLLM generalization typically requireeither more instruction data or larger advanced model architectures, both ofwhich incur non-trivial human labor or computational costs. In this work, wetake an alternative approach to enhance the robustness of MLLMs underdistribution shifts, from a representation learning perspective. Inspired bythe information bottleneck (IB) principle, we derive a variational lower boundof the IB for MLLMs and devise a practical implementation, Visual InstructionBottleneck Tuning (Vittle). We then provide a theoretical justification ofVittle by revealing its connection to an information-theoretic robustnessmetric of MLLM. Empirical validation of three MLLMs on open-ended andclosed-form question answering and object hallucination detection tasks over 45datasets, including 30 shift scenarios, demonstrates that Vittle consistentlyimproves the MLLM's robustness under shifts by pursuing the learning of aminimal sufficient representation.</description>
      <author>example@mail.com (Changdae Oh, Jiatong Li, Shawn Im, Yixuan Li)</author>
      <guid isPermaLink="false">2505.13946v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks</title>
      <link>http://arxiv.org/abs/2505.14417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WWW 2025 Companion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了非欧几里得空间在学习中的应用，特别是针对大型语言模型在网页相关应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;在基础模型和大型语言模型（LLM）时代，欧几里得空间是机器学习架构的默认几何设置，但这一选择存在根本性局限。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过非欧几里得学习克服这些局限，特别是在处理复杂关系和结构的网页相关应用中。&lt;h4&gt;方法&lt;/h4&gt;本文研究了非欧几里得空间，如双曲空间、球面空间和混合曲率空间，以及将基础模型与这些几何学相结合的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;非欧几里得空间为具有内在几何属性的数据（如社交网络拓扑、查询-文档关系和用户-项目交互）提供了更有效率的表示。&lt;h4&gt;结论&lt;/h4&gt;非欧几里得基础模型与几何学习（NEGEL）的交汇具有潜在优势，包括推进网页相关技术、挑战和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;在基础模型和大型语言模型（LLMs）的时代，欧几里得空间是我们机器学习架构的默认几何设置。然而，近期文献表明，这一选择伴随着根本性的局限。为此，非欧几里得学习正迅速获得关注，尤其是在复杂关系和结构普遍存在的网页相关应用中。非欧几里得空间，如双曲、球面和混合曲率空间，已被证明为具有内在几何属性的数据（包括社交网络拓扑、查询-文档关系和用户-项目交互）提供了更有效率的表示。将基础模型与非欧几里得几何学相结合具有巨大的潜力，可以增强它们捕捉和建模潜在结构的能力，从而在搜索、推荐和内容理解方面实现更好的性能。本次研讨会聚焦于非欧几里得基础模型与几何学习（NEGEL）的交汇，探讨其潜在益处，包括推进网页相关技术、挑战和未来方向。研讨会页面：[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701716.3717806&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of foundation models and Large Language Models (LLMs), Euclideanspace is the de facto geometric setting of our machine learning architectures.However, recent literature has demonstrated that this choice comes withfundamental limitations. To that end, non-Euclidean learning is quickly gainingtraction, particularly in web-related applications where complex relationshipsand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,spherical, and mixed-curvature spaces, have been shown to provide moreefficient and effective representations for data with intrinsic geometricproperties, including web-related data like social network topology,query-document relationships, and user-item interactions. Integratingfoundation models with non-Euclidean geometries has great potential to enhancetheir ability to capture and model the underlying structures, leading to betterperformance in search, recommendations, and content understanding. Thisworkshop focuses on the intersection of Non-Euclidean Foundation Models andGeometric Learning (NEGEL), exploring its potential benefits, including thepotential benefits for advancing web-related technologies, challenges, andfuture directions. Workshop page:[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)</description>
      <author>example@mail.com (Menglin Yang, Yifei Zhang, Jialin Chen, Melanie Weber, Rex Ying)</author>
      <guid isPermaLink="false">2505.14417v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion</title>
      <link>http://arxiv.org/abs/2505.13633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IPENS的交互式无监督多目标点云提取方法，用于植物表型分析，以提高智能育种效率。&lt;h4&gt;背景&lt;/h4&gt;植物表型分析技术在目标性状改良和智能育种加速中扮演关键角色，但现有方法依赖于大量高精度手动标注数据，且在处理自遮挡物体时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，以解决自遮挡物体在谷物水平上的分割问题，并提高植物表型分析的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;IPENS方法利用辐射场信息将SAM2分割的2D掩码提升到3D空间进行目标点云提取，并设计了多目标协作优化策略来解决单交互多目标分割挑战。&lt;h4&gt;主要发现&lt;/h4&gt;在水稻数据集上，IPENS实现了63.72%的谷物水平分割精度（mIoU），并对谷物体积、叶面积、叶长和宽等性状进行了准确预测。在小麦数据集上，分割精度进一步提高到89.68%（mIoU），同样展现了卓越的表型估计性能。&lt;h4&gt;结论&lt;/h4&gt;IPENS提供了一种非侵入性、高质量的表型提取解决方案，无需标注数据，通过简单的单轮交互即可在3分钟内快速提取谷物级点云，显著提高了智能育种效率。&lt;h4&gt;翻译&lt;/h4&gt;Advanced plant phenotyping technologies play a crucial role in targeted trait improvement and accelerating intelligent breeding. Due to the species diversity of plants, existing methods heavily rely on large-scale high-precision manually annotated data. For self-occluded objects at the grain level, unsupervised methods often prove ineffective. This study proposes IPENS, an interactive unsupervised multi-target point cloud extraction method. The method utilizes radiance field information to lift 2D masks, which are segmented by SAM2 (Segment Anything Model 2), into 3D space for target point cloud extraction. A multi-target collaborative optimization strategy is designed to effectively resolve the single-interaction multi-target segmentation challenge. Experimental validation demonstrates that IPENS achieves a grain-level segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697 (RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a wheat dataset, IPENS further improves segmentation accuracy to 89.68% (mIoU), with equally outstanding phenotypic estimation performance: spike volume prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00 (RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92 (RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality phenotyping extraction solution for rice and wheat. Without requiring annotated data, it rapidly extracts grain-level point clouds within 3 minutes through simple single-round interactions on images for multiple targets, demonstrating significant potential to accelerate intelligent breeding efficiency.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced plant phenotyping technologies play a crucial role in targeted traitimprovement and accelerating intelligent breeding. Due to the species diversityof plants, existing methods heavily rely on large-scale high-precision manuallyannotated data. For self-occluded objects at the grain level, unsupervisedmethods often prove ineffective. This study proposes IPENS, an interactiveunsupervised multi-target point cloud extraction method. The method utilizesradiance field information to lift 2D masks, which are segmented by SAM2(Segment Anything Model 2), into 3D space for target point cloud extraction. Amulti-target collaborative optimization strategy is designed to effectivelyresolve the single-interaction multi-target segmentation challenge.Experimental validation demonstrates that IPENS achieves a grain-levelsegmentation accuracy (mIoU) of 63.72% on a rice dataset, with strongphenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf lengthand width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On awheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),with equally outstanding phenotypic estimation performance: spike volumeprediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-qualityphenotyping extraction solution for rice and wheat. Without requiring annotateddata, it rapidly extracts grain-level point clouds within 3 minutes throughsimple single-round interactions on images for multiple targets, demonstratingsignificant potential to accelerate intelligent breeding efficiency.</description>
      <author>example@mail.com (Wentao Song, He Huang, Youqiang Sun, Fang Qu, Jiaqi Zhang, Longhui Fang, Yuwei Hao, Chenyang Peng)</author>
      <guid isPermaLink="false">2505.13633v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2505.13754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在动态图中寻找最大独立集（MaxIS）的无监督学习模型，该模型结合了图神经网络（GNNs）的结构学习和一个学习到的分布式更新机制，能够在单个并行步骤中推断节点在MaxIS中的成员资格。&lt;h4&gt;背景&lt;/h4&gt;动态图中的边随时间变化，目前还没有专门的无监督学习模型来处理这类问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理动态图中MaxIS问题的无监督学习模型。&lt;h4&gt;方法&lt;/h4&gt;模型结合了图神经网络（GNNs）的结构学习和一个学习到的分布式更新机制，能够快速更新节点的内部记忆并推断它们在MaxIS中的成员资格。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在合成和现实世界的动态图上表现出良好的性能，特别是在大型图上，它在解决方案质量、运行时间和内存使用方面显著优于最先进的学习框架，并且比贪婪算法快1.5-23倍。&lt;h4&gt;结论&lt;/h4&gt;该模型在性能和可扩展性方面具有竞争力，能够有效处理大型动态图中的MaxIS问题。&lt;h4&gt;翻译&lt;/h4&gt;We present the first unsupervised learning model for finding Maximum Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our method combines structural learning from graph neural networks (GNNs) with a learned distributed update mechanism that, given an edge addition or deletion event, modifies nodes' internal memories and infers their MaxIS membership in a single, parallel step. We parameterize our model by the update mechanism's radius and investigate the resulting performance-runtime tradeoffs for various dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS methods for static graphs, including a mixed integer programming solver, deterministic rule-based algorithms, and a heuristic learning framework based on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs of 100-10,000 nodes, our model achieves competitive approximation ratios with excellent scalability; on large graphs, it significantly outperforms the state-of-the-art heuristic learning framework in solution quality, runtime, and memory usage. Our model generalizes well on graphs 100x larger than the ones used for training, achieving performance at par with both a greedy technique and a commercial mixed integer programming solver while running 1.5-23x faster than greedy.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the first unsupervised learning model for finding MaximumIndependent Sets (MaxIS) in dynamic graphs where edges change over time. Ourmethod combines structural learning from graph neural networks (GNNs) with alearned distributed update mechanism that, given an edge addition or deletionevent, modifies nodes' internal memories and infers their MaxIS membership in asingle, parallel step. We parameterize our model by the update mechanism'sradius and investigate the resulting performance-runtime tradeoffs for variousdynamic graph topologies. We evaluate our model against state-of-the-art MaxISmethods for static graphs, including a mixed integer programming solver,deterministic rule-based algorithms, and a heuristic learning framework basedon dynamic programming and GNNs. Across synthetic and real-world dynamic graphsof 100-10,000 nodes, our model achieves competitive approximation ratios withexcellent scalability; on large graphs, it significantly outperforms thestate-of-the-art heuristic learning framework in solution quality, runtime, andmemory usage. Our model generalizes well on graphs 100x larger than the onesused for training, achieving performance at par with both a greedy techniqueand a commercial mixed integer programming solver while running 1.5-23x fasterthan greedy.</description>
      <author>example@mail.com (Devendra Parkar, Anya Chaturvedi, Andréa W. Richa, Joshua J. Daymude)</author>
      <guid isPermaLink="false">2505.13754v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer</title>
      <link>http://arxiv.org/abs/2505.13857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a journal paper in IEEE Transactions on Intelligent  Transportation Systems (T-ITS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TedTrajRec的新方法，用于提高GPS轨迹的采样率，以解决GPS轨迹在现实应用中由于采样率低、点间间隔不规则而导致的稀疏性问题。&lt;h4&gt;背景&lt;/h4&gt;现实应用中的GPS轨迹往往存在采样率低、点间间隔不规则的问题，这对GPS轨迹的直接应用提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决地图约束轨迹恢复的问题，提高GPS轨迹的采样率。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种新的方法TedTrajRec，包括两个部分：PD-GNN和TedFormer。PD-GNN用于捕捉空间时间交通动态，同时学习每个路段的拓扑感知动态；TedFormer是一个时间感知的Transformer，通过将闭式神经网络常微分方程整合到注意力机制中，以处理不规则采样的数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过在三个真实世界数据集上的大量实验，证明了TedTrajRec方法在轨迹恢复方面的优越性能。&lt;h4&gt;结论&lt;/h4&gt;TedTrajRec方法能够有效地提高GPS轨迹的采样率，为GPS轨迹的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;In real-world applications, GPS trajectories often suffer from low sampling rates, with large and irregular intervals between consecutive GPS points. This sparse characteristic presents challenges for their direct use in GPS-based systems. This paper addresses the task of map-constrained trajectory recovery, aiming to enhance trajectory sampling rates of GPS trajectories. Previous studies commonly adopt a sequence-to-sequence framework, where an encoder captures the trajectory patterns and a decoder reconstructs the target trajectory. Within this framework, effectively representing the road network and extracting relevant trajectory features are crucial for overall performance. Despite advancements in these models, they fail to fully leverage the complex spatio-temporal dynamics present in both the trajectory and the road network. To overcome these limitations, we categorize the spatio-temporal dynamics of trajectory data into two distinct aspects: spatial-temporal traffic dynamics and trajectory dynamics. Furthermore, we propose TedTrajRec, a novel method for trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce PD-GNN, which models periodic patterns and learns topologically aware dynamics concurrently for each road segment. For spatio-temporal trajectory dynamics, we present TedFormer, a time-aware Transformer that incorporates temporal dynamics for each GPS location by integrating closed-form neural ordinary differential equations into the attention mechanism. This allows TedFormer to effectively handle irregularly sampled data. Extensive experiments on three real-world datasets demonstrate the superior performance of TedTrajRec. The code is publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world applications, GPS trajectories often suffer from low samplingrates, with large and irregular intervals between consecutive GPS points. Thissparse characteristic presents challenges for their direct use in GPS-basedsystems. This paper addresses the task of map-constrained trajectory recovery,aiming to enhance trajectory sampling rates of GPS trajectories. Previousstudies commonly adopt a sequence-to-sequence framework, where an encodercaptures the trajectory patterns and a decoder reconstructs the targettrajectory. Within this framework, effectively representing the road networkand extracting relevant trajectory features are crucial for overallperformance. Despite advancements in these models, they fail to fully leveragethe complex spatio-temporal dynamics present in both the trajectory and theroad network.  To overcome these limitations, we categorize the spatio-temporal dynamics oftrajectory data into two distinct aspects: spatial-temporal traffic dynamicsand trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method fortrajectory recovery. To capture spatio-temporal traffic dynamics, we introducePD-GNN, which models periodic patterns and learns topologically aware dynamicsconcurrently for each road segment. For spatio-temporal trajectory dynamics, wepresent TedFormer, a time-aware Transformer that incorporates temporal dynamicsfor each GPS location by integrating closed-form neural ordinary differentialequations into the attention mechanism. This allows TedFormer to effectivelyhandle irregularly sampled data. Extensive experiments on three real-worlddatasets demonstrate the superior performance of TedTrajRec. The code ispublicly available at https://github.com/ysygMhdxw/TEDTrajRec/.</description>
      <author>example@mail.com (Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun)</author>
      <guid isPermaLink="false">2505.13857v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.14271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究人类与AI模型在生成任务中的协作带来的新挑战，即区分人类撰写、AI生成和人类-AI协作文本。&lt;h4&gt;背景&lt;/h4&gt;人类与AI模型的协作在生成任务中日益增多，这引发了区分不同类型文本的新挑战。&lt;h4&gt;目的&lt;/h4&gt;收集一个多语言、多领域、多生成器的数据集FAIDSet，并引入一个细粒度检测框架FAID来分类文本，并识别背后的AI模型家族。&lt;h4&gt;方法&lt;/h4&gt;FAID框架结合多级对比学习和多任务辅助分类来学习细微的文体特征，并通过将AI家族建模为不同的文体实体，提供改进的可解释性。此外，该方法还包含一个适应机制，以解决分布偏移问题，而无需针对未见数据重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FAID优于几种基线方法，尤其是在未见领域和新AI模型上的泛化准确性方面得到显著提升。&lt;h4&gt;结论&lt;/h4&gt;FAID为提高AI辅助写作中的透明度和问责制提供了潜在的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着人类与AI模型在生成任务中的协作日益增多，区分人类编写的、AI生成的以及人类-AI协作的文本带来了新的挑战。本研究收集了一个多语言、多领域、多生成器的数据集FAIDSet，并引入了一个细粒度检测框架FAID，用于将文本分类为这三种类别，并识别背后的AI模型家族。与现有的二元分类器不同，FAID旨在捕捉作者和模型特定的特征。该方法结合了多级对比学习和多任务辅助分类来学习细微的文体线索。通过将AI家族建模为不同的文体实体，FAID提供了改进的可解释性。该方法还包含一个适应机制，以解决分布偏移问题，而无需针对未见数据重新训练。实验结果表明，FAID优于几种基线方法，尤其是在未见领域和新AI模型上的泛化准确性方面得到了显著提升。这为提高AI辅助写作中的透明度和问责制提供了潜在的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing collaboration between humans and AI models in generative taskshas introduced new challenges in distinguishing between human-written,AI-generated, and human-AI collaborative texts. In this work, we collect amultilingual, multi-domain, multi-generator dataset FAIDSet. We furtherintroduce a fine-grained detection framework FAID to classify text into thesethree categories, meanwhile identifying the underlying AI model family. Unlikeexisting binary classifiers, FAID is built to capture both authorship andmodel-specific characteristics. Our method combines multi-level contrastivelearning with multi-task auxiliary classification to learn subtle stylisticcues. By modeling AI families as distinct stylistic entities, FAID offersimproved interpretability. We incorporate an adaptation to addressdistributional shifts without retraining for unseen data. Experimental resultsdemonstrate that FAID outperforms several baseline approaches, particularlyenhancing the generalization accuracy on unseen domains and new AI models. Itprovide a potential solution for improving transparency and accountability inAI-assisted writing.</description>
      <author>example@mail.com (Minh Ngoc Ta, Dong Cao Van, Duc-Anh Hoang, Minh Le-Anh, Truong Nguyen, My Anh Tran Nguyen, Yuxia Wang, Preslav Nakov, Sang Dinh)</author>
      <guid isPermaLink="false">2505.14271v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model</title>
      <link>http://arxiv.org/abs/2503.16282v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GFS-VL的3D点云分割方法，该方法通过结合3D视觉语言模型生成的密集但噪声伪标签与精确且稀疏的少量样本，以提高模型的分割能力。&lt;h4&gt;背景&lt;/h4&gt;现有的GFS-PCS方法在少量样本的辅助下增强模型原型，但受限于少量样本的稀疏知识；同时，3D视觉语言模型包含丰富的但噪声的新类别知识。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GFS-PCS框架，以充分利用3D VLM的伪标签和少量样本的优点。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了原型引导的伪标签选择机制，用于过滤低质量区域；2. 采用自适应填充策略，结合伪标签上下文和少量样本知识来标记过滤后的未标记区域；3. 设计了一种新型基础混合策略，将少量样本嵌入训练场景，以保留重要上下文信息；4. 为了解决现有GFS-PCS基准测试的多样性有限问题，引入了两个具有多样新类别的挑战性基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了该框架在模型和数据集上的有效性，并为GFS-PCS在现实世界中的应用提供了坚实的基础。&lt;h4&gt;结论&lt;/h4&gt;GFS-VL方法结合了3D VLM的伪标签和少量样本的优点，为3D点云分割提供了新的解决方案，并有助于推进GFS-PCS在现实世界中的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhaochongan/gfs-vl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models tonew classes with few support samples while retaining base class segmentation.Existing GFS-PCS methods enhance prototypes via interacting with support orquery features but remain limited by sparse knowledge from few-shot samples.Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-worldnovel classes, contain rich but noisy novel class knowledge. In this work, weintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labelsfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengthsof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-labelselection to filter low-quality regions, followed by an adaptive infillingstrategy that combines knowledge from pseudo-label contexts and few-shotsamples to adaptively label the filtered, unlabeled areas. Additionally, wedesign a novel-base mix strategy to embed few-shot samples into trainingscenes, preserving essential context for improved novel class learning.Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, weintroduce two challenging benchmarks with diverse novel classes forcomprehensive generalization evaluation. Experiments validate the effectivenessof our framework across models and datasets. Our approach and benchmarksprovide a solid foundation for advancing GFS-PCS in the real world. The code isat https://github.com/ZhaochongAn/GFS-VL</description>
      <author>example@mail.com (Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie)</author>
      <guid isPermaLink="false">2503.16282v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Table Foundation Models: on knowledge pre-training for tabular learning</title>
      <link>http://arxiv.org/abs/2505.14415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为TARTE的基础模型，该模型能够将表格转换为知识增强的向量表示，通过字符串捕捉语义，从而提高表格数据的预测性能。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型在数据科学中具有巨大潜力，但数据语义理解是一个挑战，预训练的神经网络通过联合建模列名和表格条目已提升预测准确率。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高效处理表格数据的基础模型，降低额外计算成本，并能与其他模型结合使用。&lt;h4&gt;方法&lt;/h4&gt;TARTE模型基于大量关系数据预训练，通过字符串捕捉语义，将表格转换为知识增强的向量表示，以促进后续学习。&lt;h4&gt;主要发现&lt;/h4&gt;TARTE模型能够以较低的额外成本促进后续学习，其表示可以进行微调或与其他学习器结合，从而提高预测性能并改善预测/计算性能的权衡。&lt;h4&gt;结论&lt;/h4&gt;TARTE模型为表格学习提供了一个有效的知识预训练方法，并推动了该领域的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;Table foundation models bring high hopes to data science: pre-trained on tabular data to embark knowledge or priors, they should facilitate downstream tasks on tables. One specific challenge is that of data semantics: numerical entries take their meaning from context, e.g., column name. Pre-trained neural networks that jointly model column names and table entries have recently boosted prediction accuracy. While these models outline the promises of world knowledge to interpret table values, they lack the convenience of popular foundation models in text or vision. Indeed, they must be fine-tuned to bring benefits, come with substantial computation costs, and cannot easily be reused or combined with other architectures. Here we introduce TARTE, a foundation model that transforms tables to knowledge-enhanced vector representations using the string to capture semantics. Pre-trained on large relational data, TARTE yields representations that facilitate subsequent learning with little additional cost. These representations can be fine-tuned or combined with other learners, giving models that push the state-of-the-art prediction performance and improve the prediction/computation performance trade-off. Specialized to a task or a domain, TARTE gives domain-specific representations that facilitate further learning. Our study demonstrates an effective approach to knowledge pre-training for tabular learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Table foundation models bring high hopes to data science: pre-trained ontabular data to embark knowledge or priors, they should facilitate downstreamtasks on tables. One specific challenge is that of data semantics: numericalentries take their meaning from context, e.g., column name. Pre-trained neuralnetworks that jointly model column names and table entries have recentlyboosted prediction accuracy. While these models outline the promises of worldknowledge to interpret table values, they lack the convenience of popularfoundation models in text or vision. Indeed, they must be fine-tuned to bringbenefits, come with sizeable computation costs, and cannot easily be reused orcombined with other architectures. Here we introduce TARTE, a foundation modelthat transforms tables to knowledge-enhanced vector representations using thestring to capture semantics. Pre-trained on large relational data, TARTE yieldsrepresentations that facilitate subsequent learning with little additionalcost. These representations can be fine-tuned or combined with other learners,giving models that push the state-of-the-art prediction performance and improvethe prediction/computation performance trade-off. Specialized to a task or adomain, TARTE gives domain-specific representations that facilitate furtherlearning. Our study demonstrates an effective approach to knowledgepre-training for tabular learning.</description>
      <author>example@mail.com (Myung Jun Kim, Félix Lefebvre, Gaëtan Brison, Alexandre Perez-Lebel, Gaël Varoquaux)</author>
      <guid isPermaLink="false">2505.14415v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search</title>
      <link>http://arxiv.org/abs/2505.14156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SymbolicGraph Ranker (SGR)的会话搜索方法，旨在利用大型语言模型（LLMs）的优势，结合文本和图结构信息来满足用户复杂的信息需求。&lt;h4&gt;背景&lt;/h4&gt;当前会话搜索策略通常优先考虑序列建模以实现深度语义理解，但忽略了交互中的图结构。一些方法虽然关注捕捉结构信息，但使用的是通用的文档表示，忽略了词语层面的语义建模。&lt;h4&gt;目的&lt;/h4&gt;提出SGR的目的是利用文本和图结构信息，并增强LLMs在文本格式中捕捉图结构的能力。&lt;h4&gt;方法&lt;/h4&gt;首先，引入一组符号语法规则将会话图转换为文本，以便将会话历史、交互过程和任务指令无缝集成到LLMs的输入中。其次，通过引入一系列自监督符号学习任务，如链接预测、节点内容生成和生成对比学习，来使LLMs能够从粗粒度到细粒度地捕捉拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;在AOL和Tiangong-ST两个基准数据集上的实验结果和综合分析证实了该方法的优势。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为传统搜索策略与现代LLMs之间架起了一座桥梁，提供了一种新颖且有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose SymbolicGraph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3589334.3645574&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Session search involves a series of interactive queries and actions tofulfill user's complex information need. Current strategies typicallyprioritize sequential modeling for deep semantic understanding, overlooking thegraph structure in interactions. While some approaches focus on capturingstructural information, they use a generalized representation for documents,neglecting the word-level semantic modeling. In this paper, we propose SymbolicGraph Ranker (SGR), which aims to take advantage of both text-based andgraph-based approaches by leveraging the power of recent Large Language Models(LLMs). Concretely, we first introduce a set of symbolic grammar rules toconvert session graph into text. This allows integrating session history,interaction process, and task instruction seamlessly as inputs for the LLM.Moreover, given the natural discrepancy between LLMs pre-trained on textualcorpora, and the symbolic language we produce using our graph-to-text grammar,our objective is to enhance LLMs' ability to capture graph structures within atextual format. To achieve this, we introduce a set of self-supervised symboliclearning tasks including link prediction, node content generation, andgenerative contrastive learning, to enable LLMs to capture the topologicalinformation from coarse-grained to fine-grained. Experiment results andcomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirmthe superiority of our approach. Our paradigm also offers a novel and effectivemethodology that bridges the gap between traditional search strategies andmodern LLMs.</description>
      <author>example@mail.com (Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan)</author>
      <guid isPermaLink="false">2505.14156v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping</title>
      <link>http://arxiv.org/abs/2505.13777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Sat2Sound是一个多模态表示学习框架，用于声音景观映射，旨在预测地球上任何地点的声音分布。&lt;h4&gt;背景&lt;/h4&gt;现有的声音景观映射方法依赖于卫星图像和配对的地标签音频样本，这些方法往往无法捕捉到特定地点声音来源的多样性。&lt;h4&gt;目的&lt;/h4&gt;通过利用视觉-语言模型（VLM）为卫星图像中的地点生成语义丰富的声音景观描述，以增强现有数据集。&lt;h4&gt;方法&lt;/h4&gt;方法包括音频、音频字幕、卫星图像和卫星图像字幕之间的对比学习。通过学习共享的声音景观概念代码簿，将每个样本表示为这些概念的加权平均值。&lt;h4&gt;主要发现&lt;/h4&gt;Sat2Sound在两个数据集（GeoSound和SoundingEarth）上的卫星图像与音频之间的跨模态检索中实现了最先进的性能。此外，利用Sat2Sound检索详细声音景观字幕的能力，引入了基于位置的声景合成应用，实现了沉浸式的听觉体验。&lt;h4&gt;结论&lt;/h4&gt;Sat2Sound的代码和模型将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Sat2Sound, a multimodal representation learning framework forsoundscape mapping, designed to predict the distribution of sounds at anylocation on Earth. Existing methods for this task rely on satellite image andpaired geotagged audio samples, which often fail to capture the diversity ofsound sources at a given location. To address this limitation, we enhanceexisting datasets by leveraging a Vision-Language Model (VLM) to generatesemantically rich soundscape descriptions for locations depicted in satelliteimages. Our approach incorporates contrastive learning across audio, audiocaptions, satellite images, and satellite image captions. We hypothesize thatthere is a fixed set of soundscape concepts shared across modalities. To thisend, we learn a shared codebook of soundscape concepts and represent eachsample as a weighted average of these concepts. Sat2Sound achievesstate-of-the-art performance in cross-modal retrieval between satellite imageand audio on two datasets: GeoSound and SoundingEarth. Additionally, buildingon Sat2Sound's ability to retrieve detailed soundscape captions, we introduce anovel application: location-based soundscape synthesis, which enables immersiveacoustic experiences. Our code and models will be publicly available.</description>
      <author>example@mail.com (Subash Khanal, Srikumar Sastry, Aayush Dhakal, Adeel Ahmad, Nathan Jacobs)</author>
      <guid isPermaLink="false">2505.13777v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Diving into the Fusion of Monocular Priors for Generalized Stereo Matching</title>
      <link>http://arxiv.org/abs/2505.14414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code:  https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了立体匹配中的融合单目先验问题，提出了一种新的融合方法以改善在难以处理的区域（如遮挡和非朗伯表面）中的匹配性能。&lt;h4&gt;背景&lt;/h4&gt;传统的立体匹配方法在处理难以处理的区域时存在困难，而融合单目先验可以提升匹配性能，但小立体数据集学习到的单目先验可能存在偏差，限制了泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的融合方法，利用视觉基础模型（VFM）中的无偏单目先验，提高难以处理区域中的立体匹配泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 分析了VFM单目先验融合过程中的三个主要问题：单目深度和视差深度的不匹配、单目特征在迭代更新中的过自信导致局部最优解、直接融合单目深度图可能导致噪声影响融合。2. 提出使用二值局部排序图引导融合，将深度图转换为二值相对格式，统一相对和绝对深度表示。3. 使用局部排序图重新加权初始视差更新，解决局部最优解和噪声问题。4. 将单目深度与视差的直接融合作为配准问题处理，使用像素级线性回归模块全局和自适应地对齐。&lt;h4&gt;主要发现&lt;/h4&gt;本文发现，通过改进融合方法，可以显著提升从SceneFlow到Middlebury和Booster数据集的泛化性能，同时效率基本没有降低。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效利用单目先验支持立体匹配，显著提高了在难以处理区域中的匹配性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的融合单目先验的方法，通过解决融合过程中的关键问题，显著提高了在难以处理区域中的立体匹配性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The matching formulation makes it naturally hard for the stereo matching tohandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusingmonocular priors has been proven helpful for ill-posed matching, but the biasedmonocular prior learned from small stereo datasets constrains thegeneralization. Recently, stereo matching has progressed by leveraging theunbiased monocular prior from the vision foundation model (VFM) to improve thegeneralization in ill-posed regions. We dive into the fusion process andobserve three main problems limiting the fusion of the VFM monocular prior. Thefirst problem is the misalignment between affine-invariant relative monoculardepth and absolute depth of disparity. Besides, when we use the monocularfeature in an iterative update structure, the over-confidence in the disparityupdate leads to local optima results. A direct fusion of a monocular depth mapcould alleviate the local optima problem, but noisy disparity results computedat the first several iterations will misguide the fusion. In this paper, wepropose a binary local ordering map to guide the fusion, which converts thedepth map into a binary relative format, unifying the relative and absolutedepth representation. The computed local ordering map is also used to re-weightthe initial disparity update, resolving the local optima and noisy problem. Inaddition, we formulate the final direct fusion of monocular depth to thedisparity as a registration problem, where a pixel-wise linear regressionmodule can globally and adaptively align them. Our method fully exploits themonocular prior to support stereo matching results effectively and efficiently.We significantly improve the performance from the experiments when generalizingfrom SceneFlow to Middlebury and Booster datasets while barely reducing theefficiency.</description>
      <author>example@mail.com (Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, Yunde Jia)</author>
      <guid isPermaLink="false">2505.14414v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>A Challenge to Build Neuro-Symbolic Video Agents</title>
      <link>http://arxiv.org/abs/2505.13851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;现代视频理解系统在场景分类、物体检测和短视频检索等任务上表现出色，但随着视频分析在现实应用中的重要性日益增加，对能够主动推理事件并采取行动的视频智能体的需求也在增长。&lt;h4&gt;背景&lt;/h4&gt;尽管深度学习模型在识别单个帧或短剪辑中的模式方面取得了显著进展，但它们在理解事件随时间序列和依赖关系方面存在困难，这对于驱动决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种神经符号方法来克服时间推理的障碍，通过将视频查询分解为原子事件，构建成连贯的序列，并验证其时间约束，以增强可解释性、实现结构化推理并提高系统行为保证。&lt;h4&gt;方法&lt;/h4&gt;提出一个挑战，要求研究社区开发下一代智能视频智能体，这些智能体集成了三个核心能力：自主视频搜索和分析、无缝现实世界交互以及高级内容生成。&lt;h4&gt;主要发现&lt;/h4&gt;这种神经符号方法可以推动从被动感知到智能视频智能体的转变，这些智能体能够推理、预测并采取行动。&lt;h4&gt;结论&lt;/h4&gt;通过解决这些核心能力，可以推动视频理解的发展，实现更加可靠的视频智能体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern video understanding systems excel at tasks such as sceneclassification, object detection, and short video retrieval. However, as videoanalysis becomes increasingly central to real-world applications, there is agrowing need for proactive video agents for the systems that not only interpretvideo streams but also reason about events and take informed actions. A keyobstacle in this direction is temporal reasoning: while deep learning modelshave made remarkable progress in recognizing patterns within individual framesor short clips, they struggle to understand the sequencing and dependencies ofevents over time, which is critical for action-driven decision-making.Addressing this limitation demands moving beyond conventional deep learningapproaches. We posit that tackling this challenge requires a neuro-symbolicperspective, where video queries are decomposed into atomic events, structuredinto coherent sequences, and validated against temporal constraints. Such anapproach can enhance interpretability, enable structured reasoning, and providestronger guarantees on system behavior, all key properties for advancingtrustworthy video agents. To this end, we present a grand challenge to theresearch community: developing the next generation of intelligent video agentsthat integrate three core capabilities: (1) autonomous video search andanalysis, (2) seamless real-world interaction, and (3) advanced contentgeneration. By addressing these pillars, we can transition from passiveperception to intelligent video agents that reason, predict, and act, pushingthe boundaries of video understanding.</description>
      <author>example@mail.com (Sahil Shah, Harsh Goel, Sai Shankar Narasimhan, Minkyu Choi, S P Sharan, Oguzhan Akcin, Sandeep Chinchali)</author>
      <guid isPermaLink="false">2505.13851v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model</title>
      <link>http://arxiv.org/abs/2505.13746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉语言模型（ReSW-VL）的手术流程分析方法，用于手术阶段识别，并展示了该方法在三个手术阶段识别数据集上的有效性。&lt;h4&gt;背景&lt;/h4&gt;手术阶段识别技术能够自动分类手术进程，广泛应用于实时手术支持、医疗资源优化、培训和技能评估以及安全提升。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的手术阶段识别方法，用于提高手术流程分析中的特征学习和识别性能。&lt;h4&gt;方法&lt;/h4&gt;通过微调CLIP视觉语言模型的图像编码器，并结合提示学习进行手术阶段识别。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的识别方法相比，该方法在三个手术阶段识别数据集上表现出更高的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的ReSW-VL方法在手术阶段识别方面具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical phase recognition from video is a technology that automaticallyclassifies the progress of a surgical procedure and has a wide range ofpotential applications, including real-time surgical support, optimization ofmedical resources, training and skill assessment, and safety improvement.Recent advances in surgical phase recognition technology have focused primarilyon Transform-based methods, although methods that extract spatial features fromindividual frames using a CNN and video features from the resulting time seriesof spatial features using time series modeling have shown high performance.However, there remains a paucity of research on training methods for CNNsemployed for feature extraction or representation learning in surgical phaserecognition. In this study, we propose a method for representation learning insurgical workflow analysis using a vision-language model (ReSW-VL). Ourproposed method involves fine-tuning the image encoder of a CLIP (ConvolutionalLanguage Image Model) vision-language model using prompt learning for surgicalphase recognition. The experimental results on three surgical phase recognitiondatasets demonstrate the effectiveness of the proposed method in comparison toconventional methods.</description>
      <author>example@mail.com (Satoshi Kondo)</author>
      <guid isPermaLink="false">2505.13746v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Byte Pair Encoding for Efficient Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages in total, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于模式的时序序列化方法，旨在提高时序分析的效率和预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的时序序列化方法将固定数量的样本编码为单个标记，导致对简单模式（如长期恒定值）产生过多的标记，增加了计算开销。&lt;h4&gt;目的&lt;/h4&gt;设计一种灵活的时序序列化方案，减少标记数量，提高时序分析的计算效率。&lt;h4&gt;方法&lt;/h4&gt;基于频繁模式创建离散词汇，将具有相同模式的样本合并为标记，并引入条件解码作为轻量级后处理优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于模式的方法在时间序列基础模型上提高了36%的预测性能，并平均提高了1990%的效率。条件解码进一步将均方误差降低了高达44%。&lt;h4&gt;结论&lt;/h4&gt;该方法能够适应不同的时间模式，推广到未见数据，并且能捕捉到时序的统计矩和趋势等特征。&lt;h4&gt;翻译&lt;/h4&gt;Existing time series tokenization methods predominantly encode a constant number of samples into individual tokens. This inflexible approach can generate excessive tokens for even simple patterns like extended constant values, resulting in substantial computational overhead. Inspired by the success of byte pair encoding, we propose the first pattern-centric tokenization scheme for time series analysis. Based on a discrete vocabulary of frequent motifs, our method merges samples with underlying patterns into tokens, compressing time series adaptively. Exploiting our finite set of motifs and the continuous properties of time series, we further introduce conditional decoding as a lightweight yet powerful post-hoc optimization method, which requires no gradient computation and adds no computational overhead. On recent time series foundation models, our motif-based tokenization improves forecasting performance by 36% and boosts efficiency by 1990% on average. Conditional decoding further reduces MSE by up to 44%. In an extensive analysis, we demonstrate the adaptiveness of our tokenization to diverse temporal patterns, its generalization to unseen data, and its meaningful token representations capturing distinct time series properties, including statistical moments and trends.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing time series tokenization methods predominantly encode a constantnumber of samples into individual tokens. This inflexible approach can generateexcessive tokens for even simple patterns like extended constant values,resulting in substantial computational overhead. Inspired by the success ofbyte pair encoding, we propose the first pattern-centric tokenization schemefor time series analysis. Based on a discrete vocabulary of frequent motifs,our method merges samples with underlying patterns into tokens, compressingtime series adaptively. Exploiting our finite set of motifs and the continuousproperties of time series, we further introduce conditional decoding as alightweight yet powerful post-hoc optimization method, which requires nogradient computation and adds no computational overhead. On recent time seriesfoundation models, our motif-based tokenization improves forecastingperformance by 36% and boosts efficiency by 1990% on average. Conditionaldecoding further reduces MSE by up to 44%. In an extensive analysis, wedemonstrate the adaptiveness of our tokenization to diverse temporal patterns,its generalization to unseen data, and its meaningful token representationscapturing distinct time series properties, including statistical moments andtrends.</description>
      <author>example@mail.com (Leon Götz, Marcel Kollovieh, Stephan Günnemann, Leo Schwinn)</author>
      <guid isPermaLink="false">2505.14411v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking</title>
      <link>http://arxiv.org/abs/2505.14402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要介绍了OmniGenBench，这是一个用于基因组学基础模型（GFMs）的模块化基准测试平台，旨在统一数据、模型、基准测试和可解释性层，以促进GFMs的标准化评估和可重复性。&lt;h4&gt;背景&lt;/h4&gt;自然代码，即DNA和RNA基因组中的代码，自生命起源以来就存在，它通过基因组建模对人类和生态系统有巨大潜力。基因组基础模型（GFMs）已成为解码基因的一种变革性方法。&lt;h4&gt;目的&lt;/h4&gt;随着GFMs的扩展和重塑AI驱动基因组学的格局，该领域迫切需要严格且可重复的评估。&lt;h4&gt;方法&lt;/h4&gt;OmniGenBench是一个模块化基准测试平台，它允许对任何GFMs进行标准化、一键式评估，并集成了超过31个开源模型。该平台通过自动化管道和社区可扩展功能，解决了数据透明度、模型互操作性、基准碎片化和黑盒可解释性等关键可重复性挑战。&lt;h4&gt;主要发现&lt;/h4&gt;OmniGenBench旨在作为可重复基因组AI研究的坚实基础，加速可信赖的发现和基因组规模建模时代的协作创新。&lt;h4&gt;结论&lt;/h4&gt;OmniGenBench为基因组学基础模型提供了统一的评估框架，有助于提高研究的可重复性和可信度，促进基因组学AI领域的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自然代码，自生命起源以来就嵌入在DNA和RNA基因组中，通过基因组建模对人类和生态系统具有巨大潜力。基因组基础模型（GFMs）已经成为解码基因的一种变革性方法。随着GFMs的扩展和重塑AI驱动基因组学的格局，该领域迫切需要严格且可重复的评估。我们提出了OmniGenBench，这是一个模块化的基准测试平台，旨在统一GFMs的数据、模型、基准测试和可解释性层。OmniGenBench允许对任何GFMs进行标准化、一键式评估，并集成了超过31个开源模型。通过自动化管道和社区可扩展功能，该平台解决了数据透明度、模型互操作性、基准碎片化和黑盒可解释性等关键可重复性挑战。OmniGenBench旨在作为可重复基因组AI研究的坚实基础，加速可信赖的发现和基因组规模建模时代的协作创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The code of nature, embedded in DNA and RNA genomes since the origin of life,holds immense potential to impact both humans and ecosystems through genomemodeling. Genomic Foundation Models (GFMs) have emerged as a transformativeapproach to decoding the genome. As GFMs scale up and reshape the landscape ofAI-driven genomics, the field faces an urgent need for rigorous andreproducible evaluation. We present OmniGenBench, a modular benchmarkingplatform designed to unify the data, model, benchmarking, and interpretabilitylayers across GFMs. OmniGenBench enables standardized, one-command evaluationof any GFM across five benchmark suites, with seamless integration of over 31open-source models. Through automated pipelines and community-extensiblefeatures, the platform addresses critical reproducibility challenges, includingdata transparency, model interoperability, benchmark fragmentation, andblack-box interpretability. OmniGenBench aims to serve as foundationalinfrastructure for reproducible genomic AI research, accelerating trustworthydiscovery and collaborative innovation in the era of genome-scale modeling.</description>
      <author>example@mail.com (Heng Yang, Jack Cole, Yuan Li, Renzhi Chen, Geyong Min, Ke Li)</author>
      <guid isPermaLink="false">2505.14402v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning</title>
      <link>http://arxiv.org/abs/2505.13085v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended report of the article accepted at Interspeech 2025 (v1)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过通用语音编解码器（USC）的说话人隐私保护表示学习方法，以解决使用人类语音录音训练大型语言模型可能带来的隐私问题。&lt;h4&gt;背景&lt;/h4&gt;使用人类语音录音训练大型语言模型可能引发隐私担忧，因为这些模型可能生成与训练数据中类似的作品。&lt;h4&gt;目的&lt;/h4&gt;提出一种隐私保护的表示学习方法，以在保持语音内容的同时，去除可能识别说话人的属性。&lt;h4&gt;方法&lt;/h4&gt;通过USC将语音分解为：隐私保护的语义丰富表示，捕捉内容和言语副语言；以及残差声学和说话人表示，实现高保真重建。&lt;h4&gt;主要发现&lt;/h4&gt;USC的语义表示保留了内容、韵律和情感，同时去除了可能的可识别说话人属性。结合两种表示，USC实现了最先进的语音重建。&lt;h4&gt;结论&lt;/h4&gt;USC在隐私保护表示学习方面表现出色，证明了在学习的语义表示中，说话人匿名化、副语言保留和内容保留之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;The use of audio recordings of human speech to train LLMs poses privacy concerns due to these models' potential to generate outputs that closely resemble artifacts in the training data. In this study, we propose a speaker privacy-preserving representation learning method through the Universal Speech Codec (USC), a computationally efficient encoder-decoder model that disentangles speech into: (i) privacy-preserving semantically rich representations, capturing content and speech paralinguistics, and (ii) residual acoustic and speaker representations that enable high-fidelity reconstruction. Extensive evaluations presented show that USC's semantic representation preserves content, prosody, and sentiment, while removing potentially identifiable speaker attributes. Combining both representations, USC achieves state-of-the-art speech reconstruction. Additionally, we introduce an evaluation methodology for measuring privacy-preserving properties, aligning with perceptual tests. We compare USC against other codecs in the literature and demonstrate its effectiveness on privacy-preserving representation learning, illustrating the trade-offs of speaker anonymization, paralinguistics retention and content preservation in the learned semantic representations. Audio samples are shared in https://www.amazon.science/usc-samples.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of audio recordings of human speech to train LLMs poses privacyconcerns due to these models' potential to generate outputs that closelyresemble artifacts in the training data. In this study, we propose a speakerprivacy-preserving representation learning method through the Universal SpeechCodec (USC), a computationally efficient encoder-decoder model thatdisentangles speech into: (i) privacy-preserving semantically richrepresentations, capturing content and speech paralinguistics, and (ii)residual acoustic and speaker representations that enables high-fidelityreconstruction. Extensive evaluations presented show that USC's semanticrepresentation preserves content, prosody, and sentiment, while removingpotentially identifiable speaker attributes. Combining both representations,USC achieves state-of-the-art speech reconstruction. Additionally, we introducean evaluation methodology for measuring privacy-preserving properties, aligningwith perceptual tests. We compare USC against other codecs in the literatureand demonstrate its effectiveness on privacy-preserving representationlearning, illustrating the trade-offs of speaker anonymization, paralinguisticsretention and content preservation in the learned semantic representations.Audio samples are shared in https://www.amazon.science/usc-samples.</description>
      <author>example@mail.com (Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Rädel, Grant Strimel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood)</author>
      <guid isPermaLink="false">2505.13085v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds</title>
      <link>http://arxiv.org/abs/2505.14396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 9 pages for the main paper, 20 pages for the references and  appendix, 25 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Causal Cartographer的框架，旨在解决因果世界模型中的挑战，包括对环境进行反事实预测、理解和建模因果关系，以及评估真实世界应用中的反事实。&lt;h4&gt;背景&lt;/h4&gt;因果世界模型能够回答关于特定环境的反事实问题，但目前这一任务对基础模型，尤其是大型语言模型（LLMs）来说具有挑战性，因为它们无法展示超越记忆现有因果关系之外的因果推理能力。&lt;h4&gt;目的&lt;/h4&gt;本文的目的是提出一种方法来提取和建模因果关系，并构建一个能够支持因果推理和反事实生成的框架。&lt;h4&gt;方法&lt;/h4&gt;该框架包括一个图检索增强的生成代理，用于从数据中检索因果关系，以及一个受因果关系约束的反事实推理代理，以执行可靠的因果推理。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，该方法可以提取因果知识，提高LLMs在因果推理任务中的鲁棒性，同时降低推理成本和虚假相关性。&lt;h4&gt;结论&lt;/h4&gt;通过提出Causal Cartographer框架，本文解决了因果世界模型中的一些关键问题，为在真实世界中应用因果推理提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a framework called Causal Cartographer to address the challenges in causal world models, including predicting counterfactuals about an environment of interest, understanding and modeling causal relationships, and evaluating counterfactuals in real-world applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ggendro/causal-cartographer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal world models are systems that can answer counterfactual questionsabout an environment of interest, i.e. predict how it would have evolved if anarbitrary subset of events had been realized differently. It requiresunderstanding the underlying causes behind chains of events and conductingcausal inference for arbitrary unseen distributions. So far, this task eludesfoundation models, notably large language models (LLMs), which do not havedemonstrated causal reasoning capabilities beyond the memorization of existingcausal relationships. Furthermore, evaluating counterfactuals in real-worldapplications is challenging since only the factual world is observed, limitingevaluation to synthetic datasets. We address these problems by explicitlyextracting and modeling causal relationships and propose the CausalCartographer framework. First, we introduce a graph retrieval-augmentedgeneration agent tasked to retrieve causal relationships from data. Thisapproach allows us to construct a large network of real-world causalrelationships that can serve as a repository of causal knowledge and buildreal-world counterfactuals. In addition, we create a counterfactual reasoningagent constrained by causal relationships to perform reliable step-by-stepcausal inference. We show that our approach can extract causal knowledge andimprove the robustness of LLMs for causal reasoning tasks while reducinginference costs and spurious correlations.</description>
      <author>example@mail.com (Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie)</author>
      <guid isPermaLink="false">2505.14396v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Self-Reinforced Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.13650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SRGCL（Self-Reinforced Graph Contrastive Learning）的图对比学习方法，用于生成鲁棒的图表示，并详细阐述了其方法、实验结果和优势。&lt;h4&gt;背景&lt;/h4&gt;图作为数据结构在多个领域应用广泛，图对比学习（GCL）是一种有效的学习技术，但保证正样本对的质量是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出SRGCL方法，通过动态评估和选择高质量的样本对，来保留原始图的内在语义和结构属性。&lt;h4&gt;方法&lt;/h4&gt;SRGCL使用模型自身的编码器来评估和选择高质量的正样本对，设计了统一的正样本对生成器，并采用流形假设指导的选择器来维持潜在空间下的几何结构。通过概率机制选择正样本对，SRGCL随着编码器表征能力的提高，迭代地优化对样本对质量的评估。&lt;h4&gt;主要发现&lt;/h4&gt;在多个图级分类任务上的实验表明，SRGCL作为一个插件模块，在多个领域内均优于最先进的GCL方法。&lt;h4&gt;结论&lt;/h4&gt;SRGCL方法具有适应性和有效性，能够生成鲁棒的图表示，并在多个领域内提高图对比学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;Graphs serve as versatile data structures in numerous real-world domains-including social networks, molecular biology, and knowledge graphs-bycapturing intricate relational information among entities. Among graph-based learning techniques, Graph Contrastive Learning (GCL) has gained significant attention for its ability to derive robust, self-supervised graph representations through the contrasting of positive and negative sample pairs. However, a critical challenge lies in ensuring high-quality positive pairs so that the intrinsic semantic and structural properties of the original graph are preserved rather than distorted. To address this issue, we propose SRGCL (Self-Reinforced Graph Contrastive Learning), a novel framework that leverages the model's own encoder to dynamically evaluate and select high-quality positive pairs. We designed a unified positive pair generator employing multiple augmentation strategies, and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism for selecting positive pairs, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Extensive experiments on diverse graph-level classification tasks demonstrate that SRGCL, as a plug-in module, consistently outperforms state-of-the-art GCL methods, underscoring its adaptability and efficacy across various domains.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs serve as versatile data structures in numerous real-worlddomains-including social networks, molecular biology, and knowledge graphs-bycapturing intricate relational information among entities. Among graph-basedlearning techniques, Graph Contrastive Learning (GCL) has gained significantattention for its ability to derive robust, self-supervised graphrepresentations through the contrasting of positive and negative sample pairs.However, a critical challenge lies in ensuring high-quality positive pairs sothat the intrinsic semantic and structural properties of the original graph arepreserved rather than distorted. To address this issue, we propose SRGCL(Self-Reinforced Graph Contrastive Learning), a novel framework that leveragesthe model's own encoder to dynamically evaluate and select high-qualitypositive pairs. We designed a unified positive pair generator employingmultiple augmentation strategies, and a selector guided by the manifoldhypothesis to maintain the underlying geometry of the latent space. By adoptinga probabilistic mechanism for selecting positive pairs, SRGCL iterativelyrefines its assessment of pair quality as the encoder's representational powerimproves. Extensive experiments on diverse graph-level classification tasksdemonstrate that SRGCL, as a plug-in module, consistently outperformsstate-of-the-art GCL methods, underscoring its adaptability and efficacy acrossvarious domains.</description>
      <author>example@mail.com (Chou-Ying Hsieh, Chun-Fu Jang, Cheng-En Hsieh, Qian-Hui Chen, Sy-Yen Kuo)</author>
      <guid isPermaLink="false">2505.13650v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking the Power of SAM 2 for Few-Shot Segmentation</title>
      <link>http://arxiv.org/abs/2505.14100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对Few-Shot Segmentation（FSS）问题的新方法，通过设计伪查询记忆生成器和迭代记忆精炼来提高分割的准确性。&lt;h4&gt;背景&lt;/h4&gt;Few-Shot Segmentation（FSS）旨在在少量类别上学习无类别的分割，但存在过拟合的风险。&lt;h4&gt;目的&lt;/h4&gt;为了解决FSS中的过拟合问题，本文利用SAM模型的知识简化学习过程，并设计了一种新的方法来提高分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;本文设计了伪查询记忆生成器来编码伪查询记忆，并采用迭代记忆精炼和支撑校准记忆注意力机制来提高分割的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在PASCAL-5i和COCO-20i数据集上的实验，发现该方法可以将1-shot mIoU提高4.2%，优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地提高Few-Shot Segmentation的准确性，为该领域的研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a new method for the Few-Shot Segmentation (FSS) problem, which uses pseudo-query memory generator and iterative memory refinement to improve the accuracy of segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sam1224/fssam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on fewclasses to segment arbitrary classes, but at the risk of overfitting. Toaddress this, some methods use the well-learned knowledge of foundation models(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAMby supporting video segmentation, whose class-agnostic matching ability isuseful to FSS. A simple idea is to encode support foreground (FG) features asmemory, with which query FG features are matched and fused. Unfortunately, theFG objects in different frames of SAM 2's video data are always the sameidentity, while those in FSS are different identities, i.e., the matching stepis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudoquery memory, matching with query features in a compatible way. However, thememories can never be as accurate as the real ones, i.e., they are likely tocontain incomplete query FG, and some unexpected query background (BG)features, leading to wrong segmentation. Hence, we further design IterativeMemory Refinement to fuse more query FG features into the memory, and devise aSupport-Calibrated Memory Attention to suppress the unexpected query BGfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shotmIoU can be 4.2\% better than the best baseline.</description>
      <author>example@mail.com (Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao)</author>
      <guid isPermaLink="false">2505.14100v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts</title>
      <link>http://arxiv.org/abs/2505.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Land-MoE，这是一种用于多光谱土地覆盖分类（MLCC）的新方法。&lt;h4&gt;背景&lt;/h4&gt;光谱偏移，由传感器差异和地理空间条件引起，在MLCC领域是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Land-MoE以解决现有方法依赖领域自适应和泛化策略的问题，这些方法通常使用小规模模型，性能有限。&lt;h4&gt;方法&lt;/h4&gt;Land-MoE通过层次化插入频率感知混合低秩标记专家来微调视觉基础模型（VFMs），以参数高效的方式进行。&lt;h4&gt;主要发现&lt;/h4&gt;Land-MoE包括两个关键模块：低秩标记专家混合（MoLTE）和频率感知滤波器（FAF）。MoLTE利用不同秩的标记来为多光谱图像中的单个实例生成不同的特征调整，而FAF则在频域对精炼后的特征进行调制。&lt;h4&gt;结论&lt;/h4&gt;在涉及跨传感器和跨地理空间设置的MLCC任务上，Land-MoE比现有方法有显著的优势，并在领域泛化语义分割任务中也取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Land-MoE, a novel approach for multispectral land coverclassification (MLCC). Spectral shift, which emerges from disparities insensors and geospatial conditions, poses a significant challenge in thisdomain. Existing methods predominantly rely on domain adaptation andgeneralization strategies, often utilizing small-scale models that exhibitlimited performance. In contrast, Land-MoE addresses these issues byhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.Specifically, Land-MoE comprises two key modules: the mixture of low-rank tokenexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leveragesrank-differentiated tokens to generate diverse feature adjustments forindividual instances within multispectral images. By dynamically combininglearnable low-rank token experts of varying ranks, it enhances the robustnessagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation onthe refined features. This process enables the model to effectively capturefrequency band information that is strongly correlated with semantic essence,while simultaneously suppressing frequency noise irrelevant to the task.Comprehensive experiments on MLCC tasks involving cross-sensor andcross-geospatial setups demonstrate that Land-MoE outperforms existing methodsby a large margin. Additionally, the proposed approach has also achievedstate-of-the-art performance in domain generalization semantic segmentationtasks of RGB remote sensing images.</description>
      <author>example@mail.com (Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang)</author>
      <guid isPermaLink="false">2505.14088v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Adversarially Pretrained Transformers may be Universally Robust In-Context Learners</title>
      <link>http://arxiv.org/abs/2505.14042v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对抗训练，指出其虽然有效但计算成本高。研究提出使用在多样化任务上预训练的transformer作为鲁棒的基础模型，以消除下游任务中的对抗训练需求。&lt;h4&gt;背景&lt;/h4&gt;对抗训练是有效的对抗防御方法，但伴随高计算成本。&lt;h4&gt;目的&lt;/h4&gt;证明在多样化任务上预训练的transformer可以作为鲁棒的基础模型，无需在下游任务中进行对抗训练。&lt;h4&gt;方法&lt;/h4&gt;通过理论证明，通过上下文学习，单个对抗预训练的transformer可以鲁棒地泛化到多个未见过的任务，无需任何额外的训练或参数更新。&lt;h4&gt;主要发现&lt;/h4&gt;该模型的鲁棒性源于其对鲁棒特征的聚焦以及对利用非预测特征攻击的抵抗力。同时，识别出模型的局限性，如不存在普遍鲁棒的单一层transformer，以及鲁棒transformer存在准确性和鲁棒性之间的权衡，需要大量上下文演示。&lt;h4&gt;结论&lt;/h4&gt;提出了一种无需对抗训练的鲁棒transformer模型，并指出其局限性和改进方向。&lt;h4&gt;翻译&lt;/h4&gt;Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we show that transformers adversarially pretrained on diverse tasks can serve as robust foundation models and eliminate the need for adversarial training in downstream tasks. Specifically, we theoretically demonstrate that through in-context learning, a single adversarially pretrained transformer can robustly generalize to multiple unseen tasks without any additional training, i.e., without any parameter updates. This robustness stems from the model's focus on robust features and its resistance to attacks that exploit non-predictive features. Besides these positive findings, we also identify several limitations. Under certain conditions (though unrealistic), no universally robust single-layer transformers exist. Moreover, robust transformers exhibit an accuracy--robustness trade-off and require a large number of in-context demonstrations. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial training is one of the most effective adversarial defenses, butit incurs a high computational cost. In this study, we show that transformersadversarially pretrained on diverse tasks can serve as robust foundation modelsand eliminate the need for adversarial training in downstream tasks.Specifically, we theoretically demonstrate that through in-context learning, asingle adversarially pretrained transformer can robustly generalize to multipleunseen tasks without any additional training, i.e., without any parameterupdates. This robustness stems from the model's focus on robust features andits resistance to attacks that exploit non-predictive features. Besides thesepositive findings, we also identify several limitations. Under certainconditions (though unrealistic), no universally robust single-layertransformers exist. Moreover, robust transformers exhibit anaccuracy--robustness trade-off and require a large number of in-contextdemonstrations. The code is available athttps://github.com/s-kumano/universally-robust-in-context-learner.</description>
      <author>example@mail.com (Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki)</author>
      <guid isPermaLink="false">2505.14042v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</title>
      <link>http://arxiv.org/abs/2505.13936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为R1 Translator的新模型，用于提高脑电图（EEG）到文本解码的性能，该模型在ROUGE和CER等指标上优于现有的T5和Brain Translator模型。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型如Gemini、GPT等的快速发展，将人脑与语言处理之间的差距缩小成为研究的重要方向。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有EEG到文本解码模型的性能限制，提出R1 Translator模型。&lt;h4&gt;方法&lt;/h4&gt;R1 Translator模型结合了双向LSTM编码器和预训练的基于transformer的解码器，利用EEG特征生成高质量的文本输出。模型通过LSTM处理EEG嵌入以捕捉序列依赖性，然后将这些信息输入到transformer解码器中进行有效的文本生成。&lt;h4&gt;主要发现&lt;/h4&gt;R1 Translator在ROUGE-1和ROUGE-L指标上均优于T5和Brain Translator，同时在CER和WER指标上也表现出色。&lt;h4&gt;结论&lt;/h4&gt;R1 Translator是一种有效的EEG到文本解码模型，在多个性能指标上均优于现有的方法。&lt;h4&gt;翻译&lt;/h4&gt;R1翻译器模型通过结合双向LSTM编码器和预训练的transformer解码器，结合脑电图特征生成高质量的文本输出，在多个性能指标上优于现有的T5和Brain翻译器模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mmurrad/eeg-to-text&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of large language models like Gemini, GPT, andothers, bridging the gap between the human brain and language processing hasbecome an important area of focus. To address this challenge, researchers havedeveloped various models to decode EEG signals into text. However, these modelsstill face significant performance limitations. To overcome these shortcomings,we propose a new model, R1 Translator, which aims to improve the performance ofEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTMencoder with a pretrained transformer-based decoder, utilizing EEG features toproduce high-quality text outputs. The model processes EEG embeddings throughthe LSTM to capture sequential dependencies, which are then fed into thetransformer decoder for effective text generation. The R1 Translator excels inROUGE metrics, outperforming both T5 (previous research) and Brain Translator.Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads inROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brainby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lowerthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performsbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) andBrain by 3.6% (0.7553). Code is available athttps://github.com/Mmurrad/EEG-To-text.</description>
      <author>example@mail.com (Saydul Akbar Murad, Ashim Dahal, Nick Rahimi)</author>
      <guid isPermaLink="false">2505.13936v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>EuLearn: A 3D database for learning Euler characteristics</title>
      <link>http://arxiv.org/abs/2505.13539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, many figures. Datasets and source code publicly available  at https://huggingface.co/datasets/appliedgeometry/EuLearn and  https://github.com/appliedgeometry/EuLearn_db&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了EuLearn，这是一个首次公平地代表多种拓扑类型的表面数据集。通过使用随机结设计具有均匀变化次数的嵌入表面，使得表面可以自缠绕。EuLearn贡献了新的拓扑数据集，包括3D网格、点云和标量场，旨在促进能够识别拓扑特征的机器学习系统的训练。&lt;h4&gt;背景&lt;/h4&gt;目前还没有一个表面数据集能够公平地代表多种拓扑类型。&lt;h4&gt;目的&lt;/h4&gt;促进能够识别拓扑特征的机器学习系统的训练。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于随机结的均匀变化次数的嵌入表面，并开发了非欧几里得统计采样方法以及基于该方法的PointNet和Transformer架构的邻接信息自适应版本。&lt;h4&gt;主要发现&lt;/h4&gt;传统的3D神经网络架构在拓扑类型分类上表现不佳，而引入拓扑信息可以显著提高性能。&lt;h4&gt;结论&lt;/h4&gt;EuLearn数据集在拓扑特征识别方面具有挑战性，但通过结合拓扑信息，可以显著提高深度学习工作流程的性能。&lt;h4&gt;翻译&lt;/h4&gt;We present EuLearn, the first surface datasets equitably representing a diversity of topological types. We designed our embedded surfaces of uniformly varying genera relying on random knots, thus allowing our surfaces to knot with themselves. EuLearn contributes new topological datasets of meshes, point clouds, and scalar fields in 3D. We aim to facilitate the training of machine learning systems that can discern topological features. We experimented with specific emblematic 3D neural network architectures, finding that their vanilla implementations perform poorly on genus classification. To enhance performance, we developed a novel, non-Euclidean, statistical sampling method adapted to graph and manifold data. We also introduce adjacency-informed adaptations of PointNet and Transformer architectures that rely on our non-Euclidean sampling strategy. Our results demonstrate that incorporating topological information into deep learning workflows significantly improves performance on these otherwise challenging EuLearn datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present EuLearn, the first surface datasets equitably representing adiversity of topological types. We designed our embedded surfaces of uniformlyvarying genera relying on random knots, thus allowing our surfaces to knot withthemselves. EuLearn contributes new topological datasets of meshes, pointclouds, and scalar fields in 3D. We aim to facilitate the training of machinelearning systems that can discern topological features. We experimented withspecific emblematic 3D neural network architectures, finding that their vanillaimplementations perform poorly on genus classification. To enhance performance,we developed a novel, non-Euclidean, statistical sampling method adapted tograph and manifold data. We also introduce adjacency-informed adaptations ofPointNet and Transformer architectures that rely on our non-Euclidean samplingstrategy. Our results demonstrate that incorporating topological informationinto deep learning workflows significantly improves performance on theseotherwise challenging EuLearn datasets.</description>
      <author>example@mail.com (Rodrigo Fritz, Pablo Suárez-Serrato, Victor Mijangos, Anayanzi D. Martinez-Hernandez, Eduardo Ivan Velazquez Richards)</author>
      <guid isPermaLink="false">2505.13539v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation</title>
      <link>http://arxiv.org/abs/2505.13919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EnvAd-Diff的模型权重生成方法，用于跨环境预测物理系统的动态行为。&lt;h4&gt;背景&lt;/h4&gt;数据驱动方法可以预测物理动态，但同一物理系统在不同环境中可能表现出不同的动态行为，导致特定环境训练的预测函数在未见过的环境中失效。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在通过建模不同环境的动态函数来提高跨环境预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;EnvAd-Diff方法首先在有限可见环境中的动态轨迹上训练专家预测函数，构建模型库，并创建预测函数权重与环境对应的样本对。然后，训练一个条件于环境的潜在空间扩散模型来建模权重和环境联合分布。此外，针对现实场景中缺乏环境先验知识的问题，提出了基于物理信息的代理标签来区分不同环境。&lt;h4&gt;主要发现&lt;/h4&gt;跨多个系统的泛化实验表明，由EnvAd-Diff生成的1M参数预测函数优于预训练的500M参数基础模型。&lt;h4&gt;结论&lt;/h4&gt;EnvAd-Diff方法能够有效地进行跨环境预测，并显著提高预测函数的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data-driven methods offer an effective equation-free solution for predictingphysical dynamics. However, the same physical system can exhibit significantlydifferent dynamic behaviors in various environments. This causes predictionfunctions trained for specific environments to fail when transferred to unseenenvironments. Therefore, cross-environment prediction requires modeling thedynamic functions of different environments. In this work, we propose a modelweight generation method, \texttt{EnvAd-Diff}. \texttt{EnvAd-Diff} operates inthe weight space of the dynamic function, generating suitable weights fromscratch based on environmental condition for zero-shot prediction.Specifically, we first train expert prediction functions on dynamictrajectories from a limited set of visible environments to create a model zoo,thereby constructing sample pairs of prediction function weights and theircorresponding environments. Subsequently, we train a latent space diffusionmodel conditioned on the environment to model the joint distribution of weightsand environments. Considering the lack of environmental prior knowledge inreal-world scenarios, we propose a physics-informed surrogate label todistinguish different environments. Generalization experiments across multiplesystems demonstrate that a 1M parameter prediction function generated by\texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.</description>
      <author>example@mail.com (Ruikun Li, Huandong Wang, Jingtao Ding, Yuan Yuan, Qingmin Liao, Yong Li)</author>
      <guid isPermaLink="false">2505.13919v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video Retrieval</title>
      <link>http://arxiv.org/abs/2505.12499v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GARE的Gap-Aware Retrieval框架，用于解决文本-视频检索中的模态间隙和批量采样中假阴性问题，通过引入可学习的增量Delta_ij来缓解优化张力。&lt;h4&gt;背景&lt;/h4&gt;现有的文本-视频检索方法忽略了文本和视频分布之间的模态间隙和批量采样中的假阴性问题，这些问题导致了InfoNCE损失下的梯度冲突，阻碍了稳定对齐。&lt;h4&gt;目的&lt;/h4&gt;提出GARE框架，以缓解文本-视频检索中的优化张力，提高检索的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;GARE通过引入一个可学习的增量Delta_ij来缓解张力，该增量通过耦合的多变量一阶泰勒近似和信任域约束来优化。同时，引入一个轻量级神经网络模块，该模块基于视频-文本对之间的语义间隙进行结构感知校正，并通过三个正则化项来稳定学习和提高可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GARE在四个检索基准上均提高了对齐准确性和对噪声监督的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GARE框架通过缓解模态间隙带来的优化张力，有效提高了文本-视频检索的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in text-video retrieval have been largely driven by contrastive learning frameworks. However, existing methods overlook a key source of optimization tension: the separation between text and video distributions in the representation space (referred to as the modality gap), and the prevalence of false negatives in batch sampling. These factors lead to conflicting gradients under the InfoNCE loss, impeding stable alignment. To mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces a learnable, pair-specific increment Delta_ij between text t_i and video v_j to offload the tension from the global anchor representation. We first derive the ideal form of Delta_ij via a coupled multivariate first-order Taylor approximation of the InfoNCE loss under a trust-region constraint, revealing it as a mechanism for resolving gradient conflicts by guiding updates along a locally optimal descent direction. Due to the high cost of directly computing Delta_ij, we introduce a lightweight neural module conditioned on the semantic gap between each video-text pair, enabling structure-aware correction guided by gradient supervision. To further stabilize learning and promote interpretability, we regularize Delta using three components: a trust-region constraint to prevent oscillation, a directional diversity term to promote semantic coverage, and an information bottleneck to limit redundancy. Experiments across four retrieval benchmarks show that GARE consistently improves alignment accuracy and robustness to noisy supervision, confirming the effectiveness of gap-aware tension mitigation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/musicman217/gare-text-video-retrieval&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-video retrieval have been largely driven bycontrastive learning frameworks. However, existing methods overlook a keysource of optimization tension: the separation between text and videodistributions in the representation space (referred to as the modality gap),and the prevalence of false negatives in batch sampling. These factors lead toconflicting gradients under the InfoNCE loss, impeding stable alignment. Tomitigate this, we propose GARE, a Gap-Aware Retrieval framework that introducesa learnable, pair-specific increment Delta_ij between text t_i and video v_j tooffload the tension from the global anchor representation. We first derive theideal form of Delta_ij via a coupled multivariate first-order Taylorapproximation of the InfoNCE loss under a trust-region constraint, revealing itas a mechanism for resolving gradient conflicts by guiding updates along alocally optimal descent direction. Due to the high cost of directly computingDelta_ij, we introduce a lightweight neural module conditioned on the semanticgap between each video-text pair, enabling structure-aware correction guided bygradient supervision. To further stabilize learning and promoteinterpretability, we regularize Delta using three components: a trust-regionconstraint to prevent oscillation, a directional diversity term to promotesemantic coverage, and an information bottleneck to limit redundancy.Experiments across four retrieval benchmarks show that GARE consistentlyimproves alignment accuracy and robustness to noisy supervision, confirming theeffectiveness of gap-aware tension mitigation.</description>
      <author>example@mail.com (Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong)</author>
      <guid isPermaLink="false">2505.12499v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Time-R1: Towards Comprehensive Temporal Reasoning in LLMs</title>
      <link>http://arxiv.org/abs/2505.13508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Time-R1框架，该框架旨在提升大型语言模型（LLMs）的时序智能能力，包括理解、预测和创造性生成。&lt;h4&gt;背景&lt;/h4&gt;现有的LLMs在时序智能方面存在不足，难以整合对过去事件的推理与对未来预测，且在知识截止点之外的事件或需要创造性预见的事件上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出Time-R1框架，为中等规模（3B参数）的LLM提供全面的时序能力。&lt;h4&gt;方法&lt;/h4&gt;Time-R1采用新颖的三阶段发展路径：1）通过强化学习（RL）课程和动态规则奖励系统建立基础时序理解和逻辑事件时间映射；2）预测知识截止点之外的事件；3）实现创造性未来场景生成的泛化能力，无需微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Time-R1在预测未来事件和创造性场景生成方面优于200倍以上的模型，包括最先进的671B DeepSeek-R1。&lt;h4&gt;结论&lt;/h4&gt;精心设计的渐进式RL微调可以使小型、高效的模型实现优越的时序性能，为真正时间感知的AI提供了实际且可扩展的途径。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce extit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a extit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event predictions skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release extit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of extit{Time-R1} checkpoints.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) demonstrate impressive capabilities but lackrobust temporal intelligence, struggling to integrate reasoning about the pastwith predictions and plausible generations of the future. Meanwhile, existingmethods typically target isolated temporal skills, such as question answeringabout past events or basic forecasting, and exhibit poor generalization,particularly when dealing with events beyond their knowledge cutoff orrequiring creative foresight. To address these limitations, we introduce\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)LLM with comprehensive temporal abilities: understanding, prediction, andcreative generation. Our approach features a novel three-stage developmentpath; the first two constitute a \textit{reinforcement learning (RL)curriculum} driven by a meticulously designed dynamic rule-based reward system.This framework progressively builds (1) foundational temporal understanding andlogical event-time mappings from historical data, (2) future event predictionskills for events beyond its knowledge cutoff, and finally (3) enablesremarkable generalization to creative future scenario generation without anyfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperformsmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,on highly challenging future event prediction and creative scenario generationbenchmarks. This work provides strong evidence that thoughtfully engineered,progressive RL fine-tuning allows smaller, efficient models to achieve superiortemporal performance, offering a practical and scalable path towards trulytime-aware AI. To foster further research, we also release \textit{Time-Bench},a large-scale multi-task temporal reasoning dataset derived from 10 years ofnews data, and our series of \textit{Time-R1} checkpoints.</description>
      <author>example@mail.com (Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You)</author>
      <guid isPermaLink="false">2505.13508v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>EfficientLLM: Efficiency in Large Language Models</title>
      <link>http://arxiv.org/abs/2505.13840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EfficientLLM是一个新的基准，对大规模语言模型的效率技术进行了全面实证研究，评估了架构预训练、微调和推理等方面的效率。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在参数数量和上下文窗口增长的同时，带来了高昂的计算、能源和货币成本。&lt;h4&gt;目的&lt;/h4&gt;介绍EfficientLLM，评估大规模语言模型的效率技术。&lt;h4&gt;方法&lt;/h4&gt;在48xGH200，8xH200 GPU的生产级集群上，系统探索了三个关键轴：架构预训练（高效的注意力变体：MQA、GQA、MLA、NSA；稀疏混合专家MoE）、微调（参数高效的LoRA、RSLoRA、DoRA）和推理（量化方法：int4、float16）。定义了六个细粒度指标（内存利用率、计算利用率、延迟、吞吐量、能耗、压缩率）。&lt;h4&gt;主要发现&lt;/h4&gt;1. 效率涉及可量化的权衡，没有一种方法在所有情况下都是最优的；2. 最优解与任务和规模相关；3. 技术可以在不同模态中推广。&lt;h4&gt;结论&lt;/h4&gt;EfficientLLM为研究人员和工程师提供了关于下一代基础模型效率-性能景观的必要指导。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）推动了显著的进步，但它们的参数数量和上下文窗口的增长带来了难以承受的计算、能源和货币成本。我们介绍了EfficientLLM，这是一个新的基准，也是第一个全面实证研究大规模LLM效率技术的。在我们的研究（在48xGH200，8xH200 GPU的生产级集群上）中，我们系统地探索了三个关键轴：1）架构预训练（高效的注意力变体：MQA、GQA、MLA、NSA；稀疏混合专家MoE），2）微调（参数高效的LoRA、RSLoRA、DoRA），3）推理（量化方法：int4、float16）。我们定义了六个细粒度指标（内存利用率、计算利用率、延迟、吞吐量、能耗、压缩率）来捕捉硬件饱和度、延迟-吞吐量平衡和碳成本。评估了100多个模型-技术对（0.5B-72B参数），我们得出了三个核心见解：（i）效率涉及可量化的权衡：没有一种方法在所有情况下都是最优的；例如，MoE减少了FLOPs并提高了精度，但增加了40%的VRAM，而int4量化在3-5%的精度下降的情况下将内存/能耗减少了多达3.9倍。（ii）最优解与任务和规模相关：MQA为受限设备提供了最佳的内存-延迟权衡，MLA实现了质量关键任务的最低困惑度，而RSLoRA只在超过14B参数时才超过了LoRA的效率。（iii）技术可以在不同模态中推广：我们将评估扩展到大型视觉模型（Stable Diffusion 3.5，Wan 2.1）和视觉-语言模型（Qwen2.5-VL），证实了有效的迁移性。通过开源数据集、评估管道和排行榜，EfficientLLM为研究人员和工程师提供了关于下一代基础模型效率-性能景观的必要指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have driven significant progress, yet theirgrowing parameter counts and context windows incur prohibitive compute, energy,and monetary costs. We introduce EfficientLLM, a novel benchmark and the firstcomprehensive empirical study evaluating efficiency techniques for LLMs atscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), ourstudy systematically explores three key axes: (1) architecture pretraining(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and(3) inference (quantization methods: int4, float16). We define six fine-grainedmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, EnergyConsumption, Compression Rate) to capture hardware saturation,latency-throughput balance, and carbon cost. Evaluating over 100model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)Efficiency involves quantifiable trade-offs: no single method is universallyoptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimalmemory-latency trade-offs for constrained devices, MLA achieves lowestperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiencyonly beyond 14B parameters. (iii) Techniques generalize across modalities: weextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) andVision-Language Models (Qwen2.5-VL), confirming effective transferability. Byopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLMprovides essential guidance for researchers and engineers navigating theefficiency-performance landscape of next-generation foundation models.</description>
      <author>example@mail.com (Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye)</author>
      <guid isPermaLink="false">2505.13840v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Panda: A pretrained forecast model for universal representation of chaotic dynamics</title>
      <link>http://arxiv.org/abs/2505.13755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Panda的模型，用于预测非线性动力系统，特别是混沌系统。&lt;h4&gt;背景&lt;/h4&gt;混沌系统对微小误差非常敏感，这使得构建预测模型具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;通过提出Panda模型，旨在解决混沌系统预测的难题。&lt;h4&gt;方法&lt;/h4&gt;Panda模型在新型合成数据集上训练，该数据集包含使用进化算法发现的20000个混沌动力系统。&lt;h4&gt;主要发现&lt;/h4&gt;Panda模型在模拟数据上表现出零样本预测未见过的真实世界混沌系统的能力，并在交叉通道注意力头中展现出非线性共振模式。此外，Panda模型能够在不重新训练的情况下预测偏微分方程。&lt;h4&gt;结论&lt;/h4&gt;Panda模型展示了预训练模型在探索非线性动力学等抽象数学领域中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：混沌系统本质上对微小误差非常敏感，这给构建真实世界动力系统（如流体流动或神经元活动）的预测数据驱动模型带来了挑战。以往的努力包括专门针对单个时间序列训练的模型，或者基于大量时间序列数据库且底层动力结构很少的基础模型。受动力系统理论的启发，我们提出了Panda，即针对非线性动力学的修补注意力（PatchedAttention for Nonlinear DynAmics）。我们使用一种新的合成、可扩展的数据集训练Panda，该数据集包含我们使用进化算法发现的20000个混沌动力系统。Panda仅在模拟数据上训练，表现出涌现特性：对未见过的真实世界混沌系统的零样本预测，以及在交叉通道注意力头中的非线性共振模式。尽管Panda仅在低维常微分方程上训练，但它自发地发展了预测偏微分方程的能力，而无需重新训练。我们展示了微分方程的神经缩放定律，强调了预训练模型在探索非线性动力学等抽象数学领域中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chaotic systems are intrinsically sensitive to small errors, challengingefforts to construct predictive data-driven models of real-world dynamicalsystems such as fluid flows or neuronal activity. Prior efforts comprise eitherspecialized models trained separately on individual time series, or foundationmodels trained on vast time series databases with little underlying dynamicalstructure. Motivated by dynamical systems theory, we present Panda, PatchedAttention for Nonlinear DynAmics. We train Panda on a novel synthetic,extensible dataset of $2 \times 10^4$ chaotic dynamical systems that wediscover using an evolutionary algorithm. Trained purely on simulated data,Panda exhibits emergent properties: zero-shot forecasting of unseen real worldchaotic systems, and nonlinear resonance patterns in cross-channel attentionheads. Despite having been trained only on low-dimensional ordinarydifferential equations, Panda spontaneously develops the ability to predictpartial differential equations without retraining. We demonstrate a neuralscaling law for differential equations, underscoring the potential ofpretrained models for probing abstract mathematical domains like nonlineardynamics.</description>
      <author>example@mail.com (Jeffrey Lai, Anthony Bao, William Gilpin)</author>
      <guid isPermaLink="false">2505.13755v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Context Diffusion Policies via Past-Token Prediction</title>
      <link>http://arxiv.org/abs/2505.09561v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Videos are available at https://long-context-dp.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法来处理长序列观察和动作的推理问题，通过显式正则化过去信息的保留来提高长上下文策略的学习效果。&lt;h4&gt;背景&lt;/h4&gt;长序列的推理对于许多机器人任务至关重要，但学习有效的长上下文策略仍然具有挑战性。随着上下文长度的增加，训练成本上升，记忆需求增加，而且策略性能往往因为虚假相关性而下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，旨在解决长上下文策略学习中存在的问题，如记忆需求高和策略性能下降。&lt;h4&gt;方法&lt;/h4&gt;1. 回顾模仿学习中的copycat问题，并识别出近期扩散策略中的挑战；2. 提出Past-Token Prediction (PTP)，一个辅助任务，让策略学习同时预测过去和未来的动作标记；3. 采用多阶段训练策略，预训练视觉编码器使用短上下文，并使用缓存的长上下文嵌入微调策略头部；4. 将PTP扩展为测试时的自我验证机制，使策略在推理过程中能够评分和选择与过去动作一致的候选方案。&lt;h4&gt;主要发现&lt;/h4&gt;PTP正则化显著提高了策略头部的时序建模能力，同时减少了对外部视觉表示的依赖。多阶段训练策略能够保留PTP的好处，同时大大降低内存和计算开销。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，该方法将长上下文扩散策略的性能提高了3倍，并使策略训练速度提高了10倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning over long sequences of observations and actions is essential formany robotic tasks. Yet, learning effective long-context policies fromdemonstrations remains challenging. As context length increases, trainingbecomes increasingly expensive due to rising memory demands, and policyperformance often degrades as a result of spurious correlations. Recent methodstypically sidestep these issues by truncating context length, discardinghistorical information that may be critical for subsequent decisions. In thispaper, we propose an alternative approach that explicitly regularizes theretention of past information. We first revisit the copycat problem inimitation learning and identify an opposite challenge in recent diffusionpolicies: rather than over-relying on prior actions, they often fail to captureessential dependencies between past and future actions. To address this, weintroduce Past-Token Prediction (PTP), an auxiliary task in which the policylearns to predict past action tokens alongside future ones. This regularizationsignificantly improves temporal modeling in the policy head, with minimalreliance on visual representations. Building on this observation, we furtherintroduce a multistage training strategy: pre-train the visual encoder withshort contexts, and fine-tune the policy head using cached long-contextembeddings. This strategy preserves the benefits of PTP while greatly reducingmemory and computational overhead. Finally, we extend PTP into aself-verification mechanism at test time, enabling the policy to score andselect candidates consistent with past actions during inference. Experimentsacross four real-world and six simulated tasks demonstrate that our proposedmethod improves the performance of long-context diffusion policies by 3x andaccelerates policy training by more than 10x.</description>
      <author>example@mail.com (Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2505.09561v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Industrial Synthetic Segment Pre-training</title>
      <link>http://arxiv.org/abs/2505.13099v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InsCore的合成预训练数据集，用于工业应用场景下的实例分割，并通过实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;当前工业应用中，实例分割面临法律伦理限制和领域差距问题，导致基于真实图像的预训练模型效果不佳。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以构建不依赖真实图像或人工标注的视觉基础模型，并在工业数据集上超越经过微调的SAM模型。&lt;h4&gt;方法&lt;/h4&gt;构建了InsCore数据集，基于公式驱动的监督学习（FDSL）生成标注的实例分割图像，反映了工业数据的特征，包括复杂遮挡、密集层次掩码和多样的非刚性形状。&lt;h4&gt;主要发现&lt;/h4&gt;使用InsCore预训练的模型在五个工业数据集上优于使用COCO和ImageNet-21k预训练的模型以及微调的SAM模型，平均实例分割性能提高了6.2个百分点，且只需10000张合成图像。&lt;h4&gt;结论&lt;/h4&gt;InsCore是一种实用且无需许可的视觉基础模型，适用于工业应用场景，并展示了数据的高效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在真实图像数据集上进行预训练已被证明对提高实例分割效果非常有效。然而，工业应用面临两个关键挑战：（1）法律和伦理限制，例如ImageNet禁止商业使用；（2）由于网络图像和工业图像之间的领域差距，可迁移性有限。即使最近的视觉基础模型，包括任何东西分割模型（SAM），在工业环境下也表现出明显的性能下降。这些挑战提出了关键问题：我们能否在不依赖真实图像或人工标注的情况下构建适用于工业应用的视觉基础模型？并且这样的模型能否在工业数据集上超越经过微调的SAM？为了回答这些问题，我们提出了实例核心分割数据集（InsCore），这是一个基于公式驱动的监督学习（FDSL）的合成预训练数据集。InsCore生成反映工业数据关键特征的完全标注的实例分割图像，包括复杂的遮挡、密集的分层掩码和多样的非刚性形状，与典型的网络图像截然不同。与以前的方法不同，InsCore不需要真实图像或人工标注。在五个工业数据集上的实验表明，使用InsCore预训练的模型优于在COCO和ImageNet-21k上训练的模型，以及微调的SAM，在实例分割性能上平均提高了6.2个百分点。这一结果仅使用10000张合成图像就实现了，比SAM的SA-1B数据集中的1100万张图像少100多倍，这证明了我们方法的数据效率。这些发现将InsCore定位为适用于工业应用的实用且无需许可的视觉基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training on real-image datasets has been widely proven effective forimproving instance segmentation. However, industrial applications face two keychallenges: (1) legal and ethical restrictions, such as ImageNet's prohibitionof commercial use, and (2) limited transferability due to the domain gapbetween web images and industrial imagery. Even recent vision foundationmodels, including the segment anything model (SAM), show notable performancedegradation in industrial settings. These challenges raise critical questions:Can we build a vision foundation model for industrial applications withoutrelying on real images or manual annotations? And can such models outperformeven fine-tuned SAM on industrial datasets? To address these questions, wepropose the Instance Core Segmentation Dataset (InsCore), a syntheticpre-training dataset based on formula-driven supervised learning (FDSL).InsCore generates fully annotated instance segmentation images that reflect keycharacteristics of industrial data, including complex occlusions, densehierarchical masks, and diverse non-rigid shapes, distinct from typical webimagery. Unlike previous methods, InsCore requires neither real images norhuman annotations. Experiments on five industrial datasets show that modelspre-trained with InsCore outperform those trained on COCO and ImageNet-21k, aswell as fine-tuned SAM, achieving an average improvement of 6.2 points ininstance segmentation performance. This result is achieved using only 100ksynthetic images, more than 100 times fewer than the 11 million images in SAM'sSA-1B dataset, demonstrating the data efficiency of our approach. Thesefindings position InsCore as a practical and license-free vision foundationmodel for industrial applications.</description>
      <author>example@mail.com (Shinichi Mae, Ryousuke Yamada, Hirokatsu Kataoka)</author>
      <guid isPermaLink="false">2505.13099v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency</title>
      <link>http://arxiv.org/abs/2505.13499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文从最优控制理论的角度研究Transformer，通过连续时间公式的工具得出关于训练和架构设计的实用见解。&lt;h4&gt;背景&lt;/h4&gt;本文将最优控制理论与Transformer的培训和架构设计相结合，旨在改进现有Transformer模型的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，旨在提高Transformer模型的性能，同时提供包括泛化能力和鲁棒性在内的理论保证。&lt;h4&gt;方法&lt;/h4&gt;该框架设计为即插即用，易于与现有Transformer模型集成，只需对实现进行轻微修改。&lt;h4&gt;主要发现&lt;/h4&gt;进行了七个大型实验，涉及文本生成、情感分析、图像分类和点云分类等任务。结果表明，该框架提高了基准模型的测试性能，且参数效率更高。例如，在nanoGPT上进行字符级文本生成时，该框架将最终测试损失减少了46%，同时参数减少了42%。在GPT-2上，最终测试损失减少了5.6%，显示出对更大模型的扩展性。&lt;h4&gt;结论&lt;/h4&gt;这是首次将最优控制理论应用于Transformer的训练和架构。它为系统性和理论驱动的改进提供了新的基础，并超越了成本高昂的试错方法。&lt;h4&gt;翻译&lt;/h4&gt;We study Transformers through the perspective of optimal control theory,using tools from continuous-time formulations to derive actionable insightsinto training and architecture design. This framework improves the performanceof existing Transformer models while providing desirable theoreticalguarantees, including generalization and robustness. Our framework is designedto be plug-and-play, enabling seamless integration with established Transformermodels and requiring only slight changes to the implementation. We conductseven extensive experiments on tasks motivated by text generation, sentimentanalysis, image classification, and point cloud classification. Experimentalresults show that the framework improves the test performance of the baselines,while being more parameter-efficient. On character-level text generation withnanoGPT, our framework achieves a 46% reduction in final test loss while using42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction infinal test loss, demonstrating scalability to larger models. To the best of ourknowledge, this is the first work that applies optimal control theory to boththe training and architecture of Transformers. It offers a new foundation forsystematic, theory-driven improvements and moves beyond costly trial-and-errorapproaches.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study Transformers through the perspective of optimal control theory,using tools from continuous-time formulations to derive actionable insightsinto training and architecture design. This framework improves the performanceof existing Transformer models while providing desirable theoreticalguarantees, including generalization and robustness. Our framework is designedto be plug-and-play, enabling seamless integration with established Transformermodels and requiring only slight changes to the implementation. We conductseven extensive experiments on tasks motivated by text generation, sentimentanalysis, image classification, and point cloud classification. Experimentalresults show that the framework improves the test performance of the baselines,while being more parameter-efficient. On character-level text generation withnanoGPT, our framework achieves a 46% reduction in final test loss while using42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction infinal test loss, demonstrating scalability to larger models. To the best of ourknowledge, this is the first work that applies optimal control theory to boththe training and architecture of Transformers. It offers a new foundation forsystematic, theory-driven improvements and moves beyond costly trial-and-errorapproaches.</description>
      <author>example@mail.com (Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2505.13499v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation</title>
      <link>http://arxiv.org/abs/2505.13550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了即时信息推荐（JIR）服务的概念，并介绍了相关的研究工作，包括JIR任务的定义、评估框架的建立以及JIR-Arena数据集的构建。&lt;h4&gt;背景&lt;/h4&gt;随着智能可穿戴设备的普及和基础模型部署的优化，即时信息推荐服务成为可能。然而，目前缺乏对JIR任务的正式定义和评估框架。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文旨在提出JIR任务的数学定义和评估指标，并构建一个多模态基准数据集JIR-Arena来评估JIR系统。&lt;h4&gt;方法&lt;/h4&gt;本文首先定义了JIR任务，并建立了相应的评估指标。接着，构建了JIR-Arena数据集，该数据集包含多样化的信息请求场景。为了提高评估的客观性和普遍性，JIR-Arena采用了多轮、多实体的验证框架。此外，还实现了一个基线JIR系统，能够处理实时信息流。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，基于基础模型的JIR系统能够以合理的精度模拟用户需求，但在召回率和有效内容检索方面存在挑战。&lt;h4&gt;结论&lt;/h4&gt;为了支持未来在这一领域的研究，本文完全发布了代码和数据。&lt;h4&gt;翻译&lt;/h4&gt;摘要：即时信息推荐（JIR）是一种旨在在用户需要时提供最相关信息的服务，通过最小化用户努力来填补他们的知识差距，并提高日常生活中的决策效率和效率。基础模型在设备高效部署方面的进步和智能可穿戴设备的广泛使用使得始终在线的JIR助手成为可能。然而，还没有系统性地努力正式定义JIR任务或建立评估框架。为了弥合这一差距，我们提出了JIR任务的第一个数学定义及其相关的评估指标。此外，我们引入了JIR-Arena，这是一个具有多样化、信息请求密集型场景的多模态基准数据集，用于评估JIR系统在关键维度上的表现：i）准确推断用户信息需求，ii）及时提供相关推荐，iii）避免可能分散用户注意力的不相关内容。由于估计用户信息需求的主观性和影响可重复性的不可控系统变量，构建JIR基准数据集面临挑战。为了解决这些问题，JIR-Arena：i）结合来自多个人类和大型AI模型的输入来近似信息需求分布；ii）通过使用静态知识库快照来评估JIR质量；iii）采用多轮、多实体的验证框架来提高客观性和普遍性。此外，我们还实现了一个基线JIR系统，能够处理与用户输入一致的实时信息流。我们对这个基线系统在JIR-Arena上的评估表明，尽管基于基础模型的JIR系统能以合理的精度模拟用户需求，但在召回率和有效内容检索方面面临挑战。为了支持这个新领域未来的研究，我们完全发布了我们的代码和数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Just-in-time Information Recommendation (JIR) is a service designed todeliver the most relevant information precisely when users need it, ,addressing their knowledge gaps with minimal effort and boostingdecision-making and efficiency in daily life. Advances in device-efficientdeployment of foundation models and the growing use of intelligent wearabledevices have made always-on JIR assistants feasible. However, there has been nosystematic effort to formally define JIR tasks or establish evaluationframeworks. To bridge this gap, we present the first mathematical definition ofJIR tasks and associated evaluation metrics. Additionally, we introduceJIR-Arena, a multimodal benchmark dataset featuring diverse,information-request-intensive scenarios to evaluate JIR systems across criticaldimensions: i) accurately inferring user information needs, ii) deliveringtimely and relevant recommendations, and iii) avoiding irrelevant content thatmay distract users.  Developing a JIR benchmark dataset poses challenges due to subjectivity inestimating user information needs and uncontrollable system variables affectingreproducibility. To address these, JIR-Arena: i) combines input from multiplehumans and large AI models to approximate information need distributions; ii)assesses JIR quality through information retrieval outcomes using staticknowledge base snapshots; and iii) employs a multi-turn, multi-entityvalidation framework to improve objectivity and generality. Furthermore, weimplement a baseline JIR system capable of processing real-time informationstreams aligned with user inputs. Our evaluation of this baseline system onJIR-Arena indicates that while foundation model-based JIR systems simulate userneeds with reasonable precision, they face challenges in recall and effectivecontent retrieval. To support future research in this new area, we fullyrelease our code and data.</description>
      <author>example@mail.com (Ke Yang, Kevin Ros, Shankar Kumar Senthil Kumar, ChengXiang Zhai)</author>
      <guid isPermaLink="false">2505.13550v1</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
      <link>http://arxiv.org/abs/2505.12638v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChromFound是一个针对scATAC-seq的专用基础模型，它通过混合架构和基因组感知分词有效地捕捉了整个基因组的长上下文和调控信号，提高了scATAC-seq数据的处理能力。&lt;h4&gt;背景&lt;/h4&gt;scATAC-seq技术为研究调控机制提供了新的视角，但目前还没有适用于scATAC-seq的基础模型能够支持零样本的高质量细胞识别和全面的多元组学分析。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于scATAC-seq的基础模型，以实现零样本的高质量细胞识别和多元组学分析。&lt;h4&gt;方法&lt;/h4&gt;ChromFound采用混合架构和基因组感知分词，并在1.97百万个来自30个组织和6种疾病条件的细胞上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;ChromFound在6个不同的任务中表现出广泛的应用性，包括生成通用细胞表示、细胞类型注释和跨组学预测，并且表现出鲁棒的零样本性能。&lt;h4&gt;结论&lt;/h4&gt;ChromFound提供了一个新的框架，可以揭示现有计算方法未检测到的增强子-基因联系，有助于理解非编码基因组中的疾病风险变异。&lt;h4&gt;翻译&lt;/h4&gt;The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) provides a novel perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of single-cell Assay for Transposase-Accessible Chromatin usingsequencing (scATAC-seq) offers an innovative perspective for decipheringregulatory mechanisms by assembling a vast repository of single-cell chromatinaccessibility data. While foundation models have achieved significant successin single-cell transcriptomics, there is currently no foundation model forscATAC-seq that supports zero-shot high-quality cell identification andcomprehensive multi-omics analysis simultaneously. Key challenges lie in thehigh dimensionality and sparsity of scATAC-seq data, as well as the lack of astandardized schema for representing open chromatin regions (OCRs). Here, wepresent ChromFound, a foundation model tailored for scATAC-seq. ChromFoundutilizes a hybrid architecture and genome-aware tokenization to effectivelycapture genome-wide long contexts and regulatory signals from dynamic chromatinlandscapes. Pretrained on 1.97 million cells from 30 tissues and 6 diseaseconditions, ChromFound demonstrates broad applicability across 6 diverse tasks.Notably, it achieves robust zero-shot performance in generating universal cellrepresentations and exhibits excellent transferability in cell type annotationand cross-omics prediction. By uncovering enhancer-gene links undetected byexisting computational methods, ChromFound offers a promising framework forunderstanding disease risk variants in the noncoding genome.</description>
      <author>example@mail.com (Yifeng Jiao, Yuchen Liu, Yu Zhang, Xin Guo, Yushuai Wu, Chen Jiang, Jiyang Li, Hongwei Zhang, Limei Han, Xin Gao, Yuan Qi, Yuan Cheng)</author>
      <guid isPermaLink="false">2505.12638v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space</title>
      <link>http://arxiv.org/abs/2503.11094v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Open3DVQA的新基准，用于全面评估当前最先进的（SOTA）基础模型在开放3D空间中的空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;空间推理是具身智能体的基本能力，在多模态大型语言模型（MLLMs）领域引起了广泛关注。&lt;h4&gt;目的&lt;/h4&gt;开发Open3DVQA基准，以评估SOTA MLLMs在空间推理方面的能力。&lt;h4&gt;方法&lt;/h4&gt;Open3DVQA包含9k个VQA样本，使用高效的半自动化工具在一个高保真城市模拟器中收集。评估了多个SOTA MLLMs在不同方面的空间推理能力，如相对和绝对空间关系、情境推理和以对象为中心的空间属性。&lt;h4&gt;主要发现&lt;/h4&gt;1) MLLMs在回答关于相对空间关系的问题上表现优于绝对空间关系；2) MLLMs在自我中心和以对象为中心的视角上的空间推理能力相似；3) 对大型模型进行微调可以显著提高其在不同空间推理任务上的表现。&lt;h4&gt;结论&lt;/h4&gt;开放源代码的数据收集工具和深入分析将启发对MLLM空间推理能力的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：空间推理是具身智能体的基本能力，在多模态大型语言模型（MLLMs）领域引起了广泛关注。在本文中，我们提出了一种名为Open3DVQA的新基准，用于全面评估当前最先进的（SOTA）基础模型在开放3D空间中的空间推理能力。Open3DVQA由9k个VQA样本组成，使用高效的半自动化工具在一个高保真城市模拟器中收集。我们评估了多个SOTA MLLMs在不同方面的空间推理能力，如相对和绝对空间关系、情境推理和以对象为中心的空间属性。我们的结果表明：1）MLLMs在回答关于相对空间关系的问题上表现优于绝对空间关系；2）MLLMs在自我中心和以对象为中心的视角上的空间推理能力相似；3）对大型模型进行微调可以显著提高其在不同空间推理任务上的表现。我们相信，我们的开放源代码数据收集工具和深入分析将启发对MLLM空间推理能力的进一步研究。基准可在https://github.com/WeichenZh/Open3DVQA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/weichenzh/open3dvqa&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial reasoning is a fundamental capability of embodied agents and hasgarnered widespread attention in the field of multimodal large language models(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, tocomprehensively evaluate the spatial reasoning capacities of currentstate-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consistsof 9k VQA samples, collected using an efficient semi-automated tool in ahigh-fidelity urban simulator. We evaluate several SOTA MLLMs across variousaspects of spatial reasoning, such as relative and absolute spatialrelationships, situational reasoning, and object-centric spatial attributes.Our results reveal that: 1) MLLMs perform better at answering questionsregarding relative spatial relationships than absolute spatial relationships,2) MLLMs demonstrate similar spatial reasoning abilities for both egocentricand allocentric perspectives, and 3) Fine-tuning large models significantlyimproves their performance across different spatial reasoning tasks. We believethat our open-source data collection tools and in-depth analyses will inspirefurther research on MLLM spatial reasoning capabilities. The benchmark isavailable at https://github.com/WeichenZh/Open3DVQA.</description>
      <author>example@mail.com (Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, Xiao-Ping Zhang)</author>
      <guid isPermaLink="false">2503.11094v2</guid>
      <pubDate>Wed, 21 May 2025 14:25:57 +0800</pubDate>
    </item>
    <item>
      <title>Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings</title>
      <link>http://arxiv.org/abs/2505.13087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于图对齐问题的图神经网络（GNN）的新基准测试方法。&lt;h4&gt;背景&lt;/h4&gt;图对齐问题是一种组合优化任务，通过将两个未标记的图对齐以最大化重叠边来泛化图同构。&lt;h4&gt;目的&lt;/h4&gt;将图对齐问题作为自监督学习任务，并生成图对齐数据集，以评估不同架构的性能。&lt;h4&gt;方法&lt;/h4&gt;使用合成随机图和来自多个领域的真实世界图数据集生成图对齐数据集。为给定图数据集，生成一系列难度递增的图对齐数据集。&lt;h4&gt;主要发现&lt;/h4&gt;各向异性图神经网络在性能上优于标准卷积架构。图对齐任务在无监督GNN预训练中表现出色，学习到的节点嵌入在三个分子回归任务上优于其他位置编码，并在PCQM4Mv2数据集上取得了最先进的成果，参数数量显著减少。&lt;h4&gt;结论&lt;/h4&gt;提供了开源Python包以生成图对齐数据集和基准测试新的GNN架构，支持可重复性和进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel benchmarking methodology for graph neural networks (GNNs)based on the graph alignment problem, a combinatorial optimization task thatgeneralizes graph isomorphism by aligning two unlabeled graphs to maximizeoverlapping edges. We frame this problem as a self-supervised learning task andpresent several methods to generate graph alignment datasets using syntheticrandom graphs and real-world graph datasets from multiple domains. For a givengraph dataset, we generate a family of graph alignment datasets with increasingdifficulty, allowing us to rank the performance of various architectures. Ourexperiments indicate that anisotropic graph neural networks outperform standardconvolutional architectures. To further demonstrate the utility of the graphalignment task, we show its effectiveness for unsupervised GNN pre-training,where the learned node embeddings outperform other positional encodings onthree molecular regression tasks and achieve state-of-the-art results on thePCQM4Mv2 dataset with significantly fewer parameters. To supportreproducibility and further research, we provide an open-source Python packageto generate graph alignment datasets and benchmark new GNN architectures.</description>
      <author>example@mail.com (Adrien Lagesse, Marc Lelarge)</author>
      <guid isPermaLink="false">2505.13087v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
  <item>
      <title>Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach</title>
      <link>http://arxiv.org/abs/2505.12902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）强化学习的设备间通信（D2D）功率分配方法，旨在优化延迟，同时确保用户公平性。&lt;h4&gt;背景&lt;/h4&gt;在无线通信中，追求速率最大化经常面临与用户公平性相关的大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决用户公平性问题，通过探索新的功率分配方法进行延迟优化。&lt;h4&gt;方法&lt;/h4&gt;采用集中式强化学习方法，中央控制器收集并处理状态信息，并使用近端策略优化（PPO）算法进行训练。将GNN层嵌入到PPO算法的actor和critic网络中，以更好地利用拓扑信息并增强方法的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有效减少了平均延迟，同时保证了用户公平性，优于基线方法，并显示出可扩展性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该功率分配方法在确保用户公平性的同时，有效降低了延迟，并在实际应用中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pursuit of rate maximization in wireless communication frequentlyencounters substantial challenges associated with user fairness. This paperaddresses these challenges by exploring a novel power allocation approach fordelay optimization, utilizing graph neural networks (GNNs)-based reinforcementlearning (RL) in device-to-device (D2D) communication. The proposed approachincorporates not only channel state information but also factors such as packetdelay, the number of backlogged packets, and the number of transmitted packetsinto the components of the state information. We adopt a centralized RL method,where a central controller collects and processes the state information. Thecentral controller functions as an agent trained using the proximal policyoptimization (PPO) algorithm. To better utilize topology information in thecommunication network and enhance the generalization of the proposed method, weembed GNN layers into both the actor and critic networks of the PPO algorithm.This integration allows for efficient parameter updates of GNNs and enables thestate information to be parameterized as a low-dimensional embedding, which isleveraged by the agent to optimize power allocation strategies. Simulationresults demonstrate that the proposed method effectively reduces average delaywhile ensuring user fairness, outperforms baseline methods, and exhibitsscalability and generalization capability.</description>
      <author>example@mail.com (Hao Fang, Kai Huang, Hao Ye, Chongtao Guo, Le Liang, Xiao Li, Shi Jin)</author>
      <guid isPermaLink="false">2505.12902v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cross-modal feature fusion for robust point cloud registration with ambiguous geometry</title>
      <link>http://arxiv.org/abs/2505.13088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.  19 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoFF的新颖的跨模态特征融合方法，用于点云配准，通过结合点云几何信息和RGB图像数据进行配准，以提升配准效果。&lt;h4&gt;背景&lt;/h4&gt;现有的点云配准方法往往忽略了从RGB图像中整合辐射信息的重要性，这限制了它们在仅凭几何数据不足以进行配准的区域的效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过结合点云几何信息和RGB图像数据，来提升点云配准的准确性和效果。&lt;h4&gt;方法&lt;/h4&gt;CoFF方法通过两阶段融合3D点云特征和2D图像特征来解决问题。第一阶段是跨模态特征融合模块，将图像特征分配给3D点云以增强3D点云特征；第二阶段是粗到精匹配模块，使用融合后的特征进行精确配准。&lt;h4&gt;主要发现&lt;/h4&gt;CoFF在四个常见数据集上进行了广泛的评估，包括3DMatch、3DLoMatch、IndoorLRS和ScanNet++数据集，结果显示CoFF在所有基准测试中均达到最先进的配准性能，例如在3DMatch和3DLoMatch数据集上分别实现了95.9%和81.6%的配准召回率。&lt;h4&gt;结论&lt;/h4&gt;CoFF方法有效地提升了点云配准的性能，特别是在几何信息不明确的区域，如对称相似性或平面结构区域。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云配准技术在深度学习技术的应用中取得了显著进展。然而，现有方法往往忽略了整合RGB图像的辐射信息。这种局限性降低了它们在配准点云对时的有效性，尤其是在仅几何数据不足以进行配准的区域。当有效地使用时，辐射信息可以通过提供从纯几何数据中缺失的上下文来增强配准过程。在本文中，我们提出了CoFF，一种新颖的跨模态特征融合方法，用于成对点云配准。假设点云和RGB图像之间的配准是可用的，CoFF通过两阶段融合3D点云特征和2D图像特征来明确解决仅几何信息不明确的问题，如在对称相似性或平面结构区域。它包含一个跨模态特征融合模块，将像素级的图像特征分配给3D输入点云以增强学习到的3D点云特征，并通过将图像块特征与superpoint特征相结合来提高粗匹配的质量。随后是一个粗到精匹配模块，使用融合后的特征准确建立对应关系。我们在四个常见数据集：3DMatch、3DLoMatch、IndoorLRS和最近发布的ScanNet++数据集上广泛评估了CoFF。此外，我们还对包含几何模糊案例的特定子数据集进行了评估。我们的实验结果表明，CoFF在所有基准测试中都实现了最先进的配准性能，包括在广泛使用的3DMatch和3DLoMatch数据集上分别实现了95.9%和81.6%的令人瞩目的配准召回率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration has seen significant advancements with theapplication of deep learning techniques. However, existing approaches oftenoverlook the potential of integrating radiometric information from RGB images.This limitation reduces their effectiveness in aligning point clouds pairs,especially in regions where geometric data alone is insufficient. When usedeffectively, radiometric information can enhance the registration process byproviding context that is missing from purely geometric data. In this paper, wepropose CoFF, a novel Cross-modal Feature Fusion method that utilizes bothpoint cloud geometry and RGB images for pairwise point cloud registration.Assuming that the co-registration between point clouds and RGB images isavailable, CoFF explicitly addresses the challenges where geometric informationalone is unclear, such as in regions with symmetric similarity or planarstructures, through a two-stage fusion of 3D point cloud features and 2D imagefeatures. It incorporates a cross-modal feature fusion module that assignspixel-wise image features to 3D input point clouds to enhance learned 3D pointfeatures, and integrates patch-wise image features with superpoint features toimprove the quality of coarse matching. This is followed by a coarse-to-finematching module that accurately establishes correspondences using the fusedfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. Inaddition, we assess CoFF on specific subset datasets containing geometricallyambiguous cases. Our experimental results demonstrate that CoFF achievesstate-of-the-art registration performance across all benchmarks, includingremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatchand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)</description>
      <author>example@mail.com (Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, Andreas Wieser)</author>
      <guid isPermaLink="false">2505.13088v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates</title>
      <link>http://arxiv.org/abs/2505.13316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures, accepted at ICME 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DDPM-PCC的基于Denoising Diffusion Probabilistic Model的点云压缩方法，用于低比特率压缩。&lt;h4&gt;背景&lt;/h4&gt;现有技术主要关注高保真重建，需要大量比特进行压缩。&lt;h4&gt;目的&lt;/h4&gt;针对带宽受限的应用，提出一种低比特率点云压缩方法。&lt;h4&gt;方法&lt;/h4&gt;使用PointNet编码器生成条件向量，并通过可学习的矢量量化器进行量化，以实现低比特率同时保持质量。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet和ModelNet40数据集上的实验表明，与标准方法和现有技术相比，在低比特率下实现了更好的率失真性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在低比特率压缩点云方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;Efficient compression of low-bit-rate point clouds is critical for bandwidth-constrained applications. However, existing techniques mainly focus on high-fidelity reconstruction, requiring many bits for compression. This paper proposes a 'Denoising Diffusion Probabilistic Model' (DDPM) architecture for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder produces the condition vector for the generation, which is then quantized via a learnable vector quantizer. This configuration allows to achieve a low bitrates while preserving quality. Experiments on ShapeNet and ModelNet40 show improved rate-distortion at low rates compared to standardized and state-of-the-art approaches. We publicly released the code at https://github.com/EIDOSLAB/DDPM-PCC.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/eidoslab/ddpm-pcc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient compression of low-bit-rate point clouds is critical forbandwidth-constrained applications. However, existing techniques mainly focuson high-fidelity reconstruction, requiring many bits for compression. Thispaper proposes a "Denoising Diffusion Probabilistic Model" (DDPM) architecturefor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoderproduces the condition vector for the generation, which is then quantized via alearnable vector quantizer. This configuration allows to achieve a low bitrateswhile preserving quality. Experiments on ShapeNet and ModelNet40 show improvedrate-distortion at low rates compared to standardized and state-of-the-artapproaches. We publicly released the code athttps://github.com/EIDOSLAB/DDPM-PCC.</description>
      <author>example@mail.com (Gabriele Spadaro, Alberto Presta, Jhony H. Giraldo, Marco Grangetto, Wei Hu, Giuseppe Valenzise, Attilio Fiandrotti, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2505.13316v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval</title>
      <link>http://arxiv.org/abs/2505.13306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GCRDP的新方法，用于解决Few-shot cross-modal retrieval中的问题，通过实验验证了其在四个基准数据集上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;Few-shot cross-modal retrieval关注在有限的训练样本下学习跨模态表示，以处理推理过程中的未见类别。与传统的跨模态检索任务不同，Few-shot retrieval涉及具有稀疏模态表示的数据。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法未能充分建模Few-shot cross-modal数据的复杂多峰分布，以及由此产生的潜在语义空间中的内模态偏差和外模态偏差问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为GCRDP的新方法，使用高斯混合模型（GMM）来捕捉数据的复杂多峰分布，并引入了多正样本对比学习机制进行全面的特征建模。此外，还引入了一种新的跨模态语义对齐策略，以改善跨模态表示的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;GCRDP方法在四个基准数据集上的实验中，表现优于六种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;GCRDP方法有效地解决了Few-shot cross-modal retrieval中的偏差问题，提高了检索的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Few-shot跨模态检索关注在有限的训练样本下学习跨模态表示，使模型能够在推理过程中处理未见类别。与假设训练和测试数据具有相同类别分布的传统跨模态检索任务不同，Few-shot检索涉及具有稀疏模态表示的数据。现有方法往往未能充分建模Few-shot跨模态数据的复杂多峰分布，导致潜在语义空间中存在两个主要偏差：内模态偏差，稀疏样本未能捕捉到类内多样性；外模态偏差，图像和文本分布之间的错位加剧了语义差距。这些偏差阻碍了检索的准确性。为了解决这些问题，我们提出了一种名为GCRDP的新方法，用于Few-shot跨模态检索。这种方法有效地使用高斯混合模型（GMM）捕捉数据的复杂多峰分布，并引入了多正样本对比学习机制以进行全面的特征建模。此外，我们还引入了一种新的跨模态语义对齐策略，通过约束图像和文本特征分布之间的相对距离，从而提高了跨模态表示的准确性。我们通过在四个基准数据集上的大量实验验证了我们的方法，证明了其相对于六种最先进方法的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot cross-modal retrieval focuses on learning cross-modalrepresentations with limited training samples, enabling the model to handleunseen classes during inference. Unlike traditional cross-modal retrievaltasks, which assume that both training and testing data share the same classdistribution, few-shot retrieval involves data with sparse representationsacross modalities. Existing methods often fail to adequately model themulti-peak distribution of few-shot cross-modal data, resulting in two mainbiases in the latent semantic space: intra-modal bias, where sparse samplesfail to capture intra-class diversity, and inter-modal bias, wheremisalignments between image and text distributions exacerbate the semantic gap.These biases hinder retrieval accuracy. To address these issues, we propose anovel method, GCRDP, for few-shot cross-modal retrieval. This approacheffectively captures the complex multi-peak distribution of data using aGaussian Mixture Model (GMM) and incorporates a multi-positive samplecontrastive learning mechanism for comprehensive feature modeling.Additionally, we introduce a new strategy for cross-modal semantic alignment,which constrains the relative distances between image and text featuredistributions, thereby improving the accuracy of cross-modal representations.We validate our approach through extensive experiments on four benchmarkdatasets, demonstrating superior performance over six state-of-the-art methods.</description>
      <author>example@mail.com (Chengsong Sun, Weiping Li, Xiang Li, Yuankun Liu, Lianlei Shan)</author>
      <guid isPermaLink="false">2505.13306v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning</title>
      <link>http://arxiv.org/abs/2505.12782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdaToken-3D的框架，用于优化3D场景理解中的大型多模态模型，以解决当前3D LMMs在计算效率和信息流冗余方面的问题。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型（LMMs）在3D场景理解方面表现出色，但使用数千个空间标记进行多模态推理的3D LMMs存在计算开销过大和信息流冗余的问题。&lt;h4&gt;目的&lt;/h4&gt;提出AdaToken-3D框架，旨在通过动态剪枝冗余标记，优化3D LMMs的计算效率和减少冗余信息流。&lt;h4&gt;方法&lt;/h4&gt;AdaToken-3D通过空间贡献分析动态剪枝冗余标记，并通过注意力模式挖掘量化标记级别的信息流，以自动调整不同3D LMM架构的剪枝策略。&lt;h4&gt;主要发现&lt;/h4&gt;在LLaVA-3D（一个7B参数的3D-LMM）上的实验表明，AdaToken-3D实现了21%的推理速度提升和63%的FLOPs减少，同时保持了原始任务精度。通过定量分析标记交互，发现超过60%的空间标记对最终预测的贡献很小（&lt;5%），为高效的3D多模态学习奠定了理论基础。&lt;h4&gt;结论&lt;/h4&gt;AdaToken-3D框架有效地提高了3D LMMs的效率和准确性，并通过定量分析揭示了多模态空间信息流中的冗余模式，为3D多模态学习提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;Large Multimodal Models (LMMs) have become a pivotal research focus in deep learning, demonstrating remarkable capabilities in 3D scene understanding. However, current 3D LMMs employing thousands of spatial tokens for multimodal reasoning suffer from critical inefficiencies: excessive computational overhead and redundant information flows. Unlike 2D VLMs processing single images, 3DLMMs exhibit inherent architectural redundancy due to the heterogeneous mechanisms between spatial tokens and visual tokens. To address this challenge, we propose AdaToken-3D, an adaptive spatial token optimization framework that dynamically prunes redundant tokens through spatial contribution analysis. Our method automatically tailors pruning strategies to different 3D LMM architectures by quantifying token-level information flows via attention pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM) demonstrate that AdaToken-3D achieves 21% faster inference speed and 63% FLOPs reduction while maintaining original task accuracy. Beyond efficiency gains, this work systematically investigates redundancy patterns in multimodal spatial information flows through quantitative token interaction analysis. Our findings reveal that over 60% of spatial tokens contribute minimally (&lt;5%) to the final predictions, establishing theoretical foundations for efficient 3D multimodal learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) have become a pivotal research focus in deeplearning, demonstrating remarkable capabilities in 3D scene understanding.However, current 3D LMMs employing thousands of spatial tokens for multimodalreasoning suffer from critical inefficiencies: excessive computational overheadand redundant information flows. Unlike 2D VLMs processing single images, 3DLMMs exhibit inherent architectural redundancy due to the heterogeneousmechanisms between spatial tokens and visual tokens. To address this challenge,we propose AdaToken-3D, an adaptive spatial token optimization framework thatdynamically prunes redundant tokens through spatial contribution analysis. Ourmethod automatically tailors pruning strategies to different 3D LMMarchitectures by quantifying token-level information flows via attentionpattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%FLOPs reduction while maintaining original task accuracy. Beyond efficiencygains, this work systematically investigates redundancy patterns in multimodalspatial information flows through quantitative token interaction analysis. Ourfindings reveal that over 60\% of spatial tokens contribute minimally ($&lt;$5\%)to the final predictions, establishing theoretical foundations for efficient 3Dmultimodal learning.</description>
      <author>example@mail.com (Kai Zhang, Xingyu Chen, Xiaofeng Zhang)</author>
      <guid isPermaLink="false">2505.12782v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MAGI-1: Autoregressive Video Generation at Scale</title>
      <link>http://arxiv.org/abs/2505.13211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为MAGI-1的世界模型，该模型通过自回归预测视频片段序列来生成视频，实现了时间建模和流式生成，并在图像到视频（I2V）任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前视频生成技术需要处理时间序列数据，而MAGI-1通过处理连续帧的固定长度片段来生成视频。&lt;h4&gt;目的&lt;/h4&gt;提高视频生成的质量和效率，同时支持可控制的生成和实时部署。&lt;h4&gt;方法&lt;/h4&gt;MAGI-1通过自回归预测视频片段序列，并训练以减少随时间单调增加的每块噪声，从而实现因果时间建模和流式生成。&lt;h4&gt;主要发现&lt;/h4&gt;MAGI-1在图像到视频（I2V）任务中表现出高时间一致性和可扩展性，其最大变体包含240亿个参数，支持长达400万个标记的上下文长度。&lt;h4&gt;结论&lt;/h4&gt;MAGI-1通过算法创新和专用基础设施实现了可扩展性和鲁棒性，其代码和模型可通过GitHub获取，产品可通过sand.ai访问。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为MAGI-1的世界模型，通过自回归预测一系列视频片段来生成视频，该片段定义为连续帧的固定长度段。经过训练以减少随时间单调增加的每块噪声，MAGI-1实现了因果时间建模和自然支持流式生成。它在基于文本指令的图像到视频（I2V）任务上取得了强大的性能，提供了高时间一致性和可扩展性，这些性能得益于多项算法创新和专用基础设施堆栈。MAGI-1通过块级提示实现可控生成，并通过保持恒定的峰值推理成本支持实时、内存高效的部署，无论视频长度如何。MAGI-1的最大变体包含240亿个参数，支持长达400万个标记的上下文长度，证明了我们方法的可扩展性和鲁棒性。代码和模型可在https://github.com/SandAI-org/MAGI-1和https://github.com/SandAI-org/MagiAttention获取。产品可通过https://sand.ai访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sandai-org/magiattention&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MAGI-1, a world model that generates videos by autoregressivelypredicting a sequence of video chunks, defined as fixed-length segments ofconsecutive frames. Trained to denoise per-chunk noise that increasesmonotonically over time, MAGI-1 enables causal temporal modeling and naturallysupports streaming generation. It achieves strong performance on image-to-video(I2V) tasks conditioned on text instructions, providing high temporalconsistency and scalability, which are made possible by several algorithmicinnovations and a dedicated infrastructure stack. MAGI-1 facilitatescontrollable generation via chunk-wise prompting and supports real-time,memory-efficient deployment by maintaining constant peak inference cost,regardless of video length. The largest variant of MAGI-1 comprises 24 billionparameters and supports context lengths of up to 4 million tokens,demonstrating the scalability and robustness of our approach. The code andmodels are available at https://github.com/SandAI-org/MAGI-1 andhttps://github.com/SandAI-org/MagiAttention. The product can be accessed athttps://sand.ai.</description>
      <author>example@mail.com (Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, Yuqiao Li)</author>
      <guid isPermaLink="false">2505.13211v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry</title>
      <link>http://arxiv.org/abs/2505.13210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于方言增强的多模态框架，用于分析古典诗词的情感，该框架结合了文本、音频和视觉特征，并在两个公开数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要基于文本意义分析情感，忽略了诗词中的节奏和视觉特征，以及方言中的古汉语语音特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的情感分析方法，以更全面地分析古典诗词的情感。&lt;h4&gt;方法&lt;/h4&gt;从诗词中提取句级音频特征，并纳入多种方言的音频，生成句级视觉特征，使用LLM翻译增强文本特征，并通过多模态对比表示学习融合多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在两个公开数据集上优于现有方法，准确率至少提高了2.51%，宏观F1值提高了1.63%。&lt;h4&gt;结论&lt;/h4&gt;该研究为多模态中文表示提供了新的思路和方法，并开源代码以促进该领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, a dialect-enhanced multimodal framework for sentiment analysis of classical Chinese poetry is proposed. The framework combines text, audio, and visual features, and achieves superior performance on two public datasets, outperforming existing methods by at least 2.51% in accuracy and 1.63% in macro F1. The research provides new insights and methods for multimodal Chinese representation and the code is open-sourced to facilitate research in this area.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhangdatalab/chinese_poetry_sentiment&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical Chinese poetry is a vital and enduring part of Chinese literature,conveying profound emotional resonance. Existing studies analyze sentimentbased on textual meanings, overlooking the unique rhythmic and visual featuresinherent in poetry,especially since it is often recited and accompanied byChinese paintings. In this work, we propose a dialect-enhanced multimodalframework for classical Chinese poetry sentiment analysis. We extractsentence-level audio features from the poetry and incorporate audio frommultiple dialects,which may retain regional ancient Chinese phonetic features,enriching the phonetic representation. Additionally, we generate sentence-levelvisual features, and the multimodal features are fused with textual featuresenhanced by LLM translation through multimodal contrastive representationlearning. Our framework outperforms state-of-the-art methods on two publicdatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macroF1. We open-source the code to facilitate research in this area and provideinsights for general multimodal Chinese representation.</description>
      <author>example@mail.com (Xiaocong Du, Haoyu Pei, Haipeng Zhang)</author>
      <guid isPermaLink="false">2505.13210v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Image Restoration for Video Surveillance: A Real-Time Approach</title>
      <link>http://arxiv.org/abs/2505.13130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文旨在解决计算机视觉领域中的图像质量问题，特别是对检测、分割、识别、监控和自动化解决方案的影响。通过提出一种实时图像恢复解决方案，提高了视频监控的图像质量。&lt;h4&gt;背景&lt;/h4&gt;图像退化，如雨、雾、光照等因素，对自动化决策产生负面影响。现有的图像恢复解决方案不适用于实时处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于视频监控的实时图像恢复解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用ResNet_50进行迁移学习，开发了一种模型，用于自动识别图像中存在的退化类型，并参考必要的处理方法进行图像恢复。&lt;h4&gt;主要发现&lt;/h4&gt;该解决方案具有灵活性和可扩展性的优势。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法为视频监控提供了实时图像恢复的解决方案，有助于提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;One of the major challenges in the field of computer vision especially for detection, segmentation, recognition, monitoring, and automated solutions, is the quality of images. Image degradation, often caused by factors such as rain, fog, lighting, etc., has a negative impact on automated decision-making. Furthermore, several image restoration solutions exist, including restoration models for single degradation and restoration models for multiple degradations. However, these solutions are not suitable for real-time processing. In this study, the aim was to develop a real-time image restoration solution for video surveillance. To achieve this, using transfer learning with ResNet_50, we developed a model for automatically identifying the types of degradation present in an image to reference the necessary treatment(s) for image restoration. Our solution has the advantage of being flexible and scalable.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.63075/2jepm102&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the major challenges in the field of computer vision especially fordetection, segmentation, recognition, monitoring, and automated solutions, isthe quality of images. Image degradation, often caused by factors such as rain,fog, lighting, etc., has a negative impact on automateddecision-making.Furthermore, several image restoration solutions exist,including restoration models for single degradation and restoration models formultiple degradations. However, these solutions are not suitable for real-timeprocessing. In this study, the aim was to develop a real-time image restorationsolution for video surveillance. To achieve this, using transfer learning withResNet_50, we developed a model for automatically identifying the types ofdegradation present in an image to reference the necessary treatment(s) forimage restoration. Our solution has the advantage of being flexible andscalable.</description>
      <author>example@mail.com (Muhammad Awais Amin, Adama Ilboudo, Abdul Samad bin Shahid, Amjad Ali, Waqas Haider Khan Bangyal)</author>
      <guid isPermaLink="false">2505.13130v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents</title>
      <link>http://arxiv.org/abs/2505.13291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Open source code available at  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,  MG and MW contributed equally, and should be considered joint first authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeSeriesGym是一个用于评估人工智能代理在时间序列机器学习工程挑战上的可扩展基准框架。&lt;h4&gt;背景&lt;/h4&gt;现有的基准缺乏可扩展性，仅关注在定义良好的环境中的模型构建，并且只评估有限的研究成果（如CSV提交文件）。&lt;h4&gt;目的&lt;/h4&gt;为了使人工智能代理的基准评估更符合机器学习工程实践，该框架在两个关键维度上进行了扩展。&lt;h4&gt;方法&lt;/h4&gt;首先，TimeSeriesGym结合了来自多个领域和任务的挑战，以评估多样化的技能。其次，它实现了对多种研究成果的评估机制，包括提交文件、代码和模型，并使用精确的数值测量和基于LLM的更灵活的评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架不仅评估独立能力，还评估能力的组合，并通过工具支持大规模挑战的设计。&lt;h4&gt;结论&lt;/h4&gt;尽管最初专注于时间序列应用，但该框架可以轻松扩展到其他数据模态，从而提高代理人工智能评估的全面性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个可扩展的基准测试框架TimeSeriesGym，用于评估人工智能代理在时间序列机器学习工程挑战上的表现。现有的基准测试缺乏可扩展性，只在定义良好的环境中关注模型构建，并且只评估有限的研究成果（例如CSV提交文件）。为了使人工智能代理的基准测试更符合机器学习工程的实践，我们的框架在两个关键维度上进行了扩展。首先，认识到有效的机器学习工程需要多种多样的技能，TimeSeriesGym结合了来自多个领域和任务的挑战，以评估孤立的能力（包括数据处理、理解研究仓库和代码转换）及其组合。我们不是独立解决每个挑战，而是开发了支持设计多个挑战的工具。其次，我们通过使用精确的数值测量和更灵活的基于LLM的评估方法，实现了对多种研究成果的评估机制，包括提交文件、代码和模型。这种双管齐下的策略在客观评估与情境判断之间取得平衡。尽管我们的初始重点是时间序列应用，但我们的框架可以轻松扩展到其他数据模态，从而提高代理人工智能评估的全面性和实用性。我们开源了这个基准测试框架，以促进未来关于人工智能代理机器学习工程能力的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/moment-timeseries-foundation-model/timeseriesgym&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TimeSeriesGym, a scalable benchmarking framework for evaluatingArtificial Intelligence (AI) agents on time series machine learning engineeringchallenges. Existing benchmarks lack scalability, focus narrowly on modelbuilding in well-defined settings, and evaluate only a limited set of researchartifacts (e.g., CSV submission files). To make AI agent benchmarking morerelevant to the practice of machine learning engineering, our framework scalesalong two critical dimensions. First, recognizing that effective ML engineeringrequires a range of diverse skills, TimeSeriesGym incorporates challenges fromdiverse sources spanning multiple domains and tasks. We design challenges toevaluate both isolated capabilities (including data handling, understandingresearch repositories, and code translation) and their combinations, and ratherthan addressing each challenge independently, we develop tools that supportdesigning multiple challenges at scale. Second, we implement evaluationmechanisms for multiple research artifacts, including submission files, code,and models, using both precise numeric measures and more flexible LLM-basedevaluation approaches. This dual strategy balances objective assessment withcontextual judgment. Although our initial focus is on time series applications,our framework can be readily extended to other data modalities, broadlyenhancing the comprehensiveness and practical utility of agentic AI evaluation.We open-source our benchmarking framework to facilitate future research on theML engineering capabilities of AI agents.</description>
      <author>example@mail.com (Yifu Cai, Xinyu Li, Mononito Goswami, Michał Wiliński, Gus Welter, Artur Dubrawski)</author>
      <guid isPermaLink="false">2505.13291v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow</title>
      <link>http://arxiv.org/abs/2505.13140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为CacheFlow的新型无监督流模型，用于3D人类动作预测，显著提高了预测速度，同时保持了预测精度。&lt;h4&gt;背景&lt;/h4&gt;现有的3D人类动作预测技术需要大量的推理时间，通常超过预测的时间范围。&lt;h4&gt;目的&lt;/h4&gt;开发一种更快的密度估计方法，以满足3D人类动作预测的需求。&lt;h4&gt;方法&lt;/h4&gt;CacheFlow利用一个无条件的流模型将高斯混合模型转换为未来动作的密度。通过预计算和缓存流模型的计算结果，将历史轨迹映射到高斯混合模型的样本上，从而减少计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;CacheFlow在标准基准数据集（如Human3.6M和AMASS）上，推理过程大约需要1毫秒，比以前的VAE方法快4倍，比以前的基于扩散的方法快30倍。该方法在密度估计精度上有所提高，并且在Human3.6M数据集上的预测精度与最先进的方法相当。&lt;h4&gt;结论&lt;/h4&gt;CacheFlow是一个有效的3D人类动作预测方法，具有快速、精确的特点。&lt;h4&gt;翻译&lt;/h4&gt;针对3D人类动作预测的密度估计方法需要大量推理时间的问题，我们提出了一种名为CacheFlow的新型基于流的方法。CacheFlow利用无条件的流模型将高斯混合模型转换为未来动作的密度，并且可以通过预计算和缓存流模型的计算结果来减少计算开销。在标准基准数据集上，CacheFlow的推理时间约为1毫秒，比以前的VAE方法快4倍，比以前的基于扩散的方法快30倍。该方法在密度估计精度上有所提高，在Human3.6M数据集上的预测精度与最先进的方法相当。我们的代码和模型将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many density estimation techniques for 3D human motion prediction require asignificant amount of inference time, often exceeding the duration of thepredicted time horizon. To address the need for faster density estimation for3D human motion prediction, we introduce a novel flow-based method for humanmotion prediction called CacheFlow. Unlike previous conditional generativemodels that suffer from time efficiency, CacheFlow takes advantage of anunconditional flow-based generative model that transforms a Gaussian mixtureinto the density of future motions. The results of the computation of theflow-based generative model can be precomputed and cached. Then, forconditional prediction, we seek a mapping from historical trajectories tosamples in the Gaussian mixture. This mapping can be done by a much morelightweight model, thus saving significant computation overhead compared to atypical conditional flow model. In such a two-stage fashion and by cachingresults from the slow flow model computation, we build our CacheFlow withoutloss of prediction accuracy and model expressiveness. This inference process iscompleted in approximately one millisecond, making it 4 times faster thanprevious VAE methods and 30 times faster than previous diffusion-based methodson standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, ourmethod demonstrates improved density estimation accuracy and comparableprediction accuracy to a SOTA method on Human3.6M. Our code and models will bepublicly available.</description>
      <author>example@mail.com (Takahiro Maeda, Jinkun Cao, Norimichi Ukita, Kris Kitani)</author>
      <guid isPermaLink="false">2505.13140v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AdS-GNN -- a Conformally Equivariant Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.12880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在广义共形变换下等变的神经网络，通过将数据从平坦欧几里得空间提升到反德西特（AdS）空间，利用了平坦空间共形变换与AdS空间等距变换之间的对应关系，实现了在几何深度学习文献中广泛研究的等距变换。该网络在计算机视觉和统计物理任务上表现出强大的性能，提高了泛化能力，并能够从训练网络中提取共形数据，如标度维度。&lt;h4&gt;背景&lt;/h4&gt;共形对称性，即保持角度的坐标变换，在物理学、数学、计算机视觉和（几何）机器学习等多个领域发挥着关键作用。&lt;h4&gt;目的&lt;/h4&gt;构建一个在广义共形变换下等变的神经网络。&lt;h4&gt;方法&lt;/h4&gt;将数据从平坦欧几里得空间提升到AdS空间，利用平坦空间共形变换与AdS空间等距变换之间的对应关系，采用基于正确距离的条件消息传递层，实现一个计算效率高的框架。&lt;h4&gt;主要发现&lt;/h4&gt;模型在计算机视觉和统计物理任务上表现出强大的性能，提高了泛化能力，并能够从训练网络中提取共形数据。&lt;h4&gt;结论&lt;/h4&gt;该神经网络能够有效处理共形数据，并在多个领域具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;Conformal symmetries, i.e. coordinate transformations that preserve angles, play a key role in many fields, including physics, mathematics, computer vision and (geometric) machine learning. Here we build a neural network that is equivariant under general conformal transformations. To achieve this, we lift data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to exploit a known correspondence between conformal transformations of flat space and isometric transformations on the AdS space. We then build upon the fact that such isometric transformations have been extensively studied on general geometries in the geometric deep learning literature. We employ message-passing layers conditioned on the proper distance, yielding a computationally efficient framework. We validate our model on tasks from computer vision and statistical physics, demonstrating strong performance, improved generalization capacities, and the ability to extract conformal data such as scaling dimensions from the trained network.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conformal symmetries, i.e.\ coordinate transformations that preserve angles,play a key role in many fields, including physics, mathematics, computer visionand (geometric) machine learning. Here we build a neural network that isequivariant under general conformal transformations. To achieve this, we liftdata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us toexploit a known correspondence between conformal transformations of flat spaceand isometric transformations on the AdS space. We then build upon the factthat such isometric transformations have been extensively studied on generalgeometries in the geometric deep learning literature. We employ message-passinglayers conditioned on the proper distance, yielding a computationally efficientframework. We validate our model on tasks from computer vision and statisticalphysics, demonstrating strong performance, improved generalization capacities,and the ability to extract conformal data such as scaling dimensions from thetrained network.</description>
      <author>example@mail.com (Maksim Zhdanov, Nabil Iqbal, Erik Bekkers, Patrick Forré)</author>
      <guid isPermaLink="false">2505.12880v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
      <link>http://arxiv.org/abs/2505.13227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OSWorld-G，一个包含564个细粒度标注样本的综合基准，旨在解决GUI grounding中的瓶颈问题。同时发布了Jedi数据集，包含400万个示例，并展示了基于Jedi的多尺度模型在多个基准测试中的有效性。&lt;h4&gt;背景&lt;/h4&gt;GUI grounding是将自然语言指令映射到图形用户界面特定动作的能力，目前的研究基准简化了grounding任务，未能捕捉现实世界交互的复杂性。&lt;h4&gt;目的&lt;/h4&gt;解决GUI grounding中的瓶颈问题，并提高计算机使用代理在复杂计算机任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;提出OSWorld-G基准，发布Jedi数据集，并在Jedi上训练多尺度模型。&lt;h4&gt;主要发现&lt;/h4&gt;Jedi数据集和基于Jedi的多尺度模型在多个基准测试中优于现有方法，Jedi的grounding能力直接提升了通用基础模型在复杂计算机任务上的能力。&lt;h4&gt;结论&lt;/h4&gt;通过详细的分析和验证，本文强调了专用数据在GUI grounding中的重要性，并开源了所有基准、数据、检查点和代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图形用户界面（GUI）的grounding，即将自然语言指令映射到特定动作的能力，在计算机使用代理开发中仍然是一个关键瓶颈。当前基准简化了grounding任务，将其视为简短的指称表达式，未能捕捉到需要软件常识、布局理解和精细操作能力的现实世界交互的复杂性。为了解决这些限制，我们引入了OSWorld-G，这是一个包含564个细粒度标注样本的综合基准，涵盖了包括文本匹配、元素识别、布局理解和精确操作在内的多种任务类型。此外，我们综合并发布了最大的计算机使用grounding数据集Jedi，它包含通过多角度解耦任务得到的400万个示例。我们在Jedi上训练的多尺度模型通过在ScreenSpot-v2、ScreenSpot-Pro和我们的OSWorld-G上的表现超过了现有方法，证明了其有效性。此外，我们证明了使用Jedi的改进grounding能力可以直接增强通用基础模型在复杂计算机任务上的代理能力，从OSWorld上的5%提高到27%。通过详细的消融研究，我们确定了影响grounding性能的关键因素，并验证了为不同界面元素组合专用数据可以促进对新界面的组合泛化。所有基准、数据、检查点和代码都是开源的，可在https://osworld-grounding.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) grounding, the ability to map natural languageinstructions to specific actions on graphical user interfaces, remains acritical bottleneck in computer use agent development. Current benchmarksoversimplify grounding tasks as short referring expressions, failing to capturethe complexity of real-world interactions that require software commonsense,layout understanding, and fine-grained manipulation capabilities. To addressthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising564 finely annotated samples across diverse task types including text matching,element recognition, layout understanding, and precise manipulation.Additionally, we synthesize and release the largest computer use groundingdataset Jedi, which contains 4 million examples through multi-perspectivedecoupling of tasks. Our multi-scale models trained on Jedi demonstrate itseffectiveness by outperforming existing approaches on ScreenSpot-v2,ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improvedgrounding with Jedi directly enhances agentic capabilities of generalfoundation models on complex computer tasks, improving from 5% to 27% onOSWorld. Through detailed ablation studies, we identify key factorscontributing to grounding performance and verify that combining specializeddata for different interface elements enables compositional generalization tonovel interfaces. All benchmark, data, checkpoints, and code are open-sourcedand available at https://osworld-grounding.github.io.</description>
      <author>example@mail.com (Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong)</author>
      <guid isPermaLink="false">2505.13227v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</title>
      <link>http://arxiv.org/abs/2505.11868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新框架，可以从单目视频中无监督地分析3D运动，以解决现有方法依赖密集多视图图像或详细部分级标注的局限性。&lt;h4&gt;背景&lt;/h4&gt;准确分析动态环境中的运动部分及其运动属性对于推进如具身智能等关键领域至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需标注训练数据，仅使用单目视频即可精确解析运动部分和运动属性的框架。&lt;h4&gt;方法&lt;/h4&gt;该方法首先通过深度估计、光流分析和点云配准方法构建场景几何，并大致分析运动部分及其初始运动属性；然后使用二维高斯扩散进行场景表示；最后，引入一个专门为关节对象设计的端到端动态场景优化算法，以细化初始分析结果，确保系统可以处理旋转、平移以及更复杂的运动（旋转+平移）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架可以有效地在无标注的情况下分析关节对象运动，展示了其在未来具身智能应用中的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架在无需标注数据的情况下，能够从单目视频中准确分析3D运动，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes an innovative framework that can analyze 3D motion from monocular videos in a zero-shot manner, addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations. The framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. The method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis, and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects is introduced, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. Experimental results show that the framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately analyzing the motion parts and their motion attributes in dynamicenvironments is crucial for advancing key areas such as embodied intelligence.Addressing the limitations of existing methods that rely on dense multi-viewimages or detailed part-level annotations, we propose an innovative frameworkthat can analyze 3D mobility from monocular videos in a zero-shot manner. Thisframework can precisely parse motion parts and motion attributes only using amonocular video, completely eliminating the need for annotated training data.Specifically, our method first constructs the scene geometry and roughlyanalyzes the motion parts and their initial motion attributes combining depthestimation, optical flow analysis and point cloud registration method, thenemploys 2D Gaussian splatting for scene representation. Building on this, weintroduce an end-to-end dynamic scene optimization algorithm specificallydesigned for articulated objects, refining the initial analysis results toensure the system can handle 'rotation', 'translation', and even complexmovements ('rotation+translation'), demonstrating high flexibility andversatility. To validate the robustness and wide applicability of our method,we created a comprehensive dataset comprising both simulated and real-worldscenarios. Experimental results show that our framework can effectively analyzearticulated object motions in an annotation-free manner, showcasing itssignificant potential in future embodied intelligence applications.</description>
      <author>example@mail.com (Hongyi Zhou, Xiaogang Wang, Yulan Guo, Kai Xu)</author>
      <guid isPermaLink="false">2505.11868v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps</title>
      <link>http://arxiv.org/abs/2505.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图像计算模型，用于预测人类在场景理解中的反应时间，并通过研究视觉系统的特性与任务相关视觉信息在图像中的空间分布之间的关系，探讨了视觉处理在理解难度形成中的重要性。&lt;h4&gt;背景&lt;/h4&gt;目前已有模型可以预测人类在目标搜索和视觉辨别等任务中的反应时间，但场景理解时间的图像计算预测器尚待开发。&lt;h4&gt;目的&lt;/h4&gt;利用视觉-语言模型（VLMs）和可比较的语言描述的定量指标，模型人类场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种融合了注视点视觉与VLMs的图像计算模型（F-SUM），以产生随注视点位置变化的空间解析场景理解图，并计算相应的F-SUM分数。&lt;h4&gt;主要发现&lt;/h4&gt;F-SUM分数与人类平均反应时间（N=17，相关系数r=0.47）和扫视次数（N=17，相关系数r=0.51）相关；也与人类描述准确度（N=16，相关系数r=-0.56）相关；且这些相关性超过了基于语言熵的杂乱、视觉复杂度和场景模糊性等标准图像指标。&lt;h4&gt;结论&lt;/h4&gt;F-SUM是一个能够预测人类场景理解反应时间的图像计算指标，证明了注视点视觉处理在形成理解难度中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although models exist that predict human response times (RTs) in tasks suchas target search and visual discrimination, the development of image-computablepredictors for scene understanding time remains an open challenge. Recentadvances in vision-language models (VLMs), which can generate scenedescriptions for arbitrary images, combined with the availability ofquantitative metrics for comparing linguistic descriptions, offer a newopportunity to model human scene understanding. We hypothesize that the primarybottleneck in human scene understanding and the driving source of variabilityin response times across scenes is the interaction between the foveated natureof the human visual system and the spatial distribution of task-relevant visualinformation within an image. Based on this assumption, we propose a novelimage-computable model that integrates foveated vision with VLMs to produce aspatially resolved map of scene understanding as a function of fixationlocation (Foveated Scene Understanding Map, or F-SUM), along with an aggregateF-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) andnumber of saccades (r=0.51) required to comprehend a scene (across 277 scenes).The F-SUM score also correlates with average (N=16) human description accuracy(r=-0.56) in time-limited presentations. These correlations significantlyexceed those of standard image-based metrics such as clutter, visualcomplexity, and scene ambiguity based on language entropy. Together, our workintroduces a new image-computable metric for predicting human response times inscene understanding and demonstrates the importance of foveated visualprocessing in shaping comprehension difficulty.</description>
      <author>example@mail.com (Ziqi Wen, Jonathan Skaza, Shravan Murlidaran, William Y. Wang, Miguel P. Eckstein)</author>
      <guid isPermaLink="false">2505.12660v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Representation of perceived prosodic similarity of conversational feedback</title>
      <link>http://arxiv.org/abs/2505.13268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了语音反馈（如“嗯”，“yeah”，“okay”）在口语对话中的重要性，探讨了其语音韵律相似性及其在现有语音表示中的反映。&lt;h4&gt;背景&lt;/h4&gt;语音反馈是口语对话中的重要组成部分，对于确保对话系统中的共同基础至关重要。&lt;h4&gt;目的&lt;/h4&gt;探究语音反馈的语音韵律相似性，以及现有语音表示如何反映这种相似性。&lt;h4&gt;方法&lt;/h4&gt;通过招募参与者进行三重比较任务，测量来自两个不同数据集的反馈响应的感知相似性。&lt;h4&gt;主要发现&lt;/h4&gt;频谱和自监督语音表示比提取的音高特征更好地编码韵律，尤其是在同一说话人的反馈情况下。此外，通过对比学习可以进一步压缩和调整表示以符合人类感知。&lt;h4&gt;结论&lt;/h4&gt;语音反馈的韵律相似性可以通过频谱和自监督语音表示来有效编码，且可以通过对比学习进一步优化以符合人类感知。&lt;h4&gt;翻译&lt;/h4&gt;摘要：语音反馈（例如，`mhm'，`yeah'，`okay'）是口语对话的一个重要组成部分，对于确保对话系统中的共同基础至关重要。这种反馈的确切意义是通过词汇和韵律形式传达的。在本研究中，我们调查了具有相同词汇形式的语音反馈的感知韵律相似性，以及现有语音表示在多大程度上反映了这种相似性。我们使用招募的参与者的三重比较任务来测量来自两个不同数据集的反馈响应的感知相似性。我们发现，频谱和自监督语音表示比提取的音高特征更好地编码韵律，特别是在同一说话人的反馈情况下。我们还发现，通过对比学习可以进一步压缩和调整表示以符合人类感知。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component ofspoken dialogue and is crucial to ensuring common ground in conversationalsystems. The exact meaning of such feedback is conveyed through both lexicaland prosodic form. In this work, we investigate the perceived prosodicsimilarity of vocal feedback with the same lexical form, and to what extentexisting speech representations reflect such similarities. A triadic comparisontask with recruited participants is used to measure perceived similarity offeedback responses taken from two different datasets. We find that spectral andself-supervised speech representations encode prosody better than extractedpitch features, especially in the case of feedback from the same speaker. Wealso find that it is possible to further condense and align the representationsto human perception through contrastive learning.</description>
      <author>example@mail.com (Livia Qian, Carol Figueroa, Gabriel Skantze)</author>
      <guid isPermaLink="false">2505.13268v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR</title>
      <link>http://arxiv.org/abs/2505.13079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Graph Matching Optimal Transport (GM-OT)的方法，用于将预训练语言模型（PLM）的语料知识迁移到声学特征学习，以提升端到端自动语音识别（E2E-ASR）的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管将PLM的语料知识迁移到声学特征学习对E2E-ASR性能提升有效，但由于语言和声学模态之间的固有差距，如何对齐这些模态之间的表示仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出GM-OT方法，以解决语言和声学模态表示对齐的问题，并提高知识迁移的效率。&lt;h4&gt;方法&lt;/h4&gt;GM-OT方法将语言和声学序列建模为结构化图，节点代表特征嵌入，边则捕捉时间和顺序关系。该方法同时最小化节点间的Wasserstein距离（WD）和边间的Gromov-Wasserstein距离（GWD），从而得到融合的Gromov-Wasserstein距离（FGWD）公式。&lt;h4&gt;主要发现&lt;/h4&gt;GM-OT方法实现了结构化的对齐，比现有的基于OT的方法更有效地进行知识迁移。理论分析表明，现有的基于OT的语言知识迁移方法可以看作是GM-OT框架的一个特例。&lt;h4&gt;结论&lt;/h4&gt;在基于CTC的E2E-ASR系统上，使用PLM进行知识迁移的实验结果表明，GM-OT方法在普通话ASR任务上取得了显著的性能提升，验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：将预训练语言模型（PLM）的语料知识迁移到声学特征学习已被证明对提升端到端自动语音识别（E2E-ASR）性能有效。然而，由于语言和声学模态之间的固有差距，对齐这些模态之间的表示仍然是一个挑战。最优传输（OT）通过最小化语言和声学特征分布之间的Wasserstein距离（WD）显示出缓解这些差距的潜力。然而，之前的基于OT的方法忽视了结构关系，将特征向量视为无序集。为了解决这个问题，我们提出了图匹配最优传输（GM-OT），该方法将语言和声学序列建模为结构化图。节点代表特征嵌入，而边则捕捉时间和顺序关系。GM-OT同时最小化节点间的WD和边间的Gromov-Wasserstein距离（GWD），从而得到融合的Gromov-Wasserstein距离（FGWD）公式。这实现了结构化的对齐，比现有的基于OT的方法更有效地进行知识迁移。理论分析进一步表明，现有的基于OT的语言知识迁移方法可以看作是我们GM-OT框架的一个特例。我们在基于CTC的E2E-ASR系统上，使用PLM进行知识迁移，对GM-OT进行了评估。实验结果表明，与最先进的模型相比，GM-OT在普通话ASR任务上取得了显著的性能提升，验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring linguistic knowledge from a pretrained language model (PLM) toacoustic feature learning has proven effective in enhancing end-to-endautomatic speech recognition (E2E-ASR). However, aligning representationsbetween linguistic and acoustic modalities remains a challenge due to inherentmodality gaps. Optimal transport (OT) has shown promise in mitigating thesegaps by minimizing the Wasserstein distance (WD) between linguistic andacoustic feature distributions. However, previous OT-based methods overlookstructural relationships, treating feature vectors as unordered sets. Toaddress this, we propose Graph Matching Optimal Transport (GM-OT), which modelslinguistic and acoustic sequences as structured graphs. Nodes represent featureembeddings, while edges capture temporal and sequential relationships. GM-OTminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)formulation. This enables structured alignment and more efficient knowledgetransfer compared to existing OT-based approaches. Theoretical analysis furthershows that prior OT-based methods in linguistic knowledge transfer can beviewed as a special case within our GM-OT framework. We evaluate GM-OT onMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledgetransfer. Experimental results demonstrate significant performance gains overstate-of-the-art models, validating the effectiveness of our approach.</description>
      <author>example@mail.com (Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai)</author>
      <guid isPermaLink="false">2505.13079v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2505.13115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为TREA的新数据集，用于评估大型音频语言模型（LALM）在推理相关任务上的能力，并通过分析发现这些模型在TREA数据集上的表现低于人类，同时提出不确定性指标，强调了对LALM进行全面评估的必要性。&lt;h4&gt;背景&lt;/h4&gt;文本大型语言模型（LLM）的成功引起了多模态社区的关注，他们希望将文本与其他模态如视觉和音频结合以实现类似的多模态能力。&lt;h4&gt;目的&lt;/h4&gt;评估大型音频语言模型（LALM）在推理相关任务上的表现，并研究它们相对于人类的能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的数据集TREA，用于评估LALM，并提出了一个不确定性指标来衡量模型对输入语义相同扰动的不变性。&lt;h4&gt;主要发现&lt;/h4&gt;LALM在TREA数据集上的表现持续低于人类能力，且准确性和不确定性指标之间没有必然的相关性。&lt;h4&gt;结论&lt;/h4&gt;为了高价值应用，需要全面评估LALM的准确性以及不确定性。&lt;h4&gt;翻译&lt;/h4&gt;The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA). We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The popular success of text-based large language models (LLM) has streamlinedthe attention of the multimodal community to combine other modalities likevision and audio along with text to achieve similar multimodal capabilities. Inthis quest, large audio language models (LALMs) have to be evaluated onreasoning related tasks which are different from traditional classification orgeneration tasks. Towards this goal, we propose a novel dataset called temporalreasoning evaluation of audio (TREA).  We benchmark open-source LALMs and observe that they are consistently behindhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, wealso propose an uncertainty metric, which computes the invariance of the modelto semantically identical perturbations of the input. Our analysis shows thatthe accuracy and uncertainty metrics are not necessarily correlated and thus,points to a need for wholesome evaluation of LALMs for high-stakesapplications.</description>
      <author>example@mail.com (Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy)</author>
      <guid isPermaLink="false">2505.13115v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning</title>
      <link>http://arxiv.org/abs/2505.13085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过通用语音编解码器（USC）实现说话人隐私保护的表示学习方法，以解决使用人类语音录音训练大型语言模型（LLM）带来的隐私问题。&lt;h4&gt;背景&lt;/h4&gt;使用人类语音录音训练LLM可能引起隐私问题，因为模型可能生成与训练数据中工件相似的输出。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来保护说话人隐私，同时在学习表示中保留语义内容和语音旁白信息。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通过USC进行语音解耦的方法，将语音分解为隐私保护的语义丰富表示和残差声学和说话人表示。&lt;h4&gt;主要发现&lt;/h4&gt;USC的语义表示保留了内容、韵律和情感，同时去除了可能可识别的说话人属性。USC在语音重建方面达到了最先进的水平。&lt;h4&gt;结论&lt;/h4&gt;USC在隐私保护表示学习方面有效，展示了在学习的语义表示中说话人匿名化、旁白保留和内容保护之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;This study proposes a speaker privacy-preserving representation learning method through the Universal Speech Codec (USC), an efficient encoder-decoder model that disentangles speech into: (i) privacy-preserving semantically rich representations, capturing content and speech paralinguistics, and (ii) residual acoustic and speaker representations that enable high-fidelity reconstruction. Extensive evaluations show that USC's semantic representation preserves content, prosody, and sentiment, while removing potentially identifiable speaker attributes. Combining both representations, USC achieves state-of-the-art speech reconstruction. Additionally, an evaluation methodology for measuring privacy-preserving properties is introduced, aligning with perceptual tests. USC is compared against other codecs in the literature and its effectiveness on privacy-preserving representation learning is demonstrated, illustrating the trade-offs of speaker anonymization, paralinguistics retention and content preservation in the learned semantic representations. Audio samples are shared at https://www.amazon.science/usc-samples.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of audio recordings of human speech to train LLMs poses privacyconcerns due to these models' potential to generate outputs that closelyresemble artifacts in the training data. In this study, we propose a speakerprivacy-preserving representation learning method through the Universal SpeechCodec (USC), a computationally efficient encoder-decoder model thatdisentangles speech into: $\textit{(i)}$ privacy-preserving semantically richrepresentations, capturing content and speech paralinguistics, and$\textit{(ii)}$ residual acoustic and speaker representations that enableshigh-fidelity reconstruction. Extensive evaluations presented show that USC'ssemantic representation preserves content, prosody, and sentiment, whileremoving potentially identifiable speaker attributes. Combining bothrepresentations, USC achieves state-of-the-art speech reconstruction.Additionally, we introduce an evaluation methodology for measuringprivacy-preserving properties, aligning with perceptual tests. We compare USCagainst other codecs in the literature and demonstrate its effectiveness onprivacy-preserving representation learning, illustrating the trade-offs ofspeaker anonymization, paralinguistics retention and content preservation inthe learned semantic representations. Audio samples are shared in$\href{https://www.amazon.science/usc-samples}{https://www.amazon.science/usc-samples}$.</description>
      <author>example@mail.com (Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Radel, Grant Strimmel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood)</author>
      <guid isPermaLink="false">2505.13085v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics</title>
      <link>http://arxiv.org/abs/2505.13192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DynaMix，一种用于动态系统重建（DSR）的新颖的多变量ALRNN混合专家架构，该架构经过预训练，能够实现零样本泛化到领域外的动态系统。&lt;h4&gt;背景&lt;/h4&gt;动态系统（DS）在复杂、随时间演变的领域中起重要作用。现有的DSR方法需要针对每个新观察到的系统进行专门训练，缺乏类似大型语言模型（LLMs）的零样本和上下文推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出DynaMix的目的是为了实现DSR的零样本泛化，即无需重新训练即可预测新动态系统的长期演化。&lt;h4&gt;方法&lt;/h4&gt;DynaMix是一种基于多变量自适应学习循环神经网络（ALRNN）的混合专家架构，经过预训练以实现DSR。&lt;h4&gt;主要发现&lt;/h4&gt;DynaMix能够从提供的上下文信号中准确预测新动态系统的长期演化，即使在现有时间序列（TS）基础模型如Chronos失败的领域，也能以更少的参数和更快的推理速度完成。DynaMix在长期统计上优于TS基础模型，甚至在短期预测中也表现良好，即使在现实世界的时间序列数据（如交通或天气数据）上也是如此，这些数据通常用于训练和评估TS模型，但并非DynaMix的训练语料库的一部分。&lt;h4&gt;结论&lt;/h4&gt;DynaMix展示了时间序列模型在DSR问题中的失败模式，并得出结论，基于DS原理构建的模型在推进时间序列预测领域也具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：复杂、随时间演变的系统，从气候到大脑活动，都受动态系统（DS）的调控。动态系统重建（DSR）旨在从观察数据中推断生成代用模型，以再现其长期行为。现有的DSR方法需要对每个新观察到的系统进行专门训练，缺乏类似于大型语言模型（LLMs）所知的零样本和上下文推理能力。在此，我们引入了DynaMix，这是一种新颖的多变量ALRNN混合专家架构，专门为DSR预训练，是第一个能够将零样本泛化到领域外动态系统的DSR模型。仅从提供的上下文信号中，无需任何重新训练，DynaMix能够准确预测新动态系统的长期演化，即使现有时间序列（TS）基础模型（如Chronos）在这些领域失败——在参数数量和推理速度上仅占其一小部分。在长期统计上，DynaMix优于TS基础模型，在短期预测中也常常表现良好，即使在现实世界的时间序列数据（如交通或天气数据）上也是如此，这些数据通常用于训练和评估TS模型，但并不属于DynaMix的训练语料库。我们展示了时间序列模型在DSR问题中的失败模式，并得出结论，基于DS原理构建的模型在推进时间序列预测领域也具有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex, temporally evolving phenomena, from climate to brain activity, aregoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infergenerative surrogate models of these from observed data, reproducing theirlong-term behavior. Existing DSR approaches require purpose-training for anynew system observed, lacking the zero-shot and in-context inferencecapabilities known from LLMs. Here we introduce DynaMix, a novel multivariateALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSRmodel able to generalize zero-shot to out-of-domain DS. Just from a providedcontext signal, without any re-training, DynaMix faithfully forecasts thelong-term evolution of novel DS where existing time series (TS) foundationmodels, like Chronos, fail -- at a fraction of the number of parameters andorders of magnitude faster inference times. DynaMix outperforms TS foundationmodels in terms of long-term statistics, and often also short-term forecasts,even on real-world time series, like traffic or weather data, typically usedfor training and evaluating TS models, but not at all part of DynaMix' trainingcorpus. We illustrate some of the failure modes of TS models for DSR problems,and conclude that models built on DS principles may bear a huge potential alsofor advancing the TS prediction field.</description>
      <author>example@mail.com (Christoph Jürgen Hemmer, Daniel Durstewitz)</author>
      <guid isPermaLink="false">2505.13192v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform</title>
      <link>http://arxiv.org/abs/2505.12631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HaarMoDic的网络，用于预测3D人体姿态，通过使用2D Haar变换将关节投影到更高分辨率的坐标，以便网络同时获取空间和时间信息。&lt;h4&gt;背景&lt;/h4&gt;3D人体姿态预测在计算机视觉和计算机图形学中至关重要，近年来引起了广泛关注。然而，现有方法由于忽略了人类运动序列在时间和空间轴上的任意性，导致在复杂情况下的预测精度受限。&lt;h4&gt;目的&lt;/h4&gt;提出HaarMoDic网络，以改善3D人体姿态预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;HaarMoDic网络利用2D Haar变换将关节投影到更高分辨率的坐标，同时获取空间和时间信息。网络中的关键模块是Multi-Resolution Haar (MR-Haar)块，它将整个运动序列投影到一个混合坐标，以便在更高分辨率的不同分辨率下同时利用两个轴的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HaarMoDic网络在Human3.6M数据集上的平均每关节位置误差（MPJPE）指标上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;HaarMoDic网络通过引入MR-Haar块，提高了3D人体姿态预测的准确性，为复杂情况下的精确预测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The 3D human pose is vital for modern computer vision and computer graphics, and its prediction has drawn attention in recent years. 3D human pose prediction aims at forecasting a human's future motion from the previous sequence. Ignoring that the arbitrariness of human motion sequences has a firm origin in transition in both temporal and spatial axes limits the performance of state-of-the-art methods, leading them to struggle with making precise predictions on complex cases, e.g., arbitrarily posing or greeting. To alleviate this problem, a network called HaarMoDic is proposed in this paper, which utilizes the 2D Haar transform to project joints to higher resolution coordinates where the network can access spatial and temporal information simultaneously. An ablation study proves that the significant contributing module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar) block. Instead of mining in one of two axes or extracting separately, the MR-Haar block projects whole motion sequences to a mixed-up coordinate in higher resolution with 2D Haar Transform, allowing the network to give scope to information from both axes in different resolutions. With the MR-Haar block, the HaarMoDic network can make predictions referring to a broader range of information. Experimental results demonstrate that HaarMoDic surpasses state-of-the-art methods in every testing interval on the Human3.6M dataset in the Mean Per Joint Position Error (MPJPE) metric.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/xhaughearl/haarmodic&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D human pose is vital for modern computer vision and computer graphics,and its prediction has drawn attention in recent years. 3D human poseprediction aims at forecasting a human's future motion from the previoussequence. Ignoring that the arbitrariness of human motion sequences has a firmorigin in transition in both temporal and spatial axes limits the performanceof state-of-the-art methods, leading them to struggle with making precisepredictions on complex cases, e.g., arbitrarily posing or greeting. Toalleviate this problem, a network called HaarMoDic is proposed in this paper,which utilizes the 2D Haar transform to project joints to higher resolutioncoordinates where the network can access spatial and temporal informationsimultaneously. An ablation study proves that the significant contributingmodule within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)block. Instead of mining in one of two axes or extracting separately, theMR-Haar block projects whole motion sequences to a mixed-up coordinate inhigher resolution with 2D Haar Transform, allowing the network to give scope toinformation from both axes in different resolutions. With the MR-Haar block,the HaarMoDic network can make predictions referring to a broader range ofinformation. Experimental results demonstrate that HaarMoDic surpassesstate-of-the-art methods in every testing interval on the Human3.6M dataset inthe Mean Per Joint Position Error (MPJPE) metric.</description>
      <author>example@mail.com (Li Lin)</author>
      <guid isPermaLink="false">2505.12631v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection</title>
      <link>http://arxiv.org/abs/2505.12966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,ICMR accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MACB-DF的音频-视觉联合学习方法，旨在解决多模态检测方法中模态学习不平衡的问题，通过对比学习实现多级和跨模态融合，以充分利用每个模态的信息。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉和深度学习的发展，深度伪造与真实媒体之间的界限变得模糊，通过音频-视觉伪造手段破坏了多媒体的可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来更好地解决模态冲突和忽视问题，提高多媒体检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种正交化的多模态Pareto模块，以保留单模态信息并解决音频-视频编码器中的梯度冲突，这些冲突是由损失函数的不同优化目标引起的。&lt;h4&gt;主要发现&lt;/h4&gt;在主流深度伪造数据集上进行的广泛实验和消融研究表明，该模型在关键评估指标上实现了持续的性能提升，多个数据集的平均准确率达到95.5%。该方法在跨数据集泛化能力方面表现出色，在DFDC数据集上训练并在DefakeAVMiT和FakeAVCeleb数据集上测试时，ACC分数相较于先前最佳方法分别提高了8.0%和7.7%。&lt;h4&gt;结论&lt;/h4&gt;MACB-DF方法在深度伪造检测方面表现出显著的效果，特别是在跨数据集泛化能力上具有优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着计算机视觉和深度学习的发展，深度伪造与真实媒体之间的界限变得模糊，通过音频-视觉伪造手段破坏了多媒体的可靠性。当前的多模态检测方法仍然受到模态学习不平衡的限制。为了解决这个问题，我们提出了一种音频-视觉联合学习方法（MACB-DF），通过利用对比学习来辅助多级和跨模态融合，从而更好地缓解模态冲突和忽视，充分利用每个模态的信息。此外，我们设计了一个正交化的多模态Pareto模块，在保留单模态信息的同时，解决了音频-视频编码器中由于损失函数的不同优化目标而引起的梯度冲突。在主流深度伪造数据集上进行的广泛实验和消融研究表明，我们的模型在关键评估指标上实现了持续的性能提升，多个数据集的平均准确率达到95.5%。值得注意的是，我们的方法在跨数据集泛化能力方面表现出色，在DFDC数据集上训练并在DefakeAVMiT和FakeAVCeleb数据集上测试时，ACC分数相较于先前最佳方法分别提高了8.0%和7.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computer vision and deep learning have blurred the line betweendeepfakes and authentic media, undermining multimedia credibility throughaudio-visual forgery. Current multimodal detection methods remain limited byunbalanced learning between modalities. To tackle this issue, we propose anAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modalityconflicts and neglect by leveraging contrastive learning to assist inmulti-level and cross-modal fusion, thereby fully balancing and exploitinginformation from each modality. Additionally, we designed anorthogonalization-multimodal pareto module that preserves unimodal informationwhile addressing gradient conflicts in audio-video encoders caused by differingoptimization targets of the loss functions. Extensive experiments and ablationstudies conducted on mainstream deepfake datasets demonstrate consistentperformance gains of our model across key evaluation metrics, achieving anaverage accuracy of 95.5% across multiple datasets. Notably, our methodexhibits superior cross-dataset generalization capabilities, with absoluteimprovements of 8.0% and 7.7% in ACC scores over the previous best-performingapproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCelebdatasets.</description>
      <author>example@mail.com (Zihan Xiong, Xiaohua Wu, Lei Chen, Fangqi Lou)</author>
      <guid isPermaLink="false">2505.12966v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</title>
      <link>http://arxiv.org/abs/2505.12911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page https://github.com/sapeirone/hiero&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HiERO的弱监督方法，用于丰富视频片段特征，通过将视频片段与其叙述描述对齐，推断出上下文、语义和时间的层次化推理。&lt;h4&gt;背景&lt;/h4&gt;人类活动复杂多变，这给深度学习模型理解它们带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;利用人类活动内在的层次化模式结构，提升对人类活动视频内容的理解。&lt;h4&gt;方法&lt;/h4&gt;HiERO通过视频片段与叙述描述的对齐，使用层次化架构进行上下文、语义和时间的推理。&lt;h4&gt;主要发现&lt;/h4&gt;HiERO在多个视频文本对齐基准测试（EgoMCQ、EgoNLQ）中证明了其丰富特征的潜力，并在零样本学习任务（EgoProceL和Ego4D Goal-Step）中取得了最先进的性能，其性能在零样本情况下比全监督方法提高了12.5%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;利用人类活动层次化结构的知识对于执行多个推理任务和以自我为中心的视觉中的推理任务具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人类活动复杂多变，这使得深度学习模型难以理解。然而，我们注意到这种可变性具有一个内在的结构，由一系列相关动作的模式组成。我们认为这种结构可以自然地从人类活动的非脚本视频中产生，并且可以利用它来更好地理解其内容。我们提出了HiERO，这是一种弱监督方法，用于丰富视频片段特征与相应的层次化活动线程。通过将视频剪辑与它们的叙述描述对齐，HiERO使用层次化架构进行上下文、语义和时间的推理。我们通过多个视频文本对齐基准（EgoMCQ、EgoNLQ）以及最小额外训练，证明了我们丰富特征的潜力，并在零样本学习任务（EgoProceL和Ego4D Goal-Step）中实现了最先进的性能。值得注意的是，HiERO在所有基准测试中都取得了最先进的性能，在零样本学习任务中，它比全监督方法有大幅度的提升（在EgoProceL上提高了12.5%的F1分数）。我们的结果表明，使用人类活动层次化结构的知识对于执行多个推理任务和以自我为中心的视觉中的推理任务具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sapeirone/hiero&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human activities are particularly complex and variable, and this makeschallenging for deep learning models to reason about them. However, we notethat such variability does have an underlying structure, composed of ahierarchy of patterns of related actions. We argue that such structure canemerge naturally from unscripted videos of human activities, and can beleveraged to better reason about their content. We present HiERO, aweakly-supervised method to enrich video segments features with thecorresponding hierarchical activity threads. By aligning video clips with theirnarrated descriptions, HiERO infers contextual, semantic and temporal reasoningwith an hierarchical architecture. We prove the potential of our enrichedfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) withminimal additional training, and in zero-shot for procedure learning tasks(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-artperformance in all the benchmarks, and for procedure learning tasks itoutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)in zero shot. Our results prove the relevance of using knowledge of thehierarchy of human activities for multiple reasoning tasks in egocentricvision.</description>
      <author>example@mail.com (Simone Alberto Peirone, Francesca Pistilli, Giuseppe Averta)</author>
      <guid isPermaLink="false">2505.12911v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions</title>
      <link>http://arxiv.org/abs/2505.12327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Conference on Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种针对自动驾驶的鲁棒规划方法，该方法结合了由扩散模型训练得到的正常和对抗性代理预测。&lt;h4&gt;背景&lt;/h4&gt;目前的自动驾驶规划方法可能过度重视对抗性行为，而忽略了低成本正常行为，或者使用硬性安全约束，这可能在所有驾驶场景中都不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种既能够抵御对抗性行为，又不过度保守的自动驾驶规划方法。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个扩散模型来学习正常代理行为的无偏分布。然后在测试时通过偏差扩散模型生成可能导致碰撞的候选计划的预测，从而得到对抗性预测的分布。使用正常和对抗性预测的混合分布来评估计划的预期成本。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在单一代理和多代理的闯红灯场景以及违反交通信号灯的场景中均显示出有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法在避免过度重视对抗性行为的同时，也考虑了正常行为的成本，提供了一种更加鲁棒的自动驾驶规划方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We describe a robust planning method for autonomous driving that mixes normaland adversarial agent predictions output by a diffusion model trained formotion prediction. We first train a diffusion model to learn an unbiaseddistribution of normal agent behaviors. We then generate a distribution ofadversarial predictions by biasing the diffusion model at test time to generatepredictions that are likely to collide with a candidate plan. We score plansusing expected cost with respect to a mixture distribution of normal andadversarial predictions, leading to a planner that is robust againstadversarial behaviors but not overly conservative when agents behave normally.Unlike current approaches, we do not use risk measures that over-weightadversarial behaviors while placing little to no weight on low-cost normalbehaviors or use hard safety constraints that may not be appropriate for alldriving scenarios. We show the effectiveness of our method on single-agent andmulti-agent jaywalking scenarios as well as a red light violation scenario.</description>
      <author>example@mail.com (Albert Zhao, Stefano Soatto)</author>
      <guid isPermaLink="false">2505.12327v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
      <link>http://arxiv.org/abs/2505.12891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First version. There are still some examples to be added into the  appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TIME的多层次基准，旨在解决大型语言模型在现实世界场景中进行时间推理的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的时间推理研究忽略了现实世界中的挑战，包括密集的时间信息、快速变化的事件动态和复杂的社会互动中的时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;通过提出TIME基准，旨在解决上述现实世界挑战，并促进时间推理在真实场景中的应用。&lt;h4&gt;方法&lt;/h4&gt;TIME基准包含38,522个问答对，涵盖3个层级和11个细粒度子任务。它包括3个子数据集：TIME-Wiki、TIME-News和TIME-Dial，分别反映不同的现实世界挑战。&lt;h4&gt;主要发现&lt;/h4&gt;进行了广泛的实验，分析了不同真实世界场景和任务中的时间推理性能，并总结了测试时间缩放对时间推理能力的影响。&lt;h4&gt;结论&lt;/h4&gt;TIME-Lite，一个人工标注的子集，被发布以促进未来研究和时间推理的标准化评估。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间推理对于大型语言模型（LLMs）理解现实世界至关重要。然而，现有工作忽略了时间推理的现实世界挑战：（1）密集的时间信息，（2）快速变化的事件动态，（3）社会互动中的复杂时间依赖关系。为了弥合这一差距，我们提出了一种名为TIME的多级基准，专为现实世界场景中的时间推理设计。TIME由38,522个问答对组成，包含3个层级和11个细粒度子任务。该基准包含3个子数据集，分别反映不同的现实世界挑战：TIME-Wiki、TIME-News和TIME-Dial。我们进行了广泛的推理模型和非推理模型的实验。我们还对跨不同真实世界场景和任务的时间推理性能进行了深入分析，并总结了测试时间缩放对时间推理能力的影响。此外，我们还发布了TIME-Lite，一个人工标注的子集，以促进未来研究和时间推理的标准化评估。代码可在https://github.com/sylvain-wei/TIME找到，数据集可在https://huggingface.co/datasets/SylvainWei/TIME找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehendthe real world. However, existing works neglect the real-world challenges fortemporal reasoning: (1) intensive temporal information, (2) fast-changing eventdynamics, and (3) complex temporal dependencies in social interactions. Tobridge this gap, we propose a multi-level benchmark TIME, designed for temporalreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3levels with 11 fine-grained sub-tasks. This benchmark encompasses 3sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,and TIME-Dial. We conduct extensive experiments on reasoning models andnon-reasoning models. And we conducted an in-depth analysis of temporalreasoning performance across diverse real-world scenarios and tasks, andsummarized the impact of test-time scaling on temporal reasoning capabilities.Additionally, we release TIME-Lite, a human-annotated subset to foster futureresearch and standardized evaluation in temporal reasoning. The code isavailable at https://github.com/sylvain-wei/TIME , and the dataset is availableat https://huggingface.co/datasets/SylvainWei/TIME .</description>
      <author>example@mail.com (Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang)</author>
      <guid isPermaLink="false">2505.12891v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data</title>
      <link>http://arxiv.org/abs/2505.12626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;scRNA-seq技术揭示了细胞异质性，细胞聚类在识别细胞类型和标记基因中发挥关键作用。本文提出了一种名为scSiameseClu的新型Siamese聚类框架，用于解释scRNA-seq数据，该框架通过三个关键步骤提高聚类性能。&lt;h4&gt;背景&lt;/h4&gt;scRNA-seq数据分析面临噪声、稀疏性和高维度的挑战，而基于图神经网络（GNN）的方法虽然提高了聚类性能，但往往存在过平滑问题，限制了其捕捉复杂生物信息的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的scSiameseClu框架，用于提高scRNA-seq数据的聚类性能，并更好地解释细胞类型和标记基因。&lt;h4&gt;方法&lt;/h4&gt;scSiameseClu框架包括三个关键步骤：(1) 双重增强模块，通过生物信息学驱动的扰动增强表示的鲁棒性；(2) Siamese融合模块，结合交叉相关优化和自适应信息融合以捕捉复杂的细胞关系，同时减轻过平滑；(3) 最优传输聚类，利用Sinkhorn距离高效地调整聚类分配与预定义比例，同时保持平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界数据集上的全面评估表明，scSiameseClu在单细胞聚类、细胞类型注释和细胞类型分类方面优于现有方法，为scRNA-seq数据解释提供了一种强大的工具。&lt;h4&gt;结论&lt;/h4&gt;scSiameseClu是一种有效的单细胞RNA测序数据分析工具，能够显著提高细胞聚类和细胞类型识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cellclustering playing a key role in identifying cell types and marker genes.Recent advances, especially graph neural networks (GNNs)-based methods, havesignificantly improved clustering performance. However, the analysis ofscRNA-seq data remains challenging due to noise, sparsity, and highdimensionality. Compounding these challenges, GNNs often suffer fromover-smoothing, limiting their ability to capture complex biologicalinformation. In response, we propose scSiameseClu, a novel Siamese Clusteringframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:(1) Dual Augmentation Module, which applies biologically informed perturbationsto the gene expression matrix and cell graph relationships to enhancerepresentation robustness; (2) Siamese Fusion Module, which combinescross-correlation refinement and adaptive information fusion to capture complexcellular relationships while mitigating over-smoothing; and (3) OptimalTransport Clustering, which utilizes Sinkhorn distance to efficiently aligncluster assignments with predefined proportions while maintaining balance.Comprehensive evaluations on seven real-world datasets demonstratethat~\methodname~outperforms state-of-the-art methods in single-cellclustering, cell type annotation, and cell type classification, providing apowerful tool for scRNA-seq data interpretation.</description>
      <author>example@mail.com (Ping Xu, Zhiyuan Ning, Pengjiang Li, Wenhao Liu, Pengyang Wang, Jiaxu Cui, Yuanchun Zhou, Pengfei Wang)</author>
      <guid isPermaLink="false">2505.12626v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models</title>
      <link>http://arxiv.org/abs/2505.12589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The dataset and code are publicly available at:  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视频监控内容理解是视觉-语言研究中的一个关键但尚未充分探索的挑战，该研究引入了SurveillanceVQA-589K，这是一个针对监控领域的最大开放性问题回答基准。&lt;h4&gt;背景&lt;/h4&gt;监控视频内容理解因其实际世界的复杂性、不规律的事件动态和安全性关键意义而成为视觉-语言研究中的一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个针对监控领域的开放性问题回答基准，以促进视频-语言理解在安全关键应用中的发展。&lt;h4&gt;方法&lt;/h4&gt;构建了包含589,380个问答对的基准数据集，包含12种认知多样的问题类型，设计了一个混合标注流程，结合了人类编写的字幕和基于提示技术的Large Vision-Language Model辅助问答生成，并提出了一种多维评估协议来评估上下文、时间和因果理解。&lt;h4&gt;主要发现&lt;/h4&gt;评估了八种Large Vision-Language Model，发现显著的性能差距，特别是在因果和异常相关任务上，这突显了当前模型在现实世界监控环境中的局限性。&lt;h4&gt;结论&lt;/h4&gt;该基准提供了一个实际且全面的资源，用于推进视频-语言理解在智能监控、事件分析和自主决策等安全关键应用中的发展。&lt;h4&gt;翻译&lt;/h4&gt;Understanding surveillance video content remains a critical yet underexplored challenge in vision-language research, particularly due to its real-world complexity, irregular event dynamics, and safety-critical implications. In this work, we introduce SurveillanceVQA-589K, the largest open-ended video question answering benchmark tailored to the surveillance domain. The dataset comprises 589,380 QA pairs spanning 12 cognitively diverse question types, including temporal reasoning, causal inference, spatial understanding, and anomaly interpretation, across both normal and abnormal video scenarios. To construct the benchmark at scale, we design a hybrid annotation pipeline that combines temporally aligned human-written captions with Large Vision-Language Model-assisted QA generation using prompt-based techniques. We also propose a multi-dimensional evaluation protocol to assess contextual, temporal, and causal comprehension. We evaluate eight LVLMs under this framework, revealing significant performance gaps, especially in causal and anomaly-related tasks, underscoring the limitations of current models in real-world surveillance contexts. Our benchmark provides a practical and comprehensive resource for advancing video-language understanding in safety-critical applications such as intelligent monitoring, incident analysis, and autonomous decision-making.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding surveillance video content remains a critical yet underexploredchallenge in vision-language research, particularly due to its real-worldcomplexity, irregular event dynamics, and safety-critical implications. In thiswork, we introduce SurveillanceVQA-589K, the largest open-ended video questionanswering benchmark tailored to the surveillance domain. The dataset comprises589,380 QA pairs spanning 12 cognitively diverse question types, includingtemporal reasoning, causal inference, spatial understanding, and anomalyinterpretation, across both normal and abnormal video scenarios. To constructthe benchmark at scale, we design a hybrid annotation pipeline that combinestemporally aligned human-written captions with Large Vision-LanguageModel-assisted QA generation using prompt-based techniques. We also propose amulti-dimensional evaluation protocol to assess contextual, temporal, andcausal comprehension. We evaluate eight LVLMs under this framework, revealingsignificant performance gaps, especially in causal and anomaly-related tasks,underscoring the limitations of current models in real-world surveillancecontexts. Our benchmark provides a practical and comprehensive resource foradvancing video-language understanding in safety-critical applications such asintelligent monitoring, incident analysis, and autonomous decision-making.</description>
      <author>example@mail.com (Bo Liu, Pengfei Qiao, Minhan Ma, Xuange Zhang, Yinan Tang, Peng Xu, Kun Liu, Tongtong Yuan)</author>
      <guid isPermaLink="false">2505.12589v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio Channels</title>
      <link>http://arxiv.org/abs/2505.13055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpaRTran是一种基于压缩感知原理的无监督表示学习方法，用于无线电信道。&lt;h4&gt;背景&lt;/h4&gt;无线电传播的物理特性是研究重点。&lt;h4&gt;目的&lt;/h4&gt;学习嵌入表示，为无线电下游任务提供优化基础。&lt;h4&gt;方法&lt;/h4&gt;SpaRTran使用稀疏门控自编码器，并学习包含原子特征的字典，以增强信号波形和时空信号模式的变化。&lt;h4&gt;主要发现&lt;/h4&gt;SpaRTran在无线电指纹识别等下游任务上，与现有方法相比，误差减少了85%。&lt;h4&gt;结论&lt;/h4&gt;SpaRTran需要更少的预训练工作量，提供更大的灵活性，并且作为基础模型，可以针对各种无线电下游任务进行微调，有效降低标注成本，同时比现有方法更具通用性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了稀疏预训练无线电变换器（SpaRTran），这是一种基于压缩感知原理的无监督表示学习方法，用于无线电信道。我们的方法学习嵌入表示，专注于无线电传播的物理特性，为基于无线电的下游任务提供优化基础。SpaRTran使用一个稀疏门控自编码器，对学习到的表示引入了简单性偏差，类似于无线电传播的稀疏性。对于信号重建，它学习一个包含原子特征的字典，这增加了信号波形和时空信号模式的变化。我们的实验表明，当在无线电指纹识别等具有挑战性的下游任务上进行微调时，SpaRTran将误差减少了高达85%，与最先进的方法相比。此外，我们的方法需要更少的预训练工作量，并提供了更大的灵活性，因为我们仅在单个无线电信号上对其进行训练。SpaRTran是一个出色的基础模型，可以针对各种无线电下游任务进行微调，有效降低标注成本。此外，它比现有方法更具通用性，并显示出优越的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the Sparse pretrained Radio Transformer (SpaRTran), anunsupervised representation learning approach based on the concept ofcompressed sensing for radio channels. Our approach learns embeddings thatfocus on the physical properties of radio propagation, to create the optimalbasis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparsegated autoencoder that induces a simplicity bias to the learnedrepresentations, resembling the sparse nature of radio propagation. For signalreconstruction, it learns a dictionary that holds atomic features, whichincreases flexibility across signal waveforms and spatiotemporal signalpatterns. Our experiments show that SpaRTran reduces errors by up to 85 %compared to state-of-the-art methods when fine-tuned on radio fingerprinting, achallenging downstream task. In addition, our method requires less pretrainingeffort and offers greater flexibility, as we train it solely on individualradio signals. SpaRTran serves as an excellent base model that can befine-tuned for various radio-based downstream tasks, effectively reducing thecost for labeling. In addition, it is significantly more versatile thanexisting methods and demonstrates superior generalization.</description>
      <author>example@mail.com (Jonathan Ott, Maximilian Stahlke, Tobias Feigl, Bjoern M. Eskofier, Christopher Mutschler)</author>
      <guid isPermaLink="false">2505.13055v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</title>
      <link>http://arxiv.org/abs/2505.12753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于LiDAR点云的多目标跟踪方法，通过改进的DETR模型和transformer架构，解决了传统跟踪系统在拥挤或快速移动场景中难以保持对象身份一致的问题。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云数据具有稀疏和不规则的特点，且需要跨帧保持时间一致性，这对多目标跟踪提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提高多目标跟踪在复杂场景中的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种两阶段的DETR inspired transformer模型，第一阶段为smoother阶段，用于优化LiDAR对象检测；第二阶段为tracker阶段，使用基于DETR的注意力机制进行跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在nuScenes和KITTI数据集上，无论是在线还是离线模式，都表现出优异的性能，在线模式在nuScenes数据集上优于基线模型和SOTA模型，离线模式提供了额外的3 pp aMOTP。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在多目标跟踪任务中表现出色，为解决复杂场景下的跟踪问题提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-object tracking from LiDAR point clouds presents unique challenges dueto the sparse and irregular nature of the data, compounded by the need fortemporal coherence across frames. Traditional tracking systems often rely onhand-crafted features and motion models, which can struggle to maintainconsistent object identities in crowded or fast-moving scenes. We present alidar-based two-staged DETR inspired transformer; a smoother and tracker. Thesmoother stage refines lidar object detections, from any off-the-shelfdetector, across a moving temporal window. The tracker stage uses a DETR-basedattention block to maintain tracks across time by associating tracked objectswith the refined detections using the point cloud as context. The model istrained on the datasets nuScenes and KITTI in both online and offline (forwardpeeking) modes demonstrating strong performance across metrics such asID-switch and multiple object tracking accuracy (MOTA). The numerical resultsindicate that the online mode outperforms the lidar-only baseline and SOTAmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,while the offline mode provides an additional 3 pp aMOTP</description>
      <author>example@mail.com (Martha Teiko Teye, Ori Maoz, Matthias Rottmann)</author>
      <guid isPermaLink="false">2505.12753v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics</title>
      <link>http://arxiv.org/abs/2505.13150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了行为基础模型（BFMs）在零样本方式下生成策略的效率，并提出了改进的FB模型以应对动态变化。&lt;h4&gt;背景&lt;/h4&gt;BFMs在零样本方式下生成策略方面取得了成功，但传统方法在动态变化时效率低下，限制了其在实际应用中的适用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的FB模型，以增强BFMs在动态变化环境下的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于transformer的信念估计器的FB模型，并通过对策略编码空间进行动态特定聚类来提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;改进的FB模型能够区分不同的动态，并通过聚类策略编码空间来提升性能，从而应对训练过程中的动态变化并泛化到未见过的动态。&lt;h4&gt;结论&lt;/h4&gt;在动态变化的场景中，该方法在离散和连续任务上相比基线实现了高达2倍的零样本回报。&lt;h4&gt;翻译&lt;/h4&gt;Behavioral Foundation Models (BFMs) have proven successful in producing policies for arbitrary tasks in a zero-shot manner, requiring no test-time training or task-specific fine-tuning. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward-Backward (FB) representation, one of the methods from the BFM family, cannot distinguish between distinct dynamics, leading to an interference among the latent directions, which parametrize different policies. To address this, we propose a FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. We also show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. These traits allow our method to respond to the dynamics observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavioral Foundation Models (BFMs) proved successful in producing policiesfor arbitrary tasks in a zero-shot manner, requiring no test-time training ortask-specific fine-tuning. Among the most promising BFMs are the ones thatestimate the successor measure learned in an unsupervised way fromtask-agnostic offline data. However, these methods fail to react to changes inthe dynamics, making them inefficient under partial observability or when thetransition function changes. This hinders the applicability of BFMs in areal-world setting, e.g., in robotics, where the dynamics can unexpectedlychange at test time. In this work, we demonstrate that Forward-Backward (FB)representation, one of the methods from the BFM family, cannot distinguishbetween distinct dynamics, leading to an interference among the latentdirections, which parametrize different policies. To address this, we propose aFB model with a transformer-based belief estimator, which greatly facilitateszero-shot adaptation. We also show that partitioning the policy encoding spaceinto dynamics-specific clusters, aligned with the context-embedding directions,yields additional gain in performance. These traits allow our method to respondto the dynamics observed during training and to generalize to unseen ones.Empirically, in the changing dynamics setting, our approach achieves up to a 2xhigher zero-shot returns compared to the baselines for both discrete andcontinuous tasks.</description>
      <author>example@mail.com (Maksim Bobrin, Ilya Zisman, Alexander Nikulin, Vladislav Kurenkov, Dmitry Dylov)</author>
      <guid isPermaLink="false">2505.13150v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.12904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过实施无监督对比学习方法，探索了在海洋环境中通过被动监听水下噪音来监测和识别声源，以减轻声污染对海洋健康威胁的可能性。&lt;h4&gt;背景&lt;/h4&gt;海洋环境中的声污染水平不断上升，对海洋健康构成威胁，因此监测水下噪音变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种无监督学习方法，以自动对水下声源进行分类，特别是在缺乏高质量标注数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种基于Conformer编码器的无监督对比学习方法，使用所谓的方差-不变-协方差正则化损失函数对低质量的未标注数据进行优化，并将结果应用于标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在识别船型和海洋哺乳动物叫声的分类任务中显示出稳健和泛化的嵌入能力。&lt;h4&gt;结论&lt;/h4&gt;这项研究表明，无监督方法在自动水下声学分析任务中具有潜力，尤其是在利用大量可用但质量较低的未标注数据时。&lt;h4&gt;翻译&lt;/h4&gt;随着海洋环境中噪音污染水平的不断上升，对海洋健康的威胁也在增加，因此监测水下噪音变得至关重要。通过被动监听这些声音，可以定位造成这种污染的源头。监测过程产生大量数据记录，记录了包括船只活动和海洋哺乳动物叫声在内的多种声源。尽管机器学习为自动声音分类提供了一种有希望的方法，但当前最先进的方法实施了监督学习，这需要大量高质量标注数据，而这些数据并未公开可用。相反，大量低质量的未标注数据是公开可用的，这为探索无监督学习技术提供了机会。本研究通过实施无监督对比学习方法来探索这种可能性。在这里，通过所谓的方差-不变-协方差正则化损失函数对这些低质量未标注数据进行优化，并实现了向标注数据的转换。通过涉及识别船型和海洋哺乳动物叫声的分类任务，我们的方法证明了产生稳健和泛化嵌入的能力。这表明无监督方法在各种自动水下声学分析任务中具有潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hildeingvildhummel/uatr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing level of sound pollution in marine environments poses anincreased threat to ocean health, making it crucial to monitor underwaternoise. By monitoring this noise, the sources responsible for this pollution canbe mapped. Monitoring is performed by passively listening to these sounds. Thisgenerates a large amount of data records, capturing a mix of sound sources suchas ship activities and marine mammal vocalizations. Although machine learningoffers a promising solution for automatic sound classification, currentstate-of-the-art methods implement supervised learning. This requires a largeamount of high-quality labeled data that is not publicly available. Incontrast, a massive amount of lower-quality unlabeled data is publiclyavailable, offering the opportunity to explore unsupervised learningtechniques. This research explores this possibility by implementing anunsupervised Contrastive Learning approach. Here, a Conformer-based encoder isoptimized by the so-called Variance-Invariance-Covariance Regularization lossfunction on these lower-quality unlabeled data and the translation to thelabeled data is made. Through classification tasks involving recognizing shiptypes and marine mammal vocalizations, our method demonstrates to producerobust and generalized embeddings. This shows to potential of unsupervisedmethods for various automatic underwater acoustic analysis tasks.</description>
      <author>example@mail.com (Hilde I. Hummel, Arwin Gansekoele, Sandjai Bhulai, Rob van der Mei)</author>
      <guid isPermaLink="false">2505.12904v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2505.12448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SSR的全新空间感知与推理方法，旨在提高视觉语言模型在多模态任务中的空间理解能力。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言模型在处理多模态任务时，由于依赖RGB输入，其空间理解能力受限。现有的空间信息整合方法要么需要专门的传感器，要么无法有效利用深度信息进行高级推理。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，将原始深度数据转换为结构化、可解释的文本推理，从而显著增强空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;SSR方法利用知识蒸馏技术将生成的推理压缩成紧凑的潜在嵌入，以便资源高效地集成到现有的视觉语言模型中，无需重新训练。同时，为了全面评估，引入了新的数据集SSR-CoT和一个综合的多任务基准SSRBench。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准上的实验表明，SSR显著提高了深度数据的利用效率，并增强了空间推理能力，推动了视觉语言模型向更类似人类的多模态理解迈进。&lt;h4&gt;结论&lt;/h4&gt;SSR方法为视觉语言模型的空间理解能力提供了新的解决方案，有助于实现更高级别的多模态理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite impressive advancements in Visual-Language Models (VLMs) formulti-modal tasks, their reliance on RGB inputs limits precise spatialunderstanding. Existing methods for integrating spatial cues, such as pointclouds or depth, either require specialized sensors or fail to effectivelyexploit depth information for higher-order reasoning. To this end, we propose anovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework thattransforms raw depth data into structured, interpretable textual rationales.These textual rationales serve as meaningful intermediate representations tosignificantly enhance spatial reasoning capabilities. Additionally, we leverageknowledge distillation to compress the generated rationales into compact latentembeddings, which facilitate resource-efficient and plug-and-play integrationinto existing VLMs without retraining. To enable comprehensive evaluation, weintroduce a new dataset named SSR-CoT, a million-scale visual-languagereasoning dataset enriched with intermediate spatial reasoning annotations, andpresent SSRBench, a comprehensive multi-task benchmark. Extensive experimentson multiple benchmarks demonstrate SSR substantially improves depth utilizationand enhances spatial reasoning, thereby advancing VLMs toward more human-likemulti-modal understanding. Our project page is athttps://yliu-cs.github.io/SSR.</description>
      <author>example@mail.com (Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2505.12448v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo</title>
      <link>http://arxiv.org/abs/2505.12714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于实例自适应的MVS模型（IA-MVS），通过缩小深度假设范围和对每个实例进行细化，提高了深度估计的精度。&lt;h4&gt;背景&lt;/h4&gt;现有的MVS模型基于渐进式深度假设缩小取得了显著进展，但尚未充分利用个体实例的深度覆盖率低于整个场景的潜力，这限制了深度估计精度的进一步提高。&lt;h4&gt;目的&lt;/h4&gt;提出IA-MVS以提高深度估计的精度，并通过改进置信度估计和鲁棒性来优化模型。&lt;h4&gt;方法&lt;/h4&gt;IA-MVS通过缩小深度假设范围和每个实例的细化来增强精度，同时引入基于实例内深度连续性的滤波机制以提升鲁棒性。此外，开发了一个基于条件概率的详细数学模型用于置信度估计。&lt;h4&gt;主要发现&lt;/h4&gt;IA-MVS在DTU基准测试中取得了最先进的性能，并且可以广泛应用于基于MVSNet的模型而不增加额外的训练负担。&lt;h4&gt;结论&lt;/h4&gt;IA-MVS通过改进深度估计精度和鲁棒性，在MVS模型中实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Multi-view stereo (MVS) models based on progressive depth hypothesis narrowing have made remarkable advancements. However, existing methods haven't fully utilized the potential that the depth coverage of individual instances is smaller than that of the entire scene, which restricts further improvements in depth estimation precision. Moreover, inevitable deviations in the initial stage accumulate as the process advances. In this paper, we propose Instance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation by narrowing the depth hypothesis range and conducting refinement on each instance. Additionally, a filtering mechanism based on intra-instance depth continuity priors is incorporated to boost robustness. Furthermore, recognizing that existing confidence estimation can degrade IA-MVS performance on point clouds. We have developed a detailed mathematical model for confidence estimation based on conditional probability. The proposed method can be widely applied in models based on MVSNet without imposing extra training burdens. Our method achieves state-of-the-art performance on the DTU benchmark. The source code is available at https://github.com/KevinWang73106/IA-MVS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view stereo (MVS) models based on progressive depth hypothesisnarrowing have made remarkable advancements. However, existing methods haven'tfully utilized the potential that the depth coverage of individual instances issmaller than that of the entire scene, which restricts further improvements indepth estimation precision. Moreover, inevitable deviations in the initialstage accumulate as the process advances. In this paper, we proposeInstance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimationby narrowing the depth hypothesis range and conducting refinement on eachinstance. Additionally, a filtering mechanism based on intra-instance depthcontinuity priors is incorporated to boost robustness. Furthermore, recognizingthat existing confidence estimation can degrade IA-MVS performance on pointclouds. We have developed a detailed mathematical model for confidenceestimation based on conditional probability. The proposed method can be widelyapplied in models based on MVSNet without imposing extra training burdens. Ourmethod achieves state-of-the-art performance on the DTU benchmark. The sourcecode is available at https://github.com/KevinWang73106/IA-MVS.</description>
      <author>example@mail.com (Yinzhe Wang, Yiwen Xiao, Hu Wang, Yiping Xu, Yan Tian)</author>
      <guid isPermaLink="false">2505.12714v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy</title>
      <link>http://arxiv.org/abs/2505.11832v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自回归过程的器官运动预测方法，用于提高放疗前的器官运动预测精度。&lt;h4&gt;背景&lt;/h4&gt;放疗过程中，由于呼吸和其他生理因素，患者可能经历器官运动。现有的预测方法主要依赖主成分分析（PCA）进行变形分析，但这种方法对配准质量依赖性高，难以捕捉运动的时间动态特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的器官运动预测方法，以更好地捕捉患者特定的运动模式，从而确保放疗的精确性。&lt;h4&gt;方法&lt;/h4&gt;通过获取每位患者的4D CT扫描，并使用自回归模型预测未来的CT相位，基于先前的相位运动模式进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在预测肺和心脏运动方面优于现有基准，证明了其在捕捉CT图像运动动态方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法有潜力提高放疗前的规划，实现更精确和自适应的放疗。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we propose an autoregressive process-based organ motion prediction method to improve the precision of organ motion prediction before radiotherapy. The existing prediction methods mainly rely on deformation analysis using principal component analysis (PCA), which is highly dependent on registration quality and struggles to capture the periodic temporal dynamics for motion modeling. The purpose of this study is to develop a new organ motion prediction method to better capture patient-specific motion patterns, thereby ensuring the precision of radiotherapy. By obtaining 4D CT scans for each patient and using an autoregressive model to predict future CT phases based on prior phase motion patterns, the method has demonstrated its effectiveness in capturing motion dynamics from CT images. These results highlight the potential of our method to improve pre-treatment planning in radiotherapy, enabling more precise and adaptive radiation delivery.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiotherapy often involves a prolonged treatment period. During this time,patients may experience organ motion due to breathing and other physiologicalfactors. Predicting and modeling this motion before treatment is crucial forensuring precise radiation delivery. However, existing pre-treatment organmotion prediction methods primarily rely on deformation analysis usingprincipal component analysis (PCA), which is highly dependent on registrationquality and struggles to capture periodic temporal dynamics for motionmodeling.In this paper, we observe that organ motion prediction closelyresembles an autoregressive process, a technique widely used in naturallanguage processing (NLP). Autoregressive models predict the next token basedon previous inputs, naturally aligning with our objective of predicting futureorgan motion phases. Building on this insight, we reformulate organ motionprediction as an autoregressive process to better capture patient-specificmotion patterns. Specifically, we acquire 4D CT scans for each patient beforetreatment, with each sequence comprising multiple 3D CT phases. These phasesare fed into the autoregressive model to predict future phases based on priorphase motion patterns. We evaluate our method on a real-world test set of 4D CTscans from 50 patients who underwent radiotherapy at our institution and apublic dataset containing 4D CT scans from 20 patients (some with multiplescans), totaling over 1,300 3D CT phases. The performance in predicting themotion of the lung and heart surpasses existing benchmarks, demonstrating itseffectiveness in capturing motion dynamics from CT images. These resultshighlight the potential of our method to improve pre-treatment planning inradiotherapy, enabling more precise and adaptive radiation delivery.</description>
      <author>example@mail.com (Yuxiang Lai, Jike Zhong, Vanessa Su, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2505.11832v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Mamba-Adaptor: State Space Model Adaptor for Visual Recognition</title>
      <link>http://arxiv.org/abs/2505.12685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mamba-Adaptor的视觉任务适配器，用于改进Mamba模型在视觉建模中的表现。&lt;h4&gt;背景&lt;/h4&gt;Mamba模型在视觉建模中表现良好，但在处理视觉任务时存在三个主要限制：无法访问全局上下文、计算当前隐藏状态时存在长期遗忘问题、空间结构建模较弱。&lt;h4&gt;目的&lt;/h4&gt;旨在解决Mamba模型在视觉任务中的性能问题。&lt;h4&gt;方法&lt;/h4&gt;提出了两个功能模块：Adaptor-T和Adaptor-S。Adaptor-T通过选择可学习的位置作为记忆增强来减轻长期遗忘问题；Adaptor-S通过多尺度扩张卷积核增强空间建模，并将图像归纳偏好引入特征输出。&lt;h4&gt;主要发现&lt;/h4&gt;Mamba-Adaptor通过扩展上下文建模和增强输出，有效解决了Mamba模型的限制。它可以在三种使用场景中提高性能：作为通用视觉骨干、作为预训练骨干的增强模块、作为高效微调模块以适应迁移学习任务。&lt;h4&gt;结论&lt;/h4&gt;Mamba-Adaptor在ImageNet和COCO基准测试中取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Recent State Space Models (SSM), especially Mamba, have demonstrated impressive performance in visual modeling and possess superior model efficiency. However, the application of Mamba to visual tasks suffers inferior performance due to three main constraints existing in the sequential model: 1) Casual computing is incapable of accessing global context; 2) Long-range forgetting when computing the current hidden states; 3) Weak spatial structural modeling due to the transformed sequential input. To address these issues, we investigate a simple yet powerful vision task Adaptor for Mamba models, which consists of two functional modules: Adaptor-T and Adaptor-S. When solving the hidden states for SSM, we apply a lightweight prediction module Adaptor-T to select a set of learnable locations as memory augmentations to ease long-range forgetting issues. Moreover, we leverage Adapator-S, composed of multi-scaledilated convolutional kernels, to enhance the spatial modeling and introducethe image inductive bias into the feature output. Both modules can enlarge the context modeling in casual computing, as the output is enhanced by the inaccessible features. We explore three usages of Mamba-Adaptor: A general visual backbone for various vision tasks; A booster module to raise the performance of pretrained backbones; A highly efficient fine-tuning module that adapts the base model for transfer learning tasks. Extensive experiments verify the effectiveness of Mamba-Adaptor in three settings. Notably, our Mamba-Adaptor achieves state-of-the-art performance on the ImageNet and COCO benchmarks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent State Space Models (SSM), especially Mamba, have demonstratedimpressive performance in visual modeling and possess superior modelefficiency. However, the application of Mamba to visual tasks suffers inferiorperformance due to three main constraints existing in the sequential model: 1)Casual computing is incapable of accessing global context; 2) Long-rangeforgetting when computing the current hidden states; 3) Weak spatial structuralmodeling due to the transformed sequential input. To address these issues, weinvestigate a simple yet powerful vision task Adaptor for Mamba models, whichconsists of two functional modules: Adaptor-T and Adaptor-S. When solving thehidden states for SSM, we apply a lightweight prediction module Adaptor-T toselect a set of learnable locations as memory augmentations to ease long-rangeforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scaledilated convolutional kernels, to enhance the spatial modeling and introducethe image inductive bias into the feature output. Both modules can enlarge thecontext modeling in casual computing, as the output is enhanced by theinaccessible features. We explore three usages of Mamba-Adaptor: A generalvisual backbone for various vision tasks; A booster module to raise theperformance of pretrained backbones; A highly efficient fine-tuning module thatadapts the base model for transfer learning tasks. Extensive experiments verifythe effectiveness of Mamba-Adaptor in three settings. Notably, ourMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCObenchmarks.</description>
      <author>example@mail.com (Fei Xie, Jiahao Nie, Yujin Tang, Wenkang Zhang, Hongshen Zhao)</author>
      <guid isPermaLink="false">2505.12685v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
      <link>http://arxiv.org/abs/2505.12363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 19 figures, 4 tables. Code, models, and dataset are  available at our project page: https://github.com/nkkbr/ViCA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViCA2是一个新型多模态大型语言模型，旨在提升空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;现有多模态大型语言模型在视觉-语言任务上表现出色，但在空间推理方面存在挑战，缺乏必要的架构组件和专门训练数据。&lt;h4&gt;目的&lt;/h4&gt;通过ViCA2增强空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;ViCA2具备双重视觉编码架构，整合SigLIP处理语义和Hiera处理空间结构，并辅以token比例控制机制以提升效率。同时，开发了一个包含超过322,000个空间基础问答对的大型数据集ViCA-322K。&lt;h4&gt;主要发现&lt;/h4&gt;ViCA2-7B模型在VSI-Bench基准测试中取得56.8的平均分，显著优于其他大型模型和领先私有模型，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;ViCA2在实现强大空间智能的同时保持模型紧凑，其代码库和数据集被发布以促进进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然多模态大型语言模型（MLLMs）在视觉-语言任务上表现出色，但在空间推理——关于空间布局、关系和动态的推理——方面仍是一个重大挑战。现有模型往往缺乏必要的架构组件和用于细粒度空间理解的专门训练数据。我们介绍了ViCA2（空间认知助手2），这是一种新型的MLLM，旨在增强空间推理。ViCA2具有双重视觉编码架构，集成了SigLIP进行语义处理和Hiera进行空间结构处理，并结合了token比例控制机制以提高效率。我们还开发了ViCA-322K，这是一个包含超过322,000个空间基础问答对的新的大型数据集，用于针对性的指令调整。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型取得了56.8的顶级平均分数，显著超过了更大的开源模型（例如，LLaVA-NeXT-Video-72B，40.9）和领先的私有模型（Gemini-1.5 Pro，45.4）。这证明了我们的方法在实现强大空间智能的同时保持模型紧凑的有效性。我们发布了ViCA2、其代码库和ViCA-322K数据集，以促进进一步的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nkkbr/vica&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Multimodal Large Language Models (MLLMs) excel at generalvision-language tasks, visuospatial cognition - reasoning about spatiallayouts, relations, and dynamics - remains a significant challenge. Existingmodels often lack the necessary architectural components and specializedtraining data for fine-grained spatial understanding. We introduce ViCA2(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatialreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIPfor semantics and Hiera for spatial structure, coupled with a token ratiocontrol mechanism for efficiency. We also developed ViCA-322K, a newlarge-scale dataset with over 322,000 spatially grounded question-answer pairsfor targeted instruction tuning. On the challenging VSI-Bench benchmark, ourViCA2-7B model achieves a state-of-the-art average score of 56.8, significantlysurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) andleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates theeffectiveness of our approach in achieving strong visuospatial intelligencewith a compact model. We release ViCA2, its codebase, and the ViCA-322K datasetto facilitate further research.</description>
      <author>example@mail.com (Qi Feng, Hidetoshi Shimodaira)</author>
      <guid isPermaLink="false">2505.12363v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>From Grunts to Grammar: Emergent Language from Cooperative Foraging</title>
      <link>http://arxiv.org/abs/2505.12872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究语言在多智能体觅食游戏中的起源，通过模拟早期人类合作的环境，发现智能体通过深度强化学习发展出具有自然语言特征的沟通协议。&lt;h4&gt;背景&lt;/h4&gt;早期人类通过手势、声音和简单信号进行协调、规划、避免捕食者和共享资源。语言如何演化、适应并成为团队合作的关键，一直是语言学和人类学研究的挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨语言在多智能体觅食游戏中的演化过程，理解语言如何从部分可观察性、时间推理和合作目标中产生。&lt;h4&gt;方法&lt;/h4&gt;设计多智能体觅食游戏环境，智能体在共享的网格世界中操作，仅对其他智能体和环境有部分了解，必须协调以完成游戏。使用端到端深度强化学习，智能体从零开始学习动作和沟通策略。&lt;h4&gt;主要发现&lt;/h4&gt;智能体发展出具有自然语言特征的沟通协议，包括任意性、可互换性、位移、文化传承和组合性。研究量化了这些特性，并分析了人口规模和时间依赖性等因素如何塑造语言的特定方面。&lt;h4&gt;结论&lt;/h4&gt;本文提出的框架为研究语言在多智能体环境中的演化提供了平台，并计划公开所有数据、代码和模型。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了语言在多智能体觅食游戏中的起源。通过模拟早期人类合作的环境，发现智能体通过深度强化学习发展出具有自然语言特征的沟通协议。早期人类依赖手势、声音和简单信号进行协调、规划、避免捕食者和共享资源。语言如何演化、适应并成为团队合作的关键，一直是语言学和人类学研究的挑战。本文旨在探讨语言在多智能体觅食游戏中的演化过程，理解语言如何从部分可观察性、时间推理和合作目标中产生。研究设计多智能体觅食游戏环境，智能体在共享的网格世界中操作，仅对其他智能体和环境有部分了解，必须协调以完成游戏。使用端到端深度强化学习，智能体从零开始学习动作和沟通策略。研究发现智能体发展出具有自然语言特征的沟通协议，包括任意性、可互换性、位移、文化传承和组合性。研究量化了这些特性，并分析了人口规模和时间依赖性等因素如何塑造语言的特定方面。本文提出的框架为研究语言在多智能体环境中的演化提供了平台，并计划公开所有数据、代码和模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early cavemen relied on gestures, vocalizations, and simple signals tocoordinate, plan, avoid predators, and share resources. Today, humanscollaborate using complex languages to achieve remarkable results. What drivesthis evolution in communication? How does language emerge, adapt, and becomevital for teamwork? Understanding the origins of language remains a challenge.A leading hypothesis in linguistics and anthropology posits that languageevolved to meet the ecological and social demands of early human cooperation.Language did not arise in isolation, but through shared survival goals.Inspired by this view, we investigate the emergence of language in multi-agentForaging Games. These environments are designed to reflect the cognitive andecological constraints believed to have influenced the evolution ofcommunication. Agents operate in a shared grid world with only partialknowledge about other agents and the environment, and must coordinate tocomplete games like picking up high-value targets or executing temporallyordered actions. Using end-to-end deep reinforcement learning, agents learnboth actions and communication strategies from scratch. We find that agentsdevelop communication protocols with hallmark features of natural language:arbitrariness, interchangeability, displacement, cultural transmission, andcompositionality. We quantify each property and analyze how different factors,such as population size and temporal dependencies, shape specific aspects ofthe emergent language. Our framework serves as a platform for studying howlanguage can evolve from partial observability, temporal reasoning, andcooperative goals in embodied multi-agent settings. We will release all data,code, and models publicly.</description>
      <author>example@mail.com (Maytus Piriyajitakonkij, Rujikorn Charakorn, Weicheng Tao, Wei Pan, Mingfei Sun, Cheston Tan, Mengmi Zhang)</author>
      <guid isPermaLink="false">2505.12872v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Unlearning</title>
      <link>http://arxiv.org/abs/2505.12614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AGU的自适应图遗忘框架，用于解决图神经网络中元素删除（如节点和边）的问题，以提高在现实应用中的有效性。&lt;h4&gt;背景&lt;/h4&gt;图遗忘对于处理可能包含过时、不准确或敏感信息的图数据至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在遗忘任务目标不一致和邻居识别不准确的问题。&lt;h4&gt;方法&lt;/h4&gt;AGU框架能够灵活适应不同的遗忘任务和图神经网络架构，确保删除元素的完全遗忘，同时保持剩余图的完整性，并准确识别受删除元素影响的邻居。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界图上的广泛实验表明，AGU在有效性、效率和遗忘能力方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;AGU框架为图神经网络中的图遗忘提供了一种有效和高效的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning, which deletes graph elements such as nodes and edges fromtrained graph neural networks (GNNs), is crucial for real-world applicationswhere graph data may contain outdated, inaccurate, or privacy-sensitiveinformation. However, existing methods often suffer from (1) incomplete or overunlearning due to neglecting the distinct objectives of different unlearningtasks, and (2) inaccurate identification of neighbors affected by deletedelements across various GNN architectures. To address these limitations, wepropose AGU, a novel Adaptive Graph Unlearning framework that flexibly adaptsto diverse unlearning tasks and GNN architectures. AGU ensures the completeforgetting of deleted elements while preserving the integrity of the remaininggraph. It also accurately identifies affected neighbors for each GNNarchitecture and prioritizes important ones to enhance unlearning performance.Extensive experiments on seven real-world graphs demonstrate that AGUoutperforms existing methods in terms of effectiveness, efficiency, andunlearning capability.</description>
      <author>example@mail.com (Pengfei Ding, Yan Wang, Guanfeng Liu, Jiajie Zhu)</author>
      <guid isPermaLink="false">2505.12614v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.12253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLaVA-4D的通用LMM框架，用于4D场景理解中的视觉表示。&lt;h4&gt;背景&lt;/h4&gt;尽管在2D图像理解方面取得了显著进展，但多模态模型（LMMs）在物理世界中由于缺乏空间表示而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时空提示，以增强LMMs对动态场景的理解。&lt;h4&gt;方法&lt;/h4&gt;LLaVA-4D通过将3D位置和1D时间编码到一个动态感知的4D坐标嵌入中，生成时空提示。此外，通过将时空组件从视觉特征中分离出来，提高了区分背景和对象的效果。&lt;h4&gt;主要发现&lt;/h4&gt;将4D时空提示嵌入到视觉特征中，LMMs能够理解物理世界中静态背景和动态对象的时空特征。&lt;h4&gt;结论&lt;/h4&gt;通过构建一个具有时空坐标注释的4D视觉-语言数据集，并通过大量实验证明了方法在4D场景理解中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管在2D图像理解方面取得了显著进展，但大型多模态模型（LMMs）由于缺乏空间表示，在物理世界中仍然面临挑战。通常，现有的3D LMMs主要通过将3D位置作为固定空间提示嵌入到视觉特征中来表示场景。然而，这些方法仅限于理解静态背景，无法捕捉动态对象的时间变化。在本文中，我们提出了一种名为LLaVA-4D的通用LMM框架，用于4D场景理解中的视觉表示。时空提示通过将3D位置和1D时间编码到一个动态感知的4D坐标嵌入中生成。此外，我们证明了从视觉特征中分离出来的空间和时空组件在区分背景和对象方面更有效。这促使我们将4D时空提示嵌入到这些特征中，以增强动态场景的表示。通过将视觉时空嵌入与语言嵌入对齐，LMMs获得了理解物理世界中静态背景和动态对象的时空特征的能力。此外，我们构建了一个具有时空坐标注释的4D视觉-语言数据集，用于指令微调LMMs。进行了大量实验，以证明我们的方法在4D场景理解的不同任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite achieving significant progress in 2D image understanding, largemultimodal models (LMMs) struggle in the physical world due to the lack ofspatial representation. Typically, existing 3D LMMs mainly embed 3D positionsas fixed spatial prompts within visual features to represent the scene.However, these methods are limited to understanding the static background andfail to capture temporally varying dynamic objects. In this paper, we proposeLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visualrepresentation in 4D scene understanding. The spatiotemporal prompt isgenerated by encoding 3D position and 1D time into a dynamic-aware 4Dcoordinate embedding. Moreover, we demonstrate that spatial and temporalcomponents disentangled from visual features are more effective indistinguishing the background from objects. This motivates embedding the 4Dspatiotemporal prompt into these features to enhance the dynamic scenerepresentation. By aligning visual spatiotemporal embeddings with languageembeddings, LMMs gain the ability to understand both spatial and temporalcharacteristics of static background and dynamic objects in the physical world.Additionally, we construct a 4D vision-language dataset with spatiotemporalcoordinate annotations for instruction fine-tuning LMMs. Extensive experimentshave been conducted to demonstrate the effectiveness of our method acrossdifferent tasks in 4D scene understanding.</description>
      <author>example@mail.com (Hanyu Zhou, Gim Hee Lee)</author>
      <guid isPermaLink="false">2505.12253v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2505.12788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于强化学习的N-tuple时序知识图谱推理方法，旨在提高推理的可解释性。&lt;h4&gt;背景&lt;/h4&gt;时序知识图谱（TKGs）使用（主体，谓词，对象，时间戳）四元组描述时序事实，而N-tuple TKGs通过使用n-tuples扩展了传统TKGs，以更细粒度地表示事实。&lt;h4&gt;目的&lt;/h4&gt;通过推理N-TKGs来预测基于历史事实的潜在未来事实，并提高推理的可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MT-Path的方法，利用时间信息遍历历史n-tuples并构建推理路径。MT-Path使用混合策略驱动的动作选择器，基于三个低级策略：谓词焦点策略、核心元素焦点策略和整个事实焦点策略。此外，它还使用一个感知辅助元素的GCN来捕捉事实之间丰富的语义依赖。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MT-Path在有效性和可解释性方面优于现有的N-TKG推理方法。&lt;h4&gt;结论&lt;/h4&gt;MT-Path是一种有效的N-TKG推理方法，能够提高推理的可解释性并更好地理解每个n-tuple。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of(subject, predicate, object, timestamp) to describe temporal facts, haveattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditionalTKGs by utilizing n-tuples to incorporate auxiliary elements alongside coreelements (i.e., subject, predicate, and object) of facts, so as to representthem in a more fine-grained manner. Reasoning over N-TKGs aims to predictpotential future facts based on historical ones. However, existing N-TKGreasoning methods often lack explainability due to their black-box nature.Therefore, we introduce a new Reinforcement Learning-based method, namedMT-Path, which leverages the temporal information to traverse historicaln-tuples and construct a temporal reasoning path. Specifically, in order tointegrate the information encapsulated within n-tuples, i.e., theentity-irrelevant information within the predicate, the information about coreelements, and the complete information about the entire n-tuples, MT-Pathutilizes a mixture policy-driven action selector, which bases on threelow-level policies, namely, the predicate-focused policy, thecore-element-focused policy and the whole-fact-focused policy. Further, MT-Pathutilizes an auxiliary element-aware GCN to capture the rich semanticdependencies among facts, thereby enabling the agent to gain a deepunderstanding of each n-tuple. Experimental results demonstrate theeffectiveness and the explainability of MT-Path.</description>
      <author>example@mail.com (Zhongni Hou, Miao Su, Xiaolong Jin, Zixuan Li, Long Bai, Jiafeng Guo, Xueqi Cheng)</author>
      <guid isPermaLink="false">2505.12788v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision</title>
      <link>http://arxiv.org/abs/2505.12526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HAL（历史平均标签）的方法，用于解决动态图中的训练效率问题，通过聚合历史节点交互生成伪标签，减少梯度方差，加速收敛。&lt;h4&gt;背景&lt;/h4&gt;TGNs（时序图网络）在动态图中由于监督信号不规律导致梯度更新稀疏，存在显著的训练效率问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来提高TGNs的训练效率，同时保持其性能。&lt;h4&gt;方法&lt;/h4&gt;通过将历史节点交互聚合为伪标签来减少梯度方差，并动态地丰富训练批次，利用历史标签分布生成伪目标。HAL通过将闲置计算转化为生产性学习步骤，确保参数的连续更新，而无需修改架构。&lt;h4&gt;主要发现&lt;/h4&gt;在TGB（时序图基准）上的实验验证了HAL的有效性，发现HAL可以将TGNv2的训练速度提高最多15倍，同时保持有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;HAL为时序图学习中的标签稀疏问题提供了一种高效、轻量级、架构无关且理论上有根据的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Temporal Graph Networks (TGNs), while being accurate, face significant training inefficiencies due to irregular supervision signals in dynamic graphs, which induce sparse gradient updates. We first theoretically establish that aggregating historical node interactions into pseudo-labels reduces gradient variance, accelerating convergence. Building on this analysis, we propose History-Averaged Labels (HAL), a method that dynamically enriches training batches with pseudo-targets derived from historical label distributions. HAL ensures continuous parameter updates without architectural modifications by converting idle computation into productive learning steps. Experiments on the Temporal Graph Benchmark (TGB) validate our findings and an assumption about slow change of user preferences: HAL accelerates TGNv2 training by up to 15x while maintaining competitive performance. Thus, this work offers an efficient, lightweight, architecture-agnostic, and theoretically motivated solution to label sparsity in temporal graph learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Graph Networks (TGNs), while being accurate, face significanttraining inefficiencies due to irregular supervision signals in dynamic graphs,which induce sparse gradient updates. We first theoretically establish thataggregating historical node interactions into pseudo-labels reduces gradientvariance, accelerating convergence. Building on this analysis, we proposeHistory-Averaged Labels (HAL), a method that dynamically enriches trainingbatches with pseudo-targets derived from historical label distributions. HALensures continuous parameter updates without architectural modifications byconverting idle computation into productive learning steps. Experiments on theTemporal Graph Benchmark (TGB) validate our findings and an assumption aboutslow change of user preferences: HAL accelerates TGNv2 training by up to 15xwhile maintaining competitive performance. Thus, this work offers an efficient,lightweight, architecture-agnostic, and theoretically motivated solution tolabel sparsity in temporal graph learning.</description>
      <author>example@mail.com (Alexander Panyshev, Dmitry Vinichenko, Oleg Travkin, Roman Alferov, Alexey Zaytsev)</author>
      <guid isPermaLink="false">2505.12526v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis</title>
      <link>http://arxiv.org/abs/2505.13033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TSPulse是一种超紧凑的时间序列预训练模型，仅含1M参数，适用于分类、异常检测、插补和检索任务，通过架构和任务层面的创新，显著提高了性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列预训练模型在时序表示学习方面取得了进展，但当前最先进模型规模较大，计算需求高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时间序列预训练模型，以提高性能并减少计算需求。&lt;h4&gt;方法&lt;/h4&gt;TSPulse采用了双重空间掩码重建，从时间和频率域学习以捕捉互补信号，并使用双重嵌入解耦生成详细和高层语义嵌入。在任务层面，它结合了TSLens组件和多头三角定位技术，以及混合掩码预训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;TSPulse在多个任务上取得了显著的性能提升，如分类基准测试提高5-16%，异常检测排行榜提高+20%，零样本插补提高+50%，时间序列检索提高+25%。这些结果是在仅1M参数的情况下实现的，使得TSPulse比现有预训练模型小10-100倍。&lt;h4&gt;结论&lt;/h4&gt;TSPulse通过其架构和任务创新，实现了高效的时序预训练模型，并有望开源。&lt;h4&gt;翻译&lt;/h4&gt;TSPulse的提出推进了时间序列预训练模型的发展，通过创新的架构和任务设计，在保证性能的同时显著降低了模型规模和计算需求，为高效的时间序列预训练模型树立了新标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of time-series pre-trained models has advanced temporalrepresentation learning, but current state-of-the-art models are oftenlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compacttime-series pre-trained models with only 1M parameters, specialized to performstrongly across classification, anomaly detection, imputation, and retrievaltasks. TSPulse introduces innovations at both the architecture and task levels.At the architecture level, it employs a dual-space masked reconstruction,learning from both time and frequency domains to capture complementary signals.This is further enhanced by a dual-embedding disentanglement, generating bothdetailed embeddings for fine-grained analysis and high-level semanticembeddings for broader task understanding. Notably, TSPulse's semanticembeddings are robust to shifts in time, magnitude, and noise, which isimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,a fine-tuning component enabling task-specific feature attention. It alsointroduces a multi-head triangulation technique that correlates deviations frommultiple prediction heads, enhancing anomaly detection by fusing complementarymodel outputs. Additionally, a hybrid mask pretraining is proposed to improveszero-shot imputation by reducing pre-training bias. These architecture and taskinnovations collectively contribute to TSPulse's significant performance gains:5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomalydetection leaderboard, +50% in zero-shot imputation, and +25% in time-seriesretrieval. Remarkably, these results are achieved with just 1M parameters,making TSPulse 10-100X smaller than existing pre-trained models. Its efficiencyenables GPU-free inference and rapid pre-training, setting a new standard forefficient time-series pre-trained models. Models will be open-sourced soon.</description>
      <author>example@mail.com (Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam)</author>
      <guid isPermaLink="false">2505.13033v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Industry-focused Synthetic Segmentation Pre-training</title>
      <link>http://arxiv.org/abs/2505.13099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InsCore的合成预训练数据集，用于工业应用的视觉基础模型，以解决工业图像数据集在预训练中的挑战。&lt;h4&gt;背景&lt;/h4&gt;在真实图像数据集上预训练对实例分割效果显著，但工业应用面临法律和道德限制以及领域差异导致的迁移性限制。&lt;h4&gt;目的&lt;/h4&gt;研究在不依赖真实图像或手动标注的情况下，能否构建用于工业应用的视觉基础模型，并探讨其性能是否能超过经过微调的SAM模型。&lt;h4&gt;方法&lt;/h4&gt;提出InsCore数据集，基于公式驱动的监督学习（FDSL）生成反映工业数据关键特征的实例分割图像，无需真实图像或人工标注。&lt;h4&gt;主要发现&lt;/h4&gt;使用InsCore预训练的模型在五个工业数据集上表现优于使用COCO、ImageNet-21k和微调SAM训练的模型，平均提升6.2个点，且仅需100k合成图像，效率远超SAM的SA-1B数据集。&lt;h4&gt;结论&lt;/h4&gt;InsCore是一个实用且无版权费的视觉基础模型，适用于工业应用。&lt;h4&gt;翻译&lt;/h4&gt;The paper proposes the Instance Core Segmentation Dataset (InsCore), a synthetic pre-training dataset for industrial vision foundation models, addressing the challenges in pre-training industrial image data sets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training on real-image datasets has been widely proven effective forimproving instance segmentation. However, industrial applications face two keychallenges: (1) legal and ethical restrictions, such as ImageNet's prohibitionof commercial use, and (2) limited transferability due to the domain gapbetween web images and industrial imagery. Even recent vision foundationmodels, including the segment anything model (SAM), show notable performancedegradation in industrial settings. These challenges raise critical questions:Can we build a vision foundation model for industrial applications withoutrelying on real images or manual annotations? And can such models outperformeven fine-tuned SAM on industrial datasets? To address these questions, wepropose the Instance Core Segmentation Dataset (InsCore), a syntheticpre-training dataset based on formula-driven supervised learning (FDSL).InsCore generates fully annotated instance segmentation images that reflect keycharacteristics of industrial data, including complex occlusions, densehierarchical masks, and diverse non-rigid shapes, distinct from typical webimagery. Unlike previous methods, InsCore requires neither real images norhuman annotations. Experiments on five industrial datasets show that modelspre-trained with InsCore outperform those trained on COCO and ImageNet-21k, aswell as fine-tuned SAM, achieving an average improvement of 6.2 points ininstance segmentation performance. This result is achieved using only 100ksynthetic images, more than 100 times fewer than the 11 million images in SAM'sSA-1B dataset, demonstrating the data efficiency of our approach. Thesefindings position InsCore as a practical and license-free vision foundationmodel for industrial applications.</description>
      <author>example@mail.com (Shinichi Mae, Ryosuke Yamada, Hirokatsu Kataoka)</author>
      <guid isPermaLink="false">2505.13099v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection</title>
      <link>http://arxiv.org/abs/2505.12507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了LM$^2$otifs，一种用于机器生成文本检测的新颖可解释框架，旨在解决现有检测方法在可解释性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;大语言模型生成自然文本的能力令人印象深刻，但也带来了作者身份验证的挑战。虽然已有多种检测方法区分机器生成文本（MGT）和人工生成文本（HGT），但这些方法的可解释性仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;提出LM$^2$otifs，旨在解决传统可解释性技术无法捕捉复杂词汇关系的问题，从而提高MGT检测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;LM$^2$otifs利用可解释图神经网络，通过三个关键阶段实现文本检测和解释：1. 将文本转换为基于词共现的图，以表示词汇依赖；2. 使用图神经网络进行预测；3. 使用后处理可解释性方法提取可解释的基序，从单个词到句子结构提供多级解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LM$^2$otifs在多个基准数据集上具有可比的性能，提取的可解释基序在区分HGT和MGT方面非常有效。定性分析揭示了MGT特有的语言指纹。&lt;h4&gt;结论&lt;/h4&gt;LM$^2$otifs是一个有效的MGT检测框架，能够提供准确的可解释性，有助于解决当前检测方法在可解释性方面的不足。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型在生成自然文本方面的惊人能力导致了作者身份验证的关键挑战。尽管已经开发了许多检测方法来区分机器生成文本（MGT）和人工生成文本（HGT），但这些方法的可解释性仍然存在显著差距。传统的可解释性技术通常无法捕捉区分HGT和MGT的复杂词汇关系。为了解决这一局限性，我们提出了LM$^2$otifs，这是一种用于MGT检测的新颖可解释框架。受概率图模型启发，我们提供了有效性的理论依据。LM$^2$otifs利用可解释图神经网络来实现准确检测和可解释性。LM$^2$otifs流程分为三个关键阶段：首先，它将文本转换为基于词共现的图，以表示词汇依赖；其次，使用图神经网络进行预测；最后，使用后处理可解释性方法提取可解释的基序，从单个词到句子结构提供多级解释。在多个基准数据集上的广泛实验表明，LM$^2$otifs具有可比的性能。提取的可解释基序的实证评估确认了它们在区分HGT和MGT方面的有效性。此外，定性分析揭示了MGT特有的、可见的语言指纹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The impressive ability of large language models to generate natural textacross various tasks has led to critical challenges in authorshipauthentication. Although numerous detection methods have been developed todifferentiate between machine-generated texts (MGT) and human-generated texts(HGT), the explainability of these methods remains a significant gap.Traditional explainability techniques often fall short in capturing the complexword relationships that distinguish HGT from MGT. To address this limitation,we present LM$^2$otifs, a novel explainable framework for MGT detection.Inspired by probabilistic graphical models, we provide a theoretical rationalefor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networksto achieve both accurate detection and interpretability. The LM$^2$otifspipeline operates in three key stages: first, it transforms text into graphsbased on word co-occurrence to represent lexical dependencies; second, graphneural networks are used for prediction; and third, a post-hoc explainabilitymethod extracts interpretable motifs, offering multi-level explanations fromindividual words to sentence structures. Extensive experiments on multiplebenchmark datasets demonstrate the comparable performance of LM$^2$otifs. Theempirical evaluation of the extracted explainable motifs confirms theireffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysisreveals distinct and visible linguistic fingerprints characteristic of MGT.</description>
      <author>example@mail.com (Xu Zheng, Zhuomin Chen, Esteban Schafir, Sipeng Chen, Hojat Allah Salehi, Haifeng Chen, Farhad Shirani, Wei Cheng, Dongsheng Luo)</author>
      <guid isPermaLink="false">2505.12507v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SPKLIP: Aligning Spike Video Streams with Natural Language</title>
      <link>http://arxiv.org/abs/2505.12656v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPKLIP是一种专为Spike-VLA（神经元视频-语言对齐）设计的架构，通过引入分层神经元特征提取器和对比学习，实现了对神经元视频和语言的直接对齐，提高了能效，并在基准数据和真实世界数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;Spike cameras具有独特的感知能力，但它们的稀疏、异步输出对语义理解构成了挑战，特别是在Spike-VLA任务中，由于模态不匹配，如CLIP等模型表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出SPKLIP架构，旨在解决Spike-VLA中的模态不匹配问题，实现神经元视频和语言的准确对齐。&lt;h4&gt;方法&lt;/h4&gt;SPKLIP使用分层神经元特征提取器来适应性地模拟事件流中的多尺度时间动态，并采用对比学习直接对齐神经元视频和语言，同时引入了全神经元视觉编码器以增强能效。&lt;h4&gt;主要发现&lt;/h4&gt;SPKLIP在基准神经元数据集上实现了最先进的性能，并在新的真实世界数据集上展示了强大的少样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SPKLIP的高能效表明其在神经形态部署中的潜力，推动了基于事件的多模态研究。&lt;h4&gt;翻译&lt;/h4&gt;Spike cameras provide unique sensing capabilities, but their sparse and asynchronous outputs pose challenges to semantic understanding, especially for SpikeVideo-Language Alignment (Spike-VLA), where models like CLIP underperform due to modality mismatch. We introduce SPKLIP, the first architecture specifically designed for Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that adaptively models multi-scale temporal dynamics in event streams, and uses spike-text contrastive learning to directly align spike video with language, enabling effective few-shot learning. A full-spiking visual encoder variant, integrating SNN components into our pipeline, demonstrates enhanced energy efficiency. Experiments show state-of-the-art performance on benchmark spike datasets and strong few-shot generalization on a newly contributed real-world dataset. SPKLIP's energy efficiency highlights its potential for neuromorphic deployment, advancing event-based multimodal research. The source code and dataset are available at [link removed for anonymity].&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spike cameras offer unique sensing capabilities but their sparse,asynchronous output challenges semantic understanding, especially for SpikeVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due tomodality mismatch. We introduce SPKLIP, the first architecture specifically forSpike-VLA. SPKLIP employs a hierarchical spike feature extractor thatadaptively models multi-scale temporal dynamics in event streams, and usesspike-text contrastive learning to directly align spike video with language,enabling effective few-shot learning. A full-spiking visual encoder variant,integrating SNN components into our pipeline, demonstrates enhanced energyefficiency. Experiments show state-of-the-art performance on benchmark spikedatasets and strong few-shot generalization on a newly contributed real-worlddataset. SPKLIP's energy efficiency highlights its potential for neuromorphicdeployment, advancing event-based multimodal research. The source code anddataset are available at [link removed for anonymity].</description>
      <author>example@mail.com (Yongchang Gao, Meiling Jin, Zhaofei Yu, Tiejun Huang, Guozhang Chen)</author>
      <guid isPermaLink="false">2505.12656v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy</title>
      <link>http://arxiv.org/abs/2505.12693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应的多模态融合框架TACOcc，用于3D语义占用预测，并通过体积渲染监督增强其性能。&lt;h4&gt;背景&lt;/h4&gt;多模态3D占用预测的性能受限于无效的融合，主要由于固定融合策略导致的几何-语义不匹配和由稀疏、噪声标注引起的表面细节损失。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决方案，以解决几何-语义不匹配和表面细节损失问题，从而提高3D占用预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种目标尺度自适应、双向对称检索机制，以解决固定邻域融合下的匹配偏差。2. 引入了一种基于3D高斯Splatting的改进体积渲染流程，用于图像渲染，并应用光度一致性监督，联合优化2D-3D一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的机制可以增强上下文感知，提高效率并抑制噪声，实现准确的跨模态特征对齐。改进的体积渲染流程可以增强表面细节重建，同时抑制噪声传播。&lt;h4&gt;结论&lt;/h4&gt;在nuScenes和SemanticKITTI基准上的实验验证了TACOcc框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态3D占用预测的性能受到无效融合的限制，这主要由于固定融合策略导致的几何-语义不匹配以及由稀疏、噪声标注引起的表面细节损失。这种不匹配源于点云和图像特征的异构尺度与分布，导致固定邻域融合下的匹配偏差。为了解决这个问题，我们提出了一种目标尺度自适应的双向对称检索机制。该机制扩大大目标周围的邻域以增强上下文感知，缩小小目标的邻域以提高效率和抑制噪声，从而实现准确的跨模态特征对齐。该机制明确建立了空间对应关系并提高了融合的准确性。对于表面细节损失，稀疏标签提供了有限的监督，导致对小对象的预测效果不佳。我们引入了一种基于3D高斯Splatting的改进体积渲染流程，它将融合特征作为输入进行图像渲染，应用光度一致性监督，并联合优化2D-3D一致性。这增强了表面细节重建，同时抑制了噪声传播。总之，我们提出了TACOcc，一种自适应的多模态融合框架，用于3D语义占用预测，并通过体积渲染监督增强其性能。在nuScenes和SemanticKITTI基准上的实验验证了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of multi-modal 3D occupancy prediction is limited byineffective fusion, mainly due to geometry-semantics mismatch from fixed fusionstrategies and surface detail loss caused by sparse, noisy annotations. Themismatch stems from the heterogeneous scale and distribution of point cloud andimage features, leading to biased matching under fixed neighborhood fusion. Toaddress this, we propose a target-scale adaptive, bidirectional symmetricretrieval mechanism. It expands the neighborhood for large targets to enhancecontext awareness and shrinks it for small ones to improve efficiency andsuppress noise, enabling accurate cross-modal feature alignment. This mechanismexplicitly establishes spatial correspondences and improves fusion accuracy.For surface detail loss, sparse labels provide limited supervision, resultingin poor predictions for small objects. We introduce an improved volumerendering pipeline based on 3D Gaussian Splatting, which takes fused featuresas input to render images, applies photometric consistency supervision, andjointly optimizes 2D-3D consistency. This enhances surface detailreconstruction while suppressing noise propagation. In summary, we proposeTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancyprediction, enhanced by volume rendering supervision. Experiments on thenuScenes and SemanticKITTI benchmarks validate its effectiveness.</description>
      <author>example@mail.com (Luyao Lei, Shuo Xu, Yifan Bai, Xing Wei)</author>
      <guid isPermaLink="false">2505.12693v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.12681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在存在领域分布偏移的情况下，如何利用对抗性数据增强（ADA）来提高迁移学习系统的鲁棒性和适应性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习在存在领域分布偏移的情况下面临挑战，而对抗性扰动传统上被视为暴露模型脆弱性的威胁。&lt;h4&gt;目的&lt;/h4&gt;系统地研究对抗性数据增强（ADA）在提高迁移学习中的鲁棒性和适应性所起的作用。&lt;h4&gt;方法&lt;/h4&gt;分析对抗样本在训练中战略性地使用如何通过丰富决策边界和减少对源域特定特征的过拟合来提高领域泛化。提出了一个将ADA与一致性正则化和领域不变表示学习相结合的统一框架。&lt;h4&gt;主要发现&lt;/h4&gt;在VisDA、DomainNet和Office-Home等多个基准数据集上的实验表明，该方法在无监督和少样本域适应设置下均能持续提高目标域的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，对抗学习具有建设性，将扰动从破坏性攻击转化为提高跨领域迁移性的正则化力量。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Transfer learning across domains with distribution shift remains a fundamental challenge in building robust and adaptable machine learning systems. While adversarial perturbations are traditionally viewed as threats that expose model vulnerabilities, recent studies suggest that they can also serve as constructive tools for data augmentation. In this work, we systematically investigate the role of adversarial data augmentation (ADA) in enhancing both robustness and adaptivity in transfer learning settings. We analyze how adversarial examples, when used strategically during training, improve domain generalization by enriching decision boundaries and reducing overfitting to source-domain-specific features. We further propose a unified framework that integrates ADA with consistency regularization and domain-invariant representation learning. Extensive experiments across multiple benchmark datasets -- including VisDA, DomainNet, and Office-Home -- demonstrate that our method consistently improves target-domain performance under both unsupervised and few-shot domain adaptation settings. Our results highlight a constructive perspective of adversarial learning, transforming perturbation from a destructive attack into a regularizing force for cross-domain transferability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning across domains with distribution shift remains afundamental challenge in building robust and adaptable machine learningsystems. While adversarial perturbations are traditionally viewed as threatsthat expose model vulnerabilities, recent studies suggest that they can alsoserve as constructive tools for data augmentation. In this work, wesystematically investigate the role of adversarial data augmentation (ADA) inenhancing both robustness and adaptivity in transfer learning settings. Weanalyze how adversarial examples, when used strategically during training,improve domain generalization by enriching decision boundaries and reducingoverfitting to source-domain-specific features. We further propose a unifiedframework that integrates ADA with consistency regularization anddomain-invariant representation learning. Extensive experiments across multiplebenchmark datasets -- including VisDA, DomainNet, and Office-Home --demonstrate that our method consistently improves target-domain performanceunder both unsupervised and few-shot domain adaptation settings. Our resultshighlight a constructive perspective of adversarial learning, transformingperturbation from a destructive attack into a regularizing force forcross-domain transferability.</description>
      <author>example@mail.com (Hana Satou, Alan Mitkiy)</author>
      <guid isPermaLink="false">2505.12681v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval</title>
      <link>http://arxiv.org/abs/2505.12499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GARE的Gap-Aware Retrieval框架，用于解决文本-视频检索中的模态差距和批量采样中错误负样本的问题，通过引入可学习的增量Delta_ij来缓解优化紧张。&lt;h4&gt;背景&lt;/h4&gt;现有的文本-视频检索方法主要受到对比学习框架的驱动，但忽略了文本和视频在表示空间中的分离（模态差距）以及批量采样中错误负样本的普遍存在。&lt;h4&gt;目的&lt;/h4&gt;提出GARE框架的目的是为了缓解优化紧张，提高检索的稳定性和准确性。&lt;h4&gt;方法&lt;/h4&gt;GARE框架通过以下方法实现：1. 引入可学习的增量Delta_ij，通过InfoNCE损失的耦合多元一阶泰勒近似来计算；2. 设计一个轻量级的神经网络模块，基于语义差距进行结构感知修正；3. 使用三个正则化组件来稳定学习和提高可解释性：信任域约束、方向多样性项和信息瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GARE框架在四个检索基准上均能提高对齐准确性和对噪声监督的鲁棒性，证实了间隙感知紧张缓解的有效性。&lt;h4&gt;结论&lt;/h4&gt;GARE框架通过引入增量Delta_ij和正则化技术，有效地缓解了文本-视频检索中的模态差距和优化紧张问题，提高了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, a Gap-Aware Retrieval framework named GARE is proposed to address the modality gap and the prevalence of false negatives in batch sampling in text-video retrieval. The framework introduces a learnable increment Delta_ij to alleviate the optimization tension. Experiments show that GARE improves alignment accuracy and robustness to noisy supervision, confirming the effectiveness of the gap-aware tension mitigation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/musicman217/gare-text-video-retrieval&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-video retrieval have been largely driven bycontrastive learning frameworks. However, existing methods overlook a keysource of optimization tension: the separation between text and videodistributions in the representation space (referred to as the modality gap),and the prevalence of false negatives in batch sampling. These factors lead toconflicting gradients under the InfoNCE loss, impeding stable alignment. Tomitigate this, we propose GARE, a Gap-Aware Retrieval framework that introducesa learnable, pair-specific increment Delta_ij between text t_i and video v_j tooffload the tension from the global anchor representation. We first derive theideal form of Delta_ij via a coupled multivariate first-order Taylorapproximation of the InfoNCE loss under a trust-region constraint, revealing itas a mechanism for resolving gradient conflicts by guiding updates along alocally optimal descent direction. Due to the high cost of directly computingDelta_ij, we introduce a lightweight neural module conditioned on the semanticgap between each video-text pair, enabling structure-aware correction guided bygradient supervision. To further stabilize learning and promoteinterpretability, we regularize Delta using three components: a trust-regionconstraint to prevent oscillation, a directional diversity term to promotesemantic coverage, and an information bottleneck to limit redundancy.Experiments across four retrieval benchmarks show that GARE consistentlyimproves alignment accuracy and robustness to noisy supervision, confirming theeffectiveness of gap-aware tension mitigation.</description>
      <author>example@mail.com (Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong)</author>
      <guid isPermaLink="false">2505.12499v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design</title>
      <link>http://arxiv.org/abs/2505.12664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to IEEE Transactions on Wireless Communications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将物理知识融入基于学习的目标感知方法，利用多个基站和用户设备之间的多视角信道状态信息（CSI）进行高精度目标感知。&lt;h4&gt;背景&lt;/h4&gt;多视角感知问题可以自然地映射到条件生成框架中。&lt;h4&gt;目的&lt;/h4&gt;设计一种双部分神经网络架构，以融合多视角CSI中的潜在目标特征，并使用这些特征作为条件输入，指导目标的重构。&lt;h4&gt;方法&lt;/h4&gt;设计了一个编码器，用于捕捉CSI与目标之间的物理相关性，并适应基站-用户对的数量和位置。通过引入空间位置嵌入方案，利用电磁波传播通道的结构来模拟CSI的视角特定性质。最后，使用加权损失的条件扩散模型从融合的特征生成目标的点云。&lt;h4&gt;主要发现&lt;/h4&gt;提出的生成多视角（Gen-MV）感知框架在目标形状和电磁性质的重构质量上表现出卓越的灵活性和显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;Gen-MV感知框架在目标感知任务中具有优异的性能和适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we incorporate physical knowledge into learning-basedhigh-precision target sensing using the multi-view channel state information(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kindof multi-view sensing problem can be naturally cast into a conditionalgeneration framework. To this end, we design a bipartite neural networkarchitecture, the first part of which uses an elaborately designed encoder tofuse the latent target features embedded in the multi-view CSI, and then thesecond uses them as conditioning inputs of a powerful generative model to guidethe target's reconstruction. Specifically, the encoder is designed to capturethe physical correlation between the CSI and the target, and also be adaptiveto the numbers and positions of BS-UE pairs. Therein the view-specific natureof CSI is assimilated by introducing a spatial positional embedding scheme,which exploits the structure of electromagnetic(EM)-wave propagation channels.Finally, a conditional diffusion model with a weighted loss is employed togenerate the target's point cloud from the fused features. Extensive numericalresults demonstrate that the proposed generative multi-view (Gen-MV) sensingframework exhibits excellent flexibility and significant performanceimprovement on the reconstruction quality of target's shape and EM properties.</description>
      <author>example@mail.com (Ziqing Xing, Zhaoyang Zhang, Zirui Chen, Hongning Ruan, Zhaohui Yang)</author>
      <guid isPermaLink="false">2505.12664v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.12246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Robotics and Automation Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于SD地图增强的场景感知和拓扑推理框架（SEPT），旨在解决自动驾驶车辆在长距离或遮挡场景下，由于车载传感器限制而导致的在线场景理解局限性。&lt;h4&gt;背景&lt;/h4&gt;在线场景感知和拓扑推理对于自动驾驶车辆理解其驾驶环境至关重要，尤其是对于减少对昂贵高清地图依赖的无地图驾驶系统。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出SEPT框架，旨在有效地将SD地图作为先验知识集成到现有的感知和推理流程中。&lt;h4&gt;方法&lt;/h4&gt;SEPT框架采用了一种新的混合特征融合策略，结合SD地图和鸟瞰图（BEV）特征，同时考虑了栅格化和矢量化表示，并减轻了SD地图与BEV特征空间之间的潜在不匹配。此外，利用SD地图特征设计了一个辅助的交叉感知关键点检测任务，以增强整体场景理解性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2数据集上的实验结果表明，通过有效集成SD地图先验知识，该框架显著提高了场景感知和拓扑推理能力，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SEPT框架通过结合SD地图信息，有效地提高了自动驾驶车辆在复杂环境下的感知和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在线场景感知和拓扑推理对自动驾驶车辆理解其驾驶环境至关重要，尤其是对于试图减少对昂贵高清地图依赖的无地图驾驶系统。然而，由于车载传感器的固有限制，在线场景理解的最新进展仍存在局限性，尤其是在长距离或遮挡场景中。为了解决这一挑战，我们提出了一种标准定义（SD）地图增强场景感知和拓扑推理（SEPT）框架，该框架探讨了如何有效地将SD地图作为先验知识集成到现有的感知和推理管道中。具体来说，我们引入了一种新的混合特征融合策略，结合SD地图与鸟瞰图（BEV）特征，同时考虑了栅格化和矢量化表示，并减轻了SD地图与BEV特征空间之间的潜在不匹配。此外，我们利用SD地图特征设计了一个辅助的交叉感知关键点检测任务，进一步增强了整体场景理解性能。在大型OpenLane-V2数据集上的实验结果表明，通过有效集成SD地图先验知识，我们的框架显著提高了场景感知和拓扑推理，大幅度优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online scene perception and topology reasoning are critical for autonomousvehicles to understand their driving environments, particularly for maplessdriving systems that endeavor to reduce reliance on costly High-Definition (HD)maps. However, recent advances in online scene understanding still facelimitations, especially in long-range or occluded scenarios, due to theinherent constraints of onboard sensors. To address this challenge, we proposea Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning(SEPT) framework, which explores how to effectively incorporate the SD map asprior knowledge into existing perception and reasoning pipelines. Specifically,we introduce a novel hybrid feature fusion strategy that combines SD maps withBird's-Eye-View (BEV) features, considering both rasterized and vectorizedrepresentations, while mitigating potential misalignment between SD maps andBEV feature spaces. Additionally, we leverage the SD map characteristics todesign an auxiliary intersection-aware keypoint detection task, which furtherenhances the overall scene understanding performance. Experimental results onthe large-scale OpenLane-V2 dataset demonstrate that by effectively integratingSD map priors, our framework significantly improves both scene perception andtopology reasoning, outperforming existing methods by a substantial margin.</description>
      <author>example@mail.com (Muleilan Pei, Jiayao Shan, Peiliang Li, Jieqi Shi, Jing Huo, Yang Gao, Shaojie Shen)</author>
      <guid isPermaLink="false">2505.12246v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables</title>
      <link>http://arxiv.org/abs/2505.12473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多模态对比学习的理论特性，探讨了其学习到的表示在超越线性表示和特定数据分布之外的特性。&lt;h4&gt;背景&lt;/h4&gt;多模态对比学习作为一种自监督的表示学习方法，在基础模型训练（如CLIP）中取得了巨大成功。&lt;h4&gt;目的&lt;/h4&gt;研究多模态对比学习所学习到的表示的理论特性，尤其是超越线性表示和特定数据分布的特性。&lt;h4&gt;方法&lt;/h4&gt;通过温度优化，分析多模态对比学习在最大化模态间互信息的同时，如何适应数据的内在维度。&lt;h4&gt;主要发现&lt;/h4&gt;多模态对比学习不仅最大化了模态间的互信息，还能适应数据的内在维度，这些维度可能远低于用户指定的表示向量维度。&lt;h4&gt;结论&lt;/h4&gt;实验表明，对比学习能够学习到低维且信息丰富的表示，将理论洞察与实际性能相结合。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态对比学习作为一种自监督的表示学习方法，在基础模型训练（如CLIP）中取得了巨大成功。在本文中，我们研究了多模态对比学习所学习到的表示的理论特性，特别是在超越线性表示和特定数据分布的范畴之外。我们的分析揭示，得益于温度优化，多模态对比学习不仅最大化了模态间的互信息，还适应了数据的内在维度，这些维度可能远低于用户指定的表示向量维度。在合成数据和真实世界数据集上的实验表明，对比学习能够学习到低维且信息丰富的表示，将理论洞察与实际性能相结合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal contrastive learning as a self-supervised representation learningtechnique has achieved great success in foundation model training, such asCLIP~\citep{radford2021learning}. In this paper, we study the theoreticalproperties of the learned representations from multi-modal contrastive learningbeyond linear representations and specific data distributions. Our analysisreveals that, enabled by temperature optimization, multi-modal contrastivelearning not only maximizes mutual information between modalities but alsoadapts to intrinsic dimensions of data, which can be much lower thanuser-specified dimensions for representation vectors. Experiments on bothsynthetic and real-world datasets demonstrate the ability of contrastivelearning to learn low-dimensional and informative representations, bridgingtheoretical insights and practical performance.</description>
      <author>example@mail.com (Yu Gui, Cong Ma, Zongming Ma)</author>
      <guid isPermaLink="false">2505.12473v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding</title>
      <link>http://arxiv.org/abs/2505.12605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，大型视觉语言模型（LVLMs）在视频理解方面取得了显著进展，但它们对时间理解的贡献要素尚不明确，这可能会限制其潜力。&lt;h4&gt;背景&lt;/h4&gt;大多数LVLMs依赖其隐含的时间理解能力来处理视频理解，但它们没有揭示对时间理解能力有重要影响的要素。&lt;h4&gt;目的&lt;/h4&gt;进行一项彻底的实证研究，以揭示影响LVLMs时间理解能力的关键组件。&lt;h4&gt;方法&lt;/h4&gt;通过实证研究揭示影响LVLMs时间理解能力的关键组件，并基于这些发现提出一种面向时间理解的方案。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，对时间理解能力有显著影响的是视觉编码器和大型语言模型之间的中间接口。&lt;h4&gt;结论&lt;/h4&gt;通过提出的时间理解方案，最终模型在标准视频理解任务上显著提升了之前的LVLMs。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, there have been outstanding advances in large vision-language models (LVLMs) for video understanding. However, they have not deciphered the important components that contribute to temporal understanding ability, which might limit the potential of these LVLMs for video understanding. In this work, we conduct a thorough empirical study to demystify the crucial components that influence the temporal understanding of LVLMs. Our empirical study reveals that significant impacts are centered around the intermediate interface between the visual encoder and the large language model. Building on these insights, we propose a temporal-oriented recipe that encompasses temporal-oriented training schemes and an upscaled interface. Our final model developed using our recipe significantly enhances previous LVLMs on standard video understanding tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have witnessed outstanding advances of large vision-languagemodels (LVLMs). In order to tackle video understanding, most of them dependupon their implicit temporal understanding capacity. As such, they have notdeciphered important components that contribute to temporal understandingability, which might limit the potential of these LVLMs for videounderstanding. In this work, we conduct a thorough empirical study to demystifycrucial components that influence the temporal understanding of LVLMs. Ourempirical study reveals that significant impacts are centered around theintermediate interface between the visual encoder and the large language model.Building on these insights, we propose a temporal-oriented recipe thatencompasses temporal-oriented training schemes and an upscaled interface. Ourfinal model developed using our recipe significantly enhances previous LVLMs onstandard video understanding tasks.</description>
      <author>example@mail.com (Thong Nguyen, Zhiyuan Hu, Xu Lin, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan)</author>
      <guid isPermaLink="false">2505.12605v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling</title>
      <link>http://arxiv.org/abs/2505.12890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ORQA的新型手术室问答基准和基础多模态模型，以提升手术室智能水平。&lt;h4&gt;背景&lt;/h4&gt;手术的复杂性要求外科医生具备深厚的全面理解以确保操作的精确性、安全性和有效性。目前的工作主要集中在单一任务上，如阶段识别或场景图生成，缺乏范围和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时解决多种手术室挑战的全面基准和多模态模型。&lt;h4&gt;方法&lt;/h4&gt;将四个公开的手术室数据集统一为一个综合基准，提出了一种融合视觉、听觉和结构化数据的跨模态大型语言模型，并引入了一种新颖的渐进式知识蒸馏范式，以生成针对不同速度和内存需求的优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;ORQA在提出的基准上显示出强大的性能，实现了零样本泛化，为可扩展的、统一的手术室建模和多模态外科智能的发展铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;本文的研究成果将推动手术室智能的发展，并为手术室的多模态建模提供新的方法和工具。&lt;h4&gt;翻译&lt;/h4&gt;The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within the operating room. Prior works, limited to single-tasks efforts like phase recognition or scene graph generation, lack scope and generalizability. In this work, we introduce ORQA, a novel OR question answering benchmark and foundational multimodal model to advance OR intelligence. By unifying all four public OR datasets into a comprehensive benchmark, we enable our approach to concurrently address a diverse range of OR challenges. The proposed multimodal large language model fuses diverse OR signals such as visual, auditory, and structured data, for a holistic modeling of the OR. Finally, we propose a novel, progressive knowledge distillation paradigm, to generate a family of models optimized for different speed and memory requirements. We show the strong performance of ORQA on our proposed benchmark, and its zero-shot generalization, paving the way for scalable, unified OR modeling and significantly advancing multimodal surgical intelligence. We will release our code and data upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The real-world complexity of surgeries necessitates surgeons to have deep andholistic comprehension to ensure precision, safety, and effectiveinterventions. Computational systems are required to have a similar level ofcomprehension within the operating room. Prior works, limited to single-taskefforts like phase recognition or scene graph generation, lack scope andgeneralizability. In this work, we introduce ORQA, a novel OR questionanswering benchmark and foundational multimodal model to advance ORintelligence. By unifying all four public OR datasets into a comprehensivebenchmark, we enable our approach to concurrently address a diverse range of ORchallenges. The proposed multimodal large language model fuses diverse ORsignals such as visual, auditory, and structured data, for a holistic modelingof the OR. Finally, we propose a novel, progressive knowledge distillationparadigm, to generate a family of models optimized for different speed andmemory requirements. We show the strong performance of ORQA on our proposedbenchmark, and its zero-shot generalization, paving the way for scalable,unified OR modeling and significantly advancing multimodal surgicalintelligence. We will release our code and data upon acceptance.</description>
      <author>example@mail.com (Ege Özsoy, Chantal Pellegrini, David Bani-Harouni, Kun Yuan, Matthias Keicher, Nassir Navab)</author>
      <guid isPermaLink="false">2505.12890v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling and Steering Connectome Organization with Interpretable Latent Variables</title>
      <link>http://arxiv.org/abs/2505.13011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过结合连接组学和表示学习，提出了一种框架，用于从果蝇连接组FlyWire中提取子图，并结合生成模型来推导神经电路的低维可解释表示。该方法能够有效地重建图结构，并能够通过操纵潜在代码来可控地生成具有预定属性的连接组子图。&lt;h4&gt;背景&lt;/h4&gt;大脑的连接组（connectome）是大脑功能的蓝图，具有巨大的复杂性，但其起源却是一个紧凑的遗传代码，这表明存在低维组织原则。&lt;h4&gt;目的&lt;/h4&gt;揭示大脑连接组中的低维组织原则。&lt;h4&gt;方法&lt;/h4&gt;提出了一种框架，该框架结合了从Drosophila连接组FlyWire中提取子图的方法，以及一个生成模型来推导神经电路的低维可解释表示。此外，引入了一个可解释模块，将潜在维度与特定的结构特征联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地重建图结构，并且能够通过操纵潜在代码来可控地生成具有预定属性的连接组子图。&lt;h4&gt;结论&lt;/h4&gt;这项研究为理解大脑架构提供了一个新的工具，并可能为设计受生物启发的神经网络开辟了一条新的途径。&lt;h4&gt;翻译&lt;/h4&gt;The brain's intricate connectome, a blueprint for its function, presents immense complexity, yet it arises from a compact genetic code, hinting at underlying low-dimensional organizational principles. This work bridges connectomics and representation learning to uncover these principles. We propose a framework that combines subgraph extraction from the Drosophila connectome, FlyWire, with a generative model to derive interpretable low-dimensional representations of neural circuitry. Crucially, an explainability module links these latent dimensions to specific structural features, offering insights into their functional relevance. We validate our approach by demonstrating effective graph reconstruction and, significantly, the ability to manipulate these latent codes to controllably generate connectome subgraphs with predefined properties. This research offers a novel tool for understanding brain architecture and a potential avenue for designing bio-inspired artificial neural networks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The brain's intricate connectome, a blueprint for its function, presentsimmense complexity, yet it arises from a compact genetic code, hinting atunderlying low-dimensional organizational principles. This work bridgesconnectomics and representation learning to uncover these principles. Wepropose a framework that combines subgraph extraction from the Drosophilaconnectome, FlyWire, with a generative model to derive interpretablelow-dimensional representations of neural circuitry. Crucially, anexplainability module links these latent dimensions to specific structuralfeatures, offering insights into their functional relevance. We validate ourapproach by demonstrating effective graph reconstruction and, significantly,the ability to manipulate these latent codes to controllably generateconnectome subgraphs with predefined properties. This research offers a noveltool for understanding brain architecture and a potential avenue for designingbio-inspired artificial neural networks.</description>
      <author>example@mail.com (Yubin Li, Xingyu Liu, Guozhang Chen)</author>
      <guid isPermaLink="false">2505.13011v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind</title>
      <link>http://arxiv.org/abs/2505.12207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AgroMind，一个综合性的农业遥感基准，旨在解决现有基准在场景多样性和任务设计上的不足。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型（LMMs）在多个领域表现出强大的能力，但在农业遥感领域，缺乏全面的基准测试。&lt;h4&gt;目的&lt;/h4&gt;通过引入AgroMind，填补农业遥感领域基准测试的空白，并提供一个标准化的评估框架。&lt;h4&gt;方法&lt;/h4&gt;AgroMind涵盖了四个任务维度：空间感知、物体理解、场景理解和场景推理，包含13种任务类型。通过整合八个公开数据集和一个私有农田地块数据集，构建了一个高质量的评估集。数据预处理包括数据收集、格式标准化和标注细化。使用LMMs进行推理和生成响应。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在空间推理和细粒度识别方面存在显著的性能差距，人类表现落后于一些领先的LMMs。&lt;h4&gt;结论&lt;/h4&gt;AgroMind揭示了LMMs在领域知识方面的局限性，并指出了未来工作的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Large Multimodal Models (LMMs) have demonstrated capabilities across various domains, but comprehensive benchmarks for agricultural remote sensing (RS) remain scarce. Existing benchmarks designed for agricultural RS scenarios exhibit notable limitations, primarily in terms of insufficient scene diversity in the dataset and oversimplified task design. To bridge this gap, we introduce AgroMind, a comprehensive agricultural remote sensing benchmark covering four task dimensions: spatial perception, object understanding, scene understanding, and scene reasoning, with a total of 13 task types, ranging from crop identification and health monitoring to environmental analysis. We curate a high-quality evaluation set by integrating eight public datasets and one private farmland plot dataset, containing 25,026 QA pairs and 15,556 images. The pipeline begins with multi-source data preprocessing, including collection, format standardization, and annotation refinement. We then generate a diverse set of agriculturally relevant questions through the systematic definition of tasks. Finally, we employ LMMs for inference, generating responses, and performing detailed examinations. We evaluated 18 open-source LMMs and 3 closed-source models on AgroMind. Experiments reveal significant performance gaps, particularly in spatial reasoning and fine-grained recognition, it is notable that human performance lags behind several leading LMMs. By establishing a standardized evaluation framework for agricultural RS, AgroMind reveals the limitations of LMMs in domain knowledge and highlights critical challenges for future work. Data and code can be accessed at https://rssysu.github.io/AgroMind/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) has demonstrated capabilities across variousdomains, but comprehensive benchmarks for agricultural remote sensing (RS)remain scarce. Existing benchmarks designed for agricultural RS scenariosexhibit notable limitations, primarily in terms of insufficient scene diversityin the dataset and oversimplified task design. To bridge this gap, we introduceAgroMind, a comprehensive agricultural remote sensing benchmark covering fourtask dimensions: spatial perception, object understanding, scene understanding,and scene reasoning, with a total of 13 task types, ranging from cropidentification and health monitoring to environmental analysis. We curate ahigh-quality evaluation set by integrating eight public datasets and oneprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.The pipeline begins with multi-source data preprocessing, including collection,format standardization, and annotation refinement. We then generate a diverseset of agriculturally relevant questions through the systematic definition oftasks. Finally, we employ LMMs for inference, generating responses, andperforming detailed examinations. We evaluated 18 open-source LMMs and 3closed-source models on AgroMind. Experiments reveal significant performancegaps, particularly in spatial reasoning and fine-grained recognition, it isnotable that human performance lags behind several leading LMMs. Byestablishing a standardized evaluation framework for agricultural RS, AgroMindreveals the limitations of LMMs in domain knowledge and highlights criticalchallenges for future work. Data and code can be accessed athttps://rssysu.github.io/AgroMind/.</description>
      <author>example@mail.com (Qingmei Li, Yang Zhang, Zurong Mai, Yuhang Chen, Shuohong Lou, Henglian Huang, Jiarui Zhang, Zhiwei Zhang, Yibin Wen, Weijia Li, Haohuan Fu, Jianxi Huang, Juepeng Zheng)</author>
      <guid isPermaLink="false">2505.12207v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding</title>
      <link>http://arxiv.org/abs/2505.12408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ViEEG的生物启发式分层EEG解码框架，该框架旨在理解和解码大脑活动为视觉表示，并取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;理解大脑活动并将其解码为视觉表示是神经科学和人工智能交叉领域的一个基本挑战。虽然基于EEG的视觉解码因其非侵入性、低成本和毫秒级时间分辨率而显示出潜力，但现有方法因依赖平面的神经表示而限制了其性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的EEG解码框架，以解决现有方法中忽略大脑内在视觉层次结构的问题。&lt;h4&gt;方法&lt;/h4&gt;ViEEG将每个视觉刺激分解为三个生物对齐的组件——轮廓、前景物体和背景场景，作为三个流EEG编码器的锚点。通过跨注意力路由逐步整合EEG特征，模拟从V1到IT再到联合皮层的皮层信息流。此外，采用分层对比学习使EEG表示与CLIP嵌入对齐，实现零样本物体识别。&lt;h4&gt;主要发现&lt;/h4&gt;在THINGS-EEG数据集上的广泛实验表明，ViEEG实现了最先进的性能，主体依赖设置中Top-1准确率为40.9%，跨主体设置中Top-1准确率为22.9%，超过现有方法45%以上。&lt;h4&gt;结论&lt;/h4&gt;ViEEG不仅推动了性能前沿，还为AI中的基于生物的脑解码设定了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;Understanding and decoding brain activity into visual representations is a fundamental challenge at the intersection of neuroscience and artificial intelligence. While EEG-based visual decoding has shown promise due to its non-invasive, low-cost nature and millisecond-level temporal resolution, existing methods are limited by their reliance on flat neural representations that overlook the brain's inherent visual hierarchy. In this paper, we introduce ViEEG, a biologically inspired hierarchical EEG decoding framework that aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes each visual stimulus into three biologically aligned components - contour, foreground object, and contextual scene - serving as anchors for a three-stream EEG encoder. These EEG features are progressively integrated via cross-attention routing, simulating cortical information flow from V1 to IT to the association cortex. We further adopt hierarchical contrastive learning to align EEG representations with CLIP embeddings, enabling zero-shot object recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that ViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in subject-dependent and 22.9% Top-1 accuracy in cross-subject settings, surpassing existing methods by over 45%. Our framework not only advances the performance frontier but also sets a new paradigm for biologically grounded brain decoding in AI.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and decoding brain activity into visual representations is afundamental challenge at the intersection of neuroscience and artificialintelligence. While EEG-based visual decoding has shown promise due to itsnon-invasive, low-cost nature and millisecond-level temporal resolution,existing methods are limited by their reliance on flat neural representationsthat overlook the brain's inherent visual hierarchy. In this paper, weintroduce ViEEG, a biologically inspired hierarchical EEG decoding frameworkthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposeseach visual stimulus into three biologically aligned components-contour,foreground object, and contextual scene-serving as anchors for a three-streamEEG encoder. These EEG features are progressively integrated viacross-attention routing, simulating cortical information flow from V1 to IT tothe association cortex. We further adopt hierarchical contrastive learning toalign EEG representations with CLIP embeddings, enabling zero-shot objectrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate thatViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy insubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,surpassing existing methods by over 45%. Our framework not only advances theperformance frontier but also sets a new paradigm for biologically groundedbrain decoding in AI.</description>
      <author>example@mail.com (Minxu Liu, Donghai Guan, Chuhang Zheng, Chunwei Tian, Jie Wen, Qi Zhu)</author>
      <guid isPermaLink="false">2505.12408v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DPCD: A Quality Assessment Database for Dynamic Point Clouds</title>
      <link>http://arxiv.org/abs/2505.12431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个大规模的动态点云质量评估数据库DPCD，并评估了动态点云质量评估（DPCQA）的性能。&lt;h4&gt;背景&lt;/h4&gt;虚拟/增强现实（VR/AR）的进步推动了动态点云（DPC）的需求，DPC能够捕捉对象或场景中的时间变化，提供更真实的现实世界模拟。尽管静态点云的质量评估研究取得了显著进展，但动态点云质量评估（DPCQA）的研究很少，这阻碍了质量导向应用的发展。&lt;h4&gt;目的&lt;/h4&gt;提出一个大规模的DPCQA数据库，评估动态点云质量评估（DPCQA）的性能，为质量导向应用的发展提供支持。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含15个参考DPC和525个受损DPC的大规模DPCQA数据库DPCD，通过渲染这些样本到处理视频序列（PVS），进行了一项全面的主体实验，从21个观众那里获得了平均意见得分（MOS）进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DPCQA比静态点云质量评估更具挑战性，DPCD验证了所提出数据库的异质性和可靠性，并作为推动DPCQA新研究努力的催化剂。&lt;h4&gt;结论&lt;/h4&gt;DPCD数据库作为推动DPCQA新研究努力的催化剂，是公开可用的。&lt;h4&gt;翻译&lt;/h4&gt;最近，虚拟/增强现实（VR/AR）的进步推动了动态点云（DPC）的需求。与静态点云不同，DPCs能够捕捉对象或场景中的时间变化，提供更真实的现实世界模拟。尽管静态点云的质量评估研究取得了显著进展，但动态点云质量评估（DPCQA）的研究很少，这阻碍了质量导向应用的发展。在本文中，我们介绍了一个名为DPCD的大规模DPCQA数据库，其中包括15个参考DPC和525个来自七种类型的有损压缩和噪声失真的受损DPC。通过将这些样本渲染到处理视频序列（PVS），进行了一项全面的主体实验，从21个观众那里获得了平均意见得分（MOS）进行分析。内容特性、各种失真影响和MOS的准确性被提出，以验证所提出数据库的异质性和可靠性。此外，我们还评估了DPCD上几个客观指标的性能。实验结果表明，DPCQA比静态点云质量评估更具挑战性。作为推动DPCQA新研究努力的催化剂，DPCD数据库是公开可用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driventhe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs arecapable of capturing temporal changes within objects or scenes, offering a moreaccurate simulation of the real world. While significant progress has been madein the quality assessment research of static point cloud, little study has beendone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders thedevelopment of quality-oriented applications, such as interframe compressionand transmission in practical scenarios. In this paper, we introduce alarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and525 distorted DPCs from seven types of lossy compression and noise distortion.By rendering these samples to Processed Video Sequences (PVS), a comprehensivesubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21viewers for analysis. The characteristic of contents, impact of variousdistortions, and accuracy of MOSs are presented to validate the heterogeneityand reliability of the proposed database. Furthermore, we evaluate theperformance of several objective metrics on DPCD. The experiment results showthat DPCQA is more challenge than that of static point cloud. The DPCD, whichserves as a catalyst for new research endeavors on DPCQA, is publicly availableat https://huggingface.co/datasets/Olivialyt/DPCD.</description>
      <author>example@mail.com (Yating Liu, Yujie Zhang, Qi Yang, Yiling Xu, Zhu Li, Ye-Kui Wang)</author>
      <guid isPermaLink="false">2505.12431v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Addressing the Scarcity of Benchmarks for Graph XAI</title>
      <link>http://arxiv.org/abs/2505.12437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自动化构建图分类XAI基准的方法，旨在解决现有基准数据集不足的问题。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在结构化数据学习中变得普遍，但其决策过程对用户不透明，限制了其在安全关键应用中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来自动化构建用于图分类的XAI基准，以解决现有基准数据集不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一个通用的方法来自动化构建XAI基准，并提供了15个现成的基准和代码，以生成超过2000个额外的XAI基准。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出的方法可以有效地生成大量的XAI基准，用于评估图解释器的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为图分类的XAI研究提供了新的基准数据，有助于提高解释器评估的质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然图神经网络（GNNs）已经成为从结构化数据中学习的实际模型，但其决策过程对最终用户来说仍然不透明，这限制了它们在安全关键应用中的部署。在图分类的情况下，可解释人工智能（XAI）技术通过识别解释预测的子图基序来解决这个主要问题。然而，该领域的进步受到已知基序的基准数据集长期短缺的阻碍，以评估解释的质量。当前的图XAI基准仅限于合成数据或由领域专家手工定制的少数几个真实世界任务。在本文中，我们提出了一种通用方法来自动化从真实世界数据集中构建图分类XAI基准。我们提供了15个现成的基准，以及使用我们的方法生成2000多个额外XAI基准的代码。作为一个用例，我们使用我们的基准来评估一些流行图解释器的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Neural Networks (GNNs) have become the de facto model forlearning from structured data, their decisional process remains opaque to theend user, undermining their deployment in safety-critical applications. In thecase of graph classification, Explainable Artificial Intelligence (XAI)techniques address this major issue by identifying sub-graph motifs thatexplain predictions. However, advancements in this field are hindered by achronic scarcity of benchmark datasets with known ground-truth motifs to assessthe explanations' quality. Current graph XAI benchmarks are limited tosynthetic data or a handful of real-world tasks hand-curated by domain experts.In this paper, we propose a general method to automate the construction of XAIbenchmarks for graph classification from real-world datasets. We provide both15 ready-made benchmarks, as well as the code to generate more than 2000additional XAI benchmarks with our method. As a use case, we employ ourbenchmarks to assess the effectiveness of some popular graph explainers.</description>
      <author>example@mail.com (Michele Fontanesi, Alessio Micheli, Marco Podda, Domenico Tortorella)</author>
      <guid isPermaLink="false">2505.12437v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>InnateCoder: Learning Programmatic Options with Foundation Models</title>
      <link>http://arxiv.org/abs/2505.12508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为InnateCoder的系统，该系统能够利用基础模型中编码的人类知识，提供程序化策略，以编码形式学习‘本能技能’。InnateCoder在零样本设置下从基础模型中学习选项，并通过组合编码这些选项的程序来寻找程序化策略，旨在提高学习程序化策略的采样效率。&lt;h4&gt;背景&lt;/h4&gt;在迁移学习之外，强化学习代理需要从零开始学习，这导致学习过程缓慢，即使是解决问题所需的最明显技能也是如此。&lt;h4&gt;目的&lt;/h4&gt;提出InnateCoder系统，以利用人类知识，提高学习程序化策略的采样效率。&lt;h4&gt;方法&lt;/h4&gt;InnateCoder从基础模型中学习选项，并通过组合程序来寻找程序化策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，InnateCoder在MicroRTS和Karel the Robot上的采样效率高于不使用选项或从经验中学习的系统版本。&lt;h4&gt;结论&lt;/h4&gt;InnateCoder的方法可以有效地提高学习程序化策略的采样效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outside of transfer learning settings, reinforcement learning agents starttheir learning process from a clean slate. As a result, such agents have to gothrough a slow process to learn even the most obvious skills required to solvea problem. In this paper, we present InnateCoder, a system that leverages humanknowledge encoded in foundation models to provide programmatic policies thatencode "innate skills" in the form of temporally extended actions, or options.In contrast to existing approaches to learning options, InnateCoder learns themfrom the general human knowledge encoded in foundation models in a zero-shotsetting, and not from the knowledge the agent gains by interacting with theenvironment. Then, InnateCoder searches for a programmatic policy by combiningthe programs encoding these options into larger and more complex programs. Wehypothesized that InnateCoder's way of learning and using options could improvethe sampling efficiency of current methods for learning programmatic policies.Empirical results in MicroRTS and Karel the Robot support our hypothesis, sincethey show that InnateCoder is more sample efficient than versions of the systemthat do not use options or learn them from experience.</description>
      <author>example@mail.com (Rubens O. Moraes, Quazi Asif Sadmine, Hendrik Baier, Levi H. S. Lelis)</author>
      <guid isPermaLink="false">2505.12508v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems</title>
      <link>http://arxiv.org/abs/2505.11535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LKAlert是一种新型的监督预警系统，用于预测潜在的Lane Keeping Assist系统（LKA）风险，提高驾驶员对自动驾驶辅助系统的信任。&lt;h4&gt;背景&lt;/h4&gt;LKA系统在现实世界中经常出现不可预测的故障，这主要是因为它们是黑盒性质，限制了驾驶员的预期和信任。&lt;h4&gt;目的&lt;/h4&gt;为了在自动化辅助和有效的人类监督之间架起桥梁，LKAlert旨在提前1-3秒预测潜在的LKA风险。&lt;h4&gt;方法&lt;/h4&gt;LKAlert处理行车记录仪视频和CAN数据，结合来自并行可解释模型的代理车道分割特征作为自动引导注意力。它使用VLM进行基于视觉的语言模型的行为预测，并生成预测警报和简洁的自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;LKAlert能够以69.8%的准确率和58.6%的F1分数正确预测即将发生的LKA故障，同时生成高质量的文本解释（71.7 ROUGE-L），并且以大约2 Hz的频率高效运行。&lt;h4&gt;结论&lt;/h4&gt;LKAlert被证明是提高当前ADAS安全性和可用性的实用解决方案，并为将VLM应用于以人为中心的黑盒自动化监督提供了一个可扩展的范例。&lt;h4&gt;翻译&lt;/h4&gt;Lane Keeping Assist systems, while increasingly prevalent, often suffer from unpredictable real-world failures, largely due to their opaque, black-box nature, which limits driver anticipation and trust. To bridge the gap between automated assistance and effective human oversight, we present LKAlert, a novel supervisory alert system that leverages VLM to forecast potential LKA risk 1-3 seconds in advance. LKAlert processes dash-cam video and CAN data, integrating surrogate lane segmentation features from a parallel interpretable model as automated guiding attention. Unlike traditional binary classifiers, LKAlert issues both predictive alert and concise natural language explanation, enhancing driver situational awareness and trust. To support the development and evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark dataset designed for predictive and explainable LKA failure warnings. It contains synchronized multimodal inputs and human-authored justifications across annotated temporal windows. We further contribute a generalizable methodological framework for VLM-based black-box behavior prediction, combining surrogate feature guidance with LoRA. This framework enables VLM to reason over structured visual context without altering its vision backbone, making it broadly applicable to other complex, opaque systems requiring interpretable oversight. Empirical results correctly predict upcoming LKA failures with 69.8% accuracy and a 58.6% F1-score. The system also generates high-quality textual explanations for drivers (71.7 ROUGE-L) and operates efficiently at approximately 2 Hz, confirming its suitability for real-time, in-vehicle use. Our findings establish LKAlert as a practical solution for enhancing the safety and usability of current ADAS and offer a scalable paradigm for applying VLMs to human-centered supervision of black-box automation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane Keeping Assist systems, while increasingly prevalent, often suffer fromunpredictable real-world failures, largely due to their opaque, black-boxnature, which limits driver anticipation and trust. To bridge the gap betweenautomated assistance and effective human oversight, we present LKAlert, a novelsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3seconds in advance. LKAlert processes dash-cam video and CAN data, integratingsurrogate lane segmentation features from a parallel interpretable model asautomated guiding attention. Unlike traditional binary classifiers, LKAlertissues both predictive alert and concise natural language explanation,enhancing driver situational awareness and trust. To support the developmentand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmarkdataset designed for predictive and explainable LKA failure warnings. Itcontains synchronized multimodal inputs and human-authored justificationsacross annotated temporal windows. We further contribute a generalizablemethodological framework for VLM-based black-box behavior prediction, combiningsurrogate feature guidance with LoRA. This framework enables VLM to reason overstructured visual context without altering its vision backbone, making itbroadly applicable to other complex, opaque systems requiring interpretableoversight. Empirical results correctly predicts upcoming LKA failures with69.8% accuracy and a 58.6\% F1-score. The system also generates high-qualitytextual explanations for drivers (71.7 ROUGE-L) and operates efficiently atapproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.Our findings establish LKAlert as a practical solution for enhancing the safetyand usability of current ADAS and offer a scalable paradigm for applying VLMsto human-centered supervision of black-box automation.</description>
      <author>example@mail.com (Yuhang Wang, Hao Zhou)</author>
      <guid isPermaLink="false">2505.11535v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</title>
      <link>http://arxiv.org/abs/2505.12903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SFTrack的新型Slow-Fast跟踪范式，该范式通过灵活适应不同的操作需求，支持高精度慢速跟踪器和高效快速跟踪器，以解决传统基于帧的跟踪算法在低延迟性能和资源受限环境中的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的跟踪算法通常依赖于低帧率的RGB相机和计算密集型的深度神经网络架构，但这类方法在低延迟性能和资源受限环境中存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跟踪方法，以解决现有跟踪算法在低延迟和资源受限环境中的不足。&lt;h4&gt;方法&lt;/h4&gt;SFTrack框架首先从高时间分辨率的事件流中进行基于图的表示学习，然后将学习到的图结构信息集成到两个基于FlashAttention的视觉骨干网络中，分别生成慢速和快速跟踪器。快速跟踪器通过轻量级网络设计和单次前向传递生成多个边界框输出以实现低延迟。最后，通过监督微调和知识蒸馏策略，将两个跟踪器无缝结合并提升快速跟踪器的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试（如FE240、COESOT和EventVOT）上的实验表明，所提出的方法在不同真实场景中具有有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;SFTrack方法在低延迟和资源受限环境中表现出色，为视觉对象跟踪提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/event-ahu/slowfast_event_track&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing tracking algorithms typically rely on low-frame-rate RGB camerascoupled with computationally intensive deep neural network architectures toachieve effective tracking. However, such frame-based methods inherently facechallenges in achieving low-latency performance and often fail inresource-constrained environments. Visual object tracking using bio-inspiredevent cameras has emerged as a promising research direction in recent years,offering distinct advantages for low-latency applications. In this paper, wepropose a novel Slow-Fast Tracking paradigm that flexibly adapts to differentoperational requirements, termed SFTrack. The proposed framework supports twocomplementary modes, i.e., a high-precision slow tracker for scenarios withsufficient computational resources, and an efficient fast tracker tailored forlatency-aware, resource-constrained environments. Specifically, our frameworkfirst performs graph-based representation learning fromhigh-temporal-resolution event streams, and then integrates the learnedgraph-structured information into two FlashAttention-based vision backbones,yielding the slow and fast trackers, respectively. The fast tracker achieveslow latency through a lightweight network design and by producing multiplebounding box outputs in a single forward pass. Finally, we seamlessly combineboth trackers via supervised fine-tuning and further enhance the fast tracker'sperformance through a knowledge distillation strategy. Extensive experiments onpublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate theeffectiveness and efficiency of our proposed method across different real-worldscenarios. The source code has been released onhttps://github.com/Event-AHU/SlowFast_Event_Track.</description>
      <author>example@mail.com (Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo)</author>
      <guid isPermaLink="false">2505.12903v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting</title>
      <link>http://arxiv.org/abs/2505.12738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的框架EpiLLM，用于时空流行病预测。&lt;h4&gt;背景&lt;/h4&gt;高级流行病预测对实现精准防控策略至关重要，对于公共卫生安全具有战略意义。尽管大型语言模型（LLMs）在特定领域任务中作为基础模型已显示出有效性，但其在流行病预测方面的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出EpiLLM框架，以实现时空流行病预测。&lt;h4&gt;方法&lt;/h4&gt;考虑现实世界流行病传播的关键因素（感染病例和人类流动性），引入双分支架构以实现LLM对复杂流行病模式和语言标记的精细粒度标记级对齐。提出自回归建模范式，将流行病预测任务重新定义为下一标记预测。引入时空提示学习技术，从数据驱动角度增强LLM对流行病的感知。&lt;h4&gt;主要发现&lt;/h4&gt;EpiLLM在真实世界COVID-19数据集上显著优于现有基线，并表现出LLMs的典型扩展行为。&lt;h4&gt;结论&lt;/h4&gt;EpiLLM是一种有效的时空流行病预测框架，为公共卫生安全提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced epidemic forecasting is critical for enabling precision containmentstrategies, highlighting its strategic importance for public health security.While recent advances in Large Language Models (LLMs) have demonstratedeffectiveness as foundation models for domain-specific tasks, their potentialfor epidemic forecasting remains largely unexplored. In this paper, weintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporalepidemic forecasting. Considering the key factors in real-world epidemictransmission: infection cases and human mobility, we introduce a dual-brancharchitecture to achieve fine-grained token-level alignment between such complexepidemic patterns and language tokens for LLM adaptation. To unleash themulti-step forecasting and generalization potential of LLM architectures, wepropose an autoregressive modeling paradigm that reformulates the epidemicforecasting task into next-token prediction. To further enhance LLM perceptionof epidemics, we introduce spatio-temporal prompt learning techniques, whichstrengthen forecasting capabilities from a data-driven perspective. Extensiveexperiments show that EpiLLM significantly outperforms existing baselines onreal-world COVID-19 datasets and exhibits scaling behavior characteristic ofLLMs.</description>
      <author>example@mail.com (Chenghua Gong, Rui Sun, Yuhao Zheng, Juyuan Zhang, Tianjun Gu, Liming Pan, Linyuan Lv)</author>
      <guid isPermaLink="false">2505.12738v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding</title>
      <link>http://arxiv.org/abs/2505.12194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Multimodal large language models (MLLMs)在处理文本和视觉输入方面的能力，并提出了一种新的方法来解决在线数据稀缺时的专业化任务问题。&lt;h4&gt;背景&lt;/h4&gt;MLLMs在通用任务如场景理解和问答方面表现良好，但在数据稀缺的专业化任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决MLLMs在数据稀缺的专业化任务中的不足，本文提出了SUN-Spot v2.0数据集和Spatial-LLaVA模型。&lt;h4&gt;方法&lt;/h4&gt;SUN-Spot v2.0数据集包含90k个图像-字幕对以及地标对象的额外注释。每个图像-字幕对使用Set-of-Marks提示作为额外指示，将图像中的地标对象映射到字幕中提到的相应对象。Spatial-LLaVA是一个在SUNSpot v2.0数据集上训练的MLLM，用于学习空间指称表达式。&lt;h4&gt;主要发现&lt;/h4&gt;Spatial-LLaVA在零样本视觉空间推理基准数据集上比以前的方法提高了3.15%的性能。&lt;h4&gt;结论&lt;/h4&gt;Spatial-LLaVA特别适用于需要精确物体识别的实际场景任务，如自主导航和交互式机器人。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍的多模态大型语言模型（MLLMs）在理解和处理文本以及视觉输入方面表现出色。通常，这些模型是在互联网上收集的大量数据集上训练的，足以处理场景理解、问答等通用任务。然而，它们在在线数据稀缺的专业化任务中表现不佳，例如确定物体之间的空间关系或在一个具有相似特征的物体组中定位独特的目标物体。为了应对这一挑战，我们引入了SUN-Spot v2.0数据集，现在包含总计90k个图像-字幕对以及地标对象的额外注释。每个图像-字幕对使用Set-of-Marks提示作为额外指示，将图像中的地标对象映射到字幕中提到的相应对象。此外，我们提出了Spatial-LLaVA，这是一个在SUNSpot v2.0数据集上使用最先进的语言模型生成的对话数据训练的MLLM。我们的方法确保了图像中的物体与其在字幕中对应的物体提及之间的稳健对齐，使我们的模型能够学习不受物体语义信息偏差的空间指称表达式。Spatial-LLaVA在零样本视觉空间推理基准数据集上优于以前的方法3.15%。Spatial-LLaVA专门设计用于精确理解空间指称表达式，因此它在需要精确物体识别的实际场景任务（如自主导航和交互式机器人）中非常有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have demonstrated remarkableabilities in comprehending visual input alongside text input. Typically, thesemodels are trained on extensive data sourced from the internet, which aresufficient for general tasks such as scene understanding and questionanswering. However, they often underperform on specialized tasks where onlinedata is scarce, such as determining spatial relationships between objects orlocalizing unique target objects within a group of objects sharing similarfeatures. In response to this challenge, we introduce the SUN-Spot v2.0dataset1, now comprising a total of 90k image-caption pairs and additionalannotations on the landmark objects. Each image-caption pair utilizesSet-of-Marks prompting as an additional indicator, mapping each landmark objectin the image to the corresponding object mentioned in the caption. Furthermore,we present Spatial-LLaVA, an MLLM trained on conversational data generated by astate-of-the-art language model using the SUNSpot v2.0 dataset. Our approachensures a robust alignment between the objects in the images and theircorresponding object mentions in the captions, enabling our model to learnspatial referring expressions without bias from the semantic information of theobjects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shotVisual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specificallydesigned to precisely understand spatial referring expressions, making ithighly applicable for tasks in real-world scenarios such as autonomousnavigation and interactive robotics, where precise object recognition iscritical.</description>
      <author>example@mail.com (Xuefei Sun, Doncey Albin, Cecilia Mauceri, Dusty Woods, Christoffer Heckman)</author>
      <guid isPermaLink="false">2505.12194v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization</title>
      <link>http://arxiv.org/abs/2505.12396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LGHRec的推荐系统框架，该框架通过利用大型语言模型（LLM）的Chain-of-Thought（CoT）推理能力来增强图神经网络（GNN）的推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现有的基于图的推荐系统依赖于稀疏的ID特征，未能充分利用文本信息，导致表示中的信息密度较低。此外，图对比学习面临挑战，包括随机负样本采样可能引入错误负样本，以及固定的温度系数无法适应不同节点的异质性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，本文旨在通过结合LLM的CoT推理能力和改进的对比学习策略来提升推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;LGHRec框架利用LLM的CoT推理能力生成语义ID，从而丰富推理过程并提高表示的信息密度和语义质量。此外，设计了一种名为Harmonized Group Policy Optimization（HGPO）的强化学习算法，用于优化对比学习中的负样本采样策略和温度系数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LGHRec通过LLM的CoT推理生成的语义ID提高了表示质量，并有效地通过HGPO提升了对比学习。该方法在多个基准模型中表现优异。&lt;h4&gt;结论&lt;/h4&gt;LGHRec通过结合LLM的CoT推理能力和HGPO算法，显著提高了推荐系统的性能和信息密度，为基于图的推荐系统提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduce LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have advanced recommender systems by modelinginteraction relationships. However, existing graph-based recommenders rely onsparse ID features and do not fully exploit textual information, resulting inlow information density within representations. Furthermore, graph contrastivelearning faces challenges. Random negative sampling can introduce falsenegative samples, while fixed temperature coefficients cannot adapt to theheterogeneity of different nodes. In addition, current efforts to enhancerecommendations with large language models (LLMs) have not fully utilized theirChain-of-Thought (CoT) reasoning capabilities to guide representation learning.To address these limitations, we introduces LGHRec (LLM-CoT Enhanced GraphNeural Recommendation with Harmonized Group Policy Optimization). Thisframework leverages the CoT reasoning ability of LLMs to generate semantic IDs,enriching reasoning processes and improving information density and semanticquality of representations. Moreover, we design a reinforcement learningalgorithm, Harmonized Group Policy Optimization (HGPO), to optimize negativesampling strategies and temperature coefficients in contrastive learning. Thisapproach enhances long-tail recommendation performance and ensures optimizationconsistency across different groups. Experimental results on three datasetsdemonstrate that LGHRec improves representation quality through semantic IDsgenerated by LLM's CoT reasoning and effectively boosts contrastive learningwith HGPO. Our method outperforms several baseline models. The code isavailable at: https://anonymous.4open.science/r/LLM-Rec.</description>
      <author>example@mail.com (Hailong Luo, Bin Wu, Hongyong Jia, Qingqing Zhu, Lianlei Shan)</author>
      <guid isPermaLink="false">2505.12396v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations</title>
      <link>http://arxiv.org/abs/2505.12237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次系统地研究了大型语言模型（LLMs）在视频编辑中的应用，提出了一种名为L-Storyboard的中间表示方法，用于将视频镜头转换为适合LLMs处理的结构化语言描述。此外，还提出了StoryFlow策略，以解决发散任务输出的不稳定性，并提高了视频编辑任务的解释性和隐私保护。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）和视觉-语言模型（VLMs）在视频理解方面表现出色，但在视频编辑中的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在视频编辑中的应用，并提高视频编辑任务的解释性和隐私保护。&lt;h4&gt;方法&lt;/h4&gt;引入L-Storyboard中间表示方法，将视频镜头转换为结构化语言描述；将视频编辑任务分为收敛任务和发散任务，并针对三个核心任务进行研究；提出StoryFlow策略以解决发散任务输出的不稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;L-Storyboard有助于将视觉信息与语言描述之间建立更稳健的映射；StoryFlow策略提高了发散任务输出的逻辑一致性和输出稳定性。&lt;h4&gt;结论&lt;/h4&gt;LLMs在智能视频编辑中具有巨大的潜力，L-Storyboard和StoryFlow策略可以显著提高视频编辑任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;This paper presents the first systematic study of LLMs in the context of video editing. To bridge the gap between visual information and language-based reasoning, we introduce L-Storyboard, an intermediate representation that transforms discrete video shots into structured language descriptions suitable for LLM processing. We categorize video editing tasks into Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot Attributes Classification, Next Shot Selection, and Shot Sequence Ordering. To address the inherent instability of divergent task outputs, we propose the StoryFlow strategy, which converts the divergent multi-path reasoning process into a convergent selection mechanism, effectively enhancing task accuracy and logical coherence. Experimental results demonstrate that L-Storyboard facilitates a more robust mapping between visual information and language descriptions, significantly improving the interpretability and privacy protection of video editing tasks. Furthermore, StoryFlow enhances the logical consistency and output stability in Shot Sequence Ordering, underscoring the substantial potential of LLMs in intelligent video editing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) and Vision-Language Models (VLMs) havedemonstrated remarkable reasoning and generalization capabilities in videounderstanding; however, their application in video editing remains largelyunderexplored. This paper presents the first systematic study of LLMs in thecontext of video editing. To bridge the gap between visual information andlanguage-based reasoning, we introduce L-Storyboard, an intermediaterepresentation that transforms discrete video shots into structured languagedescriptions suitable for LLM processing. We categorize video editing tasksinto Convergent Tasks and Divergent Tasks, focusing on three core tasks: ShotAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. Toaddress the inherent instability of divergent task outputs, we propose theStoryFlow strategy, which converts the divergent multi-path reasoning processinto a convergent selection mechanism, effectively enhancing task accuracy andlogical coherence. Experimental results demonstrate that L-Storyboardfacilitates a more robust mapping between visual information and languagedescriptions, significantly improving the interpretability and privacyprotection of video editing tasks. Furthermore, StoryFlow enhances the logicalconsistency and output stability in Shot Sequence Ordering, underscoring thesubstantial potential of LLMs in intelligent video editing.</description>
      <author>example@mail.com (Yuzhi Li, Haojun Xu, Fang Tian)</author>
      <guid isPermaLink="false">2505.12237v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations</title>
      <link>http://arxiv.org/abs/2505.12310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages,10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为DNOI-4DRO的新型学习-优化-结合四维雷达里程计模型，该模型通过创新的可微分神经网络优化迭代算子，将传统几何优化与端到端神经网络训练无缝结合。&lt;h4&gt;背景&lt;/h4&gt;在四维雷达里程计领域，需要提高雷达点云的表示能力，并提升定位精度。&lt;h4&gt;目的&lt;/h4&gt;提出DNOI-4DRO模型，旨在提升四维雷达里程计的性能。&lt;h4&gt;方法&lt;/h4&gt;模型首先使用神经网络估计点运动流，然后基于点运动与3D空间中姿态的关系构建成本函数，并使用高斯-牛顿更新来优化雷达姿态。此外，设计了一个双流四维雷达骨干网络，该网络集成了多尺度几何特征和基于聚类的类感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和Snail-Radar数据集上进行的实验表明，DNOI-4DRO模型的表现优于最近的一些经典和学习方法，甚至在使用激光雷达点云进行映射优化的情况下，其结果与A-LOAM相当。&lt;h4&gt;结论&lt;/h4&gt;DNOI-4DRO模型能够显著提高四维雷达里程计的性能，并且模型和代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;A novel learning-optimization-combined 4D radar odometry model, named DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates traditional geometric optimization with end-to-end neural network training, leveraging an innovative differentiable neural-optimization iteration operator. In this framework, point-wise motion flow is first estimated using a neural network, followed by the construction of a cost function based on the relationship between point motion and pose in 3D space. The radar pose is then refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D radar backbone that integrates multi-scale geometric features and clustering-based class-aware features to enhance the representation of sparse 4D radar point clouds. Extensive experiments on the VoD and Snail-Radar datasets demonstrate the superior performance of our model, which outperforms recent classical and learning-based approaches. Notably, our method even achieves results comparable to A-LOAM with mapping optimization using LiDAR point clouds as input. Our models and code will be publicly released.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A novel learning-optimization-combined 4D radar odometry model, namedDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integratestraditional geometric optimization with end-to-end neural network training,leveraging an innovative differentiable neural-optimization iteration operator.In this framework, point-wise motion flow is first estimated using a neuralnetwork, followed by the construction of a cost function based on therelationship between point motion and pose in 3D space. The radar pose is thenrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4Dradar backbone that integrates multi-scale geometric features andclustering-based class-aware features to enhance the representation of sparse4D radar point clouds. Extensive experiments on the VoD and Snail-Radardatasets demonstrate the superior performance of our model, which outperformsrecent classical and learning-based approaches. Notably, our method evenachieves results comparable to A-LOAM with mapping optimization using LiDARpoint clouds as input. Our models and code will be publicly released.</description>
      <author>example@mail.com (Shouyi Lu, Huanyu Zhou, Guirong Zhuo)</author>
      <guid isPermaLink="false">2505.12310v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph</title>
      <link>http://arxiv.org/abs/2505.12411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个通过重连异质图来提高图神经网络性能的方法，并证明了重连后图的同质性对节点分类性能的影响。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在分析图结构数据方面表现出色，但在异质图上表现不佳，因为连接的节点通常属于不同的类别。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供理论基础，并设计一种提高异质图同质性的重连框架，以改善图神经网络在节点分类任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;我们首先建立了边同质性与GNN嵌入平滑性和节点分类性能之间的联系，然后提出了一种使用参考图来增加图同质性的重连框架，并提出了一种从节点特征和训练标签构建同质参考图的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验，我们分析了原始图和参考图的同质性能如何影响重连图的同质性和下游GNN性能。&lt;h4&gt;结论&lt;/h4&gt;在11个真实世界的异质图数据集上评估我们的方法，结果显示它优于现有的重连技术和针对异质图的专用GNN，实现了更高的节点分类精度，同时保持了高效性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) excel at analyzing graph-structured data but struggle on heterophilic graphs, where connected nodes often belong to different classes. While this challenge is commonly addressed with specialized GNN architectures, graph rewiring remains an underexplored strategy in this context. We provide theoretical foundations linking edge homophily, GNN embedding smoothness, and node classification performance, motivating the need to enhance homophily. Building on this insight, we introduce a rewiring framework that increases graph homophily using a reference graph, with theoretical guarantees on the homophily of the rewired graph. To broaden applicability, we propose a label-driven diffusion approach for constructing a homophilic reference graph from node features and training labels. Through extensive simulations, we analyze how the homophily of both the original and reference graphs influences the rewired graph homophily and downstream GNN performance. We evaluate our method on 11 real-world heterophilic datasets and show that it outperforms existing rewiring techniques and specialized GNNs for heterophilic graphs, achieving improved node classification accuracy while remaining efficient and scalable to large graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) excel at analyzing graph-structured data butstruggle on heterophilic graphs, where connected nodes often belong todifferent classes. While this challenge is commonly addressed with specializedGNN architectures, graph rewiring remains an underexplored strategy in thiscontext. We provide theoretical foundations linking edge homophily, GNNembedding smoothness, and node classification performance, motivating the needto enhance homophily. Building on this insight, we introduce a rewiringframework that increases graph homophily using a reference graph, withtheoretical guarantees on the homophily of the rewired graph. To broadenapplicability, we propose a label-driven diffusion approach for constructing ahomophilic reference graph from node features and training labels. Throughextensive simulations, we analyze how the homophily of both the original andreference graphs influences the rewired graph homophily and downstream GNNperformance. We evaluate our method on 11 real-world heterophilic datasets andshow that it outperforms existing rewiring techniques and specialized GNNs forheterophilic graphs, achieving improved node classification accuracy whileremaining efficient and scalable to large graphs.</description>
      <author>example@mail.com (Harel Mendelman, Haggai Maron, Ronen Talmon)</author>
      <guid isPermaLink="false">2505.12411v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Causality-Inspired Robustness for Nonlinear Models via Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了分布鲁棒性在预测算法中的重要性，提出了一种在因果框架下的非线性方法，结合可识别表示学习的新进展，在非线性设置下建立了分布鲁棒性保证。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界数据中普遍存在分布偏移，分布鲁棒性成为预测算法的核心目标。&lt;h4&gt;目的&lt;/h4&gt;最小化预测模型在不确定性集（一类分布）中的最坏情况风险。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了可识别表示学习，在因果框架下建立了非线性设置下的分布鲁棒性保证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在非线性设置下提供了有限的鲁棒性保证，这是因果启发式鲁棒性方法首次在非线性设置中实现。&lt;h4&gt;结论&lt;/h4&gt;通过合成数据和真实世界单细胞数据的实证验证，证明了有限半径鲁棒性在分布鲁棒性中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;分布鲁棒性是预测算法的核心目标，因为现实世界数据普遍存在分布偏移。预测模型旨在最小化一类分布（即不确定性集）中的最坏情况风险。因果性提供了一个建模框架，在该框架下，不确定性集是数据驱动的，而不是像传统分布鲁棒性优化那样预先指定的。然而，当前的因果启发式鲁棒性方法仅在线性设置中具有有限的鲁棒性保证，其中协变量和响应之间的因果关系是线性的。在这项工作中，我们通过结合可识别表示学习的新进展，在因果框架下提出了一种非线性方法，并建立了分布鲁棒性保证。据我们所知，这是第一个在非线性设置下具有这种有限鲁棒性保证的因果启发式鲁棒性方法。对理论发现进行了合成数据和真实世界单细胞数据的实证验证，同时也说明了有限半径鲁棒性是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributional robustness is a central goal of prediction algorithms due tothe prevalent distribution shifts in real-world data. The prediction model aimsto minimize the worst-case risk among a class of distributions, a.k.a., anuncertainty set. Causality provides a modeling framework with a rigorousrobustness guarantee in the above sense, where the uncertainty set isdata-driven rather than pre-specified as in traditional distributionalrobustness optimization. However, current causality-inspired robustness methodspossess finite-radius robustness guarantees only in the linear settings, wherethe causal relationships among the covariates and the response are linear. Inthis work, we propose a nonlinear method under a causal framework byincorporating recent developments in identifiable representation learning andestablish a distributional robustness guarantee. To our best knowledge, this isthe first causality-inspired robustness method with such a finite-radiusrobustness guarantee in nonlinear settings. Empirical validation of thetheoretical findings is conducted on both synthetic data and real-worldsingle-cell data, also illustrating that finite-radius robustness is crucial.</description>
      <author>example@mail.com (Marin Šola, Peter Bühlmann, Xinwei Shen)</author>
      <guid isPermaLink="false">2505.12868v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
      <link>http://arxiv.org/abs/2505.12684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的去中心化图基础模型（GFM）训练范式FedGFM+，以解决知识纠缠问题，并通过实验验证了其在多个领域的基准测试中的优越性。&lt;h4&gt;背景&lt;/h4&gt;图机器学习近年来转向以数据为中心的范式，其中联邦图学习（FGL）和图基础模型（GFM）是两个新兴领域。FGL虽然支持多客户端协作，但面临数据和工作异质性的挑战；GFM则通常在单机上训练，无法利用跨部门的资源和数据。&lt;h4&gt;目的&lt;/h4&gt;提出FedGFM+框架，旨在减少知识纠缠，提高模型在不同领域的适应能力。&lt;h4&gt;方法&lt;/h4&gt;FedGFM+包括两个核心模块：(1) AncDAI：基于全局锚点的领域感知初始化策略，通过将局部图编码为领域特定的原型，并在这些原型周围初始化全局模型；(2) AdaDPP：局部自适应领域敏感提示池，在预训练期间学习轻量级图提示，并在微调期间选择相关提示来增强目标图属性。&lt;h4&gt;主要发现&lt;/h4&gt;FedGFM+在8个跨多个领域和任务的基准测试中表现优于20个基线方法。&lt;h4&gt;结论&lt;/h4&gt;FedGFM+通过减少知识纠缠，显著提高了图基础模型的跨领域泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in graph machine learning have shifted to data-centricparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)enables multi-client collaboration but faces challenges from data and taskheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)offer strong domain generalization but are usually trained on single machines,missing out on cross-silo data and resources.  These paradigms are complementary, and their integration brings notablebenefits. Motivated by this, we propose FedGFM, a novel decentralized GFMtraining paradigm. However, a key challenge is knowledge entanglement, wheremulti-domain knowledge merges into indistinguishable representations, hinderingdownstream adaptation.  To address this, we present FedGFM+, an enhanced framework with two coremodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-baseddomain-aware initialization strategy. Before pre-training, each client encodesits local graph into domain-specific prototypes that serve as semantic anchors.Synthetic embeddings around these anchors initialize the global model. Wetheoretically prove these prototypes are distinguishable across domains,providing a strong inductive bias to disentangle domain-specific knowledge. (2)AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns alightweight graph prompt capturing domain semantics during pre-training. Duringfine-tuning, prompts from all clients form a pool from which the GFM selectsrelevant prompts to augment target graph attributes, improving downstreamadaptation.  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains andtasks, outperforming 20 baselines from supervised learning, FGL, and federatedGFM variants.</description>
      <author>example@mail.com (Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu)</author>
      <guid isPermaLink="false">2505.12684v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TinyRS-R1: Compact Multimodal Language Model for Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.12099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to BMVC 2025. Code, models, and the captions for datasets  will be released&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TinyRS，一个针对遥感任务优化的2B参数多模态小型语言模型（MSLM），以及其推理增强变体TinyRS-R1。TinyRS在多个遥感任务中达到或超过了7B参数模型的性能，同时内存和延迟需求仅为后者的三分之一。&lt;h4&gt;背景&lt;/h4&gt;遥感应用通常运行在无法承载当前7B参数多模态语言模型的边缘硬件上。&lt;h4&gt;目的&lt;/h4&gt;提出TinyRS和TinyRS-R1，旨在为遥感任务提供高效、低资源消耗的多模态语言模型。&lt;h4&gt;方法&lt;/h4&gt;TinyRS基于Qwen2-VL-2B模型，通过四个阶段的流水线进行训练：在百万卫星图像上进行预训练，在视觉指令示例上进行指令调整，使用推理数据集的Chain-of-Thought（CoT）注释进行微调，并通过Group Relative Policy Optimization（GRPO）进行对齐。&lt;h4&gt;主要发现&lt;/h4&gt;TinyRS-R1在分类、视觉问答（VQA）、视觉基础和开放式问答等任务中，性能达到或超过了7B参数的遥感模型。CoT推理显著提高了空间定位和场景理解能力，而无需推理的TinyRS在简洁、延迟敏感的VQA任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;TinyRS-R1是第一个具有GRPO对齐CoT推理的领域专用MSLM，适用于通用遥感。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces TinyRS, the first 2B-parameter multimodal small language model (MSLM) optimized for remote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built upon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training on million satellite images, instruction tuning on visual instruction examples, fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning dataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1 achieves or surpasses the performance of recent 7B-parameter remote sensing models across classification, VQA, visual grounding, and open-ended question answering-while requiring just one-third of the memory and latency. Our analysis shows that CoT reasoning substantially benefits spatial grounding and scene understanding, while the non-reasoning TinyRS excels in concise, latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized MSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote-sensing applications often run on edge hardware that cannot hosttoday's 7B-parameter multimodal language models. This paper introduces TinyRS,the first 2B-parameter multimodal small language model (MSLM) optimized forremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Builtupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-trainingon million satellite images, instruction tuning on visual instruction examples,fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoningdataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1achieves or surpasses the performance of recent 7B-parameter remote sensingmodels across classification, VQA, visual grounding, and open-ended questionanswering-while requiring just one-third of the memory and latency. Ouranalysis shows that CoT reasoning substantially benefits spatial grounding andscene understanding, while the non-reasoning TinyRS excels in concise,latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specializedMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.</description>
      <author>example@mail.com (Aybora Koksal, A. Aydin Alatan)</author>
      <guid isPermaLink="false">2505.12099v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Not All Documents Are What You Need for Extracting Instruction Tuning Data</title>
      <link>http://arxiv.org/abs/2505.12250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EQUAL的数据提取框架，用于从包含丰富和多样化知识的网络语料库中提取指令微调数据，以解决当前指令数据多样性和适用性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;指令微调能提升大型语言模型（LLMs）的性能，但其依赖于高质量的训练数据。现有的LLMs合成指令数据方法存在多样性和适用性不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的数据提取框架，以减少计算成本并提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;EQUAL框架首先基于对比学习得到的嵌入对文档语料库进行聚类，然后采用多臂老虎机策略高效识别可能含有有价值问答对（QA）对的聚类，通过迭代交替进行文档选择和高质量的QA对提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EQUAL在AutoMathText和StackOverflow上的四个下游任务中，相比LLaMA-3.1-8B和Mistral-7B，降低了5-10倍的计算成本，并提高了2.5%的准确性。&lt;h4&gt;结论&lt;/h4&gt;EQUAL框架能有效减少指令微调的数据提取成本，并提升模型性能，在现实世界场景中具有潜在应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Instruction tuning improves the performance of large language models (LLMs),but it heavily relies on high-quality training data. Recently, LLMs have beenused to synthesize instruction data using seed question-answer (QA) pairs.However, these synthesized instructions often lack diversity and tend to besimilar to the input seeds, limiting their applicability in real-worldscenarios. To address this, we propose extracting instruction tuning data fromweb corpora that contain rich and diverse knowledge. A naive solution is toretrieve domain-specific documents and extract all QA pairs from them, but thisfaces two key challenges: (1) extracting all QA pairs using LLMs isprohibitively expensive, and (2) many extracted QA pairs may be irrelevant tothe downstream tasks, potentially degrading model performance. To tackle theseissues, we introduce EQUAL, an effective and scalable data extraction frameworkthat iteratively alternates between document selection and high-quality QA pairextraction to enhance instruction tuning. EQUAL first clusters the documentcorpus based on embeddings derived from contrastive learning, then uses amulti-armed bandit strategy to efficiently identify clusters that are likely tocontain valuable QA pairs. This iterative approach significantly reducescomputational cost while boosting model performance. Experiments onAutoMathText and StackOverflow across four downstream tasks show that EQUALreduces computational costs by 5-10x and improves accuracy by 2.5 percent onLLaMA-3.1-8B and Mistral-7B</description>
      <author>example@mail.com (Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao)</author>
      <guid isPermaLink="false">2505.12250v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration</title>
      <link>http://arxiv.org/abs/2505.11895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了统一多模态编码器在对抗攻击下的鲁棒性问题，提出了一种对抗校准框架来提高模型在不同模态下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的统一多模态编码器在跨模态任务中表现出色，但其对抗鲁棒性在安全敏感应用中仍是一个重要问题。&lt;h4&gt;目的&lt;/h4&gt;对统一多模态编码器的对抗鲁棒性进行全面研究，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种对抗校准框架，通过训练特定于模态的投影头，提高模型对不同模态数据的鲁棒性，同时保持预训练编码器和语义中心不变。&lt;h4&gt;主要发现&lt;/h4&gt;轻微的对抗扰动会导致所有模态的性能显著下降，尤其是非视觉输入如音频和点云，视觉输入如图像和视频也显著退化。&lt;h4&gt;结论&lt;/h4&gt;该方法在ε=4/255的情况下提高了47.3%的对抗鲁棒性，同时保持了或提高了清洁零样本和检索性能，且所需的训练参数不到1%。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we studied the adversarial robustness of unified multi-modal encoders and proposed an adversarial calibration framework to improve the robustness of models across different modalities.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent unified multi-modal encoders align a wide range of modalities into ashared representation space, enabling diverse cross-modal tasks. Despite theirimpressive capabilities, the robustness of these models under adversarialperturbations remains underexplored, which is a critical concern forsafety-sensitive applications. In this work, we present the first comprehensivestudy of adversarial vulnerability in unified multi-modal encoders. We findthat even mild adversarial perturbations lead to substantial performance dropsacross all modalities. Non-visual inputs, such as audio and point clouds, areespecially fragile, while visual inputs like images and videos also degradesignificantly. To address this, we propose an efficient adversarial calibrationframework that improves robustness across modalities without modifyingpretrained encoders or semantic centers, ensuring compatibility with existingfoundation models. Our method introduces modality-specific projection headstrained solely on adversarial examples, while keeping the backbone andembeddings frozen. We explore three training objectives: fixed-centercross-entropy, clean-to-adversarial L2 alignment, and clean-adversarialInfoNCE, and we introduce a regularization strategy to ensuremodality-consistent alignment under attack. Experiments on six modalities andthree Bind-style models show that our method improves adversarial robustness byup to 47.3 percent at epsilon = 4/255, while preserving or even improving cleanzero-shot and retrieval performance with less than 1 percent trainableparameters.</description>
      <author>example@mail.com (Chih-Ting Liao, Bin Ren, Guofeng Mei, Xu Zheng)</author>
      <guid isPermaLink="false">2505.11895v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
      <link>http://arxiv.org/abs/2505.12638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ChromFound，这是一个针对scATAC-seq设计的基石模型，用于解析调控机制，并展示了其在细胞类型注释和跨组学预测中的优异性能。&lt;h4&gt;背景&lt;/h4&gt;scATAC-seq技术的出现为解析调控机制提供了新的视角，但目前缺乏支持零样本高质细胞识别和综合多组学分析的基石模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于scATAC-seq的基石模型，实现零样本高质细胞识别和综合多组学分析。&lt;h4&gt;方法&lt;/h4&gt;ChromFound利用混合架构和基因组感知分词，有效捕捉基因组全局长上下文和调控信号。该模型在来自30个组织和6种疾病条件的1.97百万个细胞上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;ChromFound在6个不同的任务中展示了广泛的应用性，实现了零样本的稳健性能，并在细胞类型注释和跨组学预测中表现出良好的迁移性。它还揭示了现有计算方法未检测到的增强子-基因联系。&lt;h4&gt;结论&lt;/h4&gt;ChromFound为理解非编码基因组中的疾病风险变异提供了一个有前景的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of single-cell Assay for Transposase-Accessible Chromatin usingsequencing (scATAC-seq) offers an innovative perspective for decipheringregulatory mechanisms by assembling a vast repository of single-cell chromatinaccessibility data. While foundation models have achieved significant successin single-cell transcriptomics, there is currently no foundation model forscATAC-seq that supports zero-shot high-quality cell identification andcomprehensive multi-omics analysis simultaneously. Key challenges lie in thehigh dimensionality and sparsity of scATAC-seq data, as well as the lack of astandardized schema for representing open chromatin regions (OCRs). Here, wepresent \textbf{ChromFound}, a foundation model tailored for scATAC-seq.ChromFound utilizes a hybrid architecture and genome-aware tokenization toeffectively capture genome-wide long contexts and regulatory signals fromdynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissuesand 6 disease conditions, ChromFound demonstrates broad applicability across 6diverse tasks. Notably, it achieves robust zero-shot performance in generatinguniversal cell representations and exhibits excellent transferability in celltype annotation and cross-omics prediction. By uncovering enhancer-gene linksundetected by existing computational methods, ChromFound offers a promisingframework for understanding disease risk variants in the noncoding genome.</description>
      <author>example@mail.com (Yifeng Jiao, Yuchen Liu, Yu Zhang, Xin Guo, Yushuai Wu, Chen Jiang, Jiyang Li, Hongwei Zhang, Limei Han, Xin Gao, Yuan Qi, Yuan Cheng)</author>
      <guid isPermaLink="false">2505.12638v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges</title>
      <link>http://arxiv.org/abs/2505.11618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种层次化的时空推理基准STARK，用于评估大型语言模型（LLMs）和大型推理模型（LRMs）在时空推理能力上的表现。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型和大型推理模型在技术上取得了进展，但它们在处理复杂时空信号方面的能力仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs和LRMs在时空推理上的能力，特别是在状态估计、时空关系推理和结合领域知识的推理任务上。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含26个不同时空任务的基准，涉及14,552个挑战，并评估了3个LRMs和8个LLMs的表现。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs在需要几何推理的任务上表现有限，LRMs则在各种难度级别的任务上表现出鲁棒性，有时甚至超过了基于第一原理的传统方法。在需要世界知识的推理任务中，LLMs和LRMs的性能差距缩小，一些LLMs甚至超过了LRMs。LRM o3模型在所有评估任务中继续表现领先，这主要归因于推理模型规模更大。&lt;h4&gt;结论&lt;/h4&gt;STARK为智能CPS的未来创新提供了结构化框架，有助于识别LLMs和LRMs在时空推理上的局限性，并促进模型架构和推理范式的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).Despite advances in Large Language Models (LLMs) and Large Reasoning Models(LRMs), their capacity to reason about complex spatiotemporal signals remainsunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoningbenchmaRK, STARK, to systematically evaluate LLMs across three levels ofreasoning complexity: state estimation (e.g., predicting field variables,localizing and tracking events in space and time), spatiotemporal reasoningover states (e.g., inferring spatial-temporal relationships), andworld-knowledge-aware reasoning that integrates contextual and domain knowledge(e.g., intent prediction, landmark-aware navigation). We curate 26 distinctspatiotemporal tasks with diverse sensor modalities, comprising 14,552challenges where models answer directly or by Python Code Interpreter.Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasksrequiring geometric reasoning (e.g., multilateration or triangulation),particularly as complexity increases. Surprisingly, LRMs show robustperformance across tasks with various levels of difficulty, often competing orsurpassing traditional first-principle-based methods. Our results show that inreasoning tasks requiring world knowledge, the performance gap between LLMs andLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 modelcontinues to achieve leading performance across all evaluated tasks, a resultattributed primarily to the larger size of the reasoning models. STARKmotivates future innovations in model architectures and reasoning paradigms forintelligent CPS by providing a structured framework to identify limitations inthe spatiotemporal reasoning of LLMs and LRMs.</description>
      <author>example@mail.com (Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava)</author>
      <guid isPermaLink="false">2505.11618v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training</title>
      <link>http://arxiv.org/abs/2505.12236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures, Appear on IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TKRE（两阶段知识引导预训练关系抽取）是一个将大型语言模型与传统关系抽取模型相结合的框架，旨在解决Few-Shot Relation Extraction（FSRE）中的数据稀缺和模型泛化能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;Few-Shot Relation Extraction（FSRE）由于标注数据稀缺和现有模型泛化能力有限，是一个具有挑战性的任务。尽管大型语言模型（LLMs）通过上下文学习（ICL）在FSRE中展现出潜力，但它们的通用训练目标通常导致特定任务关系抽取的性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出TKRE框架，以克服FSRE中的挑战，提高关系抽取的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;TKRE框架包含两个关键创新：（1）利用LLMs生成解释驱动的知识和方案约束的合成数据，解决数据稀缺问题；（2）采用两阶段预训练策略，结合Masked Span Language Modeling（MSLM）和Span-Level Contrastive Learning（SCL）来增强关系推理和泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的综合实验表明，TKRE在FSRE中实现了新的最先进性能，证明了其在低资源场景中更广泛应用的潜力。&lt;h4&gt;结论&lt;/h4&gt;TKRE框架有效解决了FSRE任务中的挑战，有望在低资源场景中得到更广泛的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/uestc-gqj/tkre&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot Relation Extraction (FSRE) remains a challenging task due to thescarcity of annotated data and the limited generalization capabilities ofexisting models. Although large language models (LLMs) have demonstratedpotential in FSRE through in-context learning (ICL), their general-purposetraining objectives often result in suboptimal performance for task-specificrelation extraction. To overcome these challenges, we propose TKRE (Two-StageKnowledge-Guided Pre-training for Relation Extraction), a novel framework thatsynergistically integrates LLMs with traditional relation extraction models,bridging generative and discriminative learning paradigms. TKRE introduces twokey innovations: (1) leveraging LLMs to generate explanation-driven knowledgeand schema-constrained synthetic data, addressing the issue of data scarcity;and (2) a two-stage pre-training strategy combining Masked Span LanguageModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relationalreasoning and generalization. Together, these components enable TKRE toeffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasetsdemonstrate the efficacy of TKRE, achieving new state-of-the-art performance inFSRE and underscoring its potential for broader application in low-resourcescenarios. \footnote{The code and data are released onhttps://github.com/UESTC-GQJ/TKRE.</description>
      <author>example@mail.com (Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao)</author>
      <guid isPermaLink="false">2505.12236v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>GraphFLEx: Structure Learning Framework for Large Expanding Graphs</title>
      <link>http://arxiv.org/abs/2505.12323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphFLEx是一个用于大规模和动态扩展图上的图结构学习框架，通过限制边形成只针对通过聚类和细化技术确定的与结构相关的节点子集，以降低可扩展性瓶颈，实现高效的增量图更新。&lt;h4&gt;背景&lt;/h4&gt;图结构学习是图机器学习中的核心问题，对于揭示潜在关系和确保模型可解释性至关重要。然而，大多数现有方法不适合大规模和动态变化的图，因为它们通常需要在新节点到来时重新学习结构，并带来大量的计算和内存成本。&lt;h4&gt;目的&lt;/h4&gt;提出GraphFLEx，旨在解决大规模和动态扩展图上的图结构学习问题，以提高可扩展性和效率。&lt;h4&gt;方法&lt;/h4&gt;GraphFLEx通过结合聚类和细化技术来识别与结构相关的节点子集，从而限制边形成。框架支持48种灵活配置，通过集成不同的学习范式、细化策略和聚类方法，以适应广泛的图设置和学习目标。&lt;h4&gt;主要发现&lt;/h4&gt;在26个不同的数据集和图神经网络架构上进行的广泛实验表明，GraphFLEx实现了最先进的性能，并显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GraphFLEx框架在提高大规模和动态图上的图结构学习效率和可扩展性方面取得了显著成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph structure learning is a core problem in graph-based machine learning,essential for uncovering latent relationships and ensuring modelinterpretability. However, most existing approaches are ill-suited forlarge-scale and dynamically evolving graphs, as they often require completere-learning of the structure upon the arrival of new nodes and incursubstantial computational and memory costs. In this work, we propose GraphFLEx:a unified and scalable framework for Graph Structure Learning in Large andExpanding Graphs. GraphFLEx mitigates the scalability bottlenecks byrestricting edge formation to structurally relevant subsets of nodes identifiedthrough a combination of clustering and coarsening techniques. Thisdramatically reduces the search space and enables efficient, incremental graphupdates. The framework supports 48 flexible configurations by integratingdiverse choices of learning paradigms, coarsening strategies, and clusteringmethods, making it adaptable to a wide range of graph settings and learningobjectives. Extensive experiments across 26 diverse datasets and Graph NeuralNetwork architectures demonstrate that GraphFLEx achieves state-of-the-artperformance with significantly improved scalability.</description>
      <author>example@mail.com (Mohit Kataria, Nikita Malik, Sandeep Kumar, Jayadeva)</author>
      <guid isPermaLink="false">2505.12323v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics</title>
      <link>http://arxiv.org/abs/2505.12583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCAI 2025 Survey Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了机器人控制方法，以减轻由FMRs（基于基础模型的机器人）在物理世界中的行动带来的风险。&lt;h4&gt;背景&lt;/h4&gt;FMRs在通用技能方面有显著提升，但它们与物理世界的交互直接关系到人类和周围对象的安全。&lt;h4&gt;目的&lt;/h4&gt;全面总结FMRs从部署前到事故后的整个生命周期中减轻物理风险的控制方法。&lt;h4&gt;方法&lt;/h4&gt;将时间线分为三个阶段：部署前阶段、事故前阶段和事故后阶段，并分析了每个阶段的风险缓解策略。&lt;h4&gt;主要发现&lt;/h4&gt;发现了在事故前风险缓解策略、与人类物理交互假设的研究以及基础模型本身的基本问题等方面的研究空间。&lt;h4&gt;结论&lt;/h4&gt;希望该综述成为提供高分辨率分析FMRs物理风险及其控制的一个里程碑，有助于实现良好的人机关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Foundation Model-enabled robotics (FMRs) display greatly improvedgeneral-purpose skills, enabling more adaptable automation than conventionalrobotics. Their ability to handle diverse tasks thus creates new opportunitiesto replace human labor. However, unlike general foundation models, FMRsinteract with the physical world, where their actions directly affect thesafety of humans and surrounding objects, requiring careful deployment andcontrol. Based on this proposition, our survey comprehensively summarizes robotcontrol approaches to mitigate physical risks by covering all the lifespan ofFMRs ranging from pre-deployment to post-accident stage. Specifically, webroadly divide the timeline into the following three phases: (1) pre-deploymentphase, (2) pre-incident phase, and (3) post-incident phase. Throughout thissurvey, we find that there is much room to study (i) pre-incident riskmitigation strategies, (ii) research that assumes physical interaction withhumans, and (iii) essential issues of foundation models themselves. We hopethat this survey will be a milestone in providing a high-resolution analysis ofthe physical risks of FMRs and their control, contributing to the realizationof a good human-robot relationship.</description>
      <author>example@mail.com (Takeshi Kojima, Yaonan Zhu, Yusuke Iwasawa, Toshinori Kitamura, Gang Yan, Shu Morikuni, Ryosuke Takanami, Alfredo Solano, Tatsuya Matsushima, Akiko Murakami, Yutaka Matsuo)</author>
      <guid isPermaLink="false">2505.12583v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Depth Transfer: Learning to See Like a Simulator for Real-World Drone Navigation</title>
      <link>http://arxiv.org/abs/2505.12428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于域适应的深度迁移方法，用于解决机器人强化学习中模拟与现实之间的视觉差距问题。&lt;h4&gt;背景&lt;/h4&gt;在机器人强化学习中，模拟与现实之间的差异会严重影响策略性能，尤其是当输入是高维数据，如密集的深度估计时。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来桥接模拟和现实世界深度数据之间的视觉差距。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个变分自编码器（VAE）将模拟中的真实深度图像编码到潜在空间，然后作为强化学习策略的输入。在部署期间，编码器被进一步优化以使立体深度图像与这个潜在空间对齐，从而实现直接策略迁移而不需要微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在从真实深度输入切换到立体深度输入时，将障碍物避免的成功率提高了近一倍。此外，使用仅由IsaacGym生成的立体数据，该方法成功迁移到逼真的模拟器AvoidBench，并比最先进的基线实现了更好的性能。&lt;h4&gt;结论&lt;/h4&gt;在室内和室外环境中的实际评估证实了该方法的有效性，使基于深度的导航在多个领域具有鲁棒性和可推广性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Sim-to-real transfer is a fundamental challenge in robot reinforcement learning. Discrepancies between simulation and reality can significantly impair policy performance, especially if it receives high-dimensional inputs such as dense depth estimates from vision. We propose a novel depth transfer method based on domain adaptation to bridge the visual gap between simulated and real-world depth data. A Variational Autoencoder (VAE) is first trained to encode ground-truth depth images from simulation into a latent space, which serves as input to a reinforcement learning (RL) policy. During deployment, the encoder is refined to align stereo depth images with this latent space, enabling direct policy transfer without fine-tuning. We apply our method to the task of autonomous drone navigation through cluttered environments. Experiments in IsaacGym show that our method nearly doubles the obstacle avoidance success rate when switching from ground-truth to stereo depth input. Furthermore, we demonstrate successful transfer to the photo-realistic simulator AvoidBench using only IsaacGym-generated stereo data, achieving superior performance compared to state-of-the-art baselines. Real-world evaluations in both indoor and outdoor environments confirm the effectiveness of our approach, enabling robust and generalizable depth-based navigation across diverse domains.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sim-to-real transfer is a fundamental challenge in robot reinforcementlearning. Discrepancies between simulation and reality can significantly impairpolicy performance, especially if it receives high-dimensional inputs such asdense depth estimates from vision. We propose a novel depth transfer methodbased on domain adaptation to bridge the visual gap between simulated andreal-world depth data. A Variational Autoencoder (VAE) is first trained toencode ground-truth depth images from simulation into a latent space, whichserves as input to a reinforcement learning (RL) policy. During deployment, theencoder is refined to align stereo depth images with this latent space,enabling direct policy transfer without fine-tuning. We apply our method to thetask of autonomous drone navigation through cluttered environments. Experimentsin IsaacGym show that our method nearly doubles the obstacle avoidance successrate when switching from ground-truth to stereo depth input. Furthermore, wedemonstrate successful transfer to the photo-realistic simulator AvoidBenchusing only IsaacGym-generated stereo data, achieving superior performancecompared to state-of-the-art baselines. Real-world evaluations in both indoorand outdoor environments confirm the effectiveness of our approach, enablingrobust and generalizable depth-based navigation across diverse domains.</description>
      <author>example@mail.com (Hang Yu, Christophe De Wagter, Guido C. H. E de Croon)</author>
      <guid isPermaLink="false">2505.12428v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods</title>
      <link>http://arxiv.org/abs/2505.12132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, International Workshop on ADVANCEs in ICT  Infrastructures and Services, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要讨论了6G移动网络作为5G之后的演进步骤，预测了移动流量的爆炸性增长，并强调了其在低延迟、高数据速率、高设备密度和广泛覆盖等方面的优势。同时，摘要指出了在电信行业新系统中节能的重要性，并提到了网络切片作为6G/5G移动网络及其他新系统（如物联网、车联网和工业物联网）的基础性使能技术。然而，网络切片架构中嵌入的节能方法仍是一个研究空白。&lt;h4&gt;背景&lt;/h4&gt;6G移动网络预计将带来移动流量的爆炸性增长，提供超低延迟、高数据速率、高设备密度和广泛覆盖，对各个领域的服务产生积极影响。电信行业的新系统节能是主要关注点，因为所有参与者都期望减少碳足迹以减轻气候变化。&lt;h4&gt;目的&lt;/h4&gt;研究如何将节能方法嵌入网络切片架构中，这是全球几乎所有新创新系统的基础性使能技术。&lt;h4&gt;方法&lt;/h4&gt;通过在NS架构中部署ML-native代理，根据用户需求动态编排和优化资源，以实现节能。&lt;h4&gt;主要发现&lt;/h4&gt;论文提出了在SFI2网络切片参考架构中使用对比学习来改善资源分配的节能效果。&lt;h4&gt;结论&lt;/h4&gt;论文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了6G移动网络作为5G之后的演进步骤，预测了移动流量的爆炸性增长，并强调了其在低延迟、高数据速率、高设备密度和广泛覆盖等方面的优势。同时，摘要指出了在电信行业新系统中节能的重要性，并提到了网络切片作为6G/5G移动网络及其他新系统（如物联网、车联网和工业物联网）的基础性使能技术。然而，网络切片架构中嵌入的节能方法仍是一个研究空白。本文讨论了如何将节能方法嵌入网络切片架构中，这是全球几乎所有新创新系统的基础性使能技术。本文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。在SFI2网络切片参考架构中，本文提出了使用对比学习来改善资源分配的节能效果。本文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5281/zenodo.15449843&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6G mobile network is the next evolutionary step after 5G, with aprediction of an explosive surge in mobile traffic. It provides ultra-lowlatency, higher data rates, high device density, and ubiquitous coverage,positively impacting services in various areas. Energy saving is a majorconcern for new systems in the telecommunications sector because all playersare expected to reduce their carbon footprints to contribute to mitigatingclimate change. Network slicing is a fundamental enabler for 6G/5G mobilenetworks and various other new systems, such as the Internet of Things (IoT),Internet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-savingmethods embedded in network slicing architectures are still a research gap.This paper discusses how to embed energy-saving methods in network-slicingarchitectures that are a fundamental enabler for nearly all new innovativesystems being deployed worldwide. This paper's main contribution is a proposalto save energy in network slicing. That is achieved by deploying ML-nativeagents in NS architectures to dynamically orchestrate and optimize resourcesbased on user demands. The SFI2 network slicing reference architecture is theconcrete use case scenario in which contrastive learning improves energy savingfor resource allocation.</description>
      <author>example@mail.com (Rodrigo Moreira, Tereza C. M. Carvalho, Flávio de Oliveira Silva, Nazim Agoulmine, Joberto S. B. Martins)</author>
      <guid isPermaLink="false">2505.12132v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Visual Generalization in Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.11719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在机器人学习中，训练能够在不同视觉环境中稳健操作的视觉操作策略是一个重要且未解决的挑战。作者提出了一种扩展的解耦表示学习和关联记忆方法，将其应用于更复杂和动态的操作任务，并展示了在模拟和真实硬件上的零样本适应能力。此外，该方法在模仿学习方面也取得了显著成果，并提出了一种新的技术，使策略对二维平面旋转具有不变性。&lt;h4&gt;背景&lt;/h4&gt;当前方法通常通过依赖不变表示（如点云和深度）或通过视觉域随机化和/或大型、视觉多样化的数据集来强制推广，以避免稳健性问题。&lt;h4&gt;目的&lt;/h4&gt;目标是扩展解耦表示学习和关联记忆，使其适用于更复杂和动态的操作任务，并展示其零样本适应能力。&lt;h4&gt;方法&lt;/h4&gt;方法包括扩展解耦表示学习和关联记忆，应用于复杂任务，并引入了一种新的技术，使策略对二维平面旋转具有不变性。&lt;h4&gt;主要发现&lt;/h4&gt;主要发现是该方法在模拟和真实硬件上的零样本适应能力，以及在模仿学习方面的显著成果。&lt;h4&gt;结论&lt;/h4&gt;结论是这项工作标志着向实现不仅易于适应，而且对现实世界部署的复杂性和动态性质具有鲁棒性的操作策略的重大步骤。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在机器人学习中，训练能够在多种视觉环境中稳健的视觉操作策略仍然是一个重要且未解决的挑战。当前的方法通常通过依赖不变表示（如点云和深度）或通过视觉域随机化和/或大型、视觉多样化的数据集来回避这个问题。解耦表示学习——尤其是当与联想记忆原则相结合时——最近显示出使基于视觉的强化学习策略能够对视觉分布变化具有鲁棒性的希望。然而，这些技术主要局限于更简单的基准和玩具环境。在这项工作中，我们将解耦表示学习和联想记忆扩展到更视觉和动态复杂的操作任务中，并在模拟和真实硬件上展示了零样本对视觉扰动的适应性。我们进一步将这种方法扩展到模仿学习，特别是扩散策略，并通过实验与最先进的模仿学习方法相比，显示出显著的视觉泛化增益。最后，我们介绍了一种从模型等变性文献中借鉴的新技术，该技术将任何训练好的神经网络策略转换为对二维平面旋转具有不变性的策略，使我们的策略不仅对视觉具有鲁棒性，而且对某些相机扰动具有弹性。我们认为这项工作标志着向实现不仅易于适应，而且对现实世界部署的复杂性和动态性质具有鲁棒性的操作策略的重大步骤。补充视频可在https://sites.google.com/view/vis-gen-robotics/home上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training vision-based manipulation policies that are robust across diversevisual environments remains an important and unresolved challenge in robotlearning. Current approaches often sidestep the problem by relying on invariantrepresentations such as point clouds and depth, or by brute-forcinggeneralization through visual domain randomization and/or large, visuallydiverse datasets. Disentangled representation learning - especially whencombined with principles of associative memory - has recently shown promise inenabling vision-based reinforcement learning policies to be robust to visualdistribution shifts. However, these techniques have largely been constrained tosimpler benchmarks and toy environments. In this work, we scale disentangledrepresentation learning and associative memory to more visually and dynamicallycomplex manipulation tasks and demonstrate zero-shot adaptability to visualperturbations in both simulation and on real hardware. We further extend thisapproach to imitation learning, specifically Diffusion Policy, and empiricallyshow significant gains in visual generalization compared to state-of-the-artimitation learning methods. Finally, we introduce a novel technique adaptedfrom the model equivariance literature that transforms any trained neuralnetwork policy into one invariant to 2D planar rotations, making our policy notonly visually robust but also resilient to certain camera perturbations. Webelieve that this work marks a significant step towards manipulation policiesthat are not only adaptable out of the box, but also robust to the complexitiesand dynamical nature of real-world deployment. Supplementary videos areavailable at https://sites.google.com/view/vis-gen-robotics/home.</description>
      <author>example@mail.com (Sumeet Batra, Gaurav Sukhatme)</author>
      <guid isPermaLink="false">2505.11719v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Prompt-driven Community Search</title>
      <link>http://arxiv.org/abs/2505.12304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Pre-trained Prompt-driven Community Search (PPCS)的新模型，用于半监督社区搜索，旨在提高搜索准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;现有的半监督社区检测算法大多基于已知的社区进行检测，但检测到的社区通常不包含查询节点，不适合用于搜索给定节点的社区。&lt;h4&gt;目的&lt;/h4&gt;将“预训练，提示”范式应用于半监督社区搜索，提出PPCS模型，以增强搜索准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;PPCS由三个主要组件组成：节点编码、样本生成和提示驱动微调。节点编码组件使用图神经网络学习图中节点的局部结构模式；样本生成组件为给定节点识别初始社区，并选择与初始社区结构相似的已知社区作为训练样本；提示驱动微调组件利用这些样本作为提示来指导最终的社区预测。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的实验结果表明，PPCS的性能优于基线算法，且在社区搜索效率上高于半监督社区搜索基线方法。消融研究表明，PPCS的每个组件都是有效的。&lt;h4&gt;结论&lt;/h4&gt;PPCS模型在半监督社区搜索任务中表现出色，提高了搜索的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The "pre-train, prompt" paradigm is widely adopted in various graph-basedtasks and has shown promising performance in community detection. Most existingsemi-supervised community detection algorithms detect communities based onknown ones, and the detected communities typically do not contain the givenquery node. Therefore, they are not suitable for searching the community of agiven node. Motivated by this, we adopt this paradigm into the semi-supervisedcommunity search for the first time and propose Pre-trained Prompt-drivenCommunity Search (PPCS), a novel model designed to enhance search accuracy andefficiency. PPCS consists of three main components: node encoding, samplegeneration, and prompt-driven fine-tuning. Specifically, the node encodingcomponent employs graph neural networks to learn local structural patterns ofnodes in a graph, thereby obtaining representations for nodes and communities.Next, the sample generation component identifies an initial community for agiven node and selects known communities that are structurally similar to theinitial one as training samples. Finally, the prompt-driven fine-tuningcomponent leverages these samples as prompts to guide the final communityprediction. Experimental results on five real-world datasets demonstrate thatPPCS performs better than baseline algorithms. It also achieves highercommunity search efficiency than semi-supervised community search baselinemethods, with ablation studies verifying the effectiveness of each component ofPPCS.</description>
      <author>example@mail.com (Li Ni, Hengkai Xu, Lin Mu, Yiwen Zhang, Wenjian Luo)</author>
      <guid isPermaLink="false">2505.12304v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models</title>
      <link>http://arxiv.org/abs/2505.12534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ChemPile，一个包含超过75亿个化学数据的开源数据集，旨在为化学科学中的通用模型训练和评估提供支持。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在多个科学领域取得了显著成功，但由于缺乏反映化学领域多方面性质的大量、多样化、高质量数据集，其在化学领域的影响仍然有限。&lt;h4&gt;目的&lt;/h4&gt;ChemPile旨在为化学科学中的通用模型训练和评估提供数据支持，并促进化学AI的发展。&lt;h4&gt;方法&lt;/h4&gt;ChemPile通过数百小时的专家编辑构建，包含多种化学数据表示（如SMILES、SELFIES、IUPAC名称、InChI、分子渲染）、科学和教育文本、可执行代码和化学图像。&lt;h4&gt;主要发现&lt;/h4&gt;ChemPile集成了基础知识、专业知识、视觉理解和高级推理，反映了人类化学家通过多样化的学习材料和经验发展专业知识的过程。&lt;h4&gt;结论&lt;/h4&gt;ChemPile的发布有望成为化学AI的催化剂，促进下一代化学基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在多个科学领域取得了显著的成功，但由于缺乏反映该领域多方面性质的大量、多样化、高质量数据集，其在化学领域的影响仍然有限。我们提出了ChemPile，这是一个包含超过75亿个经过编辑的化学数据标记的开源数据集，专门用于训练和评估化学科学中的通用模型。该数据集反映了人类学习化学的旅程——从教育基础到专业化的专业知识，涵盖了多种模态和内容类型，包括不同化学表示（SMILES、SELFIES、IUPAC名称、InChI、分子渲染）的结构化数据、科学和教育文本、可执行代码和化学图像。ChemPile集成了基础知识（教科书、讲义）、专业知识（科学文章和语言接口数据）、视觉理解（分子结构、图表）和高级推理（问题解决轨迹和代码）——反映了人类化学家通过多样化的学习材料和经验发展专业知识的过程。通过数百小时的专家编辑构建，ChemPile捕捉了基础概念和领域特定复杂性。我们提供了标准化的训练、验证和测试分割，以实现稳健的基准测试。ChemPile通过HuggingFace以一致的API、许可许可和详细文档公开发布。我们希望ChemPile能够成为化学AI的催化剂，促进下一代化学基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have shown remarkable success across scientific domains,yet their impact in chemistry remains limited due to the absence of diverse,large-scale, high-quality datasets that reflect the field's multifacetednature. We present the ChemPile, an open dataset containing over 75 billiontokens of curated chemical data, specifically built for training and evaluatinggeneral-purpose models in the chemical sciences. The dataset mirrors the humanlearning journey through chemistry -- from educational foundations tospecialized expertise -- spanning multiple modalities and content typesincluding structured data in diverse chemical representations (SMILES, SELFIES,IUPAC names, InChI, molecular renderings), scientific and educational text,executable code, and chemical images. ChemPile integrates foundationalknowledge (textbooks, lecture notes), specialized expertise (scientificarticles and language-interfaced data), visual understanding (molecularstructures, diagrams), and advanced reasoning (problem-solving traces and code)-- mirroring how human chemists develop expertise through diverse learningmaterials and experiences. Constructed through hundreds of hours of expertcuration, the ChemPile captures both foundational concepts and domain-specificcomplexity. We provide standardized training, validation, and test splits,enabling robust benchmarking. ChemPile is openly released via HuggingFace witha consistent API, permissive license, and detailed documentation. We hope theChemPile will serve as a catalyst for chemical AI, enabling the development ofthe next generation of chemical foundation models.</description>
      <author>example@mail.com (Adrian Mirza, Nawaf Alampara, Martiño Ríos-García, Mohamed Abdelalim, Jack Butler, Bethany Connolly, Tunca Dogan, Marianna Nezhurina, Bünyamin Şen, Santosh Tirunagari, Mark Worrall, Adamo Young, Philippe Schwaller, Michael Pieler, Kevin Maik Jablonka)</author>
      <guid isPermaLink="false">2505.12534v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Transformer learns the cross-task prior and regularization for in-context learning</title>
      <link>http://arxiv.org/abs/2505.12138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了Transformer在逆线性回归（ILR）中的情境学习（ICL）能力，探讨了隐含情境的性质及其对下游预测的效用。&lt;h4&gt;背景&lt;/h4&gt;尽管Transformer在情境学习方面表现出色，但其推断情境的本质及其对预测的实用性仍需进一步研究。&lt;h4&gt;目的&lt;/h4&gt;通过研究ILR中的情境学习，探讨隐含情境的性质及其对预测的效用。&lt;h4&gt;方法&lt;/h4&gt;本文引入了一个线性Transformer来学习从情境示例到潜在权重向量的逆映射，并关注了权重向量中未知数多于情境长度的秩亏逆问题。&lt;h4&gt;主要发现&lt;/h4&gt;Transformer隐式地学习了一个先验分布和有效的正则化策略，优于传统的岭回归和正则化方法。研究发现低任务维度相对于情境长度对于成功学习是必要的。&lt;h4&gt;结论&lt;/h4&gt;这些结果不仅展示了Transformer解决病态逆问题的潜力，也为理解Transformer内部的知识提取机制提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;Transformers have shown a remarkable ability for in-context learning (ICL), making predictions based on contextual examples. However, while theoretical analyses have explored this prediction capability, the nature of the inferred context and its utility for downstream predictions remain open questions. This paper aims to address these questions by examining ICL for inverse linear regression (ILR), where context inference can be characterized by unsupervised learning of underlying weight vectors. Focusing on the challenging scenario of rank-deficient inverse problems, where context length is smaller than the number of unknowns in the weight vectors and regularization is necessary, we introduce a linear transformer to learn the inverse mapping from contextual examples to the underlying weight vector. Our findings reveal that the transformer implicitly learns both a prior distribution and an effective regularization strategy, outperforming traditional ridge regression and regularization methods. A key insight is the necessity of low task dimensionality relative to the context length for successful learning. Furthermore, we numerically verify that the error of the transformer estimators scales linearly with the noise level, the ratio of task dimension to context length, and the condition number of the input data. These results not only demonstrate the potential of transformers for solving ill-posed inverse problems, but also provide a new perspective towards understanding the knowledge extraction mechanism within transformers.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have shown a remarkable ability for in-context learning (ICL),making predictions based on contextual examples. However, while theoreticalanalyses have explored this prediction capability, the nature of the inferredcontext and its utility for downstream predictions remain open questions. Thispaper aims to address these questions by examining ICL for inverse linearregression (ILR), where context inference can be characterized by unsupervisedlearning of underlying weight vectors. Focusing on the challenging scenario ofrank-deficient inverse problems, where context length is smaller than thenumber of unknowns in the weight vectors and regularization is necessary, weintroduce a linear transformer to learn the inverse mapping from contextualexamples to the underlying weight vector. Our findings reveal that thetransformer implicitly learns both a prior distribution and an effectiveregularization strategy, outperforming traditional ridge regression andregularization methods. A key insight is the necessity of low taskdimensionality relative to the context length for successful learning.Furthermore, we numerically verify that the error of the transformer estimatorscales linearly with the noise level, the ratio of task dimension to contextlength, and the condition number of the input data. These results not onlydemonstrate the potential of transformers for solving ill-posed inverseproblems, but also provide a new perspective towards understanding theknowledge extraction mechanism within transformers.</description>
      <author>example@mail.com (Fei Lu, Yue Yu)</author>
      <guid isPermaLink="false">2505.12138v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement</title>
      <link>http://arxiv.org/abs/2505.11939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为FG-CLEP的ECG文本对比学习方法，旨在通过大型语言模型从不完全的报告中恢复波形特征，以改善ECG诊断模型的能力。&lt;h4&gt;背景&lt;/h4&gt;传统的ECG-text对比学习方法在诊断心血管疾病方面表现良好，但往往忽略了报告的不完整性，导致模型无法充分捕捉波形特征和诊断推理。&lt;h4&gt;目的&lt;/h4&gt;提出FG-CLEP方法，以解决从不完全报告中恢复波形特征的问题，并提高ECG诊断模型的能力。&lt;h4&gt;方法&lt;/h4&gt;FG-CLEP利用大型语言模型帮助恢复波形特征，同时克服了幻觉和非一一对应关系等挑战。此外，引入语义相似度矩阵以指导对比学习，并采用基于sigmoid的损失函数以适应ECG相关任务的标签多性质。&lt;h4&gt;主要发现&lt;/h4&gt;在六个数据集上的实验表明，FG-CLEP在零样本预测和线性探针任务上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;FG-CLEP是一种有效的ECG文本对比学习方法，能够提高ECG诊断模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：心电图（ECG）对于诊断心血管疾病至关重要。尽管之前的心电图文本对比学习方法显示出有希望的结果，但它们往往忽略了报告的不完整性。给定一个ECG，报告是通过首先识别关键波形特征，然后通过这些特征推断最终诊断来生成的。尽管这些波形特征很重要，但它们通常不会作为中间结果记录在报告中。将ECG与这种不完整的报告对齐阻碍了模型捕捉ECG波形特征的能力，并限制了其对基于这些特征进行的诊断推理的理解。为了解决这个问题，我们提出了FG-CLEP（细粒度对比语言心电图预训练），它旨在在幻觉和非一一对应关系之间波形特征与诊断的挑战下，利用大型语言模型（LLMs）从不完全的报告中恢复这些波形特征。此外，考虑到由于ECG中常见诊断的普遍存在，经常出现假阴性，我们引入了一个语义相似度矩阵来指导对比学习。此外，我们采用基于sigmoid的损失函数来适应ECG相关任务的多标签性质。在六个数据集上的实验表明，FG-CLEP在这些数据集上的零样本预测和线性探针任务上都优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiograms (ECGs) are essential for diagnosing cardiovasculardiseases. While previous ECG-text contrastive learning methods have shownpromising results, they often overlook the incompleteness of the reports. Givenan ECG, the report is generated by first identifying key waveform features andthen inferring the final diagnosis through these features. Despite theirimportance, these waveform features are often not recorded in the report asintermediate results. Aligning ECGs with such incomplete reports impedes themodel's ability to capture the ECG's waveform features and limits itsunderstanding of diagnostic reasoning based on those features. To address this,we propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), whichaims to recover these waveform features from incomplete reports with the helpof large language models (LLMs), under the challenges of hallucinations and thenon-bijective relationship between waveform features and diagnoses.Additionally, considering the frequent false negatives due to the prevalence ofcommon diagnoses in ECGs, we introduce a semantic similarity matrix to guidecontrastive learning. Furthermore, we adopt a sigmoid-based loss function toaccommodate the multi-label nature of ECG-related tasks. Experiments on sixdatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in bothzero-shot prediction and linear probing across these datasets.</description>
      <author>example@mail.com (Haitao Li, Che Liu, Zhengyao Ding, Ziyi Liu, Zhengxing Huang)</author>
      <guid isPermaLink="false">2505.11939v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling</title>
      <link>http://arxiv.org/abs/2505.12272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的框架，旨在通过结合GNN蒸馏和抽象概率交互建模（APIM）来克服知识图谱补全（KGC）中的挑战，以提高知识图谱的有效性。&lt;h4&gt;背景&lt;/h4&gt;大多数知识图谱（KGs）不完整，限制了它们在下游应用中的有效性。&lt;h4&gt;目的&lt;/h4&gt;旨在通过推断缺失链接来解决知识图谱补全的问题。&lt;h4&gt;方法&lt;/h4&gt;该方法通过结合GNN蒸馏和抽象概率交互建模（APIM），GNN蒸馏引入了迭代消息特征过滤过程来减轻过平滑，而APIM模块通过概率签名和转移矩阵学习结构化的抽象交互模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与基线模型相比，该方法在广泛使用的WN18RR和FB15K-237数据集上取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;该研究结果强调了控制信息传播和利用结构化概率建模的重要性，为推进知识图谱补全提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;This study proposes a unified framework that aims to overcome the challenges of knowledge graph completion (KGC) by integrating GNN distillation and abstract probabilistic interaction modeling (APIM), in order to enhance the effectiveness of knowledge graphs in downstream applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs (KGs) serve as fundamental structures for organizinginterconnected data across diverse domains. However, most KGs remainincomplete, limiting their effectiveness in downstream applications. Knowledgegraph completion (KGC) aims to address this issue by inferring missing links,but existing methods face critical challenges: deep graph neural networks(GNNs) suffer from over-smoothing, while embedding-based models fail to captureabstract relational features. This study aims to overcome these limitations byproposing a unified framework that integrates GNN distillation and abstractprobabilistic interaction modeling (APIM). GNN distillation approach introducesan iterative message-feature filtering process to mitigate over-smoothing,preserving the discriminative power of node representations. APIM modulecomplements this by learning structured, abstract interaction patterns throughprobabilistic signatures and transition matrices, allowing for a richer, moreflexible representation of entity and relation interactions. We apply thesemethods to GNN-based models and the APIM to embedding-based KGC models,conducting extensive evaluations on the widely used WN18RR and FB15K-237datasets. Our results demonstrate significant performance gains over baselinemodels, showcasing the effectiveness of the proposed techniques. The findingshighlight the importance of both controlling information propagation andleveraging structured probabilistic modeling, offering new avenues foradvancing knowledge graph completion. And our codes are available athttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.</description>
      <author>example@mail.com (Lingzhi Wang, Pengcheng Huang, Haotian Li, Yuliang Wei, Guodong Xin, Rui Zhang, Donglin Zhang, Zhenzhou Ji, Wei Wang)</author>
      <guid isPermaLink="false">2505.12272v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets</title>
      <link>http://arxiv.org/abs/2505.12532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Wavelet Fine-Tuning (WaveFT)的参数高效微调方法，该方法在残差矩阵的波形域中学习高度稀疏的更新，适用于极端参数高效的场景。&lt;h4&gt;背景&lt;/h4&gt;高效的适应大型基础模型对于在有限的计算和内存预算下至关重要，传统的PEFT方法如LoRA在少量参数的情况下效果有限。&lt;h4&gt;目的&lt;/h4&gt;提出WaveFT方法以实现参数高效的微调，并提供更精细的能力调整。&lt;h4&gt;方法&lt;/h4&gt;WaveFT方法通过在波形域学习残差矩阵的更新，实现高度稀疏的参数调整。同时，通过与直接在权重域应用稀疏更新的方法SHiRA进行比较，以验证波形变换的效果。&lt;h4&gt;主要发现&lt;/h4&gt;WaveFT在个人化的文本到图像生成任务上显著优于LoRA和其他PEFT方法，尤其是在低参数数量时，实现了更好的主题一致性、提示对齐和图像多样性。&lt;h4&gt;结论&lt;/h4&gt;WaveFT是一种有效的PEFT方法，能够在参数高效的情况下实现高质量的个人化图像生成，特别适合在计算资源受限的环境中使用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：有效地调整大型基础模型至关重要，尤其是在有限的计算和内存预算下。参数高效微调（PEFT）方法如LoRA在少量参数的系统中提供有限的粒度和效果。我们提出了波纹微调（WaveFT），一种新的PEFT方法，该方法在残差矩阵的波纹域中学习高度稀疏的更新。WaveFT允许精确控制可训练参数，提供细粒度的能力调整，并且具有非常低的参数数量，可能远低于LoRA的最小值——非常适合极端参数高效的场景。为了证明波形变换的效果，我们将WaveFT与一个称为SHiRA的特殊情况进行了比较，该情况涉及直接在权重域应用稀疏更新。在以Stable Diffusion XL作为基线进行个性化文本到图像生成任务时评估，WaveFT在低参数数量时显著优于LoRA和其他PEFT方法；实现了更高的主题一致性、提示对齐和图像多样性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently adapting large foundation models is critical, especially withtight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)methods such as LoRA offer limited granularity and effectiveness infew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFTmethod that learns highly sparse updates in the wavelet domain of residualmatrices. WaveFT allows precise control of trainable parameters, offeringfine-grained capacity adjustment and excelling with remarkably low parametercount, potentially far fewer than LoRA's minimum -- ideal for extremeparameter-efficient scenarios. In order to demonstrate the effect of thewavelet transform, we compare WaveFT with a special case, called SHiRA, thatentails applying sparse updates directly in the weight domain. Evaluated onpersonalized text-to-image generation using Stable Diffusion XL as baseline,WaveFT significantly outperforms LoRA and other PEFT methods, especially at lowparameter counts; achieving superior subject fidelity, prompt alignment, andimage diversity.</description>
      <author>example@mail.com (Ahmet Bilican, M. Akın Yılmaz, A. Murat Tekalp, R. Gökberk Cinbiş)</author>
      <guid isPermaLink="false">2505.12532v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Relation-Aware Graph Foundation Model</title>
      <link>http://arxiv.org/abs/2505.12027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为REEF的新框架，用于图学习中的基础模型，旨在通过利用关系标记作为基本单位来提高模型在各类自然语言处理任务中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;近年来，大型语言模型（LLMs）在自然语言处理（NLP）任务中表现出显著的泛化能力，而图基础模型（GFMs）在图学习中也展现出巨大的潜力。然而，由于图没有明确的泛化单位，设计有效的预训练策略变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出REEF框架，以解决图学习中的泛化问题，并提高模型在预训练和迁移学习任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 使用关系标记作为GFMs的基本单位；2. 构建关系词汇表以存储图中的关系信息；3. 引入两个超网络，根据关系标记自适应生成图神经网络中聚合器和分类器的参数；4. 设计另一个超网络来构建特定于数据集的项目符，并将数据集级别的特征偏差纳入初始节点表示；5. 采用图数据增强和混合数据集预训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;REEF在预训练和迁移学习任务上显著优于现有方法，展现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;REEF作为一个强大的基础模型，在图学习领域具有巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示了显著的泛化能力。同样，图基础模型（GFMs）已成为图学习领域的一个有希望的进展方向，旨在通过大规模预训练在多样化的数据集上实现泛化。然而，与依赖于显式标记表示的语言模型不同，图缺乏一个定义良好的泛化单位，这使得设计有效的预训练策略变得具有挑战性。在本工作中，我们提出了一种名为REEF的新框架，该框架利用关系标记作为GFMs的基本单位。受LLMs中标记词汇表的启发，我们构建了一个关系标记的词汇表，用于在图中存储关系信息。为了适应不同的关系，我们引入了两个超网络，根据关系标记自适应地生成图神经网络中聚合器和分类器的参数。此外，我们还设计了一个超网络来构建特定于数据集的项目符，并将数据集级别的特征偏差纳入初始节点表示，从而增强了在不同数据集上具有相同关系的灵活性。进一步地，我们采用了图数据增强和混合数据集预训练策略，使得REEF能够更有效地捕捉关系多样性，并展现出强大的泛化能力。广泛的实验表明，REEF在预训练和迁移学习任务上显著优于现有方法，凸显了其作为基于图应用的有力基础模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, large language models (LLMs) have demonstrated remarkablegeneralization capabilities across various natural language processing (NLP)tasks. Similarly, graph foundation models (GFMs) have emerged as a promisingdirection in graph learning, aiming to generalize across diverse datasetsthrough large-scale pre-training. However, unlike language models that rely onexplicit token representations, graphs lack a well-defined unit forgeneralization, making it challenging to design effective pre-trainingstrategies. In this work, we propose REEF, a novel framework that leveragesrelation tokens as the basic units for GFMs. Inspired by the token vocabularyin LLMs, we construct a relation vocabulary of relation tokens to storerelational information within graphs. To accommodate diverse relations, weintroduce two hypernetworks that adaptively generate the parameters ofaggregators and classifiers in graph neural networks based on relation tokens.In addition, we design another hypernetwork to construct dataset-specificprojectors and incorporate a dataset-level feature bias into the initial noderepresentations, enhancing flexibility across different datasets with the samerelation. Further, we adopt graph data augmentation and a mixed-datasetpre-training strategy, allowing REEF to capture relational diversity moreeffectively and exhibit strong generalization capabilities. Extensiveexperiments show that REEF significantly outperforms existing methods on bothpre-training and transfer learning tasks, underscoring its potential as apowerful foundation model for graph-based applications.</description>
      <author>example@mail.com (Jianxiang Yu, Jiapeng Zhu, Hao Qian, Ziqi Liu, Zhiqiang Zhang, Xiang Li)</author>
      <guid isPermaLink="false">2505.12027v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Invariant Risk Minimization</title>
      <link>http://arxiv.org/abs/2505.12506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新颖的无监督框架，用于不变风险最小化（IRM），扩展了不变性的概念，使其适用于标签不可用的场景。&lt;h4&gt;背景&lt;/h4&gt;传统的IRM方法依赖于标签数据来学习对环境分布变化鲁棒的特征表示。&lt;h4&gt;目的&lt;/h4&gt;通过特征分布对齐重新定义不变性，从而实现从无标签数据中学习鲁棒的特征表示。&lt;h4&gt;方法&lt;/h4&gt;在框架中引入了两种方法：主不变成分分析（PICA），一种基于高斯假设的线性方法，用于提取不变方向；以及变分不变自动编码器（VIAE），一种深度生成模型，用于分离环境不变和环境相关的潜在因子。&lt;h4&gt;主要发现&lt;/h4&gt;方法基于一种新颖的无监督结构因果模型，支持环境条件下的样本生成和干预；在合成数据集和MNIST的修改版本上的实验评估表明，该方法在捕捉不变结构、保留相关信息以及在无标签的情况下跨环境泛化方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;该方法在不依赖标签的情况下，能够有效地学习鲁棒的特征表示，并在不同环境中进行泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yotamnor/uirm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel unsupervised framework for \emph{Invariant RiskMinimization} (IRM), extending the concept of invariance to settings wherelabels are unavailable. Traditional IRM methods rely on labeled data to learnrepresentations that are robust to distributional shifts across environments.In contrast, our approach redefines invariance through feature distributionalignment, enabling robust representation learning from unlabeled data. Weintroduce two methods within this framework: Principal Invariant ComponentAnalysis (PICA), a linear method that extracts invariant directions underGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deepgenerative model that disentangles environment-invariant andenvironment-dependent latent factors. Our approach is based on a novel``unsupervised'' structural causal model and supports environment-conditionedsample-generation and intervention. Empirical evaluations on synthetic datasetand modified versions of MNIST demonstrate the effectiveness of our methods incapturing invariant structure, preserving relevant information, andgeneralizing across environments without access to labels.</description>
      <author>example@mail.com (Yotam Norman, Ron Meir)</author>
      <guid isPermaLink="false">2505.12506v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities</title>
      <link>http://arxiv.org/abs/2505.11921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DC-Seg的新方法，用于解决脑图像分割问题，该方法通过解耦对比学习，将图像分解为解剖不变表示和模态特定表示，以提高分割精度。&lt;h4&gt;背景&lt;/h4&gt;脑图像分割通常需要整合来自多个模态的信息，但并非所有患者都有所有模态的临床数据，这给分割带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提高脑图像分割的准确性，即使某些模态的数据缺失。&lt;h4&gt;方法&lt;/h4&gt;DC-Seg使用解剖对比学习和模态对比学习来解耦图像，同时引入基于分割的正则化器，以增强模型对缺失模态的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在BraTS 2020和私人白质高信号（WMH）分割数据集上的实验表明，DC-Seg在处理具有不同缺失模态的不完整多模态脑肿瘤分割任务中优于现有方法，并在WMH分割中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DC-Seg是一种有效的脑图像分割方法，能够处理模态缺失的问题，并在实际应用中表现出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：准确分割脑图像通常需要整合来自多个图像模态的互补信息。然而，并非所有患者都有所有模态的临床数据，这给分割带来了重大挑战。为了解决这个问题，先前的研究将多个模态编码到共享的潜在空间中。虽然这在一定程度上是有效的，但它仍然是不够理想的，因为每个模态都包含独特且有价值的信息。在本研究中，我们提出了DC-Seg（解耦对比学习用于分割），一种新的方法，通过使用解剖对比学习和模态对比学习分别显式地将图像解耦为解剖不变表示和模态特定表示。这种解决方案通过考虑模态差距，提高了解剖和模态特定特征的分离，导致更鲁棒的表现。此外，我们引入了一种基于分割的正则化器，增强了模型对缺失模态的鲁棒性。在BraTS 2020和私人白质高信号（WMH）分割数据集上的大量实验表明，DC-Seg在处理具有不同缺失模态的不完整多模态脑肿瘤分割任务中优于现有方法，同时也在WMH分割中表现出强大的泛化能力。代码可在https://github.com/CuCl-2/DC-Seg上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of brain images typically requires the integration ofcomplementary information from multiple image modalities. However, clinicaldata for all modalities may not be available for every patient, creating asignificant challenge. To address this, previous studies encode multiplemodalities into a shared latent space. While somewhat effective, it remainssuboptimal, as each modality contains distinct and valuable information. Inthis study, we propose DC-Seg (Disentangled Contrastive Learning forSegmentation), a new method that explicitly disentangles images intomodality-invariant anatomical representation and modality-specificrepresentation, by using anatomical contrastive learning and modalitycontrastive learning respectively. This solution improves the separation ofanatomical and modality-specific features by considering the modality gaps,leading to more robust representations. Furthermore, we introduce asegmentation-based regularizer that enhances the model's robustness to missingmodalities. Extensive experiments on the BraTS 2020 and a private white matterhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperformsstate-of-the-art methods in handling incomplete multimodal brain tumorsegmentation tasks with varying missing modalities, while also demonstratestrong generalizability in WMH segmentation. The code is available athttps://github.com/CuCl-2/DC-Seg.</description>
      <author>example@mail.com (Haitao Li, Ziyu Li, Yiheng Mao, Zhengyao Ding, Zhengxing Huang)</author>
      <guid isPermaLink="false">2505.11921v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark</title>
      <link>http://arxiv.org/abs/2505.12254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MMS-VPR，一个用于复杂、行人专用环境中的大规模多模态街级场所识别数据集。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉场所识别数据集主要依赖车载图像，缺乏多模态多样性，且在非西方城市环境中对密集、混合用途的街级空间的代表性不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些差距，提出MMS-VPR数据集，旨在提供多模态、适用于街级场所识别的数据。&lt;h4&gt;方法&lt;/h4&gt;数据集包含78,575张标注图像和2,512个视频片段，覆盖成都一个约70,800平方米的开放式商业区。每个图像都标注了精确的GPS坐标、时间戳和文本元数据。数据集采用系统化的数据收集协议，设备要求最低，以降低大规模数据集创建的门槛。数据集形成一个包含125条边、81个节点和1个子图的固有空间图，支持结构感知的场所识别。定义了两个应用特定子集——Dataset_Edges和Dataset_Points，以支持细粒度和基于图的评价任务。&lt;h4&gt;主要发现&lt;/h4&gt;使用传统VPR模型、图神经网络和多模态基线进行的大量基准测试表明，利用多模态和结构线索可以显著提高性能。&lt;h4&gt;结论&lt;/h4&gt;MMS-VPR促进了计算机视觉、地理空间理解和多模态推理交叉领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing visual place recognition (VPR) datasets predominantly rely onvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,mixed-use street-level spaces, especially in non-Western urban contexts. Toaddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset forstreet-level place recognition in complex, pedestrian-only environments. Thedataset comprises 78,575 annotated images and 2,512 video clips captured across207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district inChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,and textual metadata, and covers varied lighting conditions, viewpoints, andtimeframes. MMS-VPR follows a systematic and replicable data collectionprotocol with minimal device requirements, lowering the barrier for scalabledataset creation. Importantly, the dataset forms an inherent spatial graph with125 edges, 81 nodes, and 1 subgraph, enabling structure-aware placerecognition. We further define two application-specific subsets --Dataset_Edges and Dataset_Points -- to support fine-grained and graph-basedevaluation tasks. Extensive benchmarks using conventional VPR models, graphneural networks, and multimodal baselines show substantial improvements whenleveraging multimodal and structural cues. MMS-VPR facilitates future researchat the intersection of computer vision, geospatial understanding, andmultimodal reasoning. The dataset is publicly available athttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.</description>
      <author>example@mail.com (Yiwei Ou, Xiaobin Ren, Ronggui Sun, Guansong Gao, Ziyi Jiang, Kaiqi Zhao, Manfredo Manfredini)</author>
      <guid isPermaLink="false">2505.12254v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Capabilities of Molecular Graph Neural Networks in Materials Science Through Multimodal Learning and Physical Context Encoding</title>
      <link>http://arxiv.org/abs/2505.12137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted Spotlight Paper at CVPR 2025 for MM4Mat&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种多模态框架，结合分子图和文本描述符（如IUPAC名称、分子式、物化性质和同义词）来提高分子图神经网络（GNNs）的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的分子图神经网络主要关注基于XYZ的几何表示，忽视了公共数据库如PubChem中可用的化学上下文信息。&lt;h4&gt;目的&lt;/h4&gt;引入多模态框架，通过集成文本描述符和分子图，以及一个门控融合机制，来提高模型对互补信息的利用。&lt;h4&gt;方法&lt;/h4&gt;采用门控融合机制平衡几何和文本特征，通过实验在基准数据集上验证该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;添加文本数据对某些电子性质有显著改善，而对其他性质的影响有限。GNN架构显示出相似的性能模式，表明它们学习到了相似而不是不同的物理洞察。&lt;h4&gt;结论&lt;/h4&gt;该多模态框架能够提高GNN的性能，尤其是在处理化学性质方面，但提升效果仍有待提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular graph neural networks (GNNs) often focus exclusively on XYZ-basedgeometric representations and thus overlook valuable chemical context availablein public databases like PubChem. This work introduces a multimodal frameworkthat integrates textual descriptors, such as IUPAC names, molecular formulas,physicochemical properties, and synonyms, alongside molecular graphs. A gatedfusion mechanism balances geometric and textual features, allowing models toexploit complementary information. Experiments on benchmark datasets indicatethat adding textual data yields notable improvements for certain electronicproperties, while gains remain limited for others. Furthermore, the GNNarchitectures display similar performance patterns (improving and deterioratingon analogous targets), suggesting they learn comparable representations ratherthan distinctly different physical insights.</description>
      <author>example@mail.com (Can Polat, Hasan Kurban, Erchin Serpedin, Mustafa Kurban)</author>
      <guid isPermaLink="false">2505.12137v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies</title>
      <link>http://arxiv.org/abs/2505.12404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Hyperbolic Residual Quantization（HRQ）方法，用于生成具有潜在层次结构的数据的离散层次表示，并评估其在层次建模和层次发现任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;层次数据在众多领域出现，如生物分类、组织结构、法律代码和知识图谱。传统的Residual Quantization（RQ）方法依赖于欧几里得几何，这在处理层次结构时可能引入根本性的不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出HRQ方法，通过在双曲空间中嵌入数据和执行双曲操作来改进层次数据的表示。&lt;h4&gt;方法&lt;/h4&gt;HRQ通过在双曲流形中嵌入数据，并使用双曲操作和距离度量进行残差量化，从而自然地与层次分支对齐。&lt;h4&gt;主要发现&lt;/h4&gt;HRQ在监督层次建模和层次发现任务上优于传统的欧几里得RQ，特别是在层次建模任务上，性能提升可达20%。&lt;h4&gt;结论&lt;/h4&gt;将双曲几何引入离散表示学习可以显著提高捕获潜在层次结构的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：层次数据在无数领域出现，从生物分类和组织结构到法律代码和知识图谱。残差量化（RQ）被广泛用于通过迭代量化多级代码簿中的残差来生成此类数据的离散多令牌表示。然而，它对欧几里得几何的依赖可能导致与层次分支建模的基本不匹配，这对于忠实表示层次数据是必要的。在这项工作中，我们提出了超双曲残差量化（HRQ），它将数据原生地嵌入到超双曲流形中，并使用超双曲操作和距离度量进行残差量化。通过将嵌入网络、残差计算和距离度量适配到超双曲几何，HRQ赋予了一种与层次分支自然对齐的归纳偏好。我们声称，与RQ相比，HRQ可以为具有潜在层次结构的数据生成更有用的下游任务离散层次表示。我们在两个任务上评估了HRQ：使用WordNet同义词树的监督层次建模，其中模型被监督以学习潜在层次结构，以及层次发现，其中数据中存在潜在层次结构，但模型并未直接在涉及层次结构的任务上训练或评估。在两种情况下，HRQ层次标记在下游任务上的表现都优于欧几里得RQ，在层次建模任务上的提升可达20%。我们的结果表明，将双曲几何整合到离散表示学习中有助于显著提高捕获潜在层次结构的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical data arise in countless domains, from biological taxonomies andorganizational charts to legal codes and knowledge graphs. ResidualQuantization (RQ) is widely used to generate discrete, multitokenrepresentations for such data by iteratively quantizing residuals in amultilevel codebook. However, its reliance on Euclidean geometry can introducefundamental mismatches that hinder modeling of hierarchical branching,necessary for faithful representation of hierarchical data. In this work, wepropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in ahyperbolic manifold and performs residual quantization using hyperbolicoperations and distance metrics. By adapting the embedding network, residualcomputation, and distance metric to hyperbolic geometry, HRQ imparts aninductive bias that aligns naturally with hierarchical branching. We claim thatHRQ in comparison to RQ can generate more useful for downstream tasks discretehierarchical representations for data with latent hierarchies. We evaluate HRQon two tasks: supervised hierarchy modeling using WordNet hypernym trees, wherethe model is supervised to learn the latent hierarchy - and hierarchydiscovery, where, while latent hierarchy exists in the data, the model is notdirectly trained or evaluated on a task related to the hierarchy. Across bothscenarios, HRQ hierarchical tokens yield better performance on downstream taskscompared to Euclidean RQ with gains of up to $20\%$ for the hierarchy modelingtask. Our results demonstrate that integrating hyperbolic geometry intodiscrete representation learning substantially enhances the ability to capturelatent hierarchies.</description>
      <author>example@mail.com (Piotr Piękos, Subhradeep Kayal, Alexandros Karatzoglou)</author>
      <guid isPermaLink="false">2505.12404v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2505.12136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LSTAN-GERPE的新型交通预测模型，该模型结合了时空注意机制和图嵌入技术，通过优化旋转位置编码频率来提高交通预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;交通预测是智能交通系统中的一个关键任务，现有研究主要聚焦于将图神经网络（GNN）与其他模型结合，但GNN仅考虑短程空间信息。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效捕捉长程交通动态的新型交通预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出的模型LSTAN-GERPE结合了时间和空间注意机制，并通过网格搜索优化旋转位置编码的频率。模型还将地理位置图整合到时空嵌入中，以提高特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统优化，LSTAN-GERPE模型能够有效地捕捉复杂的交通模式，并在PeMS04和PeMS08等真实世界交通预测数据集上取得了较高的准确性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在不进行大量特征工程的情况下，实现了较高的交通预测准确性，为智能交通系统的进一步发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Traffic forecasting is a key task in the field of Intelligent Transportation Systems. Recent research on traffic forecasting has mainly focused on combining graph neural networks (GNNs) with other models. However, GNNs only considers short-range spatial information. In this study, we present a novel model termed LSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding). This model leverages both Temporal and Spatial Attention mechanisms to effectively capture long-range traffic dynamics. Additionally, the optimal frequency for rotational position encoding is determined through a grid search approach in both the spatial and temporal attention mechanisms. This systematic optimization enables the model to effectively capture complex traffic patterns. The model also enhances feature representation by incorporating geographical location maps into the spatio-temporal embeddings. Without extensive feature engineering, the proposed method in this paper achieves advanced accuracy on the real-world traffic forecasting datasets PeMS04 and PeMS08.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting is a key task in the field of Intelligent TransportationSystems. Recent research on traffic forecasting has mainly focused on combininggraph neural networks (GNNs) with other models. However, GNNs only considershort-range spatial information. In this study, we present a novel model termedLSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embeddingand Rotational Position Encoding). This model leverages both Temporal andSpatial Attention mechanisms to effectively capture long-range trafficdynamics. Additionally, the optimal frequency for rotational position encodingis determined through a grid search approach in both the spatial and temporalattention mechanisms. This systematic optimization enables the model toeffectively capture complex traffic patterns. The model also enhances featurerepresentation by incorporating geographical location maps into thespatio-temporal embeddings. Without extensive feature engineering, the proposedmethod in this paper achieves advanced accuracy on the real-world trafficforecasting datasets PeMS04 and PeMS08.</description>
      <author>example@mail.com (Xiao Wang, Shun-Ren Yang)</author>
      <guid isPermaLink="false">2505.12136v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures</title>
      <link>http://arxiv.org/abs/2505.11918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at  https://github.com/Rorschach1989/transformer-for-gmm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了Transformer在解决高斯混合模型（GMM）问题上的能力，提出了一种基于Transformer的学习框架TGMM，并展示了其在无监督学习中的潜力。&lt;h4&gt;背景&lt;/h4&gt;Transformer架构在人工智能领域展现出卓越的能力，其中隐式学习内部模型的能力被认为对理解预训练大型语言模型至关重要。然而，最近的研究主要集中在监督学习问题上，无监督学习领域相对未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究Transformer在解决GMM问题上的能力，并构建一个能够同时解决多个GMM任务的Transformer学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为TGMM的Transformer学习框架，通过共享的Transformer骨干网络学习解决多个GMM任务，并通过实验验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;TGMM在解决GMM任务上表现出色，有效缓解了传统方法如EM算法或谱算法的局限性，同时对分布变化表现出合理的鲁棒性。理论上证明了Transformer可以近似EM算法和谱方法的核心组件。&lt;h4&gt;结论&lt;/h4&gt;Transformer在无监督学习领域具有广泛的应用潜力，可以作为解决GMM等问题的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the understanding of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rorschach1989/transformer-for-gmm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The transformer architecture has demonstrated remarkable capabilities inmodern artificial intelligence, among which the capability of implicitlylearning an internal model during inference time is widely believed to play akey role in the under standing of pre-trained large language models. However,most recent works have been focusing on studying supervised learning topicssuch as in-context learning, leaving the field of unsupervised learning largelyunexplored. This paper investigates the capabilities of transformers in solvingGaussian Mixture Models (GMMs), a fundamental unsupervised learning problemthrough the lens of statistical estimation. We propose a transformer-basedlearning framework called TGMM that simultaneously learns to solve multiple GMMtasks using a shared transformer backbone. The learned models are empiricallydemonstrated to effectively mitigate the limitations of classical methods suchas Expectation-Maximization (EM) or spectral algorithms, at the same timeexhibit reasonable robustness to distribution shifts. Theoretically, we provethat transformers can approximate both the EM algorithm and a core component ofspectral methods (cubic tensor power iterations). These results bridge the gapbetween practical success and theoretical understanding, positioningtransformers as versatile tools for unsupervised learning.</description>
      <author>example@mail.com (Zhiheng Chen, Ruofan Wu, Guanhua Fang)</author>
      <guid isPermaLink="false">2505.11918v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Ripple: Scalable Incremental GNN Inferencing on Large Streaming Graphs</title>
      <link>http://arxiv.org/abs/2505.12112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint of paper to appear in the proceedings of the 45th IEEE  International Conference on Distributed Computing Systems (ICDCS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Ripple的框架，用于在大型动态图上进行高效的GNN推理。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的图通常是动态的，频繁的图拓扑和顶点边属性更新对GNN推理构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对动态图中的这些挑战，本文旨在提出一个既高效又准确的流式GNN推理框架。&lt;h4&gt;方法&lt;/h4&gt;Ripple框架通过利用GNN中底层聚合函数的性质，实现了因图拓扑或顶点特征更新而引起的嵌入的快速增量更新。&lt;h4&gt;主要发现&lt;/h4&gt;Ripple在单机上的性能表现优异，对于稀疏图如Arxiv能达到约28000次更新/秒，而对于更大且更密集的图如Products也能达到约1200次更新/秒，并且延迟在0.1毫秒到1秒之间，适合近实时应用。分布式版本的Ripple在更新期间通信成本降低了70倍，因此提供了比基线高约30倍的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;Ripple框架为动态图上的GNN推理提供了一种高效且准确的方法，特别是在分布式设置中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为Ripple的框架，用于在大型动态图上进行高效的GNN推理。现实世界中的图通常是动态的，频繁的图拓扑和顶点边属性更新对GNN推理构成了挑战。为了应对动态图中的这些挑战，本文旨在提出一个既高效又准确的流式GNN推理框架。Ripple框架通过利用GNN中底层聚合函数的性质，实现了因图拓扑或顶点特征更新而引起的嵌入的快速增量更新。Ripple在单机上的性能表现优异，对于稀疏图如Arxiv能达到约28000次更新/秒，而对于更大且更密集的图如Products也能达到约1200次更新/秒，并且延迟在0.1毫秒到1秒之间，适合近实时应用。分布式版本的Ripple在更新期间通信成本降低了70倍，因此提供了比基线高约30倍的吞吐量。Ripple框架为动态图上的GNN推理提供了一种高效且准确的方法，特别是在分布式设置中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most real-world graphs are dynamic in nature, with continuous and rapidupdates to the graph topology, and vertex and edge properties. Such frequentupdates pose significant challenges for inferencing over Graph Neural Networks(GNNs). Current approaches that perform vertex-wise and layer-wise inferencingare impractical for dynamic graphs as they cause redundant computations, expandto large neighborhoods, and incur high communication costs for distributedsetups, resulting in slow update propagation that often exceeds real-timelatency requirements. This motivates the need for streaming GNN inferenceframeworks that are efficient and accurate over large, dynamic graphs. Wepropose Ripple, a framework that performs fast incremental updates ofembeddings arising due to updates to the graph topology or vertex features.Ripple provides a generalized incremental programming model, leveraging theproperties of the underlying aggregation functions employed by GNNs toefficiently propagate updates to the affected neighborhood and compute theexact new embeddings. Besides a single-machine design, we also extend thisexecution model to distributed inferencing, to support large graphs that do notfit in a single machine's memory. Ripple on a single machine achieves up to$\approx28000$ updates/sec for sparse graphs like Arxiv and $\approx1200$updates/sec for larger and denser graphs like Products, with latencies of$0.1$ms--$1$s that are required for near-realtime applications. The distributedversion of Ripple offers up to $\approx30\times$ better throughput over thebaselines, due to $70\times$ lower communication costs during updates.</description>
      <author>example@mail.com (Pranjal Naman, Yogesh Simmhan)</author>
      <guid isPermaLink="false">2505.12112v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement</title>
      <link>http://arxiv.org/abs/2505.11822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的跨视角地理定位框架CVD，旨在解决由视角差异引起的显著外观变化和空间畸变带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;跨视角地理定位（CVGL）旨在匹配来自不同视角（如无人机和卫星）的同一地理位置的图像，尽管近年来取得了进展，但CVGL仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出一种新的框架来明确分离内容和视角因素，以解决视角差异引起的内在冲突，从而提高定位精度。&lt;h4&gt;方法&lt;/h4&gt;采用流形学习方法，将跨视角图像的特征空间建模为受内容和视角信息共同控制的复合流形。CVD框架引入了两个约束条件：内视图独立性约束和跨视图重建约束。&lt;h4&gt;主要发现&lt;/h4&gt;CVD框架能够有效分离内容和视角因素，并通过实验证明了其在多个基准数据集上的定位精度和泛化能力的提升。&lt;h4&gt;结论&lt;/h4&gt;CVD框架可以无缝集成到现有的地理定位流程中，并在多个基准数据集上显著提高了定位精度和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Cross-view geo-localization (CVGL) aims to match images of the same geographic location captured from different perspectives, such as drones and satellites. Despite recent advances, CVGL remains highly challenging due to significant appearance changes and spatial distortions caused by viewpoint variations. Existing methods typically assume that cross-view images can be directly aligned within a shared feature space by maximizing feature similarity through contrastive learning. Nonetheless, this assumption overlooks the inherent conflicts induced by viewpoint discrepancies, resulting in extracted features containing inconsistent information that hinders precise localization. In this study, we take a manifold learning perspective and model the featurespace of cross-view images as a composite manifold jointly governed by content and viewpoint information. Building upon this insight, we propose CVD, a new CVGL framework that explicitly disentangles content and viewpoint factors. To promote effective disentanglement, we introduce two constraints: (i) An intra-view independence constraint, which encourages statistical independence between the two factors by minimizing their mutual information. (ii) An inter-view reconstruction constraint that reconstructs each view by cross-combining content and viewpoint from paired images, ensuring factor-specific semantics are preserved. As a plug-and-play module, CVD can be seamlessly integrated into existing geo-localization pipelines. Extensive experiments on four benchmarks, i.e., University-1652, SUES-200, CVUSA, and CVACT, demonstrate that CVD consistently improves both localization accuracy and generalization across multiple baselines.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-view geo-localization (CVGL) aims to match images of the samegeographic location captured from different perspectives, such as drones andsatellites. Despite recent advances, CVGL remains highly challenging due tosignificant appearance changes and spatial distortions caused by viewpointvariations. Existing methods typically assume that cross-view images can bedirectly aligned within a shared feature space by maximizing feature similaritythrough contrastive learning. Nonetheless, this assumption overlooks theinherent conflicts induced by viewpoint discrepancies, resulting in extractedfeatures containing inconsistent information that hinders precise localization.In this study, we take a manifold learning perspective and model the featurespace of cross-view images as a composite manifold jointly governed by contentand viewpoint information. Building upon this insight, we propose$\textbf{CVD}$, a new CVGL framework that explicitly disentangles$\textit{content}$ and $\textit{viewpoint}$ factors. To promote effectivedisentanglement, we introduce two constraints: $\textit{(i)}$ An intra-viewindependence constraint, which encourages statistical independence between thetwo factors by minimizing their mutual information. $\textit{(ii)}$ Aninter-view reconstruction constraint that reconstructs each view bycross-combining $\textit{content}$ and $\textit{viewpoint}$ from paired images,ensuring factor-specific semantics are preserved. As a plug-and-play module,CVD can be seamlessly integrated into existing geo-localization pipelines.Extensive experiments on four benchmarks, i.e., University-1652, SUES-200,CVUSA, and CVACT, demonstrate that CVD consistently improves both localizationaccuracy and generalization across multiple baselines.</description>
      <author>example@mail.com (Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, Quan Wang)</author>
      <guid isPermaLink="false">2505.11822v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种严格的熵力理论，用于理解使用随机梯度下降（SGD）及其变体训练的神经网络的动态学习过程。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习和大型语言模型中涌现现象的快速发现，解释和理解其成因已成为迫切需要。&lt;h4&gt;目的&lt;/h4&gt;提出一种理论来解释和理解神经网络学习动态。&lt;h4&gt;方法&lt;/h4&gt;基于参数对称性和非熵损失景观理论，展示了表示学习由随机性和离散时间更新引起的涌现熵力所控制。&lt;h4&gt;主要发现&lt;/h4&gt;这些力系统地打破连续参数对称性并保持离散对称性，导致一系列类似于热系统等分性质的重力平衡现象。这些现象（a）解释了AI模型之间神经表示的普遍一致性，并证明了柏拉图表示假设；（b）调和了深度学习优化中寻求尖锐和平坦行为的看似矛盾观察。&lt;h4&gt;结论&lt;/h4&gt;熵力和对称性破坏的结合是理解深度学习中涌现现象的关键。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着深度学习和大型语言模型中涌现现象的快速发现，解释和理解其成因已成为迫切需要。在这里，我们提出了一种严格的熵力理论来理解使用随机梯度下降（SGD）及其变体训练的神经网络的动态学习过程。基于参数对称性和非熵损失景观理论，我们表明表示学习被由随机性和离散时间更新引起的涌现熵力所关键控制。这些力系统地打破连续参数对称性并保持离散对称性，导致一系列类似于热系统等分性质的重力平衡现象。这些现象反过来（a）解释了AI模型之间神经表示的普遍一致性，并证明了柏拉图表示假设；（b）调和了深度学习优化中寻求尖锐和平坦行为的看似矛盾观察。我们的理论和实验表明，熵力和对称性破坏的结合是理解深度学习中涌现现象的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid discovery of emergent phenomena in deep learning and largelanguage models, explaining and understanding their cause has become an urgentneed. Here, we propose a rigorous entropic-force theory for understanding thelearning dynamics of neural networks trained with stochastic gradient descent(SGD) and its variants. Building on the theory of parameter symmetries and anentropic loss landscape, we show that representation learning is cruciallygoverned by emergent entropic forces arising from stochasticity anddiscrete-time updates. These forces systematically break continuous parametersymmetries and preserve discrete ones, leading to a series of gradient balancephenomena that resemble the equipartition property of thermal systems. Thesephenomena, in turn, (a) explain the universal alignment of neuralrepresentations between AI models and lead to a proof of the PlatonicRepresentation Hypothesis, and (b) reconcile the seemingly contradictoryobservations of sharpness- and flatness-seeking behavior of deep learningoptimization. Our theory and experiments demonstrate that a combination ofentropic forces and symmetry breaking is key to understanding emergentphenomena in deep learning.</description>
      <author>example@mail.com (Liu Ziyin, Yizhou Xu, Isaac Chuang)</author>
      <guid isPermaLink="false">2505.12387v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Residual Feature Integration is Sufficient to Prevent Negative Transfer</title>
      <link>http://arxiv.org/abs/2505.11771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Residual Feature Integration (REFINE)的简单有效的方法，旨在减轻迁移学习中的负迁移问题。&lt;h4&gt;背景&lt;/h4&gt;迁移学习通常利用源域学习到的表示来提高目标任务的性能。然而，直接应用预训练模型提取的特征可能导致负迁移，即源域表示与目标分布不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出REFINE方法，以减轻迁移学习中的负迁移。&lt;h4&gt;方法&lt;/h4&gt;REFINE方法结合了固定的源域表示和可训练的目标域编码器，并在联合表示上拟合一个浅层神经网络，以适应目标域同时保留源域的可迁移知识。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，证明了在轻微条件下REFINE足以防止负迁移，并推导了泛化界限以展示其理论优势。实验表明，REFINE在视觉、文本和表格数据等多种应用和数据模态中一致地提高了性能，并优于许多替代方案。&lt;h4&gt;结论&lt;/h4&gt;REFINE方法轻量级、架构无关且鲁棒，是现有迁移学习工具箱中的一个有价值的补充。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning typically leverages representations learned from a sourcedomain to improve performance on a target task. A common approach is to extractfeatures from a pre-trained model and directly apply them for targetprediction. However, this strategy is prone to negative transfer where thesource representation fails to align with the target distribution. In thisarticle, we propose Residual Feature Integration (REFINE), a simple yeteffective method designed to mitigate negative transfer. Our approach combinesa fixed source-side representation with a trainable target-side encoder andfits a shallow neural network on the resulting joint representation, whichadapts to the target domain while preserving transferable knowledge from thesource domain. Theoretically, we prove that REFINE is sufficient to preventnegative transfer under mild conditions, and derive the generalization bounddemonstrating its theoretical benefit. Empirically, we show that REFINEconsistently enhances performance across diverse application and datamodalities including vision, text, and tabular data, and outperforms numerousalternative solutions. Our method is lightweight, architecture-agnostic, androbust, making it a valuable addition to the existing transfer learningtoolbox.</description>
      <author>example@mail.com (Yichen Xu, Ryumei Nakada, Linjun Zhang, Lexin Li)</author>
      <guid isPermaLink="false">2505.11771v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Generative and Contrastive Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2505.11776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图自监督学习架构，该架构结合了对比学习和生成学习的优势，在节点分类、节点聚类和链接预测等任务上实现了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在图上生成节点和图表示（即嵌入），可用于下游任务。在有限或没有标记数据的场景中，图自监督学习特别有用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图自监督学习架构，以提高节点分类、聚类和链接预测等任务的性能。&lt;h4&gt;方法&lt;/h4&gt;该架构引入了社区感知的节点级对比学习，以提供更稳健和有效的正负节点对生成，同时结合图级对比学习来捕获全局语义信息。此外，采用了一种综合的增强策略，结合特征掩码、节点扰动和边扰动，以实现稳健和多样化的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在多个任务上实现了优越的性能，包括节点分类、聚类和链接预测。在公开基准数据集上的评估表明，该模型优于最先进的方法，性能提升在0.23%-2.01%之间，具体取决于任务和数据集。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型通过结合对比学习和生成学习的优势，在图自监督学习领域取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) on graphs generates node and graphrepresentations (i.e., embeddings) that can be used for downstream tasks suchas node classification, node clustering, and link prediction. Graph SSL isparticularly useful in scenarios with limited or no labeled data. Existing SSLmethods predominantly follow contrastive or generative paradigms, eachexcelling in different tasks: contrastive methods typically perform well onclassification tasks, while generative methods often excel in link prediction.In this paper, we present a novel architecture for graph SSL that integratesthe strengths of both approaches. Our framework introduces community-awarenode-level contrastive learning, providing more robust and effective positiveand negative node pairs generation, alongside graph-level contrastive learningto capture global semantic information. Additionally, we employ a comprehensiveaugmentation strategy that combines feature masking, node perturbation, andedge perturbation, enabling robust and diverse representation learning. Byincorporating these enhancements, our model achieves superior performanceacross multiple tasks, including node classification, clustering, and linkprediction. Evaluations on open benchmark datasets demonstrate that our modeloutperforms state-of-the-art methods, achieving a performance lift of0.23%-2.01% depending on the task and dataset.</description>
      <author>example@mail.com (Jiali Chen, Avijit Mukherjee)</author>
      <guid isPermaLink="false">2505.11776v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
      <link>http://arxiv.org/abs/2505.12332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VoiceCloak是一种针对扩散模型（DMs）的多维度主动防御框架，旨在混淆说话人身份并降低潜在未授权语音克隆的感知质量。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在现实语音克隆（VC）中取得了显著成功，但也增加了恶意滥用的风险。&lt;h4&gt;目的&lt;/h4&gt;VoiceCloak的目标是混淆说话人身份并降低潜在未授权VC的感知质量。&lt;h4&gt;方法&lt;/h4&gt;VoiceCloak通过分析DMs中的特定漏洞，在参考音频中引入对抗性扰动来干扰克隆过程。它通过扭曲表示学习嵌入来最大化身份变化，并干扰关键的条件引导过程，特别是注意力上下文。此外，VoiceCloak还引入了分数幅度放大和噪声引导语义破坏来降低输出质量。&lt;h4&gt;主要发现&lt;/h4&gt;VoiceCloak在防御未授权基于扩散的语音克隆方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;VoiceCloak是一种有效的防御框架，可以降低扩散模型在语音克隆中的恶意滥用风险。&lt;h4&gt;翻译&lt;/h4&gt;VoiceCloak is a multi-dimensional proactive defense framework for diffusion models (DMs) aimed at obfuscating speaker identity and degrading the perceptual quality in potential unauthorized voice cloning. The framework analyzes specific vulnerabilities within DMs to disrupt the cloning process by introducing adversarial perturbations into the reference audio. It distorts representation learning embeddings to maximize identity variation and disrupts crucial conditional guidance processes, particularly attention context. Additionally, it introduces score magnitude amplification and noise-guided semantic corruption to degrade output quality. Extensive experiments highlight the outstanding defense success rate of VoiceCloak against unauthorized diffusion-based voice cloning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion Models (DMs) have achieved remarkable success in realistic voicecloning (VC), while they also increase the risk of malicious misuse. Existingproactive defenses designed for traditional VC models aim to disrupt theforgery process, but they have been proven incompatible with DMs due to theintricate generative mechanisms of diffusion. To bridge this gap, we introduceVoiceCloak, a multi-dimensional proactive defense framework with the goal ofobfuscating speaker identity and degrading perceptual quality in potentialunauthorized VC. To achieve these goals, we conduct a focused analysis toidentify specific vulnerabilities within DMs, allowing VoiceCloak to disruptthe cloning process by introducing adversarial perturbations into the referenceaudio. Specifically, to obfuscate speaker identity, VoiceCloak first targetsspeaker identity by distorting representation learning embeddings to maximizeidentity variation, which is guided by auditory perception principles.Additionally, VoiceCloak disrupts crucial conditional guidance processes,particularly attention context, thereby preventing the alignment of vocalcharacteristics that are essential for achieving convincing cloning. Then, toaddress the second objective, VoiceCloak introduces score magnitudeamplification to actively steer the reverse trajectory away from the generationof high-quality speech. Noise-guided semantic corruption is further employed todisrupt structural speech semantics captured by DMs, degrading output quality.Extensive experiments highlight VoiceCloak's outstanding defense success rateagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloakare available at https://voice-cloak.github.io/VoiceCloak/.</description>
      <author>example@mail.com (Qianyue Hu, Junyan Wu, Wei Lu, Xiangyang Luo)</author>
      <guid isPermaLink="false">2505.12332v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Model alignment using inter-modal bridges</title>
      <link>http://arxiv.org/abs/2505.12322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于条件流匹配的半监督模型对齐方法，用于解决不同模态（如文本和视觉）之间的模型重用问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法在跨模态模型重用方面存在局限性，因为难以对齐内部表示，且需要大量配对训练数据或局限于特定领域。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种数据高效的跨模态模型对齐方法，以最小监督实现。&lt;h4&gt;方法&lt;/h4&gt;通过以下两种设置学习不同模态潜在空间之间的条件流：(1) 通过空间桥接成本解决平衡或不平衡的最优传输问题；(2) 使用标记的示例进行内存高效的对齐。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MNIST、ImageNet和majaj2015simple数据集上的对象识别和图像生成任务中，与端到端训练模型相比，在标记训练数据稀缺（&lt;20%）的情况下，匹配了下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为跨模态模型对齐提供了一种数据高效的解决方案，即使在少量监督下也能实现良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have demonstrated remarkable performance across modalitiessuch as language and vision. However, model reuse across distinct modalities(e.g., text and vision) remains limited due to the difficulty of aligninginternal representations. Existing methods require extensive paired trainingdata or are constrained to specific domains. We introduce a semi-supervisedapproach for model alignment via conditional flow matching. The conditionalflow between latent spaces of different modalities (e.g., text-to-image orbiological-to-artificial neuronal activity) can be learned in two settings:($1$) solving a (balanced or unbalanced) optimal transport problem with aninter-space bridge cost, and ($2$) performing memory-efficient alignment usinglabelled exemplars. Despite being constrained by the original models' capacity,our method--under both settings--matches downstream task performance ofend-to-end trained models on object recognition and image generation tasksacross MNIST, ImageNet, and \cite{majaj2015simple} datasets, particularly whenlabelled training data is scarce ($&lt;20\%$). Our method provides adata-efficient solution for inter-modal model alignment with minimalsupervision.</description>
      <author>example@mail.com (Ali Gholamzadeh, Noor Sajid)</author>
      <guid isPermaLink="false">2505.12322v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing</title>
      <link>http://arxiv.org/abs/2505.11743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大规模语言模型（LLM）的AI框架，用于智能云系统故障检测和自愈机制。&lt;h4&gt;背景&lt;/h4&gt;随着云计算系统和其基础设施的快速发展和复杂性增加，实时检测和缓解故障的智能机制变得日益重要。&lt;h4&gt;目的&lt;/h4&gt;研究目的是开发一种能够有效处理现代云环境规模和动态的传统故障检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了现有的机器学习故障检测算法和LLM的自然语言理解能力，通过语义上下文处理和解析系统日志、错误报告和实时数据流。&lt;h4&gt;主要发现&lt;/h4&gt;该模型采用多层次架构，结合监督学习进行故障分类和无监督学习进行异常检测，能够在故障发生前预测潜在故障并自动触发自愈机制。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，所提出的模型在故障检测精度、系统停机时间减少和恢复速度方面均显著优于传统故障检测系统。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of cloud computing systems and the increasing complexity of their infrastructure, intelligent mechanisms to detect and mitigate failures in real time are becoming increasingly important. Traditional methods of failure detection are often difficult to cope with the scale and dynamics of modern cloud environments. In this study, we propose a novel AI framework based on Massive Language Model (LLM) for intelligent fault detection and self-healing mechanisms in cloud systems. The model combines existing machine learning fault detection algorithms with LLM's natural language understanding capabilities to process and parse system logs, error reports, and real-time data streams through semantic context. The method adopts a multi-level architecture, combined with supervised learning for fault classification and unsupervised learning for anomaly detection, so that the system can predict potential failures before they occur and automatically trigger the self-healing mechanism. Experimental results show that the proposed model is significantly better than the traditional fault detection system in terms of fault detection accuracy, system downtime reduction and recovery speed.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of cloud computing systems and the increasingcomplexity of their infrastructure, intelligent mechanisms to detect andmitigate failures in real time are becoming increasingly important. Traditionalmethods of failure detection are often difficult to cope with the scale anddynamics of modern cloud environments. In this study, we propose a novel AIframework based on Massive Language Model (LLM) for intelligent fault detectionand self-healing mechanisms in cloud systems. The model combines existingmachine learning fault detection algorithms with LLM's natural languageunderstanding capabilities to process and parse system logs, error reports, andreal-time data streams through semantic context. The method adopts amulti-level architecture, combined with supervised learning for faultclassification and unsupervised learning for anomaly detection, so that thesystem can predict potential failures before they occur and automaticallytrigger the self-healing mechanism. Experimental results show that the proposedmodel is significantly better than the traditional fault detection system interms of fault detection accuracy, system downtime reduction and recoveryspeed.</description>
      <author>example@mail.com (Cheng Ji, Huaiying Luo)</author>
      <guid isPermaLink="false">2505.11743v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI2025 early accept&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PRETI的视网膜基础模型，通过集成元数据感知学习和鲁棒的自监督表示学习，显著提高了视网膜图像分析的能力。&lt;h4&gt;背景&lt;/h4&gt;视网膜图像分析在疾病诊断中非常重要，但依赖大量标注数据且获取临床报告成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合患者特定信息并减少对标注数据依赖的视网膜图像分析模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Learnable Metadata Embedding (LME)的元数据嵌入方法，以及一个名为Retina-Aware Adaptive Masking (RAAM)的策略。此外，构建了患者级别的数据对来提高模型对非临床变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;PRETI模型能够捕捉视网膜图像的全球结构和精细病理细节，在多种疾病和生物标志物预测任务中实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;元数据指导的基础模型在视网膜疾病分析中具有重要意义，PRETI模型展示了其在不同疾病和生物标志物预测中的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：视网膜基础模型通过利用自监督学习显著推进了视网膜图像分析，减少了对标注数据的依赖并实现了强大的泛化能力。许多最近的方法通过报告监督增强了视网膜图像的理解，但获取临床报告往往成本高且具有挑战性。相比之下，元数据（如年龄、性别）广泛可用，是分析疾病进展的有价值资源。为了有效地整合患者特定的信息，我们提出了PRETI，一种集成元数据感知学习与鲁棒的自监督表示学习的视网膜基础模型。我们引入了可学习的元数据嵌入（LME），它可以动态地细化元数据表示。此外，我们构建了患者级别的数据对，将同一个体的图像关联起来以提高对非临床变化的鲁棒性。为了进一步优化视网膜图像表示，我们提出了视网膜感知自适应掩码（RAAM）策略，该策略在视网膜区域内选择性地应用掩码并在训练过程中动态调整掩码比率。PRETI能够捕捉全局结构和精细病理细节，从而实现了优越的诊断性能。广泛的实验表明，PRETI在内部和公共数据上实现了最先进的跨多种疾病和生物标志物预测结果，表明元数据引导的基础模型在视网膜疾病分析中的重要性。我们的代码和预训练模型可在https://github.com/MICV-yonsei/PRETI上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retinal foundation models have significantly advanced retinal image analysisby leveraging self-supervised learning to reduce dependence on labeled datawhile achieving strong generalization. Many recent approaches enhance retinalimage understanding using report supervision, but obtaining clinical reports isoften costly and challenging. In contrast, metadata (e.g., age, gender) iswidely available and serves as a valuable resource for analyzing diseaseprogression. To effectively incorporate patient-specific information, wepropose PRETI, a retinal foundation model that integrates metadata-awarelearning with robust self-supervised representation learning. We introduceLearnable Metadata Embedding (LME), which dynamically refines metadatarepresentations. Additionally, we construct patient-level data pairs,associating images from the same individual to improve robustness againstnon-clinical variations. To further optimize retinal image representation, wepropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectivelyapplies masking within the retinal region and dynamically adjusts the maskingratio during training. PRETI captures both global structures and fine-grainedpathological details, resulting in superior diagnostic performance. Extensiveexperiments demonstrate that PRETI achieves state-of-the-art results acrossdiverse diseases and biomarker predictions using in-house and public data,indicating the importance of metadata-guided foundation models in retinaldisease analysis. Our code and pretrained model are available athttps://github.com/MICV-yonsei/PRETI</description>
      <author>example@mail.com (Yeonkyung Lee, Woojung Han, Youngjun Jun, Hyeonmin Kim, Jungkyung Cho, Seong Jae Hwang)</author>
      <guid isPermaLink="false">2505.12233v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology</title>
      <link>http://arxiv.org/abs/2505.12120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HISTAI数据集，这是一个大型的、多模态的开放访问的病理图像数据集，旨在解决现有公开数据集规模不足、组织多样性不足和临床元数据不全面的问题。&lt;h4&gt;背景&lt;/h4&gt;数字病理学（DP）领域通过人工智能和基础模型取得了进展，强调了大规模、多样化和丰富注释数据集的重要性。然而，现有的公开全切片图像（WSI）数据集往往缺乏足够的规模、组织多样性和全面的临床元数据，限制了AI模型的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;引入HISTAI数据集，以填补现有资源的空白，促进创新、可重复性和临床相关计算病理学解决方案的发展。&lt;h4&gt;方法&lt;/h4&gt;HISTAI数据集包含超过60,000张来自各种组织类型的切片，每个案例都伴随着广泛的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。&lt;h4&gt;主要发现&lt;/h4&gt;HISTAI数据集提供了大规模、多样化和丰富注释的病理图像，有助于提高AI模型的性能和临床应用价值。&lt;h4&gt;结论&lt;/h4&gt;HISTAI数据集的发布为数字病理学领域提供了宝贵的资源，有助于推动相关研究的进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，数字病理学（DP）领域，特别是通过人工智能和基础模型，强调了大规模、多样化和丰富注释数据集的重要性。尽管它们起着关键作用，但公开可用的全切片图像（WSI）数据集通常缺乏足够的规模、组织多样性和全面的临床元数据，限制了AI模型的鲁棒性和泛化能力。为此，我们引入了HISTAI数据集，这是一个大型、多模态、开放访问的WSI数据集，包含来自各种组织类型的60,000多张切片。HISTAI数据集中的每个案例都伴随着广泛的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。该数据集旨在填补现有资源的空白，促进创新、可重复性和临床相关计算病理学解决方案的发展。数据集可通过https://github.com/HistAI/HISTAI访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/histai/histai&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Digital Pathology (DP), particularly throughartificial intelligence and Foundation Models, have underscored the importanceof large-scale, diverse, and richly annotated datasets. Despite their criticalrole, publicly available Whole Slide Image (WSI) datasets often lack sufficientscale, tissue diversity, and comprehensive clinical metadata, limiting therobustness and generalizability of AI models. In response, we introduce theHISTAI dataset, a large, multimodal, open-access WSI collection comprising over60,000 slides from various tissue types. Each case in the HISTAI dataset isaccompanied by extensive clinical metadata, including diagnosis, demographicinformation, detailed pathological annotations, and standardized diagnosticcoding. The dataset aims to fill gaps identified in existing resources,promoting innovation, reproducibility, and the development of clinicallyrelevant computational pathology solutions. The dataset can be accessed athttps://github.com/HistAI/HISTAI.</description>
      <author>example@mail.com (Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova)</author>
      <guid isPermaLink="false">2505.12120v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics</title>
      <link>http://arxiv.org/abs/2505.11930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了逻辑对时间图神经网络（Temporal GNNs）的表征，通过将它们与二维乘积逻辑联系起来，揭示了时间GNNs的表达能力取决于图和时序组件的结合方式。&lt;h4&gt;背景&lt;/h4&gt;近年来，逻辑和形式语言理论工具被用来描述各种神经网络架构的表达能力，包括图神经网络（GNNs）、Transformer和循环神经网络。随着基本架构能力的逐渐明确，越来越多的关注转向结合多种架构范式的模型。&lt;h4&gt;目的&lt;/h4&gt;本文旨在研究时间GNNs的逻辑表征，并分析其表达能力。&lt;h4&gt;方法&lt;/h4&gt;通过将时间GNNs与二维乘积逻辑相联系，分析了不同时间GNN架构的表达能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，应用静态GNNs递归地随时间变化的时序GNNs可以捕捉所有在命题时态逻辑PTL和模态逻辑K中定义的属性。而像图-时TGNN和全局TGNN这样的架构只能表达这个逻辑的子集，其中时序和空间操作符之间的交互在语法上受到限制。&lt;h4&gt;结论&lt;/h4&gt;这些结果为时间GNNs提供了首次逻辑表征，并确立了时间GNNs的新相对表达能力结果。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, the expressive power of various neural architectures --including graph neural networks (GNNs), transformers, and recurrent neuralnetworks -- has been characterised using tools from logic and formal languagetheory. As the capabilities of basic architectures are becoming wellunderstood, increasing attention is turning to models that combine multiplearchitectural paradigms. Among them particularly important, and challenging toanalyse, are temporal extensions of GNNs, which integrate both spatial(graph-structure) and temporal (evolution over time) dimensions. In this paper,we initiate the study of logical characterisation of temporal GNNs byconnecting them to two-dimensional product logics. We show that the expressivepower of temporal GNNs depends on how graph and temporal components arecombined. In particular, temporal GNNs that apply static GNNs recursively overtime can capture all properties definable in the product logic of (past)propositional temporal logic PTL and the modal logic K. In contrast,architectures such as graph-and-time TGNNs and global TGNNs can only expressrestricted fragments of this logic, where the interaction between temporal andspatial operators is syntactically constrained. These results yield the firstlogical characterisations of temporal GNNs and establish new relativeexpressiveness results for temporal GNNs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the expressive power of various neural architectures --including graph neural networks (GNNs), transformers, and recurrent neuralnetworks -- has been characterised using tools from logic and formal languagetheory. As the capabilities of basic architectures are becoming wellunderstood, increasing attention is turning to models that combine multiplearchitectural paradigms. Among them particularly important, and challenging toanalyse, are temporal extensions of GNNs, which integrate both spatial(graph-structure) and temporal (evolution over time) dimensions. In this paper,we initiate the study of logical characterisation of temporal GNNs byconnecting them to two-dimensional product logics. We show that the expressivepower of temporal GNNs depends on how graph and temporal components arecombined. In particular, temporal GNNs that apply static GNNs recursively overtime can capture all properties definable in the product logic of (past)propositional temporal logic PTL and the modal logic K. In contrast,architectures such as graph-and-time TGNNs and global TGNNs can only expressrestricted fragments of this logic, where the interaction between temporal andspatial operators is syntactically constrained. These results yield the firstlogical characterisations of temporal GNNs and establish new relativeexpressiveness results for temporal GNNs.</description>
      <author>example@mail.com (Marco Sälzer, Przemysław Andrzej Wałęga, Martin Lange)</author>
      <guid isPermaLink="false">2505.11930v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Structured Representation</title>
      <link>http://arxiv.org/abs/2505.12143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了不变表示在表征学习中的核心作用，并提出了关于不变性的稳定性、可迁移性和对任务相关信号的影响的问题。&lt;h4&gt;背景&lt;/h4&gt;不变表示是表征学习的关键，但如何发现既稳定又可迁移的不变性，同时不抑制任务相关信号，仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;研究如何确定不变性应定义的适当抽象层次，以及它们应表征系统的哪些方面。&lt;h4&gt;方法&lt;/h4&gt;本文提出，解释操作在更高阶的关系知识层面上进行，因此不变结构必须位于知识所在之处，具体来说，是在抽象知识空间中由关系路径的闭包定义的分区。&lt;h4&gt;主要发现&lt;/h4&gt;这些分区作为核心的不变表示，形成了知识存储和学习发生的结构基础。分区之间的连接器允许部署这些知识分区，编码任务相关的转换。&lt;h4&gt;结论&lt;/h4&gt;不变分区提供了结构表示的基本原理。基于闭半环，一种关系代数结构，本文正式化了不变分区结构表示的计算基础。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了不变表示在表征学习中的核心作用，并提出了关于不变性的稳定性、可迁移性和对任务相关信号的影响的问题。不变表示是表征学习的关键，但如何发现既稳定又可迁移的不变性，同时不抑制任务相关信号，仍然是一个挑战。研究如何确定不变性应定义的适当抽象层次，以及它们应表征系统的哪些方面。本文提出，解释操作在更高阶的关系知识层面上进行，因此不变结构必须位于知识所在之处，具体来说，是在抽象知识空间中由关系路径的闭包定义的分区。这些分区作为核心的不变表示，形成了知识存储和学习发生的结构基础。分区之间的连接器允许部署这些知识分区，编码任务相关的转换。不变分区提供了结构表示的基本原理。基于闭半环，一种关系代数结构，本文正式化了不变分区结构表示的计算基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Invariant representations are core to representation learning, yet a centralchallenge remains: uncovering invariants that are stable and transferablewithout suppressing task-relevant signals. This raises fundamental questions,requiring further inquiry, about the appropriate level of abstraction at whichsuch invariants should be defined, and which aspects of a system they shouldcharacterize. Interpretation of the environment relies on abstract knowledgestructures to make sense of the current state, which leads to interactions,essential drivers of learning and knowledge acquisition. We posit thatinterpretation operates at the level of higher-order relational knowledge;hence, invariant structures must be where knowledge resides, specifically, aspartitions defined by the closure of relational paths within an abstractknowledge space. These partitions serve as the core invariant representations,forming the structural substrate where knowledge is stored and learning occurs.On the other hand, inter-partition connectors enable the deployment of theseknowledge partitions encoding task-relevant transitions. Thus, invariantpartitions provide the foundational primitives of structured representation. Weformalize the computational foundations for structured representation of theinvariant partitions based on closed semiring, a relational algebraicstructure.</description>
      <author>example@mail.com (Arun Kumar, Paul Schrater)</author>
      <guid isPermaLink="false">2505.12143v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EarthSynth: Generating Informative Earth Observation with Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.12108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EarthSynth的扩散式生成基础模型，用于解决遥感图像（RSI）解释中由于标签数据稀缺导致的挑战，通过合成多类别、跨卫星的地球观测数据来提升RSI解释任务的性能。&lt;h4&gt;背景&lt;/h4&gt;RSI解释任务面临挑战，主要是因为标签数据的稀缺性，这限制了任务的性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来解决RSI解释中的标签数据稀缺问题，提升下游任务的性能。&lt;h4&gt;方法&lt;/h4&gt;EarthSynth模型基于EarthSynth-180K数据集，采用反事实组合训练策略来提高训练数据的多样性并增强类别控制。此外，还提出了基于规则的R-Filter方法来过滤更多信息性的合成数据。&lt;h4&gt;主要发现&lt;/h4&gt;EarthSynth是首个探索遥感任务多任务生成的模型，其在场景分类、物体检测和语义分割任务上的评估表明，它为提高RSI解释提供了实用解决方案。&lt;h4&gt;结论&lt;/h4&gt;EarthSynth模型为RSI解释提供了有效的方法，能够通过合成数据解决标签数据稀缺的问题，并提升下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：遥感图像（RSI）的解释通常面临着由于标签数据稀缺所带来的挑战，这限制了RSI解释任务的性能。为了应对这一挑战，我们提出了一种基于扩散的生成基础模型，名为EarthSynth，它能够合成用于下游RSI解释任务的多类别、跨卫星的地球观测数据。据我们所知，EarthSynth是首个探索遥感任务多任务生成的模型。EarthSynth在EarthSynth-180K数据集上训练，采用了反事实组合训练策略来提高训练数据的多样性并增强类别控制。此外，还提出了一种基于规则的R-Filter方法来过滤更多信息性的合成数据。我们在开放世界的场景中对EarthSynth进行了场景分类、物体检测和语义分割的评估，为RSI解释的进步提供了实用解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing image (RSI) interpretation typically faces challenges due tothe scarcity of labeled data, which limits the performance of RSIinterpretation tasks. To tackle this challenge, we propose EarthSynth, adiffusion-based generative foundation model that enables synthesizingmulti-category, cross-satellite labeled Earth observation for downstream RSIinterpretation tasks. To the best of our knowledge, EarthSynth is the first toexplore multi-task generation for remote sensing. EarthSynth, trained on theEarthSynth-180K dataset, employs the Counterfactual Composition trainingstrategy to improve training data diversity and enhance category control.Furthermore, a rule-based method of R-Filter is proposed to filter moreinformative synthetic data for downstream tasks. We evaluate our EarthSynth onscene classification, object detection, and semantic segmentation in open-worldscenarios, offering a practical solution for advancing RSI interpretation.</description>
      <author>example@mail.com (Jiancheng Pan, Shiye Lei, Yuqian Fu, Jiahao Li, Yanxing Liu, Yuze Sun, Xiao He, Long Peng, Xiaomeng Huang, Bo Zhao)</author>
      <guid isPermaLink="false">2505.12108v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Evaluation and optimization of deep learning models for enhanced detection of brain cancer using transmission optical microscopy of thin brain tissue samples</title>
      <link>http://arxiv.org/abs/2505.11735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过深度卷积神经网络（CNN）在脑组织活检样本上的应用，评估了ResNet50和DenseNet121在光学传输光谱学分析中的性能，并与传统方法进行了比较。&lt;h4&gt;背景&lt;/h4&gt;光学传输光谱学是分析脑组织结构的一种方法，但手动解释资源密集且易受观察者主观影响。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习技术自动化分析脑组织活检样本的光学图像，提高分析效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet50和DenseNet121在2,931张脑组织透明光学显微镜图像上进行训练和测试，包括1,996张用于训练，437张用于验证，498张用于测试。采用两阶段迁移学习协议，包括在冻结的预训练特征提取器上训练分类头，随后使用数据增强（旋转、翻转、强度抖动）和早停技术微调更深的卷积块。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121在测试集上达到88.35%的准确率，0.9614的精确度，0.8667的召回率和0.9116的F1分数，优于ResNet50。通过混淆矩阵、训练和验证曲线以及类别预测分布的详细分析，说明了模型的鲁棒收敛和最小偏差。&lt;h4&gt;结论&lt;/h4&gt;DenseNet121在有限医疗数据集上显示出优于ResNet50的性能，为多类别肿瘤分级和临床转化提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：光传输光谱学是理解脑组织结构特性的方法之一，但手动解读资源密集且易受观察者之间差异的影响。深度卷积神经网络（CNN）可以直接从原始明场图像中学习特征。在本研究中，我们对ResNet50和DenseNet121在2,931张薄脑组织明场传输光学显微镜图像的精选数据集上的性能进行了评估，这些图像分为1,996张用于训练，437张用于验证，498张用于测试。我们的两阶段迁移学习协议包括在冻结的预训练特征提取器上对分类头进行初始训练，然后使用大量数据增强（旋转、翻转、强度抖动）和早停技术微调更深的卷积块。与ResNet50（82.12%，0.9035，0.8142，0.8563）相比，DenseNet121在测试集上实现了88.35%的准确率，0.9614的精确度，0.8667的召回率和0.9116的F1分数，表现最佳。通过混淆矩阵、训练和验证曲线以及类别预测分布的详细分析，说明了模型的鲁棒收敛和最小偏差。这些发现证明了在有限医疗数据集上稠密连接的优越泛化能力，并概述了多类别肿瘤分级和临床转化的未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical transmission spectroscopy is one method to understand brain tissuestructural properties from brain tissue biopsy samples, yet manualinterpretation is resource intensive and prone to inter observer variability.Deep convolutional neural networks (CNNs) offer automated feature learningdirectly from raw brightfield images. Here, we evaluate ResNet50 andDenseNet121 on a curated dataset of 2,931 bright-field transmission opticalmicroscopy images of thin brain tissue, split into 1,996 for training, 437 forvalidation, and 498 for testing. Our two stage transfer learning protocolinvolves initial training of a classifier head on frozen pretrained featureextractors, followed by fine tuning of deeper convolutional blocks withextensive data augmentation (rotations, flips, intensity jitter) and earlystopping. DenseNet121 achieves 88.35 percent test accuracy, 0.9614 precision,0.8667 recall, and 0.9116 F1 score the best performance compared to ResNet50(82.12 percent, 0.9035, 0.8142, 0.8563). Detailed analysis of confusionmatrices, training and validation curves, and classwise predictiondistributions illustrates robust convergence and minimal bias. These findingsdemonstrate the superior generalization of dense connectivity on limitedmedical datasets and outline future directions for multi-class tumor gradingand clinical translation.</description>
      <author>example@mail.com (Mohnish Sao, Mousa Alrubayan, Prabhakar Pradhan)</author>
      <guid isPermaLink="false">2505.11735v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction</title>
      <link>http://arxiv.org/abs/2505.11063v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Thought-Aligner的动态思维校正模块，用于提高基于LLM的自主代理在复杂多步任务中的行为安全。&lt;h4&gt;背景&lt;/h4&gt;LLM-based自主代理具有推理、工具调用和环境交互的能力，但内部思维过程可能引入风险，导致不可逆的安全事件。&lt;h4&gt;目的&lt;/h4&gt;为了解决长期行为轨迹中的安全对齐挑战，提出Thought-Aligner模块。&lt;h4&gt;方法&lt;/h4&gt;Thought-Aligner使用轻量级和资源高效的模型，在每次动作执行前实时纠正高风险思维，并重新引入到代理中，同时不改变代理框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Thought-Aligner将代理的行为安全从未受保护设置中的约50%提高到平均90%，同时保持响应延迟低于100ms。&lt;h4&gt;结论&lt;/h4&gt;Thought-Aligner为基于LLM的代理提供了一个实用的动态安全解决方案，具有高效部署、广泛适用和及时响应的能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a dynamic thought correction module named Thought-Aligner to improve the behavioral safety of autonomous agents based on LLMs. The background is that LLM-based autonomous agents have capabilities such as reasoning, tool invocation, and environment interaction, but the internal thinking process may introduce risks, leading to irreversible safety incidents. The purpose is to address the safety alignment challenges in long-horizon behavioral trajectories by proposing the Thought-Aligner module. The method uses a lightweight and resource-efficient model to correct high-risk thoughts on the fly before each action execution and reintroduce them to the agent, without altering the underlying agent framework. The main findings show that Thought-Aligner raises the behavioral safety of the agent from about 50% in the unprotected setting to an average of 90%, while maintaining a response latency below 100ms. The conclusion is that Thought-Aligner provides a practical dynamic safety solution for LLM-based agents, demonstrating its capability for efficient deployment, broad applicability, and timely responsiveness.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based autonomous agents possess capabilities such as reasoning, toolinvocation, and environment interaction, enabling the execution of complexmulti-step tasks. The internal reasoning process, i.e., thought, of behavioraltrajectory significantly influences tool usage and subsequent actions but canintroduce potential risks. Even minor deviations in the agent's thought maytrigger cascading effects leading to irreversible safety incidents. To addressthe safety alignment challenges in long-horizon behavioral trajectories, wepropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizinga lightweight and resource-efficient model, Thought-Aligner corrects eachhigh-risk thought on the fly before each action execution. The correctedthought is then reintroduced to the agent, ensuring safer subsequent decisionsand tool interactions. Importantly, Thought-Aligner modifies only the reasoningphase without altering the underlying agent framework, making it easy to deployand widely applicable to various agent frameworks. To train the Thought-Alignermodel, we construct an instruction dataset across ten representative scenariosand simulate ReAct execution trajectories, generating 5,000 diverseinstructions and more than 11,400 safe and unsafe thought pairs. The model isfine-tuned using contrastive learning techniques. Experiments across threeagent safety benchmarks involving 12 different LLMs demonstrate thatThought-Aligner raises agent behavioral safety from approximately 50% in theunprotected setting to 90% on average. Additionally, Thought-Aligner maintainsresponse latency below 100ms with minimal resource usage, demonstrating itscapability for efficient deployment, broad applicability, and timelyresponsiveness. This method thus provides a practical dynamic safety solutionfor the LLM-based agents.</description>
      <author>example@mail.com (Changyue Jiang, Xudong Pan, Min Yang)</author>
      <guid isPermaLink="false">2505.11063v2</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming</title>
      <link>http://arxiv.org/abs/2505.11710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合图神经网络近似动态规划和进化多样性优化的协同进化防御框架，用于提高Active Directory的安全性。&lt;h4&gt;背景&lt;/h4&gt;现代企业网络越来越依赖Active Directory进行身份和访问管理，但其集中化特性使得攻击者可以攻击高价值资产。&lt;h4&gt;目的&lt;/h4&gt;为了应对动态攻击者的适应性行为，本文旨在提出一种能够适应攻击者策略变化的防御框架。&lt;h4&gt;方法&lt;/h4&gt;该框架将攻击者和防御者在Active Directory中的交互建模为一个Stackelberg博弈，并结合GNNDP和EDO来生成鲁棒的阻止策略。为了确保可扩展性，引入了FPT图减少方法以降低复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架在合成AD图上取得了接近最优的结果，并且在更大的图上（r1000和r2000）表现出了改进的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架具有可扩展性和有效性，能够提高Active Directory的安全性并防止过早收敛。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Modern enterprise networks increasingly rely on Active Directory (AD) for identity and access management. However, this centralization exposes a single point of failure, allowing adversaries to compromise high-value assets. Existing AD defense approaches often assume static attacker behavior, but real-world adversaries adapt dynamically, rendering such methods brittle. To address this, we model attacker-defender interactions in AD as a Stackelberg game between an adaptive attacker and a proactive defender. We propose a co-evolutionary defense framework that combines Graph Neural Network Approximated Dynamic Programming (GNNDP) to model attacker strategies, with Evolutionary Diversity Optimization (EDO) to generate resilient blocking strategies. To ensure scalability, we introduce a Fixed-Parameter Tractable (FPT) graph reduction method that reduces complexity while preserving strategic structure. Our framework jointly refines attacker and defender policies to improve generalization and prevent premature convergence. Experiments on synthetic AD graphs show near-optimal results (within 0.1 percent of optimality on r500) and improved performance on larger graphs (r1000 and r2000), demonstrating the framework's scalability and effectiveness.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern enterprise networks increasingly rely on Active Directory (AD) foridentity and access management. However, this centralization exposes a singlepoint of failure, allowing adversaries to compromise high-value assets.Existing AD defense approaches often assume static attacker behavior, butreal-world adversaries adapt dynamically, rendering such methods brittle. Toaddress this, we model attacker-defender interactions in AD as a Stackelberggame between an adaptive attacker and a proactive defender. We propose aco-evolutionary defense framework that combines Graph Neural NetworkApproximated Dynamic Programming (GNNDP) to model attacker strategies, withEvolutionary Diversity Optimization (EDO) to generate resilient blockingstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable(FPT) graph reduction method that reduces complexity while preserving strategicstructure. Our framework jointly refines attacker and defender policies toimprove generalization and prevent premature convergence. Experiments onsynthetic AD graphs show near-optimal results (within 0.1 percent of optimalityon r500) and improved performance on larger graphs (r1000 and r2000),demonstrating the framework's scalability and effectiveness.</description>
      <author>example@mail.com (Diksha Goel, Hussain Ahmad, Kristen Moore, Mingyu Guo)</author>
      <guid isPermaLink="false">2505.11710v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2505.11803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种适用于时间超关系知识图谱的VITA学习方法，以解决传统链接预测技术在处理时间有效性和无限有效事实时的不足。&lt;h4&gt;背景&lt;/h4&gt;知识图谱在管理动态变化的事实方面非常有效，而事实的时间有效性对于下游的链接预测任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出VITA方法，以提升链接预测性能，特别是在处理事实的时间有效性和时长信息方面。&lt;h4&gt;方法&lt;/h4&gt;VITA方法首先提出了一种灵活的时间表示，能够适应事实的四种时间有效性类型（即：自、至、期间、时间不变），然后设计VITA来有效地学习时间价值方面和时长方面的信息。&lt;h4&gt;主要发现&lt;/h4&gt;VITA在真实世界的知识图谱数据集上进行了彻底的评估，结果显示在预测缺失实体、关系、时间和其他数值字面量等链接预测任务中，VITA优于最佳基线，性能提升了75.3%。消融研究和案例研究也支持了关键设计选择。&lt;h4&gt;结论&lt;/h4&gt;VITA方法有效地提升了时间超关系知识图谱的链接预测性能，为处理动态变化的事实提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs (KGs) have become an effective paradigm for managingreal-world facts, which are not only complex but also dynamically evolve overtime. The temporal validity of facts often serves as a strong clue indownstream link prediction tasks, which predicts a missing element in a fact.Traditional link prediction techniques on temporal KGs either consider asequence of temporal snapshots of KGs with an ad-hoc defined time interval orexpand a temporal fact over its validity period under a predefined timegranularity; these approaches not only suffer from the sensitivity of theselection of time interval/granularity, but also face the computationalchallenges when handling facts with long (even infinite) validity. Although therecent hyper-relational KGs represent the temporal validity of a fact asqualifiers describing the fact, it is still suboptimal due to its ignorance ofthe infinite validity of some facts and the insufficient information encodedfrom the qualifiers about the temporal validity. Against this background, wepropose VITA, a $\underline{V}$ersatile t$\underline{I}$merepresen$\underline{TA}$tion learning method for temporal hyper-relationalknowledge graphs. We first propose a versatile time representation that canflexibly accommodate all four types of temporal validity of facts (i.e., since,until, period, time-invariant), and then design VITA to effectively learn thetime information in both aspects of time value and timespan to boost the linkprediction performance. We conduct a thorough evaluation of VITA compared to asizable collection of baselines on real-world KG datasets. Results show thatVITA outperforms the best-performing baselines in various link prediction tasks(predicting missing entities, relations, time, and other numeric literals) byup to 75.3%. Ablation studies and a case study also support our key designchoices.</description>
      <author>example@mail.com (ChongIn Un, Yuhuan Lu, Tianyue Yang, Dingqi Yang)</author>
      <guid isPermaLink="false">2505.11803v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Humble your Overconfident Networks: Unlearning Overfitting via Sequential Monte Carlo Tempered Deep Ensembles</title>
      <link>http://arxiv.org/abs/2505.11671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可扩展的SMC（序列蒙特卡洛）方法，通过结合SGHMC（随机梯度哈密顿蒙特卡洛）建议，提高了SMC的采样效率，并在图像分类、异常检测和迁移学习任务中优于标准SGD和深度集成方法。&lt;h4&gt;背景&lt;/h4&gt;传统的SMC方法在处理大规模数据时，由于需要全批量梯度评估而受到限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种可扩展的SMC方法，通过引入SGHMC建议，实现高效的迷你批量采样。&lt;h4&gt;方法&lt;/h4&gt;将SGHMC建议结合到SMC中，形成SMCSGHMC算法，并在不同任务中进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;SMCSGHMC算法在图像分类、异常检测和迁移学习任务中表现出色，且能有效减轻过拟合问题，提高模型校准。&lt;h4&gt;结论&lt;/h4&gt;SMCSGHMC为将预训练神经网络转换为校准良好的贝叶斯模型提供了一种灵活且可扩展的途径。&lt;h4&gt;翻译&lt;/h4&gt;Sequential Monte Carlo methods offer a principled approach to Bayesian uncertainty quantification but are traditionally limited by the need for full-batch gradient evaluations. We introduce a scalable variant by incorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals into SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC algorithm outperforms standard stochastic gradient descent (SGD) and deep ensembles across image classification, out-of-distribution (OOD) detection, and transfer learning tasks. We further show that SMCSGHMC mitigates overfitting and improves calibration, providing a flexible, scalable pathway for converting pretrained neural networks into well-calibrated Bayesian models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesianuncertainty quantification but are traditionally limited by the need forfull-batch gradient evaluations. We introduce a scalable variant byincorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposalsinto SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMCalgorithm outperforms standard stochastic gradient descent (SGD) and deepensembles across image classification, out-of-distribution (OOD) detection, andtransfer learning tasks. We further show that SMCSGHMC mitigates overfittingand improves calibration, providing a flexible, scalable pathway for convertingpretrained neural networks into well-calibrated Bayesian models.</description>
      <author>example@mail.com (Andrew Millard, Zheng Zhao, Joshua Murphy, Simon Maskell)</author>
      <guid isPermaLink="false">2505.11671v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Nearest Neighbor Multivariate Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.11625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于k近邻的多元时间序列(kNN-MTS)预测框架，通过在大数据存储中利用最近邻检索机制，提高了多元时间序列模型的预测性能。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测在工业和学术界有广泛的应用。近年来，空间时间图神经网络(STGNNs)在多元时间序列预测中变得流行，但现有的STGNNs由于计算复杂度限制，只能使用有限长度的输入数据，并且难以识别整个数据集中的相似模式。&lt;h4&gt;目的&lt;/h4&gt;设计一种简单有效的k近邻多元时间序列(kNN-MTS)预测框架，以解决现有方法的问题，并提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了kNN-MTS框架，该框架利用多元时间序列模型表示进行相似性搜索，无需额外训练，并扩展到对整个数据集的直接访问，同时设计了混合空间时间编码器(HSTEncoder)以捕捉长期时间依赖和短期空间时间依赖。&lt;h4&gt;主要发现&lt;/h4&gt;kNN-MTS框架在多个真实世界数据集上的实验结果表明，与现有方法相比，预测性能有显著提升。定量分析表明，kNN-MTS具有可解释性和效率，展现出更好的应用前景。&lt;h4&gt;结论&lt;/h4&gt;kNN-MTS框架为高效利用多元时间序列模型中的大数据集提供了一条新路径，具有广阔的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TNNLS.2024.3490603&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) forecasting has a wide range of applicationsin both industry and academia. Recently, spatial-temporal graph neural networks(STGNNs) have gained popularity as MTS forecasting methods. However, currentSTGNNs can only use the finite length of MTS input data due to thecomputational complexity. Moreover, they lack the ability to identify similarpatterns throughout the entire dataset and struggle with data that exhibitsparsely and discontinuously distributed correlations among variables over anextensive historical period, resulting in only marginal improvements. In thisarticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting( kNN-MTS) framework, which forecasts with a nearest neighbor retrievalmechanism over a large datastore of cached series, using representations fromthe MTS model for similarity search. This approach requires no additionaltraining and scales to give the MTS model direct access to the whole dataset attest time, resulting in a highly expressive model that consistently improvesperformance, and has the ability to extract sparse distributed but similarpatterns spanning over multivariables from the entire dataset. Furthermore, ahybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which cancapture both long-term temporal and short-term spatial-temporal dependenciesand is shown to provide accurate representation for kNN-MTSfor betterforecasting. Experimental results on several real-world datasets show asignificant improvement in the forecasting performance of kNN-MTS. Thequantitative analysis also illustrates the interpretability and efficiency ofkNN-MTS, showing better application prospects and opening up a new path forefficiently using the large dataset in MTS models.</description>
      <author>example@mail.com (Huiliang Zhang, Ping Nie, Lijun Sun, Benoit Boulet)</author>
      <guid isPermaLink="false">2505.11625v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.11781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于小波变换的深度时间序列预测方法，通过引入多阶小波导数变换（WDT）来改进频率表示学习，以提高时间序列预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;传统的傅里叶变换（FT）和小波变换（WT）在时间序列预测中广泛应用，但它们在捕捉多尺度、时间敏感的模式方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来捕捉时间序列中的多尺度、时间敏感的模式，并提高预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入了多阶小波导数变换（WDT），它基于小波变换，可以提取跨越整体趋势和微妙波动的时敏模式。将WDT嵌入到名为WaveTS的多分支框架中，该框架分解输入序列到多尺度时间频率系数，通过线性层进行细化，并通过逆WDT重构到时间域。&lt;h4&gt;主要发现&lt;/h4&gt;WDT通过操作序列的导数，有选择地放大变化率线索，并揭示对时间序列建模特别有信息量的突发状态转变。&lt;h4&gt;结论&lt;/h4&gt;在十个基准数据集上的广泛实验表明，WaveTS实现了最先进的预测精度，同时保持了高计算效率。&lt;h4&gt;翻译&lt;/h4&gt;在深度时间序列预测中，傅里叶变换（FT）广泛用于频率表示学习。然而，它往往难以捕捉多尺度、时间敏感的模式。尽管小波变换（WT）可以通过频率分解捕捉这些模式，但其系数对时间序列中的变化点不敏感，导致建模效果不佳。为了缓解这些限制，我们引入了基于WT的多阶小波导数变换（WDT），它能够提取跨越整体趋势和微妙波动的时敏模式。与建模原始序列的标准FT和WT相比，WDT操作序列的导数，有选择地放大变化率线索，并揭示对时间序列建模特别有信息量的突发状态转变。实际上，我们将WDT嵌入到名为WaveTS的多分支框架中，该框架将输入序列分解为多尺度时间频率系数，通过线性层进行细化，并通过逆WDT重构到时间域。在十个基准数据集上的广泛实验表明，WaveTS实现了最先进的预测精度，同时保持了高计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In deep time series forecasting, the Fourier Transform (FT) is extensivelyemployed for frequency representation learning. However, it often struggles incapturing multi-scale, time-sensitive patterns. Although the Wavelet Transform(WT) can capture these patterns through frequency decomposition, itscoefficients are insensitive to change points in time series, leading tosuboptimal modeling. To mitigate these limitations, we introduce themulti-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling theextraction of time-aware patterns spanning both the overall trend and subtlefluctuations. Compared with the standard FT and WT, which model the raw series,the WDT operates on the derivative of the series, selectively magnifyingrate-of-change cues and exposing abrupt regime shifts that are particularlyinformative for time series modeling. Practically, we embed the WDT into amulti-branch framework named WaveTS, which decomposes the input series intomulti-scale time-frequency coefficients, refines them via linear layers, andreconstructs them into the time domain via the inverse WDT. Extensiveexperiments on ten benchmark datasets demonstrate that WaveTS achievesstate-of-the-art forecasting accuracy while retaining high computationalefficiency.</description>
      <author>example@mail.com (Ziyu Zhou, Jiaxi Hu, Qingsong Wen, James T. Kwok, Yuxuan Liang)</author>
      <guid isPermaLink="false">2505.11781v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AoP-SAM: Automation of Prompts for Efficient Segmentation</title>
      <link>http://arxiv.org/abs/2505.11980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AoP-SAM的新方法，用于自动生成SAM（Segment Anything Model）的必要提示，从而提高其效率和实用性。&lt;h4&gt;背景&lt;/h4&gt;SAM是一种强大的图像分割基础模型，但依赖于手动提示在实际应用中不切实际，尤其是在需要快速提示和资源效率的场景中。&lt;h4&gt;目的&lt;/h4&gt;旨在提高SAM的效率和实用性，使其更适合现实世界的任务。&lt;h4&gt;方法&lt;/h4&gt;AoP-SAM使用一个轻量级且高效的提示预测模型，该模型检测图像中的关键实体并识别放置提示候选者的最佳区域。此外，还引入了一种测试时实例级别的自适应采样和过滤机制，以粗到细的方式生成提示。&lt;h4&gt;主要发现&lt;/h4&gt;AoP-SAM显著提高了提示生成效率和掩码生成准确性，同时保留了SAM的无需微调的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;AoP-SAM使SAM在自动化分割任务中更加有效。&lt;h4&gt;翻译&lt;/h4&gt;The Segment Anything Model (SAM) is a powerful foundation model for imagesegmentation, showing robust zero-shot generalization through promptengineering. However, relying on manual prompts is impractical for real-world applications, particularly in scenarios where rapid prompt provision and resource efficiency are crucial. In this paper, we propose the Automation of Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency and usability by eliminating manual input, making it better suited for real-world tasks. Our approach employs a lightweight yet efficient PromptPredictor model that detects key entities across images and identifies the optimal regions for placing prompt candidates. This method leverages SAM's image embeddings, preserving its zero-shot generalization capabilities without requiring fine-tuning. Additionally, we introduce a test-time instance-level Adaptive Sampling and Filtering mechanism that generates prompts in a coarse-to-fine manner. This notably enhances both prompt and mask generation efficiency by reducing computational overhead and minimizing redundant mask refinements. Evaluations of three datasets demonstrate that AoP-SAM substantially improves both prompt generation efficiency and mask generation accuracy, making SAM more effective for automated segmentation tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v39i2.32228&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Segment Anything Model (SAM) is a powerful foundation model for imagesegmentation, showing robust zero-shot generalization through promptengineering. However, relying on manual prompts is impractical for real-worldapplications, particularly in scenarios where rapid prompt provision andresource efficiency are crucial. In this paper, we propose the Automation ofPrompts for SAM (AoP-SAM), a novel approach that learns to generate essentialprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiencyand usability by eliminating manual input, making it better suited forreal-world tasks. Our approach employs a lightweight yet efficient PromptPredictor model that detects key entities across images and identifies theoptimal regions for placing prompt candidates. This method leverages SAM'simage embeddings, preserving its zero-shot generalization capabilities withoutrequiring fine-tuning. Additionally, we introduce a test-time instance-levelAdaptive Sampling and Filtering mechanism that generates prompts in acoarse-to-fine manner. This notably enhances both prompt and mask generationefficiency by reducing computational overhead and minimizing redundant maskrefinements. Evaluations of three datasets demonstrate that AoP-SAMsubstantially improves both prompt generation efficiency and mask generationaccuracy, making SAM more effective for automated segmentation tasks.</description>
      <author>example@mail.com (Yi Chen, Mu-Young Son, Chuanbo Hua, Joo-Young Kim)</author>
      <guid isPermaLink="false">2505.11980v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Programmable metasurfaces for future photonic artificial intelligence</title>
      <link>http://arxiv.org/abs/2505.11659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Nat. Rev. Phys. (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了光子神经网络（PNNs）在能效、延迟和吞吐量方面可能挑战传统数字神经网络，但实现可扩展的光子人工智能（AI）解决方案仍然具有挑战性。&lt;h4&gt;背景&lt;/h4&gt;光子神经网络具有高并行性和低功耗等固有优势，但在能源效率、延迟和吞吐量方面可能优于传统数字神经网络。&lt;h4&gt;目的&lt;/h4&gt;解决光子AI模型的可扩展性问题，使其在商业上可行。&lt;h4&gt;方法&lt;/h4&gt;讨论了现场可编程超表面技术可能成为实现可扩展光子AI加速器的关键硬件成分，以及它如何与当前数字电子技术竞争。&lt;h4&gt;主要发现&lt;/h4&gt;可编程或可重构性是PNN硬件的关键组成部分，它使得现场训练成为可能，并适应需要微调或迁移学习的非静态用例。&lt;h4&gt;结论&lt;/h4&gt;通过集成电子、3D堆叠和超表面的大规模制造，可编程超表面可以解决PNN面临的一些挑战，并推动下一代光子AI技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1038/s42254-025-00831-7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photonic neural networks (PNNs), which share the inherent benefits ofphotonic systems, such as high parallelism and low power consumption, couldchallenge traditional digital neural networks in terms of energy efficiency,latency, and throughput. However, producing scalable photonic artificialintelligence (AI) solutions remains challenging. To make photonic AI modelsviable, the scalability problem needs to be solved. Large optical AI modelsimplemented on PNNs are only commercially feasible if the advantages of opticalcomputation outweigh the cost of their input-output overhead. In thisPerspective, we discuss how field-programmable metasurface technology maybecome a key hardware ingredient in achieving scalable photonic AI acceleratorsand how it can compete with current digital electronic technologies.Programmability or reconfigurability is a pivotal component for PNN hardware,enabling in situ training and accommodating non-stationary use cases thatrequire fine-tuning or transfer learning. Co-integration with electronics, 3Dstacking, and large-scale manufacturing of metasurfaces would significantlyimprove PNN scalability and functionalities. Programmable metasurfaces couldaddress some of the current challenges that PNNs face and enablenext-generation photonic AI technology.</description>
      <author>example@mail.com (Loubnan Abou-Hamdan, Emil Marinov, Peter Wiecha, Philipp del Hougne, Tianyu Wang, Patrice Genevet)</author>
      <guid isPermaLink="false">2505.11659v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Subspace Optimization for Continual Learning</title>
      <link>http://arxiv.org/abs/2505.11816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了连续子空间优化（CoSO）方法，用于连续学习，旨在通过在一系列子空间中微调模型来减轻灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;连续学习旨在学习多个任务，同时保留先验知识，但获取新知识时面临着灾难性遗忘的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决连续学习中参数更新受限于固定低秩子空间的问题，从而提高模型的泛化能力和学习性能。&lt;h4&gt;方法&lt;/h4&gt;CoSO通过梯度奇异值分解动态确定一系列子空间，并通过将这些子空间中的梯度投影到模型上来更新模型，同时保持任务的特定组件以捕获当前任务的更新方向。&lt;h4&gt;主要发现&lt;/h4&gt;CoSO在多个数据集上的实验表明，它显著优于现有的最先进方法，特别是在具有长任务序列的挑战性场景中。&lt;h4&gt;结论&lt;/h4&gt;CoSO为连续学习提供了一种有效的方法，可以减轻灾难性遗忘，提高模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：连续学习旨在按顺序学习多个任务，同时保留先验知识，但在获取新知识时面临灾难性遗忘的挑战。最近，利用预训练模型的方法越来越受欢迎，以减轻这一问题，因为基础模型具有强大的泛化能力。为了调整预训练模型以适应新任务，现有方法通常采用低秩自适应，这限制了参数更新到固定的低秩子空间。然而，对优化空间的约束本质上会降低模型的学习能力，导致性能下降。为了解决这个问题，我们提出了连续子空间优化（CoSO）方法，以在一系列子空间中微调模型，而不是单一的一个。这些顺序子空间通过梯度奇异值分解动态确定。CoSO通过将这些子空间中的梯度投影到模型上来更新模型，确保内存高效的优化。为了减轻遗忘，每个任务的优化子空间被设置为与历史任务子空间正交。在任务学习过程中，CoSO维护一个特定于任务的组件，以捕获与当前任务相关的关键更新方向。完成一个任务后，该组件用于更新历史任务子空间，为后续学习奠定基础。在多个数据集上的大量实验表明，CoSO在挑战性场景中，特别是在具有长任务序列的情况下，显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning aims to learn multiple tasks sequentially while preservingprior knowledge, but faces the challenge of catastrophic forgetting whenacquiring new knowledge. Recently, approaches leveraging pre-trained modelshave gained increasing popularity to mitigate this issue, due to the stronggeneralization ability of foundation models. To adjust pre-trained models fornew tasks, existing methods usually employ low-rank adaptation, which restrictsparameter updates to a fixed low-rank subspace. However, constraining theoptimization space inherently compromises the model's learning capacity,resulting in inferior performance. To address the limitation, we proposeContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune themodel in a series of subspaces rather than a single one. These sequentialsubspaces are dynamically determined through the singular value decompositionof gradients. CoSO updates the model by projecting gradients into thesesubspaces, ensuring memory-efficient optimization. To mitigate forgetting, theoptimization subspaces of each task are set to be orthogonal to the historicaltask subspace. During task learning, CoSO maintains a task-specific componentthat captures the critical update directions associated with the current task.Upon completing a task, this component is used to update the historical tasksubspace, laying the groundwork for subsequent learning. Extensive experimentson multiple datasets demonstrate that CoSO significantly outperformsstate-of-the-art methods, especially in challenging scenarios with long tasksequences.</description>
      <author>example@mail.com (Quan Cheng, Yuanyu Wan, Lingyu Wu, Chenping Hou, Lijun Zhang)</author>
      <guid isPermaLink="false">2505.11816v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</title>
      <link>http://arxiv.org/abs/2505.11810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个名为AI Taiyan的大语言模型，专门用于理解和生成古典中文，并在古典中文信息处理的关键任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;通用大语言模型在语言理解和生成方面表现出色，但在特定领域如古典中文文本中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对古典中文理解和生成的大语言模型，以解决特定领域模型效果不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了合理的模型结构，进行了数据处理、基础训练和微调，并在仅有1.8亿参数的情况下取得了满意的结果。&lt;h4&gt;主要发现&lt;/h4&gt;AI Taiyan模型在古典中文信息处理的关键任务如标点、典故识别、词义解释和古汉译现代汉翻译等方面，优于通用大语言模型和特定领域传统模型，达到或超过了人类基准水平。&lt;h4&gt;结论&lt;/h4&gt;该研究为高效构建特定领域的专用大语言模型提供了参考，并讨论了该模型在古文书编纂、词典编辑和语言研究等领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;The study developed a large language model named AI Taiyan, specifically designed for understanding and generating Classical Chinese, and demonstrated excellent performance in key tasks related to Classical Chinese information processing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose large language models demonstrate notable capabilities inlanguage comprehension and generation, achieving results that are comparableto, or even surpass, human performance in many language information processingtasks. Nevertheless, when general models are applied to some specific domains,e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, andfine-tuning open-source foundational models similarly struggles to adequatelyincorporate domain-specific knowledge. To address this challenge, this studydeveloped a large language model, AI Taiyan, specifically designed forunderstanding and generating Classical Chinese. Experiments show that with areasonable model design, data processing, foundational training, andfine-tuning, satisfactory results can be achieved with only 1.8 billionparameters. In key tasks related to Classical Chinese information processingsuch as punctuation, identification of allusions, explanation of word meanings,and translation between ancient and modern Chinese, this model exhibits a clearadvantage over both general-purpose large models and domain-specifictraditional models, achieving levels close to or surpassing human baselines.This research provides a reference for the efficient construction ofspecialized domain-specific large language models. Furthermore, the paperdiscusses the application of this model in fields such as the collation ofancient texts, dictionary editing, and language research, combined with casestudies.</description>
      <author>example@mail.com (Shen Li, Renfen Hu, Lijun Wang)</author>
      <guid isPermaLink="false">2505.11810v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video</title>
      <link>http://arxiv.org/abs/2505.11709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对操作学习中的数据稀缺问题，提出了一个名为EgoDex的大规模数据集，用于解决当前数据集缺乏手部姿态标注和专注于物体操作的问题。&lt;h4&gt;背景&lt;/h4&gt;操作学习在数据稀缺方面存在难题，目前缺乏大型的手部操作数据集。&lt;h4&gt;目的&lt;/h4&gt;通过创建一个包含丰富手部操作数据集的EgoDex，旨在推动机器人、计算机视觉和基础模型领域的发展。&lt;h4&gt;方法&lt;/h4&gt;使用Apple Vision Pro收集EgoDex数据集，包含829小时的以自我为中心的人类操作视频和配对的3D手部及手指跟踪数据，利用多台校准相机和设备上的SLAM技术精确追踪每个手指关节的姿态。&lt;h4&gt;主要发现&lt;/h4&gt;EgoDex涵盖了从系鞋带到叠洗衣物等194种不同的桌面任务，涵盖了广泛多样的操作行为，并在该数据集上训练和评估了手部轨迹预测的模仿学习策略，引入了衡量该领域进展的指标和基准。&lt;h4&gt;结论&lt;/h4&gt;EgoDex的发布有望推动机器人、计算机视觉和基础模型领域的前沿发展。&lt;h4&gt;翻译&lt;/h4&gt;Imitation learning for manipulation has a well-known data scarcity problem. Unlike natural language and 2D computer vision, there is no Internet-scale corpus of data for dexterous manipulation. One appealing option is egocentric human video, a passively scalable data source. However, existing large-scale datasets such as Ego4D do not have native hand pose annotations and do not focus on object manipulation. To this end, we use Apple Vision Pro to collect EgoDex: the largest and most diverse dataset of dexterous human manipulation to date. EgoDex has 829 hours of egocentric video with paired 3D hand and finger tracking data collected at the time of recording, where multiple calibrated cameras and on-device SLAM can be used to precisely track the pose of every joint of each hand. The dataset covers a wide range of diverse manipulation behaviors with everyday household objects in 194 different tabletop tasks ranging from tying shoelaces to folding laundry. Furthermore, we train and systematically evaluate imitation learning policies for hand trajectory prediction on the dataset, introducing metrics and benchmarks for measuring progress in this increasingly important area. By releasing this large-scale dataset, we hope to push the frontier of robotics, computer vision, and foundation models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for manipulation has a well-known data scarcity problem.Unlike natural language and 2D computer vision, there is no Internet-scalecorpus of data for dexterous manipulation. One appealing option is egocentrichuman video, a passively scalable data source. However, existing large-scaledatasets such as Ego4D do not have native hand pose annotations and do notfocus on object manipulation. To this end, we use Apple Vision Pro to collectEgoDex: the largest and most diverse dataset of dexterous human manipulation todate. EgoDex has 829 hours of egocentric video with paired 3D hand and fingertracking data collected at the time of recording, where multiple calibratedcameras and on-device SLAM can be used to precisely track the pose of everyjoint of each hand. The dataset covers a wide range of diverse manipulationbehaviors with everyday household objects in 194 different tabletop tasksranging from tying shoelaces to folding laundry. Furthermore, we train andsystematically evaluate imitation learning policies for hand trajectoryprediction on the dataset, introducing metrics and benchmarks for measuringprogress in this increasingly important area. By releasing this large-scaledataset, we hope to push the frontier of robotics, computer vision, andfoundation models.</description>
      <author>example@mail.com (Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, Jian Zhang)</author>
      <guid isPermaLink="false">2505.11709v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Invariant Representations via Wasserstein Correlation Maximization</title>
      <link>http://arxiv.org/abs/2505.11702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用Wasserstein相关系数进行无监督表示学习的方法，该方法基于联合分布与其边缘分布的Wasserstein距离。&lt;h4&gt;背景&lt;/h4&gt;与自然在潜在空间中聚类类别的对比方法不同，本文发现，训练以最大化输入和编码分布之间的Wasserstein相关性的（自）编码器实际上充当压缩器，在减少维度的同时近似保留了输入分布的拓扑和几何属性。&lt;h4&gt;目的&lt;/h4&gt;探索Wasserstein相关系数在无监督表示学习中的应用，并研究其如何影响（自）编码器的性能。&lt;h4&gt;方法&lt;/h4&gt;使用Wasserstein相关系数来训练（自）编码器，并利用Markov-Wasserstein核定义增强编码器，以实现模型对特定增强的近似不变性。&lt;h4&gt;主要发现&lt;/h4&gt;Wasserstein相关系数最大化可以使（自）编码器对选择的增强或增强集近似不变，同时仍然近似保留非增强输入分布的结构属性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法不仅可以通过实验证明简单的前馈网络可以赋予不变性，还可以将不变性传递给预训练模型，并建立了基于最优传输的依赖度测量的各种理论结果。&lt;h4&gt;翻译&lt;/h4&gt;这项工作调查了使用Wasserstein相关系数——一种基于联合分布与其边缘分布之间的Wasserstein距离的标准化统计依赖度度量——进行无监督表示学习。与例如对比方法不同，这些方法在潜在空间中自然地聚类类别，我们发现，训练以最大化输入和编码分布之间的Wasserstein相关性的（自）编码器实际上充当压缩器，在减少维度的同时近似保留了输入分布的拓扑和几何属性。更有趣的是，我们表明，Wasserstein相关系数最大化可用于得到一个（自）编码器——无论是从头开始训练，还是扩展一个冻结的预训练模型——它对选择的增强或增强集近似不变，并且仍然近似保留了非增强输入分布的结构属性。为了做到这一点，我们首先使用Markov-Wasserstein核的机制定义了增强编码器的概念。当最大化目标应用于增强编码器，而不是基础上的确定性编码器时，所得到的模型表现出所期望的不变性属性。最后，除了我们的实验结果，这些结果表明即使简单的前馈网络也可以赋予不变性，或者可以在此训练过程中将不变性传递给预训练模型之外，我们还为基于最优传输的依赖度测量建立了各种理论结果。代码可在https://github.com/keenan-eikenberry/wasserstein_correlation_maximization 上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/keenan-eikenberry/wasserstein_correlation_maximization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates the use of Wasserstein correlation -- a normalizedmeasure of statistical dependence based on the Wasserstein distance between ajoint distribution and the product of its marginals -- for unsupervisedrepresentation learning. Unlike, for example, contrastive methods, whichnaturally cluster classes in the latent space, we find that an (auto)encodertrained to maximize Wasserstein correlation between the input and encodeddistributions instead acts as a compressor, reducing dimensionality whileapproximately preserving the topological and geometric properties of the inputdistribution. More strikingly, we show that Wasserstein correlationmaximization can be used to arrive at an (auto)encoder -- either trained fromscratch, or else one that extends a frozen, pretrained model -- that isapproximately invariant to a chosen augmentation, or collection ofaugmentations, and that still approximately preserves the structural propertiesof the non-augmented input distribution. To do this, we first define the notionof an augmented encoder using the machinery of Markov-Wasserstein kernels. Whenthe maximization objective is then applied to the augmented encoder, as opposedto the underlying, deterministic encoder, the resulting model exhibits thedesired invariance properties. Finally, besides our experimental results, whichshow that even simple feedforward networks can be imbued with invariants orcan, alternatively, be used to impart invariants to pretrained models underthis training process, we additionally establish various theoretical resultsfor optimal transport-based dependence measures. Code is available athttps://github.com/keenan-eikenberry/wasserstein_correlation_maximization .</description>
      <author>example@mail.com (Keenan Eikenberry, Lizuo Liu, Yoonsang Lee)</author>
      <guid isPermaLink="false">2505.11702v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2505.11680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于实例的零样本技能迁移方法，旨在解决开放世界机器人操作中不同物体之间技能迁移的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;机器人操作中，技能的迁移需要考虑不同物体之间的高层次结构差异，同时保持低层次交互控制的相似性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够在不将技能视为原子化的情况下，将技能分解为一系列基于任务轴（GTA）的控制器，并实现零样本迁移。&lt;h4&gt;方法&lt;/h4&gt;将技能分解为一系列GTA控制器，每个控制器定义了沿一个轴的适应控制器，如位置或力控制器。这些控制器基于物体的关键点和轴进行定位。使用如SD-DINO等基础模型检测语义上相似的关键点，以实现零样本迁移。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实机器人的实验评估，包括拧紧、倒水和刮刀刮擦等任务，证明了该框架在技能迁移方面的鲁棒性和通用性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效地在不同物体之间进行技能迁移，具有实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring skills between different objects remains one of the corechallenges of open-world robot manipulation. Generalization needs to take intoaccount the high-level structural differences between distinct objects whilestill maintaining similar low-level interaction control. In this paper, wepropose an example-based zero-shot approach to skill transfer. Rather thantreating skills as atomic, we decompose skills into a prioritized list ofgrounded task-axis (GTA) controllers. Each GTAC defines an adaptablecontroller, such as a position or force controller, along an axis. Importantly,the GTACs are grounded in object key points and axes, e.g., the relativeposition of a screw head or the axis of its shaft. Zero-shot transfer is thusachieved by finding semantically-similar grounding features on novel targetobjects. We achieve this example-based grounding of the skills through the useof foundation models, such as SD-DINO, that can detect semantically similarkeypoints of objects. We evaluate our framework on real-robot experiments,including screwing, pouring, and spatula scraping tasks, and demonstrate robustand versatile controller transfer for each.</description>
      <author>example@mail.com (M. Yunus Seker, Shobhit Aggarwal, Oliver Kroemer)</author>
      <guid isPermaLink="false">2505.11680v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach</title>
      <link>http://arxiv.org/abs/2505.11645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in International Society Journal of  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SemiGTX的半监督图学习框架，用于进行行业经济映射，旨在解决现有方法在数据稀缺场景下忽视半监督学习以及缺乏统一的多任务框架的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的经济映射方法主要依赖于监督或无监督学习，但往往忽略了半监督学习，并且缺乏用于全面行业经济分析的多任务框架。&lt;h4&gt;目的&lt;/h4&gt;提出SemiGTX框架，以解决现有方法的不足，实现更有效的行业经济映射。&lt;h4&gt;方法&lt;/h4&gt;SemiGTX框架包含专门的融合编码模块，用于处理不同地理空间数据模式，并将它们无缝集成到一个统一的图结构中。它引入了一种半信息损失函数，结合空间自监督和局部掩码监督回归，以实现更丰富和有效的区域表示。通过多任务学习，SemiGTX在一个统一模型中同时映射一、二、三产业的GDP。&lt;h4&gt;主要发现&lt;/h4&gt;在珠江三角洲地区进行的广泛实验表明，与现有方法相比，SemiGTX模型表现出优异的性能，分别实现了0.93、0.96和0.94的R2分数。在北京和成都的跨区域实验进一步说明了其通用性。系统分析揭示了不同数据模式如何影响模型预测，增强了可解释性，并为区域发展规划提供了有价值的见解。&lt;h4&gt;结论&lt;/h4&gt;SemiGTX框架通过集成多种城市数据，推进了区域经济监测，为精确的经济预测提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained economic mapping through urban representation learning hasemerged as a crucial tool for evidence-based economic decisions. While existingmethods primarily rely on supervised or unsupervised approaches, they oftenoverlook semi-supervised learning in data-scarce scenarios and lack unifiedmulti-task frameworks for comprehensive sectoral economic analysis. To addressthese gaps, we propose SemiGTX, an explainable semi-supervised graph learningframework for sectoral economic mapping. The framework is designed withdedicated fusion encoding modules for various geospatial data modalities,seamlessly integrating them into a cohesive graph structure. It introduces asemi-information loss function that combines spatial self-supervision withlocally masked supervised regression, enabling more informative and effectiveregion representations. Through multi-task learning, SemiGTX concurrently mapsGDP across primary, secondary, and tertiary sectors within a unified model.Extensive experiments conducted in the Pearl River Delta region of Chinademonstrate the model's superior performance compared to existing methods,achieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary andtertiary sectors, respectively. Cross-regional experiments in Beijing andChengdu further illustrate its generality. Systematic analysis reveals howdifferent data modalities influence model predictions, enhancing explainabilitywhile providing valuable insights for regional development planning. Thisrepresentation learning framework advances regional economic monitoring throughdiverse urban data integration, providing a robust foundation for preciseeconomic forecasting.</description>
      <author>example@mail.com (Jinzhou Cao, Xiangxu Wang, Jiashi Chen, Wei Tu, Zhenhui Li, Xindong Yang, Tianhong Zhao, Qingquan Li)</author>
      <guid isPermaLink="false">2505.11645v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for AI-Enabled Biological Design</title>
      <link>http://arxiv.org/abs/2505.11610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as part of the workshop proceedings at AAAI 2025 in the  workshop "Foundation Models for Biological Discoveries"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了AI赋能的生物学设计中的基础模型，重点讨论了将大规模、自监督模型应用于蛋白质工程、小分子设计和基因组序列设计等任务方面的最新进展。&lt;h4&gt;背景&lt;/h4&gt;该领域正在迅速发展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示和讨论当前模型和方法的分类，重点关注适应这些模型进行生物学应用中的挑战和解决方案。&lt;h4&gt;方法&lt;/h4&gt;方法包括生物序列建模架构、生成过程中的可控性和多模态集成。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了开放问题和未来的研究方向，并提出了具体下一步行动，以改善生物序列生成的质量。&lt;h4&gt;结论&lt;/h4&gt;本文提供了对AI在生物学设计中的应用的全面概述，并指出了未来研究的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper surveys foundation models for AI-enabled biological design,focusing on recent developments in applying large-scale, self-supervised modelsto tasks such as protein engineering, small molecule design, and genomicsequence design. Though this domain is evolving rapidly, this survey presentsand discusses a taxonomy of current models and methods. The focus is onchallenges and solutions in adapting these models for biological applications,including biological sequence modeling architectures, controllability ingeneration, and multi-modal integration. The survey concludes with a discussionof open problems and future directions, offering concrete next-steps to improvethe quality of biological sequence generation.</description>
      <author>example@mail.com (Asher Moldwin, Amarda Shehu)</author>
      <guid isPermaLink="false">2505.11610v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis</title>
      <link>http://arxiv.org/abs/2505.11581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 25 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文挑战了表现提升必然导致内部表示提升的观点，通过比较两种不同训练方法下的神经网络在生成单张图像任务上的表现，发现它们的内部表示存在显著差异。&lt;h4&gt;背景&lt;/h4&gt;现代AI领域对通过扩展现有系统来提升性能感到兴奋，但表现提升是否意味着更好的内部表示尚存争议。&lt;h4&gt;目的&lt;/h4&gt;探究不同训练方法对神经网络内部表示的影响，并分析其可能带来的后果。&lt;h4&gt;方法&lt;/h4&gt;将通过开放搜索过程进化的神经网络与通过传统随机梯度下降（SGD）训练的神经网络在生成单张图像的任务上进行比较，并通过可视化隐藏神经元的完整功能行为来观察网络内部表示的差异。&lt;h4&gt;主要发现&lt;/h4&gt;两种网络产生相同的输出行为，但它们的内部表示存在显著差异。SGD训练的网络表现出一种称为破碎纠缠表示（FER）的无序形式，而进化的网络则主要缺乏FER，甚至接近统一的因子表示（UFR）。&lt;h4&gt;结论&lt;/h4&gt;在大型模型中，FER可能会降低模型的核心能力，如泛化、创造力和（持续）学习。因此，理解和减轻FER对于表示学习未来的发展至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现代人工智能领域的兴奋很大程度上源于观察到的扩大现有系统会导致更好的性能。但更好的性能是否必然意味着更好的内部表示？尽管表示乐观者认为必须如此，这篇立场论文挑战了这种观点。我们比较了通过开放搜索过程进化的神经网络与通过传统随机梯度下降（SGD）在生成单个图像的简单任务上约束的网络。这种最小设置提供了一种独特的优势：每个隐藏神经元的完整功能行为可以很容易地被可视化为图像，从而揭示网络输出行为是如何内部构建的，神经元一个接一个。结果是惊人的：尽管两个网络都产生了相同的输出行为，但它们的内部表示存在显著差异。SGD训练的网络表现出一种我们称之为破碎纠缠表示（FER）的无序形式。有趣的是，进化的网络在很大程度上缺乏FER，甚至接近统一的因子表示（UFR）。在大型模型中，FER可能会降低模型的核心能力，如泛化、创造力和（持续）学习。因此，理解和减轻FER对于表示学习未来的发展可能是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Much of the excitement in modern AI is driven by the observation that scalingup existing systems leads to better performance. But does better performancenecessarily imply better internal representations? While the representationaloptimist assumes it must, this position paper challenges that view. We compareneural networks evolved through an open-ended search process to networkstrained via conventional stochastic gradient descent (SGD) on the simple taskof generating a single image. This minimal setup offers a unique advantage:each hidden neuron's full functional behavior can be easily visualized as animage, thus revealing how the network's output behavior is internallyconstructed neuron by neuron. The result is striking: while both networksproduce the same output behavior, their internal representations differdramatically. The SGD-trained networks exhibit a form of disorganization thatwe term fractured entangled representation (FER). Interestingly, the evolvednetworks largely lack FER, even approaching a unified factored representation(UFR). In large models, FER may be degrading core model capacities likegeneralization, creativity, and (continual) learning. Therefore, understandingand mitigating FER could be critical to the future of representation learning.</description>
      <author>example@mail.com (Akarsh Kumar, Jeff Clune, Joel Lehman, Kenneth O. Stanley)</author>
      <guid isPermaLink="false">2505.11581v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</title>
      <link>http://arxiv.org/abs/2505.10579v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Medical Image Computing  and Computer-Assisted Intervention (MICCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于基础模型（FMs）的乳腺摄影分类的公平性和偏差，通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集。&lt;h4&gt;背景&lt;/h4&gt;尽管计算机辅助诊断工具在乳腺癌症筛查中得到了发展，但它们的临床采用受到了数据变异和固有偏差的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究FMs在乳腺摄影分类中的公平性和偏差。&lt;h4&gt;方法&lt;/h4&gt;通过结合来自不同来源的大量数据集，包括代表性不足地区的数据和内部数据集，进行广泛的实验。&lt;h4&gt;主要发现&lt;/h4&gt;尽管特定模态的预训练可以提升性能，但基于单个数据集特征的分类器无法跨领域泛化。数据集的聚合提高了整体性能，但并不能完全消除偏差，导致代表性不足的子群体（如极端乳腺密度和年龄组）存在显著差异。领域自适应策略可以减少这些差异，但通常会带来性能权衡。相比之下，公平性感知技术可以在子群体之间产生更稳定和公平的性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了在基于FMs的模型中纳入严格公平性评估和缓解策略的必要性，以促进包容性和可泛化的AI。&lt;h4&gt;翻译&lt;/h4&gt;Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sources-including data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domain-adaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decades, computer-aided diagnosis tools for breast cancer havebeen developed to enhance screening procedures, yet their clinical adoptionremains challenged by data variability and inherent biases. Although foundationmodels (FMs) have recently demonstrated impressive generalizability andtransfer learning capabilities by leveraging vast and diverse datasets, theirperformance can be undermined by spurious correlations that arise fromvariations in image quality, labeling uncertainty, and sensitive patientattributes. In this work, we explore the fairness and bias of FMs for breastmammography classification by leveraging a large pool of datasets from diversesources-including data from underrepresented regions and an in-house dataset.Our extensive experiments show that while modality-specific pre-training of FMsenhances performance, classifiers trained on features from individual datasetsfail to generalize across domains. Aggregating datasets improves overallperformance, yet does not fully mitigate biases, leading to significantdisparities across under-represented subgroups such as extreme breast densitiesand age groups. Furthermore, while domain-adaptation strategies can reducethese disparities, they often incur a performance trade-off. In contrast,fairness-aware techniques yield more stable and equitable performance acrosssubgroups. These findings underscore the necessity of incorporating rigorousfairness evaluations and mitigation strategies into FM-based models to fosterinclusive and generalizable AI.</description>
      <author>example@mail.com (Elodie Germani, Ilayda Selin Türk, Fatima Zeineddine, Charbel Mourad, Shadi Albarqouni)</author>
      <guid isPermaLink="false">2505.10579v2</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness</title>
      <link>http://arxiv.org/abs/2505.08320v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpecSphere是一种新型双通道频谱-空间GNN，能够在保持线性时间复杂性的同时，超越1-Weisfeiler-Lehman的表达能力，并通过轻量级的MLP融合频谱分支和空间分支的表示。&lt;h4&gt;背景&lt;/h4&gt;当前图神经网络（GNN）在预测和鲁棒性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SpecSphere，旨在提高GNN的预测准确性、适应性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;SpecSphere结合了Chebyshev多项式频谱分支和注意力门控空间分支，通过合作对抗的最小-最大游戏训练轻量级MLP进行表示融合。&lt;h4&gt;主要发现&lt;/h4&gt;SpecSphere能够验证每项预测，适应全同质性-异质性频谱，并提供统一Chebyshev近似定理、最小-最大最优风险、闭式鲁棒性证书以及严格超越1-WL的通用逼近能力。&lt;h4&gt;结论&lt;/h4&gt;SpecSphere在节点分类任务上达到最先进的准确性，并提供了更紧的鲁棒性保证，证明了高表达能力、异质性适应性和可证明的鲁棒性可以在单一、可扩展的架构中共存。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SpecSphere, the first dual-pass spectral-spatial GNN thatcertifies every prediction against both $\ell\_{0}$ edge flips and$\ell\_{\infty}$ feature perturbations, adapts to the fullhomophily-heterophily spectrum, and surpasses the expressive power of1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples aChebyshev-polynomial spectral branch with an attention-gated spatial branch andfuses their representations through a lightweight MLP trained in acooperative-adversarial min-max game. We further establish (i) a uniformChebyshev approximation theorem, (ii) minimax-optimal risk across thehomophily-heterophily spectrum, (iii) closed-form robustness certificates, and(iv) universal approximation strictly beyond 1-WL. SpecSphere achievesstate-of-the-art node-classification accuracy and delivers tighter certifiedrobustness guarantees on real-world benchmarks. These results demonstrate thathigh expressivity, heterophily adaptation, and provable robustness can coexistwithin a single, scalable architecture.</description>
      <author>example@mail.com (Yoonhyuk Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2505.08320v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
  <item>
      <title>Inference for Dispersion and Curvature of Random Objects</title>
      <link>http://arxiv.org/abs/2505.09844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了随机对象的统计分析，特别是针对测地度量子空间中的数据对象，提出了基于Frechet方差和度量方差的联合分布的中心极限定理，并探讨了空间曲率与这两种散度度量之间的关系。&lt;h4&gt;背景&lt;/h4&gt;随机对象在统计分析中日益常见，但缺乏线性操作是其中的主要挑战。&lt;h4&gt;目的&lt;/h4&gt;量化统计散度或分布，并推导出Frechet方差和度量方差的联合分布的中心极限定理。&lt;h4&gt;方法&lt;/h4&gt;通过分析Frechet方差和度量方差之间的关系，提出了一种基于散度度量渐进分布来推断空间曲率的新方法。&lt;h4&gt;主要发现&lt;/h4&gt;发现测地空间的Alexandrov曲率决定了这两种散度度量之间的关系，并提出了一种检测未知空间固有曲率的新方法。&lt;h4&gt;结论&lt;/h4&gt;该方法可以应用于检测未知空间的固有曲率，并探讨了其在不同数据类型（如分布数据和点云数据）中的渐近性质和有限样本行为。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了随机对象的统计分析，其中存在许多开放性问题。主要挑战是这些空间中缺乏线性操作。基本统计任务是量化统计散度或分布。对于测地度量子空间中数据对象的两种散度度量，Frechet方差和度量方差，我们推导了它们的联合分布的中心极限定理。这一分析揭示出测地空间的Alexandrov曲率决定了这两种散度度量之间的关系。这表明了一种基于散度度量渐进分布来推断空间曲率的新方法。我们展示了如何使用这种方法来检测未知空间的固有曲率，这表现为空间和生成随机对象的潜在概率测度的一个联合属性。我们研究了该测试的渐近性质及其在包括分布数据和点云数据在内的各种数据类型中的有限样本行为。我们使用表示为对称正定矩阵的步态同步数据和球面上的能量组成数据来说明所提出的关于随机对象固有曲率的推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There are many open questions pertaining to the statistical analysis ofrandom objects, which are increasingly encountered. A major challenge is theabsence of linear operations in such spaces. A basic statistical task is toquantify statistical dispersion or spread. For two measures of dispersion fordata objects in geodesic metric spaces, Fr\'echet variance and metric variance,we derive a central limit theorem (CLT) for their joint distribution. Thisanalysis reveals that the Alexandrov curvature of the geodesic space determinesthe relationship between these two dispersion measures. This suggests a noveltest for inferring the curvature of a space based on the asymptoticdistribution of the dispersion measures. We demonstrate how this test can beemployed to detect the intrinsic curvature of an unknown underlying space,which emerges as a joint property of the space and the underlying probabilitymeasure that generates the random objects. We investigate the asymptoticproperties of the test and its finite-sample behavior for various data types,including distributional data and point cloud data. We illustrate the proposedinference for intrinsic curvature of random objects using gait synchronizationdata represented as symmetric positive definite matrices and energycompositional data on the sphere.</description>
      <author>example@mail.com (Wookyeong Song, Hans-Georg Müller)</author>
      <guid isPermaLink="false">2505.09844v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model</title>
      <link>http://arxiv.org/abs/2505.11421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了实现 Bahnaric-Vietnamesetranslation 的过程，以促进越南两个民族的文化交流。通过转移学习方法和序列到序列预训练语言模型，解决了数据收集和资源不平衡的问题，并提高了翻译的准确性。&lt;h4&gt;背景&lt;/h4&gt;翻译从 Bahnaric 到 Vietnamese 遇到困难，主要是因为缺乏原始的 Bahnaric 资源，包括词汇、语法、对话模式和双语语料库。&lt;h4&gt;目的&lt;/h4&gt;为了实现 Bahnaric-Vietnamesetranslation，以促进越南两个民族的文化交流。&lt;h4&gt;方法&lt;/h4&gt;采用转移学习方法，利用序列到序列预训练语言模型，利用预训练的越南语言模型捕捉语言特征，并使用有限的越南-Bahnaric 双语资源进行迁移学习。同时，通过数据增强和启发式方法增强数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在处理两种语言资源不平衡的问题上有效，同时优化了训练和计算过程，并提高了翻译的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法对于 Bahnaric-Vietnamesetranslation 模型非常有效，有助于语言的扩展和保护，促进两个民族之间的相互理解。&lt;h4&gt;翻译&lt;/h4&gt;本研究探索了实现巴拿尔语-越南语翻译的过程，旨在促进越南两个民族的文化交流。然而，巴拿尔语到越南语的翻译也遇到了一些困难。最突出的挑战是缺乏可用的原始巴拿尔语资源，包括词汇、语法、对话模式和双语语料库，这阻碍了数据收集过程。为了解决这个问题，我们利用了基于序列到序列预训练语言模型的迁移学习方法。首先，我们利用预训练的越南语言模型来捕捉这种语言的特征。特别是，为了进一步服务于机器翻译的目的，我们追求的是序列到序列模型，而不是像 BERT 这样的编码器仅模型或像 GPT 这样的解码器仅模型。利用两种语言之间显著的相似性，我们继续使用目前有限的越南-巴拿尔语双语资源来执行从语言模型到机器翻译的迁移学习。因此，这种方法有助于处理两种语言之间资源不平衡的问题，同时优化了训练和计算过程。此外，我们还通过数据增强增强了数据集，并定义了一些启发式方法来帮助翻译更加精确。我们的方法已被证明对于巴拿尔语-越南语翻译模型非常有效，有助于语言的扩展和保护，促进两个民族之间的相互理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the journey towards achieving Bahnaric-Vietnamesetranslation for the sake of culturally bridging the two ethnic groups inVietnam. However, translating from Bahnaric to Vietnamese also encounters somedifficulties. The most prominent challenge is the lack of available originalBahnaric resources source language, including vocabulary, grammar, dialoguepatterns and bilingual corpus, which hinders the data collection process fortraining. To address this, we leverage a transfer learning approach usingsequence-to-sequence pre-training language model. First of all, we leverage apre-trained Vietnamese language model to capture the characteristics of thislanguage. Especially, to further serve the purpose of machine translation, weaim for a sequence-to-sequence model, not encoder-only like BERT ordecoder-only like GPT. Taking advantage of significant similarity between thetwo languages, we continue training the model with the currently limitedbilingual resources of Vietnamese-Bahnaric text to perform the transferlearning from language model to machine translation. Thus, this approach canhelp to handle the problem of imbalanced resources between two languages, whilealso optimizing the training and computational processes. Additionally, we alsoenhanced the datasets using data augmentation to generate additional resourcesand defined some heuristic methods to help the translation more precise. Ourapproach has been validated to be highly effective for the Bahnaric-Vietnamesetranslation model, contributing to the expansion and preservation of languages,and facilitating better mutual understanding between the two ethnic people.</description>
      <author>example@mail.com (Phan Tran Minh Dat, Vo Hoang Nhat Khang, Quan Thanh Tho)</author>
      <guid isPermaLink="false">2505.11421v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
      <link>http://arxiv.org/abs/2505.11484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SoftCoT++，一种扩展SoftCoT到测试时间缩放（TTS）范式的方法，通过允许多样化的思维路径探索来提高推理性能。&lt;h4&gt;背景&lt;/h4&gt;现有的TTS方法通过生成更多中间步骤在离散标记空间中操作，而近期的研究表明在连续潜在空间中进行思维可以进一步增强推理性能。&lt;h4&gt;目的&lt;/h4&gt;目的是通过引入SoftCoT++来克服连续空间中固定潜在表示所导致的多样化探索限制，从而提高推理性能。&lt;h4&gt;方法&lt;/h4&gt;SoftCoT++通过使用多个专门的初始标记扰动潜在思维，并应用对比学习来促进软思维表示之间的多样性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，SoftCoT++显著提高了SoftCoT的性能，并且优于使用自一致性缩放的SoftCoT，同时与传统的缩放技术如自一致性具有很好的兼容性。&lt;h4&gt;结论&lt;/h4&gt;SoftCoT++是一种有效的TTS方法，能够通过多样化的思维路径探索显著提升推理性能。&lt;h4&gt;翻译&lt;/h4&gt;Test-Time Scaling (TTS)指的是在推理过程中分配额外计算来提高推理性能的方法，而不改变模型的参数。虽然现有的TTS方法通过生成更多中间步骤在离散标记空间中操作，但最近在Coconut和SoftCoT中的研究表明，在连续潜在空间中进行思维可以进一步增强推理性能。这种潜在思维编码了信息性思维，而没有与自回归标记生成相关的信息损失，这激发了人们对连续空间推理的兴趣。与重复采样以探索不同推理路径的离散解码不同，连续空间中的潜在表示对于给定的输入是固定的，这限制了多样化探索，因为所有解码路径都源自相同的潜在思维。为了克服这一限制，我们引入了SoftCoT++，通过允许多样化的思维路径探索来扩展SoftCoT到测试时间缩放（TTS）范式。具体来说，我们通过多个专门的初始标记扰动潜在思维，并应用对比学习来促进软思维表示之间的多样性。在五个推理基准和两种不同的LLM架构上的实验表明，SoftCoT++显著提高了SoftCoT的性能，并且优于使用自一致性缩放的SoftCoT。此外，它还显示出与传统的缩放技术，如自一致性，具有很好的兼容性。源代码可在https://github.com/xuyige/SoftCoT上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-Time Scaling (TTS) refers to approaches that improve reasoningperformance by allocating extra computation during inference, without alteringthe model's parameters. While existing TTS methods operate in a discrete tokenspace by generating more intermediate steps, recent studies in Coconut andSoftCoT have demonstrated that thinking in the continuous latent space canfurther enhance the reasoning performance. Such latent thoughts encodeinformative thinking without the information loss associated withautoregressive token generation, sparking increased interest incontinuous-space reasoning. Unlike discrete decoding, where repeated samplingenables exploring diverse reasoning paths, latent representations in continuousspace are fixed for a given input, which limits diverse exploration, as alldecoded paths originate from the same latent thought. To overcome thislimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scalingparadigm by enabling diverse exploration of thinking paths. Specifically, weperturb latent thoughts via multiple specialized initial tokens and applycontrastive learning to promote diversity among soft thought representations.Experiments across five reasoning benchmarks and two distinct LLM architecturesdemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperformsSoftCoT with self-consistency scaling. Moreover, it shows strong compatibilitywith conventional scaling techniques such as self-consistency. Source code isavailable at https://github.com/xuyige/SoftCoT.</description>
      <author>example@mail.com (Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao)</author>
      <guid isPermaLink="false">2505.11484v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Finding Counterfactual Evidences for Node Classification</title>
      <link>http://arxiv.org/abs/2505.11396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于图神经网络（GNN）的节点分类任务中寻找反事实证据的问题，提出了一种有效且高效的搜索算法和一种新颖的索引解决方案，以识别反事实证据，并证明了反事实证据在提高GNN的公平性和准确性方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;反事实学习是一个基于因果性的重要范式，它承诺缓解图神经网络（GNN）的常见问题，如公平性和可解释性。然而，在许多现实世界应用领域中，由于无法进行随机对照试验，人们必须依赖于可用的观察（事实）数据来检测反事实。&lt;h4&gt;目的&lt;/h4&gt;寻找基于GNN的节点分类任务的反事实证据，以提升GNN的公平性和准确性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种有效且高效的搜索算法和一种新颖的索引解决方案，该解决方案利用节点特征和结构信息来识别反事实证据，并且该方法超越了任何特定的GNN。&lt;h4&gt;主要发现&lt;/h4&gt;反事实证据是一对节点，尽管它们在特征和邻域子图结构上表现出极大的相似性，但被GNN分类为不同的类别。&lt;h4&gt;结论&lt;/h4&gt;反事实证据具有提高GNN公平性和准确性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Counterfactual learning is emerging as an important paradigm, rooted incausality, which promises to alleviate common issues of graph neural networks(GNNs), such as fairness and interpretability. However, as in many real-worldapplication domains where conducting randomized controlled trials isimpractical, one has to rely on available observational (factual) data todetect counterfactuals. In this paper, we introduce and tackle the problem ofsearching for counterfactual evidences for the GNN-based node classificationtask. A counterfactual evidence is a pair of nodes such that, regardless theyexhibit great similarity both in the features and in their neighborhoodsubgraph structures, they are classified differently by the GNN. We developeffective and efficient search algorithms and a novel indexing solution thatleverages both node features and structural information to identifycounterfactual evidences, and generalizes beyond any specific GNN. Throughvarious downstream applications, we demonstrate the potential of counterfactualevidences to enhance fairness and accuracy of GNNs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Counterfactual learning is emerging as an important paradigm, rooted incausality, which promises to alleviate common issues of graph neural networks(GNNs), such as fairness and interpretability. However, as in many real-worldapplication domains where conducting randomized controlled trials isimpractical, one has to rely on available observational (factual) data todetect counterfactuals. In this paper, we introduce and tackle the problem ofsearching for counterfactual evidences for the GNN-based node classificationtask. A counterfactual evidence is a pair of nodes such that, regardless theyexhibit great similarity both in the features and in their neighborhoodsubgraph structures, they are classified differently by the GNN. We developeffective and efficient search algorithms and a novel indexing solution thatleverages both node features and structural information to identifycounterfactual evidences, and generalizes beyond any specific GNN. Throughvarious downstream applications, we demonstrate the potential of counterfactualevidences to enhance fairness and accuracy of GNNs.</description>
      <author>example@mail.com (Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi)</author>
      <guid isPermaLink="false">2505.11396v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自动检索合成CAD模型的方法，用于生成高质量的3D标注数据，以训练监督深度学习模型，并验证了这种方法在点云补全和单视图CAD模型检索与对齐任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;高级3D场景理解在许多应用中至关重要，但生成准确的3D标注数据具有挑战性，这给深度学习模型的发展带来了困难。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用自动检索合成CAD模型的方法来生成高质量的3D标注数据，并训练深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;采用了一种类似于以前用于自动标注ScanNet场景中物体9D姿态和CAD模型的流程，并将其应用于ScanNet++ v1数据集。&lt;h4&gt;主要发现&lt;/h4&gt;自动获取的标注数据可以用于训练深度学习模型，并且训练出的模型在性能上优于手动标注数据训练的模型。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注有潜力提高模型性能，同时显著降低标注成本，未来将发布我们开发的标注工具SCANnotate++和训练模型。&lt;h4&gt;翻译&lt;/h4&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v4</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph Representational Learning: When Does More Expressivity Hurt Generalization?</title>
      <link>http://arxiv.org/abs/2505.11298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络（GNNs）的表达性和预测性能之间的关系，并引入了一系列前度量来捕捉图之间的结构相似度，将相似度与泛化能力联系起来，进而关联到高表达能力GNNs的性能。&lt;h4&gt;背景&lt;/h4&gt;GNNs作为处理结构化数据的强大工具，其表达性和预测性能之间的关系尚不明确。&lt;h4&gt;目的&lt;/h4&gt;介绍一种方法来评估GNNs的表达能力和预测性能，并探究其泛化能力。&lt;h4&gt;方法&lt;/h4&gt;在考虑图标签与结构特征相关联的设置下，推导出依赖于训练和测试图之间的距离、模型复杂度和训练集大小的泛化界限。&lt;h4&gt;主要发现&lt;/h4&gt;发现更具有表达能力的GNNs可能泛化能力较差，除非其增加的复杂性通过足够大的训练集或减少训练和测试图之间的距离来平衡。&lt;h4&gt;结论&lt;/h4&gt;本研究将表达性和泛化能力联系起来，提供了理论上的见解，并得到了实证结果的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图神经网络（GNNs）是学习结构化数据的强大工具，但其表达性与预测性能之间的关系尚不明确。我们介绍了一族前度量，用于捕捉图之间不同程度的结构相似性，并将这些相似性与泛化能力相关联，从而关联到高表达能力GNNs的性能。通过考虑一个图标签与结构特征相关的设置，我们推导出依赖于训练和测试图之间的距离、模型复杂度和训练集大小的泛化界限。这些界限揭示，如果其增加的复杂性不能通过足够大的训练集或减少训练和测试图之间的距离来平衡，那么更具有表达能力的GNNs可能泛化能力较差。我们的发现将表达性与泛化能力联系起来，提供了理论上的见解，并得到了实证结果的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful tools for learning on structureddata, yet the relationship between their expressivity and predictiveperformance remains unclear. We introduce a family of premetrics that capturedifferent degrees of structural similarity between graphs and relate thesesimilarities to generalization, and consequently, the performance of expressiveGNNs. By considering a setting where graph labels are correlated withstructural features, we derive generalization bounds that depend on thedistance between training and test graphs, model complexity, and training setsize. These bounds reveal that more expressive GNNs may generalize worse unlesstheir increased complexity is balanced by a sufficiently large training set orreduced distance between training and test graphs. Our findings relateexpressivity and generalization, offering theoretical insights supported byempirical results.</description>
      <author>example@mail.com (Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer)</author>
      <guid isPermaLink="false">2505.11298v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Fractal Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.11356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Fractal Graph Contrastive Learning（FractalGCL）的理论驱动框架，旨在解决图对比学习（GCL）中数据增强的局限性。该框架通过利用分形自相似性来强制执行全局拓扑一致性，并引入了两种关键创新：基于重归一化的增强和具有分形维度意识的对比损失。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）在图自监督学习领域受到广泛关注，但其性能高度依赖于能够生成语义一致正对的增广数据。现有的策略通常依赖于随机扰动或局部结构保持，但缺乏对增广视图之间全局结构一致性的显式控制。&lt;h4&gt;目的&lt;/h4&gt;提出FractalGCL以解决现有GCL方法的局限性，提高图表示质量，并减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;FractalGCL通过引入基于重归一化的增强和分形维度意识的对比损失来实现其目标。此外，为了减轻分形维度估计的计算开销，作者推导出了一个单次估计器，通过证明原始图和重归一化图之间的维度差异会弱收敛到一个中心高斯分布。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，FractalGCL在标准基准测试中实现了最先进的性能，并且在交通网络上比传统基线平均提高了约7%。&lt;h4&gt;结论&lt;/h4&gt;FractalGCL通过引入创新的技术，显著提高了图对比学习的性能，并减少了计算成本。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然图对比学习（GCL）在图自监督学习领域受到了相当大的关注，但其性能高度依赖于预期能够生成语义一致正对的增广数据。现有的策略通常依赖于随机扰动或局部结构保持，但缺乏对增广视图之间全局结构一致性的显式控制。为了解决这一局限性，我们提出了基于分形的图对比学习（FractalGCL），这是一种理论驱动的框架，它利用分形自相似性来强制执行全局拓扑一致性。FractalGCL引入了两个关键创新：一种基于重归一化的增强，通过箱覆盖生成结构对齐的正视图；以及一种具有分形维度意识的对比损失，根据它们的分形维度对齐图嵌入。虽然结合这两种创新显著提高了图表示质量，但也增加了非平凡的计算开销。为了减轻分形维度估计的计算开销，我们推导出了一个单次估计器，通过证明原始图和重归一化图之间的维度差异会弱收敛到一个中心高斯分布。这一理论洞察使得维度计算成本降低了大约一个数量级，整体训练时间减少了大约61%。实验表明，FractalGCL不仅在标准基准测试中实现了最先进的性能，而且在交通网络上平均比传统基线提高了约7%。代码可在（https://anonymous.4open.science/r/FractalGCL-0511）获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Contrastive Learning (GCL) has attracted considerable attentionin the field of graph self-supervised learning, its performance heavily relieson data augmentations that are expected to generate semantically consistentpositive pairs. Existing strategies typically resort to random perturbations orlocal structure preservation, yet lack explicit control over global structuralconsistency between augmented views. To address this limitation, we proposeFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework thatleverages fractal self-similarity to enforce global topological coherence.FractalGCL introduces two key innovations: a renormalisation-based augmentationthat generates structurally aligned positive views via box coverings; and afractal-dimension-aware contrastive loss that aligns graph embeddings accordingto their fractal dimensions. While combining the two innovations markedlyboosts graph-representation quality, it also adds non-trivial computationaloverhead. To mitigate the computational overhead of fractal dimensionestimation, we derive a one-shot estimator by proving that the dimensiondiscrepancy between original and renormalised graphs converges weakly to acentred Gaussian distribution. This theoretical insight enables a reduction indimension computation cost by an order of magnitude, cutting overall trainingtime by approximately 61%. The experiments show that FractalGCL not onlydelivers state-of-the-art results on standard benchmarks but also outperformstraditional baselines on traffic networks by an average margin of aboutremarkably 7%. Codes are available at(https://anonymous.4open.science/r/FractalGCL-0511).</description>
      <author>example@mail.com (Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang)</author>
      <guid isPermaLink="false">2505.11356v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2505.11099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Hybrid-Emba3D的新模型，用于解决点云分类任务中的效率与复杂度平衡问题。&lt;h4&gt;背景&lt;/h4&gt;点云分类任务需要高效提取局部几何特征，同时保持模型复杂度。Mamba架构利用状态空间模型（SSM）的线性复杂度优势来克服Transformer的计算瓶颈，但其在处理空间相关性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的模型，以增强局部特征的判别能力并提高分类准确率。&lt;h4&gt;方法&lt;/h4&gt;Hybrid-Emba3D模型通过结合几何特征耦合机制和跨路径特征混合，以及局部几何池化技术，来提升局部特征的提取和全局建模能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在ModelNet40数据集上达到了95.99%的分类准确率，同时仅增加了0.03M的额外参数。&lt;h4&gt;结论&lt;/h4&gt;Hybrid-Emba3D模型通过创新的设计，有效解决了点云分类任务中的效率与复杂度平衡问题，实现了新的性能水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The point cloud classification tasks face the dual challenge of efficientlyextracting local geometric features while maintaining model complexity. TheMamba architecture utilizes the linear complexity advantage of state spacemodels (SSMs) to overcome the computational bottleneck of Transformers whilebalancing global modeling capabilities. However, the inherent contradictionbetween its unidirectional dependency and the unordered nature of point cloudsimpedes modeling spatial correlation in local neighborhoods, thus constraininggeometric feature extraction. This paper proposes Hybrid-Emba3D, abidirectional Mamba model enhanced by geometry-feature coupling and cross-pathfeature hybridization. The Local geometric pooling with geometry-featurecoupling mechanism significantly enhances local feature discriminative powervia coordinated propagation and dynamic aggregation of geometric informationbetween local center points and their neighborhoods, without introducingadditional parameters. The designed Collaborative feature enhancer adoptsdual-path hybridization, effectively handling local mutations and sparse keysignals, breaking through the limitations of traditional SSM long-rangemodeling. Experimental results demonstrate that the proposed model achieves anew SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03Madditional.</description>
      <author>example@mail.com (Bin Liu, Chunyang Wang, Xuelian Liu, Guan Xi, Ge Zhang, Ziteng Yao, Mengxue Dong)</author>
      <guid isPermaLink="false">2505.11099v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Maximizing Asynchronicity in Event-based Neural Networks</title>
      <link>http://arxiv.org/abs/2505.11165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 5 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了EVA（EVent Asynchronous representation learning）框架，旨在解决事件相机异步、稀疏特性对机器学习（ML）的挑战，并实现了高表达性和通用性的事件表示学习。&lt;h4&gt;背景&lt;/h4&gt;事件相机提供高时间分辨率、低延迟和最小冗余的视觉数据，但其异步、稀疏的序列性质对标准张量机器学习提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出EVA框架，生成高表达性和通用性的事件表示，以解决异步到同步（A2S）转换中的表示表达性和泛化性问题。&lt;h4&gt;方法&lt;/h4&gt;EVA框架受到事件与语言之间相似性的启发，借鉴了语言建模中的线性注意力和自监督学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;EVA在识别任务（DVS128-Gesture和N-Cars）中优于之前的A2S方法，并且是第一个成功掌握检测任务的A2S框架，在Gen1数据集上达到了47.7 mAP的显著结果。&lt;h4&gt;结论&lt;/h4&gt;EVA框架具有推动实时事件基础视觉应用发展的转型潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras deliver visual data with high temporal resolution, low latency,and minimal redundancy, yet their asynchronous, sparse sequential naturechallenges standard tensor-based machine learning (ML). While the recentasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap byasynchronously encoding events into learned representations for ML pipelines,existing A2S approaches often sacrifice representation expressivity andgeneralizability compared to dense, synchronous methods. This paper introducesEVA (EVent Asynchronous representation learning), a novel A2S framework togenerate highly expressive and generalizable event-by-event representations.Inspired by the analogy between events and language, EVA uniquely adaptsadvances from language modeling in linear attention and self-supervisedlearning for its construction. In demonstration, EVA outperforms prior A2Smethods on recognition tasks (DVS128-Gesture and N-Cars), and represents thefirst A2S framework to successfully master demanding detection tasks, achievinga remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA'stransformative potential for advancing real-time event-based visionapplications.</description>
      <author>example@mail.com (Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang)</author>
      <guid isPermaLink="false">2505.11165v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>ASRC-SNN: Adaptive Skip Recurrent Connection Spiking Neural Network</title>
      <link>http://arxiv.org/abs/2505.11455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，循环脉冲神经网络（RSNNs）在长期时间建模方面展现出良好的潜力。许多研究集中于改进神经元模型并整合循环结构，利用其协同效应来提高脉冲神经网络（SNNs）的长期时间建模能力。然而，这些研究往往过分强调神经元的作用，忽视了将神经元和循环结构作为一个整体框架进行分析的重要性。&lt;h4&gt;背景&lt;/h4&gt;近年来，RSNNs在长期时间建模方面显示出良好的潜力，但现有研究过分强调神经元的作用，忽视了神经元和循环结构的整体分析。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在将神经元和循环结构视为一个整体系统，对时间维度上的梯度传播进行系统分析，并提出解决方案以缓解梯度消失问题，提高长期时间建模性能。&lt;h4&gt;方法&lt;/h4&gt;提出跳过循环连接（SRC）作为传统循环结构的替代方案，以有效缓解梯度消失问题并提升长期时间建模性能。此外，还提出了自适应跳过循环连接（ASRC），该方法可以在网络每层学习跳过循环连接的跳过跨度。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，将传统循环结构在RSNN中的使用替换为SRC，可以显著提高模型在时间基准数据集上的性能。此外，ASRC-SNN在时间建模能力和鲁棒性方面优于SRC-SNN。&lt;h4&gt;结论&lt;/h4&gt;通过引入跳过循环连接和自适应跳过循环连接，可以有效解决RSNN中的梯度消失问题，提高长期时间建模性能，并增强模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Recurrent Spiking Neural Networks (RSNNs) have shownpromising potential in long-term temporal modeling. Many studies focus onimproving neuron models and also integrate recurrent structures, leveragingtheir synergistic effects to improve the long-term temporal modelingcapabilities of Spiking Neural Networks (SNNs). However, these studies oftenplace an excessive emphasis on the role of neurons, overlooking the importanceof analyzing neurons and recurrent structures as an integrated framework. Inthis work, we consider neurons and recurrent structures as an integrated systemand conduct a systematic analysis of gradient propagation along the temporaldimension, revealing a challenging gradient vanishing problem. To address thisissue, we propose the Skip Recurrent Connection (SRC) as a replacement for thevanilla recurrent structure, effectively mitigating the gradient vanishingproblem and enhancing long-term temporal modeling performance. Additionally, wepropose the Adaptive Skip Recurrent Connection (ASRC), a method that can learnthe skip span of skip recurrent connection in each layer of the network.Experiments show that replacing the vanilla recurrent structure in RSNN withSRC significantly improves the model's performance on temporal benchmarkdatasets. Moreover, ASRC-SNN outperforms SRC-SNN in terms of temporal modelingcapabilities and robustness.</description>
      <author>example@mail.com (Shang Xu, Jiayu Zhang, Ziming Wang, Runhao Jiang, Rui Yan, Huajin Tang)</author>
      <guid isPermaLink="false">2505.11455v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions</title>
      <link>http://arxiv.org/abs/2505.11417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的数据集和评估基准，用于评估和改进在边缘设备上部署的小型语言模型，重点关注在智能家居环境中从多会话自然语言交互中进行用户画像。&lt;h4&gt;背景&lt;/h4&gt;现有小型语言模型在准确捕捉用户行为方面表现不足，而边缘设备上的处理具有保护用户隐私、最小化延迟和实现个性化体验等优势。&lt;h4&gt;目的&lt;/h4&gt;构建一个数据集和评估基准，以评估和提升小型语言模型在边缘设备上的表现，特别是其在用户画像方面的能力。&lt;h4&gt;方法&lt;/h4&gt;数据集的核心是结构化用户画像，包括一系列的行为模式。大语言模型（LLM）使用这些画像生成模拟真实、多样、情境感知的对话。主要任务是通过对交互历史的分析来推断用户的行为模式和偏好。&lt;h4&gt;主要发现&lt;/h4&gt;小型模型在重建用户画像方面有一定能力，但与大型模型相比，在准确捕捉用户行为方面仍有显著差距。&lt;h4&gt;结论&lt;/h4&gt;该数据集为在边缘设备上开发和发展行为建模提供了现实、结构化的测试平台，是朝着实现智能、尊重隐私的AI系统迈出的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices. The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel dataset and evaluation benchmark designed toassess and improve small language models deployable on edge devices, with afocus on user profiling from multi-session natural language interactions insmart home environments. At the core of the dataset are structured userprofiles, each defined by a set of routines - context-triggered, repeatablepatterns of behavior that govern how users interact with their home systems.Using these profiles as input, a large language model (LLM) generatescorresponding interaction sessions that simulate realistic, diverse, andcontext-aware dialogues between users and their devices.  The primary task supported by this dataset is profile reconstruction:inferring user routines and preferences solely from interactions history. Toassess how well current models can perform this task under realisticconditions, we benchmarked several state-of-the-art compact language models andcompared their performance against large foundation models. Our results showthat while small models demonstrate some capability in reconstructing profiles,they still fall significantly short of large models in accurately capturinguser behavior. This performance gap poses a major challenge - particularlybecause on-device processing offers critical advantages, such as preservinguser privacy, minimizing latency, and enabling personalized experiences withoutreliance on the cloud. By providing a realistic, structured testbed fordeveloping and evaluating behavioral modeling under these constraints, ourdataset represents a key step toward enabling intelligent, privacy-respectingAI systems that learn and adapt directly on user-owned devices.</description>
      <author>example@mail.com (Patryk Bartkowiak, Michal Podstawski)</author>
      <guid isPermaLink="false">2505.11417v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Assessing the Performance of Analog Training for Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.11067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为c-TTv2的新算法，用于解决模拟内存计算中深度学习和迁移学习所面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;模拟内存计算是一种新兴的计算范式，有望实现快速、并行和节能的深度学习训练和迁移学习。然而，由于缺乏合适的训练算法，这一承诺尚未实现。&lt;h4&gt;目的&lt;/h4&gt;评估c-TTv2算法在模拟迁移学习中的性能，并研究其对设备规格变化的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用Swin-ViT模型在CIFAR100数据集的一个子集上评估c-TTv2算法的性能，并研究其对权重传输噪声、对称点偏移和对称点变异等设备规格变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;c-TTv2算法通过利用切割技术解决了模拟内存计算中的一些挑战，并表现出良好的性能。&lt;h4&gt;结论&lt;/h4&gt;c-TTv2算法为模拟内存计算中的深度学习和迁移学习提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Analog in-memory computing is a next-generation computing paradigm that promises fast, parallel, and energy-efficient deep learning training and transfer learning (TL). However, achieving this promise has remained elusive due to a lack of suitable training algorithms. Analog memory devices exhibit asymmetric and non-linear switching behavior in addition to device-to-device variation, meaning that most, if not all, of the current off-the-shelf training algorithms cannot achieve good training outcomes. Also, recently introduced algorithms have enjoyed limited attention, as they require bi-directionally switching devices of unrealistically high symmetry and precision and are highly sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which leverages the chopped technique to address many of the challenges mentioned above. In this paper, we assess the performance of the c-TTv2 algorithm for analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also investigate the robustness of our algorithm to changes in some device specifications, including weight transfer noise, symmetry point skew, and symmetry point variability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog in-memory computing is a next-generation computing paradigm thatpromises fast, parallel, and energy-efficient deep learning training andtransfer learning (TL). However, achieving this promise has remained elusivedue to a lack of suitable training algorithms. Analog memory devices exhibitasymmetric and non-linear switching behavior in addition to device-to-devicevariation, meaning that most, if not all, of the current off-the-shelf trainingalgorithms cannot achieve good training outcomes. Also, recently introducedalgorithms have enjoyed limited attention, as they require bi-directionallyswitching devices of unrealistically high symmetry and precision and are highlysensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, whichleverages the chopped technique to address many of the challenges mentionedabove. In this paper, we assess the performance of the c-TTv2 algorithm foranalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We alsoinvestigate the robustness of our algorithm to changes in some devicespecifications, including weight transfer noise, symmetry point skew, andsymmetry point variability</description>
      <author>example@mail.com (Omobayode Fagbohungbe, Corey Lammie, Malte J. Rasch, Takashi Ando, Tayfun Gokmen, Vijay Narayanan)</author>
      <guid isPermaLink="false">2505.11067v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>STEP: A Unified Spiking Transformer Evaluation Platform for Fair and Reproducible Benchmarking</title>
      <link>http://arxiv.org/abs/2505.11151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了用于Spiking Transformers的统一基准框架STEP，旨在促进公平比较和原理分析。&lt;h4&gt;背景&lt;/h4&gt;Spiking Transformers结合了脉冲神经网络的高效性和自注意力机制的表征能力，但目前缺乏标准化实现、评估流程和一致的设计选择，阻碍了公平比较和原理分析。&lt;h4&gt;目的&lt;/h4&gt;提出STEP框架，支持广泛的任务，包括分类、分割和检测，并促进Spiking Transformers的公平比较和原理分析。&lt;h4&gt;方法&lt;/h4&gt;STEP框架提供模块化支持，包括脉冲神经元、输入编码、代理梯度以及多个后端（如SpikingJelly、BrainCog）。使用STEP框架，作者生产并评估了多个代表性模型，并进行了关于注意力设计、神经元类型、编码方案和时间建模能力的系统性消融研究。此外，还提出了一种统一的能量估计分析模型。&lt;h4&gt;主要发现&lt;/h4&gt;当前Spiking Transformers高度依赖卷积前端，缺乏强大的时间建模能力，强调了需要脉冲原生架构创新。&lt;h4&gt;结论&lt;/h4&gt;STEP框架有助于Spiking Transformers的研究和发展，指出了当前架构的局限性，并提出了未来研究的方向。&lt;h4&gt;翻译&lt;/h4&gt;Spiking Transformers作为一种结合脉冲神经网络效率和自注意力表征能力的架构，近年来受到了关注。然而，由于缺乏标准化的实现、评估流程和一致的设计选择，公平比较和原理分析受到了阻碍。本文介绍了STEP，一个用于Spiking Transformers的统一基准框架，支持广泛的任务，包括在静态、基于事件和时序数据集上的分类、分割和检测。STEP提供模块化支持，包括脉冲神经元、输入编码、代理梯度以及多个后端（如SpikingJelly、BrainCog）。使用STEP，我们生产并评估了多个代表性模型，并进行了关于注意力设计、神经元类型、编码方案和时间建模能力的系统性消融研究。我们还提出了一种统一的能量估计分析模型，考虑了脉冲稀疏性、位宽和内存访问，并表明量化的人工神经网络可能提供相当的或更好的能效。我们的结果表明，当前的Spiking Transformers高度依赖于卷积前端，缺乏强大的时间建模能力，强调了需要脉冲原生架构创新。完整代码可在https://github.com/Fancyssc/STEP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking Transformers have recently emerged as promising architectures forcombining the efficiency of spiking neural networks with the representationalpower of self-attention. However, the lack of standardized implementations,evaluation pipelines, and consistent design choices has hindered faircomparison and principled analysis. In this paper, we introduce \textbf{STEP},a unified benchmark framework for Spiking Transformers that supports a widerange of tasks, including classification, segmentation, and detection acrossstatic, event-based, and sequential datasets. STEP provides modular support fordiverse components such as spiking neurons, input encodings, surrogategradients, and multiple backends (e.g., SpikingJelly, BrainCog). Using STEP, wereproduce and evaluate several representative models, and conduct systematicablation studies on attention design, neuron types, encoding schemes, andtemporal modeling capabilities. We also propose a unified analytical model forenergy estimation, accounting for spike sparsity, bitwidth, and memory access,and show that quantized ANNs may offer comparable or better energy efficiency.Our results suggest that current Spiking Transformers rely heavily onconvolutional frontends and lack strong temporal modeling, underscoring theneed for spike-native architectural innovations. The full code is available at:https://github.com/Fancyssc/STEP</description>
      <author>example@mail.com (Sicheng Shen, Dongcheng Zhao, Linghao Feng, Zeyang Yue, Jindong Li, Tenglong Li, Guobin Shen, Yi Zeng)</author>
      <guid isPermaLink="false">2505.11151v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>What Can We Learn From MIMO Graph Convolutions?</title>
      <link>http://arxiv.org/abs/2505.11346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多输入多输出（MIMO）图卷积方法，并在多计算图上实现了局部化近似。&lt;h4&gt;背景&lt;/h4&gt;大多数图神经网络（GNNs）在单输入单输出（SISO）情况下使用图傅里叶域中导出的通用图卷积近似。&lt;h4&gt;目的&lt;/h4&gt;在MIMO情况下直接近似MIMO图卷积。&lt;h4&gt;方法&lt;/h4&gt;通过卷积定理推导出MIMO图卷积，并在MIMO情况下直接近似。引入了局部化MIMO图卷积（LMGCs），这是一种泛化了许多线性消息传递神经网络的局部近似方法。&lt;h4&gt;主要发现&lt;/h4&gt;发现MIMO图卷积的关键特性是作用在多个计算图上，或者等价地，为每对节点应用不同的特征变换。对于几乎所有的边权重选择，证明了LMGCs在多集上是单射的，并且当使用多个计算图时，结果表示是线性无关的。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，LMGC可以结合各种方法的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数图神经网络（GNNs）利用图傅里叶域中导出的通用图卷积的近似。虽然GNNs通常应用于多输入多输出（MIMO）情况，但近似是在单输入单输出（SISO）情况下进行的。在这项工作中，我们首先通过卷积定理推导出MIMO图卷积，并在MIMO情况下直接近似它。我们发现图卷积的关键MIMO特定属性是在多个计算图上操作，或者等价地，为每对节点应用不同的特征变换。作为一种局部近似，我们引入了局部化MIMO图卷积（LMGCs），它泛化了许多线性消息传递神经网络。对于几乎所有的边权重选择，我们证明了具有单个计算图的LMGC在多集上是单射的，并且当使用多个计算图时，结果表示是线性无关的。我们的实验结果证实了LMGC可以结合各种方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most graph neural networks (GNNs) utilize approximations of the general graphconvolution derived in the graph Fourier domain. While GNNs are typicallyapplied in the multi-input multi-output (MIMO) case, the approximations areperformed in the single-input single-output (SISO) case. In this work, we firstderive the MIMO graph convolution through the convolution theorem andapproximate it directly in the MIMO case. We find the key MIMO-specificproperty of the graph convolution to be operating on multiple computationalgraphs, or equivalently, applying distinct feature transformations for eachpair of nodes. As a localized approximation, we introduce localized MIMO graphconvolutions (LMGCs), which generalize many linear message-passing neuralnetworks. For almost every choice of edge weights, we prove that LMGCs with asingle computational graph are injective on multisets, and the resultingrepresentations are linearly independent when more than one computational graphis used. Our experimental results confirm that an LMGC can combine the benefitsof various methods.</description>
      <author>example@mail.com (Andreas Roth, Thomas Liebig)</author>
      <guid isPermaLink="false">2505.11346v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining</title>
      <link>http://arxiv.org/abs/2505.11293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为'B3'的批量构建策略，旨在为对比学习（CL）创建高质量批量，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种常用的训练嵌入模型的技术，通过将语义相似的示例（正例）拉近，将不相似的示例（负例）推远。&lt;h4&gt;目的&lt;/h4&gt;提高训练批次的大小和质量，以增强对比学习模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的教师嵌入模型对所有数据集中的示例进行排名，构建稀疏相似图，然后应用社区检测算法识别相互之间作为强负例的示例集群，最后使用这些集群构建包含丰富负例的批量。&lt;h4&gt;主要发现&lt;/h4&gt;在MMEB多模态嵌入基准测试（36个任务）上，该方法在7B和2B模型规模上分别比以前的最佳方法提高了1.3和2.9个点。值得注意的是，使用B3训练的模型即使批量大小仅为64，也超越了现有的最佳结果，而其他方法所需的批量大小至少是64的4-16倍。&lt;h4&gt;结论&lt;/h4&gt;B3批量构建策略能够有效提高对比学习模型的效果，即使在较小的批量大小下也能实现显著性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) is a prevalent technique for training embeddingmodels, which pulls semantically similar examples (positives) closer in therepresentation space while pushing dissimilar ones (negatives) further apart. Akey source of negatives are 'in-batch' examples, i.e., positives from otherexamples in the batch. Effectiveness of such models is hence stronglyinfluenced by the size and quality of training batches. In this work, wepropose 'Breaking the Batch Barrier' (B3), a novel batch construction strategydesigned to curate high-quality batches for CL. Our approach begins by using apretrained teacher embedding model to rank all examples in the dataset, fromwhich a sparse similarity graph is constructed. A community detection algorithmis then applied to this graph to identify clusters of examples that serve asstrong negatives for one another. The clusters are then used to constructbatches that are rich in in-batch negatives. Empirical results on the MMEBmultimodal embedding benchmark (36 tasks) demonstrate that our method sets anew state of the art, outperforming previous best methods by +1.3 and +2.9points at the 7B and 2B model scales, respectively. Notably, models trainedwith B3 surpass existing state-of-the-art results even with a batch size assmall as 64, which is 4-16x smaller than that required by other methods.</description>
      <author>example@mail.com (Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Karthikeyan K, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra)</author>
      <guid isPermaLink="false">2505.11293v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Random Client Selection on Contrastive Federated Learning for Tabular Data</title>
      <link>http://arxiv.org/abs/2505.10759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对垂直联邦学习（VFL）中的梯度攻击进行了实验分析，并评估了随机客户端选择作为防御策略的有效性。&lt;h4&gt;背景&lt;/h4&gt;垂直联邦学习（VFL）通过保护隐私的方式在多方之间进行机器学习协作，但中间计算共享过程中存在信息泄露的风险。&lt;h4&gt;目的&lt;/h4&gt;评估随机客户端选择在对比联邦学习（CFL）环境中防御梯度攻击的有效性。&lt;h4&gt;方法&lt;/h4&gt;通过广泛的实验分析梯度攻击，并评估随机客户端选择作为防御策略。&lt;h4&gt;主要发现&lt;/h4&gt;随机客户端选择在CFL网络中对抗梯度攻击特别有效。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为在对比联邦学习系统中实施鲁棒的安全措施提供了有价值的见解，有助于开发更安全的协作学习框架。&lt;h4&gt;翻译&lt;/h4&gt;Vertical Federated Learning (VFL) has revolutionised collaborative machinelearning by enabling privacy-preserving model training across multiple parties. However, it remains vulnerable to information leakage during intermediate computation sharing. While Contrastive Federated Learning (CFL) was introduced to mitigate these privacy concerns through representation learning, it still faces challenges from gradient-based attacks. This paper presents a comprehensive experimental analysis of gradient-based attacks in CFL environments and evaluates random client selection as a defensive strategy. Through extensive experimentation, we demonstrate that random client selection proves particularly effective in defending against gradient attacks in the CFL network. Our findings provide valuable insights for implementing robust security measures in contrastive federated learning systems, contributing to the development of more secure collaborative learning frameworks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vertical Federated Learning (VFL) has revolutionised collaborative machinelearning by enabling privacy-preserving model training across multiple parties.However, it remains vulnerable to information leakage during intermediatecomputation sharing. While Contrastive Federated Learning (CFL) was introducedto mitigate these privacy concerns through representation learning, it stillfaces challenges from gradient-based attacks. This paper presents acomprehensive experimental analysis of gradient-based attacks in CFLenvironments and evaluates random client selection as a defensive strategy.Through extensive experimentation, we demonstrate that random client selectionproves particularly effective in defending against gradient attacks in the CFLnetwork. Our findings provide valuable insights for implementing robustsecurity measures in contrastive federated learning systems, contributing tothe development of more secure collaborative learning frameworks</description>
      <author>example@mail.com (Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua)</author>
      <guid isPermaLink="false">2505.10759v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis</title>
      <link>http://arxiv.org/abs/2505.10751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress, accepted in Novel Approaches for Precision  Agriculture and Forestry with Autonomous Robots, ICRA 2025 Workshop - May 23,  2025 - Atlanta, GA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种生成森林环境语义分割点云的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于获取成本高、传感器要求严格以及耗时，公开可用的点云数据集很少。此外，目前没有通过结构从运动（SfM）算法对图像进行标注的公开数据集，这可能是由于缺乏能够将语义分割信息映射到精确点云中的SfM算法，尤其是在森林等具有挑战性的环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成森林环境语义分割点云的方法。&lt;h4&gt;方法&lt;/h4&gt;使用定制的森林模拟器生成多样化的森林场景的RGB图像及其相应的语义分割掩码，然后使用修改后的开源SfM软件对这些标记图像进行处理，以在3D重建过程中保留语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;生成的点云提供了几何和语义细节，为训练和评估旨在分割通过SfM获得的真实森林点云的深度学习模型提供了宝贵资源。&lt;h4&gt;结论&lt;/h4&gt;该方法为森林环境点云的语义分割提供了新的解决方案，有助于推动相关深度学习模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although the use of remote sensing technologies for monitoring forestedenvironments has gained increasing attention, publicly available point clouddatasets remain scarce due to the high costs, sensor requirements, andtime-intensive nature of their acquisition. Moreover, as far as we are aware,there are no public annotated datasets generated through Structure From Motion(SfM) algorithms applied to imagery, which may be due to the lack of SfMalgorithms that can map semantic segmentation information into an accuratepoint cloud, especially in a challenging environment like forests.  In this work, we present a novel pipeline for generating semanticallysegmented point clouds of forest environments. Using a custom-built forestsimulator, we generate realistic RGB images of diverse forest scenes along withtheir corresponding semantic segmentation masks. These labeled images are thenprocessed using modified open-source SfM software capable of preservingsemantic information during 3D reconstruction. The resulting point cloudsprovide both geometric and semantic detail, offering a valuable resource fortraining and evaluating deep learning models aimed at segmenting real forestpoint clouds obtained via SfM.</description>
      <author>example@mail.com (Francisco Raverta Capua, Pablo De Cristoforis)</author>
      <guid isPermaLink="false">2505.10751v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction</title>
      <link>http://arxiv.org/abs/2505.11063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Thought-Aligner的动态思维校正模块，用于解决LLM-based自主代理在执行复杂多步任务时的安全对齐问题。&lt;h4&gt;背景&lt;/h4&gt;LLM-based自主代理具有推理、工具调用和环境交互的能力，但内部思维过程可能带来潜在风险。&lt;h4&gt;目的&lt;/h4&gt;为了解决长时程行为轨迹中的安全对齐挑战。&lt;h4&gt;方法&lt;/h4&gt;Thought-Aligner使用轻量级和资源高效的模型，在每次动作执行前即时纠正高风险的思维，并在不改变代理框架的前提下提高安全性和适用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Thought-Aligner将代理的行为安全性从大约50%提升到90%，同时保持响应延迟低于100ms。&lt;h4&gt;结论&lt;/h4&gt;Thought-Aligner为LLM-based代理提供了一种实用的动态安全解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a dynamic thought correction module named Thought-Aligner to address the safety alignment challenges in long-horizon behavioral trajectories of LLM-based autonomous agents.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based autonomous agents possess capabilities such as reasoning, toolinvocation, and environment interaction, enabling the execution of complexmulti-step tasks. The internal reasoning process, i.e., thought, of behavioraltrajectory significantly influences tool usage and subsequent actions but canintroduce potential risks. Even minor deviations in the agent's thought maytrigger cascading effects leading to irreversible safety incidents. To addressthe safety alignment challenges in long-horizon behavioral trajectories, wepropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizinga lightweight and resource-efficient model, Thought-Aligner corrects eachhigh-risk thought on the fly before each action execution. The correctedthought is then reintroduced to the agent, ensuring safer subsequent decisionsand tool interactions. Importantly, Thought-Aligner modifies only the reasoningphase without altering the underlying agent framework, making it easy to deployand widely applicable to various agent frameworks. To train the Thought-Alignermodel, we construct an instruction dataset across ten representative scenariosand simulate ReAct execution trajectories, generating 5,000 diverseinstructions and more than 11,400 safe and unsafe thought pairs. The model isfine-tuned using contrastive learning techniques. Experiments across threeagent safety benchmarks involving 12 different LLMs demonstrate thatThought-Aligner raises agent behavioral safety from approximately 50% in theunprotected setting to 90% on average. Additionally, Thought-Aligner maintainsresponse latency below 100ms with minimal resource usage, demonstrating itscapability for efficient deployment, broad applicability, and timelyresponsiveness. This method thus provides a practical dynamic safety solutionfor the LLM-based agents.</description>
      <author>example@mail.com (Changyue Jiang, Xudong Pan, Min Yang)</author>
      <guid isPermaLink="false">2505.11063v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</title>
      <link>http://arxiv.org/abs/2505.10696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review for IEEE conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为TartanGround的大型多模态数据集，旨在推进在多样化环境中运行的地面机器人的感知和自主能力。&lt;h4&gt;背景&lt;/h4&gt;当前数据集难以泛化到不同的场景，限制了机器人感知和自主技术的发展。&lt;h4&gt;目的&lt;/h4&gt;构建TartanGround数据集，用于训练和评估机器人感知和自主任务。&lt;h4&gt;方法&lt;/h4&gt;数据集在多个逼真的模拟环境中收集，包括RGB立体相机、深度、光流、立体视差、LiDAR点云、真实姿态、语义分割图像和占用图等。使用集成自动管道生成模拟地面机器人的运动轨迹，包括轮式和足式机器人，共收集910个轨迹和1.5百万个样本。&lt;h4&gt;主要发现&lt;/h4&gt;在占用预测和SLAM任务上的评估表明，基于现有数据集训练的方法难以泛化到不同的场景。&lt;h4&gt;结论&lt;/h4&gt;TartanGround可以作为训练和评估多种学习任务的测试平台，包括占用预测、SLAM、神经场景表示、基于感知的导航等，有助于推进机器人感知和自主技术的发展，使其模型更加通用和鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, and occupancy maps with semantic labels. Data is collected using an integrated automatic pipeline, which generates trajectories mimicking the motion patterns of various ground robot platforms, including wheeled and legged robots. We collect 910 trajectories across 70 environments, resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM tasks reveal that state-of-the-art methods trained on existing datasets struggle to generalize across diverse scenes. TartanGround can serve as a testbed for training and evaluation of a broad range of learning-based tasks, including occupancy prediction, SLAM, neural scene representation, perception-based navigation, and more, enabling advancements in robotic perception and autonomy towards achieving robust models generalizable to more diverse scenarios. The dataset and codebase for data collection will be made publicly available upon acceptance. Webpage: https://tartanair.org/tartanground&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TartanGround, a large-scale, multi-modal dataset to advance theperception and autonomy of ground robots operating in diverse environments.This dataset, collected in various photorealistic simulation environmentsincludes multiple RGB stereo cameras for 360-degree coverage, along with depth,optical flow, stereo disparity, LiDAR point clouds, ground truth poses,semantic segmented images, and occupancy maps with semantic labels. Data iscollected using an integrated automatic pipeline, which generates trajectoriesmimicking the motion patterns of various ground robot platforms, includingwheeled and legged robots. We collect 910 trajectories across 70 environments,resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAMtasks reveal that state-of-the-art methods trained on existing datasetsstruggle to generalize across diverse scenes. TartanGround can serve as atestbed for training and evaluation of a broad range of learning-based tasks,including occupancy prediction, SLAM, neural scene representation,perception-based navigation, and more, enabling advancements in roboticperception and autonomy towards achieving robust models generalizable to morediverse scenarios. The dataset and codebase for data collection will be madepublicly available upon acceptance. Webpage: https://tartanair.org/tartanground</description>
      <author>example@mail.com (Manthan Patel, Fan Yang, Yuheng Qiu, Cesar Cadena, Sebastian Scherer, Marco Hutter, Wenshan Wang)</author>
      <guid isPermaLink="false">2505.10696v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Visual Planning: Let's Think Only with Images</title>
      <link>http://arxiv.org/abs/2505.11409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables  including references and appendices)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型和多模态语言模型在机器推理方面的进步，并提出了一种新的视觉规划范式，旨在通过纯视觉表示进行规划，以增强机器在涉及空间和几何信息任务中的推理能力。&lt;h4&gt;背景&lt;/h4&gt;尽管LLMs和MLLMs在多种任务中提高了机器推理能力，但它们主要依赖纯文本作为表达和结构化推理的媒介，即使在存在视觉信息的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出视觉规划范式，通过纯视觉表示进行规划，独立于文本，特别是在涉及空间和几何信息任务中。&lt;h4&gt;方法&lt;/h4&gt;引入了视觉规划通过强化学习（VPRL）框架，利用GRPO（通用视觉模型后训练）对视觉导航任务进行训练，包括FrozenLake、Maze和MiniBehavior。&lt;h4&gt;主要发现&lt;/h4&gt;视觉规划范式在规划任务中优于所有仅基于文本的推理方法，证明了视觉规划是语言推理的一个可行且有前景的替代方案。&lt;h4&gt;结论&lt;/h4&gt;视觉规划为那些从直观的图像推理中受益的任务开辟了新的途径，并建立了视觉规划在机器推理中的地位。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Models (LLMs) and their multimodalextensions (MLLMs) have substantially enhanced machine reasoning across diversetasks. However, these models predominantly rely on pure text as the medium forboth expressing and structuring reasoning, even when visual information ispresent. In this work, we argue that language may not always be the mostnatural or effective modality for reasoning, particularly in tasks involvingspatial and geometrical information. Motivated by this, we propose a newparadigm, Visual Planning, which enables planning through purely visualrepresentations, independent of text. In this paradigm, planning is executedvia sequences of images that encode step-by-step inference in the visualdomain, akin to how humans sketch or visualize future actions. We introduce anovel reinforcement learning framework, Visual Planning via ReinforcementLearning (VPRL), empowered by GRPO for post-training large vision models,leading to substantial improvements in planning in a selection ofrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Ourvisual planning paradigm outperforms all other planning variants that conductreasoning in the text-only space. Our results establish Visual Planning as aviable and promising alternative to language-based reasoning, opening newavenues for tasks that benefit from intuitive, image-based inference.</description>
      <author>example@mail.com (Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić)</author>
      <guid isPermaLink="false">2505.11409v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework</title>
      <link>http://arxiv.org/abs/2505.11335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单高效的图神经网络校准方法，旨在解决GNN预测置信度常常被低估的问题。&lt;h4&gt;背景&lt;/h4&gt;GNN在图相关任务中表现出色，但其预测置信度往往校准不当，通常表现出过度自信不足，这影响了决策的可靠性。&lt;h4&gt;目的&lt;/h4&gt;为了解决GNN置信度校准不准确的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于模型和预测置信度之间内在关系的校准方法，并建立了统一的理论框架。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明，通过降低最终层参数的权重衰减可以减轻GNN的过度自信，而节点级校准作为更精细的补充，鼓励测试节点在最终层表示中更接近其预测的类别中心。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting under-confidence, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce additional calibration components, which fail to capture the intrinsic relationship between the model and the prediction confidence, resulting in limited theoretical guarantees and increased computational overhead. To address this issue, we propose a simple yet efficient graph calibration method. We establish a unified theoretical framework revealing that model confidence is jointly governed by class-centroid-level and node-level calibration at the final layer. Based on this insight, we theoretically show that reducing the weight decay of the final-layer parameters alleviates GNN under-confidence by acting on the class-centroid level, while node-level calibration acts as a finer-grained complement to class-centroid level calibration, which encourages each test node to be closer to its predicted class centroid at the final-layer representations. Extensive experiments validate the superiority of our method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness ongraph-based tasks. However, their predictive confidence is often miscalibrated,typically exhibiting under-confidence, which harms the reliability of theirdecisions. Existing calibration methods for GNNs normally introduce additionalcalibration components, which fail to capture the intrinsic relationshipbetween the model and the prediction confidence, resulting in limitedtheoretical guarantees and increased computational overhead. To address thisissue, we propose a simple yet efficient graph calibration method. We establisha unified theoretical framework revealing that model confidence is jointlygoverned by class-centroid-level and node-level calibration at the final layer.Based on this insight, we theoretically show that reducing the weight decay ofthe final-layer parameters alleviates GNN under-confidence by acting on theclass-centroid level, while node-level calibration acts as a finer-grainedcomplement to class-centroid level calibration, which encourages each test nodeto be closer to its predicted class centroid at the final-layerrepresentations. Extensive experiments validate the superiority of our method.</description>
      <author>example@mail.com (Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu)</author>
      <guid isPermaLink="false">2505.11335v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Learning Repetition-Invariant Representations for Polymer Informatics</title>
      <link>http://arxiv.org/abs/2505.10726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,3 figuares&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GRIN的新方法，用于学习聚合物表示，该方法可以处理不同重复单元数量的聚合物结构，并在同聚物和共聚物基准测试中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物在能源存储、建筑、医药和航空航天等领域广泛应用。然而，现有的图神经网络方法对于小分子有效，但无法对具有不同单元数量的聚合物结构产生一致的向量表示。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文提出了GRIN方法，旨在学习对重复单元数量不变的聚合物表示。&lt;h4&gt;方法&lt;/h4&gt;GRIN方法通过集成基于图的最大生成树对齐和重复单元增强，确保结构一致性。从模型和数据的角度提供了重复不变性的理论保证，并证明了三个重复单元是获得最佳不变表示所需的最小增强。&lt;h4&gt;主要发现&lt;/h4&gt;GRIN在homopolymer和copolymer基准测试中优于现有基线，学习到稳定且重复不变的表示，这些表示能够有效地推广到未见规模的聚合物链。&lt;h4&gt;结论&lt;/h4&gt;GRIN是一种有效的聚合物表示学习方法，能够处理不同重复单元数量的聚合物结构，并在实际应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：聚合物是由称为单体的重复结构单元组成的大分子，广泛应用于能源存储、建筑、医药和航空航天等领域。然而，尽管现有的图神经网络方法对小分子有效，但它们只能模拟聚合物的单个单元，并且无法为具有不同单元数量的真实聚合物结构产生一致的向量表示。为了应对这一挑战，我们引入了图重复不变性（GRIN），这是一种新的学习方法，用于学习对它们图表示中重复单元数量不变性的聚合物表示。GRIN通过集成基于图的最大生成树对齐和重复单元增强来确保结构一致性。我们从模型和数据的角度提供了重复不变性的理论保证，证明了三个重复单元是获得最佳不变表示所需的最小增强。GRIN在homopolymer和copolymer基准测试中都优于最先进的基线，学习到稳定、重复不变的表示，这些表示能够有效地推广到未见规模的聚合物链。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Polymers are large macromolecules composed of repeating structural unitsknown as monomers and are widely applied in fields such as energy storage,construction, medicine, and aerospace. However, existing graph neural networkmethods, though effective for small molecules, only model the single unit ofpolymers and fail to produce consistent vector representations for the truepolymer structure with varying numbers of units. To address this challenge, weintroduce Graph Repetition Invariance (GRIN), a novel method to learn polymerrepresentations that are invariant to the number of repeating units in theirgraph representations. GRIN integrates a graph-based maximum spanning treealignment with repeat-unit augmentation to ensure structural consistency. Weprovide theoretical guarantees for repetition-invariance from both model anddata perspectives, demonstrating that three repeating units are the minimalaugmentation required for optimal invariant representation learning. GRINoutperforms state-of-the-art baselines on both homopolymer and copolymerbenchmarks, learning stable, repetition-invariant representations thatgeneralize effectively to polymer chains of unseen sizes.</description>
      <author>example@mail.com (Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang)</author>
      <guid isPermaLink="false">2505.10726v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</title>
      <link>http://arxiv.org/abs/2505.10810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 2 tables. Presented at the CVPR 2025 Human  Motion Generation (HuMoGen) Workshop. Introduces MoCLIP, a CLIP-based  fine-tuning strategy for motion generation, with results on HumanML3D dataset  and ablation studies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MoCLIP的模型，用于人类运动生成，该模型通过结合运动编码头和对比学习，提升了运动生成效果。&lt;h4&gt;背景&lt;/h4&gt;人类运动生成对于动画、机器人和虚拟现实等领域至关重要，需要模型能够从文本描述中有效捕捉运动动态。&lt;h4&gt;目的&lt;/h4&gt;提高基于文本描述的运动生成模型的运动逼真度和准确性。&lt;h4&gt;方法&lt;/h4&gt;MoCLIP是一个经过微调的CLIP模型，增加了运动编码头，并使用对比学习和锚定损失在运动序列上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;MoCLIP在保持与现有CLIP管道兼容的同时，提高了Top-1、Top-2和Top-3的准确性，同时保持了有竞争力的FID，从而改善了文本到运动的对齐结果。&lt;h4&gt;结论&lt;/h4&gt;MoCLIP是一种强大的框架，可以增强运动生成，其灵活性和有效性得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human motion generation is essential for fields such as animation, robotics,and virtual reality, requiring models that effectively capture motion dynamicsfrom text descriptions. Existing approaches often rely on ContrastiveLanguage-Image Pretraining (CLIP)-based text encoders, but their training ontext-image pairs constrains their ability to understand temporal and kinematicstructures inherent in motion and motion generation. This work introducesMoCLIP, a fine-tuned CLIP model with an additional motion encoding head,trained on motion sequences using contrastive learning and tethering loss. Byexplicitly incorporating motion-aware representations, MoCLIP enhances motionfidelity while remaining compatible with existing CLIP-based pipelines andseamlessly integrating into various CLIP-based methods. Experiments demonstratethat MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintainingcompetitive FID, leading to improved text-to-motion alignment results. Theseresults highlight MoCLIP's versatility and effectiveness, establishing it as arobust framework for enhancing motion generation.</description>
      <author>example@mail.com (Gabriel Maldonado, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi)</author>
      <guid isPermaLink="false">2505.10810v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2505.10601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，基于范围视图的激光雷达点云超分辨率技术作为一种低成本生成高分辨率点云数据的方法受到广泛关注。然而，由于激光雷达点云的稀疏和不规则结构，点云超分辨率问题仍然是一个具有挑战性的课题，尤其是在新型视图下的点云上采样方面。&lt;h4&gt;背景&lt;/h4&gt;激光雷达点云的稀疏性和不规则结构使得点云超分辨率问题具有挑战性，尤其是对于新型视图下的点云上采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SRMamba的新方法，用于在稀疏场景中对激光雷达点云进行超分辨率处理，解决从新型视图中恢复点云3D空间结构的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;SRMamba方法通过以下技术实现：基于Hough投票的投影技术消除范围图像中的水平线性空洞；通过视觉状态空间模型和多方向扫描机制提高长距离依赖关系的建立，并关注垂直三维空间中的潜在几何特征，以减少由于范围图像导致的3D空间结构信息损失；采用非对称U-Net网络以适应不同光束计数激光雷达的输入特征，实现多光束点云的超分辨率重建。&lt;h4&gt;主要发现&lt;/h4&gt;SRMamba在多个具有挑战性的公共激光雷达数据集（SemanticKITTI和nuScenes）上进行了实验，在定性和定量评估中均显示出比其他算法显著的优越性。&lt;h4&gt;结论&lt;/h4&gt;SRMamba在点云超分辨率任务中表现出色，特别是在处理稀疏场景和新型视图下的点云上采样方面。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, range-view-based LiDAR point cloud super-resolution techniques attract significant attention as a low-cost method for generating higher-resolution point cloud data. However, due to the sparsity and irregular structure of LiDAR point clouds, the point cloud super-resolution problem remains a challenging topic, especially for point cloud upsampling under novel views. In this paper, we propose SRMamba, a novel method for super-resolution of LiDAR point clouds in sparse scenes, addressing the key challenge of recovering the 3D spatial structure of point clouds from novel views. Specifically, we implement projection technique based on Hough Voting and Hole Compensation strategy to eliminate horizontally linear holes in range image. To improve the establishment of long-distance dependencies and to focus on potential geometric features in vertical 3D space, we employ Visual State Space model and Multi-Directional Scanning mechanism to mitigate the loss of 3D spatial structural information due to the range image. Additionally, an asymmetric U-Net network adapts to the input characteristics of LiDARs with different beam counts, enabling super-resolution reconstruction for multi-beam point clouds. We conduct a series of experiments on multiple challenging public LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates significant superiority over other algorithms in both qualitative and quantitative evaluations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, range-view-based LiDAR point cloud super-resolutiontechniques attract significant attention as a low-cost method for generatinghigher-resolution point cloud data. However, due to the sparsity and irregularstructure of LiDAR point clouds, the point cloud super-resolution problemremains a challenging topic, especially for point cloud upsampling under novelviews. In this paper, we propose SRMamba, a novel method for super-resolutionof LiDAR point clouds in sparse scenes, addressing the key challenge ofrecovering the 3D spatial structure of point clouds from novel views.Specifically, we implement projection technique based on Hough Voting and HoleCompensation strategy to eliminate horizontally linear holes in range image. Toimprove the establishment of long-distance dependencies and to focus onpotential geometric features in vertical 3D space, we employ Visual State Spacemodel and Multi-Directional Scanning mechanism to mitigate the loss of 3Dspatial structural information due to the range image. Additionally, anasymmetric U-Net network adapts to the input characteristics of LiDARs withdifferent beam counts, enabling super-resolution reconstruction for multi-beampoint clouds. We conduct a series of experiments on multiple challenging publicLiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstratessignificant superiority over other algorithms in both qualitative andquantitative evaluations.</description>
      <author>example@mail.com (Chuang Chen, Wenyi Ge)</author>
      <guid isPermaLink="false">2505.10601v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning</title>
      <link>http://arxiv.org/abs/2505.11349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了科学机器学习中的时间序列基础模型在预测物理系统方面的能力，发现这些模型在零样本预测方面表现出色，但未能有效捕捉底层物理规律。&lt;h4&gt;背景&lt;/h4&gt;近年来，时间序列基础模型在科学机器学习中展现出预测物理系统的能力，包括零样本预测，即模型仅根据系统短轨迹预测未来状态。&lt;h4&gt;目的&lt;/h4&gt;研究时间序列基础模型在物理系统预测中的应用及其局限性。&lt;h4&gt;方法&lt;/h4&gt;对比分析了基础模型与直接上下文鹦鹉学舌模型在预测动态系统方面的性能，并探讨了上下文鹦鹉学舌与归纳头之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在预测物理系统时准确率较高，但未能有效捕捉底层物理规律；直接上下文鹦鹉学舌模型在预测动态系统方面表现优于最先进的时间序列基础模型，且计算成本更低；上下文鹦鹉学舌与归纳头之间存在联系，解释了为何大型语言模型可以用于时间序列预测；上下文长度与预测准确率之间的关系与吸引子的分形维度相关。&lt;h4&gt;结论&lt;/h4&gt;上下文鹦鹉学舌是简单但难以超越的时间序列基础模型基准，有助于识别超越鹦鹉学舌的上下文学习策略。&lt;h4&gt;翻译&lt;/h4&gt;摘要：最近开发的时间序列基础模型在科学机器学习中展现出预测物理系统的能力。这些能力包括零样本预测，即模型仅根据系统短轨迹预测未来状态。在这里，我们表明应用于物理系统的基础模型可以给出准确的预测，但它们无法发展出对底层物理的有意义表示。相反，基础模型通常通过上下文鹦鹉学舌进行预测，这是一种简单的零样本预测策略，直接从上下文中复制。因此，一个简单的直接上下文鹦鹉学舌模型在预测各种动态系统时得分高于最先进的时间序列基础模型，计算成本仅为后者的很小一部分。我们将在上下文鹦鹉学舌与归纳头之间建立联系，解释为什么在文本上训练的大型语言模型可以重新用于时间序列预测。我们的动态系统视角将预测准确率与上下文长度之间的缩放关系与吸引子的分形维度联系起来，为先前观察到的上下文神经缩放定律提供了见解。因此，上下文鹦鹉学舌作为简单但难以超越的时间序列基础模型基准，有助于识别超越鹦鹉学舌的上下文学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently-developed time series foundation models for scientific machinelearning exhibit emergent abilities to predict physical systems. Theseabilities include zero-shot forecasting, in which a model forecasts futurestates of a system given only a short trajectory as context. Here, we show thatfoundation models applied to physical systems can give accurate predictions,but that they fail to develop meaningful representations of the underlyingphysics. Instead, foundation models often forecast by context parroting, asimple zero-shot forecasting strategy that copies directly from the context. Asa result, a naive direct context parroting model scores higher thanstate-of-the-art time-series foundation models on predicting a diverse range ofdynamical systems, at a tiny fraction of the computational cost. We draw aparallel between context parroting and induction heads, which explains whylarge language models trained on text can be repurposed for time seriesforecasting. Our dynamical systems perspective also ties the scaling betweenforecast accuracy and context length to the fractal dimension of the attractor,providing insight into the previously observed in-context neural scaling laws.Context parroting thus serves as a simple but tough-to-beat baseline for futuretime-series foundation models and can help identify in-context learningstrategies beyond parroting.</description>
      <author>example@mail.com (Yuanzhao Zhang, William Gilpin)</author>
      <guid isPermaLink="false">2505.11349v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>FRET: Feature Redundancy Elimination for Test Time Adaptation</title>
      <link>http://arxiv.org/abs/2505.10641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FRET的测试时间自适应方法，旨在解决深度学习模型在测试数据分布偏移时的一般化问题，特别是在只有预训练模型和无标签测试数据的情况下。&lt;h4&gt;背景&lt;/h4&gt;测试时间自适应（TTA）旨在提高深度学习模型在测试数据与训练数据分布不一致时的泛化能力。这在需要保护隐私的应用中尤为重要。&lt;h4&gt;目的&lt;/h4&gt;旨在减少特征冗余，提高模型对新数据的适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为FRET的方法，包括一个直接最小化特征冗余分数的简单方法（S-FRET），以及一个结合图卷积网络（GCN）和对比学习的改进方法（G-FRET）。&lt;h4&gt;主要发现&lt;/h4&gt;S-FRET在处理标签偏移时存在局限性，而G-FRET通过减少特征冗余并增强特征可区分性，在多个模型架构、任务和数据集上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;G-FRET能够帮助模型在推理过程中提取非冗余且高度可区分的特征，从而促进更鲁棒的测试时间自适应。&lt;h4&gt;翻译&lt;/h4&gt;Test-Time Adaptation (TTA) aims to enhance the generalization of deep learning models when faced with test data that exhibits distribution shifts from the training data. In this context, only a pre-trained model and unlabeled test data are available, making it particularly relevant for privacy-sensitive applications. In practice, we observe that feature redundancy in embeddings tends to increase as domain shifts intensify in TTA. However, existing TTA methods often overlook this redundancy, which can hinder the model's adaptability to new data. To address this issue, we introduce Feature Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for TTA. A straightforward approach (S-FRET) is to directly minimize the feature redundancy score as an optimization objective to improve adaptation. Despite its simplicity and effectiveness, S-FRET struggles with label shifts, limiting its robustness in real-world scenarios. To mitigate this limitation, we further propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional Network (GCN) with contrastive learning. This design not only reduces feature redundancy but also enhances feature discriminability in both the representation and prediction layers. Extensive experiments across multiple model architectures, tasks, and datasets demonstrate the effectiveness of S-FRET and show that G-FRET achieves state-of-the-art performance. Further analysis reveals that G-FRET enables the model to extract non-redundant and highly discriminative features during inference, thereby facilitating more robust test-time adaptation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-Time Adaptation (TTA) aims to enhance the generalization of deeplearning models when faced with test data that exhibits distribution shiftsfrom the training data. In this context, only a pre-trained model and unlabeledtest data are available, making it particularly relevant for privacy-sensitiveapplications. In practice, we observe that feature redundancy in embeddingstends to increase as domain shifts intensify in TTA. However, existing TTAmethods often overlook this redundancy, which can hinder the model'sadaptability to new data. To address this issue, we introduce FeatureRedundancy Elimination for Test-time Adaptation (FRET), a novel perspective forTTA. A straightforward approach (S-FRET) is to directly minimize the featureredundancy score as an optimization objective to improve adaptation. Despiteits simplicity and effectiveness, S-FRET struggles with label shifts, limitingits robustness in real-world scenarios. To mitigate this limitation, we furtherpropose Graph-based FRET (G-FRET), which integrates a Graph ConvolutionalNetwork (GCN) with contrastive learning. This design not only reduces featureredundancy but also enhances feature discriminability in both therepresentation and prediction layers. Extensive experiments across multiplemodel architectures, tasks, and datasets demonstrate the effectiveness ofS-FRET and show that G-FRET achieves state-of-the-art performance. Furtheranalysis reveals that G-FRET enables the model to extract non-redundant andhighly discriminative features during inference, thereby facilitating morerobust test-time adaptation.</description>
      <author>example@mail.com (Linjing You, Jiabao Lu, Xiayuan Huang, Xiangli Nie)</author>
      <guid isPermaLink="false">2505.10641v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unfolded Deep Graph Learning for Networked Over-the-Air Computation</title>
      <link>http://arxiv.org/abs/2505.11248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted @ IEEE TWC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多集群网络环境下的无线信道空中计算（AirComp），旨在提高多集群加权求和的AirComp速率，同时解决接收端波束成形、发射缩放和干扰管理等问题。&lt;h4&gt;背景&lt;/h4&gt;空中计算技术允许通过无线信道同时进行传输和计算，但多集群网络环境下的AirComp受到接收端波束成形、发射缩放和干扰管理等方面的挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在最大化多集群加权求和AirComp速率，并解决传输缩放和接收波束成形中的干扰问题。&lt;h4&gt;方法&lt;/h4&gt;通过分解问题，采用交替优化技术和迭代过程近似求解。利用算法展开原理，通过信道条件和网络中的相互干扰构建一个基础图，然后利用图神经网络学习权重参数，并通过随机梯度下降法进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，提出的方案优于传统方案，展开的图学习架构有效地减轻了干扰，并实现了优越的计算性能，对动态和可扩展网络具有较强的适应性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高多集群AirComp网络的计算性能，并对动态和可扩展网络具有强的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-the-air computation (AirComp) has emerged as a promising technology thatenables simultaneous transmission and computation through wireless channels. Inthis paper, we investigate the networked AirComp in multiple clusters allowingdiversified data computation, which is yet challenged by the transceivercoordination and interference management therein. Particularly, we aim tomaximize the multi-cluster weighted-sum AirComp rate, where the transmissionscalar as well as receive beamforming are jointly investigated while addressingthe interference issue. From an optimization perspective, we decompose theformulated problem and adopt the alternating optimization technique with aniterative process to approximate the solution. Then, we reinterpret theiterations through the principle of algorithm unfolding, where the channelcondition and mutual interference in the AirComp network constitute anunderlying graph. Accordingly, the proposed unfolding architecture learns theweights parameterized by graph neural networks, which is trained throughstochastic gradient descent approach. Simulation results show that ourproposals outperform the conventional schemes, and the proposed unfolded graphlearning substantially alleviates the interference and achieves superiorcomputation performance, with strong and efficient adaptation to the dynamicand scalable networks.</description>
      <author>example@mail.com (Xiao Tang, Huirong Xiao, Chao Shen, Li Sun, Qinghe Du, Dusit Niyato, Zhu Han)</author>
      <guid isPermaLink="false">2505.11248v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment</title>
      <link>http://arxiv.org/abs/2505.11230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的交通分配问题解决方案，使用消息传递神经网络作为元模型来近似随机用户均衡分配的均衡流量。&lt;h4&gt;背景&lt;/h4&gt;交通分配问题是交通建模中的基本任务，但在大规模网络中计算成本高昂，传统方法需要迭代模拟以达到均衡，这使得实时或大规模场景分析变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来加速非分布场景的评估，降低大规模交通规划的计算成本，并实现实时决策。&lt;h4&gt;方法&lt;/h4&gt;使用消息传递神经网络作为元模型来模拟传统交通模拟器中的算法结构，以更好地捕捉潜在过程，而不是仅仅数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与其他传统深度学习技术进行了基准测试，并通过在训练数据域之外的输入数据上测试其预测交通流的能力来评估模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法为加速非分布场景的评估、减少大规模交通规划的计算成本和实现实时决策提供了有希望的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：交通分配问题是交通建模中的基础任务，但在大规模网络中计算成本很高，特别是对于大规模网络。传统方法需要迭代模拟以达到均衡，这使得实时或大规模场景分析变得具有挑战性。在本文中，我们提出了一种基于学习的解决方案，使用消息传递神经网络作为元模型来近似随机用户均衡分配的均衡流量。我们的模型旨在模拟传统交通模拟器中使用的算法结构，以便更好地捕捉潜在过程，而不仅仅是数据。我们将其与其他传统深度学习技术进行了基准测试，并通过在训练数据域之外的输入数据上测试其预测交通流的能力来评估模型的鲁棒性。这种方法为加速非分布场景的评估、减少大规模交通规划的计算成本和实现实时决策提供了有希望的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Traffic Assignment Problem is a fundamental, yet computationallyexpensive, task in transportation modeling, especially for large-scalenetworks. Traditional methods require iterative simulations to reachequilibrium, making real-time or large-scale scenario analysis challenging. Inthis paper, we propose a learning-based approach using Message-Passing NeuralNetworks as a metamodel to approximate the equilibrium flow of the StochasticUser Equilibrium assignment. Our model is designed to mimic the algorithmicstructure used in conventional traffic simulators allowing it to better capturethe underlying process rather than just the data. We benchmark it against otherconventional deep learning techniques and evaluate the model's robustness bytesting its ability to predict traffic flows on input data outside the domainon which it was trained. This approach offers a promising solution foraccelerating out-of-distribution scenario assessments, reducing computationalcosts in large-scale transportation planning, and enabling real-timedecision-making.</description>
      <author>example@mail.com (Oskar Bohn Lassen, Serio Agriesti, Mohamed Eldafrawi, Daniele Gammelli, Guido Cantelmo, Guido Gentile, Francisco Camara Pereira)</author>
      <guid isPermaLink="false">2505.11230v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
      <link>http://arxiv.org/abs/2505.11325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PFNs作为一种从表格数据集进行预测的强大基础模型，在小到中等规模的数据集上实现了最先进的性能，无需调整。本文提出了一种基于Martingale后验的贝叶斯后验构建方法，用于估计预测结果的不确定性，并通过模拟和真实世界数据展示了该方法在推理应用中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;PFNs（Prior-data fitted networks）作为一种从表格数据集进行预测的模型，在无需调整的情况下，在小到中等规模的数据集上取得了最先进的性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Martingale后验的贝叶斯后验构建方法，用于估计预测结果的不确定性。&lt;h4&gt;方法&lt;/h4&gt;采用Martingale后验来构建贝叶斯后验，并证明其收敛性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟和真实世界数据中展示了不确定性量化在推理应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为PFNs提供了不确定性量化，有助于提高预测的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prior-data fitted networks (PFNs) have emerged as promising foundation modelsfor prediction from tabular data sets, achieving state-of-the-art performanceon small to moderate data sizes without tuning. While PFNs are motivated byBayesian ideas, they do not provide any uncertainty quantification forpredictive means, quantiles, or similar quantities. We propose a principled andefficient sampling procedure to construct Bayesian posteriors for suchestimates based on Martingale posteriors, and prove its convergence. Severalsimulated and real-world data examples showcase the uncertainty quantificationof our method in inference applications.</description>
      <author>example@mail.com (Thomas Nagler, David Rügamer)</author>
      <guid isPermaLink="false">2505.11325v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes</title>
      <link>http://arxiv.org/abs/2505.11270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态数据分析系统，旨在解决数据湖中数据多样性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;数据湖中的数据种类繁多，包括结构化、半结构化和非结构化数据，对数据分析提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提高多模态数据分析的准确性、效率和知识的新鲜度。&lt;h4&gt;方法&lt;/h4&gt;本文提出了基于模型上下文协议（MCP）的新型架构，允许大型语言模型（LLMs）与知识丰富的代理协同工作。&lt;h4&gt;主要发现&lt;/h4&gt;1. 现有的自然语言或SQL-like查询语言可能难以精确和全面地捕捉用户的分析意图；2. 依赖单个统一的LLM处理多样化的数据模式会导致大量的推理开销；3. 数据湖中的数据可能不完整或过时，需要整合外部开放域知识以生成及时和相关的分析结果。&lt;h4&gt;结论&lt;/h4&gt;该系统通过语义操作符层次结构和AI代理驱动的NL2Operator翻译器来连接用户意图和分析执行，同时通过MCP执行框架提高准确性和效率，并支持模块化部署以实现高可扩展性。此外，通过深度研究和机器再学习技术来更新数据湖和LLM知识，以平衡数据的新鲜度和推理效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：数据湖中数据的多样性给数据分析带来了重大挑战，数据科学家必须同时分析多模态数据，包括结构化、半结构化和非结构化数据。尽管大型语言模型（LLMs）已显示出有希望的潜力，但在准确性、效率和新鲜度方面，它们仍不足以处理多模态数据分析。首先，当前的自然语言（NL）或SQL-like查询语言可能难以精确和全面地捕捉用户的分析意图。其次，依赖于单个统一的LLM来处理多样化的数据模式通常会导致大量的推理开销。第三，存储在数据湖中的数据可能不完整或过时，因此集成外部开放域知识对于生成及时和相关的分析结果是必不可少的。在本文中，我们设想了一种新的多模态数据分析系统。具体而言，我们提出了一种基于模型上下文协议（MCP）的新型架构，该协议允许LLMs与知识丰富的代理协同工作。首先，我们定义了一个针对数据湖中多模态数据查询的语义操作符层次结构，并开发了一个由AI代理驱动的NL2Operator翻译器，以连接用户意图和分析执行。接下来，我们介绍了一个基于MCP的执行框架，其中每个MCP服务器都运行针对特定数据模式优化的专用基础模型。这种设计提高了准确性和效率，并通过模块化部署支持高可扩展性。最后，我们提出了一种更新机制，通过利用深度研究和机器再学习技术来刷新数据湖和LLM知识，目标是平衡数据的新鲜度和推理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The variety of data in data lakes presents significant challenges for dataanalytics, as data scientists must simultaneously analyze multi-modal data,including structured, semi-structured, and unstructured data. While LargeLanguage Models (LLMs) have demonstrated promising capabilities, they stillremain inadequate for multi-modal data analytics in terms of accuracy,efficiency, and freshness. First, current natural language (NL) or SQL-likequery languages may struggle to precisely and comprehensively capture users'analytical intent. Second, relying on a single unified LLM to process diversedata modalities often leads to substantial inference overhead. Third, datastored in data lakes may be incomplete or outdated, making it essential tointegrate external open-domain knowledge to generate timely and relevantanalytics results.  In this paper, we envision a new multi-modal data analytics system.Specifically, we propose a novel architecture built upon the Model ContextProtocol (MCP), an emerging paradigm that enables LLMs to collaborate withknowledgeable agents. First, we define a semantic operator hierarchy tailoredfor querying multi-modal data in data lakes and develop an AI-agent-poweredNL2Operator translator to bridge user intent and analytical execution. Next, weintroduce an MCP-based execution framework, in which each MCP server hostsspecialized foundation models optimized for specific data modalities. Thisdesign enhances both accuracy and efficiency, while supporting high scalabilitythrough modular deployment. Finally, we propose a updating mechanism byharnessing the deep research and machine unlearning techniques to refresh thedata lakes and LLM knowledges, with the goal of balancing the data freshnessand inference efficiency.</description>
      <author>example@mail.com (Chao Zhang, Shaolei Zhang, Quehuan Liu, Sibei Chen, Tong Li, Ju Fan)</author>
      <guid isPermaLink="false">2505.11270v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>IssueCourier: Multi-Relational Heterogeneous Temporal Graph Neural Network for Open-Source Issue Assignment</title>
      <link>http://arxiv.org/abs/2505.11205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IssueCourier的基于多关系异构时序图神经网络的自动化问题分配方法，用于开源软件维护中的问题分配。&lt;h4&gt;背景&lt;/h4&gt;在开源软件维护中，问题分配是一个关键环节，需要推荐最合适的开发者来处理报告的问题。由于大规模项目中问题报告数量庞大，手动分配问题既繁琐又昂贵。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有自动化问题分配方法在性能上的局限性，如数据集中标签错误和缺失、开发者贡献的长尾效应以及开发者活动随项目进展而变化的问题。&lt;h4&gt;方法&lt;/h4&gt;提出IssueCourier方法，通过正式化问题、开发者与源代码文件之间的五个关键关系来构建异构图，并采用时间切片技术将图划分为基于时间的一系列子图以学习特定阶段的模式。此外，还提供了一个带有重新标注的基准数据集以解决现有开源数据集中标签错误和缺失的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上进行的广泛实验表明，IssueCourier在top-1和MRR方面分别比最佳基线提高了45.49%和31.97%。&lt;h4&gt;结论&lt;/h4&gt;IssueCourier能够有效提高问题分配的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在开源软件（OSS）维护中，问题分配起着至关重要的作用，这涉及到推荐最合适的开发者来处理报告的问题。鉴于大规模项目中问题报告的高数量，手动分配问题是繁琐且昂贵的。先前的研究已经提出了自动问题分配方法，这些方法主要基于对问题报告文本信息、开发者专业知识或基于历史问题修复记录的问题与开发者之间交互的建模。然而，这些方法往往由于开源数据集中存在错误和缺失的标签，以及开发者贡献的长尾效应和开发者活动随项目进展而变化的问题，而受到性能限制。为了解决这些挑战，我们提出了IssueCourier，这是一种用于问题分配的新颖的多关系异构时序图神经网络方法。具体来说，我们正式化了问题、开发者和源代码文件之间的五个关键关系，以构建一个异构图。然后，我们进一步采用时间切片技术，将图划分为一系列基于时间的基础子图以学习特定阶段的模式。此外，我们还提供了一个带有重新标注的基准数据集，以解决现有开源数据集中标签错误和缺失的问题。最后，为了评估IssueCourier的性能，我们在基准数据集上进行了广泛的实验。结果表明，IssueCourier可以在top-1和MRR方面分别比最佳基线提高45.49%和31.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Issue assignment plays a critical role in open-source software (OSS)maintenance, which involves recommending the most suitable developers toaddress the reported issues. Given the high volume of issue reports inlarge-scale projects, manually assigning issues is tedious and costly. Previousstudies have proposed automated issue assignment approaches that primarilyfocus on modeling issue report textual information, developers' expertise, orinteractions between issues and developers based on historical issue-fixingrecords. However, these approaches often suffer from performance limitationsdue to the presence of incorrect and missing labels in OSS datasets, as well asthe long tail of developer contributions and the changes of developer activityas the project evolves. To address these challenges, we propose IssueCourier, anovel Multi-Relational Heterogeneous Temporal Graph Neural Network approach forissue assignment. Specifically, we formalize five key relationships amongissues, developers, and source code files to construct a heterogeneous graph.Then, we further adopt a temporal slicing technique that partitions the graphinto a sequence of time-based subgraphs to learn stage-specific patterns.Furthermore, we provide a benchmark dataset with relabeled ground truth toaddress the problem of incorrect and missing labels in existing OSS datasets.Finally, to evaluate the performance of IssueCourier, we conduct extensiveexperiments on our benchmark dataset. The results show that IssueCourier canimprove over the best baseline up to 45.49% in top-1 and 31.97% in MRR.</description>
      <author>example@mail.com (Chunying Zhou, Xiaoyuan Xie, Gong Chen, Peng He, Bing Li)</author>
      <guid isPermaLink="false">2505.11205v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Covariance Density Neural Networks</title>
      <link>http://arxiv.org/abs/2505.11139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的CoVariance Neural Networks（VNNs）模型，通过使用密度矩阵作为图位移算子（GSO），提高了网络在处理网络数据时的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络在建模和预测网络数据方面取得了进展，但选择合适的底层图结构存在争议。&lt;h4&gt;目的&lt;/h4&gt;解决选择合适的底层图结构的问题，并提高VNNs的性能。&lt;h4&gt;方法&lt;/h4&gt;将样本协方差矩阵视为随机变量空间中的准哈密顿量，构建密度矩阵作为GSO，从而在不同的尺度上提取数据成分，增强判别能力和性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够显式控制网络的稳定性-判别性权衡，相比VNNs具有更强的鲁棒性，并在实际应用中优于VNNs。特别地，该模型在脑机接口（BCI）的脑电图（EEG）运动想象分类任务中表现出色，速度快于EEGnet。&lt;h4&gt;结论&lt;/h4&gt;协方差密度神经网络为BCI的迁移性提供了基础，当在未见过的个体上评估时，能够实现良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph神经网络重新定义了我们对网络数据的建模和预测方法，但在选择合适的底层图结构以建模信号方面，缺乏共识。协方差神经网络（VNN）通过使用样本协方差矩阵作为图位移算子（GSO）来解决这个问题。在这里，我们通过构建密度矩阵来提高VNNs的性能，在这个密度矩阵中，我们将样本协方差矩阵视为系统在随机变量空间中的准哈密顿量。关键的是，使用这个密度矩阵作为GSO允许在不同的尺度上提取数据成分，从而增强了判别能力和性能。我们表明，这种方法允许显式控制网络的稳定性-判别性权衡，与VNNs相比，提供了更强的鲁棒性，并在底层协方差矩阵具有信息性的有用实际应用中表现优于它们。特别是，我们表明我们的模型在主题无关的脑电图（EEG）脑机接口（BCI）运动想象分类中可以达到很好的性能，速度比EEGnet快。这表明协方差密度神经网络为在未见过的个体上评估时的BCI迁移性这一艰巨任务提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have re-defined how we model and predict on networkdata but there lacks a consensus on choosing the correct underlying graphstructure on which to model signals. CoVariance Neural Networks (VNN) addressthis issue by using the sample covariance matrix as a Graph Shift Operator(GSO). Here, we improve on the performance of VNNs by constructing a DensityMatrix where we consider the sample Covariance matrix as a quasi-Hamiltonian ofthe system in the space of random variables. Crucially, using this densitymatrix as the GSO allows components of the data to be extracted at differentscales, allowing enhanced discriminability and performance. We show that thisapproach allows explicit control of the stability-discriminability trade-off ofthe network, provides enhanced robustness to noise compared to VNNs, andoutperforms them in useful real-life applications where the underlyingcovariance matrix is informative. In particular, we show that our model canachieve strong performance in subject-independent Brain Computer Interface EEGmotor imagery classification, outperforming EEGnet while being faster. Thisshows how covariance density neural networks provide a basis for thenotoriously difficult task of transferability of BCIs when evaluated on unseenindividuals.</description>
      <author>example@mail.com (Om Roy, Yashar Moshfeghi, Keith Smith)</author>
      <guid isPermaLink="false">2505.11139v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation</title>
      <link>http://arxiv.org/abs/2505.11221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, ICASSP 2025. The first two authors are equally contributed&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LVLM2P的新框架，用于将大型视觉-语言模型（LVLM）的知识提炼到高效的强化学习（RL）代理中，以解决决策挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型在处理复杂决策挑战方面具有潜力，但它们的大参数使得实际部署资源密集，对于受限系统来说往往不切实际。强化学习虽然对特定任务代理有希望，但样本复杂度高，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出LVLM2P框架，旨在解决上述挑战，提高强化学习代理的效率。&lt;h4&gt;方法&lt;/h4&gt;LVLM2P利用LVLM作为教师，根据RL代理收集的轨迹提供指导性动作，帮助减少学习早期的不必要探索，从而显著加速代理的学习进度。此外，通过利用LVLM直接从视觉观察中建议动作，消除了对环境手动文本描述的需要，增强了跨不同任务的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LVLM2P显著提高了基线RL算法的样本效率。&lt;h4&gt;结论&lt;/h4&gt;LVLM2P框架能够有效提高强化学习代理的样本效率，为解决复杂决策挑战提供了一种新的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICASSP49660.2025.10888998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research highlights the potential of multimodal foundation models intackling complex decision-making challenges. However, their large parametersmake real-world deployment resource-intensive and often impractical forconstrained systems. Reinforcement learning (RL) shows promise fortask-specific agents but suffers from high sample complexity, limitingpractical applications. To address these challenges, we introduce LVLM toPolicy (LVLM2P), a novel framework that distills knowledge from largevision-language models (LVLM) into more efficient RL agents. Our approachleverages the LVLM as a teacher, providing instructional actions based ontrajectories collected by the RL agent, which helps reduce less meaningfulexploration in the early stages of learning, thereby significantly acceleratingthe agent's learning progress. Additionally, by leveraging the LVLM to suggestactions directly from visual observations, we eliminate the need for manualtextual descriptors of the environment, enhancing applicability across diversetasks. Experiments show that LVLM2P significantly enhances the sampleefficiency of baseline RL algorithms.</description>
      <author>example@mail.com (Donghoon Lee, Tung M. Luu, Younghwan Lee, Chang D. Yoo)</author>
      <guid isPermaLink="false">2505.11221v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Halting Recurrent GNNs and the Graded $μ$-Calculus</title>
      <link>http://arxiv.org/abs/2505.11050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于循环图神经网络（GNNs）的停止机制，并证明了该机制能够表达所有在分级模态μ-演算中定义的节点分类器，即使对于不考虑图大小的标准GNN变种也是如此。&lt;h4&gt;背景&lt;/h4&gt;现有的循环GNNs要么假设图大小已知，要么缺乏终止保证。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够确保循环GNNs正确终止的机制。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新的近似语义，用于分级μ-演算，并利用这种语义设计了一种新的模型检查算法，称为计数算法。&lt;h4&gt;主要发现&lt;/h4&gt;计数算法不依赖于图大小，并且能够被实现在一个停止的循环GNN上。同时，证明了循环GNNs在有限范围内只能表达在分级模态μ-演算中定义的节点分类器。&lt;h4&gt;结论&lt;/h4&gt;提出的停止机制和模型检查算法为循环GNNs在处理图结构数据时提供了有效的工具，并证明了其在表达能力和终止性方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are a class of machine-learning models thatoperate on graph-structured data. Their expressive power is intimately relatedto logics that are invariant under graded bisimilarity. Current proposals forrecurrent GNNs either assume that the graph size is given to the model, orsuffer from a lack of termination guarantees. In this paper, we propose ahalting mechanism for recurrent GNNs. We prove that our halting model canexpress all node classifiers definable in graded modal mu-calculus, even forthe standard GNN variant that is oblivious to the graph size. A recentbreakthrough in the study of the expressivity of graded modal mu-calculus inthe finite suggests that conversely, restricted to node classifiers definablein monadic second-order logic, recurrent GNNs can express only node classifiersdefinable in graded modal mu-calculus. To prove our main result, we develop anew approximate semantics for graded mu-calculus, which we believe to be ofindependent interest. We leverage this new semantics into a new model-checkingalgorithm, called the counting algorithm, which is oblivious to the graph size.In a final step we show that the counting algorithm can be implemented on ahalting recurrent GNN.</description>
      <author>example@mail.com (Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema)</author>
      <guid isPermaLink="false">2505.11050v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs</title>
      <link>http://arxiv.org/abs/2505.11023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究背景知识图在生物医学研究等复杂低数据领域中的重要作用，特别是在癌症亚型分类任务中的应用。&lt;h4&gt;背景&lt;/h4&gt;在生物医学研究中，将背景知识图（如蛋白质-蛋白质相互作用网络）纳入图学习模型可以提高模型性能，但背景知识的实际贡献和对不完美知识的影响尚不明确。&lt;h4&gt;目的&lt;/h4&gt;研究背景知识在癌症亚型分类任务中的作用，并评估其性能贡献。&lt;h4&gt;方法&lt;/h4&gt;通过构建一个评估框架，包括合成设置和一系列模拟背景知识图不完美性的扰动，来测试背景知识感知模型在合成和真实生物医学环境中的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;发现最先进的图神经网络（GNN）使用背景知识并不比无信息的模型（如线性回归）表现更好，且即使背景知识图受到严重扰动，其性能也基本不变。&lt;h4&gt;结论&lt;/h4&gt;强调在GNN架构和背景知识特征之间进行仔细对齐的必要性，并指出这有可能带来显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In complex and low-data domains such as biomedical research, incorporatingbackground knowledge (BK) graphs, such as protein-protein interaction (PPI)networks, into graph-based machine learning pipelines is a promising researchdirection. However, while BK is often assumed to improve model performance, itsactual contribution and the impact of imperfect knowledge remain poorlyunderstood. In this work, we investigate the role of BK in an importantreal-world task: cancer subtype classification. Surprisingly, we find that (i)state-of-the-art GNNs using BK perform no better than uninformed models likelinear regression, and (ii) their performance remains largely unchanged evenwhen the BK graph is heavily perturbed. To understand these unexpected results,we introduce an evaluation framework, which employs (i) a synthetic settingwhere the BK is clearly informative and (ii) a set of perturbations thatsimulate various imperfections in BK graphs. With this, we test the robustnessof BK-aware models in both synthetic and real-world biomedical settings. Ourfindings reveal that careful alignment of GNN architectures and BKcharacteristics is necessary but holds the potential for significantperformance improvements.</description>
      <author>example@mail.com (Kutalmış Coşkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan Lüdtke, Martin Becker)</author>
      <guid isPermaLink="false">2505.11023v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Relational Graph Transformer</title>
      <link>http://arxiv.org/abs/2505.10960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/snap-stanford/relgt&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Relational Graph Transformer (RelGT)的新架构，用于构建基于多表关系数据的预测模型，并展示了其在21个RelBench基准任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;关系深度学习（RDL）是一种在多表关系数据上构建最先进的预测模型的方法，它通过将数据表示为异构时间图来实现。然而，常用的图神经网络模型在捕捉关系数据中的复杂结构模式和长距离依赖关系方面存在根本性限制。&lt;h4&gt;目的&lt;/h4&gt;设计一种专门针对关系表的新型图变换器架构，以解决现有图神经网络模型在处理关系数据时的局限性。&lt;h4&gt;方法&lt;/h4&gt;RelGT采用了一种新颖的多元素标记化策略，将每个节点分解为五个组件（特征、类型、跳转距离、时间和局部结构），从而高效地编码异构性、时态和拓扑结构，而不需要昂贵的预计算。该架构结合了局部注意力机制和全局注意力机制，以学习可学习的中心点，同时结合局部和数据库范围内的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在21个RelBench基准任务中，RelGT在性能上与图神经网络基线相当或优于基线，最高提升达18%，证明了图变换器在关系深度学习中的强大架构能力。&lt;h4&gt;结论&lt;/h4&gt;RelGT是一种有效的解决关系数据复杂性和时态约束的方法，为关系深度学习提供了强大的架构支持。&lt;h4&gt;翻译&lt;/h4&gt;Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational Deep Learning (RDL) is a promising approach for buildingstate-of-the-art predictive models on multi-table relational data byrepresenting it as a heterogeneous temporal graph. However, commonly used GraphNeural Network models suffer from fundamental limitations in capturing complexstructural patterns and long-range dependencies that are inherent in relationaldata. While Graph Transformers have emerged as powerful alternatives to GNNs ongeneral graphs, applying them to relational entity graphs presents uniquechallenges: (i) Traditional positional encodings fail to generalize to massive,heterogeneous graphs; (ii) existing architectures cannot model the temporaldynamics and schema constraints of relational data; (iii) existing tokenizationschemes lose critical structural information. Here we introduce the RelationalGraph Transformer (RelGT), the first graph transformer architecture designedspecifically for relational tables. RelGT employs a novel multi-elementtokenization strategy that decomposes each node into five components (features,type, hop distance, time, and local structure), enabling efficient encoding ofheterogeneity, temporality, and topology without expensive precomputation. Ourarchitecture combines local attention over sampled subgraphs with globalattention to learnable centroids, incorporating both local and database-widerepresentations. Across 21 tasks from the RelBench benchmark, RelGTconsistently matches or outperforms GNN baselines by up to 18%, establishingGraph Transformers as a powerful architecture for Relational Deep Learning.</description>
      <author>example@mail.com (Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico López, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec)</author>
      <guid isPermaLink="false">2505.10960v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model Enhancers for Graph Neural Networks: An Analysis from the Perspective of Causal Mechanism Identification</title>
      <link>http://arxiv.org/abs/2505.08265v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLMs）作为特征增强器优化节点表示，并将这些表示作为图神经网络（GNNs）输入的方法，在图表示学习中具有显著潜力，并对这一方法进行了深入研究。&lt;h4&gt;背景&lt;/h4&gt;LLMs作为特征增强器在图表示学习中的应用显示出潜力，但其基本属性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;通过更深入的分析和基于互换干预方法的实验，研究LLMs增强器和GNNs的深层属性和内在机制。&lt;h4&gt;方法&lt;/h4&gt;构建了一个具有可控因果关系的合成图数据集，用于分析，并使用互换干预方法来检查LLMs增强器和GNNs的深层属性。基于分析结果，设计了一个即插即用模块来提高LLMs增强器和GNNs之间的信息传递。&lt;h4&gt;主要发现&lt;/h4&gt;通过互换干预实验，揭示了LLMs增强器和GNNs的内在逻辑和内部机制。&lt;h4&gt;结论&lt;/h4&gt;提出的模块在多个数据集和模型上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：使用大型语言模型（LLMs）作为特征增强器以优化节点表示，这些表示随后被用作图神经网络（GNNs）的输入，在图表示学习中显示出显著潜力。然而，这种方法的基本属性尚未得到充分探索。为了解决这个问题，我们提出基于互换干预方法对此问题进行更深入的分析。首先，我们构建了一个具有可控因果关系的合成图数据集，以便精确地操作语义关系和因果建模，为分析提供数据。使用这个数据集，我们进行互换干预来检查LLMs增强器和GNNs的深层属性，揭示它们的内在逻辑和内部机制。基于分析结果，我们设计了一个即插即用模块来提高LLMs增强器和GNNs之间的信息传递。在多个数据集和模型上的实验验证了所提出的模块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) as feature enhancers to optimize noderepresentations, which are then used as inputs for graph neural networks(GNNs), has shown significant potential in graph representation learning.However, the fundamental properties of this approach remain underexplored. Toaddress this issue, we propose conducting a more in-depth analysis of thisissue based on the interchange intervention method. First, we construct asynthetic graph dataset with controllable causal relationships, enablingprecise manipulation of semantic relationships and causal modeling to providedata for analysis. Using this dataset, we conduct interchange interventions toexamine the deeper properties of LLM enhancers and GNNs, uncovering theirunderlying logic and internal mechanisms. Building on the analytical results,we design a plug-and-play optimization module to improve the informationtransfer between LLM enhancers and GNNs. Experiments across multiple datasetsand models validate the proposed module.</description>
      <author>example@mail.com (Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.08265v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</title>
      <link>http://arxiv.org/abs/2505.11214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLA模型在机器人领域日益受到关注，通过视觉语言基础模型直接从视觉观察和人类指令生成机器人动作。本文提出OE-VLA模型，以探索VLA模型在开放式多模态指令中的潜力。&lt;h4&gt;背景&lt;/h4&gt;VLA模型基于大规模互联网数据训练的视觉语言基础模型，能有效生成机器人动作，但通常只接受语言指令，限制了其在开放式人机交互中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出OE-VLA模型，旨在解决VLA模型在处理开放式多模态指令时的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入OE-VLA模型，该模型能够处理图像、白板上的指令和视频中的行为等不同形式的指令。&lt;h4&gt;主要发现&lt;/h4&gt;OE-VLA模型在语言输入的VLA模型性能可比的基础上，在四个额外的开放式任务类别中取得了令人印象深刻的结果。&lt;h4&gt;结论&lt;/h4&gt;OE-VLA模型能够显著扩展VLA模型在日常场景中的应用，并促进人机交互。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have recently become highly prominent inthe field of robotics. Leveraging vision-language foundation models trained onlarge-scale internet data, the VLA model can generate robotic actions directlyfrom visual observations and human instructions through a single end-to-endneural network. Despite their effectiveness, current VLA models usually acceptonly one form of human prompting, language instructions, which may constraintheir applicability in open-ended human-robot interactions. For example, a usermight expect the robot to retrieve an object shown in an image, follow aninstruction written on the whiteboard, or imitate a behavior demonstrated in avideo, rather than relying solely on language-based descriptions. To addressthis gap, we introduce OE-VLA, which explores the potential of VLA models foropen-ended multimodal instructions. Extensive results demonstrate that ourOE-VLA not only achieves comparable performance to traditional VLA models withlinguistic input but also delivers impressive results across four additionalcategories of open-ended tasks. The proposed methodology could significantlyexpand the applications of VLA models across various everyday scenarios andfacilitate human-robot interaction.</description>
      <author>example@mail.com (Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang)</author>
      <guid isPermaLink="false">2505.11214v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations</title>
      <link>http://arxiv.org/abs/2505.10877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高斯过程（GPs）的图结构数据预测方法，通过扩展GPs框架到单纯复形（SCs），处理边缘属性和更高阶单纯复形上的属性，并通过Hodge分解增强SC表示，从而提升预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在图结构数据预测中应用广泛，但在数据稀缺时容易过拟合，导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强GNNs在数据稀缺情况下的预测能力。&lt;h4&gt;方法&lt;/h4&gt;将Gaussian process框架扩展到单纯复形，处理边缘属性和更高阶单纯复形上的属性，并利用Hodge分解来增强SC表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在各种应用中提升了预测性能，为GPs在图和SC级别预测中的应用铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;Gaussian processes结合单纯复形和Hodge分解可以有效地提升图结构数据预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在科学应用中，预测图结构数据的标签至关重要，通常使用图神经网络（GNNs）来实现。然而，当数据稀缺时，GNNs容易过拟合，导致性能不佳。最近，提出了具有图级输入的高斯过程（GPs）作为替代方案。在这项工作中，我们将高斯过程框架扩展到单纯复形（SCs），使其能够处理边缘属性和更高阶单纯复形上的属性。我们进一步通过考虑其Hodge分解来增强结果SC表示，从而能够解释SC中的同调信息，如孔洞的数量。我们证明了我们的框架在各种应用中提高了预测能力，为GPs在图和SC级别预测的更广泛应用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the labels of graph-structured data is crucial in scientificapplications and is often achieved using graph neural networks (GNNs). However,when data is scarce, GNNs suffer from overfitting, leading to poor performance.Recently, Gaussian processes (GPs) with graph-level inputs have been proposedas an alternative. In this work, we extend the Gaussian process framework tosimplicial complexes (SCs), enabling the handling of edge-level attributes andattributes supported on higher-order simplices. We further augment theresulting SC representations by considering their Hodge decompositions,allowing us to account for homological information, such as the number ofholes, in the SC. We demonstrate that our framework enhances the predictionsacross various applications, paving the way for GPs to be more widely used forgraph and SC-level predictions.</description>
      <author>example@mail.com (Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi)</author>
      <guid isPermaLink="false">2505.10877v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.08199v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对长期时间序列预测（LTSF）中的关键问题，提出了一种基于MLP的预测框架，通过多尺度预测和动态信息整合，提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测在能源消耗和天气预报等实际应用中具有广泛用途，但由于时间序列中的复杂时间模式和内在的多尺度变化，准确预测长期变化是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;解决LTSF中的关键问题，包括多粒度信息的次优使用、忽略特定通道属性以及趋势和季节成分的独特性质。&lt;h4&gt;方法&lt;/h4&gt;引入了基于MLP的预测框架，该框架通过不同尺度的清晰、并发预测来巧妙地解开复杂的时间动态，并通过一个动态分配不同粒度信息重要性的系统来整合这些多尺度预测。&lt;h4&gt;主要发现&lt;/h4&gt;在八个LTSF基准数据集上的实验结果表明，与最近的MLP方法（TimeMixer）相比，MDMixer将平均MAE性能提高了4.64%，同时实现了训练效率和模型可解释性之间的有效平衡。&lt;h4&gt;结论&lt;/h4&gt;MDMixer方法在长期时间序列预测中取得了显著的性能提升，为该领域提供了一种有效且具有可解释性的预测框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) offers broad utility in practicalsettings like energy consumption and weather prediction. Accurately predictinglong-term changes, however, is demanding due to the intricate temporal patternsand inherent multi-scale variations within time series. This work confronts keyissues in LTSF, including the suboptimal use of multi-granularity information,the neglect of channel-specific attributes, and the unique nature of trend andseasonal components, by introducing a proficient MLP-based forecastingframework. Our method adeptly disentangles complex temporal dynamics usingclear, concurrent predictions across various scales. These multi-scaleforecasts are then skillfully integrated through a system that dynamicallyassigns importance to information from different granularities, sensitive toindividual channel characteristics. To manage the specific features of temporalpatterns, a two-pronged structure is utilized to model trend and seasonalelements independently. Experimental results on eight LTSF benchmarksdemonstrate that MDMixer improves average MAE performance by 4.64% compared tothe recent state-of-the-art MLP-based method (TimeMixer), while achieving aneffective balance between training efficiency and model interpretability.</description>
      <author>example@mail.com (Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao)</author>
      <guid isPermaLink="false">2505.08199v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</title>
      <link>http://arxiv.org/abs/2505.10579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Medical Image Computing  and Computer-Assisted Intervention (MICCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于基础模型（FM）的乳腺摄影分类的公平性和偏差问题，发现虽然FM在数据多样性的情况下表现出良好的泛化能力和迁移学习能力，但其性能可能因图像质量、标签不确定性和敏感患者属性等因素而受到影响。&lt;h4&gt;背景&lt;/h4&gt;过去几十年中，计算机辅助诊断工具被开发出来以增强乳腺癌的筛查程序，但由于数据多样性和固有的偏差，这些工具在临床应用中面临着挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集，来探索FM在乳腺摄影分类中的公平性和偏差。&lt;h4&gt;方法&lt;/h4&gt;通过广泛的实验，研究了不同数据集预训练FM的性能，以及不同域之间的一般化能力，并分析了数据聚合、域适应策略和公平性感知技术对性能和偏差的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 模态特定的预训练提高了FM的性能，但基于单个数据集训练的分类器无法在不同域之间进行泛化。2. 数据聚合提高了整体性能，但并未完全消除偏差，导致在乳腺密度和年龄等代表性不足的子群体中存在显著差异。3. 域适应策略可以减少这些差异，但通常伴随着性能上的权衡。4. 公平性感知技术能够在不同子群体之间提供更稳定和公平的性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了在基于FM的模型中纳入严格公平性评估和缓解策略的必要性，以促进包容性和可泛化的AI发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在过去的几十年中，为了增强乳腺癌的筛查程序，已经开发了计算机辅助诊断工具，但它们的临床应用仍然受到数据多样性和固有偏差的挑战。尽管基础模型（FMs）最近通过利用大量和多样化的数据集，展示了令人印象深刻的泛化能力和迁移学习能力，但它们的表现可能会因图像质量、标签不确定性和敏感患者属性的变化而产生的虚假相关性而受到影响。在这项工作中，我们通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集，探讨了FMs在乳腺摄影分类中的公平性和偏差。我们的广泛实验表明，虽然模态特定的预训练提高了FM的性能，但基于单个数据集特征的分类器无法在不同域之间泛化。数据聚合提高了整体性能，但并未完全消除偏差，导致在代表性不足的子群体（如极端乳腺密度和年龄群体）中存在显著差异。此外，虽然域适应策略可以减少这些差异，但它们通常会导致性能上的权衡。相比之下，公平性感知技术能够在不同子群体之间提供更稳定和公平的性能。这些发现强调了将严格的公平性评估和缓解策略纳入基于FM的模型的必要性，以促进包容性和可泛化的AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decades, computer-aided diagnosis tools for breast cancer havebeen developed to enhance screening procedures, yet their clinical adoptionremains challenged by data variability and inherent biases. Although foundationmodels (FMs) have recently demonstrated impressive generalizability andtransfer learning capabilities by leveraging vast and diverse datasets, theirperformance can be undermined by spurious correlations that arise fromvariations in image quality, labeling uncertainty, and sensitive patientattributes. In this work, we explore the fairness and bias of FMs for breastmammography classification by leveraging a large pool of datasets from diversesources-including data from underrepresented regions and an in-house dataset.Our extensive experiments show that while modality-specific pre-training of FMsenhances performance, classifiers trained on features from individual datasetsfail to generalize across domains. Aggregating datasets improves overallperformance, yet does not fully mitigate biases, leading to significantdisparities across under-represented subgroups such as extreme breast densitiesand age groups. Furthermore, while domain-adaptation strategies can reducethese disparities, they often incur a performance trade-off. In contrast,fairness-aware techniques yield more stable and equitable performance acrosssubgroups. These findings underscore the necessity of incorporating rigorousfairness evaluations and mitigation strategies into FM-based models to fosterinclusive and generalizable AI.</description>
      <author>example@mail.com (Germani Elodie, Selin Türk Ilayda, Zeineddine Fatima, Mourad Charbel, Albarqouni Shadi)</author>
      <guid isPermaLink="false">2505.10579v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</title>
      <link>http://arxiv.org/abs/2505.11191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着具身人工智能系统的多模态、个性化以及交互性的增强，需要从多样化的感官输入中有效学习，不断适应用户偏好，并在资源和隐私限制下安全运行。本文提出了一种新的范式——联邦基础模型（FFM），它结合了多模态多任务（M3T）基础模型和联邦学习（FL）的隐私保护分布式特性，以实现无线边缘的智能系统。&lt;h4&gt;背景&lt;/h4&gt;具身人工智能系统需要从多种感官输入中学习，适应用户偏好，并符合资源隐私限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型——联邦基础模型（FFM），以解决具身人工智能系统在多样化和复杂环境下的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入联邦基础模型（FFM），结合多模态多任务（M3T）基础模型和联邦学习（FL）的优势。&lt;h4&gt;主要发现&lt;/h4&gt;FFM能够统一多模态多任务基础模型和联邦学习的优势，为具身人工智能系统提供一种新的解决方案。&lt;h4&gt;结论&lt;/h4&gt;FFM在具身人工智能生态系统中具有重要作用，能够解决当前系统的复杂性和多样化需求。&lt;h4&gt;翻译&lt;/h4&gt;随着具身人工智能系统的多模态、个性化以及交互性的不断增强，它们必须从多样化的感官输入中有效地学习，持续适应用户偏好，并在资源和隐私约束下安全运行。这些挑战暴露了对于能够快速、情境感知地适应，同时平衡模型泛化和个性化的机器学习模型的迫切需求。在这里，两种方法作为合适的候选方案出现，每种方法都提供了这些能力的一部分：基础模型（FMs）提供了跨任务和模态泛化的途径，而联邦学习（FL）提供了分布式、隐私保护的模型更新和用户级模型个性化的基础设施。然而，当单独使用时，这些方法中的每一种都未能满足现实世界中具身环境的复杂和多样化的能力要求。在这篇愿景论文中，我们为具身人工智能引入了联邦基础模型（FFMs），这是一种新的范式，它统一了多模态多任务（M3T）FMs和FL的隐私保护分布式特性的优势，使得智能系统能够在无线边缘运行。我们在统一的框架下收集了FFMs在具身人工智能生态系统中的关键部署维度，我们将其命名为“EMBODY”：具身异质性、模态丰富性和不平衡性、带宽和计算限制、设备上的持续学习、分布式控制和自主性，以及产生安全性、隐私性和个性化。对于每一项，我们确定了具体挑战并展望了可行的研究方向。我们还提出了一种在具身人工智能系统中部署FFMs的评估框架，以及相关的权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As embodied AI systems become increasingly multi-modal, personalized, andinteractive, they must learn effectively from diverse sensory inputs, adaptcontinually to user preferences, and operate safely under resource and privacyconstraints. These challenges expose a pressing need for machine learningmodels capable of swift, context-aware adaptation while balancing modelgeneralization and personalization. Here, two methods emerge as suitablecandidates, each offering parts of these capabilities: Foundation Models (FMs)provide a pathway toward generalization across tasks and modalities, whereasFederated Learning (FL) offers the infrastructure for distributed,privacy-preserving model updates and user-level model personalization. However,when used in isolation, each of these approaches falls short of meeting thecomplex and diverse capability requirements of real-world embodiedenvironments. In this vision paper, we introduce Federated Foundation Models(FFMs) for embodied AI, a new paradigm that unifies the strengths ofmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed natureof FL, enabling intelligent systems at the wireless edge. We collect criticaldeployment dimensions of FFMs in embodied AI ecosystems under a unifiedframework, which we name "EMBODY": Embodiment heterogeneity, Modality richnessand imbalance, Bandwidth and compute constraints, On-device continual learning,Distributed control and autonomy, and Yielding safety, privacy, andpersonalization. For each, we identify concrete challenges and envisionactionable research directions. We also present an evaluation framework fordeploying FFMs in embodied AI systems, along with the associated trade-offs.</description>
      <author>example@mail.com (Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2505.11191v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>RapidGNN: Communication Efficient Large-Scale Distributed Training of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.10806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为RapidGNN的图神经网络训练方法，通过引入确定性采样策略来优化大规模图上的GNN训练，显著提高了通信效率和训练吞吐量。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在多个领域取得了最先进的性能，但在大规模图上的训练由于内存需求高和分布式环境中的通信开销大而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出RapidGNN以减少大规模图上GNN训练的内存需求和通信开销。&lt;h4&gt;方法&lt;/h4&gt;RapidGNN通过引入确定性采样策略来预计算小批量，并利用这种策略来准确预测特征访问模式，从而实现最优的缓存构建和远程特征的及时预取。&lt;h4&gt;主要发现&lt;/h4&gt;在Reddit和OGBN-Products数据集上的评估表明，RapidGNN在训练时间和远程特征获取方面实现了显著降低，在通信效率和吞吐量方面优于现有模型。RapidGNN将端到端训练吞吐量平均提高了2.10倍（在某些设置中最高可达2.45倍），同时将远程特征获取减少了超过4倍，并降低了高达23%的能量消耗。&lt;h4&gt;结论&lt;/h4&gt;RapidGNN展示了在大型真实世界图数据集上进行可扩展、高性能GNN训练的潜力，同时提高了能源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)performance in diverse domains. However, training GNNs on large-scale graphsposes significant challenges due to high memory demands and significantcommunication overhead in distributed settings. Traditional sampling-basedapproaches mitigate computation load to some extent but often fail to addresscommunication inefficiencies inherent in distributed environments. This paperpresents RapidGNN that introduces a deterministic sampling strategy toprecompute mini-batches. By leveraging the sampling strategy, RapidGNNaccurately anticipates feature access patterns, enabling optimal cacheconstruction and timely prefetching of remote features. This reduces thefrequency and latency of remote data transfers without compromising thestochastic nature of training. Evaluations on Reddit and OGBN-Products datasetsdemonstrate that RapidGNN achieves significant reductions in training time andremote feature fetches, outperforming existing models in both communicationefficiency and throughput. Our findings highlight RapidGNN's potential forscalable, high-performance GNN training across large, real-world graph datasetsalong with improving energy efficiency. Our model improves end-to-end trainingthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x insome settings), while cutting remote feature fetches by over 4x. It alsoreduces energy consumption up to 23%.</description>
      <author>example@mail.com (Arefin Niam, M S Q Zulkar Nine)</author>
      <guid isPermaLink="false">2505.10806v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Staying Fresh: Efficient Algorithms for Timely Social Information Distribution</title>
      <link>http://arxiv.org/abs/2308.13260v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work is an updated version of our previous paper titled  "Approximation Algorithms to Enhance Social Sharing of Fresh  Point-of-Interest Information."&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于位置的社会网络中，如何通过选择热点用户来提高用户的社会兴趣点信息共享。&lt;h4&gt;背景&lt;/h4&gt;在基于位置的社会网络中，用户在附近感知兴趣点信息并与朋友分享，但信息传播存在延迟。&lt;h4&gt;目的&lt;/h4&gt;研究一个组合优化问题，涉及城市感知网络和在线社交网络的交互。&lt;h4&gt;方法&lt;/h4&gt;证明了该问题为NP-hard，并通过分析两个网络的交互效应，将涉及的兴趣点共享过程转化为矩阵计算，以得到一个封闭形式的优化目标。此外，提出了一种多项式时间算法，保证了近似最优解。&lt;h4&gt;主要发现&lt;/h4&gt;该问题为NP-hard，现有近似解不可行；通过矩阵计算得到封闭形式的优化目标；提出了多项式时间算法，保证了近似最优解；提出了用户移动感知更多兴趣点的增强自适应算法。&lt;h4&gt;结论&lt;/h4&gt;理论结果通过合成和真实世界数据集的仿真结果得到验证。&lt;h4&gt;翻译&lt;/h4&gt;在基于位置的社会网络（LBSNs）中，用户在附近感知城市兴趣点（PoI）信息，并与在线社交网络中的朋友分享此类信息。鉴于用户的社交联系有限以及新鲜PoI传播的严重滞后，主要的LBSNs旨在通过选择m个用户中的k个作为热点，并将他们的新鲜PoI信息广播到整个用户社区来提高用户的社交PoI共享。这促使我们研究一个新的组合优化问题，该问题涉及城市感知网络和在线社交网络的交互。我们证明了这个问题是NP-hard的，并且使得现有的近似解决方案不可行。通过分析两个网络的交互效应，我们成功地实现了涉及两个网络的PoI共享过程，将其转化为矩阵计算，以推导出一个封闭形式的优化目标，以保持期望的性质（例如，次可加性和单调性）。这一发现使我们能够开发一个多项式时间算法，保证了近似最优解的（1-（m-2）/m（k-1）/k）k次方。此外，我们允许每个选定的用户移动并感知更多PoI信息以共享，并提出了一种具有良好性能保证的增强自适应算法。最后，我们的理论结果通过使用合成和真实世界数据集的仿真结果得到证实。&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In location-based social networks (LBSNs), users sense urbanpoint-of-interest (PoI) information in the vicinity and share such informationwith friends in online social networks. Given users' limited social connectionsand severe lags in disseminating fresh PoI to all, major LBSNs aim to enhanceusers' social PoI sharing by selecting $k$ out of $m$ users as hotspots andbroadcasting their fresh PoI information to the entire user community. Thismotivates us to study a new combinatorial optimization problem that involvesthe interplay between an urban sensing network and an online social network. Weprove that this problem is NP-hard and also renders existing approximationsolutions not viable. Through analyzing the interplay effects between the twonetworks, we successfully transform the involved PoI-sharing process across twonetworks to matrix computations for deriving a closed-form objective to holddesirable properties (e.g., submodularity and monotonicity). This findingenables us to develop a polynomial-time algorithm that guarantees a($1-\frac{m-2}{m}(\frac{k-1}{k})^k$) approximation of the optimum. Furthermore,we allow each selected user to move around and sense more PoI information toshare and propose an augmentation-adaptive algorithm with decent performanceguarantees. Finally, our theoretical results are corroborated by our simulationfindings using both synthetic and real-world datasets.</description>
      <author>example@mail.com (Songhua Li, Lingjie Duan)</author>
      <guid isPermaLink="false">2308.13260v3</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Time-Series AI Model for Realized Volatility Forecasting</title>
      <link>http://arxiv.org/abs/2505.11163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了时间序列基础模型（FMs）在波动率预测方面的有效性，特别是在金融风险管理中的核心任务。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型（FMs）已成为零样本多领域预测的一种流行范式，这些模型在多个不同的时间序列领域中被训练，包括金融数据。&lt;h4&gt;目的&lt;/h4&gt;评估TimesFM模型在波动率预测方面的有效性，并与标准计量经济学基准进行比较。&lt;h4&gt;方法&lt;/h4&gt;首先评估了预训练（零样本）的TimesFM模型，然后通过增量学习进行了自定义微调，并与标准计量经济学基准进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型提供了一个合理的基线，但研究表明，增量微调（使模型能够随着时间的推移适应新的金融回报数据）对于有效学习波动率模式是必不可少的。微调后的变体不仅提高了预测精度，而且在Diebold-Mariano和Giacomini-White测试中统计上优于传统模型。&lt;h4&gt;结论&lt;/h4&gt;这些结果突出了基础模型作为可扩展和自适应工具的潜力，它们在动态市场环境中，当与有针对性的微调策略相结合时，能够提供强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. These models are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including financial data. In this study, we evaluate the effectiveness of FMs, specifically the TimesFM model, for volatility forecasting, a core task in financial risk management. We first evaluate TimesFM in its pretrained (zero-shot) form, followed by our custom fine-tuning procedure based on incremental learning, and compare the resulting models against standard econometric benchmarks. While the pretrained model provides a reasonable baseline, our findings show that incremental fine-tuning, which allows the model to adapt to new financial return data over time, is essential for learning volatility patterns effectively. Fine-tuned variants not only improve forecast accuracy but also statistically outperform traditional models, as demonstrated through Diebold-Mariano and Giacomini-White tests. These results highlight the potential of foundation models as scalable and adaptive tools for financial forecasting-capable of delivering strong performance in dynamic market environments when paired with targeted fine-tuning strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (FMs) have emerged as a popular paradigm forzero-shot multi-domain forecasting. These models are trained on numerousdiverse datasets and claim to be effective forecasters across multipledifferent time series domains, including financial data. In this study, weevaluate the effectiveness of FMs, specifically the TimesFM model, forvolatility forecasting, a core task in financial risk management. We firstevaluate TimesFM in its pretrained (zero-shot) form, followed by our customfine-tuning procedure based on incremental learning, and compare the resultingmodels against standard econometric benchmarks. While the pretrained modelprovides a reasonable baseline, our findings show that incremental fine-tuning,which allows the model to adapt to new financial return data over time, isessential for learning volatility patterns effectively. Fine-tuned variants notonly improve forecast accuracy but also statistically outperform traditionalmodels, as demonstrated through Diebold-Mariano and Giacomini-White tests.These results highlight the potential of foundation models as scalable andadaptive tools for financial forecasting-capable of delivering strongperformance in dynamic market environments when paired with targetedfine-tuning strategies.</description>
      <author>example@mail.com (Anubha Goel, Puneet Pasricha, Martin Magris, Juho Kanniainen)</author>
      <guid isPermaLink="false">2505.11163v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics</title>
      <link>http://arxiv.org/abs/2505.10711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main article 8 pages (20 in total with supplementary information  included), 3 main article figures and 3 supplemental figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GNN-Suite，这是一个用于构建和评估图神经网络（GNN）架构的鲁棒模块化框架，在计算生物学中应用。GNN-Suite通过Nextflow工作流程标准化实验和可重复性，并展示了其在识别癌症驱动基因方面的效用。&lt;h4&gt;背景&lt;/h4&gt;GNN-Suite框架旨在解决计算生物学中GNN架构构建和评估的标准化问题。&lt;h4&gt;目的&lt;/h4&gt;目的是提供一种标准化的方法来构建、比较和评估GNN架构，以促进计算生物学中的可重复研究和基准测试标准的提升。&lt;h4&gt;方法&lt;/h4&gt;使用Nextflow工作流程进行实验标准化，构建分子网络，使用来自STRING和BioGRID的蛋白质-蛋白质相互作用（PPI）数据，并使用PCAWG、PID和COSMIC-CGC存储库中的特征进行节点注释。GNN架构包括GAT、GAT3H、GCN、GCN2、GIN、GTN、HGCN、PHGCN和GraphSAGE，以及基线逻辑回归（LR）模型。所有GNN均配置为标准化的两层模型，并使用统一的超参数进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;GCN2在基于STRING的网络中实现了最高的平衡准确率（BACC为0.807 +/- 0.035），所有GNN类型都优于LR基线，突出了基于网络学习在仅基于特征方法之上的优势。&lt;h4&gt;结论&lt;/h4&gt;GNN-Suite提供了一个共同框架，有助于识别最佳模型以及最有效的数据整合方式。通过使GNN-Suite公开可用，旨在促进可重复研究和提升计算生物学中的基准测试标准。&lt;h4&gt;翻译&lt;/h4&gt;We present GNN-Suite, a robust modular framework for constructing and benchmarking Graph Neural Network (GNN) architectures in computational biology. GNN-Suite standardises experimentation and reproducibility using the Nextflow workflow to evaluate GNN performance. We demonstrate its utility in identifying cancer-driver genes by constructing molecular networks from protein-protein interaction (PPI) data from STRING and BioGRID and annotating nodes with features from the PCAWG, PID, and COSMIC-CGC repositories. Our design enables fair comparisons among diverse GNN architectures including GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline Logistic Regression (LR) model. All GNNs were configured as standardised two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam optimizer with learning rate = 0.01; and an adjusted binary cross-entropy loss to address class imbalance) over an 80/20 train-test split for 300 epochs. Each model was evaluated over 10 independent runs with different random seeds to yield statistically robust performance metrics, with balanced accuracy (BACC) as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/- 0.035) on a STRING-based network, although all GNN types outperformed the LR baseline, highlighting the advantage of network-based learning over feature-only approaches. Our results show that a common framework for implementing and evaluating GNN architectures aids in identifying not only the best model but also the most effective means of incorporating complementary data. By making GNN-Suite publicly available, we aim to foster reproducible research and promote improved benchmarking standards in computational biology. Future work will explore additional omics datasets and further refine network architectures to enhance predictive accuracy and interpretability in biomedical applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GNN-Suite, a robust modular framework for constructing andbenchmarking Graph Neural Network (GNN) architectures in computational biology.GNN-Suite standardises experimentation and reproducibility using the Nextflowworkflow to evaluate GNN performance. We demonstrate its utility in identifyingcancer-driver genes by constructing molecular networks from protein-proteininteraction (PPI) data from STRING and BioGRID and annotating nodes withfeatures from the PCAWG, PID, and COSMIC-CGC repositories.  Our design enables fair comparisons among diverse GNN architectures includingGAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baselineLogistic Regression (LR) model. All GNNs were configured as standardisedtwo-layer models and trained with uniform hyperparameters (dropout = 0.2; Adamoptimiser with learning rate = 0.01; and an adjusted binary cross-entropy lossto address class imbalance) over an 80/20 train-test split for 300 epochs. Eachmodel was evaluated over 10 independent runs with different random seeds toyield statistically robust performance metrics, with balanced accuracy (BACC)as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-0.035) on a STRING-based network, although all GNN types outperformed the LRbaseline, highlighting the advantage of network-based learning overfeature-only approaches.  Our results show that a common framework for implementing and evaluating GNNarchitectures aids in identifying not only the best model but also the mosteffective means of incorporating complementary data. By making GNN-Suitepublicly available, we aim to foster reproducible research and promote improvedbenchmarking standards in computational biology. Future work will exploreadditional omics datasets and further refine network architectures to enhancepredictive accuracy and interpretability in biomedical applications.</description>
      <author>example@mail.com (Sebestyén Kamp, Giovanni Stracquadanio, T. Ian Simpson)</author>
      <guid isPermaLink="false">2505.10711v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video</title>
      <link>http://arxiv.org/abs/2505.11129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2405.14650&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhiNet v2是一个基于Transformer的新型自监督学习模型，它能够处理时间序列视觉输入，无需强数据增强，在计算机视觉领域取得了与现有先进模型相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在计算机视觉领域取得了显著进展，但尚未充分利用生物视觉处理系统的见解。&lt;h4&gt;目的&lt;/h4&gt;提出PhiNet v2模型，以更接近人类认知过程的方式处理视觉信息。&lt;h4&gt;方法&lt;/h4&gt;PhiNet v2基于ResNet骨干网络，使用Transformer架构，并利用变分推理从连续输入流中学习鲁棒的视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;PhiNet v2在无需强数据增强的情况下，与最先进的视觉基础模型相比，实现了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;PhiNet v2是朝着更生物可解释的计算机视觉系统迈出的重要一步，这些系统能够以更接近人类认知过程的方式处理视觉信息。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning (SSL) have revolutionizedcomputer vision through innovative architectures and learning objectives, yetthey have not fully leveraged insights from biological visual processingsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it isbased on a ResNet backbone and operates on static image inputs with strongaugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-basedarchitecture that processes temporal visual input (that is, sequences ofimages) without relying on strong augmentation. Our model leverages variationalinference to learn robust visual representations from continuous input streams,similar to human visual processing. Through extensive experimentation, wedemonstrate that PhiNet v2 achieves competitive performance compared tostate-of-the-art vision foundation models, while maintaining the ability tolearn from sequential input without strong data augmentation. This workrepresents a significant step toward more biologically plausible computervision systems that process visual information in a manner more closely alignedwith human cognitive processes.</description>
      <author>example@mail.com (Makoto Yamada, Kian Ming A. Chai, Ayoub Rhim, Satoki Ishikawa, Mohammad Sabokrou, Yao-Hung Hubert Tsai)</author>
      <guid isPermaLink="false">2505.11129v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>ExploreGS: a vision-based low overhead framework for 3D scene reconstruction</title>
      <link>http://arxiv.org/abs/2505.10578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为ExploreGS的低开销、基于视觉的3D场景重建框架，用于无人机。&lt;h4&gt;背景&lt;/h4&gt;传统的基于激光雷达的点云获取过程成本较高。&lt;h4&gt;目的&lt;/h4&gt;通过使用RGB图像，以较低的成本实现高质量的重建。&lt;h4&gt;方法&lt;/h4&gt;该框架集成了场景探索和模型重建，利用词袋（BoW）模型实现实时处理能力，并可以在设备上执行3D高斯分层（3DGS）训练。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实环境中的综合实验表明，ExploreGS框架在资源受限的设备上具有高效性和适用性，同时保持了与最先进方法相当的重建质量。&lt;h4&gt;结论&lt;/h4&gt;ExploreGS框架是一种有效且经济的无人机3D场景重建解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a low-overhead, vision-based 3D scene reconstructionframework for drones, named ExploreGS. By using RGB images, ExploreGS replacestraditional lidar-based point cloud acquisition process with a vision model,achieving a high-quality reconstruction at a lower cost. The frameworkintegrates scene exploration and model reconstruction, and leverags aBag-of-Words(BoW) model to enable real-time processing capabilities, therefore,the 3D Gaussian Splatting (3DGS) training can be executed on-board.Comprehensive experiments in both simulation and real-world environmentsdemonstrate the efficiency and applicability of the ExploreGS framework onresource-constrained devices, while maintaining reconstruction qualitycomparable to state-of-the-art methods.</description>
      <author>example@mail.com (Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu)</author>
      <guid isPermaLink="false">2505.10578v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding</title>
      <link>http://arxiv.org/abs/2505.01481v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoHallu，一个由合成视频组成的视频问答对基准，用于评估多模态大型语言模型在检测异常情况上的能力。&lt;h4&gt;背景&lt;/h4&gt;合成视频生成受到广泛关注，但容易违反常识和物理定律。这突显了需要可靠的不正常性检测器，这些检测器应理解这些原则并能够抵抗幻觉。&lt;h4&gt;目的&lt;/h4&gt;开发VideoHallu，以评估多模态大型语言模型（MLLMs）在检测人类感知明显但常因语言先验而幻觉的异常情况上的批判性思维能力。&lt;h4&gt;方法&lt;/h4&gt;VideoHallu由Veo2、Sora和Kling等模型生成的合成视频和专家制作的反直觉问答对组成。它评估MLLMs在一致性、常识和物理方面的异常检测能力。还使用了SOTA MLLMs，并通过GRPO和课程学习进行后训练。&lt;h4&gt;主要发现&lt;/h4&gt;这些模型在许多现实世界基准上表现良好，但在合成视频中的基本物理和常识推理方面仍存在困难。后训练可以改善MLLMs的异常检测和批判性思维能力。&lt;h4&gt;结论&lt;/h4&gt;针对性的训练对于提高MLLMs对常识和物理定律的理解是有价值的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zli12321/videohallu&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic video generation has gained significant attention for its realismand broad applications, but remains prone to violations of common sense andphysical laws. This highlights the need for reliable abnormality detectors thatunderstand such principles and are robust to hallucinations. To address this,we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built fromsynthetic videos generated by models like Veo2, Sora, and Kling, paired withexpert-crafted counterintuitive QA to evaluate the critical thinking abilitiesof Multi-modal Large Language Models (MLLMs) on abnormalities that areperceptually obvious to humans but often hallucinated due to language priors.VideoHallu evaluates MLLMs' abnormality detection abilities with examplesacross alignment, consistency, commonsense, and physics. We benchmark SOTAMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, andVideoChat-R1. We observe that these models perform well on many real-worldbenchmarks like MVBench and MovieChat, but still struggle with basicphysics-based and commonsense reasoning in synthetic videos. We further showthat post-training with Group Relative Policy Optimization (GRPO), usingcurriculum learning on datasets combining video QA with counterintuitivecommonsense and physics reasoning over real and synthetic videos, improvesMLLMs' abnormality detection and critical thinking, demonstrating the value oftargeted training for improving their understanding of commonsense and physicallaws.</description>
      <author>example@mail.com (Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber)</author>
      <guid isPermaLink="false">2505.01481v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GraphOracle: A Foundation Model for Knowledge Graph Reasoning</title>
      <link>http://arxiv.org/abs/2505.11125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GraphOracle的关系中心基础模型，该模型通过将知识图转换为关系依赖图（RDG）来统一知识图中的推理，并使用依赖注意力机制学习关系和实体的归纳表示。通过在多样化知识图上的预训练和分钟级微调，GraphOracle能够有效地泛化到未见过的实体、关系和整个图，并在31个不同基准测试中展现出优异的性能。&lt;h4&gt;背景&lt;/h4&gt;由于知识图的动态性质和跨领域推理的需求，开发与基础模型类似的知识图模型具有独特挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一知识图中推理的基础模型，以解决知识图的动态性质和跨领域推理的需求。&lt;h4&gt;方法&lt;/h4&gt;GraphOracle模型通过将知识图转换为RDG，并使用查询依赖的注意力机制学习关系和实体的归纳表示。此外，通过在多样化知识图上的预训练和分钟级微调来实现泛化。&lt;h4&gt;主要发现&lt;/h4&gt;GraphOracle在31个不同基准测试中表现出色，与最强大的基线相比，预测性能提高了35%。&lt;h4&gt;结论&lt;/h4&gt;GraphOracle模型通过有效的预训练和微调，在知识图推理方面实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为GraphOracle的关系中心基础模型，通过将知识图转换为关系依赖图，并使用查询依赖的注意力机制学习关系和实体的归纳表示，实现了知识图推理的统一。在多样化知识图上的预训练和分钟级微调使得该模型能够有效地泛化到未见过的实体、关系和整个图，通过31个不同基准测试的全面实验，该模型展现了优异的性能，与最强大的基线相比，预测性能提高了35%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have demonstrated remarkable capabilities across variousdomains, but developing analogous models for knowledge graphs presents uniquechallenges due to their dynamic nature and the need for cross-domain reasoning.To address these issues, we introduce \textbf{\textsc{GraphOracle}}, arelation-centric foundation model that unifies reasoning across knowledgegraphs by converting them into Relation-Dependency Graphs (RDG), explicitlyencoding compositional patterns with fewer edges than prior methods. Aquery-dependent attention mechanism is further developed to learn inductiverepresentations for both relations and entities. Pre-training on diverseknowledge graphs, followed by minutes-level fine-tuning, enables effectivegeneralization to unseen entities, relations, and entire graphs. Throughcomprehensive experiments on 31 diverse benchmarks spanning transductive,inductive, and cross-domain settings, we demonstrate consistentstate-of-the-art performance with minimal adaptation, improving the predictionperformance by up to 35\% compared to the strongest baselines.</description>
      <author>example@mail.com (Enjun Du, Siyi Liu, Yongqi Zhang)</author>
      <guid isPermaLink="false">2505.11125v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.11121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE International Geoscience and Remote Sensing  Symposium (IGARSS) 2025. Our code is available at  https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种加权特征聚合（WFA）策略，用于遥感（RS）中的视觉语言模型（VLM）预训练，以解决预训练和推理时间增加的问题。&lt;h4&gt;背景&lt;/h4&gt;通过预训练视觉语言模型（VLMs）来开发基础模型在遥感领域引起了广泛关注。VLM预训练旨在从大量的图像-文本对中学习图像和语言的对应关系。&lt;h4&gt;目的&lt;/h4&gt;目的是通过提取和利用每张图像多个字幕中的互补信息，同时通过重要性加权减少冗余信息，从而提高预训练和推理效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术来计算不同图像字幕的适应性重要性权重：(i) 非参数唯一性，基于双语评估（BLEU）分数来强调独特句子并减少重复句子的影响；(ii) 基于学习的注意力机制，通过注意力机制而不是手工特征来学习重要性权重。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的策略使得在遥感领域有效地预训练VLMs成为可能。基于实验分析，得出了根据下游任务需求和资源约束选择适当技术的指南。&lt;h4&gt;结论&lt;/h4&gt;WFA策略结合两种技术能够有效地在遥感领域预训练VLMs，并提供了根据不同需求选择技术的指导原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of foundation models through pretraining of vision-languagemodels (VLMs) has recently attracted great attention in remote sensing (RS).VLM pretraining aims to learn image and language alignments from a large numberof image-text pairs. Each pretraining image is often associated with multiplecaptions containing redundant information due to repeated or semanticallysimilar phrases, resulting in increased pretraining and inference time. Toovercome this, we introduce a weighted feature aggregation (WFA) strategy forVLM pretraining in RS. Our strategy aims to extract and exploit complementaryinformation from multiple captions per image while reducing redundanciesthrough feature aggregation with importance weighting. To calculate adaptiveimportance weights for different captions of each image, we propose twotechniques: (i) non-parametric uniqueness and (ii) learning-based attention. Inthe first technique, importance weights are calculated based on the bilingualevaluation understudy (BLEU) scores of the captions to emphasize uniquesentences and reduce the influence of repetitive ones. In the second technique,importance weights are learned through an attention mechanism instead ofrelying on hand-crafted features. The effectiveness of the proposed WFAstrategy with the two techniques is analyzed in terms of downstream performanceon text-to-image retrieval in RS. Experimental results show that the proposedstrategy enables efficient and effective pretraining of VLMs in RS. Based onthe experimental analysis, we derive guidelines for selecting appropriatetechniques depending on downstream task requirements and resource constraints.The code of this work is publicly available athttps://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.</description>
      <author>example@mail.com (Mathis Jürgen Adler, Leonard Hackel, Gencer Sumbul, Begüm Demir)</author>
      <guid isPermaLink="false">2505.11121v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
      <link>http://arxiv.org/abs/2505.00254v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, AVAS, add latency breakdown&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为AVAS的VLM-powered系统，用于开放式的先进视频分析，以解决现有系统在处理超长视频内容时的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的视频分析在多个领域变得至关重要，但现有系统通常限于特定的预定义任务，限制了其在开放式分析场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出AVAS系统，旨在实现开放式的视频理解、推理和分析。&lt;h4&gt;方法&lt;/h4&gt;AVAS系统包括两个关键创新：(1)近实时构建事件知识图谱（EKGs）以高效索引长或连续视频流；(2)利用EKGs的代理检索-生成机制来处理复杂和多样化的查询。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试LVBench和VideoMME-Long上，AVAS实现了最先进的性能，分别达到62.3%和64.1%的准确率，显著超过了现有的VLM和视频检索增强生成（RAG）系统。在新的基准AVAS-100上，AVAS也取得了顶尖性能，准确率达到75.8%。&lt;h4&gt;结论&lt;/h4&gt;AVAS系统在超长和开放式视频场景中的视频分析方面表现出色，为视频分析领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven video analytics has become increasingly pivotal across diversedomains. However, existing systems are often constrained to specific,predefined tasks, limiting their adaptability in open-ended analyticalscenarios. The recent emergence of Video-Language Models (VLMs) astransformative technologies offers significant potential for enablingopen-ended video understanding, reasoning, and analytics. Nevertheless, theirlimited context windows present challenges when processing ultra-long videocontent, which is prevalent in real-world applications. To address this, weintroduce AVAS, a VLM-powered system designed for open-ended, advanced videoanalytics. AVAS incorporates two key innovations: (1) the near real-timeconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long orcontinuous video streams, and (2) an agentic retrieval-generation mechanismthat leverages EKGs to handle complex and diverse queries. Comprehensiveevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate thatAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,respectively, significantly surpassing existing VLM and videoRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate videoanalytics in ultra-long and open-world video scenarios, we introduce a newbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hoursin duration, along with 120 manually annotated, diverse, and complexquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with anaccuracy of 75.8%.</description>
      <author>example@mail.com (Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu)</author>
      <guid isPermaLink="false">2505.00254v3</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere</title>
      <link>http://arxiv.org/abs/2505.11029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AsymVLM的概率视觉语言模型（VLM），用于处理自然语言和视觉数据中的固有模糊性和不确定性。&lt;h4&gt;背景&lt;/h4&gt;现有的确定性VLM在处理自然语言和视觉数据时无法捕捉其固有的模糊性和不确定性，而现有的概率后处理方法未能考虑模态的不对称不确定性结构和确定嵌入在单位超球面上的约束。&lt;h4&gt;目的&lt;/h4&gt;提出AsymVLM，以解决文本和视觉数据中固有的不对称不确定性结构，并从预训练的VLMs构建概率嵌入，实现不确定性量化。&lt;h4&gt;方法&lt;/h4&gt;AsymVLM通过将确定性嵌入映射到概率分布，并在单位超球面上建立概率嵌入，从而实现不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;AsymVLM在标准的基准测试中验证了其有效性，并通过消融实验展示了文本和视觉数据不确定性结构中不对称性的本质。&lt;h4&gt;结论&lt;/h4&gt;AsymVLM能够有效处理自然语言和视觉数据中的不确定性，并在相关任务中提供更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language models (VLMs) as foundation models have significantly enhanced performance across a wide range of visual and textual tasks, without requiring large-scale training from scratch for downstream tasks. However, these deterministic VLMs fail to capture the inherent ambiguity and uncertainty in natural language and visual data. Recent probabilistic post-hoc adaptation methods address this by mapping deterministic embeddings onto probability distributions; however, existing approaches do not account for the asymmetric uncertainty structure of the modalities, and the constraint that meaningful deterministic embeddings reside on a unit hypersphere, potentially leading to suboptimal performance. In this paper, we address the asymmetric uncertainty structure inherent in textual and visual data, and propose AsymVLM to build probabilistic embeddings from pre-trained VLMs on the unit hypersphere, enabling uncertainty quantification. We validate the effectiveness of the probabilistic embeddings on established benchmarks, and present comprehensive ablation studies demonstrating the inherent nature of asymmetry in the uncertainty structure of textual and visual data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) as foundation models have significantlyenhanced performance across a wide range of visual and textual tasks, withoutrequiring large-scale training from scratch for downstream tasks. However,these deterministic VLMs fail to capture the inherent ambiguity and uncertaintyin natural language and visual data. Recent probabilistic post-hoc adaptationmethods address this by mapping deterministic embeddings onto probabilitydistributions; however, existing approaches do not account for the asymmetricuncertainty structure of the modalities, and the constraint that meaningfuldeterministic embeddings reside on a unit hypersphere, potentially leading tosuboptimal performance. In this paper, we address the asymmetric uncertaintystructure inherent in textual and visual data, and propose AsymVLM to buildprobabilistic embeddings from pre-trained VLMs on the unit hypersphere,enabling uncertainty quantification. We validate the effectiveness of theprobabilistic embeddings on established benchmarks, and present comprehensiveablation studies demonstrating the inherent nature of asymmetry in theuncertainty structure of textual and visual data.</description>
      <author>example@mail.com (Li Ju, Max Andersson, Stina Fredriksson, Edward Glöckner, Andreas Hellander, Ekta Vats, Prashant Singh)</author>
      <guid isPermaLink="false">2505.11029v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges</title>
      <link>http://arxiv.org/abs/2505.10993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了计算病理学中生成建模的进展，涵盖了图像生成、文本生成、多模态图像-文本生成以及其他生成应用，如空间模拟和分子推断。&lt;h4&gt;背景&lt;/h4&gt;生成建模在计算病理学中显示出巨大潜力，包括数据高效学习、合成数据增强和多模态表示。&lt;h4&gt;目的&lt;/h4&gt;综合分析该领域的最新进展，并讨论开放挑战和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;通过分析超过150篇代表性研究，追踪生成架构从早期生成对抗网络到最近扩散模型和具有生成能力的基座模型的演变。&lt;h4&gt;主要发现&lt;/h4&gt;指出了生成高保真全切片图像、临床可解释性和合成数据伦理法律影响等挑战。&lt;h4&gt;结论&lt;/h4&gt;强调了开发统一、多模态和临床可部署的生成系统的重要性，并为研究者与实践者提供了基础参考。&lt;h4&gt;翻译&lt;/h4&gt;Generative modeling has emerged as a promising direction in computational pathology, offering capabilities such as data-efficient learning, synthetic data augmentation, and multimodal representation across diverse diagnostic tasks. This review provides a comprehensive synthesis of recent progress in the field, organized into four key domains: image generation, text generation, multimodal image-text generation, and other generative applications, including spatial simulation and molecular inference. By analyzing over 150 representative studies, we trace the evolution of generative architectures from early generative adversarial networks to recent advances in diffusion models and foundation models with generative capabilities. We further examine the datasets and evaluation protocols commonly used in this domain and highlight ongoing limitations, including challenges in generating high-fidelity whole-slide images, clinical interpretability, and concerns related to the ethical and legal implications of synthetic data. The review concludes with a discussion of open challenges and prospective research directions, with an emphasis on developing unified, multimodal, and clinically deployable generative systems. This work aims to provide a foundational reference for researchers and practitioners developing and applying generative models in computational pathology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has emerged as a promising direction in computationalpathology, offering capabilities such as data-efficient learning, syntheticdata augmentation, and multimodal representation across diverse diagnostictasks. This review provides a comprehensive synthesis of recent progress in thefield, organized into four key domains: image generation, text generation,multimodal image-text generation, and other generative applications, includingspatial simulation and molecular inference. By analyzing over 150representative studies, we trace the evolution of generative architectures fromearly generative adversarial networks to recent advances in diffusion modelsand foundation models with generative capabilities. We further examine thedatasets and evaluation protocols commonly used in this domain and highlightongoing limitations, including challenges in generating high-fidelity wholeslide images, clinical interpretability, and concerns related to the ethicaland legal implications of synthetic data. The review concludes with adiscussion of open challenges and prospective research directions, with anemphasis on developing unified, multimodal, and clinically deployablegenerative systems. This work aims to provide a foundational reference forresearchers and practitioners developing and applying generative models incomputational pathology.</description>
      <author>example@mail.com (Yuan Zhang, Xinfeng Zhang, Xiaoming Qi Xinyu Wu, Feng Chen, Guanyu Yang, Huazhu Fu)</author>
      <guid isPermaLink="false">2505.10993v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出首个针对基因组基础模型（GFMs）的统一对抗攻击基准GenoArmory，用于评估GFMs对抗攻击的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;目前缺乏对GFMs对抗攻击脆弱性的全面评估框架。&lt;h4&gt;目的&lt;/h4&gt;构建一个全面评估GFMs对抗攻击脆弱性的框架，并引入新的对抗样本数据集GenoAdv以提高GFMs的安全性。&lt;h4&gt;方法&lt;/h4&gt;使用四种广泛采用的攻击算法和三种防御策略，评估了五种最先进的GFMs的对抗鲁棒性，并分析了模型架构、量化方案和训练数据集对GFM脆弱性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分类模型相较于生成模型对对抗扰动具有更强的鲁棒性，对抗攻击常针对具有生物学意义的基因组区域，表明这些模型有效捕捉了有意义的序列特征。&lt;h4&gt;结论&lt;/h4&gt;GenoArmory提供了一个评估GFMs对抗攻击脆弱性的框架，有助于提高GFMs的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the first unified adversarial attack benchmark for GenomicFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,GenoArmory offers the first comprehensive evaluation framework tosystematically assess the vulnerability of GFMs to adversarial attacks.Methodologically, we evaluate the adversarial robustness of fivestate-of-the-art GFMs using four widely adopted attack algorithms and threedefense strategies. Importantly, our benchmark provides an accessible andcomprehensive framework to analyze GFM vulnerabilities with respect to modelarchitecture, quantization schemes, and training datasets. Additionally, weintroduce GenoAdv, a new adversarial sample dataset designed to improve GFMsafety. Empirically, classification models exhibit greater robustness toadversarial perturbations compared to generative models, highlighting theimpact of task type on model vulnerability. Moreover, adversarial attacksfrequently target biologically significant genomic regions, suggesting thatthese models effectively capture meaningful sequence features.</description>
      <author>example@mail.com (Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen)</author>
      <guid isPermaLink="false">2505.10983v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a conference paper in ICML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PITA的自监督学习框架，用于解决自回归PDE模型在处理时间依赖数据时的误差累积问题，特别是对于分布外数据。&lt;h4&gt;背景&lt;/h4&gt;自回归PDE模型在处理时间依赖数据方面展现出巨大潜力，但存在由于自回归预测导致的短路问题，导致误差累积。&lt;h4&gt;目的&lt;/h4&gt;提出PITA框架以解决自回归PDE模型在处理分布外数据时的性能问题，提高模型的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;PITA通过将物理信息约束整合到自监督信号中，对每个PDE轨迹在不同时间步发现的物理动力学进行对齐。&lt;h4&gt;主要发现&lt;/h4&gt;PITA不需要依赖已知的物理先验，仅从观测数据中推导对齐，显示出对分布外数据的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;实验表明，PITA显著提高了现有基础模型在多种时间依赖PDE数据上的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自回归偏微分方程（PDE）基础模型在处理时间依赖数据方面显示出巨大潜力。然而，这些模型受到自回归预测中根深蒂固的短路问题的困扰，导致误差累积。对于分布外数据，这一挑战尤其明显，因为预训练性能可能接近随机模型初始化，对于具有长期动态的下游任务。为了解决这个问题，我们提出了物理信息时间对齐（PITA），这是一个受逆问题解决启发的自监督学习框架。具体来说，PITA通过将物理信息约束整合到自监督信号中，对每个给定的PDE轨迹在不同时间步发现的物理动力学进行对齐。对齐是从观测数据中得出的，而不依赖于已知的物理先验，这表明了对分布外数据的强大泛化能力。大量实验表明，PITA显著提高了现有基础模型在多种时间依赖PDE数据上的准确性和鲁棒性。代码可在https://github.com/SCAILab-USTC/PITA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-regressive partial differential equation (PDE) foundation models haveshown great potential in handling time-dependent data. However, these modelssuffer from the shortcut problem deeply rooted in auto-regressive prediction,causing error accumulation. The challenge becomes particularly evident forout-of-distribution data, as the pretraining performance may approach randommodel initialization for downstream tasks with long-term dynamics. To deal withthis problem, we propose physics-informed temporal alignment (PITA), aself-supervised learning framework inspired by inverse problem solving.Specifically, PITA aligns the physical dynamics discovered at different timesteps on each given PDE trajectory by integrating physics-informed constraintsinto the self-supervision signal. The alignment is derived from observationdata without relying on known physics priors, indicating strong generalizationability to the out-of-distribution data. Extensive experiments show that PITAsignificantly enhances the accuracy and robustness of existing foundationmodels on diverse time-dependent PDE data. The code is available athttps://github.com/SCAILab-USTC/PITA.</description>
      <author>example@mail.com (Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen)</author>
      <guid isPermaLink="false">2505.10930v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision</title>
      <link>http://arxiv.org/abs/2505.10875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website and code: https://dktpt44.github.io/LV-GPT/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于空间增强的多模态大型语言模型的方法，旨在帮助盲人和低视力人士更有效地导航和互动。&lt;h4&gt;背景&lt;/h4&gt;盲人和低视力人士在环境中导航和定位物体时面临挑战，因为他们的视觉线索有限。空间推理对于这些个体至关重要，因为它使他们能够理解和解释周围环境中的空间关系。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够辅助盲人和低视力人士感知和互动周围环境的轻量级、易用系统。&lt;h4&gt;方法&lt;/h4&gt;通过微调大型语言模型以包含空间推理能力，并设计了一个眼镜附件作为硬件组件，利用高级视觉语言模型来解释视觉数据并提供实时、空间感知的反馈。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了对环境上下文的理解，并在导航和物体识别方面取得了实质性改进。&lt;h4&gt;结论&lt;/h4&gt;该研究通过VizWiz数据集的深入评估和设计的数据集，证明了该方法在现实世界情况中的有效性和准确性，同时改善了用户体验。&lt;h4&gt;翻译&lt;/h4&gt;摘要：盲人和低视力人士（pBLV）面临重大挑战，由于视觉线索有限，他们在环境中导航和定位物体时很吃力。空间推理对这些个体至关重要，因为它使他们能够理解和解释周围环境中的空间关系，从而增强他们导航和更安全、独立互动的能力。当前针对低视力人士的多模态大型语言（MLLM）模型缺乏有效辅助这些任务所需的空间推理能力。此外，缺乏轻量级、易于使用的系统，使得pBLV能够有效地感知和与其周围环境互动。在本文中，我们提出了一种针对视觉障碍个体的新颖的空间增强多模态大型语言模型方法。通过微调MLLM以包含空间推理能力，我们的方法显著提高了对环境上下文的理解，这对于导航和物体识别至关重要。这一创新还扩展到一个硬件组件，设计为眼镜附件，确保了更高的可访问性和易用性。这种集成利用高级视觉语言模型来解释视觉数据，并向用户提供实时、空间感知的反馈。我们的方法旨在弥合高级机器学习模型与实用、用户友好的辅助设备之间的差距，为视觉障碍用户提供了一种稳健的解决方案，使他们能够更有效地独立导航周围环境。论文包括使用VizWiz数据集的深入评估，证明了准确性和用户体验的实质性改进。此外，我们还设计了一个综合数据集来评估我们方法在现实世界情况中的有效性，证明了准确性和用户体验的实质性改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; People with blindness and low vision (pBLV) face significant challenges,struggling to navigate environments and locate objects due to limited visualcues. Spatial reasoning is crucial for these individuals, as it enables them tounderstand and interpret the spatial relationships in their surroundings,enhancing their ability to navigate and interact more safely and independently.Current multi-modal large language (MLLM) models for low vision people lack thespatial reasoning capabilities needed to effectively assist in these tasks.Moreover, there is a notable absence of lightweight, easy-to-use systems thatallow pBLV to effectively perceive and interact with their surroundingenvironment. In this paper, we propose a novel spatial enhanced multi-modallarge language model based approach for visually impaired individuals. Byfine-tuning the MLLM to incorporate spatial reasoning capabilities, our methodsignificantly improves the understanding of environmental context, which iscritical for navigation and object recognition. The innovation extends to ahardware component, designed as an attachment for glasses, ensuring increasedaccessibility and ease of use. This integration leverages advanced VLMs tointerpret visual data and provide real-time, spatially aware feedback to theuser. Our approach aims to bridge the gap between advanced machine learningmodels and practical, user-friendly assistive devices, offering a robustsolution for visually impaired users to navigate their surroundings moreeffectively and independently. The paper includes an in-depth evaluation usingthe VizWiz dataset, demonstrating substantial improvements in accuracy and userexperience. Additionally, we design a comprehensive dataset to evaluate ourmethod's effectiveness in realworld situations, demonstrating substantialimprovements in accuracy and user experience.</description>
      <author>example@mail.com (Alexey Magay, Dhurba Tripathi, Yu Hao, Yi Fang)</author>
      <guid isPermaLink="false">2505.10875v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers</title>
      <link>http://arxiv.org/abs/2505.10855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合变换器卷积网络（HTN）来改善AI自动分割在放疗计划中的应用，尤其是在面对与训练数据集不同特征的病例时。&lt;h4&gt;背景&lt;/h4&gt;在临床案例中，当应用与训练数据集不同特征的AI自动分割时，其效果可能会变差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同成像对比度和患者位置的混合变换器卷积网络（HTN）来分割心脏亚结构和肺癌、乳腺癌患者的图像。&lt;h4&gt;方法&lt;/h4&gt;使用包含56例增强CT（CECT）和124例非增强CT（NCCT）扫描的患者数据集创建了HTN模型，并在60例Cohort I患者和66例Cohort II患者的验证集上评估了模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;HTN模型在Cohort I和Cohort II数据集上的准确性与公开可用的TotalSegmentator相当，并且在使用一半的训练案例时，其剂量指标与手动划定的结果相似。&lt;h4&gt;结论&lt;/h4&gt;HTN在处理具有不同成像和患者特征的CT图像时表现出稳健的准确性，并且结合了预训练和平衡NCCT和CECT扫描分布的模型能够提供可靠的分割，且所需的标记数据集远少于Oracle模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在放疗计划（RTP）中应用AI自动分割时，若遇到与训练数据集特征不同的临床案例，其效果可能会下降。因此，我们将预训练的变换器改进为混合变换器卷积网络（HTN），以处理具有不同成像对比度和患者位置的胸部亚结构和肺癌、乳腺癌患者的图像。Cohort I包括56例CECT和124例NCCT扫描，来自仰卧位非小细胞肺癌患者，用于创建包含所有180个训练案例和平衡（CECT：32，NCCT：32）HTN模型的Oracle。这些模型在60例Cohort I患者和66例Cohort II患者（仰卧位n=45，俯卧位n=21）的保留验证集上进行了评估。使用DSC、HD95和剂量指标来衡量准确性。公开可用的TotalSegmentator作为基准。Oracle和平衡模型在Cohort I和Cohort II数据集上的准确性相似（DSC Cohort I：0.80 ± 0.10 versus 0.81 ± 0.10；Cohort II：0.77 ± 0.13 versus 0.80 ± 0.12），优于TotalSegmentator。使用一半训练案例的平衡模型在所有心脏亚结构上产生了与手动划定的相似剂量指标。该模型在8个亚结构中的6个对CT对比度具有鲁棒性，在8个亚结构中的5个对患者的扫描位置变化具有鲁棒性，并且准确性与患者大小和年龄的低相关性。HTN从具有不同成像和患者特征的CT图像中稳健准确地分割心脏亚结构，这是临床应用的关键要求。此外，结合预训练和NCCT和CECT扫描平衡分布的模型能够在多种条件下提供可靠的分割，与Oracle模型相比，所需的标记数据集要少得多。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI automated segmentations for radiation treatment planning (RTP) candeteriorate when applied in clinical cases with different characteristics thantraining dataset. Hence, we refined a pretrained transformer into a hybridtransformer convolutional network (HTN) to segment cardiac substructures lungand breast cancer patients acquired with varying imaging contrasts and patientscan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124non-contrast CT (NCCT) scans from patients with non-small cell lung cancersacquired in supine position, was used to create oracle with all 180 trainingcases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models wereevaluated on a held-out validation set of 60 cohort I patients and 66 patientswith breast cancer from cohort II acquired in supine (n=45) and prone (n=21)positions. Accuracy was measured using DSC, HD95, and dose metrics. Publiclyavailable TotalSegmentator served as the benchmark. The oracle and balancedmodels were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperformingTotalSegmentator. The balanced model, using half the training cases as oracle,produced similar dose metrics as manual delineations for all cardiacsubstructures. This model was robust to CT contrast in 6 out of 8 substructuresand patient scan position variations in 5 out of 8 substructures and showed lowcorrelations of accuracy to patient size and age. A HTN demonstrated robustlyaccurate (geometric and dose metrics) cardiac substructures segmentation fromCTs with varying imaging and patient characteristics, one key requirement forclinical use. Moreover, the model combining pretraining with balanceddistribution of NCCT and CECT scans was able to provide reliably accuratesegmentations under varied conditions with far fewer labeled datasets comparedto an oracle model.</description>
      <author>example@mail.com (Aneesh Rangnekar, Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, Abraham Wu, Maria Thor, Andreas Rimner, Harini Veeraraghavan)</author>
      <guid isPermaLink="false">2505.10855v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Foundation model for mass spectrometry proteomics</title>
      <link>http://arxiv.org/abs/2505.10848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的蛋白质组学质谱数据分析方法，通过预训练模型来提高谱图预测任务的性能。&lt;h4&gt;背景&lt;/h4&gt;质谱技术在蛋白质组学领域占主导地位，但数据处理和解释需要复杂的计算方法。机器学习在提高质谱数据分析方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的框架，将多种谱图预测任务集成到一个基础模型中，以改善质谱数据分析。&lt;h4&gt;方法&lt;/h4&gt;使用从头测序作为预训练任务，预训练一个谱图编码器。然后，使用这些预训练的谱图表示来提高谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测等下游任务的表现。最后，进行多任务微调。&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型在谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测等任务上都有所提升，且多任务微调进一步提高了每个任务的表现。&lt;h4&gt;结论&lt;/h4&gt;基于从头测序训练的基础模型能够学习谱图的一般化表示，提高训练数据有限的后处理任务性能，并最终增强蛋白质组学实验中的数据采集和分析。&lt;h4&gt;翻译&lt;/h4&gt;摘要：质谱学是蛋白质组学领域的领先技术，它使得对复杂生物样本中蛋白质含量的高通量分析成为可能。由于仪器复杂性和数据的复杂性，需要复杂的计算方法来处理和解释获得的质谱数据。机器学习在提高质谱数据分析方面展现出巨大的潜力，许多专门为改进数据采集和分析流程中的特定步骤而设计的机器学习方法已经得到广泛应用。在这里，我们提出将各种谱图预测任务统一到一个单一的基础模型中。为此，我们使用从头测序作为预训练任务来预训练一个谱图编码器。然后，我们表明使用这些预训练的谱图表示可以提高我们在谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测四个下游任务上的性能。最后，我们进行了多任务微调，并发现这种方法提高了每个任务的表现。总的来说，我们的工作表明，在从头测序上训练的串联质谱蛋白质组学基础模型能够学习谱图的一般化表示，提高训练数据有限的后处理任务性能，并最终增强蛋白质组学实验中的数据采集和分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mass spectrometry is the dominant technology in the field of proteomics,enabling high-throughput analysis of the protein content of complex biologicalsamples. Due to the complexity of the instrumentation and resulting data,sophisticated computational methods are required for the processing andinterpretation of acquired mass spectra. Machine learning has shown greatpromise to improve the analysis of mass spectrometry data, with numerouspurpose-built methods for improving specific steps in the data acquisition andanalysis pipeline reaching widespread adoption. Here, we propose unifyingvarious spectrum prediction tasks under a single foundation model for massspectra. To this end, we pre-train a spectrum encoder using de novo sequencingas a pre-training task. We then show that using these pre-trained spectrumrepresentations improves our performance on the four downstream tasks ofspectrum quality prediction, chimericity prediction, phosphorylationprediction, and glycosylation status prediction. Finally, we perform multi-taskfine-tuning and find that this approach improves the performance on each taskindividually. Overall, our work demonstrates that a foundation model for tandemmass spectrometry proteomics trained on de novo sequencing learns generalizablerepresentations of spectra, improves performance on downstream tasks wheretraining data is limited, and can ultimately enhance data acquisition andanalysis in proteomics experiments.</description>
      <author>example@mail.com (Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble)</author>
      <guid isPermaLink="false">2505.10848v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification</title>
      <link>http://arxiv.org/abs/2505.10823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究评估了通用和医疗领域特定基础模型提取的嵌入在多类别放射学分类中的实用性，特别是在管位评估方面，发现MedImageInsight嵌入与支持向量机适配器结合表现最佳。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大量数据集上预训练，显著推动了机器学习的发展，为各个领域提供了鲁棒且可迁移的嵌入。&lt;h4&gt;目的&lt;/h4&gt;评估通用和医疗领域特定基础模型提取的嵌入在多类别放射学分类中的实用性，特别是在管位评估方面。&lt;h4&gt;方法&lt;/h4&gt;使用包括DenseNet121、BiomedCLIP、Med-Flamingo、MedImageInsight、Rad-DINO和CXR-Foundation在内的六种基础模型提取嵌入，并使用经典机器学习算法训练适配模型。&lt;h4&gt;主要发现&lt;/h4&gt;MedImageInsight嵌入与支持向量机适配器组合的mAUC最高，达到93.8%，其次是Rad-DINO（91.1%）和CXR-Foundation（89.0%）。BiomedCLIP和DenseNet121表现中等，mAUC分别为83.0%和81.8%，而Med-Flamingo表现最低，为75.1%。适配模型计算效率高，CPU上训练仅需一分钟，推理仅需几秒。&lt;h4&gt;结论&lt;/h4&gt;基础模型嵌入，特别是MedImageInsight的嵌入，通过轻量级适配器促进了放射图像分析的准确、计算高效和公平的诊断分类。&lt;h4&gt;翻译&lt;/h4&gt;This study evaluates the utility of embeddings derived from both general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing specifically on tube placement assessment. A dataset comprising 8842 radiographs classified into seven distinct categories was employed to extract embeddings using six foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight, Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using classical machine learning algorithms. Among these combinations, MedImageInsight embeddings paired with an support vector machine adapter yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%, respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%. Notably, most adapter models demonstrated computational efficiency, achieving training within one minute and inference within seconds on CPU, underscoring their practicality for clinical applications. Furthermore, fairness analyses on adapters trained on MedImageInsight-derived embeddings indicated minimal disparities, with gender differences in performance within 2% and standard deviations across age groups not exceeding 3%. These findings confirm that foundation model embeddings-especially those from MedImageInsight-facilitate accurate, computationally efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, pretrained on extensive datasets, have significantlyadvanced machine learning by providing robust and transferable embeddingsapplicable to various domains, including medical imaging diagnostics. Thisstudy evaluates the utility of embeddings derived from both general-purpose andmedical domain-specific foundation models for training lightweight adaptermodels in multi-class radiography classification, focusing specifically on tubeplacement assessment. A dataset comprising 8842 radiographs classified intoseven distinct categories was employed to extract embeddings using sixfoundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained usingclassical machine learning algorithms. Among these combinations,MedImageInsight embeddings paired with an support vector machine adapteryielded the highest mean area under the curve (mAUC) at 93.8%, followed closelyby Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP andDenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.Notably, most adapter models demonstrated computational efficiency, achievingtraining within one minute and inference within seconds on CPU, underscoringtheir practicality for clinical applications. Furthermore, fairness analyses onadapters trained on MedImageInsight-derived embeddings indicated minimaldisparities, with gender differences in performance within 2% and standarddeviations across age groups not exceeding 3%. These findings confirm thatfoundation model embeddings-especially those from MedImageInsight-facilitateaccurate, computationally efficient, and equitable diagnostic classificationusing lightweight adapters for radiographic image analysis.</description>
      <author>example@mail.com (Xue Li, Jameson Merkow, Noel C. F. Codella, Alberto Santamaria-Pang, Naiteek Sangani, Alexander Ersoy, Christopher Burt, John W. Garrett, Richard J. Bruce, Joshua D. Warner, Tyler Bradshaw, Ivan Tarapov, Matthew P. Lungren, Alan B. McMillan)</author>
      <guid isPermaLink="false">2505.10823v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.10781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种完全弱监督的类增量学习语义分割方法，用于仅使用图像级标签学习基础和新增类别的分割。&lt;h4&gt;背景&lt;/h4&gt;传统的类增量语义分割方法需要昂贵的像素级标注进行训练，而部分弱监督方法虽然有所改进，但尚未有完全弱监督的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种完全弱监督的类增量学习语义分割方法，以降低对像素级标注的依赖。&lt;h4&gt;方法&lt;/h4&gt;1. 通过结合定位器和一系列基于不确定性的基础模型生成鲁棒的伪标签。2. 引入示例引导的数据增强方法，生成包含先前和新增类别的多样化图像。3. 在三个常见的实验设置和两种场景下进行实验：15-5 VOC、10-10 VOC、COCO-to-VOC，以及非重叠和重叠。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在15-5 VOC和10-10 VOC设置中，完全弱监督方法优于部分弱监督方法，在COCO-to-VOC设置中也能达到有竞争力的准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的完全弱监督方法在类增量学习语义分割中具有优越性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文针对完全弱监督类增量学习语义分割任务进行研究，旨在仅使用图像级标签学习基础及新增类别的分割。虽然类增量语义分割（CISS）对于处理现实世界中的多样化和新兴物体至关重要，但传统的CISS方法需要昂贵的像素级标注进行训练。为了克服这一限制，最近提出了部分弱监督方法。然而，据我们所知，这是第一个提出完全弱监督CISS方法的工作。为了实现这一目标，我们提出通过结合定位器和基于不确定性的基础模型生成鲁棒的伪标签。此外，为了减轻灾难性遗忘，我们引入了一种示例引导的数据增强方法，该方法在引导下生成包含先前和新增类别的多样化图像。最后，我们在三个常见的实验设置：15-5 VOC、10-10 VOC和COCO-to-VOC，以及两种场景：非重叠和重叠下进行了实验。实验结果表明，我们的完全弱监督方法在15-5 VOC和10-10 VOC设置中优于部分弱监督方法，在COCO-to-VOC设置中也实现了有竞争力的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work addresses the task of completely weakly supervisedclass-incremental learning for semantic segmentation to learn segmentation forboth base and additional novel classes using only image-level labels. Whileclass-incremental semantic segmentation (CISS) is crucial for handling diverseand newly emerging objects in the real world, traditional CISS methods requireexpensive pixel-level annotations for training. To overcome this limitation,partially weakly-supervised approaches have recently been proposed. However, tothe best of our knowledge, this is the first work to introduce a completelyweakly-supervised method for CISS. To achieve this, we propose to generaterobust pseudo-labels by combining pseudo-labels from a localizer and a sequenceof foundation models based on their uncertainty. Moreover, to mitigatecatastrophic forgetting, we introduce an exemplar-guided data augmentationmethod that generates diverse images containing both previous and novel classeswith guidance. Finally, we conduct experiments in three common experimentalsettings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjointand overlap. The experimental results demonstrate that our completely weaklysupervised method outperforms even partially weakly supervised methods in the15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in theCOCO-to-VOC setting.</description>
      <author>example@mail.com (David Minkwan Kim, Soeun Lee, Byeongkeun Kang)</author>
      <guid isPermaLink="false">2505.10781v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unifying Segment Anything in Microscopy with Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2505.10769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为uLLSAM的新方法，通过使用多模态大型语言模型（MLLMs）来指导Segment Anything Model（SAM）学习显微镜跨域数据，从而提高生物医学图像分割的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前生物医学图像分割的基金模型在特定数据集上表现良好，但在未见域数据上性能不佳，这归因于分割前缺乏视觉-语言知识。&lt;h4&gt;目的&lt;/h4&gt;通过利用MLLMs注入视觉-语言知识（VLK），使视觉模型在跨域数据集上表现出更强的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Vision-Language Semantic Alignment（VLSA）的模块，该模块将VLK注入SAM中。同时，为了解决边界轮廓感知的不足，进一步提出了Semantic Boundary Regularization（SBR）来指导SAM。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在9个域内显微镜数据集上，Dice和SA的性能分别提高了7.71%和12.10%，达到了最先进的性能。在10个域外数据集上，Dice和SA的性能分别提高了6.79%和10.08%，展现了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;uLLSAM方法通过结合MLLMs和SBR模块，显著提高了生物医学图像分割的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a new method called uLLSAM, which guides the Segment Anything Model (SAM) to learn cross-domain data in microscopy using Multimodal Large Language Models (MLLMs), thereby improving the accuracy of biomedical image segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of regions of interest in biomedical images holdssubstantial value in image analysis. Although several foundation models forbiomedical segmentation have currently achieved excellent performance oncertain datasets, they typically demonstrate sub-optimal performance on unseendomain data. We owe the deficiency to lack of vision-language knowledge beforesegmentation. Multimodal Large Language Models (MLLMs) bring outstandingunderstanding and reasoning capabilities to multimodal tasks, which inspires usto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enablingvision models to demonstrate superior generalization capabilities oncross-domain datasets. In this paper, we propose using MLLMs to guide SAM inlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment(VLSA) module, which injects VLK into Segment Anything Model (SAM). We findthat after SAM receives global VLK prompts, its performance improvessignificantly, but there are deficiencies in boundary contour perception.Therefore, we further propose Semantic Boundary Regularization (SBR) to promptSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%in SA across 9 in-domain microscopy datasets, achieving state-of-the-artperformance. Our method also demonstrates improvements of 6.79% in Dice and10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalizationcapabilities. Code is available at https://github.com/ieellee/uLLSAM.</description>
      <author>example@mail.com (Manyu Li, Ruian He, Zixian Zhang, Weimin Tan, Bo Yan)</author>
      <guid isPermaLink="false">2505.10769v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?</title>
      <link>http://arxiv.org/abs/2505.10714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GeoGrid-Bench，一个用于评估基础模型在网格结构中理解地理空间数据能力的基准。&lt;h4&gt;背景&lt;/h4&gt;地理空间数据集由于其密集的数值、强烈的时空依赖性和独特的多模态表示（包括表格数据、热图和地理可视化）而具有独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;为了评估基础模型如何支持这一领域的科学研究，GeoGrid-Bench包含大规模、真实世界的数据，覆盖150个地点的16个气候变量，并涉及长时间框架。&lt;h4&gt;方法&lt;/h4&gt;基准包括大约3,200个问答对，这些问答对是从8个领域专家精心设计的模板中系统生成的，以反映人类科学家遇到的实际任务。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，视觉语言模型在整体表现上最佳，并提供了对不同基础模型在不同地理空间任务中的优势和局限性的细致分析。&lt;h4&gt;结论&lt;/h4&gt;该基准为如何有效地将基础模型应用于地理空间数据分析以及如何支持科学研究提供了更清晰的见解。&lt;h4&gt;翻译&lt;/h4&gt;我们提出GeoGrid-Bench，这是一个用于评估基础模型在网格结构中理解地理空间数据能力的基准。由于地理空间数据集具有密集的数值、强烈的时空依赖性和独特的多模态表示（包括表格数据、热图和地理可视化），因此它们提出了独特的挑战。为了评估基础模型如何支持这一领域的科学研究，GeoGrid-Bench包含大规模、真实世界的数据，覆盖150个地点的16个气候变量，并涉及长时间框架。基准包括大约3,200个问答对，这些问答对是从8个领域专家精心设计的模板中系统生成的，以反映人类科学家遇到的实际任务。评估结果显示，视觉语言模型在整体表现上最佳，并提供了对不同基础模型在不同地理空间任务中的优势和局限性的细致分析。该基准为如何有效地将基础模型应用于地理空间数据分析以及如何支持科学研究提供了更清晰的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GeoGrid-Bench, a benchmark designed to evaluate the ability offoundation models to understand geo-spatial data in the grid structure.Geo-spatial datasets pose distinct challenges due to their dense numericalvalues, strong spatial and temporal dependencies, and unique multimodalrepresentations including tabular data, heatmaps, and geographicvisualizations. To assess how foundation models can support scientific researchin this domain, GeoGrid-Bench features large-scale, real-world data covering 16climate variables across 150 locations and extended time frames. The benchmarkincludes approximately 3,200 question-answer pairs, systematically generatedfrom 8 domain expert-curated templates to reflect practical tasks encounteredby human scientists. These range from basic queries at a single location andtime to complex spatiotemporal comparisons across regions and periods. Ourevaluation reveals that vision-language models perform best overall, and weprovide a fine-grained analysis of the strengths and limitations of differentfoundation models in different geo-spatial tasks. This benchmark offers clearerinsights into how foundation models can be effectively applied to geo-spatialdata analysis and used to support scientific research.</description>
      <author>example@mail.com (Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick)</author>
      <guid isPermaLink="false">2505.10714v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)</title>
      <link>http://arxiv.org/abs/2505.10640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Foundation Models（FM）如大型语言模型（LLMs）如何通过FMware系统改变软件行业，并详细探讨了FMware的发展现状、挑战、以及克服这些挑战的策略。&lt;h4&gt;背景&lt;/h4&gt;FMware是将FM作为核心组件的系统，LLMs等FM正在重塑软件行业。&lt;h4&gt;目的&lt;/h4&gt;提供对FMware的全面探索，结合挑战目录和实际生产问题。&lt;h4&gt;方法&lt;/h4&gt;讨论了构建FMware的研究和实践状态，分析了选择模型、数据对齐、工程化提示和协调自主代理的困难，并概述了从演示到生产系统的复杂过程，包括系统测试、优化、部署和与旧软件的集成。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了在FMware开发过程中遇到的挑战，如模型选择、数据对齐、提示工程和系统集成等。&lt;h4&gt;结论&lt;/h4&gt;通过工业经验和该领域的研究，提供了克服这些挑战的实用策略和技术路线图。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了如何通过集成大型语言模型等Foundation Models作为核心组件的FMware系统来重塑软件行业。在KDD 2025教程中，我们全面探讨了FMware，结合了精心制作的挑战目录和现实世界的生产问题。我们首先讨论了构建FMware的研究和实践现状，进一步分析了选择合适的模型、对齐高质量的特定领域数据、工程化鲁棒的提示和协调自主代理的困难。然后，我们概述了从令人印象深刻的演示到生产就绪系统的复杂旅程，包括系统测试、优化、部署和与旧软件的集成问题。基于我们在该领域的工业经验和最近的研究，我们提供了克服这些挑战的可行见解和技术路线图。参与者将获得在不断发展的技术环境中创建可信赖的FMware的实用策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models (FMs) such as Large Language Models (LLMs) are reshapingthe software industry by enabling FMware, systems that integrate these FMs ascore components. In this KDD 2025 tutorial, we present a comprehensiveexploration of FMware that combines a curated catalogue of challenges withreal-world production concerns. We first discuss the state of research andpractice in building FMware. We further examine the difficulties in selectingsuitable models, aligning high-quality domain-specific data, engineering robustprompts, and orchestrating autonomous agents. We then address the complexjourney from impressive demos to production-ready systems by outlining issuesin system testing, optimization, deployment, and integration with legacysoftware. Drawing on our industrial experience and recent research in the area,we provide actionable insights and a technology roadmap for overcoming thesechallenges. Attendees will gain practical strategies to enable the creation oftrustworthy FMware in the evolving technology landscape.</description>
      <author>example@mail.com (Kirill Vasilevski, Benjamin Rombaut, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Keheliya Gallaba, Filipe R. Cogo, Jiahuei, Lin, Dayi Lin, Haoxiang Zhang, Bouyan Chen, Kishanthan Thangarajah, Ahmed E. Hassan, Zhen Ming, Jiang)</author>
      <guid isPermaLink="false">2505.10640v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness</title>
      <link>http://arxiv.org/abs/2505.08320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpecSphere是一个双遍历的频谱-空间图神经网络，能够对预测进行验证，适应同质性和异质性的完整谱，并且超越了1-Weisfeiler-Lehman的表达能力，同时保持线性时间复杂度。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络方法在鲁棒性和表达能力方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图神经网络模型SpecSphere，以提高预测的鲁棒性和表达能力。&lt;h4&gt;方法&lt;/h4&gt;SpecSphere结合了切比雪夫多项式频谱分支和注意力门控空间分支，并通过轻量级的MLP在合作-对抗的min-max游戏中训练，融合两者表示。&lt;h4&gt;主要发现&lt;/h4&gt;SpecSphere实现了统一的切比雪夫逼近定理，最小-最大最优风险，闭式鲁棒性证书，以及严格超越1-WL的通用逼近能力。&lt;h4&gt;结论&lt;/h4&gt;SpecSphere在节点分类准确性和认证鲁棒性方面达到了最先进的水平，证明了高表达能力、异质性适应性和可证明的鲁棒性可以共存于单一的可扩展架构中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SpecSphere, the first dual-pass spectral-spatial GNN thatcertifies every prediction against both $\ell\_{0}$ edge flips and$\ell\_{\infty}$ feature perturbations, adapts to the fullhomophily-heterophily spectrum, and surpasses the expressive power of1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples aChebyshev-polynomial spectral branch with an attention-gated spatial branch andfuses their representations through a lightweight MLP trained in acooperative-adversarial min-max game. We further establish (i) a uniformChebyshev approximation theorem, (ii) minimax-optimal risk across thehomophily-heterophily spectrum, (iii) closed-form robustness certificates, and(iv) universal approximation strictly beyond 1-WL. SpecSphere achievesstate-of-the-art node-classification accuracy and delivers tighter certifiedrobustness guarantees on real-world benchmarks. These results demonstrate thathigh expressivity, heterophily adaptation, and provable robustness can coexistwithin a single, scalable architecture.</description>
      <author>example@mail.com (Yoonhyuk Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2505.08320v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
  <item>
      <title>Schreier-Coset Graph Propagation</title>
      <link>http://arxiv.org/abs/2505.10392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 1 figure , preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SCGP是一种基于群理论的图传播方法，通过Schreier-coset嵌入丰富节点特征，提高长距离消息传递的效率，同时保持计算效率，在处理层次化和模块化图结构时表现出优势。&lt;h4&gt;背景&lt;/h4&gt;GNNs在图结构数据学习中表现良好，但易受信息压缩影响，现有解决方案如图重连和Cayley图等方法存在可扩展性瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出SCGP方法以解决GNNs在处理大型图结构时的信息压缩问题，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;SCGP通过Schreier-coset嵌入丰富节点特征，并将无瓶颈的连接模式嵌入到紧凑的特征空间中。&lt;h4&gt;主要发现&lt;/h4&gt;SCGP在节点和图分类基准测试中表现出与扩展开图和重连GNN基线相当甚至更好的性能，尤其适用于处理层次化和模块化图结构，具有较低的计算延迟、可扩展性和内存占用。&lt;h4&gt;结论&lt;/h4&gt;SCGP是一种高效且适用于资源受限应用的图传播方法，特别适合实时和资源受限的环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) offer a principled framework for learning overgraph-structured data, yet their expressive capacity is often hindered byover-squashing, wherein information from distant nodes is compressed intofixed-size vectors. Existing solutions, including graph rewiring andbottleneck-resistant architectures such as Cayley and expander graphs, avoidthis problem but introduce scalability bottlenecks. In particular, the Cayleygraphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoreticalproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memoryusage. To address this, this work introduces Schrier-Coset Graph Propagation(SCGP), a group-theoretic augmentation method that enriches node featuresthrough Schreier-coset embeddings without altering the input graph topology.SCGP embeds bottleneck-free connectivity patterns into a compact feature space,improving long-range message passing while maintaining computationalefficiency. Empirical evaluations across standard node and graph classificationbenchmarks demonstrate that SCGP achieves performance comparable to, orexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibitsparticular advantages in processing hierarchical and modular graph structures,offering reduced inference latency, improved scalability, and a low memoryfootprint, making it suitable for real-time and resource-constrainedapplications.</description>
      <author>example@mail.com (Aryan Mishra, Lizhen Lin)</author>
      <guid isPermaLink="false">2505.10392v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
      <link>http://arxiv.org/abs/2505.10292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StoryReasoning的故事推理系统，用于解决视觉叙事系统在保持角色身份和连接动作到适当主体时遇到的问题，如参照幻觉。该系统通过将角色、物体和其他实体基于视觉元素进行固化来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;视觉叙事系统在保持角色身份和连接动作到适当主体方面存在困难，这导致参照幻觉的发生。&lt;h4&gt;目的&lt;/h4&gt;提出StoryReasoning系统，以解决视觉叙事系统中的角色身份保持和动作连接问题。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含4,178个故事和52,016张电影图像的数据集，每个故事都保持了角色和物体的一致性，并通过结构化的表格表示来显式建模多帧关系。系统采用跨帧物体重新识别、思维链推理和固化方案将文本元素与视觉实体联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调Qwen2.5-VL 7B模型，创建了Qwen Storyteller，它在整个故事中保持了一致的物体引用。与未微调的模型相比，平均每个故事上的幻觉减少了4.06到3.56（-12.3%）。&lt;h4&gt;结论&lt;/h4&gt;StoryReasoning系统有效减少了视觉叙事系统中的参照幻觉，并通过结构化的表格表示和思维链推理提供了对故事的多帧关系建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/daniel3303/storyreasoning&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual storytelling systems struggle to maintain character identity acrossframes and link actions to appropriate subjects, frequently leading toreferential hallucinations. These issues can be addressed through grounding ofcharacters, objects, and other entities on the visual elements. We proposeStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movieimages, with both structured scene analyses and grounded stories. Each storymaintains character and object consistency across frames while explicitlymodeling multi-frame relationships through structured tabular representations.Our approach features cross-frame object re-identification using visualsimilarity and face recognition, chain-of-thought reasoning for explicitnarrative modeling, and a grounding scheme that links textual elements tovisual entities across multiple frames. We establish baseline performance byfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-endobject detection, re-identification, and landmark detection while maintainingconsistent object references throughout the story. Evaluation demonstrates areduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story whencompared to a non-fine-tuned model.</description>
      <author>example@mail.com (Daniel A. P. Oliveira, David Martins de Matos)</author>
      <guid isPermaLink="false">2505.10292v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data</title>
      <link>http://arxiv.org/abs/2505.10083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语言模型和时间序列基础模型的多模态框架，以解决传统预测方法在利用文本信息方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的预测方法依赖于单模态的时间序列数据，这限制了它们利用丰富文本信息的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决事件序列配对数据的稀缺性，提出了一种解耦框架，将语言模型和时序基础模型的优势结合起来。&lt;h4&gt;方法&lt;/h4&gt;使用语言模型将文本事件转换为修订指令，这些指令用于引导时序基础模型。引入了ChronoSteer，这是一种可以通过文本修订指令进行引导的多模态时序基础模型。此外，为了缓解跨模态指令序列配对数据的短缺，设计了一种基于合成数据的两阶段训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoSteer在仅使用合成数据进行训练的情况下，与单模态基线相比，预测精度提高了25.7%，比之前的最先进的多模态方法提高了22.5%。&lt;h4&gt;结论&lt;/h4&gt;该研究有效地解决了信息泄漏问题，并通过多模态模型实现了预测精度的显著提升。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a multimodal framework based on language models and time series foundation models to address the limitations of traditional forecasting methods in utilizing rich textual information.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional forecasting methods rely on unimodal time series data, limitingtheir ability to exploit rich textual information. Recently, large languagemodels (LLMs) and time series foundation models (TSFMs) have demonstratedpowerful capability in textual reasoning and temporal modeling, respectively.Integrating the strengths of both to construct a multimodal model thatconcurrently leverages both temporal and textual information for futureinference has emerged as a critical research challenge. To address the scarcityof event-series paired data, we propose a decoupled framework: an LLM isemployed to transform textual events into revision instructions, which are thenused to steer the output of TSFM. To implement this framework, we introduceChronoSteer, a multimodal TSFM that can be steered through textual revisioninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate theshortage of cross-modal instruction-series paired data, we devise a two-stagetraining strategy based on synthetic data. In addition, we also construct ahigh-quality multimodal time series forecasting benchmark to address theinformation leakage concerns during evaluation. After integrating with an LLM,ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%improvement in prediction accuracy compared to the unimodal backbone and a22.5% gain over the previous state-of-the-art multimodal method.</description>
      <author>example@mail.com (Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao)</author>
      <guid isPermaLink="false">2505.10083v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</title>
      <link>http://arxiv.org/abs/2505.10239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be presented at ICRA 2025 conference. Video:  https://youtu.be/qy7l_wGOyzo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在物理人机交互中，如何通过姿势数据预测人类运动意图，以实现非语言协作物理操作。&lt;h4&gt;背景&lt;/h4&gt;在物理人机交互中，力反馈是最常用的方式来传达人类意图给机器人，但在没有力反馈的情况下，如操作对象没有配备力传感器时，这种方法就不适用。&lt;h4&gt;目的&lt;/h4&gt;研究在摩擦表面上协同推动和拉动重物这一工业环境中常见的任务，通过姿势数据预测人类运动意图。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于有向图神经网络的上下文感知方法，用于分析时空人类姿势数据，以预测非语言协作物理操作的人类运动意图。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，机器人辅助可以显著减少人类努力并提高任务效率。结果表明，结合基于姿势的上下文识别，无论是与力感应结合还是作为力感应的替代方案，都能增强机器人的决策和控制效率。&lt;h4&gt;结论&lt;/h4&gt;基于姿势的上下文识别可以有效地辅助机器人决策和控制，提高协作物理操作的任务效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In physical human-robot interaction, force feedback has been the most commonsensing modality to convey the human intention to the robot. It is widely usedin admittance control to allow the human to direct the robot. However, itcannot be used in scenarios where direct force feedback is not available sincemanipulated objects are not always equipped with a force sensor. In this work,we study one such scenario: the collaborative pushing and pulling of heavyobjects on frictional surfaces, a prevalent task in industrial settings. Whenhumans do it, they communicate through verbal and non-verbal cues, where bodyposes, and movements often convey more than words. We propose a novelcontext-aware approach using Directed Graph Neural Networks to analyzespatio-temporal human posture data to predict human motion intention fornon-verbal collaborative physical manipulation. Our experiments demonstratethat robot assistance significantly reduces human effort and improves taskefficiency. The results indicate that incorporating posture-based contextrecognition, either together with or as an alternative to force sensing,enhances robot decision-making and control efficiency.</description>
      <author>example@mail.com (Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani)</author>
      <guid isPermaLink="false">2505.10239v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.09858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Early accept at MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的计算机辅助手术干预方法，通过合成手术视频来克服数据不平衡问题，提高手术视频数据集的性能。&lt;h4&gt;背景&lt;/h4&gt;手术视频数据集中存在严重的数据不平衡，这阻碍了高性能模型的开发。&lt;h4&gt;目的&lt;/h4&gt;目的是通过合成手术视频来克服手术视频数据集中的数据不平衡。&lt;h4&gt;方法&lt;/h4&gt;提出了一种独特的两阶段、文本条件扩散方法来生成高保真手术视频，用于代表性不足的类别。该方法通过2D潜在扩散模型捕获空间内容，并整合时间注意力层以确保时间一致性。此外，引入了拒绝采样策略来选择最合适的合成样本，有效增加现有数据集以解决类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在手术动作识别和术中事件预测两个下游任务上评估了该方法，结果表明，结合来自该方法合成视频的模型性能显著提高。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效提高手术视频数据集的性能，并通过开源实现促进了技术的普及。&lt;h4&gt;翻译&lt;/h4&gt;摘要：计算机辅助干预可以通过利用手术视频中的时空信息，特别是通过深度学习方法来改善术中引导。然而，手术视频数据集中普遍存在的严重数据不平衡阻碍了高性能模型的发展。在本研究中，我们旨在通过合成手术视频来克服数据不平衡。我们提出了一种独特的两阶段、文本条件扩散方法，用于生成高保真手术视频，以解决代表性不足的类别。我们的方法通过文本提示条件化生成过程，并利用2D潜在扩散模型来解耦空间和时间建模，以捕获空间内容，然后通过整合时间注意力层来确保时间一致性。此外，我们引入了拒绝采样策略来选择最合适的合成样本，有效地增加现有数据集以解决类别不平衡。我们在两个下游任务——手术动作识别和术中事件预测——上评估了我们的方法，表明结合我们的方法合成视频的模型性能显著提高。我们已在https://gitlab.com/nct_tso_public/surgvgen上开源了我们的实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-assisted interventions can improve intra-operative guidance,particularly through deep learning methods that harness the spatiotemporalinformation in surgical videos. However, the severe data imbalance often foundin surgical video datasets hinders the development of high-performing models.In this work, we aim to overcome the data imbalance by synthesizing surgicalvideos. We propose a unique two-stage, text-conditioned diffusion-based methodto generate high-fidelity surgical videos for under-represented classes. Ourapproach conditions the generation process on text prompts and decouplesspatial and temporal modeling by utilizing a 2D latent diffusion model tocapture spatial content and then integrating temporal attention layers toensure temporal consistency. Furthermore, we introduce a rejection samplingstrategy to select the most suitable synthetic samples, effectively augmentingexisting datasets to address class imbalance. We evaluate our method on twodownstream tasks-surgical action recognition and intra-operative eventprediction-demonstrating that incorporating synthetic videos from our approachsubstantially enhances model performance. We open-source our implementation athttps://gitlab.com/nct_tso_public/surgvgen.</description>
      <author>example@mail.com (Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel)</author>
      <guid isPermaLink="false">2505.09858v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2505.09971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为APCoTTA的连续测试时适应（CTTA）方法，专门用于解决空中激光扫描（ALS）点云分割中的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;在现实应用中，由于环境变化、传感器类型或传感器退化等因素导致的域偏移，通常会导致模型性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出APCoTTA旨在解决ALS点云分割中由于域偏移导致的模型性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;APCoTTA通过动态可训练层选择模块、基于熵的保持一致性损失和随机参数插值机制来提高模型的适应性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，APCoTTA在两个基准测试中实现了最佳性能，相对于直接推理，mIoU提高了约9%和14%。&lt;h4&gt;结论&lt;/h4&gt;APCoTTA为ALS点云分割提供了有效的连续测试时适应方法，有助于提高模型在域偏移情况下的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task for large-scale 3D scene understanding. In real-world applications, models are typically fixed after training. However, domain shifts caused by changes in the environment, sensor types, or sensor degradation often lead to a decline in model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by adapting a source-pretrained model to evolving, unlabeled target domains. Despite its potential, research on ALS point clouds remains limited, facing challenges such as the absence of standardized datasets and the risk of catastrophic forgetting and error accumulation during prolonged adaptation. To tackle these challenges, we propose APCoTTA, the first CTTA method tailored for ALS point cloud semantic segmentation. We propose a dynamic trainable layer selection module. This module utilizes gradient information to select low-confidence layers for training, and the remaining layers are kept frozen, mitigating catastrophic forgetting. To further reduce error accumulation, we propose an entropy-based consistency loss. By losing such samples based on entropy, we apply consistency loss only to the reliable samples, enhancing model stability. In addition, we propose a random parameter interpolation mechanism, which randomly blends parameters from the selected trainable layers with those of the source model. This approach helps balance target adaptation and source knowledge retention, further alleviating forgetting. Finally, we construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA benchmarks for ALS point cloud segmentation. Experimental results demonstrate that APCoTTA achieves the best performance on two benchmarks, with mIoU improvements of approximately 9% and 14% over direct inference. The new benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gaoyuan2/apcotta&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Airborne laser scanning (ALS) point cloud segmentation is a fundamental taskfor large-scale 3D scene understanding. In real-world applications, models aretypically fixed after training. However, domain shifts caused by changes in theenvironment, sensor types, or sensor degradation often lead to a decline inmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution byadapting a source-pretrained model to evolving, unlabeled target domains.Despite its potential, research on ALS point clouds remains limited, facingchallenges such as the absence of standardized datasets and the risk ofcatastrophic forgetting and error accumulation during prolonged adaptation. Totackle these challenges, we propose APCoTTA, the first CTTA method tailored forALS point cloud semantic segmentation. We propose a dynamic trainable layerselection module. This module utilizes gradient information to selectlow-confidence layers for training, and the remaining layers are kept frozen,mitigating catastrophic forgetting. To further reduce error accumulation, wepropose an entropy-based consistency loss. By losing such samples based onentropy, we apply consistency loss only to the reliable samples, enhancingmodel stability. In addition, we propose a random parameter interpolationmechanism, which randomly blends parameters from the selected trainable layerswith those of the source model. This approach helps balance target adaptationand source knowledge retention, further alleviating forgetting. Finally, weconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTAbenchmarks for ALS point cloud segmentation. Experimental results demonstratethat APCoTTA achieves the best performance on two benchmarks, with mIoUimprovements of approximately 9% and 14% over direct inference. The newbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.</description>
      <author>example@mail.com (Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang)</author>
      <guid isPermaLink="false">2505.09971v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2505.10040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Instance-Prototype Affinity Learning (IPAL)的Graph Neural Networks (GNN)方法，用于解决非示例连续图学习中的灾难性遗忘问题，并通过实验证明了其有效性。&lt;h4&gt;背景&lt;/h4&gt;GNN在整合新信息时容易发生灾难性遗忘，影响其保存先前知识的能力。传统的重排和原型重放等技术存在内存爆炸和隐私侵犯等问题。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种新的方法来缓解GNN中的灾难性遗忘问题，同时提高模型对新知识的吸收能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Instance-Prototype Affinity Learning (IPAL)的方法，利用图结构信息，并采用拓扑集成高斯原型（TIGP）和实例-原型亲和度蒸馏（IPAD）等技术来增强模型的学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验发现，与传统的原型重放相比，原型对比学习（PCL）表现出更少的漂移现象。IPAL在四个节点分类基准数据集上的评估结果表明，该方法在塑性和稳定性之间取得了更好的平衡，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;IPAL方法能够有效地缓解GNN中的灾难性遗忘问题，并提高了模型在新知识吸收方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Graph Neural Networks (GNN) suffer from catastrophic forgetting, which undermines their ability to preserve previously acquired knowledge when assimilating new information. Rehearsal-based techniques, such as historical example revisiting, are adopted as a principal strategy to alleviate this phenomenon. However, memory explosion and privacy infringements impose significant constraints on their utility. Non-Exemplar methods, such as Prototype Replay (PR), circumvent the prior issues, yet feature drift presents new challenges. In this paper, our empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits less pronounced drift than conventional PR. Drawing upon PCL, we propose Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar Continual Graph Learning (NECGL). Exploiting graph structural information, we formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature distributions towards high-impact nodes to augment the model's capacity for assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD) safeguards task memory by regularizing discontinuities in class relationships. Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL, fostering greater inter-class discriminability. Evaluations on four node classification benchmark datasets demonstrate that our method outperforms existing state-of-the-art methods, achieving a better trade-off between plasticity and stability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) endure catastrophic forgetting, undermining theircapacity to preserve previously acquired knowledge amid the assimilation ofnovel information. Rehearsal-based techniques revisit historical examples,adopted as a principal strategy to alleviate this phenomenon. However, memoryexplosion and privacy infringements impose significant constraints on theirutility. Non-Exemplar methods circumvent the prior issues through PrototypeReplay (PR), yet feature drift presents new challenges. In this paper, ourempirical findings reveal that Prototype Contrastive Learning (PCL) exhibitsless pronounced drift than conventional PR. Drawing upon PCL, we proposeInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-ExemplarContinual Graph Learning (NECGL). Exploiting graph structural information, weformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding featuredistributions towards high-impact nodes to augment the model's capacity forassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)safeguards task memory by regularizing discontinuities in class relationships.Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,fostering greater inter-class discriminability. Evaluations on four nodeclassification benchmark datasets demonstrate that our method outperformsexisting state-of-the-art methods, achieving a better trade-off betweenplasticity and stability.</description>
      <author>example@mail.com (Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong)</author>
      <guid isPermaLink="false">2505.10040v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Vision Tokenizer Tuning</title>
      <link>http://arxiv.org/abs/2505.10562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ETT，一种端到端的视觉标记器调整方法，用于优化视觉标记器和目标自回归任务之间的联合优化。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉标记器优化与下游训练任务分离，假设视觉标记器在各种任务中具有良好的泛化能力，但对于需要不同表示和语义的任务，这种优化方法是不适用的。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉标记器优化方法导致的表示瓶颈问题，即视觉标记器的损失可能成为目标任务的瓶颈。&lt;h4&gt;方法&lt;/h4&gt;ETT利用标记器代码簿的视觉嵌入，以重建和标题目标进行端到端的视觉标记器优化，并能够无缝集成到现有的训练流程中，无需调整大型语言模型的原始代码簿或架构。&lt;h4&gt;主要发现&lt;/h4&gt;ETT实现了显著的性能提升，对于多模态理解和视觉生成任务，与冻结标记器的基线相比，性能提升了2-6%，同时保持了原始的重建能力。&lt;h4&gt;结论&lt;/h4&gt;ETT是一种简单且有效的方法，能够增强多模态基础模型，不仅在图像生成和理解方面，还包括其他领域。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes ETT, an end-to-end vision tokenizer tuning approach that optimizes the joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing vision tokenization isolates the optimization of vision tokenizersfrom downstream training, implicitly assuming the visual tokens can generalizewell across various tasks, e.g., image generation and visual questionanswering. The vision tokenizer optimized for low-level reconstruction isagnostic to downstream tasks requiring varied representations and semantics.This decoupled paradigm introduces a critical misalignment: The loss of thevision tokenization can be the representation bottleneck for target tasks. Forexample, errors in tokenizing text in a given image lead to poor results whenrecognizing or generating them. To address this, we propose ETT, an end-to-endvision tokenizer tuning approach that enables joint optimization between visiontokenization and target autoregressive tasks. Unlike prior autoregressivemodels that use only discrete indices from a frozen vision tokenizer, ETTleverages the visual embeddings of the tokenizer codebook, and optimizes thevision tokenizers end-to-end with both reconstruction and caption objectives.ETT can be seamlessly integrated into existing training pipelines with minimalarchitecture modifications. Our ETT is simple to implement and integrate,without the need to adjust the original codebooks or architectures of theemployed large language models. Extensive experiments demonstrate that ourproposed end-to-end vision tokenizer tuning unlocks significant performancegains, i.e., 2-6% for multimodal understanding and visual generation taskscompared to frozen tokenizer baselines, while preserving the originalreconstruction capability. We hope this very simple and strong method canempower multimodal foundation models besides image generation andunderstanding.</description>
      <author>example@mail.com (Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang)</author>
      <guid isPermaLink="false">2505.10562v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</title>
      <link>http://arxiv.org/abs/2505.10205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VolE的新型框架，用于通过移动设备驱动的3D重建来估计食物体积，以提高医疗营养管理和健康监测应用中的食物体积估计准确性。&lt;h4&gt;背景&lt;/h4&gt;目前的食物体积估计方法通常受到单核数据、专用硬件（如3D扫描仪）或依赖参考物体进行相机校准的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需参考物体和深度信息的食物体积估计框架，以提高估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;VolE利用AR功能的移动设备捕捉图像和相机位置，生成精确的3D模型。此外，通过食物视频分割来生成食物掩码，实现现实世界的测量。&lt;h4&gt;主要发现&lt;/h4&gt;VolE在多个数据集上优于现有的体积估计技术，实现了2.22%的MAPE，证明了其在食物体积估计方面的优越性能。&lt;h4&gt;结论&lt;/h4&gt;VolE框架为食物体积估计提供了一种高效且准确的方法，对于医疗营养管理和健康监测应用具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate food volume estimation is crucial for medical nutrition managementand health monitoring applications, but current food volume estimation methodsare often limited by mononuclear data, leveraging single-purpose hardware suchas 3D scanners, gathering sensor-oriented information such as depthinformation, or relying on camera calibration using a reference object. In thispaper, we present VolE, a novel framework that leverages mobile device-driven3D reconstruction to estimate food volume. VolE captures images and cameralocations in free motion to generate precise 3D models, thanks to AR-capablemobile devices. To achieve real-world measurement, VolE is a reference- anddepth-free framework that leverages food video segmentation for food maskgeneration. We also introduce a new food dataset encompassing the challengingscenarios absent in the previous benchmarks. Our experiments demonstrate thatVolE outperforms the existing volume estimation techniques across multipledatasets by achieving 2.22 % MAPE, highlighting its superior performance infood volume estimation.</description>
      <author>example@mail.com (Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva)</author>
      <guid isPermaLink="false">2505.10205v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2505.10547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://milanganai.github.io/fortress/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为FORTRESS的框架，用于在机器人遇到超出训练数据范围的情况时，实时生成和推理语义安全的回退策略，以防止超出分布（OOD）的故障。&lt;h4&gt;背景&lt;/h4&gt;由于大型视觉和语言模型的推理延迟较高，当前方法依赖于手动定义的干预策略来执行回退，因此缺乏规划通用、语义安全运动的能力。&lt;h4&gt;目的&lt;/h4&gt;克服上述挑战，提出FORTRESS框架，以在运行时实时生成和推理语义安全的回退策略。&lt;h4&gt;方法&lt;/h4&gt;FORTRESS在正常操作中低频使用多模态推理器来识别目标和预测故障模式。当运行时监控器触发回退响应时，FORTRESS快速合成回退到目标状态的计划，同时实时推理并避免语义不安全的区域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将开放世界、多模态推理与动态感知规划相结合，FORTRESS消除了硬编码回退和人工安全干预的需求。在合成基准和真实世界ANYmal机器人数据上的安全分类准确率方面，FORTRESS优于对慢速推理模型的即时提示，并在模拟和四旋翼硬件城市导航中的系统安全性和规划成功率方面进一步得到提升。&lt;h4&gt;结论&lt;/h4&gt;FORTRESS框架能够有效提高机器人在未知环境下的安全性和鲁棒性，为未来机器人安全导航提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models can provide robust high-level reasoning on appropriatesafety interventions in hazardous scenarios beyond a robot's training data,i.e. out-of-distribution (OOD) failures. However, due to the high inferencelatency of Large Vision and Language Models, current methods rely on manuallydefined intervention policies to enact fallbacks, thereby lacking the abilityto plan generalizable, semantically safe motions. To overcome these challengeswe present FORTRESS, a framework that generates and reasons about semanticallysafe fallback strategies in real time to prevent OOD failures. At a lowfrequency in nominal operations, FORTRESS uses multi-modal reasoners toidentify goals and anticipate failure modes. When a runtime monitor triggers afallback response, FORTRESS rapidly synthesizes plans to fallback goals whileinferring and avoiding semantically unsafe regions in real time. By bridgingopen-world, multi-modal reasoning with dynamics-aware planning, we eliminatethe need for hard-coded fallbacks and human safety interventions. FORTRESSoutperforms on-the-fly prompting of slow reasoning models in safetyclassification accuracy on synthetic benchmarks and real-world ANYmal robotdata, and further improves system safety and planning success in simulation andon quadrotor hardware for urban navigation.</description>
      <author>example@mail.com (Milan Ganai, Rohan Sinha, Christopher Agia, Daniel Morton, Marco Pavone)</author>
      <guid isPermaLink="false">2505.10547v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability</title>
      <link>http://arxiv.org/abs/2505.10351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).  We show the impacts of scaling from both data and model aspects on membership  inference for self-supervised visual encoders&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了视觉自监督模型在现实环境下的成员推断问题，提出了统一的方法PartCrop，并通过实验验证了其有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在处理大量未标记数据方面显示出潜力，但在视觉领域也面临着隐私问题。&lt;h4&gt;目的&lt;/h4&gt;在自监督训练方法和细节未知的情况下，对视觉自监督模型进行成员推断。&lt;h4&gt;方法&lt;/h4&gt;提出了PartCrop方法，通过裁剪图像中的部分对象，在表示空间中查询图像内的响应。同时，对不同的训练协议和结构进行了广泛攻击，并评估了防御方法。&lt;h4&gt;主要发现&lt;/h4&gt;PartCrop方法在多种自监督模型上验证了其有效性和泛化能力。防御实验表明，早期停止、差分隐私和缩小裁剪尺度范围等方法是有效的。此外，通过引入结构改进，提出了可扩展的PartCrop-v2。&lt;h4&gt;结论&lt;/h4&gt;PartCrop是一种有效的成员推断方法，可以用于对抗自监督模型，并通过结构改进提高了其可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自监督学习在利用大量未标记数据方面显示出潜力，但也面临着显著的隐私问题，尤其是在视觉领域。在本文中，我们在一个更现实的设置中对视觉自监督模型进行了成员推断：当攻击者以通常实践中遇到的黑盒系统面对时，他不知道自监督训练方法和细节。在这种设置中，考虑到自监督模型可能由完全不同的自监督范例（例如，掩码图像建模和对比学习）以及复杂的训练细节进行训练，我们提出了一种称为PartCrop的统一成员推断方法。它是由模型之间的共享部分感知能力以及训练数据上更强的部分响应所启发。具体来说，PartCrop通过在表示空间中裁剪图像中的部分对象来查询图像内的响应。我们使用三个广泛使用的图像数据集对具有不同训练协议和结构的自监督模型进行了广泛的攻击。结果表明，PartCrop的有效性和泛化能力。此外，为了防御PartCrop，我们评估了两种常见的方法，即早期停止和差分隐私，并提出了一个名为缩小裁剪尺度范围的自定义方法。防御实验表明，它们都是有效的。最后，除了在玩具视觉编码器和小型图像数据集上进行原型测试外，我们还从数据和模型方面对现实场景中缩放的影响进行了定量研究，并通过引入两个结构改进提出了可扩展的PartCrop-v2。我们的代码可在https://github.com/JiePKU/PartCrop上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jiepku/partcrop&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning shows promise in harnessing extensive unlabeleddata, but it also confronts significant privacy concerns, especially in vision.In this paper, we perform membership inference on visual self-supervised modelsin a more realistic setting: self-supervised training method and details areunknown for an adversary when attacking as he usually faces a black-box systemin practice. In this setting, considering that self-supervised model could betrained by completely different self-supervised paradigms, e.g., masked imagemodeling and contrastive learning, with complex training details, we propose aunified membership inference method called PartCrop. It is motivated by theshared part-aware capability among models and stronger part response on thetraining data. Specifically, PartCrop crops parts of objects in an image toquery responses within the image in representation space. We conduct extensiveattacks on self-supervised models with different training protocols andstructures using three widely used image datasets. The results verify theeffectiveness and generalization of PartCrop. Moreover, to defend againstPartCrop, we evaluate two common approaches, i.e., early stop and differentialprivacy, and propose a tailored method called shrinking crop scale range. Thedefense experiments indicate that all of them are effective. Finally, besidesprototype testing on toy visual encoders and small-scale image datasets, wequantitatively study the impacts of scaling from both data and model aspects ina realistic scenario and propose a scalable PartCrop-v2 by introducing twostructural improvements to PartCrop. Our code is athttps://github.com/JiePKU/PartCrop.</description>
      <author>example@mail.com (Jie Zhu, Jirong Zha, Ding Li, Leye Wang)</author>
      <guid isPermaLink="false">2505.10351v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>An AI-driven framework for the prediction of personalised health response to air pollution</title>
      <link>http://arxiv.org/abs/2505.10556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Kermani and Naderi share first authorship. 20 pages, 6 figures and 1  table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过整合可穿戴健身设备的生理数据和实时环境暴露，预测个人对污染的健康反应。&lt;h4&gt;背景&lt;/h4&gt;空气污染对公共健康构成重大威胁，会加剧许多呼吸和心血管疾病。气候变化导致极端天气事件增多，如野火和热浪，这些事件会增加污染水平并加剧污染的影响。&lt;h4&gt;目的&lt;/h4&gt;利用个人传感器的最新进展和人工智能的时间序列预测能力，监测和预测个人的健康结果。&lt;h4&gt;方法&lt;/h4&gt;通过安全且道德的方式收集数据，训练一个基于云的模块化框架中的AI模型，预测个人对污染暴露的健康反应。&lt;h4&gt;主要发现&lt;/h4&gt;AI模型（在这种情况下是一个对抗性自编码器神经网络）能够准确重构时间依赖的健康信号，并捕捉对污染的非线性反应。通过使用个人智能手表的数据进行迁移学习，增加了AI模型的一般化能力，并展示了该方法对现实世界用户生成数据的适应性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过整合生理数据和实时环境暴露，有效地预测了个人对污染的健康反应，为改善个人健康提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Air pollution poses a significant threat to public health, causing orexacerbating many respiratory and cardiovascular diseases. In addition, climatechange is bringing about more extreme weather events such as wildfires andheatwaves, which can increase levels of pollution and worsen the effects ofpollution exposure. Recent advances in personal sensing have transformed thecollection of behavioural and physiological data, leading to the potential fornew improvements in healthcare. We wish to capitalise on this data, alongsidenew capabilities in AI for making time series predictions, in order to monitorand predict health outcomes for an individual. Thus, we present a novelworkflow for predicting personalised health responses to pollution byintegrating physiological data from wearable fitness devices with real-timeenvironmental exposures. The data is collected from various sources in a secureand ethical manner, and is used to train an AI model to predict individualhealth responses to pollution exposure within a cloud-based, modular framework.We demonstrate that the AI model -- an Adversarial Autoencoder neural networkin this case -- accurately reconstructs time-dependent health signals andcaptures nonlinear responses to pollution. Transfer learning is applied usingdata from a personal smartwatch, which increases the generalisation abilitiesof the AI model and illustrates the adaptability of the approach to real-world,user-generated data.</description>
      <author>example@mail.com (Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain)</author>
      <guid isPermaLink="false">2505.10556v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.10105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了EmbodiedMAE，一种用于机器人操作的三维多模态表示方法。&lt;h4&gt;背景&lt;/h4&gt;现有方法在训练数据集和机器人操作任务之间存在显著的领域差距，且缺乏能够有效融合三维信息的模型架构。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些局限性，通过增强DROID数据集，构建DROID-3D作为三维具身视觉研究的有价值补充。&lt;h4&gt;方法&lt;/h4&gt;开发了一种多模态掩码自动编码器EmbodiedMAE，通过随机掩码和跨模态融合，同时学习RGB、深度和点云模态的表示。在DROID-3D上训练，EmbodiedMAE在70个模拟任务和20个真实世界机器人操作任务中，无论是在训练效率还是最终性能上，都优于最先进的视觉基础模型（VFMs）。&lt;h4&gt;主要发现&lt;/h4&gt;EmbodiedMAE在规模和从3D输入中进行有效策略学习方面表现出强大的扩展行为。实验结果确立了EmbodiedMAE作为可靠的三维多模态VFM，尤其在空间感知至关重要的精确桌面操作环境中。&lt;h4&gt;结论&lt;/h4&gt;EmbodiedMAE是具身AI系统中可靠的三维多模态VFM，特别适用于需要精确空间感知的桌面操作场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present EmbodiedMAE, a unified 3D multi-modal representation for robotmanipulation. Current approaches suffer from significant domain gaps betweentraining datasets and robot manipulation tasks, while also lacking modelarchitectures that can effectively incorporate 3D information. To overcomethese limitations, we enhance the DROID dataset with high-quality depth mapsand point clouds, constructing DROID-3D as a valuable supplement for 3Dembodied vision research. Then we develop EmbodiedMAE, a multi-modal maskedautoencoder that simultaneously learns representations across RGB, depth, andpoint cloud modalities through stochastic masking and cross-modal fusion.Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-artvision foundation models (VFMs) in both training efficiency and finalperformance across 70 simulation tasks and 20 real-world robot manipulationtasks on two robot platforms. The model exhibits strong scaling behavior withsize and promotes effective policy learning from 3D inputs. Experimentalresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM forembodied AI systems, particularly in precise tabletop manipulation settingswhere spatial perception is critical.</description>
      <author>example@mail.com (Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao)</author>
      <guid isPermaLink="false">2505.10105v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Correlating Account on Ethereum Mixing Service via Domain-Invariant feature learning</title>
      <link>http://arxiv.org/abs/2505.09892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Cryptocurrency, Ethereum, mixing services, GNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为StealthLink的新框架，用于解决Ethereum混合服务如Tornado Cash的交易不可追踪性对区块链安全和金融监管带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有方法在关联混合账户方面受到限制，因为它们依赖于有限的标记数据和容易受到噪声注释的影响。&lt;h4&gt;目的&lt;/h4&gt;提出StealthLink框架，通过跨任务域不变特征学习解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个MixFusion模块，该模块构建和编码混合子图以捕捉局部交易模式，并引入了一种知识迁移机制，通过对抗差异最小化来对齐不同领域的判别性特征。&lt;h4&gt;主要发现&lt;/h4&gt;StealthLink在真实世界的混合交易数据集上取得了最先进的性能，10次学习场景下的F1分数为96.98%，在处理不平衡数据条件下表现优于传统监督方法。&lt;h4&gt;结论&lt;/h4&gt;该研究建立了区块链取证中跨域知识迁移的第一个系统方法，为在去中心化生态系统中对抗增强隐私的金融犯罪提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：以太坊混合服务如Tornado Cash所促成的交易不可追踪性对区块链安全和金融监管构成了重大挑战。现有关联混合账户的方法由于有限的标记数据和易受噪声注释的影响而受到限制。在本文中，我们提出了StealthLink，一个通过跨任务域不变特征学习解决这些局限性的新颖框架。我们的关键创新在于从区块链异常检测领域迁移知识到数据稀缺的混合交易追踪任务。具体来说，我们设计了一个MixFusion模块，该模块构建和编码混合子图以捕捉局部交易模式，同时引入了一种通过对抗差异最小化在不同领域对齐判别性特征的知识迁移机制。这种双重方法在标签稀缺和分布偏移的情况下实现了鲁棒的特征学习。在真实世界混合交易数据集上的广泛实验表明，StealthLink在10次学习场景中实现了最先进的性能，F1分数达到96.98%。值得注意的是，我们的框架在处理不平衡数据条件下比传统监督方法表现出更优越的泛化能力。这项工作建立了区块链取证中跨域知识迁移的第一个系统方法，为在去中心化生态系统中对抗增强隐私的金融犯罪提供了一个实用解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The untraceability of transactions facilitated by Ethereum mixing serviceslike Tornado Cash poses significant challenges to blockchain security andfinancial regulation. Existing methods for correlating mixing accounts sufferfrom limited labeled data and vulnerability to noisy annotations, whichrestrict their practical applicability. In this paper, we propose StealthLink,a novel framework that addresses these limitations through cross-taskdomain-invariant feature learning. Our key innovation lies in transferringknowledge from the well-studied domain of blockchain anomaly detection to thedata-scarce task of mixing transaction tracing. Specifically, we design aMixFusion module that constructs and encodes mixing subgraphs to capture localtransactional patterns, while introducing a knowledge transfer mechanism thataligns discriminative features across domains through adversarial discrepancyminimization. This dual approach enables robust feature learning under labelscarcity and distribution shifts. Extensive experiments on real-world mixingtransaction datasets demonstrate that StealthLink achieves state-of-the-artperformance, with 96.98\% F1-score in 10-shot learning scenarios. Notably, ourframework shows superior generalization capability in imbalanced dataconditions than conventional supervised methods. This work establishes thefirst systematic approach for cross-domain knowledge transfer in blockchainforensics, providing a practical solution for combating privacy-enhancedfinancial crimes in decentralized ecosystems.</description>
      <author>example@mail.com (Zheng Che, Taoyu Li, Meng Shen, Hanbiao Du, Liehuang Zhu)</author>
      <guid isPermaLink="false">2505.09892v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Negative Metric Learning for Graphs</title>
      <link>http://arxiv.org/abs/2505.10307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的负指标学习（NML）增强的图对比学习（NML-GCL）方法，旨在解决图对比学习中的假阴性问题，并提高下游任务的性能。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）常存在假阴性问题，这会降低下游任务的性能。现有解决假阴性问题的方法通常依赖人类先验知识，导致GCL的结果仍然不理想。&lt;h4&gt;目的&lt;/h4&gt;提出NML-GCL方法，以解决GCL中的假阴性问题，并提高其在下游任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;NML-GCL使用一个可学习的负指标网络（NMN）来构建一个负指标空间，在这个空间中，基于锚节点距离，可以更好地区分假阴性样本和真实负样本。同时，提出了一种联合训练方案，结合双层次优化目标，利用自监督信号迭代优化编码器和负指标网络。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和广泛使用的基准测试的实验，验证了所提出方法的优势。&lt;h4&gt;结论&lt;/h4&gt;NML-GCL方法能够有效解决GCL中的假阴性问题，并显著提高下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) often suffers from false negatives, which degrades the performance on downstream tasks. The existing methods addressing the false negative issue usually rely on human prior knowledge, still leading GCL to suboptimal results. In this paper, we propose a novel Negative Metric Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative Metric Network (NMN) to build a negative metric space, in which false negatives can be distinguished better from true negatives based on their distance to anchor node. To overcome the lack of explicit supervision signals for NML, we propose a joint training scheme with bi-level optimization objective, which implicitly utilizes the self-supervision signals to iteratively optimize the encoder and the negative metric network. The solid theoretical analysis and the extensive experiments conducted on widely used benchmarks verify the superiority of the proposed method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) often suffers from false negatives, whichdegrades the performance on downstream tasks. The existing methods addressingthe false negative issue usually rely on human prior knowledge, still leadingGCL to suboptimal results. In this paper, we propose a novel Negative MetricLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable NegativeMetric Network (NMN) to build a negative metric space, in which false negativescan be distinguished better from true negatives based on their distance toanchor node. To overcome the lack of explicit supervision signals for NML, wepropose a joint training scheme with bi-level optimization objective, whichimplicitly utilizes the self-supervision signals to iteratively optimize theencoder and the negative metric network. The solid theoretical analysis and theextensive experiments conducted on widely used benchmarks verify thesuperiority of the proposed method.</description>
      <author>example@mail.com (Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang)</author>
      <guid isPermaLink="false">2505.10307v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms</title>
      <link>http://arxiv.org/abs/2505.09103v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合空间加权方法和新颖点描述直方图的雷达惯性里程计，用于在恶劣条件下实现自主定位。该方法有效处理稀疏和噪声的雷达测量数据，并通过加权计算模型充分利用多普勒速度，同时结合局部几何特征和雷达散射截面（RCS）特征来提高点云注册性能。&lt;h4&gt;背景&lt;/h4&gt;4D雷达惯性里程计在恶劣条件下的自主定位具有潜力，但处理稀疏和噪声的雷达测量数据仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理稀疏和噪声雷达测量数据的雷达惯性里程计方法，提高点云注册性能。&lt;h4&gt;方法&lt;/h4&gt;1. 采用空间加权方法适应点分布不均；2. 提出新颖的点描述直方图用于困难点注册；3. 提出加权计算模型以充分利用多普勒速度；4. 构建结合局部几何特征和RCS特征的点直方图描述符。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验证明了所提出的VGC-RIO方法在公共和自建数据集上的精度和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的VGC-RIO方法在处理稀疏和噪声的雷达测量数据及提高点云注册性能方面表现出良好的效果。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in 4D radar-inertial odometry have demonstrated promising potential for autonomous localization in adverse conditions. However, effective handling of sparse and noisy radar measurements remains a critical challenge. In this paper, we propose a radar-inertial odometry with a spatial weighting method that adapts to unevenly distributed points and a novel point-description histogram for challenging point registration. To make full use of the Doppler velocity from different spatial sections, we propose a weighting calculation model. To enhance the point cloud registration performance under challenging scenarios, we construct a novel point histogram descriptor that combines local geometric features and radar cross-section (RCS) features. We have also conducted extensive experiments on both public and self-constructed datasets. The results demonstrate the precision and robustness of the proposed VGC-RIO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D radar-inertial odometry have demonstrated promisingpotential for autonomous lo calization in adverse conditions. However,effective handling of sparse and noisy radar measurements remains a criticalchallenge. In this paper, we propose a radar-inertial odometry with a spatialweighting method that adapts to unevenly distributed points and a novelpoint-description histogram for challenging point registration. To make fulluse of the Doppler velocity from different spatial sections, we propose aweighting calculation model. To enhance the point cloud registrationperformance under challenging scenarios, we con struct a novel point histogramdescriptor that combines local geometric features and radar cross-section (RCS)features. We have also conducted extensive experiments on both public andself-constructed datasets. The results demonstrate the precision and robustnessof the proposed VGC-RIO.</description>
      <author>example@mail.com (Jianguang Xiang, Xiaofeng He, Zizhuo Chen, Lilian Zhang, Xincan Luo, Jun Mao)</author>
      <guid isPermaLink="false">2505.09103v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A Representation Learning Approach to Feature Drift Detection in Wireless Networks</title>
      <link>http://arxiv.org/abs/2505.10325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALERT的方法，用于检测特征分布变化并触发模型重新训练，以应对AI模型在无线网络部署中的性能退化问题。&lt;h4&gt;背景&lt;/h4&gt;AI在下一代无线网络中将扮演核心角色，实现无处不在的通信和新服务。然而，在实际部署中，特征分布的变化可能会降低AI模型性能并导致不良行为。&lt;h4&gt;目的&lt;/h4&gt;为了应对未检测到的模型退化，提出了ALERT方法，旨在检测特征分布变化并触发模型重新训练。&lt;h4&gt;方法&lt;/h4&gt;ALERT包括三个组件：表示学习、统计测试和效用评估。表示学习部分采用MLP设计，统计测试部分使用Kolmogorov-Smirnov和Population Stability Index测试，效用评估部分采用新函数。&lt;h4&gt;主要发现&lt;/h4&gt;在两个无线网络用例（无线指纹识别和链路异常检测）中，ALERT方法优于文献中的十种标准漂移检测方法。&lt;h4&gt;结论&lt;/h4&gt;ALERT方法在应对无线网络中的特征分布变化和模型性能退化方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI is foreseen to be a centerpiece in next generation wireless networksenabling enabling ubiquitous communication as well as new services. However, inreal deployment, feature distribution changes may degrade the performance of AImodels and lead to undesired behaviors. To counter for undetected modeldegradation, we propose ALERT; a method that can detect feature distributionchanges and trigger model re-training that works well on two wireless networkuse cases: wireless fingerprinting and link anomaly detection. ALERT includesthree components: representation learning, statistical testing and utilityassessment. We rely on MLP for designing the representation learning component,on Kolmogorov-Smirnov and Population Stability Index tests for designing thestatistical testing and a new function for utility assessment. We show thesuperiority of the proposed method against ten standard drift detection methodsavailable in the literature on two wireless network use cases.</description>
      <author>example@mail.com (Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna)</author>
      <guid isPermaLink="false">2505.10325v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks</title>
      <link>http://arxiv.org/abs/2505.10134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages,16 figures.This work has been submitted to the IEEE for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于基础模型的无线定位解决方案，旨在解决现有数据驱动模型在需要大量标注数据且难以泛化到不同部署场景和无线配置中的问题。&lt;h4&gt;背景&lt;/h4&gt;精确且鲁棒的定位对于5G和6G应用至关重要，如自动驾驶、扩展现实和智能制造。尽管数据驱动方法显示出潜力，但大多数现有模型需要大量标注数据，且难以泛化。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于无线定位的基础模型解决方案，以解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;首先，基于信息瓶颈理论分析不同自监督学习任务如何获取通用和特定任务的语义特征。在此基础上，设计了预训练方法，提出了大型无线定位模型（LWLM）。具体来说，提出了一种自监督学习框架，联合优化三个互补目标：空间频率掩码信道建模（SF-MCM）、域变换不变性（DTI）和位置不变对比学习（PICL）。此外，还设计了轻量级解码器，用于关键下游任务，包括到达时间（ToA）估计、到达角度（AoA）估计、单基站（BS）定位和多基站定位。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LWLM在所有定位任务中均优于基于模型和监督学习的基线。特别是，LWLM在未进行预训练的Transformer模型上实现了26.0%--87.5%的改进，并在标签有限的微调和未见过的BS配置下表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;LWLM作为一种基础模型，在无线定位方面具有巨大潜力，能够有效解决现有模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/guangjinpan/lwlm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust localization is a critical enabler for emerging 5G and 6Gapplications, including autonomous driving, extended reality (XR), and smartmanufacturing. While data-driven approaches have shown promise, most existingmodels require large amounts of labeled data and struggle to generalize acrossdeployment scenarios and wireless configurations. To address these limitations,we propose a foundation-model-based solution tailored for wirelesslocalization. We first analyze how different self-supervised learning (SSL)tasks acquire general-purpose and task-specific semantic features based oninformation bottleneck (IB) theory. Building on this foundation, we design apretraining methodology for the proposed Large Wireless Localization Model(LWLM). Specifically, we propose an SSL framework that jointly optimizes threecomplementary objectives: (i) spatial-frequency masked channel modeling(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)position-invariant contrastive learning (PICL). These objectives jointlycapture the underlying semantics of wireless channel from multipleperspectives. We further design lightweight decoders for key downstream tasks,including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,single base station (BS) localization, and multiple BS localization.Comprehensive experimental results confirm that LWLM consistently surpassesboth model-based and supervised learning baselines across all localizationtasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformermodels without pretraining, and exhibits strong generalization underlabel-limited fine-tuning and unseen BS configurations, confirming itspotential as a foundation model for wireless localization.</description>
      <author>example@mail.com (Guangjin Pan, Kaixuan Huang, Hui Chen, Shunqing Zhang, Christian Häger, Henk Wymeersch)</author>
      <guid isPermaLink="false">2505.10134v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation</title>
      <link>http://arxiv.org/abs/2505.10413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LongRefiner的插件式精炼器，用于解决现实世界RAG应用中长文本输入场景下的冗余信息和噪声问题，提高了推理效率和性能。&lt;h4&gt;背景&lt;/h4&gt;现实世界的RAG应用常常面临长文本输入的情况，这会导致更高的推理成本和性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出LongRefiner以解决长文本输入带来的挑战，提高RAG应用的性能。&lt;h4&gt;方法&lt;/h4&gt;LongRefiner通过利用长文档的结构特性，采用双层查询分析、层次化文档结构和基于单一基础模型的多任务学习进行自适应精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在七个QAdatasets上的实验表明，LongRefiner在各种场景下都达到了有竞争力的性能，同时相比最佳基线，其计算成本和延迟降低了10倍。&lt;h4&gt;结论&lt;/h4&gt;LongRefiner具有可扩展性、效率和有效性，为现实世界的长文本RAG应用提供了实用的见解。&lt;h4&gt;翻译&lt;/h4&gt;In real-world RAG applications, LongRefiner, an efficient plug-and-play refiner, is proposed to address the challenges of long-context input scenarios with redundant information and noise, improving inference efficiency and performance. The method utilizes the inherent structural characteristics of long documents, employing dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QAdatasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ignorejjj/longrefiner&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world RAG applications often encounter long-context input scenarios,where redundant information and noise results in higher inference costs andreduced performance. To address these challenges, we propose LongRefiner, anefficient plug-and-play refiner that leverages the inherent structuralcharacteristics of long documents. LongRefiner employs dual-level queryanalysis, hierarchical document structuring, and adaptive refinement throughmulti-task learning on a single foundation model. Experiments on seven QAdatasets demonstrate that LongRefiner achieves competitive performance invarious scenarios while using 10x fewer computational costs and latencycompared to the best baseline. Further analysis validates that LongRefiner isscalable, efficient, and effective, providing practical insights for real-worldlong-text RAG applications. Our code is available athttps://github.com/ignorejjj/LongRefiner.</description>
      <author>example@mail.com (Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou)</author>
      <guid isPermaLink="false">2505.10413v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Logos as a Well-Tempered Pre-train for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2505.10481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了孤立手语识别（ISLR）任务的两个方面，并提出了一种新的俄罗斯手语（RSL）数据集Logos，用于解决数据量和标签模糊性问题。&lt;h4&gt;背景&lt;/h4&gt;尽管存在多个数据集，但大多数孤立手语的数据量有限，这给跨语言ISLR模型训练和迁移学习带来了挑战。同时，相似的手势可能具有不同的语义意义，导致数据集标签模糊。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，本文提出了Logos数据集，旨在提高手语识别模型的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;本研究提出了Logos数据集，该数据集是RSL数据集中最大且词汇量最丰富的数据集。通过在Logos数据集上预训练模型，并将其作为通用编码器应用于其他语言的手语识别任务，包括少样本学习。此外，还探索了跨语言迁移学习方法，并发现使用多个分类头进行联合训练有助于提高低资源数据集的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;Logos数据集的关键特征是明确标注了视觉上相似的手势组。研究表明，明确标注视觉上相似的手势可以提升训练模型作为视觉编码器在下游任务中的质量。&lt;h4&gt;结论&lt;/h4&gt;基于提出的贡献，本文在WLASL数据集上优于现有最佳结果，在AUTSL数据集上取得了具有竞争力的结果，且仅使用单流模型处理RGB视频。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了孤立手语识别（ISLR）任务的两个方面。首先，尽管存在大量数据集，但大多数孤立手语的数据量仍然有限，这给跨语言ISLR模型训练和迁移学习带来了挑战。其次，相似的手势可能具有不同的语义意义，这导致数据集标签模糊。为了解决这些问题，本研究提出了Logos，一个全新的俄罗斯手语（RSL）数据集，是按手语者数量划分的最大的ISLR数据集之一，也是规模和词汇量最大的RSL数据集。研究表明，在Logos数据集上预训练的模型可以用作其他语言SLR任务的通用编码器，包括少样本学习。本研究探讨了跨语言迁移学习方法，并发现使用多个分类头进行联合训练对提高目标低资源数据集的准确性最为有益。Logos数据集的关键特征是明确标注了视觉上相似的手势组。研究表明，明确标注视觉上相似的手势可以提升训练模型作为视觉编码器在下游任务中的质量。基于提出的贡献，本研究在WLASL数据集上优于现有最佳结果，在AUTSL数据集上取得了具有竞争力的结果，且仅使用单流模型处理RGB视频。源代码、数据集和预训练模型均已公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines two aspects of the isolated sign language recognition(ISLR) task. First, despite the availability of a number of datasets, theamount of data for most individual sign languages is limited. It poses thechallenge of cross-language ISLR model training, including transfer learning.Second, similar signs can have different semantic meanings. It leads toambiguity in dataset labeling and raises the question of the best policy forannotating such signs. To address these issues, this study presents Logos, anovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset bythe number of signers and one of the largest available datasets while also thelargest RSL dataset in size and vocabulary. It is shown that a model,pre-trained on the Logos dataset can be used as a universal encoder for otherlanguage SLR tasks, including few-shot learning. We explore cross-languagetransfer learning approaches and find that joint training using multipleclassification heads benefits accuracy for the target lowresource datasets themost. The key feature of the Logos dataset is explicitly annotated visuallysimilar sign groups. We show that explicitly labeling visually similar signsimproves trained model quality as a visual encoder for downstream tasks. Basedon the proposed contributions, we outperform current state-of-the-art resultsfor the WLASL dataset and get competitive results for the AUTSL dataset, with asingle stream model processing solely RGB video. The source code, dataset, andpre-trained models are publicly available.</description>
      <author>example@mail.com (Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev)</author>
      <guid isPermaLink="false">2505.10481v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>LEMON-Mapping: Loop-Enhanced Large-Scale Multi-Session Point Cloud Merging and Optimization for Globally Consistent Mapping</title>
      <link>http://arxiv.org/abs/2505.10018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Lemon-Mapping的循环增强框架，用于大规模多会话点云地图融合和优化，以解决多机器人协同中的地图构建和定位问题。&lt;h4&gt;背景&lt;/h4&gt;随着机器人技术的快速发展，多机器人协作变得至关重要和具有挑战性。其中一个关键问题是整合来自多个机器人的数据，以构建一个全局一致且精确的地图，从而实现稳健的合作和精确的定位。&lt;h4&gt;目的&lt;/h4&gt;针对传统多机器人位姿图优化（PGO）方法在保持基本全局一致性方面的局限性，本文旨在提出一种新的方法，以解决地图漂移和模糊问题。&lt;h4&gt;方法&lt;/h4&gt;Lemon-Mapping框架通过以下三个关键创新来实现目标：开发了一种鲁棒的循环处理机制和新的循环召回策略；引入了一种用于多机器人地图的空间光束调整方法；设计了一种利用光束调整精化约束的PGO策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与传统的地图融合方法相比，该方法在地图精度和减少地图漂移方面具有优势，并且具有处理大量机器人场景的强大能力。&lt;h4&gt;结论&lt;/h4&gt;Lemon-Mapping框架通过合理利用循环闭合并提高地图的几何质量，为多机器人地图构建和定位提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of robotics, multi-robot collaboration has become critical and challenging. One key problem is integrating data from multiple robots to build a globally consistent and accurate map for robust cooperation and precise localization. While traditional multi-robot pose graph optimization (PGO) maintains basic global consistency, it focuses primarily on pose optimization and ignores the geometric structure of the map. Moreover, PGO only uses loop closure as a constraint between two nodes, failing to fully exploit its capability to maintaining local consistency of multi-robot maps. Therefore, PGO-based multi-robot mapping methods often suffer from serious map divergence and blur, especially in regions with overlapping submaps. To address this issue, we propose Lemon-Mapping, a loop-enhanced framework for large-scale multi-session point cloud map fusion and optimization, which reasonably utilizes loop closure and improves the geometric quality of the map. We re-examine the role of loops for multi-robot mapping and introduce three key innovations. First, we develop a robust loop processing mechanism that effectively rejects outliers and a novel loop recall strategy to recover mistakenly removed loops. Second, we introduce a spatial bundle adjustment method for multi-robot maps that significantly reduces the divergence in overlapping regions and eliminates map blur. Third, we design a PGO strategy that leverages the refined constraints of bundle adjustment to extend the local accuracy to the global map. We validate our framework on several public datasets and a self-collected dataset. Experimental results demonstrate that our method outperforms traditional map merging approaches in terms of mapping accuracy and reduction of map divergence. Scalability experiments also demonstrate the strong capability of our framework to handle scenarios involving numerous robots.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of robotics, multi-robot collaboration has becomecritical and challenging. One key problem is integrating data from multiplerobots to build a globally consistent and accurate map for robust cooperationand precise localization. While traditional multi-robot pose graph optimization(PGO) maintains basic global consistency, it focuses primarily on poseoptimization and ignores the geometric structure of the map. Moreover, PGO onlyuses loop closure as a constraint between two nodes, failing to fully exploitits capability to maintaining local consistency of multi-robot maps. Therefore,PGO-based multi-robot mapping methods often suffer from serious map divergenceand blur, especially in regions with overlapping submaps. To address thisissue, we propose Lemon-Mapping, a loop-enhanced framework for large-scalemulti-session point cloud map fusion and optimization, which reasonablyutilizes loop closure and improves the geometric quality of the map. Were-examine the role of loops for multi-robot mapping and introduce three keyinnovations. First, we develop a robust loop processing mechanism thateffectively rejects outliers and a novel loop recall strategy to recovermistakenly removed loops. Second, we introduce a spatial bundle adjustmentmethod for multi-robot maps that significantly reduces the divergence inoverlapping regions and eliminates map blur. Third, we design a PGO strategythat leverages the refined constraints of bundle adjustment to extend the localaccuracy to the global map. We validate our framework on several publicdatasets and a self-collected dataset. Experimental results demonstrate thatour method outperforms traditional map merging approaches in terms of mappingaccuracy and reduction of map divergence. Scalability experiments alsodemonstrate the strong capability of our framework to handle scenariosinvolving numerous robots.</description>
      <author>example@mail.com (Lijie Wang, Xiaoyi Zhong, Ziyi Xu, Kaixin Chai, Anke Zhao, Tianyu Zhao, Qianhao Wang, Fei Gao)</author>
      <guid isPermaLink="false">2505.10018v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2505.10088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Due to the limitation "The abstract field cannot be longer than 1,920  characters", the abstract appearing here is slightly shorter than that in the  PDF file&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多模态表示学习（MMRL）的方法，旨在解决大规模预训练视觉语言模型在少量数据下的过拟合问题，并提高其在新任务上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练视觉语言模型在迁移学习方面取得了显著进展，但在少量数据下进行模型适配时，容易导致过拟合，影响其在新任务上的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出MMRL方法，通过引入一个共享的、可学习的、模态无关的表示空间，以解决过拟合问题并提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MMRL方法通过将空间标记投影到文本和图像编码器中作为表示标记，实现更有效的跨模态交互。同时，该方法在高层编码器中插入表示标记，以优化任务特定特征，同时在低层保留预训练知识。训练过程中，联合优化类别和表示特征，并在推理时采用解耦策略，以适应不同任务。&lt;h4&gt;主要发现&lt;/h4&gt;MMRL和其扩展MMRL++在15个数据集上进行了广泛实验，结果表明，这两种方法在任务特定适应和泛化之间取得了良好的平衡，并显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;MMRL和MMRL++能够有效解决视觉语言模型在少量数据下的过拟合问题，提高模型在新任务上的泛化能力，为视觉语言模型的进一步发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this, we propose Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, modality-agnostic representation space. MMRL generates space tokens projected into both text and image encoders as representation tokens, enabling more effective cross-modal interactions. Unlike prior methods that mainly optimize class token features, MMRL inserts representation tokens into higher encoder layers--where task-specific features are more prominent--while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term aligning class and text features with the frozen VLM's zero-shot features. At inference, a decoupling strategy uses both class and representation features for basetasks, but only class features for novel tasks due to their stronger generalization. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces trainable parameters and enhances intra-modal interactions--particularly across the layers of representation tokens--allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained Vision-Language Models (VLMs) have significantlyadvanced transfer learning across diverse tasks. However, adapting these modelswith limited few-shot data often leads to overfitting, undermining theirability to generalize to new tasks. To address this, we propose Multi-ModalRepresentation Learning (MMRL), which introduces a shared, learnable,modality-agnostic representation space. MMRL generates space tokens projectedinto both text and image encoders as representation tokens, enabling moreeffective cross-modal interactions. Unlike prior methods that mainly optimizeclass token features, MMRL inserts representation tokens into higher encoderlayers--where task-specific features are more prominent--while preservinggeneral knowledge in the lower layers. During training, both class andrepresentation features are jointly optimized: a trainable projection layer isapplied to representation tokens for task adaptation, while the projectionlayer for class token remains frozen to retain pre-trained knowledge. Tofurther promote generalization, we introduce a regularization term aligningclass and text features with the frozen VLM's zero-shot features. At inference,a decoupling strategy uses both class and representation features for basetasks, but only class features for novel tasks due to their strongergeneralization. Building upon this, we propose MMRL++, a parameter-efficientand interaction-aware extension that significantly reduces trainable parametersand enhances intra-modal interactions--particularly across the layers ofrepresentation tokens--allowing gradient sharing and instance-specificinformation to propagate more effectively through the network. Extensiveexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistentlyoutperform state-of-the-art methods, achieving a strong balance betweentask-specific adaptation and generalization.</description>
      <author>example@mail.com (Yuncheng Guo, Xiaodong Gu)</author>
      <guid isPermaLink="false">2505.10088v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Robust Federated Learning on Edge Devices with Domain Heterogeneity</title>
      <link>http://arxiv.org/abs/2505.10128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IWCMC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的框架，用于解决联邦学习在领域异质性下的统计异质性问题，通过原型增强来提高全局模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;联邦学习（FL）在确保数据隐私的同时实现分布式边缘设备的协作训练，适用于隐私敏感的应用。但FL面临统计异质性的挑战，尤其是领域异质性，这阻碍了全局模式的收敛。&lt;h4&gt;目的&lt;/h4&gt;提高联邦学习全局模型在领域异质性下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入了FedAPC（联邦增强原型对比学习）框架，这是一个基于原型的FL框架，旨在增强特征多样性和模型鲁棒性。FedAPC利用增强数据的均值特征原型来捕捉更丰富的表示，并通过将局部特征与全局原型对齐，使模型能够学习有意义的语义特征，同时减少对特定领域的过度拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在Office-10和Digits数据集上的实验结果表明，该框架优于SOTA（最先进技术）基线，展示了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;FedAPC框架有效地解决了联邦学习在领域异质性下的挑战，并提高了模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Federated Learning (FL) 允许在确保数据隐私的条件下进行协作训练，使其成为隐私敏感应用的流行解决方案。然而，FL由于统计异质性的存在，特别是领域异质性，面临着显著的挑战，这阻碍了全局模式的收敛。在本研究中，我们通过使用原型增强来提高FL全局模型在领域异质性下的泛化能力，引入了一种新的框架来应对这一挑战。具体而言，我们引入了FedAPC（Federated Augmented Prototype Contrastive Learning），这是一个基于原型的FL框架，旨在增强特征多样性和模型鲁棒性。FedAPC利用从增强数据的均值特征中派生的原型来捕捉更丰富的表示。通过将局部特征与全局原型对齐，我们使模型能够学习有意义的语义特征，同时减少对任何特定领域的过度拟合。在Office-10和Digits数据集上的实验结果表明，我们的框架优于最先进技术基线，展示了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) allows collaborative training while ensuring dataprivacy across distributed edge devices, making it a popular solution forprivacy-sensitive applications. However, FL faces significant challenges due tostatistical heterogeneity, particularly domain heterogeneity, which impedes theglobal mode's convergence. In this study, we introduce a new framework toaddress this challenge by improving the generalization ability of the FL globalmodel under domain heterogeneity, using prototype augmentation. Specifically,we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), aprototype-based FL framework designed to enhance feature diversity and modelrobustness. FedAPC leverages prototypes derived from the mean features ofaugmented data to capture richer representations. By aligning local featureswith global prototypes, we enable the model to learn meaningful semanticfeatures while reducing overfitting to any specific domain. Experimentalresults on the Office-10 and Digits datasets illustrate that our frameworkoutperforms SOTA baselines, demonstrating superior performance.</description>
      <author>example@mail.com (Huy Q. Le, Latif U. Khan, Choong Seon Hong)</author>
      <guid isPermaLink="false">2505.10128v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&amp;E Images using ViT Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MIPHEI的新方法，通过结合H&amp;E染色图像和先进的ViT基础模型，实现了从H&amp;E图像预测多标记免疫荧光（mIF）信号，从而在癌症诊断中提高细胞类型识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;组织病理学分析是癌症诊断的基础，H&amp;E染色被广泛用于观察细胞形态和组织结构。尽管多标记免疫荧光（mIF）可以更精确地识别细胞类型，但由于成本和物流限制，尚未得到广泛应用。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本研究旨在开发一种方法，能够从H&amp;E图像中预测mIF信号，以实现更精确的细胞类型识别。&lt;h4&gt;方法&lt;/h4&gt;MIPHEI模型基于U-Net架构，并集成了最先进的ViT基础模型作为编码器。该模型针对一系列标记物进行训练，包括核内容、免疫谱系（T细胞、B细胞、髓细胞）、上皮、基质、血管和增殖。研究使用公开的ORION数据集进行训练，并在两个独立数据集上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;MIPHEI在仅使用H&amp;E图像的情况下实现了准确的细胞类型分类，F1分数显著优于现有的基线和随机分类器。&lt;h4&gt;结论&lt;/h4&gt;MIPHEI模型能够有效地捕捉组织背景中核形态的复杂关系，为细胞类型感知的大规模H&amp;E数据集分析提供了有希望的步骤，有助于揭示空间细胞组织与患者结果之间的关系。&lt;h4&gt;翻译&lt;/h4&gt;摘要：组织病理学分析是癌症诊断的基石，Hematoxylin和Eosin（H&amp;E）染色通常用于每位患者以可视化细胞形态和组织结构。另一方面，多重免疫荧光（mIF）能够通过蛋白质组学标记实现更精确的细胞类型识别，但由于成本和物流限制，尚未得到广泛应用。为了弥合这一差距，我们介绍了一种名为MIPHEI（从H&amp;E图像预测多重免疫荧光）的方法，该方法基于U-Net架构，并集成了最先进的ViT基础模型作为编码器，以从H&amp;E图像预测mIF信号。MIPHEI针对一系列标记物进行训练，包括核内容、免疫谱系（T细胞、B细胞、髓细胞）、上皮、基质、血管和增殖。我们使用公开的ORION数据集进行训练，并在两个独立数据集上进行验证。MIPHEI在仅使用H&amp;E图像的情况下实现了准确的细胞类型分类，F1分数显著优于现有的基线和随机分类器。我们的结果表明，我们的模型能够有效地捕捉组织背景中核形态的复杂关系，为细胞类型感知的大规模H&amp;E数据集分析提供了有希望的步骤，有助于揭示空间细胞组织与患者结果之间的关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sanofi-public/miphei-vit&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathological analysis is a cornerstone of cancer diagnosis, withHematoxylin and Eosin (H&amp;E) staining routinely acquired for every patient tovisualize cell morphology and tissue architecture. On the other hand, multipleximmunofluorescence (mIF) enables more precise cell type identification viaproteomic markers, but has yet to achieve widespread clinical adoption due tocost and logistical constraints. To bridge this gap, we introduce MIPHEI(Multiplex Immunofluorescence Prediction from H&amp;E), a U-Net-inspiredarchitecture that integrates state-of-the-art ViT foundation models as encodersto predict mIF signals from H&amp;E images. MIPHEI targets a comprehensive panel ofmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),epithelium, stroma, vasculature, and proliferation. We train our model usingthe publicly available ORION dataset of restained H&amp;E and mIF images fromcolorectal cancer tissue, and validate it on two independent datasets. MIPHEIachieves accurate cell-type classification from H&amp;E alone, with F1 scores of0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,substantially outperforming both a state-of-the-art baseline and a randomclassifier for most markers. Our results indicate that our model effectivelycaptures the complex relationships between nuclear morphologies in their tissuecontext, as visible in H&amp;E images and molecular markers defining specific celltypes. MIPHEI offers a promising step toward enabling cell-type-aware analysisof large-scale H&amp;E datasets, in view of uncovering relationships betweenspatial cellular organization and patient outcomes.</description>
      <author>example@mail.com (Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter)</author>
      <guid isPermaLink="false">2505.10294v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>An Introduction to Discrete Variational Autoencoders</title>
      <link>http://arxiv.org/abs/2505.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tutorial paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于变分自编码器（VAEs）的概率无监督学习方法，并探讨了离散潜在空间在数据模态（如文本）中的应用。&lt;h4&gt;背景&lt;/h4&gt;VAEs作为一种基于神经网络的概率无监督学习方法，通常通过编码器网络定义高斯分布的潜在空间，并通过解码器网络进行数据重建。&lt;h4&gt;目的&lt;/h4&gt;提供对离散变分自编码器的严谨而实用的介绍，特别是那些潜在空间由具有分类分布的潜在变量组成的VAEs。&lt;h4&gt;方法&lt;/h4&gt;本文假设读者具有基本的数学背景，并从第一原理出发详细推导每个步骤。然后，开发了一个具体的训练方案，并提供了一个示例实现。&lt;h4&gt;主要发现&lt;/h4&gt;离散潜在空间在近年来越来越受欢迎，可能成为许多数据模态的自然选择。&lt;h4&gt;结论&lt;/h4&gt;通过提供详细的推导和示例实现，本文为理解和应用离散变分自编码器提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;Variational Autoencoders (VAEs) 作为一种基于神经网络的概率无监督学习方法已经得到广泛认可。通常，编码器网络定义了高斯分布的潜在空间，我们可以从中采样并将实现传递给解码器网络。该模型被训练以重建其输入，并通过证据下界进行优化。近年来，离散潜在空间越来越受欢迎，表明它们可能是许多数据模态（例如文本）的自然选择。在本教程中，我们提供了一个严谨而实用的离散变分自编码器介绍——具体来说，是那些潜在空间由遵循分类分布的潜在变量组成的VAEs。我们假设读者具有基本的数学背景，并从第一原理出发仔细推导每个步骤。从那里，我们开发了一个具体的训练方案，并提供了一个示例实现，可在https://github.com/alanjeffares/discreteVAE上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/alanjeffares/discretevae&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational Autoencoders (VAEs) are well-established as a principled approachto probabilistic unsupervised learning with neural networks. Typically, anencoder network defines the parameters of a Gaussian distributed latent spacefrom which we can sample and pass realizations to a decoder network. This modelis trained to reconstruct its inputs and is optimized through the evidencelower bound. In recent years, discrete latent spaces have grown in popularity,suggesting that they may be a natural choice for many data modalities (e.g.text). In this tutorial, we provide a rigorous, yet practical, introduction todiscrete variational autoencoders -- specifically, VAEs in which the latentspace is made up of latent variables that follow a categorical distribution. Weassume only a basic mathematical background with which we carefully derive eachstep from first principles. From there, we develop a concrete training recipeand provide an example implementation, hosted athttps://github.com/alanjeffares/discreteVAE.</description>
      <author>example@mail.com (Alan Jeffares, Liyuan Liu)</author>
      <guid isPermaLink="false">2505.10344v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection</title>
      <link>http://arxiv.org/abs/2505.09848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用放射基因组数据，包括结构MRI图像和基因表达数据，进行阿尔茨海默病（AD）检测的新方法。&lt;h4&gt;背景&lt;/h4&gt;成像和基因组数据提供了独特且丰富的特征，它们的一体化可以揭示对疾病复杂景观的新见解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效分类阿尔茨海默病（AD）为三个不同阶段（AD、轻度认知障碍（MCI）和认知正常（CN））的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的异构二分图表示学习方法，具有两种不同的节点类型：基因和图像。该方法利用一个小数据集对AD进行分类，并识别在分类组中起重要作用的基因。&lt;h4&gt;主要发现&lt;/h4&gt;该网络可以有效地将阿尔茨海默病（AD）分类为三个不同阶段，并确定了每个分类组中起重要作用的基因。使用分类准确率、召回率、精确率和F1分数等指标评估了该方法的表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的技术在扩展到基于放射基因组学的疾病分类方面具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imaging and genomic data offer distinct and rich features, and theirintegration can unveil new insights into the complex landscape of diseases. Inthis study, we present a novel approach utilizing radiogenomic data includingstructural MRI images and gene expression data, for Alzheimer's diseasedetection. Our framework introduces a novel heterogeneous bipartite graphrepresentation learning featuring two distinct node types: genes and images.The network can effectively classify Alzheimer's disease (AD) into threedistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)classes, utilizing a small dataset. Additionally, it identified which genesplay a significant role in each of these classification groups. We evaluate theperformance of our approach using metrics including classification accuracy,recall, precision, and F1 score. The proposed technique holds potential forextending to radiogenomic-based classification to other diseases.</description>
      <author>example@mail.com (Aditya Raj, Golrokh Mirzaei)</author>
      <guid isPermaLink="false">2505.09848v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback</title>
      <link>http://arxiv.org/abs/2505.09925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种交互式持续学习范式，该范式使AI模型能够从实时人类反馈中动态学习新技能，同时保留先前知识。&lt;h4&gt;背景&lt;/h4&gt;本文针对传统持续学习的两个主要局限性进行研究：一是使用流式、实时人工标注数据动态更新模型，而不是使用具有固定标签的静态数据集；二是处理真实世界中常见的噪声反馈，不假设标签的纯净性。&lt;h4&gt;目的&lt;/h4&gt;提出RiCL，一个利用大型语言模型（LLMs）从动态反馈中有效学习新技能的强化交互式持续学习框架。&lt;h4&gt;方法&lt;/h4&gt;RiCL包含三个关键组件：一个时间一致性感知的净化器，用于自动识别数据流中的纯净样本和噪声样本；一个交互感知的直接偏好优化策略，通过协调AI生成和人类提供的反馈来使模型行为与人类意图一致；以及一个噪声鲁棒的对比学习模块，通过利用数据关系来捕获稳健的表示，从而避免依赖于可能不可靠的标签。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集（FewRel和TACRED）上的大量实验表明，与现有的最先进在线持续学习和噪声标签学习方法相比，RiCL方法有显著的优势。&lt;h4&gt;结论&lt;/h4&gt;RiCL方法在处理动态反馈和噪声标签方面表现出色，为持续学习领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an interactive continual learning paradigm where AImodels dynamically learn new skills from real-time human feedback whileretaining prior knowledge. This paradigm distinctively addresses two majorlimitations of traditional continual learning: (1) dynamic model updates usingstreaming, real-time human-annotated data, rather than static datasets withfixed labels, and (2) the assumption of clean labels, by explicitly handlingthe noisy feedback common in real-world interactions. To tackle these problems,we propose RiCL, a Reinforced interactive Continual Learning frameworkleveraging Large Language Models (LLMs) to learn new skills effectively fromdynamic feedback. RiCL incorporates three key components: a temporalconsistency-aware purifier to automatically discern clean from noisy samples indata streams; an interaction-aware direct preference optimization strategy toalign model behavior with human intent by reconciling AI-generated andhuman-provided feedback; and a noise-resistant contrastive learning module thatcaptures robust representations by exploiting inherent data relationships, thusavoiding reliance on potentially unreliable labels. Extensive experiments ontwo benchmark datasets (FewRel and TACRED), contaminated with realistic noisepatterns, demonstrate that our RiCL approach substantially outperforms existingcombinations of state-of-the-art online continual learning and noisy-labellearning methods.</description>
      <author>example@mail.com (Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He)</author>
      <guid isPermaLink="false">2505.09925v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Towards Safe Robot Foundation Models Using Inductive Biases</title>
      <link>http://arxiv.org/abs/2505.10219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了机器人安全在现实世界部署中的重要性，并提出了一种结合机器人基础模型和几何归纳偏差的方法，通过ATACOM安全层来确保安全状态转换。&lt;h4&gt;背景&lt;/h4&gt;尽管当前机器人基础模型在多种任务中展现出良好的泛化能力，但它们未能解决安全问题，这对于确保长期运行至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决机器人基础模型在安全性方面的不足，提供一种确保安全行为的方法，无需大量的安全行为演示和特定的微调。&lt;h4&gt;方法&lt;/h4&gt;结合机器人基础模型与几何归纳偏差，使用ATACOM安全层强制执行动作约束，以确保安全状态转换。&lt;h4&gt;主要发现&lt;/h4&gt;该研究方法可以应用于经典操作任务，避免与无关对象的意外碰撞，同时也能应用于动态任务，如机器人曲棍球环境，生成遵守复杂任务和关节空间约束的快速轨迹。&lt;h4&gt;结论&lt;/h4&gt;通过ATACOM安全层，可以在不提供大量安全行为演示和不进行特定安全微调的情况下，为通用策略提供形式化的安全保证。&lt;h4&gt;翻译&lt;/h4&gt;Safety is a critical requirement for the real-world deployment of roboticsystems. Unfortunately, while current robot foundation models show promisinggeneralization capabilities across a wide variety of tasks, they fail toaddress safety, an important aspect for ensuring long-term operation. Currentrobot foundation models assume that safe behavior should emerge by learningfrom a sufficiently large dataset of demonstrations. However, this approach hastwo clear major drawbacks. Firstly, there are no formal safety guarantees for abehavior cloning policy trained using supervised learning. Secondly, withoutexplicit knowledge of any safety constraints, the policy may require anunreasonable number of additional demonstrations to even approximate thedesired constrained behavior. To solve these key issues, we show how we caninstead combine robot foundation models with geometric inductive biases usingATACOM, a safety layer placed after the foundation policy that ensures safestate transitions by enforcing action constraints. With this approach, we canensure formal safety guarantees for generalist policies without providingextensive demonstrations of safe behavior, and without requiring any specificfine-tuning for safety. Our experiments show that our approach can bebeneficial both for classical manipulation tasks, where we avoid unwantedcollisions with irrelevant objects, and for dynamic tasks, such as the robotair hockey environment, where we can generate fast trajectories respectingcomplex tasks and joint space constraints.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety is a critical requirement for the real-world deployment of roboticsystems. Unfortunately, while current robot foundation models show promisinggeneralization capabilities across a wide variety of tasks, they fail toaddress safety, an important aspect for ensuring long-term operation. Currentrobot foundation models assume that safe behavior should emerge by learningfrom a sufficiently large dataset of demonstrations. However, this approach hastwo clear major drawbacks. Firstly, there are no formal safety guarantees for abehavior cloning policy trained using supervised learning. Secondly, withoutexplicit knowledge of any safety constraints, the policy may require anunreasonable number of additional demonstrations to even approximate thedesired constrained behavior. To solve these key issues, we show how we caninstead combine robot foundation models with geometric inductive biases usingATACOM, a safety layer placed after the foundation policy that ensures safestate transitions by enforcing action constraints. With this approach, we canensure formal safety guarantees for generalist policies without providingextensive demonstrations of safe behavior, and without requiring any specificfine-tuning for safety. Our experiments show that our approach can bebeneficial both for classical manipulation tasks, where we avoid unwantedcollisions with irrelevant objects, and for dynamic tasks, such as the robotair hockey environment, where we can generate fast trajectories respectingcomplex tasks and joint space constraints.</description>
      <author>example@mail.com (Maximilian Tölle, Theo Gruner, Daniel Palenicek, Tim Schneider, Jonas Günster, Joe Watson, Davide Tateo, Puze Liu, Jan Peters)</author>
      <guid isPermaLink="false">2505.10219v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Radar Point Cloud Enhancement via Arbitrary LiDAR Guided Diffusion Prior</title>
      <link>http://arxiv.org/abs/2505.09887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无监督的雷达点增强算法，通过使用任意激光雷达引导的扩散模型作为先验，无需配对训练数据即可提高雷达分辨率。&lt;h4&gt;背景&lt;/h4&gt;雷达在工业自动化中的机器感知中起着关键作用，但其角分辨率受限于瑞利准则，该准则依赖于雷达的工作波长和天线阵列的有效孔径。&lt;h4&gt;目的&lt;/h4&gt;克服硬件限制，提高雷达分辨率，而不依赖于配对的高分辨率地面实况数据。&lt;h4&gt;方法&lt;/h4&gt;将雷达角度估计恢复作为逆问题，并通过扩散模型结合任意激光雷达领域的先验知识。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在保持高保真度和低噪声性能方面优于传统的正则化技术，并且与配对训练方法相比，不仅达到了可比的性能，而且具有更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法通过整合扩散模型中的先验知识来增强雷达点输出，而不是依赖于配对训练数据，是目前首个采用此方法的研究。&lt;h4&gt;翻译&lt;/h4&gt;In industrial automation, radar is a critical sensor in machine perception. However, the angular resolution of radar is inherently limited by the Rayleigh criterion, which depends on both the radar's operating wavelength and the effective aperture of its antenna array. To overcome these hardware-imposed limitations, recent neural network-based methods have leveraged high-resolution LiDAR data, paired with radar measurements, during training to enhance radar point cloud resolution. While effective, these approaches require extensive paired datasets, which are costly to acquire and prone to calibration error. These challenges motivate the need for methods that can improve radar resolution without relying on paired high-resolution ground-truth data. Here, we introduce an unsupervised radar points enhancement algorithm that employs an arbitrary LiDAR-guided diffusion model as a prior without the need for paired training data. Specifically, our approach formulates radar angle estimation recovery as an inverse problem and incorporates prior knowledge through a diffusion model with arbitrary LiDAR domain knowledge. Experimental results demonstrate that our method attains high fidelity and low noise performance compared to traditional regularization techniques. Additionally, compared to paired training methods, it not only achieves comparable performance but also offers improved generalization capability. To our knowledge, this is the first approach that enhances radar points output by integrating prior knowledge via a diffusion model rather than relying on paired training data. Our code is available at https://github.com/yyxr75/RadarINV.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In industrial automation, radar is a critical sensor in machine perception.However, the angular resolution of radar is inherently limited by the Rayleighcriterion, which depends on both the radar's operating wavelength and theeffective aperture of its antenna array.To overcome these hardware-imposedlimitations, recent neural network-based methods have leveraged high-resolutionLiDAR data, paired with radar measurements, during training to enhance radarpoint cloud resolution. While effective, these approaches require extensivepaired datasets, which are costly to acquire and prone to calibration error.These challenges motivate the need for methods that can improve radarresolution without relying on paired high-resolution ground-truth data. Here,we introduce an unsupervised radar points enhancement algorithm that employs anarbitrary LiDAR-guided diffusion model as a prior without the need for pairedtraining data. Specifically, our approach formulates radar angle estimationrecovery as an inverse problem and incorporates prior knowledge through adiffusion model with arbitrary LiDAR domain knowledge. Experimental resultsdemonstrate that our method attains high fidelity and low noise performancecompared to traditional regularization techniques. Additionally, compared topaired training methods, it not only achieves comparable performance but alsooffers improved generalization capability. To our knowledge, this is the firstapproach that enhances radar points output by integrating prior knowledge via adiffusion model rather than relying on paired training data. Our code isavailable at https://github.com/yyxr75/RadarINV.</description>
      <author>example@mail.com (Yanlong Yang, Jianan Liu, Guanxiong Luo, Hao Li, Euijoon Ahn, Mostafa Rahimi Azghadi, Tao Huang)</author>
      <guid isPermaLink="false">2505.09887v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Automated grading and staging of ovarian cancer using deep learning on the transmission optical microscopy bright-field images of thin biopsy tissue samples</title>
      <link>http://arxiv.org/abs/2505.09993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures, 15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;卵巢癌的诊断和管理是一个挑战，预后很大程度上取决于发现时的阶段。准确分级和分期对治疗计划和预测结果至关重要，但这一手动过程耗时且病理学家之间存在观察者差异。随着数字病理学切片数量的增加，需要开发稳健的自动化方法来辅助这一关键的诊断步骤。&lt;h4&gt;背景&lt;/h4&gt;卵巢癌的诊断主要依赖于对活检组织样本的病理学检查，这是一个耗时且易受观察者差异影响的程序。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架，用于使用常规病理学图像自动预测卵巢癌的阶段（分为五类：0、I、II、III、IV）。&lt;h4&gt;方法&lt;/h4&gt;研究采用了迁移学习方法，微调了一个在ImageNet上预训练的ResNet-101卷积神经网络。训练过程中包含了全面的数据增强、加权随机采样和类别加权，以解决数据集的特点。使用遗传算法对学习率、dropout率和权重衰减进行了超参数优化，以提高模型性能和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在卵巢薄组织明场图像的独立测试集上评估，所开发的模型实现了97.62%的整体分类准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的深度学习框架能够有效地辅助卵巢癌的自动分级和分期，具有较高的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ovarian cancer remains a challenging malignancy to diagnose and manage, withprognosis heavily dependent on the stage at detection. Accurate grading andstaging, primarily based on histopathological examination of biopsy tissuesamples, are crucial for treatment planning and predicting outcomes. However,this manual process is time-consuming and subject to inter-observer variabilityamong pathologists. The increasing volume of digital histopathology slidesnecessitates the development of robust, automated methods to assist in thiscritical diagnostic step for ovarian cancer. (Methods) This study presents adeep learning framework for the automated prediction of ovarian cancer stage(classified into five categories: 0, I, II, III, IV) using routinehistopathological images. We employed a transfer learning approach, fine-tuninga ResNet-101 convolutional neural network pre-trained on ImageNet. The trainingprocess incorporated comprehensive data augmentation, weighted random sampling,and class weighting to address dataset characteristics. Hyperparameteroptimization for learning rate, dropout rate, and weight decay was performedusing a genetic algorithm to enhance model performance and generalization.(Results) Evaluated on an independent test set of ovarian thin tissuebrightfield images, the developed model achieved a high overall classificationaccuracy of 97.62%.</description>
      <author>example@mail.com (Ashmit K Mishra, Mousa Alrubayan, Prabhakar Pradhan)</author>
      <guid isPermaLink="false">2505.09993v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</title>
      <link>http://arxiv.org/abs/2505.09985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为有序子集多扩散模型（OSMM）的稀疏视图CT重建方法，该方法有效提高了重建图像的细节和质量。&lt;h4&gt;背景&lt;/h4&gt;基于分数的扩散模型在稀疏视图CT重建领域展现出巨大潜力，但投影数据集大且冗余，导致学习效果不佳，重建图像缺乏细节。&lt;h4&gt;目的&lt;/h4&gt;提出OSMM以解决上述问题，提高稀疏视图CT重建的细节和质量。&lt;h4&gt;方法&lt;/h4&gt;OSMM将CT投影数据分为等份，并使用多子集扩散模型（MSDM）独立学习每一份数据。此外，将整个扩散模型（OWDM）与完整的投影数据结合，作为全局信息约束，减少错误或不一致的投影数据信息。OSMM采用无监督学习框架，对稀疏视图CT的多样性具有很好的适应性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，OSMM在图像质量和噪声鲁棒性方面优于传统扩散模型，为稀疏视图CT成像提供了一种强大且通用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;OSMM是一种有效的稀疏视图CT重建方法，能够显著提高重建图像的细节和质量，适用于不同的临床场景。&lt;h4&gt;翻译&lt;/h4&gt;Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMM's unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Score-based diffusion models have shown significant promise in the field ofsparse-view CT reconstruction. However, the projection dataset is large andriddled with redundancy. Consequently, applying the diffusion model tounprocessed data results in lower learning effectiveness and higher learningdifficulty, frequently leading to reconstructed images that lack fine details.To address these issues, we propose the ordered-subsets multi-diffusion model(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CTprojection data into equal subsets and employs multi-subsets diffusion model(MSDM) to learn from each subset independently. This targeted learning approachreduces complexity and enhances the reconstruction of fine details.Furthermore, the integration of one-whole diffusion model (OWDM) with completesinogram data acts as a global information constraint, which can reduce thepossibility of generating erroneous or inconsistent sinogram information.Moreover, the OSMM's unsupervised learning framework provides strong robustnessand generalizability, adapting seamlessly to varying sparsity levels of CTsinograms. This ensures consistent and reliable performance across differentclinical scenarios. Experimental results demonstrate that OSMM outperformstraditional diffusion models in terms of image quality and noise resilience,offering a powerful and versatile solution for advanced CT imaging insparse-view scenarios.</description>
      <author>example@mail.com (Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu)</author>
      <guid isPermaLink="false">2505.09985v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VALVEFIT: An analysis-suitable B-spline-based surface fitting framework for patient-specific modeling of tricuspid valves</title>
      <link>http://arxiv.org/abs/2505.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VALVEFIT是一个基于GPU加速的可微分的B样条曲面拟合框架，用于从医疗图像分割获得的点云中快速重建平滑、适合分析的几何形状。&lt;h4&gt;背景&lt;/h4&gt;患者特异性的三尖瓣计算建模对于心脏瓣膜疾病的临床评估至关重要，但这个过程受到医疗图像数据固有限制的阻碍，如噪声和稀疏性，以及复杂的瓣膜动力学。&lt;h4&gt;目的&lt;/h4&gt;开发VALVEFIT框架，以解决医疗图像数据中的挑战，实现快速、准确的三尖瓣计算建模。&lt;h4&gt;方法&lt;/h4&gt;VALVEFIT使用理想的TVB样条模板曲面，通过创新的损失函数优化控制点位置，以拟合分割点云，并引入新的正则化项以确保表面在大变形下保持平滑、规则且无交点。&lt;h4&gt;主要发现&lt;/h4&gt;VALVEFIT在模拟点云上表现出鲁棒性和准确性，并且在不同点云密度和噪声水平上具有良好的鲁棒性。它还能拟合从不同瓣膜运动阶段的真实患者获取的点云。&lt;h4&gt;结论&lt;/h4&gt;VALVEFIT使得患者特异性的建模自动化，几乎无需人工干预，为未来直接图像到分析平台在临床应用中的开发铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：针对三尖瓣（TV）的患者特异性计算建模对于心脏瓣膜疾病的临床评估至关重要。然而，这一过程受到医疗图像数据固有局限性的阻碍，例如噪声和稀疏性，以及复杂的瓣膜动力学。我们提出了VALVEFIT，一个新颖的基于GPU加速和可微分的B样条曲面拟合框架，能够从通过医学图像分割获得的点云中快速重建平滑、适合分析的几何形状。我们从一个理想的TVB样条模板曲面开始，通过创新的损失函数优化其控制点位置以拟合分割点云。为了确保表面在大变形下保持平滑、规则且无交点，我们引入了新的正则化项。我们通过将框架应用于作为真实情况的模拟点云来展示其鲁棒性并验证其准确性。我们还展示了它在不同点云密度和噪声水平上的鲁棒性。最后，我们展示了该框架对拟合从不同瓣膜运动阶段的真实患者获取的点云的性能。然后，在对拟合表面进行等几何生物力学瓣膜模拟时，展示了其直接应用于分析的可能性。VALVEFIT使得患者特异性的建模自动化，几乎无需人工干预，为未来直接图像到分析平台在临床应用中的开发铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient-specific computational modeling of the tricuspid valve (TV) is vitalfor the clinical assessment of heart valve diseases. However, this process ishindered by limitations inherent in the medical image data, such as noise andsparsity, as well as by complex valve dynamics. We present VALVEFIT, a novelGPU-accelerated and differentiable B-spline surface fitting framework thatenables rapid reconstruction of smooth, analysis-suitable geometry from pointclouds obtained via medical image segmentation. We start with an idealized TVB-spline template surface and optimize its control point positions to fitsegmented point clouds via an innovative loss function, balancing shapefidelity and mesh regularization. Novel regularization terms are introduced toensure that the surface remains smooth, regular, and intersection-free duringlarge deformations. We demonstrate the robustness and validate the accuracy ofthe framework by first applying it to simulation-derived point clouds thatserve as the ground truth. We further show its robustness across differentpoint cloud densities and noise levels. Finally, we demonstrate the performanceof the framework toward fitting point clouds obtained from real patients atdifferent stages of valve motion. An isogeometric biomechanical valvesimulation is then performed on the fitted surfaces to show their directapplicability toward analysis. VALVEFIT enables automated patient-specificmodeling with minimal manual intervention, paving the way for the futuredevelopment of direct image-to-analysis platforms for clinical applications.</description>
      <author>example@mail.com (Ajith Moola, Ashton M. Corpuz, Michael J. Burkhart, Colton J. Ross, Arshid Mir, Harold M. Burkhart, Chung-Hao Lee, Ming-Chen Hsu, Aishwarya Pawar)</author>
      <guid isPermaLink="false">2505.09790v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments</title>
      <link>http://arxiv.org/abs/2505.09434v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 14 figures, to be published in Frontiers in Robotics and AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中引导机器人群体遵循某种群行为的工作，并引入了更现实的局部避障策略。&lt;h4&gt;背景&lt;/h4&gt;先前研究关注于在未知障碍环境中使用分布式NMPC引导机器人群体。&lt;h4&gt;目的&lt;/h4&gt;实现更真实的局部避障策略，并通过优化框架整合局部障碍避免约束。&lt;h4&gt;方法&lt;/h4&gt;将局部障碍避免约束与点云数据结合，并使用点云处理技术（包括方向过滤和下采样）以减少优化过程中的计算负担。&lt;h4&gt;主要发现&lt;/h4&gt;通过Gazebo中的3D模拟验证了算法的性能，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效减少计算负担，并在实际模拟中表现出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;本研究扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中引导机器人群体遵循某种群行为的工作，并引入了更现实的局部避障策略。具体而言，我们通过将局部障碍避免约束与点云数据结合，并将其整合到NMPC框架中。在这里，每个智能体依赖于其局部传感器的数据来感知和响应附近的障碍物。提出了一种针对二维和三维点云的点云处理技术，以在优化过程中最小化计算负担。该过程包括方向过滤和下采样，这显著减少了数据点的数量。通过Gazebo中的现实3D模拟验证了算法的性能，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3389/frobt.2025.1540808&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our prior work on the distributed nonlinear modelpredictive control (NMPC) for navigating a robot fleet following a certainflocking behavior in unknown obstructed environments with a more realisticlocal obstacle avoidance strategy. More specifically, we integrate the localobstacle avoidance constraint using point clouds into the NMPC framework. Here,each agent relies on data from its local sensor to perceive and respond tonearby obstacles. A point cloud processing technique is presented for bothtwo-dimensional and three-dimensional point clouds to minimize thecomputational burden during the optimization. The process consists ofdirectional filtering and down-sampling that significantly reduce the number ofdata points. The algorithm's performance is validated through realistic 3Dsimulations in Gazebo, and its practical feasibility is further explored viahardware-in-the-loop (HIL) simulations on embedded platforms.</description>
      <author>example@mail.com (Nuthasith Gerdpratoom, Kaoru Yamamoto)</author>
      <guid isPermaLink="false">2505.09434v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era</title>
      <link>http://arxiv.org/abs/2505.09651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了地理空间表示学习的发展，探讨了深度学习和大型语言模型在地理空间智能（LI）领域的应用。&lt;h4&gt;背景&lt;/h4&gt;地理空间智能（LI）是现代空间决策的关键，地理空间表示学习正在通过深度学习和大型语言模型（LLM）的进步而快速发展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在全面回顾地理空间表示学习，并基于数据、方法和应用视角构建一个结构化的分类法。&lt;h4&gt;方法&lt;/h4&gt;文章通过数据视角、方法视角和应用视角对地理空间表示学习进行了分类，并讨论了当前进展、现有局限性和未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;深度神经网络在结构化地理空间数据特征提取方面表现出色，而LLM的集成则增强了跨模态地理空间推理和未结构化地理文本数据处理的能力。&lt;h4&gt;结论&lt;/h4&gt;本文为地理空间智能领域提供了深入探索和未来创新的路线图。&lt;h4&gt;翻译&lt;/h4&gt;Location Intelligence (LI), the science of transforming location-centric geospatial data into actionable knowledge, has become a cornerstone of modern spatial decision-making. The rapid evolution of Geospatial Representation Learning is fundamentally reshaping LI development through two successive technological revolutions: the deep learning breakthrough and the emerging large language model (LLM) paradigm. While deep neural networks (DNNs) have demonstrated remarkable success in automated feature extraction from structured geospatial data (e.g., satellite imagery, GPS trajectories), the recent integration of LLMs introduces transformative capabilities for cross-modal geospatial reasoning and unstructured geo-textual data processing. This survey presents a comprehensive review of geospatial representation learning across both technological eras, organizing them into a structured taxonomy based on the complete pipeline comprising: (1) data perspective, (2) methodological perspective and (3) application perspective. We also highlight current advancements, discuss existing limitations, and propose potential future research directions in the LLM era. This work offers a thorough exploration of the field and providing a roadmap for further innovation in LI. The summary of the up-to-date paper list can be found in https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo continuous updates.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Location Intelligence (LI), the science of transforming location-centricgeospatial data into actionable knowledge, has become a cornerstone of modernspatial decision-making. The rapid evolution of Geospatial RepresentationLearning is fundamentally reshaping LI development through two successivetechnological revolutions: the deep learning breakthrough and the emerginglarge language model (LLM) paradigm. While deep neural networks (DNNs) havedemonstrated remarkable success in automated feature extraction from structuredgeospatial data (e.g., satellite imagery, GPS trajectories), the recentintegration of LLMs introduces transformative capabilities for cross-modalgeospatial reasoning and unstructured geo-textual data processing. This surveypresents a comprehensive review of geospatial representation learning acrossboth technological eras, organizing them into a structured taxonomy based onthe complete pipeline comprising: (1) data perspective, (2) methodologicalperspective and (3) application perspective. We also highlight currentadvancements, discuss existing limitations, and propose potential futureresearch directions in the LLM era. This work offers a thorough exploration ofthe field and providing a roadmap for further innovation in LI. The summary ofthe up-to-date paper list can be found inhttps://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergocontinuous updates.</description>
      <author>example@mail.com (Xixuan Hao, Yutian Jiang, Xingchen Zou, Jiabo Liu, Yifang Yin, Yuxuan Liang)</author>
      <guid isPermaLink="false">2505.09651v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration</title>
      <link>http://arxiv.org/abs/2505.09756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多智能体强化学习（MARL）框架，该框架中的智能体在一个具有潜在社区结构和混合成员资格的时间演化网络中合作。&lt;h4&gt;背景&lt;/h4&gt;与传统的基于邻居或固定交互图的框架不同，该框架通过允许每个智能体属于多个重叠的社区来捕捉灵活和抽象的协调模式。&lt;h4&gt;目的&lt;/h4&gt;旨在设计一种能够有效共享结构化信息，并支持迁移学习和主动学习的MARL框架。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于社区结构的actor-critic算法，智能体通过继承社区级别的策略更新和价值学习的估计来实现信息共享，而不需要访问其他智能体的策略。该框架支持通过成员资格估计适应新智能体或任务，并在探索过程中优先考虑不确定的社区。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明了在actor和critic更新中使用线性函数近似下的收敛性保证。&lt;h4&gt;结论&lt;/h4&gt;这是第一个将社区结构、可迁移性和主动学习与可证明的保证相结合的MARL框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new framework for multi-agent reinforcement learning (MARL),where the agents cooperate in a time-evolving network with latent communitystructures and mixed memberships. Unlike traditional neighbor-based or fixedinteraction graphs, our community-based framework captures flexible andabstract coordination patterns by allowing each agent to belong to multipleoverlapping communities. Each community maintains shared policy and valuefunctions, which are aggregated by individual agents according to personalizedmembership weights. We also design actor-critic algorithms that exploit thisstructure: agents inherit community-level estimates for policy updates andvalue learning, enabling structured information sharing without requiringaccess to other agents' policies. Importantly, our approach supports bothtransfer learning by adapting to new agents or tasks via membership estimation,and active learning by prioritizing uncertain communities during exploration.Theoretically, we establish convergence guarantees under linear functionapproximation for both actor and critic updates. To our knowledge, this is thefirst MARL framework that integrates community structure, transferability, andactive learning with provable guarantees.</description>
      <author>example@mail.com (Zhaoyang Shi)</author>
      <guid isPermaLink="false">2505.09756v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
      <link>http://arxiv.org/abs/2505.09193v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first learned video codec that surpasses VTM 13.2 RA across all  standard test datasets. Code will be available at  https://github.com/JiangWeibeta/ECVC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiECVC的基于双向视频压缩（BVC）的框架，该框架在视频压缩性能上超越了VVC参考软件VTM，实现了显著的比特率降低。&lt;h4&gt;背景&lt;/h4&gt;现有的BVC方法在性能上落后于仅使用前向预测的LVC方法，主要原因是它们在提取多样化和准确的上下文信息方面能力有限，并且缺乏对动态抑制有害上下文的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出BiECVC框架，以解决现有BVC方法的局限性，提高视频压缩性能。&lt;h4&gt;方法&lt;/h4&gt;BiECVC通过结合多样化的本地和非本地上下文建模以及自适应上下文门控来提高性能。它重用低层的高质量特征，并使用解码的运动向量进行对齐，同时采用线性注意力机制来建模非本地依赖关系。为了减轻上下文预测不准确的影响，引入了双向上下文门控。&lt;h4&gt;主要发现&lt;/h4&gt;BiECVC在随机访问配置下，比特率分别降低了13.4%和15.7%，超越了VTM 13.2，这是第一个在所有标准测试数据集上超越VTM 13.2 RA的基于学习的视频编解码器。&lt;h4&gt;结论&lt;/h4&gt;BiECVC在视频压缩性能上取得了突破，为BVC领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead. To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent forward prediction-based learned video compression (LVC) methods haveachieved impressive results, even surpassing VVC reference software VTM underthe Low Delay B (LDB) configuration. In contrast, learned bidirectional videocompression (BVC) remains underexplored and still lags behind its forward-onlycounterparts. This performance gap is mainly due to the limited ability toextract diverse and accurate contexts: most existing BVCs primarily exploittemporal motion while neglecting non-local correlations across frames.Moreover, they lack the adaptability to dynamically suppress harmful contextsarising from fast motion or occlusion. To tackle these challenges, we proposeBiECVC, a BVC framework that incorporates diversified local and non-localcontext modeling along with adaptive context gating. For local contextenhancement, BiECVC reuses high-quality features from lower layers and alignsthem using decoded motion vectors without introducing extra motion overhead. Tomodel non-local dependencies efficiently, we adopt a linear attention mechanismthat balances performance and complexity. To further mitigate the impact ofinaccurate context prediction, we introduce Bidirectional Context Gating,inspired by data-dependent decay in recent autoregressive language models, todynamically filter contextual information based on conditional coding results.Extensive experiments demonstrate that BiECVC achieves state-of-the-artperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2under the Random Access (RA) configuration with intra periods of 32 and 64,respectively. To our knowledge, BiECVC is the first learned video codec tosurpass VTM 13.2 RA across all standard test datasets. Code will be availableat https://github.com/JiangWeibeta/ECVC.</description>
      <author>example@mail.com (Wei Jiang, Junru Li, Kai Zhang, Li Zhang)</author>
      <guid isPermaLink="false">2505.09193v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fair In-Context Learning with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09503v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表格型基础模型在结构化数据上的强情境学习能力，以及如何通过预处理策略来减少模型中的偏见，以提高情境预测的公平性。&lt;h4&gt;背景&lt;/h4&gt;表格型基础模型在结构化数据上表现出强大的情境学习能力，可以不更新参数就对测试集进行准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究表格型情境学习能力中的公平性问题，并探索减少模型偏见的方法。&lt;h4&gt;方法&lt;/h4&gt;论文研究了三种预处理策略：相关去除、分组平衡演示选择和基于不确定性的演示选择，以减少偏见。&lt;h4&gt;主要发现&lt;/h4&gt;基于不确定性的演示选择策略可以持续提高情境预测的组公平性。&lt;h4&gt;结论&lt;/h4&gt;表格型情境学习能力中的偏见可以通过特定的预处理策略得到有效缓解。&lt;h4&gt;翻译&lt;/h4&gt;Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundational models have exhibited strong in-context learning (ICL)capabilities on structured data, allowing them to make accurate predictions ontest sets without parameter updates, using training examples as context. Thisemerging approach positions itself as a competitive alternative to traditionalgradient-boosted tree methods. However, while biases in conventional machinelearning models are well documented, it remains unclear how these biasesmanifest in tabular ICL. The paper investigates the fairness implications oftabular ICL and explores three preprocessing strategies--correlation removal,group-balanced demonstration selection, and uncertainty-based demonstrationselection--to address bias. Comprehensive experiments indicate thatuncertainty-based demonstration selection consistently enhances group fairnessof in-context predictions. The source code for reproducing the results of thiswork can be found at https://github.com/patrikken/Fair-TabICL.</description>
      <author>example@mail.com (Patrik Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji)</author>
      <guid isPermaLink="false">2505.09503v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Analog Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 8 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通用的可扩展方法，用于在具有噪声和低精度限制的模拟硬件上鲁棒地适配大型语言模型（LLMs）。该方法使得包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct在内的高级模型能够保持与4位权重、8位激活基线相当的性能，同时展示了在低精度数字硬件上进行推理的能力，并证明了在测试时计算缩放方面的优势。&lt;h4&gt;背景&lt;/h4&gt;模拟内存计算（AIMC）是一种有前途的计算范式，它旨在超越基于传统冯·诺伊曼架构的速度和功耗效率限制。然而，AIMC引入了诸如噪声计算和严格的输入输出量化约束等基本挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，使得LLMs能够在模拟硬件上保持高精度，并能够量化以适应低精度数字硬件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用的可扩展方法，该方法允许LLMs在存在模拟噪声和量化约束的情况下保持性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法使得高级模型在模拟硬件上保持与低精度数字硬件上相当的性能，并且可以通过训练方法将模拟基础模型量化以适应低精度数字硬件。&lt;h4&gt;结论&lt;/h4&gt;该方法在大型LLMs和高效率模拟硬件之间架起了一座桥梁，为高效的基础模型提供了走向节能的路径。&lt;h4&gt;翻译&lt;/h4&gt;Analog in-memory computing (AIMC) is a promising compute paradigm to improve speed and power efficiency of neural network inference beyond the limits of conventional von Neumann-based architectures. However, AIMC introduces fundamental challenges such as noisy computations and strict constraints on input and output quantization. Because of these constraints and imprecisions, off-the-shelf LLMs are not able to achieve 4-bit-level performance when deployed on AIMC-based hardware. While researchers previously investigated recovering this accuracy gap on small, mostly vision-based models, a generic method applicable to LLMs pre-trained on trillions of tokens does not yet exist. In this work, we introduce a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. Our approach enables state-of-the-art models including Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct to retain performance comparable to 4-bit weight, 8-bit activation baselines, despite the presence of analog noise and quantization constraints. Additionally, we show that as a byproduct of our training methodology, analog foundation models can be quantized for inference on low-precision digital hardware. Finally, we show that our models also benefit from test-time compute scaling, showing better scaling behavior than models trained with 4-bit weight and 8-bit static input quantization. Our work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models. Code is available at https://github.com/IBM/analog-foundation-models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ibm/analog-foundation-models&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog in-memory computing (AIMC) is a promising compute paradigm to improvespeed and power efficiency of neural network inference beyond the limits ofconventional von Neumann-based architectures. However, AIMC introducesfundamental challenges such as noisy computations and strict constraints oninput and output quantization. Because of these constraints and imprecisions,off-the-shelf LLMs are not able to achieve 4-bit-level performance whendeployed on AIMC-based hardware. While researchers previously investigatedrecovering this accuracy gap on small, mostly vision-based models, a genericmethod applicable to LLMs pre-trained on trillions of tokens does not yetexist. In this work, we introduce a general and scalable method to robustlyadapt LLMs for execution on noisy, low-precision analog hardware. Our approachenables state-of-the-art models $\unicode{x2013}$ includingPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retainperformance comparable to 4-bit weight, 8-bit activation baselines, despite thepresence of analog noise and quantization constraints. Additionally, we showthat as a byproduct of our training methodology, analog foundation models canbe quantized for inference on low-precision digital hardware. Finally, we showthat our models also benefit from test-time compute scaling, showing betterscaling behavior than models trained with 4-bit weight and 8-bit static inputquantization. Our work bridges the gap between high-capacity LLMs and efficientanalog hardware, offering a path toward energy-efficient foundation models.Code is available at https://github.com/IBM/analog-foundation-models .</description>
      <author>example@mail.com (Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian)</author>
      <guid isPermaLink="false">2505.09663v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Next Word Suggestion using Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.09649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图卷积网络（GNN）和长短期记忆网络（LSTM）的语言模型上下文嵌入方法，以预测给定前文局部上下文下的下一个词。&lt;h4&gt;背景&lt;/h4&gt;现有的语言模型通常需要大量参数、文本数据和计算资源，成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决语言模型中的上下文嵌入子任务。&lt;h4&gt;方法&lt;/h4&gt;利用GNN的图卷积操作编码上下文，并与LSTM结合预测下一个词。&lt;h4&gt;主要发现&lt;/h4&gt;在有限的资源下，该方法在自定义维基百科文本语料库上表现良好。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地预测了下一个词，为语言模型的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Language Modeling is a prevalent task in Natural Language Processing. The currently existing most recent and most successful language models often tend to build a massive model with billions of parameters, feed in a tremendous amount of text data, and train with enormous computation resources which require millions of dollars. In this project, we aim to address an important sub-task in language modeling, i.e., context embedding. We propose an approach to exploit the Graph Convolution operation in GNNs to encode the context and use it in coalition with LSTMs to predict the next word given a local context of preceding words. We test this on the custom Wikipedia text corpus using a very limited amount of resources and show that this approach works fairly well to predict the next word.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language Modeling is a prevalent task in Natural Language Processing. Thecurrently existing most recent and most successful language models often tendto build a massive model with billions of parameters, feed in a tremendousamount of text data, and train with enormous computation resources whichrequire millions of dollars. In this project, we aim to address an importantsub-task in language modeling, i.e., context embedding. We propose an approachto exploit the Graph Convolution operation in GNNs to encode the context anduse it in coalition with LSTMs to predict the next word given a local contextof preceding words. We test this on the custom Wikipedia text corpus using avery limited amount of resources and show that this approach works fairly wellto predict the next word.</description>
      <author>example@mail.com (Abisha Thapa Magar, Anup Shakya)</author>
      <guid isPermaLink="false">2505.09649v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
      <link>http://arxiv.org/abs/2505.09178v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniCAD的统一架构，用于解决视觉模型预训练的复杂性和规模增长对多任务辅助诊断系统开发和部署带来的挑战。UniCAD利用预训练视觉模型的能力，处理2D和3D医学图像，同时只需少量特定任务参数。&lt;h4&gt;背景&lt;/h4&gt;随着视觉模型预训练的复杂性和规模的增长，开发和部署多任务辅助诊断系统变得更加困难和资源密集。此外，医学图像社区缺乏一个开源的CAD平台来快速创建高效和可扩展的诊断模型。&lt;h4&gt;目的&lt;/h4&gt;提出UniCAD架构，旨在解决上述问题，实现高效且资源消耗低的医学图像诊断模型。&lt;h4&gt;方法&lt;/h4&gt;UniCAD采用了以下两种关键创新：(1) 效率：使用低秩自适应策略来适应预训练视觉模型到医学图像领域，在性能上与完全微调的模型相当，同时只引入了0.17%的可训练参数。(2) 插拔式：模块化架构，结合冻结的基础模型和多个插拔式专家，实现多样化任务和无缝功能扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同的医学数据集上进行的综合实验表明，UniCAD在准确性和部署效率方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UniCAD为医学图像诊断提供了一种高效、资源消耗低的解决方案，并促进了一个更公平、更高效的研究生态系统。&lt;h4&gt;翻译&lt;/h4&gt;The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging community lacks an open-source CAD platform to enable the rapid creation of efficient and extendable diagnostic models. To address these issues, we propose UniCAD, a unified architecture that leverages the robust capabilities of pre-trained vision foundation models to seamlessly handle both 2D and 3D medical images while requiring only minimal task-specific parameters. UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation strategy is employed to adapt a pre-trained visual model to the medical image domain, achieving performance on par with fully fine-tuned counterparts while introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular architecture that combines a frozen foundation model with multiple plug-and-play experts, enabling diverse tasks and seamless functionality expansion. Building on this unified CAD architecture, we establish an open-source platform where researchers can share and access lightweight CAD experts, fostering a more equitable and efficient research ecosystem. Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency. The source code and project page are available at https://mii-laboratory.github.io/UniCAD/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing complexity and scale of visual model pre-training have madedeveloping and deploying multi-task computer-aided diagnosis (CAD) systemsincreasingly challenging and resource-intensive. Furthermore, the medicalimaging community lacks an open-source CAD platform to enable the rapidcreation of efficient and extendable diagnostic models. To address theseissues, we propose UniCAD, a unified architecture that leverages the robustcapabilities of pre-trained vision foundation models to seamlessly handle both2D and 3D medical images while requiring only minimal task-specific parameters.UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptationstrategy is employed to adapt a pre-trained visual model to the medical imagedomain, achieving performance on par with fully fine-tuned counterparts whileintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modulararchitecture that combines a frozen foundation model with multipleplug-and-play experts, enabling diverse tasks and seamless functionalityexpansion. Building on this unified CAD architecture, we establish anopen-source platform where researchers can share and access lightweight CADexperts, fostering a more equitable and efficient research ecosystem.Comprehensive experiments across 12 diverse medical datasets demonstrate thatUniCAD consistently outperforms existing methods in both accuracy anddeployment efficiency. The source code and project page are available athttps://mii-laboratory.github.io/UniCAD/.</description>
      <author>example@mail.com (Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang)</author>
      <guid isPermaLink="false">2505.09178v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A systematic review of challenges and proposed solutions in modeling multimodal data</title>
      <link>http://arxiv.org/abs/2505.06945v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态数据建模在临床研究中成为一种强大的方法，能够整合多种数据类型，如影像、基因组学、可穿戴传感器和电子健康记录。尽管它有提高诊断准确性和支持个性化护理的潜力，但这种异构数据的建模存在重大技术挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态数据建模作为一种新兴的方法，在临床研究中显示出其重要性。&lt;h4&gt;目的&lt;/h4&gt;系统综述旨在综合69项研究的发现，以识别多模态数据建模中常见的障碍。&lt;h4&gt;方法&lt;/h4&gt;系统综述通过综合69项研究的发现来识别障碍，并强调了最近的方法学进展。&lt;h4&gt;主要发现&lt;/h4&gt;主要障碍包括缺失的模态、样本量有限、维度不平衡、可解释性问题以及找到最佳融合技术。&lt;h4&gt;结论&lt;/h4&gt;通过映射当前趋势和创新，该综述为该领域提供了全面的概述，并为多模态建模在医学应用中的未来研究和发展提供了实际见解。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据建模已成为临床研究中的一种强大方法，能够整合多种数据类型，如影像、基因组学、可穿戴传感器和电子健康记录。尽管它有提高诊断准确性和支持个性化护理的潜力，但这种异构数据的建模存在重大技术挑战。本系统综述综合了69项研究的发现，以识别常见的障碍，包括缺失的模态、样本量有限、维度不平衡、可解释性问题以及找到最佳融合技术。我们强调了最近的方法学进展，如迁移学习、生成模型、注意力机制和神经架构搜索，这些进展提供了有希望的解决方案。通过映射当前趋势和创新，本综述为该领域提供了全面的概述，并为多模态建模在医学应用中的未来研究和发展提供了实际见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data modeling has emerged as a powerful approach in clinicalresearch, enabling the integration of diverse data types such as imaging,genomics, wearable sensors, and electronic health records. Despite itspotential to improve diagnostic accuracy and support personalized care,modeling such heterogeneous data presents significant technical challenges.This systematic review synthesizes findings from 69 studies to identify commonobstacles, including missing modalities, limited sample sizes, dimensionalityimbalance, interpretability issues, and finding the optimal fusion techniques.We highlight recent methodological advances, such as transfer learning,generative models, attention mechanisms, and neural architecture search thatoffer promising solutions. By mapping current trends and innovations, thisreview provides a comprehensive overview of the field and offers practicalinsights to guide future research and development in multimodal modeling formedical applications.</description>
      <author>example@mail.com (Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binder, Nadine Binder)</author>
      <guid isPermaLink="false">2505.06945v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Graph neural networks and MSO</title>
      <link>http://arxiv.org/abs/2505.07816v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提供了一种对现有结果的替代证明，证明使用实数的循环图神经网络在受限于单调二阶逻辑（MSO）时，与分级模态替换演算具有相同的表达能力。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，使用实数的循环图神经网络在MSO逻辑限制下具有特定的表达能力。&lt;h4&gt;目的&lt;/h4&gt;提供对现有结果的替代证明，并考虑接受条件的变体。&lt;h4&gt;方法&lt;/h4&gt;通过构建分布式自动机来捕捉所有在树结构上由MSO定义的节点属性。&lt;h4&gt;主要发现&lt;/h4&gt;证明了循环图神经网络与分级模态替换演算在MSO逻辑限制下具有相同的表达能力。&lt;h4&gt;结论&lt;/h4&gt;该方法通过构建分布式自动机为循环图神经网络的表达能力提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;本文给出了一种对现有结果的替代证明，证明使用实数的循环图神经网络在受限于单调二阶逻辑MSO时，与分级模态替换演算具有相同的表达能力。证明基于构建分布式自动机来捕获所有MSO定义的树节点属性。我们还考虑了一些接受条件的变体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We give an alternative proof for the existing result that recurrent graphneural networks working with reals have the same expressive power inrestriction to monadic second-order logic MSO as the graded modal substitutioncalculus. The proof is based on constructing distributed automata that captureall MSO-definable node properties over trees. We also consider some variants ofthe acceptance conditions.</description>
      <author>example@mail.com (Veeti Ahvonen, Damian Heiman, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07816v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Streaming Sliced Optimal Transport</title>
      <link>http://arxiv.org/abs/2505.06835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 9 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了名为Stream-SW的流式计算SW距离的方法，以增强SOT或SW距离的统计和计算可扩展性。&lt;h4&gt;背景&lt;/h4&gt;SOT或SW距离因其统计和计算可扩展性而被广泛认可。&lt;h4&gt;目的&lt;/h4&gt;进一步增强SW距离的计算可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出了从样本流中计算SW的第一种方法，即Stream-SW。通过引入一维Wasserstein距离的流式计算，并利用样本流的分位数近似技术来定义流式1DW距离，从而得到Stream-SW。&lt;h4&gt;主要发现&lt;/h4&gt;Stream-SW具有低内存复杂度，并在近似误差方面提供理论保证。实验表明，Stream-SW在比较高斯分布和混合高斯分布的流样本时，比随机子采样更精确，且内存消耗更低。此外，在点云分类、点云梯度流和流式变化点检测上的实验进一步展示了Stream-SW的有利性能。&lt;h4&gt;结论&lt;/h4&gt;Stream-SW是一种有效的方法，能够提高SW距离的计算效率，同时保持较高的准确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widelyrecognized for its statistical and computational scalability. In this work, wefurther enhance the computational scalability by proposing the first method forcomputing SW from sample streams, called \emph{streaming sliced Wasserstein}(Stream-SW). To define Stream-SW, we first introduce the streaming computationof the one-dimensional Wasserstein distance. Since the one-dimensionalWasserstein (1DW) distance has a closed-form expression, given by the absolutedifference between the quantile functions of the compared distributions, weleverage quantile approximation techniques for sample streams to define thestreaming 1DW distance. By applying streaming 1DW to all projections, we obtainStream-SW. The key advantage of Stream-SW is its low memory complexity whileproviding theoretical guarantees on the approximation error. We demonstratethat Stream-SW achieves a more accurate approximation of SW than randomsubsampling, with lower memory consumption, in comparing Gaussian distributionsand mixtures of Gaussians from streaming samples. Additionally, we conductexperiments on point cloud classification, point cloud gradient flows, andstreaming change point detection to further highlight the favorable performanceof Stream-SW.</description>
      <author>example@mail.com (Khai Nguyen)</author>
      <guid isPermaLink="false">2505.06835v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
  <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出SeriesBench，一个包含105个精心挑选的叙事驱动系列的视频理解基准，旨在评估MLLMs对复杂连续叙事的理解能力。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型（MLLMs）的快速发展，现有的基准主要针对独立视频，评估视觉元素如人类动作和物体状态，而忽略了复杂连续叙事视频。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准对复杂连续叙事视频理解能力评估不足的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 选择涵盖多种类型的戏剧系列；2. 引入新的长距离叙事标注方法，结合全信息转换方法将人工标注转换为不同任务格式；3. 提出新的叙事推理框架PC-DCoT，以增强模型对剧情结构和人物关系的分析能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有MLLMs在理解叙事驱动系列方面仍面临重大挑战，而PC-DCoT能够帮助这些MLLMs提高性能。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT突出了提高模型理解叙事驱动系列能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zackhxn/seriesbench-cvpr2025&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v3</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Rhomboid Tiling for Geometric Graph Deep Learning</title>
      <link>http://arxiv.org/abs/2505.09586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Rhomboid Tiling (RT)的聚类方法，用于从几何图形数据中提取高级几何结构，并设计了一种基于RT聚类的RTPool模型，用于图分类任务，显示出优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;Graph Neural Networks (GNNs)通过邻域消息传递框架有效地从图结构数据中学习，但它们高度依赖图的连接结构，限制了捕获几何图形中固有丰富几何特征的能力。&lt;h4&gt;目的&lt;/h4&gt;提出Rhomboid Tiling (RT)聚类方法以利用数据中的复杂几何信息，并设计RTPool模型以提高图分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;引入Rhomboid Tiling (RT)聚类，该方法基于菱形镶嵌结构进行聚类，并设计RTPool，一种基于RT聚类的层次图聚类池化模型。&lt;h4&gt;主要发现&lt;/h4&gt;RTPool模型在7个基准数据集上优于21个最先进的竞争对手。&lt;h4&gt;结论&lt;/h4&gt;RT聚类方法和RTPool模型为图分类任务提供了更强大的性能，并有效地提取了数据的几何特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have proven effective for learning fromgraph-structured data through their neighborhood-based message passingframework. Many hierarchical graph clustering pooling methods modify thisframework by introducing clustering-based strategies, enabling the constructionof more expressive and powerful models. However, all of these message passingframework heavily rely on the connectivity structure of graphs, limiting theirability to capture the rich geometric features inherent in geometric graphs. Toaddress this, we propose Rhomboid Tiling (RT) clustering, a novel clusteringmethod based on the rhomboid tiling structure, which performs clustering byleveraging the complex geometric information of the data and effectivelyextracts its higher-order geometric structures. Moreover, we design RTPool, ahierarchical graph clustering pooling model based on RT clustering for graphclassification tasks. The proposed model demonstrates superior performance,outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.</description>
      <author>example@mail.com (Yipeng Zhang, Longlong Li, Kelin Xia)</author>
      <guid isPermaLink="false">2505.09586v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Context Diffusion Policies via Past-Token Prediction</title>
      <link>http://arxiv.org/abs/2505.09561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Videos are available at https://long-context-dp.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于从演示中学习有效的长上下文策略，以解决机器人任务中的长期观察和动作推理问题。&lt;h4&gt;背景&lt;/h4&gt;随着上下文长度的增加，训练长上下文策略变得越来越昂贵，且策略性能往往因为虚假相关性而下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来有效保留过去信息，同时避免因上下文长度增加而导致的训练成本上升和性能下降。&lt;h4&gt;方法&lt;/h4&gt;1. 重新审视模仿学习中的copycat问题，并识别出近期扩散策略中的新挑战。2. 引入Past-Token Prediction (PTP)，一个辅助任务，使策略学习预测过去和未来的动作标记。3. 提出多阶段训练策略：预训练视觉编码器使用短上下文，微调策略头使用缓存的长期上下文嵌入。4. 将PTP扩展为测试时的自我验证机制，使策略在推理过程中能够评分和选择与过去动作一致的候选者。&lt;h4&gt;主要发现&lt;/h4&gt;通过PTP和新的训练策略，显著提高了长上下文扩散策略的性能，同时大幅减少了内存和计算开销。&lt;h4&gt;结论&lt;/h4&gt;实验表明，该方法将长上下文扩散策略的性能提高了3倍，将策略训练速度提高了10倍以上。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在许多机器人任务中，对长序列观察和动作进行推理至关重要。然而，从演示中学习有效的长上下文策略仍然具有挑战性。随着上下文长度的增加，由于内存需求增加，训练成本越来越高，而且策略性能往往因为虚假相关性而下降。最近的方法通常通过截断上下文长度来规避这些问题，丢弃可能对后续决策至关重要的历史信息。在这篇论文中，我们提出了一种替代方法，该方法明确规范了过去信息的保留。我们首先回顾了模仿学习中的copycat问题，并确定了近期扩散政策中的相反挑战：不是过分依赖先前动作，而是往往无法捕捉过去和未来动作之间的基本依赖关系。为了解决这个问题，我们引入了Past-Token Prediction (PTP)，这是一个辅助任务，其中策略学习同时预测过去和未来的动作标记。这种规范显著提高了策略头部的时序建模，同时对视觉表示的依赖最小。基于这一观察，我们进一步引入了一种多阶段训练策略：使用短上下文预训练视觉编码器，并使用缓存的长期上下文嵌入微调策略头。这种策略保留了PTP的好处，同时大大减少了内存和计算开销。最后，我们将PTP扩展为测试时的自我验证机制，使策略能够在推理过程中评分和选择与过去动作一致的候选者。在四个真实世界和六个模拟任务上的实验表明，我们提出的方法通过3倍提高了长上下文扩散策略的性能，并将策略训练速度提高了10倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning over long sequences of observations and actions is essential formany robotic tasks. Yet, learning effective long-context policies fromdemonstrations remains challenging. As context length increases, trainingbecomes increasingly expensive due to rising memory demands, and policyperformance often degrades as a result of spurious correlations. Recent methodstypically sidestep these issues by truncating context length, discardinghistorical information that may be critical for subsequent decisions. In thispaper, we propose an alternative approach that explicitly regularizes theretention of past information. We first revisit the copycat problem inimitation learning and identify an opposite challenge in recent diffusionpolicies: rather than over-relying on prior actions, they often fail to captureessential dependencies between past and future actions. To address this, weintroduce Past-Token Prediction (PTP), an auxiliary task in which the policylearns to predict past action tokens alongside future ones. This regularizationsignificantly improves temporal modeling in the policy head, with minimalreliance on visual representations. Building on this observation, we furtherintroduce a multistage training strategy: pre-train the visual encoder withshort contexts, and fine-tune the policy head using cached long-contextembeddings. This strategy preserves the benefits of PTP while greatly reducingmemory and computational overhead. Finally, we extend PTP into aself-verification mechanism at test time, enabling the policy to score andselect candidates consistent with past actions during inference. Experimentsacross four real-world and six simulated tasks demonstrate that our proposedmethod improves the performance of long-context diffusion policies by 3x andaccelerates policy training by more than 10x.</description>
      <author>example@mail.com (Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2505.09561v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2505.09336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MultiviewVLM，一种用于从3D/4D数据中无监督地学习面部表情多视图对比表示的视觉语言模型。&lt;h4&gt;背景&lt;/h4&gt;该模型旨在解决面部表情识别的问题。&lt;h4&gt;目的&lt;/h4&gt;MultiviewVLM旨在通过多视图对比学习提高面部表情识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型架构整合了由生成的文本提示派生的伪标签，以引导情感语义的隐式对齐。为了捕捉多视图之间的共享信息，它提出了一种联合嵌入空间，无需显式监督即可对齐多视图表示。此外，它采用了一种新颖的多视图对比学习策略，利用稳定的正负样本对采样来增强模型的判别力。还引入了一种梯度友好的损失函数，以促进更平滑和更稳定的收敛，并对分布式训练进行了优化，以确保可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，MultiviewVLM优于现有的最先进方法，并且可以轻松适应各种现实世界应用，只需进行最小的修改。&lt;h4&gt;结论&lt;/h4&gt;MultiviewVLM是一个高效的面部表情识别模型，具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce MultiviewVLM, a vision-language model designedfor unsupervised contrastive multiview representation learning of facialemotions from 3D/4D data. Our architecture integrates pseudo-labels derivedfrom generated textual prompts to guide implicit alignment of emotionalsemantics. To capture shared information across multi-views, we propose a jointembedding space that aligns multiview representations without requiringexplicit supervision. We further enhance the discriminability of our modelthrough a novel multiview contrastive learning strategy that leverages stablepositive-negative pair sampling. A gradient-friendly loss function isintroduced to promote smoother and more stable convergence, and the model isoptimized for distributed training to ensure scalability. Extensive experimentsdemonstrate that MultiviewVLM outperforms existing state-of-the-art methods andcan be easily adapted to various real-world applications with minimalmodifications.</description>
      <author>example@mail.com (Muzammil Behzad)</author>
      <guid isPermaLink="false">2505.09336v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments</title>
      <link>http://arxiv.org/abs/2505.09434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 14 figures, to be published in Frontiers in Robotics and AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中导航机器人群以特定集群行为的研究，并引入了更现实的局部避障策略。&lt;h4&gt;背景&lt;/h4&gt;研究了分布式非线性模型预测控制在未知障碍环境中导航机器人群的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种更现实的局部避障策略，并将其集成到NMPC框架中。&lt;h4&gt;方法&lt;/h4&gt;将局部避障约束使用点云集成到NMPC框架中，并使用点云处理技术来优化计算负担。技术包括方向过滤和下采样，显著减少了数据点的数量。&lt;h4&gt;主要发现&lt;/h4&gt;算法性能通过Gazebo中的真实3D模拟进行了验证，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的算法能够有效实现机器人群的避障，并在实际环境中具有可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our prior work on the distributed nonlinear modelpredictive control (NMPC) for navigating a robot fleet following a certainflocking behavior in unknown obstructed environments with a more realisticlocal obstacle avoidance strategy. More specifically, we integrate the localobstacle avoidance constraint using point clouds into the NMPC framework. Here,each agent relies on data from its local sensor to perceive and respond tonearby obstacles. A point cloud processing technique is presented for bothtwo-dimensional and three-dimensional point clouds to minimize thecomputational burden during the optimization. The process consists ofdirectional filtering and down-sampling that significantly reduce the number ofdata points. The algorithm's performance is validated through realistic 3Dsimulations in Gazebo, and its practical feasibility is further explored viahardware-in-the-loop (HIL) simulations on embedded platforms.</description>
      <author>example@mail.com (Nuthasith Gerdpratoom, Kaoru Yamamoto)</author>
      <guid isPermaLink="false">2505.09434v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation</title>
      <link>http://arxiv.org/abs/2505.09564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at FIMH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于深度学习的猪心脏CT图像分割，提出了一种无需手动标注数据的自训练方法来迭代优化分割标签，以提高分割质量和减少时间上的不一致性。&lt;h4&gt;背景&lt;/h4&gt;心脏图像分割在心脏图像分析和建模任务中至关重要，而深度学习在临床环境中的分割取得了显著进展，但在猪模型等预临床成像中的应用研究有限。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型是否能够为猪心脏CT生成足够准确的伪标签，并提出一种简单的自训练方法来迭代优化这些标签。&lt;h4&gt;方法&lt;/h4&gt;提出的方法无需手动标注猪数据，而是通过迭代更新来提高分割质量，并通过自训练过程提高分割准确性和平滑连续帧之间的时间不一致性。&lt;h4&gt;主要发现&lt;/h4&gt;自训练过程不仅提高了分割准确度，还平滑了连续帧之间的时间不一致性。&lt;h4&gt;结论&lt;/h4&gt;尽管结果令人鼓舞，但仍存在改进空间，例如通过采用更复杂的自训练策略和探索更多的基础模型及其他心脏成像技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiac image segmentation is an important step in many cardiac imageanalysis and modeling tasks such as motion tracking or simulations of cardiacmechanics. While deep learning has greatly advanced segmentation in clinicalsettings, there is limited work on pre-clinical imaging, notably in porcinemodels, which are often used due to their anatomical and physiologicalsimilarity to humans. However, differences between species create a domainshift that complicates direct model transfer from human to pig data.  Recently, foundation models trained on large human datasets have shownpromise for robust medical image segmentation; yet their applicability toporcine data remains largely unexplored. In this work, we investigate whetherfoundation models can generate sufficiently accurate pseudo-labels for pigcardiac CT and propose a simple self-training approach to iteratively refinethese labels. Our method requires no manually annotated pig data, relyinginstead on iterative updates to improve segmentation quality. We demonstratethat this self-training process not only enhances segmentation accuracy butalso smooths out temporal inconsistencies across consecutive frames. Althoughour results are encouraging, there remains room for improvement, for example byincorporating more sophisticated self-training strategies and by exploringadditional foundation models and other cardiac imaging technologies.</description>
      <author>example@mail.com (Anne-Marie Rickmann, Stephanie L. Thorn, Shawn S. Ahn, Supum Lee, Selen Uman, Taras Lysyy, Rachel Burns, Nicole Guerrera, Francis G. Spinale, Jason A. Burdick, Albert J. Sinusas, James S. Duncan)</author>
      <guid isPermaLink="false">2505.09564v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting</title>
      <link>http://arxiv.org/abs/2505.09395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为量子参数自适应（QPA）的混合量子经典框架，用于高效的风暴轨迹预测模型学习，首次将量子机器学习（QML）应用于大规模风暴轨迹预测。&lt;h4&gt;背景&lt;/h4&gt;风暴轨迹预测对于灾害准备至关重要，但由于大气动力学复杂性和深度学习模型的资源需求，计算上具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的风暴轨迹预测方法。&lt;h4&gt;方法&lt;/h4&gt;利用量子神经网络（QNNs）在训练期间生成可训练参数的量子-Train（QT）框架，并将其与注意力机制的Multi-ConvGRU模型结合，实现参数高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;QPA显著减少了可训练参数的数量，同时保持了预测精度，使高性能预测更加可行和可持续。&lt;h4&gt;结论&lt;/h4&gt;QPA为气候建模提供了一种可扩展且节能的方法，是量子机器学习在风暴轨迹预测领域的首次应用，有望提高风暴预测的效率和可持续性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Typhoon trajectory forecasting is essential for disaster preparedness butremains computationally demanding due to the complexity of atmospheric dynamicsand the resource requirements of deep learning models. Quantum-Train (QT), ahybrid quantum-classical framework that leverages quantum neural networks(QNNs) to generate trainable parameters exclusively during training,eliminating the need for quantum hardware at inference time. Building on QT'ssuccess across multiple domains, including image classification, reinforcementlearning, flood prediction, and large language model (LLM) fine-tuning, weintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecastingmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPAenables parameter-efficient training while maintaining predictive accuracy.This work represents the first application of quantum machine learning (QML) tolarge-scale typhoon trajectory prediction, offering a scalable andenergy-efficient approach to climate modeling. Our results demonstrate that QPAsignificantly reduces the number of trainable parameters while preservingperformance, making high-performance forecasting more accessible andsustainable through hybrid quantum-classical learning.</description>
      <author>example@mail.com (Chen-Yu Liu, Kuan-Cheng Chen, Yi-Chien Chen, Samuel Yen-Chi Chen, Wei-Hao Huang, Wei-Jia Huang, Yen-Jui Chang)</author>
      <guid isPermaLink="false">2505.09395v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms</title>
      <link>http://arxiv.org/abs/2505.09103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种雷达惯性里程计，用于在恶劣条件下进行自主定位，并展示了其在处理稀疏和噪声雷达测量方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;4D雷达惯性里程计在恶劣条件下的自主定位具有潜在的应用价值，但其处理稀疏和噪声雷达测量的能力仍是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的雷达惯性里程计方法，以应对稀疏和噪声的雷达测量，并提高点云注册的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种空间加权方法，以适应不均匀分布的点；2. 提出了一种新的点描述直方图，用于挑战性的点注册；3. 提出了一个加权计算模型，以充分利用来自不同空间部分的Doppler速度；4. 构建了一个新的点直方图描述符，结合局部几何特征和雷达截面（RCS）特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过在公共和自建数据集上进行广泛实验，证明了所提出的VGC-RIO方法在精度和鲁棒性方面的优势。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理稀疏和噪声雷达测量以及提高点云注册性能方面是有效的，为4D雷达惯性里程计在恶劣条件下的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in 4D radar-inertial odometry have demonstrated promising potential for autonomous localization in adverse conditions. However, effective handling of sparse and noisy radar measurements remains a critical challenge. In this paper, we propose a radar-inertial odometry with a spatial weighting method that adapts to unevenly distributed points and a novel point-description histogram for challenging point registration. To make full use of the Doppler velocity from different spatial sections, we propose a weighting calculation model. To enhance the point cloud registration performance under challenging scenarios, we construct a novel point histogram descriptor that combines local geometric features and radar cross-section (RCS) features. We have also conducted extensive experiments on both public and self-constructed datasets. The results demonstrate the precision and robustness of the proposed VGC-RIO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D radar-inertial odometry  have demonstrated promising potential for autonomous lo calization in adverseconditions. However, effective handling  of sparse and noisy radar measurements remains a critical  challenge. In this paper, we propose a radar-inertial odometry  with a spatial weighting method that adapts to unevenly  distributed points and a novel point-description histogram  for challenging point registration. To make full use of the  Doppler velocity from different spatial sections, we propose  a weighting calculation model. To enhance the point cloud  registration performance under challenging scenarios, we con struct a novelpoint histogram descriptor that combines local  geometric features and radar cross-section (RCS) features. We  have also conducted extensive experiments on both public and  self-constructed datasets. The results demonstrate the precision  and robustness of the proposed VGC-RIO.</description>
      <author>example@mail.com (Jianguang Xiang, Xiaofeng He, Zizhuo Chen, Lilian Zhang, Xincan Luo, Jun Mao)</author>
      <guid isPermaLink="false">2505.09103v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Fast Learning in Quantitative Finance with Extreme Learning Machine</title>
      <link>http://arxiv.org/abs/2505.09551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文展示了使用单层神经网络（极值学习机，ELM）可以高效解决包括之前使用深度神经网络解决的问题在内的广泛类别的定量金融问题，无需迭代梯度训练。&lt;h4&gt;背景&lt;/h4&gt;定量金融领域中的许多问题之前使用深度神经网络解决，但这些方法通常需要复杂的训练过程。&lt;h4&gt;目的&lt;/h4&gt;探讨使用ELM在定量金融中的各种应用，并比较其与现有方法的性能。&lt;h4&gt;方法&lt;/h4&gt;ELM使用单层网络，具有随机初始化的隐藏节点，通过凸优化方法分析计算输出权重，实现快速训练和推理。在监督学习中，ELM用于学习参数化期权定价函数、预测日内股票回报和完成隐含波动率表面。在无监督学习中，ELM用于数值求解Black-Scholes型偏微分方程。&lt;h4&gt;主要发现&lt;/h4&gt;与深度神经网络、高斯过程回归和逻辑回归相比，ELM在计算速度、准确性和泛化能力方面表现优异。在无监督学习中，ELM在训练速度上优于物理信息神经网络，同时保持了精度。&lt;h4&gt;结论&lt;/h4&gt;ELM被确立为定量金融中各种任务的实用且高效工具。&lt;h4&gt;翻译&lt;/h4&gt;本文证明了在定量金融中，包括那些之前使用深度神经网络解决的问题在内的一系列问题，可以通过使用单层神经网络（极值学习机，ELM）而高效解决，且无需基于迭代的梯度训练。ELM利用一个具有随机初始化的隐藏节点的单层网络，通过凸优化方法获得分析计算得到的输出权重，从而实现快速的训练和推理。本文探讨了监督学习和无监督学习任务。在监督学习中，ELM被用于学习参数化期权定价函数、预测日内股票回报以及完成隐含波动率表面。与深度神经网络、高斯过程回归和逻辑回归相比，ELM在计算速度、准确性和泛化能力方面表现更优。在无监督学习中，ELM数值求解Black-Scholes型偏微分方程，且在训练速度上优于物理信息神经网络，同时保持了精度。本文简要讨论了ELM的逼近能力和泛化能力。研究结果确立了ELM作为定量金融中各种任务的实用且高效工具的地位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper demonstrates that a broad class of problems in quantitativefinance, including those previously addressed using deep neural networks, canbe efficiently solved using single-layer neural networks without iterativegradient-based training, namely extreme learning machine (ELM). ELM utilizes asingle-layer network with randomly initialized hidden nodes and analyticallycomputed output weights obtained via convex optimization, enabling rapidtraining and inference. Both supervised and unsupervised learning tasks areexplored.  In supervised learning, ELM is employed to learn parametric option pricingfunctions, predict intraday stock returns, and complete implied volatilitysurfaces. Compared with deep neural networks, Gaussian process regression, andlogistic regression, ELM achieves higher computational speed, comparableaccuracy, and superior generalization.  In unsupervised learning, ELM numerically solves Black-Scholes-type PDEs, andoutperforms Physics-Informed Neural Networks in training speed without losingprecision. The approximation and generalization abilities of ELM are brieflydiscussed.  The findings establish ELM as a practical and efficient tool for varioustasks in quantitative finance.</description>
      <author>example@mail.com (Liexin Cheng, Xue Cheng, Shuaiqiang Liu)</author>
      <guid isPermaLink="false">2505.09551v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Mixed Precision Quantization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.09361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了用于提高图神经网络（GNN）效率的混合精度量化方法，通过量化GNN层中的多种组件，实现了加速推理而不牺牲预测性能。&lt;h4&gt;背景&lt;/h4&gt;GNN在处理大规模图应用中变得至关重要，但其计算需求促使开发高效方法来加速推理。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的量化消息传递定理，并基于此定理开发混合精度量化框架（MixQ-GNN），以优化GNN层的效率。&lt;h4&gt;方法&lt;/h4&gt;引入一个定理，用于高效量化消息传递以聚合整数消息，并基于此定理开发MixQ-GNN框架，该框架灵活选择GNN层中所有组件的有效整数位宽。&lt;h4&gt;主要发现&lt;/h4&gt;MixQ-GNN与现有GNN量化方法集成，利用其图结构优势实现更高的预测性能，平均而言，在节点分类和图分类任务中，MixQ-GNN相比FP32精度的架构实现了5.5倍和5.1倍的位操作减少。&lt;h4&gt;结论&lt;/h4&gt;MixQ-GNN是一种有效的混合精度量化方法，可以显著提高GNN的推理效率，同时保持预测性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have become essential for handling large-scale graph applications. However, the computational demands of GNNs necessitate the development of efficient methods to accelerate inference. Mixed precision quantization emerges as a promising solution to enhance the efficiency of GNN architectures without compromising prediction performance. Compared to conventional deep learning architectures, GNN layers contain a wider set of components that can be quantized, including message passing functions, aggregation functions, update functions, the inputs, learnable parameters, and outputs of these functions. In this paper, we introduce a theorem for efficient quantized message passing to aggregate integer messages. It guarantees numerical equality of the aggregated messages using integer values with respect to those obtained with full (FP32) precision. Based on this theorem, we introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which flexibly selects effective integer bit-widths for all components within GNN layers. Our approach systematically navigates the wide set of possible bit-width combinations, addressing the challenge of optimizing efficiency while aiming at maintaining comparable prediction performance. MixQ-GNN integrates with existing GNN quantization methods, utilizing their graph structure advantages to achieve higher prediction performance. On average, MixQ-GNN achieved reductions in bit operations of 5.5x for node classification and 5.1x for graph classification compared to architectures represented in FP32 precision.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDE65448.2025.00301&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/samirmoustafa/mixq&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become essential for handling large-scalegraph applications. However, the computational demands of GNNs necessitate thedevelopment of efficient methods to accelerate inference. Mixed precisionquantization emerges as a promising solution to enhance the efficiency of GNNarchitectures without compromising prediction performance. Compared toconventional deep learning architectures, GNN layers contain a wider set ofcomponents that can be quantized, including message passing functions,aggregation functions, update functions, the inputs, learnable parameters, andoutputs of these functions. In this paper, we introduce a theorem for efficientquantized message passing to aggregate integer messages. It guaranteesnumerical equality of the aggregated messages using integer values with respectto those obtained with full (FP32) precision. Based on this theorem, weintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, whichflexibly selects effective integer bit-widths for all components within GNNlayers. Our approach systematically navigates the wide set of possiblebit-width combinations, addressing the challenge of optimizing efficiency whileaiming at maintaining comparable prediction performance. MixQ-GNN integrateswith existing GNN quantization methods, utilizing their graph structureadvantages to achieve higher prediction performance. On average, MixQ-GNNachieved reductions in bit operations of 5.5x for node classification and 5.1xfor graph classification compared to architectures represented in FP32precision.</description>
      <author>example@mail.com (Samir Moustafa, Nils M. Kriege, Wilfried N. Gansterer)</author>
      <guid isPermaLink="false">2505.09361v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Deployable and Generalizable Motion Prediction: Taxonomy, Open Challenges and Future Directions</title>
      <link>http://arxiv.org/abs/2505.09074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Initial draft, 162 pages, 40 figures, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了运动预测领域，包括其背景、目的、方法、主要发现和结论。&lt;h4&gt;背景&lt;/h4&gt;运动预测与人类认知有关，它连接感知和决策，使智能系统如机器人和自动驾驶汽车能够在动态、有人参与的环境中安全行动，并为更广泛的时间序列推理挑战提供信息。&lt;h4&gt;目的&lt;/h4&gt;为了解决研究基准与真实世界复杂性之间的差距，本文调查了运动预测模型的泛化性和可部署性，重点关注机器人、自动驾驶和人体运动的应用。&lt;h4&gt;方法&lt;/h4&gt;本文提供了一个关于运动预测方法的全面分类，包括表示、建模策略、应用领域和评估协议。同时，研究了两个关键挑战：如何使运动预测模型符合现实部署标准，以及如何将模型从有限的已见场景/数据集泛化到开放世界设置。&lt;h4&gt;主要发现&lt;/h4&gt;当最先进的方法在实际世界中应用时，它们往往难以泛化到开放世界条件，并且未能达到部署标准。&lt;h4&gt;结论&lt;/h4&gt;本文强调了运动预测领域的关键开放挑战，旨在指导未来的工作，使社区的努力不仅可衡量，而且对现实应用有意义。&lt;h4&gt;翻译&lt;/h4&gt;摘要：运动预测，即对未来代理状态或场景演变的预测，植根于人类认知，连接感知和决策。它使智能系统，如机器人和自动驾驶汽车，能够在动态、有人参与的环境中安全行动，并为更广泛的时间序列推理挑战提供信息。随着方法、表示和数据集的进步，该领域取得了快速进展，这反映在快速发展的基准结果中。然而，当最先进的方法在现实世界中部署时，它们往往难以泛化到开放世界条件，并且未能达到部署标准。这揭示了研究基准与真实世界复杂性之间的差距。为了解决这一差距，本文重新审视了运动预测模型的泛化性和可部署性，重点介绍了机器人、自动驾驶和人体运动的应用。首先，我们提供了一个关于运动预测方法的全面分类，包括表示、建模策略、应用领域和评估协议。然后，我们研究了两个关键挑战：（1）如何将运动预测模型推向符合现实部署标准的可部署性，其中运动预测不单独行动，而是作为闭环自主堆栈的一个模块发挥作用——它从定位和感知中获取输入，并告知下游规划和控制。（2）如何将运动预测模型从有限的已见场景/数据集泛化到开放世界设置。在整个论文中，我们强调了运动预测领域的关键开放挑战，旨在指导未来的工作，旨在调整社区的努力，促进不仅可衡量而且对现实应用有意义的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion prediction, the anticipation of future agent states or sceneevolution, is rooted in human cognition, bridging perception anddecision-making. It enables intelligent systems, such as robots andself-driving cars, to act safely in dynamic, human-involved environments, andinforms broader time-series reasoning challenges. With advances in methods,representations, and datasets, the field has seen rapid progress, reflected inquickly evolving benchmark results. Yet, when state-of-the-art methods aredeployed in the real world, they often struggle to generalize to open-worldconditions and fall short of deployment standards. This reveals a gap betweenresearch benchmarks, which are often idealized or ill-posed, and real-worldcomplexity.  To address this gap, this survey revisits the generalization anddeployability of motion prediction models, with an emphasis on the applicationsof robotics, autonomous driving, and human motion. We first offer acomprehensive taxonomy of motion prediction methods, covering representations,modeling strategies, application domains, and evaluation protocols. We thenstudy two key challenges: (1) how to push motion prediction models to bedeployable to realistic deployment standards, where motion prediction does notact in a vacuum, but functions as one module of closed-loop autonomy stacks -it takes input from the localization and perception, and informs downstreamplanning and control. 2) how to generalize motion prediction models fromlimited seen scenarios/datasets to the open-world settings. Throughout thepaper, we highlight critical open challenges to guide future work, aiming torecalibrate the community's efforts, fostering progress that is not onlymeasurable but also meaningful for real-world applications.</description>
      <author>example@mail.com (Letian Wang, Marc-Antoine Lavoie, Sandro Papais, Barza Nisar, Yuxiao Chen, Wenhao Ding, Boris Ivanovic, Hao Shao, Abulikemu Abuduweili, Evan Cook, Yang Zhou, Peter Karkus, Jiachen Li, Changliu Liu, Marco Pavone, Steven Waslander)</author>
      <guid isPermaLink="false">2505.09074v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</title>
      <link>http://arxiv.org/abs/2505.09129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 3 figures, 3 tables. The paper proposes a lightweight  weakly-supervised color intelligence model for tactical video anomaly  detection, tested on anonymized African surveillance data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于颜色特征的轻量级异常检测框架，用于在无标签、数据不可用的视频智能环境中快速识别和解释潜在威胁事件。&lt;h4&gt;背景&lt;/h4&gt;在无标签、数据不可用的视频智能环境中部署传统深度学习模型在高风险安全任务中面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在为高敏感性战术任务中的监控视频片段快速识别和解释潜在威胁事件。&lt;h4&gt;方法&lt;/h4&gt;该方法融合了无监督的KMeans聚类与RGB通道直方图建模，以实现关键帧中的结构异常和颜色突变信号的复合检测。&lt;h4&gt;主要发现&lt;/h4&gt;实验使用非洲国家的一项操作监控视频作为研究样本，在无法访问原始数据的情况下成功识别了与高能光源、目标存在和反射干扰相关的多个高度异常帧。&lt;h4&gt;结论&lt;/h4&gt;该结果表明，该方法可以有效用于战术暗杀预警、可疑物体筛查和环境剧变监测，具有较强的部署性和战术解释价值。&lt;h4&gt;翻译&lt;/h4&gt;The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operational surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of traditional deep learning models in high-risk securitytasks in an unlabeled, data-non-exploitable video intelligence environmentfaces significant challenges. In this paper, we propose a lightweight anomalydetection framework based on color features for surveillance video clips in ahigh sensitivity tactical mission, aiming to quickly identify and interpretpotential threat events under resource-constrained and data-sensitiveconditions. The method fuses unsupervised KMeans clustering with RGB channelhistogram modeling to achieve composite detection of structural anomalies andcolor mutation signals in key frames. The experiment takes an operationsurveillance video occurring in an African country as a research sample, andsuccessfully identifies multiple highly anomalous frames related to high-energylight sources, target presence, and reflective interference under the conditionof no access to the original data. The results show that this method can beeffectively used for tactical assassination warning, suspicious objectscreening and environmental drastic change monitoring with strong deployabilityand tactical interpretation value. The study emphasizes the importance of colorfeatures as low semantic battlefield signal carriers, and its battlefieldintelligent perception capability will be further extended by combining graphneural networks and temporal modeling in the future.</description>
      <author>example@mail.com (Wei Meng)</author>
      <guid isPermaLink="false">2505.09129v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories</title>
      <link>http://arxiv.org/abs/2505.09239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures, includes analytical proofs, sensitivity  analysis (95% CI), and JAX-based open-source implementation available at:  https://github.com/farukalpay/information-bottleneck-beta-optimization&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过符号连续性和熵正则化轨迹来实现信息瓶颈（IB）方法的稳定和凸优化。&lt;h4&gt;背景&lt;/h4&gt;信息瓶颈（IB）方法在优化过程中常常遇到不稳定的问题，特别是在IB权衡参数beta的关键点附近会出现表示的突然变化。&lt;h4&gt;目的&lt;/h4&gt;旨在通过引入新的方法，实现IB优化的稳定性和凸性。&lt;h4&gt;方法&lt;/h4&gt;使用符号连续性和熵正则化轨迹进行优化，并提供了关于关键点（beta）的广泛敏感性分析，以及具有统计稳健不确定性量化（95%置信区间）的分析。&lt;h4&gt;主要发现&lt;/h4&gt;证明了包含熵正则化项时IB解路径的凸性和唯一性，并展示了这种方法如何稳定表示学习，即使在广泛的beta值范围内。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法提供了一个清晰的路径，用于实际部署和未来扩展所提出的方法。&lt;h4&gt;翻译&lt;/h4&gt;The Information Bottleneck (IB) method frequently suffers from unstable optimization, characterized by abrupt representation shifts near critical points of the IB trade-off parameter, beta. In this paper, I introduce a novel approach to achieve stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories. I analytically prove convexity and uniqueness of the IB solution path when an entropy regularization term is included, and demonstrate how this stabilizes representation learning across a wide range of {eta} values. Additionally, I provide extensive sensitivity analyses around critical points (beta) with statistically robust uncertainty quantification (95% confidence intervals). The open-source implementation, experimental results, and reproducibility framework included in this work offer a clear path for practical deployment and future extension of my proposed method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/farukalpay/information-bottleneck-beta-optimization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Information Bottleneck (IB) method frequently suffers from unstableoptimization, characterized by abrupt representation shifts near criticalpoints of the IB trade-off parameter, beta. In this paper, I introduce a novelapproach to achieve stable and convex IB optimization through symboliccontinuation and entropy-regularized trajectories. I analytically proveconvexity and uniqueness of the IB solution path when an entropy regularizationterm is included, and demonstrate how this stabilizes representation learningacross a wide range of \b{eta} values. Additionally, I provide extensivesensitivity analyses around critical points (beta) with statistically robustuncertainty quantification (95% confidence intervals). The open-sourceimplementation, experimental results, and reproducibility framework included inthis work offer a clear path for practical deployment and future extension ofmy proposed method.</description>
      <author>example@mail.com (Faruk Alpay)</author>
      <guid isPermaLink="false">2505.09239v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records</title>
      <link>http://arxiv.org/abs/2505.09435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Early accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Endo-CLIP是一种针对内镜图像分析的自监督框架，通过解决非信息性背景图像、复杂医学术语和模糊的多息肉描述等挑战，显著提高了息肉检测和分类的准确性。&lt;h4&gt;背景&lt;/h4&gt;图像文本内镜记录的预训练在提高内镜图像分析方面具有巨大潜力，但面临着非信息性背景图像、复杂医学术语和模糊的多息肉描述等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Endo-CLIP框架，以解决内镜图像分析中的挑战，并提高息肉检测和分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;Endo-CLIP采用三阶段框架：清除、调谐和统一。通过清除背景帧，利用大型语言模型提取临床属性进行细粒度对比学习，以及使用患者级别的交叉注意力来解决多息肉模糊性。&lt;h4&gt;主要发现&lt;/h4&gt;Endo-CLIP在零样本和少样本息肉检测和分类中显著优于最先进的预训练方法。&lt;h4&gt;结论&lt;/h4&gt;Endo-CLIP为更准确和具有临床相关性的内镜分析铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training on image-text colonoscopy records offers substantial potentialfor improving endoscopic image analysis, but faces challenges includingnon-informative background images, complex medical terminology, and ambiguousmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervisedframework that enhances Contrastive Language-Image Pre-training (CLIP) for thisdomain. Endo-CLIP's three-stage framework--cleansing, attunement, andunification--addresses these challenges by (1) removing background frames, (2)leveraging large language models to extract clinical attributes forfine-grained contrastive learning, and (3) employing patient-levelcross-attention to resolve multi-polyp ambiguities. Extensive experimentsdemonstrate that Endo-CLIP significantly outperforms state-of-the-artpre-training methods in zero-shot and few-shot polyp detection andclassification, paving the way for more accurate and clinically relevantendoscopic analysis.</description>
      <author>example@mail.com (Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang)</author>
      <guid isPermaLink="false">2505.09435v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection</title>
      <link>http://arxiv.org/abs/2505.09168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DRRNet的伪装目标检测（COD）方法，旨在解决目标与背景在颜色、纹理和形状上的相似性问题。&lt;h4&gt;背景&lt;/h4&gt;当前COD方法在处理目标与背景相似性时，要么因过度依赖全局语义信息而丢失边缘细节，要么因仅依赖局部特征而被相似背景（如植被图案）干扰。&lt;h4&gt;目的&lt;/h4&gt;提出DRRNet以解决上述问题，实现更精确的伪装目标检测。&lt;h4&gt;方法&lt;/h4&gt;DRRNet采用“上下文-细节融合-细化”的四阶段架构，包括：全局伪装模式提取模块、局部细节提取模块以及形成场景理解和结构感知的双代表模块。解码器中引入反向细化模块，利用空间边缘先验和频域噪声抑制进行两阶段逆细化。&lt;h4&gt;主要发现&lt;/h4&gt;通过两轮逆细化，DRRNet有效抑制了背景干扰并增强了目标边界连续性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，DRRNet在基准数据集上显著优于现有方法。代码可在https://github.com/jerrySunning/DRRNet获取。&lt;h4&gt;翻译&lt;/h4&gt;The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a 'context-detail-fusion-refinement' pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jerrysunning/drrnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The core challenge in Camouflage Object Detection (COD) lies in theindistinguishable similarity between targets and backgrounds in terms of color,texture, and shape. This causes existing methods to either lose edge details(such as hair-like fine structures) due to over-reliance on global semanticinformation or be disturbed by similar backgrounds (such as vegetationpatterns) when relying solely on local features. We propose DRRNet, afour-stage architecture characterized by a "context-detail-fusion-refinement"pipeline to address these issues. Specifically, we introduce an Omni-ContextFeature Extraction Module to capture global camouflage patterns and a LocalDetail Extraction Module to supplement microstructural information for thefull-scene context module. We then design a module for forming dualrepresentations of scene understanding and structural awareness, which fusespanoramic features and local features across various scales. In the decoder, wealso introduce a reverse refinement module that leverages spatial edge priorsand frequency-domain noise suppression to perform a two-stage inverserefinement of the output. By applying two successive rounds of inverserefinement, the model effectively suppresses background interference andenhances the continuity of object boundaries. Experimental results demonstratethat DRRNet significantly outperforms state-of-the-art methods on benchmarkdatasets. Our code is available at https://github.com/jerrySunning/DRRNet.</description>
      <author>example@mail.com (Jianlin Sun, Xiaolin Fang, Juwei Guan, Dongdong Gui, Teqi Wang, Tongxin Zhu)</author>
      <guid isPermaLink="false">2505.09168v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.09422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MoRAL的动态感知多帧4D雷达和LiDAR融合框架，用于鲁棒的3D物体检测。&lt;h4&gt;背景&lt;/h4&gt;可靠的自动驾驶系统需要准确检测交通参与者，多模态融合已成为一种有效策略。&lt;h4&gt;目的&lt;/h4&gt;提高4D雷达和LiDAR融合在3D物体检测中的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个运动感知雷达编码器（MRE）来补偿移动物体引起的帧间雷达失准，并引入了一个运动注意力门控融合（MAGF）模块，将雷达运动特征整合以引导LiDAR特征关注动态前景物体。&lt;h4&gt;主要发现&lt;/h4&gt;在View-of-Delft（VoD）数据集上的广泛评估表明，MoRAL优于现有方法，在整个区域和驾驶通道中分别实现了73.30%和88.68%的最高mAP。特别值得注意的是，该方法在整个区域和驾驶通道中分别实现了69.67%和96.25%的最佳AP，针对行人和骑自行车者。&lt;h4&gt;结论&lt;/h4&gt;MoRAL框架在3D物体检测中表现出色，特别是在检测动态前景物体方面具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable autonomous driving systems require accurate detection of trafficparticipants. To this end, multi-modal fusion has emerged as an effectivestrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frameradar point clouds have demonstrated the effectiveness in bridging the pointdensity gap. However, they often neglect radar point clouds' inter-framemisalignment caused by object movement during accumulation and do not fullyexploit the object dynamic information from 4D radar. In this paper, we proposeMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework forrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) isdesigned to compensate for inter-frame radar misalignment from moving objects.Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motionfeatures to guide LiDAR features to focus on dynamic foreground objects.Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRALoutperforms existing methods, achieving the highest mAP of 73.30% in the entirearea and 88.68% in the driving corridor. Notably, our method also achieves thebest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists inthe driving corridor.</description>
      <author>example@mail.com (Xiangyuan Peng, Yu Wang, Miao Tang, Bierzynski Kay, Lorenzo Servadei, Robert Wille)</author>
      <guid isPermaLink="false">2505.09422v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</title>
      <link>http://arxiv.org/abs/2505.09160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WiMAE和ContraWiMAE两种基于自我监督学习的无线信道表示方法，通过引入对比学习，提升了无线信道表示的学习效果。&lt;h4&gt;背景&lt;/h4&gt;目前应用于无线信道表示的自我监督学习方法往往借鉴了文本和图像处理的方法，未能充分考虑无线通信的独特特性和约束。&lt;h4&gt;目的&lt;/h4&gt;填补无线通信领域自我监督学习方法的空白。&lt;h4&gt;方法&lt;/h4&gt;1. 提出WiMAE，基于Transformer的编码器-解码器基础模型，在真实开源多天线无线信道数据集上预训练。2. 基于WiMAE，开发ContraWiMAE，通过引入对比学习目标，在统一的多任务框架中提升WiMAE的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. WiMAE和ContraWiMAE在多个下游任务中表现出有效性，ContraWiMAE在线性可分性和适应多样性无线环境方面有进一步的提升。2. 与最先进的无线信道基础模型相比，本文提出的模型具有优越的性能和数据效率。&lt;h4&gt;结论&lt;/h4&gt;WiMAE和ContraWiMAE是自我监督无线信道表示学习的有效方法，有望成为未来研究的有力基准。&lt;h4&gt;翻译&lt;/h4&gt;The paper proposes two self-supervised learning methods for wireless channel representation, WiMAE and ContraWiMAE, which enhance the learning effect of wireless channel representation by introducing contrastive learning. WiMAE and ContraWiMAE are effective methods for self-supervised wireless channel representation learning, and are expected to become powerful baselines for future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current applications of self-supervised learning to wireless channelrepresentation often borrow paradigms developed for text and image processing,without fully addressing the unique characteristics and constraints of wirelesscommunications. Aiming to fill this gap, we first propose WiMAE (WirelessMasked Autoencoder), a transformer-based encoder-decoder foundation modelpretrained on a realistic open-source multi-antenna wireless channel dataset.Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE byincorporating a contrastive learning objective alongside the reconstructiontask in a unified multi-task framework. By warm-starting from pretrained WiMAEweights and generating positive pairs via noise injection, the contrastivecomponent enables the model to capture both structural and discriminativefeatures, enhancing representation quality beyond what reconstruction alone canachieve. Through extensive evaluation on unseen scenarios, we demonstrate theeffectiveness of both approaches across multiple downstream tasks, withContraWiMAE showing further improvements in linear separability andadaptability in diverse wireless environments. Comparative evaluations againsta state-of-the-art wireless channel foundation model confirm the superiorperformance and data efficiency of our models, highlighting their potential aspowerful baselines for future research in self-supervised wireless channelrepresentation learning.</description>
      <author>example@mail.com (Berkay Guler, Giovanni Geraci, Hamid Jafarkhani)</author>
      <guid isPermaLink="false">2505.09160v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fair In-Context Learning with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了表格基础模型在结构化数据上的强上下文学习能力，并探讨了其在预测准确性方面的优势与潜在偏见问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型在结构化数据上表现出强大的上下文学习能力，可以不更新参数就准确预测测试集，这使其成为传统梯度提升树方法的竞争者。&lt;h4&gt;目的&lt;/h4&gt;论文旨在探究表格上下文学习能力中的公平性影响，并评估三种预处理策略的有效性。&lt;h4&gt;方法&lt;/h4&gt;论文采用了三种预处理策略：相关性去除、分组平衡演示选择和基于不确定性的演示选择，以减轻偏见。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于不确定性的演示选择策略能够持续提升上下文预测的群体公平性。&lt;h4&gt;结论&lt;/h4&gt;通过使用基于不确定性的演示选择策略，可以有效提升表格上下文学习能力中的群体公平性。&lt;h4&gt;翻译&lt;/h4&gt;Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundational models have exhibited strong in-context learning (ICL)capabilities on structured data, allowing them to make accurate predictions ontest sets without parameter updates, using training examples as context. Thisemerging approach positions itself as a competitive alternative to traditionalgradient-boosted tree methods. However, while biases in conventional machinelearning models are well documented, it remains unclear how these biasesmanifest in tabular ICL. The paper investigates the fairness implications oftabular ICL and explores three preprocessing strategies--correlation removal,group-balanced demonstration selection, and uncertainty-based demonstrationselection--to address bias. Comprehensive experiments indicate thatuncertainty-based demonstration selection consistently enhances group fairnessof in-context predictions. The source code for reproducing the results of thiswork can be found at https://github.com/patrikken/Fair-TabICL.</description>
      <author>example@mail.com (Patrik Kenfack, Samira Ebrahimi Kaho, Ulrich Aïvodji)</author>
      <guid isPermaLink="false">2505.09503v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</title>
      <link>http://arxiv.org/abs/2505.09358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal extension of our CVPR 2024 paper, featuring new tasks,  improved efficiency, high-resolution capabilities, and enhanced accessibility&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Marigold，一套条件生成模型和微调协议，该协议能够从预训练的潜在扩散模型（如Stable Diffusion）中提取知识，并将其应用于密集图像分析任务，包括单目深度估计、表面法线预测和内在分解。&lt;h4&gt;背景&lt;/h4&gt;深度学习在计算机视觉领域的成功依赖于大量标记数据集和强大的预训练模型。在数据稀缺的环境中，这些预训练模型的质量对于有效的迁移学习至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出Marigold模型和微调协议，以解决数据稀缺情况下的密集图像分析任务。&lt;h4&gt;方法&lt;/h4&gt;Marigold模型通过最小修改预训练潜在扩散模型的架构，使用小规模合成数据集在单个GPU上训练几天，并实现最先进的零样本泛化。&lt;h4&gt;主要发现&lt;/h4&gt;Marigold模型能够生成未见内容的真实图像，表明其具有对视觉世界的深入理解。&lt;h4&gt;结论&lt;/h4&gt;Marigold模型在密集图像分析任务中展示了出色的性能，为数据稀缺环境下的计算机视觉应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of deep learning in computer vision over the past decade hashinged on large labeled datasets and strong pretrained models. In data-scarcesettings, the quality of these pretrained models becomes crucial for effectivetransfer learning. Image classification and self-supervised learning havetraditionally been the primary methods for pretraining CNNs andtransformer-based architectures. Recently, the rise of text-to-image generativemodels, particularly those using denoising diffusion in a latent space, hasintroduced a new class of foundational models trained on massive, captionedimage datasets. These models' ability to generate realistic images of unseencontent suggests they possess a deep understanding of the visual world. In thiswork, we present Marigold, a family of conditional generative models and afine-tuning protocol that extracts the knowledge from pretrained latentdiffusion models like Stable Diffusion and adapts them for dense image analysistasks, including monocular depth estimation, surface normals prediction, andintrinsic decomposition. Marigold requires minimal modification of thepre-trained latent diffusion model's architecture, trains with small syntheticdatasets on a single GPU over a few days, and demonstrates state-of-the-artzero-shot generalization. Project page:https://marigoldcomputervision.github.io</description>
      <author>example@mail.com (Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler)</author>
      <guid isPermaLink="false">2505.09358v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Efficient LiDAR Reflectance Compression via Scanning Serialization</title>
      <link>http://arxiv.org/abs/2505.09433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于序列化的神经网络压缩框架SerLiC，用于充分利用激光雷达反射率的内在特性。&lt;h4&gt;背景&lt;/h4&gt;激光雷达点云中的反射率属性对于下游任务至关重要，但在神经网络压缩方法中尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出SerLiC框架，以解决激光雷达反射率在神经网络压缩方法中的利用不足问题。&lt;h4&gt;方法&lt;/h4&gt;SerLiC首先通过扫描顺序序列化将3D激光雷达点云转换为1D序列，然后对每个点进行标记，形成包含其传感器扫描索引、径向距离和先前反射率的上下文表示。为了高效地进行序列建模，SerLiC结合了Mamba和双并行化方案，以实现同时自回归依赖关系捕获和快速处理。&lt;h4&gt;主要发现&lt;/h4&gt;SerLiC在压缩比特数上比原始反射率数据减少了超过2倍，比现有最佳方法减少了高达22%的压缩比特数，同时仅使用了其2%的参数。此外，SerLiC的轻量级版本只需111K个参数即可实现超过10 fps的帧率，这对于实际应用具有吸引力。&lt;h4&gt;结论&lt;/h4&gt;SerLiC是一种有效的神经网络压缩框架，能够显著减少激光雷达反射率的压缩比特数，同时保持较高的处理速度，适用于实际应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reflectance attributes in LiDAR point clouds provide essential informationfor downstream tasks but remain underexplored in neural compression methods. Toaddress this, we introduce SerLiC, a serialization-based neural compressionframework to fully exploit the intrinsic characteristics of LiDAR reflectance.SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-orderserialization, offering a device-centric perspective for reflectance analysis.Each point is then tokenized into a contextual representation comprising itssensor scanning index, radial distance, and prior reflectance, for effectivedependencies exploration. For efficient sequential modeling, Mamba isincorporated with a dual parallelization scheme, enabling simultaneousautoregressive dependency capture and fast processing. Extensive experimentsdemonstrate that SerLiC attains over 2x volume reduction against the originalreflectance data, outperforming the state-of-the-art method by up to 22%reduction of compressed bits while using only 2% of its parameters. Moreover, alightweight version of SerLiC achieves &gt; 10 fps (frames per second) with just111K parameters, which is attractive for real-world applications.</description>
      <author>example@mail.com (Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma)</author>
      <guid isPermaLink="false">2505.09433v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Quotient Complex Transformer (QCformer) for Perovskite Data Analysis</title>
      <link>http://arxiv.org/abs/2505.09174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于商复形的新颖表示方法，并引入了商复形Transformer（QCformer）模型进行材料性质预测，用于解决传统GNN在处理周期结构和高级别相互作用时的局限性。&lt;h4&gt;背景&lt;/h4&gt;新型功能材料的发现对于解决可持续能源生成和气候变化挑战至关重要。混合有机-无机钙钛矿（HOIPs）因其卓越的光电特性在光伏领域受到关注。几何深度学习，特别是图神经网络（GNNs），在预测材料性质和指导材料设计方面显示出强大潜力。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统GNN在处理周期结构和高级别相互作用时的局限性，提出了一种基于商复形的新颖表示方法，并引入QCformer模型进行材料性质预测。&lt;h4&gt;方法&lt;/h4&gt;将材料结构建模为商复形，通过不同维度的单纯形编码成对和多重相互作用，并通过商运算捕捉材料周期性。模型利用单纯形上的高级别特征，并通过基于单纯形的Transformer模块进行处理。在基准数据集（如材料项目和JARVIS）上预训练QCformer，并在HOIP数据集上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;QCformer在HOIP性质预测中优于现有模型，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;商复形表示和QCformer模型为钙钛矿材料的预测建模提供了一个强大的新工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：新型功能材料的发现对于解决可持续能源生成和气候变化挑战至关重要。混合有机-无机钙钛矿（HOIPs）因其卓越的光电特性在光伏领域受到关注。最近，几何深度学习，特别是图神经网络（GNNs），在预测材料性质和指导材料设计方面显示出强大潜力。然而，传统的GNN在处理这种系统中普遍存在的周期结构和高级别相互作用时往往存在困难。为了解决这些限制，我们提出了一种基于商复形（QCs）的新颖表示方法，并引入了商复形Transformer（QCformer）用于材料性质预测。一个材料结构被建模为一个商复形，通过不同维度的单纯形编码成对和多重相互作用，并通过商运算捕捉材料周期性。我们的模型利用在单纯形上定义的高级别特征，并通过基于单纯形的Transformer模块进行处理。我们在基准数据集（如材料项目和JARVIS）上预训练了QCformer，并在HOIP数据集上进行了微调。结果表明，QCformer在HOIP性质预测中优于现有模型，证明了其有效性。商复形表示和QCformer模型共同为钙钛矿材料的预测建模提供了一个强大的新工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of novel functional materials is crucial in addressing thechallenges of sustainable energy generation and climate change. Hybridorganic-inorganic perovskites (HOIPs) have gained attention for theirexceptional optoelectronic properties in photovoltaics. Recently, geometricdeep learning, particularly graph neural networks (GNNs), has shown strongpotential in predicting material properties and guiding material design.However, traditional GNNs often struggle to capture the periodic structures andhigher-order interactions prevalent in such systems. To address theselimitations, we propose a novel representation based on quotient complexes(QCs) and introduce the Quotient Complex Transformer (QCformer) for materialproperty prediction. A material structure is modeled as a quotient complex,which encodes both pairwise and many-body interactions via simplices of varyingdimensions and captures material periodicity through a quotient operation. Ourmodel leverages higher-order features defined on simplices and processes themusing a simplex-based Transformer module. We pretrain QCformer on benchmarkdatasets such as the Materials Project and JARVIS, and fine-tune it on HOIPdatasets. The results show that QCformer outperforms state-of-the-art models inHOIP property prediction, demonstrating its effectiveness. The quotient complexrepresentation and QCformer model together contribute a powerful new tool forpredictive modeling of perovskite materials.</description>
      <author>example@mail.com (Xinyu You, Xiang Liu, Chuan-Shen Hu, Kelin Xia, Tze Chien Sum)</author>
      <guid isPermaLink="false">2505.09174v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities</title>
      <link>http://arxiv.org/abs/2505.09477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE ICRA Workshop on Field Robotics 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了将基础模型（FMs）集成到机器人中，使机器人能够理解自然语言并对其环境中的语义进行推理。文章主要关注在封闭世界设置之外的开放世界中部署FM赋能的机器人。&lt;h4&gt;背景&lt;/h4&gt;现有的FM赋能机器人在封闭世界中操作，这些机器人通常拥有完整的先验地图或对其工作空间有全面视野。&lt;h4&gt;目的&lt;/h4&gt;为了在大型和未结构化的环境中有效完成任务，机器人必须积极探索其环境，导航障碍物丛生的地形，处理意外的传感器输入，并在计算限制下操作。&lt;h4&gt;方法&lt;/h4&gt;本文讨论了在实地机器人设置中最近部署的SPINE（我们的LLM赋能自主框架）。通过初步的模型精炼工作，提出了第一个使用设备上语言模型的驱动UAV规划器。&lt;h4&gt;主要发现&lt;/h4&gt;文章提供了大规模LLM赋能的机器人规划在未结构化环境中的第一个演示，这些任务覆盖了数公里。SPINE对特定的LLM无依赖，这使我们能够提炼出适合在尺寸、重量和功率（SWaP）受限的平台运行的微小语言模型。&lt;h4&gt;结论&lt;/h4&gt;论文最后提出了未来研究的几个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;该论文探讨了将基础模型（FMs）集成到机器人中，使机器人能够理解自然语言并对其环境中的语义进行推理。然而，现有的FM赋能机器人主要在封闭世界中操作，这些机器人通常拥有完整的先验地图或对其工作空间有全面视野。本文旨在解决在开放世界中部署FM赋能机器人所面临的问题，其中任务通常要求机器人能够在大型和未结构化的环境中操作。为了有效地完成这些任务，机器人必须积极探索其环境，导航障碍物丛生的地形，处理意外的传感器输入，并在计算限制下操作。本文讨论了在实地机器人设置中最近部署的SPINE（我们的LLM赋能自主框架）。据我们所知，我们展示了在未结构化环境中进行大规模LLM赋能的机器人规划的第一个实例，这些任务覆盖了数公里。SPINE对特定的LLM无依赖，这使得我们能够提炼出适合在尺寸、重量和功率（SWaP）受限的平台运行的微小语言模型。通过初步的模型精炼工作，我们进一步展示了第一个使用设备上语言模型的驱动UAV规划器。本文最后提出了未来研究的几个有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of foundation models (FMs) into robotics has enabled robotsto understand natural language and reason about the semantics in theirenvironments. However, existing FM-enabled robots primary operate inclosed-world settings, where the robot is given a full prior map or has a fullview of its workspace. This paper addresses the deployment of FM-enabled robotsin the field, where missions often require a robot to operate in large-scaleand unstructured environments. To effectively accomplish these missions, robotsmust actively explore their environments, navigate obstacle-cluttered terrain,handle unexpected sensor inputs, and operate with compute constraints. Wediscuss recent deployments of SPINE, our LLM-enabled autonomy framework, infield robotic settings. To the best of our knowledge, we present the firstdemonstration of large-scale LLM-enabled robot planning in unstructuredenvironments with several kilometers of missions. SPINE is agnostic to aparticular LLM, which allows us to distill small language models capable ofrunning onboard size, weight and power (SWaP) limited platforms. Viapreliminary model distillation work, we then present the first language-drivenUAV planner using on-device language models. We conclude our paper by proposingseveral promising directions for future research.</description>
      <author>example@mail.com (Zachary Ravichandran, Fernando Cladera, Jason Hughes, Varun Murali, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar)</author>
      <guid isPermaLink="false">2505.09477v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Interdependent Cybersecurity Threats Using Bayesian Networks: A Case Study on In-Vehicle Infotainment Systems</title>
      <link>http://arxiv.org/abs/2505.09048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了贝叶斯网络在网络安全风险评估中的应用，强调了其在表示概率依赖性、整合多种威胁指标和支持不确定条件下的推理方面的能力。&lt;h4&gt;背景&lt;/h4&gt;随着网络安全威胁的相互依赖性、不确定性和复杂性的不断演变，传统的评估方法如CVSS、STRIDE和攻击树无法充分捕捉。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示贝叶斯网络在网络安全风险评估中的潜力，并讨论其应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;通过一个结构化案例研究，将基于STRIDE的攻击树转换为贝叶斯网络，使用条件概率表（CPTs）编码逻辑关系，并从标准化DREAD评分中推导威胁可能性。&lt;h4&gt;主要发现&lt;/h4&gt;该模型不仅能够进行系统妥协的可能性概率推理，还支持使用do-calculus和局部敏感性分析进行因果分析，以识别高影响漏洞，从而为威胁传播链中最具影响力的节点提供见解。&lt;h4&gt;结论&lt;/h4&gt;研究认为，通过动态贝叶斯网络、结构学习和自适应推理的未来改进，可以更好地支持复杂环境中的实时网络安全决策。&lt;h4&gt;翻译&lt;/h4&gt;摘要：网络安全威胁日益显现出相互依赖性、不确定性和演变的复杂性挑战，传统的评估方法如CVSS、STRIDE和攻击树无法充分捕捉这些特点。本文回顾了贝叶斯网络（BNs）在网络安全风险评估中的应用，强调了其表示概率依赖性、整合多种威胁指标和支持不确定条件下推理的能力。本文提出的一个结构化案例研究中，将基于STRIDE的攻击树转换为一个贝叶斯网络。使用条件概率表（CPTs）编码逻辑关系，并从标准化的DREAD评分中推导出威胁的可能性。该模型不仅能够进行系统妥协的可能性的概率推理，还支持使用do-calculus和局部敏感性分析进行因果分析，以识别高影响漏洞。这些分析为威胁传播链中最具影响力的节点提供了见解，从而为针对性的缓解策略提供了信息。虽然展示了贝叶斯网络在动态和上下文感知风险评估中的潜力，但该研究也概述了与可扩展性、对专家输入的依赖、静态结构假设以及有限的时间建模相关的局限性。本文最后倡导通过动态贝叶斯网络、结构学习和自适应推理的未来改进，以更好地支持复杂环境中的实时网络安全决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cybersecurity threats are increasingly marked by interdependence,uncertainty, and evolving complexity challenges that traditional assessmentmethods such as CVSS, STRIDE, and attack trees fail to adequately capture. Thispaper reviews the application of Bayesian Networks (BNs) in cybersecurity riskmodeling, highlighting their capacity to represent probabilistic dependencies,integrate diverse threat indicators, and support reasoning under uncertainty. Astructured case study is presented in which a STRIDE-based attack tree for anautomotive In-Vehicle Infotainment (IVI) system is transformed into a BayesianNetwork. Logical relationships are encoded using Conditional Probability Tables(CPTs), and threat likelihoods are derived from normalized DREAD scores. Themodel enables not only probabilistic inference of system compromise likelihoodbut also supports causal analysis using do-calculus and local sensitivityanalysis to identify high-impact vulnerabilities. These analyses provideinsight into the most influential nodes within the threat propagation chain,informing targeted mitigation strategies. While demonstrating the potential ofBNs for dynamic and context-aware risk assessment, the study also outlineslimitations related to scalability, reliance on expert input, static structureassumptions, and limited temporal modeling. The paper concludes by advocatingfor future enhancements through Dynamic Bayesian Networks, structure learning,and adaptive inference to better support real-time cybersecuritydecision-making in complex environments.</description>
      <author>example@mail.com (Sangita Sridar)</author>
      <guid isPermaLink="false">2505.09048v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment</title>
      <link>http://arxiv.org/abs/2505.09372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI2025 early acceptance; First two authors contribute equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MAKE的多方面知识增强视觉语言预训练框架，用于零样本皮肤病学任务，该框架通过综合视觉特征与临床知识来提升皮肤病学诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;皮肤病学诊断是一个复杂的多元挑战，需要结合视觉特征和专业知识。虽然视觉语言预训练（VLP）在医疗人工智能方面取得了进展，但其应用于皮肤病学受限于文本长度和缺乏结构化文本。&lt;h4&gt;目的&lt;/h4&gt;提出MAKE框架，以提升零样本皮肤病学任务中的诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;MAKE框架包含：（1）多方面对比学习策略，通过大型语言模型将临床叙事分解为知识增强的子文本；（2）细粒度对齐机制，将子字幕与诊断相关的图像特征相连接；（3）诊断引导的权重方案，根据临床重要性自适应地优先考虑不同的子字幕。&lt;h4&gt;主要发现&lt;/h4&gt;通过在403,563个皮肤病学图像-文本对上进行预训练，MAKE在八项数据集上的零样本皮肤疾病分类、概念注释和跨模态检索任务中，显著优于最先进的VLP模型。&lt;h4&gt;结论&lt;/h4&gt;MAKE框架在提升皮肤病学诊断的准确性方面表现出色，其代码将在https://github.com/SiyuanYan1/MAKE上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/siyuanyan1/make&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dermatological diagnosis represents a complex multimodal challenge thatrequires integrating visual features with specialized clinical knowledge. Whilevision-language pretraining (VLP) has advanced medical AI, its effectiveness indermatology is limited by text length constraints and the lack of structuredtexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhancedvision-language pretraining framework for zero-shot dermatological tasks.Recognizing that comprehensive dermatological descriptions require multipleknowledge aspects that exceed standard text constraints, our frameworkintroduces: (1) a multi-aspect contrastive learning strategy that decomposesclinical narratives into knowledge-enhanced sub-texts through large languagemodels, (2) a fine-grained alignment mechanism that connects subcaptions withdiagnostically relevant image features, and (3) a diagnosis-guided weightingscheme that adaptively prioritizes different sub-captions based on clinicalsignificance prior. Through pretraining on 403,563 dermatological image-textpairs collected from education resources, MAKE significantly outperformsstate-of-the-art VLP models on eight datasets across zero-shot skin diseaseclassification, concept annotation, and cross-modal retrieval tasks. Our codewill be made publicly available at https: //github.com/SiyuanYan1/MAKE.</description>
      <author>example@mail.com (Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge)</author>
      <guid isPermaLink="false">2505.09372v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning</title>
      <link>http://arxiv.org/abs/2505.09118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强场景图推理（ISGR）框架，旨在通过三个互补组件提升视觉语言模型（VLMs）的交互推理能力。&lt;h4&gt;背景&lt;/h4&gt;传统的场景图主要关注空间关系，这限制了VLMs对视觉场景中复杂交互的推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键挑战：一是传统检测到构建的方法产生的关联关系集缺乏焦点和上下文相关性；二是现有方法未能形成持久的记忆来将交互推理推广到新的场景。&lt;h4&gt;方法&lt;/h4&gt;ISGR框架包含三个互补组件：一是结合SAM空间关系提取和交互感知字幕生成功能显著场景图；二是使用目标交互查询激活VLMs对物体功能的潜在知识；三是引入具有专门交互奖励函数的短期记忆强化学习策略，将短暂模式转化为长期推理启发式规则。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在交互密集的推理基准测试中显著优于基线方法，特别是在复杂场景理解任务上有特别强的改进。&lt;h4&gt;结论&lt;/h4&gt;ISGR框架通过增强VLMs的交互推理能力，显著提升了场景理解任务的表现。&lt;h4&gt;翻译&lt;/h4&gt;Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a one-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional scene graphs primarily focus on spatial relationships, limitingvision-language models' (VLMs) ability to reason about complex interactions invisual scenes. This paper addresses two key challenges: (1) conventionaldetection-to-construction methods produce unfocused, contextually irrelevantrelationship sets, and (2) existing approaches fail to form persistent memoriesfor generalizing interaction reasoning to new scenes. We proposeInteraction-augmented Scene Graph Reasoning (ISGR), a framework that enhancesVLMs' interactional reasoning through three complementary components. First,our dual-stream graph constructor combines SAM-powered spatial relationextraction with interaction-aware captioning to generate functionally salientscene graphs with spatial grounding. Second, we employ targeted interactionqueries to activate VLMs' latent knowledge of object functionalities,converting passive recognition into active reasoning about how objects worktogether. Finally, we introduce a lone-term memory reinforcement learningstrategy with a specialized interaction-focused reward function that transformstransient patterns into long-term reasoning heuristics. Extensive experimentsdemonstrate that our approach significantly outperforms baseline methods oninteraction-heavy reasoning benchmarks, with particularly strong improvementson complex scene understanding tasks. The source code can be accessed athttps://github.com/open_upon_acceptance.</description>
      <author>example@mail.com (Dayong Liang, Changmeng Zheng, Zhiyuan Wen, Yi Cai, Xiao-Yong Wei, Qing Li)</author>
      <guid isPermaLink="false">2505.09118v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
      <link>http://arxiv.org/abs/2505.09017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DyGSSM的新方法，用于动态图表示学习，以解决现有方法在同时提取全局和局部信息以及处理时间依赖性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;大多数动态图表示学习方法将动态图划分为离散快照来捕捉节点随时间的变化行为。现有方法主要使用消息传递和随机游走方法捕获每个快照中节点的局部或全局结构，然后利用序列模型（如transformers）编码节点嵌入的时间演化，并使用元学习技术更新模型参数。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的局限性，即忽略每个快照中同时提取全局和局部信息，以及未考虑模型在当前快照中的性能在参数更新时的作用，导致缺乏时间依赖性管理。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了图卷积网络（GCN）进行局部特征提取和随机游走与门控循环单元（GRU）进行全局特征提取，在每个快照中。然后，使用交叉注意力机制整合局部和全局特征。此外，采用基于HiPPO算法的状态空间模型（SSM）来考虑在更新模型参数时的长期依赖性，确保每个快照中的模型性能为后续更新提供信息。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公开数据集上的实验表明，该方法在20个案例中的17个案例中优于现有的基线和最先进（SOTA）方法。&lt;h4&gt;结论&lt;/h4&gt;DyGSSM方法在动态图表示学习中表现出色，能够更有效地处理时间依赖性和特征提取问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most of the dynamic graph representation learning methods involve dividing adynamic graph into discrete snapshots to capture the evolving behavior of nodesover time. Existing methods primarily capture only local or global structuresof each node within a snapshot using message-passing and random walk-basedmethods. Then, they utilize sequence-based models (e.g., transformers) toencode the temporal evolution of node embeddings, and meta-learning techniquesto update the model parameters. However, these approaches have two limitations.First, they neglect the extraction of global and local informationsimultaneously in each snapshot. Second, they fail to consider the model'sperformance in the current snapshot during parameter updates, resulting in alack of temporal dependency management. Recently, HiPPO (High-order PolynomialProjection Operators) algorithm has gained attention for their ability tooptimize and preserve sequence history in State Space Model (SSM). To addressthe aforementioned limitations in dynamic graph representation learning, wepropose a novel method called Multi-view Dynamic Graph Embeddings with StateSpace Model Gradient Update (DyGSSM). Our approach combines Graph ConvolutionNetworks (GCN) for local feature extraction and random walk with GatedRecurrent Unit (GRU) for global feature extraction in each snapshot. We thenintegrate the local and global features using a cross-attention mechanism.Additionally, we incorporate an SSM based on HiPPO algorithm to account forlong-term dependencies when updating model parameters, ensuring that modelperformance in each snapshot informs subsequent updates. Experiments on fivepublic datasets show that our method outperforms existing baseline andstate-of-the-art (SOTA) methods in 17 out of 20 cases.</description>
      <author>example@mail.com (Bizhan Alipour Pijan, Serdar Bozdag)</author>
      <guid isPermaLink="false">2505.09017v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion</title>
      <link>http://arxiv.org/abs/2505.09424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了模仿学习在机器人操作领域的潜力，并提出了基于SE(3)位姿引导的高效模仿学习方法，用于机器人精确插入任务。&lt;h4&gt;背景&lt;/h4&gt;现有模仿学习方法在精确操作任务上存在精度问题，且依赖于低效的图像/点云观测。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模仿学习方法，以解决机器人精确插入任务中的精度和效率问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出一种精确插入扩散策略，利用相对SE(3)位姿作为观察-动作对；2. 引入RGBD数据到位姿引导扩散策略中，设计一个目标条件RGBD编码器来捕捉当前状态和目标状态之间的差异；3. 提出一种位姿引导残差门控融合方法，以位姿特征为核心，并通过自适应门控机制选择性地补偿位姿特征的不足。&lt;h4&gt;主要发现&lt;/h4&gt;方法在6个机器人精确插入任务上表现出色，仅需要7-10次示范，成功完成精度插入任务，间隙约为0.01mm。&lt;h4&gt;结论&lt;/h4&gt;与现有基准方法相比，该方法在效率和泛化能力方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent studies have proved that imitation learning shows strong potential in the field of robotic manipulation. However, existing methods still struggle with precision manipulation tasks and rely on inefficient image/point cloud observations. In this paper, we explore to introduce SE(3) object pose into imitation learning and propose the pose-guided efficient imitation learning methods for robotic precise insertion task. First, we propose a precise insertion diffusion policy which utilizes the relative SE(3) pose as the observation-action pair. The policy models the source object SE(3) pose trajectory relative to the target object. Second, we explore to introduce the RGBD data to the pose-guided diffusion policy. Specifically, we design a goal-conditioned RGBD encoder to capture the discrepancy between the current state and the goal state. In addition, a pose-guided residual gated fusion method is proposed, which takes pose features as the backbone, and the RGBD features selectively compensate for pose feature deficiencies through an adaptive gating mechanism. Our methods are evaluated on 6 robotic precise insertion tasks, demonstrating competitive performance with only 7-10 demonstrations. Experiments demonstrate that the proposed methods can successfully complete precision insertion tasks with a clearance of about 0.01mm. Experimental results highlight its superior efficiency and generalization capability compared to existing baselines. Code will be available at https://github.com/sunhan1997/PoseInsert.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have proved that imitation learning shows strong potential inthe field of robotic manipulation. However, existing methods still strugglewith precision manipulation task and rely on inefficient image/point cloudobservations. In this paper, we explore to introduce SE(3) object pose intoimitation learning and propose the pose-guided efficient imitation learningmethods for robotic precise insertion task. First, we propose a preciseinsertion diffusion policy which utilizes the relative SE(3) pose as theobservation-action pair. The policy models the source object SE(3) posetrajectory relative to the target object. Second, we explore to introduce theRGBD data to the pose-guided diffusion policy. Specifically, we design agoal-conditioned RGBD encoder to capture the discrepancy between the currentstate and the goal state. In addition, a pose-guided residual gated fusionmethod is proposed, which takes pose features as the backbone, and the RGBDfeatures selectively compensate for pose feature deficiencies through anadaptive gating mechanism. Our methods are evaluated on 6 robotic preciseinsertion tasks, demonstrating competitive performance with only 7-10demonstrations. Experiments demonstrate that the proposed methods cansuccessfully complete precision insertion tasks with a clearance of about 0.01mm. Experimental results highlight its superior efficiency and generalizationcapability compared to existing baselines. Code will be available athttps://github.com/sunhan1997/PoseInsert.</description>
      <author>example@mail.com (Han Sun, Yizhao Wang, Zhenning Zhou, Shuai Wang, Haibo Yang, Jingyuan Sun, Qixin Cao)</author>
      <guid isPermaLink="false">2505.09424v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis</title>
      <link>http://arxiv.org/abs/2505.09329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了在开发可扩展的医疗视觉基础模型时，模型大小、训练算法、数据大小和成像模式之间的扩展行为。提出了BioVFM-21M，一个包含多种生物医学图像模态和解剖结构的规模庞大的数据集，并提出了BioVFM，一个在2100万生物医学图像上预训练的大规模医疗视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;尽管在通用任务上的扩展行为有广泛的研究，但医疗图像与自然数据存在显著差异，因此在医学领域中对扩展行为的理解不足，使得在规模上开发医疗视觉基础模型的关键因素尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过自我监督学习开发可扩展的医疗视觉基础模型，并探究扩展行为在模型大小、训练算法、数据大小和成像模式方面的表现。&lt;h4&gt;方法&lt;/h4&gt;引入了BioVFM-21M数据集，对扩展行为进行了观察和分析，并提出了BioVFM模型。&lt;h4&gt;主要发现&lt;/h4&gt;扩展在提供好处方面是有益的，但效果因任务而异。进一步的分析揭示了与扩展好处相关的几个因素。&lt;h4&gt;结论&lt;/h4&gt;虽然扩展有助于追求更好的性能，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医疗基础模型的关键考虑因素。&lt;h4&gt;翻译&lt;/h4&gt;摘要：扩展模型和数据规模在广泛的任务上展示了令人印象深刻的性能提升。尽管对通用任务的扩展行为进行了广泛的研究，但医学图像与自然数据存在显著差异。由于对医学领域中的扩展行为的理解不足，因此在规模上开发医疗视觉基础模型的关键因素尚不清楚。在本文中，我们通过自我监督学习探索了在开发可扩展的医疗视觉基础模型时，模型大小、训练算法、数据大小和成像模式之间的扩展行为。为了支持可扩展的预训练，我们引入了BioVFM-21M，一个包含广泛的生物医学图像模态和解剖结构的规模庞大的生物医学图像数据集。我们观察到，扩展确实提供了好处，但效果因任务而异。进一步的分析揭示了与扩展好处相关的几个因素。最后，我们提出了BioVFM，一个在2100万生物医学图像上预训练的大规模医疗视觉基础模型，它在12个医学基准上优于以前的最先进的基础模型。我们的结果表明，虽然扩展有助于追求更好的性能，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医疗基础模型的关键考虑因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling up model and data size have demonstrated impressive performanceimprovement over a wide range of tasks. Despite extensive studies on scalingbehaviors for general-purpose tasks, medical images exhibit substantialdifferences from natural data. It remains unclear the key factors in developingmedical vision foundation models at scale due to the absence of an extensiveunderstanding of scaling behavior in the medical domain. In this paper, weexplored the scaling behavior across model sizes, training algorithms, datasizes, and imaging modalities in developing scalable medical vision foundationmodels by self-supervised learning. To support scalable pretraining, weintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing awide range of biomedical image modalities and anatomies. We observed thatscaling up does provide benefits but varies across tasks. Additional analysisreveals several factors correlated with scaling benefits. Finally, we proposeBioVFM, a large-scale medical vision foundation model pretrained on 21 millionbiomedical images, which outperforms the previous state-of-the-art foundationmodels across 12 medical benchmarks. Our results highlight that while scalingup is beneficial for pursuing better performance, task characteristics, datadiversity, pretraining methods, and computational efficiency remain criticalconsiderations for developing scalable medical foundation models.</description>
      <author>example@mail.com (Jiarun Liu, Hong-Yu Zhou, Weijian Huang, Hao Yang, Dongning Song, Tao Tan, Yong Liang, Shanshan Wang)</author>
      <guid isPermaLink="false">2505.09329v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.08614v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WaveGuard是一种主动水印框架，用于提高深度伪造技术中的鲁棒性和不可见性，以应对隐私侵犯和身份盗窃的风险。&lt;h4&gt;背景&lt;/h4&gt;深度伪造技术带来了隐私侵犯和身份盗窃等风险。&lt;h4&gt;目的&lt;/h4&gt;提出WaveGuard以解决深度伪造技术带来的风险。&lt;h4&gt;方法&lt;/h4&gt;WaveGuard通过频率域嵌入和基于图的结构一致性来增强鲁棒性和不可见性。具体方法包括使用DT-CWT将水印嵌入到高频子带，并使用SC-GNN来保持视觉质量，同时设计一个注意力模块来提高嵌入精度。&lt;h4&gt;主要发现&lt;/h4&gt;在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WaveGuard是一种有效的水印框架，可以增强深度伪造技术的鲁棒性和不可见性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：深度伪造技术带来了日益增加的风险，如隐私侵犯和身份盗窃。为了应对这些威胁，我们提出了一种名为WaveGuard的主动水印框架，通过频域嵌入和基于图的结构一致性来提高鲁棒性和不可见性。具体来说，我们使用DT-CWT将水印嵌入到高频子带，并使用SC-GNN来保持视觉质量。我们还设计了一个注意力模块来提高嵌入精度。在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面都优于现有方法。代码可在https://github.com/vpsg-research/WaveGuard获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vpsg-research/waveguard&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake technology poses increasing risks such as privacy invasion andidentity theft. To address these threats, we propose WaveGuard, a proactivewatermarking framework that enhances robustness and imperceptibility viafrequency-domain embedding and graph-based structural consistency.Specifically, we embed watermarks into high-frequency sub-bands using Dual-TreeComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency GraphNeural Network (SC-GNN) to preserve visual quality. We also design an attentionmodule to refine embedding precision. Experimental results on face swap andreenactment tasks demonstrate that WaveGuard outperforms state-of-the-artmethods in both robustness and visual quality. Code is available athttps://github.com/vpsg-research/WaveGuard.</description>
      <author>example@mail.com (Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma)</author>
      <guid isPermaLink="false">2505.08614v2</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Recent Advances in Medical Imaging Segmentation: A Survey</title>
      <link>http://arxiv.org/abs/2505.09274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医学影像在现代医疗保健中扮演关键角色，推动诊断、治疗规划和患者护理的进步。尽管在分割方面取得进展，但稳健的泛化和领域适应性仍然是一个重大挑战。&lt;h4&gt;背景&lt;/h4&gt;医学影像分割由于数据获取、标注复杂性、结构变异性、影像模态变化和隐私限制等因素而成为最具挑战性的问题之一。&lt;h4&gt;目的&lt;/h4&gt;本综述旨在探索医学影像分割的最新进展，重点关注生成式AI、少样本学习、基础模型和通用模型等方法。&lt;h4&gt;方法&lt;/h4&gt;综述提供了这些方法的理论基础、最先进的技术和近期应用的综合概述。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法为长期存在的挑战提供了有希望的解决方案。&lt;h4&gt;结论&lt;/h4&gt;讨论了分割模型固有的局限性、未解决的问题和未来研究方向，旨在提高医学影像分割模型的实用性和可及性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：医学影像是现代医疗保健的基石，推动诊断、治疗规划和患者护理的进步。在众多任务中，分割由于数据获取、标注复杂性、结构变异性、医学影像模态变化和隐私限制等因素，仍然是最具挑战性的问题之一。尽管取得了进展，但实现稳健的泛化和领域适应性仍然是一个重大挑战，尤其是在一些提出的模型资源密集型性质及其对领域专业知识的依赖性。本综述探讨了医学影像分割的最新进展，重点关注生成式AI、少样本学习、基础模型和通用模型等方法。这些方法为长期存在的挑战提供了有希望的解决方案。我们提供了这些方法的理论基础、最先进的技术和近期应用的综合概述。最后，我们讨论了分割模型固有的局限性、未解决的问题和未来研究方向，旨在提高医学影像分割模型的实用性和可及性。我们正在维护一个GitHub存储库（https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation），以继续跟踪和更新该领域的创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging is a cornerstone of modern healthcare, driving advancementsin diagnosis, treatment planning, and patient care. Among its various tasks,segmentation remains one of the most challenging problem due to factors such asdata accessibility, annotation complexity, structural variability, variation inmedical imaging modalities, and privacy constraints. Despite recent progress,achieving robust generalization and domain adaptation remains a significanthurdle, particularly given the resource-intensive nature of some proposedmodels and their reliance on domain expertise. This survey explorescutting-edge advancements in medical image segmentation, focusing onmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, andUniversal Models. These approaches offer promising solutions to longstandingchallenges. We provide a comprehensive overview of the theoretical foundations,state-of-the-art techniques, and recent applications of these methods. Finally,we discuss inherent limitations, unresolved issues, and future researchdirections aimed at enhancing the practicality and accessibility ofsegmentation models in medical imaging. We are maintaining a\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHubRepository} to continue tracking and updating innovations in this field.</description>
      <author>example@mail.com (Fares Bougourzi, Abdenour Hadid)</author>
      <guid isPermaLink="false">2505.09274v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
      <link>http://arxiv.org/abs/2505.09193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first learned video codec that surpasses VTM 13.2 RA across all  standard test datasets. Code will be available at  https://github.com/JiangWeibeta/ECVC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiECVC的基于双向学习的视频压缩框架，旨在提高视频压缩性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于前向预测的视频压缩方法取得了显著成果，但双向视频压缩技术仍处于探索阶段，性能落后于单向方法。&lt;h4&gt;目的&lt;/h4&gt;解决现有双向视频压缩方法在提取多样性和准确性上下文方面的不足，以及缺乏对动态抑制有害上下文的适应性。&lt;h4&gt;方法&lt;/h4&gt;BiECVC通过结合多样化的局部和非局部上下文建模以及自适应上下文门控机制来实现。它重用低层高质量特征，并使用解码运动向量进行对齐，同时采用线性注意力机制来高效建模非局部依赖关系。此外，引入双向上下文门控机制，根据条件编码结果动态过滤上下文信息。&lt;h4&gt;主要发现&lt;/h4&gt;BiECVC在随机访问配置下，比特率比VTC 13.2降低了13.4%和15.7%，在所有标准测试数据集上首次超越了VTC 13.2 RA。&lt;h4&gt;结论&lt;/h4&gt;BiECVC实现了最先进的性能，是第一个在所有标准测试数据集上超越VTC 13.2 RA的学习视频编解码器。&lt;h4&gt;翻译&lt;/h4&gt;摘要：最近基于前向预测的学习视频压缩（LVC）方法取得了令人印象深刻的成果，甚至在低延迟B（LDB）配置下超过了VVC参考软件VTM。相比之下，学习双向视频压缩（BVC）仍然未被充分探索，并且仍然落后于其单向对应物。这种性能差距主要是由于提取多样性和准确性上下文的有限能力：大多数现有的BVC主要利用时间运动，而忽略了帧之间的非局部相关性。此外，它们缺乏对动态抑制来自快速运动或遮挡的有害上下文的适应性。为了解决这些挑战，我们提出了BiECVC，这是一种结合了多样化的局部和非局部上下文建模以及自适应上下文门控的BVC框架。为了增强局部上下文，BiECVC重用底层的高质量特征，并使用解码运动向量进行对齐，而不引入额外的运动开销。为了有效地建模非局部依赖关系，我们采用了一种平衡性能和复杂性的线性注意力机制。为了进一步减轻上下文预测不准确的影响，我们引入了双向上下文门控，受最近自回归语言模型中的数据相关衰减的启发，根据条件编码结果动态过滤上下文信息。大量的实验表明，BiECVC实现了最先进的性能，与VTC 13.2相比，在32和64个周期内分别降低了13.4%和15.7%的比特率。据我们所知，BiECVC是第一个在所有标准测试数据集上超越VTC 13.2 RA的学习视频编解码器。代码将在https://github.com/JiangWeibeta/ECVC上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent forward prediction-based learned video compression (LVC) methods haveachieved impressive results, even surpassing VVC reference software VTM underthe Low Delay B (LDB) configuration. In contrast, learned bidirectional videocompression (BVC) remains underexplored and still lags behind its forward-onlycounterparts. This performance gap is mainly due to the limited ability toextract diverse and accurate contexts: most existing BVCs primarily exploittemporal motion while neglecting non-local correlations across frames.Moreover, they lack the adaptability to dynamically suppress harmful contextsarising from fast motion or occlusion. To tackle these challenges, we proposeBiECVC, a BVC framework that incorporates diversified local and non-localcontext modeling along with adaptive context gating. For local contextenhancement, BiECVC reuses high-quality features from lower layers and alignsthem using decoded motion vectors without introducing extra motion overhead.Tomodel non-local dependencies efficiently, we adopt a linear attention mechanismthat balances performance and complexity. To further mitigate the impact ofinaccurate context prediction, we introduce Bidirectional Context Gating,inspired by data-dependent decay in recent autoregressive language models, todynamically filter contextual information based on conditional coding results.Extensive experiments demonstrate that BiECVC achieves state-of-the-artperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2under the Random Access (RA) configuration with intra periods of 32 and 64,respectively. To our knowledge, BiECVC is the first learned video codec tosurpass VTM 13.2 RA across all standard test datasets. Code will be availableat https://github.com/JiangWeibeta/ECVC.</description>
      <author>example@mail.com (Wei Jiang, Junru Li, Kai Zhang, Li Zhang)</author>
      <guid isPermaLink="false">2505.09193v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning</title>
      <link>http://arxiv.org/abs/2505.09265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于纯视觉基础模型的无监督视觉异常分割方法，通过统一异常分割为变化分割，使用大规模合成图像对进行训练，实现无需语言指导的异常分割。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉异常分割依赖于视觉-语言模型，但这些模型依赖于手动设计的文本提示，且视觉表示与语言独立。&lt;h4&gt;目的&lt;/h4&gt;探索纯视觉基础模型在通用视觉异常分割中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将异常分割统一为变化分割的新范式，利用来自现有图像数据集的大规模合成图像对进行训练，并提出了一个一提示元学习框架（MetaUAS）进行异常分割，并引入了软特征对齐模块来处理提示图像和查询图像之间的几何变化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了无监督的通用异常分割，无需依赖特殊的异常检测数据集和预训练的视觉-语言模型，且在零样本、少样本甚至全样本异常分割方法中表现优异。&lt;h4&gt;结论&lt;/h4&gt;MetaUAS方法有效地实现了仅用一个正常图像提示的异常分割，无需语言指导，在性能上显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a visual-based anomaly segmentation method using a pure vision foundation model, which unifies anomaly segmentation into change segmentation, utilizes large-scale synthetic image pairs for training, and achieves unsupervised universal anomaly segmentation without language guidance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero- and few-shot visual anomaly segmentation relies on powerfulvision-language models that detect unseen anomalies using manually designedtextual prompts. However, visual representations are inherently independent oflanguage. In this paper, we explore the potential of a pure visual foundationmodel as an alternative to widely used vision-language models for universalvisual anomaly segmentation. We present a novel paradigm that unifies anomalysegmentation into change segmentation. This paradigm enables us to leveragelarge-scale synthetic image pairs, featuring object-level and local regionchanges, derived from existing image datasets, which are independent of targetanomaly datasets. We propose a one-prompt Meta-learning framework for UniversalAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset andthen generalizes well to segment any novel or unseen visual anomalies in thereal world. To handle geometrical variations between prompt and query images,we propose a soft feature alignment module that bridges paired-image changeperception and single-image semantic segmentation. This is the first work toachieve universal anomaly segmentation using a pure vision model withoutrelying on special anomaly detection datasets and pre-trained visual-languagemodels. Our method effectively and efficiently segments any anomalies with onlyone normal image prompt and enjoys training-free without guidance fromlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,and even full-shot anomaly segmentation methods. The code and pre-trainedmodels are available at https://github.com/gaobb/MetaUAS.</description>
      <author>example@mail.com (Bin-Bin Gao)</author>
      <guid isPermaLink="false">2505.09265v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians</title>
      <link>http://arxiv.org/abs/2505.09413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的点云渲染方法，通过从点云预测二维高斯来达到照片逼真的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;现有的基于学习的点云渲染方法通常依赖于分类先验、密集点云或额外的优化步骤。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种无需分类先验、密集点云或额外优化的点云渲染方法。&lt;h4&gt;方法&lt;/h4&gt;方法包含两个相同的模块，采用全图块架构，网络可以推广到多个数据集。模块利用点云信息（包括法线、颜色和距离）来归一化和初始化高斯。然后，使用分割解码器通过复制和预测更精确的结果来细化初始高斯，使方法能够有效处理稀疏点云。训练后，该方法可以直接推广到不同类别的点云。预测的高斯可以直接用于渲染，无需对渲染图像进行额外优化，保留了二维高斯的优点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个数据集上取得了优越的泛化性能，并达到了当前的最佳性能（SOTA）。&lt;h4&gt;结论&lt;/h4&gt;该方法在点云渲染方面具有显著优势，并展示了良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/murcherful/gaupcrender&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current learning-based methods predict NeRF or 3D Gaussians from point cloudsto achieve photo-realistic rendering but still depend on categorical priors,dense point clouds, or additional refinements. Hence, we introduce a novelpoint cloud rendering method by predicting 2D Gaussians from point clouds. Ourmethod incorporates two identical modules with an entire-patch architectureenabling the network to be generalized to multiple datasets. The modulenormalizes and initializes the Gaussians utilizing the point cloud informationincluding normals, colors and distances. Then, splitting decoders are employedto refine the initial Gaussians by duplicating them and predicting moreaccurate results, making our methodology effectively accommodate sparse pointclouds as well. Once trained, our approach exhibits direct generalization topoint clouds across different categories. The predicted Gaussians are employeddirectly for rendering without additional refinement on the rendered images,retaining the benefits of 2D Gaussians. We conduct extensive experiments onvarious datasets, and the results demonstrate the superiority andgeneralization of our method, which achieves SOTA performance. The code isavailable athttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.</description>
      <author>example@mail.com (Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen)</author>
      <guid isPermaLink="false">2505.09413v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</title>
      <link>http://arxiv.org/abs/2505.09263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于少样本的异常生成方法AnoGen，用于解决工业检测中异常样本稀缺的问题，通过生成真实且多样化的异常样本来提高异常检测模型的性能。&lt;h4&gt;背景&lt;/h4&gt;由于工业检测中异常样本稀缺，现有的异常检测方法往往通过添加噪声或外部数据来合成异常样本，但合成样本与真实样本之间存在较大的语义差距，导致异常检测性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出AnoGen方法，通过少量真实异常样本引导扩散模型生成真实且多样化的异常样本，以提升异常检测模型的训练效果。&lt;h4&gt;方法&lt;/h4&gt;AnoGen方法分为三个阶段：第一阶段，基于少量真实异常样本学习异常分布，并将所学知识注入嵌入层；第二阶段，使用嵌入层和给定边界框引导扩散模型在特定对象或纹理上生成真实且多样化的异常样本；第三阶段，提出一种弱监督异常检测方法，利用生成的异常样本训练更强大的模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，AnoGen方法生成的异常样本有效提高了异常分类和分割任务的模型性能，例如DRAEM和DseTSeg在分割任务上的AU-PR指标分别提高了5.8%和1.5%。&lt;h4&gt;结论&lt;/h4&gt;AnoGen方法通过生成高质量的异常样本，有效解决了工业检测中异常样本稀缺的问题，为异常检测模型的训练提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, for example, DRAEM and DseTSeg achieved a 5.8% and 1.5% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gaobb/anogen&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a practical and challenging task due to the scarcity ofanomaly samples in industrial inspection. Some existing anomaly detectionmethods address this issue by synthesizing anomalies with noise or externaldata. However, there is always a large semantic gap between synthetic andreal-world anomalies, resulting in weak performance in anomaly detection. Tosolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)method, which guides the diffusion model to generate realistic and diverseanomalies with only a few real anomalies, thereby benefiting training anomalydetection models. Specifically, our work is divided into three stages. In thefirst stage, we learn the anomaly distribution based on a few given realanomalies and inject the learned knowledge into an embedding. In the secondstage, we use the embedding and given bounding boxes to guide the diffusionmodel to generate realistic and diverse anomalies on specific objects (ortextures). In the final stage, we propose a weakly-supervised anomaly detectionmethod to train a more powerful model with generated anomalies. Our methodbuilds upon DRAEM and DesTSeg as the foundation model and conducts experimentson the commonly used industrial anomaly detection dataset, MVTec. Theexperiments demonstrate that our generated anomalies effectively improve themodel performance of both anomaly classification and segmentation taskssimultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvementin AU-PR metric on segmentation task, respectively. The code and generatedanomalous data are available at https://github.com/gaobb/AnoGen.</description>
      <author>example@mail.com (Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu)</author>
      <guid isPermaLink="false">2505.09263v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2505.09140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoDiT-3D是一种拓扑感知扩散Transformer模型，通过瓶颈结构提高了3D点云生成的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的DiT模型在3D点云生成中主要关注局部特征提取，而忽略了全局拓扑信息，如空洞，这对于保持形状一致性和捕捉复杂几何形状至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出TopoDiT-3D模型，以解决现有方法在提取全局拓扑信息方面的不足。&lt;h4&gt;方法&lt;/h4&gt;TopoDiT-3D采用瓶颈结构，使用Perceiver Resampler来整合通过持久同伦提取的拓扑信息，并自适应地过滤掉冗余的局部特征以提高训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TopoDiT-3D在视觉质量、多样性和训练效率方面优于现有模型，并证明了丰富拓扑信息对于3D点云生成的重要性及其与常规局部特征学习的协同作用。&lt;h4&gt;结论&lt;/h4&gt;TopoDiT-3D模型在3D点云生成中取得了显著的性能提升，并展示了拓扑信息与局部特征学习相结合的优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning. Videos and code are available at https://github.com/Zechao-Guan/TopoDiT-3D.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zechao-guan/topodit-3d&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Diffusion Transformer (DiT) models have significantlyimproved 3D point cloud generation. However, existing methods primarily focuson local feature extraction while overlooking global topological information,such as voids, which are crucial for maintaining shape consistency andcapturing complex geometries. To address this limitation, we proposeTopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structurefor 3D point cloud generation. Specifically, we design the bottleneck structureutilizing Perceiver Resampler, which not only offers a mode to integratetopological information extracted through persistent homology into featurelearning, but also adaptively filters out redundant local features to improvetraining efficiency. Experimental results demonstrate that TopoDiT-3Doutperforms state-of-the-art models in visual quality, diversity, and trainingefficiency. Furthermore, TopoDiT-3D demonstrates the importance of richtopological information for 3D point cloud generation and its synergy withconventional local feature learning. Videos and code are available athttps://github.com/Zechao-Guan/TopoDiT-3D.</description>
      <author>example@mail.com (Zechao Guan, Feng Yan, Shuai Du, Lin Ma, Qingshan Liu)</author>
      <guid isPermaLink="false">2505.09140v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
      <link>http://arxiv.org/abs/2505.09178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniCAD的统一架构，用于解决视觉模型预训练的复杂性和规模问题，旨在开发高效的医学图像诊断模型。&lt;h4&gt;背景&lt;/h4&gt;视觉模型预训练的复杂性和规模导致多任务计算机辅助诊断（CAD）系统的开发和部署变得更加困难，同时医学图像社区缺乏一个开源的CAD平台来促进高效和可扩展的诊断模型的快速创建。&lt;h4&gt;目的&lt;/h4&gt;提出UniCAD架构，利用预训练视觉模型的能力，以最小的任务特定参数无缝处理2D和3D医学图像。&lt;h4&gt;方法&lt;/h4&gt;UniCAD引入了两种关键创新：(1) 效率：采用低秩适应策略来适应预训练视觉模型到医学图像领域，同时仅引入0.17%的可训练参数即可实现与完全微调的模型相当的性能；(2) 插件式架构：结合冻结的基础模型和多个插件式专家，实现多样化任务和无缝功能扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同的医学数据集上进行的全面实验表明，UniCAD在准确性和部署效率方面均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UniCAD提供了一个开源平台，研究人员可以共享和访问轻量级的CAD专家，促进更加公平和高效的研究生态系统。&lt;h4&gt;翻译&lt;/h4&gt;随着视觉模型预训练的复杂性和规模的增加，开发和部署多任务计算机辅助诊断（CAD）系统变得越来越具有挑战性和资源密集。此外，医学图像社区缺乏一个开源的CAD平台，以促进高效和可扩展的诊断模型的快速创建。为了解决这些问题，我们提出了UniCAD，一个利用预训练视觉模型强大能力的统一架构，以最小的任务特定参数无缝处理2D和3D医学图像。UniCAD引入了两个关键创新：(1) 效率：采用低秩适应策略将预训练视觉模型适应到医学图像领域，同时仅引入0.17%的可训练参数即可实现与完全微调的模型相当的性能；(2) 插件式：一个模块化架构，结合冻结的基础模型和多个插件式专家，实现多样化任务和无缝功能扩展。基于这个统一的CAD架构，我们建立了一个开源平台，研究人员可以共享和访问轻量级的CAD专家，促进更加公平和高效的研究生态系统。在12个不同的医学数据集上进行的全面实验表明，UniCAD在准确性和部署效率方面均优于现有方法。源代码和项目页面可在https://mii-laboratory.github.io/UniCAD/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing complexity and scale of visual model pre-training have madedeveloping and deploying multi-task computer-aided diagnosis (CAD) systemsincreasingly challenging and resource-intensive. Furthermore, the medicalimaging community lacks an open-source CAD platform to enable the rapidcreation of efficient and extendable diagnostic models. To address theseissues, we propose UniCAD, a unified architecture that leverages the robustcapabilities of pre-trained vision foundation models to seamlessly handle both2D and 3D medical images while requiring only minimal task-specific parameters.UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptationstrategy is employed to adapt a pre-trained visual model to the medical imagedomain, achieving performance on par with fully fine-tuned counterparts whileintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modulararchitecture that combines a frozen foundation model with multipleplug-and-play experts, enabling diverse tasks and seamless functionalityexpansion. Building on this unified CAD architecture, we establish anopen-source platform where researchers can share and access lightweight CADexperts, fostering a more equitable and efficient research ecosystem.Comprehensive experiments across 12 diverse medical datasets demonstrate thatUniCAD consistently outperforms existing methods in both accuracy anddeployment efficiency. The source code and project page are available athttps://mii-laboratory.github.io/UniCAD/.</description>
      <author>example@mail.com (Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang)</author>
      <guid isPermaLink="false">2505.09178v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems</title>
      <link>http://arxiv.org/abs/2505.08816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IFIP Networking 2025. Code available at  https://github.com/koukipp/contrastive_transformers_ids&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于transformer编码器的自监督对比学习方法，用于原始数据包序列上的泛化入侵检测。&lt;h4&gt;背景&lt;/h4&gt;随着数字世界的日益互联，零日攻击的频率和严重性显著增加，迫切需要创新的入侵检测系统（IDS）。&lt;h4&gt;目的&lt;/h4&gt;为了解决基于机器学习的入侵检测系统对标签数据集的依赖以及泛化未见过的流量模式的能力问题。&lt;h4&gt;方法&lt;/h4&gt;采用数据包级数据增强策略结合基于transformer的架构，自动学习全面的包序列表示，以增强异常识别任务和入侵检测的监督学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;该transformer框架在性能上优于现有的基于NetFlow的自监督方法，实现了高达3%的AUC提升（在数据集内评估）和高达20%的AUC提升（在数据集间评估）。此外，该模型在有限的标签数据集上提供了强大的监督入侵检测基线，与预训练和在同一数据集上评估的自监督NetFlow模型相比，AUC提升了1.5%。&lt;h4&gt;结论&lt;/h4&gt;该模型在跨不同数据集微调时表现出适应性，即使在缺乏目标域良性数据的情况下也能展现出强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;As the digital landscape becomes more interconnected, the frequency and severity of zero-day attacks have significantly increased, leading to an urgent need for innovative Intrusion Detection Systems (IDS). Machine Learning-based IDS that learn from the network traffic characteristics and can discern attack patterns from benign traffic offer an advanced solution to traditional signature-based IDS. However, they heavily rely on labeled datasets, and their ability to generalize when encountering unseen traffic patterns remains a challenge. This paper proposes a novel self-supervised contrastive learning approach based on transformer encoders, specifically tailored for generalizable intrusion detection on raw packet sequences. Our proposed learning scheme employs a packet-level data augmentation strategy combined with a transformer-based architecture to extract and generate meaningful representations of traffic flows. Unlike traditional methods reliant on handcrafted statistical features (NetFlow), our approach automatically learns comprehensive packet sequence representations, significantly enhancing performance in anomaly identification tasks and supervised learning for intrusion detection. Our transformer-based framework exhibits better performance in comparison to existing NetFlow self-supervised methods. Specifically, we achieve up to a 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. Moreover, our model provides a strong baseline for supervised intrusion detection with limited labeled data, exhibiting an improvement over self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated on the same dataset. Additionally, we show the adaptability of our pretrained model when fine-tuned across different datasets, demonstrating strong performance even when lacking benign data from the target domain.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the digital landscape becomes more interconnected, the frequency andseverity of zero-day attacks, have significantly increased, leading to anurgent need for innovative Intrusion Detection Systems (IDS). MachineLearning-based IDS that learn from the network traffic characteristics and candiscern attack patterns from benign traffic offer an advanced solution totraditional signature-based IDS. However, they heavily rely on labeleddatasets, and their ability to generalize when encountering unseen trafficpatterns remains a challenge. This paper proposes a novel self-supervisedcontrastive learning approach based on transformer encoders, specificallytailored for generalizable intrusion detection on raw packet sequences. Ourproposed learning scheme employs a packet-level data augmentation strategycombined with a transformer-based architecture to extract and generatemeaningful representations of traffic flows. Unlike traditional methods relianton handcrafted statistical features (NetFlow), our approach automaticallylearns comprehensive packet sequence representations, significantly enhancingperformance in anomaly identification tasks and supervised learning forintrusion detection. Our transformer-based framework exhibits betterperformance in comparison to existing NetFlow self-supervised methods.Specifically, we achieve up to a 3% higher AUC in anomaly detection forintra-dataset evaluation and up to 20% higher AUC scores in inter-datasetevaluation. Moreover, our model provides a strong baseline for supervisedintrusion detection with limited labeled data, exhibiting an improvement overself-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluatedon the same dataset. Additionally, we show the adaptability of our pretrainedmodel when fine-tuned across different datasets, demonstrating strongperformance even when lacking benign data from the target domain.</description>
      <author>example@mail.com (Ippokratis Koukoulis, Ilias Syrigos, Thanasis Korakis)</author>
      <guid isPermaLink="false">2505.08816v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Block-Biased Mamba for Long-Range Sequence Processing</title>
      <link>http://arxiv.org/abs/2505.09022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mamba通过引入输入依赖的动态特性扩展了早期的状态空间模型（SSMs），并在多个领域如语言建模、计算机视觉和基础模型中展现出强大的实证性能。然而，Mamba在长距离序列任务上的表现不佳，这是一个意外的弱点。&lt;h4&gt;背景&lt;/h4&gt;Mamba虽然在长距离依赖架构上构建，但在长距离序列任务上表现不佳，这限制了其通用性和适用性。&lt;h4&gt;目的&lt;/h4&gt;为了提高Mamba的通用性和适用性，本文从表达性、归纳偏置和训练稳定性三个角度分析了Mamba的局限性。&lt;h4&gt;方法&lt;/h4&gt;本文提出了$ext{B}_2ext{S}_6$，这是Mamba的S6单元的一个简单扩展，结合了块状选择性动态和特定通道的偏置。通过理论和实证分析，证明了这些改进有助于提升模型的归纳偏置、表达性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明，与早期的SSMs如S4D相比，Mamba在表达性、归纳偏置和训练稳定性方面存在不足。&lt;h4&gt;结论&lt;/h4&gt;$ext{B}_2ext{S}_6$在长距离序列任务（LRA）上优于S4和S4D，同时在语言建模基准测试中保持了Mamba的性能。&lt;h4&gt;翻译&lt;/h4&gt;Mamba通过引入输入依赖的动态特性扩展了早期的状态空间模型（SSMs），并在多个领域如语言建模、计算机视觉和基础模型中展现出强大的实证性能。然而，尽管构建在适用于长距离依赖的架构上，Mamba在长距离序列任务上的表现不佳。为了理解并解决这一差距，本文从表达性、归纳偏置和训练稳定性三个角度分析了Mamba的局限性。研究提出了$ext{B}_2ext{S}_6$，这是Mamba的S6单元的一个简单扩展，结合了块状选择性动态和特定通道的偏置。理论和实证分析表明，这些改进使模型具备了更好的归纳偏置，提高了其表达性和稳定性。在长距离序列任务（LRA）上，$ext{B}_2ext{S}_6$优于S4和S4D，同时在语言建模基准测试中保持了Mamba的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mamba extends earlier state space models (SSMs) by introducinginput-dependent dynamics, and has demonstrated strong empirical performanceacross a range of domains, including language modeling, computer vision, andfoundation models. However, a surprising weakness remains: despite being builton architectures designed for long-range dependencies, Mamba performs poorly onlong-range sequential tasks. Understanding and addressing this gap is importantfor improving Mamba's universality and versatility. In this work, we analyzeMamba's limitations through three perspectives: expressiveness, inductive bias,and training stability. Our theoretical results show how Mamba falls short ineach of these aspects compared to earlier SSMs such as S4D. To address theseissues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6unit that combines block-wise selective dynamics with a channel-specific bias.We prove that these changes equip the model with a better-suited inductive biasand improve its expressiveness and stability. Empirically,$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) taskswhile maintaining Mamba's performance on language modeling benchmarks.</description>
      <author>example@mail.com (Annan Yu, N. Benjamin Erichson)</author>
      <guid isPermaLink="false">2505.09022v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition</title>
      <link>http://arxiv.org/abs/2505.09073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at the IEEE International Conference on Automatic Face and  Gesture 2025 (FG2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的领域自适应框架，用于解决人脸识别中由于姿态差异导致的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;尽管人脸识别技术取得了进展，但姿态差异仍然是一个根本问题，影响了识别性能。&lt;h4&gt;目的&lt;/h4&gt;提高在姿态差异较大的情况下的人脸识别性能。&lt;h4&gt;方法&lt;/h4&gt;框架通过以下方式实现更好的姿态不变性：(1) 使用共享注意力映射强调2D面部图像和3D面部数据之间的相关模式；(2) 使用联合熵正则化损失来提高一致性，通过利用注意力图增强相交的2D和3D表示之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在FaceScape和ARL-VTF数据集上进行了评估，在姿态不变性方面优于竞争方法，分别实现了至少7.1%和1.57%的TAR@1%FAR改进。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在解决人脸识别中由于姿态差异导致的性能下降问题上表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in facial recognition, there remains a fundamentalissue concerning degradations in performance due to substantial perspective(pose) differences between enrollment and query (probe) imagery. Therefore, wepropose a novel domain adaptive framework to facilitate improved performancesacross large discrepancies in pose by enabling image-based (2D) representationsto infer properties of inherently pose invariant point cloud (3D)representations. Specifically, our proposed framework achieves better poseinvariance by using (1) a shared (joint) attention mapping to emphasize commonpatterns that are most correlated between 2D facial images and 3D facial dataand (2) a joint entropy regularizing loss to promote betterconsistency$\unicode{x2014}$enhancing correlations among the intersecting 2Dand 3D representations$\unicode{x2014}$by leveraging both attention maps. Thisframework is evaluated on FaceScape and ARL-VTF datasets, where it outperformscompetitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and1.57$\unicode{x0025}$, respectively.</description>
      <author>example@mail.com (J. Brennan Peace, Shuowen Hu, Benjamin S. Riggan)</author>
      <guid isPermaLink="false">2505.09073v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery</title>
      <link>http://arxiv.org/abs/2505.08932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Novel Approaches for Precision Agriculture and  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了无人机在森林恢复和监测中的应用，特别是种子撒播，并介绍了一种基于Segment Anything Model (SAM)的森林地面物体分割方法。&lt;h4&gt;背景&lt;/h4&gt;无人机在森林恢复和监测中越来越受欢迎，但由于森林地面的自然变异性高、环境参数快速变化以及定义不明确导致的模糊标注，详细理解森林地面仍是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，论文提出将SAM模型应用于森林地面物体（如树桩、植被和木质残骸）的分割。&lt;h4&gt;方法&lt;/h4&gt;论文采用参数高效的微调（PEFT）来微调一小部分额外模型参数，同时保持原始权重不变。调整SAM的掩码解码器以生成与数据集类别相对应的掩码，实现无需人工提示的自动分割。&lt;h4&gt;主要发现&lt;/h4&gt;基于适配器的PEFT方法实现了最高的平均交并比（mIoU），而低秩适应（LoRA）提供了参数更少的轻量级替代方案，适用于资源受限的无人机平台。&lt;h4&gt;结论&lt;/h4&gt;该方法为无人机在森林地面物体分割方面提供了高效和轻量级的解决方案，有助于提升森林恢复和监测的精确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation andforest monitoring, including seed dispersal in hard-to-reach terrains. However,a detailed understanding of the forest floor remains a challenge due to highnatural variability, quickly changing environmental parameters, and ambiguousannotations due to unclear definitions. To address this issue, we adapt theSegment Anything Model (SAM), a vision foundation model with stronggeneralization capabilities, to segment forest floor objects such as treestumps, vegetation, and woody debris. To this end, we employparameter-efficient fine-tuning (PEFT) to fine-tune a small subset ofadditional model parameters while keeping the original weights fixed. We adjustSAM's mask decoder to generate masks corresponding to our dataset categories,allowing for automatic segmentation without manual prompting. Our results showthat the adapter-based PEFT method achieves the highest mean intersection overunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers alightweight alternative for resource-constrained UAV platforms.</description>
      <author>example@mail.com (Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben)</author>
      <guid isPermaLink="false">2505.08932v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</title>
      <link>http://arxiv.org/abs/2505.08550v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于线性模型的多元时间序列预测模型OLinear，该模型在正交变换域中运行，以提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的预测模型通常采用时间预测范式，直接在时间域中编码和解码时间序列。然而，时间序列数据中的复杂依赖关系可能会阻碍这种范式的性能。&lt;h4&gt;目的&lt;/h4&gt;提高多元时间序列预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用基于正交矩阵的数据自适应变换（OrthoTrans）来对时间序列进行正交变换，以 decorrelated feature domain 中进行更有效的编码和解码。此外，引入了一个定制的线性层NormLin，使用归一化权重矩阵来捕获多元依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;NormLin模块在性能上超过了多头自注意力机制，同时计算量减少了近一半。在24个基准和140个预测任务上的实验表明，OLinear模型在效率上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;OLinear模型作为自注意力的插件替换，可以持续提升基于Transformer的预测器的性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于线性模型的多元时间序列预测模型OLinear，该模型在正交变换域中运行。为了解决时间序列数据中复杂的依赖关系，使用OrthoTrans进行数据自适应变换，并引入了NormLin线性层以捕捉多元依赖关系。实验结果表明，OLinear在效率和性能上都优于现有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jackyue1994/OLinear&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-basedmultivariate time series forecasting model that operates in an$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typicallyadopt the temporal forecast (TF) paradigm, which directly encode and decodetime series in the time domain. However, the entangled step-wise dependenciesin series data can hinder the performance of TF. To address this, someforecasters conduct encoding and decoding in the transformed domain usingfixed, dataset-independent bases (e.g., sine and cosine signals in the Fouriertransform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptivetransformation based on an orthogonal matrix that diagonalizes the series'temporal Pearson correlation matrix. This approach enables more effectiveencoding and decoding in the decorrelated feature domain and can serve as aplug-in module to enhance existing forecasters. To enhance the representationlearning for multivariate time series, we introduce a customized linear layer,$\mathbf{NormLin}$, which employs a normalized weight matrix to capturemultivariate dependencies. Empirically, the NormLin module shows a surprisingperformance advantage over multi-head self-attention, while requiring nearlyhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecastingtasks demonstrate that OLinear consistently achieves state-of-the-artperformance with high efficiency. Notably, as a plug-in replacement forself-attention, the NormLin module consistently enhances Transformer-basedforecasters. The code and datasets are available athttps://anonymous.4open.science/r/OLinear</description>
      <author>example@mail.com (Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi)</author>
      <guid isPermaLink="false">2505.08550v2</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features</title>
      <link>http://arxiv.org/abs/2505.08800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉技术的在线行为监控系统，利用定制化的有向图神经网络（DGNN）对火车司机的状态进行分类，旨在提高铁路安全。&lt;h4&gt;背景&lt;/h4&gt;司机疲劳对铁路安全构成重大挑战，传统系统如死机开关只能提供有限和基本的警报。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确分类司机状态（警觉、不警觉和病态）的系统，以优化铁路安全。&lt;h4&gt;方法&lt;/h4&gt;通过比较三种特征配置（仅骨骼特征、仅面部特征和两者结合）进行消融研究，优化模型输入表示。同时，引入了一个新的数据集，首次将模拟的病态条件纳入火车司机监控。&lt;h4&gt;主要发现&lt;/h4&gt;结合面部和骨骼特征在三类模型中达到最高准确率（80.88%），优于仅使用面部或骨骼特征的模型。此外，这种组合在二元警觉分类中达到超过99%的准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究通过使用基于视觉技术的先进在线监控，在提高铁路安全方面迈出了重要一步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：司机疲劳对铁路安全构成了重大挑战，传统系统如死机开关只能提供有限和基本的警报。本研究提出了一种利用定制化有向图神经网络（DGNN）的在线行为监控系统，用于将火车司机的状态分类为警觉、不警觉和病态三种。为了优化模型的输入表示，进行了消融研究，比较了三种特征配置：仅骨骼特征、仅面部特征和两者结合。实验结果表明，结合面部和骨骼特征在三类模型中取得了最高的准确率（80.88%），优于仅使用面部或骨骼特征的模型。此外，这种组合在二元警觉分类中达到了超过99%的准确率。此外，我们引入了一个新的数据集，首次将模拟的病态条件纳入火车司机监控，扩大了评估与疲劳和健康相关的风险的范围。这项工作通过使用基于视觉技术的先进在线监控，在提高铁路安全方面迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver fatigue poses a significant challenge to railway safety, withtraditional systems like the dead-man switch offering limited and basicalertness checks. This study presents an online behavior-based monitoringsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classifytrain driver's states into three categories: alert, not alert, andpathological. To optimize input representations for the model, an ablationstudy was performed, comparing three feature configurations: skeletal-only,facial-only, and a combination of both. Experimental results show thatcombining facial and skeletal features yields the highest accuracy (80.88%) inthe three-class model, outperforming models using only facial or skeletalfeatures. Furthermore, this combination achieves over 99% accuracy in thebinary alertness classification. Additionally, we introduced a novel datasetthat, for the first time, incorporates simulated pathological conditions intotrain driver monitoring, broadening the scope for assessing risks related tofatigue and health. This work represents a step forward in enhancing railwaysafety through advanced online monitoring using vision-based technologies.</description>
      <author>example@mail.com (Olivia Nocentini, Marta Lagomarsino, Gokhan Solak, Younggeol Cho, Qiyi Tong, Marta Lorenzini, Arash Ajoudani)</author>
      <guid isPermaLink="false">2505.08800v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Machine Learning with Triboelectric Nanogenerators: Optimizing Electrode Materials and Doping Strategies for Intelligent Energy Harves</title>
      <link>http://arxiv.org/abs/2505.07414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将机器学习技术与摩擦纳米发电机（TENGs）相结合的方法，以优化能量收集技术。通过使用图神经网络，该研究框架能够预测并提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;背景&lt;/h4&gt;将机器学习技术与TENGs结合，为优化能量收集技术提供了新的途径。&lt;h4&gt;目的&lt;/h4&gt;利用图神经网络预测和提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;方法&lt;/h4&gt;通过利用大量实验和计算结果的数据集，模型有效地对电极材料进行分类，预测最佳的掺杂比例，并建立稳健的结构-性能关系。&lt;h4&gt;主要发现&lt;/h4&gt;模型发现，铝掺杂的PTFE能量密度提高了65.7%，氟掺杂的PTFE提高了85.7%。PTFE被证明是一种高效的负极材料，当使用铜作为正极时，通过7%的银掺杂，其能量密度可达1.12 J/cm²。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅加速了材料发现，还显著降低了实验成本，为理解影响TENG性能的基本因素提供了新的见解，为智能材料设计建立了一个稳健的平台，推动了可持续能源技术和自供电系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.</description>
      <author>example@mail.com (Guanping Xu, Zirui Zhao, Zhong Lin Wang, Hai-Feng Li)</author>
      <guid isPermaLink="false">2505.07414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
  <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强3D人体运动预测的方法，通过使用估计的姿势和视频数据来减少对昂贵运动捕捉数据的依赖，以提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D人体运动预测方法依赖昂贵的运动捕捉数据，但数据收集成本高，导致数据多样性不足，模型泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提高3D人体运动预测模型的泛化能力，使其能更好地处理未见过的运动或主体。&lt;h4&gt;方法&lt;/h4&gt;将易于获取的视频中的2D姿势通过特定流程转换为运动捕捉风格的3D运动，并通过这些运动进行额外的学习，以适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在定量和定性方面都有显著影响。&lt;h4&gt;结论&lt;/h4&gt;通过使用估计的姿势和视频数据进行额外学习，可以有效提高3D人体运动预测模型的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data</title>
      <link>http://arxiv.org/abs/2505.08736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages; 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。&lt;h4&gt;背景&lt;/h4&gt;现有基于下一个标记预测的方法存在局限性，如VQ-VAE标记化导致的分辨率损失和条件生成能力不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。&lt;h4&gt;方法&lt;/h4&gt;模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。此外，模型在π介子和K介子识别等重建任务中表现出泛化能力，并展示了其微调的能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型通过使用不同的词汇表和因果多头交叉注意力，以及连续动力学条件化和无联合词汇膨胀的标记化方法，有效地解决了现有方法的局限性。&lt;h4&gt;结论&lt;/h4&gt;该模型在核物理领域具有广泛的应用前景，能够提高探测器数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。为了解决现有基于下一个标记预测的方法的局限性，我们提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。我们的模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。我们还展示了我们的模型在π介子和K介子识别等重建任务中的泛化能力，并展示了其微调的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a (proto) Foundation Model for Nuclear Physics, capable ofoperating on low-level detector inputs from Imaging Cherenkov Detectors at thefuture Electron Ion Collider. To address limitations in existing next-tokenprediction approaches-namely resolution loss from VQ-VAE tokenization and lackof conditional generation-we propose three key innovations: (i) separatevocabularies for discrete spatial features and continuous variates, combinedvia Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematicconditioning through prepended context embeddings, and (iii) scalable andsimple, high-resolution continuous variate tokenization without jointvocabulary inflation. Our model enables fast, high-fidelity generation of pixeland time sequences for Cherenkov photons, validated through closure tests inthe High Performance DIRC. We also show our model generalizes to reconstructiontasks such as pion and kaon identification, in which we show its ability toleverage fine-tuning.</description>
      <author>example@mail.com (James Giroux, Cristiano Fanelli)</author>
      <guid isPermaLink="false">2505.08736v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.08725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The dataset and code will be released at  https://github.com/zc-zhao/DriveMonkey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NuInteract的大规模数据集和DriveMonkey框架，用于改进大型视觉语言模型（LVLMs）在场景理解方面的能力。&lt;h4&gt;背景&lt;/h4&gt;LVLMs在图像理解方面取得了显著进展，但在全面场景理解和3D感知方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有LVLMs在场景理解方面的不足，特别是3D感知和指令理解的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NuInteract数据集，包含超过150万张多视角图像语言对，以及DriveMonkey框架，该框架通过可学习的查询将LVLMs与空间处理器无缝集成，并使用预训练的3D检测器初始化空间处理器。&lt;h4&gt;主要发现&lt;/h4&gt;DriveMonkey在3D视觉地面实况任务上优于一般的LVLMs，实现了9.86%的显著提升。&lt;h4&gt;结论&lt;/h4&gt;NuInteract数据集和DriveMonkey框架能够有效提高LVLMs在场景理解方面的性能。&lt;h4&gt;翻译&lt;/h4&gt;The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.</description>
      <author>example@mail.com (Zongchuang Zhao, Haoyu Fu, Dingkang Liang, Xin Zhou, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai)</author>
      <guid isPermaLink="false">2505.08725v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNN-based Precoder Design and Fine-tuning for Cell-free Massive MIMO with Real-world CSI</title>
      <link>http://arxiv.org/abs/2505.08788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无细胞大规模MIMO（CF-mMIMO）技术，并探讨了基于图神经网络（GNN）的预编码方法，通过在合成数据集和真实世界传播环境中进行训练和验证，证明了GNN预编码技术在从合成到真实无线环境中的有效泛化能力。&lt;h4&gt;背景&lt;/h4&gt;CF-mMIMO技术在未来无线网络中提供均匀高质量覆盖具有巨大潜力，但其预编码在分布式系统中的挑战需要解决。&lt;h4&gt;目的&lt;/h4&gt;通过GNN方法解决CF-mMIMO中的预编码挑战，并验证其从合成数据集到真实世界传播环境的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用模拟的信道状态信息（CSI）数据对GNN进行预训练，结合标准传播模型和小尺度瑞利衰落。然后，在物理测试平台上收集的真实CSI测量数据上对模型进行微调，采用层冻结策略以平衡预训练特征和适应真实世界条件。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的GNN模型在20 dB信噪比（SNR）下实现了约8.2比特每信道使用的增益，对应15.7%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;转移学习在GNN预编码中起着关键作用，并表明GNN预编码技术在合成到真实无线环境中的泛化具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm for delivering uniformly high-quality coverage in future wireless networks. To address the inherent challenges of precoding in such distributed systems, recent studies have explored the use of graph neural network (GNN)-based methods, using their powerful representation capabilities. However, these approaches have predominantly been trained and validated on synthetic datasets, leaving their generalizability to real-world propagation environments largely unverified. In this work, we initially pre-train the GNN using simulated channel state information (CSI) data, which incorporates standard propagation models and small-scale Rayleigh fading. Subsequently, we fine-tune the model on real-world CSI measurements collected from a physical testbed equipped with distributed access points (APs). To balance the retention of pre-trained features with adaptation to real-world conditions, we adopt a layer-freezing strategy during fine-tuning, wherein several GNN layers are frozen and only the later layers remain trainable. Numerical results demonstrate that the fine-tuned GNN significantly outperforms the pre-trained model, achieving an approximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR), corresponding to a 15.7 % improvement. These findings highlight the critical role of transfer learning and underscore the potential of GNN-based precoding techniques to effectively generalize from synthetic to real-world wireless environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm fordelivering uniformly high-quality coverage in future wireless networks. Toaddress the inherent challenges of precoding in such distributed systems,recent studies have explored the use of graph neural network (GNN)-basedmethods, using their powerful representation capabilities. However, theseapproaches have predominantly been trained and validated on synthetic datasets,leaving their generalizability to real-world propagation environments largelyunverified. In this work, we initially pre-train the GNN using simulatedchannel state information (CSI) data, which incorporates standard propagationmodels and small-scale Rayleigh fading. Subsequently, we finetune the model onreal-world CSI measurements collected from a physical testbed equipped withdistributed access points (APs). To balance the retention of pre-trainedfeatures with adaptation to real-world conditions, we adopt a layer-freezingstrategy during fine-tuning, wherein several GNN layers are frozen and only thelater layers remain trainable. Numerical results demonstrate that thefine-tuned GNN significantly outperforms the pre-trained model, achieving anapproximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR),corresponding to a 15.7 % improvement. These findings highlight the criticalrole of transfer learning and underscore the potential of GNN-based precodingtechniques to effectively generalize from synthetic to real-world wirelessenvironments.</description>
      <author>example@mail.com (Tianzheng Miao, Thomas Feys, Gilles Callebaut, Jarne Van Mulders, Emanuele Peschiera, Md Arifur Rahman, François Rottenberg)</author>
      <guid isPermaLink="false">2505.08788v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.08614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WaveGuard的主动水印框架，旨在解决Deepfake技术带来的隐私侵犯和身份盗窃风险。&lt;h4&gt;背景&lt;/h4&gt;Deepfake技术存在隐私侵犯和身份盗窃等风险。&lt;h4&gt;目的&lt;/h4&gt;提出WaveGuard框架以应对Deepfake技术的威胁。&lt;h4&gt;方法&lt;/h4&gt;WaveGuard通过频域嵌入和基于图的结构一致性来增强鲁棒性和不可见性。具体来说，使用双树复小波变换（DT-CWT）将水印嵌入到高频子带，并采用结构一致性图神经网络（SC-GNN）来保持视觉质量。此外，还设计了一个注意力模块来提高嵌入精度。&lt;h4&gt;主要发现&lt;/h4&gt;在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WaveGuard框架在应对Deepfake技术威胁方面具有良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake technology poses increasing risks such as privacy invasion andidentity theft. To address these threats, we propose WaveGuard, a proactivewatermarking framework that enhances robustness and imperceptibility viafrequency-domain embedding and graph-based structural consistency.Specifically, we embed watermarks into high-frequency sub-bands using Dual-TreeComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency GraphNeural Network (SC-GNN) to preserve visual quality. We also design an attentionmodule to refine embedding precision. Experimental results on face swap andreenactment tasks demonstrate that WaveGuard outperforms state-of-the-artmethods in both robustness and visual quality. Code is available athttps://github.com/vpsg-research/WaveGuard.</description>
      <author>example@mail.com (Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma)</author>
      <guid isPermaLink="false">2505.08614v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SAR-GTR: Attributed Scattering Information Guided SAR Graph Transformer Recognition Algorithm</title>
      <link>http://arxiv.org/abs/2505.08547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用电磁散射信息进行SAR数据解释的方法，提出了SAR图变换识别算法（SAR-GTR），通过结合GNN和Transformer机制，有效提升了SAR解释的性能。&lt;h4&gt;背景&lt;/h4&gt;电磁散射信息在SAR解释领域是一个重要的研究方向，而GNN能够有效整合专业知识和先验知识，解决SAR解释中的样本限制和泛化问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在深入探究单通道SAR的电磁逆散射信息，并重新审视GNN在SAR解释中的应用限制。&lt;h4&gt;方法&lt;/h4&gt;提出了SAR图变换识别算法（SAR-GTR），该算法通过区分离散和连续参数的映射方法，避免信息混淆和损失。GTR结合了GNN和Transformer机制，并引入边缘信息增强通道，以促进节点和边缘特征的交互式学习，捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了层次拓扑感知系统，充分利用目标的层次结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;SAR-GTR能够有效处理电磁散射参数的属性和特征，并通过边缘信息增强和层次拓扑结构提升SAR解释的性能。&lt;h4&gt;结论&lt;/h4&gt;SAR-GTR算法在ATRNet-STAR大规模车辆数据集上验证了其有效性，为SAR数据解释提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前，利用电磁散射信息进行SAR数据解释是SAR解释领域的一个重要研究方向。图神经网络（GNN）能够有效地整合领域特定的物理知识和人类先验知识，从而缓解了SAR解释中样本可用性有限和泛化能力差等挑战。在本研究中，我们深入研究了单通道SAR的电磁逆散射信息，并重新审视了将GNN应用于SAR解释的限制。我们提出了SAR图变换识别算法（SAR-GTR）。SAR-GTR通过区分离散和连续参数的映射方法，仔细考虑了不同电磁散射参数的属性和特征，从而避免了信息混淆和损失。此外，GTR结合了GNN和Transformer机制，并引入了边缘信息增强通道，以促进节点和边缘特征的交互式学习，从而能够捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了一个层次拓扑感知系统，充分利用了目标的层次结构信息。最后，通过使用ATRNet-STAR大规模车辆数据集验证了该算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Utilizing electromagnetic scattering information for SAR data interpretationis currently a prominent research focus in the SAR interpretation domain. GraphNeural Networks (GNNs) can effectively integrate domain-specific physicalknowledge and human prior knowledge, thereby alleviating challenges such aslimited sample availability and poor generalization in SAR interpretation. Inthis study, we thoroughly investigate the electromagnetic inverse scatteringinformation of single-channel SAR and re-examine the limitations of applyingGNNs to SAR interpretation. We propose the SAR Graph Transformer RecognitionAlgorithm (SAR-GTR). SAR-GTR carefully considers the attributes andcharacteristics of different electromagnetic scattering parameters bydistinguishing the mapping methods for discrete and continuous parameters,thereby avoiding information confusion and loss. Furthermore, the GTR combinesGNNs with the Transformer mechanism and introduces an edge informationenhancement channel to facilitate interactive learning of node and edgefeatures, enabling the capture of robust and global structural characteristicsof targets. Additionally, the GTR constructs a hierarchical topology-awaresystem through global node encoding and edge position encoding, fullyexploiting the hierarchical structural information of targets. Finally, theeffectiveness of the algorithm is validated using the ATRNet-STAR large-scalevehicle dataset.</description>
      <author>example@mail.com (Xuying Xiong, Xinyu Zhang, Weidong Jiang, Li Liu, Yongxiang Liu, Tianpeng Liu)</author>
      <guid isPermaLink="false">2505.08547v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
      <link>http://arxiv.org/abs/2505.08665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SkillFormer，一种用于从自视角和外视角视频中统一多视角技能水平估计的参数高效架构。&lt;h4&gt;背景&lt;/h4&gt;评估复杂活动中的人类技能水平是一个具有应用在体育、康复和培训中的挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的架构，能够从多视角视频中准确评估技能水平。&lt;h4&gt;方法&lt;/h4&gt;SkillFormer基于TimeSformer骨干网络，引入了CrossViewFusion模块，该模块通过多头交叉注意力、可学习门控和自适应自校准融合视图特定特征。此外，利用低秩适应来微调参数，显著降低训练成本。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoExo4D数据集上，SkillFormer在多视角设置中实现了最先进的准确性，同时表现出卓越的计算效率，参数数量比先前基线减少了4.5倍，训练轮数减少了3.75倍。&lt;h4&gt;结论&lt;/h4&gt;SkillFormer在多个结构化任务中表现出色，证实了多视角集成对于精细技能评估的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Assessing human skill levels in complex activities is a challenging problemwith applications in sports, rehabilitation, and training. In this work, wepresent SkillFormer, a parameter-efficient architecture for unified multi-viewproficiency estimation from egocentric and exocentric videos. Building on theTimeSformer backbone, SkillFormer introduces a CrossViewFusion module thatfuses view-specific features using multi-head cross-attention, learnablegating, and adaptive self-calibration. We leverage Low-Rank Adaptation tofine-tune only a small subset of parameters, significantly reducing trainingcosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achievesstate-of-the-art accuracy in multi-view settings while demonstrating remarkablecomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewertraining epochs than prior baselines. It excels in multiple structured tasks,confirming the value of multi-view integration for fine-grained skillassessment.</description>
      <author>example@mail.com (Edoardo Bianchi, Antonio Liotta)</author>
      <guid isPermaLink="false">2505.08665v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.08361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WM3C的强化学习新框架，通过学习利用组合因果组件来提升强化学习在未知环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在遇到具有未知动态的新环境时，泛化能力仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在通过借鉴人类的组合推理能力，提高强化学习在未知环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;WM3C框架通过识别和利用可组合元素之间的因果动力学，将语言作为一种组合模式，将潜在空间分解为有意义的组件，并使用掩码自动编码器和自适应稀疏正则化来捕获高级语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;WM3C在识别潜在过程、改进策略学习和泛化到未见任务方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WM3C为强化学习在未知环境中的泛化提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization in reinforcement learning (RL) remains a significantchallenge, especially when agents encounter novel environments with unseendynamics. Drawing inspiration from human compositional reasoning -- where knowncomponents are reconfigured to handle new situations -- we introduce WorldModeling with Compositional Causal Components (WM3C). This novel frameworkenhances RL generalization by learning and leveraging compositional causalcomponents. Unlike previous approaches focusing on invariant representationlearning or meta-learning, WM3C identifies and utilizes causal dynamics amongcomposable elements, facilitating robust adaptation to new tasks. Our approachintegrates language as a compositional modality to decompose the latent spaceinto meaningful components and provides theoretical guarantees for their uniqueidentification under mild assumptions. Our practical implementation uses amasked autoencoder with mutual information constraints and adaptive sparsityregularization to capture high-level semantic information and effectivelydisentangle transition dynamics. Experiments on numerical simulations andreal-world robotic manipulation tasks demonstrate that WM3C significantlyoutperforms existing methods in identifying latent processes, improving policylearning, and generalizing to unseen tasks.</description>
      <author>example@mail.com (Xinyue Wang, Biwei Huang)</author>
      <guid isPermaLink="false">2505.08361v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding</title>
      <link>http://arxiv.org/abs/2505.08194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLTP的语言触觉预训练框架，用于实现触觉感知与视觉-语言模型（VLMs）的整合，以增强机器人多模态感知能力。&lt;h4&gt;背景&lt;/h4&gt;现有的触觉描述主要限于表面属性，如纹理，忽略了机器人操作中关键的接触状态。&lt;h4&gt;目的&lt;/h4&gt;提出CLTP框架，以实现接触状态感知的触觉语言理解，从而提高机器人操作任务的效果。&lt;h4&gt;方法&lt;/h4&gt;收集了包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（如接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体文本和触觉模态。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了CLTP在零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互等三个下游任务中的优越性。&lt;h4&gt;结论&lt;/h4&gt;CLTP是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;近期将触觉感知与视觉-语言模型（VLMs）整合的研究取得了显著进展，展示了在机器人多模态感知方面的巨大潜力。然而，现有的触觉描述仅限于表面属性，如纹理，忽略了机器人操作中至关重要的接触状态。为了弥合这一差距，我们提出了CLTP，一个直观且有效的语言触觉预训练框架，它将触觉3D点云与自然语言对齐于各种接触场景中，从而实现接触状态感知的触觉语言理解，为富含接触操作的机器人任务提供支持。我们首先收集了一个包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（例如，接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体的文本和触觉模态。实验验证了它在三个下游任务中的优越性：零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互。据我们所知，这是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。代码和数据集已开源，网址为https://sites.google.com/view/cltp/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in integrating tactile sensing with vision-languagemodels (VLMs) have demonstrated remarkable potential for robotic multimodalperception. However, existing tactile descriptions remain limited tosuperficial attributes like texture, neglecting critical contact statesessential for robotic manipulation. To bridge this gap, we propose CLTP, anintuitive and effective language tactile pretraining framework that alignstactile 3D point clouds with natural language in various contact scenarios,thus enabling contact-state-aware tactile language understanding forcontact-rich manipulation tasks. We first collect a novel dataset of 50k+tactile 3D point cloud-language pairs, where descriptions explicitly capturemultidimensional contact states (e.g., contact location, shape, and force) fromthe tactile sensor's perspective. CLTP leverages a pre-aligned and frozenvision-language feature space to bridge holistic textual and tactilemodalities. Experiments validate its superiority in three downstream tasks:zero-shot 3D classification, contact state classification, and tactile 3D largelanguage model (LLM) interaction. To the best of our knowledge, this is thefirst study to align tactile and language representations from the contactstate perspective for manipulation tasks, providing great potential fortactile-language-action model learning. Code and datasets are open-sourced athttps://sites.google.com/view/cltp/.</description>
      <author>example@mail.com (Wenxuan Ma, Xiaoge Cao, Yixiang Zhang, Chaofan Zhang, Shaobo Yang, Peng Hao, Bin Fang, Yinghao Cai, Shaowei Cui, Shuo Wang)</author>
      <guid isPermaLink="false">2505.08194v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
      <link>http://arxiv.org/abs/2505.08430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的邻近上下文聚合框架（GNCAF），用于对全切片图像（WSI）中的三级淋巴结构（TLS）进行语义分割，以改善TLS成熟度和面积的量化，从而提高预后任务的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的TLS评估方法通常依赖于细胞代理任务，并需要额外的后处理步骤。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TLS语义分割任务（TLS-SS），以端到端的方式在WSI中分割TLS的区域和成熟阶段。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于GNN的邻近上下文聚合框架（GNCAF），该框架通过逐步聚合目标及其邻近区域的多跳上下文信息，并使用自注意力机制来指导目标区域的分割。&lt;h4&gt;主要发现&lt;/h4&gt;GNCAF能够与各种分割模型集成，增强其感知超出局部区域上下文信息的能力。在TCGA-COAD和INHOUSE-PAAD数据集上的实验表明，GNCAF在mF1和mIoU方面分别实现了22.08%和26.57%的最大提升。此外，GNCAF在淋巴结转移分割任务中也验证了其任务的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GNCAF是一种有效的TLS语义分割方法，能够显著提高TLS分割的准确性，并具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tertiary lymphoid structures (TLS) are organized clusters of immune cells,whose maturity and area can be quantified in whole slide image (WSI) forvarious prognostic tasks. Existing methods for assessing these characteristicstypically rely on cell proxy tasks and require additional post-processingsteps. In this work, We focus on a novel task-TLS Semantic Segmentation(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI inan end-to-end manner. Due to the extensive scale of WSI and patch-basedsegmentation strategies, TLS-SS necessitates integrating from neighboringpatches to guide target patch (target) segmentation. Previous techniques oftenemploy on multi-resolution approaches, constraining the capacity to leveragethe broader neighboring context while tend to preserve coarse-grainedinformation. To address this, we propose a GNN-based Neighboring ContextAggregation Framework (GNCAF), which progressively aggregates multi-hopneighboring context from the target and employs a self-attention mechanism toguide the segmentation of the target. GNCAF can be integrated with varioussegmentation models to enhance their ability to perceive contextual informationoutside of the patch. We build two TLS-SS datasets, called TCGA-COAD andINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publiclyavailable. Experiments on these datasets demonstrate the superiority of GNCAF,achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,respectively. Additionally, we also validate the task scalability of GNCAF onsegmentation of lymph node metastases.</description>
      <author>example@mail.com (Lei Su)</author>
      <guid isPermaLink="false">2505.08430v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2505.08101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的蒸馏框架，用于在资源受限环境中部署高性能模型，如Point Transformer V3，通过拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型转移到轻量级学生模型。&lt;h4&gt;背景&lt;/h4&gt;点云处理在自动驾驶和3D物体识别等应用中扮演着关键角色，但将高性能模型部署在资源受限环境中由于计算和内存需求高而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的蒸馏框架，以在资源受限环境中部署高性能点云处理模型。&lt;h4&gt;方法&lt;/h4&gt;该方法利用拓扑感知表示和梯度引导的知识蒸馏，同时捕捉点云的底层几何结构，并通过基于梯度的特征对齐指导学生模型的训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，该方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。在NuScenes数据集上，该方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。&lt;h4&gt;结论&lt;/h4&gt;提出的蒸馏框架在保持高性能的同时，显著降低了模型的计算和内存需求，适用于资源受限的环境。&lt;h4&gt;翻译&lt;/h4&gt;点云处理由于在自动驾驶和3D物体识别等应用中的关键作用而受到广泛关注。然而，由于高性能模型如Point Transformer V3的计算和内存需求高，在资源受限的环境中部署这些模型仍然具有挑战性。本研究引入了一种新颖的蒸馏框架，该框架利用拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型传递到轻量级学生模型。我们的方法捕捉了点云的底层几何结构，并通过基于梯度的特征对齐有选择地指导学生模型的训练过程。在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，所提出的方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。值得注意的是，在NuScenes数据集上，我们的方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。我们的实现代码已公开，可在https://github.com/HySonLab/PointDistill上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hysonlab/pointdistill&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing has gained significant attention due to its criticalrole in applications such as autonomous driving and 3D object recognition.However, deploying high-performance models like Point Transformer V3 inresource-constrained environments remains challenging due to their highcomputational and memory demands. This work introduces a novel distillationframework that leverages topology-aware representations and gradient-guidedknowledge distillation to effectively transfer knowledge from a high-capacityteacher to a lightweight student model. Our approach captures the underlyinggeometric structures of point clouds while selectively guiding the studentmodel's learning process through gradient-based feature alignment. Experimentalresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that theproposed method achieves competitive performance, with an approximately 16xreduction in model size and a nearly 1.9x decrease in inference time comparedto its teacher model. Notably, on NuScenes, our method achievesstate-of-the-art performance among knowledge distillation techniques trainedsolely on LiDAR data, surpassing prior knowledge distillation baselines insegmentation performance. Our implementation is available publicly at:  https://github.com/HySonLab/PointDistill</description>
      <author>example@mail.com (Luu Tung Hai, Thinh D. Le, Zhicheng Ding, Qing Tian, Truong-Son Hy)</author>
      <guid isPermaLink="false">2505.08101v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
      <link>http://arxiv.org/abs/2505.08552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了生成式AI工具在视觉内容创作中的应用，特别是视觉艺术品领域，以及由此引发的版权侵权和伪造问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，生成式AI工具在视觉内容创作中的应用日益增多，尤其是用于视觉艺术品的创作，这引起了关于版权侵权和伪造的严重担忧。&lt;h4&gt;目的&lt;/h4&gt;提出了一种名为DFA-CON的对比学习框架，旨在检测侵犯版权或伪造的AI生成艺术。&lt;h4&gt;方法&lt;/h4&gt;该框架学习一个具有判别性的表征空间，在对比学习框架中，对原创艺术品及其伪造品之间的亲和力进行建模。模型在多种攻击类型下进行训练，包括修复、风格迁移、对抗性扰动和cutmix。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，该模型在大多数攻击类型上表现出稳健的检测性能，优于最近的预训练基础模型。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个有效的方法来检测AI生成的艺术作品中的版权侵权问题，并将相关代码和模型检查点公开。&lt;h4&gt;翻译&lt;/h4&gt;The paper discusses the application of generative AI tools in visual content creation, especially in the context of visual art, and the serious concerns about copyright infringement and forgery that this has raised. In recent years, the application of generative AI tools in visual content creation has increased, especially in the creation of visual art, which has raised serious concerns about copyright infringement and forgery. This paper proposes a contrastive learning framework named DFA-CON, designed to detect copyright-infringing or forged AI-generated art. The framework learns a discriminative representation space, modeling affinity between original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. The code and model checkpoints will be publicly released upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent proliferation of generative AI tools for visual contentcreation-particularly in the context of visual artworks-has raised seriousconcerns about copyright infringement and forgery. The large-scale datasetsused to train these models often contain a mixture of copyrighted andnon-copyrighted artworks. Given the tendency of generative models to memorizetraining patterns, they are susceptible to varying degrees of copyrightviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,this work introduces DFA-CON, a contrastive learning framework designed todetect copyright-infringing or forged AI-generated art. DFA-CON learns adiscriminative representation space, posing affinity among original artworksand their forged counterparts within a contrastive learning framework. Themodel is trained across multiple attack types, including inpainting, styletransfer, adversarial perturbation, and cutmix. Evaluation results demonstraterobust detection performance across most attack types, outperforming recentpretrained foundation models. Code and model checkpoints will be releasedpublicly upon acceptance.</description>
      <author>example@mail.com (Haroon Wahab, Hassan Ugail, Irfan Mehmood)</author>
      <guid isPermaLink="false">2505.08552v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Revealing economic facts: LLMs know more than they say</title>
      <link>http://arxiv.org/abs/2505.08662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）的隐藏状态是否可以用于估计和推断经济和金融统计数据。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于县级行政区和公司层面的经济和财务变量，如失业率和总资产。&lt;h4&gt;目的&lt;/h4&gt;探究隐藏状态是否比LLMs的直接响应包含更丰富的经济信息。&lt;h4&gt;方法&lt;/h4&gt;使用简单线性模型对开源LLMs的隐藏状态进行训练，并与模型的文本输出进行比较。此外，还提出了一个迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高估计精度。&lt;h4&gt;主要发现&lt;/h4&gt;隐藏状态模型在估计经济和金融统计数据方面优于模型的文本输出，且仅需少量标记样本即可进行训练。&lt;h4&gt;结论&lt;/h4&gt;隐藏状态表示在超分辨率和数据推断任务中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了大型语言模型的隐藏状态是否可以用来估计和推断经济和金融统计数据。我们专注于县级行政区（例如失业率）和公司层面（例如总资产）的变量，我们发现基于开源LLMs隐藏状态的简单线性模型优于模型的文本输出。这表明隐藏状态比LLMs直接揭示的响应包含更丰富的经济信息。学习曲线分析表明，仅需要几十个标记样本就足以进行训练。我们还提出了一种迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高了估计精度。最后，我们展示了隐藏状态表示在超分辨率和数据推断任务中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate whether the hidden states of large language models (LLMs) canbe used to estimate and impute economic and financial statistics. Focusing oncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,we show that a simple linear model trained on the hidden states of open-sourceLLMs outperforms the models' text outputs. This suggests that hidden statescapture richer economic information than the responses of the LLMs revealdirectly. A learning curve analysis indicates that only a few dozen labelledexamples are sufficient for training. We also propose a transfer learningmethod that improves estimation accuracy without requiring any labelled datafor the target variable. Finally, we demonstrate the practical utility ofhidden-state representations in super-resolution and data imputation tasks.</description>
      <author>example@mail.com (Marcus Buckmann, Quynh Anh Nguyen, Edward Hill)</author>
      <guid isPermaLink="false">2505.08662v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification</title>
      <link>http://arxiv.org/abs/2505.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用大型语言模型（LLMs）作为特征增强器优化节点表示，并将其作为图神经网络（GNNs）输入的潜力，并基于互换干预方法进行了深入研究。&lt;h4&gt;背景&lt;/h4&gt;LLMs在图表示学习中的应用展现出巨大潜力，但其基本性质尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;通过更深入的分析，探究LLMs增强器和GNNs的深层性质及其内部机制。&lt;h4&gt;方法&lt;/h4&gt;构建了一个具有可控因果关系的合成图数据集，并使用互换干预方法进行分析。基于分析结果，设计了一个即插即用的优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了LLMs增强器和GNNs的内在逻辑和内部机制，并验证了优化模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提高LLMs增强器和GNNs之间的信息传递效率。&lt;h4&gt;翻译&lt;/h4&gt;The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) as feature enhancers to optimize noderepresentations, which are then used as inputs for graph neural networks(GNNs), has shown significant potential in graph representation learning.However, the fundamental properties of this approach remain underexplored. Toaddress this issue, we propose conducting a more in-depth analysis of thisissue based on the interchange intervention method. First, we construct asynthetic graph dataset with controllable causal relationships, enablingprecise manipulation of semantic relationships and causal modeling to providedata for analysis. Using this dataset, we conduct interchange interventions toexamine the deeper properties of LLM enhancers and GNNs, uncovering theirunderlying logic and internal mechanisms. Building on the analytical results,we design a plug-and-play optimization module to improve the informationtransfer between LLM enhancers and GNNs. Experiments across multiple datasetsand models validate the proposed module.</description>
      <author>example@mail.com (Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.08265v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
      <link>http://arxiv.org/abs/2505.08455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Video-based long-form Causal Reasoning (VCRBench)这一新的基准，用于评估大型视频语言模型（LVLMs）在视频因果推理方面的能力，并提出了一种名为Recognition-Reasoning Decomposition (RRD)的模块化方法来提高LVLMs在视频因果推理任务上的准确率。&lt;h4&gt;背景&lt;/h4&gt;尽管视频理解取得了进展，但LVLMs在视频因果推理方面的能力尚未得到充分探索，主要原因是缺乏相关和专门的基准来评估在视觉基础和目标驱动环境中的因果推理。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文旨在提出一个名为VCRBench的新基准，用于评估LVLMs在视频因果推理方面的能力。&lt;h4&gt;方法&lt;/h4&gt;VCRBench使用日常活动的程序视频创建，其中步骤被故意打乱，每个视频片段捕捉一个关键因果事件，以测试LVLMs是否能够识别、推理和正确排序实现特定目标所需的事件。此外，该基准被精心设计，以防止LVLMs利用语言捷径，同时避免开放式问答评估的挑战。作者还提出了RRD方法，将视频因果推理分解为视频识别和因果推理两个子任务。&lt;h4&gt;主要发现&lt;/h4&gt;在VCRBench上的评估表明，最先进的LVLMs在视频因果推理方面存在困难，主要因为它们难以直接从视觉观察中建模长距离因果依赖关系。RRD方法显著提高了VCRBench上的准确率，最高提升达25.2%。&lt;h4&gt;结论&lt;/h4&gt;LVLMs主要依赖于语言知识来完成复杂的视频因果推理任务。&lt;h4&gt;翻译&lt;/h4&gt;Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pritamqu/vcrbench&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.</description>
      <author>example@mail.com (Pritam Sarkar, Ali Etemad)</author>
      <guid isPermaLink="false">2505.08455v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
      <link>http://arxiv.org/abs/2505.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用低分辨率热图像和计算机视觉技术进行人员占用检测，以提高HVAC系统的能效和操作。&lt;h4&gt;背景&lt;/h4&gt;传统的HVAC系统通常不考虑占用情况，而先进的以用户为中心的控制（OCC）系统则考虑了占用状态。然而，使用RGB图像和计算机视觉技术进行占用检测存在隐私问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用低分辨率热图像和计算机视觉技术的人员占用检测模型，以解决隐私问题并降低计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;使用转移学习技术微调YOLOv5模型，以实现基于低分辨率热图像的占用检测。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在占用检测方面取得了满意的性能，精确度、召回率、mAP50和mAP50值接近1.000。&lt;h4&gt;结论&lt;/h4&gt;该模型不仅解决了隐私问题，还降低了计算资源的需求，对HVAC系统的能效和操作有积极影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy plays an essential role in influencing the energy consumption andoperation of heating, ventilation, and air conditioning (HVAC) systems.Traditional HVAC typically operate on fixed schedules without consideringoccupancy. Advanced occupant-centric control (OCC) adopted occupancy status inregulating HVAC operations. RGB images combined with computer vision (CV)techniques are widely used for occupancy detection, however, the detailedfacial and body features they capture raise significant privacy concerns.Low-resolution thermal images offer a non-invasive solution that mitigatesprivacy issues. The study developed an occupancy detection model utilizinglow-resolution thermal images and CV techniques, where transfer learning wasapplied to fine-tune the You Only Look Once version 5 (YOLOv5) model. Thedeveloped model ultimately achieved satisfactory performance, with precision,recall, mAP50, and mAP50 values approaching 1.000. The contributions of thismodel lie not only in mitigating privacy concerns but also in reducingcomputing resource demands.</description>
      <author>example@mail.com (Xue Cui, Vincent Gbouna Zakka, Minhyun Lee)</author>
      <guid isPermaLink="false">2505.08336v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
      <link>http://arxiv.org/abs/2505.08316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for full publication at CogSci 2025  (https://cognitivesciencesociety.org/cogsci-2025/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究VVS（腹侧视觉通路）的功能，提出了一种结合相对位置预测（RP）学习与对比学习的新的无监督任务驱动方法来建模VVS，并证明了VVS在计算层面与位置感知（尤其是RP预测）有关。&lt;h4&gt;背景&lt;/h4&gt;现有的无监督任务驱动方法通过对比学习建模VVS，主要关注物体识别，但作者认为VVS的功能不仅仅局限于物体识别。&lt;h4&gt;目的&lt;/h4&gt;引入相对位置预测作为VVS的附加功能，并设计一种新的无监督任务驱动方法来建模VVS。&lt;h4&gt;方法&lt;/h4&gt;理论上解释了对比学习可能无法实现RP预测的能力，并提出将RP学习与对比学习结合的新方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在提高物体识别性能的同时增强了RP预测能力，且RP预测能力普遍提高了模型的大脑相似度。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明VVS在位置感知中起着重要作用，特别是在RP预测方面。&lt;h4&gt;翻译&lt;/h4&gt;Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rdz98/unsup-vvs&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the concept that ventral visual stream (VVS) mainly functions forobject recognition, current unsupervised task-driven methods model VVS bycontrastive learning, and have achieved good brain similarity. However, webelieve functions of VVS extend beyond just object recognition. In this paper,we introduce an additional function involving VVS, named relative position (RP)prediction. We first theoretically explain contrastive learning may be unableto yield the model capability of RP prediction. Motivated by this, wesubsequently integrate RP learning with contrastive learning, and propose a newunsupervised task-driven method to model VVS, which is more inline withbiological reality. We conduct extensive experiments, demonstrating that: (i)our method significantly improves downstream performance of object recognitionwhile enhancing RP predictivity; (ii) RP predictivity generally improves themodel brain similarity. Our results provide strong evidence for the involvementof VVS in location perception (especially RP prediction) from a computationalperspective.</description>
      <author>example@mail.com (Dazhong Rong, Hao Dong, Xing Gao, Jiyu Wei, Di Hong, Yaoyao Hao, Qinming He, Yueming Wang)</author>
      <guid isPermaLink="false">2505.08316v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People</title>
      <link>http://arxiv.org/abs/2505.08215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了影响语音清晰度预测性能的关键设计因素，并提出了针对有听力障碍人群的语音清晰度预测方法。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型（SFMs）在多种下游任务中表现出色，但针对有听力障碍人群的语音清晰度预测（SIP-HI）的优化研究不足。&lt;h4&gt;目的&lt;/h4&gt;通过研究，确定影响SIP-HI性能的关键设计因素，并提出有效的语音清晰度预测方法。&lt;h4&gt;方法&lt;/h4&gt;对5个SFMs进行综合研究，关注编码器层选择、预测头架构和集成配置，并探讨关键SFM属性与其对SIP-HI性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;与传统使用所有层的方法不同，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。集成多个SFMs可以提高性能，更强个体的模型提供更大的益处。&lt;h4&gt;结论&lt;/h4&gt;研究为有效适应SFMs进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型（SFMs）在众多下游任务中表现出色，包括为听力受损人群进行语音清晰度预测（SIP-HI）。然而，针对SIP-HI对SFMs进行优化的研究尚不充分。在本文中，我们进行了一项全面的研究，以确定影响SIP-HI性能的关键设计因素，使用5个SFMs，重点关注编码器层选择、预测头架构和集成配置。我们的研究结果表明，与传统使用所有层的方法相反，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。我们还证明了集成多个SFMs可以提高性能，更强的个体模型提供更大的益处。最后，我们探讨了关键SFM属性及其对SIP-HI性能影响之间的关系。我们的研究为有效调整SFMs以进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models (SFMs) have demonstrated strong performance across avariety of downstream tasks, including speech intelligibility prediction forhearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has beeninsufficiently explored. In this paper, we conduct a comprehensive study toidentify key design factors affecting SIP-HI performance with 5 SFMs, focusingon encoder layer selection, prediction head architecture, and ensembleconfigurations. Our findings show that, contrary to traditional use-all-layersmethods, selecting a single encoder layer yields better results. Additionally,temporal modeling is crucial for effective prediction heads. We alsodemonstrate that ensembling multiple SFMs improves performance, with strongerindividual models providing greater benefit. Finally, we explore therelationship between key SFM attributes and their impact on SIP-HI performance.Our study offers practical insights into effectively adapting SFMs for speechintelligibility prediction for hearing-impaired populations.</description>
      <author>example@mail.com (Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan Xiang Wang)</author>
      <guid isPermaLink="false">2505.08215v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full version of the paper will be appearing at the Proceedings of the  Thirty-Third International Joint Conference on Artificial Intelligence  (IJCAI-25), Special Track on AI for Good&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KIIM的新型灌溉方法映射方法，通过利用Swin-Transformer模型和多种信息处理技术，实现了更准确和高效的灌溉方法映射。&lt;h4&gt;背景&lt;/h4&gt;现有的灌溉方法映射模型依赖于卫星图像的光谱特征，但由于农业景观的复杂性和有限的训练数据，这些模型效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的灌溉方法映射方法，以实现可持续农业实践和食物系统的精准灌溉。&lt;h4&gt;方法&lt;/h4&gt;KIIM方法利用了以下技术：(i) 特殊的投影矩阵来编码作物到灌溉概率；(ii) 空间注意力图来识别农业用地；(iii) 双向交叉注意力来关注不同模态的互补信息；(iv) 加权集成来结合图像和作物信息的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在五个美国州进行的实验中，KIIM方法相对于基线模型提高了22.9%的IoU（交并比），对于难以分类的滴灌，IoU提高了71.4%。此外，通过两阶段迁移学习方法，在数据有限的州中，IoU提升了51%。&lt;h4&gt;结论&lt;/h4&gt;KIIM方法通过仅使用40%的训练数据即可实现基线性能，提高了灌溉方法映射的效率和可行性，减少了大量手动标记的需求，使大规模自动化灌溉映射更加经济高效。&lt;h4&gt;翻译&lt;/h4&gt;Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate mapping of irrigation methods is crucial for sustainableagricultural practices and food systems. However, existing models that relysolely on spectral features from satellite imagery are ineffective due to thecomplexity of agricultural landscapes and limited training data, making this achallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), anovel Swin-Transformer based approach that uses (i) a specialized projectionmatrix to encode crop to irrigation probability, (ii) a spatial attention mapto identify agricultural lands from non-agricultural lands, (iii)bi-directional cross-attention to focus complementary information fromdifferent modalities, and (iv) a weighted ensemble for combining predictionsfrom images and crop information. Our experimentation on five states in the USshows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)improvement for hard-to-classify drip irrigation. In addition, we propose atwo-phase transfer learning approach to enhance cross-state irrigation mapping,achieving a 51% IoU boost in a state with limited labeled data. The ability toachieve baseline performance with only 40% of the training data highlights itsefficiency, reducing the dependency on extensive manual labeling efforts andmaking large-scale, automated irrigation mapping more feasible andcost-effective.</description>
      <author>example@mail.com (Oishee Bintey Hoque, Nibir Chandra Mandal, Abhijin Adiga, Samarth Swarup, Sayjro Kossi Nouwakpo, Amanda Wilson, Madhav Marathe)</author>
      <guid isPermaLink="false">2505.08302v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了城市数字孪生（UDTs）在城市管理中的重要性，并提出了一个综合的多模态城市数字孪生基准数据集TUM2TWIN，以解决城市数字孪生创建过程中的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生在城市管理中扮演着关键角色，但创建过程中存在多个阶段的挑战，如获取精确的3D数据、重建高保真模型、维护模型更新和确保数据兼容性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决城市数字孪生创建中的挑战，并推动相关研究和实际应用。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，该数据集包含地理参照的、语义对齐的3D模型和网络，以及多种地面、移动、空中和卫星观测数据，支持对传感器进行稳健分析和发展先进的重建方法。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持新型视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建等下游任务，展示了其在城市数字孪生创建中的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服现有城市数字孪生创建的局限性奠定了基础，并为更智能、数据驱动的城市环境的研究和实际解决方案提供了支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理不可或缺的工具，并能够整合来自不同来源的复杂、异构数据。创建城市数字孪生涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新以及确保下游任务的无缝互操作性。当前的数据库通常仅限于处理链的一部分，阻碍了综合城市数字孪生的验证。为了解决这些挑战，我们引入了第一个综合的多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，拥有32个数据子集，覆盖约100,000平方米，目前数据量为767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持对传感器进行稳健分析和发展先进的重建方法。此外，我们还探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和高斯喷溅的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信这一贡献为克服现有城市数字孪生创建的局限性，促进新的研究方向和实践解决方案奠定了基础。项目可在以下网址获取：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
      <link>http://arxiv.org/abs/2505.08111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference publication submitted to IEEE I2MTC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用基于床的压力敏感垫（PSM）监测睡眠中的患者睡眠姿势，并使用深度学习模型进行睡眠姿势分类。&lt;h4&gt;背景&lt;/h4&gt;睡眠姿势对睡眠质量和睡眠障碍（如呼吸暂停）的发生率有影响。&lt;h4&gt;目的&lt;/h4&gt;通过使用深度学习模型，准确估计睡眠姿势，克服临床环境中训练深度学习模型所需的大量标记数据的挑战。&lt;h4&gt;方法&lt;/h4&gt;在睡眠诊所的床上放置PSM收集数据，使用迁移学习来适应预训练的深度学习模型，包括Vision Transformer模型（ViTMAE）和预训练的人体姿态估计模型（ViTPose），并在低分辨率PSM数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;该方法优于基于PSM的睡眠姿势分类的先前工作，包括使用深度学习（TCN）和传统机器学习模型（SVM、XGBoost、随机森林）的方法，并在112个夜晚的患者记录和13个患者的更高分辨率数据集上进行了评估和验证。&lt;h4&gt;结论&lt;/h4&gt;尽管从低分辨率PSM数据中区分睡眠姿势存在挑战，但该方法在临床环境中的实际部署显示出希望。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way ofmonitoring patients during sleep. We focus on four-way sleep positionclassification using data collected from a PSM placed under a mattress in asleep clinic. Sleep positions can affect sleep quality and the prevalence ofsleep disorders, such as apnea. Measurements were performed on patients withsuspected sleep disorders referred for assessments at a sleep clinic. Trainingdeep learning models can be challenging in clinical settings due to the needfor large amounts of labeled data. To overcome the shortage of labeled trainingdata, we utilize transfer learning to adapt pre-trained deep learning models toaccurately estimate sleep positions from a low-resolution PSM dataset collectedin a polysomnography sleep lab. Our approach leverages Vision Transformermodels pre-trained on ImageNet using masked autoencoding (ViTMAE) and apre-trained model for human pose estimation (ViTPose). These approachesoutperform previous work from PSM-based sleep pose classification using deeplearning (TCN) as well as traditional machine learning models (SVM, XGBoost,Random Forest) that use engineered features. We evaluate the performance ofsleep position classification from 112 nights of patient recordings andvalidate it on a higher resolution 13-patient dataset. Despite the challengesof differentiating between sleep positions from low-resolution PSM data, ourapproach shows promise for real-world deployment in clinical settings</description>
      <author>example@mail.com (Olivier Papillon, Rafik Goubran, James Green, Julien Larivière-Chartier, Caitlin Higginson, Frank Knoefel, Rébecca Robillard)</author>
      <guid isPermaLink="false">2505.08111v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
      <link>http://arxiv.org/abs/2505.08723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TiMo是一种针对卫星图像时间序列分析的新型分层视觉Transformer基础模型，通过引入时空陀螺仪注意力机制，能够动态捕捉时空变化的多尺度模式，并在多个时空任务中展现出优越性。&lt;h4&gt;背景&lt;/h4&gt;现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，限制了其在下游任务中的有效性。&lt;h4&gt;目的&lt;/h4&gt;提出TiMo模型，以克服现有模型的局限性，提高时空基础模型在环境管理和灾害评估等应用中的效果。&lt;h4&gt;方法&lt;/h4&gt;引入时空陀螺仪注意力机制，动态捕捉时空变化的多尺度模式；构建MillionST数据集，包含100万张图像，涵盖五年内的10个时间相和100,000个地理位置；利用Masked Image Modeling对TiMo进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;TiMo在森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测等时空任务中，表现优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TiMo模型能够有效学习并编码可推广的时空表示，适用于多种时空分析任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要：卫星图像时间序列（SITS）提供了对地球表面的连续观测，对于环境管理和灾害评估等应用至关重要。然而，现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，这限制了它们在下游任务中的有效性。为了克服这一挑战，我们提出了TiMo，一种针对SITS分析的分层视觉Transformer基础模型。在核心部分，我们引入了一种时空陀螺仪注意力机制，能够动态地捕捉时间和空间上的演变多尺度模式。为了预训练，我们精心制作了MillionST数据集，包含从100,000个地理位置捕获的100万张图像，这些图像在五年内的10个时间相中捕捉到了多样化的地理空间变化和季节性变化。利用这个数据集，我们将掩码图像建模应用于预训练TiMo，使其能够有效地学习并编码可推广的时空表示。在多个时空任务（包括森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测）上的广泛实验表明，TiMo在性能上优于最先进的方法。代码、模型和数据集将在https://github.com/MiliLab/TiMo发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mililab/timo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Satellite image time series (SITS) provide continuous observations of theEarth's surface, making them essential for applications such as environmentalmanagement and disaster assessment. However, existing spatiotemporal foundationmodels rely on plain vision transformers, which encode entire temporalsequences without explicitly capturing multiscale spatiotemporal relationshipsbetween land objects. This limitation hinders their effectiveness in downstreamtasks. To overcome this challenge, we propose TiMo, a novel hierarchical visiontransformer foundation model tailored for SITS analysis. At its core, weintroduce a spatiotemporal gyroscope attention mechanism that dynamicallycaptures evolving multiscale patterns across both time and space. Forpre-training, we curate MillionST, a large-scale dataset of one million imagesfrom 100,000 geographic locations, each captured across 10 temporal phases overfive years, encompassing diverse geospatial changes and seasonal variations.Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,enabling it to effectively learn and encode generalizable spatiotemporalrepresentations.Extensive experiments across multiple spatiotemporaltasks-including deforestation monitoring, land cover segmentation, crop typeclassification, and flood detection-demonstrate TiMo's superiority overstate-of-the-art methods. Code, model, and dataset will be released athttps://github.com/MiliLab/TiMo.</description>
      <author>example@mail.com (Xiaolei Qin, Di Wang, Jing Zhang, Fengxiang Wang, Xin Su, Bo Du, Liangpei Zhang)</author>
      <guid isPermaLink="false">2505.08723v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Training Strategies for Efficient Embodied Reasoning</title>
      <link>http://arxiv.org/abs/2505.08243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了机器人思维链推理（CoT）在提高机器人策略泛化能力和性能方面的作用，特别是视觉-语言-动作模型（VLAs）。提出了新的机器人推理方法，并测试了其效果。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人CoT推理方法在性能和泛化能力上有提升，但存在局限性，如需要特定的机器人推理数据集和较慢的推理速度。&lt;h4&gt;目的&lt;/h4&gt;设计新的机器人推理方法，解决现有方法的局限性，并深入理解推理如何帮助策略性能提升。&lt;h4&gt;方法&lt;/h4&gt;提出了三种机器人推理提升策略，并设计了简单变体来测试每种策略。通过学习生成推理和关注推理，来提升VLA模型的表示学习和动作预测。&lt;h4&gt;主要发现&lt;/h4&gt;学习生成推理可以提升VLA表示，关注推理有助于利用这些特征进行更好的动作预测。新方法在LIBERO-90基准测试中取得了显著性能提升，推理速度比标准方法快3倍。&lt;h4&gt;结论&lt;/h4&gt;CoT推理有助于提升VLAs，提出了两种简单轻量级的替代方法，这些方法在性能和推理速度上都有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpfulintermediate representations before choosing actions -- provides an effectivemethod for improving the generalization and performance of robot policies,especially vision-language-action models (VLAs). While such approaches havebeen shown to improve performance and generalization, they suffer from corelimitations, like needing specialized robot reasoning data and slow inferencespeeds. To design new robot reasoning approaches that address these issues, amore complete characterization of why reasoning helps policy performance iscritical. We hypothesize several mechanisms by which robot reasoning improvespolicies -- (1) better representation learning, (2) improved learningcurricularization, and (3) increased expressivity -- then devise simplevariants of robot CoT reasoning to isolate and test each one. We find thatlearning to generate reasonings does lead to better VLA representations, whileattending to the reasonings aids in actually leveraging these features forimproved action prediction. Our results provide us with a better understandingof why CoT reasoning helps VLAs, which we use to introduce two simple andlightweight alternative recipes for robot reasoning. Our proposed approachesachieve significant performance gains over non-reasoning policies,state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedupcompared to standard robot reasoning.</description>
      <author>example@mail.com (William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine)</author>
      <guid isPermaLink="false">2505.08243v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
      <link>http://arxiv.org/abs/2505.08266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphVision Network (GVN)的有效框架，以及其高效的变体E-GVN，旨在通过视觉感知增强来提高链接预测任务中的MPNNs性能。&lt;h4&gt;背景&lt;/h4&gt;MPNNs和结构特征（SFs）是链接预测任务的基础，但视觉感知在MPNN社区中尚未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够利用视觉感知的框架，以提升MPNNs在链接预测任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了GraphVision Network (GVN)和E-GVN两种框架，并通过七个链接预测数据集进行了实证研究。&lt;h4&gt;主要发现&lt;/h4&gt;GVN在七个链接预测数据集上，包括挑战性的大规模图数据集，都实现了视觉增强带来的性能提升，这些改进与现有最先进（SOTA）方法兼容，并且GVN达到了新的SOTA结果。&lt;h4&gt;结论&lt;/h4&gt;GVN和E-GVN为链接预测任务提供了一个有希望的新的研究方向，证明了视觉感知在MPNNs中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message-passing graph neural networks (MPNNs) and structural features (SFs)are cornerstones for the link prediction task. However, as a common andintuitive mode of understanding, the potential of visual perception has beenoverlooked in the MPNN community. For the first time, we equip MPNNs withvision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensiveempirical results demonstrate that with the proposed frameworks, GVNconsistently benefits from the vision enhancement across seven link predictiondatasets, including challenging large-scale graphs. Such improvements arecompatible with existing state-of-the-art (SOTA) methods and GVNs achieve newSOTA results, thereby underscoring a promising novel direction for linkprediction.</description>
      <author>example@mail.com (Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok)</author>
      <guid isPermaLink="false">2505.08266v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation</title>
      <link>http://arxiv.org/abs/2505.08157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双曲对比学习的知识感知推荐方法，旨在解决现有对比学习方法在捕获用户-物品二分图和知识图中的层次结构以及生成正样本时的偏好学习偏移问题。&lt;h4&gt;背景&lt;/h4&gt;基于图神经网络（GNNs）和对比学习的知识感知推荐已成为主流，但现有方法在有效捕获用户-物品二分图和知识图中的层次结构以及生成正样本时存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的双曲对比学习方法，通过模型增强来提高知识感知推荐的效果。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新的洛伦兹知识聚合机制来捕获内在的层次图结构，并提出了三种模型级增强技术来辅助双曲对比学习，避免增强正样本对之间的偏好偏移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在实验中显示出优于现有基线的优越性，最大提升了11.03%。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高知识感知推荐的效果，为解决现有对比学习方法中的问题提供了一种新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benefiting from the effectiveness of graph neural networks (GNNs) andcontrastive learning, GNN-based contrastive learning has become mainstream forknowledge-aware recommendation. However, most existing contrastivelearning-based methods have difficulties in effectively capturing theunderlying hierarchical structure within user-item bipartite graphs andknowledge graphs. Moreover, they commonly generate positive samples forcontrastive learning by perturbing the graph structure, which may lead to ashift in user preference learning. To overcome these limitations, we proposehyperbolic contrastive learning with model-augmentation for knowledge-awarerecommendation. To capture the intrinsic hierarchical graph structures, wefirst design a novel Lorentzian knowledge aggregation mechanism, which enablesmore effective representations of users and items. Then, we propose threemodel-level augmentation techniques to assist Hyperbolic contrastive learning.Different from the classical structure-level augmentation (e.g., edgedropping), the proposed model-augmentations can avoid preference shifts betweenthe augmented positive pair. Finally, we conduct extensive experiments todemonstrate the superiority (maximum improvement of $11.03\%$) of proposedmethods over existing baselines.</description>
      <author>example@mail.com (Shengyin Sun, Chen Ma)</author>
      <guid isPermaLink="false">2505.08157v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
      <link>http://arxiv.org/abs/2505.08086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习（TL）的多模态AI模型，用于伤口分类，旨在提高伤口诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;有效的诊断急性难以愈合的伤口对于提供有效的患者护理至关重要。不良的临床结果通常与感染、周围血管疾病和伤口深度增加有关。&lt;h4&gt;目的&lt;/h4&gt;通过结合两种最先进的架构Xception和GMRNN，开发一个多模态网络，以改善伤口类型（糖尿病、压力、手术和静脉溃疡）的分类。&lt;h4&gt;方法&lt;/h4&gt;该模型通过连接迁移学习算法提取的特征和位置特征来分类伤口类型，并与深度神经网络（DNN）进行综合比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法的伤口分类准确率从78.77%到100%不等，证明了其在准确分类最常见伤口类型方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了所提出的方法在利用伤口图像及其相应位置准确分类最常见伤口类型方面的优异表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The effective diagnosis of acute and hard-to-heal wounds is crucial for woundcare practitioners to provide effective patient care. Poor clinical outcomesare often linked to infection, peripheral vascular disease, and increasingwound depth, which collectively exacerbate these comorbidities. However,diagnostic tools based on Artificial Intelligence (AI) speed up theinterpretation of medical images and improve early detection of disease. Inthis article, we propose a multi-modal AI model based on transfer learning(TL), which combines two state-of-the-art architectures, Xception and GMRNN,for wound classification. The multi-modal network is developed by concatenatingthe features extracted by a transfer learning algorithm and location featuresto classify the wound types of diabetic, pressure, surgical, and venous ulcers.The proposed method is comprehensively compared with deep neural networks (DNN)for medical image analysis. The experimental results demonstrate a notablewound-class classifications (containing only diabetic, pressure, surgical, andvenous) vary from 78.77 to 100\% in various experiments. The results presentedin this study showcase the exceptional accuracy of the proposed methodology inaccurately classifying the most commonly occurring wound types using woundimages and their corresponding locations.</description>
      <author>example@mail.com (Ramin Mousa, Ehsan Matbooe, Hakimeh Khojasteh, Amirali Bengari, Mohammadmahdi Vahediahmar)</author>
      <guid isPermaLink="false">2505.08086v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
      <link>http://arxiv.org/abs/2505.08607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BooSTer的新框架，用于解决立体匹配方法中标签获取困难、数据稀缺以及合成图像与真实世界图像之间的领域差异问题。&lt;h4&gt;背景&lt;/h4&gt;立体匹配方法需要密集的像素级地面真实标签，这在实际数据集中非常耗时，而且合成图像与真实世界图像之间的数据稀少和领域差距也带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，以利用视觉基础模型和大规模混合图像源（包括合成、真实和单视图图像）来解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种数据生成策略，结合单目深度估计和扩散模型，从单视图图像中生成密集的立体匹配数据。2. 通过使用伪单目深度标签和动态尺度及平移不变损失，从单目深度估计模型中迁移知识，以解决现实世界数据集中的稀疏标签问题。3. 将视觉基础模型作为编码器，提取鲁棒且可迁移的特征，以提高准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，该方法在准确率方面取得了显著的提升，特别是在数据标签有限和领域迁移的场景中。&lt;h4&gt;结论&lt;/h4&gt;BooSTer框架在解决立体匹配中的挑战方面是有效的，尤其是在标签稀缺和领域迁移的情况下，可以显著提高准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereo matching methods rely on dense pixel-wise ground truth labels, whichare laborious to obtain, especially for real-world datasets. The scarcity oflabeled data and domain gaps between synthetic and real-world images also posenotable challenges. In this paper, we propose a novel framework,\textbf{BooSTer}, that leverages both vision foundation models and large-scalemixed image sources, including synthetic, real, and single-view images. First,to fully unleash the potential of large-scale single-view images, we design adata generation strategy combining monocular depth estimation and diffusionmodels to generate dense stereo matching data from single-view images. Second,to tackle sparse labels in real-world datasets, we transfer knowledge frommonocular depth estimation models, using pseudo-mono depth labels and a dynamicscale- and shift-invariant loss for additional supervision. Furthermore, weincorporate vision foundation model as an encoder to extract robust andtransferable features, boosting accuracy and generalization. Extensiveexperiments on benchmark datasets demonstrate the effectiveness of ourapproach, achieving significant improvements in accuracy over existing methods,particularly in scenarios with limited labeled data and domain shifts.</description>
      <author>example@mail.com (Yuran Wang, Yingping Liang, Ying Fu)</author>
      <guid isPermaLink="false">2505.08607v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.08199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多粒度信息的MLP预测框架，用于解决长期时间序列预测中的关键问题，并在八个基准数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测在能源消耗和天气预报等领域具有广泛的应用价值，但准确预测长期变化面临复杂的时间模式和内在的多尺度变化。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种高效的基于MLP的预测框架，解决长期时间序列预测中的多粒度信息利用不足、忽略通道特定属性以及趋势和季节成分的独特性质等问题。&lt;h4&gt;方法&lt;/h4&gt;该方法能够清晰并同步地预测不同尺度的复杂时间动态，并通过一个动态分配不同粒度信息重要性的系统将多尺度预测巧妙地整合。同时，使用双叉结构独立建模趋势和季节性元素。&lt;h4&gt;主要发现&lt;/h4&gt;在八个长期时间序列预测基准数据集上的实验结果表明，与最新的基于MLP的方法（TimeMixer）相比，MDMixer将平均MAE性能提高了4.64%，同时实现了训练效率和模型可解释性之间的有效平衡。&lt;h4&gt;结论&lt;/h4&gt;MDMixer是一种有效的长期时间序列预测方法，在保持训练效率的同时提高了预测准确性，并且具有良好的模型可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) offers broad utility in practicalsettings like energy consumption and weather prediction. Accurately predictinglong-term changes, however, is demanding due to the intricate temporal patternsand inherent multi-scale variations within time series. This work confronts keyissues in LTSF, including the suboptimal use of multi-granularity information,the neglect of channel-specific attributes, and the unique nature of trend andseasonal components, by introducing a proficient MLP-based forecastingframework. Our method adeptly disentangles complex temporal dynamics usingclear, concurrent predictions across various scales. These multi-scaleforecasts are then skillfully integrated through a system that dynamicallyassigns importance to information from different granularities, sensitive toindividual channel characteristics. To manage the specific features of temporalpatterns, a two-pronged structure is utilized to model trend and seasonalelements independently. Experimental results on eight LTSF benchmarksdemonstrate that MDMixer improves average MAE performance by 4.64% compared tothe recent state-of-the-art MLP-based method (TimeMixer), while achieving aneffective balance between training efficiency and model interpretability.</description>
      <author>example@mail.com (Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao)</author>
      <guid isPermaLink="false">2505.08199v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models</title>
      <link>http://arxiv.org/abs/2505.08590v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将RAG增强型LLM与病理学基础模型结合应用于甲状腺细胞学诊断，通过利用精确的知识库，提高诊断效率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;人工智能的进步正在通过整合大型语言模型（LLM）与检索增强生成（RAG）和特定领域的基座模型来改变病理学。&lt;h4&gt;目的&lt;/h4&gt;解决细胞学解释、标准化和诊断准确性的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用RAG动态检索相关案例研究、诊断标准和专家解释，同时利用病理学基础模型在高清病理图像上进行特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;这些AI驱动的方法提高了诊断一致性，减少了变异性，并支持病理学家区分良性甲状腺病变和恶性病变。结果显示，将RAG与特定领域的LLM结合显著提高了诊断效率和可解释性。&lt;h4&gt;结论&lt;/h4&gt;AI辅助的甲状腺细胞学诊断具有潜力，其中基础模型UNI在从甲状腺细胞学样本中预测手术病理诊断方面实现了AUC 0.73-0.93的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in artificial intelligence (AI) are transforming pathology byintegrat-ing large language models (LLMs) with retrieval-augmented generation(RAG) and domain-specific foundation models. This study explores theapplication of RAG-enhanced LLMs coupled with pathology foundation models forthyroid cytology diagnosis, addressing challenges in cytologicalinterpretation, standardization, and diagnostic accuracy. By leveraging acurated knowledge base, RAG facilitates dy-namic retrieval of relevant casestudies, diagnostic criteria, and expert interpreta-tion, improving thecontextual understanding of LLMs. Meanwhile, pathology foun-dation models,trained on high-resolution pathology images, refine feature extrac-tion andclassification capabilities. The fusion of these AI-driven approaches en-hancesdiagnostic consistency, reduces variability, and supports pathologists indis-tinguishing benign from malignant thyroid lesions. Our results demonstratethat integrating RAG with pathology-specific LLMs significantly improvesdiagnostic efficiency and interpretability, paving the way for AI-assistedthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 forcorrect prediction of surgi-cal pathology diagnosis from thyroid cytologysamples.</description>
      <author>example@mail.com (Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus)</author>
      <guid isPermaLink="false">2505.08590v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
      <link>http://arxiv.org/abs/2505.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了现有因果模型在CNN和ViT变体上的影响，并提出了TSCNet，一种两阶段因果建模方法，以通过多尺度因果干预发现细粒度因果关联，以减轻长尾分类中的偏差。&lt;h4&gt;背景&lt;/h4&gt;因果推理成为减轻长尾分类偏差的可行方法，但随着从CNN到ViT的高级骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。&lt;h4&gt;目的&lt;/h4&gt;研究现有因果模型对CNN和ViT变体的影响，并提出一种新的方法来解决尾类分类中的难题。&lt;h4&gt;方法&lt;/h4&gt;提出TSCNet，包含两个阶段：1. 层次因果表示学习阶段（HCRL），通过解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签；2. 反事实对数偏置校准阶段（CLBC），通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。&lt;h4&gt;主要发现&lt;/h4&gt;ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，导致在具有相似视觉外观的尾类分类中存在困难。&lt;h4&gt;结论&lt;/h4&gt;TSCNet可以消除数据不平衡引入的多个偏差，在长尾基准测试中优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因果推理已成为减轻长尾分类偏差的有前途的方法，通过处理由类别不平衡引入的偏差。然而，随着从卷积神经网络（CNN）到视觉Transformer（ViT）的先进骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。本文研究了现有因果模型对CNN和ViT变体的影响，强调ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，这导致了在具有相似视觉外观的尾类分类中的困难。为了解决这些问题，本文提出了TSCNet，一种两阶段因果建模方法，通过多尺度因果干预来发现细粒度因果关联。具体来说，在层次因果表示学习阶段（HCRL）中，它解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签，从而增强细粒度因果表示。在反事实对数偏置校准阶段（CLBC）中，它通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。在各个长尾基准上进行的广泛实验表明，所提出的TSCNet可以消除数据不平衡引入的多个偏差，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal inference has emerged as a promising approach to mitigate long-tailclassification by handling the biases introduced by class imbalance. However,along with the change of advanced backbone models from Convolutional NeuralNetworks (CNNs) to Visual Transformers (ViT), existing causal models may notachieve an expected performance gain. This paper investigates the influence ofexisting causal models on CNNs and ViT variants, highlighting that ViT's globalfeature representation makes it hard for causal methods to model associationsbetween fine-grained features and predictions, which leads to difficulties inclassifying tail classes with similar visual appearance. To address theseissues, this paper proposes TSCNet, a two-stage causal modeling method todiscover fine-grained causal associations through multi-scale causalinterventions. Specifically, in the hierarchical causal representation learningstage (HCRL), it decouples the background and objects, applying backdoorinterventions at both the patch and feature level to prevent model from usingclass-irrelevant areas to infer labels which enhances fine-grained causalrepresentation. In the counterfactual logits bias calibration stage (CLBC), itrefines the optimization of model's decision boundary by adaptive constructingcounterfactual balanced data distribution to remove the spurious associationsin the logits caused by data distribution. Extensive experiments conducted onvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminatemultiple biases introduced by data imbalance, which outperforms existingmethods.</description>
      <author>example@mail.com (Xiaoshuo Yan, Zhaochuan Li, Lei Meng, Zhuang Qi, Wei Wu, Zixuan Li, Xiangxu Meng)</author>
      <guid isPermaLink="false">2505.08173v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
      <link>http://arxiv.org/abs/2505.08561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为轨迹感知自适应标记采样（TATS）的新型预训练策略，用于视觉基础模型，并通过在四个数据集上的实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;MVM（掩码视频建模）是一种有效的视觉基础模型预训练策略，但选择合适的掩码策略是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的掩码策略，以优化视频中的掩码标记选择，并提高预训练模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入了TATS，该策略可以建模标记的运动动态，并集成到掩码自动编码器（MAE）框架中。同时，提出了一种统一的训练策略，使用近端策略优化（PPO）对MAE和TATS进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;TATS模型允许进行激进的掩码，同时在不影响动作识别等下游任务性能的情况下保持预训练的内存效率。&lt;h4&gt;结论&lt;/h4&gt;在四个数据集上的实验表明，与现有方法相比，TATS在有效性、迁移性、泛化性和效率方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：掩码视频建模（MVM）已成为视觉基础模型的高效预训练策略，其中模型使用可见标记的信息重建掩码时空标记。然而，这类方法的一个关键挑战在于选择合适的掩码策略。以往的研究探索了预定义的掩码技术，包括随机和基于管道的掩码，以及利用关键运动先验、光流和来自外部预训练模型的语义线索的方法。在本研究中，我们引入了一种新颖且通用的轨迹感知自适应标记采样（TATS），该策略可以建模标记的运动动态，并且可以无缝集成到掩码自动编码器（MAE）框架中以选择视频中的运动中心标记。此外，我们提出了一种统一的训练策略，通过近端策略优化（PPO）从零开始联合优化MAE和TATS。我们表明，我们的模型允许进行激进的掩码，同时在不损害动作识别等下游任务性能的情况下确保预训练保持内存效率。在四个基准数据集（包括Something-Something v2、Kinetics-400、UCF101和HMDB51）上对所提出方法的大量实验表明，与最先进的方法相比，我们的工作在有效性、迁移性、泛化性和效率方面具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked video modeling~(MVM) has emerged as a highly effective pre-trainingstrategy for visual foundation models, whereby the model reconstructs maskedspatiotemporal tokens using information from visible tokens. However, a keychallenge in such approaches lies in selecting an appropriate masking strategy.Previous studies have explored predefined masking techniques, including randomand tube-based masking, as well as approaches that leverage key motion priors,optical flow and semantic cues from externally pre-trained models. In thiswork, we introduce a novel and generalizable Trajectory-Aware Adaptive TokenSampler (TATS), which models the motion dynamics of tokens and can beseamlessly integrated into the masked autoencoder (MAE) framework to selectmotion-centric tokens in videos. Additionally, we propose a unified trainingstrategy that enables joint optimization of both MAE and TATS from scratchusing Proximal Policy Optimization (PPO). We show that our model allows foraggressive masking without compromising performance on the downstream task ofaction recognition while also ensuring that the pre-training remains memoryefficient. Extensive experiments of the proposed approach across fourbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,demonstrate the effectiveness, transferability, generalization, and efficiencyof our work compared to other state-of-the-art methods.</description>
      <author>example@mail.com (Ayush K. Rai, Kyle Min, Tarun Krishna, Feiyan Hu, Alan F. Smeaton, Noel E. O'Connor)</author>
      <guid isPermaLink="false">2505.08561v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic</title>
      <link>http://arxiv.org/abs/2505.08021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）在处理图结构数据中的应用，特别是它们如何处理不同大小的输入图和在图同构下的不变性。文章通过将有限模型理论的方法和工具应用于图表示学习领域，揭示了GNNs的逻辑表达能力。&lt;h4&gt;背景&lt;/h4&gt;GNNs解决了在图结构数据上应用深度学习时遇到的两个主要挑战：处理不同大小的输入图和确保图同构下的不变性。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs的表达能力，特别是有限GNN架构与一阶逻辑（FO）特定片段的关系。&lt;h4&gt;方法&lt;/h4&gt;应用一阶和模态逻辑的有限模型理论方法于图表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;有限GNN架构对应于一阶逻辑的特定片段，包括模态逻辑（ML）、分级模态逻辑（GML）、带有普遍模态的模态逻辑（ML(A)）、二变量片段（FO2）及其扩展计数量词（C2）。&lt;h4&gt;结论&lt;/h4&gt;这一研究提供了一个统一框架，以理解GNNs在FO中的逻辑表达能力。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we show that bounded GNN architectures correspond to specific fragments of first-order logic (FO), including modal logic (ML), graded modal logic (GML), modal logic with the universal modality (ML(A)), the two-variable fragment (FO2) and its extension with counting quantifiers (C2). To establish these results, we apply methods and tools from finite model theory of first-order and modal logics to the domain of graph representation learning. This provides a unifying framework for understanding the logical expressiveness of GNNs within FO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) address two key challenges in applying deeplearning to graph-structured data: they handle varying size input graphs andensure invariance under graph isomorphism. While GNNs have demonstrated broadapplicability, understanding their expressive power remains an importantquestion. In this paper, we show that bounded GNN architectures correspond tospecific fragments of first-order logic (FO), including modal logic (ML),graded modal logic (GML), modal logic with the universal modality (ML(A)), thetwo-variable fragment (FO2) and its extension with counting quantifiers (C2).To establish these results, we apply methods and tools from finite model theoryof first-order and modal logics to the domain of graph representation learning.This provides a unifying framework for understanding the logical expressivenessof GNNs within FO.</description>
      <author>example@mail.com (Bernardo Cuenca Grau, Przemysław A. Wałęga)</author>
      <guid isPermaLink="false">2505.08021v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</title>
      <link>http://arxiv.org/abs/2505.08550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;当前预测模型通常采用时间域的时序预测（TF）范式，直接在时间域中编码和解码时间序列。然而，时间序列数据中的交错步骤依赖关系可能会阻碍TF的性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决TF的性能问题，本文旨在提出一种更有效的编码和解码方法，并增强多变量时间序列的表示学习。&lt;h4&gt;方法&lt;/h4&gt;本文采用基于正交矩阵的数据自适应变换OrthoTrans，对时间序列的时序皮尔逊相关矩阵进行对角化，从而在去相关的特征域中进行更有效的编码和解码。此外，引入了定制的线性层NormLin，使用归一化权重矩阵来捕捉多变量依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，NormLin模块在性能上优于多头自注意力机制，同时所需的浮点运算数（FLOPs）大约是其一半。在24个基准和140个预测任务上的广泛实验表明，OLinear模型在效率上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;作为自注意力机制的插件替代品，NormLin模块能够持续增强基于Transformer的预测器。代码和数据集可通过指定链接获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jackyue1994/OLinear&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-basedmultivariate time series forecasting model that operates in an$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typicallyadopt the temporal forecast (TF) paradigm, which directly encode and decodetime series in the time domain. However, the entangled step-wise dependenciesin series data can hinder the performance of TF. To address this, someforecasters conduct encoding and decoding in the transformed domain usingfixed, dataset-independent bases (e.g., sine and cosine signals in the Fouriertransform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptivetransformation based on an orthogonal matrix that diagonalizes the series'temporal Pearson correlation matrix. This approach enables more effectiveencoding and decoding in the decorrelated feature domain and can serve as aplug-in module to enhance existing forecasters. To enhance the representationlearning for multivariate time series, we introduce a customized linear layer,$\mathbf{NormLin}$, which employs a normalized weight matrix to capturemultivariate dependencies. Empirically, the NormLin module shows a surprisingperformance advantage over multi-head self-attention, while requiring nearlyhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecastingtasks demonstrate that OLinear consistently achieves state-of-the-artperformance with high efficiency. Notably, as a plug-in replacement forself-attention, the NormLin module consistently enhances Transformer-basedforecasters. The code and datasets are available athttps://anonymous.4open.science/r/OLinear</description>
      <author>example@mail.com (Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi)</author>
      <guid isPermaLink="false">2505.08550v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant graph neural network surrogates for predicting the properties of relaxed atomic configurations</title>
      <link>http://arxiv.org/abs/2505.08121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文使用密度泛函理论（DFT）计算确定结构的最小形成能量，并提出了等变图神经网络（EGNN）模型来预测对特定结构的DFT计算结果。&lt;h4&gt;背景&lt;/h4&gt;传统的簇展开方法在处理固定晶格以外的结构，如间隙原子、非晶材料和多结构材料时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出EGNN模型，以更灵活地处理结构变化，同时尊重系统的对称性。&lt;h4&gt;方法&lt;/h4&gt;在锂钴氧化物（LCO）的多种锂含量和锂原子排列组合下，使用EGNN模型进行数学框架构建和训练。&lt;h4&gt;主要发现&lt;/h4&gt;EGNN模型能够准确预测训练集之外的量，包括最大原子位移、应变张量和能量，以及形成能量。&lt;h4&gt;结论&lt;/h4&gt;EGNN模型提供了对研究系统的深入理解，无需进行更多的DFT计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Density functional theory (DFT) calculations determine the relaxed atomicpositions and lattice parameters that minimize the formation energy of astructure. We present an equivariant graph neural network (EGNN) model topredict the outcome of DFT calculations for structures of interest. Clusterexpansions are a well established approach for representing the formationenergies. However, traditional cluster expansions are limited in their abilityto handle variations from a fixed lattice, including interstitial atoms,amorphous materials, and materials with multiple structures. EGNNs offer a moreflexible framework that inherently respects the symmetry of the system withoutbeing reliant on a particular lattice. In this work, we present themathematical framework and the results of training for lithium cobalt oxide(LCO) at various compositions of lithium and arrangements of the lithium atoms.Our results demonstrate that the EGNN can accurately predict quantities outsidethe training set including the largest atomic displacements, the strain tensorand energy, and the formation energy providing greater insight into the systembeing studied without the need for more DFT calculations.</description>
      <author>example@mail.com (Jamie Holber, Krishna Garikipati)</author>
      <guid isPermaLink="false">2505.08121v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks</title>
      <link>http://arxiv.org/abs/2505.07895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HGNN-IMA的新型模型，用于多模态异构网络中的节点分类，通过捕捉信息传播过程中的多模态相互影响，实现自适应多模态融合。&lt;h4&gt;背景&lt;/h4&gt;当前在线平台如豆瓣电影网络和亚马逊产品评论网络等可以描述为多模态异构网络（MMHNs），在这些网络中准确分类节点对于分析对应实体至关重要，需要有效的节点表示学习方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的节点分类模型，以解决现有多模态融合方法在早期融合和晚期融合中的不足。&lt;h4&gt;方法&lt;/h4&gt;模型名为HGNN-IMA，采用异构图Transformer框架，集成了嵌套的跨模态注意力机制，并考虑了模态对齐以促进具有跨模态一致相似性的节点间的传播，同时增加了注意力损失以减轻缺失模态的影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了创新视角，尤其是在伴随网络结构的情况下。&lt;h4&gt;结论&lt;/h4&gt;HGNN-IMA模型能够有效地处理多模态数据，特别是在节点分类任务中具有显著优势，为多模态网络分析提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：如今，众多在线平台可以描述为多模态异构网络（MMHNs），如豆瓣的电影网络和亚马逊的产品评论网络。在这些网络中对节点进行准确分类对于分析相应的实体至关重要，这需要节点上的有效表示学习方法。然而，现有的多模态融合方法通常采用早期融合策略，这可能会导致丢失各个模态的独特特征，或者采用晚期融合方法，忽略了基于GNN的信息传播中的跨模态指导。在本文中，我们提出了一种名为异构图神经网络与跨模态注意力（HGNN-IMA）的新型模型，用于MMHNs中的节点分类。它通过在异构图Transformer框架内捕捉信息传播过程中的多模态相互影响来学习节点表示。具体来说，将嵌套的跨模态注意力机制整合到节点间注意力中，以实现自适应多模态融合，并考虑模态对齐以促进具有所有模态一致相似性的节点间的传播。此外，还增加了注意力损失以减轻缺失模态的影响。大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了一个创新的方法，尤其是在伴随网络结构的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nowadays, numerous online platforms can be described as multi-modalheterogeneous networks (MMHNs), such as Douban's movie networks and Amazon'sproduct review networks. Accurately categorizing nodes within these networks iscrucial for analyzing the corresponding entities, which requires effectiverepresentation learning on nodes. However, existing multi-modal fusion methodsoften adopt either early fusion strategies which may lose the uniquecharacteristics of individual modalities, or late fusion approaches overlookingthe cross-modal guidance in GNN-based information propagation. In this paper,we propose a novel model for node classification in MMHNs, named HeterogeneousGraph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns noderepresentations by capturing the mutual influence of multiple modalities duringthe information propagation process, within the framework of heterogeneousgraph transformer. Specifically, a nested inter-modal attention mechanism isintegrated into the inter-node attention to achieve adaptive multi-modalfusion, and modality alignment is also taken into account to encourage thepropagation among nodes with consistent similarities across all modalities.Moreover, an attention loss is augmented to mitigate the impact of missingmodalities. Extensive experiments validate the superiority of the model in thenode classification task, providing an innovative view to handle multi-modaldata, especially when accompanied with network structures.</description>
      <author>example@mail.com (Jiafan Li, Jiaqi Zhu, Liang Chang, Yilin Li, Miaomiao Li, Yang Wang, Hongan Wang)</author>
      <guid isPermaLink="false">2505.07895v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>ExEBench: Benchmarking Foundation Models on Extreme Earth Events</title>
      <link>http://arxiv.org/abs/2505.08529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ExE-Bench，一个针对极端事件的基准数据集，旨在评估机器学习模型在极端事件管理中的可靠性。&lt;h4&gt;背景&lt;/h4&gt;地球正面临越来越频繁的极端事件，这些事件对人类生活和生态系统构成重大风险。机器学习在提取特征和灾难管理方面展现出潜力，但模型可能存在训练数据中的偏差。&lt;h4&gt;目的&lt;/h4&gt;ExEBench旨在（1）评估机器学习模型在多样化、高影响任务和领域中的泛化能力，（2）促进有助于灾难管理的创新机器学习方法的发展，（3）提供一个分析极端事件相互作用和级联效应的平台，以加深我们对地球系统，尤其是未来几十年气候变化预期下的理解。&lt;h4&gt;方法&lt;/h4&gt;ExEBench包含七个极端事件类别，包括洪水、野火、风暴、热带气旋、极端降水、热浪和冷浪，具有全球覆盖、不同数据量和多样化的数据来源。&lt;h4&gt;主要发现&lt;/h4&gt;ExEBench旨在解决机器学习模型在极端事件管理中的挑战，并提供一个评估模型性能的基准。&lt;h4&gt;结论&lt;/h4&gt;ExEBench是一个公共数据集和代码库，旨在推动机器学习在极端事件管理中的应用和发展。&lt;h4&gt;翻译&lt;/h4&gt;Our planet is facing increasingly frequent extreme events, which pose major risks to human lives and ecosystems. Recent advances in machine learning (ML), especially with foundation models (FMs) trained on extensive datasets, excel in extracting features and show promise in disaster management. Nevertheless, these models often inherit biases from training data, challenging their performance over extreme values. To explore the reliability of FM in the context of extreme events, we introduce extbf{ExE}Bench (extbf{Ex}tremeextbf{E}arth Benchmark), a collection of seven extreme event categories across floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, and spectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needs in extreme events detection, monitoring, and forecasting. ExEBench aims to (1) assess FM generalizability across diverse, high-impact tasks and domains, (2) promote the development of novel ML methods that benefit disaster management, and (3) offer a platform for analyzing the interactions and cascading effects of extreme events to advance our understanding of Earth system, especially under the climate change expected in the decades to come. The dataset and code are public https://github.com/zhaoshan2/EarthExtreme-Bench.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our planet is facing increasingly frequent extreme events, which pose majorrisks to human lives and ecosystems. Recent advances in machine learning (ML),especially with foundation models (FMs) trained on extensive datasets, excel inextracting features and show promise in disaster management. Nevertheless,these models often inherit biases from training data, challenging theirperformance over extreme values. To explore the reliability of FM in thecontext of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme\textbf{E}arth Benchmark), a collection of seven extreme event categoriesacross floods, wildfires, storms, tropical cyclones, extreme precipitation,heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, andspectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needsin extreme events detection, monitoring, and forecasting. ExEBench aims to (1)assess FM generalizability across diverse, high-impact tasks and domains, (2)promote the development of novel ML methods that benefit disaster management,and (3) offer a platform for analyzing the interactions and cascading effectsof extreme events to advance our understanding of Earth system, especiallyunder the climate change expected in the decades to come. The dataset and codeare public https://github.com/zhaoshan2/EarthExtreme-Bench.</description>
      <author>example@mail.com (Shan Zhao, Zhitong Xiong, Jie Zhao, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2505.08529v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Symbolically-Guided Visual Plan Inference from Uncurated Video Data</title>
      <link>http://arxiv.org/abs/2505.08444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vis2Plan是一个高效的、可解释的、基于符号指导的视觉规划框架，它在长周期操作任务中实现了良好的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉规划方法通常依赖于视频生成模型来获取子目标，但存在模型幻觉和计算成本高的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够自动提取任务符号并构建高级符号转换图，以实现多目标、多阶段规划的框架。&lt;h4&gt;方法&lt;/h4&gt;Vis2Plan从原始的无标签游戏数据中提取一组紧凑的任务符号，并在符号级别进行规划，生成一系列基于符号表示的物理一致的中间子目标图像。&lt;h4&gt;主要发现&lt;/h4&gt;Vis2Plan在真实机器人环境中比基于扩散视频生成模型的视觉规划器有53%更高的成功率，并且生成视觉计划的效率提高了35倍。&lt;h4&gt;结论&lt;/h4&gt;Vis2Plan能够生成物理一致的图像目标，并提供了完全可检查的推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual planning, by offering a sequence of intermediate visual subgoals to agoal-conditioned low-level policy, achieves promising performance onlong-horizon manipulation tasks. To obtain the subgoals, existing methodstypically resort to video generation models but suffer from model hallucinationand computational cost. We present Vis2Plan, an efficient, explainable andwhite-box visual planning framework powered by symbolic guidance. From raw,unlabeled play data, Vis2Plan harnesses vision foundation models toautomatically extract a compact set of task symbols, which allows building ahigh-level symbolic transition graph for multi-goal, multi-stage planning. Attest time, given a desired task goal, our planner conducts planning at thesymbolic level and assembles a sequence of physically consistent intermediatesub-goal images grounded by the underlying symbolic representation. OurVis2Plan outperforms strong diffusion video generation-based visual planners bydelivering 53\% higher aggregate success rate in real robot settings whilegenerating visual plans 35$\times$ faster. The results indicate that Vis2Planis able to generate physically consistent image goals while offering fullyinspectable reasoning steps.</description>
      <author>example@mail.com (Wenyan Yang, Ahmet Tikna, Yi Zhao, Yuying Zhang, Luigi Palopoli, Marco Roveri, Joni Pajarinen)</author>
      <guid isPermaLink="false">2505.08444v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
      <link>http://arxiv.org/abs/2505.08414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Meta-EyeFM是一个多功能的底层模型，集成了大语言模型和视觉基础模型，用于眼科疾病的评估。该模型通过路由机制实现基于文本查询的准确任务分析，并通过低秩适应对视觉基础模型进行微调，以提高疾病的检测、严重程度区分和常见眼部标志识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。&lt;h4&gt;目的&lt;/h4&gt;提出Meta-EyeFM模型，旨在为眼科疾病的评估提供一种高效、准确的方法。&lt;h4&gt;方法&lt;/h4&gt;Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。通过低秩适应对视觉基础模型进行微调，以检测眼部和全身疾病，区分眼部疾病的严重程度，并识别常见眼部标志。&lt;h4&gt;主要发现&lt;/h4&gt;Meta-EyeFM在将眼底图像路由到适当的视觉基础模型方面达到了100%的准确率，在疾病检测、严重程度区分和标志识别方面的准确率分别达到了≥82.2%、≥89%和≥76%。该模型在检测各种眼病方面的准确率比Gemini-1.5-flash和ChatGPT-4oLMMs高11%到43%，并且与眼科医生的水平相当。&lt;h4&gt;结论&lt;/h4&gt;Meta-EyeFM系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线大语言模型眼底评估的有价值决策支持工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。我们提出了Meta-EyeFM，一个集成了大型语言模型（LLM）和视觉基础模型（VFMs）的多功能基础模型，用于眼科疾病的评估。Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。使用低秩适应，我们微调了我们的VFMs以检测眼部和全身疾病，区分眼部疾病严重程度，并识别常见眼部标志。该模型在将眼底图像路由到适当的VFMs方面达到了100%的准确率，VFMs在疾病检测方面的准确率达到了≥82.2%，在严重程度区分方面达到了≥89%，在标志识别方面达到了≥76%。与Gemini-1.5-flash和ChatGPT-4oLMMs相比，Meta-EyeFM在检测各种眼病方面的准确率提高了11%到43%，并且与眼科医生的水平相当。该系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线LLM眼底评估的有价值决策支持工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current deep learning models are mostly task specific and lack auser-friendly interface to operate. We present Meta-EyeFM, a multi-functionfoundation model that integrates a large language model (LLM) with visionfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages arouting mechanism to enable accurate task-specific analysis based on textqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular andsystemic diseases, differentiate ocular disease severity, and identify commonocular signs. The model achieved 100% accuracy in routing fundus images toappropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4oLMMs in detecting various eye diseases and comparable to an ophthalmologist.This system offers enhanced usability and diagnostic performance, making it avaluable decision support tool for primary eye care or an online LLM for fundusevaluation.</description>
      <author>example@mail.com (Zhi Da Soh, Yang Bai, Kai Yu, Yang Zhou, Xiaofeng Lei, Sahil Thakur, Zann Lee, Lee Ching Linette Phang, Qingsheng Peng, Can Can Xue, Rachel Shujuan Chong, Quan V. Hoang, Lavanya Raghavan, Yih Chung Tham, Charumathi Sabanayagam, Wei-Chi Wu, Ming-Chih Ho, Jiangnan He, Preeti Gupta, Ecosse Lamoureux, Seang Mei Saw, Vinay Nangia, Songhomitra Panda-Jonas, Jie Xu, Ya Xing Wang, Xinxing Xu, Jost B. Jonas, Tien Yin Wong, Rick Siow Mong Goh, Yong Liu, Ching-Yu Cheng)</author>
      <guid isPermaLink="false">2505.08414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>BLAB: Brutally Long Audio Bench</title>
      <link>http://arxiv.org/abs/2505.03054v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为BLAB的音频基准测试，用于评估音频语言模型在长音频内容上的理解能力。&lt;h4&gt;背景&lt;/h4&gt;理解多样化的语音交互对于语言技术的发展至关重要，而当前研究主要关注短音频片段，缺乏对长对话音频片段的探索。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理长音频内容的音频语言模型，提高语言技术在不同用户群体中的可访问性。&lt;h4&gt;方法&lt;/h4&gt;BLAB包含超过833小时的多样化全长度音频剪辑，平均长度为51分钟，并配以基于文本的标注问题及答案。对六个开源和专有音频语言模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在BLAB上的表现都存在困难，尤其是在定位、时长估计、情感和计数任务上。长音频理解能力存在挑战，模型性能随音频时长增加而下降，依赖提示而非音频内容。&lt;h4&gt;结论&lt;/h4&gt;BLAB为评估和开发具有强大长音频理解能力的音频语言模型提供了一个挑战性的框架。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing large audio language models (LMs) capable of understanding diversespoken interactions is essential for accommodating the multimodal nature ofhuman communication and can increase the accessibility of language technologiesacross different user populations. Recent work on audio LMs has primarilyevaluated their performance on short audio segments, typically under 30seconds, with limited exploration of long-form conversational speech segmentsthat more closely reflect natural user interactions with these models. Weintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audiobenchmark that evaluates audio LMs on localization, duration estimation,emotion, and counting tasks using audio segments averaging 51 minutes inlength. BLAB consists of 833+ hours of diverse, full-length audio clips, eachpaired with human-annotated, text-based natural language questions and answers.Our audio data were collected from permissively licensed sources and underwenta human-assisted filtering process to ensure task compliance. We evaluate sixopen-source and proprietary audio LMs on BLAB and find that all of them,including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with thetasks in BLAB. Our comprehensive analysis reveals key insights into thetrade-offs between task difficulty and audio duration. In general, we find thataudio LMs struggle with long-form speech, with performance declining asduration increases. They perform poorly on localization, temporal reasoning,counting, and struggle to understand non-phonemic information, relying more onprompts than audio content. BLAB serves as a challenging evaluation frameworkto develop audio LMs with robust long-form audio understanding capabilities.</description>
      <author>example@mail.com (Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar)</author>
      <guid isPermaLink="false">2505.03054v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。文章通过理论分析，提出了一个基于分布鲁棒优化的理论框架DRRho风险最小化，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;模型引导在训练大型基础模型等场景中已有所应用，但其内在原理理解不足，导致性能欠佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论框架以指导模型引导，提高模型训练的泛化能力和数据效率。&lt;h4&gt;方法&lt;/h4&gt;提出DRRho风险最小化理论框架，通过分布鲁棒优化（DRO）来分析模型引导的原理，并引入DRRho-CLIP方法进行对比语言-图像预训练。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了模型引导方法比无参考模型训练在泛化能力和数据效率方面的优势，并通过实验验证了这一发现。&lt;h4&gt;结论&lt;/h4&gt;模型引导方法在提高模型训练效果方面具有潜力，且理论框架DRRho风险最小化为其提供了理论支持。&lt;h4&gt;翻译&lt;/h4&gt;本文正式化了一种新兴的学习范式，即使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。尽管模型引导在诸如大型基础模型训练的各种场景中已有所应用，但其背后的原理仍然没有得到充分的理解，这导致了性能的不优化。在这项工作中，我们提出了一种基于分布鲁棒优化（DRO）的理论驱动框架，称为DRRho风险最小化。通过泛化分析，我们提供了理论见解，说明了为什么这种方法比没有参考模型训练提高了泛化能力和数据效率。据我们所知，这是首次为这种新的学习范式提供这样的理论见解，这大大增强了我们对于模型引导的理解和实践。基于这些见解以及对比学习和DRO之间的联系，我们引入了一种名为DRRho-CLIP的对比语言-图像预训练的新方法。广泛的实验验证了理论见解，揭示了与没有参考模型的CLIP相比的优越扩展规律，并展示了其相对于现有启发式方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DSADF: Thinking Fast and Slow for Decision Making</title>
      <link>http://arxiv.org/abs/2505.08189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DSADF的双系统自适应决策框架，通过结合快速直觉决策和深度分析推理，提高强化学习代理在动态环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理在静态环境中表现良好，但在动态环境中泛化能力有限，因为它们依赖于试错交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决强化学习代理在动态环境中泛化能力不足的问题，提出了一种新的决策框架。&lt;h4&gt;方法&lt;/h4&gt;DSADF框架包含两个互补模块：System 1（由强化学习代理和记忆空间组成，用于快速直觉决策）和System 2（由视觉语言模型驱动，用于深度分析推理）。&lt;h4&gt;主要发现&lt;/h4&gt;DSADF在视频游戏环境Crafter和Housekeep中的实证研究表明，该方法在未知和已知任务中均显著提高了决策能力。&lt;h4&gt;结论&lt;/h4&gt;DSADF通过结合直觉和深度推理，实现了高效和自适应的决策，为强化学习代理在动态环境中的泛化提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管强化学习代理在定义良好的环境中效果显著，但它们往往难以将学到的策略泛化到动态环境中，这是因为它们依赖于试错交互。最近的研究探索了通过策略优化指导或先验知识应用大型语言模型（LLMs）或视觉语言模型（VLMs）来提高强化学习代理的泛化能力。然而，这些方法通常缺乏强化学习代理和基础模型之间的无缝协调，导致在不熟悉的环境中的不合理决策和效率瓶颈。充分利用基础模型的推理能力、强化学习代理的快速响应能力，并增强两者之间的交互以形成一个双系统，仍然是一个悬而未决的科学问题。为了解决这个问题，我们借鉴了Kahneman的快速思考（系统1）和慢速思考（系统2）的理论，证明了在复杂世界中平衡直觉和深度推理可以实现敏捷的决策。在本研究中，我们提出了一种双系统自适应决策框架（DSADF），它集成了两个互补的模块：系统1，包括一个强化学习代理和一个用于快速直觉决策的记忆空间；系统2，由一个视觉语言模型驱动，用于深度分析推理。DSADF通过结合两个系统的优势，促进了高效和自适应的决策。在视频游戏环境Crafter和Housekeep中的实证研究证明了我们提出的方法的有效性，显示出在未知和已知任务中决策能力的显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Reinforcement Learning (RL) agents are effective in well-definedenvironments, they often struggle to generalize their learned policies todynamic settings due to their reliance on trial-and-error interactions. Recentwork has explored applying Large Language Models (LLMs) or Vision LanguageModels (VLMs) to boost the generalization of RL agents through policyoptimization guidance or prior knowledge. However, these approaches often lackseamless coordination between the RL agent and the foundation model, leading tounreasonable decision-making in unfamiliar environments and efficiencybottlenecks. Making full use of the inferential capabilities of foundationmodels and the rapid response capabilities of RL agents and enhancing theinteraction between the two to form a dual system is still a lingeringscientific question. To address this problem, we draw inspiration fromKahneman's theory of fast thinking (System 1) and slow thinking (System 2),demonstrating that balancing intuition and deep reasoning can achieve nimbledecision-making in a complex world. In this study, we propose a Dual-SystemAdaptive Decision Framework (DSADF), integrating two complementary modules:System 1, comprising an RL agent and a memory space for fast and intuitivedecision making, and System 2, driven by a VLM for deep and analyticalreasoning. DSADF facilitates efficient and adaptive decision-making bycombining the strengths of both systems. The empirical study in the video gameenvironment: Crafter and Housekeep demonstrates the effectiveness of ourproposed method, showing significant improvements in decision abilities forboth unseen and known tasks.</description>
      <author>example@mail.com (Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang)</author>
      <guid isPermaLink="false">2505.08189v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast</title>
      <link>http://arxiv.org/abs/2505.08151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了锂离子电池容量退化准确估计的重要性，提出了一种针对时间序列基础模型的自适应微调策略，以增强电池退化预测的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统专家模型适用于特定场景，而数据驱动技术的发展为电池容量退化预测提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于电池退化预测的时间序列基础模型的微调策略，实现零样本泛化。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调策略对Timer模型进行微调，并在约10GB的公开电池充放电数据上进行应用；提出知识蒸馏框架，将预训练基础模型的知识转移到紧凑的专家模型中。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的Battery-Timer在容量退化预测方面具有强大的零样本泛化能力；知识蒸馏框架显著提高了专家模型的多条件泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提升电池容量退化预测的准确性，为电池可靠性和安全性提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sjtu-chan-joey/battery-timer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate estimation of lithium-ion battery capacity degradation is criticalfor enhancing the reliability and safety of battery operations. Traditionalexpert models, tailored to specific scenarios, provide isolated estimations.With the rapid advancement of data-driven techniques, a series ofgeneral-purpose time-series foundation models have been developed. However,foundation models specifically designed for battery capacity degradation remainlargely unexplored. To enable zero-shot generalization in battery degradationprediction using large model technology, this study proposes adegradation-aware fine-tuning strategy for time-series foundation models. Weapply this strategy to fine-tune the Timer model on approximately 10 GB ofopen-source battery charge discharge data. Validation on our releasedCycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timerpossesses strong zero-shot generalization capability in capacity degradationforecasting. To address the computational challenges of deploying large models,we further propose a knowledge distillation framework that transfers theknowledge of pre-trained foundation models into compact expert models.Distillation results across several state-of-the-art time-series expert modelsconfirm that foundation model knowledge significantly improves themulti-condition generalization of expert models.</description>
      <author>example@mail.com (Joey Chan, Zhen Chen, Ershun Pan)</author>
      <guid isPermaLink="false">2505.08151v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Games for graded modal substitution calculus</title>
      <link>http://arxiv.org/abs/2505.07966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了针对分级模态替换演算（GMSC）及其变体的两种语义游戏和公式大小游戏，用于研究计算模型的表达能力。&lt;h4&gt;背景&lt;/h4&gt;GMSC及其变体被用于逻辑描述各种计算框架，如图神经网络、普通神经网络和分布式计算。&lt;h4&gt;目的&lt;/h4&gt;引入语义游戏和公式大小游戏，以研究GMSC的等价类以及这些类在给定的GMSC程序大小下的等价性。&lt;h4&gt;方法&lt;/h4&gt;通过引入新的语义游戏和公式大小游戏，展示了这些游戏如何描述GMSC程序等价类之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;公式大小游戏可以用来研究被描述的计算机模型的表达能力；GMSC在词上具有与确定性线性带限制图灵机（确定性线性界限自动机）相同的表达能力。&lt;h4&gt;结论&lt;/h4&gt;公式大小游戏是研究GMSC及其变体表达能力的一个有效工具，同时GMSC的强大表达能力与确定性线性带限制图灵机相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graded modal substitution calculus (GMSC) and its variants has been used forlogical characterizations of various computing frameworks such as graph neuralnetworks, ordinary neural networks and distributed computing. In this paper weintroduce two different semantic games and formula size game for graded modalsubstitution calculus and its variants. Ultimately, we show that the formulasize game characterizes the equivalence of classes of pointed Kripke models upto programs of GMSC of given size. Thus, the formula size game can be used tostudy the expressive power mentioned characterized classes of computing models.Moreover, we show that over words GMSC has the same expressive power asdeterministic linearly tape-bounded Turing machines also known as deterministiclinear bounded automata.</description>
      <author>example@mail.com (Veeti Ahvonen, Reijo Jaakkola, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07966v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem</title>
      <link>http://arxiv.org/abs/2505.08148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了14,904个定制GPT模型，评估了它们对七种可利用威胁的易受攻击性，并引入了一个多指标排名系统来考察定制GPT的流行度与其关联的安全风险之间的关系。&lt;h4&gt;背景&lt;/h4&gt;许多用户使用基于GPT的语言模型执行各种任务，定制GPT模型因其特殊需求而越来越受欢迎，但同时也引发了安全漏洞的担忧。&lt;h4&gt;目的&lt;/h4&gt;研究定制GPT模型的安全风险，并分析其与流行度的关系。&lt;h4&gt;方法&lt;/h4&gt;分析了14,904个定制GPT模型，评估了其对七种威胁的易受攻击性，并引入了多指标排名系统。&lt;h4&gt;主要发现&lt;/h4&gt;超过95%的定制GPT缺乏适当的安全保护，最常见的漏洞包括角色扮演攻击（96.51%）、系统提示泄露（92.20%）和钓鱼（91.22%）。此外，OpenAI的基础模型存在固有的安全弱点。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了增强安全措施和严格内容审查的紧迫需求，以确保GPT应用的安全部署。&lt;h4&gt;翻译&lt;/h4&gt;Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks. In this study, we analyze 14,904 custom GPTs to assess their susceptibility to seven exploitable threats, such as roleplay-based attacks, system prompt leakage, phishing content generation, and malicious code synthesis, across various categories and popularity tiers within the OpenAI marketplace. We introduce a multi-metric ranking system to examine the relationship between a custom GPT's popularity and its associated security risks. Our findings reveal that over 95% of custom GPTs lack adequate security protections. The most prevalent vulnerabilities include roleplay-based vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing (91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit inherent security weaknesses, which are often inherited or amplified in custom GPTs. These results highlight the urgent need for enhanced security measures and stricter content moderation to ensure the safe deployment of GPT-based applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/customgptvulnerability/custom-gpt-vulnerability-assessment&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of users leverage generative pretrained transformer (GPT)-basedlanguage models developed by leading model providers for a wide range of tasks.To support enhanced user interaction and customization, many platforms-such asOpenAI-now enable developers to create and publish tailored model instances,known as custom GPTs, via dedicated repositories or application stores. Thesecustom GPTs empower users to browse and interact with specialized applicationsdesigned to meet specific needs. However, as custom GPTs see growing adoption,concerns regarding their security vulnerabilities have intensified. Existingresearch on these vulnerabilities remains largely theoretical, often lackingempirical, large-scale, and statistically rigorous assessments of associatedrisks.  In this study, we analyze 14,904 custom GPTs to assess their susceptibilityto seven exploitable threats, such as roleplay-based attacks, system promptleakage, phishing content generation, and malicious code synthesis, acrossvarious categories and popularity tiers within the OpenAI marketplace. Weintroduce a multi-metric ranking system to examine the relationship between acustom GPT's popularity and its associated security risks.  Our findings reveal that over 95% of custom GPTs lack adequate securityprotections. The most prevalent vulnerabilities include roleplay-basedvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibitinherent security weaknesses, which are often inherited or amplified in customGPTs. These results highlight the urgent need for enhanced security measuresand stricter content moderation to ensure the safe deployment of GPT-basedapplications.</description>
      <author>example@mail.com (Sunday Oyinlola Ogundoyin, Muhammad Ikram, Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Dali Kaafar)</author>
      <guid isPermaLink="false">2505.08148v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines</title>
      <link>http://arxiv.org/abs/2505.07857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 10 figures(including 6 graphs)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对乌尔都语特定意图检测的独特对比学习方法，该方法利用未标记的乌尔都语数据重新训练预训练的语言模型，以提高乌尔都语意图检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;尽管多种语言都开发了意图检测预测器，但乌尔都语这一第十大语言在该领域仍处于发展阶段。&lt;h4&gt;目的&lt;/h4&gt;提高乌尔都语意图检测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于预训练语言模型的对比学习方法，并探索了6种不同的语言模型和13种不同的相似度计算方法，构建了一个综合的端到端LLMPIA意图检测流程。&lt;h4&gt;主要发现&lt;/h4&gt;在ATIS和Web Queries数据集上，LLMPIA在4-way 1 shot和4-way 5 shot实验设置下分别达到了83.28%和98.25%的F1-Score，在Web Queries数据集上达到了76.23%和84.42%的F1-Score，并且在一个额外的案例研究中，LLMPIA比最先进的预测器高出53.55%的F1-Score。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有效提高了乌尔都语意图检测的性能，为乌尔都语在意图检测领域的进一步发展奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multifarious intent detection predictors are developed for differentlanguages, including English, Chinese and French, however, the field remainsunderdeveloped for Urdu, the 10th most spoken language. In the realm ofwell-known languages, intent detection predictors utilize the strategy offew-shot learning and prediction of unseen classes based on the model trainingon seen classes. However, Urdu language lacks few-shot strategy based intentdetection predictors and traditional predictors are focused on prediction ofthe same classes which models have seen in the train set. To empower Urdulanguage specific intent detection, this introduces a unique contrastivelearning approach that leverages unlabeled Urdu data to re-train pre-trainedlanguage models. This re-training empowers LLMs representation learning for thedownstream intent detection task. Finally, it reaps the combined potential ofpre-trained LLMs and the prototype-informed attention mechanism to create acomprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigmof proposed predictive pipeline, it explores the potential of 6 distinctlanguage models and 13 distinct similarity computation methods. The proposedframework is evaluated on 2 public benchmark datasets, namely ATIS encompassing5836 samples and Web Queries having 8519 samples. Across ATIS dataset under4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,respectively. In an additional case study on the Web Queries dataset under sameclasses train and test set settings, LLMPIA outperformed state-of-the-artpredictor by 53.55% F1-Score.</description>
      <author>example@mail.com (Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel)</author>
      <guid isPermaLink="false">2505.07857v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey</title>
      <link>http://arxiv.org/abs/2505.08034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE COMPSAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要探讨了生成式人工智能（GenAI）在智慧城市中的应用，特别是基于对话界面的GenAI在市民、运营者和规划者等关键用户类型中的应用。&lt;h4&gt;背景&lt;/h4&gt;随着物联网（IoT）和数字孪生技术的普及，为智慧城市提供了丰富的数据基础，旨在改善城市生活和运营。&lt;h4&gt;目的&lt;/h4&gt;研究GenAI在智慧城市中的具体应用，并探讨如何利用GenAI模型和技术在智慧城市的不同子系统内为不同用户类型提供定制化服务和统一界面。&lt;h4&gt;方法&lt;/h4&gt;本文对已提出的GenAI模型和技术进行了识别和回顾，并考虑了如何基于现有的城市记录、IoT数据流和城市数字孪生来构建GenAI。&lt;h4&gt;主要发现&lt;/h4&gt;GenAI能够通过处理多模态内容生成新的输出，如文本和模拟，从而显著提升传统AI分析预测的能力。&lt;h4&gt;结论&lt;/h4&gt;本文认为，这项工作代表了从智慧城市关键用户视角对GenAI技术在智慧城市中应用的首次全面总结。&lt;h4&gt;翻译&lt;/h4&gt;The paper mainly discusses the application of Generative AI (GenAI) in smart cities, especially the application of GenAI based on conversational interfaces for different user types such as citizens, operators, and planners.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of IoT in cities, combined with Digital Twins, creates arich data foundation for Smart Cities aimed at improving urban life andoperations. Generative AI (GenAI) significantly enhances this potential, movingbeyond traditional AI analytics and predictions by processing multimodalcontent and generating novel outputs like text and simulations. Usingspecialized or foundational models, GenAI's natural language abilities such asNatural Language Understanding (NLU) and Natural Language Generation (NLG) canpower tailored applications and unified interfaces, dramatically loweringbarriers for users interacting with complex smart city systems. In this paper,we focus on GenAI applications based on conversational interfaces within thecontext of three critical user archetypes in a Smart City - Citizens, Operatorsand Planners. We identify and review GenAI models and techniques that have beenproposed or deployed for various urban subsystems in the contexts of these userarchetypes. We also consider how GenAI can be built on the existing datafoundation of official city records, IoT data streams and Urban Digital Twins.We believe this work represents the first comprehensive summarization of GenAItechniques for Smart Cities from the lens of the critical users in a SmartCity.</description>
      <author>example@mail.com (Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj)</author>
      <guid isPermaLink="false">2505.08034v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for the Workshop "Safely Leveraging Vision-Language  Foundation Models in Robotics: Challenges and Opportunities" at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了语义异常检测，通过利用最先进视觉基础模型的语义先验知识，直接在图像上进行操作。提出了一种框架，将运行时图像的局部视觉嵌入与被认为是安全且性能良好的名义场景数据库进行比较。&lt;h4&gt;背景&lt;/h4&gt;语义异常是自主系统中常见的视觉元素组合异常，可能导致系统推理失败。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，用于检测自主系统中的语义异常。&lt;h4&gt;方法&lt;/h4&gt;提出两种框架变体：一种使用基于原始网格的嵌入，另一种利用实例分割进行对象中心表示。为了提高鲁棒性，引入了一种简单的过滤机制来抑制假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;在CARLA模拟的异常评估中，基于实例的方法结合过滤机制的性能与GPT-4o相当，同时提供了精确的异常定位。&lt;h4&gt;结论&lt;/h4&gt;基础模型中的视觉嵌入对于自主系统中的实时异常检测具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic anomalies are contextually invalid or unusual combinations offamiliar visual elements that can cause undefined behavior and failures insystem-level reasoning for autonomous systems. This work explores semanticanomaly detection by leveraging the semantic priors of state-of-the-art visionfoundation models, operating directly on the image. We propose a framework thatcompares local vision embeddings from runtime images to a database of nominalscenarios in which the autonomous system is deemed safe and performant. In thiswork, we consider two variants of the proposed framework: one using rawgrid-based embeddings, and another leveraging instance segmentation forobject-centric representations. To further improve robustness, we introduce asimple filtering mechanism to suppress false positives. Our evaluations onCARLA-simulated anomalies show that the instance-based method with filteringachieves performance comparable to GPT-4o, while providing precise anomalylocalization. These results highlight the potential utility of visionembeddings from foundation models for real-time anomaly detection in autonomoussystems.</description>
      <author>example@mail.com (Max Peter Ronecker, Matthew Foutter, Amine Elhafsi, Daniele Gammelli, Ihor Barakaiev, Marco Pavone, Daniel Watzenig)</author>
      <guid isPermaLink="false">2505.07998v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Development of a WAZOBIA-Named Entity Recognition System</title>
      <link>http://arxiv.org/abs/2505.07884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个针对尼日利亚三种主要语言（豪萨语、约鲁巴语和伊博语）的WAZOBIA-NER实体识别系统。&lt;h4&gt;背景&lt;/h4&gt;尽管计算语言学对非洲语言越来越感兴趣，但现有的NER系统主要关注英语、欧洲语言和一些全球语言，对资源匮乏的语言关注不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于尼日利亚三种主要语言的WAZOBIA-NER系统，以解决数据稀缺和语言多样性挑战。&lt;h4&gt;方法&lt;/h4&gt;研究首先为每种语言编制了标注数据集，然后探索了最新的机器学习技术，如条件随机场（CRF）和深度学习模型（如双向长短期记忆网络（BiLSTM）、双向编码器表示的Transformer（Bert）和循环神经网络（RNN）的微调）。系统还利用光学字符识别（OCR）技术将文本图像转换为机器可读文本。&lt;h4&gt;主要发现&lt;/h4&gt;系统在识别人、组织和地点三个实体方面取得了0.9511的精确度、0.9400的召回率、0.9564的F1分数和0.9301的准确率。模型在三种语言上进行了评估，精确度、召回率、F1分数和准确率是关键评估指标。&lt;h4&gt;结论&lt;/h4&gt;Wazobia-NER系统表明，使用当前的NLP框架和迁移学习为资源匮乏的非洲语言构建稳健的NER工具是可行的。&lt;h4&gt;翻译&lt;/h4&gt;Named Entity Recognition (NER) is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Named Entity Recognition NER is very crucial for various natural languageprocessing applications, including information extraction, machine translation,and sentiment analysis. Despite the ever-increasing interest in Africanlanguages within computational linguistics, existing NER systems focus mainlyon English, European, and a few other global languages, leaving a significantgap for under-resourced languages. This research presents the development of aWAZOBIA-NER system tailored for the three most prominent Nigerian languages:Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilationof annotated datasets for each language, addressing data scarcity andlinguistic diversity challenges. Exploring the state-of-the-art machinelearning technique, Conditional Random Fields (CRF) and deep learning modelssuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional EncoderRepresentation from Transformers (Bert) and fine-tune with a Recurrent NeuralNetwork (RNN), the study evaluates the effectiveness of these approaches inrecognizing three entities: persons, organizations, and locations. The systemutilizes optical character recognition (OCR) technology to convert textualimages into machine-readable text, thereby enabling the Wazobia system toaccept both input text and textual images for extraction purposes. The systemachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 inF1-score, and 0.9301 in accuracy. The model's evaluation was conducted acrossthree languages, with precision, recall, F1-score, and accuracy as keyassessment metrics. The Wazobia-NER system demonstrates that it is feasible tobuild robust NER tools for under-resourced African languages using current NLPframeworks and transfer learning.</description>
      <author>example@mail.com (S. E Emedem, I. E Onyenwe, E. G Onyedinma)</author>
      <guid isPermaLink="false">2505.07884v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）的预测解释方法，特别是针对全局反事实解释中的共同回溯问题，并提出了一种有效的算法COMRECGC。&lt;h4&gt;背景&lt;/h4&gt;GNNs在多个领域如社交网络、分子生物学和推荐系统中得到广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;提出并解决GNNs全局反事实解释中的共同回溯问题，设计有效的算法来生成与所有输入拒绝图相关的接受图。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同回溯解释问题，并设计了COMRECGC算法来解决。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个真实世界图数据集上与强基线进行了基准测试，表现出优于竞争对手的性能。&lt;h4&gt;结论&lt;/h4&gt;共同回溯解释与图反事实解释相比，在药物发现或计算生物学等应用中具有可比性或优越性，值得考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ssggreg/comrecgc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Recent results of (semi)leptonic decays of charm hadrons at BESIII</title>
      <link>http://arxiv.org/abs/2505.05123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  contribution to the 2025 Electroweak session of the 59th Rencontres  de Moriond&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文报告了BESIII实验最近对 charm 等离子子 (半)轻子衰变的测量结果。&lt;h4&gt;背景&lt;/h4&gt;包括 D^+ → μ^+ν_μ, D^+ → τ^+ν_τ, D^+ → η′ℓ^+ν_ℓ, 以及 D^{0(+)} → K_ℓ^+ν_ℓ (其中 ℓ=e, μ) 的衰变。&lt;h4&gt;目的&lt;/h4&gt;这些测量提供了最精确或首次对 CKM 矩阵元素 |V_{cs(d)}|，衰变常数 f_{D^+}，以及强子形式因子 f_+^{D^+→η′}(0) 和 f_+^{D→K}(0) 的确定。&lt;h4&gt;方法&lt;/h4&gt;利用了机器学习中的图神经网络首次观察到了稀有β衰变 Λ_c^+ → n e^+ν_e。&lt;h4&gt;主要发现&lt;/h4&gt;测量提供了对 CKM 矩阵元素、衰变常数和强子形式因子的精确测定，并测试了电子-μ子以及τ-μ的轻子味 universality。&lt;h4&gt;结论&lt;/h4&gt;论文通过BESIII实验的测量结果，对 charm 等离子子的轻子衰变特性有了更深入的了解，并通过机器学习方法实现了对稀有β衰变的首次观察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this talk, we report the recent measurements of (semi)leptonic decays ofcharm mesons from the BESIII experiment, including $D^+\to\mu^+\nu_\mu$,$D^+\to\tau^+\nu_\tau$, $D^+\to\eta^\prime\ell^+\nu_\ell$, and $D^{0(+)}\to\barK\ell^+\nu_\ell$ (where $\ell=e, \mu$). These measurements provide the mostprecise or first determinations to date of the CKM matrix elements$|V_{cs(d)}|$, the decay constant $f_{D^+}$, and the hadronic form factors$f_+^{D^+\to \eta^\prime}(0)$ and $f_+^{D\to \bar K}(0)$. Lepton flavoruniversality of $e-\mu$ and $\tau-\mu$ are also tested with these decays.Additionally, we present the first observation of the rare beta decay$\Lambda_c^+\to ne^+\nu_e$ with machine learning of Graph Neural Network.</description>
      <author>example@mail.com (Xiang Pan)</author>
      <guid isPermaLink="false">2505.05123v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
  <item>
      <title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
      <link>http://arxiv.org/abs/2505.07611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了147项关于基于视觉的交通事故预测（Vision-TAA）的研究，重点关注监督学习、无监督学习和混合深度学习模型在事故预测中的应用，以及使用真实世界和合成数据集的方法。&lt;h4&gt;背景&lt;/h4&gt;交通事故预测和检测对于提高道路安全至关重要，基于视觉的交通事故预测在深度学习时代成为一种有前景的方法。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供关于Vision-TAA系统发展的基础参考，以促进道路安全和交通管理。&lt;h4&gt;方法&lt;/h4&gt;本文将现有方法分为四种主要方法：基于图像和视频特征的预测、基于时空特征的预测、场景理解和多模态数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些方法显示出巨大的潜力，但数据稀缺、在复杂场景中泛化能力有限和实时性能限制等问题仍然普遍存在。&lt;h4&gt;结论&lt;/h4&gt;本文强调了未来研究的机遇，包括多模态数据融合、自监督学习和基于Transformer架构的集成，以提高预测准确性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic accident prediction and detection are critical for enhancing roadsafety,and vision-based traffic accident anticipation (Vision-TAA) has emergedas a promising approach in the era of deep learning.This paper reviews 147recent studies,focusing on the application of supervised,unsupervised,andhybrid deep learning models for accident prediction,alongside the use ofreal-world and synthetic datasets.Current methodologies are categorized intofour key approaches: image and video feature-based prediction, spatiotemporalfeature-based prediction, scene understanding,and multimodal data fusion.Whilethese methods demonstrate significant potential,challenges such as datascarcity,limited generalization to complex scenarios,and real-time performanceconstraints remain prevalent. This review highlights opportunities for futureresearch,including the integration of multimodal data fusion, self-supervisedlearning,and Transformer-based architectures to enhance prediction accuracy andscalability.By synthesizing existing advancements and identifying criticalgaps, this paper provides a foundational reference for developing robust andadaptive Vision-TAA systems,contributing to road safety and traffic management.</description>
      <author>example@mail.com (Yi Zhang, Wenye Zhou, Ruonan Lin, Xin Yang, Hao Zheng)</author>
      <guid isPermaLink="false">2505.07611v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
      <link>http://arxiv.org/abs/2505.07818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://dancegrpo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DanceGRPO是一个统一的框架，将Group Relative Policy Optimization (GRPO)应用于视觉生成，能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。&lt;h4&gt;背景&lt;/h4&gt;视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。&lt;h4&gt;目的&lt;/h4&gt;提出DanceGRPO，旨在提供一个统一的强化学习算法，以解决上述问题，并实现视觉生成中的高效反馈。&lt;h4&gt;方法&lt;/h4&gt;DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。&lt;h4&gt;主要发现&lt;/h4&gt;DanceGRPO在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。&lt;h4&gt;结论&lt;/h4&gt;DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;DanceGRPO是一个将Group Relative Policy Optimization (GRPO)应用于视觉生成的统一框架，它能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。在视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。本文提出了DanceGRPO，旨在解决这些问题，并实现视觉生成中的高效反馈。DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中，DanceGRPO优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in generative models-particularly diffusion models andrectified flows-have revolutionized visual content creation, yet aligning modeloutputs with human preferences remains a critical challenge. Existingreinforcement learning (RL)-based methods for visual generation face criticallimitations: incompatibility with modern Ordinary Differential Equations(ODEs)-based sampling paradigms, instability in large-scale training, and lackof validation for video generation. This paper introduces DanceGRPO, the firstunified framework to adapt Group Relative Policy Optimization (GRPO) to visualgeneration paradigms, unleashing one unified RL algorithm across two generativeparadigms (diffusion models and rectified flows), three tasks (text-to-image,text-to-video, image-to-video), four foundation models (Stable Diffusion,HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/videoaesthetics, text-image alignment, video motion quality, and binary reward). Toour knowledge, DanceGRPO is the first RL-based unified framework capable ofseamless adaptation across diverse generative paradigms, tasks, foundationalmodels, and reward models. DanceGRPO demonstrates consistent and substantialimprovements, which outperform baselines by up to 181% on benchmarks such asHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only canstabilize policy optimization for complex video generation, but also enablesgenerative policy to better capture denoising trajectories for Best-of-Ninference scaling and learn from sparse binary feedback. Our results establishDanceGRPO as a robust and versatile solution for scaling Reinforcement Learningfrom Human Feedback (RLHF) tasks in visual generation, offering new insightsinto harmonizing reinforcement learning and visual synthesis. The code will bereleased.</description>
      <author>example@mail.com (Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo)</author>
      <guid isPermaLink="false">2505.07818v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文报告了ACVLAB团队为ICRA 2025 GOOSE 2D语义分割挑战赛开发的语义分割框架，该框架在真实世界条件下将户外场景解析为九个语义类别。&lt;h4&gt;背景&lt;/h4&gt;该框架旨在解决户外场景中由于光照不一致导致的图像分割问题。&lt;h4&gt;目的&lt;/h4&gt;提高语义分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该框架采用Swin Transformer骨干网络，并增强其空间泛化能力；引入了旋转位置编码（RoPE）以增强空间信息；设计了色彩偏移估计与校正模块以补偿自然环境中的光照不一致；采用基于分位数的方法进行降噪，降低误差最大的2.5%像素的影响。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在GOOSE官方测试集上实现了平均交并比（mIoU）0.848，证明了色彩校正、位置编码和误差感知降噪在鲁棒语义分割中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架结合了多种技术，在真实世界条件下的户外场景语义分割中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This report presents our semantic segmentation framework developed by teamACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, whichfocuses on parsing outdoor scenes into nine semantic categories underreal-world conditions. Our method integrates a Swin Transformer backboneenhanced with Rotary Position Embedding (RoPE) for improved spatialgeneralization, alongside a Color Shift Estimation-and-Correction moduledesigned to compensate for illumination inconsistencies in naturalenvironments. To further improve training stability, we adopt a quantile-baseddenoising strategy that downweights the top 2.5\% of highest-error pixels,treating them as noise and suppressing their influence during optimization.Evaluated on the official GOOSE test set, our approach achieved a meanIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness ofcombining color correction, positional encoding, and error-aware denoising inrobust semantic segmentation.</description>
      <author>example@mail.com (Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, Yu-Jou Hsiao)</author>
      <guid isPermaLink="false">2505.06991v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A class of distributed automata that contains the modal mu-fragment</title>
      <link>http://arxiv.org/abs/2505.07816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文将分阶模态μ-演算的μ-片段翻译成一类分布式消息传递自动机。&lt;h4&gt;背景&lt;/h4&gt;在逻辑和计算模型领域，研究分阶模态μ-演算与分布式消息传递自动机之间的关系。&lt;h4&gt;目的&lt;/h4&gt;探索分阶模态μ-演算在分布式消息传递自动机中的表达能力和应用。&lt;h4&gt;方法&lt;/h4&gt;通过翻译技术将分阶模态μ-演算的μ-片段转换为分布式消息传递自动机。&lt;h4&gt;主要发现&lt;/h4&gt;作为推论，本文获得了一个关于循环图神经网络在实数和分阶模态替换演算下具有相同表达能力定理的替代证明。&lt;h4&gt;结论&lt;/h4&gt;分阶模态μ-演算和分布式消息传递自动机之间存在密切的联系，可以相互转换，且在逻辑单形二阶逻辑MSO的约束下具有相同的表达能力。&lt;h4&gt;翻译&lt;/h4&gt;本文实现了分阶模态μ-演算的μ-片段到分布式消息传递自动机的翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper gives a translation from the $\mu$-fragment of the graded modal$\mu$-calculus to a class of distributed message-passing automata. As acorollary, we obtain an alternative proof for a theorem from\cite{ahvonen_neurips} stating that recurrent graph neural networks workingwith reals and graded modal substitution calculus have the same expressivepower in restriction to the logic monadic second-order logic MSO.</description>
      <author>example@mail.com (Veeti Ahvonen, Damian Heiman, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07816v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.06951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, International Conference on Robotics and  Automation(ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对热图像语义分割的跨光谱无监督领域自适应（UDA）方法，通过提高互补信息交换和增强夜间场景下的性能，解决了传统方法在领域自适应中的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶领域，热图像语义分割由于能在恶劣视觉条件下提供稳健的场景理解而成为关键研究领域。然而，由于缺乏标注的热图像数据集，无监督领域自适应方法成为解决这一问题的有效途径。&lt;h4&gt;目的&lt;/h4&gt;旨在通过无监督领域自适应方法解决热图像语义分割中标签数据不足的问题，并提高模型在不同领域的适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型的掩码互学习策略，通过在光谱模型间选择性传递结果并屏蔽不确定区域，促进互补信息的交换。同时，引入了一种新型的原型自监督损失函数，用于增强夜间场景下热分割模型的表现，解决RGB预训练网络在低光照条件下知识迁移能力不足的问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在实验中表现出比之前UDA方法更高的性能，并且与最先进的监督方法具有可比的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法有效提高了热图像语义分割的性能，特别是在夜间场景和领域自适应方面。&lt;h4&gt;翻译&lt;/h4&gt;In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous driving, thermal image semantic segmentation has emerged as acritical research area, owing to its ability to provide robust sceneunderstanding under adverse visual conditions. In particular, unsuperviseddomain adaptation (UDA) for thermal image segmentation can be an efficientsolution to address the lack of labeled thermal datasets. Nevertheless, sincethese methods do not effectively utilize the complementary information betweenRGB and thermal images, they significantly decrease performance during domainadaptation. In this paper, we present a comprehensive study on cross-spectralUDA for thermal image semantic segmentation. We first propose a novel maskedmutual learning strategy that promotes complementary information exchange byselectively transferring results between each spectral model while masking outuncertain regions. Additionally, we introduce a novel prototypicalself-supervised loss designed to enhance the performance of the thermalsegmentation model in nighttime scenarios. This approach addresses thelimitations of RGB pre-trained networks, which cannot effectively transferknowledge under low illumination due to the inherent constraints of RGBsensors. In experiments, our method achieves higher performance over previousUDA methods and comparable performance to state-of-the-art supervised methods.</description>
      <author>example@mail.com (Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi)</author>
      <guid isPermaLink="false">2505.06951v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.07398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DepthFusion的深度感知混合特征融合策略，用于提高LiDAR相机3D目标检测器的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的3D目标检测器主要关注特征融合，但忽略了深度因素在设计融合策略时的重要性。&lt;h4&gt;目的&lt;/h4&gt;通过统计分析和可视化，观察不同模态在不同深度下的作用，并提出一种新的融合策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一种DepthFusion策略，通过在全局和局部级别引入深度编码来引导点云和RGB图像模态的权重。具体包括Depth-GFusion模块和Depth-LFusion模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过统计分析和可视化发现，不同模态在深度变化时扮演不同的角色。&lt;h4&gt;结论&lt;/h4&gt;DepthFusion方法在nuScenes和KITTI数据集上的实验结果表明，该方法优于现有方法，并且对各种类型的损坏更加鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art LiDAR-camera 3D object detectors usually focus on featurefusion. However, they neglect the factor of depth while designing the fusionstrategy. In this work, we are the first to observe that different modalitiesplay different roles as depth varies via statistical analysis andvisualization. Based on this finding, we propose a Depth-Aware Hybrid FeatureFusion (DepthFusion) strategy that guides the weights of point cloud and RGBimage modalities by introducing depth encoding at both global and local levels.Specifically, the Depth-GFusion module adaptively adjusts the weights of imageBird's-Eye-View (BEV) features in multi-modal global features via depthencoding. Furthermore, to compensate for the information lost when transferringraw features to the BEV space, we propose a Depth-LFusion module, whichadaptively adjusts the weights of original voxel features and multi-view imagefeatures in multi-modal local features via depth encoding. Extensiveexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusionmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusionis more robust to various kinds of corruptions, outperforming previous methodson the nuScenes-C dataset.</description>
      <author>example@mail.com (Mingqian Ji, Jian Yang, Shanshan Zhang)</author>
      <guid isPermaLink="false">2505.07398v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了首个综合多模态城市数字孪生（UDT）基准数据集TUM2TWIN，旨在解决城市数字孪生创建中的挑战，并推动智能、数据驱动的城市环境的发展。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生（UDTs）对于城市管理及整合来自不同来源的复杂、异构数据变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决创建UDT过程中遇到的挑战，如获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，包含地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，覆盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，支持传感器分析及高级重建方法的发展。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持下游任务，如NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理及整合来自不同来源的复杂、异构数据的关键。创建UDT涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性。当前数据集通常仅限于处理链的一部分，阻碍了全面UDT验证。为了解决这些挑战，我们引入了首个综合多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，涵盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持传感器分析及高级重建方法的发展。此外，我们探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信，这一贡献为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。项目可访问：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models</title>
      <link>http://arxiv.org/abs/2505.07364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，跨模态医学图像翻译领域的研究取得了丰硕成果，特别是基于生成对抗网络（GAN）的架构在处理大型多模态数据集稀缺的问题上表现出良好的性能。&lt;h4&gt;目的&lt;/h4&gt;旨在设计并比较不同的GAN框架，以从T1加权MRI数据生成合成[18F]氟代脱氧葡萄糖（FDG）PET图像，并评估这些合成数据在无监督异常检测模型训练中的应用。&lt;h4&gt;方法&lt;/h4&gt;设计了基于GAN的框架来生成合成FDG PET图像，进行了定性和定量视觉质量评估，并探索了这些合成数据在无监督异常检测模型训练中的影响。引入了针对无监督检测任务的合成FDG PET数据的诊断任务导向质量指标，并使用这些合成数据训练了一个结合Siamese自编码器深度表示学习和OC-SVM密度支持估计模型的无监督异常检测（UAD）模型。&lt;h4&gt;主要发现&lt;/h4&gt;最好的GAN模型能够生成与真实控制数据集在结构相似性（SSIM）和峰值信噪比（PSNR）值接近0.9和23.8的合成PET图像。在基于这些合成正常PET数据的最佳UAD模型上训练，达到了74%的敏感性。&lt;h4&gt;结论&lt;/h4&gt;基于GAN的模型最适合进行MR T1到FDG PET的翻译，优于transformer或扩散模型。此外，这些合成数据对UAD模型的训练和癫痫患者临床检查的评估也具有诊断价值。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background and Objective. Research in the cross-modal medical imagetranslation domain has been very productive over the past few years in tacklingthe scarce availability of large curated multimodality datasets with thepromising performance of GAN-based architectures. However, only a few of thesestudies assessed task-based related performance of these synthetic data,especially for the training of deep models. Method. We design and comparedifferent GAN-based frameworks for generating synthetic brain[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We firstperform standard qualitative and quantitative visual quality evaluation. Then,we explore further impact of using these fake PET data in the training of adeep unsupervised anomaly detection (UAD) model designed to detect subtleepilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostictask-oriented quality metrics of the synthetic FDG PET data tailored to ourunsupervised detection task, then use these fake data to train a use case UADmodel combining a deep representation learning based on siamese autoencoderswith a OC-SVM density support estimation model. This model is trained on normalsubjects only and allows the detection of any variation from the pattern of thenormal population. We compare the detection performance of models trained on 35paired real MR T1 of normal subjects paired either on 35 true PET images or on35 synthetic PET images generated from the best performing generative models.Performance analysis is conducted on 17 exams of epilepsy patients undergoingsurgery. Results. The best performing GAN-based models allow generatingrealistic fake PET images of control subject with SSIM and PSNR values around0.9 and 23.8, respectively and in distribution (ID) with regard to the truecontrol dataset. The best UAD model trained on these synthetic normative PETdata allows reaching 74% sensitivity. Conclusion. Our results confirm thatGAN-based models are the best suited for MR T1 to FDG PET translation,outperforming transformer or diffusion models. We also demonstrate thediagnostic value of these synthetic data for the training of UAD models andevaluation on clinical exams of epilepsy patients. Our code and the normativeimage dataset are available.</description>
      <author>example@mail.com (Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien)</author>
      <guid isPermaLink="false">2505.07364v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Gameplay Highlights Generation</title>
      <link>http://arxiv.org/abs/2505.07721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。&lt;h4&gt;背景&lt;/h4&gt;传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。&lt;h4&gt;方法&lt;/h4&gt;首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。&lt;h4&gt;结论&lt;/h4&gt;X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;h4&gt;翻译&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。本研究开发了一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we enable gamers to share their gaming experience on socialmedia by automatically generating eye-catching highlight reels from theirgameplay session Our automation will save time for gamers while increasingaudience engagement. We approach the highlight generation problem by firstidentifying intervals in the video where interesting events occur and thenconcatenate them. We developed an in-house gameplay event detection datasetcontaining interesting events annotated by humans using VIA video annotator.Traditional techniques for highlight detection such as game engine integrationrequires expensive collaboration with game developers. OCR techniques whichdetect patches of specific images or texts require expensive per gameengineering and may not generalize across game UI and different language. Wefinetuned a multimodal general purpose video understanding model such as X-CLIPusing our dataset which generalizes across multiple games in a genre withoutper game engineering. Prompt engineering was performed to improve theclassification performance of this multimodal model. Our evaluation showed thatsuch a finetuned model can detect interesting events in first person shootinggames from unseen gameplay footage with more than 90% accuracy. Moreover, ourmodel performed significantly better on low resource games (small dataset) whentrained along with high resource games, showing signs of transfer learning. Tomake the model production ready, we used ONNX libraries to enable crossplatform inference. These libraries also provide post training quantizationtools to reduce model size and inference time for deployment. ONNX runtimelibraries with DirectML backend were used to perform efficient inference onWindows OS. We show that natural language supervision in the X-CLIP model leadsto data efficient and highly performant video recognition models.</description>
      <author>example@mail.com (Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo)</author>
      <guid isPermaLink="false">2505.07721v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比学习的异构图异常检测模型EAGLE，通过对比异常节点与正常节点到局部上下文距离，提高了异常检测的效率。&lt;h4&gt;背景&lt;/h4&gt;图异常检测在多个实际场景中非常重要，已有基于深度学习的方法在性能上优于传统方法，但现有方法在效率上存在不足。&lt;h4&gt;目的&lt;/h4&gt;针对现有方法效率不足的问题，提出一种高效的异构图异常检测模型。&lt;h4&gt;方法&lt;/h4&gt;EAGLE模型首先在元路径级别上采样实例对进行对比学习，然后使用基于图自动编码器的模型无监督地学习节点嵌入，并结合判别器预测节点的异常分数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EAGLE在三个异构网络数据集上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;EAGLE模型能够有效提高异构图异常检测的效率，并在实际数据集上取得了良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/MIS.2022.3229147&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection is a popular and vital task in various real-worldscenarios, which has been studied for several decades. Recently, many studiesextending deep learning-based methods have shown preferable performance ongraph anomaly detection. However, existing methods are lack of efficiency thatis definitely necessary for embedded devices. Towards this end, we propose anEfficient Anomaly detection model on heterogeneous Graphs via contrastiveLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms oftheir distances to the local context. The proposed method first samplesinstance pairs on meta path-level for contrastive learning. Then, a graphautoencoder-based model is applied to learn informative node embeddings in anunsupervised way, which will be further combined with the discriminator topredict the anomaly scores of nodes. Experimental results show that EAGLEoutperforms the state-of-the-art methods on three heterogeneous networkdatasets.</description>
      <author>example@mail.com (Jing Ren, Mingliang Hou, Zhixuan Liu, Xiaomei Bai)</author>
      <guid isPermaLink="false">2505.07508v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过使用视频中的估计姿态来增强3D人体运动预测（HMP）模型的方法，以降低数据收集成本并提高模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D HMP模型训练需要昂贵的运动捕捉数据，而此类数据的收集成本限制了数据的多样性，导致模型在未见过的运动或主体上的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过使用易于获取的视频中的估计姿态来增强HMP模型，以提高其泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将来自单目视频的2D姿态经过处理转化为运动捕捉风格的3D运动，并通过额外学习使HMP模型适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法对HMP模型有定性和定量的影响。&lt;h4&gt;结论&lt;/h4&gt;该方法可以有效地提高HMP模型的泛化能力，并减少对昂贵运动捕捉数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在3D人体运动预测（HMP）中，传统方法使用昂贵的运动捕捉数据训练HMP模型。然而，此类运动捕捉数据的数据收集成本限制了数据的多样性，导致对未见过的运动或主体的泛化能力差。为了解决这个问题，本文提出通过使用从易于获取的视频中估计的姿态来增强HMP模型的方法。通过从获得的运动中进行额外学习，HMP模型被适应到测试域。实验结果表明了我们的方法对HMP模型的定性和定量影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Tagging fully hadronic exotic decays of the vectorlike $\mathbf{B}$ quark using a graph neural network</title>
      <link>http://arxiv.org/abs/2505.07769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对偶产生的矢量型B夸克衰变到新规范单态（伪）标量场Φ和b夸克的LHC前景，并使用混合深度学习模型来提高探测效率。&lt;h4&gt;背景&lt;/h4&gt;之前的研究中，作者已经探讨了机器学习增强的矢量型单态B夸克衰变到单态标量或伪标量场的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在通过深度学习模型提高对这种衰变模式的探测能力。&lt;h4&gt;方法&lt;/h4&gt;采用包含图神经网络和深度神经网络的混合深度学习模型，以克服标准模型背景大和缺乏轻子末态的问题。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习分析流程的性能可以达到半轻子模式的水平，在B夸克完全异质衰变的情况下，即BR(B→bΦ) = 100%，在HL-LHC上可以达到约MB=1.8（2.4）TeV的发现（排除）范围。&lt;h4&gt;结论&lt;/h4&gt;混合深度学习模型能够显著提高对矢量型B夸克衰变到新规范单态场的研究效率，有望在HL-LHC上探测到这种衰变模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following up on our earlier study in [J. Bardhan et al., Machinelearning-enhanced search for a vectorlike singlet B quark decaying to a singletscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], weinvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decayingexotically to a new gauge-singlet (pseudo)scalar field $\Phi$ and a $b$ quark.After the electroweak symmetry breaking, the $\Phi$ decays predominantly to$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.Because of the large Standard Model background and the lack of leptonichandles, it is a difficult channel to probe. To overcome the challenge, weemploy a hybrid deep learning model containing a graph neural network followedby a deep neural network. We estimate that such a state-of-the-art deeplearning analysis pipeline can lead to a performance comparable to that in thesemi-leptonic mode, taking the discovery (exclusion) reach up to about$M_B=1.8\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\to b\Phi) = 100\%$.</description>
      <author>example@mail.com (Jai Bardhan, Tanumoy Mandal, Subhadip Mitra, Cyrin Neeraj, Mihir Rawat)</author>
      <guid isPermaLink="false">2505.07769v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Camera Control at the Edge with Language Models for Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures. This work was presented and published at the 11th  IEEE International Conference on Control, Automation and Robotics (ICCAR) in  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了优化提示统一系统（OPUS），该系统利用大型语言模型（LLM）控制PTZ相机，并提供对自然环境的上下文理解。&lt;h4&gt;背景&lt;/h4&gt;传统的PTZ相机控制方法复杂且效率低。&lt;h4&gt;目的&lt;/h4&gt;OPUS系统旨在提高PTZ相机控制的成本效益，并实现与环境的高效交互。&lt;h4&gt;方法&lt;/h4&gt;OPUS系统通过高级相机控制API生成关键词，并使用合成数据通过监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型。此外，通过将多个摄像头的数据进行文本描述，OPUS提高了环境意识。&lt;h4&gt;主要发现&lt;/h4&gt;在基准测试中，OPUS方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。&lt;h4&gt;结论&lt;/h4&gt;OPUS系统通过直观的自然语言界面简化了PTZ相机的操作，消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，这代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种优化提示统一系统（OPUS），这是一个利用大型语言模型（LLM）控制PTZ相机、提供对自然环境的上下文理解的框架。为了实现这一目标，OPUS系统通过从高级相机控制API生成关键词并通过对合成数据的监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型来提高成本效益。这使其能够在保持与GPT-4等大型模型相当性能的同时实现高效的边缘部署。OPUS通过将来自多个摄像头的数据进行文本描述来增强环境意识，消除了对专用感官标记的需求。在基准测试中，我们的方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。该系统展示了OPUS通过直观的自然语言界面简化PTZ相机操作的能力。这种方法消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Optimized Prompt-based Unified System (OPUS), aframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom(PTZ) cameras, providing contextual understanding of natural environments. Toachieve this goal, the OPUS system improves cost-effectiveness by generatingkeywords from a high-level camera control API and transferring knowledge fromlarger closed-source language models to smaller ones through SupervisedFine-Tuning (SFT) on synthetic data. This enables efficient edge deploymentwhile maintaining performance comparable to larger models like GPT-4. OPUSenhances environmental awareness by converting data from multiple cameras intotextual descriptions for language models, eliminating the need for specializedsensory tokens. In benchmark testing, our approach significantly outperformedboth traditional language model techniques and more complex prompting methods,achieving a 35% improvement over advanced techniques and a 20% higher taskaccuracy compared to closed-source models like Gemini Pro. The systemdemonstrates OPUS's capability to simplify PTZ camera operations through anintuitive natural language interface. This approach eliminates the need forexplicit programming and provides a conversational method for interacting withcamera systems, representing a significant advancement in how users can controland utilize PTZ camera technology.</description>
      <author>example@mail.com (Alexiy Buynitsky, Sina Ehsani, Bhanu Pallakonda, Pragyana Mishra)</author>
      <guid isPermaLink="false">2505.06402v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Generating Skyline Explanations for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。&lt;h4&gt;目的&lt;/h4&gt;引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。&lt;h4&gt;方法&lt;/h4&gt;1. 将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。2. 设计了高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。3. 进一步开发了一个算法来多样化解释，以提供更全面的视角。&lt;h4&gt;主要发现&lt;/h4&gt;1. 天际线解释生成问题是困难的。2. 所设计的算法在保证质量的同时，能够高效地生成解释。3. 通过多样化解释，可以提供更全面的视角。&lt;h4&gt;结论&lt;/h4&gt;使用真实世界的图，通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。现有GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。我们引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。我们将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。我们设计了一种高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。我们进一步开发了一个算法来多样化解释，以提供更全面的视角。使用真实世界的图，我们通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel approach to generate subgraph explanations forgraph neural networks GNNs that simultaneously optimize multiple measures forexplainability. Existing GNN explanation methods often compute subgraphs(called ``explanatory subgraphs'') that optimize a pre-defined, singleexplainability measure, such as fidelity or conciseness. This can lead tobiased explanations that cannot provide a comprehensive explanation to clarifythe output of GNN models. We introduce skyline explanation, a GNN explanationparadigm that aims to identify k explanatory subgraphs by simultaneouslyoptimizing multiple explainability measures. (1) We formulate skylineexplanation generation as a multi-objective optimization problem, and pursueexplanations that approximate a skyline set of explanatory subgraphs. We showthe hardness for skyline explanation generation. (2) We design efficientalgorithms with an onion-peeling approach that strategically removes edges fromneighbors of nodes of interests, and incrementally improves explanations as itexplores an interpretation domain, with provable quality guarantees. (3) Wefurther develop an algorithm to diversify explanations to provide morecomprehensive perspectives. Using real-world graphs, we empirically verify theeffectiveness, efficiency, and scalability of our algorithms.</description>
      <author>example@mail.com (Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu)</author>
      <guid isPermaLink="false">2505.07635v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GLFM的多类点云异常检测方法，通过全局-局部特征匹配逐步分离不同类别间易混淆的数据。&lt;h4&gt;背景&lt;/h4&gt;随着产品类别的增加，单类无监督方法在计算和存储成本上的限制使得多类无监督方法成为必要。&lt;h4&gt;目的&lt;/h4&gt;为了解决正常点和异常点在不同类别数据中特征相似导致的多类方法性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;GLFM分为三个阶段：第一阶段提出异常合成管道，通过拉伸点云创建丰富的异常数据来优化特征提取器；第二阶段根据全局和局部特征分布建立全局和局部记忆库，减弱特征混淆对记忆库建立的影响；第三阶段利用测试数据与全局和局部记忆库的特征距离进行异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的实验表明，GLFM在点云异常检测方面具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;GLFM是一种有效的多类点云异常检测方法，能够提高异常检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云异常检测对于各种工业应用至关重要。由于产品类别的增加导致的巨大计算和存储成本限制了单类无监督方法的应用，需要发展多类无监督方法。然而，正常点和异常点在不同类别数据中的特征相似性导致了特征混淆问题，这极大地阻碍了多类方法的表现。因此，我们引入了一种名为GLFM的多类点云异常检测方法，利用全局-局部特征匹配逐步分离跨多个类别易混淆的数据。具体来说，GLFM分为三个阶段：第一阶段提出了一种异常合成管道，通过拉伸点云创建丰富的异常数据，用于优化点云特征提取器；第二阶段根据所有训练数据的全局和局部特征分布建立全局和局部记忆库，减弱了特征混淆对记忆库建立的影响；第三阶段利用测试数据通过其与全局和局部记忆库的特征距离进行异常检测。在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的大量实验展示了我们提出的GLFM在点云异常检测方面的优越性能。代码可在https://github.com/hustCYQ/GLFM-Multi-class-3DAD上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hustCYQ/GLFM-Multi-class-3DAD&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud anomaly detection is essential for various industrialapplications. The huge computation and storage costs caused by the increasingproduct classes limit the application of single-class unsupervised methods,necessitating the development of multi-class unsupervised methods. However, thefeature similarity between normal and anomalous points from different classdata leads to the feature confusion problem, which greatly hinders theperformance of multi-class methods. Therefore, we introduce a multi-class pointcloud anomaly detection method, named GLFM, leveraging global-local featurematching to progressively separate data that are prone to confusion acrossmultiple classes. Specifically, GLFM is structured into three stages: Stage-Iproposes an anomaly synthesis pipeline that stretches point clouds to createabundant anomaly data that are utilized to adapt the point cloud featureextractor for better feature representation. Stage-II establishes the globaland local memory banks according to the global and local feature distributionsof all the training data, weakening the impact of feature confusion on theestablishment of the memory bank. Stage-III implements anomaly detection oftest data leveraging its feature distance from global and local memory banks.Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry partsdataset showcase our proposed GLFM's superior point cloud anomaly detectionperformance. The code is available athttps://github.com/hustCYQ/GLFM-Multi-class-3DAD.</description>
      <author>example@mail.com (Yuqi Cheng, Yunkang Cao, Dongfang Wang, Weiming Shen, Wenlong Li)</author>
      <guid isPermaLink="false">2505.07375v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation</title>
      <link>http://arxiv.org/abs/2505.07674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注在复杂拓扑环境中预测网络流量的挑战，提出了一种结合图卷积网络（GCN）和门控循环单元（GRU）的时空建模方法，通过实验验证了该方法在复杂网络流量预测场景中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;网络流量预测在复杂拓扑环境中是一个挑战，需要有效的方法来捕捉空间依赖和时间演变。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合GCN和GRU的时空建模方法，以精确预测未来的网络流量模式。&lt;h4&gt;方法&lt;/h4&gt;该方法通过GCN捕捉网络节点间的空间依赖，GRU模拟流量数据的时间演变。通过在真实世界Abilene网络流量数据集上进行实验，将所提模型与多种深度学习方法进行比较，并进行了消融实验以检验不同组件对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提方法在多个指标上均优于其他方法，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空建模方法在复杂网络流量预测中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本研究关注于在复杂拓扑环境中预测网络流量的挑战。它引入了一种时空建模方法，该方法结合了图卷积网络（GCN）和门控循环单元（GRU）。GCN组件捕捉网络节点之间的空间依赖，而GRU组件模拟流量数据的时间演变。这种组合允许精确预测未来的流量模式。通过在真实世界的Abilene网络流量数据集上进行全面实验，验证了所提模型的有效性。该模型与几种流行的深度学习方法进行了基准测试。此外，进行了一系列消融实验来检验各种组件对性能的影响，包括图卷积层的数量变化、不同的时间建模策略以及构建邻接矩阵的方法。结果表明，所提方法在多个指标上均取得了优异的性能，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the challenge of predicting network traffic withincomplex topological environments. It introduces a spatiotemporal modelingapproach that integrates Graph Convolutional Networks (GCN) with GatedRecurrent Units (GRU). The GCN component captures spatial dependencies amongnetwork nodes, while the GRU component models the temporal evolution of trafficdata. This combination allows for precise forecasting of future trafficpatterns. The effectiveness of the proposed model is validated throughcomprehensive experiments on the real-world Abilene network traffic dataset.The model is benchmarked against several popular deep learning methods.Furthermore, a set of ablation experiments is conducted to examine theinfluence of various components on performance, including changes in the numberof graph convolution layers, different temporal modeling strategies, andmethods for constructing the adjacency matrix. Results indicate that theproposed approach achieves superior performance across multiple metrics,demonstrating robust stability and strong generalization capabilities incomplex network traffic forecasting scenarios.</description>
      <author>example@mail.com (Nan Jiang, Wenxuan Zhu, Xu Han, Weiqiang Huang, Yumeng Sun)</author>
      <guid isPermaLink="false">2505.07674v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Across Fixed-Income Product Classes</title>
      <link>http://arxiv.org/abs/2505.07676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。&lt;h4&gt;背景&lt;/h4&gt;由于从稀疏或噪声数据中估计折扣曲线的挑战，将核岭回归（KR）扩展到向量值设置。&lt;h4&gt;目的&lt;/h4&gt;提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。&lt;h4&gt;方法&lt;/h4&gt;在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的正则化项导致有效的可分离核结构，理论贡献是可分离核引起的向量值RKHS范数的分解。&lt;h4&gt;结论&lt;/h4&gt;示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。由于从稀疏或噪声数据中估计折扣曲线的挑战，我们将核岭回归（KR）扩展到向量值设置。提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a framework for transfer learning of discount curves acrossdifferent fixed-income product classes. Motivated by challenges in estimatingdiscount curves from sparse or noisy data, we extend kernel ridge regression(KR) to a vector-valued setting, formulating a convex optimization problem in avector-valued reproducing kernel Hilbert space (RKHS). Each component of thesolution corresponds to the discount curve implied by a specific product class.We introduce an additional regularization term motivated by economicprinciples, promoting smoothness of spread curves between product classes, andshow that it leads to a valid separable kernel structure. A main theoreticalcontribution is a decomposition of the vector-valued RKHS norm induced byseparable kernels. We further provide a Gaussian process interpretation ofvector-valued KR, enabling quantification of estimation uncertainty.Illustrative examples demonstrate that transfer learning significantly improvesextrapolation performance and tightens confidence intervals compared tosingle-curve estimation.</description>
      <author>example@mail.com (Nicolas Camenzind, Damir Filipovic)</author>
      <guid isPermaLink="false">2505.07676v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2505.07322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RealRep的通用SDR到HDR转换方法，用于处理现实场景中风格多样的SDR内容。&lt;h4&gt;背景&lt;/h4&gt;HDR-WCG技术越来越普及，对将SDR内容转换为HDR的需求日益增加。现有的方法主要依赖于固定的色调映射算子，对于处理具有多种风格的SDR输入不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了一种名为RealRep的方法，可以处理现实场景中风格多样的SDR内容。&lt;h4&gt;方法&lt;/h4&gt;通过分离亮度（luminance）和色度（chrominance），分析不同风格内容之间的内在差异，并提出了一个解耦的多视角风格表示学习方法。该方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。此外，为了解决直接利用退化表示先验的困难，引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。&lt;h4&gt;主要发现&lt;/h4&gt;RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;RealRep和DDACMNet能够有效地将SDR内容转换为HDR，为HDR内容的制作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高动态范围宽色域（HDR-WCG）技术越来越普及，对将标准动态范围（SDR）内容转换为HDR的需求日益增加。现有方法主要依赖于固定的色调映射算子，对于处理现实场景中常见的多种风格的SDR输入不足。为了解决这一挑战，我们提出了一种处理现实场景中风格多样的SDR内容的通用SDR到HDR方法，称为Realistic Style Disentangled Representation Learning（RealRep）。通过分离亮度（luminance）和色度（chrominance），我们分析了具有不同风格的内容之间的内在差异，并提出了一种解耦的多视角风格表示学习方法。这种方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。受直接利用退化表示先验的困难所启发，我们进一步引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。大量的实验表明，RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becomingincreasingly prevalent, intensifying the demand for converting Standard DynamicRange (SDR) content to HDR. Existing methods primarily rely on fixed tonemapping operators, which are inadequate for handling SDR inputs with diversestyles commonly found in real-world scenarios. To address this challenge, wepropose a generalized SDR-to-HDR method that handles diverse styles inreal-world SDR content, termed Realistic Style Disentangled RepresentationLearning (RealRep). By disentangling luminance and chrominance, we analyze theintrinsic differences between contents with varying styles and propose adisentangled multi-view style representation learning method. This approachcaptures the guidance prior of true luminance and chrominance distributionsacross different styles, even when the SDR style distributions exhibitsignificant variations, thereby establishing a robust embedding space forinverse tone mapping. Motivated by the difficulty of directly utilizingdegradation representation priors, we further introduce the Degradation-DomainAware Controlled Mapping Network (DDACMNet), a two-stage framework thatperforms adaptive hierarchical mapping guided by a control-aware normalizationmechanism. DDACMNet dynamically modulates the mapping process viadegradation-conditioned hierarchical features, enabling robust adaptationacross diverse degradation domains. Extensive experiments show that RealRepconsistently outperforms state-of-the-art methods with superior generalizationand perceptually faithful HDR color gamut reconstruction.</description>
      <author>example@mail.com (Gang He, Siqi Wang, Kepeng Xu, Lin Zhang)</author>
      <guid isPermaLink="false">2505.07322v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes</title>
      <link>http://arxiv.org/abs/2505.07315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FederatedInvariant Features Learning (FedIFL)的联邦跨域诊断框架，用于解决工业数据稀缺导致的故障诊断模型训练难题。&lt;h4&gt;背景&lt;/h4&gt;由于工业数据稀缺，尤其是对于初创企业，独立训练全面的故障诊断模型存在困难；联邦学习能够在保证数据隐私的同时实现协同训练，因此成为一个理想的解决方案。&lt;h4&gt;目的&lt;/h4&gt;为了解决联邦诊断场景中标签空间不一致导致的问题，提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FedIFL框架通过原型对比学习减轻客户端域偏移，同时确保本地模型能够以隐私友好的方式访问其他客户端的分布。此外，引入特征解耦机制来减轻跨客户端域偏移，并设计实例级联邦实例一致性损失来保证不同客户端之间不变特征的实例级一致性。还构建了联邦实例个性化损失和正交损失来区分特定特征与不变特征。&lt;h4&gt;主要发现&lt;/h4&gt;FedIFL框架在全局标签空间中实现了良好的泛化能力，能够在标签空间不一致的情况下为目标客户端的电动机驱动系统（MDS）提供准确的故障诊断。&lt;h4&gt;结论&lt;/h4&gt;FedIFL在联邦跨域诊断中表现出色，能够有效解决不一致故障模式的问题。&lt;h4&gt;翻译&lt;/h4&gt;Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focusing on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the scarcity of industrial data, individual equipment users,particularly start-ups, struggle to independently train a comprehensive faultdiagnosis model; federated learning enables collaborative training whileensuring data privacy, making it an ideal solution. However, the diversity ofworking conditions leads to variations in fault modes, resulting ininconsistent label spaces across different clients. In federated diagnosticscenarios, label space inconsistency leads to local models focus onclient-specific fault modes and causes local models from different clients tomap different failure modes to similar feature representations, which weakensthe aggregated global model's generalization. To tackle this issue, thisarticle proposed a federated cross-domain diagnostic framework termed FederatedInvariant Features Learning (FedIFL). In intra-client training, prototypecontrastive learning mitigates intra-client domain shifts, subsequently,feature generating ensures local models can access distributions of otherclients in a privacy-friendly manner. Besides, in cross-client training, afeature disentanglement mechanism is introduced to mitigate cross-client domainshifts, specifically, an instance-level federated instance consistency loss isdesigned to ensure the instance-level consistency of invariant features betweendifferent clients, furthermore, a federated instance personalization loss andan orthogonal loss are constructed to distinguish specific features that fromthe invariant features. Eventually, the aggregated model achieves promisinggeneralization among global label spaces, enabling accurate fault diagnosis fortarget clients' Motor Driven Systems (MDSs) with inconsistent label spaces.Experiments on real-world MDSs validate the effectiveness and superiority ofFedIFL in federated cross-domain diagnosis with inconsistent fault modes.</description>
      <author>example@mail.com (Zexiao Wang, Yankai Wang, Xiaoqiang Liao, Xinguo Ming, Weiming Shen)</author>
      <guid isPermaLink="false">2505.07315v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Chronocept: Instilling a Sense of Time in Machines</title>
      <link>http://arxiv.org/abs/2505.07637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 8 figures, 18 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。&lt;h4&gt;背景&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Chronocept，作为第一个将时间有效性建模为时间上的连续概率分布的基准。&lt;h4&gt;方法&lt;/h4&gt;Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;h4&gt;结论&lt;/h4&gt;Chronocept通过提供对时间有效性的建模，为人工智能在时间推理方面的发展提供了新的可能性，并促进了相关领域的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human cognition is deeply intertwined with a sense of time, known asChronoception. This sense allows us to judge how long facts remain valid andwhen knowledge becomes outdated. Despite progress in vision, language, andmotor control, AI still struggles to reason about temporal validity. Weintroduce Chronocept, the first benchmark to model temporal validity as acontinuous probability distribution over time. Using skew-normal curves fittedalong semantically decomposed temporal axes, Chronocept captures nuancedpatterns of emergence, decay, and peak relevance. It includes two datasets:Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).Annotations show strong inter-annotator agreement (84% and 89%). Our baselinespredict curve parameters - location, scale, and skewness - enablinginterpretable, generalizable learning and outperforming classification-basedapproaches. Chronocept fills a foundational gap in AI's temporal reasoning,supporting applications in knowledge grounding, fact-checking,retrieval-augmented generation (RAG), and proactive agents. Code and data arepublicly available.</description>
      <author>example@mail.com (Krish Goel, Sanskar Pandey, KS Mahadevan, Harsh Kumar, Vishesh Khadaria)</author>
      <guid isPermaLink="false">2505.07637v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures</title>
      <link>http://arxiv.org/abs/2505.07070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了神经网络语言模型在训练用于预测下一个标记时如何获取语言结构。通过分析随机层次模型（RHM）生成的合成数据集，得出了神经网络性能的理论扩展规律。&lt;h4&gt;背景&lt;/h4&gt;研究者已经开发了一种基于数据相关性的表示学习理论，该理论解释了深度学习模型如何按层次结构逐层捕捉数据。&lt;h4&gt;目的&lt;/h4&gt;研究目的是扩展理论框架以考虑架构差异，并预测和验证卷积网络和Transformer模型在性能扩展方面的差异。&lt;h4&gt;方法&lt;/h4&gt;研究者通过随机层次模型（RHM）生成合成数据集，并比较了卷积网络和Transformer模型在这些数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，卷积网络由于其结构通过局部性和权重共享与生成过程对齐，其性能扩展速度比依赖全局自注意力机制的Transformer模型更快。&lt;h4&gt;结论&lt;/h4&gt;这一发现阐明了神经网络扩展规律背后的架构偏差，并强调了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;h4&gt;翻译&lt;/h4&gt;摘要：神经网络语言模型在训练用于预测下一个标记时如何获取语言结构？我们通过推导神经网络在由随机层次模型（RHM）生成的合成数据集上的性能理论扩展规律来回答这个问题。RHM是一组概率上下文无关文法集合，旨在捕获自然语言的层次结构，同时保持可分析性。此前，我们已开发了一种基于数据相关性的表示学习理论，解释了深度学习模型如何按层次结构逐层捕捉数据。在这里，我们将我们的理论框架扩展以考虑架构差异。特别是，我们预测并经验性地验证了卷积网络（其结构通过局部性和权重共享与生成过程对齐）的性能扩展速度比依赖全局自注意力机制的Transformer模型更快。这一发现阐明了神经网络扩展规律背后的架构偏差，并突出了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How do neural language models acquire a language's structure when trained fornext-token prediction? We address this question by deriving theoretical scalinglaws for neural network performance on synthetic datasets generated by theRandom Hierarchy Model (RHM) -- an ensemble of probabilistic context-freegrammars designed to capture the hierarchical structure of natural languagewhile remaining analytically tractable. Previously, we developed a theory ofrepresentation learning based on data correlations that explains how deeplearning models capture the hierarchical structure of the data sequentially,one layer at a time. Here, we extend our theoretical framework to account forarchitectural differences. In particular, we predict and empirically validatethat convolutional networks, whose structure aligns with that of the generativeprocess through locality and weight sharing, enjoy a faster scaling ofperformance compared to transformer models, which rely on global self-attentionmechanisms. This finding clarifies the architectural biases underlying neuralscaling laws and highlights how representation learning is shaped by theinteraction between model architecture and the statistical properties of data.</description>
      <author>example@mail.com (Francesco Cagnetta, Alessandro Favero, Antonio Sclocchi, Matthieu Wyart)</author>
      <guid isPermaLink="false">2505.07070v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Is MixIT Really Unsuitable for Correlated Sources? Exploring MixIT for Unsupervised Pre-training in Music Source Separation</title>
      <link>http://arxiv.org/abs/2505.07631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于MixIT的预训练在音乐源分离（MSS）中的应用，探讨了MixIT在MSS中的潜力和改进方法。&lt;h4&gt;背景&lt;/h4&gt;音乐源分离是一个高成本的过程，因此利用未标记数据进行预训练是一个有前景的方法。尽管MixIT这类无监督学习方法在一般声音分离中被探索，但在MSS中由于其隐含的源独立性假设而被忽视。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过MixIT预训练来提高MSS的性能，并探索MixIT在MSS中的潜力。&lt;h4&gt;方法&lt;/h4&gt;首先，使用MixIT在未经标记的野外部数据上进行模型预训练，然后在使用MUSDB18数据集进行监督的情况下进行微调。使用band-split TF-Locoformer模型进行实验，这是一种最先进MSS模型之一。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，尽管MixIT不假设任何源模型并且处理模糊性有困难，但它仍能在一定程度上分离乐器，显示出其在无监督预训练中的潜力。&lt;h4&gt;结论&lt;/h4&gt;MixIT预训练可以提高MSS的性能，优于从头开始训练的方法。&lt;h4&gt;翻译&lt;/h4&gt;In music source separation (MSS), obtaining isolated sources or stems is highly costly, making pre-training on unlabeled data a promising approach. Although source-agnostic unsupervised learning like mixture-invariant training (MixIT) has been explored in general sound separation, they have been largely overlooked in MSS due to its implicit assumption of source independence. We hypothesize, however, that the difficulty of applying MixIT to MSS arises from the ill-posed nature of MSS itself, where stem definitions are application-dependent and models lack explicit knowledge of what should or should not be separated, rather than from high inter-source correlation. While MixIT does not assume any source model and struggles with such ambiguities, our preliminary experiments show that it can still separate instruments to some extent, suggesting its potential for unsupervised pre-training. Motivated by these insights, this study investigates MixIT-based pre-training for MSS. We first pre-train a model on in-the-wild, unlabeled data from the Free Music Archive using MixIT, and then fine-tune it on MUSDB18 with supervision. Using the band-split TF-Locoformer, one of the state-of-the-art MSS models, we demonstrate that MixIT-based pre-training improves the performance over training from scratch.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In music source separation (MSS), obtaining isolated sources or stems ishighly costly, making pre-training on unlabeled data a promising approach.Although source-agnostic unsupervised learning like mixture-invariant training(MixIT) has been explored in general sound separation, they have been largelyoverlooked in MSS due to its implicit assumption of source independence. Wehypothesize, however, that the difficulty of applying MixIT to MSS arises fromthe ill-posed nature of MSS itself, where stem definitions areapplication-dependent and models lack explicit knowledge of what should orshould not be separated, rather than from high inter-source correlation. WhileMixIT does not assume any source model and struggles with such ambiguities, ourpreliminary experiments show that it can still separate instruments to someextent, suggesting its potential for unsupervised pre-training. Motivated bythese insights, this study investigates MixIT-based pre-training for MSS. Wefirst pre-train a model on in-the-wild, unlabeled data from the Free MusicArchive using MixIT, and then fine-tune it on MUSDB18 with supervision. Usingthe band-split TF-Locoformer, one of the state-of-the-art MSS models, wedemonstrate that MixIT-based pre-training improves the performance overtraining from scratch.</description>
      <author>example@mail.com (Kohei Saijo, Yoshiaki Bando)</author>
      <guid isPermaLink="false">2505.07631v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用自动检索合成CAD模型的方法，生成高质量的3D标注数据，用于训练深度学习模型，从而提高模型性能并降低标注成本。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成准确的3D标注数据对深度学习模型的发展构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用自动检索合成CAD模型的方法来生成高质量的3D标注数据，并验证这种方法在训练深度学习模型中的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程，应用于ScanNet++ v1数据集，以生成自动标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，使用自动获得的标注数据训练的深度学习模型不仅可行，而且性能优于使用手动标注数据训练的模型。验证了该方法在点云补全和单视图CAD模型检索与对齐两个任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注有潜力提高模型性能，同时显著降低标注成本，并将发布相关标注数据和训练模型以支持未来的3D场景理解研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高级3D场景理解在许多应用中至关重要。然而，生成准确3D标注的挑战使得深度学习模型的发展变得困难。我们转向最近在自动检索合成CAD模型方面的进展，并表明这种方法生成的数据可以用作训练监督深度学习模型的高质量真实标签。更具体地说，我们采用了与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程。这次，我们将它应用于之前缺乏此类标注的ScanNet++ v1数据集。我们的发现表明，不仅可以在这些自动获得的标注数据上训练深度学习模型，而且所得到的模型在性能上优于在手动标注数据上训练的模型。我们在两个不同的任务上验证了这一点：点云补全和单视图CAD模型检索与对齐。我们的结果强调了自动3D标注在提高模型性能的同时显著降低标注成本的可能性。为了支持3D场景理解的未来研究，我们将发布我们的标注，我们称之为SCANnotate++，以及我们的训练模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Wireless Link Scheduling with State-Augmented Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模无线自组织网络中的最优链路调度问题，旨在在保证链路传输公平性的前提下，最大化长期平均性能。&lt;h4&gt;背景&lt;/h4&gt;针对无线自组织网络中链路调度的问题，本文提出了一种基于图神经网络的优化调度策略。&lt;h4&gt;目的&lt;/h4&gt;目标是实现链路调度的长期平均性能最大化，同时确保每个链路的最小传输需求，以保证公平性。&lt;h4&gt;方法&lt;/h4&gt;利用图结构来表示链路冲突，构建了一个受约束的优化问题，并通过图神经网络（GNN）参数化调度策略。使用状态增强技术应对长期性能的挑战，通过将拉格朗日对偶变量作为调度策略的动态输入，训练GNN逐渐调整调度决策以实现最小传输需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟验证了所提策略的有效性，并在各种网络设置中将其性能与多个基线进行了比较。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于GNN的链路调度策略在保证公平性的同时，实现了长期平均性能的最大化，具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term average performance, subject to a minimum transmission requirement for each link to ensure fairness. With a graph structure utilized to represent the conflicts of links, we formulate a constrained optimization problem to learn the scheduling policy, which is parameterized with a graph neural network (GNN). To address the challenge of long-term performance, we use the state-augmentation technique. In particular, by augmenting the Lagrangian dual variables as dynamic inputs to the scheduling policy, the GNN can be trained to gradually adapt the scheduling decisions to achieve the minimum transmission requirements. We verify the efficacy of our proposed policy through numerical simulations and compare its performance with several baselines in various network settings.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term averageperformance, subject to a minimum transmission requirement for each link toensure fairness. With a graph structure utilized to represent the conflicts oflinks, we formulate a constrained optimization problem to learn the schedulingpolicy, which is parameterized with a graph neural network (GNN). To addressthe challenge of long-term performance, we use the state-augmentationtechnique. In particular, by augmenting the Lagrangian dual variables asdynamic inputs to the scheduling policy, the GNN can be trained to graduallyadapt the scheduling decisions to achieve the minimum transmissionrequirements. We verify the efficacy of our proposed policy through numericalsimulations and compare its performance with several baselines in variousnetwork settings.</description>
      <author>example@mail.com (Romina Garcia Camargo, Zhiyang Wang, Navid NaderiAlizadeh, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2505.07598v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种用于单目3D目标检测的深度估计方法，通过链式预测（CoP）来提高深度估计的准确性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;3D属性预测对于单目3D目标检测至关重要，其中深度估计是最具挑战性的部分，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP方法，通过条件预测和特征链式传播来提高深度估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用以下设计：使用轻量级属性网络（AN）学习每个3D属性的特征；构建显式的特征传播链；使用残差连接聚合特征，确保后续属性预测基于所有已处理的属性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MonoCoP在KITTI排行榜上达到了最先进的性能，并在Waymo和nuScenes frontal数据集上超过了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP是一种有效的单目3D目标检测方法，能够显著提高深度估计的准确性，且无需额外数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
      <link>http://arxiv.org/abs/2505.07576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种在半导体制造领域应用的视觉异常检测（VAD）方法，通过利用MIIC数据集建立了一个基准，验证了现代VAD方法在该领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是一个复杂的多阶段过程，自动视觉检测对于减少设备停机时间和控制成本至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需大量异常标记样本的VAD方法，避免昂贵的缺陷收集阶段，同时提供预测的解释。&lt;h4&gt;方法&lt;/h4&gt;通过利用MIIC数据集，建立了一个VAD在半导体领域的基准，并展示了现代VAD方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;现代VAD方法在半导体领域显示出良好的效果。&lt;h4&gt;结论&lt;/h4&gt;VAD方法在半导体制造领域具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is a complex, multistage process. Automatedvisual inspection of Scanning Electron Microscope (SEM) images is indispensablefor minimizing equipment downtime and containing costs. Most previous researchconsiders supervised approaches, assuming a sufficient number of anomalouslylabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emergingresearch domain, focuses on unsupervised learning, avoiding the costly defectcollection phase while providing explanations of the predictions. We introducea benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.Our results demonstrate the efficacy of modern VAD approaches in this field.</description>
      <author>example@mail.com (Manuel Barusco, Francesco Borsatti, Youssef Ben Khalifa, Davide Dalle Pezze, Gian Antonio Susto)</author>
      <guid isPermaLink="false">2505.07576v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
      <link>http://arxiv.org/abs/2505.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a long paper at the Educational Data Mining (EDM)  Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了教师课堂中视觉注意力的分布对学生学习参与度、成就和专业教师培训的重要影响，并提出了一种自动化处理流程，利用先进的人脸检测和识别技术来减少手动标注数据的需求。&lt;h4&gt;背景&lt;/h4&gt;虽然教师对学生的关注点对学生学习有重要影响，但推断教师关注的位置和学生不是一件容易的事。移动眼动追踪可以提供帮助，但需要大量手动标注。&lt;h4&gt;目的&lt;/h4&gt;减少手动标注数据，以识别教师关注的特定学生。&lt;h4&gt;方法&lt;/h4&gt;使用最先进的面部检测模型和特征嵌入训练面部识别模型，结合移动眼动追踪数据，并在课堂环境中进行迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的课堂环境中评估了该方法，结果表明在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。&lt;h4&gt;结论&lt;/h4&gt;该方法可以在不要求大量手动标注数据的情况下，以非侵入的方式处理教师的视觉注意力，有助于改进教学策略、增强课堂管理和提供专业教师发展的反馈。&lt;h4&gt;翻译&lt;/h4&gt;摘要：教师对课堂学生的视觉注意力和其分布对学生参与度、成就以及专业教师培训具有重要作用。尽管如此，推断教师关注的位置和对象并非易事。移动眼动追踪可以提供重要帮助，但仅使用移动眼动追踪需要大量的手动标注。为了解决这一局限性，我们提出了一种自动化处理流程的概念，该流程需要最少的手动标注数据来识别教师关注的特定学生。为此，我们利用最先进的面部检测模型和面部识别特征嵌入，在课堂环境中进行迁移学习，以训练面部识别模型，并将这些模型与来自移动眼动追踪器的教师的注视结合。我们使用从四个不同课堂收集的数据评估了我们的方法，结果表明，虽然在我们的所有课堂设置中都可以以合理的表现估计视觉关注的学生，但在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。虽然我们没有评估我们的方法在师生互动中的应用，并且专注于技术方法的合理性，但鉴于我们的方法不需要大量的手动标注数据，并以非侵入的方式处理教师的视觉注意力，它可以帮助改进教学策略，增强课堂管理，并为专业教师发展提供反馈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teachers' visual attention and its distribution across the students inclassrooms can constitute important implications for student engagement,achievement, and professional teacher training. Despite that, inferring theinformation about where and which student teachers focus on is not trivial.Mobile eye tracking can provide vital help to solve this issue; however, theuse of mobile eye tracking alone requires a significant amount of manualannotations. To address this limitation, we present an automated processingpipeline concept that requires minimal manually annotated data to recognizewhich student the teachers focus on. To this end, we utilize state-of-the-artface detection models and face recognition feature embeddings to train facerecognition models with transfer learning in the classroom context and combinethese models with the teachers' gaze from mobile eye trackers. We evaluated ourapproach with data collected from four different classrooms, and our resultsshow that while it is possible to estimate the visually focused students withreasonable performance in all of our classroom setups, U-shaped and smallclassrooms led to the best results with accuracies of approximately 0.7 and0.9, respectively. While we did not evaluate our method for teacher-studentinteractions and focused on the validity of the technical approach, as ourmethodology does not require a vast amount of manually annotated data andoffers a non-intrusive way of handling teachers' visual attention, it couldhelp improve instructional strategies, enhance classroom management, andprovide feedback for professional teacher development.</description>
      <author>example@mail.com (Efe Bozkir, Christian Kosel, Tina Seidel, Enkelejda Kasneci)</author>
      <guid isPermaLink="false">2505.07552v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling</title>
      <link>http://arxiv.org/abs/2505.07157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，该架构利用大型语言模型（LLMs）来解决传统主题模型在处理语境细微差别、多义词和罕见词时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型在处理语境细微差别、多义词和罕见词时存在困难，导致生成的主题缺乏连贯性和质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法，通过使用LLMs生成初始主题，并通过神经网络增强语义融合来精炼这些主题嵌入。&lt;h4&gt;方法&lt;/h4&gt;使用BERT和图神经网络（GNN）进行主题嵌入的精炼，并引入了一种新的计算相似性的方法，以进一步精炼主题嵌入并提取前k个主题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用两个医疗数据集（一个英语和一个法语）的六个集合，HAMLET在主题建模方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;HAMLET通过结合LLMs、BERT、SBERT和GNN等技术，能够有效地提高主题建模的质量和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的主题模型往往难以处理语境细微差别，无法充分处理多义词和罕见词。这种限制通常会导致缺乏连贯性和质量的主题。大型语言模型（LLMs）可以通过生成一组初始主题来缓解这个问题。然而，这些原始主题通常缺乏精炼和代表性，导致冗余且缺乏词汇相似性，以及可解释性降低。本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，它使用LLMs。所提出的方法利用神经网络增强语义融合来精炼LLM生成的主题嵌入。这种方法不依赖于仅统计共现或人类解释来从文档语料库中提取主题，而是引入了一种主题嵌入精炼方法，该方法使用双向编码器表示从Transformer（BERT）和图神经网络（GNN）。在主题生成后，采用BERT和Sentence-BERT（SBERT）相结合的混合技术进行嵌入。使用GNN进一步精炼主题表示，GNN在文档、主题、单词、相似主题和相似单词之间建立连接。引入了一种新的计算相似性的方法。因此，精炼了主题嵌入，并提取了前k个主题。使用两个医疗数据集（一个英语和一个法语）的六个集合进行了实验。结果表明，HAMLET在主题建模方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models often struggle with contextual nuances and fail toadequately handle polysemy and rare words. This limitation typically results intopics that lack coherence and quality. Large Language Models (LLMs) canmitigate this issue by generating an initial set of topics. However, these rawtopics frequently lack refinement and representativeness, which leads toredundancy without lexical similarity and reduced interpretability. This paperintroduces HAMLET, a graph-driven architecture for cross-lingual healthcaretopic modeling that uses LLMs. The proposed approach leverages neural-enhancedsemantic fusion to refine the embeddings of topics generated by the LLM.Instead of relying solely on statistical co-occurrence or human interpretationto extract topics from a document corpus, this method introduces a topicembedding refinement that uses Bidirectional Encoder Representations fromTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, ahybrid technique that involves BERT and Sentence-BERT (SBERT) is employed forembedding. The topic representations are further refined using a GNN, whichestablishes connections between documents, topics, words, similar topics, andsimilar words. A novel method is introduced to compute similarities.Consequently, the topic embeddings are refined, and the top k topics areextracted. Experiments were conducted using two healthcare datasets, one inEnglish and one in French, from which six sets were derived. The resultsdemonstrate the effectiveness of HAMLET.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2505.07157v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Seed1.5-VL Technical Report</title>
      <link>http://arxiv.org/abs/2505.07062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Seed1.5-VL，这是一个用于提升通用多模态理解和推理能力的视觉语言基础模型。&lt;h4&gt;背景&lt;/h4&gt;Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。&lt;h4&gt;目的&lt;/h4&gt;Seed1.5-VL旨在在公共VLM基准测试和内部评估套件中提供强大性能，并超越现有的多模态系统。&lt;h4&gt;方法&lt;/h4&gt;模型设计、数据构建和训练过程中的经验被综合回顾。&lt;h4&gt;主要发现&lt;/h4&gt;Seed1.5-VL在60个公共基准测试中的38个上达到了最先进的性能，并在GUI控制和游戏等以代理为中心的任务中优于OpenAICUA和Claude 3.7。&lt;h4&gt;结论&lt;/h4&gt;Seed1.5-VL的推理能力使其特别适用于多模态推理挑战，如视觉谜题，并有望在多个任务中应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Seed1.5-VL，一个旨在提升通用多模态理解和推理能力的视觉语言基础模型。Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。尽管其架构相对紧凑，但它能在广泛的公共VLM基准测试和内部评估套件中提供强大性能，在60个公共基准测试中的38个上达到了最先进的性能。此外，在以代理为中心的任务，如GUI控制和游戏玩法中，Seed1.5-VL超越了包括OpenAICUA和Claude 3.7在内的领先的多模态系统。除了视觉和视频理解之外，它还表现出强大的推理能力，使其特别适用于多模态推理挑战，如视觉谜题。我们相信这些能力将使它在多个任务中具有更广泛的应用。在本报告中，我们主要提供了在模型设计、数据构建和训练各阶段构建Seed1.5-VL的经验的综合回顾，希望这份报告能够激发进一步的研究。Seed1.5-VL现在可通过https://www.volcengine.com/（火山引擎模型ID：doubao-1-5-thinking-vision-pro-250428）获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Seed1.5-VL, a vision-language foundation model designed to advancegeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composedwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20Bactive parameters. Despite its relatively compact architecture, it deliversstrong performance across a wide spectrum of public VLM benchmarks and internalevaluation suites, achieving the state-of-the-art performance on 38 out of 60public benchmarks. Moreover, in agent-centric tasks such as GUI control andgameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAICUA and Claude 3.7. Beyond visual and video understanding, it also demonstratesstrong reasoning abilities, making it particularly effective for multimodalreasoning challenges such as visual puzzles. We believe these capabilities willempower broader applications across diverse tasks. In this report, we mainlyprovide a comprehensive review of our experiences in building Seed1.5-VL acrossmodel design, data construction, and training at various stages, hoping thatthis report can inspire further research. Seed1.5-VL is now accessible athttps://www.volcengine.com/ (Volcano Engine Model ID:doubao-1-5-thinking-vision-pro-250428)</description>
      <author>example@mail.com (Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, Zuquan Song)</author>
      <guid isPermaLink="false">2505.07062v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition</title>
      <link>http://arxiv.org/abs/2505.07166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in SIGIR-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。&lt;h4&gt;背景&lt;/h4&gt;密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。&lt;h4&gt;目的&lt;/h4&gt;重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。&lt;h4&gt;方法&lt;/h4&gt;测试了不同的表示方法（比较使用CLS标记与平均池化）、骨干架构（仅编码器的BERT与仅解码器的LLaMA）和额外的数据集（MSMARCO和Natural Questions）。&lt;h4&gt;主要发现&lt;/h4&gt;在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化（Contriever）和解码器基于（LLaMA）模型中。&lt;h4&gt;结论&lt;/h4&gt;确保了研究的可重复性，并将实现公开提供。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。背景是密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。研究目的是重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。研究方法包括测试不同的表示方法、骨干架构和额外的数据集。主要发现是在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化和解码器基于模型中。结论是确保了研究的可重复性，并将实现公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ielab/denseretriever-knowledge-acquisition&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense retrievers utilize pre-trained backbone language models (e.g., BERT,LLaMA) that are fine-tuned via contrastive learning to perform the task ofencoding text into sense representations that can be then compared via ashallow similarity operation, e.g. inner product. Recent research hasquestioned the role of fine-tuning vs. that of pre-training within denseretrievers, specifically arguing that retrieval knowledge is primarily gainedduring pre-training, meaning knowledge not acquired during pre-training cannotbe sub-sequentially acquired via fine-tuning. We revisit this idea here as theclaim was only studied in the context of a BERT-based encoder using DPR asrepresentative dense retriever. We extend the previous analysis by testingother representation approaches (comparing the use of CLS tokens with that ofmean pooling), backbone architectures (encoder-only BERT vs. decoder-onlyLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Ourstudy confirms that in DPR tuning, pre-trained knowledge underpins retrievalperformance, with fine-tuning primarily adjusting neuron activation rather thanreorganizing knowledge. However, this pattern does not hold universally, suchas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure fullreproducibility and make our implementation publicly available athttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition.</description>
      <author>example@mail.com (Zheng Yao, Shuai Wang, Guido Zuccon)</author>
      <guid isPermaLink="false">2505.07166v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis</title>
      <link>http://arxiv.org/abs/2505.07487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LinuxData，一个涵盖多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。&lt;h4&gt;背景&lt;/h4&gt;配置Linux内核以满足特定要求（如二进制大小）非常具有挑战性，因为内核选项众多且版本间快速变化。&lt;h4&gt;目的&lt;/h4&gt;为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。&lt;h4&gt;方法&lt;/h4&gt;LinuxData通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;LinuxData通过OpenML平台易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;h4&gt;翻译&lt;/h4&gt;本文提出LinuxData，一个包含多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。由于内核选项众多且版本间快速变化，配置Linux内核以满足特定要求（如二进制大小）具有很大挑战性。为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。该数据集通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。通过OpenML平台，LinuxData易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Configuring the Linux kernel to meet specific requirements, such as binarysize, is highly challenging due to its immense complexity-with over 15,000interdependent options evolving rapidly across different versions. Althoughseveral studies have explored sampling strategies and machine learning methodsto understand and predict the impact of configuration options, the literaturestill lacks a comprehensive and large-scale dataset encompassing multiplekernel versions along with detailed quantitative measurements. To bridge thisgap, we introduce LinuxData, an accessible collection of kernel configurationsspanning several kernel releases, specifically from versions 4.13 to 5.8. Thisdataset, gathered through automated tools and build processes, comprises over240,000 kernel configurations systematically labeled with compilation outcomesand binary sizes. By providing detailed records of configuration evolution andcapturing the intricate interplay among kernel options, our dataset enablesinnovative research in feature subset selection, prediction models based onmachine learning, and transfer learning across kernel versions. Throughout thispaper, we describe how the dataset has been made easily accessible via OpenMLand illustrate how it can be leveraged using only a few lines of Python code toevaluate AI-based techniques, such as supervised machine learning. Weanticipate that this dataset will significantly enhance reproducibility andfoster new insights into configuration-space analysis at a scale that presentsunique opportunities and inherent challenges, thereby advancing ourunderstanding of the Linux kernel's configurability and evolution.</description>
      <author>example@mail.com (Heraldo Borges, Juliana Alves Pereira, Djamel Eddine Khelladi, Mathieu Acher)</author>
      <guid isPermaLink="false">2505.07487v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
      <link>http://arxiv.org/abs/2505.06746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为M$^3$CAD的新颖基准测试，旨在推进通用合作式自动驾驶研究。&lt;h4&gt;背景&lt;/h4&gt;M$^3$CAD包含204个序列和30k个帧，涵盖了各种合作驾驶场景。&lt;h4&gt;目的&lt;/h4&gt;M$^3$CAD旨在支持包括目标检测与跟踪、地图构建、运动预测、占用预测和路径规划在内的多种自动驾驶任务。&lt;h4&gt;方法&lt;/h4&gt;M$^3$CAD采用多种传感模态，如激光雷达点云、RGB图像和GPS/IMU，以支持单车和多车自动驾驶研究。&lt;h4&gt;主要发现&lt;/h4&gt;M$^3$CAD是迄今为止针对合作多任务自动驾驶研究最全面的基准测试。&lt;h4&gt;结论&lt;/h4&gt;M$^3$CAD及其基线模型和评估结果被发布以支持鲁棒的合作自动驾驶系统的开发。&lt;h4&gt;翻译&lt;/h4&gt;We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce M$^3$CAD, a novel benchmark designed to advance research ingeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with30k frames, spanning a diverse range of cooperative driving scenarios. Eachsequence includes multiple vehicles and sensing modalities, e.g., LiDAR pointclouds, RGB images, and GPS/IMU, supporting a variety of autonomous drivingtasks, including object detection and tracking, mapping, motion forecasting,occupancy prediction, and path planning. This rich multimodal setup enablesM$^3$CAD to support both single-vehicle and multi-vehicle autonomous drivingresearch, significantly broadening the scope of research in the field. To ourknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailoredfor cooperative multi-task autonomous driving research. We evaluate thestate-of-the-art end-to-end solution on M$^3$CAD to establish baselineperformance. To foster cooperative autonomous driving research, we also proposeE2EC, a simple yet effective framework for cooperative driving solution thatleverages inter-vehicle shared information for improved path planning. Werelease M$^3$CAD, along with our baseline models and evaluation results, tosupport the development of robust cooperative autonomous driving systems. Allresources will be made publicly available on https://github.com/zhumorui/M3CAD</description>
      <author>example@mail.com (Morui Zhu, Yongqi Zhu, Yihao Zhu, Qi Chen, Deyuan Qu, Song Fu, Qing Yang)</author>
      <guid isPermaLink="false">2505.06746v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
      <link>http://arxiv.org/abs/2505.06903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CheXLearner是一个统一了解剖区域检测、基于黎曼流形的结构对齐和细粒度区域语义引导的端到端框架，用于时间医学图像分析。&lt;h4&gt;背景&lt;/h4&gt;现有的医学图像分析方法要么在粗粒度上对齐图像和文本，导致潜在的语义不匹配，要么仅依赖于视觉信息，缺乏医学语义整合。&lt;h4&gt;目的&lt;/h4&gt;提出CheXLearner，以解决现有方法的问题，实现图像和文本的精确对齐，并整合医学语义。&lt;h4&gt;方法&lt;/h4&gt;CheXLearner使用超曲几何来对齐解剖结构，并捕获时间胸片中的病理学意义差异。通过引入区域进展描述作为监督，实现跨模态表示学习，并支持动态低级别特征优化。&lt;h4&gt;主要发现&lt;/h4&gt;CheXLearner在解剖区域进展检测上达到81.12%的平均准确率和80.32%的F1分数，显著优于现有基准。在下游疾病分类中，模型达到91.52%的平均AUC分数。&lt;h4&gt;结论&lt;/h4&gt;CheXLearner在时间医学图像分析中实现了优异的性能，验证了其在特征表示方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal medical image analysis is essential for clinical decision-making,yet existing methods either align images and text at a coarse level - causingpotential semantic mismatches - or depend solely on visual information, lackingmedical semantic integration. We present CheXLearner, the first end-to-endframework that unifies anatomical region detection, Riemannian manifold-basedstructure alignment, and fine-grained regional semantic guidance. Our proposedMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry torobustly align anatomical structures and capture pathologically meaningfuldiscrepancies across temporal chest X-rays. By introducing regional progressiondescriptions as supervision, CheXLearner achieves enhanced cross-modalrepresentation learning and supports dynamic low-level feature optimization.Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and80.32% (+11.05%) F1-score on anatomical region progression detection -substantially outperforming state-of-the-art baselines, especially instructurally complex regions. Additionally, our model attains a 91.52% averageAUC score in downstream disease classification, validating its superior featurerepresentation.</description>
      <author>example@mail.com (Yuanzhuo Wang, Junwen Duan, Xinyu Li, Jianxin Wang)</author>
      <guid isPermaLink="false">2505.06903v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了图神经网络（GNNs）及其解释方法，提出了一个有效的算法COMRECGC来求解全局反事实解释中的共同补救方法问题，并通过实验证明了其性能优于其他算法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在社交网络、分子生物学、推荐系统等领域得到了广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;设计一个算法来求解全局反事实解释中的共同补救方法问题，并证明其性能优于其他算法。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同补救方法解释问题，并设计了COMRECGC算法来解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个不同的真实世界图数据集上进行了基准测试，表现优于其他算法。同时，共同补救方法解释与图反事实解释进行了比较，结果表明共同补救方法解释在药物发现或计算生物学等应用中具有可比性或优越性。&lt;h4&gt;结论&lt;/h4&gt;共同补救方法解释对于GNNs的全局反事实解释问题是一个有价值的解决方案，值得在药物发现或计算生物学等应用中进行考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML2025 (spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在持续预训练（CPT）过程中大型语言模型的学习动态，特别关注了通用性能和下游领域性能在每一步训练中的演变。&lt;h4&gt;背景&lt;/h4&gt;CPT已成为将强大基础模型应用于特定下游任务的流行且有效的方法。&lt;h4&gt;目的&lt;/h4&gt;研究CPT过程中学习动态，特别是通用和下游领域性能的变化。&lt;h4&gt;方法&lt;/h4&gt;通过验证损失来衡量领域性能，并观察到CPT损失曲线描述了从一条曲线到另一条隐藏曲线的过渡，该曲线可以由解耦分布偏移和学习率衰减效应来描述。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种CPT扩展定律，结合了分布偏移和学习率衰减两个因素，能够预测任何（持续）训练步骤和CPT中的学习率计划（LRS）下的损失。&lt;h4&gt;结论&lt;/h4&gt;该定律在多种CPT数据集和训练超参数下均有效，可以用于调整训练超参数，以平衡通用性能和特定领域性能。&lt;h4&gt;翻译&lt;/h4&gt;Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Pre-Training (CPT) has become a popular and effective method toapply strong foundation models to specific downstream tasks. In this work, weexplore the learning dynamics throughout the CPT process for large languagemodels. We specifically focus on how general and downstream domain performanceevolves at each training step, with domain performance measured via validationlosses. We have observed that the CPT loss curve fundamentally characterizesthe transition from one curve to another hidden curve, and could be describedby decoupling the effects of distribution shift and learning rate annealing. Wederive a CPT scaling law that combines the two factors, enabling the predictionof loss at any (continual) training steps and across learning rate schedules(LRS) in CPT. Our formulation presents a comprehensive understanding of severalcritical factors in CPT, including loss potential, peak learning rate, trainingsteps, replay ratio, etc. Moreover, our approach can be adapted to customizetraining hyper-parameters to different CPT goals such as balancing general anddomain-specific performance. Extensive experiments demonstrate that our scalinglaw holds across various CPT datasets and training hyper-parameters.</description>
      <author>example@mail.com (Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng)</author>
      <guid isPermaLink="false">2505.07796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
      <link>http://arxiv.org/abs/2505.06814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了M4IVQA挑战赛，旨在进一步推动多模态、多语言和多跳医疗教学问答系统的研究。&lt;h4&gt;背景&lt;/h4&gt;继NLPCC 2023 Foshan的CMIVQA和NLPCC 2024 Hangzhou的MMIVQA挑战赛成功举办后，今年引入了新的M4IVQA任务。&lt;h4&gt;目的&lt;/h4&gt;M4IVQA挑战赛专注于评估能够从医疗教学视频中整合信息、理解多种语言并回答需要跨模态推理的多跳问题的模型。&lt;h4&gt;方法&lt;/h4&gt;挑战赛包括三个赛道：多模态、多语言和多跳视频中的时序答案定位（M4TAGSV）、多模态、多语言和多跳视频语料库检索（M4VCR）和多模态、多语言和多跳视频语料库中的时序答案定位（M4TAGVC）。参与者需要开发能够处理视频和文本数据、理解多语言查询并为多跳医疗问题提供相关答案的算法。&lt;h4&gt;主要发现&lt;/h4&gt;M4IVQA挑战赛将推动多模态推理系统在医疗场景中的应用创新，最终有助于构建更智能的应急响应系统和在多语言社区中更有效的医学教育平台。&lt;h4&gt;结论&lt;/h4&gt;M4IVQA挑战赛有望促进多模态推理系统在医疗领域的创新，对医疗教育有重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has beenintroduced to further advance research in multi-modal, multilingual, andmulti-hop medical instructional question answering (M4IVQA) systems, with aspecific focus on medical instructional videos. The M4IVQA challenge focuses onevaluating models that integrate information from medical instructional videos,understand multiple languages, and answer multi-hop questions requiringreasoning over various modalities. This task consists of three tracks:multi-modal, multilingual, and multi-hop Temporal Answer Grounding in SingleVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video CorpusRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal AnswerGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected todevelop algorithms capable of processing both video and text data,understanding multilingual queries, and providing relevant answers to multi-hopmedical questions. We believe the newly introduced M4IVQA challenge will driveinnovations in multimodal reasoning systems for healthcare scenarios,ultimately contributing to smarter emergency response systems and moreeffective medical education platforms in multilingual communities. Our officialwebsite is https://cmivqa.github.io/</description>
      <author>example@mail.com (Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, Shoujun Zhou)</author>
      <guid isPermaLink="false">2505.06814v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Inference for Small Cohorts via Transfer Learning and Weighted Integration of Multiple Datasets</title>
      <link>http://arxiv.org/abs/2505.07153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了东北部美国地区肺部感染问题，指出国家eICU协作数据库中该地区患者数据不足，强调了数据的代表性不足。提出了一个新的加权方法TRANSLATE，用于整合来自不同来源的数据，以提高对感染结果的推断准确性。&lt;h4&gt;背景&lt;/h4&gt;东北部美国地区的肺部感染是一个重要问题，但国家eICU协作数据库中该地区患者数据不足，表明数据代表性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的加权方法，以解决小样本问题，并提高对肺部感染结果的推断准确性。&lt;h4&gt;方法&lt;/h4&gt;使用了一种名为TRANSLATE的新加权方法，通过学习权重将外部数据与目标队列对齐，从而整合来自不同来源的数据。&lt;h4&gt;主要发现&lt;/h4&gt;TRANSLATE方法在模拟和实际数据应用中提高了对肺部感染结果的推断准确性，同时考虑了区域异质性。&lt;h4&gt;结论&lt;/h4&gt;TRANSLATE方法为提高对肺部感染结果的推断提供了理论保证，并适用于多种统计估计，如均值、方差和分布函数。&lt;h4&gt;翻译&lt;/h4&gt;Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.</description>
      <author>example@mail.com (Subharup Guha, Mengqi Xu, Yi Li)</author>
      <guid isPermaLink="false">2505.07153v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
      <link>http://arxiv.org/abs/2505.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRACE的新方法，用于估计人类与场景接触的几何级别，通过结合点云编码器-解码器架构和层次特征提取与融合模块，实现了对3D人类几何结构与2D图像交互语义的有效整合，从而准确建模接触区域。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要依赖于参数化人类模型（如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系，但这种方法的泛化能力在不同的人类几何形状上受到限制。&lt;h4&gt;目的&lt;/h4&gt;提高人类与场景接触估计的准确性，并增强对不同人类几何形状的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;GRACE方法结合了点云编码器-解码器架构和层次特征提取与融合模块，通过视觉线索将几何特征映射到3D人类网格的顶点空间，从而实现接触区域的准确建模。&lt;h4&gt;主要发现&lt;/h4&gt;GRACE在多个基准数据集上的实验表明，其在接触估计方面达到了最先进的性能，并且其鲁棒性验证了其在非结构化人类点云上的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GRACE是一种有效且具有良好泛化能力的3D人类接触估计方法，适用于人类行为分析、具身人工智能和AR/VR等应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：估算人类-场景接触的几何级别旨在将特定的接触表面点定位在3D人类几何上，这提供了一个空间先验，并架起了人类与场景之间的桥梁，支持人类行为分析、具身人工智能和AR/VR等应用。为了完成这项任务，现有的方法主要依赖于参数化人类模型（例如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系。实际上，这种方法完成了从图像特征到有序序列的映射。然而，这种方法缺乏对几何形状的考虑，限制了其在不同人类几何形状上的泛化能力。在本文中，我们引入了GRACE（用于3D人类-场景接触估计的几何级别推理），这是一种新的3D人类接触估计范式。GRACE结合了点云编码器-解码器架构以及层次特征提取和融合模块，使得3D人类几何结构与从图像中提取的2D交互语义能够有效整合。在视觉线索的引导下，GRACE建立了从几何特征到3D人类网格顶点空间的隐式映射，从而实现了接触区域的准确建模。这种设计确保了高预测精度，并赋予了框架在多种人类几何形状上的强大泛化能力。在多个基准数据集上的大量实验表明，GRACE在接触估计方面达到了最先进的性能，进一步的结果进一步验证了其在非结构化人类点云上的鲁棒泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the geometry level of human-scene contact aims to ground specificcontact surface points at 3D human geometries, which provides a spatial priorand bridges the interaction between human and scene, supporting applicationssuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,existing approaches predominantly rely on parametric human models (e.g., SMPL),which establish correspondences between images and contact regions throughfixed SMPL vertex sequences. This actually completes the mapping from imagefeatures to an ordered sequence. However, this approach lacks consideration ofgeometry, limiting its generalizability in distinct human geometries. In thispaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene ContactEstimation), a new paradigm for 3D human contact estimation. GRACE incorporatesa point cloud encoder-decoder architecture along with a hierarchical featureextraction and fusion module, enabling the effective integration of 3D humangeometric structures with 2D interaction semantics derived from images. Guidedby visual cues, GRACE establishes an implicit mapping from geometric featuresto the vertex space of the 3D human mesh, thereby achieving accurate modelingof contact regions. This design ensures high prediction accuracy and endows theframework with strong generalization capability across diverse humangeometries. Extensive experiments on multiple benchmark datasets demonstratethat GRACE achieves state-of-the-art performance in contact estimation, withadditional results further validating its robust generalization to unstructuredhuman point clouds.</description>
      <author>example@mail.com (Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha)</author>
      <guid isPermaLink="false">2505.06575v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MarkMatch: Same-Hand Stuffing Detection</title>
      <link>http://arxiv.org/abs/2505.07032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。&lt;h4&gt;背景&lt;/h4&gt;与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。&lt;h4&gt;目的&lt;/h4&gt;提高在书写变化和视觉噪声下的泛化能力，并增强对真实匹配的高置信度。&lt;h4&gt;方法&lt;/h4&gt;模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;MarkMatch实现了0.943的F1分数，超过了BubbleSig的最佳性能。系统还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。&lt;h4&gt;结论&lt;/h4&gt;MarkMatch为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。我们的模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比，从而使模型能够学习细微的书写差异，并提高在书写变化和视觉噪声下的泛化能力，而斜对角监督则加强了真实匹配的高置信度。模型实现了0.943的F1分数，超过了BubbleSig的最佳性能。MarkMatch还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。该系统为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MarkMatch, a retrieval system for detecting whether two paperballot marks were filled by the same hand. Unlike the previous SOTA methodBubbleSig, which used binary classification on isolated mark pairs, MarkMatchranks stylistic similarity between a query mark and a mark in the databaseusing contrastive learning. Our model is trained with a dense batch similaritymatrix and a dual loss objective. Each sample is contrasted against manynegatives within each batch, enabling the model to learn subtle handwritingdifference and improve generalization under handwriting variation and visualnoise, while diagonal supervision reinforces high confidence on true matches.The model achieves an F1 score of 0.943, surpassing BubbleSig's bestperformance. MarkMatch also integrates Segment Anything Model for flexible markextraction via box- or point-based prompts. The system offers election auditorsa practical tool for visual, non-biometric investigation of suspicious ballots.</description>
      <author>example@mail.com (Fei Zhao, Runlin Zhang, Chengcui Zhang, Nitesh Saxena)</author>
      <guid isPermaLink="false">2505.07032v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
      <link>http://arxiv.org/abs/2505.06886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，发现两者在上下文（群体水平）和自下而上（单细胞水平）的映射存在显著相似性，并通过添加神经响应归一化层（NeuRN）进一步增强了这种相似性，提高了模型在现实世界任务中的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;小鼠是系统神经科学中研究最广泛的动物模型之一。了解小鼠视觉皮层中由各种自然场景刺激引起的神经表征的普遍模式是计算视觉领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;研究小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，并提出一种新的框架来比较两者之间的功能架构。&lt;h4&gt;方法&lt;/h4&gt;引入了一种通用的表征学习策略，并添加了受视觉皮层中兴奋性和抑制性神经元激活特征启发的NeuRN层。&lt;h4&gt;主要发现&lt;/h4&gt;发现小鼠视觉皮层的功能映射与高性能深度学习模型在上下文和自下而上的场景中存在显著相似性；NeuRN层的添加显著提高了深度学习模型在领域泛化任务中对数据变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;研究结果对从小鼠视觉皮层中获得灵感的先进AI模型的发展具有重要意义，表明这些模型可以作为研究小鼠视觉皮层神经表征的有价值工具，并因此提高它们在现实世界任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The mouse is one of the most studied animal models in the field of systemsneuroscience. Understanding the generalized patterns and decoding the neuralrepresentations that are evoked by the diverse range of natural scene stimuliin the mouse visual cortex is one of the key quests in computational vision. Inrecent years, significant parallels have been drawn between the primate visualcortex and hierarchical deep neural networks. However, their generalizedefficacy in understanding mouse vision has been limited. In this study, weinvestigate the functional alignment between the mouse visual cortex and deeplearning models for object classification tasks. We first introduce ageneralized representational learning strategy that uncovers a strikingresemblance between the functional mapping of the mouse visual cortex andhigh-performing deep learning models on both top-down (population-level) andbottom-up (single cell-level) scenarios. Next, this representational similarityacross the two systems is further enhanced by the addition of Neural ResponseNormalization (NeuRN) layer, inspired by the activation profile of excitatoryand inhibitory neurons in the visual cortex. To test the performance effect ofNeuRN on real-world tasks, we integrate it into deep learning models andobserve significant improvements in their robustness against data shifts indomain generalization tasks. Our work proposes a novel framework for comparingthe functional architecture of the mouse visual cortex with deep learningmodels. Our findings carry broad implications for the development of advancedAI models that draw inspiration from the mouse visual cortex, suggesting thatthese models serve as valuable tools for studying the neural representations ofthe mouse visual cortex and, as a result, enhancing their performance onreal-world tasks.</description>
      <author>example@mail.com (Ahmed Qazi, Hamd Jalil, Asim Iqbal)</author>
      <guid isPermaLink="false">2505.06886v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>BodyGPS: Anatomical Positioning System</title>
      <link>http://arxiv.org/abs/2505.07744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的基础模型，用于解析医学图像中的人类解剖结构，适用于不同的模态，支持监督或无监督训练，能够进行匹配、配准、分类或分割，可带或不带用户交互。&lt;h4&gt;背景&lt;/h4&gt;当前解析医学图像中的人类解剖结构需要针对不同模态的特定模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适用于不同模态、支持多种任务和交互方式的基础模型。&lt;h4&gt;方法&lt;/h4&gt;通过训练一个神经网络估计器，将查询位置映射到图谱坐标，通过回归实现。通过稀疏采样输入数据，提高效率，实现小于1毫秒的响应时间，无需额外的加速硬件。&lt;h4&gt;主要发现&lt;/h4&gt;该算法在CT和MRI模态中均表现出良好的实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法和模型在医学图像解析中具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a new type of foundational model for parsing human anatomy inmedical images that works for different modalities. It supports supervised orunsupervised training and can perform matching, registration, classification,or segmentation with or without user interaction. We achieve this by training aneural network estimator that maps query locations to atlas coordinates viaregression. Efficiency is improved by sparsely sampling the input, enablingresponse times of less than 1 ms without additional accelerator hardware. Wedemonstrate the utility of the algorithm in both CT and MRI modalities.</description>
      <author>example@mail.com (Halid Ziya Yerebakan, Kritika Iyer, Xueqi Guo, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez)</author>
      <guid isPermaLink="false">2505.07744v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
      <link>http://arxiv.org/abs/2505.06573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ElectricSight的系统，用于3D距离测量和监测电力传输线路潜在威胁，如大型起重机，以提高安全。&lt;h4&gt;背景&lt;/h4&gt;保护电力传输线路免受潜在危害是关键任务，其中之一是准确测量电力线路与潜在威胁之间的距离。现有的基于传感器的测量方法在平衡准确性和成本方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ElectricSight系统，旨在解决现有方法的局限性，实现低成本且精确的3D距离测量。&lt;h4&gt;方法&lt;/h4&gt;ElectricSight系统结合实时图像和环境影响点云先验信息，采用单目深度估计方法，将3D点云数据集成到基于图像的估计中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ElectricSight在距离测量上实现了平均精度1.08米，在早期预警上实现了92%的准确率。&lt;h4&gt;结论&lt;/h4&gt;ElectricSight系统通过其创新的设计和实施，提供了有效的解决方案，以解决电力传输线路安全监测中的距离测量问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protecting power transmission lines from potential hazards involves criticaltasks, one of which is the accurate measurement of distances between powerlines and potential threats, such as large cranes. The challenge with this taskis that the current sensor-based methods face challenges in balancing accuracyand cost in distance measurement. A common practice is to install cameras ontransmission towers, which, however, struggle to measure true 3D distances dueto the lack of depth information. Although 3D lasers can provide accurate depthdata, their high cost makes large-scale deployment impractical.  To address this challenge, we present ElectricSight, a system designed for 3Ddistance measurement and monitoring of potential hazards to power transmissionlines. This work's key innovations lie in both the overall system framework anda monocular depth estimation method. Specifically, the system frameworkcombines real-time images with environmental point cloud priors, enablingcost-effective and precise 3D distance measurements. As a core component of thesystem, the monocular depth estimation method enhances the performance byintegrating 3D point cloud data into image-based estimates, improving both theaccuracy and reliability of the system.  To assess ElectricSight's performance, we conducted tests with data from areal-world power transmission scenario. The experimental results demonstratethat ElectricSight achieves an average accuracy of 1.08 m for distancemeasurements and an early warning accuracy of 92%.</description>
      <author>example@mail.com (Xingchen Li, LiDian Wang, Yu Sheng, ZhiPeng Tang, Haojie Ren, Guoliang You, YiFan Duan, Jianmin Ji, Yanyong Zhang)</author>
      <guid isPermaLink="false">2505.06573v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
      <link>http://arxiv.org/abs/2505.06855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Multi-Masking Strategy (MMS)的文本识别方法，通过结合随机块状和跨度掩码，优化了自监督学习在文本识别任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界标注数据集的稀缺，大多数文本识别方法都是在大规模合成数据集上训练的。合成图像无法真实反映现实场景，如不均匀光照、不规则布局、遮挡和退化等问题，导致在处理复杂现实图像时性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了缩小现实世界和合成数据集之间的差距，本文旨在通过自监督学习技术提高文本识别的性能。&lt;h4&gt;方法&lt;/h4&gt;本文分析了原始的Masked AutoEncoder (MAE)并指出，随机块状掩码主要捕捉低级纹理特征，但忽略了高级上下文表示。为了充分利用高级上下文表示，我们引入了随机块状和跨度掩码。Multi-Masking Strategy (MMS)将随机块状和跨度掩码整合到MIM框架中，共同学习低级和高级文本表示。&lt;h4&gt;主要发现&lt;/h4&gt;经过与真实数据的微调，MMS在文本识别、分割和文本图像超分辨率等文本相关任务中优于现有的自监督方法。&lt;h4&gt;结论&lt;/h4&gt;MMS通过引入新的掩码策略，显著提高了自监督学习在文本识别任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing text recognition methods are trained on large-scale syntheticdatasets due to the scarcity of labeled real-world datasets. Synthetic images,however, cannot faithfully reproduce real-world scenarios, such as unevenillumination, irregular layout, occlusion, and degradation, resulting inperformance disparities when handling complex real-world images. Recentself-supervised learning techniques, notably contrastive learning and maskedimage modeling (MIM), narrow this domain gap by exploiting unlabeled real textimages. This study first analyzes the original Masked AutoEncoder (MAE) andobserves that random patch masking predominantly captures low-level texturalfeatures but misses high-level contextual representations. To fully exploit thehigh-level contextual representations, we introduce random blockwise and spanmasking in the text recognition task. These strategies can mask the continuousimage patches and completely remove some characters, forcing the model to inferrelationships among characters within a word. Our Multi-Masking Strategy (MMS)integrates random patch, blockwise, and span masking into the MIM frame, whichjointly learns low and high-level textual representations. After fine-tuningwith real data, MMS outperforms the state-of-the-art self-supervised methods invarious text-related tasks, including text recognition, segmentation, andtext-image super-resolution.</description>
      <author>example@mail.com (Zhengmi Tang, Yuto Mitsui, Tomo Miyazaki, Shinichiro Omachi)</author>
      <guid isPermaLink="false">2505.06855v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Survival Modeling in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2505.07683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过利用基础模型和病理报告文本信息提取，研究了训练经典的多模态生存模型的可行性，并展示了其在预测癌症生存方面的优势。&lt;h4&gt;背景&lt;/h4&gt;TCGA数据库通过其基因组学、临床和图像数据的整合，为癌症研究提供了大规模参考。基础模型在生物医学深度学习中用于提取有意义特征嵌入，而病理报告文本在TCGA数据库中虽存在，但长期以来未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;研究通过使用基础模型从零样本嵌入中训练经典的多模态生存模型的可行性。&lt;h4&gt;方法&lt;/h4&gt;通过分析TCGA数据，结合基础模型提取的零样本嵌入和病理报告文本，构建多模态生存模型，并与单模态模型进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现多模态融合模型易于实现且效果优于单模态模型，同时加入病理报告文本也有助于提高模型的预测性能。&lt;h4&gt;结论&lt;/h4&gt;利用基础模型和病理报告文本信息提取可以现代化生存建模，提高癌症生存预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as alarge-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models fromunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learningis the development of foundation models (FMs) to derive meaningful featureembeddings, agnostic to a specific modeling task. Biomedical text especiallyhas seen growing development of FMs. While TCGA contains free-text data aspathology reports, these have been historically underutilized. Here, weinvestigate the feasibility of training classical, multimodal survival modelsover zero-shot embeddings extracted by FMs. We show the ease and additiveeffect of multimodal fusion, outperforming unimodal models. We demonstrate thebenefit of including pathology report text and rigorously evaluate the effectof model-based text summarization and hallucination. Overall, we modernizesurvival modeling by leveraging FMs and information extraction from pathologyreports.</description>
      <author>example@mail.com (Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman)</author>
      <guid isPermaLink="false">2505.07683v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
      <link>http://arxiv.org/abs/2505.06710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于弱监督方案的多实例学习（MIL）特征提取器预训练方法，旨在改善MIL在病理图像分析中的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的MIL方法强调了特征聚合器的重要性，但很大程度上忽视了实例级别的表征学习。同时，这些方法假设可以直接利用或微调预训练的特征提取器，但这种情况并不总是成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种预训练MIL特征提取器的方法，通过传播弱标签到对应的实例来实现。&lt;h4&gt;方法&lt;/h4&gt;通过弱标签传播的方式预训练特征提取器，并深入研究了几个关键组件，包括强大的数据增强、非线性预测头和鲁棒的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在常见的病理图像数据集上，该方法在不同下游任务中取得了比其他预训练方案（如ImageNet预训练和自监督学习）更好的性能。该方法还显示出了兼容性和可扩展性，适用于病理特定模型的微调和多个数据集的预训练。&lt;h4&gt;结论&lt;/h4&gt;这是首个专注于MIL表征学习的研究工作，提出了有效的预训练方法，并展示了其在实际应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various multi-instance learning (MIL) based approaches have been developedand successfully applied to whole-slide pathological images (WSI). Existing MILmethods emphasize the importance of feature aggregators, but largely neglectthe instance-level representation learning. They assume that the availabilityof a pre-trained feature extractor can be directly utilized or fine-tuned,which is not always the case. This paper proposes to pre-train featureextractor for MIL via a weakly-supervised scheme, i.e., propagating the weakbag-level labels to the corresponding instances for supervised learning. Tolearn effective features for MIL, we further delve into several key components,including strong data augmentation, a non-linear prediction head and the robustloss function. We conduct experiments on common large-scale WSI datasets andfind it achieves better performance than other pre-training schemes (e.g.,ImageNet pre-training and self-supervised learning) in different downstreamtasks. We further show the compatibility and scalability of the proposed schemeby deploying it in fine-tuning the pathological-specific models andpre-training on merged multiple datasets. To our knowledge, this is the firstwork focusing on the representation learning for MIL.</description>
      <author>example@mail.com (Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu)</author>
      <guid isPermaLink="false">2505.06710v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NetSight: Graph Attention Based Traffic Forecasting in Computer Networks</title>
      <link>http://arxiv.org/abs/2505.07034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NetSight的新方法，用于预测网络中给定指标的值。NetSight能够同时学习全局和局部尺度的时空依赖关系，从而提高预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前网络交通受到节点交互和节点需求波动的影响，传统的统计预测方法已无法适应这种非线性动态时空依赖。&lt;h4&gt;目的&lt;/h4&gt;提出NetSight以解决传统方法在处理网络交通中的时空依赖关系方面的不足。&lt;h4&gt;方法&lt;/h4&gt;NetSight通过学习网络中各个节点的测量时间序列数据，同时考虑全局和局部尺度的时空依赖关系，实现网络指标的预测。&lt;h4&gt;主要发现&lt;/h4&gt;NetSight在两个大型真实网络的数据集上进行了广泛评估，结果显示其在预测准确性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;NetSight通过同时学习全局和局部尺度的时空依赖关系，提高了网络指标预测的准确性，为网络交通预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio-temporal dependencies present in today's network traffic. The most promising direction of research today is graph neural networks (GNNs) based prediction approaches that are naturally suited to handle graph-structured data. Unfortunately, the state-of-the-art GNN approaches separate the modeling of spatial and temporal information, resulting in the loss of important information about joint dependencies. These GNN based approaches further do not model information at both local and global scales simultaneously, leaving significant room for improvement. To address these challenges, we propose NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at both global and local scales from the time-series of measurements of any given network metric collected at various nodes in a network. Using the learned information, NetSight can then accurately predict the future values of the given network metric at those nodes in the network. We propose several new concepts and techniques in the design of NetSight, such as spatio-temporal adjacency matrix and node normalization. Through extensive evaluations and comparison with prior approaches using data from two large real-world networks, we show that NetSight significantly outperforms all prior state-of-the-art approaches. We will release the source code and data used in the evaluation of NetSight on the acceptance of this paper.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The traffic in today's networks is increasingly influenced by theinteractions among network nodes as well as by the temporal fluctuations in thedemands of the nodes. Traditional statistical prediction methods are becomingobsolete due to their inability to address the non-linear and dynamicspatio-temporal dependencies present in today's network traffic. The mostpromising direction of research today is graph neural networks (GNNs) basedprediction approaches that are naturally suited to handle graph-structureddata. Unfortunately, the state-of-the-art GNN approaches separate the modelingof spatial and temporal information, resulting in the loss of importantinformation about joint dependencies. These GNN based approaches further do notmodel information at both local and global scales simultaneously, leavingsignificant room for improvement. To address these challenges, we proposeNetSight. NetSight learns joint spatio-temporal dependencies simultaneously atboth global and local scales from the time-series of measurements of any givennetwork metric collected at various nodes in a network. Using the learnedinformation, NetSight can then accurately predict the future values of thegiven network metric at those nodes in the network. We propose several newconcepts and techniques in the design of NetSight, such as spatio-temporaladjacency matrix and node normalization. Through extensive evaluations andcomparison with prior approaches using data from two large real-world networks,we show that NetSight significantly outperforms all prior state-of-the-artapproaches. We will release the source code and data used in the evaluation ofNetSight on the acceptance of this paper.</description>
      <author>example@mail.com (Jinming Xing, Guoheng Sun, Hui Sun, Linchao Pan, Shakir Mahmood, Xuanhao Luo, Muhammad Shahzad)</author>
      <guid isPermaLink="false">2505.07034v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
      <link>http://arxiv.org/abs/2505.07019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCOLD的智能农业领域视觉-语言基础模型，用于叶病识别，通过结合图像和文本模态，解决了现有方法中模态整合不足和依赖预训练数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;叶病识别在智能农业中至关重要，但现有研究在整合图像和文本模态以及使用预训练数据集方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SCOLD模型，旨在解决农业任务中的模态整合和预训练数据集的局限性。&lt;h4&gt;方法&lt;/h4&gt;SCOLD模型使用超过186,000个植物叶片图像及其对应的症状描述进行训练，通过无任务预训练和软目标对比学习来提高模型的泛化能力和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SCOLD在零样本和少样本分类、图像-文本检索和图像分类等多个基准测试中优于现有视觉-语言模型，同时保持了有竞争力的参数规模。&lt;h4&gt;结论&lt;/h4&gt;SCOLD模型显著推进了农业视觉-语言基础模型的发展，以最小的或无需监督微调即可实现强大的性能，为未来研究长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：叶病识别在智能农业中发挥着关键作用。然而，许多现有研究仍然难以整合图像和文本模态以弥补彼此的局限性。此外，许多这些方法依赖于使用ImageNet等受限制的数据集进行预训练，这些数据集缺乏领域特定信息。我们提出了SCOLD（用于叶病识别的软目标对比学习），这是一种针对解决这些挑战的农业任务而定制的内容感知视觉-语言基础模型。SCOLD使用一个包含超过186,000个植物叶片图像及其对应症状描述的多样化语料库进行开发，这些图像-字幕对与97个独特概念相匹配。通过无任务预训练，SCOLD利用上下文软目标通过平滑标签来减轻对比学习中的过度自信，从而提高模型在细粒度分类任务上的泛化能力和鲁棒性。实验结果表明，SCOLD在多个基准测试中（包括零样本和少样本分类、图像-文本检索和图像分类）优于现有的视觉-语言模型（如OpenAI-CLIP-L、BioCLIP和SigLIP2），同时保持有竞争力的参数规模。消融研究进一步突出了SCOLD相对于其竞争对手的有效性。所提出的方法显著推进了农业视觉-语言基础模型，以最小的或无需监督微调即可实现强大的性能。这项工作为使用长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统的研究奠定了坚实的基础。本研究的代码可在https://huggingface.co/enalis/scold获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leaf disease identification plays a pivotal role in smart agriculture.However, many existing studies still struggle to integrate image and textualmodalities to compensate for each other's limitations. Furthermore, many ofthese approaches rely on pretraining with constrained datasets such asImageNet, which lack domain-specific information. We propose SCOLD (Soft-targetCOntrastive learning for Leaf Disease identification), a context-awarevision-language foundation model tailored to address these challenges foragricultural tasks. SCOLD is developed using a diverse corpus of plant leafimages and corresponding symptom descriptions, comprising over 186,000image-caption pairs aligned with 97 unique concepts. Through task-agnosticpretraining, SCOLD leverages contextual soft targets to mitigate overconfidencein contrastive learning by smoothing labels, thereby improving modelgeneralization and robustness on fine-grained classification tasks.Experimental results demonstrate that SCOLD outperforms existingvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 acrossseveral benchmarks, including zero-shot and few-shot classification, image-textretrieval, and image classification, while maintaining a competitive parameterfootprint. Ablation studies further highlight SCOLD's effectiveness in contrastto its counterparts. The proposed approach significantly advances theagricultural vision-language foundation model, offering strong performance withminimal or no supervised fine-tuning. This work lays a solid groundwork forfuture research on models trained with long-form and simplified contexts, tasksinvolving class ambiguity, and multi-modal systems for intelligent plantdisease diagnostics. The code for this study is available athttps://huggingface.co/enalis/scold</description>
      <author>example@mail.com (Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach)</author>
      <guid isPermaLink="false">2505.07019v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Class Distribution Mismatch</title>
      <link>http://arxiv.org/abs/2505.06948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UCDM的无监督学习方法，用于解决训练数据与目标任务类别分布不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;以往的方法在半监督场景下设计分类器，将未知或新类别归为“其他”类别，但过分依赖标记数据，限制了其适用性和性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出UCDM方法，通过从无标签数据中构建正负样本对来进行分类器训练。&lt;h4&gt;方法&lt;/h4&gt;UCDM方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，引入基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将其纳入训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的大量实验表明，UCDM方法优于以往半监督方法。在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，UCDM方法无需依赖标记数据，在分类已知、未知和新类别上分别超越了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;h4&gt;结论&lt;/h4&gt;UCDM方法有效地解决了类别分布不匹配问题，在无需大量标记数据的情况下，显著提高了分类器的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：类别分布不匹配（CDM）是指训练数据中的类别分布与目标任务之间的差异。先前的方法通过设计分类器来对训练期间已知的类别进行分类，将未知或新类别分组到“其他”类别中。然而，它们侧重于半监督场景，并且高度依赖标记数据，限制了它们的适用性和性能。为了解决这个问题，我们提出了无监督学习用于类别分布不匹配（UCDM），该方法从无标签数据中为分类器训练构建正负样本对。我们的方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，我们引入了一种基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将它们纳入训练过程。在三个数据集上的大量实验表明UCDM方法优于先前半监督方法。具体而言，在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，我们的方法，不依赖于标记数据，在分类已知、未知和新类别上分别超过了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Class distribution mismatch (CDM) refers to the discrepancy between classdistributions in training data and target tasks. Previous methods address thisby designing classifiers to categorize classes known during training, whilegrouping unknown or new classes into an "other" category. However, they focuson semi-supervised scenarios and heavily rely on labeled data, limiting theirapplicability and performance. To address this, we propose UnsupervisedLearning for Class Distribution Mismatch (UCDM), which constructspositive-negative pairs from unlabeled data for classifier training. Ourapproach randomly samples images and uses a diffusion model to add or erasesemantic classes, synthesizing diverse training pairs. Additionally, weintroduce a confidence-based labeling mechanism that iteratively assignspseudo-labels to valuable real-world data and incorporates them into thetraining process. Extensive experiments on three datasets demonstrate UCDM'ssuperiority over previous semi-supervised methods. Specifically, with a 60%mismatch proportion on Tiny-ImageNet dataset, our approach, without relying onlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,and 72.5% in classifying known, unknown, and new classes.</description>
      <author>example@mail.com (Pan Du, Wangbo Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You)</author>
      <guid isPermaLink="false">2505.06948v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics</title>
      <link>http://arxiv.org/abs/2505.06435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 30 figures. IEEE Transactions on Pattern Analysis and  Machine Intelligence (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对连续敏感属性的公平表示学习（FRL）算法，旨在解决现有FRL算法在处理连续敏感属性（如年龄或收入）时的局限性。&lt;h4&gt;背景&lt;/h4&gt;AI公平性，也称为算法公平性，旨在确保算法在操作过程中不对任何个人或群体产生偏见或歧视。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的FRL算法，使其能够应用于连续敏感属性。&lt;h4&gt;方法&lt;/h4&gt;引入了期望积分概率度量（EIPM）来评估连续敏感属性表示空间的公平性水平，并证明了低EIPM值的表示分布上的预测头构建将具有公平性。此外，EIPM可以通过有限样本的估计器进行准确估计。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FRL算法FREM（基于EIPM和MMD的公平表示）在实验中优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;FREM算法能够有效处理连续敏感属性，提高算法的公平性，并在实际应用中优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI fairness, also known as algorithmic fairness, aims to ensure thatalgorithms operate without bias or discrimination towards any individual orgroup. Among various AI algorithms, the Fair Representation Learning (FRL)approach has gained significant interest in recent years. However, existing FRLalgorithms have a limitation: they are primarily designed for categoricalsensitive attributes and thus cannot be applied to continuous sensitiveattributes, such as age or income. In this paper, we propose an FRL algorithmfor continuous sensitive attributes. First, we introduce a measure called theExpectation of Integral Probability Metrics (EIPM) to assess the fairness levelof representation space for continuous sensitive attributes. We demonstratethat if the distribution of the representation has a low EIPM value, then anyprediction head constructed on the top of the representation become fair,regardless of the selection of the prediction head. Furthermore, EIPM possessesa distinguished advantage in that it can be accurately estimated using ourproposed estimator with finite samples. Based on these properties, we propose anew FRL algorithm called Fair Representation using EIPM with MMD (FREM).Experimental evidences show that FREM outperforms other baseline methods.</description>
      <author>example@mail.com (Insung Kong, Kunwoong Kim, Yongdai Kim)</author>
      <guid isPermaLink="false">2505.06435v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A systematic review of challenges and proposed solutions in modeling multimodal data</title>
      <link>http://arxiv.org/abs/2505.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态数据建模在临床研究中已成为一种强大的方法，它能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。然而，建模这种异构数据存在技术挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态数据建模能够提高诊断准确性和支持个性化护理，但建模这种异构数据面临技术挑战。&lt;h4&gt;目的&lt;/h4&gt;本文通过综合69项研究的发现，旨在识别多模态数据建模中的常见障碍。&lt;h4&gt;方法&lt;/h4&gt;本文采用系统综述的方法，分析了69项相关研究。&lt;h4&gt;主要发现&lt;/h4&gt;主要障碍包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。&lt;h4&gt;结论&lt;/h4&gt;本文强调了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。此外，本文通过映射当前趋势和创新，为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态数据建模已成为临床研究中一种强大的方法，能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。尽管其能够提高诊断准确性和支持个性化护理，但建模这种异构数据仍然存在显著的技术挑战。本文通过综合69项研究的发现，识别出常见的障碍，包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。本文突出了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。通过映射当前趋势和创新，本文为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data modeling has emerged as a powerful approach in clinicalresearch, enabling the integration of diverse data types such as imaging,genomics, wearable sensors, and electronic health records. Despite itspotential to improve diagnostic accuracy and support personalized care,modeling such heterogeneous data presents significant technical challenges.This systematic review synthesizes findings from 69 studies to identify commonobstacles, including missing modalities, limited sample sizes, dimensionalityimbalance, interpretability issues, and finding the optimal fusion techniques.We highlight recent methodological advances, such as transfer learning,generative models, attention mechanisms, and neural architecture search thatoffer promising solutions. By mapping current trends and innovations, thisreview provides a comprehensive overview of the field and offers practicalinsights to guide future research and development in multimodal modeling formedical applications.</description>
      <author>example@mail.com (Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binde, Nadine Binder)</author>
      <guid isPermaLink="false">2505.06945v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
      <link>http://arxiv.org/abs/2505.06890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合了表示条件的扩散模型，通过使用Vision Transformer（ViT）的表示来调节基于Transformer的扩散模型内部过程，实现了表示条件的数据生成，并利用自监督学习在无标签数据上解决大规模标注数据集的挑战。&lt;h4&gt;背景&lt;/h4&gt;在图像分类任务中，大规模标注数据集的获取是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来生成表示条件的数据，并提高图像分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformer（ViT）的表示作为条件来调节扩散模型，通过零样本分类任务在脑部影像中检测血肿来评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与DINOv2等对比学习基线相比，该方法在准确性上提高了6.15%，在F1分数上提高了13.60%，显示了其在图像分类中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合表示条件和扩散模型，在图像分类任务中取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a diffusion model that integrates arepresentation-conditioning mechanism, where the representations derived from aVision Transformer (ViT) are used to condition the internal process of aTransformer-based diffusion model. This approach enablesrepresentation-conditioned data generation, addressing the challenge ofrequiring large-scale labeled datasets by leveraging self-supervised learningon unlabeled data. We evaluate our method through a zero-shot classificationtask for hematoma detection in brain imaging. Compared to the strongcontrastive learning baseline, DINOv2, our method achieves a notableimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating itseffectiveness in image classification.</description>
      <author>example@mail.com (Kosuke Ukita, Ye Xiaolong, Tsuyoshi Okita)</author>
      <guid isPermaLink="false">2505.06890v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects</title>
      <link>http://arxiv.org/abs/2505.06363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了基于人类演示学习物体模型的方法，用于多自由度物体的精确控制。&lt;h4&gt;背景&lt;/h4&gt;随着机器人在多样化环境中应用，它们需要与具有多个独立关节或自由度的复杂物体交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法依赖先验知识、仅关注单自由度物体、无法处理遮挡关节和忽略获取它们所需的操作序列的问题。&lt;h4&gt;方法&lt;/h4&gt;通过学习人类演示的物体模型，引入了物体运动学序列机器（OKSMs）这一新表示，它捕捉了多自由度物体的运动学约束和操作顺序。为了从点云数据中估计这些模型，提出了Pokenet，这是一个在人类演示上训练的深度神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上，Pokenet比现有方法提高了超过20%的关节轴和状态估计。OKSMs在Sawyer机器人上通过基于逆运动学规划的策略操作多自由度物体。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在模拟和真实世界的数据上均有效，能够提高机器人对多自由度物体的控制精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robots become more generalized and deployed in diverse environments, theymust interact with complex objects, many with multiple independent joints ordegrees of freedom (DoF) requiring precise control. A common strategy is objectmodeling, where compact state-space models are learned from real-worldobservations and paired with classical planning. However, existing methodsoften rely on prior knowledge or focus on single-DoF objects, limiting theirapplicability. They also fail to handle occluded joints and ignore themanipulation sequences needed to access them. We address this by learningobject models from human demonstrations. We introduce Object Kinematic SequenceMachines (OKSMs), a novel representation capturing both kinematic constraintsand manipulation order for multi-DoF objects. To estimate these models frompoint cloud data, we present Pokenet, a deep neural network trained on humandemonstrations. We validate our approach on 8,000 simulated and 1,600real-world annotated samples. Pokenet improves joint axis and state estimationby over 20 percent on real-world data compared to prior methods. Finally, wedemonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning tomanipulate multi-DoF objects.</description>
      <author>example@mail.com (Anmol Gupta, Weiwei Gu, Omkar Patil, Jun Ki Lee, Nakul Gopalan)</author>
      <guid isPermaLink="false">2505.06363v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Similarity Search in Automotive Production</title>
      <link>http://arxiv.org/abs/2505.07256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Procedia CIRP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合视觉基础模型和合成数据的图像分类流程，用于汽车生产中的视觉质量检测，以提高检测的准确性和降低数据收集成本。&lt;h4&gt;背景&lt;/h4&gt;视觉质量检测对于保证汽车的安全和可靠性至关重要，计算机视觉技术在成本效益和可靠性方面表现出色。&lt;h4&gt;目的&lt;/h4&gt;为了减少大量标注数据的收集需求，提出了一个创新的图像分类流程。&lt;h4&gt;方法&lt;/h4&gt;该方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用合成数据而非真实图像作为参考，该流程实现了高分类准确率，同时不依赖于真实数据。&lt;h4&gt;结论&lt;/h4&gt;在八个实际检测场景中评估了该方法，证明它满足了生产环境中的高性能要求。&lt;h4&gt;翻译&lt;/h4&gt;在汽车生产中，视觉质量检查对于确保车辆的安全和可靠性至关重要。由于成本效益和可靠性，计算机视觉（CV）已经成为这些检查的流行解决方案。然而，CV模型需要大量标注的数据集，收集这些数据集既昂贵又耗时。为了减少对大量训练数据的需求，我们提出了一种结合基于视觉的基础模型和合成数据的图像分类流程。我们的方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。通过使用合成数据而不是真实图像作为参考，我们的流程在不依赖于真实数据的情况下实现了高分类准确率。我们在八个现实世界的检测场景中评估了这种方法，证明了它满足生产环境的高性能要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual quality inspection in automotive production is essential for ensuringthe safety and reliability of vehicles. Computer vision (CV) has become apopular solution for these inspections due to its cost-effectiveness andreliability. However, CV models require large, annotated datasets, which arecostly and time-consuming to collect. To reduce the need for extensive trainingdata, we propose a novel image classification pipeline that combines similaritysearch using a vision-based foundation model with synthetic data. Our approachleverages a DINOv2 model to transform input images into feature vectors, whichare then compared to pre-classified reference images using cosine distancemeasurements. By utilizing synthetic data instead of real images as references,our pipeline achieves high classification accuracy without relying on realdata. We evaluate this approach in eight real-world inspection scenarios anddemonstrate that it meets the high performance requirements of productionenvironments.</description>
      <author>example@mail.com (Christoph Huber, Ludwig Schleeh, Dino Knoll, Michael Guthe)</author>
      <guid isPermaLink="false">2505.07256v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting</title>
      <link>http://arxiv.org/abs/2505.06862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了BIGBIRD-PEGASUS模型在长文档摘要任务上的表现，并提出了改进策略以解决模型处理超长文档时性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果，但其处理能力有限，最大只能处理4,096个token的文档，导致在处理超长文档时性能下降。&lt;h4&gt;目的&lt;/h4&gt;旨在提高BIGBIRD-PEGASUS模型处理超长文档的能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调预训练的BIGBIRD-PEGASUS模型，并在其他领域的数据集上进行训练。首先，筛选出长度超过20,000个token的文档，以专注于超长文档。为了解决领域迁移问题和数据集小导致的过拟合，通过将文档摘要训练对拆分成部分，以适应4,096个token的文档。&lt;h4&gt;主要发现&lt;/h4&gt;使用微调的方法可以有效提高模型处理超长文档的能力。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提升BIGBIRD-PEGASUS模型在超长文档摘要任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果。然而，其容量仍限于最多4,096个token，因此在处理超长文档时会导致性能下降。处理此问题的常见方法是对文档进行截断。在本研究中，我们将采用不同的方法。我们将使用预训练的BIGBIRD-PEGASUS模型，并通过在其它领域的数据集上微调模型来改进它。首先，我们将所有长度少于20,000个token的文档过滤掉，以专注于超长文档。为了避免领域迁移问题和由于数据集小导致的迁移学习过拟合，我们将数据集通过将文档摘要训练对拆分成部分来增强，以适应4,096个token的文档。源代码可在https://github.com/lhfazry/SPIN-summ上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lhfazry/spin-summ&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ onabstractive text summarization for long documents. However it's capacity stilllimited to maximum of $4,096$ tokens, thus caused performance degradation onsummarization for very long documents. Common method to deal with the issue isto truncate the documents. In this reasearch, we'll use different approach.We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned themodel on other domain dataset. First, we filter out all documents which lengthless than $20,000$ tokens to focus on very long documents. To prevent domainshifting problem and overfitting on transfer learning due to small dataset, weaugment the dataset by splitting document-summary training pair into parts, tofit the document into $4,096$ tokens. Source code available on$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.</description>
      <author>example@mail.com (Lhuqita Fazry)</author>
      <guid isPermaLink="false">2505.06862v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
      <link>http://arxiv.org/abs/2505.06796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态假新闻检测方法，旨在应对deepfake建模攻击。&lt;h4&gt;背景&lt;/h4&gt;多模态新闻信息丰富，但易受deepfake建模攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;设计一个新的多模态假新闻检测数据集（MFND），并构建一个模型来检测和定位高度逼真的假新闻。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Shallow-Deep Multitask Learning（SDML）的模型，该模型利用单模态和互模态特征挖掘新闻的内在语义。在浅层推理中，采用动量蒸馏的轻惩罚对比学习进行细粒度空间图像和文本语义对齐，并引入自适应跨模态融合模块以增强互模态特征。在深层推理中，设计了两个分支框架，分别增强图像和文本的单模态特征，并合并互模态特征进行四个预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在主流和提出的数据集上均表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;SDML模型在多模态假新闻检测方面具有显著效果。&lt;h4&gt;翻译&lt;/h4&gt;The abstract is about a new multimodal fake news detection method aimed at combating deepfake modeling attacks. The method is demonstrated to be effective in detecting and localizing highly authentic fake news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yunan-wang33/sdml&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal news contains a wealth of information and is easily affected bydeepfake modeling attacks. To combat the latest image and text generationmethods, we present a new Multimodal Fake News Detection dataset (MFND)containing 11 manipulated types, designed to detect and localize highlyauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning(SDML) model for fake news, which fully uses unimodal and mutual modal featuresto mine the intrinsic semantics of news. Under shallow inference, we proposethe momentum distillation-based light punishment contrastive learning forfine-grained uniform spatial image and text semantic alignment, and an adaptivecross-modal fusion module to enhance mutual modal features. Under deepinference, we design a two-branch framework to augment the image and textunimodal features, respectively merging with mutual modalities features, forfour predictions via dedicated detection and localization projections.Experiments on both mainstream and our proposed datasets demonstrate thesuperiority of the model. Codes and dataset are released athttps://github.com/yunan-wang33/sdml.</description>
      <author>example@mail.com (Ye Zhu, Yunan Wang, Zitong Yu)</author>
      <guid isPermaLink="false">2505.06796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
      <link>http://arxiv.org/abs/2505.07214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIRA的对话式AI代理，旨在辅助用户在虚拟现实(VR)环境中对3D医学概念进行定位、分割和可视化。&lt;h4&gt;背景&lt;/h4&gt;手动分割医学扫描图像（如MRI、CT）工作量大，容易出错，难以掌握，而全自动算法可以从用户反馈中受益。&lt;h4&gt;目的&lt;/h4&gt;开发一种辅助工具，通过语音交互帮助用户理解影像学特征，定位临床目标，并生成可以快速细化分割掩码的3D医学概念。&lt;h4&gt;方法&lt;/h4&gt;结合最新的影像学AI基础模型和VR的直观数据交互，设计了SAMIRA，并比较了VR控制器指向、头部指向和眼动追踪作为输入模式，以确定最佳交互范式。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究显示，SAMIRA具有高可用性（SUS=90.0 ± 9.0），总体任务负荷低，以及对VR系统的指导、培训潜力和AI在影像学分割任务中集成的高度支持。&lt;h4&gt;结论&lt;/h4&gt;SAMIRA是一个有效且用户友好的工具，可以增强对患者的解剖理解，并可能提高影像学分割任务的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crucial in disease analysis and surgical planning, manual segmentation ofvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, andchallenging to master, while fully automatic algorithms can benefit fromuser-feedback. Therefore, with the complementary power of the latestradiological AI foundation models and virtual reality (VR)'s intuitive datainteraction, we propose SAMIRA, a novel conversational AI agent that assistsusers with localizing, segmenting, and visualizing 3D medical concepts in VR.Through speech-based interaction, the agent helps users understand radiologicalfeatures, locate clinical targets, and generate segmentation masks that can berefined with just a few point prompts. The system also supports true-to-scale3D visualization of segmented pathology to enhance patient-specific anatomicalunderstanding. Furthermore, to determine the optimal interaction paradigm undernear-far attention-switching for refining segmentation masks in an immersive,human-in-the-loop workflow, we compare VR controller pointing, head pointing,and eye tracking as input modes. With a user study, evaluations demonstrated ahigh usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well asstrong support for the proposed VR system's guidance, training potential, andintegration of AI in radiological segmentation tasks.</description>
      <author>example@mail.com (Pascal Spiegler, Arash Harirpoush, Yiming Xiao)</author>
      <guid isPermaLink="false">2505.07214v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
      <link>http://arxiv.org/abs/2505.06853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 6th BioSMART Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。&lt;h4&gt;背景&lt;/h4&gt;拉丁美洲的癌症病例预计将在2022年达到420万，到2045年将增加到670万。骨肉瘤是一种常见的、对年轻人影响严重的骨癌，由于其独特的质地和强度，难以检测。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，以估计膝关节周围骨肉瘤手术的安全边界置信区间。&lt;h4&gt;方法&lt;/h4&gt;该方法使用来自开源仓库的MRI和X射线数据，结合数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突出了自动、针对患者特定情况确定安全边界的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法有望提高骨肉瘤手术的安全性和精确性。&lt;h4&gt;翻译&lt;/h4&gt;根据泛美卫生组织，2022年拉丁美洲的癌症病例估计为420万，预计到2045年将增加到670万。骨肉瘤是影响年轻人的最常见和致命的骨癌之一，由于其独特的质地和强度，难以检测。手术切除骨肉瘤需要精确的安全边界以确保完全切除同时保留健康组织。因此，本研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。提出的方法使用来自开源仓库的MRI和X射线数据，数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。实验结果突出了自动化、针对患者特定情况确定安全边界的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the Pan American Health Organization, the number of cancer casesin Latin America was estimated at 4.2 million in 2022 and is projected to riseto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bonecancers affecting young people, is difficult to detect due to its uniquetexture and intensity. Surgical removal of osteosarcoma requires precise safetymargins to ensure complete resection while preserving healthy tissue.Therefore, this study proposes a method for estimating the confidence intervalof surgical safety margins in osteosarcoma surgery around the knee. Theproposed approach uses MRI and X-ray data from open-source repositories,digital processing techniques, and unsupervised learning algorithms (such ask-means clustering) to define tumor boundaries. Experimental results highlightthe potential for automated, patient-specific determination of safety margins.</description>
      <author>example@mail.com (Carolina Vargas-Ecos, Edwin Salcedo)</author>
      <guid isPermaLink="false">2505.06853v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>QSeer: A Quantum-Inspired Graph Neural Network for Parameter Initialization in Quantum Approximate Optimization Algorithm Circuits</title>
      <link>http://arxiv.org/abs/2505.06810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QSeer的量子灵感的图神经网络，旨在准确预测量子近似优化算法（QAOA）的参数初始化，以提高在NISQ时代的QAOA优化效果。&lt;h4&gt;背景&lt;/h4&gt;量子近似优化算法（QAOA）在优化过程中存在 barren plateau problem，即性能提升停滞的问题。有效的参数初始化对于在近期的有噪声中等规模量子（NISQ）时代优化QAOA至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有参数初始化方法的局限性，提高QAOA在NISQ时代的性能。&lt;h4&gt;方法&lt;/h4&gt;QSeer是一种量子灵感的图神经网络，它借鉴了先前优化的QAOA参数，并结合了关键的物理信息，如参数浓度、对称性和绝热进化原则。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的方法相比，QSeer提高了QAOA电路在多样化图上的初始近似比和收敛速度，分别提升了6%-68%和5x-10x。&lt;h4&gt;结论&lt;/h4&gt;QSeer作为一种新型量子灵感图神经网络，能够显著提高QAOA的优化性能，为NISQ时代的量子计算提供了一种有效的参数初始化方法。&lt;h4&gt;翻译&lt;/h4&gt;To mitigate the barren plateau problem, effective parameter initialization is crucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) in the near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-driven approaches leveraged the optimal parameter concentration phenomenon, utilizing medium values of previously optimized QAOA parameters stored in databases as initialization for new graphs. However, this medium-value-based strategy lacks generalization capability. Conversely, prior computer-science-based approaches employed graph neural networks (GNNs) trained on previously optimized QAOA parameters to predict initialization values for new graphs. However, these approaches neglect key physics-informed QAOA principles, such as parameter concentration, symmetry, and adiabatic evolution, resulting in suboptimal parameter predictions and limited performance improvements. Furthermore, no existing GNN-based methods support parameter initialization for QAOA circuits with variable depths or for solving weighted Max-Cut problems. This paper introduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameter prediction. Compared to prior physics- and computer-science-driven methods, QSeer improves the initial approximation ratio and convergence speed of QAOA circuits across diverse graphs by 6%-68% and 5x-10x, respectively.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To mitigate the barren plateau problem, effective parameter initialization iscrucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) inthe near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-drivenapproaches leveraged the optimal parameter concentration phenomenon, utilizingmedium values of previously optimized QAOA parameters stored in databases asinitialization for new graphs. However, this medium-value-based strategy lacksgeneralization capability. Conversely, prior computer-science-based approachesemployed graph neural networks (GNNs) trained on previously optimized QAOAparameters to predict initialization values for new graphs. However, theseapproaches neglect key physics-informed QAOA principles, such as parameterconcentration, symmetry, and adiabatic evolution, resulting in suboptimalparameter predictions and limited performance improvements. Furthermore, noexisting GNN-based methods support parameter initialization for QAOA circuitswith variable depths or for solving weighted Max-Cut problems. This paperintroduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameterprediction. Compared to prior physics- and computer-science-driven methods,QSeer improves the initial approximation ratio and convergence speed of QAOAcircuits across diverse graphs by 6%-68% and 5x-10x, respectively.</description>
      <author>example@mail.com (Lei Jiang, Chi Zhang, Fan Chen)</author>
      <guid isPermaLink="false">2505.06810v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Architectural Precedents for General Agents using Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 2 figures. Submitted to AGI25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文总结了在多种预Transformer AI架构中出现的认知设计模式，并探讨了这些模式在利用大型语言模型（LLMs）的系统中的应用，特别是对于推理和交互式（“代理”）用例。&lt;h4&gt;背景&lt;/h4&gt;人工智能和通用人工智能（AGI）的目标之一是识别和理解足以实现通用智能的具体机制和表示。在AI/AGI领域，许多认知架构已被探索。&lt;h4&gt;目的&lt;/h4&gt;识别和理解足以实现通用智能的具体机制和表示。&lt;h4&gt;方法&lt;/h4&gt;总结预Transformer AI架构中的认知设计模式，并分析这些模式在LLMs系统中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;不同研究组和不同研究传统独立地识别了相似/共同的过程和表示或认知设计模式，这些模式在现有架构中体现。LLMs提供了一种新的机制和表示组合，用于探索通用智能的可能性。&lt;h4&gt;结论&lt;/h4&gt;通过分析和应用这些重复出现的模式，可以预测今天代理LLM系统中的差距或不足，并确定使用LLMs和其他生成基础模型实现通用智能的未来的研究主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One goal of AI (and AGI) is to identify and understand specific mechanismsand representations sufficient for general intelligence. Often, this workmanifests in research focused on architectures and many cognitive architectureshave been explored in AI/AGI. However, different research groups and evendifferent research traditions have somewhat independently identifiedsimilar/common patterns of processes and representations or cognitive designpatterns that are manifest in existing architectures. Today, AI systemsexploiting large language models (LLMs) offer a relatively new combination ofmechanism and representation available for exploring the possibilities ofgeneral intelligence. In this paper, we summarize a few recurring cognitivedesign patterns that have appeared in various pre-transformer AI architectures.We then explore how these patterns are evident in systems using LLMs,especially for reasoning and interactive ("agentic") use cases. By examiningand applying these recurring patterns, we can also predict gaps or deficienciesin today's Agentic LLM Systems and identify likely subjects of future researchtowards general intelligence using LLMs and other generative foundation models.</description>
      <author>example@mail.com (Robert E. Wray, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2505.07087v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，通过战略性地选择或加权数据。该方法名为DRRho风险最小化，基于分布鲁棒优化（DRO）。通过泛化分析，本文提供了理论上的见解，解释了为什么与没有参考模型训练相比，这种方法可以改善泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;虽然模型引导在包括大型基础模型训练的各种场景中已经使用，但其基本原理理解不足，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于理论的框架DRRho风险最小化，用于模型引导，并引入一种名为DRRho-CLIP的新方法，用于具有参考模型的对比语言-图像预训练（CLIP）。&lt;h4&gt;方法&lt;/h4&gt;本文通过泛化分析，对模型引导提供了理论上的见解，并引入了DRRho-CLIP方法。&lt;h4&gt;主要发现&lt;/h4&gt;本文提供了模型引导的第一个理论见解，增强了我们对模型引导的理解和实践，并通过实验验证了理论见解的有效性。&lt;h4&gt;结论&lt;/h4&gt;DRRho-CLIP方法比没有参考模型的CLIP具有更好的扩展性和性能，优于现有的启发式方法。&lt;h4&gt;翻译&lt;/h4&gt;本文将摘要内容翻译成了中文，并按照要求进行了结构化表达。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. That, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Networks for Cross-Energy Particle Identification at RHIC and LHC</title>
      <link>http://arxiv.org/abs/2505.06732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Journal of Physics G: Nuclear and Particle Physics on 30  April 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了在强横动量区域应用深度神经网络进行粒子识别的方法。&lt;h4&gt;背景&lt;/h4&gt;研究基于模拟的大型强子对撞机（LHC）质子-质子碰撞数据（sqrt(s) = 13TeV）。&lt;h4&gt;目的&lt;/h4&gt;目的是训练一个模型，使用七个动力学特征来区分九种不同的粒子。&lt;h4&gt;方法&lt;/h4&gt;模型在模拟数据上训练，并在高横动量RHIC数据上测试，未进行迁移学习、微调或权重调整。&lt;h4&gt;主要发现&lt;/h4&gt;模型在LHC和RHIC数据集上均保持了超过91%的准确率，在所有RHIC数据集上均达到了超过96%的准确率，包括横动量大于7 GeV/c的数据集，尽管模型未在RHIC数据上训练。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，该模型能够捕捉到高能碰撞的底层物理，而不仅仅是过拟合训练数据。&lt;h4&gt;翻译&lt;/h4&gt;这项工作强调了模拟训练模型在不同能量域中的应用潜力，特别是在数据较少或代表性不足的设置中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work demonstrates the application of a deep neural network for particleidentification in the high transverse momentum regime. A model trained onsimulated Large Hadron Collider (LHC) proton-proton collisions (sqrt(s) = 13TeV) is used to classify nine distinct particles using seven kinematic-levelfeatures. The model is then tested on high transverse momentum RHIC datawithout any transfer learning, fine-tuning, or weight adjustment. It maintainsaccuracy above 91% for both LHC and RHIC sets, while achieving above 96%accuracy for all RHIC sets, including the pT greater than 7 GeV/c set, despitenot having been trained on any RHIC data. These results indicate that the modelcaptures the underlying physics of high-energy collisions rather than justoverfitting to the training data. This work highlights the potential ofsimulation-trained models to be deployed in different energy domains,especially in underrepresented or data-limited settings.</description>
      <author>example@mail.com (Omar M. Khalaf, Ahmed M. Hamed)</author>
      <guid isPermaLink="false">2505.06732v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Graph Representation of Agent Diffuser</title>
      <link>http://arxiv.org/abs/2505.06761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAMAS2025 International Conference on Autonomous Agents  and Multiagent Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LGR-AD的新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。该系统通过模拟分布式系统中相互作用的智能体来建模生成过程，每个智能体代表一个专家子模型，并通过图神经网络进行协作。&lt;h4&gt;背景&lt;/h4&gt;扩散生成模型在文本到图像合成方面取得了显著进展，但静态模型参数可能无法最佳地处理生成过程的各个阶段。&lt;h4&gt;目的&lt;/h4&gt;提出LGR-AD系统，以提高动态计算机视觉任务中的适应性。&lt;h4&gt;方法&lt;/h4&gt;LGR-AD将生成过程建模为分布式系统，每个智能体代表一个专家子模型，并通过图神经网络进行协作。使用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，显示出其在复杂图像生成任务中的可扩展性和灵活性。&lt;h4&gt;结论&lt;/h4&gt;LGR-AD系统在动态计算机视觉任务中具有提高适应性的潜力，并在图像生成任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于扩散的生成模型在文本到图像合成方面取得了显著进展，展示了令人印象深刻的文本理解和零样本泛化能力。这些模型根据文本提示从随机噪声中细化图像，初始对文本输入的依赖逐渐转向增强视觉保真度。这种转变表明，静态模型参数可能无法最佳地处理生成过程的各个阶段。我们引入了LGR-AD（学习智能体扩散器图表示），这是一种新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。LGR-AD将生成过程建模为相互作用的智能体分布式系统，每个智能体代表一个专家子模型。这些智能体通过编码其关系和性能指标的图神经网络动态适应变化条件并协作。我们的方法采用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，突出了其在复杂图像生成任务中的可扩展性和灵活性的潜力。代码可在以下链接获取：https://github.com/YousIA/LGR_AD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yousia/lgr_ad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion-based generative models have significantly advanced text-to-imagesynthesis, demonstrating impressive text comprehension and zero-shotgeneralization. These models refine images from random noise based on textualprompts, with initial reliance on text input shifting towards enhanced visualfidelity over time. This transition suggests that static model parameters mightnot optimally address the distinct phases of generation. We introduce LGR-AD(Learning Graph Representation of Agent Diffusers), a novel multi-agent systemdesigned to improve adaptability in dynamic computer vision tasks. LGR-ADmodels the generation process as a distributed system of interacting agents,each representing an expert sub-model. These agents dynamically adapt tovarying conditions and collaborate through a graph neural network that encodestheir relationships and performance metrics. Our approach employs acoordination mechanism based on top-$k$ maximum spanning trees, optimizing thegeneration process. Each agent's decision-making is guided by a meta-model thatminimizes a novel loss function, balancing accuracy and diversity. Theoreticalanalysis and extensive empirical evaluations show that LGR-AD outperformstraditional diffusion models across various benchmarks, highlighting itspotential for scalable and flexible solutions in complex image generationtasks. Code is available at: https://github.com/YousIA/LGR_AD</description>
      <author>example@mail.com (Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi)</author>
      <guid isPermaLink="false">2505.06761v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mixer-Informer-Based Two-Stage Transfer Learning for Long-Sequence Load Forecasting in Newly Constructed Electric Vehicle Charging Stations</title>
      <link>http://arxiv.org/abs/2505.06657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MIK-TST的创新两阶段迁移学习框架，用于精确预测电动汽车充电站的负荷，旨在提高智能电网效率和支撑可持续的电动汽车基础设施扩展。&lt;h4&gt;背景&lt;/h4&gt;随着电动汽车的快速普及，对充电站负荷的精确预测变得至关重要，但这一预测面临着长期时间依赖性和新设施数据有限的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效预测充电站负荷的MIK-TST框架，以提高充电站负荷预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;MIK-TST框架整合了Mixer、Informer和Kolmogorov-Arnold Networks (KAN)技术。Mixer融合了多源特征，Informer通过ProbSparse注意力机制捕捉长距离依赖性，而KAN则通过可学习的激活函数增强了非线性建模。该框架在大量数据上预训练，并在有限的目标数据上微调。&lt;h4&gt;主要发现&lt;/h4&gt;MIK-TST在MAE和MSE方面分别实现了4%和8%的降低，在Boulder，美国的一个包含26个充电站的数据库上优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;MIK-TST是一种可扩展的解决方案，能够提高智能电网的效率并支持可持续的电动汽车基础设施的扩展。&lt;h4&gt;翻译&lt;/h4&gt;The rapid rise in electric vehicle (EV) adoption demands precise charging station load forecasting, challenged by long-sequence temporal dependencies and limited data in new facilities. This study proposes MIK-TST, a novel two-stage transfer learning framework integrating Mixer, Informer, and Kolmogorov-Arnold Networks (KAN). The Mixer fuses multi-source features, Informer captures long-range dependencies via ProbSparse attention, and KAN enhances non-linear modeling with learnable activation functions. Pre-trained on extensive data and fine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAE and MSE, respectively, outperforming baselines on a dataset of 26 charging stations in Boulder, USA. This scalable solution enhances smart grid efficiency and supports sustainable EV infrastructure expansion.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid rise in electric vehicle (EV) adoption demands precise chargingstation load forecasting, challenged by long-sequence temporal dependencies andlimited data in new facilities. This study proposes MIK-TST, a novel two-stagetransfer learning framework integrating Mixer, Informer, and Kolmogorov-ArnoldNetworks (KAN). The Mixer fuses multi-source features, Informer captureslong-range dependencies via ProbSparse attention, and KAN enhances nonlinearmodeling with learnable activation functions. Pre-trained on extensive data andfine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAEand MSE, respectively, outperforming baselines on a dataset of 26 chargingstations in Boulder, USA. This scalable solution enhances smart grid efficiencyand supports sustainable EV infrastructure expansion.</description>
      <author>example@mail.com (Zhenhua Zhou, Bozhen Jiang, Qin Wang)</author>
      <guid isPermaLink="false">2505.06657v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Contrastive Federated Semi-Supervised Learning Intrusion Detection Framework for Internet of Robotic Things</title>
      <link>http://arxiv.org/abs/2505.06636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对物联网机器人（IoRT）网络入侵检测和防御的框架CFedSSL-NID，用于解决机器人本地没有标记数据且需要保护数据隐私的实际场景。&lt;h4&gt;背景&lt;/h4&gt;在智能工业和自动驾驶等环境中，物联网（IoT）与机器人高度集成形成物联网机器人（IoRT）。然而，IoRT的网络入侵可能导致数据泄露、服务中断，甚至通过控制机器人或车辆造成物理损害。&lt;h4&gt;目的&lt;/h4&gt;为了解决IoRT机器人本地缺乏标记数据且需要保护数据隐私的问题，提出了一种名为CFedSSL-NID的对比联邦半监督学习网络入侵检测框架。&lt;h4&gt;方法&lt;/h4&gt;CFedSSL-NID框架结合了随机弱和强数据增强、潜在对比学习和EMA更新，以整合监督信号，从而在机器人本地未标记数据上增强性能和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，CFedSSL-NID在基准数据集上优于现有的联邦半监督和全监督方法，并且具有更低的资源需求。&lt;h4&gt;结论&lt;/h4&gt;CFedSSL-NID是一种有效的IoRT入侵检测和防御框架，能够有效处理数据隐私保护问题，并具有较低的资源需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In intelligent industry, autonomous driving and other environments, theInternet of Things (IoT) highly integrated with robotic to form the Internet ofRobotic Things (IoRT). However, network intrusion to IoRT can lead to dataleakage, service interruption in IoRT and even physical damage by controllingrobots or vehicles. This paper proposes a Contrastive Federated Semi-SupervisedLearning Network Intrusion Detection framework (CFedSSL-NID) for IoRT intrusiondetection and defense, to address the practical scenario of IoRT where robotsdon't possess labeled data locally and the requirement for data privacypreserving. CFedSSL-NID integrates randomly weak and strong augmentation,latent contrastive learning, and EMA update to integrate supervised signals,thereby enhancing performance and robustness on robots' local unlabeled data.Extensive experiments demonstrate that CFedSSL-NID outperforms existingfederated semi-supervised and fully supervised methods on benchmark dataset andhas lower resource requirements.</description>
      <author>example@mail.com (Yifan Zeng)</author>
      <guid isPermaLink="false">2505.06636v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Reconstructing Brain Causal Dynamics for Subject and Task Fingerprints using fMRI Time-series Data</title>
      <link>http://arxiv.org/abs/2505.06392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种利用因果动力学进行fMRI基于主体和任务指纹识别的新方法，通过分析多尺度脑网络中的复杂关系，验证了其可行性和有效性。&lt;h4&gt;背景&lt;/h4&gt;系统神经科学因果模型近年来因其在多尺度脑网络中解析复杂关系的能力而重新引起关注。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，利用因果动力学，以实现基于fMRI的个体和任务指纹识别。&lt;h4&gt;方法&lt;/h4&gt;应用隐式-显式离散化方案，开发了一种双时标线性状态空间模型。通过数据驱动识别其参数，该模型捕捉因果特征，包括从空间角度分析大脑区域之间的有向交互，以及从时间角度解耦大脑活动的快慢动态模式。这些因果特征随后与基于模型的主体识别的模态分解和投影方法以及用于基于学习的任务分类的图神经网络（GNN）框架相结合。此外，引入了大脑可达性景观的概念作为新的可视化工具，定量描述了在各种fMRI任务下大脑区域可能的最大激活水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用人类连接组项目数据集评估了所提出的方法，并证明了其优于非因果方法的优点。获得的因果特征被可视化，并显示出与大脑功能已建立的理解的明确生物学相关性。&lt;h4&gt;结论&lt;/h4&gt;验证了利用大脑因果特征进行主体和任务指纹识别的可行性和有效性。此外，这项工作为因果指纹在健康对照和神经退行性疾病中的潜在应用的研究铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The abstract of this paper is summarized as follows: Recently, there has been a renewed interest in system neuroscience causation models, driven by their unique capability to unravel complex relationships in multi-scale brain networks. In this paper, we present a novel method that leverages causal dynamics to achieve effective fMRI-based subject and task fingerprinting. By applying an implicit-explicit discretization scheme, we develop a two-timescale linear state-space model. Through data-driven identification of its parameters, the model captures causal signatures, including directed interactions among brain regions from a spatial perspective, and disentangled fast and slow dynamic modes of brain activity from a temporal perspective. These causal signatures are then integrated with: (i) a modal decomposition and projection method for model-based subject identification, and (ii) a Graph Neural Network (GNN) framework for learning-based task classification. Furthermore, we introduce the concept of the brain reachability landscape as a novel visualization tool, which quantitatively characterizes the maximum possible activation levels of brain regions under various fMRI tasks. We evaluate the proposed approach using the Human Connectome Project dataset and demonstrate its advantage over non-causality-based methods. The obtained causal signatures are visualized and demonstrate clear biological relevance with established understandings of brain function. We verified the feasibility and effectiveness of utilizing brain causal signatures for subject and task fingerprinting. Additionally, our work paves the way for further studies on causal fingerprints with potential applications in both healthy controls and neurodegenerative diseases.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Recently, there has been a revived interest in system neurosciencecausation models, driven by their unique capability to unravel complexrelationships in multi-scale brain networks. In this paper, we present a novelmethod that leverages causal dynamics to achieve effective fMRI-based subjectand task fingerprinting. Methods: By applying an implicit-explicitdiscretization scheme, we develop a two-timescale linear state-space model.Through data-driven identification of its parameters, the model captures causalsignatures, including directed interactions among brain regions from a spatialperspective, and disentangled fast and slow dynamic modes of brain activityfrom a temporal perspective. These causal signatures are then integrated with:(i) a modal decomposition and projection method for model-based subjectidentification, and (ii) a Graph Neural Network (GNN) framework forlearning-based task classification. Furthermore, we introduce the concept ofthe brain reachability landscape as a novel visualization tool, whichquantitatively characterizes the maximum possible activation levels of brainregions under various fMRI tasks. Results: We evaluate the proposed approachusing the Human Connectome Project dataset and demonstrate its advantage overnon-causality-based methods. The obtained causal signatures are visualized anddemonstrate clear biological relevance with established understandings of brainfunction. Conclusion: We verified the feasibility and effectiveness ofutilizing brain causal signatures for subject and task fingerprinting.Additionally, our work paves the way for further studies on causal fingerprintswith potential applications in both healthy controls and neurodegenerativediseases.</description>
      <author>example@mail.com (Dachuan Song, Li Shen, Duy Duong-Tran, Xuan Wang)</author>
      <guid isPermaLink="false">2505.06392v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines</title>
      <link>http://arxiv.org/abs/2505.06333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, 2 tables, IJCAI 2025 (International Joint  Conferences on Artificial Intelligence) Special Track on AI4Tech: AI Enabling  Critical Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经符号AI和融合的多模态异常预测方法，用于现代装配管道中的产品质量和运营效率保障。&lt;h4&gt;背景&lt;/h4&gt;在复杂的多模态环境和高数据量的预测环境中，传统的单模态方法无法捕捉到精确异常预测所需的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;确保装配管道中的产品质量和运营效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于时间序列和图像的融合模型，并采用了决策级融合技术。研究基于三个主要创新方法：时间序列和图像的决策级融合建模、迁移学习用于融合和知识注入学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用迁移学习的神经符号AI融合方法能够有效地利用时间序列和图像数据的互补优势，为装配管道中的异常预测提供了一种稳健且可解释的方法，并提高了性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在公开的多模态数据集上进行了评估，并通过消融研究验证了预处理技术和融合模型的有效性。&lt;h4&gt;翻译&lt;/h4&gt;In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. The datasets, codes to reproduce the results, supplementary materials, and demo are available at https://github.com/ChathurangiShyalika/NSF-MAP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/chathurangishyalika/nsf-map&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern assembly pipelines, identifying anomalies is crucial in ensuringproduct quality and operational efficiency. Conventional single-modalitymethods fail to capture the intricate relationships required for preciseanomaly prediction in complex predictive environments with abundant data andmultiple modalities. This paper proposes a neurosymbolic AI and fusion-basedapproach for multimodal anomaly prediction in assembly pipelines. We introducea time series and image-based fusion model that leverages decision-level fusiontechniques. Our research builds upon three primary novel approaches inmultimodal learning: time series and image-based decision-level fusionmodeling, transfer learning for fusion, and knowledge-infused learning. Weevaluate the novel method using our derived and publicly available multimodaldataset and conduct comprehensive ablation studies to assess the impact of ourpreprocessing techniques and fusion model compared to traditional baselines.The results demonstrate that a neurosymbolic AI-based fusion approach that usestransfer learning can effectively harness the complementary strengths of timeseries and image data, offering a robust and interpretable approach for anomalyprediction in assembly pipelines with enhanced performance. \noindent Thedatasets, codes to reproduce the results, supplementary materials, and demo areavailable at https://github.com/ChathurangiShyalika/NSF-MAP.</description>
      <author>example@mail.com (Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth)</author>
      <guid isPermaLink="false">2505.06333v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2505.06628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ACORN的算法，用于提高机器人在现实世界中的应用中的鲁棒性和安全性。&lt;h4&gt;背景&lt;/h4&gt;传统的Embodied AI研究过于关注成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一差距，本文引入了四个新的以安全性为中心的指标，并提出了ACORN算法，以提高策略的鲁棒性而不牺牲性能。&lt;h4&gt;方法&lt;/h4&gt;ACORN利用对比学习同时将轨迹与专家演示对齐，并从可能的不安全行为中分离出来。它通过结构化高斯噪声注入生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。&lt;h4&gt;结论&lt;/h4&gt;ACORN在提高Embodied AI在安全性关键的实际应用中的可靠部署方面具有重大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Embodied AI研究传统上强调成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。在实际情况中，智能体不断遇到不可预测的情况和分布变化，导致看似可靠的策略出现灾难性失败，尤其是在操作任务中。为了解决这一差距，我们引入了四个新的以安全性为中心的指标，以量化智能体对环境扰动的弹性。在这些指标的基础上，我们提出了自适应对比优化算法ACORN，这是一种即插即用的算法，它增强了策略的鲁棒性，而不牺牲性能。ACORN利用对比学习同时将轨迹与专家演示对齐，同时从可能的不安全行为中分离出来。我们的方法通过结构化高斯噪声注入高效地生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。这些发现强调了ACORN在使Embodied AI在安全性关键的实际应用中可靠部署方面的重要潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied AI research has traditionally emphasized performance metrics such assuccess rate and cumulative reward, overlooking critical robustness and safetyconsiderations that emerge during real-world deployment. In actualenvironments, agents continuously encounter unpredicted situations anddistribution shifts, causing seemingly reliable policies to experiencecatastrophic failures, particularly in manipulation tasks. To address this gap,we introduce four novel safety-centric metrics that quantify an agent'sresilience to environmental perturbations. Building on these metrics, wepresent Adaptive Contrastive Optimization for Robust Manipulation (ACORN), aplug-and-play algorithm that enhances policy robustness without sacrificingperformance. ACORN leverages contrastive learning to simultaneously aligntrajectories with expert demonstrations while diverging from potentially unsafebehaviors. Our approach efficiently generates informative negative samplesthrough structured Gaussian noise injection, employing a double perturbationtechnique that maintains sample diversity while minimizing computationaloverhead. Comprehensive experiments across diverse manipulation environmentsvalidate ACORN's effectiveness, yielding improvements of up to 23% in safetymetrics under disturbance compared to baseline methods. These findingsunderscore ACORN's significant potential for enabling reliable deployment ofembodied agents in safety-critical real-world applications.</description>
      <author>example@mail.com (Zhongquan Zhou, Shuhao Li, Zixian Yue)</author>
      <guid isPermaLink="false">2505.06628v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
      <link>http://arxiv.org/abs/2505.06557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TCSVT 2025, doi at https://ieeexplore.ieee.org/document/10970001&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Positive Sample Mining (PSM)的框架，用于弱监督时间句子定位（WSTSG）任务，旨在从无剪辑视频中检测与语言描述相对应的时间区间。&lt;h4&gt;背景&lt;/h4&gt;现有的WSTSG方法通常通过生成负样本进行对比学习，但这些负样本与锚样本高度相似，直接将其作为负样本会导致优化困难，并忽略了这些相似样本与锚样本之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出PSM框架，从训练集中挖掘正样本，提供更具区分性的监督。&lt;h4&gt;方法&lt;/h4&gt;PSM框架通过文本查询的相似性将剩余训练集划分为语义相似和不同子集。为了有效利用这些相关性，引入了PSM指导的对比损失和排名损失，以确保锚样本与相似样本更近，与不相似样本更远，并区分锚样本和负样本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PSM框架在WSTSG和grounded VideoQA任务上表现出有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;PSM框架能够有效提高WSTSG任务的性能，为视频理解提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The task of weakly supervised temporal sentence grounding (WSTSG) aims todetect temporal intervals corresponding to a language description fromuntrimmed videos with only video-level video-language correspondence. For ananchor sample, most existing approaches generate negative samples either fromother videos or within the same video for contrastive learning. However, sometraining samples are highly similar to the anchor sample, directly regardingthem as negative samples leads to difficulties for optimization and ignores thecorrelations between these similar samples and the anchor sample. To addressthis, we propose Positive Sample Mining (PSM), a novel framework that minespositive samples from the training set to provide more discriminativesupervision. Specifically, for a given anchor sample, we partition theremaining training set into semantically similar and dissimilar subsets basedon the similarity of their text queries. To effectively leverage thesecorrelations, we introduce a PSM-guided contrastive loss to ensure that theanchor proposal is closer to similar samples and further from dissimilar ones.Additionally, we design a PSM-guided rank loss to ensure that similar samplesare closer to the anchor proposal than to the negative intra-video proposal,aiming to distinguish the anchor proposal and the negative intra-videoproposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate theeffectiveness and superiority of our method.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2505.06557v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该摘要介绍了一种名为MMSA的多模态自监督分子表示预训练框架，用于提升分子图表示的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确提取分子表示是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，特别是基于图像和2D/3D拓扑的多模态分子表示方法变得越来越主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间相互作用，未能充分捕捉分子之间的复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于结构感知的多模态自监督分子表示预训练框架（MMSA），通过利用分子之间的不变知识来增强分子图表示。该框架包括两个主要模块：多模态分子表示学习模块和结构感知模块。&lt;h4&gt;主要发现&lt;/h4&gt;多模态分子表示学习模块协同处理同一种分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来建模分子之间的高阶相关性，并引入一个记忆机制来存储典型的分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型的一般化能力。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验证明了MMSA的有效性，在MoleculeNet基准测试中取得了最先进的性能，与基线方法相比，平均ROC-AUC提高了1.8%至9.6%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning</title>
      <link>http://arxiv.org/abs/2505.06321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用图学习来增强大型语言模型（LLMs）推理能力的框架。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型在各种领域取得了显著成功，但它们在训练成本和解决复杂推理问题方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，以实现LLMs更灵活和自适应的推理能力。&lt;h4&gt;方法&lt;/h4&gt;该框架将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。此外，引入了图神经网络（GNN）模块来对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，这种方法在不要求额外训练或特定任务提示设计的情况下，显著提高了多个任务上的推理性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为LLMs提供了更灵活和自适应的推理能力，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各个领域取得了显著的成功。然而，它们仍然面临着包括训练高计算成本和解决复杂推理问题限制在内的重大挑战。尽管现有方法通过结构化范式扩展了LLMs的推理能力，但这些方法通常依赖于特定任务的提示和预定义的推理过程，这限制了它们的灵活性和泛化能力。为了解决这些限制，我们提出了一种新颖的框架，该框架利用图学习来为LLMs提供更灵活和自适应的推理能力。具体而言，这种方法将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。为了进一步增强模型的适应性，我们引入了一个图神经网络（GNN）模块，对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。实验结果表明，这种方法在多个任务上显著提高了推理性能，而不需要额外的训练或特定任务的提示设计。代码可在https://github.com/zch65458525/L2T上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zch65458525/l2t&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success across variousdomains. However, they still face significant challenges, including highcomputational costs for training and limitations in solving complex reasoningproblems. Although existing methods have extended the reasoning capabilities ofLLMs through structured paradigms, these approaches often rely on task-specificprompts and predefined reasoning processes, which constrain their flexibilityand generalizability. To address these limitations, we propose a novelframework that leverages graph learning to enable more flexible and adaptivereasoning capabilities for LLMs. Specifically, this approach models thereasoning process of a problem as a graph and employs LLM-based graph learningto guide the adaptive generation of each reasoning step. To further enhance theadaptability of the model, we introduce a Graph Neural Network (GNN) module toperform representation learning on the generated reasoning process, enablingreal-time adjustments to both the model and the prompt. Experimental resultsdemonstrate that this method significantly improves reasoning performanceacross multiple tasks without requiring additional training or task-specificprompt design. Code can be found in https://github.com/zch65458525/L2T.</description>
      <author>example@mail.com (Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.06321v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在图数据上的应用，提出了一种新的框架RELGCL，通过保留自然相对相似性模式来提高学习效果。&lt;h4&gt;背景&lt;/h4&gt;GCL在计算机视觉领域取得了成功，但在图数据上由于图的离散性和非欧几里得性质，传统方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，以解决图数据中的挑战，并提高学习效果。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个真实世界的图，发现了一个普遍模式：随着结构距离的增加，标签一致性会系统地减少。利用随机游走理论对这一模式进行理论保证，并提出了RELGCL框架。&lt;h4&gt;主要发现&lt;/h4&gt;发现图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;RELGCL框架在同质性和异质性图上均优于20种现有方法，验证了利用自然相对相似性比人工绝对相似性更有效。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
      <link>http://arxiv.org/abs/2505.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  On going work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了大型语言模型（LLMs）如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域，并提出了一种名为人工个性化智能（API）的愿景，旨在通过个性化定制来满足用户的具体需求，同时保持隐私和效率。&lt;h4&gt;背景&lt;/h4&gt;LLMs如ChatGPT等在生成类似人类内容方面表现出色，接近实现通用人工智能（AGI），但它们的大规模特性、对隐私的敏感性以及计算需求给个性化定制带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出人工个性化智能（API）的概念，旨在通过个性化联邦智能（PFI）将强大的模型适应于用户的具体需求，同时保持隐私和效率。&lt;h4&gt;方法&lt;/h4&gt;本文首先回顾了联邦学习（FL）和基础模型（FMs）的最新进展，并讨论了利用FMs增强联邦系统的潜力。然后，文章探讨了实现PFI的关键动机和这一领域的有希望的机会，包括高效的PFI、可信的PFI和由检索增强生成（RAG）赋能的PFI。&lt;h4&gt;主要发现&lt;/h4&gt;PFI结合了联邦学习的隐私保护优势以及基础模型的零样本泛化能力，使得在边缘进行个性化、高效且隐私保护的应用成为可能。&lt;h4&gt;结论&lt;/h4&gt;本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;h4&gt;翻译&lt;/h4&gt;本文讨论了大型语言模型（LLMs），如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域。作为构建在LLMs之上的基础模型（FMs）的突出例子，这些模型在生成类似人类内容方面表现出色，使我们更接近实现通用人工智能（AGI）。然而，它们的大规模特性、对隐私的敏感性和大量的计算需求给为最终用户进行个性化定制带来了重大挑战。为了弥合这一差距，本文提出了人工个性化智能（API）的愿景，重点在于将这些强大的模型适应于满足用户的具体需求，同时保持隐私和效率。具体而言，本文提出了个性化联邦智能（PFI），它将联邦学习的隐私保护优势与FMs的零样本泛化能力相结合，使得在边缘实现个性化、高效和隐私保护的应用成为可能。我们首先回顾了FL和FMs的最近进展，并讨论了利用FMs增强联邦系统的潜力。然后，我们提出了实现PFI的关键动机并探索了这一领域的有希望的机会，包括高效PFI、可信PFI以及由检索增强生成（RAG）赋能的PFI。最后，我们概述了在边缘部署FM驱动的FL系统时的关键挑战和未来研究方向，以实现改进的个性化、计算效率和隐私保证。总的来说，本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of large language models (LLMs), such as ChatGPT, DeepSeek, andGrok-3, has reshaped the artificial intelligence landscape. As prominentexamples of foundational models (FMs) built on LLMs, these models exhibitremarkable capabilities in generating human-like content, bringing us closer toachieving artificial general intelligence (AGI). However, their large-scalenature, sensitivity to privacy concerns, and substantial computational demandspresent significant challenges to personalized customization for end users. Tobridge this gap, this paper presents the vision of artificial personalizedintelligence (API), focusing on adapting these powerful models to meet thespecific needs and preferences of users while maintaining privacy andefficiency. Specifically, this paper proposes personalized federatedintelligence (PFI), which integrates the privacy-preserving advantages offederated learning (FL) with the zero-shot generalization capabilities of FMs,enabling personalized, efficient, and privacy-protective deployment at theedge. We first review recent advances in both FL and FMs, and discuss thepotential of leveraging FMs to enhance federated systems. We then present thekey motivations behind realizing PFI and explore promising opportunities inthis space, including efficient PFI, trustworthy PFI, and PFI empowered byretrieval-augmented generation (RAG). Finally, we outline key challenges andfuture research directions for deploying FM-powered FL systems at the edge withimproved personalization, computational efficiency, and privacy guarantees.Overall, this survey aims to lay the groundwork for the development of API as acomplement to AGI, with a particular focus on PFI as a key enabling technique.</description>
      <author>example@mail.com (Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong)</author>
      <guid isPermaLink="false">2505.06907v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data</title>
      <link>http://arxiv.org/abs/2505.06276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a poster in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了SynSHRP2，一个由SHRP 2 NDS数据合成的多模态驾驶数据集，用于解决SCEs数据难以获取的问题。&lt;h4&gt;背景&lt;/h4&gt;SCEs（与驾驶相关的安全关键事件）对于自动驾驶系统的发展和安全评估至关重要，但由于SCEs的稀有性和数据中存在的隐私信息，其获取存在挑战。&lt;h4&gt;目的&lt;/h4&gt;合成数据集以解决SCEs数据获取的难题，同时保护个人隐私。&lt;h4&gt;方法&lt;/h4&gt;使用StableDiffusion和ControlNet技术生成去识别化的关键帧，并包含详细标注，包括SCE类型、环境交通条件和事件前后的时序运动数据。&lt;h4&gt;主要发现&lt;/h4&gt;SynSHRP2数据集包含1874起碰撞和6924起险情，可用于事件属性分类和场景理解，为安全研究和自动驾驶系统开发提供潜力。&lt;h4&gt;结论&lt;/h4&gt;SynSHRP2数据集为安全研究提供了宝贵的资源，并展示了其在自动驾驶系统开发中的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;The paper introduces SynSHRP2, a synthetically created multimodal driving dataset derived from the SHRP 2 NDS, to address the challenge of accessing SCEs data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving-related safety-critical events (SCEs), including crashes andnear-crashes, provide essential insights for the development and safetyevaluation of automated driving systems. However, two major challenges limittheir accessibility: the rarity of SCEs and the presence of sensitive privacyinformation in the data. The Second Strategic Highway Research Program (SHRP 2)Naturalistic Driving Study (NDS), the largest NDS to date, collected millionsof hours of multimodal, high-resolution, high-frequency driving data fromthousands of participants, capturing thousands of SCEs. While this dataset isinvaluable for safety research, privacy concerns and data use restrictionssignificantly limit public access to the raw data. To address these challenges,we introduce SynSHRP2, a publicly available, synthetic, multimodal drivingdataset containing over 1874 crashes and 6924 near-crashes derived from theSHRP 2 NDS. The dataset features de-identified keyframes generated using StableDiffusion and ControlNet, ensuring the preservation of critical safety-relatedinformation while eliminating personally identifiable data. Additionally,SynSHRP2 includes detailed annotations on SCE type, environmental and trafficconditions, and time-series kinematic data spanning 5 seconds before and duringeach event. Synchronized keyframes and narrative descriptions further enhanceits usability. This paper presents two benchmarks for event attributeclassification and scene understanding, demonstrating the potentialapplications of SynSHRP2 in advancing safety research and automated drivingsystem development.</description>
      <author>example@mail.com (Liang Shi, Boyu Jiang, Zhenyuan Yuan, Miguel A. Perez, Feng Guo)</author>
      <guid isPermaLink="false">2505.06276v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders</title>
      <link>http://arxiv.org/abs/2505.06316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHCOMP的新方法，用于科学数据的错误受限有损压缩，该方法通过利用图神经网络技术来提高压缩比并控制数据失真。&lt;h4&gt;背景&lt;/h4&gt;科学数据的生成量巨大，对存储、传输和分析提出了挑战。现有的错误受限有损压缩方法往往忽略了科学数据中固有的空间和时间相关性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图基于方法，用于实现科学数据的错误受限有损压缩，同时保持较高的压缩比和较低的数据失真。&lt;h4&gt;方法&lt;/h4&gt;对原始网格数据进行不规则分割，生成保留空间和时间相关性的图表示。利用图神经网络设计一个时间图自编码器，学习压缩后的图表示。解压缩过程使用学习到的图模型和潜在表示来重建近似原始数据。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPHCOMP在多个数据集上实现了最高的压缩比，比第二好的方法高出22%至50%。&lt;h4&gt;结论&lt;/h4&gt;GRAPHCOMP是一种有效的科学数据错误受限有损压缩方法，能够显著提高压缩比，同时满足用户定义的误差界限。&lt;h4&gt;翻译&lt;/h4&gt;The generation of large amounts of scientific data poses significant challenges for efficient storage, transfer, and analysis. Recently, error-bounded lossy compression methods have emerged due to their ability to achieve high compression ratios while controlling data distortion. However, they often overlook the inherent spatial and temporal correlations within scientific data, thus missing opportunities for higher compression. In this paper, we propose GRAPHCOMP, a novel graph-based method for error-bounded lossy compression of scientific data. We perform irregular segmentation of the original grid data and generate a graph representation that preserves the spatial and temporal correlations. Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph autoencoder to learn latent representations that significantly reduce the size of the graph, effectively compressing the original data. Decompression reverses the process and utilizes the learned graph model together with the latent representation to reconstruct an approximation of the original data. The decompressed data are guaranteed to satisfy a user-defined point-wise error bound. We compare our method against the state-of-the-art error-bounded lossy methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic data. GRAPHCOMP consistently achieves the highest compression ratio across most datasets, outperforming the second-best method by margins ranging from 22% to 50%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generation of voluminous scientific data poses significant challenges forefficient storage, transfer, and analysis. Recently, error-bounded lossycompression methods emerged due to their ability to achieve high compressionratios while controlling data distortion. However, they often overlook theinherent spatial and temporal correlations within scientific data, thus missingopportunities for higher compression. In this paper we propose GRAPHCOMP, anovel graph-based method for error-bounded lossy compression of scientificdata. We perform irregular segmentation of the original grid data and generatea graph representation that preserves the spatial and temporal correlations.Inspired by Graph Neural Networks (GNNs), we then propose a temporal graphautoencoder to learn latent representations that significantly reduce the sizeof the graph, effectively compressing the original data. Decompression reversesthe process and utilizes the learnt graph model together with the latentrepresentation to reconstruct an approximation of the original data. Thedecompressed data are guaranteed to satisfy a user-defined point-wise errorbound. We compare our method against the state-of-the-art error-bounded lossymethods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and syntheticdata. GRAPHCOMP consistently achieves the highest compression ratio across mostdatasets, outperforming the second-best method by margins ranging from 22% to50%.</description>
      <author>example@mail.com (Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Ibrahim Hoteit, Panos Kalnis)</author>
      <guid isPermaLink="false">2505.06316v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Improving Generalization of Medical Image Registration Foundation Model</title>
      <link>http://arxiv.org/abs/2505.06527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了可变形配准在医学图像处理中的重要性，以及如何通过结合Sharpness-Aware Minimization (SAM)技术来提升基础模型在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;可变形配准是医学图像处理中的基本任务，旨在通过建立非线性对应关系实现图像的精确对齐。传统方法在适应性和可解释性方面表现良好，但计算效率有限。深度学习方法提高了配准速度和准确性，但通常缺乏灵活性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的局限性，本文提出将SAM技术整合到基础模型中，以提高其在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过优化损失函数的平坦度，SAM技术增强了模型在不同数据分布下的稳定性，并提升了其处理复杂临床场景的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，整合了SAM的基础模型在跨数据集配准性能方面取得了显著提升，为医学图像配准技术的进步提供了新的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法通过结合SAM技术，有效提升了基础模型在医学图像配准中的性能，为该领域的研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;This paper discusses the importance of deformable registration in medical image processing and proposes the integration of Sharpness-Aware Minimization (SAM) technology to enhance the generalization and robustness of foundation models in medical image registration.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/promise13/fm_sam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deformable registration is a fundamental task in medical image processing,aiming to achieve precise alignment by establishing nonlinear correspondencesbetween images. Traditional methods offer good adaptability andinterpretability but are limited by computational efficiency. Although deeplearning approaches have significantly improved registration speed andaccuracy, they often lack flexibility and generalizability across differentdatasets and tasks. In recent years, foundation models have emerged as apromising direction, leveraging large and diverse datasets to learn universalfeatures and transformation patterns for image registration, thus demonstratingstrong cross-task transferability. However, these models still face challengesin generalization and robustness when encountering novel anatomical structures,varying imaging conditions, or unseen modalities. To address these limitations,this paper incorporates Sharpness-Aware Minimization (SAM) into foundationmodels to enhance their generalization and robustness in medical imageregistration. By optimizing the flatness of the loss landscape, SAM improvesmodel stability across diverse data distributions and strengthens its abilityto handle complex clinical scenarios. Experimental results show that foundationmodels integrated with SAM achieve significant improvements in cross-datasetregistration performance, offering new insights for the advancement of medicalimage registration technology. Our code is available athttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.</description>
      <author>example@mail.com (Jing Hu, Kaiwei Yu, Hongjiang Xian, Shu Hu, Xin Wang)</author>
      <guid isPermaLink="false">2505.06527v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere</title>
      <link>http://arxiv.org/abs/2505.06474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。&lt;h4&gt;背景&lt;/h4&gt;现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种条件生成模型作为替代方案，并展示cBottle框架在气候模拟和再分析中的应用。&lt;h4&gt;方法&lt;/h4&gt;cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。&lt;h4&gt;主要发现&lt;/h4&gt;cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。&lt;h4&gt;结论&lt;/h4&gt;cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。然而，现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。为此，本文提出条件生成模型作为替代方案，并展示了cBottle框架在气候模拟和再分析中的应用。cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI emulators offer a path to compressing, boosting limited ensembles, andimproving the latency of interacting with petabyte-scale climate predictiondata. However, prevailing auto-regressive paradigms offer limited flexibility,and are challenging to train on climate time horizons due to drifts,instabilities and component-coupling challenges. Conditionally generativemodels offer an appealing alternative. In this context we demonstrate agenerative diffusion-based framework -- Climate in a Bottle (cBottle) -- foremulating global km-scale climate simulations and reanalysis on the equal-areaHEALPix grid. cBottle consists of two model stages: a globally-trainedcoarse-resolution image generator that generates 100km (50k-pixel) fields givenmonthly average sea surface temperatures and solar conditioning, followed by alocally-trained 16x super-resolution stage that generates 5km (12.5M-pixel)fields; global super-resolution is made affordable using an overlappingpatch-based multi-diffusion. Overall, cBottle shows promise as an emulatoracross a battery of climate model diagnostics, including diurnal-to-seasonalscale variability, large-scale modes of variability, tropical cyclonestatistics, and trends of climate change and weather extremes. Moreover,cBottle is a step towards a foundation model, by bridging multiple datamodalities (reanalysis and simulation) with corresponding utility beyondemulation to tasks such as zero-shot bias correction, climate downscaling, andchannel in-filling.</description>
      <author>example@mail.com (Noah D. Brenowitz, Tao Ge, Akshay Subramaniam, Aayush Gupta, David M. Hall, Morteza Mardani, Arash Vahdat, Karthik Kashinath, Michael S. Pritchard)</author>
      <guid isPermaLink="false">2505.06474v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Stochastic Variational Propagation (SVP)方法，作为深度学习中的反向传播（BP）的替代方案，以提高可扩展性和减少内存占用。&lt;h4&gt;背景&lt;/h4&gt;反向传播（BP）依赖于全局梯度同步，这限制了其可扩展性并带来显著的内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP方法，将训练重新构造成层次化的变分推断，以实现可扩展的训练。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了防止层间表示的崩溃，SVP使用固定随机矩阵将激活投影到低维空间，同时结合特征对齐损失以保持层间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并带来显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，作为一种可扩展的替代方案，将训练重新构造成层次化的变分推断。SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能导致层间表示的崩溃，因为过度压缩。为了防止这种情况，SVP通过固定随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合特征对齐损失以保持层间一致性，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A New DAPO Algorithm for Stock Trading</title>
      <link>http://arxiv.org/abs/2505.06408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE IDS 2025 Special Track: Financial Reinforcement  Learning and Foundation Models (FinRLFM). 3 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究强化学习在金融交易中的应用，设计了一种结合改进的GRPO算法、DAPO思想和LLM提取的风险和情绪信号的交易代理。&lt;h4&gt;背景&lt;/h4&gt;强化学习在语言模型配合下表现优异，但其在金融交易中的潜力尚待探索。&lt;h4&gt;目的&lt;/h4&gt;探讨是否可以通过强化学习实现金融交易中的类似增益。&lt;h4&gt;方法&lt;/h4&gt;设计了一种交易代理，结合改进的GRPO算法、DAPO思想和从金融新闻中提取的LLM风险和情绪信号。&lt;h4&gt;主要发现&lt;/h4&gt;在NASDAQ-100指数上，该代理实现了230.49%的累计回报和0.37的信息比率，优于CPPO-DeepSeek基准。同时，训练时间缩短至2.5小时，RAM使用量显著降低。&lt;h4&gt;结论&lt;/h4&gt;提出的RL-LLM框架为数据高效交易代理提供了一个可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;The study explores the application of reinforcement learning in financial trading, designing a trading agent that combines an improved GRPO algorithm, DAPO ideas, and LLM-based risk and sentiment signals extracted from financial news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in reinforcement learning, such as Dynamic Sampling PolicyOptimization (DAPO), show strong performance when paired with large languagemodels (LLMs). Motivated by this success, we ask whether similar gains can berealized in financial trading. We design a trading agent that combines animproved Group Relative Policy Optimization (GRPO) algorithm, augmented withideas from DAPO, with LLM-based risk and sentiment signals extracted fromfinancial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains acumulative return of 230.49 percent and an information ratio of 0.37,outperforming the CPPO-DeepSeek baseline. It also cuts training time from about8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. Theproposed RL-LLM framework offers a scalable path toward data-efficient tradingagents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/</description>
      <author>example@mail.com (Ruijian Zha, Bojun Liu)</author>
      <guid isPermaLink="false">2505.06408v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.06301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于边缘增强的图神经网络框架，用于解决人类活动识别（HAR）中的跨用户变异性问题。&lt;h4&gt;背景&lt;/h4&gt;由于传感器放置、身体动态和行为模式的不同，跨用户变异性在人类活动识别中仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够捕捉用户间共有的生物力学不变量，从而提高传统方法的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该方法将解剖学相关性知识整合到一个统一的图神经网络架构中，通过编码领域不变特征来解决用户特定变异性，并使用变分边缘特征提取器来处理这个问题。梯度反转层（GRL）用于执行对抗性领域泛化，确保对未见用户的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在OPPORTUNITY和DSADS数据集上的大量实验表明，该方法取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过信息融合技术，本文将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在人类活动识别（HAR）中，由于传感器放置、身体动态和行为模式的不同，跨用户变异性仍然是一个关键挑战。传统的方 法往往无法捕捉用户间持续存在的生物力学不变量，限制了它们的泛化能力。我们提出了一种边缘增强的图神经网络（GNN）架构的对抗性领域泛化（EEG-ADG）框架，将解剖学相关性知识整合到统一框架中。通过建模三个生物力学动机关系——相互连接单元、类似单元和侧向单元——我们的方法编码了领域不变特征，并通过变分边缘特征提取器解决了用户特定变异性问题。梯度反转层（GRL）执行对抗性领域泛化，确保对未见用户的鲁棒性。在OPPORTUNITY和DSADS数据集上的大量实验证明了该方法的最佳性能。通过信息融合技术，我们的工作将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-user variability in Human Activity Recognition (HAR) remains a criticalchallenge due to differences in sensor placement, body dynamics, and behavioralpatterns. Traditional methods often fail to capture biomechanical invariantsthat persist across users, limiting their generalization capability. We proposean Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)framework that integrates anatomical correlation knowledge into a unified graphneural network (GNN) architecture. By modeling three biomechanically motivatedrelationships together-Interconnected Units, Analogous Units, and LateralUnits-our method encodes domain-invariant features while addressinguser-specific variability through Variational Edge Feature Extractor. AGradient Reversal Layer (GRL) enforces adversarial domain generalization,ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY andDSADS datasets demonstrate state-of-the-art performance. Our work bridgesbiomechanical principles with graph-based adversarial learning by integratinginformation fusion techniques. This fusion of information underpins our unifiedand generalized model for cross-user HAR.</description>
      <author>example@mail.com (Xiaozhou Ye, Kevin I-Kai Wang)</author>
      <guid isPermaLink="false">2505.06301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.06282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted by SIGIR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IFL-GCL的图对比学习方法，旨在解决传统GCL在语义相似对被错误分类为负样本导致的采样偏差问题，并通过在图预训练框架和LLM增强器中实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）在图基础模型或LLM作为图增强器的研究中发挥着关键作用。然而，传统的GCL方法通过数据增强定义自监督任务，导致语义相似对被错误分类为负样本，从而限制了性能。&lt;h4&gt;目的&lt;/h4&gt;将GCL视为正未标记（PU）学习问题，并通过语义指导的自监督任务定义来改进GCL的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为IFL-GCL的方法，使用InfoNCE来提取语义信息，并通过证明在InfoNCE下节点对的表示相似性与对应对比样本为正样本的概率一致，重新定义了基于校正样本的最大似然目标，从而得到一个新的InfoNCE损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在图预训练框架和LLM作为增强器的情况下，IFL-GCL在独立同分布（IID）和无关领域（OOD）场景中都实现了显著的性能提升，最高达到了9.05%的改进。&lt;h4&gt;结论&lt;/h4&gt;语义指导的IFL-GCL方法验证了其在图对比学习中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating augmented pairs as positive samples and others as negative. However, this leads to semantically similar pairs being classified as negative, causing significant sampling bias and limiting performance. In this paper, we argue that GCL is essentially a Positive-Unlabeled (PU) learning problem, where the definition of self-supervised tasks should be semantically guided, i.e., augmented samples with similar semantics are considered positive, while others, with unknown semantics, are treated as unlabeled. From this perspective, the key lies in how to extract semantic information. To achieve this, we propose IFL-GCL, using InfoNCE as a 'free lunch' to extract semantic information. Specifically, we first prove that under InfoNCE, the representation similarity of node pairs aligns with the probability that the corresponding contrastive sample is positive. Then we redefine the maximum likelihood objective based on the corrected samples, leading to a new InfoNCE loss function. Extensive experiments on both the graph pretraining framework and LLM as an enhancer show significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving up to a 9.05% improvement, validating the effectiveness of semantically guided. Code for IFL-GCL is publicly available at: https://github.com/Camel-Prince/IFL-GCL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3726302.3730007&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an important graph pre-training method, Graph Contrastive Learning (GCL)continues to play a crucial role in the ongoing surge of research on graphfoundation models or LLM as enhancer for graphs. Traditional GCL optimizesInfoNCE by using augmentations to define self-supervised tasks, treatingaugmented pairs as positive samples and others as negative. However, this leadsto semantically similar pairs being classified as negative, causing significantsampling bias and limiting performance. In this paper, we argue that GCL isessentially a Positive-Unlabeled (PU) learning problem, where the definition ofself-supervised tasks should be semantically guided, i.e., augmented sampleswith similar semantics are considered positive, while others, with unknownsemantics, are treated as unlabeled. From this perspective, the key lies in howto extract semantic information. To achieve this, we propose IFL-GCL, usingInfoNCE as a "free lunch" to extract semantic information. Specifically, Wefirst prove that under InfoNCE, the representation similarity of node pairsaligns with the probability that the corresponding contrastive sample ispositive. Then we redefine the maximum likelihood objective based on thecorrected samples, leading to a new InfoNCE loss function. Extensiveexperiments on both the graph pretraining framework and LLM as an enhancer showsignificantly improvements of IFL-GCL in both IID and OOD scenarios, achievingup to a 9.05% improvement, validating the effectiveness of semantically guided.Code for IFL-GCL is publicly available at:https://github.com/Camel-Prince/IFL-GCL.</description>
      <author>example@mail.com (Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng)</author>
      <guid isPermaLink="false">2505.06282v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume</title>
      <link>http://arxiv.org/abs/2505.06292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Graph Neural Network for Urban Interpolation (GNNUI)，一种新型的城市交通流量估计方法，并提出了两个新的开放的大规模城市交通流量基准。&lt;h4&gt;背景&lt;/h4&gt;交通流量数据对城市规划至关重要，但交通传感器部署和维护成本高，导致数据稀疏。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的城市交通流量估计方法，解决交通数据稀疏的问题。&lt;h4&gt;方法&lt;/h4&gt;GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。同时，提出了两个新的城市交通流量基准：柏林的Strava自行车数据和纽约市的出租车数据。&lt;h4&gt;主要发现&lt;/h4&gt;GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于其他基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。在Strava和出租车数据集上，GNNUI在极端数据稀缺的情况下表现良好，MAE仅略有增加。&lt;h4&gt;结论&lt;/h4&gt;GNNUI是一种有效且鲁棒的城市交通流量估计方法，有助于解决交通数据稀疏问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：可靠的地面级交通流量数据，涵盖多种交通方式，有助于城市规划，通过提供基础设施改善、交通管理和公共交通决策的信息。然而，由于部署和维护成本高，测量交通流量的传感器通常分布稀疏。为了解决这个问题，插值方法可以使用可用数据估计未观测位置的交通流量。图神经网络在交通流量预测中表现出强大的性能，尤其是在高速公路和主要干道网络上。然而，将它们应用于城市环境带来独特的挑战：城市网络表现出更大的结构多样性，交通流量高度过度分散，存在许多零值，最佳的空间依赖性建模方法尚不清楚，传感器覆盖率通常非常稀疏。我们引入了Graph Neural Network for Urban Interpolation（GNNUI），一种新颖的城市交通流量估计方法。GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。除了模型之外，我们还引入了两个新的公开的大规模城市交通流量基准，涵盖了不同的交通方式：柏林和纽约市的Strava自行车数据和出租车数据。GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于最近的基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。例如，在Strava上，MAE仅从7.1增加到10.5，在出租车数据上从23.0增加到40.4，表明在现实世界城市环境中常见的数据稀缺情况下，GNNUI表现良好。我们还考察了图连接选择如何影响模型精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable street-level traffic volume data, covering multiple modes oftransportation, helps urban planning by informing decisions on infrastructureimprovements, traffic management, and public transportation. Yet, trafficsensors measuring traffic volume are typically scarcely located, due to theirhigh deployment and maintenance costs. To address this, interpolation methodscan estimate traffic volumes at unobserved locations using available data.Graph Neural Networks have shown strong performance in traffic volumeforecasting, particularly on highways and major arterial networks. Applyingthem to urban settings, however, presents unique challenges: urban networksexhibit greater structural diversity, traffic volumes are highly overdispersedwith many zeros, the best way to account for spatial dependencies remainsunclear, and sensor coverage is often very sparse. We introduce the GraphNeural Network for Urban Interpolation (GNNUI), a novel urban traffic volumeestimation approach. GNNUI employs a masking algorithm to learn interpolation,integrates node features to capture functional roles, and uses a loss functiontailored to zero-inflated traffic distributions. In addition to the model, weintroduce two new open, large-scale urban traffic volume benchmarks, coveringdifferent transportation modes: Strava cycling data from Berlin and New YorkCity taxi data. GNNUI outperforms recent, some graph-based, interpolationmethods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAErises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strongperformance under extreme data scarcity, common in real-world urban settings.We also examine how graph connectivity choices influence model accuracy.</description>
      <author>example@mail.com (Silke K. Kaiser, Filipe Rodrigues, Carlos Lima Azevedo, Lynn H. Kaack)</author>
      <guid isPermaLink="false">2505.06292v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Soft causal learning for generalized molecule property prediction: An environment perspective</title>
      <link>http://arxiv.org/abs/2505.06283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。&lt;h4&gt;背景&lt;/h4&gt;分子图学习在AI科学中日益重要，但现有方法在处理样本外分布样本时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，以充分建模分子环境并绕过不变子图，从而解决分子科学中的样本外分布挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 将化学理论融入图增长生成器以模仿扩展环境；2) 设计基于GIB的目标函数以将环境从整个图中分离出来；3) 引入基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。&lt;h4&gt;主要发现&lt;/h4&gt;1) 扩展的原子模式导致基于不变理性模型的失败；2) 发现的分子子图与对应属性之间的关联复杂，因果子结构无法完全解释标签；3) 环境和不变性之间的相互作用相互影响，难以建模。&lt;h4&gt;结论&lt;/h4&gt;实验表明，所提出的框架具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。在AI科学中，分子图学习已成为一个日益重要的主题，但现有方法在处理样本外分布样本时存在局限性。本文提出了一种框架，旨在通过充分建模分子环境并绕过不变子图来解决分子科学中的样本外分布挑战。具体来说，本文首先将化学理论融入图增长生成器以模仿扩展环境，然后设计了一种基于GIB的目标函数以将环境从整个图中分离出来，最后引入了一种基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。通过在七个数据集上进行的实验，包括模仿不同类型的样本外泛化场景，广泛的比较、消融实验以及可视化案例研究，证明了所提出框架的良好泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on molecule graphs has become an increasingly important topic in AIfor science, which takes full advantage of AI to facilitate scientificdiscovery. Existing solutions on modeling molecules utilize Graph NeuralNetworks (GNNs) to achieve representations but they mostly fail to adapt modelsto out-of-distribution (OOD) samples. Although recent advances on OOD-orientedgraph learning have discovered the invariant rationale on graphs, they stillignore three important issues, i.e., 1) the expanding atom patterns regardingenvironments on graphs lead to failures of invariant rationale based models, 2)the associations between discovered molecular subgraphs and correspondingproperties are complex where causal substructures cannot fully interpret thelabels. 3) the interactions between environments and invariances can influencewith each other thus are challenging to be modeled. To this end, we propose asoft causal learning framework, to tackle the unresolved OOD challenge inmolecular science, from the perspective of fully modeling the moleculeenvironments and bypassing the invariant subgraphs. Specifically, we firstincorporate chemistry theories into our graph growth generator to imitateexpaned environments, and then devise an GIB-based objective to disentangleenvironment from whole graphs and finally introduce a cross-attention basedsoft causal interaction, which allows dynamic interactions between environmentsand invariances. We perform experiments on seven datasets by imitatingdifferent kinds of OOD generalization scenarios. Extensive comparison, ablationexperiments as well as visualized case studies demonstrate well generalizationability of our proposal.</description>
      <author>example@mail.com (Limin Li, Kuo Yang, Wenjie Du, Pengkun Wang, Zhengyang Zhou, Yang Wang)</author>
      <guid isPermaLink="false">2505.06283v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Mogao的统一框架，该框架通过因果方法实现交错的多模态生成，并在架构设计上进行了多项关键技术改进。&lt;h4&gt;背景&lt;/h4&gt;尽管统一模型在图像理解和生成方面取得了显著进展，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，以实现交错的多模态生成，并提升多模态理解和文本到图像生成的性能。&lt;h4&gt;方法&lt;/h4&gt;Mogao通过以下技术改进实现其目标：深度融合设计、双重视觉编码器、交错旋转位置嵌入和多模态无分类器引导。此外，还引入了一种高效的大规模数据集训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，同时擅长生成高质量、连贯的交错输出。其在零样本图像编辑和组合生成方面的能力，使其成为实用的全模态基础模型。&lt;h4&gt;结论&lt;/h4&gt;Mogao为统一多模态系统的发展开辟了道路，并具有未来扩展的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>http://arxiv.org/abs/2505.02350v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用稀疏椭圆径向基函数网络逼近点云符号距离函数（SDF）的机器学习方法，实现了紧凑且精确的表面表示。&lt;h4&gt;背景&lt;/h4&gt;点云表面表示是计算机图形学和视觉领域的一个基本问题。&lt;h4&gt;目的&lt;/h4&gt;目的是通过尽可能少的椭圆径向基函数（ERBFs）精确地逼近点云的SDF，以实现SDF的稀疏表示。&lt;h4&gt;方法&lt;/h4&gt;方法包括：引入动态多目标优化策略平衡稀疏性和逼近精度；采用基于最近邻的数据结构提高计算效率；在CUDA上并行化每个核的计算；以及设计基于八叉树的细化策略进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为点云表面表示提供了一种高效且准确的新途径，其对应的可执行程序已公开。&lt;h4&gt;翻译&lt;/h4&gt;Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using a sparse ellipsoidal radial basis function network, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding executable program is publicly available at https://github.com/lianbobo/SE-RBFNet.git.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lianbobo/se-rbfnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud surface representation is a fundamental problem in computergraphics and vision. This paper presents a machine learning approach forapproximating the signed distance function (SDF) of a point cloud using asparse ellipsoidal radial basis function network, enabling a compact andaccurate surface representation. Given the SDF values defined on the gridpoints constructed from the point cloud, our method approximates the SDFaccurately with as few ellipsoidal radial basis functions (ERBFs) as possible,i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsityand approximation precision, a dynamic multi-objective optimization strategy isintroduced, which adaptively adds the regularization terms and jointlyoptimizes the weights, centers, shapes, and orientations of ERBFs. To improvecomputational efficiency, a nearest-neighbor-based data structure is employed,restricting function calculations to points near each Gaussian kernel center.The computations for each kernel are further parallelized on CUDA, whichsignificantly improves the optimization speed. Additionally, a hierarchicaloctree-based refinement strategy is designed for training. Specifically, theinitialization and optimization of network parameters are conducted usingcoarse grid points in the octree lattice structure. Subsequently, fine latticepoints are progressively incorporated to accelerate model convergence andenhance training efficiency. Extensive experiments on multiple benchmarkdatasets demonstrate that our method outperforms previous sparse representationapproaches in terms of accuracy, robustness, and computational efficiency. Thecorresponding executable program is publicly available athttps://github.com/lianbobo/SE-RBFNet.git.</description>
      <author>example@mail.com (Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen)</author>
      <guid isPermaLink="false">2505.02350v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法，在此基础上，开发创新计算方法以实现高性能的自动疼痛评估，并适用于真实临床环境。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论和现有自动疼痛评估方法出发，探讨了疼痛感知的影响因素。&lt;h4&gt;目的&lt;/h4&gt;主要目的是开发适用于不同场景的自动疼痛评估流程，并研究影响疼痛感知的显著因素。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法研究影响疼痛感知的人口统计学元素，设计、开发、提出并提供了适用于单模态和多模态配置的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中的研究展示了所提出方法的有效性，实现了最先进的结果，并为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;本文提出的自动疼痛评估方法在临床环境中具有应用潜力，并为人工智能领域的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract: This thesis initially aims to study the painassessment process from a clinical-theoretical perspective while exploring andexamining existing automatic approaches. Building on this foundation, theprimary objective of this Ph.D. project is to develop innovative computationalmethods for automatic pain assessment that achieve high performance and areapplicable in real clinical settings. A primary goal is to thoroughlyinvestigate and assess significant factors, including demographic elements thatimpact pain perception, as recognized in pain research, through a computationalstandpoint. Within the limits of the available data in this research area, ourgoal was to design, develop, propose, and offer automatic pain assessmentpipelines for unimodal and multimodal configurations that are applicable to thespecific requirements of different scenarios. The studies published in thisPh.D. thesis showcased the effectiveness of the proposed methods, achievingstate-of-the-art results. Additionally, they paved the way for exploring newapproaches in artificial intelligence, foundation models, and generativeartificial intelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
      <link>http://arxiv.org/abs/2505.06291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALFEE的新型混合Transformer架构，用于EEG信号表示学习，以解决现有EEG模型在信号噪声比、个体差异和跨范式差异方面的问题。&lt;h4&gt;背景&lt;/h4&gt;虽然基础模型在文本、图像和视频领域表现出色，但关键生物信号，尤其是脑电图（EEG），仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决EEG模型在信号处理和泛化方面的局限性，提出ALFEE框架。&lt;h4&gt;方法&lt;/h4&gt;ALFEE采用混合注意力机制，将通道特征聚合与时间动态建模分离，具有两个学习阶段：预训练和微调。预训练阶段优化任务预测、通道和时间掩码重建以及时间预测，而微调阶段使用特定任务的标记字典和交叉注意力层。&lt;h4&gt;主要发现&lt;/h4&gt;ALFEE在六个下游EEG任务上表现出优于现有模型的效果，经过25,000小时的预训练后，在多任务上提升了性能。&lt;h4&gt;结论&lt;/h4&gt;ALFEE框架为生物信号分析提供了一个可扩展的基础，其实现在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然基础模型在文本、图像和视频领域表现出色，但关键的生物信号，尤其是脑电图（EEG），仍然没有得到充分的探索。EEG由于其高时间分辨率、操作实用性和安全性，为神经科学研究提供了益处。然而，低信号噪声比、个体差异和跨范式差异阻碍了现有模型的泛化。现有的方法通常采用简化的策略，如使用单个损失函数或通道-时间联合表示模块，并且在预训练和评估任务之间存在领域差距，这损害了效率和适应性。为了解决这些局限性，我们提出了自适应大型基础模型用于EEG信号表示（ALFEE）框架，这是一种新颖的混合Transformer架构，具有两个学习阶段，用于鲁棒的EEG表示学习。ALFEE采用混合注意力，将通道特征聚合与时间动态建模分离，使具有可变通道配置的EEG表示鲁棒。通道编码器自适应地压缩可变通道信息，时间编码器捕获任务指导的演变，混合解码器在时间和频率域中重建信号。在预训练期间，ALFEE优化任务预测、通道和时间掩码重建以及时间预测，以增强多尺度和多通道表示。在微调期间，使用特定任务的标记字典和交叉注意力层进行全模型适应性，以提升跨多个任务的表现。经过25,000小时的预训练后，在六个下游EEG任务上的大量实验结果表明，ALFEE的性能优于现有模型。我们的ALFEE框架为生物信号分析建立了一个可扩展的基础，其实现在https://github.com/xw1216/ALFEE上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models excel in text, image, and video domains, the criticalbiological signals, particularly electroencephalography(EEG), remainunderexplored. EEG benefits neurological research with its high temporalresolution, operational practicality, and safety profile. However, lowsignal-to-noise ratio, inter-subject variability, and cross-paradigmdifferences hinder the generalization of current models. Existing methods oftenemploy simplified strategies, such as a single loss function or achannel-temporal joint representation module, and suffer from a domain gapbetween pretraining and evaluation tasks that compromises efficiency andadaptability. To address these limitations, we propose the Adaptive LargeFoundation model for EEG signal representation(ALFEE) framework, a novel hybridtransformer architecture with two learning stages for robust EEG representationlearning. ALFEE employs a hybrid attention that separates channel-wise featureaggregation from temporal dynamics modeling, enabling robust EEG representationwith variable channel configurations. A channel encoder adaptively compressesvariable channel information, a temporal encoder captures task-guidedevolution, and a hybrid decoder reconstructs signals in both temporal andfrequency domains. During pretraining, ALFEE optimizes task prediction, channeland temporal mask reconstruction, and temporal forecasting to enhancemulti-scale and multi-channel representation. During fine-tuning, a full-modeladaptation with a task-specific token dictionary and a cross-attention layerboosts performance across multiple tasks. After 25,000 hours of pretraining,extensive experimental results on six downstream EEG tasks demonstrate thesuperior performance of ALFEE over existing models. Our ALFEE frameworkestablishes a scalable foundation for biological signal analysis withimplementation at https://github.com/xw1216/ALFEE.</description>
      <author>example@mail.com (Wei Xiong, Junming Lin, Jiangtong Li, Jie Li, Changjun Jiang)</author>
      <guid isPermaLink="false">2505.06291v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation</title>
      <link>http://arxiv.org/abs/2505.06288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的等距沉浸核学习方法（IIKL），用于从离散的非欧几里得数据中构建黎曼流形并等距地诱导黎曼度量，以保持数据内在的几何和拓扑属性。&lt;h4&gt;背景&lt;/h4&gt;在科学应用中，保留离散非欧几里得数据的内在几何和拓扑属性对于几何表示学习至关重要。传统方法通常将非欧几里得离散数据映射到欧几里得空间，可能导致关键几何信息的丢失。&lt;h4&gt;目的&lt;/h4&gt;提出IIKL方法，以保持离散非欧几里得数据的几何结构，并提高下游任务（如数据重建和分类）的准确性。&lt;h4&gt;方法&lt;/h4&gt;IIKL方法等距沉浸黎曼流形，并通过最大似然估计（MLE）导出参数化学习模型和交替训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用IIKL学习到的黎曼流形及其度量，模型在3D和高维数据集中成功地保持了数据的内在几何表示，显著提高了下游任务的准确性，如数据重建和分类。与最先进的（SOTA）方法相比，该方法可以减少超过90%的内积不变损失，平均提高了40%的下游重建准确性，并在涉及等距和共形的几何度量方面减少了90%的错误。&lt;h4&gt;结论&lt;/h4&gt;IIKL方法在保持离散非欧几里得数据的几何结构方面表现出色，对于下游任务如数据重建和分类有显著的提升效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric representation learning in preserving the intrinsic geometric andtopological properties for discrete non-Euclidean data is crucial in scientificapplications. Previous research generally mapped non-Euclidean discrete datainto Euclidean space during representation learning, which may lead to the lossof some critical geometric information. In this paper, we propose a novelIsometric Immersion Kernel Learning (IIKL) method to build Riemannian manifoldand isometrically induce Riemannian metric from discrete non-Euclidean data. Weprove that Isometric immersion is equivalent to the kernel function in thetangent bundle on the manifold, which explicitly guarantees the invariance ofthe inner product between vectors in the arbitrary tangent space throughout thelearning process, thus maintaining the geometric structure of the originaldata. Moreover, a novel parameterized learning model based on IIKL isintroduced, and an alternating training method for this model is derived usingMaximum Likelihood Estimation (MLE), ensuring efficient convergence.Experimental results proved that using the learned Riemannian manifold and itsmetric, our model preserved the intrinsic geometric representation of data inboth 3D and high-dimensional datasets successfully, and significantly improvedthe accuracy of downstream tasks, such as data reconstruction andclassification. It is showed that our method could reduce the inner productinvariant loss by more than 90% compared to state-of-the-art (SOTA) methods,also achieved an average 40% improvement in downstream reconstruction accuracyand a 90% reduction in error for geometric metrics involving isometric andconformal.</description>
      <author>example@mail.com (Zihao Chen, Wenyong Wang, Jiachen Yang, Yu Xiang)</author>
      <guid isPermaLink="false">2505.06288v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Learning Dynamics in Unsupervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.06279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无监督强化学习（URL）智能体可解释性框架，旨在理解内在动机如何影响注意力、行为和表示学习。&lt;h4&gt;背景&lt;/h4&gt;分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。&lt;h4&gt;目的&lt;/h4&gt;理解智能体如何随时间感知和适应，并引入两个度量指标：注意力多样性和注意力变化率。&lt;h4&gt;方法&lt;/h4&gt;使用Grad-CAM、层级相关性传播（LRP）、探索指标和潜在空间聚类进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;好奇心驱动的智能体比外在动机的智能体表现出更广泛、更动态的注意力和探索行为。TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。&lt;h4&gt;结论&lt;/h4&gt;结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，该框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种针对无监督强化学习（URL）智能体的可解释性框架，旨在理解内在动机如何塑造注意力、行为和表示学习。我们分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。为了捕捉智能体如何随时间感知和适应，我们引入了两个指标：注意力多样性和注意力变化率。我们的研究结果表明，好奇心驱动的智能体比外在动机的智能体表现出更广泛的注意力范围和更动态的探索行为。其中，TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。我们的结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，所提出的框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an interpretability framework for unsupervised reinforcementlearning (URL) agents, aimed at understanding how intrinsic motivation shapesattention, behavior, and representation learning. We analyze five agents DQN,RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generatedenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),exploration metrics, and latent space clustering. To capture how agentsperceive and adapt over time, we introduce two metrics: attention diversity,which measures the spatial breadth of focus, and attention change rate, whichquantifies temporal shifts in attention. Our findings show thatcuriosity-driven agents display broader, more dynamic attention and exploratorybehavior than their extrinsically motivated counterparts. Among them,TransformerRND combines wide attention, high exploration coverage, and compact,structured latent representations. Our results highlight the influence ofarchitectural inductive biases and training signals on internal agent dynamics.Beyond reward-centric evaluation, the proposed framework offers diagnostictools to probe perception and abstraction in RL agents, enabling moreinterpretable and generalizable behavior.</description>
      <author>example@mail.com (Shashwat Pandey)</author>
      <guid isPermaLink="false">2505.06279v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
      <link>http://arxiv.org/abs/2504.17040v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DyMU是一种高效的、无需训练的框架，能够在保持高任务性能的同时动态减少视觉语言模型（VLMs）的计算负担。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理图像和文本时存在计算效率低的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态减少视觉语言模型计算负担的方法，同时保持高任务性能。&lt;h4&gt;方法&lt;/h4&gt;DyMU包含两个关键组件：动态标记合并（DToMe）和虚拟标记解合并（VTU）。DToMe通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，VTU则通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列。&lt;h4&gt;主要发现&lt;/h4&gt;DyMU可以在减少32%-85%的平均视觉标记数量的同时，在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。通过定性分析，DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。&lt;h4&gt;结论&lt;/h4&gt;DyMU是一种适用于大多数最先进VLM架构的有效解决方案，它通过动态适应标记压缩到图像内容，并在不进行额外微调的情况下运行，从而提高了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种高效的、无需训练的框架DyMU，该框架在保持高任务性能的同时动态减少了视觉语言模型（VLMs）的计算负担。我们的方法包含两个关键组件。首先，动态标记合并（DToMe）通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，解决了视觉转换器固定长度输出的固有低效问题。其次，虚拟标记解合并（VTU）通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与先前的方法不同，我们的方法动态地将标记压缩适应到图像内容，并且完全无需训练，使其适用于大多数最先进的VLM架构。在图像和视频理解任务上的大量实验表明，DyMU可以将平均视觉标记数量减少32%-85%，同时在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。此外，通过定性分析，我们证明了DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。项目页面：https://mikewangwzhl.github.io/dymu/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DyMU, an efficient, training-free framework that dynamicallyreduces the computational burden of vision-language models (VLMs) whilemaintaining high task performance. Our approach comprises two key components.First, Dynamic Token Merging (DToMe) reduces the number of visual tokenembeddings by merging similar tokens based on image complexity, addressing theinherent inefficiency of fixed-length outputs in vision transformers. Second,Virtual Token Unmerging (VTU) simulates the expected token sequence for largelanguage models (LLMs) by efficiently reconstructing the attention dynamics ofa full sequence, thus preserving the downstream performance without additionalfine-tuning. Unlike previous approaches, our method dynamically adapts tokencompression to the content of the image and operates completely training-free,making it readily applicable to most state-of-the-art VLM architectures.Extensive experiments on image and video understanding tasks demonstrate thatDyMU can reduce the average visual token count by 32%-85% while achievingcomparable performance to full-length models across diverse VLM architectures,including the recently popularized AnyRes-based visual encoders. Furthermore,through qualitative analyses, we demonstrate that DToMe effectively adaptstoken reduction based on image complexity and, unlike existing systems,provides users more control over computational costs. Project page:https://mikewangwzhl.github.io/dymu/.</description>
      <author>example@mail.com (Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu)</author>
      <guid isPermaLink="false">2504.17040v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction</title>
      <link>http://arxiv.org/abs/2505.04918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Joint Conferences on Artificial Intelligence (IJCAI  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PASSAT是一种新的深度学习模型，用于天气预测，它结合了物理和地球表面拓扑信息，以改进预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的深度学习模型在天气预测中存在忽视物理和地球表面拓扑的问题。&lt;h4&gt;目的&lt;/h4&gt;开发PASSAT模型，以解决现有模型的不足，提高天气预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;PASSAT模型将天气演变归因于两个关键因素：平流过程和地球-大气相互作用。它考虑地球表面的拓扑结构，并在球面上数值求解平流方程和纳维-斯托克斯方程。使用球面图神经网络来捕捉地球-大气相互作用，并生成初始速度场。&lt;h4&gt;主要发现&lt;/h4&gt;在5.625°分辨率的ERA5数据集中，PASSAT优于现有的深度学习模型和IFS T42数值天气预报模型。&lt;h4&gt;结论&lt;/h4&gt;PASSAT模型通过结合物理和拓扑信息，在天气预测方面取得了显著的改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度学习模型在天气预测中显示出巨大的潜力，但大多数模型忽略了底层天气演变的物理或地球表面的拓扑。鉴于这些缺点，我们开发了PASSAT，一种新的物理辅助和拓扑信息深度学习模型，用于天气预测。PASSAT将天气演变归因于两个关键因素：（i）可以由平流方程和纳维-斯托克斯方程表征的平流过程；（ii）难以建模和计算的地球-大气相互作用。PASSAT还考虑了地球表面的拓扑结构，而不仅仅是将其视为平面。在这些考虑的基础上，PASSAT在球面上数值求解平流方程和纳维-斯托克斯方程，利用球面图神经网络来捕捉地球-大气相互作用，并从同一球面图神经网络生成解决平流方程的关键初始速度场。在5.625°分辨率的ERA5数据集中，PASSAT优于基于深度学习的最先进的天气预测模型和IFS T42业务数值天气预报模型。代码和检查点可在https://github.com/Yumenomae/PASSAT_5p625上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yumenomae/passat_5p625&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although deep learning models have demonstrated remarkable potential inweather prediction, most of them overlook either the \textbf{physics} of theunderlying weather evolution or the \textbf{topology} of the Earth's surface.In light of these disadvantages, we develop PASSAT, a novel Physics-ASSistedAnd Topology-informed deep learning model for weather prediction. PASSATattributes the weather evolution to two key factors: (i) the advection processthat can be characterized by the advection equation and the Navier-Stokesequation; (ii) the Earth-atmosphere interaction that is difficult to both modeland calculate. PASSAT also takes the topology of the Earth's surface intoconsideration, other than simply treating it as a plane. With theseconsiderations, PASSAT numerically solves the advection equation and theNavier-Stokes equation on the spherical manifold, utilizes a spherical graphneural network to capture the Earth-atmosphere interaction, and generates theinitial velocity fields that are critical to solving the advection equationfrom the same spherical graph neural network. In the $5.625^\circ$-resolutionERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-basedweather prediction models and the operational numerical weather predictionmodel IFS T42. Code and checkpoint are available athttps://github.com/Yumenomae/PASSAT_5p625.</description>
      <author>example@mail.com (Jiaqi Zheng, Qing Ling, Yerong Feng)</author>
      <guid isPermaLink="false">2505.04918v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
  <item>
      <title>Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</title>
      <link>http://arxiv.org/abs/2505.06224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了模型表示评估的重要性，提出了超越下游探针的方法，并引入了一种标准化协议来量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;背景&lt;/h4&gt;下游探针是评估模型表示的主要方法，但在评估过程中忽略了等变性、不变性和解耦性等属性，这些属性对表示的可解释性、适应性和实用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;强调模型表示评估的重要性，并提出一种统一的评估框架，以量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种标准化协议，用于评估图像和语音领域不同模型、不同架构和预训练方法下的表示，并识别可控的可变因素。&lt;h4&gt;主要发现&lt;/h4&gt;发现具有相似下游性能的模型在信息量、等变性、不变性和解耦性等方面可能表现出显著差异，暗示其下游性能背后的机制功能不同。&lt;h4&gt;结论&lt;/h4&gt;提出新的研究方向，以理解和改进模型表示，并强调在评估模型表示时需要考虑更广泛的属性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Downstream probing has been the dominant method for evaluating modelrepresentations, an important process given the increasing prominence ofself-supervised learning and foundation models. However, downstream probingprimarily assesses the availability of task-relevant information in the model'slatent space, overlooking attributes such as equivariance, invariance, anddisentanglement, which contribute to the interpretability, adaptability, andutility of representations in real-world applications. While some attempts havebeen made to measure these qualities in representations, no unified evaluationframework with modular, generalizable, and interpretable metrics exists.  In this paper, we argue for the importance of representation evaluationbeyond downstream probing. We introduce a standardized protocol to quantifyinformativeness, equivariance, invariance, and disentanglement of factors ofvariation in model representations. We use it to evaluate representations froma variety of models in the image and speech domains using differentarchitectures and pretraining approaches on identified controllable factors ofvariation. We find that representations from models with similar downstreamperformance can behave substantially differently with regard to theseattributes. This hints that the respective mechanisms underlying theirdownstream performance are functionally different, prompting new researchdirections to understand and improve representations.</description>
      <author>example@mail.com (Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels)</author>
      <guid isPermaLink="false">2505.06224v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
      <link>http://arxiv.org/abs/2505.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度学习在淋巴结分割中的应用，并讨论了卷积神经网络、编码器-解码器网络和Transformer等不同深度学习架构在分析不同模态医学影像数据的方法。&lt;h4&gt;背景&lt;/h4&gt;自动淋巴结分割是计算机视觉任务中癌症早期检测和分期的基础。传统的分割方法受限于人工描绘和操作者技能的差异性，限制了其达到高准确性的能力。&lt;h4&gt;目的&lt;/h4&gt;评估深度学习在淋巴结分割中的应用，并探讨解决淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;方法&lt;/h4&gt;本文提供了对深度学习技术在淋巴结分割任务中应用的全面概述，并探索了包括多模态融合技术、迁移学习和使用大规模预训练模型等潜在的未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;尽管深度学习技术在淋巴结分割方面取得了进展，但仍然面临淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;结论&lt;/h4&gt;这是首次对深度学习技术在淋巴结分割任务中的应用进行综合概述的研究，并为未来的研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic lymph node segmentation is the cornerstone for advances in computervision tasks for early detection and staging of cancer. Traditionalsegmentation methods are constrained by manual delineation and variability inoperator proficiency, limiting their ability to achieve high accuracy. Theintroduction of deep learning technologies offers new possibilities forimproving the accuracy of lymph node image analysis. This study evaluates theapplication of deep learning in lymph node segmentation and discusses themethodologies of various deep learning architectures such as convolutionalneural networks, encoder-decoder networks, and transformers in analyzingmedical imaging data across different modalities. Despite the advancements, itstill confronts challenges like the shape diversity of lymph nodes, thescarcity of accurately labeled datasets, and the inadequate development ofmethods that are robust and generalizable across different imaging modalities.To the best of our knowledge, this is the first study that provides acomprehensive overview of the application of deep learning techniques in lymphnode segmentation task. Furthermore, this study also explores potential futureresearch directions, including multimodal fusion techniques, transfer learning,and the use of large-scale pre-trained models to overcome current limitationswhile enhancing cancer diagnosis and treatment planning strategies.</description>
      <author>example@mail.com (Jingguo Qu, Xinyang Han, Man-Lik Chui, Yao Pu, Simon Takadiyi Gunda, Ziman Chen, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying)</author>
      <guid isPermaLink="false">2505.06118v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</title>
      <link>http://arxiv.org/abs/2505.05851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted for presentation at the 7th Workshop on Long-term Human  Motion Prediction (LHMP) at International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在拥挤和遮挡频繁的环境中，应用超宽带（UWB）定位技术作为人类运动捕捉的可扩展替代方案的可能性。&lt;h4&gt;背景&lt;/h4&gt;随着机器人越来越多地融入人类环境，理解和预测人类运动对于安全高效的人机交互至关重要。目前，人类运动和活动预测方法需要高质量和数量的数据进行训练和评估，通常从运动捕捉系统、车载或固定传感器中收集。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种在更大和更复杂环境中（如仓库、机场或会议中心）评估传感方式（如UWB）的可扩展和准确的运动数据收集方法。&lt;h4&gt;方法&lt;/h4&gt;包括额外的传感方式，如眼动追踪、车载机器人激光雷达和雷达传感器，并记录运动捕捉数据作为评估和比较的基准。环境模拟博物馆设置，最多有四个参与者以自然的方式向随机目标移动，提供超过130分钟的多模态数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究为超越基于视觉的系统提供了可扩展和准确的运动数据收集的步骤。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为在更大和更复杂的环境中评估传感方式（如UWB）提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With robots increasingly integrating into human environments, understandingand predicting human motion is essential for safe and efficient interactions.Modern human motion and activity prediction approaches require high quality andquantity of data for training and evaluation, usually collected from motioncapture systems, onboard or stationary sensors. Setting up these systems ischallenging due to the intricate setup of hardware components, extensivecalibration procedures, occlusions, and substantial costs. These constraintsmake deploying such systems in new and large environments difficult and limittheir usability for in-the-wild measurements. In this paper we investigate thepossibility to apply the novel Ultra-Wideband (UWB) localization technology asa scalable alternative for human motion capture in crowded and occlusion-proneenvironments. We include additional sensing modalities such as eye-tracking,onboard robot LiDAR and radar sensors, and record motion capture data as groundtruth for evaluation and comparison. The environment imitates a museum setup,with up to four active participants navigating toward random goals in a naturalway, and offers more than 130 minutes of multi-modal data. Our investigationprovides a step toward scalable and accurate motion data collection beyondvision-based systems, laying a foundation for evaluating sensing modalitieslike UWB in larger and complex environments like warehouses, airports, orconvention centers.</description>
      <author>example@mail.com (Janik Kaden, Maximilian Hilger, Tim Schreiter, Marius Schaab, Thomas Graichen, Andrey Rudenko, Ulrich Heinkel, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2505.05851v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2505.06113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图，实现低成本且精确的环境感知。&lt;h4&gt;背景&lt;/h4&gt;传统的自动驾驶感知系统依赖昂贵的LiDAR传感器来生成精确的环境表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要LiDAR传感器的低成本感知框架，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;结合YOLOv1和DepthAnythingV2技术，通过多摄像头输入实现物体检测和单目深度估计，以实现全面的360度场景理解。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2和NuScenes数据集上评估，该方法在道路分割精度达到85%，车辆检测率在85-90%，平均位置误差限制在1.2米。&lt;h4&gt;结论&lt;/h4&gt;深度学习有潜力仅使用摄像头输入提取丰富的空间信息，从而实现低成本且不牺牲精度的自主导航。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动驾驶车辆的感知系统传统上依赖昂贵的LiDAR传感器来生成精确的环境表示。在本文中，我们提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图。我们的方法结合了基于YOLOv1的对象检测和DepthAnythingV2的单目深度估计，通过多摄像头输入实现全面的360度场景理解。我们在OpenLane-V2和NuScenes数据集上评估了我们的方法，与LiDAR地面真实值相比，实现了高达85%的道路分割精度和85-90%的车辆检测率，平均位置误差限制在1.2米。这些结果突出了深度学习仅使用摄像头输入提取丰富空间信息的潜力，使得低成本且不牺牲精度的自主导航成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicle perception systems have traditionally relied on costlyLiDAR sensors to generate precise environmental representations. In this paper,we propose a camera-only perception framework that produces Bird's Eye View(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combinesYOLOv11-based object detection with DepthAnythingV2 monocular depth estimationacross multi-camera inputs to achieve comprehensive 360-degree sceneunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenesdatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicledetection rates when compared against LiDAR ground truth, with averagepositional errors limited to 1.2 meters. These results highlight the potentialof deep learning to extract rich spatial information using only camera inputs,enabling cost-efficient autonomous navigation without sacrificing accuracy.</description>
      <author>example@mail.com (Anupkumar Bochare)</author>
      <guid isPermaLink="false">2505.06113v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</title>
      <link>http://arxiv.org/abs/2505.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LRW-OOD的图Out-Of-Distribution（OOD）泛化学习方法，旨在解决图神经网络在分布偏移下的性能退化问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在分布偏移的情况下表现不佳，现有的图OOD方法通常基于不变风险最小化和结构因果模型，但它们可能不符合现实情况。&lt;h4&gt;目的&lt;/h4&gt;提出可学习的随机游走（LRW）作为不变知识的实例，以实现图OOD泛化学习。&lt;h4&gt;方法&lt;/h4&gt;LRW-OOD使用可学习的随机游走采样器和路径编码器来参数化转移矩阵，并提出基于核密度估计（KDE）的互信息（MI）损失来生成符合OOD原则的随机游走序列。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该模型可以在各种类型的分布偏移下有效地增强图OOD泛化，并且相对于最先进的图OOD泛化基线，准确率提高了3.87%。&lt;h4&gt;结论&lt;/h4&gt;LRW-OOD是一种有效的图OOD泛化学习方法，能够显著提高模型在分布偏移情况下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-Of-Distribution (OOD) generalization has gained increasing attentions formachine learning on graphs, as graph neural networks (GNNs) often exhibitperformance degradation under distribution shifts. Existing graph OOD methodstend to follow the basic ideas of invariant risk minimization and structuralcausal models, interpreting the invariant knowledge across datasets undervarious distribution shifts as graph topology or graph spectrum. However, theseinterpretations may be inconsistent with real-world scenarios, as neitherinvariant topology nor spectrum is assured. In this paper, we advocate thelearnable random walk (LRW) perspective as the instantiation of invariantknowledge, and propose LRW-OOD to realize graph OOD generalization learning.Instead of employing fixed probability transition matrix (i.e.,degree-normalized adjacency matrix), we parameterize the transition matrix withan LRW-sampler and a path encoder. Furthermore, we propose the kernel densityestimation (KDE)-based mutual information (MI) loss to generate random walksequences that adhere to OOD principles. Extensive experiment demonstrates thatour model can effectively enhance graph OOD generalization under various typesof distribution shifts and yield a significant accuracy improvement of 3.87%over state-of-the-art graph OOD generalization baselines.</description>
      <author>example@mail.com (Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2505.05785v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Discovery of the Polar Ring Galaxies with deep learning</title>
      <link>http://arxiv.org/abs/2505.05890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 11 figures. The article is submitted to Astron. &amp; Astrophys&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在创建一个强和良好候选行星状星云（PRGs）的目录，开发一种基于机器学习方法的图像搜索技术，用于在大规模巡天中搜索和发现PRGs，并探索CIGALE软件在确定其多波段特性的能力。&lt;h4&gt;背景&lt;/h4&gt;本研究首次将深度学习方法应用于PRGs的搜索。&lt;h4&gt;目的&lt;/h4&gt;创建PRGs目录，开发基于机器学习的图像搜索方法，探索CIGALE软件在确定PRGs多波段特性的能力。&lt;h4&gt;方法&lt;/h4&gt;使用深度学习方法搜索PRGs，对现有PRGs目录中的星系进行视觉检查以创建训练样本，采用数据增强、图像分割和集成学习方法。使用转移学习技术通过GALFIT生成的合成图像扩大训练样本。&lt;h4&gt;主要发现&lt;/h4&gt;发现了三个新的PRGs，并在SDSS环状星系目录中发现了四个PRGs。使用CIGALE软件对其中一个发现的新PRG进行了研究，确定了其红外到紫外波段的谱能量分布，并估计了其恒星形成率（SFR）和总恒星质量。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法在发现新的PRGs方面是有效的，且CIGALE软件能够确定PRGs的多波段特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of our research is to create a catalog of strong and good candidatesfor PRGs using existing catalogs of PRGs, develop an image-based approach withmachine learning methods for the search and discovery of PRGs in a big skysurvey, and explore the capability of the CIGALE software for determining theirmultiwavelength properties. For the first time, we applied a deep learningmethod to the search for PRGs. We visually inspected galaxies from existingcatalogs of PRGs to create a training sample based on high-quality SDSS images.Since the resulting training sample was extremely small (87 strong and goodPRGs), we applied augmentation, image segmentation, and ensemble learningtechniques. However, most effective method was transfer learning with itsability to enlarge the training sample by synthetic images generated by GALFIT.To examine deep learning approach for finding new PRGs we used the SDSS catalogof galaxies at z &lt; 0.1. The method with synthetic images showed that even withovertraining we were able to find galaxies with a ring pattern. Our deeplearning approach has resulted in the discovery of three PRGs (SDSSJ140644.42+471602.0; SDSS J133650.48+492745.3; SDSS J095717.30+364953.5). Also,we visually inspected the Catalog of the SDSS Ring galaxies at z &lt; 0.1 anddiscovered four PRGs among ~2,200 ring galaxies (SDSS J095851.32+320422.9; SDSSJ104211.05+234448.2; SDSS J162212.63+272032.2; SDSS J104600.10+090627.2). Oneof the discovered galaxies with transfer learning, SDSS J140644.42+471602.0,was studied with CIGALE software to determine its spectral energy distributionin IR-UV bands. The current SFR is 71 M_sun per year, although the lack of FUVdata limits this estimate. The total stellar mass is 8.34x10^{10} M_sun. Thepredominance of an old stellar population (two-thirds of the total mass)suggests that this PRG is undergoing interaction process.</description>
      <author>example@mail.com (D. V. Dobrycheva, O. O. Hetmantsev, I. B. Vavilova, A. Shportko, O. Gugnin, O. V. Kompaniiets)</author>
      <guid isPermaLink="false">2505.05890v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
      <link>http://arxiv.org/abs/2505.05845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于机器学习的木材节子检测和配对自动化流程，提高了木材加工中的效率。&lt;h4&gt;背景&lt;/h4&gt;木材节子对木材的美观和结构完整性至关重要，传统手工标注节子费时费力。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级且全自动的节子检测和配对方法。&lt;h4&gt;方法&lt;/h4&gt;利用工业级相机采集木质板材的高分辨率表面图像，并手动标注和预处理大规模数据集。使用YOLOv8进行节子检测，采用三重神经网络进行特征映射和配对。&lt;h4&gt;主要发现&lt;/h4&gt;YOLOv8在检测阶段达到mAP@0.5的0.887，三重神经网络在配对阶段达到0.85的配对准确率。分析表明，节子起点和终点到底部的距离以及纵向坐标在提高配对准确性中起关键作用。&lt;h4&gt;结论&lt;/h4&gt;该方法有效验证了人工智能在推进木材科学和工业中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knots in wood are critical to both aesthetics and structural integrity,making their detection and pairing essential in timber processing. However,traditional manual annotation was labor-intensive and inefficient,necessitating automation. This paper proposes a lightweight and fully automatedpipeline for knot detection and pairing based on machine learning techniques.In the detection stage, high-resolution surface images of wooden boards werecollected using industrial-grade cameras, and a large-scale dataset wasmanually annotated and preprocessed. After the transfer learning, the YOLOv8lachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots wereanalyzed and paired based on multidimensional feature extraction. A tripletneural network was used to map the features into a latent space, enablingclustering algorithms to identify and pair corresponding knots. The tripletnetwork with learnable weights achieved a pairing accuracy of 0.85. Furtheranalysis revealed that he distances from the knot's start and end points to thebottom of the wooden board, and the longitudinal coordinates play crucial rolesin achieving high pairing accuracy. Our experiments validate the effectivenessof the proposed solution, demonstrating the potential of AI in advancing woodscience and industry.</description>
      <author>example@mail.com (Guohao Lin, Shidong Pan, Rasul Khanbayov, Changxi Yang, Ani Khaloian-Sarnaghi, Andriy Kovryga)</author>
      <guid isPermaLink="false">2505.05845v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
      <link>http://arxiv.org/abs/2505.06184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at MisD @ AAAI ICWSM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLM）的社交媒体用户画像方法，旨在解决现有画像技术的局限性，如可迁移性差、特征不可解释、需要大量标注数据集或依赖刚性预定义类别等。&lt;h4&gt;背景&lt;/h4&gt;社交媒体用户画像对于虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应不同领域、减少对大量标注数据集依赖、生成可解释的用户画像的新方法。&lt;h4&gt;方法&lt;/h4&gt;该方法采用两阶段方法：首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在创建灵活、适应性强和可解释的用户画像方面显著优于现有方法，提高了9.8%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过利用LLM的内在知识，在最小化人工验证的同时，实现了跨领域的适应性，并有效利用LLM的推理和知识能力，为下游社交网络任务提供支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：通过内容分析进行社交媒体用户画像对于诸如虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。然而，现有的画像技术，包括推文摘要、基于属性的画像和潜在表示学习，面临着重大局限性：它们通常缺乏可迁移性，产生不可解释的特征，需要大量标注数据集，或者依赖于限制适应性的刚性预定义类别。我们提出了一种基于大型语言模型（LLM）的新方法，该方法利用领域定义语句，这些语句作为领域关键特征的轮廓，构成了领域画像的基础。我们的两阶段方法首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。通过利用LLM的内在知识以及最小的人工验证，我们的方法在减少对大量标注数据集需求的同时，实现了跨领域的适应性。我们的方法生成可解释的自然语言用户画像，将大量的用户数据压缩到可以解锁LLM推理和知识能力的规模，从而为下游社交网络任务提供支持。我们贡献了一个波斯政治Twitter（X）数据集和一个基于LLM的评估框架，并进行了人工验证。实验结果表明，我们的方法在创建灵活、适应性强和可解释的用户画像方面显著优于最先进的LLM和传统方法，提高了9.8%，证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social media user profiling through content analysis is crucial for taskslike misinformation detection, engagement prediction, hate speech monitoring,and user behavior modeling. However, existing profiling techniques, includingtweet summarization, attribute-based profiling, and latent representationlearning, face significant limitations: they often lack transferability,produce non-interpretable features, require large labeled datasets, or rely onrigid predefined categories that limit adaptability. We introduce a novel largelanguage model (LLM)-based approach that leverages domain-defining statements,which serve as key characteristics outlining the important pillars of a domainas foundations for profiling. Our two-stage method first employssemi-supervised filtering with a domain-specific knowledge base, then generatesboth abstractive (synthesized descriptions) and extractive (representativetweet selections) user profiles. By harnessing LLMs' inherent knowledge withminimal human validation, our approach is adaptable across domains whilereducing the need for large labeled datasets. Our method generatesinterpretable natural language user profiles, condensing extensive user datainto a scale that unlocks LLMs' reasoning and knowledge capabilities fordownstream social network tasks. We contribute a Persian political Twitter (X)dataset and an LLM-based evaluation framework with human validation.Experimental results show our method significantly outperforms state-of-the-artLLM-based and traditional methods by 9.8%, demonstrating its effectiveness increating flexible, adaptable, and interpretable user profiles.</description>
      <author>example@mail.com (Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour)</author>
      <guid isPermaLink="false">2505.06184v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
      <link>http://arxiv.org/abs/2505.05752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于点云数据的几何测量和合规性评估自动化框架，通过深度学习和信号处理技术自动化基础设施测绘任务。&lt;h4&gt;背景&lt;/h4&gt;自动化在提高基础设施测绘的效率、准确性和可扩展性方面可以发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，用于利用点云数据自动评估道路边缘坡道是否符合美国残疾人法案（ADA）的要求。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了基于深度学习的检测和分割技术，以及几何和信号处理技术，并使用了一个新收集的大规模标注数据集进行模型训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在自动化测绘任务中具有较高的准确性和可靠性，可以显著减少手动工作并提高基础设施评估的一致性。&lt;h4&gt;结论&lt;/h4&gt;该框架为基础设施测绘和自动施工评估提供了基础，并促进了点云数据在这些领域的更广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动化在提高基础设施测绘的效率、准确性及可扩展性方面扮演着重要角色。本文提出了一种基于点云数据的几何测量和合规性评估自动化框架。该框架集成了深度学习检测和分割技术与几何和信号处理技术，以自动化测绘任务。作为一种概念验证，我们应用该框架自动评估道路边缘坡道是否符合美国残疾人法案（ADA），展示了点云数据在测绘自动化中的实用性。该方法利用了一个新收集的大型标注数据集，作为本工作的一个部分公开，以促进稳健的模型训练和评估。包括与多个坡道的手动现场测量的比较在内的实验结果，验证了所提方法的准确性和可靠性，突出了其在显著减少手动工作并提高基础设施评估一致性方面的潜力。除了ADA合规性之外，该框架为基础设施测绘和自动施工评估的更广泛应用奠定了基础，并促进了点云数据在这些领域的更广泛应用。标注数据库、手动坡道调查数据和开发算法在项目的GitHub页面上公开：https://github.com/Soltanilara/SurveyAutomation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/soltanilara/surveyautomation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automation can play a prominent role in improving efficiency, accuracy, andscalability in infrastructure surveying and assessing construction andcompliance standards. This paper presents a framework for automation ofgeometric measurements and compliance assessment using point cloud data. Theproposed approach integrates deep learning-based detection and segmentation, inconjunction with geometric and signal processing techniques, to automatesurveying tasks. As a proof of concept, we apply this framework toautomatically evaluate the compliance of curb ramps with the Americans withDisabilities Act (ADA), demonstrating the utility of point cloud data in surveyautomation. The method leverages a newly collected, large annotated dataset ofcurb ramps, made publicly available as part of this work, to facilitate robustmodel training and evaluation. Experimental results, including comparison withmanual field measurements of several ramps, validate the accuracy andreliability of the proposed method, highlighting its potential to significantlyreduce manual effort and improve consistency in infrastructure assessment.Beyond ADA compliance, the proposed framework lays the groundwork for broaderapplications in infrastructure surveying and automated construction evaluation,promoting wider adoption of point cloud data in these domains. The annotateddatabase, manual ramp survey data, and developed algorithms are publiclyavailable on the project's GitHub page:https://github.com/Soltanilara/SurveyAutomation.</description>
      <author>example@mail.com (Amin Ghafourian, Andrew Lee, Dechen Gao, Tyler Beer, Kin Yen, Iman Soltani)</author>
      <guid isPermaLink="false">2505.05752v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2505.06217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新框架，用于将Segment Anything Model (SAM) 应用于医学图像分类。&lt;h4&gt;背景&lt;/h4&gt;SAM在图像分割任务中表现出色，但其适应医学图像分类的研究还较少。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，以适应SAM进行医学图像分类。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器作为特征提取器，捕获图像的分割特征，并提出了一种新的空间局部化通道注意力（SLCA）机制来计算特征图的空间局部化注意力权重，从而增强分类模型对图像中空间相关或有意义区域的关注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开的医学图像分类数据集上的实验结果表明，该方法有效且数据效率高。&lt;h4&gt;结论&lt;/h4&gt;该方法在医学图像分类中取得了良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models, such as the Segment Anything Model(SAM), have shown strong performance in various vision tasks, particularlyimage segmentation, due to their impressive zero-shot segmentationcapabilities. However, effectively adapting such models for medical imageclassification is still a less explored topic. In this paper, we introduce anew framework to adapt SAM for medical image classification. First, we utilizethe SAM image encoder as a feature extractor to capture segmentation-basedfeatures that convey important spatial and contextual details of the image,while freezing its weights to avoid unnecessary overhead during training. Next,we propose a novel Spatially Localized Channel Attention (SLCA) mechanism tocompute spatially localized attention weights for the feature maps. Thefeatures extracted from SAM's image encoder are processed through SLCA tocompute attention weights, which are then integrated into deep learningclassification models to enhance their focus on spatially relevant ormeaningful regions of the image, thus improving classification performance.Experimental results on three public medical image classification datasetsdemonstrate the effectiveness and data-efficiency of our approach.</description>
      <author>example@mail.com (Pengfei Gu, Haoteng Tang, Islam A. Ebeid, Jose A. Nunez, Fabian Vazquez, Diego Adame, Marcus Zhan, Huimin Li, Bin Fu, Danny Z. Chen)</author>
      <guid isPermaLink="false">2505.06217v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
      <link>http://arxiv.org/abs/2505.05710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于Transformer的HyperspectralMAE模型，用于高光谱数据，通过双掩码策略和波长感知嵌入，提高了高光谱图像的重建和分析能力。&lt;h4&gt;背景&lt;/h4&gt;高光谱图像在空间和光谱维度上具有高维度，这给数据处理带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理高光谱数据的模型，并提高其重建和分析性能。&lt;h4&gt;方法&lt;/h4&gt;HyperspectralMAE模型采用双掩码策略，在预训练过程中随机遮挡50%的空间块和50%的光谱带，并引入基于波长的可学习谐波傅里叶位置嵌入来编码光谱顺序。重建目标结合均方误差（MSE）和光谱角映射（SAM）来平衡像素级精度和光谱形状保真度。&lt;h4&gt;主要发现&lt;/h4&gt;HyperspectralMAE在Indian Pines数据集上实现了最先进的迁移学习准确率，证明了掩码双维度预训练能够产生鲁棒的光谱-空间表示。&lt;h4&gt;结论&lt;/h4&gt;双掩码和波长感知嵌入技术能够提升高光谱图像的重建和下游分析能力。&lt;h4&gt;翻译&lt;/h4&gt;Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose HyperspectralMAE, a Transformer-based foundation model for hyperspectral data that employs a dual masking strategy: during pre-training we randomly occlude 50% of spatial patches and 50% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity. The resulting model contains about 1.8×10^8 parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion (~1,600 scenes, ~3×10^11 pixel spectra) and DLR EnMAP Level-0 (~1,300 scenes, ~3×10^11 pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imagery provides rich spectral detail but poses uniquechallenges because of its high dimensionality in both spatial and spectraldomains. We propose \textit{HyperspectralMAE}, a Transformer-based foundationmodel for hyperspectral data that employs a \textit{dual masking} strategy:during pre-training we randomly occlude 50\% of spatial patches and 50\% ofspectral bands. This forces the model to learn representations capable ofreconstructing missing information across both dimensions. To encode spectralorder, we introduce learnable harmonic Fourier positional embeddings based onwavelength. The reconstruction objective combines mean-squared error (MSE) withthe spectral angle mapper (SAM) to balance pixel-level accuracy andspectral-shape fidelity.  The resulting model contains about $1.8\times10^{8}$ parameters and produces768-dimensional embeddings, giving it sufficient capacity for transferlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixelspectra) -- and fine-tuned it for land-cover classification on the Indian Pinesbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learningaccuracy on Indian Pines, confirming that masked dual-dimensional pre-trainingyields robust spectral-spatial representations. These results demonstrate thatdual masking and wavelength-aware embeddings advance hyperspectral imagereconstruction and downstream analysis.</description>
      <author>example@mail.com (Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim)</author>
      <guid isPermaLink="false">2505.05710v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2505.05587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025, Project page: https://vita-group.github.io/SteepGS/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;3D Gaussian Splatting (3DGS) 是一种实时、高分辨率的新视角合成技术。为了优化场景覆盖和捕捉细节，3DGS 使用密化算法生成额外的点，但往往导致冗余点云，增加了内存使用、性能下降和存储需求。本文提出了一种理论框架，通过优化密化控制来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;3DGS 通过将场景表示为高斯原语混合体，利用 GPU 光栅化管道进行高效渲染和重建。&lt;h4&gt;目的&lt;/h4&gt;优化场景覆盖和捕捉细节，同时减少冗余点云，提高资源受限设备的部署效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种理论框架，分析密化过程，通过优化理论方法确定必要的条件，最小化后代的数量，确定最优的参数更新方向，并提供后代不透明度的解析解。基于这些发现，引入了 SteepGS，这是一种最陡密度控制策略，旨在最小化损失并保持紧凑的点云。&lt;h4&gt;主要发现&lt;/h4&gt;分析表明，分割对于逃离鞍点至关重要。通过优化理论方法，确定了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并为后代不透明度的标准化提供了解析解。&lt;h4&gt;结论&lt;/h4&gt;SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯喷射（3DGS）已经成为实时、高分辨率新视角合成的强大技术。通过将场景表示为高斯原语混合体，3DGS 利用 GPU 光栅化管道进行高效渲染和重建。为了优化场景覆盖和捕捉细节，3DGS 采用密化算法生成额外的点。然而，这个过程通常会导致冗余点云，从而增加内存使用、降低性能和大量存储需求，这对资源受限设备的部署构成了重大挑战。为了解决这一限制，我们提出了一种理论框架，揭示了 3DGS 中密度控制背后的原理并提高了其效率。我们的分析表明，分割对于逃离鞍点至关重要。通过一种优化理论方法，我们建立了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并提供了后代不透明度标准化的解析解。基于这些见解，我们引入了 SteepGS，这是一种采用最陡密度控制的策略，旨在最小化损失的同时保持紧凑的点云。SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a powerful technique forreal-time, high-resolution novel view synthesis. By representing scenes as amixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines forefficient rendering and reconstruction. To optimize scene coverage and capturefine details, 3DGS employs a densification algorithm to generate additionalpoints. However, this process often leads to redundant point clouds, resultingin excessive memory usage, slower performance, and substantial storage demands- posing significant challenges for deployment on resource-constrained devices.To address this limitation, we propose a theoretical framework that demystifiesand improves density control in 3DGS. Our analysis reveals that splitting iscrucial for escaping saddle points. Through an optimization-theoretic approach,we establish the necessary conditions for densification, determine the minimalnumber of offspring Gaussians, identify the optimal parameter update direction,and provide an analytical solution for normalizing off-spring opacity. Buildingon these insights, we introduce SteepGS, incorporating steepest densitycontrol, a principled strategy that minimizes loss while maintaining a compactpoint cloud. SteepGS achieves a ~50% reduction in Gaussian points withoutcompromising rendering quality, significantly enhancing both efficiency andscalability.</description>
      <author>example@mail.com (Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan)</author>
      <guid isPermaLink="false">2505.05587v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
      <link>http://arxiv.org/abs/2505.06145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自适应微调、对比学习和正则化优化的策略，以提升基于Transformer模型的文本分类性能，并在FewRel 2.0数据集上进行了实验。&lt;h4&gt;背景&lt;/h4&gt;Few-shot文本分类在低资源环境中具有重要的应用价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种策略来提升Transformer模型在少样本情况下的分类性能。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调、对比学习和正则化优化等方法。&lt;h4&gt;主要发现&lt;/h4&gt;T5-small、DeBERTa-v3和RoBERTa-base在少样本任务中表现良好，特别是5-shot设置可以更有效地捕捉文本特征并提高分类准确率。不同关系类别之间的分类难度存在显著差异，引入对比损失和正则化损失可以增强模型的泛化能力，有效缓解少样本环境中的过拟合问题。&lt;h4&gt;结论&lt;/h4&gt;使用Transformer模型或具有更强自注意力机制的生成架构可以提高少样本分类的稳定性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot text classification has important application value in low-resourceenvironments. This paper proposes a strategy that combines adaptivefine-tuning, contrastive learning, and regularization optimization to improvethe classification performance of Transformer-based models. Experiments on theFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base performwell in few-shot tasks, especially in the 5-shot setting, which can moreeffectively capture text features and improve classification accuracy. Theexperiment also found that there are significant differences in theclassification difficulty of different relationship categories. Some categorieshave fuzzy semantic boundaries or complex feature distributions, making itdifficult for the standard cross entropy loss to learn the discriminativeinformation required to distinguish categories. By introducing contrastive lossand regularization loss, the generalization ability of the model is enhanced,effectively alleviating the overfitting problem in few-shot environments. Inaddition, the research results show that the use of Transformer models orgenerative architectures with stronger self-attention mechanisms can helpimprove the stability and accuracy of few-shot classification.</description>
      <author>example@mail.com (Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, Junliang Du)</author>
      <guid isPermaLink="false">2505.06145v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title>
      <link>http://arxiv.org/abs/2505.05638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统地评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了模型参数减少对驾驶性能的影响。&lt;h4&gt;背景&lt;/h4&gt;近年来，由于运动预测竞赛和基准的推动，基于学习的预测模型规模越来越大，参数数量可达数百万，旨在通过厘米级的精度提高开环预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;评估这些模型在集成到自动驾驶堆栈后是否能够提高性能。&lt;h4&gt;方法&lt;/h4&gt;通过实验评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了参数减少的模型在闭环驾驶性能上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;高开环准确性并不总是与更好的闭环驾驶行为相关，预测的时间一致性和规划器的兼容性等其他因素也起着关键作用。减少参数的模型在某些情况下表现出可比较甚至更优的闭环驾驶性能。&lt;h4&gt;结论&lt;/h4&gt;高开环准确性与闭环驾驶性能之间没有必然联系，模型参数的减少可能不会影响驾驶性能。&lt;h4&gt;翻译&lt;/h4&gt;Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/continental/pred2plan.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fueled by motion prediction competitions and benchmarks, recent years haveseen the emergence of increasingly large learning based prediction models, manywith millions of parameters, focused on improving open-loop prediction accuracyby mere centimeters. However, these benchmarks fail to assess whether suchimprovements translate to better performance when integrated into an autonomousdriving stack. In this work, we systematically evaluate the interplay betweenstate-of-the-art motion predictors and motion planners. Our results show thathigher open-loop accuracy does not always correlate with better closed-loopdriving behavior and that other factors, such as temporal consistency ofpredictions and planner compatibility, also play a critical role. Furthermore,we investigate downsized variants of these models, and, surprisingly, find thatin some cases models with up to 86% fewer parameters yield comparable or evensuperior closed-loop driving performance. Our code is available athttps://github.com/continental/pred2plan.</description>
      <author>example@mail.com (Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel Göhring, Jörg Reichardt)</author>
      <guid isPermaLink="false">2505.05638v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Fourier Features for Transfer Learning of Interatomic Potentials</title>
      <link>http://arxiv.org/abs/2505.05652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了franken，一个可扩展且轻量级的迁移学习框架，用于训练机器学习原子间势，旨在提高计算效率和数据效率。&lt;h4&gt;背景&lt;/h4&gt;在原子模拟中，训练既计算高效又数据高效的机器学习原子间势是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出franken框架，以实现原子间势的快速和准确适应，无需调整超参数或修改架构。&lt;h4&gt;方法&lt;/h4&gt;franken从预训练的图神经网络中提取原子描述符，并使用随机傅里叶特征将它们转移到新系统中。&lt;h4&gt;主要发现&lt;/h4&gt;在27种过渡金属的基准数据集上，franken在训练时间和准确性方面优于优化的核方法，将模型训练时间从数十小时缩短到几分钟。&lt;h4&gt;结论&lt;/h4&gt;franken框架展示了强大的数据效率，能够使用仅数十个训练结构稳定且准确地训练大量水以及Pt(111)/水界面的原子间势。&lt;h4&gt;翻译&lt;/h4&gt;Training machine learning interatomic potentials that are both computationally and data-efficient is a key challenge for enabling their routine use in atomistic simulations. To this effect, we introduce franken, a scalable and lightweight transfer learning framework that extracts atomic descriptors from pretrained graph neural networks and transfer them to new systems using random Fourier features-an efficient and scalable approximation of kernel methods. Franken enables fast and accurate adaptation of general-purpose potentials to new systems or levels of quantum mechanical theory without requiring hyperparameter tuning or architectural modifications. On a benchmark dataset of 27 transition metals, franken outperforms optimized kernel-based methods in both training time and accuracy, reducing model training from tens of hours to minutes on a single GPU. We further demonstrate the framework's strong data-efficiency by training stable and accurate potentials for bulk water and the Pt(111)/water interface using just tens of training structures. Our open-source implementation (https://franken.readthedocs.io) offers a fast and practical solution for training potentials and deploying them for molecular dynamics simulations across diverse systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training machine learning interatomic potentials that are bothcomputationally and data-efficient is a key challenge for enabling theirroutine use in atomistic simulations. To this effect, we introduce franken, ascalable and lightweight transfer learning framework that extracts atomicdescriptors from pretrained graph neural networks and transfer them to newsystems using random Fourier features-an efficient and scalable approximationof kernel methods. Franken enables fast and accurate adaptation ofgeneral-purpose potentials to new systems or levels of quantum mechanicaltheory without requiring hyperparameter tuning or architectural modifications.On a benchmark dataset of 27 transition metals, franken outperforms optimizedkernel-based methods in both training time and accuracy, reducing modeltraining from tens of hours to minutes on a single GPU. We further demonstratethe framework's strong data-efficiency by training stable and accuratepotentials for bulk water and the Pt(111)/water interface using just tens oftraining structures. Our open-source implementation(https://franken.readthedocs.io) offers a fast and practical solution fortraining potentials and deploying them for molecular dynamics simulationsacross diverse systems.</description>
      <author>example@mail.com (Pietro Novelli, Giacomo Meanti, Pedro J. Buigues, Lorenzo Rosasco, Michele Parrinello, Massimiliano Pontil, Luigi Bonati)</author>
      <guid isPermaLink="false">2505.05652v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在处理图数据时的挑战，提出了一种新的框架RELGCL，以解决传统方法在处理离散、非欧几里得性质图数据时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的GCL方法在保持增强视图之间的绝对相似性方面取得了成功，但在处理图数据时面临挑战，因为图数据具有离散和非欧几里得性质，导致视图生成破坏语义有效性和相似性验证的不可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，能够有效地利用图数据中的自然相对相似性模式。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个现实世界的图，发现了一个超越同质性和异质性的普遍模式：随着结构距离的增加，标签一致性系统性地下降，在亲社会性图中表现为平滑衰减，在异质图中表现为振荡衰减。通过随机游走理论建立了对这个模式的保证，证明了标签分布收敛并描述了不同衰减行为背后的机制。&lt;h4&gt;主要发现&lt;/h4&gt;图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;提出的RELGCL框架在亲社会性和异质图中均优于20种现有方法，验证了利用自然相对相似性而非人工绝对相似性的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks</title>
      <link>http://arxiv.org/abs/2505.05989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注异构信息网络中的路径建模问题，并提出了一个多跳路径感知推荐框架。&lt;h4&gt;背景&lt;/h4&gt;研究背景是异构信息网络中的路径建模问题。&lt;h4&gt;目的&lt;/h4&gt;目的是提出一个能够有效捕获高阶交互语义的多跳路径感知推荐框架。&lt;h4&gt;方法&lt;/h4&gt;方法包括三个阶段：路径选择、语义表示和基于注意力的融合。路径选择阶段引入路径过滤机制去除冗余和噪声信息；表示学习阶段使用序列建模结构联合编码实体和关系；融合阶段使用注意力机制为每条路径分配不同权重以生成全局用户兴趣表示。&lt;h4&gt;主要发现&lt;/h4&gt;在亚马逊-图书等真实数据集上的实验表明，该方法在HR@10、Recall@10和Precision@10等多个评估指标上显著优于现有推荐模型。&lt;h4&gt;结论&lt;/h4&gt;结果表明，多跳路径在捕获高阶交互语义方面是有效的，并且该框架在异构推荐场景中具有强大的建模能力。该方法通过在异构网络中整合结构信息建模与推荐算法设计，提供了理论上的价值和实际应用的价值，为在复杂数据环境中学习用户偏好提供了一种更具有表达性和灵活性的范式。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the problem of path modeling in heterogeneous information networks and proposes a multi-hop path-aware recommendation framework. The method centers on multi-hop paths composed of various types of entities and relations. It models user preferences through three stages: path selection, semantic representation, and attention-based fusion. In the path selection stage, a path filtering mechanism is introduced to remove redundant and noisy information. In the representation learning stage, a sequential modeling structure is used to jointly encode entities and relations, preserving the semantic dependencies within paths. In the fusion stage, an attention mechanism assigns different weights to each path to generate a global user interest representation. Experiments conducted on real-world datasets such as Amazon-Book show that the proposed method significantly outperforms existing recommendation models across multiple evaluation metrics, including HR@10, Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop paths in capturing high-order interaction semantics and demonstrate the expressive modeling capabilities of the framework in heterogeneous recommendation scenarios. This method provides both theoretical and practical value by integrating structural information modeling in heterogeneous networks with recommendation algorithm design. It offers a more expressive and flexible paradigm for learning user preferences in complex data environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the problem of path modeling in heterogeneousinformation networks and proposes a multi-hop path-aware recommendationframework. The method centers on multi-hop paths composed of various types ofentities and relations. It models user preferences through three stages: pathselection, semantic representation, and attention-based fusion. In the pathselection stage, a path filtering mechanism is introduced to remove redundantand noisy information. In the representation learning stage, a sequentialmodeling structure is used to jointly encode entities and relations, preservingthe semantic dependencies within paths. In the fusion stage, an attentionmechanism assigns different weights to each path to generate a global userinterest representation. Experiments conducted on real-world datasets such asAmazon-Book show that the proposed method significantly outperforms existingrecommendation models across multiple evaluation metrics, including HR@10,Recall@10, and Precision@10. The results confirm the effectiveness of multi-hoppaths in capturing high-order interaction semantics and demonstrate theexpressive modeling capabilities of the framework in heterogeneousrecommendation scenarios. This method provides both theoretical and practicalvalue by integrating structural information modeling in heterogeneous networkswith recommendation algorithm design. It offers a more expressive and flexibleparadigm for learning user preferences in complex data environments.</description>
      <author>example@mail.com (Hongye Zheng, Yue Xing, Lipeng Zhu, Xu Han, Junliang Du, Wanyu Cui)</author>
      <guid isPermaLink="false">2505.05989v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
      <link>http://arxiv.org/abs/2505.05736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MINT是一种通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐的框架，用于提高预训练大型语言模型在生物医学任务中的微调能力。&lt;h4&gt;背景&lt;/h4&gt;高质量的多模态生物医学数据稀缺，限制了预训练大型语言模型在特定生物医学任务中的微调效果。&lt;h4&gt;目的&lt;/h4&gt;提出MINT框架，以解决高质量多模态生物医学数据稀缺的问题，提高预训练大型语言模型在生物医学任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;MINT通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐，使用Odds Ratio Preference Optimization (ORPO)框架作为其核心优化技术，并利用上游的多模态机器学习模型来转移领域特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;MINT在两个关键应用中展示了其有效性：(1) 从文本中预测罕见遗传疾病，MINT生成的模型在仅依赖文本输入的情况下优于使用SFT、RAG或DPO训练的模型；(2) 使用细胞核图像进行组织类型分类，MINT生成的模型显著提高了Llama 3.2-Vision-11B-Instruct在组织类型分类上的性能。&lt;h4&gt;结论&lt;/h4&gt;MINT通过偏好优化提供了一种有效的方法，将单模态大型语言模型与高质量的多模态专业知识对齐，从而提高其在生物医学任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of high-quality multimodal biomedical data limits the ability toeffectively fine-tune pretrained Large Language Models (LLMs) for specializedbiomedical tasks. To address this challenge, we introduce MINT (MultimodalIntegrated kNowledge Transfer), a framework that aligns unimodal large decodermodels with domain-specific decision patterns from multimodal biomedical datathrough preference optimization. While MINT supports different optimizationtechniques, we primarily implement it with the Odds Ratio PreferenceOptimization (ORPO) framework as its backbone. This strategy enables thealigned LLMs to perform predictive tasks using text-only or image-only inputswhile retaining knowledge learnt from multimodal data. MINT leverages anupstream multimodal machine learning (MML) model trained on high-qualitymultimodal data to transfer domain-specific insights to downstream text-only orimage-only LLMs. We demonstrate its effectiveness through two key applications:(1) Rare genetic disease prediction from texts, where MINT uses a multimodalencoder model, trained on facial photos and clinical notes, to generate apreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despiterelying on text input only, the MINT-derived model outperforms models trainedwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissuetype classification using cell nucleus images, where MINT uses avision-language foundation model as the preference generator, containingknowledge learnt from both text and histopathological images to aligndownstream image-only models. The resulting MINT-derived model significantlyimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue typeclassification. In summary, MINT provides an effective strategy to alignunimodal LLMs with high-quality multimodal expertise through preferenceoptimization.</description>
      <author>example@mail.com (Da Wu, Zhanliang Wang, Quan Nguyen, Zhuoran Xu, Kai Wang)</author>
      <guid isPermaLink="false">2505.05736v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了在3D场景理解中的应用，提出了一种基于自动检索合成CAD模型的方法，以生成高质量的真实标签数据，用于训练深度学习模型。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成精确的3D标注数据是深度学习模型开发的一大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究目的是利用自动检索合成CAD模型的方法，为监督式深度学习模型训练提供高质量的真实标签数据。&lt;h4&gt;方法&lt;/h4&gt;论文采用了与之前用于自动标注ScanNet场景中物体9D姿态和CAD模型相似的流程，并将其应用于ScanNet++ v1数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，基于自动获得的标注数据可以训练深度学习模型，且训练出的模型性能优于基于人工标注数据的模型。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注具有提升模型性能的潜力，同时显著降低标注成本。论文将发布名为SCANnotate++的标注数据和训练模型，以支持未来3D场景理解的研究。&lt;h4&gt;翻译&lt;/h4&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构感知的多模态自监督分子表示预训练框架（MMSA），旨在通过利用分子之间的不变知识来增强分子图表示。&lt;h4&gt;背景&lt;/h4&gt;分子表示的准确提取是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，其中基于图像和2D/3D拓扑的多模态分子表示方法已成为主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间交互作用和无法充分捕捉分子间复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括两个主要模块：多模态分子表示学习模块和结构感知模块。多模态分子表示学习模块协同处理同一分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来模拟分子之间的高阶相关性，并引入记忆机制来存储典型分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MMSA在MoleculeNet基准测试上实现了最先进的性能，平均ROC-AUC比基线方法提高了1.8%到9.6%。&lt;h4&gt;结论&lt;/h4&gt;MMSA框架有效提高了分子表示的准确性，对药物发现过程具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model generalization ability. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Sensorimotor Learning for Open-world Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.06136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了开放世界机器人操作问题，即机器人必须能够泛化或快速适应它没有预先编程或预训练的新对象、场景或任务。通过有效的感觉运动学习方法，本文提出了新的观点和方法来提高数据效率，使机器人能够学习通用的操作技能。&lt;h4&gt;背景&lt;/h4&gt;开放世界机器人操作是一个挑战，因为机器人需要处理未知的新对象和任务。&lt;h4&gt;目的&lt;/h4&gt;通过有效的传感器运动学习方法解决开放世界机器人操作问题，提高机器人适应新环境和任务的能力。&lt;h4&gt;方法&lt;/h4&gt;利用有限的演示数据中的规律性，使机器人能够学习通用的操作技能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 引入方法赋予机器人以对象为中心的先验知识，使其能够从少量远程操作演示中学习通用的闭环传感器运动策略。2. 引入方法使机器人能够理解空间，从而从野外观测视频中模仿操作技能。3. 引入方法使机器人能够从以往的经验中识别可重用的技能，从而能够连续模仿多个任务。&lt;h4&gt;结论&lt;/h4&gt;本文的贡献为构建通用型个人机器人奠定了基础，这些机器人能够以低成本的数据收集快速适应新情况或任务，并且能够轻松与人类互动。通过使机器人能够从有限的数据中学习和泛化，本文向实现能够无缝集成到日常场景中的智能机器人助手的目标迈进。&lt;h4&gt;翻译&lt;/h4&gt;This dissertation considers Open-world Robot Manipulation, a manipulation problem where a robot must generalize or quickly adapt to new objects, scenes, or tasks for which it has not been pre-programmed or pre-trained. This dissertation tackles the problem using a methodology of efficient sensorimotor learning. The key to enabling efficient sensorimotor learning lies in leveraging regular patterns that exist in limited amounts of demonstration data. These patterns, referred to as ``regularity,'' enable the data-efficient learning of generalizable manipulation skills. This dissertation offers a new perspective on formulating manipulation problems through the lens of regularity. Building upon this notion, we introduce three major contributions. First, we introduce methods that endow robots with object-centric priors, allowing them to learn generalizable, closed-loop sensorimotor policies from a small number of teleoperation demonstrations. Second, we introduce methods that constitute robots' spatial understanding, unlocking their ability to imitate manipulation skills from in-the-wild video observations. Last but not least, we introduce methods that enable robots to identify reusable skills from their past experiences, resulting in systems that can continually imitate multiple tasks in a sequential manner. Altogether, the contributions of this dissertation help lay the groundwork for building general-purpose personal robots that can quickly adapt to new situations or tasks with low-cost data collection and interact easily with humans. By enabling robots to learn and generalize from limited data, this dissertation takes a step toward realizing the vision of intelligent robotic assistants that can be seamlessly integrated into everyday scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This dissertation considers Open-world Robot Manipulation, a manipulationproblem where a robot must generalize or quickly adapt to new objects, scenes,or tasks for which it has not been pre-programmed or pre-trained. Thisdissertation tackles the problem using a methodology of efficient sensorimotorlearning. The key to enabling efficient sensorimotor learning lies inleveraging regular patterns that exist in limited amounts of demonstrationdata. These patterns, referred to as ``regularity,'' enable the data-efficientlearning of generalizable manipulation skills. This dissertation offers a newperspective on formulating manipulation problems through the lens ofregularity. Building upon this notion, we introduce three major contributions.First, we introduce methods that endow robots with object-centric priors,allowing them to learn generalizable, closed-loop sensorimotor policies from asmall number of teleoperation demonstrations. Second, we introduce methods thatconstitute robots' spatial understanding, unlocking their ability to imitatemanipulation skills from in-the-wild video observations. Last but not least, weintroduce methods that enable robots to identify reusable skills from theirpast experiences, resulting in systems that can continually imitate multipletasks in a sequential manner. Altogether, the contributions of thisdissertation help lay the groundwork for building general-purpose personalrobots that can quickly adapt to new situations or tasks with low-cost datacollection and interact easily with humans. By enabling robots to learn andgeneralize from limited data, this dissertation takes a step toward realizingthe vision of intelligent robotic assistants that can be seamlessly integratedinto everyday scenarios.</description>
      <author>example@mail.com (Yifeng Zhu)</author>
      <guid isPermaLink="false">2505.06136v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.05732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of the paper published in SDM25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种改进的Denoising Diffusion Models（DDMs）的多级去噪自编码器框架，以提高其表示能力，并通过实验验证了其在图像嵌入和语义表示方面的优越性。&lt;h4&gt;背景&lt;/h4&gt;生成模型能够捕捉数据的真实分布，生成语义丰富的表示。尽管去噪扩散模型（DDMs）在生成能力上表现出色，但对其有效表示学习的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;扩展DDMs的表示能力，并通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;采用多级去噪自编码器框架，引入序列一致的扩散Transformer和额外的时步依赖编码器，通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法生成的嵌入在多个数据集上进行了广泛的实验，结果表明，DDMs最优学习的嵌入在大多数情况下超过了最先进的自监督表示学习方法，达到了显著的判别语义表示质量。&lt;h4&gt;结论&lt;/h4&gt;DDMs不仅适合于生成任务，而且对于通用深度学习应用也可能具有潜在优势。&lt;h4&gt;翻译&lt;/h4&gt;Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1137/1.9781611978520.1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models capture the true distribution of data, yieldingsemantically rich representations. Denoising diffusion models (DDMs) exhibitsuperior generative capabilities, though efficient representation learning forthem are lacking. In this work, we employ a multi-level denoising autoencoderframework to expand the representation capacity of DDMs, which introducessequentially consistent Diffusion Transformers and an additionaltimestep-dependent encoder to acquire embedding representations on thedenoising Markov chain through self-conditional diffusion learning.Intuitively, the encoder, conditioned on the entire diffusion process,compresses high-dimensional data into directional vectors in latent underdifferent noise levels, facilitating the learning of image embeddings acrossall timesteps. To verify the semantic adequacy of embeddings generated throughthis approach, extensive experiments are conducted on various datasets,demonstrating that optimally learned embeddings by DDMs surpassstate-of-the-art self-supervised representation learning methods in most cases,achieving remarkable discriminative semantic representation quality. Our workjustifies that DDMs are not only suitable for generative tasks, but alsopotentially advantageous for general-purpose deep learning applications.</description>
      <author>example@mail.com (Limai Jiang, Yunpeng Cai)</author>
      <guid isPermaLink="false">2505.05732v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
      <link>http://arxiv.org/abs/2505.05681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对卷尾猴的自然行为研究，使用预训练的视频文本基础模型，通过优化和调整，开发了一种从视频中检索有用片段的计算模型。&lt;h4&gt;背景&lt;/h4&gt;非人类灵长类动物的自然栖息地视频是研究其在野外行为的常见来源。&lt;h4&gt;目的&lt;/h4&gt;开发有用的计算模型，帮助研究人员从视频中检索有用片段。&lt;h4&gt;方法&lt;/h4&gt;使用原始、未标记的视频素材，结合弱音频描述，基于多模态大型语言模型（MLLMs）和视觉-语言模型（VLMs），提出了一种两阶段方法：数据预处理管道和微调过程。数据预处理管道自动提取干净且语义对齐的视频文本对，然后使用低秩适应（LoRA）方法微调预训练的Microsoft X-CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;在领域数据上，16帧模型和8帧模型的$Hits@5$分别提升了167%和114%。基于$NDCG@K$结果，模型能够很好地对大多数考虑的行为进行排序，而测试的原始预训练模型则无法对它们进行排序。&lt;h4&gt;结论&lt;/h4&gt;研究成功开发了一种有效的计算模型，能够帮助研究人员从视频中检索到有用的片段，特别是在处理噪声视频和音频内容方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the study of the natural behavior of capuchin monkeys using pre-trained video-text fundamental models, aiming to develop computational models that can help researchers retrieve useful clips from videos. The research successfully developed an effective computational model that can help researchers retrieve useful clips from videos, especially in dealing with noisy video and audio content.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video recordings of nonhuman primates in their natural habitat are a commonsource for studying their behavior in the wild. We fine-tune pre-trainedvideo-text foundational models for the specific domain of capuchin monkeys,with the goal of developing useful computational models to help researchers toretrieve useful clips from videos. We focus on the challenging problem oftraining a model based solely on raw, unlabeled video footage, using weak audiodescriptions sometimes provided by field collaborators. We leverage recentadvances in Multimodal Large Language Models (MLLMs) and Vision-Language Models(VLMs) to address the extremely noisy nature of both video and audio content.Specifically, we propose a two-folded approach: an agentic data treatmentpipeline and a fine-tuning process. The data processing pipeline automaticallyextracts clean and semantically aligned video-text pairs from the raw videos,which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP modelthrough Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of$167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame modelon our domain data. Moreover, based on $NDCG@K$ results, our model is able torank well most of the considered behaviors, while the tested raw pre-trainedmodels are not able to rank them at all. The code will be made available uponacceptance.</description>
      <author>example@mail.com (Giulio Cesare Mastrocinque Santo, Patrícia Izar, Irene Delval, Victor de Napole Gregolin, Nina S. T. Hirata)</author>
      <guid isPermaLink="false">2505.05681v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>3D Hand-Eye Calibration for Collaborative Robot Arm: Look at Robot Base Once</title>
      <link>http://arxiv.org/abs/2504.21619v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  updated&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。&lt;h4&gt;背景&lt;/h4&gt;手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。&lt;h4&gt;目的&lt;/h4&gt;目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。&lt;h4&gt;结论&lt;/h4&gt;提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-eye calibration is a common problem in the field of collaborativerobotics, involving the determination of the transformation matrix between thevisual sensor and the robot flange to enable vision-based robotic tasks.However, this process typically requires multiple movements of the robot armand an external calibration object, making it both time-consuming andinconvenient, especially in scenarios where frequent recalibration isnecessary. In this work, we extend our previous method which eliminates theneed for external calibration objects such as a chessboard. We propose ageneric dataset generation approach for point cloud registration, focusing onaligning the robot base point cloud with the scanned data. Furthermore, a moredetailed simulation study is conducted involving several differentcollaborative robot arms, followed by real-world experiments in an industrialsetting. Our improved method is simulated and evaluated using a total of 14robotic arms from 9 different brands, including KUKA, Universal Robots,UFACTORY, and Franka Emika, all of which are widely used in the field ofcollaborative robotics. Physical experiments demonstrate that our extendedapproach achieves performance comparable to existing commercial hand-eyecalibration solutions, while completing the entire calibration procedure injust a few seconds. In addition, we provide a user-friendly hand-eyecalibration solution, with the code publicly available atgithub.com/leihui6/LRBO.</description>
      <author>example@mail.com (Leihui Li, Lixuepiao Wan, Volker Krueger, Xuping Zhang)</author>
      <guid isPermaLink="false">2504.21619v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
      <link>http://arxiv.org/abs/2505.05644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将月球图像的反射率参数估计和基于图像的3D重建作为多模态学习问题，并设计了一种统一的变压器架构，用于学习不同数据源之间的共享表示。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是多学科新兴的研究主题，但在行星科学中应用较少。&lt;h4&gt;目的&lt;/h4&gt;将月球图像的反射率参数估计和基于图像的3D重建问题转化为多模态学习问题，并提出相应的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的变压器架构，用于学习灰度图像、数字高程模型、表面法线和反照率图等多种数据源之间的共享表示。&lt;h4&gt;主要发现&lt;/h4&gt;该架构能够从灰度图像中同时预测数字高程模型和反照率图，解决了行星表面3D重建的任务，并解耦了光度参数和高度信息。&lt;h4&gt;结论&lt;/h4&gt;该基础模型能够在四种模态之间学习到物理上合理的关联关系，未来可以通过添加更多输入模态来实现诸如光度归一化和共定位等任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning is an emerging research topic across multiple disciplinesbut has rarely been applied to planetary science. In this contribution, weidentify that reflectance parameter estimation and image-based 3Dreconstruction of lunar images can be formulated as a multimodal learningproblem. We propose a single, unified transformer architecture trained to learnshared representations between multiple sources like grayscale images, digitalelevation models, surface normals, and albedo maps. The architecture supportsflexible translation from any input modality to any target modality. PredictingDEMs and albedo maps from grayscale images simultaneously solves the task of 3Dreconstruction of planetary surfaces and disentangles photometric parametersand height information. Our results demonstrate that our foundation modellearns physically plausible relations across these four modalities. Adding moreinput modalities in the future will enable tasks such as photometricnormalization and co-registration.</description>
      <author>example@mail.com (Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian Wöhler)</author>
      <guid isPermaLink="false">2505.05644v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models</title>
      <link>http://arxiv.org/abs/2505.05577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 42nd International Conference on Machine Learning,  Vancouver, Canada. PMLR 267, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PyTDC，一个开源的机器学习平台，用于多模态生物AI模型的训练、评估和推理。PyTDC整合了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。&lt;h4&gt;背景&lt;/h4&gt;现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。&lt;h4&gt;目的&lt;/h4&gt;提出PyTDC平台，以促进多模态、上下文感知的基础模型在生物医学AI开放问题上的研究。&lt;h4&gt;方法&lt;/h4&gt;讨论了PyTDC架构的组件，并进行了首个针对单细胞药物靶点提名机器学习任务的案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;在图表示学习和图理论领域的最先进方法在该任务上表现不佳。虽然发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态。&lt;h4&gt;结论&lt;/h4&gt;PyTDC平台能够促进关于开发多模态、上下文感知的基础模型的研究，这些模型可以解决生物医学AI中的开放问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。我们提出了PyTDC，一个开源的机器学习平台，提供了多模态生物AI模型的简化训练、评估和推理软件。PyTDC统一了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。本文讨论了PyTDC架构的组件，并进行了我们知识范围内的首个关于引入的单细胞药物靶点提名机器学习任务的案例研究。我们发现图表示学习和图理论领域的最先进方法在该任务上表现不佳。尽管我们发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态，这突显了PyTDC平台促进关于开发多模态、上下文感知的基础模型的研究的能力，这些模型可以解决生物医学AI中的开放问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/apliko-xyz/pytdc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing biomedical benchmarks do not provide end-to-end infrastructure fortraining, evaluation, and inference of models that integrate multimodalbiological data and a broad range of machine learning tasks in therapeutics. Wepresent PyTDC, an open-source machine-learning platform providing streamlinedtraining, evaluation, and inference software for multimodal biological AImodels. PyTDC unifies distributed, heterogeneous, continuously updated datasources and model weights and standardizes benchmarking and inferenceendpoints. This paper discusses the components of PyTDC's architecture and, toour knowledge, the first-of-its-kind case study on the introduced single-celldrug-target nomination ML task. We find state-of-the-art methods in graphrepresentation learning and domain-specific methods from graph theory performpoorly on this task. Though we find a context-aware geometric deep learningmethod that outperforms the evaluated SoTA and domain-specific baselinemethods, the model is unable to generalize to unseen cell types or incorporateadditional modalities, highlighting PyTDC's capacity to facilitate an excitingavenue of research developing multimodal, context-aware, foundation models foropen problems in biomedical AI.</description>
      <author>example@mail.com (Alejandro Velez-Arce, Marinka Zitnik)</author>
      <guid isPermaLink="false">2505.05577v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2505.05612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;scDrugMap是一个集成的框架，用于预测单细胞数据中的药物反应，旨在解决癌症治疗中的药物耐药性问题。&lt;h4&gt;背景&lt;/h4&gt;药物耐药性是癌症治疗中的主要挑战，单细胞分析有助于了解细胞异质性，但大规模基础模型在预测单细胞数据药物反应中的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发scDrugMap框架，以预测单细胞数据中的药物反应，并建立大规模基准。&lt;h4&gt;方法&lt;/h4&gt;scDrugMap使用Python命令行界面和Web服务器进行药物反应预测。它评估了包括八种单细胞模型和两种大型语言模型在内的多种基础模型。使用超过326,000个细胞的原始数据集和18,800个细胞的验证集进行评估，涵盖了36个数据集和多种组织和癌症类型。模型性能在池化数据和跨数据评估设置下进行了基准测试，使用了层冻结和低秩适应（LoRA）微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;在池化数据场景中，scFoundation模型在层冻结和微调情况下均表现出最佳性能，F1分数分别为0.971和0.947，超过表现最差的模型50%以上。在跨数据设置中，UCE模型在微调后表现突出（平均F1：0.774），而scGPT在零样本学习中领先（平均F1：0.858）。&lt;h4&gt;结论&lt;/h4&gt;scDrugMap为单细胞数据中药物反应预测的基础模型提供了第一个大规模基准，是一个用户友好且灵活的平台，用于推进药物发现和转化研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/qsong-github/scdrugmap&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug resistance presents a major challenge in cancer therapy. Single cellprofiling offers insights into cellular heterogeneity, yet the application oflarge-scale foundation models for predicting drug response in single cell dataremains underexplored. To address this, we developed scDrugMap, an integratedframework featuring both a Python command-line interface and a web server fordrug response prediction. scDrugMap evaluates a wide range of foundationmodels, including eight single-cell models and two large language models, usinga curated dataset of over 326,000 cells in the primary collection and 18,800cells in the validation set, spanning 36 datasets and diverse tissue and cancertypes. We benchmarked model performance under pooled-data and cross-dataevaluation settings, employing both layer freezing and Low-Rank Adaptation(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundationachieved the best performance, with mean F1 scores of 0.971 (layer freezing)and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMapprovides the first large-scale benchmark of foundation models for drug responseprediction in single-cell data and serves as a user-friendly, flexible platformfor advancing drug discovery and translational research.</description>
      <author>example@mail.com (Qing Wang, Yining Pan, Minghao Zhou, Zijia Tang, Yanfei Wang, Guangyu Wang, Qianqian Song)</author>
      <guid isPermaLink="false">2505.05612v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究如何结合长期观察数据和短期实验数据来估计长期因果关系，并提出了一种基于自然数据异质性的方法来识别潜在混杂因素，避免了对理想化假设的依赖。&lt;h4&gt;背景&lt;/h4&gt;在现实场景中，估计长期因果关系是一个关键但具有挑战性的问题，现有的方法通常依赖于理想化的假设，而这些假设在实际情况中往往不成立，限制了其实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要理想化假设的方法来估计长期个体因果关系。&lt;h4&gt;方法&lt;/h4&gt;利用数据的自然异质性，如来自多个来源的数据，来识别潜在混杂因素，并设计了一种基于潜在表示学习估计长期因果效应的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个合成和半合成数据集上进行的实验研究，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法可以有效地估计长期因果关系，且不依赖于理想化假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/learnwjj/ICEVAE&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Griffin: Towards a Graph-Centric Relational Database Foundation Model</title>
      <link>http://arxiv.org/abs/2505.05568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Griffin，这是第一个专门为关系数据库（RDBs）设计的基座模型。Griffin通过统一数据编码器和任务解码器来处理各种任务，并集成了交叉注意力模块和新型聚合器，在单表和RDB数据集上进行预训练，展示了其在处理大规模、异构和时间图上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;目前存在一些小型模型专注于单一RDB任务，但它们无法处理多样化的任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够处理关系数据库多样化任务的基座模型。&lt;h4&gt;方法&lt;/h4&gt;Griffin模型集成了交叉注意力模块和新型聚合器，并在单表和RDB数据集上进行预训练，使用高级编码器处理分类、数值和元数据特征。&lt;h4&gt;主要发现&lt;/h4&gt;Griffin在大型、异构和时间图上表现出优异或相当的性能，擅长处理低数据场景，并在预训练新数据集和任务时显示出强大的迁移能力。&lt;h4&gt;结论&lt;/h4&gt;Griffin作为一个适用于关系数据库的通用基座模型具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Griffin, the first foundation model attemptation designed specifically for Relational Databases (RDBs). Unlike previous smaller models focused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture by incorporating a cross-attention module and a novel aggregator. Griffin utilizes pretraining on both single-table and RDB datasets, employing advanced encoders for categorical, numerical, and metadata features, along with innovative components such as cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture the complexities of relational data. Evaluated on large-scale, heterogeneous, and temporal graphs extracted from RDBs across various domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels in low-data scenarios, and shows strong transferability with similarity and diversity in pretraining across new datasets and tasks, highlighting its potential as a universally applicable foundation model for RDBs. Code available at https://github.com/yanxwb/Griffin.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Griffin, the first foundation model attemptation designedspecifically for Relational Databases (RDBs). Unlike previous smaller modelsfocused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture byincorporating a cross-attention module and a novel aggregator. Griffin utilizespretraining on both single-table and RDB datasets, employing advanced encodersfor categorical, numerical, and metadata features, along with innovativecomponents such as cross-attention modules and enhanced message-passing neuralnetworks (MPNNs) to capture the complexities of relational data. Evaluated onlarge-scale, heterogeneous, and temporal graphs extracted from RDBs acrossvarious domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels inlow-data scenarios, and shows strong transferability with similarity anddiversity in pretraining across new datasets and tasks, highlighting itspotential as a universally applicable foundation model for RDBs. Code availableat https://github.com/yanxwb/Griffin.</description>
      <author>example@mail.com (Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang)</author>
      <guid isPermaLink="false">2505.05568v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model (SAM)的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM的引入为语义分割应用铺平了道路，但其不确定性量化存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时尊重随机、认知和任务不确定性的不确定性量化模型。&lt;h4&gt;方法&lt;/h4&gt;使用贝叶斯熵公式训练了USAM，这是一种轻量级的后处理不确定性量化方法。该模型将不确定性的根源追溯到欠参数化模型、不充分的提示或图像模糊性。&lt;h4&gt;主要发现&lt;/h4&gt;USAM在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上显示出优越的预测能力，提供了一种计算成本低、易于使用的UQ替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM可以作为支持用户提示、增强半监督流程或平衡准确性和成本效率之间权衡的不确定性量化工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/GreenAutoML4FAS/UncertainSAM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
      <link>http://arxiv.org/abs/2505.05518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的跟踪模型，用于实时跟踪心脏内导管尖端的位置，以提高介入性电生理和结构性心脏病治疗中的操作效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;心脏内超声心动图（ICE）在电生理和结构性心脏病介入治疗中扮演着关键角色，但由于手动操作ICE导管时需要频繁调整，持续监测导管尖端位置是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个AI驱动的跟踪模型，以估计ICE成像平面内的导管尖端入射角和通过点，确保连续的可视化并便于机器人ICE导管控制。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合数据集生成策略，结合临床ICE序列和合成数据增强以增强模型鲁棒性。在水箱设置中收集ICE图像，并使用电磁传感器确定精确的真实位置。合成序列通过将导管尖端叠加到真实ICE图像上来创建，以保持运动连续性并模拟不同的解剖场景。最终数据集包含5,698个ICE尖端图像对。模型架构集成了预训练的超声基础模型，并使用基于变换器的网络处理连续的ICE帧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法实现了3.32度的入口角度误差和12.76度的旋转角度误差，为实时机器人ICE导管调整奠定了基础，最小化了操作者的工作量，同时确保了治疗设备的连续可视性。&lt;h4&gt;结论&lt;/h4&gt;该AI驱动的框架有助于提高心脏介入治疗中的操作效率和安全性，未来工作将集中于扩大临床数据集以进一步提高模型泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control. A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage. Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy. Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intra-cardiac Echocardiography (ICE) plays a critical role inElectrophysiology (EP) and Structural Heart Disease (SHD) interventions byproviding real-time visualization of intracardiac structures. However,maintaining continuous visibility of the therapy device tip remains a challengedue to frequent adjustments required during manual ICE catheter manipulation.To address this, we propose an AI-driven tracking model that estimates thedevice tip incident angle and passing point within the ICE imaging plane,ensuring continuous visibility and facilitating robotic ICE catheter control.  A key innovation of our approach is the hybrid dataset generation strategy,which combines clinical ICE sequences with synthetic data augmentation toenhance model robustness. We collected ICE images in a water chamber setup,equipping both the ICE catheter and device tip with electromagnetic (EM)sensors to establish precise ground-truth locations. Synthetic sequences werecreated by overlaying catheter tips onto real ICE images, preserving motioncontinuity while simulating diverse anatomical scenarios. The final datasetconsists of 5,698 ICE-tip image pairs, ensuring comprehensive trainingcoverage.  Our model architecture integrates a pretrained ultrasound (US) foundationmodel, trained on 37.4M echocardiography images, for feature extraction. Atransformer-based network processes sequential ICE frames, leveraginghistorical passing points and incident angles to improve prediction accuracy.  Experimental results demonstrate that our method achieves 3.32 degree entryangle error, 12.76 degree rotation angle error. This AI-driven framework laysthe foundation for real-time robotic ICE catheter adjustments, minimizingoperator workload while ensuring consistent therapy device visibility. Futurework will focus on expanding clinical datasets to further enhance modelgeneralization.</description>
      <author>example@mail.com (Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim)</author>
      <guid isPermaLink="false">2505.05518v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>AI-powered virtual eye: perspective, challenges and opportunities</title>
      <link>http://arxiv.org/abs/2505.05516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 Pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为‘虚拟眼’的下一代AI平台，利用互联的基础模型模拟眼睛的复杂结构和生物功能。&lt;h4&gt;背景&lt;/h4&gt;AI、成像和多组学技术的进步为构建高保真的人类眼睛数字复制品提供了肥沃的土壤。&lt;h4&gt;目的&lt;/h4&gt;追溯从早期机械和基于规则的模型到当代AI驱动方法的发展历程，并整合一个具有多模态、多尺度、动态预测能力和嵌入式反馈机制的综合模型。&lt;h4&gt;方法&lt;/h4&gt;提出一个发展路线图，强调大规模多模态数据集、生成AI、基础模型、基于代理的架构和交互式界面的作用。&lt;h4&gt;主要发现&lt;/h4&gt;尽管存在可解释性、伦理、数据处理和评估的挑战，虚拟眼有潜力革命性地改变个性化眼科护理并加速眼健康和疾病的研究。&lt;h4&gt;结论&lt;/h4&gt;虚拟眼有望在眼科护理和眼健康研究领域带来重大变革。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We envision the "virtual eye" as a next-generation, AI-powered platform thatuses interconnected foundation models to simulate the eye's intricate structureand biological function across all scales. Advances in AI, imaging, andmultiomics provide a fertile ground for constructing a universal, high-fidelitydigital replica of the human eye. This perspective traces the evolution fromearly mechanistic and rule-based models to contemporary AI-driven approaches,integrating in a unified model with multimodal, multiscale, dynamic predictivecapabilities and embedded feedback mechanisms. We propose a development roadmapemphasizing the roles of large-scale multimodal datasets, generative AI,foundation models, agent-based architectures, and interactive interfaces.Despite challenges in interpretability, ethics, data processing and evaluation,the virtual eye holds the potential to revolutionize personalized ophthalmiccare and accelerate research into ocular health and disease.</description>
      <author>example@mail.com (Yue Wu, Yibo Guo, Yulong Yan, Jiancheng Yang, Xin Zhou, Ching-Yu Cheng, Danli Shi, Mingguang He)</author>
      <guid isPermaLink="false">2505.05516v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience</title>
      <link>http://arxiv.org/abs/2505.05515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个基于神经科学的自主推理框架，旨在提高智能系统的自主推理能力。&lt;h4&gt;背景&lt;/h4&gt;随着自主人工智能的发展，研究如何使智能代理真正实现自主性成为一个关键问题。&lt;h4&gt;目的&lt;/h4&gt;探究智能代理自主性的核心，即代理推理，并建立一个新的神经科学启发框架。&lt;h4&gt;方法&lt;/h4&gt;基于神经科学的三个定义，提出了一个统一的框架，包括感知、维度、逻辑和交互四种核心推理类型，并应用于分类和分析现有的AI推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架有助于理解和评估现有AI推理方法的理论基础、计算设计和实践限制，并提出了构建更具通用性和认知一致性的智能代理的新方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合认知神经科学和人工智能，该研究为推进智能系统中的代理推理提供了理论基础和实践路线。&lt;h4&gt;翻译&lt;/h4&gt;Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous AI is no longer a hard-to-reach concept, it enables the agents tomove beyond executing tasks to independently addressing complex problems,adapting to change while handling the uncertainty of the environment. However,what makes the agents truly autonomous? It is agentic reasoning, that iscrucial for foundation models to develop symbolic logic, statisticalcorrelations, or large-scale pattern recognition to process information, drawinferences, and make decisions. However, it remains unclear why and howexisting agentic reasoning approaches work, in comparison to biologicalreasoning, which instead is deeply rooted in neural mechanisms involvinghierarchical cognition, multimodal integration, and dynamic interactions. Inthis work, we propose a novel neuroscience-inspired framework for agenticreasoning. Grounded in three neuroscience-based definitions and supported bymathematical and biological foundations, we propose a unified frameworkmodeling reasoning from perception to action, encompassing four core types,perceptual, dimensional, logical, and interactive, inspired by distinctfunctional roles observed in the human brain. We apply this framework tosystematically classify and analyze existing AI reasoning methods, evaluatingtheir theoretical foundations, computational designs, and practicallimitations. We also explore its implications for building more generalizable,cognitively aligned agents in physical and virtual environments. Finally,building on our framework, we outline future directions and propose newneural-inspired reasoning methods, analogous to chain-of-thought prompting. Bybridging cognitive neuroscience and AI, this work offers a theoreticalfoundation and practical roadmap for advancing agentic reasoning in intelligentsystems. The associated project can be found at:https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .</description>
      <author>example@mail.com (Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun Tang, Lin Wang)</author>
      <guid isPermaLink="false">2505.05515v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAISY的运动感知图像合成方法，用于消除医学图像采集过程中的运动模糊，并通过实验证明其性能优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;患者运动会导致医学图像模糊、鬼影和器官变形，从而增加图像解读的难度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来消除医学图像中的运动伪影，并提高图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先对运动进行特征化，然后利用Segment Anything Model (SAM)动态学习解剖边界处的空间模式，并引入Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;与现有技术相比，MAISY在胸部和头部CT数据集上的Peak Signal-to-Noise Ratio (PSNR)、SSIM和Dice指数均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在消除医学图像中的运动伪影方面表现优异，有效提高了图像质量。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v3</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
      <link>http://arxiv.org/abs/2407.06188v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://yukangcao.github.io/CrowdMoGen&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrowdMoGen是一个零样本框架，用于集体运动生成，能够从文本提示中有效分组个体并生成与事件对齐的运动序列。&lt;h4&gt;背景&lt;/h4&gt;当前文本到运动生成技术通常假设所有个体作为一个单一单元，难以扩展到更大的人群并确保个体对特定事件做出适当的反应。&lt;h4&gt;目的&lt;/h4&gt;提出CrowdMoGen，以解决上述挑战，实现集体运动生成。&lt;h4&gt;方法&lt;/h4&gt;1) 利用预训练的大语言模型（LLMs）组织个体成不同群体；2) 集成基于SMPL的关节先验生成与情境相符的活动，包括关节轨迹和文本描述；3) 引入集体运动生成器，将活动整合到基于transformer的网络中，在多步去噪过程中保持空间约束。&lt;h4&gt;主要发现&lt;/h4&gt;CrowdMoGen在实验中显著优于先前方法，能够生成逼真、由事件驱动且空间上连贯的运动序列。&lt;h4&gt;结论&lt;/h4&gt;CrowdMoGen作为第一个集体运动生成框架，有望推动城市模拟、人群规划和其他大规模交互环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in text-to-motion generation have shown promisingresults, they typically assume all individuals are grouped as a single unit.Scaling these methods to handle larger crowds and ensuring that individualsrespond appropriately to specific events remains a significant challenge. Thisis primarily due to the complexities of scene planning, which involvesorganizing groups, planning their activities, and coordinating interactions,and controllable motion generation. In this paper, we present CrowdMoGen, thefirst zero-shot framework for collective motion generation, which effectivelygroups individuals and generates event-aligned motion sequences from textprompts. 1) Being limited by the available datasets for training an effectivescene planning module in a supervised manner, we instead propose a crowd sceneplanner that leverages pre-trained large language models (LLMs) to organizeindividuals into distinct groups. While LLMs offer high-level guidance forgroup divisions, they lack the low-level understanding of human motion. Toaddress this, we further propose integrating an SMPL-based joint prior togenerate context-appropriate activities, which consists of both jointtrajectories and textual descriptions. 2) Secondly, to incorporate the assignedactivities into the generative network, we introduce a collective motiongenerator that integrates the activities into a transformer-based network in ajoint-wise manner, maintaining the spatial constraints during the multi-stepdenoising process. Extensive experiments demonstrate that CrowdMoGensignificantly outperforms previous approaches, delivering realistic,event-driven motion sequences that are spatially coherent. As the firstframework of collective motion generation, CrowdMoGen has the potential toadvance applications in urban simulation, crowd planning, and other large-scaleinteractive environments.</description>
      <author>example@mail.com (Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu)</author>
      <guid isPermaLink="false">2407.06188v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach</title>
      <link>http://arxiv.org/abs/2505.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the MORSE workshop of CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算机视觉领域的应用，提出了一个基于“能力编码”的方法来预测模型在多个下游任务上的性能，旨在简化模型选择并提供未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;在地球观测领域，过去四年内开发了超过75个遥感视觉基础模型，但它们在所有任务上并未表现出一致的优越性。&lt;h4&gt;目的&lt;/h4&gt;为了促进模型间的比较，提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。&lt;h4&gt;方法&lt;/h4&gt;该方法基于“能力编码”，旨在简化选择适用于特定新任务的基础模型，并用于分析现有文献，提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;通过“能力编码”方法，可以简化模型选择，并为现有文献提供新的视角。&lt;h4&gt;结论&lt;/h4&gt;该方法为选择和评估基础模型提供了新的途径，并为未来的研究指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在计算机视觉领域取得了显著进步：经过一次昂贵但有效的训练阶段后，它们可以处理各种任务。在地球观测领域，过去四年中已经开发了75多个遥感视觉基础模型。然而，没有一个模型在所有可用的下游任务上始终优于其他模型。为了促进它们的比较，我们提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。这种方法基于我们所说的“能力编码”。这种新颖方法的效用有两方面：我们展示了它在简化选择特定新任务的基础模型方面的潜力，并使用它来提供对现有文献的新视角，提出了未来研究的途径。代码可在https://github.com/pierreadorni/capabilities-encoding上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pierreadorni/capabilities-encoding&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models constitute a significant advancement in computer vision:after a single, albeit costly, training phase, they can address a wide array oftasks. In the field of Earth observation, over 75 remote sensing visionfoundation models have been developed in the past four years. However, none hasconsistently outperformed the others across all available downstream tasks. Tofacilitate their comparison, we propose a cost-effective method for predictinga model's performance on multiple downstream tasks without the need forfine-tuning on each one. This method is based on what we call "capabilitiesencoding." The utility of this novel approach is twofold: we demonstrate itspotential to simplify the selection of a foundation model for a given new task,and we employ it to offer a fresh perspective on the existing literature,suggesting avenues for future research. Codes are available athttps://github.com/pierreadorni/capabilities-encoding.</description>
      <author>example@mail.com (Pierre Adorni, Minh-Tan Pham, Stéphane May, Sébastien Lefèvre)</author>
      <guid isPermaLink="false">2505.03299v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
  <item>
      <title>Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization</title>
      <link>http://arxiv.org/abs/2505.05343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is  available at https://github.com/swimmiing/ACL-SSL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉-语言模型在声音源定位中的应用，提出了一种无显式文本输入的自监督方法，通过音频驱动嵌入和对比音频-视觉对应目标，实现了对声音源的高效定位。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉-语言模型在多模态对齐和泛化方面表现出色，其中CLIP模型尤为成功。&lt;h4&gt;目的&lt;/h4&gt;将CLIP模型应用于声音源定位，提出一种无需显式文本输入的自监督方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将音频映射到与CLIP文本编码器兼容的token的框架，生成音频驱动嵌入，用于生成声音区域掩码，并通过对比音频-视觉对应目标提取视觉特征与音频嵌入对齐。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的多模态基础模型的对齐知识使得该方法能够生成更完整和紧凑的声音源定位，进一步通过LLM引导扩展，将对象感知的音频-视觉场景理解提炼到模型中，增强了对齐。&lt;h4&gt;结论&lt;/h4&gt;在五个不同任务上的广泛实验表明，该方法在各种变体中都优于现有方法，并在零样本设置中实现了强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper studies the application of large-scale vision-language models in sound source localization, proposes a self-supervised method without explicit text input, and achieves efficient localization of sound sources through audio-driven embeddings and contrastive audio-visual correspondence objectives.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/swimmiing/ACL-SSL&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models demonstrate strong multimodal alignmentand generalization across diverse tasks. Among them, CLIP stands out as one ofthe most successful approaches. In this work, we extend the application of CLIPto sound source localization, proposing a self-supervised method operateswithout explicit text input. We introduce a framework that maps audios intotokens compatible with CLIP's text encoder, producing audio-driven embeddings.These embeddings are used to generate sounding region masks, from which visualfeatures are extracted and aligned with the audio embeddings through acontrastive audio-visual correspondence objective. Our findings show thatalignment knowledge of pre-trained multimodal foundation model enables ourmethod to generate more complete and compact localization for sounding objects.We further propose an LLM-guided extension that distills object-awareaudio-visual scene understanding into the model during training to enhancealignment. Extensive experiments across five diverse tasks demonstrate that ourmethod, in all variants, outperforms state-of-the-art approaches and achievesstrong generalization in zero-shot settings.</description>
      <author>example@mail.com (Sooyoung Park, Arda Senocak, Joon Son Chung)</author>
      <guid isPermaLink="false">2505.05343v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合长期观察数据和短期实验数据来估计长期因果效应的方法，以解决现实场景中因果推断的挑战。&lt;h4&gt;背景&lt;/h4&gt;在现有方法中，通常假设潜在无混淆性或加性等混淆偏倚，但这些假设在现实应用中往往不成立，限制了方法的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需上述假设的长期个体因果效应估计方法。&lt;h4&gt;方法&lt;/h4&gt;利用数据自然异质性，如来自多个来源的数据，来识别潜在混淆因子，从而避免对理想化假设的依赖。具体方法是设计一个基于潜在表示学习的长期因果效应估计器。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析，建立了潜在混淆因子的可识别性，并在此基础上实现了长期效应识别。实验结果表明，该方法在多个合成和半合成数据集上有效。&lt;h4&gt;结论&lt;/h4&gt;该方法在估计长期因果效应时，能够有效避免对理想化假设的依赖，提高了在现实世界中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model</title>
      <link>http://arxiv.org/abs/2505.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对路边感知在智能交通系统和车辆到一切（V2X）任务中的应用，提出了一种基于Mamba模型的PillarMamba框架，用于路边点云感知，并通过混合状态空间块（HSB）解决了空间模型在扫描方向限制下的局部连接中断和历史关系遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;路边感知在智能交通系统和车辆到一切（V2X）任务中越来越受到重视，因为它可以扩展连接车辆的感知范围并提高交通安全。然而，基于路边点云的3D目标检测尚未得到有效探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Mamba的PillarMamba框架，用于路边点云感知，并解决空间模型在扫描方向限制下的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入Mamba模型到路边点云感知，并基于交叉阶段状态空间组（CSG）提出PillarMamba框架。通过混合状态空间块（HSB）增强网络的表示能力，并通过交叉阶段特征融合实现高效计算。HSB通过局部卷积增强邻域连接，通过残差注意力保留历史记忆。&lt;h4&gt;主要发现&lt;/h4&gt;PillarMamba在DAIR-V2X-I这个流行的路边大规模数据集上优于现有方法，表明了所提出的方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;PillarMamba框架能够有效地进行路边点云感知，并通过HSB解决了空间模型在扫描方向限制下的局限性，为智能交通系统和V2X任务提供了有效的感知支持。&lt;h4&gt;翻译&lt;/h4&gt;Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud-oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything(V2X) tasks, roadside perception has received increasing attention in recentyears, as it can extend the perception range of connected vehicles and improvetraffic safety. However, roadside point cloud oriented 3D object detection hasnot been effectively explored. To some extent, the key to the performance of apoint cloud detector lies in the receptive field of the network and the abilityto effectively utilize the scene context. The recent emergence of Mamba, basedon State Space Model (SSM), has shaken up the traditional convolution andtransformers that have long been the foundational building blocks, due to itsefficient global receptive field. In this work, we introduce Mamba topillar-based roadside point cloud perception and propose a framework based onCross-stage State-space Group (CSG), called PillarMamba. It enhances theexpressiveness of the network and achieves efficient computation throughcross-stage feature fusion. However, due to the limitations of scan directions,state space model faces local connection disrupted and historical relationshipforgotten. To address this, we propose the Hybrid State-space Block (HSB) toobtain the local-global context of roadside point cloud. Specifically, itenhances neighborhood connections through local convolution and preserveshistorical memory through residual attention. The proposed method outperformsthe state-of-the-art methods on the popular large scale roadside benchmark:DAIR-V2X-I. The code will be released soon.</description>
      <author>example@mail.com (Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Tianze Wang, Jianghao Leng)</author>
      <guid isPermaLink="false">2505.05397v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning</title>
      <link>http://arxiv.org/abs/2505.05208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE IJCNN 2025 has accepted the paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于模糊sigmoid卷积（FSC）的肿瘤检测方法，通过减少可训练参数数量来提高模型性能，同时保持分类精度。&lt;h4&gt;背景&lt;/h4&gt;早期检测和准确诊断对于提高患者预后至关重要，卷积神经网络（CNN）在肿瘤检测中显示出潜力，但现有模型往往存在过参数化问题。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种高效、准确且参数较少的肿瘤检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括模糊sigmoid卷积（FSC）、漏斗顶部模块和漏斗中部模块。FSC通过有效的感受野扩展和保持输入数据完整性，实现高效的特征图减少和模型肿瘤检测能力提升。模糊sigmoid激活函数被引入卷积层以提高特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;FSC模型在三个基准数据集上实现了99.17%、99.75%和99.89%的分类精度，参数数量比大规模迁移学习架构少100倍。&lt;h4&gt;结论&lt;/h4&gt;FSC模型是一种高效、轻量级的深度学习模型，适用于医学影像应用中的脑肿瘤早期检测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：早期检测和准确诊断对于改善患者预后至关重要。卷积神经网络（CNN）在肿瘤检测中显示出希望，但现有模型通常受到过参数化的限制，这限制了其性能提升。在本研究中，引入了模糊sigmoid卷积（FSC）以及两个附加模块：漏斗顶部和中部。所提出的方法显著减少了可训练参数的数量，而没有损害分类精度。这种方法的核心是一个新颖的卷积算子，它有效地扩展了感受野同时保持输入数据完整性。这使高效的特征图减少和增强模型肿瘤检测能力成为可能。在基于FSC的模型中，模糊sigmoid激活函数被整合到卷积层中，以改进特征提取和分类。将模糊逻辑纳入架构提高了其适应性和鲁棒性。在三个基准数据集上进行的广泛实验表明了所提出模型的优越性能和效率。基于FSC的架构在三个不同的数据集上达到了99.17%、99.75%和99.89%的分类精度。该模型使用的参数数量比大规模迁移学习架构少100倍，突出了其计算效率和适用于早期检测脑肿瘤的适用性。这项研究为医学影像应用提供了轻量级、高性能的深度学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection and accurate diagnosis are essential to improving patientoutcomes. The use of convolutional neural networks (CNNs) for tumor detectionhas shown promise, but existing models often suffer from overparameterization,which limits their performance gains. In this study, fuzzy sigmoid convolution(FSC) is introduced along with two additional modules: top-of-the-funnel andmiddle-of-the-funnel. The proposed methodology significantly reduces the numberof trainable parameters without compromising classification accuracy. A novelconvolutional operator is central to this approach, effectively dilating thereceptive field while preserving input data integrity. This enables efficientfeature map reduction and enhances the model's tumor detection capability. Inthe FSC-based model, fuzzy sigmoid activation functions are incorporated withinconvolutional layers to improve feature extraction and classification. Theinclusion of fuzzy logic into the architecture improves its adaptability androbustness. Extensive experiments on three benchmark datasets demonstrate thesuperior performance and efficiency of the proposed model. The FSC-basedarchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%on three different datasets. The model employs 100 times fewer parameters thanlarge-scale transfer learning architectures, highlighting its computationalefficiency and suitability for detecting brain tumors early. This researchoffers lightweight, high-performance deep-learning models for medical imagingapplications.</description>
      <author>example@mail.com (Muhammad Irfan, Anum Nawaz, Riku Klen, Abdulhamit Subasi, Tomi Westerlund, Wei Chen)</author>
      <guid isPermaLink="false">2505.05208v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Analysis for Contrastive Representation Learning under Non-IID Settings</title>
      <link>http://arxiv.org/abs/2505.04937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To Appear in ICML, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比表示学习（CRL）的泛化行为，并针对非独立同分布（non-$i.i.d.$）数据集提供了泛化分析。&lt;h4&gt;背景&lt;/h4&gt;尽管CRL在各种领域取得了显著成功，但其泛化行为的理论理解有限，且现有文献仅分析了在数据元对独立同分布假设下的泛化界限。&lt;h4&gt;目的&lt;/h4&gt;本文旨在对CRL框架在非独立同分布设置下的泛化行为进行理论分析，并提供更符合实际情况的泛化界限。&lt;h4&gt;方法&lt;/h4&gt;本文借鉴U统计学的文献，推导了泛化界限，表明每个类中所需样本的数量与每个类关联的可学习特征表示类的覆盖数的对数成比例。&lt;h4&gt;主要发现&lt;/h4&gt;本文推导了针对线性映射和神经网络等常见函数类的过量风险界限。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为CRL在非独立同分布数据集上的应用提供了理论支持，有助于理解和优化CRL模型的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL is limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Representation Learning (CRL) has achieved impressive success invarious domains in recent years. Nevertheless, the theoretical understanding ofthe generalization behavior of CRL is limited. Moreover, to the best of ourknowledge, the current literature only analyzes generalization bounds under theassumption that the data tuples used for contrastive learning are independentlyand identically distributed. However, in practice, we are often limited to afixed pool of reusable labeled data points, making it inevitable to recycledata across tuples to create sufficiently large datasets. Therefore, thetuple-wise independence condition imposed by previous works is invalidated. Inthis paper, we provide a generalization analysis for the CRL framework undernon-$i.i.d.$ settings that adheres to practice more realistically. Drawinginspiration from the literature on U-statistics, we derive generalizationbounds which indicate the required number of samples in each class scales asthe logarithm of the covering number of the class of learnable featurerepresentations associated to each class. Next, we apply our main results toderive excess risk bounds for common function classes such as linear maps andneural networks.</description>
      <author>example@mail.com (Nong Minh Hieu, Antoine Ledent)</author>
      <guid isPermaLink="false">2505.04937v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PADriver: Towards Personalized Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.05240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。&lt;h4&gt;背景&lt;/h4&gt;目前自动驾驶领域的研究主要集中在基于规则的系统和基于数据的方法，但缺乏对个性化需求的考虑。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够根据个性化需求进行自动驾驶的框架，提高自动驾驶系统的适应性和安全性。&lt;h4&gt;方法&lt;/h4&gt;PADriver使用多模态大型语言模型处理输入数据，自动进行场景理解、危险水平估计和行动决策。同时，构建了基于Highway-Env模拟器的闭环基准PAD-Highway，以评估系统在遵守交通规则下的决策性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;h4&gt;结论&lt;/h4&gt;PADriver能够有效提高个性化自动驾驶系统的性能，为自动驾驶技术的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose PADriver, a novel closed-loop framework forpersonalized autonomous driving (PAD). Built upon Multi-modal Large LanguageModel (MLLM), PADriver takes streaming frames and personalized textual promptsas inputs. It autoaggressively performs scene understanding, danger levelestimation and action decision. The predicted danger level reflects the risk ofthe potential action and provides an explicit reference for the final action,which corresponds to the preset personalized prompt. Moreover, we construct aclosed-loop benchmark named PAD-Highway based on Highway-Env simulator tocomprehensively evaluate the decision performance under traffic rules. Thedataset contains 250 hours videos with high-quality annotation to facilitatethe development of PAD behavior analysis. Experimental results on theconstructed benchmark show that PADriver outperforms state-of-the-artapproaches on different evaluation metrics, and enables various driving modes.</description>
      <author>example@mail.com (Genghua Kou, Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Ziheng Zhang, Osamu Yoshie, Tiancai Wang, Ying Li, Xiangyu Zhang)</author>
      <guid isPermaLink="false">2505.05240v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Stochastic Variational Propagation（SVP）的深度学习算法，作为反向传播（BP）的替代方案，旨在提高深度学习的可扩展性和降低内存开销。&lt;h4&gt;背景&lt;/h4&gt;反向传播在深度学习中至关重要，但其依赖于全局梯度同步，限制了可扩展性并增加了内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP算法，通过将训练重新定义为分层变分推理，以实现可扩展性和降低内存开销。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了避免层间表示崩溃，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP实现了具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并造成了显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，一种可扩展的替代方案，将训练重新定义为分层变分推理。SVP将层激活视为潜在变量，并优化局部证据下界 (ELBOs)，允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能会由于过度压缩而导致层间表示崩溃。为了防止这种情况，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合用于层间一致性的特征对齐损失，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上实现了与BP具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP通过预测链（CoP）和三个关键设计，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计在单目3D物体检测中是一个挑战，因为2D图像到3D空间的映射存在固有模糊性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法MonoCoP，以解决深度估计问题并提高整体准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP使用轻量级属性网络（AN）学习每个3D属性的特征，构建一个显式的预测链来传播这些特征，并使用残差连接聚合特征。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI、Waymo和nuScenes数据集上实现了最先进的性能，且无需额外数据。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测和特征传播，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;翻译&lt;/h4&gt;Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
      <link>http://arxiv.org/abs/2505.04962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and  Embodied Intelligence (MOMA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种系统，用于从有序或无序堆积中自主精确地拾取立方体物体。该系统采用了一种高效的方法来估计立方体物体的姿态，旨在以时间高效的方式减少目标姿态误差。&lt;h4&gt;背景&lt;/h4&gt;传统的姿态估计方法，如全局点云注册，容易产生微小的姿态误差。为了提高姿态精度，通常使用局部注册算法，但这些方法存在执行时间开销和最终姿态误差不确定的问题。&lt;h4&gt;目的&lt;/h4&gt;降低目标姿态误差，并以时间高效的方式进行姿态估计和校正。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的线性时间方法来估计和校正姿态误差，并详细描述了算法的各个模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过新的线性时间方法，可以在保持姿态估计精度的同时，减少执行时间和提高姿态校正的准确性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地减少姿态估计误差，适用于立方体物体的自主拾取场景。&lt;h4&gt;翻译&lt;/h4&gt;The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proposed system outlined in this paper is a solution to a use case thatrequires the autonomous picking of cuboidal objects from an organized orunorganized pile with high precision. This paper presents an efficient methodfor precise pose estimation of cuboid-shaped objects, which aims to reduceerrors in target pose in a time-efficient manner. Typical pose estimationmethods like global point cloud registrations are prone to minor pose errorsfor which local registration algorithms are generally used to improve poseaccuracy. However, due to the execution time overhead and uncertainty in theerror of the final achieved pose, an alternate, linear time approach isproposed for pose error estimation and correction. This paper presents anoverview of the solution followed by a detailed description of individualmodules of the proposed algorithm.</description>
      <author>example@mail.com (Utsav Rai, Hardik Mehta, Vismay Vakharia, Aditya Choudhary, Amit Parmar, Rolif Lima, Kaushik Das)</author>
      <guid isPermaLink="false">2505.04962v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
      <link>http://arxiv.org/abs/2505.05089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025. Project Page:  https://wynelio.github.io/E-NMSTFlow&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E-NMSTFlow的新型无监督事件驱动光流网络，旨在提高长时间序列中的光流估计准确性。&lt;h4&gt;背景&lt;/h4&gt;现有基于学习的事件光流方法多采用基于帧的技术，忽略了事件的空间时间特性，并假设事件间的运动是线性的，这导致长时间序列中的光流误差增加。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在通过利用丰富的时间和空间信息以及事件间的非线性运动，提高事件驱动光流估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Spatio-Temporal Motion Feature Aware (STMFA)模块和一种Adaptive Motion Feature Enhancement (AMFE)模块，这两个模块都利用丰富的时空信息来学习时空数据关联。同时，提出了一种非线性运动补偿损失函数，利用事件间的准确非线性运动来提高网络的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在MVSEC和DSEC-Flow数据集上优于其他无监督学习方法，排名第一。&lt;h4&gt;结论&lt;/h4&gt;E-NMSTFlow网络在长时间序列光流估计方面表现出色，为事件驱动光流估计提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras have the potential to capture continuous motion informationover time and space, making them well-suited for optical flow estimation.However, most existing learning-based methods for event-based optical flowadopt frame-based techniques, ignoring the spatio-temporal characteristics ofevents. Additionally, these methods assume linear motion between consecutiveevents within the loss time window, which increases optical flow errors inlong-time sequences. In this work, we observe that rich spatio-temporalinformation and accurate nonlinear motion between events are crucial forevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novelunsupervised event-based optical flow network focusing on long-time sequences.We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and anAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize richspatio-temporal information to learn spatio-temporal data associations.Meanwhile, we propose a nonlinear motion compensation loss that utilizes theaccurate nonlinear motion between events to improve the unsupervised learningof our network. Extensive experiments demonstrate the effectiveness andsuperiority of our method. Remarkably, our method ranks first amongunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our projectpage is available at https://wynelio.github.io/E-NMSTFlow.</description>
      <author>example@mail.com (Zuntao Liu, Hao Zhuang, Junjie Jiang, Yuhang Song, Zheng Fang)</author>
      <guid isPermaLink="false">2505.05089v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks</title>
      <link>http://arxiv.org/abs/2505.04981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的深度强化学习（DRL）算法，用于动态THz无人机网络中的资源分配，以最大化资源效率。&lt;h4&gt;背景&lt;/h4&gt;THz无人机网络具有灵活的拓扑结构和超高速数据传输能力，在安全监控、灾害响应和环境保护等领域具有广泛应用前景。然而，动态拓扑结构给无人机之间THz链路的长期联合功率和天线阵列资源分配带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出GLOVE算法，通过GNN学习无人机与其邻近无人机之间的关系，同时强调无人机自身的特征，以实现资源效率的最大化。&lt;h4&gt;方法&lt;/h4&gt;GLOVE算法通过图神经网络（GNN）学习无人机与其邻近无人机之间的交互关系，并利用多任务结构协同训练所有无人机功率和子阵列的资源分配决策。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GLOVE算法在资源效率最高和延迟最低方面优于基准方案，并且在整个训练过程中保持了零数据包丢失，显示出其在高度动态THz无人机网络中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GLOVE算法能够有效解决动态THz无人机网络中的资源分配问题，为无人机网络的应用提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexibletopologies and ultra-high data rates are expected to empower numerousapplications in security surveillance, disaster response, and environmentalmonitoring, among others. However, the dynamic topologies hinder the efficientlong-term joint power and antenna array resource allocation for THz links amongUAVs. Furthermore, the continuous nature of power and the discrete nature ofantennas cause this joint resource allocation problem to be a mixed-integernonlinear programming (MINLP) problem with non-convexity and NP-hardness.Inspired by recent rapid advancements in deep reinforcement learning (DRL), agraph neural network (GNN) aided DRL algorithm for resource allocation in thedynamic THz UAV network with an emphasis on self-node features (GLOVE) isproposed in this paper, with the aim of resource efficiency (RE) maximization.When training the allocation policy for each UAV, GLOVE learns the relationshipbetween this UAV and its neighboring UAVs via GNN, while also emphasizing theimportant self-node features of this UAV. In addition, a multi-task structureis leveraged by GLOVE to cooperatively train resource allocation decisions forthe power and sub-arrays of all UAVs. Experimental results illustrate thatGLOVE outperforms benchmark schemes in terms of the highest RE and the lowestlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVEmaintains zero packet loss during the entire training process, demonstratingits better robustness under the highly dynamic THz UAV network.</description>
      <author>example@mail.com (Zhifeng Hu, Chong Han)</author>
      <guid isPermaLink="false">2505.04981v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
      <link>http://arxiv.org/abs/2505.05467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamBridge是一种将离线视频语言模型（Video-LLMs）转换为流式模型的简单而有效的框架。&lt;h4&gt;背景&lt;/h4&gt;将现有模型应用于在线场景时，存在两个主要挑战：多轮实时理解和缺乏主动响应机制。&lt;h4&gt;目的&lt;/h4&gt;StreamBridge旨在解决上述挑战，提高离线视频语言模型在流式场景下的理解能力。&lt;h4&gt;方法&lt;/h4&gt;StreamBridge包含以下特点：(1) 结合内存缓冲区和圆周衰减压缩策略，支持长上下文的多轮交互；(2) 解耦的轻量级激活模型，易于集成到现有的Video-LLMs中，实现连续的主动响应。同时，构建了大规模数据集Stream-IT，用于流式视频理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，StreamBridge在多种任务中显著提高了离线视频语言模型的流式理解能力，甚至在某些方面超过了GPT-4o和Gemini1.5 Pro等专有模型。在标准视频理解基准测试中，它也实现了有竞争力的或优越的性能。&lt;h4&gt;结论&lt;/h4&gt;StreamBridge是一个有效的框架，可以显著提升离线视频语言模型的流式理解能力，并在多个任务中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present StreamBridge, a simple yet effective framework that seamlesslytransforms offline Video-LLMs into streaming-capable models. It addresses twofundamental challenges in adapting existing models into online scenarios: (1)limited capability for multi-turn real-time understanding, and (2) lack ofproactive response mechanisms. Specifically, StreamBridge incorporates (1) amemory buffer combined with a round-decayed compression strategy, supportinglong-context multi-turn interactions, and (2) a decoupled, lightweightactivation model that can be effortlessly integrated into existing Video-LLMs,enabling continuous proactive responses. To further support StreamBridge, weconstruct Stream-IT, a large-scale dataset tailored for streaming videounderstanding, featuring interleaved video-text sequences and diverseinstruction formats. Extensive experiments show that StreamBridge significantlyimproves the streaming understanding capabilities of offline Video-LLMs acrossvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini1.5 Pro. Simultaneously, it achieves competitive or superior performance onstandard video understanding benchmarks.</description>
      <author>example@mail.com (Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, Ping Huang)</author>
      <guid isPermaLink="false">2505.05467v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.04907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为VaCDA的多源域适应框架，旨在解决可穿戴设备产生的海量未标注数据带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着科技的发展，可穿戴设备中的传感器持续监控用户活动，生成大量未标注数据。这些数据难以解释，手动标注既费时又易出错。数据分布因设备放置、类型和使用行为的不同而异，传统迁移学习方法效果不佳，难以识别日常活动。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，VaCDA结合了变分自编码器（VAE）和对比学习，以改善特征表示并减少源域和目标域之间的异质性。&lt;h4&gt;方法&lt;/h4&gt;VaCDA使用VAE从可用传感器数据中学习一个共享的低维潜在空间，该空间可以泛化不同传感器之间的数据。同时，通过对比学习，将同一类别的实例在域间对齐，同时分离不同类别，以增强特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开数据集上，VaCDA在跨位置和跨设备场景中优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;VaCDA在处理可穿戴设备数据方面表现出色，特别是在解决数据异质性和提升特征表示方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：技术进步导致了带有传感器的可穿戴设备的兴起，这些设备持续监控用户活动，生成大量未标记数据。这些数据难以解释，手动标注既费力又容易出错。由于设备放置、类型和使用行为的差异，数据分布往往异质。因此，传统的迁移学习方法表现不佳，难以识别日常活动。为了解决这些挑战，我们使用变分自编码器（VAE）从可用的传感器数据中学习一个共享的低维潜在空间。这个空间可以泛化不同传感器之间的数据，减轻异质性，并帮助目标域的鲁棒适应。我们通过对比学习来增强特征表示，通过对齐跨域的同一类实例同时分离不同类别。我们提出了变分对比域适应（VaCDA），这是一个结合VAE和对比学习的多源域适应框架，旨在改善特征表示并减少源域和目标域之间的异质性。我们在三个异质场景（跨个人、跨位置和跨设备）的多个公开数据集上评估了VaCDA。在跨位置和跨设备场景中，VaCDA优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technological advancements have led to the rise of wearable devices withsensors that continuously monitor user activities, generating vast amounts ofunlabeled data. This data is challenging to interpret, and manual annotation islabor-intensive and error-prone. Additionally, data distribution is oftenheterogeneous due to device placement, type, and user behavior variations. As aresult, traditional transfer learning methods perform suboptimally, making itdifficult to recognize daily activities. To address these challenges, we use avariational autoencoder (VAE) to learn a shared, low-dimensional latent spacefrom available sensor data. This space generalizes data across diverse sensors,mitigating heterogeneity and aiding robust adaptation to the target domain. Weintegrate contrastive learning to enhance feature representation by aligninginstances of the same class across domains while separating different classes.We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-sourcedomain adaptation framework combining VAEs and contrastive learning to improvefeature representation and reduce heterogeneity between source and targetdomains. We evaluate VaCDA on multiple publicly available datasets across threeheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDAoutperforms the baselines in cross-position and cross-device scenarios.</description>
      <author>example@mail.com (Soham Khisa, Avijoy Chakma)</author>
      <guid isPermaLink="false">2505.04907v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction</title>
      <link>http://arxiv.org/abs/2505.05094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究旨在通过结合联合图学习和网络分析来解决高血压共病问题的早期识别挑战。&lt;h4&gt;背景&lt;/h4&gt;高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;方法&lt;/h4&gt;该方法包括：a) 基于疾病编码构建两个网络，包括患者网络和疾病差异网络；b) 生成三个共病网络特征以捕捉共病与风险疾病之间的潜在关系；c) 结合计算结构干预和学习特征表示，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;主要发现&lt;/h4&gt;基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。&lt;h4&gt;结论&lt;/h4&gt;CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;h4&gt;翻译&lt;/h4&gt;本研究旨在通过结合联合图学习与网络分析解决高血压共病问题的早期识别挑战。研究背景指出，高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。研究目的在于开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。研究方法包括基于疾病编码构建患者网络和疾病差异网络，生成共病网络特征，并结合计算结构干预和学习特征表示。主要发现表明，基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。研究结论指出，CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The comorbidities of hypertension impose a heavy burden on patients andsociety. Early identification is necessary to prompt intervention, but itremains a challenging task. This study aims to address this challenge bycombining joint graph learning with network analysis. Motivated by thisdiscovery, we develop a Conjoint Graph Representation Learning (CGRL) frameworkthat: a) constructs two networks based on disease coding, including the patientnetwork and the disease difference network. Three comorbidity network featureswere generated based on the basic difference network to capture the potentialrelationship between comorbidities and risk diseases; b) incorporatescomputational structure intervention and learning feature representation, CGRLwas developed to predict the risks of diabetes and coronary heart disease inpatients; and c) analysis the comorbidity patterns and exploring the pathwaysof disease progression, the pathological pathogenesis of diabetes and coronaryheart disease may be revealed. The results show that the network featuresextracted based on the difference network are important, and the framework weproposed provides more accurate predictions than other strong models in termsof accuracy.</description>
      <author>example@mail.com (Leming Zhou, Zuo Wang, Zhixuan Duan)</author>
      <guid isPermaLink="false">2505.05094v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Does CLIP perceive art the same way we do?</title>
      <link>http://arxiv.org/abs/2505.05229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力，同时探讨了其与人类感知和上下文理解的一致性。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一个能够通过联合嵌入连接图像和文本的强大多模态模型。&lt;h4&gt;目的&lt;/h4&gt;研究CLIP在提取绘画（包括人类和AI生成的图像）中的高层次语义和风格信息的能力。&lt;h4&gt;方法&lt;/h4&gt;设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，以探索其与人类感知和上下文理解的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。&lt;h4&gt;结论&lt;/h4&gt;讨论了这些发现对于将CLIP用作生成过程（如风格迁移或基于提示的图像合成）中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力。通过设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，探讨了其与人类感知和上下文理解的一致性。研究揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。进一步讨论了这些发现对于将CLIP用作生成过程中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP has emerged as a powerful multimodal model capable of connecting imagesand text through joint embeddings, but to what extent does it "see" the sameway humans do - especially when interpreting artworks? In this paper, weinvestigate CLIP's ability to extract high-level semantic and stylisticinformation from paintings, including both human-created and AI-generatedimagery. We evaluate its perception across multiple dimensions: content, sceneunderstanding, artistic style, historical period, and the presence of visualdeformations or artifacts. By designing targeted probing tasks and comparingCLIP's responses to human annotations and expert benchmarks, we explore itsalignment with human perceptual and contextual understanding. Our findingsreveal both strengths and limitations in CLIP's visual representations,particularly in relation to aesthetic cues and artistic intent. We furtherdiscuss the implications of these insights for using CLIP as a guidancemechanism during generative processes, such as style transfer or prompt-basedimage synthesis. Our work highlights the need for deeper interpretability inmultimodal systems, especially when applied to creative domains where nuanceand subjectivity play a central role.</description>
      <author>example@mail.com (Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu)</author>
      <guid isPermaLink="false">2505.05229v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
      <link>http://arxiv.org/abs/2505.04846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at the Platform for Advanced Scientific  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HiPerRAG的RAG工作流程，通过高性能计算技术索引和检索超过360万篇科学文献中的知识，旨在解决科学文献量增长带来的问题。&lt;h4&gt;背景&lt;/h4&gt;科学文献量呈指数增长，导致发现未被充分利用、重复工作和跨学科合作受限。&lt;h4&gt;目的&lt;/h4&gt;通过提高大语言模型处理信息的事实性来协助科学家。&lt;h4&gt;方法&lt;/h4&gt;HiPerRAG利用高性能计算，包括Oreo模型进行多模态文档解析和ColTrast查询感知编码器算法来增强检索准确性。&lt;h4&gt;主要发现&lt;/h4&gt;HiPerRAG在现有的科学问答基准和两个新基准上表现出色，SciQ准确率达到90%，PubMedQA准确率达到76%，超过了PubMedGPT和GPT-4等特定领域的模型和商业LLM。&lt;h4&gt;结论&lt;/h4&gt;HiPerRAG在Polaris、Sunspot和Frontier超级计算机上扩展到数千个GPU，提供了百万文档规模的RAG工作流程，以统一科学知识和促进跨学科创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3732775.3733586&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The volume of scientific literature is growing exponentially, leading tounderutilized discoveries, duplicated efforts, and limited cross-disciplinarycollaboration. Retrieval Augmented Generation (RAG) offers a way to assistscientists by improving the factuality of Large Language Models (LLMs) inprocessing this influx of information. However, scaling RAG to handle millionsof articles introduces significant challenges, including the high computationalcosts associated with parsing documents and embedding scientific knowledge, aswell as the algorithmic complexity of aligning these representations with thenuanced semantics of scientific content. To address these issues, we introduceHiPerRAG, a RAG workflow powered by high performance computing (HPC) to indexand retrieve knowledge from more than 3.6 million scientific articles. At itscore are Oreo, a high-throughput model for multimodal document parsing, andColTrast, a query-aware encoder fine-tuning algorithm that enhances retrievalaccuracy by using contrastive learning and late-interaction techniques.HiPerRAG delivers robust performance on existing scientific question answeringbenchmarks and two new benchmarks introduced in this work, achieving 90%accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific modelslike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUson the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers milliondocument-scale RAG workflows for unifying scientific knowledge and fosteringinterdisciplinary innovation.</description>
      <author>example@mail.com (Ozan Gokdemir, Carlo Siebenschuh, Alexander Brace, Azton Wells, Brian Hsu, Kyle Hippe, Priyanka V. Setty, Aswathy Ajith, J. Gregory Pauloski, Varuni Sastry, Sam Foreman, Huihuo Zheng, Heng Ma, Bharat Kale, Nicholas Chia, Thomas Gibbs, Michael E. Papka, Thomas Brettin, Francis J. Alexander, Anima Anandkumar, Ian Foster, Rick Stevens, Venkatram Vishwanath, Arvind Ramanathan)</author>
      <guid isPermaLink="false">2505.04846v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
      <link>http://arxiv.org/abs/2505.05001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:  text overlap with arXiv:2403.06378&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视频拼接框架StabStitch++，用于解决视频拼接中出现的扭曲抖动问题，并通过实验证明其在性能、鲁棒性和效率方面优于现有解决方案。&lt;h4&gt;背景&lt;/h4&gt;视频拼接过程中，由于图像扭曲不连续，即使输入视频稳定，拼接后的视频也可能出现扭曲抖动，影响视觉体验。&lt;h4&gt;目的&lt;/h4&gt;提出一种视频拼接框架，实现空间拼接和时序稳定性的同时优化。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一个可微分的双向分解模块，将单应性变换解耦并融入空间扭曲中，平衡两个视图的对齐负担和投影畸变。2. 从视频稳定中的摄像机路径得到视频拼接轨迹的数学表达式。3. 提出了一个扭曲平滑模型，通过混合损失函数同时促进内容对齐、轨迹平滑和在线协作。&lt;h4&gt;主要发现&lt;/h4&gt;StabStitch++在在线模式下同时优化了对齐和稳定性，优于牺牲对齐以换取稳定性的StabStitch。&lt;h4&gt;结论&lt;/h4&gt;StabStitch++通过构建实时在线视频拼接系统，在视频拼接性能、鲁棒性和效率方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nie-lang/stabstitch2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We retarget video stitching to an emerging issue, named warping shake, whichunveils the temporal content shakes induced by sequentially unsmooth warps whenextending image stitching to video stitching. Even if the input videos arestable, the stitched video can inevitably cause undesired warping shakes andaffect the visual experience. To address this issue, we propose StabStitch++, anovel video stitching framework to realize spatial stitching and temporalstabilization with unsupervised learning simultaneously. First, different fromexisting learning-based image stitching solutions that typically warp one imageto align with another, we suppose a virtual midplane between original imageplanes and project them onto it. Concretely, we design a differentiablebidirectional decomposition module to disentangle the homography transformationand incorporate it into our spatial warp, evenly spreading alignment burdensand projective distortions across two views. Then, inspired by camera paths invideo stabilization, we derive the mathematical expression of stitchingtrajectories in video stitching by elaborately integrating spatial and temporalwarps. Finally, a warp smoothing model is presented to produce stable stitchedvideos with a hybrid loss to simultaneously encourage content alignment,trajectory smoothness, and online collaboration. Compared with StabStitch thatsacrifices alignment for stabilization, StabStitch++ makes no compromise andoptimizes both of them simultaneously, especially in the online mode. Toestablish an evaluation benchmark and train the learning framework, we build avideo stitching dataset with a rich diversity in camera motions and scenes.Experiments exhibit that StabStitch++ surpasses current solutions in stitchingperformance, robustness, and efficiency, offering compelling advancements inthis field by building a real-time online video stitching system.</description>
      <author>example@mail.com (Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao)</author>
      <guid isPermaLink="false">2505.05001v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mogao，一个通过因果方法实现交错多模态生成的统一框架，在图像理解和生成方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;当前统一模型在图像理解和生成方面取得了一定的进步，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，通过交错多模态生成来推进这一范式。&lt;h4&gt;方法&lt;/h4&gt;Mogao集成了架构设计方面的多项关键技术改进，包括深度融合设计、双视觉编码器、交错旋转位置嵌入和多模态无分类器引导，以及在大规模内部数据集上的高效训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，并且在产生高质量、连贯的交错输出方面表现出色。它在零样本图像编辑和组合生成方面的能力突显了Mogao作为一个实用的全模态基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;Mogao作为一个实用的全模态基础模型，为未来统一多模态系统的发展铺平了道路，并有望进一步扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation</title>
      <link>http://arxiv.org/abs/2505.05085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了通过传递和Koopman算子方法将复杂非线性动力学系统线性化表示的框架，并提出了一种通过机器学习优化线性算子的近似表示方法。&lt;h4&gt;背景&lt;/h4&gt;传递和Koopman算子方法为表示非线性动力学系统提供了一种线性变换的框架，但直接从数据中高效地估计这些算子的谱是具有挑战性的。&lt;h4&gt;目的&lt;/h4&gt;目的是通过一般算子和表征学习的方法，使用高效的有限维表示来近似这些线性算子。&lt;h4&gt;方法&lt;/h4&gt;使用机器学习技术学习正交归一且局部支撑的基函数，这些基函数根据系统的动力学特性动态定制。这种学习到的基函数提供了算子操作的准确近似以及几乎不变的有限维子空间。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，可以检索估计算子的谱属性，并强调机器学习基函数的动态自适应特性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种基于机器学习的方法来优化线性算子的近似表示，为理解复杂非线性动力学系统的动力学提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling for a deeper understanding of the underlying dynamics. The spectrum of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We tackle this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal, locally supported basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasize the dynamically adaptive quality of the machine-learned basis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer and Koopman operator methods offer a framework for representingcomplex, nonlinear dynamical systems via linear transformations, enabling for adeeper understanding of the underlying dynamics. The spectrum of theseoperators provide important insights into system predictability and emergentbehaviour, although efficiently estimating them from data can be challenging.We tackle this issue through the lens of general operator and representationallearning, in which we approximate these linear operators using efficientfinite-dimensional representations. Specifically, we machine-learn orthonormal,locally supported basis functions that are dynamically tailored to the system.This learned basis provides a particularly accurate approximation of theoperator's action as well as a nearly invariant finite-dimensional subspace. Weillustrate our approach with examples that showcase the retrieval of spectralproperties from the estimated operator, and emphasise the dynamically adaptivequality of the machine-learned basis.</description>
      <author>example@mail.com (Gary Froyland, Kevin Kühl)</author>
      <guid isPermaLink="false">2505.05085v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.04659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GSsplat的可泛化语义高斯混合方法，用于高效地生成未见过的场景的语义合成。&lt;h4&gt;背景&lt;/h4&gt;在3D场景理解的研究中，从多个视角对未见过的场景进行语义合成至关重要。当前的方法在渲染新视角图像和语义图方面表现良好，但速度和分割性能存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了提高合成速度和分割性能，提出了GSsplat方法。&lt;h4&gt;方法&lt;/h4&gt;GSsplat通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定的高斯混合中的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息，并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。&lt;h4&gt;主要发现&lt;/h4&gt;GSsplat在多视角输入变化的情况下，以最快的速度实现了语义合成中的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;GSsplat方法在提高合成速度和分割性能方面取得了显著的成果，为3D场景理解研究提供了有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：未见场景的语义合成从多个视角对3D场景理解研究至关重要。当前的方法能够通过重建可泛化的神经辐射场来渲染新视角的图像和语义图。然而，它们往往在速度和分割性能方面存在限制。我们提出了一种可泛化的语义高斯混合方法（GSsplat）用于高效的novel-view合成。我们的模型通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定高斯混合的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。当用变化数量的多视角输入进行评估时，GSsplat在语义合成方面达到了最快的速度和最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The semantic synthesis of unseen scenes from multiple viewpoints is crucialfor research in 3D scene understanding. Current methods are capable ofrendering novel-view images and semantic maps by reconstructing generalizableNeural Radiance Fields. However, they often suffer from limitations in speedand segmentation performance. We propose a generalizable semantic GaussianSplatting method (GSsplat) for efficient novel-view synthesis. Our modelpredicts the positions and attributes of scene-adaptive Gaussian distributionsfrom once input, replacing the densification and pruning processes oftraditional scene-specific Gaussian Splatting. In the multi-task framework, ahybrid network is designed to extract color and semantic information andpredict Gaussian parameters. To augment the spatial perception of Gaussians forhigh-quality rendering, we put forward a novel offset learning module throughgroup-based supervision and a point-level interaction module with spatial unitaggregation. When evaluated with varying numbers of multi-view inputs, GSsplatachieves state-of-the-art performance for semantic synthesis at the fastestspeed.</description>
      <author>example@mail.com (Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang)</author>
      <guid isPermaLink="false">2505.04659v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
      <link>http://arxiv.org/abs/2505.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对远视图像中密集分布的小孔检测的适应性检测方法，旨在提高爆破作业的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;现有的检测方法在处理小尺度目标、高度密集的排列和有限的小孔视觉特征时存在困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了基于现有架构（如YOLO）的适应性检测方法，该方法通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;该方法引入了三个协同组件：(1)自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；(2)嵌入稳定化，确保一致且可靠的特征提取；(3)上下文细化，利用空间上下文提高检测精度。&lt;h4&gt;主要发现&lt;/h4&gt;EMA在本方法中的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。&lt;h4&gt;结论&lt;/h4&gt;在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在控制的爆破作业中，从远视图像中准确检测密集分布的小孔对于操作安全和效率至关重要。然而，现有的检测方法由于小目标尺度、高度密集的排列和小孔的有限视觉特征而常常遇到困难。为了解决这些挑战，我们提出了一种基于现有架构（例如YOLO）的适应性检测方法，该方法通过通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。我们的方法引入了三个协同组件：（1）自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；（2）嵌入稳定化，确保一致且可靠的特征提取；（3）上下文细化，利用空间上下文提高检测精度。在本方法中EMA的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In controlled blasting operations, accurately detecting densely distributedtiny boreholes from far-view imagery is critical for operational safety andefficiency. However, existing detection methods often struggle due to smallobject scales, highly dense arrangements, and limited distinctive visualfeatures of boreholes. To address these challenges, we propose an adaptivedetection approach that builds upon existing architectures (e.g., YOLO) byexplicitly leveraging consistent embedding representations derived throughexponential moving average (EMA)-based statistical updates.  Our method introduces three synergistic components: (1) adaptive augmentationutilizing dynamically updated image statistics to robustly handle illuminationand texture variations; (2) embedding stabilization to ensure consistent andreliable feature extraction; and (3) contextual refinement leveraging spatialcontext for improved detection accuracy. The pervasive use of EMA in our methodis particularly advantageous given the limited visual complexity and smallscale of boreholes, allowing stable and robust representation learning evenunder challenging visual conditions. Experiments on a challenging proprietaryquarry-site dataset demonstrate substantial improvements over baselineYOLO-based architectures, highlighting our method's effectiveness in realisticand complex industrial scenarios.</description>
      <author>example@mail.com (Xuesong Liu, Tianyu Hao, Emmett J. Ientilucci)</author>
      <guid isPermaLink="false">2505.05008v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FA-KPConv，一种基于KPConv的神经网络架构，用于3D点云分析。通过帧平均技术，FA-KPConv使网络对点云的平移、旋转和反射具有精确的不变性和/或等变性，提高了点云分类和点云注册的性能。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其对欧几里得变换的不变性和/或等变性只能在大数据集或数据增强的情况下近似实现。&lt;h4&gt;目的&lt;/h4&gt;通过引入帧平均技术，使点云神经网络对平移、旋转和/或反射具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;FA-KPConv通过在现有的KPConv网络基础上添加帧平均技术，将几何先验知识嵌入到网络中，同时保持可学习参数的数量，不牺牲任何输入信息。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中展现出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv通过引入帧平均技术，提高了点云神经网络对欧几里得变换的不变性和/或等变性，从而提高了点云分类和点云注册的性能。&lt;h4&gt;翻译&lt;/h4&gt;We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks</title>
      <link>http://arxiv.org/abs/2505.04894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE DCOSS-IoT 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TH-GCN的图卷积网络方法，用于优化密集5G网络中的切换管理，旨在提高网络稳定性。&lt;h4&gt;背景&lt;/h4&gt;5G技术的快速发展改变了车载网络，提供了高带宽、低延迟和快速数据率，这些特性对于智能城市和车辆中的实时应用至关重要，同时也提高了交通安全和娱乐服务。&lt;h4&gt;目的&lt;/h4&gt;解决5G网络在覆盖范围有限和频繁切换导致的网络不稳定问题，尤其是在高移动性环境中。&lt;h4&gt;方法&lt;/h4&gt;TH-GCN使用图神经网络（GNN）将车辆和基站建模为动态图中的节点，该图包含信号质量、吞吐量、车辆速度和基站负载等特征。通过整合用户设备和基站的角度，实现自适应的实时切换决策。&lt;h4&gt;主要发现&lt;/h4&gt;TH-GCN将切换次数减少了多达78%，并将信号质量提高了10%，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TH-GCN通过优化切换管理，显著提高了5G网络的稳定性和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of 5G has transformed vehicular networks, offering highbandwidth, low latency, and fast data rates essential for real-timeapplications in smart cities and vehicles. These improvements enhance trafficsafety and entertainment services. However, the limited coverage and frequenthandovers in 5G networks cause network instability, especially in high-mobilityenvironments due to the ping-pong effect. This paper presents TH-GCN(Throughput-oriented Graph Convolutional Network), a novel approach foroptimizing handover management in dense 5G networks. Using graph neuralnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamicgraph enriched with features such as signal quality, throughput, vehicle speed,and base station load. By integrating both user equipment and base stationperspectives, this dual-centric approach enables adaptive, real-time handoverdecisions that improve network stability. Simulation results show that TH-GCNreduces handovers by up to 78 percent and improves signal quality by 10percent, outperforming existing methods.</description>
      <author>example@mail.com (Nazanin Mehregan, Robson E. De Grande)</author>
      <guid isPermaLink="false">2505.04894v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graffe: Graph Representation Learning via Diffusion Probabilistic Models</title>
      <link>http://arxiv.org/abs/2505.04956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Graffe，一种用于图表示学习的自监督扩散模型。该模型通过图编码器将源图压缩成紧凑表示，该表示作为条件引导扩散解码器的去噪过程。实验表明，Graffe在节点和图分类任务上取得了竞争性结果，并在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;扩散概率模型（DPMs）因其生成高质量样本的潜力而广为人知，但在表示学习领域往往被忽视。尽管最近的研究突出了其在捕获视觉语义方面的潜力，但将DPMs应用于图表示学习仍然处于初级阶段。&lt;h4&gt;目的&lt;/h4&gt;提出Graffe模型，旨在将扩散模型应用于图表示学习，并评估其有效性。&lt;h4&gt;方法&lt;/h4&gt;Graffe模型包括一个图编码器和一个扩散解码器。图编码器将源图压缩成紧凑表示，该表示作为条件来引导扩散解码器的去噪过程。通过理论证明和实践验证来评估模型的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明表明，去噪目标隐式最大化了数据和其表示之间的条件互信息。在实证研究中，Graffe在节点和图分类任务上取得了优异的性能，在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;结论&lt;/h4&gt;扩散模型，尤其是扩散概率模型，是图表示学习的有效工具，特别是Graffe模型在图表示学习方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;Diffusion probabilistic models (DPMs), widely recognized for their potential to generate high-quality samples, tend to go unnoticed in representation learning. While recent progress has highlighted their potential for capturing visual semantics, adapting DPMs to graph representation learning remains in its infancy. In this paper, we introduce Graffe, a self-supervised diffusion model proposed for graph representation learning. It features a graph encoder that distills a source graph into a compact representation, which, in turn, serves as the condition to guide the denoising process of the diffusion decoder. To evaluate the effectiveness of our model, we first explore the theoretical foundations of applying diffusion models to representation learning, proving that the denoising objective implicitly maximizes the conditional mutual information between data and its representation. Specifically, we prove that the negative logarithm of the denoising score matching loss is a tractable lower bound for the conditional mutual information. Empirically, we conduct a series of case studies to validate our theoretical insights. In addition, Graffe delivers competitive results under the linear probing setting on node and graph classification tasks, achieving state-of-the-art performance on 9 of the 11 real-world datasets. These findings indicate that powerful generative models, especially diffusion models, serve as an effective tool for graph representation learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion probabilistic models (DPMs), widely recognized for their potentialto generate high-quality samples, tend to go unnoticed in representationlearning. While recent progress has highlighted their potential for capturingvisual semantics, adapting DPMs to graph representation learning remains in itsinfancy. In this paper, we introduce Graffe, a self-supervised diffusion modelproposed for graph representation learning. It features a graph encoder thatdistills a source graph into a compact representation, which, in turn, servesas the condition to guide the denoising process of the diffusion decoder. Toevaluate the effectiveness of our model, we first explore the theoreticalfoundations of applying diffusion models to representation learning, provingthat the denoising objective implicitly maximizes the conditional mutualinformation between data and its representation. Specifically, we prove thatthe negative logarithm of the denoising score matching loss is a tractablelower bound for the conditional mutual information. Empirically, we conduct aseries of case studies to validate our theoretical insights. In addition,Graffe delivers competitive results under the linear probing setting on nodeand graph classification tasks, achieving state-of-the-art performance on 9 ofthe 11 real-world datasets. These findings indicate that powerful generativemodels, especially diffusion models, serve as an effective tool for graphrepresentation learning.</description>
      <author>example@mail.com (Dingshuo Chen, Shuchen Xue, Liuji Chen, Yingheng Wang, Qiang Liu, Shu Wu, Zhi-Ming Ma, Liang Wang)</author>
      <guid isPermaLink="false">2505.04956v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法。在此基础上，主要目标是开发高性能、适用于实际临床环境的自动疼痛评估计算方法。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论角度出发，对疼痛评估过程进行了研究，并考察了现有的自动方法。&lt;h4&gt;目的&lt;/h4&gt;开发创新计算方法，实现高性能的自动疼痛评估，并在实际临床环境中应用。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法，深入研究和评估影响疼痛感知的显著因素，包括人口统计学元素，并设计、开发、提出适用于不同场景需求的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中提出的方法在疼痛评估方面取得了最先进的结果，并展示了其有效性。&lt;h4&gt;结论&lt;/h4&gt;论文为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract:  This thesis initially aims to study the pain assessment process from aclinical-theoretical perspective while exploring and examining existingautomatic approaches. Building on this foundation, the primary objective ofthis Ph.D. project is to develop innovative computational methods for automaticpain assessment that achieve high performance and are applicable in realclinical settings. A primary goal is to thoroughly investigate and assesssignificant factors, including demographic elements that impact painperception, as recognized in pain research, through a computational standpoint.Within the limits of the available data in this research area, our goal was todesign, develop, propose, and offer automatic pain assessment pipelines forunimodal and multimodal configurations that are applicable to the specificrequirements of different scenarios. The studies published in this Ph.D. thesisshowcased the effectiveness of the proposed methods, achieving state-of-the-artresults. Additionally, they paved the way for exploring new approaches inartificial intelligence, foundation models, and generative artificialintelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Method for Radio Interferometry Deconvolution</title>
      <link>http://arxiv.org/abs/2505.04887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了如何利用压缩感知（CS）理论解决射电干涉仪空间频率采样不完整的问题，提出了一种基于深度字典的全新无监督学习方法，以实现对天体信息的高精度恢复。&lt;h4&gt;背景&lt;/h4&gt;射电干涉仪的空间频率采样不完整导致天体信息恢复困难，压缩感知理论提供了一种稳定且唯一恢复天空中亮度分布的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于压缩感知理论的方法，以实现对射电干涉仪观测到的天空中亮度分布的精确恢复。&lt;h4&gt;方法&lt;/h4&gt;开发了一个深度字典（通过卷积神经网络实现），该字典是多分辨率和过完备的，以实现稀疏表示，并将其整合到压缩感知框架中。在去卷积过程中，模型图像和深度字典交替更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地从噪声测量中恢复具有复杂形态的扩展源，与现有算法相比，动态范围（DR）几乎提高了45到100倍。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法结合了压缩感知的数学严谨性和深度神经网络的表 达能力，在解决射电干涉仪观测数据恢复问题上取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical information remains challenging. To address this ill-posed problem, compressive sensing (CS) provides a robust framework for stable and unique recovery of sky brightness distributions in noisy environments, contingent upon satisfying specific conditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparse representation. Building on this insight, we develop a deep dictionary (realized through a convolutional neural network), which is designed to be multi-resolution and overcomplete, to achieve sparse representation and integrate it within the CS framework. The resulting method is a novel, fully interpretable unsupervised learning approach that combines the mathematical rigor of CS with the expressive power of deep neural networks, effectively bridging the gap between deep learning and classical dictionary methods. During the deconvolution process, the model image and the deep dictionary are updated alternately. This approach enables efficient and accurate recovery of extended sources with complex morphologies from noisy measurements. Comparative analyses with state-of-the-art algorithms demonstrate the outstanding performance of our method, i.e., achieving a dynamic range (DR) nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3847/1538-4365/add1b7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical informationremains challenging. To address this ill-posed problem, compressive sensing(CS)provides a robust framework for stable and unique recovery of sky brightnessdistributions in noisy environments, contingent upon satisfying specificconditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparserepresentation. {{Building on this insight, we develop a deep dictionary(realized through a convolutional neural network), which is designed to bemulti-resolution and overcomplete, to achieve sparse representation andintegrate it within the CS framework. The resulting method is a novel, fullyinterpretable unsupervised learning approach that combines}} the mathematicalrigor of CS with the expressive power of deep neural networks, effectivelybridging the gap between deep learning and classical dictionary methods.{{During the deconvolution process, the model image and the deep dictionary areupdated alternatively.}} This approach enables efficient and accurate recoveryof extended sources with complex morphologies from noisy measurements.Comparative analyses with state-of-the-art algorithms demonstrate theoutstanding performance of our method, i.e., achieving a dynamic range (DR)nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).</description>
      <author>example@mail.com (Lei Yu, Bin Liu, Cheng-Jin Jin, Ru-Rong Chen, Hong-Wei Xi, Bo Peng)</author>
      <guid isPermaLink="false">2505.04887v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
      <link>http://arxiv.org/abs/2505.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在视网膜成像中，基于自监督学习的Vision Transformers（ViTs）在AMD识别任务上的表现，并探讨了领域内预训练的必要性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习使ViTs能够从大规模自然图像数据集中学习鲁棒表示，增强了其在不同领域的泛化能力。在视网膜成像中，基于自然或眼科数据的预训练模型显示出潜力，但领域内预训练的好处尚未确定。&lt;h4&gt;目的&lt;/h4&gt;为了调查领域内预训练的好处，本研究在七个包含70,000张专家标注的DFI数据集上对六个SSL预训练的ViTs进行了基准测试，以识别中晚期AMD。&lt;h4&gt;方法&lt;/h4&gt;研究者对自然图像预训练的iBOT、领域特定模型以及未预训练的ViT-L进行了比较，并发布了BRAMD，一个包含巴西DFI和AMD标签的公开数据集。&lt;h4&gt;主要发现&lt;/h4&gt;自然图像预训练的iBOT在分布外泛化方面表现最佳，其AUROCs为0.80-0.97，优于领域特定模型（AUROCs为0.78-0.96）和未预训练的ViT-L（AUROCs为0.68-0.91）。这些发现强调了基础模型在提高AMD识别价值中的重要性，并挑战了领域内预训练的必要性。&lt;h4&gt;结论&lt;/h4&gt;领域内预训练可能不是必要的，基础模型能够有效提高AMD识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) tolearn robust representations from large-scale natural image datasets, enhancingtheir generalization across domains. In retinal imaging, foundation modelspretrained on either natural or ophthalmic data have shown promise, but thebenefits of in-domain pretraining remain uncertain. To investigate this, webenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasetstotaling 70,000 expert-annotated images for the task of moderate-to-lateage-related macular degeneration (AMD) identification. Our results show thatiBOT pretrained on natural images achieves the highest out-of-distributiongeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,which achieved AUROCs of 0.68-0.91. These findings highlight the value offoundation models in improving AMD identification and challenge the assumptionthat in-domain pretraining is necessary. Furthermore, we release BRAMD, anopen-access dataset (n=587) of DFIs with AMD labels from Brazil.</description>
      <author>example@mail.com (Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar)</author>
      <guid isPermaLink="false">2505.05291v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Network Digital Twin for Route Optimization in 5G/B5G Transport Slicing with What-If Analysis</title>
      <link>http://arxiv.org/abs/2505.04879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at IEEE International  Conference on Communications. \c{opyright}2025 IEEE. Personal use of this  material is permitted. Permission from IEEE must be obtained for all other  uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了5G和B5G网络中动态监控和高级解决方案的需求，以保障服务质量。提出了网络数字孪生（NDT）作为虚拟网络测试的解决方案，并在传输网络领域设计了一个实验平台，以实现智能决策和动态路由优化。&lt;h4&gt;背景&lt;/h4&gt;5G和B5G网络的多样化服务需求，如超低延迟和高带宽，要求动态监控和高级解决方案来确保服务质量。&lt;h4&gt;目的&lt;/h4&gt;设计一个实验平台，利用网络数字孪生（NDT）在传输网络领域进行配置和算法测试，以实现智能决策和动态路由优化。&lt;h4&gt;方法&lt;/h4&gt;构建了一个由图神经网络（GNN）组成的NDT，并在包含8、16和30个节点的三种不同网络拓扑中进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;NDT在实现后的延迟预测中取得了较低的MAPE值，表明其具有较高的准确性，能够为网络性能提供精确的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在生成网络性能的精确见解方面是有效的，为5G/B5G场景中的动态路由优化问题提供了智能决策支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：第五代（5G）和超越5G（B5G）网络的出现引入了从超低延迟到高带宽的多样化服务需求，需要动态监控和高级解决方案来确保服务质量（QoS）。负责连接无线接入网络和核心网络的传输网络将越来越多地面临管理复杂流量模式的不利挑战。网络数字孪生（NDT）概念作为在虚拟网络中测试配置和算法的解决方案而出现。在这种情况下，这项工作在传输网络领域设计了一个包含NDT的实验平台，与虚拟对应物和推荐系统同步，以实现5G/B5G场景中动态路由优化问题的智能决策。我们的NDT由一个图神经网络（GNN）组成，在由8、16和30个节点组成的三种不同网络拓扑中进行了评估。与实施解决方案后的实际延迟相比，它实现了URLLC和eMBB切片的较低MAPE值。这些值表明了高精度，证明了该解决方案在生成特定解决方案实施时网络性能精确见解方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of fifth-generation (5G) and Beyond 5G (B5G) networks introducesdiverse service requirements, from ultra-low latency to high bandwidth,demanding dynamic monitoring and advanced solutions to ensure Quality ofService (QoS). The transport network - responsible for interconnecting theradio access network and core networks - will increasingly face challenges inefficiently managing complex traffic patterns. The Network Digital Twin (NDT)concept emerges as a promising solution for testing configurations andalgorithms in a virtual network before real-world deployment. In this context,this work designs an experimental platform with NDT in a transport networkdomain, synchronizing with the virtual counterpart and a recommendation systemfor what-if analysis, enabling intelligent decision-making for dynamic routeoptimization problems in 5G/B5G scenarios. Our NDT, composed of a Graph NeuralNetwork (GNN), was evaluated across three different network topologiesconsisting of 8, 16, and 30 nodes. It achieved lower MAPE values for URLLC andeMBB slices, comparing latency predictions with actual latency after thesolution implementation. These values indicate high accuracy, demonstrating thesolution's effectiveness in generating precise insights into networkperformance if a particular solution were implemented.</description>
      <author>example@mail.com (Rebecca Aben-Athar, Heitor Anglada, Lucas Costa, João Albuquerque, Abrahão Ferreira, Cristiano Bonato Both, Kleber Cardoso, Silvia Lins, Andrey Silva, Glauco Gonçalves, Ilan Correa, Aldebaro Klautau)</author>
      <guid isPermaLink="false">2505.04879v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Accurate Prediction of Sequential Tensor Properties Using Equivariant Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了利用光学光谱研究材料与光相互作用的强大工具，并展示了其在光电子器件开发中的应用，同时提出了一种新的神经网络模型StepENN，用于预测材料的光学响应。&lt;h4&gt;背景&lt;/h4&gt;光学光谱对于揭示材料的电子结构至关重要，特别是在光电子器件如太阳能电池、发光二极管和光电探测器中，电子结构的理解直接影响器件性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测各向异性块体材料中光学响应的模型，以支持光电子器件的数据驱动设计。&lt;h4&gt;方法&lt;/h4&gt;提出了序列张量属性等变神经网络（StepENN），这是一种图神经网络架构，可以将晶体结构直接映射到不同光子频率下的完整光学张量。StepENN通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基于1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了模型在推导其他光谱相关性质（如光学电导率）的潜力。&lt;h4&gt;结论&lt;/h4&gt;StepENN框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：光学光谱是研究材料与光相互作用的强大工具，揭示了复杂的电子结构，如平坦能带和非平凡拓扑特征。这些见解对于光子器件（包括太阳能电池、发光二极管和光电探测器）的开发和优化至关重要，其中对电子结构的理解直接影响到器件的性能。此外，在各向异性块体材料中，光学响应是方向依赖的，由于其固有的复杂性和晶体对称性的限制，预测这些响应张量仍然具有计算上的挑战。为了解决这一挑战，我们引入了序列张量属性等变神经网络（StepENN），这是一种将晶体结构直接映射到不同光子频率下的完整光学张量的图神经网络架构。通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，StepENN确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。在基于从第一性原理方法计算出的1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了其在推导其他光谱相关性质（如光学电导率）的潜力。这一框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical spectra serve as a powerful tool for probing the interactions betweenmaterials and light, unveiling complex electronic structures such as flat bandsand nontrivial topological features. These insights are crucial for thedevelopment and optimization of photonic devices, including solar cells,light-emitting diodes, and photodetectors, where understanding the electronicstructure directly impacts device performance. Moreover, in anisotropic bulkmaterials, the optical responses are direction-dependent, and predicting thoseresponse tensors still remains computationally demanding due to its inherentcomplexity and the constraint from crystal symmetry. To address this challenge,we introduce the sequential tensorial properties equivariant neural network(StepENN), a graph neural network architecture that maps crystal structuresdirectly to their full optical tensors across different photon frequencies. Byencoding the isotropic sequential scalar components and anisotropic sequentialtensor components into l=0 and l=2 spherical tensor components, StepENN ensuressymmetry-aware sequential tensor predictions that are consistent with theinherent symmetry constraints of crystal systems. Trained on a dataset offrequency-dependent permittivity tensors for 1,432 bulk semiconductors computedfrom first-principles methods, our model achieves a mean absolute error (MAE)of 24.216 millifarads per meter (mF/m) on the predicted tensorial spectra with85.7% of its predictions exhibiting less than 10% relative error, demonstratingits potential for deriving other spectrum-related properties, such as opticalconductivity. This framework opens new avenues for the data-driven design ofmaterials with engineered anisotropic optical responses, accelerating materialadvances in optoelectronic applications.</description>
      <author>example@mail.com (Ting-Wei Hsu, Zhenyao Fang, Arun Bansil, Qimin Yan)</author>
      <guid isPermaLink="false">2505.04862v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Piecewise Constant Spectral Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to TMLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PieCoN（分段常数谱图神经网络），这是一种结合常数谱滤波器和多项式滤波器的图神经网络，旨在更灵活地利用图结构，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;背景&lt;/h4&gt;现有的谱GNN使用低阶多项式滤波器捕获图谱特性，但可能无法充分识别图的谱特性，因为多项式阶数低；同时，增加多项式阶数会导致计算成本上升，且超过一定阈值会带来性能平台期或下降。&lt;h4&gt;目的&lt;/h4&gt;设计PieCoN以解决现有谱GNN的这些挑战，提供一种更灵活的方法来利用图结构。&lt;h4&gt;方法&lt;/h4&gt;PieCoN结合常数谱滤波器和多项式滤波器，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;主要发现&lt;/h4&gt;在包括同质和异质图的九个基准数据集上的实验表明，PieCoN在异质数据集上尤其有效，突出了其在广泛领域的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;PieCoN通过结合不同类型的滤波器并自适应地处理谱区间，有效地提高了图神经网络学习谱特性的能力，尤其是在异质图数据集上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network (PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved significant success across variousdomains by leveraging graph structures in data. Existing spectral GNNs, whichuse low-degree polynomial filters to capture graph spectral properties, may notfully identify the graph's spectral characteristics because of the polynomial'ssmall degree. However, increasing the polynomial degree is computationallyexpensive and beyond certain thresholds leads to performance plateaus ordegradation. In this paper, we introduce the Piecewise Constant Spectral GraphNeural Network(PieCoN) to address these challenges. PieCoN combines constantspectral filters with polynomial filters to provide a more flexible way toleverage the graph structure. By adaptively partitioning the spectrum intointervals, our approach increases the range of spectral properties that can beeffectively learned. Experiments on nine benchmark datasets, including bothhomophilic and heterophilic graphs, demonstrate that PieCoN is particularlyeffective on heterophilic datasets, highlighting its potential for a wide rangeof applications.</description>
      <author>example@mail.com (Vahan Martirosyan, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2505.04808v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.02393v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IEF-VAD的视频异常检测框架，该框架通过图像-事件融合的方法，从RGB视频中直接合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合，以解决现有视频异常检测器依赖RGB帧而缺乏时间分辨率的问题。&lt;h4&gt;背景&lt;/h4&gt;大多数现有的视频异常检测器依赖于RGB帧，这些帧无法捕捉到异常事件的突然或短暂的运动线索，而这些线索是异常事件的关键指标。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的视频异常检测方法，以提高检测的准确性和鲁棒性，并减少对特定事件传感器的需求。&lt;h4&gt;方法&lt;/h4&gt;IEF-VAD框架包括：(i) 使用Student-t似然模型对重尾传感器噪声进行建模，并通过拉普拉斯近似推导值级逆方差权重；(ii) 应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；(iii) 通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。&lt;h4&gt;主要发现&lt;/h4&gt;IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平，无需专用的事件传感器或帧级标签。&lt;h4&gt;结论&lt;/h4&gt;合成事件表示在强调运动线索方面具有效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的视频异常检测器仅依赖于RGB帧，这缺乏捕捉突然或短暂运动线索的时间分辨率，而运动线索是异常事件的关键指标。为了解决这一局限性，我们提出了图像-事件融合视频异常检测（IEF-VAD）框架，该框架直接从RGB视频中合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合。该系统（i）使用Student-t似然对重尾传感器噪声进行建模，通过拉普拉斯近似推导值级逆方差权重；（ii）应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；（iii）通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。在没有专用事件传感器或帧级标签的情况下，IEF-VAD在多个真实世界的异常检测基准测试中设定了新的水平。这些发现突出了合成事件表示在强调运动线索方面的效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。代码和模型可在https://github.com/EavnJeong/IEF-VAD上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/eavnjeong/ief-vad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing video anomaly detectors rely solely on RGB frames, which lackthe temporal resolution needed to capture abrupt or transient motion cues, keyindicators of anomalous events. To address this limitation, we proposeImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework thatsynthesizes event representations directly from RGB videos and fuses them withimage features through a principled, uncertainty-aware process. The system (i)models heavy-tailed sensor noise with a Student`s-t likelihood, derivingvalue-level inverse-variance weights via a Laplace approximation; (ii) appliesKalman-style frame-wise updates to balance modalities over time; and (iii)iteratively refines the fused latent state to erase residual cross-modal noise.Without any dedicated event sensor or frame-level labels, IEF-VAD sets a newstate of the art across multiple real-world anomaly detection benchmarks. Thesefindings highlight the utility of synthetic event representations inemphasizing motion cues that are often underrepresented in RGB frames, enablingaccurate and robust video understanding across diverse applications withoutrequiring dedicated event sensors. Code and models are available athttps://github.com/EavnJeong/IEF-VAD.</description>
      <author>example@mail.com (Sungheon Jeong, Jihong Park, Mohsen Imani)</author>
      <guid isPermaLink="false">2505.02393v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2505.05279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MTL-UE的统一框架，用于生成多任务数据和多任务学习模型不可学习的示例。&lt;h4&gt;背景&lt;/h4&gt;现有的不可学习策略主要集中在防止未经授权的用户使用个人数据训练单任务学习模型。然而，近年来，研究重点已转向多任务数据和多任务学习，旨在开发能够同时处理多个任务的通用和基础模型。&lt;h4&gt;目的&lt;/h4&gt;尽管多任务数据和模型日益重要，但在追求不可学习策略时，它们却被大量忽视。本文旨在提出一种针对多任务数据和模型的不可学习策略。&lt;h4&gt;方法&lt;/h4&gt;MTL-UE通过设计一个基于生成器的结构，引入标签先验和类别的特征嵌入，来优化扰动，从而提高攻击性能。此外，它还结合了任务内和任务间嵌入正则化，以增加类间分离并抑制类内方差，从而大大增强攻击的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;MTL-UE对密集预测任务中的多任务学习提供了良好的支持，并且可以即插即用，允许与现有的依赖代理的不可学习方法轻松集成。实验表明，MTL-UE在4个多任务数据集、3种基础不可学习方法和5种模型骨干以及5种多任务任务加权策略上均实现了优越的攻击性能。&lt;h4&gt;结论&lt;/h4&gt;MTL-UE是一种有效的方法，可以生成多任务数据和模型不可学习的示例，并在多个数据集和模型上表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing unlearnable strategies focus on preventing unauthorized usersfrom training single-task learning (STL) models with personal data.Nevertheless, the paradigm has recently shifted towards multi-task data andmulti-task learning (MTL), targeting generalist and foundation models that canhandle multiple tasks simultaneously. Despite their growing importance, MTLdata and models have been largely neglected while pursuing unlearnablestrategies. This paper presents MTL-UE, the first unified framework forgenerating unlearnable examples for multi-task data and MTL models. Instead ofoptimizing perturbations for each sample, we design a generator-based structurethat introduces label priors and class-wise feature embeddings which leads tomuch better attacking performance. In addition, MTL-UE incorporates intra-taskand inter-task embedding regularization to increase inter-class separation andsuppress intra-class variance which enhances the attack robustness greatly.Furthermore, MTL-UE is versatile with good supports for dense prediction tasksin MTL. It is also plug-and-play allowing integrating existingsurrogate-dependent unlearnable methods with little adaptation. Extensiveexperiments show that MTL-UE achieves superior attacking performanceconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5MTL task-weighting strategies.</description>
      <author>example@mail.com (Yi Yu, Song Xia, Siyuan Yang, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot)</author>
      <guid isPermaLink="false">2505.05279v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
      <link>http://arxiv.org/abs/2505.04899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Organ-Wise Tokenization (OWT)的框架，通过Token Group-based Reconstruction (TGR)训练范式来解决图像表示学习中的可解释性和泛化性问题。&lt;h4&gt;背景&lt;/h4&gt;当前代表学习依赖黑盒嵌入，难以解释和泛化，这在医学图像分析中尤为重要。&lt;h4&gt;目的&lt;/h4&gt;提出OWT框架以解决上述局限性，提高医学图像分析的解可释性、泛化性和效率。&lt;h4&gt;方法&lt;/h4&gt;OWT将图像显式地分解为可分离的token组，每组对应一个独特的器官或语义实体。&lt;h4&gt;主要发现&lt;/h4&gt;OWT在CT和MRI数据集上的实验表明，其在图像重建和分割方面的性能强，并且能够实现新型语义级生成和检索应用。&lt;h4&gt;结论&lt;/h4&gt;OWT作为语义解耦表示学习的基础框架，具有广泛的适用性和可扩展性，适用于实际医学图像分析场景及其他领域。&lt;h4&gt;翻译&lt;/h4&gt;近期在表示学习方面取得的进展常常依赖于整体、黑盒的嵌入，这会混杂多个语义组件，限制了可解释性和泛化性。这些问题在医学影像分析中尤为关键。为了解决这些限制，我们提出了一种基于器官的标记化（OWT）框架和基于标记组重建（TGR）训练范式。与产生整体特征的常规方法不同，OWT明确地将图像分解为可分离的标记组，每个组对应一个独特的器官或语义实体。我们的设计确保每个标记组封装了器官特定的信息，从而提高了可解释性、泛化性和效率，同时在下游任务中允许细粒度的控制。在CT和MRI数据集上的实验表明，OWT不仅实现了强大的图像重建和分割性能，而且还能实现标准整体嵌入方法无法达到的新型语义级生成和检索应用。这些发现强调了OWT作为语义解耦表示学习基础框架的潜力，它提供了广泛的扩展性和适用性，可以应用于现实世界的医学图像分析场景及其它领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning often rely on holistic, black-boxembeddings that entangle multiple semantic components, limitinginterpretability and generalization. These issues are especially critical inmedical imaging. To address these limitations, we propose an Organ-WiseTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)training paradigm. Unlike conventional approaches that produce holisticfeatures, OWT explicitly disentangles an image into separable token groups,each corresponding to a distinct organ or semantic entity. Our design ensureseach token group encapsulates organ-specific information, boostinginterpretability, generalization, and efficiency while allowing fine-grainedcontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate theeffectiveness of OWT in not only achieving strong image reconstruction andsegmentation performance, but also enabling novel semantic-level generation andretrieval applications that are out of reach for standard holistic embeddingmethods. These findings underscore the potential of OWT as a foundationalframework for semantically disentangled representation learning, offering broadscalability and applicability to real-world medical imaging scenarios andbeyond.</description>
      <author>example@mail.com (Sifan Song, Siyeop Yoon, Pengfei Jin, Sekeun Kim, Matthew Tivnan, Yujin Oh, Runqi Meng, Ling Chen, Zhiliang Lyu, Dufan Wu, Ning Guo, Xiang Li, Quanzheng Li)</author>
      <guid isPermaLink="false">2505.04899v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Representing spherical tensors with scalar-based machine-learning models</title>
      <link>http://arxiv.org/abs/2505.05404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在物理学中旋转对称性的重要作用，以及如何通过旋转群的结构来描述三维物体在刚体旋转作用下的性质变化。&lt;h4&gt;背景&lt;/h4&gt;旋转对称性在物理学中扮演核心角色，它提供了一个优雅的框架来描述从原子到宏观尺度三维物体的性质在刚体旋转作用下的变换。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索一种新的方法来解决旋转对称性学习问题，该方法通过将等变函数表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积来实现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的方法，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积，同时提出了对通用表达式的近似，这些近似虽然缺乏通用逼近能力，但速度快、易于实现且在实际应用中准确。&lt;h4&gt;主要发现&lt;/h4&gt;发现了一种新的等变函数表示方法，该方法通过将标量函数和具有适当对称性的小张量基相乘来处理旋转对称性学习问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出的近似方法在处理旋转对称性学习问题时既快速又简单，同时在实际应用中保持了较高的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：旋转对称性在物理学中起着核心作用，提供了一个优雅的框架来描述三维物体（从原子到宏观尺度）在刚体旋转作用下的性质变化。三维点云的等变模型能够以与旋转群结构完全一致的方式近似结构-性质关系，通过结合自身为球张量的中间表示。然而，对称性约束使得这种方法在计算上具有挑战性，且实现起来繁琐，这促使越来越受欢迎的无约束架构在训练过程中学习近似对称性。在本工作中，我们探索了第三种解决学习问题的途径，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积。我们还提出了对通用表达式的近似，虽然缺乏通用逼近能力，但这些近似方法快速、易于实现，且在实际设置中准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rotational symmetry plays a central role in physics, providing an elegantframework to describe how the properties of 3D objects -- from atoms to themacroscopic scale -- transform under the action of rigid rotations. Equivariantmodels of 3D point clouds are able to approximate structure-property relationsin a way that is fully consistent with the structure of the rotation group, bycombining intermediate representations that are themselves spherical tensors.The symmetry constraints however make this approach computationally demandingand cumbersome to implement, which motivates increasingly popular unconstrainedarchitectures that learn approximate symmetries as part of the trainingprocess. In this work, we explore a third route to tackle this learningproblem, where equivariant functions are expressed as the product of a scalarfunction of the point cloud coordinates and a small basis of tensors with theappropriate symmetry. We also propose approximations of the general expressionsthat, while lacking universal approximation properties, are fast, simple toimplement, and accurate in practical settings.</description>
      <author>example@mail.com (Michelangelo Domina, Filippo Bigi, Paolo Pegolo, Michele Ceriotti)</author>
      <guid isPermaLink="false">2505.05404v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Clustering with Communication: A Variational Framework for Single Cell Representation Learning</title>
      <link>http://arxiv.org/abs/2505.04891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCCVAE的新型变分自动编码器框架，该框架将细胞间通讯信号纳入单细胞表征学习中，通过实验证明其能提高单细胞分析的聚类性能。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序（scRNA-seq）揭示了细胞异质性，但理解生物功能还需要考虑细胞间通讯（CCC），即通过配体-受体对介导的信号交互来协调细胞行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将细胞间通讯信号纳入单细胞表征学习的方法，以更好地理解细胞行为。&lt;h4&gt;方法&lt;/h4&gt;提出CCCVAE框架，利用从配体-受体相互作用中导出的通讯感知核和稀疏高斯过程，将生物学信息先验编码到潜在空间中。与传统的独立处理每个细胞的VAE不同，CCCVAE鼓励潜在嵌入反映转录相似性和细胞间通讯背景。&lt;h4&gt;主要发现&lt;/h4&gt;在四个scRNA-seq数据集上的实证结果表明，CCCVAE提高了聚类性能，其评估分数高于标准的VAE基线。&lt;h4&gt;结论&lt;/h4&gt;将生物学先验嵌入深度生成模型对于无监督单细胞分析具有价值。&lt;h4&gt;翻译&lt;/h4&gt;Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular heterogeneity, but recent studies emphasize that understanding biological function also requires modeling cell-cell communication (CCC), the signaling interactions mediated by ligand-receptor pairs that coordinate cellular behavior. Tools like CellChat have demonstrated that CCC plays a critical role in processes such as cell differentiation, tissue regeneration, and immune response, and that transcriptomic data inherently encodes rich information about intercellular signaling. We propose CCCVAE, a novel variational autoencoder framework that incorporates CCC signals into single-cell representation learning. By leveraging a communication-aware kernel derived from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes biologically informed priors into the latent space. Unlike conventional VAEs that treat each cell independently, CCCVAE encourages latent embeddings to reflect both transcriptional similarity and intercellular signaling context. Empirical results across four scRNA-seq datasets show that CCCVAE improves clustering performance, achieving higher evaluation scores than standard VAE baselines. This work demonstrates the value of embedding biological priors into deep generative models for unsupervised single-cell analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) has revealed complex cellularheterogeneity, but recent studies emphasize that understanding biologicalfunction also requires modeling cell-cell communication (CCC), the signalinginteractions mediated by ligand-receptor pairs that coordinate cellularbehavior. Tools like CellChat have demonstrated that CCC plays a critical rolein processes such as cell differentiation, tissue regeneration, and immuneresponse, and that transcriptomic data inherently encodes rich informationabout intercellular signaling. We propose CCCVAE, a novel variationalautoencoder framework that incorporates CCC signals into single-cellrepresentation learning. By leveraging a communication-aware kernel derivedfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodesbiologically informed priors into the latent space. Unlike conventional VAEsthat treat each cell independently, CCCVAE encourages latent embeddings toreflect both transcriptional similarity and intercellular signaling context.Empirical results across four scRNA-seq datasets show that CCCVAE improvesclustering performance, achieving higher evaluation scores than standard VAEbaselines. This work demonstrates the value of embedding biological priors intodeep generative models for unsupervised single-cell analysis.</description>
      <author>example@mail.com (Cong Qi, Yeqing Chen, Jie Zhang, Wei Zhi)</author>
      <guid isPermaLink="false">2505.04891v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
      <link>http://arxiv.org/abs/2505.05062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉基础模型在长尾半监督学习（LTSSL）中的影响，提出了一种新的轻量级微调策略ULFine，以降低训练成本并提高预测准确性。&lt;h4&gt;背景&lt;/h4&gt;基于CLIP等大规模视觉基础模型在下游任务中的成功，本文旨在探索这些模型对LTSSL的影响。&lt;h4&gt;目的&lt;/h4&gt;分析大规模视觉基础模型对LTSSL的影响，并提出一种新的微调策略。&lt;h4&gt;方法&lt;/h4&gt;采用三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））对基础模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;FFT导致模型性能下降，LP和LFT虽然提高了整体模型性能，但对长尾类别的影响微乎其微。LP由于训练数据未充分学习产生大量错误伪标签，而LFT减少了这些错误标签的数量，但训练数据偏差导致对它们过度自信。&lt;h4&gt;结论&lt;/h4&gt;ULFine策略通过文本原型自信度感知的适应性调整和双对数互补融合，减轻了过度自信，并对抗了伪标签和分类器偏差，显著降低了训练成本并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;基于大规模视觉基础模型（如CLIP）在各种下游任务中的成功，本文最初尝试通过采用基础模型的三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））来探索其对长尾半监督学习（LTSSL）的影响。我们的分析提出了以下见解：i）与从头开始训练的LTSSL算法相比，FFT导致模型性能下降，而LP和LFT虽然提高了整体模型性能，但对长尾类别的益处微乎其微。ii）LP由于未充分学习的训练数据而产生大量错误伪标签，而LFT可以减少这些错误标签的数量，但由于训练数据偏差，对它们变得过度自信。这加剧了LTSSL中固有的伪标签和分类器偏差，限制了长尾类别的性能提升。基于这些见解，我们提出了一种无偏轻量级微调策略，extbf{ULFine}，通过文本原型自信度感知的适应性调整减轻过度自信，并通过双对数互补融合对抗伪标签和分类器偏差。大量实验表明，与最先进的方法相比，ULFine显著降低了训练成本超过十倍，并大幅提高了预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the success of large-scale visual foundation models like CLIP invarious downstream tasks, this paper initially attempts to explore their impacton Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundationmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning(LFT), and Full Fine-Tuning (FFT). Our analysis presents the followinginsights: i) Compared to LTSSL algorithms trained from scratch, FFT results ina decline in model performance, whereas LP and LFT, although boosting overallmodel performance, exhibit negligible benefits to tail classes. ii) LP producesnumerous false pseudo-labels due to \textit{underlearned} training data, whileLFT can reduce the number of these false labels but becomes overconfident aboutthem owing to \textit{biased fitting} training data. This exacerbates thepseudo-labeled and classifier biases inherent in LTSSL, limiting performanceimprovement in the tail classes. With these insights, we propose a UnbiasedLightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates theoverconfidence via confidence-aware adaptive fitting of textual prototypes andcounteracts the pseudo-labeled and classifier biases via complementary fusionof dual logits. Extensive experiments demonstrate that ULFine markedlydecreases training costs by over ten times and substantially increasesprediction accuracies compared to state-of-the-art methods.</description>
      <author>example@mail.com (Enhao Zhang, Chaohua Li, Chuanxing Geng, Songcan Chen)</author>
      <guid isPermaLink="false">2505.05062v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model（SAM）的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM模型在语义分割应用中取得了成功，但其不确定性量化（UQ）方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的不确定性量化模型，用于量化SAM的不确定性。&lt;h4&gt;方法&lt;/h4&gt;基于贝叶斯熵公式，提出了一种名为USAM的轻量级后处理UQ方法，并分析了不确定性来源。&lt;h4&gt;主要发现&lt;/h4&gt;USAM模型在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上表现出优异的预测能力，为UQ提供了一种计算成本低、易于使用的替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM模型能够支持用户提示、增强半监督流程或平衡准确性与成本效率之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://nianticlabs.github.io/placeit3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的任务：在真实3D场景中进行语言引导的物体放置。该任务通过文本提示来指导3D资产的放置，并提出了新的基准和评估协议。&lt;h4&gt;背景&lt;/h4&gt;在3D场景中进行语言引导的定位任务存在挑战，如放置位置的模糊性和对3D几何关系和自由空间的理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够根据文本提示在真实3D场景中放置3D资产的模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基准和评估协议，引入了一个新的数据集用于训练3D语言大模型，并提出了第一个非平凡基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;该任务具有多解性，需要考虑3D几何关系和自由空间。&lt;h4&gt;结论&lt;/h4&gt;该任务和新的基准有望成为评估和比较通用3D语言大模型的标准之一。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了在真实3D场景中进行语言引导的物体放置的新任务。我们的模型被给定一个3D场景的点云、一个3D资产和一个文本提示，该提示广泛描述了3D资产应该放置的位置。这里的任务是找到尊重提示的有效放置。与3D场景中的其他语言引导定位任务（如基础）相比，这个任务具有特定的挑战：它是不确定的，因为它有多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出一个新的基准和评估协议来启动这个任务。我们还介绍了一个新的数据集，用于在这个任务上训练3D LLMs，以及第一个作为非平凡基线的方法。我们认为这个具有挑战性的任务和我们的新基准可以成为用于评估和比较通用3D LLM模型的标准工具包的一部分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the novel task of Language-Guided Object Placement in Real 3DScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textualprompt broadly describing where the 3D asset should be placed. The task here isto find a valid placement for the 3D asset that respects the prompt. Comparedwith other language-guided localization tasks in 3D scenes such as grounding,this task has specific challenges: it is ambiguous because it has multiplevalid solutions, and it requires reasoning about 3D geometric relationships andfree space. We inaugurate this task by proposing a new benchmark and evaluationprotocol. We also introduce a new dataset for training 3D LLMs on this task, aswell as the first method to serve as a non-trivial baseline. We believe thatthis challenging task and our new benchmark could become part of the suite ofbenchmarks used to evaluate and compare generalist 3D LLM models.</description>
      <author>example@mail.com (Ahmed Abdelreheem, Filippo Aleotti, Jamie Watson, Zawar Qureshi, Abdelrahman Eldesokey, Peter Wonka, Gabriel Brostow, Sara Vicente, Guillermo Garcia-Hernando)</author>
      <guid isPermaLink="false">2505.05288v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture</title>
      <link>http://arxiv.org/abs/2505.04980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。&lt;h4&gt;背景&lt;/h4&gt;LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。&lt;h4&gt;目的&lt;/h4&gt;集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。&lt;h4&gt;方法&lt;/h4&gt;生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。&lt;h4&gt;结论&lt;/h4&gt;通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。本文集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel Large Vision-Language Model (LVLM) and ModelPredictive Control (MPC) integration framework that delivers both taskscalability and safety for Autonomous Driving (AD). LVLMs excel at high-leveltask planning across diverse driving scenarios. However, since these foundationmodels are not specifically designed for driving and their reasoning is notconsistent with the feasibility of low-level motion planning, concerns remainregarding safety and smooth task switching. This paper integrates LVLMs withMPC Builder, which automatically generates MPCs on demand, based on symbolictask commands generated by the LVLM, while ensuring optimality and safety. Thegenerated MPCs can strongly assist the execution or rejection of LVLM-driventask switching by providing feedback on the feasibility of the given tasks andgenerating task-switching-aware MPCs. Our approach provides a safe, flexible,and adaptable control framework, bridging the gap between cutting-edgefoundation models and reliable vehicle operation. We demonstrate theeffectiveness of our approach through a simulation experiment, showing that oursystem can safely and effectively handle highway driving while maintaining theflexibility and adaptability of LVLMs.</description>
      <author>example@mail.com (Kazuki Atsuta, Kohei Honda, Hiroyuki Okuda, Tatsuya Suzuki)</author>
      <guid isPermaLink="false">2505.04980v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2505.04965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DenseGrounding的新方法，用于使智能代理通过自然语言理解和交互3D环境，该方法在ego-centric 3D visual grounding任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;理解3D环境并通过自然语言与之互动对于推动机器人和人机交互至关重要。ego-centric 3D visual grounding任务要求代理根据口头描述在真实世界的3D空间中定位目标对象。&lt;h4&gt;目的&lt;/h4&gt;解决ego-centric 3D visual grounding任务中存在的两个主要挑战：(1) 由于点云与ego-centric多视图图像融合的稀疏性导致的细粒度视觉语义丢失；(2) 由于任意语言描述而导致的有限的文本语义上下文。&lt;h4&gt;方法&lt;/h4&gt;DenseGrounding通过增强视觉和文本语义来解决上述问题。对于视觉特征，引入了层次场景语义增强器（Hierarchical Scene Semantic Enhancer），通过捕捉细粒度的全局场景特征并促进跨模态对齐来保留密集的语义。对于文本描述，提出了语言语义增强器（Language Semantic Enhancer），利用大型语言模型提供丰富的上下文和多样化的语言描述，并在模型训练期间增加额外的上下文。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，DenseGrounding在整体准确率方面显著优于现有方法，在全数据集和较小的迷你子集上分别提高了5.81%和7.56%，进一步推动了ego-centric 3D visual grounding的SOTA（最先进的技术水平）。该方法在CVPR 2024自主挑战赛的多视图3D visual grounding赛道中获得了第一名并获得了创新奖，验证了其有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DenseGrounding是一种有效的ego-centric 3D visual grounding方法，能够显著提高准确率，并在实际竞赛中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling intelligent agents to comprehend and interact with 3D environmentsthrough natural language is crucial for advancing robotics and human-computerinteraction. A fundamental task in this field is ego-centric 3D visualgrounding, where agents locate target objects in real-world 3D spaces based onverbal descriptions. However, this task faces two significant challenges: (1)loss of fine-grained visual semantics due to sparse fusion of point clouds withego-centric multi-view images, (2) limited textual semantic context due toarbitrary language descriptions. We propose DenseGrounding, a novel approachdesigned to address these issues by enhancing both visual and textualsemantics. For visual features, we introduce the Hierarchical Scene SemanticEnhancer, which retains dense semantics by capturing fine-grained global scenefeatures and facilitating cross-modal alignment. For text descriptions, wepropose a Language Semantic Enhancer that leverages large language models toprovide rich context and diverse language descriptions with additional contextduring model training. Extensive experiments show that DenseGroundingsignificantly outperforms existing methods in overall accuracy, withimprovements of 5.81% and 7.56% when trained on the comprehensive full datasetand smaller mini subset, respectively, further advancing the SOTA in egocentric3D visual grounding. Our method also achieves 1st place and receives theInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3DVisual Grounding Track, validating its effectiveness and robustness.</description>
      <author>example@mail.com (Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Zhongchao Shi, Gao Huang)</author>
      <guid isPermaLink="false">2505.04965v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization</title>
      <link>http://arxiv.org/abs/2505.04880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GroverGPT-2的基于大型语言模型（LLM）的方法，用于模拟Grover算法，并通过实验验证了经典模型能够捕捉量子算法结构的能力。&lt;h4&gt;背景&lt;/h4&gt;量子计算在特定任务上理论上优于经典计算，但实际量子优势的边界尚不明确。研究经典机器学习模拟量子算法的能力对于理解这一边界至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究经典机器学习模拟量子算法的能力，特别是探索大型语言模型在模拟Grover算法方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出GroverGPT-2，一种基于LLM的方法，使用思维链（CoT）推理和量子本地标记化来模拟Grover算法。GroverGPT-2能够直接从量子电路表示中执行模拟，并生成逻辑结构和可解释的输出。&lt;h4&gt;主要发现&lt;/h4&gt;GroverGPT-2能够通过有效处理量子本地标记来学习和内化量子电路逻辑，提供了经典模型如LLM能够捕捉量子算法结构的直接证据。GroverGPT-2的输出将电路数据与自然语言交织，将明确的推理嵌入到模拟中。此外，还发现GroverGPT-2的实证缩放定律与量子比特数量增加有关，为可扩展的经典模拟提供了一条路径。&lt;h4&gt;结论&lt;/h4&gt;这些发现为探索经典模拟的极限、提高量子教育与研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：量子计算在特定任务上相对于经典计算具有理论优势，然而实际量子优势的边界仍是一个未解之谜。为了研究这个边界，了解经典机器是否能以及如何学习模拟量子算法至关重要。近年来大型语言模型（LLM）在推理能力上的进步，促使人们探索其在这一挑战中的潜力。在这项工作中，我们引入了GroverGPT-2，这是一种基于LLM的方法，利用思维链（CoT）推理和量子本地标记化来模拟Grover算法。在先前的作品基础上，GroverGPT-2可以直接从量子电路表示中进行模拟，并产生逻辑结构化和可解释的输出。我们的结果表明，GroverGPT-2可以通过高效处理量子本地标记来学习和内化量子电路逻辑，为经典模型如LLM能够捕捉量子算法结构提供了直接证据。此外，GroverGPT-2将电路数据与自然语言交织，将明确的推理嵌入到模拟中。这种双重能力使GroverGPT-2成为提高机器对量子算法理解和建模量子电路逻辑的原型。我们还确定了GroverGPT-2随量子比特数量增加的实证缩放定律，为可扩展的经典模拟指明了一条路径。这些发现为探索经典模拟的极限、增强量子教育和研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum computing offers theoretical advantages over classical computing forspecific tasks, yet the boundary of practical quantum advantage remains an openquestion. To investigate this boundary, it is crucial to understand whether,and how, classical machines can learn and simulate quantum algorithms. Recentprogress in large language models (LLMs) has demonstrated strong reasoningabilities, prompting exploration into their potential for this challenge. Inthis work, we introduce GroverGPT-2, an LLM-based method for simulatingGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-nativetokenization. Building on its predecessor, GroverGPT-2 performs simulationdirectly from quantum circuit representations while producing logicallystructured and interpretable outputs. Our results show that GroverGPT-2 canlearn and internalize quantum circuit logic through efficient processing ofquantum-native tokens, providing direct evidence that classical models likeLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2outputs interleave circuit data with natural language, embedding explicitreasoning into the simulation. This dual capability positions GroverGPT-2 as aprototype for advancing machine understanding of quantum algorithms andmodeling quantum circuit logic. We also identify an empirical scaling law forGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalableclassical simulation. These findings open new directions for exploring thelimits of classical simulatability, enhancing quantum education and research,and laying groundwork for future foundation models in quantum computing.</description>
      <author>example@mail.com (Min Chen, Jinglei Cheng, Pingzhi Li, Haoran Wang, Tianlong Chen, Junyu Liu)</author>
      <guid isPermaLink="false">2505.04880v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.04861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mix-QSAM的混合精度后训练量化框架，用于提高Segment Anything Model (SAM)在资源受限设备上的部署性能。&lt;h4&gt;背景&lt;/h4&gt;SAM是一个流行的视觉基础模型，但由于其计算和内存需求高，在资源受限设备上的部署面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对现有后训练量化方法依赖于固定位宽量化导致的精度和效率问题，提出Mix-QSAM框架以提高模型的性能。&lt;h4&gt;方法&lt;/h4&gt;Mix-QSAM引入了层间重要性评分和跨层协同度量，用于量化层贡献和捕捉相邻层之间的依赖关系。通过这些度量，构建了整数二次规划问题，以确定模型大小和位操作约束下的最优位宽分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Mix-QSAM在实例分割和目标检测任务上优于现有PTQ方法，在6位和4位混合精度设置下，平均精度提高可达20%，同时保持了计算效率。&lt;h4&gt;结论&lt;/h4&gt;Mix-QSAM通过优化位宽分配，有效提高了SAM在资源受限设备上的部署性能，同时保持了模型的精度和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Segment Anything Model (SAM) is a popular vision foundation model;however, its high computational and memory demands make deployment onresource-constrained devices challenging. While Post-Training Quantization(PTQ) is a practical approach for reducing computational overhead, existing PTQmethods rely on fixed bit-width quantization, leading to suboptimal accuracyand efficiency. To address this limitation, we propose Mix-QSAM, amixed-precision PTQ framework for SAM. First, we introduce a layer-wiseimportance score, derived using Kullback-Leibler (KL) divergence, to quantifyeach layer's contribution to the model's output. Second, we introducecross-layer synergy, a novel metric based on causal mutual information, tocapture dependencies between adjacent layers. This ensures that highlyinterdependent layers maintain similar bit-widths, preventing abrupt precisionmismatches that degrade feature propagation and numerical stability. Usingthese metrics, we formulate an Integer Quadratic Programming (IQP) problem todetermine optimal bit-width allocation under model size and bit-operationconstraints, assigning higher precision to critical layers while minimizingbit-width in less influential layers. Experimental results demonstrate thatMix-QSAM consistently outperforms existing PTQ methods on instance segmentationand object detection tasks, achieving up to 20% higher average precision under6-bit and 4-bit mixed-precision settings, while maintaining computationalefficiency.</description>
      <author>example@mail.com (Navin Ranjan, Andreas Savakis)</author>
      <guid isPermaLink="false">2505.04861v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SeriesBench的基准测试，用于评估多模态大型语言模型在理解叙事驱动视频系列方面的能力。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型的快速发展，现有的基准测试主要关注独立视频和视觉元素，如人类动作和物体状态，而忽略了视频系列中的复杂连续叙事。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了SeriesBench，旨在评估模型对叙事驱动视频系列的理解能力。&lt;h4&gt;方法&lt;/h4&gt;SeriesBench包含105个精心挑选的叙事驱动系列，涵盖28个需要深度叙事理解的专项任务。它采用了新颖的长篇叙事标注方法和全信息转换方法，以及一个名为PC-DCoT的叙事推理框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有的MLLMs在理解叙事驱动系列方面仍面临重大挑战，而PC-DCoT能够帮助这些MLLMs实现性能提升。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT强调了提升模型理解叙事驱动系列能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess 'visual elements' like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zackhxn/seriesbench-cvpr2025&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title>
      <link>http://arxiv.org/abs/2505.04813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://threedle.github.io/wir3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WIR3D的技术，通过在三维空间中提取一组视觉上有意义的曲线来抽象三维形状。&lt;h4&gt;背景&lt;/h4&gt;WIR3D技术旨在通过曲线来表示三维形状的几何和视觉特征。&lt;h4&gt;目的&lt;/h4&gt;目的是为了能够从任意视角忠实地表示形状的几何和显著视觉特征，如纹理。&lt;h4&gt;方法&lt;/h4&gt;方法包括优化贝塞尔曲线的参数，利用预训练的基础模型（CLIP）的中间激活来指导优化过程，将优化分为两个阶段：一个用于捕捉形状的粗略几何形状，另一个用于表示细粒度特征。第二个阶段通过一个新颖的局部关键点损失进行空间引导，确保对原始表面的忠实度通过神经SDF损失。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，该方法能够成功应用于具有不同复杂度、几何结构和纹理的形状的大数据集，并展示了特征控制和形状变形的下游应用。&lt;h4&gt;结论&lt;/h4&gt;结论是WIR3D技术能够有效地抽象三维形状，并在实际应用中表现出良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present WIR3D, a technique for abstracting 3D shapes through a sparse setof visually meaningful curves in 3D. We optimize the parameters of Beziercurves such that they faithfully represent both the geometry and salient visualfeatures (e.g. texture) of the shape from arbitrary viewpoints. We leverage theintermediate activations of a pre-trained foundation model (CLIP) to guide ouroptimization process. We divide our optimization into two phases: one forcapturing the coarse geometry of the shape, and the other for representingfine-grained features. Our second phase supervision is spatially guided by anovel localized keypoint loss. This spatial guidance enables user control overabstracted features. We ensure fidelity to the original surface through aneural SDF loss, which allows the curves to be used as intuitive deformationhandles. We successfully apply our method for shape abstraction over a broaddataset of shapes with varying complexity, geometric structure, and texture,and demonstrate downstream applications for feature control and shapedeformation.</description>
      <author>example@mail.com (Richard Liu, Daniel Fu, Noah Tan, Itai Lang, Rana Hanocka)</author>
      <guid isPermaLink="false">2505.04813v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
      <link>http://arxiv.org/abs/2505.04802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ORBIT-2，一种用于全球超分辨率气候降尺度的可扩展基础模型，旨在解决现有方法在变量和地理范围泛化能力有限以及ViT自注意力二次复杂度约束的问题。&lt;h4&gt;背景&lt;/h4&gt;稀疏观测和粗分辨率气候模型限制了区域决策的有效性，强调了稳健降尺度的必要性。&lt;h4&gt;目的&lt;/h4&gt;提出ORBIT-2模型，以实现高效、鲁棒的预测，并降低自注意力复杂度，支持长序列处理和大规模并行计算。&lt;h4&gt;方法&lt;/h4&gt;ORBIT-2包含两个关键创新：(1) Residual Slim ViT (Reslim)，一种轻量级架构，结合残差学习和贝叶斯正则化；(2) TILES，一种分块序列缩放算法，将自注意力复杂度从二次降低到线性。&lt;h4&gt;主要发现&lt;/h4&gt;ORBIT-2可以扩展到10亿参数，在32,768个GPU上运行，实现高达1.8 ExaFLOPS的持续吞吐量和92-98%的强扩展效率。它可以支持0.9公里全球分辨率的降尺度，并处理高达42亿个标记的序列。在7公里分辨率的基准测试中，ORBIT-2与观测数据相比，实现了0.98到0.99的R^2分数的高精度。&lt;h4&gt;结论&lt;/h4&gt;ORBIT-2模型在提高气候降尺度精度和效率方面具有显著优势，为区域决策提供了强有力的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse observations and coarse-resolution climate models limit effectiveregional decision-making, underscoring the need for robust downscaling.However, existing AI methods struggle with generalization across variables andgeographies and are constrained by the quadratic complexity of VisionTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundationmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporatestwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecturewith residual learning and Bayesian regularization for efficient, robustprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reducesself-attention complexity from quadratic to linear, enabling long-sequenceprocessing and massive parallelism. ORBIT-2 scales to 10 billion parametersacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and92-98% strong scaling efficiency. It supports downscaling to 0.9 km globalresolution and processes sequences up to 4.2 billion tokens. On 7 km resolutionbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98to 0.99 against observation data.</description>
      <author>example@mail.com (Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu)</author>
      <guid isPermaLink="false">2505.04802v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2505.04911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了SpatialPrompting，这是一种新型框架，利用现成多模态大型语言模型的涌现推理能力，在三维（3D）环境中实现零样本空间推理。&lt;h4&gt;背景&lt;/h4&gt;现有方法依赖昂贵的3D特定微调，使用如点云或基于体素的特征等专业的3D输入。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种利用直观视觉和位置线索的灵活空间推理新范式。&lt;h4&gt;方法&lt;/h4&gt;SpatialPrompting采用关键帧驱动的提示生成策略，使用视觉语言相似度、马氏距离、视场和图像锐度等指标，从图像序列中选择多样化的关键帧，并与相应的相机位姿数据集成，以有效地抽象空间关系并推断复杂的3D结构。&lt;h4&gt;主要发现&lt;/h4&gt;该框架不仅建立了一种利用直观视觉和位置线索的灵活空间推理新范式，而且在基准数据集（如ScanQA和SQA3D）上实现了最先进的零样本性能，并在多个指标上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地消除了对专业3D输入和微调的需求，提供了一种比传统方法更简单、更可扩展的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces SpatialPrompting, a novel framework that harnesses theemergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D)environments. Unlike existing methods that rely on expensive 3D-specificfine-tuning with specialized 3D inputs such as point clouds or voxel-basedfeatures, SpatialPrompting employs a keyframe-driven prompt generationstrategy. This framework uses metrics such as vision-language similarity,Mahalanobis distance, field of view, and image sharpness to select a diverseand informative set of keyframes from image sequences and then integrates themwith corresponding camera pose data to effectively abstract spatialrelationships and infer complex 3D structures. The proposed framework not onlyestablishes a new paradigm for flexible spatial reasoning that utilizesintuitive visual and positional cues but also achieves state-of-the-artzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, acrossseveral metrics. The proposed method effectively eliminates the need forspecialized 3D inputs and fine-tuning, offering a simpler and more scalablealternative to conventional approaches.</description>
      <author>example@mail.com (Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, Hiroyuki Sakai)</author>
      <guid isPermaLink="false">2505.04911v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MAISY的新方法，用于消除医学图像采集过程中因患者运动导致的模糊、鬼影和器官扭曲问题，从而提高图像解释的准确性。&lt;h4&gt;背景&lt;/h4&gt;患者运动导致医学图像模糊，影响图像解读。现有的基于生成对抗网络（GAN）的算法虽然能生成无运动图像，但存在忽视局部特征和难以处理像素强度、亮度因素及方差变化的问题。&lt;h4&gt;目的&lt;/h4&gt;提出MAISY方法，旨在更好地处理运动模糊，并保留关键病理信息。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法通过以下步骤实现：（a）利用Segment Anything Model（SAM）动态学习运动伪影最明显的解剖边界处的空间模式；（b）引入变差选择性的结构相似性指数（VS-SSIM）损失，自适应地强调高像素变差的区域，以保留必要的解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;在胸部和头部CT数据集上的实验表明，MAISY模型在峰值信噪比（PSNR）、结构相似性（SSIM）和Dice系数上均优于现有算法，PSNR提高40%，SSIM提高10%，Dice系数提高16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在处理运动模糊和提高医学图像质量方面表现出色，为医学图像分析提供了有效工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems</title>
      <link>http://arxiv.org/abs/2505.03140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Hamiltonian-Masked Autoencoding (HMAE)框架，用于解决量子机器学习中标签数据稀缺和模拟计算昂贵的问题。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习在处理自旋和分子系统时面临数据稀缺和计算成本高的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出HMAE框架，通过在未标记的量子哈密顿量上预训练transformer，实现高效的少样本迁移学习。&lt;h4&gt;方法&lt;/h4&gt;HMAE采用基于量子信息理论的物理信息策略，根据哈密顿项的物理意义进行选择性掩码。&lt;h4&gt;主要发现&lt;/h4&gt;在12,500个量子哈密顿量上的实验表明，HMAE在相分类中达到85.3% ± 1.5%的准确率，在基态能量预测中达到0.15 ± 0.02 eV的MAE，仅使用10个标记示例。&lt;h4&gt;结论&lt;/h4&gt;HMAE具有出色的样本效率，比基线方法减少了3-5倍的标记示例需求，但该方法目前限于小型量子系统，不能直接应用于材料科学和量子化学中感兴趣的大系统。&lt;h4&gt;翻译&lt;/h4&gt;Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike randommasking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% ± 1.5%accuracy in phase classification and 0.15 ± 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% ± 2.1%)and quantum neural networks (76.8% ± 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlikerandom masking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.</description>
      <author>example@mail.com (Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma)</author>
      <guid isPermaLink="false">2505.03140v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
  <item>
      <title>Geospatial Mechanistic Interpretability of Large Language Models</title>
      <link>http://arxiv.org/abs/2505.03368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究大型语言模型（LLMs）在地理信息处理方面的能力和内部工作机制。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，并在地理领域被用于地理知识库和推理工具，但其内部处理地理信息的方式尚不明确。&lt;h4&gt;目的&lt;/h4&gt;建立一个新的框架来研究地理空间机制的可解释性，通过空间分析来揭示LLMs处理地理信息的方式。&lt;h4&gt;方法&lt;/h4&gt;使用探针技术揭示LLMs内部的内部结构，引入机制可解释性领域，讨论叠加假设和稀疏自动编码器在分解LLMs的多义内部表示为可解释的单义特征中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;实验中，通过空间自相关展示了地名特征如何显示与地理位置相关的空间模式，从而可以地理空间地解释，为LLMs如何处理地理信息提供了洞察。&lt;h4&gt;结论&lt;/h4&gt;本文提出的框架有助于地理学中基础模型的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各种自然语言处理任务中展现出了前所未有的能力。它们处理和生成有效文本和代码的能力使它们在许多领域变得无处不在，而它们作为知识库和“推理”工具的应用仍是一个持续研究的领域。在地理学中，越来越多的文献开始关注评估LLMs的地理知识和它们执行空间推理的能力。然而，关于这些模型内部工作方式的了解仍然非常有限，特别是关于它们如何处理地理信息。在本章中，我们建立了一个研究地理空间机制可解释性的新框架——使用空间分析来逆向工程LLMs如何处理地理信息。我们的目标是深化我们对这些复杂模型在处理地理信息时生成的内部表示的理解——如果这样的表述不是过度拟人化的话，我们可以称之为“LLMs如何思考地理信息”。我们首先概述了使用探针揭示LLMs内部结构的方法。然后，我们介绍了机制可解释性的领域，讨论了叠加假设和稀疏自动编码器在将LLMs的多义内部表示分解为更可解释的单义特征中的作用。在我们的实验中，我们使用空间自相关来展示地名特征如何显示出与地理位置相关的空间模式，从而可以地理空间地解释，为这些模型如何处理地理信息提供了洞察。我们最后讨论了我们的框架如何有助于塑造地理学中基础模型的研究和应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sdesabbata/geospatial-mechanistic-interpretability&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated unprecedented capabilitiesacross various natural language processing tasks. Their ability to process andgenerate viable text and code has made them ubiquitous in many fields, whiletheir deployment as knowledge bases and "reasoning" tools remains an area ofongoing research. In geography, a growing body of literature has been focusingon evaluating LLMs' geographical knowledge and their ability to perform spatialreasoning. However, very little is still known about the internal functioningof these models, especially about how they process geographical information.  In this chapter, we establish a novel framework for the study of geospatialmechanistic interpretability - using spatial analysis to reverse engineer howLLMs handle geographical information. Our aim is to advance our understandingof the internal representations that these complex models generate whileprocessing geographical information - what one might call "how LLMs think aboutgeographic information" if such phrasing was not an undue anthropomorphism.  We first outline the use of probing in revealing internal structures withinLLMs. We then introduce the field of mechanistic interpretability, discussingthe superposition hypothesis and the role of sparse autoencoders indisentangling polysemantic internal representations of LLMs into moreinterpretable, monosemantic features. In our experiments, we use spatialautocorrelation to show how features obtained for placenames display spatialpatterns related to their geographic location and can thus be interpretedgeospatially, providing insights into how these models process geographicalinformation. We conclude by discussing how our framework can help shape thestudy and use of foundation models in geography.</description>
      <author>example@mail.com (Stef De Sabbata, Stefano Mizzaro, Kevin Roitero)</author>
      <guid isPermaLink="false">2505.03368v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Registration of 3D Point Sets Using Exponential-based Similarity Matrix</title>
      <link>http://arxiv.org/abs/2505.04540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。&lt;h4&gt;背景&lt;/h4&gt;点云注册是计算机视觉和机器人领域的一个基本问题，涉及使用如激光雷达或结构光等深度传感器从不同视点捕获的3D点集的对齐。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有注册技术在大旋转差异或数据腐蚀情况下性能不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。&lt;h4&gt;主要发现&lt;/h4&gt;ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。&lt;h4&gt;结论&lt;/h4&gt;ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。本文提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/aralab-unr/esm_icp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental problem in computer vision androbotics, involving the alignment of 3D point sets captured from varyingviewpoints using depth sensors such as LiDAR or structured light. In modernrobotic systems, especially those focused on mapping, it is essential to mergemultiple views of the same environment accurately. However, state-of-the-artregistration techniques often struggle when large rotational differences existbetween point sets or when the data is significantly corrupted by sensor noise.These challenges can lead to misalignments and, consequently, to inaccurate ordistorted 3D reconstructions. In this work, we address both these limitationsby proposing a robust modification to the classic Iterative Closest Point (ICP)algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),integrates a Gaussian-inspired exponential weighting scheme to construct asimilarity matrix that dynamically adapts across iterations. This matrixfacilitates improved estimation of both rotational and translational componentsduring alignment. We demonstrate the robustness of ESM-ICP in two challengingscenarios: (i) large rotational discrepancies between the source and targetpoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results showthat ESM-ICP outperforms traditional geometric registration techniques as wellas several recent learning-based methods. To encourage reproducibility andcommunity engagement, our full implementation is made publicly available onGitHub. https://github.com/aralab-unr/ESM_ICP</description>
      <author>example@mail.com (Ashutosh Singandhupe, Sanket Lokhande, Hung Manh La)</author>
      <guid isPermaLink="false">2505.04540v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种基于预测链（CoP）的3D属性预测方法，旨在解决单目3D目标检测中的深度估计问题，通过条件预测提高准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计是单目3D目标检测中最具挑战性的问题，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP以通过条件预测提高3D属性预测的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用三个关键设计：1）使用轻量级的属性网络（AN）学习每个3D属性的特征；2）构建显式链来传播这些特征；3）使用残差连接聚合特征，确保后续属性预测基于先前处理的所有属性。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI排行榜上实现了最先进的性能，且无需额外数据，在Waymo和nuScenes前向数据集上超越了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测链提高了单目3D目标检测的深度估计准确性，为该领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>On Path to Multimodal Generalist: General-Level and General-Bench</title>
      <link>http://arxiv.org/abs/2505.04620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML'25, 305 pages, 115 tables, 177 figures, project page:  https://generalist.top/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态大型语言模型（MLLM）的快速发展及其在多模态理解与生成方面的能力提升。&lt;h4&gt;背景&lt;/h4&gt;MLLM的发展推动了多模态通用主义范式的出现，从最初的多模态理解发展到跨模态生成，能力从粗粒度扩展到细粒度，支持的模态从有限扩展到任意。&lt;h4&gt;目的&lt;/h4&gt;评估MLLM的性能和泛化能力，推动向更强大的多模态通用主义和通用人工智能（AGI）的发展。&lt;h4&gt;方法&lt;/h4&gt;引入了“通用级别”评估框架，定义了MLLM性能和泛化能力的5个级别，以及“通用基准”（General-Bench），包含700多个任务和325,800个实例。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，尽管存在许多基准，但高任务性能并不一定意味着更强的MLLM能力，且达到真正的人工智能仍面临挑战。&lt;h4&gt;结论&lt;/h4&gt;该项目为下一代多模态基础模型的研究铺平了道路，为加速实现AGI提供了坚实的框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态大型语言模型（MLLM）目前正在经历快速增长，这得益于LLM的先进能力。与早期的专家不同，现有的MLLM正在向多模态通用主义范式进化。最初仅限于理解多个模态，这些模型已经发展到不仅理解而且能够跨模态生成。它们的能力已从粗粒度扩展到细粒度的多模态理解，从支持有限的模态扩展到任意的模态。尽管存在许多基准来评估MLLM，但一个关键问题出现了：我们能否简单地假设在任务上的更高性能意味着更强的MLLM能力，从而让我们更接近人类水平的人工智能？我们认为答案并不像它看起来那么简单。本项目引入了通用级别，这是一个定义MLLM性能和泛化能力的5个级别的评估框架，提供了一种比较MLLM和衡量现有系统向更强大的多模态通用主义以及最终向AGI发展的方法。该框架的核心是协同的概念，它衡量模型是否在理解和生成之间以及在不同模态之间保持一致的能力。为了支持这种评估，我们提出了通用基准，它包含更广泛的技能、模态、格式和能力，包括700多个任务和325,800个实例。涉及100多个现有最先进MLLM的评估结果揭示了通用主义者的能力排名，突显了达到真正人工智能的挑战。我们期望这个项目为下一代多模态基础模型的研究铺平道路，为加速实现AGI提供坚实的基础。项目页面：https://generalist.top/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Multimodal Large Language Model (MLLM) is currently experiencing rapidgrowth, driven by the advanced capabilities of LLMs. Unlike earlierspecialists, existing MLLMs are evolving towards a Multimodal Generalistparadigm. Initially limited to understanding multiple modalities, these modelshave advanced to not only comprehend but also generate across modalities. Theircapabilities have expanded from coarse-grained to fine-grained multimodalunderstanding and from supporting limited modalities to arbitrary ones. Whilemany benchmarks exist to assess MLLMs, a critical question arises: Can wesimply assume that higher performance across tasks indicates a stronger MLLMcapability, bringing us closer to human-level AI? We argue that the answer isnot as straightforward as it seems. This project introduces General-Level, anevaluation framework that defines 5-scale levels of MLLM performance andgenerality, offering a methodology to compare MLLMs and gauge the progress ofexisting systems towards more robust multimodal generalists and, ultimately,towards AGI. At the core of the framework is the concept of Synergy, whichmeasures whether models maintain consistent capabilities across comprehensionand generation, and across multiple modalities. To support this evaluation, wepresent General-Bench, which encompasses a broader spectrum of skills,modalities, formats, and capabilities, including over 700 tasks and 325,800instances. The evaluation results that involve over 100 existingstate-of-the-art MLLMs uncover the capability rankings of generalists,highlighting the challenges in reaching genuine AI. We expect this project topave the way for future research on next-generation multimodal foundationmodels, providing a robust infrastructure to accelerate the realization of AGI.Project page: https://generalist.top/</description>
      <author>example@mail.com (Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang)</author>
      <guid isPermaLink="false">2505.04620v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution</title>
      <link>http://arxiv.org/abs/2505.04384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on  11-Jul-2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于解耦和对比学习的多类别人脸操纵技术分类框架，用于提升开放世界半监督深度伪造归属（OSS-DFA）任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度伪造归属（DFA）旨在对不同的人脸操纵技术进行多分类，以减轻伪造内容对社会秩序和个人声誉的负面影响。然而，先前的方法仅关注特定方法的线索，容易导致过拟合，并忽略了通用伪造特征的重要性。此外，它们在更实际的开世界场景中难以区分不确定的新类别。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的基于多解耦的对比学习框架，以增强在开放世界半监督深度伪造归属任务中对新类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DATA框架首次定义了'正交归一深度伪造基'的概念，并利用它来解耦特定方法特征，减少对伪造无关信息的过拟合。此外，设计了一种增强记忆机制以帮助新类别发现和对比学习，并通过实例级别的解耦来获得新类别的清晰边界。DATA还使用基础对比损失和中心对比损失作为辅助模块，以增强特征的标准化和区分度。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验评估表明，DATA在OSS-DFA基准上取得了最先进的性能，与现有方法相比，在不同设置下准确率提高了2.55% / 5.7%。&lt;h4&gt;结论&lt;/h4&gt;DATA框架在OSS-DFA任务中表现优异，能够有效提升对新类别的泛化能力，并具有更好的标准化和区分度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake attribution (DFA) aims to perform multiclassification on differentfacial manipulation techniques, thereby mitigating the detrimental effects offorgery content on the social order and personal reputations. However, previousmethods focus only on method-specific clues, which easily lead to overfitting,while overlooking the crucial role of common forgery features. Additionally,they struggle to distinguish between uncertain novel classes in more practicalopen-world scenarios. To address these issues, in this paper we propose aninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, toenhance the generalization ability on novel classes for the open-worldsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since allgeneration techniques can be abstracted into a similar architecture, DATAdefines the concept of 'Orthonormal Deepfake Basis' for the first time andutilizes it to disentangle method-specific features, thereby reducing theoverfitting on forgery-irrelevant information. Furthermore, an augmented-memorymechanism is designed to assist in novel class discovery and contrastivelearning, which aims to obtain clear class boundaries for the novel classesthrough instance-level disentanglements. Additionally, to enhance thestandardization and discrimination of features, DATA uses bases contrastiveloss and center contrastive loss as auxiliaries for the aforementioned modules.Extensive experimental evaluations show that DATA achieves state-of-the-artperformance on the OSS-DFA benchmark, e.g., there are notable accuracyimprovements in 2.55% / 5.7% under different settings, compared with theexisting methods.</description>
      <author>example@mail.com (Ming-Hui Liu, Xiao-Qian Liu, Xin Luo, Xin-Shun Xu)</author>
      <guid isPermaLink="false">2505.04384v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granular Attention based Heterogeneous Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MGA-HHN的多粒度注意力异构超图神经网络，用于异构图表示学习，并通过实验证明了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的异构图神经网络（HeteGNNs）通过元路径基于的消息传递学习潜在节点表示，但存在无法捕捉高阶关系和信息扭曲等问题。&lt;h4&gt;目的&lt;/h4&gt;提出MGA-HHN以解决HeteGNNs中的上述局限性。&lt;h4&gt;方法&lt;/h4&gt;MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作，以捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。&lt;h4&gt;主要发现&lt;/h4&gt;MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。&lt;h4&gt;结论&lt;/h4&gt;MGA-HHN在真实世界基准数据集上的实验表明，它优于现有的模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：异构图神经网络（HeteGNNs）通过有效地提取异构图中的复杂结构和语义信息，在学习节点表示方面显示出强大的能力。大多数现存的HeteGNNs遵循邻域聚合范式，利用基于元路径的消息传递来学习潜在节点表示。然而，由于元路径的成对性质，这些模型无法捕捉节点间的高阶关系，导致性能次优。此外，由于HeteGNNs中的长程消息传递导致的“过度压缩”，进一步限制了这些模型的有效性。为了解决这些局限性，本文提出了一种基于多粒度注意力的异构超图神经网络MGA-HHN，用于异构图表示学习。MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作。这种机制使得模型能够捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。因此，MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。在真实世界基准数据集上的大量实验表明，MGA-HHN优于现有模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graph neural networks (HeteGNNs) have demonstrated strongabilities to learn node representations by effectively extracting complexstructural and semantic information in heterogeneous graphs. Most of theprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveragingmeta-path based message passing to learn latent node representations. However,due to the pairwise nature of meta-paths, these models fail to capturehigh-order relations among nodes, resulting in suboptimal performance.Additionally, the challenge of ``over-squashing'', where long-range messagepassing in HeteGNNs leads to severe information distortion, further limits theefficacy of these models. To address these limitations, this paper proposesMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph NeuralNetwork for heterogeneous graph representation learning. MGA-HHN introduces twokey innovations: (1) a novel approach for constructing meta-path basedheterogeneous hypergraphs that explicitly models higher-order semanticinformation in heterogeneous graphs through multiple views, and (2) amulti-granular attention mechanism that operates at both the node and hyperedgelevels. This mechanism enables the model to capture fine-grained interactionsamong nodes sharing the same semantic context within a hyperedge type, whilepreserving the diversity of semantics across different hyperedge types. Assuch, MGA-HHN effectively mitigates long-range message distortion and generatesmore expressive node representations. Extensive experiments on real-worldbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-artmodels, showcasing its effectiveness in node classification, node clusteringand visualization tasks.</description>
      <author>example@mail.com (Hong Jin, Kaicheng Zhou, Jie Yin, Lan You, Zhifeng Zhou)</author>
      <guid isPermaLink="false">2505.04340v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
      <link>http://arxiv.org/abs/2505.04529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为RAFT的新框架，用于通过数据增强、特征增强和主动学习，利用最少量的标注数据对图像分割模型进行适应，以解决合成数据训练的深度神经网络在实际应用中性能不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;图像分割是场景理解的有力计算机视觉技术，但在实际应用中，高质量的标注数据集是必需的，而手动数据收集和标注成本高昂。合成数据虽然提供了高质量的标签，但深度神经网络在合成数据上训练后，往往在真实世界部署时面临Syn2Real问题，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述图像分割的差距，提出RAFT框架，旨在通过数据增强、特征增强和主动学习，使用最少量的标注数据对图像分割模型进行适应。&lt;h4&gt;方法&lt;/h4&gt;RAFT框架通过数据增强、特征增强和主动学习，利用少量标注数据对图像分割模型进行训练和优化。&lt;h4&gt;主要发现&lt;/h4&gt;在合成到真实的“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试中，RAFT框架超过了之前的最先进方法HALO，分别实现了mIoU提升2.1%/79.9%和0.4%/78.2%。在真实到真实的基准测试“Cityscapes-&gt;ACDC”中，RAFT同样超越了HALO，mIoU提升1.3%/73.2%。此外，还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;h4&gt;结论&lt;/h4&gt;RAFT框架有效提高了图像分割模型在真实世界中的应用性能，并通过实验验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图像分割是场景理解的有力计算机视觉技术。然而，由于需要高质量、精心标注的数据集，实际部署受到了阻碍。合成数据提供了高质量的标签，同时减少了手动数据收集和标注的需求。然而，在合成数据上训练的深度神经网络往往面临Syn2Real问题，导致在真实世界部署中性能不佳。为了缓解上述图像分割的差距，我们提出了RAFT，一种新的框架，通过数据增强、特征增强以及主动学习，利用最少量的标注数据对图像分割模型进行适应。为了验证RAFT，我们在合成到真实“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试上进行了实验。我们成功超过了之前的最佳水平HALO。在“SYNTHIA-&gt;Cityscapes”中，经过领域适应后，mIoU提高了2.1%/79.9%，在“GTAV-&gt;Cityscapes”中，mIoU提高了0.4%/78.2%。此外，我们在真实到真实基准测试“Cityscapes-&gt;ACDC”上也测试了我们的方法，再次超过了HALO，在适应后mIoU提高了1.3%/73.2%。最后，我们还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a powerful computer vision technique for sceneunderstanding. However, real-world deployment is stymied by the need forhigh-quality, meticulously labeled datasets. Synthetic data provideshigh-quality labels while reducing the need for manual data collection andannotation. However, deep neural networks trained on synthetic data often facethe Syn2Real problem, leading to poor performance in real-world deployments.  To mitigate the aforementioned gap in image segmentation, we propose RAFT, anovel framework for adapting image segmentation models using minimal labeledreal-world data through data and feature augmentations, as well as activelearning. To validate RAFT, we perform experiments on the synthetic-to-real"SYNTHIA-&gt;Cityscapes" and "GTAV-&gt;Cityscapes" benchmarks. We managed to surpassthe previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences animprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV-&gt;Cityscapesexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approachon the real-to-real benchmark of "Cityscapes-&gt;ACDC", and again surpass HALO,with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine theeffect of the allocated annotation budget and various components of RAFT uponthe final transfer mIoU.</description>
      <author>example@mail.com (Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin)</author>
      <guid isPermaLink="false">2505.04529v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning</title>
      <link>http://arxiv.org/abs/2505.04601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenVision，这是一个完全开放的、成本效益高的视觉编码器家族，其性能与OpenAI的CLIP相当甚至更优，适用于构建多模态基础模型。&lt;h4&gt;背景&lt;/h4&gt;OpenAI的CLIP自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但它们并未完全开放，其训练数据和训练方法未公开。&lt;h4&gt;目的&lt;/h4&gt;本文旨在填补这一空白，提出OpenVision，一个完全开放的视觉编码器，以提升多模态模型的质量。&lt;h4&gt;方法&lt;/h4&gt;OpenVision基于现有的工作，如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。&lt;h4&gt;主要发现&lt;/h4&gt;OpenVision提供了从5.9M到632.1M参数范围的视觉编码器，使得构建多模态模型时可以在容量和效率之间进行灵活的权衡：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;h4&gt;结论&lt;/h4&gt;OpenVision为多模态模型构建提供了新的选择，有助于推动多模态技术的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;OpenAI的CLIP，自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但据我们所知，它们都没有完全开放：它们的训练数据仍然是专有的，或者它们的训练方法并未公开。本文通过提出OpenVision，一个完全开放、成本效益高的视觉编码器家族，来填补这一空白。OpenVision基于现有工作，例如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。通过发布从5.9M到632.1M参数范围的视觉编码器，OpenVision为实践者提供了在构建多模态模型时在容量和效率之间进行灵活权衡的选择：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; OpenAI's CLIP, released in early 2021, have long been the go-to choice ofvision encoder for building multimodal foundation models. Although recentalternatives such as SigLIP have begun to challenge this status quo, to ourknowledge none are fully open: their training data remains proprietary and/ortheir training recipes are not released. This paper fills this gap withOpenVision, a fully-open, cost-effective family of vision encoders that matchor surpass the performance of OpenAI's CLIP when integrated into multimodalframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS fortraining framework and Recap-DataComp-1B for training data -- while revealingmultiple key insights in enhancing encoder quality and showcasing practicalbenefits in advancing multimodal models. By releasing vision encoders spanningfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexibletrade-off between capacity and efficiency in building multimodal models: largermodels deliver enhanced multimodal performance, while smaller versions enablelightweight, edge-ready multimodal deployments.</description>
      <author>example@mail.com (Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, Cihang Xie)</author>
      <guid isPermaLink="false">2505.04601v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</title>
      <link>http://arxiv.org/abs/2505.04596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 Figures, Accepted at AIRC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种优化动态监控环境中PTZ（全向俯仰缩放）相机调度和控制的新方法。&lt;h4&gt;背景&lt;/h4&gt;PTZ相机在动态监控环境中的应用需要高效的视频捕获。&lt;h4&gt;目的&lt;/h4&gt;提高动态监控环境中PTZ相机的调度和控制效率。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型来增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，实现精确的相机任务调度。此外，引入基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。&lt;h4&gt;结论&lt;/h4&gt;该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法来优化动态监控环境中PTZ相机的调度和控制。该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型以增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，从而实现精确的相机任务调度。此外，还引入了一个基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。广泛的模拟表明，与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。总的来说，该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach for optimizing the scheduling andcontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.The proposed method integrates Kalman filters for motion prediction with adynamic network flow model to enhance real-time video capture efficiency. Byassigning Kalman filters to tracked objects, the system predicts futurelocations, enabling precise scheduling of camera tasks. This prediction-drivenapproach is formulated as a network flow optimization, ensuring scalability andadaptability to various surveillance scenarios. To further reduce redundantmonitoring, we also incorporate group-tracking nodes, allowing multiple objectsto be captured within a single camera focus when appropriate. In addition, avalue-based system is introduced to prioritize camera actions, focusing on thetimely capture of critical events. By adjusting the decay rates of these valuesover time, the system ensures prompt responses to tasks with imminentdeadlines. Extensive simulations demonstrate that this approach improvescoverage, reduces average wait times, and minimizes missed events compared totraditional master-slave camera systems. Overall, our method significantlyenhances the efficiency, scalability, and effectiveness of surveillancesystems, particularly in dynamic and crowded environments.</description>
      <author>example@mail.com (Mohammad Merati, David Castañón)</author>
      <guid isPermaLink="false">2505.04596v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces</title>
      <link>http://arxiv.org/abs/2505.04335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Filtration-based Hyperbolic Fuzzy C-Means (HypeFCM)的聚类算法，用于在非欧几里得空间中更好地表示数据关系。&lt;h4&gt;背景&lt;/h4&gt;传统的聚类技术在处理复杂、高维和非欧几里得数据集时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;克服传统聚类技术在非欧几里得空间中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HypeFCM算法结合了模糊聚类原理和双曲几何，使用基于权重的过滤机制来提高性能。算法使用狄利克雷分布初始化权重，并根据Poincaré圆盘模型中的双曲度量迭代地优化聚类中心和成员分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HypeFCM在非欧几里得设置中显著优于传统的模糊聚类方法，证明了其鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;HypeFCM是一种有效的聚类算法，特别适用于在非欧几里得空间中处理复杂数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering algorithms play a pivotal role in unsupervised learning byidentifying and grouping similar objects based on shared characteristics. Whiletraditional clustering techniques, such as hard and fuzzy center-basedclustering, have been widely used, they struggle with complex,high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibitsnotable limitations in non-Euclidean spaces. Euclidean spaces assume linearseparability and uniform distance scaling, limiting their effectiveness incapturing complex, hierarchical, or non-Euclidean structures in fuzzyclustering. To overcome these challenges, we introduce Filtration-basedHyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored forbetter representation of data relationships in non-Euclidean spaces. HypeFCMintegrates the principles of fuzzy clustering with hyperbolic geometry andemploys a weight-based filtering mechanism to improve performance. Thealgorithm initializes weights using a Dirichlet distribution and iterativelyrefines cluster centroids and membership assignments based on a hyperbolicmetric in the Poincar\'e Disc model. Extensive experimental evaluationsdemonstrate that HypeFCM significantly outperforms conventional fuzzyclustering methods in non-Euclidean settings, underscoring its robustness andeffectiveness.</description>
      <author>example@mail.com (Swagato Das, Arghya Pratihar, Swagatam Das)</author>
      <guid isPermaLink="false">2505.04335v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Frame-Averaging Kernel-Point Convolution (FA-KPConv)，这是一种基于知名KPConv架构的神经网络架构，适用于3D点云分析。FA-KPConv通过FrameAveraging技术使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性，同时不增加可学习参数数量，不损失任何输入信息，并在点云分类和点云注册任务中显示出优势。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其基于KPConv的网络在训练大数据集或进行数据增强时，只能近似实现欧几里得变换的不变性和/或等变性。&lt;h4&gt;目的&lt;/h4&gt;设计一个神经网络架构，使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;使用FrameAveraging技术，将几何先验知识嵌入到KPConv网络中，通过简单地封装现有的KPConv网络来实现。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在网络结构上对现有的KPConv进行了改进，使其具有精确的不变性和/或等变性，同时保持了参数数量和输入信息的完整性。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中显示出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为Frame-Averaging Kernel-Point Convolution (FA-KPConv)的神经网络架构，该架构基于著名的KPConv，被广泛用于3D点云分析。尽管许多常见任务需要欧几里得变换的不变性和/或等变性，但基于KPConv的网络在训练大数据集或进行显著数据增强时只能近似实现这些特性。通过使用FrameAveraging，我们允许灵活定制使用KPConv层构建的点云神经网络，使它们对输入点云的平移、旋转和/或反射具有精确的不变性和/或等变性。通过简单地封装现有的基于KPConv的网络，FA-KPConv将几何先验知识嵌入其中，同时保持可学习参数的数量，不牺牲任何输入信息。我们展示了这种引入的偏差对点云分类和点云注册的好处，尤其是在训练数据稀缺或测试数据随机旋转的困难情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality</title>
      <link>http://arxiv.org/abs/2505.03440v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, submitted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。&lt;h4&gt;背景&lt;/h4&gt;生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。&lt;h4&gt;目的&lt;/h4&gt;通过结合VR控制器和眼动追踪硬件，加速深度学习细胞追踪模型的真值生成和校对。&lt;h4&gt;方法&lt;/h4&gt;将深度学习模型的增量标注、训练和校对循环提升到三维空间，并应用手势和眼动追踪等自然用户界面来加速细胞追踪流程。&lt;h4&gt;主要发现&lt;/h4&gt;该系统桥接了基于深度学习的细胞追踪软件和3D/VR可视化之间的差距，提高了细胞追踪的效率和空间理解。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有助于加速细胞追踪工作流程，并提升生命科学家的研究效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。我们利用VR控制器和眼动追踪硬件来促进基于深度学习的细胞追踪模型的快速真值生成和校对。生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。传统上，这种细胞谱系树的重建涉及通过所有记录的时间点追踪单个细胞，手动标注其位置，然后将它们随时间链接起来以创建完整的轨迹。基于深度学习的算法加速了这一过程，但严重依赖于手动标注的高质量真值数据和整理。在此过程中，图像数据的视觉表示仍然主要依赖于2D渲染，这极大地限制了空间理解和导航。在本研究中，我们通过将深度学习模型的增量标注、训练和校对循环提升到第三维度，并应用手势和眼动追踪等自然用户界面，来加速生命科学家的细胞追踪工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose manvr3d, a novel VR-ready platform for interactivehuman-in-the-loop cell tracking. We utilize VR controllers and eye-trackinghardware to facilitate rapid ground truth generation and proofreading for deeplearning-based cell tracking models. Life scientists reconstruct thedevelopmental history of organisms on the cellular level by analyzing 3Dtime-lapse microscopy images acquired at high spatio-temporal resolution. Thereconstruction of such cell lineage trees traditionally involves trackingindividual cells through all recorded time points, manually annotating theirpositions, and then linking them over time to create complete trajectories.Deep learning-based algorithms accelerate this process, yet depend heavily onmanually-annotated high-quality ground truth data and curation. Visualrepresentation of the image data in this process still relies primarily on 2Drenderings, which greatly limits spatial understanding and navigation. In thiswork, we bridge the gap between deep learning-based cell tracking software and3D/VR visualization to create a human-in-the-loop cell tracking system. We liftthe incremental annotation, training and proofreading loop of the deep learningmodel into the 3rd dimension and apply natural user interfaces like handgestures and eye tracking to accelerate the cell tracking workflow for lifescientists.</description>
      <author>example@mail.com (Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther)</author>
      <guid isPermaLink="false">2505.03440v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
      <link>http://arxiv.org/abs/2505.04461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025 Survey Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了时间交互图（TIGs）及其在复杂动态系统行为建模中的应用，并重点介绍了时间交互图表示学习（TIGRL）的发展现状。&lt;h4&gt;背景&lt;/h4&gt;时间交互图（TIGs）由于其能够模拟复杂动态系统行为的能力，在现实世界中得到了广泛应用。&lt;h4&gt;目的&lt;/h4&gt;时间交互图表示学习（TIGRL）的目标是将TIGs中的节点嵌入到低维度的表示中，以有效地保留结构和时间信息，从而提高分类、预测和聚类等下游任务在动态数据环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;本文首先介绍了TIGs的基础概念，强调了时间依赖性的关键作用；接着提出了一个全面的分类法，根据学习过程中利用的信息类型对最先进的TIGRL方法进行系统分类；同时整理了数据集和基准测试的资源，为实证研究提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;本文系统地分类了TIGRL方法，并探讨了TIGRL领域的关键开放挑战和有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;本文为TIGRL领域未来的发展奠定了基础，有望推动该领域的发展进程。&lt;h4&gt;翻译&lt;/h4&gt;Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal interaction graphs (TIGs), defined by sequences of timestampedinteraction events, have become ubiquitous in real-world applications due totheir capability to model complex dynamic system behaviors. As a result,temporal interaction graph representation learning (TIGRL) has garneredsignificant attention in recent years. TIGRL aims to embed nodes in TIGs intolow-dimensional representations that effectively preserve both structural andtemporal information, thereby enhancing the performance of downstream taskssuch as classification, prediction, and clustering within constantly evolvingdata environments. In this paper, we begin by introducing the foundationalconcepts of TIGs and emphasize the critical role of temporal dependencies. Wethen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,systematically categorizing them based on the types of information utilizedduring the learning process to address the unique challenges inherent to TIGs.To facilitate further research and practical applications, we curate the sourceof datasets and benchmarks, providing valuable resources for empiricalinvestigations. Finally, we examine key open challenges and explore promisingresearch directions in TIGRL, laying the groundwork for future advancementsthat have the potential to shape the evolution of this field.</description>
      <author>example@mail.com (Pengfei Jiao, Hongjiang Chen, Xuan Guo, Zhidong Zhao, Dongxiao He, Di Jin)</author>
      <guid isPermaLink="false">2505.04461v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
      <link>http://arxiv.org/abs/2505.04121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，提供了一种更自然的方式来捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。&lt;h4&gt;背景&lt;/h4&gt;现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。&lt;h4&gt;目的&lt;/h4&gt;提出Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。&lt;h4&gt;方法&lt;/h4&gt;VGP的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。&lt;h4&gt;主要发现&lt;/h4&gt;VGP显著提高了ViG在多种下游任务上的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。&lt;h4&gt;结论&lt;/h4&gt;VGP在视觉图结构上的应用为提高ViG的迁移性能提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，以更自然的方式捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。然而，现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。在本文中，我们提出了Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。我们的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，我们引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。广泛的实验表明，我们的方法在多种下游任务上显著提高了ViG的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。我们的代码可在https://github.com/zhoujiahuan1991/ICML2025-VGP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision GNN (ViG) demonstrates superior performance by representing images asgraph structures, providing a more natural way to capture irregular semanticpatterns beyond traditional grid or sequence-based representations. Toefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuningtechniques like visual prompting become increasingly essential. However,existing prompting methods are primarily designed for Transformer-based models,neglecting the rich topological relationships among nodes and edges ingraph-based representations, limiting their capacity to model complexsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novelframework tailored for vision graph structures. Our core insight reveals thatsemantically connected components in the graph exhibit low-rank properties.Building on this observation, we introduce a semantic low-rank prompting methodthat decomposes low-rank semantic features and integrates them with prompts onvision graph topologies, capturing both global structural patterns andfine-grained semantic dependencies. Extensive experiments demonstrate ourmethod significantly improves ViG's transfer performance on diverse downstreamtasks, achieving results comparable to full fine-tuning while maintainingparameter efficiency. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04121v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
      <link>http://arxiv.org/abs/2505.04410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeCLIP是一个新型的视觉语言模型框架，通过解耦CLIP的自注意力模块来增强其在密集预测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;密集视觉预测任务受限于预定义类别，限制了其在实际场景中的应用，而VLMs如CLIP在开放词汇任务中表现良好，但在密集预测中存在局部特征表示的限制。&lt;h4&gt;目的&lt;/h4&gt;提出DeCLIP框架以解决CLIP在图像token中难以有效聚合空间或语义相关区域信息的问题。&lt;h4&gt;方法&lt;/h4&gt;DeCLIP通过解耦自注意力模块获得“内容”和“上下文”特征，其中“内容”特征与图像裁剪表示对齐以增强局部可判别性，“上下文”特征在视觉基础模型如DINO的指导下学习保留空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;DeCLIP在多个开放词汇密集预测任务中（如目标检测和语义分割）显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;DeCLIP通过改进局部特征表示和空间一致性，提高了VLMs在密集预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;This abstract discusses a novel framework called DeCLIP, which enhances the performance of Vision-Language Models (VLMs) like CLIP in dense prediction tasks. The framework addresses limitations in local feature representation by decoupling the self-attention module to obtain 'content' and 'context' features, which are aligned with image crop representations and guided by vision foundation models to improve local discriminability and spatial consistency. Extensive experiments demonstrate its superiority over existing methods in tasks like object detection and semantic segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense visual prediction tasks have been constrained by their reliance onpredefined categories, limiting their applicability in real-world scenarioswhere visual concepts are unbounded. While Vision-Language Models (VLMs) likeCLIP have shown promise in open-vocabulary tasks, their direct application todense prediction often leads to suboptimal performance due to limitations inlocal feature representation. In this work, we present our observation thatCLIP's image tokens struggle to effectively aggregate information fromspatially or semantically related regions, resulting in features that lacklocal discriminability and spatial consistency. To address this issue, wepropose DeCLIP, a novel framework that enhances CLIP by decoupling theself-attention module to obtain ``content'' and ``context'' featuresrespectively. The ``content'' features are aligned with image croprepresentations to improve local discriminability, while ``context'' featureslearn to retain the spatial correlations under the guidance of visionfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIPsignificantly outperforms existing methods across multiple open-vocabularydense prediction tasks, including object detection and semantic segmentation.Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.</description>
      <author>example@mail.com (Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian)</author>
      <guid isPermaLink="false">2505.04410v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal cascade feature transfer for polymer property prediction</title>
      <link>http://arxiv.org/abs/2505.03704v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多模态级联模型特征迁移的聚合物性质预测的新颖迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物数据以多种格式存在，包括分子描述符、添加剂信息以及化学结构。&lt;h4&gt;目的&lt;/h4&gt;提高聚合物物理性质的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型通过结合由图卷积神经网络（GCN）从化学结构中提取的特征，以及分子描述符和添加剂信息等特征，实现更准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;使用多个聚合物数据集进行实证评估表明，与使用单一特征的传统方法相比，该方法表现出更高的预测性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在聚合物性质预测方面具有较高的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction. Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks (GCN) with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel transfer learning approach calledmulti-modal cascade model with feature transfer for polymer propertyprediction.Polymers are characterized by a composite of data in severaldifferent formats, including molecular descriptors and additive information aswell as chemical structures. However, in conventional approaches, predictionmodels were often constructed using each type of data separately. Our modelenables more accurate prediction of physical properties for polymers bycombining features extracted from the chemical structure by graph convolutionalneural networks (GCN) with features such as molecular descriptors and additiveinformation. The predictive performance of the proposed method is empiricallyevaluated using several polymer datasets. We report that the proposed methodshows high predictive performance compared to the baseline conventionalapproach using a single feature.</description>
      <author>example@mail.com (Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui)</author>
      <guid isPermaLink="false">2505.03704v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization</title>
      <link>http://arxiv.org/abs/2505.04412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 11 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自编码器的方法，旨在从高维数据中发现和表示低维结构，同时保留关键拓扑和几何属性。该方法在点云数据集上的实验表明，其在发现噪声数据中的流形结构并通过降维保持这些结构方面优于t-SNE、UMAP和拓扑自编码器等基线方法。&lt;h4&gt;背景&lt;/h4&gt;流形学习旨在发现和表示高维数据中的低维结构，但现有方法往往无法同时捕捉局部细节和全局拓扑完整性，或者无法构建平衡的降维。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的流形学习方法，以从噪声数据中发现流形结构，并通过降维保持这些结构。&lt;h4&gt;方法&lt;/h4&gt;该方法集成了一个流形重建层，该层从噪声点云中揭示潜在流形结构，并在降维过程中提供拓扑和几何属性的规范化。这两个组件在训练过程中相互促进。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在发现噪声数据中的流形结构并保持这些结构方面优于基线方法，这一结论通过可视化和定量指标得到了验证。&lt;h4&gt;结论&lt;/h4&gt;结合流形重建和流形学习对于实现可靠地表示潜在流形具有重要意义，尤其是在处理噪声数据时。&lt;h4&gt;翻译&lt;/h4&gt;Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/thanatorika/mrtg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manifold learning aims to discover and represent low-dimensional structuresunderlying high-dimensional data while preserving critical topological andgeometric properties. Existing methods often fail to capture local details withglobal topological integrity from noisy data or construct a balanceddimensionality reduction, resulting in distorted or fractured embeddings. Wepresent an AutoEncoder-based method that integrates a manifold reconstructionlayer, which uncovers latent manifold structures from noisy point clouds, andfurther provides regularizations on topological and geometric properties duringdimensionality reduction, whereas the two components promote each other duringtraining. Experiments on point cloud datasets demonstrate that our methodoutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders indiscovering manifold structures from noisy data and preserving them throughdimensionality reduction, as validated by visualization and quantitativemetrics. This work demonstrates the significance of combining manifoldreconstruction with manifold learning to achieve reliable representation of thelatent manifold, particularly when dealing with noisy real-world data. Coderepository: https://github.com/Thanatorika/mrtg.</description>
      <author>example@mail.com (Ren Wang, Pengcheng Zhou)</author>
      <guid isPermaLink="false">2505.04412v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MFSeg: Efficient Multi-frame 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.04408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种高效的多帧3D语义分割框架MFSeg。&lt;h4&gt;背景&lt;/h4&gt;当前多帧3D语义分割方法在保持高精度的同时，计算开销较大。&lt;h4&gt;目的&lt;/h4&gt;减少计算开销，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;通过在特征级别聚合点云序列，并正则化特征提取和聚合过程。使用轻量级的基于MLP的点解码器，消除了从过去帧中上采样冗余点的需求。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo数据集上的实验表明，MFSeg优于现有方法，证明了其有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;MFSeg是一个有效且高效的3D语义分割框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose MFSeg, an efficient multi-frame 3D semantic segmentationframework. By aggregating point cloud sequences at the feature level andregularizing the feature extraction and aggregation process, MFSeg reducescomputational overhead while maintaining high accuracy. Moreover, by employinga lightweight MLP-based point decoder, our method eliminates the need toupsample redundant points from past frames. Experiments on the nuScenes andWaymo datasets show that MFSeg outperforms existing methods, demonstrating itseffectiveness and efficiency.</description>
      <author>example@mail.com (Chengjie Huang, Krzysztof Czarnecki)</author>
      <guid isPermaLink="false">2505.04408v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training</title>
      <link>http://arxiv.org/abs/2505.04083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Plexus的三维并行方法，用于全图训练，以解决大规模图数据在GPU内存容量限制、采样和数据传输速度慢、分布式全图训练通信开销大和负载不均衡等问题。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界图数据规模庞大，很多图数据超过了GPU的内存容量，使用图神经网络（GNN）处理这些数据需要采用如小批量采样等技术进行扩展。&lt;h4&gt;目的&lt;/h4&gt;旨在解决大规模图数据在训练过程中的内存限制、计算效率低下和分布式训练中的通信开销问题。&lt;h4&gt;方法&lt;/h4&gt;Plexus是一种三维并行方法，包括负载平衡的排列方案和性能模型以预测最优的三维配置。&lt;h4&gt;主要发现&lt;/h4&gt;Plexus在多个图数据集上进行了评估，并在Perlmutter上扩展到2048个GPU，在Frontier上扩展到2048个GCD。Plexus相比现有方法实现了2.3x-12.5x的速度提升，并在Perlmutter上减少了5.2-8.7倍的时间到解，在Frontier上减少了7-54.2倍的时间到解。&lt;h4&gt;结论&lt;/h4&gt;Plexus实现了前所未有的加速效果，显著提高了大规模图数据的处理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a potent class of neural networkscapable of leveraging the connectivity and structure of real-world graphs tolearn intricate properties and relationships between nodes. Many real-worldgraphs exceed the memory capacity of a GPU due to their sheer size, and usingGNNs on them requires techniques such as mini-batch sampling to scale. However,this can lead to reduced accuracy in some cases, and sampling and data transferfrom the CPU to the GPU can also slow down training. On the other hand,distributed full-graph training suffers from high communication overhead andload imbalance due to the irregular structure of graphs. We propose Plexus, athree-dimensional (3D) parallel approach for full-graph training that tacklesthese issues and scales to billion-edge graphs. Additionally, we introduceoptimizations such as a permutation scheme for load balancing, and aperformance model to predict the optimal 3D configuration. We evaluate Plexuson several graph datasets and show scaling results for up to 2048 GPUs onPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexusachieves unprecedented speedups of 2.3x-12.5x over existing methods and areduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x onFrontier.</description>
      <author>example@mail.com (Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele)</author>
      <guid isPermaLink="false">2505.04083v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</title>
      <link>http://arxiv.org/abs/2505.04354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从专家依赖的优化问题解决流程转变为进化代理工作流程的观点。&lt;h4&gt;背景&lt;/h4&gt;传统的优化实践依赖于人类专家进行问题制定、算法选择和超参数调整，这导致了工业界对先进方法的采纳障碍。&lt;h4&gt;目的&lt;/h4&gt;本文主张通过基于基础模型和进化搜索的进化代理工作流程，可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。&lt;h4&gt;方法&lt;/h4&gt;通过云计算资源调度和ADMM参数适应的案例研究，展示了这一方法如何连接学术界创新与工业实施之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出了优化问题解决流程转变的新思路，并提供了实际案例支持其观点。&lt;h4&gt;翻译&lt;/h4&gt;本文主张优化问题解决流程从专家依赖向进化代理工作流程转变。传统的优化实践依赖人类专家，导致工业界采纳先进方法的障碍。本文认为，基于基础模型和进化搜索的进化代理工作流程可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。通过云计算资源调度和ADMM参数适应的案例研究，本文展示了这一方法如何连接学术界创新与工业实施之间的差距。本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This position paper argues that optimization problem solving can transitionfrom expert-dependent to evolutionary agentic workflows. Traditionaloptimization practices rely on human specialists for problem formulation,algorithm selection, and hyperparameter tuning, creating bottlenecks thatimpede industrial adoption of cutting-edge methods. We contend that anevolutionary agentic workflow, powered by foundation models and evolutionarysearch, can autonomously navigate the optimization space, comprising problem,formulation, algorithm, and hyperparameter spaces. Through case studies incloud resource scheduling and ADMM parameter adaptation, we demonstrate howthis approach can bridge the gap between academic innovation and industrialimplementation. Our position challenges the status quo of human-centricoptimization workflows and advocates for a more scalable, adaptive approach tosolving real-world optimization problems.</description>
      <author>example@mail.com (Wenhao Li, Bo Jin, Mingyi Hong, Changhong Lu, Xiangfeng Wang)</author>
      <guid isPermaLink="false">2505.04354v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了医学图像采集过程中患者运动导致的图像模糊、鬼影和器官变形问题，提出了一种新的运动感知图像合成方法MAISY，有效提升了图像质量。&lt;h4&gt;背景&lt;/h4&gt;医学图像采集过程中患者运动会导致图像模糊、鬼影和器官变形，影响图像解读。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来校正医学图像中的运动伪影，提升图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先通过Segment Anything Model (SAM)动态学习解剖边界处的空间模式，然后利用引入的Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MAISY模型在胸部和头部CT数据集上优于现有方法，PSNR提高了40%，SSIM提高了10%，Dice提高了16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法能够有效校正医学图像中的运动伪影，提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;During medical image acquisition, patient motion causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. The current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods, with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss, effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging.Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning: a Lecture Note</title>
      <link>http://arxiv.org/abs/2505.03861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本讲义旨在为数据科学或相关学科的低年级硕士生和博士生提供机器学习的基础理念。&lt;h4&gt;背景&lt;/h4&gt;讲义从现代机器学习的基本理念开始，以分类作为主要目标任务。&lt;h4&gt;目的&lt;/h4&gt;帮助学生掌握损失函数、反向传播、随机梯度下降、泛化、模型选择以及人工神经网络的基本模块。&lt;h4&gt;方法&lt;/h4&gt;基于这些基本理念，讲义深入探讨了无监督学习的概率方法，包括有向潜在变量模型、专家乘积、生成对抗网络和自回归模型。&lt;h4&gt;主要发现&lt;/h4&gt;讲义还涵盖了强化学习、集成方法和元学习等多种进一步的话题。&lt;h4&gt;结论&lt;/h4&gt;阅读完本讲义后，学生应准备好开始研究机器学习以及更广泛的人工智能领域的更高级主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This lecture note is intended to prepare early-year master's and PhD studentsin data science or a related discipline with foundational ideas in machinelearning. It starts with basic ideas in modern machine learning withclassification as a main target task. These basic ideas include lossformulation, backpropagation, stochastic gradient descent, generalization,model selection as well as fundamental blocks of artificial neural networks.Based on these basic ideas, the lecture note explores in depth the probablisticapproach to unsupervised learning, covering directed latent variable models,product of experts, generative adversarial networks and autoregressive models.Finally, the note ends by covering a diverse set of further topics, such asreinforcement learning, ensemble methods and meta-learning. After reading thislecture note, a student should be ready to embark on studying and researchingmore advanced topics in machine learning and more broadly artificialintelligence.</description>
      <author>example@mail.com (Kyunghyun Cho)</author>
      <guid isPermaLink="false">2505.03861v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
      <link>http://arxiv.org/abs/2505.02366v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无监督对比学习方法，用于自然语言处理中的文本语义嵌入，该方法通过联合张量表示模量约束和交叉注意力机制，提高了对比学习的效果。&lt;h4&gt;背景&lt;/h4&gt;无监督对比学习在自然语言处理中成为热门研究主题，现有工作通常关注正负样本在高维语义空间中的方向分布，但忽略了模量特征，导致对比学习效果不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练目标，旨在对语义表示张量施加模量约束，以增强对比学习中正样本的对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 提出一种新的训练目标，用于对语义表示张量施加模量约束。2. 提出了一种交叉注意力结构，增强模型对CLS token的关注，优化CLS Pooling的质量。3. 结合上述两种动机，提出了一种新的联合张量表示模量约束和交叉注意力无监督对比学习文本嵌入框架JTCSE。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，JTCSE的孪塔集成模型和单塔蒸馏模型优于其他基线，成为当前SOTA。此外，JTCSE在超过130个下游零样本任务上整体优于其他基线。&lt;h4&gt;结论&lt;/h4&gt;JTCSE框架通过联合张量表示模量约束和交叉注意力机制，有效提高了无监督对比学习在自然语言处理中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new Joint Tensor representation modulus constraint and Cross-attention unsupervised contrastive learning Sentence Embedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tianyuzong/jtcse&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised contrastive learning has become a hot research topic in naturallanguage processing. Existing works usually aim at constraining the orientationdistribution of the representations of positive and negative samples in thehigh-dimensional semantic space in contrastive learning, but the semanticrepresentation tensor possesses both modulus and orientation features, and theexisting works ignore the modulus feature of the representations and causeinsufficient contrastive learning. % Therefore, we firstly propose a trainingobjective that aims at modulus constraints on the semantic representationtensor, to strengthen the alignment between the positive samples in contrastivelearning. Therefore, we first propose a training objective that is designed toimpose modulus constraints on the semantic representation tensor, to strengthenthe alignment between positive samples in contrastive learning. Then, theBERT-like model suffers from the phenomenon of sinking attention, leading to alack of attention to CLS tokens that aggregate semantic information. Inresponse, we propose a cross-attention structure among the twin-tower ensemblemodels to enhance the model's attention to CLS token and optimize the qualityof CLS Pooling. Combining the above two motivations, we propose a new\textbf{J}oint \textbf{T}ensor representation modulus constraint and\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence\textbf{E}mbedding representation framework JTCSE, which we evaluate in sevensemantic text similarity computation tasks, and the experimental results showthat JTCSE's twin-tower ensemble model and single-tower distillation modeloutperform the other baselines and become the current SOTA. In addition, wehave conducted an extensive zero-shot downstream task evaluation, which showsthat JTCSE outperforms other baselines overall on more than 130 tasks.</description>
      <author>example@mail.com (Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu)</author>
      <guid isPermaLink="false">2505.02366v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
      <link>http://arxiv.org/abs/2505.04119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GAPrompt的新方法，用于提高3D视觉模型在点云数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的3D视觉模型在点云数据上表现出色，但完全微调这些模型对于下游任务来说计算成本高且存储密集。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提高3D视觉模型的可适应性和性能，同时减少计算和存储成本。&lt;h4&gt;方法&lt;/h4&gt;GAPrompt利用几何线索来增强3D视觉模型的适应性，包括引入点提示和点平移提示器，以及Prompt传播机制。&lt;h4&gt;主要发现&lt;/h4&gt;GAPrompt在多个基准测试中显著优于现有的参数高效微调方法，并且与全微调相比，只使用了2.19%的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;GAPrompt是一种有效的参数高效微调方法，能够提高3D视觉模型在点云数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained 3D vision models have gained significant attention for theirpromising performance on point cloud data. However, fully fine-tuning thesemodels for downstream tasks is computationally expensive and storage-intensive.Existing parameter-efficient fine-tuning (PEFT) approaches, which focusprimarily on input token prompting, struggle to achieve competitive performancedue to their limited ability to capture the geometric information inherent inpoint clouds. To address this challenge, we propose a novel Geometry-AwarePoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance theadaptability of 3D vision models. First, we introduce a Point Prompt thatserves as an auxiliary input alongside the original point cloud, explicitlyguiding the model to capture fine-grained geometric details. Additionally, wepresent a Point Shift Prompter designed to extract global shape informationfrom the point cloud, enabling instance-specific geometric adjustments at theinput level. Moreover, our proposed Prompt Propagation mechanism incorporatesthe shape information into the model's feature extraction process, furtherstrengthening its ability to capture essential geometric characteristics.Extensive experiments demonstrate that GAPrompt significantly outperformsstate-of-the-art PEFT methods and achieves competitive results compared to fullfine-tuning on various benchmarks, while utilizing only 2.19% of trainableparameters. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04119v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
      <link>http://arxiv.org/abs/2505.04018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的深度学习框架，用于结构健康监测和控制，通过结合图神经网络（GNN）、变换器和物理信息损失函数，实现了对结构群体进行模态分解和识别。&lt;h4&gt;背景&lt;/h4&gt;模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种能够准确分解动态响应并识别模态特性的模型，无需标签数据，且不受外部载荷或结构配置变化的影响。&lt;h4&gt;方法&lt;/h4&gt;该模型通过将图神经网络（GNN）和变换器模块结合，将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，同时利用GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。模型以纯物理信息和无监督的方式进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟和实验室实验验证，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;与现有的模态识别技术和模型变体相比，该模型在性能上具有优越性，是一种有利的基于群体的结构健康监测方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。本研究提出了一种新型的深度学习框架，该框架结合了图神经网络（GNN）、变换器和物理信息损失函数，以实现对结构群体进行模态分解和识别。变换器模块将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，从而便于识别自然频率和阻尼比。同时，GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。所提出的模型以纯物理信息和无监督的方式进行训练，利用模态分解理论和结构模态的独立性来指导学习，无需标签数据。通过数值模拟和实验室实验的验证表明，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面具有有效性。与现有的模态识别技术和模型变体进行的比较分析进一步强调了其优越性能，使其成为基于群体的结构健康监测的有利方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modal identification is crucial for structural health monitoring andstructural control, providing critical insights into structural dynamics andperformance. This study presents a novel deep learning framework thatintegrates graph neural networks (GNNs), transformers, and a physics-informedloss function to achieve modal decomposition and identification across apopulation of structures. The transformer module decomposesmulti-degrees-of-freedom (MDOF) structural dynamic measurements intosingle-degree-of-freedom (SDOF) modal responses, facilitating theidentification of natural frequencies and damping ratios. Concurrently, the GNNcaptures the structural configurations and identifies mode shapes correspondingto the decomposed SDOF modal responses. The proposed model is trained in apurely physics-informed and unsupervised manner, leveraging modal decompositiontheory and the independence of structural modes to guide learning without theneed for labeled data. Validation through numerical simulations and laboratoryexperiments demonstrates its effectiveness in accurately decomposing dynamicresponses and identifying modal properties from sparse structural dynamicmeasurements, regardless of variations in external loads or structuralconfigurations. Comparative analyses against established modal identificationtechniques and model variations further underscore its superior performance,positioning it as a favorable approach for population-based structural healthmonitoring.</description>
      <author>example@mail.com (Xudong Jian, Kiran Bacsa, Gregory Duthé, Eleni Chatzi)</author>
      <guid isPermaLink="false">2505.04018v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</title>
      <link>http://arxiv.org/abs/2505.03821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Dataset:  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型（VLMs）在执行视觉视角推理的能力，通过一组新的视觉任务进行评估。&lt;h4&gt;背景&lt;/h4&gt;研究者受到人类测试的启发，设计了新的视觉任务来测试VLMs。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在场景理解、空间推理和视觉视角推理三个层面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;研究者利用精心控制的场景，其中一个人形迷你人物与一个物体配对，通过改变物体位置、人形迷你人物的方向以及使用鸟瞰图和表面视图，创建了144个独特的视觉任务。每个任务都与一系列7个诊断问题相匹配，以评估上述三个认知层面。&lt;h4&gt;主要发现&lt;/h4&gt;评估了包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型。结果显示，这些模型在场景理解方面表现良好，但在空间推理方面表现显著下降，在视角推理方面进一步恶化。&lt;h4&gt;结论&lt;/h4&gt;分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了视觉语言模型（VLMs）执行视觉视角推理的能力，通过一组受人类测试启发的全新视觉任务进行探究。我们的方法利用了精心控制的场景，其中一个人形迷你人物与一个物体配对。通过系统地改变空间配置，如物体相对于人形迷你人物的位置和人形迷你人物的方向，以及使用鸟瞰图和表面视图，我们创建了144个独特的视觉任务。每个视觉任务都与一系列7个诊断问题相匹配，旨在评估三个层次的视觉认知：场景理解、空间推理和视觉视角推理。我们对包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型进行了评估，发现尽管它们在场景理解方面表现出色，但在空间推理方面的表现显著下降，在视角推理方面的表现进一步恶化。我们的分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the ability of Vision Language Models (VLMs) to perform visualperspective taking using a novel set of visual tasks inspired by establishedhuman tests. Our approach leverages carefully controlled scenes, in which asingle humanoid minifigure is paired with a single object. By systematicallyvarying spatial configurations - such as object position relative to thehumanoid minifigure and the humanoid minifigure's orientation - and using bothbird's-eye and surface-level views, we created 144 unique visual tasks. Eachvisual task is paired with a series of 7 diagnostic questions designed toassess three levels of visual cognition: scene understanding, spatialreasoning, and visual perspective taking. Our evaluation of severalstate-of-the-art models, including GPT-4-Turbo, GPT-4o,Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals thatwhile they excel in scene understanding, the performance declines significantlyon spatial reasoning and further deteriorates on perspective-taking. Ouranalysis suggests a gap between surface-level object recognition and the deeperspatial and perspective reasoning required for complex visual tasks, pointingto the need for integrating explicit geometric representations and tailoredtraining protocols in future VLM development.</description>
      <author>example@mail.com (Gracjan Góral, Alicja Ziarko, Piotr Miłoś, Michał Nauman, Maciej Wołczyk, Michał Kosiński)</author>
      <guid isPermaLink="false">2505.03821v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems</title>
      <link>http://arxiv.org/abs/2505.03946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了高性能计算（HPC）环境中的资源分配问题，提出了一种基于DD-PPO算法的新型调度器，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;高性能计算环境中的资源分配是一个复杂的问题，调度算法不仅要高效分配系统资源，还要优化多个性能指标，如作业等待时间和系统利用率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更加适应性和智能的调度策略，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于DD-PPO算法的调度器，该算法支持大规模分布式训练，且无需在每一步进行参数同步。&lt;h4&gt;主要发现&lt;/h4&gt;与传统调度器和现有的基于RL的调度算法相比，DD-PPO调度器在调度性能上有所提升。&lt;h4&gt;结论&lt;/h4&gt;DD-PPO调度器通过消除对集中更新共享策略的依赖，提高了可扩展性、训练效率和样本利用率，从而在HPC调度中展现出更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高性能计算（HPC）环境中的资源分配对作业调度算法提出了复杂且多方面的挑战。除了高效分配系统资源之外，调度器还必须考虑并优化多个性能指标，包括作业等待时间和系统利用率。虽然基于规则的调度算法在当前的HPC系统部署中占主导地位，但随着这些系统的异构性和规模的增加，预计将挑战这些算法在最小化作业等待时间和最大化利用率方面的效率和灵活性。最近的研究努力集中在利用强化学习（RL）的进步来开发更适应性和智能的调度策略。最近基于RL的调度方法探索了从深度Q网络（DQN）到近端策略优化（PPO）等多种算法，以及最近将图神经网络与RL技术相结合的混合方法。然而，这些方法的共同局限性是它们依赖于相对较小的数据集，并且当使用大数据集时，这些方法面临着可扩展性问题。本研究介绍了一种基于DD-PPO算法的新型RL调度器，该算法支持跨多个工作者的分布式训练，而无需在每一步进行参数同步。通过消除对共享策略集中更新的依赖，DD-PPO调度器提高了可扩展性、训练效率和样本利用率。验证数据集利用了超过1150万个真实的HPC作业跟踪数据，用于比较DD-PPO在传统和先进调度方法之间的性能，实验结果表明，与基于规则的调度器和现有的基于RL的调度算法相比，DD-PPO调度器的调度性能有所改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Resource allocation in High Performance Computing (HPC) environments presentsa complex and multifaceted challenge for job scheduling algorithms. Beyond theefficient allocation of system resources, schedulers must account for andoptimize multiple performance metrics, including job wait time and systemutilization. While traditional rule-based scheduling algorithms dominate thecurrent deployments of HPC systems, the increasing heterogeneity and scale ofthose systems is expected to challenge the efficiency and flexibility of thosealgorithms in minimizing job wait time and maximizing utilization. Recentresearch efforts have focused on leveraging advancements in ReinforcementLearning (RL) to develop more adaptable and intelligent scheduling strategies.Recent RL-based scheduling approaches have explored a range of algorithms, fromDeep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,hybrid methods that integrate Graph Neural Networks with RL techniques.However, a common limitation across these methods is their reliance onrelatively small datasets, and these methods face scalability issues when usinglarge datasets. This study introduces a novel RL-based scheduler utilizing theDecentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,which supports large-scale distributed training across multiple workers withoutrequiring parameter synchronization at every step. By eliminating reliance oncentralized updates to a shared policy, the DD-PPO scheduler enhancesscalability, training efficiency, and sample utilization. The validationdataset leveraged over 11.5 million real HPC job traces for comparing DD-PPOperformance between traditional and advanced scheduling approaches, and theexperimental results demonstrate improved scheduling performance in comparisonto both rule-based schedulers and existing RL-based scheduling algorithms.</description>
      <author>example@mail.com (Matthew Sgambati, Aleksandar Vakanski, Matthew Anderson)</author>
      <guid isPermaLink="false">2505.03946v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype</title>
      <link>http://arxiv.org/abs/2505.03853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRAPE的基因调控网络构建方法，通过结合预训练的大型语言模型和DNA序列模型，提高基因调控网络的构建效率。&lt;h4&gt;背景&lt;/h4&gt;预测基因扰动有助于在实验室实验之前识别关键基因，提高实验效率。构建基因调控网络对于理解和预测基因扰动的影响至关重要。&lt;h4&gt;目的&lt;/h4&gt;提高基因调控网络的构建效率和准确性，并能够捕捉基因间的潜在相互作用。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的大型语言模型和DNA序列模型提取基因描述和DNA序列数据中的特征，引入基因生物型信息，并通过图结构学习动态优化基因调控网络。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPE方法在公开数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;GRAPE方法通过结合多源信息和图结构学习，能够有效构建基因调控网络，提高基因扰动预测的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting genetic perturbations enables the identification of potentiallycrucial genes prior to wet-lab experiments, significantly improving overallexperimental efficiency. Since genes are the foundation of cellular life,building gene regulatory networks (GRN) is essential to understand and predictthe effects of genetic perturbations. However, current methods fail to fullyleverage gene-related information, and solely rely on simple evaluation metricsto construct coarse-grained GRN. More importantly, they ignore functionaldifferences between biotypes, limiting the ability to capture potential geneinteractions. In this work, we leverage pre-trained large language model andDNA sequence model to extract features from gene descriptions and DNA sequencedata, respectively, which serve as the initialization for gene representations.Additionally, we introduce gene biotype information for the first time ingenetic perturbation, simulating the distinct roles of genes with differentbiotypes in regulating cellular processes, while capturing implicit generelationships through graph structure learning (GSL). We propose GRAPE, aheterogeneous graph neural network (HGNN) that leverages gene representationsinitialized with features from descriptions and sequences, models the distinctroles of genes with different biotypes, and dynamically refines the GRN throughGSL. The results on publicly available datasets show that our method achievesstate-of-the-art performance.</description>
      <author>example@mail.com (Changxi Chi, Jun Xia, Jingbo Zhou, Jiabei Cheng, Chang Yu, Stan Z. Li)</author>
      <guid isPermaLink="false">2505.03853v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>QStore: Quantization-Aware Compressed Model Storage</title>
      <link>http://arxiv.org/abs/2505.04081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了QStore，一种用于高效存储模型两种精度（高精度和低精度）的无损压缩格式。&lt;h4&gt;背景&lt;/h4&gt;现代应用广泛使用大型多模态基础模型，这些应用通常具有复杂的流程，需要存储和使用多个精度的相似模型。&lt;h4&gt;目的&lt;/h4&gt;降低存储成本，同时不牺牲模型加载速度。&lt;h4&gt;方法&lt;/h4&gt;QStore存储低精度模型以及重建高精度模型所需的残差信息，而不是分别存储低精度和高精度模型。&lt;h4&gt;主要发现&lt;/h4&gt;QStore在压缩多个精度的流行基础模型时，可以将整体存储占用减少高达2.2倍（原大小的45%），同时模型保存和加载速度比现有方法快1.7倍和1.8倍。&lt;h4&gt;结论&lt;/h4&gt;QStore是一种有效的解决方案，可以同时存储模型的两种精度，同时显著降低存储成本并提高加载速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern applications commonly leverage large, multi-modal foundation models.These applications often feature complex workflows that demand the storage andusage of similar models in multiple precisions. A straightforward approach isto maintain a separate file for each model precision (e.g., INT8, BF16), whichis indeed the approach taken by many model providers such as HuggingFace andOllama. However, this approach incurs excessive storage costs since a higherprecision model (e.g., BF16) is a strict superset of a lower precision model(e.g., INT8) in terms of information. Unfortunately, simply maintaining onlythe higher-precision model and requiring every user to dynamically convert themodel precision is not desirable because every user of lower precision modelsmust pay the cost for model download and precision conversion.  In this paper, we present QStore, a unified, lossless compression format forsimultaneously storing a model in two (high and low) precisions efficiently.Instead of storing low-precision and high-precision models separately, QStorestores low-precision model and only the residual information needed toreconstruct high-precision models. The size of residual information issignificantly smaller than the original high-precision models, thus achievinghigh savings in storage cost. Moreover, QStore does not compromise the speed ofmodel loading. The low-precision models can be loaded quickly just like before.The high-precision models can also be reconstructed efficiently in memory bymerging low-precision data and the residual with QStore's lightweight decodinglogic. We evaluate QStore for compressing multiple precisions of popularfoundation models, and show that QStore reduces overall storage footprint by upto 2.2x (45% of the original size) while enabling up to 1.7x and 1.8x fastermodel saving and loading versus existing approaches.</description>
      <author>example@mail.com (Raunak Shah, Zhaoheng Li, Yongjoo Park)</author>
      <guid isPermaLink="false">2505.04081v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>BuildingBlock: A Hybrid Approach for Structured Building Generation</title>
      <link>http://arxiv.org/abs/2505.04051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025 (Conference Track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BuildingBlock的混合方法，用于生成多样化、结构化且层次一致的建筑模型，适用于游戏、虚拟现实和数字孪生等领域。&lt;h4&gt;背景&lt;/h4&gt;当前三维建筑生成方法面临生成多样化、结构化和层次一致建筑模型的挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出BuildingBlock方法来解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;BuildingBlock方法包括两个阶段：布局生成阶段（LGP）和建筑构建阶段（BCP）。LGP将基于箱子的布局生成重新定义为点云生成任务，使用新构建的建筑数据集和基于Transformer的扩散模型创建全局一致的布局。利用LLM将这些布局扩展为基于规则的层次化设计，无缝整合组件风格和空间结构。BCP利用这些布局来指导程序内容生成（PCG），实现局部可定制、高质量的结构化建筑生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，BuildingBlock方法在生成多样化、层次化结构化的建筑方面是有效的，在多个基准测试中达到了最先进的结果，并为进一步的可扩展和直观的建筑工作流程铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;BuildingBlock方法为三维建筑生成提供了一种新的有效途径，有助于推动相关领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3721238.3730705&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional building generation is vital for applications in gaming,virtual reality, and digital twins, yet current methods face challenges inproducing diverse, structured, and hierarchically coherent buildings. Wepropose BuildingBlock, a hybrid approach that integrates generative models,procedural content generation (PCG), and large language models (LLMs) toaddress these limitations. Specifically, our method introduces a two-phasepipeline: the Layout Generation Phase (LGP) and the Building Construction Phase(BCP).  LGP reframes box-based layout generation as a point-cloud generation task,utilizing a newly constructed architectural dataset and a Transformer-baseddiffusion model to create globally consistent layouts. With LLMs, these layoutsare extended into rule-based hierarchical designs, seamlessly incorporatingcomponent styles and spatial structures.  The BCP leverages these layouts to guide PCG, enabling local-customizable,high-quality structured building generation. Experimental results demonstrateBuildingBlock's effectiveness in generating diverse and hierarchicallystructured buildings, achieving state-of-the-art results on multiplebenchmarks, and paving the way for scalable and intuitive architecturalworkflows.</description>
      <author>example@mail.com (Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu)</author>
      <guid isPermaLink="false">2505.04051v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>VideoLLM Benchmarks and Evaluation: A Survey</title>
      <link>http://arxiv.org/abs/2505.03829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对视频大型语言模型（VideoLLMs）的基准和评估方法进行了全面分析。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）的快速发展推动了视频理解技术的进步。&lt;h4&gt;目的&lt;/h4&gt;旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别视频理解领域的发展方向。&lt;h4&gt;方法&lt;/h4&gt;分析了视频理解基准的特点、评估协议和局限性，包括闭集、开集和针对时间和时空理解任务的专业评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;指出了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。&lt;h4&gt;结论&lt;/h4&gt;提出了未来研究方向，包括改进基准设计、评估指标和协议，需要更多样化、多模态和可解释性强的基准。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）的快速发展推动了视频理解技术的显著进步。本文对专为视频大型语言模型（VideoLLMs）设计或使用的基准和评估方法进行了全面分析。我们考察了视频理解基准的现状，讨论了其特点、评估协议和局限性。论文分析了各种评估方法，包括闭集、开集以及针对时间和时空理解任务的专业评估。我们强调了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。此外，我们提出了未来研究方向，包括改进基准设计、评估指标和协议，包括需要更多样化、多模态和可解释性强的基准。本调查旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别利用大型语言模型推进视频理解领域的前景方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Large Language Models (LLMs) has catalyzedsignificant advancements in video understanding technologies. This surveyprovides a comprehensive analysis of benchmarks and evaluation methodologiesspecifically designed or used for Video Large Language Models (VideoLLMs). Weexamine the current landscape of video understanding benchmarks, discussingtheir characteristics, evaluation protocols, and limitations. The paperanalyzes various evaluation methodologies, including closed-set, open-set, andspecialized evaluations for temporal and spatiotemporal understanding tasks. Wehighlight the performance trends of state-of-the-art VideoLLMs across thesebenchmarks and identify key challenges in current evaluation frameworks.Additionally, we propose future research directions to enhance benchmarkdesign, evaluation metrics, and protocols, including the need for more diverse,multimodal, and interpretability-focused benchmarks. This survey aims to equipresearchers with a structured understanding of how to effectively evaluateVideoLLMs and identify promising avenues for advancing the field of videounderstanding with large language models.</description>
      <author>example@mail.com (Yogesh Kumar)</author>
      <guid isPermaLink="false">2505.03829v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2505.03848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  46 pages, 22 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种集成了拓扑数据分析（TDA）、自监督学习和迁移学习的高级聚类框架，用于处理半导体制造中生成的大量图像数据，以提高缺陷识别和产量优化。&lt;h4&gt;背景&lt;/h4&gt;半导体制造产生的图像数据量巨大，对于缺陷识别和产量优化至关重要，但通常超过了人工检查的能力。&lt;h4&gt;目的&lt;/h4&gt;旨在通过一种新的无监督图像聚类方法，提高高维、未标记数据在缺陷识别和产量优化方面的有效性。&lt;h4&gt;方法&lt;/h4&gt;该框架结合了TDA捕获内在拓扑特征，自监督学习从未标记数据中提取有意义的表示，以及迁移学习提高框架的适应性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和开源半导体图像数据集上验证，该框架成功识别了与缺陷模式和工艺变化一致的聚类。&lt;h4&gt;结论&lt;/h4&gt;本研究突出了结合TDA、自监督学习和迁移学习的变革潜力，为半导体制造和其他具有大规模图像数据集的领域提供了可扩展的主动过程监控和质量控制解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing generates vast amounts of image data, crucial fordefect identification and yield optimization, yet often exceeds manualinspection capabilities. Traditional clustering techniques struggle withhigh-dimensional, unlabeled data, limiting their effectiveness in capturingnuanced patterns. This paper introduces an advanced clustering framework thatintegrates deep Topological Data Analysis (TDA) with self-supervised andtransfer learning techniques, offering a novel approach to unsupervised imageclustering. TDA captures intrinsic topological features, while self-supervisedlearning extracts meaningful representations from unlabeled data, reducingreliance on labeled datasets. Transfer learning enhances the framework'sadaptability and scalability, allowing fine-tuning to new datasets withoutretraining from scratch. Validated on synthetic and open-source semiconductorimage datasets, the framework successfully identifies clusters aligned withdefect patterns and process variations. This study highlights thetransformative potential of combining TDA, self-supervised learning, andtransfer learning, providing a scalable solution for proactive processmonitoring and quality control in semiconductor manufacturing and other domainswith large-scale image datasets.</description>
      <author>example@mail.com (Janhavi Giri, Attila Lengyel, Don Kent, Edward Kibardin)</author>
      <guid isPermaLink="false">2505.03848v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
      <link>http://arxiv.org/abs/2505.01880v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5figures. This paper has been accepted for IJCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式音频-语言共学习网络（LOCO），用于定位部分伪造音频的伪造区域，通过协同学习和自监督方式在弱监督场景下提升定位性能。&lt;h4&gt;背景&lt;/h4&gt;现有的音频时间伪造定位方法依赖于使用细粒度注释训练高效网络，但在实际场景中获取这些注释成本高且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文旨在提出一种能够有效定位伪造区域的方法。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了音频-语言共学习模块，通过时间维度和全局维度的语义对齐来捕捉伪造的一致性特征；2. 使用语句级注释和可学习提示构建伪造感知提示，动态地将语义先验纳入时间内容特征；3. 应用伪造定位模块，基于融合的伪造类激活序列生成伪造提议；4. 引入渐进式细化策略，生成伪帧级标签，并利用监督语义对比学习增强真实内容与伪造内容之间的语义区别，从而持续优化伪造感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开基准数据集上的实验表明，提出的LOCO实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在音频时间伪造定位任务上取得了显著的成果，为解决现实场景中的挑战提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio temporal forgery localization (ATFL) aims to find the precise forgeryregions of the partial spoof audio that is purposefully modified. Existing ATFLmethods rely on training efficient networks using fine-grained annotations,which are obtained costly and challenging in real-world scenarios. To meet thischallenge, in this paper, we propose a progressive audio-language co-learningnetwork (LOCO) that adopts co-learning and self-supervision manners to promptlocalization performance under weak supervision scenarios. Specifically, anaudio-language co-learning module is first designed to capture forgeryconsensus features by aligning semantics from temporal and global perspectives.In this module, forgery-aware prompts are constructed by using utterance-levelannotations together with learnable prompts, which can incorporate semanticpriors into temporal content features dynamically. In addition, a forgerylocalization module is applied to produce forgery proposals based on fusedforgery-class activation sequences. Finally, a progressive refinement strategyis introduced to generate pseudo frame-level labels and leverage supervisedsemantic contrastive learning to amplify the semantic distinction between realand fake content, thereby continuously optimizing forgery-aware features.Extensive experiments show that the proposed LOCO achieves SOTA performance onthree public benchmarks.</description>
      <author>example@mail.com (Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo)</author>
      <guid isPermaLink="false">2505.01880v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
      <link>http://arxiv.org/abs/2505.02831v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Self-Representation Alignment for Diffusion Transformers&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Self-Representation Alignment (SRA)的方法，通过自我蒸馏的方式，在不引入额外复杂框架或依赖外部预训练模型的情况下，提高扩散变换器的内部表示学习，从而加速生成训练并提高生成质量。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么需要引入额外的复杂表示训练框架，要么依赖大规模预训练的表示基础模型来在原始生成训练过程中提供表示指导。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，使扩散变换器能够在不依赖外部表示组件的情况下提供表示指导。&lt;h4&gt;方法&lt;/h4&gt;SRA通过自我蒸馏的方式，将扩散变换器早期层的高噪声输出潜在表示与后期层的低噪声输出潜在表示进行对齐，以逐步增强仅在生成训练过程中的整体表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将SRA应用于DiTs和SiTs可以带来一致的性能提升。SRA不仅显著优于依赖于辅助复杂表示训练框架的方法，而且其性能与高度依赖强大外部表示先验的方法相当。&lt;h4&gt;结论&lt;/h4&gt;SRA是一种简单而直接的方法，可以有效地提高扩散变换器的表示学习，并显著提升生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated that learning a meaningful internalrepresentation can both accelerate generative training and enhance generationquality of the diffusion transformers. However, existing approaches necessitateto either introduce an additional and complex representation training frameworkor rely on a large-scale, pre-trained representation foundation model toprovide representation guidance during the original generative trainingprocess. In this study, we posit that the unique discriminative processinherent to diffusion transformers enables them to offer such guidance withoutrequiring external representation components. We therefore proposeSelf-Representation Alignment (SRA), a simple yet straightforward method thatobtain representation guidance through a self-distillation manner.Specifically, SRA aligns the output latent representation of the diffusiontransformer in earlier layer with higher noise to that in later layer withlower noise to progressively enhance the overall representation learning duringonly generative training process. Experimental results indicate that applyingSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRAnot only significantly outperforms approaches relying on auxiliary, complexrepresentation training frameworks but also achieves performance comparable tomethods that heavily dependent on powerful external representation priors.</description>
      <author>example@mail.com (Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang)</author>
      <guid isPermaLink="false">2505.02831v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments</title>
      <link>http://arxiv.org/abs/2505.03825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in Expert Systems with Applications (DOI pending)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF)的框架，用于从多维时间序列中学习有效的表示，并解决了低训练数据环境下学习复杂特征的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界系统中对多维时间序列进行分类需要精细学习复杂特征，如跨维依赖和类内变化，同时面临训练数据不足的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种既灵活又高效的方法来学习多维时间序列的有效表示，并提高分类性能。&lt;h4&gt;方法&lt;/h4&gt;ITA-CTF框架包括CTF模块和ITA模块。CTF模块学习时间序列的核心解释成分及其联合依赖，并通过对比损失优化来提高分类性能。ITA模块生成针对性强且信息丰富的增强数据，以强调原始数据中的类内模式，同时保留类属性。&lt;h4&gt;主要发现&lt;/h4&gt;与标准TF和多个深度学习基准相比，ITA-CTF在五个不同的分类任务上实现了显著的性能提升，最高可达18.7%。&lt;h4&gt;结论&lt;/h4&gt;ITA-CTF是一种有效的框架，可以在低训练数据环境下对多维时间序列进行分类，并通过智能增强和对比学习显著提高分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification of multi-dimensional time series from real-world systemsrequire fine-grained learning of complex features such as cross-dimensionaldependencies and intra-class variations-all under the practical challenge oflow training data availability. However, standard deep learning (DL) strugglesto learn generalizable features in low-data environments due to modeloverfitting. We propose a versatile yet data-efficient framework, IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effectiverepresentations from multi-dimensional time series. The CTF module learns coreexplanatory components of the time series (e.g., sensor factors, temporalfactors), and importantly, their joint dependencies. Notably, unlike standardtensor factorization (TF), the CTF module incorporates a new contrastive lossoptimization to induce similarity learning and class-awareness into the learntrepresentations for better classification performance. To strengthen thiscontrastive learning, the preceding ITA module generates targeted butinformative augmentations that highlight realistic intra-class patterns in theoriginal data, while preserving class-wise properties. This is achieved bydynamically sampling a "soft" class prototype to guide the warping of eachquery data sample, which results in an augmentation that is intelligentlypattern-mixed between the "soft" class prototype and the query sample. Theseaugmentations enable the CTF module to recognize complex intra-class variationsdespite the limited original training data, and seek out invariant class-wiseproperties for accurate classification performance. The proposed method iscomprehensively evaluated on five different classification tasks. Compared tostandard TF and several DL benchmarks, notable performance improvements up to18.7% were achieved.</description>
      <author>example@mail.com (Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau)</author>
      <guid isPermaLink="false">2505.03825v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
      <link>http://arxiv.org/abs/2505.02173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间序列聚类方法，用于将具有相似行为的时间序列数据分类到不同的组中。&lt;h4&gt;背景&lt;/h4&gt;时间序列聚类是一种无监督学习方法，已在医疗保健、金融、经济、能源和气候科学等领域得到应用。已有多种时间序列聚类方法被提出并使用超过四十年，大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的相似性度量方法，称为排序皮尔逊相关差异（RDPC），并将其应用于层次聚类，以评估和比较现有聚类算法的性能。&lt;h4&gt;方法&lt;/h4&gt;RDPC方法结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。&lt;h4&gt;主要发现&lt;/h4&gt;RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。&lt;h4&gt;结论&lt;/h4&gt;通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间序列聚类是一种无监督学习方法，用于将具有相似行为的时间序列数据分类到具有相似行为的组中。它在医疗保健、金融、经济、能源和气候科学等领域得到应用。已经提出了多种时间序列聚类方法，并且已经使用了超过四十年。大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。在这项工作中，我们提出了一种新的差异度量方法，称为排序皮尔逊相关差异（RDPC），它结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。它被纳入层次聚类中。性能被评估并与现有聚类算法进行了比较。结果表明，RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。最后，我们通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series clustering is an unsupervised learning method for classifyingtime series data into groups with similar behavior. It is used in applicationssuch as healthcare, finance, economics, energy, and climate science. Severaltime series clustering methods have been introduced and used for over fourdecades. Most of them focus on measuring either Euclidean distances orassociation dissimilarities between time series. In this work, we propose a newdissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),which combines a weighted average of a specified fraction of the largestelement-wise differences with the well-known Pearson correlation dissimilarity.It is incorporated into hierarchical clustering. The performance is evaluatedand compared with existing clustering algorithms. The results show that theRDPC algorithm outperforms others in complicated cases involving differentseasonal patterns, trends, and peaks. Finally, we demonstrate our method byclustering a random sample of customers from a Thai electricity consumptiontime series dataset into seven groups with unique characteristics.</description>
      <author>example@mail.com (Chutiphan Charoensuk, Nathakhun Wiroonsri)</author>
      <guid isPermaLink="false">2505.02173v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2505.03833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointExplainer的可解释诊断策略，用于分析手绘信号以早期诊断帕金森病，并通过实验证明其能够提供直观的解释而不会降低诊断性能。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在分析数字化手绘信号方面显示出潜力，但现有诊断方法的可解释性不足，这给临床信任带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出PointExplainer，旨在识别推动模型诊断的手绘区域，并量化它们对模型决策的相对贡献。&lt;h4&gt;方法&lt;/h4&gt;PointExplainer包括一个诊断模块和一个解释模块。诊断模块将手绘信号编码为3D点云以表示手绘轨迹；解释模块训练一个可解释的代理模型来近似黑盒诊断模型的行为。此外，还引入了一致性度量来解决解释中的忠实度问题。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集和一个新构建的数据集上的实验表明，PointExplainer能够提供直观的解释，并且没有诊断性能的下降。&lt;h4&gt;结论&lt;/h4&gt;PointExplainer是一种有效的可解释诊断策略，有助于提高临床对帕金森病早期诊断的信任。&lt;h4&gt;翻译&lt;/h4&gt;Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.</description>
      <author>example@mail.com (Xuechao Wang, Sven Nomm, Junqing Huang, Kadri Medijainen, Aaro Toomela, Michael Ruzhansky)</author>
      <guid isPermaLink="false">2505.03833v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating the Impact of Electrode Shift on Classification Performance in Electromyography-Based Motion Prediction Using Sliding-Window Normalization</title>
      <link>http://arxiv.org/abs/2504.03196v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要讨论了电磁肌电图（EMG）信号在假肢、辅助服装和康复等领域的应用，提出了滑动窗口归一化（SWN）技术来减少电极偏移引起的性能下降。&lt;h4&gt;背景&lt;/h4&gt;EMG信号广泛应用于多种领域，但在跨个体泛化、电极偏移和日常变化等方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出SWN技术，旨在减少电极偏移对分类性能的影响，从而提高EMG信号的实用性。&lt;h4&gt;方法&lt;/h4&gt;采用滑动窗口归一化方法，结合z分数归一化和滑动窗口技术，在实时预测场景下减少电极偏移导致的分类性能下降。&lt;h4&gt;主要发现&lt;/h4&gt;SWN技术在肘关节运动（休息、屈曲和伸展）的EMG信号分类中，将分类准确率的下降减少了1.0%，相比无归一化的情况提高了6.6%。当SWN与多电极位置混合策略结合使用时，分类准确率比基准提高了2.4%。&lt;h4&gt;结论&lt;/h4&gt;SWN技术能够有效减少电极偏移引起的性能下降，增强基于EMG的运动估计系统的实用性。&lt;h4&gt;翻译&lt;/h4&gt;Electromyography (EMG) signals are used in many applications, including prosthetic hands, assistive suits, and rehabilitation. Recent advances in motion estimation have improved performance, yet challenges remain in cross-subject generalization, electrode shift, and daily variations. When electrode shift occurs, both transfer learning and adversarial domain adaptation improve classification performance by reducing the performance gap to -1% (eight-class scenario). However, additional data are needed for re-training in transfer learning or for training in adversarial domain adaptation. To address this issue, we investigated a sliding-window normalization (SWN) technique in a real-time prediction scenario. This method combines z-score normalization with a sliding-window approach to reduce the decline in classification performance caused by electrode shift. We validated the effectiveness of SWN using experimental data from a target trajectory tracking task involving the right arm. For three motions classification (rest, flexion, and extension of the elbow) obtained from EMG signals, our offline analysis showed that SWN reduced the differential classification accuracy to -1.0%, representing a 6.6% improvement compared to the case without normalization (-7.6%). Furthermore, when SWN was combined with a strategy that uses a mixture of multiple electrode positions, classification accuracy improved by an additional 2.4% over the baseline. These results suggest that SWN can effectively reduce the performance degradation caused by electrode shift, thereby enhancing the practicality of EMG-based motion estimation systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are used in many applications, includingprosthetic hands, assistive suits, and rehabilitation. Recent advances inmotion estimation have improved performance, yet challenges remain incross-subject generalization, electrode shift, and daily variations. Whenelectrode shift occurs, both transfer learning and adversarial domainadaptation improve classification performance by reducing the performance gapto -1\% (eight-class scenario). However, additional data are needed forre-training in transfer learning or for training in adversarial domainadaptation. To address this issue, we investigated a sliding-windownormalization (SWN) technique in a real-time prediction scenario. This methodcombines z-score normalization with a sliding-window approach to reduce thedecline in classification performance caused by electrode shift. We validatedthe effectiveness of SWN using experimental data from a target trajectorytracking task involving the right arm. For three motions classification (rest,flexion, and extension of the elbow) obtained from EMG signals, our offlineanalysis showed that SWN reduced the differential classification accuracy to-1.0\%, representing a 6.6\% improvement compared to the case withoutnormalization (-7.6\%). Furthermore, when SWN was combined with a strategy thatuses a mixture of multiple electrode positions, classification accuracyimproved by an additional 2.4\% over the baseline. These results suggest thatSWN can effectively reduce the performance degradation caused by electrodeshift, thereby enhancing the practicality of EMG-based motion estimationsystems.</description>
      <author>example@mail.com (Taichi Tanaka, Isao Nambu, Yasuhiro Wada)</author>
      <guid isPermaLink="false">2504.03196v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective</title>
      <link>http://arxiv.org/abs/2505.03828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 tables, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面回顾了从自然语言处理角度的带有情感感知的推荐系统，涵盖了2023年到2025年初的进展。&lt;h4&gt;背景&lt;/h4&gt;电子商务平台产生了大量的用户反馈，如星级评分、书面评论和评论，但大多数推荐引擎主要依赖数值评分，往往忽略了自由文本中嵌入的细微意见。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过将情感分析集成到电子商务推荐器中，通过详细的意见提取来提高预测准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;本文将近期的工作分为四种主要方法：结合情感嵌入与用户物品交互的深度学习分类器、基于transformer的细微特征提取方法、传播情感信号的图神经网络和实时适应用户反馈的对话式推荐器。&lt;h4&gt;主要发现&lt;/h4&gt;本文总结了模型架构，并展示了情感如何通过推荐管道传递，影响基于对话的建议。关键挑战包括处理嘈杂或讽刺性文本、动态用户偏好和偏见缓解。&lt;h4&gt;结论&lt;/h4&gt;本文概述了研究差距，并提供了开发更智能、更公平、更以用户为中心的推荐工具的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; E-commerce platforms generate vast volumes of user feedback, such as starratings, written reviews, and comments. However, most recommendation enginesrely primarily on numerical scores, often overlooking the nuanced opinionsembedded in free text. This paper comprehensively reviews sentiment-awarerecommendation systems from a natural language processing perspective, coveringadvancements from 2023 to early 2025. It highlights the benefits of integratingsentiment analysis into e-commerce recommenders to enhance prediction accuracyand explainability through detailed opinion extraction. Our survey categorizesrecent work into four main approaches: deep learning classifiers that combinesentiment embeddings with user item interactions, transformer based methods fornuanced feature extraction, graph neural networks that propagate sentimentsignals, and conversational recommenders that adapt in real time to userfeedback. We summarize model architectures and demonstrate how sentiment flowsthrough recommendation pipelines, impacting dialogue-based suggestions. Keychallenges include handling noisy or sarcastic text, dynamic user preferences,and bias mitigation. Finally, we outline research gaps and provide a roadmapfor developing smarter, fairer, and more user-centric recommendation tools.</description>
      <author>example@mail.com (Yogesh Gajula)</author>
      <guid isPermaLink="false">2505.03828v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
      <link>http://arxiv.org/abs/2505.03799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in International Joint Conference on Neural Networks  (IJCNN), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDM-InstructGLM的新型指令调整图语言模型框架，用于解决大规模图中LLMs在处理图结构时的可扩展性和效率问题。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，但在处理图相关问题时，由于可扩展性限制和缺乏针对图结构的专用机制，其应用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;旨在提高LLMs在图结构处理中的可扩展性和效率，同时避免依赖GNNs。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于相似度-度数偏差的随机游走机制，该机制根据节点特征相似度和度中心性选择性地采样和编码图信息，确保在LLM中实现自适应和结构化的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了令牌效率，减少了随机采样引起的信息损失，并在节点分类和链接预测等图相关任务上提高了性能。结果表明，仅使用LLMs进行图处理是可行的。&lt;h4&gt;结论&lt;/h4&gt;该研究为无GNN的图学习方法铺平了道路，利用LLMs作为独立的图推理模型。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong capabilities in variousnatural language processing tasks; however, their application to graph-relatedproblems remains limited, primarily due to scalability constraints and theabsence of dedicated mechanisms for processing graph structures. Existingapproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),using GNNs as feature encoders or auxiliary components. However, directlyencoding graph structures within LLMs has been underexplored, particularly inthe context of large-scale graphs where token limitations hinder effectiverepresentation. To address these challenges, we propose SDM-InstructGLM, anovel instruction-tuned Graph Language Model (InstructGLM) framework thatenhances scalability and efficiency without relying on GNNs. Our methodintroduces a similarity-degree-based biased random walk mechanism, whichselectively samples and encodes graph information based on node-featuresimilarity and degree centrality, ensuring an adaptive and structuredrepresentation within the LLM. This approach significantly improves tokenefficiency, mitigates information loss due to random sampling, and enhancesperformance on graph-based tasks such as node classification and linkprediction. Furthermore, our results demonstrate the feasibility of LLM-onlygraph processing, enabling scalable and interpretable Graph Language Models(GLMs) optimized through instruction-based fine-tuning. This work paves the wayfor GNN-free approaches to graph learning, leveraging LLMs as standalone graphreasoning models. Our source code is available on GitHub.</description>
      <author>example@mail.com (Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra)</author>
      <guid isPermaLink="false">2505.03799v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation</title>
      <link>http://arxiv.org/abs/2505.03844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用空间条件技术将卫星SAR图像转换为机载SAR表示的新方法，旨在解决SAR图像数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，合成孔径雷达（SAR）卫星图像的获取能力显著提高，但高分辨率SAR图像的获取仍然昂贵且有限。缺乏开源、标注良好的SAR文本图像数据集限制了现有基础模型在遥感应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;通过合成图像生成技术来扩充稀缺的SAR图像数据，以促进更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;利用ONERA超过15年的航空数据，创建了包含11万张SAR图像的全面训练数据集，并利用一个预训练的包含35亿参数的潜在扩散模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地将模拟图像与ONERA基于物理的模拟器EMPRISE生成的图像的现实感相连接，探索了AI在推进SAR成像技术中的应用。&lt;h4&gt;结论&lt;/h4&gt;本研究首次在文献中引入了这种新方法，为SAR图像处理领域提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The availability of Synthetic Aperture Radar (SAR) satellite imagery hasincreased considerably in recent years, with datasets commercially available.However, the acquisition of high-resolution SAR images in airborneconfigurations, remains costly and limited. Thus, the lack of open source,well-labeled, or easily exploitable SAR text-image datasets is a barrier to theuse of existing foundation models in remote sensing applications. In thiscontext, synthetic image generation is a promising solution to augment thisscarce data, enabling a broader range of applications. Leveraging over 15 yearsof ONERA's extensive archival airborn data from acquisition campaigns, wecreated a comprehensive training dataset of 110 thousands SAR images to exploita 3.5 billion parameters pre-trained latent diffusion model. In this work, wepresent a novel approach utilizing spatial conditioning techniques within afoundation model to transform satellite SAR imagery into airborne SARrepresentations. Additionally, we demonstrate that our pipeline is effectivefor bridging the realism of simulated images generated by ONERA's physics-basedsimulator EMPRISE. Our method explores a key application of AI in advancing SARimaging technology. To the best of our knowledge, we are the first to introducethis approach in the literature.</description>
      <author>example@mail.com (Solène Debuysère, Nicolas Trouvé, Nathan Letheule, Olivier Lévêque, Elise Colin)</author>
      <guid isPermaLink="false">2505.03844v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    </channel>
</rss>