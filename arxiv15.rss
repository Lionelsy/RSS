<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 20 May 2025 14:39:47 +0800</lastBuildDate>
    <item>
      <title>Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings</title>
      <link>http://arxiv.org/abs/2505.13087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于图对齐问题的图神经网络（GNN）的新基准测试方法。&lt;h4&gt;背景&lt;/h4&gt;图对齐问题是一种组合优化任务，通过将两个未标记的图对齐以最大化重叠边来泛化图同构。&lt;h4&gt;目的&lt;/h4&gt;将图对齐问题作为自监督学习任务，并生成图对齐数据集，以评估不同架构的性能。&lt;h4&gt;方法&lt;/h4&gt;使用合成随机图和来自多个领域的真实世界图数据集生成图对齐数据集。为给定图数据集，生成一系列难度递增的图对齐数据集。&lt;h4&gt;主要发现&lt;/h4&gt;各向异性图神经网络在性能上优于标准卷积架构。图对齐任务在无监督GNN预训练中表现出色，学习到的节点嵌入在三个分子回归任务上优于其他位置编码，并在PCQM4Mv2数据集上取得了最先进的成果，参数数量显著减少。&lt;h4&gt;结论&lt;/h4&gt;提供了开源Python包以生成图对齐数据集和基准测试新的GNN架构，支持可重复性和进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel benchmarking methodology for graph neural networks (GNNs)based on the graph alignment problem, a combinatorial optimization task thatgeneralizes graph isomorphism by aligning two unlabeled graphs to maximizeoverlapping edges. We frame this problem as a self-supervised learning task andpresent several methods to generate graph alignment datasets using syntheticrandom graphs and real-world graph datasets from multiple domains. For a givengraph dataset, we generate a family of graph alignment datasets with increasingdifficulty, allowing us to rank the performance of various architectures. Ourexperiments indicate that anisotropic graph neural networks outperform standardconvolutional architectures. To further demonstrate the utility of the graphalignment task, we show its effectiveness for unsupervised GNN pre-training,where the learned node embeddings outperform other positional encodings onthree molecular regression tasks and achieve state-of-the-art results on thePCQM4Mv2 dataset with significantly fewer parameters. To supportreproducibility and further research, we provide an open-source Python packageto generate graph alignment datasets and benchmark new GNN architectures.</description>
      <author>example@mail.com (Adrien Lagesse, Marc Lelarge)</author>
      <guid isPermaLink="false">2505.13087v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
  <item>
      <title>Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach</title>
      <link>http://arxiv.org/abs/2505.12902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）强化学习的设备间通信（D2D）功率分配方法，旨在优化延迟，同时确保用户公平性。&lt;h4&gt;背景&lt;/h4&gt;在无线通信中，追求速率最大化经常面临与用户公平性相关的大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决用户公平性问题，通过探索新的功率分配方法进行延迟优化。&lt;h4&gt;方法&lt;/h4&gt;采用集中式强化学习方法，中央控制器收集并处理状态信息，并使用近端策略优化（PPO）算法进行训练。将GNN层嵌入到PPO算法的actor和critic网络中，以更好地利用拓扑信息并增强方法的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有效减少了平均延迟，同时保证了用户公平性，优于基线方法，并显示出可扩展性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该功率分配方法在确保用户公平性的同时，有效降低了延迟，并在实际应用中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pursuit of rate maximization in wireless communication frequentlyencounters substantial challenges associated with user fairness. This paperaddresses these challenges by exploring a novel power allocation approach fordelay optimization, utilizing graph neural networks (GNNs)-based reinforcementlearning (RL) in device-to-device (D2D) communication. The proposed approachincorporates not only channel state information but also factors such as packetdelay, the number of backlogged packets, and the number of transmitted packetsinto the components of the state information. We adopt a centralized RL method,where a central controller collects and processes the state information. Thecentral controller functions as an agent trained using the proximal policyoptimization (PPO) algorithm. To better utilize topology information in thecommunication network and enhance the generalization of the proposed method, weembed GNN layers into both the actor and critic networks of the PPO algorithm.This integration allows for efficient parameter updates of GNNs and enables thestate information to be parameterized as a low-dimensional embedding, which isleveraged by the agent to optimize power allocation strategies. Simulationresults demonstrate that the proposed method effectively reduces average delaywhile ensuring user fairness, outperforms baseline methods, and exhibitsscalability and generalization capability.</description>
      <author>example@mail.com (Hao Fang, Kai Huang, Hao Ye, Chongtao Guo, Le Liang, Xiao Li, Shi Jin)</author>
      <guid isPermaLink="false">2505.12902v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cross-modal feature fusion for robust point cloud registration with ambiguous geometry</title>
      <link>http://arxiv.org/abs/2505.13088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.  19 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoFF的新颖的跨模态特征融合方法，用于点云配准，通过结合点云几何信息和RGB图像数据进行配准，以提升配准效果。&lt;h4&gt;背景&lt;/h4&gt;现有的点云配准方法往往忽略了从RGB图像中整合辐射信息的重要性，这限制了它们在仅凭几何数据不足以进行配准的区域的效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过结合点云几何信息和RGB图像数据，来提升点云配准的准确性和效果。&lt;h4&gt;方法&lt;/h4&gt;CoFF方法通过两阶段融合3D点云特征和2D图像特征来解决问题。第一阶段是跨模态特征融合模块，将图像特征分配给3D点云以增强3D点云特征；第二阶段是粗到精匹配模块，使用融合后的特征进行精确配准。&lt;h4&gt;主要发现&lt;/h4&gt;CoFF在四个常见数据集上进行了广泛的评估，包括3DMatch、3DLoMatch、IndoorLRS和ScanNet++数据集，结果显示CoFF在所有基准测试中均达到最先进的配准性能，例如在3DMatch和3DLoMatch数据集上分别实现了95.9%和81.6%的配准召回率。&lt;h4&gt;结论&lt;/h4&gt;CoFF方法有效地提升了点云配准的性能，特别是在几何信息不明确的区域，如对称相似性或平面结构区域。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云配准技术在深度学习技术的应用中取得了显著进展。然而，现有方法往往忽略了整合RGB图像的辐射信息。这种局限性降低了它们在配准点云对时的有效性，尤其是在仅几何数据不足以进行配准的区域。当有效地使用时，辐射信息可以通过提供从纯几何数据中缺失的上下文来增强配准过程。在本文中，我们提出了CoFF，一种新颖的跨模态特征融合方法，用于成对点云配准。假设点云和RGB图像之间的配准是可用的，CoFF通过两阶段融合3D点云特征和2D图像特征来明确解决仅几何信息不明确的问题，如在对称相似性或平面结构区域。它包含一个跨模态特征融合模块，将像素级的图像特征分配给3D输入点云以增强学习到的3D点云特征，并通过将图像块特征与superpoint特征相结合来提高粗匹配的质量。随后是一个粗到精匹配模块，使用融合后的特征准确建立对应关系。我们在四个常见数据集：3DMatch、3DLoMatch、IndoorLRS和最近发布的ScanNet++数据集上广泛评估了CoFF。此外，我们还对包含几何模糊案例的特定子数据集进行了评估。我们的实验结果表明，CoFF在所有基准测试中都实现了最先进的配准性能，包括在广泛使用的3DMatch和3DLoMatch数据集上分别实现了95.9%和81.6%的令人瞩目的配准召回率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration has seen significant advancements with theapplication of deep learning techniques. However, existing approaches oftenoverlook the potential of integrating radiometric information from RGB images.This limitation reduces their effectiveness in aligning point clouds pairs,especially in regions where geometric data alone is insufficient. When usedeffectively, radiometric information can enhance the registration process byproviding context that is missing from purely geometric data. In this paper, wepropose CoFF, a novel Cross-modal Feature Fusion method that utilizes bothpoint cloud geometry and RGB images for pairwise point cloud registration.Assuming that the co-registration between point clouds and RGB images isavailable, CoFF explicitly addresses the challenges where geometric informationalone is unclear, such as in regions with symmetric similarity or planarstructures, through a two-stage fusion of 3D point cloud features and 2D imagefeatures. It incorporates a cross-modal feature fusion module that assignspixel-wise image features to 3D input point clouds to enhance learned 3D pointfeatures, and integrates patch-wise image features with superpoint features toimprove the quality of coarse matching. This is followed by a coarse-to-finematching module that accurately establishes correspondences using the fusedfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. Inaddition, we assess CoFF on specific subset datasets containing geometricallyambiguous cases. Our experimental results demonstrate that CoFF achievesstate-of-the-art registration performance across all benchmarks, includingremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatchand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)</description>
      <author>example@mail.com (Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, Andreas Wieser)</author>
      <guid isPermaLink="false">2505.13088v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates</title>
      <link>http://arxiv.org/abs/2505.13316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures, accepted at ICME 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DDPM-PCC的基于Denoising Diffusion Probabilistic Model的点云压缩方法，用于低比特率压缩。&lt;h4&gt;背景&lt;/h4&gt;现有技术主要关注高保真重建，需要大量比特进行压缩。&lt;h4&gt;目的&lt;/h4&gt;针对带宽受限的应用，提出一种低比特率点云压缩方法。&lt;h4&gt;方法&lt;/h4&gt;使用PointNet编码器生成条件向量，并通过可学习的矢量量化器进行量化，以实现低比特率同时保持质量。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet和ModelNet40数据集上的实验表明，与标准方法和现有技术相比，在低比特率下实现了更好的率失真性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在低比特率压缩点云方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;Efficient compression of low-bit-rate point clouds is critical for bandwidth-constrained applications. However, existing techniques mainly focus on high-fidelity reconstruction, requiring many bits for compression. This paper proposes a 'Denoising Diffusion Probabilistic Model' (DDPM) architecture for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder produces the condition vector for the generation, which is then quantized via a learnable vector quantizer. This configuration allows to achieve a low bitrates while preserving quality. Experiments on ShapeNet and ModelNet40 show improved rate-distortion at low rates compared to standardized and state-of-the-art approaches. We publicly released the code at https://github.com/EIDOSLAB/DDPM-PCC.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/eidoslab/ddpm-pcc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient compression of low-bit-rate point clouds is critical forbandwidth-constrained applications. However, existing techniques mainly focuson high-fidelity reconstruction, requiring many bits for compression. Thispaper proposes a "Denoising Diffusion Probabilistic Model" (DDPM) architecturefor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoderproduces the condition vector for the generation, which is then quantized via alearnable vector quantizer. This configuration allows to achieve a low bitrateswhile preserving quality. Experiments on ShapeNet and ModelNet40 show improvedrate-distortion at low rates compared to standardized and state-of-the-artapproaches. We publicly released the code athttps://github.com/EIDOSLAB/DDPM-PCC.</description>
      <author>example@mail.com (Gabriele Spadaro, Alberto Presta, Jhony H. Giraldo, Marco Grangetto, Wei Hu, Giuseppe Valenzise, Attilio Fiandrotti, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2505.13316v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval</title>
      <link>http://arxiv.org/abs/2505.13306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GCRDP的新方法，用于解决Few-shot cross-modal retrieval中的问题，通过实验验证了其在四个基准数据集上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;Few-shot cross-modal retrieval关注在有限的训练样本下学习跨模态表示，以处理推理过程中的未见类别。与传统的跨模态检索任务不同，Few-shot retrieval涉及具有稀疏模态表示的数据。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法未能充分建模Few-shot cross-modal数据的复杂多峰分布，以及由此产生的潜在语义空间中的内模态偏差和外模态偏差问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为GCRDP的新方法，使用高斯混合模型（GMM）来捕捉数据的复杂多峰分布，并引入了多正样本对比学习机制进行全面的特征建模。此外，还引入了一种新的跨模态语义对齐策略，以改善跨模态表示的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;GCRDP方法在四个基准数据集上的实验中，表现优于六种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;GCRDP方法有效地解决了Few-shot cross-modal retrieval中的偏差问题，提高了检索的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Few-shot跨模态检索关注在有限的训练样本下学习跨模态表示，使模型能够在推理过程中处理未见类别。与假设训练和测试数据具有相同类别分布的传统跨模态检索任务不同，Few-shot检索涉及具有稀疏模态表示的数据。现有方法往往未能充分建模Few-shot跨模态数据的复杂多峰分布，导致潜在语义空间中存在两个主要偏差：内模态偏差，稀疏样本未能捕捉到类内多样性；外模态偏差，图像和文本分布之间的错位加剧了语义差距。这些偏差阻碍了检索的准确性。为了解决这些问题，我们提出了一种名为GCRDP的新方法，用于Few-shot跨模态检索。这种方法有效地使用高斯混合模型（GMM）捕捉数据的复杂多峰分布，并引入了多正样本对比学习机制以进行全面的特征建模。此外，我们还引入了一种新的跨模态语义对齐策略，通过约束图像和文本特征分布之间的相对距离，从而提高了跨模态表示的准确性。我们通过在四个基准数据集上的大量实验验证了我们的方法，证明了其相对于六种最先进方法的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot cross-modal retrieval focuses on learning cross-modalrepresentations with limited training samples, enabling the model to handleunseen classes during inference. Unlike traditional cross-modal retrievaltasks, which assume that both training and testing data share the same classdistribution, few-shot retrieval involves data with sparse representationsacross modalities. Existing methods often fail to adequately model themulti-peak distribution of few-shot cross-modal data, resulting in two mainbiases in the latent semantic space: intra-modal bias, where sparse samplesfail to capture intra-class diversity, and inter-modal bias, wheremisalignments between image and text distributions exacerbate the semantic gap.These biases hinder retrieval accuracy. To address these issues, we propose anovel method, GCRDP, for few-shot cross-modal retrieval. This approacheffectively captures the complex multi-peak distribution of data using aGaussian Mixture Model (GMM) and incorporates a multi-positive samplecontrastive learning mechanism for comprehensive feature modeling.Additionally, we introduce a new strategy for cross-modal semantic alignment,which constrains the relative distances between image and text featuredistributions, thereby improving the accuracy of cross-modal representations.We validate our approach through extensive experiments on four benchmarkdatasets, demonstrating superior performance over six state-of-the-art methods.</description>
      <author>example@mail.com (Chengsong Sun, Weiping Li, Xiang Li, Yuankun Liu, Lianlei Shan)</author>
      <guid isPermaLink="false">2505.13306v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning</title>
      <link>http://arxiv.org/abs/2505.12782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdaToken-3D的框架，用于优化3D场景理解中的大型多模态模型，以解决当前3D LMMs在计算效率和信息流冗余方面的问题。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型（LMMs）在3D场景理解方面表现出色，但使用数千个空间标记进行多模态推理的3D LMMs存在计算开销过大和信息流冗余的问题。&lt;h4&gt;目的&lt;/h4&gt;提出AdaToken-3D框架，旨在通过动态剪枝冗余标记，优化3D LMMs的计算效率和减少冗余信息流。&lt;h4&gt;方法&lt;/h4&gt;AdaToken-3D通过空间贡献分析动态剪枝冗余标记，并通过注意力模式挖掘量化标记级别的信息流，以自动调整不同3D LMM架构的剪枝策略。&lt;h4&gt;主要发现&lt;/h4&gt;在LLaVA-3D（一个7B参数的3D-LMM）上的实验表明，AdaToken-3D实现了21%的推理速度提升和63%的FLOPs减少，同时保持了原始任务精度。通过定量分析标记交互，发现超过60%的空间标记对最终预测的贡献很小（&lt;5%），为高效的3D多模态学习奠定了理论基础。&lt;h4&gt;结论&lt;/h4&gt;AdaToken-3D框架有效地提高了3D LMMs的效率和准确性，并通过定量分析揭示了多模态空间信息流中的冗余模式，为3D多模态学习提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;Large Multimodal Models (LMMs) have become a pivotal research focus in deep learning, demonstrating remarkable capabilities in 3D scene understanding. However, current 3D LMMs employing thousands of spatial tokens for multimodal reasoning suffer from critical inefficiencies: excessive computational overhead and redundant information flows. Unlike 2D VLMs processing single images, 3DLMMs exhibit inherent architectural redundancy due to the heterogeneous mechanisms between spatial tokens and visual tokens. To address this challenge, we propose AdaToken-3D, an adaptive spatial token optimization framework that dynamically prunes redundant tokens through spatial contribution analysis. Our method automatically tailors pruning strategies to different 3D LMM architectures by quantifying token-level information flows via attention pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM) demonstrate that AdaToken-3D achieves 21% faster inference speed and 63% FLOPs reduction while maintaining original task accuracy. Beyond efficiency gains, this work systematically investigates redundancy patterns in multimodal spatial information flows through quantitative token interaction analysis. Our findings reveal that over 60% of spatial tokens contribute minimally (&lt;5%) to the final predictions, establishing theoretical foundations for efficient 3D multimodal learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) have become a pivotal research focus in deeplearning, demonstrating remarkable capabilities in 3D scene understanding.However, current 3D LMMs employing thousands of spatial tokens for multimodalreasoning suffer from critical inefficiencies: excessive computational overheadand redundant information flows. Unlike 2D VLMs processing single images, 3DLMMs exhibit inherent architectural redundancy due to the heterogeneousmechanisms between spatial tokens and visual tokens. To address this challenge,we propose AdaToken-3D, an adaptive spatial token optimization framework thatdynamically prunes redundant tokens through spatial contribution analysis. Ourmethod automatically tailors pruning strategies to different 3D LMMarchitectures by quantifying token-level information flows via attentionpattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%FLOPs reduction while maintaining original task accuracy. Beyond efficiencygains, this work systematically investigates redundancy patterns in multimodalspatial information flows through quantitative token interaction analysis. Ourfindings reveal that over 60\% of spatial tokens contribute minimally ($&lt;$5\%)to the final predictions, establishing theoretical foundations for efficient 3Dmultimodal learning.</description>
      <author>example@mail.com (Kai Zhang, Xingyu Chen, Xiaofeng Zhang)</author>
      <guid isPermaLink="false">2505.12782v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MAGI-1: Autoregressive Video Generation at Scale</title>
      <link>http://arxiv.org/abs/2505.13211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为MAGI-1的世界模型，该模型通过自回归预测视频片段序列来生成视频，实现了时间建模和流式生成，并在图像到视频（I2V）任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前视频生成技术需要处理时间序列数据，而MAGI-1通过处理连续帧的固定长度片段来生成视频。&lt;h4&gt;目的&lt;/h4&gt;提高视频生成的质量和效率，同时支持可控制的生成和实时部署。&lt;h4&gt;方法&lt;/h4&gt;MAGI-1通过自回归预测视频片段序列，并训练以减少随时间单调增加的每块噪声，从而实现因果时间建模和流式生成。&lt;h4&gt;主要发现&lt;/h4&gt;MAGI-1在图像到视频（I2V）任务中表现出高时间一致性和可扩展性，其最大变体包含240亿个参数，支持长达400万个标记的上下文长度。&lt;h4&gt;结论&lt;/h4&gt;MAGI-1通过算法创新和专用基础设施实现了可扩展性和鲁棒性，其代码和模型可通过GitHub获取，产品可通过sand.ai访问。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为MAGI-1的世界模型，通过自回归预测一系列视频片段来生成视频，该片段定义为连续帧的固定长度段。经过训练以减少随时间单调增加的每块噪声，MAGI-1实现了因果时间建模和自然支持流式生成。它在基于文本指令的图像到视频（I2V）任务上取得了强大的性能，提供了高时间一致性和可扩展性，这些性能得益于多项算法创新和专用基础设施堆栈。MAGI-1通过块级提示实现可控生成，并通过保持恒定的峰值推理成本支持实时、内存高效的部署，无论视频长度如何。MAGI-1的最大变体包含240亿个参数，支持长达400万个标记的上下文长度，证明了我们方法的可扩展性和鲁棒性。代码和模型可在https://github.com/SandAI-org/MAGI-1和https://github.com/SandAI-org/MagiAttention获取。产品可通过https://sand.ai访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sandai-org/magiattention&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MAGI-1, a world model that generates videos by autoregressivelypredicting a sequence of video chunks, defined as fixed-length segments ofconsecutive frames. Trained to denoise per-chunk noise that increasesmonotonically over time, MAGI-1 enables causal temporal modeling and naturallysupports streaming generation. It achieves strong performance on image-to-video(I2V) tasks conditioned on text instructions, providing high temporalconsistency and scalability, which are made possible by several algorithmicinnovations and a dedicated infrastructure stack. MAGI-1 facilitatescontrollable generation via chunk-wise prompting and supports real-time,memory-efficient deployment by maintaining constant peak inference cost,regardless of video length. The largest variant of MAGI-1 comprises 24 billionparameters and supports context lengths of up to 4 million tokens,demonstrating the scalability and robustness of our approach. The code andmodels are available at https://github.com/SandAI-org/MAGI-1 andhttps://github.com/SandAI-org/MagiAttention. The product can be accessed athttps://sand.ai.</description>
      <author>example@mail.com (Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, Yuqiao Li)</author>
      <guid isPermaLink="false">2505.13211v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry</title>
      <link>http://arxiv.org/abs/2505.13210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于方言增强的多模态框架，用于分析古典诗词的情感，该框架结合了文本、音频和视觉特征，并在两个公开数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要基于文本意义分析情感，忽略了诗词中的节奏和视觉特征，以及方言中的古汉语语音特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的情感分析方法，以更全面地分析古典诗词的情感。&lt;h4&gt;方法&lt;/h4&gt;从诗词中提取句级音频特征，并纳入多种方言的音频，生成句级视觉特征，使用LLM翻译增强文本特征，并通过多模态对比表示学习融合多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在两个公开数据集上优于现有方法，准确率至少提高了2.51%，宏观F1值提高了1.63%。&lt;h4&gt;结论&lt;/h4&gt;该研究为多模态中文表示提供了新的思路和方法，并开源代码以促进该领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, a dialect-enhanced multimodal framework for sentiment analysis of classical Chinese poetry is proposed. The framework combines text, audio, and visual features, and achieves superior performance on two public datasets, outperforming existing methods by at least 2.51% in accuracy and 1.63% in macro F1. The research provides new insights and methods for multimodal Chinese representation and the code is open-sourced to facilitate research in this area.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhangdatalab/chinese_poetry_sentiment&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical Chinese poetry is a vital and enduring part of Chinese literature,conveying profound emotional resonance. Existing studies analyze sentimentbased on textual meanings, overlooking the unique rhythmic and visual featuresinherent in poetry,especially since it is often recited and accompanied byChinese paintings. In this work, we propose a dialect-enhanced multimodalframework for classical Chinese poetry sentiment analysis. We extractsentence-level audio features from the poetry and incorporate audio frommultiple dialects,which may retain regional ancient Chinese phonetic features,enriching the phonetic representation. Additionally, we generate sentence-levelvisual features, and the multimodal features are fused with textual featuresenhanced by LLM translation through multimodal contrastive representationlearning. Our framework outperforms state-of-the-art methods on two publicdatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macroF1. We open-source the code to facilitate research in this area and provideinsights for general multimodal Chinese representation.</description>
      <author>example@mail.com (Xiaocong Du, Haoyu Pei, Haipeng Zhang)</author>
      <guid isPermaLink="false">2505.13210v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Image Restoration for Video Surveillance: A Real-Time Approach</title>
      <link>http://arxiv.org/abs/2505.13130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文旨在解决计算机视觉领域中的图像质量问题，特别是对检测、分割、识别、监控和自动化解决方案的影响。通过提出一种实时图像恢复解决方案，提高了视频监控的图像质量。&lt;h4&gt;背景&lt;/h4&gt;图像退化，如雨、雾、光照等因素，对自动化决策产生负面影响。现有的图像恢复解决方案不适用于实时处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于视频监控的实时图像恢复解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用ResNet_50进行迁移学习，开发了一种模型，用于自动识别图像中存在的退化类型，并参考必要的处理方法进行图像恢复。&lt;h4&gt;主要发现&lt;/h4&gt;该解决方案具有灵活性和可扩展性的优势。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法为视频监控提供了实时图像恢复的解决方案，有助于提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;One of the major challenges in the field of computer vision especially for detection, segmentation, recognition, monitoring, and automated solutions, is the quality of images. Image degradation, often caused by factors such as rain, fog, lighting, etc., has a negative impact on automated decision-making. Furthermore, several image restoration solutions exist, including restoration models for single degradation and restoration models for multiple degradations. However, these solutions are not suitable for real-time processing. In this study, the aim was to develop a real-time image restoration solution for video surveillance. To achieve this, using transfer learning with ResNet_50, we developed a model for automatically identifying the types of degradation present in an image to reference the necessary treatment(s) for image restoration. Our solution has the advantage of being flexible and scalable.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.63075/2jepm102&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the major challenges in the field of computer vision especially fordetection, segmentation, recognition, monitoring, and automated solutions, isthe quality of images. Image degradation, often caused by factors such as rain,fog, lighting, etc., has a negative impact on automateddecision-making.Furthermore, several image restoration solutions exist,including restoration models for single degradation and restoration models formultiple degradations. However, these solutions are not suitable for real-timeprocessing. In this study, the aim was to develop a real-time image restorationsolution for video surveillance. To achieve this, using transfer learning withResNet_50, we developed a model for automatically identifying the types ofdegradation present in an image to reference the necessary treatment(s) forimage restoration. Our solution has the advantage of being flexible andscalable.</description>
      <author>example@mail.com (Muhammad Awais Amin, Adama Ilboudo, Abdul Samad bin Shahid, Amjad Ali, Waqas Haider Khan Bangyal)</author>
      <guid isPermaLink="false">2505.13130v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents</title>
      <link>http://arxiv.org/abs/2505.13291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Open source code available at  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,  MG and MW contributed equally, and should be considered joint first authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeSeriesGym是一个用于评估人工智能代理在时间序列机器学习工程挑战上的可扩展基准框架。&lt;h4&gt;背景&lt;/h4&gt;现有的基准缺乏可扩展性，仅关注在定义良好的环境中的模型构建，并且只评估有限的研究成果（如CSV提交文件）。&lt;h4&gt;目的&lt;/h4&gt;为了使人工智能代理的基准评估更符合机器学习工程实践，该框架在两个关键维度上进行了扩展。&lt;h4&gt;方法&lt;/h4&gt;首先，TimeSeriesGym结合了来自多个领域和任务的挑战，以评估多样化的技能。其次，它实现了对多种研究成果的评估机制，包括提交文件、代码和模型，并使用精确的数值测量和基于LLM的更灵活的评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架不仅评估独立能力，还评估能力的组合，并通过工具支持大规模挑战的设计。&lt;h4&gt;结论&lt;/h4&gt;尽管最初专注于时间序列应用，但该框架可以轻松扩展到其他数据模态，从而提高代理人工智能评估的全面性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个可扩展的基准测试框架TimeSeriesGym，用于评估人工智能代理在时间序列机器学习工程挑战上的表现。现有的基准测试缺乏可扩展性，只在定义良好的环境中关注模型构建，并且只评估有限的研究成果（例如CSV提交文件）。为了使人工智能代理的基准测试更符合机器学习工程的实践，我们的框架在两个关键维度上进行了扩展。首先，认识到有效的机器学习工程需要多种多样的技能，TimeSeriesGym结合了来自多个领域和任务的挑战，以评估孤立的能力（包括数据处理、理解研究仓库和代码转换）及其组合。我们不是独立解决每个挑战，而是开发了支持设计多个挑战的工具。其次，我们通过使用精确的数值测量和更灵活的基于LLM的评估方法，实现了对多种研究成果的评估机制，包括提交文件、代码和模型。这种双管齐下的策略在客观评估与情境判断之间取得平衡。尽管我们的初始重点是时间序列应用，但我们的框架可以轻松扩展到其他数据模态，从而提高代理人工智能评估的全面性和实用性。我们开源了这个基准测试框架，以促进未来关于人工智能代理机器学习工程能力的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/moment-timeseries-foundation-model/timeseriesgym&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TimeSeriesGym, a scalable benchmarking framework for evaluatingArtificial Intelligence (AI) agents on time series machine learning engineeringchallenges. Existing benchmarks lack scalability, focus narrowly on modelbuilding in well-defined settings, and evaluate only a limited set of researchartifacts (e.g., CSV submission files). To make AI agent benchmarking morerelevant to the practice of machine learning engineering, our framework scalesalong two critical dimensions. First, recognizing that effective ML engineeringrequires a range of diverse skills, TimeSeriesGym incorporates challenges fromdiverse sources spanning multiple domains and tasks. We design challenges toevaluate both isolated capabilities (including data handling, understandingresearch repositories, and code translation) and their combinations, and ratherthan addressing each challenge independently, we develop tools that supportdesigning multiple challenges at scale. Second, we implement evaluationmechanisms for multiple research artifacts, including submission files, code,and models, using both precise numeric measures and more flexible LLM-basedevaluation approaches. This dual strategy balances objective assessment withcontextual judgment. Although our initial focus is on time series applications,our framework can be readily extended to other data modalities, broadlyenhancing the comprehensiveness and practical utility of agentic AI evaluation.We open-source our benchmarking framework to facilitate future research on theML engineering capabilities of AI agents.</description>
      <author>example@mail.com (Yifu Cai, Xinyu Li, Mononito Goswami, Michał Wiliński, Gus Welter, Artur Dubrawski)</author>
      <guid isPermaLink="false">2505.13291v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow</title>
      <link>http://arxiv.org/abs/2505.13140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为CacheFlow的新型无监督流模型，用于3D人类动作预测，显著提高了预测速度，同时保持了预测精度。&lt;h4&gt;背景&lt;/h4&gt;现有的3D人类动作预测技术需要大量的推理时间，通常超过预测的时间范围。&lt;h4&gt;目的&lt;/h4&gt;开发一种更快的密度估计方法，以满足3D人类动作预测的需求。&lt;h4&gt;方法&lt;/h4&gt;CacheFlow利用一个无条件的流模型将高斯混合模型转换为未来动作的密度。通过预计算和缓存流模型的计算结果，将历史轨迹映射到高斯混合模型的样本上，从而减少计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;CacheFlow在标准基准数据集（如Human3.6M和AMASS）上，推理过程大约需要1毫秒，比以前的VAE方法快4倍，比以前的基于扩散的方法快30倍。该方法在密度估计精度上有所提高，并且在Human3.6M数据集上的预测精度与最先进的方法相当。&lt;h4&gt;结论&lt;/h4&gt;CacheFlow是一个有效的3D人类动作预测方法，具有快速、精确的特点。&lt;h4&gt;翻译&lt;/h4&gt;针对3D人类动作预测的密度估计方法需要大量推理时间的问题，我们提出了一种名为CacheFlow的新型基于流的方法。CacheFlow利用无条件的流模型将高斯混合模型转换为未来动作的密度，并且可以通过预计算和缓存流模型的计算结果来减少计算开销。在标准基准数据集上，CacheFlow的推理时间约为1毫秒，比以前的VAE方法快4倍，比以前的基于扩散的方法快30倍。该方法在密度估计精度上有所提高，在Human3.6M数据集上的预测精度与最先进的方法相当。我们的代码和模型将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many density estimation techniques for 3D human motion prediction require asignificant amount of inference time, often exceeding the duration of thepredicted time horizon. To address the need for faster density estimation for3D human motion prediction, we introduce a novel flow-based method for humanmotion prediction called CacheFlow. Unlike previous conditional generativemodels that suffer from time efficiency, CacheFlow takes advantage of anunconditional flow-based generative model that transforms a Gaussian mixtureinto the density of future motions. The results of the computation of theflow-based generative model can be precomputed and cached. Then, forconditional prediction, we seek a mapping from historical trajectories tosamples in the Gaussian mixture. This mapping can be done by a much morelightweight model, thus saving significant computation overhead compared to atypical conditional flow model. In such a two-stage fashion and by cachingresults from the slow flow model computation, we build our CacheFlow withoutloss of prediction accuracy and model expressiveness. This inference process iscompleted in approximately one millisecond, making it 4 times faster thanprevious VAE methods and 30 times faster than previous diffusion-based methodson standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, ourmethod demonstrates improved density estimation accuracy and comparableprediction accuracy to a SOTA method on Human3.6M. Our code and models will bepublicly available.</description>
      <author>example@mail.com (Takahiro Maeda, Jinkun Cao, Norimichi Ukita, Kris Kitani)</author>
      <guid isPermaLink="false">2505.13140v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AdS-GNN -- a Conformally Equivariant Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.12880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在广义共形变换下等变的神经网络，通过将数据从平坦欧几里得空间提升到反德西特（AdS）空间，利用了平坦空间共形变换与AdS空间等距变换之间的对应关系，实现了在几何深度学习文献中广泛研究的等距变换。该网络在计算机视觉和统计物理任务上表现出强大的性能，提高了泛化能力，并能够从训练网络中提取共形数据，如标度维度。&lt;h4&gt;背景&lt;/h4&gt;共形对称性，即保持角度的坐标变换，在物理学、数学、计算机视觉和（几何）机器学习等多个领域发挥着关键作用。&lt;h4&gt;目的&lt;/h4&gt;构建一个在广义共形变换下等变的神经网络。&lt;h4&gt;方法&lt;/h4&gt;将数据从平坦欧几里得空间提升到AdS空间，利用平坦空间共形变换与AdS空间等距变换之间的对应关系，采用基于正确距离的条件消息传递层，实现一个计算效率高的框架。&lt;h4&gt;主要发现&lt;/h4&gt;模型在计算机视觉和统计物理任务上表现出强大的性能，提高了泛化能力，并能够从训练网络中提取共形数据。&lt;h4&gt;结论&lt;/h4&gt;该神经网络能够有效处理共形数据，并在多个领域具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;Conformal symmetries, i.e. coordinate transformations that preserve angles, play a key role in many fields, including physics, mathematics, computer vision and (geometric) machine learning. Here we build a neural network that is equivariant under general conformal transformations. To achieve this, we lift data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to exploit a known correspondence between conformal transformations of flat space and isometric transformations on the AdS space. We then build upon the fact that such isometric transformations have been extensively studied on general geometries in the geometric deep learning literature. We employ message-passing layers conditioned on the proper distance, yielding a computationally efficient framework. We validate our model on tasks from computer vision and statistical physics, demonstrating strong performance, improved generalization capacities, and the ability to extract conformal data such as scaling dimensions from the trained network.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conformal symmetries, i.e.\ coordinate transformations that preserve angles,play a key role in many fields, including physics, mathematics, computer visionand (geometric) machine learning. Here we build a neural network that isequivariant under general conformal transformations. To achieve this, we liftdata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us toexploit a known correspondence between conformal transformations of flat spaceand isometric transformations on the AdS space. We then build upon the factthat such isometric transformations have been extensively studied on generalgeometries in the geometric deep learning literature. We employ message-passinglayers conditioned on the proper distance, yielding a computationally efficientframework. We validate our model on tasks from computer vision and statisticalphysics, demonstrating strong performance, improved generalization capacities,and the ability to extract conformal data such as scaling dimensions from thetrained network.</description>
      <author>example@mail.com (Maksim Zhdanov, Nabil Iqbal, Erik Bekkers, Patrick Forré)</author>
      <guid isPermaLink="false">2505.12880v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
      <link>http://arxiv.org/abs/2505.13227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OSWorld-G，一个包含564个细粒度标注样本的综合基准，旨在解决GUI grounding中的瓶颈问题。同时发布了Jedi数据集，包含400万个示例，并展示了基于Jedi的多尺度模型在多个基准测试中的有效性。&lt;h4&gt;背景&lt;/h4&gt;GUI grounding是将自然语言指令映射到图形用户界面特定动作的能力，目前的研究基准简化了grounding任务，未能捕捉现实世界交互的复杂性。&lt;h4&gt;目的&lt;/h4&gt;解决GUI grounding中的瓶颈问题，并提高计算机使用代理在复杂计算机任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;提出OSWorld-G基准，发布Jedi数据集，并在Jedi上训练多尺度模型。&lt;h4&gt;主要发现&lt;/h4&gt;Jedi数据集和基于Jedi的多尺度模型在多个基准测试中优于现有方法，Jedi的grounding能力直接提升了通用基础模型在复杂计算机任务上的能力。&lt;h4&gt;结论&lt;/h4&gt;通过详细的分析和验证，本文强调了专用数据在GUI grounding中的重要性，并开源了所有基准、数据、检查点和代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图形用户界面（GUI）的grounding，即将自然语言指令映射到特定动作的能力，在计算机使用代理开发中仍然是一个关键瓶颈。当前基准简化了grounding任务，将其视为简短的指称表达式，未能捕捉到需要软件常识、布局理解和精细操作能力的现实世界交互的复杂性。为了解决这些限制，我们引入了OSWorld-G，这是一个包含564个细粒度标注样本的综合基准，涵盖了包括文本匹配、元素识别、布局理解和精确操作在内的多种任务类型。此外，我们综合并发布了最大的计算机使用grounding数据集Jedi，它包含通过多角度解耦任务得到的400万个示例。我们在Jedi上训练的多尺度模型通过在ScreenSpot-v2、ScreenSpot-Pro和我们的OSWorld-G上的表现超过了现有方法，证明了其有效性。此外，我们证明了使用Jedi的改进grounding能力可以直接增强通用基础模型在复杂计算机任务上的代理能力，从OSWorld上的5%提高到27%。通过详细的消融研究，我们确定了影响grounding性能的关键因素，并验证了为不同界面元素组合专用数据可以促进对新界面的组合泛化。所有基准、数据、检查点和代码都是开源的，可在https://osworld-grounding.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) grounding, the ability to map natural languageinstructions to specific actions on graphical user interfaces, remains acritical bottleneck in computer use agent development. Current benchmarksoversimplify grounding tasks as short referring expressions, failing to capturethe complexity of real-world interactions that require software commonsense,layout understanding, and fine-grained manipulation capabilities. To addressthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising564 finely annotated samples across diverse task types including text matching,element recognition, layout understanding, and precise manipulation.Additionally, we synthesize and release the largest computer use groundingdataset Jedi, which contains 4 million examples through multi-perspectivedecoupling of tasks. Our multi-scale models trained on Jedi demonstrate itseffectiveness by outperforming existing approaches on ScreenSpot-v2,ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improvedgrounding with Jedi directly enhances agentic capabilities of generalfoundation models on complex computer tasks, improving from 5% to 27% onOSWorld. Through detailed ablation studies, we identify key factorscontributing to grounding performance and verify that combining specializeddata for different interface elements enables compositional generalization tonovel interfaces. All benchmark, data, checkpoints, and code are open-sourcedand available at https://osworld-grounding.github.io.</description>
      <author>example@mail.com (Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong)</author>
      <guid isPermaLink="false">2505.13227v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</title>
      <link>http://arxiv.org/abs/2505.11868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新框架，可以从单目视频中无监督地分析3D运动，以解决现有方法依赖密集多视图图像或详细部分级标注的局限性。&lt;h4&gt;背景&lt;/h4&gt;准确分析动态环境中的运动部分及其运动属性对于推进如具身智能等关键领域至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需标注训练数据，仅使用单目视频即可精确解析运动部分和运动属性的框架。&lt;h4&gt;方法&lt;/h4&gt;该方法首先通过深度估计、光流分析和点云配准方法构建场景几何，并大致分析运动部分及其初始运动属性；然后使用二维高斯扩散进行场景表示；最后，引入一个专门为关节对象设计的端到端动态场景优化算法，以细化初始分析结果，确保系统可以处理旋转、平移以及更复杂的运动（旋转+平移）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架可以有效地在无标注的情况下分析关节对象运动，展示了其在未来具身智能应用中的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架在无需标注数据的情况下，能够从单目视频中准确分析3D运动，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes an innovative framework that can analyze 3D motion from monocular videos in a zero-shot manner, addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations. The framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. The method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis, and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects is introduced, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. Experimental results show that the framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately analyzing the motion parts and their motion attributes in dynamicenvironments is crucial for advancing key areas such as embodied intelligence.Addressing the limitations of existing methods that rely on dense multi-viewimages or detailed part-level annotations, we propose an innovative frameworkthat can analyze 3D mobility from monocular videos in a zero-shot manner. Thisframework can precisely parse motion parts and motion attributes only using amonocular video, completely eliminating the need for annotated training data.Specifically, our method first constructs the scene geometry and roughlyanalyzes the motion parts and their initial motion attributes combining depthestimation, optical flow analysis and point cloud registration method, thenemploys 2D Gaussian splatting for scene representation. Building on this, weintroduce an end-to-end dynamic scene optimization algorithm specificallydesigned for articulated objects, refining the initial analysis results toensure the system can handle 'rotation', 'translation', and even complexmovements ('rotation+translation'), demonstrating high flexibility andversatility. To validate the robustness and wide applicability of our method,we created a comprehensive dataset comprising both simulated and real-worldscenarios. Experimental results show that our framework can effectively analyzearticulated object motions in an annotation-free manner, showcasing itssignificant potential in future embodied intelligence applications.</description>
      <author>example@mail.com (Hongyi Zhou, Xiaogang Wang, Yulan Guo, Kai Xu)</author>
      <guid isPermaLink="false">2505.11868v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps</title>
      <link>http://arxiv.org/abs/2505.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图像计算模型，用于预测人类在场景理解中的反应时间，并通过研究视觉系统的特性与任务相关视觉信息在图像中的空间分布之间的关系，探讨了视觉处理在理解难度形成中的重要性。&lt;h4&gt;背景&lt;/h4&gt;目前已有模型可以预测人类在目标搜索和视觉辨别等任务中的反应时间，但场景理解时间的图像计算预测器尚待开发。&lt;h4&gt;目的&lt;/h4&gt;利用视觉-语言模型（VLMs）和可比较的语言描述的定量指标，模型人类场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种融合了注视点视觉与VLMs的图像计算模型（F-SUM），以产生随注视点位置变化的空间解析场景理解图，并计算相应的F-SUM分数。&lt;h4&gt;主要发现&lt;/h4&gt;F-SUM分数与人类平均反应时间（N=17，相关系数r=0.47）和扫视次数（N=17，相关系数r=0.51）相关；也与人类描述准确度（N=16，相关系数r=-0.56）相关；且这些相关性超过了基于语言熵的杂乱、视觉复杂度和场景模糊性等标准图像指标。&lt;h4&gt;结论&lt;/h4&gt;F-SUM是一个能够预测人类场景理解反应时间的图像计算指标，证明了注视点视觉处理在形成理解难度中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although models exist that predict human response times (RTs) in tasks suchas target search and visual discrimination, the development of image-computablepredictors for scene understanding time remains an open challenge. Recentadvances in vision-language models (VLMs), which can generate scenedescriptions for arbitrary images, combined with the availability ofquantitative metrics for comparing linguistic descriptions, offer a newopportunity to model human scene understanding. We hypothesize that the primarybottleneck in human scene understanding and the driving source of variabilityin response times across scenes is the interaction between the foveated natureof the human visual system and the spatial distribution of task-relevant visualinformation within an image. Based on this assumption, we propose a novelimage-computable model that integrates foveated vision with VLMs to produce aspatially resolved map of scene understanding as a function of fixationlocation (Foveated Scene Understanding Map, or F-SUM), along with an aggregateF-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) andnumber of saccades (r=0.51) required to comprehend a scene (across 277 scenes).The F-SUM score also correlates with average (N=16) human description accuracy(r=-0.56) in time-limited presentations. These correlations significantlyexceed those of standard image-based metrics such as clutter, visualcomplexity, and scene ambiguity based on language entropy. Together, our workintroduces a new image-computable metric for predicting human response times inscene understanding and demonstrates the importance of foveated visualprocessing in shaping comprehension difficulty.</description>
      <author>example@mail.com (Ziqi Wen, Jonathan Skaza, Shravan Murlidaran, William Y. Wang, Miguel P. Eckstein)</author>
      <guid isPermaLink="false">2505.12660v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Representation of perceived prosodic similarity of conversational feedback</title>
      <link>http://arxiv.org/abs/2505.13268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了语音反馈（如“嗯”，“yeah”，“okay”）在口语对话中的重要性，探讨了其语音韵律相似性及其在现有语音表示中的反映。&lt;h4&gt;背景&lt;/h4&gt;语音反馈是口语对话中的重要组成部分，对于确保对话系统中的共同基础至关重要。&lt;h4&gt;目的&lt;/h4&gt;探究语音反馈的语音韵律相似性，以及现有语音表示如何反映这种相似性。&lt;h4&gt;方法&lt;/h4&gt;通过招募参与者进行三重比较任务，测量来自两个不同数据集的反馈响应的感知相似性。&lt;h4&gt;主要发现&lt;/h4&gt;频谱和自监督语音表示比提取的音高特征更好地编码韵律，尤其是在同一说话人的反馈情况下。此外，通过对比学习可以进一步压缩和调整表示以符合人类感知。&lt;h4&gt;结论&lt;/h4&gt;语音反馈的韵律相似性可以通过频谱和自监督语音表示来有效编码，且可以通过对比学习进一步优化以符合人类感知。&lt;h4&gt;翻译&lt;/h4&gt;摘要：语音反馈（例如，`mhm'，`yeah'，`okay'）是口语对话的一个重要组成部分，对于确保对话系统中的共同基础至关重要。这种反馈的确切意义是通过词汇和韵律形式传达的。在本研究中，我们调查了具有相同词汇形式的语音反馈的感知韵律相似性，以及现有语音表示在多大程度上反映了这种相似性。我们使用招募的参与者的三重比较任务来测量来自两个不同数据集的反馈响应的感知相似性。我们发现，频谱和自监督语音表示比提取的音高特征更好地编码韵律，特别是在同一说话人的反馈情况下。我们还发现，通过对比学习可以进一步压缩和调整表示以符合人类感知。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component ofspoken dialogue and is crucial to ensuring common ground in conversationalsystems. The exact meaning of such feedback is conveyed through both lexicaland prosodic form. In this work, we investigate the perceived prosodicsimilarity of vocal feedback with the same lexical form, and to what extentexisting speech representations reflect such similarities. A triadic comparisontask with recruited participants is used to measure perceived similarity offeedback responses taken from two different datasets. We find that spectral andself-supervised speech representations encode prosody better than extractedpitch features, especially in the case of feedback from the same speaker. Wealso find that it is possible to further condense and align the representationsto human perception through contrastive learning.</description>
      <author>example@mail.com (Livia Qian, Carol Figueroa, Gabriel Skantze)</author>
      <guid isPermaLink="false">2505.13268v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR</title>
      <link>http://arxiv.org/abs/2505.13079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Graph Matching Optimal Transport (GM-OT)的方法，用于将预训练语言模型（PLM）的语料知识迁移到声学特征学习，以提升端到端自动语音识别（E2E-ASR）的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管将PLM的语料知识迁移到声学特征学习对E2E-ASR性能提升有效，但由于语言和声学模态之间的固有差距，如何对齐这些模态之间的表示仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出GM-OT方法，以解决语言和声学模态表示对齐的问题，并提高知识迁移的效率。&lt;h4&gt;方法&lt;/h4&gt;GM-OT方法将语言和声学序列建模为结构化图，节点代表特征嵌入，边则捕捉时间和顺序关系。该方法同时最小化节点间的Wasserstein距离（WD）和边间的Gromov-Wasserstein距离（GWD），从而得到融合的Gromov-Wasserstein距离（FGWD）公式。&lt;h4&gt;主要发现&lt;/h4&gt;GM-OT方法实现了结构化的对齐，比现有的基于OT的方法更有效地进行知识迁移。理论分析表明，现有的基于OT的语言知识迁移方法可以看作是GM-OT框架的一个特例。&lt;h4&gt;结论&lt;/h4&gt;在基于CTC的E2E-ASR系统上，使用PLM进行知识迁移的实验结果表明，GM-OT方法在普通话ASR任务上取得了显著的性能提升，验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：将预训练语言模型（PLM）的语料知识迁移到声学特征学习已被证明对提升端到端自动语音识别（E2E-ASR）性能有效。然而，由于语言和声学模态之间的固有差距，对齐这些模态之间的表示仍然是一个挑战。最优传输（OT）通过最小化语言和声学特征分布之间的Wasserstein距离（WD）显示出缓解这些差距的潜力。然而，之前的基于OT的方法忽视了结构关系，将特征向量视为无序集。为了解决这个问题，我们提出了图匹配最优传输（GM-OT），该方法将语言和声学序列建模为结构化图。节点代表特征嵌入，而边则捕捉时间和顺序关系。GM-OT同时最小化节点间的WD和边间的Gromov-Wasserstein距离（GWD），从而得到融合的Gromov-Wasserstein距离（FGWD）公式。这实现了结构化的对齐，比现有的基于OT的方法更有效地进行知识迁移。理论分析进一步表明，现有的基于OT的语言知识迁移方法可以看作是我们GM-OT框架的一个特例。我们在基于CTC的E2E-ASR系统上，使用PLM进行知识迁移，对GM-OT进行了评估。实验结果表明，与最先进的模型相比，GM-OT在普通话ASR任务上取得了显著的性能提升，验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring linguistic knowledge from a pretrained language model (PLM) toacoustic feature learning has proven effective in enhancing end-to-endautomatic speech recognition (E2E-ASR). However, aligning representationsbetween linguistic and acoustic modalities remains a challenge due to inherentmodality gaps. Optimal transport (OT) has shown promise in mitigating thesegaps by minimizing the Wasserstein distance (WD) between linguistic andacoustic feature distributions. However, previous OT-based methods overlookstructural relationships, treating feature vectors as unordered sets. Toaddress this, we propose Graph Matching Optimal Transport (GM-OT), which modelslinguistic and acoustic sequences as structured graphs. Nodes represent featureembeddings, while edges capture temporal and sequential relationships. GM-OTminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)formulation. This enables structured alignment and more efficient knowledgetransfer compared to existing OT-based approaches. Theoretical analysis furthershows that prior OT-based methods in linguistic knowledge transfer can beviewed as a special case within our GM-OT framework. We evaluate GM-OT onMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledgetransfer. Experimental results demonstrate significant performance gains overstate-of-the-art models, validating the effectiveness of our approach.</description>
      <author>example@mail.com (Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai)</author>
      <guid isPermaLink="false">2505.13079v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2505.13115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为TREA的新数据集，用于评估大型音频语言模型（LALM）在推理相关任务上的能力，并通过分析发现这些模型在TREA数据集上的表现低于人类，同时提出不确定性指标，强调了对LALM进行全面评估的必要性。&lt;h4&gt;背景&lt;/h4&gt;文本大型语言模型（LLM）的成功引起了多模态社区的关注，他们希望将文本与其他模态如视觉和音频结合以实现类似的多模态能力。&lt;h4&gt;目的&lt;/h4&gt;评估大型音频语言模型（LALM）在推理相关任务上的表现，并研究它们相对于人类的能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的数据集TREA，用于评估LALM，并提出了一个不确定性指标来衡量模型对输入语义相同扰动的不变性。&lt;h4&gt;主要发现&lt;/h4&gt;LALM在TREA数据集上的表现持续低于人类能力，且准确性和不确定性指标之间没有必然的相关性。&lt;h4&gt;结论&lt;/h4&gt;为了高价值应用，需要全面评估LALM的准确性以及不确定性。&lt;h4&gt;翻译&lt;/h4&gt;The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA). We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The popular success of text-based large language models (LLM) has streamlinedthe attention of the multimodal community to combine other modalities likevision and audio along with text to achieve similar multimodal capabilities. Inthis quest, large audio language models (LALMs) have to be evaluated onreasoning related tasks which are different from traditional classification orgeneration tasks. Towards this goal, we propose a novel dataset called temporalreasoning evaluation of audio (TREA).  We benchmark open-source LALMs and observe that they are consistently behindhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, wealso propose an uncertainty metric, which computes the invariance of the modelto semantically identical perturbations of the input. Our analysis shows thatthe accuracy and uncertainty metrics are not necessarily correlated and thus,points to a need for wholesome evaluation of LALMs for high-stakesapplications.</description>
      <author>example@mail.com (Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy)</author>
      <guid isPermaLink="false">2505.13115v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Universal Semantic Disentangled Privacy-preserving Speech Representation Learning</title>
      <link>http://arxiv.org/abs/2505.13085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过通用语音编解码器（USC）实现说话人隐私保护的表示学习方法，以解决使用人类语音录音训练大型语言模型（LLM）带来的隐私问题。&lt;h4&gt;背景&lt;/h4&gt;使用人类语音录音训练LLM可能引起隐私问题，因为模型可能生成与训练数据中工件相似的输出。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来保护说话人隐私，同时在学习表示中保留语义内容和语音旁白信息。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通过USC进行语音解耦的方法，将语音分解为隐私保护的语义丰富表示和残差声学和说话人表示。&lt;h4&gt;主要发现&lt;/h4&gt;USC的语义表示保留了内容、韵律和情感，同时去除了可能可识别的说话人属性。USC在语音重建方面达到了最先进的水平。&lt;h4&gt;结论&lt;/h4&gt;USC在隐私保护表示学习方面有效，展示了在学习的语义表示中说话人匿名化、旁白保留和内容保护之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;This study proposes a speaker privacy-preserving representation learning method through the Universal Speech Codec (USC), an efficient encoder-decoder model that disentangles speech into: (i) privacy-preserving semantically rich representations, capturing content and speech paralinguistics, and (ii) residual acoustic and speaker representations that enable high-fidelity reconstruction. Extensive evaluations show that USC's semantic representation preserves content, prosody, and sentiment, while removing potentially identifiable speaker attributes. Combining both representations, USC achieves state-of-the-art speech reconstruction. Additionally, an evaluation methodology for measuring privacy-preserving properties is introduced, aligning with perceptual tests. USC is compared against other codecs in the literature and its effectiveness on privacy-preserving representation learning is demonstrated, illustrating the trade-offs of speaker anonymization, paralinguistics retention and content preservation in the learned semantic representations. Audio samples are shared at https://www.amazon.science/usc-samples.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of audio recordings of human speech to train LLMs poses privacyconcerns due to these models' potential to generate outputs that closelyresemble artifacts in the training data. In this study, we propose a speakerprivacy-preserving representation learning method through the Universal SpeechCodec (USC), a computationally efficient encoder-decoder model thatdisentangles speech into: $\textit{(i)}$ privacy-preserving semantically richrepresentations, capturing content and speech paralinguistics, and$\textit{(ii)}$ residual acoustic and speaker representations that enableshigh-fidelity reconstruction. Extensive evaluations presented show that USC'ssemantic representation preserves content, prosody, and sentiment, whileremoving potentially identifiable speaker attributes. Combining bothrepresentations, USC achieves state-of-the-art speech reconstruction.Additionally, we introduce an evaluation methodology for measuringprivacy-preserving properties, aligning with perceptual tests. We compare USCagainst other codecs in the literature and demonstrate its effectiveness onprivacy-preserving representation learning, illustrating the trade-offs ofspeaker anonymization, paralinguistics retention and content preservation inthe learned semantic representations. Audio samples are shared in$\href{https://www.amazon.science/usc-samples}{https://www.amazon.science/usc-samples}$.</description>
      <author>example@mail.com (Biel Tura Vecino, Subhadeep Maji, Aravind Varier, Antonio Bonafonte, Ivan Valles, Michael Owen, Leif Radel, Grant Strimmel, Seyi Feyisetan, Roberto Barra Chicote, Ariya Rastrow, Constantinos Papayiannis, Volker Leutnant, Trevor Wood)</author>
      <guid isPermaLink="false">2505.13085v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics</title>
      <link>http://arxiv.org/abs/2505.13192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DynaMix，一种用于动态系统重建（DSR）的新颖的多变量ALRNN混合专家架构，该架构经过预训练，能够实现零样本泛化到领域外的动态系统。&lt;h4&gt;背景&lt;/h4&gt;动态系统（DS）在复杂、随时间演变的领域中起重要作用。现有的DSR方法需要针对每个新观察到的系统进行专门训练，缺乏类似大型语言模型（LLMs）的零样本和上下文推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出DynaMix的目的是为了实现DSR的零样本泛化，即无需重新训练即可预测新动态系统的长期演化。&lt;h4&gt;方法&lt;/h4&gt;DynaMix是一种基于多变量自适应学习循环神经网络（ALRNN）的混合专家架构，经过预训练以实现DSR。&lt;h4&gt;主要发现&lt;/h4&gt;DynaMix能够从提供的上下文信号中准确预测新动态系统的长期演化，即使在现有时间序列（TS）基础模型如Chronos失败的领域，也能以更少的参数和更快的推理速度完成。DynaMix在长期统计上优于TS基础模型，甚至在短期预测中也表现良好，即使在现实世界的时间序列数据（如交通或天气数据）上也是如此，这些数据通常用于训练和评估TS模型，但并非DynaMix的训练语料库的一部分。&lt;h4&gt;结论&lt;/h4&gt;DynaMix展示了时间序列模型在DSR问题中的失败模式，并得出结论，基于DS原理构建的模型在推进时间序列预测领域也具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：复杂、随时间演变的系统，从气候到大脑活动，都受动态系统（DS）的调控。动态系统重建（DSR）旨在从观察数据中推断生成代用模型，以再现其长期行为。现有的DSR方法需要对每个新观察到的系统进行专门训练，缺乏类似于大型语言模型（LLMs）所知的零样本和上下文推理能力。在此，我们引入了DynaMix，这是一种新颖的多变量ALRNN混合专家架构，专门为DSR预训练，是第一个能够将零样本泛化到领域外动态系统的DSR模型。仅从提供的上下文信号中，无需任何重新训练，DynaMix能够准确预测新动态系统的长期演化，即使现有时间序列（TS）基础模型（如Chronos）在这些领域失败——在参数数量和推理速度上仅占其一小部分。在长期统计上，DynaMix优于TS基础模型，在短期预测中也常常表现良好，即使在现实世界的时间序列数据（如交通或天气数据）上也是如此，这些数据通常用于训练和评估TS模型，但并不属于DynaMix的训练语料库。我们展示了时间序列模型在DSR问题中的失败模式，并得出结论，基于DS原理构建的模型在推进时间序列预测领域也具有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex, temporally evolving phenomena, from climate to brain activity, aregoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infergenerative surrogate models of these from observed data, reproducing theirlong-term behavior. Existing DSR approaches require purpose-training for anynew system observed, lacking the zero-shot and in-context inferencecapabilities known from LLMs. Here we introduce DynaMix, a novel multivariateALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSRmodel able to generalize zero-shot to out-of-domain DS. Just from a providedcontext signal, without any re-training, DynaMix faithfully forecasts thelong-term evolution of novel DS where existing time series (TS) foundationmodels, like Chronos, fail -- at a fraction of the number of parameters andorders of magnitude faster inference times. DynaMix outperforms TS foundationmodels in terms of long-term statistics, and often also short-term forecasts,even on real-world time series, like traffic or weather data, typically usedfor training and evaluating TS models, but not at all part of DynaMix' trainingcorpus. We illustrate some of the failure modes of TS models for DSR problems,and conclude that models built on DS principles may bear a huge potential alsofor advancing the TS prediction field.</description>
      <author>example@mail.com (Christoph Jürgen Hemmer, Daniel Durstewitz)</author>
      <guid isPermaLink="false">2505.13192v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform</title>
      <link>http://arxiv.org/abs/2505.12631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HaarMoDic的网络，用于预测3D人体姿态，通过使用2D Haar变换将关节投影到更高分辨率的坐标，以便网络同时获取空间和时间信息。&lt;h4&gt;背景&lt;/h4&gt;3D人体姿态预测在计算机视觉和计算机图形学中至关重要，近年来引起了广泛关注。然而，现有方法由于忽略了人类运动序列在时间和空间轴上的任意性，导致在复杂情况下的预测精度受限。&lt;h4&gt;目的&lt;/h4&gt;提出HaarMoDic网络，以改善3D人体姿态预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;HaarMoDic网络利用2D Haar变换将关节投影到更高分辨率的坐标，同时获取空间和时间信息。网络中的关键模块是Multi-Resolution Haar (MR-Haar)块，它将整个运动序列投影到一个混合坐标，以便在更高分辨率的不同分辨率下同时利用两个轴的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HaarMoDic网络在Human3.6M数据集上的平均每关节位置误差（MPJPE）指标上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;HaarMoDic网络通过引入MR-Haar块，提高了3D人体姿态预测的准确性，为复杂情况下的精确预测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The 3D human pose is vital for modern computer vision and computer graphics, and its prediction has drawn attention in recent years. 3D human pose prediction aims at forecasting a human's future motion from the previous sequence. Ignoring that the arbitrariness of human motion sequences has a firm origin in transition in both temporal and spatial axes limits the performance of state-of-the-art methods, leading them to struggle with making precise predictions on complex cases, e.g., arbitrarily posing or greeting. To alleviate this problem, a network called HaarMoDic is proposed in this paper, which utilizes the 2D Haar transform to project joints to higher resolution coordinates where the network can access spatial and temporal information simultaneously. An ablation study proves that the significant contributing module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar) block. Instead of mining in one of two axes or extracting separately, the MR-Haar block projects whole motion sequences to a mixed-up coordinate in higher resolution with 2D Haar Transform, allowing the network to give scope to information from both axes in different resolutions. With the MR-Haar block, the HaarMoDic network can make predictions referring to a broader range of information. Experimental results demonstrate that HaarMoDic surpasses state-of-the-art methods in every testing interval on the Human3.6M dataset in the Mean Per Joint Position Error (MPJPE) metric.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/xhaughearl/haarmodic&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D human pose is vital for modern computer vision and computer graphics,and its prediction has drawn attention in recent years. 3D human poseprediction aims at forecasting a human's future motion from the previoussequence. Ignoring that the arbitrariness of human motion sequences has a firmorigin in transition in both temporal and spatial axes limits the performanceof state-of-the-art methods, leading them to struggle with making precisepredictions on complex cases, e.g., arbitrarily posing or greeting. Toalleviate this problem, a network called HaarMoDic is proposed in this paper,which utilizes the 2D Haar transform to project joints to higher resolutioncoordinates where the network can access spatial and temporal informationsimultaneously. An ablation study proves that the significant contributingmodule within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)block. Instead of mining in one of two axes or extracting separately, theMR-Haar block projects whole motion sequences to a mixed-up coordinate inhigher resolution with 2D Haar Transform, allowing the network to give scope toinformation from both axes in different resolutions. With the MR-Haar block,the HaarMoDic network can make predictions referring to a broader range ofinformation. Experimental results demonstrate that HaarMoDic surpassesstate-of-the-art methods in every testing interval on the Human3.6M dataset inthe Mean Per Joint Position Error (MPJPE) metric.</description>
      <author>example@mail.com (Li Lin)</author>
      <guid isPermaLink="false">2505.12631v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection</title>
      <link>http://arxiv.org/abs/2505.12966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,ICMR accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MACB-DF的音频-视觉联合学习方法，旨在解决多模态检测方法中模态学习不平衡的问题，通过对比学习实现多级和跨模态融合，以充分利用每个模态的信息。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉和深度学习的发展，深度伪造与真实媒体之间的界限变得模糊，通过音频-视觉伪造手段破坏了多媒体的可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来更好地解决模态冲突和忽视问题，提高多媒体检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种正交化的多模态Pareto模块，以保留单模态信息并解决音频-视频编码器中的梯度冲突，这些冲突是由损失函数的不同优化目标引起的。&lt;h4&gt;主要发现&lt;/h4&gt;在主流深度伪造数据集上进行的广泛实验和消融研究表明，该模型在关键评估指标上实现了持续的性能提升，多个数据集的平均准确率达到95.5%。该方法在跨数据集泛化能力方面表现出色，在DFDC数据集上训练并在DefakeAVMiT和FakeAVCeleb数据集上测试时，ACC分数相较于先前最佳方法分别提高了8.0%和7.7%。&lt;h4&gt;结论&lt;/h4&gt;MACB-DF方法在深度伪造检测方面表现出显著的效果，特别是在跨数据集泛化能力上具有优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着计算机视觉和深度学习的发展，深度伪造与真实媒体之间的界限变得模糊，通过音频-视觉伪造手段破坏了多媒体的可靠性。当前的多模态检测方法仍然受到模态学习不平衡的限制。为了解决这个问题，我们提出了一种音频-视觉联合学习方法（MACB-DF），通过利用对比学习来辅助多级和跨模态融合，从而更好地缓解模态冲突和忽视，充分利用每个模态的信息。此外，我们设计了一个正交化的多模态Pareto模块，在保留单模态信息的同时，解决了音频-视频编码器中由于损失函数的不同优化目标而引起的梯度冲突。在主流深度伪造数据集上进行的广泛实验和消融研究表明，我们的模型在关键评估指标上实现了持续的性能提升，多个数据集的平均准确率达到95.5%。值得注意的是，我们的方法在跨数据集泛化能力方面表现出色，在DFDC数据集上训练并在DefakeAVMiT和FakeAVCeleb数据集上测试时，ACC分数相较于先前最佳方法分别提高了8.0%和7.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computer vision and deep learning have blurred the line betweendeepfakes and authentic media, undermining multimedia credibility throughaudio-visual forgery. Current multimodal detection methods remain limited byunbalanced learning between modalities. To tackle this issue, we propose anAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modalityconflicts and neglect by leveraging contrastive learning to assist inmulti-level and cross-modal fusion, thereby fully balancing and exploitinginformation from each modality. Additionally, we designed anorthogonalization-multimodal pareto module that preserves unimodal informationwhile addressing gradient conflicts in audio-video encoders caused by differingoptimization targets of the loss functions. Extensive experiments and ablationstudies conducted on mainstream deepfake datasets demonstrate consistentperformance gains of our model across key evaluation metrics, achieving anaverage accuracy of 95.5% across multiple datasets. Notably, our methodexhibits superior cross-dataset generalization capabilities, with absoluteimprovements of 8.0% and 7.7% in ACC scores over the previous best-performingapproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCelebdatasets.</description>
      <author>example@mail.com (Zihan Xiong, Xiaohua Wu, Lei Chen, Fangqi Lou)</author>
      <guid isPermaLink="false">2505.12966v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos</title>
      <link>http://arxiv.org/abs/2505.12911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page https://github.com/sapeirone/hiero&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HiERO的弱监督方法，用于丰富视频片段特征，通过将视频片段与其叙述描述对齐，推断出上下文、语义和时间的层次化推理。&lt;h4&gt;背景&lt;/h4&gt;人类活动复杂多变，这给深度学习模型理解它们带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;利用人类活动内在的层次化模式结构，提升对人类活动视频内容的理解。&lt;h4&gt;方法&lt;/h4&gt;HiERO通过视频片段与叙述描述的对齐，使用层次化架构进行上下文、语义和时间的推理。&lt;h4&gt;主要发现&lt;/h4&gt;HiERO在多个视频文本对齐基准测试（EgoMCQ、EgoNLQ）中证明了其丰富特征的潜力，并在零样本学习任务（EgoProceL和Ego4D Goal-Step）中取得了最先进的性能，其性能在零样本情况下比全监督方法提高了12.5%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;利用人类活动层次化结构的知识对于执行多个推理任务和以自我为中心的视觉中的推理任务具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人类活动复杂多变，这使得深度学习模型难以理解。然而，我们注意到这种可变性具有一个内在的结构，由一系列相关动作的模式组成。我们认为这种结构可以自然地从人类活动的非脚本视频中产生，并且可以利用它来更好地理解其内容。我们提出了HiERO，这是一种弱监督方法，用于丰富视频片段特征与相应的层次化活动线程。通过将视频剪辑与它们的叙述描述对齐，HiERO使用层次化架构进行上下文、语义和时间的推理。我们通过多个视频文本对齐基准（EgoMCQ、EgoNLQ）以及最小额外训练，证明了我们丰富特征的潜力，并在零样本学习任务（EgoProceL和Ego4D Goal-Step）中实现了最先进的性能。值得注意的是，HiERO在所有基准测试中都取得了最先进的性能，在零样本学习任务中，它比全监督方法有大幅度的提升（在EgoProceL上提高了12.5%的F1分数）。我们的结果表明，使用人类活动层次化结构的知识对于执行多个推理任务和以自我为中心的视觉中的推理任务具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sapeirone/hiero&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human activities are particularly complex and variable, and this makeschallenging for deep learning models to reason about them. However, we notethat such variability does have an underlying structure, composed of ahierarchy of patterns of related actions. We argue that such structure canemerge naturally from unscripted videos of human activities, and can beleveraged to better reason about their content. We present HiERO, aweakly-supervised method to enrich video segments features with thecorresponding hierarchical activity threads. By aligning video clips with theirnarrated descriptions, HiERO infers contextual, semantic and temporal reasoningwith an hierarchical architecture. We prove the potential of our enrichedfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) withminimal additional training, and in zero-shot for procedure learning tasks(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-artperformance in all the benchmarks, and for procedure learning tasks itoutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)in zero shot. Our results prove the relevance of using knowledge of thehierarchy of human activities for multiple reasoning tasks in egocentricvision.</description>
      <author>example@mail.com (Simone Alberto Peirone, Francesca Pistilli, Giuseppe Averta)</author>
      <guid isPermaLink="false">2505.12911v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions</title>
      <link>http://arxiv.org/abs/2505.12327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Conference on Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种针对自动驾驶的鲁棒规划方法，该方法结合了由扩散模型训练得到的正常和对抗性代理预测。&lt;h4&gt;背景&lt;/h4&gt;目前的自动驾驶规划方法可能过度重视对抗性行为，而忽略了低成本正常行为，或者使用硬性安全约束，这可能在所有驾驶场景中都不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种既能够抵御对抗性行为，又不过度保守的自动驾驶规划方法。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个扩散模型来学习正常代理行为的无偏分布。然后在测试时通过偏差扩散模型生成可能导致碰撞的候选计划的预测，从而得到对抗性预测的分布。使用正常和对抗性预测的混合分布来评估计划的预期成本。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在单一代理和多代理的闯红灯场景以及违反交通信号灯的场景中均显示出有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法在避免过度重视对抗性行为的同时，也考虑了正常行为的成本，提供了一种更加鲁棒的自动驾驶规划方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We describe a robust planning method for autonomous driving that mixes normaland adversarial agent predictions output by a diffusion model trained formotion prediction. We first train a diffusion model to learn an unbiaseddistribution of normal agent behaviors. We then generate a distribution ofadversarial predictions by biasing the diffusion model at test time to generatepredictions that are likely to collide with a candidate plan. We score plansusing expected cost with respect to a mixture distribution of normal andadversarial predictions, leading to a planner that is robust againstadversarial behaviors but not overly conservative when agents behave normally.Unlike current approaches, we do not use risk measures that over-weightadversarial behaviors while placing little to no weight on low-cost normalbehaviors or use hard safety constraints that may not be appropriate for alldriving scenarios. We show the effectiveness of our method on single-agent andmulti-agent jaywalking scenarios as well as a red light violation scenario.</description>
      <author>example@mail.com (Albert Zhao, Stefano Soatto)</author>
      <guid isPermaLink="false">2505.12327v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
      <link>http://arxiv.org/abs/2505.12891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First version. There are still some examples to be added into the  appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TIME的多层次基准，旨在解决大型语言模型在现实世界场景中进行时间推理的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的时间推理研究忽略了现实世界中的挑战，包括密集的时间信息、快速变化的事件动态和复杂的社会互动中的时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;通过提出TIME基准，旨在解决上述现实世界挑战，并促进时间推理在真实场景中的应用。&lt;h4&gt;方法&lt;/h4&gt;TIME基准包含38,522个问答对，涵盖3个层级和11个细粒度子任务。它包括3个子数据集：TIME-Wiki、TIME-News和TIME-Dial，分别反映不同的现实世界挑战。&lt;h4&gt;主要发现&lt;/h4&gt;进行了广泛的实验，分析了不同真实世界场景和任务中的时间推理性能，并总结了测试时间缩放对时间推理能力的影响。&lt;h4&gt;结论&lt;/h4&gt;TIME-Lite，一个人工标注的子集，被发布以促进未来研究和时间推理的标准化评估。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间推理对于大型语言模型（LLMs）理解现实世界至关重要。然而，现有工作忽略了时间推理的现实世界挑战：（1）密集的时间信息，（2）快速变化的事件动态，（3）社会互动中的复杂时间依赖关系。为了弥合这一差距，我们提出了一种名为TIME的多级基准，专为现实世界场景中的时间推理设计。TIME由38,522个问答对组成，包含3个层级和11个细粒度子任务。该基准包含3个子数据集，分别反映不同的现实世界挑战：TIME-Wiki、TIME-News和TIME-Dial。我们进行了广泛的推理模型和非推理模型的实验。我们还对跨不同真实世界场景和任务的时间推理性能进行了深入分析，并总结了测试时间缩放对时间推理能力的影响。此外，我们还发布了TIME-Lite，一个人工标注的子集，以促进未来研究和时间推理的标准化评估。代码可在https://github.com/sylvain-wei/TIME找到，数据集可在https://huggingface.co/datasets/SylvainWei/TIME找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehendthe real world. However, existing works neglect the real-world challenges fortemporal reasoning: (1) intensive temporal information, (2) fast-changing eventdynamics, and (3) complex temporal dependencies in social interactions. Tobridge this gap, we propose a multi-level benchmark TIME, designed for temporalreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3levels with 11 fine-grained sub-tasks. This benchmark encompasses 3sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,and TIME-Dial. We conduct extensive experiments on reasoning models andnon-reasoning models. And we conducted an in-depth analysis of temporalreasoning performance across diverse real-world scenarios and tasks, andsummarized the impact of test-time scaling on temporal reasoning capabilities.Additionally, we release TIME-Lite, a human-annotated subset to foster futureresearch and standardized evaluation in temporal reasoning. The code isavailable at https://github.com/sylvain-wei/TIME , and the dataset is availableat https://huggingface.co/datasets/SylvainWei/TIME .</description>
      <author>example@mail.com (Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang)</author>
      <guid isPermaLink="false">2505.12891v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data</title>
      <link>http://arxiv.org/abs/2505.12626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;scRNA-seq技术揭示了细胞异质性，细胞聚类在识别细胞类型和标记基因中发挥关键作用。本文提出了一种名为scSiameseClu的新型Siamese聚类框架，用于解释scRNA-seq数据，该框架通过三个关键步骤提高聚类性能。&lt;h4&gt;背景&lt;/h4&gt;scRNA-seq数据分析面临噪声、稀疏性和高维度的挑战，而基于图神经网络（GNN）的方法虽然提高了聚类性能，但往往存在过平滑问题，限制了其捕捉复杂生物信息的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的scSiameseClu框架，用于提高scRNA-seq数据的聚类性能，并更好地解释细胞类型和标记基因。&lt;h4&gt;方法&lt;/h4&gt;scSiameseClu框架包括三个关键步骤：(1) 双重增强模块，通过生物信息学驱动的扰动增强表示的鲁棒性；(2) Siamese融合模块，结合交叉相关优化和自适应信息融合以捕捉复杂的细胞关系，同时减轻过平滑；(3) 最优传输聚类，利用Sinkhorn距离高效地调整聚类分配与预定义比例，同时保持平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界数据集上的全面评估表明，scSiameseClu在单细胞聚类、细胞类型注释和细胞类型分类方面优于现有方法，为scRNA-seq数据解释提供了一种强大的工具。&lt;h4&gt;结论&lt;/h4&gt;scSiameseClu是一种有效的单细胞RNA测序数据分析工具，能够显著提高细胞聚类和细胞类型识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cellclustering playing a key role in identifying cell types and marker genes.Recent advances, especially graph neural networks (GNNs)-based methods, havesignificantly improved clustering performance. However, the analysis ofscRNA-seq data remains challenging due to noise, sparsity, and highdimensionality. Compounding these challenges, GNNs often suffer fromover-smoothing, limiting their ability to capture complex biologicalinformation. In response, we propose scSiameseClu, a novel Siamese Clusteringframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:(1) Dual Augmentation Module, which applies biologically informed perturbationsto the gene expression matrix and cell graph relationships to enhancerepresentation robustness; (2) Siamese Fusion Module, which combinescross-correlation refinement and adaptive information fusion to capture complexcellular relationships while mitigating over-smoothing; and (3) OptimalTransport Clustering, which utilizes Sinkhorn distance to efficiently aligncluster assignments with predefined proportions while maintaining balance.Comprehensive evaluations on seven real-world datasets demonstratethat~\methodname~outperforms state-of-the-art methods in single-cellclustering, cell type annotation, and cell type classification, providing apowerful tool for scRNA-seq data interpretation.</description>
      <author>example@mail.com (Ping Xu, Zhiyuan Ning, Pengjiang Li, Wenhao Liu, Pengyang Wang, Jiaxu Cui, Yuanchun Zhou, Pengfei Wang)</author>
      <guid isPermaLink="false">2505.12626v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models</title>
      <link>http://arxiv.org/abs/2505.12589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The dataset and code are publicly available at:  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视频监控内容理解是视觉-语言研究中的一个关键但尚未充分探索的挑战，该研究引入了SurveillanceVQA-589K，这是一个针对监控领域的最大开放性问题回答基准。&lt;h4&gt;背景&lt;/h4&gt;监控视频内容理解因其实际世界的复杂性、不规律的事件动态和安全性关键意义而成为视觉-语言研究中的一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个针对监控领域的开放性问题回答基准，以促进视频-语言理解在安全关键应用中的发展。&lt;h4&gt;方法&lt;/h4&gt;构建了包含589,380个问答对的基准数据集，包含12种认知多样的问题类型，设计了一个混合标注流程，结合了人类编写的字幕和基于提示技术的Large Vision-Language Model辅助问答生成，并提出了一种多维评估协议来评估上下文、时间和因果理解。&lt;h4&gt;主要发现&lt;/h4&gt;评估了八种Large Vision-Language Model，发现显著的性能差距，特别是在因果和异常相关任务上，这突显了当前模型在现实世界监控环境中的局限性。&lt;h4&gt;结论&lt;/h4&gt;该基准提供了一个实际且全面的资源，用于推进视频-语言理解在智能监控、事件分析和自主决策等安全关键应用中的发展。&lt;h4&gt;翻译&lt;/h4&gt;Understanding surveillance video content remains a critical yet underexplored challenge in vision-language research, particularly due to its real-world complexity, irregular event dynamics, and safety-critical implications. In this work, we introduce SurveillanceVQA-589K, the largest open-ended video question answering benchmark tailored to the surveillance domain. The dataset comprises 589,380 QA pairs spanning 12 cognitively diverse question types, including temporal reasoning, causal inference, spatial understanding, and anomaly interpretation, across both normal and abnormal video scenarios. To construct the benchmark at scale, we design a hybrid annotation pipeline that combines temporally aligned human-written captions with Large Vision-Language Model-assisted QA generation using prompt-based techniques. We also propose a multi-dimensional evaluation protocol to assess contextual, temporal, and causal comprehension. We evaluate eight LVLMs under this framework, revealing significant performance gaps, especially in causal and anomaly-related tasks, underscoring the limitations of current models in real-world surveillance contexts. Our benchmark provides a practical and comprehensive resource for advancing video-language understanding in safety-critical applications such as intelligent monitoring, incident analysis, and autonomous decision-making.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding surveillance video content remains a critical yet underexploredchallenge in vision-language research, particularly due to its real-worldcomplexity, irregular event dynamics, and safety-critical implications. In thiswork, we introduce SurveillanceVQA-589K, the largest open-ended video questionanswering benchmark tailored to the surveillance domain. The dataset comprises589,380 QA pairs spanning 12 cognitively diverse question types, includingtemporal reasoning, causal inference, spatial understanding, and anomalyinterpretation, across both normal and abnormal video scenarios. To constructthe benchmark at scale, we design a hybrid annotation pipeline that combinestemporally aligned human-written captions with Large Vision-LanguageModel-assisted QA generation using prompt-based techniques. We also propose amulti-dimensional evaluation protocol to assess contextual, temporal, andcausal comprehension. We evaluate eight LVLMs under this framework, revealingsignificant performance gaps, especially in causal and anomaly-related tasks,underscoring the limitations of current models in real-world surveillancecontexts. Our benchmark provides a practical and comprehensive resource foradvancing video-language understanding in safety-critical applications such asintelligent monitoring, incident analysis, and autonomous decision-making.</description>
      <author>example@mail.com (Bo Liu, Pengfei Qiao, Minhan Ma, Xuange Zhang, Yinan Tang, Peng Xu, Kun Liu, Tongtong Yuan)</author>
      <guid isPermaLink="false">2505.12589v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio Channels</title>
      <link>http://arxiv.org/abs/2505.13055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpaRTran是一种基于压缩感知原理的无监督表示学习方法，用于无线电信道。&lt;h4&gt;背景&lt;/h4&gt;无线电传播的物理特性是研究重点。&lt;h4&gt;目的&lt;/h4&gt;学习嵌入表示，为无线电下游任务提供优化基础。&lt;h4&gt;方法&lt;/h4&gt;SpaRTran使用稀疏门控自编码器，并学习包含原子特征的字典，以增强信号波形和时空信号模式的变化。&lt;h4&gt;主要发现&lt;/h4&gt;SpaRTran在无线电指纹识别等下游任务上，与现有方法相比，误差减少了85%。&lt;h4&gt;结论&lt;/h4&gt;SpaRTran需要更少的预训练工作量，提供更大的灵活性，并且作为基础模型，可以针对各种无线电下游任务进行微调，有效降低标注成本，同时比现有方法更具通用性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了稀疏预训练无线电变换器（SpaRTran），这是一种基于压缩感知原理的无监督表示学习方法，用于无线电信道。我们的方法学习嵌入表示，专注于无线电传播的物理特性，为基于无线电的下游任务提供优化基础。SpaRTran使用一个稀疏门控自编码器，对学习到的表示引入了简单性偏差，类似于无线电传播的稀疏性。对于信号重建，它学习一个包含原子特征的字典，这增加了信号波形和时空信号模式的变化。我们的实验表明，当在无线电指纹识别等具有挑战性的下游任务上进行微调时，SpaRTran将误差减少了高达85%，与最先进的方法相比。此外，我们的方法需要更少的预训练工作量，并提供了更大的灵活性，因为我们仅在单个无线电信号上对其进行训练。SpaRTran是一个出色的基础模型，可以针对各种无线电下游任务进行微调，有效降低标注成本。此外，它比现有方法更具通用性，并显示出优越的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the Sparse pretrained Radio Transformer (SpaRTran), anunsupervised representation learning approach based on the concept ofcompressed sensing for radio channels. Our approach learns embeddings thatfocus on the physical properties of radio propagation, to create the optimalbasis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparsegated autoencoder that induces a simplicity bias to the learnedrepresentations, resembling the sparse nature of radio propagation. For signalreconstruction, it learns a dictionary that holds atomic features, whichincreases flexibility across signal waveforms and spatiotemporal signalpatterns. Our experiments show that SpaRTran reduces errors by up to 85 %compared to state-of-the-art methods when fine-tuned on radio fingerprinting, achallenging downstream task. In addition, our method requires less pretrainingeffort and offers greater flexibility, as we train it solely on individualradio signals. SpaRTran serves as an excellent base model that can befine-tuned for various radio-based downstream tasks, effectively reducing thecost for labeling. In addition, it is significantly more versatile thanexisting methods and demonstrates superior generalization.</description>
      <author>example@mail.com (Jonathan Ott, Maximilian Stahlke, Tobias Feigl, Bjoern M. Eskofier, Christopher Mutschler)</author>
      <guid isPermaLink="false">2505.13055v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</title>
      <link>http://arxiv.org/abs/2505.12753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于LiDAR点云的多目标跟踪方法，通过改进的DETR模型和transformer架构，解决了传统跟踪系统在拥挤或快速移动场景中难以保持对象身份一致的问题。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云数据具有稀疏和不规则的特点，且需要跨帧保持时间一致性，这对多目标跟踪提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提高多目标跟踪在复杂场景中的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种两阶段的DETR inspired transformer模型，第一阶段为smoother阶段，用于优化LiDAR对象检测；第二阶段为tracker阶段，使用基于DETR的注意力机制进行跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在nuScenes和KITTI数据集上，无论是在线还是离线模式，都表现出优异的性能，在线模式在nuScenes数据集上优于基线模型和SOTA模型，离线模式提供了额外的3 pp aMOTP。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在多目标跟踪任务中表现出色，为解决复杂场景下的跟踪问题提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-object tracking from LiDAR point clouds presents unique challenges dueto the sparse and irregular nature of the data, compounded by the need fortemporal coherence across frames. Traditional tracking systems often rely onhand-crafted features and motion models, which can struggle to maintainconsistent object identities in crowded or fast-moving scenes. We present alidar-based two-staged DETR inspired transformer; a smoother and tracker. Thesmoother stage refines lidar object detections, from any off-the-shelfdetector, across a moving temporal window. The tracker stage uses a DETR-basedattention block to maintain tracks across time by associating tracked objectswith the refined detections using the point cloud as context. The model istrained on the datasets nuScenes and KITTI in both online and offline (forwardpeeking) modes demonstrating strong performance across metrics such asID-switch and multiple object tracking accuracy (MOTA). The numerical resultsindicate that the online mode outperforms the lidar-only baseline and SOTAmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,while the offline mode provides an additional 3 pp aMOTP</description>
      <author>example@mail.com (Martha Teiko Teye, Ori Maoz, Matthias Rottmann)</author>
      <guid isPermaLink="false">2505.12753v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics</title>
      <link>http://arxiv.org/abs/2505.13150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了行为基础模型（BFMs）在零样本方式下生成策略的效率，并提出了改进的FB模型以应对动态变化。&lt;h4&gt;背景&lt;/h4&gt;BFMs在零样本方式下生成策略方面取得了成功，但传统方法在动态变化时效率低下，限制了其在实际应用中的适用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的FB模型，以增强BFMs在动态变化环境下的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于transformer的信念估计器的FB模型，并通过对策略编码空间进行动态特定聚类来提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;改进的FB模型能够区分不同的动态，并通过聚类策略编码空间来提升性能，从而应对训练过程中的动态变化并泛化到未见过的动态。&lt;h4&gt;结论&lt;/h4&gt;在动态变化的场景中，该方法在离散和连续任务上相比基线实现了高达2倍的零样本回报。&lt;h4&gt;翻译&lt;/h4&gt;Behavioral Foundation Models (BFMs) have proven successful in producing policies for arbitrary tasks in a zero-shot manner, requiring no test-time training or task-specific fine-tuning. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward-Backward (FB) representation, one of the methods from the BFM family, cannot distinguish between distinct dynamics, leading to an interference among the latent directions, which parametrize different policies. To address this, we propose a FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. We also show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. These traits allow our method to respond to the dynamics observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavioral Foundation Models (BFMs) proved successful in producing policiesfor arbitrary tasks in a zero-shot manner, requiring no test-time training ortask-specific fine-tuning. Among the most promising BFMs are the ones thatestimate the successor measure learned in an unsupervised way fromtask-agnostic offline data. However, these methods fail to react to changes inthe dynamics, making them inefficient under partial observability or when thetransition function changes. This hinders the applicability of BFMs in areal-world setting, e.g., in robotics, where the dynamics can unexpectedlychange at test time. In this work, we demonstrate that Forward-Backward (FB)representation, one of the methods from the BFM family, cannot distinguishbetween distinct dynamics, leading to an interference among the latentdirections, which parametrize different policies. To address this, we propose aFB model with a transformer-based belief estimator, which greatly facilitateszero-shot adaptation. We also show that partitioning the policy encoding spaceinto dynamics-specific clusters, aligned with the context-embedding directions,yields additional gain in performance. These traits allow our method to respondto the dynamics observed during training and to generalize to unseen ones.Empirically, in the changing dynamics setting, our approach achieves up to a 2xhigher zero-shot returns compared to the baselines for both discrete andcontinuous tasks.</description>
      <author>example@mail.com (Maksim Bobrin, Ilya Zisman, Alexander Nikulin, Vladislav Kurenkov, Dmitry Dylov)</author>
      <guid isPermaLink="false">2505.13150v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.12904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过实施无监督对比学习方法，探索了在海洋环境中通过被动监听水下噪音来监测和识别声源，以减轻声污染对海洋健康威胁的可能性。&lt;h4&gt;背景&lt;/h4&gt;海洋环境中的声污染水平不断上升，对海洋健康构成威胁，因此监测水下噪音变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种无监督学习方法，以自动对水下声源进行分类，特别是在缺乏高质量标注数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种基于Conformer编码器的无监督对比学习方法，使用所谓的方差-不变-协方差正则化损失函数对低质量的未标注数据进行优化，并将结果应用于标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在识别船型和海洋哺乳动物叫声的分类任务中显示出稳健和泛化的嵌入能力。&lt;h4&gt;结论&lt;/h4&gt;这项研究表明，无监督方法在自动水下声学分析任务中具有潜力，尤其是在利用大量可用但质量较低的未标注数据时。&lt;h4&gt;翻译&lt;/h4&gt;随着海洋环境中噪音污染水平的不断上升，对海洋健康的威胁也在增加，因此监测水下噪音变得至关重要。通过被动监听这些声音，可以定位造成这种污染的源头。监测过程产生大量数据记录，记录了包括船只活动和海洋哺乳动物叫声在内的多种声源。尽管机器学习为自动声音分类提供了一种有希望的方法，但当前最先进的方法实施了监督学习，这需要大量高质量标注数据，而这些数据并未公开可用。相反，大量低质量的未标注数据是公开可用的，这为探索无监督学习技术提供了机会。本研究通过实施无监督对比学习方法来探索这种可能性。在这里，通过所谓的方差-不变-协方差正则化损失函数对这些低质量未标注数据进行优化，并实现了向标注数据的转换。通过涉及识别船型和海洋哺乳动物叫声的分类任务，我们的方法证明了产生稳健和泛化嵌入的能力。这表明无监督方法在各种自动水下声学分析任务中具有潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hildeingvildhummel/uatr&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing level of sound pollution in marine environments poses anincreased threat to ocean health, making it crucial to monitor underwaternoise. By monitoring this noise, the sources responsible for this pollution canbe mapped. Monitoring is performed by passively listening to these sounds. Thisgenerates a large amount of data records, capturing a mix of sound sources suchas ship activities and marine mammal vocalizations. Although machine learningoffers a promising solution for automatic sound classification, currentstate-of-the-art methods implement supervised learning. This requires a largeamount of high-quality labeled data that is not publicly available. Incontrast, a massive amount of lower-quality unlabeled data is publiclyavailable, offering the opportunity to explore unsupervised learningtechniques. This research explores this possibility by implementing anunsupervised Contrastive Learning approach. Here, a Conformer-based encoder isoptimized by the so-called Variance-Invariance-Covariance Regularization lossfunction on these lower-quality unlabeled data and the translation to thelabeled data is made. Through classification tasks involving recognizing shiptypes and marine mammal vocalizations, our method demonstrates to producerobust and generalized embeddings. This shows to potential of unsupervisedmethods for various automatic underwater acoustic analysis tasks.</description>
      <author>example@mail.com (Hilde I. Hummel, Arwin Gansekoele, Sandjai Bhulai, Rob van der Mei)</author>
      <guid isPermaLink="false">2505.12904v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2505.12448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SSR的全新空间感知与推理方法，旨在提高视觉语言模型在多模态任务中的空间理解能力。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言模型在处理多模态任务时，由于依赖RGB输入，其空间理解能力受限。现有的空间信息整合方法要么需要专门的传感器，要么无法有效利用深度信息进行高级推理。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，将原始深度数据转换为结构化、可解释的文本推理，从而显著增强空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;SSR方法利用知识蒸馏技术将生成的推理压缩成紧凑的潜在嵌入，以便资源高效地集成到现有的视觉语言模型中，无需重新训练。同时，为了全面评估，引入了新的数据集SSR-CoT和一个综合的多任务基准SSRBench。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准上的实验表明，SSR显著提高了深度数据的利用效率，并增强了空间推理能力，推动了视觉语言模型向更类似人类的多模态理解迈进。&lt;h4&gt;结论&lt;/h4&gt;SSR方法为视觉语言模型的空间理解能力提供了新的解决方案，有助于实现更高级别的多模态理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite impressive advancements in Visual-Language Models (VLMs) formulti-modal tasks, their reliance on RGB inputs limits precise spatialunderstanding. Existing methods for integrating spatial cues, such as pointclouds or depth, either require specialized sensors or fail to effectivelyexploit depth information for higher-order reasoning. To this end, we propose anovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework thattransforms raw depth data into structured, interpretable textual rationales.These textual rationales serve as meaningful intermediate representations tosignificantly enhance spatial reasoning capabilities. Additionally, we leverageknowledge distillation to compress the generated rationales into compact latentembeddings, which facilitate resource-efficient and plug-and-play integrationinto existing VLMs without retraining. To enable comprehensive evaluation, weintroduce a new dataset named SSR-CoT, a million-scale visual-languagereasoning dataset enriched with intermediate spatial reasoning annotations, andpresent SSRBench, a comprehensive multi-task benchmark. Extensive experimentson multiple benchmarks demonstrate SSR substantially improves depth utilizationand enhances spatial reasoning, thereby advancing VLMs toward more human-likemulti-modal understanding. Our project page is athttps://yliu-cs.github.io/SSR.</description>
      <author>example@mail.com (Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2505.12448v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo</title>
      <link>http://arxiv.org/abs/2505.12714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于实例自适应的MVS模型（IA-MVS），通过缩小深度假设范围和对每个实例进行细化，提高了深度估计的精度。&lt;h4&gt;背景&lt;/h4&gt;现有的MVS模型基于渐进式深度假设缩小取得了显著进展，但尚未充分利用个体实例的深度覆盖率低于整个场景的潜力，这限制了深度估计精度的进一步提高。&lt;h4&gt;目的&lt;/h4&gt;提出IA-MVS以提高深度估计的精度，并通过改进置信度估计和鲁棒性来优化模型。&lt;h4&gt;方法&lt;/h4&gt;IA-MVS通过缩小深度假设范围和每个实例的细化来增强精度，同时引入基于实例内深度连续性的滤波机制以提升鲁棒性。此外，开发了一个基于条件概率的详细数学模型用于置信度估计。&lt;h4&gt;主要发现&lt;/h4&gt;IA-MVS在DTU基准测试中取得了最先进的性能，并且可以广泛应用于基于MVSNet的模型而不增加额外的训练负担。&lt;h4&gt;结论&lt;/h4&gt;IA-MVS通过改进深度估计精度和鲁棒性，在MVS模型中实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Multi-view stereo (MVS) models based on progressive depth hypothesis narrowing have made remarkable advancements. However, existing methods haven't fully utilized the potential that the depth coverage of individual instances is smaller than that of the entire scene, which restricts further improvements in depth estimation precision. Moreover, inevitable deviations in the initial stage accumulate as the process advances. In this paper, we propose Instance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation by narrowing the depth hypothesis range and conducting refinement on each instance. Additionally, a filtering mechanism based on intra-instance depth continuity priors is incorporated to boost robustness. Furthermore, recognizing that existing confidence estimation can degrade IA-MVS performance on point clouds. We have developed a detailed mathematical model for confidence estimation based on conditional probability. The proposed method can be widely applied in models based on MVSNet without imposing extra training burdens. Our method achieves state-of-the-art performance on the DTU benchmark. The source code is available at https://github.com/KevinWang73106/IA-MVS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view stereo (MVS) models based on progressive depth hypothesisnarrowing have made remarkable advancements. However, existing methods haven'tfully utilized the potential that the depth coverage of individual instances issmaller than that of the entire scene, which restricts further improvements indepth estimation precision. Moreover, inevitable deviations in the initialstage accumulate as the process advances. In this paper, we proposeInstance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimationby narrowing the depth hypothesis range and conducting refinement on eachinstance. Additionally, a filtering mechanism based on intra-instance depthcontinuity priors is incorporated to boost robustness. Furthermore, recognizingthat existing confidence estimation can degrade IA-MVS performance on pointclouds. We have developed a detailed mathematical model for confidenceestimation based on conditional probability. The proposed method can be widelyapplied in models based on MVSNet without imposing extra training burdens. Ourmethod achieves state-of-the-art performance on the DTU benchmark. The sourcecode is available at https://github.com/KevinWang73106/IA-MVS.</description>
      <author>example@mail.com (Yinzhe Wang, Yiwen Xiao, Hu Wang, Yiping Xu, Yan Tian)</author>
      <guid isPermaLink="false">2505.12714v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy</title>
      <link>http://arxiv.org/abs/2505.11832v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自回归过程的器官运动预测方法，用于提高放疗前的器官运动预测精度。&lt;h4&gt;背景&lt;/h4&gt;放疗过程中，由于呼吸和其他生理因素，患者可能经历器官运动。现有的预测方法主要依赖主成分分析（PCA）进行变形分析，但这种方法对配准质量依赖性高，难以捕捉运动的时间动态特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的器官运动预测方法，以更好地捕捉患者特定的运动模式，从而确保放疗的精确性。&lt;h4&gt;方法&lt;/h4&gt;通过获取每位患者的4D CT扫描，并使用自回归模型预测未来的CT相位，基于先前的相位运动模式进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在预测肺和心脏运动方面优于现有基准，证明了其在捕捉CT图像运动动态方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法有潜力提高放疗前的规划，实现更精确和自适应的放疗。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we propose an autoregressive process-based organ motion prediction method to improve the precision of organ motion prediction before radiotherapy. The existing prediction methods mainly rely on deformation analysis using principal component analysis (PCA), which is highly dependent on registration quality and struggles to capture the periodic temporal dynamics for motion modeling. The purpose of this study is to develop a new organ motion prediction method to better capture patient-specific motion patterns, thereby ensuring the precision of radiotherapy. By obtaining 4D CT scans for each patient and using an autoregressive model to predict future CT phases based on prior phase motion patterns, the method has demonstrated its effectiveness in capturing motion dynamics from CT images. These results highlight the potential of our method to improve pre-treatment planning in radiotherapy, enabling more precise and adaptive radiation delivery.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiotherapy often involves a prolonged treatment period. During this time,patients may experience organ motion due to breathing and other physiologicalfactors. Predicting and modeling this motion before treatment is crucial forensuring precise radiation delivery. However, existing pre-treatment organmotion prediction methods primarily rely on deformation analysis usingprincipal component analysis (PCA), which is highly dependent on registrationquality and struggles to capture periodic temporal dynamics for motionmodeling.In this paper, we observe that organ motion prediction closelyresembles an autoregressive process, a technique widely used in naturallanguage processing (NLP). Autoregressive models predict the next token basedon previous inputs, naturally aligning with our objective of predicting futureorgan motion phases. Building on this insight, we reformulate organ motionprediction as an autoregressive process to better capture patient-specificmotion patterns. Specifically, we acquire 4D CT scans for each patient beforetreatment, with each sequence comprising multiple 3D CT phases. These phasesare fed into the autoregressive model to predict future phases based on priorphase motion patterns. We evaluate our method on a real-world test set of 4D CTscans from 50 patients who underwent radiotherapy at our institution and apublic dataset containing 4D CT scans from 20 patients (some with multiplescans), totaling over 1,300 3D CT phases. The performance in predicting themotion of the lung and heart surpasses existing benchmarks, demonstrating itseffectiveness in capturing motion dynamics from CT images. These resultshighlight the potential of our method to improve pre-treatment planning inradiotherapy, enabling more precise and adaptive radiation delivery.</description>
      <author>example@mail.com (Yuxiang Lai, Jike Zhong, Vanessa Su, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2505.11832v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Mamba-Adaptor: State Space Model Adaptor for Visual Recognition</title>
      <link>http://arxiv.org/abs/2505.12685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mamba-Adaptor的视觉任务适配器，用于改进Mamba模型在视觉建模中的表现。&lt;h4&gt;背景&lt;/h4&gt;Mamba模型在视觉建模中表现良好，但在处理视觉任务时存在三个主要限制：无法访问全局上下文、计算当前隐藏状态时存在长期遗忘问题、空间结构建模较弱。&lt;h4&gt;目的&lt;/h4&gt;旨在解决Mamba模型在视觉任务中的性能问题。&lt;h4&gt;方法&lt;/h4&gt;提出了两个功能模块：Adaptor-T和Adaptor-S。Adaptor-T通过选择可学习的位置作为记忆增强来减轻长期遗忘问题；Adaptor-S通过多尺度扩张卷积核增强空间建模，并将图像归纳偏好引入特征输出。&lt;h4&gt;主要发现&lt;/h4&gt;Mamba-Adaptor通过扩展上下文建模和增强输出，有效解决了Mamba模型的限制。它可以在三种使用场景中提高性能：作为通用视觉骨干、作为预训练骨干的增强模块、作为高效微调模块以适应迁移学习任务。&lt;h4&gt;结论&lt;/h4&gt;Mamba-Adaptor在ImageNet和COCO基准测试中取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Recent State Space Models (SSM), especially Mamba, have demonstrated impressive performance in visual modeling and possess superior model efficiency. However, the application of Mamba to visual tasks suffers inferior performance due to three main constraints existing in the sequential model: 1) Casual computing is incapable of accessing global context; 2) Long-range forgetting when computing the current hidden states; 3) Weak spatial structural modeling due to the transformed sequential input. To address these issues, we investigate a simple yet powerful vision task Adaptor for Mamba models, which consists of two functional modules: Adaptor-T and Adaptor-S. When solving the hidden states for SSM, we apply a lightweight prediction module Adaptor-T to select a set of learnable locations as memory augmentations to ease long-range forgetting issues. Moreover, we leverage Adapator-S, composed of multi-scaledilated convolutional kernels, to enhance the spatial modeling and introducethe image inductive bias into the feature output. Both modules can enlarge the context modeling in casual computing, as the output is enhanced by the inaccessible features. We explore three usages of Mamba-Adaptor: A general visual backbone for various vision tasks; A booster module to raise the performance of pretrained backbones; A highly efficient fine-tuning module that adapts the base model for transfer learning tasks. Extensive experiments verify the effectiveness of Mamba-Adaptor in three settings. Notably, our Mamba-Adaptor achieves state-of-the-art performance on the ImageNet and COCO benchmarks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent State Space Models (SSM), especially Mamba, have demonstratedimpressive performance in visual modeling and possess superior modelefficiency. However, the application of Mamba to visual tasks suffers inferiorperformance due to three main constraints existing in the sequential model: 1)Casual computing is incapable of accessing global context; 2) Long-rangeforgetting when computing the current hidden states; 3) Weak spatial structuralmodeling due to the transformed sequential input. To address these issues, weinvestigate a simple yet powerful vision task Adaptor for Mamba models, whichconsists of two functional modules: Adaptor-T and Adaptor-S. When solving thehidden states for SSM, we apply a lightweight prediction module Adaptor-T toselect a set of learnable locations as memory augmentations to ease long-rangeforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scaledilated convolutional kernels, to enhance the spatial modeling and introducethe image inductive bias into the feature output. Both modules can enlarge thecontext modeling in casual computing, as the output is enhanced by theinaccessible features. We explore three usages of Mamba-Adaptor: A generalvisual backbone for various vision tasks; A booster module to raise theperformance of pretrained backbones; A highly efficient fine-tuning module thatadapts the base model for transfer learning tasks. Extensive experiments verifythe effectiveness of Mamba-Adaptor in three settings. Notably, ourMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCObenchmarks.</description>
      <author>example@mail.com (Fei Xie, Jiahao Nie, Yujin Tang, Wenkang Zhang, Hongshen Zhao)</author>
      <guid isPermaLink="false">2505.12685v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
      <link>http://arxiv.org/abs/2505.12363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 19 figures, 4 tables. Code, models, and dataset are  available at our project page: https://github.com/nkkbr/ViCA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViCA2是一个新型多模态大型语言模型，旨在提升空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;现有多模态大型语言模型在视觉-语言任务上表现出色，但在空间推理方面存在挑战，缺乏必要的架构组件和专门训练数据。&lt;h4&gt;目的&lt;/h4&gt;通过ViCA2增强空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;ViCA2具备双重视觉编码架构，整合SigLIP处理语义和Hiera处理空间结构，并辅以token比例控制机制以提升效率。同时，开发了一个包含超过322,000个空间基础问答对的大型数据集ViCA-322K。&lt;h4&gt;主要发现&lt;/h4&gt;ViCA2-7B模型在VSI-Bench基准测试中取得56.8的平均分，显著优于其他大型模型和领先私有模型，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;ViCA2在实现强大空间智能的同时保持模型紧凑，其代码库和数据集被发布以促进进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然多模态大型语言模型（MLLMs）在视觉-语言任务上表现出色，但在空间推理——关于空间布局、关系和动态的推理——方面仍是一个重大挑战。现有模型往往缺乏必要的架构组件和用于细粒度空间理解的专门训练数据。我们介绍了ViCA2（空间认知助手2），这是一种新型的MLLM，旨在增强空间推理。ViCA2具有双重视觉编码架构，集成了SigLIP进行语义处理和Hiera进行空间结构处理，并结合了token比例控制机制以提高效率。我们还开发了ViCA-322K，这是一个包含超过322,000个空间基础问答对的新的大型数据集，用于针对性的指令调整。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型取得了56.8的顶级平均分数，显著超过了更大的开源模型（例如，LLaVA-NeXT-Video-72B，40.9）和领先的私有模型（Gemini-1.5 Pro，45.4）。这证明了我们的方法在实现强大空间智能的同时保持模型紧凑的有效性。我们发布了ViCA2、其代码库和ViCA-322K数据集，以促进进一步的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nkkbr/vica&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Multimodal Large Language Models (MLLMs) excel at generalvision-language tasks, visuospatial cognition - reasoning about spatiallayouts, relations, and dynamics - remains a significant challenge. Existingmodels often lack the necessary architectural components and specializedtraining data for fine-grained spatial understanding. We introduce ViCA2(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatialreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIPfor semantics and Hiera for spatial structure, coupled with a token ratiocontrol mechanism for efficiency. We also developed ViCA-322K, a newlarge-scale dataset with over 322,000 spatially grounded question-answer pairsfor targeted instruction tuning. On the challenging VSI-Bench benchmark, ourViCA2-7B model achieves a state-of-the-art average score of 56.8, significantlysurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) andleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates theeffectiveness of our approach in achieving strong visuospatial intelligencewith a compact model. We release ViCA2, its codebase, and the ViCA-322K datasetto facilitate further research.</description>
      <author>example@mail.com (Qi Feng, Hidetoshi Shimodaira)</author>
      <guid isPermaLink="false">2505.12363v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>From Grunts to Grammar: Emergent Language from Cooperative Foraging</title>
      <link>http://arxiv.org/abs/2505.12872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究语言在多智能体觅食游戏中的起源，通过模拟早期人类合作的环境，发现智能体通过深度强化学习发展出具有自然语言特征的沟通协议。&lt;h4&gt;背景&lt;/h4&gt;早期人类通过手势、声音和简单信号进行协调、规划、避免捕食者和共享资源。语言如何演化、适应并成为团队合作的关键，一直是语言学和人类学研究的挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨语言在多智能体觅食游戏中的演化过程，理解语言如何从部分可观察性、时间推理和合作目标中产生。&lt;h4&gt;方法&lt;/h4&gt;设计多智能体觅食游戏环境，智能体在共享的网格世界中操作，仅对其他智能体和环境有部分了解，必须协调以完成游戏。使用端到端深度强化学习，智能体从零开始学习动作和沟通策略。&lt;h4&gt;主要发现&lt;/h4&gt;智能体发展出具有自然语言特征的沟通协议，包括任意性、可互换性、位移、文化传承和组合性。研究量化了这些特性，并分析了人口规模和时间依赖性等因素如何塑造语言的特定方面。&lt;h4&gt;结论&lt;/h4&gt;本文提出的框架为研究语言在多智能体环境中的演化提供了平台，并计划公开所有数据、代码和模型。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了语言在多智能体觅食游戏中的起源。通过模拟早期人类合作的环境，发现智能体通过深度强化学习发展出具有自然语言特征的沟通协议。早期人类依赖手势、声音和简单信号进行协调、规划、避免捕食者和共享资源。语言如何演化、适应并成为团队合作的关键，一直是语言学和人类学研究的挑战。本文旨在探讨语言在多智能体觅食游戏中的演化过程，理解语言如何从部分可观察性、时间推理和合作目标中产生。研究设计多智能体觅食游戏环境，智能体在共享的网格世界中操作，仅对其他智能体和环境有部分了解，必须协调以完成游戏。使用端到端深度强化学习，智能体从零开始学习动作和沟通策略。研究发现智能体发展出具有自然语言特征的沟通协议，包括任意性、可互换性、位移、文化传承和组合性。研究量化了这些特性，并分析了人口规模和时间依赖性等因素如何塑造语言的特定方面。本文提出的框架为研究语言在多智能体环境中的演化提供了平台，并计划公开所有数据、代码和模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early cavemen relied on gestures, vocalizations, and simple signals tocoordinate, plan, avoid predators, and share resources. Today, humanscollaborate using complex languages to achieve remarkable results. What drivesthis evolution in communication? How does language emerge, adapt, and becomevital for teamwork? Understanding the origins of language remains a challenge.A leading hypothesis in linguistics and anthropology posits that languageevolved to meet the ecological and social demands of early human cooperation.Language did not arise in isolation, but through shared survival goals.Inspired by this view, we investigate the emergence of language in multi-agentForaging Games. These environments are designed to reflect the cognitive andecological constraints believed to have influenced the evolution ofcommunication. Agents operate in a shared grid world with only partialknowledge about other agents and the environment, and must coordinate tocomplete games like picking up high-value targets or executing temporallyordered actions. Using end-to-end deep reinforcement learning, agents learnboth actions and communication strategies from scratch. We find that agentsdevelop communication protocols with hallmark features of natural language:arbitrariness, interchangeability, displacement, cultural transmission, andcompositionality. We quantify each property and analyze how different factors,such as population size and temporal dependencies, shape specific aspects ofthe emergent language. Our framework serves as a platform for studying howlanguage can evolve from partial observability, temporal reasoning, andcooperative goals in embodied multi-agent settings. We will release all data,code, and models publicly.</description>
      <author>example@mail.com (Maytus Piriyajitakonkij, Rujikorn Charakorn, Weicheng Tao, Wei Pan, Mingfei Sun, Cheston Tan, Mengmi Zhang)</author>
      <guid isPermaLink="false">2505.12872v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Unlearning</title>
      <link>http://arxiv.org/abs/2505.12614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AGU的自适应图遗忘框架，用于解决图神经网络中元素删除（如节点和边）的问题，以提高在现实应用中的有效性。&lt;h4&gt;背景&lt;/h4&gt;图遗忘对于处理可能包含过时、不准确或敏感信息的图数据至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在遗忘任务目标不一致和邻居识别不准确的问题。&lt;h4&gt;方法&lt;/h4&gt;AGU框架能够灵活适应不同的遗忘任务和图神经网络架构，确保删除元素的完全遗忘，同时保持剩余图的完整性，并准确识别受删除元素影响的邻居。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界图上的广泛实验表明，AGU在有效性、效率和遗忘能力方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;AGU框架为图神经网络中的图遗忘提供了一种有效和高效的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning, which deletes graph elements such as nodes and edges fromtrained graph neural networks (GNNs), is crucial for real-world applicationswhere graph data may contain outdated, inaccurate, or privacy-sensitiveinformation. However, existing methods often suffer from (1) incomplete or overunlearning due to neglecting the distinct objectives of different unlearningtasks, and (2) inaccurate identification of neighbors affected by deletedelements across various GNN architectures. To address these limitations, wepropose AGU, a novel Adaptive Graph Unlearning framework that flexibly adaptsto diverse unlearning tasks and GNN architectures. AGU ensures the completeforgetting of deleted elements while preserving the integrity of the remaininggraph. It also accurately identifies affected neighbors for each GNNarchitecture and prioritizes important ones to enhance unlearning performance.Extensive experiments on seven real-world graphs demonstrate that AGUoutperforms existing methods in terms of effectiveness, efficiency, andunlearning capability.</description>
      <author>example@mail.com (Pengfei Ding, Yan Wang, Guanfeng Liu, Jiajie Zhu)</author>
      <guid isPermaLink="false">2505.12614v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.12253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLaVA-4D的通用LMM框架，用于4D场景理解中的视觉表示。&lt;h4&gt;背景&lt;/h4&gt;尽管在2D图像理解方面取得了显著进展，但多模态模型（LMMs）在物理世界中由于缺乏空间表示而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时空提示，以增强LMMs对动态场景的理解。&lt;h4&gt;方法&lt;/h4&gt;LLaVA-4D通过将3D位置和1D时间编码到一个动态感知的4D坐标嵌入中，生成时空提示。此外，通过将时空组件从视觉特征中分离出来，提高了区分背景和对象的效果。&lt;h4&gt;主要发现&lt;/h4&gt;将4D时空提示嵌入到视觉特征中，LMMs能够理解物理世界中静态背景和动态对象的时空特征。&lt;h4&gt;结论&lt;/h4&gt;通过构建一个具有时空坐标注释的4D视觉-语言数据集，并通过大量实验证明了方法在4D场景理解中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管在2D图像理解方面取得了显著进展，但大型多模态模型（LMMs）由于缺乏空间表示，在物理世界中仍然面临挑战。通常，现有的3D LMMs主要通过将3D位置作为固定空间提示嵌入到视觉特征中来表示场景。然而，这些方法仅限于理解静态背景，无法捕捉动态对象的时间变化。在本文中，我们提出了一种名为LLaVA-4D的通用LMM框架，用于4D场景理解中的视觉表示。时空提示通过将3D位置和1D时间编码到一个动态感知的4D坐标嵌入中生成。此外，我们证明了从视觉特征中分离出来的空间和时空组件在区分背景和对象方面更有效。这促使我们将4D时空提示嵌入到这些特征中，以增强动态场景的表示。通过将视觉时空嵌入与语言嵌入对齐，LMMs获得了理解物理世界中静态背景和动态对象的时空特征的能力。此外，我们构建了一个具有时空坐标注释的4D视觉-语言数据集，用于指令微调LMMs。进行了大量实验，以证明我们的方法在4D场景理解的不同任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite achieving significant progress in 2D image understanding, largemultimodal models (LMMs) struggle in the physical world due to the lack ofspatial representation. Typically, existing 3D LMMs mainly embed 3D positionsas fixed spatial prompts within visual features to represent the scene.However, these methods are limited to understanding the static background andfail to capture temporally varying dynamic objects. In this paper, we proposeLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visualrepresentation in 4D scene understanding. The spatiotemporal prompt isgenerated by encoding 3D position and 1D time into a dynamic-aware 4Dcoordinate embedding. Moreover, we demonstrate that spatial and temporalcomponents disentangled from visual features are more effective indistinguishing the background from objects. This motivates embedding the 4Dspatiotemporal prompt into these features to enhance the dynamic scenerepresentation. By aligning visual spatiotemporal embeddings with languageembeddings, LMMs gain the ability to understand both spatial and temporalcharacteristics of static background and dynamic objects in the physical world.Additionally, we construct a 4D vision-language dataset with spatiotemporalcoordinate annotations for instruction fine-tuning LMMs. Extensive experimentshave been conducted to demonstrate the effectiveness of our method acrossdifferent tasks in 4D scene understanding.</description>
      <author>example@mail.com (Hanyu Zhou, Gim Hee Lee)</author>
      <guid isPermaLink="false">2505.12253v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2505.12788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于强化学习的N-tuple时序知识图谱推理方法，旨在提高推理的可解释性。&lt;h4&gt;背景&lt;/h4&gt;时序知识图谱（TKGs）使用（主体，谓词，对象，时间戳）四元组描述时序事实，而N-tuple TKGs通过使用n-tuples扩展了传统TKGs，以更细粒度地表示事实。&lt;h4&gt;目的&lt;/h4&gt;通过推理N-TKGs来预测基于历史事实的潜在未来事实，并提高推理的可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MT-Path的方法，利用时间信息遍历历史n-tuples并构建推理路径。MT-Path使用混合策略驱动的动作选择器，基于三个低级策略：谓词焦点策略、核心元素焦点策略和整个事实焦点策略。此外，它还使用一个感知辅助元素的GCN来捕捉事实之间丰富的语义依赖。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MT-Path在有效性和可解释性方面优于现有的N-TKG推理方法。&lt;h4&gt;结论&lt;/h4&gt;MT-Path是一种有效的N-TKG推理方法，能够提高推理的可解释性并更好地理解每个n-tuple。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of(subject, predicate, object, timestamp) to describe temporal facts, haveattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditionalTKGs by utilizing n-tuples to incorporate auxiliary elements alongside coreelements (i.e., subject, predicate, and object) of facts, so as to representthem in a more fine-grained manner. Reasoning over N-TKGs aims to predictpotential future facts based on historical ones. However, existing N-TKGreasoning methods often lack explainability due to their black-box nature.Therefore, we introduce a new Reinforcement Learning-based method, namedMT-Path, which leverages the temporal information to traverse historicaln-tuples and construct a temporal reasoning path. Specifically, in order tointegrate the information encapsulated within n-tuples, i.e., theentity-irrelevant information within the predicate, the information about coreelements, and the complete information about the entire n-tuples, MT-Pathutilizes a mixture policy-driven action selector, which bases on threelow-level policies, namely, the predicate-focused policy, thecore-element-focused policy and the whole-fact-focused policy. Further, MT-Pathutilizes an auxiliary element-aware GCN to capture the rich semanticdependencies among facts, thereby enabling the agent to gain a deepunderstanding of each n-tuple. Experimental results demonstrate theeffectiveness and the explainability of MT-Path.</description>
      <author>example@mail.com (Zhongni Hou, Miao Su, Xiaolong Jin, Zixuan Li, Long Bai, Jiafeng Guo, Xueqi Cheng)</author>
      <guid isPermaLink="false">2505.12788v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision</title>
      <link>http://arxiv.org/abs/2505.12526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HAL（历史平均标签）的方法，用于解决动态图中的训练效率问题，通过聚合历史节点交互生成伪标签，减少梯度方差，加速收敛。&lt;h4&gt;背景&lt;/h4&gt;TGNs（时序图网络）在动态图中由于监督信号不规律导致梯度更新稀疏，存在显著的训练效率问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来提高TGNs的训练效率，同时保持其性能。&lt;h4&gt;方法&lt;/h4&gt;通过将历史节点交互聚合为伪标签来减少梯度方差，并动态地丰富训练批次，利用历史标签分布生成伪目标。HAL通过将闲置计算转化为生产性学习步骤，确保参数的连续更新，而无需修改架构。&lt;h4&gt;主要发现&lt;/h4&gt;在TGB（时序图基准）上的实验验证了HAL的有效性，发现HAL可以将TGNv2的训练速度提高最多15倍，同时保持有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;HAL为时序图学习中的标签稀疏问题提供了一种高效、轻量级、架构无关且理论上有根据的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Temporal Graph Networks (TGNs), while being accurate, face significant training inefficiencies due to irregular supervision signals in dynamic graphs, which induce sparse gradient updates. We first theoretically establish that aggregating historical node interactions into pseudo-labels reduces gradient variance, accelerating convergence. Building on this analysis, we propose History-Averaged Labels (HAL), a method that dynamically enriches training batches with pseudo-targets derived from historical label distributions. HAL ensures continuous parameter updates without architectural modifications by converting idle computation into productive learning steps. Experiments on the Temporal Graph Benchmark (TGB) validate our findings and an assumption about slow change of user preferences: HAL accelerates TGNv2 training by up to 15x while maintaining competitive performance. Thus, this work offers an efficient, lightweight, architecture-agnostic, and theoretically motivated solution to label sparsity in temporal graph learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Graph Networks (TGNs), while being accurate, face significanttraining inefficiencies due to irregular supervision signals in dynamic graphs,which induce sparse gradient updates. We first theoretically establish thataggregating historical node interactions into pseudo-labels reduces gradientvariance, accelerating convergence. Building on this analysis, we proposeHistory-Averaged Labels (HAL), a method that dynamically enriches trainingbatches with pseudo-targets derived from historical label distributions. HALensures continuous parameter updates without architectural modifications byconverting idle computation into productive learning steps. Experiments on theTemporal Graph Benchmark (TGB) validate our findings and an assumption aboutslow change of user preferences: HAL accelerates TGNv2 training by up to 15xwhile maintaining competitive performance. Thus, this work offers an efficient,lightweight, architecture-agnostic, and theoretically motivated solution tolabel sparsity in temporal graph learning.</description>
      <author>example@mail.com (Alexander Panyshev, Dmitry Vinichenko, Oleg Travkin, Roman Alferov, Alexey Zaytsev)</author>
      <guid isPermaLink="false">2505.12526v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis</title>
      <link>http://arxiv.org/abs/2505.13033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TSPulse是一种超紧凑的时间序列预训练模型，仅含1M参数，适用于分类、异常检测、插补和检索任务，通过架构和任务层面的创新，显著提高了性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列预训练模型在时序表示学习方面取得了进展，但当前最先进模型规模较大，计算需求高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时间序列预训练模型，以提高性能并减少计算需求。&lt;h4&gt;方法&lt;/h4&gt;TSPulse采用了双重空间掩码重建，从时间和频率域学习以捕捉互补信号，并使用双重嵌入解耦生成详细和高层语义嵌入。在任务层面，它结合了TSLens组件和多头三角定位技术，以及混合掩码预训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;TSPulse在多个任务上取得了显著的性能提升，如分类基准测试提高5-16%，异常检测排行榜提高+20%，零样本插补提高+50%，时间序列检索提高+25%。这些结果是在仅1M参数的情况下实现的，使得TSPulse比现有预训练模型小10-100倍。&lt;h4&gt;结论&lt;/h4&gt;TSPulse通过其架构和任务创新，实现了高效的时序预训练模型，并有望开源。&lt;h4&gt;翻译&lt;/h4&gt;TSPulse的提出推进了时间序列预训练模型的发展，通过创新的架构和任务设计，在保证性能的同时显著降低了模型规模和计算需求，为高效的时间序列预训练模型树立了新标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of time-series pre-trained models has advanced temporalrepresentation learning, but current state-of-the-art models are oftenlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compacttime-series pre-trained models with only 1M parameters, specialized to performstrongly across classification, anomaly detection, imputation, and retrievaltasks. TSPulse introduces innovations at both the architecture and task levels.At the architecture level, it employs a dual-space masked reconstruction,learning from both time and frequency domains to capture complementary signals.This is further enhanced by a dual-embedding disentanglement, generating bothdetailed embeddings for fine-grained analysis and high-level semanticembeddings for broader task understanding. Notably, TSPulse's semanticembeddings are robust to shifts in time, magnitude, and noise, which isimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,a fine-tuning component enabling task-specific feature attention. It alsointroduces a multi-head triangulation technique that correlates deviations frommultiple prediction heads, enhancing anomaly detection by fusing complementarymodel outputs. Additionally, a hybrid mask pretraining is proposed to improveszero-shot imputation by reducing pre-training bias. These architecture and taskinnovations collectively contribute to TSPulse's significant performance gains:5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomalydetection leaderboard, +50% in zero-shot imputation, and +25% in time-seriesretrieval. Remarkably, these results are achieved with just 1M parameters,making TSPulse 10-100X smaller than existing pre-trained models. Its efficiencyenables GPU-free inference and rapid pre-training, setting a new standard forefficient time-series pre-trained models. Models will be open-sourced soon.</description>
      <author>example@mail.com (Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam)</author>
      <guid isPermaLink="false">2505.13033v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Industry-focused Synthetic Segmentation Pre-training</title>
      <link>http://arxiv.org/abs/2505.13099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InsCore的合成预训练数据集，用于工业应用的视觉基础模型，以解决工业图像数据集在预训练中的挑战。&lt;h4&gt;背景&lt;/h4&gt;在真实图像数据集上预训练对实例分割效果显著，但工业应用面临法律和道德限制以及领域差异导致的迁移性限制。&lt;h4&gt;目的&lt;/h4&gt;研究在不依赖真实图像或手动标注的情况下，能否构建用于工业应用的视觉基础模型，并探讨其性能是否能超过经过微调的SAM模型。&lt;h4&gt;方法&lt;/h4&gt;提出InsCore数据集，基于公式驱动的监督学习（FDSL）生成反映工业数据关键特征的实例分割图像，无需真实图像或人工标注。&lt;h4&gt;主要发现&lt;/h4&gt;使用InsCore预训练的模型在五个工业数据集上表现优于使用COCO、ImageNet-21k和微调SAM训练的模型，平均提升6.2个点，且仅需100k合成图像，效率远超SAM的SA-1B数据集。&lt;h4&gt;结论&lt;/h4&gt;InsCore是一个实用且无版权费的视觉基础模型，适用于工业应用。&lt;h4&gt;翻译&lt;/h4&gt;The paper proposes the Instance Core Segmentation Dataset (InsCore), a synthetic pre-training dataset for industrial vision foundation models, addressing the challenges in pre-training industrial image data sets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training on real-image datasets has been widely proven effective forimproving instance segmentation. However, industrial applications face two keychallenges: (1) legal and ethical restrictions, such as ImageNet's prohibitionof commercial use, and (2) limited transferability due to the domain gapbetween web images and industrial imagery. Even recent vision foundationmodels, including the segment anything model (SAM), show notable performancedegradation in industrial settings. These challenges raise critical questions:Can we build a vision foundation model for industrial applications withoutrelying on real images or manual annotations? And can such models outperformeven fine-tuned SAM on industrial datasets? To address these questions, wepropose the Instance Core Segmentation Dataset (InsCore), a syntheticpre-training dataset based on formula-driven supervised learning (FDSL).InsCore generates fully annotated instance segmentation images that reflect keycharacteristics of industrial data, including complex occlusions, densehierarchical masks, and diverse non-rigid shapes, distinct from typical webimagery. Unlike previous methods, InsCore requires neither real images norhuman annotations. Experiments on five industrial datasets show that modelspre-trained with InsCore outperform those trained on COCO and ImageNet-21k, aswell as fine-tuned SAM, achieving an average improvement of 6.2 points ininstance segmentation performance. This result is achieved using only 100ksynthetic images, more than 100 times fewer than the 11 million images in SAM'sSA-1B dataset, demonstrating the data efficiency of our approach. Thesefindings position InsCore as a practical and license-free vision foundationmodel for industrial applications.</description>
      <author>example@mail.com (Shinichi Mae, Ryosuke Yamada, Hirokatsu Kataoka)</author>
      <guid isPermaLink="false">2505.13099v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection</title>
      <link>http://arxiv.org/abs/2505.12507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了LM$^2$otifs，一种用于机器生成文本检测的新颖可解释框架，旨在解决现有检测方法在可解释性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;大语言模型生成自然文本的能力令人印象深刻，但也带来了作者身份验证的挑战。虽然已有多种检测方法区分机器生成文本（MGT）和人工生成文本（HGT），但这些方法的可解释性仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;提出LM$^2$otifs，旨在解决传统可解释性技术无法捕捉复杂词汇关系的问题，从而提高MGT检测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;LM$^2$otifs利用可解释图神经网络，通过三个关键阶段实现文本检测和解释：1. 将文本转换为基于词共现的图，以表示词汇依赖；2. 使用图神经网络进行预测；3. 使用后处理可解释性方法提取可解释的基序，从单个词到句子结构提供多级解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LM$^2$otifs在多个基准数据集上具有可比的性能，提取的可解释基序在区分HGT和MGT方面非常有效。定性分析揭示了MGT特有的语言指纹。&lt;h4&gt;结论&lt;/h4&gt;LM$^2$otifs是一个有效的MGT检测框架，能够提供准确的可解释性，有助于解决当前检测方法在可解释性方面的不足。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型在生成自然文本方面的惊人能力导致了作者身份验证的关键挑战。尽管已经开发了许多检测方法来区分机器生成文本（MGT）和人工生成文本（HGT），但这些方法的可解释性仍然存在显著差距。传统的可解释性技术通常无法捕捉区分HGT和MGT的复杂词汇关系。为了解决这一局限性，我们提出了LM$^2$otifs，这是一种用于MGT检测的新颖可解释框架。受概率图模型启发，我们提供了有效性的理论依据。LM$^2$otifs利用可解释图神经网络来实现准确检测和可解释性。LM$^2$otifs流程分为三个关键阶段：首先，它将文本转换为基于词共现的图，以表示词汇依赖；其次，使用图神经网络进行预测；最后，使用后处理可解释性方法提取可解释的基序，从单个词到句子结构提供多级解释。在多个基准数据集上的广泛实验表明，LM$^2$otifs具有可比的性能。提取的可解释基序的实证评估确认了它们在区分HGT和MGT方面的有效性。此外，定性分析揭示了MGT特有的、可见的语言指纹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The impressive ability of large language models to generate natural textacross various tasks has led to critical challenges in authorshipauthentication. Although numerous detection methods have been developed todifferentiate between machine-generated texts (MGT) and human-generated texts(HGT), the explainability of these methods remains a significant gap.Traditional explainability techniques often fall short in capturing the complexword relationships that distinguish HGT from MGT. To address this limitation,we present LM$^2$otifs, a novel explainable framework for MGT detection.Inspired by probabilistic graphical models, we provide a theoretical rationalefor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networksto achieve both accurate detection and interpretability. The LM$^2$otifspipeline operates in three key stages: first, it transforms text into graphsbased on word co-occurrence to represent lexical dependencies; second, graphneural networks are used for prediction; and third, a post-hoc explainabilitymethod extracts interpretable motifs, offering multi-level explanations fromindividual words to sentence structures. Extensive experiments on multiplebenchmark datasets demonstrate the comparable performance of LM$^2$otifs. Theempirical evaluation of the extracted explainable motifs confirms theireffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysisreveals distinct and visible linguistic fingerprints characteristic of MGT.</description>
      <author>example@mail.com (Xu Zheng, Zhuomin Chen, Esteban Schafir, Sipeng Chen, Hojat Allah Salehi, Haifeng Chen, Farhad Shirani, Wei Cheng, Dongsheng Luo)</author>
      <guid isPermaLink="false">2505.12507v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SPKLIP: Aligning Spike Video Streams with Natural Language</title>
      <link>http://arxiv.org/abs/2505.12656v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPKLIP是一种专为Spike-VLA（神经元视频-语言对齐）设计的架构，通过引入分层神经元特征提取器和对比学习，实现了对神经元视频和语言的直接对齐，提高了能效，并在基准数据和真实世界数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;Spike cameras具有独特的感知能力，但它们的稀疏、异步输出对语义理解构成了挑战，特别是在Spike-VLA任务中，由于模态不匹配，如CLIP等模型表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出SPKLIP架构，旨在解决Spike-VLA中的模态不匹配问题，实现神经元视频和语言的准确对齐。&lt;h4&gt;方法&lt;/h4&gt;SPKLIP使用分层神经元特征提取器来适应性地模拟事件流中的多尺度时间动态，并采用对比学习直接对齐神经元视频和语言，同时引入了全神经元视觉编码器以增强能效。&lt;h4&gt;主要发现&lt;/h4&gt;SPKLIP在基准神经元数据集上实现了最先进的性能，并在新的真实世界数据集上展示了强大的少样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SPKLIP的高能效表明其在神经形态部署中的潜力，推动了基于事件的多模态研究。&lt;h4&gt;翻译&lt;/h4&gt;Spike cameras provide unique sensing capabilities, but their sparse and asynchronous outputs pose challenges to semantic understanding, especially for SpikeVideo-Language Alignment (Spike-VLA), where models like CLIP underperform due to modality mismatch. We introduce SPKLIP, the first architecture specifically designed for Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that adaptively models multi-scale temporal dynamics in event streams, and uses spike-text contrastive learning to directly align spike video with language, enabling effective few-shot learning. A full-spiking visual encoder variant, integrating SNN components into our pipeline, demonstrates enhanced energy efficiency. Experiments show state-of-the-art performance on benchmark spike datasets and strong few-shot generalization on a newly contributed real-world dataset. SPKLIP's energy efficiency highlights its potential for neuromorphic deployment, advancing event-based multimodal research. The source code and dataset are available at [link removed for anonymity].&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spike cameras offer unique sensing capabilities but their sparse,asynchronous output challenges semantic understanding, especially for SpikeVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due tomodality mismatch. We introduce SPKLIP, the first architecture specifically forSpike-VLA. SPKLIP employs a hierarchical spike feature extractor thatadaptively models multi-scale temporal dynamics in event streams, and usesspike-text contrastive learning to directly align spike video with language,enabling effective few-shot learning. A full-spiking visual encoder variant,integrating SNN components into our pipeline, demonstrates enhanced energyefficiency. Experiments show state-of-the-art performance on benchmark spikedatasets and strong few-shot generalization on a newly contributed real-worlddataset. SPKLIP's energy efficiency highlights its potential for neuromorphicdeployment, advancing event-based multimodal research. The source code anddataset are available at [link removed for anonymity].</description>
      <author>example@mail.com (Yongchang Gao, Meiling Jin, Zhaofei Yu, Tiejun Huang, Guozhang Chen)</author>
      <guid isPermaLink="false">2505.12656v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy</title>
      <link>http://arxiv.org/abs/2505.12693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应的多模态融合框架TACOcc，用于3D语义占用预测，并通过体积渲染监督增强其性能。&lt;h4&gt;背景&lt;/h4&gt;多模态3D占用预测的性能受限于无效的融合，主要由于固定融合策略导致的几何-语义不匹配和由稀疏、噪声标注引起的表面细节损失。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决方案，以解决几何-语义不匹配和表面细节损失问题，从而提高3D占用预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种目标尺度自适应、双向对称检索机制，以解决固定邻域融合下的匹配偏差。2. 引入了一种基于3D高斯Splatting的改进体积渲染流程，用于图像渲染，并应用光度一致性监督，联合优化2D-3D一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的机制可以增强上下文感知，提高效率并抑制噪声，实现准确的跨模态特征对齐。改进的体积渲染流程可以增强表面细节重建，同时抑制噪声传播。&lt;h4&gt;结论&lt;/h4&gt;在nuScenes和SemanticKITTI基准上的实验验证了TACOcc框架的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态3D占用预测的性能受到无效融合的限制，这主要由于固定融合策略导致的几何-语义不匹配以及由稀疏、噪声标注引起的表面细节损失。这种不匹配源于点云和图像特征的异构尺度与分布，导致固定邻域融合下的匹配偏差。为了解决这个问题，我们提出了一种目标尺度自适应的双向对称检索机制。该机制扩大大目标周围的邻域以增强上下文感知，缩小小目标的邻域以提高效率和抑制噪声，从而实现准确的跨模态特征对齐。该机制明确建立了空间对应关系并提高了融合的准确性。对于表面细节损失，稀疏标签提供了有限的监督，导致对小对象的预测效果不佳。我们引入了一种基于3D高斯Splatting的改进体积渲染流程，它将融合特征作为输入进行图像渲染，应用光度一致性监督，并联合优化2D-3D一致性。这增强了表面细节重建，同时抑制了噪声传播。总之，我们提出了TACOcc，一种自适应的多模态融合框架，用于3D语义占用预测，并通过体积渲染监督增强其性能。在nuScenes和SemanticKITTI基准上的实验验证了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of multi-modal 3D occupancy prediction is limited byineffective fusion, mainly due to geometry-semantics mismatch from fixed fusionstrategies and surface detail loss caused by sparse, noisy annotations. Themismatch stems from the heterogeneous scale and distribution of point cloud andimage features, leading to biased matching under fixed neighborhood fusion. Toaddress this, we propose a target-scale adaptive, bidirectional symmetricretrieval mechanism. It expands the neighborhood for large targets to enhancecontext awareness and shrinks it for small ones to improve efficiency andsuppress noise, enabling accurate cross-modal feature alignment. This mechanismexplicitly establishes spatial correspondences and improves fusion accuracy.For surface detail loss, sparse labels provide limited supervision, resultingin poor predictions for small objects. We introduce an improved volumerendering pipeline based on 3D Gaussian Splatting, which takes fused featuresas input to render images, applies photometric consistency supervision, andjointly optimizes 2D-3D consistency. This enhances surface detailreconstruction while suppressing noise propagation. In summary, we proposeTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancyprediction, enhanced by volume rendering supervision. Experiments on thenuScenes and SemanticKITTI benchmarks validate its effectiveness.</description>
      <author>example@mail.com (Luyao Lei, Shuo Xu, Yifan Bai, Xing Wei)</author>
      <guid isPermaLink="false">2505.12693v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.12681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在存在领域分布偏移的情况下，如何利用对抗性数据增强（ADA）来提高迁移学习系统的鲁棒性和适应性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习在存在领域分布偏移的情况下面临挑战，而对抗性扰动传统上被视为暴露模型脆弱性的威胁。&lt;h4&gt;目的&lt;/h4&gt;系统地研究对抗性数据增强（ADA）在提高迁移学习中的鲁棒性和适应性所起的作用。&lt;h4&gt;方法&lt;/h4&gt;分析对抗样本在训练中战略性地使用如何通过丰富决策边界和减少对源域特定特征的过拟合来提高领域泛化。提出了一个将ADA与一致性正则化和领域不变表示学习相结合的统一框架。&lt;h4&gt;主要发现&lt;/h4&gt;在VisDA、DomainNet和Office-Home等多个基准数据集上的实验表明，该方法在无监督和少样本域适应设置下均能持续提高目标域的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，对抗学习具有建设性，将扰动从破坏性攻击转化为提高跨领域迁移性的正则化力量。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Transfer learning across domains with distribution shift remains a fundamental challenge in building robust and adaptable machine learning systems. While adversarial perturbations are traditionally viewed as threats that expose model vulnerabilities, recent studies suggest that they can also serve as constructive tools for data augmentation. In this work, we systematically investigate the role of adversarial data augmentation (ADA) in enhancing both robustness and adaptivity in transfer learning settings. We analyze how adversarial examples, when used strategically during training, improve domain generalization by enriching decision boundaries and reducing overfitting to source-domain-specific features. We further propose a unified framework that integrates ADA with consistency regularization and domain-invariant representation learning. Extensive experiments across multiple benchmark datasets -- including VisDA, DomainNet, and Office-Home -- demonstrate that our method consistently improves target-domain performance under both unsupervised and few-shot domain adaptation settings. Our results highlight a constructive perspective of adversarial learning, transforming perturbation from a destructive attack into a regularizing force for cross-domain transferability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning across domains with distribution shift remains afundamental challenge in building robust and adaptable machine learningsystems. While adversarial perturbations are traditionally viewed as threatsthat expose model vulnerabilities, recent studies suggest that they can alsoserve as constructive tools for data augmentation. In this work, wesystematically investigate the role of adversarial data augmentation (ADA) inenhancing both robustness and adaptivity in transfer learning settings. Weanalyze how adversarial examples, when used strategically during training,improve domain generalization by enriching decision boundaries and reducingoverfitting to source-domain-specific features. We further propose a unifiedframework that integrates ADA with consistency regularization anddomain-invariant representation learning. Extensive experiments across multiplebenchmark datasets -- including VisDA, DomainNet, and Office-Home --demonstrate that our method consistently improves target-domain performanceunder both unsupervised and few-shot domain adaptation settings. Our resultshighlight a constructive perspective of adversarial learning, transformingperturbation from a destructive attack into a regularizing force forcross-domain transferability.</description>
      <author>example@mail.com (Hana Satou, Alan Mitkiy)</author>
      <guid isPermaLink="false">2505.12681v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval</title>
      <link>http://arxiv.org/abs/2505.12499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GARE的Gap-Aware Retrieval框架，用于解决文本-视频检索中的模态差距和批量采样中错误负样本的问题，通过引入可学习的增量Delta_ij来缓解优化紧张。&lt;h4&gt;背景&lt;/h4&gt;现有的文本-视频检索方法主要受到对比学习框架的驱动，但忽略了文本和视频在表示空间中的分离（模态差距）以及批量采样中错误负样本的普遍存在。&lt;h4&gt;目的&lt;/h4&gt;提出GARE框架的目的是为了缓解优化紧张，提高检索的稳定性和准确性。&lt;h4&gt;方法&lt;/h4&gt;GARE框架通过以下方法实现：1. 引入可学习的增量Delta_ij，通过InfoNCE损失的耦合多元一阶泰勒近似来计算；2. 设计一个轻量级的神经网络模块，基于语义差距进行结构感知修正；3. 使用三个正则化组件来稳定学习和提高可解释性：信任域约束、方向多样性项和信息瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GARE框架在四个检索基准上均能提高对齐准确性和对噪声监督的鲁棒性，证实了间隙感知紧张缓解的有效性。&lt;h4&gt;结论&lt;/h4&gt;GARE框架通过引入增量Delta_ij和正则化技术，有效地缓解了文本-视频检索中的模态差距和优化紧张问题，提高了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, a Gap-Aware Retrieval framework named GARE is proposed to address the modality gap and the prevalence of false negatives in batch sampling in text-video retrieval. The framework introduces a learnable increment Delta_ij to alleviate the optimization tension. Experiments show that GARE improves alignment accuracy and robustness to noisy supervision, confirming the effectiveness of the gap-aware tension mitigation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/musicman217/gare-text-video-retrieval&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-video retrieval have been largely driven bycontrastive learning frameworks. However, existing methods overlook a keysource of optimization tension: the separation between text and videodistributions in the representation space (referred to as the modality gap),and the prevalence of false negatives in batch sampling. These factors lead toconflicting gradients under the InfoNCE loss, impeding stable alignment. Tomitigate this, we propose GARE, a Gap-Aware Retrieval framework that introducesa learnable, pair-specific increment Delta_ij between text t_i and video v_j tooffload the tension from the global anchor representation. We first derive theideal form of Delta_ij via a coupled multivariate first-order Taylorapproximation of the InfoNCE loss under a trust-region constraint, revealing itas a mechanism for resolving gradient conflicts by guiding updates along alocally optimal descent direction. Due to the high cost of directly computingDelta_ij, we introduce a lightweight neural module conditioned on the semanticgap between each video-text pair, enabling structure-aware correction guided bygradient supervision. To further stabilize learning and promoteinterpretability, we regularize Delta using three components: a trust-regionconstraint to prevent oscillation, a directional diversity term to promotesemantic coverage, and an information bottleneck to limit redundancy.Experiments across four retrieval benchmarks show that GARE consistentlyimproves alignment accuracy and robustness to noisy supervision, confirming theeffectiveness of gap-aware tension mitigation.</description>
      <author>example@mail.com (Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong)</author>
      <guid isPermaLink="false">2505.12499v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design</title>
      <link>http://arxiv.org/abs/2505.12664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to IEEE Transactions on Wireless Communications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将物理知识融入基于学习的目标感知方法，利用多个基站和用户设备之间的多视角信道状态信息（CSI）进行高精度目标感知。&lt;h4&gt;背景&lt;/h4&gt;多视角感知问题可以自然地映射到条件生成框架中。&lt;h4&gt;目的&lt;/h4&gt;设计一种双部分神经网络架构，以融合多视角CSI中的潜在目标特征，并使用这些特征作为条件输入，指导目标的重构。&lt;h4&gt;方法&lt;/h4&gt;设计了一个编码器，用于捕捉CSI与目标之间的物理相关性，并适应基站-用户对的数量和位置。通过引入空间位置嵌入方案，利用电磁波传播通道的结构来模拟CSI的视角特定性质。最后，使用加权损失的条件扩散模型从融合的特征生成目标的点云。&lt;h4&gt;主要发现&lt;/h4&gt;提出的生成多视角（Gen-MV）感知框架在目标形状和电磁性质的重构质量上表现出卓越的灵活性和显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;Gen-MV感知框架在目标感知任务中具有优异的性能和适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we incorporate physical knowledge into learning-basedhigh-precision target sensing using the multi-view channel state information(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kindof multi-view sensing problem can be naturally cast into a conditionalgeneration framework. To this end, we design a bipartite neural networkarchitecture, the first part of which uses an elaborately designed encoder tofuse the latent target features embedded in the multi-view CSI, and then thesecond uses them as conditioning inputs of a powerful generative model to guidethe target's reconstruction. Specifically, the encoder is designed to capturethe physical correlation between the CSI and the target, and also be adaptiveto the numbers and positions of BS-UE pairs. Therein the view-specific natureof CSI is assimilated by introducing a spatial positional embedding scheme,which exploits the structure of electromagnetic(EM)-wave propagation channels.Finally, a conditional diffusion model with a weighted loss is employed togenerate the target's point cloud from the fused features. Extensive numericalresults demonstrate that the proposed generative multi-view (Gen-MV) sensingframework exhibits excellent flexibility and significant performanceimprovement on the reconstruction quality of target's shape and EM properties.</description>
      <author>example@mail.com (Ziqing Xing, Zhaoyang Zhang, Zirui Chen, Hongning Ruan, Zhaohui Yang)</author>
      <guid isPermaLink="false">2505.12664v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.12246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Robotics and Automation Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于SD地图增强的场景感知和拓扑推理框架（SEPT），旨在解决自动驾驶车辆在长距离或遮挡场景下，由于车载传感器限制而导致的在线场景理解局限性。&lt;h4&gt;背景&lt;/h4&gt;在线场景感知和拓扑推理对于自动驾驶车辆理解其驾驶环境至关重要，尤其是对于减少对昂贵高清地图依赖的无地图驾驶系统。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出SEPT框架，旨在有效地将SD地图作为先验知识集成到现有的感知和推理流程中。&lt;h4&gt;方法&lt;/h4&gt;SEPT框架采用了一种新的混合特征融合策略，结合SD地图和鸟瞰图（BEV）特征，同时考虑了栅格化和矢量化表示，并减轻了SD地图与BEV特征空间之间的潜在不匹配。此外，利用SD地图特征设计了一个辅助的交叉感知关键点检测任务，以增强整体场景理解性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2数据集上的实验结果表明，通过有效集成SD地图先验知识，该框架显著提高了场景感知和拓扑推理能力，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SEPT框架通过结合SD地图信息，有效地提高了自动驾驶车辆在复杂环境下的感知和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在线场景感知和拓扑推理对自动驾驶车辆理解其驾驶环境至关重要，尤其是对于试图减少对昂贵高清地图依赖的无地图驾驶系统。然而，由于车载传感器的固有限制，在线场景理解的最新进展仍存在局限性，尤其是在长距离或遮挡场景中。为了解决这一挑战，我们提出了一种标准定义（SD）地图增强场景感知和拓扑推理（SEPT）框架，该框架探讨了如何有效地将SD地图作为先验知识集成到现有的感知和推理管道中。具体来说，我们引入了一种新的混合特征融合策略，结合SD地图与鸟瞰图（BEV）特征，同时考虑了栅格化和矢量化表示，并减轻了SD地图与BEV特征空间之间的潜在不匹配。此外，我们利用SD地图特征设计了一个辅助的交叉感知关键点检测任务，进一步增强了整体场景理解性能。在大型OpenLane-V2数据集上的实验结果表明，通过有效集成SD地图先验知识，我们的框架显著提高了场景感知和拓扑推理，大幅度优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online scene perception and topology reasoning are critical for autonomousvehicles to understand their driving environments, particularly for maplessdriving systems that endeavor to reduce reliance on costly High-Definition (HD)maps. However, recent advances in online scene understanding still facelimitations, especially in long-range or occluded scenarios, due to theinherent constraints of onboard sensors. To address this challenge, we proposea Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning(SEPT) framework, which explores how to effectively incorporate the SD map asprior knowledge into existing perception and reasoning pipelines. Specifically,we introduce a novel hybrid feature fusion strategy that combines SD maps withBird's-Eye-View (BEV) features, considering both rasterized and vectorizedrepresentations, while mitigating potential misalignment between SD maps andBEV feature spaces. Additionally, we leverage the SD map characteristics todesign an auxiliary intersection-aware keypoint detection task, which furtherenhances the overall scene understanding performance. Experimental results onthe large-scale OpenLane-V2 dataset demonstrate that by effectively integratingSD map priors, our framework significantly improves both scene perception andtopology reasoning, outperforming existing methods by a substantial margin.</description>
      <author>example@mail.com (Muleilan Pei, Jiayao Shan, Peiliang Li, Jieqi Shi, Jing Huo, Yang Gao, Shaojie Shen)</author>
      <guid isPermaLink="false">2505.12246v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables</title>
      <link>http://arxiv.org/abs/2505.12473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多模态对比学习的理论特性，探讨了其学习到的表示在超越线性表示和特定数据分布之外的特性。&lt;h4&gt;背景&lt;/h4&gt;多模态对比学习作为一种自监督的表示学习方法，在基础模型训练（如CLIP）中取得了巨大成功。&lt;h4&gt;目的&lt;/h4&gt;研究多模态对比学习所学习到的表示的理论特性，尤其是超越线性表示和特定数据分布的特性。&lt;h4&gt;方法&lt;/h4&gt;通过温度优化，分析多模态对比学习在最大化模态间互信息的同时，如何适应数据的内在维度。&lt;h4&gt;主要发现&lt;/h4&gt;多模态对比学习不仅最大化了模态间的互信息，还能适应数据的内在维度，这些维度可能远低于用户指定的表示向量维度。&lt;h4&gt;结论&lt;/h4&gt;实验表明，对比学习能够学习到低维且信息丰富的表示，将理论洞察与实际性能相结合。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态对比学习作为一种自监督的表示学习方法，在基础模型训练（如CLIP）中取得了巨大成功。在本文中，我们研究了多模态对比学习所学习到的表示的理论特性，特别是在超越线性表示和特定数据分布的范畴之外。我们的分析揭示，得益于温度优化，多模态对比学习不仅最大化了模态间的互信息，还适应了数据的内在维度，这些维度可能远低于用户指定的表示向量维度。在合成数据和真实世界数据集上的实验表明，对比学习能够学习到低维且信息丰富的表示，将理论洞察与实际性能相结合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal contrastive learning as a self-supervised representation learningtechnique has achieved great success in foundation model training, such asCLIP~\citep{radford2021learning}. In this paper, we study the theoreticalproperties of the learned representations from multi-modal contrastive learningbeyond linear representations and specific data distributions. Our analysisreveals that, enabled by temperature optimization, multi-modal contrastivelearning not only maximizes mutual information between modalities but alsoadapts to intrinsic dimensions of data, which can be much lower thanuser-specified dimensions for representation vectors. Experiments on bothsynthetic and real-world datasets demonstrate the ability of contrastivelearning to learn low-dimensional and informative representations, bridgingtheoretical insights and practical performance.</description>
      <author>example@mail.com (Yu Gui, Cong Ma, Zongming Ma)</author>
      <guid isPermaLink="false">2505.12473v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding</title>
      <link>http://arxiv.org/abs/2505.12605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，大型视觉语言模型（LVLMs）在视频理解方面取得了显著进展，但它们对时间理解的贡献要素尚不明确，这可能会限制其潜力。&lt;h4&gt;背景&lt;/h4&gt;大多数LVLMs依赖其隐含的时间理解能力来处理视频理解，但它们没有揭示对时间理解能力有重要影响的要素。&lt;h4&gt;目的&lt;/h4&gt;进行一项彻底的实证研究，以揭示影响LVLMs时间理解能力的关键组件。&lt;h4&gt;方法&lt;/h4&gt;通过实证研究揭示影响LVLMs时间理解能力的关键组件，并基于这些发现提出一种面向时间理解的方案。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，对时间理解能力有显著影响的是视觉编码器和大型语言模型之间的中间接口。&lt;h4&gt;结论&lt;/h4&gt;通过提出的时间理解方案，最终模型在标准视频理解任务上显著提升了之前的LVLMs。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, there have been outstanding advances in large vision-language models (LVLMs) for video understanding. However, they have not deciphered the important components that contribute to temporal understanding ability, which might limit the potential of these LVLMs for video understanding. In this work, we conduct a thorough empirical study to demystify the crucial components that influence the temporal understanding of LVLMs. Our empirical study reveals that significant impacts are centered around the intermediate interface between the visual encoder and the large language model. Building on these insights, we propose a temporal-oriented recipe that encompasses temporal-oriented training schemes and an upscaled interface. Our final model developed using our recipe significantly enhances previous LVLMs on standard video understanding tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have witnessed outstanding advances of large vision-languagemodels (LVLMs). In order to tackle video understanding, most of them dependupon their implicit temporal understanding capacity. As such, they have notdeciphered important components that contribute to temporal understandingability, which might limit the potential of these LVLMs for videounderstanding. In this work, we conduct a thorough empirical study to demystifycrucial components that influence the temporal understanding of LVLMs. Ourempirical study reveals that significant impacts are centered around theintermediate interface between the visual encoder and the large language model.Building on these insights, we propose a temporal-oriented recipe thatencompasses temporal-oriented training schemes and an upscaled interface. Ourfinal model developed using our recipe significantly enhances previous LVLMs onstandard video understanding tasks.</description>
      <author>example@mail.com (Thong Nguyen, Zhiyuan Hu, Xu Lin, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan)</author>
      <guid isPermaLink="false">2505.12605v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling</title>
      <link>http://arxiv.org/abs/2505.12890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ORQA的新型手术室问答基准和基础多模态模型，以提升手术室智能水平。&lt;h4&gt;背景&lt;/h4&gt;手术的复杂性要求外科医生具备深厚的全面理解以确保操作的精确性、安全性和有效性。目前的工作主要集中在单一任务上，如阶段识别或场景图生成，缺乏范围和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时解决多种手术室挑战的全面基准和多模态模型。&lt;h4&gt;方法&lt;/h4&gt;将四个公开的手术室数据集统一为一个综合基准，提出了一种融合视觉、听觉和结构化数据的跨模态大型语言模型，并引入了一种新颖的渐进式知识蒸馏范式，以生成针对不同速度和内存需求的优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;ORQA在提出的基准上显示出强大的性能，实现了零样本泛化，为可扩展的、统一的手术室建模和多模态外科智能的发展铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;本文的研究成果将推动手术室智能的发展，并为手术室的多模态建模提供新的方法和工具。&lt;h4&gt;翻译&lt;/h4&gt;The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within the operating room. Prior works, limited to single-tasks efforts like phase recognition or scene graph generation, lack scope and generalizability. In this work, we introduce ORQA, a novel OR question answering benchmark and foundational multimodal model to advance OR intelligence. By unifying all four public OR datasets into a comprehensive benchmark, we enable our approach to concurrently address a diverse range of OR challenges. The proposed multimodal large language model fuses diverse OR signals such as visual, auditory, and structured data, for a holistic modeling of the OR. Finally, we propose a novel, progressive knowledge distillation paradigm, to generate a family of models optimized for different speed and memory requirements. We show the strong performance of ORQA on our proposed benchmark, and its zero-shot generalization, paving the way for scalable, unified OR modeling and significantly advancing multimodal surgical intelligence. We will release our code and data upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The real-world complexity of surgeries necessitates surgeons to have deep andholistic comprehension to ensure precision, safety, and effectiveinterventions. Computational systems are required to have a similar level ofcomprehension within the operating room. Prior works, limited to single-taskefforts like phase recognition or scene graph generation, lack scope andgeneralizability. In this work, we introduce ORQA, a novel OR questionanswering benchmark and foundational multimodal model to advance ORintelligence. By unifying all four public OR datasets into a comprehensivebenchmark, we enable our approach to concurrently address a diverse range of ORchallenges. The proposed multimodal large language model fuses diverse ORsignals such as visual, auditory, and structured data, for a holistic modelingof the OR. Finally, we propose a novel, progressive knowledge distillationparadigm, to generate a family of models optimized for different speed andmemory requirements. We show the strong performance of ORQA on our proposedbenchmark, and its zero-shot generalization, paving the way for scalable,unified OR modeling and significantly advancing multimodal surgicalintelligence. We will release our code and data upon acceptance.</description>
      <author>example@mail.com (Ege Özsoy, Chantal Pellegrini, David Bani-Harouni, Kun Yuan, Matthias Keicher, Nassir Navab)</author>
      <guid isPermaLink="false">2505.12890v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling and Steering Connectome Organization with Interpretable Latent Variables</title>
      <link>http://arxiv.org/abs/2505.13011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过结合连接组学和表示学习，提出了一种框架，用于从果蝇连接组FlyWire中提取子图，并结合生成模型来推导神经电路的低维可解释表示。该方法能够有效地重建图结构，并能够通过操纵潜在代码来可控地生成具有预定属性的连接组子图。&lt;h4&gt;背景&lt;/h4&gt;大脑的连接组（connectome）是大脑功能的蓝图，具有巨大的复杂性，但其起源却是一个紧凑的遗传代码，这表明存在低维组织原则。&lt;h4&gt;目的&lt;/h4&gt;揭示大脑连接组中的低维组织原则。&lt;h4&gt;方法&lt;/h4&gt;提出了一种框架，该框架结合了从Drosophila连接组FlyWire中提取子图的方法，以及一个生成模型来推导神经电路的低维可解释表示。此外，引入了一个可解释模块，将潜在维度与特定的结构特征联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地重建图结构，并且能够通过操纵潜在代码来可控地生成具有预定属性的连接组子图。&lt;h4&gt;结论&lt;/h4&gt;这项研究为理解大脑架构提供了一个新的工具，并可能为设计受生物启发的神经网络开辟了一条新的途径。&lt;h4&gt;翻译&lt;/h4&gt;The brain's intricate connectome, a blueprint for its function, presents immense complexity, yet it arises from a compact genetic code, hinting at underlying low-dimensional organizational principles. This work bridges connectomics and representation learning to uncover these principles. We propose a framework that combines subgraph extraction from the Drosophila connectome, FlyWire, with a generative model to derive interpretable low-dimensional representations of neural circuitry. Crucially, an explainability module links these latent dimensions to specific structural features, offering insights into their functional relevance. We validate our approach by demonstrating effective graph reconstruction and, significantly, the ability to manipulate these latent codes to controllably generate connectome subgraphs with predefined properties. This research offers a novel tool for understanding brain architecture and a potential avenue for designing bio-inspired artificial neural networks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The brain's intricate connectome, a blueprint for its function, presentsimmense complexity, yet it arises from a compact genetic code, hinting atunderlying low-dimensional organizational principles. This work bridgesconnectomics and representation learning to uncover these principles. Wepropose a framework that combines subgraph extraction from the Drosophilaconnectome, FlyWire, with a generative model to derive interpretablelow-dimensional representations of neural circuitry. Crucially, anexplainability module links these latent dimensions to specific structuralfeatures, offering insights into their functional relevance. We validate ourapproach by demonstrating effective graph reconstruction and, significantly,the ability to manipulate these latent codes to controllably generateconnectome subgraphs with predefined properties. This research offers a noveltool for understanding brain architecture and a potential avenue for designingbio-inspired artificial neural networks.</description>
      <author>example@mail.com (Yubin Li, Xingyu Liu, Guozhang Chen)</author>
      <guid isPermaLink="false">2505.13011v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind</title>
      <link>http://arxiv.org/abs/2505.12207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AgroMind，一个综合性的农业遥感基准，旨在解决现有基准在场景多样性和任务设计上的不足。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型（LMMs）在多个领域表现出强大的能力，但在农业遥感领域，缺乏全面的基准测试。&lt;h4&gt;目的&lt;/h4&gt;通过引入AgroMind，填补农业遥感领域基准测试的空白，并提供一个标准化的评估框架。&lt;h4&gt;方法&lt;/h4&gt;AgroMind涵盖了四个任务维度：空间感知、物体理解、场景理解和场景推理，包含13种任务类型。通过整合八个公开数据集和一个私有农田地块数据集，构建了一个高质量的评估集。数据预处理包括数据收集、格式标准化和标注细化。使用LMMs进行推理和生成响应。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在空间推理和细粒度识别方面存在显著的性能差距，人类表现落后于一些领先的LMMs。&lt;h4&gt;结论&lt;/h4&gt;AgroMind揭示了LMMs在领域知识方面的局限性，并指出了未来工作的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Large Multimodal Models (LMMs) have demonstrated capabilities across various domains, but comprehensive benchmarks for agricultural remote sensing (RS) remain scarce. Existing benchmarks designed for agricultural RS scenarios exhibit notable limitations, primarily in terms of insufficient scene diversity in the dataset and oversimplified task design. To bridge this gap, we introduce AgroMind, a comprehensive agricultural remote sensing benchmark covering four task dimensions: spatial perception, object understanding, scene understanding, and scene reasoning, with a total of 13 task types, ranging from crop identification and health monitoring to environmental analysis. We curate a high-quality evaluation set by integrating eight public datasets and one private farmland plot dataset, containing 25,026 QA pairs and 15,556 images. The pipeline begins with multi-source data preprocessing, including collection, format standardization, and annotation refinement. We then generate a diverse set of agriculturally relevant questions through the systematic definition of tasks. Finally, we employ LMMs for inference, generating responses, and performing detailed examinations. We evaluated 18 open-source LMMs and 3 closed-source models on AgroMind. Experiments reveal significant performance gaps, particularly in spatial reasoning and fine-grained recognition, it is notable that human performance lags behind several leading LMMs. By establishing a standardized evaluation framework for agricultural RS, AgroMind reveals the limitations of LMMs in domain knowledge and highlights critical challenges for future work. Data and code can be accessed at https://rssysu.github.io/AgroMind/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Multimodal Models (LMMs) has demonstrated capabilities across variousdomains, but comprehensive benchmarks for agricultural remote sensing (RS)remain scarce. Existing benchmarks designed for agricultural RS scenariosexhibit notable limitations, primarily in terms of insufficient scene diversityin the dataset and oversimplified task design. To bridge this gap, we introduceAgroMind, a comprehensive agricultural remote sensing benchmark covering fourtask dimensions: spatial perception, object understanding, scene understanding,and scene reasoning, with a total of 13 task types, ranging from cropidentification and health monitoring to environmental analysis. We curate ahigh-quality evaluation set by integrating eight public datasets and oneprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.The pipeline begins with multi-source data preprocessing, including collection,format standardization, and annotation refinement. We then generate a diverseset of agriculturally relevant questions through the systematic definition oftasks. Finally, we employ LMMs for inference, generating responses, andperforming detailed examinations. We evaluated 18 open-source LMMs and 3closed-source models on AgroMind. Experiments reveal significant performancegaps, particularly in spatial reasoning and fine-grained recognition, it isnotable that human performance lags behind several leading LMMs. Byestablishing a standardized evaluation framework for agricultural RS, AgroMindreveals the limitations of LMMs in domain knowledge and highlights criticalchallenges for future work. Data and code can be accessed athttps://rssysu.github.io/AgroMind/.</description>
      <author>example@mail.com (Qingmei Li, Yang Zhang, Zurong Mai, Yuhang Chen, Shuohong Lou, Henglian Huang, Jiarui Zhang, Zhiwei Zhang, Yibin Wen, Weijia Li, Haohuan Fu, Jianxi Huang, Juepeng Zheng)</author>
      <guid isPermaLink="false">2505.12207v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding</title>
      <link>http://arxiv.org/abs/2505.12408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ViEEG的生物启发式分层EEG解码框架，该框架旨在理解和解码大脑活动为视觉表示，并取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;理解大脑活动并将其解码为视觉表示是神经科学和人工智能交叉领域的一个基本挑战。虽然基于EEG的视觉解码因其非侵入性、低成本和毫秒级时间分辨率而显示出潜力，但现有方法因依赖平面的神经表示而限制了其性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的EEG解码框架，以解决现有方法中忽略大脑内在视觉层次结构的问题。&lt;h4&gt;方法&lt;/h4&gt;ViEEG将每个视觉刺激分解为三个生物对齐的组件——轮廓、前景物体和背景场景，作为三个流EEG编码器的锚点。通过跨注意力路由逐步整合EEG特征，模拟从V1到IT再到联合皮层的皮层信息流。此外，采用分层对比学习使EEG表示与CLIP嵌入对齐，实现零样本物体识别。&lt;h4&gt;主要发现&lt;/h4&gt;在THINGS-EEG数据集上的广泛实验表明，ViEEG实现了最先进的性能，主体依赖设置中Top-1准确率为40.9%，跨主体设置中Top-1准确率为22.9%，超过现有方法45%以上。&lt;h4&gt;结论&lt;/h4&gt;ViEEG不仅推动了性能前沿，还为AI中的基于生物的脑解码设定了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;Understanding and decoding brain activity into visual representations is a fundamental challenge at the intersection of neuroscience and artificial intelligence. While EEG-based visual decoding has shown promise due to its non-invasive, low-cost nature and millisecond-level temporal resolution, existing methods are limited by their reliance on flat neural representations that overlook the brain's inherent visual hierarchy. In this paper, we introduce ViEEG, a biologically inspired hierarchical EEG decoding framework that aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes each visual stimulus into three biologically aligned components - contour, foreground object, and contextual scene - serving as anchors for a three-stream EEG encoder. These EEG features are progressively integrated via cross-attention routing, simulating cortical information flow from V1 to IT to the association cortex. We further adopt hierarchical contrastive learning to align EEG representations with CLIP embeddings, enabling zero-shot object recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that ViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in subject-dependent and 22.9% Top-1 accuracy in cross-subject settings, surpassing existing methods by over 45%. Our framework not only advances the performance frontier but also sets a new paradigm for biologically grounded brain decoding in AI.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and decoding brain activity into visual representations is afundamental challenge at the intersection of neuroscience and artificialintelligence. While EEG-based visual decoding has shown promise due to itsnon-invasive, low-cost nature and millisecond-level temporal resolution,existing methods are limited by their reliance on flat neural representationsthat overlook the brain's inherent visual hierarchy. In this paper, weintroduce ViEEG, a biologically inspired hierarchical EEG decoding frameworkthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposeseach visual stimulus into three biologically aligned components-contour,foreground object, and contextual scene-serving as anchors for a three-streamEEG encoder. These EEG features are progressively integrated viacross-attention routing, simulating cortical information flow from V1 to IT tothe association cortex. We further adopt hierarchical contrastive learning toalign EEG representations with CLIP embeddings, enabling zero-shot objectrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate thatViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy insubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,surpassing existing methods by over 45%. Our framework not only advances theperformance frontier but also sets a new paradigm for biologically groundedbrain decoding in AI.</description>
      <author>example@mail.com (Minxu Liu, Donghai Guan, Chuhang Zheng, Chunwei Tian, Jie Wen, Qi Zhu)</author>
      <guid isPermaLink="false">2505.12408v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DPCD: A Quality Assessment Database for Dynamic Point Clouds</title>
      <link>http://arxiv.org/abs/2505.12431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个大规模的动态点云质量评估数据库DPCD，并评估了动态点云质量评估（DPCQA）的性能。&lt;h4&gt;背景&lt;/h4&gt;虚拟/增强现实（VR/AR）的进步推动了动态点云（DPC）的需求，DPC能够捕捉对象或场景中的时间变化，提供更真实的现实世界模拟。尽管静态点云的质量评估研究取得了显著进展，但动态点云质量评估（DPCQA）的研究很少，这阻碍了质量导向应用的发展。&lt;h4&gt;目的&lt;/h4&gt;提出一个大规模的DPCQA数据库，评估动态点云质量评估（DPCQA）的性能，为质量导向应用的发展提供支持。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含15个参考DPC和525个受损DPC的大规模DPCQA数据库DPCD，通过渲染这些样本到处理视频序列（PVS），进行了一项全面的主体实验，从21个观众那里获得了平均意见得分（MOS）进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DPCQA比静态点云质量评估更具挑战性，DPCD验证了所提出数据库的异质性和可靠性，并作为推动DPCQA新研究努力的催化剂。&lt;h4&gt;结论&lt;/h4&gt;DPCD数据库作为推动DPCQA新研究努力的催化剂，是公开可用的。&lt;h4&gt;翻译&lt;/h4&gt;最近，虚拟/增强现实（VR/AR）的进步推动了动态点云（DPC）的需求。与静态点云不同，DPCs能够捕捉对象或场景中的时间变化，提供更真实的现实世界模拟。尽管静态点云的质量评估研究取得了显著进展，但动态点云质量评估（DPCQA）的研究很少，这阻碍了质量导向应用的发展。在本文中，我们介绍了一个名为DPCD的大规模DPCQA数据库，其中包括15个参考DPC和525个来自七种类型的有损压缩和噪声失真的受损DPC。通过将这些样本渲染到处理视频序列（PVS），进行了一项全面的主体实验，从21个观众那里获得了平均意见得分（MOS）进行分析。内容特性、各种失真影响和MOS的准确性被提出，以验证所提出数据库的异质性和可靠性。此外，我们还评估了DPCD上几个客观指标的性能。实验结果表明，DPCQA比静态点云质量评估更具挑战性。作为推动DPCQA新研究努力的催化剂，DPCD数据库是公开可用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driventhe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs arecapable of capturing temporal changes within objects or scenes, offering a moreaccurate simulation of the real world. While significant progress has been madein the quality assessment research of static point cloud, little study has beendone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders thedevelopment of quality-oriented applications, such as interframe compressionand transmission in practical scenarios. In this paper, we introduce alarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and525 distorted DPCs from seven types of lossy compression and noise distortion.By rendering these samples to Processed Video Sequences (PVS), a comprehensivesubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21viewers for analysis. The characteristic of contents, impact of variousdistortions, and accuracy of MOSs are presented to validate the heterogeneityand reliability of the proposed database. Furthermore, we evaluate theperformance of several objective metrics on DPCD. The experiment results showthat DPCQA is more challenge than that of static point cloud. The DPCD, whichserves as a catalyst for new research endeavors on DPCQA, is publicly availableat https://huggingface.co/datasets/Olivialyt/DPCD.</description>
      <author>example@mail.com (Yating Liu, Yujie Zhang, Qi Yang, Yiling Xu, Zhu Li, Ye-Kui Wang)</author>
      <guid isPermaLink="false">2505.12431v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Addressing the Scarcity of Benchmarks for Graph XAI</title>
      <link>http://arxiv.org/abs/2505.12437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自动化构建图分类XAI基准的方法，旨在解决现有基准数据集不足的问题。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在结构化数据学习中变得普遍，但其决策过程对用户不透明，限制了其在安全关键应用中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来自动化构建用于图分类的XAI基准，以解决现有基准数据集不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一个通用的方法来自动化构建XAI基准，并提供了15个现成的基准和代码，以生成超过2000个额外的XAI基准。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出的方法可以有效地生成大量的XAI基准，用于评估图解释器的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为图分类的XAI研究提供了新的基准数据，有助于提高解释器评估的质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然图神经网络（GNNs）已经成为从结构化数据中学习的实际模型，但其决策过程对最终用户来说仍然不透明，这限制了它们在安全关键应用中的部署。在图分类的情况下，可解释人工智能（XAI）技术通过识别解释预测的子图基序来解决这个主要问题。然而，该领域的进步受到已知基序的基准数据集长期短缺的阻碍，以评估解释的质量。当前的图XAI基准仅限于合成数据或由领域专家手工定制的少数几个真实世界任务。在本文中，我们提出了一种通用方法来自动化从真实世界数据集中构建图分类XAI基准。我们提供了15个现成的基准，以及使用我们的方法生成2000多个额外XAI基准的代码。作为一个用例，我们使用我们的基准来评估一些流行图解释器的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Neural Networks (GNNs) have become the de facto model forlearning from structured data, their decisional process remains opaque to theend user, undermining their deployment in safety-critical applications. In thecase of graph classification, Explainable Artificial Intelligence (XAI)techniques address this major issue by identifying sub-graph motifs thatexplain predictions. However, advancements in this field are hindered by achronic scarcity of benchmark datasets with known ground-truth motifs to assessthe explanations' quality. Current graph XAI benchmarks are limited tosynthetic data or a handful of real-world tasks hand-curated by domain experts.In this paper, we propose a general method to automate the construction of XAIbenchmarks for graph classification from real-world datasets. We provide both15 ready-made benchmarks, as well as the code to generate more than 2000additional XAI benchmarks with our method. As a use case, we employ ourbenchmarks to assess the effectiveness of some popular graph explainers.</description>
      <author>example@mail.com (Michele Fontanesi, Alessio Micheli, Marco Podda, Domenico Tortorella)</author>
      <guid isPermaLink="false">2505.12437v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>InnateCoder: Learning Programmatic Options with Foundation Models</title>
      <link>http://arxiv.org/abs/2505.12508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为InnateCoder的系统，该系统能够利用基础模型中编码的人类知识，提供程序化策略，以编码形式学习‘本能技能’。InnateCoder在零样本设置下从基础模型中学习选项，并通过组合编码这些选项的程序来寻找程序化策略，旨在提高学习程序化策略的采样效率。&lt;h4&gt;背景&lt;/h4&gt;在迁移学习之外，强化学习代理需要从零开始学习，这导致学习过程缓慢，即使是解决问题所需的最明显技能也是如此。&lt;h4&gt;目的&lt;/h4&gt;提出InnateCoder系统，以利用人类知识，提高学习程序化策略的采样效率。&lt;h4&gt;方法&lt;/h4&gt;InnateCoder从基础模型中学习选项，并通过组合程序来寻找程序化策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，InnateCoder在MicroRTS和Karel the Robot上的采样效率高于不使用选项或从经验中学习的系统版本。&lt;h4&gt;结论&lt;/h4&gt;InnateCoder的方法可以有效地提高学习程序化策略的采样效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outside of transfer learning settings, reinforcement learning agents starttheir learning process from a clean slate. As a result, such agents have to gothrough a slow process to learn even the most obvious skills required to solvea problem. In this paper, we present InnateCoder, a system that leverages humanknowledge encoded in foundation models to provide programmatic policies thatencode "innate skills" in the form of temporally extended actions, or options.In contrast to existing approaches to learning options, InnateCoder learns themfrom the general human knowledge encoded in foundation models in a zero-shotsetting, and not from the knowledge the agent gains by interacting with theenvironment. Then, InnateCoder searches for a programmatic policy by combiningthe programs encoding these options into larger and more complex programs. Wehypothesized that InnateCoder's way of learning and using options could improvethe sampling efficiency of current methods for learning programmatic policies.Empirical results in MicroRTS and Karel the Robot support our hypothesis, sincethey show that InnateCoder is more sample efficient than versions of the systemthat do not use options or learn them from experience.</description>
      <author>example@mail.com (Rubens O. Moraes, Quazi Asif Sadmine, Hendrik Baier, Levi H. S. Lelis)</author>
      <guid isPermaLink="false">2505.12508v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems</title>
      <link>http://arxiv.org/abs/2505.11535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LKAlert是一种新型的监督预警系统，用于预测潜在的Lane Keeping Assist系统（LKA）风险，提高驾驶员对自动驾驶辅助系统的信任。&lt;h4&gt;背景&lt;/h4&gt;LKA系统在现实世界中经常出现不可预测的故障，这主要是因为它们是黑盒性质，限制了驾驶员的预期和信任。&lt;h4&gt;目的&lt;/h4&gt;为了在自动化辅助和有效的人类监督之间架起桥梁，LKAlert旨在提前1-3秒预测潜在的LKA风险。&lt;h4&gt;方法&lt;/h4&gt;LKAlert处理行车记录仪视频和CAN数据，结合来自并行可解释模型的代理车道分割特征作为自动引导注意力。它使用VLM进行基于视觉的语言模型的行为预测，并生成预测警报和简洁的自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;LKAlert能够以69.8%的准确率和58.6%的F1分数正确预测即将发生的LKA故障，同时生成高质量的文本解释（71.7 ROUGE-L），并且以大约2 Hz的频率高效运行。&lt;h4&gt;结论&lt;/h4&gt;LKAlert被证明是提高当前ADAS安全性和可用性的实用解决方案，并为将VLM应用于以人为中心的黑盒自动化监督提供了一个可扩展的范例。&lt;h4&gt;翻译&lt;/h4&gt;Lane Keeping Assist systems, while increasingly prevalent, often suffer from unpredictable real-world failures, largely due to their opaque, black-box nature, which limits driver anticipation and trust. To bridge the gap between automated assistance and effective human oversight, we present LKAlert, a novel supervisory alert system that leverages VLM to forecast potential LKA risk 1-3 seconds in advance. LKAlert processes dash-cam video and CAN data, integrating surrogate lane segmentation features from a parallel interpretable model as automated guiding attention. Unlike traditional binary classifiers, LKAlert issues both predictive alert and concise natural language explanation, enhancing driver situational awareness and trust. To support the development and evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark dataset designed for predictive and explainable LKA failure warnings. It contains synchronized multimodal inputs and human-authored justifications across annotated temporal windows. We further contribute a generalizable methodological framework for VLM-based black-box behavior prediction, combining surrogate feature guidance with LoRA. This framework enables VLM to reason over structured visual context without altering its vision backbone, making it broadly applicable to other complex, opaque systems requiring interpretable oversight. Empirical results correctly predict upcoming LKA failures with 69.8% accuracy and a 58.6% F1-score. The system also generates high-quality textual explanations for drivers (71.7 ROUGE-L) and operates efficiently at approximately 2 Hz, confirming its suitability for real-time, in-vehicle use. Our findings establish LKAlert as a practical solution for enhancing the safety and usability of current ADAS and offer a scalable paradigm for applying VLMs to human-centered supervision of black-box automation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane Keeping Assist systems, while increasingly prevalent, often suffer fromunpredictable real-world failures, largely due to their opaque, black-boxnature, which limits driver anticipation and trust. To bridge the gap betweenautomated assistance and effective human oversight, we present LKAlert, a novelsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3seconds in advance. LKAlert processes dash-cam video and CAN data, integratingsurrogate lane segmentation features from a parallel interpretable model asautomated guiding attention. Unlike traditional binary classifiers, LKAlertissues both predictive alert and concise natural language explanation,enhancing driver situational awareness and trust. To support the developmentand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmarkdataset designed for predictive and explainable LKA failure warnings. Itcontains synchronized multimodal inputs and human-authored justificationsacross annotated temporal windows. We further contribute a generalizablemethodological framework for VLM-based black-box behavior prediction, combiningsurrogate feature guidance with LoRA. This framework enables VLM to reason overstructured visual context without altering its vision backbone, making itbroadly applicable to other complex, opaque systems requiring interpretableoversight. Empirical results correctly predicts upcoming LKA failures with69.8% accuracy and a 58.6\% F1-score. The system also generates high-qualitytextual explanations for drivers (71.7 ROUGE-L) and operates efficiently atapproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.Our findings establish LKAlert as a practical solution for enhancing the safetyand usability of current ADAS and offer a scalable paradigm for applying VLMsto human-centered supervision of black-box automation.</description>
      <author>example@mail.com (Yuhang Wang, Hao Zhou)</author>
      <guid isPermaLink="false">2505.11535v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</title>
      <link>http://arxiv.org/abs/2505.12903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SFTrack的新型Slow-Fast跟踪范式，该范式通过灵活适应不同的操作需求，支持高精度慢速跟踪器和高效快速跟踪器，以解决传统基于帧的跟踪算法在低延迟性能和资源受限环境中的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的跟踪算法通常依赖于低帧率的RGB相机和计算密集型的深度神经网络架构，但这类方法在低延迟性能和资源受限环境中存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跟踪方法，以解决现有跟踪算法在低延迟和资源受限环境中的不足。&lt;h4&gt;方法&lt;/h4&gt;SFTrack框架首先从高时间分辨率的事件流中进行基于图的表示学习，然后将学习到的图结构信息集成到两个基于FlashAttention的视觉骨干网络中，分别生成慢速和快速跟踪器。快速跟踪器通过轻量级网络设计和单次前向传递生成多个边界框输出以实现低延迟。最后，通过监督微调和知识蒸馏策略，将两个跟踪器无缝结合并提升快速跟踪器的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试（如FE240、COESOT和EventVOT）上的实验表明，所提出的方法在不同真实场景中具有有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;SFTrack方法在低延迟和资源受限环境中表现出色，为视觉对象跟踪提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/event-ahu/slowfast_event_track&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing tracking algorithms typically rely on low-frame-rate RGB camerascoupled with computationally intensive deep neural network architectures toachieve effective tracking. However, such frame-based methods inherently facechallenges in achieving low-latency performance and often fail inresource-constrained environments. Visual object tracking using bio-inspiredevent cameras has emerged as a promising research direction in recent years,offering distinct advantages for low-latency applications. In this paper, wepropose a novel Slow-Fast Tracking paradigm that flexibly adapts to differentoperational requirements, termed SFTrack. The proposed framework supports twocomplementary modes, i.e., a high-precision slow tracker for scenarios withsufficient computational resources, and an efficient fast tracker tailored forlatency-aware, resource-constrained environments. Specifically, our frameworkfirst performs graph-based representation learning fromhigh-temporal-resolution event streams, and then integrates the learnedgraph-structured information into two FlashAttention-based vision backbones,yielding the slow and fast trackers, respectively. The fast tracker achieveslow latency through a lightweight network design and by producing multiplebounding box outputs in a single forward pass. Finally, we seamlessly combineboth trackers via supervised fine-tuning and further enhance the fast tracker'sperformance through a knowledge distillation strategy. Extensive experiments onpublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate theeffectiveness and efficiency of our proposed method across different real-worldscenarios. The source code has been released onhttps://github.com/Event-AHU/SlowFast_Event_Track.</description>
      <author>example@mail.com (Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo)</author>
      <guid isPermaLink="false">2505.12903v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting</title>
      <link>http://arxiv.org/abs/2505.12738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的框架EpiLLM，用于时空流行病预测。&lt;h4&gt;背景&lt;/h4&gt;高级流行病预测对实现精准防控策略至关重要，对于公共卫生安全具有战略意义。尽管大型语言模型（LLMs）在特定领域任务中作为基础模型已显示出有效性，但其在流行病预测方面的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出EpiLLM框架，以实现时空流行病预测。&lt;h4&gt;方法&lt;/h4&gt;考虑现实世界流行病传播的关键因素（感染病例和人类流动性），引入双分支架构以实现LLM对复杂流行病模式和语言标记的精细粒度标记级对齐。提出自回归建模范式，将流行病预测任务重新定义为下一标记预测。引入时空提示学习技术，从数据驱动角度增强LLM对流行病的感知。&lt;h4&gt;主要发现&lt;/h4&gt;EpiLLM在真实世界COVID-19数据集上显著优于现有基线，并表现出LLMs的典型扩展行为。&lt;h4&gt;结论&lt;/h4&gt;EpiLLM是一种有效的时空流行病预测框架，为公共卫生安全提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced epidemic forecasting is critical for enabling precision containmentstrategies, highlighting its strategic importance for public health security.While recent advances in Large Language Models (LLMs) have demonstratedeffectiveness as foundation models for domain-specific tasks, their potentialfor epidemic forecasting remains largely unexplored. In this paper, weintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporalepidemic forecasting. Considering the key factors in real-world epidemictransmission: infection cases and human mobility, we introduce a dual-brancharchitecture to achieve fine-grained token-level alignment between such complexepidemic patterns and language tokens for LLM adaptation. To unleash themulti-step forecasting and generalization potential of LLM architectures, wepropose an autoregressive modeling paradigm that reformulates the epidemicforecasting task into next-token prediction. To further enhance LLM perceptionof epidemics, we introduce spatio-temporal prompt learning techniques, whichstrengthen forecasting capabilities from a data-driven perspective. Extensiveexperiments show that EpiLLM significantly outperforms existing baselines onreal-world COVID-19 datasets and exhibits scaling behavior characteristic ofLLMs.</description>
      <author>example@mail.com (Chenghua Gong, Rui Sun, Yuhao Zheng, Juyuan Zhang, Tianjun Gu, Liming Pan, Linyuan Lv)</author>
      <guid isPermaLink="false">2505.12738v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding</title>
      <link>http://arxiv.org/abs/2505.12194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Multimodal large language models (MLLMs)在处理文本和视觉输入方面的能力，并提出了一种新的方法来解决在线数据稀缺时的专业化任务问题。&lt;h4&gt;背景&lt;/h4&gt;MLLMs在通用任务如场景理解和问答方面表现良好，但在数据稀缺的专业化任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决MLLMs在数据稀缺的专业化任务中的不足，本文提出了SUN-Spot v2.0数据集和Spatial-LLaVA模型。&lt;h4&gt;方法&lt;/h4&gt;SUN-Spot v2.0数据集包含90k个图像-字幕对以及地标对象的额外注释。每个图像-字幕对使用Set-of-Marks提示作为额外指示，将图像中的地标对象映射到字幕中提到的相应对象。Spatial-LLaVA是一个在SUNSpot v2.0数据集上训练的MLLM，用于学习空间指称表达式。&lt;h4&gt;主要发现&lt;/h4&gt;Spatial-LLaVA在零样本视觉空间推理基准数据集上比以前的方法提高了3.15%的性能。&lt;h4&gt;结论&lt;/h4&gt;Spatial-LLaVA特别适用于需要精确物体识别的实际场景任务，如自主导航和交互式机器人。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍的多模态大型语言模型（MLLMs）在理解和处理文本以及视觉输入方面表现出色。通常，这些模型是在互联网上收集的大量数据集上训练的，足以处理场景理解、问答等通用任务。然而，它们在在线数据稀缺的专业化任务中表现不佳，例如确定物体之间的空间关系或在一个具有相似特征的物体组中定位独特的目标物体。为了应对这一挑战，我们引入了SUN-Spot v2.0数据集，现在包含总计90k个图像-字幕对以及地标对象的额外注释。每个图像-字幕对使用Set-of-Marks提示作为额外指示，将图像中的地标对象映射到字幕中提到的相应对象。此外，我们提出了Spatial-LLaVA，这是一个在SUNSpot v2.0数据集上使用最先进的语言模型生成的对话数据训练的MLLM。我们的方法确保了图像中的物体与其在字幕中对应的物体提及之间的稳健对齐，使我们的模型能够学习不受物体语义信息偏差的空间指称表达式。Spatial-LLaVA在零样本视觉空间推理基准数据集上优于以前的方法3.15%。Spatial-LLaVA专门设计用于精确理解空间指称表达式，因此它在需要精确物体识别的实际场景任务（如自主导航和交互式机器人）中非常有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have demonstrated remarkableabilities in comprehending visual input alongside text input. Typically, thesemodels are trained on extensive data sourced from the internet, which aresufficient for general tasks such as scene understanding and questionanswering. However, they often underperform on specialized tasks where onlinedata is scarce, such as determining spatial relationships between objects orlocalizing unique target objects within a group of objects sharing similarfeatures. In response to this challenge, we introduce the SUN-Spot v2.0dataset1, now comprising a total of 90k image-caption pairs and additionalannotations on the landmark objects. Each image-caption pair utilizesSet-of-Marks prompting as an additional indicator, mapping each landmark objectin the image to the corresponding object mentioned in the caption. Furthermore,we present Spatial-LLaVA, an MLLM trained on conversational data generated by astate-of-the-art language model using the SUNSpot v2.0 dataset. Our approachensures a robust alignment between the objects in the images and theircorresponding object mentions in the captions, enabling our model to learnspatial referring expressions without bias from the semantic information of theobjects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shotVisual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specificallydesigned to precisely understand spatial referring expressions, making ithighly applicable for tasks in real-world scenarios such as autonomousnavigation and interactive robotics, where precise object recognition iscritical.</description>
      <author>example@mail.com (Xuefei Sun, Doncey Albin, Cecilia Mauceri, Dusty Woods, Christoffer Heckman)</author>
      <guid isPermaLink="false">2505.12194v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization</title>
      <link>http://arxiv.org/abs/2505.12396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LGHRec的推荐系统框架，该框架通过利用大型语言模型（LLM）的Chain-of-Thought（CoT）推理能力来增强图神经网络（GNN）的推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现有的基于图的推荐系统依赖于稀疏的ID特征，未能充分利用文本信息，导致表示中的信息密度较低。此外，图对比学习面临挑战，包括随机负样本采样可能引入错误负样本，以及固定的温度系数无法适应不同节点的异质性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，本文旨在通过结合LLM的CoT推理能力和改进的对比学习策略来提升推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;LGHRec框架利用LLM的CoT推理能力生成语义ID，从而丰富推理过程并提高表示的信息密度和语义质量。此外，设计了一种名为Harmonized Group Policy Optimization（HGPO）的强化学习算法，用于优化对比学习中的负样本采样策略和温度系数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LGHRec通过LLM的CoT推理生成的语义ID提高了表示质量，并有效地通过HGPO提升了对比学习。该方法在多个基准模型中表现优异。&lt;h4&gt;结论&lt;/h4&gt;LGHRec通过结合LLM的CoT推理能力和HGPO算法，显著提高了推荐系统的性能和信息密度，为基于图的推荐系统提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have advanced recommender systems by modeling interaction relationships. However, existing graph-based recommenders rely on sparse ID features and do not fully exploit textual information, resulting in low information density within representations. Furthermore, graph contrastive learning faces challenges. Random negative sampling can introduce false negative samples, while fixed temperature coefficients cannot adapt to the heterogeneity of different nodes. In addition, current efforts to enhance recommendations with large language models (LLMs) have not fully utilized their Chain-of-Thought (CoT) reasoning capabilities to guide representation learning. To address these limitations, we introduce LGHRec (LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization). This framework leverages the CoT reasoning ability of LLMs to generate semantic IDs, enriching reasoning processes and improving information density and semantic quality of representations. Moreover, we design a reinforcement learning algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative sampling strategies and temperature coefficients in contrastive learning. This approach enhances long-tail recommendation performance and ensures optimization consistency across different groups. Experimental results on three datasets demonstrate that LGHRec improves representation quality through semantic IDs generated by LLM's CoT reasoning and effectively boosts contrastive learning with HGPO. Our method outperforms several baseline models. The code is available at: https://anonymous.4open.science/r/LLM-Rec.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have advanced recommender systems by modelinginteraction relationships. However, existing graph-based recommenders rely onsparse ID features and do not fully exploit textual information, resulting inlow information density within representations. Furthermore, graph contrastivelearning faces challenges. Random negative sampling can introduce falsenegative samples, while fixed temperature coefficients cannot adapt to theheterogeneity of different nodes. In addition, current efforts to enhancerecommendations with large language models (LLMs) have not fully utilized theirChain-of-Thought (CoT) reasoning capabilities to guide representation learning.To address these limitations, we introduces LGHRec (LLM-CoT Enhanced GraphNeural Recommendation with Harmonized Group Policy Optimization). Thisframework leverages the CoT reasoning ability of LLMs to generate semantic IDs,enriching reasoning processes and improving information density and semanticquality of representations. Moreover, we design a reinforcement learningalgorithm, Harmonized Group Policy Optimization (HGPO), to optimize negativesampling strategies and temperature coefficients in contrastive learning. Thisapproach enhances long-tail recommendation performance and ensures optimizationconsistency across different groups. Experimental results on three datasetsdemonstrate that LGHRec improves representation quality through semantic IDsgenerated by LLM's CoT reasoning and effectively boosts contrastive learningwith HGPO. Our method outperforms several baseline models. The code isavailable at: https://anonymous.4open.science/r/LLM-Rec.</description>
      <author>example@mail.com (Hailong Luo, Bin Wu, Hongyong Jia, Qingqing Zhu, Lianlei Shan)</author>
      <guid isPermaLink="false">2505.12396v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations</title>
      <link>http://arxiv.org/abs/2505.12237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次系统地研究了大型语言模型（LLMs）在视频编辑中的应用，提出了一种名为L-Storyboard的中间表示方法，用于将视频镜头转换为适合LLMs处理的结构化语言描述。此外，还提出了StoryFlow策略，以解决发散任务输出的不稳定性，并提高了视频编辑任务的解释性和隐私保护。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）和视觉-语言模型（VLMs）在视频理解方面表现出色，但在视频编辑中的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在视频编辑中的应用，并提高视频编辑任务的解释性和隐私保护。&lt;h4&gt;方法&lt;/h4&gt;引入L-Storyboard中间表示方法，将视频镜头转换为结构化语言描述；将视频编辑任务分为收敛任务和发散任务，并针对三个核心任务进行研究；提出StoryFlow策略以解决发散任务输出的不稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;L-Storyboard有助于将视觉信息与语言描述之间建立更稳健的映射；StoryFlow策略提高了发散任务输出的逻辑一致性和输出稳定性。&lt;h4&gt;结论&lt;/h4&gt;LLMs在智能视频编辑中具有巨大的潜力，L-Storyboard和StoryFlow策略可以显著提高视频编辑任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;This paper presents the first systematic study of LLMs in the context of video editing. To bridge the gap between visual information and language-based reasoning, we introduce L-Storyboard, an intermediate representation that transforms discrete video shots into structured language descriptions suitable for LLM processing. We categorize video editing tasks into Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot Attributes Classification, Next Shot Selection, and Shot Sequence Ordering. To address the inherent instability of divergent task outputs, we propose the StoryFlow strategy, which converts the divergent multi-path reasoning process into a convergent selection mechanism, effectively enhancing task accuracy and logical coherence. Experimental results demonstrate that L-Storyboard facilitates a more robust mapping between visual information and language descriptions, significantly improving the interpretability and privacy protection of video editing tasks. Furthermore, StoryFlow enhances the logical consistency and output stability in Shot Sequence Ordering, underscoring the substantial potential of LLMs in intelligent video editing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) and Vision-Language Models (VLMs) havedemonstrated remarkable reasoning and generalization capabilities in videounderstanding; however, their application in video editing remains largelyunderexplored. This paper presents the first systematic study of LLMs in thecontext of video editing. To bridge the gap between visual information andlanguage-based reasoning, we introduce L-Storyboard, an intermediaterepresentation that transforms discrete video shots into structured languagedescriptions suitable for LLM processing. We categorize video editing tasksinto Convergent Tasks and Divergent Tasks, focusing on three core tasks: ShotAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. Toaddress the inherent instability of divergent task outputs, we propose theStoryFlow strategy, which converts the divergent multi-path reasoning processinto a convergent selection mechanism, effectively enhancing task accuracy andlogical coherence. Experimental results demonstrate that L-Storyboardfacilitates a more robust mapping between visual information and languagedescriptions, significantly improving the interpretability and privacyprotection of video editing tasks. Furthermore, StoryFlow enhances the logicalconsistency and output stability in Shot Sequence Ordering, underscoring thesubstantial potential of LLMs in intelligent video editing.</description>
      <author>example@mail.com (Yuzhi Li, Haojun Xu, Fang Tian)</author>
      <guid isPermaLink="false">2505.12237v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations</title>
      <link>http://arxiv.org/abs/2505.12310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages,10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为DNOI-4DRO的新型学习-优化-结合四维雷达里程计模型，该模型通过创新的可微分神经网络优化迭代算子，将传统几何优化与端到端神经网络训练无缝结合。&lt;h4&gt;背景&lt;/h4&gt;在四维雷达里程计领域，需要提高雷达点云的表示能力，并提升定位精度。&lt;h4&gt;目的&lt;/h4&gt;提出DNOI-4DRO模型，旨在提升四维雷达里程计的性能。&lt;h4&gt;方法&lt;/h4&gt;模型首先使用神经网络估计点运动流，然后基于点运动与3D空间中姿态的关系构建成本函数，并使用高斯-牛顿更新来优化雷达姿态。此外，设计了一个双流四维雷达骨干网络，该网络集成了多尺度几何特征和基于聚类的类感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和Snail-Radar数据集上进行的实验表明，DNOI-4DRO模型的表现优于最近的一些经典和学习方法，甚至在使用激光雷达点云进行映射优化的情况下，其结果与A-LOAM相当。&lt;h4&gt;结论&lt;/h4&gt;DNOI-4DRO模型能够显著提高四维雷达里程计的性能，并且模型和代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;A novel learning-optimization-combined 4D radar odometry model, named DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates traditional geometric optimization with end-to-end neural network training, leveraging an innovative differentiable neural-optimization iteration operator. In this framework, point-wise motion flow is first estimated using a neural network, followed by the construction of a cost function based on the relationship between point motion and pose in 3D space. The radar pose is then refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D radar backbone that integrates multi-scale geometric features and clustering-based class-aware features to enhance the representation of sparse 4D radar point clouds. Extensive experiments on the VoD and Snail-Radar datasets demonstrate the superior performance of our model, which outperforms recent classical and learning-based approaches. Notably, our method even achieves results comparable to A-LOAM with mapping optimization using LiDAR point clouds as input. Our models and code will be publicly released.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A novel learning-optimization-combined 4D radar odometry model, namedDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integratestraditional geometric optimization with end-to-end neural network training,leveraging an innovative differentiable neural-optimization iteration operator.In this framework, point-wise motion flow is first estimated using a neuralnetwork, followed by the construction of a cost function based on therelationship between point motion and pose in 3D space. The radar pose is thenrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4Dradar backbone that integrates multi-scale geometric features andclustering-based class-aware features to enhance the representation of sparse4D radar point clouds. Extensive experiments on the VoD and Snail-Radardatasets demonstrate the superior performance of our model, which outperformsrecent classical and learning-based approaches. Notably, our method evenachieves results comparable to A-LOAM with mapping optimization using LiDARpoint clouds as input. Our models and code will be publicly released.</description>
      <author>example@mail.com (Shouyi Lu, Huanyu Zhou, Guirong Zhuo)</author>
      <guid isPermaLink="false">2505.12310v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph</title>
      <link>http://arxiv.org/abs/2505.12411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个通过重连异质图来提高图神经网络性能的方法，并证明了重连后图的同质性对节点分类性能的影响。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在分析图结构数据方面表现出色，但在异质图上表现不佳，因为连接的节点通常属于不同的类别。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供理论基础，并设计一种提高异质图同质性的重连框架，以改善图神经网络在节点分类任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;我们首先建立了边同质性与GNN嵌入平滑性和节点分类性能之间的联系，然后提出了一种使用参考图来增加图同质性的重连框架，并提出了一种从节点特征和训练标签构建同质参考图的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验，我们分析了原始图和参考图的同质性能如何影响重连图的同质性和下游GNN性能。&lt;h4&gt;结论&lt;/h4&gt;在11个真实世界的异质图数据集上评估我们的方法，结果显示它优于现有的重连技术和针对异质图的专用GNN，实现了更高的节点分类精度，同时保持了高效性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) excel at analyzing graph-structured data but struggle on heterophilic graphs, where connected nodes often belong to different classes. While this challenge is commonly addressed with specialized GNN architectures, graph rewiring remains an underexplored strategy in this context. We provide theoretical foundations linking edge homophily, GNN embedding smoothness, and node classification performance, motivating the need to enhance homophily. Building on this insight, we introduce a rewiring framework that increases graph homophily using a reference graph, with theoretical guarantees on the homophily of the rewired graph. To broaden applicability, we propose a label-driven diffusion approach for constructing a homophilic reference graph from node features and training labels. Through extensive simulations, we analyze how the homophily of both the original and reference graphs influences the rewired graph homophily and downstream GNN performance. We evaluate our method on 11 real-world heterophilic datasets and show that it outperforms existing rewiring techniques and specialized GNNs for heterophilic graphs, achieving improved node classification accuracy while remaining efficient and scalable to large graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) excel at analyzing graph-structured data butstruggle on heterophilic graphs, where connected nodes often belong todifferent classes. While this challenge is commonly addressed with specializedGNN architectures, graph rewiring remains an underexplored strategy in thiscontext. We provide theoretical foundations linking edge homophily, GNNembedding smoothness, and node classification performance, motivating the needto enhance homophily. Building on this insight, we introduce a rewiringframework that increases graph homophily using a reference graph, withtheoretical guarantees on the homophily of the rewired graph. To broadenapplicability, we propose a label-driven diffusion approach for constructing ahomophilic reference graph from node features and training labels. Throughextensive simulations, we analyze how the homophily of both the original andreference graphs influences the rewired graph homophily and downstream GNNperformance. We evaluate our method on 11 real-world heterophilic datasets andshow that it outperforms existing rewiring techniques and specialized GNNs forheterophilic graphs, achieving improved node classification accuracy whileremaining efficient and scalable to large graphs.</description>
      <author>example@mail.com (Harel Mendelman, Haggai Maron, Ronen Talmon)</author>
      <guid isPermaLink="false">2505.12411v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Causality-Inspired Robustness for Nonlinear Models via Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了分布鲁棒性在预测算法中的重要性，提出了一种在因果框架下的非线性方法，结合可识别表示学习的新进展，在非线性设置下建立了分布鲁棒性保证。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界数据中普遍存在分布偏移，分布鲁棒性成为预测算法的核心目标。&lt;h4&gt;目的&lt;/h4&gt;最小化预测模型在不确定性集（一类分布）中的最坏情况风险。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了可识别表示学习，在因果框架下建立了非线性设置下的分布鲁棒性保证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在非线性设置下提供了有限的鲁棒性保证，这是因果启发式鲁棒性方法首次在非线性设置中实现。&lt;h4&gt;结论&lt;/h4&gt;通过合成数据和真实世界单细胞数据的实证验证，证明了有限半径鲁棒性在分布鲁棒性中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;分布鲁棒性是预测算法的核心目标，因为现实世界数据普遍存在分布偏移。预测模型旨在最小化一类分布（即不确定性集）中的最坏情况风险。因果性提供了一个建模框架，在该框架下，不确定性集是数据驱动的，而不是像传统分布鲁棒性优化那样预先指定的。然而，当前的因果启发式鲁棒性方法仅在线性设置中具有有限的鲁棒性保证，其中协变量和响应之间的因果关系是线性的。在这项工作中，我们通过结合可识别表示学习的新进展，在因果框架下提出了一种非线性方法，并建立了分布鲁棒性保证。据我们所知，这是第一个在非线性设置下具有这种有限鲁棒性保证的因果启发式鲁棒性方法。对理论发现进行了合成数据和真实世界单细胞数据的实证验证，同时也说明了有限半径鲁棒性是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributional robustness is a central goal of prediction algorithms due tothe prevalent distribution shifts in real-world data. The prediction model aimsto minimize the worst-case risk among a class of distributions, a.k.a., anuncertainty set. Causality provides a modeling framework with a rigorousrobustness guarantee in the above sense, where the uncertainty set isdata-driven rather than pre-specified as in traditional distributionalrobustness optimization. However, current causality-inspired robustness methodspossess finite-radius robustness guarantees only in the linear settings, wherethe causal relationships among the covariates and the response are linear. Inthis work, we propose a nonlinear method under a causal framework byincorporating recent developments in identifiable representation learning andestablish a distributional robustness guarantee. To our best knowledge, this isthe first causality-inspired robustness method with such a finite-radiusrobustness guarantee in nonlinear settings. Empirical validation of thetheoretical findings is conducted on both synthetic data and real-worldsingle-cell data, also illustrating that finite-radius robustness is crucial.</description>
      <author>example@mail.com (Marin Šola, Peter Bühlmann, Xinwei Shen)</author>
      <guid isPermaLink="false">2505.12868v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
      <link>http://arxiv.org/abs/2505.12684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的去中心化图基础模型（GFM）训练范式FedGFM+，以解决知识纠缠问题，并通过实验验证了其在多个领域的基准测试中的优越性。&lt;h4&gt;背景&lt;/h4&gt;图机器学习近年来转向以数据为中心的范式，其中联邦图学习（FGL）和图基础模型（GFM）是两个新兴领域。FGL虽然支持多客户端协作，但面临数据和工作异质性的挑战；GFM则通常在单机上训练，无法利用跨部门的资源和数据。&lt;h4&gt;目的&lt;/h4&gt;提出FedGFM+框架，旨在减少知识纠缠，提高模型在不同领域的适应能力。&lt;h4&gt;方法&lt;/h4&gt;FedGFM+包括两个核心模块：(1) AncDAI：基于全局锚点的领域感知初始化策略，通过将局部图编码为领域特定的原型，并在这些原型周围初始化全局模型；(2) AdaDPP：局部自适应领域敏感提示池，在预训练期间学习轻量级图提示，并在微调期间选择相关提示来增强目标图属性。&lt;h4&gt;主要发现&lt;/h4&gt;FedGFM+在8个跨多个领域和任务的基准测试中表现优于20个基线方法。&lt;h4&gt;结论&lt;/h4&gt;FedGFM+通过减少知识纠缠，显著提高了图基础模型的跨领域泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in graph machine learning have shifted to data-centricparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)enables multi-client collaboration but faces challenges from data and taskheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)offer strong domain generalization but are usually trained on single machines,missing out on cross-silo data and resources.  These paradigms are complementary, and their integration brings notablebenefits. Motivated by this, we propose FedGFM, a novel decentralized GFMtraining paradigm. However, a key challenge is knowledge entanglement, wheremulti-domain knowledge merges into indistinguishable representations, hinderingdownstream adaptation.  To address this, we present FedGFM+, an enhanced framework with two coremodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-baseddomain-aware initialization strategy. Before pre-training, each client encodesits local graph into domain-specific prototypes that serve as semantic anchors.Synthetic embeddings around these anchors initialize the global model. Wetheoretically prove these prototypes are distinguishable across domains,providing a strong inductive bias to disentangle domain-specific knowledge. (2)AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns alightweight graph prompt capturing domain semantics during pre-training. Duringfine-tuning, prompts from all clients form a pool from which the GFM selectsrelevant prompts to augment target graph attributes, improving downstreamadaptation.  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains andtasks, outperforming 20 baselines from supervised learning, FGL, and federatedGFM variants.</description>
      <author>example@mail.com (Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu)</author>
      <guid isPermaLink="false">2505.12684v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>TinyRS-R1: Compact Multimodal Language Model for Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.12099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to BMVC 2025. Code, models, and the captions for datasets  will be released&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TinyRS，一个针对遥感任务优化的2B参数多模态小型语言模型（MSLM），以及其推理增强变体TinyRS-R1。TinyRS在多个遥感任务中达到或超过了7B参数模型的性能，同时内存和延迟需求仅为后者的三分之一。&lt;h4&gt;背景&lt;/h4&gt;遥感应用通常运行在无法承载当前7B参数多模态语言模型的边缘硬件上。&lt;h4&gt;目的&lt;/h4&gt;提出TinyRS和TinyRS-R1，旨在为遥感任务提供高效、低资源消耗的多模态语言模型。&lt;h4&gt;方法&lt;/h4&gt;TinyRS基于Qwen2-VL-2B模型，通过四个阶段的流水线进行训练：在百万卫星图像上进行预训练，在视觉指令示例上进行指令调整，使用推理数据集的Chain-of-Thought（CoT）注释进行微调，并通过Group Relative Policy Optimization（GRPO）进行对齐。&lt;h4&gt;主要发现&lt;/h4&gt;TinyRS-R1在分类、视觉问答（VQA）、视觉基础和开放式问答等任务中，性能达到或超过了7B参数的遥感模型。CoT推理显著提高了空间定位和场景理解能力，而无需推理的TinyRS在简洁、延迟敏感的VQA任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;TinyRS-R1是第一个具有GRPO对齐CoT推理的领域专用MSLM，适用于通用遥感。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces TinyRS, the first 2B-parameter multimodal small language model (MSLM) optimized for remote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built upon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training on million satellite images, instruction tuning on visual instruction examples, fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning dataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1 achieves or surpasses the performance of recent 7B-parameter remote sensing models across classification, VQA, visual grounding, and open-ended question answering-while requiring just one-third of the memory and latency. Our analysis shows that CoT reasoning substantially benefits spatial grounding and scene understanding, while the non-reasoning TinyRS excels in concise, latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized MSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote-sensing applications often run on edge hardware that cannot hosttoday's 7B-parameter multimodal language models. This paper introduces TinyRS,the first 2B-parameter multimodal small language model (MSLM) optimized forremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Builtupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-trainingon million satellite images, instruction tuning on visual instruction examples,fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoningdataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1achieves or surpasses the performance of recent 7B-parameter remote sensingmodels across classification, VQA, visual grounding, and open-ended questionanswering-while requiring just one-third of the memory and latency. Ouranalysis shows that CoT reasoning substantially benefits spatial grounding andscene understanding, while the non-reasoning TinyRS excels in concise,latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specializedMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.</description>
      <author>example@mail.com (Aybora Koksal, A. Aydin Alatan)</author>
      <guid isPermaLink="false">2505.12099v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Not All Documents Are What You Need for Extracting Instruction Tuning Data</title>
      <link>http://arxiv.org/abs/2505.12250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EQUAL的数据提取框架，用于从包含丰富和多样化知识的网络语料库中提取指令微调数据，以解决当前指令数据多样性和适用性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;指令微调能提升大型语言模型（LLMs）的性能，但其依赖于高质量的训练数据。现有的LLMs合成指令数据方法存在多样性和适用性不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的数据提取框架，以减少计算成本并提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;EQUAL框架首先基于对比学习得到的嵌入对文档语料库进行聚类，然后采用多臂老虎机策略高效识别可能含有有价值问答对（QA）对的聚类，通过迭代交替进行文档选择和高质量的QA对提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EQUAL在AutoMathText和StackOverflow上的四个下游任务中，相比LLaMA-3.1-8B和Mistral-7B，降低了5-10倍的计算成本，并提高了2.5%的准确性。&lt;h4&gt;结论&lt;/h4&gt;EQUAL框架能有效减少指令微调的数据提取成本，并提升模型性能，在现实世界场景中具有潜在应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Instruction tuning improves the performance of large language models (LLMs),but it heavily relies on high-quality training data. Recently, LLMs have beenused to synthesize instruction data using seed question-answer (QA) pairs.However, these synthesized instructions often lack diversity and tend to besimilar to the input seeds, limiting their applicability in real-worldscenarios. To address this, we propose extracting instruction tuning data fromweb corpora that contain rich and diverse knowledge. A naive solution is toretrieve domain-specific documents and extract all QA pairs from them, but thisfaces two key challenges: (1) extracting all QA pairs using LLMs isprohibitively expensive, and (2) many extracted QA pairs may be irrelevant tothe downstream tasks, potentially degrading model performance. To tackle theseissues, we introduce EQUAL, an effective and scalable data extraction frameworkthat iteratively alternates between document selection and high-quality QA pairextraction to enhance instruction tuning. EQUAL first clusters the documentcorpus based on embeddings derived from contrastive learning, then uses amulti-armed bandit strategy to efficiently identify clusters that are likely tocontain valuable QA pairs. This iterative approach significantly reducescomputational cost while boosting model performance. Experiments onAutoMathText and StackOverflow across four downstream tasks show that EQUALreduces computational costs by 5-10x and improves accuracy by 2.5 percent onLLaMA-3.1-8B and Mistral-7B</description>
      <author>example@mail.com (Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao)</author>
      <guid isPermaLink="false">2505.12250v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration</title>
      <link>http://arxiv.org/abs/2505.11895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了统一多模态编码器在对抗攻击下的鲁棒性问题，提出了一种对抗校准框架来提高模型在不同模态下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的统一多模态编码器在跨模态任务中表现出色，但其对抗鲁棒性在安全敏感应用中仍是一个重要问题。&lt;h4&gt;目的&lt;/h4&gt;对统一多模态编码器的对抗鲁棒性进行全面研究，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种对抗校准框架，通过训练特定于模态的投影头，提高模型对不同模态数据的鲁棒性，同时保持预训练编码器和语义中心不变。&lt;h4&gt;主要发现&lt;/h4&gt;轻微的对抗扰动会导致所有模态的性能显著下降，尤其是非视觉输入如音频和点云，视觉输入如图像和视频也显著退化。&lt;h4&gt;结论&lt;/h4&gt;该方法在ε=4/255的情况下提高了47.3%的对抗鲁棒性，同时保持了或提高了清洁零样本和检索性能，且所需的训练参数不到1%。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we studied the adversarial robustness of unified multi-modal encoders and proposed an adversarial calibration framework to improve the robustness of models across different modalities.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent unified multi-modal encoders align a wide range of modalities into ashared representation space, enabling diverse cross-modal tasks. Despite theirimpressive capabilities, the robustness of these models under adversarialperturbations remains underexplored, which is a critical concern forsafety-sensitive applications. In this work, we present the first comprehensivestudy of adversarial vulnerability in unified multi-modal encoders. We findthat even mild adversarial perturbations lead to substantial performance dropsacross all modalities. Non-visual inputs, such as audio and point clouds, areespecially fragile, while visual inputs like images and videos also degradesignificantly. To address this, we propose an efficient adversarial calibrationframework that improves robustness across modalities without modifyingpretrained encoders or semantic centers, ensuring compatibility with existingfoundation models. Our method introduces modality-specific projection headstrained solely on adversarial examples, while keeping the backbone andembeddings frozen. We explore three training objectives: fixed-centercross-entropy, clean-to-adversarial L2 alignment, and clean-adversarialInfoNCE, and we introduce a regularization strategy to ensuremodality-consistent alignment under attack. Experiments on six modalities andthree Bind-style models show that our method improves adversarial robustness byup to 47.3 percent at epsilon = 4/255, while preserving or even improving cleanzero-shot and retrieval performance with less than 1 percent trainableparameters.</description>
      <author>example@mail.com (Chih-Ting Liao, Bin Ren, Guofeng Mei, Xu Zheng)</author>
      <guid isPermaLink="false">2505.11895v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
      <link>http://arxiv.org/abs/2505.12638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ChromFound，这是一个针对scATAC-seq设计的基石模型，用于解析调控机制，并展示了其在细胞类型注释和跨组学预测中的优异性能。&lt;h4&gt;背景&lt;/h4&gt;scATAC-seq技术的出现为解析调控机制提供了新的视角，但目前缺乏支持零样本高质细胞识别和综合多组学分析的基石模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于scATAC-seq的基石模型，实现零样本高质细胞识别和综合多组学分析。&lt;h4&gt;方法&lt;/h4&gt;ChromFound利用混合架构和基因组感知分词，有效捕捉基因组全局长上下文和调控信号。该模型在来自30个组织和6种疾病条件的1.97百万个细胞上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;ChromFound在6个不同的任务中展示了广泛的应用性，实现了零样本的稳健性能，并在细胞类型注释和跨组学预测中表现出良好的迁移性。它还揭示了现有计算方法未检测到的增强子-基因联系。&lt;h4&gt;结论&lt;/h4&gt;ChromFound为理解非编码基因组中的疾病风险变异提供了一个有前景的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of single-cell Assay for Transposase-Accessible Chromatin usingsequencing (scATAC-seq) offers an innovative perspective for decipheringregulatory mechanisms by assembling a vast repository of single-cell chromatinaccessibility data. While foundation models have achieved significant successin single-cell transcriptomics, there is currently no foundation model forscATAC-seq that supports zero-shot high-quality cell identification andcomprehensive multi-omics analysis simultaneously. Key challenges lie in thehigh dimensionality and sparsity of scATAC-seq data, as well as the lack of astandardized schema for representing open chromatin regions (OCRs). Here, wepresent \textbf{ChromFound}, a foundation model tailored for scATAC-seq.ChromFound utilizes a hybrid architecture and genome-aware tokenization toeffectively capture genome-wide long contexts and regulatory signals fromdynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissuesand 6 disease conditions, ChromFound demonstrates broad applicability across 6diverse tasks. Notably, it achieves robust zero-shot performance in generatinguniversal cell representations and exhibits excellent transferability in celltype annotation and cross-omics prediction. By uncovering enhancer-gene linksundetected by existing computational methods, ChromFound offers a promisingframework for understanding disease risk variants in the noncoding genome.</description>
      <author>example@mail.com (Yifeng Jiao, Yuchen Liu, Yu Zhang, Xin Guo, Yushuai Wu, Chen Jiang, Jiyang Li, Hongwei Zhang, Limei Han, Xin Gao, Yuan Qi, Yuan Cheng)</author>
      <guid isPermaLink="false">2505.12638v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges</title>
      <link>http://arxiv.org/abs/2505.11618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种层次化的时空推理基准STARK，用于评估大型语言模型（LLMs）和大型推理模型（LRMs）在时空推理能力上的表现。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型和大型推理模型在技术上取得了进展，但它们在处理复杂时空信号方面的能力仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs和LRMs在时空推理上的能力，特别是在状态估计、时空关系推理和结合领域知识的推理任务上。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含26个不同时空任务的基准，涉及14,552个挑战，并评估了3个LRMs和8个LLMs的表现。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs在需要几何推理的任务上表现有限，LRMs则在各种难度级别的任务上表现出鲁棒性，有时甚至超过了基于第一原理的传统方法。在需要世界知识的推理任务中，LLMs和LRMs的性能差距缩小，一些LLMs甚至超过了LRMs。LRM o3模型在所有评估任务中继续表现领先，这主要归因于推理模型规模更大。&lt;h4&gt;结论&lt;/h4&gt;STARK为智能CPS的未来创新提供了结构化框架，有助于识别LLMs和LRMs在时空推理上的局限性，并促进模型架构和推理范式的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).Despite advances in Large Language Models (LLMs) and Large Reasoning Models(LRMs), their capacity to reason about complex spatiotemporal signals remainsunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoningbenchmaRK, STARK, to systematically evaluate LLMs across three levels ofreasoning complexity: state estimation (e.g., predicting field variables,localizing and tracking events in space and time), spatiotemporal reasoningover states (e.g., inferring spatial-temporal relationships), andworld-knowledge-aware reasoning that integrates contextual and domain knowledge(e.g., intent prediction, landmark-aware navigation). We curate 26 distinctspatiotemporal tasks with diverse sensor modalities, comprising 14,552challenges where models answer directly or by Python Code Interpreter.Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasksrequiring geometric reasoning (e.g., multilateration or triangulation),particularly as complexity increases. Surprisingly, LRMs show robustperformance across tasks with various levels of difficulty, often competing orsurpassing traditional first-principle-based methods. Our results show that inreasoning tasks requiring world knowledge, the performance gap between LLMs andLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 modelcontinues to achieve leading performance across all evaluated tasks, a resultattributed primarily to the larger size of the reasoning models. STARKmotivates future innovations in model architectures and reasoning paradigms forintelligent CPS by providing a structured framework to identify limitations inthe spatiotemporal reasoning of LLMs and LRMs.</description>
      <author>example@mail.com (Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava)</author>
      <guid isPermaLink="false">2505.11618v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training</title>
      <link>http://arxiv.org/abs/2505.12236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures, Appear on IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TKRE（两阶段知识引导预训练关系抽取）是一个将大型语言模型与传统关系抽取模型相结合的框架，旨在解决Few-Shot Relation Extraction（FSRE）中的数据稀缺和模型泛化能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;Few-Shot Relation Extraction（FSRE）由于标注数据稀缺和现有模型泛化能力有限，是一个具有挑战性的任务。尽管大型语言模型（LLMs）通过上下文学习（ICL）在FSRE中展现出潜力，但它们的通用训练目标通常导致特定任务关系抽取的性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出TKRE框架，以克服FSRE中的挑战，提高关系抽取的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;TKRE框架包含两个关键创新：（1）利用LLMs生成解释驱动的知识和方案约束的合成数据，解决数据稀缺问题；（2）采用两阶段预训练策略，结合Masked Span Language Modeling（MSLM）和Span-Level Contrastive Learning（SCL）来增强关系推理和泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的综合实验表明，TKRE在FSRE中实现了新的最先进性能，证明了其在低资源场景中更广泛应用的潜力。&lt;h4&gt;结论&lt;/h4&gt;TKRE框架有效解决了FSRE任务中的挑战，有望在低资源场景中得到更广泛的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/uestc-gqj/tkre&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot Relation Extraction (FSRE) remains a challenging task due to thescarcity of annotated data and the limited generalization capabilities ofexisting models. Although large language models (LLMs) have demonstratedpotential in FSRE through in-context learning (ICL), their general-purposetraining objectives often result in suboptimal performance for task-specificrelation extraction. To overcome these challenges, we propose TKRE (Two-StageKnowledge-Guided Pre-training for Relation Extraction), a novel framework thatsynergistically integrates LLMs with traditional relation extraction models,bridging generative and discriminative learning paradigms. TKRE introduces twokey innovations: (1) leveraging LLMs to generate explanation-driven knowledgeand schema-constrained synthetic data, addressing the issue of data scarcity;and (2) a two-stage pre-training strategy combining Masked Span LanguageModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relationalreasoning and generalization. Together, these components enable TKRE toeffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasetsdemonstrate the efficacy of TKRE, achieving new state-of-the-art performance inFSRE and underscoring its potential for broader application in low-resourcescenarios. \footnote{The code and data are released onhttps://github.com/UESTC-GQJ/TKRE.</description>
      <author>example@mail.com (Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao)</author>
      <guid isPermaLink="false">2505.12236v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>GraphFLEx: Structure Learning Framework for Large Expanding Graphs</title>
      <link>http://arxiv.org/abs/2505.12323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphFLEx是一个用于大规模和动态扩展图上的图结构学习框架，通过限制边形成只针对通过聚类和细化技术确定的与结构相关的节点子集，以降低可扩展性瓶颈，实现高效的增量图更新。&lt;h4&gt;背景&lt;/h4&gt;图结构学习是图机器学习中的核心问题，对于揭示潜在关系和确保模型可解释性至关重要。然而，大多数现有方法不适合大规模和动态变化的图，因为它们通常需要在新节点到来时重新学习结构，并带来大量的计算和内存成本。&lt;h4&gt;目的&lt;/h4&gt;提出GraphFLEx，旨在解决大规模和动态扩展图上的图结构学习问题，以提高可扩展性和效率。&lt;h4&gt;方法&lt;/h4&gt;GraphFLEx通过结合聚类和细化技术来识别与结构相关的节点子集，从而限制边形成。框架支持48种灵活配置，通过集成不同的学习范式、细化策略和聚类方法，以适应广泛的图设置和学习目标。&lt;h4&gt;主要发现&lt;/h4&gt;在26个不同的数据集和图神经网络架构上进行的广泛实验表明，GraphFLEx实现了最先进的性能，并显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GraphFLEx框架在提高大规模和动态图上的图结构学习效率和可扩展性方面取得了显著成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph structure learning is a core problem in graph-based machine learning,essential for uncovering latent relationships and ensuring modelinterpretability. However, most existing approaches are ill-suited forlarge-scale and dynamically evolving graphs, as they often require completere-learning of the structure upon the arrival of new nodes and incursubstantial computational and memory costs. In this work, we propose GraphFLEx:a unified and scalable framework for Graph Structure Learning in Large andExpanding Graphs. GraphFLEx mitigates the scalability bottlenecks byrestricting edge formation to structurally relevant subsets of nodes identifiedthrough a combination of clustering and coarsening techniques. Thisdramatically reduces the search space and enables efficient, incremental graphupdates. The framework supports 48 flexible configurations by integratingdiverse choices of learning paradigms, coarsening strategies, and clusteringmethods, making it adaptable to a wide range of graph settings and learningobjectives. Extensive experiments across 26 diverse datasets and Graph NeuralNetwork architectures demonstrate that GraphFLEx achieves state-of-the-artperformance with significantly improved scalability.</description>
      <author>example@mail.com (Mohit Kataria, Nikita Malik, Sandeep Kumar, Jayadeva)</author>
      <guid isPermaLink="false">2505.12323v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics</title>
      <link>http://arxiv.org/abs/2505.12583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCAI 2025 Survey Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了机器人控制方法，以减轻由FMRs（基于基础模型的机器人）在物理世界中的行动带来的风险。&lt;h4&gt;背景&lt;/h4&gt;FMRs在通用技能方面有显著提升，但它们与物理世界的交互直接关系到人类和周围对象的安全。&lt;h4&gt;目的&lt;/h4&gt;全面总结FMRs从部署前到事故后的整个生命周期中减轻物理风险的控制方法。&lt;h4&gt;方法&lt;/h4&gt;将时间线分为三个阶段：部署前阶段、事故前阶段和事故后阶段，并分析了每个阶段的风险缓解策略。&lt;h4&gt;主要发现&lt;/h4&gt;发现了在事故前风险缓解策略、与人类物理交互假设的研究以及基础模型本身的基本问题等方面的研究空间。&lt;h4&gt;结论&lt;/h4&gt;希望该综述成为提供高分辨率分析FMRs物理风险及其控制的一个里程碑，有助于实现良好的人机关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Foundation Model-enabled robotics (FMRs) display greatly improvedgeneral-purpose skills, enabling more adaptable automation than conventionalrobotics. Their ability to handle diverse tasks thus creates new opportunitiesto replace human labor. However, unlike general foundation models, FMRsinteract with the physical world, where their actions directly affect thesafety of humans and surrounding objects, requiring careful deployment andcontrol. Based on this proposition, our survey comprehensively summarizes robotcontrol approaches to mitigate physical risks by covering all the lifespan ofFMRs ranging from pre-deployment to post-accident stage. Specifically, webroadly divide the timeline into the following three phases: (1) pre-deploymentphase, (2) pre-incident phase, and (3) post-incident phase. Throughout thissurvey, we find that there is much room to study (i) pre-incident riskmitigation strategies, (ii) research that assumes physical interaction withhumans, and (iii) essential issues of foundation models themselves. We hopethat this survey will be a milestone in providing a high-resolution analysis ofthe physical risks of FMRs and their control, contributing to the realizationof a good human-robot relationship.</description>
      <author>example@mail.com (Takeshi Kojima, Yaonan Zhu, Yusuke Iwasawa, Toshinori Kitamura, Gang Yan, Shu Morikuni, Ryosuke Takanami, Alfredo Solano, Tatsuya Matsushima, Akiko Murakami, Yutaka Matsuo)</author>
      <guid isPermaLink="false">2505.12583v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Depth Transfer: Learning to See Like a Simulator for Real-World Drone Navigation</title>
      <link>http://arxiv.org/abs/2505.12428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于域适应的深度迁移方法，用于解决机器人强化学习中模拟与现实之间的视觉差距问题。&lt;h4&gt;背景&lt;/h4&gt;在机器人强化学习中，模拟与现实之间的差异会严重影响策略性能，尤其是当输入是高维数据，如密集的深度估计时。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来桥接模拟和现实世界深度数据之间的视觉差距。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个变分自编码器（VAE）将模拟中的真实深度图像编码到潜在空间，然后作为强化学习策略的输入。在部署期间，编码器被进一步优化以使立体深度图像与这个潜在空间对齐，从而实现直接策略迁移而不需要微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在从真实深度输入切换到立体深度输入时，将障碍物避免的成功率提高了近一倍。此外，使用仅由IsaacGym生成的立体数据，该方法成功迁移到逼真的模拟器AvoidBench，并比最先进的基线实现了更好的性能。&lt;h4&gt;结论&lt;/h4&gt;在室内和室外环境中的实际评估证实了该方法的有效性，使基于深度的导航在多个领域具有鲁棒性和可推广性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Sim-to-real transfer is a fundamental challenge in robot reinforcement learning. Discrepancies between simulation and reality can significantly impair policy performance, especially if it receives high-dimensional inputs such as dense depth estimates from vision. We propose a novel depth transfer method based on domain adaptation to bridge the visual gap between simulated and real-world depth data. A Variational Autoencoder (VAE) is first trained to encode ground-truth depth images from simulation into a latent space, which serves as input to a reinforcement learning (RL) policy. During deployment, the encoder is refined to align stereo depth images with this latent space, enabling direct policy transfer without fine-tuning. We apply our method to the task of autonomous drone navigation through cluttered environments. Experiments in IsaacGym show that our method nearly doubles the obstacle avoidance success rate when switching from ground-truth to stereo depth input. Furthermore, we demonstrate successful transfer to the photo-realistic simulator AvoidBench using only IsaacGym-generated stereo data, achieving superior performance compared to state-of-the-art baselines. Real-world evaluations in both indoor and outdoor environments confirm the effectiveness of our approach, enabling robust and generalizable depth-based navigation across diverse domains.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sim-to-real transfer is a fundamental challenge in robot reinforcementlearning. Discrepancies between simulation and reality can significantly impairpolicy performance, especially if it receives high-dimensional inputs such asdense depth estimates from vision. We propose a novel depth transfer methodbased on domain adaptation to bridge the visual gap between simulated andreal-world depth data. A Variational Autoencoder (VAE) is first trained toencode ground-truth depth images from simulation into a latent space, whichserves as input to a reinforcement learning (RL) policy. During deployment, theencoder is refined to align stereo depth images with this latent space,enabling direct policy transfer without fine-tuning. We apply our method to thetask of autonomous drone navigation through cluttered environments. Experimentsin IsaacGym show that our method nearly doubles the obstacle avoidance successrate when switching from ground-truth to stereo depth input. Furthermore, wedemonstrate successful transfer to the photo-realistic simulator AvoidBenchusing only IsaacGym-generated stereo data, achieving superior performancecompared to state-of-the-art baselines. Real-world evaluations in both indoorand outdoor environments confirm the effectiveness of our approach, enablingrobust and generalizable depth-based navigation across diverse domains.</description>
      <author>example@mail.com (Hang Yu, Christophe De Wagter, Guido C. H. E de Croon)</author>
      <guid isPermaLink="false">2505.12428v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods</title>
      <link>http://arxiv.org/abs/2505.12132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, International Workshop on ADVANCEs in ICT  Infrastructures and Services, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要讨论了6G移动网络作为5G之后的演进步骤，预测了移动流量的爆炸性增长，并强调了其在低延迟、高数据速率、高设备密度和广泛覆盖等方面的优势。同时，摘要指出了在电信行业新系统中节能的重要性，并提到了网络切片作为6G/5G移动网络及其他新系统（如物联网、车联网和工业物联网）的基础性使能技术。然而，网络切片架构中嵌入的节能方法仍是一个研究空白。&lt;h4&gt;背景&lt;/h4&gt;6G移动网络预计将带来移动流量的爆炸性增长，提供超低延迟、高数据速率、高设备密度和广泛覆盖，对各个领域的服务产生积极影响。电信行业的新系统节能是主要关注点，因为所有参与者都期望减少碳足迹以减轻气候变化。&lt;h4&gt;目的&lt;/h4&gt;研究如何将节能方法嵌入网络切片架构中，这是全球几乎所有新创新系统的基础性使能技术。&lt;h4&gt;方法&lt;/h4&gt;通过在NS架构中部署ML-native代理，根据用户需求动态编排和优化资源，以实现节能。&lt;h4&gt;主要发现&lt;/h4&gt;论文提出了在SFI2网络切片参考架构中使用对比学习来改善资源分配的节能效果。&lt;h4&gt;结论&lt;/h4&gt;论文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了6G移动网络作为5G之后的演进步骤，预测了移动流量的爆炸性增长，并强调了其在低延迟、高数据速率、高设备密度和广泛覆盖等方面的优势。同时，摘要指出了在电信行业新系统中节能的重要性，并提到了网络切片作为6G/5G移动网络及其他新系统（如物联网、车联网和工业物联网）的基础性使能技术。然而，网络切片架构中嵌入的节能方法仍是一个研究空白。本文讨论了如何将节能方法嵌入网络切片架构中，这是全球几乎所有新创新系统的基础性使能技术。本文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。在SFI2网络切片参考架构中，本文提出了使用对比学习来改善资源分配的节能效果。本文的主要贡献是提出了一种在网络切片中节能的方法，通过动态优化资源使用来降低能耗。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5281/zenodo.15449843&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6G mobile network is the next evolutionary step after 5G, with aprediction of an explosive surge in mobile traffic. It provides ultra-lowlatency, higher data rates, high device density, and ubiquitous coverage,positively impacting services in various areas. Energy saving is a majorconcern for new systems in the telecommunications sector because all playersare expected to reduce their carbon footprints to contribute to mitigatingclimate change. Network slicing is a fundamental enabler for 6G/5G mobilenetworks and various other new systems, such as the Internet of Things (IoT),Internet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-savingmethods embedded in network slicing architectures are still a research gap.This paper discusses how to embed energy-saving methods in network-slicingarchitectures that are a fundamental enabler for nearly all new innovativesystems being deployed worldwide. This paper's main contribution is a proposalto save energy in network slicing. That is achieved by deploying ML-nativeagents in NS architectures to dynamically orchestrate and optimize resourcesbased on user demands. The SFI2 network slicing reference architecture is theconcrete use case scenario in which contrastive learning improves energy savingfor resource allocation.</description>
      <author>example@mail.com (Rodrigo Moreira, Tereza C. M. Carvalho, Flávio de Oliveira Silva, Nazim Agoulmine, Joberto S. B. Martins)</author>
      <guid isPermaLink="false">2505.12132v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Visual Generalization in Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.11719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在机器人学习中，训练能够在不同视觉环境中稳健操作的视觉操作策略是一个重要且未解决的挑战。作者提出了一种扩展的解耦表示学习和关联记忆方法，将其应用于更复杂和动态的操作任务，并展示了在模拟和真实硬件上的零样本适应能力。此外，该方法在模仿学习方面也取得了显著成果，并提出了一种新的技术，使策略对二维平面旋转具有不变性。&lt;h4&gt;背景&lt;/h4&gt;当前方法通常通过依赖不变表示（如点云和深度）或通过视觉域随机化和/或大型、视觉多样化的数据集来强制推广，以避免稳健性问题。&lt;h4&gt;目的&lt;/h4&gt;目标是扩展解耦表示学习和关联记忆，使其适用于更复杂和动态的操作任务，并展示其零样本适应能力。&lt;h4&gt;方法&lt;/h4&gt;方法包括扩展解耦表示学习和关联记忆，应用于复杂任务，并引入了一种新的技术，使策略对二维平面旋转具有不变性。&lt;h4&gt;主要发现&lt;/h4&gt;主要发现是该方法在模拟和真实硬件上的零样本适应能力，以及在模仿学习方面的显著成果。&lt;h4&gt;结论&lt;/h4&gt;结论是这项工作标志着向实现不仅易于适应，而且对现实世界部署的复杂性和动态性质具有鲁棒性的操作策略的重大步骤。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在机器人学习中，训练能够在多种视觉环境中稳健的视觉操作策略仍然是一个重要且未解决的挑战。当前的方法通常通过依赖不变表示（如点云和深度）或通过视觉域随机化和/或大型、视觉多样化的数据集来回避这个问题。解耦表示学习——尤其是当与联想记忆原则相结合时——最近显示出使基于视觉的强化学习策略能够对视觉分布变化具有鲁棒性的希望。然而，这些技术主要局限于更简单的基准和玩具环境。在这项工作中，我们将解耦表示学习和联想记忆扩展到更视觉和动态复杂的操作任务中，并在模拟和真实硬件上展示了零样本对视觉扰动的适应性。我们进一步将这种方法扩展到模仿学习，特别是扩散策略，并通过实验与最先进的模仿学习方法相比，显示出显著的视觉泛化增益。最后，我们介绍了一种从模型等变性文献中借鉴的新技术，该技术将任何训练好的神经网络策略转换为对二维平面旋转具有不变性的策略，使我们的策略不仅对视觉具有鲁棒性，而且对某些相机扰动具有弹性。我们认为这项工作标志着向实现不仅易于适应，而且对现实世界部署的复杂性和动态性质具有鲁棒性的操作策略的重大步骤。补充视频可在https://sites.google.com/view/vis-gen-robotics/home上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training vision-based manipulation policies that are robust across diversevisual environments remains an important and unresolved challenge in robotlearning. Current approaches often sidestep the problem by relying on invariantrepresentations such as point clouds and depth, or by brute-forcinggeneralization through visual domain randomization and/or large, visuallydiverse datasets. Disentangled representation learning - especially whencombined with principles of associative memory - has recently shown promise inenabling vision-based reinforcement learning policies to be robust to visualdistribution shifts. However, these techniques have largely been constrained tosimpler benchmarks and toy environments. In this work, we scale disentangledrepresentation learning and associative memory to more visually and dynamicallycomplex manipulation tasks and demonstrate zero-shot adaptability to visualperturbations in both simulation and on real hardware. We further extend thisapproach to imitation learning, specifically Diffusion Policy, and empiricallyshow significant gains in visual generalization compared to state-of-the-artimitation learning methods. Finally, we introduce a novel technique adaptedfrom the model equivariance literature that transforms any trained neuralnetwork policy into one invariant to 2D planar rotations, making our policy notonly visually robust but also resilient to certain camera perturbations. Webelieve that this work marks a significant step towards manipulation policiesthat are not only adaptable out of the box, but also robust to the complexitiesand dynamical nature of real-world deployment. Supplementary videos areavailable at https://sites.google.com/view/vis-gen-robotics/home.</description>
      <author>example@mail.com (Sumeet Batra, Gaurav Sukhatme)</author>
      <guid isPermaLink="false">2505.11719v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Prompt-driven Community Search</title>
      <link>http://arxiv.org/abs/2505.12304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Pre-trained Prompt-driven Community Search (PPCS)的新模型，用于半监督社区搜索，旨在提高搜索准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;现有的半监督社区检测算法大多基于已知的社区进行检测，但检测到的社区通常不包含查询节点，不适合用于搜索给定节点的社区。&lt;h4&gt;目的&lt;/h4&gt;将“预训练，提示”范式应用于半监督社区搜索，提出PPCS模型，以增强搜索准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;PPCS由三个主要组件组成：节点编码、样本生成和提示驱动微调。节点编码组件使用图神经网络学习图中节点的局部结构模式；样本生成组件为给定节点识别初始社区，并选择与初始社区结构相似的已知社区作为训练样本；提示驱动微调组件利用这些样本作为提示来指导最终的社区预测。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的实验结果表明，PPCS的性能优于基线算法，且在社区搜索效率上高于半监督社区搜索基线方法。消融研究表明，PPCS的每个组件都是有效的。&lt;h4&gt;结论&lt;/h4&gt;PPCS模型在半监督社区搜索任务中表现出色，提高了搜索的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The "pre-train, prompt" paradigm is widely adopted in various graph-basedtasks and has shown promising performance in community detection. Most existingsemi-supervised community detection algorithms detect communities based onknown ones, and the detected communities typically do not contain the givenquery node. Therefore, they are not suitable for searching the community of agiven node. Motivated by this, we adopt this paradigm into the semi-supervisedcommunity search for the first time and propose Pre-trained Prompt-drivenCommunity Search (PPCS), a novel model designed to enhance search accuracy andefficiency. PPCS consists of three main components: node encoding, samplegeneration, and prompt-driven fine-tuning. Specifically, the node encodingcomponent employs graph neural networks to learn local structural patterns ofnodes in a graph, thereby obtaining representations for nodes and communities.Next, the sample generation component identifies an initial community for agiven node and selects known communities that are structurally similar to theinitial one as training samples. Finally, the prompt-driven fine-tuningcomponent leverages these samples as prompts to guide the final communityprediction. Experimental results on five real-world datasets demonstrate thatPPCS performs better than baseline algorithms. It also achieves highercommunity search efficiency than semi-supervised community search baselinemethods, with ablation studies verifying the effectiveness of each component ofPPCS.</description>
      <author>example@mail.com (Li Ni, Hengkai Xu, Lin Mu, Yiwen Zhang, Wenjian Luo)</author>
      <guid isPermaLink="false">2505.12304v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models</title>
      <link>http://arxiv.org/abs/2505.12534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ChemPile，一个包含超过75亿个化学数据的开源数据集，旨在为化学科学中的通用模型训练和评估提供支持。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在多个科学领域取得了显著成功，但由于缺乏反映化学领域多方面性质的大量、多样化、高质量数据集，其在化学领域的影响仍然有限。&lt;h4&gt;目的&lt;/h4&gt;ChemPile旨在为化学科学中的通用模型训练和评估提供数据支持，并促进化学AI的发展。&lt;h4&gt;方法&lt;/h4&gt;ChemPile通过数百小时的专家编辑构建，包含多种化学数据表示（如SMILES、SELFIES、IUPAC名称、InChI、分子渲染）、科学和教育文本、可执行代码和化学图像。&lt;h4&gt;主要发现&lt;/h4&gt;ChemPile集成了基础知识、专业知识、视觉理解和高级推理，反映了人类化学家通过多样化的学习材料和经验发展专业知识的过程。&lt;h4&gt;结论&lt;/h4&gt;ChemPile的发布有望成为化学AI的催化剂，促进下一代化学基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在多个科学领域取得了显著的成功，但由于缺乏反映该领域多方面性质的大量、多样化、高质量数据集，其在化学领域的影响仍然有限。我们提出了ChemPile，这是一个包含超过75亿个经过编辑的化学数据标记的开源数据集，专门用于训练和评估化学科学中的通用模型。该数据集反映了人类学习化学的旅程——从教育基础到专业化的专业知识，涵盖了多种模态和内容类型，包括不同化学表示（SMILES、SELFIES、IUPAC名称、InChI、分子渲染）的结构化数据、科学和教育文本、可执行代码和化学图像。ChemPile集成了基础知识（教科书、讲义）、专业知识（科学文章和语言接口数据）、视觉理解（分子结构、图表）和高级推理（问题解决轨迹和代码）——反映了人类化学家通过多样化的学习材料和经验发展专业知识的过程。通过数百小时的专家编辑构建，ChemPile捕捉了基础概念和领域特定复杂性。我们提供了标准化的训练、验证和测试分割，以实现稳健的基准测试。ChemPile通过HuggingFace以一致的API、许可许可和详细文档公开发布。我们希望ChemPile能够成为化学AI的催化剂，促进下一代化学基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have shown remarkable success across scientific domains,yet their impact in chemistry remains limited due to the absence of diverse,large-scale, high-quality datasets that reflect the field's multifacetednature. We present the ChemPile, an open dataset containing over 75 billiontokens of curated chemical data, specifically built for training and evaluatinggeneral-purpose models in the chemical sciences. The dataset mirrors the humanlearning journey through chemistry -- from educational foundations tospecialized expertise -- spanning multiple modalities and content typesincluding structured data in diverse chemical representations (SMILES, SELFIES,IUPAC names, InChI, molecular renderings), scientific and educational text,executable code, and chemical images. ChemPile integrates foundationalknowledge (textbooks, lecture notes), specialized expertise (scientificarticles and language-interfaced data), visual understanding (molecularstructures, diagrams), and advanced reasoning (problem-solving traces and code)-- mirroring how human chemists develop expertise through diverse learningmaterials and experiences. Constructed through hundreds of hours of expertcuration, the ChemPile captures both foundational concepts and domain-specificcomplexity. We provide standardized training, validation, and test splits,enabling robust benchmarking. ChemPile is openly released via HuggingFace witha consistent API, permissive license, and detailed documentation. We hope theChemPile will serve as a catalyst for chemical AI, enabling the development ofthe next generation of chemical foundation models.</description>
      <author>example@mail.com (Adrian Mirza, Nawaf Alampara, Martiño Ríos-García, Mohamed Abdelalim, Jack Butler, Bethany Connolly, Tunca Dogan, Marianna Nezhurina, Bünyamin Şen, Santosh Tirunagari, Mark Worrall, Adamo Young, Philippe Schwaller, Michael Pieler, Kevin Maik Jablonka)</author>
      <guid isPermaLink="false">2505.12534v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Transformer learns the cross-task prior and regularization for in-context learning</title>
      <link>http://arxiv.org/abs/2505.12138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了Transformer在逆线性回归（ILR）中的情境学习（ICL）能力，探讨了隐含情境的性质及其对下游预测的效用。&lt;h4&gt;背景&lt;/h4&gt;尽管Transformer在情境学习方面表现出色，但其推断情境的本质及其对预测的实用性仍需进一步研究。&lt;h4&gt;目的&lt;/h4&gt;通过研究ILR中的情境学习，探讨隐含情境的性质及其对预测的效用。&lt;h4&gt;方法&lt;/h4&gt;本文引入了一个线性Transformer来学习从情境示例到潜在权重向量的逆映射，并关注了权重向量中未知数多于情境长度的秩亏逆问题。&lt;h4&gt;主要发现&lt;/h4&gt;Transformer隐式地学习了一个先验分布和有效的正则化策略，优于传统的岭回归和正则化方法。研究发现低任务维度相对于情境长度对于成功学习是必要的。&lt;h4&gt;结论&lt;/h4&gt;这些结果不仅展示了Transformer解决病态逆问题的潜力，也为理解Transformer内部的知识提取机制提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;Transformers have shown a remarkable ability for in-context learning (ICL), making predictions based on contextual examples. However, while theoretical analyses have explored this prediction capability, the nature of the inferred context and its utility for downstream predictions remain open questions. This paper aims to address these questions by examining ICL for inverse linear regression (ILR), where context inference can be characterized by unsupervised learning of underlying weight vectors. Focusing on the challenging scenario of rank-deficient inverse problems, where context length is smaller than the number of unknowns in the weight vectors and regularization is necessary, we introduce a linear transformer to learn the inverse mapping from contextual examples to the underlying weight vector. Our findings reveal that the transformer implicitly learns both a prior distribution and an effective regularization strategy, outperforming traditional ridge regression and regularization methods. A key insight is the necessity of low task dimensionality relative to the context length for successful learning. Furthermore, we numerically verify that the error of the transformer estimators scales linearly with the noise level, the ratio of task dimension to context length, and the condition number of the input data. These results not only demonstrate the potential of transformers for solving ill-posed inverse problems, but also provide a new perspective towards understanding the knowledge extraction mechanism within transformers.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have shown a remarkable ability for in-context learning (ICL),making predictions based on contextual examples. However, while theoreticalanalyses have explored this prediction capability, the nature of the inferredcontext and its utility for downstream predictions remain open questions. Thispaper aims to address these questions by examining ICL for inverse linearregression (ILR), where context inference can be characterized by unsupervisedlearning of underlying weight vectors. Focusing on the challenging scenario ofrank-deficient inverse problems, where context length is smaller than thenumber of unknowns in the weight vectors and regularization is necessary, weintroduce a linear transformer to learn the inverse mapping from contextualexamples to the underlying weight vector. Our findings reveal that thetransformer implicitly learns both a prior distribution and an effectiveregularization strategy, outperforming traditional ridge regression andregularization methods. A key insight is the necessity of low taskdimensionality relative to the context length for successful learning.Furthermore, we numerically verify that the error of the transformer estimatorscales linearly with the noise level, the ratio of task dimension to contextlength, and the condition number of the input data. These results not onlydemonstrate the potential of transformers for solving ill-posed inverseproblems, but also provide a new perspective towards understanding theknowledge extraction mechanism within transformers.</description>
      <author>example@mail.com (Fei Lu, Yue Yu)</author>
      <guid isPermaLink="false">2505.12138v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement</title>
      <link>http://arxiv.org/abs/2505.11939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为FG-CLEP的ECG文本对比学习方法，旨在通过大型语言模型从不完全的报告中恢复波形特征，以改善ECG诊断模型的能力。&lt;h4&gt;背景&lt;/h4&gt;传统的ECG-text对比学习方法在诊断心血管疾病方面表现良好，但往往忽略了报告的不完整性，导致模型无法充分捕捉波形特征和诊断推理。&lt;h4&gt;目的&lt;/h4&gt;提出FG-CLEP方法，以解决从不完全报告中恢复波形特征的问题，并提高ECG诊断模型的能力。&lt;h4&gt;方法&lt;/h4&gt;FG-CLEP利用大型语言模型帮助恢复波形特征，同时克服了幻觉和非一一对应关系等挑战。此外，引入语义相似度矩阵以指导对比学习，并采用基于sigmoid的损失函数以适应ECG相关任务的标签多性质。&lt;h4&gt;主要发现&lt;/h4&gt;在六个数据集上的实验表明，FG-CLEP在零样本预测和线性探针任务上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;FG-CLEP是一种有效的ECG文本对比学习方法，能够提高ECG诊断模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：心电图（ECG）对于诊断心血管疾病至关重要。尽管之前的心电图文本对比学习方法显示出有希望的结果，但它们往往忽略了报告的不完整性。给定一个ECG，报告是通过首先识别关键波形特征，然后通过这些特征推断最终诊断来生成的。尽管这些波形特征很重要，但它们通常不会作为中间结果记录在报告中。将ECG与这种不完整的报告对齐阻碍了模型捕捉ECG波形特征的能力，并限制了其对基于这些特征进行的诊断推理的理解。为了解决这个问题，我们提出了FG-CLEP（细粒度对比语言心电图预训练），它旨在在幻觉和非一一对应关系之间波形特征与诊断的挑战下，利用大型语言模型（LLMs）从不完全的报告中恢复这些波形特征。此外，考虑到由于ECG中常见诊断的普遍存在，经常出现假阴性，我们引入了一个语义相似度矩阵来指导对比学习。此外，我们采用基于sigmoid的损失函数来适应ECG相关任务的多标签性质。在六个数据集上的实验表明，FG-CLEP在这些数据集上的零样本预测和线性探针任务上都优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiograms (ECGs) are essential for diagnosing cardiovasculardiseases. While previous ECG-text contrastive learning methods have shownpromising results, they often overlook the incompleteness of the reports. Givenan ECG, the report is generated by first identifying key waveform features andthen inferring the final diagnosis through these features. Despite theirimportance, these waveform features are often not recorded in the report asintermediate results. Aligning ECGs with such incomplete reports impedes themodel's ability to capture the ECG's waveform features and limits itsunderstanding of diagnostic reasoning based on those features. To address this,we propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), whichaims to recover these waveform features from incomplete reports with the helpof large language models (LLMs), under the challenges of hallucinations and thenon-bijective relationship between waveform features and diagnoses.Additionally, considering the frequent false negatives due to the prevalence ofcommon diagnoses in ECGs, we introduce a semantic similarity matrix to guidecontrastive learning. Furthermore, we adopt a sigmoid-based loss function toaccommodate the multi-label nature of ECG-related tasks. Experiments on sixdatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in bothzero-shot prediction and linear probing across these datasets.</description>
      <author>example@mail.com (Haitao Li, Che Liu, Zhengyao Ding, Ziyi Liu, Zhengxing Huang)</author>
      <guid isPermaLink="false">2505.11939v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling</title>
      <link>http://arxiv.org/abs/2505.12272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的框架，旨在通过结合GNN蒸馏和抽象概率交互建模（APIM）来克服知识图谱补全（KGC）中的挑战，以提高知识图谱的有效性。&lt;h4&gt;背景&lt;/h4&gt;大多数知识图谱（KGs）不完整，限制了它们在下游应用中的有效性。&lt;h4&gt;目的&lt;/h4&gt;旨在通过推断缺失链接来解决知识图谱补全的问题。&lt;h4&gt;方法&lt;/h4&gt;该方法通过结合GNN蒸馏和抽象概率交互建模（APIM），GNN蒸馏引入了迭代消息特征过滤过程来减轻过平滑，而APIM模块通过概率签名和转移矩阵学习结构化的抽象交互模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与基线模型相比，该方法在广泛使用的WN18RR和FB15K-237数据集上取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;该研究结果强调了控制信息传播和利用结构化概率建模的重要性，为推进知识图谱补全提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;This study proposes a unified framework that aims to overcome the challenges of knowledge graph completion (KGC) by integrating GNN distillation and abstract probabilistic interaction modeling (APIM), in order to enhance the effectiveness of knowledge graphs in downstream applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs (KGs) serve as fundamental structures for organizinginterconnected data across diverse domains. However, most KGs remainincomplete, limiting their effectiveness in downstream applications. Knowledgegraph completion (KGC) aims to address this issue by inferring missing links,but existing methods face critical challenges: deep graph neural networks(GNNs) suffer from over-smoothing, while embedding-based models fail to captureabstract relational features. This study aims to overcome these limitations byproposing a unified framework that integrates GNN distillation and abstractprobabilistic interaction modeling (APIM). GNN distillation approach introducesan iterative message-feature filtering process to mitigate over-smoothing,preserving the discriminative power of node representations. APIM modulecomplements this by learning structured, abstract interaction patterns throughprobabilistic signatures and transition matrices, allowing for a richer, moreflexible representation of entity and relation interactions. We apply thesemethods to GNN-based models and the APIM to embedding-based KGC models,conducting extensive evaluations on the widely used WN18RR and FB15K-237datasets. Our results demonstrate significant performance gains over baselinemodels, showcasing the effectiveness of the proposed techniques. The findingshighlight the importance of both controlling information propagation andleveraging structured probabilistic modeling, offering new avenues foradvancing knowledge graph completion. And our codes are available athttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.</description>
      <author>example@mail.com (Lingzhi Wang, Pengcheng Huang, Haotian Li, Yuliang Wei, Guodong Xin, Rui Zhang, Donglin Zhang, Zhenzhou Ji, Wei Wang)</author>
      <guid isPermaLink="false">2505.12272v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets</title>
      <link>http://arxiv.org/abs/2505.12532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Wavelet Fine-Tuning (WaveFT)的参数高效微调方法，该方法在残差矩阵的波形域中学习高度稀疏的更新，适用于极端参数高效的场景。&lt;h4&gt;背景&lt;/h4&gt;高效的适应大型基础模型对于在有限的计算和内存预算下至关重要，传统的PEFT方法如LoRA在少量参数的情况下效果有限。&lt;h4&gt;目的&lt;/h4&gt;提出WaveFT方法以实现参数高效的微调，并提供更精细的能力调整。&lt;h4&gt;方法&lt;/h4&gt;WaveFT方法通过在波形域学习残差矩阵的更新，实现高度稀疏的参数调整。同时，通过与直接在权重域应用稀疏更新的方法SHiRA进行比较，以验证波形变换的效果。&lt;h4&gt;主要发现&lt;/h4&gt;WaveFT在个人化的文本到图像生成任务上显著优于LoRA和其他PEFT方法，尤其是在低参数数量时，实现了更好的主题一致性、提示对齐和图像多样性。&lt;h4&gt;结论&lt;/h4&gt;WaveFT是一种有效的PEFT方法，能够在参数高效的情况下实现高质量的个人化图像生成，特别适合在计算资源受限的环境中使用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：有效地调整大型基础模型至关重要，尤其是在有限的计算和内存预算下。参数高效微调（PEFT）方法如LoRA在少量参数的系统中提供有限的粒度和效果。我们提出了波纹微调（WaveFT），一种新的PEFT方法，该方法在残差矩阵的波纹域中学习高度稀疏的更新。WaveFT允许精确控制可训练参数，提供细粒度的能力调整，并且具有非常低的参数数量，可能远低于LoRA的最小值——非常适合极端参数高效的场景。为了证明波形变换的效果，我们将WaveFT与一个称为SHiRA的特殊情况进行了比较，该情况涉及直接在权重域应用稀疏更新。在以Stable Diffusion XL作为基线进行个性化文本到图像生成任务时评估，WaveFT在低参数数量时显著优于LoRA和其他PEFT方法；实现了更高的主题一致性、提示对齐和图像多样性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently adapting large foundation models is critical, especially withtight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)methods such as LoRA offer limited granularity and effectiveness infew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFTmethod that learns highly sparse updates in the wavelet domain of residualmatrices. WaveFT allows precise control of trainable parameters, offeringfine-grained capacity adjustment and excelling with remarkably low parametercount, potentially far fewer than LoRA's minimum -- ideal for extremeparameter-efficient scenarios. In order to demonstrate the effect of thewavelet transform, we compare WaveFT with a special case, called SHiRA, thatentails applying sparse updates directly in the weight domain. Evaluated onpersonalized text-to-image generation using Stable Diffusion XL as baseline,WaveFT significantly outperforms LoRA and other PEFT methods, especially at lowparameter counts; achieving superior subject fidelity, prompt alignment, andimage diversity.</description>
      <author>example@mail.com (Ahmet Bilican, M. Akın Yılmaz, A. Murat Tekalp, R. Gökberk Cinbiş)</author>
      <guid isPermaLink="false">2505.12532v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Relation-Aware Graph Foundation Model</title>
      <link>http://arxiv.org/abs/2505.12027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为REEF的新框架，用于图学习中的基础模型，旨在通过利用关系标记作为基本单位来提高模型在各类自然语言处理任务中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;近年来，大型语言模型（LLMs）在自然语言处理（NLP）任务中表现出显著的泛化能力，而图基础模型（GFMs）在图学习中也展现出巨大的潜力。然而，由于图没有明确的泛化单位，设计有效的预训练策略变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出REEF框架，以解决图学习中的泛化问题，并提高模型在预训练和迁移学习任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 使用关系标记作为GFMs的基本单位；2. 构建关系词汇表以存储图中的关系信息；3. 引入两个超网络，根据关系标记自适应生成图神经网络中聚合器和分类器的参数；4. 设计另一个超网络来构建特定于数据集的项目符，并将数据集级别的特征偏差纳入初始节点表示；5. 采用图数据增强和混合数据集预训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;REEF在预训练和迁移学习任务上显著优于现有方法，展现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;REEF作为一个强大的基础模型，在图学习领域具有巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示了显著的泛化能力。同样，图基础模型（GFMs）已成为图学习领域的一个有希望的进展方向，旨在通过大规模预训练在多样化的数据集上实现泛化。然而，与依赖于显式标记表示的语言模型不同，图缺乏一个定义良好的泛化单位，这使得设计有效的预训练策略变得具有挑战性。在本工作中，我们提出了一种名为REEF的新框架，该框架利用关系标记作为GFMs的基本单位。受LLMs中标记词汇表的启发，我们构建了一个关系标记的词汇表，用于在图中存储关系信息。为了适应不同的关系，我们引入了两个超网络，根据关系标记自适应地生成图神经网络中聚合器和分类器的参数。此外，我们还设计了一个超网络来构建特定于数据集的项目符，并将数据集级别的特征偏差纳入初始节点表示，从而增强了在不同数据集上具有相同关系的灵活性。进一步地，我们采用了图数据增强和混合数据集预训练策略，使得REEF能够更有效地捕捉关系多样性，并展现出强大的泛化能力。广泛的实验表明，REEF在预训练和迁移学习任务上显著优于现有方法，凸显了其作为基于图应用的有力基础模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, large language models (LLMs) have demonstrated remarkablegeneralization capabilities across various natural language processing (NLP)tasks. Similarly, graph foundation models (GFMs) have emerged as a promisingdirection in graph learning, aiming to generalize across diverse datasetsthrough large-scale pre-training. However, unlike language models that rely onexplicit token representations, graphs lack a well-defined unit forgeneralization, making it challenging to design effective pre-trainingstrategies. In this work, we propose REEF, a novel framework that leveragesrelation tokens as the basic units for GFMs. Inspired by the token vocabularyin LLMs, we construct a relation vocabulary of relation tokens to storerelational information within graphs. To accommodate diverse relations, weintroduce two hypernetworks that adaptively generate the parameters ofaggregators and classifiers in graph neural networks based on relation tokens.In addition, we design another hypernetwork to construct dataset-specificprojectors and incorporate a dataset-level feature bias into the initial noderepresentations, enhancing flexibility across different datasets with the samerelation. Further, we adopt graph data augmentation and a mixed-datasetpre-training strategy, allowing REEF to capture relational diversity moreeffectively and exhibit strong generalization capabilities. Extensiveexperiments show that REEF significantly outperforms existing methods on bothpre-training and transfer learning tasks, underscoring its potential as apowerful foundation model for graph-based applications.</description>
      <author>example@mail.com (Jianxiang Yu, Jiapeng Zhu, Hao Qian, Ziqi Liu, Zhiqiang Zhang, Xiang Li)</author>
      <guid isPermaLink="false">2505.12027v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Invariant Risk Minimization</title>
      <link>http://arxiv.org/abs/2505.12506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新颖的无监督框架，用于不变风险最小化（IRM），扩展了不变性的概念，使其适用于标签不可用的场景。&lt;h4&gt;背景&lt;/h4&gt;传统的IRM方法依赖于标签数据来学习对环境分布变化鲁棒的特征表示。&lt;h4&gt;目的&lt;/h4&gt;通过特征分布对齐重新定义不变性，从而实现从无标签数据中学习鲁棒的特征表示。&lt;h4&gt;方法&lt;/h4&gt;在框架中引入了两种方法：主不变成分分析（PICA），一种基于高斯假设的线性方法，用于提取不变方向；以及变分不变自动编码器（VIAE），一种深度生成模型，用于分离环境不变和环境相关的潜在因子。&lt;h4&gt;主要发现&lt;/h4&gt;方法基于一种新颖的无监督结构因果模型，支持环境条件下的样本生成和干预；在合成数据集和MNIST的修改版本上的实验评估表明，该方法在捕捉不变结构、保留相关信息以及在无标签的情况下跨环境泛化方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;该方法在不依赖标签的情况下，能够有效地学习鲁棒的特征表示，并在不同环境中进行泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yotamnor/uirm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel unsupervised framework for \emph{Invariant RiskMinimization} (IRM), extending the concept of invariance to settings wherelabels are unavailable. Traditional IRM methods rely on labeled data to learnrepresentations that are robust to distributional shifts across environments.In contrast, our approach redefines invariance through feature distributionalignment, enabling robust representation learning from unlabeled data. Weintroduce two methods within this framework: Principal Invariant ComponentAnalysis (PICA), a linear method that extracts invariant directions underGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deepgenerative model that disentangles environment-invariant andenvironment-dependent latent factors. Our approach is based on a novel``unsupervised'' structural causal model and supports environment-conditionedsample-generation and intervention. Empirical evaluations on synthetic datasetand modified versions of MNIST demonstrate the effectiveness of our methods incapturing invariant structure, preserving relevant information, andgeneralizing across environments without access to labels.</description>
      <author>example@mail.com (Yotam Norman, Ron Meir)</author>
      <guid isPermaLink="false">2505.12506v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities</title>
      <link>http://arxiv.org/abs/2505.11921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DC-Seg的新方法，用于解决脑图像分割问题，该方法通过解耦对比学习，将图像分解为解剖不变表示和模态特定表示，以提高分割精度。&lt;h4&gt;背景&lt;/h4&gt;脑图像分割通常需要整合来自多个模态的信息，但并非所有患者都有所有模态的临床数据，这给分割带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提高脑图像分割的准确性，即使某些模态的数据缺失。&lt;h4&gt;方法&lt;/h4&gt;DC-Seg使用解剖对比学习和模态对比学习来解耦图像，同时引入基于分割的正则化器，以增强模型对缺失模态的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在BraTS 2020和私人白质高信号（WMH）分割数据集上的实验表明，DC-Seg在处理具有不同缺失模态的不完整多模态脑肿瘤分割任务中优于现有方法，并在WMH分割中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;DC-Seg是一种有效的脑图像分割方法，能够处理模态缺失的问题，并在实际应用中表现出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：准确分割脑图像通常需要整合来自多个图像模态的互补信息。然而，并非所有患者都有所有模态的临床数据，这给分割带来了重大挑战。为了解决这个问题，先前的研究将多个模态编码到共享的潜在空间中。虽然这在一定程度上是有效的，但它仍然是不够理想的，因为每个模态都包含独特且有价值的信息。在本研究中，我们提出了DC-Seg（解耦对比学习用于分割），一种新的方法，通过使用解剖对比学习和模态对比学习分别显式地将图像解耦为解剖不变表示和模态特定表示。这种解决方案通过考虑模态差距，提高了解剖和模态特定特征的分离，导致更鲁棒的表现。此外，我们引入了一种基于分割的正则化器，增强了模型对缺失模态的鲁棒性。在BraTS 2020和私人白质高信号（WMH）分割数据集上的大量实验表明，DC-Seg在处理具有不同缺失模态的不完整多模态脑肿瘤分割任务中优于现有方法，同时也在WMH分割中表现出强大的泛化能力。代码可在https://github.com/CuCl-2/DC-Seg上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of brain images typically requires the integration ofcomplementary information from multiple image modalities. However, clinicaldata for all modalities may not be available for every patient, creating asignificant challenge. To address this, previous studies encode multiplemodalities into a shared latent space. While somewhat effective, it remainssuboptimal, as each modality contains distinct and valuable information. Inthis study, we propose DC-Seg (Disentangled Contrastive Learning forSegmentation), a new method that explicitly disentangles images intomodality-invariant anatomical representation and modality-specificrepresentation, by using anatomical contrastive learning and modalitycontrastive learning respectively. This solution improves the separation ofanatomical and modality-specific features by considering the modality gaps,leading to more robust representations. Furthermore, we introduce asegmentation-based regularizer that enhances the model's robustness to missingmodalities. Extensive experiments on the BraTS 2020 and a private white matterhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperformsstate-of-the-art methods in handling incomplete multimodal brain tumorsegmentation tasks with varying missing modalities, while also demonstratestrong generalizability in WMH segmentation. The code is available athttps://github.com/CuCl-2/DC-Seg.</description>
      <author>example@mail.com (Haitao Li, Ziyu Li, Yiheng Mao, Zhengyao Ding, Zhengxing Huang)</author>
      <guid isPermaLink="false">2505.11921v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark</title>
      <link>http://arxiv.org/abs/2505.12254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MMS-VPR，一个用于复杂、行人专用环境中的大规模多模态街级场所识别数据集。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉场所识别数据集主要依赖车载图像，缺乏多模态多样性，且在非西方城市环境中对密集、混合用途的街级空间的代表性不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些差距，提出MMS-VPR数据集，旨在提供多模态、适用于街级场所识别的数据。&lt;h4&gt;方法&lt;/h4&gt;数据集包含78,575张标注图像和2,512个视频片段，覆盖成都一个约70,800平方米的开放式商业区。每个图像都标注了精确的GPS坐标、时间戳和文本元数据。数据集采用系统化的数据收集协议，设备要求最低，以降低大规模数据集创建的门槛。数据集形成一个包含125条边、81个节点和1个子图的固有空间图，支持结构感知的场所识别。定义了两个应用特定子集——Dataset_Edges和Dataset_Points，以支持细粒度和基于图的评价任务。&lt;h4&gt;主要发现&lt;/h4&gt;使用传统VPR模型、图神经网络和多模态基线进行的大量基准测试表明，利用多模态和结构线索可以显著提高性能。&lt;h4&gt;结论&lt;/h4&gt;MMS-VPR促进了计算机视觉、地理空间理解和多模态推理交叉领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing visual place recognition (VPR) datasets predominantly rely onvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,mixed-use street-level spaces, especially in non-Western urban contexts. Toaddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset forstreet-level place recognition in complex, pedestrian-only environments. Thedataset comprises 78,575 annotated images and 2,512 video clips captured across207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district inChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,and textual metadata, and covers varied lighting conditions, viewpoints, andtimeframes. MMS-VPR follows a systematic and replicable data collectionprotocol with minimal device requirements, lowering the barrier for scalabledataset creation. Importantly, the dataset forms an inherent spatial graph with125 edges, 81 nodes, and 1 subgraph, enabling structure-aware placerecognition. We further define two application-specific subsets --Dataset_Edges and Dataset_Points -- to support fine-grained and graph-basedevaluation tasks. Extensive benchmarks using conventional VPR models, graphneural networks, and multimodal baselines show substantial improvements whenleveraging multimodal and structural cues. MMS-VPR facilitates future researchat the intersection of computer vision, geospatial understanding, andmultimodal reasoning. The dataset is publicly available athttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.</description>
      <author>example@mail.com (Yiwei Ou, Xiaobin Ren, Ronggui Sun, Guansong Gao, Ziyi Jiang, Kaiqi Zhao, Manfredo Manfredini)</author>
      <guid isPermaLink="false">2505.12254v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Capabilities of Molecular Graph Neural Networks in Materials Science Through Multimodal Learning and Physical Context Encoding</title>
      <link>http://arxiv.org/abs/2505.12137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted Spotlight Paper at CVPR 2025 for MM4Mat&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种多模态框架，结合分子图和文本描述符（如IUPAC名称、分子式、物化性质和同义词）来提高分子图神经网络（GNNs）的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的分子图神经网络主要关注基于XYZ的几何表示，忽视了公共数据库如PubChem中可用的化学上下文信息。&lt;h4&gt;目的&lt;/h4&gt;引入多模态框架，通过集成文本描述符和分子图，以及一个门控融合机制，来提高模型对互补信息的利用。&lt;h4&gt;方法&lt;/h4&gt;采用门控融合机制平衡几何和文本特征，通过实验在基准数据集上验证该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;添加文本数据对某些电子性质有显著改善，而对其他性质的影响有限。GNN架构显示出相似的性能模式，表明它们学习到了相似而不是不同的物理洞察。&lt;h4&gt;结论&lt;/h4&gt;该多模态框架能够提高GNN的性能，尤其是在处理化学性质方面，但提升效果仍有待提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular graph neural networks (GNNs) often focus exclusively on XYZ-basedgeometric representations and thus overlook valuable chemical context availablein public databases like PubChem. This work introduces a multimodal frameworkthat integrates textual descriptors, such as IUPAC names, molecular formulas,physicochemical properties, and synonyms, alongside molecular graphs. A gatedfusion mechanism balances geometric and textual features, allowing models toexploit complementary information. Experiments on benchmark datasets indicatethat adding textual data yields notable improvements for certain electronicproperties, while gains remain limited for others. Furthermore, the GNNarchitectures display similar performance patterns (improving and deterioratingon analogous targets), suggesting they learn comparable representations ratherthan distinctly different physical insights.</description>
      <author>example@mail.com (Can Polat, Hasan Kurban, Erchin Serpedin, Mustafa Kurban)</author>
      <guid isPermaLink="false">2505.12137v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies</title>
      <link>http://arxiv.org/abs/2505.12404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Hyperbolic Residual Quantization（HRQ）方法，用于生成具有潜在层次结构的数据的离散层次表示，并评估其在层次建模和层次发现任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;层次数据在众多领域出现，如生物分类、组织结构、法律代码和知识图谱。传统的Residual Quantization（RQ）方法依赖于欧几里得几何，这在处理层次结构时可能引入根本性的不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出HRQ方法，通过在双曲空间中嵌入数据和执行双曲操作来改进层次数据的表示。&lt;h4&gt;方法&lt;/h4&gt;HRQ通过在双曲流形中嵌入数据，并使用双曲操作和距离度量进行残差量化，从而自然地与层次分支对齐。&lt;h4&gt;主要发现&lt;/h4&gt;HRQ在监督层次建模和层次发现任务上优于传统的欧几里得RQ，特别是在层次建模任务上，性能提升可达20%。&lt;h4&gt;结论&lt;/h4&gt;将双曲几何引入离散表示学习可以显著提高捕获潜在层次结构的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：层次数据在无数领域出现，从生物分类和组织结构到法律代码和知识图谱。残差量化（RQ）被广泛用于通过迭代量化多级代码簿中的残差来生成此类数据的离散多令牌表示。然而，它对欧几里得几何的依赖可能导致与层次分支建模的基本不匹配，这对于忠实表示层次数据是必要的。在这项工作中，我们提出了超双曲残差量化（HRQ），它将数据原生地嵌入到超双曲流形中，并使用超双曲操作和距离度量进行残差量化。通过将嵌入网络、残差计算和距离度量适配到超双曲几何，HRQ赋予了一种与层次分支自然对齐的归纳偏好。我们声称，与RQ相比，HRQ可以为具有潜在层次结构的数据生成更有用的下游任务离散层次表示。我们在两个任务上评估了HRQ：使用WordNet同义词树的监督层次建模，其中模型被监督以学习潜在层次结构，以及层次发现，其中数据中存在潜在层次结构，但模型并未直接在涉及层次结构的任务上训练或评估。在两种情况下，HRQ层次标记在下游任务上的表现都优于欧几里得RQ，在层次建模任务上的提升可达20%。我们的结果表明，将双曲几何整合到离散表示学习中有助于显著提高捕获潜在层次结构的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical data arise in countless domains, from biological taxonomies andorganizational charts to legal codes and knowledge graphs. ResidualQuantization (RQ) is widely used to generate discrete, multitokenrepresentations for such data by iteratively quantizing residuals in amultilevel codebook. However, its reliance on Euclidean geometry can introducefundamental mismatches that hinder modeling of hierarchical branching,necessary for faithful representation of hierarchical data. In this work, wepropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in ahyperbolic manifold and performs residual quantization using hyperbolicoperations and distance metrics. By adapting the embedding network, residualcomputation, and distance metric to hyperbolic geometry, HRQ imparts aninductive bias that aligns naturally with hierarchical branching. We claim thatHRQ in comparison to RQ can generate more useful for downstream tasks discretehierarchical representations for data with latent hierarchies. We evaluate HRQon two tasks: supervised hierarchy modeling using WordNet hypernym trees, wherethe model is supervised to learn the latent hierarchy - and hierarchydiscovery, where, while latent hierarchy exists in the data, the model is notdirectly trained or evaluated on a task related to the hierarchy. Across bothscenarios, HRQ hierarchical tokens yield better performance on downstream taskscompared to Euclidean RQ with gains of up to $20\%$ for the hierarchy modelingtask. Our results demonstrate that integrating hyperbolic geometry intodiscrete representation learning substantially enhances the ability to capturelatent hierarchies.</description>
      <author>example@mail.com (Piotr Piękos, Subhradeep Kayal, Alexandros Karatzoglou)</author>
      <guid isPermaLink="false">2505.12404v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2505.12136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LSTAN-GERPE的新型交通预测模型，该模型结合了时空注意机制和图嵌入技术，通过优化旋转位置编码频率来提高交通预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;交通预测是智能交通系统中的一个关键任务，现有研究主要聚焦于将图神经网络（GNN）与其他模型结合，但GNN仅考虑短程空间信息。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效捕捉长程交通动态的新型交通预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出的模型LSTAN-GERPE结合了时间和空间注意机制，并通过网格搜索优化旋转位置编码的频率。模型还将地理位置图整合到时空嵌入中，以提高特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统优化，LSTAN-GERPE模型能够有效地捕捉复杂的交通模式，并在PeMS04和PeMS08等真实世界交通预测数据集上取得了较高的准确性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在不进行大量特征工程的情况下，实现了较高的交通预测准确性，为智能交通系统的进一步发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Traffic forecasting is a key task in the field of Intelligent Transportation Systems. Recent research on traffic forecasting has mainly focused on combining graph neural networks (GNNs) with other models. However, GNNs only considers short-range spatial information. In this study, we present a novel model termed LSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding). This model leverages both Temporal and Spatial Attention mechanisms to effectively capture long-range traffic dynamics. Additionally, the optimal frequency for rotational position encoding is determined through a grid search approach in both the spatial and temporal attention mechanisms. This systematic optimization enables the model to effectively capture complex traffic patterns. The model also enhances feature representation by incorporating geographical location maps into the spatio-temporal embeddings. Without extensive feature engineering, the proposed method in this paper achieves advanced accuracy on the real-world traffic forecasting datasets PeMS04 and PeMS08.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting is a key task in the field of Intelligent TransportationSystems. Recent research on traffic forecasting has mainly focused on combininggraph neural networks (GNNs) with other models. However, GNNs only considershort-range spatial information. In this study, we present a novel model termedLSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embeddingand Rotational Position Encoding). This model leverages both Temporal andSpatial Attention mechanisms to effectively capture long-range trafficdynamics. Additionally, the optimal frequency for rotational position encodingis determined through a grid search approach in both the spatial and temporalattention mechanisms. This systematic optimization enables the model toeffectively capture complex traffic patterns. The model also enhances featurerepresentation by incorporating geographical location maps into thespatio-temporal embeddings. Without extensive feature engineering, the proposedmethod in this paper achieves advanced accuracy on the real-world trafficforecasting datasets PeMS04 and PeMS08.</description>
      <author>example@mail.com (Xiao Wang, Shun-Ren Yang)</author>
      <guid isPermaLink="false">2505.12136v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures</title>
      <link>http://arxiv.org/abs/2505.11918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at  https://github.com/Rorschach1989/transformer-for-gmm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了Transformer在解决高斯混合模型（GMM）问题上的能力，提出了一种基于Transformer的学习框架TGMM，并展示了其在无监督学习中的潜力。&lt;h4&gt;背景&lt;/h4&gt;Transformer架构在人工智能领域展现出卓越的能力，其中隐式学习内部模型的能力被认为对理解预训练大型语言模型至关重要。然而，最近的研究主要集中在监督学习问题上，无监督学习领域相对未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究Transformer在解决GMM问题上的能力，并构建一个能够同时解决多个GMM任务的Transformer学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为TGMM的Transformer学习框架，通过共享的Transformer骨干网络学习解决多个GMM任务，并通过实验验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;TGMM在解决GMM任务上表现出色，有效缓解了传统方法如EM算法或谱算法的局限性，同时对分布变化表现出合理的鲁棒性。理论上证明了Transformer可以近似EM算法和谱方法的核心组件。&lt;h4&gt;结论&lt;/h4&gt;Transformer在无监督学习领域具有广泛的应用潜力，可以作为解决GMM等问题的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the understanding of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rorschach1989/transformer-for-gmm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The transformer architecture has demonstrated remarkable capabilities inmodern artificial intelligence, among which the capability of implicitlylearning an internal model during inference time is widely believed to play akey role in the under standing of pre-trained large language models. However,most recent works have been focusing on studying supervised learning topicssuch as in-context learning, leaving the field of unsupervised learning largelyunexplored. This paper investigates the capabilities of transformers in solvingGaussian Mixture Models (GMMs), a fundamental unsupervised learning problemthrough the lens of statistical estimation. We propose a transformer-basedlearning framework called TGMM that simultaneously learns to solve multiple GMMtasks using a shared transformer backbone. The learned models are empiricallydemonstrated to effectively mitigate the limitations of classical methods suchas Expectation-Maximization (EM) or spectral algorithms, at the same timeexhibit reasonable robustness to distribution shifts. Theoretically, we provethat transformers can approximate both the EM algorithm and a core component ofspectral methods (cubic tensor power iterations). These results bridge the gapbetween practical success and theoretical understanding, positioningtransformers as versatile tools for unsupervised learning.</description>
      <author>example@mail.com (Zhiheng Chen, Ruofan Wu, Guanhua Fang)</author>
      <guid isPermaLink="false">2505.11918v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Ripple: Scalable Incremental GNN Inferencing on Large Streaming Graphs</title>
      <link>http://arxiv.org/abs/2505.12112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint of paper to appear in the proceedings of the 45th IEEE  International Conference on Distributed Computing Systems (ICDCS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Ripple的框架，用于在大型动态图上进行高效的GNN推理。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的图通常是动态的，频繁的图拓扑和顶点边属性更新对GNN推理构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对动态图中的这些挑战，本文旨在提出一个既高效又准确的流式GNN推理框架。&lt;h4&gt;方法&lt;/h4&gt;Ripple框架通过利用GNN中底层聚合函数的性质，实现了因图拓扑或顶点特征更新而引起的嵌入的快速增量更新。&lt;h4&gt;主要发现&lt;/h4&gt;Ripple在单机上的性能表现优异，对于稀疏图如Arxiv能达到约28000次更新/秒，而对于更大且更密集的图如Products也能达到约1200次更新/秒，并且延迟在0.1毫秒到1秒之间，适合近实时应用。分布式版本的Ripple在更新期间通信成本降低了70倍，因此提供了比基线高约30倍的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;Ripple框架为动态图上的GNN推理提供了一种高效且准确的方法，特别是在分布式设置中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为Ripple的框架，用于在大型动态图上进行高效的GNN推理。现实世界中的图通常是动态的，频繁的图拓扑和顶点边属性更新对GNN推理构成了挑战。为了应对动态图中的这些挑战，本文旨在提出一个既高效又准确的流式GNN推理框架。Ripple框架通过利用GNN中底层聚合函数的性质，实现了因图拓扑或顶点特征更新而引起的嵌入的快速增量更新。Ripple在单机上的性能表现优异，对于稀疏图如Arxiv能达到约28000次更新/秒，而对于更大且更密集的图如Products也能达到约1200次更新/秒，并且延迟在0.1毫秒到1秒之间，适合近实时应用。分布式版本的Ripple在更新期间通信成本降低了70倍，因此提供了比基线高约30倍的吞吐量。Ripple框架为动态图上的GNN推理提供了一种高效且准确的方法，特别是在分布式设置中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most real-world graphs are dynamic in nature, with continuous and rapidupdates to the graph topology, and vertex and edge properties. Such frequentupdates pose significant challenges for inferencing over Graph Neural Networks(GNNs). Current approaches that perform vertex-wise and layer-wise inferencingare impractical for dynamic graphs as they cause redundant computations, expandto large neighborhoods, and incur high communication costs for distributedsetups, resulting in slow update propagation that often exceeds real-timelatency requirements. This motivates the need for streaming GNN inferenceframeworks that are efficient and accurate over large, dynamic graphs. Wepropose Ripple, a framework that performs fast incremental updates ofembeddings arising due to updates to the graph topology or vertex features.Ripple provides a generalized incremental programming model, leveraging theproperties of the underlying aggregation functions employed by GNNs toefficiently propagate updates to the affected neighborhood and compute theexact new embeddings. Besides a single-machine design, we also extend thisexecution model to distributed inferencing, to support large graphs that do notfit in a single machine's memory. Ripple on a single machine achieves up to$\approx28000$ updates/sec for sparse graphs like Arxiv and $\approx1200$updates/sec for larger and denser graphs like Products, with latencies of$0.1$ms--$1$s that are required for near-realtime applications. The distributedversion of Ripple offers up to $\approx30\times$ better throughput over thebaselines, due to $70\times$ lower communication costs during updates.</description>
      <author>example@mail.com (Pranjal Naman, Yogesh Simmhan)</author>
      <guid isPermaLink="false">2505.12112v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement</title>
      <link>http://arxiv.org/abs/2505.11822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的跨视角地理定位框架CVD，旨在解决由视角差异引起的显著外观变化和空间畸变带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;跨视角地理定位（CVGL）旨在匹配来自不同视角（如无人机和卫星）的同一地理位置的图像，尽管近年来取得了进展，但CVGL仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出一种新的框架来明确分离内容和视角因素，以解决视角差异引起的内在冲突，从而提高定位精度。&lt;h4&gt;方法&lt;/h4&gt;采用流形学习方法，将跨视角图像的特征空间建模为受内容和视角信息共同控制的复合流形。CVD框架引入了两个约束条件：内视图独立性约束和跨视图重建约束。&lt;h4&gt;主要发现&lt;/h4&gt;CVD框架能够有效分离内容和视角因素，并通过实验证明了其在多个基准数据集上的定位精度和泛化能力的提升。&lt;h4&gt;结论&lt;/h4&gt;CVD框架可以无缝集成到现有的地理定位流程中，并在多个基准数据集上显著提高了定位精度和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Cross-view geo-localization (CVGL) aims to match images of the same geographic location captured from different perspectives, such as drones and satellites. Despite recent advances, CVGL remains highly challenging due to significant appearance changes and spatial distortions caused by viewpoint variations. Existing methods typically assume that cross-view images can be directly aligned within a shared feature space by maximizing feature similarity through contrastive learning. Nonetheless, this assumption overlooks the inherent conflicts induced by viewpoint discrepancies, resulting in extracted features containing inconsistent information that hinders precise localization. In this study, we take a manifold learning perspective and model the featurespace of cross-view images as a composite manifold jointly governed by content and viewpoint information. Building upon this insight, we propose CVD, a new CVGL framework that explicitly disentangles content and viewpoint factors. To promote effective disentanglement, we introduce two constraints: (i) An intra-view independence constraint, which encourages statistical independence between the two factors by minimizing their mutual information. (ii) An inter-view reconstruction constraint that reconstructs each view by cross-combining content and viewpoint from paired images, ensuring factor-specific semantics are preserved. As a plug-and-play module, CVD can be seamlessly integrated into existing geo-localization pipelines. Extensive experiments on four benchmarks, i.e., University-1652, SUES-200, CVUSA, and CVACT, demonstrate that CVD consistently improves both localization accuracy and generalization across multiple baselines.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-view geo-localization (CVGL) aims to match images of the samegeographic location captured from different perspectives, such as drones andsatellites. Despite recent advances, CVGL remains highly challenging due tosignificant appearance changes and spatial distortions caused by viewpointvariations. Existing methods typically assume that cross-view images can bedirectly aligned within a shared feature space by maximizing feature similaritythrough contrastive learning. Nonetheless, this assumption overlooks theinherent conflicts induced by viewpoint discrepancies, resulting in extractedfeatures containing inconsistent information that hinders precise localization.In this study, we take a manifold learning perspective and model the featurespace of cross-view images as a composite manifold jointly governed by contentand viewpoint information. Building upon this insight, we propose$\textbf{CVD}$, a new CVGL framework that explicitly disentangles$\textit{content}$ and $\textit{viewpoint}$ factors. To promote effectivedisentanglement, we introduce two constraints: $\textit{(i)}$ An intra-viewindependence constraint, which encourages statistical independence between thetwo factors by minimizing their mutual information. $\textit{(ii)}$ Aninter-view reconstruction constraint that reconstructs each view bycross-combining $\textit{content}$ and $\textit{viewpoint}$ from paired images,ensuring factor-specific semantics are preserved. As a plug-and-play module,CVD can be seamlessly integrated into existing geo-localization pipelines.Extensive experiments on four benchmarks, i.e., University-1652, SUES-200,CVUSA, and CVACT, demonstrate that CVD consistently improves both localizationaccuracy and generalization across multiple baselines.</description>
      <author>example@mail.com (Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, Quan Wang)</author>
      <guid isPermaLink="false">2505.11822v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种严格的熵力理论，用于理解使用随机梯度下降（SGD）及其变体训练的神经网络的动态学习过程。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习和大型语言模型中涌现现象的快速发现，解释和理解其成因已成为迫切需要。&lt;h4&gt;目的&lt;/h4&gt;提出一种理论来解释和理解神经网络学习动态。&lt;h4&gt;方法&lt;/h4&gt;基于参数对称性和非熵损失景观理论，展示了表示学习由随机性和离散时间更新引起的涌现熵力所控制。&lt;h4&gt;主要发现&lt;/h4&gt;这些力系统地打破连续参数对称性并保持离散对称性，导致一系列类似于热系统等分性质的重力平衡现象。这些现象（a）解释了AI模型之间神经表示的普遍一致性，并证明了柏拉图表示假设；（b）调和了深度学习优化中寻求尖锐和平坦行为的看似矛盾观察。&lt;h4&gt;结论&lt;/h4&gt;熵力和对称性破坏的结合是理解深度学习中涌现现象的关键。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着深度学习和大型语言模型中涌现现象的快速发现，解释和理解其成因已成为迫切需要。在这里，我们提出了一种严格的熵力理论来理解使用随机梯度下降（SGD）及其变体训练的神经网络的动态学习过程。基于参数对称性和非熵损失景观理论，我们表明表示学习被由随机性和离散时间更新引起的涌现熵力所关键控制。这些力系统地打破连续参数对称性并保持离散对称性，导致一系列类似于热系统等分性质的重力平衡现象。这些现象反过来（a）解释了AI模型之间神经表示的普遍一致性，并证明了柏拉图表示假设；（b）调和了深度学习优化中寻求尖锐和平坦行为的看似矛盾观察。我们的理论和实验表明，熵力和对称性破坏的结合是理解深度学习中涌现现象的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid discovery of emergent phenomena in deep learning and largelanguage models, explaining and understanding their cause has become an urgentneed. Here, we propose a rigorous entropic-force theory for understanding thelearning dynamics of neural networks trained with stochastic gradient descent(SGD) and its variants. Building on the theory of parameter symmetries and anentropic loss landscape, we show that representation learning is cruciallygoverned by emergent entropic forces arising from stochasticity anddiscrete-time updates. These forces systematically break continuous parametersymmetries and preserve discrete ones, leading to a series of gradient balancephenomena that resemble the equipartition property of thermal systems. Thesephenomena, in turn, (a) explain the universal alignment of neuralrepresentations between AI models and lead to a proof of the PlatonicRepresentation Hypothesis, and (b) reconcile the seemingly contradictoryobservations of sharpness- and flatness-seeking behavior of deep learningoptimization. Our theory and experiments demonstrate that a combination ofentropic forces and symmetry breaking is key to understanding emergentphenomena in deep learning.</description>
      <author>example@mail.com (Liu Ziyin, Yizhou Xu, Isaac Chuang)</author>
      <guid isPermaLink="false">2505.12387v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Residual Feature Integration is Sufficient to Prevent Negative Transfer</title>
      <link>http://arxiv.org/abs/2505.11771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Residual Feature Integration (REFINE)的简单有效的方法，旨在减轻迁移学习中的负迁移问题。&lt;h4&gt;背景&lt;/h4&gt;迁移学习通常利用源域学习到的表示来提高目标任务的性能。然而，直接应用预训练模型提取的特征可能导致负迁移，即源域表示与目标分布不匹配。&lt;h4&gt;目的&lt;/h4&gt;提出REFINE方法，以减轻迁移学习中的负迁移。&lt;h4&gt;方法&lt;/h4&gt;REFINE方法结合了固定的源域表示和可训练的目标域编码器，并在联合表示上拟合一个浅层神经网络，以适应目标域同时保留源域的可迁移知识。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，证明了在轻微条件下REFINE足以防止负迁移，并推导了泛化界限以展示其理论优势。实验表明，REFINE在视觉、文本和表格数据等多种应用和数据模态中一致地提高了性能，并优于许多替代方案。&lt;h4&gt;结论&lt;/h4&gt;REFINE方法轻量级、架构无关且鲁棒，是现有迁移学习工具箱中的一个有价值的补充。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning typically leverages representations learned from a sourcedomain to improve performance on a target task. A common approach is to extractfeatures from a pre-trained model and directly apply them for targetprediction. However, this strategy is prone to negative transfer where thesource representation fails to align with the target distribution. In thisarticle, we propose Residual Feature Integration (REFINE), a simple yeteffective method designed to mitigate negative transfer. Our approach combinesa fixed source-side representation with a trainable target-side encoder andfits a shallow neural network on the resulting joint representation, whichadapts to the target domain while preserving transferable knowledge from thesource domain. Theoretically, we prove that REFINE is sufficient to preventnegative transfer under mild conditions, and derive the generalization bounddemonstrating its theoretical benefit. Empirically, we show that REFINEconsistently enhances performance across diverse application and datamodalities including vision, text, and tabular data, and outperforms numerousalternative solutions. Our method is lightweight, architecture-agnostic, androbust, making it a valuable addition to the existing transfer learningtoolbox.</description>
      <author>example@mail.com (Yichen Xu, Ryumei Nakada, Linjun Zhang, Lexin Li)</author>
      <guid isPermaLink="false">2505.11771v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Generative and Contrastive Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2505.11776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图自监督学习架构，该架构结合了对比学习和生成学习的优势，在节点分类、节点聚类和链接预测等任务上实现了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在图上生成节点和图表示（即嵌入），可用于下游任务。在有限或没有标记数据的场景中，图自监督学习特别有用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图自监督学习架构，以提高节点分类、聚类和链接预测等任务的性能。&lt;h4&gt;方法&lt;/h4&gt;该架构引入了社区感知的节点级对比学习，以提供更稳健和有效的正负节点对生成，同时结合图级对比学习来捕获全局语义信息。此外，采用了一种综合的增强策略，结合特征掩码、节点扰动和边扰动，以实现稳健和多样化的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在多个任务上实现了优越的性能，包括节点分类、聚类和链接预测。在公开基准数据集上的评估表明，该模型优于最先进的方法，性能提升在0.23%-2.01%之间，具体取决于任务和数据集。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型通过结合对比学习和生成学习的优势，在图自监督学习领域取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) on graphs generates node and graphrepresentations (i.e., embeddings) that can be used for downstream tasks suchas node classification, node clustering, and link prediction. Graph SSL isparticularly useful in scenarios with limited or no labeled data. Existing SSLmethods predominantly follow contrastive or generative paradigms, eachexcelling in different tasks: contrastive methods typically perform well onclassification tasks, while generative methods often excel in link prediction.In this paper, we present a novel architecture for graph SSL that integratesthe strengths of both approaches. Our framework introduces community-awarenode-level contrastive learning, providing more robust and effective positiveand negative node pairs generation, alongside graph-level contrastive learningto capture global semantic information. Additionally, we employ a comprehensiveaugmentation strategy that combines feature masking, node perturbation, andedge perturbation, enabling robust and diverse representation learning. Byincorporating these enhancements, our model achieves superior performanceacross multiple tasks, including node classification, clustering, and linkprediction. Evaluations on open benchmark datasets demonstrate that our modeloutperforms state-of-the-art methods, achieving a performance lift of0.23%-2.01% depending on the task and dataset.</description>
      <author>example@mail.com (Jiali Chen, Avijit Mukherjee)</author>
      <guid isPermaLink="false">2505.11776v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
      <link>http://arxiv.org/abs/2505.12332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VoiceCloak是一种针对扩散模型（DMs）的多维度主动防御框架，旨在混淆说话人身份并降低潜在未授权语音克隆的感知质量。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在现实语音克隆（VC）中取得了显著成功，但也增加了恶意滥用的风险。&lt;h4&gt;目的&lt;/h4&gt;VoiceCloak的目标是混淆说话人身份并降低潜在未授权VC的感知质量。&lt;h4&gt;方法&lt;/h4&gt;VoiceCloak通过分析DMs中的特定漏洞，在参考音频中引入对抗性扰动来干扰克隆过程。它通过扭曲表示学习嵌入来最大化身份变化，并干扰关键的条件引导过程，特别是注意力上下文。此外，VoiceCloak还引入了分数幅度放大和噪声引导语义破坏来降低输出质量。&lt;h4&gt;主要发现&lt;/h4&gt;VoiceCloak在防御未授权基于扩散的语音克隆方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;VoiceCloak是一种有效的防御框架，可以降低扩散模型在语音克隆中的恶意滥用风险。&lt;h4&gt;翻译&lt;/h4&gt;VoiceCloak is a multi-dimensional proactive defense framework for diffusion models (DMs) aimed at obfuscating speaker identity and degrading the perceptual quality in potential unauthorized voice cloning. The framework analyzes specific vulnerabilities within DMs to disrupt the cloning process by introducing adversarial perturbations into the reference audio. It distorts representation learning embeddings to maximize identity variation and disrupts crucial conditional guidance processes, particularly attention context. Additionally, it introduces score magnitude amplification and noise-guided semantic corruption to degrade output quality. Extensive experiments highlight the outstanding defense success rate of VoiceCloak against unauthorized diffusion-based voice cloning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion Models (DMs) have achieved remarkable success in realistic voicecloning (VC), while they also increase the risk of malicious misuse. Existingproactive defenses designed for traditional VC models aim to disrupt theforgery process, but they have been proven incompatible with DMs due to theintricate generative mechanisms of diffusion. To bridge this gap, we introduceVoiceCloak, a multi-dimensional proactive defense framework with the goal ofobfuscating speaker identity and degrading perceptual quality in potentialunauthorized VC. To achieve these goals, we conduct a focused analysis toidentify specific vulnerabilities within DMs, allowing VoiceCloak to disruptthe cloning process by introducing adversarial perturbations into the referenceaudio. Specifically, to obfuscate speaker identity, VoiceCloak first targetsspeaker identity by distorting representation learning embeddings to maximizeidentity variation, which is guided by auditory perception principles.Additionally, VoiceCloak disrupts crucial conditional guidance processes,particularly attention context, thereby preventing the alignment of vocalcharacteristics that are essential for achieving convincing cloning. Then, toaddress the second objective, VoiceCloak introduces score magnitudeamplification to actively steer the reverse trajectory away from the generationof high-quality speech. Noise-guided semantic corruption is further employed todisrupt structural speech semantics captured by DMs, degrading output quality.Extensive experiments highlight VoiceCloak's outstanding defense success rateagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloakare available at https://voice-cloak.github.io/VoiceCloak/.</description>
      <author>example@mail.com (Qianyue Hu, Junyan Wu, Wei Lu, Xiangyang Luo)</author>
      <guid isPermaLink="false">2505.12332v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Model alignment using inter-modal bridges</title>
      <link>http://arxiv.org/abs/2505.12322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于条件流匹配的半监督模型对齐方法，用于解决不同模态（如文本和视觉）之间的模型重用问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法在跨模态模型重用方面存在局限性，因为难以对齐内部表示，且需要大量配对训练数据或局限于特定领域。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种数据高效的跨模态模型对齐方法，以最小监督实现。&lt;h4&gt;方法&lt;/h4&gt;通过以下两种设置学习不同模态潜在空间之间的条件流：(1) 通过空间桥接成本解决平衡或不平衡的最优传输问题；(2) 使用标记的示例进行内存高效的对齐。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MNIST、ImageNet和majaj2015simple数据集上的对象识别和图像生成任务中，与端到端训练模型相比，在标记训练数据稀缺（&lt;20%）的情况下，匹配了下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为跨模态模型对齐提供了一种数据高效的解决方案，即使在少量监督下也能实现良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have demonstrated remarkable performance across modalitiessuch as language and vision. However, model reuse across distinct modalities(e.g., text and vision) remains limited due to the difficulty of aligninginternal representations. Existing methods require extensive paired trainingdata or are constrained to specific domains. We introduce a semi-supervisedapproach for model alignment via conditional flow matching. The conditionalflow between latent spaces of different modalities (e.g., text-to-image orbiological-to-artificial neuronal activity) can be learned in two settings:($1$) solving a (balanced or unbalanced) optimal transport problem with aninter-space bridge cost, and ($2$) performing memory-efficient alignment usinglabelled exemplars. Despite being constrained by the original models' capacity,our method--under both settings--matches downstream task performance ofend-to-end trained models on object recognition and image generation tasksacross MNIST, ImageNet, and \cite{majaj2015simple} datasets, particularly whenlabelled training data is scarce ($&lt;20\%$). Our method provides adata-efficient solution for inter-modal model alignment with minimalsupervision.</description>
      <author>example@mail.com (Ali Gholamzadeh, Noor Sajid)</author>
      <guid isPermaLink="false">2505.12322v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing</title>
      <link>http://arxiv.org/abs/2505.11743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大规模语言模型（LLM）的AI框架，用于智能云系统故障检测和自愈机制。&lt;h4&gt;背景&lt;/h4&gt;随着云计算系统和其基础设施的快速发展和复杂性增加，实时检测和缓解故障的智能机制变得日益重要。&lt;h4&gt;目的&lt;/h4&gt;研究目的是开发一种能够有效处理现代云环境规模和动态的传统故障检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了现有的机器学习故障检测算法和LLM的自然语言理解能力，通过语义上下文处理和解析系统日志、错误报告和实时数据流。&lt;h4&gt;主要发现&lt;/h4&gt;该模型采用多层次架构，结合监督学习进行故障分类和无监督学习进行异常检测，能够在故障发生前预测潜在故障并自动触发自愈机制。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，所提出的模型在故障检测精度、系统停机时间减少和恢复速度方面均显著优于传统故障检测系统。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of cloud computing systems and the increasing complexity of their infrastructure, intelligent mechanisms to detect and mitigate failures in real time are becoming increasingly important. Traditional methods of failure detection are often difficult to cope with the scale and dynamics of modern cloud environments. In this study, we propose a novel AI framework based on Massive Language Model (LLM) for intelligent fault detection and self-healing mechanisms in cloud systems. The model combines existing machine learning fault detection algorithms with LLM's natural language understanding capabilities to process and parse system logs, error reports, and real-time data streams through semantic context. The method adopts a multi-level architecture, combined with supervised learning for fault classification and unsupervised learning for anomaly detection, so that the system can predict potential failures before they occur and automatically trigger the self-healing mechanism. Experimental results show that the proposed model is significantly better than the traditional fault detection system in terms of fault detection accuracy, system downtime reduction and recovery speed.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of cloud computing systems and the increasingcomplexity of their infrastructure, intelligent mechanisms to detect andmitigate failures in real time are becoming increasingly important. Traditionalmethods of failure detection are often difficult to cope with the scale anddynamics of modern cloud environments. In this study, we propose a novel AIframework based on Massive Language Model (LLM) for intelligent fault detectionand self-healing mechanisms in cloud systems. The model combines existingmachine learning fault detection algorithms with LLM's natural languageunderstanding capabilities to process and parse system logs, error reports, andreal-time data streams through semantic context. The method adopts amulti-level architecture, combined with supervised learning for faultclassification and unsupervised learning for anomaly detection, so that thesystem can predict potential failures before they occur and automaticallytrigger the self-healing mechanism. Experimental results show that the proposedmodel is significantly better than the traditional fault detection system interms of fault detection accuracy, system downtime reduction and recoveryspeed.</description>
      <author>example@mail.com (Cheng Ji, Huaiying Luo)</author>
      <guid isPermaLink="false">2505.11743v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning</title>
      <link>http://arxiv.org/abs/2505.12233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI2025 early accept&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PRETI的视网膜基础模型，通过集成元数据感知学习和鲁棒的自监督表示学习，显著提高了视网膜图像分析的能力。&lt;h4&gt;背景&lt;/h4&gt;视网膜图像分析在疾病诊断中非常重要，但依赖大量标注数据且获取临床报告成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合患者特定信息并减少对标注数据依赖的视网膜图像分析模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Learnable Metadata Embedding (LME)的元数据嵌入方法，以及一个名为Retina-Aware Adaptive Masking (RAAM)的策略。此外，构建了患者级别的数据对来提高模型对非临床变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;PRETI模型能够捕捉视网膜图像的全球结构和精细病理细节，在多种疾病和生物标志物预测任务中实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;元数据指导的基础模型在视网膜疾病分析中具有重要意义，PRETI模型展示了其在不同疾病和生物标志物预测中的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：视网膜基础模型通过利用自监督学习显著推进了视网膜图像分析，减少了对标注数据的依赖并实现了强大的泛化能力。许多最近的方法通过报告监督增强了视网膜图像的理解，但获取临床报告往往成本高且具有挑战性。相比之下，元数据（如年龄、性别）广泛可用，是分析疾病进展的有价值资源。为了有效地整合患者特定的信息，我们提出了PRETI，一种集成元数据感知学习与鲁棒的自监督表示学习的视网膜基础模型。我们引入了可学习的元数据嵌入（LME），它可以动态地细化元数据表示。此外，我们构建了患者级别的数据对，将同一个体的图像关联起来以提高对非临床变化的鲁棒性。为了进一步优化视网膜图像表示，我们提出了视网膜感知自适应掩码（RAAM）策略，该策略在视网膜区域内选择性地应用掩码并在训练过程中动态调整掩码比率。PRETI能够捕捉全局结构和精细病理细节，从而实现了优越的诊断性能。广泛的实验表明，PRETI在内部和公共数据上实现了最先进的跨多种疾病和生物标志物预测结果，表明元数据引导的基础模型在视网膜疾病分析中的重要性。我们的代码和预训练模型可在https://github.com/MICV-yonsei/PRETI上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retinal foundation models have significantly advanced retinal image analysisby leveraging self-supervised learning to reduce dependence on labeled datawhile achieving strong generalization. Many recent approaches enhance retinalimage understanding using report supervision, but obtaining clinical reports isoften costly and challenging. In contrast, metadata (e.g., age, gender) iswidely available and serves as a valuable resource for analyzing diseaseprogression. To effectively incorporate patient-specific information, wepropose PRETI, a retinal foundation model that integrates metadata-awarelearning with robust self-supervised representation learning. We introduceLearnable Metadata Embedding (LME), which dynamically refines metadatarepresentations. Additionally, we construct patient-level data pairs,associating images from the same individual to improve robustness againstnon-clinical variations. To further optimize retinal image representation, wepropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectivelyapplies masking within the retinal region and dynamically adjusts the maskingratio during training. PRETI captures both global structures and fine-grainedpathological details, resulting in superior diagnostic performance. Extensiveexperiments demonstrate that PRETI achieves state-of-the-art results acrossdiverse diseases and biomarker predictions using in-house and public data,indicating the importance of metadata-guided foundation models in retinaldisease analysis. Our code and pretrained model are available athttps://github.com/MICV-yonsei/PRETI</description>
      <author>example@mail.com (Yeonkyung Lee, Woojung Han, Youngjun Jun, Hyeonmin Kim, Jungkyung Cho, Seong Jae Hwang)</author>
      <guid isPermaLink="false">2505.12233v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology</title>
      <link>http://arxiv.org/abs/2505.12120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HISTAI数据集，这是一个大型的、多模态的开放访问的病理图像数据集，旨在解决现有公开数据集规模不足、组织多样性不足和临床元数据不全面的问题。&lt;h4&gt;背景&lt;/h4&gt;数字病理学（DP）领域通过人工智能和基础模型取得了进展，强调了大规模、多样化和丰富注释数据集的重要性。然而，现有的公开全切片图像（WSI）数据集往往缺乏足够的规模、组织多样性和全面的临床元数据，限制了AI模型的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;引入HISTAI数据集，以填补现有资源的空白，促进创新、可重复性和临床相关计算病理学解决方案的发展。&lt;h4&gt;方法&lt;/h4&gt;HISTAI数据集包含超过60,000张来自各种组织类型的切片，每个案例都伴随着广泛的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。&lt;h4&gt;主要发现&lt;/h4&gt;HISTAI数据集提供了大规模、多样化和丰富注释的病理图像，有助于提高AI模型的性能和临床应用价值。&lt;h4&gt;结论&lt;/h4&gt;HISTAI数据集的发布为数字病理学领域提供了宝贵的资源，有助于推动相关研究的进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，数字病理学（DP）领域，特别是通过人工智能和基础模型，强调了大规模、多样化和丰富注释数据集的重要性。尽管它们起着关键作用，但公开可用的全切片图像（WSI）数据集通常缺乏足够的规模、组织多样性和全面的临床元数据，限制了AI模型的鲁棒性和泛化能力。为此，我们引入了HISTAI数据集，这是一个大型、多模态、开放访问的WSI数据集，包含来自各种组织类型的60,000多张切片。HISTAI数据集中的每个案例都伴随着广泛的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。该数据集旨在填补现有资源的空白，促进创新、可重复性和临床相关计算病理学解决方案的发展。数据集可通过https://github.com/HistAI/HISTAI访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/histai/histai&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Digital Pathology (DP), particularly throughartificial intelligence and Foundation Models, have underscored the importanceof large-scale, diverse, and richly annotated datasets. Despite their criticalrole, publicly available Whole Slide Image (WSI) datasets often lack sufficientscale, tissue diversity, and comprehensive clinical metadata, limiting therobustness and generalizability of AI models. In response, we introduce theHISTAI dataset, a large, multimodal, open-access WSI collection comprising over60,000 slides from various tissue types. Each case in the HISTAI dataset isaccompanied by extensive clinical metadata, including diagnosis, demographicinformation, detailed pathological annotations, and standardized diagnosticcoding. The dataset aims to fill gaps identified in existing resources,promoting innovation, reproducibility, and the development of clinicallyrelevant computational pathology solutions. The dataset can be accessed athttps://github.com/HistAI/HISTAI.</description>
      <author>example@mail.com (Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova)</author>
      <guid isPermaLink="false">2505.12120v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics</title>
      <link>http://arxiv.org/abs/2505.11930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了逻辑对时间图神经网络（Temporal GNNs）的表征，通过将它们与二维乘积逻辑联系起来，揭示了时间GNNs的表达能力取决于图和时序组件的结合方式。&lt;h4&gt;背景&lt;/h4&gt;近年来，逻辑和形式语言理论工具被用来描述各种神经网络架构的表达能力，包括图神经网络（GNNs）、Transformer和循环神经网络。随着基本架构能力的逐渐明确，越来越多的关注转向结合多种架构范式的模型。&lt;h4&gt;目的&lt;/h4&gt;本文旨在研究时间GNNs的逻辑表征，并分析其表达能力。&lt;h4&gt;方法&lt;/h4&gt;通过将时间GNNs与二维乘积逻辑相联系，分析了不同时间GNN架构的表达能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，应用静态GNNs递归地随时间变化的时序GNNs可以捕捉所有在命题时态逻辑PTL和模态逻辑K中定义的属性。而像图-时TGNN和全局TGNN这样的架构只能表达这个逻辑的子集，其中时序和空间操作符之间的交互在语法上受到限制。&lt;h4&gt;结论&lt;/h4&gt;这些结果为时间GNNs提供了首次逻辑表征，并确立了时间GNNs的新相对表达能力结果。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, the expressive power of various neural architectures --including graph neural networks (GNNs), transformers, and recurrent neuralnetworks -- has been characterised using tools from logic and formal languagetheory. As the capabilities of basic architectures are becoming wellunderstood, increasing attention is turning to models that combine multiplearchitectural paradigms. Among them particularly important, and challenging toanalyse, are temporal extensions of GNNs, which integrate both spatial(graph-structure) and temporal (evolution over time) dimensions. In this paper,we initiate the study of logical characterisation of temporal GNNs byconnecting them to two-dimensional product logics. We show that the expressivepower of temporal GNNs depends on how graph and temporal components arecombined. In particular, temporal GNNs that apply static GNNs recursively overtime can capture all properties definable in the product logic of (past)propositional temporal logic PTL and the modal logic K. In contrast,architectures such as graph-and-time TGNNs and global TGNNs can only expressrestricted fragments of this logic, where the interaction between temporal andspatial operators is syntactically constrained. These results yield the firstlogical characterisations of temporal GNNs and establish new relativeexpressiveness results for temporal GNNs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the expressive power of various neural architectures --including graph neural networks (GNNs), transformers, and recurrent neuralnetworks -- has been characterised using tools from logic and formal languagetheory. As the capabilities of basic architectures are becoming wellunderstood, increasing attention is turning to models that combine multiplearchitectural paradigms. Among them particularly important, and challenging toanalyse, are temporal extensions of GNNs, which integrate both spatial(graph-structure) and temporal (evolution over time) dimensions. In this paper,we initiate the study of logical characterisation of temporal GNNs byconnecting them to two-dimensional product logics. We show that the expressivepower of temporal GNNs depends on how graph and temporal components arecombined. In particular, temporal GNNs that apply static GNNs recursively overtime can capture all properties definable in the product logic of (past)propositional temporal logic PTL and the modal logic K. In contrast,architectures such as graph-and-time TGNNs and global TGNNs can only expressrestricted fragments of this logic, where the interaction between temporal andspatial operators is syntactically constrained. These results yield the firstlogical characterisations of temporal GNNs and establish new relativeexpressiveness results for temporal GNNs.</description>
      <author>example@mail.com (Marco Sälzer, Przemysław Andrzej Wałęga, Martin Lange)</author>
      <guid isPermaLink="false">2505.11930v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Structured Representation</title>
      <link>http://arxiv.org/abs/2505.12143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了不变表示在表征学习中的核心作用，并提出了关于不变性的稳定性、可迁移性和对任务相关信号的影响的问题。&lt;h4&gt;背景&lt;/h4&gt;不变表示是表征学习的关键，但如何发现既稳定又可迁移的不变性，同时不抑制任务相关信号，仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;研究如何确定不变性应定义的适当抽象层次，以及它们应表征系统的哪些方面。&lt;h4&gt;方法&lt;/h4&gt;本文提出，解释操作在更高阶的关系知识层面上进行，因此不变结构必须位于知识所在之处，具体来说，是在抽象知识空间中由关系路径的闭包定义的分区。&lt;h4&gt;主要发现&lt;/h4&gt;这些分区作为核心的不变表示，形成了知识存储和学习发生的结构基础。分区之间的连接器允许部署这些知识分区，编码任务相关的转换。&lt;h4&gt;结论&lt;/h4&gt;不变分区提供了结构表示的基本原理。基于闭半环，一种关系代数结构，本文正式化了不变分区结构表示的计算基础。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了不变表示在表征学习中的核心作用，并提出了关于不变性的稳定性、可迁移性和对任务相关信号的影响的问题。不变表示是表征学习的关键，但如何发现既稳定又可迁移的不变性，同时不抑制任务相关信号，仍然是一个挑战。研究如何确定不变性应定义的适当抽象层次，以及它们应表征系统的哪些方面。本文提出，解释操作在更高阶的关系知识层面上进行，因此不变结构必须位于知识所在之处，具体来说，是在抽象知识空间中由关系路径的闭包定义的分区。这些分区作为核心的不变表示，形成了知识存储和学习发生的结构基础。分区之间的连接器允许部署这些知识分区，编码任务相关的转换。不变分区提供了结构表示的基本原理。基于闭半环，一种关系代数结构，本文正式化了不变分区结构表示的计算基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Invariant representations are core to representation learning, yet a centralchallenge remains: uncovering invariants that are stable and transferablewithout suppressing task-relevant signals. This raises fundamental questions,requiring further inquiry, about the appropriate level of abstraction at whichsuch invariants should be defined, and which aspects of a system they shouldcharacterize. Interpretation of the environment relies on abstract knowledgestructures to make sense of the current state, which leads to interactions,essential drivers of learning and knowledge acquisition. We posit thatinterpretation operates at the level of higher-order relational knowledge;hence, invariant structures must be where knowledge resides, specifically, aspartitions defined by the closure of relational paths within an abstractknowledge space. These partitions serve as the core invariant representations,forming the structural substrate where knowledge is stored and learning occurs.On the other hand, inter-partition connectors enable the deployment of theseknowledge partitions encoding task-relevant transitions. Thus, invariantpartitions provide the foundational primitives of structured representation. Weformalize the computational foundations for structured representation of theinvariant partitions based on closed semiring, a relational algebraicstructure.</description>
      <author>example@mail.com (Arun Kumar, Paul Schrater)</author>
      <guid isPermaLink="false">2505.12143v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EarthSynth: Generating Informative Earth Observation with Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.12108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EarthSynth的扩散式生成基础模型，用于解决遥感图像（RSI）解释中由于标签数据稀缺导致的挑战，通过合成多类别、跨卫星的地球观测数据来提升RSI解释任务的性能。&lt;h4&gt;背景&lt;/h4&gt;RSI解释任务面临挑战，主要是因为标签数据的稀缺性，这限制了任务的性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来解决RSI解释中的标签数据稀缺问题，提升下游任务的性能。&lt;h4&gt;方法&lt;/h4&gt;EarthSynth模型基于EarthSynth-180K数据集，采用反事实组合训练策略来提高训练数据的多样性并增强类别控制。此外，还提出了基于规则的R-Filter方法来过滤更多信息性的合成数据。&lt;h4&gt;主要发现&lt;/h4&gt;EarthSynth是首个探索遥感任务多任务生成的模型，其在场景分类、物体检测和语义分割任务上的评估表明，它为提高RSI解释提供了实用解决方案。&lt;h4&gt;结论&lt;/h4&gt;EarthSynth模型为RSI解释提供了有效的方法，能够通过合成数据解决标签数据稀缺的问题，并提升下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：遥感图像（RSI）的解释通常面临着由于标签数据稀缺所带来的挑战，这限制了RSI解释任务的性能。为了应对这一挑战，我们提出了一种基于扩散的生成基础模型，名为EarthSynth，它能够合成用于下游RSI解释任务的多类别、跨卫星的地球观测数据。据我们所知，EarthSynth是首个探索遥感任务多任务生成的模型。EarthSynth在EarthSynth-180K数据集上训练，采用了反事实组合训练策略来提高训练数据的多样性并增强类别控制。此外，还提出了一种基于规则的R-Filter方法来过滤更多信息性的合成数据。我们在开放世界的场景中对EarthSynth进行了场景分类、物体检测和语义分割的评估，为RSI解释的进步提供了实用解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing image (RSI) interpretation typically faces challenges due tothe scarcity of labeled data, which limits the performance of RSIinterpretation tasks. To tackle this challenge, we propose EarthSynth, adiffusion-based generative foundation model that enables synthesizingmulti-category, cross-satellite labeled Earth observation for downstream RSIinterpretation tasks. To the best of our knowledge, EarthSynth is the first toexplore multi-task generation for remote sensing. EarthSynth, trained on theEarthSynth-180K dataset, employs the Counterfactual Composition trainingstrategy to improve training data diversity and enhance category control.Furthermore, a rule-based method of R-Filter is proposed to filter moreinformative synthetic data for downstream tasks. We evaluate our EarthSynth onscene classification, object detection, and semantic segmentation in open-worldscenarios, offering a practical solution for advancing RSI interpretation.</description>
      <author>example@mail.com (Jiancheng Pan, Shiye Lei, Yuqian Fu, Jiahao Li, Yanxing Liu, Yuze Sun, Xiao He, Long Peng, Xiaomeng Huang, Bo Zhao)</author>
      <guid isPermaLink="false">2505.12108v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Evaluation and optimization of deep learning models for enhanced detection of brain cancer using transmission optical microscopy of thin brain tissue samples</title>
      <link>http://arxiv.org/abs/2505.11735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过深度卷积神经网络（CNN）在脑组织活检样本上的应用，评估了ResNet50和DenseNet121在光学传输光谱学分析中的性能，并与传统方法进行了比较。&lt;h4&gt;背景&lt;/h4&gt;光学传输光谱学是分析脑组织结构的一种方法，但手动解释资源密集且易受观察者主观影响。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习技术自动化分析脑组织活检样本的光学图像，提高分析效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet50和DenseNet121在2,931张脑组织透明光学显微镜图像上进行训练和测试，包括1,996张用于训练，437张用于验证，498张用于测试。采用两阶段迁移学习协议，包括在冻结的预训练特征提取器上训练分类头，随后使用数据增强（旋转、翻转、强度抖动）和早停技术微调更深的卷积块。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121在测试集上达到88.35%的准确率，0.9614的精确度，0.8667的召回率和0.9116的F1分数，优于ResNet50。通过混淆矩阵、训练和验证曲线以及类别预测分布的详细分析，说明了模型的鲁棒收敛和最小偏差。&lt;h4&gt;结论&lt;/h4&gt;DenseNet121在有限医疗数据集上显示出优于ResNet50的性能，为多类别肿瘤分级和临床转化提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：光传输光谱学是理解脑组织结构特性的方法之一，但手动解读资源密集且易受观察者之间差异的影响。深度卷积神经网络（CNN）可以直接从原始明场图像中学习特征。在本研究中，我们对ResNet50和DenseNet121在2,931张薄脑组织明场传输光学显微镜图像的精选数据集上的性能进行了评估，这些图像分为1,996张用于训练，437张用于验证，498张用于测试。我们的两阶段迁移学习协议包括在冻结的预训练特征提取器上对分类头进行初始训练，然后使用大量数据增强（旋转、翻转、强度抖动）和早停技术微调更深的卷积块。与ResNet50（82.12%，0.9035，0.8142，0.8563）相比，DenseNet121在测试集上实现了88.35%的准确率，0.9614的精确度，0.8667的召回率和0.9116的F1分数，表现最佳。通过混淆矩阵、训练和验证曲线以及类别预测分布的详细分析，说明了模型的鲁棒收敛和最小偏差。这些发现证明了在有限医疗数据集上稠密连接的优越泛化能力，并概述了多类别肿瘤分级和临床转化的未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical transmission spectroscopy is one method to understand brain tissuestructural properties from brain tissue biopsy samples, yet manualinterpretation is resource intensive and prone to inter observer variability.Deep convolutional neural networks (CNNs) offer automated feature learningdirectly from raw brightfield images. Here, we evaluate ResNet50 andDenseNet121 on a curated dataset of 2,931 bright-field transmission opticalmicroscopy images of thin brain tissue, split into 1,996 for training, 437 forvalidation, and 498 for testing. Our two stage transfer learning protocolinvolves initial training of a classifier head on frozen pretrained featureextractors, followed by fine tuning of deeper convolutional blocks withextensive data augmentation (rotations, flips, intensity jitter) and earlystopping. DenseNet121 achieves 88.35 percent test accuracy, 0.9614 precision,0.8667 recall, and 0.9116 F1 score the best performance compared to ResNet50(82.12 percent, 0.9035, 0.8142, 0.8563). Detailed analysis of confusionmatrices, training and validation curves, and classwise predictiondistributions illustrates robust convergence and minimal bias. These findingsdemonstrate the superior generalization of dense connectivity on limitedmedical datasets and outline future directions for multi-class tumor gradingand clinical translation.</description>
      <author>example@mail.com (Mohnish Sao, Mousa Alrubayan, Prabhakar Pradhan)</author>
      <guid isPermaLink="false">2505.11735v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction</title>
      <link>http://arxiv.org/abs/2505.11063v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Thought-Aligner的动态思维校正模块，用于提高基于LLM的自主代理在复杂多步任务中的行为安全。&lt;h4&gt;背景&lt;/h4&gt;LLM-based自主代理具有推理、工具调用和环境交互的能力，但内部思维过程可能引入风险，导致不可逆的安全事件。&lt;h4&gt;目的&lt;/h4&gt;为了解决长期行为轨迹中的安全对齐挑战，提出Thought-Aligner模块。&lt;h4&gt;方法&lt;/h4&gt;Thought-Aligner使用轻量级和资源高效的模型，在每次动作执行前实时纠正高风险思维，并重新引入到代理中，同时不改变代理框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Thought-Aligner将代理的行为安全从未受保护设置中的约50%提高到平均90%，同时保持响应延迟低于100ms。&lt;h4&gt;结论&lt;/h4&gt;Thought-Aligner为基于LLM的代理提供了一个实用的动态安全解决方案，具有高效部署、广泛适用和及时响应的能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a dynamic thought correction module named Thought-Aligner to improve the behavioral safety of autonomous agents based on LLMs. The background is that LLM-based autonomous agents have capabilities such as reasoning, tool invocation, and environment interaction, but the internal thinking process may introduce risks, leading to irreversible safety incidents. The purpose is to address the safety alignment challenges in long-horizon behavioral trajectories by proposing the Thought-Aligner module. The method uses a lightweight and resource-efficient model to correct high-risk thoughts on the fly before each action execution and reintroduce them to the agent, without altering the underlying agent framework. The main findings show that Thought-Aligner raises the behavioral safety of the agent from about 50% in the unprotected setting to an average of 90%, while maintaining a response latency below 100ms. The conclusion is that Thought-Aligner provides a practical dynamic safety solution for LLM-based agents, demonstrating its capability for efficient deployment, broad applicability, and timely responsiveness.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based autonomous agents possess capabilities such as reasoning, toolinvocation, and environment interaction, enabling the execution of complexmulti-step tasks. The internal reasoning process, i.e., thought, of behavioraltrajectory significantly influences tool usage and subsequent actions but canintroduce potential risks. Even minor deviations in the agent's thought maytrigger cascading effects leading to irreversible safety incidents. To addressthe safety alignment challenges in long-horizon behavioral trajectories, wepropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizinga lightweight and resource-efficient model, Thought-Aligner corrects eachhigh-risk thought on the fly before each action execution. The correctedthought is then reintroduced to the agent, ensuring safer subsequent decisionsand tool interactions. Importantly, Thought-Aligner modifies only the reasoningphase without altering the underlying agent framework, making it easy to deployand widely applicable to various agent frameworks. To train the Thought-Alignermodel, we construct an instruction dataset across ten representative scenariosand simulate ReAct execution trajectories, generating 5,000 diverseinstructions and more than 11,400 safe and unsafe thought pairs. The model isfine-tuned using contrastive learning techniques. Experiments across threeagent safety benchmarks involving 12 different LLMs demonstrate thatThought-Aligner raises agent behavioral safety from approximately 50% in theunprotected setting to 90% on average. Additionally, Thought-Aligner maintainsresponse latency below 100ms with minimal resource usage, demonstrating itscapability for efficient deployment, broad applicability, and timelyresponsiveness. This method thus provides a practical dynamic safety solutionfor the LLM-based agents.</description>
      <author>example@mail.com (Changyue Jiang, Xudong Pan, Min Yang)</author>
      <guid isPermaLink="false">2505.11063v2</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming</title>
      <link>http://arxiv.org/abs/2505.11710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合图神经网络近似动态规划和进化多样性优化的协同进化防御框架，用于提高Active Directory的安全性。&lt;h4&gt;背景&lt;/h4&gt;现代企业网络越来越依赖Active Directory进行身份和访问管理，但其集中化特性使得攻击者可以攻击高价值资产。&lt;h4&gt;目的&lt;/h4&gt;为了应对动态攻击者的适应性行为，本文旨在提出一种能够适应攻击者策略变化的防御框架。&lt;h4&gt;方法&lt;/h4&gt;该框架将攻击者和防御者在Active Directory中的交互建模为一个Stackelberg博弈，并结合GNNDP和EDO来生成鲁棒的阻止策略。为了确保可扩展性，引入了FPT图减少方法以降低复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架在合成AD图上取得了接近最优的结果，并且在更大的图上（r1000和r2000）表现出了改进的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架具有可扩展性和有效性，能够提高Active Directory的安全性并防止过早收敛。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Modern enterprise networks increasingly rely on Active Directory (AD) for identity and access management. However, this centralization exposes a single point of failure, allowing adversaries to compromise high-value assets. Existing AD defense approaches often assume static attacker behavior, but real-world adversaries adapt dynamically, rendering such methods brittle. To address this, we model attacker-defender interactions in AD as a Stackelberg game between an adaptive attacker and a proactive defender. We propose a co-evolutionary defense framework that combines Graph Neural Network Approximated Dynamic Programming (GNNDP) to model attacker strategies, with Evolutionary Diversity Optimization (EDO) to generate resilient blocking strategies. To ensure scalability, we introduce a Fixed-Parameter Tractable (FPT) graph reduction method that reduces complexity while preserving strategic structure. Our framework jointly refines attacker and defender policies to improve generalization and prevent premature convergence. Experiments on synthetic AD graphs show near-optimal results (within 0.1 percent of optimality on r500) and improved performance on larger graphs (r1000 and r2000), demonstrating the framework's scalability and effectiveness.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern enterprise networks increasingly rely on Active Directory (AD) foridentity and access management. However, this centralization exposes a singlepoint of failure, allowing adversaries to compromise high-value assets.Existing AD defense approaches often assume static attacker behavior, butreal-world adversaries adapt dynamically, rendering such methods brittle. Toaddress this, we model attacker-defender interactions in AD as a Stackelberggame between an adaptive attacker and a proactive defender. We propose aco-evolutionary defense framework that combines Graph Neural NetworkApproximated Dynamic Programming (GNNDP) to model attacker strategies, withEvolutionary Diversity Optimization (EDO) to generate resilient blockingstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable(FPT) graph reduction method that reduces complexity while preserving strategicstructure. Our framework jointly refines attacker and defender policies toimprove generalization and prevent premature convergence. Experiments onsynthetic AD graphs show near-optimal results (within 0.1 percent of optimalityon r500) and improved performance on larger graphs (r1000 and r2000),demonstrating the framework's scalability and effectiveness.</description>
      <author>example@mail.com (Diksha Goel, Hussain Ahmad, Kristen Moore, Mingyu Guo)</author>
      <guid isPermaLink="false">2505.11710v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2505.11803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种适用于时间超关系知识图谱的VITA学习方法，以解决传统链接预测技术在处理时间有效性和无限有效事实时的不足。&lt;h4&gt;背景&lt;/h4&gt;知识图谱在管理动态变化的事实方面非常有效，而事实的时间有效性对于下游的链接预测任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出VITA方法，以提升链接预测性能，特别是在处理事实的时间有效性和时长信息方面。&lt;h4&gt;方法&lt;/h4&gt;VITA方法首先提出了一种灵活的时间表示，能够适应事实的四种时间有效性类型（即：自、至、期间、时间不变），然后设计VITA来有效地学习时间价值方面和时长方面的信息。&lt;h4&gt;主要发现&lt;/h4&gt;VITA在真实世界的知识图谱数据集上进行了彻底的评估，结果显示在预测缺失实体、关系、时间和其他数值字面量等链接预测任务中，VITA优于最佳基线，性能提升了75.3%。消融研究和案例研究也支持了关键设计选择。&lt;h4&gt;结论&lt;/h4&gt;VITA方法有效地提升了时间超关系知识图谱的链接预测性能，为处理动态变化的事实提供了新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs (KGs) have become an effective paradigm for managingreal-world facts, which are not only complex but also dynamically evolve overtime. The temporal validity of facts often serves as a strong clue indownstream link prediction tasks, which predicts a missing element in a fact.Traditional link prediction techniques on temporal KGs either consider asequence of temporal snapshots of KGs with an ad-hoc defined time interval orexpand a temporal fact over its validity period under a predefined timegranularity; these approaches not only suffer from the sensitivity of theselection of time interval/granularity, but also face the computationalchallenges when handling facts with long (even infinite) validity. Although therecent hyper-relational KGs represent the temporal validity of a fact asqualifiers describing the fact, it is still suboptimal due to its ignorance ofthe infinite validity of some facts and the insufficient information encodedfrom the qualifiers about the temporal validity. Against this background, wepropose VITA, a $\underline{V}$ersatile t$\underline{I}$merepresen$\underline{TA}$tion learning method for temporal hyper-relationalknowledge graphs. We first propose a versatile time representation that canflexibly accommodate all four types of temporal validity of facts (i.e., since,until, period, time-invariant), and then design VITA to effectively learn thetime information in both aspects of time value and timespan to boost the linkprediction performance. We conduct a thorough evaluation of VITA compared to asizable collection of baselines on real-world KG datasets. Results show thatVITA outperforms the best-performing baselines in various link prediction tasks(predicting missing entities, relations, time, and other numeric literals) byup to 75.3%. Ablation studies and a case study also support our key designchoices.</description>
      <author>example@mail.com (ChongIn Un, Yuhuan Lu, Tianyue Yang, Dingqi Yang)</author>
      <guid isPermaLink="false">2505.11803v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Humble your Overconfident Networks: Unlearning Overfitting via Sequential Monte Carlo Tempered Deep Ensembles</title>
      <link>http://arxiv.org/abs/2505.11671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可扩展的SMC（序列蒙特卡洛）方法，通过结合SGHMC（随机梯度哈密顿蒙特卡洛）建议，提高了SMC的采样效率，并在图像分类、异常检测和迁移学习任务中优于标准SGD和深度集成方法。&lt;h4&gt;背景&lt;/h4&gt;传统的SMC方法在处理大规模数据时，由于需要全批量梯度评估而受到限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种可扩展的SMC方法，通过引入SGHMC建议，实现高效的迷你批量采样。&lt;h4&gt;方法&lt;/h4&gt;将SGHMC建议结合到SMC中，形成SMCSGHMC算法，并在不同任务中进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;SMCSGHMC算法在图像分类、异常检测和迁移学习任务中表现出色，且能有效减轻过拟合问题，提高模型校准。&lt;h4&gt;结论&lt;/h4&gt;SMCSGHMC为将预训练神经网络转换为校准良好的贝叶斯模型提供了一种灵活且可扩展的途径。&lt;h4&gt;翻译&lt;/h4&gt;Sequential Monte Carlo methods offer a principled approach to Bayesian uncertainty quantification but are traditionally limited by the need for full-batch gradient evaluations. We introduce a scalable variant by incorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals into SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC algorithm outperforms standard stochastic gradient descent (SGD) and deep ensembles across image classification, out-of-distribution (OOD) detection, and transfer learning tasks. We further show that SMCSGHMC mitigates overfitting and improves calibration, providing a flexible, scalable pathway for converting pretrained neural networks into well-calibrated Bayesian models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesianuncertainty quantification but are traditionally limited by the need forfull-batch gradient evaluations. We introduce a scalable variant byincorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposalsinto SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMCalgorithm outperforms standard stochastic gradient descent (SGD) and deepensembles across image classification, out-of-distribution (OOD) detection, andtransfer learning tasks. We further show that SMCSGHMC mitigates overfittingand improves calibration, providing a flexible, scalable pathway for convertingpretrained neural networks into well-calibrated Bayesian models.</description>
      <author>example@mail.com (Andrew Millard, Zheng Zhao, Joshua Murphy, Simon Maskell)</author>
      <guid isPermaLink="false">2505.11671v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Nearest Neighbor Multivariate Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.11625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于k近邻的多元时间序列(kNN-MTS)预测框架，通过在大数据存储中利用最近邻检索机制，提高了多元时间序列模型的预测性能。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测在工业和学术界有广泛的应用。近年来，空间时间图神经网络(STGNNs)在多元时间序列预测中变得流行，但现有的STGNNs由于计算复杂度限制，只能使用有限长度的输入数据，并且难以识别整个数据集中的相似模式。&lt;h4&gt;目的&lt;/h4&gt;设计一种简单有效的k近邻多元时间序列(kNN-MTS)预测框架，以解决现有方法的问题，并提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了kNN-MTS框架，该框架利用多元时间序列模型表示进行相似性搜索，无需额外训练，并扩展到对整个数据集的直接访问，同时设计了混合空间时间编码器(HSTEncoder)以捕捉长期时间依赖和短期空间时间依赖。&lt;h4&gt;主要发现&lt;/h4&gt;kNN-MTS框架在多个真实世界数据集上的实验结果表明，与现有方法相比，预测性能有显著提升。定量分析表明，kNN-MTS具有可解释性和效率，展现出更好的应用前景。&lt;h4&gt;结论&lt;/h4&gt;kNN-MTS框架为高效利用多元时间序列模型中的大数据集提供了一条新路径，具有广阔的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TNNLS.2024.3490603&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) forecasting has a wide range of applicationsin both industry and academia. Recently, spatial-temporal graph neural networks(STGNNs) have gained popularity as MTS forecasting methods. However, currentSTGNNs can only use the finite length of MTS input data due to thecomputational complexity. Moreover, they lack the ability to identify similarpatterns throughout the entire dataset and struggle with data that exhibitsparsely and discontinuously distributed correlations among variables over anextensive historical period, resulting in only marginal improvements. In thisarticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting( kNN-MTS) framework, which forecasts with a nearest neighbor retrievalmechanism over a large datastore of cached series, using representations fromthe MTS model for similarity search. This approach requires no additionaltraining and scales to give the MTS model direct access to the whole dataset attest time, resulting in a highly expressive model that consistently improvesperformance, and has the ability to extract sparse distributed but similarpatterns spanning over multivariables from the entire dataset. Furthermore, ahybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which cancapture both long-term temporal and short-term spatial-temporal dependenciesand is shown to provide accurate representation for kNN-MTSfor betterforecasting. Experimental results on several real-world datasets show asignificant improvement in the forecasting performance of kNN-MTS. Thequantitative analysis also illustrates the interpretability and efficiency ofkNN-MTS, showing better application prospects and opening up a new path forefficiently using the large dataset in MTS models.</description>
      <author>example@mail.com (Huiliang Zhang, Ping Nie, Lijun Sun, Benoit Boulet)</author>
      <guid isPermaLink="false">2505.11625v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.11781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于小波变换的深度时间序列预测方法，通过引入多阶小波导数变换（WDT）来改进频率表示学习，以提高时间序列预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;传统的傅里叶变换（FT）和小波变换（WT）在时间序列预测中广泛应用，但它们在捕捉多尺度、时间敏感的模式方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来捕捉时间序列中的多尺度、时间敏感的模式，并提高预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入了多阶小波导数变换（WDT），它基于小波变换，可以提取跨越整体趋势和微妙波动的时敏模式。将WDT嵌入到名为WaveTS的多分支框架中，该框架分解输入序列到多尺度时间频率系数，通过线性层进行细化，并通过逆WDT重构到时间域。&lt;h4&gt;主要发现&lt;/h4&gt;WDT通过操作序列的导数，有选择地放大变化率线索，并揭示对时间序列建模特别有信息量的突发状态转变。&lt;h4&gt;结论&lt;/h4&gt;在十个基准数据集上的广泛实验表明，WaveTS实现了最先进的预测精度，同时保持了高计算效率。&lt;h4&gt;翻译&lt;/h4&gt;在深度时间序列预测中，傅里叶变换（FT）广泛用于频率表示学习。然而，它往往难以捕捉多尺度、时间敏感的模式。尽管小波变换（WT）可以通过频率分解捕捉这些模式，但其系数对时间序列中的变化点不敏感，导致建模效果不佳。为了缓解这些限制，我们引入了基于WT的多阶小波导数变换（WDT），它能够提取跨越整体趋势和微妙波动的时敏模式。与建模原始序列的标准FT和WT相比，WDT操作序列的导数，有选择地放大变化率线索，并揭示对时间序列建模特别有信息量的突发状态转变。实际上，我们将WDT嵌入到名为WaveTS的多分支框架中，该框架将输入序列分解为多尺度时间频率系数，通过线性层进行细化，并通过逆WDT重构到时间域。在十个基准数据集上的广泛实验表明，WaveTS实现了最先进的预测精度，同时保持了高计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In deep time series forecasting, the Fourier Transform (FT) is extensivelyemployed for frequency representation learning. However, it often struggles incapturing multi-scale, time-sensitive patterns. Although the Wavelet Transform(WT) can capture these patterns through frequency decomposition, itscoefficients are insensitive to change points in time series, leading tosuboptimal modeling. To mitigate these limitations, we introduce themulti-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling theextraction of time-aware patterns spanning both the overall trend and subtlefluctuations. Compared with the standard FT and WT, which model the raw series,the WDT operates on the derivative of the series, selectively magnifyingrate-of-change cues and exposing abrupt regime shifts that are particularlyinformative for time series modeling. Practically, we embed the WDT into amulti-branch framework named WaveTS, which decomposes the input series intomulti-scale time-frequency coefficients, refines them via linear layers, andreconstructs them into the time domain via the inverse WDT. Extensiveexperiments on ten benchmark datasets demonstrate that WaveTS achievesstate-of-the-art forecasting accuracy while retaining high computationalefficiency.</description>
      <author>example@mail.com (Ziyu Zhou, Jiaxi Hu, Qingsong Wen, James T. Kwok, Yuxuan Liang)</author>
      <guid isPermaLink="false">2505.11781v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>AoP-SAM: Automation of Prompts for Efficient Segmentation</title>
      <link>http://arxiv.org/abs/2505.11980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AoP-SAM的新方法，用于自动生成SAM（Segment Anything Model）的必要提示，从而提高其效率和实用性。&lt;h4&gt;背景&lt;/h4&gt;SAM是一种强大的图像分割基础模型，但依赖于手动提示在实际应用中不切实际，尤其是在需要快速提示和资源效率的场景中。&lt;h4&gt;目的&lt;/h4&gt;旨在提高SAM的效率和实用性，使其更适合现实世界的任务。&lt;h4&gt;方法&lt;/h4&gt;AoP-SAM使用一个轻量级且高效的提示预测模型，该模型检测图像中的关键实体并识别放置提示候选者的最佳区域。此外，还引入了一种测试时实例级别的自适应采样和过滤机制，以粗到细的方式生成提示。&lt;h4&gt;主要发现&lt;/h4&gt;AoP-SAM显著提高了提示生成效率和掩码生成准确性，同时保留了SAM的无需微调的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;AoP-SAM使SAM在自动化分割任务中更加有效。&lt;h4&gt;翻译&lt;/h4&gt;The Segment Anything Model (SAM) is a powerful foundation model for imagesegmentation, showing robust zero-shot generalization through promptengineering. However, relying on manual prompts is impractical for real-world applications, particularly in scenarios where rapid prompt provision and resource efficiency are crucial. In this paper, we propose the Automation of Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency and usability by eliminating manual input, making it better suited for real-world tasks. Our approach employs a lightweight yet efficient PromptPredictor model that detects key entities across images and identifies the optimal regions for placing prompt candidates. This method leverages SAM's image embeddings, preserving its zero-shot generalization capabilities without requiring fine-tuning. Additionally, we introduce a test-time instance-level Adaptive Sampling and Filtering mechanism that generates prompts in a coarse-to-fine manner. This notably enhances both prompt and mask generation efficiency by reducing computational overhead and minimizing redundant mask refinements. Evaluations of three datasets demonstrate that AoP-SAM substantially improves both prompt generation efficiency and mask generation accuracy, making SAM more effective for automated segmentation tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v39i2.32228&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Segment Anything Model (SAM) is a powerful foundation model for imagesegmentation, showing robust zero-shot generalization through promptengineering. However, relying on manual prompts is impractical for real-worldapplications, particularly in scenarios where rapid prompt provision andresource efficiency are crucial. In this paper, we propose the Automation ofPrompts for SAM (AoP-SAM), a novel approach that learns to generate essentialprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiencyand usability by eliminating manual input, making it better suited forreal-world tasks. Our approach employs a lightweight yet efficient PromptPredictor model that detects key entities across images and identifies theoptimal regions for placing prompt candidates. This method leverages SAM'simage embeddings, preserving its zero-shot generalization capabilities withoutrequiring fine-tuning. Additionally, we introduce a test-time instance-levelAdaptive Sampling and Filtering mechanism that generates prompts in acoarse-to-fine manner. This notably enhances both prompt and mask generationefficiency by reducing computational overhead and minimizing redundant maskrefinements. Evaluations of three datasets demonstrate that AoP-SAMsubstantially improves both prompt generation efficiency and mask generationaccuracy, making SAM more effective for automated segmentation tasks.</description>
      <author>example@mail.com (Yi Chen, Mu-Young Son, Chuanbo Hua, Joo-Young Kim)</author>
      <guid isPermaLink="false">2505.11980v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Programmable metasurfaces for future photonic artificial intelligence</title>
      <link>http://arxiv.org/abs/2505.11659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Nat. Rev. Phys. (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了光子神经网络（PNNs）在能效、延迟和吞吐量方面可能挑战传统数字神经网络，但实现可扩展的光子人工智能（AI）解决方案仍然具有挑战性。&lt;h4&gt;背景&lt;/h4&gt;光子神经网络具有高并行性和低功耗等固有优势，但在能源效率、延迟和吞吐量方面可能优于传统数字神经网络。&lt;h4&gt;目的&lt;/h4&gt;解决光子AI模型的可扩展性问题，使其在商业上可行。&lt;h4&gt;方法&lt;/h4&gt;讨论了现场可编程超表面技术可能成为实现可扩展光子AI加速器的关键硬件成分，以及它如何与当前数字电子技术竞争。&lt;h4&gt;主要发现&lt;/h4&gt;可编程或可重构性是PNN硬件的关键组成部分，它使得现场训练成为可能，并适应需要微调或迁移学习的非静态用例。&lt;h4&gt;结论&lt;/h4&gt;通过集成电子、3D堆叠和超表面的大规模制造，可编程超表面可以解决PNN面临的一些挑战，并推动下一代光子AI技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1038/s42254-025-00831-7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photonic neural networks (PNNs), which share the inherent benefits ofphotonic systems, such as high parallelism and low power consumption, couldchallenge traditional digital neural networks in terms of energy efficiency,latency, and throughput. However, producing scalable photonic artificialintelligence (AI) solutions remains challenging. To make photonic AI modelsviable, the scalability problem needs to be solved. Large optical AI modelsimplemented on PNNs are only commercially feasible if the advantages of opticalcomputation outweigh the cost of their input-output overhead. In thisPerspective, we discuss how field-programmable metasurface technology maybecome a key hardware ingredient in achieving scalable photonic AI acceleratorsand how it can compete with current digital electronic technologies.Programmability or reconfigurability is a pivotal component for PNN hardware,enabling in situ training and accommodating non-stationary use cases thatrequire fine-tuning or transfer learning. Co-integration with electronics, 3Dstacking, and large-scale manufacturing of metasurfaces would significantlyimprove PNN scalability and functionalities. Programmable metasurfaces couldaddress some of the current challenges that PNNs face and enablenext-generation photonic AI technology.</description>
      <author>example@mail.com (Loubnan Abou-Hamdan, Emil Marinov, Peter Wiecha, Philipp del Hougne, Tianyu Wang, Patrice Genevet)</author>
      <guid isPermaLink="false">2505.11659v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Subspace Optimization for Continual Learning</title>
      <link>http://arxiv.org/abs/2505.11816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了连续子空间优化（CoSO）方法，用于连续学习，旨在通过在一系列子空间中微调模型来减轻灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;连续学习旨在学习多个任务，同时保留先验知识，但获取新知识时面临着灾难性遗忘的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决连续学习中参数更新受限于固定低秩子空间的问题，从而提高模型的泛化能力和学习性能。&lt;h4&gt;方法&lt;/h4&gt;CoSO通过梯度奇异值分解动态确定一系列子空间，并通过将这些子空间中的梯度投影到模型上来更新模型，同时保持任务的特定组件以捕获当前任务的更新方向。&lt;h4&gt;主要发现&lt;/h4&gt;CoSO在多个数据集上的实验表明，它显著优于现有的最先进方法，特别是在具有长任务序列的挑战性场景中。&lt;h4&gt;结论&lt;/h4&gt;CoSO为连续学习提供了一种有效的方法，可以减轻灾难性遗忘，提高模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：连续学习旨在按顺序学习多个任务，同时保留先验知识，但在获取新知识时面临灾难性遗忘的挑战。最近，利用预训练模型的方法越来越受欢迎，以减轻这一问题，因为基础模型具有强大的泛化能力。为了调整预训练模型以适应新任务，现有方法通常采用低秩自适应，这限制了参数更新到固定的低秩子空间。然而，对优化空间的约束本质上会降低模型的学习能力，导致性能下降。为了解决这个问题，我们提出了连续子空间优化（CoSO）方法，以在一系列子空间中微调模型，而不是单一的一个。这些顺序子空间通过梯度奇异值分解动态确定。CoSO通过将这些子空间中的梯度投影到模型上来更新模型，确保内存高效的优化。为了减轻遗忘，每个任务的优化子空间被设置为与历史任务子空间正交。在任务学习过程中，CoSO维护一个特定于任务的组件，以捕获与当前任务相关的关键更新方向。完成一个任务后，该组件用于更新历史任务子空间，为后续学习奠定基础。在多个数据集上的大量实验表明，CoSO在挑战性场景中，特别是在具有长任务序列的情况下，显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning aims to learn multiple tasks sequentially while preservingprior knowledge, but faces the challenge of catastrophic forgetting whenacquiring new knowledge. Recently, approaches leveraging pre-trained modelshave gained increasing popularity to mitigate this issue, due to the stronggeneralization ability of foundation models. To adjust pre-trained models fornew tasks, existing methods usually employ low-rank adaptation, which restrictsparameter updates to a fixed low-rank subspace. However, constraining theoptimization space inherently compromises the model's learning capacity,resulting in inferior performance. To address the limitation, we proposeContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune themodel in a series of subspaces rather than a single one. These sequentialsubspaces are dynamically determined through the singular value decompositionof gradients. CoSO updates the model by projecting gradients into thesesubspaces, ensuring memory-efficient optimization. To mitigate forgetting, theoptimization subspaces of each task are set to be orthogonal to the historicaltask subspace. During task learning, CoSO maintains a task-specific componentthat captures the critical update directions associated with the current task.Upon completing a task, this component is used to update the historical tasksubspace, laying the groundwork for subsequent learning. Extensive experimentson multiple datasets demonstrate that CoSO significantly outperformsstate-of-the-art methods, especially in challenging scenarios with long tasksequences.</description>
      <author>example@mail.com (Quan Cheng, Yuanyu Wan, Lingyu Wu, Chenping Hou, Lijun Zhang)</author>
      <guid isPermaLink="false">2505.11816v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</title>
      <link>http://arxiv.org/abs/2505.11810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个名为AI Taiyan的大语言模型，专门用于理解和生成古典中文，并在古典中文信息处理的关键任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;通用大语言模型在语言理解和生成方面表现出色，但在特定领域如古典中文文本中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对古典中文理解和生成的大语言模型，以解决特定领域模型效果不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了合理的模型结构，进行了数据处理、基础训练和微调，并在仅有1.8亿参数的情况下取得了满意的结果。&lt;h4&gt;主要发现&lt;/h4&gt;AI Taiyan模型在古典中文信息处理的关键任务如标点、典故识别、词义解释和古汉译现代汉翻译等方面，优于通用大语言模型和特定领域传统模型，达到或超过了人类基准水平。&lt;h4&gt;结论&lt;/h4&gt;该研究为高效构建特定领域的专用大语言模型提供了参考，并讨论了该模型在古文书编纂、词典编辑和语言研究等领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;The study developed a large language model named AI Taiyan, specifically designed for understanding and generating Classical Chinese, and demonstrated excellent performance in key tasks related to Classical Chinese information processing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose large language models demonstrate notable capabilities inlanguage comprehension and generation, achieving results that are comparableto, or even surpass, human performance in many language information processingtasks. Nevertheless, when general models are applied to some specific domains,e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, andfine-tuning open-source foundational models similarly struggles to adequatelyincorporate domain-specific knowledge. To address this challenge, this studydeveloped a large language model, AI Taiyan, specifically designed forunderstanding and generating Classical Chinese. Experiments show that with areasonable model design, data processing, foundational training, andfine-tuning, satisfactory results can be achieved with only 1.8 billionparameters. In key tasks related to Classical Chinese information processingsuch as punctuation, identification of allusions, explanation of word meanings,and translation between ancient and modern Chinese, this model exhibits a clearadvantage over both general-purpose large models and domain-specifictraditional models, achieving levels close to or surpassing human baselines.This research provides a reference for the efficient construction ofspecialized domain-specific large language models. Furthermore, the paperdiscusses the application of this model in fields such as the collation ofancient texts, dictionary editing, and language research, combined with casestudies.</description>
      <author>example@mail.com (Shen Li, Renfen Hu, Lijun Wang)</author>
      <guid isPermaLink="false">2505.11810v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video</title>
      <link>http://arxiv.org/abs/2505.11709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对操作学习中的数据稀缺问题，提出了一个名为EgoDex的大规模数据集，用于解决当前数据集缺乏手部姿态标注和专注于物体操作的问题。&lt;h4&gt;背景&lt;/h4&gt;操作学习在数据稀缺方面存在难题，目前缺乏大型的手部操作数据集。&lt;h4&gt;目的&lt;/h4&gt;通过创建一个包含丰富手部操作数据集的EgoDex，旨在推动机器人、计算机视觉和基础模型领域的发展。&lt;h4&gt;方法&lt;/h4&gt;使用Apple Vision Pro收集EgoDex数据集，包含829小时的以自我为中心的人类操作视频和配对的3D手部及手指跟踪数据，利用多台校准相机和设备上的SLAM技术精确追踪每个手指关节的姿态。&lt;h4&gt;主要发现&lt;/h4&gt;EgoDex涵盖了从系鞋带到叠洗衣物等194种不同的桌面任务，涵盖了广泛多样的操作行为，并在该数据集上训练和评估了手部轨迹预测的模仿学习策略，引入了衡量该领域进展的指标和基准。&lt;h4&gt;结论&lt;/h4&gt;EgoDex的发布有望推动机器人、计算机视觉和基础模型领域的前沿发展。&lt;h4&gt;翻译&lt;/h4&gt;Imitation learning for manipulation has a well-known data scarcity problem. Unlike natural language and 2D computer vision, there is no Internet-scale corpus of data for dexterous manipulation. One appealing option is egocentric human video, a passively scalable data source. However, existing large-scale datasets such as Ego4D do not have native hand pose annotations and do not focus on object manipulation. To this end, we use Apple Vision Pro to collect EgoDex: the largest and most diverse dataset of dexterous human manipulation to date. EgoDex has 829 hours of egocentric video with paired 3D hand and finger tracking data collected at the time of recording, where multiple calibrated cameras and on-device SLAM can be used to precisely track the pose of every joint of each hand. The dataset covers a wide range of diverse manipulation behaviors with everyday household objects in 194 different tabletop tasks ranging from tying shoelaces to folding laundry. Furthermore, we train and systematically evaluate imitation learning policies for hand trajectory prediction on the dataset, introducing metrics and benchmarks for measuring progress in this increasingly important area. By releasing this large-scale dataset, we hope to push the frontier of robotics, computer vision, and foundation models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for manipulation has a well-known data scarcity problem.Unlike natural language and 2D computer vision, there is no Internet-scalecorpus of data for dexterous manipulation. One appealing option is egocentrichuman video, a passively scalable data source. However, existing large-scaledatasets such as Ego4D do not have native hand pose annotations and do notfocus on object manipulation. To this end, we use Apple Vision Pro to collectEgoDex: the largest and most diverse dataset of dexterous human manipulation todate. EgoDex has 829 hours of egocentric video with paired 3D hand and fingertracking data collected at the time of recording, where multiple calibratedcameras and on-device SLAM can be used to precisely track the pose of everyjoint of each hand. The dataset covers a wide range of diverse manipulationbehaviors with everyday household objects in 194 different tabletop tasksranging from tying shoelaces to folding laundry. Furthermore, we train andsystematically evaluate imitation learning policies for hand trajectoryprediction on the dataset, introducing metrics and benchmarks for measuringprogress in this increasingly important area. By releasing this large-scaledataset, we hope to push the frontier of robotics, computer vision, andfoundation models.</description>
      <author>example@mail.com (Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, Jian Zhang)</author>
      <guid isPermaLink="false">2505.11709v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Invariant Representations via Wasserstein Correlation Maximization</title>
      <link>http://arxiv.org/abs/2505.11702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用Wasserstein相关系数进行无监督表示学习的方法，该方法基于联合分布与其边缘分布的Wasserstein距离。&lt;h4&gt;背景&lt;/h4&gt;与自然在潜在空间中聚类类别的对比方法不同，本文发现，训练以最大化输入和编码分布之间的Wasserstein相关性的（自）编码器实际上充当压缩器，在减少维度的同时近似保留了输入分布的拓扑和几何属性。&lt;h4&gt;目的&lt;/h4&gt;探索Wasserstein相关系数在无监督表示学习中的应用，并研究其如何影响（自）编码器的性能。&lt;h4&gt;方法&lt;/h4&gt;使用Wasserstein相关系数来训练（自）编码器，并利用Markov-Wasserstein核定义增强编码器，以实现模型对特定增强的近似不变性。&lt;h4&gt;主要发现&lt;/h4&gt;Wasserstein相关系数最大化可以使（自）编码器对选择的增强或增强集近似不变，同时仍然近似保留非增强输入分布的结构属性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法不仅可以通过实验证明简单的前馈网络可以赋予不变性，还可以将不变性传递给预训练模型，并建立了基于最优传输的依赖度测量的各种理论结果。&lt;h4&gt;翻译&lt;/h4&gt;这项工作调查了使用Wasserstein相关系数——一种基于联合分布与其边缘分布之间的Wasserstein距离的标准化统计依赖度度量——进行无监督表示学习。与例如对比方法不同，这些方法在潜在空间中自然地聚类类别，我们发现，训练以最大化输入和编码分布之间的Wasserstein相关性的（自）编码器实际上充当压缩器，在减少维度的同时近似保留了输入分布的拓扑和几何属性。更有趣的是，我们表明，Wasserstein相关系数最大化可用于得到一个（自）编码器——无论是从头开始训练，还是扩展一个冻结的预训练模型——它对选择的增强或增强集近似不变，并且仍然近似保留了非增强输入分布的结构属性。为了做到这一点，我们首先使用Markov-Wasserstein核的机制定义了增强编码器的概念。当最大化目标应用于增强编码器，而不是基础上的确定性编码器时，所得到的模型表现出所期望的不变性属性。最后，除了我们的实验结果，这些结果表明即使简单的前馈网络也可以赋予不变性，或者可以在此训练过程中将不变性传递给预训练模型之外，我们还为基于最优传输的依赖度测量建立了各种理论结果。代码可在https://github.com/keenan-eikenberry/wasserstein_correlation_maximization 上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/keenan-eikenberry/wasserstein_correlation_maximization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates the use of Wasserstein correlation -- a normalizedmeasure of statistical dependence based on the Wasserstein distance between ajoint distribution and the product of its marginals -- for unsupervisedrepresentation learning. Unlike, for example, contrastive methods, whichnaturally cluster classes in the latent space, we find that an (auto)encodertrained to maximize Wasserstein correlation between the input and encodeddistributions instead acts as a compressor, reducing dimensionality whileapproximately preserving the topological and geometric properties of the inputdistribution. More strikingly, we show that Wasserstein correlationmaximization can be used to arrive at an (auto)encoder -- either trained fromscratch, or else one that extends a frozen, pretrained model -- that isapproximately invariant to a chosen augmentation, or collection ofaugmentations, and that still approximately preserves the structural propertiesof the non-augmented input distribution. To do this, we first define the notionof an augmented encoder using the machinery of Markov-Wasserstein kernels. Whenthe maximization objective is then applied to the augmented encoder, as opposedto the underlying, deterministic encoder, the resulting model exhibits thedesired invariance properties. Finally, besides our experimental results, whichshow that even simple feedforward networks can be imbued with invariants orcan, alternatively, be used to impart invariants to pretrained models underthis training process, we additionally establish various theoretical resultsfor optimal transport-based dependence measures. Code is available athttps://github.com/keenan-eikenberry/wasserstein_correlation_maximization .</description>
      <author>example@mail.com (Keenan Eikenberry, Lizuo Liu, Yoonsang Lee)</author>
      <guid isPermaLink="false">2505.11702v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2505.11680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于实例的零样本技能迁移方法，旨在解决开放世界机器人操作中不同物体之间技能迁移的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;机器人操作中，技能的迁移需要考虑不同物体之间的高层次结构差异，同时保持低层次交互控制的相似性。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够在不将技能视为原子化的情况下，将技能分解为一系列基于任务轴（GTA）的控制器，并实现零样本迁移。&lt;h4&gt;方法&lt;/h4&gt;将技能分解为一系列GTA控制器，每个控制器定义了沿一个轴的适应控制器，如位置或力控制器。这些控制器基于物体的关键点和轴进行定位。使用如SD-DINO等基础模型检测语义上相似的关键点，以实现零样本迁移。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实机器人的实验评估，包括拧紧、倒水和刮刀刮擦等任务，证明了该框架在技能迁移方面的鲁棒性和通用性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效地在不同物体之间进行技能迁移，具有实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring skills between different objects remains one of the corechallenges of open-world robot manipulation. Generalization needs to take intoaccount the high-level structural differences between distinct objects whilestill maintaining similar low-level interaction control. In this paper, wepropose an example-based zero-shot approach to skill transfer. Rather thantreating skills as atomic, we decompose skills into a prioritized list ofgrounded task-axis (GTA) controllers. Each GTAC defines an adaptablecontroller, such as a position or force controller, along an axis. Importantly,the GTACs are grounded in object key points and axes, e.g., the relativeposition of a screw head or the axis of its shaft. Zero-shot transfer is thusachieved by finding semantically-similar grounding features on novel targetobjects. We achieve this example-based grounding of the skills through the useof foundation models, such as SD-DINO, that can detect semantically similarkeypoints of objects. We evaluate our framework on real-robot experiments,including screwing, pouring, and spatula scraping tasks, and demonstrate robustand versatile controller transfer for each.</description>
      <author>example@mail.com (M. Yunus Seker, Shobhit Aggarwal, Oliver Kroemer)</author>
      <guid isPermaLink="false">2505.11680v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach</title>
      <link>http://arxiv.org/abs/2505.11645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in International Society Journal of  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SemiGTX的半监督图学习框架，用于进行行业经济映射，旨在解决现有方法在数据稀缺场景下忽视半监督学习以及缺乏统一的多任务框架的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的经济映射方法主要依赖于监督或无监督学习，但往往忽略了半监督学习，并且缺乏用于全面行业经济分析的多任务框架。&lt;h4&gt;目的&lt;/h4&gt;提出SemiGTX框架，以解决现有方法的不足，实现更有效的行业经济映射。&lt;h4&gt;方法&lt;/h4&gt;SemiGTX框架包含专门的融合编码模块，用于处理不同地理空间数据模式，并将它们无缝集成到一个统一的图结构中。它引入了一种半信息损失函数，结合空间自监督和局部掩码监督回归，以实现更丰富和有效的区域表示。通过多任务学习，SemiGTX在一个统一模型中同时映射一、二、三产业的GDP。&lt;h4&gt;主要发现&lt;/h4&gt;在珠江三角洲地区进行的广泛实验表明，与现有方法相比，SemiGTX模型表现出优异的性能，分别实现了0.93、0.96和0.94的R2分数。在北京和成都的跨区域实验进一步说明了其通用性。系统分析揭示了不同数据模式如何影响模型预测，增强了可解释性，并为区域发展规划提供了有价值的见解。&lt;h4&gt;结论&lt;/h4&gt;SemiGTX框架通过集成多种城市数据，推进了区域经济监测，为精确的经济预测提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained economic mapping through urban representation learning hasemerged as a crucial tool for evidence-based economic decisions. While existingmethods primarily rely on supervised or unsupervised approaches, they oftenoverlook semi-supervised learning in data-scarce scenarios and lack unifiedmulti-task frameworks for comprehensive sectoral economic analysis. To addressthese gaps, we propose SemiGTX, an explainable semi-supervised graph learningframework for sectoral economic mapping. The framework is designed withdedicated fusion encoding modules for various geospatial data modalities,seamlessly integrating them into a cohesive graph structure. It introduces asemi-information loss function that combines spatial self-supervision withlocally masked supervised regression, enabling more informative and effectiveregion representations. Through multi-task learning, SemiGTX concurrently mapsGDP across primary, secondary, and tertiary sectors within a unified model.Extensive experiments conducted in the Pearl River Delta region of Chinademonstrate the model's superior performance compared to existing methods,achieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary andtertiary sectors, respectively. Cross-regional experiments in Beijing andChengdu further illustrate its generality. Systematic analysis reveals howdifferent data modalities influence model predictions, enhancing explainabilitywhile providing valuable insights for regional development planning. Thisrepresentation learning framework advances regional economic monitoring throughdiverse urban data integration, providing a robust foundation for preciseeconomic forecasting.</description>
      <author>example@mail.com (Jinzhou Cao, Xiangxu Wang, Jiashi Chen, Wei Tu, Zhenhui Li, Xindong Yang, Tianhong Zhao, Qingquan Li)</author>
      <guid isPermaLink="false">2505.11645v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for AI-Enabled Biological Design</title>
      <link>http://arxiv.org/abs/2505.11610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as part of the workshop proceedings at AAAI 2025 in the  workshop "Foundation Models for Biological Discoveries"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了AI赋能的生物学设计中的基础模型，重点讨论了将大规模、自监督模型应用于蛋白质工程、小分子设计和基因组序列设计等任务方面的最新进展。&lt;h4&gt;背景&lt;/h4&gt;该领域正在迅速发展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示和讨论当前模型和方法的分类，重点关注适应这些模型进行生物学应用中的挑战和解决方案。&lt;h4&gt;方法&lt;/h4&gt;方法包括生物序列建模架构、生成过程中的可控性和多模态集成。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了开放问题和未来的研究方向，并提出了具体下一步行动，以改善生物序列生成的质量。&lt;h4&gt;结论&lt;/h4&gt;本文提供了对AI在生物学设计中的应用的全面概述，并指出了未来研究的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper surveys foundation models for AI-enabled biological design,focusing on recent developments in applying large-scale, self-supervised modelsto tasks such as protein engineering, small molecule design, and genomicsequence design. Though this domain is evolving rapidly, this survey presentsand discusses a taxonomy of current models and methods. The focus is onchallenges and solutions in adapting these models for biological applications,including biological sequence modeling architectures, controllability ingeneration, and multi-modal integration. The survey concludes with a discussionof open problems and future directions, offering concrete next-steps to improvethe quality of biological sequence generation.</description>
      <author>example@mail.com (Asher Moldwin, Amarda Shehu)</author>
      <guid isPermaLink="false">2505.11610v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis</title>
      <link>http://arxiv.org/abs/2505.11581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 25 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文挑战了表现提升必然导致内部表示提升的观点，通过比较两种不同训练方法下的神经网络在生成单张图像任务上的表现，发现它们的内部表示存在显著差异。&lt;h4&gt;背景&lt;/h4&gt;现代AI领域对通过扩展现有系统来提升性能感到兴奋，但表现提升是否意味着更好的内部表示尚存争议。&lt;h4&gt;目的&lt;/h4&gt;探究不同训练方法对神经网络内部表示的影响，并分析其可能带来的后果。&lt;h4&gt;方法&lt;/h4&gt;将通过开放搜索过程进化的神经网络与通过传统随机梯度下降（SGD）训练的神经网络在生成单张图像的任务上进行比较，并通过可视化隐藏神经元的完整功能行为来观察网络内部表示的差异。&lt;h4&gt;主要发现&lt;/h4&gt;两种网络产生相同的输出行为，但它们的内部表示存在显著差异。SGD训练的网络表现出一种称为破碎纠缠表示（FER）的无序形式，而进化的网络则主要缺乏FER，甚至接近统一的因子表示（UFR）。&lt;h4&gt;结论&lt;/h4&gt;在大型模型中，FER可能会降低模型的核心能力，如泛化、创造力和（持续）学习。因此，理解和减轻FER对于表示学习未来的发展至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现代人工智能领域的兴奋很大程度上源于观察到的扩大现有系统会导致更好的性能。但更好的性能是否必然意味着更好的内部表示？尽管表示乐观者认为必须如此，这篇立场论文挑战了这种观点。我们比较了通过开放搜索过程进化的神经网络与通过传统随机梯度下降（SGD）在生成单个图像的简单任务上约束的网络。这种最小设置提供了一种独特的优势：每个隐藏神经元的完整功能行为可以很容易地被可视化为图像，从而揭示网络输出行为是如何内部构建的，神经元一个接一个。结果是惊人的：尽管两个网络都产生了相同的输出行为，但它们的内部表示存在显著差异。SGD训练的网络表现出一种我们称之为破碎纠缠表示（FER）的无序形式。有趣的是，进化的网络在很大程度上缺乏FER，甚至接近统一的因子表示（UFR）。在大型模型中，FER可能会降低模型的核心能力，如泛化、创造力和（持续）学习。因此，理解和减轻FER对于表示学习未来的发展可能是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Much of the excitement in modern AI is driven by the observation that scalingup existing systems leads to better performance. But does better performancenecessarily imply better internal representations? While the representationaloptimist assumes it must, this position paper challenges that view. We compareneural networks evolved through an open-ended search process to networkstrained via conventional stochastic gradient descent (SGD) on the simple taskof generating a single image. This minimal setup offers a unique advantage:each hidden neuron's full functional behavior can be easily visualized as animage, thus revealing how the network's output behavior is internallyconstructed neuron by neuron. The result is striking: while both networksproduce the same output behavior, their internal representations differdramatically. The SGD-trained networks exhibit a form of disorganization thatwe term fractured entangled representation (FER). Interestingly, the evolvednetworks largely lack FER, even approaching a unified factored representation(UFR). In large models, FER may be degrading core model capacities likegeneralization, creativity, and (continual) learning. Therefore, understandingand mitigating FER could be critical to the future of representation learning.</description>
      <author>example@mail.com (Akarsh Kumar, Jeff Clune, Joel Lehman, Kenneth O. Stanley)</author>
      <guid isPermaLink="false">2505.11581v1</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</title>
      <link>http://arxiv.org/abs/2505.10579v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Medical Image Computing  and Computer-Assisted Intervention (MICCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于基础模型（FMs）的乳腺摄影分类的公平性和偏差，通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集。&lt;h4&gt;背景&lt;/h4&gt;尽管计算机辅助诊断工具在乳腺癌症筛查中得到了发展，但它们的临床采用受到了数据变异和固有偏差的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究FMs在乳腺摄影分类中的公平性和偏差。&lt;h4&gt;方法&lt;/h4&gt;通过结合来自不同来源的大量数据集，包括代表性不足地区的数据和内部数据集，进行广泛的实验。&lt;h4&gt;主要发现&lt;/h4&gt;尽管特定模态的预训练可以提升性能，但基于单个数据集特征的分类器无法跨领域泛化。数据集的聚合提高了整体性能，但并不能完全消除偏差，导致代表性不足的子群体（如极端乳腺密度和年龄组）存在显著差异。领域自适应策略可以减少这些差异，但通常会带来性能权衡。相比之下，公平性感知技术可以在子群体之间产生更稳定和公平的性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了在基于FMs的模型中纳入严格公平性评估和缓解策略的必要性，以促进包容性和可泛化的AI。&lt;h4&gt;翻译&lt;/h4&gt;Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sources-including data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domain-adaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decades, computer-aided diagnosis tools for breast cancer havebeen developed to enhance screening procedures, yet their clinical adoptionremains challenged by data variability and inherent biases. Although foundationmodels (FMs) have recently demonstrated impressive generalizability andtransfer learning capabilities by leveraging vast and diverse datasets, theirperformance can be undermined by spurious correlations that arise fromvariations in image quality, labeling uncertainty, and sensitive patientattributes. In this work, we explore the fairness and bias of FMs for breastmammography classification by leveraging a large pool of datasets from diversesources-including data from underrepresented regions and an in-house dataset.Our extensive experiments show that while modality-specific pre-training of FMsenhances performance, classifiers trained on features from individual datasetsfail to generalize across domains. Aggregating datasets improves overallperformance, yet does not fully mitigate biases, leading to significantdisparities across under-represented subgroups such as extreme breast densitiesand age groups. Furthermore, while domain-adaptation strategies can reducethese disparities, they often incur a performance trade-off. In contrast,fairness-aware techniques yield more stable and equitable performance acrosssubgroups. These findings underscore the necessity of incorporating rigorousfairness evaluations and mitigation strategies into FM-based models to fosterinclusive and generalizable AI.</description>
      <author>example@mail.com (Elodie Germani, Ilayda Selin Türk, Fatima Zeineddine, Charbel Mourad, Shadi Albarqouni)</author>
      <guid isPermaLink="false">2505.10579v2</guid>
      <pubDate>Tue, 20 May 2025 14:39:47 +0800</pubDate>
    </item>
    <item>
      <title>SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness</title>
      <link>http://arxiv.org/abs/2505.08320v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpecSphere是一种新型双通道频谱-空间GNN，能够在保持线性时间复杂性的同时，超越1-Weisfeiler-Lehman的表达能力，并通过轻量级的MLP融合频谱分支和空间分支的表示。&lt;h4&gt;背景&lt;/h4&gt;当前图神经网络（GNN）在预测和鲁棒性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SpecSphere，旨在提高GNN的预测准确性、适应性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;SpecSphere结合了Chebyshev多项式频谱分支和注意力门控空间分支，通过合作对抗的最小-最大游戏训练轻量级MLP进行表示融合。&lt;h4&gt;主要发现&lt;/h4&gt;SpecSphere能够验证每项预测，适应全同质性-异质性频谱，并提供统一Chebyshev近似定理、最小-最大最优风险、闭式鲁棒性证书以及严格超越1-WL的通用逼近能力。&lt;h4&gt;结论&lt;/h4&gt;SpecSphere在节点分类任务上达到最先进的准确性，并提供了更紧的鲁棒性保证，证明了高表达能力、异质性适应性和可证明的鲁棒性可以在单一、可扩展的架构中共存。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SpecSphere, the first dual-pass spectral-spatial GNN thatcertifies every prediction against both $\ell\_{0}$ edge flips and$\ell\_{\infty}$ feature perturbations, adapts to the fullhomophily-heterophily spectrum, and surpasses the expressive power of1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples aChebyshev-polynomial spectral branch with an attention-gated spatial branch andfuses their representations through a lightweight MLP trained in acooperative-adversarial min-max game. We further establish (i) a uniformChebyshev approximation theorem, (ii) minimax-optimal risk across thehomophily-heterophily spectrum, (iii) closed-form robustness certificates, and(iv) universal approximation strictly beyond 1-WL. SpecSphere achievesstate-of-the-art node-classification accuracy and delivers tighter certifiedrobustness guarantees on real-world benchmarks. These results demonstrate thathigh expressivity, heterophily adaptation, and provable robustness can coexistwithin a single, scalable architecture.</description>
      <author>example@mail.com (Yoonhyuk Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2505.08320v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
  <item>
      <title>Inference for Dispersion and Curvature of Random Objects</title>
      <link>http://arxiv.org/abs/2505.09844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了随机对象的统计分析，特别是针对测地度量子空间中的数据对象，提出了基于Frechet方差和度量方差的联合分布的中心极限定理，并探讨了空间曲率与这两种散度度量之间的关系。&lt;h4&gt;背景&lt;/h4&gt;随机对象在统计分析中日益常见，但缺乏线性操作是其中的主要挑战。&lt;h4&gt;目的&lt;/h4&gt;量化统计散度或分布，并推导出Frechet方差和度量方差的联合分布的中心极限定理。&lt;h4&gt;方法&lt;/h4&gt;通过分析Frechet方差和度量方差之间的关系，提出了一种基于散度度量渐进分布来推断空间曲率的新方法。&lt;h4&gt;主要发现&lt;/h4&gt;发现测地空间的Alexandrov曲率决定了这两种散度度量之间的关系，并提出了一种检测未知空间固有曲率的新方法。&lt;h4&gt;结论&lt;/h4&gt;该方法可以应用于检测未知空间的固有曲率，并探讨了其在不同数据类型（如分布数据和点云数据）中的渐近性质和有限样本行为。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了随机对象的统计分析，其中存在许多开放性问题。主要挑战是这些空间中缺乏线性操作。基本统计任务是量化统计散度或分布。对于测地度量子空间中数据对象的两种散度度量，Frechet方差和度量方差，我们推导了它们的联合分布的中心极限定理。这一分析揭示出测地空间的Alexandrov曲率决定了这两种散度度量之间的关系。这表明了一种基于散度度量渐进分布来推断空间曲率的新方法。我们展示了如何使用这种方法来检测未知空间的固有曲率，这表现为空间和生成随机对象的潜在概率测度的一个联合属性。我们研究了该测试的渐近性质及其在包括分布数据和点云数据在内的各种数据类型中的有限样本行为。我们使用表示为对称正定矩阵的步态同步数据和球面上的能量组成数据来说明所提出的关于随机对象固有曲率的推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There are many open questions pertaining to the statistical analysis ofrandom objects, which are increasingly encountered. A major challenge is theabsence of linear operations in such spaces. A basic statistical task is toquantify statistical dispersion or spread. For two measures of dispersion fordata objects in geodesic metric spaces, Fr\'echet variance and metric variance,we derive a central limit theorem (CLT) for their joint distribution. Thisanalysis reveals that the Alexandrov curvature of the geodesic space determinesthe relationship between these two dispersion measures. This suggests a noveltest for inferring the curvature of a space based on the asymptoticdistribution of the dispersion measures. We demonstrate how this test can beemployed to detect the intrinsic curvature of an unknown underlying space,which emerges as a joint property of the space and the underlying probabilitymeasure that generates the random objects. We investigate the asymptoticproperties of the test and its finite-sample behavior for various data types,including distributional data and point cloud data. We illustrate the proposedinference for intrinsic curvature of random objects using gait synchronizationdata represented as symmetric positive definite matrices and energycompositional data on the sphere.</description>
      <author>example@mail.com (Wookyeong Song, Hans-Georg Müller)</author>
      <guid isPermaLink="false">2505.09844v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model</title>
      <link>http://arxiv.org/abs/2505.11421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了实现 Bahnaric-Vietnamesetranslation 的过程，以促进越南两个民族的文化交流。通过转移学习方法和序列到序列预训练语言模型，解决了数据收集和资源不平衡的问题，并提高了翻译的准确性。&lt;h4&gt;背景&lt;/h4&gt;翻译从 Bahnaric 到 Vietnamese 遇到困难，主要是因为缺乏原始的 Bahnaric 资源，包括词汇、语法、对话模式和双语语料库。&lt;h4&gt;目的&lt;/h4&gt;为了实现 Bahnaric-Vietnamesetranslation，以促进越南两个民族的文化交流。&lt;h4&gt;方法&lt;/h4&gt;采用转移学习方法，利用序列到序列预训练语言模型，利用预训练的越南语言模型捕捉语言特征，并使用有限的越南-Bahnaric 双语资源进行迁移学习。同时，通过数据增强和启发式方法增强数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在处理两种语言资源不平衡的问题上有效，同时优化了训练和计算过程，并提高了翻译的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法对于 Bahnaric-Vietnamesetranslation 模型非常有效，有助于语言的扩展和保护，促进两个民族之间的相互理解。&lt;h4&gt;翻译&lt;/h4&gt;本研究探索了实现巴拿尔语-越南语翻译的过程，旨在促进越南两个民族的文化交流。然而，巴拿尔语到越南语的翻译也遇到了一些困难。最突出的挑战是缺乏可用的原始巴拿尔语资源，包括词汇、语法、对话模式和双语语料库，这阻碍了数据收集过程。为了解决这个问题，我们利用了基于序列到序列预训练语言模型的迁移学习方法。首先，我们利用预训练的越南语言模型来捕捉这种语言的特征。特别是，为了进一步服务于机器翻译的目的，我们追求的是序列到序列模型，而不是像 BERT 这样的编码器仅模型或像 GPT 这样的解码器仅模型。利用两种语言之间显著的相似性，我们继续使用目前有限的越南-巴拿尔语双语资源来执行从语言模型到机器翻译的迁移学习。因此，这种方法有助于处理两种语言之间资源不平衡的问题，同时优化了训练和计算过程。此外，我们还通过数据增强增强了数据集，并定义了一些启发式方法来帮助翻译更加精确。我们的方法已被证明对于巴拿尔语-越南语翻译模型非常有效，有助于语言的扩展和保护，促进两个民族之间的相互理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the journey towards achieving Bahnaric-Vietnamesetranslation for the sake of culturally bridging the two ethnic groups inVietnam. However, translating from Bahnaric to Vietnamese also encounters somedifficulties. The most prominent challenge is the lack of available originalBahnaric resources source language, including vocabulary, grammar, dialoguepatterns and bilingual corpus, which hinders the data collection process fortraining. To address this, we leverage a transfer learning approach usingsequence-to-sequence pre-training language model. First of all, we leverage apre-trained Vietnamese language model to capture the characteristics of thislanguage. Especially, to further serve the purpose of machine translation, weaim for a sequence-to-sequence model, not encoder-only like BERT ordecoder-only like GPT. Taking advantage of significant similarity between thetwo languages, we continue training the model with the currently limitedbilingual resources of Vietnamese-Bahnaric text to perform the transferlearning from language model to machine translation. Thus, this approach canhelp to handle the problem of imbalanced resources between two languages, whilealso optimizing the training and computational processes. Additionally, we alsoenhanced the datasets using data augmentation to generate additional resourcesand defined some heuristic methods to help the translation more precise. Ourapproach has been validated to be highly effective for the Bahnaric-Vietnamesetranslation model, contributing to the expansion and preservation of languages,and facilitating better mutual understanding between the two ethnic people.</description>
      <author>example@mail.com (Phan Tran Minh Dat, Vo Hoang Nhat Khang, Quan Thanh Tho)</author>
      <guid isPermaLink="false">2505.11421v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
      <link>http://arxiv.org/abs/2505.11484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SoftCoT++，一种扩展SoftCoT到测试时间缩放（TTS）范式的方法，通过允许多样化的思维路径探索来提高推理性能。&lt;h4&gt;背景&lt;/h4&gt;现有的TTS方法通过生成更多中间步骤在离散标记空间中操作，而近期的研究表明在连续潜在空间中进行思维可以进一步增强推理性能。&lt;h4&gt;目的&lt;/h4&gt;目的是通过引入SoftCoT++来克服连续空间中固定潜在表示所导致的多样化探索限制，从而提高推理性能。&lt;h4&gt;方法&lt;/h4&gt;SoftCoT++通过使用多个专门的初始标记扰动潜在思维，并应用对比学习来促进软思维表示之间的多样性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，SoftCoT++显著提高了SoftCoT的性能，并且优于使用自一致性缩放的SoftCoT，同时与传统的缩放技术如自一致性具有很好的兼容性。&lt;h4&gt;结论&lt;/h4&gt;SoftCoT++是一种有效的TTS方法，能够通过多样化的思维路径探索显著提升推理性能。&lt;h4&gt;翻译&lt;/h4&gt;Test-Time Scaling (TTS)指的是在推理过程中分配额外计算来提高推理性能的方法，而不改变模型的参数。虽然现有的TTS方法通过生成更多中间步骤在离散标记空间中操作，但最近在Coconut和SoftCoT中的研究表明，在连续潜在空间中进行思维可以进一步增强推理性能。这种潜在思维编码了信息性思维，而没有与自回归标记生成相关的信息损失，这激发了人们对连续空间推理的兴趣。与重复采样以探索不同推理路径的离散解码不同，连续空间中的潜在表示对于给定的输入是固定的，这限制了多样化探索，因为所有解码路径都源自相同的潜在思维。为了克服这一限制，我们引入了SoftCoT++，通过允许多样化的思维路径探索来扩展SoftCoT到测试时间缩放（TTS）范式。具体来说，我们通过多个专门的初始标记扰动潜在思维，并应用对比学习来促进软思维表示之间的多样性。在五个推理基准和两种不同的LLM架构上的实验表明，SoftCoT++显著提高了SoftCoT的性能，并且优于使用自一致性缩放的SoftCoT。此外，它还显示出与传统的缩放技术，如自一致性，具有很好的兼容性。源代码可在https://github.com/xuyige/SoftCoT上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-Time Scaling (TTS) refers to approaches that improve reasoningperformance by allocating extra computation during inference, without alteringthe model's parameters. While existing TTS methods operate in a discrete tokenspace by generating more intermediate steps, recent studies in Coconut andSoftCoT have demonstrated that thinking in the continuous latent space canfurther enhance the reasoning performance. Such latent thoughts encodeinformative thinking without the information loss associated withautoregressive token generation, sparking increased interest incontinuous-space reasoning. Unlike discrete decoding, where repeated samplingenables exploring diverse reasoning paths, latent representations in continuousspace are fixed for a given input, which limits diverse exploration, as alldecoded paths originate from the same latent thought. To overcome thislimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scalingparadigm by enabling diverse exploration of thinking paths. Specifically, weperturb latent thoughts via multiple specialized initial tokens and applycontrastive learning to promote diversity among soft thought representations.Experiments across five reasoning benchmarks and two distinct LLM architecturesdemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperformsSoftCoT with self-consistency scaling. Moreover, it shows strong compatibilitywith conventional scaling techniques such as self-consistency. Source code isavailable at https://github.com/xuyige/SoftCoT.</description>
      <author>example@mail.com (Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao)</author>
      <guid isPermaLink="false">2505.11484v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Finding Counterfactual Evidences for Node Classification</title>
      <link>http://arxiv.org/abs/2505.11396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于图神经网络（GNN）的节点分类任务中寻找反事实证据的问题，提出了一种有效且高效的搜索算法和一种新颖的索引解决方案，以识别反事实证据，并证明了反事实证据在提高GNN的公平性和准确性方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;反事实学习是一个基于因果性的重要范式，它承诺缓解图神经网络（GNN）的常见问题，如公平性和可解释性。然而，在许多现实世界应用领域中，由于无法进行随机对照试验，人们必须依赖于可用的观察（事实）数据来检测反事实。&lt;h4&gt;目的&lt;/h4&gt;寻找基于GNN的节点分类任务的反事实证据，以提升GNN的公平性和准确性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种有效且高效的搜索算法和一种新颖的索引解决方案，该解决方案利用节点特征和结构信息来识别反事实证据，并且该方法超越了任何特定的GNN。&lt;h4&gt;主要发现&lt;/h4&gt;反事实证据是一对节点，尽管它们在特征和邻域子图结构上表现出极大的相似性，但被GNN分类为不同的类别。&lt;h4&gt;结论&lt;/h4&gt;反事实证据具有提高GNN公平性和准确性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Counterfactual learning is emerging as an important paradigm, rooted incausality, which promises to alleviate common issues of graph neural networks(GNNs), such as fairness and interpretability. However, as in many real-worldapplication domains where conducting randomized controlled trials isimpractical, one has to rely on available observational (factual) data todetect counterfactuals. In this paper, we introduce and tackle the problem ofsearching for counterfactual evidences for the GNN-based node classificationtask. A counterfactual evidence is a pair of nodes such that, regardless theyexhibit great similarity both in the features and in their neighborhoodsubgraph structures, they are classified differently by the GNN. We developeffective and efficient search algorithms and a novel indexing solution thatleverages both node features and structural information to identifycounterfactual evidences, and generalizes beyond any specific GNN. Throughvarious downstream applications, we demonstrate the potential of counterfactualevidences to enhance fairness and accuracy of GNNs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Counterfactual learning is emerging as an important paradigm, rooted incausality, which promises to alleviate common issues of graph neural networks(GNNs), such as fairness and interpretability. However, as in many real-worldapplication domains where conducting randomized controlled trials isimpractical, one has to rely on available observational (factual) data todetect counterfactuals. In this paper, we introduce and tackle the problem ofsearching for counterfactual evidences for the GNN-based node classificationtask. A counterfactual evidence is a pair of nodes such that, regardless theyexhibit great similarity both in the features and in their neighborhoodsubgraph structures, they are classified differently by the GNN. We developeffective and efficient search algorithms and a novel indexing solution thatleverages both node features and structural information to identifycounterfactual evidences, and generalizes beyond any specific GNN. Throughvarious downstream applications, we demonstrate the potential of counterfactualevidences to enhance fairness and accuracy of GNNs.</description>
      <author>example@mail.com (Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi)</author>
      <guid isPermaLink="false">2505.11396v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自动检索合成CAD模型的方法，用于生成高质量的3D标注数据，以训练监督深度学习模型，并验证了这种方法在点云补全和单视图CAD模型检索与对齐任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;高级3D场景理解在许多应用中至关重要，但生成准确的3D标注数据具有挑战性，这给深度学习模型的发展带来了困难。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用自动检索合成CAD模型的方法来生成高质量的3D标注数据，并训练深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;采用了一种类似于以前用于自动标注ScanNet场景中物体9D姿态和CAD模型的流程，并将其应用于ScanNet++ v1数据集。&lt;h4&gt;主要发现&lt;/h4&gt;自动获取的标注数据可以用于训练深度学习模型，并且训练出的模型在性能上优于手动标注数据训练的模型。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注有潜力提高模型性能，同时显著降低标注成本，未来将发布我们开发的标注工具SCANnotate++和训练模型。&lt;h4&gt;翻译&lt;/h4&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v4</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph Representational Learning: When Does More Expressivity Hurt Generalization?</title>
      <link>http://arxiv.org/abs/2505.11298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络（GNNs）的表达性和预测性能之间的关系，并引入了一系列前度量来捕捉图之间的结构相似度，将相似度与泛化能力联系起来，进而关联到高表达能力GNNs的性能。&lt;h4&gt;背景&lt;/h4&gt;GNNs作为处理结构化数据的强大工具，其表达性和预测性能之间的关系尚不明确。&lt;h4&gt;目的&lt;/h4&gt;介绍一种方法来评估GNNs的表达能力和预测性能，并探究其泛化能力。&lt;h4&gt;方法&lt;/h4&gt;在考虑图标签与结构特征相关联的设置下，推导出依赖于训练和测试图之间的距离、模型复杂度和训练集大小的泛化界限。&lt;h4&gt;主要发现&lt;/h4&gt;发现更具有表达能力的GNNs可能泛化能力较差，除非其增加的复杂性通过足够大的训练集或减少训练和测试图之间的距离来平衡。&lt;h4&gt;结论&lt;/h4&gt;本研究将表达性和泛化能力联系起来，提供了理论上的见解，并得到了实证结果的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图神经网络（GNNs）是学习结构化数据的强大工具，但其表达性与预测性能之间的关系尚不明确。我们介绍了一族前度量，用于捕捉图之间不同程度的结构相似性，并将这些相似性与泛化能力相关联，从而关联到高表达能力GNNs的性能。通过考虑一个图标签与结构特征相关的设置，我们推导出依赖于训练和测试图之间的距离、模型复杂度和训练集大小的泛化界限。这些界限揭示，如果其增加的复杂性不能通过足够大的训练集或减少训练和测试图之间的距离来平衡，那么更具有表达能力的GNNs可能泛化能力较差。我们的发现将表达性与泛化能力联系起来，提供了理论上的见解，并得到了实证结果的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful tools for learning on structureddata, yet the relationship between their expressivity and predictiveperformance remains unclear. We introduce a family of premetrics that capturedifferent degrees of structural similarity between graphs and relate thesesimilarities to generalization, and consequently, the performance of expressiveGNNs. By considering a setting where graph labels are correlated withstructural features, we derive generalization bounds that depend on thedistance between training and test graphs, model complexity, and training setsize. These bounds reveal that more expressive GNNs may generalize worse unlesstheir increased complexity is balanced by a sufficiently large training set orreduced distance between training and test graphs. Our findings relateexpressivity and generalization, offering theoretical insights supported byempirical results.</description>
      <author>example@mail.com (Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer)</author>
      <guid isPermaLink="false">2505.11298v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Fractal Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.11356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Fractal Graph Contrastive Learning（FractalGCL）的理论驱动框架，旨在解决图对比学习（GCL）中数据增强的局限性。该框架通过利用分形自相似性来强制执行全局拓扑一致性，并引入了两种关键创新：基于重归一化的增强和具有分形维度意识的对比损失。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）在图自监督学习领域受到广泛关注，但其性能高度依赖于能够生成语义一致正对的增广数据。现有的策略通常依赖于随机扰动或局部结构保持，但缺乏对增广视图之间全局结构一致性的显式控制。&lt;h4&gt;目的&lt;/h4&gt;提出FractalGCL以解决现有GCL方法的局限性，提高图表示质量，并减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;FractalGCL通过引入基于重归一化的增强和分形维度意识的对比损失来实现其目标。此外，为了减轻分形维度估计的计算开销，作者推导出了一个单次估计器，通过证明原始图和重归一化图之间的维度差异会弱收敛到一个中心高斯分布。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，FractalGCL在标准基准测试中实现了最先进的性能，并且在交通网络上比传统基线平均提高了约7%。&lt;h4&gt;结论&lt;/h4&gt;FractalGCL通过引入创新的技术，显著提高了图对比学习的性能，并减少了计算成本。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然图对比学习（GCL）在图自监督学习领域受到了相当大的关注，但其性能高度依赖于预期能够生成语义一致正对的增广数据。现有的策略通常依赖于随机扰动或局部结构保持，但缺乏对增广视图之间全局结构一致性的显式控制。为了解决这一局限性，我们提出了基于分形的图对比学习（FractalGCL），这是一种理论驱动的框架，它利用分形自相似性来强制执行全局拓扑一致性。FractalGCL引入了两个关键创新：一种基于重归一化的增强，通过箱覆盖生成结构对齐的正视图；以及一种具有分形维度意识的对比损失，根据它们的分形维度对齐图嵌入。虽然结合这两种创新显著提高了图表示质量，但也增加了非平凡的计算开销。为了减轻分形维度估计的计算开销，我们推导出了一个单次估计器，通过证明原始图和重归一化图之间的维度差异会弱收敛到一个中心高斯分布。这一理论洞察使得维度计算成本降低了大约一个数量级，整体训练时间减少了大约61%。实验表明，FractalGCL不仅在标准基准测试中实现了最先进的性能，而且在交通网络上平均比传统基线提高了约7%。代码可在（https://anonymous.4open.science/r/FractalGCL-0511）获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Contrastive Learning (GCL) has attracted considerable attentionin the field of graph self-supervised learning, its performance heavily relieson data augmentations that are expected to generate semantically consistentpositive pairs. Existing strategies typically resort to random perturbations orlocal structure preservation, yet lack explicit control over global structuralconsistency between augmented views. To address this limitation, we proposeFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework thatleverages fractal self-similarity to enforce global topological coherence.FractalGCL introduces two key innovations: a renormalisation-based augmentationthat generates structurally aligned positive views via box coverings; and afractal-dimension-aware contrastive loss that aligns graph embeddings accordingto their fractal dimensions. While combining the two innovations markedlyboosts graph-representation quality, it also adds non-trivial computationaloverhead. To mitigate the computational overhead of fractal dimensionestimation, we derive a one-shot estimator by proving that the dimensiondiscrepancy between original and renormalised graphs converges weakly to acentred Gaussian distribution. This theoretical insight enables a reduction indimension computation cost by an order of magnitude, cutting overall trainingtime by approximately 61%. The experiments show that FractalGCL not onlydelivers state-of-the-art results on standard benchmarks but also outperformstraditional baselines on traffic networks by an average margin of aboutremarkably 7%. Codes are available at(https://anonymous.4open.science/r/FractalGCL-0511).</description>
      <author>example@mail.com (Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang)</author>
      <guid isPermaLink="false">2505.11356v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2505.11099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Hybrid-Emba3D的新模型，用于解决点云分类任务中的效率与复杂度平衡问题。&lt;h4&gt;背景&lt;/h4&gt;点云分类任务需要高效提取局部几何特征，同时保持模型复杂度。Mamba架构利用状态空间模型（SSM）的线性复杂度优势来克服Transformer的计算瓶颈，但其在处理空间相关性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的模型，以增强局部特征的判别能力并提高分类准确率。&lt;h4&gt;方法&lt;/h4&gt;Hybrid-Emba3D模型通过结合几何特征耦合机制和跨路径特征混合，以及局部几何池化技术，来提升局部特征的提取和全局建模能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在ModelNet40数据集上达到了95.99%的分类准确率，同时仅增加了0.03M的额外参数。&lt;h4&gt;结论&lt;/h4&gt;Hybrid-Emba3D模型通过创新的设计，有效解决了点云分类任务中的效率与复杂度平衡问题，实现了新的性能水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The point cloud classification tasks face the dual challenge of efficientlyextracting local geometric features while maintaining model complexity. TheMamba architecture utilizes the linear complexity advantage of state spacemodels (SSMs) to overcome the computational bottleneck of Transformers whilebalancing global modeling capabilities. However, the inherent contradictionbetween its unidirectional dependency and the unordered nature of point cloudsimpedes modeling spatial correlation in local neighborhoods, thus constraininggeometric feature extraction. This paper proposes Hybrid-Emba3D, abidirectional Mamba model enhanced by geometry-feature coupling and cross-pathfeature hybridization. The Local geometric pooling with geometry-featurecoupling mechanism significantly enhances local feature discriminative powervia coordinated propagation and dynamic aggregation of geometric informationbetween local center points and their neighborhoods, without introducingadditional parameters. The designed Collaborative feature enhancer adoptsdual-path hybridization, effectively handling local mutations and sparse keysignals, breaking through the limitations of traditional SSM long-rangemodeling. Experimental results demonstrate that the proposed model achieves anew SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03Madditional.</description>
      <author>example@mail.com (Bin Liu, Chunyang Wang, Xuelian Liu, Guan Xi, Ge Zhang, Ziteng Yao, Mengxue Dong)</author>
      <guid isPermaLink="false">2505.11099v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Maximizing Asynchronicity in Event-based Neural Networks</title>
      <link>http://arxiv.org/abs/2505.11165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 5 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了EVA（EVent Asynchronous representation learning）框架，旨在解决事件相机异步、稀疏特性对机器学习（ML）的挑战，并实现了高表达性和通用性的事件表示学习。&lt;h4&gt;背景&lt;/h4&gt;事件相机提供高时间分辨率、低延迟和最小冗余的视觉数据，但其异步、稀疏的序列性质对标准张量机器学习提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出EVA框架，生成高表达性和通用性的事件表示，以解决异步到同步（A2S）转换中的表示表达性和泛化性问题。&lt;h4&gt;方法&lt;/h4&gt;EVA框架受到事件与语言之间相似性的启发，借鉴了语言建模中的线性注意力和自监督学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;EVA在识别任务（DVS128-Gesture和N-Cars）中优于之前的A2S方法，并且是第一个成功掌握检测任务的A2S框架，在Gen1数据集上达到了47.7 mAP的显著结果。&lt;h4&gt;结论&lt;/h4&gt;EVA框架具有推动实时事件基础视觉应用发展的转型潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras deliver visual data with high temporal resolution, low latency,and minimal redundancy, yet their asynchronous, sparse sequential naturechallenges standard tensor-based machine learning (ML). While the recentasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap byasynchronously encoding events into learned representations for ML pipelines,existing A2S approaches often sacrifice representation expressivity andgeneralizability compared to dense, synchronous methods. This paper introducesEVA (EVent Asynchronous representation learning), a novel A2S framework togenerate highly expressive and generalizable event-by-event representations.Inspired by the analogy between events and language, EVA uniquely adaptsadvances from language modeling in linear attention and self-supervisedlearning for its construction. In demonstration, EVA outperforms prior A2Smethods on recognition tasks (DVS128-Gesture and N-Cars), and represents thefirst A2S framework to successfully master demanding detection tasks, achievinga remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA'stransformative potential for advancing real-time event-based visionapplications.</description>
      <author>example@mail.com (Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang)</author>
      <guid isPermaLink="false">2505.11165v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>ASRC-SNN: Adaptive Skip Recurrent Connection Spiking Neural Network</title>
      <link>http://arxiv.org/abs/2505.11455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，循环脉冲神经网络（RSNNs）在长期时间建模方面展现出良好的潜力。许多研究集中于改进神经元模型并整合循环结构，利用其协同效应来提高脉冲神经网络（SNNs）的长期时间建模能力。然而，这些研究往往过分强调神经元的作用，忽视了将神经元和循环结构作为一个整体框架进行分析的重要性。&lt;h4&gt;背景&lt;/h4&gt;近年来，RSNNs在长期时间建模方面显示出良好的潜力，但现有研究过分强调神经元的作用，忽视了神经元和循环结构的整体分析。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在将神经元和循环结构视为一个整体系统，对时间维度上的梯度传播进行系统分析，并提出解决方案以缓解梯度消失问题，提高长期时间建模性能。&lt;h4&gt;方法&lt;/h4&gt;提出跳过循环连接（SRC）作为传统循环结构的替代方案，以有效缓解梯度消失问题并提升长期时间建模性能。此外，还提出了自适应跳过循环连接（ASRC），该方法可以在网络每层学习跳过循环连接的跳过跨度。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，将传统循环结构在RSNN中的使用替换为SRC，可以显著提高模型在时间基准数据集上的性能。此外，ASRC-SNN在时间建模能力和鲁棒性方面优于SRC-SNN。&lt;h4&gt;结论&lt;/h4&gt;通过引入跳过循环连接和自适应跳过循环连接，可以有效解决RSNN中的梯度消失问题，提高长期时间建模性能，并增强模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Recurrent Spiking Neural Networks (RSNNs) have shownpromising potential in long-term temporal modeling. Many studies focus onimproving neuron models and also integrate recurrent structures, leveragingtheir synergistic effects to improve the long-term temporal modelingcapabilities of Spiking Neural Networks (SNNs). However, these studies oftenplace an excessive emphasis on the role of neurons, overlooking the importanceof analyzing neurons and recurrent structures as an integrated framework. Inthis work, we consider neurons and recurrent structures as an integrated systemand conduct a systematic analysis of gradient propagation along the temporaldimension, revealing a challenging gradient vanishing problem. To address thisissue, we propose the Skip Recurrent Connection (SRC) as a replacement for thevanilla recurrent structure, effectively mitigating the gradient vanishingproblem and enhancing long-term temporal modeling performance. Additionally, wepropose the Adaptive Skip Recurrent Connection (ASRC), a method that can learnthe skip span of skip recurrent connection in each layer of the network.Experiments show that replacing the vanilla recurrent structure in RSNN withSRC significantly improves the model's performance on temporal benchmarkdatasets. Moreover, ASRC-SNN outperforms SRC-SNN in terms of temporal modelingcapabilities and robustness.</description>
      <author>example@mail.com (Shang Xu, Jiayu Zhang, Ziming Wang, Runhao Jiang, Rui Yan, Huajin Tang)</author>
      <guid isPermaLink="false">2505.11455v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions</title>
      <link>http://arxiv.org/abs/2505.11417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的数据集和评估基准，用于评估和改进在边缘设备上部署的小型语言模型，重点关注在智能家居环境中从多会话自然语言交互中进行用户画像。&lt;h4&gt;背景&lt;/h4&gt;现有小型语言模型在准确捕捉用户行为方面表现不足，而边缘设备上的处理具有保护用户隐私、最小化延迟和实现个性化体验等优势。&lt;h4&gt;目的&lt;/h4&gt;构建一个数据集和评估基准，以评估和提升小型语言模型在边缘设备上的表现，特别是其在用户画像方面的能力。&lt;h4&gt;方法&lt;/h4&gt;数据集的核心是结构化用户画像，包括一系列的行为模式。大语言模型（LLM）使用这些画像生成模拟真实、多样、情境感知的对话。主要任务是通过对交互历史的分析来推断用户的行为模式和偏好。&lt;h4&gt;主要发现&lt;/h4&gt;小型模型在重建用户画像方面有一定能力，但与大型模型相比，在准确捕捉用户行为方面仍有显著差距。&lt;h4&gt;结论&lt;/h4&gt;该数据集为在边缘设备上开发和发展行为建模提供了现实、结构化的测试平台，是朝着实现智能、尊重隐私的AI系统迈出的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices. The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel dataset and evaluation benchmark designed toassess and improve small language models deployable on edge devices, with afocus on user profiling from multi-session natural language interactions insmart home environments. At the core of the dataset are structured userprofiles, each defined by a set of routines - context-triggered, repeatablepatterns of behavior that govern how users interact with their home systems.Using these profiles as input, a large language model (LLM) generatescorresponding interaction sessions that simulate realistic, diverse, andcontext-aware dialogues between users and their devices.  The primary task supported by this dataset is profile reconstruction:inferring user routines and preferences solely from interactions history. Toassess how well current models can perform this task under realisticconditions, we benchmarked several state-of-the-art compact language models andcompared their performance against large foundation models. Our results showthat while small models demonstrate some capability in reconstructing profiles,they still fall significantly short of large models in accurately capturinguser behavior. This performance gap poses a major challenge - particularlybecause on-device processing offers critical advantages, such as preservinguser privacy, minimizing latency, and enabling personalized experiences withoutreliance on the cloud. By providing a realistic, structured testbed fordeveloping and evaluating behavioral modeling under these constraints, ourdataset represents a key step toward enabling intelligent, privacy-respectingAI systems that learn and adapt directly on user-owned devices.</description>
      <author>example@mail.com (Patryk Bartkowiak, Michal Podstawski)</author>
      <guid isPermaLink="false">2505.11417v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Assessing the Performance of Analog Training for Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.11067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为c-TTv2的新算法，用于解决模拟内存计算中深度学习和迁移学习所面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;模拟内存计算是一种新兴的计算范式，有望实现快速、并行和节能的深度学习训练和迁移学习。然而，由于缺乏合适的训练算法，这一承诺尚未实现。&lt;h4&gt;目的&lt;/h4&gt;评估c-TTv2算法在模拟迁移学习中的性能，并研究其对设备规格变化的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用Swin-ViT模型在CIFAR100数据集的一个子集上评估c-TTv2算法的性能，并研究其对权重传输噪声、对称点偏移和对称点变异等设备规格变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;c-TTv2算法通过利用切割技术解决了模拟内存计算中的一些挑战，并表现出良好的性能。&lt;h4&gt;结论&lt;/h4&gt;c-TTv2算法为模拟内存计算中的深度学习和迁移学习提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Analog in-memory computing is a next-generation computing paradigm that promises fast, parallel, and energy-efficient deep learning training and transfer learning (TL). However, achieving this promise has remained elusive due to a lack of suitable training algorithms. Analog memory devices exhibit asymmetric and non-linear switching behavior in addition to device-to-device variation, meaning that most, if not all, of the current off-the-shelf training algorithms cannot achieve good training outcomes. Also, recently introduced algorithms have enjoyed limited attention, as they require bi-directionally switching devices of unrealistically high symmetry and precision and are highly sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which leverages the chopped technique to address many of the challenges mentioned above. In this paper, we assess the performance of the c-TTv2 algorithm for analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also investigate the robustness of our algorithm to changes in some device specifications, including weight transfer noise, symmetry point skew, and symmetry point variability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog in-memory computing is a next-generation computing paradigm thatpromises fast, parallel, and energy-efficient deep learning training andtransfer learning (TL). However, achieving this promise has remained elusivedue to a lack of suitable training algorithms. Analog memory devices exhibitasymmetric and non-linear switching behavior in addition to device-to-devicevariation, meaning that most, if not all, of the current off-the-shelf trainingalgorithms cannot achieve good training outcomes. Also, recently introducedalgorithms have enjoyed limited attention, as they require bi-directionallyswitching devices of unrealistically high symmetry and precision and are highlysensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, whichleverages the chopped technique to address many of the challenges mentionedabove. In this paper, we assess the performance of the c-TTv2 algorithm foranalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We alsoinvestigate the robustness of our algorithm to changes in some devicespecifications, including weight transfer noise, symmetry point skew, andsymmetry point variability</description>
      <author>example@mail.com (Omobayode Fagbohungbe, Corey Lammie, Malte J. Rasch, Takashi Ando, Tayfun Gokmen, Vijay Narayanan)</author>
      <guid isPermaLink="false">2505.11067v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>STEP: A Unified Spiking Transformer Evaluation Platform for Fair and Reproducible Benchmarking</title>
      <link>http://arxiv.org/abs/2505.11151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了用于Spiking Transformers的统一基准框架STEP，旨在促进公平比较和原理分析。&lt;h4&gt;背景&lt;/h4&gt;Spiking Transformers结合了脉冲神经网络的高效性和自注意力机制的表征能力，但目前缺乏标准化实现、评估流程和一致的设计选择，阻碍了公平比较和原理分析。&lt;h4&gt;目的&lt;/h4&gt;提出STEP框架，支持广泛的任务，包括分类、分割和检测，并促进Spiking Transformers的公平比较和原理分析。&lt;h4&gt;方法&lt;/h4&gt;STEP框架提供模块化支持，包括脉冲神经元、输入编码、代理梯度以及多个后端（如SpikingJelly、BrainCog）。使用STEP框架，作者生产并评估了多个代表性模型，并进行了关于注意力设计、神经元类型、编码方案和时间建模能力的系统性消融研究。此外，还提出了一种统一的能量估计分析模型。&lt;h4&gt;主要发现&lt;/h4&gt;当前Spiking Transformers高度依赖卷积前端，缺乏强大的时间建模能力，强调了需要脉冲原生架构创新。&lt;h4&gt;结论&lt;/h4&gt;STEP框架有助于Spiking Transformers的研究和发展，指出了当前架构的局限性，并提出了未来研究的方向。&lt;h4&gt;翻译&lt;/h4&gt;Spiking Transformers作为一种结合脉冲神经网络效率和自注意力表征能力的架构，近年来受到了关注。然而，由于缺乏标准化的实现、评估流程和一致的设计选择，公平比较和原理分析受到了阻碍。本文介绍了STEP，一个用于Spiking Transformers的统一基准框架，支持广泛的任务，包括在静态、基于事件和时序数据集上的分类、分割和检测。STEP提供模块化支持，包括脉冲神经元、输入编码、代理梯度以及多个后端（如SpikingJelly、BrainCog）。使用STEP，我们生产并评估了多个代表性模型，并进行了关于注意力设计、神经元类型、编码方案和时间建模能力的系统性消融研究。我们还提出了一种统一的能量估计分析模型，考虑了脉冲稀疏性、位宽和内存访问，并表明量化的人工神经网络可能提供相当的或更好的能效。我们的结果表明，当前的Spiking Transformers高度依赖于卷积前端，缺乏强大的时间建模能力，强调了需要脉冲原生架构创新。完整代码可在https://github.com/Fancyssc/STEP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking Transformers have recently emerged as promising architectures forcombining the efficiency of spiking neural networks with the representationalpower of self-attention. However, the lack of standardized implementations,evaluation pipelines, and consistent design choices has hindered faircomparison and principled analysis. In this paper, we introduce \textbf{STEP},a unified benchmark framework for Spiking Transformers that supports a widerange of tasks, including classification, segmentation, and detection acrossstatic, event-based, and sequential datasets. STEP provides modular support fordiverse components such as spiking neurons, input encodings, surrogategradients, and multiple backends (e.g., SpikingJelly, BrainCog). Using STEP, wereproduce and evaluate several representative models, and conduct systematicablation studies on attention design, neuron types, encoding schemes, andtemporal modeling capabilities. We also propose a unified analytical model forenergy estimation, accounting for spike sparsity, bitwidth, and memory access,and show that quantized ANNs may offer comparable or better energy efficiency.Our results suggest that current Spiking Transformers rely heavily onconvolutional frontends and lack strong temporal modeling, underscoring theneed for spike-native architectural innovations. The full code is available at:https://github.com/Fancyssc/STEP</description>
      <author>example@mail.com (Sicheng Shen, Dongcheng Zhao, Linghao Feng, Zeyang Yue, Jindong Li, Tenglong Li, Guobin Shen, Yi Zeng)</author>
      <guid isPermaLink="false">2505.11151v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>What Can We Learn From MIMO Graph Convolutions?</title>
      <link>http://arxiv.org/abs/2505.11346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多输入多输出（MIMO）图卷积方法，并在多计算图上实现了局部化近似。&lt;h4&gt;背景&lt;/h4&gt;大多数图神经网络（GNNs）在单输入单输出（SISO）情况下使用图傅里叶域中导出的通用图卷积近似。&lt;h4&gt;目的&lt;/h4&gt;在MIMO情况下直接近似MIMO图卷积。&lt;h4&gt;方法&lt;/h4&gt;通过卷积定理推导出MIMO图卷积，并在MIMO情况下直接近似。引入了局部化MIMO图卷积（LMGCs），这是一种泛化了许多线性消息传递神经网络的局部近似方法。&lt;h4&gt;主要发现&lt;/h4&gt;发现MIMO图卷积的关键特性是作用在多个计算图上，或者等价地，为每对节点应用不同的特征变换。对于几乎所有的边权重选择，证明了LMGCs在多集上是单射的，并且当使用多个计算图时，结果表示是线性无关的。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，LMGC可以结合各种方法的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数图神经网络（GNNs）利用图傅里叶域中导出的通用图卷积的近似。虽然GNNs通常应用于多输入多输出（MIMO）情况，但近似是在单输入单输出（SISO）情况下进行的。在这项工作中，我们首先通过卷积定理推导出MIMO图卷积，并在MIMO情况下直接近似它。我们发现图卷积的关键MIMO特定属性是在多个计算图上操作，或者等价地，为每对节点应用不同的特征变换。作为一种局部近似，我们引入了局部化MIMO图卷积（LMGCs），它泛化了许多线性消息传递神经网络。对于几乎所有的边权重选择，我们证明了具有单个计算图的LMGC在多集上是单射的，并且当使用多个计算图时，结果表示是线性无关的。我们的实验结果证实了LMGC可以结合各种方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most graph neural networks (GNNs) utilize approximations of the general graphconvolution derived in the graph Fourier domain. While GNNs are typicallyapplied in the multi-input multi-output (MIMO) case, the approximations areperformed in the single-input single-output (SISO) case. In this work, we firstderive the MIMO graph convolution through the convolution theorem andapproximate it directly in the MIMO case. We find the key MIMO-specificproperty of the graph convolution to be operating on multiple computationalgraphs, or equivalently, applying distinct feature transformations for eachpair of nodes. As a localized approximation, we introduce localized MIMO graphconvolutions (LMGCs), which generalize many linear message-passing neuralnetworks. For almost every choice of edge weights, we prove that LMGCs with asingle computational graph are injective on multisets, and the resultingrepresentations are linearly independent when more than one computational graphis used. Our experimental results confirm that an LMGC can combine the benefitsof various methods.</description>
      <author>example@mail.com (Andreas Roth, Thomas Liebig)</author>
      <guid isPermaLink="false">2505.11346v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining</title>
      <link>http://arxiv.org/abs/2505.11293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为'B3'的批量构建策略，旨在为对比学习（CL）创建高质量批量，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种常用的训练嵌入模型的技术，通过将语义相似的示例（正例）拉近，将不相似的示例（负例）推远。&lt;h4&gt;目的&lt;/h4&gt;提高训练批次的大小和质量，以增强对比学习模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的教师嵌入模型对所有数据集中的示例进行排名，构建稀疏相似图，然后应用社区检测算法识别相互之间作为强负例的示例集群，最后使用这些集群构建包含丰富负例的批量。&lt;h4&gt;主要发现&lt;/h4&gt;在MMEB多模态嵌入基准测试（36个任务）上，该方法在7B和2B模型规模上分别比以前的最佳方法提高了1.3和2.9个点。值得注意的是，使用B3训练的模型即使批量大小仅为64，也超越了现有的最佳结果，而其他方法所需的批量大小至少是64的4-16倍。&lt;h4&gt;结论&lt;/h4&gt;B3批量构建策略能够有效提高对比学习模型的效果，即使在较小的批量大小下也能实现显著性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) is a prevalent technique for training embeddingmodels, which pulls semantically similar examples (positives) closer in therepresentation space while pushing dissimilar ones (negatives) further apart. Akey source of negatives are 'in-batch' examples, i.e., positives from otherexamples in the batch. Effectiveness of such models is hence stronglyinfluenced by the size and quality of training batches. In this work, wepropose 'Breaking the Batch Barrier' (B3), a novel batch construction strategydesigned to curate high-quality batches for CL. Our approach begins by using apretrained teacher embedding model to rank all examples in the dataset, fromwhich a sparse similarity graph is constructed. A community detection algorithmis then applied to this graph to identify clusters of examples that serve asstrong negatives for one another. The clusters are then used to constructbatches that are rich in in-batch negatives. Empirical results on the MMEBmultimodal embedding benchmark (36 tasks) demonstrate that our method sets anew state of the art, outperforming previous best methods by +1.3 and +2.9points at the 7B and 2B model scales, respectively. Notably, models trainedwith B3 surpass existing state-of-the-art results even with a batch size assmall as 64, which is 4-16x smaller than that required by other methods.</description>
      <author>example@mail.com (Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Karthikeyan K, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra)</author>
      <guid isPermaLink="false">2505.11293v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Random Client Selection on Contrastive Federated Learning for Tabular Data</title>
      <link>http://arxiv.org/abs/2505.10759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对垂直联邦学习（VFL）中的梯度攻击进行了实验分析，并评估了随机客户端选择作为防御策略的有效性。&lt;h4&gt;背景&lt;/h4&gt;垂直联邦学习（VFL）通过保护隐私的方式在多方之间进行机器学习协作，但中间计算共享过程中存在信息泄露的风险。&lt;h4&gt;目的&lt;/h4&gt;评估随机客户端选择在对比联邦学习（CFL）环境中防御梯度攻击的有效性。&lt;h4&gt;方法&lt;/h4&gt;通过广泛的实验分析梯度攻击，并评估随机客户端选择作为防御策略。&lt;h4&gt;主要发现&lt;/h4&gt;随机客户端选择在CFL网络中对抗梯度攻击特别有效。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为在对比联邦学习系统中实施鲁棒的安全措施提供了有价值的见解，有助于开发更安全的协作学习框架。&lt;h4&gt;翻译&lt;/h4&gt;Vertical Federated Learning (VFL) has revolutionised collaborative machinelearning by enabling privacy-preserving model training across multiple parties. However, it remains vulnerable to information leakage during intermediate computation sharing. While Contrastive Federated Learning (CFL) was introduced to mitigate these privacy concerns through representation learning, it still faces challenges from gradient-based attacks. This paper presents a comprehensive experimental analysis of gradient-based attacks in CFL environments and evaluates random client selection as a defensive strategy. Through extensive experimentation, we demonstrate that random client selection proves particularly effective in defending against gradient attacks in the CFL network. Our findings provide valuable insights for implementing robust security measures in contrastive federated learning systems, contributing to the development of more secure collaborative learning frameworks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vertical Federated Learning (VFL) has revolutionised collaborative machinelearning by enabling privacy-preserving model training across multiple parties.However, it remains vulnerable to information leakage during intermediatecomputation sharing. While Contrastive Federated Learning (CFL) was introducedto mitigate these privacy concerns through representation learning, it stillfaces challenges from gradient-based attacks. This paper presents acomprehensive experimental analysis of gradient-based attacks in CFLenvironments and evaluates random client selection as a defensive strategy.Through extensive experimentation, we demonstrate that random client selectionproves particularly effective in defending against gradient attacks in the CFLnetwork. Our findings provide valuable insights for implementing robustsecurity measures in contrastive federated learning systems, contributing tothe development of more secure collaborative learning frameworks</description>
      <author>example@mail.com (Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua)</author>
      <guid isPermaLink="false">2505.10759v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis</title>
      <link>http://arxiv.org/abs/2505.10751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress, accepted in Novel Approaches for Precision  Agriculture and Forestry with Autonomous Robots, ICRA 2025 Workshop - May 23,  2025 - Atlanta, GA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种生成森林环境语义分割点云的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于获取成本高、传感器要求严格以及耗时，公开可用的点云数据集很少。此外，目前没有通过结构从运动（SfM）算法对图像进行标注的公开数据集，这可能是由于缺乏能够将语义分割信息映射到精确点云中的SfM算法，尤其是在森林等具有挑战性的环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成森林环境语义分割点云的方法。&lt;h4&gt;方法&lt;/h4&gt;使用定制的森林模拟器生成多样化的森林场景的RGB图像及其相应的语义分割掩码，然后使用修改后的开源SfM软件对这些标记图像进行处理，以在3D重建过程中保留语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;生成的点云提供了几何和语义细节，为训练和评估旨在分割通过SfM获得的真实森林点云的深度学习模型提供了宝贵资源。&lt;h4&gt;结论&lt;/h4&gt;该方法为森林环境点云的语义分割提供了新的解决方案，有助于推动相关深度学习模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although the use of remote sensing technologies for monitoring forestedenvironments has gained increasing attention, publicly available point clouddatasets remain scarce due to the high costs, sensor requirements, andtime-intensive nature of their acquisition. Moreover, as far as we are aware,there are no public annotated datasets generated through Structure From Motion(SfM) algorithms applied to imagery, which may be due to the lack of SfMalgorithms that can map semantic segmentation information into an accuratepoint cloud, especially in a challenging environment like forests.  In this work, we present a novel pipeline for generating semanticallysegmented point clouds of forest environments. Using a custom-built forestsimulator, we generate realistic RGB images of diverse forest scenes along withtheir corresponding semantic segmentation masks. These labeled images are thenprocessed using modified open-source SfM software capable of preservingsemantic information during 3D reconstruction. The resulting point cloudsprovide both geometric and semantic detail, offering a valuable resource fortraining and evaluating deep learning models aimed at segmenting real forestpoint clouds obtained via SfM.</description>
      <author>example@mail.com (Francisco Raverta Capua, Pablo De Cristoforis)</author>
      <guid isPermaLink="false">2505.10751v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction</title>
      <link>http://arxiv.org/abs/2505.11063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Thought-Aligner的动态思维校正模块，用于解决LLM-based自主代理在执行复杂多步任务时的安全对齐问题。&lt;h4&gt;背景&lt;/h4&gt;LLM-based自主代理具有推理、工具调用和环境交互的能力，但内部思维过程可能带来潜在风险。&lt;h4&gt;目的&lt;/h4&gt;为了解决长时程行为轨迹中的安全对齐挑战。&lt;h4&gt;方法&lt;/h4&gt;Thought-Aligner使用轻量级和资源高效的模型，在每次动作执行前即时纠正高风险的思维，并在不改变代理框架的前提下提高安全性和适用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Thought-Aligner将代理的行为安全性从大约50%提升到90%，同时保持响应延迟低于100ms。&lt;h4&gt;结论&lt;/h4&gt;Thought-Aligner为LLM-based代理提供了一种实用的动态安全解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a dynamic thought correction module named Thought-Aligner to address the safety alignment challenges in long-horizon behavioral trajectories of LLM-based autonomous agents.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based autonomous agents possess capabilities such as reasoning, toolinvocation, and environment interaction, enabling the execution of complexmulti-step tasks. The internal reasoning process, i.e., thought, of behavioraltrajectory significantly influences tool usage and subsequent actions but canintroduce potential risks. Even minor deviations in the agent's thought maytrigger cascading effects leading to irreversible safety incidents. To addressthe safety alignment challenges in long-horizon behavioral trajectories, wepropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizinga lightweight and resource-efficient model, Thought-Aligner corrects eachhigh-risk thought on the fly before each action execution. The correctedthought is then reintroduced to the agent, ensuring safer subsequent decisionsand tool interactions. Importantly, Thought-Aligner modifies only the reasoningphase without altering the underlying agent framework, making it easy to deployand widely applicable to various agent frameworks. To train the Thought-Alignermodel, we construct an instruction dataset across ten representative scenariosand simulate ReAct execution trajectories, generating 5,000 diverseinstructions and more than 11,400 safe and unsafe thought pairs. The model isfine-tuned using contrastive learning techniques. Experiments across threeagent safety benchmarks involving 12 different LLMs demonstrate thatThought-Aligner raises agent behavioral safety from approximately 50% in theunprotected setting to 90% on average. Additionally, Thought-Aligner maintainsresponse latency below 100ms with minimal resource usage, demonstrating itscapability for efficient deployment, broad applicability, and timelyresponsiveness. This method thus provides a practical dynamic safety solutionfor the LLM-based agents.</description>
      <author>example@mail.com (Changyue Jiang, Xudong Pan, Min Yang)</author>
      <guid isPermaLink="false">2505.11063v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</title>
      <link>http://arxiv.org/abs/2505.10696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review for IEEE conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为TartanGround的大型多模态数据集，旨在推进在多样化环境中运行的地面机器人的感知和自主能力。&lt;h4&gt;背景&lt;/h4&gt;当前数据集难以泛化到不同的场景，限制了机器人感知和自主技术的发展。&lt;h4&gt;目的&lt;/h4&gt;构建TartanGround数据集，用于训练和评估机器人感知和自主任务。&lt;h4&gt;方法&lt;/h4&gt;数据集在多个逼真的模拟环境中收集，包括RGB立体相机、深度、光流、立体视差、LiDAR点云、真实姿态、语义分割图像和占用图等。使用集成自动管道生成模拟地面机器人的运动轨迹，包括轮式和足式机器人，共收集910个轨迹和1.5百万个样本。&lt;h4&gt;主要发现&lt;/h4&gt;在占用预测和SLAM任务上的评估表明，基于现有数据集训练的方法难以泛化到不同的场景。&lt;h4&gt;结论&lt;/h4&gt;TartanGround可以作为训练和评估多种学习任务的测试平台，包括占用预测、SLAM、神经场景表示、基于感知的导航等，有助于推进机器人感知和自主技术的发展，使其模型更加通用和鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, and occupancy maps with semantic labels. Data is collected using an integrated automatic pipeline, which generates trajectories mimicking the motion patterns of various ground robot platforms, including wheeled and legged robots. We collect 910 trajectories across 70 environments, resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM tasks reveal that state-of-the-art methods trained on existing datasets struggle to generalize across diverse scenes. TartanGround can serve as a testbed for training and evaluation of a broad range of learning-based tasks, including occupancy prediction, SLAM, neural scene representation, perception-based navigation, and more, enabling advancements in robotic perception and autonomy towards achieving robust models generalizable to more diverse scenarios. The dataset and codebase for data collection will be made publicly available upon acceptance. Webpage: https://tartanair.org/tartanground&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TartanGround, a large-scale, multi-modal dataset to advance theperception and autonomy of ground robots operating in diverse environments.This dataset, collected in various photorealistic simulation environmentsincludes multiple RGB stereo cameras for 360-degree coverage, along with depth,optical flow, stereo disparity, LiDAR point clouds, ground truth poses,semantic segmented images, and occupancy maps with semantic labels. Data iscollected using an integrated automatic pipeline, which generates trajectoriesmimicking the motion patterns of various ground robot platforms, includingwheeled and legged robots. We collect 910 trajectories across 70 environments,resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAMtasks reveal that state-of-the-art methods trained on existing datasetsstruggle to generalize across diverse scenes. TartanGround can serve as atestbed for training and evaluation of a broad range of learning-based tasks,including occupancy prediction, SLAM, neural scene representation,perception-based navigation, and more, enabling advancements in roboticperception and autonomy towards achieving robust models generalizable to morediverse scenarios. The dataset and codebase for data collection will be madepublicly available upon acceptance. Webpage: https://tartanair.org/tartanground</description>
      <author>example@mail.com (Manthan Patel, Fan Yang, Yuheng Qiu, Cesar Cadena, Sebastian Scherer, Marco Hutter, Wenshan Wang)</author>
      <guid isPermaLink="false">2505.10696v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Visual Planning: Let's Think Only with Images</title>
      <link>http://arxiv.org/abs/2505.11409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables  including references and appendices)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型和多模态语言模型在机器推理方面的进步，并提出了一种新的视觉规划范式，旨在通过纯视觉表示进行规划，以增强机器在涉及空间和几何信息任务中的推理能力。&lt;h4&gt;背景&lt;/h4&gt;尽管LLMs和MLLMs在多种任务中提高了机器推理能力，但它们主要依赖纯文本作为表达和结构化推理的媒介，即使在存在视觉信息的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出视觉规划范式，通过纯视觉表示进行规划，独立于文本，特别是在涉及空间和几何信息任务中。&lt;h4&gt;方法&lt;/h4&gt;引入了视觉规划通过强化学习（VPRL）框架，利用GRPO（通用视觉模型后训练）对视觉导航任务进行训练，包括FrozenLake、Maze和MiniBehavior。&lt;h4&gt;主要发现&lt;/h4&gt;视觉规划范式在规划任务中优于所有仅基于文本的推理方法，证明了视觉规划是语言推理的一个可行且有前景的替代方案。&lt;h4&gt;结论&lt;/h4&gt;视觉规划为那些从直观的图像推理中受益的任务开辟了新的途径，并建立了视觉规划在机器推理中的地位。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Models (LLMs) and their multimodalextensions (MLLMs) have substantially enhanced machine reasoning across diversetasks. However, these models predominantly rely on pure text as the medium forboth expressing and structuring reasoning, even when visual information ispresent. In this work, we argue that language may not always be the mostnatural or effective modality for reasoning, particularly in tasks involvingspatial and geometrical information. Motivated by this, we propose a newparadigm, Visual Planning, which enables planning through purely visualrepresentations, independent of text. In this paradigm, planning is executedvia sequences of images that encode step-by-step inference in the visualdomain, akin to how humans sketch or visualize future actions. We introduce anovel reinforcement learning framework, Visual Planning via ReinforcementLearning (VPRL), empowered by GRPO for post-training large vision models,leading to substantial improvements in planning in a selection ofrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Ourvisual planning paradigm outperforms all other planning variants that conductreasoning in the text-only space. Our results establish Visual Planning as aviable and promising alternative to language-based reasoning, opening newavenues for tasks that benefit from intuitive, image-based inference.</description>
      <author>example@mail.com (Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić)</author>
      <guid isPermaLink="false">2505.11409v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework</title>
      <link>http://arxiv.org/abs/2505.11335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单高效的图神经网络校准方法，旨在解决GNN预测置信度常常被低估的问题。&lt;h4&gt;背景&lt;/h4&gt;GNN在图相关任务中表现出色，但其预测置信度往往校准不当，通常表现出过度自信不足，这影响了决策的可靠性。&lt;h4&gt;目的&lt;/h4&gt;为了解决GNN置信度校准不准确的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于模型和预测置信度之间内在关系的校准方法，并建立了统一的理论框架。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明，通过降低最终层参数的权重衰减可以减轻GNN的过度自信，而节点级校准作为更精细的补充，鼓励测试节点在最终层表示中更接近其预测的类别中心。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting under-confidence, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce additional calibration components, which fail to capture the intrinsic relationship between the model and the prediction confidence, resulting in limited theoretical guarantees and increased computational overhead. To address this issue, we propose a simple yet efficient graph calibration method. We establish a unified theoretical framework revealing that model confidence is jointly governed by class-centroid-level and node-level calibration at the final layer. Based on this insight, we theoretically show that reducing the weight decay of the final-layer parameters alleviates GNN under-confidence by acting on the class-centroid level, while node-level calibration acts as a finer-grained complement to class-centroid level calibration, which encourages each test node to be closer to its predicted class centroid at the final-layer representations. Extensive experiments validate the superiority of our method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness ongraph-based tasks. However, their predictive confidence is often miscalibrated,typically exhibiting under-confidence, which harms the reliability of theirdecisions. Existing calibration methods for GNNs normally introduce additionalcalibration components, which fail to capture the intrinsic relationshipbetween the model and the prediction confidence, resulting in limitedtheoretical guarantees and increased computational overhead. To address thisissue, we propose a simple yet efficient graph calibration method. We establisha unified theoretical framework revealing that model confidence is jointlygoverned by class-centroid-level and node-level calibration at the final layer.Based on this insight, we theoretically show that reducing the weight decay ofthe final-layer parameters alleviates GNN under-confidence by acting on theclass-centroid level, while node-level calibration acts as a finer-grainedcomplement to class-centroid level calibration, which encourages each test nodeto be closer to its predicted class centroid at the final-layerrepresentations. Extensive experiments validate the superiority of our method.</description>
      <author>example@mail.com (Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu)</author>
      <guid isPermaLink="false">2505.11335v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Learning Repetition-Invariant Representations for Polymer Informatics</title>
      <link>http://arxiv.org/abs/2505.10726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,3 figuares&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GRIN的新方法，用于学习聚合物表示，该方法可以处理不同重复单元数量的聚合物结构，并在同聚物和共聚物基准测试中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物在能源存储、建筑、医药和航空航天等领域广泛应用。然而，现有的图神经网络方法对于小分子有效，但无法对具有不同单元数量的聚合物结构产生一致的向量表示。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文提出了GRIN方法，旨在学习对重复单元数量不变的聚合物表示。&lt;h4&gt;方法&lt;/h4&gt;GRIN方法通过集成基于图的最大生成树对齐和重复单元增强，确保结构一致性。从模型和数据的角度提供了重复不变性的理论保证，并证明了三个重复单元是获得最佳不变表示所需的最小增强。&lt;h4&gt;主要发现&lt;/h4&gt;GRIN在homopolymer和copolymer基准测试中优于现有基线，学习到稳定且重复不变的表示，这些表示能够有效地推广到未见规模的聚合物链。&lt;h4&gt;结论&lt;/h4&gt;GRIN是一种有效的聚合物表示学习方法，能够处理不同重复单元数量的聚合物结构，并在实际应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：聚合物是由称为单体的重复结构单元组成的大分子，广泛应用于能源存储、建筑、医药和航空航天等领域。然而，尽管现有的图神经网络方法对小分子有效，但它们只能模拟聚合物的单个单元，并且无法为具有不同单元数量的真实聚合物结构产生一致的向量表示。为了应对这一挑战，我们引入了图重复不变性（GRIN），这是一种新的学习方法，用于学习对它们图表示中重复单元数量不变性的聚合物表示。GRIN通过集成基于图的最大生成树对齐和重复单元增强来确保结构一致性。我们从模型和数据的角度提供了重复不变性的理论保证，证明了三个重复单元是获得最佳不变表示所需的最小增强。GRIN在homopolymer和copolymer基准测试中都优于最先进的基线，学习到稳定、重复不变的表示，这些表示能够有效地推广到未见规模的聚合物链。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Polymers are large macromolecules composed of repeating structural unitsknown as monomers and are widely applied in fields such as energy storage,construction, medicine, and aerospace. However, existing graph neural networkmethods, though effective for small molecules, only model the single unit ofpolymers and fail to produce consistent vector representations for the truepolymer structure with varying numbers of units. To address this challenge, weintroduce Graph Repetition Invariance (GRIN), a novel method to learn polymerrepresentations that are invariant to the number of repeating units in theirgraph representations. GRIN integrates a graph-based maximum spanning treealignment with repeat-unit augmentation to ensure structural consistency. Weprovide theoretical guarantees for repetition-invariance from both model anddata perspectives, demonstrating that three repeating units are the minimalaugmentation required for optimal invariant representation learning. GRINoutperforms state-of-the-art baselines on both homopolymer and copolymerbenchmarks, learning stable, repetition-invariant representations thatgeneralize effectively to polymer chains of unseen sizes.</description>
      <author>example@mail.com (Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang)</author>
      <guid isPermaLink="false">2505.10726v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation</title>
      <link>http://arxiv.org/abs/2505.10810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 2 tables. Presented at the CVPR 2025 Human  Motion Generation (HuMoGen) Workshop. Introduces MoCLIP, a CLIP-based  fine-tuning strategy for motion generation, with results on HumanML3D dataset  and ablation studies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MoCLIP的模型，用于人类运动生成，该模型通过结合运动编码头和对比学习，提升了运动生成效果。&lt;h4&gt;背景&lt;/h4&gt;人类运动生成对于动画、机器人和虚拟现实等领域至关重要，需要模型能够从文本描述中有效捕捉运动动态。&lt;h4&gt;目的&lt;/h4&gt;提高基于文本描述的运动生成模型的运动逼真度和准确性。&lt;h4&gt;方法&lt;/h4&gt;MoCLIP是一个经过微调的CLIP模型，增加了运动编码头，并使用对比学习和锚定损失在运动序列上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;MoCLIP在保持与现有CLIP管道兼容的同时，提高了Top-1、Top-2和Top-3的准确性，同时保持了有竞争力的FID，从而改善了文本到运动的对齐结果。&lt;h4&gt;结论&lt;/h4&gt;MoCLIP是一种强大的框架，可以增强运动生成，其灵活性和有效性得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human motion generation is essential for fields such as animation, robotics,and virtual reality, requiring models that effectively capture motion dynamicsfrom text descriptions. Existing approaches often rely on ContrastiveLanguage-Image Pretraining (CLIP)-based text encoders, but their training ontext-image pairs constrains their ability to understand temporal and kinematicstructures inherent in motion and motion generation. This work introducesMoCLIP, a fine-tuned CLIP model with an additional motion encoding head,trained on motion sequences using contrastive learning and tethering loss. Byexplicitly incorporating motion-aware representations, MoCLIP enhances motionfidelity while remaining compatible with existing CLIP-based pipelines andseamlessly integrating into various CLIP-based methods. Experiments demonstratethat MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintainingcompetitive FID, leading to improved text-to-motion alignment results. Theseresults highlight MoCLIP's versatility and effectiveness, establishing it as arobust framework for enhancing motion generation.</description>
      <author>example@mail.com (Gabriel Maldonado, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi)</author>
      <guid isPermaLink="false">2505.10810v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2505.10601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，基于范围视图的激光雷达点云超分辨率技术作为一种低成本生成高分辨率点云数据的方法受到广泛关注。然而，由于激光雷达点云的稀疏和不规则结构，点云超分辨率问题仍然是一个具有挑战性的课题，尤其是在新型视图下的点云上采样方面。&lt;h4&gt;背景&lt;/h4&gt;激光雷达点云的稀疏性和不规则结构使得点云超分辨率问题具有挑战性，尤其是对于新型视图下的点云上采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SRMamba的新方法，用于在稀疏场景中对激光雷达点云进行超分辨率处理，解决从新型视图中恢复点云3D空间结构的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;SRMamba方法通过以下技术实现：基于Hough投票的投影技术消除范围图像中的水平线性空洞；通过视觉状态空间模型和多方向扫描机制提高长距离依赖关系的建立，并关注垂直三维空间中的潜在几何特征，以减少由于范围图像导致的3D空间结构信息损失；采用非对称U-Net网络以适应不同光束计数激光雷达的输入特征，实现多光束点云的超分辨率重建。&lt;h4&gt;主要发现&lt;/h4&gt;SRMamba在多个具有挑战性的公共激光雷达数据集（SemanticKITTI和nuScenes）上进行了实验，在定性和定量评估中均显示出比其他算法显著的优越性。&lt;h4&gt;结论&lt;/h4&gt;SRMamba在点云超分辨率任务中表现出色，特别是在处理稀疏场景和新型视图下的点云上采样方面。&lt;h4&gt;翻译&lt;/h4&gt;In recent years, range-view-based LiDAR point cloud super-resolution techniques attract significant attention as a low-cost method for generating higher-resolution point cloud data. However, due to the sparsity and irregular structure of LiDAR point clouds, the point cloud super-resolution problem remains a challenging topic, especially for point cloud upsampling under novel views. In this paper, we propose SRMamba, a novel method for super-resolution of LiDAR point clouds in sparse scenes, addressing the key challenge of recovering the 3D spatial structure of point clouds from novel views. Specifically, we implement projection technique based on Hough Voting and Hole Compensation strategy to eliminate horizontally linear holes in range image. To improve the establishment of long-distance dependencies and to focus on potential geometric features in vertical 3D space, we employ Visual State Space model and Multi-Directional Scanning mechanism to mitigate the loss of 3D spatial structural information due to the range image. Additionally, an asymmetric U-Net network adapts to the input characteristics of LiDARs with different beam counts, enabling super-resolution reconstruction for multi-beam point clouds. We conduct a series of experiments on multiple challenging public LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates significant superiority over other algorithms in both qualitative and quantitative evaluations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, range-view-based LiDAR point cloud super-resolutiontechniques attract significant attention as a low-cost method for generatinghigher-resolution point cloud data. However, due to the sparsity and irregularstructure of LiDAR point clouds, the point cloud super-resolution problemremains a challenging topic, especially for point cloud upsampling under novelviews. In this paper, we propose SRMamba, a novel method for super-resolutionof LiDAR point clouds in sparse scenes, addressing the key challenge ofrecovering the 3D spatial structure of point clouds from novel views.Specifically, we implement projection technique based on Hough Voting and HoleCompensation strategy to eliminate horizontally linear holes in range image. Toimprove the establishment of long-distance dependencies and to focus onpotential geometric features in vertical 3D space, we employ Visual State Spacemodel and Multi-Directional Scanning mechanism to mitigate the loss of 3Dspatial structural information due to the range image. Additionally, anasymmetric U-Net network adapts to the input characteristics of LiDARs withdifferent beam counts, enabling super-resolution reconstruction for multi-beampoint clouds. We conduct a series of experiments on multiple challenging publicLiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstratessignificant superiority over other algorithms in both qualitative andquantitative evaluations.</description>
      <author>example@mail.com (Chuang Chen, Wenyi Ge)</author>
      <guid isPermaLink="false">2505.10601v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning</title>
      <link>http://arxiv.org/abs/2505.11349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了科学机器学习中的时间序列基础模型在预测物理系统方面的能力，发现这些模型在零样本预测方面表现出色，但未能有效捕捉底层物理规律。&lt;h4&gt;背景&lt;/h4&gt;近年来，时间序列基础模型在科学机器学习中展现出预测物理系统的能力，包括零样本预测，即模型仅根据系统短轨迹预测未来状态。&lt;h4&gt;目的&lt;/h4&gt;研究时间序列基础模型在物理系统预测中的应用及其局限性。&lt;h4&gt;方法&lt;/h4&gt;对比分析了基础模型与直接上下文鹦鹉学舌模型在预测动态系统方面的性能，并探讨了上下文鹦鹉学舌与归纳头之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在预测物理系统时准确率较高，但未能有效捕捉底层物理规律；直接上下文鹦鹉学舌模型在预测动态系统方面表现优于最先进的时间序列基础模型，且计算成本更低；上下文鹦鹉学舌与归纳头之间存在联系，解释了为何大型语言模型可以用于时间序列预测；上下文长度与预测准确率之间的关系与吸引子的分形维度相关。&lt;h4&gt;结论&lt;/h4&gt;上下文鹦鹉学舌是简单但难以超越的时间序列基础模型基准，有助于识别超越鹦鹉学舌的上下文学习策略。&lt;h4&gt;翻译&lt;/h4&gt;摘要：最近开发的时间序列基础模型在科学机器学习中展现出预测物理系统的能力。这些能力包括零样本预测，即模型仅根据系统短轨迹预测未来状态。在这里，我们表明应用于物理系统的基础模型可以给出准确的预测，但它们无法发展出对底层物理的有意义表示。相反，基础模型通常通过上下文鹦鹉学舌进行预测，这是一种简单的零样本预测策略，直接从上下文中复制。因此，一个简单的直接上下文鹦鹉学舌模型在预测各种动态系统时得分高于最先进的时间序列基础模型，计算成本仅为后者的很小一部分。我们将在上下文鹦鹉学舌与归纳头之间建立联系，解释为什么在文本上训练的大型语言模型可以重新用于时间序列预测。我们的动态系统视角将预测准确率与上下文长度之间的缩放关系与吸引子的分形维度联系起来，为先前观察到的上下文神经缩放定律提供了见解。因此，上下文鹦鹉学舌作为简单但难以超越的时间序列基础模型基准，有助于识别超越鹦鹉学舌的上下文学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently-developed time series foundation models for scientific machinelearning exhibit emergent abilities to predict physical systems. Theseabilities include zero-shot forecasting, in which a model forecasts futurestates of a system given only a short trajectory as context. Here, we show thatfoundation models applied to physical systems can give accurate predictions,but that they fail to develop meaningful representations of the underlyingphysics. Instead, foundation models often forecast by context parroting, asimple zero-shot forecasting strategy that copies directly from the context. Asa result, a naive direct context parroting model scores higher thanstate-of-the-art time-series foundation models on predicting a diverse range ofdynamical systems, at a tiny fraction of the computational cost. We draw aparallel between context parroting and induction heads, which explains whylarge language models trained on text can be repurposed for time seriesforecasting. Our dynamical systems perspective also ties the scaling betweenforecast accuracy and context length to the fractal dimension of the attractor,providing insight into the previously observed in-context neural scaling laws.Context parroting thus serves as a simple but tough-to-beat baseline for futuretime-series foundation models and can help identify in-context learningstrategies beyond parroting.</description>
      <author>example@mail.com (Yuanzhao Zhang, William Gilpin)</author>
      <guid isPermaLink="false">2505.11349v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>FRET: Feature Redundancy Elimination for Test Time Adaptation</title>
      <link>http://arxiv.org/abs/2505.10641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FRET的测试时间自适应方法，旨在解决深度学习模型在测试数据分布偏移时的一般化问题，特别是在只有预训练模型和无标签测试数据的情况下。&lt;h4&gt;背景&lt;/h4&gt;测试时间自适应（TTA）旨在提高深度学习模型在测试数据与训练数据分布不一致时的泛化能力。这在需要保护隐私的应用中尤为重要。&lt;h4&gt;目的&lt;/h4&gt;旨在减少特征冗余，提高模型对新数据的适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为FRET的方法，包括一个直接最小化特征冗余分数的简单方法（S-FRET），以及一个结合图卷积网络（GCN）和对比学习的改进方法（G-FRET）。&lt;h4&gt;主要发现&lt;/h4&gt;S-FRET在处理标签偏移时存在局限性，而G-FRET通过减少特征冗余并增强特征可区分性，在多个模型架构、任务和数据集上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;G-FRET能够帮助模型在推理过程中提取非冗余且高度可区分的特征，从而促进更鲁棒的测试时间自适应。&lt;h4&gt;翻译&lt;/h4&gt;Test-Time Adaptation (TTA) aims to enhance the generalization of deep learning models when faced with test data that exhibits distribution shifts from the training data. In this context, only a pre-trained model and unlabeled test data are available, making it particularly relevant for privacy-sensitive applications. In practice, we observe that feature redundancy in embeddings tends to increase as domain shifts intensify in TTA. However, existing TTA methods often overlook this redundancy, which can hinder the model's adaptability to new data. To address this issue, we introduce Feature Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for TTA. A straightforward approach (S-FRET) is to directly minimize the feature redundancy score as an optimization objective to improve adaptation. Despite its simplicity and effectiveness, S-FRET struggles with label shifts, limiting its robustness in real-world scenarios. To mitigate this limitation, we further propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional Network (GCN) with contrastive learning. This design not only reduces feature redundancy but also enhances feature discriminability in both the representation and prediction layers. Extensive experiments across multiple model architectures, tasks, and datasets demonstrate the effectiveness of S-FRET and show that G-FRET achieves state-of-the-art performance. Further analysis reveals that G-FRET enables the model to extract non-redundant and highly discriminative features during inference, thereby facilitating more robust test-time adaptation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-Time Adaptation (TTA) aims to enhance the generalization of deeplearning models when faced with test data that exhibits distribution shiftsfrom the training data. In this context, only a pre-trained model and unlabeledtest data are available, making it particularly relevant for privacy-sensitiveapplications. In practice, we observe that feature redundancy in embeddingstends to increase as domain shifts intensify in TTA. However, existing TTAmethods often overlook this redundancy, which can hinder the model'sadaptability to new data. To address this issue, we introduce FeatureRedundancy Elimination for Test-time Adaptation (FRET), a novel perspective forTTA. A straightforward approach (S-FRET) is to directly minimize the featureredundancy score as an optimization objective to improve adaptation. Despiteits simplicity and effectiveness, S-FRET struggles with label shifts, limitingits robustness in real-world scenarios. To mitigate this limitation, we furtherpropose Graph-based FRET (G-FRET), which integrates a Graph ConvolutionalNetwork (GCN) with contrastive learning. This design not only reduces featureredundancy but also enhances feature discriminability in both therepresentation and prediction layers. Extensive experiments across multiplemodel architectures, tasks, and datasets demonstrate the effectiveness ofS-FRET and show that G-FRET achieves state-of-the-art performance. Furtheranalysis reveals that G-FRET enables the model to extract non-redundant andhighly discriminative features during inference, thereby facilitating morerobust test-time adaptation.</description>
      <author>example@mail.com (Linjing You, Jiabao Lu, Xiayuan Huang, Xiangli Nie)</author>
      <guid isPermaLink="false">2505.10641v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unfolded Deep Graph Learning for Networked Over-the-Air Computation</title>
      <link>http://arxiv.org/abs/2505.11248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted @ IEEE TWC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多集群网络环境下的无线信道空中计算（AirComp），旨在提高多集群加权求和的AirComp速率，同时解决接收端波束成形、发射缩放和干扰管理等问题。&lt;h4&gt;背景&lt;/h4&gt;空中计算技术允许通过无线信道同时进行传输和计算，但多集群网络环境下的AirComp受到接收端波束成形、发射缩放和干扰管理等方面的挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在最大化多集群加权求和AirComp速率，并解决传输缩放和接收波束成形中的干扰问题。&lt;h4&gt;方法&lt;/h4&gt;通过分解问题，采用交替优化技术和迭代过程近似求解。利用算法展开原理，通过信道条件和网络中的相互干扰构建一个基础图，然后利用图神经网络学习权重参数，并通过随机梯度下降法进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，提出的方案优于传统方案，展开的图学习架构有效地减轻了干扰，并实现了优越的计算性能，对动态和可扩展网络具有较强的适应性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高多集群AirComp网络的计算性能，并对动态和可扩展网络具有强的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-the-air computation (AirComp) has emerged as a promising technology thatenables simultaneous transmission and computation through wireless channels. Inthis paper, we investigate the networked AirComp in multiple clusters allowingdiversified data computation, which is yet challenged by the transceivercoordination and interference management therein. Particularly, we aim tomaximize the multi-cluster weighted-sum AirComp rate, where the transmissionscalar as well as receive beamforming are jointly investigated while addressingthe interference issue. From an optimization perspective, we decompose theformulated problem and adopt the alternating optimization technique with aniterative process to approximate the solution. Then, we reinterpret theiterations through the principle of algorithm unfolding, where the channelcondition and mutual interference in the AirComp network constitute anunderlying graph. Accordingly, the proposed unfolding architecture learns theweights parameterized by graph neural networks, which is trained throughstochastic gradient descent approach. Simulation results show that ourproposals outperform the conventional schemes, and the proposed unfolded graphlearning substantially alleviates the interference and achieves superiorcomputation performance, with strong and efficient adaptation to the dynamicand scalable networks.</description>
      <author>example@mail.com (Xiao Tang, Huirong Xiao, Chao Shen, Li Sun, Qinghe Du, Dusit Niyato, Zhu Han)</author>
      <guid isPermaLink="false">2505.11248v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment</title>
      <link>http://arxiv.org/abs/2505.11230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的交通分配问题解决方案，使用消息传递神经网络作为元模型来近似随机用户均衡分配的均衡流量。&lt;h4&gt;背景&lt;/h4&gt;交通分配问题是交通建模中的基本任务，但在大规模网络中计算成本高昂，传统方法需要迭代模拟以达到均衡，这使得实时或大规模场景分析变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来加速非分布场景的评估，降低大规模交通规划的计算成本，并实现实时决策。&lt;h4&gt;方法&lt;/h4&gt;使用消息传递神经网络作为元模型来模拟传统交通模拟器中的算法结构，以更好地捕捉潜在过程，而不是仅仅数据。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与其他传统深度学习技术进行了基准测试，并通过在训练数据域之外的输入数据上测试其预测交通流的能力来评估模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法为加速非分布场景的评估、减少大规模交通规划的计算成本和实现实时决策提供了有希望的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：交通分配问题是交通建模中的基础任务，但在大规模网络中计算成本很高，特别是对于大规模网络。传统方法需要迭代模拟以达到均衡，这使得实时或大规模场景分析变得具有挑战性。在本文中，我们提出了一种基于学习的解决方案，使用消息传递神经网络作为元模型来近似随机用户均衡分配的均衡流量。我们的模型旨在模拟传统交通模拟器中使用的算法结构，以便更好地捕捉潜在过程，而不仅仅是数据。我们将其与其他传统深度学习技术进行了基准测试，并通过在训练数据域之外的输入数据上测试其预测交通流的能力来评估模型的鲁棒性。这种方法为加速非分布场景的评估、减少大规模交通规划的计算成本和实现实时决策提供了有希望的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Traffic Assignment Problem is a fundamental, yet computationallyexpensive, task in transportation modeling, especially for large-scalenetworks. Traditional methods require iterative simulations to reachequilibrium, making real-time or large-scale scenario analysis challenging. Inthis paper, we propose a learning-based approach using Message-Passing NeuralNetworks as a metamodel to approximate the equilibrium flow of the StochasticUser Equilibrium assignment. Our model is designed to mimic the algorithmicstructure used in conventional traffic simulators allowing it to better capturethe underlying process rather than just the data. We benchmark it against otherconventional deep learning techniques and evaluate the model's robustness bytesting its ability to predict traffic flows on input data outside the domainon which it was trained. This approach offers a promising solution foraccelerating out-of-distribution scenario assessments, reducing computationalcosts in large-scale transportation planning, and enabling real-timedecision-making.</description>
      <author>example@mail.com (Oskar Bohn Lassen, Serio Agriesti, Mohamed Eldafrawi, Daniele Gammelli, Guido Cantelmo, Guido Gentile, Francisco Camara Pereira)</author>
      <guid isPermaLink="false">2505.11230v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
      <link>http://arxiv.org/abs/2505.11325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PFNs作为一种从表格数据集进行预测的强大基础模型，在小到中等规模的数据集上实现了最先进的性能，无需调整。本文提出了一种基于Martingale后验的贝叶斯后验构建方法，用于估计预测结果的不确定性，并通过模拟和真实世界数据展示了该方法在推理应用中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;PFNs（Prior-data fitted networks）作为一种从表格数据集进行预测的模型，在无需调整的情况下，在小到中等规模的数据集上取得了最先进的性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Martingale后验的贝叶斯后验构建方法，用于估计预测结果的不确定性。&lt;h4&gt;方法&lt;/h4&gt;采用Martingale后验来构建贝叶斯后验，并证明其收敛性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟和真实世界数据中展示了不确定性量化在推理应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为PFNs提供了不确定性量化，有助于提高预测的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prior-data fitted networks (PFNs) have emerged as promising foundation modelsfor prediction from tabular data sets, achieving state-of-the-art performanceon small to moderate data sizes without tuning. While PFNs are motivated byBayesian ideas, they do not provide any uncertainty quantification forpredictive means, quantiles, or similar quantities. We propose a principled andefficient sampling procedure to construct Bayesian posteriors for suchestimates based on Martingale posteriors, and prove its convergence. Severalsimulated and real-world data examples showcase the uncertainty quantificationof our method in inference applications.</description>
      <author>example@mail.com (Thomas Nagler, David Rügamer)</author>
      <guid isPermaLink="false">2505.11325v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes</title>
      <link>http://arxiv.org/abs/2505.11270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态数据分析系统，旨在解决数据湖中数据多样性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;数据湖中的数据种类繁多，包括结构化、半结构化和非结构化数据，对数据分析提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提高多模态数据分析的准确性、效率和知识的新鲜度。&lt;h4&gt;方法&lt;/h4&gt;本文提出了基于模型上下文协议（MCP）的新型架构，允许大型语言模型（LLMs）与知识丰富的代理协同工作。&lt;h4&gt;主要发现&lt;/h4&gt;1. 现有的自然语言或SQL-like查询语言可能难以精确和全面地捕捉用户的分析意图；2. 依赖单个统一的LLM处理多样化的数据模式会导致大量的推理开销；3. 数据湖中的数据可能不完整或过时，需要整合外部开放域知识以生成及时和相关的分析结果。&lt;h4&gt;结论&lt;/h4&gt;该系统通过语义操作符层次结构和AI代理驱动的NL2Operator翻译器来连接用户意图和分析执行，同时通过MCP执行框架提高准确性和效率，并支持模块化部署以实现高可扩展性。此外，通过深度研究和机器再学习技术来更新数据湖和LLM知识，以平衡数据的新鲜度和推理效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：数据湖中数据的多样性给数据分析带来了重大挑战，数据科学家必须同时分析多模态数据，包括结构化、半结构化和非结构化数据。尽管大型语言模型（LLMs）已显示出有希望的潜力，但在准确性、效率和新鲜度方面，它们仍不足以处理多模态数据分析。首先，当前的自然语言（NL）或SQL-like查询语言可能难以精确和全面地捕捉用户的分析意图。其次，依赖于单个统一的LLM来处理多样化的数据模式通常会导致大量的推理开销。第三，存储在数据湖中的数据可能不完整或过时，因此集成外部开放域知识对于生成及时和相关的分析结果是必不可少的。在本文中，我们设想了一种新的多模态数据分析系统。具体而言，我们提出了一种基于模型上下文协议（MCP）的新型架构，该协议允许LLMs与知识丰富的代理协同工作。首先，我们定义了一个针对数据湖中多模态数据查询的语义操作符层次结构，并开发了一个由AI代理驱动的NL2Operator翻译器，以连接用户意图和分析执行。接下来，我们介绍了一个基于MCP的执行框架，其中每个MCP服务器都运行针对特定数据模式优化的专用基础模型。这种设计提高了准确性和效率，并通过模块化部署支持高可扩展性。最后，我们提出了一种更新机制，通过利用深度研究和机器再学习技术来刷新数据湖和LLM知识，目标是平衡数据的新鲜度和推理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The variety of data in data lakes presents significant challenges for dataanalytics, as data scientists must simultaneously analyze multi-modal data,including structured, semi-structured, and unstructured data. While LargeLanguage Models (LLMs) have demonstrated promising capabilities, they stillremain inadequate for multi-modal data analytics in terms of accuracy,efficiency, and freshness. First, current natural language (NL) or SQL-likequery languages may struggle to precisely and comprehensively capture users'analytical intent. Second, relying on a single unified LLM to process diversedata modalities often leads to substantial inference overhead. Third, datastored in data lakes may be incomplete or outdated, making it essential tointegrate external open-domain knowledge to generate timely and relevantanalytics results.  In this paper, we envision a new multi-modal data analytics system.Specifically, we propose a novel architecture built upon the Model ContextProtocol (MCP), an emerging paradigm that enables LLMs to collaborate withknowledgeable agents. First, we define a semantic operator hierarchy tailoredfor querying multi-modal data in data lakes and develop an AI-agent-poweredNL2Operator translator to bridge user intent and analytical execution. Next, weintroduce an MCP-based execution framework, in which each MCP server hostsspecialized foundation models optimized for specific data modalities. Thisdesign enhances both accuracy and efficiency, while supporting high scalabilitythrough modular deployment. Finally, we propose a updating mechanism byharnessing the deep research and machine unlearning techniques to refresh thedata lakes and LLM knowledges, with the goal of balancing the data freshnessand inference efficiency.</description>
      <author>example@mail.com (Chao Zhang, Shaolei Zhang, Quehuan Liu, Sibei Chen, Tong Li, Ju Fan)</author>
      <guid isPermaLink="false">2505.11270v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>IssueCourier: Multi-Relational Heterogeneous Temporal Graph Neural Network for Open-Source Issue Assignment</title>
      <link>http://arxiv.org/abs/2505.11205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IssueCourier的基于多关系异构时序图神经网络的自动化问题分配方法，用于开源软件维护中的问题分配。&lt;h4&gt;背景&lt;/h4&gt;在开源软件维护中，问题分配是一个关键环节，需要推荐最合适的开发者来处理报告的问题。由于大规模项目中问题报告数量庞大，手动分配问题既繁琐又昂贵。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有自动化问题分配方法在性能上的局限性，如数据集中标签错误和缺失、开发者贡献的长尾效应以及开发者活动随项目进展而变化的问题。&lt;h4&gt;方法&lt;/h4&gt;提出IssueCourier方法，通过正式化问题、开发者与源代码文件之间的五个关键关系来构建异构图，并采用时间切片技术将图划分为基于时间的一系列子图以学习特定阶段的模式。此外，还提供了一个带有重新标注的基准数据集以解决现有开源数据集中标签错误和缺失的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上进行的广泛实验表明，IssueCourier在top-1和MRR方面分别比最佳基线提高了45.49%和31.97%。&lt;h4&gt;结论&lt;/h4&gt;IssueCourier能够有效提高问题分配的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在开源软件（OSS）维护中，问题分配起着至关重要的作用，这涉及到推荐最合适的开发者来处理报告的问题。鉴于大规模项目中问题报告的高数量，手动分配问题是繁琐且昂贵的。先前的研究已经提出了自动问题分配方法，这些方法主要基于对问题报告文本信息、开发者专业知识或基于历史问题修复记录的问题与开发者之间交互的建模。然而，这些方法往往由于开源数据集中存在错误和缺失的标签，以及开发者贡献的长尾效应和开发者活动随项目进展而变化的问题，而受到性能限制。为了解决这些挑战，我们提出了IssueCourier，这是一种用于问题分配的新颖的多关系异构时序图神经网络方法。具体来说，我们正式化了问题、开发者和源代码文件之间的五个关键关系，以构建一个异构图。然后，我们进一步采用时间切片技术，将图划分为一系列基于时间的基础子图以学习特定阶段的模式。此外，我们还提供了一个带有重新标注的基准数据集，以解决现有开源数据集中标签错误和缺失的问题。最后，为了评估IssueCourier的性能，我们在基准数据集上进行了广泛的实验。结果表明，IssueCourier可以在top-1和MRR方面分别比最佳基线提高45.49%和31.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Issue assignment plays a critical role in open-source software (OSS)maintenance, which involves recommending the most suitable developers toaddress the reported issues. Given the high volume of issue reports inlarge-scale projects, manually assigning issues is tedious and costly. Previousstudies have proposed automated issue assignment approaches that primarilyfocus on modeling issue report textual information, developers' expertise, orinteractions between issues and developers based on historical issue-fixingrecords. However, these approaches often suffer from performance limitationsdue to the presence of incorrect and missing labels in OSS datasets, as well asthe long tail of developer contributions and the changes of developer activityas the project evolves. To address these challenges, we propose IssueCourier, anovel Multi-Relational Heterogeneous Temporal Graph Neural Network approach forissue assignment. Specifically, we formalize five key relationships amongissues, developers, and source code files to construct a heterogeneous graph.Then, we further adopt a temporal slicing technique that partitions the graphinto a sequence of time-based subgraphs to learn stage-specific patterns.Furthermore, we provide a benchmark dataset with relabeled ground truth toaddress the problem of incorrect and missing labels in existing OSS datasets.Finally, to evaluate the performance of IssueCourier, we conduct extensiveexperiments on our benchmark dataset. The results show that IssueCourier canimprove over the best baseline up to 45.49% in top-1 and 31.97% in MRR.</description>
      <author>example@mail.com (Chunying Zhou, Xiaoyuan Xie, Gong Chen, Peng He, Bing Li)</author>
      <guid isPermaLink="false">2505.11205v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Covariance Density Neural Networks</title>
      <link>http://arxiv.org/abs/2505.11139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的CoVariance Neural Networks（VNNs）模型，通过使用密度矩阵作为图位移算子（GSO），提高了网络在处理网络数据时的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络在建模和预测网络数据方面取得了进展，但选择合适的底层图结构存在争议。&lt;h4&gt;目的&lt;/h4&gt;解决选择合适的底层图结构的问题，并提高VNNs的性能。&lt;h4&gt;方法&lt;/h4&gt;将样本协方差矩阵视为随机变量空间中的准哈密顿量，构建密度矩阵作为GSO，从而在不同的尺度上提取数据成分，增强判别能力和性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够显式控制网络的稳定性-判别性权衡，相比VNNs具有更强的鲁棒性，并在实际应用中优于VNNs。特别地，该模型在脑机接口（BCI）的脑电图（EEG）运动想象分类任务中表现出色，速度快于EEGnet。&lt;h4&gt;结论&lt;/h4&gt;协方差密度神经网络为BCI的迁移性提供了基础，当在未见过的个体上评估时，能够实现良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph神经网络重新定义了我们对网络数据的建模和预测方法，但在选择合适的底层图结构以建模信号方面，缺乏共识。协方差神经网络（VNN）通过使用样本协方差矩阵作为图位移算子（GSO）来解决这个问题。在这里，我们通过构建密度矩阵来提高VNNs的性能，在这个密度矩阵中，我们将样本协方差矩阵视为系统在随机变量空间中的准哈密顿量。关键的是，使用这个密度矩阵作为GSO允许在不同的尺度上提取数据成分，从而增强了判别能力和性能。我们表明，这种方法允许显式控制网络的稳定性-判别性权衡，与VNNs相比，提供了更强的鲁棒性，并在底层协方差矩阵具有信息性的有用实际应用中表现优于它们。特别是，我们表明我们的模型在主题无关的脑电图（EEG）脑机接口（BCI）运动想象分类中可以达到很好的性能，速度比EEGnet快。这表明协方差密度神经网络为在未见过的个体上评估时的BCI迁移性这一艰巨任务提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have re-defined how we model and predict on networkdata but there lacks a consensus on choosing the correct underlying graphstructure on which to model signals. CoVariance Neural Networks (VNN) addressthis issue by using the sample covariance matrix as a Graph Shift Operator(GSO). Here, we improve on the performance of VNNs by constructing a DensityMatrix where we consider the sample Covariance matrix as a quasi-Hamiltonian ofthe system in the space of random variables. Crucially, using this densitymatrix as the GSO allows components of the data to be extracted at differentscales, allowing enhanced discriminability and performance. We show that thisapproach allows explicit control of the stability-discriminability trade-off ofthe network, provides enhanced robustness to noise compared to VNNs, andoutperforms them in useful real-life applications where the underlyingcovariance matrix is informative. In particular, we show that our model canachieve strong performance in subject-independent Brain Computer Interface EEGmotor imagery classification, outperforming EEGnet while being faster. Thisshows how covariance density neural networks provide a basis for thenotoriously difficult task of transferability of BCIs when evaluated on unseenindividuals.</description>
      <author>example@mail.com (Om Roy, Yashar Moshfeghi, Keith Smith)</author>
      <guid isPermaLink="false">2505.11139v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation</title>
      <link>http://arxiv.org/abs/2505.11221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, ICASSP 2025. The first two authors are equally contributed&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LVLM2P的新框架，用于将大型视觉-语言模型（LVLM）的知识提炼到高效的强化学习（RL）代理中，以解决决策挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型在处理复杂决策挑战方面具有潜力，但它们的大参数使得实际部署资源密集，对于受限系统来说往往不切实际。强化学习虽然对特定任务代理有希望，但样本复杂度高，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出LVLM2P框架，旨在解决上述挑战，提高强化学习代理的效率。&lt;h4&gt;方法&lt;/h4&gt;LVLM2P利用LVLM作为教师，根据RL代理收集的轨迹提供指导性动作，帮助减少学习早期的不必要探索，从而显著加速代理的学习进度。此外，通过利用LVLM直接从视觉观察中建议动作，消除了对环境手动文本描述的需要，增强了跨不同任务的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LVLM2P显著提高了基线RL算法的样本效率。&lt;h4&gt;结论&lt;/h4&gt;LVLM2P框架能够有效提高强化学习代理的样本效率，为解决复杂决策挑战提供了一种新的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICASSP49660.2025.10888998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research highlights the potential of multimodal foundation models intackling complex decision-making challenges. However, their large parametersmake real-world deployment resource-intensive and often impractical forconstrained systems. Reinforcement learning (RL) shows promise fortask-specific agents but suffers from high sample complexity, limitingpractical applications. To address these challenges, we introduce LVLM toPolicy (LVLM2P), a novel framework that distills knowledge from largevision-language models (LVLM) into more efficient RL agents. Our approachleverages the LVLM as a teacher, providing instructional actions based ontrajectories collected by the RL agent, which helps reduce less meaningfulexploration in the early stages of learning, thereby significantly acceleratingthe agent's learning progress. Additionally, by leveraging the LVLM to suggestactions directly from visual observations, we eliminate the need for manualtextual descriptors of the environment, enhancing applicability across diversetasks. Experiments show that LVLM2P significantly enhances the sampleefficiency of baseline RL algorithms.</description>
      <author>example@mail.com (Donghoon Lee, Tung M. Luu, Younghwan Lee, Chang D. Yoo)</author>
      <guid isPermaLink="false">2505.11221v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Halting Recurrent GNNs and the Graded $μ$-Calculus</title>
      <link>http://arxiv.org/abs/2505.11050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于循环图神经网络（GNNs）的停止机制，并证明了该机制能够表达所有在分级模态μ-演算中定义的节点分类器，即使对于不考虑图大小的标准GNN变种也是如此。&lt;h4&gt;背景&lt;/h4&gt;现有的循环GNNs要么假设图大小已知，要么缺乏终止保证。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够确保循环GNNs正确终止的机制。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新的近似语义，用于分级μ-演算，并利用这种语义设计了一种新的模型检查算法，称为计数算法。&lt;h4&gt;主要发现&lt;/h4&gt;计数算法不依赖于图大小，并且能够被实现在一个停止的循环GNN上。同时，证明了循环GNNs在有限范围内只能表达在分级模态μ-演算中定义的节点分类器。&lt;h4&gt;结论&lt;/h4&gt;提出的停止机制和模型检查算法为循环GNNs在处理图结构数据时提供了有效的工具，并证明了其在表达能力和终止性方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are a class of machine-learning models thatoperate on graph-structured data. Their expressive power is intimately relatedto logics that are invariant under graded bisimilarity. Current proposals forrecurrent GNNs either assume that the graph size is given to the model, orsuffer from a lack of termination guarantees. In this paper, we propose ahalting mechanism for recurrent GNNs. We prove that our halting model canexpress all node classifiers definable in graded modal mu-calculus, even forthe standard GNN variant that is oblivious to the graph size. A recentbreakthrough in the study of the expressivity of graded modal mu-calculus inthe finite suggests that conversely, restricted to node classifiers definablein monadic second-order logic, recurrent GNNs can express only node classifiersdefinable in graded modal mu-calculus. To prove our main result, we develop anew approximate semantics for graded mu-calculus, which we believe to be ofindependent interest. We leverage this new semantics into a new model-checkingalgorithm, called the counting algorithm, which is oblivious to the graph size.In a final step we show that the counting algorithm can be implemented on ahalting recurrent GNN.</description>
      <author>example@mail.com (Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema)</author>
      <guid isPermaLink="false">2505.11050v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs</title>
      <link>http://arxiv.org/abs/2505.11023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究背景知识图在生物医学研究等复杂低数据领域中的重要作用，特别是在癌症亚型分类任务中的应用。&lt;h4&gt;背景&lt;/h4&gt;在生物医学研究中，将背景知识图（如蛋白质-蛋白质相互作用网络）纳入图学习模型可以提高模型性能，但背景知识的实际贡献和对不完美知识的影响尚不明确。&lt;h4&gt;目的&lt;/h4&gt;研究背景知识在癌症亚型分类任务中的作用，并评估其性能贡献。&lt;h4&gt;方法&lt;/h4&gt;通过构建一个评估框架，包括合成设置和一系列模拟背景知识图不完美性的扰动，来测试背景知识感知模型在合成和真实生物医学环境中的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;发现最先进的图神经网络（GNN）使用背景知识并不比无信息的模型（如线性回归）表现更好，且即使背景知识图受到严重扰动，其性能也基本不变。&lt;h4&gt;结论&lt;/h4&gt;强调在GNN架构和背景知识特征之间进行仔细对齐的必要性，并指出这有可能带来显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In complex and low-data domains such as biomedical research, incorporatingbackground knowledge (BK) graphs, such as protein-protein interaction (PPI)networks, into graph-based machine learning pipelines is a promising researchdirection. However, while BK is often assumed to improve model performance, itsactual contribution and the impact of imperfect knowledge remain poorlyunderstood. In this work, we investigate the role of BK in an importantreal-world task: cancer subtype classification. Surprisingly, we find that (i)state-of-the-art GNNs using BK perform no better than uninformed models likelinear regression, and (ii) their performance remains largely unchanged evenwhen the BK graph is heavily perturbed. To understand these unexpected results,we introduce an evaluation framework, which employs (i) a synthetic settingwhere the BK is clearly informative and (ii) a set of perturbations thatsimulate various imperfections in BK graphs. With this, we test the robustnessof BK-aware models in both synthetic and real-world biomedical settings. Ourfindings reveal that careful alignment of GNN architectures and BKcharacteristics is necessary but holds the potential for significantperformance improvements.</description>
      <author>example@mail.com (Kutalmış Coşkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan Lüdtke, Martin Becker)</author>
      <guid isPermaLink="false">2505.11023v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Relational Graph Transformer</title>
      <link>http://arxiv.org/abs/2505.10960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/snap-stanford/relgt&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Relational Graph Transformer (RelGT)的新架构，用于构建基于多表关系数据的预测模型，并展示了其在21个RelBench基准任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;关系深度学习（RDL）是一种在多表关系数据上构建最先进的预测模型的方法，它通过将数据表示为异构时间图来实现。然而，常用的图神经网络模型在捕捉关系数据中的复杂结构模式和长距离依赖关系方面存在根本性限制。&lt;h4&gt;目的&lt;/h4&gt;设计一种专门针对关系表的新型图变换器架构，以解决现有图神经网络模型在处理关系数据时的局限性。&lt;h4&gt;方法&lt;/h4&gt;RelGT采用了一种新颖的多元素标记化策略，将每个节点分解为五个组件（特征、类型、跳转距离、时间和局部结构），从而高效地编码异构性、时态和拓扑结构，而不需要昂贵的预计算。该架构结合了局部注意力机制和全局注意力机制，以学习可学习的中心点，同时结合局部和数据库范围内的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在21个RelBench基准任务中，RelGT在性能上与图神经网络基线相当或优于基线，最高提升达18%，证明了图变换器在关系深度学习中的强大架构能力。&lt;h4&gt;结论&lt;/h4&gt;RelGT是一种有效的解决关系数据复杂性和时态约束的方法，为关系深度学习提供了强大的架构支持。&lt;h4&gt;翻译&lt;/h4&gt;Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational Deep Learning (RDL) is a promising approach for buildingstate-of-the-art predictive models on multi-table relational data byrepresenting it as a heterogeneous temporal graph. However, commonly used GraphNeural Network models suffer from fundamental limitations in capturing complexstructural patterns and long-range dependencies that are inherent in relationaldata. While Graph Transformers have emerged as powerful alternatives to GNNs ongeneral graphs, applying them to relational entity graphs presents uniquechallenges: (i) Traditional positional encodings fail to generalize to massive,heterogeneous graphs; (ii) existing architectures cannot model the temporaldynamics and schema constraints of relational data; (iii) existing tokenizationschemes lose critical structural information. Here we introduce the RelationalGraph Transformer (RelGT), the first graph transformer architecture designedspecifically for relational tables. RelGT employs a novel multi-elementtokenization strategy that decomposes each node into five components (features,type, hop distance, time, and local structure), enabling efficient encoding ofheterogeneity, temporality, and topology without expensive precomputation. Ourarchitecture combines local attention over sampled subgraphs with globalattention to learnable centroids, incorporating both local and database-widerepresentations. Across 21 tasks from the RelBench benchmark, RelGTconsistently matches or outperforms GNN baselines by up to 18%, establishingGraph Transformers as a powerful architecture for Relational Deep Learning.</description>
      <author>example@mail.com (Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico López, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec)</author>
      <guid isPermaLink="false">2505.10960v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model Enhancers for Graph Neural Networks: An Analysis from the Perspective of Causal Mechanism Identification</title>
      <link>http://arxiv.org/abs/2505.08265v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLMs）作为特征增强器优化节点表示，并将这些表示作为图神经网络（GNNs）输入的方法，在图表示学习中具有显著潜力，并对这一方法进行了深入研究。&lt;h4&gt;背景&lt;/h4&gt;LLMs作为特征增强器在图表示学习中的应用显示出潜力，但其基本属性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;通过更深入的分析和基于互换干预方法的实验，研究LLMs增强器和GNNs的深层属性和内在机制。&lt;h4&gt;方法&lt;/h4&gt;构建了一个具有可控因果关系的合成图数据集，用于分析，并使用互换干预方法来检查LLMs增强器和GNNs的深层属性。基于分析结果，设计了一个即插即用模块来提高LLMs增强器和GNNs之间的信息传递。&lt;h4&gt;主要发现&lt;/h4&gt;通过互换干预实验，揭示了LLMs增强器和GNNs的内在逻辑和内部机制。&lt;h4&gt;结论&lt;/h4&gt;提出的模块在多个数据集和模型上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：使用大型语言模型（LLMs）作为特征增强器以优化节点表示，这些表示随后被用作图神经网络（GNNs）的输入，在图表示学习中显示出显著潜力。然而，这种方法的基本属性尚未得到充分探索。为了解决这个问题，我们提出基于互换干预方法对此问题进行更深入的分析。首先，我们构建了一个具有可控因果关系的合成图数据集，以便精确地操作语义关系和因果建模，为分析提供数据。使用这个数据集，我们进行互换干预来检查LLMs增强器和GNNs的深层属性，揭示它们的内在逻辑和内部机制。基于分析结果，我们设计了一个即插即用模块来提高LLMs增强器和GNNs之间的信息传递。在多个数据集和模型上的实验验证了所提出的模块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) as feature enhancers to optimize noderepresentations, which are then used as inputs for graph neural networks(GNNs), has shown significant potential in graph representation learning.However, the fundamental properties of this approach remain underexplored. Toaddress this issue, we propose conducting a more in-depth analysis of thisissue based on the interchange intervention method. First, we construct asynthetic graph dataset with controllable causal relationships, enablingprecise manipulation of semantic relationships and causal modeling to providedata for analysis. Using this dataset, we conduct interchange interventions toexamine the deeper properties of LLM enhancers and GNNs, uncovering theirunderlying logic and internal mechanisms. Building on the analytical results,we design a plug-and-play optimization module to improve the informationtransfer between LLM enhancers and GNNs. Experiments across multiple datasetsand models validate the proposed module.</description>
      <author>example@mail.com (Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.08265v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</title>
      <link>http://arxiv.org/abs/2505.11214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLA模型在机器人领域日益受到关注，通过视觉语言基础模型直接从视觉观察和人类指令生成机器人动作。本文提出OE-VLA模型，以探索VLA模型在开放式多模态指令中的潜力。&lt;h4&gt;背景&lt;/h4&gt;VLA模型基于大规模互联网数据训练的视觉语言基础模型，能有效生成机器人动作，但通常只接受语言指令，限制了其在开放式人机交互中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出OE-VLA模型，旨在解决VLA模型在处理开放式多模态指令时的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入OE-VLA模型，该模型能够处理图像、白板上的指令和视频中的行为等不同形式的指令。&lt;h4&gt;主要发现&lt;/h4&gt;OE-VLA模型在语言输入的VLA模型性能可比的基础上，在四个额外的开放式任务类别中取得了令人印象深刻的结果。&lt;h4&gt;结论&lt;/h4&gt;OE-VLA模型能够显著扩展VLA模型在日常场景中的应用，并促进人机交互。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have recently become highly prominent inthe field of robotics. Leveraging vision-language foundation models trained onlarge-scale internet data, the VLA model can generate robotic actions directlyfrom visual observations and human instructions through a single end-to-endneural network. Despite their effectiveness, current VLA models usually acceptonly one form of human prompting, language instructions, which may constraintheir applicability in open-ended human-robot interactions. For example, a usermight expect the robot to retrieve an object shown in an image, follow aninstruction written on the whiteboard, or imitate a behavior demonstrated in avideo, rather than relying solely on language-based descriptions. To addressthis gap, we introduce OE-VLA, which explores the potential of VLA models foropen-ended multimodal instructions. Extensive results demonstrate that ourOE-VLA not only achieves comparable performance to traditional VLA models withlinguistic input but also delivers impressive results across four additionalcategories of open-ended tasks. The proposed methodology could significantlyexpand the applications of VLA models across various everyday scenarios andfacilitate human-robot interaction.</description>
      <author>example@mail.com (Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang)</author>
      <guid isPermaLink="false">2505.11214v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations</title>
      <link>http://arxiv.org/abs/2505.10877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高斯过程（GPs）的图结构数据预测方法，通过扩展GPs框架到单纯复形（SCs），处理边缘属性和更高阶单纯复形上的属性，并通过Hodge分解增强SC表示，从而提升预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在图结构数据预测中应用广泛，但在数据稀缺时容易过拟合，导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强GNNs在数据稀缺情况下的预测能力。&lt;h4&gt;方法&lt;/h4&gt;将Gaussian process框架扩展到单纯复形，处理边缘属性和更高阶单纯复形上的属性，并利用Hodge分解来增强SC表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在各种应用中提升了预测性能，为GPs在图和SC级别预测中的应用铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;Gaussian processes结合单纯复形和Hodge分解可以有效地提升图结构数据预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在科学应用中，预测图结构数据的标签至关重要，通常使用图神经网络（GNNs）来实现。然而，当数据稀缺时，GNNs容易过拟合，导致性能不佳。最近，提出了具有图级输入的高斯过程（GPs）作为替代方案。在这项工作中，我们将高斯过程框架扩展到单纯复形（SCs），使其能够处理边缘属性和更高阶单纯复形上的属性。我们进一步通过考虑其Hodge分解来增强结果SC表示，从而能够解释SC中的同调信息，如孔洞的数量。我们证明了我们的框架在各种应用中提高了预测能力，为GPs在图和SC级别预测的更广泛应用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the labels of graph-structured data is crucial in scientificapplications and is often achieved using graph neural networks (GNNs). However,when data is scarce, GNNs suffer from overfitting, leading to poor performance.Recently, Gaussian processes (GPs) with graph-level inputs have been proposedas an alternative. In this work, we extend the Gaussian process framework tosimplicial complexes (SCs), enabling the handling of edge-level attributes andattributes supported on higher-order simplices. We further augment theresulting SC representations by considering their Hodge decompositions,allowing us to account for homological information, such as the number ofholes, in the SC. We demonstrate that our framework enhances the predictionsacross various applications, paving the way for GPs to be more widely used forgraph and SC-level predictions.</description>
      <author>example@mail.com (Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi)</author>
      <guid isPermaLink="false">2505.10877v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.08199v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对长期时间序列预测（LTSF）中的关键问题，提出了一种基于MLP的预测框架，通过多尺度预测和动态信息整合，提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测在能源消耗和天气预报等实际应用中具有广泛用途，但由于时间序列中的复杂时间模式和内在的多尺度变化，准确预测长期变化是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;解决LTSF中的关键问题，包括多粒度信息的次优使用、忽略特定通道属性以及趋势和季节成分的独特性质。&lt;h4&gt;方法&lt;/h4&gt;引入了基于MLP的预测框架，该框架通过不同尺度的清晰、并发预测来巧妙地解开复杂的时间动态，并通过一个动态分配不同粒度信息重要性的系统来整合这些多尺度预测。&lt;h4&gt;主要发现&lt;/h4&gt;在八个LTSF基准数据集上的实验结果表明，与最近的MLP方法（TimeMixer）相比，MDMixer将平均MAE性能提高了4.64%，同时实现了训练效率和模型可解释性之间的有效平衡。&lt;h4&gt;结论&lt;/h4&gt;MDMixer方法在长期时间序列预测中取得了显著的性能提升，为该领域提供了一种有效且具有可解释性的预测框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) offers broad utility in practicalsettings like energy consumption and weather prediction. Accurately predictinglong-term changes, however, is demanding due to the intricate temporal patternsand inherent multi-scale variations within time series. This work confronts keyissues in LTSF, including the suboptimal use of multi-granularity information,the neglect of channel-specific attributes, and the unique nature of trend andseasonal components, by introducing a proficient MLP-based forecastingframework. Our method adeptly disentangles complex temporal dynamics usingclear, concurrent predictions across various scales. These multi-scaleforecasts are then skillfully integrated through a system that dynamicallyassigns importance to information from different granularities, sensitive toindividual channel characteristics. To manage the specific features of temporalpatterns, a two-pronged structure is utilized to model trend and seasonalelements independently. Experimental results on eight LTSF benchmarksdemonstrate that MDMixer improves average MAE performance by 4.64% compared tothe recent state-of-the-art MLP-based method (TimeMixer), while achieving aneffective balance between training efficiency and model interpretability.</description>
      <author>example@mail.com (Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao)</author>
      <guid isPermaLink="false">2505.08199v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</title>
      <link>http://arxiv.org/abs/2505.10579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Medical Image Computing  and Computer-Assisted Intervention (MICCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于基础模型（FM）的乳腺摄影分类的公平性和偏差问题，发现虽然FM在数据多样性的情况下表现出良好的泛化能力和迁移学习能力，但其性能可能因图像质量、标签不确定性和敏感患者属性等因素而受到影响。&lt;h4&gt;背景&lt;/h4&gt;过去几十年中，计算机辅助诊断工具被开发出来以增强乳腺癌的筛查程序，但由于数据多样性和固有的偏差，这些工具在临床应用中面临着挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集，来探索FM在乳腺摄影分类中的公平性和偏差。&lt;h4&gt;方法&lt;/h4&gt;通过广泛的实验，研究了不同数据集预训练FM的性能，以及不同域之间的一般化能力，并分析了数据聚合、域适应策略和公平性感知技术对性能和偏差的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 模态特定的预训练提高了FM的性能，但基于单个数据集训练的分类器无法在不同域之间进行泛化。2. 数据聚合提高了整体性能，但并未完全消除偏差，导致在乳腺密度和年龄等代表性不足的子群体中存在显著差异。3. 域适应策略可以减少这些差异，但通常伴随着性能上的权衡。4. 公平性感知技术能够在不同子群体之间提供更稳定和公平的性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了在基于FM的模型中纳入严格公平性评估和缓解策略的必要性，以促进包容性和可泛化的AI发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在过去的几十年中，为了增强乳腺癌的筛查程序，已经开发了计算机辅助诊断工具，但它们的临床应用仍然受到数据多样性和固有偏差的挑战。尽管基础模型（FMs）最近通过利用大量和多样化的数据集，展示了令人印象深刻的泛化能力和迁移学习能力，但它们的表现可能会因图像质量、标签不确定性和敏感患者属性的变化而产生的虚假相关性而受到影响。在这项工作中，我们通过利用来自不同来源的大量数据集，包括来自代表性不足地区的数据和内部数据集，探讨了FMs在乳腺摄影分类中的公平性和偏差。我们的广泛实验表明，虽然模态特定的预训练提高了FM的性能，但基于单个数据集特征的分类器无法在不同域之间泛化。数据聚合提高了整体性能，但并未完全消除偏差，导致在代表性不足的子群体（如极端乳腺密度和年龄群体）中存在显著差异。此外，虽然域适应策略可以减少这些差异，但它们通常会导致性能上的权衡。相比之下，公平性感知技术能够在不同子群体之间提供更稳定和公平的性能。这些发现强调了将严格的公平性评估和缓解策略纳入基于FM的模型的必要性，以促进包容性和可泛化的AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past decades, computer-aided diagnosis tools for breast cancer havebeen developed to enhance screening procedures, yet their clinical adoptionremains challenged by data variability and inherent biases. Although foundationmodels (FMs) have recently demonstrated impressive generalizability andtransfer learning capabilities by leveraging vast and diverse datasets, theirperformance can be undermined by spurious correlations that arise fromvariations in image quality, labeling uncertainty, and sensitive patientattributes. In this work, we explore the fairness and bias of FMs for breastmammography classification by leveraging a large pool of datasets from diversesources-including data from underrepresented regions and an in-house dataset.Our extensive experiments show that while modality-specific pre-training of FMsenhances performance, classifiers trained on features from individual datasetsfail to generalize across domains. Aggregating datasets improves overallperformance, yet does not fully mitigate biases, leading to significantdisparities across under-represented subgroups such as extreme breast densitiesand age groups. Furthermore, while domain-adaptation strategies can reducethese disparities, they often incur a performance trade-off. In contrast,fairness-aware techniques yield more stable and equitable performance acrosssubgroups. These findings underscore the necessity of incorporating rigorousfairness evaluations and mitigation strategies into FM-based models to fosterinclusive and generalizable AI.</description>
      <author>example@mail.com (Germani Elodie, Selin Türk Ilayda, Zeineddine Fatima, Mourad Charbel, Albarqouni Shadi)</author>
      <guid isPermaLink="false">2505.10579v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</title>
      <link>http://arxiv.org/abs/2505.11191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着具身人工智能系统的多模态、个性化以及交互性的增强，需要从多样化的感官输入中有效学习，不断适应用户偏好，并在资源和隐私限制下安全运行。本文提出了一种新的范式——联邦基础模型（FFM），它结合了多模态多任务（M3T）基础模型和联邦学习（FL）的隐私保护分布式特性，以实现无线边缘的智能系统。&lt;h4&gt;背景&lt;/h4&gt;具身人工智能系统需要从多种感官输入中学习，适应用户偏好，并符合资源隐私限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型——联邦基础模型（FFM），以解决具身人工智能系统在多样化和复杂环境下的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入联邦基础模型（FFM），结合多模态多任务（M3T）基础模型和联邦学习（FL）的优势。&lt;h4&gt;主要发现&lt;/h4&gt;FFM能够统一多模态多任务基础模型和联邦学习的优势，为具身人工智能系统提供一种新的解决方案。&lt;h4&gt;结论&lt;/h4&gt;FFM在具身人工智能生态系统中具有重要作用，能够解决当前系统的复杂性和多样化需求。&lt;h4&gt;翻译&lt;/h4&gt;随着具身人工智能系统的多模态、个性化以及交互性的不断增强，它们必须从多样化的感官输入中有效地学习，持续适应用户偏好，并在资源和隐私约束下安全运行。这些挑战暴露了对于能够快速、情境感知地适应，同时平衡模型泛化和个性化的机器学习模型的迫切需求。在这里，两种方法作为合适的候选方案出现，每种方法都提供了这些能力的一部分：基础模型（FMs）提供了跨任务和模态泛化的途径，而联邦学习（FL）提供了分布式、隐私保护的模型更新和用户级模型个性化的基础设施。然而，当单独使用时，这些方法中的每一种都未能满足现实世界中具身环境的复杂和多样化的能力要求。在这篇愿景论文中，我们为具身人工智能引入了联邦基础模型（FFMs），这是一种新的范式，它统一了多模态多任务（M3T）FMs和FL的隐私保护分布式特性的优势，使得智能系统能够在无线边缘运行。我们在统一的框架下收集了FFMs在具身人工智能生态系统中的关键部署维度，我们将其命名为“EMBODY”：具身异质性、模态丰富性和不平衡性、带宽和计算限制、设备上的持续学习、分布式控制和自主性，以及产生安全性、隐私性和个性化。对于每一项，我们确定了具体挑战并展望了可行的研究方向。我们还提出了一种在具身人工智能系统中部署FFMs的评估框架，以及相关的权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As embodied AI systems become increasingly multi-modal, personalized, andinteractive, they must learn effectively from diverse sensory inputs, adaptcontinually to user preferences, and operate safely under resource and privacyconstraints. These challenges expose a pressing need for machine learningmodels capable of swift, context-aware adaptation while balancing modelgeneralization and personalization. Here, two methods emerge as suitablecandidates, each offering parts of these capabilities: Foundation Models (FMs)provide a pathway toward generalization across tasks and modalities, whereasFederated Learning (FL) offers the infrastructure for distributed,privacy-preserving model updates and user-level model personalization. However,when used in isolation, each of these approaches falls short of meeting thecomplex and diverse capability requirements of real-world embodiedenvironments. In this vision paper, we introduce Federated Foundation Models(FFMs) for embodied AI, a new paradigm that unifies the strengths ofmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed natureof FL, enabling intelligent systems at the wireless edge. We collect criticaldeployment dimensions of FFMs in embodied AI ecosystems under a unifiedframework, which we name "EMBODY": Embodiment heterogeneity, Modality richnessand imbalance, Bandwidth and compute constraints, On-device continual learning,Distributed control and autonomy, and Yielding safety, privacy, andpersonalization. For each, we identify concrete challenges and envisionactionable research directions. We also present an evaluation framework fordeploying FFMs in embodied AI systems, along with the associated trade-offs.</description>
      <author>example@mail.com (Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2505.11191v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>RapidGNN: Communication Efficient Large-Scale Distributed Training of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.10806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为RapidGNN的图神经网络训练方法，通过引入确定性采样策略来优化大规模图上的GNN训练，显著提高了通信效率和训练吞吐量。&lt;h4&gt;背景&lt;/h4&gt;尽管图神经网络（GNNs）在多个领域取得了最先进的性能，但在大规模图上的训练由于内存需求高和分布式环境中的通信开销大而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出RapidGNN以减少大规模图上GNN训练的内存需求和通信开销。&lt;h4&gt;方法&lt;/h4&gt;RapidGNN通过引入确定性采样策略来预计算小批量，并利用这种策略来准确预测特征访问模式，从而实现最优的缓存构建和远程特征的及时预取。&lt;h4&gt;主要发现&lt;/h4&gt;在Reddit和OGBN-Products数据集上的评估表明，RapidGNN在训练时间和远程特征获取方面实现了显著降低，在通信效率和吞吐量方面优于现有模型。RapidGNN将端到端训练吞吐量平均提高了2.10倍（在某些设置中最高可达2.45倍），同时将远程特征获取减少了超过4倍，并降低了高达23%的能量消耗。&lt;h4&gt;结论&lt;/h4&gt;RapidGNN展示了在大型真实世界图数据集上进行可扩展、高性能GNN训练的潜力，同时提高了能源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)performance in diverse domains. However, training GNNs on large-scale graphsposes significant challenges due to high memory demands and significantcommunication overhead in distributed settings. Traditional sampling-basedapproaches mitigate computation load to some extent but often fail to addresscommunication inefficiencies inherent in distributed environments. This paperpresents RapidGNN that introduces a deterministic sampling strategy toprecompute mini-batches. By leveraging the sampling strategy, RapidGNNaccurately anticipates feature access patterns, enabling optimal cacheconstruction and timely prefetching of remote features. This reduces thefrequency and latency of remote data transfers without compromising thestochastic nature of training. Evaluations on Reddit and OGBN-Products datasetsdemonstrate that RapidGNN achieves significant reductions in training time andremote feature fetches, outperforming existing models in both communicationefficiency and throughput. Our findings highlight RapidGNN's potential forscalable, high-performance GNN training across large, real-world graph datasetsalong with improving energy efficiency. Our model improves end-to-end trainingthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x insome settings), while cutting remote feature fetches by over 4x. It alsoreduces energy consumption up to 23%.</description>
      <author>example@mail.com (Arefin Niam, M S Q Zulkar Nine)</author>
      <guid isPermaLink="false">2505.10806v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Staying Fresh: Efficient Algorithms for Timely Social Information Distribution</title>
      <link>http://arxiv.org/abs/2308.13260v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work is an updated version of our previous paper titled  "Approximation Algorithms to Enhance Social Sharing of Fresh  Point-of-Interest Information."&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于位置的社会网络中，如何通过选择热点用户来提高用户的社会兴趣点信息共享。&lt;h4&gt;背景&lt;/h4&gt;在基于位置的社会网络中，用户在附近感知兴趣点信息并与朋友分享，但信息传播存在延迟。&lt;h4&gt;目的&lt;/h4&gt;研究一个组合优化问题，涉及城市感知网络和在线社交网络的交互。&lt;h4&gt;方法&lt;/h4&gt;证明了该问题为NP-hard，并通过分析两个网络的交互效应，将涉及的兴趣点共享过程转化为矩阵计算，以得到一个封闭形式的优化目标。此外，提出了一种多项式时间算法，保证了近似最优解。&lt;h4&gt;主要发现&lt;/h4&gt;该问题为NP-hard，现有近似解不可行；通过矩阵计算得到封闭形式的优化目标；提出了多项式时间算法，保证了近似最优解；提出了用户移动感知更多兴趣点的增强自适应算法。&lt;h4&gt;结论&lt;/h4&gt;理论结果通过合成和真实世界数据集的仿真结果得到验证。&lt;h4&gt;翻译&lt;/h4&gt;在基于位置的社会网络（LBSNs）中，用户在附近感知城市兴趣点（PoI）信息，并与在线社交网络中的朋友分享此类信息。鉴于用户的社交联系有限以及新鲜PoI传播的严重滞后，主要的LBSNs旨在通过选择m个用户中的k个作为热点，并将他们的新鲜PoI信息广播到整个用户社区来提高用户的社交PoI共享。这促使我们研究一个新的组合优化问题，该问题涉及城市感知网络和在线社交网络的交互。我们证明了这个问题是NP-hard的，并且使得现有的近似解决方案不可行。通过分析两个网络的交互效应，我们成功地实现了涉及两个网络的PoI共享过程，将其转化为矩阵计算，以推导出一个封闭形式的优化目标，以保持期望的性质（例如，次可加性和单调性）。这一发现使我们能够开发一个多项式时间算法，保证了近似最优解的（1-（m-2）/m（k-1）/k）k次方。此外，我们允许每个选定的用户移动并感知更多PoI信息以共享，并提出了一种具有良好性能保证的增强自适应算法。最后，我们的理论结果通过使用合成和真实世界数据集的仿真结果得到证实。&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In location-based social networks (LBSNs), users sense urbanpoint-of-interest (PoI) information in the vicinity and share such informationwith friends in online social networks. Given users' limited social connectionsand severe lags in disseminating fresh PoI to all, major LBSNs aim to enhanceusers' social PoI sharing by selecting $k$ out of $m$ users as hotspots andbroadcasting their fresh PoI information to the entire user community. Thismotivates us to study a new combinatorial optimization problem that involvesthe interplay between an urban sensing network and an online social network. Weprove that this problem is NP-hard and also renders existing approximationsolutions not viable. Through analyzing the interplay effects between the twonetworks, we successfully transform the involved PoI-sharing process across twonetworks to matrix computations for deriving a closed-form objective to holddesirable properties (e.g., submodularity and monotonicity). This findingenables us to develop a polynomial-time algorithm that guarantees a($1-\frac{m-2}{m}(\frac{k-1}{k})^k$) approximation of the optimum. Furthermore,we allow each selected user to move around and sense more PoI information toshare and propose an augmentation-adaptive algorithm with decent performanceguarantees. Finally, our theoretical results are corroborated by our simulationfindings using both synthetic and real-world datasets.</description>
      <author>example@mail.com (Songhua Li, Lingjie Duan)</author>
      <guid isPermaLink="false">2308.13260v3</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Time-Series AI Model for Realized Volatility Forecasting</title>
      <link>http://arxiv.org/abs/2505.11163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了时间序列基础模型（FMs）在波动率预测方面的有效性，特别是在金融风险管理中的核心任务。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型（FMs）已成为零样本多领域预测的一种流行范式，这些模型在多个不同的时间序列领域中被训练，包括金融数据。&lt;h4&gt;目的&lt;/h4&gt;评估TimesFM模型在波动率预测方面的有效性，并与标准计量经济学基准进行比较。&lt;h4&gt;方法&lt;/h4&gt;首先评估了预训练（零样本）的TimesFM模型，然后通过增量学习进行了自定义微调，并与标准计量经济学基准进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型提供了一个合理的基线，但研究表明，增量微调（使模型能够随着时间的推移适应新的金融回报数据）对于有效学习波动率模式是必不可少的。微调后的变体不仅提高了预测精度，而且在Diebold-Mariano和Giacomini-White测试中统计上优于传统模型。&lt;h4&gt;结论&lt;/h4&gt;这些结果突出了基础模型作为可扩展和自适应工具的潜力，它们在动态市场环境中，当与有针对性的微调策略相结合时，能够提供强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. These models are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including financial data. In this study, we evaluate the effectiveness of FMs, specifically the TimesFM model, for volatility forecasting, a core task in financial risk management. We first evaluate TimesFM in its pretrained (zero-shot) form, followed by our custom fine-tuning procedure based on incremental learning, and compare the resulting models against standard econometric benchmarks. While the pretrained model provides a reasonable baseline, our findings show that incremental fine-tuning, which allows the model to adapt to new financial return data over time, is essential for learning volatility patterns effectively. Fine-tuned variants not only improve forecast accuracy but also statistically outperform traditional models, as demonstrated through Diebold-Mariano and Giacomini-White tests. These results highlight the potential of foundation models as scalable and adaptive tools for financial forecasting-capable of delivering strong performance in dynamic market environments when paired with targeted fine-tuning strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (FMs) have emerged as a popular paradigm forzero-shot multi-domain forecasting. These models are trained on numerousdiverse datasets and claim to be effective forecasters across multipledifferent time series domains, including financial data. In this study, weevaluate the effectiveness of FMs, specifically the TimesFM model, forvolatility forecasting, a core task in financial risk management. We firstevaluate TimesFM in its pretrained (zero-shot) form, followed by our customfine-tuning procedure based on incremental learning, and compare the resultingmodels against standard econometric benchmarks. While the pretrained modelprovides a reasonable baseline, our findings show that incremental fine-tuning,which allows the model to adapt to new financial return data over time, isessential for learning volatility patterns effectively. Fine-tuned variants notonly improve forecast accuracy but also statistically outperform traditionalmodels, as demonstrated through Diebold-Mariano and Giacomini-White tests.These results highlight the potential of foundation models as scalable andadaptive tools for financial forecasting-capable of delivering strongperformance in dynamic market environments when paired with targetedfine-tuning strategies.</description>
      <author>example@mail.com (Anubha Goel, Puneet Pasricha, Martin Magris, Juho Kanniainen)</author>
      <guid isPermaLink="false">2505.11163v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics</title>
      <link>http://arxiv.org/abs/2505.10711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main article 8 pages (20 in total with supplementary information  included), 3 main article figures and 3 supplemental figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GNN-Suite，这是一个用于构建和评估图神经网络（GNN）架构的鲁棒模块化框架，在计算生物学中应用。GNN-Suite通过Nextflow工作流程标准化实验和可重复性，并展示了其在识别癌症驱动基因方面的效用。&lt;h4&gt;背景&lt;/h4&gt;GNN-Suite框架旨在解决计算生物学中GNN架构构建和评估的标准化问题。&lt;h4&gt;目的&lt;/h4&gt;目的是提供一种标准化的方法来构建、比较和评估GNN架构，以促进计算生物学中的可重复研究和基准测试标准的提升。&lt;h4&gt;方法&lt;/h4&gt;使用Nextflow工作流程进行实验标准化，构建分子网络，使用来自STRING和BioGRID的蛋白质-蛋白质相互作用（PPI）数据，并使用PCAWG、PID和COSMIC-CGC存储库中的特征进行节点注释。GNN架构包括GAT、GAT3H、GCN、GCN2、GIN、GTN、HGCN、PHGCN和GraphSAGE，以及基线逻辑回归（LR）模型。所有GNN均配置为标准化的两层模型，并使用统一的超参数进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;GCN2在基于STRING的网络中实现了最高的平衡准确率（BACC为0.807 +/- 0.035），所有GNN类型都优于LR基线，突出了基于网络学习在仅基于特征方法之上的优势。&lt;h4&gt;结论&lt;/h4&gt;GNN-Suite提供了一个共同框架，有助于识别最佳模型以及最有效的数据整合方式。通过使GNN-Suite公开可用，旨在促进可重复研究和提升计算生物学中的基准测试标准。&lt;h4&gt;翻译&lt;/h4&gt;We present GNN-Suite, a robust modular framework for constructing and benchmarking Graph Neural Network (GNN) architectures in computational biology. GNN-Suite standardises experimentation and reproducibility using the Nextflow workflow to evaluate GNN performance. We demonstrate its utility in identifying cancer-driver genes by constructing molecular networks from protein-protein interaction (PPI) data from STRING and BioGRID and annotating nodes with features from the PCAWG, PID, and COSMIC-CGC repositories. Our design enables fair comparisons among diverse GNN architectures including GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline Logistic Regression (LR) model. All GNNs were configured as standardised two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam optimizer with learning rate = 0.01; and an adjusted binary cross-entropy loss to address class imbalance) over an 80/20 train-test split for 300 epochs. Each model was evaluated over 10 independent runs with different random seeds to yield statistically robust performance metrics, with balanced accuracy (BACC) as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/- 0.035) on a STRING-based network, although all GNN types outperformed the LR baseline, highlighting the advantage of network-based learning over feature-only approaches. Our results show that a common framework for implementing and evaluating GNN architectures aids in identifying not only the best model but also the most effective means of incorporating complementary data. By making GNN-Suite publicly available, we aim to foster reproducible research and promote improved benchmarking standards in computational biology. Future work will explore additional omics datasets and further refine network architectures to enhance predictive accuracy and interpretability in biomedical applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GNN-Suite, a robust modular framework for constructing andbenchmarking Graph Neural Network (GNN) architectures in computational biology.GNN-Suite standardises experimentation and reproducibility using the Nextflowworkflow to evaluate GNN performance. We demonstrate its utility in identifyingcancer-driver genes by constructing molecular networks from protein-proteininteraction (PPI) data from STRING and BioGRID and annotating nodes withfeatures from the PCAWG, PID, and COSMIC-CGC repositories.  Our design enables fair comparisons among diverse GNN architectures includingGAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baselineLogistic Regression (LR) model. All GNNs were configured as standardisedtwo-layer models and trained with uniform hyperparameters (dropout = 0.2; Adamoptimiser with learning rate = 0.01; and an adjusted binary cross-entropy lossto address class imbalance) over an 80/20 train-test split for 300 epochs. Eachmodel was evaluated over 10 independent runs with different random seeds toyield statistically robust performance metrics, with balanced accuracy (BACC)as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-0.035) on a STRING-based network, although all GNN types outperformed the LRbaseline, highlighting the advantage of network-based learning overfeature-only approaches.  Our results show that a common framework for implementing and evaluating GNNarchitectures aids in identifying not only the best model but also the mosteffective means of incorporating complementary data. By making GNN-Suitepublicly available, we aim to foster reproducible research and promote improvedbenchmarking standards in computational biology. Future work will exploreadditional omics datasets and further refine network architectures to enhancepredictive accuracy and interpretability in biomedical applications.</description>
      <author>example@mail.com (Sebestyén Kamp, Giovanni Stracquadanio, T. Ian Simpson)</author>
      <guid isPermaLink="false">2505.10711v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video</title>
      <link>http://arxiv.org/abs/2505.11129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2405.14650&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhiNet v2是一个基于Transformer的新型自监督学习模型，它能够处理时间序列视觉输入，无需强数据增强，在计算机视觉领域取得了与现有先进模型相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在计算机视觉领域取得了显著进展，但尚未充分利用生物视觉处理系统的见解。&lt;h4&gt;目的&lt;/h4&gt;提出PhiNet v2模型，以更接近人类认知过程的方式处理视觉信息。&lt;h4&gt;方法&lt;/h4&gt;PhiNet v2基于ResNet骨干网络，使用Transformer架构，并利用变分推理从连续输入流中学习鲁棒的视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;PhiNet v2在无需强数据增强的情况下，与最先进的视觉基础模型相比，实现了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;PhiNet v2是朝着更生物可解释的计算机视觉系统迈出的重要一步，这些系统能够以更接近人类认知过程的方式处理视觉信息。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning (SSL) have revolutionizedcomputer vision through innovative architectures and learning objectives, yetthey have not fully leveraged insights from biological visual processingsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it isbased on a ResNet backbone and operates on static image inputs with strongaugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-basedarchitecture that processes temporal visual input (that is, sequences ofimages) without relying on strong augmentation. Our model leverages variationalinference to learn robust visual representations from continuous input streams,similar to human visual processing. Through extensive experimentation, wedemonstrate that PhiNet v2 achieves competitive performance compared tostate-of-the-art vision foundation models, while maintaining the ability tolearn from sequential input without strong data augmentation. This workrepresents a significant step toward more biologically plausible computervision systems that process visual information in a manner more closely alignedwith human cognitive processes.</description>
      <author>example@mail.com (Makoto Yamada, Kian Ming A. Chai, Ayoub Rhim, Satoki Ishikawa, Mohammad Sabokrou, Yao-Hung Hubert Tsai)</author>
      <guid isPermaLink="false">2505.11129v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>ExploreGS: a vision-based low overhead framework for 3D scene reconstruction</title>
      <link>http://arxiv.org/abs/2505.10578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为ExploreGS的低开销、基于视觉的3D场景重建框架，用于无人机。&lt;h4&gt;背景&lt;/h4&gt;传统的基于激光雷达的点云获取过程成本较高。&lt;h4&gt;目的&lt;/h4&gt;通过使用RGB图像，以较低的成本实现高质量的重建。&lt;h4&gt;方法&lt;/h4&gt;该框架集成了场景探索和模型重建，利用词袋（BoW）模型实现实时处理能力，并可以在设备上执行3D高斯分层（3DGS）训练。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实环境中的综合实验表明，ExploreGS框架在资源受限的设备上具有高效性和适用性，同时保持了与最先进方法相当的重建质量。&lt;h4&gt;结论&lt;/h4&gt;ExploreGS框架是一种有效且经济的无人机3D场景重建解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a low-overhead, vision-based 3D scene reconstructionframework for drones, named ExploreGS. By using RGB images, ExploreGS replacestraditional lidar-based point cloud acquisition process with a vision model,achieving a high-quality reconstruction at a lower cost. The frameworkintegrates scene exploration and model reconstruction, and leverags aBag-of-Words(BoW) model to enable real-time processing capabilities, therefore,the 3D Gaussian Splatting (3DGS) training can be executed on-board.Comprehensive experiments in both simulation and real-world environmentsdemonstrate the efficiency and applicability of the ExploreGS framework onresource-constrained devices, while maintaining reconstruction qualitycomparable to state-of-the-art methods.</description>
      <author>example@mail.com (Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu)</author>
      <guid isPermaLink="false">2505.10578v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding</title>
      <link>http://arxiv.org/abs/2505.01481v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoHallu，一个由合成视频组成的视频问答对基准，用于评估多模态大型语言模型在检测异常情况上的能力。&lt;h4&gt;背景&lt;/h4&gt;合成视频生成受到广泛关注，但容易违反常识和物理定律。这突显了需要可靠的不正常性检测器，这些检测器应理解这些原则并能够抵抗幻觉。&lt;h4&gt;目的&lt;/h4&gt;开发VideoHallu，以评估多模态大型语言模型（MLLMs）在检测人类感知明显但常因语言先验而幻觉的异常情况上的批判性思维能力。&lt;h4&gt;方法&lt;/h4&gt;VideoHallu由Veo2、Sora和Kling等模型生成的合成视频和专家制作的反直觉问答对组成。它评估MLLMs在一致性、常识和物理方面的异常检测能力。还使用了SOTA MLLMs，并通过GRPO和课程学习进行后训练。&lt;h4&gt;主要发现&lt;/h4&gt;这些模型在许多现实世界基准上表现良好，但在合成视频中的基本物理和常识推理方面仍存在困难。后训练可以改善MLLMs的异常检测和批判性思维能力。&lt;h4&gt;结论&lt;/h4&gt;针对性的训练对于提高MLLMs对常识和物理定律的理解是有价值的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zli12321/videohallu&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic video generation has gained significant attention for its realismand broad applications, but remains prone to violations of common sense andphysical laws. This highlights the need for reliable abnormality detectors thatunderstand such principles and are robust to hallucinations. To address this,we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built fromsynthetic videos generated by models like Veo2, Sora, and Kling, paired withexpert-crafted counterintuitive QA to evaluate the critical thinking abilitiesof Multi-modal Large Language Models (MLLMs) on abnormalities that areperceptually obvious to humans but often hallucinated due to language priors.VideoHallu evaluates MLLMs' abnormality detection abilities with examplesacross alignment, consistency, commonsense, and physics. We benchmark SOTAMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, andVideoChat-R1. We observe that these models perform well on many real-worldbenchmarks like MVBench and MovieChat, but still struggle with basicphysics-based and commonsense reasoning in synthetic videos. We further showthat post-training with Group Relative Policy Optimization (GRPO), usingcurriculum learning on datasets combining video QA with counterintuitivecommonsense and physics reasoning over real and synthetic videos, improvesMLLMs' abnormality detection and critical thinking, demonstrating the value oftargeted training for improving their understanding of commonsense and physicallaws.</description>
      <author>example@mail.com (Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber)</author>
      <guid isPermaLink="false">2505.01481v2</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GraphOracle: A Foundation Model for Knowledge Graph Reasoning</title>
      <link>http://arxiv.org/abs/2505.11125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GraphOracle的关系中心基础模型，该模型通过将知识图转换为关系依赖图（RDG）来统一知识图中的推理，并使用依赖注意力机制学习关系和实体的归纳表示。通过在多样化知识图上的预训练和分钟级微调，GraphOracle能够有效地泛化到未见过的实体、关系和整个图，并在31个不同基准测试中展现出优异的性能。&lt;h4&gt;背景&lt;/h4&gt;由于知识图的动态性质和跨领域推理的需求，开发与基础模型类似的知识图模型具有独特挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一知识图中推理的基础模型，以解决知识图的动态性质和跨领域推理的需求。&lt;h4&gt;方法&lt;/h4&gt;GraphOracle模型通过将知识图转换为RDG，并使用查询依赖的注意力机制学习关系和实体的归纳表示。此外，通过在多样化知识图上的预训练和分钟级微调来实现泛化。&lt;h4&gt;主要发现&lt;/h4&gt;GraphOracle在31个不同基准测试中表现出色，与最强大的基线相比，预测性能提高了35%。&lt;h4&gt;结论&lt;/h4&gt;GraphOracle模型通过有效的预训练和微调，在知识图推理方面实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为GraphOracle的关系中心基础模型，通过将知识图转换为关系依赖图，并使用查询依赖的注意力机制学习关系和实体的归纳表示，实现了知识图推理的统一。在多样化知识图上的预训练和分钟级微调使得该模型能够有效地泛化到未见过的实体、关系和整个图，通过31个不同基准测试的全面实验，该模型展现了优异的性能，与最强大的基线相比，预测性能提高了35%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have demonstrated remarkable capabilities across variousdomains, but developing analogous models for knowledge graphs presents uniquechallenges due to their dynamic nature and the need for cross-domain reasoning.To address these issues, we introduce \textbf{\textsc{GraphOracle}}, arelation-centric foundation model that unifies reasoning across knowledgegraphs by converting them into Relation-Dependency Graphs (RDG), explicitlyencoding compositional patterns with fewer edges than prior methods. Aquery-dependent attention mechanism is further developed to learn inductiverepresentations for both relations and entities. Pre-training on diverseknowledge graphs, followed by minutes-level fine-tuning, enables effectivegeneralization to unseen entities, relations, and entire graphs. Throughcomprehensive experiments on 31 diverse benchmarks spanning transductive,inductive, and cross-domain settings, we demonstrate consistentstate-of-the-art performance with minimal adaptation, improving the predictionperformance by up to 35\% compared to the strongest baselines.</description>
      <author>example@mail.com (Enjun Du, Siyi Liu, Yongqi Zhang)</author>
      <guid isPermaLink="false">2505.11125v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.11121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE International Geoscience and Remote Sensing  Symposium (IGARSS) 2025. Our code is available at  https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种加权特征聚合（WFA）策略，用于遥感（RS）中的视觉语言模型（VLM）预训练，以解决预训练和推理时间增加的问题。&lt;h4&gt;背景&lt;/h4&gt;通过预训练视觉语言模型（VLMs）来开发基础模型在遥感领域引起了广泛关注。VLM预训练旨在从大量的图像-文本对中学习图像和语言的对应关系。&lt;h4&gt;目的&lt;/h4&gt;目的是通过提取和利用每张图像多个字幕中的互补信息，同时通过重要性加权减少冗余信息，从而提高预训练和推理效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术来计算不同图像字幕的适应性重要性权重：(i) 非参数唯一性，基于双语评估（BLEU）分数来强调独特句子并减少重复句子的影响；(ii) 基于学习的注意力机制，通过注意力机制而不是手工特征来学习重要性权重。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的策略使得在遥感领域有效地预训练VLMs成为可能。基于实验分析，得出了根据下游任务需求和资源约束选择适当技术的指南。&lt;h4&gt;结论&lt;/h4&gt;WFA策略结合两种技术能够有效地在遥感领域预训练VLMs，并提供了根据不同需求选择技术的指导原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of foundation models through pretraining of vision-languagemodels (VLMs) has recently attracted great attention in remote sensing (RS).VLM pretraining aims to learn image and language alignments from a large numberof image-text pairs. Each pretraining image is often associated with multiplecaptions containing redundant information due to repeated or semanticallysimilar phrases, resulting in increased pretraining and inference time. Toovercome this, we introduce a weighted feature aggregation (WFA) strategy forVLM pretraining in RS. Our strategy aims to extract and exploit complementaryinformation from multiple captions per image while reducing redundanciesthrough feature aggregation with importance weighting. To calculate adaptiveimportance weights for different captions of each image, we propose twotechniques: (i) non-parametric uniqueness and (ii) learning-based attention. Inthe first technique, importance weights are calculated based on the bilingualevaluation understudy (BLEU) scores of the captions to emphasize uniquesentences and reduce the influence of repetitive ones. In the second technique,importance weights are learned through an attention mechanism instead ofrelying on hand-crafted features. The effectiveness of the proposed WFAstrategy with the two techniques is analyzed in terms of downstream performanceon text-to-image retrieval in RS. Experimental results show that the proposedstrategy enables efficient and effective pretraining of VLMs in RS. Based onthe experimental analysis, we derive guidelines for selecting appropriatetechniques depending on downstream task requirements and resource constraints.The code of this work is publicly available athttps://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.</description>
      <author>example@mail.com (Mathis Jürgen Adler, Leonard Hackel, Gencer Sumbul, Begüm Demir)</author>
      <guid isPermaLink="false">2505.11121v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
      <link>http://arxiv.org/abs/2505.00254v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, AVAS, add latency breakdown&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为AVAS的VLM-powered系统，用于开放式的先进视频分析，以解决现有系统在处理超长视频内容时的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的视频分析在多个领域变得至关重要，但现有系统通常限于特定的预定义任务，限制了其在开放式分析场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出AVAS系统，旨在实现开放式的视频理解、推理和分析。&lt;h4&gt;方法&lt;/h4&gt;AVAS系统包括两个关键创新：(1)近实时构建事件知识图谱（EKGs）以高效索引长或连续视频流；(2)利用EKGs的代理检索-生成机制来处理复杂和多样化的查询。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试LVBench和VideoMME-Long上，AVAS实现了最先进的性能，分别达到62.3%和64.1%的准确率，显著超过了现有的VLM和视频检索增强生成（RAG）系统。在新的基准AVAS-100上，AVAS也取得了顶尖性能，准确率达到75.8%。&lt;h4&gt;结论&lt;/h4&gt;AVAS系统在超长和开放式视频场景中的视频分析方面表现出色，为视频分析领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven video analytics has become increasingly pivotal across diversedomains. However, existing systems are often constrained to specific,predefined tasks, limiting their adaptability in open-ended analyticalscenarios. The recent emergence of Video-Language Models (VLMs) astransformative technologies offers significant potential for enablingopen-ended video understanding, reasoning, and analytics. Nevertheless, theirlimited context windows present challenges when processing ultra-long videocontent, which is prevalent in real-world applications. To address this, weintroduce AVAS, a VLM-powered system designed for open-ended, advanced videoanalytics. AVAS incorporates two key innovations: (1) the near real-timeconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long orcontinuous video streams, and (2) an agentic retrieval-generation mechanismthat leverages EKGs to handle complex and diverse queries. Comprehensiveevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate thatAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,respectively, significantly surpassing existing VLM and videoRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate videoanalytics in ultra-long and open-world video scenarios, we introduce a newbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hoursin duration, along with 120 manually annotated, diverse, and complexquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with anaccuracy of 75.8%.</description>
      <author>example@mail.com (Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu)</author>
      <guid isPermaLink="false">2505.00254v3</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere</title>
      <link>http://arxiv.org/abs/2505.11029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AsymVLM的概率视觉语言模型（VLM），用于处理自然语言和视觉数据中的固有模糊性和不确定性。&lt;h4&gt;背景&lt;/h4&gt;现有的确定性VLM在处理自然语言和视觉数据时无法捕捉其固有的模糊性和不确定性，而现有的概率后处理方法未能考虑模态的不对称不确定性结构和确定嵌入在单位超球面上的约束。&lt;h4&gt;目的&lt;/h4&gt;提出AsymVLM，以解决文本和视觉数据中固有的不对称不确定性结构，并从预训练的VLMs构建概率嵌入，实现不确定性量化。&lt;h4&gt;方法&lt;/h4&gt;AsymVLM通过将确定性嵌入映射到概率分布，并在单位超球面上建立概率嵌入，从而实现不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;AsymVLM在标准的基准测试中验证了其有效性，并通过消融实验展示了文本和视觉数据不确定性结构中不对称性的本质。&lt;h4&gt;结论&lt;/h4&gt;AsymVLM能够有效处理自然语言和视觉数据中的不确定性，并在相关任务中提供更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language models (VLMs) as foundation models have significantly enhanced performance across a wide range of visual and textual tasks, without requiring large-scale training from scratch for downstream tasks. However, these deterministic VLMs fail to capture the inherent ambiguity and uncertainty in natural language and visual data. Recent probabilistic post-hoc adaptation methods address this by mapping deterministic embeddings onto probability distributions; however, existing approaches do not account for the asymmetric uncertainty structure of the modalities, and the constraint that meaningful deterministic embeddings reside on a unit hypersphere, potentially leading to suboptimal performance. In this paper, we address the asymmetric uncertainty structure inherent in textual and visual data, and propose AsymVLM to build probabilistic embeddings from pre-trained VLMs on the unit hypersphere, enabling uncertainty quantification. We validate the effectiveness of the probabilistic embeddings on established benchmarks, and present comprehensive ablation studies demonstrating the inherent nature of asymmetry in the uncertainty structure of textual and visual data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) as foundation models have significantlyenhanced performance across a wide range of visual and textual tasks, withoutrequiring large-scale training from scratch for downstream tasks. However,these deterministic VLMs fail to capture the inherent ambiguity and uncertaintyin natural language and visual data. Recent probabilistic post-hoc adaptationmethods address this by mapping deterministic embeddings onto probabilitydistributions; however, existing approaches do not account for the asymmetricuncertainty structure of the modalities, and the constraint that meaningfuldeterministic embeddings reside on a unit hypersphere, potentially leading tosuboptimal performance. In this paper, we address the asymmetric uncertaintystructure inherent in textual and visual data, and propose AsymVLM to buildprobabilistic embeddings from pre-trained VLMs on the unit hypersphere,enabling uncertainty quantification. We validate the effectiveness of theprobabilistic embeddings on established benchmarks, and present comprehensiveablation studies demonstrating the inherent nature of asymmetry in theuncertainty structure of textual and visual data.</description>
      <author>example@mail.com (Li Ju, Max Andersson, Stina Fredriksson, Edward Glöckner, Andreas Hellander, Ekta Vats, Prashant Singh)</author>
      <guid isPermaLink="false">2505.11029v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges</title>
      <link>http://arxiv.org/abs/2505.10993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了计算病理学中生成建模的进展，涵盖了图像生成、文本生成、多模态图像-文本生成以及其他生成应用，如空间模拟和分子推断。&lt;h4&gt;背景&lt;/h4&gt;生成建模在计算病理学中显示出巨大潜力，包括数据高效学习、合成数据增强和多模态表示。&lt;h4&gt;目的&lt;/h4&gt;综合分析该领域的最新进展，并讨论开放挑战和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;通过分析超过150篇代表性研究，追踪生成架构从早期生成对抗网络到最近扩散模型和具有生成能力的基座模型的演变。&lt;h4&gt;主要发现&lt;/h4&gt;指出了生成高保真全切片图像、临床可解释性和合成数据伦理法律影响等挑战。&lt;h4&gt;结论&lt;/h4&gt;强调了开发统一、多模态和临床可部署的生成系统的重要性，并为研究者与实践者提供了基础参考。&lt;h4&gt;翻译&lt;/h4&gt;Generative modeling has emerged as a promising direction in computational pathology, offering capabilities such as data-efficient learning, synthetic data augmentation, and multimodal representation across diverse diagnostic tasks. This review provides a comprehensive synthesis of recent progress in the field, organized into four key domains: image generation, text generation, multimodal image-text generation, and other generative applications, including spatial simulation and molecular inference. By analyzing over 150 representative studies, we trace the evolution of generative architectures from early generative adversarial networks to recent advances in diffusion models and foundation models with generative capabilities. We further examine the datasets and evaluation protocols commonly used in this domain and highlight ongoing limitations, including challenges in generating high-fidelity whole-slide images, clinical interpretability, and concerns related to the ethical and legal implications of synthetic data. The review concludes with a discussion of open challenges and prospective research directions, with an emphasis on developing unified, multimodal, and clinically deployable generative systems. This work aims to provide a foundational reference for researchers and practitioners developing and applying generative models in computational pathology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has emerged as a promising direction in computationalpathology, offering capabilities such as data-efficient learning, syntheticdata augmentation, and multimodal representation across diverse diagnostictasks. This review provides a comprehensive synthesis of recent progress in thefield, organized into four key domains: image generation, text generation,multimodal image-text generation, and other generative applications, includingspatial simulation and molecular inference. By analyzing over 150representative studies, we trace the evolution of generative architectures fromearly generative adversarial networks to recent advances in diffusion modelsand foundation models with generative capabilities. We further examine thedatasets and evaluation protocols commonly used in this domain and highlightongoing limitations, including challenges in generating high-fidelity wholeslide images, clinical interpretability, and concerns related to the ethicaland legal implications of synthetic data. The review concludes with adiscussion of open challenges and prospective research directions, with anemphasis on developing unified, multimodal, and clinically deployablegenerative systems. This work aims to provide a foundational reference forresearchers and practitioners developing and applying generative models incomputational pathology.</description>
      <author>example@mail.com (Yuan Zhang, Xinfeng Zhang, Xiaoming Qi Xinyu Wu, Feng Chen, Guanyu Yang, Huazhu Fu)</author>
      <guid isPermaLink="false">2505.10993v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出首个针对基因组基础模型（GFMs）的统一对抗攻击基准GenoArmory，用于评估GFMs对抗攻击的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;目前缺乏对GFMs对抗攻击脆弱性的全面评估框架。&lt;h4&gt;目的&lt;/h4&gt;构建一个全面评估GFMs对抗攻击脆弱性的框架，并引入新的对抗样本数据集GenoAdv以提高GFMs的安全性。&lt;h4&gt;方法&lt;/h4&gt;使用四种广泛采用的攻击算法和三种防御策略，评估了五种最先进的GFMs的对抗鲁棒性，并分析了模型架构、量化方案和训练数据集对GFM脆弱性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分类模型相较于生成模型对对抗扰动具有更强的鲁棒性，对抗攻击常针对具有生物学意义的基因组区域，表明这些模型有效捕捉了有意义的序列特征。&lt;h4&gt;结论&lt;/h4&gt;GenoArmory提供了一个评估GFMs对抗攻击脆弱性的框架，有助于提高GFMs的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the first unified adversarial attack benchmark for GenomicFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,GenoArmory offers the first comprehensive evaluation framework tosystematically assess the vulnerability of GFMs to adversarial attacks.Methodologically, we evaluate the adversarial robustness of fivestate-of-the-art GFMs using four widely adopted attack algorithms and threedefense strategies. Importantly, our benchmark provides an accessible andcomprehensive framework to analyze GFM vulnerabilities with respect to modelarchitecture, quantization schemes, and training datasets. Additionally, weintroduce GenoAdv, a new adversarial sample dataset designed to improve GFMsafety. Empirically, classification models exhibit greater robustness toadversarial perturbations compared to generative models, highlighting theimpact of task type on model vulnerability. Moreover, adversarial attacksfrequently target biologically significant genomic regions, suggesting thatthese models effectively capture meaningful sequence features.</description>
      <author>example@mail.com (Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen)</author>
      <guid isPermaLink="false">2505.10983v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a conference paper in ICML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PITA的自监督学习框架，用于解决自回归PDE模型在处理时间依赖数据时的误差累积问题，特别是对于分布外数据。&lt;h4&gt;背景&lt;/h4&gt;自回归PDE模型在处理时间依赖数据方面展现出巨大潜力，但存在由于自回归预测导致的短路问题，导致误差累积。&lt;h4&gt;目的&lt;/h4&gt;提出PITA框架以解决自回归PDE模型在处理分布外数据时的性能问题，提高模型的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;PITA通过将物理信息约束整合到自监督信号中，对每个PDE轨迹在不同时间步发现的物理动力学进行对齐。&lt;h4&gt;主要发现&lt;/h4&gt;PITA不需要依赖已知的物理先验，仅从观测数据中推导对齐，显示出对分布外数据的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;实验表明，PITA显著提高了现有基础模型在多种时间依赖PDE数据上的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自回归偏微分方程（PDE）基础模型在处理时间依赖数据方面显示出巨大潜力。然而，这些模型受到自回归预测中根深蒂固的短路问题的困扰，导致误差累积。对于分布外数据，这一挑战尤其明显，因为预训练性能可能接近随机模型初始化，对于具有长期动态的下游任务。为了解决这个问题，我们提出了物理信息时间对齐（PITA），这是一个受逆问题解决启发的自监督学习框架。具体来说，PITA通过将物理信息约束整合到自监督信号中，对每个给定的PDE轨迹在不同时间步发现的物理动力学进行对齐。对齐是从观测数据中得出的，而不依赖于已知的物理先验，这表明了对分布外数据的强大泛化能力。大量实验表明，PITA显著提高了现有基础模型在多种时间依赖PDE数据上的准确性和鲁棒性。代码可在https://github.com/SCAILab-USTC/PITA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-regressive partial differential equation (PDE) foundation models haveshown great potential in handling time-dependent data. However, these modelssuffer from the shortcut problem deeply rooted in auto-regressive prediction,causing error accumulation. The challenge becomes particularly evident forout-of-distribution data, as the pretraining performance may approach randommodel initialization for downstream tasks with long-term dynamics. To deal withthis problem, we propose physics-informed temporal alignment (PITA), aself-supervised learning framework inspired by inverse problem solving.Specifically, PITA aligns the physical dynamics discovered at different timesteps on each given PDE trajectory by integrating physics-informed constraintsinto the self-supervision signal. The alignment is derived from observationdata without relying on known physics priors, indicating strong generalizationability to the out-of-distribution data. Extensive experiments show that PITAsignificantly enhances the accuracy and robustness of existing foundationmodels on diverse time-dependent PDE data. The code is available athttps://github.com/SCAILab-USTC/PITA.</description>
      <author>example@mail.com (Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen)</author>
      <guid isPermaLink="false">2505.10930v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision</title>
      <link>http://arxiv.org/abs/2505.10875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website and code: https://dktpt44.github.io/LV-GPT/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于空间增强的多模态大型语言模型的方法，旨在帮助盲人和低视力人士更有效地导航和互动。&lt;h4&gt;背景&lt;/h4&gt;盲人和低视力人士在环境中导航和定位物体时面临挑战，因为他们的视觉线索有限。空间推理对于这些个体至关重要，因为它使他们能够理解和解释周围环境中的空间关系。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够辅助盲人和低视力人士感知和互动周围环境的轻量级、易用系统。&lt;h4&gt;方法&lt;/h4&gt;通过微调大型语言模型以包含空间推理能力，并设计了一个眼镜附件作为硬件组件，利用高级视觉语言模型来解释视觉数据并提供实时、空间感知的反馈。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了对环境上下文的理解，并在导航和物体识别方面取得了实质性改进。&lt;h4&gt;结论&lt;/h4&gt;该研究通过VizWiz数据集的深入评估和设计的数据集，证明了该方法在现实世界情况中的有效性和准确性，同时改善了用户体验。&lt;h4&gt;翻译&lt;/h4&gt;摘要：盲人和低视力人士（pBLV）面临重大挑战，由于视觉线索有限，他们在环境中导航和定位物体时很吃力。空间推理对这些个体至关重要，因为它使他们能够理解和解释周围环境中的空间关系，从而增强他们导航和更安全、独立互动的能力。当前针对低视力人士的多模态大型语言（MLLM）模型缺乏有效辅助这些任务所需的空间推理能力。此外，缺乏轻量级、易于使用的系统，使得pBLV能够有效地感知和与其周围环境互动。在本文中，我们提出了一种针对视觉障碍个体的新颖的空间增强多模态大型语言模型方法。通过微调MLLM以包含空间推理能力，我们的方法显著提高了对环境上下文的理解，这对于导航和物体识别至关重要。这一创新还扩展到一个硬件组件，设计为眼镜附件，确保了更高的可访问性和易用性。这种集成利用高级视觉语言模型来解释视觉数据，并向用户提供实时、空间感知的反馈。我们的方法旨在弥合高级机器学习模型与实用、用户友好的辅助设备之间的差距，为视觉障碍用户提供了一种稳健的解决方案，使他们能够更有效地独立导航周围环境。论文包括使用VizWiz数据集的深入评估，证明了准确性和用户体验的实质性改进。此外，我们还设计了一个综合数据集来评估我们方法在现实世界情况中的有效性，证明了准确性和用户体验的实质性改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; People with blindness and low vision (pBLV) face significant challenges,struggling to navigate environments and locate objects due to limited visualcues. Spatial reasoning is crucial for these individuals, as it enables them tounderstand and interpret the spatial relationships in their surroundings,enhancing their ability to navigate and interact more safely and independently.Current multi-modal large language (MLLM) models for low vision people lack thespatial reasoning capabilities needed to effectively assist in these tasks.Moreover, there is a notable absence of lightweight, easy-to-use systems thatallow pBLV to effectively perceive and interact with their surroundingenvironment. In this paper, we propose a novel spatial enhanced multi-modallarge language model based approach for visually impaired individuals. Byfine-tuning the MLLM to incorporate spatial reasoning capabilities, our methodsignificantly improves the understanding of environmental context, which iscritical for navigation and object recognition. The innovation extends to ahardware component, designed as an attachment for glasses, ensuring increasedaccessibility and ease of use. This integration leverages advanced VLMs tointerpret visual data and provide real-time, spatially aware feedback to theuser. Our approach aims to bridge the gap between advanced machine learningmodels and practical, user-friendly assistive devices, offering a robustsolution for visually impaired users to navigate their surroundings moreeffectively and independently. The paper includes an in-depth evaluation usingthe VizWiz dataset, demonstrating substantial improvements in accuracy and userexperience. Additionally, we design a comprehensive dataset to evaluate ourmethod's effectiveness in realworld situations, demonstrating substantialimprovements in accuracy and user experience.</description>
      <author>example@mail.com (Alexey Magay, Dhurba Tripathi, Yu Hao, Yi Fang)</author>
      <guid isPermaLink="false">2505.10875v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers</title>
      <link>http://arxiv.org/abs/2505.10855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合变换器卷积网络（HTN）来改善AI自动分割在放疗计划中的应用，尤其是在面对与训练数据集不同特征的病例时。&lt;h4&gt;背景&lt;/h4&gt;在临床案例中，当应用与训练数据集不同特征的AI自动分割时，其效果可能会变差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同成像对比度和患者位置的混合变换器卷积网络（HTN）来分割心脏亚结构和肺癌、乳腺癌患者的图像。&lt;h4&gt;方法&lt;/h4&gt;使用包含56例增强CT（CECT）和124例非增强CT（NCCT）扫描的患者数据集创建了HTN模型，并在60例Cohort I患者和66例Cohort II患者的验证集上评估了模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;HTN模型在Cohort I和Cohort II数据集上的准确性与公开可用的TotalSegmentator相当，并且在使用一半的训练案例时，其剂量指标与手动划定的结果相似。&lt;h4&gt;结论&lt;/h4&gt;HTN在处理具有不同成像和患者特征的CT图像时表现出稳健的准确性，并且结合了预训练和平衡NCCT和CECT扫描分布的模型能够提供可靠的分割，且所需的标记数据集远少于Oracle模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在放疗计划（RTP）中应用AI自动分割时，若遇到与训练数据集特征不同的临床案例，其效果可能会下降。因此，我们将预训练的变换器改进为混合变换器卷积网络（HTN），以处理具有不同成像对比度和患者位置的胸部亚结构和肺癌、乳腺癌患者的图像。Cohort I包括56例CECT和124例NCCT扫描，来自仰卧位非小细胞肺癌患者，用于创建包含所有180个训练案例和平衡（CECT：32，NCCT：32）HTN模型的Oracle。这些模型在60例Cohort I患者和66例Cohort II患者（仰卧位n=45，俯卧位n=21）的保留验证集上进行了评估。使用DSC、HD95和剂量指标来衡量准确性。公开可用的TotalSegmentator作为基准。Oracle和平衡模型在Cohort I和Cohort II数据集上的准确性相似（DSC Cohort I：0.80 ± 0.10 versus 0.81 ± 0.10；Cohort II：0.77 ± 0.13 versus 0.80 ± 0.12），优于TotalSegmentator。使用一半训练案例的平衡模型在所有心脏亚结构上产生了与手动划定的相似剂量指标。该模型在8个亚结构中的6个对CT对比度具有鲁棒性，在8个亚结构中的5个对患者的扫描位置变化具有鲁棒性，并且准确性与患者大小和年龄的低相关性。HTN从具有不同成像和患者特征的CT图像中稳健准确地分割心脏亚结构，这是临床应用的关键要求。此外，结合预训练和NCCT和CECT扫描平衡分布的模型能够在多种条件下提供可靠的分割，与Oracle模型相比，所需的标记数据集要少得多。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI automated segmentations for radiation treatment planning (RTP) candeteriorate when applied in clinical cases with different characteristics thantraining dataset. Hence, we refined a pretrained transformer into a hybridtransformer convolutional network (HTN) to segment cardiac substructures lungand breast cancer patients acquired with varying imaging contrasts and patientscan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124non-contrast CT (NCCT) scans from patients with non-small cell lung cancersacquired in supine position, was used to create oracle with all 180 trainingcases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models wereevaluated on a held-out validation set of 60 cohort I patients and 66 patientswith breast cancer from cohort II acquired in supine (n=45) and prone (n=21)positions. Accuracy was measured using DSC, HD95, and dose metrics. Publiclyavailable TotalSegmentator served as the benchmark. The oracle and balancedmodels were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperformingTotalSegmentator. The balanced model, using half the training cases as oracle,produced similar dose metrics as manual delineations for all cardiacsubstructures. This model was robust to CT contrast in 6 out of 8 substructuresand patient scan position variations in 5 out of 8 substructures and showed lowcorrelations of accuracy to patient size and age. A HTN demonstrated robustlyaccurate (geometric and dose metrics) cardiac substructures segmentation fromCTs with varying imaging and patient characteristics, one key requirement forclinical use. Moreover, the model combining pretraining with balanceddistribution of NCCT and CECT scans was able to provide reliably accuratesegmentations under varied conditions with far fewer labeled datasets comparedto an oracle model.</description>
      <author>example@mail.com (Aneesh Rangnekar, Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, Abraham Wu, Maria Thor, Andreas Rimner, Harini Veeraraghavan)</author>
      <guid isPermaLink="false">2505.10855v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Foundation model for mass spectrometry proteomics</title>
      <link>http://arxiv.org/abs/2505.10848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的蛋白质组学质谱数据分析方法，通过预训练模型来提高谱图预测任务的性能。&lt;h4&gt;背景&lt;/h4&gt;质谱技术在蛋白质组学领域占主导地位，但数据处理和解释需要复杂的计算方法。机器学习在提高质谱数据分析方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的框架，将多种谱图预测任务集成到一个基础模型中，以改善质谱数据分析。&lt;h4&gt;方法&lt;/h4&gt;使用从头测序作为预训练任务，预训练一个谱图编码器。然后，使用这些预训练的谱图表示来提高谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测等下游任务的表现。最后，进行多任务微调。&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型在谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测等任务上都有所提升，且多任务微调进一步提高了每个任务的表现。&lt;h4&gt;结论&lt;/h4&gt;基于从头测序训练的基础模型能够学习谱图的一般化表示，提高训练数据有限的后处理任务性能，并最终增强蛋白质组学实验中的数据采集和分析。&lt;h4&gt;翻译&lt;/h4&gt;摘要：质谱学是蛋白质组学领域的领先技术，它使得对复杂生物样本中蛋白质含量的高通量分析成为可能。由于仪器复杂性和数据的复杂性，需要复杂的计算方法来处理和解释获得的质谱数据。机器学习在提高质谱数据分析方面展现出巨大的潜力，许多专门为改进数据采集和分析流程中的特定步骤而设计的机器学习方法已经得到广泛应用。在这里，我们提出将各种谱图预测任务统一到一个单一的基础模型中。为此，我们使用从头测序作为预训练任务来预训练一个谱图编码器。然后，我们表明使用这些预训练的谱图表示可以提高我们在谱图质量预测、嵌合体预测、磷酸化预测和糖基化状态预测四个下游任务上的性能。最后，我们进行了多任务微调，并发现这种方法提高了每个任务的表现。总的来说，我们的工作表明，在从头测序上训练的串联质谱蛋白质组学基础模型能够学习谱图的一般化表示，提高训练数据有限的后处理任务性能，并最终增强蛋白质组学实验中的数据采集和分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mass spectrometry is the dominant technology in the field of proteomics,enabling high-throughput analysis of the protein content of complex biologicalsamples. Due to the complexity of the instrumentation and resulting data,sophisticated computational methods are required for the processing andinterpretation of acquired mass spectra. Machine learning has shown greatpromise to improve the analysis of mass spectrometry data, with numerouspurpose-built methods for improving specific steps in the data acquisition andanalysis pipeline reaching widespread adoption. Here, we propose unifyingvarious spectrum prediction tasks under a single foundation model for massspectra. To this end, we pre-train a spectrum encoder using de novo sequencingas a pre-training task. We then show that using these pre-trained spectrumrepresentations improves our performance on the four downstream tasks ofspectrum quality prediction, chimericity prediction, phosphorylationprediction, and glycosylation status prediction. Finally, we perform multi-taskfine-tuning and find that this approach improves the performance on each taskindividually. Overall, our work demonstrates that a foundation model for tandemmass spectrometry proteomics trained on de novo sequencing learns generalizablerepresentations of spectra, improves performance on downstream tasks wheretraining data is limited, and can ultimately enhance data acquisition andanalysis in proteomics experiments.</description>
      <author>example@mail.com (Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble)</author>
      <guid isPermaLink="false">2505.10848v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification</title>
      <link>http://arxiv.org/abs/2505.10823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究评估了通用和医疗领域特定基础模型提取的嵌入在多类别放射学分类中的实用性，特别是在管位评估方面，发现MedImageInsight嵌入与支持向量机适配器结合表现最佳。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大量数据集上预训练，显著推动了机器学习的发展，为各个领域提供了鲁棒且可迁移的嵌入。&lt;h4&gt;目的&lt;/h4&gt;评估通用和医疗领域特定基础模型提取的嵌入在多类别放射学分类中的实用性，特别是在管位评估方面。&lt;h4&gt;方法&lt;/h4&gt;使用包括DenseNet121、BiomedCLIP、Med-Flamingo、MedImageInsight、Rad-DINO和CXR-Foundation在内的六种基础模型提取嵌入，并使用经典机器学习算法训练适配模型。&lt;h4&gt;主要发现&lt;/h4&gt;MedImageInsight嵌入与支持向量机适配器组合的mAUC最高，达到93.8%，其次是Rad-DINO（91.1%）和CXR-Foundation（89.0%）。BiomedCLIP和DenseNet121表现中等，mAUC分别为83.0%和81.8%，而Med-Flamingo表现最低，为75.1%。适配模型计算效率高，CPU上训练仅需一分钟，推理仅需几秒。&lt;h4&gt;结论&lt;/h4&gt;基础模型嵌入，特别是MedImageInsight的嵌入，通过轻量级适配器促进了放射图像分析的准确、计算高效和公平的诊断分类。&lt;h4&gt;翻译&lt;/h4&gt;This study evaluates the utility of embeddings derived from both general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing specifically on tube placement assessment. A dataset comprising 8842 radiographs classified into seven distinct categories was employed to extract embeddings using six foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight, Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using classical machine learning algorithms. Among these combinations, MedImageInsight embeddings paired with an support vector machine adapter yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%, respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%. Notably, most adapter models demonstrated computational efficiency, achieving training within one minute and inference within seconds on CPU, underscoring their practicality for clinical applications. Furthermore, fairness analyses on adapters trained on MedImageInsight-derived embeddings indicated minimal disparities, with gender differences in performance within 2% and standard deviations across age groups not exceeding 3%. These findings confirm that foundation model embeddings-especially those from MedImageInsight-facilitate accurate, computationally efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, pretrained on extensive datasets, have significantlyadvanced machine learning by providing robust and transferable embeddingsapplicable to various domains, including medical imaging diagnostics. Thisstudy evaluates the utility of embeddings derived from both general-purpose andmedical domain-specific foundation models for training lightweight adaptermodels in multi-class radiography classification, focusing specifically on tubeplacement assessment. A dataset comprising 8842 radiographs classified intoseven distinct categories was employed to extract embeddings using sixfoundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained usingclassical machine learning algorithms. Among these combinations,MedImageInsight embeddings paired with an support vector machine adapteryielded the highest mean area under the curve (mAUC) at 93.8%, followed closelyby Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP andDenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.Notably, most adapter models demonstrated computational efficiency, achievingtraining within one minute and inference within seconds on CPU, underscoringtheir practicality for clinical applications. Furthermore, fairness analyses onadapters trained on MedImageInsight-derived embeddings indicated minimaldisparities, with gender differences in performance within 2% and standarddeviations across age groups not exceeding 3%. These findings confirm thatfoundation model embeddings-especially those from MedImageInsight-facilitateaccurate, computationally efficient, and equitable diagnostic classificationusing lightweight adapters for radiographic image analysis.</description>
      <author>example@mail.com (Xue Li, Jameson Merkow, Noel C. F. Codella, Alberto Santamaria-Pang, Naiteek Sangani, Alexander Ersoy, Christopher Burt, John W. Garrett, Richard J. Bruce, Joshua D. Warner, Tyler Bradshaw, Ivan Tarapov, Matthew P. Lungren, Alan B. McMillan)</author>
      <guid isPermaLink="false">2505.10823v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.10781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种完全弱监督的类增量学习语义分割方法，用于仅使用图像级标签学习基础和新增类别的分割。&lt;h4&gt;背景&lt;/h4&gt;传统的类增量语义分割方法需要昂贵的像素级标注进行训练，而部分弱监督方法虽然有所改进，但尚未有完全弱监督的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种完全弱监督的类增量学习语义分割方法，以降低对像素级标注的依赖。&lt;h4&gt;方法&lt;/h4&gt;1. 通过结合定位器和一系列基于不确定性的基础模型生成鲁棒的伪标签。2. 引入示例引导的数据增强方法，生成包含先前和新增类别的多样化图像。3. 在三个常见的实验设置和两种场景下进行实验：15-5 VOC、10-10 VOC、COCO-to-VOC，以及非重叠和重叠。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在15-5 VOC和10-10 VOC设置中，完全弱监督方法优于部分弱监督方法，在COCO-to-VOC设置中也能达到有竞争力的准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的完全弱监督方法在类增量学习语义分割中具有优越性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文针对完全弱监督类增量学习语义分割任务进行研究，旨在仅使用图像级标签学习基础及新增类别的分割。虽然类增量语义分割（CISS）对于处理现实世界中的多样化和新兴物体至关重要，但传统的CISS方法需要昂贵的像素级标注进行训练。为了克服这一限制，最近提出了部分弱监督方法。然而，据我们所知，这是第一个提出完全弱监督CISS方法的工作。为了实现这一目标，我们提出通过结合定位器和基于不确定性的基础模型生成鲁棒的伪标签。此外，为了减轻灾难性遗忘，我们引入了一种示例引导的数据增强方法，该方法在引导下生成包含先前和新增类别的多样化图像。最后，我们在三个常见的实验设置：15-5 VOC、10-10 VOC和COCO-to-VOC，以及两种场景：非重叠和重叠下进行了实验。实验结果表明，我们的完全弱监督方法在15-5 VOC和10-10 VOC设置中优于部分弱监督方法，在COCO-to-VOC设置中也实现了有竞争力的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work addresses the task of completely weakly supervisedclass-incremental learning for semantic segmentation to learn segmentation forboth base and additional novel classes using only image-level labels. Whileclass-incremental semantic segmentation (CISS) is crucial for handling diverseand newly emerging objects in the real world, traditional CISS methods requireexpensive pixel-level annotations for training. To overcome this limitation,partially weakly-supervised approaches have recently been proposed. However, tothe best of our knowledge, this is the first work to introduce a completelyweakly-supervised method for CISS. To achieve this, we propose to generaterobust pseudo-labels by combining pseudo-labels from a localizer and a sequenceof foundation models based on their uncertainty. Moreover, to mitigatecatastrophic forgetting, we introduce an exemplar-guided data augmentationmethod that generates diverse images containing both previous and novel classeswith guidance. Finally, we conduct experiments in three common experimentalsettings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjointand overlap. The experimental results demonstrate that our completely weaklysupervised method outperforms even partially weakly supervised methods in the15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in theCOCO-to-VOC setting.</description>
      <author>example@mail.com (David Minkwan Kim, Soeun Lee, Byeongkeun Kang)</author>
      <guid isPermaLink="false">2505.10781v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>Unifying Segment Anything in Microscopy with Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2505.10769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为uLLSAM的新方法，通过使用多模态大型语言模型（MLLMs）来指导Segment Anything Model（SAM）学习显微镜跨域数据，从而提高生物医学图像分割的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前生物医学图像分割的基金模型在特定数据集上表现良好，但在未见域数据上性能不佳，这归因于分割前缺乏视觉-语言知识。&lt;h4&gt;目的&lt;/h4&gt;通过利用MLLMs注入视觉-语言知识（VLK），使视觉模型在跨域数据集上表现出更强的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Vision-Language Semantic Alignment（VLSA）的模块，该模块将VLK注入SAM中。同时，为了解决边界轮廓感知的不足，进一步提出了Semantic Boundary Regularization（SBR）来指导SAM。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在9个域内显微镜数据集上，Dice和SA的性能分别提高了7.71%和12.10%，达到了最先进的性能。在10个域外数据集上，Dice和SA的性能分别提高了6.79%和10.08%，展现了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;uLLSAM方法通过结合MLLMs和SBR模块，显著提高了生物医学图像分割的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a new method called uLLSAM, which guides the Segment Anything Model (SAM) to learn cross-domain data in microscopy using Multimodal Large Language Models (MLLMs), thereby improving the accuracy of biomedical image segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of regions of interest in biomedical images holdssubstantial value in image analysis. Although several foundation models forbiomedical segmentation have currently achieved excellent performance oncertain datasets, they typically demonstrate sub-optimal performance on unseendomain data. We owe the deficiency to lack of vision-language knowledge beforesegmentation. Multimodal Large Language Models (MLLMs) bring outstandingunderstanding and reasoning capabilities to multimodal tasks, which inspires usto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enablingvision models to demonstrate superior generalization capabilities oncross-domain datasets. In this paper, we propose using MLLMs to guide SAM inlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment(VLSA) module, which injects VLK into Segment Anything Model (SAM). We findthat after SAM receives global VLK prompts, its performance improvessignificantly, but there are deficiencies in boundary contour perception.Therefore, we further propose Semantic Boundary Regularization (SBR) to promptSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%in SA across 9 in-domain microscopy datasets, achieving state-of-the-artperformance. Our method also demonstrates improvements of 6.79% in Dice and10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalizationcapabilities. Code is available at https://github.com/ieellee/uLLSAM.</description>
      <author>example@mail.com (Manyu Li, Ruian He, Zixian Zhang, Weimin Tan, Bo Yan)</author>
      <guid isPermaLink="false">2505.10769v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?</title>
      <link>http://arxiv.org/abs/2505.10714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GeoGrid-Bench，一个用于评估基础模型在网格结构中理解地理空间数据能力的基准。&lt;h4&gt;背景&lt;/h4&gt;地理空间数据集由于其密集的数值、强烈的时空依赖性和独特的多模态表示（包括表格数据、热图和地理可视化）而具有独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;为了评估基础模型如何支持这一领域的科学研究，GeoGrid-Bench包含大规模、真实世界的数据，覆盖150个地点的16个气候变量，并涉及长时间框架。&lt;h4&gt;方法&lt;/h4&gt;基准包括大约3,200个问答对，这些问答对是从8个领域专家精心设计的模板中系统生成的，以反映人类科学家遇到的实际任务。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，视觉语言模型在整体表现上最佳，并提供了对不同基础模型在不同地理空间任务中的优势和局限性的细致分析。&lt;h4&gt;结论&lt;/h4&gt;该基准为如何有效地将基础模型应用于地理空间数据分析以及如何支持科学研究提供了更清晰的见解。&lt;h4&gt;翻译&lt;/h4&gt;我们提出GeoGrid-Bench，这是一个用于评估基础模型在网格结构中理解地理空间数据能力的基准。由于地理空间数据集具有密集的数值、强烈的时空依赖性和独特的多模态表示（包括表格数据、热图和地理可视化），因此它们提出了独特的挑战。为了评估基础模型如何支持这一领域的科学研究，GeoGrid-Bench包含大规模、真实世界的数据，覆盖150个地点的16个气候变量，并涉及长时间框架。基准包括大约3,200个问答对，这些问答对是从8个领域专家精心设计的模板中系统生成的，以反映人类科学家遇到的实际任务。评估结果显示，视觉语言模型在整体表现上最佳，并提供了对不同基础模型在不同地理空间任务中的优势和局限性的细致分析。该基准为如何有效地将基础模型应用于地理空间数据分析以及如何支持科学研究提供了更清晰的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GeoGrid-Bench, a benchmark designed to evaluate the ability offoundation models to understand geo-spatial data in the grid structure.Geo-spatial datasets pose distinct challenges due to their dense numericalvalues, strong spatial and temporal dependencies, and unique multimodalrepresentations including tabular data, heatmaps, and geographicvisualizations. To assess how foundation models can support scientific researchin this domain, GeoGrid-Bench features large-scale, real-world data covering 16climate variables across 150 locations and extended time frames. The benchmarkincludes approximately 3,200 question-answer pairs, systematically generatedfrom 8 domain expert-curated templates to reflect practical tasks encounteredby human scientists. These range from basic queries at a single location andtime to complex spatiotemporal comparisons across regions and periods. Ourevaluation reveals that vision-language models perform best overall, and weprovide a fine-grained analysis of the strengths and limitations of differentfoundation models in different geo-spatial tasks. This benchmark offers clearerinsights into how foundation models can be effectively applied to geo-spatialdata analysis and used to support scientific research.</description>
      <author>example@mail.com (Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick)</author>
      <guid isPermaLink="false">2505.10714v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)</title>
      <link>http://arxiv.org/abs/2505.10640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Foundation Models（FM）如大型语言模型（LLMs）如何通过FMware系统改变软件行业，并详细探讨了FMware的发展现状、挑战、以及克服这些挑战的策略。&lt;h4&gt;背景&lt;/h4&gt;FMware是将FM作为核心组件的系统，LLMs等FM正在重塑软件行业。&lt;h4&gt;目的&lt;/h4&gt;提供对FMware的全面探索，结合挑战目录和实际生产问题。&lt;h4&gt;方法&lt;/h4&gt;讨论了构建FMware的研究和实践状态，分析了选择模型、数据对齐、工程化提示和协调自主代理的困难，并概述了从演示到生产系统的复杂过程，包括系统测试、优化、部署和与旧软件的集成。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了在FMware开发过程中遇到的挑战，如模型选择、数据对齐、提示工程和系统集成等。&lt;h4&gt;结论&lt;/h4&gt;通过工业经验和该领域的研究，提供了克服这些挑战的实用策略和技术路线图。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了如何通过集成大型语言模型等Foundation Models作为核心组件的FMware系统来重塑软件行业。在KDD 2025教程中，我们全面探讨了FMware，结合了精心制作的挑战目录和现实世界的生产问题。我们首先讨论了构建FMware的研究和实践现状，进一步分析了选择合适的模型、对齐高质量的特定领域数据、工程化鲁棒的提示和协调自主代理的困难。然后，我们概述了从令人印象深刻的演示到生产就绪系统的复杂旅程，包括系统测试、优化、部署和与旧软件的集成问题。基于我们在该领域的工业经验和最近的研究，我们提供了克服这些挑战的可行见解和技术路线图。参与者将获得在不断发展的技术环境中创建可信赖的FMware的实用策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models (FMs) such as Large Language Models (LLMs) are reshapingthe software industry by enabling FMware, systems that integrate these FMs ascore components. In this KDD 2025 tutorial, we present a comprehensiveexploration of FMware that combines a curated catalogue of challenges withreal-world production concerns. We first discuss the state of research andpractice in building FMware. We further examine the difficulties in selectingsuitable models, aligning high-quality domain-specific data, engineering robustprompts, and orchestrating autonomous agents. We then address the complexjourney from impressive demos to production-ready systems by outlining issuesin system testing, optimization, deployment, and integration with legacysoftware. Drawing on our industrial experience and recent research in the area,we provide actionable insights and a technology roadmap for overcoming thesechallenges. Attendees will gain practical strategies to enable the creation oftrustworthy FMware in the evolving technology landscape.</description>
      <author>example@mail.com (Kirill Vasilevski, Benjamin Rombaut, Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Keheliya Gallaba, Filipe R. Cogo, Jiahuei, Lin, Dayi Lin, Haoxiang Zhang, Bouyan Chen, Kishanthan Thangarajah, Ahmed E. Hassan, Zhen Ming, Jiang)</author>
      <guid isPermaLink="false">2505.10640v1</guid>
      <pubDate>Mon, 19 May 2025 14:22:06 +0800</pubDate>
    </item>
    <item>
      <title>SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness</title>
      <link>http://arxiv.org/abs/2505.08320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpecSphere是一个双遍历的频谱-空间图神经网络，能够对预测进行验证，适应同质性和异质性的完整谱，并且超越了1-Weisfeiler-Lehman的表达能力，同时保持线性时间复杂度。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络方法在鲁棒性和表达能力方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图神经网络模型SpecSphere，以提高预测的鲁棒性和表达能力。&lt;h4&gt;方法&lt;/h4&gt;SpecSphere结合了切比雪夫多项式频谱分支和注意力门控空间分支，并通过轻量级的MLP在合作-对抗的min-max游戏中训练，融合两者表示。&lt;h4&gt;主要发现&lt;/h4&gt;SpecSphere实现了统一的切比雪夫逼近定理，最小-最大最优风险，闭式鲁棒性证书，以及严格超越1-WL的通用逼近能力。&lt;h4&gt;结论&lt;/h4&gt;SpecSphere在节点分类准确性和认证鲁棒性方面达到了最先进的水平，证明了高表达能力、异质性适应性和可证明的鲁棒性可以共存于单一的可扩展架构中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SpecSphere, the first dual-pass spectral-spatial GNN thatcertifies every prediction against both $\ell\_{0}$ edge flips and$\ell\_{\infty}$ feature perturbations, adapts to the fullhomophily-heterophily spectrum, and surpasses the expressive power of1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples aChebyshev-polynomial spectral branch with an attention-gated spatial branch andfuses their representations through a lightweight MLP trained in acooperative-adversarial min-max game. We further establish (i) a uniformChebyshev approximation theorem, (ii) minimax-optimal risk across thehomophily-heterophily spectrum, (iii) closed-form robustness certificates, and(iv) universal approximation strictly beyond 1-WL. SpecSphere achievesstate-of-the-art node-classification accuracy and delivers tighter certifiedrobustness guarantees on real-world benchmarks. These results demonstrate thathigh expressivity, heterophily adaptation, and provable robustness can coexistwithin a single, scalable architecture.</description>
      <author>example@mail.com (Yoonhyuk Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2505.08320v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
  <item>
      <title>Schreier-Coset Graph Propagation</title>
      <link>http://arxiv.org/abs/2505.10392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 1 figure , preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SCGP是一种基于群理论的图传播方法，通过Schreier-coset嵌入丰富节点特征，提高长距离消息传递的效率，同时保持计算效率，在处理层次化和模块化图结构时表现出优势。&lt;h4&gt;背景&lt;/h4&gt;GNNs在图结构数据学习中表现良好，但易受信息压缩影响，现有解决方案如图重连和Cayley图等方法存在可扩展性瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出SCGP方法以解决GNNs在处理大型图结构时的信息压缩问题，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;SCGP通过Schreier-coset嵌入丰富节点特征，并将无瓶颈的连接模式嵌入到紧凑的特征空间中。&lt;h4&gt;主要发现&lt;/h4&gt;SCGP在节点和图分类基准测试中表现出与扩展开图和重连GNN基线相当甚至更好的性能，尤其适用于处理层次化和模块化图结构，具有较低的计算延迟、可扩展性和内存占用。&lt;h4&gt;结论&lt;/h4&gt;SCGP是一种高效且适用于资源受限应用的图传播方法，特别适合实时和资源受限的环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) offer a principled framework for learning overgraph-structured data, yet their expressive capacity is often hindered byover-squashing, wherein information from distant nodes is compressed intofixed-size vectors. Existing solutions, including graph rewiring andbottleneck-resistant architectures such as Cayley and expander graphs, avoidthis problem but introduce scalability bottlenecks. In particular, the Cayleygraphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoreticalproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memoryusage. To address this, this work introduces Schrier-Coset Graph Propagation(SCGP), a group-theoretic augmentation method that enriches node featuresthrough Schreier-coset embeddings without altering the input graph topology.SCGP embeds bottleneck-free connectivity patterns into a compact feature space,improving long-range message passing while maintaining computationalefficiency. Empirical evaluations across standard node and graph classificationbenchmarks demonstrate that SCGP achieves performance comparable to, orexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibitsparticular advantages in processing hierarchical and modular graph structures,offering reduced inference latency, improved scalability, and a low memoryfootprint, making it suitable for real-time and resource-constrainedapplications.</description>
      <author>example@mail.com (Aryan Mishra, Lizhen Lin)</author>
      <guid isPermaLink="false">2505.10392v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
      <link>http://arxiv.org/abs/2505.10292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StoryReasoning的故事推理系统，用于解决视觉叙事系统在保持角色身份和连接动作到适当主体时遇到的问题，如参照幻觉。该系统通过将角色、物体和其他实体基于视觉元素进行固化来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;视觉叙事系统在保持角色身份和连接动作到适当主体方面存在困难，这导致参照幻觉的发生。&lt;h4&gt;目的&lt;/h4&gt;提出StoryReasoning系统，以解决视觉叙事系统中的角色身份保持和动作连接问题。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含4,178个故事和52,016张电影图像的数据集，每个故事都保持了角色和物体的一致性，并通过结构化的表格表示来显式建模多帧关系。系统采用跨帧物体重新识别、思维链推理和固化方案将文本元素与视觉实体联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调Qwen2.5-VL 7B模型，创建了Qwen Storyteller，它在整个故事中保持了一致的物体引用。与未微调的模型相比，平均每个故事上的幻觉减少了4.06到3.56（-12.3%）。&lt;h4&gt;结论&lt;/h4&gt;StoryReasoning系统有效减少了视觉叙事系统中的参照幻觉，并通过结构化的表格表示和思维链推理提供了对故事的多帧关系建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/daniel3303/storyreasoning&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual storytelling systems struggle to maintain character identity acrossframes and link actions to appropriate subjects, frequently leading toreferential hallucinations. These issues can be addressed through grounding ofcharacters, objects, and other entities on the visual elements. We proposeStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movieimages, with both structured scene analyses and grounded stories. Each storymaintains character and object consistency across frames while explicitlymodeling multi-frame relationships through structured tabular representations.Our approach features cross-frame object re-identification using visualsimilarity and face recognition, chain-of-thought reasoning for explicitnarrative modeling, and a grounding scheme that links textual elements tovisual entities across multiple frames. We establish baseline performance byfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-endobject detection, re-identification, and landmark detection while maintainingconsistent object references throughout the story. Evaluation demonstrates areduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story whencompared to a non-fine-tuned model.</description>
      <author>example@mail.com (Daniel A. P. Oliveira, David Martins de Matos)</author>
      <guid isPermaLink="false">2505.10292v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data</title>
      <link>http://arxiv.org/abs/2505.10083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语言模型和时间序列基础模型的多模态框架，以解决传统预测方法在利用文本信息方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的预测方法依赖于单模态的时间序列数据，这限制了它们利用丰富文本信息的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决事件序列配对数据的稀缺性，提出了一种解耦框架，将语言模型和时序基础模型的优势结合起来。&lt;h4&gt;方法&lt;/h4&gt;使用语言模型将文本事件转换为修订指令，这些指令用于引导时序基础模型。引入了ChronoSteer，这是一种可以通过文本修订指令进行引导的多模态时序基础模型。此外，为了缓解跨模态指令序列配对数据的短缺，设计了一种基于合成数据的两阶段训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoSteer在仅使用合成数据进行训练的情况下，与单模态基线相比，预测精度提高了25.7%，比之前的最先进的多模态方法提高了22.5%。&lt;h4&gt;结论&lt;/h4&gt;该研究有效地解决了信息泄漏问题，并通过多模态模型实现了预测精度的显著提升。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a multimodal framework based on language models and time series foundation models to address the limitations of traditional forecasting methods in utilizing rich textual information.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional forecasting methods rely on unimodal time series data, limitingtheir ability to exploit rich textual information. Recently, large languagemodels (LLMs) and time series foundation models (TSFMs) have demonstratedpowerful capability in textual reasoning and temporal modeling, respectively.Integrating the strengths of both to construct a multimodal model thatconcurrently leverages both temporal and textual information for futureinference has emerged as a critical research challenge. To address the scarcityof event-series paired data, we propose a decoupled framework: an LLM isemployed to transform textual events into revision instructions, which are thenused to steer the output of TSFM. To implement this framework, we introduceChronoSteer, a multimodal TSFM that can be steered through textual revisioninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate theshortage of cross-modal instruction-series paired data, we devise a two-stagetraining strategy based on synthetic data. In addition, we also construct ahigh-quality multimodal time series forecasting benchmark to address theinformation leakage concerns during evaluation. After integrating with an LLM,ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%improvement in prediction accuracy compared to the unimodal backbone and a22.5% gain over the previous state-of-the-art multimodal method.</description>
      <author>example@mail.com (Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao)</author>
      <guid isPermaLink="false">2505.10083v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Context-aware collaborative pushing of heavy objects using skeleton-based intention prediction</title>
      <link>http://arxiv.org/abs/2505.10239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be presented at ICRA 2025 conference. Video:  https://youtu.be/qy7l_wGOyzo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在物理人机交互中，如何通过姿势数据预测人类运动意图，以实现非语言协作物理操作。&lt;h4&gt;背景&lt;/h4&gt;在物理人机交互中，力反馈是最常用的方式来传达人类意图给机器人，但在没有力反馈的情况下，如操作对象没有配备力传感器时，这种方法就不适用。&lt;h4&gt;目的&lt;/h4&gt;研究在摩擦表面上协同推动和拉动重物这一工业环境中常见的任务，通过姿势数据预测人类运动意图。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于有向图神经网络的上下文感知方法，用于分析时空人类姿势数据，以预测非语言协作物理操作的人类运动意图。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，机器人辅助可以显著减少人类努力并提高任务效率。结果表明，结合基于姿势的上下文识别，无论是与力感应结合还是作为力感应的替代方案，都能增强机器人的决策和控制效率。&lt;h4&gt;结论&lt;/h4&gt;基于姿势的上下文识别可以有效地辅助机器人决策和控制，提高协作物理操作的任务效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In physical human-robot interaction, force feedback has been the most commonsensing modality to convey the human intention to the robot. It is widely usedin admittance control to allow the human to direct the robot. However, itcannot be used in scenarios where direct force feedback is not available sincemanipulated objects are not always equipped with a force sensor. In this work,we study one such scenario: the collaborative pushing and pulling of heavyobjects on frictional surfaces, a prevalent task in industrial settings. Whenhumans do it, they communicate through verbal and non-verbal cues, where bodyposes, and movements often convey more than words. We propose a novelcontext-aware approach using Directed Graph Neural Networks to analyzespatio-temporal human posture data to predict human motion intention fornon-verbal collaborative physical manipulation. Our experiments demonstratethat robot assistance significantly reduces human effort and improves taskefficiency. The results indicate that incorporating posture-based contextrecognition, either together with or as an alternative to force sensing,enhances robot decision-making and control efficiency.</description>
      <author>example@mail.com (Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani)</author>
      <guid isPermaLink="false">2505.10239v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.09858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Early accept at MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的计算机辅助手术干预方法，通过合成手术视频来克服数据不平衡问题，提高手术视频数据集的性能。&lt;h4&gt;背景&lt;/h4&gt;手术视频数据集中存在严重的数据不平衡，这阻碍了高性能模型的开发。&lt;h4&gt;目的&lt;/h4&gt;目的是通过合成手术视频来克服手术视频数据集中的数据不平衡。&lt;h4&gt;方法&lt;/h4&gt;提出了一种独特的两阶段、文本条件扩散方法来生成高保真手术视频，用于代表性不足的类别。该方法通过2D潜在扩散模型捕获空间内容，并整合时间注意力层以确保时间一致性。此外，引入了拒绝采样策略来选择最合适的合成样本，有效增加现有数据集以解决类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在手术动作识别和术中事件预测两个下游任务上评估了该方法，结果表明，结合来自该方法合成视频的模型性能显著提高。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效提高手术视频数据集的性能，并通过开源实现促进了技术的普及。&lt;h4&gt;翻译&lt;/h4&gt;摘要：计算机辅助干预可以通过利用手术视频中的时空信息，特别是通过深度学习方法来改善术中引导。然而，手术视频数据集中普遍存在的严重数据不平衡阻碍了高性能模型的发展。在本研究中，我们旨在通过合成手术视频来克服数据不平衡。我们提出了一种独特的两阶段、文本条件扩散方法，用于生成高保真手术视频，以解决代表性不足的类别。我们的方法通过文本提示条件化生成过程，并利用2D潜在扩散模型来解耦空间和时间建模，以捕获空间内容，然后通过整合时间注意力层来确保时间一致性。此外，我们引入了拒绝采样策略来选择最合适的合成样本，有效地增加现有数据集以解决类别不平衡。我们在两个下游任务——手术动作识别和术中事件预测——上评估了我们的方法，表明结合我们的方法合成视频的模型性能显著提高。我们已在https://gitlab.com/nct_tso_public/surgvgen上开源了我们的实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-assisted interventions can improve intra-operative guidance,particularly through deep learning methods that harness the spatiotemporalinformation in surgical videos. However, the severe data imbalance often foundin surgical video datasets hinders the development of high-performing models.In this work, we aim to overcome the data imbalance by synthesizing surgicalvideos. We propose a unique two-stage, text-conditioned diffusion-based methodto generate high-fidelity surgical videos for under-represented classes. Ourapproach conditions the generation process on text prompts and decouplesspatial and temporal modeling by utilizing a 2D latent diffusion model tocapture spatial content and then integrating temporal attention layers toensure temporal consistency. Furthermore, we introduce a rejection samplingstrategy to select the most suitable synthetic samples, effectively augmentingexisting datasets to address class imbalance. We evaluate our method on twodownstream tasks-surgical action recognition and intra-operative eventprediction-demonstrating that incorporating synthetic videos from our approachsubstantially enhances model performance. We open-source our implementation athttps://gitlab.com/nct_tso_public/surgvgen.</description>
      <author>example@mail.com (Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel)</author>
      <guid isPermaLink="false">2505.09858v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2505.09971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为APCoTTA的连续测试时适应（CTTA）方法，专门用于解决空中激光扫描（ALS）点云分割中的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;在现实应用中，由于环境变化、传感器类型或传感器退化等因素导致的域偏移，通常会导致模型性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出APCoTTA旨在解决ALS点云分割中由于域偏移导致的模型性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;APCoTTA通过动态可训练层选择模块、基于熵的保持一致性损失和随机参数插值机制来提高模型的适应性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，APCoTTA在两个基准测试中实现了最佳性能，相对于直接推理，mIoU提高了约9%和14%。&lt;h4&gt;结论&lt;/h4&gt;APCoTTA为ALS点云分割提供了有效的连续测试时适应方法，有助于提高模型在域偏移情况下的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task for large-scale 3D scene understanding. In real-world applications, models are typically fixed after training. However, domain shifts caused by changes in the environment, sensor types, or sensor degradation often lead to a decline in model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by adapting a source-pretrained model to evolving, unlabeled target domains. Despite its potential, research on ALS point clouds remains limited, facing challenges such as the absence of standardized datasets and the risk of catastrophic forgetting and error accumulation during prolonged adaptation. To tackle these challenges, we propose APCoTTA, the first CTTA method tailored for ALS point cloud semantic segmentation. We propose a dynamic trainable layer selection module. This module utilizes gradient information to select low-confidence layers for training, and the remaining layers are kept frozen, mitigating catastrophic forgetting. To further reduce error accumulation, we propose an entropy-based consistency loss. By losing such samples based on entropy, we apply consistency loss only to the reliable samples, enhancing model stability. In addition, we propose a random parameter interpolation mechanism, which randomly blends parameters from the selected trainable layers with those of the source model. This approach helps balance target adaptation and source knowledge retention, further alleviating forgetting. Finally, we construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA benchmarks for ALS point cloud segmentation. Experimental results demonstrate that APCoTTA achieves the best performance on two benchmarks, with mIoU improvements of approximately 9% and 14% over direct inference. The new benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gaoyuan2/apcotta&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Airborne laser scanning (ALS) point cloud segmentation is a fundamental taskfor large-scale 3D scene understanding. In real-world applications, models aretypically fixed after training. However, domain shifts caused by changes in theenvironment, sensor types, or sensor degradation often lead to a decline inmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution byadapting a source-pretrained model to evolving, unlabeled target domains.Despite its potential, research on ALS point clouds remains limited, facingchallenges such as the absence of standardized datasets and the risk ofcatastrophic forgetting and error accumulation during prolonged adaptation. Totackle these challenges, we propose APCoTTA, the first CTTA method tailored forALS point cloud semantic segmentation. We propose a dynamic trainable layerselection module. This module utilizes gradient information to selectlow-confidence layers for training, and the remaining layers are kept frozen,mitigating catastrophic forgetting. To further reduce error accumulation, wepropose an entropy-based consistency loss. By losing such samples based onentropy, we apply consistency loss only to the reliable samples, enhancingmodel stability. In addition, we propose a random parameter interpolationmechanism, which randomly blends parameters from the selected trainable layerswith those of the source model. This approach helps balance target adaptationand source knowledge retention, further alleviating forgetting. Finally, weconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTAbenchmarks for ALS point cloud segmentation. Experimental results demonstratethat APCoTTA achieves the best performance on two benchmarks, with mIoUimprovements of approximately 9% and 14% over direct inference. The newbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.</description>
      <author>example@mail.com (Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang)</author>
      <guid isPermaLink="false">2505.09971v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2505.10040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Instance-Prototype Affinity Learning (IPAL)的Graph Neural Networks (GNN)方法，用于解决非示例连续图学习中的灾难性遗忘问题，并通过实验证明了其有效性。&lt;h4&gt;背景&lt;/h4&gt;GNN在整合新信息时容易发生灾难性遗忘，影响其保存先前知识的能力。传统的重排和原型重放等技术存在内存爆炸和隐私侵犯等问题。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种新的方法来缓解GNN中的灾难性遗忘问题，同时提高模型对新知识的吸收能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Instance-Prototype Affinity Learning (IPAL)的方法，利用图结构信息，并采用拓扑集成高斯原型（TIGP）和实例-原型亲和度蒸馏（IPAD）等技术来增强模型的学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验发现，与传统的原型重放相比，原型对比学习（PCL）表现出更少的漂移现象。IPAL在四个节点分类基准数据集上的评估结果表明，该方法在塑性和稳定性之间取得了更好的平衡，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;IPAL方法能够有效地缓解GNN中的灾难性遗忘问题，并提高了模型在新知识吸收方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Graph Neural Networks (GNN) suffer from catastrophic forgetting, which undermines their ability to preserve previously acquired knowledge when assimilating new information. Rehearsal-based techniques, such as historical example revisiting, are adopted as a principal strategy to alleviate this phenomenon. However, memory explosion and privacy infringements impose significant constraints on their utility. Non-Exemplar methods, such as Prototype Replay (PR), circumvent the prior issues, yet feature drift presents new challenges. In this paper, our empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits less pronounced drift than conventional PR. Drawing upon PCL, we propose Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar Continual Graph Learning (NECGL). Exploiting graph structural information, we formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature distributions towards high-impact nodes to augment the model's capacity for assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD) safeguards task memory by regularizing discontinuities in class relationships. Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL, fostering greater inter-class discriminability. Evaluations on four node classification benchmark datasets demonstrate that our method outperforms existing state-of-the-art methods, achieving a better trade-off between plasticity and stability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) endure catastrophic forgetting, undermining theircapacity to preserve previously acquired knowledge amid the assimilation ofnovel information. Rehearsal-based techniques revisit historical examples,adopted as a principal strategy to alleviate this phenomenon. However, memoryexplosion and privacy infringements impose significant constraints on theirutility. Non-Exemplar methods circumvent the prior issues through PrototypeReplay (PR), yet feature drift presents new challenges. In this paper, ourempirical findings reveal that Prototype Contrastive Learning (PCL) exhibitsless pronounced drift than conventional PR. Drawing upon PCL, we proposeInstance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-ExemplarContinual Graph Learning (NECGL). Exploiting graph structural information, weformulate Topology-Integrated Gaussian Prototypes (TIGP), guiding featuredistributions towards high-impact nodes to augment the model's capacity forassimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)safeguards task memory by regularizing discontinuities in class relationships.Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,fostering greater inter-class discriminability. Evaluations on four nodeclassification benchmark datasets demonstrate that our method outperformsexisting state-of-the-art methods, achieving a better trade-off betweenplasticity and stability.</description>
      <author>example@mail.com (Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong)</author>
      <guid isPermaLink="false">2505.10040v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Vision Tokenizer Tuning</title>
      <link>http://arxiv.org/abs/2505.10562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ETT，一种端到端的视觉标记器调整方法，用于优化视觉标记器和目标自回归任务之间的联合优化。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉标记器优化与下游训练任务分离，假设视觉标记器在各种任务中具有良好的泛化能力，但对于需要不同表示和语义的任务，这种优化方法是不适用的。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉标记器优化方法导致的表示瓶颈问题，即视觉标记器的损失可能成为目标任务的瓶颈。&lt;h4&gt;方法&lt;/h4&gt;ETT利用标记器代码簿的视觉嵌入，以重建和标题目标进行端到端的视觉标记器优化，并能够无缝集成到现有的训练流程中，无需调整大型语言模型的原始代码簿或架构。&lt;h4&gt;主要发现&lt;/h4&gt;ETT实现了显著的性能提升，对于多模态理解和视觉生成任务，与冻结标记器的基线相比，性能提升了2-6%，同时保持了原始的重建能力。&lt;h4&gt;结论&lt;/h4&gt;ETT是一种简单且有效的方法，能够增强多模态基础模型，不仅在图像生成和理解方面，还包括其他领域。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes ETT, an end-to-end vision tokenizer tuning approach that optimizes the joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing vision tokenization isolates the optimization of vision tokenizersfrom downstream training, implicitly assuming the visual tokens can generalizewell across various tasks, e.g., image generation and visual questionanswering. The vision tokenizer optimized for low-level reconstruction isagnostic to downstream tasks requiring varied representations and semantics.This decoupled paradigm introduces a critical misalignment: The loss of thevision tokenization can be the representation bottleneck for target tasks. Forexample, errors in tokenizing text in a given image lead to poor results whenrecognizing or generating them. To address this, we propose ETT, an end-to-endvision tokenizer tuning approach that enables joint optimization between visiontokenization and target autoregressive tasks. Unlike prior autoregressivemodels that use only discrete indices from a frozen vision tokenizer, ETTleverages the visual embeddings of the tokenizer codebook, and optimizes thevision tokenizers end-to-end with both reconstruction and caption objectives.ETT can be seamlessly integrated into existing training pipelines with minimalarchitecture modifications. Our ETT is simple to implement and integrate,without the need to adjust the original codebooks or architectures of theemployed large language models. Extensive experiments demonstrate that ourproposed end-to-end vision tokenizer tuning unlocks significant performancegains, i.e., 2-6% for multimodal understanding and visual generation taskscompared to frozen tokenizer baselines, while preserving the originalreconstruction capability. We hope this very simple and strong method canempower multimodal foundation models besides image generation andunderstanding.</description>
      <author>example@mail.com (Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang)</author>
      <guid isPermaLink="false">2505.10562v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</title>
      <link>http://arxiv.org/abs/2505.10205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VolE的新型框架，用于通过移动设备驱动的3D重建来估计食物体积，以提高医疗营养管理和健康监测应用中的食物体积估计准确性。&lt;h4&gt;背景&lt;/h4&gt;目前的食物体积估计方法通常受到单核数据、专用硬件（如3D扫描仪）或依赖参考物体进行相机校准的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需参考物体和深度信息的食物体积估计框架，以提高估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;VolE利用AR功能的移动设备捕捉图像和相机位置，生成精确的3D模型。此外，通过食物视频分割来生成食物掩码，实现现实世界的测量。&lt;h4&gt;主要发现&lt;/h4&gt;VolE在多个数据集上优于现有的体积估计技术，实现了2.22%的MAPE，证明了其在食物体积估计方面的优越性能。&lt;h4&gt;结论&lt;/h4&gt;VolE框架为食物体积估计提供了一种高效且准确的方法，对于医疗营养管理和健康监测应用具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate food volume estimation is crucial for medical nutrition managementand health monitoring applications, but current food volume estimation methodsare often limited by mononuclear data, leveraging single-purpose hardware suchas 3D scanners, gathering sensor-oriented information such as depthinformation, or relying on camera calibration using a reference object. In thispaper, we present VolE, a novel framework that leverages mobile device-driven3D reconstruction to estimate food volume. VolE captures images and cameralocations in free motion to generate precise 3D models, thanks to AR-capablemobile devices. To achieve real-world measurement, VolE is a reference- anddepth-free framework that leverages food video segmentation for food maskgeneration. We also introduce a new food dataset encompassing the challengingscenarios absent in the previous benchmarks. Our experiments demonstrate thatVolE outperforms the existing volume estimation techniques across multipledatasets by achieving 2.22 % MAPE, highlighting its superior performance infood volume estimation.</description>
      <author>example@mail.com (Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva)</author>
      <guid isPermaLink="false">2505.10205v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2505.10547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://milanganai.github.io/fortress/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为FORTRESS的框架，用于在机器人遇到超出训练数据范围的情况时，实时生成和推理语义安全的回退策略，以防止超出分布（OOD）的故障。&lt;h4&gt;背景&lt;/h4&gt;由于大型视觉和语言模型的推理延迟较高，当前方法依赖于手动定义的干预策略来执行回退，因此缺乏规划通用、语义安全运动的能力。&lt;h4&gt;目的&lt;/h4&gt;克服上述挑战，提出FORTRESS框架，以在运行时实时生成和推理语义安全的回退策略。&lt;h4&gt;方法&lt;/h4&gt;FORTRESS在正常操作中低频使用多模态推理器来识别目标和预测故障模式。当运行时监控器触发回退响应时，FORTRESS快速合成回退到目标状态的计划，同时实时推理并避免语义不安全的区域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将开放世界、多模态推理与动态感知规划相结合，FORTRESS消除了硬编码回退和人工安全干预的需求。在合成基准和真实世界ANYmal机器人数据上的安全分类准确率方面，FORTRESS优于对慢速推理模型的即时提示，并在模拟和四旋翼硬件城市导航中的系统安全性和规划成功率方面进一步得到提升。&lt;h4&gt;结论&lt;/h4&gt;FORTRESS框架能够有效提高机器人在未知环境下的安全性和鲁棒性，为未来机器人安全导航提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models can provide robust high-level reasoning on appropriatesafety interventions in hazardous scenarios beyond a robot's training data,i.e. out-of-distribution (OOD) failures. However, due to the high inferencelatency of Large Vision and Language Models, current methods rely on manuallydefined intervention policies to enact fallbacks, thereby lacking the abilityto plan generalizable, semantically safe motions. To overcome these challengeswe present FORTRESS, a framework that generates and reasons about semanticallysafe fallback strategies in real time to prevent OOD failures. At a lowfrequency in nominal operations, FORTRESS uses multi-modal reasoners toidentify goals and anticipate failure modes. When a runtime monitor triggers afallback response, FORTRESS rapidly synthesizes plans to fallback goals whileinferring and avoiding semantically unsafe regions in real time. By bridgingopen-world, multi-modal reasoning with dynamics-aware planning, we eliminatethe need for hard-coded fallbacks and human safety interventions. FORTRESSoutperforms on-the-fly prompting of slow reasoning models in safetyclassification accuracy on synthetic benchmarks and real-world ANYmal robotdata, and further improves system safety and planning success in simulation andon quadrotor hardware for urban navigation.</description>
      <author>example@mail.com (Milan Ganai, Rohan Sinha, Christopher Agia, Daniel Morton, Marco Pavone)</author>
      <guid isPermaLink="false">2505.10547v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability</title>
      <link>http://arxiv.org/abs/2505.10351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).  We show the impacts of scaling from both data and model aspects on membership  inference for self-supervised visual encoders&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了视觉自监督模型在现实环境下的成员推断问题，提出了统一的方法PartCrop，并通过实验验证了其有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在处理大量未标记数据方面显示出潜力，但在视觉领域也面临着隐私问题。&lt;h4&gt;目的&lt;/h4&gt;在自监督训练方法和细节未知的情况下，对视觉自监督模型进行成员推断。&lt;h4&gt;方法&lt;/h4&gt;提出了PartCrop方法，通过裁剪图像中的部分对象，在表示空间中查询图像内的响应。同时，对不同的训练协议和结构进行了广泛攻击，并评估了防御方法。&lt;h4&gt;主要发现&lt;/h4&gt;PartCrop方法在多种自监督模型上验证了其有效性和泛化能力。防御实验表明，早期停止、差分隐私和缩小裁剪尺度范围等方法是有效的。此外，通过引入结构改进，提出了可扩展的PartCrop-v2。&lt;h4&gt;结论&lt;/h4&gt;PartCrop是一种有效的成员推断方法，可以用于对抗自监督模型，并通过结构改进提高了其可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自监督学习在利用大量未标记数据方面显示出潜力，但也面临着显著的隐私问题，尤其是在视觉领域。在本文中，我们在一个更现实的设置中对视觉自监督模型进行了成员推断：当攻击者以通常实践中遇到的黑盒系统面对时，他不知道自监督训练方法和细节。在这种设置中，考虑到自监督模型可能由完全不同的自监督范例（例如，掩码图像建模和对比学习）以及复杂的训练细节进行训练，我们提出了一种称为PartCrop的统一成员推断方法。它是由模型之间的共享部分感知能力以及训练数据上更强的部分响应所启发。具体来说，PartCrop通过在表示空间中裁剪图像中的部分对象来查询图像内的响应。我们使用三个广泛使用的图像数据集对具有不同训练协议和结构的自监督模型进行了广泛的攻击。结果表明，PartCrop的有效性和泛化能力。此外，为了防御PartCrop，我们评估了两种常见的方法，即早期停止和差分隐私，并提出了一个名为缩小裁剪尺度范围的自定义方法。防御实验表明，它们都是有效的。最后，除了在玩具视觉编码器和小型图像数据集上进行原型测试外，我们还从数据和模型方面对现实场景中缩放的影响进行了定量研究，并通过引入两个结构改进提出了可扩展的PartCrop-v2。我们的代码可在https://github.com/JiePKU/PartCrop上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jiepku/partcrop&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning shows promise in harnessing extensive unlabeleddata, but it also confronts significant privacy concerns, especially in vision.In this paper, we perform membership inference on visual self-supervised modelsin a more realistic setting: self-supervised training method and details areunknown for an adversary when attacking as he usually faces a black-box systemin practice. In this setting, considering that self-supervised model could betrained by completely different self-supervised paradigms, e.g., masked imagemodeling and contrastive learning, with complex training details, we propose aunified membership inference method called PartCrop. It is motivated by theshared part-aware capability among models and stronger part response on thetraining data. Specifically, PartCrop crops parts of objects in an image toquery responses within the image in representation space. We conduct extensiveattacks on self-supervised models with different training protocols andstructures using three widely used image datasets. The results verify theeffectiveness and generalization of PartCrop. Moreover, to defend againstPartCrop, we evaluate two common approaches, i.e., early stop and differentialprivacy, and propose a tailored method called shrinking crop scale range. Thedefense experiments indicate that all of them are effective. Finally, besidesprototype testing on toy visual encoders and small-scale image datasets, wequantitatively study the impacts of scaling from both data and model aspects ina realistic scenario and propose a scalable PartCrop-v2 by introducing twostructural improvements to PartCrop. Our code is athttps://github.com/JiePKU/PartCrop.</description>
      <author>example@mail.com (Jie Zhu, Jirong Zha, Ding Li, Leye Wang)</author>
      <guid isPermaLink="false">2505.10351v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>An AI-driven framework for the prediction of personalised health response to air pollution</title>
      <link>http://arxiv.org/abs/2505.10556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Kermani and Naderi share first authorship. 20 pages, 6 figures and 1  table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过整合可穿戴健身设备的生理数据和实时环境暴露，预测个人对污染的健康反应。&lt;h4&gt;背景&lt;/h4&gt;空气污染对公共健康构成重大威胁，会加剧许多呼吸和心血管疾病。气候变化导致极端天气事件增多，如野火和热浪，这些事件会增加污染水平并加剧污染的影响。&lt;h4&gt;目的&lt;/h4&gt;利用个人传感器的最新进展和人工智能的时间序列预测能力，监测和预测个人的健康结果。&lt;h4&gt;方法&lt;/h4&gt;通过安全且道德的方式收集数据，训练一个基于云的模块化框架中的AI模型，预测个人对污染暴露的健康反应。&lt;h4&gt;主要发现&lt;/h4&gt;AI模型（在这种情况下是一个对抗性自编码器神经网络）能够准确重构时间依赖的健康信号，并捕捉对污染的非线性反应。通过使用个人智能手表的数据进行迁移学习，增加了AI模型的一般化能力，并展示了该方法对现实世界用户生成数据的适应性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过整合生理数据和实时环境暴露，有效地预测了个人对污染的健康反应，为改善个人健康提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Air pollution poses a significant threat to public health, causing orexacerbating many respiratory and cardiovascular diseases. In addition, climatechange is bringing about more extreme weather events such as wildfires andheatwaves, which can increase levels of pollution and worsen the effects ofpollution exposure. Recent advances in personal sensing have transformed thecollection of behavioural and physiological data, leading to the potential fornew improvements in healthcare. We wish to capitalise on this data, alongsidenew capabilities in AI for making time series predictions, in order to monitorand predict health outcomes for an individual. Thus, we present a novelworkflow for predicting personalised health responses to pollution byintegrating physiological data from wearable fitness devices with real-timeenvironmental exposures. The data is collected from various sources in a secureand ethical manner, and is used to train an AI model to predict individualhealth responses to pollution exposure within a cloud-based, modular framework.We demonstrate that the AI model -- an Adversarial Autoencoder neural networkin this case -- accurately reconstructs time-dependent health signals andcaptures nonlinear responses to pollution. Transfer learning is applied usingdata from a personal smartwatch, which increases the generalisation abilitiesof the AI model and illustrates the adaptability of the approach to real-world,user-generated data.</description>
      <author>example@mail.com (Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain)</author>
      <guid isPermaLink="false">2505.10556v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.10105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了EmbodiedMAE，一种用于机器人操作的三维多模态表示方法。&lt;h4&gt;背景&lt;/h4&gt;现有方法在训练数据集和机器人操作任务之间存在显著的领域差距，且缺乏能够有效融合三维信息的模型架构。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些局限性，通过增强DROID数据集，构建DROID-3D作为三维具身视觉研究的有价值补充。&lt;h4&gt;方法&lt;/h4&gt;开发了一种多模态掩码自动编码器EmbodiedMAE，通过随机掩码和跨模态融合，同时学习RGB、深度和点云模态的表示。在DROID-3D上训练，EmbodiedMAE在70个模拟任务和20个真实世界机器人操作任务中，无论是在训练效率还是最终性能上，都优于最先进的视觉基础模型（VFMs）。&lt;h4&gt;主要发现&lt;/h4&gt;EmbodiedMAE在规模和从3D输入中进行有效策略学习方面表现出强大的扩展行为。实验结果确立了EmbodiedMAE作为可靠的三维多模态VFM，尤其在空间感知至关重要的精确桌面操作环境中。&lt;h4&gt;结论&lt;/h4&gt;EmbodiedMAE是具身AI系统中可靠的三维多模态VFM，特别适用于需要精确空间感知的桌面操作场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present EmbodiedMAE, a unified 3D multi-modal representation for robotmanipulation. Current approaches suffer from significant domain gaps betweentraining datasets and robot manipulation tasks, while also lacking modelarchitectures that can effectively incorporate 3D information. To overcomethese limitations, we enhance the DROID dataset with high-quality depth mapsand point clouds, constructing DROID-3D as a valuable supplement for 3Dembodied vision research. Then we develop EmbodiedMAE, a multi-modal maskedautoencoder that simultaneously learns representations across RGB, depth, andpoint cloud modalities through stochastic masking and cross-modal fusion.Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-artvision foundation models (VFMs) in both training efficiency and finalperformance across 70 simulation tasks and 20 real-world robot manipulationtasks on two robot platforms. The model exhibits strong scaling behavior withsize and promotes effective policy learning from 3D inputs. Experimentalresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM forembodied AI systems, particularly in precise tabletop manipulation settingswhere spatial perception is critical.</description>
      <author>example@mail.com (Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao)</author>
      <guid isPermaLink="false">2505.10105v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Correlating Account on Ethereum Mixing Service via Domain-Invariant feature learning</title>
      <link>http://arxiv.org/abs/2505.09892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Cryptocurrency, Ethereum, mixing services, GNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为StealthLink的新框架，用于解决Ethereum混合服务如Tornado Cash的交易不可追踪性对区块链安全和金融监管带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有方法在关联混合账户方面受到限制，因为它们依赖于有限的标记数据和容易受到噪声注释的影响。&lt;h4&gt;目的&lt;/h4&gt;提出StealthLink框架，通过跨任务域不变特征学习解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个MixFusion模块，该模块构建和编码混合子图以捕捉局部交易模式，并引入了一种知识迁移机制，通过对抗差异最小化来对齐不同领域的判别性特征。&lt;h4&gt;主要发现&lt;/h4&gt;StealthLink在真实世界的混合交易数据集上取得了最先进的性能，10次学习场景下的F1分数为96.98%，在处理不平衡数据条件下表现优于传统监督方法。&lt;h4&gt;结论&lt;/h4&gt;该研究建立了区块链取证中跨域知识迁移的第一个系统方法，为在去中心化生态系统中对抗增强隐私的金融犯罪提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：以太坊混合服务如Tornado Cash所促成的交易不可追踪性对区块链安全和金融监管构成了重大挑战。现有关联混合账户的方法由于有限的标记数据和易受噪声注释的影响而受到限制。在本文中，我们提出了StealthLink，一个通过跨任务域不变特征学习解决这些局限性的新颖框架。我们的关键创新在于从区块链异常检测领域迁移知识到数据稀缺的混合交易追踪任务。具体来说，我们设计了一个MixFusion模块，该模块构建和编码混合子图以捕捉局部交易模式，同时引入了一种通过对抗差异最小化在不同领域对齐判别性特征的知识迁移机制。这种双重方法在标签稀缺和分布偏移的情况下实现了鲁棒的特征学习。在真实世界混合交易数据集上的广泛实验表明，StealthLink在10次学习场景中实现了最先进的性能，F1分数达到96.98%。值得注意的是，我们的框架在处理不平衡数据条件下比传统监督方法表现出更优越的泛化能力。这项工作建立了区块链取证中跨域知识迁移的第一个系统方法，为在去中心化生态系统中对抗增强隐私的金融犯罪提供了一个实用解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The untraceability of transactions facilitated by Ethereum mixing serviceslike Tornado Cash poses significant challenges to blockchain security andfinancial regulation. Existing methods for correlating mixing accounts sufferfrom limited labeled data and vulnerability to noisy annotations, whichrestrict their practical applicability. In this paper, we propose StealthLink,a novel framework that addresses these limitations through cross-taskdomain-invariant feature learning. Our key innovation lies in transferringknowledge from the well-studied domain of blockchain anomaly detection to thedata-scarce task of mixing transaction tracing. Specifically, we design aMixFusion module that constructs and encodes mixing subgraphs to capture localtransactional patterns, while introducing a knowledge transfer mechanism thataligns discriminative features across domains through adversarial discrepancyminimization. This dual approach enables robust feature learning under labelscarcity and distribution shifts. Extensive experiments on real-world mixingtransaction datasets demonstrate that StealthLink achieves state-of-the-artperformance, with 96.98\% F1-score in 10-shot learning scenarios. Notably, ourframework shows superior generalization capability in imbalanced dataconditions than conventional supervised methods. This work establishes thefirst systematic approach for cross-domain knowledge transfer in blockchainforensics, providing a practical solution for combating privacy-enhancedfinancial crimes in decentralized ecosystems.</description>
      <author>example@mail.com (Zheng Che, Taoyu Li, Meng Shen, Hanbiao Du, Liehuang Zhu)</author>
      <guid isPermaLink="false">2505.09892v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Negative Metric Learning for Graphs</title>
      <link>http://arxiv.org/abs/2505.10307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的负指标学习（NML）增强的图对比学习（NML-GCL）方法，旨在解决图对比学习中的假阴性问题，并提高下游任务的性能。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）常存在假阴性问题，这会降低下游任务的性能。现有解决假阴性问题的方法通常依赖人类先验知识，导致GCL的结果仍然不理想。&lt;h4&gt;目的&lt;/h4&gt;提出NML-GCL方法，以解决GCL中的假阴性问题，并提高其在下游任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;NML-GCL使用一个可学习的负指标网络（NMN）来构建一个负指标空间，在这个空间中，基于锚节点距离，可以更好地区分假阴性样本和真实负样本。同时，提出了一种联合训练方案，结合双层次优化目标，利用自监督信号迭代优化编码器和负指标网络。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和广泛使用的基准测试的实验，验证了所提出方法的优势。&lt;h4&gt;结论&lt;/h4&gt;NML-GCL方法能够有效解决GCL中的假阴性问题，并显著提高下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) often suffers from false negatives, which degrades the performance on downstream tasks. The existing methods addressing the false negative issue usually rely on human prior knowledge, still leading GCL to suboptimal results. In this paper, we propose a novel Negative Metric Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative Metric Network (NMN) to build a negative metric space, in which false negatives can be distinguished better from true negatives based on their distance to anchor node. To overcome the lack of explicit supervision signals for NML, we propose a joint training scheme with bi-level optimization objective, which implicitly utilizes the self-supervision signals to iteratively optimize the encoder and the negative metric network. The solid theoretical analysis and the extensive experiments conducted on widely used benchmarks verify the superiority of the proposed method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) often suffers from false negatives, whichdegrades the performance on downstream tasks. The existing methods addressingthe false negative issue usually rely on human prior knowledge, still leadingGCL to suboptimal results. In this paper, we propose a novel Negative MetricLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable NegativeMetric Network (NMN) to build a negative metric space, in which false negativescan be distinguished better from true negatives based on their distance toanchor node. To overcome the lack of explicit supervision signals for NML, wepropose a joint training scheme with bi-level optimization objective, whichimplicitly utilizes the self-supervision signals to iteratively optimize theencoder and the negative metric network. The solid theoretical analysis and theextensive experiments conducted on widely used benchmarks verify thesuperiority of the proposed method.</description>
      <author>example@mail.com (Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang)</author>
      <guid isPermaLink="false">2505.10307v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms</title>
      <link>http://arxiv.org/abs/2505.09103v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合空间加权方法和新颖点描述直方图的雷达惯性里程计，用于在恶劣条件下实现自主定位。该方法有效处理稀疏和噪声的雷达测量数据，并通过加权计算模型充分利用多普勒速度，同时结合局部几何特征和雷达散射截面（RCS）特征来提高点云注册性能。&lt;h4&gt;背景&lt;/h4&gt;4D雷达惯性里程计在恶劣条件下的自主定位具有潜力，但处理稀疏和噪声的雷达测量数据仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理稀疏和噪声雷达测量数据的雷达惯性里程计方法，提高点云注册性能。&lt;h4&gt;方法&lt;/h4&gt;1. 采用空间加权方法适应点分布不均；2. 提出新颖的点描述直方图用于困难点注册；3. 提出加权计算模型以充分利用多普勒速度；4. 构建结合局部几何特征和RCS特征的点直方图描述符。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验证明了所提出的VGC-RIO方法在公共和自建数据集上的精度和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的VGC-RIO方法在处理稀疏和噪声的雷达测量数据及提高点云注册性能方面表现出良好的效果。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in 4D radar-inertial odometry have demonstrated promising potential for autonomous localization in adverse conditions. However, effective handling of sparse and noisy radar measurements remains a critical challenge. In this paper, we propose a radar-inertial odometry with a spatial weighting method that adapts to unevenly distributed points and a novel point-description histogram for challenging point registration. To make full use of the Doppler velocity from different spatial sections, we propose a weighting calculation model. To enhance the point cloud registration performance under challenging scenarios, we construct a novel point histogram descriptor that combines local geometric features and radar cross-section (RCS) features. We have also conducted extensive experiments on both public and self-constructed datasets. The results demonstrate the precision and robustness of the proposed VGC-RIO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D radar-inertial odometry have demonstrated promisingpotential for autonomous lo calization in adverse conditions. However,effective handling of sparse and noisy radar measurements remains a criticalchallenge. In this paper, we propose a radar-inertial odometry with a spatialweighting method that adapts to unevenly distributed points and a novelpoint-description histogram for challenging point registration. To make fulluse of the Doppler velocity from different spatial sections, we propose aweighting calculation model. To enhance the point cloud registrationperformance under challenging scenarios, we con struct a novel point histogramdescriptor that combines local geometric features and radar cross-section (RCS)features. We have also conducted extensive experiments on both public andself-constructed datasets. The results demonstrate the precision and robustnessof the proposed VGC-RIO.</description>
      <author>example@mail.com (Jianguang Xiang, Xiaofeng He, Zizhuo Chen, Lilian Zhang, Xincan Luo, Jun Mao)</author>
      <guid isPermaLink="false">2505.09103v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A Representation Learning Approach to Feature Drift Detection in Wireless Networks</title>
      <link>http://arxiv.org/abs/2505.10325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALERT的方法，用于检测特征分布变化并触发模型重新训练，以应对AI模型在无线网络部署中的性能退化问题。&lt;h4&gt;背景&lt;/h4&gt;AI在下一代无线网络中将扮演核心角色，实现无处不在的通信和新服务。然而，在实际部署中，特征分布的变化可能会降低AI模型性能并导致不良行为。&lt;h4&gt;目的&lt;/h4&gt;为了应对未检测到的模型退化，提出了ALERT方法，旨在检测特征分布变化并触发模型重新训练。&lt;h4&gt;方法&lt;/h4&gt;ALERT包括三个组件：表示学习、统计测试和效用评估。表示学习部分采用MLP设计，统计测试部分使用Kolmogorov-Smirnov和Population Stability Index测试，效用评估部分采用新函数。&lt;h4&gt;主要发现&lt;/h4&gt;在两个无线网络用例（无线指纹识别和链路异常检测）中，ALERT方法优于文献中的十种标准漂移检测方法。&lt;h4&gt;结论&lt;/h4&gt;ALERT方法在应对无线网络中的特征分布变化和模型性能退化方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI is foreseen to be a centerpiece in next generation wireless networksenabling enabling ubiquitous communication as well as new services. However, inreal deployment, feature distribution changes may degrade the performance of AImodels and lead to undesired behaviors. To counter for undetected modeldegradation, we propose ALERT; a method that can detect feature distributionchanges and trigger model re-training that works well on two wireless networkuse cases: wireless fingerprinting and link anomaly detection. ALERT includesthree components: representation learning, statistical testing and utilityassessment. We rely on MLP for designing the representation learning component,on Kolmogorov-Smirnov and Population Stability Index tests for designing thestatistical testing and a new function for utility assessment. We show thesuperiority of the proposed method against ten standard drift detection methodsavailable in the literature on two wireless network use cases.</description>
      <author>example@mail.com (Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna)</author>
      <guid isPermaLink="false">2505.10325v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks</title>
      <link>http://arxiv.org/abs/2505.10134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages,16 figures.This work has been submitted to the IEEE for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于基础模型的无线定位解决方案，旨在解决现有数据驱动模型在需要大量标注数据且难以泛化到不同部署场景和无线配置中的问题。&lt;h4&gt;背景&lt;/h4&gt;精确且鲁棒的定位对于5G和6G应用至关重要，如自动驾驶、扩展现实和智能制造。尽管数据驱动方法显示出潜力，但大多数现有模型需要大量标注数据，且难以泛化。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于无线定位的基础模型解决方案，以解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;首先，基于信息瓶颈理论分析不同自监督学习任务如何获取通用和特定任务的语义特征。在此基础上，设计了预训练方法，提出了大型无线定位模型（LWLM）。具体来说，提出了一种自监督学习框架，联合优化三个互补目标：空间频率掩码信道建模（SF-MCM）、域变换不变性（DTI）和位置不变对比学习（PICL）。此外，还设计了轻量级解码器，用于关键下游任务，包括到达时间（ToA）估计、到达角度（AoA）估计、单基站（BS）定位和多基站定位。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LWLM在所有定位任务中均优于基于模型和监督学习的基线。特别是，LWLM在未进行预训练的Transformer模型上实现了26.0%--87.5%的改进，并在标签有限的微调和未见过的BS配置下表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;LWLM作为一种基础模型，在无线定位方面具有巨大潜力，能够有效解决现有模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/guangjinpan/lwlm&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust localization is a critical enabler for emerging 5G and 6Gapplications, including autonomous driving, extended reality (XR), and smartmanufacturing. While data-driven approaches have shown promise, most existingmodels require large amounts of labeled data and struggle to generalize acrossdeployment scenarios and wireless configurations. To address these limitations,we propose a foundation-model-based solution tailored for wirelesslocalization. We first analyze how different self-supervised learning (SSL)tasks acquire general-purpose and task-specific semantic features based oninformation bottleneck (IB) theory. Building on this foundation, we design apretraining methodology for the proposed Large Wireless Localization Model(LWLM). Specifically, we propose an SSL framework that jointly optimizes threecomplementary objectives: (i) spatial-frequency masked channel modeling(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)position-invariant contrastive learning (PICL). These objectives jointlycapture the underlying semantics of wireless channel from multipleperspectives. We further design lightweight decoders for key downstream tasks,including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,single base station (BS) localization, and multiple BS localization.Comprehensive experimental results confirm that LWLM consistently surpassesboth model-based and supervised learning baselines across all localizationtasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformermodels without pretraining, and exhibits strong generalization underlabel-limited fine-tuning and unseen BS configurations, confirming itspotential as a foundation model for wireless localization.</description>
      <author>example@mail.com (Guangjin Pan, Kaixuan Huang, Hui Chen, Shunqing Zhang, Christian Häger, Henk Wymeersch)</author>
      <guid isPermaLink="false">2505.10134v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation</title>
      <link>http://arxiv.org/abs/2505.10413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LongRefiner的插件式精炼器，用于解决现实世界RAG应用中长文本输入场景下的冗余信息和噪声问题，提高了推理效率和性能。&lt;h4&gt;背景&lt;/h4&gt;现实世界的RAG应用常常面临长文本输入的情况，这会导致更高的推理成本和性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出LongRefiner以解决长文本输入带来的挑战，提高RAG应用的性能。&lt;h4&gt;方法&lt;/h4&gt;LongRefiner通过利用长文档的结构特性，采用双层查询分析、层次化文档结构和基于单一基础模型的多任务学习进行自适应精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在七个QAdatasets上的实验表明，LongRefiner在各种场景下都达到了有竞争力的性能，同时相比最佳基线，其计算成本和延迟降低了10倍。&lt;h4&gt;结论&lt;/h4&gt;LongRefiner具有可扩展性、效率和有效性，为现实世界的长文本RAG应用提供了实用的见解。&lt;h4&gt;翻译&lt;/h4&gt;In real-world RAG applications, LongRefiner, an efficient plug-and-play refiner, is proposed to address the challenges of long-context input scenarios with redundant information and noise, improving inference efficiency and performance. The method utilizes the inherent structural characteristics of long documents, employing dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QAdatasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ignorejjj/longrefiner&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world RAG applications often encounter long-context input scenarios,where redundant information and noise results in higher inference costs andreduced performance. To address these challenges, we propose LongRefiner, anefficient plug-and-play refiner that leverages the inherent structuralcharacteristics of long documents. LongRefiner employs dual-level queryanalysis, hierarchical document structuring, and adaptive refinement throughmulti-task learning on a single foundation model. Experiments on seven QAdatasets demonstrate that LongRefiner achieves competitive performance invarious scenarios while using 10x fewer computational costs and latencycompared to the best baseline. Further analysis validates that LongRefiner isscalable, efficient, and effective, providing practical insights for real-worldlong-text RAG applications. Our code is available athttps://github.com/ignorejjj/LongRefiner.</description>
      <author>example@mail.com (Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou)</author>
      <guid isPermaLink="false">2505.10413v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Logos as a Well-Tempered Pre-train for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2505.10481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了孤立手语识别（ISLR）任务的两个方面，并提出了一种新的俄罗斯手语（RSL）数据集Logos，用于解决数据量和标签模糊性问题。&lt;h4&gt;背景&lt;/h4&gt;尽管存在多个数据集，但大多数孤立手语的数据量有限，这给跨语言ISLR模型训练和迁移学习带来了挑战。同时，相似的手势可能具有不同的语义意义，导致数据集标签模糊。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，本文提出了Logos数据集，旨在提高手语识别模型的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;本研究提出了Logos数据集，该数据集是RSL数据集中最大且词汇量最丰富的数据集。通过在Logos数据集上预训练模型，并将其作为通用编码器应用于其他语言的手语识别任务，包括少样本学习。此外，还探索了跨语言迁移学习方法，并发现使用多个分类头进行联合训练有助于提高低资源数据集的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;Logos数据集的关键特征是明确标注了视觉上相似的手势组。研究表明，明确标注视觉上相似的手势可以提升训练模型作为视觉编码器在下游任务中的质量。&lt;h4&gt;结论&lt;/h4&gt;基于提出的贡献，本文在WLASL数据集上优于现有最佳结果，在AUTSL数据集上取得了具有竞争力的结果，且仅使用单流模型处理RGB视频。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了孤立手语识别（ISLR）任务的两个方面。首先，尽管存在大量数据集，但大多数孤立手语的数据量仍然有限，这给跨语言ISLR模型训练和迁移学习带来了挑战。其次，相似的手势可能具有不同的语义意义，这导致数据集标签模糊。为了解决这些问题，本研究提出了Logos，一个全新的俄罗斯手语（RSL）数据集，是按手语者数量划分的最大的ISLR数据集之一，也是规模和词汇量最大的RSL数据集。研究表明，在Logos数据集上预训练的模型可以用作其他语言SLR任务的通用编码器，包括少样本学习。本研究探讨了跨语言迁移学习方法，并发现使用多个分类头进行联合训练对提高目标低资源数据集的准确性最为有益。Logos数据集的关键特征是明确标注了视觉上相似的手势组。研究表明，明确标注视觉上相似的手势可以提升训练模型作为视觉编码器在下游任务中的质量。基于提出的贡献，本研究在WLASL数据集上优于现有最佳结果，在AUTSL数据集上取得了具有竞争力的结果，且仅使用单流模型处理RGB视频。源代码、数据集和预训练模型均已公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines two aspects of the isolated sign language recognition(ISLR) task. First, despite the availability of a number of datasets, theamount of data for most individual sign languages is limited. It poses thechallenge of cross-language ISLR model training, including transfer learning.Second, similar signs can have different semantic meanings. It leads toambiguity in dataset labeling and raises the question of the best policy forannotating such signs. To address these issues, this study presents Logos, anovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset bythe number of signers and one of the largest available datasets while also thelargest RSL dataset in size and vocabulary. It is shown that a model,pre-trained on the Logos dataset can be used as a universal encoder for otherlanguage SLR tasks, including few-shot learning. We explore cross-languagetransfer learning approaches and find that joint training using multipleclassification heads benefits accuracy for the target lowresource datasets themost. The key feature of the Logos dataset is explicitly annotated visuallysimilar sign groups. We show that explicitly labeling visually similar signsimproves trained model quality as a visual encoder for downstream tasks. Basedon the proposed contributions, we outperform current state-of-the-art resultsfor the WLASL dataset and get competitive results for the AUTSL dataset, with asingle stream model processing solely RGB video. The source code, dataset, andpre-trained models are publicly available.</description>
      <author>example@mail.com (Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev)</author>
      <guid isPermaLink="false">2505.10481v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>LEMON-Mapping: Loop-Enhanced Large-Scale Multi-Session Point Cloud Merging and Optimization for Globally Consistent Mapping</title>
      <link>http://arxiv.org/abs/2505.10018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Lemon-Mapping的循环增强框架，用于大规模多会话点云地图融合和优化，以解决多机器人协同中的地图构建和定位问题。&lt;h4&gt;背景&lt;/h4&gt;随着机器人技术的快速发展，多机器人协作变得至关重要和具有挑战性。其中一个关键问题是整合来自多个机器人的数据，以构建一个全局一致且精确的地图，从而实现稳健的合作和精确的定位。&lt;h4&gt;目的&lt;/h4&gt;针对传统多机器人位姿图优化（PGO）方法在保持基本全局一致性方面的局限性，本文旨在提出一种新的方法，以解决地图漂移和模糊问题。&lt;h4&gt;方法&lt;/h4&gt;Lemon-Mapping框架通过以下三个关键创新来实现目标：开发了一种鲁棒的循环处理机制和新的循环召回策略；引入了一种用于多机器人地图的空间光束调整方法；设计了一种利用光束调整精化约束的PGO策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与传统的地图融合方法相比，该方法在地图精度和减少地图漂移方面具有优势，并且具有处理大量机器人场景的强大能力。&lt;h4&gt;结论&lt;/h4&gt;Lemon-Mapping框架通过合理利用循环闭合并提高地图的几何质量，为多机器人地图构建和定位提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of robotics, multi-robot collaboration has become critical and challenging. One key problem is integrating data from multiple robots to build a globally consistent and accurate map for robust cooperation and precise localization. While traditional multi-robot pose graph optimization (PGO) maintains basic global consistency, it focuses primarily on pose optimization and ignores the geometric structure of the map. Moreover, PGO only uses loop closure as a constraint between two nodes, failing to fully exploit its capability to maintaining local consistency of multi-robot maps. Therefore, PGO-based multi-robot mapping methods often suffer from serious map divergence and blur, especially in regions with overlapping submaps. To address this issue, we propose Lemon-Mapping, a loop-enhanced framework for large-scale multi-session point cloud map fusion and optimization, which reasonably utilizes loop closure and improves the geometric quality of the map. We re-examine the role of loops for multi-robot mapping and introduce three key innovations. First, we develop a robust loop processing mechanism that effectively rejects outliers and a novel loop recall strategy to recover mistakenly removed loops. Second, we introduce a spatial bundle adjustment method for multi-robot maps that significantly reduces the divergence in overlapping regions and eliminates map blur. Third, we design a PGO strategy that leverages the refined constraints of bundle adjustment to extend the local accuracy to the global map. We validate our framework on several public datasets and a self-collected dataset. Experimental results demonstrate that our method outperforms traditional map merging approaches in terms of mapping accuracy and reduction of map divergence. Scalability experiments also demonstrate the strong capability of our framework to handle scenarios involving numerous robots.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of robotics, multi-robot collaboration has becomecritical and challenging. One key problem is integrating data from multiplerobots to build a globally consistent and accurate map for robust cooperationand precise localization. While traditional multi-robot pose graph optimization(PGO) maintains basic global consistency, it focuses primarily on poseoptimization and ignores the geometric structure of the map. Moreover, PGO onlyuses loop closure as a constraint between two nodes, failing to fully exploitits capability to maintaining local consistency of multi-robot maps. Therefore,PGO-based multi-robot mapping methods often suffer from serious map divergenceand blur, especially in regions with overlapping submaps. To address thisissue, we propose Lemon-Mapping, a loop-enhanced framework for large-scalemulti-session point cloud map fusion and optimization, which reasonablyutilizes loop closure and improves the geometric quality of the map. Were-examine the role of loops for multi-robot mapping and introduce three keyinnovations. First, we develop a robust loop processing mechanism thateffectively rejects outliers and a novel loop recall strategy to recovermistakenly removed loops. Second, we introduce a spatial bundle adjustmentmethod for multi-robot maps that significantly reduces the divergence inoverlapping regions and eliminates map blur. Third, we design a PGO strategythat leverages the refined constraints of bundle adjustment to extend the localaccuracy to the global map. We validate our framework on several publicdatasets and a self-collected dataset. Experimental results demonstrate thatour method outperforms traditional map merging approaches in terms of mappingaccuracy and reduction of map divergence. Scalability experiments alsodemonstrate the strong capability of our framework to handle scenariosinvolving numerous robots.</description>
      <author>example@mail.com (Lijie Wang, Xiaoyi Zhong, Ziyi Xu, Kaixin Chai, Anke Zhao, Tianyu Zhao, Qianhao Wang, Fei Gao)</author>
      <guid isPermaLink="false">2505.10018v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2505.10088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Due to the limitation "The abstract field cannot be longer than 1,920  characters", the abstract appearing here is slightly shorter than that in the  PDF file&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多模态表示学习（MMRL）的方法，旨在解决大规模预训练视觉语言模型在少量数据下的过拟合问题，并提高其在新任务上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练视觉语言模型在迁移学习方面取得了显著进展，但在少量数据下进行模型适配时，容易导致过拟合，影响其在新任务上的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出MMRL方法，通过引入一个共享的、可学习的、模态无关的表示空间，以解决过拟合问题并提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MMRL方法通过将空间标记投影到文本和图像编码器中作为表示标记，实现更有效的跨模态交互。同时，该方法在高层编码器中插入表示标记，以优化任务特定特征，同时在低层保留预训练知识。训练过程中，联合优化类别和表示特征，并在推理时采用解耦策略，以适应不同任务。&lt;h4&gt;主要发现&lt;/h4&gt;MMRL和其扩展MMRL++在15个数据集上进行了广泛实验，结果表明，这两种方法在任务特定适应和泛化之间取得了良好的平衡，并显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;MMRL和MMRL++能够有效解决视觉语言模型在少量数据下的过拟合问题，提高模型在新任务上的泛化能力，为视觉语言模型的进一步发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this, we propose Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, modality-agnostic representation space. MMRL generates space tokens projected into both text and image encoders as representation tokens, enabling more effective cross-modal interactions. Unlike prior methods that mainly optimize class token features, MMRL inserts representation tokens into higher encoder layers--where task-specific features are more prominent--while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term aligning class and text features with the frozen VLM's zero-shot features. At inference, a decoupling strategy uses both class and representation features for basetasks, but only class features for novel tasks due to their stronger generalization. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces trainable parameters and enhances intra-modal interactions--particularly across the layers of representation tokens--allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained Vision-Language Models (VLMs) have significantlyadvanced transfer learning across diverse tasks. However, adapting these modelswith limited few-shot data often leads to overfitting, undermining theirability to generalize to new tasks. To address this, we propose Multi-ModalRepresentation Learning (MMRL), which introduces a shared, learnable,modality-agnostic representation space. MMRL generates space tokens projectedinto both text and image encoders as representation tokens, enabling moreeffective cross-modal interactions. Unlike prior methods that mainly optimizeclass token features, MMRL inserts representation tokens into higher encoderlayers--where task-specific features are more prominent--while preservinggeneral knowledge in the lower layers. During training, both class andrepresentation features are jointly optimized: a trainable projection layer isapplied to representation tokens for task adaptation, while the projectionlayer for class token remains frozen to retain pre-trained knowledge. Tofurther promote generalization, we introduce a regularization term aligningclass and text features with the frozen VLM's zero-shot features. At inference,a decoupling strategy uses both class and representation features for basetasks, but only class features for novel tasks due to their strongergeneralization. Building upon this, we propose MMRL++, a parameter-efficientand interaction-aware extension that significantly reduces trainable parametersand enhances intra-modal interactions--particularly across the layers ofrepresentation tokens--allowing gradient sharing and instance-specificinformation to propagate more effectively through the network. Extensiveexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistentlyoutperform state-of-the-art methods, achieving a strong balance betweentask-specific adaptation and generalization.</description>
      <author>example@mail.com (Yuncheng Guo, Xiaodong Gu)</author>
      <guid isPermaLink="false">2505.10088v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Robust Federated Learning on Edge Devices with Domain Heterogeneity</title>
      <link>http://arxiv.org/abs/2505.10128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IWCMC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的框架，用于解决联邦学习在领域异质性下的统计异质性问题，通过原型增强来提高全局模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;联邦学习（FL）在确保数据隐私的同时实现分布式边缘设备的协作训练，适用于隐私敏感的应用。但FL面临统计异质性的挑战，尤其是领域异质性，这阻碍了全局模式的收敛。&lt;h4&gt;目的&lt;/h4&gt;提高联邦学习全局模型在领域异质性下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入了FedAPC（联邦增强原型对比学习）框架，这是一个基于原型的FL框架，旨在增强特征多样性和模型鲁棒性。FedAPC利用增强数据的均值特征原型来捕捉更丰富的表示，并通过将局部特征与全局原型对齐，使模型能够学习有意义的语义特征，同时减少对特定领域的过度拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在Office-10和Digits数据集上的实验结果表明，该框架优于SOTA（最先进技术）基线，展示了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;FedAPC框架有效地解决了联邦学习在领域异质性下的挑战，并提高了模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Federated Learning (FL) 允许在确保数据隐私的条件下进行协作训练，使其成为隐私敏感应用的流行解决方案。然而，FL由于统计异质性的存在，特别是领域异质性，面临着显著的挑战，这阻碍了全局模式的收敛。在本研究中，我们通过使用原型增强来提高FL全局模型在领域异质性下的泛化能力，引入了一种新的框架来应对这一挑战。具体而言，我们引入了FedAPC（Federated Augmented Prototype Contrastive Learning），这是一个基于原型的FL框架，旨在增强特征多样性和模型鲁棒性。FedAPC利用从增强数据的均值特征中派生的原型来捕捉更丰富的表示。通过将局部特征与全局原型对齐，我们使模型能够学习有意义的语义特征，同时减少对任何特定领域的过度拟合。在Office-10和Digits数据集上的实验结果表明，我们的框架优于最先进技术基线，展示了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) allows collaborative training while ensuring dataprivacy across distributed edge devices, making it a popular solution forprivacy-sensitive applications. However, FL faces significant challenges due tostatistical heterogeneity, particularly domain heterogeneity, which impedes theglobal mode's convergence. In this study, we introduce a new framework toaddress this challenge by improving the generalization ability of the FL globalmodel under domain heterogeneity, using prototype augmentation. Specifically,we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), aprototype-based FL framework designed to enhance feature diversity and modelrobustness. FedAPC leverages prototypes derived from the mean features ofaugmented data to capture richer representations. By aligning local featureswith global prototypes, we enable the model to learn meaningful semanticfeatures while reducing overfitting to any specific domain. Experimentalresults on the Office-10 and Digits datasets illustrate that our frameworkoutperforms SOTA baselines, demonstrating superior performance.</description>
      <author>example@mail.com (Huy Q. Le, Latif U. Khan, Choong Seon Hong)</author>
      <guid isPermaLink="false">2505.10128v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&amp;E Images using ViT Foundation Models</title>
      <link>http://arxiv.org/abs/2505.10294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MIPHEI的新方法，通过结合H&amp;E染色图像和先进的ViT基础模型，实现了从H&amp;E图像预测多标记免疫荧光（mIF）信号，从而在癌症诊断中提高细胞类型识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;组织病理学分析是癌症诊断的基础，H&amp;E染色被广泛用于观察细胞形态和组织结构。尽管多标记免疫荧光（mIF）可以更精确地识别细胞类型，但由于成本和物流限制，尚未得到广泛应用。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本研究旨在开发一种方法，能够从H&amp;E图像中预测mIF信号，以实现更精确的细胞类型识别。&lt;h4&gt;方法&lt;/h4&gt;MIPHEI模型基于U-Net架构，并集成了最先进的ViT基础模型作为编码器。该模型针对一系列标记物进行训练，包括核内容、免疫谱系（T细胞、B细胞、髓细胞）、上皮、基质、血管和增殖。研究使用公开的ORION数据集进行训练，并在两个独立数据集上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;MIPHEI在仅使用H&amp;E图像的情况下实现了准确的细胞类型分类，F1分数显著优于现有的基线和随机分类器。&lt;h4&gt;结论&lt;/h4&gt;MIPHEI模型能够有效地捕捉组织背景中核形态的复杂关系，为细胞类型感知的大规模H&amp;E数据集分析提供了有希望的步骤，有助于揭示空间细胞组织与患者结果之间的关系。&lt;h4&gt;翻译&lt;/h4&gt;摘要：组织病理学分析是癌症诊断的基石，Hematoxylin和Eosin（H&amp;E）染色通常用于每位患者以可视化细胞形态和组织结构。另一方面，多重免疫荧光（mIF）能够通过蛋白质组学标记实现更精确的细胞类型识别，但由于成本和物流限制，尚未得到广泛应用。为了弥合这一差距，我们介绍了一种名为MIPHEI（从H&amp;E图像预测多重免疫荧光）的方法，该方法基于U-Net架构，并集成了最先进的ViT基础模型作为编码器，以从H&amp;E图像预测mIF信号。MIPHEI针对一系列标记物进行训练，包括核内容、免疫谱系（T细胞、B细胞、髓细胞）、上皮、基质、血管和增殖。我们使用公开的ORION数据集进行训练，并在两个独立数据集上进行验证。MIPHEI在仅使用H&amp;E图像的情况下实现了准确的细胞类型分类，F1分数显著优于现有的基线和随机分类器。我们的结果表明，我们的模型能够有效地捕捉组织背景中核形态的复杂关系，为细胞类型感知的大规模H&amp;E数据集分析提供了有希望的步骤，有助于揭示空间细胞组织与患者结果之间的关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sanofi-public/miphei-vit&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathological analysis is a cornerstone of cancer diagnosis, withHematoxylin and Eosin (H&amp;E) staining routinely acquired for every patient tovisualize cell morphology and tissue architecture. On the other hand, multipleximmunofluorescence (mIF) enables more precise cell type identification viaproteomic markers, but has yet to achieve widespread clinical adoption due tocost and logistical constraints. To bridge this gap, we introduce MIPHEI(Multiplex Immunofluorescence Prediction from H&amp;E), a U-Net-inspiredarchitecture that integrates state-of-the-art ViT foundation models as encodersto predict mIF signals from H&amp;E images. MIPHEI targets a comprehensive panel ofmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),epithelium, stroma, vasculature, and proliferation. We train our model usingthe publicly available ORION dataset of restained H&amp;E and mIF images fromcolorectal cancer tissue, and validate it on two independent datasets. MIPHEIachieves accurate cell-type classification from H&amp;E alone, with F1 scores of0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,substantially outperforming both a state-of-the-art baseline and a randomclassifier for most markers. Our results indicate that our model effectivelycaptures the complex relationships between nuclear morphologies in their tissuecontext, as visible in H&amp;E images and molecular markers defining specific celltypes. MIPHEI offers a promising step toward enabling cell-type-aware analysisof large-scale H&amp;E datasets, in view of uncovering relationships betweenspatial cellular organization and patient outcomes.</description>
      <author>example@mail.com (Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter)</author>
      <guid isPermaLink="false">2505.10294v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>An Introduction to Discrete Variational Autoencoders</title>
      <link>http://arxiv.org/abs/2505.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tutorial paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于变分自编码器（VAEs）的概率无监督学习方法，并探讨了离散潜在空间在数据模态（如文本）中的应用。&lt;h4&gt;背景&lt;/h4&gt;VAEs作为一种基于神经网络的概率无监督学习方法，通常通过编码器网络定义高斯分布的潜在空间，并通过解码器网络进行数据重建。&lt;h4&gt;目的&lt;/h4&gt;提供对离散变分自编码器的严谨而实用的介绍，特别是那些潜在空间由具有分类分布的潜在变量组成的VAEs。&lt;h4&gt;方法&lt;/h4&gt;本文假设读者具有基本的数学背景，并从第一原理出发详细推导每个步骤。然后，开发了一个具体的训练方案，并提供了一个示例实现。&lt;h4&gt;主要发现&lt;/h4&gt;离散潜在空间在近年来越来越受欢迎，可能成为许多数据模态的自然选择。&lt;h4&gt;结论&lt;/h4&gt;通过提供详细的推导和示例实现，本文为理解和应用离散变分自编码器提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;Variational Autoencoders (VAEs) 作为一种基于神经网络的概率无监督学习方法已经得到广泛认可。通常，编码器网络定义了高斯分布的潜在空间，我们可以从中采样并将实现传递给解码器网络。该模型被训练以重建其输入，并通过证据下界进行优化。近年来，离散潜在空间越来越受欢迎，表明它们可能是许多数据模态（例如文本）的自然选择。在本教程中，我们提供了一个严谨而实用的离散变分自编码器介绍——具体来说，是那些潜在空间由遵循分类分布的潜在变量组成的VAEs。我们假设读者具有基本的数学背景，并从第一原理出发仔细推导每个步骤。从那里，我们开发了一个具体的训练方案，并提供了一个示例实现，可在https://github.com/alanjeffares/discreteVAE上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/alanjeffares/discretevae&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational Autoencoders (VAEs) are well-established as a principled approachto probabilistic unsupervised learning with neural networks. Typically, anencoder network defines the parameters of a Gaussian distributed latent spacefrom which we can sample and pass realizations to a decoder network. This modelis trained to reconstruct its inputs and is optimized through the evidencelower bound. In recent years, discrete latent spaces have grown in popularity,suggesting that they may be a natural choice for many data modalities (e.g.text). In this tutorial, we provide a rigorous, yet practical, introduction todiscrete variational autoencoders -- specifically, VAEs in which the latentspace is made up of latent variables that follow a categorical distribution. Weassume only a basic mathematical background with which we carefully derive eachstep from first principles. From there, we develop a concrete training recipeand provide an example implementation, hosted athttps://github.com/alanjeffares/discreteVAE.</description>
      <author>example@mail.com (Alan Jeffares, Liyuan Liu)</author>
      <guid isPermaLink="false">2505.10344v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection</title>
      <link>http://arxiv.org/abs/2505.09848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用放射基因组数据，包括结构MRI图像和基因表达数据，进行阿尔茨海默病（AD）检测的新方法。&lt;h4&gt;背景&lt;/h4&gt;成像和基因组数据提供了独特且丰富的特征，它们的一体化可以揭示对疾病复杂景观的新见解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效分类阿尔茨海默病（AD）为三个不同阶段（AD、轻度认知障碍（MCI）和认知正常（CN））的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的异构二分图表示学习方法，具有两种不同的节点类型：基因和图像。该方法利用一个小数据集对AD进行分类，并识别在分类组中起重要作用的基因。&lt;h4&gt;主要发现&lt;/h4&gt;该网络可以有效地将阿尔茨海默病（AD）分类为三个不同阶段，并确定了每个分类组中起重要作用的基因。使用分类准确率、召回率、精确率和F1分数等指标评估了该方法的表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的技术在扩展到基于放射基因组学的疾病分类方面具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imaging and genomic data offer distinct and rich features, and theirintegration can unveil new insights into the complex landscape of diseases. Inthis study, we present a novel approach utilizing radiogenomic data includingstructural MRI images and gene expression data, for Alzheimer's diseasedetection. Our framework introduces a novel heterogeneous bipartite graphrepresentation learning featuring two distinct node types: genes and images.The network can effectively classify Alzheimer's disease (AD) into threedistinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)classes, utilizing a small dataset. Additionally, it identified which genesplay a significant role in each of these classification groups. We evaluate theperformance of our approach using metrics including classification accuracy,recall, precision, and F1 score. The proposed technique holds potential forextending to radiogenomic-based classification to other diseases.</description>
      <author>example@mail.com (Aditya Raj, Golrokh Mirzaei)</author>
      <guid isPermaLink="false">2505.09848v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback</title>
      <link>http://arxiv.org/abs/2505.09925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种交互式持续学习范式，该范式使AI模型能够从实时人类反馈中动态学习新技能，同时保留先前知识。&lt;h4&gt;背景&lt;/h4&gt;本文针对传统持续学习的两个主要局限性进行研究：一是使用流式、实时人工标注数据动态更新模型，而不是使用具有固定标签的静态数据集；二是处理真实世界中常见的噪声反馈，不假设标签的纯净性。&lt;h4&gt;目的&lt;/h4&gt;提出RiCL，一个利用大型语言模型（LLMs）从动态反馈中有效学习新技能的强化交互式持续学习框架。&lt;h4&gt;方法&lt;/h4&gt;RiCL包含三个关键组件：一个时间一致性感知的净化器，用于自动识别数据流中的纯净样本和噪声样本；一个交互感知的直接偏好优化策略，通过协调AI生成和人类提供的反馈来使模型行为与人类意图一致；以及一个噪声鲁棒的对比学习模块，通过利用数据关系来捕获稳健的表示，从而避免依赖于可能不可靠的标签。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集（FewRel和TACRED）上的大量实验表明，与现有的最先进在线持续学习和噪声标签学习方法相比，RiCL方法有显著的优势。&lt;h4&gt;结论&lt;/h4&gt;RiCL方法在处理动态反馈和噪声标签方面表现出色，为持续学习领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an interactive continual learning paradigm where AImodels dynamically learn new skills from real-time human feedback whileretaining prior knowledge. This paradigm distinctively addresses two majorlimitations of traditional continual learning: (1) dynamic model updates usingstreaming, real-time human-annotated data, rather than static datasets withfixed labels, and (2) the assumption of clean labels, by explicitly handlingthe noisy feedback common in real-world interactions. To tackle these problems,we propose RiCL, a Reinforced interactive Continual Learning frameworkleveraging Large Language Models (LLMs) to learn new skills effectively fromdynamic feedback. RiCL incorporates three key components: a temporalconsistency-aware purifier to automatically discern clean from noisy samples indata streams; an interaction-aware direct preference optimization strategy toalign model behavior with human intent by reconciling AI-generated andhuman-provided feedback; and a noise-resistant contrastive learning module thatcaptures robust representations by exploiting inherent data relationships, thusavoiding reliance on potentially unreliable labels. Extensive experiments ontwo benchmark datasets (FewRel and TACRED), contaminated with realistic noisepatterns, demonstrate that our RiCL approach substantially outperforms existingcombinations of state-of-the-art online continual learning and noisy-labellearning methods.</description>
      <author>example@mail.com (Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He)</author>
      <guid isPermaLink="false">2505.09925v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Towards Safe Robot Foundation Models Using Inductive Biases</title>
      <link>http://arxiv.org/abs/2505.10219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了机器人安全在现实世界部署中的重要性，并提出了一种结合机器人基础模型和几何归纳偏差的方法，通过ATACOM安全层来确保安全状态转换。&lt;h4&gt;背景&lt;/h4&gt;尽管当前机器人基础模型在多种任务中展现出良好的泛化能力，但它们未能解决安全问题，这对于确保长期运行至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决机器人基础模型在安全性方面的不足，提供一种确保安全行为的方法，无需大量的安全行为演示和特定的微调。&lt;h4&gt;方法&lt;/h4&gt;结合机器人基础模型与几何归纳偏差，使用ATACOM安全层强制执行动作约束，以确保安全状态转换。&lt;h4&gt;主要发现&lt;/h4&gt;该研究方法可以应用于经典操作任务，避免与无关对象的意外碰撞，同时也能应用于动态任务，如机器人曲棍球环境，生成遵守复杂任务和关节空间约束的快速轨迹。&lt;h4&gt;结论&lt;/h4&gt;通过ATACOM安全层，可以在不提供大量安全行为演示和不进行特定安全微调的情况下，为通用策略提供形式化的安全保证。&lt;h4&gt;翻译&lt;/h4&gt;Safety is a critical requirement for the real-world deployment of roboticsystems. Unfortunately, while current robot foundation models show promisinggeneralization capabilities across a wide variety of tasks, they fail toaddress safety, an important aspect for ensuring long-term operation. Currentrobot foundation models assume that safe behavior should emerge by learningfrom a sufficiently large dataset of demonstrations. However, this approach hastwo clear major drawbacks. Firstly, there are no formal safety guarantees for abehavior cloning policy trained using supervised learning. Secondly, withoutexplicit knowledge of any safety constraints, the policy may require anunreasonable number of additional demonstrations to even approximate thedesired constrained behavior. To solve these key issues, we show how we caninstead combine robot foundation models with geometric inductive biases usingATACOM, a safety layer placed after the foundation policy that ensures safestate transitions by enforcing action constraints. With this approach, we canensure formal safety guarantees for generalist policies without providingextensive demonstrations of safe behavior, and without requiring any specificfine-tuning for safety. Our experiments show that our approach can bebeneficial both for classical manipulation tasks, where we avoid unwantedcollisions with irrelevant objects, and for dynamic tasks, such as the robotair hockey environment, where we can generate fast trajectories respectingcomplex tasks and joint space constraints.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety is a critical requirement for the real-world deployment of roboticsystems. Unfortunately, while current robot foundation models show promisinggeneralization capabilities across a wide variety of tasks, they fail toaddress safety, an important aspect for ensuring long-term operation. Currentrobot foundation models assume that safe behavior should emerge by learningfrom a sufficiently large dataset of demonstrations. However, this approach hastwo clear major drawbacks. Firstly, there are no formal safety guarantees for abehavior cloning policy trained using supervised learning. Secondly, withoutexplicit knowledge of any safety constraints, the policy may require anunreasonable number of additional demonstrations to even approximate thedesired constrained behavior. To solve these key issues, we show how we caninstead combine robot foundation models with geometric inductive biases usingATACOM, a safety layer placed after the foundation policy that ensures safestate transitions by enforcing action constraints. With this approach, we canensure formal safety guarantees for generalist policies without providingextensive demonstrations of safe behavior, and without requiring any specificfine-tuning for safety. Our experiments show that our approach can bebeneficial both for classical manipulation tasks, where we avoid unwantedcollisions with irrelevant objects, and for dynamic tasks, such as the robotair hockey environment, where we can generate fast trajectories respectingcomplex tasks and joint space constraints.</description>
      <author>example@mail.com (Maximilian Tölle, Theo Gruner, Daniel Palenicek, Tim Schneider, Jonas Günster, Joe Watson, Davide Tateo, Puze Liu, Jan Peters)</author>
      <guid isPermaLink="false">2505.10219v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Radar Point Cloud Enhancement via Arbitrary LiDAR Guided Diffusion Prior</title>
      <link>http://arxiv.org/abs/2505.09887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无监督的雷达点增强算法，通过使用任意激光雷达引导的扩散模型作为先验，无需配对训练数据即可提高雷达分辨率。&lt;h4&gt;背景&lt;/h4&gt;雷达在工业自动化中的机器感知中起着关键作用，但其角分辨率受限于瑞利准则，该准则依赖于雷达的工作波长和天线阵列的有效孔径。&lt;h4&gt;目的&lt;/h4&gt;克服硬件限制，提高雷达分辨率，而不依赖于配对的高分辨率地面实况数据。&lt;h4&gt;方法&lt;/h4&gt;将雷达角度估计恢复作为逆问题，并通过扩散模型结合任意激光雷达领域的先验知识。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在保持高保真度和低噪声性能方面优于传统的正则化技术，并且与配对训练方法相比，不仅达到了可比的性能，而且具有更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法通过整合扩散模型中的先验知识来增强雷达点输出，而不是依赖于配对训练数据，是目前首个采用此方法的研究。&lt;h4&gt;翻译&lt;/h4&gt;In industrial automation, radar is a critical sensor in machine perception. However, the angular resolution of radar is inherently limited by the Rayleigh criterion, which depends on both the radar's operating wavelength and the effective aperture of its antenna array. To overcome these hardware-imposed limitations, recent neural network-based methods have leveraged high-resolution LiDAR data, paired with radar measurements, during training to enhance radar point cloud resolution. While effective, these approaches require extensive paired datasets, which are costly to acquire and prone to calibration error. These challenges motivate the need for methods that can improve radar resolution without relying on paired high-resolution ground-truth data. Here, we introduce an unsupervised radar points enhancement algorithm that employs an arbitrary LiDAR-guided diffusion model as a prior without the need for paired training data. Specifically, our approach formulates radar angle estimation recovery as an inverse problem and incorporates prior knowledge through a diffusion model with arbitrary LiDAR domain knowledge. Experimental results demonstrate that our method attains high fidelity and low noise performance compared to traditional regularization techniques. Additionally, compared to paired training methods, it not only achieves comparable performance but also offers improved generalization capability. To our knowledge, this is the first approach that enhances radar points output by integrating prior knowledge via a diffusion model rather than relying on paired training data. Our code is available at https://github.com/yyxr75/RadarINV.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In industrial automation, radar is a critical sensor in machine perception.However, the angular resolution of radar is inherently limited by the Rayleighcriterion, which depends on both the radar's operating wavelength and theeffective aperture of its antenna array.To overcome these hardware-imposedlimitations, recent neural network-based methods have leveraged high-resolutionLiDAR data, paired with radar measurements, during training to enhance radarpoint cloud resolution. While effective, these approaches require extensivepaired datasets, which are costly to acquire and prone to calibration error.These challenges motivate the need for methods that can improve radarresolution without relying on paired high-resolution ground-truth data. Here,we introduce an unsupervised radar points enhancement algorithm that employs anarbitrary LiDAR-guided diffusion model as a prior without the need for pairedtraining data. Specifically, our approach formulates radar angle estimationrecovery as an inverse problem and incorporates prior knowledge through adiffusion model with arbitrary LiDAR domain knowledge. Experimental resultsdemonstrate that our method attains high fidelity and low noise performancecompared to traditional regularization techniques. Additionally, compared topaired training methods, it not only achieves comparable performance but alsooffers improved generalization capability. To our knowledge, this is the firstapproach that enhances radar points output by integrating prior knowledge via adiffusion model rather than relying on paired training data. Our code isavailable at https://github.com/yyxr75/RadarINV.</description>
      <author>example@mail.com (Yanlong Yang, Jianan Liu, Guanxiong Luo, Hao Li, Euijoon Ahn, Mostafa Rahimi Azghadi, Tao Huang)</author>
      <guid isPermaLink="false">2505.09887v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Automated grading and staging of ovarian cancer using deep learning on the transmission optical microscopy bright-field images of thin biopsy tissue samples</title>
      <link>http://arxiv.org/abs/2505.09993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures, 15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;卵巢癌的诊断和管理是一个挑战，预后很大程度上取决于发现时的阶段。准确分级和分期对治疗计划和预测结果至关重要，但这一手动过程耗时且病理学家之间存在观察者差异。随着数字病理学切片数量的增加，需要开发稳健的自动化方法来辅助这一关键的诊断步骤。&lt;h4&gt;背景&lt;/h4&gt;卵巢癌的诊断主要依赖于对活检组织样本的病理学检查，这是一个耗时且易受观察者差异影响的程序。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架，用于使用常规病理学图像自动预测卵巢癌的阶段（分为五类：0、I、II、III、IV）。&lt;h4&gt;方法&lt;/h4&gt;研究采用了迁移学习方法，微调了一个在ImageNet上预训练的ResNet-101卷积神经网络。训练过程中包含了全面的数据增强、加权随机采样和类别加权，以解决数据集的特点。使用遗传算法对学习率、dropout率和权重衰减进行了超参数优化，以提高模型性能和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在卵巢薄组织明场图像的独立测试集上评估，所开发的模型实现了97.62%的整体分类准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的深度学习框架能够有效地辅助卵巢癌的自动分级和分期，具有较高的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ovarian cancer remains a challenging malignancy to diagnose and manage, withprognosis heavily dependent on the stage at detection. Accurate grading andstaging, primarily based on histopathological examination of biopsy tissuesamples, are crucial for treatment planning and predicting outcomes. However,this manual process is time-consuming and subject to inter-observer variabilityamong pathologists. The increasing volume of digital histopathology slidesnecessitates the development of robust, automated methods to assist in thiscritical diagnostic step for ovarian cancer. (Methods) This study presents adeep learning framework for the automated prediction of ovarian cancer stage(classified into five categories: 0, I, II, III, IV) using routinehistopathological images. We employed a transfer learning approach, fine-tuninga ResNet-101 convolutional neural network pre-trained on ImageNet. The trainingprocess incorporated comprehensive data augmentation, weighted random sampling,and class weighting to address dataset characteristics. Hyperparameteroptimization for learning rate, dropout rate, and weight decay was performedusing a genetic algorithm to enhance model performance and generalization.(Results) Evaluated on an independent test set of ovarian thin tissuebrightfield images, the developed model achieved a high overall classificationaccuracy of 97.62%.</description>
      <author>example@mail.com (Ashmit K Mishra, Mousa Alrubayan, Prabhakar Pradhan)</author>
      <guid isPermaLink="false">2505.09993v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</title>
      <link>http://arxiv.org/abs/2505.09985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为有序子集多扩散模型（OSMM）的稀疏视图CT重建方法，该方法有效提高了重建图像的细节和质量。&lt;h4&gt;背景&lt;/h4&gt;基于分数的扩散模型在稀疏视图CT重建领域展现出巨大潜力，但投影数据集大且冗余，导致学习效果不佳，重建图像缺乏细节。&lt;h4&gt;目的&lt;/h4&gt;提出OSMM以解决上述问题，提高稀疏视图CT重建的细节和质量。&lt;h4&gt;方法&lt;/h4&gt;OSMM将CT投影数据分为等份，并使用多子集扩散模型（MSDM）独立学习每一份数据。此外，将整个扩散模型（OWDM）与完整的投影数据结合，作为全局信息约束，减少错误或不一致的投影数据信息。OSMM采用无监督学习框架，对稀疏视图CT的多样性具有很好的适应性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，OSMM在图像质量和噪声鲁棒性方面优于传统扩散模型，为稀疏视图CT成像提供了一种强大且通用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;OSMM是一种有效的稀疏视图CT重建方法，能够显著提高重建图像的细节和质量，适用于不同的临床场景。&lt;h4&gt;翻译&lt;/h4&gt;Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMM's unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Score-based diffusion models have shown significant promise in the field ofsparse-view CT reconstruction. However, the projection dataset is large andriddled with redundancy. Consequently, applying the diffusion model tounprocessed data results in lower learning effectiveness and higher learningdifficulty, frequently leading to reconstructed images that lack fine details.To address these issues, we propose the ordered-subsets multi-diffusion model(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CTprojection data into equal subsets and employs multi-subsets diffusion model(MSDM) to learn from each subset independently. This targeted learning approachreduces complexity and enhances the reconstruction of fine details.Furthermore, the integration of one-whole diffusion model (OWDM) with completesinogram data acts as a global information constraint, which can reduce thepossibility of generating erroneous or inconsistent sinogram information.Moreover, the OSMM's unsupervised learning framework provides strong robustnessand generalizability, adapting seamlessly to varying sparsity levels of CTsinograms. This ensures consistent and reliable performance across differentclinical scenarios. Experimental results demonstrate that OSMM outperformstraditional diffusion models in terms of image quality and noise resilience,offering a powerful and versatile solution for advanced CT imaging insparse-view scenarios.</description>
      <author>example@mail.com (Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu)</author>
      <guid isPermaLink="false">2505.09985v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>VALVEFIT: An analysis-suitable B-spline-based surface fitting framework for patient-specific modeling of tricuspid valves</title>
      <link>http://arxiv.org/abs/2505.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VALVEFIT是一个基于GPU加速的可微分的B样条曲面拟合框架，用于从医疗图像分割获得的点云中快速重建平滑、适合分析的几何形状。&lt;h4&gt;背景&lt;/h4&gt;患者特异性的三尖瓣计算建模对于心脏瓣膜疾病的临床评估至关重要，但这个过程受到医疗图像数据固有限制的阻碍，如噪声和稀疏性，以及复杂的瓣膜动力学。&lt;h4&gt;目的&lt;/h4&gt;开发VALVEFIT框架，以解决医疗图像数据中的挑战，实现快速、准确的三尖瓣计算建模。&lt;h4&gt;方法&lt;/h4&gt;VALVEFIT使用理想的TVB样条模板曲面，通过创新的损失函数优化控制点位置，以拟合分割点云，并引入新的正则化项以确保表面在大变形下保持平滑、规则且无交点。&lt;h4&gt;主要发现&lt;/h4&gt;VALVEFIT在模拟点云上表现出鲁棒性和准确性，并且在不同点云密度和噪声水平上具有良好的鲁棒性。它还能拟合从不同瓣膜运动阶段的真实患者获取的点云。&lt;h4&gt;结论&lt;/h4&gt;VALVEFIT使得患者特异性的建模自动化，几乎无需人工干预，为未来直接图像到分析平台在临床应用中的开发铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：针对三尖瓣（TV）的患者特异性计算建模对于心脏瓣膜疾病的临床评估至关重要。然而，这一过程受到医疗图像数据固有局限性的阻碍，例如噪声和稀疏性，以及复杂的瓣膜动力学。我们提出了VALVEFIT，一个新颖的基于GPU加速和可微分的B样条曲面拟合框架，能够从通过医学图像分割获得的点云中快速重建平滑、适合分析的几何形状。我们从一个理想的TVB样条模板曲面开始，通过创新的损失函数优化其控制点位置以拟合分割点云。为了确保表面在大变形下保持平滑、规则且无交点，我们引入了新的正则化项。我们通过将框架应用于作为真实情况的模拟点云来展示其鲁棒性并验证其准确性。我们还展示了它在不同点云密度和噪声水平上的鲁棒性。最后，我们展示了该框架对拟合从不同瓣膜运动阶段的真实患者获取的点云的性能。然后，在对拟合表面进行等几何生物力学瓣膜模拟时，展示了其直接应用于分析的可能性。VALVEFIT使得患者特异性的建模自动化，几乎无需人工干预，为未来直接图像到分析平台在临床应用中的开发铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient-specific computational modeling of the tricuspid valve (TV) is vitalfor the clinical assessment of heart valve diseases. However, this process ishindered by limitations inherent in the medical image data, such as noise andsparsity, as well as by complex valve dynamics. We present VALVEFIT, a novelGPU-accelerated and differentiable B-spline surface fitting framework thatenables rapid reconstruction of smooth, analysis-suitable geometry from pointclouds obtained via medical image segmentation. We start with an idealized TVB-spline template surface and optimize its control point positions to fitsegmented point clouds via an innovative loss function, balancing shapefidelity and mesh regularization. Novel regularization terms are introduced toensure that the surface remains smooth, regular, and intersection-free duringlarge deformations. We demonstrate the robustness and validate the accuracy ofthe framework by first applying it to simulation-derived point clouds thatserve as the ground truth. We further show its robustness across differentpoint cloud densities and noise levels. Finally, we demonstrate the performanceof the framework toward fitting point clouds obtained from real patients atdifferent stages of valve motion. An isogeometric biomechanical valvesimulation is then performed on the fitted surfaces to show their directapplicability toward analysis. VALVEFIT enables automated patient-specificmodeling with minimal manual intervention, paving the way for the futuredevelopment of direct image-to-analysis platforms for clinical applications.</description>
      <author>example@mail.com (Ajith Moola, Ashton M. Corpuz, Michael J. Burkhart, Colton J. Ross, Arshid Mir, Harold M. Burkhart, Chung-Hao Lee, Ming-Chen Hsu, Aishwarya Pawar)</author>
      <guid isPermaLink="false">2505.09790v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments</title>
      <link>http://arxiv.org/abs/2505.09434v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 14 figures, to be published in Frontiers in Robotics and AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中引导机器人群体遵循某种群行为的工作，并引入了更现实的局部避障策略。&lt;h4&gt;背景&lt;/h4&gt;先前研究关注于在未知障碍环境中使用分布式NMPC引导机器人群体。&lt;h4&gt;目的&lt;/h4&gt;实现更真实的局部避障策略，并通过优化框架整合局部障碍避免约束。&lt;h4&gt;方法&lt;/h4&gt;将局部障碍避免约束与点云数据结合，并使用点云处理技术（包括方向过滤和下采样）以减少优化过程中的计算负担。&lt;h4&gt;主要发现&lt;/h4&gt;通过Gazebo中的3D模拟验证了算法的性能，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效减少计算负担，并在实际模拟中表现出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;本研究扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中引导机器人群体遵循某种群行为的工作，并引入了更现实的局部避障策略。具体而言，我们通过将局部障碍避免约束与点云数据结合，并将其整合到NMPC框架中。在这里，每个智能体依赖于其局部传感器的数据来感知和响应附近的障碍物。提出了一种针对二维和三维点云的点云处理技术，以在优化过程中最小化计算负担。该过程包括方向过滤和下采样，这显著减少了数据点的数量。通过Gazebo中的现实3D模拟验证了算法的性能，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3389/frobt.2025.1540808&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our prior work on the distributed nonlinear modelpredictive control (NMPC) for navigating a robot fleet following a certainflocking behavior in unknown obstructed environments with a more realisticlocal obstacle avoidance strategy. More specifically, we integrate the localobstacle avoidance constraint using point clouds into the NMPC framework. Here,each agent relies on data from its local sensor to perceive and respond tonearby obstacles. A point cloud processing technique is presented for bothtwo-dimensional and three-dimensional point clouds to minimize thecomputational burden during the optimization. The process consists ofdirectional filtering and down-sampling that significantly reduce the number ofdata points. The algorithm's performance is validated through realistic 3Dsimulations in Gazebo, and its practical feasibility is further explored viahardware-in-the-loop (HIL) simulations on embedded platforms.</description>
      <author>example@mail.com (Nuthasith Gerdpratoom, Kaoru Yamamoto)</author>
      <guid isPermaLink="false">2505.09434v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era</title>
      <link>http://arxiv.org/abs/2505.09651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了地理空间表示学习的发展，探讨了深度学习和大型语言模型在地理空间智能（LI）领域的应用。&lt;h4&gt;背景&lt;/h4&gt;地理空间智能（LI）是现代空间决策的关键，地理空间表示学习正在通过深度学习和大型语言模型（LLM）的进步而快速发展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在全面回顾地理空间表示学习，并基于数据、方法和应用视角构建一个结构化的分类法。&lt;h4&gt;方法&lt;/h4&gt;文章通过数据视角、方法视角和应用视角对地理空间表示学习进行了分类，并讨论了当前进展、现有局限性和未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;深度神经网络在结构化地理空间数据特征提取方面表现出色，而LLM的集成则增强了跨模态地理空间推理和未结构化地理文本数据处理的能力。&lt;h4&gt;结论&lt;/h4&gt;本文为地理空间智能领域提供了深入探索和未来创新的路线图。&lt;h4&gt;翻译&lt;/h4&gt;Location Intelligence (LI), the science of transforming location-centric geospatial data into actionable knowledge, has become a cornerstone of modern spatial decision-making. The rapid evolution of Geospatial Representation Learning is fundamentally reshaping LI development through two successive technological revolutions: the deep learning breakthrough and the emerging large language model (LLM) paradigm. While deep neural networks (DNNs) have demonstrated remarkable success in automated feature extraction from structured geospatial data (e.g., satellite imagery, GPS trajectories), the recent integration of LLMs introduces transformative capabilities for cross-modal geospatial reasoning and unstructured geo-textual data processing. This survey presents a comprehensive review of geospatial representation learning across both technological eras, organizing them into a structured taxonomy based on the complete pipeline comprising: (1) data perspective, (2) methodological perspective and (3) application perspective. We also highlight current advancements, discuss existing limitations, and propose potential future research directions in the LLM era. This work offers a thorough exploration of the field and providing a roadmap for further innovation in LI. The summary of the up-to-date paper list can be found in https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo continuous updates.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Location Intelligence (LI), the science of transforming location-centricgeospatial data into actionable knowledge, has become a cornerstone of modernspatial decision-making. The rapid evolution of Geospatial RepresentationLearning is fundamentally reshaping LI development through two successivetechnological revolutions: the deep learning breakthrough and the emerginglarge language model (LLM) paradigm. While deep neural networks (DNNs) havedemonstrated remarkable success in automated feature extraction from structuredgeospatial data (e.g., satellite imagery, GPS trajectories), the recentintegration of LLMs introduces transformative capabilities for cross-modalgeospatial reasoning and unstructured geo-textual data processing. This surveypresents a comprehensive review of geospatial representation learning acrossboth technological eras, organizing them into a structured taxonomy based onthe complete pipeline comprising: (1) data perspective, (2) methodologicalperspective and (3) application perspective. We also highlight currentadvancements, discuss existing limitations, and propose potential futureresearch directions in the LLM era. This work offers a thorough exploration ofthe field and providing a roadmap for further innovation in LI. The summary ofthe up-to-date paper list can be found inhttps://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergocontinuous updates.</description>
      <author>example@mail.com (Xixuan Hao, Yutian Jiang, Xingchen Zou, Jiabo Liu, Yifang Yin, Yuxuan Liang)</author>
      <guid isPermaLink="false">2505.09651v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration</title>
      <link>http://arxiv.org/abs/2505.09756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多智能体强化学习（MARL）框架，该框架中的智能体在一个具有潜在社区结构和混合成员资格的时间演化网络中合作。&lt;h4&gt;背景&lt;/h4&gt;与传统的基于邻居或固定交互图的框架不同，该框架通过允许每个智能体属于多个重叠的社区来捕捉灵活和抽象的协调模式。&lt;h4&gt;目的&lt;/h4&gt;旨在设计一种能够有效共享结构化信息，并支持迁移学习和主动学习的MARL框架。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于社区结构的actor-critic算法，智能体通过继承社区级别的策略更新和价值学习的估计来实现信息共享，而不需要访问其他智能体的策略。该框架支持通过成员资格估计适应新智能体或任务，并在探索过程中优先考虑不确定的社区。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明了在actor和critic更新中使用线性函数近似下的收敛性保证。&lt;h4&gt;结论&lt;/h4&gt;这是第一个将社区结构、可迁移性和主动学习与可证明的保证相结合的MARL框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new framework for multi-agent reinforcement learning (MARL),where the agents cooperate in a time-evolving network with latent communitystructures and mixed memberships. Unlike traditional neighbor-based or fixedinteraction graphs, our community-based framework captures flexible andabstract coordination patterns by allowing each agent to belong to multipleoverlapping communities. Each community maintains shared policy and valuefunctions, which are aggregated by individual agents according to personalizedmembership weights. We also design actor-critic algorithms that exploit thisstructure: agents inherit community-level estimates for policy updates andvalue learning, enabling structured information sharing without requiringaccess to other agents' policies. Importantly, our approach supports bothtransfer learning by adapting to new agents or tasks via membership estimation,and active learning by prioritizing uncertain communities during exploration.Theoretically, we establish convergence guarantees under linear functionapproximation for both actor and critic updates. To our knowledge, this is thefirst MARL framework that integrates community structure, transferability, andactive learning with provable guarantees.</description>
      <author>example@mail.com (Zhaoyang Shi)</author>
      <guid isPermaLink="false">2505.09756v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
      <link>http://arxiv.org/abs/2505.09193v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first learned video codec that surpasses VTM 13.2 RA across all  standard test datasets. Code will be available at  https://github.com/JiangWeibeta/ECVC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiECVC的基于双向视频压缩（BVC）的框架，该框架在视频压缩性能上超越了VVC参考软件VTM，实现了显著的比特率降低。&lt;h4&gt;背景&lt;/h4&gt;现有的BVC方法在性能上落后于仅使用前向预测的LVC方法，主要原因是它们在提取多样化和准确的上下文信息方面能力有限，并且缺乏对动态抑制有害上下文的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出BiECVC框架，以解决现有BVC方法的局限性，提高视频压缩性能。&lt;h4&gt;方法&lt;/h4&gt;BiECVC通过结合多样化的本地和非本地上下文建模以及自适应上下文门控来提高性能。它重用低层的高质量特征，并使用解码的运动向量进行对齐，同时采用线性注意力机制来建模非本地依赖关系。为了减轻上下文预测不准确的影响，引入了双向上下文门控。&lt;h4&gt;主要发现&lt;/h4&gt;BiECVC在随机访问配置下，比特率分别降低了13.4%和15.7%，超越了VTM 13.2，这是第一个在所有标准测试数据集上超越VTM 13.2 RA的基于学习的视频编解码器。&lt;h4&gt;结论&lt;/h4&gt;BiECVC在视频压缩性能上取得了突破，为BVC领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead. To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent forward prediction-based learned video compression (LVC) methods haveachieved impressive results, even surpassing VVC reference software VTM underthe Low Delay B (LDB) configuration. In contrast, learned bidirectional videocompression (BVC) remains underexplored and still lags behind its forward-onlycounterparts. This performance gap is mainly due to the limited ability toextract diverse and accurate contexts: most existing BVCs primarily exploittemporal motion while neglecting non-local correlations across frames.Moreover, they lack the adaptability to dynamically suppress harmful contextsarising from fast motion or occlusion. To tackle these challenges, we proposeBiECVC, a BVC framework that incorporates diversified local and non-localcontext modeling along with adaptive context gating. For local contextenhancement, BiECVC reuses high-quality features from lower layers and alignsthem using decoded motion vectors without introducing extra motion overhead. Tomodel non-local dependencies efficiently, we adopt a linear attention mechanismthat balances performance and complexity. To further mitigate the impact ofinaccurate context prediction, we introduce Bidirectional Context Gating,inspired by data-dependent decay in recent autoregressive language models, todynamically filter contextual information based on conditional coding results.Extensive experiments demonstrate that BiECVC achieves state-of-the-artperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2under the Random Access (RA) configuration with intra periods of 32 and 64,respectively. To our knowledge, BiECVC is the first learned video codec tosurpass VTM 13.2 RA across all standard test datasets. Code will be availableat https://github.com/JiangWeibeta/ECVC.</description>
      <author>example@mail.com (Wei Jiang, Junru Li, Kai Zhang, Li Zhang)</author>
      <guid isPermaLink="false">2505.09193v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fair In-Context Learning with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09503v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表格型基础模型在结构化数据上的强情境学习能力，以及如何通过预处理策略来减少模型中的偏见，以提高情境预测的公平性。&lt;h4&gt;背景&lt;/h4&gt;表格型基础模型在结构化数据上表现出强大的情境学习能力，可以不更新参数就对测试集进行准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究表格型情境学习能力中的公平性问题，并探索减少模型偏见的方法。&lt;h4&gt;方法&lt;/h4&gt;论文研究了三种预处理策略：相关去除、分组平衡演示选择和基于不确定性的演示选择，以减少偏见。&lt;h4&gt;主要发现&lt;/h4&gt;基于不确定性的演示选择策略可以持续提高情境预测的组公平性。&lt;h4&gt;结论&lt;/h4&gt;表格型情境学习能力中的偏见可以通过特定的预处理策略得到有效缓解。&lt;h4&gt;翻译&lt;/h4&gt;Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundational models have exhibited strong in-context learning (ICL)capabilities on structured data, allowing them to make accurate predictions ontest sets without parameter updates, using training examples as context. Thisemerging approach positions itself as a competitive alternative to traditionalgradient-boosted tree methods. However, while biases in conventional machinelearning models are well documented, it remains unclear how these biasesmanifest in tabular ICL. The paper investigates the fairness implications oftabular ICL and explores three preprocessing strategies--correlation removal,group-balanced demonstration selection, and uncertainty-based demonstrationselection--to address bias. Comprehensive experiments indicate thatuncertainty-based demonstration selection consistently enhances group fairnessof in-context predictions. The source code for reproducing the results of thiswork can be found at https://github.com/patrikken/Fair-TabICL.</description>
      <author>example@mail.com (Patrik Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji)</author>
      <guid isPermaLink="false">2505.09503v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Analog Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 8 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通用的可扩展方法，用于在具有噪声和低精度限制的模拟硬件上鲁棒地适配大型语言模型（LLMs）。该方法使得包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct在内的高级模型能够保持与4位权重、8位激活基线相当的性能，同时展示了在低精度数字硬件上进行推理的能力，并证明了在测试时计算缩放方面的优势。&lt;h4&gt;背景&lt;/h4&gt;模拟内存计算（AIMC）是一种有前途的计算范式，它旨在超越基于传统冯·诺伊曼架构的速度和功耗效率限制。然而，AIMC引入了诸如噪声计算和严格的输入输出量化约束等基本挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，使得LLMs能够在模拟硬件上保持高精度，并能够量化以适应低精度数字硬件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用的可扩展方法，该方法允许LLMs在存在模拟噪声和量化约束的情况下保持性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法使得高级模型在模拟硬件上保持与低精度数字硬件上相当的性能，并且可以通过训练方法将模拟基础模型量化以适应低精度数字硬件。&lt;h4&gt;结论&lt;/h4&gt;该方法在大型LLMs和高效率模拟硬件之间架起了一座桥梁，为高效的基础模型提供了走向节能的路径。&lt;h4&gt;翻译&lt;/h4&gt;Analog in-memory computing (AIMC) is a promising compute paradigm to improve speed and power efficiency of neural network inference beyond the limits of conventional von Neumann-based architectures. However, AIMC introduces fundamental challenges such as noisy computations and strict constraints on input and output quantization. Because of these constraints and imprecisions, off-the-shelf LLMs are not able to achieve 4-bit-level performance when deployed on AIMC-based hardware. While researchers previously investigated recovering this accuracy gap on small, mostly vision-based models, a generic method applicable to LLMs pre-trained on trillions of tokens does not yet exist. In this work, we introduce a general and scalable method to robustly adapt LLMs for execution on noisy, low-precision analog hardware. Our approach enables state-of-the-art models including Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct to retain performance comparable to 4-bit weight, 8-bit activation baselines, despite the presence of analog noise and quantization constraints. Additionally, we show that as a byproduct of our training methodology, analog foundation models can be quantized for inference on low-precision digital hardware. Finally, we show that our models also benefit from test-time compute scaling, showing better scaling behavior than models trained with 4-bit weight and 8-bit static input quantization. Our work bridges the gap between high-capacity LLMs and efficient analog hardware, offering a path toward energy-efficient foundation models. Code is available at https://github.com/IBM/analog-foundation-models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ibm/analog-foundation-models&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog in-memory computing (AIMC) is a promising compute paradigm to improvespeed and power efficiency of neural network inference beyond the limits ofconventional von Neumann-based architectures. However, AIMC introducesfundamental challenges such as noisy computations and strict constraints oninput and output quantization. Because of these constraints and imprecisions,off-the-shelf LLMs are not able to achieve 4-bit-level performance whendeployed on AIMC-based hardware. While researchers previously investigatedrecovering this accuracy gap on small, mostly vision-based models, a genericmethod applicable to LLMs pre-trained on trillions of tokens does not yetexist. In this work, we introduce a general and scalable method to robustlyadapt LLMs for execution on noisy, low-precision analog hardware. Our approachenables state-of-the-art models $\unicode{x2013}$ includingPhi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retainperformance comparable to 4-bit weight, 8-bit activation baselines, despite thepresence of analog noise and quantization constraints. Additionally, we showthat as a byproduct of our training methodology, analog foundation models canbe quantized for inference on low-precision digital hardware. Finally, we showthat our models also benefit from test-time compute scaling, showing betterscaling behavior than models trained with 4-bit weight and 8-bit static inputquantization. Our work bridges the gap between high-capacity LLMs and efficientanalog hardware, offering a path toward energy-efficient foundation models.Code is available at https://github.com/IBM/analog-foundation-models .</description>
      <author>example@mail.com (Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian)</author>
      <guid isPermaLink="false">2505.09663v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Next Word Suggestion using Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.09649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图卷积网络（GNN）和长短期记忆网络（LSTM）的语言模型上下文嵌入方法，以预测给定前文局部上下文下的下一个词。&lt;h4&gt;背景&lt;/h4&gt;现有的语言模型通常需要大量参数、文本数据和计算资源，成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决语言模型中的上下文嵌入子任务。&lt;h4&gt;方法&lt;/h4&gt;利用GNN的图卷积操作编码上下文，并与LSTM结合预测下一个词。&lt;h4&gt;主要发现&lt;/h4&gt;在有限的资源下，该方法在自定义维基百科文本语料库上表现良好。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地预测了下一个词，为语言模型的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Language Modeling is a prevalent task in Natural Language Processing. The currently existing most recent and most successful language models often tend to build a massive model with billions of parameters, feed in a tremendous amount of text data, and train with enormous computation resources which require millions of dollars. In this project, we aim to address an important sub-task in language modeling, i.e., context embedding. We propose an approach to exploit the Graph Convolution operation in GNNs to encode the context and use it in coalition with LSTMs to predict the next word given a local context of preceding words. We test this on the custom Wikipedia text corpus using a very limited amount of resources and show that this approach works fairly well to predict the next word.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language Modeling is a prevalent task in Natural Language Processing. Thecurrently existing most recent and most successful language models often tendto build a massive model with billions of parameters, feed in a tremendousamount of text data, and train with enormous computation resources whichrequire millions of dollars. In this project, we aim to address an importantsub-task in language modeling, i.e., context embedding. We propose an approachto exploit the Graph Convolution operation in GNNs to encode the context anduse it in coalition with LSTMs to predict the next word given a local contextof preceding words. We test this on the custom Wikipedia text corpus using avery limited amount of resources and show that this approach works fairly wellto predict the next word.</description>
      <author>example@mail.com (Abisha Thapa Magar, Anup Shakya)</author>
      <guid isPermaLink="false">2505.09649v1</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
      <link>http://arxiv.org/abs/2505.09178v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniCAD的统一架构，用于解决视觉模型预训练的复杂性和规模增长对多任务辅助诊断系统开发和部署带来的挑战。UniCAD利用预训练视觉模型的能力，处理2D和3D医学图像，同时只需少量特定任务参数。&lt;h4&gt;背景&lt;/h4&gt;随着视觉模型预训练的复杂性和规模的增长，开发和部署多任务辅助诊断系统变得更加困难和资源密集。此外，医学图像社区缺乏一个开源的CAD平台来快速创建高效和可扩展的诊断模型。&lt;h4&gt;目的&lt;/h4&gt;提出UniCAD架构，旨在解决上述问题，实现高效且资源消耗低的医学图像诊断模型。&lt;h4&gt;方法&lt;/h4&gt;UniCAD采用了以下两种关键创新：(1) 效率：使用低秩自适应策略来适应预训练视觉模型到医学图像领域，在性能上与完全微调的模型相当，同时只引入了0.17%的可训练参数。(2) 插拔式：模块化架构，结合冻结的基础模型和多个插拔式专家，实现多样化任务和无缝功能扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同的医学数据集上进行的综合实验表明，UniCAD在准确性和部署效率方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UniCAD为医学图像诊断提供了一种高效、资源消耗低的解决方案，并促进了一个更公平、更高效的研究生态系统。&lt;h4&gt;翻译&lt;/h4&gt;The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging community lacks an open-source CAD platform to enable the rapid creation of efficient and extendable diagnostic models. To address these issues, we propose UniCAD, a unified architecture that leverages the robust capabilities of pre-trained vision foundation models to seamlessly handle both 2D and 3D medical images while requiring only minimal task-specific parameters. UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation strategy is employed to adapt a pre-trained visual model to the medical image domain, achieving performance on par with fully fine-tuned counterparts while introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular architecture that combines a frozen foundation model with multiple plug-and-play experts, enabling diverse tasks and seamless functionality expansion. Building on this unified CAD architecture, we establish an open-source platform where researchers can share and access lightweight CAD experts, fostering a more equitable and efficient research ecosystem. Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency. The source code and project page are available at https://mii-laboratory.github.io/UniCAD/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing complexity and scale of visual model pre-training have madedeveloping and deploying multi-task computer-aided diagnosis (CAD) systemsincreasingly challenging and resource-intensive. Furthermore, the medicalimaging community lacks an open-source CAD platform to enable the rapidcreation of efficient and extendable diagnostic models. To address theseissues, we propose UniCAD, a unified architecture that leverages the robustcapabilities of pre-trained vision foundation models to seamlessly handle both2D and 3D medical images while requiring only minimal task-specific parameters.UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptationstrategy is employed to adapt a pre-trained visual model to the medical imagedomain, achieving performance on par with fully fine-tuned counterparts whileintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modulararchitecture that combines a frozen foundation model with multipleplug-and-play experts, enabling diverse tasks and seamless functionalityexpansion. Building on this unified CAD architecture, we establish anopen-source platform where researchers can share and access lightweight CADexperts, fostering a more equitable and efficient research ecosystem.Comprehensive experiments across 12 diverse medical datasets demonstrate thatUniCAD consistently outperforms existing methods in both accuracy anddeployment efficiency. The source code and project page are available athttps://mii-laboratory.github.io/UniCAD/.</description>
      <author>example@mail.com (Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang)</author>
      <guid isPermaLink="false">2505.09178v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>A systematic review of challenges and proposed solutions in modeling multimodal data</title>
      <link>http://arxiv.org/abs/2505.06945v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态数据建模在临床研究中成为一种强大的方法，能够整合多种数据类型，如影像、基因组学、可穿戴传感器和电子健康记录。尽管它有提高诊断准确性和支持个性化护理的潜力，但这种异构数据的建模存在重大技术挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态数据建模作为一种新兴的方法，在临床研究中显示出其重要性。&lt;h4&gt;目的&lt;/h4&gt;系统综述旨在综合69项研究的发现，以识别多模态数据建模中常见的障碍。&lt;h4&gt;方法&lt;/h4&gt;系统综述通过综合69项研究的发现来识别障碍，并强调了最近的方法学进展。&lt;h4&gt;主要发现&lt;/h4&gt;主要障碍包括缺失的模态、样本量有限、维度不平衡、可解释性问题以及找到最佳融合技术。&lt;h4&gt;结论&lt;/h4&gt;通过映射当前趋势和创新，该综述为该领域提供了全面的概述，并为多模态建模在医学应用中的未来研究和发展提供了实际见解。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据建模已成为临床研究中的一种强大方法，能够整合多种数据类型，如影像、基因组学、可穿戴传感器和电子健康记录。尽管它有提高诊断准确性和支持个性化护理的潜力，但这种异构数据的建模存在重大技术挑战。本系统综述综合了69项研究的发现，以识别常见的障碍，包括缺失的模态、样本量有限、维度不平衡、可解释性问题以及找到最佳融合技术。我们强调了最近的方法学进展，如迁移学习、生成模型、注意力机制和神经架构搜索，这些进展提供了有希望的解决方案。通过映射当前趋势和创新，本综述为该领域提供了全面的概述，并为多模态建模在医学应用中的未来研究和发展提供了实际见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data modeling has emerged as a powerful approach in clinicalresearch, enabling the integration of diverse data types such as imaging,genomics, wearable sensors, and electronic health records. Despite itspotential to improve diagnostic accuracy and support personalized care,modeling such heterogeneous data presents significant technical challenges.This systematic review synthesizes findings from 69 studies to identify commonobstacles, including missing modalities, limited sample sizes, dimensionalityimbalance, interpretability issues, and finding the optimal fusion techniques.We highlight recent methodological advances, such as transfer learning,generative models, attention mechanisms, and neural architecture search thatoffer promising solutions. By mapping current trends and innovations, thisreview provides a comprehensive overview of the field and offers practicalinsights to guide future research and development in multimodal modeling formedical applications.</description>
      <author>example@mail.com (Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binder, Nadine Binder)</author>
      <guid isPermaLink="false">2505.06945v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Graph neural networks and MSO</title>
      <link>http://arxiv.org/abs/2505.07816v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提供了一种对现有结果的替代证明，证明使用实数的循环图神经网络在受限于单调二阶逻辑（MSO）时，与分级模态替换演算具有相同的表达能力。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，使用实数的循环图神经网络在MSO逻辑限制下具有特定的表达能力。&lt;h4&gt;目的&lt;/h4&gt;提供对现有结果的替代证明，并考虑接受条件的变体。&lt;h4&gt;方法&lt;/h4&gt;通过构建分布式自动机来捕捉所有在树结构上由MSO定义的节点属性。&lt;h4&gt;主要发现&lt;/h4&gt;证明了循环图神经网络与分级模态替换演算在MSO逻辑限制下具有相同的表达能力。&lt;h4&gt;结论&lt;/h4&gt;该方法通过构建分布式自动机为循环图神经网络的表达能力提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;本文给出了一种对现有结果的替代证明，证明使用实数的循环图神经网络在受限于单调二阶逻辑MSO时，与分级模态替换演算具有相同的表达能力。证明基于构建分布式自动机来捕获所有MSO定义的树节点属性。我们还考虑了一些接受条件的变体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We give an alternative proof for the existing result that recurrent graphneural networks working with reals have the same expressive power inrestriction to monadic second-order logic MSO as the graded modal substitutioncalculus. The proof is based on constructing distributed automata that captureall MSO-definable node properties over trees. We also consider some variants ofthe acceptance conditions.</description>
      <author>example@mail.com (Veeti Ahvonen, Damian Heiman, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07816v2</guid>
      <pubDate>Fri, 16 May 2025 14:16:29 +0800</pubDate>
    </item>
    <item>
      <title>Streaming Sliced Optimal Transport</title>
      <link>http://arxiv.org/abs/2505.06835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 9 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了名为Stream-SW的流式计算SW距离的方法，以增强SOT或SW距离的统计和计算可扩展性。&lt;h4&gt;背景&lt;/h4&gt;SOT或SW距离因其统计和计算可扩展性而被广泛认可。&lt;h4&gt;目的&lt;/h4&gt;进一步增强SW距离的计算可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出了从样本流中计算SW的第一种方法，即Stream-SW。通过引入一维Wasserstein距离的流式计算，并利用样本流的分位数近似技术来定义流式1DW距离，从而得到Stream-SW。&lt;h4&gt;主要发现&lt;/h4&gt;Stream-SW具有低内存复杂度，并在近似误差方面提供理论保证。实验表明，Stream-SW在比较高斯分布和混合高斯分布的流样本时，比随机子采样更精确，且内存消耗更低。此外，在点云分类、点云梯度流和流式变化点检测上的实验进一步展示了Stream-SW的有利性能。&lt;h4&gt;结论&lt;/h4&gt;Stream-SW是一种有效的方法，能够提高SW距离的计算效率，同时保持较高的准确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widelyrecognized for its statistical and computational scalability. In this work, wefurther enhance the computational scalability by proposing the first method forcomputing SW from sample streams, called \emph{streaming sliced Wasserstein}(Stream-SW). To define Stream-SW, we first introduce the streaming computationof the one-dimensional Wasserstein distance. Since the one-dimensionalWasserstein (1DW) distance has a closed-form expression, given by the absolutedifference between the quantile functions of the compared distributions, weleverage quantile approximation techniques for sample streams to define thestreaming 1DW distance. By applying streaming 1DW to all projections, we obtainStream-SW. The key advantage of Stream-SW is its low memory complexity whileproviding theoretical guarantees on the approximation error. We demonstratethat Stream-SW achieves a more accurate approximation of SW than randomsubsampling, with lower memory consumption, in comparing Gaussian distributionsand mixtures of Gaussians from streaming samples. Additionally, we conductexperiments on point cloud classification, point cloud gradient flows, andstreaming change point detection to further highlight the favorable performanceof Stream-SW.</description>
      <author>example@mail.com (Khai Nguyen)</author>
      <guid isPermaLink="false">2505.06835v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
  <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出SeriesBench，一个包含105个精心挑选的叙事驱动系列的视频理解基准，旨在评估MLLMs对复杂连续叙事的理解能力。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型（MLLMs）的快速发展，现有的基准主要针对独立视频，评估视觉元素如人类动作和物体状态，而忽略了复杂连续叙事视频。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准对复杂连续叙事视频理解能力评估不足的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 选择涵盖多种类型的戏剧系列；2. 引入新的长距离叙事标注方法，结合全信息转换方法将人工标注转换为不同任务格式；3. 提出新的叙事推理框架PC-DCoT，以增强模型对剧情结构和人物关系的分析能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有MLLMs在理解叙事驱动系列方面仍面临重大挑战，而PC-DCoT能够帮助这些MLLMs提高性能。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT突出了提高模型理解叙事驱动系列能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zackhxn/seriesbench-cvpr2025&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v3</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Rhomboid Tiling for Geometric Graph Deep Learning</title>
      <link>http://arxiv.org/abs/2505.09586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Rhomboid Tiling (RT)的聚类方法，用于从几何图形数据中提取高级几何结构，并设计了一种基于RT聚类的RTPool模型，用于图分类任务，显示出优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;Graph Neural Networks (GNNs)通过邻域消息传递框架有效地从图结构数据中学习，但它们高度依赖图的连接结构，限制了捕获几何图形中固有丰富几何特征的能力。&lt;h4&gt;目的&lt;/h4&gt;提出Rhomboid Tiling (RT)聚类方法以利用数据中的复杂几何信息，并设计RTPool模型以提高图分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;引入Rhomboid Tiling (RT)聚类，该方法基于菱形镶嵌结构进行聚类，并设计RTPool，一种基于RT聚类的层次图聚类池化模型。&lt;h4&gt;主要发现&lt;/h4&gt;RTPool模型在7个基准数据集上优于21个最先进的竞争对手。&lt;h4&gt;结论&lt;/h4&gt;RT聚类方法和RTPool模型为图分类任务提供了更强大的性能，并有效地提取了数据的几何特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have proven effective for learning fromgraph-structured data through their neighborhood-based message passingframework. Many hierarchical graph clustering pooling methods modify thisframework by introducing clustering-based strategies, enabling the constructionof more expressive and powerful models. However, all of these message passingframework heavily rely on the connectivity structure of graphs, limiting theirability to capture the rich geometric features inherent in geometric graphs. Toaddress this, we propose Rhomboid Tiling (RT) clustering, a novel clusteringmethod based on the rhomboid tiling structure, which performs clustering byleveraging the complex geometric information of the data and effectivelyextracts its higher-order geometric structures. Moreover, we design RTPool, ahierarchical graph clustering pooling model based on RT clustering for graphclassification tasks. The proposed model demonstrates superior performance,outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.</description>
      <author>example@mail.com (Yipeng Zhang, Longlong Li, Kelin Xia)</author>
      <guid isPermaLink="false">2505.09586v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Context Diffusion Policies via Past-Token Prediction</title>
      <link>http://arxiv.org/abs/2505.09561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Videos are available at https://long-context-dp.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于从演示中学习有效的长上下文策略，以解决机器人任务中的长期观察和动作推理问题。&lt;h4&gt;背景&lt;/h4&gt;随着上下文长度的增加，训练长上下文策略变得越来越昂贵，且策略性能往往因为虚假相关性而下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来有效保留过去信息，同时避免因上下文长度增加而导致的训练成本上升和性能下降。&lt;h4&gt;方法&lt;/h4&gt;1. 重新审视模仿学习中的copycat问题，并识别出近期扩散策略中的新挑战。2. 引入Past-Token Prediction (PTP)，一个辅助任务，使策略学习预测过去和未来的动作标记。3. 提出多阶段训练策略：预训练视觉编码器使用短上下文，微调策略头使用缓存的长期上下文嵌入。4. 将PTP扩展为测试时的自我验证机制，使策略在推理过程中能够评分和选择与过去动作一致的候选者。&lt;h4&gt;主要发现&lt;/h4&gt;通过PTP和新的训练策略，显著提高了长上下文扩散策略的性能，同时大幅减少了内存和计算开销。&lt;h4&gt;结论&lt;/h4&gt;实验表明，该方法将长上下文扩散策略的性能提高了3倍，将策略训练速度提高了10倍以上。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在许多机器人任务中，对长序列观察和动作进行推理至关重要。然而，从演示中学习有效的长上下文策略仍然具有挑战性。随着上下文长度的增加，由于内存需求增加，训练成本越来越高，而且策略性能往往因为虚假相关性而下降。最近的方法通常通过截断上下文长度来规避这些问题，丢弃可能对后续决策至关重要的历史信息。在这篇论文中，我们提出了一种替代方法，该方法明确规范了过去信息的保留。我们首先回顾了模仿学习中的copycat问题，并确定了近期扩散政策中的相反挑战：不是过分依赖先前动作，而是往往无法捕捉过去和未来动作之间的基本依赖关系。为了解决这个问题，我们引入了Past-Token Prediction (PTP)，这是一个辅助任务，其中策略学习同时预测过去和未来的动作标记。这种规范显著提高了策略头部的时序建模，同时对视觉表示的依赖最小。基于这一观察，我们进一步引入了一种多阶段训练策略：使用短上下文预训练视觉编码器，并使用缓存的长期上下文嵌入微调策略头。这种策略保留了PTP的好处，同时大大减少了内存和计算开销。最后，我们将PTP扩展为测试时的自我验证机制，使策略能够在推理过程中评分和选择与过去动作一致的候选者。在四个真实世界和六个模拟任务上的实验表明，我们提出的方法通过3倍提高了长上下文扩散策略的性能，并将策略训练速度提高了10倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning over long sequences of observations and actions is essential formany robotic tasks. Yet, learning effective long-context policies fromdemonstrations remains challenging. As context length increases, trainingbecomes increasingly expensive due to rising memory demands, and policyperformance often degrades as a result of spurious correlations. Recent methodstypically sidestep these issues by truncating context length, discardinghistorical information that may be critical for subsequent decisions. In thispaper, we propose an alternative approach that explicitly regularizes theretention of past information. We first revisit the copycat problem inimitation learning and identify an opposite challenge in recent diffusionpolicies: rather than over-relying on prior actions, they often fail to captureessential dependencies between past and future actions. To address this, weintroduce Past-Token Prediction (PTP), an auxiliary task in which the policylearns to predict past action tokens alongside future ones. This regularizationsignificantly improves temporal modeling in the policy head, with minimalreliance on visual representations. Building on this observation, we furtherintroduce a multistage training strategy: pre-train the visual encoder withshort contexts, and fine-tune the policy head using cached long-contextembeddings. This strategy preserves the benefits of PTP while greatly reducingmemory and computational overhead. Finally, we extend PTP into aself-verification mechanism at test time, enabling the policy to score andselect candidates consistent with past actions during inference. Experimentsacross four real-world and six simulated tasks demonstrate that our proposedmethod improves the performance of long-context diffusion policies by 3x andaccelerates policy training by more than 10x.</description>
      <author>example@mail.com (Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2505.09561v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2505.09336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MultiviewVLM，一种用于从3D/4D数据中无监督地学习面部表情多视图对比表示的视觉语言模型。&lt;h4&gt;背景&lt;/h4&gt;该模型旨在解决面部表情识别的问题。&lt;h4&gt;目的&lt;/h4&gt;MultiviewVLM旨在通过多视图对比学习提高面部表情识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型架构整合了由生成的文本提示派生的伪标签，以引导情感语义的隐式对齐。为了捕捉多视图之间的共享信息，它提出了一种联合嵌入空间，无需显式监督即可对齐多视图表示。此外，它采用了一种新颖的多视图对比学习策略，利用稳定的正负样本对采样来增强模型的判别力。还引入了一种梯度友好的损失函数，以促进更平滑和更稳定的收敛，并对分布式训练进行了优化，以确保可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，MultiviewVLM优于现有的最先进方法，并且可以轻松适应各种现实世界应用，只需进行最小的修改。&lt;h4&gt;结论&lt;/h4&gt;MultiviewVLM是一个高效的面部表情识别模型，具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce MultiviewVLM, a vision-language model designedfor unsupervised contrastive multiview representation learning of facialemotions from 3D/4D data. Our architecture integrates pseudo-labels derivedfrom generated textual prompts to guide implicit alignment of emotionalsemantics. To capture shared information across multi-views, we propose a jointembedding space that aligns multiview representations without requiringexplicit supervision. We further enhance the discriminability of our modelthrough a novel multiview contrastive learning strategy that leverages stablepositive-negative pair sampling. A gradient-friendly loss function isintroduced to promote smoother and more stable convergence, and the model isoptimized for distributed training to ensure scalability. Extensive experimentsdemonstrate that MultiviewVLM outperforms existing state-of-the-art methods andcan be easily adapted to various real-world applications with minimalmodifications.</description>
      <author>example@mail.com (Muzammil Behzad)</author>
      <guid isPermaLink="false">2505.09336v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments</title>
      <link>http://arxiv.org/abs/2505.09434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 14 figures, to be published in Frontiers in Robotics and AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了先前关于分布式非线性模型预测控制（NMPC）在未知障碍环境中导航机器人群以特定集群行为的研究，并引入了更现实的局部避障策略。&lt;h4&gt;背景&lt;/h4&gt;研究了分布式非线性模型预测控制在未知障碍环境中导航机器人群的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种更现实的局部避障策略，并将其集成到NMPC框架中。&lt;h4&gt;方法&lt;/h4&gt;将局部避障约束使用点云集成到NMPC框架中，并使用点云处理技术来优化计算负担。技术包括方向过滤和下采样，显著减少了数据点的数量。&lt;h4&gt;主要发现&lt;/h4&gt;算法性能通过Gazebo中的真实3D模拟进行了验证，并通过嵌入式平台上的硬件在环（HIL）模拟进一步探索了其实际可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的算法能够有效实现机器人群的避障，并在实际环境中具有可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our prior work on the distributed nonlinear modelpredictive control (NMPC) for navigating a robot fleet following a certainflocking behavior in unknown obstructed environments with a more realisticlocal obstacle avoidance strategy. More specifically, we integrate the localobstacle avoidance constraint using point clouds into the NMPC framework. Here,each agent relies on data from its local sensor to perceive and respond tonearby obstacles. A point cloud processing technique is presented for bothtwo-dimensional and three-dimensional point clouds to minimize thecomputational burden during the optimization. The process consists ofdirectional filtering and down-sampling that significantly reduce the number ofdata points. The algorithm's performance is validated through realistic 3Dsimulations in Gazebo, and its practical feasibility is further explored viahardware-in-the-loop (HIL) simulations on embedded platforms.</description>
      <author>example@mail.com (Nuthasith Gerdpratoom, Kaoru Yamamoto)</author>
      <guid isPermaLink="false">2505.09434v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation</title>
      <link>http://arxiv.org/abs/2505.09564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at FIMH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于深度学习的猪心脏CT图像分割，提出了一种无需手动标注数据的自训练方法来迭代优化分割标签，以提高分割质量和减少时间上的不一致性。&lt;h4&gt;背景&lt;/h4&gt;心脏图像分割在心脏图像分析和建模任务中至关重要，而深度学习在临床环境中的分割取得了显著进展，但在猪模型等预临床成像中的应用研究有限。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型是否能够为猪心脏CT生成足够准确的伪标签，并提出一种简单的自训练方法来迭代优化这些标签。&lt;h4&gt;方法&lt;/h4&gt;提出的方法无需手动标注猪数据，而是通过迭代更新来提高分割质量，并通过自训练过程提高分割准确性和平滑连续帧之间的时间不一致性。&lt;h4&gt;主要发现&lt;/h4&gt;自训练过程不仅提高了分割准确度，还平滑了连续帧之间的时间不一致性。&lt;h4&gt;结论&lt;/h4&gt;尽管结果令人鼓舞，但仍存在改进空间，例如通过采用更复杂的自训练策略和探索更多的基础模型及其他心脏成像技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiac image segmentation is an important step in many cardiac imageanalysis and modeling tasks such as motion tracking or simulations of cardiacmechanics. While deep learning has greatly advanced segmentation in clinicalsettings, there is limited work on pre-clinical imaging, notably in porcinemodels, which are often used due to their anatomical and physiologicalsimilarity to humans. However, differences between species create a domainshift that complicates direct model transfer from human to pig data.  Recently, foundation models trained on large human datasets have shownpromise for robust medical image segmentation; yet their applicability toporcine data remains largely unexplored. In this work, we investigate whetherfoundation models can generate sufficiently accurate pseudo-labels for pigcardiac CT and propose a simple self-training approach to iteratively refinethese labels. Our method requires no manually annotated pig data, relyinginstead on iterative updates to improve segmentation quality. We demonstratethat this self-training process not only enhances segmentation accuracy butalso smooths out temporal inconsistencies across consecutive frames. Althoughour results are encouraging, there remains room for improvement, for example byincorporating more sophisticated self-training strategies and by exploringadditional foundation models and other cardiac imaging technologies.</description>
      <author>example@mail.com (Anne-Marie Rickmann, Stephanie L. Thorn, Shawn S. Ahn, Supum Lee, Selen Uman, Taras Lysyy, Rachel Burns, Nicole Guerrera, Francis G. Spinale, Jason A. Burdick, Albert J. Sinusas, James S. Duncan)</author>
      <guid isPermaLink="false">2505.09564v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting</title>
      <link>http://arxiv.org/abs/2505.09395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为量子参数自适应（QPA）的混合量子经典框架，用于高效的风暴轨迹预测模型学习，首次将量子机器学习（QML）应用于大规模风暴轨迹预测。&lt;h4&gt;背景&lt;/h4&gt;风暴轨迹预测对于灾害准备至关重要，但由于大气动力学复杂性和深度学习模型的资源需求，计算上具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的风暴轨迹预测方法。&lt;h4&gt;方法&lt;/h4&gt;利用量子神经网络（QNNs）在训练期间生成可训练参数的量子-Train（QT）框架，并将其与注意力机制的Multi-ConvGRU模型结合，实现参数高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;QPA显著减少了可训练参数的数量，同时保持了预测精度，使高性能预测更加可行和可持续。&lt;h4&gt;结论&lt;/h4&gt;QPA为气候建模提供了一种可扩展且节能的方法，是量子机器学习在风暴轨迹预测领域的首次应用，有望提高风暴预测的效率和可持续性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Typhoon trajectory forecasting is essential for disaster preparedness butremains computationally demanding due to the complexity of atmospheric dynamicsand the resource requirements of deep learning models. Quantum-Train (QT), ahybrid quantum-classical framework that leverages quantum neural networks(QNNs) to generate trainable parameters exclusively during training,eliminating the need for quantum hardware at inference time. Building on QT'ssuccess across multiple domains, including image classification, reinforcementlearning, flood prediction, and large language model (LLM) fine-tuning, weintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecastingmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPAenables parameter-efficient training while maintaining predictive accuracy.This work represents the first application of quantum machine learning (QML) tolarge-scale typhoon trajectory prediction, offering a scalable andenergy-efficient approach to climate modeling. Our results demonstrate that QPAsignificantly reduces the number of trainable parameters while preservingperformance, making high-performance forecasting more accessible andsustainable through hybrid quantum-classical learning.</description>
      <author>example@mail.com (Chen-Yu Liu, Kuan-Cheng Chen, Yi-Chien Chen, Samuel Yen-Chi Chen, Wei-Hao Huang, Wei-Jia Huang, Yen-Jui Chang)</author>
      <guid isPermaLink="false">2505.09395v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms</title>
      <link>http://arxiv.org/abs/2505.09103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种雷达惯性里程计，用于在恶劣条件下进行自主定位，并展示了其在处理稀疏和噪声雷达测量方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;4D雷达惯性里程计在恶劣条件下的自主定位具有潜在的应用价值，但其处理稀疏和噪声雷达测量的能力仍是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的雷达惯性里程计方法，以应对稀疏和噪声的雷达测量，并提高点云注册的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种空间加权方法，以适应不均匀分布的点；2. 提出了一种新的点描述直方图，用于挑战性的点注册；3. 提出了一个加权计算模型，以充分利用来自不同空间部分的Doppler速度；4. 构建了一个新的点直方图描述符，结合局部几何特征和雷达截面（RCS）特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过在公共和自建数据集上进行广泛实验，证明了所提出的VGC-RIO方法在精度和鲁棒性方面的优势。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理稀疏和噪声雷达测量以及提高点云注册性能方面是有效的，为4D雷达惯性里程计在恶劣条件下的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in 4D radar-inertial odometry have demonstrated promising potential for autonomous localization in adverse conditions. However, effective handling of sparse and noisy radar measurements remains a critical challenge. In this paper, we propose a radar-inertial odometry with a spatial weighting method that adapts to unevenly distributed points and a novel point-description histogram for challenging point registration. To make full use of the Doppler velocity from different spatial sections, we propose a weighting calculation model. To enhance the point cloud registration performance under challenging scenarios, we construct a novel point histogram descriptor that combines local geometric features and radar cross-section (RCS) features. We have also conducted extensive experiments on both public and self-constructed datasets. The results demonstrate the precision and robustness of the proposed VGC-RIO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D radar-inertial odometry  have demonstrated promising potential for autonomous lo calization in adverseconditions. However, effective handling  of sparse and noisy radar measurements remains a critical  challenge. In this paper, we propose a radar-inertial odometry  with a spatial weighting method that adapts to unevenly  distributed points and a novel point-description histogram  for challenging point registration. To make full use of the  Doppler velocity from different spatial sections, we propose  a weighting calculation model. To enhance the point cloud  registration performance under challenging scenarios, we con struct a novelpoint histogram descriptor that combines local  geometric features and radar cross-section (RCS) features. We  have also conducted extensive experiments on both public and  self-constructed datasets. The results demonstrate the precision  and robustness of the proposed VGC-RIO.</description>
      <author>example@mail.com (Jianguang Xiang, Xiaofeng He, Zizhuo Chen, Lilian Zhang, Xincan Luo, Jun Mao)</author>
      <guid isPermaLink="false">2505.09103v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Fast Learning in Quantitative Finance with Extreme Learning Machine</title>
      <link>http://arxiv.org/abs/2505.09551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文展示了使用单层神经网络（极值学习机，ELM）可以高效解决包括之前使用深度神经网络解决的问题在内的广泛类别的定量金融问题，无需迭代梯度训练。&lt;h4&gt;背景&lt;/h4&gt;定量金融领域中的许多问题之前使用深度神经网络解决，但这些方法通常需要复杂的训练过程。&lt;h4&gt;目的&lt;/h4&gt;探讨使用ELM在定量金融中的各种应用，并比较其与现有方法的性能。&lt;h4&gt;方法&lt;/h4&gt;ELM使用单层网络，具有随机初始化的隐藏节点，通过凸优化方法分析计算输出权重，实现快速训练和推理。在监督学习中，ELM用于学习参数化期权定价函数、预测日内股票回报和完成隐含波动率表面。在无监督学习中，ELM用于数值求解Black-Scholes型偏微分方程。&lt;h4&gt;主要发现&lt;/h4&gt;与深度神经网络、高斯过程回归和逻辑回归相比，ELM在计算速度、准确性和泛化能力方面表现优异。在无监督学习中，ELM在训练速度上优于物理信息神经网络，同时保持了精度。&lt;h4&gt;结论&lt;/h4&gt;ELM被确立为定量金融中各种任务的实用且高效工具。&lt;h4&gt;翻译&lt;/h4&gt;本文证明了在定量金融中，包括那些之前使用深度神经网络解决的问题在内的一系列问题，可以通过使用单层神经网络（极值学习机，ELM）而高效解决，且无需基于迭代的梯度训练。ELM利用一个具有随机初始化的隐藏节点的单层网络，通过凸优化方法获得分析计算得到的输出权重，从而实现快速的训练和推理。本文探讨了监督学习和无监督学习任务。在监督学习中，ELM被用于学习参数化期权定价函数、预测日内股票回报以及完成隐含波动率表面。与深度神经网络、高斯过程回归和逻辑回归相比，ELM在计算速度、准确性和泛化能力方面表现更优。在无监督学习中，ELM数值求解Black-Scholes型偏微分方程，且在训练速度上优于物理信息神经网络，同时保持了精度。本文简要讨论了ELM的逼近能力和泛化能力。研究结果确立了ELM作为定量金融中各种任务的实用且高效工具的地位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper demonstrates that a broad class of problems in quantitativefinance, including those previously addressed using deep neural networks, canbe efficiently solved using single-layer neural networks without iterativegradient-based training, namely extreme learning machine (ELM). ELM utilizes asingle-layer network with randomly initialized hidden nodes and analyticallycomputed output weights obtained via convex optimization, enabling rapidtraining and inference. Both supervised and unsupervised learning tasks areexplored.  In supervised learning, ELM is employed to learn parametric option pricingfunctions, predict intraday stock returns, and complete implied volatilitysurfaces. Compared with deep neural networks, Gaussian process regression, andlogistic regression, ELM achieves higher computational speed, comparableaccuracy, and superior generalization.  In unsupervised learning, ELM numerically solves Black-Scholes-type PDEs, andoutperforms Physics-Informed Neural Networks in training speed without losingprecision. The approximation and generalization abilities of ELM are brieflydiscussed.  The findings establish ELM as a practical and efficient tool for varioustasks in quantitative finance.</description>
      <author>example@mail.com (Liexin Cheng, Xue Cheng, Shuaiqiang Liu)</author>
      <guid isPermaLink="false">2505.09551v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Mixed Precision Quantization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.09361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了用于提高图神经网络（GNN）效率的混合精度量化方法，通过量化GNN层中的多种组件，实现了加速推理而不牺牲预测性能。&lt;h4&gt;背景&lt;/h4&gt;GNN在处理大规模图应用中变得至关重要，但其计算需求促使开发高效方法来加速推理。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的量化消息传递定理，并基于此定理开发混合精度量化框架（MixQ-GNN），以优化GNN层的效率。&lt;h4&gt;方法&lt;/h4&gt;引入一个定理，用于高效量化消息传递以聚合整数消息，并基于此定理开发MixQ-GNN框架，该框架灵活选择GNN层中所有组件的有效整数位宽。&lt;h4&gt;主要发现&lt;/h4&gt;MixQ-GNN与现有GNN量化方法集成，利用其图结构优势实现更高的预测性能，平均而言，在节点分类和图分类任务中，MixQ-GNN相比FP32精度的架构实现了5.5倍和5.1倍的位操作减少。&lt;h4&gt;结论&lt;/h4&gt;MixQ-GNN是一种有效的混合精度量化方法，可以显著提高GNN的推理效率，同时保持预测性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have become essential for handling large-scale graph applications. However, the computational demands of GNNs necessitate the development of efficient methods to accelerate inference. Mixed precision quantization emerges as a promising solution to enhance the efficiency of GNN architectures without compromising prediction performance. Compared to conventional deep learning architectures, GNN layers contain a wider set of components that can be quantized, including message passing functions, aggregation functions, update functions, the inputs, learnable parameters, and outputs of these functions. In this paper, we introduce a theorem for efficient quantized message passing to aggregate integer messages. It guarantees numerical equality of the aggregated messages using integer values with respect to those obtained with full (FP32) precision. Based on this theorem, we introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which flexibly selects effective integer bit-widths for all components within GNN layers. Our approach systematically navigates the wide set of possible bit-width combinations, addressing the challenge of optimizing efficiency while aiming at maintaining comparable prediction performance. MixQ-GNN integrates with existing GNN quantization methods, utilizing their graph structure advantages to achieve higher prediction performance. On average, MixQ-GNN achieved reductions in bit operations of 5.5x for node classification and 5.1x for graph classification compared to architectures represented in FP32 precision.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDE65448.2025.00301&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/samirmoustafa/mixq&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become essential for handling large-scalegraph applications. However, the computational demands of GNNs necessitate thedevelopment of efficient methods to accelerate inference. Mixed precisionquantization emerges as a promising solution to enhance the efficiency of GNNarchitectures without compromising prediction performance. Compared toconventional deep learning architectures, GNN layers contain a wider set ofcomponents that can be quantized, including message passing functions,aggregation functions, update functions, the inputs, learnable parameters, andoutputs of these functions. In this paper, we introduce a theorem for efficientquantized message passing to aggregate integer messages. It guaranteesnumerical equality of the aggregated messages using integer values with respectto those obtained with full (FP32) precision. Based on this theorem, weintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, whichflexibly selects effective integer bit-widths for all components within GNNlayers. Our approach systematically navigates the wide set of possiblebit-width combinations, addressing the challenge of optimizing efficiency whileaiming at maintaining comparable prediction performance. MixQ-GNN integrateswith existing GNN quantization methods, utilizing their graph structureadvantages to achieve higher prediction performance. On average, MixQ-GNNachieved reductions in bit operations of 5.5x for node classification and 5.1xfor graph classification compared to architectures represented in FP32precision.</description>
      <author>example@mail.com (Samir Moustafa, Nils M. Kriege, Wilfried N. Gansterer)</author>
      <guid isPermaLink="false">2505.09361v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Deployable and Generalizable Motion Prediction: Taxonomy, Open Challenges and Future Directions</title>
      <link>http://arxiv.org/abs/2505.09074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Initial draft, 162 pages, 40 figures, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了运动预测领域，包括其背景、目的、方法、主要发现和结论。&lt;h4&gt;背景&lt;/h4&gt;运动预测与人类认知有关，它连接感知和决策，使智能系统如机器人和自动驾驶汽车能够在动态、有人参与的环境中安全行动，并为更广泛的时间序列推理挑战提供信息。&lt;h4&gt;目的&lt;/h4&gt;为了解决研究基准与真实世界复杂性之间的差距，本文调查了运动预测模型的泛化性和可部署性，重点关注机器人、自动驾驶和人体运动的应用。&lt;h4&gt;方法&lt;/h4&gt;本文提供了一个关于运动预测方法的全面分类，包括表示、建模策略、应用领域和评估协议。同时，研究了两个关键挑战：如何使运动预测模型符合现实部署标准，以及如何将模型从有限的已见场景/数据集泛化到开放世界设置。&lt;h4&gt;主要发现&lt;/h4&gt;当最先进的方法在实际世界中应用时，它们往往难以泛化到开放世界条件，并且未能达到部署标准。&lt;h4&gt;结论&lt;/h4&gt;本文强调了运动预测领域的关键开放挑战，旨在指导未来的工作，使社区的努力不仅可衡量，而且对现实应用有意义。&lt;h4&gt;翻译&lt;/h4&gt;摘要：运动预测，即对未来代理状态或场景演变的预测，植根于人类认知，连接感知和决策。它使智能系统，如机器人和自动驾驶汽车，能够在动态、有人参与的环境中安全行动，并为更广泛的时间序列推理挑战提供信息。随着方法、表示和数据集的进步，该领域取得了快速进展，这反映在快速发展的基准结果中。然而，当最先进的方法在现实世界中部署时，它们往往难以泛化到开放世界条件，并且未能达到部署标准。这揭示了研究基准与真实世界复杂性之间的差距。为了解决这一差距，本文重新审视了运动预测模型的泛化性和可部署性，重点介绍了机器人、自动驾驶和人体运动的应用。首先，我们提供了一个关于运动预测方法的全面分类，包括表示、建模策略、应用领域和评估协议。然后，我们研究了两个关键挑战：（1）如何将运动预测模型推向符合现实部署标准的可部署性，其中运动预测不单独行动，而是作为闭环自主堆栈的一个模块发挥作用——它从定位和感知中获取输入，并告知下游规划和控制。（2）如何将运动预测模型从有限的已见场景/数据集泛化到开放世界设置。在整个论文中，我们强调了运动预测领域的关键开放挑战，旨在指导未来的工作，旨在调整社区的努力，促进不仅可衡量而且对现实应用有意义的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion prediction, the anticipation of future agent states or sceneevolution, is rooted in human cognition, bridging perception anddecision-making. It enables intelligent systems, such as robots andself-driving cars, to act safely in dynamic, human-involved environments, andinforms broader time-series reasoning challenges. With advances in methods,representations, and datasets, the field has seen rapid progress, reflected inquickly evolving benchmark results. Yet, when state-of-the-art methods aredeployed in the real world, they often struggle to generalize to open-worldconditions and fall short of deployment standards. This reveals a gap betweenresearch benchmarks, which are often idealized or ill-posed, and real-worldcomplexity.  To address this gap, this survey revisits the generalization anddeployability of motion prediction models, with an emphasis on the applicationsof robotics, autonomous driving, and human motion. We first offer acomprehensive taxonomy of motion prediction methods, covering representations,modeling strategies, application domains, and evaluation protocols. We thenstudy two key challenges: (1) how to push motion prediction models to bedeployable to realistic deployment standards, where motion prediction does notact in a vacuum, but functions as one module of closed-loop autonomy stacks -it takes input from the localization and perception, and informs downstreamplanning and control. 2) how to generalize motion prediction models fromlimited seen scenarios/datasets to the open-world settings. Throughout thepaper, we highlight critical open challenges to guide future work, aiming torecalibrate the community's efforts, fostering progress that is not onlymeasurable but also meaningful for real-world applications.</description>
      <author>example@mail.com (Letian Wang, Marc-Antoine Lavoie, Sandro Papais, Barza Nisar, Yuxiao Chen, Wenhao Ding, Boris Ivanovic, Hao Shao, Abulikemu Abuduweili, Evan Cook, Yang Zhou, Peter Karkus, Jiachen Li, Changliu Liu, Marco Pavone, Steven Waslander)</author>
      <guid isPermaLink="false">2505.09074v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes</title>
      <link>http://arxiv.org/abs/2505.09129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 3 figures, 3 tables. The paper proposes a lightweight  weakly-supervised color intelligence model for tactical video anomaly  detection, tested on anonymized African surveillance data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于颜色特征的轻量级异常检测框架，用于在无标签、数据不可用的视频智能环境中快速识别和解释潜在威胁事件。&lt;h4&gt;背景&lt;/h4&gt;在无标签、数据不可用的视频智能环境中部署传统深度学习模型在高风险安全任务中面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在为高敏感性战术任务中的监控视频片段快速识别和解释潜在威胁事件。&lt;h4&gt;方法&lt;/h4&gt;该方法融合了无监督的KMeans聚类与RGB通道直方图建模，以实现关键帧中的结构异常和颜色突变信号的复合检测。&lt;h4&gt;主要发现&lt;/h4&gt;实验使用非洲国家的一项操作监控视频作为研究样本，在无法访问原始数据的情况下成功识别了与高能光源、目标存在和反射干扰相关的多个高度异常帧。&lt;h4&gt;结论&lt;/h4&gt;该结果表明，该方法可以有效用于战术暗杀预警、可疑物体筛查和环境剧变监测，具有较强的部署性和战术解释价值。&lt;h4&gt;翻译&lt;/h4&gt;The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operational surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of traditional deep learning models in high-risk securitytasks in an unlabeled, data-non-exploitable video intelligence environmentfaces significant challenges. In this paper, we propose a lightweight anomalydetection framework based on color features for surveillance video clips in ahigh sensitivity tactical mission, aiming to quickly identify and interpretpotential threat events under resource-constrained and data-sensitiveconditions. The method fuses unsupervised KMeans clustering with RGB channelhistogram modeling to achieve composite detection of structural anomalies andcolor mutation signals in key frames. The experiment takes an operationsurveillance video occurring in an African country as a research sample, andsuccessfully identifies multiple highly anomalous frames related to high-energylight sources, target presence, and reflective interference under the conditionof no access to the original data. The results show that this method can beeffectively used for tactical assassination warning, suspicious objectscreening and environmental drastic change monitoring with strong deployabilityand tactical interpretation value. The study emphasizes the importance of colorfeatures as low semantic battlefield signal carriers, and its battlefieldintelligent perception capability will be further extended by combining graphneural networks and temporal modeling in the future.</description>
      <author>example@mail.com (Wei Meng)</author>
      <guid isPermaLink="false">2505.09129v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories</title>
      <link>http://arxiv.org/abs/2505.09239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures, includes analytical proofs, sensitivity  analysis (95% CI), and JAX-based open-source implementation available at:  https://github.com/farukalpay/information-bottleneck-beta-optimization&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过符号连续性和熵正则化轨迹来实现信息瓶颈（IB）方法的稳定和凸优化。&lt;h4&gt;背景&lt;/h4&gt;信息瓶颈（IB）方法在优化过程中常常遇到不稳定的问题，特别是在IB权衡参数beta的关键点附近会出现表示的突然变化。&lt;h4&gt;目的&lt;/h4&gt;旨在通过引入新的方法，实现IB优化的稳定性和凸性。&lt;h4&gt;方法&lt;/h4&gt;使用符号连续性和熵正则化轨迹进行优化，并提供了关于关键点（beta）的广泛敏感性分析，以及具有统计稳健不确定性量化（95%置信区间）的分析。&lt;h4&gt;主要发现&lt;/h4&gt;证明了包含熵正则化项时IB解路径的凸性和唯一性，并展示了这种方法如何稳定表示学习，即使在广泛的beta值范围内。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法提供了一个清晰的路径，用于实际部署和未来扩展所提出的方法。&lt;h4&gt;翻译&lt;/h4&gt;The Information Bottleneck (IB) method frequently suffers from unstable optimization, characterized by abrupt representation shifts near critical points of the IB trade-off parameter, beta. In this paper, I introduce a novel approach to achieve stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories. I analytically prove convexity and uniqueness of the IB solution path when an entropy regularization term is included, and demonstrate how this stabilizes representation learning across a wide range of {eta} values. Additionally, I provide extensive sensitivity analyses around critical points (beta) with statistically robust uncertainty quantification (95% confidence intervals). The open-source implementation, experimental results, and reproducibility framework included in this work offer a clear path for practical deployment and future extension of my proposed method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/farukalpay/information-bottleneck-beta-optimization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Information Bottleneck (IB) method frequently suffers from unstableoptimization, characterized by abrupt representation shifts near criticalpoints of the IB trade-off parameter, beta. In this paper, I introduce a novelapproach to achieve stable and convex IB optimization through symboliccontinuation and entropy-regularized trajectories. I analytically proveconvexity and uniqueness of the IB solution path when an entropy regularizationterm is included, and demonstrate how this stabilizes representation learningacross a wide range of \b{eta} values. Additionally, I provide extensivesensitivity analyses around critical points (beta) with statistically robustuncertainty quantification (95% confidence intervals). The open-sourceimplementation, experimental results, and reproducibility framework included inthis work offer a clear path for practical deployment and future extension ofmy proposed method.</description>
      <author>example@mail.com (Faruk Alpay)</author>
      <guid isPermaLink="false">2505.09239v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records</title>
      <link>http://arxiv.org/abs/2505.09435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Early accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Endo-CLIP是一种针对内镜图像分析的自监督框架，通过解决非信息性背景图像、复杂医学术语和模糊的多息肉描述等挑战，显著提高了息肉检测和分类的准确性。&lt;h4&gt;背景&lt;/h4&gt;图像文本内镜记录的预训练在提高内镜图像分析方面具有巨大潜力，但面临着非信息性背景图像、复杂医学术语和模糊的多息肉描述等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Endo-CLIP框架，以解决内镜图像分析中的挑战，并提高息肉检测和分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;Endo-CLIP采用三阶段框架：清除、调谐和统一。通过清除背景帧，利用大型语言模型提取临床属性进行细粒度对比学习，以及使用患者级别的交叉注意力来解决多息肉模糊性。&lt;h4&gt;主要发现&lt;/h4&gt;Endo-CLIP在零样本和少样本息肉检测和分类中显著优于最先进的预训练方法。&lt;h4&gt;结论&lt;/h4&gt;Endo-CLIP为更准确和具有临床相关性的内镜分析铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training on image-text colonoscopy records offers substantial potentialfor improving endoscopic image analysis, but faces challenges includingnon-informative background images, complex medical terminology, and ambiguousmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervisedframework that enhances Contrastive Language-Image Pre-training (CLIP) for thisdomain. Endo-CLIP's three-stage framework--cleansing, attunement, andunification--addresses these challenges by (1) removing background frames, (2)leveraging large language models to extract clinical attributes forfine-grained contrastive learning, and (3) employing patient-levelcross-attention to resolve multi-polyp ambiguities. Extensive experimentsdemonstrate that Endo-CLIP significantly outperforms state-of-the-artpre-training methods in zero-shot and few-shot polyp detection andclassification, paving the way for more accurate and clinically relevantendoscopic analysis.</description>
      <author>example@mail.com (Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang)</author>
      <guid isPermaLink="false">2505.09435v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection</title>
      <link>http://arxiv.org/abs/2505.09168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DRRNet的伪装目标检测（COD）方法，旨在解决目标与背景在颜色、纹理和形状上的相似性问题。&lt;h4&gt;背景&lt;/h4&gt;当前COD方法在处理目标与背景相似性时，要么因过度依赖全局语义信息而丢失边缘细节，要么因仅依赖局部特征而被相似背景（如植被图案）干扰。&lt;h4&gt;目的&lt;/h4&gt;提出DRRNet以解决上述问题，实现更精确的伪装目标检测。&lt;h4&gt;方法&lt;/h4&gt;DRRNet采用“上下文-细节融合-细化”的四阶段架构，包括：全局伪装模式提取模块、局部细节提取模块以及形成场景理解和结构感知的双代表模块。解码器中引入反向细化模块，利用空间边缘先验和频域噪声抑制进行两阶段逆细化。&lt;h4&gt;主要发现&lt;/h4&gt;通过两轮逆细化，DRRNet有效抑制了背景干扰并增强了目标边界连续性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，DRRNet在基准数据集上显著优于现有方法。代码可在https://github.com/jerrySunning/DRRNet获取。&lt;h4&gt;翻译&lt;/h4&gt;The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a 'context-detail-fusion-refinement' pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jerrysunning/drrnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The core challenge in Camouflage Object Detection (COD) lies in theindistinguishable similarity between targets and backgrounds in terms of color,texture, and shape. This causes existing methods to either lose edge details(such as hair-like fine structures) due to over-reliance on global semanticinformation or be disturbed by similar backgrounds (such as vegetationpatterns) when relying solely on local features. We propose DRRNet, afour-stage architecture characterized by a "context-detail-fusion-refinement"pipeline to address these issues. Specifically, we introduce an Omni-ContextFeature Extraction Module to capture global camouflage patterns and a LocalDetail Extraction Module to supplement microstructural information for thefull-scene context module. We then design a module for forming dualrepresentations of scene understanding and structural awareness, which fusespanoramic features and local features across various scales. In the decoder, wealso introduce a reverse refinement module that leverages spatial edge priorsand frequency-domain noise suppression to perform a two-stage inverserefinement of the output. By applying two successive rounds of inverserefinement, the model effectively suppresses background interference andenhances the continuity of object boundaries. Experimental results demonstratethat DRRNet significantly outperforms state-of-the-art methods on benchmarkdatasets. Our code is available at https://github.com/jerrySunning/DRRNet.</description>
      <author>example@mail.com (Jianlin Sun, Xiaolin Fang, Juwei Guan, Dongdong Gui, Teqi Wang, Tongxin Zhu)</author>
      <guid isPermaLink="false">2505.09168v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.09422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MoRAL的动态感知多帧4D雷达和LiDAR融合框架，用于鲁棒的3D物体检测。&lt;h4&gt;背景&lt;/h4&gt;可靠的自动驾驶系统需要准确检测交通参与者，多模态融合已成为一种有效策略。&lt;h4&gt;目的&lt;/h4&gt;提高4D雷达和LiDAR融合在3D物体检测中的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个运动感知雷达编码器（MRE）来补偿移动物体引起的帧间雷达失准，并引入了一个运动注意力门控融合（MAGF）模块，将雷达运动特征整合以引导LiDAR特征关注动态前景物体。&lt;h4&gt;主要发现&lt;/h4&gt;在View-of-Delft（VoD）数据集上的广泛评估表明，MoRAL优于现有方法，在整个区域和驾驶通道中分别实现了73.30%和88.68%的最高mAP。特别值得注意的是，该方法在整个区域和驾驶通道中分别实现了69.67%和96.25%的最佳AP，针对行人和骑自行车者。&lt;h4&gt;结论&lt;/h4&gt;MoRAL框架在3D物体检测中表现出色，特别是在检测动态前景物体方面具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable autonomous driving systems require accurate detection of trafficparticipants. To this end, multi-modal fusion has emerged as an effectivestrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frameradar point clouds have demonstrated the effectiveness in bridging the pointdensity gap. However, they often neglect radar point clouds' inter-framemisalignment caused by object movement during accumulation and do not fullyexploit the object dynamic information from 4D radar. In this paper, we proposeMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework forrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) isdesigned to compensate for inter-frame radar misalignment from moving objects.Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motionfeatures to guide LiDAR features to focus on dynamic foreground objects.Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRALoutperforms existing methods, achieving the highest mAP of 73.30% in the entirearea and 88.68% in the driving corridor. Notably, our method also achieves thebest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists inthe driving corridor.</description>
      <author>example@mail.com (Xiangyuan Peng, Yu Wang, Miao Tang, Bierzynski Kay, Lorenzo Servadei, Robert Wille)</author>
      <guid isPermaLink="false">2505.09422v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</title>
      <link>http://arxiv.org/abs/2505.09160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WiMAE和ContraWiMAE两种基于自我监督学习的无线信道表示方法，通过引入对比学习，提升了无线信道表示的学习效果。&lt;h4&gt;背景&lt;/h4&gt;目前应用于无线信道表示的自我监督学习方法往往借鉴了文本和图像处理的方法，未能充分考虑无线通信的独特特性和约束。&lt;h4&gt;目的&lt;/h4&gt;填补无线通信领域自我监督学习方法的空白。&lt;h4&gt;方法&lt;/h4&gt;1. 提出WiMAE，基于Transformer的编码器-解码器基础模型，在真实开源多天线无线信道数据集上预训练。2. 基于WiMAE，开发ContraWiMAE，通过引入对比学习目标，在统一的多任务框架中提升WiMAE的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. WiMAE和ContraWiMAE在多个下游任务中表现出有效性，ContraWiMAE在线性可分性和适应多样性无线环境方面有进一步的提升。2. 与最先进的无线信道基础模型相比，本文提出的模型具有优越的性能和数据效率。&lt;h4&gt;结论&lt;/h4&gt;WiMAE和ContraWiMAE是自我监督无线信道表示学习的有效方法，有望成为未来研究的有力基准。&lt;h4&gt;翻译&lt;/h4&gt;The paper proposes two self-supervised learning methods for wireless channel representation, WiMAE and ContraWiMAE, which enhance the learning effect of wireless channel representation by introducing contrastive learning. WiMAE and ContraWiMAE are effective methods for self-supervised wireless channel representation learning, and are expected to become powerful baselines for future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current applications of self-supervised learning to wireless channelrepresentation often borrow paradigms developed for text and image processing,without fully addressing the unique characteristics and constraints of wirelesscommunications. Aiming to fill this gap, we first propose WiMAE (WirelessMasked Autoencoder), a transformer-based encoder-decoder foundation modelpretrained on a realistic open-source multi-antenna wireless channel dataset.Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE byincorporating a contrastive learning objective alongside the reconstructiontask in a unified multi-task framework. By warm-starting from pretrained WiMAEweights and generating positive pairs via noise injection, the contrastivecomponent enables the model to capture both structural and discriminativefeatures, enhancing representation quality beyond what reconstruction alone canachieve. Through extensive evaluation on unseen scenarios, we demonstrate theeffectiveness of both approaches across multiple downstream tasks, withContraWiMAE showing further improvements in linear separability andadaptability in diverse wireless environments. Comparative evaluations againsta state-of-the-art wireless channel foundation model confirm the superiorperformance and data efficiency of our models, highlighting their potential aspowerful baselines for future research in self-supervised wireless channelrepresentation learning.</description>
      <author>example@mail.com (Berkay Guler, Giovanni Geraci, Hamid Jafarkhani)</author>
      <guid isPermaLink="false">2505.09160v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fair In-Context Learning with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2505.09503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了表格基础模型在结构化数据上的强上下文学习能力，并探讨了其在预测准确性方面的优势与潜在偏见问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型在结构化数据上表现出强大的上下文学习能力，可以不更新参数就准确预测测试集，这使其成为传统梯度提升树方法的竞争者。&lt;h4&gt;目的&lt;/h4&gt;论文旨在探究表格上下文学习能力中的公平性影响，并评估三种预处理策略的有效性。&lt;h4&gt;方法&lt;/h4&gt;论文采用了三种预处理策略：相关性去除、分组平衡演示选择和基于不确定性的演示选择，以减轻偏见。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于不确定性的演示选择策略能够持续提升上下文预测的群体公平性。&lt;h4&gt;结论&lt;/h4&gt;通过使用基于不确定性的演示选择策略，可以有效提升表格上下文学习能力中的群体公平性。&lt;h4&gt;翻译&lt;/h4&gt;Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundational models have exhibited strong in-context learning (ICL)capabilities on structured data, allowing them to make accurate predictions ontest sets without parameter updates, using training examples as context. Thisemerging approach positions itself as a competitive alternative to traditionalgradient-boosted tree methods. However, while biases in conventional machinelearning models are well documented, it remains unclear how these biasesmanifest in tabular ICL. The paper investigates the fairness implications oftabular ICL and explores three preprocessing strategies--correlation removal,group-balanced demonstration selection, and uncertainty-based demonstrationselection--to address bias. Comprehensive experiments indicate thatuncertainty-based demonstration selection consistently enhances group fairnessof in-context predictions. The source code for reproducing the results of thiswork can be found at https://github.com/patrikken/Fair-TabICL.</description>
      <author>example@mail.com (Patrik Kenfack, Samira Ebrahimi Kaho, Ulrich Aïvodji)</author>
      <guid isPermaLink="false">2505.09503v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis</title>
      <link>http://arxiv.org/abs/2505.09358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal extension of our CVPR 2024 paper, featuring new tasks,  improved efficiency, high-resolution capabilities, and enhanced accessibility&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Marigold，一套条件生成模型和微调协议，该协议能够从预训练的潜在扩散模型（如Stable Diffusion）中提取知识，并将其应用于密集图像分析任务，包括单目深度估计、表面法线预测和内在分解。&lt;h4&gt;背景&lt;/h4&gt;深度学习在计算机视觉领域的成功依赖于大量标记数据集和强大的预训练模型。在数据稀缺的环境中，这些预训练模型的质量对于有效的迁移学习至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出Marigold模型和微调协议，以解决数据稀缺情况下的密集图像分析任务。&lt;h4&gt;方法&lt;/h4&gt;Marigold模型通过最小修改预训练潜在扩散模型的架构，使用小规模合成数据集在单个GPU上训练几天，并实现最先进的零样本泛化。&lt;h4&gt;主要发现&lt;/h4&gt;Marigold模型能够生成未见内容的真实图像，表明其具有对视觉世界的深入理解。&lt;h4&gt;结论&lt;/h4&gt;Marigold模型在密集图像分析任务中展示了出色的性能，为数据稀缺环境下的计算机视觉应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of deep learning in computer vision over the past decade hashinged on large labeled datasets and strong pretrained models. In data-scarcesettings, the quality of these pretrained models becomes crucial for effectivetransfer learning. Image classification and self-supervised learning havetraditionally been the primary methods for pretraining CNNs andtransformer-based architectures. Recently, the rise of text-to-image generativemodels, particularly those using denoising diffusion in a latent space, hasintroduced a new class of foundational models trained on massive, captionedimage datasets. These models' ability to generate realistic images of unseencontent suggests they possess a deep understanding of the visual world. In thiswork, we present Marigold, a family of conditional generative models and afine-tuning protocol that extracts the knowledge from pretrained latentdiffusion models like Stable Diffusion and adapts them for dense image analysistasks, including monocular depth estimation, surface normals prediction, andintrinsic decomposition. Marigold requires minimal modification of thepre-trained latent diffusion model's architecture, trains with small syntheticdatasets on a single GPU over a few days, and demonstrates state-of-the-artzero-shot generalization. Project page:https://marigoldcomputervision.github.io</description>
      <author>example@mail.com (Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler)</author>
      <guid isPermaLink="false">2505.09358v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Efficient LiDAR Reflectance Compression via Scanning Serialization</title>
      <link>http://arxiv.org/abs/2505.09433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于序列化的神经网络压缩框架SerLiC，用于充分利用激光雷达反射率的内在特性。&lt;h4&gt;背景&lt;/h4&gt;激光雷达点云中的反射率属性对于下游任务至关重要，但在神经网络压缩方法中尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出SerLiC框架，以解决激光雷达反射率在神经网络压缩方法中的利用不足问题。&lt;h4&gt;方法&lt;/h4&gt;SerLiC首先通过扫描顺序序列化将3D激光雷达点云转换为1D序列，然后对每个点进行标记，形成包含其传感器扫描索引、径向距离和先前反射率的上下文表示。为了高效地进行序列建模，SerLiC结合了Mamba和双并行化方案，以实现同时自回归依赖关系捕获和快速处理。&lt;h4&gt;主要发现&lt;/h4&gt;SerLiC在压缩比特数上比原始反射率数据减少了超过2倍，比现有最佳方法减少了高达22%的压缩比特数，同时仅使用了其2%的参数。此外，SerLiC的轻量级版本只需111K个参数即可实现超过10 fps的帧率，这对于实际应用具有吸引力。&lt;h4&gt;结论&lt;/h4&gt;SerLiC是一种有效的神经网络压缩框架，能够显著减少激光雷达反射率的压缩比特数，同时保持较高的处理速度，适用于实际应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reflectance attributes in LiDAR point clouds provide essential informationfor downstream tasks but remain underexplored in neural compression methods. Toaddress this, we introduce SerLiC, a serialization-based neural compressionframework to fully exploit the intrinsic characteristics of LiDAR reflectance.SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-orderserialization, offering a device-centric perspective for reflectance analysis.Each point is then tokenized into a contextual representation comprising itssensor scanning index, radial distance, and prior reflectance, for effectivedependencies exploration. For efficient sequential modeling, Mamba isincorporated with a dual parallelization scheme, enabling simultaneousautoregressive dependency capture and fast processing. Extensive experimentsdemonstrate that SerLiC attains over 2x volume reduction against the originalreflectance data, outperforming the state-of-the-art method by up to 22%reduction of compressed bits while using only 2% of its parameters. Moreover, alightweight version of SerLiC achieves &gt; 10 fps (frames per second) with just111K parameters, which is attractive for real-world applications.</description>
      <author>example@mail.com (Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma)</author>
      <guid isPermaLink="false">2505.09433v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Quotient Complex Transformer (QCformer) for Perovskite Data Analysis</title>
      <link>http://arxiv.org/abs/2505.09174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于商复形的新颖表示方法，并引入了商复形Transformer（QCformer）模型进行材料性质预测，用于解决传统GNN在处理周期结构和高级别相互作用时的局限性。&lt;h4&gt;背景&lt;/h4&gt;新型功能材料的发现对于解决可持续能源生成和气候变化挑战至关重要。混合有机-无机钙钛矿（HOIPs）因其卓越的光电特性在光伏领域受到关注。几何深度学习，特别是图神经网络（GNNs），在预测材料性质和指导材料设计方面显示出强大潜力。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统GNN在处理周期结构和高级别相互作用时的局限性，提出了一种基于商复形的新颖表示方法，并引入QCformer模型进行材料性质预测。&lt;h4&gt;方法&lt;/h4&gt;将材料结构建模为商复形，通过不同维度的单纯形编码成对和多重相互作用，并通过商运算捕捉材料周期性。模型利用单纯形上的高级别特征，并通过基于单纯形的Transformer模块进行处理。在基准数据集（如材料项目和JARVIS）上预训练QCformer，并在HOIP数据集上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;QCformer在HOIP性质预测中优于现有模型，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;商复形表示和QCformer模型为钙钛矿材料的预测建模提供了一个强大的新工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：新型功能材料的发现对于解决可持续能源生成和气候变化挑战至关重要。混合有机-无机钙钛矿（HOIPs）因其卓越的光电特性在光伏领域受到关注。最近，几何深度学习，特别是图神经网络（GNNs），在预测材料性质和指导材料设计方面显示出强大潜力。然而，传统的GNN在处理这种系统中普遍存在的周期结构和高级别相互作用时往往存在困难。为了解决这些限制，我们提出了一种基于商复形（QCs）的新颖表示方法，并引入了商复形Transformer（QCformer）用于材料性质预测。一个材料结构被建模为一个商复形，通过不同维度的单纯形编码成对和多重相互作用，并通过商运算捕捉材料周期性。我们的模型利用在单纯形上定义的高级别特征，并通过基于单纯形的Transformer模块进行处理。我们在基准数据集（如材料项目和JARVIS）上预训练了QCformer，并在HOIP数据集上进行了微调。结果表明，QCformer在HOIP性质预测中优于现有模型，证明了其有效性。商复形表示和QCformer模型共同为钙钛矿材料的预测建模提供了一个强大的新工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of novel functional materials is crucial in addressing thechallenges of sustainable energy generation and climate change. Hybridorganic-inorganic perovskites (HOIPs) have gained attention for theirexceptional optoelectronic properties in photovoltaics. Recently, geometricdeep learning, particularly graph neural networks (GNNs), has shown strongpotential in predicting material properties and guiding material design.However, traditional GNNs often struggle to capture the periodic structures andhigher-order interactions prevalent in such systems. To address theselimitations, we propose a novel representation based on quotient complexes(QCs) and introduce the Quotient Complex Transformer (QCformer) for materialproperty prediction. A material structure is modeled as a quotient complex,which encodes both pairwise and many-body interactions via simplices of varyingdimensions and captures material periodicity through a quotient operation. Ourmodel leverages higher-order features defined on simplices and processes themusing a simplex-based Transformer module. We pretrain QCformer on benchmarkdatasets such as the Materials Project and JARVIS, and fine-tune it on HOIPdatasets. The results show that QCformer outperforms state-of-the-art models inHOIP property prediction, demonstrating its effectiveness. The quotient complexrepresentation and QCformer model together contribute a powerful new tool forpredictive modeling of perovskite materials.</description>
      <author>example@mail.com (Xinyu You, Xiang Liu, Chuan-Shen Hu, Kelin Xia, Tze Chien Sum)</author>
      <guid isPermaLink="false">2505.09174v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities</title>
      <link>http://arxiv.org/abs/2505.09477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE ICRA Workshop on Field Robotics 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了将基础模型（FMs）集成到机器人中，使机器人能够理解自然语言并对其环境中的语义进行推理。文章主要关注在封闭世界设置之外的开放世界中部署FM赋能的机器人。&lt;h4&gt;背景&lt;/h4&gt;现有的FM赋能机器人在封闭世界中操作，这些机器人通常拥有完整的先验地图或对其工作空间有全面视野。&lt;h4&gt;目的&lt;/h4&gt;为了在大型和未结构化的环境中有效完成任务，机器人必须积极探索其环境，导航障碍物丛生的地形，处理意外的传感器输入，并在计算限制下操作。&lt;h4&gt;方法&lt;/h4&gt;本文讨论了在实地机器人设置中最近部署的SPINE（我们的LLM赋能自主框架）。通过初步的模型精炼工作，提出了第一个使用设备上语言模型的驱动UAV规划器。&lt;h4&gt;主要发现&lt;/h4&gt;文章提供了大规模LLM赋能的机器人规划在未结构化环境中的第一个演示，这些任务覆盖了数公里。SPINE对特定的LLM无依赖，这使我们能够提炼出适合在尺寸、重量和功率（SWaP）受限的平台运行的微小语言模型。&lt;h4&gt;结论&lt;/h4&gt;论文最后提出了未来研究的几个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;该论文探讨了将基础模型（FMs）集成到机器人中，使机器人能够理解自然语言并对其环境中的语义进行推理。然而，现有的FM赋能机器人主要在封闭世界中操作，这些机器人通常拥有完整的先验地图或对其工作空间有全面视野。本文旨在解决在开放世界中部署FM赋能机器人所面临的问题，其中任务通常要求机器人能够在大型和未结构化的环境中操作。为了有效地完成这些任务，机器人必须积极探索其环境，导航障碍物丛生的地形，处理意外的传感器输入，并在计算限制下操作。本文讨论了在实地机器人设置中最近部署的SPINE（我们的LLM赋能自主框架）。据我们所知，我们展示了在未结构化环境中进行大规模LLM赋能的机器人规划的第一个实例，这些任务覆盖了数公里。SPINE对特定的LLM无依赖，这使得我们能够提炼出适合在尺寸、重量和功率（SWaP）受限的平台运行的微小语言模型。通过初步的模型精炼工作，我们进一步展示了第一个使用设备上语言模型的驱动UAV规划器。本文最后提出了未来研究的几个有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of foundation models (FMs) into robotics has enabled robotsto understand natural language and reason about the semantics in theirenvironments. However, existing FM-enabled robots primary operate inclosed-world settings, where the robot is given a full prior map or has a fullview of its workspace. This paper addresses the deployment of FM-enabled robotsin the field, where missions often require a robot to operate in large-scaleand unstructured environments. To effectively accomplish these missions, robotsmust actively explore their environments, navigate obstacle-cluttered terrain,handle unexpected sensor inputs, and operate with compute constraints. Wediscuss recent deployments of SPINE, our LLM-enabled autonomy framework, infield robotic settings. To the best of our knowledge, we present the firstdemonstration of large-scale LLM-enabled robot planning in unstructuredenvironments with several kilometers of missions. SPINE is agnostic to aparticular LLM, which allows us to distill small language models capable ofrunning onboard size, weight and power (SWaP) limited platforms. Viapreliminary model distillation work, we then present the first language-drivenUAV planner using on-device language models. We conclude our paper by proposingseveral promising directions for future research.</description>
      <author>example@mail.com (Zachary Ravichandran, Fernando Cladera, Jason Hughes, Varun Murali, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar)</author>
      <guid isPermaLink="false">2505.09477v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Interdependent Cybersecurity Threats Using Bayesian Networks: A Case Study on In-Vehicle Infotainment Systems</title>
      <link>http://arxiv.org/abs/2505.09048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了贝叶斯网络在网络安全风险评估中的应用，强调了其在表示概率依赖性、整合多种威胁指标和支持不确定条件下的推理方面的能力。&lt;h4&gt;背景&lt;/h4&gt;随着网络安全威胁的相互依赖性、不确定性和复杂性的不断演变，传统的评估方法如CVSS、STRIDE和攻击树无法充分捕捉。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示贝叶斯网络在网络安全风险评估中的潜力，并讨论其应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;通过一个结构化案例研究，将基于STRIDE的攻击树转换为贝叶斯网络，使用条件概率表（CPTs）编码逻辑关系，并从标准化DREAD评分中推导威胁可能性。&lt;h4&gt;主要发现&lt;/h4&gt;该模型不仅能够进行系统妥协的可能性概率推理，还支持使用do-calculus和局部敏感性分析进行因果分析，以识别高影响漏洞，从而为威胁传播链中最具影响力的节点提供见解。&lt;h4&gt;结论&lt;/h4&gt;研究认为，通过动态贝叶斯网络、结构学习和自适应推理的未来改进，可以更好地支持复杂环境中的实时网络安全决策。&lt;h4&gt;翻译&lt;/h4&gt;摘要：网络安全威胁日益显现出相互依赖性、不确定性和演变的复杂性挑战，传统的评估方法如CVSS、STRIDE和攻击树无法充分捕捉这些特点。本文回顾了贝叶斯网络（BNs）在网络安全风险评估中的应用，强调了其表示概率依赖性、整合多种威胁指标和支持不确定条件下推理的能力。本文提出的一个结构化案例研究中，将基于STRIDE的攻击树转换为一个贝叶斯网络。使用条件概率表（CPTs）编码逻辑关系，并从标准化的DREAD评分中推导出威胁的可能性。该模型不仅能够进行系统妥协的可能性的概率推理，还支持使用do-calculus和局部敏感性分析进行因果分析，以识别高影响漏洞。这些分析为威胁传播链中最具影响力的节点提供了见解，从而为针对性的缓解策略提供了信息。虽然展示了贝叶斯网络在动态和上下文感知风险评估中的潜力，但该研究也概述了与可扩展性、对专家输入的依赖、静态结构假设以及有限的时间建模相关的局限性。本文最后倡导通过动态贝叶斯网络、结构学习和自适应推理的未来改进，以更好地支持复杂环境中的实时网络安全决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cybersecurity threats are increasingly marked by interdependence,uncertainty, and evolving complexity challenges that traditional assessmentmethods such as CVSS, STRIDE, and attack trees fail to adequately capture. Thispaper reviews the application of Bayesian Networks (BNs) in cybersecurity riskmodeling, highlighting their capacity to represent probabilistic dependencies,integrate diverse threat indicators, and support reasoning under uncertainty. Astructured case study is presented in which a STRIDE-based attack tree for anautomotive In-Vehicle Infotainment (IVI) system is transformed into a BayesianNetwork. Logical relationships are encoded using Conditional Probability Tables(CPTs), and threat likelihoods are derived from normalized DREAD scores. Themodel enables not only probabilistic inference of system compromise likelihoodbut also supports causal analysis using do-calculus and local sensitivityanalysis to identify high-impact vulnerabilities. These analyses provideinsight into the most influential nodes within the threat propagation chain,informing targeted mitigation strategies. While demonstrating the potential ofBNs for dynamic and context-aware risk assessment, the study also outlineslimitations related to scalability, reliance on expert input, static structureassumptions, and limited temporal modeling. The paper concludes by advocatingfor future enhancements through Dynamic Bayesian Networks, structure learning,and adaptive inference to better support real-time cybersecuritydecision-making in complex environments.</description>
      <author>example@mail.com (Sangita Sridar)</author>
      <guid isPermaLink="false">2505.09048v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment</title>
      <link>http://arxiv.org/abs/2505.09372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI2025 early acceptance; First two authors contribute equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MAKE的多方面知识增强视觉语言预训练框架，用于零样本皮肤病学任务，该框架通过综合视觉特征与临床知识来提升皮肤病学诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;皮肤病学诊断是一个复杂的多元挑战，需要结合视觉特征和专业知识。虽然视觉语言预训练（VLP）在医疗人工智能方面取得了进展，但其应用于皮肤病学受限于文本长度和缺乏结构化文本。&lt;h4&gt;目的&lt;/h4&gt;提出MAKE框架，以提升零样本皮肤病学任务中的诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;MAKE框架包含：（1）多方面对比学习策略，通过大型语言模型将临床叙事分解为知识增强的子文本；（2）细粒度对齐机制，将子字幕与诊断相关的图像特征相连接；（3）诊断引导的权重方案，根据临床重要性自适应地优先考虑不同的子字幕。&lt;h4&gt;主要发现&lt;/h4&gt;通过在403,563个皮肤病学图像-文本对上进行预训练，MAKE在八项数据集上的零样本皮肤疾病分类、概念注释和跨模态检索任务中，显著优于最先进的VLP模型。&lt;h4&gt;结论&lt;/h4&gt;MAKE框架在提升皮肤病学诊断的准确性方面表现出色，其代码将在https://github.com/SiyuanYan1/MAKE上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/siyuanyan1/make&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dermatological diagnosis represents a complex multimodal challenge thatrequires integrating visual features with specialized clinical knowledge. Whilevision-language pretraining (VLP) has advanced medical AI, its effectiveness indermatology is limited by text length constraints and the lack of structuredtexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhancedvision-language pretraining framework for zero-shot dermatological tasks.Recognizing that comprehensive dermatological descriptions require multipleknowledge aspects that exceed standard text constraints, our frameworkintroduces: (1) a multi-aspect contrastive learning strategy that decomposesclinical narratives into knowledge-enhanced sub-texts through large languagemodels, (2) a fine-grained alignment mechanism that connects subcaptions withdiagnostically relevant image features, and (3) a diagnosis-guided weightingscheme that adaptively prioritizes different sub-captions based on clinicalsignificance prior. Through pretraining on 403,563 dermatological image-textpairs collected from education resources, MAKE significantly outperformsstate-of-the-art VLP models on eight datasets across zero-shot skin diseaseclassification, concept annotation, and cross-modal retrieval tasks. Our codewill be made publicly available at https: //github.com/SiyuanYan1/MAKE.</description>
      <author>example@mail.com (Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge)</author>
      <guid isPermaLink="false">2505.09372v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning</title>
      <link>http://arxiv.org/abs/2505.09118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强场景图推理（ISGR）框架，旨在通过三个互补组件提升视觉语言模型（VLMs）的交互推理能力。&lt;h4&gt;背景&lt;/h4&gt;传统的场景图主要关注空间关系，这限制了VLMs对视觉场景中复杂交互的推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键挑战：一是传统检测到构建的方法产生的关联关系集缺乏焦点和上下文相关性；二是现有方法未能形成持久的记忆来将交互推理推广到新的场景。&lt;h4&gt;方法&lt;/h4&gt;ISGR框架包含三个互补组件：一是结合SAM空间关系提取和交互感知字幕生成功能显著场景图；二是使用目标交互查询激活VLMs对物体功能的潜在知识；三是引入具有专门交互奖励函数的短期记忆强化学习策略，将短暂模式转化为长期推理启发式规则。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在交互密集的推理基准测试中显著优于基线方法，特别是在复杂场景理解任务上有特别强的改进。&lt;h4&gt;结论&lt;/h4&gt;ISGR框架通过增强VLMs的交互推理能力，显著提升了场景理解任务的表现。&lt;h4&gt;翻译&lt;/h4&gt;Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a one-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional scene graphs primarily focus on spatial relationships, limitingvision-language models' (VLMs) ability to reason about complex interactions invisual scenes. This paper addresses two key challenges: (1) conventionaldetection-to-construction methods produce unfocused, contextually irrelevantrelationship sets, and (2) existing approaches fail to form persistent memoriesfor generalizing interaction reasoning to new scenes. We proposeInteraction-augmented Scene Graph Reasoning (ISGR), a framework that enhancesVLMs' interactional reasoning through three complementary components. First,our dual-stream graph constructor combines SAM-powered spatial relationextraction with interaction-aware captioning to generate functionally salientscene graphs with spatial grounding. Second, we employ targeted interactionqueries to activate VLMs' latent knowledge of object functionalities,converting passive recognition into active reasoning about how objects worktogether. Finally, we introduce a lone-term memory reinforcement learningstrategy with a specialized interaction-focused reward function that transformstransient patterns into long-term reasoning heuristics. Extensive experimentsdemonstrate that our approach significantly outperforms baseline methods oninteraction-heavy reasoning benchmarks, with particularly strong improvementson complex scene understanding tasks. The source code can be accessed athttps://github.com/open_upon_acceptance.</description>
      <author>example@mail.com (Dayong Liang, Changmeng Zheng, Zhiyuan Wen, Yi Cai, Xiao-Yong Wei, Qing Li)</author>
      <guid isPermaLink="false">2505.09118v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
      <link>http://arxiv.org/abs/2505.09017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DyGSSM的新方法，用于动态图表示学习，以解决现有方法在同时提取全局和局部信息以及处理时间依赖性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;大多数动态图表示学习方法将动态图划分为离散快照来捕捉节点随时间的变化行为。现有方法主要使用消息传递和随机游走方法捕获每个快照中节点的局部或全局结构，然后利用序列模型（如transformers）编码节点嵌入的时间演化，并使用元学习技术更新模型参数。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的局限性，即忽略每个快照中同时提取全局和局部信息，以及未考虑模型在当前快照中的性能在参数更新时的作用，导致缺乏时间依赖性管理。&lt;h4&gt;方法&lt;/h4&gt;提出的方法结合了图卷积网络（GCN）进行局部特征提取和随机游走与门控循环单元（GRU）进行全局特征提取，在每个快照中。然后，使用交叉注意力机制整合局部和全局特征。此外，采用基于HiPPO算法的状态空间模型（SSM）来考虑在更新模型参数时的长期依赖性，确保每个快照中的模型性能为后续更新提供信息。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公开数据集上的实验表明，该方法在20个案例中的17个案例中优于现有的基线和最先进（SOTA）方法。&lt;h4&gt;结论&lt;/h4&gt;DyGSSM方法在动态图表示学习中表现出色，能够更有效地处理时间依赖性和特征提取问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most of the dynamic graph representation learning methods involve dividing adynamic graph into discrete snapshots to capture the evolving behavior of nodesover time. Existing methods primarily capture only local or global structuresof each node within a snapshot using message-passing and random walk-basedmethods. Then, they utilize sequence-based models (e.g., transformers) toencode the temporal evolution of node embeddings, and meta-learning techniquesto update the model parameters. However, these approaches have two limitations.First, they neglect the extraction of global and local informationsimultaneously in each snapshot. Second, they fail to consider the model'sperformance in the current snapshot during parameter updates, resulting in alack of temporal dependency management. Recently, HiPPO (High-order PolynomialProjection Operators) algorithm has gained attention for their ability tooptimize and preserve sequence history in State Space Model (SSM). To addressthe aforementioned limitations in dynamic graph representation learning, wepropose a novel method called Multi-view Dynamic Graph Embeddings with StateSpace Model Gradient Update (DyGSSM). Our approach combines Graph ConvolutionNetworks (GCN) for local feature extraction and random walk with GatedRecurrent Unit (GRU) for global feature extraction in each snapshot. We thenintegrate the local and global features using a cross-attention mechanism.Additionally, we incorporate an SSM based on HiPPO algorithm to account forlong-term dependencies when updating model parameters, ensuring that modelperformance in each snapshot informs subsequent updates. Experiments on fivepublic datasets show that our method outperforms existing baseline andstate-of-the-art (SOTA) methods in 17 out of 20 cases.</description>
      <author>example@mail.com (Bizhan Alipour Pijan, Serdar Bozdag)</author>
      <guid isPermaLink="false">2505.09017v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion</title>
      <link>http://arxiv.org/abs/2505.09424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了模仿学习在机器人操作领域的潜力，并提出了基于SE(3)位姿引导的高效模仿学习方法，用于机器人精确插入任务。&lt;h4&gt;背景&lt;/h4&gt;现有模仿学习方法在精确操作任务上存在精度问题，且依赖于低效的图像/点云观测。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模仿学习方法，以解决机器人精确插入任务中的精度和效率问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出一种精确插入扩散策略，利用相对SE(3)位姿作为观察-动作对；2. 引入RGBD数据到位姿引导扩散策略中，设计一个目标条件RGBD编码器来捕捉当前状态和目标状态之间的差异；3. 提出一种位姿引导残差门控融合方法，以位姿特征为核心，并通过自适应门控机制选择性地补偿位姿特征的不足。&lt;h4&gt;主要发现&lt;/h4&gt;方法在6个机器人精确插入任务上表现出色，仅需要7-10次示范，成功完成精度插入任务，间隙约为0.01mm。&lt;h4&gt;结论&lt;/h4&gt;与现有基准方法相比，该方法在效率和泛化能力方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent studies have proved that imitation learning shows strong potential in the field of robotic manipulation. However, existing methods still struggle with precision manipulation tasks and rely on inefficient image/point cloud observations. In this paper, we explore to introduce SE(3) object pose into imitation learning and propose the pose-guided efficient imitation learning methods for robotic precise insertion task. First, we propose a precise insertion diffusion policy which utilizes the relative SE(3) pose as the observation-action pair. The policy models the source object SE(3) pose trajectory relative to the target object. Second, we explore to introduce the RGBD data to the pose-guided diffusion policy. Specifically, we design a goal-conditioned RGBD encoder to capture the discrepancy between the current state and the goal state. In addition, a pose-guided residual gated fusion method is proposed, which takes pose features as the backbone, and the RGBD features selectively compensate for pose feature deficiencies through an adaptive gating mechanism. Our methods are evaluated on 6 robotic precise insertion tasks, demonstrating competitive performance with only 7-10 demonstrations. Experiments demonstrate that the proposed methods can successfully complete precision insertion tasks with a clearance of about 0.01mm. Experimental results highlight its superior efficiency and generalization capability compared to existing baselines. Code will be available at https://github.com/sunhan1997/PoseInsert.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have proved that imitation learning shows strong potential inthe field of robotic manipulation. However, existing methods still strugglewith precision manipulation task and rely on inefficient image/point cloudobservations. In this paper, we explore to introduce SE(3) object pose intoimitation learning and propose the pose-guided efficient imitation learningmethods for robotic precise insertion task. First, we propose a preciseinsertion diffusion policy which utilizes the relative SE(3) pose as theobservation-action pair. The policy models the source object SE(3) posetrajectory relative to the target object. Second, we explore to introduce theRGBD data to the pose-guided diffusion policy. Specifically, we design agoal-conditioned RGBD encoder to capture the discrepancy between the currentstate and the goal state. In addition, a pose-guided residual gated fusionmethod is proposed, which takes pose features as the backbone, and the RGBDfeatures selectively compensate for pose feature deficiencies through anadaptive gating mechanism. Our methods are evaluated on 6 robotic preciseinsertion tasks, demonstrating competitive performance with only 7-10demonstrations. Experiments demonstrate that the proposed methods cansuccessfully complete precision insertion tasks with a clearance of about 0.01mm. Experimental results highlight its superior efficiency and generalizationcapability compared to existing baselines. Code will be available athttps://github.com/sunhan1997/PoseInsert.</description>
      <author>example@mail.com (Han Sun, Yizhao Wang, Zhenning Zhou, Shuai Wang, Haibo Yang, Jingyuan Sun, Qixin Cao)</author>
      <guid isPermaLink="false">2505.09424v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis</title>
      <link>http://arxiv.org/abs/2505.09329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了在开发可扩展的医疗视觉基础模型时，模型大小、训练算法、数据大小和成像模式之间的扩展行为。提出了BioVFM-21M，一个包含多种生物医学图像模态和解剖结构的规模庞大的数据集，并提出了BioVFM，一个在2100万生物医学图像上预训练的大规模医疗视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;尽管在通用任务上的扩展行为有广泛的研究，但医疗图像与自然数据存在显著差异，因此在医学领域中对扩展行为的理解不足，使得在规模上开发医疗视觉基础模型的关键因素尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过自我监督学习开发可扩展的医疗视觉基础模型，并探究扩展行为在模型大小、训练算法、数据大小和成像模式方面的表现。&lt;h4&gt;方法&lt;/h4&gt;引入了BioVFM-21M数据集，对扩展行为进行了观察和分析，并提出了BioVFM模型。&lt;h4&gt;主要发现&lt;/h4&gt;扩展在提供好处方面是有益的，但效果因任务而异。进一步的分析揭示了与扩展好处相关的几个因素。&lt;h4&gt;结论&lt;/h4&gt;虽然扩展有助于追求更好的性能，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医疗基础模型的关键考虑因素。&lt;h4&gt;翻译&lt;/h4&gt;摘要：扩展模型和数据规模在广泛的任务上展示了令人印象深刻的性能提升。尽管对通用任务的扩展行为进行了广泛的研究，但医学图像与自然数据存在显著差异。由于对医学领域中的扩展行为的理解不足，因此在规模上开发医疗视觉基础模型的关键因素尚不清楚。在本文中，我们通过自我监督学习探索了在开发可扩展的医疗视觉基础模型时，模型大小、训练算法、数据大小和成像模式之间的扩展行为。为了支持可扩展的预训练，我们引入了BioVFM-21M，一个包含广泛的生物医学图像模态和解剖结构的规模庞大的生物医学图像数据集。我们观察到，扩展确实提供了好处，但效果因任务而异。进一步的分析揭示了与扩展好处相关的几个因素。最后，我们提出了BioVFM，一个在2100万生物医学图像上预训练的大规模医疗视觉基础模型，它在12个医学基准上优于以前的最先进的基础模型。我们的结果表明，虽然扩展有助于追求更好的性能，但任务特性、数据多样性、预训练方法和计算效率仍然是开发可扩展医疗基础模型的关键考虑因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling up model and data size have demonstrated impressive performanceimprovement over a wide range of tasks. Despite extensive studies on scalingbehaviors for general-purpose tasks, medical images exhibit substantialdifferences from natural data. It remains unclear the key factors in developingmedical vision foundation models at scale due to the absence of an extensiveunderstanding of scaling behavior in the medical domain. In this paper, weexplored the scaling behavior across model sizes, training algorithms, datasizes, and imaging modalities in developing scalable medical vision foundationmodels by self-supervised learning. To support scalable pretraining, weintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing awide range of biomedical image modalities and anatomies. We observed thatscaling up does provide benefits but varies across tasks. Additional analysisreveals several factors correlated with scaling benefits. Finally, we proposeBioVFM, a large-scale medical vision foundation model pretrained on 21 millionbiomedical images, which outperforms the previous state-of-the-art foundationmodels across 12 medical benchmarks. Our results highlight that while scalingup is beneficial for pursuing better performance, task characteristics, datadiversity, pretraining methods, and computational efficiency remain criticalconsiderations for developing scalable medical foundation models.</description>
      <author>example@mail.com (Jiarun Liu, Hong-Yu Zhou, Weijian Huang, Hao Yang, Dongning Song, Tao Tan, Yong Liang, Shanshan Wang)</author>
      <guid isPermaLink="false">2505.09329v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.08614v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WaveGuard是一种主动水印框架，用于提高深度伪造技术中的鲁棒性和不可见性，以应对隐私侵犯和身份盗窃的风险。&lt;h4&gt;背景&lt;/h4&gt;深度伪造技术带来了隐私侵犯和身份盗窃等风险。&lt;h4&gt;目的&lt;/h4&gt;提出WaveGuard以解决深度伪造技术带来的风险。&lt;h4&gt;方法&lt;/h4&gt;WaveGuard通过频率域嵌入和基于图的结构一致性来增强鲁棒性和不可见性。具体方法包括使用DT-CWT将水印嵌入到高频子带，并使用SC-GNN来保持视觉质量，同时设计一个注意力模块来提高嵌入精度。&lt;h4&gt;主要发现&lt;/h4&gt;在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WaveGuard是一种有效的水印框架，可以增强深度伪造技术的鲁棒性和不可见性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：深度伪造技术带来了日益增加的风险，如隐私侵犯和身份盗窃。为了应对这些威胁，我们提出了一种名为WaveGuard的主动水印框架，通过频域嵌入和基于图的结构一致性来提高鲁棒性和不可见性。具体来说，我们使用DT-CWT将水印嵌入到高频子带，并使用SC-GNN来保持视觉质量。我们还设计了一个注意力模块来提高嵌入精度。在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面都优于现有方法。代码可在https://github.com/vpsg-research/WaveGuard获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vpsg-research/waveguard&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake technology poses increasing risks such as privacy invasion andidentity theft. To address these threats, we propose WaveGuard, a proactivewatermarking framework that enhances robustness and imperceptibility viafrequency-domain embedding and graph-based structural consistency.Specifically, we embed watermarks into high-frequency sub-bands using Dual-TreeComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency GraphNeural Network (SC-GNN) to preserve visual quality. We also design an attentionmodule to refine embedding precision. Experimental results on face swap andreenactment tasks demonstrate that WaveGuard outperforms state-of-the-artmethods in both robustness and visual quality. Code is available athttps://github.com/vpsg-research/WaveGuard.</description>
      <author>example@mail.com (Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma)</author>
      <guid isPermaLink="false">2505.08614v2</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Recent Advances in Medical Imaging Segmentation: A Survey</title>
      <link>http://arxiv.org/abs/2505.09274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医学影像在现代医疗保健中扮演关键角色，推动诊断、治疗规划和患者护理的进步。尽管在分割方面取得进展，但稳健的泛化和领域适应性仍然是一个重大挑战。&lt;h4&gt;背景&lt;/h4&gt;医学影像分割由于数据获取、标注复杂性、结构变异性、影像模态变化和隐私限制等因素而成为最具挑战性的问题之一。&lt;h4&gt;目的&lt;/h4&gt;本综述旨在探索医学影像分割的最新进展，重点关注生成式AI、少样本学习、基础模型和通用模型等方法。&lt;h4&gt;方法&lt;/h4&gt;综述提供了这些方法的理论基础、最先进的技术和近期应用的综合概述。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法为长期存在的挑战提供了有希望的解决方案。&lt;h4&gt;结论&lt;/h4&gt;讨论了分割模型固有的局限性、未解决的问题和未来研究方向，旨在提高医学影像分割模型的实用性和可及性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：医学影像是现代医疗保健的基石，推动诊断、治疗规划和患者护理的进步。在众多任务中，分割由于数据获取、标注复杂性、结构变异性、医学影像模态变化和隐私限制等因素，仍然是最具挑战性的问题之一。尽管取得了进展，但实现稳健的泛化和领域适应性仍然是一个重大挑战，尤其是在一些提出的模型资源密集型性质及其对领域专业知识的依赖性。本综述探讨了医学影像分割的最新进展，重点关注生成式AI、少样本学习、基础模型和通用模型等方法。这些方法为长期存在的挑战提供了有希望的解决方案。我们提供了这些方法的理论基础、最先进的技术和近期应用的综合概述。最后，我们讨论了分割模型固有的局限性、未解决的问题和未来研究方向，旨在提高医学影像分割模型的实用性和可及性。我们正在维护一个GitHub存储库（https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation），以继续跟踪和更新该领域的创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging is a cornerstone of modern healthcare, driving advancementsin diagnosis, treatment planning, and patient care. Among its various tasks,segmentation remains one of the most challenging problem due to factors such asdata accessibility, annotation complexity, structural variability, variation inmedical imaging modalities, and privacy constraints. Despite recent progress,achieving robust generalization and domain adaptation remains a significanthurdle, particularly given the resource-intensive nature of some proposedmodels and their reliance on domain expertise. This survey explorescutting-edge advancements in medical image segmentation, focusing onmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, andUniversal Models. These approaches offer promising solutions to longstandingchallenges. We provide a comprehensive overview of the theoretical foundations,state-of-the-art techniques, and recent applications of these methods. Finally,we discuss inherent limitations, unresolved issues, and future researchdirections aimed at enhancing the practicality and accessibility ofsegmentation models in medical imaging. We are maintaining a\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHubRepository} to continue tracking and updating innovations in this field.</description>
      <author>example@mail.com (Fares Bougourzi, Abdenour Hadid)</author>
      <guid isPermaLink="false">2505.09274v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression</title>
      <link>http://arxiv.org/abs/2505.09193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first learned video codec that surpasses VTM 13.2 RA across all  standard test datasets. Code will be available at  https://github.com/JiangWeibeta/ECVC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiECVC的基于双向学习的视频压缩框架，旨在提高视频压缩性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于前向预测的视频压缩方法取得了显著成果，但双向视频压缩技术仍处于探索阶段，性能落后于单向方法。&lt;h4&gt;目的&lt;/h4&gt;解决现有双向视频压缩方法在提取多样性和准确性上下文方面的不足，以及缺乏对动态抑制有害上下文的适应性。&lt;h4&gt;方法&lt;/h4&gt;BiECVC通过结合多样化的局部和非局部上下文建模以及自适应上下文门控机制来实现。它重用低层高质量特征，并使用解码运动向量进行对齐，同时采用线性注意力机制来高效建模非局部依赖关系。此外，引入双向上下文门控机制，根据条件编码结果动态过滤上下文信息。&lt;h4&gt;主要发现&lt;/h4&gt;BiECVC在随机访问配置下，比特率比VTC 13.2降低了13.4%和15.7%，在所有标准测试数据集上首次超越了VTC 13.2 RA。&lt;h4&gt;结论&lt;/h4&gt;BiECVC实现了最先进的性能，是第一个在所有标准测试数据集上超越VTC 13.2 RA的学习视频编解码器。&lt;h4&gt;翻译&lt;/h4&gt;摘要：最近基于前向预测的学习视频压缩（LVC）方法取得了令人印象深刻的成果，甚至在低延迟B（LDB）配置下超过了VVC参考软件VTM。相比之下，学习双向视频压缩（BVC）仍然未被充分探索，并且仍然落后于其单向对应物。这种性能差距主要是由于提取多样性和准确性上下文的有限能力：大多数现有的BVC主要利用时间运动，而忽略了帧之间的非局部相关性。此外，它们缺乏对动态抑制来自快速运动或遮挡的有害上下文的适应性。为了解决这些挑战，我们提出了BiECVC，这是一种结合了多样化的局部和非局部上下文建模以及自适应上下文门控的BVC框架。为了增强局部上下文，BiECVC重用底层的高质量特征，并使用解码运动向量进行对齐，而不引入额外的运动开销。为了有效地建模非局部依赖关系，我们采用了一种平衡性能和复杂性的线性注意力机制。为了进一步减轻上下文预测不准确的影响，我们引入了双向上下文门控，受最近自回归语言模型中的数据相关衰减的启发，根据条件编码结果动态过滤上下文信息。大量的实验表明，BiECVC实现了最先进的性能，与VTC 13.2相比，在32和64个周期内分别降低了13.4%和15.7%的比特率。据我们所知，BiECVC是第一个在所有标准测试数据集上超越VTC 13.2 RA的学习视频编解码器。代码将在https://github.com/JiangWeibeta/ECVC上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent forward prediction-based learned video compression (LVC) methods haveachieved impressive results, even surpassing VVC reference software VTM underthe Low Delay B (LDB) configuration. In contrast, learned bidirectional videocompression (BVC) remains underexplored and still lags behind its forward-onlycounterparts. This performance gap is mainly due to the limited ability toextract diverse and accurate contexts: most existing BVCs primarily exploittemporal motion while neglecting non-local correlations across frames.Moreover, they lack the adaptability to dynamically suppress harmful contextsarising from fast motion or occlusion. To tackle these challenges, we proposeBiECVC, a BVC framework that incorporates diversified local and non-localcontext modeling along with adaptive context gating. For local contextenhancement, BiECVC reuses high-quality features from lower layers and alignsthem using decoded motion vectors without introducing extra motion overhead.Tomodel non-local dependencies efficiently, we adopt a linear attention mechanismthat balances performance and complexity. To further mitigate the impact ofinaccurate context prediction, we introduce Bidirectional Context Gating,inspired by data-dependent decay in recent autoregressive language models, todynamically filter contextual information based on conditional coding results.Extensive experiments demonstrate that BiECVC achieves state-of-the-artperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2under the Random Access (RA) configuration with intra periods of 32 and 64,respectively. To our knowledge, BiECVC is the first learned video codec tosurpass VTM 13.2 RA across all standard test datasets. Code will be availableat https://github.com/JiangWeibeta/ECVC.</description>
      <author>example@mail.com (Wei Jiang, Junru Li, Kai Zhang, Li Zhang)</author>
      <guid isPermaLink="false">2505.09193v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning</title>
      <link>http://arxiv.org/abs/2505.09265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于纯视觉基础模型的无监督视觉异常分割方法，通过统一异常分割为变化分割，使用大规模合成图像对进行训练，实现无需语言指导的异常分割。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉异常分割依赖于视觉-语言模型，但这些模型依赖于手动设计的文本提示，且视觉表示与语言独立。&lt;h4&gt;目的&lt;/h4&gt;探索纯视觉基础模型在通用视觉异常分割中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将异常分割统一为变化分割的新范式，利用来自现有图像数据集的大规模合成图像对进行训练，并提出了一个一提示元学习框架（MetaUAS）进行异常分割，并引入了软特征对齐模块来处理提示图像和查询图像之间的几何变化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了无监督的通用异常分割，无需依赖特殊的异常检测数据集和预训练的视觉-语言模型，且在零样本、少样本甚至全样本异常分割方法中表现优异。&lt;h4&gt;结论&lt;/h4&gt;MetaUAS方法有效地实现了仅用一个正常图像提示的异常分割，无需语言指导，在性能上显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a visual-based anomaly segmentation method using a pure vision foundation model, which unifies anomaly segmentation into change segmentation, utilizes large-scale synthetic image pairs for training, and achieves unsupervised universal anomaly segmentation without language guidance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero- and few-shot visual anomaly segmentation relies on powerfulvision-language models that detect unseen anomalies using manually designedtextual prompts. However, visual representations are inherently independent oflanguage. In this paper, we explore the potential of a pure visual foundationmodel as an alternative to widely used vision-language models for universalvisual anomaly segmentation. We present a novel paradigm that unifies anomalysegmentation into change segmentation. This paradigm enables us to leveragelarge-scale synthetic image pairs, featuring object-level and local regionchanges, derived from existing image datasets, which are independent of targetanomaly datasets. We propose a one-prompt Meta-learning framework for UniversalAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset andthen generalizes well to segment any novel or unseen visual anomalies in thereal world. To handle geometrical variations between prompt and query images,we propose a soft feature alignment module that bridges paired-image changeperception and single-image semantic segmentation. This is the first work toachieve universal anomaly segmentation using a pure vision model withoutrelying on special anomaly detection datasets and pre-trained visual-languagemodels. Our method effectively and efficiently segments any anomalies with onlyone normal image prompt and enjoys training-free without guidance fromlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,and even full-shot anomaly segmentation methods. The code and pre-trainedmodels are available at https://github.com/gaobb/MetaUAS.</description>
      <author>example@mail.com (Bin-Bin Gao)</author>
      <guid isPermaLink="false">2505.09265v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians</title>
      <link>http://arxiv.org/abs/2505.09413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的点云渲染方法，通过从点云预测二维高斯来达到照片逼真的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;现有的基于学习的点云渲染方法通常依赖于分类先验、密集点云或额外的优化步骤。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种无需分类先验、密集点云或额外优化的点云渲染方法。&lt;h4&gt;方法&lt;/h4&gt;方法包含两个相同的模块，采用全图块架构，网络可以推广到多个数据集。模块利用点云信息（包括法线、颜色和距离）来归一化和初始化高斯。然后，使用分割解码器通过复制和预测更精确的结果来细化初始高斯，使方法能够有效处理稀疏点云。训练后，该方法可以直接推广到不同类别的点云。预测的高斯可以直接用于渲染，无需对渲染图像进行额外优化，保留了二维高斯的优点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个数据集上取得了优越的泛化性能，并达到了当前的最佳性能（SOTA）。&lt;h4&gt;结论&lt;/h4&gt;该方法在点云渲染方面具有显著优势，并展示了良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/murcherful/gaupcrender&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current learning-based methods predict NeRF or 3D Gaussians from point cloudsto achieve photo-realistic rendering but still depend on categorical priors,dense point clouds, or additional refinements. Hence, we introduce a novelpoint cloud rendering method by predicting 2D Gaussians from point clouds. Ourmethod incorporates two identical modules with an entire-patch architectureenabling the network to be generalized to multiple datasets. The modulenormalizes and initializes the Gaussians utilizing the point cloud informationincluding normals, colors and distances. Then, splitting decoders are employedto refine the initial Gaussians by duplicating them and predicting moreaccurate results, making our methodology effectively accommodate sparse pointclouds as well. Once trained, our approach exhibits direct generalization topoint clouds across different categories. The predicted Gaussians are employeddirectly for rendering without additional refinement on the rendered images,retaining the benefits of 2D Gaussians. We conduct extensive experiments onvarious datasets, and the results demonstrate the superiority andgeneralization of our method, which achieves SOTA performance. The code isavailable athttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.</description>
      <author>example@mail.com (Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen)</author>
      <guid isPermaLink="false">2505.09413v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</title>
      <link>http://arxiv.org/abs/2505.09263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于少样本的异常生成方法AnoGen，用于解决工业检测中异常样本稀缺的问题，通过生成真实且多样化的异常样本来提高异常检测模型的性能。&lt;h4&gt;背景&lt;/h4&gt;由于工业检测中异常样本稀缺，现有的异常检测方法往往通过添加噪声或外部数据来合成异常样本，但合成样本与真实样本之间存在较大的语义差距，导致异常检测性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出AnoGen方法，通过少量真实异常样本引导扩散模型生成真实且多样化的异常样本，以提升异常检测模型的训练效果。&lt;h4&gt;方法&lt;/h4&gt;AnoGen方法分为三个阶段：第一阶段，基于少量真实异常样本学习异常分布，并将所学知识注入嵌入层；第二阶段，使用嵌入层和给定边界框引导扩散模型在特定对象或纹理上生成真实且多样化的异常样本；第三阶段，提出一种弱监督异常检测方法，利用生成的异常样本训练更强大的模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，AnoGen方法生成的异常样本有效提高了异常分类和分割任务的模型性能，例如DRAEM和DseTSeg在分割任务上的AU-PR指标分别提高了5.8%和1.5%。&lt;h4&gt;结论&lt;/h4&gt;AnoGen方法通过生成高质量的异常样本，有效解决了工业检测中异常样本稀缺的问题，为异常检测模型的训练提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, for example, DRAEM and DseTSeg achieved a 5.8% and 1.5% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gaobb/anogen&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a practical and challenging task due to the scarcity ofanomaly samples in industrial inspection. Some existing anomaly detectionmethods address this issue by synthesizing anomalies with noise or externaldata. However, there is always a large semantic gap between synthetic andreal-world anomalies, resulting in weak performance in anomaly detection. Tosolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)method, which guides the diffusion model to generate realistic and diverseanomalies with only a few real anomalies, thereby benefiting training anomalydetection models. Specifically, our work is divided into three stages. In thefirst stage, we learn the anomaly distribution based on a few given realanomalies and inject the learned knowledge into an embedding. In the secondstage, we use the embedding and given bounding boxes to guide the diffusionmodel to generate realistic and diverse anomalies on specific objects (ortextures). In the final stage, we propose a weakly-supervised anomaly detectionmethod to train a more powerful model with generated anomalies. Our methodbuilds upon DRAEM and DesTSeg as the foundation model and conducts experimentson the commonly used industrial anomaly detection dataset, MVTec. Theexperiments demonstrate that our generated anomalies effectively improve themodel performance of both anomaly classification and segmentation taskssimultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvementin AU-PR metric on segmentation task, respectively. The code and generatedanomalous data are available at https://github.com/gaobb/AnoGen.</description>
      <author>example@mail.com (Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu)</author>
      <guid isPermaLink="false">2505.09263v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2505.09140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoDiT-3D是一种拓扑感知扩散Transformer模型，通过瓶颈结构提高了3D点云生成的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的DiT模型在3D点云生成中主要关注局部特征提取，而忽略了全局拓扑信息，如空洞，这对于保持形状一致性和捕捉复杂几何形状至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出TopoDiT-3D模型，以解决现有方法在提取全局拓扑信息方面的不足。&lt;h4&gt;方法&lt;/h4&gt;TopoDiT-3D采用瓶颈结构，使用Perceiver Resampler来整合通过持久同伦提取的拓扑信息，并自适应地过滤掉冗余的局部特征以提高训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TopoDiT-3D在视觉质量、多样性和训练效率方面优于现有模型，并证明了丰富拓扑信息对于3D点云生成的重要性及其与常规局部特征学习的协同作用。&lt;h4&gt;结论&lt;/h4&gt;TopoDiT-3D模型在3D点云生成中取得了显著的性能提升，并展示了拓扑信息与局部特征学习相结合的优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning. Videos and code are available at https://github.com/Zechao-Guan/TopoDiT-3D.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zechao-guan/topodit-3d&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Diffusion Transformer (DiT) models have significantlyimproved 3D point cloud generation. However, existing methods primarily focuson local feature extraction while overlooking global topological information,such as voids, which are crucial for maintaining shape consistency andcapturing complex geometries. To address this limitation, we proposeTopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structurefor 3D point cloud generation. Specifically, we design the bottleneck structureutilizing Perceiver Resampler, which not only offers a mode to integratetopological information extracted through persistent homology into featurelearning, but also adaptively filters out redundant local features to improvetraining efficiency. Experimental results demonstrate that TopoDiT-3Doutperforms state-of-the-art models in visual quality, diversity, and trainingefficiency. Furthermore, TopoDiT-3D demonstrates the importance of richtopological information for 3D point cloud generation and its synergy withconventional local feature learning. Videos and code are available athttps://github.com/Zechao-Guan/TopoDiT-3D.</description>
      <author>example@mail.com (Zechao Guan, Feng Yan, Shuai Du, Lin Ma, Qingshan Liu)</author>
      <guid isPermaLink="false">2505.09140v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System</title>
      <link>http://arxiv.org/abs/2505.09178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniCAD的统一架构，用于解决视觉模型预训练的复杂性和规模问题，旨在开发高效的医学图像诊断模型。&lt;h4&gt;背景&lt;/h4&gt;视觉模型预训练的复杂性和规模导致多任务计算机辅助诊断（CAD）系统的开发和部署变得更加困难，同时医学图像社区缺乏一个开源的CAD平台来促进高效和可扩展的诊断模型的快速创建。&lt;h4&gt;目的&lt;/h4&gt;提出UniCAD架构，利用预训练视觉模型的能力，以最小的任务特定参数无缝处理2D和3D医学图像。&lt;h4&gt;方法&lt;/h4&gt;UniCAD引入了两种关键创新：(1) 效率：采用低秩适应策略来适应预训练视觉模型到医学图像领域，同时仅引入0.17%的可训练参数即可实现与完全微调的模型相当的性能；(2) 插件式架构：结合冻结的基础模型和多个插件式专家，实现多样化任务和无缝功能扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同的医学数据集上进行的全面实验表明，UniCAD在准确性和部署效率方面均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UniCAD提供了一个开源平台，研究人员可以共享和访问轻量级的CAD专家，促进更加公平和高效的研究生态系统。&lt;h4&gt;翻译&lt;/h4&gt;随着视觉模型预训练的复杂性和规模的增加，开发和部署多任务计算机辅助诊断（CAD）系统变得越来越具有挑战性和资源密集。此外，医学图像社区缺乏一个开源的CAD平台，以促进高效和可扩展的诊断模型的快速创建。为了解决这些问题，我们提出了UniCAD，一个利用预训练视觉模型强大能力的统一架构，以最小的任务特定参数无缝处理2D和3D医学图像。UniCAD引入了两个关键创新：(1) 效率：采用低秩适应策略将预训练视觉模型适应到医学图像领域，同时仅引入0.17%的可训练参数即可实现与完全微调的模型相当的性能；(2) 插件式：一个模块化架构，结合冻结的基础模型和多个插件式专家，实现多样化任务和无缝功能扩展。基于这个统一的CAD架构，我们建立了一个开源平台，研究人员可以共享和访问轻量级的CAD专家，促进更加公平和高效的研究生态系统。在12个不同的医学数据集上进行的全面实验表明，UniCAD在准确性和部署效率方面均优于现有方法。源代码和项目页面可在https://mii-laboratory.github.io/UniCAD/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing complexity and scale of visual model pre-training have madedeveloping and deploying multi-task computer-aided diagnosis (CAD) systemsincreasingly challenging and resource-intensive. Furthermore, the medicalimaging community lacks an open-source CAD platform to enable the rapidcreation of efficient and extendable diagnostic models. To address theseissues, we propose UniCAD, a unified architecture that leverages the robustcapabilities of pre-trained vision foundation models to seamlessly handle both2D and 3D medical images while requiring only minimal task-specific parameters.UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptationstrategy is employed to adapt a pre-trained visual model to the medical imagedomain, achieving performance on par with fully fine-tuned counterparts whileintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modulararchitecture that combines a frozen foundation model with multipleplug-and-play experts, enabling diverse tasks and seamless functionalityexpansion. Building on this unified CAD architecture, we establish anopen-source platform where researchers can share and access lightweight CADexperts, fostering a more equitable and efficient research ecosystem.Comprehensive experiments across 12 diverse medical datasets demonstrate thatUniCAD consistently outperforms existing methods in both accuracy anddeployment efficiency. The source code and project page are available athttps://mii-laboratory.github.io/UniCAD/.</description>
      <author>example@mail.com (Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang)</author>
      <guid isPermaLink="false">2505.09178v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems</title>
      <link>http://arxiv.org/abs/2505.08816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IFIP Networking 2025. Code available at  https://github.com/koukipp/contrastive_transformers_ids&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于transformer编码器的自监督对比学习方法，用于原始数据包序列上的泛化入侵检测。&lt;h4&gt;背景&lt;/h4&gt;随着数字世界的日益互联，零日攻击的频率和严重性显著增加，迫切需要创新的入侵检测系统（IDS）。&lt;h4&gt;目的&lt;/h4&gt;为了解决基于机器学习的入侵检测系统对标签数据集的依赖以及泛化未见过的流量模式的能力问题。&lt;h4&gt;方法&lt;/h4&gt;采用数据包级数据增强策略结合基于transformer的架构，自动学习全面的包序列表示，以增强异常识别任务和入侵检测的监督学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;该transformer框架在性能上优于现有的基于NetFlow的自监督方法，实现了高达3%的AUC提升（在数据集内评估）和高达20%的AUC提升（在数据集间评估）。此外，该模型在有限的标签数据集上提供了强大的监督入侵检测基线，与预训练和在同一数据集上评估的自监督NetFlow模型相比，AUC提升了1.5%。&lt;h4&gt;结论&lt;/h4&gt;该模型在跨不同数据集微调时表现出适应性，即使在缺乏目标域良性数据的情况下也能展现出强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;As the digital landscape becomes more interconnected, the frequency and severity of zero-day attacks have significantly increased, leading to an urgent need for innovative Intrusion Detection Systems (IDS). Machine Learning-based IDS that learn from the network traffic characteristics and can discern attack patterns from benign traffic offer an advanced solution to traditional signature-based IDS. However, they heavily rely on labeled datasets, and their ability to generalize when encountering unseen traffic patterns remains a challenge. This paper proposes a novel self-supervised contrastive learning approach based on transformer encoders, specifically tailored for generalizable intrusion detection on raw packet sequences. Our proposed learning scheme employs a packet-level data augmentation strategy combined with a transformer-based architecture to extract and generate meaningful representations of traffic flows. Unlike traditional methods reliant on handcrafted statistical features (NetFlow), our approach automatically learns comprehensive packet sequence representations, significantly enhancing performance in anomaly identification tasks and supervised learning for intrusion detection. Our transformer-based framework exhibits better performance in comparison to existing NetFlow self-supervised methods. Specifically, we achieve up to a 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. Moreover, our model provides a strong baseline for supervised intrusion detection with limited labeled data, exhibiting an improvement over self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated on the same dataset. Additionally, we show the adaptability of our pretrained model when fine-tuned across different datasets, demonstrating strong performance even when lacking benign data from the target domain.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the digital landscape becomes more interconnected, the frequency andseverity of zero-day attacks, have significantly increased, leading to anurgent need for innovative Intrusion Detection Systems (IDS). MachineLearning-based IDS that learn from the network traffic characteristics and candiscern attack patterns from benign traffic offer an advanced solution totraditional signature-based IDS. However, they heavily rely on labeleddatasets, and their ability to generalize when encountering unseen trafficpatterns remains a challenge. This paper proposes a novel self-supervisedcontrastive learning approach based on transformer encoders, specificallytailored for generalizable intrusion detection on raw packet sequences. Ourproposed learning scheme employs a packet-level data augmentation strategycombined with a transformer-based architecture to extract and generatemeaningful representations of traffic flows. Unlike traditional methods relianton handcrafted statistical features (NetFlow), our approach automaticallylearns comprehensive packet sequence representations, significantly enhancingperformance in anomaly identification tasks and supervised learning forintrusion detection. Our transformer-based framework exhibits betterperformance in comparison to existing NetFlow self-supervised methods.Specifically, we achieve up to a 3% higher AUC in anomaly detection forintra-dataset evaluation and up to 20% higher AUC scores in inter-datasetevaluation. Moreover, our model provides a strong baseline for supervisedintrusion detection with limited labeled data, exhibiting an improvement overself-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluatedon the same dataset. Additionally, we show the adaptability of our pretrainedmodel when fine-tuned across different datasets, demonstrating strongperformance even when lacking benign data from the target domain.</description>
      <author>example@mail.com (Ippokratis Koukoulis, Ilias Syrigos, Thanasis Korakis)</author>
      <guid isPermaLink="false">2505.08816v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Block-Biased Mamba for Long-Range Sequence Processing</title>
      <link>http://arxiv.org/abs/2505.09022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mamba通过引入输入依赖的动态特性扩展了早期的状态空间模型（SSMs），并在多个领域如语言建模、计算机视觉和基础模型中展现出强大的实证性能。然而，Mamba在长距离序列任务上的表现不佳，这是一个意外的弱点。&lt;h4&gt;背景&lt;/h4&gt;Mamba虽然在长距离依赖架构上构建，但在长距离序列任务上表现不佳，这限制了其通用性和适用性。&lt;h4&gt;目的&lt;/h4&gt;为了提高Mamba的通用性和适用性，本文从表达性、归纳偏置和训练稳定性三个角度分析了Mamba的局限性。&lt;h4&gt;方法&lt;/h4&gt;本文提出了$ext{B}_2ext{S}_6$，这是Mamba的S6单元的一个简单扩展，结合了块状选择性动态和特定通道的偏置。通过理论和实证分析，证明了这些改进有助于提升模型的归纳偏置、表达性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明，与早期的SSMs如S4D相比，Mamba在表达性、归纳偏置和训练稳定性方面存在不足。&lt;h4&gt;结论&lt;/h4&gt;$ext{B}_2ext{S}_6$在长距离序列任务（LRA）上优于S4和S4D，同时在语言建模基准测试中保持了Mamba的性能。&lt;h4&gt;翻译&lt;/h4&gt;Mamba通过引入输入依赖的动态特性扩展了早期的状态空间模型（SSMs），并在多个领域如语言建模、计算机视觉和基础模型中展现出强大的实证性能。然而，尽管构建在适用于长距离依赖的架构上，Mamba在长距离序列任务上的表现不佳。为了理解并解决这一差距，本文从表达性、归纳偏置和训练稳定性三个角度分析了Mamba的局限性。研究提出了$ext{B}_2ext{S}_6$，这是Mamba的S6单元的一个简单扩展，结合了块状选择性动态和特定通道的偏置。理论和实证分析表明，这些改进使模型具备了更好的归纳偏置，提高了其表达性和稳定性。在长距离序列任务（LRA）上，$ext{B}_2ext{S}_6$优于S4和S4D，同时在语言建模基准测试中保持了Mamba的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mamba extends earlier state space models (SSMs) by introducinginput-dependent dynamics, and has demonstrated strong empirical performanceacross a range of domains, including language modeling, computer vision, andfoundation models. However, a surprising weakness remains: despite being builton architectures designed for long-range dependencies, Mamba performs poorly onlong-range sequential tasks. Understanding and addressing this gap is importantfor improving Mamba's universality and versatility. In this work, we analyzeMamba's limitations through three perspectives: expressiveness, inductive bias,and training stability. Our theoretical results show how Mamba falls short ineach of these aspects compared to earlier SSMs such as S4D. To address theseissues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6unit that combines block-wise selective dynamics with a channel-specific bias.We prove that these changes equip the model with a better-suited inductive biasand improve its expressiveness and stability. Empirically,$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) taskswhile maintaining Mamba's performance on language modeling benchmarks.</description>
      <author>example@mail.com (Annan Yu, N. Benjamin Erichson)</author>
      <guid isPermaLink="false">2505.09022v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition</title>
      <link>http://arxiv.org/abs/2505.09073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at the IEEE International Conference on Automatic Face and  Gesture 2025 (FG2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的领域自适应框架，用于解决人脸识别中由于姿态差异导致的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;尽管人脸识别技术取得了进展，但姿态差异仍然是一个根本问题，影响了识别性能。&lt;h4&gt;目的&lt;/h4&gt;提高在姿态差异较大的情况下的人脸识别性能。&lt;h4&gt;方法&lt;/h4&gt;框架通过以下方式实现更好的姿态不变性：(1) 使用共享注意力映射强调2D面部图像和3D面部数据之间的相关模式；(2) 使用联合熵正则化损失来提高一致性，通过利用注意力图增强相交的2D和3D表示之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在FaceScape和ARL-VTF数据集上进行了评估，在姿态不变性方面优于竞争方法，分别实现了至少7.1%和1.57%的TAR@1%FAR改进。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在解决人脸识别中由于姿态差异导致的性能下降问题上表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in facial recognition, there remains a fundamentalissue concerning degradations in performance due to substantial perspective(pose) differences between enrollment and query (probe) imagery. Therefore, wepropose a novel domain adaptive framework to facilitate improved performancesacross large discrepancies in pose by enabling image-based (2D) representationsto infer properties of inherently pose invariant point cloud (3D)representations. Specifically, our proposed framework achieves better poseinvariance by using (1) a shared (joint) attention mapping to emphasize commonpatterns that are most correlated between 2D facial images and 3D facial dataand (2) a joint entropy regularizing loss to promote betterconsistency$\unicode{x2014}$enhancing correlations among the intersecting 2Dand 3D representations$\unicode{x2014}$by leveraging both attention maps. Thisframework is evaluated on FaceScape and ARL-VTF datasets, where it outperformscompetitive methods by achieving profile (90$\unicode{x00b0}$$\unicode{x002b}$)TAR @ 1$\unicode{x0025}$ FAR improvements of at least 7.1$\unicode{x0025}$ and1.57$\unicode{x0025}$, respectively.</description>
      <author>example@mail.com (J. Brennan Peace, Shuowen Hu, Benjamin S. Riggan)</author>
      <guid isPermaLink="false">2505.09073v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery</title>
      <link>http://arxiv.org/abs/2505.08932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Novel Approaches for Precision Agriculture and  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了无人机在森林恢复和监测中的应用，特别是种子撒播，并介绍了一种基于Segment Anything Model (SAM)的森林地面物体分割方法。&lt;h4&gt;背景&lt;/h4&gt;无人机在森林恢复和监测中越来越受欢迎，但由于森林地面的自然变异性高、环境参数快速变化以及定义不明确导致的模糊标注，详细理解森林地面仍是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，论文提出将SAM模型应用于森林地面物体（如树桩、植被和木质残骸）的分割。&lt;h4&gt;方法&lt;/h4&gt;论文采用参数高效的微调（PEFT）来微调一小部分额外模型参数，同时保持原始权重不变。调整SAM的掩码解码器以生成与数据集类别相对应的掩码，实现无需人工提示的自动分割。&lt;h4&gt;主要发现&lt;/h4&gt;基于适配器的PEFT方法实现了最高的平均交并比（mIoU），而低秩适应（LoRA）提供了参数更少的轻量级替代方案，适用于资源受限的无人机平台。&lt;h4&gt;结论&lt;/h4&gt;该方法为无人机在森林地面物体分割方面提供了高效和轻量级的解决方案，有助于提升森林恢复和监测的精确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation andforest monitoring, including seed dispersal in hard-to-reach terrains. However,a detailed understanding of the forest floor remains a challenge due to highnatural variability, quickly changing environmental parameters, and ambiguousannotations due to unclear definitions. To address this issue, we adapt theSegment Anything Model (SAM), a vision foundation model with stronggeneralization capabilities, to segment forest floor objects such as treestumps, vegetation, and woody debris. To this end, we employparameter-efficient fine-tuning (PEFT) to fine-tune a small subset ofadditional model parameters while keeping the original weights fixed. We adjustSAM's mask decoder to generate masks corresponding to our dataset categories,allowing for automatic segmentation without manual prompting. Our results showthat the adapter-based PEFT method achieves the highest mean intersection overunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers alightweight alternative for resource-constrained UAV platforms.</description>
      <author>example@mail.com (Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben)</author>
      <guid isPermaLink="false">2505.08932v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</title>
      <link>http://arxiv.org/abs/2505.08550v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于线性模型的多元时间序列预测模型OLinear，该模型在正交变换域中运行，以提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的预测模型通常采用时间预测范式，直接在时间域中编码和解码时间序列。然而，时间序列数据中的复杂依赖关系可能会阻碍这种范式的性能。&lt;h4&gt;目的&lt;/h4&gt;提高多元时间序列预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用基于正交矩阵的数据自适应变换（OrthoTrans）来对时间序列进行正交变换，以 decorrelated feature domain 中进行更有效的编码和解码。此外，引入了一个定制的线性层NormLin，使用归一化权重矩阵来捕获多元依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;NormLin模块在性能上超过了多头自注意力机制，同时计算量减少了近一半。在24个基准和140个预测任务上的实验表明，OLinear模型在效率上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;OLinear模型作为自注意力的插件替换，可以持续提升基于Transformer的预测器的性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于线性模型的多元时间序列预测模型OLinear，该模型在正交变换域中运行。为了解决时间序列数据中复杂的依赖关系，使用OrthoTrans进行数据自适应变换，并引入了NormLin线性层以捕捉多元依赖关系。实验结果表明，OLinear在效率和性能上都优于现有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jackyue1994/OLinear&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-basedmultivariate time series forecasting model that operates in an$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typicallyadopt the temporal forecast (TF) paradigm, which directly encode and decodetime series in the time domain. However, the entangled step-wise dependenciesin series data can hinder the performance of TF. To address this, someforecasters conduct encoding and decoding in the transformed domain usingfixed, dataset-independent bases (e.g., sine and cosine signals in the Fouriertransform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptivetransformation based on an orthogonal matrix that diagonalizes the series'temporal Pearson correlation matrix. This approach enables more effectiveencoding and decoding in the decorrelated feature domain and can serve as aplug-in module to enhance existing forecasters. To enhance the representationlearning for multivariate time series, we introduce a customized linear layer,$\mathbf{NormLin}$, which employs a normalized weight matrix to capturemultivariate dependencies. Empirically, the NormLin module shows a surprisingperformance advantage over multi-head self-attention, while requiring nearlyhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecastingtasks demonstrate that OLinear consistently achieves state-of-the-artperformance with high efficiency. Notably, as a plug-in replacement forself-attention, the NormLin module consistently enhances Transformer-basedforecasters. The code and datasets are available athttps://anonymous.4open.science/r/OLinear</description>
      <author>example@mail.com (Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi)</author>
      <guid isPermaLink="false">2505.08550v2</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features</title>
      <link>http://arxiv.org/abs/2505.08800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉技术的在线行为监控系统，利用定制化的有向图神经网络（DGNN）对火车司机的状态进行分类，旨在提高铁路安全。&lt;h4&gt;背景&lt;/h4&gt;司机疲劳对铁路安全构成重大挑战，传统系统如死机开关只能提供有限和基本的警报。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确分类司机状态（警觉、不警觉和病态）的系统，以优化铁路安全。&lt;h4&gt;方法&lt;/h4&gt;通过比较三种特征配置（仅骨骼特征、仅面部特征和两者结合）进行消融研究，优化模型输入表示。同时，引入了一个新的数据集，首次将模拟的病态条件纳入火车司机监控。&lt;h4&gt;主要发现&lt;/h4&gt;结合面部和骨骼特征在三类模型中达到最高准确率（80.88%），优于仅使用面部或骨骼特征的模型。此外，这种组合在二元警觉分类中达到超过99%的准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究通过使用基于视觉技术的先进在线监控，在提高铁路安全方面迈出了重要一步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：司机疲劳对铁路安全构成了重大挑战，传统系统如死机开关只能提供有限和基本的警报。本研究提出了一种利用定制化有向图神经网络（DGNN）的在线行为监控系统，用于将火车司机的状态分类为警觉、不警觉和病态三种。为了优化模型的输入表示，进行了消融研究，比较了三种特征配置：仅骨骼特征、仅面部特征和两者结合。实验结果表明，结合面部和骨骼特征在三类模型中取得了最高的准确率（80.88%），优于仅使用面部或骨骼特征的模型。此外，这种组合在二元警觉分类中达到了超过99%的准确率。此外，我们引入了一个新的数据集，首次将模拟的病态条件纳入火车司机监控，扩大了评估与疲劳和健康相关的风险的范围。这项工作通过使用基于视觉技术的先进在线监控，在提高铁路安全方面迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver fatigue poses a significant challenge to railway safety, withtraditional systems like the dead-man switch offering limited and basicalertness checks. This study presents an online behavior-based monitoringsystem utilizing a customised Directed-Graph Neural Network (DGNN) to classifytrain driver's states into three categories: alert, not alert, andpathological. To optimize input representations for the model, an ablationstudy was performed, comparing three feature configurations: skeletal-only,facial-only, and a combination of both. Experimental results show thatcombining facial and skeletal features yields the highest accuracy (80.88%) inthe three-class model, outperforming models using only facial or skeletalfeatures. Furthermore, this combination achieves over 99% accuracy in thebinary alertness classification. Additionally, we introduced a novel datasetthat, for the first time, incorporates simulated pathological conditions intotrain driver monitoring, broadening the scope for assessing risks related tofatigue and health. This work represents a step forward in enhancing railwaysafety through advanced online monitoring using vision-based technologies.</description>
      <author>example@mail.com (Olivia Nocentini, Marta Lagomarsino, Gokhan Solak, Younggeol Cho, Qiyi Tong, Marta Lorenzini, Arash Ajoudani)</author>
      <guid isPermaLink="false">2505.08800v1</guid>
      <pubDate>Thu, 15 May 2025 14:15:20 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Machine Learning with Triboelectric Nanogenerators: Optimizing Electrode Materials and Doping Strategies for Intelligent Energy Harves</title>
      <link>http://arxiv.org/abs/2505.07414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将机器学习技术与摩擦纳米发电机（TENGs）相结合的方法，以优化能量收集技术。通过使用图神经网络，该研究框架能够预测并提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;背景&lt;/h4&gt;将机器学习技术与TENGs结合，为优化能量收集技术提供了新的途径。&lt;h4&gt;目的&lt;/h4&gt;利用图神经网络预测和提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;方法&lt;/h4&gt;通过利用大量实验和计算结果的数据集，模型有效地对电极材料进行分类，预测最佳的掺杂比例，并建立稳健的结构-性能关系。&lt;h4&gt;主要发现&lt;/h4&gt;模型发现，铝掺杂的PTFE能量密度提高了65.7%，氟掺杂的PTFE提高了85.7%。PTFE被证明是一种高效的负极材料，当使用铜作为正极时，通过7%的银掺杂，其能量密度可达1.12 J/cm²。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅加速了材料发现，还显著降低了实验成本，为理解影响TENG性能的基本因素提供了新的见解，为智能材料设计建立了一个稳健的平台，推动了可持续能源技术和自供电系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.</description>
      <author>example@mail.com (Guanping Xu, Zirui Zhao, Zhong Lin Wang, Hai-Feng Li)</author>
      <guid isPermaLink="false">2505.07414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
  <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强3D人体运动预测的方法，通过使用估计的姿势和视频数据来减少对昂贵运动捕捉数据的依赖，以提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D人体运动预测方法依赖昂贵的运动捕捉数据，但数据收集成本高，导致数据多样性不足，模型泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提高3D人体运动预测模型的泛化能力，使其能更好地处理未见过的运动或主体。&lt;h4&gt;方法&lt;/h4&gt;将易于获取的视频中的2D姿势通过特定流程转换为运动捕捉风格的3D运动，并通过这些运动进行额外的学习，以适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在定量和定性方面都有显著影响。&lt;h4&gt;结论&lt;/h4&gt;通过使用估计的姿势和视频数据进行额外学习，可以有效提高3D人体运动预测模型的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data</title>
      <link>http://arxiv.org/abs/2505.08736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages; 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。&lt;h4&gt;背景&lt;/h4&gt;现有基于下一个标记预测的方法存在局限性，如VQ-VAE标记化导致的分辨率损失和条件生成能力不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。&lt;h4&gt;方法&lt;/h4&gt;模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。此外，模型在π介子和K介子识别等重建任务中表现出泛化能力，并展示了其微调的能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型通过使用不同的词汇表和因果多头交叉注意力，以及连续动力学条件化和无联合词汇膨胀的标记化方法，有效地解决了现有方法的局限性。&lt;h4&gt;结论&lt;/h4&gt;该模型在核物理领域具有广泛的应用前景，能够提高探测器数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。为了解决现有基于下一个标记预测的方法的局限性，我们提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。我们的模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。我们还展示了我们的模型在π介子和K介子识别等重建任务中的泛化能力，并展示了其微调的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a (proto) Foundation Model for Nuclear Physics, capable ofoperating on low-level detector inputs from Imaging Cherenkov Detectors at thefuture Electron Ion Collider. To address limitations in existing next-tokenprediction approaches-namely resolution loss from VQ-VAE tokenization and lackof conditional generation-we propose three key innovations: (i) separatevocabularies for discrete spatial features and continuous variates, combinedvia Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematicconditioning through prepended context embeddings, and (iii) scalable andsimple, high-resolution continuous variate tokenization without jointvocabulary inflation. Our model enables fast, high-fidelity generation of pixeland time sequences for Cherenkov photons, validated through closure tests inthe High Performance DIRC. We also show our model generalizes to reconstructiontasks such as pion and kaon identification, in which we show its ability toleverage fine-tuning.</description>
      <author>example@mail.com (James Giroux, Cristiano Fanelli)</author>
      <guid isPermaLink="false">2505.08736v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.08725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The dataset and code will be released at  https://github.com/zc-zhao/DriveMonkey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NuInteract的大规模数据集和DriveMonkey框架，用于改进大型视觉语言模型（LVLMs）在场景理解方面的能力。&lt;h4&gt;背景&lt;/h4&gt;LVLMs在图像理解方面取得了显著进展，但在全面场景理解和3D感知方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有LVLMs在场景理解方面的不足，特别是3D感知和指令理解的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NuInteract数据集，包含超过150万张多视角图像语言对，以及DriveMonkey框架，该框架通过可学习的查询将LVLMs与空间处理器无缝集成，并使用预训练的3D检测器初始化空间处理器。&lt;h4&gt;主要发现&lt;/h4&gt;DriveMonkey在3D视觉地面实况任务上优于一般的LVLMs，实现了9.86%的显著提升。&lt;h4&gt;结论&lt;/h4&gt;NuInteract数据集和DriveMonkey框架能够有效提高LVLMs在场景理解方面的性能。&lt;h4&gt;翻译&lt;/h4&gt;The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.</description>
      <author>example@mail.com (Zongchuang Zhao, Haoyu Fu, Dingkang Liang, Xin Zhou, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai)</author>
      <guid isPermaLink="false">2505.08725v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNN-based Precoder Design and Fine-tuning for Cell-free Massive MIMO with Real-world CSI</title>
      <link>http://arxiv.org/abs/2505.08788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无细胞大规模MIMO（CF-mMIMO）技术，并探讨了基于图神经网络（GNN）的预编码方法，通过在合成数据集和真实世界传播环境中进行训练和验证，证明了GNN预编码技术在从合成到真实无线环境中的有效泛化能力。&lt;h4&gt;背景&lt;/h4&gt;CF-mMIMO技术在未来无线网络中提供均匀高质量覆盖具有巨大潜力，但其预编码在分布式系统中的挑战需要解决。&lt;h4&gt;目的&lt;/h4&gt;通过GNN方法解决CF-mMIMO中的预编码挑战，并验证其从合成数据集到真实世界传播环境的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用模拟的信道状态信息（CSI）数据对GNN进行预训练，结合标准传播模型和小尺度瑞利衰落。然后，在物理测试平台上收集的真实CSI测量数据上对模型进行微调，采用层冻结策略以平衡预训练特征和适应真实世界条件。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的GNN模型在20 dB信噪比（SNR）下实现了约8.2比特每信道使用的增益，对应15.7%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;转移学习在GNN预编码中起着关键作用，并表明GNN预编码技术在合成到真实无线环境中的泛化具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm for delivering uniformly high-quality coverage in future wireless networks. To address the inherent challenges of precoding in such distributed systems, recent studies have explored the use of graph neural network (GNN)-based methods, using their powerful representation capabilities. However, these approaches have predominantly been trained and validated on synthetic datasets, leaving their generalizability to real-world propagation environments largely unverified. In this work, we initially pre-train the GNN using simulated channel state information (CSI) data, which incorporates standard propagation models and small-scale Rayleigh fading. Subsequently, we fine-tune the model on real-world CSI measurements collected from a physical testbed equipped with distributed access points (APs). To balance the retention of pre-trained features with adaptation to real-world conditions, we adopt a layer-freezing strategy during fine-tuning, wherein several GNN layers are frozen and only the later layers remain trainable. Numerical results demonstrate that the fine-tuned GNN significantly outperforms the pre-trained model, achieving an approximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR), corresponding to a 15.7 % improvement. These findings highlight the critical role of transfer learning and underscore the potential of GNN-based precoding techniques to effectively generalize from synthetic to real-world wireless environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm fordelivering uniformly high-quality coverage in future wireless networks. Toaddress the inherent challenges of precoding in such distributed systems,recent studies have explored the use of graph neural network (GNN)-basedmethods, using their powerful representation capabilities. However, theseapproaches have predominantly been trained and validated on synthetic datasets,leaving their generalizability to real-world propagation environments largelyunverified. In this work, we initially pre-train the GNN using simulatedchannel state information (CSI) data, which incorporates standard propagationmodels and small-scale Rayleigh fading. Subsequently, we finetune the model onreal-world CSI measurements collected from a physical testbed equipped withdistributed access points (APs). To balance the retention of pre-trainedfeatures with adaptation to real-world conditions, we adopt a layer-freezingstrategy during fine-tuning, wherein several GNN layers are frozen and only thelater layers remain trainable. Numerical results demonstrate that thefine-tuned GNN significantly outperforms the pre-trained model, achieving anapproximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR),corresponding to a 15.7 % improvement. These findings highlight the criticalrole of transfer learning and underscore the potential of GNN-based precodingtechniques to effectively generalize from synthetic to real-world wirelessenvironments.</description>
      <author>example@mail.com (Tianzheng Miao, Thomas Feys, Gilles Callebaut, Jarne Van Mulders, Emanuele Peschiera, Md Arifur Rahman, François Rottenberg)</author>
      <guid isPermaLink="false">2505.08788v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.08614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WaveGuard的主动水印框架，旨在解决Deepfake技术带来的隐私侵犯和身份盗窃风险。&lt;h4&gt;背景&lt;/h4&gt;Deepfake技术存在隐私侵犯和身份盗窃等风险。&lt;h4&gt;目的&lt;/h4&gt;提出WaveGuard框架以应对Deepfake技术的威胁。&lt;h4&gt;方法&lt;/h4&gt;WaveGuard通过频域嵌入和基于图的结构一致性来增强鲁棒性和不可见性。具体来说，使用双树复小波变换（DT-CWT）将水印嵌入到高频子带，并采用结构一致性图神经网络（SC-GNN）来保持视觉质量。此外，还设计了一个注意力模块来提高嵌入精度。&lt;h4&gt;主要发现&lt;/h4&gt;在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WaveGuard框架在应对Deepfake技术威胁方面具有良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake technology poses increasing risks such as privacy invasion andidentity theft. To address these threats, we propose WaveGuard, a proactivewatermarking framework that enhances robustness and imperceptibility viafrequency-domain embedding and graph-based structural consistency.Specifically, we embed watermarks into high-frequency sub-bands using Dual-TreeComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency GraphNeural Network (SC-GNN) to preserve visual quality. We also design an attentionmodule to refine embedding precision. Experimental results on face swap andreenactment tasks demonstrate that WaveGuard outperforms state-of-the-artmethods in both robustness and visual quality. Code is available athttps://github.com/vpsg-research/WaveGuard.</description>
      <author>example@mail.com (Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma)</author>
      <guid isPermaLink="false">2505.08614v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SAR-GTR: Attributed Scattering Information Guided SAR Graph Transformer Recognition Algorithm</title>
      <link>http://arxiv.org/abs/2505.08547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用电磁散射信息进行SAR数据解释的方法，提出了SAR图变换识别算法（SAR-GTR），通过结合GNN和Transformer机制，有效提升了SAR解释的性能。&lt;h4&gt;背景&lt;/h4&gt;电磁散射信息在SAR解释领域是一个重要的研究方向，而GNN能够有效整合专业知识和先验知识，解决SAR解释中的样本限制和泛化问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在深入探究单通道SAR的电磁逆散射信息，并重新审视GNN在SAR解释中的应用限制。&lt;h4&gt;方法&lt;/h4&gt;提出了SAR图变换识别算法（SAR-GTR），该算法通过区分离散和连续参数的映射方法，避免信息混淆和损失。GTR结合了GNN和Transformer机制，并引入边缘信息增强通道，以促进节点和边缘特征的交互式学习，捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了层次拓扑感知系统，充分利用目标的层次结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;SAR-GTR能够有效处理电磁散射参数的属性和特征，并通过边缘信息增强和层次拓扑结构提升SAR解释的性能。&lt;h4&gt;结论&lt;/h4&gt;SAR-GTR算法在ATRNet-STAR大规模车辆数据集上验证了其有效性，为SAR数据解释提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前，利用电磁散射信息进行SAR数据解释是SAR解释领域的一个重要研究方向。图神经网络（GNN）能够有效地整合领域特定的物理知识和人类先验知识，从而缓解了SAR解释中样本可用性有限和泛化能力差等挑战。在本研究中，我们深入研究了单通道SAR的电磁逆散射信息，并重新审视了将GNN应用于SAR解释的限制。我们提出了SAR图变换识别算法（SAR-GTR）。SAR-GTR通过区分离散和连续参数的映射方法，仔细考虑了不同电磁散射参数的属性和特征，从而避免了信息混淆和损失。此外，GTR结合了GNN和Transformer机制，并引入了边缘信息增强通道，以促进节点和边缘特征的交互式学习，从而能够捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了一个层次拓扑感知系统，充分利用了目标的层次结构信息。最后，通过使用ATRNet-STAR大规模车辆数据集验证了该算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Utilizing electromagnetic scattering information for SAR data interpretationis currently a prominent research focus in the SAR interpretation domain. GraphNeural Networks (GNNs) can effectively integrate domain-specific physicalknowledge and human prior knowledge, thereby alleviating challenges such aslimited sample availability and poor generalization in SAR interpretation. Inthis study, we thoroughly investigate the electromagnetic inverse scatteringinformation of single-channel SAR and re-examine the limitations of applyingGNNs to SAR interpretation. We propose the SAR Graph Transformer RecognitionAlgorithm (SAR-GTR). SAR-GTR carefully considers the attributes andcharacteristics of different electromagnetic scattering parameters bydistinguishing the mapping methods for discrete and continuous parameters,thereby avoiding information confusion and loss. Furthermore, the GTR combinesGNNs with the Transformer mechanism and introduces an edge informationenhancement channel to facilitate interactive learning of node and edgefeatures, enabling the capture of robust and global structural characteristicsof targets. Additionally, the GTR constructs a hierarchical topology-awaresystem through global node encoding and edge position encoding, fullyexploiting the hierarchical structural information of targets. Finally, theeffectiveness of the algorithm is validated using the ATRNet-STAR large-scalevehicle dataset.</description>
      <author>example@mail.com (Xuying Xiong, Xinyu Zhang, Weidong Jiang, Li Liu, Yongxiang Liu, Tianpeng Liu)</author>
      <guid isPermaLink="false">2505.08547v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
      <link>http://arxiv.org/abs/2505.08665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SkillFormer，一种用于从自视角和外视角视频中统一多视角技能水平估计的参数高效架构。&lt;h4&gt;背景&lt;/h4&gt;评估复杂活动中的人类技能水平是一个具有应用在体育、康复和培训中的挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的架构，能够从多视角视频中准确评估技能水平。&lt;h4&gt;方法&lt;/h4&gt;SkillFormer基于TimeSformer骨干网络，引入了CrossViewFusion模块，该模块通过多头交叉注意力、可学习门控和自适应自校准融合视图特定特征。此外，利用低秩适应来微调参数，显著降低训练成本。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoExo4D数据集上，SkillFormer在多视角设置中实现了最先进的准确性，同时表现出卓越的计算效率，参数数量比先前基线减少了4.5倍，训练轮数减少了3.75倍。&lt;h4&gt;结论&lt;/h4&gt;SkillFormer在多个结构化任务中表现出色，证实了多视角集成对于精细技能评估的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Assessing human skill levels in complex activities is a challenging problemwith applications in sports, rehabilitation, and training. In this work, wepresent SkillFormer, a parameter-efficient architecture for unified multi-viewproficiency estimation from egocentric and exocentric videos. Building on theTimeSformer backbone, SkillFormer introduces a CrossViewFusion module thatfuses view-specific features using multi-head cross-attention, learnablegating, and adaptive self-calibration. We leverage Low-Rank Adaptation tofine-tune only a small subset of parameters, significantly reducing trainingcosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achievesstate-of-the-art accuracy in multi-view settings while demonstrating remarkablecomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewertraining epochs than prior baselines. It excels in multiple structured tasks,confirming the value of multi-view integration for fine-grained skillassessment.</description>
      <author>example@mail.com (Edoardo Bianchi, Antonio Liotta)</author>
      <guid isPermaLink="false">2505.08665v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.08361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WM3C的强化学习新框架，通过学习利用组合因果组件来提升强化学习在未知环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在遇到具有未知动态的新环境时，泛化能力仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在通过借鉴人类的组合推理能力，提高强化学习在未知环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;WM3C框架通过识别和利用可组合元素之间的因果动力学，将语言作为一种组合模式，将潜在空间分解为有意义的组件，并使用掩码自动编码器和自适应稀疏正则化来捕获高级语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;WM3C在识别潜在过程、改进策略学习和泛化到未见任务方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WM3C为强化学习在未知环境中的泛化提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization in reinforcement learning (RL) remains a significantchallenge, especially when agents encounter novel environments with unseendynamics. Drawing inspiration from human compositional reasoning -- where knowncomponents are reconfigured to handle new situations -- we introduce WorldModeling with Compositional Causal Components (WM3C). This novel frameworkenhances RL generalization by learning and leveraging compositional causalcomponents. Unlike previous approaches focusing on invariant representationlearning or meta-learning, WM3C identifies and utilizes causal dynamics amongcomposable elements, facilitating robust adaptation to new tasks. Our approachintegrates language as a compositional modality to decompose the latent spaceinto meaningful components and provides theoretical guarantees for their uniqueidentification under mild assumptions. Our practical implementation uses amasked autoencoder with mutual information constraints and adaptive sparsityregularization to capture high-level semantic information and effectivelydisentangle transition dynamics. Experiments on numerical simulations andreal-world robotic manipulation tasks demonstrate that WM3C significantlyoutperforms existing methods in identifying latent processes, improving policylearning, and generalizing to unseen tasks.</description>
      <author>example@mail.com (Xinyue Wang, Biwei Huang)</author>
      <guid isPermaLink="false">2505.08361v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding</title>
      <link>http://arxiv.org/abs/2505.08194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLTP的语言触觉预训练框架，用于实现触觉感知与视觉-语言模型（VLMs）的整合，以增强机器人多模态感知能力。&lt;h4&gt;背景&lt;/h4&gt;现有的触觉描述主要限于表面属性，如纹理，忽略了机器人操作中关键的接触状态。&lt;h4&gt;目的&lt;/h4&gt;提出CLTP框架，以实现接触状态感知的触觉语言理解，从而提高机器人操作任务的效果。&lt;h4&gt;方法&lt;/h4&gt;收集了包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（如接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体文本和触觉模态。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了CLTP在零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互等三个下游任务中的优越性。&lt;h4&gt;结论&lt;/h4&gt;CLTP是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;近期将触觉感知与视觉-语言模型（VLMs）整合的研究取得了显著进展，展示了在机器人多模态感知方面的巨大潜力。然而，现有的触觉描述仅限于表面属性，如纹理，忽略了机器人操作中至关重要的接触状态。为了弥合这一差距，我们提出了CLTP，一个直观且有效的语言触觉预训练框架，它将触觉3D点云与自然语言对齐于各种接触场景中，从而实现接触状态感知的触觉语言理解，为富含接触操作的机器人任务提供支持。我们首先收集了一个包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（例如，接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体的文本和触觉模态。实验验证了它在三个下游任务中的优越性：零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互。据我们所知，这是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。代码和数据集已开源，网址为https://sites.google.com/view/cltp/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in integrating tactile sensing with vision-languagemodels (VLMs) have demonstrated remarkable potential for robotic multimodalperception. However, existing tactile descriptions remain limited tosuperficial attributes like texture, neglecting critical contact statesessential for robotic manipulation. To bridge this gap, we propose CLTP, anintuitive and effective language tactile pretraining framework that alignstactile 3D point clouds with natural language in various contact scenarios,thus enabling contact-state-aware tactile language understanding forcontact-rich manipulation tasks. We first collect a novel dataset of 50k+tactile 3D point cloud-language pairs, where descriptions explicitly capturemultidimensional contact states (e.g., contact location, shape, and force) fromthe tactile sensor's perspective. CLTP leverages a pre-aligned and frozenvision-language feature space to bridge holistic textual and tactilemodalities. Experiments validate its superiority in three downstream tasks:zero-shot 3D classification, contact state classification, and tactile 3D largelanguage model (LLM) interaction. To the best of our knowledge, this is thefirst study to align tactile and language representations from the contactstate perspective for manipulation tasks, providing great potential fortactile-language-action model learning. Code and datasets are open-sourced athttps://sites.google.com/view/cltp/.</description>
      <author>example@mail.com (Wenxuan Ma, Xiaoge Cao, Yixiang Zhang, Chaofan Zhang, Shaobo Yang, Peng Hao, Bin Fang, Yinghao Cai, Shaowei Cui, Shuo Wang)</author>
      <guid isPermaLink="false">2505.08194v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
      <link>http://arxiv.org/abs/2505.08430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的邻近上下文聚合框架（GNCAF），用于对全切片图像（WSI）中的三级淋巴结构（TLS）进行语义分割，以改善TLS成熟度和面积的量化，从而提高预后任务的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的TLS评估方法通常依赖于细胞代理任务，并需要额外的后处理步骤。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TLS语义分割任务（TLS-SS），以端到端的方式在WSI中分割TLS的区域和成熟阶段。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于GNN的邻近上下文聚合框架（GNCAF），该框架通过逐步聚合目标及其邻近区域的多跳上下文信息，并使用自注意力机制来指导目标区域的分割。&lt;h4&gt;主要发现&lt;/h4&gt;GNCAF能够与各种分割模型集成，增强其感知超出局部区域上下文信息的能力。在TCGA-COAD和INHOUSE-PAAD数据集上的实验表明，GNCAF在mF1和mIoU方面分别实现了22.08%和26.57%的最大提升。此外，GNCAF在淋巴结转移分割任务中也验证了其任务的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GNCAF是一种有效的TLS语义分割方法，能够显著提高TLS分割的准确性，并具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tertiary lymphoid structures (TLS) are organized clusters of immune cells,whose maturity and area can be quantified in whole slide image (WSI) forvarious prognostic tasks. Existing methods for assessing these characteristicstypically rely on cell proxy tasks and require additional post-processingsteps. In this work, We focus on a novel task-TLS Semantic Segmentation(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI inan end-to-end manner. Due to the extensive scale of WSI and patch-basedsegmentation strategies, TLS-SS necessitates integrating from neighboringpatches to guide target patch (target) segmentation. Previous techniques oftenemploy on multi-resolution approaches, constraining the capacity to leveragethe broader neighboring context while tend to preserve coarse-grainedinformation. To address this, we propose a GNN-based Neighboring ContextAggregation Framework (GNCAF), which progressively aggregates multi-hopneighboring context from the target and employs a self-attention mechanism toguide the segmentation of the target. GNCAF can be integrated with varioussegmentation models to enhance their ability to perceive contextual informationoutside of the patch. We build two TLS-SS datasets, called TCGA-COAD andINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publiclyavailable. Experiments on these datasets demonstrate the superiority of GNCAF,achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,respectively. Additionally, we also validate the task scalability of GNCAF onsegmentation of lymph node metastases.</description>
      <author>example@mail.com (Lei Su)</author>
      <guid isPermaLink="false">2505.08430v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2505.08101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的蒸馏框架，用于在资源受限环境中部署高性能模型，如Point Transformer V3，通过拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型转移到轻量级学生模型。&lt;h4&gt;背景&lt;/h4&gt;点云处理在自动驾驶和3D物体识别等应用中扮演着关键角色，但将高性能模型部署在资源受限环境中由于计算和内存需求高而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的蒸馏框架，以在资源受限环境中部署高性能点云处理模型。&lt;h4&gt;方法&lt;/h4&gt;该方法利用拓扑感知表示和梯度引导的知识蒸馏，同时捕捉点云的底层几何结构，并通过基于梯度的特征对齐指导学生模型的训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，该方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。在NuScenes数据集上，该方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。&lt;h4&gt;结论&lt;/h4&gt;提出的蒸馏框架在保持高性能的同时，显著降低了模型的计算和内存需求，适用于资源受限的环境。&lt;h4&gt;翻译&lt;/h4&gt;点云处理由于在自动驾驶和3D物体识别等应用中的关键作用而受到广泛关注。然而，由于高性能模型如Point Transformer V3的计算和内存需求高，在资源受限的环境中部署这些模型仍然具有挑战性。本研究引入了一种新颖的蒸馏框架，该框架利用拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型传递到轻量级学生模型。我们的方法捕捉了点云的底层几何结构，并通过基于梯度的特征对齐有选择地指导学生模型的训练过程。在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，所提出的方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。值得注意的是，在NuScenes数据集上，我们的方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。我们的实现代码已公开，可在https://github.com/HySonLab/PointDistill上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hysonlab/pointdistill&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing has gained significant attention due to its criticalrole in applications such as autonomous driving and 3D object recognition.However, deploying high-performance models like Point Transformer V3 inresource-constrained environments remains challenging due to their highcomputational and memory demands. This work introduces a novel distillationframework that leverages topology-aware representations and gradient-guidedknowledge distillation to effectively transfer knowledge from a high-capacityteacher to a lightweight student model. Our approach captures the underlyinggeometric structures of point clouds while selectively guiding the studentmodel's learning process through gradient-based feature alignment. Experimentalresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that theproposed method achieves competitive performance, with an approximately 16xreduction in model size and a nearly 1.9x decrease in inference time comparedto its teacher model. Notably, on NuScenes, our method achievesstate-of-the-art performance among knowledge distillation techniques trainedsolely on LiDAR data, surpassing prior knowledge distillation baselines insegmentation performance. Our implementation is available publicly at:  https://github.com/HySonLab/PointDistill</description>
      <author>example@mail.com (Luu Tung Hai, Thinh D. Le, Zhicheng Ding, Qing Tian, Truong-Son Hy)</author>
      <guid isPermaLink="false">2505.08101v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
      <link>http://arxiv.org/abs/2505.08552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了生成式AI工具在视觉内容创作中的应用，特别是视觉艺术品领域，以及由此引发的版权侵权和伪造问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，生成式AI工具在视觉内容创作中的应用日益增多，尤其是用于视觉艺术品的创作，这引起了关于版权侵权和伪造的严重担忧。&lt;h4&gt;目的&lt;/h4&gt;提出了一种名为DFA-CON的对比学习框架，旨在检测侵犯版权或伪造的AI生成艺术。&lt;h4&gt;方法&lt;/h4&gt;该框架学习一个具有判别性的表征空间，在对比学习框架中，对原创艺术品及其伪造品之间的亲和力进行建模。模型在多种攻击类型下进行训练，包括修复、风格迁移、对抗性扰动和cutmix。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，该模型在大多数攻击类型上表现出稳健的检测性能，优于最近的预训练基础模型。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个有效的方法来检测AI生成的艺术作品中的版权侵权问题，并将相关代码和模型检查点公开。&lt;h4&gt;翻译&lt;/h4&gt;The paper discusses the application of generative AI tools in visual content creation, especially in the context of visual art, and the serious concerns about copyright infringement and forgery that this has raised. In recent years, the application of generative AI tools in visual content creation has increased, especially in the creation of visual art, which has raised serious concerns about copyright infringement and forgery. This paper proposes a contrastive learning framework named DFA-CON, designed to detect copyright-infringing or forged AI-generated art. The framework learns a discriminative representation space, modeling affinity between original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. The code and model checkpoints will be publicly released upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent proliferation of generative AI tools for visual contentcreation-particularly in the context of visual artworks-has raised seriousconcerns about copyright infringement and forgery. The large-scale datasetsused to train these models often contain a mixture of copyrighted andnon-copyrighted artworks. Given the tendency of generative models to memorizetraining patterns, they are susceptible to varying degrees of copyrightviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,this work introduces DFA-CON, a contrastive learning framework designed todetect copyright-infringing or forged AI-generated art. DFA-CON learns adiscriminative representation space, posing affinity among original artworksand their forged counterparts within a contrastive learning framework. Themodel is trained across multiple attack types, including inpainting, styletransfer, adversarial perturbation, and cutmix. Evaluation results demonstraterobust detection performance across most attack types, outperforming recentpretrained foundation models. Code and model checkpoints will be releasedpublicly upon acceptance.</description>
      <author>example@mail.com (Haroon Wahab, Hassan Ugail, Irfan Mehmood)</author>
      <guid isPermaLink="false">2505.08552v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Revealing economic facts: LLMs know more than they say</title>
      <link>http://arxiv.org/abs/2505.08662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）的隐藏状态是否可以用于估计和推断经济和金融统计数据。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于县级行政区和公司层面的经济和财务变量，如失业率和总资产。&lt;h4&gt;目的&lt;/h4&gt;探究隐藏状态是否比LLMs的直接响应包含更丰富的经济信息。&lt;h4&gt;方法&lt;/h4&gt;使用简单线性模型对开源LLMs的隐藏状态进行训练，并与模型的文本输出进行比较。此外，还提出了一个迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高估计精度。&lt;h4&gt;主要发现&lt;/h4&gt;隐藏状态模型在估计经济和金融统计数据方面优于模型的文本输出，且仅需少量标记样本即可进行训练。&lt;h4&gt;结论&lt;/h4&gt;隐藏状态表示在超分辨率和数据推断任务中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了大型语言模型的隐藏状态是否可以用来估计和推断经济和金融统计数据。我们专注于县级行政区（例如失业率）和公司层面（例如总资产）的变量，我们发现基于开源LLMs隐藏状态的简单线性模型优于模型的文本输出。这表明隐藏状态比LLMs直接揭示的响应包含更丰富的经济信息。学习曲线分析表明，仅需要几十个标记样本就足以进行训练。我们还提出了一种迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高了估计精度。最后，我们展示了隐藏状态表示在超分辨率和数据推断任务中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate whether the hidden states of large language models (LLMs) canbe used to estimate and impute economic and financial statistics. Focusing oncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,we show that a simple linear model trained on the hidden states of open-sourceLLMs outperforms the models' text outputs. This suggests that hidden statescapture richer economic information than the responses of the LLMs revealdirectly. A learning curve analysis indicates that only a few dozen labelledexamples are sufficient for training. We also propose a transfer learningmethod that improves estimation accuracy without requiring any labelled datafor the target variable. Finally, we demonstrate the practical utility ofhidden-state representations in super-resolution and data imputation tasks.</description>
      <author>example@mail.com (Marcus Buckmann, Quynh Anh Nguyen, Edward Hill)</author>
      <guid isPermaLink="false">2505.08662v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification</title>
      <link>http://arxiv.org/abs/2505.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用大型语言模型（LLMs）作为特征增强器优化节点表示，并将其作为图神经网络（GNNs）输入的潜力，并基于互换干预方法进行了深入研究。&lt;h4&gt;背景&lt;/h4&gt;LLMs在图表示学习中的应用展现出巨大潜力，但其基本性质尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;通过更深入的分析，探究LLMs增强器和GNNs的深层性质及其内部机制。&lt;h4&gt;方法&lt;/h4&gt;构建了一个具有可控因果关系的合成图数据集，并使用互换干预方法进行分析。基于分析结果，设计了一个即插即用的优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了LLMs增强器和GNNs的内在逻辑和内部机制，并验证了优化模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提高LLMs增强器和GNNs之间的信息传递效率。&lt;h4&gt;翻译&lt;/h4&gt;The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) as feature enhancers to optimize noderepresentations, which are then used as inputs for graph neural networks(GNNs), has shown significant potential in graph representation learning.However, the fundamental properties of this approach remain underexplored. Toaddress this issue, we propose conducting a more in-depth analysis of thisissue based on the interchange intervention method. First, we construct asynthetic graph dataset with controllable causal relationships, enablingprecise manipulation of semantic relationships and causal modeling to providedata for analysis. Using this dataset, we conduct interchange interventions toexamine the deeper properties of LLM enhancers and GNNs, uncovering theirunderlying logic and internal mechanisms. Building on the analytical results,we design a plug-and-play optimization module to improve the informationtransfer between LLM enhancers and GNNs. Experiments across multiple datasetsand models validate the proposed module.</description>
      <author>example@mail.com (Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.08265v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
      <link>http://arxiv.org/abs/2505.08455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Video-based long-form Causal Reasoning (VCRBench)这一新的基准，用于评估大型视频语言模型（LVLMs）在视频因果推理方面的能力，并提出了一种名为Recognition-Reasoning Decomposition (RRD)的模块化方法来提高LVLMs在视频因果推理任务上的准确率。&lt;h4&gt;背景&lt;/h4&gt;尽管视频理解取得了进展，但LVLMs在视频因果推理方面的能力尚未得到充分探索，主要原因是缺乏相关和专门的基准来评估在视觉基础和目标驱动环境中的因果推理。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文旨在提出一个名为VCRBench的新基准，用于评估LVLMs在视频因果推理方面的能力。&lt;h4&gt;方法&lt;/h4&gt;VCRBench使用日常活动的程序视频创建，其中步骤被故意打乱，每个视频片段捕捉一个关键因果事件，以测试LVLMs是否能够识别、推理和正确排序实现特定目标所需的事件。此外，该基准被精心设计，以防止LVLMs利用语言捷径，同时避免开放式问答评估的挑战。作者还提出了RRD方法，将视频因果推理分解为视频识别和因果推理两个子任务。&lt;h4&gt;主要发现&lt;/h4&gt;在VCRBench上的评估表明，最先进的LVLMs在视频因果推理方面存在困难，主要因为它们难以直接从视觉观察中建模长距离因果依赖关系。RRD方法显著提高了VCRBench上的准确率，最高提升达25.2%。&lt;h4&gt;结论&lt;/h4&gt;LVLMs主要依赖于语言知识来完成复杂的视频因果推理任务。&lt;h4&gt;翻译&lt;/h4&gt;Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pritamqu/vcrbench&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.</description>
      <author>example@mail.com (Pritam Sarkar, Ali Etemad)</author>
      <guid isPermaLink="false">2505.08455v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
      <link>http://arxiv.org/abs/2505.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用低分辨率热图像和计算机视觉技术进行人员占用检测，以提高HVAC系统的能效和操作。&lt;h4&gt;背景&lt;/h4&gt;传统的HVAC系统通常不考虑占用情况，而先进的以用户为中心的控制（OCC）系统则考虑了占用状态。然而，使用RGB图像和计算机视觉技术进行占用检测存在隐私问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用低分辨率热图像和计算机视觉技术的人员占用检测模型，以解决隐私问题并降低计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;使用转移学习技术微调YOLOv5模型，以实现基于低分辨率热图像的占用检测。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在占用检测方面取得了满意的性能，精确度、召回率、mAP50和mAP50值接近1.000。&lt;h4&gt;结论&lt;/h4&gt;该模型不仅解决了隐私问题，还降低了计算资源的需求，对HVAC系统的能效和操作有积极影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy plays an essential role in influencing the energy consumption andoperation of heating, ventilation, and air conditioning (HVAC) systems.Traditional HVAC typically operate on fixed schedules without consideringoccupancy. Advanced occupant-centric control (OCC) adopted occupancy status inregulating HVAC operations. RGB images combined with computer vision (CV)techniques are widely used for occupancy detection, however, the detailedfacial and body features they capture raise significant privacy concerns.Low-resolution thermal images offer a non-invasive solution that mitigatesprivacy issues. The study developed an occupancy detection model utilizinglow-resolution thermal images and CV techniques, where transfer learning wasapplied to fine-tune the You Only Look Once version 5 (YOLOv5) model. Thedeveloped model ultimately achieved satisfactory performance, with precision,recall, mAP50, and mAP50 values approaching 1.000. The contributions of thismodel lie not only in mitigating privacy concerns but also in reducingcomputing resource demands.</description>
      <author>example@mail.com (Xue Cui, Vincent Gbouna Zakka, Minhyun Lee)</author>
      <guid isPermaLink="false">2505.08336v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
      <link>http://arxiv.org/abs/2505.08316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for full publication at CogSci 2025  (https://cognitivesciencesociety.org/cogsci-2025/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究VVS（腹侧视觉通路）的功能，提出了一种结合相对位置预测（RP）学习与对比学习的新的无监督任务驱动方法来建模VVS，并证明了VVS在计算层面与位置感知（尤其是RP预测）有关。&lt;h4&gt;背景&lt;/h4&gt;现有的无监督任务驱动方法通过对比学习建模VVS，主要关注物体识别，但作者认为VVS的功能不仅仅局限于物体识别。&lt;h4&gt;目的&lt;/h4&gt;引入相对位置预测作为VVS的附加功能，并设计一种新的无监督任务驱动方法来建模VVS。&lt;h4&gt;方法&lt;/h4&gt;理论上解释了对比学习可能无法实现RP预测的能力，并提出将RP学习与对比学习结合的新方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在提高物体识别性能的同时增强了RP预测能力，且RP预测能力普遍提高了模型的大脑相似度。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明VVS在位置感知中起着重要作用，特别是在RP预测方面。&lt;h4&gt;翻译&lt;/h4&gt;Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rdz98/unsup-vvs&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the concept that ventral visual stream (VVS) mainly functions forobject recognition, current unsupervised task-driven methods model VVS bycontrastive learning, and have achieved good brain similarity. However, webelieve functions of VVS extend beyond just object recognition. In this paper,we introduce an additional function involving VVS, named relative position (RP)prediction. We first theoretically explain contrastive learning may be unableto yield the model capability of RP prediction. Motivated by this, wesubsequently integrate RP learning with contrastive learning, and propose a newunsupervised task-driven method to model VVS, which is more inline withbiological reality. We conduct extensive experiments, demonstrating that: (i)our method significantly improves downstream performance of object recognitionwhile enhancing RP predictivity; (ii) RP predictivity generally improves themodel brain similarity. Our results provide strong evidence for the involvementof VVS in location perception (especially RP prediction) from a computationalperspective.</description>
      <author>example@mail.com (Dazhong Rong, Hao Dong, Xing Gao, Jiyu Wei, Di Hong, Yaoyao Hao, Qinming He, Yueming Wang)</author>
      <guid isPermaLink="false">2505.08316v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People</title>
      <link>http://arxiv.org/abs/2505.08215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了影响语音清晰度预测性能的关键设计因素，并提出了针对有听力障碍人群的语音清晰度预测方法。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型（SFMs）在多种下游任务中表现出色，但针对有听力障碍人群的语音清晰度预测（SIP-HI）的优化研究不足。&lt;h4&gt;目的&lt;/h4&gt;通过研究，确定影响SIP-HI性能的关键设计因素，并提出有效的语音清晰度预测方法。&lt;h4&gt;方法&lt;/h4&gt;对5个SFMs进行综合研究，关注编码器层选择、预测头架构和集成配置，并探讨关键SFM属性与其对SIP-HI性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;与传统使用所有层的方法不同，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。集成多个SFMs可以提高性能，更强个体的模型提供更大的益处。&lt;h4&gt;结论&lt;/h4&gt;研究为有效适应SFMs进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型（SFMs）在众多下游任务中表现出色，包括为听力受损人群进行语音清晰度预测（SIP-HI）。然而，针对SIP-HI对SFMs进行优化的研究尚不充分。在本文中，我们进行了一项全面的研究，以确定影响SIP-HI性能的关键设计因素，使用5个SFMs，重点关注编码器层选择、预测头架构和集成配置。我们的研究结果表明，与传统使用所有层的方法相反，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。我们还证明了集成多个SFMs可以提高性能，更强的个体模型提供更大的益处。最后，我们探讨了关键SFM属性及其对SIP-HI性能影响之间的关系。我们的研究为有效调整SFMs以进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models (SFMs) have demonstrated strong performance across avariety of downstream tasks, including speech intelligibility prediction forhearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has beeninsufficiently explored. In this paper, we conduct a comprehensive study toidentify key design factors affecting SIP-HI performance with 5 SFMs, focusingon encoder layer selection, prediction head architecture, and ensembleconfigurations. Our findings show that, contrary to traditional use-all-layersmethods, selecting a single encoder layer yields better results. Additionally,temporal modeling is crucial for effective prediction heads. We alsodemonstrate that ensembling multiple SFMs improves performance, with strongerindividual models providing greater benefit. Finally, we explore therelationship between key SFM attributes and their impact on SIP-HI performance.Our study offers practical insights into effectively adapting SFMs for speechintelligibility prediction for hearing-impaired populations.</description>
      <author>example@mail.com (Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan Xiang Wang)</author>
      <guid isPermaLink="false">2505.08215v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full version of the paper will be appearing at the Proceedings of the  Thirty-Third International Joint Conference on Artificial Intelligence  (IJCAI-25), Special Track on AI for Good&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KIIM的新型灌溉方法映射方法，通过利用Swin-Transformer模型和多种信息处理技术，实现了更准确和高效的灌溉方法映射。&lt;h4&gt;背景&lt;/h4&gt;现有的灌溉方法映射模型依赖于卫星图像的光谱特征，但由于农业景观的复杂性和有限的训练数据，这些模型效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的灌溉方法映射方法，以实现可持续农业实践和食物系统的精准灌溉。&lt;h4&gt;方法&lt;/h4&gt;KIIM方法利用了以下技术：(i) 特殊的投影矩阵来编码作物到灌溉概率；(ii) 空间注意力图来识别农业用地；(iii) 双向交叉注意力来关注不同模态的互补信息；(iv) 加权集成来结合图像和作物信息的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在五个美国州进行的实验中，KIIM方法相对于基线模型提高了22.9%的IoU（交并比），对于难以分类的滴灌，IoU提高了71.4%。此外，通过两阶段迁移学习方法，在数据有限的州中，IoU提升了51%。&lt;h4&gt;结论&lt;/h4&gt;KIIM方法通过仅使用40%的训练数据即可实现基线性能，提高了灌溉方法映射的效率和可行性，减少了大量手动标记的需求，使大规模自动化灌溉映射更加经济高效。&lt;h4&gt;翻译&lt;/h4&gt;Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate mapping of irrigation methods is crucial for sustainableagricultural practices and food systems. However, existing models that relysolely on spectral features from satellite imagery are ineffective due to thecomplexity of agricultural landscapes and limited training data, making this achallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), anovel Swin-Transformer based approach that uses (i) a specialized projectionmatrix to encode crop to irrigation probability, (ii) a spatial attention mapto identify agricultural lands from non-agricultural lands, (iii)bi-directional cross-attention to focus complementary information fromdifferent modalities, and (iv) a weighted ensemble for combining predictionsfrom images and crop information. Our experimentation on five states in the USshows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)improvement for hard-to-classify drip irrigation. In addition, we propose atwo-phase transfer learning approach to enhance cross-state irrigation mapping,achieving a 51% IoU boost in a state with limited labeled data. The ability toachieve baseline performance with only 40% of the training data highlights itsefficiency, reducing the dependency on extensive manual labeling efforts andmaking large-scale, automated irrigation mapping more feasible andcost-effective.</description>
      <author>example@mail.com (Oishee Bintey Hoque, Nibir Chandra Mandal, Abhijin Adiga, Samarth Swarup, Sayjro Kossi Nouwakpo, Amanda Wilson, Madhav Marathe)</author>
      <guid isPermaLink="false">2505.08302v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了城市数字孪生（UDTs）在城市管理中的重要性，并提出了一个综合的多模态城市数字孪生基准数据集TUM2TWIN，以解决城市数字孪生创建过程中的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生在城市管理中扮演着关键角色，但创建过程中存在多个阶段的挑战，如获取精确的3D数据、重建高保真模型、维护模型更新和确保数据兼容性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决城市数字孪生创建中的挑战，并推动相关研究和实际应用。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，该数据集包含地理参照的、语义对齐的3D模型和网络，以及多种地面、移动、空中和卫星观测数据，支持对传感器进行稳健分析和发展先进的重建方法。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持新型视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建等下游任务，展示了其在城市数字孪生创建中的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服现有城市数字孪生创建的局限性奠定了基础，并为更智能、数据驱动的城市环境的研究和实际解决方案提供了支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理不可或缺的工具，并能够整合来自不同来源的复杂、异构数据。创建城市数字孪生涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新以及确保下游任务的无缝互操作性。当前的数据库通常仅限于处理链的一部分，阻碍了综合城市数字孪生的验证。为了解决这些挑战，我们引入了第一个综合的多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，拥有32个数据子集，覆盖约100,000平方米，目前数据量为767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持对传感器进行稳健分析和发展先进的重建方法。此外，我们还探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和高斯喷溅的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信这一贡献为克服现有城市数字孪生创建的局限性，促进新的研究方向和实践解决方案奠定了基础。项目可在以下网址获取：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
      <link>http://arxiv.org/abs/2505.08111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference publication submitted to IEEE I2MTC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用基于床的压力敏感垫（PSM）监测睡眠中的患者睡眠姿势，并使用深度学习模型进行睡眠姿势分类。&lt;h4&gt;背景&lt;/h4&gt;睡眠姿势对睡眠质量和睡眠障碍（如呼吸暂停）的发生率有影响。&lt;h4&gt;目的&lt;/h4&gt;通过使用深度学习模型，准确估计睡眠姿势，克服临床环境中训练深度学习模型所需的大量标记数据的挑战。&lt;h4&gt;方法&lt;/h4&gt;在睡眠诊所的床上放置PSM收集数据，使用迁移学习来适应预训练的深度学习模型，包括Vision Transformer模型（ViTMAE）和预训练的人体姿态估计模型（ViTPose），并在低分辨率PSM数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;该方法优于基于PSM的睡眠姿势分类的先前工作，包括使用深度学习（TCN）和传统机器学习模型（SVM、XGBoost、随机森林）的方法，并在112个夜晚的患者记录和13个患者的更高分辨率数据集上进行了评估和验证。&lt;h4&gt;结论&lt;/h4&gt;尽管从低分辨率PSM数据中区分睡眠姿势存在挑战，但该方法在临床环境中的实际部署显示出希望。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way ofmonitoring patients during sleep. We focus on four-way sleep positionclassification using data collected from a PSM placed under a mattress in asleep clinic. Sleep positions can affect sleep quality and the prevalence ofsleep disorders, such as apnea. Measurements were performed on patients withsuspected sleep disorders referred for assessments at a sleep clinic. Trainingdeep learning models can be challenging in clinical settings due to the needfor large amounts of labeled data. To overcome the shortage of labeled trainingdata, we utilize transfer learning to adapt pre-trained deep learning models toaccurately estimate sleep positions from a low-resolution PSM dataset collectedin a polysomnography sleep lab. Our approach leverages Vision Transformermodels pre-trained on ImageNet using masked autoencoding (ViTMAE) and apre-trained model for human pose estimation (ViTPose). These approachesoutperform previous work from PSM-based sleep pose classification using deeplearning (TCN) as well as traditional machine learning models (SVM, XGBoost,Random Forest) that use engineered features. We evaluate the performance ofsleep position classification from 112 nights of patient recordings andvalidate it on a higher resolution 13-patient dataset. Despite the challengesof differentiating between sleep positions from low-resolution PSM data, ourapproach shows promise for real-world deployment in clinical settings</description>
      <author>example@mail.com (Olivier Papillon, Rafik Goubran, James Green, Julien Larivière-Chartier, Caitlin Higginson, Frank Knoefel, Rébecca Robillard)</author>
      <guid isPermaLink="false">2505.08111v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
      <link>http://arxiv.org/abs/2505.08723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TiMo是一种针对卫星图像时间序列分析的新型分层视觉Transformer基础模型，通过引入时空陀螺仪注意力机制，能够动态捕捉时空变化的多尺度模式，并在多个时空任务中展现出优越性。&lt;h4&gt;背景&lt;/h4&gt;现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，限制了其在下游任务中的有效性。&lt;h4&gt;目的&lt;/h4&gt;提出TiMo模型，以克服现有模型的局限性，提高时空基础模型在环境管理和灾害评估等应用中的效果。&lt;h4&gt;方法&lt;/h4&gt;引入时空陀螺仪注意力机制，动态捕捉时空变化的多尺度模式；构建MillionST数据集，包含100万张图像，涵盖五年内的10个时间相和100,000个地理位置；利用Masked Image Modeling对TiMo进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;TiMo在森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测等时空任务中，表现优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TiMo模型能够有效学习并编码可推广的时空表示，适用于多种时空分析任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要：卫星图像时间序列（SITS）提供了对地球表面的连续观测，对于环境管理和灾害评估等应用至关重要。然而，现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，这限制了它们在下游任务中的有效性。为了克服这一挑战，我们提出了TiMo，一种针对SITS分析的分层视觉Transformer基础模型。在核心部分，我们引入了一种时空陀螺仪注意力机制，能够动态地捕捉时间和空间上的演变多尺度模式。为了预训练，我们精心制作了MillionST数据集，包含从100,000个地理位置捕获的100万张图像，这些图像在五年内的10个时间相中捕捉到了多样化的地理空间变化和季节性变化。利用这个数据集，我们将掩码图像建模应用于预训练TiMo，使其能够有效地学习并编码可推广的时空表示。在多个时空任务（包括森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测）上的广泛实验表明，TiMo在性能上优于最先进的方法。代码、模型和数据集将在https://github.com/MiliLab/TiMo发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mililab/timo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Satellite image time series (SITS) provide continuous observations of theEarth's surface, making them essential for applications such as environmentalmanagement and disaster assessment. However, existing spatiotemporal foundationmodels rely on plain vision transformers, which encode entire temporalsequences without explicitly capturing multiscale spatiotemporal relationshipsbetween land objects. This limitation hinders their effectiveness in downstreamtasks. To overcome this challenge, we propose TiMo, a novel hierarchical visiontransformer foundation model tailored for SITS analysis. At its core, weintroduce a spatiotemporal gyroscope attention mechanism that dynamicallycaptures evolving multiscale patterns across both time and space. Forpre-training, we curate MillionST, a large-scale dataset of one million imagesfrom 100,000 geographic locations, each captured across 10 temporal phases overfive years, encompassing diverse geospatial changes and seasonal variations.Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,enabling it to effectively learn and encode generalizable spatiotemporalrepresentations.Extensive experiments across multiple spatiotemporaltasks-including deforestation monitoring, land cover segmentation, crop typeclassification, and flood detection-demonstrate TiMo's superiority overstate-of-the-art methods. Code, model, and dataset will be released athttps://github.com/MiliLab/TiMo.</description>
      <author>example@mail.com (Xiaolei Qin, Di Wang, Jing Zhang, Fengxiang Wang, Xin Su, Bo Du, Liangpei Zhang)</author>
      <guid isPermaLink="false">2505.08723v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Training Strategies for Efficient Embodied Reasoning</title>
      <link>http://arxiv.org/abs/2505.08243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了机器人思维链推理（CoT）在提高机器人策略泛化能力和性能方面的作用，特别是视觉-语言-动作模型（VLAs）。提出了新的机器人推理方法，并测试了其效果。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人CoT推理方法在性能和泛化能力上有提升，但存在局限性，如需要特定的机器人推理数据集和较慢的推理速度。&lt;h4&gt;目的&lt;/h4&gt;设计新的机器人推理方法，解决现有方法的局限性，并深入理解推理如何帮助策略性能提升。&lt;h4&gt;方法&lt;/h4&gt;提出了三种机器人推理提升策略，并设计了简单变体来测试每种策略。通过学习生成推理和关注推理，来提升VLA模型的表示学习和动作预测。&lt;h4&gt;主要发现&lt;/h4&gt;学习生成推理可以提升VLA表示，关注推理有助于利用这些特征进行更好的动作预测。新方法在LIBERO-90基准测试中取得了显著性能提升，推理速度比标准方法快3倍。&lt;h4&gt;结论&lt;/h4&gt;CoT推理有助于提升VLAs，提出了两种简单轻量级的替代方法，这些方法在性能和推理速度上都有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpfulintermediate representations before choosing actions -- provides an effectivemethod for improving the generalization and performance of robot policies,especially vision-language-action models (VLAs). While such approaches havebeen shown to improve performance and generalization, they suffer from corelimitations, like needing specialized robot reasoning data and slow inferencespeeds. To design new robot reasoning approaches that address these issues, amore complete characterization of why reasoning helps policy performance iscritical. We hypothesize several mechanisms by which robot reasoning improvespolicies -- (1) better representation learning, (2) improved learningcurricularization, and (3) increased expressivity -- then devise simplevariants of robot CoT reasoning to isolate and test each one. We find thatlearning to generate reasonings does lead to better VLA representations, whileattending to the reasonings aids in actually leveraging these features forimproved action prediction. Our results provide us with a better understandingof why CoT reasoning helps VLAs, which we use to introduce two simple andlightweight alternative recipes for robot reasoning. Our proposed approachesachieve significant performance gains over non-reasoning policies,state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedupcompared to standard robot reasoning.</description>
      <author>example@mail.com (William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine)</author>
      <guid isPermaLink="false">2505.08243v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
      <link>http://arxiv.org/abs/2505.08266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphVision Network (GVN)的有效框架，以及其高效的变体E-GVN，旨在通过视觉感知增强来提高链接预测任务中的MPNNs性能。&lt;h4&gt;背景&lt;/h4&gt;MPNNs和结构特征（SFs）是链接预测任务的基础，但视觉感知在MPNN社区中尚未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够利用视觉感知的框架，以提升MPNNs在链接预测任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了GraphVision Network (GVN)和E-GVN两种框架，并通过七个链接预测数据集进行了实证研究。&lt;h4&gt;主要发现&lt;/h4&gt;GVN在七个链接预测数据集上，包括挑战性的大规模图数据集，都实现了视觉增强带来的性能提升，这些改进与现有最先进（SOTA）方法兼容，并且GVN达到了新的SOTA结果。&lt;h4&gt;结论&lt;/h4&gt;GVN和E-GVN为链接预测任务提供了一个有希望的新的研究方向，证明了视觉感知在MPNNs中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message-passing graph neural networks (MPNNs) and structural features (SFs)are cornerstones for the link prediction task. However, as a common andintuitive mode of understanding, the potential of visual perception has beenoverlooked in the MPNN community. For the first time, we equip MPNNs withvision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensiveempirical results demonstrate that with the proposed frameworks, GVNconsistently benefits from the vision enhancement across seven link predictiondatasets, including challenging large-scale graphs. Such improvements arecompatible with existing state-of-the-art (SOTA) methods and GVNs achieve newSOTA results, thereby underscoring a promising novel direction for linkprediction.</description>
      <author>example@mail.com (Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok)</author>
      <guid isPermaLink="false">2505.08266v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation</title>
      <link>http://arxiv.org/abs/2505.08157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双曲对比学习的知识感知推荐方法，旨在解决现有对比学习方法在捕获用户-物品二分图和知识图中的层次结构以及生成正样本时的偏好学习偏移问题。&lt;h4&gt;背景&lt;/h4&gt;基于图神经网络（GNNs）和对比学习的知识感知推荐已成为主流，但现有方法在有效捕获用户-物品二分图和知识图中的层次结构以及生成正样本时存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的双曲对比学习方法，通过模型增强来提高知识感知推荐的效果。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新的洛伦兹知识聚合机制来捕获内在的层次图结构，并提出了三种模型级增强技术来辅助双曲对比学习，避免增强正样本对之间的偏好偏移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在实验中显示出优于现有基线的优越性，最大提升了11.03%。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高知识感知推荐的效果，为解决现有对比学习方法中的问题提供了一种新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benefiting from the effectiveness of graph neural networks (GNNs) andcontrastive learning, GNN-based contrastive learning has become mainstream forknowledge-aware recommendation. However, most existing contrastivelearning-based methods have difficulties in effectively capturing theunderlying hierarchical structure within user-item bipartite graphs andknowledge graphs. Moreover, they commonly generate positive samples forcontrastive learning by perturbing the graph structure, which may lead to ashift in user preference learning. To overcome these limitations, we proposehyperbolic contrastive learning with model-augmentation for knowledge-awarerecommendation. To capture the intrinsic hierarchical graph structures, wefirst design a novel Lorentzian knowledge aggregation mechanism, which enablesmore effective representations of users and items. Then, we propose threemodel-level augmentation techniques to assist Hyperbolic contrastive learning.Different from the classical structure-level augmentation (e.g., edgedropping), the proposed model-augmentations can avoid preference shifts betweenthe augmented positive pair. Finally, we conduct extensive experiments todemonstrate the superiority (maximum improvement of $11.03\%$) of proposedmethods over existing baselines.</description>
      <author>example@mail.com (Shengyin Sun, Chen Ma)</author>
      <guid isPermaLink="false">2505.08157v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
      <link>http://arxiv.org/abs/2505.08086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习（TL）的多模态AI模型，用于伤口分类，旨在提高伤口诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;有效的诊断急性难以愈合的伤口对于提供有效的患者护理至关重要。不良的临床结果通常与感染、周围血管疾病和伤口深度增加有关。&lt;h4&gt;目的&lt;/h4&gt;通过结合两种最先进的架构Xception和GMRNN，开发一个多模态网络，以改善伤口类型（糖尿病、压力、手术和静脉溃疡）的分类。&lt;h4&gt;方法&lt;/h4&gt;该模型通过连接迁移学习算法提取的特征和位置特征来分类伤口类型，并与深度神经网络（DNN）进行综合比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法的伤口分类准确率从78.77%到100%不等，证明了其在准确分类最常见伤口类型方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了所提出的方法在利用伤口图像及其相应位置准确分类最常见伤口类型方面的优异表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The effective diagnosis of acute and hard-to-heal wounds is crucial for woundcare practitioners to provide effective patient care. Poor clinical outcomesare often linked to infection, peripheral vascular disease, and increasingwound depth, which collectively exacerbate these comorbidities. However,diagnostic tools based on Artificial Intelligence (AI) speed up theinterpretation of medical images and improve early detection of disease. Inthis article, we propose a multi-modal AI model based on transfer learning(TL), which combines two state-of-the-art architectures, Xception and GMRNN,for wound classification. The multi-modal network is developed by concatenatingthe features extracted by a transfer learning algorithm and location featuresto classify the wound types of diabetic, pressure, surgical, and venous ulcers.The proposed method is comprehensively compared with deep neural networks (DNN)for medical image analysis. The experimental results demonstrate a notablewound-class classifications (containing only diabetic, pressure, surgical, andvenous) vary from 78.77 to 100\% in various experiments. The results presentedin this study showcase the exceptional accuracy of the proposed methodology inaccurately classifying the most commonly occurring wound types using woundimages and their corresponding locations.</description>
      <author>example@mail.com (Ramin Mousa, Ehsan Matbooe, Hakimeh Khojasteh, Amirali Bengari, Mohammadmahdi Vahediahmar)</author>
      <guid isPermaLink="false">2505.08086v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
      <link>http://arxiv.org/abs/2505.08607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BooSTer的新框架，用于解决立体匹配方法中标签获取困难、数据稀缺以及合成图像与真实世界图像之间的领域差异问题。&lt;h4&gt;背景&lt;/h4&gt;立体匹配方法需要密集的像素级地面真实标签，这在实际数据集中非常耗时，而且合成图像与真实世界图像之间的数据稀少和领域差距也带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，以利用视觉基础模型和大规模混合图像源（包括合成、真实和单视图图像）来解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种数据生成策略，结合单目深度估计和扩散模型，从单视图图像中生成密集的立体匹配数据。2. 通过使用伪单目深度标签和动态尺度及平移不变损失，从单目深度估计模型中迁移知识，以解决现实世界数据集中的稀疏标签问题。3. 将视觉基础模型作为编码器，提取鲁棒且可迁移的特征，以提高准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，该方法在准确率方面取得了显著的提升，特别是在数据标签有限和领域迁移的场景中。&lt;h4&gt;结论&lt;/h4&gt;BooSTer框架在解决立体匹配中的挑战方面是有效的，尤其是在标签稀缺和领域迁移的情况下，可以显著提高准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereo matching methods rely on dense pixel-wise ground truth labels, whichare laborious to obtain, especially for real-world datasets. The scarcity oflabeled data and domain gaps between synthetic and real-world images also posenotable challenges. In this paper, we propose a novel framework,\textbf{BooSTer}, that leverages both vision foundation models and large-scalemixed image sources, including synthetic, real, and single-view images. First,to fully unleash the potential of large-scale single-view images, we design adata generation strategy combining monocular depth estimation and diffusionmodels to generate dense stereo matching data from single-view images. Second,to tackle sparse labels in real-world datasets, we transfer knowledge frommonocular depth estimation models, using pseudo-mono depth labels and a dynamicscale- and shift-invariant loss for additional supervision. Furthermore, weincorporate vision foundation model as an encoder to extract robust andtransferable features, boosting accuracy and generalization. Extensiveexperiments on benchmark datasets demonstrate the effectiveness of ourapproach, achieving significant improvements in accuracy over existing methods,particularly in scenarios with limited labeled data and domain shifts.</description>
      <author>example@mail.com (Yuran Wang, Yingping Liang, Ying Fu)</author>
      <guid isPermaLink="false">2505.08607v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.08199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多粒度信息的MLP预测框架，用于解决长期时间序列预测中的关键问题，并在八个基准数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测在能源消耗和天气预报等领域具有广泛的应用价值，但准确预测长期变化面临复杂的时间模式和内在的多尺度变化。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种高效的基于MLP的预测框架，解决长期时间序列预测中的多粒度信息利用不足、忽略通道特定属性以及趋势和季节成分的独特性质等问题。&lt;h4&gt;方法&lt;/h4&gt;该方法能够清晰并同步地预测不同尺度的复杂时间动态，并通过一个动态分配不同粒度信息重要性的系统将多尺度预测巧妙地整合。同时，使用双叉结构独立建模趋势和季节性元素。&lt;h4&gt;主要发现&lt;/h4&gt;在八个长期时间序列预测基准数据集上的实验结果表明，与最新的基于MLP的方法（TimeMixer）相比，MDMixer将平均MAE性能提高了4.64%，同时实现了训练效率和模型可解释性之间的有效平衡。&lt;h4&gt;结论&lt;/h4&gt;MDMixer是一种有效的长期时间序列预测方法，在保持训练效率的同时提高了预测准确性，并且具有良好的模型可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) offers broad utility in practicalsettings like energy consumption and weather prediction. Accurately predictinglong-term changes, however, is demanding due to the intricate temporal patternsand inherent multi-scale variations within time series. This work confronts keyissues in LTSF, including the suboptimal use of multi-granularity information,the neglect of channel-specific attributes, and the unique nature of trend andseasonal components, by introducing a proficient MLP-based forecastingframework. Our method adeptly disentangles complex temporal dynamics usingclear, concurrent predictions across various scales. These multi-scaleforecasts are then skillfully integrated through a system that dynamicallyassigns importance to information from different granularities, sensitive toindividual channel characteristics. To manage the specific features of temporalpatterns, a two-pronged structure is utilized to model trend and seasonalelements independently. Experimental results on eight LTSF benchmarksdemonstrate that MDMixer improves average MAE performance by 4.64% compared tothe recent state-of-the-art MLP-based method (TimeMixer), while achieving aneffective balance between training efficiency and model interpretability.</description>
      <author>example@mail.com (Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao)</author>
      <guid isPermaLink="false">2505.08199v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models</title>
      <link>http://arxiv.org/abs/2505.08590v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将RAG增强型LLM与病理学基础模型结合应用于甲状腺细胞学诊断，通过利用精确的知识库，提高诊断效率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;人工智能的进步正在通过整合大型语言模型（LLM）与检索增强生成（RAG）和特定领域的基座模型来改变病理学。&lt;h4&gt;目的&lt;/h4&gt;解决细胞学解释、标准化和诊断准确性的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用RAG动态检索相关案例研究、诊断标准和专家解释，同时利用病理学基础模型在高清病理图像上进行特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;这些AI驱动的方法提高了诊断一致性，减少了变异性，并支持病理学家区分良性甲状腺病变和恶性病变。结果显示，将RAG与特定领域的LLM结合显著提高了诊断效率和可解释性。&lt;h4&gt;结论&lt;/h4&gt;AI辅助的甲状腺细胞学诊断具有潜力，其中基础模型UNI在从甲状腺细胞学样本中预测手术病理诊断方面实现了AUC 0.73-0.93的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in artificial intelligence (AI) are transforming pathology byintegrat-ing large language models (LLMs) with retrieval-augmented generation(RAG) and domain-specific foundation models. This study explores theapplication of RAG-enhanced LLMs coupled with pathology foundation models forthyroid cytology diagnosis, addressing challenges in cytologicalinterpretation, standardization, and diagnostic accuracy. By leveraging acurated knowledge base, RAG facilitates dy-namic retrieval of relevant casestudies, diagnostic criteria, and expert interpreta-tion, improving thecontextual understanding of LLMs. Meanwhile, pathology foun-dation models,trained on high-resolution pathology images, refine feature extrac-tion andclassification capabilities. The fusion of these AI-driven approaches en-hancesdiagnostic consistency, reduces variability, and supports pathologists indis-tinguishing benign from malignant thyroid lesions. Our results demonstratethat integrating RAG with pathology-specific LLMs significantly improvesdiagnostic efficiency and interpretability, paving the way for AI-assistedthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 forcorrect prediction of surgi-cal pathology diagnosis from thyroid cytologysamples.</description>
      <author>example@mail.com (Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus)</author>
      <guid isPermaLink="false">2505.08590v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
      <link>http://arxiv.org/abs/2505.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了现有因果模型在CNN和ViT变体上的影响，并提出了TSCNet，一种两阶段因果建模方法，以通过多尺度因果干预发现细粒度因果关联，以减轻长尾分类中的偏差。&lt;h4&gt;背景&lt;/h4&gt;因果推理成为减轻长尾分类偏差的可行方法，但随着从CNN到ViT的高级骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。&lt;h4&gt;目的&lt;/h4&gt;研究现有因果模型对CNN和ViT变体的影响，并提出一种新的方法来解决尾类分类中的难题。&lt;h4&gt;方法&lt;/h4&gt;提出TSCNet，包含两个阶段：1. 层次因果表示学习阶段（HCRL），通过解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签；2. 反事实对数偏置校准阶段（CLBC），通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。&lt;h4&gt;主要发现&lt;/h4&gt;ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，导致在具有相似视觉外观的尾类分类中存在困难。&lt;h4&gt;结论&lt;/h4&gt;TSCNet可以消除数据不平衡引入的多个偏差，在长尾基准测试中优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因果推理已成为减轻长尾分类偏差的有前途的方法，通过处理由类别不平衡引入的偏差。然而，随着从卷积神经网络（CNN）到视觉Transformer（ViT）的先进骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。本文研究了现有因果模型对CNN和ViT变体的影响，强调ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，这导致了在具有相似视觉外观的尾类分类中的困难。为了解决这些问题，本文提出了TSCNet，一种两阶段因果建模方法，通过多尺度因果干预来发现细粒度因果关联。具体来说，在层次因果表示学习阶段（HCRL）中，它解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签，从而增强细粒度因果表示。在反事实对数偏置校准阶段（CLBC）中，它通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。在各个长尾基准上进行的广泛实验表明，所提出的TSCNet可以消除数据不平衡引入的多个偏差，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal inference has emerged as a promising approach to mitigate long-tailclassification by handling the biases introduced by class imbalance. However,along with the change of advanced backbone models from Convolutional NeuralNetworks (CNNs) to Visual Transformers (ViT), existing causal models may notachieve an expected performance gain. This paper investigates the influence ofexisting causal models on CNNs and ViT variants, highlighting that ViT's globalfeature representation makes it hard for causal methods to model associationsbetween fine-grained features and predictions, which leads to difficulties inclassifying tail classes with similar visual appearance. To address theseissues, this paper proposes TSCNet, a two-stage causal modeling method todiscover fine-grained causal associations through multi-scale causalinterventions. Specifically, in the hierarchical causal representation learningstage (HCRL), it decouples the background and objects, applying backdoorinterventions at both the patch and feature level to prevent model from usingclass-irrelevant areas to infer labels which enhances fine-grained causalrepresentation. In the counterfactual logits bias calibration stage (CLBC), itrefines the optimization of model's decision boundary by adaptive constructingcounterfactual balanced data distribution to remove the spurious associationsin the logits caused by data distribution. Extensive experiments conducted onvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminatemultiple biases introduced by data imbalance, which outperforms existingmethods.</description>
      <author>example@mail.com (Xiaoshuo Yan, Zhaochuan Li, Lei Meng, Zhuang Qi, Wei Wu, Zixuan Li, Xiangxu Meng)</author>
      <guid isPermaLink="false">2505.08173v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
      <link>http://arxiv.org/abs/2505.08561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为轨迹感知自适应标记采样（TATS）的新型预训练策略，用于视觉基础模型，并通过在四个数据集上的实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;MVM（掩码视频建模）是一种有效的视觉基础模型预训练策略，但选择合适的掩码策略是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的掩码策略，以优化视频中的掩码标记选择，并提高预训练模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入了TATS，该策略可以建模标记的运动动态，并集成到掩码自动编码器（MAE）框架中。同时，提出了一种统一的训练策略，使用近端策略优化（PPO）对MAE和TATS进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;TATS模型允许进行激进的掩码，同时在不影响动作识别等下游任务性能的情况下保持预训练的内存效率。&lt;h4&gt;结论&lt;/h4&gt;在四个数据集上的实验表明，与现有方法相比，TATS在有效性、迁移性、泛化性和效率方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：掩码视频建模（MVM）已成为视觉基础模型的高效预训练策略，其中模型使用可见标记的信息重建掩码时空标记。然而，这类方法的一个关键挑战在于选择合适的掩码策略。以往的研究探索了预定义的掩码技术，包括随机和基于管道的掩码，以及利用关键运动先验、光流和来自外部预训练模型的语义线索的方法。在本研究中，我们引入了一种新颖且通用的轨迹感知自适应标记采样（TATS），该策略可以建模标记的运动动态，并且可以无缝集成到掩码自动编码器（MAE）框架中以选择视频中的运动中心标记。此外，我们提出了一种统一的训练策略，通过近端策略优化（PPO）从零开始联合优化MAE和TATS。我们表明，我们的模型允许进行激进的掩码，同时在不损害动作识别等下游任务性能的情况下确保预训练保持内存效率。在四个基准数据集（包括Something-Something v2、Kinetics-400、UCF101和HMDB51）上对所提出方法的大量实验表明，与最先进的方法相比，我们的工作在有效性、迁移性、泛化性和效率方面具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked video modeling~(MVM) has emerged as a highly effective pre-trainingstrategy for visual foundation models, whereby the model reconstructs maskedspatiotemporal tokens using information from visible tokens. However, a keychallenge in such approaches lies in selecting an appropriate masking strategy.Previous studies have explored predefined masking techniques, including randomand tube-based masking, as well as approaches that leverage key motion priors,optical flow and semantic cues from externally pre-trained models. In thiswork, we introduce a novel and generalizable Trajectory-Aware Adaptive TokenSampler (TATS), which models the motion dynamics of tokens and can beseamlessly integrated into the masked autoencoder (MAE) framework to selectmotion-centric tokens in videos. Additionally, we propose a unified trainingstrategy that enables joint optimization of both MAE and TATS from scratchusing Proximal Policy Optimization (PPO). We show that our model allows foraggressive masking without compromising performance on the downstream task ofaction recognition while also ensuring that the pre-training remains memoryefficient. Extensive experiments of the proposed approach across fourbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,demonstrate the effectiveness, transferability, generalization, and efficiencyof our work compared to other state-of-the-art methods.</description>
      <author>example@mail.com (Ayush K. Rai, Kyle Min, Tarun Krishna, Feiyan Hu, Alan F. Smeaton, Noel E. O'Connor)</author>
      <guid isPermaLink="false">2505.08561v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic</title>
      <link>http://arxiv.org/abs/2505.08021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）在处理图结构数据中的应用，特别是它们如何处理不同大小的输入图和在图同构下的不变性。文章通过将有限模型理论的方法和工具应用于图表示学习领域，揭示了GNNs的逻辑表达能力。&lt;h4&gt;背景&lt;/h4&gt;GNNs解决了在图结构数据上应用深度学习时遇到的两个主要挑战：处理不同大小的输入图和确保图同构下的不变性。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs的表达能力，特别是有限GNN架构与一阶逻辑（FO）特定片段的关系。&lt;h4&gt;方法&lt;/h4&gt;应用一阶和模态逻辑的有限模型理论方法于图表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;有限GNN架构对应于一阶逻辑的特定片段，包括模态逻辑（ML）、分级模态逻辑（GML）、带有普遍模态的模态逻辑（ML(A)）、二变量片段（FO2）及其扩展计数量词（C2）。&lt;h4&gt;结论&lt;/h4&gt;这一研究提供了一个统一框架，以理解GNNs在FO中的逻辑表达能力。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we show that bounded GNN architectures correspond to specific fragments of first-order logic (FO), including modal logic (ML), graded modal logic (GML), modal logic with the universal modality (ML(A)), the two-variable fragment (FO2) and its extension with counting quantifiers (C2). To establish these results, we apply methods and tools from finite model theory of first-order and modal logics to the domain of graph representation learning. This provides a unifying framework for understanding the logical expressiveness of GNNs within FO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) address two key challenges in applying deeplearning to graph-structured data: they handle varying size input graphs andensure invariance under graph isomorphism. While GNNs have demonstrated broadapplicability, understanding their expressive power remains an importantquestion. In this paper, we show that bounded GNN architectures correspond tospecific fragments of first-order logic (FO), including modal logic (ML),graded modal logic (GML), modal logic with the universal modality (ML(A)), thetwo-variable fragment (FO2) and its extension with counting quantifiers (C2).To establish these results, we apply methods and tools from finite model theoryof first-order and modal logics to the domain of graph representation learning.This provides a unifying framework for understanding the logical expressivenessof GNNs within FO.</description>
      <author>example@mail.com (Bernardo Cuenca Grau, Przemysław A. Wałęga)</author>
      <guid isPermaLink="false">2505.08021v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</title>
      <link>http://arxiv.org/abs/2505.08550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;当前预测模型通常采用时间域的时序预测（TF）范式，直接在时间域中编码和解码时间序列。然而，时间序列数据中的交错步骤依赖关系可能会阻碍TF的性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决TF的性能问题，本文旨在提出一种更有效的编码和解码方法，并增强多变量时间序列的表示学习。&lt;h4&gt;方法&lt;/h4&gt;本文采用基于正交矩阵的数据自适应变换OrthoTrans，对时间序列的时序皮尔逊相关矩阵进行对角化，从而在去相关的特征域中进行更有效的编码和解码。此外，引入了定制的线性层NormLin，使用归一化权重矩阵来捕捉多变量依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，NormLin模块在性能上优于多头自注意力机制，同时所需的浮点运算数（FLOPs）大约是其一半。在24个基准和140个预测任务上的广泛实验表明，OLinear模型在效率上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;作为自注意力机制的插件替代品，NormLin模块能够持续增强基于Transformer的预测器。代码和数据集可通过指定链接获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jackyue1994/OLinear&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-basedmultivariate time series forecasting model that operates in an$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typicallyadopt the temporal forecast (TF) paradigm, which directly encode and decodetime series in the time domain. However, the entangled step-wise dependenciesin series data can hinder the performance of TF. To address this, someforecasters conduct encoding and decoding in the transformed domain usingfixed, dataset-independent bases (e.g., sine and cosine signals in the Fouriertransform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptivetransformation based on an orthogonal matrix that diagonalizes the series'temporal Pearson correlation matrix. This approach enables more effectiveencoding and decoding in the decorrelated feature domain and can serve as aplug-in module to enhance existing forecasters. To enhance the representationlearning for multivariate time series, we introduce a customized linear layer,$\mathbf{NormLin}$, which employs a normalized weight matrix to capturemultivariate dependencies. Empirically, the NormLin module shows a surprisingperformance advantage over multi-head self-attention, while requiring nearlyhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecastingtasks demonstrate that OLinear consistently achieves state-of-the-artperformance with high efficiency. Notably, as a plug-in replacement forself-attention, the NormLin module consistently enhances Transformer-basedforecasters. The code and datasets are available athttps://anonymous.4open.science/r/OLinear</description>
      <author>example@mail.com (Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi)</author>
      <guid isPermaLink="false">2505.08550v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant graph neural network surrogates for predicting the properties of relaxed atomic configurations</title>
      <link>http://arxiv.org/abs/2505.08121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文使用密度泛函理论（DFT）计算确定结构的最小形成能量，并提出了等变图神经网络（EGNN）模型来预测对特定结构的DFT计算结果。&lt;h4&gt;背景&lt;/h4&gt;传统的簇展开方法在处理固定晶格以外的结构，如间隙原子、非晶材料和多结构材料时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出EGNN模型，以更灵活地处理结构变化，同时尊重系统的对称性。&lt;h4&gt;方法&lt;/h4&gt;在锂钴氧化物（LCO）的多种锂含量和锂原子排列组合下，使用EGNN模型进行数学框架构建和训练。&lt;h4&gt;主要发现&lt;/h4&gt;EGNN模型能够准确预测训练集之外的量，包括最大原子位移、应变张量和能量，以及形成能量。&lt;h4&gt;结论&lt;/h4&gt;EGNN模型提供了对研究系统的深入理解，无需进行更多的DFT计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Density functional theory (DFT) calculations determine the relaxed atomicpositions and lattice parameters that minimize the formation energy of astructure. We present an equivariant graph neural network (EGNN) model topredict the outcome of DFT calculations for structures of interest. Clusterexpansions are a well established approach for representing the formationenergies. However, traditional cluster expansions are limited in their abilityto handle variations from a fixed lattice, including interstitial atoms,amorphous materials, and materials with multiple structures. EGNNs offer a moreflexible framework that inherently respects the symmetry of the system withoutbeing reliant on a particular lattice. In this work, we present themathematical framework and the results of training for lithium cobalt oxide(LCO) at various compositions of lithium and arrangements of the lithium atoms.Our results demonstrate that the EGNN can accurately predict quantities outsidethe training set including the largest atomic displacements, the strain tensorand energy, and the formation energy providing greater insight into the systembeing studied without the need for more DFT calculations.</description>
      <author>example@mail.com (Jamie Holber, Krishna Garikipati)</author>
      <guid isPermaLink="false">2505.08121v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks</title>
      <link>http://arxiv.org/abs/2505.07895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HGNN-IMA的新型模型，用于多模态异构网络中的节点分类，通过捕捉信息传播过程中的多模态相互影响，实现自适应多模态融合。&lt;h4&gt;背景&lt;/h4&gt;当前在线平台如豆瓣电影网络和亚马逊产品评论网络等可以描述为多模态异构网络（MMHNs），在这些网络中准确分类节点对于分析对应实体至关重要，需要有效的节点表示学习方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的节点分类模型，以解决现有多模态融合方法在早期融合和晚期融合中的不足。&lt;h4&gt;方法&lt;/h4&gt;模型名为HGNN-IMA，采用异构图Transformer框架，集成了嵌套的跨模态注意力机制，并考虑了模态对齐以促进具有跨模态一致相似性的节点间的传播，同时增加了注意力损失以减轻缺失模态的影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了创新视角，尤其是在伴随网络结构的情况下。&lt;h4&gt;结论&lt;/h4&gt;HGNN-IMA模型能够有效地处理多模态数据，特别是在节点分类任务中具有显著优势，为多模态网络分析提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：如今，众多在线平台可以描述为多模态异构网络（MMHNs），如豆瓣的电影网络和亚马逊的产品评论网络。在这些网络中对节点进行准确分类对于分析相应的实体至关重要，这需要节点上的有效表示学习方法。然而，现有的多模态融合方法通常采用早期融合策略，这可能会导致丢失各个模态的独特特征，或者采用晚期融合方法，忽略了基于GNN的信息传播中的跨模态指导。在本文中，我们提出了一种名为异构图神经网络与跨模态注意力（HGNN-IMA）的新型模型，用于MMHNs中的节点分类。它通过在异构图Transformer框架内捕捉信息传播过程中的多模态相互影响来学习节点表示。具体来说，将嵌套的跨模态注意力机制整合到节点间注意力中，以实现自适应多模态融合，并考虑模态对齐以促进具有所有模态一致相似性的节点间的传播。此外，还增加了注意力损失以减轻缺失模态的影响。大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了一个创新的方法，尤其是在伴随网络结构的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nowadays, numerous online platforms can be described as multi-modalheterogeneous networks (MMHNs), such as Douban's movie networks and Amazon'sproduct review networks. Accurately categorizing nodes within these networks iscrucial for analyzing the corresponding entities, which requires effectiverepresentation learning on nodes. However, existing multi-modal fusion methodsoften adopt either early fusion strategies which may lose the uniquecharacteristics of individual modalities, or late fusion approaches overlookingthe cross-modal guidance in GNN-based information propagation. In this paper,we propose a novel model for node classification in MMHNs, named HeterogeneousGraph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns noderepresentations by capturing the mutual influence of multiple modalities duringthe information propagation process, within the framework of heterogeneousgraph transformer. Specifically, a nested inter-modal attention mechanism isintegrated into the inter-node attention to achieve adaptive multi-modalfusion, and modality alignment is also taken into account to encourage thepropagation among nodes with consistent similarities across all modalities.Moreover, an attention loss is augmented to mitigate the impact of missingmodalities. Extensive experiments validate the superiority of the model in thenode classification task, providing an innovative view to handle multi-modaldata, especially when accompanied with network structures.</description>
      <author>example@mail.com (Jiafan Li, Jiaqi Zhu, Liang Chang, Yilin Li, Miaomiao Li, Yang Wang, Hongan Wang)</author>
      <guid isPermaLink="false">2505.07895v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>ExEBench: Benchmarking Foundation Models on Extreme Earth Events</title>
      <link>http://arxiv.org/abs/2505.08529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ExE-Bench，一个针对极端事件的基准数据集，旨在评估机器学习模型在极端事件管理中的可靠性。&lt;h4&gt;背景&lt;/h4&gt;地球正面临越来越频繁的极端事件，这些事件对人类生活和生态系统构成重大风险。机器学习在提取特征和灾难管理方面展现出潜力，但模型可能存在训练数据中的偏差。&lt;h4&gt;目的&lt;/h4&gt;ExEBench旨在（1）评估机器学习模型在多样化、高影响任务和领域中的泛化能力，（2）促进有助于灾难管理的创新机器学习方法的发展，（3）提供一个分析极端事件相互作用和级联效应的平台，以加深我们对地球系统，尤其是未来几十年气候变化预期下的理解。&lt;h4&gt;方法&lt;/h4&gt;ExEBench包含七个极端事件类别，包括洪水、野火、风暴、热带气旋、极端降水、热浪和冷浪，具有全球覆盖、不同数据量和多样化的数据来源。&lt;h4&gt;主要发现&lt;/h4&gt;ExEBench旨在解决机器学习模型在极端事件管理中的挑战，并提供一个评估模型性能的基准。&lt;h4&gt;结论&lt;/h4&gt;ExEBench是一个公共数据集和代码库，旨在推动机器学习在极端事件管理中的应用和发展。&lt;h4&gt;翻译&lt;/h4&gt;Our planet is facing increasingly frequent extreme events, which pose major risks to human lives and ecosystems. Recent advances in machine learning (ML), especially with foundation models (FMs) trained on extensive datasets, excel in extracting features and show promise in disaster management. Nevertheless, these models often inherit biases from training data, challenging their performance over extreme values. To explore the reliability of FM in the context of extreme events, we introduce extbf{ExE}Bench (extbf{Ex}tremeextbf{E}arth Benchmark), a collection of seven extreme event categories across floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, and spectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needs in extreme events detection, monitoring, and forecasting. ExEBench aims to (1) assess FM generalizability across diverse, high-impact tasks and domains, (2) promote the development of novel ML methods that benefit disaster management, and (3) offer a platform for analyzing the interactions and cascading effects of extreme events to advance our understanding of Earth system, especially under the climate change expected in the decades to come. The dataset and code are public https://github.com/zhaoshan2/EarthExtreme-Bench.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our planet is facing increasingly frequent extreme events, which pose majorrisks to human lives and ecosystems. Recent advances in machine learning (ML),especially with foundation models (FMs) trained on extensive datasets, excel inextracting features and show promise in disaster management. Nevertheless,these models often inherit biases from training data, challenging theirperformance over extreme values. To explore the reliability of FM in thecontext of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme\textbf{E}arth Benchmark), a collection of seven extreme event categoriesacross floods, wildfires, storms, tropical cyclones, extreme precipitation,heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, andspectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needsin extreme events detection, monitoring, and forecasting. ExEBench aims to (1)assess FM generalizability across diverse, high-impact tasks and domains, (2)promote the development of novel ML methods that benefit disaster management,and (3) offer a platform for analyzing the interactions and cascading effectsof extreme events to advance our understanding of Earth system, especiallyunder the climate change expected in the decades to come. The dataset and codeare public https://github.com/zhaoshan2/EarthExtreme-Bench.</description>
      <author>example@mail.com (Shan Zhao, Zhitong Xiong, Jie Zhao, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2505.08529v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Symbolically-Guided Visual Plan Inference from Uncurated Video Data</title>
      <link>http://arxiv.org/abs/2505.08444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vis2Plan是一个高效的、可解释的、基于符号指导的视觉规划框架，它在长周期操作任务中实现了良好的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉规划方法通常依赖于视频生成模型来获取子目标，但存在模型幻觉和计算成本高的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够自动提取任务符号并构建高级符号转换图，以实现多目标、多阶段规划的框架。&lt;h4&gt;方法&lt;/h4&gt;Vis2Plan从原始的无标签游戏数据中提取一组紧凑的任务符号，并在符号级别进行规划，生成一系列基于符号表示的物理一致的中间子目标图像。&lt;h4&gt;主要发现&lt;/h4&gt;Vis2Plan在真实机器人环境中比基于扩散视频生成模型的视觉规划器有53%更高的成功率，并且生成视觉计划的效率提高了35倍。&lt;h4&gt;结论&lt;/h4&gt;Vis2Plan能够生成物理一致的图像目标，并提供了完全可检查的推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual planning, by offering a sequence of intermediate visual subgoals to agoal-conditioned low-level policy, achieves promising performance onlong-horizon manipulation tasks. To obtain the subgoals, existing methodstypically resort to video generation models but suffer from model hallucinationand computational cost. We present Vis2Plan, an efficient, explainable andwhite-box visual planning framework powered by symbolic guidance. From raw,unlabeled play data, Vis2Plan harnesses vision foundation models toautomatically extract a compact set of task symbols, which allows building ahigh-level symbolic transition graph for multi-goal, multi-stage planning. Attest time, given a desired task goal, our planner conducts planning at thesymbolic level and assembles a sequence of physically consistent intermediatesub-goal images grounded by the underlying symbolic representation. OurVis2Plan outperforms strong diffusion video generation-based visual planners bydelivering 53\% higher aggregate success rate in real robot settings whilegenerating visual plans 35$\times$ faster. The results indicate that Vis2Planis able to generate physically consistent image goals while offering fullyinspectable reasoning steps.</description>
      <author>example@mail.com (Wenyan Yang, Ahmet Tikna, Yi Zhao, Yuying Zhang, Luigi Palopoli, Marco Roveri, Joni Pajarinen)</author>
      <guid isPermaLink="false">2505.08444v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
      <link>http://arxiv.org/abs/2505.08414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Meta-EyeFM是一个多功能的底层模型，集成了大语言模型和视觉基础模型，用于眼科疾病的评估。该模型通过路由机制实现基于文本查询的准确任务分析，并通过低秩适应对视觉基础模型进行微调，以提高疾病的检测、严重程度区分和常见眼部标志识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。&lt;h4&gt;目的&lt;/h4&gt;提出Meta-EyeFM模型，旨在为眼科疾病的评估提供一种高效、准确的方法。&lt;h4&gt;方法&lt;/h4&gt;Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。通过低秩适应对视觉基础模型进行微调，以检测眼部和全身疾病，区分眼部疾病的严重程度，并识别常见眼部标志。&lt;h4&gt;主要发现&lt;/h4&gt;Meta-EyeFM在将眼底图像路由到适当的视觉基础模型方面达到了100%的准确率，在疾病检测、严重程度区分和标志识别方面的准确率分别达到了≥82.2%、≥89%和≥76%。该模型在检测各种眼病方面的准确率比Gemini-1.5-flash和ChatGPT-4oLMMs高11%到43%，并且与眼科医生的水平相当。&lt;h4&gt;结论&lt;/h4&gt;Meta-EyeFM系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线大语言模型眼底评估的有价值决策支持工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。我们提出了Meta-EyeFM，一个集成了大型语言模型（LLM）和视觉基础模型（VFMs）的多功能基础模型，用于眼科疾病的评估。Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。使用低秩适应，我们微调了我们的VFMs以检测眼部和全身疾病，区分眼部疾病严重程度，并识别常见眼部标志。该模型在将眼底图像路由到适当的VFMs方面达到了100%的准确率，VFMs在疾病检测方面的准确率达到了≥82.2%，在严重程度区分方面达到了≥89%，在标志识别方面达到了≥76%。与Gemini-1.5-flash和ChatGPT-4oLMMs相比，Meta-EyeFM在检测各种眼病方面的准确率提高了11%到43%，并且与眼科医生的水平相当。该系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线LLM眼底评估的有价值决策支持工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current deep learning models are mostly task specific and lack auser-friendly interface to operate. We present Meta-EyeFM, a multi-functionfoundation model that integrates a large language model (LLM) with visionfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages arouting mechanism to enable accurate task-specific analysis based on textqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular andsystemic diseases, differentiate ocular disease severity, and identify commonocular signs. The model achieved 100% accuracy in routing fundus images toappropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4oLMMs in detecting various eye diseases and comparable to an ophthalmologist.This system offers enhanced usability and diagnostic performance, making it avaluable decision support tool for primary eye care or an online LLM for fundusevaluation.</description>
      <author>example@mail.com (Zhi Da Soh, Yang Bai, Kai Yu, Yang Zhou, Xiaofeng Lei, Sahil Thakur, Zann Lee, Lee Ching Linette Phang, Qingsheng Peng, Can Can Xue, Rachel Shujuan Chong, Quan V. Hoang, Lavanya Raghavan, Yih Chung Tham, Charumathi Sabanayagam, Wei-Chi Wu, Ming-Chih Ho, Jiangnan He, Preeti Gupta, Ecosse Lamoureux, Seang Mei Saw, Vinay Nangia, Songhomitra Panda-Jonas, Jie Xu, Ya Xing Wang, Xinxing Xu, Jost B. Jonas, Tien Yin Wong, Rick Siow Mong Goh, Yong Liu, Ching-Yu Cheng)</author>
      <guid isPermaLink="false">2505.08414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>BLAB: Brutally Long Audio Bench</title>
      <link>http://arxiv.org/abs/2505.03054v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为BLAB的音频基准测试，用于评估音频语言模型在长音频内容上的理解能力。&lt;h4&gt;背景&lt;/h4&gt;理解多样化的语音交互对于语言技术的发展至关重要，而当前研究主要关注短音频片段，缺乏对长对话音频片段的探索。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理长音频内容的音频语言模型，提高语言技术在不同用户群体中的可访问性。&lt;h4&gt;方法&lt;/h4&gt;BLAB包含超过833小时的多样化全长度音频剪辑，平均长度为51分钟，并配以基于文本的标注问题及答案。对六个开源和专有音频语言模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在BLAB上的表现都存在困难，尤其是在定位、时长估计、情感和计数任务上。长音频理解能力存在挑战，模型性能随音频时长增加而下降，依赖提示而非音频内容。&lt;h4&gt;结论&lt;/h4&gt;BLAB为评估和开发具有强大长音频理解能力的音频语言模型提供了一个挑战性的框架。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing large audio language models (LMs) capable of understanding diversespoken interactions is essential for accommodating the multimodal nature ofhuman communication and can increase the accessibility of language technologiesacross different user populations. Recent work on audio LMs has primarilyevaluated their performance on short audio segments, typically under 30seconds, with limited exploration of long-form conversational speech segmentsthat more closely reflect natural user interactions with these models. Weintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audiobenchmark that evaluates audio LMs on localization, duration estimation,emotion, and counting tasks using audio segments averaging 51 minutes inlength. BLAB consists of 833+ hours of diverse, full-length audio clips, eachpaired with human-annotated, text-based natural language questions and answers.Our audio data were collected from permissively licensed sources and underwenta human-assisted filtering process to ensure task compliance. We evaluate sixopen-source and proprietary audio LMs on BLAB and find that all of them,including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with thetasks in BLAB. Our comprehensive analysis reveals key insights into thetrade-offs between task difficulty and audio duration. In general, we find thataudio LMs struggle with long-form speech, with performance declining asduration increases. They perform poorly on localization, temporal reasoning,counting, and struggle to understand non-phonemic information, relying more onprompts than audio content. BLAB serves as a challenging evaluation frameworkto develop audio LMs with robust long-form audio understanding capabilities.</description>
      <author>example@mail.com (Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar)</author>
      <guid isPermaLink="false">2505.03054v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。文章通过理论分析，提出了一个基于分布鲁棒优化的理论框架DRRho风险最小化，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;模型引导在训练大型基础模型等场景中已有所应用，但其内在原理理解不足，导致性能欠佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论框架以指导模型引导，提高模型训练的泛化能力和数据效率。&lt;h4&gt;方法&lt;/h4&gt;提出DRRho风险最小化理论框架，通过分布鲁棒优化（DRO）来分析模型引导的原理，并引入DRRho-CLIP方法进行对比语言-图像预训练。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了模型引导方法比无参考模型训练在泛化能力和数据效率方面的优势，并通过实验验证了这一发现。&lt;h4&gt;结论&lt;/h4&gt;模型引导方法在提高模型训练效果方面具有潜力，且理论框架DRRho风险最小化为其提供了理论支持。&lt;h4&gt;翻译&lt;/h4&gt;本文正式化了一种新兴的学习范式，即使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。尽管模型引导在诸如大型基础模型训练的各种场景中已有所应用，但其背后的原理仍然没有得到充分的理解，这导致了性能的不优化。在这项工作中，我们提出了一种基于分布鲁棒优化（DRO）的理论驱动框架，称为DRRho风险最小化。通过泛化分析，我们提供了理论见解，说明了为什么这种方法比没有参考模型训练提高了泛化能力和数据效率。据我们所知，这是首次为这种新的学习范式提供这样的理论见解，这大大增强了我们对于模型引导的理解和实践。基于这些见解以及对比学习和DRO之间的联系，我们引入了一种名为DRRho-CLIP的对比语言-图像预训练的新方法。广泛的实验验证了理论见解，揭示了与没有参考模型的CLIP相比的优越扩展规律，并展示了其相对于现有启发式方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DSADF: Thinking Fast and Slow for Decision Making</title>
      <link>http://arxiv.org/abs/2505.08189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DSADF的双系统自适应决策框架，通过结合快速直觉决策和深度分析推理，提高强化学习代理在动态环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理在静态环境中表现良好，但在动态环境中泛化能力有限，因为它们依赖于试错交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决强化学习代理在动态环境中泛化能力不足的问题，提出了一种新的决策框架。&lt;h4&gt;方法&lt;/h4&gt;DSADF框架包含两个互补模块：System 1（由强化学习代理和记忆空间组成，用于快速直觉决策）和System 2（由视觉语言模型驱动，用于深度分析推理）。&lt;h4&gt;主要发现&lt;/h4&gt;DSADF在视频游戏环境Crafter和Housekeep中的实证研究表明，该方法在未知和已知任务中均显著提高了决策能力。&lt;h4&gt;结论&lt;/h4&gt;DSADF通过结合直觉和深度推理，实现了高效和自适应的决策，为强化学习代理在动态环境中的泛化提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管强化学习代理在定义良好的环境中效果显著，但它们往往难以将学到的策略泛化到动态环境中，这是因为它们依赖于试错交互。最近的研究探索了通过策略优化指导或先验知识应用大型语言模型（LLMs）或视觉语言模型（VLMs）来提高强化学习代理的泛化能力。然而，这些方法通常缺乏强化学习代理和基础模型之间的无缝协调，导致在不熟悉的环境中的不合理决策和效率瓶颈。充分利用基础模型的推理能力、强化学习代理的快速响应能力，并增强两者之间的交互以形成一个双系统，仍然是一个悬而未决的科学问题。为了解决这个问题，我们借鉴了Kahneman的快速思考（系统1）和慢速思考（系统2）的理论，证明了在复杂世界中平衡直觉和深度推理可以实现敏捷的决策。在本研究中，我们提出了一种双系统自适应决策框架（DSADF），它集成了两个互补的模块：系统1，包括一个强化学习代理和一个用于快速直觉决策的记忆空间；系统2，由一个视觉语言模型驱动，用于深度分析推理。DSADF通过结合两个系统的优势，促进了高效和自适应的决策。在视频游戏环境Crafter和Housekeep中的实证研究证明了我们提出的方法的有效性，显示出在未知和已知任务中决策能力的显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Reinforcement Learning (RL) agents are effective in well-definedenvironments, they often struggle to generalize their learned policies todynamic settings due to their reliance on trial-and-error interactions. Recentwork has explored applying Large Language Models (LLMs) or Vision LanguageModels (VLMs) to boost the generalization of RL agents through policyoptimization guidance or prior knowledge. However, these approaches often lackseamless coordination between the RL agent and the foundation model, leading tounreasonable decision-making in unfamiliar environments and efficiencybottlenecks. Making full use of the inferential capabilities of foundationmodels and the rapid response capabilities of RL agents and enhancing theinteraction between the two to form a dual system is still a lingeringscientific question. To address this problem, we draw inspiration fromKahneman's theory of fast thinking (System 1) and slow thinking (System 2),demonstrating that balancing intuition and deep reasoning can achieve nimbledecision-making in a complex world. In this study, we propose a Dual-SystemAdaptive Decision Framework (DSADF), integrating two complementary modules:System 1, comprising an RL agent and a memory space for fast and intuitivedecision making, and System 2, driven by a VLM for deep and analyticalreasoning. DSADF facilitates efficient and adaptive decision-making bycombining the strengths of both systems. The empirical study in the video gameenvironment: Crafter and Housekeep demonstrates the effectiveness of ourproposed method, showing significant improvements in decision abilities forboth unseen and known tasks.</description>
      <author>example@mail.com (Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang)</author>
      <guid isPermaLink="false">2505.08189v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast</title>
      <link>http://arxiv.org/abs/2505.08151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了锂离子电池容量退化准确估计的重要性，提出了一种针对时间序列基础模型的自适应微调策略，以增强电池退化预测的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统专家模型适用于特定场景，而数据驱动技术的发展为电池容量退化预测提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于电池退化预测的时间序列基础模型的微调策略，实现零样本泛化。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调策略对Timer模型进行微调，并在约10GB的公开电池充放电数据上进行应用；提出知识蒸馏框架，将预训练基础模型的知识转移到紧凑的专家模型中。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的Battery-Timer在容量退化预测方面具有强大的零样本泛化能力；知识蒸馏框架显著提高了专家模型的多条件泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提升电池容量退化预测的准确性，为电池可靠性和安全性提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sjtu-chan-joey/battery-timer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate estimation of lithium-ion battery capacity degradation is criticalfor enhancing the reliability and safety of battery operations. Traditionalexpert models, tailored to specific scenarios, provide isolated estimations.With the rapid advancement of data-driven techniques, a series ofgeneral-purpose time-series foundation models have been developed. However,foundation models specifically designed for battery capacity degradation remainlargely unexplored. To enable zero-shot generalization in battery degradationprediction using large model technology, this study proposes adegradation-aware fine-tuning strategy for time-series foundation models. Weapply this strategy to fine-tune the Timer model on approximately 10 GB ofopen-source battery charge discharge data. Validation on our releasedCycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timerpossesses strong zero-shot generalization capability in capacity degradationforecasting. To address the computational challenges of deploying large models,we further propose a knowledge distillation framework that transfers theknowledge of pre-trained foundation models into compact expert models.Distillation results across several state-of-the-art time-series expert modelsconfirm that foundation model knowledge significantly improves themulti-condition generalization of expert models.</description>
      <author>example@mail.com (Joey Chan, Zhen Chen, Ershun Pan)</author>
      <guid isPermaLink="false">2505.08151v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Games for graded modal substitution calculus</title>
      <link>http://arxiv.org/abs/2505.07966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了针对分级模态替换演算（GMSC）及其变体的两种语义游戏和公式大小游戏，用于研究计算模型的表达能力。&lt;h4&gt;背景&lt;/h4&gt;GMSC及其变体被用于逻辑描述各种计算框架，如图神经网络、普通神经网络和分布式计算。&lt;h4&gt;目的&lt;/h4&gt;引入语义游戏和公式大小游戏，以研究GMSC的等价类以及这些类在给定的GMSC程序大小下的等价性。&lt;h4&gt;方法&lt;/h4&gt;通过引入新的语义游戏和公式大小游戏，展示了这些游戏如何描述GMSC程序等价类之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;公式大小游戏可以用来研究被描述的计算机模型的表达能力；GMSC在词上具有与确定性线性带限制图灵机（确定性线性界限自动机）相同的表达能力。&lt;h4&gt;结论&lt;/h4&gt;公式大小游戏是研究GMSC及其变体表达能力的一个有效工具，同时GMSC的强大表达能力与确定性线性带限制图灵机相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graded modal substitution calculus (GMSC) and its variants has been used forlogical characterizations of various computing frameworks such as graph neuralnetworks, ordinary neural networks and distributed computing. In this paper weintroduce two different semantic games and formula size game for graded modalsubstitution calculus and its variants. Ultimately, we show that the formulasize game characterizes the equivalence of classes of pointed Kripke models upto programs of GMSC of given size. Thus, the formula size game can be used tostudy the expressive power mentioned characterized classes of computing models.Moreover, we show that over words GMSC has the same expressive power asdeterministic linearly tape-bounded Turing machines also known as deterministiclinear bounded automata.</description>
      <author>example@mail.com (Veeti Ahvonen, Reijo Jaakkola, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07966v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem</title>
      <link>http://arxiv.org/abs/2505.08148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了14,904个定制GPT模型，评估了它们对七种可利用威胁的易受攻击性，并引入了一个多指标排名系统来考察定制GPT的流行度与其关联的安全风险之间的关系。&lt;h4&gt;背景&lt;/h4&gt;许多用户使用基于GPT的语言模型执行各种任务，定制GPT模型因其特殊需求而越来越受欢迎，但同时也引发了安全漏洞的担忧。&lt;h4&gt;目的&lt;/h4&gt;研究定制GPT模型的安全风险，并分析其与流行度的关系。&lt;h4&gt;方法&lt;/h4&gt;分析了14,904个定制GPT模型，评估了其对七种威胁的易受攻击性，并引入了多指标排名系统。&lt;h4&gt;主要发现&lt;/h4&gt;超过95%的定制GPT缺乏适当的安全保护，最常见的漏洞包括角色扮演攻击（96.51%）、系统提示泄露（92.20%）和钓鱼（91.22%）。此外，OpenAI的基础模型存在固有的安全弱点。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了增强安全措施和严格内容审查的紧迫需求，以确保GPT应用的安全部署。&lt;h4&gt;翻译&lt;/h4&gt;Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks. In this study, we analyze 14,904 custom GPTs to assess their susceptibility to seven exploitable threats, such as roleplay-based attacks, system prompt leakage, phishing content generation, and malicious code synthesis, across various categories and popularity tiers within the OpenAI marketplace. We introduce a multi-metric ranking system to examine the relationship between a custom GPT's popularity and its associated security risks. Our findings reveal that over 95% of custom GPTs lack adequate security protections. The most prevalent vulnerabilities include roleplay-based vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing (91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit inherent security weaknesses, which are often inherited or amplified in custom GPTs. These results highlight the urgent need for enhanced security measures and stricter content moderation to ensure the safe deployment of GPT-based applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/customgptvulnerability/custom-gpt-vulnerability-assessment&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of users leverage generative pretrained transformer (GPT)-basedlanguage models developed by leading model providers for a wide range of tasks.To support enhanced user interaction and customization, many platforms-such asOpenAI-now enable developers to create and publish tailored model instances,known as custom GPTs, via dedicated repositories or application stores. Thesecustom GPTs empower users to browse and interact with specialized applicationsdesigned to meet specific needs. However, as custom GPTs see growing adoption,concerns regarding their security vulnerabilities have intensified. Existingresearch on these vulnerabilities remains largely theoretical, often lackingempirical, large-scale, and statistically rigorous assessments of associatedrisks.  In this study, we analyze 14,904 custom GPTs to assess their susceptibilityto seven exploitable threats, such as roleplay-based attacks, system promptleakage, phishing content generation, and malicious code synthesis, acrossvarious categories and popularity tiers within the OpenAI marketplace. Weintroduce a multi-metric ranking system to examine the relationship between acustom GPT's popularity and its associated security risks.  Our findings reveal that over 95% of custom GPTs lack adequate securityprotections. The most prevalent vulnerabilities include roleplay-basedvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibitinherent security weaknesses, which are often inherited or amplified in customGPTs. These results highlight the urgent need for enhanced security measuresand stricter content moderation to ensure the safe deployment of GPT-basedapplications.</description>
      <author>example@mail.com (Sunday Oyinlola Ogundoyin, Muhammad Ikram, Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Dali Kaafar)</author>
      <guid isPermaLink="false">2505.08148v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines</title>
      <link>http://arxiv.org/abs/2505.07857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 10 figures(including 6 graphs)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对乌尔都语特定意图检测的独特对比学习方法，该方法利用未标记的乌尔都语数据重新训练预训练的语言模型，以提高乌尔都语意图检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;尽管多种语言都开发了意图检测预测器，但乌尔都语这一第十大语言在该领域仍处于发展阶段。&lt;h4&gt;目的&lt;/h4&gt;提高乌尔都语意图检测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于预训练语言模型的对比学习方法，并探索了6种不同的语言模型和13种不同的相似度计算方法，构建了一个综合的端到端LLMPIA意图检测流程。&lt;h4&gt;主要发现&lt;/h4&gt;在ATIS和Web Queries数据集上，LLMPIA在4-way 1 shot和4-way 5 shot实验设置下分别达到了83.28%和98.25%的F1-Score，在Web Queries数据集上达到了76.23%和84.42%的F1-Score，并且在一个额外的案例研究中，LLMPIA比最先进的预测器高出53.55%的F1-Score。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有效提高了乌尔都语意图检测的性能，为乌尔都语在意图检测领域的进一步发展奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multifarious intent detection predictors are developed for differentlanguages, including English, Chinese and French, however, the field remainsunderdeveloped for Urdu, the 10th most spoken language. In the realm ofwell-known languages, intent detection predictors utilize the strategy offew-shot learning and prediction of unseen classes based on the model trainingon seen classes. However, Urdu language lacks few-shot strategy based intentdetection predictors and traditional predictors are focused on prediction ofthe same classes which models have seen in the train set. To empower Urdulanguage specific intent detection, this introduces a unique contrastivelearning approach that leverages unlabeled Urdu data to re-train pre-trainedlanguage models. This re-training empowers LLMs representation learning for thedownstream intent detection task. Finally, it reaps the combined potential ofpre-trained LLMs and the prototype-informed attention mechanism to create acomprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigmof proposed predictive pipeline, it explores the potential of 6 distinctlanguage models and 13 distinct similarity computation methods. The proposedframework is evaluated on 2 public benchmark datasets, namely ATIS encompassing5836 samples and Web Queries having 8519 samples. Across ATIS dataset under4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,respectively. In an additional case study on the Web Queries dataset under sameclasses train and test set settings, LLMPIA outperformed state-of-the-artpredictor by 53.55% F1-Score.</description>
      <author>example@mail.com (Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel)</author>
      <guid isPermaLink="false">2505.07857v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey</title>
      <link>http://arxiv.org/abs/2505.08034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE COMPSAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要探讨了生成式人工智能（GenAI）在智慧城市中的应用，特别是基于对话界面的GenAI在市民、运营者和规划者等关键用户类型中的应用。&lt;h4&gt;背景&lt;/h4&gt;随着物联网（IoT）和数字孪生技术的普及，为智慧城市提供了丰富的数据基础，旨在改善城市生活和运营。&lt;h4&gt;目的&lt;/h4&gt;研究GenAI在智慧城市中的具体应用，并探讨如何利用GenAI模型和技术在智慧城市的不同子系统内为不同用户类型提供定制化服务和统一界面。&lt;h4&gt;方法&lt;/h4&gt;本文对已提出的GenAI模型和技术进行了识别和回顾，并考虑了如何基于现有的城市记录、IoT数据流和城市数字孪生来构建GenAI。&lt;h4&gt;主要发现&lt;/h4&gt;GenAI能够通过处理多模态内容生成新的输出，如文本和模拟，从而显著提升传统AI分析预测的能力。&lt;h4&gt;结论&lt;/h4&gt;本文认为，这项工作代表了从智慧城市关键用户视角对GenAI技术在智慧城市中应用的首次全面总结。&lt;h4&gt;翻译&lt;/h4&gt;The paper mainly discusses the application of Generative AI (GenAI) in smart cities, especially the application of GenAI based on conversational interfaces for different user types such as citizens, operators, and planners.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of IoT in cities, combined with Digital Twins, creates arich data foundation for Smart Cities aimed at improving urban life andoperations. Generative AI (GenAI) significantly enhances this potential, movingbeyond traditional AI analytics and predictions by processing multimodalcontent and generating novel outputs like text and simulations. Usingspecialized or foundational models, GenAI's natural language abilities such asNatural Language Understanding (NLU) and Natural Language Generation (NLG) canpower tailored applications and unified interfaces, dramatically loweringbarriers for users interacting with complex smart city systems. In this paper,we focus on GenAI applications based on conversational interfaces within thecontext of three critical user archetypes in a Smart City - Citizens, Operatorsand Planners. We identify and review GenAI models and techniques that have beenproposed or deployed for various urban subsystems in the contexts of these userarchetypes. We also consider how GenAI can be built on the existing datafoundation of official city records, IoT data streams and Urban Digital Twins.We believe this work represents the first comprehensive summarization of GenAItechniques for Smart Cities from the lens of the critical users in a SmartCity.</description>
      <author>example@mail.com (Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj)</author>
      <guid isPermaLink="false">2505.08034v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for the Workshop "Safely Leveraging Vision-Language  Foundation Models in Robotics: Challenges and Opportunities" at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了语义异常检测，通过利用最先进视觉基础模型的语义先验知识，直接在图像上进行操作。提出了一种框架，将运行时图像的局部视觉嵌入与被认为是安全且性能良好的名义场景数据库进行比较。&lt;h4&gt;背景&lt;/h4&gt;语义异常是自主系统中常见的视觉元素组合异常，可能导致系统推理失败。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，用于检测自主系统中的语义异常。&lt;h4&gt;方法&lt;/h4&gt;提出两种框架变体：一种使用基于原始网格的嵌入，另一种利用实例分割进行对象中心表示。为了提高鲁棒性，引入了一种简单的过滤机制来抑制假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;在CARLA模拟的异常评估中，基于实例的方法结合过滤机制的性能与GPT-4o相当，同时提供了精确的异常定位。&lt;h4&gt;结论&lt;/h4&gt;基础模型中的视觉嵌入对于自主系统中的实时异常检测具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic anomalies are contextually invalid or unusual combinations offamiliar visual elements that can cause undefined behavior and failures insystem-level reasoning for autonomous systems. This work explores semanticanomaly detection by leveraging the semantic priors of state-of-the-art visionfoundation models, operating directly on the image. We propose a framework thatcompares local vision embeddings from runtime images to a database of nominalscenarios in which the autonomous system is deemed safe and performant. In thiswork, we consider two variants of the proposed framework: one using rawgrid-based embeddings, and another leveraging instance segmentation forobject-centric representations. To further improve robustness, we introduce asimple filtering mechanism to suppress false positives. Our evaluations onCARLA-simulated anomalies show that the instance-based method with filteringachieves performance comparable to GPT-4o, while providing precise anomalylocalization. These results highlight the potential utility of visionembeddings from foundation models for real-time anomaly detection in autonomoussystems.</description>
      <author>example@mail.com (Max Peter Ronecker, Matthew Foutter, Amine Elhafsi, Daniele Gammelli, Ihor Barakaiev, Marco Pavone, Daniel Watzenig)</author>
      <guid isPermaLink="false">2505.07998v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Development of a WAZOBIA-Named Entity Recognition System</title>
      <link>http://arxiv.org/abs/2505.07884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个针对尼日利亚三种主要语言（豪萨语、约鲁巴语和伊博语）的WAZOBIA-NER实体识别系统。&lt;h4&gt;背景&lt;/h4&gt;尽管计算语言学对非洲语言越来越感兴趣，但现有的NER系统主要关注英语、欧洲语言和一些全球语言，对资源匮乏的语言关注不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于尼日利亚三种主要语言的WAZOBIA-NER系统，以解决数据稀缺和语言多样性挑战。&lt;h4&gt;方法&lt;/h4&gt;研究首先为每种语言编制了标注数据集，然后探索了最新的机器学习技术，如条件随机场（CRF）和深度学习模型（如双向长短期记忆网络（BiLSTM）、双向编码器表示的Transformer（Bert）和循环神经网络（RNN）的微调）。系统还利用光学字符识别（OCR）技术将文本图像转换为机器可读文本。&lt;h4&gt;主要发现&lt;/h4&gt;系统在识别人、组织和地点三个实体方面取得了0.9511的精确度、0.9400的召回率、0.9564的F1分数和0.9301的准确率。模型在三种语言上进行了评估，精确度、召回率、F1分数和准确率是关键评估指标。&lt;h4&gt;结论&lt;/h4&gt;Wazobia-NER系统表明，使用当前的NLP框架和迁移学习为资源匮乏的非洲语言构建稳健的NER工具是可行的。&lt;h4&gt;翻译&lt;/h4&gt;Named Entity Recognition (NER) is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Named Entity Recognition NER is very crucial for various natural languageprocessing applications, including information extraction, machine translation,and sentiment analysis. Despite the ever-increasing interest in Africanlanguages within computational linguistics, existing NER systems focus mainlyon English, European, and a few other global languages, leaving a significantgap for under-resourced languages. This research presents the development of aWAZOBIA-NER system tailored for the three most prominent Nigerian languages:Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilationof annotated datasets for each language, addressing data scarcity andlinguistic diversity challenges. Exploring the state-of-the-art machinelearning technique, Conditional Random Fields (CRF) and deep learning modelssuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional EncoderRepresentation from Transformers (Bert) and fine-tune with a Recurrent NeuralNetwork (RNN), the study evaluates the effectiveness of these approaches inrecognizing three entities: persons, organizations, and locations. The systemutilizes optical character recognition (OCR) technology to convert textualimages into machine-readable text, thereby enabling the Wazobia system toaccept both input text and textual images for extraction purposes. The systemachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 inF1-score, and 0.9301 in accuracy. The model's evaluation was conducted acrossthree languages, with precision, recall, F1-score, and accuracy as keyassessment metrics. The Wazobia-NER system demonstrates that it is feasible tobuild robust NER tools for under-resourced African languages using current NLPframeworks and transfer learning.</description>
      <author>example@mail.com (S. E Emedem, I. E Onyenwe, E. G Onyedinma)</author>
      <guid isPermaLink="false">2505.07884v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）的预测解释方法，特别是针对全局反事实解释中的共同回溯问题，并提出了一种有效的算法COMRECGC。&lt;h4&gt;背景&lt;/h4&gt;GNNs在多个领域如社交网络、分子生物学和推荐系统中得到广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;提出并解决GNNs全局反事实解释中的共同回溯问题，设计有效的算法来生成与所有输入拒绝图相关的接受图。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同回溯解释问题，并设计了COMRECGC算法来解决。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个真实世界图数据集上与强基线进行了基准测试，表现出优于竞争对手的性能。&lt;h4&gt;结论&lt;/h4&gt;共同回溯解释与图反事实解释相比，在药物发现或计算生物学等应用中具有可比性或优越性，值得考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ssggreg/comrecgc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Recent results of (semi)leptonic decays of charm hadrons at BESIII</title>
      <link>http://arxiv.org/abs/2505.05123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  contribution to the 2025 Electroweak session of the 59th Rencontres  de Moriond&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文报告了BESIII实验最近对 charm 等离子子 (半)轻子衰变的测量结果。&lt;h4&gt;背景&lt;/h4&gt;包括 D^+ → μ^+ν_μ, D^+ → τ^+ν_τ, D^+ → η′ℓ^+ν_ℓ, 以及 D^{0(+)} → K_ℓ^+ν_ℓ (其中 ℓ=e, μ) 的衰变。&lt;h4&gt;目的&lt;/h4&gt;这些测量提供了最精确或首次对 CKM 矩阵元素 |V_{cs(d)}|，衰变常数 f_{D^+}，以及强子形式因子 f_+^{D^+→η′}(0) 和 f_+^{D→K}(0) 的确定。&lt;h4&gt;方法&lt;/h4&gt;利用了机器学习中的图神经网络首次观察到了稀有β衰变 Λ_c^+ → n e^+ν_e。&lt;h4&gt;主要发现&lt;/h4&gt;测量提供了对 CKM 矩阵元素、衰变常数和强子形式因子的精确测定，并测试了电子-μ子以及τ-μ的轻子味 universality。&lt;h4&gt;结论&lt;/h4&gt;论文通过BESIII实验的测量结果，对 charm 等离子子的轻子衰变特性有了更深入的了解，并通过机器学习方法实现了对稀有β衰变的首次观察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this talk, we report the recent measurements of (semi)leptonic decays ofcharm mesons from the BESIII experiment, including $D^+\to\mu^+\nu_\mu$,$D^+\to\tau^+\nu_\tau$, $D^+\to\eta^\prime\ell^+\nu_\ell$, and $D^{0(+)}\to\barK\ell^+\nu_\ell$ (where $\ell=e, \mu$). These measurements provide the mostprecise or first determinations to date of the CKM matrix elements$|V_{cs(d)}|$, the decay constant $f_{D^+}$, and the hadronic form factors$f_+^{D^+\to \eta^\prime}(0)$ and $f_+^{D\to \bar K}(0)$. Lepton flavoruniversality of $e-\mu$ and $\tau-\mu$ are also tested with these decays.Additionally, we present the first observation of the rare beta decay$\Lambda_c^+\to ne^+\nu_e$ with machine learning of Graph Neural Network.</description>
      <author>example@mail.com (Xiang Pan)</author>
      <guid isPermaLink="false">2505.05123v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
  <item>
      <title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
      <link>http://arxiv.org/abs/2505.07611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了147项关于基于视觉的交通事故预测（Vision-TAA）的研究，重点关注监督学习、无监督学习和混合深度学习模型在事故预测中的应用，以及使用真实世界和合成数据集的方法。&lt;h4&gt;背景&lt;/h4&gt;交通事故预测和检测对于提高道路安全至关重要，基于视觉的交通事故预测在深度学习时代成为一种有前景的方法。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供关于Vision-TAA系统发展的基础参考，以促进道路安全和交通管理。&lt;h4&gt;方法&lt;/h4&gt;本文将现有方法分为四种主要方法：基于图像和视频特征的预测、基于时空特征的预测、场景理解和多模态数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些方法显示出巨大的潜力，但数据稀缺、在复杂场景中泛化能力有限和实时性能限制等问题仍然普遍存在。&lt;h4&gt;结论&lt;/h4&gt;本文强调了未来研究的机遇，包括多模态数据融合、自监督学习和基于Transformer架构的集成，以提高预测准确性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic accident prediction and detection are critical for enhancing roadsafety,and vision-based traffic accident anticipation (Vision-TAA) has emergedas a promising approach in the era of deep learning.This paper reviews 147recent studies,focusing on the application of supervised,unsupervised,andhybrid deep learning models for accident prediction,alongside the use ofreal-world and synthetic datasets.Current methodologies are categorized intofour key approaches: image and video feature-based prediction, spatiotemporalfeature-based prediction, scene understanding,and multimodal data fusion.Whilethese methods demonstrate significant potential,challenges such as datascarcity,limited generalization to complex scenarios,and real-time performanceconstraints remain prevalent. This review highlights opportunities for futureresearch,including the integration of multimodal data fusion, self-supervisedlearning,and Transformer-based architectures to enhance prediction accuracy andscalability.By synthesizing existing advancements and identifying criticalgaps, this paper provides a foundational reference for developing robust andadaptive Vision-TAA systems,contributing to road safety and traffic management.</description>
      <author>example@mail.com (Yi Zhang, Wenye Zhou, Ruonan Lin, Xin Yang, Hao Zheng)</author>
      <guid isPermaLink="false">2505.07611v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
      <link>http://arxiv.org/abs/2505.07818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://dancegrpo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DanceGRPO是一个统一的框架，将Group Relative Policy Optimization (GRPO)应用于视觉生成，能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。&lt;h4&gt;背景&lt;/h4&gt;视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。&lt;h4&gt;目的&lt;/h4&gt;提出DanceGRPO，旨在提供一个统一的强化学习算法，以解决上述问题，并实现视觉生成中的高效反馈。&lt;h4&gt;方法&lt;/h4&gt;DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。&lt;h4&gt;主要发现&lt;/h4&gt;DanceGRPO在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。&lt;h4&gt;结论&lt;/h4&gt;DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;DanceGRPO是一个将Group Relative Policy Optimization (GRPO)应用于视觉生成的统一框架，它能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。在视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。本文提出了DanceGRPO，旨在解决这些问题，并实现视觉生成中的高效反馈。DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中，DanceGRPO优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in generative models-particularly diffusion models andrectified flows-have revolutionized visual content creation, yet aligning modeloutputs with human preferences remains a critical challenge. Existingreinforcement learning (RL)-based methods for visual generation face criticallimitations: incompatibility with modern Ordinary Differential Equations(ODEs)-based sampling paradigms, instability in large-scale training, and lackof validation for video generation. This paper introduces DanceGRPO, the firstunified framework to adapt Group Relative Policy Optimization (GRPO) to visualgeneration paradigms, unleashing one unified RL algorithm across two generativeparadigms (diffusion models and rectified flows), three tasks (text-to-image,text-to-video, image-to-video), four foundation models (Stable Diffusion,HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/videoaesthetics, text-image alignment, video motion quality, and binary reward). Toour knowledge, DanceGRPO is the first RL-based unified framework capable ofseamless adaptation across diverse generative paradigms, tasks, foundationalmodels, and reward models. DanceGRPO demonstrates consistent and substantialimprovements, which outperform baselines by up to 181% on benchmarks such asHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only canstabilize policy optimization for complex video generation, but also enablesgenerative policy to better capture denoising trajectories for Best-of-Ninference scaling and learn from sparse binary feedback. Our results establishDanceGRPO as a robust and versatile solution for scaling Reinforcement Learningfrom Human Feedback (RLHF) tasks in visual generation, offering new insightsinto harmonizing reinforcement learning and visual synthesis. The code will bereleased.</description>
      <author>example@mail.com (Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo)</author>
      <guid isPermaLink="false">2505.07818v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文报告了ACVLAB团队为ICRA 2025 GOOSE 2D语义分割挑战赛开发的语义分割框架，该框架在真实世界条件下将户外场景解析为九个语义类别。&lt;h4&gt;背景&lt;/h4&gt;该框架旨在解决户外场景中由于光照不一致导致的图像分割问题。&lt;h4&gt;目的&lt;/h4&gt;提高语义分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该框架采用Swin Transformer骨干网络，并增强其空间泛化能力；引入了旋转位置编码（RoPE）以增强空间信息；设计了色彩偏移估计与校正模块以补偿自然环境中的光照不一致；采用基于分位数的方法进行降噪，降低误差最大的2.5%像素的影响。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在GOOSE官方测试集上实现了平均交并比（mIoU）0.848，证明了色彩校正、位置编码和误差感知降噪在鲁棒语义分割中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架结合了多种技术，在真实世界条件下的户外场景语义分割中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This report presents our semantic segmentation framework developed by teamACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, whichfocuses on parsing outdoor scenes into nine semantic categories underreal-world conditions. Our method integrates a Swin Transformer backboneenhanced with Rotary Position Embedding (RoPE) for improved spatialgeneralization, alongside a Color Shift Estimation-and-Correction moduledesigned to compensate for illumination inconsistencies in naturalenvironments. To further improve training stability, we adopt a quantile-baseddenoising strategy that downweights the top 2.5\% of highest-error pixels,treating them as noise and suppressing their influence during optimization.Evaluated on the official GOOSE test set, our approach achieved a meanIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness ofcombining color correction, positional encoding, and error-aware denoising inrobust semantic segmentation.</description>
      <author>example@mail.com (Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, Yu-Jou Hsiao)</author>
      <guid isPermaLink="false">2505.06991v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A class of distributed automata that contains the modal mu-fragment</title>
      <link>http://arxiv.org/abs/2505.07816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文将分阶模态μ-演算的μ-片段翻译成一类分布式消息传递自动机。&lt;h4&gt;背景&lt;/h4&gt;在逻辑和计算模型领域，研究分阶模态μ-演算与分布式消息传递自动机之间的关系。&lt;h4&gt;目的&lt;/h4&gt;探索分阶模态μ-演算在分布式消息传递自动机中的表达能力和应用。&lt;h4&gt;方法&lt;/h4&gt;通过翻译技术将分阶模态μ-演算的μ-片段转换为分布式消息传递自动机。&lt;h4&gt;主要发现&lt;/h4&gt;作为推论，本文获得了一个关于循环图神经网络在实数和分阶模态替换演算下具有相同表达能力定理的替代证明。&lt;h4&gt;结论&lt;/h4&gt;分阶模态μ-演算和分布式消息传递自动机之间存在密切的联系，可以相互转换，且在逻辑单形二阶逻辑MSO的约束下具有相同的表达能力。&lt;h4&gt;翻译&lt;/h4&gt;本文实现了分阶模态μ-演算的μ-片段到分布式消息传递自动机的翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper gives a translation from the $\mu$-fragment of the graded modal$\mu$-calculus to a class of distributed message-passing automata. As acorollary, we obtain an alternative proof for a theorem from\cite{ahvonen_neurips} stating that recurrent graph neural networks workingwith reals and graded modal substitution calculus have the same expressivepower in restriction to the logic monadic second-order logic MSO.</description>
      <author>example@mail.com (Veeti Ahvonen, Damian Heiman, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07816v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.06951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, International Conference on Robotics and  Automation(ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对热图像语义分割的跨光谱无监督领域自适应（UDA）方法，通过提高互补信息交换和增强夜间场景下的性能，解决了传统方法在领域自适应中的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶领域，热图像语义分割由于能在恶劣视觉条件下提供稳健的场景理解而成为关键研究领域。然而，由于缺乏标注的热图像数据集，无监督领域自适应方法成为解决这一问题的有效途径。&lt;h4&gt;目的&lt;/h4&gt;旨在通过无监督领域自适应方法解决热图像语义分割中标签数据不足的问题，并提高模型在不同领域的适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型的掩码互学习策略，通过在光谱模型间选择性传递结果并屏蔽不确定区域，促进互补信息的交换。同时，引入了一种新型的原型自监督损失函数，用于增强夜间场景下热分割模型的表现，解决RGB预训练网络在低光照条件下知识迁移能力不足的问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在实验中表现出比之前UDA方法更高的性能，并且与最先进的监督方法具有可比的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法有效提高了热图像语义分割的性能，特别是在夜间场景和领域自适应方面。&lt;h4&gt;翻译&lt;/h4&gt;In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous driving, thermal image semantic segmentation has emerged as acritical research area, owing to its ability to provide robust sceneunderstanding under adverse visual conditions. In particular, unsuperviseddomain adaptation (UDA) for thermal image segmentation can be an efficientsolution to address the lack of labeled thermal datasets. Nevertheless, sincethese methods do not effectively utilize the complementary information betweenRGB and thermal images, they significantly decrease performance during domainadaptation. In this paper, we present a comprehensive study on cross-spectralUDA for thermal image semantic segmentation. We first propose a novel maskedmutual learning strategy that promotes complementary information exchange byselectively transferring results between each spectral model while masking outuncertain regions. Additionally, we introduce a novel prototypicalself-supervised loss designed to enhance the performance of the thermalsegmentation model in nighttime scenarios. This approach addresses thelimitations of RGB pre-trained networks, which cannot effectively transferknowledge under low illumination due to the inherent constraints of RGBsensors. In experiments, our method achieves higher performance over previousUDA methods and comparable performance to state-of-the-art supervised methods.</description>
      <author>example@mail.com (Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi)</author>
      <guid isPermaLink="false">2505.06951v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.07398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DepthFusion的深度感知混合特征融合策略，用于提高LiDAR相机3D目标检测器的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的3D目标检测器主要关注特征融合，但忽略了深度因素在设计融合策略时的重要性。&lt;h4&gt;目的&lt;/h4&gt;通过统计分析和可视化，观察不同模态在不同深度下的作用，并提出一种新的融合策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一种DepthFusion策略，通过在全局和局部级别引入深度编码来引导点云和RGB图像模态的权重。具体包括Depth-GFusion模块和Depth-LFusion模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过统计分析和可视化发现，不同模态在深度变化时扮演不同的角色。&lt;h4&gt;结论&lt;/h4&gt;DepthFusion方法在nuScenes和KITTI数据集上的实验结果表明，该方法优于现有方法，并且对各种类型的损坏更加鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art LiDAR-camera 3D object detectors usually focus on featurefusion. However, they neglect the factor of depth while designing the fusionstrategy. In this work, we are the first to observe that different modalitiesplay different roles as depth varies via statistical analysis andvisualization. Based on this finding, we propose a Depth-Aware Hybrid FeatureFusion (DepthFusion) strategy that guides the weights of point cloud and RGBimage modalities by introducing depth encoding at both global and local levels.Specifically, the Depth-GFusion module adaptively adjusts the weights of imageBird's-Eye-View (BEV) features in multi-modal global features via depthencoding. Furthermore, to compensate for the information lost when transferringraw features to the BEV space, we propose a Depth-LFusion module, whichadaptively adjusts the weights of original voxel features and multi-view imagefeatures in multi-modal local features via depth encoding. Extensiveexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusionmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusionis more robust to various kinds of corruptions, outperforming previous methodson the nuScenes-C dataset.</description>
      <author>example@mail.com (Mingqian Ji, Jian Yang, Shanshan Zhang)</author>
      <guid isPermaLink="false">2505.07398v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了首个综合多模态城市数字孪生（UDT）基准数据集TUM2TWIN，旨在解决城市数字孪生创建中的挑战，并推动智能、数据驱动的城市环境的发展。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生（UDTs）对于城市管理及整合来自不同来源的复杂、异构数据变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决创建UDT过程中遇到的挑战，如获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，包含地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，覆盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，支持传感器分析及高级重建方法的发展。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持下游任务，如NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理及整合来自不同来源的复杂、异构数据的关键。创建UDT涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性。当前数据集通常仅限于处理链的一部分，阻碍了全面UDT验证。为了解决这些挑战，我们引入了首个综合多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，涵盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持传感器分析及高级重建方法的发展。此外，我们探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信，这一贡献为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。项目可访问：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models</title>
      <link>http://arxiv.org/abs/2505.07364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，跨模态医学图像翻译领域的研究取得了丰硕成果，特别是基于生成对抗网络（GAN）的架构在处理大型多模态数据集稀缺的问题上表现出良好的性能。&lt;h4&gt;目的&lt;/h4&gt;旨在设计并比较不同的GAN框架，以从T1加权MRI数据生成合成[18F]氟代脱氧葡萄糖（FDG）PET图像，并评估这些合成数据在无监督异常检测模型训练中的应用。&lt;h4&gt;方法&lt;/h4&gt;设计了基于GAN的框架来生成合成FDG PET图像，进行了定性和定量视觉质量评估，并探索了这些合成数据在无监督异常检测模型训练中的影响。引入了针对无监督检测任务的合成FDG PET数据的诊断任务导向质量指标，并使用这些合成数据训练了一个结合Siamese自编码器深度表示学习和OC-SVM密度支持估计模型的无监督异常检测（UAD）模型。&lt;h4&gt;主要发现&lt;/h4&gt;最好的GAN模型能够生成与真实控制数据集在结构相似性（SSIM）和峰值信噪比（PSNR）值接近0.9和23.8的合成PET图像。在基于这些合成正常PET数据的最佳UAD模型上训练，达到了74%的敏感性。&lt;h4&gt;结论&lt;/h4&gt;基于GAN的模型最适合进行MR T1到FDG PET的翻译，优于transformer或扩散模型。此外，这些合成数据对UAD模型的训练和癫痫患者临床检查的评估也具有诊断价值。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background and Objective. Research in the cross-modal medical imagetranslation domain has been very productive over the past few years in tacklingthe scarce availability of large curated multimodality datasets with thepromising performance of GAN-based architectures. However, only a few of thesestudies assessed task-based related performance of these synthetic data,especially for the training of deep models. Method. We design and comparedifferent GAN-based frameworks for generating synthetic brain[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We firstperform standard qualitative and quantitative visual quality evaluation. Then,we explore further impact of using these fake PET data in the training of adeep unsupervised anomaly detection (UAD) model designed to detect subtleepilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostictask-oriented quality metrics of the synthetic FDG PET data tailored to ourunsupervised detection task, then use these fake data to train a use case UADmodel combining a deep representation learning based on siamese autoencoderswith a OC-SVM density support estimation model. This model is trained on normalsubjects only and allows the detection of any variation from the pattern of thenormal population. We compare the detection performance of models trained on 35paired real MR T1 of normal subjects paired either on 35 true PET images or on35 synthetic PET images generated from the best performing generative models.Performance analysis is conducted on 17 exams of epilepsy patients undergoingsurgery. Results. The best performing GAN-based models allow generatingrealistic fake PET images of control subject with SSIM and PSNR values around0.9 and 23.8, respectively and in distribution (ID) with regard to the truecontrol dataset. The best UAD model trained on these synthetic normative PETdata allows reaching 74% sensitivity. Conclusion. Our results confirm thatGAN-based models are the best suited for MR T1 to FDG PET translation,outperforming transformer or diffusion models. We also demonstrate thediagnostic value of these synthetic data for the training of UAD models andevaluation on clinical exams of epilepsy patients. Our code and the normativeimage dataset are available.</description>
      <author>example@mail.com (Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien)</author>
      <guid isPermaLink="false">2505.07364v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Gameplay Highlights Generation</title>
      <link>http://arxiv.org/abs/2505.07721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。&lt;h4&gt;背景&lt;/h4&gt;传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。&lt;h4&gt;方法&lt;/h4&gt;首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。&lt;h4&gt;结论&lt;/h4&gt;X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;h4&gt;翻译&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。本研究开发了一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we enable gamers to share their gaming experience on socialmedia by automatically generating eye-catching highlight reels from theirgameplay session Our automation will save time for gamers while increasingaudience engagement. We approach the highlight generation problem by firstidentifying intervals in the video where interesting events occur and thenconcatenate them. We developed an in-house gameplay event detection datasetcontaining interesting events annotated by humans using VIA video annotator.Traditional techniques for highlight detection such as game engine integrationrequires expensive collaboration with game developers. OCR techniques whichdetect patches of specific images or texts require expensive per gameengineering and may not generalize across game UI and different language. Wefinetuned a multimodal general purpose video understanding model such as X-CLIPusing our dataset which generalizes across multiple games in a genre withoutper game engineering. Prompt engineering was performed to improve theclassification performance of this multimodal model. Our evaluation showed thatsuch a finetuned model can detect interesting events in first person shootinggames from unseen gameplay footage with more than 90% accuracy. Moreover, ourmodel performed significantly better on low resource games (small dataset) whentrained along with high resource games, showing signs of transfer learning. Tomake the model production ready, we used ONNX libraries to enable crossplatform inference. These libraries also provide post training quantizationtools to reduce model size and inference time for deployment. ONNX runtimelibraries with DirectML backend were used to perform efficient inference onWindows OS. We show that natural language supervision in the X-CLIP model leadsto data efficient and highly performant video recognition models.</description>
      <author>example@mail.com (Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo)</author>
      <guid isPermaLink="false">2505.07721v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比学习的异构图异常检测模型EAGLE，通过对比异常节点与正常节点到局部上下文距离，提高了异常检测的效率。&lt;h4&gt;背景&lt;/h4&gt;图异常检测在多个实际场景中非常重要，已有基于深度学习的方法在性能上优于传统方法，但现有方法在效率上存在不足。&lt;h4&gt;目的&lt;/h4&gt;针对现有方法效率不足的问题，提出一种高效的异构图异常检测模型。&lt;h4&gt;方法&lt;/h4&gt;EAGLE模型首先在元路径级别上采样实例对进行对比学习，然后使用基于图自动编码器的模型无监督地学习节点嵌入，并结合判别器预测节点的异常分数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EAGLE在三个异构网络数据集上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;EAGLE模型能够有效提高异构图异常检测的效率，并在实际数据集上取得了良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/MIS.2022.3229147&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection is a popular and vital task in various real-worldscenarios, which has been studied for several decades. Recently, many studiesextending deep learning-based methods have shown preferable performance ongraph anomaly detection. However, existing methods are lack of efficiency thatis definitely necessary for embedded devices. Towards this end, we propose anEfficient Anomaly detection model on heterogeneous Graphs via contrastiveLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms oftheir distances to the local context. The proposed method first samplesinstance pairs on meta path-level for contrastive learning. Then, a graphautoencoder-based model is applied to learn informative node embeddings in anunsupervised way, which will be further combined with the discriminator topredict the anomaly scores of nodes. Experimental results show that EAGLEoutperforms the state-of-the-art methods on three heterogeneous networkdatasets.</description>
      <author>example@mail.com (Jing Ren, Mingliang Hou, Zhixuan Liu, Xiaomei Bai)</author>
      <guid isPermaLink="false">2505.07508v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过使用视频中的估计姿态来增强3D人体运动预测（HMP）模型的方法，以降低数据收集成本并提高模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D HMP模型训练需要昂贵的运动捕捉数据，而此类数据的收集成本限制了数据的多样性，导致模型在未见过的运动或主体上的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过使用易于获取的视频中的估计姿态来增强HMP模型，以提高其泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将来自单目视频的2D姿态经过处理转化为运动捕捉风格的3D运动，并通过额外学习使HMP模型适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法对HMP模型有定性和定量的影响。&lt;h4&gt;结论&lt;/h4&gt;该方法可以有效地提高HMP模型的泛化能力，并减少对昂贵运动捕捉数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在3D人体运动预测（HMP）中，传统方法使用昂贵的运动捕捉数据训练HMP模型。然而，此类运动捕捉数据的数据收集成本限制了数据的多样性，导致对未见过的运动或主体的泛化能力差。为了解决这个问题，本文提出通过使用从易于获取的视频中估计的姿态来增强HMP模型的方法。通过从获得的运动中进行额外学习，HMP模型被适应到测试域。实验结果表明了我们的方法对HMP模型的定性和定量影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Tagging fully hadronic exotic decays of the vectorlike $\mathbf{B}$ quark using a graph neural network</title>
      <link>http://arxiv.org/abs/2505.07769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对偶产生的矢量型B夸克衰变到新规范单态（伪）标量场Φ和b夸克的LHC前景，并使用混合深度学习模型来提高探测效率。&lt;h4&gt;背景&lt;/h4&gt;之前的研究中，作者已经探讨了机器学习增强的矢量型单态B夸克衰变到单态标量或伪标量场的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在通过深度学习模型提高对这种衰变模式的探测能力。&lt;h4&gt;方法&lt;/h4&gt;采用包含图神经网络和深度神经网络的混合深度学习模型，以克服标准模型背景大和缺乏轻子末态的问题。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习分析流程的性能可以达到半轻子模式的水平，在B夸克完全异质衰变的情况下，即BR(B→bΦ) = 100%，在HL-LHC上可以达到约MB=1.8（2.4）TeV的发现（排除）范围。&lt;h4&gt;结论&lt;/h4&gt;混合深度学习模型能够显著提高对矢量型B夸克衰变到新规范单态场的研究效率，有望在HL-LHC上探测到这种衰变模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following up on our earlier study in [J. Bardhan et al., Machinelearning-enhanced search for a vectorlike singlet B quark decaying to a singletscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], weinvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decayingexotically to a new gauge-singlet (pseudo)scalar field $\Phi$ and a $b$ quark.After the electroweak symmetry breaking, the $\Phi$ decays predominantly to$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.Because of the large Standard Model background and the lack of leptonichandles, it is a difficult channel to probe. To overcome the challenge, weemploy a hybrid deep learning model containing a graph neural network followedby a deep neural network. We estimate that such a state-of-the-art deeplearning analysis pipeline can lead to a performance comparable to that in thesemi-leptonic mode, taking the discovery (exclusion) reach up to about$M_B=1.8\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\to b\Phi) = 100\%$.</description>
      <author>example@mail.com (Jai Bardhan, Tanumoy Mandal, Subhadip Mitra, Cyrin Neeraj, Mihir Rawat)</author>
      <guid isPermaLink="false">2505.07769v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Camera Control at the Edge with Language Models for Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures. This work was presented and published at the 11th  IEEE International Conference on Control, Automation and Robotics (ICCAR) in  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了优化提示统一系统（OPUS），该系统利用大型语言模型（LLM）控制PTZ相机，并提供对自然环境的上下文理解。&lt;h4&gt;背景&lt;/h4&gt;传统的PTZ相机控制方法复杂且效率低。&lt;h4&gt;目的&lt;/h4&gt;OPUS系统旨在提高PTZ相机控制的成本效益，并实现与环境的高效交互。&lt;h4&gt;方法&lt;/h4&gt;OPUS系统通过高级相机控制API生成关键词，并使用合成数据通过监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型。此外，通过将多个摄像头的数据进行文本描述，OPUS提高了环境意识。&lt;h4&gt;主要发现&lt;/h4&gt;在基准测试中，OPUS方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。&lt;h4&gt;结论&lt;/h4&gt;OPUS系统通过直观的自然语言界面简化了PTZ相机的操作，消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，这代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种优化提示统一系统（OPUS），这是一个利用大型语言模型（LLM）控制PTZ相机、提供对自然环境的上下文理解的框架。为了实现这一目标，OPUS系统通过从高级相机控制API生成关键词并通过对合成数据的监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型来提高成本效益。这使其能够在保持与GPT-4等大型模型相当性能的同时实现高效的边缘部署。OPUS通过将来自多个摄像头的数据进行文本描述来增强环境意识，消除了对专用感官标记的需求。在基准测试中，我们的方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。该系统展示了OPUS通过直观的自然语言界面简化PTZ相机操作的能力。这种方法消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Optimized Prompt-based Unified System (OPUS), aframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom(PTZ) cameras, providing contextual understanding of natural environments. Toachieve this goal, the OPUS system improves cost-effectiveness by generatingkeywords from a high-level camera control API and transferring knowledge fromlarger closed-source language models to smaller ones through SupervisedFine-Tuning (SFT) on synthetic data. This enables efficient edge deploymentwhile maintaining performance comparable to larger models like GPT-4. OPUSenhances environmental awareness by converting data from multiple cameras intotextual descriptions for language models, eliminating the need for specializedsensory tokens. In benchmark testing, our approach significantly outperformedboth traditional language model techniques and more complex prompting methods,achieving a 35% improvement over advanced techniques and a 20% higher taskaccuracy compared to closed-source models like Gemini Pro. The systemdemonstrates OPUS's capability to simplify PTZ camera operations through anintuitive natural language interface. This approach eliminates the need forexplicit programming and provides a conversational method for interacting withcamera systems, representing a significant advancement in how users can controland utilize PTZ camera technology.</description>
      <author>example@mail.com (Alexiy Buynitsky, Sina Ehsani, Bhanu Pallakonda, Pragyana Mishra)</author>
      <guid isPermaLink="false">2505.06402v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Generating Skyline Explanations for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。&lt;h4&gt;目的&lt;/h4&gt;引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。&lt;h4&gt;方法&lt;/h4&gt;1. 将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。2. 设计了高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。3. 进一步开发了一个算法来多样化解释，以提供更全面的视角。&lt;h4&gt;主要发现&lt;/h4&gt;1. 天际线解释生成问题是困难的。2. 所设计的算法在保证质量的同时，能够高效地生成解释。3. 通过多样化解释，可以提供更全面的视角。&lt;h4&gt;结论&lt;/h4&gt;使用真实世界的图，通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。现有GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。我们引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。我们将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。我们设计了一种高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。我们进一步开发了一个算法来多样化解释，以提供更全面的视角。使用真实世界的图，我们通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel approach to generate subgraph explanations forgraph neural networks GNNs that simultaneously optimize multiple measures forexplainability. Existing GNN explanation methods often compute subgraphs(called ``explanatory subgraphs'') that optimize a pre-defined, singleexplainability measure, such as fidelity or conciseness. This can lead tobiased explanations that cannot provide a comprehensive explanation to clarifythe output of GNN models. We introduce skyline explanation, a GNN explanationparadigm that aims to identify k explanatory subgraphs by simultaneouslyoptimizing multiple explainability measures. (1) We formulate skylineexplanation generation as a multi-objective optimization problem, and pursueexplanations that approximate a skyline set of explanatory subgraphs. We showthe hardness for skyline explanation generation. (2) We design efficientalgorithms with an onion-peeling approach that strategically removes edges fromneighbors of nodes of interests, and incrementally improves explanations as itexplores an interpretation domain, with provable quality guarantees. (3) Wefurther develop an algorithm to diversify explanations to provide morecomprehensive perspectives. Using real-world graphs, we empirically verify theeffectiveness, efficiency, and scalability of our algorithms.</description>
      <author>example@mail.com (Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu)</author>
      <guid isPermaLink="false">2505.07635v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GLFM的多类点云异常检测方法，通过全局-局部特征匹配逐步分离不同类别间易混淆的数据。&lt;h4&gt;背景&lt;/h4&gt;随着产品类别的增加，单类无监督方法在计算和存储成本上的限制使得多类无监督方法成为必要。&lt;h4&gt;目的&lt;/h4&gt;为了解决正常点和异常点在不同类别数据中特征相似导致的多类方法性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;GLFM分为三个阶段：第一阶段提出异常合成管道，通过拉伸点云创建丰富的异常数据来优化特征提取器；第二阶段根据全局和局部特征分布建立全局和局部记忆库，减弱特征混淆对记忆库建立的影响；第三阶段利用测试数据与全局和局部记忆库的特征距离进行异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的实验表明，GLFM在点云异常检测方面具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;GLFM是一种有效的多类点云异常检测方法，能够提高异常检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云异常检测对于各种工业应用至关重要。由于产品类别的增加导致的巨大计算和存储成本限制了单类无监督方法的应用，需要发展多类无监督方法。然而，正常点和异常点在不同类别数据中的特征相似性导致了特征混淆问题，这极大地阻碍了多类方法的表现。因此，我们引入了一种名为GLFM的多类点云异常检测方法，利用全局-局部特征匹配逐步分离跨多个类别易混淆的数据。具体来说，GLFM分为三个阶段：第一阶段提出了一种异常合成管道，通过拉伸点云创建丰富的异常数据，用于优化点云特征提取器；第二阶段根据所有训练数据的全局和局部特征分布建立全局和局部记忆库，减弱了特征混淆对记忆库建立的影响；第三阶段利用测试数据通过其与全局和局部记忆库的特征距离进行异常检测。在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的大量实验展示了我们提出的GLFM在点云异常检测方面的优越性能。代码可在https://github.com/hustCYQ/GLFM-Multi-class-3DAD上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hustCYQ/GLFM-Multi-class-3DAD&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud anomaly detection is essential for various industrialapplications. The huge computation and storage costs caused by the increasingproduct classes limit the application of single-class unsupervised methods,necessitating the development of multi-class unsupervised methods. However, thefeature similarity between normal and anomalous points from different classdata leads to the feature confusion problem, which greatly hinders theperformance of multi-class methods. Therefore, we introduce a multi-class pointcloud anomaly detection method, named GLFM, leveraging global-local featurematching to progressively separate data that are prone to confusion acrossmultiple classes. Specifically, GLFM is structured into three stages: Stage-Iproposes an anomaly synthesis pipeline that stretches point clouds to createabundant anomaly data that are utilized to adapt the point cloud featureextractor for better feature representation. Stage-II establishes the globaland local memory banks according to the global and local feature distributionsof all the training data, weakening the impact of feature confusion on theestablishment of the memory bank. Stage-III implements anomaly detection oftest data leveraging its feature distance from global and local memory banks.Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry partsdataset showcase our proposed GLFM's superior point cloud anomaly detectionperformance. The code is available athttps://github.com/hustCYQ/GLFM-Multi-class-3DAD.</description>
      <author>example@mail.com (Yuqi Cheng, Yunkang Cao, Dongfang Wang, Weiming Shen, Wenlong Li)</author>
      <guid isPermaLink="false">2505.07375v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation</title>
      <link>http://arxiv.org/abs/2505.07674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注在复杂拓扑环境中预测网络流量的挑战，提出了一种结合图卷积网络（GCN）和门控循环单元（GRU）的时空建模方法，通过实验验证了该方法在复杂网络流量预测场景中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;网络流量预测在复杂拓扑环境中是一个挑战，需要有效的方法来捕捉空间依赖和时间演变。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合GCN和GRU的时空建模方法，以精确预测未来的网络流量模式。&lt;h4&gt;方法&lt;/h4&gt;该方法通过GCN捕捉网络节点间的空间依赖，GRU模拟流量数据的时间演变。通过在真实世界Abilene网络流量数据集上进行实验，将所提模型与多种深度学习方法进行比较，并进行了消融实验以检验不同组件对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提方法在多个指标上均优于其他方法，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空建模方法在复杂网络流量预测中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本研究关注于在复杂拓扑环境中预测网络流量的挑战。它引入了一种时空建模方法，该方法结合了图卷积网络（GCN）和门控循环单元（GRU）。GCN组件捕捉网络节点之间的空间依赖，而GRU组件模拟流量数据的时间演变。这种组合允许精确预测未来的流量模式。通过在真实世界的Abilene网络流量数据集上进行全面实验，验证了所提模型的有效性。该模型与几种流行的深度学习方法进行了基准测试。此外，进行了一系列消融实验来检验各种组件对性能的影响，包括图卷积层的数量变化、不同的时间建模策略以及构建邻接矩阵的方法。结果表明，所提方法在多个指标上均取得了优异的性能，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the challenge of predicting network traffic withincomplex topological environments. It introduces a spatiotemporal modelingapproach that integrates Graph Convolutional Networks (GCN) with GatedRecurrent Units (GRU). The GCN component captures spatial dependencies amongnetwork nodes, while the GRU component models the temporal evolution of trafficdata. This combination allows for precise forecasting of future trafficpatterns. The effectiveness of the proposed model is validated throughcomprehensive experiments on the real-world Abilene network traffic dataset.The model is benchmarked against several popular deep learning methods.Furthermore, a set of ablation experiments is conducted to examine theinfluence of various components on performance, including changes in the numberof graph convolution layers, different temporal modeling strategies, andmethods for constructing the adjacency matrix. Results indicate that theproposed approach achieves superior performance across multiple metrics,demonstrating robust stability and strong generalization capabilities incomplex network traffic forecasting scenarios.</description>
      <author>example@mail.com (Nan Jiang, Wenxuan Zhu, Xu Han, Weiqiang Huang, Yumeng Sun)</author>
      <guid isPermaLink="false">2505.07674v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Across Fixed-Income Product Classes</title>
      <link>http://arxiv.org/abs/2505.07676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。&lt;h4&gt;背景&lt;/h4&gt;由于从稀疏或噪声数据中估计折扣曲线的挑战，将核岭回归（KR）扩展到向量值设置。&lt;h4&gt;目的&lt;/h4&gt;提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。&lt;h4&gt;方法&lt;/h4&gt;在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的正则化项导致有效的可分离核结构，理论贡献是可分离核引起的向量值RKHS范数的分解。&lt;h4&gt;结论&lt;/h4&gt;示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。由于从稀疏或噪声数据中估计折扣曲线的挑战，我们将核岭回归（KR）扩展到向量值设置。提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a framework for transfer learning of discount curves acrossdifferent fixed-income product classes. Motivated by challenges in estimatingdiscount curves from sparse or noisy data, we extend kernel ridge regression(KR) to a vector-valued setting, formulating a convex optimization problem in avector-valued reproducing kernel Hilbert space (RKHS). Each component of thesolution corresponds to the discount curve implied by a specific product class.We introduce an additional regularization term motivated by economicprinciples, promoting smoothness of spread curves between product classes, andshow that it leads to a valid separable kernel structure. A main theoreticalcontribution is a decomposition of the vector-valued RKHS norm induced byseparable kernels. We further provide a Gaussian process interpretation ofvector-valued KR, enabling quantification of estimation uncertainty.Illustrative examples demonstrate that transfer learning significantly improvesextrapolation performance and tightens confidence intervals compared tosingle-curve estimation.</description>
      <author>example@mail.com (Nicolas Camenzind, Damir Filipovic)</author>
      <guid isPermaLink="false">2505.07676v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2505.07322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RealRep的通用SDR到HDR转换方法，用于处理现实场景中风格多样的SDR内容。&lt;h4&gt;背景&lt;/h4&gt;HDR-WCG技术越来越普及，对将SDR内容转换为HDR的需求日益增加。现有的方法主要依赖于固定的色调映射算子，对于处理具有多种风格的SDR输入不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了一种名为RealRep的方法，可以处理现实场景中风格多样的SDR内容。&lt;h4&gt;方法&lt;/h4&gt;通过分离亮度（luminance）和色度（chrominance），分析不同风格内容之间的内在差异，并提出了一个解耦的多视角风格表示学习方法。该方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。此外，为了解决直接利用退化表示先验的困难，引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。&lt;h4&gt;主要发现&lt;/h4&gt;RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;RealRep和DDACMNet能够有效地将SDR内容转换为HDR，为HDR内容的制作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高动态范围宽色域（HDR-WCG）技术越来越普及，对将标准动态范围（SDR）内容转换为HDR的需求日益增加。现有方法主要依赖于固定的色调映射算子，对于处理现实场景中常见的多种风格的SDR输入不足。为了解决这一挑战，我们提出了一种处理现实场景中风格多样的SDR内容的通用SDR到HDR方法，称为Realistic Style Disentangled Representation Learning（RealRep）。通过分离亮度（luminance）和色度（chrominance），我们分析了具有不同风格的内容之间的内在差异，并提出了一种解耦的多视角风格表示学习方法。这种方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。受直接利用退化表示先验的困难所启发，我们进一步引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。大量的实验表明，RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becomingincreasingly prevalent, intensifying the demand for converting Standard DynamicRange (SDR) content to HDR. Existing methods primarily rely on fixed tonemapping operators, which are inadequate for handling SDR inputs with diversestyles commonly found in real-world scenarios. To address this challenge, wepropose a generalized SDR-to-HDR method that handles diverse styles inreal-world SDR content, termed Realistic Style Disentangled RepresentationLearning (RealRep). By disentangling luminance and chrominance, we analyze theintrinsic differences between contents with varying styles and propose adisentangled multi-view style representation learning method. This approachcaptures the guidance prior of true luminance and chrominance distributionsacross different styles, even when the SDR style distributions exhibitsignificant variations, thereby establishing a robust embedding space forinverse tone mapping. Motivated by the difficulty of directly utilizingdegradation representation priors, we further introduce the Degradation-DomainAware Controlled Mapping Network (DDACMNet), a two-stage framework thatperforms adaptive hierarchical mapping guided by a control-aware normalizationmechanism. DDACMNet dynamically modulates the mapping process viadegradation-conditioned hierarchical features, enabling robust adaptationacross diverse degradation domains. Extensive experiments show that RealRepconsistently outperforms state-of-the-art methods with superior generalizationand perceptually faithful HDR color gamut reconstruction.</description>
      <author>example@mail.com (Gang He, Siqi Wang, Kepeng Xu, Lin Zhang)</author>
      <guid isPermaLink="false">2505.07322v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes</title>
      <link>http://arxiv.org/abs/2505.07315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FederatedInvariant Features Learning (FedIFL)的联邦跨域诊断框架，用于解决工业数据稀缺导致的故障诊断模型训练难题。&lt;h4&gt;背景&lt;/h4&gt;由于工业数据稀缺，尤其是对于初创企业，独立训练全面的故障诊断模型存在困难；联邦学习能够在保证数据隐私的同时实现协同训练，因此成为一个理想的解决方案。&lt;h4&gt;目的&lt;/h4&gt;为了解决联邦诊断场景中标签空间不一致导致的问题，提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FedIFL框架通过原型对比学习减轻客户端域偏移，同时确保本地模型能够以隐私友好的方式访问其他客户端的分布。此外，引入特征解耦机制来减轻跨客户端域偏移，并设计实例级联邦实例一致性损失来保证不同客户端之间不变特征的实例级一致性。还构建了联邦实例个性化损失和正交损失来区分特定特征与不变特征。&lt;h4&gt;主要发现&lt;/h4&gt;FedIFL框架在全局标签空间中实现了良好的泛化能力，能够在标签空间不一致的情况下为目标客户端的电动机驱动系统（MDS）提供准确的故障诊断。&lt;h4&gt;结论&lt;/h4&gt;FedIFL在联邦跨域诊断中表现出色，能够有效解决不一致故障模式的问题。&lt;h4&gt;翻译&lt;/h4&gt;Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focusing on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the scarcity of industrial data, individual equipment users,particularly start-ups, struggle to independently train a comprehensive faultdiagnosis model; federated learning enables collaborative training whileensuring data privacy, making it an ideal solution. However, the diversity ofworking conditions leads to variations in fault modes, resulting ininconsistent label spaces across different clients. In federated diagnosticscenarios, label space inconsistency leads to local models focus onclient-specific fault modes and causes local models from different clients tomap different failure modes to similar feature representations, which weakensthe aggregated global model's generalization. To tackle this issue, thisarticle proposed a federated cross-domain diagnostic framework termed FederatedInvariant Features Learning (FedIFL). In intra-client training, prototypecontrastive learning mitigates intra-client domain shifts, subsequently,feature generating ensures local models can access distributions of otherclients in a privacy-friendly manner. Besides, in cross-client training, afeature disentanglement mechanism is introduced to mitigate cross-client domainshifts, specifically, an instance-level federated instance consistency loss isdesigned to ensure the instance-level consistency of invariant features betweendifferent clients, furthermore, a federated instance personalization loss andan orthogonal loss are constructed to distinguish specific features that fromthe invariant features. Eventually, the aggregated model achieves promisinggeneralization among global label spaces, enabling accurate fault diagnosis fortarget clients' Motor Driven Systems (MDSs) with inconsistent label spaces.Experiments on real-world MDSs validate the effectiveness and superiority ofFedIFL in federated cross-domain diagnosis with inconsistent fault modes.</description>
      <author>example@mail.com (Zexiao Wang, Yankai Wang, Xiaoqiang Liao, Xinguo Ming, Weiming Shen)</author>
      <guid isPermaLink="false">2505.07315v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Chronocept: Instilling a Sense of Time in Machines</title>
      <link>http://arxiv.org/abs/2505.07637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 8 figures, 18 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。&lt;h4&gt;背景&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Chronocept，作为第一个将时间有效性建模为时间上的连续概率分布的基准。&lt;h4&gt;方法&lt;/h4&gt;Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;h4&gt;结论&lt;/h4&gt;Chronocept通过提供对时间有效性的建模，为人工智能在时间推理方面的发展提供了新的可能性，并促进了相关领域的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human cognition is deeply intertwined with a sense of time, known asChronoception. This sense allows us to judge how long facts remain valid andwhen knowledge becomes outdated. Despite progress in vision, language, andmotor control, AI still struggles to reason about temporal validity. Weintroduce Chronocept, the first benchmark to model temporal validity as acontinuous probability distribution over time. Using skew-normal curves fittedalong semantically decomposed temporal axes, Chronocept captures nuancedpatterns of emergence, decay, and peak relevance. It includes two datasets:Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).Annotations show strong inter-annotator agreement (84% and 89%). Our baselinespredict curve parameters - location, scale, and skewness - enablinginterpretable, generalizable learning and outperforming classification-basedapproaches. Chronocept fills a foundational gap in AI's temporal reasoning,supporting applications in knowledge grounding, fact-checking,retrieval-augmented generation (RAG), and proactive agents. Code and data arepublicly available.</description>
      <author>example@mail.com (Krish Goel, Sanskar Pandey, KS Mahadevan, Harsh Kumar, Vishesh Khadaria)</author>
      <guid isPermaLink="false">2505.07637v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures</title>
      <link>http://arxiv.org/abs/2505.07070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了神经网络语言模型在训练用于预测下一个标记时如何获取语言结构。通过分析随机层次模型（RHM）生成的合成数据集，得出了神经网络性能的理论扩展规律。&lt;h4&gt;背景&lt;/h4&gt;研究者已经开发了一种基于数据相关性的表示学习理论，该理论解释了深度学习模型如何按层次结构逐层捕捉数据。&lt;h4&gt;目的&lt;/h4&gt;研究目的是扩展理论框架以考虑架构差异，并预测和验证卷积网络和Transformer模型在性能扩展方面的差异。&lt;h4&gt;方法&lt;/h4&gt;研究者通过随机层次模型（RHM）生成合成数据集，并比较了卷积网络和Transformer模型在这些数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，卷积网络由于其结构通过局部性和权重共享与生成过程对齐，其性能扩展速度比依赖全局自注意力机制的Transformer模型更快。&lt;h4&gt;结论&lt;/h4&gt;这一发现阐明了神经网络扩展规律背后的架构偏差，并强调了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;h4&gt;翻译&lt;/h4&gt;摘要：神经网络语言模型在训练用于预测下一个标记时如何获取语言结构？我们通过推导神经网络在由随机层次模型（RHM）生成的合成数据集上的性能理论扩展规律来回答这个问题。RHM是一组概率上下文无关文法集合，旨在捕获自然语言的层次结构，同时保持可分析性。此前，我们已开发了一种基于数据相关性的表示学习理论，解释了深度学习模型如何按层次结构逐层捕捉数据。在这里，我们将我们的理论框架扩展以考虑架构差异。特别是，我们预测并经验性地验证了卷积网络（其结构通过局部性和权重共享与生成过程对齐）的性能扩展速度比依赖全局自注意力机制的Transformer模型更快。这一发现阐明了神经网络扩展规律背后的架构偏差，并突出了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How do neural language models acquire a language's structure when trained fornext-token prediction? We address this question by deriving theoretical scalinglaws for neural network performance on synthetic datasets generated by theRandom Hierarchy Model (RHM) -- an ensemble of probabilistic context-freegrammars designed to capture the hierarchical structure of natural languagewhile remaining analytically tractable. Previously, we developed a theory ofrepresentation learning based on data correlations that explains how deeplearning models capture the hierarchical structure of the data sequentially,one layer at a time. Here, we extend our theoretical framework to account forarchitectural differences. In particular, we predict and empirically validatethat convolutional networks, whose structure aligns with that of the generativeprocess through locality and weight sharing, enjoy a faster scaling ofperformance compared to transformer models, which rely on global self-attentionmechanisms. This finding clarifies the architectural biases underlying neuralscaling laws and highlights how representation learning is shaped by theinteraction between model architecture and the statistical properties of data.</description>
      <author>example@mail.com (Francesco Cagnetta, Alessandro Favero, Antonio Sclocchi, Matthieu Wyart)</author>
      <guid isPermaLink="false">2505.07070v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Is MixIT Really Unsuitable for Correlated Sources? Exploring MixIT for Unsupervised Pre-training in Music Source Separation</title>
      <link>http://arxiv.org/abs/2505.07631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于MixIT的预训练在音乐源分离（MSS）中的应用，探讨了MixIT在MSS中的潜力和改进方法。&lt;h4&gt;背景&lt;/h4&gt;音乐源分离是一个高成本的过程，因此利用未标记数据进行预训练是一个有前景的方法。尽管MixIT这类无监督学习方法在一般声音分离中被探索，但在MSS中由于其隐含的源独立性假设而被忽视。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过MixIT预训练来提高MSS的性能，并探索MixIT在MSS中的潜力。&lt;h4&gt;方法&lt;/h4&gt;首先，使用MixIT在未经标记的野外部数据上进行模型预训练，然后在使用MUSDB18数据集进行监督的情况下进行微调。使用band-split TF-Locoformer模型进行实验，这是一种最先进MSS模型之一。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，尽管MixIT不假设任何源模型并且处理模糊性有困难，但它仍能在一定程度上分离乐器，显示出其在无监督预训练中的潜力。&lt;h4&gt;结论&lt;/h4&gt;MixIT预训练可以提高MSS的性能，优于从头开始训练的方法。&lt;h4&gt;翻译&lt;/h4&gt;In music source separation (MSS), obtaining isolated sources or stems is highly costly, making pre-training on unlabeled data a promising approach. Although source-agnostic unsupervised learning like mixture-invariant training (MixIT) has been explored in general sound separation, they have been largely overlooked in MSS due to its implicit assumption of source independence. We hypothesize, however, that the difficulty of applying MixIT to MSS arises from the ill-posed nature of MSS itself, where stem definitions are application-dependent and models lack explicit knowledge of what should or should not be separated, rather than from high inter-source correlation. While MixIT does not assume any source model and struggles with such ambiguities, our preliminary experiments show that it can still separate instruments to some extent, suggesting its potential for unsupervised pre-training. Motivated by these insights, this study investigates MixIT-based pre-training for MSS. We first pre-train a model on in-the-wild, unlabeled data from the Free Music Archive using MixIT, and then fine-tune it on MUSDB18 with supervision. Using the band-split TF-Locoformer, one of the state-of-the-art MSS models, we demonstrate that MixIT-based pre-training improves the performance over training from scratch.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In music source separation (MSS), obtaining isolated sources or stems ishighly costly, making pre-training on unlabeled data a promising approach.Although source-agnostic unsupervised learning like mixture-invariant training(MixIT) has been explored in general sound separation, they have been largelyoverlooked in MSS due to its implicit assumption of source independence. Wehypothesize, however, that the difficulty of applying MixIT to MSS arises fromthe ill-posed nature of MSS itself, where stem definitions areapplication-dependent and models lack explicit knowledge of what should orshould not be separated, rather than from high inter-source correlation. WhileMixIT does not assume any source model and struggles with such ambiguities, ourpreliminary experiments show that it can still separate instruments to someextent, suggesting its potential for unsupervised pre-training. Motivated bythese insights, this study investigates MixIT-based pre-training for MSS. Wefirst pre-train a model on in-the-wild, unlabeled data from the Free MusicArchive using MixIT, and then fine-tune it on MUSDB18 with supervision. Usingthe band-split TF-Locoformer, one of the state-of-the-art MSS models, wedemonstrate that MixIT-based pre-training improves the performance overtraining from scratch.</description>
      <author>example@mail.com (Kohei Saijo, Yoshiaki Bando)</author>
      <guid isPermaLink="false">2505.07631v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用自动检索合成CAD模型的方法，生成高质量的3D标注数据，用于训练深度学习模型，从而提高模型性能并降低标注成本。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成准确的3D标注数据对深度学习模型的发展构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用自动检索合成CAD模型的方法来生成高质量的3D标注数据，并验证这种方法在训练深度学习模型中的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程，应用于ScanNet++ v1数据集，以生成自动标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，使用自动获得的标注数据训练的深度学习模型不仅可行，而且性能优于使用手动标注数据训练的模型。验证了该方法在点云补全和单视图CAD模型检索与对齐两个任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注有潜力提高模型性能，同时显著降低标注成本，并将发布相关标注数据和训练模型以支持未来的3D场景理解研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高级3D场景理解在许多应用中至关重要。然而，生成准确3D标注的挑战使得深度学习模型的发展变得困难。我们转向最近在自动检索合成CAD模型方面的进展，并表明这种方法生成的数据可以用作训练监督深度学习模型的高质量真实标签。更具体地说，我们采用了与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程。这次，我们将它应用于之前缺乏此类标注的ScanNet++ v1数据集。我们的发现表明，不仅可以在这些自动获得的标注数据上训练深度学习模型，而且所得到的模型在性能上优于在手动标注数据上训练的模型。我们在两个不同的任务上验证了这一点：点云补全和单视图CAD模型检索与对齐。我们的结果强调了自动3D标注在提高模型性能的同时显著降低标注成本的可能性。为了支持3D场景理解的未来研究，我们将发布我们的标注，我们称之为SCANnotate++，以及我们的训练模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Wireless Link Scheduling with State-Augmented Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模无线自组织网络中的最优链路调度问题，旨在在保证链路传输公平性的前提下，最大化长期平均性能。&lt;h4&gt;背景&lt;/h4&gt;针对无线自组织网络中链路调度的问题，本文提出了一种基于图神经网络的优化调度策略。&lt;h4&gt;目的&lt;/h4&gt;目标是实现链路调度的长期平均性能最大化，同时确保每个链路的最小传输需求，以保证公平性。&lt;h4&gt;方法&lt;/h4&gt;利用图结构来表示链路冲突，构建了一个受约束的优化问题，并通过图神经网络（GNN）参数化调度策略。使用状态增强技术应对长期性能的挑战，通过将拉格朗日对偶变量作为调度策略的动态输入，训练GNN逐渐调整调度决策以实现最小传输需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟验证了所提策略的有效性，并在各种网络设置中将其性能与多个基线进行了比较。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于GNN的链路调度策略在保证公平性的同时，实现了长期平均性能的最大化，具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term average performance, subject to a minimum transmission requirement for each link to ensure fairness. With a graph structure utilized to represent the conflicts of links, we formulate a constrained optimization problem to learn the scheduling policy, which is parameterized with a graph neural network (GNN). To address the challenge of long-term performance, we use the state-augmentation technique. In particular, by augmenting the Lagrangian dual variables as dynamic inputs to the scheduling policy, the GNN can be trained to gradually adapt the scheduling decisions to achieve the minimum transmission requirements. We verify the efficacy of our proposed policy through numerical simulations and compare its performance with several baselines in various network settings.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term averageperformance, subject to a minimum transmission requirement for each link toensure fairness. With a graph structure utilized to represent the conflicts oflinks, we formulate a constrained optimization problem to learn the schedulingpolicy, which is parameterized with a graph neural network (GNN). To addressthe challenge of long-term performance, we use the state-augmentationtechnique. In particular, by augmenting the Lagrangian dual variables asdynamic inputs to the scheduling policy, the GNN can be trained to graduallyadapt the scheduling decisions to achieve the minimum transmissionrequirements. We verify the efficacy of our proposed policy through numericalsimulations and compare its performance with several baselines in variousnetwork settings.</description>
      <author>example@mail.com (Romina Garcia Camargo, Zhiyang Wang, Navid NaderiAlizadeh, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2505.07598v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种用于单目3D目标检测的深度估计方法，通过链式预测（CoP）来提高深度估计的准确性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;3D属性预测对于单目3D目标检测至关重要，其中深度估计是最具挑战性的部分，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP方法，通过条件预测和特征链式传播来提高深度估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用以下设计：使用轻量级属性网络（AN）学习每个3D属性的特征；构建显式的特征传播链；使用残差连接聚合特征，确保后续属性预测基于所有已处理的属性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MonoCoP在KITTI排行榜上达到了最先进的性能，并在Waymo和nuScenes frontal数据集上超过了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP是一种有效的单目3D目标检测方法，能够显著提高深度估计的准确性，且无需额外数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
      <link>http://arxiv.org/abs/2505.07576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种在半导体制造领域应用的视觉异常检测（VAD）方法，通过利用MIIC数据集建立了一个基准，验证了现代VAD方法在该领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是一个复杂的多阶段过程，自动视觉检测对于减少设备停机时间和控制成本至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需大量异常标记样本的VAD方法，避免昂贵的缺陷收集阶段，同时提供预测的解释。&lt;h4&gt;方法&lt;/h4&gt;通过利用MIIC数据集，建立了一个VAD在半导体领域的基准，并展示了现代VAD方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;现代VAD方法在半导体领域显示出良好的效果。&lt;h4&gt;结论&lt;/h4&gt;VAD方法在半导体制造领域具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is a complex, multistage process. Automatedvisual inspection of Scanning Electron Microscope (SEM) images is indispensablefor minimizing equipment downtime and containing costs. Most previous researchconsiders supervised approaches, assuming a sufficient number of anomalouslylabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emergingresearch domain, focuses on unsupervised learning, avoiding the costly defectcollection phase while providing explanations of the predictions. We introducea benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.Our results demonstrate the efficacy of modern VAD approaches in this field.</description>
      <author>example@mail.com (Manuel Barusco, Francesco Borsatti, Youssef Ben Khalifa, Davide Dalle Pezze, Gian Antonio Susto)</author>
      <guid isPermaLink="false">2505.07576v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
      <link>http://arxiv.org/abs/2505.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a long paper at the Educational Data Mining (EDM)  Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了教师课堂中视觉注意力的分布对学生学习参与度、成就和专业教师培训的重要影响，并提出了一种自动化处理流程，利用先进的人脸检测和识别技术来减少手动标注数据的需求。&lt;h4&gt;背景&lt;/h4&gt;虽然教师对学生的关注点对学生学习有重要影响，但推断教师关注的位置和学生不是一件容易的事。移动眼动追踪可以提供帮助，但需要大量手动标注。&lt;h4&gt;目的&lt;/h4&gt;减少手动标注数据，以识别教师关注的特定学生。&lt;h4&gt;方法&lt;/h4&gt;使用最先进的面部检测模型和特征嵌入训练面部识别模型，结合移动眼动追踪数据，并在课堂环境中进行迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的课堂环境中评估了该方法，结果表明在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。&lt;h4&gt;结论&lt;/h4&gt;该方法可以在不要求大量手动标注数据的情况下，以非侵入的方式处理教师的视觉注意力，有助于改进教学策略、增强课堂管理和提供专业教师发展的反馈。&lt;h4&gt;翻译&lt;/h4&gt;摘要：教师对课堂学生的视觉注意力和其分布对学生参与度、成就以及专业教师培训具有重要作用。尽管如此，推断教师关注的位置和对象并非易事。移动眼动追踪可以提供重要帮助，但仅使用移动眼动追踪需要大量的手动标注。为了解决这一局限性，我们提出了一种自动化处理流程的概念，该流程需要最少的手动标注数据来识别教师关注的特定学生。为此，我们利用最先进的面部检测模型和面部识别特征嵌入，在课堂环境中进行迁移学习，以训练面部识别模型，并将这些模型与来自移动眼动追踪器的教师的注视结合。我们使用从四个不同课堂收集的数据评估了我们的方法，结果表明，虽然在我们的所有课堂设置中都可以以合理的表现估计视觉关注的学生，但在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。虽然我们没有评估我们的方法在师生互动中的应用，并且专注于技术方法的合理性，但鉴于我们的方法不需要大量的手动标注数据，并以非侵入的方式处理教师的视觉注意力，它可以帮助改进教学策略，增强课堂管理，并为专业教师发展提供反馈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teachers' visual attention and its distribution across the students inclassrooms can constitute important implications for student engagement,achievement, and professional teacher training. Despite that, inferring theinformation about where and which student teachers focus on is not trivial.Mobile eye tracking can provide vital help to solve this issue; however, theuse of mobile eye tracking alone requires a significant amount of manualannotations. To address this limitation, we present an automated processingpipeline concept that requires minimal manually annotated data to recognizewhich student the teachers focus on. To this end, we utilize state-of-the-artface detection models and face recognition feature embeddings to train facerecognition models with transfer learning in the classroom context and combinethese models with the teachers' gaze from mobile eye trackers. We evaluated ourapproach with data collected from four different classrooms, and our resultsshow that while it is possible to estimate the visually focused students withreasonable performance in all of our classroom setups, U-shaped and smallclassrooms led to the best results with accuracies of approximately 0.7 and0.9, respectively. While we did not evaluate our method for teacher-studentinteractions and focused on the validity of the technical approach, as ourmethodology does not require a vast amount of manually annotated data andoffers a non-intrusive way of handling teachers' visual attention, it couldhelp improve instructional strategies, enhance classroom management, andprovide feedback for professional teacher development.</description>
      <author>example@mail.com (Efe Bozkir, Christian Kosel, Tina Seidel, Enkelejda Kasneci)</author>
      <guid isPermaLink="false">2505.07552v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling</title>
      <link>http://arxiv.org/abs/2505.07157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，该架构利用大型语言模型（LLMs）来解决传统主题模型在处理语境细微差别、多义词和罕见词时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型在处理语境细微差别、多义词和罕见词时存在困难，导致生成的主题缺乏连贯性和质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法，通过使用LLMs生成初始主题，并通过神经网络增强语义融合来精炼这些主题嵌入。&lt;h4&gt;方法&lt;/h4&gt;使用BERT和图神经网络（GNN）进行主题嵌入的精炼，并引入了一种新的计算相似性的方法，以进一步精炼主题嵌入并提取前k个主题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用两个医疗数据集（一个英语和一个法语）的六个集合，HAMLET在主题建模方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;HAMLET通过结合LLMs、BERT、SBERT和GNN等技术，能够有效地提高主题建模的质量和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的主题模型往往难以处理语境细微差别，无法充分处理多义词和罕见词。这种限制通常会导致缺乏连贯性和质量的主题。大型语言模型（LLMs）可以通过生成一组初始主题来缓解这个问题。然而，这些原始主题通常缺乏精炼和代表性，导致冗余且缺乏词汇相似性，以及可解释性降低。本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，它使用LLMs。所提出的方法利用神经网络增强语义融合来精炼LLM生成的主题嵌入。这种方法不依赖于仅统计共现或人类解释来从文档语料库中提取主题，而是引入了一种主题嵌入精炼方法，该方法使用双向编码器表示从Transformer（BERT）和图神经网络（GNN）。在主题生成后，采用BERT和Sentence-BERT（SBERT）相结合的混合技术进行嵌入。使用GNN进一步精炼主题表示，GNN在文档、主题、单词、相似主题和相似单词之间建立连接。引入了一种新的计算相似性的方法。因此，精炼了主题嵌入，并提取了前k个主题。使用两个医疗数据集（一个英语和一个法语）的六个集合进行了实验。结果表明，HAMLET在主题建模方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models often struggle with contextual nuances and fail toadequately handle polysemy and rare words. This limitation typically results intopics that lack coherence and quality. Large Language Models (LLMs) canmitigate this issue by generating an initial set of topics. However, these rawtopics frequently lack refinement and representativeness, which leads toredundancy without lexical similarity and reduced interpretability. This paperintroduces HAMLET, a graph-driven architecture for cross-lingual healthcaretopic modeling that uses LLMs. The proposed approach leverages neural-enhancedsemantic fusion to refine the embeddings of topics generated by the LLM.Instead of relying solely on statistical co-occurrence or human interpretationto extract topics from a document corpus, this method introduces a topicembedding refinement that uses Bidirectional Encoder Representations fromTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, ahybrid technique that involves BERT and Sentence-BERT (SBERT) is employed forembedding. The topic representations are further refined using a GNN, whichestablishes connections between documents, topics, words, similar topics, andsimilar words. A novel method is introduced to compute similarities.Consequently, the topic embeddings are refined, and the top k topics areextracted. Experiments were conducted using two healthcare datasets, one inEnglish and one in French, from which six sets were derived. The resultsdemonstrate the effectiveness of HAMLET.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2505.07157v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Seed1.5-VL Technical Report</title>
      <link>http://arxiv.org/abs/2505.07062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Seed1.5-VL，这是一个用于提升通用多模态理解和推理能力的视觉语言基础模型。&lt;h4&gt;背景&lt;/h4&gt;Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。&lt;h4&gt;目的&lt;/h4&gt;Seed1.5-VL旨在在公共VLM基准测试和内部评估套件中提供强大性能，并超越现有的多模态系统。&lt;h4&gt;方法&lt;/h4&gt;模型设计、数据构建和训练过程中的经验被综合回顾。&lt;h4&gt;主要发现&lt;/h4&gt;Seed1.5-VL在60个公共基准测试中的38个上达到了最先进的性能，并在GUI控制和游戏等以代理为中心的任务中优于OpenAICUA和Claude 3.7。&lt;h4&gt;结论&lt;/h4&gt;Seed1.5-VL的推理能力使其特别适用于多模态推理挑战，如视觉谜题，并有望在多个任务中应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Seed1.5-VL，一个旨在提升通用多模态理解和推理能力的视觉语言基础模型。Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。尽管其架构相对紧凑，但它能在广泛的公共VLM基准测试和内部评估套件中提供强大性能，在60个公共基准测试中的38个上达到了最先进的性能。此外，在以代理为中心的任务，如GUI控制和游戏玩法中，Seed1.5-VL超越了包括OpenAICUA和Claude 3.7在内的领先的多模态系统。除了视觉和视频理解之外，它还表现出强大的推理能力，使其特别适用于多模态推理挑战，如视觉谜题。我们相信这些能力将使它在多个任务中具有更广泛的应用。在本报告中，我们主要提供了在模型设计、数据构建和训练各阶段构建Seed1.5-VL的经验的综合回顾，希望这份报告能够激发进一步的研究。Seed1.5-VL现在可通过https://www.volcengine.com/（火山引擎模型ID：doubao-1-5-thinking-vision-pro-250428）获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Seed1.5-VL, a vision-language foundation model designed to advancegeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composedwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20Bactive parameters. Despite its relatively compact architecture, it deliversstrong performance across a wide spectrum of public VLM benchmarks and internalevaluation suites, achieving the state-of-the-art performance on 38 out of 60public benchmarks. Moreover, in agent-centric tasks such as GUI control andgameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAICUA and Claude 3.7. Beyond visual and video understanding, it also demonstratesstrong reasoning abilities, making it particularly effective for multimodalreasoning challenges such as visual puzzles. We believe these capabilities willempower broader applications across diverse tasks. In this report, we mainlyprovide a comprehensive review of our experiences in building Seed1.5-VL acrossmodel design, data construction, and training at various stages, hoping thatthis report can inspire further research. Seed1.5-VL is now accessible athttps://www.volcengine.com/ (Volcano Engine Model ID:doubao-1-5-thinking-vision-pro-250428)</description>
      <author>example@mail.com (Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, Zuquan Song)</author>
      <guid isPermaLink="false">2505.07062v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition</title>
      <link>http://arxiv.org/abs/2505.07166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in SIGIR-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。&lt;h4&gt;背景&lt;/h4&gt;密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。&lt;h4&gt;目的&lt;/h4&gt;重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。&lt;h4&gt;方法&lt;/h4&gt;测试了不同的表示方法（比较使用CLS标记与平均池化）、骨干架构（仅编码器的BERT与仅解码器的LLaMA）和额外的数据集（MSMARCO和Natural Questions）。&lt;h4&gt;主要发现&lt;/h4&gt;在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化（Contriever）和解码器基于（LLaMA）模型中。&lt;h4&gt;结论&lt;/h4&gt;确保了研究的可重复性，并将实现公开提供。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。背景是密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。研究目的是重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。研究方法包括测试不同的表示方法、骨干架构和额外的数据集。主要发现是在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化和解码器基于模型中。结论是确保了研究的可重复性，并将实现公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ielab/denseretriever-knowledge-acquisition&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense retrievers utilize pre-trained backbone language models (e.g., BERT,LLaMA) that are fine-tuned via contrastive learning to perform the task ofencoding text into sense representations that can be then compared via ashallow similarity operation, e.g. inner product. Recent research hasquestioned the role of fine-tuning vs. that of pre-training within denseretrievers, specifically arguing that retrieval knowledge is primarily gainedduring pre-training, meaning knowledge not acquired during pre-training cannotbe sub-sequentially acquired via fine-tuning. We revisit this idea here as theclaim was only studied in the context of a BERT-based encoder using DPR asrepresentative dense retriever. We extend the previous analysis by testingother representation approaches (comparing the use of CLS tokens with that ofmean pooling), backbone architectures (encoder-only BERT vs. decoder-onlyLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Ourstudy confirms that in DPR tuning, pre-trained knowledge underpins retrievalperformance, with fine-tuning primarily adjusting neuron activation rather thanreorganizing knowledge. However, this pattern does not hold universally, suchas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure fullreproducibility and make our implementation publicly available athttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition.</description>
      <author>example@mail.com (Zheng Yao, Shuai Wang, Guido Zuccon)</author>
      <guid isPermaLink="false">2505.07166v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis</title>
      <link>http://arxiv.org/abs/2505.07487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LinuxData，一个涵盖多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。&lt;h4&gt;背景&lt;/h4&gt;配置Linux内核以满足特定要求（如二进制大小）非常具有挑战性，因为内核选项众多且版本间快速变化。&lt;h4&gt;目的&lt;/h4&gt;为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。&lt;h4&gt;方法&lt;/h4&gt;LinuxData通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;LinuxData通过OpenML平台易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;h4&gt;翻译&lt;/h4&gt;本文提出LinuxData，一个包含多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。由于内核选项众多且版本间快速变化，配置Linux内核以满足特定要求（如二进制大小）具有很大挑战性。为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。该数据集通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。通过OpenML平台，LinuxData易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Configuring the Linux kernel to meet specific requirements, such as binarysize, is highly challenging due to its immense complexity-with over 15,000interdependent options evolving rapidly across different versions. Althoughseveral studies have explored sampling strategies and machine learning methodsto understand and predict the impact of configuration options, the literaturestill lacks a comprehensive and large-scale dataset encompassing multiplekernel versions along with detailed quantitative measurements. To bridge thisgap, we introduce LinuxData, an accessible collection of kernel configurationsspanning several kernel releases, specifically from versions 4.13 to 5.8. Thisdataset, gathered through automated tools and build processes, comprises over240,000 kernel configurations systematically labeled with compilation outcomesand binary sizes. By providing detailed records of configuration evolution andcapturing the intricate interplay among kernel options, our dataset enablesinnovative research in feature subset selection, prediction models based onmachine learning, and transfer learning across kernel versions. Throughout thispaper, we describe how the dataset has been made easily accessible via OpenMLand illustrate how it can be leveraged using only a few lines of Python code toevaluate AI-based techniques, such as supervised machine learning. Weanticipate that this dataset will significantly enhance reproducibility andfoster new insights into configuration-space analysis at a scale that presentsunique opportunities and inherent challenges, thereby advancing ourunderstanding of the Linux kernel's configurability and evolution.</description>
      <author>example@mail.com (Heraldo Borges, Juliana Alves Pereira, Djamel Eddine Khelladi, Mathieu Acher)</author>
      <guid isPermaLink="false">2505.07487v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
      <link>http://arxiv.org/abs/2505.06746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为M$^3$CAD的新颖基准测试，旨在推进通用合作式自动驾驶研究。&lt;h4&gt;背景&lt;/h4&gt;M$^3$CAD包含204个序列和30k个帧，涵盖了各种合作驾驶场景。&lt;h4&gt;目的&lt;/h4&gt;M$^3$CAD旨在支持包括目标检测与跟踪、地图构建、运动预测、占用预测和路径规划在内的多种自动驾驶任务。&lt;h4&gt;方法&lt;/h4&gt;M$^3$CAD采用多种传感模态，如激光雷达点云、RGB图像和GPS/IMU，以支持单车和多车自动驾驶研究。&lt;h4&gt;主要发现&lt;/h4&gt;M$^3$CAD是迄今为止针对合作多任务自动驾驶研究最全面的基准测试。&lt;h4&gt;结论&lt;/h4&gt;M$^3$CAD及其基线模型和评估结果被发布以支持鲁棒的合作自动驾驶系统的开发。&lt;h4&gt;翻译&lt;/h4&gt;We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce M$^3$CAD, a novel benchmark designed to advance research ingeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with30k frames, spanning a diverse range of cooperative driving scenarios. Eachsequence includes multiple vehicles and sensing modalities, e.g., LiDAR pointclouds, RGB images, and GPS/IMU, supporting a variety of autonomous drivingtasks, including object detection and tracking, mapping, motion forecasting,occupancy prediction, and path planning. This rich multimodal setup enablesM$^3$CAD to support both single-vehicle and multi-vehicle autonomous drivingresearch, significantly broadening the scope of research in the field. To ourknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailoredfor cooperative multi-task autonomous driving research. We evaluate thestate-of-the-art end-to-end solution on M$^3$CAD to establish baselineperformance. To foster cooperative autonomous driving research, we also proposeE2EC, a simple yet effective framework for cooperative driving solution thatleverages inter-vehicle shared information for improved path planning. Werelease M$^3$CAD, along with our baseline models and evaluation results, tosupport the development of robust cooperative autonomous driving systems. Allresources will be made publicly available on https://github.com/zhumorui/M3CAD</description>
      <author>example@mail.com (Morui Zhu, Yongqi Zhu, Yihao Zhu, Qi Chen, Deyuan Qu, Song Fu, Qing Yang)</author>
      <guid isPermaLink="false">2505.06746v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
      <link>http://arxiv.org/abs/2505.06903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CheXLearner是一个统一了解剖区域检测、基于黎曼流形的结构对齐和细粒度区域语义引导的端到端框架，用于时间医学图像分析。&lt;h4&gt;背景&lt;/h4&gt;现有的医学图像分析方法要么在粗粒度上对齐图像和文本，导致潜在的语义不匹配，要么仅依赖于视觉信息，缺乏医学语义整合。&lt;h4&gt;目的&lt;/h4&gt;提出CheXLearner，以解决现有方法的问题，实现图像和文本的精确对齐，并整合医学语义。&lt;h4&gt;方法&lt;/h4&gt;CheXLearner使用超曲几何来对齐解剖结构，并捕获时间胸片中的病理学意义差异。通过引入区域进展描述作为监督，实现跨模态表示学习，并支持动态低级别特征优化。&lt;h4&gt;主要发现&lt;/h4&gt;CheXLearner在解剖区域进展检测上达到81.12%的平均准确率和80.32%的F1分数，显著优于现有基准。在下游疾病分类中，模型达到91.52%的平均AUC分数。&lt;h4&gt;结论&lt;/h4&gt;CheXLearner在时间医学图像分析中实现了优异的性能，验证了其在特征表示方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal medical image analysis is essential for clinical decision-making,yet existing methods either align images and text at a coarse level - causingpotential semantic mismatches - or depend solely on visual information, lackingmedical semantic integration. We present CheXLearner, the first end-to-endframework that unifies anatomical region detection, Riemannian manifold-basedstructure alignment, and fine-grained regional semantic guidance. Our proposedMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry torobustly align anatomical structures and capture pathologically meaningfuldiscrepancies across temporal chest X-rays. By introducing regional progressiondescriptions as supervision, CheXLearner achieves enhanced cross-modalrepresentation learning and supports dynamic low-level feature optimization.Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and80.32% (+11.05%) F1-score on anatomical region progression detection -substantially outperforming state-of-the-art baselines, especially instructurally complex regions. Additionally, our model attains a 91.52% averageAUC score in downstream disease classification, validating its superior featurerepresentation.</description>
      <author>example@mail.com (Yuanzhuo Wang, Junwen Duan, Xinyu Li, Jianxin Wang)</author>
      <guid isPermaLink="false">2505.06903v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了图神经网络（GNNs）及其解释方法，提出了一个有效的算法COMRECGC来求解全局反事实解释中的共同补救方法问题，并通过实验证明了其性能优于其他算法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在社交网络、分子生物学、推荐系统等领域得到了广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;设计一个算法来求解全局反事实解释中的共同补救方法问题，并证明其性能优于其他算法。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同补救方法解释问题，并设计了COMRECGC算法来解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个不同的真实世界图数据集上进行了基准测试，表现优于其他算法。同时，共同补救方法解释与图反事实解释进行了比较，结果表明共同补救方法解释在药物发现或计算生物学等应用中具有可比性或优越性。&lt;h4&gt;结论&lt;/h4&gt;共同补救方法解释对于GNNs的全局反事实解释问题是一个有价值的解决方案，值得在药物发现或计算生物学等应用中进行考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML2025 (spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在持续预训练（CPT）过程中大型语言模型的学习动态，特别关注了通用性能和下游领域性能在每一步训练中的演变。&lt;h4&gt;背景&lt;/h4&gt;CPT已成为将强大基础模型应用于特定下游任务的流行且有效的方法。&lt;h4&gt;目的&lt;/h4&gt;研究CPT过程中学习动态，特别是通用和下游领域性能的变化。&lt;h4&gt;方法&lt;/h4&gt;通过验证损失来衡量领域性能，并观察到CPT损失曲线描述了从一条曲线到另一条隐藏曲线的过渡，该曲线可以由解耦分布偏移和学习率衰减效应来描述。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种CPT扩展定律，结合了分布偏移和学习率衰减两个因素，能够预测任何（持续）训练步骤和CPT中的学习率计划（LRS）下的损失。&lt;h4&gt;结论&lt;/h4&gt;该定律在多种CPT数据集和训练超参数下均有效，可以用于调整训练超参数，以平衡通用性能和特定领域性能。&lt;h4&gt;翻译&lt;/h4&gt;Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Pre-Training (CPT) has become a popular and effective method toapply strong foundation models to specific downstream tasks. In this work, weexplore the learning dynamics throughout the CPT process for large languagemodels. We specifically focus on how general and downstream domain performanceevolves at each training step, with domain performance measured via validationlosses. We have observed that the CPT loss curve fundamentally characterizesthe transition from one curve to another hidden curve, and could be describedby decoupling the effects of distribution shift and learning rate annealing. Wederive a CPT scaling law that combines the two factors, enabling the predictionof loss at any (continual) training steps and across learning rate schedules(LRS) in CPT. Our formulation presents a comprehensive understanding of severalcritical factors in CPT, including loss potential, peak learning rate, trainingsteps, replay ratio, etc. Moreover, our approach can be adapted to customizetraining hyper-parameters to different CPT goals such as balancing general anddomain-specific performance. Extensive experiments demonstrate that our scalinglaw holds across various CPT datasets and training hyper-parameters.</description>
      <author>example@mail.com (Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng)</author>
      <guid isPermaLink="false">2505.07796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
      <link>http://arxiv.org/abs/2505.06814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了M4IVQA挑战赛，旨在进一步推动多模态、多语言和多跳医疗教学问答系统的研究。&lt;h4&gt;背景&lt;/h4&gt;继NLPCC 2023 Foshan的CMIVQA和NLPCC 2024 Hangzhou的MMIVQA挑战赛成功举办后，今年引入了新的M4IVQA任务。&lt;h4&gt;目的&lt;/h4&gt;M4IVQA挑战赛专注于评估能够从医疗教学视频中整合信息、理解多种语言并回答需要跨模态推理的多跳问题的模型。&lt;h4&gt;方法&lt;/h4&gt;挑战赛包括三个赛道：多模态、多语言和多跳视频中的时序答案定位（M4TAGSV）、多模态、多语言和多跳视频语料库检索（M4VCR）和多模态、多语言和多跳视频语料库中的时序答案定位（M4TAGVC）。参与者需要开发能够处理视频和文本数据、理解多语言查询并为多跳医疗问题提供相关答案的算法。&lt;h4&gt;主要发现&lt;/h4&gt;M4IVQA挑战赛将推动多模态推理系统在医疗场景中的应用创新，最终有助于构建更智能的应急响应系统和在多语言社区中更有效的医学教育平台。&lt;h4&gt;结论&lt;/h4&gt;M4IVQA挑战赛有望促进多模态推理系统在医疗领域的创新，对医疗教育有重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has beenintroduced to further advance research in multi-modal, multilingual, andmulti-hop medical instructional question answering (M4IVQA) systems, with aspecific focus on medical instructional videos. The M4IVQA challenge focuses onevaluating models that integrate information from medical instructional videos,understand multiple languages, and answer multi-hop questions requiringreasoning over various modalities. This task consists of three tracks:multi-modal, multilingual, and multi-hop Temporal Answer Grounding in SingleVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video CorpusRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal AnswerGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected todevelop algorithms capable of processing both video and text data,understanding multilingual queries, and providing relevant answers to multi-hopmedical questions. We believe the newly introduced M4IVQA challenge will driveinnovations in multimodal reasoning systems for healthcare scenarios,ultimately contributing to smarter emergency response systems and moreeffective medical education platforms in multilingual communities. Our officialwebsite is https://cmivqa.github.io/</description>
      <author>example@mail.com (Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, Shoujun Zhou)</author>
      <guid isPermaLink="false">2505.06814v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Inference for Small Cohorts via Transfer Learning and Weighted Integration of Multiple Datasets</title>
      <link>http://arxiv.org/abs/2505.07153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了东北部美国地区肺部感染问题，指出国家eICU协作数据库中该地区患者数据不足，强调了数据的代表性不足。提出了一个新的加权方法TRANSLATE，用于整合来自不同来源的数据，以提高对感染结果的推断准确性。&lt;h4&gt;背景&lt;/h4&gt;东北部美国地区的肺部感染是一个重要问题，但国家eICU协作数据库中该地区患者数据不足，表明数据代表性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的加权方法，以解决小样本问题，并提高对肺部感染结果的推断准确性。&lt;h4&gt;方法&lt;/h4&gt;使用了一种名为TRANSLATE的新加权方法，通过学习权重将外部数据与目标队列对齐，从而整合来自不同来源的数据。&lt;h4&gt;主要发现&lt;/h4&gt;TRANSLATE方法在模拟和实际数据应用中提高了对肺部感染结果的推断准确性，同时考虑了区域异质性。&lt;h4&gt;结论&lt;/h4&gt;TRANSLATE方法为提高对肺部感染结果的推断提供了理论保证，并适用于多种统计估计，如均值、方差和分布函数。&lt;h4&gt;翻译&lt;/h4&gt;Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.</description>
      <author>example@mail.com (Subharup Guha, Mengqi Xu, Yi Li)</author>
      <guid isPermaLink="false">2505.07153v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
      <link>http://arxiv.org/abs/2505.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRACE的新方法，用于估计人类与场景接触的几何级别，通过结合点云编码器-解码器架构和层次特征提取与融合模块，实现了对3D人类几何结构与2D图像交互语义的有效整合，从而准确建模接触区域。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要依赖于参数化人类模型（如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系，但这种方法的泛化能力在不同的人类几何形状上受到限制。&lt;h4&gt;目的&lt;/h4&gt;提高人类与场景接触估计的准确性，并增强对不同人类几何形状的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;GRACE方法结合了点云编码器-解码器架构和层次特征提取与融合模块，通过视觉线索将几何特征映射到3D人类网格的顶点空间，从而实现接触区域的准确建模。&lt;h4&gt;主要发现&lt;/h4&gt;GRACE在多个基准数据集上的实验表明，其在接触估计方面达到了最先进的性能，并且其鲁棒性验证了其在非结构化人类点云上的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GRACE是一种有效且具有良好泛化能力的3D人类接触估计方法，适用于人类行为分析、具身人工智能和AR/VR等应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：估算人类-场景接触的几何级别旨在将特定的接触表面点定位在3D人类几何上，这提供了一个空间先验，并架起了人类与场景之间的桥梁，支持人类行为分析、具身人工智能和AR/VR等应用。为了完成这项任务，现有的方法主要依赖于参数化人类模型（例如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系。实际上，这种方法完成了从图像特征到有序序列的映射。然而，这种方法缺乏对几何形状的考虑，限制了其在不同人类几何形状上的泛化能力。在本文中，我们引入了GRACE（用于3D人类-场景接触估计的几何级别推理），这是一种新的3D人类接触估计范式。GRACE结合了点云编码器-解码器架构以及层次特征提取和融合模块，使得3D人类几何结构与从图像中提取的2D交互语义能够有效整合。在视觉线索的引导下，GRACE建立了从几何特征到3D人类网格顶点空间的隐式映射，从而实现了接触区域的准确建模。这种设计确保了高预测精度，并赋予了框架在多种人类几何形状上的强大泛化能力。在多个基准数据集上的大量实验表明，GRACE在接触估计方面达到了最先进的性能，进一步的结果进一步验证了其在非结构化人类点云上的鲁棒泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the geometry level of human-scene contact aims to ground specificcontact surface points at 3D human geometries, which provides a spatial priorand bridges the interaction between human and scene, supporting applicationssuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,existing approaches predominantly rely on parametric human models (e.g., SMPL),which establish correspondences between images and contact regions throughfixed SMPL vertex sequences. This actually completes the mapping from imagefeatures to an ordered sequence. However, this approach lacks consideration ofgeometry, limiting its generalizability in distinct human geometries. In thispaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene ContactEstimation), a new paradigm for 3D human contact estimation. GRACE incorporatesa point cloud encoder-decoder architecture along with a hierarchical featureextraction and fusion module, enabling the effective integration of 3D humangeometric structures with 2D interaction semantics derived from images. Guidedby visual cues, GRACE establishes an implicit mapping from geometric featuresto the vertex space of the 3D human mesh, thereby achieving accurate modelingof contact regions. This design ensures high prediction accuracy and endows theframework with strong generalization capability across diverse humangeometries. Extensive experiments on multiple benchmark datasets demonstratethat GRACE achieves state-of-the-art performance in contact estimation, withadditional results further validating its robust generalization to unstructuredhuman point clouds.</description>
      <author>example@mail.com (Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha)</author>
      <guid isPermaLink="false">2505.06575v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MarkMatch: Same-Hand Stuffing Detection</title>
      <link>http://arxiv.org/abs/2505.07032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。&lt;h4&gt;背景&lt;/h4&gt;与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。&lt;h4&gt;目的&lt;/h4&gt;提高在书写变化和视觉噪声下的泛化能力，并增强对真实匹配的高置信度。&lt;h4&gt;方法&lt;/h4&gt;模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;MarkMatch实现了0.943的F1分数，超过了BubbleSig的最佳性能。系统还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。&lt;h4&gt;结论&lt;/h4&gt;MarkMatch为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。我们的模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比，从而使模型能够学习细微的书写差异，并提高在书写变化和视觉噪声下的泛化能力，而斜对角监督则加强了真实匹配的高置信度。模型实现了0.943的F1分数，超过了BubbleSig的最佳性能。MarkMatch还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。该系统为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MarkMatch, a retrieval system for detecting whether two paperballot marks were filled by the same hand. Unlike the previous SOTA methodBubbleSig, which used binary classification on isolated mark pairs, MarkMatchranks stylistic similarity between a query mark and a mark in the databaseusing contrastive learning. Our model is trained with a dense batch similaritymatrix and a dual loss objective. Each sample is contrasted against manynegatives within each batch, enabling the model to learn subtle handwritingdifference and improve generalization under handwriting variation and visualnoise, while diagonal supervision reinforces high confidence on true matches.The model achieves an F1 score of 0.943, surpassing BubbleSig's bestperformance. MarkMatch also integrates Segment Anything Model for flexible markextraction via box- or point-based prompts. The system offers election auditorsa practical tool for visual, non-biometric investigation of suspicious ballots.</description>
      <author>example@mail.com (Fei Zhao, Runlin Zhang, Chengcui Zhang, Nitesh Saxena)</author>
      <guid isPermaLink="false">2505.07032v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
      <link>http://arxiv.org/abs/2505.06886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，发现两者在上下文（群体水平）和自下而上（单细胞水平）的映射存在显著相似性，并通过添加神经响应归一化层（NeuRN）进一步增强了这种相似性，提高了模型在现实世界任务中的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;小鼠是系统神经科学中研究最广泛的动物模型之一。了解小鼠视觉皮层中由各种自然场景刺激引起的神经表征的普遍模式是计算视觉领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;研究小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，并提出一种新的框架来比较两者之间的功能架构。&lt;h4&gt;方法&lt;/h4&gt;引入了一种通用的表征学习策略，并添加了受视觉皮层中兴奋性和抑制性神经元激活特征启发的NeuRN层。&lt;h4&gt;主要发现&lt;/h4&gt;发现小鼠视觉皮层的功能映射与高性能深度学习模型在上下文和自下而上的场景中存在显著相似性；NeuRN层的添加显著提高了深度学习模型在领域泛化任务中对数据变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;研究结果对从小鼠视觉皮层中获得灵感的先进AI模型的发展具有重要意义，表明这些模型可以作为研究小鼠视觉皮层神经表征的有价值工具，并因此提高它们在现实世界任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The mouse is one of the most studied animal models in the field of systemsneuroscience. Understanding the generalized patterns and decoding the neuralrepresentations that are evoked by the diverse range of natural scene stimuliin the mouse visual cortex is one of the key quests in computational vision. Inrecent years, significant parallels have been drawn between the primate visualcortex and hierarchical deep neural networks. However, their generalizedefficacy in understanding mouse vision has been limited. In this study, weinvestigate the functional alignment between the mouse visual cortex and deeplearning models for object classification tasks. We first introduce ageneralized representational learning strategy that uncovers a strikingresemblance between the functional mapping of the mouse visual cortex andhigh-performing deep learning models on both top-down (population-level) andbottom-up (single cell-level) scenarios. Next, this representational similarityacross the two systems is further enhanced by the addition of Neural ResponseNormalization (NeuRN) layer, inspired by the activation profile of excitatoryand inhibitory neurons in the visual cortex. To test the performance effect ofNeuRN on real-world tasks, we integrate it into deep learning models andobserve significant improvements in their robustness against data shifts indomain generalization tasks. Our work proposes a novel framework for comparingthe functional architecture of the mouse visual cortex with deep learningmodels. Our findings carry broad implications for the development of advancedAI models that draw inspiration from the mouse visual cortex, suggesting thatthese models serve as valuable tools for studying the neural representations ofthe mouse visual cortex and, as a result, enhancing their performance onreal-world tasks.</description>
      <author>example@mail.com (Ahmed Qazi, Hamd Jalil, Asim Iqbal)</author>
      <guid isPermaLink="false">2505.06886v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>BodyGPS: Anatomical Positioning System</title>
      <link>http://arxiv.org/abs/2505.07744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的基础模型，用于解析医学图像中的人类解剖结构，适用于不同的模态，支持监督或无监督训练，能够进行匹配、配准、分类或分割，可带或不带用户交互。&lt;h4&gt;背景&lt;/h4&gt;当前解析医学图像中的人类解剖结构需要针对不同模态的特定模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适用于不同模态、支持多种任务和交互方式的基础模型。&lt;h4&gt;方法&lt;/h4&gt;通过训练一个神经网络估计器，将查询位置映射到图谱坐标，通过回归实现。通过稀疏采样输入数据，提高效率，实现小于1毫秒的响应时间，无需额外的加速硬件。&lt;h4&gt;主要发现&lt;/h4&gt;该算法在CT和MRI模态中均表现出良好的实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法和模型在医学图像解析中具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a new type of foundational model for parsing human anatomy inmedical images that works for different modalities. It supports supervised orunsupervised training and can perform matching, registration, classification,or segmentation with or without user interaction. We achieve this by training aneural network estimator that maps query locations to atlas coordinates viaregression. Efficiency is improved by sparsely sampling the input, enablingresponse times of less than 1 ms without additional accelerator hardware. Wedemonstrate the utility of the algorithm in both CT and MRI modalities.</description>
      <author>example@mail.com (Halid Ziya Yerebakan, Kritika Iyer, Xueqi Guo, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez)</author>
      <guid isPermaLink="false">2505.07744v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
      <link>http://arxiv.org/abs/2505.06573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ElectricSight的系统，用于3D距离测量和监测电力传输线路潜在威胁，如大型起重机，以提高安全。&lt;h4&gt;背景&lt;/h4&gt;保护电力传输线路免受潜在危害是关键任务，其中之一是准确测量电力线路与潜在威胁之间的距离。现有的基于传感器的测量方法在平衡准确性和成本方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ElectricSight系统，旨在解决现有方法的局限性，实现低成本且精确的3D距离测量。&lt;h4&gt;方法&lt;/h4&gt;ElectricSight系统结合实时图像和环境影响点云先验信息，采用单目深度估计方法，将3D点云数据集成到基于图像的估计中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ElectricSight在距离测量上实现了平均精度1.08米，在早期预警上实现了92%的准确率。&lt;h4&gt;结论&lt;/h4&gt;ElectricSight系统通过其创新的设计和实施，提供了有效的解决方案，以解决电力传输线路安全监测中的距离测量问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protecting power transmission lines from potential hazards involves criticaltasks, one of which is the accurate measurement of distances between powerlines and potential threats, such as large cranes. The challenge with this taskis that the current sensor-based methods face challenges in balancing accuracyand cost in distance measurement. A common practice is to install cameras ontransmission towers, which, however, struggle to measure true 3D distances dueto the lack of depth information. Although 3D lasers can provide accurate depthdata, their high cost makes large-scale deployment impractical.  To address this challenge, we present ElectricSight, a system designed for 3Ddistance measurement and monitoring of potential hazards to power transmissionlines. This work's key innovations lie in both the overall system framework anda monocular depth estimation method. Specifically, the system frameworkcombines real-time images with environmental point cloud priors, enablingcost-effective and precise 3D distance measurements. As a core component of thesystem, the monocular depth estimation method enhances the performance byintegrating 3D point cloud data into image-based estimates, improving both theaccuracy and reliability of the system.  To assess ElectricSight's performance, we conducted tests with data from areal-world power transmission scenario. The experimental results demonstratethat ElectricSight achieves an average accuracy of 1.08 m for distancemeasurements and an early warning accuracy of 92%.</description>
      <author>example@mail.com (Xingchen Li, LiDian Wang, Yu Sheng, ZhiPeng Tang, Haojie Ren, Guoliang You, YiFan Duan, Jianmin Ji, Yanyong Zhang)</author>
      <guid isPermaLink="false">2505.06573v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
      <link>http://arxiv.org/abs/2505.06855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Multi-Masking Strategy (MMS)的文本识别方法，通过结合随机块状和跨度掩码，优化了自监督学习在文本识别任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界标注数据集的稀缺，大多数文本识别方法都是在大规模合成数据集上训练的。合成图像无法真实反映现实场景，如不均匀光照、不规则布局、遮挡和退化等问题，导致在处理复杂现实图像时性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了缩小现实世界和合成数据集之间的差距，本文旨在通过自监督学习技术提高文本识别的性能。&lt;h4&gt;方法&lt;/h4&gt;本文分析了原始的Masked AutoEncoder (MAE)并指出，随机块状掩码主要捕捉低级纹理特征，但忽略了高级上下文表示。为了充分利用高级上下文表示，我们引入了随机块状和跨度掩码。Multi-Masking Strategy (MMS)将随机块状和跨度掩码整合到MIM框架中，共同学习低级和高级文本表示。&lt;h4&gt;主要发现&lt;/h4&gt;经过与真实数据的微调，MMS在文本识别、分割和文本图像超分辨率等文本相关任务中优于现有的自监督方法。&lt;h4&gt;结论&lt;/h4&gt;MMS通过引入新的掩码策略，显著提高了自监督学习在文本识别任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing text recognition methods are trained on large-scale syntheticdatasets due to the scarcity of labeled real-world datasets. Synthetic images,however, cannot faithfully reproduce real-world scenarios, such as unevenillumination, irregular layout, occlusion, and degradation, resulting inperformance disparities when handling complex real-world images. Recentself-supervised learning techniques, notably contrastive learning and maskedimage modeling (MIM), narrow this domain gap by exploiting unlabeled real textimages. This study first analyzes the original Masked AutoEncoder (MAE) andobserves that random patch masking predominantly captures low-level texturalfeatures but misses high-level contextual representations. To fully exploit thehigh-level contextual representations, we introduce random blockwise and spanmasking in the text recognition task. These strategies can mask the continuousimage patches and completely remove some characters, forcing the model to inferrelationships among characters within a word. Our Multi-Masking Strategy (MMS)integrates random patch, blockwise, and span masking into the MIM frame, whichjointly learns low and high-level textual representations. After fine-tuningwith real data, MMS outperforms the state-of-the-art self-supervised methods invarious text-related tasks, including text recognition, segmentation, andtext-image super-resolution.</description>
      <author>example@mail.com (Zhengmi Tang, Yuto Mitsui, Tomo Miyazaki, Shinichiro Omachi)</author>
      <guid isPermaLink="false">2505.06855v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Survival Modeling in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2505.07683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过利用基础模型和病理报告文本信息提取，研究了训练经典的多模态生存模型的可行性，并展示了其在预测癌症生存方面的优势。&lt;h4&gt;背景&lt;/h4&gt;TCGA数据库通过其基因组学、临床和图像数据的整合，为癌症研究提供了大规模参考。基础模型在生物医学深度学习中用于提取有意义特征嵌入，而病理报告文本在TCGA数据库中虽存在，但长期以来未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;研究通过使用基础模型从零样本嵌入中训练经典的多模态生存模型的可行性。&lt;h4&gt;方法&lt;/h4&gt;通过分析TCGA数据，结合基础模型提取的零样本嵌入和病理报告文本，构建多模态生存模型，并与单模态模型进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现多模态融合模型易于实现且效果优于单模态模型，同时加入病理报告文本也有助于提高模型的预测性能。&lt;h4&gt;结论&lt;/h4&gt;利用基础模型和病理报告文本信息提取可以现代化生存建模，提高癌症生存预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as alarge-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models fromunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learningis the development of foundation models (FMs) to derive meaningful featureembeddings, agnostic to a specific modeling task. Biomedical text especiallyhas seen growing development of FMs. While TCGA contains free-text data aspathology reports, these have been historically underutilized. Here, weinvestigate the feasibility of training classical, multimodal survival modelsover zero-shot embeddings extracted by FMs. We show the ease and additiveeffect of multimodal fusion, outperforming unimodal models. We demonstrate thebenefit of including pathology report text and rigorously evaluate the effectof model-based text summarization and hallucination. Overall, we modernizesurvival modeling by leveraging FMs and information extraction from pathologyreports.</description>
      <author>example@mail.com (Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman)</author>
      <guid isPermaLink="false">2505.07683v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
      <link>http://arxiv.org/abs/2505.06710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于弱监督方案的多实例学习（MIL）特征提取器预训练方法，旨在改善MIL在病理图像分析中的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的MIL方法强调了特征聚合器的重要性，但很大程度上忽视了实例级别的表征学习。同时，这些方法假设可以直接利用或微调预训练的特征提取器，但这种情况并不总是成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种预训练MIL特征提取器的方法，通过传播弱标签到对应的实例来实现。&lt;h4&gt;方法&lt;/h4&gt;通过弱标签传播的方式预训练特征提取器，并深入研究了几个关键组件，包括强大的数据增强、非线性预测头和鲁棒的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在常见的病理图像数据集上，该方法在不同下游任务中取得了比其他预训练方案（如ImageNet预训练和自监督学习）更好的性能。该方法还显示出了兼容性和可扩展性，适用于病理特定模型的微调和多个数据集的预训练。&lt;h4&gt;结论&lt;/h4&gt;这是首个专注于MIL表征学习的研究工作，提出了有效的预训练方法，并展示了其在实际应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various multi-instance learning (MIL) based approaches have been developedand successfully applied to whole-slide pathological images (WSI). Existing MILmethods emphasize the importance of feature aggregators, but largely neglectthe instance-level representation learning. They assume that the availabilityof a pre-trained feature extractor can be directly utilized or fine-tuned,which is not always the case. This paper proposes to pre-train featureextractor for MIL via a weakly-supervised scheme, i.e., propagating the weakbag-level labels to the corresponding instances for supervised learning. Tolearn effective features for MIL, we further delve into several key components,including strong data augmentation, a non-linear prediction head and the robustloss function. We conduct experiments on common large-scale WSI datasets andfind it achieves better performance than other pre-training schemes (e.g.,ImageNet pre-training and self-supervised learning) in different downstreamtasks. We further show the compatibility and scalability of the proposed schemeby deploying it in fine-tuning the pathological-specific models andpre-training on merged multiple datasets. To our knowledge, this is the firstwork focusing on the representation learning for MIL.</description>
      <author>example@mail.com (Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu)</author>
      <guid isPermaLink="false">2505.06710v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NetSight: Graph Attention Based Traffic Forecasting in Computer Networks</title>
      <link>http://arxiv.org/abs/2505.07034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NetSight的新方法，用于预测网络中给定指标的值。NetSight能够同时学习全局和局部尺度的时空依赖关系，从而提高预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前网络交通受到节点交互和节点需求波动的影响，传统的统计预测方法已无法适应这种非线性动态时空依赖。&lt;h4&gt;目的&lt;/h4&gt;提出NetSight以解决传统方法在处理网络交通中的时空依赖关系方面的不足。&lt;h4&gt;方法&lt;/h4&gt;NetSight通过学习网络中各个节点的测量时间序列数据，同时考虑全局和局部尺度的时空依赖关系，实现网络指标的预测。&lt;h4&gt;主要发现&lt;/h4&gt;NetSight在两个大型真实网络的数据集上进行了广泛评估，结果显示其在预测准确性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;NetSight通过同时学习全局和局部尺度的时空依赖关系，提高了网络指标预测的准确性，为网络交通预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio-temporal dependencies present in today's network traffic. The most promising direction of research today is graph neural networks (GNNs) based prediction approaches that are naturally suited to handle graph-structured data. Unfortunately, the state-of-the-art GNN approaches separate the modeling of spatial and temporal information, resulting in the loss of important information about joint dependencies. These GNN based approaches further do not model information at both local and global scales simultaneously, leaving significant room for improvement. To address these challenges, we propose NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at both global and local scales from the time-series of measurements of any given network metric collected at various nodes in a network. Using the learned information, NetSight can then accurately predict the future values of the given network metric at those nodes in the network. We propose several new concepts and techniques in the design of NetSight, such as spatio-temporal adjacency matrix and node normalization. Through extensive evaluations and comparison with prior approaches using data from two large real-world networks, we show that NetSight significantly outperforms all prior state-of-the-art approaches. We will release the source code and data used in the evaluation of NetSight on the acceptance of this paper.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The traffic in today's networks is increasingly influenced by theinteractions among network nodes as well as by the temporal fluctuations in thedemands of the nodes. Traditional statistical prediction methods are becomingobsolete due to their inability to address the non-linear and dynamicspatio-temporal dependencies present in today's network traffic. The mostpromising direction of research today is graph neural networks (GNNs) basedprediction approaches that are naturally suited to handle graph-structureddata. Unfortunately, the state-of-the-art GNN approaches separate the modelingof spatial and temporal information, resulting in the loss of importantinformation about joint dependencies. These GNN based approaches further do notmodel information at both local and global scales simultaneously, leavingsignificant room for improvement. To address these challenges, we proposeNetSight. NetSight learns joint spatio-temporal dependencies simultaneously atboth global and local scales from the time-series of measurements of any givennetwork metric collected at various nodes in a network. Using the learnedinformation, NetSight can then accurately predict the future values of thegiven network metric at those nodes in the network. We propose several newconcepts and techniques in the design of NetSight, such as spatio-temporaladjacency matrix and node normalization. Through extensive evaluations andcomparison with prior approaches using data from two large real-world networks,we show that NetSight significantly outperforms all prior state-of-the-artapproaches. We will release the source code and data used in the evaluation ofNetSight on the acceptance of this paper.</description>
      <author>example@mail.com (Jinming Xing, Guoheng Sun, Hui Sun, Linchao Pan, Shakir Mahmood, Xuanhao Luo, Muhammad Shahzad)</author>
      <guid isPermaLink="false">2505.07034v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
      <link>http://arxiv.org/abs/2505.07019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCOLD的智能农业领域视觉-语言基础模型，用于叶病识别，通过结合图像和文本模态，解决了现有方法中模态整合不足和依赖预训练数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;叶病识别在智能农业中至关重要，但现有研究在整合图像和文本模态以及使用预训练数据集方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SCOLD模型，旨在解决农业任务中的模态整合和预训练数据集的局限性。&lt;h4&gt;方法&lt;/h4&gt;SCOLD模型使用超过186,000个植物叶片图像及其对应的症状描述进行训练，通过无任务预训练和软目标对比学习来提高模型的泛化能力和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SCOLD在零样本和少样本分类、图像-文本检索和图像分类等多个基准测试中优于现有视觉-语言模型，同时保持了有竞争力的参数规模。&lt;h4&gt;结论&lt;/h4&gt;SCOLD模型显著推进了农业视觉-语言基础模型的发展，以最小的或无需监督微调即可实现强大的性能，为未来研究长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：叶病识别在智能农业中发挥着关键作用。然而，许多现有研究仍然难以整合图像和文本模态以弥补彼此的局限性。此外，许多这些方法依赖于使用ImageNet等受限制的数据集进行预训练，这些数据集缺乏领域特定信息。我们提出了SCOLD（用于叶病识别的软目标对比学习），这是一种针对解决这些挑战的农业任务而定制的内容感知视觉-语言基础模型。SCOLD使用一个包含超过186,000个植物叶片图像及其对应症状描述的多样化语料库进行开发，这些图像-字幕对与97个独特概念相匹配。通过无任务预训练，SCOLD利用上下文软目标通过平滑标签来减轻对比学习中的过度自信，从而提高模型在细粒度分类任务上的泛化能力和鲁棒性。实验结果表明，SCOLD在多个基准测试中（包括零样本和少样本分类、图像-文本检索和图像分类）优于现有的视觉-语言模型（如OpenAI-CLIP-L、BioCLIP和SigLIP2），同时保持有竞争力的参数规模。消融研究进一步突出了SCOLD相对于其竞争对手的有效性。所提出的方法显著推进了农业视觉-语言基础模型，以最小的或无需监督微调即可实现强大的性能。这项工作为使用长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统的研究奠定了坚实的基础。本研究的代码可在https://huggingface.co/enalis/scold获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leaf disease identification plays a pivotal role in smart agriculture.However, many existing studies still struggle to integrate image and textualmodalities to compensate for each other's limitations. Furthermore, many ofthese approaches rely on pretraining with constrained datasets such asImageNet, which lack domain-specific information. We propose SCOLD (Soft-targetCOntrastive learning for Leaf Disease identification), a context-awarevision-language foundation model tailored to address these challenges foragricultural tasks. SCOLD is developed using a diverse corpus of plant leafimages and corresponding symptom descriptions, comprising over 186,000image-caption pairs aligned with 97 unique concepts. Through task-agnosticpretraining, SCOLD leverages contextual soft targets to mitigate overconfidencein contrastive learning by smoothing labels, thereby improving modelgeneralization and robustness on fine-grained classification tasks.Experimental results demonstrate that SCOLD outperforms existingvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 acrossseveral benchmarks, including zero-shot and few-shot classification, image-textretrieval, and image classification, while maintaining a competitive parameterfootprint. Ablation studies further highlight SCOLD's effectiveness in contrastto its counterparts. The proposed approach significantly advances theagricultural vision-language foundation model, offering strong performance withminimal or no supervised fine-tuning. This work lays a solid groundwork forfuture research on models trained with long-form and simplified contexts, tasksinvolving class ambiguity, and multi-modal systems for intelligent plantdisease diagnostics. The code for this study is available athttps://huggingface.co/enalis/scold</description>
      <author>example@mail.com (Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach)</author>
      <guid isPermaLink="false">2505.07019v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Class Distribution Mismatch</title>
      <link>http://arxiv.org/abs/2505.06948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UCDM的无监督学习方法，用于解决训练数据与目标任务类别分布不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;以往的方法在半监督场景下设计分类器，将未知或新类别归为“其他”类别，但过分依赖标记数据，限制了其适用性和性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出UCDM方法，通过从无标签数据中构建正负样本对来进行分类器训练。&lt;h4&gt;方法&lt;/h4&gt;UCDM方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，引入基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将其纳入训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的大量实验表明，UCDM方法优于以往半监督方法。在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，UCDM方法无需依赖标记数据，在分类已知、未知和新类别上分别超越了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;h4&gt;结论&lt;/h4&gt;UCDM方法有效地解决了类别分布不匹配问题，在无需大量标记数据的情况下，显著提高了分类器的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：类别分布不匹配（CDM）是指训练数据中的类别分布与目标任务之间的差异。先前的方法通过设计分类器来对训练期间已知的类别进行分类，将未知或新类别分组到“其他”类别中。然而，它们侧重于半监督场景，并且高度依赖标记数据，限制了它们的适用性和性能。为了解决这个问题，我们提出了无监督学习用于类别分布不匹配（UCDM），该方法从无标签数据中为分类器训练构建正负样本对。我们的方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，我们引入了一种基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将它们纳入训练过程。在三个数据集上的大量实验表明UCDM方法优于先前半监督方法。具体而言，在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，我们的方法，不依赖于标记数据，在分类已知、未知和新类别上分别超过了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Class distribution mismatch (CDM) refers to the discrepancy between classdistributions in training data and target tasks. Previous methods address thisby designing classifiers to categorize classes known during training, whilegrouping unknown or new classes into an "other" category. However, they focuson semi-supervised scenarios and heavily rely on labeled data, limiting theirapplicability and performance. To address this, we propose UnsupervisedLearning for Class Distribution Mismatch (UCDM), which constructspositive-negative pairs from unlabeled data for classifier training. Ourapproach randomly samples images and uses a diffusion model to add or erasesemantic classes, synthesizing diverse training pairs. Additionally, weintroduce a confidence-based labeling mechanism that iteratively assignspseudo-labels to valuable real-world data and incorporates them into thetraining process. Extensive experiments on three datasets demonstrate UCDM'ssuperiority over previous semi-supervised methods. Specifically, with a 60%mismatch proportion on Tiny-ImageNet dataset, our approach, without relying onlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,and 72.5% in classifying known, unknown, and new classes.</description>
      <author>example@mail.com (Pan Du, Wangbo Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You)</author>
      <guid isPermaLink="false">2505.06948v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics</title>
      <link>http://arxiv.org/abs/2505.06435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 30 figures. IEEE Transactions on Pattern Analysis and  Machine Intelligence (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对连续敏感属性的公平表示学习（FRL）算法，旨在解决现有FRL算法在处理连续敏感属性（如年龄或收入）时的局限性。&lt;h4&gt;背景&lt;/h4&gt;AI公平性，也称为算法公平性，旨在确保算法在操作过程中不对任何个人或群体产生偏见或歧视。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的FRL算法，使其能够应用于连续敏感属性。&lt;h4&gt;方法&lt;/h4&gt;引入了期望积分概率度量（EIPM）来评估连续敏感属性表示空间的公平性水平，并证明了低EIPM值的表示分布上的预测头构建将具有公平性。此外，EIPM可以通过有限样本的估计器进行准确估计。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FRL算法FREM（基于EIPM和MMD的公平表示）在实验中优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;FREM算法能够有效处理连续敏感属性，提高算法的公平性，并在实际应用中优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI fairness, also known as algorithmic fairness, aims to ensure thatalgorithms operate without bias or discrimination towards any individual orgroup. Among various AI algorithms, the Fair Representation Learning (FRL)approach has gained significant interest in recent years. However, existing FRLalgorithms have a limitation: they are primarily designed for categoricalsensitive attributes and thus cannot be applied to continuous sensitiveattributes, such as age or income. In this paper, we propose an FRL algorithmfor continuous sensitive attributes. First, we introduce a measure called theExpectation of Integral Probability Metrics (EIPM) to assess the fairness levelof representation space for continuous sensitive attributes. We demonstratethat if the distribution of the representation has a low EIPM value, then anyprediction head constructed on the top of the representation become fair,regardless of the selection of the prediction head. Furthermore, EIPM possessesa distinguished advantage in that it can be accurately estimated using ourproposed estimator with finite samples. Based on these properties, we propose anew FRL algorithm called Fair Representation using EIPM with MMD (FREM).Experimental evidences show that FREM outperforms other baseline methods.</description>
      <author>example@mail.com (Insung Kong, Kunwoong Kim, Yongdai Kim)</author>
      <guid isPermaLink="false">2505.06435v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A systematic review of challenges and proposed solutions in modeling multimodal data</title>
      <link>http://arxiv.org/abs/2505.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态数据建模在临床研究中已成为一种强大的方法，它能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。然而，建模这种异构数据存在技术挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态数据建模能够提高诊断准确性和支持个性化护理，但建模这种异构数据面临技术挑战。&lt;h4&gt;目的&lt;/h4&gt;本文通过综合69项研究的发现，旨在识别多模态数据建模中的常见障碍。&lt;h4&gt;方法&lt;/h4&gt;本文采用系统综述的方法，分析了69项相关研究。&lt;h4&gt;主要发现&lt;/h4&gt;主要障碍包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。&lt;h4&gt;结论&lt;/h4&gt;本文强调了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。此外，本文通过映射当前趋势和创新，为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态数据建模已成为临床研究中一种强大的方法，能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。尽管其能够提高诊断准确性和支持个性化护理，但建模这种异构数据仍然存在显著的技术挑战。本文通过综合69项研究的发现，识别出常见的障碍，包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。本文突出了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。通过映射当前趋势和创新，本文为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data modeling has emerged as a powerful approach in clinicalresearch, enabling the integration of diverse data types such as imaging,genomics, wearable sensors, and electronic health records. Despite itspotential to improve diagnostic accuracy and support personalized care,modeling such heterogeneous data presents significant technical challenges.This systematic review synthesizes findings from 69 studies to identify commonobstacles, including missing modalities, limited sample sizes, dimensionalityimbalance, interpretability issues, and finding the optimal fusion techniques.We highlight recent methodological advances, such as transfer learning,generative models, attention mechanisms, and neural architecture search thatoffer promising solutions. By mapping current trends and innovations, thisreview provides a comprehensive overview of the field and offers practicalinsights to guide future research and development in multimodal modeling formedical applications.</description>
      <author>example@mail.com (Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binde, Nadine Binder)</author>
      <guid isPermaLink="false">2505.06945v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
      <link>http://arxiv.org/abs/2505.06890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合了表示条件的扩散模型，通过使用Vision Transformer（ViT）的表示来调节基于Transformer的扩散模型内部过程，实现了表示条件的数据生成，并利用自监督学习在无标签数据上解决大规模标注数据集的挑战。&lt;h4&gt;背景&lt;/h4&gt;在图像分类任务中，大规模标注数据集的获取是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来生成表示条件的数据，并提高图像分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformer（ViT）的表示作为条件来调节扩散模型，通过零样本分类任务在脑部影像中检测血肿来评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与DINOv2等对比学习基线相比，该方法在准确性上提高了6.15%，在F1分数上提高了13.60%，显示了其在图像分类中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合表示条件和扩散模型，在图像分类任务中取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a diffusion model that integrates arepresentation-conditioning mechanism, where the representations derived from aVision Transformer (ViT) are used to condition the internal process of aTransformer-based diffusion model. This approach enablesrepresentation-conditioned data generation, addressing the challenge ofrequiring large-scale labeled datasets by leveraging self-supervised learningon unlabeled data. We evaluate our method through a zero-shot classificationtask for hematoma detection in brain imaging. Compared to the strongcontrastive learning baseline, DINOv2, our method achieves a notableimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating itseffectiveness in image classification.</description>
      <author>example@mail.com (Kosuke Ukita, Ye Xiaolong, Tsuyoshi Okita)</author>
      <guid isPermaLink="false">2505.06890v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects</title>
      <link>http://arxiv.org/abs/2505.06363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了基于人类演示学习物体模型的方法，用于多自由度物体的精确控制。&lt;h4&gt;背景&lt;/h4&gt;随着机器人在多样化环境中应用，它们需要与具有多个独立关节或自由度的复杂物体交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法依赖先验知识、仅关注单自由度物体、无法处理遮挡关节和忽略获取它们所需的操作序列的问题。&lt;h4&gt;方法&lt;/h4&gt;通过学习人类演示的物体模型，引入了物体运动学序列机器（OKSMs）这一新表示，它捕捉了多自由度物体的运动学约束和操作顺序。为了从点云数据中估计这些模型，提出了Pokenet，这是一个在人类演示上训练的深度神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上，Pokenet比现有方法提高了超过20%的关节轴和状态估计。OKSMs在Sawyer机器人上通过基于逆运动学规划的策略操作多自由度物体。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在模拟和真实世界的数据上均有效，能够提高机器人对多自由度物体的控制精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robots become more generalized and deployed in diverse environments, theymust interact with complex objects, many with multiple independent joints ordegrees of freedom (DoF) requiring precise control. A common strategy is objectmodeling, where compact state-space models are learned from real-worldobservations and paired with classical planning. However, existing methodsoften rely on prior knowledge or focus on single-DoF objects, limiting theirapplicability. They also fail to handle occluded joints and ignore themanipulation sequences needed to access them. We address this by learningobject models from human demonstrations. We introduce Object Kinematic SequenceMachines (OKSMs), a novel representation capturing both kinematic constraintsand manipulation order for multi-DoF objects. To estimate these models frompoint cloud data, we present Pokenet, a deep neural network trained on humandemonstrations. We validate our approach on 8,000 simulated and 1,600real-world annotated samples. Pokenet improves joint axis and state estimationby over 20 percent on real-world data compared to prior methods. Finally, wedemonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning tomanipulate multi-DoF objects.</description>
      <author>example@mail.com (Anmol Gupta, Weiwei Gu, Omkar Patil, Jun Ki Lee, Nakul Gopalan)</author>
      <guid isPermaLink="false">2505.06363v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Similarity Search in Automotive Production</title>
      <link>http://arxiv.org/abs/2505.07256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Procedia CIRP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合视觉基础模型和合成数据的图像分类流程，用于汽车生产中的视觉质量检测，以提高检测的准确性和降低数据收集成本。&lt;h4&gt;背景&lt;/h4&gt;视觉质量检测对于保证汽车的安全和可靠性至关重要，计算机视觉技术在成本效益和可靠性方面表现出色。&lt;h4&gt;目的&lt;/h4&gt;为了减少大量标注数据的收集需求，提出了一个创新的图像分类流程。&lt;h4&gt;方法&lt;/h4&gt;该方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用合成数据而非真实图像作为参考，该流程实现了高分类准确率，同时不依赖于真实数据。&lt;h4&gt;结论&lt;/h4&gt;在八个实际检测场景中评估了该方法，证明它满足了生产环境中的高性能要求。&lt;h4&gt;翻译&lt;/h4&gt;在汽车生产中，视觉质量检查对于确保车辆的安全和可靠性至关重要。由于成本效益和可靠性，计算机视觉（CV）已经成为这些检查的流行解决方案。然而，CV模型需要大量标注的数据集，收集这些数据集既昂贵又耗时。为了减少对大量训练数据的需求，我们提出了一种结合基于视觉的基础模型和合成数据的图像分类流程。我们的方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。通过使用合成数据而不是真实图像作为参考，我们的流程在不依赖于真实数据的情况下实现了高分类准确率。我们在八个现实世界的检测场景中评估了这种方法，证明了它满足生产环境的高性能要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual quality inspection in automotive production is essential for ensuringthe safety and reliability of vehicles. Computer vision (CV) has become apopular solution for these inspections due to its cost-effectiveness andreliability. However, CV models require large, annotated datasets, which arecostly and time-consuming to collect. To reduce the need for extensive trainingdata, we propose a novel image classification pipeline that combines similaritysearch using a vision-based foundation model with synthetic data. Our approachleverages a DINOv2 model to transform input images into feature vectors, whichare then compared to pre-classified reference images using cosine distancemeasurements. By utilizing synthetic data instead of real images as references,our pipeline achieves high classification accuracy without relying on realdata. We evaluate this approach in eight real-world inspection scenarios anddemonstrate that it meets the high performance requirements of productionenvironments.</description>
      <author>example@mail.com (Christoph Huber, Ludwig Schleeh, Dino Knoll, Michael Guthe)</author>
      <guid isPermaLink="false">2505.07256v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting</title>
      <link>http://arxiv.org/abs/2505.06862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了BIGBIRD-PEGASUS模型在长文档摘要任务上的表现，并提出了改进策略以解决模型处理超长文档时性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果，但其处理能力有限，最大只能处理4,096个token的文档，导致在处理超长文档时性能下降。&lt;h4&gt;目的&lt;/h4&gt;旨在提高BIGBIRD-PEGASUS模型处理超长文档的能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调预训练的BIGBIRD-PEGASUS模型，并在其他领域的数据集上进行训练。首先，筛选出长度超过20,000个token的文档，以专注于超长文档。为了解决领域迁移问题和数据集小导致的过拟合，通过将文档摘要训练对拆分成部分，以适应4,096个token的文档。&lt;h4&gt;主要发现&lt;/h4&gt;使用微调的方法可以有效提高模型处理超长文档的能力。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提升BIGBIRD-PEGASUS模型在超长文档摘要任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果。然而，其容量仍限于最多4,096个token，因此在处理超长文档时会导致性能下降。处理此问题的常见方法是对文档进行截断。在本研究中，我们将采用不同的方法。我们将使用预训练的BIGBIRD-PEGASUS模型，并通过在其它领域的数据集上微调模型来改进它。首先，我们将所有长度少于20,000个token的文档过滤掉，以专注于超长文档。为了避免领域迁移问题和由于数据集小导致的迁移学习过拟合，我们将数据集通过将文档摘要训练对拆分成部分来增强，以适应4,096个token的文档。源代码可在https://github.com/lhfazry/SPIN-summ上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lhfazry/spin-summ&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ onabstractive text summarization for long documents. However it's capacity stilllimited to maximum of $4,096$ tokens, thus caused performance degradation onsummarization for very long documents. Common method to deal with the issue isto truncate the documents. In this reasearch, we'll use different approach.We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned themodel on other domain dataset. First, we filter out all documents which lengthless than $20,000$ tokens to focus on very long documents. To prevent domainshifting problem and overfitting on transfer learning due to small dataset, weaugment the dataset by splitting document-summary training pair into parts, tofit the document into $4,096$ tokens. Source code available on$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.</description>
      <author>example@mail.com (Lhuqita Fazry)</author>
      <guid isPermaLink="false">2505.06862v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
      <link>http://arxiv.org/abs/2505.06796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态假新闻检测方法，旨在应对deepfake建模攻击。&lt;h4&gt;背景&lt;/h4&gt;多模态新闻信息丰富，但易受deepfake建模攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;设计一个新的多模态假新闻检测数据集（MFND），并构建一个模型来检测和定位高度逼真的假新闻。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Shallow-Deep Multitask Learning（SDML）的模型，该模型利用单模态和互模态特征挖掘新闻的内在语义。在浅层推理中，采用动量蒸馏的轻惩罚对比学习进行细粒度空间图像和文本语义对齐，并引入自适应跨模态融合模块以增强互模态特征。在深层推理中，设计了两个分支框架，分别增强图像和文本的单模态特征，并合并互模态特征进行四个预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在主流和提出的数据集上均表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;SDML模型在多模态假新闻检测方面具有显著效果。&lt;h4&gt;翻译&lt;/h4&gt;The abstract is about a new multimodal fake news detection method aimed at combating deepfake modeling attacks. The method is demonstrated to be effective in detecting and localizing highly authentic fake news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yunan-wang33/sdml&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal news contains a wealth of information and is easily affected bydeepfake modeling attacks. To combat the latest image and text generationmethods, we present a new Multimodal Fake News Detection dataset (MFND)containing 11 manipulated types, designed to detect and localize highlyauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning(SDML) model for fake news, which fully uses unimodal and mutual modal featuresto mine the intrinsic semantics of news. Under shallow inference, we proposethe momentum distillation-based light punishment contrastive learning forfine-grained uniform spatial image and text semantic alignment, and an adaptivecross-modal fusion module to enhance mutual modal features. Under deepinference, we design a two-branch framework to augment the image and textunimodal features, respectively merging with mutual modalities features, forfour predictions via dedicated detection and localization projections.Experiments on both mainstream and our proposed datasets demonstrate thesuperiority of the model. Codes and dataset are released athttps://github.com/yunan-wang33/sdml.</description>
      <author>example@mail.com (Ye Zhu, Yunan Wang, Zitong Yu)</author>
      <guid isPermaLink="false">2505.06796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
      <link>http://arxiv.org/abs/2505.07214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIRA的对话式AI代理，旨在辅助用户在虚拟现实(VR)环境中对3D医学概念进行定位、分割和可视化。&lt;h4&gt;背景&lt;/h4&gt;手动分割医学扫描图像（如MRI、CT）工作量大，容易出错，难以掌握，而全自动算法可以从用户反馈中受益。&lt;h4&gt;目的&lt;/h4&gt;开发一种辅助工具，通过语音交互帮助用户理解影像学特征，定位临床目标，并生成可以快速细化分割掩码的3D医学概念。&lt;h4&gt;方法&lt;/h4&gt;结合最新的影像学AI基础模型和VR的直观数据交互，设计了SAMIRA，并比较了VR控制器指向、头部指向和眼动追踪作为输入模式，以确定最佳交互范式。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究显示，SAMIRA具有高可用性（SUS=90.0 ± 9.0），总体任务负荷低，以及对VR系统的指导、培训潜力和AI在影像学分割任务中集成的高度支持。&lt;h4&gt;结论&lt;/h4&gt;SAMIRA是一个有效且用户友好的工具，可以增强对患者的解剖理解，并可能提高影像学分割任务的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crucial in disease analysis and surgical planning, manual segmentation ofvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, andchallenging to master, while fully automatic algorithms can benefit fromuser-feedback. Therefore, with the complementary power of the latestradiological AI foundation models and virtual reality (VR)'s intuitive datainteraction, we propose SAMIRA, a novel conversational AI agent that assistsusers with localizing, segmenting, and visualizing 3D medical concepts in VR.Through speech-based interaction, the agent helps users understand radiologicalfeatures, locate clinical targets, and generate segmentation masks that can berefined with just a few point prompts. The system also supports true-to-scale3D visualization of segmented pathology to enhance patient-specific anatomicalunderstanding. Furthermore, to determine the optimal interaction paradigm undernear-far attention-switching for refining segmentation masks in an immersive,human-in-the-loop workflow, we compare VR controller pointing, head pointing,and eye tracking as input modes. With a user study, evaluations demonstrated ahigh usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well asstrong support for the proposed VR system's guidance, training potential, andintegration of AI in radiological segmentation tasks.</description>
      <author>example@mail.com (Pascal Spiegler, Arash Harirpoush, Yiming Xiao)</author>
      <guid isPermaLink="false">2505.07214v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
      <link>http://arxiv.org/abs/2505.06853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 6th BioSMART Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。&lt;h4&gt;背景&lt;/h4&gt;拉丁美洲的癌症病例预计将在2022年达到420万，到2045年将增加到670万。骨肉瘤是一种常见的、对年轻人影响严重的骨癌，由于其独特的质地和强度，难以检测。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，以估计膝关节周围骨肉瘤手术的安全边界置信区间。&lt;h4&gt;方法&lt;/h4&gt;该方法使用来自开源仓库的MRI和X射线数据，结合数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突出了自动、针对患者特定情况确定安全边界的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法有望提高骨肉瘤手术的安全性和精确性。&lt;h4&gt;翻译&lt;/h4&gt;根据泛美卫生组织，2022年拉丁美洲的癌症病例估计为420万，预计到2045年将增加到670万。骨肉瘤是影响年轻人的最常见和致命的骨癌之一，由于其独特的质地和强度，难以检测。手术切除骨肉瘤需要精确的安全边界以确保完全切除同时保留健康组织。因此，本研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。提出的方法使用来自开源仓库的MRI和X射线数据，数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。实验结果突出了自动化、针对患者特定情况确定安全边界的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the Pan American Health Organization, the number of cancer casesin Latin America was estimated at 4.2 million in 2022 and is projected to riseto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bonecancers affecting young people, is difficult to detect due to its uniquetexture and intensity. Surgical removal of osteosarcoma requires precise safetymargins to ensure complete resection while preserving healthy tissue.Therefore, this study proposes a method for estimating the confidence intervalof surgical safety margins in osteosarcoma surgery around the knee. Theproposed approach uses MRI and X-ray data from open-source repositories,digital processing techniques, and unsupervised learning algorithms (such ask-means clustering) to define tumor boundaries. Experimental results highlightthe potential for automated, patient-specific determination of safety margins.</description>
      <author>example@mail.com (Carolina Vargas-Ecos, Edwin Salcedo)</author>
      <guid isPermaLink="false">2505.06853v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>QSeer: A Quantum-Inspired Graph Neural Network for Parameter Initialization in Quantum Approximate Optimization Algorithm Circuits</title>
      <link>http://arxiv.org/abs/2505.06810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QSeer的量子灵感的图神经网络，旨在准确预测量子近似优化算法（QAOA）的参数初始化，以提高在NISQ时代的QAOA优化效果。&lt;h4&gt;背景&lt;/h4&gt;量子近似优化算法（QAOA）在优化过程中存在 barren plateau problem，即性能提升停滞的问题。有效的参数初始化对于在近期的有噪声中等规模量子（NISQ）时代优化QAOA至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有参数初始化方法的局限性，提高QAOA在NISQ时代的性能。&lt;h4&gt;方法&lt;/h4&gt;QSeer是一种量子灵感的图神经网络，它借鉴了先前优化的QAOA参数，并结合了关键的物理信息，如参数浓度、对称性和绝热进化原则。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的方法相比，QSeer提高了QAOA电路在多样化图上的初始近似比和收敛速度，分别提升了6%-68%和5x-10x。&lt;h4&gt;结论&lt;/h4&gt;QSeer作为一种新型量子灵感图神经网络，能够显著提高QAOA的优化性能，为NISQ时代的量子计算提供了一种有效的参数初始化方法。&lt;h4&gt;翻译&lt;/h4&gt;To mitigate the barren plateau problem, effective parameter initialization is crucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) in the near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-driven approaches leveraged the optimal parameter concentration phenomenon, utilizing medium values of previously optimized QAOA parameters stored in databases as initialization for new graphs. However, this medium-value-based strategy lacks generalization capability. Conversely, prior computer-science-based approaches employed graph neural networks (GNNs) trained on previously optimized QAOA parameters to predict initialization values for new graphs. However, these approaches neglect key physics-informed QAOA principles, such as parameter concentration, symmetry, and adiabatic evolution, resulting in suboptimal parameter predictions and limited performance improvements. Furthermore, no existing GNN-based methods support parameter initialization for QAOA circuits with variable depths or for solving weighted Max-Cut problems. This paper introduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameter prediction. Compared to prior physics- and computer-science-driven methods, QSeer improves the initial approximation ratio and convergence speed of QAOA circuits across diverse graphs by 6%-68% and 5x-10x, respectively.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To mitigate the barren plateau problem, effective parameter initialization iscrucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) inthe near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-drivenapproaches leveraged the optimal parameter concentration phenomenon, utilizingmedium values of previously optimized QAOA parameters stored in databases asinitialization for new graphs. However, this medium-value-based strategy lacksgeneralization capability. Conversely, prior computer-science-based approachesemployed graph neural networks (GNNs) trained on previously optimized QAOAparameters to predict initialization values for new graphs. However, theseapproaches neglect key physics-informed QAOA principles, such as parameterconcentration, symmetry, and adiabatic evolution, resulting in suboptimalparameter predictions and limited performance improvements. Furthermore, noexisting GNN-based methods support parameter initialization for QAOA circuitswith variable depths or for solving weighted Max-Cut problems. This paperintroduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameterprediction. Compared to prior physics- and computer-science-driven methods,QSeer improves the initial approximation ratio and convergence speed of QAOAcircuits across diverse graphs by 6%-68% and 5x-10x, respectively.</description>
      <author>example@mail.com (Lei Jiang, Chi Zhang, Fan Chen)</author>
      <guid isPermaLink="false">2505.06810v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Architectural Precedents for General Agents using Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 2 figures. Submitted to AGI25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文总结了在多种预Transformer AI架构中出现的认知设计模式，并探讨了这些模式在利用大型语言模型（LLMs）的系统中的应用，特别是对于推理和交互式（“代理”）用例。&lt;h4&gt;背景&lt;/h4&gt;人工智能和通用人工智能（AGI）的目标之一是识别和理解足以实现通用智能的具体机制和表示。在AI/AGI领域，许多认知架构已被探索。&lt;h4&gt;目的&lt;/h4&gt;识别和理解足以实现通用智能的具体机制和表示。&lt;h4&gt;方法&lt;/h4&gt;总结预Transformer AI架构中的认知设计模式，并分析这些模式在LLMs系统中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;不同研究组和不同研究传统独立地识别了相似/共同的过程和表示或认知设计模式，这些模式在现有架构中体现。LLMs提供了一种新的机制和表示组合，用于探索通用智能的可能性。&lt;h4&gt;结论&lt;/h4&gt;通过分析和应用这些重复出现的模式，可以预测今天代理LLM系统中的差距或不足，并确定使用LLMs和其他生成基础模型实现通用智能的未来的研究主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One goal of AI (and AGI) is to identify and understand specific mechanismsand representations sufficient for general intelligence. Often, this workmanifests in research focused on architectures and many cognitive architectureshave been explored in AI/AGI. However, different research groups and evendifferent research traditions have somewhat independently identifiedsimilar/common patterns of processes and representations or cognitive designpatterns that are manifest in existing architectures. Today, AI systemsexploiting large language models (LLMs) offer a relatively new combination ofmechanism and representation available for exploring the possibilities ofgeneral intelligence. In this paper, we summarize a few recurring cognitivedesign patterns that have appeared in various pre-transformer AI architectures.We then explore how these patterns are evident in systems using LLMs,especially for reasoning and interactive ("agentic") use cases. By examiningand applying these recurring patterns, we can also predict gaps or deficienciesin today's Agentic LLM Systems and identify likely subjects of future researchtowards general intelligence using LLMs and other generative foundation models.</description>
      <author>example@mail.com (Robert E. Wray, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2505.07087v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，通过战略性地选择或加权数据。该方法名为DRRho风险最小化，基于分布鲁棒优化（DRO）。通过泛化分析，本文提供了理论上的见解，解释了为什么与没有参考模型训练相比，这种方法可以改善泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;虽然模型引导在包括大型基础模型训练的各种场景中已经使用，但其基本原理理解不足，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于理论的框架DRRho风险最小化，用于模型引导，并引入一种名为DRRho-CLIP的新方法，用于具有参考模型的对比语言-图像预训练（CLIP）。&lt;h4&gt;方法&lt;/h4&gt;本文通过泛化分析，对模型引导提供了理论上的见解，并引入了DRRho-CLIP方法。&lt;h4&gt;主要发现&lt;/h4&gt;本文提供了模型引导的第一个理论见解，增强了我们对模型引导的理解和实践，并通过实验验证了理论见解的有效性。&lt;h4&gt;结论&lt;/h4&gt;DRRho-CLIP方法比没有参考模型的CLIP具有更好的扩展性和性能，优于现有的启发式方法。&lt;h4&gt;翻译&lt;/h4&gt;本文将摘要内容翻译成了中文，并按照要求进行了结构化表达。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. That, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Networks for Cross-Energy Particle Identification at RHIC and LHC</title>
      <link>http://arxiv.org/abs/2505.06732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Journal of Physics G: Nuclear and Particle Physics on 30  April 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了在强横动量区域应用深度神经网络进行粒子识别的方法。&lt;h4&gt;背景&lt;/h4&gt;研究基于模拟的大型强子对撞机（LHC）质子-质子碰撞数据（sqrt(s) = 13TeV）。&lt;h4&gt;目的&lt;/h4&gt;目的是训练一个模型，使用七个动力学特征来区分九种不同的粒子。&lt;h4&gt;方法&lt;/h4&gt;模型在模拟数据上训练，并在高横动量RHIC数据上测试，未进行迁移学习、微调或权重调整。&lt;h4&gt;主要发现&lt;/h4&gt;模型在LHC和RHIC数据集上均保持了超过91%的准确率，在所有RHIC数据集上均达到了超过96%的准确率，包括横动量大于7 GeV/c的数据集，尽管模型未在RHIC数据上训练。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，该模型能够捕捉到高能碰撞的底层物理，而不仅仅是过拟合训练数据。&lt;h4&gt;翻译&lt;/h4&gt;这项工作强调了模拟训练模型在不同能量域中的应用潜力，特别是在数据较少或代表性不足的设置中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work demonstrates the application of a deep neural network for particleidentification in the high transverse momentum regime. A model trained onsimulated Large Hadron Collider (LHC) proton-proton collisions (sqrt(s) = 13TeV) is used to classify nine distinct particles using seven kinematic-levelfeatures. The model is then tested on high transverse momentum RHIC datawithout any transfer learning, fine-tuning, or weight adjustment. It maintainsaccuracy above 91% for both LHC and RHIC sets, while achieving above 96%accuracy for all RHIC sets, including the pT greater than 7 GeV/c set, despitenot having been trained on any RHIC data. These results indicate that the modelcaptures the underlying physics of high-energy collisions rather than justoverfitting to the training data. This work highlights the potential ofsimulation-trained models to be deployed in different energy domains,especially in underrepresented or data-limited settings.</description>
      <author>example@mail.com (Omar M. Khalaf, Ahmed M. Hamed)</author>
      <guid isPermaLink="false">2505.06732v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Graph Representation of Agent Diffuser</title>
      <link>http://arxiv.org/abs/2505.06761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAMAS2025 International Conference on Autonomous Agents  and Multiagent Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LGR-AD的新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。该系统通过模拟分布式系统中相互作用的智能体来建模生成过程，每个智能体代表一个专家子模型，并通过图神经网络进行协作。&lt;h4&gt;背景&lt;/h4&gt;扩散生成模型在文本到图像合成方面取得了显著进展，但静态模型参数可能无法最佳地处理生成过程的各个阶段。&lt;h4&gt;目的&lt;/h4&gt;提出LGR-AD系统，以提高动态计算机视觉任务中的适应性。&lt;h4&gt;方法&lt;/h4&gt;LGR-AD将生成过程建模为分布式系统，每个智能体代表一个专家子模型，并通过图神经网络进行协作。使用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，显示出其在复杂图像生成任务中的可扩展性和灵活性。&lt;h4&gt;结论&lt;/h4&gt;LGR-AD系统在动态计算机视觉任务中具有提高适应性的潜力，并在图像生成任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于扩散的生成模型在文本到图像合成方面取得了显著进展，展示了令人印象深刻的文本理解和零样本泛化能力。这些模型根据文本提示从随机噪声中细化图像，初始对文本输入的依赖逐渐转向增强视觉保真度。这种转变表明，静态模型参数可能无法最佳地处理生成过程的各个阶段。我们引入了LGR-AD（学习智能体扩散器图表示），这是一种新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。LGR-AD将生成过程建模为相互作用的智能体分布式系统，每个智能体代表一个专家子模型。这些智能体通过编码其关系和性能指标的图神经网络动态适应变化条件并协作。我们的方法采用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，突出了其在复杂图像生成任务中的可扩展性和灵活性的潜力。代码可在以下链接获取：https://github.com/YousIA/LGR_AD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yousia/lgr_ad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion-based generative models have significantly advanced text-to-imagesynthesis, demonstrating impressive text comprehension and zero-shotgeneralization. These models refine images from random noise based on textualprompts, with initial reliance on text input shifting towards enhanced visualfidelity over time. This transition suggests that static model parameters mightnot optimally address the distinct phases of generation. We introduce LGR-AD(Learning Graph Representation of Agent Diffusers), a novel multi-agent systemdesigned to improve adaptability in dynamic computer vision tasks. LGR-ADmodels the generation process as a distributed system of interacting agents,each representing an expert sub-model. These agents dynamically adapt tovarying conditions and collaborate through a graph neural network that encodestheir relationships and performance metrics. Our approach employs acoordination mechanism based on top-$k$ maximum spanning trees, optimizing thegeneration process. Each agent's decision-making is guided by a meta-model thatminimizes a novel loss function, balancing accuracy and diversity. Theoreticalanalysis and extensive empirical evaluations show that LGR-AD outperformstraditional diffusion models across various benchmarks, highlighting itspotential for scalable and flexible solutions in complex image generationtasks. Code is available at: https://github.com/YousIA/LGR_AD</description>
      <author>example@mail.com (Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi)</author>
      <guid isPermaLink="false">2505.06761v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mixer-Informer-Based Two-Stage Transfer Learning for Long-Sequence Load Forecasting in Newly Constructed Electric Vehicle Charging Stations</title>
      <link>http://arxiv.org/abs/2505.06657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MIK-TST的创新两阶段迁移学习框架，用于精确预测电动汽车充电站的负荷，旨在提高智能电网效率和支撑可持续的电动汽车基础设施扩展。&lt;h4&gt;背景&lt;/h4&gt;随着电动汽车的快速普及，对充电站负荷的精确预测变得至关重要，但这一预测面临着长期时间依赖性和新设施数据有限的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效预测充电站负荷的MIK-TST框架，以提高充电站负荷预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;MIK-TST框架整合了Mixer、Informer和Kolmogorov-Arnold Networks (KAN)技术。Mixer融合了多源特征，Informer通过ProbSparse注意力机制捕捉长距离依赖性，而KAN则通过可学习的激活函数增强了非线性建模。该框架在大量数据上预训练，并在有限的目标数据上微调。&lt;h4&gt;主要发现&lt;/h4&gt;MIK-TST在MAE和MSE方面分别实现了4%和8%的降低，在Boulder，美国的一个包含26个充电站的数据库上优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;MIK-TST是一种可扩展的解决方案，能够提高智能电网的效率并支持可持续的电动汽车基础设施的扩展。&lt;h4&gt;翻译&lt;/h4&gt;The rapid rise in electric vehicle (EV) adoption demands precise charging station load forecasting, challenged by long-sequence temporal dependencies and limited data in new facilities. This study proposes MIK-TST, a novel two-stage transfer learning framework integrating Mixer, Informer, and Kolmogorov-Arnold Networks (KAN). The Mixer fuses multi-source features, Informer captures long-range dependencies via ProbSparse attention, and KAN enhances non-linear modeling with learnable activation functions. Pre-trained on extensive data and fine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAE and MSE, respectively, outperforming baselines on a dataset of 26 charging stations in Boulder, USA. This scalable solution enhances smart grid efficiency and supports sustainable EV infrastructure expansion.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid rise in electric vehicle (EV) adoption demands precise chargingstation load forecasting, challenged by long-sequence temporal dependencies andlimited data in new facilities. This study proposes MIK-TST, a novel two-stagetransfer learning framework integrating Mixer, Informer, and Kolmogorov-ArnoldNetworks (KAN). The Mixer fuses multi-source features, Informer captureslong-range dependencies via ProbSparse attention, and KAN enhances nonlinearmodeling with learnable activation functions. Pre-trained on extensive data andfine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAEand MSE, respectively, outperforming baselines on a dataset of 26 chargingstations in Boulder, USA. This scalable solution enhances smart grid efficiencyand supports sustainable EV infrastructure expansion.</description>
      <author>example@mail.com (Zhenhua Zhou, Bozhen Jiang, Qin Wang)</author>
      <guid isPermaLink="false">2505.06657v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Contrastive Federated Semi-Supervised Learning Intrusion Detection Framework for Internet of Robotic Things</title>
      <link>http://arxiv.org/abs/2505.06636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对物联网机器人（IoRT）网络入侵检测和防御的框架CFedSSL-NID，用于解决机器人本地没有标记数据且需要保护数据隐私的实际场景。&lt;h4&gt;背景&lt;/h4&gt;在智能工业和自动驾驶等环境中，物联网（IoT）与机器人高度集成形成物联网机器人（IoRT）。然而，IoRT的网络入侵可能导致数据泄露、服务中断，甚至通过控制机器人或车辆造成物理损害。&lt;h4&gt;目的&lt;/h4&gt;为了解决IoRT机器人本地缺乏标记数据且需要保护数据隐私的问题，提出了一种名为CFedSSL-NID的对比联邦半监督学习网络入侵检测框架。&lt;h4&gt;方法&lt;/h4&gt;CFedSSL-NID框架结合了随机弱和强数据增强、潜在对比学习和EMA更新，以整合监督信号，从而在机器人本地未标记数据上增强性能和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，CFedSSL-NID在基准数据集上优于现有的联邦半监督和全监督方法，并且具有更低的资源需求。&lt;h4&gt;结论&lt;/h4&gt;CFedSSL-NID是一种有效的IoRT入侵检测和防御框架，能够有效处理数据隐私保护问题，并具有较低的资源需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In intelligent industry, autonomous driving and other environments, theInternet of Things (IoT) highly integrated with robotic to form the Internet ofRobotic Things (IoRT). However, network intrusion to IoRT can lead to dataleakage, service interruption in IoRT and even physical damage by controllingrobots or vehicles. This paper proposes a Contrastive Federated Semi-SupervisedLearning Network Intrusion Detection framework (CFedSSL-NID) for IoRT intrusiondetection and defense, to address the practical scenario of IoRT where robotsdon't possess labeled data locally and the requirement for data privacypreserving. CFedSSL-NID integrates randomly weak and strong augmentation,latent contrastive learning, and EMA update to integrate supervised signals,thereby enhancing performance and robustness on robots' local unlabeled data.Extensive experiments demonstrate that CFedSSL-NID outperforms existingfederated semi-supervised and fully supervised methods on benchmark dataset andhas lower resource requirements.</description>
      <author>example@mail.com (Yifan Zeng)</author>
      <guid isPermaLink="false">2505.06636v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Reconstructing Brain Causal Dynamics for Subject and Task Fingerprints using fMRI Time-series Data</title>
      <link>http://arxiv.org/abs/2505.06392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种利用因果动力学进行fMRI基于主体和任务指纹识别的新方法，通过分析多尺度脑网络中的复杂关系，验证了其可行性和有效性。&lt;h4&gt;背景&lt;/h4&gt;系统神经科学因果模型近年来因其在多尺度脑网络中解析复杂关系的能力而重新引起关注。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，利用因果动力学，以实现基于fMRI的个体和任务指纹识别。&lt;h4&gt;方法&lt;/h4&gt;应用隐式-显式离散化方案，开发了一种双时标线性状态空间模型。通过数据驱动识别其参数，该模型捕捉因果特征，包括从空间角度分析大脑区域之间的有向交互，以及从时间角度解耦大脑活动的快慢动态模式。这些因果特征随后与基于模型的主体识别的模态分解和投影方法以及用于基于学习的任务分类的图神经网络（GNN）框架相结合。此外，引入了大脑可达性景观的概念作为新的可视化工具，定量描述了在各种fMRI任务下大脑区域可能的最大激活水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用人类连接组项目数据集评估了所提出的方法，并证明了其优于非因果方法的优点。获得的因果特征被可视化，并显示出与大脑功能已建立的理解的明确生物学相关性。&lt;h4&gt;结论&lt;/h4&gt;验证了利用大脑因果特征进行主体和任务指纹识别的可行性和有效性。此外，这项工作为因果指纹在健康对照和神经退行性疾病中的潜在应用的研究铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The abstract of this paper is summarized as follows: Recently, there has been a renewed interest in system neuroscience causation models, driven by their unique capability to unravel complex relationships in multi-scale brain networks. In this paper, we present a novel method that leverages causal dynamics to achieve effective fMRI-based subject and task fingerprinting. By applying an implicit-explicit discretization scheme, we develop a two-timescale linear state-space model. Through data-driven identification of its parameters, the model captures causal signatures, including directed interactions among brain regions from a spatial perspective, and disentangled fast and slow dynamic modes of brain activity from a temporal perspective. These causal signatures are then integrated with: (i) a modal decomposition and projection method for model-based subject identification, and (ii) a Graph Neural Network (GNN) framework for learning-based task classification. Furthermore, we introduce the concept of the brain reachability landscape as a novel visualization tool, which quantitatively characterizes the maximum possible activation levels of brain regions under various fMRI tasks. We evaluate the proposed approach using the Human Connectome Project dataset and demonstrate its advantage over non-causality-based methods. The obtained causal signatures are visualized and demonstrate clear biological relevance with established understandings of brain function. We verified the feasibility and effectiveness of utilizing brain causal signatures for subject and task fingerprinting. Additionally, our work paves the way for further studies on causal fingerprints with potential applications in both healthy controls and neurodegenerative diseases.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Recently, there has been a revived interest in system neurosciencecausation models, driven by their unique capability to unravel complexrelationships in multi-scale brain networks. In this paper, we present a novelmethod that leverages causal dynamics to achieve effective fMRI-based subjectand task fingerprinting. Methods: By applying an implicit-explicitdiscretization scheme, we develop a two-timescale linear state-space model.Through data-driven identification of its parameters, the model captures causalsignatures, including directed interactions among brain regions from a spatialperspective, and disentangled fast and slow dynamic modes of brain activityfrom a temporal perspective. These causal signatures are then integrated with:(i) a modal decomposition and projection method for model-based subjectidentification, and (ii) a Graph Neural Network (GNN) framework forlearning-based task classification. Furthermore, we introduce the concept ofthe brain reachability landscape as a novel visualization tool, whichquantitatively characterizes the maximum possible activation levels of brainregions under various fMRI tasks. Results: We evaluate the proposed approachusing the Human Connectome Project dataset and demonstrate its advantage overnon-causality-based methods. The obtained causal signatures are visualized anddemonstrate clear biological relevance with established understandings of brainfunction. Conclusion: We verified the feasibility and effectiveness ofutilizing brain causal signatures for subject and task fingerprinting.Additionally, our work paves the way for further studies on causal fingerprintswith potential applications in both healthy controls and neurodegenerativediseases.</description>
      <author>example@mail.com (Dachuan Song, Li Shen, Duy Duong-Tran, Xuan Wang)</author>
      <guid isPermaLink="false">2505.06392v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines</title>
      <link>http://arxiv.org/abs/2505.06333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, 2 tables, IJCAI 2025 (International Joint  Conferences on Artificial Intelligence) Special Track on AI4Tech: AI Enabling  Critical Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经符号AI和融合的多模态异常预测方法，用于现代装配管道中的产品质量和运营效率保障。&lt;h4&gt;背景&lt;/h4&gt;在复杂的多模态环境和高数据量的预测环境中，传统的单模态方法无法捕捉到精确异常预测所需的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;确保装配管道中的产品质量和运营效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于时间序列和图像的融合模型，并采用了决策级融合技术。研究基于三个主要创新方法：时间序列和图像的决策级融合建模、迁移学习用于融合和知识注入学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用迁移学习的神经符号AI融合方法能够有效地利用时间序列和图像数据的互补优势，为装配管道中的异常预测提供了一种稳健且可解释的方法，并提高了性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在公开的多模态数据集上进行了评估，并通过消融研究验证了预处理技术和融合模型的有效性。&lt;h4&gt;翻译&lt;/h4&gt;In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. The datasets, codes to reproduce the results, supplementary materials, and demo are available at https://github.com/ChathurangiShyalika/NSF-MAP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/chathurangishyalika/nsf-map&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern assembly pipelines, identifying anomalies is crucial in ensuringproduct quality and operational efficiency. Conventional single-modalitymethods fail to capture the intricate relationships required for preciseanomaly prediction in complex predictive environments with abundant data andmultiple modalities. This paper proposes a neurosymbolic AI and fusion-basedapproach for multimodal anomaly prediction in assembly pipelines. We introducea time series and image-based fusion model that leverages decision-level fusiontechniques. Our research builds upon three primary novel approaches inmultimodal learning: time series and image-based decision-level fusionmodeling, transfer learning for fusion, and knowledge-infused learning. Weevaluate the novel method using our derived and publicly available multimodaldataset and conduct comprehensive ablation studies to assess the impact of ourpreprocessing techniques and fusion model compared to traditional baselines.The results demonstrate that a neurosymbolic AI-based fusion approach that usestransfer learning can effectively harness the complementary strengths of timeseries and image data, offering a robust and interpretable approach for anomalyprediction in assembly pipelines with enhanced performance. \noindent Thedatasets, codes to reproduce the results, supplementary materials, and demo areavailable at https://github.com/ChathurangiShyalika/NSF-MAP.</description>
      <author>example@mail.com (Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth)</author>
      <guid isPermaLink="false">2505.06333v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2505.06628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ACORN的算法，用于提高机器人在现实世界中的应用中的鲁棒性和安全性。&lt;h4&gt;背景&lt;/h4&gt;传统的Embodied AI研究过于关注成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一差距，本文引入了四个新的以安全性为中心的指标，并提出了ACORN算法，以提高策略的鲁棒性而不牺牲性能。&lt;h4&gt;方法&lt;/h4&gt;ACORN利用对比学习同时将轨迹与专家演示对齐，并从可能的不安全行为中分离出来。它通过结构化高斯噪声注入生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。&lt;h4&gt;结论&lt;/h4&gt;ACORN在提高Embodied AI在安全性关键的实际应用中的可靠部署方面具有重大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Embodied AI研究传统上强调成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。在实际情况中，智能体不断遇到不可预测的情况和分布变化，导致看似可靠的策略出现灾难性失败，尤其是在操作任务中。为了解决这一差距，我们引入了四个新的以安全性为中心的指标，以量化智能体对环境扰动的弹性。在这些指标的基础上，我们提出了自适应对比优化算法ACORN，这是一种即插即用的算法，它增强了策略的鲁棒性，而不牺牲性能。ACORN利用对比学习同时将轨迹与专家演示对齐，同时从可能的不安全行为中分离出来。我们的方法通过结构化高斯噪声注入高效地生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。这些发现强调了ACORN在使Embodied AI在安全性关键的实际应用中可靠部署方面的重要潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied AI research has traditionally emphasized performance metrics such assuccess rate and cumulative reward, overlooking critical robustness and safetyconsiderations that emerge during real-world deployment. In actualenvironments, agents continuously encounter unpredicted situations anddistribution shifts, causing seemingly reliable policies to experiencecatastrophic failures, particularly in manipulation tasks. To address this gap,we introduce four novel safety-centric metrics that quantify an agent'sresilience to environmental perturbations. Building on these metrics, wepresent Adaptive Contrastive Optimization for Robust Manipulation (ACORN), aplug-and-play algorithm that enhances policy robustness without sacrificingperformance. ACORN leverages contrastive learning to simultaneously aligntrajectories with expert demonstrations while diverging from potentially unsafebehaviors. Our approach efficiently generates informative negative samplesthrough structured Gaussian noise injection, employing a double perturbationtechnique that maintains sample diversity while minimizing computationaloverhead. Comprehensive experiments across diverse manipulation environmentsvalidate ACORN's effectiveness, yielding improvements of up to 23% in safetymetrics under disturbance compared to baseline methods. These findingsunderscore ACORN's significant potential for enabling reliable deployment ofembodied agents in safety-critical real-world applications.</description>
      <author>example@mail.com (Zhongquan Zhou, Shuhao Li, Zixian Yue)</author>
      <guid isPermaLink="false">2505.06628v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
      <link>http://arxiv.org/abs/2505.06557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TCSVT 2025, doi at https://ieeexplore.ieee.org/document/10970001&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Positive Sample Mining (PSM)的框架，用于弱监督时间句子定位（WSTSG）任务，旨在从无剪辑视频中检测与语言描述相对应的时间区间。&lt;h4&gt;背景&lt;/h4&gt;现有的WSTSG方法通常通过生成负样本进行对比学习，但这些负样本与锚样本高度相似，直接将其作为负样本会导致优化困难，并忽略了这些相似样本与锚样本之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出PSM框架，从训练集中挖掘正样本，提供更具区分性的监督。&lt;h4&gt;方法&lt;/h4&gt;PSM框架通过文本查询的相似性将剩余训练集划分为语义相似和不同子集。为了有效利用这些相关性，引入了PSM指导的对比损失和排名损失，以确保锚样本与相似样本更近，与不相似样本更远，并区分锚样本和负样本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PSM框架在WSTSG和grounded VideoQA任务上表现出有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;PSM框架能够有效提高WSTSG任务的性能，为视频理解提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The task of weakly supervised temporal sentence grounding (WSTSG) aims todetect temporal intervals corresponding to a language description fromuntrimmed videos with only video-level video-language correspondence. For ananchor sample, most existing approaches generate negative samples either fromother videos or within the same video for contrastive learning. However, sometraining samples are highly similar to the anchor sample, directly regardingthem as negative samples leads to difficulties for optimization and ignores thecorrelations between these similar samples and the anchor sample. To addressthis, we propose Positive Sample Mining (PSM), a novel framework that minespositive samples from the training set to provide more discriminativesupervision. Specifically, for a given anchor sample, we partition theremaining training set into semantically similar and dissimilar subsets basedon the similarity of their text queries. To effectively leverage thesecorrelations, we introduce a PSM-guided contrastive loss to ensure that theanchor proposal is closer to similar samples and further from dissimilar ones.Additionally, we design a PSM-guided rank loss to ensure that similar samplesare closer to the anchor proposal than to the negative intra-video proposal,aiming to distinguish the anchor proposal and the negative intra-videoproposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate theeffectiveness and superiority of our method.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2505.06557v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该摘要介绍了一种名为MMSA的多模态自监督分子表示预训练框架，用于提升分子图表示的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确提取分子表示是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，特别是基于图像和2D/3D拓扑的多模态分子表示方法变得越来越主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间相互作用，未能充分捕捉分子之间的复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于结构感知的多模态自监督分子表示预训练框架（MMSA），通过利用分子之间的不变知识来增强分子图表示。该框架包括两个主要模块：多模态分子表示学习模块和结构感知模块。&lt;h4&gt;主要发现&lt;/h4&gt;多模态分子表示学习模块协同处理同一种分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来建模分子之间的高阶相关性，并引入一个记忆机制来存储典型的分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型的一般化能力。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验证明了MMSA的有效性，在MoleculeNet基准测试中取得了最先进的性能，与基线方法相比，平均ROC-AUC提高了1.8%至9.6%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning</title>
      <link>http://arxiv.org/abs/2505.06321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用图学习来增强大型语言模型（LLMs）推理能力的框架。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型在各种领域取得了显著成功，但它们在训练成本和解决复杂推理问题方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，以实现LLMs更灵活和自适应的推理能力。&lt;h4&gt;方法&lt;/h4&gt;该框架将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。此外，引入了图神经网络（GNN）模块来对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，这种方法在不要求额外训练或特定任务提示设计的情况下，显著提高了多个任务上的推理性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为LLMs提供了更灵活和自适应的推理能力，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各个领域取得了显著的成功。然而，它们仍然面临着包括训练高计算成本和解决复杂推理问题限制在内的重大挑战。尽管现有方法通过结构化范式扩展了LLMs的推理能力，但这些方法通常依赖于特定任务的提示和预定义的推理过程，这限制了它们的灵活性和泛化能力。为了解决这些限制，我们提出了一种新颖的框架，该框架利用图学习来为LLMs提供更灵活和自适应的推理能力。具体而言，这种方法将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。为了进一步增强模型的适应性，我们引入了一个图神经网络（GNN）模块，对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。实验结果表明，这种方法在多个任务上显著提高了推理性能，而不需要额外的训练或特定任务的提示设计。代码可在https://github.com/zch65458525/L2T上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zch65458525/l2t&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success across variousdomains. However, they still face significant challenges, including highcomputational costs for training and limitations in solving complex reasoningproblems. Although existing methods have extended the reasoning capabilities ofLLMs through structured paradigms, these approaches often rely on task-specificprompts and predefined reasoning processes, which constrain their flexibilityand generalizability. To address these limitations, we propose a novelframework that leverages graph learning to enable more flexible and adaptivereasoning capabilities for LLMs. Specifically, this approach models thereasoning process of a problem as a graph and employs LLM-based graph learningto guide the adaptive generation of each reasoning step. To further enhance theadaptability of the model, we introduce a Graph Neural Network (GNN) module toperform representation learning on the generated reasoning process, enablingreal-time adjustments to both the model and the prompt. Experimental resultsdemonstrate that this method significantly improves reasoning performanceacross multiple tasks without requiring additional training or task-specificprompt design. Code can be found in https://github.com/zch65458525/L2T.</description>
      <author>example@mail.com (Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.06321v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在图数据上的应用，提出了一种新的框架RELGCL，通过保留自然相对相似性模式来提高学习效果。&lt;h4&gt;背景&lt;/h4&gt;GCL在计算机视觉领域取得了成功，但在图数据上由于图的离散性和非欧几里得性质，传统方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，以解决图数据中的挑战，并提高学习效果。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个真实世界的图，发现了一个普遍模式：随着结构距离的增加，标签一致性会系统地减少。利用随机游走理论对这一模式进行理论保证，并提出了RELGCL框架。&lt;h4&gt;主要发现&lt;/h4&gt;发现图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;RELGCL框架在同质性和异质性图上均优于20种现有方法，验证了利用自然相对相似性比人工绝对相似性更有效。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
      <link>http://arxiv.org/abs/2505.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  On going work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了大型语言模型（LLMs）如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域，并提出了一种名为人工个性化智能（API）的愿景，旨在通过个性化定制来满足用户的具体需求，同时保持隐私和效率。&lt;h4&gt;背景&lt;/h4&gt;LLMs如ChatGPT等在生成类似人类内容方面表现出色，接近实现通用人工智能（AGI），但它们的大规模特性、对隐私的敏感性以及计算需求给个性化定制带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出人工个性化智能（API）的概念，旨在通过个性化联邦智能（PFI）将强大的模型适应于用户的具体需求，同时保持隐私和效率。&lt;h4&gt;方法&lt;/h4&gt;本文首先回顾了联邦学习（FL）和基础模型（FMs）的最新进展，并讨论了利用FMs增强联邦系统的潜力。然后，文章探讨了实现PFI的关键动机和这一领域的有希望的机会，包括高效的PFI、可信的PFI和由检索增强生成（RAG）赋能的PFI。&lt;h4&gt;主要发现&lt;/h4&gt;PFI结合了联邦学习的隐私保护优势以及基础模型的零样本泛化能力，使得在边缘进行个性化、高效且隐私保护的应用成为可能。&lt;h4&gt;结论&lt;/h4&gt;本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;h4&gt;翻译&lt;/h4&gt;本文讨论了大型语言模型（LLMs），如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域。作为构建在LLMs之上的基础模型（FMs）的突出例子，这些模型在生成类似人类内容方面表现出色，使我们更接近实现通用人工智能（AGI）。然而，它们的大规模特性、对隐私的敏感性和大量的计算需求给为最终用户进行个性化定制带来了重大挑战。为了弥合这一差距，本文提出了人工个性化智能（API）的愿景，重点在于将这些强大的模型适应于满足用户的具体需求，同时保持隐私和效率。具体而言，本文提出了个性化联邦智能（PFI），它将联邦学习的隐私保护优势与FMs的零样本泛化能力相结合，使得在边缘实现个性化、高效和隐私保护的应用成为可能。我们首先回顾了FL和FMs的最近进展，并讨论了利用FMs增强联邦系统的潜力。然后，我们提出了实现PFI的关键动机并探索了这一领域的有希望的机会，包括高效PFI、可信PFI以及由检索增强生成（RAG）赋能的PFI。最后，我们概述了在边缘部署FM驱动的FL系统时的关键挑战和未来研究方向，以实现改进的个性化、计算效率和隐私保证。总的来说，本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of large language models (LLMs), such as ChatGPT, DeepSeek, andGrok-3, has reshaped the artificial intelligence landscape. As prominentexamples of foundational models (FMs) built on LLMs, these models exhibitremarkable capabilities in generating human-like content, bringing us closer toachieving artificial general intelligence (AGI). However, their large-scalenature, sensitivity to privacy concerns, and substantial computational demandspresent significant challenges to personalized customization for end users. Tobridge this gap, this paper presents the vision of artificial personalizedintelligence (API), focusing on adapting these powerful models to meet thespecific needs and preferences of users while maintaining privacy andefficiency. Specifically, this paper proposes personalized federatedintelligence (PFI), which integrates the privacy-preserving advantages offederated learning (FL) with the zero-shot generalization capabilities of FMs,enabling personalized, efficient, and privacy-protective deployment at theedge. We first review recent advances in both FL and FMs, and discuss thepotential of leveraging FMs to enhance federated systems. We then present thekey motivations behind realizing PFI and explore promising opportunities inthis space, including efficient PFI, trustworthy PFI, and PFI empowered byretrieval-augmented generation (RAG). Finally, we outline key challenges andfuture research directions for deploying FM-powered FL systems at the edge withimproved personalization, computational efficiency, and privacy guarantees.Overall, this survey aims to lay the groundwork for the development of API as acomplement to AGI, with a particular focus on PFI as a key enabling technique.</description>
      <author>example@mail.com (Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong)</author>
      <guid isPermaLink="false">2505.06907v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data</title>
      <link>http://arxiv.org/abs/2505.06276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a poster in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了SynSHRP2，一个由SHRP 2 NDS数据合成的多模态驾驶数据集，用于解决SCEs数据难以获取的问题。&lt;h4&gt;背景&lt;/h4&gt;SCEs（与驾驶相关的安全关键事件）对于自动驾驶系统的发展和安全评估至关重要，但由于SCEs的稀有性和数据中存在的隐私信息，其获取存在挑战。&lt;h4&gt;目的&lt;/h4&gt;合成数据集以解决SCEs数据获取的难题，同时保护个人隐私。&lt;h4&gt;方法&lt;/h4&gt;使用StableDiffusion和ControlNet技术生成去识别化的关键帧，并包含详细标注，包括SCE类型、环境交通条件和事件前后的时序运动数据。&lt;h4&gt;主要发现&lt;/h4&gt;SynSHRP2数据集包含1874起碰撞和6924起险情，可用于事件属性分类和场景理解，为安全研究和自动驾驶系统开发提供潜力。&lt;h4&gt;结论&lt;/h4&gt;SynSHRP2数据集为安全研究提供了宝贵的资源，并展示了其在自动驾驶系统开发中的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;The paper introduces SynSHRP2, a synthetically created multimodal driving dataset derived from the SHRP 2 NDS, to address the challenge of accessing SCEs data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving-related safety-critical events (SCEs), including crashes andnear-crashes, provide essential insights for the development and safetyevaluation of automated driving systems. However, two major challenges limittheir accessibility: the rarity of SCEs and the presence of sensitive privacyinformation in the data. The Second Strategic Highway Research Program (SHRP 2)Naturalistic Driving Study (NDS), the largest NDS to date, collected millionsof hours of multimodal, high-resolution, high-frequency driving data fromthousands of participants, capturing thousands of SCEs. While this dataset isinvaluable for safety research, privacy concerns and data use restrictionssignificantly limit public access to the raw data. To address these challenges,we introduce SynSHRP2, a publicly available, synthetic, multimodal drivingdataset containing over 1874 crashes and 6924 near-crashes derived from theSHRP 2 NDS. The dataset features de-identified keyframes generated using StableDiffusion and ControlNet, ensuring the preservation of critical safety-relatedinformation while eliminating personally identifiable data. Additionally,SynSHRP2 includes detailed annotations on SCE type, environmental and trafficconditions, and time-series kinematic data spanning 5 seconds before and duringeach event. Synchronized keyframes and narrative descriptions further enhanceits usability. This paper presents two benchmarks for event attributeclassification and scene understanding, demonstrating the potentialapplications of SynSHRP2 in advancing safety research and automated drivingsystem development.</description>
      <author>example@mail.com (Liang Shi, Boyu Jiang, Zhenyuan Yuan, Miguel A. Perez, Feng Guo)</author>
      <guid isPermaLink="false">2505.06276v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders</title>
      <link>http://arxiv.org/abs/2505.06316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHCOMP的新方法，用于科学数据的错误受限有损压缩，该方法通过利用图神经网络技术来提高压缩比并控制数据失真。&lt;h4&gt;背景&lt;/h4&gt;科学数据的生成量巨大，对存储、传输和分析提出了挑战。现有的错误受限有损压缩方法往往忽略了科学数据中固有的空间和时间相关性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图基于方法，用于实现科学数据的错误受限有损压缩，同时保持较高的压缩比和较低的数据失真。&lt;h4&gt;方法&lt;/h4&gt;对原始网格数据进行不规则分割，生成保留空间和时间相关性的图表示。利用图神经网络设计一个时间图自编码器，学习压缩后的图表示。解压缩过程使用学习到的图模型和潜在表示来重建近似原始数据。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPHCOMP在多个数据集上实现了最高的压缩比，比第二好的方法高出22%至50%。&lt;h4&gt;结论&lt;/h4&gt;GRAPHCOMP是一种有效的科学数据错误受限有损压缩方法，能够显著提高压缩比，同时满足用户定义的误差界限。&lt;h4&gt;翻译&lt;/h4&gt;The generation of large amounts of scientific data poses significant challenges for efficient storage, transfer, and analysis. Recently, error-bounded lossy compression methods have emerged due to their ability to achieve high compression ratios while controlling data distortion. However, they often overlook the inherent spatial and temporal correlations within scientific data, thus missing opportunities for higher compression. In this paper, we propose GRAPHCOMP, a novel graph-based method for error-bounded lossy compression of scientific data. We perform irregular segmentation of the original grid data and generate a graph representation that preserves the spatial and temporal correlations. Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph autoencoder to learn latent representations that significantly reduce the size of the graph, effectively compressing the original data. Decompression reverses the process and utilizes the learned graph model together with the latent representation to reconstruct an approximation of the original data. The decompressed data are guaranteed to satisfy a user-defined point-wise error bound. We compare our method against the state-of-the-art error-bounded lossy methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic data. GRAPHCOMP consistently achieves the highest compression ratio across most datasets, outperforming the second-best method by margins ranging from 22% to 50%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generation of voluminous scientific data poses significant challenges forefficient storage, transfer, and analysis. Recently, error-bounded lossycompression methods emerged due to their ability to achieve high compressionratios while controlling data distortion. However, they often overlook theinherent spatial and temporal correlations within scientific data, thus missingopportunities for higher compression. In this paper we propose GRAPHCOMP, anovel graph-based method for error-bounded lossy compression of scientificdata. We perform irregular segmentation of the original grid data and generatea graph representation that preserves the spatial and temporal correlations.Inspired by Graph Neural Networks (GNNs), we then propose a temporal graphautoencoder to learn latent representations that significantly reduce the sizeof the graph, effectively compressing the original data. Decompression reversesthe process and utilizes the learnt graph model together with the latentrepresentation to reconstruct an approximation of the original data. Thedecompressed data are guaranteed to satisfy a user-defined point-wise errorbound. We compare our method against the state-of-the-art error-bounded lossymethods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and syntheticdata. GRAPHCOMP consistently achieves the highest compression ratio across mostdatasets, outperforming the second-best method by margins ranging from 22% to50%.</description>
      <author>example@mail.com (Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Ibrahim Hoteit, Panos Kalnis)</author>
      <guid isPermaLink="false">2505.06316v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Improving Generalization of Medical Image Registration Foundation Model</title>
      <link>http://arxiv.org/abs/2505.06527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了可变形配准在医学图像处理中的重要性，以及如何通过结合Sharpness-Aware Minimization (SAM)技术来提升基础模型在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;可变形配准是医学图像处理中的基本任务，旨在通过建立非线性对应关系实现图像的精确对齐。传统方法在适应性和可解释性方面表现良好，但计算效率有限。深度学习方法提高了配准速度和准确性，但通常缺乏灵活性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的局限性，本文提出将SAM技术整合到基础模型中，以提高其在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过优化损失函数的平坦度，SAM技术增强了模型在不同数据分布下的稳定性，并提升了其处理复杂临床场景的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，整合了SAM的基础模型在跨数据集配准性能方面取得了显著提升，为医学图像配准技术的进步提供了新的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法通过结合SAM技术，有效提升了基础模型在医学图像配准中的性能，为该领域的研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;This paper discusses the importance of deformable registration in medical image processing and proposes the integration of Sharpness-Aware Minimization (SAM) technology to enhance the generalization and robustness of foundation models in medical image registration.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/promise13/fm_sam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deformable registration is a fundamental task in medical image processing,aiming to achieve precise alignment by establishing nonlinear correspondencesbetween images. Traditional methods offer good adaptability andinterpretability but are limited by computational efficiency. Although deeplearning approaches have significantly improved registration speed andaccuracy, they often lack flexibility and generalizability across differentdatasets and tasks. In recent years, foundation models have emerged as apromising direction, leveraging large and diverse datasets to learn universalfeatures and transformation patterns for image registration, thus demonstratingstrong cross-task transferability. However, these models still face challengesin generalization and robustness when encountering novel anatomical structures,varying imaging conditions, or unseen modalities. To address these limitations,this paper incorporates Sharpness-Aware Minimization (SAM) into foundationmodels to enhance their generalization and robustness in medical imageregistration. By optimizing the flatness of the loss landscape, SAM improvesmodel stability across diverse data distributions and strengthens its abilityto handle complex clinical scenarios. Experimental results show that foundationmodels integrated with SAM achieve significant improvements in cross-datasetregistration performance, offering new insights for the advancement of medicalimage registration technology. Our code is available athttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.</description>
      <author>example@mail.com (Jing Hu, Kaiwei Yu, Hongjiang Xian, Shu Hu, Xin Wang)</author>
      <guid isPermaLink="false">2505.06527v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere</title>
      <link>http://arxiv.org/abs/2505.06474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。&lt;h4&gt;背景&lt;/h4&gt;现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种条件生成模型作为替代方案，并展示cBottle框架在气候模拟和再分析中的应用。&lt;h4&gt;方法&lt;/h4&gt;cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。&lt;h4&gt;主要发现&lt;/h4&gt;cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。&lt;h4&gt;结论&lt;/h4&gt;cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。然而，现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。为此，本文提出条件生成模型作为替代方案，并展示了cBottle框架在气候模拟和再分析中的应用。cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI emulators offer a path to compressing, boosting limited ensembles, andimproving the latency of interacting with petabyte-scale climate predictiondata. However, prevailing auto-regressive paradigms offer limited flexibility,and are challenging to train on climate time horizons due to drifts,instabilities and component-coupling challenges. Conditionally generativemodels offer an appealing alternative. In this context we demonstrate agenerative diffusion-based framework -- Climate in a Bottle (cBottle) -- foremulating global km-scale climate simulations and reanalysis on the equal-areaHEALPix grid. cBottle consists of two model stages: a globally-trainedcoarse-resolution image generator that generates 100km (50k-pixel) fields givenmonthly average sea surface temperatures and solar conditioning, followed by alocally-trained 16x super-resolution stage that generates 5km (12.5M-pixel)fields; global super-resolution is made affordable using an overlappingpatch-based multi-diffusion. Overall, cBottle shows promise as an emulatoracross a battery of climate model diagnostics, including diurnal-to-seasonalscale variability, large-scale modes of variability, tropical cyclonestatistics, and trends of climate change and weather extremes. Moreover,cBottle is a step towards a foundation model, by bridging multiple datamodalities (reanalysis and simulation) with corresponding utility beyondemulation to tasks such as zero-shot bias correction, climate downscaling, andchannel in-filling.</description>
      <author>example@mail.com (Noah D. Brenowitz, Tao Ge, Akshay Subramaniam, Aayush Gupta, David M. Hall, Morteza Mardani, Arash Vahdat, Karthik Kashinath, Michael S. Pritchard)</author>
      <guid isPermaLink="false">2505.06474v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Stochastic Variational Propagation (SVP)方法，作为深度学习中的反向传播（BP）的替代方案，以提高可扩展性和减少内存占用。&lt;h4&gt;背景&lt;/h4&gt;反向传播（BP）依赖于全局梯度同步，这限制了其可扩展性并带来显著的内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP方法，将训练重新构造成层次化的变分推断，以实现可扩展的训练。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了防止层间表示的崩溃，SVP使用固定随机矩阵将激活投影到低维空间，同时结合特征对齐损失以保持层间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并带来显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，作为一种可扩展的替代方案，将训练重新构造成层次化的变分推断。SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能导致层间表示的崩溃，因为过度压缩。为了防止这种情况，SVP通过固定随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合特征对齐损失以保持层间一致性，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A New DAPO Algorithm for Stock Trading</title>
      <link>http://arxiv.org/abs/2505.06408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE IDS 2025 Special Track: Financial Reinforcement  Learning and Foundation Models (FinRLFM). 3 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究强化学习在金融交易中的应用，设计了一种结合改进的GRPO算法、DAPO思想和LLM提取的风险和情绪信号的交易代理。&lt;h4&gt;背景&lt;/h4&gt;强化学习在语言模型配合下表现优异，但其在金融交易中的潜力尚待探索。&lt;h4&gt;目的&lt;/h4&gt;探讨是否可以通过强化学习实现金融交易中的类似增益。&lt;h4&gt;方法&lt;/h4&gt;设计了一种交易代理，结合改进的GRPO算法、DAPO思想和从金融新闻中提取的LLM风险和情绪信号。&lt;h4&gt;主要发现&lt;/h4&gt;在NASDAQ-100指数上，该代理实现了230.49%的累计回报和0.37的信息比率，优于CPPO-DeepSeek基准。同时，训练时间缩短至2.5小时，RAM使用量显著降低。&lt;h4&gt;结论&lt;/h4&gt;提出的RL-LLM框架为数据高效交易代理提供了一个可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;The study explores the application of reinforcement learning in financial trading, designing a trading agent that combines an improved GRPO algorithm, DAPO ideas, and LLM-based risk and sentiment signals extracted from financial news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in reinforcement learning, such as Dynamic Sampling PolicyOptimization (DAPO), show strong performance when paired with large languagemodels (LLMs). Motivated by this success, we ask whether similar gains can berealized in financial trading. We design a trading agent that combines animproved Group Relative Policy Optimization (GRPO) algorithm, augmented withideas from DAPO, with LLM-based risk and sentiment signals extracted fromfinancial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains acumulative return of 230.49 percent and an information ratio of 0.37,outperforming the CPPO-DeepSeek baseline. It also cuts training time from about8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. Theproposed RL-LLM framework offers a scalable path toward data-efficient tradingagents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/</description>
      <author>example@mail.com (Ruijian Zha, Bojun Liu)</author>
      <guid isPermaLink="false">2505.06408v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.06301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于边缘增强的图神经网络框架，用于解决人类活动识别（HAR）中的跨用户变异性问题。&lt;h4&gt;背景&lt;/h4&gt;由于传感器放置、身体动态和行为模式的不同，跨用户变异性在人类活动识别中仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够捕捉用户间共有的生物力学不变量，从而提高传统方法的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该方法将解剖学相关性知识整合到一个统一的图神经网络架构中，通过编码领域不变特征来解决用户特定变异性，并使用变分边缘特征提取器来处理这个问题。梯度反转层（GRL）用于执行对抗性领域泛化，确保对未见用户的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在OPPORTUNITY和DSADS数据集上的大量实验表明，该方法取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过信息融合技术，本文将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在人类活动识别（HAR）中，由于传感器放置、身体动态和行为模式的不同，跨用户变异性仍然是一个关键挑战。传统的方 法往往无法捕捉用户间持续存在的生物力学不变量，限制了它们的泛化能力。我们提出了一种边缘增强的图神经网络（GNN）架构的对抗性领域泛化（EEG-ADG）框架，将解剖学相关性知识整合到统一框架中。通过建模三个生物力学动机关系——相互连接单元、类似单元和侧向单元——我们的方法编码了领域不变特征，并通过变分边缘特征提取器解决了用户特定变异性问题。梯度反转层（GRL）执行对抗性领域泛化，确保对未见用户的鲁棒性。在OPPORTUNITY和DSADS数据集上的大量实验证明了该方法的最佳性能。通过信息融合技术，我们的工作将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-user variability in Human Activity Recognition (HAR) remains a criticalchallenge due to differences in sensor placement, body dynamics, and behavioralpatterns. Traditional methods often fail to capture biomechanical invariantsthat persist across users, limiting their generalization capability. We proposean Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)framework that integrates anatomical correlation knowledge into a unified graphneural network (GNN) architecture. By modeling three biomechanically motivatedrelationships together-Interconnected Units, Analogous Units, and LateralUnits-our method encodes domain-invariant features while addressinguser-specific variability through Variational Edge Feature Extractor. AGradient Reversal Layer (GRL) enforces adversarial domain generalization,ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY andDSADS datasets demonstrate state-of-the-art performance. Our work bridgesbiomechanical principles with graph-based adversarial learning by integratinginformation fusion techniques. This fusion of information underpins our unifiedand generalized model for cross-user HAR.</description>
      <author>example@mail.com (Xiaozhou Ye, Kevin I-Kai Wang)</author>
      <guid isPermaLink="false">2505.06301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.06282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted by SIGIR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IFL-GCL的图对比学习方法，旨在解决传统GCL在语义相似对被错误分类为负样本导致的采样偏差问题，并通过在图预训练框架和LLM增强器中实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）在图基础模型或LLM作为图增强器的研究中发挥着关键作用。然而，传统的GCL方法通过数据增强定义自监督任务，导致语义相似对被错误分类为负样本，从而限制了性能。&lt;h4&gt;目的&lt;/h4&gt;将GCL视为正未标记（PU）学习问题，并通过语义指导的自监督任务定义来改进GCL的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为IFL-GCL的方法，使用InfoNCE来提取语义信息，并通过证明在InfoNCE下节点对的表示相似性与对应对比样本为正样本的概率一致，重新定义了基于校正样本的最大似然目标，从而得到一个新的InfoNCE损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在图预训练框架和LLM作为增强器的情况下，IFL-GCL在独立同分布（IID）和无关领域（OOD）场景中都实现了显著的性能提升，最高达到了9.05%的改进。&lt;h4&gt;结论&lt;/h4&gt;语义指导的IFL-GCL方法验证了其在图对比学习中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating augmented pairs as positive samples and others as negative. However, this leads to semantically similar pairs being classified as negative, causing significant sampling bias and limiting performance. In this paper, we argue that GCL is essentially a Positive-Unlabeled (PU) learning problem, where the definition of self-supervised tasks should be semantically guided, i.e., augmented samples with similar semantics are considered positive, while others, with unknown semantics, are treated as unlabeled. From this perspective, the key lies in how to extract semantic information. To achieve this, we propose IFL-GCL, using InfoNCE as a 'free lunch' to extract semantic information. Specifically, we first prove that under InfoNCE, the representation similarity of node pairs aligns with the probability that the corresponding contrastive sample is positive. Then we redefine the maximum likelihood objective based on the corrected samples, leading to a new InfoNCE loss function. Extensive experiments on both the graph pretraining framework and LLM as an enhancer show significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving up to a 9.05% improvement, validating the effectiveness of semantically guided. Code for IFL-GCL is publicly available at: https://github.com/Camel-Prince/IFL-GCL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3726302.3730007&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an important graph pre-training method, Graph Contrastive Learning (GCL)continues to play a crucial role in the ongoing surge of research on graphfoundation models or LLM as enhancer for graphs. Traditional GCL optimizesInfoNCE by using augmentations to define self-supervised tasks, treatingaugmented pairs as positive samples and others as negative. However, this leadsto semantically similar pairs being classified as negative, causing significantsampling bias and limiting performance. In this paper, we argue that GCL isessentially a Positive-Unlabeled (PU) learning problem, where the definition ofself-supervised tasks should be semantically guided, i.e., augmented sampleswith similar semantics are considered positive, while others, with unknownsemantics, are treated as unlabeled. From this perspective, the key lies in howto extract semantic information. To achieve this, we propose IFL-GCL, usingInfoNCE as a "free lunch" to extract semantic information. Specifically, Wefirst prove that under InfoNCE, the representation similarity of node pairsaligns with the probability that the corresponding contrastive sample ispositive. Then we redefine the maximum likelihood objective based on thecorrected samples, leading to a new InfoNCE loss function. Extensiveexperiments on both the graph pretraining framework and LLM as an enhancer showsignificantly improvements of IFL-GCL in both IID and OOD scenarios, achievingup to a 9.05% improvement, validating the effectiveness of semantically guided.Code for IFL-GCL is publicly available at:https://github.com/Camel-Prince/IFL-GCL.</description>
      <author>example@mail.com (Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng)</author>
      <guid isPermaLink="false">2505.06282v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume</title>
      <link>http://arxiv.org/abs/2505.06292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Graph Neural Network for Urban Interpolation (GNNUI)，一种新型的城市交通流量估计方法，并提出了两个新的开放的大规模城市交通流量基准。&lt;h4&gt;背景&lt;/h4&gt;交通流量数据对城市规划至关重要，但交通传感器部署和维护成本高，导致数据稀疏。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的城市交通流量估计方法，解决交通数据稀疏的问题。&lt;h4&gt;方法&lt;/h4&gt;GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。同时，提出了两个新的城市交通流量基准：柏林的Strava自行车数据和纽约市的出租车数据。&lt;h4&gt;主要发现&lt;/h4&gt;GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于其他基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。在Strava和出租车数据集上，GNNUI在极端数据稀缺的情况下表现良好，MAE仅略有增加。&lt;h4&gt;结论&lt;/h4&gt;GNNUI是一种有效且鲁棒的城市交通流量估计方法，有助于解决交通数据稀疏问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：可靠的地面级交通流量数据，涵盖多种交通方式，有助于城市规划，通过提供基础设施改善、交通管理和公共交通决策的信息。然而，由于部署和维护成本高，测量交通流量的传感器通常分布稀疏。为了解决这个问题，插值方法可以使用可用数据估计未观测位置的交通流量。图神经网络在交通流量预测中表现出强大的性能，尤其是在高速公路和主要干道网络上。然而，将它们应用于城市环境带来独特的挑战：城市网络表现出更大的结构多样性，交通流量高度过度分散，存在许多零值，最佳的空间依赖性建模方法尚不清楚，传感器覆盖率通常非常稀疏。我们引入了Graph Neural Network for Urban Interpolation（GNNUI），一种新颖的城市交通流量估计方法。GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。除了模型之外，我们还引入了两个新的公开的大规模城市交通流量基准，涵盖了不同的交通方式：柏林和纽约市的Strava自行车数据和出租车数据。GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于最近的基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。例如，在Strava上，MAE仅从7.1增加到10.5，在出租车数据上从23.0增加到40.4，表明在现实世界城市环境中常见的数据稀缺情况下，GNNUI表现良好。我们还考察了图连接选择如何影响模型精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable street-level traffic volume data, covering multiple modes oftransportation, helps urban planning by informing decisions on infrastructureimprovements, traffic management, and public transportation. Yet, trafficsensors measuring traffic volume are typically scarcely located, due to theirhigh deployment and maintenance costs. To address this, interpolation methodscan estimate traffic volumes at unobserved locations using available data.Graph Neural Networks have shown strong performance in traffic volumeforecasting, particularly on highways and major arterial networks. Applyingthem to urban settings, however, presents unique challenges: urban networksexhibit greater structural diversity, traffic volumes are highly overdispersedwith many zeros, the best way to account for spatial dependencies remainsunclear, and sensor coverage is often very sparse. We introduce the GraphNeural Network for Urban Interpolation (GNNUI), a novel urban traffic volumeestimation approach. GNNUI employs a masking algorithm to learn interpolation,integrates node features to capture functional roles, and uses a loss functiontailored to zero-inflated traffic distributions. In addition to the model, weintroduce two new open, large-scale urban traffic volume benchmarks, coveringdifferent transportation modes: Strava cycling data from Berlin and New YorkCity taxi data. GNNUI outperforms recent, some graph-based, interpolationmethods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAErises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strongperformance under extreme data scarcity, common in real-world urban settings.We also examine how graph connectivity choices influence model accuracy.</description>
      <author>example@mail.com (Silke K. Kaiser, Filipe Rodrigues, Carlos Lima Azevedo, Lynn H. Kaack)</author>
      <guid isPermaLink="false">2505.06292v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Soft causal learning for generalized molecule property prediction: An environment perspective</title>
      <link>http://arxiv.org/abs/2505.06283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。&lt;h4&gt;背景&lt;/h4&gt;分子图学习在AI科学中日益重要，但现有方法在处理样本外分布样本时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，以充分建模分子环境并绕过不变子图，从而解决分子科学中的样本外分布挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 将化学理论融入图增长生成器以模仿扩展环境；2) 设计基于GIB的目标函数以将环境从整个图中分离出来；3) 引入基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。&lt;h4&gt;主要发现&lt;/h4&gt;1) 扩展的原子模式导致基于不变理性模型的失败；2) 发现的分子子图与对应属性之间的关联复杂，因果子结构无法完全解释标签；3) 环境和不变性之间的相互作用相互影响，难以建模。&lt;h4&gt;结论&lt;/h4&gt;实验表明，所提出的框架具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。在AI科学中，分子图学习已成为一个日益重要的主题，但现有方法在处理样本外分布样本时存在局限性。本文提出了一种框架，旨在通过充分建模分子环境并绕过不变子图来解决分子科学中的样本外分布挑战。具体来说，本文首先将化学理论融入图增长生成器以模仿扩展环境，然后设计了一种基于GIB的目标函数以将环境从整个图中分离出来，最后引入了一种基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。通过在七个数据集上进行的实验，包括模仿不同类型的样本外泛化场景，广泛的比较、消融实验以及可视化案例研究，证明了所提出框架的良好泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on molecule graphs has become an increasingly important topic in AIfor science, which takes full advantage of AI to facilitate scientificdiscovery. Existing solutions on modeling molecules utilize Graph NeuralNetworks (GNNs) to achieve representations but they mostly fail to adapt modelsto out-of-distribution (OOD) samples. Although recent advances on OOD-orientedgraph learning have discovered the invariant rationale on graphs, they stillignore three important issues, i.e., 1) the expanding atom patterns regardingenvironments on graphs lead to failures of invariant rationale based models, 2)the associations between discovered molecular subgraphs and correspondingproperties are complex where causal substructures cannot fully interpret thelabels. 3) the interactions between environments and invariances can influencewith each other thus are challenging to be modeled. To this end, we propose asoft causal learning framework, to tackle the unresolved OOD challenge inmolecular science, from the perspective of fully modeling the moleculeenvironments and bypassing the invariant subgraphs. Specifically, we firstincorporate chemistry theories into our graph growth generator to imitateexpaned environments, and then devise an GIB-based objective to disentangleenvironment from whole graphs and finally introduce a cross-attention basedsoft causal interaction, which allows dynamic interactions between environmentsand invariances. We perform experiments on seven datasets by imitatingdifferent kinds of OOD generalization scenarios. Extensive comparison, ablationexperiments as well as visualized case studies demonstrate well generalizationability of our proposal.</description>
      <author>example@mail.com (Limin Li, Kuo Yang, Wenjie Du, Pengkun Wang, Zhengyang Zhou, Yang Wang)</author>
      <guid isPermaLink="false">2505.06283v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Mogao的统一框架，该框架通过因果方法实现交错的多模态生成，并在架构设计上进行了多项关键技术改进。&lt;h4&gt;背景&lt;/h4&gt;尽管统一模型在图像理解和生成方面取得了显著进展，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，以实现交错的多模态生成，并提升多模态理解和文本到图像生成的性能。&lt;h4&gt;方法&lt;/h4&gt;Mogao通过以下技术改进实现其目标：深度融合设计、双重视觉编码器、交错旋转位置嵌入和多模态无分类器引导。此外，还引入了一种高效的大规模数据集训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，同时擅长生成高质量、连贯的交错输出。其在零样本图像编辑和组合生成方面的能力，使其成为实用的全模态基础模型。&lt;h4&gt;结论&lt;/h4&gt;Mogao为统一多模态系统的发展开辟了道路，并具有未来扩展的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>http://arxiv.org/abs/2505.02350v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用稀疏椭圆径向基函数网络逼近点云符号距离函数（SDF）的机器学习方法，实现了紧凑且精确的表面表示。&lt;h4&gt;背景&lt;/h4&gt;点云表面表示是计算机图形学和视觉领域的一个基本问题。&lt;h4&gt;目的&lt;/h4&gt;目的是通过尽可能少的椭圆径向基函数（ERBFs）精确地逼近点云的SDF，以实现SDF的稀疏表示。&lt;h4&gt;方法&lt;/h4&gt;方法包括：引入动态多目标优化策略平衡稀疏性和逼近精度；采用基于最近邻的数据结构提高计算效率；在CUDA上并行化每个核的计算；以及设计基于八叉树的细化策略进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为点云表面表示提供了一种高效且准确的新途径，其对应的可执行程序已公开。&lt;h4&gt;翻译&lt;/h4&gt;Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using a sparse ellipsoidal radial basis function network, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding executable program is publicly available at https://github.com/lianbobo/SE-RBFNet.git.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lianbobo/se-rbfnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud surface representation is a fundamental problem in computergraphics and vision. This paper presents a machine learning approach forapproximating the signed distance function (SDF) of a point cloud using asparse ellipsoidal radial basis function network, enabling a compact andaccurate surface representation. Given the SDF values defined on the gridpoints constructed from the point cloud, our method approximates the SDFaccurately with as few ellipsoidal radial basis functions (ERBFs) as possible,i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsityand approximation precision, a dynamic multi-objective optimization strategy isintroduced, which adaptively adds the regularization terms and jointlyoptimizes the weights, centers, shapes, and orientations of ERBFs. To improvecomputational efficiency, a nearest-neighbor-based data structure is employed,restricting function calculations to points near each Gaussian kernel center.The computations for each kernel are further parallelized on CUDA, whichsignificantly improves the optimization speed. Additionally, a hierarchicaloctree-based refinement strategy is designed for training. Specifically, theinitialization and optimization of network parameters are conducted usingcoarse grid points in the octree lattice structure. Subsequently, fine latticepoints are progressively incorporated to accelerate model convergence andenhance training efficiency. Extensive experiments on multiple benchmarkdatasets demonstrate that our method outperforms previous sparse representationapproaches in terms of accuracy, robustness, and computational efficiency. Thecorresponding executable program is publicly available athttps://github.com/lianbobo/SE-RBFNet.git.</description>
      <author>example@mail.com (Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen)</author>
      <guid isPermaLink="false">2505.02350v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法，在此基础上，开发创新计算方法以实现高性能的自动疼痛评估，并适用于真实临床环境。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论和现有自动疼痛评估方法出发，探讨了疼痛感知的影响因素。&lt;h4&gt;目的&lt;/h4&gt;主要目的是开发适用于不同场景的自动疼痛评估流程，并研究影响疼痛感知的显著因素。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法研究影响疼痛感知的人口统计学元素，设计、开发、提出并提供了适用于单模态和多模态配置的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中的研究展示了所提出方法的有效性，实现了最先进的结果，并为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;本文提出的自动疼痛评估方法在临床环境中具有应用潜力，并为人工智能领域的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract: This thesis initially aims to study the painassessment process from a clinical-theoretical perspective while exploring andexamining existing automatic approaches. Building on this foundation, theprimary objective of this Ph.D. project is to develop innovative computationalmethods for automatic pain assessment that achieve high performance and areapplicable in real clinical settings. A primary goal is to thoroughlyinvestigate and assess significant factors, including demographic elements thatimpact pain perception, as recognized in pain research, through a computationalstandpoint. Within the limits of the available data in this research area, ourgoal was to design, develop, propose, and offer automatic pain assessmentpipelines for unimodal and multimodal configurations that are applicable to thespecific requirements of different scenarios. The studies published in thisPh.D. thesis showcased the effectiveness of the proposed methods, achievingstate-of-the-art results. Additionally, they paved the way for exploring newapproaches in artificial intelligence, foundation models, and generativeartificial intelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
      <link>http://arxiv.org/abs/2505.06291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALFEE的新型混合Transformer架构，用于EEG信号表示学习，以解决现有EEG模型在信号噪声比、个体差异和跨范式差异方面的问题。&lt;h4&gt;背景&lt;/h4&gt;虽然基础模型在文本、图像和视频领域表现出色，但关键生物信号，尤其是脑电图（EEG），仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决EEG模型在信号处理和泛化方面的局限性，提出ALFEE框架。&lt;h4&gt;方法&lt;/h4&gt;ALFEE采用混合注意力机制，将通道特征聚合与时间动态建模分离，具有两个学习阶段：预训练和微调。预训练阶段优化任务预测、通道和时间掩码重建以及时间预测，而微调阶段使用特定任务的标记字典和交叉注意力层。&lt;h4&gt;主要发现&lt;/h4&gt;ALFEE在六个下游EEG任务上表现出优于现有模型的效果，经过25,000小时的预训练后，在多任务上提升了性能。&lt;h4&gt;结论&lt;/h4&gt;ALFEE框架为生物信号分析提供了一个可扩展的基础，其实现在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然基础模型在文本、图像和视频领域表现出色，但关键的生物信号，尤其是脑电图（EEG），仍然没有得到充分的探索。EEG由于其高时间分辨率、操作实用性和安全性，为神经科学研究提供了益处。然而，低信号噪声比、个体差异和跨范式差异阻碍了现有模型的泛化。现有的方法通常采用简化的策略，如使用单个损失函数或通道-时间联合表示模块，并且在预训练和评估任务之间存在领域差距，这损害了效率和适应性。为了解决这些局限性，我们提出了自适应大型基础模型用于EEG信号表示（ALFEE）框架，这是一种新颖的混合Transformer架构，具有两个学习阶段，用于鲁棒的EEG表示学习。ALFEE采用混合注意力，将通道特征聚合与时间动态建模分离，使具有可变通道配置的EEG表示鲁棒。通道编码器自适应地压缩可变通道信息，时间编码器捕获任务指导的演变，混合解码器在时间和频率域中重建信号。在预训练期间，ALFEE优化任务预测、通道和时间掩码重建以及时间预测，以增强多尺度和多通道表示。在微调期间，使用特定任务的标记字典和交叉注意力层进行全模型适应性，以提升跨多个任务的表现。经过25,000小时的预训练后，在六个下游EEG任务上的大量实验结果表明，ALFEE的性能优于现有模型。我们的ALFEE框架为生物信号分析建立了一个可扩展的基础，其实现在https://github.com/xw1216/ALFEE上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models excel in text, image, and video domains, the criticalbiological signals, particularly electroencephalography(EEG), remainunderexplored. EEG benefits neurological research with its high temporalresolution, operational practicality, and safety profile. However, lowsignal-to-noise ratio, inter-subject variability, and cross-paradigmdifferences hinder the generalization of current models. Existing methods oftenemploy simplified strategies, such as a single loss function or achannel-temporal joint representation module, and suffer from a domain gapbetween pretraining and evaluation tasks that compromises efficiency andadaptability. To address these limitations, we propose the Adaptive LargeFoundation model for EEG signal representation(ALFEE) framework, a novel hybridtransformer architecture with two learning stages for robust EEG representationlearning. ALFEE employs a hybrid attention that separates channel-wise featureaggregation from temporal dynamics modeling, enabling robust EEG representationwith variable channel configurations. A channel encoder adaptively compressesvariable channel information, a temporal encoder captures task-guidedevolution, and a hybrid decoder reconstructs signals in both temporal andfrequency domains. During pretraining, ALFEE optimizes task prediction, channeland temporal mask reconstruction, and temporal forecasting to enhancemulti-scale and multi-channel representation. During fine-tuning, a full-modeladaptation with a task-specific token dictionary and a cross-attention layerboosts performance across multiple tasks. After 25,000 hours of pretraining,extensive experimental results on six downstream EEG tasks demonstrate thesuperior performance of ALFEE over existing models. Our ALFEE frameworkestablishes a scalable foundation for biological signal analysis withimplementation at https://github.com/xw1216/ALFEE.</description>
      <author>example@mail.com (Wei Xiong, Junming Lin, Jiangtong Li, Jie Li, Changjun Jiang)</author>
      <guid isPermaLink="false">2505.06291v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation</title>
      <link>http://arxiv.org/abs/2505.06288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的等距沉浸核学习方法（IIKL），用于从离散的非欧几里得数据中构建黎曼流形并等距地诱导黎曼度量，以保持数据内在的几何和拓扑属性。&lt;h4&gt;背景&lt;/h4&gt;在科学应用中，保留离散非欧几里得数据的内在几何和拓扑属性对于几何表示学习至关重要。传统方法通常将非欧几里得离散数据映射到欧几里得空间，可能导致关键几何信息的丢失。&lt;h4&gt;目的&lt;/h4&gt;提出IIKL方法，以保持离散非欧几里得数据的几何结构，并提高下游任务（如数据重建和分类）的准确性。&lt;h4&gt;方法&lt;/h4&gt;IIKL方法等距沉浸黎曼流形，并通过最大似然估计（MLE）导出参数化学习模型和交替训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用IIKL学习到的黎曼流形及其度量，模型在3D和高维数据集中成功地保持了数据的内在几何表示，显著提高了下游任务的准确性，如数据重建和分类。与最先进的（SOTA）方法相比，该方法可以减少超过90%的内积不变损失，平均提高了40%的下游重建准确性，并在涉及等距和共形的几何度量方面减少了90%的错误。&lt;h4&gt;结论&lt;/h4&gt;IIKL方法在保持离散非欧几里得数据的几何结构方面表现出色，对于下游任务如数据重建和分类有显著的提升效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric representation learning in preserving the intrinsic geometric andtopological properties for discrete non-Euclidean data is crucial in scientificapplications. Previous research generally mapped non-Euclidean discrete datainto Euclidean space during representation learning, which may lead to the lossof some critical geometric information. In this paper, we propose a novelIsometric Immersion Kernel Learning (IIKL) method to build Riemannian manifoldand isometrically induce Riemannian metric from discrete non-Euclidean data. Weprove that Isometric immersion is equivalent to the kernel function in thetangent bundle on the manifold, which explicitly guarantees the invariance ofthe inner product between vectors in the arbitrary tangent space throughout thelearning process, thus maintaining the geometric structure of the originaldata. Moreover, a novel parameterized learning model based on IIKL isintroduced, and an alternating training method for this model is derived usingMaximum Likelihood Estimation (MLE), ensuring efficient convergence.Experimental results proved that using the learned Riemannian manifold and itsmetric, our model preserved the intrinsic geometric representation of data inboth 3D and high-dimensional datasets successfully, and significantly improvedthe accuracy of downstream tasks, such as data reconstruction andclassification. It is showed that our method could reduce the inner productinvariant loss by more than 90% compared to state-of-the-art (SOTA) methods,also achieved an average 40% improvement in downstream reconstruction accuracyand a 90% reduction in error for geometric metrics involving isometric andconformal.</description>
      <author>example@mail.com (Zihao Chen, Wenyong Wang, Jiachen Yang, Yu Xiang)</author>
      <guid isPermaLink="false">2505.06288v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Learning Dynamics in Unsupervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.06279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无监督强化学习（URL）智能体可解释性框架，旨在理解内在动机如何影响注意力、行为和表示学习。&lt;h4&gt;背景&lt;/h4&gt;分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。&lt;h4&gt;目的&lt;/h4&gt;理解智能体如何随时间感知和适应，并引入两个度量指标：注意力多样性和注意力变化率。&lt;h4&gt;方法&lt;/h4&gt;使用Grad-CAM、层级相关性传播（LRP）、探索指标和潜在空间聚类进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;好奇心驱动的智能体比外在动机的智能体表现出更广泛、更动态的注意力和探索行为。TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。&lt;h4&gt;结论&lt;/h4&gt;结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，该框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种针对无监督强化学习（URL）智能体的可解释性框架，旨在理解内在动机如何塑造注意力、行为和表示学习。我们分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。为了捕捉智能体如何随时间感知和适应，我们引入了两个指标：注意力多样性和注意力变化率。我们的研究结果表明，好奇心驱动的智能体比外在动机的智能体表现出更广泛的注意力范围和更动态的探索行为。其中，TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。我们的结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，所提出的框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an interpretability framework for unsupervised reinforcementlearning (URL) agents, aimed at understanding how intrinsic motivation shapesattention, behavior, and representation learning. We analyze five agents DQN,RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generatedenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),exploration metrics, and latent space clustering. To capture how agentsperceive and adapt over time, we introduce two metrics: attention diversity,which measures the spatial breadth of focus, and attention change rate, whichquantifies temporal shifts in attention. Our findings show thatcuriosity-driven agents display broader, more dynamic attention and exploratorybehavior than their extrinsically motivated counterparts. Among them,TransformerRND combines wide attention, high exploration coverage, and compact,structured latent representations. Our results highlight the influence ofarchitectural inductive biases and training signals on internal agent dynamics.Beyond reward-centric evaluation, the proposed framework offers diagnostictools to probe perception and abstraction in RL agents, enabling moreinterpretable and generalizable behavior.</description>
      <author>example@mail.com (Shashwat Pandey)</author>
      <guid isPermaLink="false">2505.06279v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
      <link>http://arxiv.org/abs/2504.17040v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DyMU是一种高效的、无需训练的框架，能够在保持高任务性能的同时动态减少视觉语言模型（VLMs）的计算负担。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理图像和文本时存在计算效率低的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态减少视觉语言模型计算负担的方法，同时保持高任务性能。&lt;h4&gt;方法&lt;/h4&gt;DyMU包含两个关键组件：动态标记合并（DToMe）和虚拟标记解合并（VTU）。DToMe通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，VTU则通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列。&lt;h4&gt;主要发现&lt;/h4&gt;DyMU可以在减少32%-85%的平均视觉标记数量的同时，在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。通过定性分析，DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。&lt;h4&gt;结论&lt;/h4&gt;DyMU是一种适用于大多数最先进VLM架构的有效解决方案，它通过动态适应标记压缩到图像内容，并在不进行额外微调的情况下运行，从而提高了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种高效的、无需训练的框架DyMU，该框架在保持高任务性能的同时动态减少了视觉语言模型（VLMs）的计算负担。我们的方法包含两个关键组件。首先，动态标记合并（DToMe）通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，解决了视觉转换器固定长度输出的固有低效问题。其次，虚拟标记解合并（VTU）通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与先前的方法不同，我们的方法动态地将标记压缩适应到图像内容，并且完全无需训练，使其适用于大多数最先进的VLM架构。在图像和视频理解任务上的大量实验表明，DyMU可以将平均视觉标记数量减少32%-85%，同时在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。此外，通过定性分析，我们证明了DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。项目页面：https://mikewangwzhl.github.io/dymu/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DyMU, an efficient, training-free framework that dynamicallyreduces the computational burden of vision-language models (VLMs) whilemaintaining high task performance. Our approach comprises two key components.First, Dynamic Token Merging (DToMe) reduces the number of visual tokenembeddings by merging similar tokens based on image complexity, addressing theinherent inefficiency of fixed-length outputs in vision transformers. Second,Virtual Token Unmerging (VTU) simulates the expected token sequence for largelanguage models (LLMs) by efficiently reconstructing the attention dynamics ofa full sequence, thus preserving the downstream performance without additionalfine-tuning. Unlike previous approaches, our method dynamically adapts tokencompression to the content of the image and operates completely training-free,making it readily applicable to most state-of-the-art VLM architectures.Extensive experiments on image and video understanding tasks demonstrate thatDyMU can reduce the average visual token count by 32%-85% while achievingcomparable performance to full-length models across diverse VLM architectures,including the recently popularized AnyRes-based visual encoders. Furthermore,through qualitative analyses, we demonstrate that DToMe effectively adaptstoken reduction based on image complexity and, unlike existing systems,provides users more control over computational costs. Project page:https://mikewangwzhl.github.io/dymu/.</description>
      <author>example@mail.com (Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu)</author>
      <guid isPermaLink="false">2504.17040v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction</title>
      <link>http://arxiv.org/abs/2505.04918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Joint Conferences on Artificial Intelligence (IJCAI  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PASSAT是一种新的深度学习模型，用于天气预测，它结合了物理和地球表面拓扑信息，以改进预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的深度学习模型在天气预测中存在忽视物理和地球表面拓扑的问题。&lt;h4&gt;目的&lt;/h4&gt;开发PASSAT模型，以解决现有模型的不足，提高天气预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;PASSAT模型将天气演变归因于两个关键因素：平流过程和地球-大气相互作用。它考虑地球表面的拓扑结构，并在球面上数值求解平流方程和纳维-斯托克斯方程。使用球面图神经网络来捕捉地球-大气相互作用，并生成初始速度场。&lt;h4&gt;主要发现&lt;/h4&gt;在5.625°分辨率的ERA5数据集中，PASSAT优于现有的深度学习模型和IFS T42数值天气预报模型。&lt;h4&gt;结论&lt;/h4&gt;PASSAT模型通过结合物理和拓扑信息，在天气预测方面取得了显著的改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度学习模型在天气预测中显示出巨大的潜力，但大多数模型忽略了底层天气演变的物理或地球表面的拓扑。鉴于这些缺点，我们开发了PASSAT，一种新的物理辅助和拓扑信息深度学习模型，用于天气预测。PASSAT将天气演变归因于两个关键因素：（i）可以由平流方程和纳维-斯托克斯方程表征的平流过程；（ii）难以建模和计算的地球-大气相互作用。PASSAT还考虑了地球表面的拓扑结构，而不仅仅是将其视为平面。在这些考虑的基础上，PASSAT在球面上数值求解平流方程和纳维-斯托克斯方程，利用球面图神经网络来捕捉地球-大气相互作用，并从同一球面图神经网络生成解决平流方程的关键初始速度场。在5.625°分辨率的ERA5数据集中，PASSAT优于基于深度学习的最先进的天气预测模型和IFS T42业务数值天气预报模型。代码和检查点可在https://github.com/Yumenomae/PASSAT_5p625上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yumenomae/passat_5p625&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although deep learning models have demonstrated remarkable potential inweather prediction, most of them overlook either the \textbf{physics} of theunderlying weather evolution or the \textbf{topology} of the Earth's surface.In light of these disadvantages, we develop PASSAT, a novel Physics-ASSistedAnd Topology-informed deep learning model for weather prediction. PASSATattributes the weather evolution to two key factors: (i) the advection processthat can be characterized by the advection equation and the Navier-Stokesequation; (ii) the Earth-atmosphere interaction that is difficult to both modeland calculate. PASSAT also takes the topology of the Earth's surface intoconsideration, other than simply treating it as a plane. With theseconsiderations, PASSAT numerically solves the advection equation and theNavier-Stokes equation on the spherical manifold, utilizes a spherical graphneural network to capture the Earth-atmosphere interaction, and generates theinitial velocity fields that are critical to solving the advection equationfrom the same spherical graph neural network. In the $5.625^\circ$-resolutionERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-basedweather prediction models and the operational numerical weather predictionmodel IFS T42. Code and checkpoint are available athttps://github.com/Yumenomae/PASSAT_5p625.</description>
      <author>example@mail.com (Jiaqi Zheng, Qing Ling, Yerong Feng)</author>
      <guid isPermaLink="false">2505.04918v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
  <item>
      <title>Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</title>
      <link>http://arxiv.org/abs/2505.06224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了模型表示评估的重要性，提出了超越下游探针的方法，并引入了一种标准化协议来量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;背景&lt;/h4&gt;下游探针是评估模型表示的主要方法，但在评估过程中忽略了等变性、不变性和解耦性等属性，这些属性对表示的可解释性、适应性和实用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;强调模型表示评估的重要性，并提出一种统一的评估框架，以量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种标准化协议，用于评估图像和语音领域不同模型、不同架构和预训练方法下的表示，并识别可控的可变因素。&lt;h4&gt;主要发现&lt;/h4&gt;发现具有相似下游性能的模型在信息量、等变性、不变性和解耦性等方面可能表现出显著差异，暗示其下游性能背后的机制功能不同。&lt;h4&gt;结论&lt;/h4&gt;提出新的研究方向，以理解和改进模型表示，并强调在评估模型表示时需要考虑更广泛的属性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Downstream probing has been the dominant method for evaluating modelrepresentations, an important process given the increasing prominence ofself-supervised learning and foundation models. However, downstream probingprimarily assesses the availability of task-relevant information in the model'slatent space, overlooking attributes such as equivariance, invariance, anddisentanglement, which contribute to the interpretability, adaptability, andutility of representations in real-world applications. While some attempts havebeen made to measure these qualities in representations, no unified evaluationframework with modular, generalizable, and interpretable metrics exists.  In this paper, we argue for the importance of representation evaluationbeyond downstream probing. We introduce a standardized protocol to quantifyinformativeness, equivariance, invariance, and disentanglement of factors ofvariation in model representations. We use it to evaluate representations froma variety of models in the image and speech domains using differentarchitectures and pretraining approaches on identified controllable factors ofvariation. We find that representations from models with similar downstreamperformance can behave substantially differently with regard to theseattributes. This hints that the respective mechanisms underlying theirdownstream performance are functionally different, prompting new researchdirections to understand and improve representations.</description>
      <author>example@mail.com (Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels)</author>
      <guid isPermaLink="false">2505.06224v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
      <link>http://arxiv.org/abs/2505.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度学习在淋巴结分割中的应用，并讨论了卷积神经网络、编码器-解码器网络和Transformer等不同深度学习架构在分析不同模态医学影像数据的方法。&lt;h4&gt;背景&lt;/h4&gt;自动淋巴结分割是计算机视觉任务中癌症早期检测和分期的基础。传统的分割方法受限于人工描绘和操作者技能的差异性，限制了其达到高准确性的能力。&lt;h4&gt;目的&lt;/h4&gt;评估深度学习在淋巴结分割中的应用，并探讨解决淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;方法&lt;/h4&gt;本文提供了对深度学习技术在淋巴结分割任务中应用的全面概述，并探索了包括多模态融合技术、迁移学习和使用大规模预训练模型等潜在的未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;尽管深度学习技术在淋巴结分割方面取得了进展，但仍然面临淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;结论&lt;/h4&gt;这是首次对深度学习技术在淋巴结分割任务中的应用进行综合概述的研究，并为未来的研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic lymph node segmentation is the cornerstone for advances in computervision tasks for early detection and staging of cancer. Traditionalsegmentation methods are constrained by manual delineation and variability inoperator proficiency, limiting their ability to achieve high accuracy. Theintroduction of deep learning technologies offers new possibilities forimproving the accuracy of lymph node image analysis. This study evaluates theapplication of deep learning in lymph node segmentation and discusses themethodologies of various deep learning architectures such as convolutionalneural networks, encoder-decoder networks, and transformers in analyzingmedical imaging data across different modalities. Despite the advancements, itstill confronts challenges like the shape diversity of lymph nodes, thescarcity of accurately labeled datasets, and the inadequate development ofmethods that are robust and generalizable across different imaging modalities.To the best of our knowledge, this is the first study that provides acomprehensive overview of the application of deep learning techniques in lymphnode segmentation task. Furthermore, this study also explores potential futureresearch directions, including multimodal fusion techniques, transfer learning,and the use of large-scale pre-trained models to overcome current limitationswhile enhancing cancer diagnosis and treatment planning strategies.</description>
      <author>example@mail.com (Jingguo Qu, Xinyang Han, Man-Lik Chui, Yao Pu, Simon Takadiyi Gunda, Ziman Chen, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying)</author>
      <guid isPermaLink="false">2505.06118v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</title>
      <link>http://arxiv.org/abs/2505.05851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted for presentation at the 7th Workshop on Long-term Human  Motion Prediction (LHMP) at International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在拥挤和遮挡频繁的环境中，应用超宽带（UWB）定位技术作为人类运动捕捉的可扩展替代方案的可能性。&lt;h4&gt;背景&lt;/h4&gt;随着机器人越来越多地融入人类环境，理解和预测人类运动对于安全高效的人机交互至关重要。目前，人类运动和活动预测方法需要高质量和数量的数据进行训练和评估，通常从运动捕捉系统、车载或固定传感器中收集。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种在更大和更复杂环境中（如仓库、机场或会议中心）评估传感方式（如UWB）的可扩展和准确的运动数据收集方法。&lt;h4&gt;方法&lt;/h4&gt;包括额外的传感方式，如眼动追踪、车载机器人激光雷达和雷达传感器，并记录运动捕捉数据作为评估和比较的基准。环境模拟博物馆设置，最多有四个参与者以自然的方式向随机目标移动，提供超过130分钟的多模态数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究为超越基于视觉的系统提供了可扩展和准确的运动数据收集的步骤。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为在更大和更复杂的环境中评估传感方式（如UWB）提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With robots increasingly integrating into human environments, understandingand predicting human motion is essential for safe and efficient interactions.Modern human motion and activity prediction approaches require high quality andquantity of data for training and evaluation, usually collected from motioncapture systems, onboard or stationary sensors. Setting up these systems ischallenging due to the intricate setup of hardware components, extensivecalibration procedures, occlusions, and substantial costs. These constraintsmake deploying such systems in new and large environments difficult and limittheir usability for in-the-wild measurements. In this paper we investigate thepossibility to apply the novel Ultra-Wideband (UWB) localization technology asa scalable alternative for human motion capture in crowded and occlusion-proneenvironments. We include additional sensing modalities such as eye-tracking,onboard robot LiDAR and radar sensors, and record motion capture data as groundtruth for evaluation and comparison. The environment imitates a museum setup,with up to four active participants navigating toward random goals in a naturalway, and offers more than 130 minutes of multi-modal data. Our investigationprovides a step toward scalable and accurate motion data collection beyondvision-based systems, laying a foundation for evaluating sensing modalitieslike UWB in larger and complex environments like warehouses, airports, orconvention centers.</description>
      <author>example@mail.com (Janik Kaden, Maximilian Hilger, Tim Schreiter, Marius Schaab, Thomas Graichen, Andrey Rudenko, Ulrich Heinkel, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2505.05851v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2505.06113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图，实现低成本且精确的环境感知。&lt;h4&gt;背景&lt;/h4&gt;传统的自动驾驶感知系统依赖昂贵的LiDAR传感器来生成精确的环境表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要LiDAR传感器的低成本感知框架，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;结合YOLOv1和DepthAnythingV2技术，通过多摄像头输入实现物体检测和单目深度估计，以实现全面的360度场景理解。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2和NuScenes数据集上评估，该方法在道路分割精度达到85%，车辆检测率在85-90%，平均位置误差限制在1.2米。&lt;h4&gt;结论&lt;/h4&gt;深度学习有潜力仅使用摄像头输入提取丰富的空间信息，从而实现低成本且不牺牲精度的自主导航。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动驾驶车辆的感知系统传统上依赖昂贵的LiDAR传感器来生成精确的环境表示。在本文中，我们提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图。我们的方法结合了基于YOLOv1的对象检测和DepthAnythingV2的单目深度估计，通过多摄像头输入实现全面的360度场景理解。我们在OpenLane-V2和NuScenes数据集上评估了我们的方法，与LiDAR地面真实值相比，实现了高达85%的道路分割精度和85-90%的车辆检测率，平均位置误差限制在1.2米。这些结果突出了深度学习仅使用摄像头输入提取丰富空间信息的潜力，使得低成本且不牺牲精度的自主导航成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicle perception systems have traditionally relied on costlyLiDAR sensors to generate precise environmental representations. In this paper,we propose a camera-only perception framework that produces Bird's Eye View(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combinesYOLOv11-based object detection with DepthAnythingV2 monocular depth estimationacross multi-camera inputs to achieve comprehensive 360-degree sceneunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenesdatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicledetection rates when compared against LiDAR ground truth, with averagepositional errors limited to 1.2 meters. These results highlight the potentialof deep learning to extract rich spatial information using only camera inputs,enabling cost-efficient autonomous navigation without sacrificing accuracy.</description>
      <author>example@mail.com (Anupkumar Bochare)</author>
      <guid isPermaLink="false">2505.06113v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</title>
      <link>http://arxiv.org/abs/2505.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LRW-OOD的图Out-Of-Distribution（OOD）泛化学习方法，旨在解决图神经网络在分布偏移下的性能退化问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在分布偏移的情况下表现不佳，现有的图OOD方法通常基于不变风险最小化和结构因果模型，但它们可能不符合现实情况。&lt;h4&gt;目的&lt;/h4&gt;提出可学习的随机游走（LRW）作为不变知识的实例，以实现图OOD泛化学习。&lt;h4&gt;方法&lt;/h4&gt;LRW-OOD使用可学习的随机游走采样器和路径编码器来参数化转移矩阵，并提出基于核密度估计（KDE）的互信息（MI）损失来生成符合OOD原则的随机游走序列。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该模型可以在各种类型的分布偏移下有效地增强图OOD泛化，并且相对于最先进的图OOD泛化基线，准确率提高了3.87%。&lt;h4&gt;结论&lt;/h4&gt;LRW-OOD是一种有效的图OOD泛化学习方法，能够显著提高模型在分布偏移情况下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-Of-Distribution (OOD) generalization has gained increasing attentions formachine learning on graphs, as graph neural networks (GNNs) often exhibitperformance degradation under distribution shifts. Existing graph OOD methodstend to follow the basic ideas of invariant risk minimization and structuralcausal models, interpreting the invariant knowledge across datasets undervarious distribution shifts as graph topology or graph spectrum. However, theseinterpretations may be inconsistent with real-world scenarios, as neitherinvariant topology nor spectrum is assured. In this paper, we advocate thelearnable random walk (LRW) perspective as the instantiation of invariantknowledge, and propose LRW-OOD to realize graph OOD generalization learning.Instead of employing fixed probability transition matrix (i.e.,degree-normalized adjacency matrix), we parameterize the transition matrix withan LRW-sampler and a path encoder. Furthermore, we propose the kernel densityestimation (KDE)-based mutual information (MI) loss to generate random walksequences that adhere to OOD principles. Extensive experiment demonstrates thatour model can effectively enhance graph OOD generalization under various typesof distribution shifts and yield a significant accuracy improvement of 3.87%over state-of-the-art graph OOD generalization baselines.</description>
      <author>example@mail.com (Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2505.05785v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Discovery of the Polar Ring Galaxies with deep learning</title>
      <link>http://arxiv.org/abs/2505.05890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 11 figures. The article is submitted to Astron. &amp; Astrophys&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在创建一个强和良好候选行星状星云（PRGs）的目录，开发一种基于机器学习方法的图像搜索技术，用于在大规模巡天中搜索和发现PRGs，并探索CIGALE软件在确定其多波段特性的能力。&lt;h4&gt;背景&lt;/h4&gt;本研究首次将深度学习方法应用于PRGs的搜索。&lt;h4&gt;目的&lt;/h4&gt;创建PRGs目录，开发基于机器学习的图像搜索方法，探索CIGALE软件在确定PRGs多波段特性的能力。&lt;h4&gt;方法&lt;/h4&gt;使用深度学习方法搜索PRGs，对现有PRGs目录中的星系进行视觉检查以创建训练样本，采用数据增强、图像分割和集成学习方法。使用转移学习技术通过GALFIT生成的合成图像扩大训练样本。&lt;h4&gt;主要发现&lt;/h4&gt;发现了三个新的PRGs，并在SDSS环状星系目录中发现了四个PRGs。使用CIGALE软件对其中一个发现的新PRG进行了研究，确定了其红外到紫外波段的谱能量分布，并估计了其恒星形成率（SFR）和总恒星质量。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法在发现新的PRGs方面是有效的，且CIGALE软件能够确定PRGs的多波段特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of our research is to create a catalog of strong and good candidatesfor PRGs using existing catalogs of PRGs, develop an image-based approach withmachine learning methods for the search and discovery of PRGs in a big skysurvey, and explore the capability of the CIGALE software for determining theirmultiwavelength properties. For the first time, we applied a deep learningmethod to the search for PRGs. We visually inspected galaxies from existingcatalogs of PRGs to create a training sample based on high-quality SDSS images.Since the resulting training sample was extremely small (87 strong and goodPRGs), we applied augmentation, image segmentation, and ensemble learningtechniques. However, most effective method was transfer learning with itsability to enlarge the training sample by synthetic images generated by GALFIT.To examine deep learning approach for finding new PRGs we used the SDSS catalogof galaxies at z &lt; 0.1. The method with synthetic images showed that even withovertraining we were able to find galaxies with a ring pattern. Our deeplearning approach has resulted in the discovery of three PRGs (SDSSJ140644.42+471602.0; SDSS J133650.48+492745.3; SDSS J095717.30+364953.5). Also,we visually inspected the Catalog of the SDSS Ring galaxies at z &lt; 0.1 anddiscovered four PRGs among ~2,200 ring galaxies (SDSS J095851.32+320422.9; SDSSJ104211.05+234448.2; SDSS J162212.63+272032.2; SDSS J104600.10+090627.2). Oneof the discovered galaxies with transfer learning, SDSS J140644.42+471602.0,was studied with CIGALE software to determine its spectral energy distributionin IR-UV bands. The current SFR is 71 M_sun per year, although the lack of FUVdata limits this estimate. The total stellar mass is 8.34x10^{10} M_sun. Thepredominance of an old stellar population (two-thirds of the total mass)suggests that this PRG is undergoing interaction process.</description>
      <author>example@mail.com (D. V. Dobrycheva, O. O. Hetmantsev, I. B. Vavilova, A. Shportko, O. Gugnin, O. V. Kompaniiets)</author>
      <guid isPermaLink="false">2505.05890v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
      <link>http://arxiv.org/abs/2505.05845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于机器学习的木材节子检测和配对自动化流程，提高了木材加工中的效率。&lt;h4&gt;背景&lt;/h4&gt;木材节子对木材的美观和结构完整性至关重要，传统手工标注节子费时费力。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级且全自动的节子检测和配对方法。&lt;h4&gt;方法&lt;/h4&gt;利用工业级相机采集木质板材的高分辨率表面图像，并手动标注和预处理大规模数据集。使用YOLOv8进行节子检测，采用三重神经网络进行特征映射和配对。&lt;h4&gt;主要发现&lt;/h4&gt;YOLOv8在检测阶段达到mAP@0.5的0.887，三重神经网络在配对阶段达到0.85的配对准确率。分析表明，节子起点和终点到底部的距离以及纵向坐标在提高配对准确性中起关键作用。&lt;h4&gt;结论&lt;/h4&gt;该方法有效验证了人工智能在推进木材科学和工业中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knots in wood are critical to both aesthetics and structural integrity,making their detection and pairing essential in timber processing. However,traditional manual annotation was labor-intensive and inefficient,necessitating automation. This paper proposes a lightweight and fully automatedpipeline for knot detection and pairing based on machine learning techniques.In the detection stage, high-resolution surface images of wooden boards werecollected using industrial-grade cameras, and a large-scale dataset wasmanually annotated and preprocessed. After the transfer learning, the YOLOv8lachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots wereanalyzed and paired based on multidimensional feature extraction. A tripletneural network was used to map the features into a latent space, enablingclustering algorithms to identify and pair corresponding knots. The tripletnetwork with learnable weights achieved a pairing accuracy of 0.85. Furtheranalysis revealed that he distances from the knot's start and end points to thebottom of the wooden board, and the longitudinal coordinates play crucial rolesin achieving high pairing accuracy. Our experiments validate the effectivenessof the proposed solution, demonstrating the potential of AI in advancing woodscience and industry.</description>
      <author>example@mail.com (Guohao Lin, Shidong Pan, Rasul Khanbayov, Changxi Yang, Ani Khaloian-Sarnaghi, Andriy Kovryga)</author>
      <guid isPermaLink="false">2505.05845v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
      <link>http://arxiv.org/abs/2505.06184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at MisD @ AAAI ICWSM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLM）的社交媒体用户画像方法，旨在解决现有画像技术的局限性，如可迁移性差、特征不可解释、需要大量标注数据集或依赖刚性预定义类别等。&lt;h4&gt;背景&lt;/h4&gt;社交媒体用户画像对于虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应不同领域、减少对大量标注数据集依赖、生成可解释的用户画像的新方法。&lt;h4&gt;方法&lt;/h4&gt;该方法采用两阶段方法：首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在创建灵活、适应性强和可解释的用户画像方面显著优于现有方法，提高了9.8%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过利用LLM的内在知识，在最小化人工验证的同时，实现了跨领域的适应性，并有效利用LLM的推理和知识能力，为下游社交网络任务提供支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：通过内容分析进行社交媒体用户画像对于诸如虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。然而，现有的画像技术，包括推文摘要、基于属性的画像和潜在表示学习，面临着重大局限性：它们通常缺乏可迁移性，产生不可解释的特征，需要大量标注数据集，或者依赖于限制适应性的刚性预定义类别。我们提出了一种基于大型语言模型（LLM）的新方法，该方法利用领域定义语句，这些语句作为领域关键特征的轮廓，构成了领域画像的基础。我们的两阶段方法首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。通过利用LLM的内在知识以及最小的人工验证，我们的方法在减少对大量标注数据集需求的同时，实现了跨领域的适应性。我们的方法生成可解释的自然语言用户画像，将大量的用户数据压缩到可以解锁LLM推理和知识能力的规模，从而为下游社交网络任务提供支持。我们贡献了一个波斯政治Twitter（X）数据集和一个基于LLM的评估框架，并进行了人工验证。实验结果表明，我们的方法在创建灵活、适应性强和可解释的用户画像方面显著优于最先进的LLM和传统方法，提高了9.8%，证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social media user profiling through content analysis is crucial for taskslike misinformation detection, engagement prediction, hate speech monitoring,and user behavior modeling. However, existing profiling techniques, includingtweet summarization, attribute-based profiling, and latent representationlearning, face significant limitations: they often lack transferability,produce non-interpretable features, require large labeled datasets, or rely onrigid predefined categories that limit adaptability. We introduce a novel largelanguage model (LLM)-based approach that leverages domain-defining statements,which serve as key characteristics outlining the important pillars of a domainas foundations for profiling. Our two-stage method first employssemi-supervised filtering with a domain-specific knowledge base, then generatesboth abstractive (synthesized descriptions) and extractive (representativetweet selections) user profiles. By harnessing LLMs' inherent knowledge withminimal human validation, our approach is adaptable across domains whilereducing the need for large labeled datasets. Our method generatesinterpretable natural language user profiles, condensing extensive user datainto a scale that unlocks LLMs' reasoning and knowledge capabilities fordownstream social network tasks. We contribute a Persian political Twitter (X)dataset and an LLM-based evaluation framework with human validation.Experimental results show our method significantly outperforms state-of-the-artLLM-based and traditional methods by 9.8%, demonstrating its effectiveness increating flexible, adaptable, and interpretable user profiles.</description>
      <author>example@mail.com (Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour)</author>
      <guid isPermaLink="false">2505.06184v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
      <link>http://arxiv.org/abs/2505.05752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于点云数据的几何测量和合规性评估自动化框架，通过深度学习和信号处理技术自动化基础设施测绘任务。&lt;h4&gt;背景&lt;/h4&gt;自动化在提高基础设施测绘的效率、准确性和可扩展性方面可以发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，用于利用点云数据自动评估道路边缘坡道是否符合美国残疾人法案（ADA）的要求。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了基于深度学习的检测和分割技术，以及几何和信号处理技术，并使用了一个新收集的大规模标注数据集进行模型训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在自动化测绘任务中具有较高的准确性和可靠性，可以显著减少手动工作并提高基础设施评估的一致性。&lt;h4&gt;结论&lt;/h4&gt;该框架为基础设施测绘和自动施工评估提供了基础，并促进了点云数据在这些领域的更广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动化在提高基础设施测绘的效率、准确性及可扩展性方面扮演着重要角色。本文提出了一种基于点云数据的几何测量和合规性评估自动化框架。该框架集成了深度学习检测和分割技术与几何和信号处理技术，以自动化测绘任务。作为一种概念验证，我们应用该框架自动评估道路边缘坡道是否符合美国残疾人法案（ADA），展示了点云数据在测绘自动化中的实用性。该方法利用了一个新收集的大型标注数据集，作为本工作的一个部分公开，以促进稳健的模型训练和评估。包括与多个坡道的手动现场测量的比较在内的实验结果，验证了所提方法的准确性和可靠性，突出了其在显著减少手动工作并提高基础设施评估一致性方面的潜力。除了ADA合规性之外，该框架为基础设施测绘和自动施工评估的更广泛应用奠定了基础，并促进了点云数据在这些领域的更广泛应用。标注数据库、手动坡道调查数据和开发算法在项目的GitHub页面上公开：https://github.com/Soltanilara/SurveyAutomation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/soltanilara/surveyautomation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automation can play a prominent role in improving efficiency, accuracy, andscalability in infrastructure surveying and assessing construction andcompliance standards. This paper presents a framework for automation ofgeometric measurements and compliance assessment using point cloud data. Theproposed approach integrates deep learning-based detection and segmentation, inconjunction with geometric and signal processing techniques, to automatesurveying tasks. As a proof of concept, we apply this framework toautomatically evaluate the compliance of curb ramps with the Americans withDisabilities Act (ADA), demonstrating the utility of point cloud data in surveyautomation. The method leverages a newly collected, large annotated dataset ofcurb ramps, made publicly available as part of this work, to facilitate robustmodel training and evaluation. Experimental results, including comparison withmanual field measurements of several ramps, validate the accuracy andreliability of the proposed method, highlighting its potential to significantlyreduce manual effort and improve consistency in infrastructure assessment.Beyond ADA compliance, the proposed framework lays the groundwork for broaderapplications in infrastructure surveying and automated construction evaluation,promoting wider adoption of point cloud data in these domains. The annotateddatabase, manual ramp survey data, and developed algorithms are publiclyavailable on the project's GitHub page:https://github.com/Soltanilara/SurveyAutomation.</description>
      <author>example@mail.com (Amin Ghafourian, Andrew Lee, Dechen Gao, Tyler Beer, Kin Yen, Iman Soltani)</author>
      <guid isPermaLink="false">2505.05752v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2505.06217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新框架，用于将Segment Anything Model (SAM) 应用于医学图像分类。&lt;h4&gt;背景&lt;/h4&gt;SAM在图像分割任务中表现出色，但其适应医学图像分类的研究还较少。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，以适应SAM进行医学图像分类。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器作为特征提取器，捕获图像的分割特征，并提出了一种新的空间局部化通道注意力（SLCA）机制来计算特征图的空间局部化注意力权重，从而增强分类模型对图像中空间相关或有意义区域的关注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开的医学图像分类数据集上的实验结果表明，该方法有效且数据效率高。&lt;h4&gt;结论&lt;/h4&gt;该方法在医学图像分类中取得了良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models, such as the Segment Anything Model(SAM), have shown strong performance in various vision tasks, particularlyimage segmentation, due to their impressive zero-shot segmentationcapabilities. However, effectively adapting such models for medical imageclassification is still a less explored topic. In this paper, we introduce anew framework to adapt SAM for medical image classification. First, we utilizethe SAM image encoder as a feature extractor to capture segmentation-basedfeatures that convey important spatial and contextual details of the image,while freezing its weights to avoid unnecessary overhead during training. Next,we propose a novel Spatially Localized Channel Attention (SLCA) mechanism tocompute spatially localized attention weights for the feature maps. Thefeatures extracted from SAM's image encoder are processed through SLCA tocompute attention weights, which are then integrated into deep learningclassification models to enhance their focus on spatially relevant ormeaningful regions of the image, thus improving classification performance.Experimental results on three public medical image classification datasetsdemonstrate the effectiveness and data-efficiency of our approach.</description>
      <author>example@mail.com (Pengfei Gu, Haoteng Tang, Islam A. Ebeid, Jose A. Nunez, Fabian Vazquez, Diego Adame, Marcus Zhan, Huimin Li, Bin Fu, Danny Z. Chen)</author>
      <guid isPermaLink="false">2505.06217v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
      <link>http://arxiv.org/abs/2505.05710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于Transformer的HyperspectralMAE模型，用于高光谱数据，通过双掩码策略和波长感知嵌入，提高了高光谱图像的重建和分析能力。&lt;h4&gt;背景&lt;/h4&gt;高光谱图像在空间和光谱维度上具有高维度，这给数据处理带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理高光谱数据的模型，并提高其重建和分析性能。&lt;h4&gt;方法&lt;/h4&gt;HyperspectralMAE模型采用双掩码策略，在预训练过程中随机遮挡50%的空间块和50%的光谱带，并引入基于波长的可学习谐波傅里叶位置嵌入来编码光谱顺序。重建目标结合均方误差（MSE）和光谱角映射（SAM）来平衡像素级精度和光谱形状保真度。&lt;h4&gt;主要发现&lt;/h4&gt;HyperspectralMAE在Indian Pines数据集上实现了最先进的迁移学习准确率，证明了掩码双维度预训练能够产生鲁棒的光谱-空间表示。&lt;h4&gt;结论&lt;/h4&gt;双掩码和波长感知嵌入技术能够提升高光谱图像的重建和下游分析能力。&lt;h4&gt;翻译&lt;/h4&gt;Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose HyperspectralMAE, a Transformer-based foundation model for hyperspectral data that employs a dual masking strategy: during pre-training we randomly occlude 50% of spatial patches and 50% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity. The resulting model contains about 1.8×10^8 parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion (~1,600 scenes, ~3×10^11 pixel spectra) and DLR EnMAP Level-0 (~1,300 scenes, ~3×10^11 pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imagery provides rich spectral detail but poses uniquechallenges because of its high dimensionality in both spatial and spectraldomains. We propose \textit{HyperspectralMAE}, a Transformer-based foundationmodel for hyperspectral data that employs a \textit{dual masking} strategy:during pre-training we randomly occlude 50\% of spatial patches and 50\% ofspectral bands. This forces the model to learn representations capable ofreconstructing missing information across both dimensions. To encode spectralorder, we introduce learnable harmonic Fourier positional embeddings based onwavelength. The reconstruction objective combines mean-squared error (MSE) withthe spectral angle mapper (SAM) to balance pixel-level accuracy andspectral-shape fidelity.  The resulting model contains about $1.8\times10^{8}$ parameters and produces768-dimensional embeddings, giving it sufficient capacity for transferlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixelspectra) -- and fine-tuned it for land-cover classification on the Indian Pinesbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learningaccuracy on Indian Pines, confirming that masked dual-dimensional pre-trainingyields robust spectral-spatial representations. These results demonstrate thatdual masking and wavelength-aware embeddings advance hyperspectral imagereconstruction and downstream analysis.</description>
      <author>example@mail.com (Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim)</author>
      <guid isPermaLink="false">2505.05710v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2505.05587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025, Project page: https://vita-group.github.io/SteepGS/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;3D Gaussian Splatting (3DGS) 是一种实时、高分辨率的新视角合成技术。为了优化场景覆盖和捕捉细节，3DGS 使用密化算法生成额外的点，但往往导致冗余点云，增加了内存使用、性能下降和存储需求。本文提出了一种理论框架，通过优化密化控制来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;3DGS 通过将场景表示为高斯原语混合体，利用 GPU 光栅化管道进行高效渲染和重建。&lt;h4&gt;目的&lt;/h4&gt;优化场景覆盖和捕捉细节，同时减少冗余点云，提高资源受限设备的部署效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种理论框架，分析密化过程，通过优化理论方法确定必要的条件，最小化后代的数量，确定最优的参数更新方向，并提供后代不透明度的解析解。基于这些发现，引入了 SteepGS，这是一种最陡密度控制策略，旨在最小化损失并保持紧凑的点云。&lt;h4&gt;主要发现&lt;/h4&gt;分析表明，分割对于逃离鞍点至关重要。通过优化理论方法，确定了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并为后代不透明度的标准化提供了解析解。&lt;h4&gt;结论&lt;/h4&gt;SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯喷射（3DGS）已经成为实时、高分辨率新视角合成的强大技术。通过将场景表示为高斯原语混合体，3DGS 利用 GPU 光栅化管道进行高效渲染和重建。为了优化场景覆盖和捕捉细节，3DGS 采用密化算法生成额外的点。然而，这个过程通常会导致冗余点云，从而增加内存使用、降低性能和大量存储需求，这对资源受限设备的部署构成了重大挑战。为了解决这一限制，我们提出了一种理论框架，揭示了 3DGS 中密度控制背后的原理并提高了其效率。我们的分析表明，分割对于逃离鞍点至关重要。通过一种优化理论方法，我们建立了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并提供了后代不透明度标准化的解析解。基于这些见解，我们引入了 SteepGS，这是一种采用最陡密度控制的策略，旨在最小化损失的同时保持紧凑的点云。SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a powerful technique forreal-time, high-resolution novel view synthesis. By representing scenes as amixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines forefficient rendering and reconstruction. To optimize scene coverage and capturefine details, 3DGS employs a densification algorithm to generate additionalpoints. However, this process often leads to redundant point clouds, resultingin excessive memory usage, slower performance, and substantial storage demands- posing significant challenges for deployment on resource-constrained devices.To address this limitation, we propose a theoretical framework that demystifiesand improves density control in 3DGS. Our analysis reveals that splitting iscrucial for escaping saddle points. Through an optimization-theoretic approach,we establish the necessary conditions for densification, determine the minimalnumber of offspring Gaussians, identify the optimal parameter update direction,and provide an analytical solution for normalizing off-spring opacity. Buildingon these insights, we introduce SteepGS, incorporating steepest densitycontrol, a principled strategy that minimizes loss while maintaining a compactpoint cloud. SteepGS achieves a ~50% reduction in Gaussian points withoutcompromising rendering quality, significantly enhancing both efficiency andscalability.</description>
      <author>example@mail.com (Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan)</author>
      <guid isPermaLink="false">2505.05587v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
      <link>http://arxiv.org/abs/2505.06145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自适应微调、对比学习和正则化优化的策略，以提升基于Transformer模型的文本分类性能，并在FewRel 2.0数据集上进行了实验。&lt;h4&gt;背景&lt;/h4&gt;Few-shot文本分类在低资源环境中具有重要的应用价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种策略来提升Transformer模型在少样本情况下的分类性能。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调、对比学习和正则化优化等方法。&lt;h4&gt;主要发现&lt;/h4&gt;T5-small、DeBERTa-v3和RoBERTa-base在少样本任务中表现良好，特别是5-shot设置可以更有效地捕捉文本特征并提高分类准确率。不同关系类别之间的分类难度存在显著差异，引入对比损失和正则化损失可以增强模型的泛化能力，有效缓解少样本环境中的过拟合问题。&lt;h4&gt;结论&lt;/h4&gt;使用Transformer模型或具有更强自注意力机制的生成架构可以提高少样本分类的稳定性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot text classification has important application value in low-resourceenvironments. This paper proposes a strategy that combines adaptivefine-tuning, contrastive learning, and regularization optimization to improvethe classification performance of Transformer-based models. Experiments on theFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base performwell in few-shot tasks, especially in the 5-shot setting, which can moreeffectively capture text features and improve classification accuracy. Theexperiment also found that there are significant differences in theclassification difficulty of different relationship categories. Some categorieshave fuzzy semantic boundaries or complex feature distributions, making itdifficult for the standard cross entropy loss to learn the discriminativeinformation required to distinguish categories. By introducing contrastive lossand regularization loss, the generalization ability of the model is enhanced,effectively alleviating the overfitting problem in few-shot environments. Inaddition, the research results show that the use of Transformer models orgenerative architectures with stronger self-attention mechanisms can helpimprove the stability and accuracy of few-shot classification.</description>
      <author>example@mail.com (Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, Junliang Du)</author>
      <guid isPermaLink="false">2505.06145v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title>
      <link>http://arxiv.org/abs/2505.05638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统地评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了模型参数减少对驾驶性能的影响。&lt;h4&gt;背景&lt;/h4&gt;近年来，由于运动预测竞赛和基准的推动，基于学习的预测模型规模越来越大，参数数量可达数百万，旨在通过厘米级的精度提高开环预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;评估这些模型在集成到自动驾驶堆栈后是否能够提高性能。&lt;h4&gt;方法&lt;/h4&gt;通过实验评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了参数减少的模型在闭环驾驶性能上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;高开环准确性并不总是与更好的闭环驾驶行为相关，预测的时间一致性和规划器的兼容性等其他因素也起着关键作用。减少参数的模型在某些情况下表现出可比较甚至更优的闭环驾驶性能。&lt;h4&gt;结论&lt;/h4&gt;高开环准确性与闭环驾驶性能之间没有必然联系，模型参数的减少可能不会影响驾驶性能。&lt;h4&gt;翻译&lt;/h4&gt;Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/continental/pred2plan.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fueled by motion prediction competitions and benchmarks, recent years haveseen the emergence of increasingly large learning based prediction models, manywith millions of parameters, focused on improving open-loop prediction accuracyby mere centimeters. However, these benchmarks fail to assess whether suchimprovements translate to better performance when integrated into an autonomousdriving stack. In this work, we systematically evaluate the interplay betweenstate-of-the-art motion predictors and motion planners. Our results show thathigher open-loop accuracy does not always correlate with better closed-loopdriving behavior and that other factors, such as temporal consistency ofpredictions and planner compatibility, also play a critical role. Furthermore,we investigate downsized variants of these models, and, surprisingly, find thatin some cases models with up to 86% fewer parameters yield comparable or evensuperior closed-loop driving performance. Our code is available athttps://github.com/continental/pred2plan.</description>
      <author>example@mail.com (Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel Göhring, Jörg Reichardt)</author>
      <guid isPermaLink="false">2505.05638v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Fourier Features for Transfer Learning of Interatomic Potentials</title>
      <link>http://arxiv.org/abs/2505.05652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了franken，一个可扩展且轻量级的迁移学习框架，用于训练机器学习原子间势，旨在提高计算效率和数据效率。&lt;h4&gt;背景&lt;/h4&gt;在原子模拟中，训练既计算高效又数据高效的机器学习原子间势是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出franken框架，以实现原子间势的快速和准确适应，无需调整超参数或修改架构。&lt;h4&gt;方法&lt;/h4&gt;franken从预训练的图神经网络中提取原子描述符，并使用随机傅里叶特征将它们转移到新系统中。&lt;h4&gt;主要发现&lt;/h4&gt;在27种过渡金属的基准数据集上，franken在训练时间和准确性方面优于优化的核方法，将模型训练时间从数十小时缩短到几分钟。&lt;h4&gt;结论&lt;/h4&gt;franken框架展示了强大的数据效率，能够使用仅数十个训练结构稳定且准确地训练大量水以及Pt(111)/水界面的原子间势。&lt;h4&gt;翻译&lt;/h4&gt;Training machine learning interatomic potentials that are both computationally and data-efficient is a key challenge for enabling their routine use in atomistic simulations. To this effect, we introduce franken, a scalable and lightweight transfer learning framework that extracts atomic descriptors from pretrained graph neural networks and transfer them to new systems using random Fourier features-an efficient and scalable approximation of kernel methods. Franken enables fast and accurate adaptation of general-purpose potentials to new systems or levels of quantum mechanical theory without requiring hyperparameter tuning or architectural modifications. On a benchmark dataset of 27 transition metals, franken outperforms optimized kernel-based methods in both training time and accuracy, reducing model training from tens of hours to minutes on a single GPU. We further demonstrate the framework's strong data-efficiency by training stable and accurate potentials for bulk water and the Pt(111)/water interface using just tens of training structures. Our open-source implementation (https://franken.readthedocs.io) offers a fast and practical solution for training potentials and deploying them for molecular dynamics simulations across diverse systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training machine learning interatomic potentials that are bothcomputationally and data-efficient is a key challenge for enabling theirroutine use in atomistic simulations. To this effect, we introduce franken, ascalable and lightweight transfer learning framework that extracts atomicdescriptors from pretrained graph neural networks and transfer them to newsystems using random Fourier features-an efficient and scalable approximationof kernel methods. Franken enables fast and accurate adaptation ofgeneral-purpose potentials to new systems or levels of quantum mechanicaltheory without requiring hyperparameter tuning or architectural modifications.On a benchmark dataset of 27 transition metals, franken outperforms optimizedkernel-based methods in both training time and accuracy, reducing modeltraining from tens of hours to minutes on a single GPU. We further demonstratethe framework's strong data-efficiency by training stable and accuratepotentials for bulk water and the Pt(111)/water interface using just tens oftraining structures. Our open-source implementation(https://franken.readthedocs.io) offers a fast and practical solution fortraining potentials and deploying them for molecular dynamics simulationsacross diverse systems.</description>
      <author>example@mail.com (Pietro Novelli, Giacomo Meanti, Pedro J. Buigues, Lorenzo Rosasco, Michele Parrinello, Massimiliano Pontil, Luigi Bonati)</author>
      <guid isPermaLink="false">2505.05652v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在处理图数据时的挑战，提出了一种新的框架RELGCL，以解决传统方法在处理离散、非欧几里得性质图数据时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的GCL方法在保持增强视图之间的绝对相似性方面取得了成功，但在处理图数据时面临挑战，因为图数据具有离散和非欧几里得性质，导致视图生成破坏语义有效性和相似性验证的不可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，能够有效地利用图数据中的自然相对相似性模式。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个现实世界的图，发现了一个超越同质性和异质性的普遍模式：随着结构距离的增加，标签一致性系统性地下降，在亲社会性图中表现为平滑衰减，在异质图中表现为振荡衰减。通过随机游走理论建立了对这个模式的保证，证明了标签分布收敛并描述了不同衰减行为背后的机制。&lt;h4&gt;主要发现&lt;/h4&gt;图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;提出的RELGCL框架在亲社会性和异质图中均优于20种现有方法，验证了利用自然相对相似性而非人工绝对相似性的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks</title>
      <link>http://arxiv.org/abs/2505.05989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注异构信息网络中的路径建模问题，并提出了一个多跳路径感知推荐框架。&lt;h4&gt;背景&lt;/h4&gt;研究背景是异构信息网络中的路径建模问题。&lt;h4&gt;目的&lt;/h4&gt;目的是提出一个能够有效捕获高阶交互语义的多跳路径感知推荐框架。&lt;h4&gt;方法&lt;/h4&gt;方法包括三个阶段：路径选择、语义表示和基于注意力的融合。路径选择阶段引入路径过滤机制去除冗余和噪声信息；表示学习阶段使用序列建模结构联合编码实体和关系；融合阶段使用注意力机制为每条路径分配不同权重以生成全局用户兴趣表示。&lt;h4&gt;主要发现&lt;/h4&gt;在亚马逊-图书等真实数据集上的实验表明，该方法在HR@10、Recall@10和Precision@10等多个评估指标上显著优于现有推荐模型。&lt;h4&gt;结论&lt;/h4&gt;结果表明，多跳路径在捕获高阶交互语义方面是有效的，并且该框架在异构推荐场景中具有强大的建模能力。该方法通过在异构网络中整合结构信息建模与推荐算法设计，提供了理论上的价值和实际应用的价值，为在复杂数据环境中学习用户偏好提供了一种更具有表达性和灵活性的范式。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the problem of path modeling in heterogeneous information networks and proposes a multi-hop path-aware recommendation framework. The method centers on multi-hop paths composed of various types of entities and relations. It models user preferences through three stages: path selection, semantic representation, and attention-based fusion. In the path selection stage, a path filtering mechanism is introduced to remove redundant and noisy information. In the representation learning stage, a sequential modeling structure is used to jointly encode entities and relations, preserving the semantic dependencies within paths. In the fusion stage, an attention mechanism assigns different weights to each path to generate a global user interest representation. Experiments conducted on real-world datasets such as Amazon-Book show that the proposed method significantly outperforms existing recommendation models across multiple evaluation metrics, including HR@10, Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop paths in capturing high-order interaction semantics and demonstrate the expressive modeling capabilities of the framework in heterogeneous recommendation scenarios. This method provides both theoretical and practical value by integrating structural information modeling in heterogeneous networks with recommendation algorithm design. It offers a more expressive and flexible paradigm for learning user preferences in complex data environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the problem of path modeling in heterogeneousinformation networks and proposes a multi-hop path-aware recommendationframework. The method centers on multi-hop paths composed of various types ofentities and relations. It models user preferences through three stages: pathselection, semantic representation, and attention-based fusion. In the pathselection stage, a path filtering mechanism is introduced to remove redundantand noisy information. In the representation learning stage, a sequentialmodeling structure is used to jointly encode entities and relations, preservingthe semantic dependencies within paths. In the fusion stage, an attentionmechanism assigns different weights to each path to generate a global userinterest representation. Experiments conducted on real-world datasets such asAmazon-Book show that the proposed method significantly outperforms existingrecommendation models across multiple evaluation metrics, including HR@10,Recall@10, and Precision@10. The results confirm the effectiveness of multi-hoppaths in capturing high-order interaction semantics and demonstrate theexpressive modeling capabilities of the framework in heterogeneousrecommendation scenarios. This method provides both theoretical and practicalvalue by integrating structural information modeling in heterogeneous networkswith recommendation algorithm design. It offers a more expressive and flexibleparadigm for learning user preferences in complex data environments.</description>
      <author>example@mail.com (Hongye Zheng, Yue Xing, Lipeng Zhu, Xu Han, Junliang Du, Wanyu Cui)</author>
      <guid isPermaLink="false">2505.05989v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
      <link>http://arxiv.org/abs/2505.05736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MINT是一种通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐的框架，用于提高预训练大型语言模型在生物医学任务中的微调能力。&lt;h4&gt;背景&lt;/h4&gt;高质量的多模态生物医学数据稀缺，限制了预训练大型语言模型在特定生物医学任务中的微调效果。&lt;h4&gt;目的&lt;/h4&gt;提出MINT框架，以解决高质量多模态生物医学数据稀缺的问题，提高预训练大型语言模型在生物医学任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;MINT通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐，使用Odds Ratio Preference Optimization (ORPO)框架作为其核心优化技术，并利用上游的多模态机器学习模型来转移领域特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;MINT在两个关键应用中展示了其有效性：(1) 从文本中预测罕见遗传疾病，MINT生成的模型在仅依赖文本输入的情况下优于使用SFT、RAG或DPO训练的模型；(2) 使用细胞核图像进行组织类型分类，MINT生成的模型显著提高了Llama 3.2-Vision-11B-Instruct在组织类型分类上的性能。&lt;h4&gt;结论&lt;/h4&gt;MINT通过偏好优化提供了一种有效的方法，将单模态大型语言模型与高质量的多模态专业知识对齐，从而提高其在生物医学任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of high-quality multimodal biomedical data limits the ability toeffectively fine-tune pretrained Large Language Models (LLMs) for specializedbiomedical tasks. To address this challenge, we introduce MINT (MultimodalIntegrated kNowledge Transfer), a framework that aligns unimodal large decodermodels with domain-specific decision patterns from multimodal biomedical datathrough preference optimization. While MINT supports different optimizationtechniques, we primarily implement it with the Odds Ratio PreferenceOptimization (ORPO) framework as its backbone. This strategy enables thealigned LLMs to perform predictive tasks using text-only or image-only inputswhile retaining knowledge learnt from multimodal data. MINT leverages anupstream multimodal machine learning (MML) model trained on high-qualitymultimodal data to transfer domain-specific insights to downstream text-only orimage-only LLMs. We demonstrate its effectiveness through two key applications:(1) Rare genetic disease prediction from texts, where MINT uses a multimodalencoder model, trained on facial photos and clinical notes, to generate apreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despiterelying on text input only, the MINT-derived model outperforms models trainedwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissuetype classification using cell nucleus images, where MINT uses avision-language foundation model as the preference generator, containingknowledge learnt from both text and histopathological images to aligndownstream image-only models. The resulting MINT-derived model significantlyimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue typeclassification. In summary, MINT provides an effective strategy to alignunimodal LLMs with high-quality multimodal expertise through preferenceoptimization.</description>
      <author>example@mail.com (Da Wu, Zhanliang Wang, Quan Nguyen, Zhuoran Xu, Kai Wang)</author>
      <guid isPermaLink="false">2505.05736v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了在3D场景理解中的应用，提出了一种基于自动检索合成CAD模型的方法，以生成高质量的真实标签数据，用于训练深度学习模型。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成精确的3D标注数据是深度学习模型开发的一大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究目的是利用自动检索合成CAD模型的方法，为监督式深度学习模型训练提供高质量的真实标签数据。&lt;h4&gt;方法&lt;/h4&gt;论文采用了与之前用于自动标注ScanNet场景中物体9D姿态和CAD模型相似的流程，并将其应用于ScanNet++ v1数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，基于自动获得的标注数据可以训练深度学习模型，且训练出的模型性能优于基于人工标注数据的模型。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注具有提升模型性能的潜力，同时显著降低标注成本。论文将发布名为SCANnotate++的标注数据和训练模型，以支持未来3D场景理解的研究。&lt;h4&gt;翻译&lt;/h4&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构感知的多模态自监督分子表示预训练框架（MMSA），旨在通过利用分子之间的不变知识来增强分子图表示。&lt;h4&gt;背景&lt;/h4&gt;分子表示的准确提取是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，其中基于图像和2D/3D拓扑的多模态分子表示方法已成为主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间交互作用和无法充分捕捉分子间复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括两个主要模块：多模态分子表示学习模块和结构感知模块。多模态分子表示学习模块协同处理同一分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来模拟分子之间的高阶相关性，并引入记忆机制来存储典型分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MMSA在MoleculeNet基准测试上实现了最先进的性能，平均ROC-AUC比基线方法提高了1.8%到9.6%。&lt;h4&gt;结论&lt;/h4&gt;MMSA框架有效提高了分子表示的准确性，对药物发现过程具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model generalization ability. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Sensorimotor Learning for Open-world Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.06136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了开放世界机器人操作问题，即机器人必须能够泛化或快速适应它没有预先编程或预训练的新对象、场景或任务。通过有效的感觉运动学习方法，本文提出了新的观点和方法来提高数据效率，使机器人能够学习通用的操作技能。&lt;h4&gt;背景&lt;/h4&gt;开放世界机器人操作是一个挑战，因为机器人需要处理未知的新对象和任务。&lt;h4&gt;目的&lt;/h4&gt;通过有效的传感器运动学习方法解决开放世界机器人操作问题，提高机器人适应新环境和任务的能力。&lt;h4&gt;方法&lt;/h4&gt;利用有限的演示数据中的规律性，使机器人能够学习通用的操作技能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 引入方法赋予机器人以对象为中心的先验知识，使其能够从少量远程操作演示中学习通用的闭环传感器运动策略。2. 引入方法使机器人能够理解空间，从而从野外观测视频中模仿操作技能。3. 引入方法使机器人能够从以往的经验中识别可重用的技能，从而能够连续模仿多个任务。&lt;h4&gt;结论&lt;/h4&gt;本文的贡献为构建通用型个人机器人奠定了基础，这些机器人能够以低成本的数据收集快速适应新情况或任务，并且能够轻松与人类互动。通过使机器人能够从有限的数据中学习和泛化，本文向实现能够无缝集成到日常场景中的智能机器人助手的目标迈进。&lt;h4&gt;翻译&lt;/h4&gt;This dissertation considers Open-world Robot Manipulation, a manipulation problem where a robot must generalize or quickly adapt to new objects, scenes, or tasks for which it has not been pre-programmed or pre-trained. This dissertation tackles the problem using a methodology of efficient sensorimotor learning. The key to enabling efficient sensorimotor learning lies in leveraging regular patterns that exist in limited amounts of demonstration data. These patterns, referred to as ``regularity,'' enable the data-efficient learning of generalizable manipulation skills. This dissertation offers a new perspective on formulating manipulation problems through the lens of regularity. Building upon this notion, we introduce three major contributions. First, we introduce methods that endow robots with object-centric priors, allowing them to learn generalizable, closed-loop sensorimotor policies from a small number of teleoperation demonstrations. Second, we introduce methods that constitute robots' spatial understanding, unlocking their ability to imitate manipulation skills from in-the-wild video observations. Last but not least, we introduce methods that enable robots to identify reusable skills from their past experiences, resulting in systems that can continually imitate multiple tasks in a sequential manner. Altogether, the contributions of this dissertation help lay the groundwork for building general-purpose personal robots that can quickly adapt to new situations or tasks with low-cost data collection and interact easily with humans. By enabling robots to learn and generalize from limited data, this dissertation takes a step toward realizing the vision of intelligent robotic assistants that can be seamlessly integrated into everyday scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This dissertation considers Open-world Robot Manipulation, a manipulationproblem where a robot must generalize or quickly adapt to new objects, scenes,or tasks for which it has not been pre-programmed or pre-trained. Thisdissertation tackles the problem using a methodology of efficient sensorimotorlearning. The key to enabling efficient sensorimotor learning lies inleveraging regular patterns that exist in limited amounts of demonstrationdata. These patterns, referred to as ``regularity,'' enable the data-efficientlearning of generalizable manipulation skills. This dissertation offers a newperspective on formulating manipulation problems through the lens ofregularity. Building upon this notion, we introduce three major contributions.First, we introduce methods that endow robots with object-centric priors,allowing them to learn generalizable, closed-loop sensorimotor policies from asmall number of teleoperation demonstrations. Second, we introduce methods thatconstitute robots' spatial understanding, unlocking their ability to imitatemanipulation skills from in-the-wild video observations. Last but not least, weintroduce methods that enable robots to identify reusable skills from theirpast experiences, resulting in systems that can continually imitate multipletasks in a sequential manner. Altogether, the contributions of thisdissertation help lay the groundwork for building general-purpose personalrobots that can quickly adapt to new situations or tasks with low-cost datacollection and interact easily with humans. By enabling robots to learn andgeneralize from limited data, this dissertation takes a step toward realizingthe vision of intelligent robotic assistants that can be seamlessly integratedinto everyday scenarios.</description>
      <author>example@mail.com (Yifeng Zhu)</author>
      <guid isPermaLink="false">2505.06136v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.05732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of the paper published in SDM25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种改进的Denoising Diffusion Models（DDMs）的多级去噪自编码器框架，以提高其表示能力，并通过实验验证了其在图像嵌入和语义表示方面的优越性。&lt;h4&gt;背景&lt;/h4&gt;生成模型能够捕捉数据的真实分布，生成语义丰富的表示。尽管去噪扩散模型（DDMs）在生成能力上表现出色，但对其有效表示学习的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;扩展DDMs的表示能力，并通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;采用多级去噪自编码器框架，引入序列一致的扩散Transformer和额外的时步依赖编码器，通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法生成的嵌入在多个数据集上进行了广泛的实验，结果表明，DDMs最优学习的嵌入在大多数情况下超过了最先进的自监督表示学习方法，达到了显著的判别语义表示质量。&lt;h4&gt;结论&lt;/h4&gt;DDMs不仅适合于生成任务，而且对于通用深度学习应用也可能具有潜在优势。&lt;h4&gt;翻译&lt;/h4&gt;Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1137/1.9781611978520.1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models capture the true distribution of data, yieldingsemantically rich representations. Denoising diffusion models (DDMs) exhibitsuperior generative capabilities, though efficient representation learning forthem are lacking. In this work, we employ a multi-level denoising autoencoderframework to expand the representation capacity of DDMs, which introducessequentially consistent Diffusion Transformers and an additionaltimestep-dependent encoder to acquire embedding representations on thedenoising Markov chain through self-conditional diffusion learning.Intuitively, the encoder, conditioned on the entire diffusion process,compresses high-dimensional data into directional vectors in latent underdifferent noise levels, facilitating the learning of image embeddings acrossall timesteps. To verify the semantic adequacy of embeddings generated throughthis approach, extensive experiments are conducted on various datasets,demonstrating that optimally learned embeddings by DDMs surpassstate-of-the-art self-supervised representation learning methods in most cases,achieving remarkable discriminative semantic representation quality. Our workjustifies that DDMs are not only suitable for generative tasks, but alsopotentially advantageous for general-purpose deep learning applications.</description>
      <author>example@mail.com (Limai Jiang, Yunpeng Cai)</author>
      <guid isPermaLink="false">2505.05732v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
      <link>http://arxiv.org/abs/2505.05681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对卷尾猴的自然行为研究，使用预训练的视频文本基础模型，通过优化和调整，开发了一种从视频中检索有用片段的计算模型。&lt;h4&gt;背景&lt;/h4&gt;非人类灵长类动物的自然栖息地视频是研究其在野外行为的常见来源。&lt;h4&gt;目的&lt;/h4&gt;开发有用的计算模型，帮助研究人员从视频中检索有用片段。&lt;h4&gt;方法&lt;/h4&gt;使用原始、未标记的视频素材，结合弱音频描述，基于多模态大型语言模型（MLLMs）和视觉-语言模型（VLMs），提出了一种两阶段方法：数据预处理管道和微调过程。数据预处理管道自动提取干净且语义对齐的视频文本对，然后使用低秩适应（LoRA）方法微调预训练的Microsoft X-CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;在领域数据上，16帧模型和8帧模型的$Hits@5$分别提升了167%和114%。基于$NDCG@K$结果，模型能够很好地对大多数考虑的行为进行排序，而测试的原始预训练模型则无法对它们进行排序。&lt;h4&gt;结论&lt;/h4&gt;研究成功开发了一种有效的计算模型，能够帮助研究人员从视频中检索到有用的片段，特别是在处理噪声视频和音频内容方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the study of the natural behavior of capuchin monkeys using pre-trained video-text fundamental models, aiming to develop computational models that can help researchers retrieve useful clips from videos. The research successfully developed an effective computational model that can help researchers retrieve useful clips from videos, especially in dealing with noisy video and audio content.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video recordings of nonhuman primates in their natural habitat are a commonsource for studying their behavior in the wild. We fine-tune pre-trainedvideo-text foundational models for the specific domain of capuchin monkeys,with the goal of developing useful computational models to help researchers toretrieve useful clips from videos. We focus on the challenging problem oftraining a model based solely on raw, unlabeled video footage, using weak audiodescriptions sometimes provided by field collaborators. We leverage recentadvances in Multimodal Large Language Models (MLLMs) and Vision-Language Models(VLMs) to address the extremely noisy nature of both video and audio content.Specifically, we propose a two-folded approach: an agentic data treatmentpipeline and a fine-tuning process. The data processing pipeline automaticallyextracts clean and semantically aligned video-text pairs from the raw videos,which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP modelthrough Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of$167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame modelon our domain data. Moreover, based on $NDCG@K$ results, our model is able torank well most of the considered behaviors, while the tested raw pre-trainedmodels are not able to rank them at all. The code will be made available uponacceptance.</description>
      <author>example@mail.com (Giulio Cesare Mastrocinque Santo, Patrícia Izar, Irene Delval, Victor de Napole Gregolin, Nina S. T. Hirata)</author>
      <guid isPermaLink="false">2505.05681v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>3D Hand-Eye Calibration for Collaborative Robot Arm: Look at Robot Base Once</title>
      <link>http://arxiv.org/abs/2504.21619v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  updated&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。&lt;h4&gt;背景&lt;/h4&gt;手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。&lt;h4&gt;目的&lt;/h4&gt;目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。&lt;h4&gt;结论&lt;/h4&gt;提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-eye calibration is a common problem in the field of collaborativerobotics, involving the determination of the transformation matrix between thevisual sensor and the robot flange to enable vision-based robotic tasks.However, this process typically requires multiple movements of the robot armand an external calibration object, making it both time-consuming andinconvenient, especially in scenarios where frequent recalibration isnecessary. In this work, we extend our previous method which eliminates theneed for external calibration objects such as a chessboard. We propose ageneric dataset generation approach for point cloud registration, focusing onaligning the robot base point cloud with the scanned data. Furthermore, a moredetailed simulation study is conducted involving several differentcollaborative robot arms, followed by real-world experiments in an industrialsetting. Our improved method is simulated and evaluated using a total of 14robotic arms from 9 different brands, including KUKA, Universal Robots,UFACTORY, and Franka Emika, all of which are widely used in the field ofcollaborative robotics. Physical experiments demonstrate that our extendedapproach achieves performance comparable to existing commercial hand-eyecalibration solutions, while completing the entire calibration procedure injust a few seconds. In addition, we provide a user-friendly hand-eyecalibration solution, with the code publicly available atgithub.com/leihui6/LRBO.</description>
      <author>example@mail.com (Leihui Li, Lixuepiao Wan, Volker Krueger, Xuping Zhang)</author>
      <guid isPermaLink="false">2504.21619v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
      <link>http://arxiv.org/abs/2505.05644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将月球图像的反射率参数估计和基于图像的3D重建作为多模态学习问题，并设计了一种统一的变压器架构，用于学习不同数据源之间的共享表示。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是多学科新兴的研究主题，但在行星科学中应用较少。&lt;h4&gt;目的&lt;/h4&gt;将月球图像的反射率参数估计和基于图像的3D重建问题转化为多模态学习问题，并提出相应的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的变压器架构，用于学习灰度图像、数字高程模型、表面法线和反照率图等多种数据源之间的共享表示。&lt;h4&gt;主要发现&lt;/h4&gt;该架构能够从灰度图像中同时预测数字高程模型和反照率图，解决了行星表面3D重建的任务，并解耦了光度参数和高度信息。&lt;h4&gt;结论&lt;/h4&gt;该基础模型能够在四种模态之间学习到物理上合理的关联关系，未来可以通过添加更多输入模态来实现诸如光度归一化和共定位等任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning is an emerging research topic across multiple disciplinesbut has rarely been applied to planetary science. In this contribution, weidentify that reflectance parameter estimation and image-based 3Dreconstruction of lunar images can be formulated as a multimodal learningproblem. We propose a single, unified transformer architecture trained to learnshared representations between multiple sources like grayscale images, digitalelevation models, surface normals, and albedo maps. The architecture supportsflexible translation from any input modality to any target modality. PredictingDEMs and albedo maps from grayscale images simultaneously solves the task of 3Dreconstruction of planetary surfaces and disentangles photometric parametersand height information. Our results demonstrate that our foundation modellearns physically plausible relations across these four modalities. Adding moreinput modalities in the future will enable tasks such as photometricnormalization and co-registration.</description>
      <author>example@mail.com (Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian Wöhler)</author>
      <guid isPermaLink="false">2505.05644v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models</title>
      <link>http://arxiv.org/abs/2505.05577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 42nd International Conference on Machine Learning,  Vancouver, Canada. PMLR 267, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PyTDC，一个开源的机器学习平台，用于多模态生物AI模型的训练、评估和推理。PyTDC整合了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。&lt;h4&gt;背景&lt;/h4&gt;现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。&lt;h4&gt;目的&lt;/h4&gt;提出PyTDC平台，以促进多模态、上下文感知的基础模型在生物医学AI开放问题上的研究。&lt;h4&gt;方法&lt;/h4&gt;讨论了PyTDC架构的组件，并进行了首个针对单细胞药物靶点提名机器学习任务的案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;在图表示学习和图理论领域的最先进方法在该任务上表现不佳。虽然发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态。&lt;h4&gt;结论&lt;/h4&gt;PyTDC平台能够促进关于开发多模态、上下文感知的基础模型的研究，这些模型可以解决生物医学AI中的开放问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。我们提出了PyTDC，一个开源的机器学习平台，提供了多模态生物AI模型的简化训练、评估和推理软件。PyTDC统一了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。本文讨论了PyTDC架构的组件，并进行了我们知识范围内的首个关于引入的单细胞药物靶点提名机器学习任务的案例研究。我们发现图表示学习和图理论领域的最先进方法在该任务上表现不佳。尽管我们发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态，这突显了PyTDC平台促进关于开发多模态、上下文感知的基础模型的研究的能力，这些模型可以解决生物医学AI中的开放问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/apliko-xyz/pytdc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing biomedical benchmarks do not provide end-to-end infrastructure fortraining, evaluation, and inference of models that integrate multimodalbiological data and a broad range of machine learning tasks in therapeutics. Wepresent PyTDC, an open-source machine-learning platform providing streamlinedtraining, evaluation, and inference software for multimodal biological AImodels. PyTDC unifies distributed, heterogeneous, continuously updated datasources and model weights and standardizes benchmarking and inferenceendpoints. This paper discusses the components of PyTDC's architecture and, toour knowledge, the first-of-its-kind case study on the introduced single-celldrug-target nomination ML task. We find state-of-the-art methods in graphrepresentation learning and domain-specific methods from graph theory performpoorly on this task. Though we find a context-aware geometric deep learningmethod that outperforms the evaluated SoTA and domain-specific baselinemethods, the model is unable to generalize to unseen cell types or incorporateadditional modalities, highlighting PyTDC's capacity to facilitate an excitingavenue of research developing multimodal, context-aware, foundation models foropen problems in biomedical AI.</description>
      <author>example@mail.com (Alejandro Velez-Arce, Marinka Zitnik)</author>
      <guid isPermaLink="false">2505.05577v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2505.05612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;scDrugMap是一个集成的框架，用于预测单细胞数据中的药物反应，旨在解决癌症治疗中的药物耐药性问题。&lt;h4&gt;背景&lt;/h4&gt;药物耐药性是癌症治疗中的主要挑战，单细胞分析有助于了解细胞异质性，但大规模基础模型在预测单细胞数据药物反应中的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发scDrugMap框架，以预测单细胞数据中的药物反应，并建立大规模基准。&lt;h4&gt;方法&lt;/h4&gt;scDrugMap使用Python命令行界面和Web服务器进行药物反应预测。它评估了包括八种单细胞模型和两种大型语言模型在内的多种基础模型。使用超过326,000个细胞的原始数据集和18,800个细胞的验证集进行评估，涵盖了36个数据集和多种组织和癌症类型。模型性能在池化数据和跨数据评估设置下进行了基准测试，使用了层冻结和低秩适应（LoRA）微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;在池化数据场景中，scFoundation模型在层冻结和微调情况下均表现出最佳性能，F1分数分别为0.971和0.947，超过表现最差的模型50%以上。在跨数据设置中，UCE模型在微调后表现突出（平均F1：0.774），而scGPT在零样本学习中领先（平均F1：0.858）。&lt;h4&gt;结论&lt;/h4&gt;scDrugMap为单细胞数据中药物反应预测的基础模型提供了第一个大规模基准，是一个用户友好且灵活的平台，用于推进药物发现和转化研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/qsong-github/scdrugmap&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug resistance presents a major challenge in cancer therapy. Single cellprofiling offers insights into cellular heterogeneity, yet the application oflarge-scale foundation models for predicting drug response in single cell dataremains underexplored. To address this, we developed scDrugMap, an integratedframework featuring both a Python command-line interface and a web server fordrug response prediction. scDrugMap evaluates a wide range of foundationmodels, including eight single-cell models and two large language models, usinga curated dataset of over 326,000 cells in the primary collection and 18,800cells in the validation set, spanning 36 datasets and diverse tissue and cancertypes. We benchmarked model performance under pooled-data and cross-dataevaluation settings, employing both layer freezing and Low-Rank Adaptation(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundationachieved the best performance, with mean F1 scores of 0.971 (layer freezing)and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMapprovides the first large-scale benchmark of foundation models for drug responseprediction in single-cell data and serves as a user-friendly, flexible platformfor advancing drug discovery and translational research.</description>
      <author>example@mail.com (Qing Wang, Yining Pan, Minghao Zhou, Zijia Tang, Yanfei Wang, Guangyu Wang, Qianqian Song)</author>
      <guid isPermaLink="false">2505.05612v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究如何结合长期观察数据和短期实验数据来估计长期因果关系，并提出了一种基于自然数据异质性的方法来识别潜在混杂因素，避免了对理想化假设的依赖。&lt;h4&gt;背景&lt;/h4&gt;在现实场景中，估计长期因果关系是一个关键但具有挑战性的问题，现有的方法通常依赖于理想化的假设，而这些假设在实际情况中往往不成立，限制了其实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要理想化假设的方法来估计长期个体因果关系。&lt;h4&gt;方法&lt;/h4&gt;利用数据的自然异质性，如来自多个来源的数据，来识别潜在混杂因素，并设计了一种基于潜在表示学习估计长期因果效应的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个合成和半合成数据集上进行的实验研究，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法可以有效地估计长期因果关系，且不依赖于理想化假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/learnwjj/ICEVAE&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Griffin: Towards a Graph-Centric Relational Database Foundation Model</title>
      <link>http://arxiv.org/abs/2505.05568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Griffin，这是第一个专门为关系数据库（RDBs）设计的基座模型。Griffin通过统一数据编码器和任务解码器来处理各种任务，并集成了交叉注意力模块和新型聚合器，在单表和RDB数据集上进行预训练，展示了其在处理大规模、异构和时间图上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;目前存在一些小型模型专注于单一RDB任务，但它们无法处理多样化的任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够处理关系数据库多样化任务的基座模型。&lt;h4&gt;方法&lt;/h4&gt;Griffin模型集成了交叉注意力模块和新型聚合器，并在单表和RDB数据集上进行预训练，使用高级编码器处理分类、数值和元数据特征。&lt;h4&gt;主要发现&lt;/h4&gt;Griffin在大型、异构和时间图上表现出优异或相当的性能，擅长处理低数据场景，并在预训练新数据集和任务时显示出强大的迁移能力。&lt;h4&gt;结论&lt;/h4&gt;Griffin作为一个适用于关系数据库的通用基座模型具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Griffin, the first foundation model attemptation designed specifically for Relational Databases (RDBs). Unlike previous smaller models focused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture by incorporating a cross-attention module and a novel aggregator. Griffin utilizes pretraining on both single-table and RDB datasets, employing advanced encoders for categorical, numerical, and metadata features, along with innovative components such as cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture the complexities of relational data. Evaluated on large-scale, heterogeneous, and temporal graphs extracted from RDBs across various domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels in low-data scenarios, and shows strong transferability with similarity and diversity in pretraining across new datasets and tasks, highlighting its potential as a universally applicable foundation model for RDBs. Code available at https://github.com/yanxwb/Griffin.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Griffin, the first foundation model attemptation designedspecifically for Relational Databases (RDBs). Unlike previous smaller modelsfocused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture byincorporating a cross-attention module and a novel aggregator. Griffin utilizespretraining on both single-table and RDB datasets, employing advanced encodersfor categorical, numerical, and metadata features, along with innovativecomponents such as cross-attention modules and enhanced message-passing neuralnetworks (MPNNs) to capture the complexities of relational data. Evaluated onlarge-scale, heterogeneous, and temporal graphs extracted from RDBs acrossvarious domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels inlow-data scenarios, and shows strong transferability with similarity anddiversity in pretraining across new datasets and tasks, highlighting itspotential as a universally applicable foundation model for RDBs. Code availableat https://github.com/yanxwb/Griffin.</description>
      <author>example@mail.com (Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang)</author>
      <guid isPermaLink="false">2505.05568v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model (SAM)的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM的引入为语义分割应用铺平了道路，但其不确定性量化存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时尊重随机、认知和任务不确定性的不确定性量化模型。&lt;h4&gt;方法&lt;/h4&gt;使用贝叶斯熵公式训练了USAM，这是一种轻量级的后处理不确定性量化方法。该模型将不确定性的根源追溯到欠参数化模型、不充分的提示或图像模糊性。&lt;h4&gt;主要发现&lt;/h4&gt;USAM在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上显示出优越的预测能力，提供了一种计算成本低、易于使用的UQ替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM可以作为支持用户提示、增强半监督流程或平衡准确性和成本效率之间权衡的不确定性量化工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/GreenAutoML4FAS/UncertainSAM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
      <link>http://arxiv.org/abs/2505.05518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的跟踪模型，用于实时跟踪心脏内导管尖端的位置，以提高介入性电生理和结构性心脏病治疗中的操作效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;心脏内超声心动图（ICE）在电生理和结构性心脏病介入治疗中扮演着关键角色，但由于手动操作ICE导管时需要频繁调整，持续监测导管尖端位置是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个AI驱动的跟踪模型，以估计ICE成像平面内的导管尖端入射角和通过点，确保连续的可视化并便于机器人ICE导管控制。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合数据集生成策略，结合临床ICE序列和合成数据增强以增强模型鲁棒性。在水箱设置中收集ICE图像，并使用电磁传感器确定精确的真实位置。合成序列通过将导管尖端叠加到真实ICE图像上来创建，以保持运动连续性并模拟不同的解剖场景。最终数据集包含5,698个ICE尖端图像对。模型架构集成了预训练的超声基础模型，并使用基于变换器的网络处理连续的ICE帧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法实现了3.32度的入口角度误差和12.76度的旋转角度误差，为实时机器人ICE导管调整奠定了基础，最小化了操作者的工作量，同时确保了治疗设备的连续可视性。&lt;h4&gt;结论&lt;/h4&gt;该AI驱动的框架有助于提高心脏介入治疗中的操作效率和安全性，未来工作将集中于扩大临床数据集以进一步提高模型泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control. A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage. Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy. Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intra-cardiac Echocardiography (ICE) plays a critical role inElectrophysiology (EP) and Structural Heart Disease (SHD) interventions byproviding real-time visualization of intracardiac structures. However,maintaining continuous visibility of the therapy device tip remains a challengedue to frequent adjustments required during manual ICE catheter manipulation.To address this, we propose an AI-driven tracking model that estimates thedevice tip incident angle and passing point within the ICE imaging plane,ensuring continuous visibility and facilitating robotic ICE catheter control.  A key innovation of our approach is the hybrid dataset generation strategy,which combines clinical ICE sequences with synthetic data augmentation toenhance model robustness. We collected ICE images in a water chamber setup,equipping both the ICE catheter and device tip with electromagnetic (EM)sensors to establish precise ground-truth locations. Synthetic sequences werecreated by overlaying catheter tips onto real ICE images, preserving motioncontinuity while simulating diverse anatomical scenarios. The final datasetconsists of 5,698 ICE-tip image pairs, ensuring comprehensive trainingcoverage.  Our model architecture integrates a pretrained ultrasound (US) foundationmodel, trained on 37.4M echocardiography images, for feature extraction. Atransformer-based network processes sequential ICE frames, leveraginghistorical passing points and incident angles to improve prediction accuracy.  Experimental results demonstrate that our method achieves 3.32 degree entryangle error, 12.76 degree rotation angle error. This AI-driven framework laysthe foundation for real-time robotic ICE catheter adjustments, minimizingoperator workload while ensuring consistent therapy device visibility. Futurework will focus on expanding clinical datasets to further enhance modelgeneralization.</description>
      <author>example@mail.com (Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim)</author>
      <guid isPermaLink="false">2505.05518v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>AI-powered virtual eye: perspective, challenges and opportunities</title>
      <link>http://arxiv.org/abs/2505.05516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 Pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为‘虚拟眼’的下一代AI平台，利用互联的基础模型模拟眼睛的复杂结构和生物功能。&lt;h4&gt;背景&lt;/h4&gt;AI、成像和多组学技术的进步为构建高保真的人类眼睛数字复制品提供了肥沃的土壤。&lt;h4&gt;目的&lt;/h4&gt;追溯从早期机械和基于规则的模型到当代AI驱动方法的发展历程，并整合一个具有多模态、多尺度、动态预测能力和嵌入式反馈机制的综合模型。&lt;h4&gt;方法&lt;/h4&gt;提出一个发展路线图，强调大规模多模态数据集、生成AI、基础模型、基于代理的架构和交互式界面的作用。&lt;h4&gt;主要发现&lt;/h4&gt;尽管存在可解释性、伦理、数据处理和评估的挑战，虚拟眼有潜力革命性地改变个性化眼科护理并加速眼健康和疾病的研究。&lt;h4&gt;结论&lt;/h4&gt;虚拟眼有望在眼科护理和眼健康研究领域带来重大变革。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We envision the "virtual eye" as a next-generation, AI-powered platform thatuses interconnected foundation models to simulate the eye's intricate structureand biological function across all scales. Advances in AI, imaging, andmultiomics provide a fertile ground for constructing a universal, high-fidelitydigital replica of the human eye. This perspective traces the evolution fromearly mechanistic and rule-based models to contemporary AI-driven approaches,integrating in a unified model with multimodal, multiscale, dynamic predictivecapabilities and embedded feedback mechanisms. We propose a development roadmapemphasizing the roles of large-scale multimodal datasets, generative AI,foundation models, agent-based architectures, and interactive interfaces.Despite challenges in interpretability, ethics, data processing and evaluation,the virtual eye holds the potential to revolutionize personalized ophthalmiccare and accelerate research into ocular health and disease.</description>
      <author>example@mail.com (Yue Wu, Yibo Guo, Yulong Yan, Jiancheng Yang, Xin Zhou, Ching-Yu Cheng, Danli Shi, Mingguang He)</author>
      <guid isPermaLink="false">2505.05516v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience</title>
      <link>http://arxiv.org/abs/2505.05515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个基于神经科学的自主推理框架，旨在提高智能系统的自主推理能力。&lt;h4&gt;背景&lt;/h4&gt;随着自主人工智能的发展，研究如何使智能代理真正实现自主性成为一个关键问题。&lt;h4&gt;目的&lt;/h4&gt;探究智能代理自主性的核心，即代理推理，并建立一个新的神经科学启发框架。&lt;h4&gt;方法&lt;/h4&gt;基于神经科学的三个定义，提出了一个统一的框架，包括感知、维度、逻辑和交互四种核心推理类型，并应用于分类和分析现有的AI推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架有助于理解和评估现有AI推理方法的理论基础、计算设计和实践限制，并提出了构建更具通用性和认知一致性的智能代理的新方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合认知神经科学和人工智能，该研究为推进智能系统中的代理推理提供了理论基础和实践路线。&lt;h4&gt;翻译&lt;/h4&gt;Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous AI is no longer a hard-to-reach concept, it enables the agents tomove beyond executing tasks to independently addressing complex problems,adapting to change while handling the uncertainty of the environment. However,what makes the agents truly autonomous? It is agentic reasoning, that iscrucial for foundation models to develop symbolic logic, statisticalcorrelations, or large-scale pattern recognition to process information, drawinferences, and make decisions. However, it remains unclear why and howexisting agentic reasoning approaches work, in comparison to biologicalreasoning, which instead is deeply rooted in neural mechanisms involvinghierarchical cognition, multimodal integration, and dynamic interactions. Inthis work, we propose a novel neuroscience-inspired framework for agenticreasoning. Grounded in three neuroscience-based definitions and supported bymathematical and biological foundations, we propose a unified frameworkmodeling reasoning from perception to action, encompassing four core types,perceptual, dimensional, logical, and interactive, inspired by distinctfunctional roles observed in the human brain. We apply this framework tosystematically classify and analyze existing AI reasoning methods, evaluatingtheir theoretical foundations, computational designs, and practicallimitations. We also explore its implications for building more generalizable,cognitively aligned agents in physical and virtual environments. Finally,building on our framework, we outline future directions and propose newneural-inspired reasoning methods, analogous to chain-of-thought prompting. Bybridging cognitive neuroscience and AI, this work offers a theoreticalfoundation and practical roadmap for advancing agentic reasoning in intelligentsystems. The associated project can be found at:https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .</description>
      <author>example@mail.com (Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun Tang, Lin Wang)</author>
      <guid isPermaLink="false">2505.05515v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAISY的运动感知图像合成方法，用于消除医学图像采集过程中的运动模糊，并通过实验证明其性能优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;患者运动会导致医学图像模糊、鬼影和器官变形，从而增加图像解读的难度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来消除医学图像中的运动伪影，并提高图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先对运动进行特征化，然后利用Segment Anything Model (SAM)动态学习解剖边界处的空间模式，并引入Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;与现有技术相比，MAISY在胸部和头部CT数据集上的Peak Signal-to-Noise Ratio (PSNR)、SSIM和Dice指数均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在消除医学图像中的运动伪影方面表现优异，有效提高了图像质量。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v3</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
      <link>http://arxiv.org/abs/2407.06188v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://yukangcao.github.io/CrowdMoGen&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrowdMoGen是一个零样本框架，用于集体运动生成，能够从文本提示中有效分组个体并生成与事件对齐的运动序列。&lt;h4&gt;背景&lt;/h4&gt;当前文本到运动生成技术通常假设所有个体作为一个单一单元，难以扩展到更大的人群并确保个体对特定事件做出适当的反应。&lt;h4&gt;目的&lt;/h4&gt;提出CrowdMoGen，以解决上述挑战，实现集体运动生成。&lt;h4&gt;方法&lt;/h4&gt;1) 利用预训练的大语言模型（LLMs）组织个体成不同群体；2) 集成基于SMPL的关节先验生成与情境相符的活动，包括关节轨迹和文本描述；3) 引入集体运动生成器，将活动整合到基于transformer的网络中，在多步去噪过程中保持空间约束。&lt;h4&gt;主要发现&lt;/h4&gt;CrowdMoGen在实验中显著优于先前方法，能够生成逼真、由事件驱动且空间上连贯的运动序列。&lt;h4&gt;结论&lt;/h4&gt;CrowdMoGen作为第一个集体运动生成框架，有望推动城市模拟、人群规划和其他大规模交互环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in text-to-motion generation have shown promisingresults, they typically assume all individuals are grouped as a single unit.Scaling these methods to handle larger crowds and ensuring that individualsrespond appropriately to specific events remains a significant challenge. Thisis primarily due to the complexities of scene planning, which involvesorganizing groups, planning their activities, and coordinating interactions,and controllable motion generation. In this paper, we present CrowdMoGen, thefirst zero-shot framework for collective motion generation, which effectivelygroups individuals and generates event-aligned motion sequences from textprompts. 1) Being limited by the available datasets for training an effectivescene planning module in a supervised manner, we instead propose a crowd sceneplanner that leverages pre-trained large language models (LLMs) to organizeindividuals into distinct groups. While LLMs offer high-level guidance forgroup divisions, they lack the low-level understanding of human motion. Toaddress this, we further propose integrating an SMPL-based joint prior togenerate context-appropriate activities, which consists of both jointtrajectories and textual descriptions. 2) Secondly, to incorporate the assignedactivities into the generative network, we introduce a collective motiongenerator that integrates the activities into a transformer-based network in ajoint-wise manner, maintaining the spatial constraints during the multi-stepdenoising process. Extensive experiments demonstrate that CrowdMoGensignificantly outperforms previous approaches, delivering realistic,event-driven motion sequences that are spatially coherent. As the firstframework of collective motion generation, CrowdMoGen has the potential toadvance applications in urban simulation, crowd planning, and other large-scaleinteractive environments.</description>
      <author>example@mail.com (Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu)</author>
      <guid isPermaLink="false">2407.06188v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach</title>
      <link>http://arxiv.org/abs/2505.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the MORSE workshop of CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算机视觉领域的应用，提出了一个基于“能力编码”的方法来预测模型在多个下游任务上的性能，旨在简化模型选择并提供未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;在地球观测领域，过去四年内开发了超过75个遥感视觉基础模型，但它们在所有任务上并未表现出一致的优越性。&lt;h4&gt;目的&lt;/h4&gt;为了促进模型间的比较，提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。&lt;h4&gt;方法&lt;/h4&gt;该方法基于“能力编码”，旨在简化选择适用于特定新任务的基础模型，并用于分析现有文献，提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;通过“能力编码”方法，可以简化模型选择，并为现有文献提供新的视角。&lt;h4&gt;结论&lt;/h4&gt;该方法为选择和评估基础模型提供了新的途径，并为未来的研究指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在计算机视觉领域取得了显著进步：经过一次昂贵但有效的训练阶段后，它们可以处理各种任务。在地球观测领域，过去四年中已经开发了75多个遥感视觉基础模型。然而，没有一个模型在所有可用的下游任务上始终优于其他模型。为了促进它们的比较，我们提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。这种方法基于我们所说的“能力编码”。这种新颖方法的效用有两方面：我们展示了它在简化选择特定新任务的基础模型方面的潜力，并使用它来提供对现有文献的新视角，提出了未来研究的途径。代码可在https://github.com/pierreadorni/capabilities-encoding上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pierreadorni/capabilities-encoding&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models constitute a significant advancement in computer vision:after a single, albeit costly, training phase, they can address a wide array oftasks. In the field of Earth observation, over 75 remote sensing visionfoundation models have been developed in the past four years. However, none hasconsistently outperformed the others across all available downstream tasks. Tofacilitate their comparison, we propose a cost-effective method for predictinga model's performance on multiple downstream tasks without the need forfine-tuning on each one. This method is based on what we call "capabilitiesencoding." The utility of this novel approach is twofold: we demonstrate itspotential to simplify the selection of a foundation model for a given new task,and we employ it to offer a fresh perspective on the existing literature,suggesting avenues for future research. Codes are available athttps://github.com/pierreadorni/capabilities-encoding.</description>
      <author>example@mail.com (Pierre Adorni, Minh-Tan Pham, Stéphane May, Sébastien Lefèvre)</author>
      <guid isPermaLink="false">2505.03299v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
  <item>
      <title>Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization</title>
      <link>http://arxiv.org/abs/2505.05343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is  available at https://github.com/swimmiing/ACL-SSL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉-语言模型在声音源定位中的应用，提出了一种无显式文本输入的自监督方法，通过音频驱动嵌入和对比音频-视觉对应目标，实现了对声音源的高效定位。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉-语言模型在多模态对齐和泛化方面表现出色，其中CLIP模型尤为成功。&lt;h4&gt;目的&lt;/h4&gt;将CLIP模型应用于声音源定位，提出一种无需显式文本输入的自监督方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将音频映射到与CLIP文本编码器兼容的token的框架，生成音频驱动嵌入，用于生成声音区域掩码，并通过对比音频-视觉对应目标提取视觉特征与音频嵌入对齐。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的多模态基础模型的对齐知识使得该方法能够生成更完整和紧凑的声音源定位，进一步通过LLM引导扩展，将对象感知的音频-视觉场景理解提炼到模型中，增强了对齐。&lt;h4&gt;结论&lt;/h4&gt;在五个不同任务上的广泛实验表明，该方法在各种变体中都优于现有方法，并在零样本设置中实现了强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper studies the application of large-scale vision-language models in sound source localization, proposes a self-supervised method without explicit text input, and achieves efficient localization of sound sources through audio-driven embeddings and contrastive audio-visual correspondence objectives.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/swimmiing/ACL-SSL&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models demonstrate strong multimodal alignmentand generalization across diverse tasks. Among them, CLIP stands out as one ofthe most successful approaches. In this work, we extend the application of CLIPto sound source localization, proposing a self-supervised method operateswithout explicit text input. We introduce a framework that maps audios intotokens compatible with CLIP's text encoder, producing audio-driven embeddings.These embeddings are used to generate sounding region masks, from which visualfeatures are extracted and aligned with the audio embeddings through acontrastive audio-visual correspondence objective. Our findings show thatalignment knowledge of pre-trained multimodal foundation model enables ourmethod to generate more complete and compact localization for sounding objects.We further propose an LLM-guided extension that distills object-awareaudio-visual scene understanding into the model during training to enhancealignment. Extensive experiments across five diverse tasks demonstrate that ourmethod, in all variants, outperforms state-of-the-art approaches and achievesstrong generalization in zero-shot settings.</description>
      <author>example@mail.com (Sooyoung Park, Arda Senocak, Joon Son Chung)</author>
      <guid isPermaLink="false">2505.05343v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合长期观察数据和短期实验数据来估计长期因果效应的方法，以解决现实场景中因果推断的挑战。&lt;h4&gt;背景&lt;/h4&gt;在现有方法中，通常假设潜在无混淆性或加性等混淆偏倚，但这些假设在现实应用中往往不成立，限制了方法的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需上述假设的长期个体因果效应估计方法。&lt;h4&gt;方法&lt;/h4&gt;利用数据自然异质性，如来自多个来源的数据，来识别潜在混淆因子，从而避免对理想化假设的依赖。具体方法是设计一个基于潜在表示学习的长期因果效应估计器。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析，建立了潜在混淆因子的可识别性，并在此基础上实现了长期效应识别。实验结果表明，该方法在多个合成和半合成数据集上有效。&lt;h4&gt;结论&lt;/h4&gt;该方法在估计长期因果效应时，能够有效避免对理想化假设的依赖，提高了在现实世界中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model</title>
      <link>http://arxiv.org/abs/2505.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对路边感知在智能交通系统和车辆到一切（V2X）任务中的应用，提出了一种基于Mamba模型的PillarMamba框架，用于路边点云感知，并通过混合状态空间块（HSB）解决了空间模型在扫描方向限制下的局部连接中断和历史关系遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;路边感知在智能交通系统和车辆到一切（V2X）任务中越来越受到重视，因为它可以扩展连接车辆的感知范围并提高交通安全。然而，基于路边点云的3D目标检测尚未得到有效探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Mamba的PillarMamba框架，用于路边点云感知，并解决空间模型在扫描方向限制下的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入Mamba模型到路边点云感知，并基于交叉阶段状态空间组（CSG）提出PillarMamba框架。通过混合状态空间块（HSB）增强网络的表示能力，并通过交叉阶段特征融合实现高效计算。HSB通过局部卷积增强邻域连接，通过残差注意力保留历史记忆。&lt;h4&gt;主要发现&lt;/h4&gt;PillarMamba在DAIR-V2X-I这个流行的路边大规模数据集上优于现有方法，表明了所提出的方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;PillarMamba框架能够有效地进行路边点云感知，并通过HSB解决了空间模型在扫描方向限制下的局限性，为智能交通系统和V2X任务提供了有效的感知支持。&lt;h4&gt;翻译&lt;/h4&gt;Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud-oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything(V2X) tasks, roadside perception has received increasing attention in recentyears, as it can extend the perception range of connected vehicles and improvetraffic safety. However, roadside point cloud oriented 3D object detection hasnot been effectively explored. To some extent, the key to the performance of apoint cloud detector lies in the receptive field of the network and the abilityto effectively utilize the scene context. The recent emergence of Mamba, basedon State Space Model (SSM), has shaken up the traditional convolution andtransformers that have long been the foundational building blocks, due to itsefficient global receptive field. In this work, we introduce Mamba topillar-based roadside point cloud perception and propose a framework based onCross-stage State-space Group (CSG), called PillarMamba. It enhances theexpressiveness of the network and achieves efficient computation throughcross-stage feature fusion. However, due to the limitations of scan directions,state space model faces local connection disrupted and historical relationshipforgotten. To address this, we propose the Hybrid State-space Block (HSB) toobtain the local-global context of roadside point cloud. Specifically, itenhances neighborhood connections through local convolution and preserveshistorical memory through residual attention. The proposed method outperformsthe state-of-the-art methods on the popular large scale roadside benchmark:DAIR-V2X-I. The code will be released soon.</description>
      <author>example@mail.com (Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Tianze Wang, Jianghao Leng)</author>
      <guid isPermaLink="false">2505.05397v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning</title>
      <link>http://arxiv.org/abs/2505.05208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE IJCNN 2025 has accepted the paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于模糊sigmoid卷积（FSC）的肿瘤检测方法，通过减少可训练参数数量来提高模型性能，同时保持分类精度。&lt;h4&gt;背景&lt;/h4&gt;早期检测和准确诊断对于提高患者预后至关重要，卷积神经网络（CNN）在肿瘤检测中显示出潜力，但现有模型往往存在过参数化问题。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种高效、准确且参数较少的肿瘤检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括模糊sigmoid卷积（FSC）、漏斗顶部模块和漏斗中部模块。FSC通过有效的感受野扩展和保持输入数据完整性，实现高效的特征图减少和模型肿瘤检测能力提升。模糊sigmoid激活函数被引入卷积层以提高特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;FSC模型在三个基准数据集上实现了99.17%、99.75%和99.89%的分类精度，参数数量比大规模迁移学习架构少100倍。&lt;h4&gt;结论&lt;/h4&gt;FSC模型是一种高效、轻量级的深度学习模型，适用于医学影像应用中的脑肿瘤早期检测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：早期检测和准确诊断对于改善患者预后至关重要。卷积神经网络（CNN）在肿瘤检测中显示出希望，但现有模型通常受到过参数化的限制，这限制了其性能提升。在本研究中，引入了模糊sigmoid卷积（FSC）以及两个附加模块：漏斗顶部和中部。所提出的方法显著减少了可训练参数的数量，而没有损害分类精度。这种方法的核心是一个新颖的卷积算子，它有效地扩展了感受野同时保持输入数据完整性。这使高效的特征图减少和增强模型肿瘤检测能力成为可能。在基于FSC的模型中，模糊sigmoid激活函数被整合到卷积层中，以改进特征提取和分类。将模糊逻辑纳入架构提高了其适应性和鲁棒性。在三个基准数据集上进行的广泛实验表明了所提出模型的优越性能和效率。基于FSC的架构在三个不同的数据集上达到了99.17%、99.75%和99.89%的分类精度。该模型使用的参数数量比大规模迁移学习架构少100倍，突出了其计算效率和适用于早期检测脑肿瘤的适用性。这项研究为医学影像应用提供了轻量级、高性能的深度学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection and accurate diagnosis are essential to improving patientoutcomes. The use of convolutional neural networks (CNNs) for tumor detectionhas shown promise, but existing models often suffer from overparameterization,which limits their performance gains. In this study, fuzzy sigmoid convolution(FSC) is introduced along with two additional modules: top-of-the-funnel andmiddle-of-the-funnel. The proposed methodology significantly reduces the numberof trainable parameters without compromising classification accuracy. A novelconvolutional operator is central to this approach, effectively dilating thereceptive field while preserving input data integrity. This enables efficientfeature map reduction and enhances the model's tumor detection capability. Inthe FSC-based model, fuzzy sigmoid activation functions are incorporated withinconvolutional layers to improve feature extraction and classification. Theinclusion of fuzzy logic into the architecture improves its adaptability androbustness. Extensive experiments on three benchmark datasets demonstrate thesuperior performance and efficiency of the proposed model. The FSC-basedarchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%on three different datasets. The model employs 100 times fewer parameters thanlarge-scale transfer learning architectures, highlighting its computationalefficiency and suitability for detecting brain tumors early. This researchoffers lightweight, high-performance deep-learning models for medical imagingapplications.</description>
      <author>example@mail.com (Muhammad Irfan, Anum Nawaz, Riku Klen, Abdulhamit Subasi, Tomi Westerlund, Wei Chen)</author>
      <guid isPermaLink="false">2505.05208v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Analysis for Contrastive Representation Learning under Non-IID Settings</title>
      <link>http://arxiv.org/abs/2505.04937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To Appear in ICML, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比表示学习（CRL）的泛化行为，并针对非独立同分布（non-$i.i.d.$）数据集提供了泛化分析。&lt;h4&gt;背景&lt;/h4&gt;尽管CRL在各种领域取得了显著成功，但其泛化行为的理论理解有限，且现有文献仅分析了在数据元对独立同分布假设下的泛化界限。&lt;h4&gt;目的&lt;/h4&gt;本文旨在对CRL框架在非独立同分布设置下的泛化行为进行理论分析，并提供更符合实际情况的泛化界限。&lt;h4&gt;方法&lt;/h4&gt;本文借鉴U统计学的文献，推导了泛化界限，表明每个类中所需样本的数量与每个类关联的可学习特征表示类的覆盖数的对数成比例。&lt;h4&gt;主要发现&lt;/h4&gt;本文推导了针对线性映射和神经网络等常见函数类的过量风险界限。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为CRL在非独立同分布数据集上的应用提供了理论支持，有助于理解和优化CRL模型的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL is limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Representation Learning (CRL) has achieved impressive success invarious domains in recent years. Nevertheless, the theoretical understanding ofthe generalization behavior of CRL is limited. Moreover, to the best of ourknowledge, the current literature only analyzes generalization bounds under theassumption that the data tuples used for contrastive learning are independentlyand identically distributed. However, in practice, we are often limited to afixed pool of reusable labeled data points, making it inevitable to recycledata across tuples to create sufficiently large datasets. Therefore, thetuple-wise independence condition imposed by previous works is invalidated. Inthis paper, we provide a generalization analysis for the CRL framework undernon-$i.i.d.$ settings that adheres to practice more realistically. Drawinginspiration from the literature on U-statistics, we derive generalizationbounds which indicate the required number of samples in each class scales asthe logarithm of the covering number of the class of learnable featurerepresentations associated to each class. Next, we apply our main results toderive excess risk bounds for common function classes such as linear maps andneural networks.</description>
      <author>example@mail.com (Nong Minh Hieu, Antoine Ledent)</author>
      <guid isPermaLink="false">2505.04937v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PADriver: Towards Personalized Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.05240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。&lt;h4&gt;背景&lt;/h4&gt;目前自动驾驶领域的研究主要集中在基于规则的系统和基于数据的方法，但缺乏对个性化需求的考虑。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够根据个性化需求进行自动驾驶的框架，提高自动驾驶系统的适应性和安全性。&lt;h4&gt;方法&lt;/h4&gt;PADriver使用多模态大型语言模型处理输入数据，自动进行场景理解、危险水平估计和行动决策。同时，构建了基于Highway-Env模拟器的闭环基准PAD-Highway，以评估系统在遵守交通规则下的决策性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;h4&gt;结论&lt;/h4&gt;PADriver能够有效提高个性化自动驾驶系统的性能，为自动驾驶技术的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose PADriver, a novel closed-loop framework forpersonalized autonomous driving (PAD). Built upon Multi-modal Large LanguageModel (MLLM), PADriver takes streaming frames and personalized textual promptsas inputs. It autoaggressively performs scene understanding, danger levelestimation and action decision. The predicted danger level reflects the risk ofthe potential action and provides an explicit reference for the final action,which corresponds to the preset personalized prompt. Moreover, we construct aclosed-loop benchmark named PAD-Highway based on Highway-Env simulator tocomprehensively evaluate the decision performance under traffic rules. Thedataset contains 250 hours videos with high-quality annotation to facilitatethe development of PAD behavior analysis. Experimental results on theconstructed benchmark show that PADriver outperforms state-of-the-artapproaches on different evaluation metrics, and enables various driving modes.</description>
      <author>example@mail.com (Genghua Kou, Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Ziheng Zhang, Osamu Yoshie, Tiancai Wang, Ying Li, Xiangyu Zhang)</author>
      <guid isPermaLink="false">2505.05240v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Stochastic Variational Propagation（SVP）的深度学习算法，作为反向传播（BP）的替代方案，旨在提高深度学习的可扩展性和降低内存开销。&lt;h4&gt;背景&lt;/h4&gt;反向传播在深度学习中至关重要，但其依赖于全局梯度同步，限制了可扩展性并增加了内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP算法，通过将训练重新定义为分层变分推理，以实现可扩展性和降低内存开销。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了避免层间表示崩溃，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP实现了具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并造成了显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，一种可扩展的替代方案，将训练重新定义为分层变分推理。SVP将层激活视为潜在变量，并优化局部证据下界 (ELBOs)，允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能会由于过度压缩而导致层间表示崩溃。为了防止这种情况，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合用于层间一致性的特征对齐损失，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上实现了与BP具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP通过预测链（CoP）和三个关键设计，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计在单目3D物体检测中是一个挑战，因为2D图像到3D空间的映射存在固有模糊性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法MonoCoP，以解决深度估计问题并提高整体准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP使用轻量级属性网络（AN）学习每个3D属性的特征，构建一个显式的预测链来传播这些特征，并使用残差连接聚合特征。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI、Waymo和nuScenes数据集上实现了最先进的性能，且无需额外数据。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测和特征传播，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;翻译&lt;/h4&gt;Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
      <link>http://arxiv.org/abs/2505.04962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and  Embodied Intelligence (MOMA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种系统，用于从有序或无序堆积中自主精确地拾取立方体物体。该系统采用了一种高效的方法来估计立方体物体的姿态，旨在以时间高效的方式减少目标姿态误差。&lt;h4&gt;背景&lt;/h4&gt;传统的姿态估计方法，如全局点云注册，容易产生微小的姿态误差。为了提高姿态精度，通常使用局部注册算法，但这些方法存在执行时间开销和最终姿态误差不确定的问题。&lt;h4&gt;目的&lt;/h4&gt;降低目标姿态误差，并以时间高效的方式进行姿态估计和校正。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的线性时间方法来估计和校正姿态误差，并详细描述了算法的各个模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过新的线性时间方法，可以在保持姿态估计精度的同时，减少执行时间和提高姿态校正的准确性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地减少姿态估计误差，适用于立方体物体的自主拾取场景。&lt;h4&gt;翻译&lt;/h4&gt;The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proposed system outlined in this paper is a solution to a use case thatrequires the autonomous picking of cuboidal objects from an organized orunorganized pile with high precision. This paper presents an efficient methodfor precise pose estimation of cuboid-shaped objects, which aims to reduceerrors in target pose in a time-efficient manner. Typical pose estimationmethods like global point cloud registrations are prone to minor pose errorsfor which local registration algorithms are generally used to improve poseaccuracy. However, due to the execution time overhead and uncertainty in theerror of the final achieved pose, an alternate, linear time approach isproposed for pose error estimation and correction. This paper presents anoverview of the solution followed by a detailed description of individualmodules of the proposed algorithm.</description>
      <author>example@mail.com (Utsav Rai, Hardik Mehta, Vismay Vakharia, Aditya Choudhary, Amit Parmar, Rolif Lima, Kaushik Das)</author>
      <guid isPermaLink="false">2505.04962v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
      <link>http://arxiv.org/abs/2505.05089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025. Project Page:  https://wynelio.github.io/E-NMSTFlow&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E-NMSTFlow的新型无监督事件驱动光流网络，旨在提高长时间序列中的光流估计准确性。&lt;h4&gt;背景&lt;/h4&gt;现有基于学习的事件光流方法多采用基于帧的技术，忽略了事件的空间时间特性，并假设事件间的运动是线性的，这导致长时间序列中的光流误差增加。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在通过利用丰富的时间和空间信息以及事件间的非线性运动，提高事件驱动光流估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Spatio-Temporal Motion Feature Aware (STMFA)模块和一种Adaptive Motion Feature Enhancement (AMFE)模块，这两个模块都利用丰富的时空信息来学习时空数据关联。同时，提出了一种非线性运动补偿损失函数，利用事件间的准确非线性运动来提高网络的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在MVSEC和DSEC-Flow数据集上优于其他无监督学习方法，排名第一。&lt;h4&gt;结论&lt;/h4&gt;E-NMSTFlow网络在长时间序列光流估计方面表现出色，为事件驱动光流估计提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras have the potential to capture continuous motion informationover time and space, making them well-suited for optical flow estimation.However, most existing learning-based methods for event-based optical flowadopt frame-based techniques, ignoring the spatio-temporal characteristics ofevents. Additionally, these methods assume linear motion between consecutiveevents within the loss time window, which increases optical flow errors inlong-time sequences. In this work, we observe that rich spatio-temporalinformation and accurate nonlinear motion between events are crucial forevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novelunsupervised event-based optical flow network focusing on long-time sequences.We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and anAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize richspatio-temporal information to learn spatio-temporal data associations.Meanwhile, we propose a nonlinear motion compensation loss that utilizes theaccurate nonlinear motion between events to improve the unsupervised learningof our network. Extensive experiments demonstrate the effectiveness andsuperiority of our method. Remarkably, our method ranks first amongunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our projectpage is available at https://wynelio.github.io/E-NMSTFlow.</description>
      <author>example@mail.com (Zuntao Liu, Hao Zhuang, Junjie Jiang, Yuhang Song, Zheng Fang)</author>
      <guid isPermaLink="false">2505.05089v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks</title>
      <link>http://arxiv.org/abs/2505.04981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的深度强化学习（DRL）算法，用于动态THz无人机网络中的资源分配，以最大化资源效率。&lt;h4&gt;背景&lt;/h4&gt;THz无人机网络具有灵活的拓扑结构和超高速数据传输能力，在安全监控、灾害响应和环境保护等领域具有广泛应用前景。然而，动态拓扑结构给无人机之间THz链路的长期联合功率和天线阵列资源分配带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出GLOVE算法，通过GNN学习无人机与其邻近无人机之间的关系，同时强调无人机自身的特征，以实现资源效率的最大化。&lt;h4&gt;方法&lt;/h4&gt;GLOVE算法通过图神经网络（GNN）学习无人机与其邻近无人机之间的交互关系，并利用多任务结构协同训练所有无人机功率和子阵列的资源分配决策。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GLOVE算法在资源效率最高和延迟最低方面优于基准方案，并且在整个训练过程中保持了零数据包丢失，显示出其在高度动态THz无人机网络中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GLOVE算法能够有效解决动态THz无人机网络中的资源分配问题，为无人机网络的应用提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexibletopologies and ultra-high data rates are expected to empower numerousapplications in security surveillance, disaster response, and environmentalmonitoring, among others. However, the dynamic topologies hinder the efficientlong-term joint power and antenna array resource allocation for THz links amongUAVs. Furthermore, the continuous nature of power and the discrete nature ofantennas cause this joint resource allocation problem to be a mixed-integernonlinear programming (MINLP) problem with non-convexity and NP-hardness.Inspired by recent rapid advancements in deep reinforcement learning (DRL), agraph neural network (GNN) aided DRL algorithm for resource allocation in thedynamic THz UAV network with an emphasis on self-node features (GLOVE) isproposed in this paper, with the aim of resource efficiency (RE) maximization.When training the allocation policy for each UAV, GLOVE learns the relationshipbetween this UAV and its neighboring UAVs via GNN, while also emphasizing theimportant self-node features of this UAV. In addition, a multi-task structureis leveraged by GLOVE to cooperatively train resource allocation decisions forthe power and sub-arrays of all UAVs. Experimental results illustrate thatGLOVE outperforms benchmark schemes in terms of the highest RE and the lowestlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVEmaintains zero packet loss during the entire training process, demonstratingits better robustness under the highly dynamic THz UAV network.</description>
      <author>example@mail.com (Zhifeng Hu, Chong Han)</author>
      <guid isPermaLink="false">2505.04981v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
      <link>http://arxiv.org/abs/2505.05467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamBridge是一种将离线视频语言模型（Video-LLMs）转换为流式模型的简单而有效的框架。&lt;h4&gt;背景&lt;/h4&gt;将现有模型应用于在线场景时，存在两个主要挑战：多轮实时理解和缺乏主动响应机制。&lt;h4&gt;目的&lt;/h4&gt;StreamBridge旨在解决上述挑战，提高离线视频语言模型在流式场景下的理解能力。&lt;h4&gt;方法&lt;/h4&gt;StreamBridge包含以下特点：(1) 结合内存缓冲区和圆周衰减压缩策略，支持长上下文的多轮交互；(2) 解耦的轻量级激活模型，易于集成到现有的Video-LLMs中，实现连续的主动响应。同时，构建了大规模数据集Stream-IT，用于流式视频理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，StreamBridge在多种任务中显著提高了离线视频语言模型的流式理解能力，甚至在某些方面超过了GPT-4o和Gemini1.5 Pro等专有模型。在标准视频理解基准测试中，它也实现了有竞争力的或优越的性能。&lt;h4&gt;结论&lt;/h4&gt;StreamBridge是一个有效的框架，可以显著提升离线视频语言模型的流式理解能力，并在多个任务中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present StreamBridge, a simple yet effective framework that seamlesslytransforms offline Video-LLMs into streaming-capable models. It addresses twofundamental challenges in adapting existing models into online scenarios: (1)limited capability for multi-turn real-time understanding, and (2) lack ofproactive response mechanisms. Specifically, StreamBridge incorporates (1) amemory buffer combined with a round-decayed compression strategy, supportinglong-context multi-turn interactions, and (2) a decoupled, lightweightactivation model that can be effortlessly integrated into existing Video-LLMs,enabling continuous proactive responses. To further support StreamBridge, weconstruct Stream-IT, a large-scale dataset tailored for streaming videounderstanding, featuring interleaved video-text sequences and diverseinstruction formats. Extensive experiments show that StreamBridge significantlyimproves the streaming understanding capabilities of offline Video-LLMs acrossvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini1.5 Pro. Simultaneously, it achieves competitive or superior performance onstandard video understanding benchmarks.</description>
      <author>example@mail.com (Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, Ping Huang)</author>
      <guid isPermaLink="false">2505.05467v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.04907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为VaCDA的多源域适应框架，旨在解决可穿戴设备产生的海量未标注数据带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着科技的发展，可穿戴设备中的传感器持续监控用户活动，生成大量未标注数据。这些数据难以解释，手动标注既费时又易出错。数据分布因设备放置、类型和使用行为的不同而异，传统迁移学习方法效果不佳，难以识别日常活动。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，VaCDA结合了变分自编码器（VAE）和对比学习，以改善特征表示并减少源域和目标域之间的异质性。&lt;h4&gt;方法&lt;/h4&gt;VaCDA使用VAE从可用传感器数据中学习一个共享的低维潜在空间，该空间可以泛化不同传感器之间的数据。同时，通过对比学习，将同一类别的实例在域间对齐，同时分离不同类别，以增强特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开数据集上，VaCDA在跨位置和跨设备场景中优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;VaCDA在处理可穿戴设备数据方面表现出色，特别是在解决数据异质性和提升特征表示方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：技术进步导致了带有传感器的可穿戴设备的兴起，这些设备持续监控用户活动，生成大量未标记数据。这些数据难以解释，手动标注既费力又容易出错。由于设备放置、类型和使用行为的差异，数据分布往往异质。因此，传统的迁移学习方法表现不佳，难以识别日常活动。为了解决这些挑战，我们使用变分自编码器（VAE）从可用的传感器数据中学习一个共享的低维潜在空间。这个空间可以泛化不同传感器之间的数据，减轻异质性，并帮助目标域的鲁棒适应。我们通过对比学习来增强特征表示，通过对齐跨域的同一类实例同时分离不同类别。我们提出了变分对比域适应（VaCDA），这是一个结合VAE和对比学习的多源域适应框架，旨在改善特征表示并减少源域和目标域之间的异质性。我们在三个异质场景（跨个人、跨位置和跨设备）的多个公开数据集上评估了VaCDA。在跨位置和跨设备场景中，VaCDA优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technological advancements have led to the rise of wearable devices withsensors that continuously monitor user activities, generating vast amounts ofunlabeled data. This data is challenging to interpret, and manual annotation islabor-intensive and error-prone. Additionally, data distribution is oftenheterogeneous due to device placement, type, and user behavior variations. As aresult, traditional transfer learning methods perform suboptimally, making itdifficult to recognize daily activities. To address these challenges, we use avariational autoencoder (VAE) to learn a shared, low-dimensional latent spacefrom available sensor data. This space generalizes data across diverse sensors,mitigating heterogeneity and aiding robust adaptation to the target domain. Weintegrate contrastive learning to enhance feature representation by aligninginstances of the same class across domains while separating different classes.We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-sourcedomain adaptation framework combining VAEs and contrastive learning to improvefeature representation and reduce heterogeneity between source and targetdomains. We evaluate VaCDA on multiple publicly available datasets across threeheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDAoutperforms the baselines in cross-position and cross-device scenarios.</description>
      <author>example@mail.com (Soham Khisa, Avijoy Chakma)</author>
      <guid isPermaLink="false">2505.04907v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction</title>
      <link>http://arxiv.org/abs/2505.05094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究旨在通过结合联合图学习和网络分析来解决高血压共病问题的早期识别挑战。&lt;h4&gt;背景&lt;/h4&gt;高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;方法&lt;/h4&gt;该方法包括：a) 基于疾病编码构建两个网络，包括患者网络和疾病差异网络；b) 生成三个共病网络特征以捕捉共病与风险疾病之间的潜在关系；c) 结合计算结构干预和学习特征表示，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;主要发现&lt;/h4&gt;基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。&lt;h4&gt;结论&lt;/h4&gt;CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;h4&gt;翻译&lt;/h4&gt;本研究旨在通过结合联合图学习与网络分析解决高血压共病问题的早期识别挑战。研究背景指出，高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。研究目的在于开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。研究方法包括基于疾病编码构建患者网络和疾病差异网络，生成共病网络特征，并结合计算结构干预和学习特征表示。主要发现表明，基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。研究结论指出，CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The comorbidities of hypertension impose a heavy burden on patients andsociety. Early identification is necessary to prompt intervention, but itremains a challenging task. This study aims to address this challenge bycombining joint graph learning with network analysis. Motivated by thisdiscovery, we develop a Conjoint Graph Representation Learning (CGRL) frameworkthat: a) constructs two networks based on disease coding, including the patientnetwork and the disease difference network. Three comorbidity network featureswere generated based on the basic difference network to capture the potentialrelationship between comorbidities and risk diseases; b) incorporatescomputational structure intervention and learning feature representation, CGRLwas developed to predict the risks of diabetes and coronary heart disease inpatients; and c) analysis the comorbidity patterns and exploring the pathwaysof disease progression, the pathological pathogenesis of diabetes and coronaryheart disease may be revealed. The results show that the network featuresextracted based on the difference network are important, and the framework weproposed provides more accurate predictions than other strong models in termsof accuracy.</description>
      <author>example@mail.com (Leming Zhou, Zuo Wang, Zhixuan Duan)</author>
      <guid isPermaLink="false">2505.05094v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Does CLIP perceive art the same way we do?</title>
      <link>http://arxiv.org/abs/2505.05229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力，同时探讨了其与人类感知和上下文理解的一致性。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一个能够通过联合嵌入连接图像和文本的强大多模态模型。&lt;h4&gt;目的&lt;/h4&gt;研究CLIP在提取绘画（包括人类和AI生成的图像）中的高层次语义和风格信息的能力。&lt;h4&gt;方法&lt;/h4&gt;设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，以探索其与人类感知和上下文理解的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。&lt;h4&gt;结论&lt;/h4&gt;讨论了这些发现对于将CLIP用作生成过程（如风格迁移或基于提示的图像合成）中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力。通过设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，探讨了其与人类感知和上下文理解的一致性。研究揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。进一步讨论了这些发现对于将CLIP用作生成过程中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP has emerged as a powerful multimodal model capable of connecting imagesand text through joint embeddings, but to what extent does it "see" the sameway humans do - especially when interpreting artworks? In this paper, weinvestigate CLIP's ability to extract high-level semantic and stylisticinformation from paintings, including both human-created and AI-generatedimagery. We evaluate its perception across multiple dimensions: content, sceneunderstanding, artistic style, historical period, and the presence of visualdeformations or artifacts. By designing targeted probing tasks and comparingCLIP's responses to human annotations and expert benchmarks, we explore itsalignment with human perceptual and contextual understanding. Our findingsreveal both strengths and limitations in CLIP's visual representations,particularly in relation to aesthetic cues and artistic intent. We furtherdiscuss the implications of these insights for using CLIP as a guidancemechanism during generative processes, such as style transfer or prompt-basedimage synthesis. Our work highlights the need for deeper interpretability inmultimodal systems, especially when applied to creative domains where nuanceand subjectivity play a central role.</description>
      <author>example@mail.com (Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu)</author>
      <guid isPermaLink="false">2505.05229v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
      <link>http://arxiv.org/abs/2505.04846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at the Platform for Advanced Scientific  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HiPerRAG的RAG工作流程，通过高性能计算技术索引和检索超过360万篇科学文献中的知识，旨在解决科学文献量增长带来的问题。&lt;h4&gt;背景&lt;/h4&gt;科学文献量呈指数增长，导致发现未被充分利用、重复工作和跨学科合作受限。&lt;h4&gt;目的&lt;/h4&gt;通过提高大语言模型处理信息的事实性来协助科学家。&lt;h4&gt;方法&lt;/h4&gt;HiPerRAG利用高性能计算，包括Oreo模型进行多模态文档解析和ColTrast查询感知编码器算法来增强检索准确性。&lt;h4&gt;主要发现&lt;/h4&gt;HiPerRAG在现有的科学问答基准和两个新基准上表现出色，SciQ准确率达到90%，PubMedQA准确率达到76%，超过了PubMedGPT和GPT-4等特定领域的模型和商业LLM。&lt;h4&gt;结论&lt;/h4&gt;HiPerRAG在Polaris、Sunspot和Frontier超级计算机上扩展到数千个GPU，提供了百万文档规模的RAG工作流程，以统一科学知识和促进跨学科创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3732775.3733586&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The volume of scientific literature is growing exponentially, leading tounderutilized discoveries, duplicated efforts, and limited cross-disciplinarycollaboration. Retrieval Augmented Generation (RAG) offers a way to assistscientists by improving the factuality of Large Language Models (LLMs) inprocessing this influx of information. However, scaling RAG to handle millionsof articles introduces significant challenges, including the high computationalcosts associated with parsing documents and embedding scientific knowledge, aswell as the algorithmic complexity of aligning these representations with thenuanced semantics of scientific content. To address these issues, we introduceHiPerRAG, a RAG workflow powered by high performance computing (HPC) to indexand retrieve knowledge from more than 3.6 million scientific articles. At itscore are Oreo, a high-throughput model for multimodal document parsing, andColTrast, a query-aware encoder fine-tuning algorithm that enhances retrievalaccuracy by using contrastive learning and late-interaction techniques.HiPerRAG delivers robust performance on existing scientific question answeringbenchmarks and two new benchmarks introduced in this work, achieving 90%accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific modelslike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUson the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers milliondocument-scale RAG workflows for unifying scientific knowledge and fosteringinterdisciplinary innovation.</description>
      <author>example@mail.com (Ozan Gokdemir, Carlo Siebenschuh, Alexander Brace, Azton Wells, Brian Hsu, Kyle Hippe, Priyanka V. Setty, Aswathy Ajith, J. Gregory Pauloski, Varuni Sastry, Sam Foreman, Huihuo Zheng, Heng Ma, Bharat Kale, Nicholas Chia, Thomas Gibbs, Michael E. Papka, Thomas Brettin, Francis J. Alexander, Anima Anandkumar, Ian Foster, Rick Stevens, Venkatram Vishwanath, Arvind Ramanathan)</author>
      <guid isPermaLink="false">2505.04846v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
      <link>http://arxiv.org/abs/2505.05001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:  text overlap with arXiv:2403.06378&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视频拼接框架StabStitch++，用于解决视频拼接中出现的扭曲抖动问题，并通过实验证明其在性能、鲁棒性和效率方面优于现有解决方案。&lt;h4&gt;背景&lt;/h4&gt;视频拼接过程中，由于图像扭曲不连续，即使输入视频稳定，拼接后的视频也可能出现扭曲抖动，影响视觉体验。&lt;h4&gt;目的&lt;/h4&gt;提出一种视频拼接框架，实现空间拼接和时序稳定性的同时优化。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一个可微分的双向分解模块，将单应性变换解耦并融入空间扭曲中，平衡两个视图的对齐负担和投影畸变。2. 从视频稳定中的摄像机路径得到视频拼接轨迹的数学表达式。3. 提出了一个扭曲平滑模型，通过混合损失函数同时促进内容对齐、轨迹平滑和在线协作。&lt;h4&gt;主要发现&lt;/h4&gt;StabStitch++在在线模式下同时优化了对齐和稳定性，优于牺牲对齐以换取稳定性的StabStitch。&lt;h4&gt;结论&lt;/h4&gt;StabStitch++通过构建实时在线视频拼接系统，在视频拼接性能、鲁棒性和效率方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nie-lang/stabstitch2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We retarget video stitching to an emerging issue, named warping shake, whichunveils the temporal content shakes induced by sequentially unsmooth warps whenextending image stitching to video stitching. Even if the input videos arestable, the stitched video can inevitably cause undesired warping shakes andaffect the visual experience. To address this issue, we propose StabStitch++, anovel video stitching framework to realize spatial stitching and temporalstabilization with unsupervised learning simultaneously. First, different fromexisting learning-based image stitching solutions that typically warp one imageto align with another, we suppose a virtual midplane between original imageplanes and project them onto it. Concretely, we design a differentiablebidirectional decomposition module to disentangle the homography transformationand incorporate it into our spatial warp, evenly spreading alignment burdensand projective distortions across two views. Then, inspired by camera paths invideo stabilization, we derive the mathematical expression of stitchingtrajectories in video stitching by elaborately integrating spatial and temporalwarps. Finally, a warp smoothing model is presented to produce stable stitchedvideos with a hybrid loss to simultaneously encourage content alignment,trajectory smoothness, and online collaboration. Compared with StabStitch thatsacrifices alignment for stabilization, StabStitch++ makes no compromise andoptimizes both of them simultaneously, especially in the online mode. Toestablish an evaluation benchmark and train the learning framework, we build avideo stitching dataset with a rich diversity in camera motions and scenes.Experiments exhibit that StabStitch++ surpasses current solutions in stitchingperformance, robustness, and efficiency, offering compelling advancements inthis field by building a real-time online video stitching system.</description>
      <author>example@mail.com (Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao)</author>
      <guid isPermaLink="false">2505.05001v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mogao，一个通过因果方法实现交错多模态生成的统一框架，在图像理解和生成方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;当前统一模型在图像理解和生成方面取得了一定的进步，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，通过交错多模态生成来推进这一范式。&lt;h4&gt;方法&lt;/h4&gt;Mogao集成了架构设计方面的多项关键技术改进，包括深度融合设计、双视觉编码器、交错旋转位置嵌入和多模态无分类器引导，以及在大规模内部数据集上的高效训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，并且在产生高质量、连贯的交错输出方面表现出色。它在零样本图像编辑和组合生成方面的能力突显了Mogao作为一个实用的全模态基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;Mogao作为一个实用的全模态基础模型，为未来统一多模态系统的发展铺平了道路，并有望进一步扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation</title>
      <link>http://arxiv.org/abs/2505.05085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了通过传递和Koopman算子方法将复杂非线性动力学系统线性化表示的框架，并提出了一种通过机器学习优化线性算子的近似表示方法。&lt;h4&gt;背景&lt;/h4&gt;传递和Koopman算子方法为表示非线性动力学系统提供了一种线性变换的框架，但直接从数据中高效地估计这些算子的谱是具有挑战性的。&lt;h4&gt;目的&lt;/h4&gt;目的是通过一般算子和表征学习的方法，使用高效的有限维表示来近似这些线性算子。&lt;h4&gt;方法&lt;/h4&gt;使用机器学习技术学习正交归一且局部支撑的基函数，这些基函数根据系统的动力学特性动态定制。这种学习到的基函数提供了算子操作的准确近似以及几乎不变的有限维子空间。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，可以检索估计算子的谱属性，并强调机器学习基函数的动态自适应特性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种基于机器学习的方法来优化线性算子的近似表示，为理解复杂非线性动力学系统的动力学提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling for a deeper understanding of the underlying dynamics. The spectrum of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We tackle this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal, locally supported basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasize the dynamically adaptive quality of the machine-learned basis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer and Koopman operator methods offer a framework for representingcomplex, nonlinear dynamical systems via linear transformations, enabling for adeeper understanding of the underlying dynamics. The spectrum of theseoperators provide important insights into system predictability and emergentbehaviour, although efficiently estimating them from data can be challenging.We tackle this issue through the lens of general operator and representationallearning, in which we approximate these linear operators using efficientfinite-dimensional representations. Specifically, we machine-learn orthonormal,locally supported basis functions that are dynamically tailored to the system.This learned basis provides a particularly accurate approximation of theoperator's action as well as a nearly invariant finite-dimensional subspace. Weillustrate our approach with examples that showcase the retrieval of spectralproperties from the estimated operator, and emphasise the dynamically adaptivequality of the machine-learned basis.</description>
      <author>example@mail.com (Gary Froyland, Kevin Kühl)</author>
      <guid isPermaLink="false">2505.05085v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.04659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GSsplat的可泛化语义高斯混合方法，用于高效地生成未见过的场景的语义合成。&lt;h4&gt;背景&lt;/h4&gt;在3D场景理解的研究中，从多个视角对未见过的场景进行语义合成至关重要。当前的方法在渲染新视角图像和语义图方面表现良好，但速度和分割性能存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了提高合成速度和分割性能，提出了GSsplat方法。&lt;h4&gt;方法&lt;/h4&gt;GSsplat通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定的高斯混合中的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息，并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。&lt;h4&gt;主要发现&lt;/h4&gt;GSsplat在多视角输入变化的情况下，以最快的速度实现了语义合成中的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;GSsplat方法在提高合成速度和分割性能方面取得了显著的成果，为3D场景理解研究提供了有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：未见场景的语义合成从多个视角对3D场景理解研究至关重要。当前的方法能够通过重建可泛化的神经辐射场来渲染新视角的图像和语义图。然而，它们往往在速度和分割性能方面存在限制。我们提出了一种可泛化的语义高斯混合方法（GSsplat）用于高效的novel-view合成。我们的模型通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定高斯混合的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。当用变化数量的多视角输入进行评估时，GSsplat在语义合成方面达到了最快的速度和最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The semantic synthesis of unseen scenes from multiple viewpoints is crucialfor research in 3D scene understanding. Current methods are capable ofrendering novel-view images and semantic maps by reconstructing generalizableNeural Radiance Fields. However, they often suffer from limitations in speedand segmentation performance. We propose a generalizable semantic GaussianSplatting method (GSsplat) for efficient novel-view synthesis. Our modelpredicts the positions and attributes of scene-adaptive Gaussian distributionsfrom once input, replacing the densification and pruning processes oftraditional scene-specific Gaussian Splatting. In the multi-task framework, ahybrid network is designed to extract color and semantic information andpredict Gaussian parameters. To augment the spatial perception of Gaussians forhigh-quality rendering, we put forward a novel offset learning module throughgroup-based supervision and a point-level interaction module with spatial unitaggregation. When evaluated with varying numbers of multi-view inputs, GSsplatachieves state-of-the-art performance for semantic synthesis at the fastestspeed.</description>
      <author>example@mail.com (Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang)</author>
      <guid isPermaLink="false">2505.04659v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
      <link>http://arxiv.org/abs/2505.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对远视图像中密集分布的小孔检测的适应性检测方法，旨在提高爆破作业的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;现有的检测方法在处理小尺度目标、高度密集的排列和有限的小孔视觉特征时存在困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了基于现有架构（如YOLO）的适应性检测方法，该方法通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;该方法引入了三个协同组件：(1)自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；(2)嵌入稳定化，确保一致且可靠的特征提取；(3)上下文细化，利用空间上下文提高检测精度。&lt;h4&gt;主要发现&lt;/h4&gt;EMA在本方法中的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。&lt;h4&gt;结论&lt;/h4&gt;在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在控制的爆破作业中，从远视图像中准确检测密集分布的小孔对于操作安全和效率至关重要。然而，现有的检测方法由于小目标尺度、高度密集的排列和小孔的有限视觉特征而常常遇到困难。为了解决这些挑战，我们提出了一种基于现有架构（例如YOLO）的适应性检测方法，该方法通过通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。我们的方法引入了三个协同组件：（1）自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；（2）嵌入稳定化，确保一致且可靠的特征提取；（3）上下文细化，利用空间上下文提高检测精度。在本方法中EMA的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In controlled blasting operations, accurately detecting densely distributedtiny boreholes from far-view imagery is critical for operational safety andefficiency. However, existing detection methods often struggle due to smallobject scales, highly dense arrangements, and limited distinctive visualfeatures of boreholes. To address these challenges, we propose an adaptivedetection approach that builds upon existing architectures (e.g., YOLO) byexplicitly leveraging consistent embedding representations derived throughexponential moving average (EMA)-based statistical updates.  Our method introduces three synergistic components: (1) adaptive augmentationutilizing dynamically updated image statistics to robustly handle illuminationand texture variations; (2) embedding stabilization to ensure consistent andreliable feature extraction; and (3) contextual refinement leveraging spatialcontext for improved detection accuracy. The pervasive use of EMA in our methodis particularly advantageous given the limited visual complexity and smallscale of boreholes, allowing stable and robust representation learning evenunder challenging visual conditions. Experiments on a challenging proprietaryquarry-site dataset demonstrate substantial improvements over baselineYOLO-based architectures, highlighting our method's effectiveness in realisticand complex industrial scenarios.</description>
      <author>example@mail.com (Xuesong Liu, Tianyu Hao, Emmett J. Ientilucci)</author>
      <guid isPermaLink="false">2505.05008v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FA-KPConv，一种基于KPConv的神经网络架构，用于3D点云分析。通过帧平均技术，FA-KPConv使网络对点云的平移、旋转和反射具有精确的不变性和/或等变性，提高了点云分类和点云注册的性能。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其对欧几里得变换的不变性和/或等变性只能在大数据集或数据增强的情况下近似实现。&lt;h4&gt;目的&lt;/h4&gt;通过引入帧平均技术，使点云神经网络对平移、旋转和/或反射具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;FA-KPConv通过在现有的KPConv网络基础上添加帧平均技术，将几何先验知识嵌入到网络中，同时保持可学习参数的数量，不牺牲任何输入信息。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中展现出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv通过引入帧平均技术，提高了点云神经网络对欧几里得变换的不变性和/或等变性，从而提高了点云分类和点云注册的性能。&lt;h4&gt;翻译&lt;/h4&gt;We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks</title>
      <link>http://arxiv.org/abs/2505.04894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE DCOSS-IoT 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TH-GCN的图卷积网络方法，用于优化密集5G网络中的切换管理，旨在提高网络稳定性。&lt;h4&gt;背景&lt;/h4&gt;5G技术的快速发展改变了车载网络，提供了高带宽、低延迟和快速数据率，这些特性对于智能城市和车辆中的实时应用至关重要，同时也提高了交通安全和娱乐服务。&lt;h4&gt;目的&lt;/h4&gt;解决5G网络在覆盖范围有限和频繁切换导致的网络不稳定问题，尤其是在高移动性环境中。&lt;h4&gt;方法&lt;/h4&gt;TH-GCN使用图神经网络（GNN）将车辆和基站建模为动态图中的节点，该图包含信号质量、吞吐量、车辆速度和基站负载等特征。通过整合用户设备和基站的角度，实现自适应的实时切换决策。&lt;h4&gt;主要发现&lt;/h4&gt;TH-GCN将切换次数减少了多达78%，并将信号质量提高了10%，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TH-GCN通过优化切换管理，显著提高了5G网络的稳定性和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of 5G has transformed vehicular networks, offering highbandwidth, low latency, and fast data rates essential for real-timeapplications in smart cities and vehicles. These improvements enhance trafficsafety and entertainment services. However, the limited coverage and frequenthandovers in 5G networks cause network instability, especially in high-mobilityenvironments due to the ping-pong effect. This paper presents TH-GCN(Throughput-oriented Graph Convolutional Network), a novel approach foroptimizing handover management in dense 5G networks. Using graph neuralnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamicgraph enriched with features such as signal quality, throughput, vehicle speed,and base station load. By integrating both user equipment and base stationperspectives, this dual-centric approach enables adaptive, real-time handoverdecisions that improve network stability. Simulation results show that TH-GCNreduces handovers by up to 78 percent and improves signal quality by 10percent, outperforming existing methods.</description>
      <author>example@mail.com (Nazanin Mehregan, Robson E. De Grande)</author>
      <guid isPermaLink="false">2505.04894v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graffe: Graph Representation Learning via Diffusion Probabilistic Models</title>
      <link>http://arxiv.org/abs/2505.04956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Graffe，一种用于图表示学习的自监督扩散模型。该模型通过图编码器将源图压缩成紧凑表示，该表示作为条件引导扩散解码器的去噪过程。实验表明，Graffe在节点和图分类任务上取得了竞争性结果，并在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;扩散概率模型（DPMs）因其生成高质量样本的潜力而广为人知，但在表示学习领域往往被忽视。尽管最近的研究突出了其在捕获视觉语义方面的潜力，但将DPMs应用于图表示学习仍然处于初级阶段。&lt;h4&gt;目的&lt;/h4&gt;提出Graffe模型，旨在将扩散模型应用于图表示学习，并评估其有效性。&lt;h4&gt;方法&lt;/h4&gt;Graffe模型包括一个图编码器和一个扩散解码器。图编码器将源图压缩成紧凑表示，该表示作为条件来引导扩散解码器的去噪过程。通过理论证明和实践验证来评估模型的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明表明，去噪目标隐式最大化了数据和其表示之间的条件互信息。在实证研究中，Graffe在节点和图分类任务上取得了优异的性能，在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;结论&lt;/h4&gt;扩散模型，尤其是扩散概率模型，是图表示学习的有效工具，特别是Graffe模型在图表示学习方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;Diffusion probabilistic models (DPMs), widely recognized for their potential to generate high-quality samples, tend to go unnoticed in representation learning. While recent progress has highlighted their potential for capturing visual semantics, adapting DPMs to graph representation learning remains in its infancy. In this paper, we introduce Graffe, a self-supervised diffusion model proposed for graph representation learning. It features a graph encoder that distills a source graph into a compact representation, which, in turn, serves as the condition to guide the denoising process of the diffusion decoder. To evaluate the effectiveness of our model, we first explore the theoretical foundations of applying diffusion models to representation learning, proving that the denoising objective implicitly maximizes the conditional mutual information between data and its representation. Specifically, we prove that the negative logarithm of the denoising score matching loss is a tractable lower bound for the conditional mutual information. Empirically, we conduct a series of case studies to validate our theoretical insights. In addition, Graffe delivers competitive results under the linear probing setting on node and graph classification tasks, achieving state-of-the-art performance on 9 of the 11 real-world datasets. These findings indicate that powerful generative models, especially diffusion models, serve as an effective tool for graph representation learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion probabilistic models (DPMs), widely recognized for their potentialto generate high-quality samples, tend to go unnoticed in representationlearning. While recent progress has highlighted their potential for capturingvisual semantics, adapting DPMs to graph representation learning remains in itsinfancy. In this paper, we introduce Graffe, a self-supervised diffusion modelproposed for graph representation learning. It features a graph encoder thatdistills a source graph into a compact representation, which, in turn, servesas the condition to guide the denoising process of the diffusion decoder. Toevaluate the effectiveness of our model, we first explore the theoreticalfoundations of applying diffusion models to representation learning, provingthat the denoising objective implicitly maximizes the conditional mutualinformation between data and its representation. Specifically, we prove thatthe negative logarithm of the denoising score matching loss is a tractablelower bound for the conditional mutual information. Empirically, we conduct aseries of case studies to validate our theoretical insights. In addition,Graffe delivers competitive results under the linear probing setting on nodeand graph classification tasks, achieving state-of-the-art performance on 9 ofthe 11 real-world datasets. These findings indicate that powerful generativemodels, especially diffusion models, serve as an effective tool for graphrepresentation learning.</description>
      <author>example@mail.com (Dingshuo Chen, Shuchen Xue, Liuji Chen, Yingheng Wang, Qiang Liu, Shu Wu, Zhi-Ming Ma, Liang Wang)</author>
      <guid isPermaLink="false">2505.04956v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法。在此基础上，主要目标是开发高性能、适用于实际临床环境的自动疼痛评估计算方法。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论角度出发，对疼痛评估过程进行了研究，并考察了现有的自动方法。&lt;h4&gt;目的&lt;/h4&gt;开发创新计算方法，实现高性能的自动疼痛评估，并在实际临床环境中应用。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法，深入研究和评估影响疼痛感知的显著因素，包括人口统计学元素，并设计、开发、提出适用于不同场景需求的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中提出的方法在疼痛评估方面取得了最先进的结果，并展示了其有效性。&lt;h4&gt;结论&lt;/h4&gt;论文为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract:  This thesis initially aims to study the pain assessment process from aclinical-theoretical perspective while exploring and examining existingautomatic approaches. Building on this foundation, the primary objective ofthis Ph.D. project is to develop innovative computational methods for automaticpain assessment that achieve high performance and are applicable in realclinical settings. A primary goal is to thoroughly investigate and assesssignificant factors, including demographic elements that impact painperception, as recognized in pain research, through a computational standpoint.Within the limits of the available data in this research area, our goal was todesign, develop, propose, and offer automatic pain assessment pipelines forunimodal and multimodal configurations that are applicable to the specificrequirements of different scenarios. The studies published in this Ph.D. thesisshowcased the effectiveness of the proposed methods, achieving state-of-the-artresults. Additionally, they paved the way for exploring new approaches inartificial intelligence, foundation models, and generative artificialintelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Method for Radio Interferometry Deconvolution</title>
      <link>http://arxiv.org/abs/2505.04887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了如何利用压缩感知（CS）理论解决射电干涉仪空间频率采样不完整的问题，提出了一种基于深度字典的全新无监督学习方法，以实现对天体信息的高精度恢复。&lt;h4&gt;背景&lt;/h4&gt;射电干涉仪的空间频率采样不完整导致天体信息恢复困难，压缩感知理论提供了一种稳定且唯一恢复天空中亮度分布的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于压缩感知理论的方法，以实现对射电干涉仪观测到的天空中亮度分布的精确恢复。&lt;h4&gt;方法&lt;/h4&gt;开发了一个深度字典（通过卷积神经网络实现），该字典是多分辨率和过完备的，以实现稀疏表示，并将其整合到压缩感知框架中。在去卷积过程中，模型图像和深度字典交替更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地从噪声测量中恢复具有复杂形态的扩展源，与现有算法相比，动态范围（DR）几乎提高了45到100倍。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法结合了压缩感知的数学严谨性和深度神经网络的表 达能力，在解决射电干涉仪观测数据恢复问题上取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical information remains challenging. To address this ill-posed problem, compressive sensing (CS) provides a robust framework for stable and unique recovery of sky brightness distributions in noisy environments, contingent upon satisfying specific conditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparse representation. Building on this insight, we develop a deep dictionary (realized through a convolutional neural network), which is designed to be multi-resolution and overcomplete, to achieve sparse representation and integrate it within the CS framework. The resulting method is a novel, fully interpretable unsupervised learning approach that combines the mathematical rigor of CS with the expressive power of deep neural networks, effectively bridging the gap between deep learning and classical dictionary methods. During the deconvolution process, the model image and the deep dictionary are updated alternately. This approach enables efficient and accurate recovery of extended sources with complex morphologies from noisy measurements. Comparative analyses with state-of-the-art algorithms demonstrate the outstanding performance of our method, i.e., achieving a dynamic range (DR) nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3847/1538-4365/add1b7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical informationremains challenging. To address this ill-posed problem, compressive sensing(CS)provides a robust framework for stable and unique recovery of sky brightnessdistributions in noisy environments, contingent upon satisfying specificconditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparserepresentation. {{Building on this insight, we develop a deep dictionary(realized through a convolutional neural network), which is designed to bemulti-resolution and overcomplete, to achieve sparse representation andintegrate it within the CS framework. The resulting method is a novel, fullyinterpretable unsupervised learning approach that combines}} the mathematicalrigor of CS with the expressive power of deep neural networks, effectivelybridging the gap between deep learning and classical dictionary methods.{{During the deconvolution process, the model image and the deep dictionary areupdated alternatively.}} This approach enables efficient and accurate recoveryof extended sources with complex morphologies from noisy measurements.Comparative analyses with state-of-the-art algorithms demonstrate theoutstanding performance of our method, i.e., achieving a dynamic range (DR)nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).</description>
      <author>example@mail.com (Lei Yu, Bin Liu, Cheng-Jin Jin, Ru-Rong Chen, Hong-Wei Xi, Bo Peng)</author>
      <guid isPermaLink="false">2505.04887v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
      <link>http://arxiv.org/abs/2505.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在视网膜成像中，基于自监督学习的Vision Transformers（ViTs）在AMD识别任务上的表现，并探讨了领域内预训练的必要性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习使ViTs能够从大规模自然图像数据集中学习鲁棒表示，增强了其在不同领域的泛化能力。在视网膜成像中，基于自然或眼科数据的预训练模型显示出潜力，但领域内预训练的好处尚未确定。&lt;h4&gt;目的&lt;/h4&gt;为了调查领域内预训练的好处，本研究在七个包含70,000张专家标注的DFI数据集上对六个SSL预训练的ViTs进行了基准测试，以识别中晚期AMD。&lt;h4&gt;方法&lt;/h4&gt;研究者对自然图像预训练的iBOT、领域特定模型以及未预训练的ViT-L进行了比较，并发布了BRAMD，一个包含巴西DFI和AMD标签的公开数据集。&lt;h4&gt;主要发现&lt;/h4&gt;自然图像预训练的iBOT在分布外泛化方面表现最佳，其AUROCs为0.80-0.97，优于领域特定模型（AUROCs为0.78-0.96）和未预训练的ViT-L（AUROCs为0.68-0.91）。这些发现强调了基础模型在提高AMD识别价值中的重要性，并挑战了领域内预训练的必要性。&lt;h4&gt;结论&lt;/h4&gt;领域内预训练可能不是必要的，基础模型能够有效提高AMD识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) tolearn robust representations from large-scale natural image datasets, enhancingtheir generalization across domains. In retinal imaging, foundation modelspretrained on either natural or ophthalmic data have shown promise, but thebenefits of in-domain pretraining remain uncertain. To investigate this, webenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasetstotaling 70,000 expert-annotated images for the task of moderate-to-lateage-related macular degeneration (AMD) identification. Our results show thatiBOT pretrained on natural images achieves the highest out-of-distributiongeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,which achieved AUROCs of 0.68-0.91. These findings highlight the value offoundation models in improving AMD identification and challenge the assumptionthat in-domain pretraining is necessary. Furthermore, we release BRAMD, anopen-access dataset (n=587) of DFIs with AMD labels from Brazil.</description>
      <author>example@mail.com (Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar)</author>
      <guid isPermaLink="false">2505.05291v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Network Digital Twin for Route Optimization in 5G/B5G Transport Slicing with What-If Analysis</title>
      <link>http://arxiv.org/abs/2505.04879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at IEEE International  Conference on Communications. \c{opyright}2025 IEEE. Personal use of this  material is permitted. Permission from IEEE must be obtained for all other  uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了5G和B5G网络中动态监控和高级解决方案的需求，以保障服务质量。提出了网络数字孪生（NDT）作为虚拟网络测试的解决方案，并在传输网络领域设计了一个实验平台，以实现智能决策和动态路由优化。&lt;h4&gt;背景&lt;/h4&gt;5G和B5G网络的多样化服务需求，如超低延迟和高带宽，要求动态监控和高级解决方案来确保服务质量。&lt;h4&gt;目的&lt;/h4&gt;设计一个实验平台，利用网络数字孪生（NDT）在传输网络领域进行配置和算法测试，以实现智能决策和动态路由优化。&lt;h4&gt;方法&lt;/h4&gt;构建了一个由图神经网络（GNN）组成的NDT，并在包含8、16和30个节点的三种不同网络拓扑中进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;NDT在实现后的延迟预测中取得了较低的MAPE值，表明其具有较高的准确性，能够为网络性能提供精确的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在生成网络性能的精确见解方面是有效的，为5G/B5G场景中的动态路由优化问题提供了智能决策支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：第五代（5G）和超越5G（B5G）网络的出现引入了从超低延迟到高带宽的多样化服务需求，需要动态监控和高级解决方案来确保服务质量（QoS）。负责连接无线接入网络和核心网络的传输网络将越来越多地面临管理复杂流量模式的不利挑战。网络数字孪生（NDT）概念作为在虚拟网络中测试配置和算法的解决方案而出现。在这种情况下，这项工作在传输网络领域设计了一个包含NDT的实验平台，与虚拟对应物和推荐系统同步，以实现5G/B5G场景中动态路由优化问题的智能决策。我们的NDT由一个图神经网络（GNN）组成，在由8、16和30个节点组成的三种不同网络拓扑中进行了评估。与实施解决方案后的实际延迟相比，它实现了URLLC和eMBB切片的较低MAPE值。这些值表明了高精度，证明了该解决方案在生成特定解决方案实施时网络性能精确见解方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of fifth-generation (5G) and Beyond 5G (B5G) networks introducesdiverse service requirements, from ultra-low latency to high bandwidth,demanding dynamic monitoring and advanced solutions to ensure Quality ofService (QoS). The transport network - responsible for interconnecting theradio access network and core networks - will increasingly face challenges inefficiently managing complex traffic patterns. The Network Digital Twin (NDT)concept emerges as a promising solution for testing configurations andalgorithms in a virtual network before real-world deployment. In this context,this work designs an experimental platform with NDT in a transport networkdomain, synchronizing with the virtual counterpart and a recommendation systemfor what-if analysis, enabling intelligent decision-making for dynamic routeoptimization problems in 5G/B5G scenarios. Our NDT, composed of a Graph NeuralNetwork (GNN), was evaluated across three different network topologiesconsisting of 8, 16, and 30 nodes. It achieved lower MAPE values for URLLC andeMBB slices, comparing latency predictions with actual latency after thesolution implementation. These values indicate high accuracy, demonstrating thesolution's effectiveness in generating precise insights into networkperformance if a particular solution were implemented.</description>
      <author>example@mail.com (Rebecca Aben-Athar, Heitor Anglada, Lucas Costa, João Albuquerque, Abrahão Ferreira, Cristiano Bonato Both, Kleber Cardoso, Silvia Lins, Andrey Silva, Glauco Gonçalves, Ilan Correa, Aldebaro Klautau)</author>
      <guid isPermaLink="false">2505.04879v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Accurate Prediction of Sequential Tensor Properties Using Equivariant Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了利用光学光谱研究材料与光相互作用的强大工具，并展示了其在光电子器件开发中的应用，同时提出了一种新的神经网络模型StepENN，用于预测材料的光学响应。&lt;h4&gt;背景&lt;/h4&gt;光学光谱对于揭示材料的电子结构至关重要，特别是在光电子器件如太阳能电池、发光二极管和光电探测器中，电子结构的理解直接影响器件性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测各向异性块体材料中光学响应的模型，以支持光电子器件的数据驱动设计。&lt;h4&gt;方法&lt;/h4&gt;提出了序列张量属性等变神经网络（StepENN），这是一种图神经网络架构，可以将晶体结构直接映射到不同光子频率下的完整光学张量。StepENN通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基于1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了模型在推导其他光谱相关性质（如光学电导率）的潜力。&lt;h4&gt;结论&lt;/h4&gt;StepENN框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：光学光谱是研究材料与光相互作用的强大工具，揭示了复杂的电子结构，如平坦能带和非平凡拓扑特征。这些见解对于光子器件（包括太阳能电池、发光二极管和光电探测器）的开发和优化至关重要，其中对电子结构的理解直接影响到器件的性能。此外，在各向异性块体材料中，光学响应是方向依赖的，由于其固有的复杂性和晶体对称性的限制，预测这些响应张量仍然具有计算上的挑战。为了解决这一挑战，我们引入了序列张量属性等变神经网络（StepENN），这是一种将晶体结构直接映射到不同光子频率下的完整光学张量的图神经网络架构。通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，StepENN确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。在基于从第一性原理方法计算出的1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了其在推导其他光谱相关性质（如光学电导率）的潜力。这一框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical spectra serve as a powerful tool for probing the interactions betweenmaterials and light, unveiling complex electronic structures such as flat bandsand nontrivial topological features. These insights are crucial for thedevelopment and optimization of photonic devices, including solar cells,light-emitting diodes, and photodetectors, where understanding the electronicstructure directly impacts device performance. Moreover, in anisotropic bulkmaterials, the optical responses are direction-dependent, and predicting thoseresponse tensors still remains computationally demanding due to its inherentcomplexity and the constraint from crystal symmetry. To address this challenge,we introduce the sequential tensorial properties equivariant neural network(StepENN), a graph neural network architecture that maps crystal structuresdirectly to their full optical tensors across different photon frequencies. Byencoding the isotropic sequential scalar components and anisotropic sequentialtensor components into l=0 and l=2 spherical tensor components, StepENN ensuressymmetry-aware sequential tensor predictions that are consistent with theinherent symmetry constraints of crystal systems. Trained on a dataset offrequency-dependent permittivity tensors for 1,432 bulk semiconductors computedfrom first-principles methods, our model achieves a mean absolute error (MAE)of 24.216 millifarads per meter (mF/m) on the predicted tensorial spectra with85.7% of its predictions exhibiting less than 10% relative error, demonstratingits potential for deriving other spectrum-related properties, such as opticalconductivity. This framework opens new avenues for the data-driven design ofmaterials with engineered anisotropic optical responses, accelerating materialadvances in optoelectronic applications.</description>
      <author>example@mail.com (Ting-Wei Hsu, Zhenyao Fang, Arun Bansil, Qimin Yan)</author>
      <guid isPermaLink="false">2505.04862v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Piecewise Constant Spectral Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to TMLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PieCoN（分段常数谱图神经网络），这是一种结合常数谱滤波器和多项式滤波器的图神经网络，旨在更灵活地利用图结构，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;背景&lt;/h4&gt;现有的谱GNN使用低阶多项式滤波器捕获图谱特性，但可能无法充分识别图的谱特性，因为多项式阶数低；同时，增加多项式阶数会导致计算成本上升，且超过一定阈值会带来性能平台期或下降。&lt;h4&gt;目的&lt;/h4&gt;设计PieCoN以解决现有谱GNN的这些挑战，提供一种更灵活的方法来利用图结构。&lt;h4&gt;方法&lt;/h4&gt;PieCoN结合常数谱滤波器和多项式滤波器，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;主要发现&lt;/h4&gt;在包括同质和异质图的九个基准数据集上的实验表明，PieCoN在异质数据集上尤其有效，突出了其在广泛领域的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;PieCoN通过结合不同类型的滤波器并自适应地处理谱区间，有效地提高了图神经网络学习谱特性的能力，尤其是在异质图数据集上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network (PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved significant success across variousdomains by leveraging graph structures in data. Existing spectral GNNs, whichuse low-degree polynomial filters to capture graph spectral properties, may notfully identify the graph's spectral characteristics because of the polynomial'ssmall degree. However, increasing the polynomial degree is computationallyexpensive and beyond certain thresholds leads to performance plateaus ordegradation. In this paper, we introduce the Piecewise Constant Spectral GraphNeural Network(PieCoN) to address these challenges. PieCoN combines constantspectral filters with polynomial filters to provide a more flexible way toleverage the graph structure. By adaptively partitioning the spectrum intointervals, our approach increases the range of spectral properties that can beeffectively learned. Experiments on nine benchmark datasets, including bothhomophilic and heterophilic graphs, demonstrate that PieCoN is particularlyeffective on heterophilic datasets, highlighting its potential for a wide rangeof applications.</description>
      <author>example@mail.com (Vahan Martirosyan, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2505.04808v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.02393v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IEF-VAD的视频异常检测框架，该框架通过图像-事件融合的方法，从RGB视频中直接合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合，以解决现有视频异常检测器依赖RGB帧而缺乏时间分辨率的问题。&lt;h4&gt;背景&lt;/h4&gt;大多数现有的视频异常检测器依赖于RGB帧，这些帧无法捕捉到异常事件的突然或短暂的运动线索，而这些线索是异常事件的关键指标。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的视频异常检测方法，以提高检测的准确性和鲁棒性，并减少对特定事件传感器的需求。&lt;h4&gt;方法&lt;/h4&gt;IEF-VAD框架包括：(i) 使用Student-t似然模型对重尾传感器噪声进行建模，并通过拉普拉斯近似推导值级逆方差权重；(ii) 应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；(iii) 通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。&lt;h4&gt;主要发现&lt;/h4&gt;IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平，无需专用的事件传感器或帧级标签。&lt;h4&gt;结论&lt;/h4&gt;合成事件表示在强调运动线索方面具有效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的视频异常检测器仅依赖于RGB帧，这缺乏捕捉突然或短暂运动线索的时间分辨率，而运动线索是异常事件的关键指标。为了解决这一局限性，我们提出了图像-事件融合视频异常检测（IEF-VAD）框架，该框架直接从RGB视频中合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合。该系统（i）使用Student-t似然对重尾传感器噪声进行建模，通过拉普拉斯近似推导值级逆方差权重；（ii）应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；（iii）通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。在没有专用事件传感器或帧级标签的情况下，IEF-VAD在多个真实世界的异常检测基准测试中设定了新的水平。这些发现突出了合成事件表示在强调运动线索方面的效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。代码和模型可在https://github.com/EavnJeong/IEF-VAD上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/eavnjeong/ief-vad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing video anomaly detectors rely solely on RGB frames, which lackthe temporal resolution needed to capture abrupt or transient motion cues, keyindicators of anomalous events. To address this limitation, we proposeImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework thatsynthesizes event representations directly from RGB videos and fuses them withimage features through a principled, uncertainty-aware process. The system (i)models heavy-tailed sensor noise with a Student`s-t likelihood, derivingvalue-level inverse-variance weights via a Laplace approximation; (ii) appliesKalman-style frame-wise updates to balance modalities over time; and (iii)iteratively refines the fused latent state to erase residual cross-modal noise.Without any dedicated event sensor or frame-level labels, IEF-VAD sets a newstate of the art across multiple real-world anomaly detection benchmarks. Thesefindings highlight the utility of synthetic event representations inemphasizing motion cues that are often underrepresented in RGB frames, enablingaccurate and robust video understanding across diverse applications withoutrequiring dedicated event sensors. Code and models are available athttps://github.com/EavnJeong/IEF-VAD.</description>
      <author>example@mail.com (Sungheon Jeong, Jihong Park, Mohsen Imani)</author>
      <guid isPermaLink="false">2505.02393v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2505.05279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MTL-UE的统一框架，用于生成多任务数据和多任务学习模型不可学习的示例。&lt;h4&gt;背景&lt;/h4&gt;现有的不可学习策略主要集中在防止未经授权的用户使用个人数据训练单任务学习模型。然而，近年来，研究重点已转向多任务数据和多任务学习，旨在开发能够同时处理多个任务的通用和基础模型。&lt;h4&gt;目的&lt;/h4&gt;尽管多任务数据和模型日益重要，但在追求不可学习策略时，它们却被大量忽视。本文旨在提出一种针对多任务数据和模型的不可学习策略。&lt;h4&gt;方法&lt;/h4&gt;MTL-UE通过设计一个基于生成器的结构，引入标签先验和类别的特征嵌入，来优化扰动，从而提高攻击性能。此外，它还结合了任务内和任务间嵌入正则化，以增加类间分离并抑制类内方差，从而大大增强攻击的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;MTL-UE对密集预测任务中的多任务学习提供了良好的支持，并且可以即插即用，允许与现有的依赖代理的不可学习方法轻松集成。实验表明，MTL-UE在4个多任务数据集、3种基础不可学习方法和5种模型骨干以及5种多任务任务加权策略上均实现了优越的攻击性能。&lt;h4&gt;结论&lt;/h4&gt;MTL-UE是一种有效的方法，可以生成多任务数据和模型不可学习的示例，并在多个数据集和模型上表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing unlearnable strategies focus on preventing unauthorized usersfrom training single-task learning (STL) models with personal data.Nevertheless, the paradigm has recently shifted towards multi-task data andmulti-task learning (MTL), targeting generalist and foundation models that canhandle multiple tasks simultaneously. Despite their growing importance, MTLdata and models have been largely neglected while pursuing unlearnablestrategies. This paper presents MTL-UE, the first unified framework forgenerating unlearnable examples for multi-task data and MTL models. Instead ofoptimizing perturbations for each sample, we design a generator-based structurethat introduces label priors and class-wise feature embeddings which leads tomuch better attacking performance. In addition, MTL-UE incorporates intra-taskand inter-task embedding regularization to increase inter-class separation andsuppress intra-class variance which enhances the attack robustness greatly.Furthermore, MTL-UE is versatile with good supports for dense prediction tasksin MTL. It is also plug-and-play allowing integrating existingsurrogate-dependent unlearnable methods with little adaptation. Extensiveexperiments show that MTL-UE achieves superior attacking performanceconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5MTL task-weighting strategies.</description>
      <author>example@mail.com (Yi Yu, Song Xia, Siyuan Yang, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot)</author>
      <guid isPermaLink="false">2505.05279v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
      <link>http://arxiv.org/abs/2505.04899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Organ-Wise Tokenization (OWT)的框架，通过Token Group-based Reconstruction (TGR)训练范式来解决图像表示学习中的可解释性和泛化性问题。&lt;h4&gt;背景&lt;/h4&gt;当前代表学习依赖黑盒嵌入，难以解释和泛化，这在医学图像分析中尤为重要。&lt;h4&gt;目的&lt;/h4&gt;提出OWT框架以解决上述局限性，提高医学图像分析的解可释性、泛化性和效率。&lt;h4&gt;方法&lt;/h4&gt;OWT将图像显式地分解为可分离的token组，每组对应一个独特的器官或语义实体。&lt;h4&gt;主要发现&lt;/h4&gt;OWT在CT和MRI数据集上的实验表明，其在图像重建和分割方面的性能强，并且能够实现新型语义级生成和检索应用。&lt;h4&gt;结论&lt;/h4&gt;OWT作为语义解耦表示学习的基础框架，具有广泛的适用性和可扩展性，适用于实际医学图像分析场景及其他领域。&lt;h4&gt;翻译&lt;/h4&gt;近期在表示学习方面取得的进展常常依赖于整体、黑盒的嵌入，这会混杂多个语义组件，限制了可解释性和泛化性。这些问题在医学影像分析中尤为关键。为了解决这些限制，我们提出了一种基于器官的标记化（OWT）框架和基于标记组重建（TGR）训练范式。与产生整体特征的常规方法不同，OWT明确地将图像分解为可分离的标记组，每个组对应一个独特的器官或语义实体。我们的设计确保每个标记组封装了器官特定的信息，从而提高了可解释性、泛化性和效率，同时在下游任务中允许细粒度的控制。在CT和MRI数据集上的实验表明，OWT不仅实现了强大的图像重建和分割性能，而且还能实现标准整体嵌入方法无法达到的新型语义级生成和检索应用。这些发现强调了OWT作为语义解耦表示学习基础框架的潜力，它提供了广泛的扩展性和适用性，可以应用于现实世界的医学图像分析场景及其它领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning often rely on holistic, black-boxembeddings that entangle multiple semantic components, limitinginterpretability and generalization. These issues are especially critical inmedical imaging. To address these limitations, we propose an Organ-WiseTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)training paradigm. Unlike conventional approaches that produce holisticfeatures, OWT explicitly disentangles an image into separable token groups,each corresponding to a distinct organ or semantic entity. Our design ensureseach token group encapsulates organ-specific information, boostinginterpretability, generalization, and efficiency while allowing fine-grainedcontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate theeffectiveness of OWT in not only achieving strong image reconstruction andsegmentation performance, but also enabling novel semantic-level generation andretrieval applications that are out of reach for standard holistic embeddingmethods. These findings underscore the potential of OWT as a foundationalframework for semantically disentangled representation learning, offering broadscalability and applicability to real-world medical imaging scenarios andbeyond.</description>
      <author>example@mail.com (Sifan Song, Siyeop Yoon, Pengfei Jin, Sekeun Kim, Matthew Tivnan, Yujin Oh, Runqi Meng, Ling Chen, Zhiliang Lyu, Dufan Wu, Ning Guo, Xiang Li, Quanzheng Li)</author>
      <guid isPermaLink="false">2505.04899v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Representing spherical tensors with scalar-based machine-learning models</title>
      <link>http://arxiv.org/abs/2505.05404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在物理学中旋转对称性的重要作用，以及如何通过旋转群的结构来描述三维物体在刚体旋转作用下的性质变化。&lt;h4&gt;背景&lt;/h4&gt;旋转对称性在物理学中扮演核心角色，它提供了一个优雅的框架来描述从原子到宏观尺度三维物体的性质在刚体旋转作用下的变换。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索一种新的方法来解决旋转对称性学习问题，该方法通过将等变函数表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积来实现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的方法，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积，同时提出了对通用表达式的近似，这些近似虽然缺乏通用逼近能力，但速度快、易于实现且在实际应用中准确。&lt;h4&gt;主要发现&lt;/h4&gt;发现了一种新的等变函数表示方法，该方法通过将标量函数和具有适当对称性的小张量基相乘来处理旋转对称性学习问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出的近似方法在处理旋转对称性学习问题时既快速又简单，同时在实际应用中保持了较高的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：旋转对称性在物理学中起着核心作用，提供了一个优雅的框架来描述三维物体（从原子到宏观尺度）在刚体旋转作用下的性质变化。三维点云的等变模型能够以与旋转群结构完全一致的方式近似结构-性质关系，通过结合自身为球张量的中间表示。然而，对称性约束使得这种方法在计算上具有挑战性，且实现起来繁琐，这促使越来越受欢迎的无约束架构在训练过程中学习近似对称性。在本工作中，我们探索了第三种解决学习问题的途径，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积。我们还提出了对通用表达式的近似，虽然缺乏通用逼近能力，但这些近似方法快速、易于实现，且在实际设置中准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rotational symmetry plays a central role in physics, providing an elegantframework to describe how the properties of 3D objects -- from atoms to themacroscopic scale -- transform under the action of rigid rotations. Equivariantmodels of 3D point clouds are able to approximate structure-property relationsin a way that is fully consistent with the structure of the rotation group, bycombining intermediate representations that are themselves spherical tensors.The symmetry constraints however make this approach computationally demandingand cumbersome to implement, which motivates increasingly popular unconstrainedarchitectures that learn approximate symmetries as part of the trainingprocess. In this work, we explore a third route to tackle this learningproblem, where equivariant functions are expressed as the product of a scalarfunction of the point cloud coordinates and a small basis of tensors with theappropriate symmetry. We also propose approximations of the general expressionsthat, while lacking universal approximation properties, are fast, simple toimplement, and accurate in practical settings.</description>
      <author>example@mail.com (Michelangelo Domina, Filippo Bigi, Paolo Pegolo, Michele Ceriotti)</author>
      <guid isPermaLink="false">2505.05404v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Clustering with Communication: A Variational Framework for Single Cell Representation Learning</title>
      <link>http://arxiv.org/abs/2505.04891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCCVAE的新型变分自动编码器框架，该框架将细胞间通讯信号纳入单细胞表征学习中，通过实验证明其能提高单细胞分析的聚类性能。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序（scRNA-seq）揭示了细胞异质性，但理解生物功能还需要考虑细胞间通讯（CCC），即通过配体-受体对介导的信号交互来协调细胞行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将细胞间通讯信号纳入单细胞表征学习的方法，以更好地理解细胞行为。&lt;h4&gt;方法&lt;/h4&gt;提出CCCVAE框架，利用从配体-受体相互作用中导出的通讯感知核和稀疏高斯过程，将生物学信息先验编码到潜在空间中。与传统的独立处理每个细胞的VAE不同，CCCVAE鼓励潜在嵌入反映转录相似性和细胞间通讯背景。&lt;h4&gt;主要发现&lt;/h4&gt;在四个scRNA-seq数据集上的实证结果表明，CCCVAE提高了聚类性能，其评估分数高于标准的VAE基线。&lt;h4&gt;结论&lt;/h4&gt;将生物学先验嵌入深度生成模型对于无监督单细胞分析具有价值。&lt;h4&gt;翻译&lt;/h4&gt;Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular heterogeneity, but recent studies emphasize that understanding biological function also requires modeling cell-cell communication (CCC), the signaling interactions mediated by ligand-receptor pairs that coordinate cellular behavior. Tools like CellChat have demonstrated that CCC plays a critical role in processes such as cell differentiation, tissue regeneration, and immune response, and that transcriptomic data inherently encodes rich information about intercellular signaling. We propose CCCVAE, a novel variational autoencoder framework that incorporates CCC signals into single-cell representation learning. By leveraging a communication-aware kernel derived from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes biologically informed priors into the latent space. Unlike conventional VAEs that treat each cell independently, CCCVAE encourages latent embeddings to reflect both transcriptional similarity and intercellular signaling context. Empirical results across four scRNA-seq datasets show that CCCVAE improves clustering performance, achieving higher evaluation scores than standard VAE baselines. This work demonstrates the value of embedding biological priors into deep generative models for unsupervised single-cell analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) has revealed complex cellularheterogeneity, but recent studies emphasize that understanding biologicalfunction also requires modeling cell-cell communication (CCC), the signalinginteractions mediated by ligand-receptor pairs that coordinate cellularbehavior. Tools like CellChat have demonstrated that CCC plays a critical rolein processes such as cell differentiation, tissue regeneration, and immuneresponse, and that transcriptomic data inherently encodes rich informationabout intercellular signaling. We propose CCCVAE, a novel variationalautoencoder framework that incorporates CCC signals into single-cellrepresentation learning. By leveraging a communication-aware kernel derivedfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodesbiologically informed priors into the latent space. Unlike conventional VAEsthat treat each cell independently, CCCVAE encourages latent embeddings toreflect both transcriptional similarity and intercellular signaling context.Empirical results across four scRNA-seq datasets show that CCCVAE improvesclustering performance, achieving higher evaluation scores than standard VAEbaselines. This work demonstrates the value of embedding biological priors intodeep generative models for unsupervised single-cell analysis.</description>
      <author>example@mail.com (Cong Qi, Yeqing Chen, Jie Zhang, Wei Zhi)</author>
      <guid isPermaLink="false">2505.04891v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
      <link>http://arxiv.org/abs/2505.05062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉基础模型在长尾半监督学习（LTSSL）中的影响，提出了一种新的轻量级微调策略ULFine，以降低训练成本并提高预测准确性。&lt;h4&gt;背景&lt;/h4&gt;基于CLIP等大规模视觉基础模型在下游任务中的成功，本文旨在探索这些模型对LTSSL的影响。&lt;h4&gt;目的&lt;/h4&gt;分析大规模视觉基础模型对LTSSL的影响，并提出一种新的微调策略。&lt;h4&gt;方法&lt;/h4&gt;采用三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））对基础模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;FFT导致模型性能下降，LP和LFT虽然提高了整体模型性能，但对长尾类别的影响微乎其微。LP由于训练数据未充分学习产生大量错误伪标签，而LFT减少了这些错误标签的数量，但训练数据偏差导致对它们过度自信。&lt;h4&gt;结论&lt;/h4&gt;ULFine策略通过文本原型自信度感知的适应性调整和双对数互补融合，减轻了过度自信，并对抗了伪标签和分类器偏差，显著降低了训练成本并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;基于大规模视觉基础模型（如CLIP）在各种下游任务中的成功，本文最初尝试通过采用基础模型的三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））来探索其对长尾半监督学习（LTSSL）的影响。我们的分析提出了以下见解：i）与从头开始训练的LTSSL算法相比，FFT导致模型性能下降，而LP和LFT虽然提高了整体模型性能，但对长尾类别的益处微乎其微。ii）LP由于未充分学习的训练数据而产生大量错误伪标签，而LFT可以减少这些错误标签的数量，但由于训练数据偏差，对它们变得过度自信。这加剧了LTSSL中固有的伪标签和分类器偏差，限制了长尾类别的性能提升。基于这些见解，我们提出了一种无偏轻量级微调策略，extbf{ULFine}，通过文本原型自信度感知的适应性调整减轻过度自信，并通过双对数互补融合对抗伪标签和分类器偏差。大量实验表明，与最先进的方法相比，ULFine显著降低了训练成本超过十倍，并大幅提高了预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the success of large-scale visual foundation models like CLIP invarious downstream tasks, this paper initially attempts to explore their impacton Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundationmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning(LFT), and Full Fine-Tuning (FFT). Our analysis presents the followinginsights: i) Compared to LTSSL algorithms trained from scratch, FFT results ina decline in model performance, whereas LP and LFT, although boosting overallmodel performance, exhibit negligible benefits to tail classes. ii) LP producesnumerous false pseudo-labels due to \textit{underlearned} training data, whileLFT can reduce the number of these false labels but becomes overconfident aboutthem owing to \textit{biased fitting} training data. This exacerbates thepseudo-labeled and classifier biases inherent in LTSSL, limiting performanceimprovement in the tail classes. With these insights, we propose a UnbiasedLightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates theoverconfidence via confidence-aware adaptive fitting of textual prototypes andcounteracts the pseudo-labeled and classifier biases via complementary fusionof dual logits. Extensive experiments demonstrate that ULFine markedlydecreases training costs by over ten times and substantially increasesprediction accuracies compared to state-of-the-art methods.</description>
      <author>example@mail.com (Enhao Zhang, Chaohua Li, Chuanxing Geng, Songcan Chen)</author>
      <guid isPermaLink="false">2505.05062v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model（SAM）的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM模型在语义分割应用中取得了成功，但其不确定性量化（UQ）方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的不确定性量化模型，用于量化SAM的不确定性。&lt;h4&gt;方法&lt;/h4&gt;基于贝叶斯熵公式，提出了一种名为USAM的轻量级后处理UQ方法，并分析了不确定性来源。&lt;h4&gt;主要发现&lt;/h4&gt;USAM模型在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上表现出优异的预测能力，为UQ提供了一种计算成本低、易于使用的替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM模型能够支持用户提示、增强半监督流程或平衡准确性与成本效率之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://nianticlabs.github.io/placeit3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的任务：在真实3D场景中进行语言引导的物体放置。该任务通过文本提示来指导3D资产的放置，并提出了新的基准和评估协议。&lt;h4&gt;背景&lt;/h4&gt;在3D场景中进行语言引导的定位任务存在挑战，如放置位置的模糊性和对3D几何关系和自由空间的理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够根据文本提示在真实3D场景中放置3D资产的模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基准和评估协议，引入了一个新的数据集用于训练3D语言大模型，并提出了第一个非平凡基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;该任务具有多解性，需要考虑3D几何关系和自由空间。&lt;h4&gt;结论&lt;/h4&gt;该任务和新的基准有望成为评估和比较通用3D语言大模型的标准之一。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了在真实3D场景中进行语言引导的物体放置的新任务。我们的模型被给定一个3D场景的点云、一个3D资产和一个文本提示，该提示广泛描述了3D资产应该放置的位置。这里的任务是找到尊重提示的有效放置。与3D场景中的其他语言引导定位任务（如基础）相比，这个任务具有特定的挑战：它是不确定的，因为它有多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出一个新的基准和评估协议来启动这个任务。我们还介绍了一个新的数据集，用于在这个任务上训练3D LLMs，以及第一个作为非平凡基线的方法。我们认为这个具有挑战性的任务和我们的新基准可以成为用于评估和比较通用3D LLM模型的标准工具包的一部分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the novel task of Language-Guided Object Placement in Real 3DScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textualprompt broadly describing where the 3D asset should be placed. The task here isto find a valid placement for the 3D asset that respects the prompt. Comparedwith other language-guided localization tasks in 3D scenes such as grounding,this task has specific challenges: it is ambiguous because it has multiplevalid solutions, and it requires reasoning about 3D geometric relationships andfree space. We inaugurate this task by proposing a new benchmark and evaluationprotocol. We also introduce a new dataset for training 3D LLMs on this task, aswell as the first method to serve as a non-trivial baseline. We believe thatthis challenging task and our new benchmark could become part of the suite ofbenchmarks used to evaluate and compare generalist 3D LLM models.</description>
      <author>example@mail.com (Ahmed Abdelreheem, Filippo Aleotti, Jamie Watson, Zawar Qureshi, Abdelrahman Eldesokey, Peter Wonka, Gabriel Brostow, Sara Vicente, Guillermo Garcia-Hernando)</author>
      <guid isPermaLink="false">2505.05288v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture</title>
      <link>http://arxiv.org/abs/2505.04980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。&lt;h4&gt;背景&lt;/h4&gt;LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。&lt;h4&gt;目的&lt;/h4&gt;集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。&lt;h4&gt;方法&lt;/h4&gt;生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。&lt;h4&gt;结论&lt;/h4&gt;通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。本文集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel Large Vision-Language Model (LVLM) and ModelPredictive Control (MPC) integration framework that delivers both taskscalability and safety for Autonomous Driving (AD). LVLMs excel at high-leveltask planning across diverse driving scenarios. However, since these foundationmodels are not specifically designed for driving and their reasoning is notconsistent with the feasibility of low-level motion planning, concerns remainregarding safety and smooth task switching. This paper integrates LVLMs withMPC Builder, which automatically generates MPCs on demand, based on symbolictask commands generated by the LVLM, while ensuring optimality and safety. Thegenerated MPCs can strongly assist the execution or rejection of LVLM-driventask switching by providing feedback on the feasibility of the given tasks andgenerating task-switching-aware MPCs. Our approach provides a safe, flexible,and adaptable control framework, bridging the gap between cutting-edgefoundation models and reliable vehicle operation. We demonstrate theeffectiveness of our approach through a simulation experiment, showing that oursystem can safely and effectively handle highway driving while maintaining theflexibility and adaptability of LVLMs.</description>
      <author>example@mail.com (Kazuki Atsuta, Kohei Honda, Hiroyuki Okuda, Tatsuya Suzuki)</author>
      <guid isPermaLink="false">2505.04980v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2505.04965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DenseGrounding的新方法，用于使智能代理通过自然语言理解和交互3D环境，该方法在ego-centric 3D visual grounding任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;理解3D环境并通过自然语言与之互动对于推动机器人和人机交互至关重要。ego-centric 3D visual grounding任务要求代理根据口头描述在真实世界的3D空间中定位目标对象。&lt;h4&gt;目的&lt;/h4&gt;解决ego-centric 3D visual grounding任务中存在的两个主要挑战：(1) 由于点云与ego-centric多视图图像融合的稀疏性导致的细粒度视觉语义丢失；(2) 由于任意语言描述而导致的有限的文本语义上下文。&lt;h4&gt;方法&lt;/h4&gt;DenseGrounding通过增强视觉和文本语义来解决上述问题。对于视觉特征，引入了层次场景语义增强器（Hierarchical Scene Semantic Enhancer），通过捕捉细粒度的全局场景特征并促进跨模态对齐来保留密集的语义。对于文本描述，提出了语言语义增强器（Language Semantic Enhancer），利用大型语言模型提供丰富的上下文和多样化的语言描述，并在模型训练期间增加额外的上下文。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，DenseGrounding在整体准确率方面显著优于现有方法，在全数据集和较小的迷你子集上分别提高了5.81%和7.56%，进一步推动了ego-centric 3D visual grounding的SOTA（最先进的技术水平）。该方法在CVPR 2024自主挑战赛的多视图3D visual grounding赛道中获得了第一名并获得了创新奖，验证了其有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DenseGrounding是一种有效的ego-centric 3D visual grounding方法，能够显著提高准确率，并在实际竞赛中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling intelligent agents to comprehend and interact with 3D environmentsthrough natural language is crucial for advancing robotics and human-computerinteraction. A fundamental task in this field is ego-centric 3D visualgrounding, where agents locate target objects in real-world 3D spaces based onverbal descriptions. However, this task faces two significant challenges: (1)loss of fine-grained visual semantics due to sparse fusion of point clouds withego-centric multi-view images, (2) limited textual semantic context due toarbitrary language descriptions. We propose DenseGrounding, a novel approachdesigned to address these issues by enhancing both visual and textualsemantics. For visual features, we introduce the Hierarchical Scene SemanticEnhancer, which retains dense semantics by capturing fine-grained global scenefeatures and facilitating cross-modal alignment. For text descriptions, wepropose a Language Semantic Enhancer that leverages large language models toprovide rich context and diverse language descriptions with additional contextduring model training. Extensive experiments show that DenseGroundingsignificantly outperforms existing methods in overall accuracy, withimprovements of 5.81% and 7.56% when trained on the comprehensive full datasetand smaller mini subset, respectively, further advancing the SOTA in egocentric3D visual grounding. Our method also achieves 1st place and receives theInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3DVisual Grounding Track, validating its effectiveness and robustness.</description>
      <author>example@mail.com (Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Zhongchao Shi, Gao Huang)</author>
      <guid isPermaLink="false">2505.04965v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization</title>
      <link>http://arxiv.org/abs/2505.04880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GroverGPT-2的基于大型语言模型（LLM）的方法，用于模拟Grover算法，并通过实验验证了经典模型能够捕捉量子算法结构的能力。&lt;h4&gt;背景&lt;/h4&gt;量子计算在特定任务上理论上优于经典计算，但实际量子优势的边界尚不明确。研究经典机器学习模拟量子算法的能力对于理解这一边界至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究经典机器学习模拟量子算法的能力，特别是探索大型语言模型在模拟Grover算法方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出GroverGPT-2，一种基于LLM的方法，使用思维链（CoT）推理和量子本地标记化来模拟Grover算法。GroverGPT-2能够直接从量子电路表示中执行模拟，并生成逻辑结构和可解释的输出。&lt;h4&gt;主要发现&lt;/h4&gt;GroverGPT-2能够通过有效处理量子本地标记来学习和内化量子电路逻辑，提供了经典模型如LLM能够捕捉量子算法结构的直接证据。GroverGPT-2的输出将电路数据与自然语言交织，将明确的推理嵌入到模拟中。此外，还发现GroverGPT-2的实证缩放定律与量子比特数量增加有关，为可扩展的经典模拟提供了一条路径。&lt;h4&gt;结论&lt;/h4&gt;这些发现为探索经典模拟的极限、提高量子教育与研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：量子计算在特定任务上相对于经典计算具有理论优势，然而实际量子优势的边界仍是一个未解之谜。为了研究这个边界，了解经典机器是否能以及如何学习模拟量子算法至关重要。近年来大型语言模型（LLM）在推理能力上的进步，促使人们探索其在这一挑战中的潜力。在这项工作中，我们引入了GroverGPT-2，这是一种基于LLM的方法，利用思维链（CoT）推理和量子本地标记化来模拟Grover算法。在先前的作品基础上，GroverGPT-2可以直接从量子电路表示中进行模拟，并产生逻辑结构化和可解释的输出。我们的结果表明，GroverGPT-2可以通过高效处理量子本地标记来学习和内化量子电路逻辑，为经典模型如LLM能够捕捉量子算法结构提供了直接证据。此外，GroverGPT-2将电路数据与自然语言交织，将明确的推理嵌入到模拟中。这种双重能力使GroverGPT-2成为提高机器对量子算法理解和建模量子电路逻辑的原型。我们还确定了GroverGPT-2随量子比特数量增加的实证缩放定律，为可扩展的经典模拟指明了一条路径。这些发现为探索经典模拟的极限、增强量子教育和研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum computing offers theoretical advantages over classical computing forspecific tasks, yet the boundary of practical quantum advantage remains an openquestion. To investigate this boundary, it is crucial to understand whether,and how, classical machines can learn and simulate quantum algorithms. Recentprogress in large language models (LLMs) has demonstrated strong reasoningabilities, prompting exploration into their potential for this challenge. Inthis work, we introduce GroverGPT-2, an LLM-based method for simulatingGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-nativetokenization. Building on its predecessor, GroverGPT-2 performs simulationdirectly from quantum circuit representations while producing logicallystructured and interpretable outputs. Our results show that GroverGPT-2 canlearn and internalize quantum circuit logic through efficient processing ofquantum-native tokens, providing direct evidence that classical models likeLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2outputs interleave circuit data with natural language, embedding explicitreasoning into the simulation. This dual capability positions GroverGPT-2 as aprototype for advancing machine understanding of quantum algorithms andmodeling quantum circuit logic. We also identify an empirical scaling law forGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalableclassical simulation. These findings open new directions for exploring thelimits of classical simulatability, enhancing quantum education and research,and laying groundwork for future foundation models in quantum computing.</description>
      <author>example@mail.com (Min Chen, Jinglei Cheng, Pingzhi Li, Haoran Wang, Tianlong Chen, Junyu Liu)</author>
      <guid isPermaLink="false">2505.04880v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.04861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mix-QSAM的混合精度后训练量化框架，用于提高Segment Anything Model (SAM)在资源受限设备上的部署性能。&lt;h4&gt;背景&lt;/h4&gt;SAM是一个流行的视觉基础模型，但由于其计算和内存需求高，在资源受限设备上的部署面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对现有后训练量化方法依赖于固定位宽量化导致的精度和效率问题，提出Mix-QSAM框架以提高模型的性能。&lt;h4&gt;方法&lt;/h4&gt;Mix-QSAM引入了层间重要性评分和跨层协同度量，用于量化层贡献和捕捉相邻层之间的依赖关系。通过这些度量，构建了整数二次规划问题，以确定模型大小和位操作约束下的最优位宽分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Mix-QSAM在实例分割和目标检测任务上优于现有PTQ方法，在6位和4位混合精度设置下，平均精度提高可达20%，同时保持了计算效率。&lt;h4&gt;结论&lt;/h4&gt;Mix-QSAM通过优化位宽分配，有效提高了SAM在资源受限设备上的部署性能，同时保持了模型的精度和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Segment Anything Model (SAM) is a popular vision foundation model;however, its high computational and memory demands make deployment onresource-constrained devices challenging. While Post-Training Quantization(PTQ) is a practical approach for reducing computational overhead, existing PTQmethods rely on fixed bit-width quantization, leading to suboptimal accuracyand efficiency. To address this limitation, we propose Mix-QSAM, amixed-precision PTQ framework for SAM. First, we introduce a layer-wiseimportance score, derived using Kullback-Leibler (KL) divergence, to quantifyeach layer's contribution to the model's output. Second, we introducecross-layer synergy, a novel metric based on causal mutual information, tocapture dependencies between adjacent layers. This ensures that highlyinterdependent layers maintain similar bit-widths, preventing abrupt precisionmismatches that degrade feature propagation and numerical stability. Usingthese metrics, we formulate an Integer Quadratic Programming (IQP) problem todetermine optimal bit-width allocation under model size and bit-operationconstraints, assigning higher precision to critical layers while minimizingbit-width in less influential layers. Experimental results demonstrate thatMix-QSAM consistently outperforms existing PTQ methods on instance segmentationand object detection tasks, achieving up to 20% higher average precision under6-bit and 4-bit mixed-precision settings, while maintaining computationalefficiency.</description>
      <author>example@mail.com (Navin Ranjan, Andreas Savakis)</author>
      <guid isPermaLink="false">2505.04861v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SeriesBench的基准测试，用于评估多模态大型语言模型在理解叙事驱动视频系列方面的能力。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型的快速发展，现有的基准测试主要关注独立视频和视觉元素，如人类动作和物体状态，而忽略了视频系列中的复杂连续叙事。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了SeriesBench，旨在评估模型对叙事驱动视频系列的理解能力。&lt;h4&gt;方法&lt;/h4&gt;SeriesBench包含105个精心挑选的叙事驱动系列，涵盖28个需要深度叙事理解的专项任务。它采用了新颖的长篇叙事标注方法和全信息转换方法，以及一个名为PC-DCoT的叙事推理框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有的MLLMs在理解叙事驱动系列方面仍面临重大挑战，而PC-DCoT能够帮助这些MLLMs实现性能提升。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT强调了提升模型理解叙事驱动系列能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess 'visual elements' like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zackhxn/seriesbench-cvpr2025&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title>
      <link>http://arxiv.org/abs/2505.04813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://threedle.github.io/wir3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WIR3D的技术，通过在三维空间中提取一组视觉上有意义的曲线来抽象三维形状。&lt;h4&gt;背景&lt;/h4&gt;WIR3D技术旨在通过曲线来表示三维形状的几何和视觉特征。&lt;h4&gt;目的&lt;/h4&gt;目的是为了能够从任意视角忠实地表示形状的几何和显著视觉特征，如纹理。&lt;h4&gt;方法&lt;/h4&gt;方法包括优化贝塞尔曲线的参数，利用预训练的基础模型（CLIP）的中间激活来指导优化过程，将优化分为两个阶段：一个用于捕捉形状的粗略几何形状，另一个用于表示细粒度特征。第二个阶段通过一个新颖的局部关键点损失进行空间引导，确保对原始表面的忠实度通过神经SDF损失。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，该方法能够成功应用于具有不同复杂度、几何结构和纹理的形状的大数据集，并展示了特征控制和形状变形的下游应用。&lt;h4&gt;结论&lt;/h4&gt;结论是WIR3D技术能够有效地抽象三维形状，并在实际应用中表现出良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present WIR3D, a technique for abstracting 3D shapes through a sparse setof visually meaningful curves in 3D. We optimize the parameters of Beziercurves such that they faithfully represent both the geometry and salient visualfeatures (e.g. texture) of the shape from arbitrary viewpoints. We leverage theintermediate activations of a pre-trained foundation model (CLIP) to guide ouroptimization process. We divide our optimization into two phases: one forcapturing the coarse geometry of the shape, and the other for representingfine-grained features. Our second phase supervision is spatially guided by anovel localized keypoint loss. This spatial guidance enables user control overabstracted features. We ensure fidelity to the original surface through aneural SDF loss, which allows the curves to be used as intuitive deformationhandles. We successfully apply our method for shape abstraction over a broaddataset of shapes with varying complexity, geometric structure, and texture,and demonstrate downstream applications for feature control and shapedeformation.</description>
      <author>example@mail.com (Richard Liu, Daniel Fu, Noah Tan, Itai Lang, Rana Hanocka)</author>
      <guid isPermaLink="false">2505.04813v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
      <link>http://arxiv.org/abs/2505.04802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ORBIT-2，一种用于全球超分辨率气候降尺度的可扩展基础模型，旨在解决现有方法在变量和地理范围泛化能力有限以及ViT自注意力二次复杂度约束的问题。&lt;h4&gt;背景&lt;/h4&gt;稀疏观测和粗分辨率气候模型限制了区域决策的有效性，强调了稳健降尺度的必要性。&lt;h4&gt;目的&lt;/h4&gt;提出ORBIT-2模型，以实现高效、鲁棒的预测，并降低自注意力复杂度，支持长序列处理和大规模并行计算。&lt;h4&gt;方法&lt;/h4&gt;ORBIT-2包含两个关键创新：(1) Residual Slim ViT (Reslim)，一种轻量级架构，结合残差学习和贝叶斯正则化；(2) TILES，一种分块序列缩放算法，将自注意力复杂度从二次降低到线性。&lt;h4&gt;主要发现&lt;/h4&gt;ORBIT-2可以扩展到10亿参数，在32,768个GPU上运行，实现高达1.8 ExaFLOPS的持续吞吐量和92-98%的强扩展效率。它可以支持0.9公里全球分辨率的降尺度，并处理高达42亿个标记的序列。在7公里分辨率的基准测试中，ORBIT-2与观测数据相比，实现了0.98到0.99的R^2分数的高精度。&lt;h4&gt;结论&lt;/h4&gt;ORBIT-2模型在提高气候降尺度精度和效率方面具有显著优势，为区域决策提供了强有力的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse observations and coarse-resolution climate models limit effectiveregional decision-making, underscoring the need for robust downscaling.However, existing AI methods struggle with generalization across variables andgeographies and are constrained by the quadratic complexity of VisionTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundationmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporatestwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecturewith residual learning and Bayesian regularization for efficient, robustprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reducesself-attention complexity from quadratic to linear, enabling long-sequenceprocessing and massive parallelism. ORBIT-2 scales to 10 billion parametersacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and92-98% strong scaling efficiency. It supports downscaling to 0.9 km globalresolution and processes sequences up to 4.2 billion tokens. On 7 km resolutionbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98to 0.99 against observation data.</description>
      <author>example@mail.com (Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu)</author>
      <guid isPermaLink="false">2505.04802v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2505.04911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了SpatialPrompting，这是一种新型框架，利用现成多模态大型语言模型的涌现推理能力，在三维（3D）环境中实现零样本空间推理。&lt;h4&gt;背景&lt;/h4&gt;现有方法依赖昂贵的3D特定微调，使用如点云或基于体素的特征等专业的3D输入。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种利用直观视觉和位置线索的灵活空间推理新范式。&lt;h4&gt;方法&lt;/h4&gt;SpatialPrompting采用关键帧驱动的提示生成策略，使用视觉语言相似度、马氏距离、视场和图像锐度等指标，从图像序列中选择多样化的关键帧，并与相应的相机位姿数据集成，以有效地抽象空间关系并推断复杂的3D结构。&lt;h4&gt;主要发现&lt;/h4&gt;该框架不仅建立了一种利用直观视觉和位置线索的灵活空间推理新范式，而且在基准数据集（如ScanQA和SQA3D）上实现了最先进的零样本性能，并在多个指标上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地消除了对专业3D输入和微调的需求，提供了一种比传统方法更简单、更可扩展的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces SpatialPrompting, a novel framework that harnesses theemergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D)environments. Unlike existing methods that rely on expensive 3D-specificfine-tuning with specialized 3D inputs such as point clouds or voxel-basedfeatures, SpatialPrompting employs a keyframe-driven prompt generationstrategy. This framework uses metrics such as vision-language similarity,Mahalanobis distance, field of view, and image sharpness to select a diverseand informative set of keyframes from image sequences and then integrates themwith corresponding camera pose data to effectively abstract spatialrelationships and infer complex 3D structures. The proposed framework not onlyestablishes a new paradigm for flexible spatial reasoning that utilizesintuitive visual and positional cues but also achieves state-of-the-artzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, acrossseveral metrics. The proposed method effectively eliminates the need forspecialized 3D inputs and fine-tuning, offering a simpler and more scalablealternative to conventional approaches.</description>
      <author>example@mail.com (Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, Hiroyuki Sakai)</author>
      <guid isPermaLink="false">2505.04911v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MAISY的新方法，用于消除医学图像采集过程中因患者运动导致的模糊、鬼影和器官扭曲问题，从而提高图像解释的准确性。&lt;h4&gt;背景&lt;/h4&gt;患者运动导致医学图像模糊，影响图像解读。现有的基于生成对抗网络（GAN）的算法虽然能生成无运动图像，但存在忽视局部特征和难以处理像素强度、亮度因素及方差变化的问题。&lt;h4&gt;目的&lt;/h4&gt;提出MAISY方法，旨在更好地处理运动模糊，并保留关键病理信息。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法通过以下步骤实现：（a）利用Segment Anything Model（SAM）动态学习运动伪影最明显的解剖边界处的空间模式；（b）引入变差选择性的结构相似性指数（VS-SSIM）损失，自适应地强调高像素变差的区域，以保留必要的解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;在胸部和头部CT数据集上的实验表明，MAISY模型在峰值信噪比（PSNR）、结构相似性（SSIM）和Dice系数上均优于现有算法，PSNR提高40%，SSIM提高10%，Dice系数提高16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在处理运动模糊和提高医学图像质量方面表现出色，为医学图像分析提供了有效工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems</title>
      <link>http://arxiv.org/abs/2505.03140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Hamiltonian-Masked Autoencoding (HMAE)框架，用于解决量子机器学习中标签数据稀缺和模拟计算昂贵的问题。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习在处理自旋和分子系统时面临数据稀缺和计算成本高的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出HMAE框架，通过在未标记的量子哈密顿量上预训练transformer，实现高效的少样本迁移学习。&lt;h4&gt;方法&lt;/h4&gt;HMAE采用基于量子信息理论的物理信息策略，根据哈密顿项的物理意义进行选择性掩码。&lt;h4&gt;主要发现&lt;/h4&gt;在12,500个量子哈密顿量上的实验表明，HMAE在相分类中达到85.3% ± 1.5%的准确率，在基态能量预测中达到0.15 ± 0.02 eV的MAE，仅使用10个标记示例。&lt;h4&gt;结论&lt;/h4&gt;HMAE具有出色的样本效率，比基线方法减少了3-5倍的标记示例需求，但该方法目前限于小型量子系统，不能直接应用于材料科学和量子化学中感兴趣的大系统。&lt;h4&gt;翻译&lt;/h4&gt;Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike randommasking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% ± 1.5%accuracy in phase classification and 0.15 ± 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% ± 2.1%)and quantum neural networks (76.8% ± 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlikerandom masking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.</description>
      <author>example@mail.com (Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma)</author>
      <guid isPermaLink="false">2505.03140v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
  <item>
      <title>Geospatial Mechanistic Interpretability of Large Language Models</title>
      <link>http://arxiv.org/abs/2505.03368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究大型语言模型（LLMs）在地理信息处理方面的能力和内部工作机制。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，并在地理领域被用于地理知识库和推理工具，但其内部处理地理信息的方式尚不明确。&lt;h4&gt;目的&lt;/h4&gt;建立一个新的框架来研究地理空间机制的可解释性，通过空间分析来揭示LLMs处理地理信息的方式。&lt;h4&gt;方法&lt;/h4&gt;使用探针技术揭示LLMs内部的内部结构，引入机制可解释性领域，讨论叠加假设和稀疏自动编码器在分解LLMs的多义内部表示为可解释的单义特征中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;实验中，通过空间自相关展示了地名特征如何显示与地理位置相关的空间模式，从而可以地理空间地解释，为LLMs如何处理地理信息提供了洞察。&lt;h4&gt;结论&lt;/h4&gt;本文提出的框架有助于地理学中基础模型的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各种自然语言处理任务中展现出了前所未有的能力。它们处理和生成有效文本和代码的能力使它们在许多领域变得无处不在，而它们作为知识库和“推理”工具的应用仍是一个持续研究的领域。在地理学中，越来越多的文献开始关注评估LLMs的地理知识和它们执行空间推理的能力。然而，关于这些模型内部工作方式的了解仍然非常有限，特别是关于它们如何处理地理信息。在本章中，我们建立了一个研究地理空间机制可解释性的新框架——使用空间分析来逆向工程LLMs如何处理地理信息。我们的目标是深化我们对这些复杂模型在处理地理信息时生成的内部表示的理解——如果这样的表述不是过度拟人化的话，我们可以称之为“LLMs如何思考地理信息”。我们首先概述了使用探针揭示LLMs内部结构的方法。然后，我们介绍了机制可解释性的领域，讨论了叠加假设和稀疏自动编码器在将LLMs的多义内部表示分解为更可解释的单义特征中的作用。在我们的实验中，我们使用空间自相关来展示地名特征如何显示出与地理位置相关的空间模式，从而可以地理空间地解释，为这些模型如何处理地理信息提供了洞察。我们最后讨论了我们的框架如何有助于塑造地理学中基础模型的研究和应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sdesabbata/geospatial-mechanistic-interpretability&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated unprecedented capabilitiesacross various natural language processing tasks. Their ability to process andgenerate viable text and code has made them ubiquitous in many fields, whiletheir deployment as knowledge bases and "reasoning" tools remains an area ofongoing research. In geography, a growing body of literature has been focusingon evaluating LLMs' geographical knowledge and their ability to perform spatialreasoning. However, very little is still known about the internal functioningof these models, especially about how they process geographical information.  In this chapter, we establish a novel framework for the study of geospatialmechanistic interpretability - using spatial analysis to reverse engineer howLLMs handle geographical information. Our aim is to advance our understandingof the internal representations that these complex models generate whileprocessing geographical information - what one might call "how LLMs think aboutgeographic information" if such phrasing was not an undue anthropomorphism.  We first outline the use of probing in revealing internal structures withinLLMs. We then introduce the field of mechanistic interpretability, discussingthe superposition hypothesis and the role of sparse autoencoders indisentangling polysemantic internal representations of LLMs into moreinterpretable, monosemantic features. In our experiments, we use spatialautocorrelation to show how features obtained for placenames display spatialpatterns related to their geographic location and can thus be interpretedgeospatially, providing insights into how these models process geographicalinformation. We conclude by discussing how our framework can help shape thestudy and use of foundation models in geography.</description>
      <author>example@mail.com (Stef De Sabbata, Stefano Mizzaro, Kevin Roitero)</author>
      <guid isPermaLink="false">2505.03368v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Registration of 3D Point Sets Using Exponential-based Similarity Matrix</title>
      <link>http://arxiv.org/abs/2505.04540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。&lt;h4&gt;背景&lt;/h4&gt;点云注册是计算机视觉和机器人领域的一个基本问题，涉及使用如激光雷达或结构光等深度传感器从不同视点捕获的3D点集的对齐。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有注册技术在大旋转差异或数据腐蚀情况下性能不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。&lt;h4&gt;主要发现&lt;/h4&gt;ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。&lt;h4&gt;结论&lt;/h4&gt;ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。本文提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/aralab-unr/esm_icp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental problem in computer vision androbotics, involving the alignment of 3D point sets captured from varyingviewpoints using depth sensors such as LiDAR or structured light. In modernrobotic systems, especially those focused on mapping, it is essential to mergemultiple views of the same environment accurately. However, state-of-the-artregistration techniques often struggle when large rotational differences existbetween point sets or when the data is significantly corrupted by sensor noise.These challenges can lead to misalignments and, consequently, to inaccurate ordistorted 3D reconstructions. In this work, we address both these limitationsby proposing a robust modification to the classic Iterative Closest Point (ICP)algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),integrates a Gaussian-inspired exponential weighting scheme to construct asimilarity matrix that dynamically adapts across iterations. This matrixfacilitates improved estimation of both rotational and translational componentsduring alignment. We demonstrate the robustness of ESM-ICP in two challengingscenarios: (i) large rotational discrepancies between the source and targetpoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results showthat ESM-ICP outperforms traditional geometric registration techniques as wellas several recent learning-based methods. To encourage reproducibility andcommunity engagement, our full implementation is made publicly available onGitHub. https://github.com/aralab-unr/ESM_ICP</description>
      <author>example@mail.com (Ashutosh Singandhupe, Sanket Lokhande, Hung Manh La)</author>
      <guid isPermaLink="false">2505.04540v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种基于预测链（CoP）的3D属性预测方法，旨在解决单目3D目标检测中的深度估计问题，通过条件预测提高准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计是单目3D目标检测中最具挑战性的问题，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP以通过条件预测提高3D属性预测的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用三个关键设计：1）使用轻量级的属性网络（AN）学习每个3D属性的特征；2）构建显式链来传播这些特征；3）使用残差连接聚合特征，确保后续属性预测基于先前处理的所有属性。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI排行榜上实现了最先进的性能，且无需额外数据，在Waymo和nuScenes前向数据集上超越了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测链提高了单目3D目标检测的深度估计准确性，为该领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>On Path to Multimodal Generalist: General-Level and General-Bench</title>
      <link>http://arxiv.org/abs/2505.04620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML'25, 305 pages, 115 tables, 177 figures, project page:  https://generalist.top/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态大型语言模型（MLLM）的快速发展及其在多模态理解与生成方面的能力提升。&lt;h4&gt;背景&lt;/h4&gt;MLLM的发展推动了多模态通用主义范式的出现，从最初的多模态理解发展到跨模态生成，能力从粗粒度扩展到细粒度，支持的模态从有限扩展到任意。&lt;h4&gt;目的&lt;/h4&gt;评估MLLM的性能和泛化能力，推动向更强大的多模态通用主义和通用人工智能（AGI）的发展。&lt;h4&gt;方法&lt;/h4&gt;引入了“通用级别”评估框架，定义了MLLM性能和泛化能力的5个级别，以及“通用基准”（General-Bench），包含700多个任务和325,800个实例。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，尽管存在许多基准，但高任务性能并不一定意味着更强的MLLM能力，且达到真正的人工智能仍面临挑战。&lt;h4&gt;结论&lt;/h4&gt;该项目为下一代多模态基础模型的研究铺平了道路，为加速实现AGI提供了坚实的框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态大型语言模型（MLLM）目前正在经历快速增长，这得益于LLM的先进能力。与早期的专家不同，现有的MLLM正在向多模态通用主义范式进化。最初仅限于理解多个模态，这些模型已经发展到不仅理解而且能够跨模态生成。它们的能力已从粗粒度扩展到细粒度的多模态理解，从支持有限的模态扩展到任意的模态。尽管存在许多基准来评估MLLM，但一个关键问题出现了：我们能否简单地假设在任务上的更高性能意味着更强的MLLM能力，从而让我们更接近人类水平的人工智能？我们认为答案并不像它看起来那么简单。本项目引入了通用级别，这是一个定义MLLM性能和泛化能力的5个级别的评估框架，提供了一种比较MLLM和衡量现有系统向更强大的多模态通用主义以及最终向AGI发展的方法。该框架的核心是协同的概念，它衡量模型是否在理解和生成之间以及在不同模态之间保持一致的能力。为了支持这种评估，我们提出了通用基准，它包含更广泛的技能、模态、格式和能力，包括700多个任务和325,800个实例。涉及100多个现有最先进MLLM的评估结果揭示了通用主义者的能力排名，突显了达到真正人工智能的挑战。我们期望这个项目为下一代多模态基础模型的研究铺平道路，为加速实现AGI提供坚实的基础。项目页面：https://generalist.top/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Multimodal Large Language Model (MLLM) is currently experiencing rapidgrowth, driven by the advanced capabilities of LLMs. Unlike earlierspecialists, existing MLLMs are evolving towards a Multimodal Generalistparadigm. Initially limited to understanding multiple modalities, these modelshave advanced to not only comprehend but also generate across modalities. Theircapabilities have expanded from coarse-grained to fine-grained multimodalunderstanding and from supporting limited modalities to arbitrary ones. Whilemany benchmarks exist to assess MLLMs, a critical question arises: Can wesimply assume that higher performance across tasks indicates a stronger MLLMcapability, bringing us closer to human-level AI? We argue that the answer isnot as straightforward as it seems. This project introduces General-Level, anevaluation framework that defines 5-scale levels of MLLM performance andgenerality, offering a methodology to compare MLLMs and gauge the progress ofexisting systems towards more robust multimodal generalists and, ultimately,towards AGI. At the core of the framework is the concept of Synergy, whichmeasures whether models maintain consistent capabilities across comprehensionand generation, and across multiple modalities. To support this evaluation, wepresent General-Bench, which encompasses a broader spectrum of skills,modalities, formats, and capabilities, including over 700 tasks and 325,800instances. The evaluation results that involve over 100 existingstate-of-the-art MLLMs uncover the capability rankings of generalists,highlighting the challenges in reaching genuine AI. We expect this project topave the way for future research on next-generation multimodal foundationmodels, providing a robust infrastructure to accelerate the realization of AGI.Project page: https://generalist.top/</description>
      <author>example@mail.com (Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang)</author>
      <guid isPermaLink="false">2505.04620v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution</title>
      <link>http://arxiv.org/abs/2505.04384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on  11-Jul-2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于解耦和对比学习的多类别人脸操纵技术分类框架，用于提升开放世界半监督深度伪造归属（OSS-DFA）任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度伪造归属（DFA）旨在对不同的人脸操纵技术进行多分类，以减轻伪造内容对社会秩序和个人声誉的负面影响。然而，先前的方法仅关注特定方法的线索，容易导致过拟合，并忽略了通用伪造特征的重要性。此外，它们在更实际的开世界场景中难以区分不确定的新类别。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的基于多解耦的对比学习框架，以增强在开放世界半监督深度伪造归属任务中对新类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DATA框架首次定义了'正交归一深度伪造基'的概念，并利用它来解耦特定方法特征，减少对伪造无关信息的过拟合。此外，设计了一种增强记忆机制以帮助新类别发现和对比学习，并通过实例级别的解耦来获得新类别的清晰边界。DATA还使用基础对比损失和中心对比损失作为辅助模块，以增强特征的标准化和区分度。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验评估表明，DATA在OSS-DFA基准上取得了最先进的性能，与现有方法相比，在不同设置下准确率提高了2.55% / 5.7%。&lt;h4&gt;结论&lt;/h4&gt;DATA框架在OSS-DFA任务中表现优异，能够有效提升对新类别的泛化能力，并具有更好的标准化和区分度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake attribution (DFA) aims to perform multiclassification on differentfacial manipulation techniques, thereby mitigating the detrimental effects offorgery content on the social order and personal reputations. However, previousmethods focus only on method-specific clues, which easily lead to overfitting,while overlooking the crucial role of common forgery features. Additionally,they struggle to distinguish between uncertain novel classes in more practicalopen-world scenarios. To address these issues, in this paper we propose aninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, toenhance the generalization ability on novel classes for the open-worldsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since allgeneration techniques can be abstracted into a similar architecture, DATAdefines the concept of 'Orthonormal Deepfake Basis' for the first time andutilizes it to disentangle method-specific features, thereby reducing theoverfitting on forgery-irrelevant information. Furthermore, an augmented-memorymechanism is designed to assist in novel class discovery and contrastivelearning, which aims to obtain clear class boundaries for the novel classesthrough instance-level disentanglements. Additionally, to enhance thestandardization and discrimination of features, DATA uses bases contrastiveloss and center contrastive loss as auxiliaries for the aforementioned modules.Extensive experimental evaluations show that DATA achieves state-of-the-artperformance on the OSS-DFA benchmark, e.g., there are notable accuracyimprovements in 2.55% / 5.7% under different settings, compared with theexisting methods.</description>
      <author>example@mail.com (Ming-Hui Liu, Xiao-Qian Liu, Xin Luo, Xin-Shun Xu)</author>
      <guid isPermaLink="false">2505.04384v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granular Attention based Heterogeneous Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MGA-HHN的多粒度注意力异构超图神经网络，用于异构图表示学习，并通过实验证明了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的异构图神经网络（HeteGNNs）通过元路径基于的消息传递学习潜在节点表示，但存在无法捕捉高阶关系和信息扭曲等问题。&lt;h4&gt;目的&lt;/h4&gt;提出MGA-HHN以解决HeteGNNs中的上述局限性。&lt;h4&gt;方法&lt;/h4&gt;MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作，以捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。&lt;h4&gt;主要发现&lt;/h4&gt;MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。&lt;h4&gt;结论&lt;/h4&gt;MGA-HHN在真实世界基准数据集上的实验表明，它优于现有的模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：异构图神经网络（HeteGNNs）通过有效地提取异构图中的复杂结构和语义信息，在学习节点表示方面显示出强大的能力。大多数现存的HeteGNNs遵循邻域聚合范式，利用基于元路径的消息传递来学习潜在节点表示。然而，由于元路径的成对性质，这些模型无法捕捉节点间的高阶关系，导致性能次优。此外，由于HeteGNNs中的长程消息传递导致的“过度压缩”，进一步限制了这些模型的有效性。为了解决这些局限性，本文提出了一种基于多粒度注意力的异构超图神经网络MGA-HHN，用于异构图表示学习。MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作。这种机制使得模型能够捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。因此，MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。在真实世界基准数据集上的大量实验表明，MGA-HHN优于现有模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graph neural networks (HeteGNNs) have demonstrated strongabilities to learn node representations by effectively extracting complexstructural and semantic information in heterogeneous graphs. Most of theprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveragingmeta-path based message passing to learn latent node representations. However,due to the pairwise nature of meta-paths, these models fail to capturehigh-order relations among nodes, resulting in suboptimal performance.Additionally, the challenge of ``over-squashing'', where long-range messagepassing in HeteGNNs leads to severe information distortion, further limits theefficacy of these models. To address these limitations, this paper proposesMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph NeuralNetwork for heterogeneous graph representation learning. MGA-HHN introduces twokey innovations: (1) a novel approach for constructing meta-path basedheterogeneous hypergraphs that explicitly models higher-order semanticinformation in heterogeneous graphs through multiple views, and (2) amulti-granular attention mechanism that operates at both the node and hyperedgelevels. This mechanism enables the model to capture fine-grained interactionsamong nodes sharing the same semantic context within a hyperedge type, whilepreserving the diversity of semantics across different hyperedge types. Assuch, MGA-HHN effectively mitigates long-range message distortion and generatesmore expressive node representations. Extensive experiments on real-worldbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-artmodels, showcasing its effectiveness in node classification, node clusteringand visualization tasks.</description>
      <author>example@mail.com (Hong Jin, Kaicheng Zhou, Jie Yin, Lan You, Zhifeng Zhou)</author>
      <guid isPermaLink="false">2505.04340v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
      <link>http://arxiv.org/abs/2505.04529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为RAFT的新框架，用于通过数据增强、特征增强和主动学习，利用最少量的标注数据对图像分割模型进行适应，以解决合成数据训练的深度神经网络在实际应用中性能不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;图像分割是场景理解的有力计算机视觉技术，但在实际应用中，高质量的标注数据集是必需的，而手动数据收集和标注成本高昂。合成数据虽然提供了高质量的标签，但深度神经网络在合成数据上训练后，往往在真实世界部署时面临Syn2Real问题，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述图像分割的差距，提出RAFT框架，旨在通过数据增强、特征增强和主动学习，使用最少量的标注数据对图像分割模型进行适应。&lt;h4&gt;方法&lt;/h4&gt;RAFT框架通过数据增强、特征增强和主动学习，利用少量标注数据对图像分割模型进行训练和优化。&lt;h4&gt;主要发现&lt;/h4&gt;在合成到真实的“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试中，RAFT框架超过了之前的最先进方法HALO，分别实现了mIoU提升2.1%/79.9%和0.4%/78.2%。在真实到真实的基准测试“Cityscapes-&gt;ACDC”中，RAFT同样超越了HALO，mIoU提升1.3%/73.2%。此外，还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;h4&gt;结论&lt;/h4&gt;RAFT框架有效提高了图像分割模型在真实世界中的应用性能，并通过实验验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图像分割是场景理解的有力计算机视觉技术。然而，由于需要高质量、精心标注的数据集，实际部署受到了阻碍。合成数据提供了高质量的标签，同时减少了手动数据收集和标注的需求。然而，在合成数据上训练的深度神经网络往往面临Syn2Real问题，导致在真实世界部署中性能不佳。为了缓解上述图像分割的差距，我们提出了RAFT，一种新的框架，通过数据增强、特征增强以及主动学习，利用最少量的标注数据对图像分割模型进行适应。为了验证RAFT，我们在合成到真实“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试上进行了实验。我们成功超过了之前的最佳水平HALO。在“SYNTHIA-&gt;Cityscapes”中，经过领域适应后，mIoU提高了2.1%/79.9%，在“GTAV-&gt;Cityscapes”中，mIoU提高了0.4%/78.2%。此外，我们在真实到真实基准测试“Cityscapes-&gt;ACDC”上也测试了我们的方法，再次超过了HALO，在适应后mIoU提高了1.3%/73.2%。最后，我们还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a powerful computer vision technique for sceneunderstanding. However, real-world deployment is stymied by the need forhigh-quality, meticulously labeled datasets. Synthetic data provideshigh-quality labels while reducing the need for manual data collection andannotation. However, deep neural networks trained on synthetic data often facethe Syn2Real problem, leading to poor performance in real-world deployments.  To mitigate the aforementioned gap in image segmentation, we propose RAFT, anovel framework for adapting image segmentation models using minimal labeledreal-world data through data and feature augmentations, as well as activelearning. To validate RAFT, we perform experiments on the synthetic-to-real"SYNTHIA-&gt;Cityscapes" and "GTAV-&gt;Cityscapes" benchmarks. We managed to surpassthe previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences animprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV-&gt;Cityscapesexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approachon the real-to-real benchmark of "Cityscapes-&gt;ACDC", and again surpass HALO,with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine theeffect of the allocated annotation budget and various components of RAFT uponthe final transfer mIoU.</description>
      <author>example@mail.com (Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin)</author>
      <guid isPermaLink="false">2505.04529v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning</title>
      <link>http://arxiv.org/abs/2505.04601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenVision，这是一个完全开放的、成本效益高的视觉编码器家族，其性能与OpenAI的CLIP相当甚至更优，适用于构建多模态基础模型。&lt;h4&gt;背景&lt;/h4&gt;OpenAI的CLIP自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但它们并未完全开放，其训练数据和训练方法未公开。&lt;h4&gt;目的&lt;/h4&gt;本文旨在填补这一空白，提出OpenVision，一个完全开放的视觉编码器，以提升多模态模型的质量。&lt;h4&gt;方法&lt;/h4&gt;OpenVision基于现有的工作，如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。&lt;h4&gt;主要发现&lt;/h4&gt;OpenVision提供了从5.9M到632.1M参数范围的视觉编码器，使得构建多模态模型时可以在容量和效率之间进行灵活的权衡：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;h4&gt;结论&lt;/h4&gt;OpenVision为多模态模型构建提供了新的选择，有助于推动多模态技术的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;OpenAI的CLIP，自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但据我们所知，它们都没有完全开放：它们的训练数据仍然是专有的，或者它们的训练方法并未公开。本文通过提出OpenVision，一个完全开放、成本效益高的视觉编码器家族，来填补这一空白。OpenVision基于现有工作，例如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。通过发布从5.9M到632.1M参数范围的视觉编码器，OpenVision为实践者提供了在构建多模态模型时在容量和效率之间进行灵活权衡的选择：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; OpenAI's CLIP, released in early 2021, have long been the go-to choice ofvision encoder for building multimodal foundation models. Although recentalternatives such as SigLIP have begun to challenge this status quo, to ourknowledge none are fully open: their training data remains proprietary and/ortheir training recipes are not released. This paper fills this gap withOpenVision, a fully-open, cost-effective family of vision encoders that matchor surpass the performance of OpenAI's CLIP when integrated into multimodalframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS fortraining framework and Recap-DataComp-1B for training data -- while revealingmultiple key insights in enhancing encoder quality and showcasing practicalbenefits in advancing multimodal models. By releasing vision encoders spanningfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexibletrade-off between capacity and efficiency in building multimodal models: largermodels deliver enhanced multimodal performance, while smaller versions enablelightweight, edge-ready multimodal deployments.</description>
      <author>example@mail.com (Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, Cihang Xie)</author>
      <guid isPermaLink="false">2505.04601v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</title>
      <link>http://arxiv.org/abs/2505.04596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 Figures, Accepted at AIRC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种优化动态监控环境中PTZ（全向俯仰缩放）相机调度和控制的新方法。&lt;h4&gt;背景&lt;/h4&gt;PTZ相机在动态监控环境中的应用需要高效的视频捕获。&lt;h4&gt;目的&lt;/h4&gt;提高动态监控环境中PTZ相机的调度和控制效率。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型来增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，实现精确的相机任务调度。此外，引入基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。&lt;h4&gt;结论&lt;/h4&gt;该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法来优化动态监控环境中PTZ相机的调度和控制。该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型以增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，从而实现精确的相机任务调度。此外，还引入了一个基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。广泛的模拟表明，与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。总的来说，该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach for optimizing the scheduling andcontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.The proposed method integrates Kalman filters for motion prediction with adynamic network flow model to enhance real-time video capture efficiency. Byassigning Kalman filters to tracked objects, the system predicts futurelocations, enabling precise scheduling of camera tasks. This prediction-drivenapproach is formulated as a network flow optimization, ensuring scalability andadaptability to various surveillance scenarios. To further reduce redundantmonitoring, we also incorporate group-tracking nodes, allowing multiple objectsto be captured within a single camera focus when appropriate. In addition, avalue-based system is introduced to prioritize camera actions, focusing on thetimely capture of critical events. By adjusting the decay rates of these valuesover time, the system ensures prompt responses to tasks with imminentdeadlines. Extensive simulations demonstrate that this approach improvescoverage, reduces average wait times, and minimizes missed events compared totraditional master-slave camera systems. Overall, our method significantlyenhances the efficiency, scalability, and effectiveness of surveillancesystems, particularly in dynamic and crowded environments.</description>
      <author>example@mail.com (Mohammad Merati, David Castañón)</author>
      <guid isPermaLink="false">2505.04596v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces</title>
      <link>http://arxiv.org/abs/2505.04335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Filtration-based Hyperbolic Fuzzy C-Means (HypeFCM)的聚类算法，用于在非欧几里得空间中更好地表示数据关系。&lt;h4&gt;背景&lt;/h4&gt;传统的聚类技术在处理复杂、高维和非欧几里得数据集时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;克服传统聚类技术在非欧几里得空间中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HypeFCM算法结合了模糊聚类原理和双曲几何，使用基于权重的过滤机制来提高性能。算法使用狄利克雷分布初始化权重，并根据Poincaré圆盘模型中的双曲度量迭代地优化聚类中心和成员分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HypeFCM在非欧几里得设置中显著优于传统的模糊聚类方法，证明了其鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;HypeFCM是一种有效的聚类算法，特别适用于在非欧几里得空间中处理复杂数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering algorithms play a pivotal role in unsupervised learning byidentifying and grouping similar objects based on shared characteristics. Whiletraditional clustering techniques, such as hard and fuzzy center-basedclustering, have been widely used, they struggle with complex,high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibitsnotable limitations in non-Euclidean spaces. Euclidean spaces assume linearseparability and uniform distance scaling, limiting their effectiveness incapturing complex, hierarchical, or non-Euclidean structures in fuzzyclustering. To overcome these challenges, we introduce Filtration-basedHyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored forbetter representation of data relationships in non-Euclidean spaces. HypeFCMintegrates the principles of fuzzy clustering with hyperbolic geometry andemploys a weight-based filtering mechanism to improve performance. Thealgorithm initializes weights using a Dirichlet distribution and iterativelyrefines cluster centroids and membership assignments based on a hyperbolicmetric in the Poincar\'e Disc model. Extensive experimental evaluationsdemonstrate that HypeFCM significantly outperforms conventional fuzzyclustering methods in non-Euclidean settings, underscoring its robustness andeffectiveness.</description>
      <author>example@mail.com (Swagato Das, Arghya Pratihar, Swagatam Das)</author>
      <guid isPermaLink="false">2505.04335v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Frame-Averaging Kernel-Point Convolution (FA-KPConv)，这是一种基于知名KPConv架构的神经网络架构，适用于3D点云分析。FA-KPConv通过FrameAveraging技术使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性，同时不增加可学习参数数量，不损失任何输入信息，并在点云分类和点云注册任务中显示出优势。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其基于KPConv的网络在训练大数据集或进行数据增强时，只能近似实现欧几里得变换的不变性和/或等变性。&lt;h4&gt;目的&lt;/h4&gt;设计一个神经网络架构，使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;使用FrameAveraging技术，将几何先验知识嵌入到KPConv网络中，通过简单地封装现有的KPConv网络来实现。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在网络结构上对现有的KPConv进行了改进，使其具有精确的不变性和/或等变性，同时保持了参数数量和输入信息的完整性。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中显示出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为Frame-Averaging Kernel-Point Convolution (FA-KPConv)的神经网络架构，该架构基于著名的KPConv，被广泛用于3D点云分析。尽管许多常见任务需要欧几里得变换的不变性和/或等变性，但基于KPConv的网络在训练大数据集或进行显著数据增强时只能近似实现这些特性。通过使用FrameAveraging，我们允许灵活定制使用KPConv层构建的点云神经网络，使它们对输入点云的平移、旋转和/或反射具有精确的不变性和/或等变性。通过简单地封装现有的基于KPConv的网络，FA-KPConv将几何先验知识嵌入其中，同时保持可学习参数的数量，不牺牲任何输入信息。我们展示了这种引入的偏差对点云分类和点云注册的好处，尤其是在训练数据稀缺或测试数据随机旋转的困难情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality</title>
      <link>http://arxiv.org/abs/2505.03440v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, submitted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。&lt;h4&gt;背景&lt;/h4&gt;生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。&lt;h4&gt;目的&lt;/h4&gt;通过结合VR控制器和眼动追踪硬件，加速深度学习细胞追踪模型的真值生成和校对。&lt;h4&gt;方法&lt;/h4&gt;将深度学习模型的增量标注、训练和校对循环提升到三维空间，并应用手势和眼动追踪等自然用户界面来加速细胞追踪流程。&lt;h4&gt;主要发现&lt;/h4&gt;该系统桥接了基于深度学习的细胞追踪软件和3D/VR可视化之间的差距，提高了细胞追踪的效率和空间理解。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有助于加速细胞追踪工作流程，并提升生命科学家的研究效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。我们利用VR控制器和眼动追踪硬件来促进基于深度学习的细胞追踪模型的快速真值生成和校对。生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。传统上，这种细胞谱系树的重建涉及通过所有记录的时间点追踪单个细胞，手动标注其位置，然后将它们随时间链接起来以创建完整的轨迹。基于深度学习的算法加速了这一过程，但严重依赖于手动标注的高质量真值数据和整理。在此过程中，图像数据的视觉表示仍然主要依赖于2D渲染，这极大地限制了空间理解和导航。在本研究中，我们通过将深度学习模型的增量标注、训练和校对循环提升到第三维度，并应用手势和眼动追踪等自然用户界面，来加速生命科学家的细胞追踪工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose manvr3d, a novel VR-ready platform for interactivehuman-in-the-loop cell tracking. We utilize VR controllers and eye-trackinghardware to facilitate rapid ground truth generation and proofreading for deeplearning-based cell tracking models. Life scientists reconstruct thedevelopmental history of organisms on the cellular level by analyzing 3Dtime-lapse microscopy images acquired at high spatio-temporal resolution. Thereconstruction of such cell lineage trees traditionally involves trackingindividual cells through all recorded time points, manually annotating theirpositions, and then linking them over time to create complete trajectories.Deep learning-based algorithms accelerate this process, yet depend heavily onmanually-annotated high-quality ground truth data and curation. Visualrepresentation of the image data in this process still relies primarily on 2Drenderings, which greatly limits spatial understanding and navigation. In thiswork, we bridge the gap between deep learning-based cell tracking software and3D/VR visualization to create a human-in-the-loop cell tracking system. We liftthe incremental annotation, training and proofreading loop of the deep learningmodel into the 3rd dimension and apply natural user interfaces like handgestures and eye tracking to accelerate the cell tracking workflow for lifescientists.</description>
      <author>example@mail.com (Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther)</author>
      <guid isPermaLink="false">2505.03440v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
      <link>http://arxiv.org/abs/2505.04461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025 Survey Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了时间交互图（TIGs）及其在复杂动态系统行为建模中的应用，并重点介绍了时间交互图表示学习（TIGRL）的发展现状。&lt;h4&gt;背景&lt;/h4&gt;时间交互图（TIGs）由于其能够模拟复杂动态系统行为的能力，在现实世界中得到了广泛应用。&lt;h4&gt;目的&lt;/h4&gt;时间交互图表示学习（TIGRL）的目标是将TIGs中的节点嵌入到低维度的表示中，以有效地保留结构和时间信息，从而提高分类、预测和聚类等下游任务在动态数据环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;本文首先介绍了TIGs的基础概念，强调了时间依赖性的关键作用；接着提出了一个全面的分类法，根据学习过程中利用的信息类型对最先进的TIGRL方法进行系统分类；同时整理了数据集和基准测试的资源，为实证研究提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;本文系统地分类了TIGRL方法，并探讨了TIGRL领域的关键开放挑战和有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;本文为TIGRL领域未来的发展奠定了基础，有望推动该领域的发展进程。&lt;h4&gt;翻译&lt;/h4&gt;Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal interaction graphs (TIGs), defined by sequences of timestampedinteraction events, have become ubiquitous in real-world applications due totheir capability to model complex dynamic system behaviors. As a result,temporal interaction graph representation learning (TIGRL) has garneredsignificant attention in recent years. TIGRL aims to embed nodes in TIGs intolow-dimensional representations that effectively preserve both structural andtemporal information, thereby enhancing the performance of downstream taskssuch as classification, prediction, and clustering within constantly evolvingdata environments. In this paper, we begin by introducing the foundationalconcepts of TIGs and emphasize the critical role of temporal dependencies. Wethen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,systematically categorizing them based on the types of information utilizedduring the learning process to address the unique challenges inherent to TIGs.To facilitate further research and practical applications, we curate the sourceof datasets and benchmarks, providing valuable resources for empiricalinvestigations. Finally, we examine key open challenges and explore promisingresearch directions in TIGRL, laying the groundwork for future advancementsthat have the potential to shape the evolution of this field.</description>
      <author>example@mail.com (Pengfei Jiao, Hongjiang Chen, Xuan Guo, Zhidong Zhao, Dongxiao He, Di Jin)</author>
      <guid isPermaLink="false">2505.04461v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
      <link>http://arxiv.org/abs/2505.04121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，提供了一种更自然的方式来捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。&lt;h4&gt;背景&lt;/h4&gt;现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。&lt;h4&gt;目的&lt;/h4&gt;提出Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。&lt;h4&gt;方法&lt;/h4&gt;VGP的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。&lt;h4&gt;主要发现&lt;/h4&gt;VGP显著提高了ViG在多种下游任务上的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。&lt;h4&gt;结论&lt;/h4&gt;VGP在视觉图结构上的应用为提高ViG的迁移性能提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，以更自然的方式捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。然而，现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。在本文中，我们提出了Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。我们的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，我们引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。广泛的实验表明，我们的方法在多种下游任务上显著提高了ViG的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。我们的代码可在https://github.com/zhoujiahuan1991/ICML2025-VGP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision GNN (ViG) demonstrates superior performance by representing images asgraph structures, providing a more natural way to capture irregular semanticpatterns beyond traditional grid or sequence-based representations. Toefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuningtechniques like visual prompting become increasingly essential. However,existing prompting methods are primarily designed for Transformer-based models,neglecting the rich topological relationships among nodes and edges ingraph-based representations, limiting their capacity to model complexsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novelframework tailored for vision graph structures. Our core insight reveals thatsemantically connected components in the graph exhibit low-rank properties.Building on this observation, we introduce a semantic low-rank prompting methodthat decomposes low-rank semantic features and integrates them with prompts onvision graph topologies, capturing both global structural patterns andfine-grained semantic dependencies. Extensive experiments demonstrate ourmethod significantly improves ViG's transfer performance on diverse downstreamtasks, achieving results comparable to full fine-tuning while maintainingparameter efficiency. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04121v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
      <link>http://arxiv.org/abs/2505.04410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeCLIP是一个新型的视觉语言模型框架，通过解耦CLIP的自注意力模块来增强其在密集预测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;密集视觉预测任务受限于预定义类别，限制了其在实际场景中的应用，而VLMs如CLIP在开放词汇任务中表现良好，但在密集预测中存在局部特征表示的限制。&lt;h4&gt;目的&lt;/h4&gt;提出DeCLIP框架以解决CLIP在图像token中难以有效聚合空间或语义相关区域信息的问题。&lt;h4&gt;方法&lt;/h4&gt;DeCLIP通过解耦自注意力模块获得“内容”和“上下文”特征，其中“内容”特征与图像裁剪表示对齐以增强局部可判别性，“上下文”特征在视觉基础模型如DINO的指导下学习保留空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;DeCLIP在多个开放词汇密集预测任务中（如目标检测和语义分割）显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;DeCLIP通过改进局部特征表示和空间一致性，提高了VLMs在密集预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;This abstract discusses a novel framework called DeCLIP, which enhances the performance of Vision-Language Models (VLMs) like CLIP in dense prediction tasks. The framework addresses limitations in local feature representation by decoupling the self-attention module to obtain 'content' and 'context' features, which are aligned with image crop representations and guided by vision foundation models to improve local discriminability and spatial consistency. Extensive experiments demonstrate its superiority over existing methods in tasks like object detection and semantic segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense visual prediction tasks have been constrained by their reliance onpredefined categories, limiting their applicability in real-world scenarioswhere visual concepts are unbounded. While Vision-Language Models (VLMs) likeCLIP have shown promise in open-vocabulary tasks, their direct application todense prediction often leads to suboptimal performance due to limitations inlocal feature representation. In this work, we present our observation thatCLIP's image tokens struggle to effectively aggregate information fromspatially or semantically related regions, resulting in features that lacklocal discriminability and spatial consistency. To address this issue, wepropose DeCLIP, a novel framework that enhances CLIP by decoupling theself-attention module to obtain ``content'' and ``context'' featuresrespectively. The ``content'' features are aligned with image croprepresentations to improve local discriminability, while ``context'' featureslearn to retain the spatial correlations under the guidance of visionfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIPsignificantly outperforms existing methods across multiple open-vocabularydense prediction tasks, including object detection and semantic segmentation.Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.</description>
      <author>example@mail.com (Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian)</author>
      <guid isPermaLink="false">2505.04410v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal cascade feature transfer for polymer property prediction</title>
      <link>http://arxiv.org/abs/2505.03704v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多模态级联模型特征迁移的聚合物性质预测的新颖迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物数据以多种格式存在，包括分子描述符、添加剂信息以及化学结构。&lt;h4&gt;目的&lt;/h4&gt;提高聚合物物理性质的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型通过结合由图卷积神经网络（GCN）从化学结构中提取的特征，以及分子描述符和添加剂信息等特征，实现更准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;使用多个聚合物数据集进行实证评估表明，与使用单一特征的传统方法相比，该方法表现出更高的预测性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在聚合物性质预测方面具有较高的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction. Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks (GCN) with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel transfer learning approach calledmulti-modal cascade model with feature transfer for polymer propertyprediction.Polymers are characterized by a composite of data in severaldifferent formats, including molecular descriptors and additive information aswell as chemical structures. However, in conventional approaches, predictionmodels were often constructed using each type of data separately. Our modelenables more accurate prediction of physical properties for polymers bycombining features extracted from the chemical structure by graph convolutionalneural networks (GCN) with features such as molecular descriptors and additiveinformation. The predictive performance of the proposed method is empiricallyevaluated using several polymer datasets. We report that the proposed methodshows high predictive performance compared to the baseline conventionalapproach using a single feature.</description>
      <author>example@mail.com (Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui)</author>
      <guid isPermaLink="false">2505.03704v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization</title>
      <link>http://arxiv.org/abs/2505.04412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 11 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自编码器的方法，旨在从高维数据中发现和表示低维结构，同时保留关键拓扑和几何属性。该方法在点云数据集上的实验表明，其在发现噪声数据中的流形结构并通过降维保持这些结构方面优于t-SNE、UMAP和拓扑自编码器等基线方法。&lt;h4&gt;背景&lt;/h4&gt;流形学习旨在发现和表示高维数据中的低维结构，但现有方法往往无法同时捕捉局部细节和全局拓扑完整性，或者无法构建平衡的降维。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的流形学习方法，以从噪声数据中发现流形结构，并通过降维保持这些结构。&lt;h4&gt;方法&lt;/h4&gt;该方法集成了一个流形重建层，该层从噪声点云中揭示潜在流形结构，并在降维过程中提供拓扑和几何属性的规范化。这两个组件在训练过程中相互促进。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在发现噪声数据中的流形结构并保持这些结构方面优于基线方法，这一结论通过可视化和定量指标得到了验证。&lt;h4&gt;结论&lt;/h4&gt;结合流形重建和流形学习对于实现可靠地表示潜在流形具有重要意义，尤其是在处理噪声数据时。&lt;h4&gt;翻译&lt;/h4&gt;Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/thanatorika/mrtg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manifold learning aims to discover and represent low-dimensional structuresunderlying high-dimensional data while preserving critical topological andgeometric properties. Existing methods often fail to capture local details withglobal topological integrity from noisy data or construct a balanceddimensionality reduction, resulting in distorted or fractured embeddings. Wepresent an AutoEncoder-based method that integrates a manifold reconstructionlayer, which uncovers latent manifold structures from noisy point clouds, andfurther provides regularizations on topological and geometric properties duringdimensionality reduction, whereas the two components promote each other duringtraining. Experiments on point cloud datasets demonstrate that our methodoutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders indiscovering manifold structures from noisy data and preserving them throughdimensionality reduction, as validated by visualization and quantitativemetrics. This work demonstrates the significance of combining manifoldreconstruction with manifold learning to achieve reliable representation of thelatent manifold, particularly when dealing with noisy real-world data. Coderepository: https://github.com/Thanatorika/mrtg.</description>
      <author>example@mail.com (Ren Wang, Pengcheng Zhou)</author>
      <guid isPermaLink="false">2505.04412v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MFSeg: Efficient Multi-frame 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.04408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种高效的多帧3D语义分割框架MFSeg。&lt;h4&gt;背景&lt;/h4&gt;当前多帧3D语义分割方法在保持高精度的同时，计算开销较大。&lt;h4&gt;目的&lt;/h4&gt;减少计算开销，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;通过在特征级别聚合点云序列，并正则化特征提取和聚合过程。使用轻量级的基于MLP的点解码器，消除了从过去帧中上采样冗余点的需求。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo数据集上的实验表明，MFSeg优于现有方法，证明了其有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;MFSeg是一个有效且高效的3D语义分割框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose MFSeg, an efficient multi-frame 3D semantic segmentationframework. By aggregating point cloud sequences at the feature level andregularizing the feature extraction and aggregation process, MFSeg reducescomputational overhead while maintaining high accuracy. Moreover, by employinga lightweight MLP-based point decoder, our method eliminates the need toupsample redundant points from past frames. Experiments on the nuScenes andWaymo datasets show that MFSeg outperforms existing methods, demonstrating itseffectiveness and efficiency.</description>
      <author>example@mail.com (Chengjie Huang, Krzysztof Czarnecki)</author>
      <guid isPermaLink="false">2505.04408v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training</title>
      <link>http://arxiv.org/abs/2505.04083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Plexus的三维并行方法，用于全图训练，以解决大规模图数据在GPU内存容量限制、采样和数据传输速度慢、分布式全图训练通信开销大和负载不均衡等问题。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界图数据规模庞大，很多图数据超过了GPU的内存容量，使用图神经网络（GNN）处理这些数据需要采用如小批量采样等技术进行扩展。&lt;h4&gt;目的&lt;/h4&gt;旨在解决大规模图数据在训练过程中的内存限制、计算效率低下和分布式训练中的通信开销问题。&lt;h4&gt;方法&lt;/h4&gt;Plexus是一种三维并行方法，包括负载平衡的排列方案和性能模型以预测最优的三维配置。&lt;h4&gt;主要发现&lt;/h4&gt;Plexus在多个图数据集上进行了评估，并在Perlmutter上扩展到2048个GPU，在Frontier上扩展到2048个GCD。Plexus相比现有方法实现了2.3x-12.5x的速度提升，并在Perlmutter上减少了5.2-8.7倍的时间到解，在Frontier上减少了7-54.2倍的时间到解。&lt;h4&gt;结论&lt;/h4&gt;Plexus实现了前所未有的加速效果，显著提高了大规模图数据的处理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a potent class of neural networkscapable of leveraging the connectivity and structure of real-world graphs tolearn intricate properties and relationships between nodes. Many real-worldgraphs exceed the memory capacity of a GPU due to their sheer size, and usingGNNs on them requires techniques such as mini-batch sampling to scale. However,this can lead to reduced accuracy in some cases, and sampling and data transferfrom the CPU to the GPU can also slow down training. On the other hand,distributed full-graph training suffers from high communication overhead andload imbalance due to the irregular structure of graphs. We propose Plexus, athree-dimensional (3D) parallel approach for full-graph training that tacklesthese issues and scales to billion-edge graphs. Additionally, we introduceoptimizations such as a permutation scheme for load balancing, and aperformance model to predict the optimal 3D configuration. We evaluate Plexuson several graph datasets and show scaling results for up to 2048 GPUs onPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexusachieves unprecedented speedups of 2.3x-12.5x over existing methods and areduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x onFrontier.</description>
      <author>example@mail.com (Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele)</author>
      <guid isPermaLink="false">2505.04083v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</title>
      <link>http://arxiv.org/abs/2505.04354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从专家依赖的优化问题解决流程转变为进化代理工作流程的观点。&lt;h4&gt;背景&lt;/h4&gt;传统的优化实践依赖于人类专家进行问题制定、算法选择和超参数调整，这导致了工业界对先进方法的采纳障碍。&lt;h4&gt;目的&lt;/h4&gt;本文主张通过基于基础模型和进化搜索的进化代理工作流程，可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。&lt;h4&gt;方法&lt;/h4&gt;通过云计算资源调度和ADMM参数适应的案例研究，展示了这一方法如何连接学术界创新与工业实施之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出了优化问题解决流程转变的新思路，并提供了实际案例支持其观点。&lt;h4&gt;翻译&lt;/h4&gt;本文主张优化问题解决流程从专家依赖向进化代理工作流程转变。传统的优化实践依赖人类专家，导致工业界采纳先进方法的障碍。本文认为，基于基础模型和进化搜索的进化代理工作流程可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。通过云计算资源调度和ADMM参数适应的案例研究，本文展示了这一方法如何连接学术界创新与工业实施之间的差距。本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This position paper argues that optimization problem solving can transitionfrom expert-dependent to evolutionary agentic workflows. Traditionaloptimization practices rely on human specialists for problem formulation,algorithm selection, and hyperparameter tuning, creating bottlenecks thatimpede industrial adoption of cutting-edge methods. We contend that anevolutionary agentic workflow, powered by foundation models and evolutionarysearch, can autonomously navigate the optimization space, comprising problem,formulation, algorithm, and hyperparameter spaces. Through case studies incloud resource scheduling and ADMM parameter adaptation, we demonstrate howthis approach can bridge the gap between academic innovation and industrialimplementation. Our position challenges the status quo of human-centricoptimization workflows and advocates for a more scalable, adaptive approach tosolving real-world optimization problems.</description>
      <author>example@mail.com (Wenhao Li, Bo Jin, Mingyi Hong, Changhong Lu, Xiangfeng Wang)</author>
      <guid isPermaLink="false">2505.04354v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了医学图像采集过程中患者运动导致的图像模糊、鬼影和器官变形问题，提出了一种新的运动感知图像合成方法MAISY，有效提升了图像质量。&lt;h4&gt;背景&lt;/h4&gt;医学图像采集过程中患者运动会导致图像模糊、鬼影和器官变形，影响图像解读。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来校正医学图像中的运动伪影，提升图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先通过Segment Anything Model (SAM)动态学习解剖边界处的空间模式，然后利用引入的Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MAISY模型在胸部和头部CT数据集上优于现有方法，PSNR提高了40%，SSIM提高了10%，Dice提高了16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法能够有效校正医学图像中的运动伪影，提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;During medical image acquisition, patient motion causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. The current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods, with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss, effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging.Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning: a Lecture Note</title>
      <link>http://arxiv.org/abs/2505.03861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本讲义旨在为数据科学或相关学科的低年级硕士生和博士生提供机器学习的基础理念。&lt;h4&gt;背景&lt;/h4&gt;讲义从现代机器学习的基本理念开始，以分类作为主要目标任务。&lt;h4&gt;目的&lt;/h4&gt;帮助学生掌握损失函数、反向传播、随机梯度下降、泛化、模型选择以及人工神经网络的基本模块。&lt;h4&gt;方法&lt;/h4&gt;基于这些基本理念，讲义深入探讨了无监督学习的概率方法，包括有向潜在变量模型、专家乘积、生成对抗网络和自回归模型。&lt;h4&gt;主要发现&lt;/h4&gt;讲义还涵盖了强化学习、集成方法和元学习等多种进一步的话题。&lt;h4&gt;结论&lt;/h4&gt;阅读完本讲义后，学生应准备好开始研究机器学习以及更广泛的人工智能领域的更高级主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This lecture note is intended to prepare early-year master's and PhD studentsin data science or a related discipline with foundational ideas in machinelearning. It starts with basic ideas in modern machine learning withclassification as a main target task. These basic ideas include lossformulation, backpropagation, stochastic gradient descent, generalization,model selection as well as fundamental blocks of artificial neural networks.Based on these basic ideas, the lecture note explores in depth the probablisticapproach to unsupervised learning, covering directed latent variable models,product of experts, generative adversarial networks and autoregressive models.Finally, the note ends by covering a diverse set of further topics, such asreinforcement learning, ensemble methods and meta-learning. After reading thislecture note, a student should be ready to embark on studying and researchingmore advanced topics in machine learning and more broadly artificialintelligence.</description>
      <author>example@mail.com (Kyunghyun Cho)</author>
      <guid isPermaLink="false">2505.03861v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
      <link>http://arxiv.org/abs/2505.02366v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无监督对比学习方法，用于自然语言处理中的文本语义嵌入，该方法通过联合张量表示模量约束和交叉注意力机制，提高了对比学习的效果。&lt;h4&gt;背景&lt;/h4&gt;无监督对比学习在自然语言处理中成为热门研究主题，现有工作通常关注正负样本在高维语义空间中的方向分布，但忽略了模量特征，导致对比学习效果不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练目标，旨在对语义表示张量施加模量约束，以增强对比学习中正样本的对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 提出一种新的训练目标，用于对语义表示张量施加模量约束。2. 提出了一种交叉注意力结构，增强模型对CLS token的关注，优化CLS Pooling的质量。3. 结合上述两种动机，提出了一种新的联合张量表示模量约束和交叉注意力无监督对比学习文本嵌入框架JTCSE。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，JTCSE的孪塔集成模型和单塔蒸馏模型优于其他基线，成为当前SOTA。此外，JTCSE在超过130个下游零样本任务上整体优于其他基线。&lt;h4&gt;结论&lt;/h4&gt;JTCSE框架通过联合张量表示模量约束和交叉注意力机制，有效提高了无监督对比学习在自然语言处理中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new Joint Tensor representation modulus constraint and Cross-attention unsupervised contrastive learning Sentence Embedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tianyuzong/jtcse&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised contrastive learning has become a hot research topic in naturallanguage processing. Existing works usually aim at constraining the orientationdistribution of the representations of positive and negative samples in thehigh-dimensional semantic space in contrastive learning, but the semanticrepresentation tensor possesses both modulus and orientation features, and theexisting works ignore the modulus feature of the representations and causeinsufficient contrastive learning. % Therefore, we firstly propose a trainingobjective that aims at modulus constraints on the semantic representationtensor, to strengthen the alignment between the positive samples in contrastivelearning. Therefore, we first propose a training objective that is designed toimpose modulus constraints on the semantic representation tensor, to strengthenthe alignment between positive samples in contrastive learning. Then, theBERT-like model suffers from the phenomenon of sinking attention, leading to alack of attention to CLS tokens that aggregate semantic information. Inresponse, we propose a cross-attention structure among the twin-tower ensemblemodels to enhance the model's attention to CLS token and optimize the qualityof CLS Pooling. Combining the above two motivations, we propose a new\textbf{J}oint \textbf{T}ensor representation modulus constraint and\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence\textbf{E}mbedding representation framework JTCSE, which we evaluate in sevensemantic text similarity computation tasks, and the experimental results showthat JTCSE's twin-tower ensemble model and single-tower distillation modeloutperform the other baselines and become the current SOTA. In addition, wehave conducted an extensive zero-shot downstream task evaluation, which showsthat JTCSE outperforms other baselines overall on more than 130 tasks.</description>
      <author>example@mail.com (Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu)</author>
      <guid isPermaLink="false">2505.02366v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
      <link>http://arxiv.org/abs/2505.04119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GAPrompt的新方法，用于提高3D视觉模型在点云数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的3D视觉模型在点云数据上表现出色，但完全微调这些模型对于下游任务来说计算成本高且存储密集。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提高3D视觉模型的可适应性和性能，同时减少计算和存储成本。&lt;h4&gt;方法&lt;/h4&gt;GAPrompt利用几何线索来增强3D视觉模型的适应性，包括引入点提示和点平移提示器，以及Prompt传播机制。&lt;h4&gt;主要发现&lt;/h4&gt;GAPrompt在多个基准测试中显著优于现有的参数高效微调方法，并且与全微调相比，只使用了2.19%的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;GAPrompt是一种有效的参数高效微调方法，能够提高3D视觉模型在点云数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained 3D vision models have gained significant attention for theirpromising performance on point cloud data. However, fully fine-tuning thesemodels for downstream tasks is computationally expensive and storage-intensive.Existing parameter-efficient fine-tuning (PEFT) approaches, which focusprimarily on input token prompting, struggle to achieve competitive performancedue to their limited ability to capture the geometric information inherent inpoint clouds. To address this challenge, we propose a novel Geometry-AwarePoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance theadaptability of 3D vision models. First, we introduce a Point Prompt thatserves as an auxiliary input alongside the original point cloud, explicitlyguiding the model to capture fine-grained geometric details. Additionally, wepresent a Point Shift Prompter designed to extract global shape informationfrom the point cloud, enabling instance-specific geometric adjustments at theinput level. Moreover, our proposed Prompt Propagation mechanism incorporatesthe shape information into the model's feature extraction process, furtherstrengthening its ability to capture essential geometric characteristics.Extensive experiments demonstrate that GAPrompt significantly outperformsstate-of-the-art PEFT methods and achieves competitive results compared to fullfine-tuning on various benchmarks, while utilizing only 2.19% of trainableparameters. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04119v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
      <link>http://arxiv.org/abs/2505.04018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的深度学习框架，用于结构健康监测和控制，通过结合图神经网络（GNN）、变换器和物理信息损失函数，实现了对结构群体进行模态分解和识别。&lt;h4&gt;背景&lt;/h4&gt;模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种能够准确分解动态响应并识别模态特性的模型，无需标签数据，且不受外部载荷或结构配置变化的影响。&lt;h4&gt;方法&lt;/h4&gt;该模型通过将图神经网络（GNN）和变换器模块结合，将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，同时利用GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。模型以纯物理信息和无监督的方式进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟和实验室实验验证，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;与现有的模态识别技术和模型变体相比，该模型在性能上具有优越性，是一种有利的基于群体的结构健康监测方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。本研究提出了一种新型的深度学习框架，该框架结合了图神经网络（GNN）、变换器和物理信息损失函数，以实现对结构群体进行模态分解和识别。变换器模块将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，从而便于识别自然频率和阻尼比。同时，GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。所提出的模型以纯物理信息和无监督的方式进行训练，利用模态分解理论和结构模态的独立性来指导学习，无需标签数据。通过数值模拟和实验室实验的验证表明，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面具有有效性。与现有的模态识别技术和模型变体进行的比较分析进一步强调了其优越性能，使其成为基于群体的结构健康监测的有利方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modal identification is crucial for structural health monitoring andstructural control, providing critical insights into structural dynamics andperformance. This study presents a novel deep learning framework thatintegrates graph neural networks (GNNs), transformers, and a physics-informedloss function to achieve modal decomposition and identification across apopulation of structures. The transformer module decomposesmulti-degrees-of-freedom (MDOF) structural dynamic measurements intosingle-degree-of-freedom (SDOF) modal responses, facilitating theidentification of natural frequencies and damping ratios. Concurrently, the GNNcaptures the structural configurations and identifies mode shapes correspondingto the decomposed SDOF modal responses. The proposed model is trained in apurely physics-informed and unsupervised manner, leveraging modal decompositiontheory and the independence of structural modes to guide learning without theneed for labeled data. Validation through numerical simulations and laboratoryexperiments demonstrates its effectiveness in accurately decomposing dynamicresponses and identifying modal properties from sparse structural dynamicmeasurements, regardless of variations in external loads or structuralconfigurations. Comparative analyses against established modal identificationtechniques and model variations further underscore its superior performance,positioning it as a favorable approach for population-based structural healthmonitoring.</description>
      <author>example@mail.com (Xudong Jian, Kiran Bacsa, Gregory Duthé, Eleni Chatzi)</author>
      <guid isPermaLink="false">2505.04018v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</title>
      <link>http://arxiv.org/abs/2505.03821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Dataset:  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型（VLMs）在执行视觉视角推理的能力，通过一组新的视觉任务进行评估。&lt;h4&gt;背景&lt;/h4&gt;研究者受到人类测试的启发，设计了新的视觉任务来测试VLMs。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在场景理解、空间推理和视觉视角推理三个层面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;研究者利用精心控制的场景，其中一个人形迷你人物与一个物体配对，通过改变物体位置、人形迷你人物的方向以及使用鸟瞰图和表面视图，创建了144个独特的视觉任务。每个任务都与一系列7个诊断问题相匹配，以评估上述三个认知层面。&lt;h4&gt;主要发现&lt;/h4&gt;评估了包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型。结果显示，这些模型在场景理解方面表现良好，但在空间推理方面表现显著下降，在视角推理方面进一步恶化。&lt;h4&gt;结论&lt;/h4&gt;分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了视觉语言模型（VLMs）执行视觉视角推理的能力，通过一组受人类测试启发的全新视觉任务进行探究。我们的方法利用了精心控制的场景，其中一个人形迷你人物与一个物体配对。通过系统地改变空间配置，如物体相对于人形迷你人物的位置和人形迷你人物的方向，以及使用鸟瞰图和表面视图，我们创建了144个独特的视觉任务。每个视觉任务都与一系列7个诊断问题相匹配，旨在评估三个层次的视觉认知：场景理解、空间推理和视觉视角推理。我们对包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型进行了评估，发现尽管它们在场景理解方面表现出色，但在空间推理方面的表现显著下降，在视角推理方面的表现进一步恶化。我们的分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the ability of Vision Language Models (VLMs) to perform visualperspective taking using a novel set of visual tasks inspired by establishedhuman tests. Our approach leverages carefully controlled scenes, in which asingle humanoid minifigure is paired with a single object. By systematicallyvarying spatial configurations - such as object position relative to thehumanoid minifigure and the humanoid minifigure's orientation - and using bothbird's-eye and surface-level views, we created 144 unique visual tasks. Eachvisual task is paired with a series of 7 diagnostic questions designed toassess three levels of visual cognition: scene understanding, spatialreasoning, and visual perspective taking. Our evaluation of severalstate-of-the-art models, including GPT-4-Turbo, GPT-4o,Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals thatwhile they excel in scene understanding, the performance declines significantlyon spatial reasoning and further deteriorates on perspective-taking. Ouranalysis suggests a gap between surface-level object recognition and the deeperspatial and perspective reasoning required for complex visual tasks, pointingto the need for integrating explicit geometric representations and tailoredtraining protocols in future VLM development.</description>
      <author>example@mail.com (Gracjan Góral, Alicja Ziarko, Piotr Miłoś, Michał Nauman, Maciej Wołczyk, Michał Kosiński)</author>
      <guid isPermaLink="false">2505.03821v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems</title>
      <link>http://arxiv.org/abs/2505.03946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了高性能计算（HPC）环境中的资源分配问题，提出了一种基于DD-PPO算法的新型调度器，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;高性能计算环境中的资源分配是一个复杂的问题，调度算法不仅要高效分配系统资源，还要优化多个性能指标，如作业等待时间和系统利用率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更加适应性和智能的调度策略，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于DD-PPO算法的调度器，该算法支持大规模分布式训练，且无需在每一步进行参数同步。&lt;h4&gt;主要发现&lt;/h4&gt;与传统调度器和现有的基于RL的调度算法相比，DD-PPO调度器在调度性能上有所提升。&lt;h4&gt;结论&lt;/h4&gt;DD-PPO调度器通过消除对集中更新共享策略的依赖，提高了可扩展性、训练效率和样本利用率，从而在HPC调度中展现出更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高性能计算（HPC）环境中的资源分配对作业调度算法提出了复杂且多方面的挑战。除了高效分配系统资源之外，调度器还必须考虑并优化多个性能指标，包括作业等待时间和系统利用率。虽然基于规则的调度算法在当前的HPC系统部署中占主导地位，但随着这些系统的异构性和规模的增加，预计将挑战这些算法在最小化作业等待时间和最大化利用率方面的效率和灵活性。最近的研究努力集中在利用强化学习（RL）的进步来开发更适应性和智能的调度策略。最近基于RL的调度方法探索了从深度Q网络（DQN）到近端策略优化（PPO）等多种算法，以及最近将图神经网络与RL技术相结合的混合方法。然而，这些方法的共同局限性是它们依赖于相对较小的数据集，并且当使用大数据集时，这些方法面临着可扩展性问题。本研究介绍了一种基于DD-PPO算法的新型RL调度器，该算法支持跨多个工作者的分布式训练，而无需在每一步进行参数同步。通过消除对共享策略集中更新的依赖，DD-PPO调度器提高了可扩展性、训练效率和样本利用率。验证数据集利用了超过1150万个真实的HPC作业跟踪数据，用于比较DD-PPO在传统和先进调度方法之间的性能，实验结果表明，与基于规则的调度器和现有的基于RL的调度算法相比，DD-PPO调度器的调度性能有所改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Resource allocation in High Performance Computing (HPC) environments presentsa complex and multifaceted challenge for job scheduling algorithms. Beyond theefficient allocation of system resources, schedulers must account for andoptimize multiple performance metrics, including job wait time and systemutilization. While traditional rule-based scheduling algorithms dominate thecurrent deployments of HPC systems, the increasing heterogeneity and scale ofthose systems is expected to challenge the efficiency and flexibility of thosealgorithms in minimizing job wait time and maximizing utilization. Recentresearch efforts have focused on leveraging advancements in ReinforcementLearning (RL) to develop more adaptable and intelligent scheduling strategies.Recent RL-based scheduling approaches have explored a range of algorithms, fromDeep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,hybrid methods that integrate Graph Neural Networks with RL techniques.However, a common limitation across these methods is their reliance onrelatively small datasets, and these methods face scalability issues when usinglarge datasets. This study introduces a novel RL-based scheduler utilizing theDecentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,which supports large-scale distributed training across multiple workers withoutrequiring parameter synchronization at every step. By eliminating reliance oncentralized updates to a shared policy, the DD-PPO scheduler enhancesscalability, training efficiency, and sample utilization. The validationdataset leveraged over 11.5 million real HPC job traces for comparing DD-PPOperformance between traditional and advanced scheduling approaches, and theexperimental results demonstrate improved scheduling performance in comparisonto both rule-based schedulers and existing RL-based scheduling algorithms.</description>
      <author>example@mail.com (Matthew Sgambati, Aleksandar Vakanski, Matthew Anderson)</author>
      <guid isPermaLink="false">2505.03946v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype</title>
      <link>http://arxiv.org/abs/2505.03853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRAPE的基因调控网络构建方法，通过结合预训练的大型语言模型和DNA序列模型，提高基因调控网络的构建效率。&lt;h4&gt;背景&lt;/h4&gt;预测基因扰动有助于在实验室实验之前识别关键基因，提高实验效率。构建基因调控网络对于理解和预测基因扰动的影响至关重要。&lt;h4&gt;目的&lt;/h4&gt;提高基因调控网络的构建效率和准确性，并能够捕捉基因间的潜在相互作用。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的大型语言模型和DNA序列模型提取基因描述和DNA序列数据中的特征，引入基因生物型信息，并通过图结构学习动态优化基因调控网络。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPE方法在公开数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;GRAPE方法通过结合多源信息和图结构学习，能够有效构建基因调控网络，提高基因扰动预测的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting genetic perturbations enables the identification of potentiallycrucial genes prior to wet-lab experiments, significantly improving overallexperimental efficiency. Since genes are the foundation of cellular life,building gene regulatory networks (GRN) is essential to understand and predictthe effects of genetic perturbations. However, current methods fail to fullyleverage gene-related information, and solely rely on simple evaluation metricsto construct coarse-grained GRN. More importantly, they ignore functionaldifferences between biotypes, limiting the ability to capture potential geneinteractions. In this work, we leverage pre-trained large language model andDNA sequence model to extract features from gene descriptions and DNA sequencedata, respectively, which serve as the initialization for gene representations.Additionally, we introduce gene biotype information for the first time ingenetic perturbation, simulating the distinct roles of genes with differentbiotypes in regulating cellular processes, while capturing implicit generelationships through graph structure learning (GSL). We propose GRAPE, aheterogeneous graph neural network (HGNN) that leverages gene representationsinitialized with features from descriptions and sequences, models the distinctroles of genes with different biotypes, and dynamically refines the GRN throughGSL. The results on publicly available datasets show that our method achievesstate-of-the-art performance.</description>
      <author>example@mail.com (Changxi Chi, Jun Xia, Jingbo Zhou, Jiabei Cheng, Chang Yu, Stan Z. Li)</author>
      <guid isPermaLink="false">2505.03853v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>QStore: Quantization-Aware Compressed Model Storage</title>
      <link>http://arxiv.org/abs/2505.04081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了QStore，一种用于高效存储模型两种精度（高精度和低精度）的无损压缩格式。&lt;h4&gt;背景&lt;/h4&gt;现代应用广泛使用大型多模态基础模型，这些应用通常具有复杂的流程，需要存储和使用多个精度的相似模型。&lt;h4&gt;目的&lt;/h4&gt;降低存储成本，同时不牺牲模型加载速度。&lt;h4&gt;方法&lt;/h4&gt;QStore存储低精度模型以及重建高精度模型所需的残差信息，而不是分别存储低精度和高精度模型。&lt;h4&gt;主要发现&lt;/h4&gt;QStore在压缩多个精度的流行基础模型时，可以将整体存储占用减少高达2.2倍（原大小的45%），同时模型保存和加载速度比现有方法快1.7倍和1.8倍。&lt;h4&gt;结论&lt;/h4&gt;QStore是一种有效的解决方案，可以同时存储模型的两种精度，同时显著降低存储成本并提高加载速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern applications commonly leverage large, multi-modal foundation models.These applications often feature complex workflows that demand the storage andusage of similar models in multiple precisions. A straightforward approach isto maintain a separate file for each model precision (e.g., INT8, BF16), whichis indeed the approach taken by many model providers such as HuggingFace andOllama. However, this approach incurs excessive storage costs since a higherprecision model (e.g., BF16) is a strict superset of a lower precision model(e.g., INT8) in terms of information. Unfortunately, simply maintaining onlythe higher-precision model and requiring every user to dynamically convert themodel precision is not desirable because every user of lower precision modelsmust pay the cost for model download and precision conversion.  In this paper, we present QStore, a unified, lossless compression format forsimultaneously storing a model in two (high and low) precisions efficiently.Instead of storing low-precision and high-precision models separately, QStorestores low-precision model and only the residual information needed toreconstruct high-precision models. The size of residual information issignificantly smaller than the original high-precision models, thus achievinghigh savings in storage cost. Moreover, QStore does not compromise the speed ofmodel loading. The low-precision models can be loaded quickly just like before.The high-precision models can also be reconstructed efficiently in memory bymerging low-precision data and the residual with QStore's lightweight decodinglogic. We evaluate QStore for compressing multiple precisions of popularfoundation models, and show that QStore reduces overall storage footprint by upto 2.2x (45% of the original size) while enabling up to 1.7x and 1.8x fastermodel saving and loading versus existing approaches.</description>
      <author>example@mail.com (Raunak Shah, Zhaoheng Li, Yongjoo Park)</author>
      <guid isPermaLink="false">2505.04081v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>BuildingBlock: A Hybrid Approach for Structured Building Generation</title>
      <link>http://arxiv.org/abs/2505.04051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025 (Conference Track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BuildingBlock的混合方法，用于生成多样化、结构化且层次一致的建筑模型，适用于游戏、虚拟现实和数字孪生等领域。&lt;h4&gt;背景&lt;/h4&gt;当前三维建筑生成方法面临生成多样化、结构化和层次一致建筑模型的挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出BuildingBlock方法来解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;BuildingBlock方法包括两个阶段：布局生成阶段（LGP）和建筑构建阶段（BCP）。LGP将基于箱子的布局生成重新定义为点云生成任务，使用新构建的建筑数据集和基于Transformer的扩散模型创建全局一致的布局。利用LLM将这些布局扩展为基于规则的层次化设计，无缝整合组件风格和空间结构。BCP利用这些布局来指导程序内容生成（PCG），实现局部可定制、高质量的结构化建筑生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，BuildingBlock方法在生成多样化、层次化结构化的建筑方面是有效的，在多个基准测试中达到了最先进的结果，并为进一步的可扩展和直观的建筑工作流程铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;BuildingBlock方法为三维建筑生成提供了一种新的有效途径，有助于推动相关领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3721238.3730705&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional building generation is vital for applications in gaming,virtual reality, and digital twins, yet current methods face challenges inproducing diverse, structured, and hierarchically coherent buildings. Wepropose BuildingBlock, a hybrid approach that integrates generative models,procedural content generation (PCG), and large language models (LLMs) toaddress these limitations. Specifically, our method introduces a two-phasepipeline: the Layout Generation Phase (LGP) and the Building Construction Phase(BCP).  LGP reframes box-based layout generation as a point-cloud generation task,utilizing a newly constructed architectural dataset and a Transformer-baseddiffusion model to create globally consistent layouts. With LLMs, these layoutsare extended into rule-based hierarchical designs, seamlessly incorporatingcomponent styles and spatial structures.  The BCP leverages these layouts to guide PCG, enabling local-customizable,high-quality structured building generation. Experimental results demonstrateBuildingBlock's effectiveness in generating diverse and hierarchicallystructured buildings, achieving state-of-the-art results on multiplebenchmarks, and paving the way for scalable and intuitive architecturalworkflows.</description>
      <author>example@mail.com (Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu)</author>
      <guid isPermaLink="false">2505.04051v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>VideoLLM Benchmarks and Evaluation: A Survey</title>
      <link>http://arxiv.org/abs/2505.03829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对视频大型语言模型（VideoLLMs）的基准和评估方法进行了全面分析。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）的快速发展推动了视频理解技术的进步。&lt;h4&gt;目的&lt;/h4&gt;旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别视频理解领域的发展方向。&lt;h4&gt;方法&lt;/h4&gt;分析了视频理解基准的特点、评估协议和局限性，包括闭集、开集和针对时间和时空理解任务的专业评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;指出了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。&lt;h4&gt;结论&lt;/h4&gt;提出了未来研究方向，包括改进基准设计、评估指标和协议，需要更多样化、多模态和可解释性强的基准。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）的快速发展推动了视频理解技术的显著进步。本文对专为视频大型语言模型（VideoLLMs）设计或使用的基准和评估方法进行了全面分析。我们考察了视频理解基准的现状，讨论了其特点、评估协议和局限性。论文分析了各种评估方法，包括闭集、开集以及针对时间和时空理解任务的专业评估。我们强调了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。此外，我们提出了未来研究方向，包括改进基准设计、评估指标和协议，包括需要更多样化、多模态和可解释性强的基准。本调查旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别利用大型语言模型推进视频理解领域的前景方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Large Language Models (LLMs) has catalyzedsignificant advancements in video understanding technologies. This surveyprovides a comprehensive analysis of benchmarks and evaluation methodologiesspecifically designed or used for Video Large Language Models (VideoLLMs). Weexamine the current landscape of video understanding benchmarks, discussingtheir characteristics, evaluation protocols, and limitations. The paperanalyzes various evaluation methodologies, including closed-set, open-set, andspecialized evaluations for temporal and spatiotemporal understanding tasks. Wehighlight the performance trends of state-of-the-art VideoLLMs across thesebenchmarks and identify key challenges in current evaluation frameworks.Additionally, we propose future research directions to enhance benchmarkdesign, evaluation metrics, and protocols, including the need for more diverse,multimodal, and interpretability-focused benchmarks. This survey aims to equipresearchers with a structured understanding of how to effectively evaluateVideoLLMs and identify promising avenues for advancing the field of videounderstanding with large language models.</description>
      <author>example@mail.com (Yogesh Kumar)</author>
      <guid isPermaLink="false">2505.03829v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2505.03848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  46 pages, 22 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种集成了拓扑数据分析（TDA）、自监督学习和迁移学习的高级聚类框架，用于处理半导体制造中生成的大量图像数据，以提高缺陷识别和产量优化。&lt;h4&gt;背景&lt;/h4&gt;半导体制造产生的图像数据量巨大，对于缺陷识别和产量优化至关重要，但通常超过了人工检查的能力。&lt;h4&gt;目的&lt;/h4&gt;旨在通过一种新的无监督图像聚类方法，提高高维、未标记数据在缺陷识别和产量优化方面的有效性。&lt;h4&gt;方法&lt;/h4&gt;该框架结合了TDA捕获内在拓扑特征，自监督学习从未标记数据中提取有意义的表示，以及迁移学习提高框架的适应性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和开源半导体图像数据集上验证，该框架成功识别了与缺陷模式和工艺变化一致的聚类。&lt;h4&gt;结论&lt;/h4&gt;本研究突出了结合TDA、自监督学习和迁移学习的变革潜力，为半导体制造和其他具有大规模图像数据集的领域提供了可扩展的主动过程监控和质量控制解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing generates vast amounts of image data, crucial fordefect identification and yield optimization, yet often exceeds manualinspection capabilities. Traditional clustering techniques struggle withhigh-dimensional, unlabeled data, limiting their effectiveness in capturingnuanced patterns. This paper introduces an advanced clustering framework thatintegrates deep Topological Data Analysis (TDA) with self-supervised andtransfer learning techniques, offering a novel approach to unsupervised imageclustering. TDA captures intrinsic topological features, while self-supervisedlearning extracts meaningful representations from unlabeled data, reducingreliance on labeled datasets. Transfer learning enhances the framework'sadaptability and scalability, allowing fine-tuning to new datasets withoutretraining from scratch. Validated on synthetic and open-source semiconductorimage datasets, the framework successfully identifies clusters aligned withdefect patterns and process variations. This study highlights thetransformative potential of combining TDA, self-supervised learning, andtransfer learning, providing a scalable solution for proactive processmonitoring and quality control in semiconductor manufacturing and other domainswith large-scale image datasets.</description>
      <author>example@mail.com (Janhavi Giri, Attila Lengyel, Don Kent, Edward Kibardin)</author>
      <guid isPermaLink="false">2505.03848v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
      <link>http://arxiv.org/abs/2505.01880v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5figures. This paper has been accepted for IJCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式音频-语言共学习网络（LOCO），用于定位部分伪造音频的伪造区域，通过协同学习和自监督方式在弱监督场景下提升定位性能。&lt;h4&gt;背景&lt;/h4&gt;现有的音频时间伪造定位方法依赖于使用细粒度注释训练高效网络，但在实际场景中获取这些注释成本高且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文旨在提出一种能够有效定位伪造区域的方法。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了音频-语言共学习模块，通过时间维度和全局维度的语义对齐来捕捉伪造的一致性特征；2. 使用语句级注释和可学习提示构建伪造感知提示，动态地将语义先验纳入时间内容特征；3. 应用伪造定位模块，基于融合的伪造类激活序列生成伪造提议；4. 引入渐进式细化策略，生成伪帧级标签，并利用监督语义对比学习增强真实内容与伪造内容之间的语义区别，从而持续优化伪造感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开基准数据集上的实验表明，提出的LOCO实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在音频时间伪造定位任务上取得了显著的成果，为解决现实场景中的挑战提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio temporal forgery localization (ATFL) aims to find the precise forgeryregions of the partial spoof audio that is purposefully modified. Existing ATFLmethods rely on training efficient networks using fine-grained annotations,which are obtained costly and challenging in real-world scenarios. To meet thischallenge, in this paper, we propose a progressive audio-language co-learningnetwork (LOCO) that adopts co-learning and self-supervision manners to promptlocalization performance under weak supervision scenarios. Specifically, anaudio-language co-learning module is first designed to capture forgeryconsensus features by aligning semantics from temporal and global perspectives.In this module, forgery-aware prompts are constructed by using utterance-levelannotations together with learnable prompts, which can incorporate semanticpriors into temporal content features dynamically. In addition, a forgerylocalization module is applied to produce forgery proposals based on fusedforgery-class activation sequences. Finally, a progressive refinement strategyis introduced to generate pseudo frame-level labels and leverage supervisedsemantic contrastive learning to amplify the semantic distinction between realand fake content, thereby continuously optimizing forgery-aware features.Extensive experiments show that the proposed LOCO achieves SOTA performance onthree public benchmarks.</description>
      <author>example@mail.com (Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo)</author>
      <guid isPermaLink="false">2505.01880v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
      <link>http://arxiv.org/abs/2505.02831v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Self-Representation Alignment for Diffusion Transformers&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Self-Representation Alignment (SRA)的方法，通过自我蒸馏的方式，在不引入额外复杂框架或依赖外部预训练模型的情况下，提高扩散变换器的内部表示学习，从而加速生成训练并提高生成质量。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么需要引入额外的复杂表示训练框架，要么依赖大规模预训练的表示基础模型来在原始生成训练过程中提供表示指导。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，使扩散变换器能够在不依赖外部表示组件的情况下提供表示指导。&lt;h4&gt;方法&lt;/h4&gt;SRA通过自我蒸馏的方式，将扩散变换器早期层的高噪声输出潜在表示与后期层的低噪声输出潜在表示进行对齐，以逐步增强仅在生成训练过程中的整体表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将SRA应用于DiTs和SiTs可以带来一致的性能提升。SRA不仅显著优于依赖于辅助复杂表示训练框架的方法，而且其性能与高度依赖强大外部表示先验的方法相当。&lt;h4&gt;结论&lt;/h4&gt;SRA是一种简单而直接的方法，可以有效地提高扩散变换器的表示学习，并显著提升生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated that learning a meaningful internalrepresentation can both accelerate generative training and enhance generationquality of the diffusion transformers. However, existing approaches necessitateto either introduce an additional and complex representation training frameworkor rely on a large-scale, pre-trained representation foundation model toprovide representation guidance during the original generative trainingprocess. In this study, we posit that the unique discriminative processinherent to diffusion transformers enables them to offer such guidance withoutrequiring external representation components. We therefore proposeSelf-Representation Alignment (SRA), a simple yet straightforward method thatobtain representation guidance through a self-distillation manner.Specifically, SRA aligns the output latent representation of the diffusiontransformer in earlier layer with higher noise to that in later layer withlower noise to progressively enhance the overall representation learning duringonly generative training process. Experimental results indicate that applyingSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRAnot only significantly outperforms approaches relying on auxiliary, complexrepresentation training frameworks but also achieves performance comparable tomethods that heavily dependent on powerful external representation priors.</description>
      <author>example@mail.com (Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang)</author>
      <guid isPermaLink="false">2505.02831v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments</title>
      <link>http://arxiv.org/abs/2505.03825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in Expert Systems with Applications (DOI pending)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF)的框架，用于从多维时间序列中学习有效的表示，并解决了低训练数据环境下学习复杂特征的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界系统中对多维时间序列进行分类需要精细学习复杂特征，如跨维依赖和类内变化，同时面临训练数据不足的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种既灵活又高效的方法来学习多维时间序列的有效表示，并提高分类性能。&lt;h4&gt;方法&lt;/h4&gt;ITA-CTF框架包括CTF模块和ITA模块。CTF模块学习时间序列的核心解释成分及其联合依赖，并通过对比损失优化来提高分类性能。ITA模块生成针对性强且信息丰富的增强数据，以强调原始数据中的类内模式，同时保留类属性。&lt;h4&gt;主要发现&lt;/h4&gt;与标准TF和多个深度学习基准相比，ITA-CTF在五个不同的分类任务上实现了显著的性能提升，最高可达18.7%。&lt;h4&gt;结论&lt;/h4&gt;ITA-CTF是一种有效的框架，可以在低训练数据环境下对多维时间序列进行分类，并通过智能增强和对比学习显著提高分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification of multi-dimensional time series from real-world systemsrequire fine-grained learning of complex features such as cross-dimensionaldependencies and intra-class variations-all under the practical challenge oflow training data availability. However, standard deep learning (DL) strugglesto learn generalizable features in low-data environments due to modeloverfitting. We propose a versatile yet data-efficient framework, IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effectiverepresentations from multi-dimensional time series. The CTF module learns coreexplanatory components of the time series (e.g., sensor factors, temporalfactors), and importantly, their joint dependencies. Notably, unlike standardtensor factorization (TF), the CTF module incorporates a new contrastive lossoptimization to induce similarity learning and class-awareness into the learntrepresentations for better classification performance. To strengthen thiscontrastive learning, the preceding ITA module generates targeted butinformative augmentations that highlight realistic intra-class patterns in theoriginal data, while preserving class-wise properties. This is achieved bydynamically sampling a "soft" class prototype to guide the warping of eachquery data sample, which results in an augmentation that is intelligentlypattern-mixed between the "soft" class prototype and the query sample. Theseaugmentations enable the CTF module to recognize complex intra-class variationsdespite the limited original training data, and seek out invariant class-wiseproperties for accurate classification performance. The proposed method iscomprehensively evaluated on five different classification tasks. Compared tostandard TF and several DL benchmarks, notable performance improvements up to18.7% were achieved.</description>
      <author>example@mail.com (Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau)</author>
      <guid isPermaLink="false">2505.03825v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
      <link>http://arxiv.org/abs/2505.02173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间序列聚类方法，用于将具有相似行为的时间序列数据分类到不同的组中。&lt;h4&gt;背景&lt;/h4&gt;时间序列聚类是一种无监督学习方法，已在医疗保健、金融、经济、能源和气候科学等领域得到应用。已有多种时间序列聚类方法被提出并使用超过四十年，大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的相似性度量方法，称为排序皮尔逊相关差异（RDPC），并将其应用于层次聚类，以评估和比较现有聚类算法的性能。&lt;h4&gt;方法&lt;/h4&gt;RDPC方法结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。&lt;h4&gt;主要发现&lt;/h4&gt;RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。&lt;h4&gt;结论&lt;/h4&gt;通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间序列聚类是一种无监督学习方法，用于将具有相似行为的时间序列数据分类到具有相似行为的组中。它在医疗保健、金融、经济、能源和气候科学等领域得到应用。已经提出了多种时间序列聚类方法，并且已经使用了超过四十年。大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。在这项工作中，我们提出了一种新的差异度量方法，称为排序皮尔逊相关差异（RDPC），它结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。它被纳入层次聚类中。性能被评估并与现有聚类算法进行了比较。结果表明，RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。最后，我们通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series clustering is an unsupervised learning method for classifyingtime series data into groups with similar behavior. It is used in applicationssuch as healthcare, finance, economics, energy, and climate science. Severaltime series clustering methods have been introduced and used for over fourdecades. Most of them focus on measuring either Euclidean distances orassociation dissimilarities between time series. In this work, we propose a newdissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),which combines a weighted average of a specified fraction of the largestelement-wise differences with the well-known Pearson correlation dissimilarity.It is incorporated into hierarchical clustering. The performance is evaluatedand compared with existing clustering algorithms. The results show that theRDPC algorithm outperforms others in complicated cases involving differentseasonal patterns, trends, and peaks. Finally, we demonstrate our method byclustering a random sample of customers from a Thai electricity consumptiontime series dataset into seven groups with unique characteristics.</description>
      <author>example@mail.com (Chutiphan Charoensuk, Nathakhun Wiroonsri)</author>
      <guid isPermaLink="false">2505.02173v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2505.03833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointExplainer的可解释诊断策略，用于分析手绘信号以早期诊断帕金森病，并通过实验证明其能够提供直观的解释而不会降低诊断性能。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在分析数字化手绘信号方面显示出潜力，但现有诊断方法的可解释性不足，这给临床信任带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出PointExplainer，旨在识别推动模型诊断的手绘区域，并量化它们对模型决策的相对贡献。&lt;h4&gt;方法&lt;/h4&gt;PointExplainer包括一个诊断模块和一个解释模块。诊断模块将手绘信号编码为3D点云以表示手绘轨迹；解释模块训练一个可解释的代理模型来近似黑盒诊断模型的行为。此外，还引入了一致性度量来解决解释中的忠实度问题。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集和一个新构建的数据集上的实验表明，PointExplainer能够提供直观的解释，并且没有诊断性能的下降。&lt;h4&gt;结论&lt;/h4&gt;PointExplainer是一种有效的可解释诊断策略，有助于提高临床对帕金森病早期诊断的信任。&lt;h4&gt;翻译&lt;/h4&gt;Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.</description>
      <author>example@mail.com (Xuechao Wang, Sven Nomm, Junqing Huang, Kadri Medijainen, Aaro Toomela, Michael Ruzhansky)</author>
      <guid isPermaLink="false">2505.03833v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating the Impact of Electrode Shift on Classification Performance in Electromyography-Based Motion Prediction Using Sliding-Window Normalization</title>
      <link>http://arxiv.org/abs/2504.03196v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要讨论了电磁肌电图（EMG）信号在假肢、辅助服装和康复等领域的应用，提出了滑动窗口归一化（SWN）技术来减少电极偏移引起的性能下降。&lt;h4&gt;背景&lt;/h4&gt;EMG信号广泛应用于多种领域，但在跨个体泛化、电极偏移和日常变化等方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出SWN技术，旨在减少电极偏移对分类性能的影响，从而提高EMG信号的实用性。&lt;h4&gt;方法&lt;/h4&gt;采用滑动窗口归一化方法，结合z分数归一化和滑动窗口技术，在实时预测场景下减少电极偏移导致的分类性能下降。&lt;h4&gt;主要发现&lt;/h4&gt;SWN技术在肘关节运动（休息、屈曲和伸展）的EMG信号分类中，将分类准确率的下降减少了1.0%，相比无归一化的情况提高了6.6%。当SWN与多电极位置混合策略结合使用时，分类准确率比基准提高了2.4%。&lt;h4&gt;结论&lt;/h4&gt;SWN技术能够有效减少电极偏移引起的性能下降，增强基于EMG的运动估计系统的实用性。&lt;h4&gt;翻译&lt;/h4&gt;Electromyography (EMG) signals are used in many applications, including prosthetic hands, assistive suits, and rehabilitation. Recent advances in motion estimation have improved performance, yet challenges remain in cross-subject generalization, electrode shift, and daily variations. When electrode shift occurs, both transfer learning and adversarial domain adaptation improve classification performance by reducing the performance gap to -1% (eight-class scenario). However, additional data are needed for re-training in transfer learning or for training in adversarial domain adaptation. To address this issue, we investigated a sliding-window normalization (SWN) technique in a real-time prediction scenario. This method combines z-score normalization with a sliding-window approach to reduce the decline in classification performance caused by electrode shift. We validated the effectiveness of SWN using experimental data from a target trajectory tracking task involving the right arm. For three motions classification (rest, flexion, and extension of the elbow) obtained from EMG signals, our offline analysis showed that SWN reduced the differential classification accuracy to -1.0%, representing a 6.6% improvement compared to the case without normalization (-7.6%). Furthermore, when SWN was combined with a strategy that uses a mixture of multiple electrode positions, classification accuracy improved by an additional 2.4% over the baseline. These results suggest that SWN can effectively reduce the performance degradation caused by electrode shift, thereby enhancing the practicality of EMG-based motion estimation systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are used in many applications, includingprosthetic hands, assistive suits, and rehabilitation. Recent advances inmotion estimation have improved performance, yet challenges remain incross-subject generalization, electrode shift, and daily variations. Whenelectrode shift occurs, both transfer learning and adversarial domainadaptation improve classification performance by reducing the performance gapto -1\% (eight-class scenario). However, additional data are needed forre-training in transfer learning or for training in adversarial domainadaptation. To address this issue, we investigated a sliding-windownormalization (SWN) technique in a real-time prediction scenario. This methodcombines z-score normalization with a sliding-window approach to reduce thedecline in classification performance caused by electrode shift. We validatedthe effectiveness of SWN using experimental data from a target trajectorytracking task involving the right arm. For three motions classification (rest,flexion, and extension of the elbow) obtained from EMG signals, our offlineanalysis showed that SWN reduced the differential classification accuracy to-1.0\%, representing a 6.6\% improvement compared to the case withoutnormalization (-7.6\%). Furthermore, when SWN was combined with a strategy thatuses a mixture of multiple electrode positions, classification accuracyimproved by an additional 2.4\% over the baseline. These results suggest thatSWN can effectively reduce the performance degradation caused by electrodeshift, thereby enhancing the practicality of EMG-based motion estimationsystems.</description>
      <author>example@mail.com (Taichi Tanaka, Isao Nambu, Yasuhiro Wada)</author>
      <guid isPermaLink="false">2504.03196v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective</title>
      <link>http://arxiv.org/abs/2505.03828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 tables, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面回顾了从自然语言处理角度的带有情感感知的推荐系统，涵盖了2023年到2025年初的进展。&lt;h4&gt;背景&lt;/h4&gt;电子商务平台产生了大量的用户反馈，如星级评分、书面评论和评论，但大多数推荐引擎主要依赖数值评分，往往忽略了自由文本中嵌入的细微意见。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过将情感分析集成到电子商务推荐器中，通过详细的意见提取来提高预测准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;本文将近期的工作分为四种主要方法：结合情感嵌入与用户物品交互的深度学习分类器、基于transformer的细微特征提取方法、传播情感信号的图神经网络和实时适应用户反馈的对话式推荐器。&lt;h4&gt;主要发现&lt;/h4&gt;本文总结了模型架构，并展示了情感如何通过推荐管道传递，影响基于对话的建议。关键挑战包括处理嘈杂或讽刺性文本、动态用户偏好和偏见缓解。&lt;h4&gt;结论&lt;/h4&gt;本文概述了研究差距，并提供了开发更智能、更公平、更以用户为中心的推荐工具的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; E-commerce platforms generate vast volumes of user feedback, such as starratings, written reviews, and comments. However, most recommendation enginesrely primarily on numerical scores, often overlooking the nuanced opinionsembedded in free text. This paper comprehensively reviews sentiment-awarerecommendation systems from a natural language processing perspective, coveringadvancements from 2023 to early 2025. It highlights the benefits of integratingsentiment analysis into e-commerce recommenders to enhance prediction accuracyand explainability through detailed opinion extraction. Our survey categorizesrecent work into four main approaches: deep learning classifiers that combinesentiment embeddings with user item interactions, transformer based methods fornuanced feature extraction, graph neural networks that propagate sentimentsignals, and conversational recommenders that adapt in real time to userfeedback. We summarize model architectures and demonstrate how sentiment flowsthrough recommendation pipelines, impacting dialogue-based suggestions. Keychallenges include handling noisy or sarcastic text, dynamic user preferences,and bias mitigation. Finally, we outline research gaps and provide a roadmapfor developing smarter, fairer, and more user-centric recommendation tools.</description>
      <author>example@mail.com (Yogesh Gajula)</author>
      <guid isPermaLink="false">2505.03828v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
      <link>http://arxiv.org/abs/2505.03799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in International Joint Conference on Neural Networks  (IJCNN), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDM-InstructGLM的新型指令调整图语言模型框架，用于解决大规模图中LLMs在处理图结构时的可扩展性和效率问题。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，但在处理图相关问题时，由于可扩展性限制和缺乏针对图结构的专用机制，其应用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;旨在提高LLMs在图结构处理中的可扩展性和效率，同时避免依赖GNNs。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于相似度-度数偏差的随机游走机制，该机制根据节点特征相似度和度中心性选择性地采样和编码图信息，确保在LLM中实现自适应和结构化的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了令牌效率，减少了随机采样引起的信息损失，并在节点分类和链接预测等图相关任务上提高了性能。结果表明，仅使用LLMs进行图处理是可行的。&lt;h4&gt;结论&lt;/h4&gt;该研究为无GNN的图学习方法铺平了道路，利用LLMs作为独立的图推理模型。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong capabilities in variousnatural language processing tasks; however, their application to graph-relatedproblems remains limited, primarily due to scalability constraints and theabsence of dedicated mechanisms for processing graph structures. Existingapproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),using GNNs as feature encoders or auxiliary components. However, directlyencoding graph structures within LLMs has been underexplored, particularly inthe context of large-scale graphs where token limitations hinder effectiverepresentation. To address these challenges, we propose SDM-InstructGLM, anovel instruction-tuned Graph Language Model (InstructGLM) framework thatenhances scalability and efficiency without relying on GNNs. Our methodintroduces a similarity-degree-based biased random walk mechanism, whichselectively samples and encodes graph information based on node-featuresimilarity and degree centrality, ensuring an adaptive and structuredrepresentation within the LLM. This approach significantly improves tokenefficiency, mitigates information loss due to random sampling, and enhancesperformance on graph-based tasks such as node classification and linkprediction. Furthermore, our results demonstrate the feasibility of LLM-onlygraph processing, enabling scalable and interpretable Graph Language Models(GLMs) optimized through instruction-based fine-tuning. This work paves the wayfor GNN-free approaches to graph learning, leveraging LLMs as standalone graphreasoning models. Our source code is available on GitHub.</description>
      <author>example@mail.com (Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra)</author>
      <guid isPermaLink="false">2505.03799v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation</title>
      <link>http://arxiv.org/abs/2505.03844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用空间条件技术将卫星SAR图像转换为机载SAR表示的新方法，旨在解决SAR图像数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，合成孔径雷达（SAR）卫星图像的获取能力显著提高，但高分辨率SAR图像的获取仍然昂贵且有限。缺乏开源、标注良好的SAR文本图像数据集限制了现有基础模型在遥感应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;通过合成图像生成技术来扩充稀缺的SAR图像数据，以促进更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;利用ONERA超过15年的航空数据，创建了包含11万张SAR图像的全面训练数据集，并利用一个预训练的包含35亿参数的潜在扩散模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地将模拟图像与ONERA基于物理的模拟器EMPRISE生成的图像的现实感相连接，探索了AI在推进SAR成像技术中的应用。&lt;h4&gt;结论&lt;/h4&gt;本研究首次在文献中引入了这种新方法，为SAR图像处理领域提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The availability of Synthetic Aperture Radar (SAR) satellite imagery hasincreased considerably in recent years, with datasets commercially available.However, the acquisition of high-resolution SAR images in airborneconfigurations, remains costly and limited. Thus, the lack of open source,well-labeled, or easily exploitable SAR text-image datasets is a barrier to theuse of existing foundation models in remote sensing applications. In thiscontext, synthetic image generation is a promising solution to augment thisscarce data, enabling a broader range of applications. Leveraging over 15 yearsof ONERA's extensive archival airborn data from acquisition campaigns, wecreated a comprehensive training dataset of 110 thousands SAR images to exploita 3.5 billion parameters pre-trained latent diffusion model. In this work, wepresent a novel approach utilizing spatial conditioning techniques within afoundation model to transform satellite SAR imagery into airborne SARrepresentations. Additionally, we demonstrate that our pipeline is effectivefor bridging the realism of simulated images generated by ONERA's physics-basedsimulator EMPRISE. Our method explores a key application of AI in advancing SARimaging technology. To the best of our knowledge, we are the first to introducethis approach in the literature.</description>
      <author>example@mail.com (Solène Debuysère, Nicolas Trouvé, Nathan Letheule, Olivier Lévêque, Elise Colin)</author>
      <guid isPermaLink="false">2505.03844v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.02052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于压力传感器的活动识别方法，通过使用生成式基础模型和压力特定的人体活动识别技术，实现了压力数据的自然语言解释，从而提高了人体活动识别的性能。&lt;h4&gt;背景&lt;/h4&gt;人体活动识别（HAR）主要关注惯性测量单元和视觉数据，而忽略了压力传感器在捕捉身体动态和重心变化方面的独特能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决压力传感器在HAR领域应用不足的问题，提出利用生成式基础模型和压力特定技术来处理压力数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一个双向的文本×压力（Text$imes$Pressure，简称TxP）模型，该模型使用生成式基础模型将压力数据解释为自然语言，并实现两个任务：将活动文本描述转换为压力序列（Text2Pressure）和从动态压力图中生成活动描述和分类（Pressure2Text）。TxP在包含超过81,100个文本-压力对的合成PressLang数据集上训练，并使用预训练模型如CLIP和LLaMA 2 13B Chat。&lt;h4&gt;主要发现&lt;/h4&gt;TxP在真实世界数据上验证，如瑜伽和日常任务，提供了基于原子动作的数据增强和分类的新方法，将宏观F1分数提高了最多12.4%，与现有技术相比，显著提升了基于压力的HAR性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过使用生成式基础模型和压力特定技术，实现了对压力数据的自然语言解释，从而提高了人体活动识别的性能，为压力传感器的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;基于传感器的活动识别（HAR）主要关注惯性测量单元和视觉数据，往往忽略了压力传感器在捕捉细微的身体动态和重心变化方面的独特能力。尽管压力传感器在姿势和平衡相关活动方面具有潜力，但由于数据集有限，在HAR领域仍然没有得到充分利用。为了弥合这一差距，我们提出利用生成式基础模型和压力特定的HAR技术。具体来说，我们提出了一种双向文本×压力（Text×Pressure）模型，该模型使用生成式基础模型将压力数据解释为自然语言。TxP实现了两个任务：（1）文本2压力，将活动文本描述转换为压力序列；（2）压力2文本，从动态压力图中生成活动描述和分类。利用预训练模型如CLIP和LLaMA 2 13B Chat，TxP在包含超过81,100个文本-压力对的合成PressLang数据集上训练。在真实世界数据（如瑜伽和日常任务）上验证后，TxP提供了基于原子动作的数据增强和分类的新方法。这使宏观F1分数与现有技术相比提高了最多12.4%，推动了基于压力的HAR的应用和人类运动的深入理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensor-based human activity recognition (HAR) has predominantly focused onInertial Measurement Units and vision data, often overlooking the capabilitiesunique to pressure sensors, which capture subtle body dynamics and shifts inthe center of mass. Despite their potential for postural and balance-basedactivities, pressure sensors remain underutilized in the HAR domain due tolimited datasets. To bridge this gap, we propose to exploit generativefoundation models with pressure-specific HAR techniques. Specifically, wepresent a bidirectional Text$\times$Pressure model that uses generativefoundation models to interpret pressure data as natural language. TxPaccomplishes two tasks: (1) Text2Pressure, converting activity textdescriptions into pressure sequences, and (2) Pressure2Text, generatingactivity descriptions and classifications from dynamic pressure maps.Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained onour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.Validated on real-world data for activities such as yoga and daily tasks, TxPprovides novel approaches to data augmentation and classification grounded inatomic actions. This consequently improved HAR performance by up to 12.4\% inmacro F1 score compared to the state-of-the-art, advancing pressure-based HARwith broader applications and deeper insights into human movement.</description>
      <author>example@mail.com (Lala Shakti Swarup Ray, Lars Krupp, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz)</author>
      <guid isPermaLink="false">2505.02052v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
  <item>
      <title>Joint Generalized Cosine Similarity: A Novel Method for N-Modal Semantic Alignment Based on Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.03532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对多模态深度学习中的对齐任务，提出了一种新型的相似度测量方法，并应用于对比学习中。&lt;h4&gt;背景&lt;/h4&gt;对齐是多模态深度学习中的一个关键任务，而对比学习在该领域被广泛应用。然而，当存在多于两种模态时，现有方法通常计算成对损失函数并聚合它们作为复合损失函数以优化模型参数。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统相似度测量方法的局限性（即它们只能计算两个向量之间的相似度），提出了一个新的相似度测量方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的相似度测量方法：联合广义余弦相似度（JGCS），它围绕从Gram行列式导出的角度进行。基于此，引入了相应的对比学习损失函数GHA Loss以及新的跨模态对比学习范式。&lt;h4&gt;主要发现&lt;/h4&gt;在Derm7pt数据集和模拟数据集上的实验表明，该方法在具有噪声鲁棒性、计算效率和可扩展性等显著优势的同时，实现了优异的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的联合广义余弦相似度不仅适用于对比学习，而且可以轻松扩展到其他领域。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a new similarity measurement method, Joint Generalized Cosine Similarity (JGCS), for alignment tasks in multi-modal deep learning, and applies it in contrastive learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alignment remains a crucial task in multi-modal deep learning, andcontrastive learning has been widely applied in this field. However, when thereare more than two modalities, existing methods typically calculate pairwiseloss function and aggregate them into a composite loss function for theoptimization of model parameters. This limitation mainly stems from thedrawbacks of traditional similarity measurement method (i.e. they can onlycalculate the similarity between two vectors). To address this issue, wepropose a novel similarity measurement method: the Joint Generalized CosineSimilarity (JGCS). Unlike traditional pairwise methods (e.g., dot product orcosine similarity), JGCS centers around the angle derived from the Gramdeterminant. To the best of our knowledge, this is the first similaritymeasurement method capable of handling tasks involving an arbitrary number ofvectors. Based on this, we introduce the corresponding contrastive learningloss function , GHA Loss, and the new inter-modal contrastive learningparadigm. Additionally, comprehensive experiments conducted on the Derm7ptdataset and simulated datasets demonstrate that our method achieves superiorperformance while exhibiting remarkable advantages such as noise robustness,computational efficiency, and scalability. Finally, it is worth mentioning thatthe Joint Generalized Cosine Similarity proposed by us can not only be appliedin contrastive learning, but also be easily extended to other domains.</description>
      <author>example@mail.com (Yiqiao Chen, Zijian Huang)</author>
      <guid isPermaLink="false">2505.03532v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis</title>
      <link>http://arxiv.org/abs/2505.03123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个多模态时空图神经网络框架，用于预测结直肠癌肝转移（CRLM）的进展。&lt;h4&gt;背景&lt;/h4&gt;现有的临床模型未能有效整合肿瘤的空间异质性、动态演变和复杂的多模态数据关系，限制了其预测精度。&lt;h4&gt;目的&lt;/h4&gt;提高CRLM进展的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;将术前CT影像和临床数据结合成异构图结构，通过空间拓扑和跨模态边进行联合建模。使用GraphSAGE聚合时空邻域信息，并利用监督和对比学习策略来增强模型捕捉时间特征和鲁棒性。还提供了一个参数减少78.55%的轻量级模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，模型在MSKCC CRLM数据集上达到了85%的时间邻近准确率，平均绝对误差为1.1005，显著优于现有方法。创新性异构图构建和时空解耦机制有效地揭示了动态肿瘤微环境变化与预后的关联。&lt;h4&gt;结论&lt;/h4&gt;该模型为个性化治疗决策提供了可靠的定量支持。&lt;h4&gt;翻译&lt;/h4&gt;We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a multimodal spatiotemporal graph neural network (STG) frameworkto predict colorectal cancer liver metastasis (CRLM) progression. Currentclinical models do not effectively integrate the tumor's spatial heterogeneity,dynamic evolution, and complex multimodal data relationships, limiting theirpredictive accuracy. Our STG framework combines preoperative CT imaging andclinical data into a heterogeneous graph structure, enabling joint modeling oftumor distribution and temporal evolution through spatial topology andcross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporalneighborhood information and leverages supervised and contrastive learningstrategies to enhance the model's ability to capture temporal features andimprove robustness. A lightweight version of the model reduces parameter countby 78.55%, maintaining near-state-of-the-art performance. The model jointlyoptimizes recurrence risk regression and survival analysis tasks, withcontrastive loss improving feature representational discriminability andcross-modal consistency. Experimental results on the MSKCC CRLM dataset show atime-adjacent accuracy of 85% and a mean absolute error of 1.1005,significantly outperforming existing methods. The innovative heterogeneousgraph construction and spatiotemporal decoupling mechanism effectively uncoverthe associations between dynamic tumor microenvironment changes and prognosis,providing reliable quantitative support for personalized treatment decisions.</description>
      <author>example@mail.com (Yiran Zhu, Wei Yang, Yan su, Zesheng Li, Chengchang Pan, Honggang Qi)</author>
      <guid isPermaLink="false">2505.03123v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing</title>
      <link>http://arxiv.org/abs/2505.03621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysLLM是一个协同优化框架，结合了LLMs和rPPG领域的特定组件，以提高rPPG信号的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;rPPG技术虽然可以实现非接触生理测量，但易受光照变化、运动伪影和时间建模限制。&lt;h4&gt;目的&lt;/h4&gt;提出PhysLLM框架，以解决LLMs在处理rPPG信号时的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 采用Text Prototype Guidance (TPG)策略，通过将生理特征投影到LLM可理解的语义空间，实现跨模态对齐。2. 提出Dual-Domain Stationary (DDS)算法，通过自适应时频域特征重加权来解决问题的不稳定性。3. 通过生理统计、环境上下文回答和任务描述，将生理先验系统地注入rPPG任务，利用跨模态学习整合视觉和文本信息。&lt;h4&gt;主要发现&lt;/h4&gt;PhysLLM在四个基准数据集上实现了最先进的准确性和鲁棒性，证明了其在光照变化和运动场景中的优越泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PhysLLM通过结合LLMs和rPPG特定组件，有效提高了rPPG信号的测量精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote photoplethysmography (rPPG) enables non-contact physiologicalmeasurement but remains highly susceptible to illumination changes, motionartifacts, and limited temporal modeling. Large Language Models (LLMs) excel atcapturing long-range dependencies, offering a potential solution but strugglewith the continuous, noise-sensitive nature of rPPG signals due to theirtext-centric design. To bridge this gap, we introduce PhysLLM, a collaborativeoptimization framework that synergizes LLMs with domain-specific rPPGcomponents. Specifically, the Text Prototype Guidance (TPG) strategy isproposed to establish cross-modal alignment by projecting hemodynamic featuresinto LLM-interpretable semantic space, effectively bridging therepresentational gap between physiological signals and linguistic tokens.Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed forresolving signal instability through adaptive time-frequency domain featurere-weighting. Finally, rPPG task-specific cues systematically injectphysiological priors through physiological statistics, environmental contextualanswering, and task description, leveraging cross-modal learning to integrateboth visual and textual information, enabling dynamic adaptation to challengingscenarios like variable illumination and subject movements. Evaluation on fourbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,demonstrating superior generalization across lighting variations and motionscenarios.</description>
      <author>example@mail.com (Yiping Xie, Bo Zhao, Mingtong Dai, Jian-Ping Zhou, Yue Sun, Tao Tan, Weicheng Xie, Linlin Shen, Zitong Yu)</author>
      <guid isPermaLink="false">2505.03621v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Behavior Intelligence: Technology, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2505.03315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, Pre-print for IWIS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了人工行为智能（ABI）的技术框架，全面分析了人类姿态、面部表情、情绪、行为序列和上下文线索，并探讨了在自动驾驶、智能医疗、监控系统和社会机器人等领域应用ABI的潜力。&lt;h4&gt;背景&lt;/h4&gt;理解和预测人类行为已成为人工智能应用领域的关键能力。&lt;h4&gt;目的&lt;/h4&gt;定义人工行为智能（ABI）的技术框架，并分析其组成部分，同时探讨如何通过大规模预训练模型提高行为识别的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;本文详细阐述了ABI的必要组成部分，包括姿态估计、面部和情绪识别、序列行为分析和上下文感知建模。此外，还介绍了近期在大型语言模型（LLMs）、视觉基础模型和多模态集成模型方面的进展，以及如何通过优化策略解决ABI在实际应用中的技术挑战。&lt;h4&gt;主要发现&lt;/h4&gt;本文指出，为了将ABI应用于实际场景，需要解决从有限数据中学习行为智能、量化复杂行为预测中的不确定性以及优化模型结构以适应低功耗、实时推理等问题。&lt;h4&gt;结论&lt;/h4&gt;研究团队对ABI领域有浓厚兴趣，并正在积极研究，特别是开发能够高效推断复杂人类行为的轻量级智能模型。同时，探索了包括轻量级Transformer、基于图的识别架构、能量感知损失函数和多模态知识蒸馏在内的多种优化策略，并在实时环境中验证其适用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：理解和预测人类行为已成为人工智能应用领域的关键能力。本文定义了人工行为智能（ABI）的技术框架，全面分析了人类姿态、面部表情、情绪、行为序列和上下文线索。它详细介绍了ABI的基本组成部分，包括姿态估计、面部和情绪识别、序列行为分析和上下文感知建模。此外，我们强调了近期在大型语言模型（LLMs）、视觉基础模型和多模态集成模型方面的进展，这些进展在显著提高行为识别的准确性和可解释性方面具有变革性的潜力。我们的研究团队对ABI领域有浓厚兴趣，并正在积极进行研究，特别是专注于开发能够高效推断复杂人类行为的智能轻量级模型。本文确定了在现实应用中部署ABI必须解决的一些技术挑战，包括从有限数据中学习行为智能、量化复杂行为预测中的不确定性以及优化模型结构以适应低功耗、实时推理。为了应对这些挑战，我们的团队正在探索各种优化策略，包括轻量级Transformer、基于图的识别架构、能量感知损失函数和多模态知识蒸馏，同时验证它们在实时环境中的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting human behavior has emerged as a core capabilityin various AI application domains such as autonomous driving, smart healthcare,surveillance systems, and social robotics. This paper defines the technicalframework of Artificial Behavior Intelligence (ABI), which comprehensivelyanalyzes and interprets human posture, facial expressions, emotions, behavioralsequences, and contextual cues. It details the essential components of ABI,including pose estimation, face and emotion recognition, sequential behavioranalysis, and context-aware modeling. Furthermore, we highlight thetransformative potential of recent advances in large-scale pretrained models,such as large language models (LLMs), vision foundation models, and multimodalintegration models, in significantly improving the accuracy andinterpretability of behavior recognition. Our research team has a stronginterest in the ABI domain and is actively conducting research, particularlyfocusing on the development of intelligent lightweight models capable ofefficiently inferring complex human behaviors. This paper identifies severaltechnical challenges that must be addressed to deploy ABI in real-worldapplications including learning behavioral intelligence from limited data,quantifying uncertainty in complex behavior prediction, and optimizing modelstructures for low-power, real-time inference. To tackle these challenges, ourteam is exploring various optimization strategies including lightweighttransformers, graph-based recognition architectures, energy-aware lossfunctions, and multimodal knowledge distillation, while validating theirapplicability in real-time environments.</description>
      <author>example@mail.com (Kanghyun Jo, Jehwan Choi, Kwanho Kim, Seongmin Kim, Duy-Linh Nguyen, Xuan-Thuy Vo, Adri Priadana, Tien-Dat Tran)</author>
      <guid isPermaLink="false">2505.03315v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense</title>
      <link>http://arxiv.org/abs/2505.03424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为GNN-AID的开源框架，旨在解决机器学习模型中可解释性和鲁棒性的问题，特别针对图数据。&lt;h4&gt;背景&lt;/h4&gt;随着对可信人工智能（TAI）需求的增长，机器学习模型的可解释性和鲁棒性变得尤为重要。然而，许多现有工具忽视了图数据，且很少将这两个方面结合在一起。&lt;h4&gt;目的&lt;/h4&gt;提出GNN-AID框架，用于分析、解释和防御图神经网络（GNNs），以解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;GNN-AID是一个基于Python库的框架，支持高级信任方法和架构层，允许用户通过攻击、防御和可解释性方法来分析图数据集和GNN的行为。它建立在PyTorch-Geometric之上，提供预加载的数据集、模型和自定义接口，以支持任何GNNs。还包括一个Web界面，具有图形可视化和无需编码的功能，如交互式模型构建器，简化了GNNs的探索和分析。此外，GNN-AID还支持MLOps技术，确保可重复性和结果版本化。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-AID是一个灵活的工具，可以帮助开发者创建、分析和定制图模型，同时提供预构建的数据集和模型进行快速实验。研究人员可以使用该框架探索可解释性和鲁棒性之间的关系，测试防御策略，并组合方法以保护免受不同类型攻击。此外，还发现了针对规避和中毒攻击的防御策略在图数据上可能产生冲突，突出了防御策略之间的复杂联系。&lt;h4&gt;结论&lt;/h4&gt;GNN-AID是一个强大的工具，旨在帮助开发者和研究人员在机器学习模型的可解释性和鲁棒性方面取得进展，特别针对图数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing need for Trusted AI (TAI) highlights the importance ofinterpretability and robustness in machine learning models. However, manyexisting tools overlook graph data and rarely combine these two aspects into asingle solution. Graph Neural Networks (GNNs) have become a popular approach,achieving top results across various tasks. We introduce GNN-AID (Graph NeuralNetwork Analysis, Interpretation, and Defense), an open-source frameworkdesigned for graph data to address this gap. Built as a Python library, GNN-AIDsupports advanced trust methods and architectural layers, allowing users toanalyze graph datasets and GNN behavior using attacks, defenses, andinterpretability methods.  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,and support for any GNNs through customizable interfaces. It also includes aweb interface with tools for graph visualization and no-code features like aninteractive model builder, simplifying the exploration and analysis of GNNs.The framework also supports MLOps techniques, ensuring reproducibility andresult versioning to track and revisit analyses efficiently.  GNN-AID is a flexible tool for developers and researchers. It helpsdevelopers create, analyze, and customize graph models, while also providingaccess to prebuilt datasets and models for quick experimentation. Researcherscan use the framework to explore advanced topics on the relationship betweeninterpretability and robustness, test defense strategies, and combine methodsto protect against different types of attacks.  We also show how defenses against evasion and poisoning attacks can conflictwhen applied to graph data, highlighting the complex connections betweendefense strategies.  GNN-AID is available at\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}</description>
      <author>example@mail.com (Kirill Lukyanov, Mikhail Drobyshevskiy, Georgii Sazonov, Mikhail Soloviov, Ilya Makarov)</author>
      <guid isPermaLink="false">2505.03424v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Optimization and Program Search using Language Models for Task and Motion Planning</title>
      <link>http://arxiv.org/abs/2505.03725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 8 figures, under review for the 9th Annual Conference on  Robot Learning (CoRL 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的TAMP方法，通过元优化技术解决高级规划和低级控制之间的接口问题。&lt;h4&gt;背景&lt;/h4&gt;智能交互需要机器人代理联合推理高级计划和低级控制。TAMP通过结合符号规划和连续轨迹生成来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;寻找高级规划和低级运动生成之间的最佳接口。&lt;h4&gt;方法&lt;/h4&gt;提出的方法通过：(i) 使用程序搜索作为基础模型和机器人控制之间的接口，来优化轨迹优化问题；(ii) 利用零阶方法优化基础模型输出的数值参数。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的物体操作和绘图任务上的结果证实，该方法优于之前的TAMP方法。&lt;h4&gt;结论&lt;/h4&gt;该方法通过元优化技术提高了TAMP方法的效果，为智能交互提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：与真实世界的智能交互需要机器人代理联合推理高级计划和低级控制。任务和运动规划（TAMP）通过结合符号规划和连续轨迹生成来解决这个问题。最近，TAMP的基础模型方法取得了令人印象深刻的成果，包括快速规划和执行自然语言指令。然而，高级规划和低级运动生成之间的最佳接口仍然是一个未解决的问题：先前的方法要么过于抽象（例如，链式简化技能原语），要么缺乏抽象（例如，直接预测关节角度）。我们的方法通过以下方式引入了一种新颖的技术来解决这些问题：(i) 使用程序搜索作为基础模型和机器人控制之间的接口，用于优化轨迹优化问题；(ii) 利用零阶方法优化基础模型输出的数值参数。在具有挑战性的物体操作和绘图任务上的结果证实，我们提出的方法优于先前的TAMP方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent interaction with the real world requires robotic agents tojointly reason over high-level plans and low-level controls. Task and motionplanning (TAMP) addresses this by combining symbolic planning and continuoustrajectory generation. Recently, foundation model approaches to TAMP havepresented impressive results, including fast planning times and the executionof natural language instructions. Yet, the optimal interface between high-levelplanning and low-level motion generation remains an open question: priorapproaches are limited by either too much abstraction (e.g., chainingsimplified skill primitives) or a lack thereof (e.g., direct joint angleprediction). Our method introduces a novel technique employing a form ofmeta-optimization to address these issues by: (i) using program search overtrajectory optimization problems as an interface between a foundation model androbot control, and (ii) leveraging a zero-order method to optimize numericalparameters in the foundation model output. Results on challenging objectmanipulation and drawing tasks confirm that our proposed method improves overprior TAMP approaches.</description>
      <author>example@mail.com (Denis Shcherba, Eckart Cobo-Briesewitz, Cornelius V. Braun, Marc Toussaint)</author>
      <guid isPermaLink="false">2505.03725v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning</title>
      <link>http://arxiv.org/abs/2505.03703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了新的衡量方法和有效技术来解决视觉-语言模型中的模态差距问题，并通过实验验证了其在下游任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型（VLMs）可以将文本和图像嵌入到共享表示空间中，但存在模态差距现象，即不同模态的嵌入在表示空间中存在明显分离。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种通用的、实用的方法来精确评估和减少模态差距。&lt;h4&gt;方法&lt;/h4&gt;提出了基于光谱和最优传输方法的新的衡量措施和有效技术。&lt;h4&gt;主要发现&lt;/h4&gt;在多个图像-文本数据集和模型上进行的实验表明，这些方法对下游任务（如多模态检索、多模态聚类或零样本分类等）具有积极的影响。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法和技术在减少模态差距和提高下游任务性能方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc., no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) allow to embed texts and images in a sharedrepresentation space. However, it has been shown that these models are subjectto a modality gap phenomenon meaning there exists a clear separation betweenthe embeddings from one modality and another in the embedding space. While thismisalignment is detrimental for downstream tasks such as multimodal retrieval,multimodal clustering or zero-shot classification, etc. no generic andpractical methods have so far been proposed to assess it precisely and evenreduce it. We therefore propose novel measures and effective techniques(spectral- and optimal transport-based methods) to achieve this goal. Extensiveexperiments conducted on several image-text datasets and models demonstratetheir effectiveness and beneficial effects on downstream tasks. Our code isavailable at the URL provided in the paper's abstract.</description>
      <author>example@mail.com (François Role, Sébastien Meyer, Victor Amblard)</author>
      <guid isPermaLink="false">2505.03703v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.03721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于太阳能传感器的可持续智能农场网络，通过深度强化学习和决策理论优化监控效果和能源效率，以应对网络攻击和能量供应波动等挑战。&lt;h4&gt;背景&lt;/h4&gt;太阳能传感器监测系统在农业领域的应用日益广泛，但系统对网络攻击的抵御能力和适应动态能量供应的能力尚不明确。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，论文旨在设计一种可持续的智能农场网络，以在各种网络攻击和能量供应波动条件下保持高质量的动物监测。&lt;h4&gt;方法&lt;/h4&gt;该研究采用深度强化学习（DRL）来制定最佳策略，同时利用迁移学习（TL）和决策理论（DT）加速学习过程，并通过决策理论指导的策略优化监控质量和能源可持续性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，基于决策理论的DRL模型在性能上优于仅使用迁移学习的DRL模型，系统性能得到提升，训练时间缩短了47.5%。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效提高太阳能传感器监测系统的性能和能源效率，为农业领域的智能监测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于太阳能传感器的监测系统已成为农业创新的关键，通过集成传感器技术、物联网、边缘和云计算，推动了农场管理和动物福利的进步。然而，这些系统对网络攻击的抵抗力及其对动态和受限能源供应的适应性仍鲜有研究。为了解决这些挑战，我们提出了一种可持续的智能农场网络，旨在在各种网络和对抗威胁以及波动的能源条件下保持高质量的动物监测。我们的方法利用深度强化学习（DRL）来制定最佳策略，以最大化监控效果和能源效率。为了克服DRL固有的收敛速度慢的挑战，我们整合了迁移学习（TL）和决策理论（DT）来加速学习过程。通过整合决策理论指导的策略，我们优化了监控质量和能源可持续性，显著减少了训练时间，同时实现了可比较的性能回报。我们的实验结果表明，决策理论指导的DRL优于迁移学习增强的DRL模型，提高了系统性能，并将训练时间缩短了47.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar sensor-based monitoring systems have become a crucial agriculturalinnovation, advancing farm management and animal welfare through integratingsensor technology, Internet-of-Things, and edge and cloud computing. However,the resilience of these systems to cyber-attacks and their adaptability todynamic and constrained energy supplies remain largely unexplored. To addressthese challenges, we propose a sustainable smart farm network designed tomaintain high-quality animal monitoring under various cyber and adversarialthreats, as well as fluctuating energy conditions. Our approach utilizes deepreinforcement learning (DRL) to devise optimal policies that maximize bothmonitoring effectiveness and energy efficiency. To overcome DRL's inherentchallenge of slow convergence, we integrate transfer learning (TL) and decisiontheory (DT) to accelerate the learning process. By incorporating DT-guidedstrategies, we optimize monitoring quality and energy sustainability,significantly reducing training time while achieving comparable performancerewards. Our experimental results prove that DT-guided DRL outperformsTL-enhanced DRL models, improving system performance and reducing trainingruntime by 47.5%.</description>
      <author>example@mail.com (Dian Chen, Zelin Wan, Dong Sam Ha, Jin-Hee Cho)</author>
      <guid isPermaLink="false">2505.03721v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GNN-enabled Precoding for Massive MIMO LEO Satellite Communications</title>
      <link>http://arxiv.org/abs/2505.03311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了针对LEO卫星通信中大规模MIMO预编码挑战的解决方案。&lt;h4&gt;背景&lt;/h4&gt;低地球轨道卫星通信是6G网络发展的关键部分，而大规模MIMO技术的集成正在积极探索以增强其性能。&lt;h4&gt;目的&lt;/h4&gt;提出的方法旨在解决LEO卫星通信在受限功率条件下提高通信能效的挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 引入端到端图神经网络（GNN）框架以降低传统预编码方法的计算复杂度；2. 提出Dinkelbach算法的深度展开和加权最小均方误差（WMMSE）方法以实现增强的能效；3. 将泰勒展开法应用于GNN中矩阵求逆，以增强方法的可解释性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;数值实验证明了所提出方法在复杂性和鲁棒性方面的有效性，并显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在提高LEO卫星通信能效方面具有显著潜力，有助于实现更高效和可持续的通信。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low Earth Orbit (LEO) satellite communication is a critical component in thedevelopment of sixth generation (6G) networks. The integration of massivemultiple-input multiple-output (MIMO) technology is being actively explored toenhance the performance of LEO satellite communications. However, the limitedpower of LEO satellites poses a significant challenge in improvingcommunication energy efficiency (EE) under constrained power conditions.Artificial intelligence (AI) methods are increasingly recognized as promisingsolutions for optimizing energy consumption while enhancing system performance,thus enabling more efficient and sustainable communications. This paperproposes approaches to address the challenges associated with precoding inmassive MIMO LEO satellite communications. First, we introduce an end-to-endgraph neural network (GNN) framework that effectively reduces the computationalcomplexity of traditional precoding methods. Next, we introduce a deepunfolding of the Dinkelbach algorithm and the weighted minimum mean squareerror (WMMSE) approach to achieve enhanced EE, transforming iterativeoptimization processes into a structured neural network, thereby improvingconvergence speed and computational efficiency. Furthermore, we incorporate theTaylor expansion method to approximate matrix inversion within the GNN,enhancing both the interpretability and performance of the proposed method.Numerical experiments demonstrate the validity of our proposed method in termsof complexity and robustness, achieving significant improvements overstate-of-the-art methods.</description>
      <author>example@mail.com (Huibin Zhou, Xinrui Gong, Christos G. Tsinos, Li You, Xiqi Gao, Björn Ottersten)</author>
      <guid isPermaLink="false">2505.03311v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality</title>
      <link>http://arxiv.org/abs/2505.03440v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, submitted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为manvr3d的VR平台，用于交互式细胞追踪，结合VR控制器和眼动追踪设备，以加速深度学习细胞追踪模型的校对和验证。&lt;h4&gt;背景&lt;/h4&gt;生物学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平上的发育历史。传统的细胞谱系树重建涉及手动标注细胞位置，并随时间链接它们以创建完整轨迹。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合深度学习细胞追踪软件和3D/VR可视化的系统，以提升细胞追踪的效率和科学家的工作流程。&lt;h4&gt;方法&lt;/h4&gt;利用VR控制器和眼动追踪硬件，将深度学习模型的标注、训练和校对流程提升到三维空间，并应用自然用户界面如手势和眼动追踪。&lt;h4&gt;主要发现&lt;/h4&gt;该系统通过将交互式细胞追踪提升到三维空间，提高了细胞追踪的工作效率和科学家对数据的理解。&lt;h4&gt;结论&lt;/h4&gt;manvr3d平台能够有效加速深度学习细胞追踪模型的训练和验证过程，提高生命科学家的研究效率。&lt;h4&gt;翻译&lt;/h4&gt;We propose manvr3d, a novel VR-ready platform for interactive human-in-the-loop cell tracking. We utilize VR controllers and eye-tracking hardware to facilitate rapid ground truth generation and proofreading for deep learning-based cell tracking models. Life scientists reconstruct the developmental history of organisms on the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. The reconstruction of such cell lineage trees traditionally involves tracking individual cells through all recorded time points, manually annotating their positions, and then linking them over time to create complete trajectories. Deep learning-based algorithms accelerate this process, yet depend heavily on manually-annotated high-quality ground truth data and curation. Visual representation of the image data in this process still relies primarily on 2D renderings, which greatly limits spatial understanding and navigation. In this work, we bridge the gap between deep learning-based cell tracking software and 3D/VR visualization to create a human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the 3rd dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose manvr3d, a novel VR-ready platform for interactivehuman-in-the-loop cell tracking. We utilize VR controllers and eye-trackinghardware to facilitate rapid ground truth generation and proofreading for deeplearning-based cell tracking models. Life scientists reconstruct thedevelopmental history of organisms on the cellular level by analyzing 3Dtime-lapse microscopy images acquired at high spatio-temporal resolution. Thereconstruction of such cell lineage trees traditionally involves trackingindividual cells through all recorded time points, manually annotating theirpositions, and then linking them over time to create complete trajectories.Deep learning-based algorithms accelerate this process, yet depend heavily onmanually-annotated high-quality ground truth data and curation. Visualrepresentation of the image data in this process still relies primarily on 2Drenderings, which greatly limits spatial understanding and navigation. In thiswork, we bridge the gap between deep learning-based cell tracking software and3D/VR visualization to create a human-in-the-loop cell tracking system. We liftthe incremental annotation, training and proofreading loop of the deep learningmodel into the 3rd dimension and apply natural user interfaces like handgestures and eye tracking to accelerate the cell tracking workflow for lifescientists.</description>
      <author>example@mail.com (Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther)</author>
      <guid isPermaLink="false">2505.03440v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph</title>
      <link>http://arxiv.org/abs/2505.03173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RAVU是一个用于增强视频理解的新框架，通过在时空图上的组合推理来提升对长视频的理解能力。&lt;h4&gt;背景&lt;/h4&gt;目前的大多模态模型在处理几分钟到几小时的视频时存在困难，因为它们缺乏明确的记忆和检索机制。&lt;h4&gt;目的&lt;/h4&gt;为了解决LMMs在处理长视频时的局限性，提出了RAVU框架。&lt;h4&gt;方法&lt;/h4&gt;RAVU构建了视频的图表示，捕捉实体之间的空间和时间关系，作为长期记忆来追踪对象及其随时间的变化。为了回答复杂查询，将查询分解为一系列推理步骤，并在图上执行这些步骤，检索相关关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;RAVU方法在两个主要的视频问答数据集NExT-QA和EgoSchema上，与SOTA方法和基线相比，在有限的检索帧（5-10帧）内表现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;RAVU在理解长视频，特别是需要多跳推理和跨帧追踪对象查询方面，能够实现更准确的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehending long videos remains a significant challenge for LargeMulti-modal Models (LMMs). Current LMMs struggle to process even minutes tohours videos due to their lack of explicit memory and retrieval mechanisms. Toaddress this limitation, we propose RAVU (Retrieval Augmented VideoUnderstanding), a novel framework for video understanding enhanced by retrievalwith compositional reasoning over a spatio-temporal graph. We construct a graphrepresentation of the video, capturing both spatial and temporal relationshipsbetween entities. This graph serves as a long-term memory, allowing us to trackobjects and their actions across time. To answer complex queries, we decomposethe queries into a sequence of reasoning steps and execute these steps on thegraph, retrieving relevant key information. Our approach enables more accurateunderstanding of long videos, particularly for queries that require multi-hopreasoning and tracking objects across frames. Our approach demonstrate superiorperformances with limited retrieved frames (5-10) compared with other SOTAmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.</description>
      <author>example@mail.com (Sameer Malik, Moyuru Yamada, Ayush Singh, Dishank Aggarwal)</author>
      <guid isPermaLink="false">2505.03173v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic Environments</title>
      <link>http://arxiv.org/abs/2505.03128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对混合几何和语义3D环境中的机器人导航问题进行研究，提出了一种高效的路径规划算法HCOA*，通过利用环境层次结构，显著降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;在混合几何和语义3D环境中，机器人导航需要考虑环境的几何和语义信息，这对计算资源提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;在保持计算效率的同时，实现从起点到目标点的导航。&lt;h4&gt;方法&lt;/h4&gt;提出了HCOA*算法，该算法利用环境层次结构进行路径规划，并证明了其理论性能保障。同时，引入了两种高层节点分类方法：基于图神经网络的分类方法和基于多数类分类方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验，HCOA*算法在uHumans2 3DSG数据集上能够找到最优路径，同时减少了25%的扩展节点数量，并降低了16%的计算时间。&lt;h4&gt;结论&lt;/h4&gt;HCOA*算法在混合几何和语义3D环境中表现出色，有效提高了导航效率。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了在混合几何和语义3D环境中的机器人导航问题。在给定环境层次结构的情况下，旨在通过最小化计算成本，从起点导航到目标点。我们引入了分层类有序A*（HCOA*）算法，该算法利用环境层次结构进行语义图中的高效路径规划，显著减少了计算工作量。我们使用语义类的一个全序，并证明了该算法的理论性能保证。我们提出了两种基于最低层节点语义的高层节点分类方法：基于图神经网络的分类方法和基于多数类的分类方法。我们通过在3D场景图（3DSG）上的模拟评估了我们的方法，将其与最先进的方法进行了比较，并评估了其分类方法的性能。结果表明，HCOA*能够在3DSG数据集上找到最优路径，同时减少了25%的扩展节点数量，并在uHumans2 3DSG数据集上实现了16%的计算时间降低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problem of robot navigation in mixed geometric andsemantic 3D environments. Given a hierarchical representation of theenvironment, the objective is to navigate from a start position to a goal whileminimizing the computational cost. We introduce Hierarchical Class-ordered A*(HCOA*), an algorithm that leverages the environmental hierarchy for efficientpath-planning in semantic graphs, significantly reducing computational effort.We use a total order over the semantic classes and prove theoreticalperformance guarantees for the algorithm. We propose two approaches forhigher-layer node classification based on the node semantics of the lowestlayer: a Graph Neural Network-based method and a Majority-Class method. Weevaluate our approach through simulations on a 3D Scene Graph (3DSG), comparingit to the state-of-the-art and assessing its performance against ourclassification approaches. Results show that HCOA* can find the optimal pathwhile reducing the number of expanded nodes by 25% and achieving a 16%reduction in computational time on the uHumans2 3DSG dataset.</description>
      <author>example@mail.com (Evangelos Psomiadis, Panagiotis Tsiotras)</author>
      <guid isPermaLink="false">2505.03128v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>BLAB: Brutally Long Audio Bench</title>
      <link>http://arxiv.org/abs/2505.03054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为BLAB的音频语言模型评估基准，用于评估音频模型在处理长音频段时的性能。&lt;h4&gt;背景&lt;/h4&gt;为了适应人类通信的多模态特性，开发能够理解多样化口语交互的大型音频语言模型（LMs）至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索音频语言模型在处理长音频对话片段时的表现，并评估其在定位、持续时间估计、情感和计数任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;BLAB包含833小时以上的不同音频片段，平均长度为51分钟，并配有人工标注的文本问答。研究人员评估了六种开源和专有音频LMs在BLAB上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;所有测试的音频LMs在BLAB上都面临挑战，包括高级模型如Gemini 2.0 Pro和GPT-4o。研究发现，音频LMs在处理长音频时表现不佳，性能随音频时长增加而下降，在定位、时间推理、计数任务上表现不佳，且难以理解非音素信息，更依赖于提示而非音频内容。&lt;h4&gt;结论&lt;/h4&gt;BLAB为开发具有稳健长音频理解能力的音频LMs提供了一个具有挑战性的评估框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：开发能够理解多样化口语交互的大型音频语言模型（LMs）对于适应人类通信的多模态特性至关重要，并且可以增加语言技术在不同用户群体中的可及性。最近关于音频LMs的研究主要评估了它们在短音频片段上的性能，通常长度不超过30秒，对更接近自然用户交互的长音频对话片段的探索有限。我们引入了Brutally Long Audio Bench（BLAB），这是一个具有挑战性的长音频基准，用于评估音频LMs在定位、持续时间估计、情感和计数任务上的表现，使用的音频片段平均长度为51分钟。BLAB由超过833小时的多样化完整音频剪辑组成，每个剪辑都与基于文本的自然语言问答配对。我们的音频数据来自许可来源，并经过人工辅助过滤过程以确保任务符合性。我们在BLAB上评估了六种开源和专有音频LMs，并发现所有这些模型，包括如Gemini 2.0 Pro和GPT-4o等高级模型，在BLAB任务上都面临挑战。我们的综合分析揭示了任务难度与音频时长之间的权衡。总的来说，我们发现音频LMs在处理长音频时表现不佳，随着音频时长的增加，性能下降。它们在定位、时间推理、计数任务上表现不佳，难以理解非音素信息，更多地依赖于提示而非音频内容。BLAB作为一个具有挑战性的评估框架，为开发具有稳健长音频理解能力的音频LMs提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing large audio language models (LMs) capable of understanding diversespoken interactions is essential for accommodating the multimodal nature ofhuman communication and can increase the accessibility of language technologiesacross different user populations. Recent work on audio LMs has primarilyevaluated their performance on short audio segments, typically under 30seconds, with limited exploration of long-form conversational speech segmentsthat more closely reflect natural user interactions with these models. Weintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audiobenchmark that evaluates audio LMs on localization, duration estimation,emotion, and counting tasks using audio segments averaging 51 minutes inlength. BLAB consists of 833+ hours of diverse, full-length audio clips, eachpaired with human-annotated, text-based natural language questions and answers.Our audio data were collected from permissively licensed sources and underwenta human-assisted filtering process to ensure task compliance. We evaluate sixopen-source and proprietary audio LMs on BLAB and find that all of them,including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with thetasks in BLAB. Our comprehensive analysis reveals key insights into thetrade-offs between task difficulty and audio duration. In general, we find thataudio LMs struggle with long-form speech, with performance declining asduration increases. They perform poorly on localization, temporal reasoning,counting, and struggle to understand non-phonemic information, relying more onprompts than audio content. BLAB serves as a challenging evaluation frameworkto develop audio LMs with robust long-form audio understanding capabilities.</description>
      <author>example@mail.com (Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar)</author>
      <guid isPermaLink="false">2505.03054v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2505.03692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多视角点云配准中的位姿图构建和运动同步问题。&lt;h4&gt;背景&lt;/h4&gt;多视角点云配准在机器人、自动化和计算机视觉领域具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的方法来构建可靠的位姿图并进行运动同步。&lt;h4&gt;方法&lt;/h4&gt;设计了一个网络模型，通过提取点云对之间的匹配距离信息来识别可靠的配对；提出另一个神经网络模型，以数据驱动的方式计算绝对位姿，避免优化手工设计的损失函数；模型考虑了几何分布信息，并使用改进的注意力机制以促进灵活可靠的特征交互。&lt;h4&gt;主要发现&lt;/h4&gt;在室内和室外数据集上的实验结果表明，该方法有效且具有泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在多视角点云配准中具有良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2024.3455783&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview point cloud registration plays a crucial role in robotics,automation, and computer vision fields. This paper concentrates on pose graphconstruction and motion synchronization within multiview registration. Previousmethods for pose graph construction often pruned fully connected graphs orconstructed sparse graph using global feature aggregated from localdescriptors, which may not consistently yield reliable results. To identifydependable pairs for pose graph construction, we design a network model thatextracts information from the matching distance between point cloud pairs. Formotion synchronization, we propose another neural network model to calculatethe absolute pose in a data-driven manner, rather than optimizing inaccuratehandcrafted loss functions. Our model takes into account geometric distributioninformation and employs a modified attention mechanism to facilitate flexibleand reliable feature interaction. Experimental results on diverse indoor andoutdoor datasets confirm the effectiveness and generalizability of ourapproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.</description>
      <author>example@mail.com (Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Di Wang)</author>
      <guid isPermaLink="false">2505.03692v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation</title>
      <link>http://arxiv.org/abs/2505.03114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于路径和骨轮廓正则化的无监督MRI到CT转换方法，用于提高骨结构在转换过程中的准确性。&lt;h4&gt;背景&lt;/h4&gt;MRI和CT扫描的结合对于医学影像学具有重要意义，但获取配对的MRI和CT扫描存在实际挑战，因此需要开发能够利用无配对数据集的稳健方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确转换骨结构的方法，适用于需要精确骨表示的放射治疗等应用。&lt;h4&gt;方法&lt;/h4&gt;提出的方法将MRI和CT图像投影到共享的潜在空间，通过神经常微分方程建模MRI到CT的映射，并最小化流的转换路径长度。此外，引入一个可训练的神经网络生成骨轮廓，并实施机制以直接和间接鼓励模型关注骨轮廓及其邻近区域。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的评估表明，该方法优于现有的无监督MRI到CT转换方法，实现了更低的总体误差率。在下游骨分割任务中，该方法在保持骨结构保真度方面表现出优异的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在MRI到CT的无监督转换中表现出良好的性能，尤其是在骨结构的准确转换方面。&lt;h4&gt;翻译&lt;/h4&gt;我们的代码可在https://github.com/kennysyp/PaBoT找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate MRI-to-CT translation promises the integration of complementaryimaging information without the need for additional imaging sessions. Given thepractical challenges associated with acquiring paired MRI and CT scans, thedevelopment of robust methods capable of leveraging unpaired datasets isessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CTtranslation methods, which predominantly rely on cycle consistency andcontrastive learning frameworks, frequently encounter challenges in accuratelytranslating anatomical features that are highly discernible on CT but lessdistinguishable on MRI, such as bone structures. This limitation renders theseapproaches less suitable for applications in radiation therapy, where precisebone representation is essential for accurate treatment planning. To addressthis challenge, we propose a path- and bone-contour regularized approach forunpaired MRI-to-CT translation. In our method, MRI and CT images are projectedto a shared latent space, where the MRI-to-CT mapping is modeled as acontinuous flow governed by neural ordinary differential equations. The optimalmapping is obtained by minimizing the transition path length of the flow. Toenhance the accuracy of translated bone structures, we introduce a trainableneural network to generate bone contours from MRI and implement mechanisms todirectly and indirectly encourage the model to focus on bone contours and theiradjacent regions. Evaluations conducted on three datasets demonstrate that ourmethod outperforms existing unpaired MRI-to-CT translation approaches,achieving lower overall error rates. Moreover, in a downstream bonesegmentation task, our approach exhibits superior performance in preserving thefidelity of bone structures. Our code is available at:https://github.com/kennysyp/PaBoT.</description>
      <author>example@mail.com (Teng Zhou, Jax Luo, Yuping Sun, Yiheng Tan, Shun Yao, Nazim Haouchine, Scott Raymond)</author>
      <guid isPermaLink="false">2505.03114v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Gene finding revisited: improved robustness through structured decoding from learned embeddings</title>
      <link>http://arxiv.org/abs/2505.03377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的基因定位方法，通过结合原始遗传序列的学习嵌入和潜条件随机场的精确解码，提高了基因发现的效果和训练鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;基因发现是识别基因组中编码序列位置的任务，随着原始基因组序列数量的不断增加，基因发现成为了解释（新）生物遗传信息以及学习不同物种间共有的进化模式的重要途径。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一种能够利用深度学习技术在基因发现中取得更好性能的方法。&lt;h4&gt;方法&lt;/h4&gt;本文提出的方法结合了原始遗传序列的学习嵌入和潜条件随机场的精确解码，以提高基因发现的性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在性能上达到了现有最佳水平，同时增加了训练的鲁棒性，并消除了手动拟合长度分布的需求。&lt;h4&gt;结论&lt;/h4&gt;随着DNA语言模型的改进，这种方法为更有效的跨物种基因发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基因定位是识别基因组中编码序列位置的任务。随着原始基因组序列数量的不断增长，基因定位成为了理解（新型）生物遗传信息以及学习进化多样物种间共有模式的 重要途径。当前最先进的方法通常是针对每个生物训练的图形模型，并且需要手动管理的数据集。然而，这些模型缺乏灵活性，无法融入近年来在蛋白质序列分析中具有革命性的深度学习表示学习方法，而这些方法可能会帮助基因发现者利用越来越多的已测序基因组来提高跨多个生物的性能。在此，我们提出了一种新的方法，结合了原始遗传序列的学习嵌入和潜条件随机场的精确解码。我们表明，该模型在性能上达到了现有最佳水平，同时提高了训练的鲁棒性，并消除了需要手动拟合长度分布的需求。随着DNA语言模型的改进，这为更高效的跨物种基因发现开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gene finding is the task of identifying the locations of coding sequenceswithin the vast amount of genetic code contained in the genome. With an everincreasing quantity of raw genome sequences, gene finding is an importantavenue towards understanding the genetic information of (novel) organisms, aswell as learning shared patterns across evolutionarily diverse species. Thecurrent state of the art are graphical models usually trained per organism andrequiring manually curated datasets. However, these models lack the flexibilityto incorporate deep learning representation learning techniques that have inrecent years been transformative in the analysis of pro tein sequences, andwhich could potentially help gene finders exploit the growing number of thesequenced genomes to expand performance across multiple organisms. Here, wepropose a novel approach, combining learned embeddings of raw genetic sequenceswith exact decoding using a latent conditional random field. We show that themodel achieves performance matching the current state of the art, whileincreasing training robustness, and removing the need for manually fittedlength distributions. As language models for DNA improve, this paves the wayfor more performant cross-organism gene-finders.</description>
      <author>example@mail.com (Frederikke I. Marin, Dennis Pultz, Wouter Boomsma)</author>
      <guid isPermaLink="false">2505.03377v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal cascade feature transfer for polymer property prediction</title>
      <link>http://arxiv.org/abs/2505.03704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为多模态级联模型及特征迁移的聚合物性质预测的新颖迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物性质预测依赖于多种数据格式，如分子描述符、添加剂信息及化学结构。&lt;h4&gt;目的&lt;/h4&gt;提高聚合物物理性质的预测精度。&lt;h4&gt;方法&lt;/h4&gt;通过结合由图卷积神经网络（GCN）从化学结构中提取的特征以及分子描述符和添加剂信息等特征。&lt;h4&gt;主要发现&lt;/h4&gt;与仅使用单一特征的基线传统方法相比，所提出的方法在多个聚合物数据集上表现出更高的预测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在聚合物性质预测方面显示出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种名为多模态级联模型及特征迁移的聚合物性质预测的新颖迁移学习方法。聚合物具有多种不同格式的数据特征，包括分子描述符、添加剂信息以及化学结构。然而，在传统方法中，预测模型通常使用每种类型的数据分别构建。我们的模型通过结合由图卷积神经网络（GCN）从化学结构中提取的特征以及分子描述符和添加剂信息等特征，使聚合物物理性质的预测更加准确。使用多个聚合物数据集对所提出的方法的预测性能进行了实证评估。我们报告说，与仅使用单一特征的基线传统方法相比，所提出的方法表现出更高的预测性能。该方法在聚合物性质预测方面显示出优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel transfer learning approach calledmulti-modal cascade model with feature transfer for polymer propertyprediction.Polymers are characterized by a composite of data in severaldifferent formats, including molecular descriptors and additive information aswell as chemical structures. However, in conventional approaches, predictionmodels were often constructed using each type of data separately. Our modelenables more accurate prediction of physical properties for polymers bycombining features extracted from the chemical structure by graph convolutionalneural networks (GCN) with features such as molecular descriptors and additiveinformation. The predictive performance of the proposed method is empiricallyevaluated using several polymer datasets. We report that the proposed methodshows high predictive performance compared to the baseline conventionalapproach using a single feature.</description>
      <author>example@mail.com (Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui)</author>
      <guid isPermaLink="false">2505.03704v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting</title>
      <link>http://arxiv.org/abs/2505.03679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于相机-雷达融合的物体分割框架，通过结合相机和雷达传感器的信息，提高在恶劣天气条件下的语义分割性能。&lt;h4&gt;背景&lt;/h4&gt;物体分割在自动驾驶和机器人领域至关重要，相机传感器在恶劣天气下易受影响，而雷达传感器虽然鲁棒但数据稀疏且噪声大。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，通过融合相机和雷达传感器的信息，增强仅使用相机的分割基准，并提高在恶劣天气条件下的分割性能。&lt;h4&gt;方法&lt;/h4&gt;将扩散模型集成到相机-雷达融合架构中，利用雷达点特征和Segment-Anything模型创建伪掩码，并引入噪声降低单元对伪掩码进行去噪，最终生成补全原始图像缺失信息的重绘图像。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在Waterscenes数据集上，将仅使用相机的分割基准的mIoU提高了2.63%，并将相机-雷达融合架构的mIoU提高了1.48%。&lt;h4&gt;结论&lt;/h4&gt;该方法在恶劣天气条件下使用相机-雷达融合进行语义分割是有效的。&lt;h4&gt;翻译&lt;/h4&gt;Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmenting objects in an environment is a crucial task for autonomous drivingand robotics, as it enables a better understanding of the surroundings of eachagent. Although camera sensors provide rich visual details, they are vulnerableto adverse weather conditions. In contrast, radar sensors remain robust undersuch conditions, but often produce sparse and noisy data. Therefore, apromising approach is to fuse information from both sensors. In this work, wepropose a novel framework to enhance camera-only baselines by integrating adiffusion model into a camera-radar fusion architecture. We leverage radarpoint features to create pseudo-masks using the Segment-Anything model,treating the projected radar points as point prompts. Additionally, we proposea noise reduction unit to denoise these pseudo-masks, which are further used togenerate inpainted images that complete the missing information in the originalimages. Our method improves the camera-only segmentation baseline by 2.63% inmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on theWaterscenes dataset. This demonstrates the effectiveness of our approach forsemantic segmentation using camera-radar fusion under adverse weatherconditions.</description>
      <author>example@mail.com (Huawei Sun, Bora Kunter Sahin, Georg Stettinger, Maximilian Bernhard, Matthias Schubert, Robert Wille)</author>
      <guid isPermaLink="false">2505.03679v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices</title>
      <link>http://arxiv.org/abs/2505.03303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 10 figures, 4 tables, submitted to Springer - Pattern  Recognition and Image Analysis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面评估了轻量级深度学习模型在图像分类中的应用，特别强调其在资源受限环境（如低内存设备）中的适用性。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的环境中，如低内存设备，对轻量级深度学习模型的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;评估轻量级深度学习模型在图像分类任务中的性能，并探究其在资源受限环境中的适用性。&lt;h4&gt;方法&lt;/h4&gt;对五种最先进的架构（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S和ShuffleNetV2）在三个不同数据集（CIFAR-10、CIFAR-100和Tiny ImageNet）上的表现进行了基准测试。使用四个关键性能指标进行评估：分类准确率、推理时间、浮点运算次数（FLOPs）和模型大小。此外，通过比较预训练模型与从头开始训练的模型，研究了超参数调整、数据增强和训练方法的影响，重点关注MobileNetV3 Small。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习显著提高了模型准确率和计算效率，尤其是在Tiny ImageNet等复杂数据集上。EfficientNetV2在准确率上表现最佳，MobileNetV3在准确率和效率之间提供了最佳平衡，SqueezeNet在推理速度和紧凑性方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;本研究的发现突出了准确性和效率之间的关键权衡，为在计算资源有限的现实世界应用中部署轻量级模型提供了有价值的见解。通过解决这些挑战，本研究有助于优化边缘计算和移动平台上的深度学习系统。&lt;h4&gt;翻译&lt;/h4&gt;本文提出对轻量级深度学习模型进行全面的评估，着重于其在资源受限环境（如低内存设备）中的适用性。五项最先进的架构——MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S和ShuffleNetV2——在三个不同的数据集（CIFAR-10、CIFAR-100和Tiny ImageNet）上进行了基准测试。使用四个关键性能指标进行评估：分类准确率、推理时间、浮点运算次数（FLOPs）和模型大小。此外，通过比较预训练模型与从头开始训练的模型，研究了超参数调整、数据增强和训练方法的影响，重点关注MobileNetV3 Small。我们的发现表明，迁移学习显著提高了模型准确率和计算效率，尤其是在Tiny ImageNet等复杂数据集上。EfficientNetV2在准确率上始终表现最佳，而MobileNetV3在准确率和效率之间提供了最佳平衡，SqueezeNet在推理速度和紧凑性方面表现突出。本研究强调了准确性和效率之间的关键权衡，为在计算资源有限的现实世界应用中部署轻量级模型提供了可操作的见解。通过解决这些挑战，本研究有助于优化边缘计算和移动平台上的深度学习系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a comprehensive evaluation of lightweight deep learningmodels for image classification, emphasizing their suitability for deploymentin resource-constrained environments such as low-memory devices. Fivestate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diversedatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed usingfour key performance metrics: classification accuracy, inference time,floating-point operations (FLOPs), and model size. Additionally, we investigatethe impact of hyperparameter tuning, data augmentation, and training paradigmsby comparing pretrained models with scratch-trained counterparts, focusing onMobileNetV3 Small. Our findings reveal that transfer learning significantlyenhances model accuracy and computational efficiency, particularly for complexdatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highestaccuracy, while MobileNetV3 offers the best balance between accuracy andefficiency, and SqueezeNet excels in inference speed and compactness. Thisstudy highlights critical trade-offs between accuracy and efficiency, offeringactionable insights for deploying lightweight models in real-world applicationswhere computational resources are limited. By addressing these challenges, thisresearch contributes to optimizing deep learning systems for edge computing andmobile platforms.</description>
      <author>example@mail.com (Tasnim Shahriar)</author>
      <guid isPermaLink="false">2505.03303v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fast Large Deformation Matching with the Energy Distance Kernel</title>
      <link>http://arxiv.org/abs/2505.03342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种高效的点云与度量注册框架，使用双Lipschitz同胚，达到O(n log n)的复杂度。&lt;h4&gt;背景&lt;/h4&gt;现有方法在处理点云和度量注册时，存在计算复杂度高和超参数调优困难的问题。&lt;h4&gt;目的&lt;/h4&gt;旨在解决点云和度量注册的复杂性和超参数调优问题。&lt;h4&gt;方法&lt;/h4&gt;使用Energy-Distance (ED)内核，并通过其一维切片投影近似，每个投影的计算复杂度为O(n log n)，从而避免了超参数调优并实现了高效的大规模优化。引入了两种模型来正则化变形，同时保持低计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;提出的第一种模型依赖于TV正则化，第二种模型通过限制其使用范围到测度空间或点云空间，避免了非光滑的TV正则化。&lt;h4&gt;结论&lt;/h4&gt;在合成数据和真实数据上展示了模型的数值鲁棒性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;We propose an efficient framework for point cloud and measure registration using bi-Lipschitz homeomorphisms, achieving O(n log n) complexity, where n is the number of points. By leveraging the Energy-Distance (ED) kernel, which can be approximated by its sliced one-dimensional projections, each computable in O(n log n), our method avoids hyperparameter tuning and enables efficient large-scale optimization. The main issue to be solved is the lack of regularity of the ED kernel. To this goal, we introduce two models that regularize the deformations and retain a low computational footprint. The first model relies on TV regularization, while the second model avoids the non-smooth TV regularization at the cost of restricting its use to the space of measures, or cloud of points. Last, we demonstrate the numerical robustness and scalability of our models on synthetic and real data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an efficient framework for point cloud and measure registrationusing bi-Lipschitz homeomorphisms, achieving O(n log n) complexity, where n isthe number of points. By leveraging the Energy-Distance (ED) kernel, which canbe approximated by its sliced one-dimensional projections, each computable inO(n log n), our method avoids hyperparameter tuning and enables efficientlarge-scale optimization. The main issue to be solved is the lack of regularityof the ED kernel. To this goal, we introduce two models that regularize thedeformations and retain a low computational footprint. The first model relieson TV regularization, while the second model avoids the non-smooth TVregularization at the cost of restricting its use to the space of measures, orcloud of points. Last, we demonstrate the numerical robustness and scalabilityof our models on synthetic and real data.</description>
      <author>example@mail.com (Siwan Boufadene, François-Xavier Vialard, Jean Feydy)</author>
      <guid isPermaLink="false">2505.03342v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fairness of Automatic Speech Recognition in Cleft Lip and Palate Speech</title>
      <link>http://arxiv.org/abs/2505.03697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Digital Signal Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了裂唇裂腭（CLP）患者产生的语音在自动语音识别（ASR）系统中的公平性问题，并通过实验证实了公共ASR系统对CLP语音的公平性存在降低。研究提出了通过增强策略来提高ASR对CLP语音的识别公平性的方法。&lt;h4&gt;背景&lt;/h4&gt;CLP患者的语音由于结构异常通常表现为高度鼻音和呼吸声，这影响了ASR的性能和公平性。&lt;h4&gt;目的&lt;/h4&gt;验证公共ASR系统对CLP语音的公平性，并提出通过增强策略来提高ASR对CLP语音的识别公平性。&lt;h4&gt;方法&lt;/h4&gt;研究系统地探索了在不同严重程度下用正常语音增强CLP语音的方法，并在AIISH和NMCPC数据集上测试了GMM-HMM、Whisper和XLS-R三种ASR模型。&lt;h4&gt;主要发现&lt;/h4&gt;增强CLP语音的方法在ASR中表现出了良好的效果，如GMM-HMM模型在AIISH数据集上的词错误率（WER）从22.64%下降到18.76%，Whisper在NMCPC数据集上的WER从28.45%下降到18.89%。GMM-HMM在AIISH数据集上表现优于XLS-R和Whisper，这可能是因为GMM-HMM更适合识别卡纳达语儿童的语音。通过引入公平性分数，研究显示在增强后，AIISH和NMCPC数据集的公平性分别提高了17.89%和47.50%。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，通过使用正常语音增强CLP语音可以显著提高ASR系统的公平性和识别准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech produced by individuals with cleft lip and palate (CLP) is oftenhighly nasalized and breathy due to structural anomalies, causing shifts informant structure that affect automatic speech recognition (ASR) performanceand fairness. This study hypothesizes that publicly available ASR systemsexhibit reduced fairness for CLP speech and confirms this through experiments.Despite formant disruptions, mild and moderate CLP speech retains somespectro-temporal alignment with normal speech, motivating augmentationstrategies to enhance fairness. The study systematically explores augmentingCLP speech with normal speech across severity levels and evaluates its impacton ASR fairness. Three ASR models-GMM-HMM, Whisper, and XLS-R-were tested onAIISH and NMCPC datasets. Results indicate that training with normal speech andtesting on mixed data improves word error rate (WER). Notably, WER decreasedfrom $22.64\%$ to $18.76\%$ (GMM-HMM, AIISH) and $28.45\%$ to $18.89\%$(Whisper, NMCPC). The superior performance of GMM-HMM on AIISH may be due toits suitability for Kannada children's speech, a challenge for foundationmodels like XLS-R and Whisper. To assess fairness, a fairness score wasintroduced, revealing improvements of $17.89\%$ (AIISH) and $47.50\%$ (NMCPC)with augmentation.</description>
      <author>example@mail.com (Susmita Bhattacharjee, Jagabandhu Mishra, H. S. Shekhawat, S. R. Mahadeva Prasanna)</author>
      <guid isPermaLink="false">2505.03697v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2505.03581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DyGEnc的新方法，用于编码动态图，旨在解决动态环境中事件分析的问题。&lt;h4&gt;背景&lt;/h4&gt;动态环境中事件分析对智能代理和机器人的开发构成了挑战，当前方法主要利用视觉模型，但往往缺乏可解释的空间时间对象表示。&lt;h4&gt;目的&lt;/h4&gt;DyGEnc的目的是通过集成压缩的空间时间结构观察表示和大型语言模型的认知能力，以实现基于文本场景图的先进问答。&lt;h4&gt;方法&lt;/h4&gt;DyGEnc结合了压缩的空间时间结构观察表示和大型语言模型的能力，并通过STAR和AGQA数据集上的扩展评估证明了其优越性。&lt;h4&gt;主要发现&lt;/h4&gt;DyGEnc在处理有关人类与对象交互历史的问题时，比现有视觉方法高15-25%。此外，该方法可以无缝扩展到使用基础模型提取文本场景图以处理原始输入图像。&lt;h4&gt;结论&lt;/h4&gt;DyGEnc有望为长时程推理实现鲁棒和压缩的基于图的机器人记忆系统做出贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要分析了在动态环境中对事件进行分析的挑战，主要介绍了DyGEnc这一新型动态图编码方法，强调了其基于文本场景图的问答能力，并通过实验证明了其性能优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The analysis of events in dynamic environments poses a fundamental challengein the development of intelligent agents and robots capable of interacting withhumans. Current approaches predominantly utilize visual models. However, thesemethods often capture information implicitly from images, lacking interpretablespatial-temporal object representations. To address this issue we introduceDyGEnc - a novel method for Encoding a Dynamic Graph. This method integratescompressed spatial-temporal structural observation representation with thecognitive capabilities of large language models. The purpose of thisintegration is to enable advanced question answering based on a sequence oftextual scene graphs. Extended evaluations on the STAR and AGQA datasetsindicate that DyGEnc outperforms existing visual methods by a large margin of15-25% in addressing queries regarding the history of human-to-objectinteractions. Furthermore, the proposed method can be seamlessly extended toprocess raw input images utilizing foundational models for extracting explicittextual scene graphs, as substantiated by the results of a robotic experimentconducted with a wheeled manipulator platform. We hope that these findings willcontribute to the implementation of robust and compressed graph-based roboticmemory for long-horizon reasoning. Code is available atgithub.com/linukc/DyGEnc.</description>
      <author>example@mail.com (Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin)</author>
      <guid isPermaLink="false">2505.03581v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges</title>
      <link>http://arxiv.org/abs/2505.03556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大型人工智能模型（LAMs）在通信领域的应用进行了全面回顾，包括其基础、应用和面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;6G无线通信旨在建立一个智能的、无处不在的连接世界，提供前所未有的通信体验。LAMs与典型的AI模型相比，具有显著更大的规模（例如，数十亿或数万亿的参数），并展现出出色的认知能力。&lt;h4&gt;目的&lt;/h4&gt;研究LAMs在通信中的应用，以解决未来无线通信系统中的复杂挑战。&lt;h4&gt;方法&lt;/h4&gt;介绍了基于AI的通信系统的当前状态，强调了将LAMs集成到通信中的动机，并总结了关键贡献。概述了LAMs在通信中的基本概念，包括主要架构、分类、训练方法和评估技术。还介绍了优化策略，如思维链（CoT）、检索增强生成（RAG）和代理系统。&lt;h4&gt;主要发现&lt;/h4&gt;LAMs在通信中具有强大的泛化能力和处理未在训练中看到的任务的能力，对于提供多样化的AI服务至关重要。&lt;h4&gt;结论&lt;/h4&gt;本文分析了LAMs在通信领域的应用进展，讨论了当前研究的挑战，并提供了未来研究方向的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：6G无线通信旨在建立一个智能的、无处不在的连接世界，提供前所未有的通信体验。大型人工智能模型（LAMs）与典型的AI模型相比，具有显著更大的规模（例如，数十亿或数万亿的参数），并展现出出色的认知能力，包括强大的泛化能力以微调下游任务，以及处理训练期间未见任务的涌现能力。因此，LAMs高效地为各种通信应用提供AI服务，成为解决未来无线通信系统中复杂挑战的关键工具。本研究对LAMs在通信中的基础、应用和挑战进行了全面回顾。首先，我们介绍了基于AI的通信系统的当前状态，强调了将LAMs集成到通信中的动机，并总结了关键贡献。然后，我们概述了LAMs在通信中的基本概念。这包括对LAMs主要架构的介绍，如transformer、扩散模型和mamba。我们还探讨了LAMs的分类，包括大型语言模型（LLMs）、大型视觉模型（LVMs）、大型多模态模型（LMMs）和世界模型，并考察了它们在通信中的潜在应用。此外，我们还涵盖了LAMs在通信系统中的训练方法和评估技术。最后，我们介绍了优化策略，如思维链（CoT）、检索增强生成（RAG）和代理系统。在此之后，我们讨论了LAMs在各种通信场景中的研究进展。最后，我们分析了当前研究的挑战，并提供了对未来研究方向的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6G wireless communications aim to establish an intelligent world ofubiquitous connectivity, providing an unprecedented communication experience.Large artificial intelligence models (LAMs) are characterized by significantlylarger scales (e.g., billions or trillions of parameters) compared to typicalartificial intelligence (AI) models. LAMs exhibit outstanding cognitiveabilities, including strong generalization capabilities for fine-tuning todownstream tasks, and emergent capabilities to handle tasks unseen duringtraining. Therefore, LAMs efficiently provide AI services for diversecommunication applications, making them crucial tools for addressing complexchallenges in future wireless communication systems. This study provides acomprehensive review of the foundations, applications, and challenges of LAMsin communication. First, we introduce the current state of AI-basedcommunication systems, emphasizing the motivation behind integrating LAMs intocommunications and summarizing the key contributions. We then present anoverview of the essential concepts of LAMs in communication. This includes anintroduction to the main architectures of LAMs, such as transformer, diffusionmodels, and mamba. We also explore the classification of LAMs, including largelanguage models (LLMs), large vision models (LVMs), large multimodal models(LMMs), and world models, and examine their potential applications incommunication. Additionally, we cover the training methods and evaluationtechniques for LAMs in communication systems. Lastly, we introduce optimizationstrategies such as chain of thought (CoT), retrieval augmented generation(RAG), and agentic systems. Following this, we discuss the researchadvancements of LAMs across various communication scenarios. Finally, weanalyze the challenges in the current research and provide insights intopotential future research directions.</description>
      <author>example@mail.com (Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Merouane Debbah, Dusit Niyato, Zhu Han)</author>
      <guid isPermaLink="false">2505.03556v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization</title>
      <link>http://arxiv.org/abs/2505.03186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了CoGenAV模型，一个用于学习跨多种语音和音频视觉任务的多功能表示的模型，通过优化来自自然音频视觉同步、对比特征对齐和生成文本预测的双目标，使用仅223小时标记数据从LRS2数据集进行训练，有效提高了多种语音处理任务的表现。&lt;h4&gt;背景&lt;/h4&gt;语音处理在传统音频-only系统在挑战性条件下失效时面临困难，而说话者的唇部动作、声音和底层语言内容之间的内在同步为改进语音处理任务提供了丰富的信息来源。&lt;h4&gt;目的&lt;/h4&gt;开发一个数据高效的模型，能够学习通用的音频视觉表示，适用于广泛的语音和音频视觉任务。&lt;h4&gt;方法&lt;/h4&gt;CoGenAV通过优化双目标进行训练，双目标来源于自然音频视觉同步、对比特征对齐和生成文本预测，使用LRS2数据集的223小时标记数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;CoGenAV在多个基准测试中展示了其有效性和多功能性，在LRS2数据集上应用于音频视觉语音识别（AVSR）时，实现了最先进的Word Error Rate（WER）为1.27，在视觉语音识别（VSR）上也表现出色，WER为22.0，在嘈杂环境中的性能提高了超过70%，并且对语音重建任务也有益处，提高了语音增强和分离的性能，在音频视觉同步任务如主动说话者检测（ASD）中也取得了有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;CoGenAV模型将被开源，以促进学术界和工业界进一步的开发和合作。&lt;h4&gt;翻译&lt;/h4&gt;摘要：说话者的唇部动作、声音和底层语言内容之间的内在同步为改进语音处理任务提供了丰富的信息来源，特别是在传统音频-only系统在挑战性条件下失效的情况下。我们介绍了CoGenAV，一个强大且数据高效的模型，旨在学习通用的音频视觉表示，适用于广泛的语音和音频视觉任务。CoGenAV通过优化来自自然音频视觉同步、对比特征对齐和生成文本预测的双目标进行训练，仅使用LRS2数据集的223小时标记数据。这种对比生成同步策略有效地捕捉了跨模态的基本相关性。我们在多个基准测试中展示了所学习的CoGenAV表示的有效性和多功能性。当在LRS2上用于音频视觉语音识别（AVSR）时，这些表示有助于实现最先进的Word Error Rate（WER）为1.27。它们还在视觉语音识别（VSR）上表现出色，LRS2上的WER为22.0，并且通过超过70%的提高在嘈杂环境中显著提高了性能。此外，CoGenAV表示对语音重建任务也有益处，提高了语音增强和分离的性能，并在音频视觉同步任务如主动说话者检测（ASD）中取得了有竞争力的结果。我们的模型将被开源，以促进学术界和工业界进一步的开发和合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The inherent synchronization between a speaker's lip movements, voice, andthe underlying linguistic content offers a rich source of information forimproving speech processing tasks, especially in challenging conditions wheretraditional audio-only systems falter. We introduce CoGenAV, a powerful anddata-efficient model designed to learn versatile audio-visual representationsapplicable across a wide range of speech and audio-visual tasks. CoGenAV istrained by optimizing a dual objective derived from natural audio-visualsynchrony, contrastive feature alignment and generative text prediction, usingonly 223 hours of labeled data from the LRS2 dataset. Thiscontrastive-generative synchronization strategy effectively capturesfundamental cross-modal correlations. We showcase the effectiveness andversatility of the learned CoGenAV representations on multiple benchmarks. Whenutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, theserepresentations contribute to achieving a state-of-the-art Word Error Rate(WER) of 1.27. They also enable strong performance in Visual Speech Recognition(VSR) with a WER of 22.0 on LRS2, and significantly improve performance innoisy environments by over 70%. Furthermore, CoGenAV representations benefitspeech reconstruction tasks, boosting performance in Speech Enhancement andSeparation, and achieve competitive results in audio-visual synchronizationtasks like Active Speaker Detection (ASD). Our model will be open-sourced tofacilitate further development and collaboration within both academia andindustry.</description>
      <author>example@mail.com (Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo)</author>
      <guid isPermaLink="false">2505.03186v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs</title>
      <link>http://arxiv.org/abs/2505.03460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态大型语言模型（MLLMs）的物流无人机（UAV）递送系统，旨在解决精细粒度末端递送的需求。&lt;h4&gt;背景&lt;/h4&gt;随着智能物流需求的增长，特别是精细粒度末端递送的需求，需要自主无人机递送系统。然而，现有研究主要依赖于地面机器人，而基于视觉-语言导航（VLN）的任务主要关注粗粒度、长距离目标，不适合精确的末端递送。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一差距，本文提出了LogisticsVLN系统，一个可扩展的空中递送系统，用于自主终端递送。&lt;h4&gt;方法&lt;/h4&gt;LogisticsVLN系统集成了轻量级大型语言模型（LLMs）和视觉-语言模型（VLMs），在一个模块化管道中用于请求理解、楼层定位、目标检测和动作决策。为了支持这一新环境的研究和评估，构建了Vision-Language Delivery（VLD）数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在VLD数据集上的实验结果表明LogisticsVLN系统的可行性。此外，对系统每个模块进行了子任务级别的评估，为提高基于基础模型视觉-语言递送系统的鲁棒性和实际部署提供了宝贵见解。&lt;h4&gt;结论&lt;/h4&gt;LogisticsVLN系统为智能物流的精细粒度末端递送提供了一种可行的解决方案，并通过实验验证了其有效性和可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing demand for intelligent logistics, particularly fine-grainedterminal delivery, underscores the need for autonomous UAV (Unmanned AerialVehicle)-based delivery systems. However, most existing last-mile deliverystudies rely on ground robots, while current UAV-based Vision-LanguageNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,making them unsuitable for precise terminal delivery. To bridge this gap, wepropose LogisticsVLN, a scalable aerial delivery system built on multimodallarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLNintegrates lightweight Large Language Models (LLMs) and Visual-Language Models(VLMs) in a modular pipeline for request understanding, floor localization,object detection, and action-decision making. To support research andevaluation in this new setting, we construct the Vision-Language Delivery (VLD)dataset within the CARLA simulator. Experimental results on the VLD datasetshowcase the feasibility of the LogisticsVLN system. In addition, we conductsubtask-level evaluations of each module of our system, offering valuableinsights for improving the robustness and real-world deployment of foundationmodel-based vision-language delivery systems.</description>
      <author>example@mail.com (Xinyuan Zhang, Yonglin Tian, Fei Lin, Yue Liu, Jing Ma, Kornélia Sára Szatmáry, Fei-Yue Wang)</author>
      <guid isPermaLink="false">2505.03460v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds</title>
      <link>http://arxiv.org/abs/2505.02972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个名为GeoERM的几何感知多任务学习框架，通过将共享表示嵌入其自然的黎曼流形上并利用显式流形操作来优化它，以提升统计能力和学习效率。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务学习（MTL）方法通常将潜在表示矩阵视为普通欧几里得空间中的一个点，忽略了它往往具有非欧几里得几何，因此在任务异质或对抗性时鲁棒性不足。&lt;h4&gt;目的&lt;/h4&gt;提升统计能力和学习效率，提高多任务学习的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;GeoERM框架通过在每个训练周期进行（1）遵循搜索空间内在曲率的黎曼梯度步，随后（2）高效的极向收缩以保持在流形上，保证了每一步的几何保真度。该方法适用于广泛的矩阵分解MTL模型，且每次迭代的成本与欧几里得基线相同。&lt;h4&gt;主要发现&lt;/h4&gt;在合成实验和可穿戴传感器活动识别基准测试中，GeoERM一致性地提高了估计准确性，减少了负面迁移，并且在对抗性标签噪声下保持稳定，优于现有的MTL和单任务方法。&lt;h4&gt;结论&lt;/h4&gt;GeoERM是一个有效且鲁棒的多任务学习方法，通过利用几何结构提升了学习效率并降低了负面迁移和噪声的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Task Learning (MTL) seeks to boost statistical power and learningefficiency by discovering structure shared across related tasks.State-of-the-art MTL representation methods, however, usually treat the latentrepresentation matrix as a point in ordinary Euclidean space, ignoring itsoften non-Euclidean geometry, thus sacrificing robustness when tasks areheterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTLframework that embeds the shared representation on its natural Riemannianmanifold and optimizes it via explicit manifold operations. Each training cycleperforms (i) a Riemannian gradient step that respects the intrinsic curvatureof the search space, followed by (ii) an efficient polar retraction to remainon the manifold, guaranteeing geometric fidelity at every iteration. Theprocedure applies to a broad class of matrix-factorized MTL models and retainsthe same per-iteration cost as Euclidean baselines. Across a set of syntheticexperiments with task heterogeneity and on a wearable-sensoractivity-recognition benchmark, GeoERM consistently improves estimationaccuracy, reduces negative transfer, and remains stable under adversarial labelnoise, outperforming leading MTL and single-task alternatives.</description>
      <author>example@mail.com (Aoran Chen, Yang Feng)</author>
      <guid isPermaLink="false">2505.02972v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.00507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in CVPRw2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HeAL的启发式增强主动学习算法，用于3D物体检测，通过结合启发式特征、定位和分类，选择对模型训练最有贡献的样本。&lt;h4&gt;背景&lt;/h4&gt;主动学习在自动驾驶模型训练的样本选择中得到了应用，但在3D物体检测中，未控制场景下的样本选择具有挑战性，且现有方法主要关注理论方面，忽视了实际应用中的洞察。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的主动学习方法，以解决3D物体检测中样本选择的问题，并提高检测模型的性能。&lt;h4&gt;方法&lt;/h4&gt;HeAL算法通过整合启发式特征（如物体距离和点数）来估计不确定性，从而增强所选样本对训练检测模型的有用性。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上的定量评估显示，HeAL在mAP（平均精度）方面与现有最佳方法具有竞争力，并且只使用24%的样本就达到了与全监督基线相同的mAP。&lt;h4&gt;结论&lt;/h4&gt;HeAL算法在3D物体检测中通过提高样本选择的质量，显著提升了模型的性能，同时减少了训练所需的样本数量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Active Learning has proved to be a relevant approach to perform sampleselection for training models for Autonomous Driving. Particularly, previousworks on active learning for 3D object detection have shown that selection ofsamples in uncontrolled scenarios is challenging. Furthermore, currentapproaches focus exclusively on the theoretical aspects of the sample selectionproblem but neglect the practical insights that can be obtained from theextensive literature and application of 3D detection models. In this paper, weintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)which integrates those heuristical features together with Localization andClassification to deliver the most contributing samples to the model'straining. In contrast to previous works, our approach integrates heuristicalfeatures such as object distance and point-quantity to estimate theuncertainty, which enhance the usefulness of selected samples to traindetection models. Our quantitative evaluation on KITTI shows that HeAL presentscompetitive mAP with respect to the State-of-the-Art, and achieves the same mAPas the full-supervised baseline with only 24% of the samples.</description>
      <author>example@mail.com (Esteban Rivera, Surya Prabhakaran, Markus Lienkamp)</author>
      <guid isPermaLink="false">2505.00507v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.03300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IV2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的3D语义分割流程，用于自动驾驶和基础设施管理，该流程通过监督学习实现，避免了直接3D标注或依赖其他模态（如相机图像）的需求。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR点云的语义分割对于自动驾驶和基础设施管理至关重要，但需要大量的标注数据集并面临领域迁移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D语义分割方法，无需直接3D标注或依赖其他模态，并用于伪标签生成。&lt;h4&gt;方法&lt;/h4&gt;该流程利用对齐的场景和最先进的2D分割方法，从LiDAR扫描中生成2D视图，并使用相机域预训练模型对这些视图进行2D语义分割。然后，通过简单的基于投票的估计器将分割的2D输出回投影到3D点上。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出了一个全局的3D语义分割流程，无需先前的3D标注或其他模态进行推理，并展示了生成的伪标签在无监督领域自适应任务中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法为3D语义分割提供了一种新的解决方案，适用于自动驾驶和基础设施管理等领域，且无需大量3D标注数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV55156.2024.10588443&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of 3D LiDAR point clouds, essential for autonomousdriving and infrastructure management, is best achieved by supervised learning,which demands extensive annotated datasets and faces the problem of domainshifts. We introduce a new 3D semantic segmentation pipeline that leveragesaligned scenes and state-of-the-art 2D segmentation methods, avoiding the needfor direct 3D annotation or reliance on additional modalities such as cameraimages at inference time. Our approach generates 2D views from LiDAR scanscolored by sensor intensity and applies 2D semantic segmentation to these viewsusing a camera-domain pretrained model. The segmented 2D outputs are thenback-projected onto the 3D points, with a simple voting-based estimator thatmerges the labels associated to each 3D point. Our main contribution is aglobal pipeline for 3D semantic segmentation requiring no prior 3D annotationand not other modality for inference, which can be used for pseudo-labelgeneration. We conduct a thorough ablation study and demonstrate the potentialof the generated pseudo-labels for the Unsupervised Domain Adaptation task.</description>
      <author>example@mail.com (Andrew Caunes, Thierry Chateau, Vincent Frémont)</author>
      <guid isPermaLink="false">2505.03300v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2505.03406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）在医疗保健领域的应用，特别是通过整合医院特定数据和量化低秩适应（QLoRA）微调的检索增强生成（RAG）来增强医疗决策支持。&lt;h4&gt;背景&lt;/h4&gt;医疗决策支持需要准确的信息检索和生成，而LLMs在处理自然语言方面具有优势。&lt;h4&gt;目的&lt;/h4&gt;提高医疗决策的准确性和效率，通过使用LLMs和RAG技术。&lt;h4&gt;方法&lt;/h4&gt;使用Llama 3.2-3B-Instruct作为基础模型，通过嵌入和检索与医疗保健相关的信息，并使用QLoRA进行参数效率和内存优化。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在多个医疗基准测试中表现良好，可以用于提供基本的医疗建议。系统还支持疾病预测、治疗建议和复杂医疗报告的总结。&lt;h4&gt;结论&lt;/h4&gt;LLMs在医疗保健领域具有广泛的应用潜力，但需要考虑患者隐私、数据安全和临床验证等伦理和实际挑战。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了大型语言模型（LLMs）在医疗保健中的应用，特别是通过整合医院特定数据和量化低秩适应（QLoRA）微调的检索增强生成（RAG）来增强医疗决策支持。该系统以Llama 3.2-3B-Instruct作为其基础模型。通过嵌入和检索与医疗保健相关的上下文信息，该系统显著提高了响应准确性。QLoRA促进了参数效率和内存优化，通过专门的量化技术保留了医疗信息的完整性。我们的研究还表明，我们的模型在各种医疗基准测试中表现相对良好，表明它可以用于提供基本的医疗建议。本文详细介绍了系统的技术组件，包括其架构、量化方法和关键医疗保健应用，如从患者症状和病史中增强疾病预测、治疗建议和复杂医疗报告的有效总结。我们触及了伦理考虑——患者隐私、数据安全以及严格临床验证的需求——以及将此类系统整合到现实世界医疗工作流程中的实际挑战。此外，轻量级的量化权重确保了可扩展性和部署的简便性，即使在资源有限的医院环境中也是如此。最后，本文分析了LLMs对医疗保健的更广泛影响，并概述了LLMs在医疗环境中的未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research paper investigates the application of Large Language Models(LLMs) in healthcare, specifically focusing on enhancing medical decisionsupport through Retrieval-Augmented Generation (RAG) integrated withhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. Byembedding and retrieving context-relevant healthcare information, the systemsignificantly improves response accuracy. QLoRA facilitates notable parameterefficiency and memory optimization, preserving the integrity of medicalinformation through specialized quantization techniques. Our research alsoshows that our model performs relatively well on various medical benchmarks,indicating that it can be used to make basic medical suggestions. This paperdetails the system's technical components, including its architecture,quantization methods, and key healthcare applications such as enhanced diseaseprediction from patient symptoms and medical history, treatment suggestions,and efficient summarization of complex medical reports. We touch on the ethicalconsiderations-patient privacy, data security, and the need for rigorousclinical validation-as well as the practical challenges of integrating suchsystems into real-world healthcare workflows. Furthermore, the lightweightquantized weights ensure scalability and ease of deployment even inlow-resource hospital environments. Finally, the paper concludes with ananalysis of the broader impact of LLMs on healthcare and outlines futuredirections for LLMs in medical settings.</description>
      <author>example@mail.com (Mohammad Shoaib Ansari, Mohd Sohail Ali Khan, Shubham Revankar, Aditya Varma, Anil S. Mokhade)</author>
      <guid isPermaLink="false">2505.03406v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera</title>
      <link>http://arxiv.org/abs/2505.03093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用消费级360度视频摄像头进行森林资源测量的低成本替代方案。&lt;h4&gt;背景&lt;/h4&gt;森林资源调查依赖于胸径（DBH）的准确测量，而基于LiDAR的技术虽然精度高，但成本高且操作复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种低成本、易于操作的替代方法来测量DBH。&lt;h4&gt;方法&lt;/h4&gt;开发了一个半自动化的流程，包括使用Agisoft Metashape软件进行密集点云重建，通过将SAM掩模投影到3D云上执行语义树干分割，以及使用基于RANSAC的稳健技术来估计横截面形状和DBH。&lt;h4&gt;主要发现&lt;/h4&gt;在61个不同条件下的43棵树的测量中，该方法相对于人工测量的“真实值”的中值绝对相对误差为5-9%，仅比基于LiDAR的估计高出2-4%，同时使用成本低得多的单台360度摄像头，设置简单，且易于获取。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种经济高效的DBH测量方法，适用于森林资源调查和碳核算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forest inventories rely on accurate measurements of the diameter at breastheight (DBH) for ecological monitoring, resource management, and carbonaccounting. While LiDAR-based techniques can achieve centimeter-levelprecision, they are cost-prohibitive and operationally complex. We present alow-cost alternative that only needs a consumer-grade 360 video camera. Oursemi-automated pipeline comprises of (i) a dense point cloud reconstructionusing Structure from Motion (SfM) photogrammetry software called AgisoftMetashape, (ii) semantic trunk segmentation by projecting Grounded SegmentAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-basedtechnique to estimate cross section shape and DBH. We introduce an interactivevisualization tool for inspecting segmented trees and their estimated DBH. On61 acquisitions of 43 trees under a variety of conditions, our method attainsmedian absolute relative errors of 5-9% with respect to "ground-truth" manualmeasurements. This is only 2-4% higher than LiDAR-based estimates, whileemploying a single 360 camera that costs orders of magnitude less, requiresminimal setup, and is widely available.</description>
      <author>example@mail.com (Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2505.03093v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Early Prediction of Sepsis: Feature-Aligned Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.02889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A project implemented for MACHINE LEARNING IN HEALTH AND BIOMEDICAL  SCIENCE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为特征对齐迁移学习（FATL）的方法，旨在通过机器学习预测早期脓毒症，以帮助医疗提供者更早地进行干预，从而改善患者预后、降低医疗成本并支持更公平的医疗保健服务。&lt;h4&gt;背景&lt;/h4&gt;脓毒症是一种危及生命的疾病，当身体对感染产生极端反应时，会导致全身炎症、器官衰竭甚至死亡。由于脓毒症进展迅速，早期检测至关重要。然而，现有的诊断方法往往在严重损害发生后才能识别脓毒症。&lt;h4&gt;目的&lt;/h4&gt;本项目旨在通过开发一个基于机器学习的系统来预测早期脓毒症，为医疗提供者提供更多干预时间。&lt;h4&gt;方法&lt;/h4&gt;论文提出的方法FATL通过识别和关注多个研究中最重要的和最常报告的特征来解决现有模型中患者信息或特征变异大的问题，确保模型的一致性和临床相关性。FATL通过结合来自不同人群的模型的知识的加权方法来解决训练在狭窄患者群体上的模型导致的群体偏差问题。&lt;h4&gt;主要发现&lt;/h4&gt;FATL提供了一种实用且可扩展的早期脓毒症检测解决方案，尤其适用于资源有限的医院，并有可能改善患者预后、降低医疗成本并支持更公平的医疗保健服务。&lt;h4&gt;结论&lt;/h4&gt;FATL方法能够提高脓毒症早期检测的准确性和有效性，有助于改善患者治疗结果，降低医疗成本，并促进医疗服务的公平性。&lt;h4&gt;翻译&lt;/h4&gt;Sepsis is a life-threatening medical condition that occurs when the body has an extreme response to infection, leading to widespread inflammation, organ failure, and potentially death. Because sepsis can worsen rapidly, early detection is critical to saving lives. However, current diagnostic methods often identify sepsis only after significant damage has already occurred. Our project aims to address this challenge by developing a machine learning based system to predict sepsis in its early stages, giving healthcare providers more time to intervene. A major problem with existing models is the wide variability in the patient information or features they use, such as heart rate, temperature, and lab results. This inconsistency makes models difficult to compare and limits their ability to work across different hospitals and settings. To solve this, we propose a method called Feature Aligned Transfer Learning (FATL), which identifies and focuses on the most important and commonly reported features across multiple studies, ensuring the model remains consistent and clinically relevant. Most existing models are trained on narrow patient groups, leading to population bias. FATL addresses this by combining knowledge from models trained on diverse populations, using a weighted approach that reflects each model's contribution. This makes the system more generalizable and effective across different patient demographics and clinical environments. FATL offers a practical and scalable solution for early sepsis detection, particularly in hospitals with limited resources, and has the potential to improve patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sepsis is a life threatening medical condition that occurs when the body hasan extreme response to infection, leading to widespread inflammation, organfailure, and potentially death. Because sepsis can worsen rapidly, earlydetection is critical to saving lives. However, current diagnostic methodsoften identify sepsis only after significant damage has already occurred. Ourproject aims to address this challenge by developing a machine learning basedsystem to predict sepsis in its early stages, giving healthcare providers moretime to intervene.  A major problem with existing models is the wide variability in the patientinformation or features they use, such as heart rate, temperature, and labresults. This inconsistency makes models difficult to compare and limits theirability to work across different hospitals and settings. To solve this, wepropose a method called Feature Aligned Transfer Learning (FATL), whichidentifies and focuses on the most important and commonly reported featuresacross multiple studies, ensuring the model remains consistent and clinicallyrelevant.  Most existing models are trained on narrow patient groups, leading topopulation bias. FATL addresses this by combining knowledge from models trainedon diverse populations, using a weighted approach that reflects each modelscontribution. This makes the system more generalizable and effective acrossdifferent patient demographics and clinical environments. FATL offers apractical and scalable solution for early sepsis detection, particularly inhospitals with limited resources, and has the potential to improve patientoutcomes, reduce healthcare costs, and support more equitable healthcaredelivery.</description>
      <author>example@mail.com (Oyindolapo O. Komolafe, Zhimin Mei, David Morales Zarate, Gregory William Spangenberg)</author>
      <guid isPermaLink="false">2505.02889v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights</title>
      <link>http://arxiv.org/abs/2505.02845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了浮动车观测器（FCO）在微观交通模拟中的应用，通过集成车载传感器检测和定位其他交通参与者，提供更丰富、更详细的交通数据。通过仿真实验，探讨了不同建模方法的效果，并引入了一种基于神经网络的仿真技术，以评估FCO在智能交通系统（ITS）中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;浮动车观测器（FCO）通过集成车载传感器，扩展了传统的浮动车数据（FCD），能够检测和定位其他交通参与者，提供更详细的交通数据。&lt;h4&gt;目的&lt;/h4&gt;探索各种建模方法，以评估FCO在微观交通模拟中的应用潜力，特别是在智能交通系统（ITS）中的应用。&lt;h4&gt;方法&lt;/h4&gt;采用了从二维光线追踪到高保真协同模拟的多种建模方法，后者模拟真实世界传感器并集成3D目标检测算法，以更精确地复制FCO的检测过程。此外，还引入了一种基于神经网络的仿真技术，以近似高保真协同模拟的结果。&lt;h4&gt;主要发现&lt;/h4&gt;即使在20%的渗透率下，基于LiDAR检测的FCO能够在各种交叉口和交通需求场景中识别65%的车辆。通过整合时间洞察，可以恢复之前检测但现在不可见的车辆，数据驱动方法能够恢复超过80%的这些车辆，位置偏差最小。&lt;h4&gt;结论&lt;/h4&gt;FCO在ITS中具有巨大潜力，尤其是在增强交通状态估计和监测方面，尤其是在不同渗透率和交通条件下。&lt;h4&gt;翻译&lt;/h4&gt;This paper investigates the application of Floating Car Observers (FCO) in microscopic traffic simulations, which integrate onboard sensors to detect and localize other traffic participants, providing richer and more detailed traffic data. Through simulation experiments, various modeling methods are explored to evaluate their potential for Intelligent Transportation System (ITS) applications. These methods range from 2D raytracing to high-fidelity co-simulations that emulate real-world sensors and integrate 3D object detection algorithms to closely replicate FCO detections. Additionally, a neural network-based emulation technique is introduced to effectively approximate the results of high-fidelity co-simulations. This approach captures the unique characteristics of FCO detections while offering a fast and scalable solution for modeling. Using this emulation method, the impact of FCO data in a digital twin of a traffic network modeled in SUMO is investigated. Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based detections can identify 65% of vehicles across various intersections and traffic demand scenarios. Further potential emerges when temporal insights are integrated, enabling the recovery of previously detected but currently unseen vehicles. By employing data-driven methods, over 80% of these vehicles are recovered with minimal positional deviations. These findings underscore the potential of FCOs for ITS, particularly in enhancing traffic state estimation and monitoring under varying penetration rates and traffic conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) byintegrating onboard sensors to detect and localize other traffic participants,providing richer and more detailed traffic data. In this work, we explorevarious modeling approaches for FCO detections within microscopic trafficsimulations to evaluate their potential for Intelligent Transportation System(ITS) applications. These approaches range from 2D raytracing to high-fidelityco-simulations that emulate real-world sensors and integrate 3D objectdetection algorithms to closely replicate FCO detections. Additionally, weintroduce a neural network-based emulation technique that effectivelyapproximates the results of high-fidelity co-simulations. This approachcaptures the unique characteristics of FCO detections while offering a fast andscalable solution for modeling. Using this emulation method, we investigate theimpact of FCO data in a digital twin of a traffic network modeled in SUMO.Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-baseddetections can identify 65% of vehicles across various intersections andtraffic demand scenarios. Further potential emerges when temporal insights areintegrated, enabling the recovery of previously detected but currently unseenvehicles. By employing data-driven methods, we recover over 80% of thesevehicles with minimal positional deviations. These findings underscore thepotential of FCOs for ITS, particularly in enhancing traffic state estimationand monitoring under varying penetration rates and traffic conditions.</description>
      <author>example@mail.com (Jeremias Gerner, Klaus Bogenberger, Stefanie Schmidtner)</author>
      <guid isPermaLink="false">2505.02845v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection</title>
      <link>http://arxiv.org/abs/2505.03359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMBC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用基于语音的AI模型检测抑郁症和创伤后应激障碍（PTSD）的方法，并提出了针对性别偏见问题的解决方案。&lt;h4&gt;背景&lt;/h4&gt;基于语音的AI模型在心理健康评估中显示出潜力，但存在性别偏见问题，可能导致不公平和不准确的预测。&lt;h4&gt;目的&lt;/h4&gt;通过引入领域对抗训练方法，考虑性别差异，提高基于语音的抑郁症和PTSD检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;将不同性别视为不同的领域，并将这些信息整合到预训练的语音基础模型中，并在E-DAIC数据集上验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著提高了检测性能，与基线相比，F1分数提高了高达13.29个百分点。&lt;h4&gt;结论&lt;/h4&gt;解决AI驱动的心理健康评估中的性别差异问题至关重要。&lt;h4&gt;翻译&lt;/h4&gt;Speech-based AI models are emerging as powerful tools for detecting depression and the presence of Post-traumatic stress disorder (PTSD), offering a non-invasive and cost-effective way to assess mental health. However, these models often struggle with gender bias, which can lead to unfair and inaccurate predictions. In this study, our study addresses this issue by introducing a domain adversarial training approach that explicitly considers gender differences in speech-based depression and PTSD detection. Specifically, we treat different genders as distinct domains and integrate this information into a pretrained speech foundation model. We then validate its effectiveness on the E-DAIC dataset to assess its impact on performance. Experimental results show that our method notably improves detection performance, increasing the F1-score by up to 13.29 percentage points compared to the baseline. This highlights the importance of addressing demographic disparities in AI-driven mental health assessment.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech-based AI models are emerging as powerful tools for detectingdepression and the presence of Post-traumatic stress disorder (PTSD), offeringa non-invasive and cost-effective way to assess mental health. However, thesemodels often struggle with gender bias, which can lead to unfair and inaccuratepredictions. In this study, our study addresses this issue by introducing adomain adversarial training approach that explicitly considers genderdifferences in speech-based depression and PTSD detection. Specifically, wetreat different genders as distinct domains and integrate this information intoa pretrained speech foundation model. We then validate its effectiveness on theE-DAIC dataset to assess its impact on performance. Experimental results showthat our method notably improves detection performance, increasing the F1-scoreby up to 13.29 percentage points compared to the baseline. This highlights theimportance of addressing demographic disparities in AI-driven mental healthassessment.</description>
      <author>example@mail.com (June-Woo Kim, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang)</author>
      <guid isPermaLink="false">2505.03359v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</title>
      <link>http://arxiv.org/abs/2505.03233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用大规模合成动作数据训练视觉-语言-动作模型的可能性，并提出了一种名为GraspVLA的模型，该模型在合成动作数据上进行了预训练，以促进抓取任务的零样本泛化和小样本适应性。&lt;h4&gt;背景&lt;/h4&gt;实体基础模型因其零样本泛化、可扩展性和通过少量后训练适应新任务的能力而受到越来越多的关注。然而，现有模型严重依赖真实世界数据，收集成本高且劳动密集。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一差距，研究旨在探索使用大规模合成动作数据完全训练视觉-语言-动作模型的可行性。&lt;h4&gt;方法&lt;/h4&gt;研究人员创建了SynGrasp-1B，这是一个包含十亿帧机器人抓取数据的集，通过模拟生成，具有逼真的渲染和广泛的领域随机化。基于此，他们提出了GraspVLA模型，该模型在大型合成动作数据上进行了预训练，并将自回归感知任务和基于流匹配的动作生成整合到一个统一的思维链过程中，以实现合成动作数据和互联网语义数据的联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;GraspVLA在真实世界和模拟基准测试中表现出先进的零样本泛化能力和小样本适应性，能够将学习到的动作转移到更广泛的互联网覆盖对象上，实现了抓取任务的开放词汇泛化。&lt;h4&gt;结论&lt;/h4&gt;研究证明了使用合成数据训练视觉-语言-动作模型的可行性，并提出了一个能够实现零样本泛化和小样本适应性的模型，这将对机器人抓取任务的研究和应用产生积极影响。&lt;h4&gt;翻译&lt;/h4&gt;Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied foundation models are gaining increasing attention for theirzero-shot generalization, scalability, and adaptability to new tasks throughfew-shot post-training. However, existing models rely heavily on real-worlddata, which is costly and labor-intensive to collect. Synthetic data offers acost-effective alternative, yet its potential remains largely underexplored. Tobridge this gap, we explore the feasibility of training Vision-Language-Actionmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,a billion-frame robotic grasping dataset generated in simulation withphotorealistic rendering and extensive domain randomization. Building on this,we present GraspVLA, a VLA model pretrained on large-scale synthetic actiondata as a foundational model for grasping tasks. GraspVLA integratesautoregressive perception tasks and flow-matching-based action generation intoa unified Chain-of-Thought process, enabling joint training on synthetic actiondata and Internet semantics data. This design helps mitigate sim-to-real gapsand facilitates the transfer of learned actions to a broader range ofInternet-covered objects, achieving open-vocabulary generalization in grasping.Extensive evaluations across real-world and simulation benchmarks demonstrateGraspVLA's advanced zero-shot generalizability and few-shot adaptability tospecific human preferences. We will release SynGrasp-1B dataset and pre-trainedweights to benefit the community.</description>
      <author>example@mail.com (Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2505.03233v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis</title>
      <link>http://arxiv.org/abs/2505.03132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了VISLIX，一个用于视觉分析的新框架，旨在帮助领域专家分析计算机视觉模型中的数据切片，以支持机器学习操作的生命周期。&lt;h4&gt;背景&lt;/h4&gt;在实际部署机器学习模型之前，特别是在自动驾驶和监控等安全关键领域，需要对模型进行严格的评估。数据切片是评估机器学习模型常用的方法，但存在一些挑战。&lt;h4&gt;目的&lt;/h4&gt;克服数据切片在视觉模型验证中的挑战，并支持机器学习操作的生命周期。&lt;h4&gt;方法&lt;/h4&gt;VISLIX使用最先进的基座模型，无需图像元数据或视觉概念，自动生成自然语言洞察，并允许用户交互式地测试数据切片假设。&lt;h4&gt;主要发现&lt;/h4&gt;VISLIX通过专家研究和三个用例评估，证明了该工具在为验证目标检测模型提供全面洞察方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;VISLIX是一个有效的工具，可以帮助领域专家在计算机视觉模型中分析数据切片，从而提高机器学习模型在安全关键领域的评估效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world machine learning models require rigorous evaluation beforedeployment, especially in safety-critical domains like autonomous driving andsurveillance. The evaluation of machine learning models often focuses on dataslices, which are subsets of the data that share a set of characteristics. Dataslice finding automatically identifies conditions or data subgroups wheremodels underperform, aiding developers in mitigating performance issues.Despite its popularity and effectiveness, data slicing for vision modelvalidation faces several challenges. First, data slicing often needs additionalimage metadata or visual concepts, and falls short in certain computer visiontasks, such as object detection. Second, understanding data slices is alabor-intensive and mentally demanding process that heavily relies on theexpert's domain knowledge. Third, data slicing lacks a human-in-the-loopsolution that allows experts to form hypothesis and test them interactively. Toovercome these limitations and better support the machine learning operationslifecycle, we introduce VISLIX, a novel visual analytics framework that employsstate-of-the-art foundation models to help domain experts analyze slices incomputer vision models. Our approach does not require image metadata or visualconcepts, automatically generates natural language insights, and allows usersto test data slice hypothesis interactively. We evaluate VISLIX with an expertstudy and three use cases, that demonstrate the effectiveness of our tool inproviding comprehensive insights for validating object detection models.</description>
      <author>example@mail.com (Xinyuan Yan, Xiwei Xuan, Jorge Piazentin Ono, Jiajing Guo, Vikram Mohanty, Shekar Arvind Kumar, Liang Gou, Bei Wang, Liu Ren)</author>
      <guid isPermaLink="false">2505.03132v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion</title>
      <link>http://arxiv.org/abs/2505.03118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种自适应阈值机制的多标签分类方法，该方法在重类别不平衡和噪声条件下预测多个标签，并实现了在AmazonCat-13K基准测试中的显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;多标签分类（MLC）通常需要在重类别不平衡和噪声条件下预测多个标签，传统方法忽略了上下文和全局稀有性。&lt;h4&gt;目的&lt;/h4&gt;提高多标签分类在重类别不平衡和噪声条件下的预测准确率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种自适应阈值机制，融合了全局（基于IDF）和局部（基于KNN）信号来产生每个标签、每个实例的阈值。这些阈值被处理为损失函数中的可微分惩罚，提供平滑的监督和更好的校准。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在AmazonCat-13K基准测试中实现了0.1712的宏观F1值，显著优于基于树和预训练转换器的传统方法。&lt;h4&gt;结论&lt;/h4&gt;该方法具有轻量级、可解释性和高度模块化等特点，并且代码已公开发布以确保可重复性和未来的扩展。&lt;h4&gt;翻译&lt;/h4&gt;We introduce an adaptive thresholding mechanism for multi-label classification (MLC) that predicts multiple labels per sample, often under heavy class imbalance and noisy conditions. Traditional approaches apply fixed thresholds or treat labels independently, overlooking context and global rarity. We introduce an adaptive thresholding mechanism that fuses global (IDF-based) and local (KNN-based) signals to produce per-label, per-instance thresholds. Instead of applying these as hard cutoffs, we treat them as differentiable penalties in the loss, providing smooth supervision and better calibration. Our architecture is lightweight, interpretable, and highly modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712, substantially outperforming tree-based and pretrained transformer-based methods. We release full code for reproducibility and future extensions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-label classification (MLC) requires predicting multiple labels persample, often under heavy class imbalance and noisy conditions. Traditionalapproaches apply fixed thresholds or treat labels independently, overlookingcontext and global rarity. We introduce an adaptive thresholding mechanism thatfuses global (IDF-based) and local (KNN-based) signals to produce per-label,per-instance thresholds. Instead of applying these as hard cutoffs, we treatthem as differentiable penalties in the loss, providing smooth supervision andbetter calibration. Our architecture is lightweight, interpretable, and highlymodular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,substantially outperforming tree-based and pretrained transformer-basedmethods. We release full code for reproducibility and future extensions.</description>
      <author>example@mail.com (Dmytro Shamatrin)</author>
      <guid isPermaLink="false">2505.03118v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs</title>
      <link>http://arxiv.org/abs/2505.03112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合传统信号处理技术与大型语言模型（LLMs）的创新框架，用于自动调制分类（AMC），以应对信号干扰和噪声的复杂交互。&lt;h4&gt;背景&lt;/h4&gt;AMC对于高效频谱管理和稳健的无线通信至关重要，但由于信号干扰和噪声的复杂交互，AMC仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合传统信号处理技术与LLMs的创新框架，以解决AMC的挑战。&lt;h4&gt;方法&lt;/h4&gt;该方法利用高阶统计和累积量估计将定量信号特征转换为结构化自然语言提示，并通过将示例上下文纳入这些提示中，利用LLM对经典信号处理的熟悉程度，实现有效的单次分类，无需额外的训练或预处理。&lt;h4&gt;主要发现&lt;/h4&gt;在包括无噪声和噪声条件在内的合成数据集上的实验评估表明，该框架在多种调制方案和信噪比（SNR）下实现了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为在不同信道条件下的无线通信中的稳健基础模型铺平了道路，显著降低了开发特定信道模型的开支，为下一代无线网络中的可扩展、可解释和通用的信号分类系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;自动调制分类（AMC）对于高效频谱管理和稳健的无线通信至关重要。然而，由于信号干扰和噪声的复杂交互，AMC仍然具有挑战性。在本研究中，我们提出了一种将传统信号处理技术与大型语言模型（LLMs）相结合的创新框架，以应对AMC的挑战。我们的方法利用高阶统计和累积量估计将定量信号特征转换为结构化自然语言提示。通过将这些提示中的示例上下文纳入，我们的方法利用了LLM对经典信号处理的内在熟悉程度，从而实现有效的单次分类，无需额外的训练或预处理（例如去噪）。在包括无噪声和噪声条件在内的合成数据集上的实验评估表明，我们的框架在多种调制方案和信噪比（SNR）下实现了具有竞争力的性能。此外，我们的方法为在不同信道条件下的无线通信中的稳健基础模型铺平了道路，显著降低了开发特定信道模型的开支。这项工作为下一代无线网络中的可扩展、可解释和通用的信号分类系统奠定了基础。源代码可在https://github.com/RU-SIT/context-is-king找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic Modulation Classification (AMC) is critical for efficient spectrummanagement and robust wireless communications. However, AMC remains challengingdue to the complex interplay of signal interference and noise. In this work, wepropose an innovative framework that integrates traditional signal processingtechniques with Large-Language Models (LLMs) to address AMC. Our approachleverages higher-order statistics and cumulant estimation to convertquantitative signal features into structured natural language prompts. Byincorporating exemplar contexts into these prompts, our method exploits theLLM's inherent familiarity with classical signal processing, enabling effectiveone-shot classification without additional training or preprocessing (e.g.,denoising). Experimental evaluations on synthetically generated datasets,spanning both noiseless and noisy conditions, demonstrate that our frameworkachieves competitive performance across diverse modulation schemes andSignal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robustfoundation models in wireless communications across varying channel conditions,significantly reducing the expense associated with developing channel-specificmodels. This work lays the foundation for scalable, interpretable, andversatile signal classification systems in next-generation wireless networks.The source code is available at https://github.com/RU-SIT/context-is-king</description>
      <author>example@mail.com (Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao)</author>
      <guid isPermaLink="false">2505.03112v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning</title>
      <link>http://arxiv.org/abs/2505.03035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORE的新方法，用于增强语言模型解决零样本移动操作规划问题的能力，以应对场景动态、未知区域和错误恢复等挑战。&lt;h4&gt;背景&lt;/h4&gt;自主长时域移动操作涉及众多挑战，包括场景动态、未知区域和错误恢复。现有的基于基础模型的方法在处理大量对象和大规模环境时性能下降。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，提高移动操作规划的能力。&lt;h4&gt;方法&lt;/h4&gt;MORE利用场景图来表示环境，结合实例区分，并引入了一种主动过滤方案，提取与任务相关的对象和区域实例的子图，从而将规划问题限定在有限范围内，有效减轻幻觉并提高可靠性。此外，还引入了几个增强功能，以实现室内和室外环境的规划。&lt;h4&gt;主要发现&lt;/h4&gt;在BEHAVIOR-1K基准测试的81个多样化的重组任务中，MORE成为第一个成功解决基准测试中很大一部分问题的方法，并优于基于基础模型的方法。此外，该方法在模拟日常活动的几个复杂真实任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;MORE是一种有效的移动操作规划方法，能够处理多种挑战，并在实际应用中显示出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at https://more-model.cs.uni-freiburg.de.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous long-horizon mobile manipulation encompasses a multitude ofchallenges, including scene dynamics, unexplored areas, and error recovery.Recent works have leveraged foundation models for scene-level robotic reasoningand planning. However, the performance of these methods degrades when dealingwith a large number of objects and large-scale environments. To address theselimitations, we propose MORE, a novel approach for enhancing the capabilitiesof language models to solve zero-shot mobile manipulation planning forrearrangement tasks. MORE leverages scene graphs to represent environments,incorporates instance differentiation, and introduces an active filteringscheme that extracts task-relevant subgraphs of object and region instances.These steps yield a bounded planning problem, effectively mitigatinghallucinations and improving reliability. Additionally, we introduce severalenhancements that enable planning across both indoor and outdoor environments.We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1Kbenchmark, where it becomes the first approach to successfully solve asignificant share of the benchmark, outperforming recent foundation model-basedapproaches. Furthermore, we demonstrate the capabilities of our approach inseveral complex real-world tasks, mimicking everyday activities. We make thecode publicly available at https://more-model.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Mohammad Mohammadi, Daniel Honerkamp, Martin Büchner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada)</author>
      <guid isPermaLink="false">2505.03035v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DyTTP: Trajectory Prediction with Normalization-Free Transformers</title>
      <link>http://arxiv.org/abs/2504.05356v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于提高自动驾驶系统中轨迹预测准确性的方法，该方法结合了动态Tanh（DyT）和快照集成策略，以解决传统Transformer架构中正常化层带来的计算开销和训练不稳定问题。&lt;h4&gt;背景&lt;/h4&gt;准确的轨迹预测对自动驾驶系统的安全运行至关重要，而理解周围动态环境的行为是关键。Transformer架构在捕捉复杂的时空依赖关系方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;提高轨迹预测的准确性、推理速度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 将DyT集成到Transformer架构中，替换传统的层归一化，简化网络架构并提高推理稳定性。2. 采用快照集成策略，通过循环学习率调度在单个训练运行中捕获多个模型快照，并在推理时通过简单平均聚合这些快照。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse数据集上的实验表明，该方法显著提高了预测准确性、推理速度和在不同驾驶场景中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;无归一化的Transformer设计结合轻量级集成技术在推进自动驾驶车辆轨迹预测方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;精准的轨迹预测是自动驾驶系统安全运行的基础，理解周围环境的动态行为至关重要。基于Transformer的架构在捕捉复杂的时空依赖关系方面显示出巨大的潜力。然而，它们对归一化层的依赖可能导致计算开销和训练不稳定。在本研究中，我们提出了一种两阶段的方法来应对这些挑战。首先，我们将最新的促进Transformer的方法——动态Tanh（DyT）集成到主干网络中，取代了传统的层归一化。这种修改简化了网络架构并提高了推理的稳定性。我们是第一个将DyT应用于轨迹预测任务的工作。作为补充，我们采用了快照集成策略，进一步提升了轨迹预测性能。使用循环学习率调度，在单个训练运行中捕获多个模型快照。这些快照在推理时通过简单平均进行聚合，使得模型能够从多种假设中受益，而不会产生额外的计算成本。在Argoverse数据集上的大量实验表明，我们的结合方法显著提高了预测准确性、推理速度和在不同驾驶场景中的鲁棒性。这项工作强调了在推进自动驾驶车辆轨迹预测方面，无归一化Transformer设计结合轻量级集成技术的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate trajectory prediction is a cornerstone for the safe operation ofautonomous driving systems, where understanding the dynamic behavior ofsurrounding agents is crucial. Transformer-based architectures havedemonstrated significant promise in capturing complex spatio-temporalitydependencies. However, their reliance on normalization layers can lead tocomputation overhead and training instabilities. In this work, we present atwo-fold approach to address these challenges. First, we integrate DynamicTanh(DyT), which is the latest method to promote transformers, into the backbone,replacing traditional layer normalization. This modification simplifies thenetwork architecture and improves the stability of the inference. We are thefirst work to deploy the DyT to the trajectory prediction task. Complementingthis, we employ a snapshot ensemble strategy to further boost trajectoryprediction performance. Using cyclical learning rate scheduling, multiple modelsnapshots are captured during a single training run. These snapshots are thenaggregated via simple averaging at inference time, allowing the model tobenefit from diverse hypotheses without incurring substantial additionalcomputational cost. Extensive experiments on Argoverse datasets demonstratethat our combined approach significantly improves prediction accuracy,inference speed and robustness in diverse driving scenarios. This workunderscores the potential of normalization-free transformer designs augmentedwith lightweight ensemble techniques in advancing trajectory forecasting forautonomous vehicles.</description>
      <author>example@mail.com (JianLin Zhu, HongKuo Niu)</author>
      <guid isPermaLink="false">2504.05356v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2504.21254v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为ABG-NAS的自动图神经网络架构搜索框架，旨在解决现有图神经网络在适应复杂图结构时的局限性，从而提高图表示学习的效率和效果。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）在节点分类、链接预测和子图搜索等下游任务中至关重要，但现有的GNN架构往往难以适应不同的复杂图结构，限制了其生成结构感知和任务判别性表示的能力。&lt;h4&gt;目的&lt;/h4&gt;提出ABG-NAS框架，以实现高效的图表示学习。&lt;h4&gt;方法&lt;/h4&gt;ABG-NAS包括三个关键组件：全面架构搜索空间（CASS）、自适应遗传优化策略（AGOS）和贝叶斯引导调优模块（BGTM）。CASS探索多样化的传播和转换操作，AGOS动态平衡探索和利用，BGTM定期优化超参数。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集（Cora、PubMed、Citeseer和CoraFull）上的实验表明，ABG-NAS在性能上优于手动设计的GNN和最先进的神经架构搜索（NAS）方法。&lt;h4&gt;结论&lt;/h4&gt;ABG-NAS具有提供可扩展和自适应解决方案的潜力，可以推动图表示学习的发展。&lt;h4&gt;翻译&lt;/h4&gt;Effective and efficient graph representation learning is essential for enabling critical downstream tasks, such as node classification, link prediction, and subgraph search. However, existing graph neural network (GNN) architectures often struggle to adapt to diverse and complex graph structures, limiting their ability to produce structure-aware and task-discriminative representations. To address this challenge, we propose ABG-NAS, a novel framework for automated graph neural network architecture search tailored for efficient graph representation learning. ABG-NAS encompasses three key components: a Comprehensive Architecture Search Space (CASS), an Adaptive Genetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS systematically explores diverse propagation (P) and transformation (T) operations, enabling the discovery of GNN architectures capable of capturing intricate graph characteristics. AGOS dynamically balances exploration and exploitation, ensuring search efficiency and preserving solution diversity. BGTM further optimizes hyperparameters periodically, enhancing the scalability and robustness of the resulting architectures. Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that ABG-NAS consistently outperforms both manually designed GNNs and state-of-the-art neural architecture search (NAS) methods. These results highlight the potential of ABG-NAS to advance graph representation learning by providing scalable and adaptive solutions for diverse graph structures. Our code is publicly available at https://github.com/sserranw/ABG-NAS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective and efficient graph representation learning is essential forenabling critical downstream tasks, such as node classification, linkprediction, and subgraph search. However, existing graph neural network (GNN)architectures often struggle to adapt to diverse and complex graph structures,limiting their ability to produce structure-aware and task-discriminativerepresentations. To address this challenge, we propose ABG-NAS, a novelframework for automated graph neural network architecture search tailored forefficient graph representation learning. ABG-NAS encompasses three keycomponents: a Comprehensive Architecture Search Space (CASS), an AdaptiveGenetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module(BGTM). CASS systematically explores diverse propagation (P) and transformation(T) operations, enabling the discovery of GNN architectures capable ofcapturing intricate graph characteristics. AGOS dynamically balancesexploration and exploitation, ensuring search efficiency and preservingsolution diversity. BGTM further optimizes hyperparameters periodically,enhancing the scalability and robustness of the resulting architectures.Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, andCoraFull) demonstrate that ABG-NAS consistently outperforms both manuallydesigned GNNs and state-of-the-art neural architecture search (NAS) methods.These results highlight the potential of ABG-NAS to advance graphrepresentation learning by providing scalable and adaptive solutions fordiverse graph structures. Our code is publicly available athttps://github.com/sserranw/ABG-NAS.</description>
      <author>example@mail.com (Sixuan Wang, Jiao Yin, Jinli Cao, MingJian Tang, Hua Wang, Yanchun Zhang)</author>
      <guid isPermaLink="false">2504.21254v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</title>
      <link>http://arxiv.org/abs/2504.19244v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SALCR的无监督可见光-红外行人重识别框架，旨在通过优化特定细粒度模式，实现不同模态标签分布的互补对齐。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通过标签关联算法统一跨模态图像的伪标签，并设计对比学习框架进行全局特征学习，但忽略了特征表示和伪标签分布中的跨模态变化。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中由于仅优化全局特征而导致的模态共享学习不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出SALCR框架，包括DAGI模块统一跨模态实例的伪标签，FGSAL模块探索跨模态实例中各模态强调的部分级语义对齐模式，以及GPCR模块动态挖掘可靠的正样本集以优化实例间关系。&lt;h4&gt;主要发现&lt;/h4&gt;SALCR框架通过强调细粒度模式，实现了不同模态标签分布的互补对齐，并通过优化实例间关系来减轻噪声伪标签的副作用。&lt;h4&gt;结论&lt;/h4&gt;实验表明，SALCR方法在性能上优于现有方法，并提供了可用的代码。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework for unsupervised visible-infrared person re-identification, which aims to achieve complementary alignment between the label distributions of different modalities by optimizing specific fine-grained patterns. The existing methods unify pseudo-labels of cross-modality images through label association algorithms and design contrastive learning frameworks for global feature learning, but overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. To address this issue, the SALCR framework is proposed, which includes a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bidirectional manner, a Fine-Grained Semantic-Aligned Learning (FGSAL) module to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances, and a Global-Part Collaborative Refinement (GPCR) module to dynamically mine reliable positive sample sets for the global and part features and optimize the inter-instance relationships. Extensive experiments demonstrate that the proposed method achieves superior performance compared to existing methods, and the code is available.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks tomatch pedestrian images of the same individual across different modalitieswithout human annotations for model learning. Previous methods unifypseudo-labels of cross-modality images through label association algorithms andthen design contrastive learning framework for global feature learning.However, these methods overlook the cross-modality variations in featurerepresentation and pseudo-label distributions brought by fine-grained patterns.This insight results in insufficient modality-shared learning when only globalfeatures are optimized. To address this issue, we propose a Semantic-AlignedLearning with Collaborative Refinement (SALCR) framework, which builds upoptimization objective for specific fine-grained patterns emphasized by eachmodality, thereby achieving complementary alignment between the labeldistributions of different modalities. Specifically, we first introduce a DualAssociation with Global Learning (DAGI) module to unify the pseudo-labels ofcross-modality instances in a bi-directional manner. Afterward, a Fine-GrainedSemantic-Aligned Learning (FGSAL) module is carried out to explore part-levelsemantic-aligned patterns emphasized by each modality from cross-modalityinstances. Optimization objective is then formulated based on thesemantic-aligned features and their corresponding label space. To alleviate theside-effects arising from noisy pseudo-labels, we propose a Global-PartCollaborative Refinement (GPCR) module to mine reliable positive sample setsfor the global and part features dynamically and optimize the inter-instancerelationships. Extensive experiments demonstrate the effectiveness of theproposed method, which achieves superior performances to state-of-the-artmethods. Our code is available at\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</description>
      <author>example@mail.com (De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2504.19244v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion</title>
      <link>http://arxiv.org/abs/2505.01857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DualDiff的双重分支条件扩散模型，用于增强多视角驾驶场景生成，旨在提高场景重建的准确性和高保真度。&lt;h4&gt;背景&lt;/h4&gt;现有的驾驶场景重建方法主要依赖3D边界框和前景背景的二值图，这些方法无法充分捕捉场景的复杂性并整合多模态信息。&lt;h4&gt;目的&lt;/h4&gt;提高场景重建的准确性和高保真度，增强多视角驾驶场景生成。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为Occupancy Ray Sampling (ORS)的语义丰富的3D表示方法，以实现全面的前景和背景控制。2. 设计了语义融合注意力（SFA）机制，以改善跨模态信息整合。3. 设计了前景感知掩码（FGM）损失，以增强小对象的生成。&lt;h4&gt;主要发现&lt;/h4&gt;DualDiff在FID评分中达到了最先进的性能，并在下游的BEV分割和3D目标检测任务中取得了持续的良好结果。&lt;h4&gt;结论&lt;/h4&gt;DualDiff模型在驾驶场景重建方面具有显著优势，能够有效提升重建的准确性和细节表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and high-fidelity driving scene reconstruction relies on fullyleveraging scene information as conditioning. However, existing approaches,which primarily use 3D bounding boxes and binary maps for foreground andbackground control, fall short in capturing the complexity of the scene andintegrating multi-modal information. In this paper, we propose DualDiff, adual-branch conditional diffusion model designed to enhance multi-view drivingscene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3Drepresentation, alongside numerical driving scene representation, forcomprehensive foreground and background control. To improve cross-modalinformation integration, we propose a Semantic Fusion Attention (SFA) mechanismthat aligns and fuses features across modalities. Furthermore, we design aforeground-aware masked (FGM) loss to enhance the generation of tiny objects.DualDiff achieves state-of-the-art performance in FID score, as well asconsistently better results in downstream BEV segmentation and 3D objectdetection tasks.</description>
      <author>example@mail.com (Haoteng Li, Zhao Yang, Zezhong Qian, Gongpeng Zhao, Yuqi Huang, Jun Yu, Huazheng Zhou, Longjun Liu)</author>
      <guid isPermaLink="false">2505.01857v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
  <item>
      <title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
      <link>http://arxiv.org/abs/2505.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Self-Representation Alignment for Diffusion Transformers. arXiv admin  note: text overlap with arXiv:2410.06940 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Self-Representation Alignment（SRA）方法，通过自我蒸馏的方式，在仅生成训练过程中，提升扩散变换器的内部表示学习，从而加速生成训练并提高生成质量。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么需要引入额外的复杂表示训练框架，要么依赖大规模预训练的表示基础模型来提供表示指导。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要外部表示组件的简单方法，利用扩散变换器自身的独特判别过程来提供表示指导。&lt;h4&gt;方法&lt;/h4&gt;SRA方法通过将扩散变换器早期层输出高噪声的潜在表示与后期层低噪声的潜在表示进行对齐，以逐步增强仅在生成训练过程中的整体表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将SRA应用于DiTs和SiTs可以获得一致的性能提升。SRA不仅显著优于依赖于辅助的复杂表示训练框架的方法，而且达到了依赖于强大外部表示先验的方法的性能。&lt;h4&gt;结论&lt;/h4&gt;SRA方法能够有效提升扩散变换器的生成训练速度和生成质量，是一种简单且有效的内部表示学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vvvvvjdy/SRA&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated that learning a meaningful internalrepresentation can both accelerate generative training and enhance generationquality of the diffusion transformers. However, existing approaches necessitateto either introduce an additional and complex representation training frameworkor rely on a large-scale, pre-trained representation foundation model toprovide representation guidance during the original generative trainingprocess. In this study, we posit that the unique discriminative processinherent to diffusion transformers enables them to offer such guidance withoutrequiring external representation components. We therefore proposeSelf-Representation A}lignment (SRA), a simple yet straightforward method thatobtain representation guidance through a self-distillation manner.Specifically, SRA aligns the output latent representation of the diffusiontransformer in earlier layer with higher noise to that in later layer withlower noise to progressively enhance the overall representation learning duringonly generative training process. Experimental results indicate that applyingSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRAnot only significantly outperforms approaches relying on auxiliary, complexrepresentation training frameworks but also achieves performance comparable tomethods that heavily dependent on powerful external representation priors.</description>
      <author>example@mail.com (Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang)</author>
      <guid isPermaLink="false">2505.02831v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter</title>
      <link>http://arxiv.org/abs/2505.02664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 Pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Grasp the Graph 2.0（GtG 2.0）方法，这是一种轻量级且高效的机器人抓取框架，通过集成图神经网络从点云数据中进行高效的几何推理。&lt;h4&gt;背景&lt;/h4&gt;在杂乱的真实环境中进行抓取姿态检测是一个重大挑战，因为噪声和不完整的感觉数据与复杂对象几何形状相结合。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理噪声和不完整数据，并适应复杂对象几何形状的抓取姿态检测方法。&lt;h4&gt;方法&lt;/h4&gt;GtG 2.0方法利用传统的抓取姿态生成器高效地生成7自由度的抓取候选者，并通过包含夹爪爪口内点和周围环境点的集成图神经网络模型对这些候选者进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;GtG 2.0在GraspNet-1Billion基准测试中，与基于假设和测试以及基于图神经网络的方法相比，平均精度提高了35%，并排名前三。&lt;h4&gt;结论&lt;/h4&gt;GtG 2.0在3自由度Delta并联机器人和Kinect-v1摄像头上的实验中，成功率达到91%，杂乱完成率为100%，证明了其灵活性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在杂乱的真实环境中进行抓取姿态检测仍然是一个重大挑战，因为噪声和不完整的感觉数据与复杂对象几何形状相结合。本文介绍了Grasp the Graph 2.0（GtG 2.0）方法，这是一种轻量级且高效的机器人抓取框架，通过集成图神经网络从点云数据中进行高效的几何推理。基于GtG 1.0的成功，它证明了图神经网络在抓取检测中的潜力，但受限于完整、无噪声的点云和4自由度抓取的假设，GtG 2.0采用传统的抓取姿态生成器来高效地生成7自由度的抓取候选者。候选者通过包含夹爪爪口内点和周围环境点的集成图神经网络模型进行评估。这种改进的表示提高了抓取检测性能，与使用相同生成器的先前方法相比，GtG 2.0在GraspNet-1Billion基准测试中的平均精度提高了35%，并排名前三。使用3自由度Delta并联机器人和Kinect-v1摄像头进行的实验表明，成功率为91%，杂乱完成率为100%，证明了其灵活性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasp pose detection in cluttered, real-world environments remains asignificant challenge due to noisy and incomplete sensory data combined withcomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)method, a lightweight yet highly effective hypothesis-and-test roboticsgrasping framework which leverages an ensemble of Graph Neural Networks forefficient geometric reasoning from point cloud data. Building on the success ofGtG 1.0, which demonstrated the potential of Graph Neural Networks for graspdetection but was limited by assumptions of complete, noise-free point cloudsand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator toefficiently produce 7-Dof grasp candidates. Candidates are assessed with anensemble Graph Neural Network model which includes points within the gripperjaws (inside points) and surrounding contextual points (outside points). Thisimproved representation boosts grasp detection performance over previousmethods using the same generator. GtG 2.0 shows up to a 35% improvement inAverage Precision on the GraspNet-1Billion benchmark compared tohypothesis-and-test and Graph Neural Network-based methods, ranking it amongthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot andKinect-v1 camera show a success rate of 91% and a clutter completion rate of100%, demonstrating its flexibility and reliability.</description>
      <author>example@mail.com (Ali Rashidi Moghadam, Sayedmohammadreza Rastegari, Mehdi Tale Masouleh, Ahmad Kalhor)</author>
      <guid isPermaLink="false">2505.02664v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration</title>
      <link>http://arxiv.org/abs/2505.02787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的无监督描述符学习方法，用于眼底图像配准，无需依赖关键点检测，并在多个测试中展示了其准确性和性能。&lt;h4&gt;背景&lt;/h4&gt;当前眼底图像配准方法受限于缺乏标记数据，尤其在医疗领域更为显著，这促使了无监督学习技术的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖关键点检测的无监督描述符学习方法，以实现眼底图像配准。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的无监督描述符学习方法，并在公共眼底图像注册数据集上进行了广泛和全面的比较。同时，测试了多种不同类型的关键点检测器，并提出了新的检测器。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在注册精度上不亚于监督方法，且在不同关键点检测器下均表现出准确性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作在医疗领域利用无监督学习方面迈出了重要一步。&lt;h4&gt;翻译&lt;/h4&gt;Current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. Therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. This enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference. To validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. Additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. Our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. Additionally, it demonstrates accurate performance regardless of the keypoint detector used. Thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current color fundus image registration approaches are limited, among otherthings, by the lack of labeled data, which is even more significant in themedical domain, motivating the use of unsupervised learning. Therefore, in thiswork, we develop a novel unsupervised descriptor learning method that does notrely on keypoint detection. This enables the resulting descriptor network to beagnostic to the keypoint detector used during the registration inference.  To validate this approach, we perform an extensive and comprehensivecomparison on the reference public retinal image registration dataset.Additionally, we test our method with multiple keypoint detectors of variednature, even proposing some novel ones. Our results demonstrate that theproposed approach offers accurate registration, not incurring in anyperformance loss versus supervised methods. Additionally, it demonstratesaccurate performance regardless of the keypoint detector used. Thus, this workrepresents a notable step towards leveraging unsupervised learning in themedical domain.</description>
      <author>example@mail.com (David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo)</author>
      <guid isPermaLink="false">2505.02787v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.02393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为IEF-VAD的视频异常检测框架，通过融合图像和事件表示来提高检测精度。&lt;h4&gt;背景&lt;/h4&gt;现有的视频异常检测器主要依赖于RGB帧，这限制了捕捉突发事件中的快速或短暂运动。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，该框架能够从RGB视频中直接合成事件表示，并通过不确定性感知的过程与图像特征融合。&lt;h4&gt;方法&lt;/h4&gt;IEF-VAD框架包括：(i) 使用Student-t分布来建模传感器噪声，并通过Laplace近似得到价值级别的逆方差权重；(ii) 应用类似Kalman的帧级更新来平衡不同模态随时间的变化；(iii) 通过迭代细化融合的潜在状态来消除残存的跨模态噪声。&lt;h4&gt;主要发现&lt;/h4&gt;IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平，并且无需专用的事件传感器或帧级标签。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了合成事件表示在强调通常在RGB帧中未被充分表示的运动线索中的效用，这使得在不同应用中实现准确和鲁棒的视频理解成为可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的视频异常检测器仅依赖于RGB帧，这缺乏捕捉突变或短暂运动线索所需的时间分辨率，而这些线索是异常事件的关键指标。为了解决这一限制，我们提出了用于视频异常检测的图像-事件融合（IEF-VAD）框架，该框架直接从RGB视频中合成事件表示，并通过一种原则性、不确定性感知的过程与图像特征融合。该系统（i）使用Student-t分布来建模传感器噪声，通过Laplace近似推导出价值级别的逆方差权重；（ii）应用类似Kalman的帧级更新来平衡模态随时间的变化；（iii）迭代细化融合的潜在状态，以消除残留的跨模态噪声。无需专用的事件传感器或帧级标签，IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平。这些发现突出了合成事件表示在强调通常在RGB帧中未被充分表示的运动线索中的效用，使得在不同应用中实现准确和鲁棒的视频理解成为可能。代码和模型可在https://github.com/EavnJeong/IEF-VAD上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing video anomaly detectors rely solely on RGB frames, which lackthe temporal resolution needed to capture abrupt or transient motion cues, keyindicators of anomalous events. To address this limitation, we proposeImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework thatsynthesizes event representations directly from RGB videos and fuses them withimage features through a principled, uncertainty-aware process. The system (i)models heavy-tailed sensor noise with a Student`s-t likelihood, derivingvalue-level inverse-variance weights via a Laplace approximation; (ii) appliesKalman-style frame-wise updates to balance modalities over time; and (iii)iteratively refines the fused latent state to erase residual cross-modal noise.Without any dedicated event sensor or frame-level labels, IEF-VAD sets a newstate of the art across multiple real-world anomaly detection benchmarks. Thesefindings highlight the utility of synthetic event representations inemphasizing motion cues that are often underrepresented in RGB frames, enablingaccurate and robust video understanding across diverse applications withoutrequiring dedicated event sensors. Code and models are available athttps://github.com/EavnJeong/IEF-VAD.</description>
      <author>example@mail.com (Sungheon Jeong, Jihong Park, Mohsen Imani)</author>
      <guid isPermaLink="false">2505.02393v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery</title>
      <link>http://arxiv.org/abs/2505.02049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages. arXiv admin note: substantial text overlap with  arXiv:2409.11532&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要介绍了激光雷达技术的最新进展，特别是在点云分辨率提高和生成360度低分辨率图像方面的突破。这些图像的应用使深度学习技术能够在激光雷达系统中替代传统方法，提高了在恶劣环境下的鲁棒性，并解决了点云几何信息退化的问题。&lt;h4&gt;背景&lt;/h4&gt;背景提到了激光雷达技术的进步，以及由此带来的点云分辨率提升和生成360度低分辨率图像的能力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一个新颖的框架，利用基于深度学习的彩色化和超分辨率技术在激光雷达图像上提取可靠样本，以提高里程计估计的精度。&lt;h4&gt;方法&lt;/h4&gt;该方法采用深度学习技术对激光雷达图像进行彩色化和超分辨率处理，以提高关键点检测的准确性，进而进行有效的点云降采样，从而提升点云注册精度并减少由于几何信息不足或误点引起的误差。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在翻译和旋转误差方面优于先前的方法，并且在使用更少点的情况下取得了更好的效果。&lt;h4&gt;结论&lt;/h4&gt;结论强调了该研究提出的框架在提高激光雷达里程计估计精度方面的有效性，并指出了其在减少错误和提高数据处理效率方面的优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in lidar technology have led to improved point cloud resolution as well as the generation of 360 degrees, low-resolution images by encoding depth, reflectivity, or near-infrared light within each pixel. These images enable the application of deep learning (DL) approaches, originally developed for RGB images from cameras to lidar-only systems, eliminating other efforts, such as lidar-camera calibration. Compared with conventional RGB images, lidar imagery demonstrates greater robustness in adverse environmental conditions, such as low light and foggy weather. Moreover, the imaging capability addresses the challenges in environments where the geometric information in point clouds may be degraded, such as long corridors, and dense point clouds may be misleading, potentially leading to drift errors. Therefore, this paper proposes a novel framework that leverages DL-based colorization and super-resolution techniques on lidar imagery to extract reliable samples from lidar point clouds for odometry estimation. The enhanced lidar images, enriched with additional information, facilitate improved keypoint detection, which is subsequently employed for more effective point cloud downsampling. The proposed method enhances point cloud registration accuracy and mitigates mismatches arising from insufficient geometric information or misleading extra points. Experimental results indicate that our approach surpasses previous methods, achieving lower translation and rotation errors while using fewer points.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in lidar technology have led to improved point cloudresolution as well as the generation of 360 degrees, low-resolution images byencoding depth, reflectivity, or near-infrared light within each pixel. Theseimages enable the application of deep learning (DL) approaches, originallydeveloped for RGB images from cameras to lidar-only systems, eliminating otherefforts, such as lidar-camera calibration. Compared with conventional RGBimages, lidar imagery demonstrates greater robustness in adverse environmentalconditions, such as low light and foggy weather. Moreover, the imagingcapability addresses the challenges in environments where the geometricinformation in point clouds may be degraded, such as long corridors, and densepoint clouds may be misleading, potentially leading to drift errors.  Therefore, this paper proposes a novel framework that leverages DL-basedcolorization and super-resolution techniques on lidar imagery to extractreliable samples from lidar point clouds for odometry estimation. The enhancedlidar images, enriched with additional information, facilitate improvedkeypoint detection, which is subsequently employed for more effective pointcloud downsampling. The proposed method enhances point cloud registrationaccuracy and mitigates mismatches arising from insufficient geometricinformation or misleading extra points. Experimental results indicate that ourapproach surpasses previous methods, achieving lower translation and rotationerrors while using fewer points.</description>
      <author>example@mail.com (Sier Ha, Honghao Du, Xianjia Yu, Tomi Westerlund)</author>
      <guid isPermaLink="false">2505.02049v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
      <link>http://arxiv.org/abs/2505.02366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无监督对比学习方法JTCSE，用于自然语言处理中的文本语义嵌入。&lt;h4&gt;背景&lt;/h4&gt;无监督对比学习在自然语言处理领域受到关注，但现有方法忽略了语义表示张量的模量特征，导致对比学习效果不足。&lt;h4&gt;目的&lt;/h4&gt;旨在增强对比学习中正样本之间的对齐，并优化模型对CLS token的注意力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种训练目标，旨在对语义表示张量施加模量约束，并设计了交叉注意力结构以增强模型对CLS token的注意力。&lt;h4&gt;主要发现&lt;/h4&gt;JTCSE在七个语义文本相似度计算任务中表现出色，其双塔集成模型和单塔蒸馏模型优于其他基线，成为当前SOTA。在超过130个零样本下游任务评估中，JTCSE整体优于其他基线。&lt;h4&gt;结论&lt;/h4&gt;JTCSE通过结合模量约束和交叉注意力机制，在文本语义嵌入方面取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised contrastive learning has become a hot research topic in naturallanguage processing. Existing works usually aim at constraining the orientationdistribution of the representations of positive and negative samples in thehigh-dimensional semantic space in contrastive learning, but the semanticrepresentation tensor possesses both modulus and orientation features, and theexisting works ignore the modulus feature of the representations and causeinsufficient contrastive learning. % Therefore, we firstly propose a trainingobjective that aims at modulus constraints on the semantic representationtensor, to strengthen the alignment between the positive samples in contrastivelearning. Therefore, we first propose a training objective that is designed toimpose modulus constraints on the semantic representation tensor, to strengthenthe alignment between positive samples in contrastive learning. Then, theBERT-like model suffers from the phenomenon of sinking attention, leading to alack of attention to CLS tokens that aggregate semantic information. Inresponse, we propose a cross-attention structure among the twin-tower ensemblemodels to enhance the model's attention to CLS token and optimize the qualityof CLS Pooling. Combining the above two motivations, we propose a new\textbf{J}oint \textbf{T}ensor representation modulus constraint and\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence\textbf{E}mbedding representation framework JTCSE, which we evaluate in sevensemantic text similarity computation tasks, and the experimental results showthat JTCSE's twin-tower ensemble model and single-tower distillation modeloutperform the other baselines and become the current SOTA. In addition, wehave conducted an extensive zero-shot downstream task evaluation, which showsthat JTCSE outperforms other baselines overall on more than 130 tasks.</description>
      <author>example@mail.com (Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu)</author>
      <guid isPermaLink="false">2505.02366v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
      <link>http://arxiv.org/abs/2505.02712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过深度强化学习控制布尔网络模型，以探索细胞重编程策略，并展示该方法在处理复杂生物系统中的有效性。&lt;h4&gt;背景&lt;/h4&gt;细胞重编程在治疗复杂疾病方面具有潜在的治疗价值，但传统的实验方法耗时且成本高。&lt;h4&gt;目的&lt;/h4&gt;利用深度强化学习来控制布尔网络模型，以发现细胞重编程的策略。&lt;h4&gt;方法&lt;/h4&gt;提出了布尔网络模型在异步更新模式下的新型控制问题，引入了伪吸引子的概念并改进了伪吸引子状态识别的流程，并设计了计算框架来解决控制问题。将图神经网络和图卷积集成到人工神经网络近似器中，以利用生物系统的结构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个来自文献的大规模真实世界生物网络上进行了实验，证明了该方法的可扩展性和有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法在细胞重编程中具有实际应用潜力，并展示了深度强化学习在处理复杂生物系统中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cellular reprogramming, the artificial transformation of one cell type intoanother, has been attracting increasing research attention due to itstherapeutic potential for complex diseases. However, discovering reprogrammingstrategies through classical wet-lab experiments is hindered by lengthy timecommitments and high costs. In this study, we explore the use of deepreinforcement learning (DRL) to control Boolean network models of complexbiological systems, such as gene regulatory networks and signalling pathwaynetworks. We formulate a novel control problem for Boolean network models underthe asynchronous update mode in the context of cellular reprogramming. Tofacilitate scalability, we consider our previously introduced concept of apseudo-attractor and we improve our procedure for effective identification ofpseudo-attractor states. Finally, we devise a computational framework to solvethe control problem. To leverage the structure of biological systems, weincorporate graph neural networks with graph convolutions into the artificialneural network approximator for the action-value function learned by the DRLagent. Experiments on a number of large real-world biological networks fromliterature demonstrate the scalability and effectiveness of our approach.</description>
      <author>example@mail.com (Andrzej Mizera, Jakub Zarzycki)</author>
      <guid isPermaLink="false">2505.02712v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>fastabx: A library for efficient computation of ABX discriminability</title>
      <link>http://arxiv.org/abs/2505.02692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了名为fastabx的高性能Python库，用于构建ABXdiscrimination任务。&lt;h4&gt;背景&lt;/h4&gt;ABX是一种衡量感兴趣的一般类别之间分离程度的度量，广泛用于评估自监督语音表示中的语音可辨性。然而，由于其缺乏适当的工具，其更广泛的应用受到了限制。&lt;h4&gt;目的&lt;/h4&gt;fastabx通过提供一个能够构建任何类型ABX任务的框架，同时提供快速开发周期所需的效率，来解决这一差距。&lt;h4&gt;方法&lt;/h4&gt;fastabx提供了一种构建ABX任务的方法，并能够计算表示之间的距离。&lt;h4&gt;主要发现&lt;/h4&gt;fastabx将成为更广泛的表示学习社区的有价值资源，使研究人员能够系统地研究可以从学习到的表示中直接提取哪些信息，而不仅限于语音处理领域。&lt;h4&gt;结论&lt;/h4&gt;fastabx的源代码可在https://github.com/bootphon/fastabx上获取。&lt;h4&gt;翻译&lt;/h4&gt;We introduce fastabx, a high-performance Python library for building ABXdiscrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce fastabx, a high-performance Python library for building ABXdiscrimination tasks. ABX is a measure of the separation between genericcategories of interest. It has been used extensively to evaluate phoneticdiscriminability in self-supervised speech representations. However, itsbroader adoption has been limited by the absence of adequate tools. fastabxaddresses this gap by providing a framework capable of constructing any type ofABX task while delivering the efficiency necessary for rapid developmentcycles, both in task creation and in calculating distances betweenrepresentations. We believe that fastabx will serve as a valuable resource forthe broader representation learning community, enabling researchers tosystematically investigate what information can be directly extracted fromlearned representations across several domains beyond speech processing. Thesource code is available at https://github.com/bootphon/fastabx.</description>
      <author>example@mail.com (Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2505.02692v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation</title>
      <link>http://arxiv.org/abs/2505.02476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print for IEEE IAVVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为点云重组的方法，用于增强现实世界点云数据，以解决智能移动系统在开放世界应用中基于LiDAR感知的验证问题。&lt;h4&gt;背景&lt;/h4&gt;由于真实环境条件的可变性，基于LiDAR的感知验证是一个挑战。虚拟模拟可以生成任意场景，但缺乏物理传感器特性；而真实世界数据提供真实的传感器特性，但难以控制影响因素。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，通过整合实验室环境中测量的物理目标对象的点云，系统地增强捕获的点云场景，以创建大量和多样化的可重复、物理准确的测试场景。&lt;h4&gt;方法&lt;/h4&gt;点云重组方法通过将实验室环境中测量的物理目标对象的点云与真实世界场景中的点云进行整合，从而增强真实世界点云数据。&lt;h4&gt;主要发现&lt;/h4&gt;使用Ouster OS1-128 Rev7传感器，该方法展示了如何将具有不同服装和姿势的人形目标添加到真实世界的城市和乡村场景中，以实现可重复的位置定位。重组的场景与真实传感器的输出非常接近，从而实现了有针对性的测试、可扩展的故障分析和系统安全性的提高。&lt;h4&gt;结论&lt;/h4&gt;通过提供受控且传感器真实的数据，该方法使得对特定传感器及其算法局限性的结论更加可靠，例如物体检测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：由于真实环境条件的变化，基于LiDAR的智能移动系统在开放世界应用中的感知验证仍然是一个挑战。虚拟模拟允许在受控条件下生成任意场景，但缺乏物理传感器特性，如强度响应或材料依赖效应。相比之下，真实世界数据提供了真实的传感器特性，但提供了较少的控制影响因素，阻碍了充分的验证。现有的方法通过在场景之间转移对象来增强真实世界点云数据，但这些问题没有考虑验证，并且由于依赖于经验数据而在可控性方面有限。我们通过提出点云重组来解决这些限制，该方法通过整合在受控实验室环境中测量的物理目标对象的点云，系统地增强捕获的点云场景。因此，能够创建大量和多样化的可重复、物理准确的测试场景，与具有注册3D网格的现象感知遮挡相关。使用Ouster OS1-128 Rev7传感器，我们展示了如何通过添加具有不同服装和姿势的人形目标来增强真实世界的城市和乡村场景，以实现可重复的位置定位。我们表明，重组的场景与真实传感器的输出非常接近，从而实现了有针对性的测试、可扩展的故障分析和系统安全性的提高。通过提供受控但传感器真实的数据，我们的方法使得对特定传感器及其算法局限性的结论更加可靠，例如物体检测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The validation of LiDAR-based perception of intelligent mobile systemsoperating in open-world applications remains a challenge due to the variabilityof real environmental conditions. Virtual simulations allow the generation ofarbitrary scenes under controlled conditions but lack physical sensorcharacteristics, such as intensity responses or material-dependent effects. Incontrast, real-world data offers true sensor realism but provides less controlover influencing factors, hindering sufficient validation. Existing approachesaddress this problem with augmentation of real-world point cloud data bytransferring objects between scenes. However, these methods do not considervalidation and remain limited in controllability because they rely on empiricaldata. We solve these limitations by proposing Point Cloud Recombination, whichsystematically augments captured point cloud scenes by integrating point cloudsacquired from physical target objects measured in controlled laboratoryenvironments. Thus enabling the creation of vast amounts and varieties ofrepeatable, physically accurate test scenes with respect to phenomena-awareocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, wedemonstrate the augmentation of real-world urban and rural scenes with humanoidtargets featuring varied clothing and poses, for repeatable positioning. Weshow that the recombined scenes closely match real sensor outputs, enablingtargeted testing, scalable failure analysis, and improved system safety. Byproviding controlled yet sensor-realistic data, our method enables trustworthyconclusions about the limitations of specific sensors in compound with theiralgorithms, e.g., object detection.</description>
      <author>example@mail.com (Hubert Padusinski, Christian Steinhauser, Christian Scherl, Julian Gaal, Jacob Langner)</author>
      <guid isPermaLink="false">2505.02476v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.02634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于迁移学习、多目标深度强化学习（DRL）的方法，能够根据气动和结构标准优化任何机翼的几何形状。&lt;h4&gt;背景&lt;/h4&gt;研究背景未在摘要中提及。&lt;h4&gt;目的&lt;/h4&gt;旨在展示该方法，通过最大化升阻比（$C_L/C_D$）同时保持机翼的结构完整性（通过最大厚度建模），并使用不同的迁移学习（TL）策略训练DRL智能体。&lt;h4&gt;方法&lt;/h4&gt;将DRL智能体的性能与粒子群优化（PSO）进行比较，PSO是一种传统的无梯度优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;DRL智能体能够执行多目标形状优化，DRL方法在计算效率和形状优化性能方面优于PSO，迁移学习增强的DRL智能体在性能上与DRL相当，同时节省了大量计算资源。&lt;h4&gt;结论&lt;/h4&gt;DRL方法在多目标形状优化中表现优异，迁移学习可以显著提高DRL的性能并节省计算资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The main objective of this paper is to introduce a transferlearning-enhanced, multi-objective, deep reinforcement learning (DRL)methodology that is able to optimise the geometry of any airfoil based onconcomitant aerodynamic and structural criteria. To showcase the method, we aimto maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structuralintegrity of the airfoil -- as modelled by its maximum thickness -- and trainthe DRL agent using a list of different transfer learning (TL) strategies. Theperformance of the DRL agent is compared with Particle Swarm Optimisation(PSO), a traditional gradient-free optimisation method. Results indicate thatDRL agents are able to perform multi-objective shape optimisation, that the DRLapproach outperforms PSO in terms of computational efficiency and shapeoptimisation performance, and that the TL-enhanced DRL agent achievesperformance comparable to the DRL one, while further saving substantialcomputational resources.</description>
      <author>example@mail.com (David Ramos, Lucas Lacasa, Eusebio Valero, Gonzalo Rubio)</author>
      <guid isPermaLink="false">2505.02634v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection</title>
      <link>http://arxiv.org/abs/2505.02331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Source code and pre-trained models will be available at  https://github.com/MSA-LMC/VAEmo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VAEmo是一种高效的两阶段框架，用于基于情感联合视觉-听觉表示学习，并通过外部知识注入解决情感识别的挑战。&lt;h4&gt;背景&lt;/h4&gt;音频视觉情感识别（AVER）旨在从非语言视觉-听觉线索中推断人类情感，具有模态互补和语言无关的优势，但面临情感表达的不确定性、跨模态表达差异和可靠标注数据稀缺等问题。&lt;h4&gt;目的&lt;/h4&gt;提出VAEmo，以解决AVER中的挑战，实现高效的跨模态情感语义建模。&lt;h4&gt;方法&lt;/h4&gt;第一阶段，通过掩码重建和对比性目标，在大型以说话者为中心的VA语料库上预训练一个统一且轻量级的表示网络；第二阶段，使用多模态大型语言模型根据设计的思维链提示生成详细的情感描述，并通过双路径对比学习将丰富的文本语义注入到VA表示中。&lt;h4&gt;主要发现&lt;/h4&gt;VAEmo在多个下游AVER基准测试中实现了最先进的性能，证明了统一跨模态编码和情感感知语义指导对高效、通用VA情感表示的益处。&lt;h4&gt;结论&lt;/h4&gt;VAEmo通过统一跨模态编码和情感感知语义指导，为高效、通用的VA情感表示提供了新的解决方案，并实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual emotion recognition (AVER) aims to infer human emotions fromnonverbal visual-audio (VA) cues, offering modality-complementary andlanguage-agnostic advantages. However, AVER remains challenging due to theinherent ambiguity of emotional expressions, cross-modal expressivedisparities, and the scarcity of reliably annotated data. Recentself-supervised AVER approaches have introduced strong multimodalrepresentations, yet they predominantly rely on modality-specific encoders andcoarse content-level alignment, limiting fine-grained emotional semanticmodeling. To address these issues, we propose VAEmo, an efficient two-stageframework for emotion-centric joint VA representation learning with externalknowledge injection. In Stage 1, a unified and lightweight representationnetwork is pre-trained on large-scale speaker-centric VA corpora via maskedreconstruction and contrastive objectives, mitigating the modality gap andlearning expressive, complementary representations without emotion labels. InStage 2, multimodal large language models automatically generate detailedaffective descriptions according to our well-designed chain-of-thoughtprompting for only a small subset of VA samples; these rich textual semanticsare then injected by aligning their corresponding embeddings with VArepresentations through dual-path contrastive learning, further bridging theemotion gap. Extensive experiments on multiple downstream AVER benchmarks showthat VAEmo achieves state-of-the-art performance with a compact design,highlighting the benefit of unified cross-modal encoding and emotion-awaresemantic guidance for efficient, generalizable VA emotion representations.</description>
      <author>example@mail.com (Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong)</author>
      <guid isPermaLink="false">2505.02331v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>http://arxiv.org/abs/2505.02350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用稀疏椭球径向基函数网络逼近点云上签名的距离函数（SDF）的机器学习方法，实现了紧凑且精确的表面表示。&lt;h4&gt;背景&lt;/h4&gt;点云表面表示是计算机图形学和视觉中的基本问题。&lt;h4&gt;目的&lt;/h4&gt;通过使用尽可能少的椭球径向基函数（ERBFs）来逼近SDF，实现点云的紧凑和精确的表面表示。&lt;h4&gt;方法&lt;/h4&gt;引入了一种动态多目标优化策略，自适应地添加正则化项，并联合优化ERBFs的权重、中心、形状和方向。为了提高计算效率，采用了基于最近邻的数据结构，并将每个核的计算并行化在CUDA上。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个基准数据集上的广泛实验，证明了该方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为点云表面表示提供了一种高效且准确的解决方案，并公开了相应的代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云表面表示是计算机图形学和视觉中的基本问题。本文提出了一种机器学习方法，使用稀疏椭球径向基函数网络逼近点云的签名距离函数（SDF），从而实现紧凑且精确的表面表示。给定由点云构建的网格点上的SDF值，我们的方法尽可能准确地逼近SDF，即使用尽可能少的椭球径向基函数（ERBFs）来表示点云的SDF。为了平衡稀疏性和逼近精度，引入了一种动态多目标优化策略，该策略自适应地添加正则化项，并联合优化ERBFs的权重、中心、形状和方向。为了提高计算效率，采用了一种基于最近邻的数据结构，将函数计算限制在每个高斯核中心附近的点。进一步地，每个核的计算在CUDA上进行了并行化，这显著提高了优化速度。此外，设计了一种基于分层八叉树的细化策略进行训练。具体来说，使用八叉树晶格结构中的粗网格点初始化和优化网络参数。随后，逐步引入细晶格点以加速模型收敛并提高训练效率。在多个基准数据集上的广泛实验表明，我们的方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。相应的代码可在https://github.com/lianbobo/SE-RBFNet.git上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud surface representation is a fundamental problem in computergraphics and vision. This paper presents a machine learning approach forapproximating the signed distance function (SDF) of a point cloud using sparseellipsoidal radial basis function networks, enabling a compact and accuratesurface representation. Given the SDF values defined on the grid pointsconstructed from the point cloud, our method approximates the SDF accuratelywith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,represent the SDF of a point cloud by sparse ERBFs. To balance sparsity andapproximation precision, a dynamic multi-objective optimization strategy isintroduced, which adaptively adds the regularization terms and jointlyoptimizes the weights, centers, shapes, and orientations of ERBFs. To improvecomputational efficiency, a nearest-neighbor-based data structure is employed,restricting function calculations to points near each Gaussian kernel center.The computations for each kernel are further parallelized on CUDA, whichsignificantly improves the optimization speed. Additionally, a hierarchicaloctree-based refinement strategy is designed for training. Specifically, theinitialization and optimization of network parameters are conducted usingcoarse grid points in the octree lattice structure. Subsequently, fine latticepoints are progressively incorporated to accelerate model convergence andenhance training efficiency. Extensive experiments on multiple benchmarkdatasets demonstrate that our method outperforms previous sparse representationapproaches in terms of accuracy, robustness, and computational efficiency. Thecorresponding code is publicly available athttps://github.com/lianbobo/SE-RBFNet.git.</description>
      <author>example@mail.com (Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen)</author>
      <guid isPermaLink="false">2505.02350v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</title>
      <link>http://arxiv.org/abs/2505.01583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TEMPURA的视频理解框架，旨在提高视觉语言模型对视频的时序理解和因果事件关系处理能力。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在处理视频时，要么压缩视频标记以降低时间分辨率，要么将视频视为未分割的流，这导致无法精确识别事件边界和建模因果依赖关系。&lt;h4&gt;目的&lt;/h4&gt;提高视频时序理解能力，实现细粒度的时间定位。&lt;h4&gt;方法&lt;/h4&gt;TEMPURA采用两阶段训练框架：首先，通过掩码事件预测推理重建缺失事件并生成因果解释；其次，学习视频分割和密集描述，将视频分解为非重叠事件，并配以详细的时间戳对齐描述。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPURA在时序定位和突出检测基准测试中优于强基线模型，证明了将因果推理与细粒度时序分割相结合可以提升视频理解能力。&lt;h4&gt;结论&lt;/h4&gt;TEMPURA框架通过结合因果推理和细粒度时序分割，有效提高了视觉语言模型对视频的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：理解因果事件关系和实现视频中的细粒度时间定位对视觉语言模型来说仍然具有挑战性。现有方法要么压缩视频标记以降低时间分辨率，要么将视频视为未分割的流，这模糊了细粒度事件边界并限制了因果依赖关系的建模。我们提出了TEMPURA（用于动作推理的时序事件掩码预测和理解），一个两阶段训练框架，用于增强视频时序理解。TEMPURA首先应用掩码事件预测推理来重建缺失事件，并从密集事件注释中生成逐步的因果解释，从中汲取有效的填充技术。然后，TEMPURA学习执行视频分割和密集描述，将视频分解为非重叠事件，并配以详细的时间戳对齐描述。我们在我们自己编纂的VER数据集上训练TEMPURA，该数据集包含100万个训练实例和50万个具有时间对齐事件描述和结构化推理步骤的视频。在时序定位和突出检测基准测试中进行的实验表明，TEMPURA优于强基线模型，证实了将因果推理与细粒度时序分割相结合可提高视频理解能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding causal event relationships and achieving fine-grained temporalgrounding in videos remain challenging for vision-language models. Existingmethods either compress video tokens to reduce temporal resolution, or treatvideos as unsegmented streams, which obscures fine-grained event boundaries andlimits the modeling of causal dependencies. We propose TEMPURA (Temporal EventMasked Prediction and Understanding for Reasoning in Action), a two-stagetraining framework that enhances video temporal understanding. TEMPURA firstapplies masked event prediction reasoning to reconstruct missing events andgenerate step-by-step causal explanations from dense event annotations, drawinginspiration from effective infilling techniques. TEMPURA then learns to performvideo segmentation and dense captioning to decompose videos intonon-overlapping events with detailed, timestamp-aligned descriptions. We trainTEMPURA on VER, a large-scale dataset curated by us that comprises 1M traininginstances and 500K videos with temporally aligned event descriptions andstructured reasoning steps. Experiments on temporal grounding and highlightdetection benchmarks demonstrate that TEMPURA outperforms strong baselinemodels, confirming that integrating causal reasoning with fine-grained temporalsegmentation leads to improved video understanding.</description>
      <author>example@mail.com (Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, Wenhao Chai, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2505.01583v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</title>
      <link>http://arxiv.org/abs/2505.02075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了视觉基础模型（VFMs）在密集预测任务中的有效性，并提出了一种任务无关的特征上采样模块来提高VFM特征分辨率。&lt;h4&gt;背景&lt;/h4&gt;随着VFMs的流行，人们对其在密集预测任务中的效果越来越感兴趣，但由于VFM通常产生低分辨率特征，限制了其直接应用。&lt;h4&gt;目的&lt;/h4&gt;为了评估特征上采样方法在VFM上的有效性，本研究以交互式分割（IS）作为新的基准。&lt;h4&gt;方法&lt;/h4&gt;通过使用任务无关的特征上采样模块来提高VFM特征分辨率，并利用交互式分割作为评估环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，选择合适的上采样策略可以显著提高VFM特征的质量。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的方法可以有效地提高VFM在密集预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models (VFMs) are large-scale, pre-trained models thatserve as general-purpose backbones for various computer vision tasks. As VFMs'popularity grows, there is an increasing interest in understanding theireffectiveness for dense prediction tasks. However, VFMs typically producelow-resolution features, limiting their direct applicability in this context.One way to tackle this limitation is by employing a task-agnostic featureupsampling module that refines VFM features resolution. To assess theeffectiveness of this approach, we investigate Interactive Segmentation (IS) asa novel benchmark for evaluating feature upsampling methods on VFMs. Due to itsinherent multimodal input, consisting of an image and a set of user-definedclicks, as well as its dense mask output, IS creates a challenging environmentthat demands comprehensive visual scene understanding. Our benchmarkingexperiments show that selecting appropriate upsampling strategies significantlyimproves VFM features quality. The code is released athttps://github.com/havrylovv/iSegProbe</description>
      <author>example@mail.com (Volodymyr Havrylov, Haiwen Huang, Dan Zhang, Andreas Geiger)</author>
      <guid isPermaLink="false">2505.02075v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery</title>
      <link>http://arxiv.org/abs/2505.02829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 10 figures, 19 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LISAt的视觉-语言模型，用于描述复杂遥感场景、回答相关问题并分割感兴趣的对象。&lt;h4&gt;背景&lt;/h4&gt;现有的分割模型可以识别图像中的预定义对象，但难以处理涉及多个对象的复杂用户查询。&lt;h4&gt;目的&lt;/h4&gt;开发LISAt模型，使其能够描述复杂遥感场景，回答相关问题，并分割感兴趣的对象。&lt;h4&gt;方法&lt;/h4&gt;在新的地理空间推理-分割数据集GRES上训练LISAt，该数据集包含27,615个标注和9,205张图像，以及包含超过100万个问答对的模态预训练数据集PreGRES。&lt;h4&gt;主要发现&lt;/h4&gt;LISAt在遥感描述任务上优于现有的地理空间基础模型RS-GPT4V，在推理分割任务上超越了最先进的开放域模型，分别提高了10.04%和143.36%。&lt;h4&gt;结论&lt;/h4&gt;LISAt模型、数据集和代码可在https://lisat-bair.github.io/LISAt/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmentation models can recognize a pre-defined set of objects in images.However, models that can reason over complex user queries that implicitly referto multiple objects of interest are still in their infancy. Recent advances inreasoning segmentation--generating segmentation masks from complex, implicitquery text--demonstrate that vision-language models can operate across an opendomain and produce reasonable outputs. However, our experiments show that suchmodels struggle with complex remote-sensing imagery. In this work, we introduceLISAt, a vision-language model designed to describe complex remote-sensingscenes, answer questions about them, and segment objects of interest. Wetrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,with 27,615 annotations over 9,205 images, and a multimodal pretrainingdataset, PreGRES, containing over 1 million question-answer pairs. LISAtoutperforms existing geospatial foundation models such as RS-GPT4V by over10.04 % (BLEU-4) on remote-sensing description tasks, and surpassesstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %(gIoU). Our model, datasets, and code are available athttps://lisat-bair.github.io/LISAt/</description>
      <author>example@mail.com (Jerome Quenum, Wen-Han Hsieh, Tsung-Han Wu, Ritwik Gupta, Trevor Darrell, David M. Chan)</author>
      <guid isPermaLink="false">2505.02829v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design</title>
      <link>http://arxiv.org/abs/2505.02168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025 (https://openreview.net/forum?id=rbnf7oe6JQ)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CircuitFusion，这是一种多模态和实现感知的电路编码器，能够将电路编码成支持不同下游电路设计任务的一般表示。&lt;h4&gt;背景&lt;/h4&gt;AI的快速发展依赖于IC的支持，但数字IC的日益复杂使得传统的IC设计过程成本高昂且耗时。&lt;h4&gt;目的&lt;/h4&gt;提出CircuitFusion，以解决传统IC设计过程的成本和时间问题，并提高电路设计效率。&lt;h4&gt;方法&lt;/h4&gt;CircuitFusion融合了三种电路模态：硬件代码、结构图和功能摘要，并识别了电路的四个独特属性：并行执行、功能等效变换、多个设计阶段和电路可重用性。&lt;h4&gt;主要发现&lt;/h4&gt;CircuitFusion在五个不同的电路设计任务上表现优于特定于每个任务的SOTA监督方法，证明了其泛化能力和学习电路固有属性的能力。&lt;h4&gt;结论&lt;/h4&gt;CircuitFusion为电路设计提供了一种新的、高效的方法，能够显著提高设计效率和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements of AI rely on the support of ICs. However, the growingcomplexity of digital ICs makes the traditional IC design process costly andtime-consuming. In recent years, AI-assisted IC design methods havedemonstrated great potential, but most methods are task-specific or focussolely on the circuit structure in graph format, overlooking other circuitmodalities with rich functional information. In this paper, we introduceCircuitFusion, the first multimodal and implementation-aware circuit encoder.It encodes circuits into general representations that support differentdownstream circuit design tasks. To learn from circuits, we propose to fusethree circuit modalities: hardware code, structural graph, and functionalitysummary. More importantly, we identify four unique properties of circuits:parallel execution, functional equivalent transformation, multiple designstages, and circuit reusability. Based on these properties, we propose newstrategies for both the development and application of CircuitFusion: 1) Duringcircuit preprocessing, utilizing the parallel nature of circuits, we split eachcircuit into multiple sub-circuits based on sequential-element boundaries, eachsub-circuit in three modalities. 2) During CircuitFusion pre-training, weintroduce three self-supervised tasks that utilize equivalent transformationsboth within and across modalities. 3) When applying CircuitFusion to downstreamtasks, we propose a new retrieval-augmented inference method, which retrievessimilar known circuits as a reference for predictions. It improves fine-tuningperformance and even enables zero-shot inference. Evaluated on five differentcircuit design tasks, CircuitFusion consistently outperforms the SOTAsupervised method specifically developed for every single task, demonstratingits generalizability and ability to learn circuits' inherent properties.</description>
      <author>example@mail.com (Wenji Fang, Shang Liu, Jing Wang, Zhiyao Xie)</author>
      <guid isPermaLink="false">2505.02168v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2505.02304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GSP-MC的生成式手语描述提示多正对比学习方法，用于手语识别，通过结合检索增强生成和领域特定的大型语言模型，以及多步提示工程和专家验证的手语语料库，生成精确的多部分描述。&lt;h4&gt;背景&lt;/h4&gt;手语识别（SLR）在创建准确标注方面面临挑战，因为同时手动和非手动信号固有的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，将生成式大型语言模型（LLMs）集成到手语识别任务中。&lt;h4&gt;方法&lt;/h4&gt;GSP-MC方法利用检索增强生成（RAG）与领域特定LLMs，结合多步提示工程和专家验证的手语语料库。它还采用双编码器架构，通过概率匹配双向对齐层次骨骼特征与多个文本描述（全局、同义词和部分级别）。该方法结合全局和部分级别的损失，优化KL散度，确保所有相关文本-骨骼对之间的鲁棒对齐，同时捕获手势级别的语义和详细的部件动态。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在中文SLR500（达到97.1%）和土耳其AUTSL数据集（97.07%准确率）上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法的多语言有效性突显了其在开发包容性通信技术方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：手语识别（SLR）在创建准确标注方面面临基本挑战，因为同时手动和非手动信号固有的复杂性。据我们所知，这是首次将生成式大型语言模型（LLMs）集成到手语识别任务中的工作。我们提出了一种名为GSP-MC的新颖的生成式手语描述提示多正对比学习方法，该方法利用检索增强生成（RAG）与领域特定LLMs，结合多步提示工程和专家验证的手语语料库，以生成精确的多部分描述。GSP-MC方法还采用双编码器架构，通过概率匹配双向对齐层次骨骼特征与多个文本描述（全局、同义词和部分级别）。我们的方法结合全局和部分级别的损失，优化KL散度，以确保所有相关文本-骨骼对之间的鲁棒对齐，同时捕获手势级别的语义和详细的部件动态。实验表明，该方法在中文SLR500（达到97.1%）和土耳其AUTSL数据集（97.07%准确率）上优于现有方法。该方法的多语言有效性突显了其在开发包容性通信技术方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language recognition (SLR) faces fundamental challenges in creatingaccurate annotations due to the inherent complexity of simultaneous manual andnon-manual signals. To the best of our knowledge, this is the first work tointegrate generative large language models (LLMs) into SLR tasks. We propose anovel Generative Sign-description Prompts Multi-positive Contrastive learning(GSP-MC) method that leverages retrieval-augmented generation (RAG) withdomain-specific LLMs, incorporating multi-step prompt engineering andexpert-validated sign language corpora to produce precise multipartdescriptions. The GSP-MC method also employs a dual-encoder architecture tobidirectionally align hierarchical skeleton features with multiple textdescriptions (global, synonym, and part level) through probabilistic matching.Our approach combines global and part-level losses, optimizing KL divergence toensure robust alignment across all relevant text-skeleton pairs while capturingboth sign-level semantics and detailed part dynamics. Experiments demonstratestate-of-the-art performance against existing methods on the Chinese SLR500(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method'scross-lingual effectiveness highlight its potential for developing inclusivecommunication technologies.</description>
      <author>example@mail.com (Siyu Liang, Yunan Li, Wentian Xin, Huizhou Chen, Xujie Liu, Kang Liu, Qiguang Miao)</author>
      <guid isPermaLink="false">2505.02304v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
      <link>http://arxiv.org/abs/2505.02173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间序列聚类方法，用于将具有相似行为的时间序列数据分类成组。&lt;h4&gt;背景&lt;/h4&gt;时间序列聚类是一种无监督学习方法，用于将时间序列数据分类。它广泛应用于医疗保健、金融、经济、能源和气候科学等领域。已有多种时间序列聚类方法被提出并使用了四十多年。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的相似性度量方法，并将其应用于时间序列聚类。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为“排序皮尔逊相关差异度”（RDPC）的新差异度量方法，该方法结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异度。该方法被整合到层次聚类中，并与其他聚类算法的性能进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。&lt;h4&gt;结论&lt;/h4&gt;通过将泰国的随机样本客户聚类到具有独特特征的七个组中，展示了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间序列聚类是一种无监督学习方法，用于将具有相似行为的时间序列数据分类成组。它在医疗保健、金融、经济、能源和气候科学等应用中得到了使用。几十年来，已经提出了几种时间序列聚类方法。大多数方法都集中在测量时间序列之间的欧几里得距离或关联差异度。在这项工作中，我们提出了一种新的差异度量方法，称为排序皮尔逊相关差异度（RDPC），它将指定分数的最大元素差异的加权平均与已知的皮尔逊相关差异度相结合。它被整合到层次聚类中。性能得到了评估，并与现有的聚类算法进行了比较。结果表明，RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。最后，我们通过将来自泰国电力消耗时间序列数据集的随机样本客户聚类到具有独特特征的七个组中，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series clustering is an unsupervised learning method for classifyingtime series data into groups with similar behavior. It is used in applicationssuch as healthcare, finance, economics, energy, and climate science. Severaltime series clustering methods have been introduced and used for over fourdecades. Most of them focus on measuring either Euclidean distances orassociation dissimilarities between time series. In this work, we propose a newdissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),which combines a weighted average of a specified fraction of the largestelement-wise differences with the well-known Pearson correlation dissimilarity.It is incorporated into hierarchical clustering. The performance is evaluatedand compared with existing clustering algorithms. The results show that theRDPC algorithm outperforms others in complicated cases involving differentseasonal patterns, trends, and peaks. Finally, we demonstrate our method byclustering a random sample of customers from a Thai electricity consumptiontime series dataset into seven groups with unique characteristics.</description>
      <author>example@mail.com (Chutiphan Charoensuk, Nathakhun Wiroonsri)</author>
      <guid isPermaLink="false">2505.02173v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</title>
      <link>http://arxiv.org/abs/2505.02126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GarmentGS的新方法，用于高保真地重建服装表面，并生成非密封的单层网格。该方法通过密集点云引导，实现快速重建和高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统的3D服装制作需要大量的人工操作，导致时间和劳动力成本高。3D高斯Splatting在3D场景重建和渲染方面取得了突破性进展，为3D服装重建开辟了新的途径。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，以解决传统方法中由于高斯原元的无结构和不规则性质导致的难以重建高保真、非密封的3D服装的问题。&lt;h4&gt;方法&lt;/h4&gt;GarmentGS方法引入了一个快速密集点云重建模块，可以在10分钟内完成服装点云重建，并使用密集点云引导高斯原元的移动、展平和旋转，以在服装表面上实现更好的分布，达到优异的渲染效果和几何精度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了快速训练和实时渲染，同时保持了有竞争力的质量。&lt;h4&gt;结论&lt;/h4&gt;GarmentGS方法通过密集点云引导，能够高效地重建高保真服装表面，为3D服装设计提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的3D服装制作需要大量的人工操作，导致时间和劳动力成本高。近年来，3D高斯Splatting在3D场景重建和渲染方面取得了突破性进展，引起了广泛关注，并为3D服装重建开辟了新的途径。然而，由于高斯原元的无结构和不规则性质，难以重建高保真、非密封的3D服装。在本文中，我们提出了一种名为GarmentGS的密集点云引导方法，可以重建具有高几何精度的服装表面，并生成非密封的单层网格。我们的方法引入了一个快速密集点云重建模块，可以在10分钟内完成服装点云重建，比传统方法所需的几个小时要快。此外，我们使用密集点云来引导高斯原元的移动、展平和旋转，以在服装表面上实现更好的分布，从而实现优异的渲染效果和几何精度。通过数值和可视化比较，我们的方法在保持有竞争力的质量的同时，实现了快速训练和实时渲染。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional 3D garment creation requires extensive manual operations,resulting in time and labor costs. Recently, 3D Gaussian Splatting has achievedbreakthrough progress in 3D scene reconstruction and rendering, attractingwidespread attention and opening new pathways for 3D garment reconstruction.However, due to the unstructured and irregular nature of Gaussian primitives,it is difficult to reconstruct high-fidelity, non-watertight 3D garments. Inthis paper, we present GarmentGS, a dense point cloud-guided method that canreconstruct high-fidelity garment surfaces with high geometric accuracy andgenerate non-watertight, single-layer meshes. Our method introduces a fastdense point cloud reconstruction module that can complete garment point cloudreconstruction in 10 minutes, compared to traditional methods that requireseveral hours. Furthermore, we use dense point clouds to guide the movement,flattening, and rotation of Gaussian primitives, enabling better distributionon the garment surface to achieve superior rendering effects and geometricaccuracy. Through numerical and visual comparisons, our method achieves fasttraining and real-time rendering while maintaining competitive quality.</description>
      <author>example@mail.com (Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao)</author>
      <guid isPermaLink="false">2505.02126v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</title>
      <link>http://arxiv.org/abs/2505.02179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为ProDisc-VAD的弱监督视频异常检测框架，通过两个协同组件解决标签模糊性问题，提高了特征学习的区分度。&lt;h4&gt;背景&lt;/h4&gt;现有的基于MIL的WS-VAD方法在处理标签模糊性时，难以进行有效的特征学习。&lt;h4&gt;目的&lt;/h4&gt;提出ProDisc-VAD框架，旨在解决标签模糊性问题，提高异常检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;ProDisc-VAD包含原型交互层（PIL）和伪实例判别增强（PIDE）损失。PIL使用少量可学习的原型进行正常性建模，而PIDE损失通过对比学习增强最可靠的极端评分实例的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;ProDisc-VAD在ShanghaiTech和UCF-Crime数据集上取得了优异的AUC值（97.98%和87.12%），且参数数量仅为0.4M，远低于基于ViT的方法。&lt;h4&gt;结论&lt;/h4&gt;ProDisc-VAD框架在效率和性能方面表现出色，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;摘要：使用多个实例学习（MIL）的弱监督视频异常检测（WS-VAD）由于标签模糊性而受到阻碍，影响了特征学习的区分度。我们提出了ProDisc-VAD，一个高效的框架，通过两个协同组件来解决这个问题。原型交互层（PIL）通过使用一组可学习的原型提供受控的正常性建模，建立了一个稳健的基线，而不会被主导的正常数据所淹没。伪实例判别增强（PIDE）损失通过仅对最可靠的极端评分实例（最高/最低评分）应用有针对性的对比学习来增强可分性。ProDisc-VAD仅使用0.4M参数就实现了强大的AUCs（ShanghaiTech：97.98%，UCF-Crime：87.12%），比最近的基于ViT的方法如VadCLIP少800多倍，展示了卓越的效率以及最先进的性能。代码可在https://github.com/modadundun/ProDisc-VAD上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly-supervised video anomaly detection (WS-VAD) using Multiple InstanceLearning (MIL) suffers from label ambiguity, hindering discriminative featurelearning. We propose ProDisc-VAD, an efficient framework tackling this via twosynergistic components. The Prototype Interaction Layer (PIL) providescontrolled normality modeling using a small set of learnable prototypes,establishing a robust baseline without being overwhelmed by dominant normaldata. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boostsseparability by applying targeted contrastive learning exclusively to the mostreliable extreme-scoring instances (highest/lowest scores). ProDisc-VADachieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4Mparameters, over 800x fewer than recent ViT-based methods like VadCLIP,demonstrating exceptional efficiency alongside state-of-the-art performance.Code is available at https://github.com/modadundun/ProDisc-VAD.</description>
      <author>example@mail.com (Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu)</author>
      <guid isPermaLink="false">2505.02179v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
      <link>http://arxiv.org/abs/2505.02139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对订单簿（LOB）表示学习进行了系统性比较研究，以识别提取可迁移、紧凑特征的有效方法，这些特征能够捕捉订单簿的基本属性。&lt;h4&gt;背景&lt;/h4&gt;订单簿是金融市场最基本的数据之一，提供了市场动态的精细视图，但由于其强烈的自相关性、交叉特征约束和特征尺度差异，处理深度模型时面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在分析现有方法中代表学习与特定下游任务的紧密耦合，并提出一种有效的方法来提取可迁移的特征。&lt;h4&gt;方法&lt;/h4&gt;引入了LOBench，一个使用真实中国A股市场数据的标准化基准，提供精心制作的数据集、统一的预处理、一致的评估指标和强大的基线。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了订单簿表示对于各种下游任务的充分性和必要性，并突出了其相对于传统任务特定端到端模型和高级表示学习模型在通用时间序列上的优势。&lt;h4&gt;结论&lt;/h4&gt;本研究建立了一个可重复的框架，并为未来的研究提供了明确的指导，数据集和代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constraints, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Limit Order Book (LOB), the mostly fundamental data of the financialmarket, provides a fine-grained view of market dynamics while poses significantchallenges in dealing with the esteemed deep models due to its strongautocorrelation, cross-feature constrains, and feature scale disparity.Existing approaches often tightly couple representation learning with specificdownstream tasks in an end-to-end manner, failed to analyze the learnedrepresentations individually and explicitly, limiting their reusability andgeneralization. This paper conducts the first systematic comparative study ofLOB representation learning, aiming to identify the effective way of extractingtransferable, compact features that capture essential LOB properties. Weintroduce LOBench, a standardized benchmark with real China A-share marketdata, offering curated datasets, unified preprocessing, consistent evaluationmetrics, and strong baselines. Extensive experiments validate the sufficiencyand necessity of LOB representations for various downstream tasks and highlighttheir advantages over both the traditional task-specific end-to-end models andthe advanced representation learning models for general time series. Our workestablishes a reproducible framework and provides clear guidelines for futureresearch. Datasets and code will be publicly available athttps://github.com/financial-simulation-lab/LOBench.</description>
      <author>example@mail.com (Muyao Zhong, Yushi Lin, Peng Yang)</author>
      <guid isPermaLink="false">2505.02139v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora</title>
      <link>http://arxiv.org/abs/2505.02147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的深度学习方法，使用卷积神经网络和迁移学习技术对60种不同的草药进行分类，旨在解决在生物多样性丰富的地区（如尼泊尔）中草药分类的挑战。&lt;h4&gt;背景&lt;/h4&gt;草药分类在植物研究中是一个关键的挑战，特别是在像尼泊尔这样生物多样性丰富的地区。&lt;h4&gt;目的&lt;/h4&gt;开发一个鲁棒的机器学习模型，用于对草药进行分类，以解决现有草药识别方法中的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究使用了12000张草药图片的手动整理数据集，并采用了多种模型架构，包括DenseNet121、ResNet50、VGG16、InceptionV3、EfficientNetV2和Vision Transformer（VIT），其中DenseNet121最终表现出最佳性能。还应用了数据增强和正则化技术来减轻过拟合并提高模型的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121模型在草药分类任务中表现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了草药分类技术的发展，同时保护了传统的植物学知识并促进了草药的可持续利用。&lt;h4&gt;翻译&lt;/h4&gt;Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Herb classification presents a critical challenge in botanical research,particularly in regions with rich biodiversity such as Nepal. This studyintroduces a novel deep learning approach for classifying 60 different herbspecies using Convolutional Neural Networks (CNNs) and transfer learningtechniques. Using a manually curated dataset of 12,000 herb images, wedeveloped a robust machine learning model that addresses existing limitationsin herb recognition methodologies. Our research employed multiple modelarchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,and Vision Transformer (VIT), with DenseNet121 ultimately demonstratingsuperior performance. Data augmentation and regularization techniques wereapplied to mitigate overfitting and enhance the generalizability of the model.This work advances herb classification techniques, preserving traditionalbotanical knowledge and promoting sustainable herb utilization.</description>
      <author>example@mail.com (Prajwal Thapa, Mridul Sharma, Jinu Nyachhyon, Yagya Raj Pandeya)</author>
      <guid isPermaLink="false">2505.02147v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
      <link>http://arxiv.org/abs/2505.01880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5figures. This paper has been accepted for IJCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式音频-语言共学习网络（LOCO），用于定位音频时间伪造区域，旨在解决现有方法依赖昂贵且难以获取的精细标注的问题。&lt;h4&gt;背景&lt;/h4&gt;音频时间伪造定位（ATFL）旨在识别部分伪造音频中被故意修改的精确伪造区域。现有的ATFL方法依赖于使用精细标注来训练高效的网络，而这种标注在现实场景中获取成本高昂且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了克服上述挑战，本文旨在提出一种能够在弱监督场景下提升定位性能的方法。&lt;h4&gt;方法&lt;/h4&gt;本文提出的LOCO网络采用共学习和自监督的方法，具体包括：1）设计了一个音频-语言共学习模块，通过从时间和全局视角对齐语义来捕获伪造一致性特征；2）通过结合语音级别的标注和可学习提示构建伪造感知提示，动态地将语义先验融入时间内容特征；3）应用伪造定位模块，基于融合的伪造类别激活序列生成伪造提案；4）引入渐进式细化策略，生成伪帧级标签，利用监督语义对比学习增强真实内容与伪造内容之间的语义区别，从而不断优化伪造感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;大量的实验表明，提出的LOCO在网络性能上达到了在三个公开基准测试上的SOTA（最先进的技术水平）。&lt;h4&gt;结论&lt;/h4&gt;本文提出的LOCO方法能够有效地定位音频时间伪造区域，并且在公开基准测试中取得了优异的性能，为音频伪造检测领域提供了新的思路和方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio temporal forgery localization (ATFL) aims to find the precise forgeryregions of the partial spoof audio that is purposefully modified. Existing ATFLmethods rely on training efficient networks using fine-grained annotations,which are obtained costly and challenging in real-world scenarios. To meet thischallenge, in this paper, we propose a progressive audio-language co-learningnetwork (LOCO) that adopts co-learning and self-supervision manners to promptlocalization performance under weak supervision scenarios. Specifically, anaudio-language co-learning module is first designed to capture forgeryconsensus features by aligning semantics from temporal and global perspectives.In this module, forgery-aware prompts are constructed by using utterance-levelannotations together with learnable prompts, which can incorporate semanticpriors into temporal content features dynamically. In addition, a forgerylocalization module is applied to produce forgery proposals based on fusedforgery-class activation sequences. Finally, a progressive refinement strategyis introduced to generate pseudo frame-level labels and leverage supervisedsemantic contrastive learning to amplify the semantic distinction between realand fake content, thereby continuously optimizing forgery-aware features.Extensive experiments show that the proposed LOCO achieves SOTA performance onthree public benchmarks.</description>
      <author>example@mail.com (Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo)</author>
      <guid isPermaLink="false">2505.01880v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Deep Representation Learning for Electronic Design Automation</title>
      <link>http://arxiv.org/abs/2505.02105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在电子设计自动化（EDA）领域中，表示学习作为一种有效技术，如何利用自然表示工作流程元素作为图像、网格和图，通过解决电路复杂性增加和严格的功率、性能和面积（PPA）要求，自动从复杂数据格式中提取有意义的特征。&lt;h4&gt;背景&lt;/h4&gt;表示学习已成为EDA算法中的一种有效技术，它利用工作流程元素的自然表示，如图像、网格和图。&lt;h4&gt;目的&lt;/h4&gt;本文旨在分析表示学习在EDA中的应用，包括基础概念，以及时间预测、可布线分析和自动化布局等任务的先前工作和案例研究。&lt;h4&gt;方法&lt;/h4&gt;本文介绍了基于图像的方法、基于图的方法和混合多模态解决方案等关键技术，以展示表示学习在路由、时序和寄生预测方面的改进。&lt;h4&gt;主要发现&lt;/h4&gt;本文提供的进步展示了表示学习在提高当前集成电路设计流程中的效率、准确性和可扩展性方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;表示学习在EDA领域有广泛的应用前景，可以显著提高集成电路设计的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning has become an effective technique utilized byelectronic design automation (EDA) algorithms, which leverage the naturalrepresentation of workflow elements as images, grids, and graphs. By addressingchallenges related to the increasing complexity of circuits and stringentpower, performance, and area (PPA) requirements, representation learningfacilitates the automatic extraction of meaningful features from complex dataformats, including images, grids, and graphs. This paper examines theapplication of representation learning in EDA, covering foundational conceptsand analyzing prior work and case studies on tasks that include timingprediction, routability analysis, and automated placement. Key techniques,including image-based methods, graph-based approaches, and hybrid multimodalsolutions, are presented to illustrate the improvements provided in routing,timing, and parasitic prediction. The provided advancements demonstrate thepotential of representation learning to enhance efficiency, accuracy, andscalability in current integrated circuit design flows.</description>
      <author>example@mail.com (Pratik Shrestha, Saran Phatharodom, Alec Aversa, David Blankenship, Zhengfeng Wu, Ioannis Savidis)</author>
      <guid isPermaLink="false">2505.02105v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Segment Any RGB-Thermal Model with Language-aided Distillation</title>
      <link>http://arxiv.org/abs/2505.01950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2412.04220 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SARTM是一个针对RGB-T语义分割的框架，通过改进SAM模型，并结合语义理解模块，提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;SAM模型在RGB数据上表现良好，但在RGB-T语义分割上有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SARTM框架，以增强SAM模型在RGB-T语义分割中的应用。&lt;h4&gt;方法&lt;/h4&gt;1. 通过添加LoRA层微调SAM模型，保留其泛化能力和分割能力。2. 引入语言信息作为训练指导。3. 引入跨模态知识蒸馏模块（CMKD）以实现模态适应。4. 调整SAM的分割头，并加入辅助语义分割头，融合多尺度特征。&lt;h4&gt;主要发现&lt;/h4&gt;SARTM在三个多模态RGB-T语义分割基准测试中（MFNET、PST900、FMB）表现出色，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SARTM显著提升了RGB-T语义分割的性能，适用于各种视觉条件。&lt;h4&gt;翻译&lt;/h4&gt;The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low-light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation (CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent Segment Anything Model (SAM) demonstrates strong instancesegmentation performance across various downstream tasks. However, SAM istrained solely on RGB data, limiting its direct applicability to RGB-thermal(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution forscene understanding in adverse weather and lighting conditions, such as lowlight and overexposure, we propose a novel framework, SARTM, which customizesthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleashthe potential of SAM while introduce semantic understanding modules for RGB-Tdata pairs. Specifically, our framework first involves fine tuning the originalSAM by adding extra LoRA layers, aiming at preserving SAM's stronggeneralization and segmentation capabilities for downstream tasks. Secondly, weintroduce language information as guidance for training our SARTM. To addresscross-modal inconsistencies, we introduce a Cross-Modal KnowledgeDistillation(CMKD) module that effectively achieves modality adaptation whilemaintaining its generalization capabilities. This semantic module enables theminimization of modality gaps and alleviates semantic ambiguity, facilitatingthe combination of any modality under any visual conditions. Furthermore, weenhance the segmentation performance by adjusting the segmentation head of SAMand incorporating an auxiliary semantic segmentation head, which integratesmulti-scale features for effective fusion. Extensive experiments are conductedacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,and FMB. Both quantitative and qualitative results consistently demonstratethat the proposed SARTM significantly outperforms state-of-the-art approachesacross a variety of conditions.</description>
      <author>example@mail.com (Dong Xing, Xianxun Zhu, Wei Zhou, Qika Lin, Hang Yang, Yuqing Wang)</author>
      <guid isPermaLink="false">2505.01950v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
      <link>http://arxiv.org/abs/2505.02746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用智能网络搜索策略和知识图谱来训练高质量CLIP模型的方法，显著减少了所需数据量，从而降低了训练成本。&lt;h4&gt;背景&lt;/h4&gt;训练高质量的CLIP模型通常需要大量的数据集，这限制了特定领域模型的发展，尤其是在CLIP模型覆盖不佳的领域，同时也增加了训练成本。&lt;h4&gt;目的&lt;/h4&gt;为了解决需要对CLIP模型训练过程进行精细控制的科学研究的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过采用智能网络搜索策略和知识图谱，从少量数据中从头开始训练一个鲁棒的CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;只需要10M张图片就可以构建一个专门针对生物有机体的专家基础模型，并引入了包含3300万张图片和4600万文本描述的EntityNet数据集，这显著缩短了训练通用CLIP模型所需的时间。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地降低了训练高质量CLIP模型的数据需求，为特定领域模型的发展提供了新的可能性，并减少了训练成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training high-quality CLIP models typically requires enormous datasets, whichlimits the development of domain-specific models -- especially in areas thateven the largest CLIP models do not cover well -- and drives up training costs.This poses challenges for scientific research that needs fine-grained controlover the training procedure of CLIP models. In this work, we show that byemploying smart web search strategies enhanced with knowledge graphs, a robustCLIP model can be trained from scratch with considerably less data.Specifically, we demonstrate that an expert foundation model for livingorganisms can be built using just 10M images. Moreover, we introduce EntityNet,a dataset comprising 33M images paired with 46M text descriptions, whichenables the training of a generic CLIP model in significantly reduced time.</description>
      <author>example@mail.com (Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox)</author>
      <guid isPermaLink="false">2505.02746v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
      <link>http://arxiv.org/abs/2505.02573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了联邦图学习（Federated Graph Learning）的一种新范式FedGM，旨在解决现有联邦图学习中数据异质性和隐私风险的问题。&lt;h4&gt;背景&lt;/h4&gt;联邦图学习通过多客户端的图促进图神经网络（GNNs）的协作训练，但现有方法在联邦优化中过于依赖模型参数或梯度的通信，未能有效处理由复杂多变的图分布引入的数据异质性。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为压缩图的优化载体，以及新的FGL范式FedGM，以解决FGL的数据异质性和隐私风险问题。&lt;h4&gt;方法&lt;/h4&gt;FedGM利用广义的压缩图共识聚合分布式图的综合知识，同时通过单次传输压缩数据来最小化通信成本和隐私风险。&lt;h4&gt;主要发现&lt;/h4&gt;在六个公开数据集上的大量实验表明，FedGM优于最先进的基础方案，突显了其在新型FGL范式中的潜力。&lt;h4&gt;结论&lt;/h4&gt;FedGM是一种有效的联邦图学习新方法，可以解决数据异质性和隐私风险问题，具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated graph learning is a widely recognized technique that promotescollaborative training of graph neural networks (GNNs) by multi-clientgraphs.However, existing approaches heavily rely on the communication of modelparameters or gradients for federated optimization and fail to adequatelyaddress the data heterogeneity introduced by intricate and diverse graphdistributions. Although some methods attempt to share additional messages amongthe server and clients to improve federated convergence during communication,they introduce significant privacy risks and increase communication overhead.To address these issues, we introduce the concept of a condensed graph as anovel optimization carrier to address FGL data heterogeneity and propose a newFGL paradigm called FedGM. Specifically, we utilize a generalized condensationgraph consensus to aggregate comprehensive knowledge from distributed graphs,while minimizing communication costs and privacy risks through a singletransmission of the condensed data. Extensive experiments on six publicdatasets consistently demonstrate the superiority of FedGM overstate-of-the-art baselines, highlighting its potential for a novel FGLparadigm.</description>
      <author>example@mail.com (Hao Zhang, Xunkai Li, Yinlin Zhu, Lianglin Hu)</author>
      <guid isPermaLink="false">2505.02573v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
      <link>http://arxiv.org/abs/2505.01743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Llambda的劳动力节约系统，旨在支持低分辨率的人机行为理解（HBU）。该系统通过利用有限的标记数据和大量的未标记数据来指导大视觉语言模型（LVLM）生成信息丰富的字幕，从而有效微调LVLM模型以理解低分辨率视频。&lt;h4&gt;背景&lt;/h4&gt;随着大视觉语言模型（LVLMs）的快速发展，它们在生成关于低分辨率视觉系统（如深度、热成像和红外）的人机行为理解（HBU）描述方面具有超越传统标注的潜力。然而，现有的LVLM方法无法很好地理解低分辨率数据，因为它们主要设计用于处理高分辨率数据，如RGB图像。&lt;h4&gt;目的&lt;/h4&gt;提出一种劳动力节约的系统Llambda，以支持低分辨率HBU。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种对比性导向的数据标注器，可以从长低分辨率视频中捕获与行为相关的信息，并通过对比学习生成高质量的伪标签。2. 提出了一种物理知识引导的字幕生成器，利用空间和时间一致性检查来减轻伪标签中的错误。3. 使用基于LoRA的高效微调技术来确保设备上的部署。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Llambda在区域尺度真实测试平台上使用三个不同的低分辨率数据集进行评估时，平均在Bert-Score上优于几个最先进的LVLM系统，提高了40.03%。&lt;h4&gt;结论&lt;/h4&gt;Llambda系统通过有效利用有限标记数据和大量未标记数据，能够显著提高LVLM模型对低分辨率视频的理解能力，并在实际应用中展现出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements in Large Vision Language Models (LVLMs) offer thepotential to surpass conventional labeling by generating richer, more detaileddescriptions of on-device human behavior understanding (HBU) in low-resolutionvision systems, such as depth, thermal, and infrared. However, existing largevision language model (LVLM) approaches are unable to understand low-resolutiondata well as they are primarily designed for high-resolution data, such as RGBimages. A quick fixing approach is to caption a large amount of low-resolutiondata, but it requires a significant amount of labor-intensive annotationefforts. In this paper, we propose a novel, labor-saving system, Llambda,designed to support low-resolution HBU. The core idea is to leverage limitedlabeled data and a large amount of unlabeled data to guide LLMs in generatinginformative captions, which can be combined with raw data to effectivelyfine-tune LVLM models for understanding low-resolution videos in HBU. First, wepropose a Contrastive-Oriented Data Labeler, which can capturebehavior-relevant information from long, low-resolution videos and generatehigh-quality pseudo labels for unlabeled data via contrastive learning. Second,we propose a Physical-Knowledge Guided Captioner, which utilizes spatial andtemporal consistency checks to mitigate errors in pseudo labels. Therefore, itcan improve LLMs' understanding of sequential data and then generatehigh-quality video captions. Finally, to ensure on-device deployability, weemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.We evaluate Llambda using a region-scale real-world testbed and three distinctlow-resolution datasets, and the experiments show that Llambda outperformsseveral state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.</description>
      <author>example@mail.com (Siyang Jiang, Bufang Yang, Lilin Xu, Mu Yuan, Yeerzhati Abudunuer, Kaiwei Liu, Liekang Zeng, Hongkai Chen, Zhenyu Yan, Xiaofan Jiang, Guoliang Xing)</author>
      <guid isPermaLink="false">2505.01743v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Embracing Diffraction: A Paradigm Shift in Wireless Sensing and Communication</title>
      <link>http://arxiv.org/abs/2505.01625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将电磁衍射作为无线信号传播中的一个重要且未被充分利用的机制，并探讨了其在环境感知和通信中的应用。&lt;h4&gt;背景&lt;/h4&gt;无线信号在现代社会中至关重要，而电磁衍射现象通常被视为次要效应或校正因子。&lt;h4&gt;目的&lt;/h4&gt;通过理解和利用衍射效应，提高无线系统的感知能力和通信策略的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个通用的优化框架，以形式化利用衍射的概念，并讨论了边缘衍射和凯勒的几何衍射理论（GTD）在射频感知和通信中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;衍射诱导的元素具有丰富的信息，可以揭示其几何特性等底层属性。&lt;h4&gt;结论&lt;/h4&gt;本文为将衍射系统地纳入未来无线系统的设计和运行提供了愿景，为增强感知能力和更鲁棒的通信策略铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无线信号是现代社会不可或缺的一部分，它既支持通信，也越来越多地用于环境感知。尽管存在各种传播模型，从经验方法到全波模拟，电磁衍射现象通常被视为次要效应或校正因子。本文将衍射定位为一个基本重要且未被充分利用的机制，它富含关于物理环境的丰富信息。具体来说，衍射诱导的元素产生了丰富的特征，这些特征蕴含了它们底层属性（如几何形状）的信息。我们进一步论证，通过理解和利用这些关系，衍射可以被战略性地利用。我们引入了一个通用的优化框架来形式化这一概念，说明了如何利用衍射来解决逆问题（如从测量场中感知场景细节，如物体几何形状）和正问题（通过配置衍射元素来塑造射频场以实现通信目标）。主要关注边缘衍射和凯勒的几何衍射理论（GTD），我们讨论了在射频感知中用于场景理解以及在通信中用于射频场编程的具体应用，借鉴了最近的研究成果。总的来说，本文为将衍射系统地纳入未来无线系统的设计和运行提供了愿景，为增强感知能力和更鲁棒的通信策略铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless signals are integral to modern society, enabling both communicationand increasingly, environmental sensing. While various propagation modelsexist, ranging from empirical methods to full-wave simulations, the phenomenonof electromagnetic diffraction is often treated as a secondary effect or acorrection factor. This paper positions diffraction as a fundamentallyimportant and underutilized mechanism that is rich with information about thephysical environment. Specifically, diffraction-inducing elements generatedistinct signatures that are rich with information about their underlyingproperties such as their geometries. We then argue that by understanding andexploiting these relationships, diffraction can be harnessed strategically. Weintroduce a general optimization framework to formalize this concept,illustrating how diffraction can be leveraged for both inverse problems(sensing scene details such as object geometries from measured fields) andforward problems (shaping RF fields for communication objectives by configuringdiffracting elements). Focusing primarily on edge diffraction and Keller'sGeometrical Theory of Diffraction (GTD), we discuss specific applications in RFsensing for scene understanding and in communications for RF field programming,drawing upon recent work. Overall, this paper lays out a vision forsystematically incorporating diffraction into the design and operation offuture wireless systems, paving the way for enhanced sensing capabilities andmore robust communication strategies.</description>
      <author>example@mail.com (Anurag Pallaprolu, Winston Hurst, Yasamin Mostofi)</author>
      <guid isPermaLink="false">2505.01625v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning</title>
      <link>http://arxiv.org/abs/2505.02071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为COCA的紧凑聚类注意力层，用于对象中心表示学习，并解决单图像中的无监督对象发现任务。&lt;h4&gt;背景&lt;/h4&gt;在多对象场景中提取对象中心表示，并构建了COCA-Net，一个自下而上的层次网络架构。&lt;h4&gt;目的&lt;/h4&gt;提高对象检测和分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;COCA使用一种新的聚类算法，利用紧凑性的物理概念来突出场景中的不同对象质心，并提供空间归纳偏差。&lt;h4&gt;主要发现&lt;/h4&gt;COCA-Net在解码器和编码器端都生成高质量的分割掩码，且不受预定义对象掩码数量的限制，比竞争对手更好地处理背景元素的分割。&lt;h4&gt;结论&lt;/h4&gt;在六个广泛采用的数据集上，COCA-Net在九个不同的评估指标上实现了优于或与最先进模型相当的结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种紧凑聚类注意力层（COCA），这是一种有效的构建块，它引入了一种面向对象表示学习的高级策略，同时解决了单图像中的无监督对象发现任务。COCA是一个基于注意力的聚类模块，能够从多对象场景中提取对象中心表示，当级联到自下而上的层次网络架构中时，被称为COCA-Net。在其核心，COCA利用了一种新的聚类算法，该算法利用紧凑性的物理概念来突出场景中的不同对象质心，从而提供空间归纳偏差。多亏了这种策略，COCA-Net在其管道的解码器和编码器端都生成了高质量的分割掩码。此外，COCA-Net不受它生成的预定义对象掩码数量的限制，并且比竞争对手更好地处理背景元素的分割。我们在六个广泛采用的数据集上展示了COCA-Net的分割性能，在九个不同的评估指标上与最先进模型相比实现了优越或竞争性的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Compact Clustering Attention (COCA) layer, an effectivebuilding block that introduces a hierarchical strategy for object-centricrepresentation learning, while solving the unsupervised object discovery taskon single images. COCA is an attention-based clustering module capable ofextracting object-centric representations from multi-object scenes, whencascaded into a bottom-up hierarchical network architecture, referred to asCOCA-Net. At its core, COCA utilizes a novel clustering algorithm thatleverages the physical concept of compactness, to highlight distinct objectcentroids in a scene, providing a spatial inductive bias. Thanks to thisstrategy, COCA-Net generates high-quality segmentation masks on both thedecoder side and, notably, the encoder side of its pipeline. Additionally,COCA-Net is not bound by a predetermined number of object masks that itgenerates and handles the segmentation of background elements better than itscompetitors. We demonstrate COCA-Net's segmentation performance on six widelyadopted datasets, achieving superior or competitive results against thestate-of-the-art models across nine different evaluation metrics.</description>
      <author>example@mail.com (Can Küçüksözen, Yücel Yemez)</author>
      <guid isPermaLink="false">2505.02071v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
      <link>http://arxiv.org/abs/2505.02707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures, Website: https://voila.maitrix.org&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Voila是一个大型语音语言基础模型系列，旨在实现一个能够无缝融入日常生活、自主、实时且情感表达的语音AI代理。&lt;h4&gt;背景&lt;/h4&gt;目前语音AI代理主要依赖于命令反应，缺乏自主性和情感表达。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现流畅、动态和情感共鸣的交互的语音AI代理。&lt;h4&gt;方法&lt;/h4&gt;Voila采用了一种新的端到端架构，实现了全双工、低延迟的对话，并保留了丰富的语音细微差别。它集成了大型语言模型（LLMs）的推理能力和强大的声学建模，支持自然、个性化的语音生成。此外，Voila支持超过一百万个预构建的语音和基于简短音频样本的高效定制。&lt;h4&gt;主要发现&lt;/h4&gt;Voila实现了195毫秒的响应延迟，超过了人类的平均反应时间。它能够通过文本指令定义说话者的身份、语气和其他特征，支持自动语音识别（ASR）、文本到语音（TTS）和多语言语音翻译。&lt;h4&gt;结论&lt;/h4&gt;Voila是一个统一模型，适用于各种基于语音的应用，并且是完全开源的，以支持开放研究和加速下一代人机交互的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：一个能够无缝融入日常生活的语音AI代理将以自主、实时和情感表达的方式与人类互动。它不仅会响应命令，还会持续监听、推理并主动响应，从而促进流畅、动态和情感共鸣的交互。我们介绍了Voila，这是一系列大型语音语言基础模型，朝着这个愿景迈出了第一步。Voila超越了传统的管道系统，采用了一种新的端到端架构，实现了全双工、低延迟的对话，同时保留了丰富的语音细微差别，如音调、节奏和情感。它实现了仅195毫秒的响应延迟，超过了人类的平均反应时间。其分层多尺度Transformer集成了大型语言模型（LLMs）的推理能力和强大的声学建模，实现了自然、个性化的语音生成——用户可以通过简单的文本指令来定义说话者的身份、语气和其他特征。此外，Voila支持超过一百万个预构建的语音和基于10秒短音频样本的高效定制。除了语音对话外，Voila还被设计为一个统一模型，适用于广泛的基于语音的应用，包括自动语音识别（ASR）、文本到语音（TTS）和，经过最小适应，多语言语音翻译。Voila是完全开源的，以支持开放研究和加速向下一代人机交互的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A voice AI agent that blends seamlessly into daily life would interact withhumans in an autonomous, real-time, and emotionally expressive manner. Ratherthan merely reacting to commands, it would continuously listen, reason, andrespond proactively, fostering fluid, dynamic, and emotionally resonantinteractions. We introduce Voila, a family of large voice-language foundationmodels that make a step towards this vision. Voila moves beyond traditionalpipeline systems by adopting a new end-to-end architecture that enablesfull-duplex, low-latency conversations while preserving rich vocal nuances suchas tone, rhythm, and emotion. It achieves a response latency of just 195milliseconds, surpassing the average human response time. Its hierarchicalmulti-scale Transformer integrates the reasoning capabilities of large languagemodels (LLMs) with powerful acoustic modeling, enabling natural, persona-awarevoice generation -- where users can simply write text instructions to definethe speaker's identity, tone, and other characteristics. Moreover, Voilasupports over one million pre-built voices and efficient customization of newones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,Voila is designed as a unified model for a wide range of voice-basedapplications, including automatic speech recognition (ASR), Text-to-Speech(TTS), and, with minimal adaptation, multilingual speech translation. Voila isfully open-sourced to support open research and accelerate progress towardnext-generation human-machine interactions.</description>
      <author>example@mail.com (Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu)</author>
      <guid isPermaLink="false">2505.02707v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Robustness questions the interpretability of graph neural networks: what to do?</title>
      <link>http://arxiv.org/abs/2505.02566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出一个全面的基准，用于系统分析影响图神经网络（GNN）可解释性的各种因素，包括增强鲁棒性的防御机制的影响。&lt;h4&gt;背景&lt;/h4&gt;GNN在生物信息学、社交网络和推荐系统等领域得到广泛应用，但其模型可解释性与鲁棒性之间的相互作用在对抗性场景（如中毒和逃避攻击）下理解不足。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过防御中毒和逃避攻击来影响GNN的可解释性，并强调鲁棒性与可解释性之间的关键权衡。&lt;h4&gt;方法&lt;/h4&gt;评估基于GCN、SAGE、GIN和GAT的六种GNN架构，在两个不同领域的五个数据集上使用四个可解释性指标（保真度、稳定性、一致性和稀疏性）。&lt;h4&gt;主要发现&lt;/h4&gt;根据选择的防御方法和模型架构特征，可解释性存在显著差异。&lt;h4&gt;结论&lt;/h4&gt;通过建立标准化基准，本研究为开发既具有鲁棒性又可解释的GNN提供了基础，有助于在敏感应用中部署这些模型时建立信任。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms. We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source. The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a cornerstone in graph-based dataanalysis, with applications in diverse domains such as bioinformatics, socialnetworks, and recommendation systems. However, the interplay between modelinterpretability and robustness remains poorly understood, especially underadversarial scenarios like poisoning and evasion attacks. This paper presents acomprehensive benchmark to systematically analyze the impact of various factorson the interpretability of GNNs, including the influence ofrobustness-enhancing defense mechanisms.  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT acrossfive datasets from two distinct domains, employing four interpretabilitymetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines howdefenses against poisoning and evasion attacks, applied before and during modeltraining, affect interpretability and highlights critical trade-offs betweenrobustness and interpretability. The framework will be published as opensource.  The results reveal significant variations in interpretability depending onthe chosen defense methods and model architecture characteristics. Byestablishing a standardized benchmark, this work provides a foundation fordeveloping GNNs that are both robust to adversarial threats and interpretable,facilitating trust in their deployment in sensitive applications.</description>
      <author>example@mail.com (Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, Ilya Makarov)</author>
      <guid isPermaLink="false">2505.02566v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach</title>
      <link>http://arxiv.org/abs/2505.01947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 29th International Conference on Engineering of  Complex Computer Systems (ICECCS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RADD，一种结合规则挖掘和无监督学习的无人机异常检测方法。&lt;h4&gt;背景&lt;/h4&gt;无人机（UAVs）因其多用途而日益受到关注，但传感器故障可能导致物理不稳定和安全问题。&lt;h4&gt;目的&lt;/h4&gt;为了减少这些风险，本文旨在提出一种能够检测无人机异常并允许操作者在运行时采取预防措施的方法。&lt;h4&gt;方法&lt;/h4&gt;RADD结合了规则挖掘和无监督学习，利用规则捕获传感器和执行器之间的预期关系，并使用无监督学习技术覆盖规则可能遗漏的微妙关系。&lt;h4&gt;主要发现&lt;/h4&gt;RADD在Gazebo模拟器中实现了基于ArduPilot无人机软件的方法，使用44条规则和五个无监督学习模型，成功检测了93.84%的异常，错误警报率低（2.33%）。&lt;h4&gt;结论&lt;/h4&gt;RADD在检测无人机故障方面优于基于LSTM的最新方法，并且可以有效地在运行时部署。&lt;h4&gt;翻译&lt;/h4&gt;UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; UAVs, commonly referred to as drones, have witnessed a remarkable surge inpopularity due to their versatile applications. These cyber-physical systemsdepend on multiple sensor inputs, such as cameras, GPS receivers,accelerometers, and gyroscopes, with faults potentially leading to physicalinstability and serious safety concerns. To mitigate such risks, anomalydetection has emerged as a crucial safeguarding mechanism, capable ofidentifying the physical manifestations of emerging issues and allowingoperators to take preemptive action at runtime. Recent anomaly detectionmethods based on LSTM neural networks have shown promising results, but threechallenges persist: the need for models that can generalise across the diversemission profiles of drones; the need for interpretability, enabling operatorsto understand the nature of detected problems; and the need for capturingdomain knowledge that is difficult to infer solely from log data. Motivated bythese challenges, this paper introduces RADD, an integrated approach to anomalydetection in drones that combines rule mining and unsupervised learning. Inparticular, we leverage rules (or invariants) to capture expected relationshipsbetween sensors and actuators during missions, and utilise unsupervisedlearning techniques to cover more subtle relationships that the rules may havemissed. We implement this approach using the ArduPilot drone software in theGazebo simulator, utilising 44 rules derived across the main phases of dronemissions, in conjunction with an ensemble of five unsupervised learning models.We find that our integrated approach successfully detects 93.84% of anomaliesover six types of faults with a low false positive rate (2.33%), and can bedeployed effectively at runtime. Furthermore, RADD outperforms astate-of-the-art LSTM-based method in detecting the different types of faultsevaluated in our study.</description>
      <author>example@mail.com (Ivan Tan, Wei Minn, Christopher M. Poskitt, Lwin Khin Shar, Lingxiao Jiang)</author>
      <guid isPermaLink="false">2505.01947v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</title>
      <link>http://arxiv.org/abs/2505.02677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了利用视网膜图像和临床数据对中风进行检测和风险评估的影响。&lt;h4&gt;背景&lt;/h4&gt;中风是全球主要的公共卫生问题，深度学习在提高中风诊断和风险评估方面显示出潜力，但现有方法依赖于昂贵的医学成像技术。&lt;h4&gt;目的&lt;/h4&gt;研究视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多模态深度神经网络，处理光学相干断层扫描（OCT）和红外反射视网膜扫描，并结合临床数据，如人口统计学、生命体征和诊断代码。使用自监督学习框架和包含37k扫描的现实世界数据集进行预训练，然后使用较小的标记子集进行微调和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与仅使用单模态图像的基线相比，该框架实现了5%的AUROC提升，与现有最先进的基础模型相比提高了8%。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：中风是全球主要的公共卫生问题，影响着数百万人的健康。深度学习最近在提高中风诊断和风险评估方面显示出希望。然而，现有方法依赖于昂贵的医学成像技术，如计算机断层扫描。最近的研究表明，视网膜成像可以作为一种成本效益高的替代方案，用于评估脑血管健康，因为视网膜和大脑之间存在共同的临床途径。因此，本研究探讨了利用视网膜图像和临床数据对中风检测和风险评估的影响。我们提出了一种多模态深度神经网络，处理光学相干断层扫描（OCT）和红外反射视网膜扫描，并结合临床数据，如人口统计学、生命体征和诊断代码。我们使用自监督学习框架和包含37k扫描的现实世界数据集进行预训练，然后使用较小的标记子集进行微调和评估。我们的实证发现确立了所考虑模态在检测与急性中风相关的视网膜持久效应和预测特定时间范围内的未来风险方面的预测能力。实验结果表明，与仅使用单模态图像的基线相比，我们的提议框架实现了5%的AUROC提升，与现有最先进的基础模型相比提高了8%。总之，我们的研究突出了视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stroke is a major public health problem, affecting millions worldwide. Deeplearning has recently demonstrated promise for enhancing the diagnosis and riskprediction of stroke. However, existing methods rely on costly medical imagingmodalities, such as computed tomography. Recent studies suggest that retinalimaging could offer a cost-effective alternative for cerebrovascular healthassessment due to the shared clinical pathways between the retina and thebrain. Hence, this study explores the impact of leveraging retinal images andclinical data for stroke detection and risk prediction. We propose a multimodaldeep neural network that processes Optical Coherence Tomography (OCT) andinfrared reflectance retinal scans, combined with clinical data, such asdemographics, vital signs, and diagnosis codes. We pretrained our model using aself-supervised learning framework using a real-world dataset consisting of$37$ k scans, and then fine-tuned and evaluated the model using a smallerlabeled subset. Our empirical findings establish the predictive ability of theconsidered modalities in detecting lasting effects in the retina associatedwith acute stroke and forecasting future risk within a specific time horizon.The experimental results demonstrate the effectiveness of our proposedframework by achieving $5$\% AUROC improvement as compared to the unimodalimage-only baseline, and $8$\% improvement compared to an existingstate-of-the-art foundation model. In conclusion, our study highlights thepotential of retinal imaging in identifying high-risk patients and improvinglong-term outcomes.</description>
      <author>example@mail.com (Saeed Shurrab, Aadim Nepal, Terrence J. Lee-St. John, Nicola G. Ghazi, Bartlomiej Piechowski-Jozwiak, Farah E. Shamout)</author>
      <guid isPermaLink="false">2505.02677v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction</title>
      <link>http://arxiv.org/abs/2505.02628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepSparse的深度学习模型，用于稀疏视图锥束CT（CBCT）重建，以减少辐射暴露，并通过实验证明其优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;CBCT是医学领域的重要3D成像技术，但高质量成像所需的辐射暴露对易受伤害的人群引起担忧。稀疏视图重建通过使用较少的X射线投影来减少辐射，但现有方法面临计算需求高和泛化性差的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的深度学习模型DeepSparse，以解决现有稀疏视图CBCT重建方法的问题，并提高重建质量。&lt;h4&gt;方法&lt;/h4&gt;DeepSparse模型采用DiCE网络，该网络整合了多视图2D特征和多尺度3D特征。此外，引入了HyViP框架进行预训练，并在新数据集上进行两步微调。&lt;h4&gt;主要发现&lt;/h4&gt;DeepSparse在重建质量上优于现有方法，为更安全、更高效的CBCT成像铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;DeepSparse模型能够有效减少CBCT成像的辐射暴露，同时保持高质量的重建图像。&lt;h4&gt;翻译&lt;/h4&gt;Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. Sparse-view reconstruction reduces radiation by using fewer X-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. To overcome these limitations, we propose DeepSparse, the first foundation model for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional Cross-Scale Embedding), a novel network that integrates multi-view 2D features and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View Sampling Pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. Extensive experiments and ablation studies demonstrate that our proposed DeepSparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient CBCT imaging.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cone-beam computed tomography (CBCT) is a critical 3D imaging technology inthe medical field, while the high radiation exposure required for high-qualityimaging raises significant concerns, particularly for vulnerable populations.Sparse-view reconstruction reduces radiation by using fewer X-ray projectionswhile maintaining image quality, yet existing methods face challenges such ashigh computational demands and poor generalizability to different datasets. Toovercome these limitations, we propose DeepSparse, the first foundation modelfor sparse-view CBCT reconstruction, featuring DiCE (Dual-DimensionalCross-Scale Embedding), a novel network that integrates multi-view 2D featuresand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid ViewSampling Pretraining) framework, which pretrains the model on large datasetswith both sparse-view and dense-view projections, and a two-step finetuningstrategy to adapt and refine the model for new datasets. Extensive experimentsand ablation studies demonstrate that our proposed DeepSparse achieves superiorreconstruction quality compared to state-of-the-art methods, paving the way forsafer and more efficient CBCT imaging.</description>
      <author>example@mail.com (Yiqun Lin, Hualiang Wang, Jixiang Chen, Jiewen Yang, Jiarong Guo, Xiaomeng Li)</author>
      <guid isPermaLink="false">2505.02628v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition</title>
      <link>http://arxiv.org/abs/2505.01783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为C-PP-COAD的上下文感知预测驱动的在线异常检测框架，用于解决网络安全、医疗保健和工业监控等领域中实时识别行为偏差的问题。&lt;h4&gt;背景&lt;/h4&gt;在线异常检测在网络安全、医疗保健和工业监控等领域至关重要，这些领域需要及时识别偏差以避免关键故障或安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;为了解决有限的真实校准数据带来的挑战，本文旨在提出一种新的异常检测方法，以减少对真实校准数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;C-PP-COAD框架利用合成校准数据来缓解数据稀缺问题，并基于上下文线索自适应地整合真实数据。它使用符合性p值、主动p值统计和在线FDR控制机制来保持严格的异常检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的实验表明，C-PP-COAD显著减少了对外部校准数据的依赖，同时保证了FDR控制的可靠性。&lt;h4&gt;结论&lt;/h4&gt;C-PP-COAD是一种有效的在线异常检测方法，能够减少对真实校准数据的依赖，同时保持对FDR的保证控制。&lt;h4&gt;翻译&lt;/h4&gt;Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online anomaly detection is essential in fields such as cybersecurity,healthcare, and industrial monitoring, where promptly identifying deviationsfrom expected behavior can avert critical failures or security breaches. Whilenumerous anomaly scoring methods based on supervised or unsupervised learninghave been proposed, current approaches typically rely on a continuous stream ofreal-world calibration data to provide assumption-free guarantees on the falsediscovery rate (FDR). To address the inherent challenges posed by limited realcalibration data, we introduce context-aware prediction-powered conformalonline anomaly detection (C-PP-COAD). Our framework strategically leveragessynthetic calibration data to mitigate data scarcity, while adaptivelyintegrating real data based on contextual cues. C-PP-COAD utilizes conformalp-values, active p-value statistics, and online FDR control mechanisms tomaintain rigorous and reliable anomaly detection performance over time.Experiments conducted on both synthetic and real-world datasets demonstratethat C-PP-COAD significantly reduces dependency on real calibration datawithout compromising guaranteed FDR control.</description>
      <author>example@mail.com (Amirmohammad Farzaneh, Osvaldo Simeone)</author>
      <guid isPermaLink="false">2505.01783v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks</title>
      <link>http://arxiv.org/abs/2505.02022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了NbBench，第一个用于纳米抗体表征学习的全面基准套件，旨在提升纳米抗体建模的标准化和可重复性。&lt;h4&gt;背景&lt;/h4&gt;纳米抗体作为单域抗体片段，具有体积小、稳定性高和结合亲和力强的特点，在治疗和诊断领域具有潜在价值。&lt;h4&gt;目的&lt;/h4&gt;为了解决纳米抗体特定建模未充分探索且缺乏统一基准的问题，引入NbBench。&lt;h4&gt;方法&lt;/h4&gt;NbBench包含八个生物意义明确的任务，涵盖结构注释、结合预测和开发性评估，并对十一种代表性模型进行了系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;抗体语言模型在抗原相关任务中表现优秀，但在回归任务（如热稳定性和亲和力）上所有模型的性能都面临挑战，没有单一模型在所有任务中都优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;NbBench通过标准化数据集、任务定义和评估协议，为评估和推进纳米抗体建模提供了一个可重复的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nanobodies, single-domain antibody fragments derived from camelidheavy-chain-only antibodies, exhibit unique advantages such as compact size,high stability, and strong binding affinity, making them valuable tools intherapeutics and diagnostics. While recent advances in pretrained protein andantibody language models (PPLMs and PALMs) have greatly enhanced biomolecularunderstanding, nanobody-specific modeling remains underexplored and lacks aunified benchmark. To address this gap, we introduce NbBench, the firstcomprehensive benchmark suite for nanobody representation learning. Spanningeight biologically meaningful tasks across nine curated datasets, NbBenchencompasses structure annotation, binding prediction, and developabilityassessment. We systematically evaluate eleven representative models--includinggeneral-purpose protein LMs, antibody-specific LMs, and nanobody-specificLMs--in a frozen setting. Our analysis reveals that antibody language modelsexcel in antigen-related tasks, while performance on regression tasks such asthermostability and affinity remains challenging across all models. Notably, nosingle model consistently outperforms others across all tasks. By standardizingdatasets, task definitions, and evaluation protocols, NbBench offers areproducible foundation for assessing and advancing nanobody modeling.</description>
      <author>example@mail.com (Yiming Zhang, Koji Tsuda)</author>
      <guid isPermaLink="false">2505.02022v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Low-Complexity Acoustic Scene Classification with Device Information in the DCASE 2025 Challenge</title>
      <link>http://arxiv.org/abs/2505.01747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Task Description Page:  https://dcase.community/challenge2025/task-low-complexity-acoustic-scene-classification-with-device-information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DCASE 2025挑战赛中低复杂度声场景分类任务及其基线系统，继续关注低复杂度模型、数据效率和设备不匹配问题，并引入了新的关键变化。&lt;h4&gt;背景&lt;/h4&gt;前几届DCASE挑战赛（2022-2024年）已经关注了低复杂度模型、数据效率和设备不匹配问题。&lt;h4&gt;目的&lt;/h4&gt;开发设备特定的模型，利用设备特性，反映实际部署场景中模型对底层硬件的了解。&lt;h4&gt;方法&lt;/h4&gt;今年的任务引入了新的变化，即在推理时提供记录设备信息。训练集与DCASE 2024挑战赛使用的25%子集相匹配，没有对外部数据使用的限制，突出了迁移学习的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;基线系统在十个类别的任务中，使用设备通用模型达到了50.72%的准确率，当使用可用的设备信息时，准确率提高到了51.89%。&lt;h4&gt;结论&lt;/h4&gt;通过提供设备信息，可以开发出更适应特定设备的模型，从而提高声场景分类的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Low-Complexity Acoustic Scene Classification withDevice Information Task of the DCASE 2025 Challenge and its baseline system.Continuing the focus on low-complexity models, data efficiency, and devicemismatch from previous editions (2022--2024), this year's task introduces a keychange: recording device information is now provided at inference time. Thisenables the development of device-specific models that leverage devicecharacteristics -- reflecting real-world deployment scenarios in which a modelis designed with awareness of the underlying hardware. The training set matchesthe 25% subset used in the corresponding DCASE 2024 challenge, with norestrictions on external data use, highlighting transfer learning as a centraltopic. The baseline achieves 50.72% accuracy on this ten-class problem with adevice-general model, improving to 51.89% when using the available deviceinformation.</description>
      <author>example@mail.com (Florian Schmid, Paul Primus, Toni Heittola, Annamaria Mesaros, Irene Martín-Morató, Gerhard Widmer)</author>
      <guid isPermaLink="false">2505.01747v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction</title>
      <link>http://arxiv.org/abs/2505.02043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从点云数据直接预测并重建CAD模型的网络方法，通过改进的Transformer直接检测和预测草图曲线，重建拓扑结构并优化曲线参数，实现高精度的CAD模型重建。&lt;h4&gt;背景&lt;/h4&gt;现有的CAD模型重建方法通常使用隐式场来表示草图，导致曲线形状的重建精度不高。&lt;h4&gt;目的&lt;/h4&gt;提高从点云数据重建CAD模型的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Point2Primitive的CAD重建网络，该网络通过直接预测扩展原语的所有元素来生成可编辑的CAD模型。使用改进的Transformer直接从点云中检测和预测草图曲线，并将草图曲线参数作为位置查询进行自回归优化，重建拓扑结构并通过结合预测曲线和计算出的扩展操作来恢复每个扩展参数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在原语预测精度和CAD重建方面表现出色，重建的形状具有较高的几何精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为从点云数据重建CAD模型提供了一种有效且准确的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：从点云恢复CAD模型，尤其是草图扩展过程，可以看作是重建拓扑结构和扩展原语的过程。先前的方法利用隐式场来表示草图，导致曲线形状的重建。在本文中，我们提出了一种CAD重建网络（Point2Primitive），它通过直接预测扩展原语的每个元素从输入点云生成可编辑的CAD模型。Point2Primitive基于改进的Transformer直接从点云中检测和预测草图曲线（类型和参数）。草图曲线参数被表示为位置查询并以自回归方式进行优化，从而实现高参数精度。拓扑结构通过扩展分割重建，每个扩展参数（草图和扩展操作）通过结合预测曲线和计算出的扩展操作来恢复。大量实验表明，我们的方法在原语预测精度和CAD重建方面优于现有方法。重建的形状具有高几何保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering CAD models from point clouds, especially the sketch-extrusionprocess, can be seen as the process of rebuilding the topology and extrusionprimitives. Previous methods utilize implicit fields for sketch representation,leading to shape reconstruction of curved edges. In this paper, we proposed aCAD reconstruction network that produces editable CAD models from input pointclouds (Point2Primitive) by directly predicting every element of the extrusionprimitives. Point2Primitive can directly detect and predict sketch curves (typeand parameter) from point clouds based on an improved transformer. The sketchcurve parameters are formulated as position queries and optimized in anautoregressive way, leading to high parameter accuracy. The topology is rebuiltby extrusion segmentation, and each extrusion parameter (sketch and extrusionoperation) is recovered by combining the predicted curves and the computedextrusion operation. Extensive experiments demonstrate that our method issuperior in primitive prediction accuracy and CAD reconstruction. Thereconstructed shapes are of high geometrical fidelity.</description>
      <author>example@mail.com (Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang)</author>
      <guid isPermaLink="false">2505.02043v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>MolQAE: Quantum Autoencoder for Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2505.01875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为量子分子自动编码器的新方法，该方法将量子计算与分子表示学习相结合，旨在解决传统分子表示方法在处理高维数据时遇到的计算瓶颈。&lt;h4&gt;背景&lt;/h4&gt;传统分子表示方法在处理高维数据时存在计算瓶颈，而量子计算通过其固有的并行性和量子叠加特性提供了有希望的替代方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于量子电路的自动编码器架构，将SMILES分子表示映射到量子状态空间，并利用SWAP测试评估编码质量。&lt;h4&gt;方法&lt;/h4&gt;量子自动编码器架构通过参数化量子电路进行降维，并使用SWAP测试来评估编码质量。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明，该方法可以在指数级较小的空间中保留分子特征，同时保持分子之间的相似性关系。实验结果表明，量子自动编码器有效地捕捉了分子结构和化学性质。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅为分子表示学习建立了量子途径，也为药物发现和材料设计开辟了新的可能性。作为分子表示学习与量子计算交叉领域的首次研究，本研究为化学信息学的发展奠定了理论和实践基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为量子分子自动编码器的新方法，该方法将量子计算与分子表示学习相结合。传统分子表示方法在处理高维数据时存在计算瓶颈，而量子计算通过其固有的并行性和量子叠加特性提供了有希望的替代方案。本文提出了一种基于量子电路的自动编码器架构，将SMILES分子表示映射到量子状态空间，并利用SWAP测试评估编码质量。理论研究表明，该方法可以在指数级较小的空间中保留分子特征，同时保持分子之间的相似性关系。实验结果表明，量子自动编码器有效地捕捉了分子结构和化学性质。该框架不仅为分子表示学习建立了量子途径，也为药物发现和材料设计开辟了新的可能性。作为分子表示学习与量子计算交叉领域的首次研究，本研究为化学信息学的发展奠定了理论和实践基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces the Quantum Molecular Autoencoder, a novel approachthat integrates quantum computing with molecular representation learning. Whileconventional molecular representation methods face computational bottleneckswhen processing high-dimensional data, quantum computing offers a promisingalternative through its inherent parallelism and quantum superpositionproperties. We present a quantum circuit-based autoencoder architecture thatmaps SMILES molecular representations into quantum state space, employsparameterized quantum circuits for dimensional reduction, and utilizes SWAPtests to evaluate encoding quality. Theoretically, our approach preservesessential molecular features in exponentially smaller spaces while maintainingsimilarity relationships between molecules. Experimental results demonstratethat quantum autoencoders effectively capture molecular structures and chemicalproperties. The proposed framework not only establishes a quantum pathway formolecular representation learning but also opens new possibilities forapplications in drug discovery and materials design. As the first investigationat the intersection of molecular representation learning and quantum computing,this research lays both theoretical and practical foundations for theadvancement of cheminformatics.</description>
      <author>example@mail.com (Yi Pan, Hanqi Jiang, Wei Ruan, Dajiang Zhu, Xiang Li, Yohannes Abate, Yingfeng Wang, Tianming Liu)</author>
      <guid isPermaLink="false">2505.01875v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments</title>
      <link>http://arxiv.org/abs/2505.01632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的神经网络框架，用于解决非平稳环境噪声对自动语音识别的影响。&lt;h4&gt;背景&lt;/h4&gt;非平稳环境噪声对自动语音识别系统的影响是一个持续且重要的研究焦点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经网络框架，以克服非平稳环境噪声带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了鲁棒的前端处理，并在Aurora-2语音数据库上使用基于ResNet的迁移学习方法，对Mel频率声学特征集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与卷积神经网络和长短期记忆网络相比，该方法在识别准确率上有显著提升，在干净和噪声环境下的准确率分别为98.94%和91.21%。&lt;h4&gt;结论&lt;/h4&gt;该神经网络框架在自动语音识别中有效提高了识别准确率，特别是在噪声环境下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICTIS62692.2024.10894239&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing the detrimental impact of non-stationary environmental noise onautomatic speech recognition (ASR) has been a persistent and significantresearch focus. Despite advancements, this challenge continues to be a majorconcern. Recently, data-driven supervised approaches, such as deep neuralnetworks, have emerged as promising alternatives to traditional unsupervisedmethods. With extensive training, these approaches have the potential toovercome the challenges posed by diverse real-life acoustic environments. Inthis light, this paper introduces a novel neural framework that incorporates arobust frontend into ASR systems in both clean and noisy environments.Utilizing the Aurora-2 speech database, the authors evaluate the effectivenessof an acoustic feature set for Mel-frequency, employing the approach oftransfer learning based on Residual neural network (ResNet). The experimentalresults demonstrate a significant improvement in recognition accuracy comparedto convolutional neural networks (CNN) and long short-term memory (LSTM)networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.</description>
      <author>example@mail.com (Noussaiba Djeffal, Djamel Addou, Hamza Kheddar, Sid Ahmed Selouani)</author>
      <guid isPermaLink="false">2505.01632v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.01969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的多类别3D异常检测（MC3D-AD）模型，旨在利用局部和全局几何感知信息来重建所有类别的正常表示，以提高检测效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的3D异常检测方法通常需要针对每个类别独立训练模型，导致成本高、效率低和泛化能力弱。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的模型，实现多类别3D异常检测，利用局部和全局几何感知信息来重建正常表示。&lt;h4&gt;方法&lt;/h4&gt;1. 提出自适应几何感知掩码注意力模块，提取几何变化信息以引导掩码注意力；2. 引入局部几何感知编码器，增强掩码注意力以编码组级特征标记；3. 设计全局查询解码器，利用点云位置嵌入来改善解码过程和重建能力。&lt;h4&gt;主要发现&lt;/h4&gt;MC3D-AD在Real3D-AD和Anomaly-ShapeNet数据集上评估，在对象级AUROC上分别比现有单类别方法提高了3.1%和9.3%。&lt;h4&gt;结论&lt;/h4&gt;MC3D-AD模型在多类别3D异常检测任务中表现出显著优越性，能够有效提高检测效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1% and 9.3% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Anomaly Detection (AD) is a promising means of controlling the quality ofmanufactured products. However, existing methods typically require carefullytraining a task-specific model for each category independently, leading to highcost, low efficiency, and weak generalization. Therefore, this paper presents anovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aimsto utilize both local and global geometry-aware information to reconstructnormal representations of all categories. First, to learn robust andgeneralized features of different categories, we propose an adaptivegeometry-aware masked attention module that extracts geometry variationinformation to guide mask attention. Then, we introduce a local geometry-awareencoder reinforced by the improved mask attention to encode group-level featuretokens. Finally, we design a global query decoder that utilizes point cloudposition embeddings to improve the decoding process and reconstruction ability.This leads to local and global geometry-aware reconstructed feature tokens forthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD andAnomaly-ShapeNet datasets, and exhibits significant superiority over currentstate-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvementin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. Thesource code will be released upon acceptance.</description>
      <author>example@mail.com (Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, Jinbao Wang)</author>
      <guid isPermaLink="false">2505.01969v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Machine-learning interatomic potentials from a users perspective: A comparison of accuracy, speed and data efficiency</title>
      <link>http://arxiv.org/abs/2505.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习原子间势（MLIPs）显著改变了原子建模领域，提高了密度泛函理论在大规模模拟中的精度，同时接近经典原子间势的运行速度。&lt;h4&gt;背景&lt;/h4&gt;近年来，开发了多种类型的MLIPs，但对于特定问题设置，判断哪种方法最佳往往很困难。&lt;h4&gt;目的&lt;/h4&gt;针对结构和化学上复杂的固体（如Al-Cu-Zr和Si-O），比较了多种MLIPs方法，包括GAP、HDNNP、MTP、线性和非线性ACE、NequIP、Allegro和MACE。&lt;h4&gt;方法&lt;/h4&gt;进行了基准测试，并分析了非线性ACE、NequIP和MACE在精度与计算成本之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;非线性ACE、NequIP和MACE在精度与计算成本之间达到Pareto最优。对于Al-Cu-Zr系统，MACE和Allegro提供最高精度，而NequIP在Si-O系统中表现更优。GPU可以大幅加速MLIPs，使其在可访问的时间尺度上与未加速的经典原子间势相当甚至更优。&lt;h4&gt;结论&lt;/h4&gt;探讨了相应势的外推行为，研究了势能表面的平滑性，并评估了相应的拟合代码和分子动力学界面的用户友好性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：机器学习原子间势（MLIPs）极大地改变了原子建模领域。它们在大型模拟中提高了密度泛函理论的精度，同时几乎与经典原子间势一样快。在过去的几年里，开发了各种类型的MLIPs，但对于给定的问题设置，判断哪种方法最佳往往很困难。对于结构和化学上复杂的固体，即Al-Cu-Zr和Si-O，我们对一系列机器学习原子间势方法进行了基准测试，特别是高斯近似势（GAP）、高维神经网络势（HDNNP）、矩张量势（MTP）、线性和非线性原子簇展开（ACE）、神经网络等变原子间势（NequIP）、Allegro和MACE。我们发现非线性ACE、等变的消息传递图神经网络NequIP和MACE在精度与计算成本权衡中形成了Pareto前沿。在Al-Cu-Zr系统中，我们发现MACE和Allegro提供了最高的精度，而NequIP在Si-O中表现更优。此外，GPU可以大幅加速MLIPs，使它们在可访问的时间尺度上与未加速的经典原子间势相当甚至更优。最后，我们探讨了相应势的外推行为，探究了势能表面的平滑性，并最终估计了相应的拟合代码和分子动力学界面的用户友好性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials (MLIPs) have massively changed thefield of atomistic modeling. They enable the accuracy of density functionaltheory in large-scale simulations while being nearly as fast as classicalinteratomic potentials. Over the last few years, a wide range of differenttypes of MLIPs have been developed, but it is often difficult to judge whichapproach is the best for a given problem setting. For the case of structurallyand chemically complex solids, namely Al-Cu-Zr and Si-O, we benchmark a rangeof machine learning interatomic potential approaches, in particular, theGaussian approximation potential (GAP), high-dimensional neural networkpotentials (HDNNP), moment tensor potentials (MTP), the atomic clusterexpansion (ACE) in its linear and nonlinear version, neural equivariantinteratomic potentials (NequIP), Allegro, and MACE. We find that nonlinear ACEand the equivariant, message-passing graph neural networks NequIP and MACE formthe Pareto front in the accuracy vs. computational cost trade-off. In case ofthe Al-Cu-Zr system we find that MACE and Allegro offer the highest accuracy,while NequIP outperforms them for Si-O. Furthermore, GPUs can massivelyaccelerate the MLIPs, bringing them on par with and even ahead ofnon-accelerated classical interatomic potentials (IPs) with regards toaccessible timescales. Finally, we explore the extrapolation behavior of thecorresponding potentials, probe the smoothness of the potential energysurfaces, and finally estimate the user friendliness of the correspondingfitting codes and molecular dynamics interfaces.</description>
      <author>example@mail.com (Niklas Leimeroth, Linus C. Erhard, Karsten Albe, Jochen Rohrer)</author>
      <guid isPermaLink="false">2505.02503v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement</title>
      <link>http://arxiv.org/abs/2505.01831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review at Neural Networks&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MTRL-FIE的多尺度目标感知表示学习框架，用于高效的眼底图像增强。&lt;h4&gt;背景&lt;/h4&gt;高质量的眼底图像对于临床筛查和眼科疾病诊断至关重要，但由于硬件限制、操作可变性和患者依从性，眼底图像往往存在分辨率低和信噪比低的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了一种新的图像增强框架，以恢复全面的多尺度信息，并针对图像增强的目标，如病变，进行优化。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个多尺度特征编码器（MFE），使用小波分解嵌入低频结构信息和高频细节，以及一个结构保持的层次解码器（SHD），用于融合多尺度特征嵌入以实现真实眼底图像的恢复。SHD集成了层次融合和组注意力机制，同时使用目标感知特征聚合（TFA）模块来增强病理区域并减少伪影。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MTRL-FIE在多个眼底图像数据集上展现了有效性和泛化能力，与现有方法相比，MTRL-FIE在更轻量级的架构下实现了更优越的增强性能。&lt;h4&gt;结论&lt;/h4&gt;MTRL-FIE不仅能够有效增强眼底图像，而且可以泛化到其他眼科图像处理任务，无需监督微调，具有临床应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a multi-scale target-aware representation learning framework named MTRL-FIE for efficient fundus image enhancement.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality fundus images provide essential anatomical information forclinical screening and ophthalmic disease diagnosis. Yet, due to hardwarelimitations, operational variability, and patient compliance, fundus imagesoften suffer from low resolution and signal-to-noise ratio. Recent years havewitnessed promising progress in fundus image enhancement. However, existingworks usually focus on restoring structural details or global characteristicsof fundus images, lacking a unified image enhancement framework to recovercomprehensive multi-scale information. Moreover, few methods pinpoint thetarget of image enhancement, e.g., lesions, which is crucial for medicalimage-based diagnosis. To address these challenges, we propose a multi-scaletarget-aware representation learning framework (MTRL-FIE) for efficient fundusimage enhancement. Specifically, we propose a multi-scale feature encoder (MFE)that employs wavelet decomposition to embed both low-frequency structuralinformation and high-frequency details. Next, we design a structure-preservinghierarchical decoder (SHD) to fuse multi-scale feature embeddings for realfundus image restoration. SHD integrates hierarchical fusion and groupattention mechanisms to achieve adaptive feature fusion while retaining localstructural smoothness. Meanwhile, a target-aware feature aggregation (TFA)module is used to enhance pathological regions and reduce artifacts.Experimental results on multiple fundus image datasets demonstrate theeffectiveness and generalizability of MTRL-FIE for fundus image enhancement.Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancementperformance with a more lightweight architecture. Furthermore, our approachgeneralizes to other ophthalmic image processing tasks without supervisedfine-tuning, highlighting its potential for clinical applications.</description>
      <author>example@mail.com (Haofan Wu, Yin Huang, Yuqing Wu, Qiuyu Yang, Bingfang Wang, Li Zhang, Muhammad Fahadullah Khan, Ali Zia, M. Saleh Memon, Syed Sohail Bukhari, Abdul Fattah Memon, Daizong Ji, Ya Zhang, Ghulam Mustafa, Yin Fang)</author>
      <guid isPermaLink="false">2505.01831v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Text to Image Generation and Editing: A Survey</title>
      <link>http://arxiv.org/abs/2505.02527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages,3 figures,3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对2021年至2024年间141篇关于文本到图像生成（T2I）的研究进行了全面综述。&lt;h4&gt;背景&lt;/h4&gt;近年来，T2I技术受到广泛关注，涌现出大量研究成果。&lt;h4&gt;目的&lt;/h4&gt;本文旨在为未来研究者提供有价值的指导，并推动T2I领域的发展。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了T2I的四种基础模型架构（自回归、非自回归、GAN和扩散）以及常用的关键技术（自动编码器、注意力和无分类器引导）。然后，系统比较了这些研究在T2I生成和T2I编辑两个方向上的方法，包括编码器及其使用的关键技术。此外，还从数据集、评估指标、训练资源和推理速度等方面对比了这些研究的性能。除了四种基础模型外，还调查了T2I领域的其他工作，如基于能量的模型、最新的Mamba和多模态模型。最后，探讨了T2I的社会影响并提供了一些解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出了提高T2I模型性能的独特见解和可能的未来发展方向。&lt;h4&gt;结论&lt;/h4&gt;本文是T2I领域的首次系统性和全面性概述，为未来研究者提供了宝贵的参考，并刺激了该领域的持续进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image generation (T2I) refers to the text-guided generation ofhigh-quality images. In the past few years, T2I has attracted widespreadattention and numerous works have emerged. In this survey, we comprehensivelyreview 141 works conducted from 2021 to 2024. First, we introduce fourfoundation model architectures of T2I (autoregression, non-autoregression, GANand diffusion) and the commonly used key technologies (autoencoder, attentionand classifier-free guidance). Secondly, we systematically compare the methodsof these studies in two directions, T2I generation and T2I editing, includingthe encoders and the key technologies they use. In addition, we also comparethe performance of these researches side by side in terms of datasets,evaluation metrics, training resources, and inference speed. In addition to thefour foundation models, we survey other works on T2I, such as energy-basedmodels and recent Mamba and multimodality. We also investigate the potentialsocial impact of T2I and provide some solutions. Finally, we propose uniqueinsights of improving the performance of T2I models and possible futuredevelopment directions. In summary, this survey is the first systematic andcomprehensive overview of T2I, aiming to provide a valuable guide for futureresearchers and stimulate continued progress in this field.</description>
      <author>example@mail.com (Pengfei Yang, Ngai-Man Cheung, Xinda Ma)</author>
      <guid isPermaLink="false">2505.02527v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis</title>
      <link>http://arxiv.org/abs/2505.01785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TV-SurvCaus的新框架，用于估计时间变化治疗对生存结果的影响，并证明了其在动态治疗制度中的有效性。&lt;h4&gt;背景&lt;/h4&gt;在医学等领域，治疗协议会随时间变化，这使得估计时间变化治疗对生存结果的影响成为一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过引入TV-SurvCaus框架，扩展表示平衡技术，以处理时间变化治疗设置下的生存分析。&lt;h4&gt;方法&lt;/h4&gt;TV-SurvCaus框架通过以下方法提供理论保证：(1) 对估计异质性效应的时间变化精度提供了一般性界限；(2) 通过顺序平衡权重控制方差；(3) 对动态治疗制度的一致性结果；(4) 对于具有时间依赖性的表示学习提供了收敛速率；(5) 对治疗-混杂因素反馈的偏差提供了正式界限。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的广泛实验，证明了TV-SurvCaus在估计具有时间变化协变量和治疗的个体化治疗效果方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该框架通过在动态、纵向设置中更准确地估计治疗效应，推动了因果推断领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：估计时间变化治疗对生存结果的影响在许多领域都是一个具有挑战性的任务，尤其是在医学领域，治疗协议会随时间变化。尽管最近在表示学习方面的进展已经改善了静态治疗中的因果推断，但将这些方法扩展到具有生存结果的时间变化治疗制度仍然没有得到充分探索。在本文中，我们引入了TV-SurvCaus，这是一种新颖的框架，它将表示平衡技术扩展到时间变化治疗设置下的生存分析。我们通过以下方式提供理论保证：(1) 对估计异质性效应的时间变化精度提供了一般性界限；(2) 通过顺序平衡权重控制方差；(3) 对动态治疗制度的一致性结果；(4) 对于具有时间依赖性的表示学习提供了收敛速率；(5) 对治疗-混杂因素反馈的偏差提供了正式界限。我们的神经架构通过序列建模来处理时间依赖性，同时平衡时间依赖性表示。通过在合成和真实世界数据集上的广泛实验，我们证明了TV-SurvCaus在估计具有时间变化协变量和治疗的个体化治疗效果方面优于现有方法。我们的框架通过在动态、纵向设置中更准确地估计治疗效应，推动了因果推断领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the causal effect of time-varying treatments on survival outcomesis a challenging task in many domains, particularly in medicine where treatmentprotocols adapt over time. While recent advances in representation learninghave improved causal inference for static treatments, extending these methodsto dynamic treatment regimes with survival outcomes remains under-explored. Inthis paper, we introduce TV-SurvCaus, a novel framework that extendsrepresentation balancing techniques to the time-varying treatment setting forsurvival analysis. We provide theoretical guarantees through (1) a generalizedbound for time-varying precision in estimation of heterogeneous effects, (2)variance control via sequential balancing weights, (3) consistency results fordynamic treatment regimes, (4) convergence rates for representation learningwith temporal dependencies, and (5) a formal bound on the bias due totreatment-confounder feedback. Our neural architecture incorporates sequencemodeling to handle temporal dependencies while balancing time-dependentrepresentations. Through extensive experiments on both synthetic and real-worlddatasets, we demonstrate that TV-SurvCaus outperforms existing methods inestimating individualized treatment effects with time-varying covariates andtreatments. Our framework advances the field of causal inference by enablingmore accurate estimation of treatment effects in dynamic, longitudinal settingswith survival outcomes.</description>
      <author>example@mail.com (Ayoub Abraich)</author>
      <guid isPermaLink="false">2505.01785v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Beyond the model: Key differentiators in large language models and multi-agent services</title>
      <link>http://arxiv.org/abs/2505.02489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大语言模型（LLMs）不再是生成AI的唯一定义因素，优化周边生态系统成为关键。&lt;h4&gt;背景&lt;/h4&gt;随着DeepSeek、Manus AI和Llama 4等基础模型的推出，LLMs不再独占鳌头。&lt;h4&gt;目的&lt;/h4&gt;分析确保现代AI服务高效和盈利的关键不同点。&lt;h4&gt;方法&lt;/h4&gt;探讨数据质量和管理、计算效率、延迟和评估框架等方面的优化。&lt;h4&gt;主要发现&lt;/h4&gt;优化周边生态系统对于生成AI至关重要。&lt;h4&gt;结论&lt;/h4&gt;除了模型大小，数据管理、计算效率等因素对AI服务的效率与盈利性具有决定性影响。&lt;h4&gt;翻译&lt;/h4&gt;With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, ithas become evident that large language models (LLMs) are no longer the soledefining factor in generative AI. As many now operate at comparable levels ofcapability, the real race is not about having the biggest model but optimizingthe surrounding ecosystem, including data quality and management, computationalefficiency, latency, and evaluation frameworks. This review article delves intothese critical differentiators that ensure modern AI services are efficient andprofitable.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.30574/wjarr.2025.26.1.1295&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, ithas become evident that large language models (LLMs) are no longer the soledefining factor in generative AI. As many now operate at comparable levels ofcapability, the real race is not about having the biggest model but optimizingthe surrounding ecosystem, including data quality and management, computationalefficiency, latency, and evaluation frameworks. This review article delves intothese critical differentiators that ensure modern AI services are efficient andprofitable.</description>
      <author>example@mail.com (Muskaan Goyal, Pranav Bhasin)</author>
      <guid isPermaLink="false">2505.02489v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Real-time Spatial Retrieval Augmented Generation for Urban Environments</title>
      <link>http://arxiv.org/abs/2505.02271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了生成式人工智能，特别是大型语言模型在城市化应用中的潜力，并提出了一个基于FIWARE的实时空间RAG架构，以解决传统RAG架构在城市化环境中的不足。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能在城市化应用中具有变革性潜力，但基础模型存在局限性，更新耗时且成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种实时空间RAG架构，以有效整合生成式人工智能到城市中，并解决传统RAG架构在城市化环境中的不足。&lt;h4&gt;方法&lt;/h4&gt;使用FIWARE开发实时空间RAG架构，利用链接数据的时空过滤能力。&lt;h4&gt;主要发现&lt;/h4&gt;该架构在Madrid市的旅游助手用例中得到了验证，证明了通过RAG架构正确整合基础模型的可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的实时空间RAG架构能够有效解决城市化环境中的信息注入问题，为生成式人工智能在城市化中的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The paper discusses the potential of generative artificial intelligence, especially large language models, in urban applications, and proposes a real-time spatial RAG architecture based on FIWARE to address the limitations of traditional RAG architectures in urban contexts.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Generative Artificial Ingelligence (AI), especiallyLarge Language Models, presents transformative opportunities for urbanapplications through Urban Foundation Models. However, base models facelimitations, as they only contain the knowledge available at the time oftraining, and updating them is both time-consuming and costly. RetrievalAugmented Generation (RAG) has emerged in the literature as the preferredapproach for injecting contextual information into Foundation Models. Itprevails over techniques such as fine-tuning, which are less effective indynamic, real-time scenarios like those found in urban environments. However,traditional RAG architectures, based on semantic databases, knowledge graphs,structured data, or AI-powered web searches, do not fully meet the demands ofurban contexts. Urban environments are complex systems characterized by largevolumes of interconnected data, frequent updates, real-time processingrequirements, security needs, and strong links to the physical world. This workproposes a real-time spatial RAG architecture that defines the necessarycomponents for the effective integration of generative AI into cities,leveraging temporal and spatial filtering capabilities through linked data. Theproposed architecture is implemented using FIWARE, an ecosystem of softwarecomponents to develop smart city solutions and digital twins. The design andimplementation are demonstrated through the use case of a tourism assistant inthe city of Madrid. The use case serves to validate the correct integration ofFoundation Models through the proposed RAG architecture.</description>
      <author>example@mail.com (David Nazareno Campo, Javier Conde, Álvaro Alonso, Gabriel Huecas, Joaquín Salvachúa, Pedro Reviriego)</author>
      <guid isPermaLink="false">2505.02271v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder</title>
      <link>http://arxiv.org/abs/2505.01938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridGS的3D Gaussian Splatting压缩框架，旨在提高3DGS数据的压缩效率。&lt;h4&gt;背景&lt;/h4&gt;现有的3DGS压缩方案主要通过隐式数据嵌入来生成紧凑的表示，但存在编码时间长、数据格式高度定制化的问题，难以广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个既能够生成紧凑表示，又能进行标准化点云数据编码的3DGS压缩框架。&lt;h4&gt;方法&lt;/h4&gt;HybridGS首先生成紧凑且显式的3DGS数据，引入双通道稀疏表示来监督基本位置和特征位深。然后，使用规范化的点云编码器进行进一步的数据压缩，形成标准输出位流。此外，提出了一种简单有效的速率控制方案来调整可解释的数据压缩方案。&lt;h4&gt;主要发现&lt;/h4&gt;目前，HybridGS不包含旨在提高3DGS生成质量的模块。然而，实验结果表明，它仍然提供了与现有方法相当的重构性能，并且编码和解码速度明显更高。&lt;h4&gt;结论&lt;/h4&gt;HybridGS是一种高效且易于部署的3DGS压缩框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的3D高斯分层（3DGS）压缩方案都侧重于通过隐式数据嵌入来生成紧凑的3DGS表示。它们具有较长的编码时间和高定制化的数据格式，这使得它们难以广泛应用。本文提出了一种新的3DGS压缩框架，称为HybridGS，它利用了紧凑生成和标准化点云数据编码的优势。HybridGS首先生成紧凑且显式的3DGS数据。引入双通道稀疏表示来监督基本位置和特征位深。然后，利用规范化的点云编码器进行进一步的数据压缩，形成标准输出位流。提出了一种简单有效的速率控制方案来调整可解释的数据压缩方案。目前，HybridGS不包含旨在提高3DGS生成质量的模块。但实验结果表明，它仍然提供了与现有方法相当的重构性能，并且编码和解码速度明显更高。代码可在https://github.com/Qi-Yangsjtu/HybridGS上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing 3D Gaussian Splatting (3DGS) compression schemes focus onproducing compact 3DGS representation via implicit data embedding. They havelong coding times and highly customized data format, making it difficult forwidespread deployment. This paper presents a new 3DGS compression frameworkcalled HybridGS, which takes advantage of both compact generation andstandardized point cloud data encoding. HybridGS first generates compact andexplicit 3DGS data. A dual-channel sparse representation is introduced tosupervise the primitive position and feature bit depth. It then utilizes acanonical point cloud encoder to perform further data compression and formstandard output bitstreams. A simple and effective rate control scheme isproposed to pivot the interpretable data compression scheme. At the currentstage, HybridGS does not include any modules aimed at improving 3DGS qualityduring generation. But experiment results show that it still providescomparable reconstruction performance against state-of-the-art methods, withevidently higher encoding and decoding speed. The code is publicly available athttps://github.com/Qi-Yangsjtu/HybridGS.</description>
      <author>example@mail.com (Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li)</author>
      <guid isPermaLink="false">2505.01938v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation</title>
      <link>http://arxiv.org/abs/2505.02247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的解释方法，专门针对3D Geometric Graph Neural Networks（GNNs），旨在解决3D GNNs在分子数据建模中的可解释性问题。&lt;h4&gt;背景&lt;/h4&gt;3D GNNs在分子数据建模中具有强大的预测能力，但通常缺乏可解释性，这在需要可靠和透明洞察的科学应用中引起担忧。&lt;h4&gt;目的&lt;/h4&gt;提高3D GNNs的可解释性，使其在科学应用中更加可靠和透明。&lt;h4&gt;方法&lt;/h4&gt;该方法将解释局部化到每个节点的3D空间中的直接邻域，并为每个节点分配一个影响半径，定义了消息传递捕获空间和结构交互的局部化区域。&lt;h4&gt;主要发现&lt;/h4&gt;通过限制子图到局部化的影响半径，这种方法不仅增强了可解释性，而且与3D图应用中典型的物理和结构依赖性相一致，如分子学习。&lt;h4&gt;结论&lt;/h4&gt;该方法利用了3D图固有的空间和几何特征，为3D GNNs提供了一种新的解释方法，有助于提高模型的可解释性和透明度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Geometric Graph Neural Networks (GNNs) have emerged as transformativetools for modeling molecular data. Despite their predictive power, these modelsoften suffer from limited interpretability, raising concerns for scientificapplications that require reliable and transparent insights. While existingmethods have primarily focused on explaining molecular substructures in 2DGNNs, the transition to 3D GNNs introduces unique challenges, such as handlingthe implicit dense edge structures created by a cut-off radius. To tackle this,we introduce a novel explanation method specifically designed for 3D GNNs,which localizes the explanation to the immediate neighborhood of each nodewithin the 3D space. Each node is assigned an radius of influence, defining thelocalized region within which message passing captures spatial and structuralinteractions crucial for the model's predictions. This method leverages thespatial and geometric characteristics inherent in 3D graphs. By constrainingthe subgraph to a localized radius of influence, the approach not only enhancesinterpretability but also aligns with the physical and structural dependenciestypical of 3D graph applications, such as molecular learning.</description>
      <author>example@mail.com (Jingxiang Qu, Wenhan Gao, Jiaxing Zhang, Xufeng Liu, Hua Wei, Haibin Ling, Yi Liu)</author>
      <guid isPermaLink="false">2505.02247v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning</title>
      <link>http://arxiv.org/abs/2505.02232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合提示式基础模型和强化学习的方法，使机器人能够以提示响应的方式执行灵活的操纵任务。&lt;h4&gt;背景&lt;/h4&gt;构建对输入提示有响应的模型是机器学习领域的一次变革，这种方法在机器人领域，如杂乱环境中的针对性操作等问题上具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以将高级命令与精细的灵活控制相联系的问题。&lt;h4&gt;方法&lt;/h4&gt;采用了一种记忆增强的学生-教师学习框架，使用Segment-Anything 2 (SAM 2)模型作为感知骨干，从用户提示中推断出感兴趣的对象。虽然检测可能不完美，但它们的时序为记忆增强模型提供了丰富的隐含状态估计信息。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功地学习了提示响应策略，并在从杂乱场景中拾取物体等任务中得到了演示。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在机器人灵活操纵任务中表现出色，并展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building models responsive to input prompts represents a transformative shiftin machine learning. This paradigm holds significant potential for roboticsproblems, such as targeted manipulation amidst clutter. In this work, wepresent a novel approach to combine promptable foundation models withreinforcement learning (RL), enabling robots to perform dexterous manipulationtasks in a prompt-responsive manner. Existing methods struggle to linkhigh-level commands with fine-grained dexterous control. We address this gapwith a memory-augmented student-teacher learning framework. We use theSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object ofinterest from user prompts. While detections are imperfect, their temporalsequence provides rich information for implicit state estimation bymemory-augmented models. Our approach successfully learns prompt-responsivepolicies, demonstrated in picking objects from cluttered scenes. Videos andcode are available at https://memory-student-teacher.github.io</description>
      <author>example@mail.com (Malte Mosbach, Sven Behnke)</author>
      <guid isPermaLink="false">2505.02232v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement</title>
      <link>http://arxiv.org/abs/2505.01766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Information Fusion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于图的鲁棒多模态方法，以整合视觉和运动数据，提高手术流程识别的准确性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;手术流程识别对于自动化任务、支持决策和培训新外科医生至关重要，但数据损坏可能导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;旨在提高手术流程识别的鲁棒性，特别是在有领域偏移或数据损坏的复杂场景中。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为GRAD的多模态图表示网络，用于捕捉精细的视觉信息，并通过基于图的消息建模显式地建模视觉和运动嵌入之间的复杂关系。还提出了一个视觉-运动对抗框架和上下文校准解码器。&lt;h4&gt;主要发现&lt;/h4&gt;模型和模块的有效性通过广泛的比较和消融实验得到了证明。鲁棒性实验表明，该方法能够有效处理存储和传输过程中的数据损坏，表现出良好的稳定性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法旨在推进自动化手术流程识别，解决手术程序固有的复杂性和动态性。&lt;h4&gt;翻译&lt;/h4&gt;Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical workflow recognition is vital for automating tasks, supportingdecision-making, and training novice surgeons, ultimately improving patientsafety and standardizing procedures. However, data corruption can lead toperformance degradation due to issues like occlusion from bleeding or smoke insurgical scenes and problems with data storage and transmission. In this case,we explore a robust graph-based multimodal approach to integrating vision andkinematic data to enhance accuracy and reliability. Vision data capturesdynamic surgical scenes, while kinematic data provides precise movementinformation, overcoming limitations of visual recognition under adverseconditions. We propose a multimodal Graph Representation network withAdversarial feature Disentanglement (GRAD) for robust surgical workflowrecognition in challenging scenarios with domain shifts or corrupted data.Specifically, we introduce a Multimodal Disentanglement Graph Network thatcaptures fine-grained visual information while explicitly modeling the complexrelationships between vision and kinematic embeddings through graph-basedmessage modeling. To align feature spaces across modalities, we propose aVision-Kinematic Adversarial framework that leverages adversarial training toreduce modality gaps and improve feature consistency. Furthermore, we design aContextual Calibrated Decoder, incorporating temporal and contextual priors toenhance robustness against domain shifts and corrupted data. Extensivecomparative and ablation experiments demonstrate the effectiveness of our modeland proposed modules. Moreover, our robustness experiments show that our methodeffectively handles data corruption during storage and transmission, exhibitingexcellent stability and robustness. Our approach aims to advance automatedsurgical workflow recognition, addressing the complexities and dynamisminherent in surgical procedures.</description>
      <author>example@mail.com (Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, Hongliang Ren)</author>
      <guid isPermaLink="false">2505.01766v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Exploring new Approaches for Information Retrieval through Natural Language Processing</title>
      <link>http://arxiv.org/abs/2505.02199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, comprehensive literature review covering six key  IR-NLP papers, plus keywords and full reference list&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了信息检索（IR）在自然语言处理（NLP）中的应用的最新进展和新兴方法。&lt;h4&gt;背景&lt;/h4&gt;文章回顾了传统的IR模型，包括布尔模型、向量空间模型、概率模型和推理网络模型。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探讨现代技术，如深度学习、强化学习和预训练的转换器模型（如BERT）。&lt;h4&gt;方法&lt;/h4&gt;文章讨论了用于高效文本索引和搜索的关键工具和库，如Lucene、Anserini和Pyserini。&lt;h4&gt;主要发现&lt;/h4&gt;文章对稀疏、密集和混合检索方法进行了比较分析，并展示了它们在网页搜索引擎、跨语言IR、论点挖掘、私人信息检索和仇恨言论检测中的应用。&lt;h4&gt;结论&lt;/h4&gt;最后，文章确定了提高检索精度、可扩展性和考虑伦理问题的开放挑战和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review paper explores recent advancements and emerging approaches inInformation Retrieval (IR) applied to Natural Language Processing (NLP). Weexamine traditional IR models such as Boolean, vector space, probabilistic, andinference network models, and highlight modern techniques including deeplearning, reinforcement learning, and pretrained transformer models like BERT.We discuss key tools and libraries - Lucene, Anserini, and Pyserini - forefficient text indexing and search. A comparative analysis of sparse, dense,and hybrid retrieval methods is presented, along with applications in websearch engines, cross-language IR, argument mining, private informationretrieval, and hate speech detection. Finally, we identify open challenges andfuture research directions to enhance retrieval accuracy, scalability, andethical considerations.</description>
      <author>example@mail.com (Manak Raj, Nidhi Mishra)</author>
      <guid isPermaLink="false">2505.02199v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Contextures: Representations from Contexts</title>
      <link>http://arxiv.org/abs/2505.01557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025, longer version. arXiv admin note: substantial text overlap  with arXiv:2504.19792&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了上下文结构理论，用以描述大型模型学习到的表示，并证明了许多流行的学习方法都可以通过学习输入与上下文变量之间的关联来表征。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在实证上取得了成功，但我们缺乏对这些模型学习到的表示的系统描述。&lt;h4&gt;目的&lt;/h4&gt;建立上下文结构理论，以系统地描述大型模型学习到的表示。&lt;h4&gt;方法&lt;/h4&gt;通过证明许多流行的学习方法可以描述为从输入和上下文变量之间的关联中学习，以及上下文结构理论在监督学习、自监督学习和流形学习等不同学习范式中的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;上下文结构理论表明，学习上下文结构的表示在兼容上下文的任务上是最优的；模型规模达到可以近似最高奇异函数的程度后，进一步扩大模型规模将带来递减的回报；提出了一种评估上下文有用性的指标，并通过实验证明它与编码器在实际数据集上的性能有很好的相关性。&lt;h4&gt;结论&lt;/h4&gt;上下文结构理论为理解大型模型的学习提供了新的视角，并指出除了规模扩大外，提高上下文质量也是进一步提升模型性能的关键。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管基础模型在实证上取得了成功，但我们缺乏对这些模型学习到的表示的系统描述。在本文中，我们建立了上下文结构理论。它表明，一类广泛的表示学习方法可以表征为从输入与上下文变量之间的关联中学习。具体来说，我们表明许多流行的方法旨在逼近由上下文诱导的期望算子的最高奇异函数，在这种情况下，我们说表示学习上下文结构。我们通过证明表示学习在各种学习范式（监督学习、自监督学习和流形学习）中都可以从这种角度进行研究，来演示上下文结构理论的普遍性。我们还证明，学习上下文结构的表示在兼容上下文的任务上是最佳的。上下文结构理论的一个重要含义是，一旦模型足够大，可以逼近最高奇异函数，进一步扩大模型规模将带来递减的回报。因此，规模扩大并不是我们所需要的全部，进一步的改进需要更好的上下文。为此，我们研究了如何在不知道下游任务的情况下评估上下文的有用性。我们提出了一种指标，并通过实验表明，它与编码器在许多真实数据集上的实际性能有很好的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the empirical success of foundation models, we do not have asystematic characterization of the representations that these models learn. Inthis paper, we establish the contexture theory. It shows that a large class ofrepresentation learning methods can be characterized as learning from theassociation between the input and a context variable. Specifically, we showthat many popular methods aim to approximate the top-d singular functions ofthe expectation operator induced by the context, in which case we say that therepresentation learns the contexture. We demonstrate the generality of thecontexture theory by proving that representation learning within variouslearning paradigms -- supervised, self-supervised, and manifold learning -- canall be studied from such a perspective. We also prove that the representationsthat learn the contexture are optimal on those tasks that are compatible withthe context. One important implication of the contexture theory is that oncethe model is large enough to approximate the top singular functions, furtherscaling up the model size yields diminishing returns. Therefore, scaling is notall we need, and further improvement requires better contexts. To this end, westudy how to evaluate the usefulness of a context without knowing thedownstream tasks. We propose a metric and show by experiments that itcorrelates well with the actual performance of the encoder on many realdatasets.</description>
      <author>example@mail.com (Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar)</author>
      <guid isPermaLink="false">2505.01557v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Sparfels: Fast Reconstruction from Sparse Unposed Imagery</title>
      <link>http://arxiv.org/abs/2505.02178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page : https://shubhendu-jena.github.io/Sparfels/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用表面元素平铺的稀疏视图重建方法，该方法在消费级GPU上运行时间少于3分钟。&lt;h4&gt;背景&lt;/h4&gt;目前关于从噪声或未定位的稀疏相机学习稀疏辐射场的方法较少，在此设置下的形状恢复相对较少研究。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且简单的流程，利用最新的3D基础模型，以实现稀疏视图重建。&lt;h4&gt;方法&lt;/h4&gt;利用3D基础模型的多个任务头，特别是点图和相机初始化，构建一个2D高斯平铺（2DGS）模型，并通过图像对应关系指导2DGS训练中的相机优化。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种新的沿射线平铺颜色变差的公式，该公式可以高效计算。在训练中降低这一时刻可以导致更准确的形状重建。&lt;h4&gt;结论&lt;/h4&gt;在稀疏未校准设置中的重建和基于已建立的多视图数据集的新视角基准测试中，该方法展示了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种稀疏视图重建方法，该方法使用表面元素平铺，在消费级GPU上运行时间少于3分钟。虽然很少有方法解决从噪声或未定位的稀疏相机学习稀疏辐射场的问题，但在此设置下的形状恢复相对较少被探索。一些辐射场和形状学习测试时间优化方法通过学习数据先验或使用外部单目几何先验的组合来解决稀疏定位设置。不同之处在于，我们提出了一种高效且简单的流程，利用单个最新的3D基础模型。我们利用其各种任务头，特别是点图和相机初始化来实例化一个2D高斯平铺（2DGS）模型，并通过图像对应关系引导2DGS训练中的相机优化。我们贡献的关键是一个新的沿射线平铺颜色变差的公式，该公式可以高效计算。在训练中降低这一时刻可以导致更准确的形状重建。我们在稀疏未校准设置中的重建和基于已建立的多视图数据集的新视角基准测试中展示了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a method for Sparse view reconstruction with surface elementsplatting that runs within 3 minutes on a consumer grade GPU. While few methodsaddress sparse radiance field learning from noisy or unposed sparse cameras,shape recovery remains relatively underexplored in this setting. Severalradiance and shape learning test-time optimization methods address the sparseposed setting by learning data priors or using combinations of externalmonocular geometry priors. Differently, we propose an efficient and simplepipeline harnessing a single recent 3D foundation model. We leverage itsvarious task heads, notably point maps and camera initializations toinstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and imagecorrespondences to guide camera optimization midst 2DGS training. Key to ourcontribution is a novel formulation of splatted color variance along rays,which can be computed efficiently. Reducing this moment in training leads tomore accurate shape reconstructions. We demonstrate state-of-the-artperformances in the sparse uncalibrated setting in reconstruction and novelview benchmarks based on established multi-view datasets.</description>
      <author>example@mail.com (Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma)</author>
      <guid isPermaLink="false">2505.02178v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</title>
      <link>http://arxiv.org/abs/2505.01809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的弱监督视觉定位方法，用于在点云中根据自然语言描述定位定向3D框，无需标注来指导模型学习。&lt;h4&gt;背景&lt;/h4&gt;该任务面临两个主要挑战：类别级别的模糊性和实例级别的复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了一个能够明确区分类别和实例的新方法。&lt;h4&gt;方法&lt;/h4&gt;在类别级别分支中，利用预训练的外部检测器的大量类别知识，将对象提议特征与句子级别的类别特征对齐，从而增强类别意识。在实例级别分支中，利用语言查询中的空间关系描述来细化对象提议特征，确保对象之间的清晰区分。&lt;h4&gt;主要发现&lt;/h4&gt;这些设计使得模型能够准确识别目标类别对象，同时区分同一类别内的实例。&lt;h4&gt;结论&lt;/h4&gt;与先前的方法相比，该方法在三个广泛使用的基准测试（Nr3D、Sr3D和ScanRef）上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D weakly-supervised visual grounding task aims to localize oriented 3Dboxes in point clouds based on natural language descriptions without requiringannotations to guide model learning. This setting presents two primarychallenges: category-level ambiguity and instance-level complexity.Category-level ambiguity arises from representing objects of fine-grainedcategories in a highly sparse point cloud format, making category distinctionchallenging. Instance-level complexity stems from multiple instances of thesame category coexisting in a scene, leading to distractions during grounding.To address these challenges, we propose a novel weakly-supervised groundingapproach that explicitly differentiates between categories and instances. Inthe category-level branch, we utilize extensive category knowledge from apre-trained external detector to align object proposal features withsentence-level category features, thereby enhancing category awareness. In theinstance-level branch, we utilize spatial relationship descriptions fromlanguage queries to refine object proposal features, ensuring cleardifferentiation among objects. These designs enable our model to accuratelyidentify target-category objects while distinguishing instances within the samecategory. Compared to previous methods, our approach achieves state-of-the-artperformance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.</description>
      <author>example@mail.com (Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu)</author>
      <guid isPermaLink="false">2505.01809v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data</title>
      <link>http://arxiv.org/abs/2505.02130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML2025 Accept&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;注意力机制对大型语言模型（LLMs）的成功至关重要，但在处理图结构数据时，与图神经网络（GNNs）中使用的基于固定链接的消息传递机制相比，存在不足。&lt;h4&gt;背景&lt;/h4&gt;对于图结构数据，需要强调拓扑连接，而现有的注意力机制在处理这类数据时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;从注意力机制的角度进行实证研究，探索LLMs如何处理图结构数据，以深入了解LLMs在图结构上的注意力行为。&lt;h4&gt;方法&lt;/h4&gt;分析LLMs在图结构数据上的注意力应用，并评估不同注意力模型的效果。&lt;h4&gt;主要发现&lt;/h4&gt;1) LLMs可以识别图数据并捕捉文本节点之间的交互，但由于固有架构限制，难以建模图结构内部的节点关系。2) LLMs在图节点上的注意力分布不符合理想的结构模式，表明其未能适应图拓扑的细微差别。3) 无论是全连接注意力还是固定连接性都不理想；每种方法在其应用场景中都有特定的局限性。相反，中间状态注意力窗口可以提高LLMs的训练性能，并在推理过程中无缝过渡到全连接窗口。&lt;h4&gt;结论&lt;/h4&gt;LLMs在处理图结构数据时存在局限性，需要改进模型以更好地处理此类数据。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制对于大型语言模型（LLMs）的成功至关重要，然而，在处理需要强调拓扑连接的图结构数据时，与在固定链接上使用的消息传递机制（如图神经网络GNNs所采用的）相比，它们存在不足。然而，对于图结构数据，需要强调拓扑连接，而现有的注意力机制在处理这类数据时存在局限性。受这些观察的启发，我们从注意力机制的角度进行了实证研究，以探索LLMs如何处理图结构数据。目的是深入了解LLMs在图结构上的注意力行为。我们发现了一些关于LLMs如何将注意力应用于图结构数据的独特现象，并分析了这些发现以改进LLMs对这类数据的建模。研究的主要发现是：1）虽然LLMs可以识别图数据并捕捉文本节点之间的交互，但由于固有的架构限制，它们难以建模图结构内部的节点关系。2）LLMs在图节点上的注意力分布不符合理想的结构模式，表明其未能适应图拓扑的细微差别。3）无论是全连接注意力还是固定连接性都不理想；每种方法在其应用场景中都有特定的局限性。相反，中间状态注意力窗口可以提高LLMs的训练性能，并在推理过程中无缝过渡到全连接窗口。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms are critical to the success of large language models(LLMs), driving significant advancements in multiple fields. However, forgraph-structured data, which requires emphasis on topological connections, theyfall short compared to message-passing mechanisms on fixed links, such as thoseemployed by Graph Neural Networks (GNNs). This raises a question: ``Doesattention fail for graphs in natural language settings?'' Motivated by theseobservations, we embarked on an empirical study from the perspective ofattention mechanisms to explore how LLMs process graph-structured data. Thegoal is to gain deeper insights into the attention behavior of LLMs over graphstructures. We uncovered unique phenomena regarding how LLMs apply attention tograph-structured data and analyzed these findings to improve the modeling ofsuch data by LLMs. The primary findings of our research are: 1) While LLMs canrecognize graph data and capture text-node interactions, they struggle to modelinter-node relationships within graph structures due to inherent architecturalconstraints. 2) The attention distribution of LLMs across graph nodes does notalign with ideal structural patterns, indicating a failure to adapt to graphtopology nuances. 3) Neither fully connected attention nor fixed connectivityis optimal; each has specific limitations in its application scenarios.Instead, intermediate-state attention windows improve LLM training performanceand seamlessly transition to fully connected windows during inference. Sourcecode: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}</description>
      <author>example@mail.com (Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan)</author>
      <guid isPermaLink="false">2505.02130v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</title>
      <link>http://arxiv.org/abs/2505.02152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Interleave-VLA框架，该框架能够理解交错图像-文本指令并在物理世界中直接生成连续的动作序列。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型仅限于处理机器人的观察和文本指令，缺乏数字世界中由基础模型进展带来的交错多模态指令的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一个灵活的、模型无关的范式，通过最小修改扩展最先进的VLA模型，并实现强大的零样本泛化。&lt;h4&gt;方法&lt;/h4&gt;开发了一个自动管道，将Open X-Embodiment中的真实世界数据集的纯文本指令转换为交错图像-文本指令，构建了第一个包含210k个场景的大型真实世界交错具身数据集。&lt;h4&gt;主要发现&lt;/h4&gt;Interleave-VLA在模拟基准和真实机器人实验中表现出显著优势：1）与最先进的基线相比，它将域外泛化能力提高了2-3倍；2）支持灵活的任务接口；3）以零样本方式处理多样化的用户提供的图像指令，如手绘草图。&lt;h4&gt;结论&lt;/h4&gt;Interleave-VLA通过交错范式有效地利用了异构数据集和多样化的指令图像，包括来自互联网的图像，显示出强大的扩展潜力。模型和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型在物理世界的通用机器人操作方面展现出巨大潜力。然而，现有的模型局限于机器人观察和纯文本指令，缺乏由数字世界中基础模型进展带来的交错多模态指令的灵活性。在本文中，我们提出了Interleave-VLA，这是第一个能够理解交错图像-文本指令并在物理世界中直接生成连续动作序列的框架。它提供了一个灵活的、模型无关的范式，通过最小修改扩展了最先进的VLA模型，并实现了强大的零样本泛化。实现Interleave-VLA的一个关键挑战是缺乏大规模的交错具身数据集。为了弥合这一差距，我们开发了一个自动管道，将Open X-Embodiment中的真实世界数据集的纯文本指令转换为交错图像-文本指令，从而产生了第一个包含210k个场景的大型真实世界交错具身数据集。通过在模拟基准和真实机器人实验上的综合评估，我们证明了Interleave-VLA的优势：1）与最先进的基线相比，它将域外泛化能力提高了2-3倍；2）支持灵活的任务接口；3）以零样本方式处理多样化的用户提供的图像指令，如手绘草图。我们进一步分析了Interleave-VLA强大零样本性能背后的因素，表明交错范式有效地利用了异构数据集和多样化的指令图像，包括来自互联网的图像，这表明了强大的扩展潜力。我们的模型和数据集将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have shown great promise for generalistrobotic manipulation in the physical world. However, existing models arerestricted to robot observations and text-only instructions, lacking theflexibility of interleaved multimodal instructions enabled by recent advancesin foundation models in the digital world. In this paper, we presentInterleave-VLA, the first framework capable of comprehending interleavedimage-text instructions and directly generating continuous action sequences inthe physical world. It offers a flexible, model-agnostic paradigm that extendsstate-of-the-art VLA models with minimal modifications and strong zero-shotgeneralization. A key challenge in realizing Interleave-VLA is the absence oflarge-scale interleaved embodied datasets. To bridge this gap, we develop anautomatic pipeline that converts text-only instructions from real-worlddatasets in Open X-Embodiment into interleaved image-text instructions,resulting in the first large-scale real-world interleaved embodied dataset with210k episodes. Through comprehensive evaluation on simulation benchmarks andreal-robot experiments, we demonstrate that Interleave-VLA offers significantbenefits: 1) it improves out-of-domain generalization to unseen objects by 2-3xcompared to state-of-the-art baselines, 2) supports flexible task interfaces,and 3) handles diverse user-provided image instructions in a zero-shot manner,such as hand-drawn sketches. We further analyze the factors behindInterleave-VLA's strong zero-shot performance, showing that the interleavedparadigm effectively leverages heterogeneous datasets and diverse instructionimages, including those from the Internet, which demonstrates strong potentialfor scaling up. Our model and dataset will be open-sourced.</description>
      <author>example@mail.com (Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding)</author>
      <guid isPermaLink="false">2505.02152v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes</title>
      <link>http://arxiv.org/abs/2505.01726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025 Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NPISeg3D的新颖概率框架，用于解决交互式3D分割中的两个关键挑战：从稀疏用户点击生成准确分割以及量化预测不确定性。&lt;h4&gt;背景&lt;/h4&gt;交互式3D分割通过结合用户点击在复杂3D场景中生成准确的对象掩码，但存在两个未充分探索的挑战：从稀疏用户点击到准确分割的有效泛化，以及量化预测不确定性以帮助用户识别不可靠区域。&lt;h4&gt;目的&lt;/h4&gt;提出NPISeg3D框架，以解决上述两个挑战，实现更准确的分割和可靠的预测不确定性估计。&lt;h4&gt;方法&lt;/h4&gt;NPISeg3D利用神经过程（NPs）构建了一个具有场景特异性和对象特异性潜在变量的分层潜在变量结构，以增强少样本泛化能力。此外，设计了一个概率原型调制器，通过对象特异性潜在变量自适应地调节点击原型，提高模型捕捉对象感知上下文和量化预测不确定性的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个3D点云数据集上的实验表明，NPISeg3D在较少点击的情况下实现了优越的分割性能，并提供了可靠的预测不确定性估计。&lt;h4&gt;结论&lt;/h4&gt;NPISeg3D框架能够有效地解决交互式3D分割中的关键挑战，为生成准确的对象掩码提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D segmentation has emerged as a promising solution forgenerating accurate object masks in complex 3D scenes by incorporatinguser-provided clicks. However, two critical challenges remain underexplored:(1) effectively generalizing from sparse user clicks to produce accuratesegmentation, and (2) quantifying predictive uncertainty to help users identifyunreliable regions. In this work, we propose NPISeg3D, a novel probabilisticframework that builds upon Neural Processes (NPs) to address these challenges.Specifically, NPISeg3D introduces a hierarchical latent variable structure withscene-specific and object-specific latent variables to enhance few-shotgeneralization by capturing both global context and object-specificcharacteristics. Additionally, we design a probabilistic prototype modulatorthat adaptively modulates click prototypes with object-specific latentvariables, improving the model's ability to capture object-aware context andquantify predictive uncertainty. Experiments on four 3D point cloud datasetsdemonstrate that NPISeg3D achieves superior segmentation performance with fewerclicks while providing reliable uncertainty estimations.</description>
      <author>example@mail.com (Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves)</author>
      <guid isPermaLink="false">2505.01726v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing</title>
      <link>http://arxiv.org/abs/2505.02096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TeMTG的多模态优化框架，用于解析视频中的事件类别和发生时间。&lt;h4&gt;背景&lt;/h4&gt;现有的音频-视觉视频解析方法通常通过弱标签隐式建模音频和视觉特征，而没有挖掘不同模态之间的语义关系以及显式建模事件的时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出TeMTG框架以更准确地解析在弱监督下每个段落的 eventos 信息。&lt;h4&gt;方法&lt;/h4&gt;TeMTG框架结合了文本增强和多跳时间图建模。具体来说，利用预训练的多模态模型生成特定模态的文本嵌入，并将其与音频-视觉特征融合以增强这些特征的语义表示。此外，引入了多跳时间图神经网络，它显式地建模了段落之间的局部时间关系，捕捉了短期和长期事件的时间连续性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在LLP数据集的多个关键指标上，该方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;TeMTG框架在音频-视觉视频解析任务中取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Video Parsing (AVVP) task aims to parse the event categories andoccurrence times from audio and visual modalities in a given video. Existingmethods usually focus on implicitly modeling audio and visual features throughweak labels, without mining semantic relationships for different modalities andexplicit modeling of event temporal dependencies. This makes it difficult forthe model to accurately parse event information for each segment under weaksupervision, especially when high similarity between segmental modal featuresleads to ambiguous event boundaries. Hence, we propose a multimodaloptimization framework, TeMTG, that combines text enhancement and multi-hoptemporal graph modeling. Specifically, we leverage pre-trained multimodalmodels to generate modality-specific text embeddings, and fuse them withaudio-visual features to enhance the semantic representation of these features.In addition, we introduce a multi-hop temporal graph neural network, whichexplicitly models the local temporal relationships between segments, capturingthe temporal continuity of both short-term and long-range events. Experimentalresults demonstrate that our proposed method achieves state-of-the-art (SOTA)performance in multiple key indicators in the LLP dataset.</description>
      <author>example@mail.com (Yaru Chen, Peiliang Zhang, Fei Li, Faegheh Sardari, Ruohao Guo, Zhenbo Li, Wenwu Wang)</author>
      <guid isPermaLink="false">2505.02096v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency</title>
      <link>http://arxiv.org/abs/2505.02133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）在自动代码生成中的应用，探讨了提升代码生成功能、可靠性和实用性的方法。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在代码生成领域的应用已成为人工智能研究的重要焦点，随着模型的发展，其在理解和生成复杂代码结构方面的能力为自动化编程任务提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;旨在通过结合多代理协作和基于运行时执行信息调试两种方法，提升代码生成的功能、可靠性和实用性。&lt;h4&gt;方法&lt;/h4&gt;进行实证研究，评估了单独策略以及两种策略组合的效果，使用了19个LLM来检验策略的性能，并分析了不同编程活动组合和训练范式对代码生成有效性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实现了结合两种策略的链式系统，使用两个常用的代码生成基准数据集评估了功能准确性、代码可靠性和生成延迟。&lt;h4&gt;结论&lt;/h4&gt;研究结果为寻求稳健的AI驱动编码解决方案的组织提供了有价值的见解，指导他们选择能够更好地适应复杂后训练策略的模型，从而促进更有效和可靠的代码生成技术的采用。&lt;h4&gt;翻译&lt;/h4&gt;The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study uses 19 LLMs to examine the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) for automated code generation hasemerged as a significant focus within AI research. As these pretrained modelscontinue to evolve, their ability to understand and generate complex codestructures has opened new possibilities for automating intricate programmingtasks for the sake of accurate code generation. Although contemporaryfoundational models demonstrate promoting results, researchers continue toexplore optimal post-training strategies to enhance code quality. These includesupervised fine-tuning, retrieval-augmented generation (RAG), debugging, andmany others. In this paper, we combine two widely used approaches namelymulti-agent collaboration and runtime execution information-based debugging,for improving code generation functionality, reliability, and practicalapplicability. We perform an empirical study in order to extend the evaluationof the individual strategies as well as the proposed composition of theactivities of both strategies. Our study use 19 LLMs to examines theperformance of individual and the proposed strategies, offering comprehensiveinsights into how different programming activities compositions and trainingparadigms influence code generation effectiveness. In particular, we implementa chained system that combines both strategies to assess their combined impacton functional accuracy, code reliability, and generation latency using twobenchmark datasets commonly used for code generation. Our findings providevaluable insights for organizations seeking robust AI-driven coding solutionsby guiding them in selecting models that can better adapt to complexpost-training strategies, ultimately fostering the adoption of more effectiveand reliable code generation technologies.</description>
      <author>example@mail.com (Nazmus Ashrafi, Salah Bouktif, Mohammed Mediani)</author>
      <guid isPermaLink="false">2505.02133v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</title>
      <link>http://arxiv.org/abs/2505.01615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于跨注意力机制的Transformer方法，用于多模态传感器融合，以构建船舶周围环境的鸟瞰图，支持更安全的自主航行。&lt;h4&gt;背景&lt;/h4&gt;为了提高航行准确性和鲁棒性，需要详细可靠的场景表示。&lt;h4&gt;目的&lt;/h4&gt;通过深度融合多视角RGB图像、长波红外图像和稀疏LiDAR点云，以及X波段雷达和电子海图数据，提高导航准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用跨注意力Transformer模型进行多模态传感器融合，结合多种数据源进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够提供详细的可靠场景表示，即使在恶劣天气和复杂的航海环境中也能有效工作。&lt;h4&gt;结论&lt;/h4&gt;该方法在真实世界的海试中得到了验证，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于跨注意力机制的Transformer方法，用于多模态传感器融合，以构建船舶周围环境的鸟瞰图，支持更安全的自主航行。该模型深度融合多视角RGB图像和长波红外图像与稀疏LiDAR点云。训练还整合了X波段雷达和电子海图数据以提供预测信息。所得到的视图提供了一个详细的可靠场景表示，提高了导航的准确性和鲁棒性。现实世界的海试证实了该方法的有效性，即使在恶劣天气和复杂的航海环境中也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a cross attention transformer based method for multimodal sensorfusion to build a birds eye view of a vessels surroundings supporting saferautonomous marine navigation. The model deeply fuses multiview RGB and longwave infrared images with sparse LiDAR point clouds. Training also integrates Xband radar and electronic chart data to inform predictions. The resulting viewprovides a detailed reliable scene representation improving navigationalaccuracy and robustness. Real world sea trials confirm the methodseffectiveness even in adverse weather and complex maritime settings.</description>
      <author>example@mail.com (Dimitrios Dagdilelis, Panagiotis Grigoriadis, Roberto Galeazzi)</author>
      <guid isPermaLink="false">2505.01615v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Defense Against Adversarial Attacks in Time Series Classification</title>
      <link>http://arxiv.org/abs/2505.02073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures. Accepted at RAFDA Workshop, PAKDD 2025  (Springer, EI &amp; Scopus indexed). Code:  https://github.com/Yi126/Lightweight-Defence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对时间序列分类（TSC）领域，提出了五种基于数据增强的时间序列对抗防御方法，以降低计算成本，并提升了模型的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;随着时间序列分类在计算机视觉领域的兴起，保证TSC模型对抗攻击的鲁棒性变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发高效、计算成本低的对抗防御方法，以提高TSC模型的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出五种基于数据增强的防御方法，其中最计算密集的方法相比原始TSC模型仅增加14.07%的计算资源。创建了两种结合方法：一种是所有提出技术的集成，另一种是集成方法与PGD-based AT方法的对比。&lt;h4&gt;主要发现&lt;/h4&gt;提出的集成方法不仅提供了比基于PGD的AT方法更好的防御性能，而且增强了TSC模型的泛化能力，且所需的计算资源仅为PGD-based AT的三分之一以下。&lt;h4&gt;结论&lt;/h4&gt;这些方法推进了数据挖掘中鲁棒TSC的发展，并为将数据增强对抗防御与大规模预训练模型结合的未来研究提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As time series classification (TSC) gains prominence, ensuring robust TSCmodels against adversarial attacks is crucial. While adversarial defense iswell-studied in Computer Vision (CV), the TSC field has primarily relied onadversarial training (AT), which is computationally expensive. In this paper,five data augmentation-based defense methods tailored for time series aredeveloped, with the most computationally intensive method among them increasingthe computational resources by only 14.07% compared to the original TSC model.Moreover, the deployment process for these methods is straightforward. Byleveraging these advantages of our methods, we create two combined methods. Oneof these methods is an ensemble of all the proposed techniques, which not onlyprovides better defense performance than PGD-based AT but also enhances thegeneralization ability of TSC models. Moreover, the computational resourcesrequired for our ensemble are less than one-third of those required forPGD-based AT. These methods advance robust TSC in data mining. Furthermore, asfoundation models are increasingly explored for time series feature learning,our work provides insights into integrating data augmentation-based adversarialdefense with large-scale pre-trained models in future research.</description>
      <author>example@mail.com (Yi Han)</author>
      <guid isPermaLink="false">2505.02073v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup</title>
      <link>http://arxiv.org/abs/2505.01902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种机器学习框架，用于预测FIFA世界杯比赛的胜者。该框架结合了球队历史数据和球员个人表现指标，并使用分类技术以及降维和超参数优化，以创建鲁棒的预测模型。&lt;h4&gt;背景&lt;/h4&gt;准确预测FIFA世界杯比赛结果对分析师、教练、赌徒和球迷都有重要价值。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习框架，用于预测FIFA世界杯比赛的胜者。&lt;h4&gt;方法&lt;/h4&gt;通过整合球队历史数据和球员个人表现指标，创建年度球队档案，采用分类技术、降维和超参数优化来提高预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在FIFA 2022世界杯数据上的预测准确性优于基线方法，强调了结合个人球员属性和球队构成的重要性。&lt;h4&gt;结论&lt;/h4&gt;本文强调了丰富、以球员为中心的数据在体育分析中的变革潜力，并为未来探索高级学习架构如图神经网络来模拟复杂团队互动奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of FIFA World Cup match outcomes holds significant valuefor analysts, coaches, bettors, and fans. This paper presents a machinelearning framework specifically designed to forecast match winners in FIFAWorld Cup. By integrating both team-level historical data and player-specificperformance metrics such as goals, assists, passing accuracy, and tackles, wecapture nuanced interactions often overlooked by traditional aggregate models.Our methodology processes multi-year data to create year-specific team profilesthat account for evolving rosters and player development. We employclassification techniques complemented by dimensionality reduction andhyperparameter optimization, to yield robust predictive models. Experimentalresults on data from the FIFA 2022 World Cup demonstrate our approach'ssuperior accuracy compared to baseline method. Our findings highlight theimportance of incorporating individual player attributes and team-levelcomposition to enhance predictive performance, offering new insights intoplayer synergy, strategic match-ups, and tournament progression scenarios. Thiswork underscores the transformative potential of rich, player-centric data insports analytics, setting a foundation for future exploration of advancedlearning architectures such as graph neural networks to model complex teaminteractions.</description>
      <author>example@mail.com (Ali Al-Bustami, Zaid Ghazal)</author>
      <guid isPermaLink="false">2505.01902v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation</title>
      <link>http://arxiv.org/abs/2505.01984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ADaFGrad的方法，用于增强全切片图像（WSI）分析中的终身学习能力，以提高癌症诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;全切片图像在癌症诊断和预后中起着关键作用，但由于其巨大的数据量，存储、处理和模型训练都面临挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种终身学习的方法，以利用分布在多个机构的切片来开发一个统一的在线模型，作为临床和医院环境中癌症诊断的计算工具。&lt;h4&gt;方法&lt;/h4&gt;ADaFGrad方法利用病理视觉语言基础模型，开发了一个框架，允许切片的区域组织特征与预定义的基于文本的原型缓冲区进行交互。此外，还提出了一种梯度蒸馏机制，模拟了在持续学习环境中，过去和当前迭代中logit相对于分类头参数的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ADaFGrad在仅经过几个训练周期后，在类增量学习场景中比最先进的WSI特定方法和传统持续学习方法表现更好，提高了5.068%。此外，ADaFGrad的准确率比基线提高了40.084%，显示出所提出模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;ADaFGrad方法在癌症诊断中的终身学习能力方面具有显著优势，能够提高诊断的准确性并减少知识遗忘。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosisand prognosis, as they provide tissue details at the cellular level. However,the rapid growth of computational tasks involving WSIs poses significantchallenges. Given that WSIs are gigapixels in size, they present difficultiesin terms of storage, processing, and model training. Therefore, it is essentialto develop lifelong learning approaches for WSI analysis. In scenarios whereslides are distributed across multiple institutes, we aim to leverage them todevelop a unified online model as a computational tool for cancer diagnosis inclinical and hospital settings. In this study, we introduce ADaFGrad, a methoddesigned to enhance lifelong learning for whole-slide image (WSI) analysis.First, we leverage pathology vision-language foundation models to develop aframework that enables interaction between a slide's regional tissue featuresand a predefined text-based prototype buffer. Additionally, we propose agradient-distillation mechanism that mimics the gradient of a logit withrespect to the classification-head parameters across past and currentiterations in a continual-learning setting. We construct a sequence of six TCGAdatasets for training and evaluation. Experimental results show that ADaFGradoutperforms both state-of-the-art WSI-specific and conventionalcontinual-learning methods after only a few training epochs, exceeding them byup to +5.068% in the class-incremental learning scenario while exhibiting theleast forgetting (i.e., retaining the most knowledge from previous tasks).Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,further demonstrating the effectiveness of the proposed modules.</description>
      <author>example@mail.com (Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima)</author>
      <guid isPermaLink="false">2505.01984v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
      <link>http://arxiv.org/abs/2505.01912v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了在分子发现领域，基于深度学习和生成模型的数据驱动发现流程的发展。研究了机器学习模型在过滤和设计新型分子时的应用，同时评估了模型在分子性质预测中的泛化能力，并提出一个新的开源基准研究BOOM（Benchmark for Out-of-Distribution Molecular Property Predictions）。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习和生成模型的发展，数据驱动分子发现管道受到广泛关注，这类管道利用机器学习模型过滤和设计新型分子，而无需进行昂贵的第一性原理模拟。然而，发现新的分子需要准确的超出分布（OOD）预测，而ML模型在泛化OOD方面往往存在困难。&lt;h4&gt;目的&lt;/h4&gt;研究并提出一个基准（BOOM）来评估基于属性的超出分布分子性质预测模型，通过评估多个模型和性质预测任务组合的泛化能力，为深度学习模型在OOD性能上提供基准。&lt;h4&gt;方法&lt;/h4&gt;研究人员评估了超过140种模型和性质预测任务组合，以在多个任务上基准测试深度学习模型的OOD性能，并通过消融实验研究了数据生成、预训练、超参数优化、模型架构和分子表示对OOD性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现没有现存的模型在所有任务上都能实现强的OOD泛化；即使表现最好的模型，其平均OOD误差也比分布内的误差大3倍。此外，具有高归纳偏置的深度学习模型在具有简单、特定性质的OOD任务上表现良好。化学基础模型虽然在有限的训练数据场景中提供了有希望的解决方案，但目前的模型并未展现出强大的OOD外推能力。&lt;h4&gt;结论&lt;/h4&gt;提出发展具有强大OOD泛化的机器学习模型是化学机器学习模型开发的新前沿挑战，并且开源的BOOM基准研究将被发布在Github上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in deep learning and generative modeling have driven interest indata-driven molecule discovery pipelines, whereby machine learning (ML) modelsare used to filter and design novel molecules without requiring prohibitivelyexpensive first-principles simulations. Although the discovery of novelmolecules that extend the boundaries of known chemistry requires accurateout-of-distribution (OOD) predictions, ML models often struggle to generalizeOOD. Furthermore, there are currently no systematic benchmarks for molecularOOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for$\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecularproperty predictions -- a benchmark study of property-based out-of-distributionmodels for common molecular property prediction models. We evaluate more than140 combinations of models and property prediction tasks to benchmark deeplearning models on their OOD performance. Overall, we do not find any existingmodels that achieve strong OOD generalization across all tasks: even the topperforming model exhibited an average OOD error 3x larger than in-distribution.We find that deep learning models with high inductive bias can perform well onOOD tasks with simple, specific properties. Although chemical foundation modelswith transfer and in-context learning offer a promising solution for limitedtraining data scenarios, we find that current foundation models do not showstrong OOD extrapolation capabilities. We perform extensive ablationexperiments to highlight how OOD performance is impacted by data generation,pre-training, hyperparameter optimization, model architecture, and molecularrepresentation. We propose that developing ML models with strong OODgeneralization is a new frontier challenge in chemical ML model development.This open-source benchmark will be made available on Github.</description>
      <author>example@mail.com (Evan R. Antoniuk, Shehtab Zaman, Tal Ben-Nun, Peggy Li, James Diffenderfer, Busra Demirci, Obadiah Smolenski, Tim Hsu, Anna M. Hiszpanski, Kenneth Chiu, Bhavya Kailkhura, Brian Van Essen)</author>
      <guid isPermaLink="false">2505.01912v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2505.01862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReLI的语言无关框架，旨在使自主代理能够在不同语言中自然交流、语义推理并执行任务，解决了跨语言应用中与环境的交互和执行人类指令的问题。&lt;h4&gt;背景&lt;/h4&gt;在工业、家庭和其他日常任务中，自适应自主代理的应用正在增加，但在全球或跨语言的应用环境中，确保自主代理能够有效与环境互动并执行多样化语言的指令仍然是一个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;提出ReLI框架，以实现自主代理在不同语言环境中自然交流、语义推理并执行任务，不受指令语言起源的限制。&lt;h4&gt;方法&lt;/h4&gt;ReLI框架首先将大规模预训练的基础模型转化为语言到行动模型，这些模型可以通过自然的人类-机器人对话直接提供常识推理和高级机器人控制。此外，还进行了跨语言的模型固化，以确保ReLI能够跨全球语言泛化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的模拟和真实世界实验，包括零样本和少样本的时空导航、场景信息检索和查询导向任务，ReLI在140种语言的超过70K次多轮对话中进行了性能评估。平均而言，ReLI在跨语言指令解析和任务执行成功率方面达到了90%±0.2的准确率。&lt;h4&gt;结论&lt;/h4&gt;ReLI有望增强现实世界中的人机自然交互，同时支持语言多样性。演示和资源将在https://linusnep.github.io/ReLI/上公开提供。&lt;h4&gt;翻译&lt;/h4&gt;摘要翻译：Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to conversenaturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%±0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adapting autonomous agents to industrial, domestic, and other daily tasks iscurrently gaining momentum. However, in the global or cross-lingual applicationcontexts, ensuring effective interaction with the environment and executingunrestricted human task-specified instructions in diverse languages remains anunsolved problem. To address this challenge, we propose ReLI, alanguage-agnostic framework designed to enable autonomous agents to conversenaturally, semantically reason about the environment, and to perform downstreamtasks, regardless of the task instruction's linguistic origin. First, we groundlarge-scale pre-trained foundation models and transform them intolanguage-to-action models that can directly provide common-sense reasoning andhigh-level robot control through natural, free-flow human-robot conversationalinteractions. Further, we perform cross-lingual grounding of the models toensure that ReLI generalises across the global languages. To demonstrate theReLI's robustness, we conducted extensive simulated and real-world experimentson various short- and long-horizon tasks, including zero-shot and few-shotspatial navigation, scene information retrieval, and query-oriented tasks. Webenchmarked the performance on 140 languages involving over 70K multi-turnconversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy incross-lingual instruction parsing and task execution success rates. Theseresults demonstrate the ReLI's potential to enhance natural human-robotinteraction in the real world while championing linguistic diversity.Demonstrations and resources will be publicly available athttps://linusnep.github.io/ReLI/.</description>
      <author>example@mail.com (Linus Nwankwo, Bjoern Ellensohn, Ozan Özdenizci, Elmar Rueckert)</author>
      <guid isPermaLink="false">2505.01862v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2</title>
      <link>http://arxiv.org/abs/2505.01854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Short-Long Memory SAM 2（SLM-SAM 2）的新型架构，用于提高医学图像分割的自动化标注准确性。&lt;h4&gt;背景&lt;/h4&gt;医学图像（如MRI和CT）的手动标注是劳动密集型且耗时的过程，而视频对象分割的基础模型（如SAM 2）有望加快标注速度。&lt;h4&gt;目的&lt;/h4&gt;为了解决SAM 2在边界区域易出错的问题，提出SLM-SAM 2以提升分割精度。&lt;h4&gt;方法&lt;/h4&gt;SLM-SAM 2结合了短时和长时记忆银行以及独立的注意力模块，并在三个公开数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;SLM-SAM 2在5个体积和1个体积可用的情况下，与默认SAM 2相比，分别实现了Dice相似系数平均提升0.14和0.11。SLM-SAM 2对过度传播具有更强的抵抗力。&lt;h4&gt;结论&lt;/h4&gt;SLM-SAM 2在医学图像分割的自动化标注方面取得了显著进展，有助于分割模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manual annotation of volumetric medical images, such as magnetic resonanceimaging (MRI) and computed tomography (CT), is a labor-intensive andtime-consuming process. Recent advancements in foundation models for videoobject segmentation, such as Segment Anything Model 2 (SAM 2), offer apotential opportunity to significantly speed up the annotation process bymanually annotating one or a few slices and then propagating target masksacross the entire volume. However, the performance of SAM 2 in this contextvaries. Our experiments show that relying on a single memory bank and attentionmodule is prone to error propagation, particularly at boundary regions wherethe target is present in the previous slice but absent in the current one. Toaddress this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novelarchitecture that integrates distinct short-term and long-term memory bankswith separate attention modules to improve segmentation accuracy. We evaluateSLM-SAM 2 on three public datasets covering organs, bones, and muscles acrossMRI and CT modalities. We show that the proposed method markedly outperformsthe default SAM 2, achieving average Dice Similarity Coefficient improvement of0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available forthe initial adaptation, respectively. SLM-SAM 2 also exhibits strongerresistance to over-propagation, making a notable step toward more accurateautomated annotation of medical images for segmentation model development.</description>
      <author>example@mail.com (Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski)</author>
      <guid isPermaLink="false">2505.01854v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings</title>
      <link>http://arxiv.org/abs/2505.01711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CXR-TextInter的新框架，用于通过利用大型语言模型（LLMs）来提高胸部X光片（CXR）的自动解释能力，同时结合医学知识模块增强临床推理。&lt;h4&gt;背景&lt;/h4&gt;自动解释胸部X光片是提高临床工作流程和患者护理的关键任务。尽管多模态基础模型有潜力，但有效利用大型语言模型（LLMs）进行视觉任务仍是一个未充分探索的领域。&lt;h4&gt;目的&lt;/h4&gt;开发CXR-TextInter框架，通过结构化文本表示和医学知识模块，实现CXR的准确解释。&lt;h4&gt;方法&lt;/h4&gt;开发了一个名为MediInstruct-CXR的数据集，其中包含结构化图像表示和多样化的临床指令-响应示例，以及CXR-ClinEval基准，用于综合评估各种解释任务。通过在CXR-ClinEval上进行大量实验，评估CXR-TextInter的性能。&lt;h4&gt;主要发现&lt;/h4&gt;CXR-TextInter在病理检测、报告生成和视觉问答等任务上达到了最先进的性能，超过了现有的多模态基础模型。消融实验证实了知识集成模块的关键作用。盲法评估表明，认证放射科医生对CXR-TextInter生成的输出质量有显著偏好。&lt;h4&gt;结论&lt;/h4&gt;本文验证了医学图像AI的替代范式，展示了当视觉信息得到有效结构化且领域知识得到整合时，利用高级LLM能力的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated interpretation of chest X-rays (CXR) is a critical task with thepotential to significantly improve clinical workflow and patient care. Whilerecent advances in multimodal foundation models have shown promise, effectivelyleveraging the full power of large language models (LLMs) for this visual taskremains an underexplored area. This paper introduces CXR-TextInter, a novelframework that repurposes powerful text-centric LLMs for CXR interpretation byoperating solely on a rich, structured textual representation of the imagecontent, generated by an upstream image analysis pipeline. We augment thisLLM-centric approach with an integrated medical knowledge module to enhanceclinical reasoning. To facilitate training and evaluation, we developed theMediInstruct-CXR dataset, containing structured image representations pairedwith diverse, clinically relevant instruction-response examples, and theCXR-ClinEval benchmark for comprehensive assessment across variousinterpretation tasks. Extensive experiments on CXR-ClinEval demonstrate thatCXR-TextInter achieves state-of-the-art quantitative performance acrosspathology detection, report generation, and visual question answering,surpassing existing multimodal foundation models. Ablation studies confirm thecritical contribution of the knowledge integration module. Furthermore, blindedhuman evaluation by board-certified radiologists shows a significant preferencefor the clinical quality of outputs generated by CXR-TextInter. Our workvalidates an alternative paradigm for medical image AI, showcasing thepotential of harnessing advanced LLM capabilities when visual information iseffectively structured and domain knowledge is integrated.</description>
      <author>example@mail.com (Alexander Davis, Rafael Souza, Jia-Hao Lim)</author>
      <guid isPermaLink="false">2505.01711v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification</title>
      <link>http://arxiv.org/abs/2505.01660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Focal-SAM的新方法，用于解决现实世界数据集中长尾分布导致的泛化困难问题。&lt;h4&gt;背景&lt;/h4&gt;现实世界数据集通常遵循长尾分布，使得对尾部类别的泛化变得困难。&lt;h4&gt;目的&lt;/h4&gt;提高模型对尾部类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Focal-SAM通过为类间锐度分配不同的惩罚，在不进行额外反向传播的情况下实现细粒度控制，从而保持效率。&lt;h4&gt;主要发现&lt;/h4&gt;Focal-SAM在提高泛化能力的同时，避免了ImbSAM和CC-SAM在计算效率和损失景观控制之间的权衡。&lt;h4&gt;结论&lt;/h4&gt;Focal-SAM在传统和基础模型上的广泛实验验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world datasets often follow a long-tailed distribution, makinggeneralization to tail classes difficult. Recent methods resorted to long-tailvariants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, toimprove generalization by flattening the loss landscape. However, theseattempts face a trade-off between computational efficiency and control over theloss landscape. On the one hand, ImbSAM is efficient but offers only coarsecontrol as it excludes head classes from the SAM process. On the other hand,CC-SAM provides fine-grained control through class-dependent perturbations butat the cost of efficiency due to multiple backpropagations. Seeing thisdilemma, we introduce Focal-SAM, which assigns different penalties toclass-wise sharpness, achieving fine-grained control without extrabackpropagations, thus maintaining efficiency. Furthermore, we theoreticallyanalyze Focal-SAM's generalization ability and derive a sharper generalizationbound. Extensive experiments on both traditional and foundation models validatethe effectiveness of Focal-SAM.</description>
      <author>example@mail.com (Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang)</author>
      <guid isPermaLink="false">2505.01660v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
      <link>http://arxiv.org/abs/2505.01651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages including references and appendix, 25 pages core text, 3  figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HAIG框架，用于分析人类-人工智能关系中的信任动态。&lt;h4&gt;背景&lt;/h4&gt;现有的分类框架（如“人机交互”模型）不足以捕捉人工智能系统如何从工具发展到伙伴，尤其是在基础模型展现出涌现能力，多智能体系统表现出自主目标设定行为时。&lt;h4&gt;目的&lt;/h4&gt;HAIG框架旨在更好地描述系统发展过程中代理权的复杂分配模式，以及信任关系的维护。&lt;h4&gt;方法&lt;/h4&gt;HAIG框架在三个层面运作：维度（决策权分配、过程自主性和问责配置）、连续体（每个维度的渐进变化）和阈值（需要治理适应的关键点）。&lt;h4&gt;主要发现&lt;/h4&gt;HAIG框架采用信任-效用导向，关注维持适当的信任关系，以最大化效用并确保足够的保障措施。分析揭示了自我监督、推理权限和分布式决策在非均匀信任演化中的作用，这些演化是在情境变化和技术进步的驱动下发生的。&lt;h4&gt;结论&lt;/h4&gt;案例研究在医疗保健和欧洲法规中展示了HAIG框架如何补充现有框架，并为预测治理挑战的替代方法提供基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为HAIG的框架，用于分析人类与人工智能之间信任动态的发展。现有的分类框架，如“人机交互”模型，无法充分捕捉人工智能系统从工具发展到伙伴的过程，尤其是在基础模型展现出涌现能力，多智能体系统表现出自主目标设定行为时。HAIG框架在三个层面运作：维度（决策权分配、过程自主性和问责配置）、连续体（每个维度的渐进变化）和阈值（需要治理适应的关键点）。该框架采用信任-效用导向，关注维持适当的信任关系，以最大化效用并确保足够的保障措施。研究发现，自我监督、推理权限和分布式决策在非均匀信任演化中发挥了作用，这些演化是在情境变化和技术进步的驱动下发生的。在医疗保健和欧洲法规的案例研究中，HAIG框架展示了其如何补充现有框架，并为预测治理挑战的替代方法提供基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces the HAIG framework for analysing trust dynamics acrossevolving human-AI relationships. Current categorical frameworks (e.g.,"human-in-the-loop" models) inadequately capture how AI systems evolve fromtools to partners, particularly as foundation models demonstrate emergentcapabilities and multi-agent systems exhibit autonomous goal-settingbehaviours. As systems advance, agency redistributes in complex patterns thatare better represented as positions along continua rather than discretecategories, though progression may include both gradual shifts and significantstep changes. The HAIG framework operates across three levels: dimensions(Decision Authority Distribution, Process Autonomy, and AccountabilityConfiguration), continua (gradual shifts along each dimension), and thresholds(critical points requiring governance adaptation). Unlike risk-based orprinciple-based approaches, HAIG adopts a trust-utility orientation, focusingon maintaining appropriate trust relationships that maximise utility whileensuring sufficient safeguards. Our analysis reveals how technical advances inself-supervision, reasoning authority, and distributed decision-making drivenon-uniform trust evolution across both contextual variation and technologicaladvancement. Case studies in healthcare and European regulation demonstrate howHAIG complements existing frameworks while offering a foundation foralternative approaches that anticipate governance challenges before theyemerge.</description>
      <author>example@mail.com (Zeynep Engin)</author>
      <guid isPermaLink="false">2505.01651v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
      <link>http://arxiv.org/abs/2505.01571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PainFormer的自动疼痛评估系统，通过多任务学习在14个任务/数据集上训练，能够有效提取多种输入模态的高质量嵌入，并在疼痛评估方面取得卓越表现。&lt;h4&gt;背景&lt;/h4&gt;疼痛是一个影响大量人群的复杂状况，准确可靠的疼痛评估对于开发有效的疼痛管理方案至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行连续监测并支持决策过程的自动疼痛评估系统，以减轻痛苦并预防功能下降。&lt;h4&gt;方法&lt;/h4&gt;PainFormer是一个基于视觉的多任务学习基础模型，同时训练于14个任务/数据集，共计1090万个样本。它作为各种输入模态的嵌入提取器，为基于Transformer的Embedding-Mixer模块提供特征表示，该模块执行最终的疼痛评估。&lt;h4&gt;主要发现&lt;/h4&gt;使用包括RGB、合成热成像和估计深度视频在内的行为模态以及ECG、EMG、GSR和fNIRS在内的生理模态的广泛实验表明，PainFormer能够从不同的输入模态中有效提取高质量嵌入。&lt;h4&gt;结论&lt;/h4&gt;在单模态和多模态设置中进行的实验证明了该框架在各个模态中均达到最先进的性能，为通用自动疼痛评估模型的发展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The study proposes an automatic pain assessment system named PainFormer, which is a vision foundation model based on multi-task learning principles trained on 14 tasks/datasets with a total of 10.9 million samples. It acts as an embedding extractor for various input modalities, providing feature representations to the Embedding-Mixer module, which is based on Transformer and performs the final pain assessment. Extensive experiments using behavioral modalities such as RGB, synthetic thermal, and estimated depth videos, as well as physiological modalities such as ECG, EMG, GSR, and fNIRS, show that PainFormer can effectively extract high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pain is a manifold condition that impacts a significant percentage of thepopulation. Accurate and reliable pain evaluation for the people suffering iscrucial to developing effective and advanced pain management protocols.Automatic pain assessment systems provide continuous monitoring and supportdecision-making processes, ultimately aiming to alleviate distress and preventfunctionality decline. This study introduces PainFormer, a vision foundationmodel based on multi-task learning principles trained simultaneously on 14tasks/datasets with a total of 10.9 million samples. Functioning as anembedding extractor for various input modalities, the foundation model providesfeature representations to the Embedding-Mixer, a transformer-based module thatperforms the final pain assessment. Extensive experiments employing behavioralmodalities-including RGB, synthetic thermal, and estimated depth videos-andphysiological modalities such as ECG, EMG, GSR, and fNIRS revealed thatPainFormer effectively extracts high-quality embeddings from diverse inputmodalities. The proposed framework is evaluated on two pain datasets, BioVidand AI4Pain, and directly compared to 73 different methodologies documented inthe literature. Experiments conducted in unimodal and multimodal settingsdemonstrate state-of-the-art performances across modalities and pave the waytoward general-purpose models for automatic pain assessment.</description>
      <author>example@mail.com (Stefanos Gkikas, Raul Fernandez Rojas, Manolis Tsiknakis)</author>
      <guid isPermaLink="false">2505.01571v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</title>
      <link>http://arxiv.org/abs/2505.01558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the 2025 CVPR Workshop on Foundation and Large Vision  Models in Remote Sensing, to appear in CVPR 2025 Workshop Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用新兴地理空间基础模型进行领域泛化的方法，通过结合软对齐伪标签和源到目标生成预训练，以提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;遥感技术在土地覆盖和土地利用制图、作物产量预测以及环境监测等方面有广泛应用。尽管卫星技术的发展扩大了遥感数据集，但高性能分割模型仍然依赖于大量标注数据，受到标注稀缺性和传感器、光照和地理差异性的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种领域泛化方法，以改善模型泛化能力。&lt;h4&gt;方法&lt;/h4&gt;结合软对齐伪标签和源到目标生成预训练，利用新兴地理空间基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;新的数学洞察力被应用于基于MAE的生成学习，以实现领域不变特征学习。&lt;h4&gt;结论&lt;/h4&gt;在超光谱和多光谱遥感数据集上的实验证实了该方法在提高适应性和分割效果方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：遥感技术使多种关键应用成为可能，如土地覆盖和土地利用制图、作物产量预测和环境监测。卫星技术的进步扩大了遥感数据集，但高性能分割模型仍然依赖于大量标注数据，面临着标注稀缺性和传感器、光照和地理差异性的挑战。领域自适应提供了一种改善模型泛化能力的有希望的方法。本文介绍了一种利用新兴地理空间基础模型进行领域泛化的方法，通过结合软对齐伪标签与源到目标生成预训练。我们进一步对基于MAE的生成学习进行了新的数学洞察，以实现领域不变特征学习。在超光谱和多光谱遥感数据集上的实验证实了我们的方法在提高适应性和分割效果方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing enables a wide range of critical applications such as landcover and land use mapping, crop yield prediction, and environmentalmonitoring. Advances in satellite technology have expanded remote sensingdatasets, yet high-performance segmentation models remain dependent onextensive labeled data, challenged by annotation scarcity and variabilityacross sensors, illumination, and geography. Domain adaptation offers apromising solution to improve model generalization. This paper introduces adomain generalization approach to leveraging emerging geospatial foundationmodels by combining soft-alignment pseudo-labeling with source-to-targetgenerative pre-training. We further provide new mathematical insights intoMAE-based generative learning for domain-invariant feature learning.Experiments with hyperspectral and multispectral remote sensing datasetsconfirm our method's effectiveness in enhancing adaptability and segmentation.</description>
      <author>example@mail.com (Anan Yaghmour, Melba M. Crawford, Saurabh Prasad)</author>
      <guid isPermaLink="false">2505.01558v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>
      <link>http://arxiv.org/abs/2505.01481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoHallu，一个用于评估合成视频真实性的基准，并探讨了现有模型在常识和物理法则上的幻觉问题。&lt;h4&gt;背景&lt;/h4&gt;合成视频生成技术因其真实性和广泛应用而受到关注，但现有模型在生成内容时往往忽视常识和物理法则，导致异常内容。&lt;h4&gt;目的&lt;/h4&gt;提出VideoHallu基准，用于检测合成视频中的异常内容，并研究如何提高多模态大型语言模型（MLLMs）在合成视频中的推理能力。&lt;h4&gt;方法&lt;/h4&gt;设计包含多种类别和专家设计的问答任务的基准，评估多个MLLMs模型，并通过Group Relative Policy Optimization（GRPO）对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;尽管在MVBench和MovieChat等任务上表现良好，这些模型在合成视频中的常识和物理任务上仍存在幻觉问题。微调后，模型在推理能力上取得了显著提升。&lt;h4&gt;结论&lt;/h4&gt;VideoHallu基准有助于评估合成视频的真实性，微调MLLMs可以提升其在合成视频中的推理能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces VideoHallu, a benchmark for evaluating the authenticity of synthetic videos, and explores the hallucination problems of existing models in common sense and physical laws.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic video generation with foundation models has gained attention forits realism and wide applications. While these models produce high-qualityframes, they often fail to respect common sense and physical laws, resulting inabnormal content. Existing metrics like VideoScore emphasize general qualitybut ignore such violations and lack interpretability. A more insightfulapproach is using multi-modal large language models (MLLMs) as interpretableevaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalitiesin synthetic videos remains underexplored. To address this, we introduceVideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,and Kling, paired with expert-designed QA tasks solvable via human-levelreasoning across various categories. We assess several SoTA MLLMs, includingGPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 andVideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,these models still hallucinate on basic commonsense and physics tasks insynthetic settings, underscoring the challenge of hallucination. We furtherfine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on realand synthetic commonsense/physics data. Results show notable accuracy gains,especially with counterexample integration, advancing MLLMs' reasoningcapabilities. Our data is available at https://github.com/zli12321/VideoHallu.</description>
      <author>example@mail.com (Zongxia Li, Xiyang Wu, Yubin Qin, Guangyao Shi, Hongyang Du, Dinesh Manocha, Tianyi Zhou, Jordan Lee Boyd-Graber)</author>
      <guid isPermaLink="false">2505.01481v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials</title>
      <link>http://arxiv.org/abs/2505.01438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在动态加载下，两相随机材料（TRMs）中的全局应力演变和时空超分辨率问题。&lt;h4&gt;背景&lt;/h4&gt;材料应力分析对材料和性能优化至关重要。在动态加载下，材料的全局应力演变表现出复杂的时空特性，特别是在两相随机材料中。这种材料的失效通常与应力集中有关，相界面是应力集中的关键位置。&lt;h4&gt;目的&lt;/h4&gt;解决在实际工程应用中，由于微结构数据的时空分辨率有限，深学习方法在生成高分辨率时空应力场方面面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态加载下两相随机材料全局应力生成和时空超分辨率的框架。首先，引入了一种基于扩散模型的时空应力扩散（STS-diffusion）方法来生成全局时空应力数据。其次，开发了一种物理信息网络，称为时空超分辨率物理信息算子（ST-SRPINN），用于时空超分辨率。ST-SRPINN是一种无监督学习方法，详细探讨了数据驱动和物理信息损失函数权重对模型准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架通过物理约束，可以在训练过程中仅需要低分辨率的应力场数据，并将其时空分辨率升级到任意放大倍数。&lt;h4&gt;结论&lt;/h4&gt;该研究为两相随机材料在动态加载下的应力分析和超分辨率提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporals super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Material stress analysis is a critical aspect of material design andperformance optimization. Under dynamic loading, the global stress evolution inmaterials exhibits complex spatiotemporal characteristics, especially intwo-phase random materials (TRMs). Such kind of material failure is oftenassociated with stress concentration, and the phase boundaries are keylocations where stress concentration occurs. In practical engineeringapplications, the spatiotemporal resolution of acquired microstructural dataand its dynamic stress evolution is often limited. This poses challenges fordeep learning methods in generating high-resolution spatiotemporal stressfields, particularly for accurately capturing stress concentration regions. Inthis study, we propose a framework for global stress generation andspatiotemporal super-resolution in TRMs under dynamic loading. First, weintroduce a diffusion model-based approach, named as Spatiotemporal StressDiffusion (STS-diffusion), for generating global spatiotemporal stress data.This framework incorporates Space-Time U-Net (STU-net), and we systematicallyinvestigate the impact of different attention positions on model accuracy.Next, we develop a physics-informed network for spatiotemporalsuper-resolution, termed as Spatiotemporal Super-Resolution Physics-InformedOperator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learningmethod. The influence of data-driven and physics-informed loss function weightson model accuracy is explored in detail. Benefiting from physics-basedconstraints, ST-SRPINN requires only low-resolution stress field data duringtraining and can upscale the spatiotemporal resolution of stress fields toarbitrary magnifications.</description>
      <author>example@mail.com (Tengfei Xing, Xiaodan Ren, Jie Li)</author>
      <guid isPermaLink="false">2505.01438v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    </channel>
</rss>